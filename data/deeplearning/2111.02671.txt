2
2
0
2

b
e
F
9

]
E
S
.
s
c
[

3
v
1
7
6
2
0
.
1
1
1
2
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

1

GraphSearchNet: Enhancing GNNs via
Capturing Global Dependency for
Semantic Code Search

Shangqing Liu, Xiaofei Xie, Jingkai Siow, Lei Ma, Guozhu Meng, and Yang Liu

Abstract—Code search aims to retrieve accurate code snippets based on a natural language query to improve software productivity and
quality. With the massive amount of available programs such as (on GitHub or Stack Overﬂow), identifying and localizing the precise code
is critical for the software developers. In addition, Deep learning has recently been widely applied to different code-related scenarios, e.g.,
vulnerability detection, source code summarization. However, automated deep code search is still challenging due to the semantic gap
between the program and the natural language query. Most existing deep learning-based approaches for code search rely on the
sequential text i.e., feeding the program and the query as a ﬂat sequence of tokens to learn the program semantics while the structural
information is not fully considered. Furthermore, the widely adopted Graph Neural Networks (GNNs) have proved the effectiveness in
learning program semantics, however, they also suffer the problem of capturing the global dependency in the constructed graph, which
limits the model learning capacity. To address these challenges, in this paper, we design a novel neural network framework, named
GraphSearchNet, to enable an effective and accurate source code search by jointly learning rich semantics of both source code and
natural language queries. Speciﬁcally, we propose to construct graphs for the source code and queries with bidirectional GGNN
(BiGGNN) to capture the local structural information of the source code and queries. Furthermore, we enhance BiGGNN by utilizing the
multi-head attention module to supplement the global dependency that BiGGNN missed to improve the model learning capacity. The
extensive experiments on Java and Python programming language from the public benchmark CodeSearchNet conﬁrm that
GraphSearchNet outperforms current state-of-the-art works by a signiﬁcant margin.

Index Terms—Code Search, Graph Neural Networks, Multi-Head Attention

(cid:70)

1 INTRODUCTION

With the fast development of the software industry over the past few
years, the global source code over public and private repositories
(e.g., on GitHub or Bitbucket) is reaching an unprecedented amount.
It is already commonly recognized that the software industry is
entering the “Big Code” era. Code search, which aims to search the
relevant code snippets based on the natural language query from a
large code corpus (e.g., Github, Stack Overﬂow, or private ones),
has become a critical problem in the “Big Code” era. In addition,
some studies [1], [2] also have shown that more than 90% of the
efforts from the software developers aim at reusing the existing
code. Hence, an accurate code search system can greatly improve
software productivity and quality, while reducing the software
development cost.

Automated code search is far from settled. Some early attempts
were started with leveraging information retrieval (IR) techniques
to capture the relationship of the code and the query by keyword
matching [2], [3], [4], [5]. However, such techniques are ad-hoc,
making it limited especially when no common keywords exist in the

•

Shangqing Liu, Jingkai Siow, Yang Liu are with Nanyang Tech-
nological University, Singapore. E-mail: shangqin001@e.ntu.edu.sg,
jingkai001@e.ntu.edu.sg, yangliu@ntu.edu.sg

• Xiaofei Xie is with Singapore Management University, Singapore. E-mail:

xiaofei.xfxie@gmail.com
Lei Ma is with University of Alberta, Canada. E-mail: ma.lei@acm.org
•
• Guozhu Meng is with SKLOIS, Institute of Information Engineering,
Chinese Academy of Sciences, China. E-mail: mengguozhu@iie.ac.cn

• Xiaofei Xie is the corresponding author.

Manuscript received February 5, 2022

source code and queries. Furthermore, the extracted keywords from
the query tend to be short, which cannot represent the rich semantics
behind the text. To address these limitations, many works expand
the query format and reformulate it with different expressions [3],
[6], [7], [8], [9]. For example, Lu et al. [3] expanded the query with
some synonyms generated from WordNet [10] to improve the hit
ratio. CodeMatcher [11] proposed an IR-based model by collecting
metadata for query words to identify irrelevant/noisy ones and
iteratively performing the fuzzy search with the important query
words on the codebase to return the program candidates. These IR-
based techniques essentially perform the matching process based
on keywords, the semantic gap between the program and the natural
language query has still existed.

To further mitigate the semantic gap between the program and
the query, more recent attempts shifted to deep learning (DL)-
based techniques [12], [13], [14], [15], [16], [17], [18], [19], [20],
which encode the source code and the query into vectors (i.e.,
learning the representations behind the program and the query).
Then, the similarity between two vectors, such as cosine similarity,
is computed to measure the semantic relevance between the code
snippet and the query. Code snippets with a higher similarity score
of the query are returned as the searching results. However, most of
these works only rely on sequential models i.e., Long-Short Term
Memory (LSTMs) [21], Self-Attention [22] to learn the vector
representations for both code and query. These sequential models
are struggling to learn the semantic relations because they ignore
the structural information hidden in the text to learn. In addition,
MMAN [17] encoded the program sequence, abstract syntax tree
(AST) and control ﬂow graph (CFG) with LSTM [21], Tree-

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

LSTM [23] and GGNN [24] respectively on the C programming
language. Then it further encoded the query sequence with another
LSTM to learn the mapping relationship between the code and
query, however, MMAN ignored the structural information behind
the query and the limitations of GGNN i.e., the missed global
dependency in a graph [25], [26] in the process of GGNN learning,
is not well-addressed. To sum up, although some recent progress
have been made for automated code search, the key challenges
still exist: (1) source code and the natural language queries are
heterogeneous [12], they have completely different grammatical
rules and language structure, which leads to the semantic mapping
is hard; (2) the rich structural information behind in the code and
the query fails to explore. Failing to utilize the rich structural
information beyond the simple text may limit the effectiveness of
these approaches for code search; (3) Although there are some
existing works [17] attempted to use GNNs for code search, the
limitations of GNNs are not well-addressed.

To address these challenges, in this paper, we design a novel
neural network framework, named GraphSearchNet, towards learn-
ing the representations that fully utilize the structural information
to capture the semantic relations of source code and queries for
code search. In particular, since the large corpus of the query
dataset is hard to collect, we follow the existing works [12],
[13], [27], [28] and use the summary of a code snippet for the
replacement to jointly train the program encoder and the summary
encoder. Speciﬁcally, we convert the program to a graph with
syntactic edges (AST Edge, NextToken SubToken) and data-ﬂow
edges (ComputedFrom, LastUse and LastWrite) to represent the
program semantics. Furthermore, we also build the summary graph
based on the dependency parsing [29]. For each encoder, we
feed the constructed graph to Bidirectional Gated Graph Neural
Network i.e., BiGGNN [30] to capture the structural information
in a graph. We further enhance BiGGNN by the multi-head
attention module to supplement the missed global dependency
that BiGGNN fails to learn to improve the model learning capacity.
Once GraphSearchNet is trained, at the query phase, given a natural
language query, the summary encoder is utilized to obtain the query
vector, then the top-k program candidates are returned based on
the cosine similarity between the query vector and the program
vectors that are embedded from the program encoder on the large
search code database.

To demonstrate the effectiveness of our approach, we in-
vestigate the performance of GraphSearchNet against 11 state-
of-the-art baselines on Java and Python dataset from the open-
sourced CodeSearchNet [13], which has over 2 million functions
and evaluate these approaches in terms of 5 evaluation metrics
such as R@k, MRR, NDCG. We further conduct a quantita-
tive analysis on 99 real queries to conﬁrm the effectiveness
of GraphSearchNet. The extensive experimental results show
that GraphSearchNet signiﬁcantly outperforms the baselines on
the evaluation metrics. Furthermore, GraphSearchNet can also
produce high-quality programs based on the real natural language
query from the quantitative analysis. Our code is available
at https://github.com/shangqing-liu/GraphSearchNet. Overall, we
highlight our contributions as follows:
• We propose a novel graph-based framework to capture the
structural and semantic information for accurately learning
semantic mapping of the program and the query for code search.
• We design GraphSearchNet to improve model learning capacity
by BiGGNN to capture the local structural information in a graph
and multi-head attention to capture the global dependency that

2

BiGGNN fails to learn.

• We conduct an extensive evaluation to demonstrate the effec-
tiveness of GraphSearchNet on the large code corpora and the
experimental results demonstrate that GraphSearchNet outper-
forms the state-of-the-art baselines by a signiﬁcant margin. we
have made our code public to beneﬁt academia and the industry.
The reminder of this paper is organized as follows: Section 2
presents the background and the motivation of GraphSearchNet.
We elaborate our approach in Section 3. Section 4 and Section 5
are our experimental setup and the experimental results. Section 6
gives some discussions about GraphSearchNet, followed by the
related works in Section 7. We conclude our paper in Section 8.

2 BACKGROUND AND MOTIVATION
In this section, we ﬁrst brieﬂy introduce the background of graph
neural networks, then detail the motivation for the design of
GraphSearchNet and followed by introducing the existing datasets
for code search and the multi-head attention that we will use in
GraphSearchNet.

2.1 Graph Neural Networks

Graph Neural Networks (GNNs) [24], [31], [32], [33] have attracted
wide attention over the past few years since GNNs can handle
complex structural data, which contains the elements (nodes)
with the relations (edges) between them. Hence, a variety of
scenarios such as social networks [34], programs [35], chemical and
biological systems [36] leverage GNNs to model graph-structured
data. Generally, a directed graph G = (V, E) contains a node list
V and the edge list E, where (u, v) ∈ E denotes an edge from
the node u to the node v. The learning process for GNNs is to
propagate neural messages (node features) in the neighboring nodes
and it is named neural message passing. It consists of multiple
hops and at every hop, each node aggregates the received messages
from its neighbors and updates the representation by combining the
aggregated incoming messages with its own previous representation.
Formally, for a node v has an initial representation h0
v ∈ Rd, where
d is the dimensional length. The initial representation is usually
derived from its label or the given features. Then, a hop updates
its representation by the message passing and obtain the new
representation h1
v ∈ Rd. This process can be expressed as follows:

v = f (h(k−1)
h(k)

v

, {h(k−1)
u

|u ∈ Nv}; θk)

(1)

where Nv is a set of nodes that have edges to v. k denotes k-th
hop and the total number of hops K is determined empirically as a
hyper-parameter and 1 ≤ k ≤ K. The function f usually distin-
guishes different GNN variants. For example, graph convolution
networks (GCN) [32] can be expressed as:

h(k)

v = σ(

(cid:88)

u∈Nv∪{v}

1
cv,u

W h(k−1)
u

)

(2)

is often set

where cv,u is a normalization factor and it
to
(cid:112)|Nv| · |Nu| or |Nv|, W is the learnable weight and σ is a
non-linearity such as rectiﬁed linear unit (ReLU) [37]. Another
widely used GNN variant in the program scenario is gated graph
neural network (GGNN) [24], which uses a recurrent unit r e.g.,
GRU [38] or LSTM [21] for the update. The message passing can
be calculated as follows:

v = r(h(k−1)
h(k)

v

,

(cid:88)

u∈Nv

W h(k−1)
u

; θr)

(3)

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

where θr is the recurrent cell parameters and W is the learnable
weight.

2.2 Motivation

Program semantics learning by deep learning techniques can be
considered as the fundamental problem for a variety of code-related
tasks. Compared with considering the program as a ﬂat sequence
of tokens with the sequential model such as LSTMs [21] or Self-
Attention [22] to learn program semantics, Allamanis et al [35]
lighted up this ﬁeld by converting the program into a graph with
different relations of the nodes to represent the program semantics
and further utilized GNNs to capture the node relations. After that,
many graph-based works for different tasks [17], [25], [39], [40],
[41], [42] have emerged in both AI and SE community. These works
have proved the effectiveness of GNNs [24], [32], [35] in modelling
a program to capture program semantics. Furthermore, due to the
powerful relation learning capacity of GNNs, they also have been
widely used in many NLP tasks, e.g., natural question generation
(QG) [30], [43], conversational machine comprehension (MC) [44],
[45], [46]. Annervaz et al. [47] further conﬁrmed that augmenting
graph information with LSTM can improve the performance of
many NLP tasks. Hence, inspired by these works, in this paper, we
propose our approach to convert the program and the query into
graphs with GNNs to learn the semantic relations for code search.
However, recent advanced works on GNNs [25], [26], [42]
have proved that GNNs are powerful to capture the signals from
the short-range nodes, while the long-range information from the
distant nodes cannot well-handled. It is caused by the message
passing computation process. Speciﬁcally, the total number of hops
K limits the network can only sense the range of the interaction
between nodes at a radius of K i.e., the receptive ﬁeld at K. We
give a simple example as shown in Fig. 1 for better explanation. In
particular, when K = 1, node 1 can only know the information
from its neighbor i.e., node 2. Furthermore, when K = 2, node
1 knows the information from its neighbor i.e., node 2 and node
2’s neighbor i.e., node 3. When we increase K to 3, node 1 can
sense the information from node 2, node 3 and node 4. Hence, to
let node 1 know the information about its far-distant node, we must
increase K to a large value. However, the hyper-parameter K can
not be set to a large value due to the over-squashing [26] and over-
smoothing [48], [49] problem in GNNs. Hence, empirically, K is
often set to a small value [25], [32] to avoid this problem. It leads
GNNs are powerful in capturing the local structural information
while failing to capture the global dependency in a graph and this
inspires us to explore how to alleviate this limitation and increase
the learning capacity of GNNs for code search.

3

Fig. 1: Message passing of GNNs within K-th neighborhood
information where the dash area is the receptive ﬁeld that node 1
knows.

search, the sufﬁcient amount of (program, query) pairs is hard to
collect and CodeSearchNet utilizes (program, summary) pairs for
the replacement at the learning phase, where the summary describes
the functionality of a function. CodeSearchNet further provides a
number of 99 real natural language queries e.g., “convert decimal
to hex” to retrieve the related programs from the search codebase,
which is independent of the dataset that is used for training the
model. We also follow these settings for evaluation.

2.4 Multi-Head Attention

Self-attention is the key idea in the transformer [22], which is
widely used in NLP. An attention function can be described as
mapping a query and a set of key-value pairs to an output, where
the query, keys, values and output are all vectors. The particular
Scaled Dot-Product Attentionr [22] can be expressed as follows:

Attention(Q, K, V ) = Softmax(

QKT
√
dk

)V

(4)

where dk is the dimensional length, Q, K, and V represent the
query, key, and value matrices. To further improve the expression
capacity of the self-attention, it is beneﬁcial to linearly project the
queries, keys and values h times with different linear projections
and then concatenate and project to obtain the ﬁnal outputs and
this calculation process is named multi-head attention [22]:

MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O

where headi = Attention(QW Q

i , KW K

i , V W V
i )

(5)

i ∈ Rdmodel×dk , W K

where W Q
i ∈
Rdmodel×dv and W O ∈ Rhdv×dmodel are model parameters and
h is the number of heads.

i ∈ Rdmodel×dk , W V

2.3 Existing Datasets for Code Search

There are some existing datasets designed for code search, such as
CodeSearchNet [13], DeepCS [50], FB-Java [27], CosBench [28].
The released dataset by DeepCS are processed by the authors and
the raw data cannot be obtained for the graph construction. The
other datasets such as FB-Java and CosBench are both collected
from Java projects. In contrast, CodeSearchNet contains the data
from six programming languages, besides Java, it also consists
data from other programming languages such as Python. Since
different programming language has speciﬁc grammatical features,
GraphSearchNet aims at proving the proposed approach is robust
against different languages, hence we select Python and Java data
from CodeSearchNet for the evaluation. Furthermore, in deep code

3 GRAPHSEARCHNET
In this section, we introduce our framework GraphSearchNet, as
shown in Fig. 2, which includes three sequential parts: 1) Graph
Construction, which constructs directed graphs for programs and
summaries with comprehensive semantics. 2) Training Phase,
which jointly learns two separate encoders i.e., the program encoder
fc and the summary encoder fs to obtain the vector representations
respectively. Each encoder includes two modules: BiGGNN, which
aims at capturing local graph structural information, and multi-head
attention, which supplements the global dependency that GNNs
missed to enhance the model learning capacity. 3) Query Phase,
which returns a set of top-k candidates from the search codebase

123456X1-hop2-hop3-hopJOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

4

Fig. 2: The framework of GraphSearchNet.

that are most similar with the given query based on the cosine
similarity, where the query is independent with the summary used
for model training and the search codebase is different from the
training set that used to train the model.

3.2 Graph Construction

We introduce the graph construction for both programs and
summaries that will be used for the encoder to learn.

3.1 Problem Formulation

The goal of code search is to ﬁnd the most relevant program
fragment c based on the query q in natural language. Formally,
given a set of training data D = {(ci, qi)|ci ∈ C, qi ∈ Q}, i ∈
{1, 2, ..., n}, where C and Q denote the set of functions and the
corresponding queries. We deﬁne the learning problem as:

min

n
(cid:88)

i=1

L(fc(ci), fq(qi))

(6)

where fc and fq are the separate encoders i.e., the neural network
for programs and queries, which are learnt from the training data
D. However, in the real scenario, since D is hard to collect, it
is a common practise [12], [13], [27], [28] to replace qi with the
summary si regarding the function ci i.e., D = {(ci, si)|ci ∈
C, si ∈ S} where S is the corresponding summary set. The
summary si usually describes the functionality of the function
ci. Once the encoders are trained i.e., fc and fs, given a query q
and the search code database Cbase where Cbase (cid:54)= D , we can
obtain the most relevant program as:

c = max

ci∈Cbase

sim(fc(ci), fs(q))

(7)

where sim is a function such as the cosine similarity function
to measure the semantic similarity between the program vector
and query vector, which are produced by the encoder fc and fs
respectively.

3.2.1 Program Graph Construction

Given a program c, we follow Allamanis et al. [35] and extract
its multi-edged directed graph G = (V, E), where V is a set of
nodes that are built on the Abstract Syntax Tree (AST) and E is the
edges that represent the relationships between nodes. In particular,
AST consists of terminal nodes and non-terminal nodes where
terminal nodes correspond to the identiﬁers in the program and
the non-terminal nodes represent different compilation units such
as “Assign”, “BinOp”, “Expr”. The edges can be categorized into
syntactic edges i.e., AST Edge, NextToken and data-ﬂow edges
i.e., ComputedFrom, LastUse, LastWrite. In addition, according
to Cvitkovic et al. [51], SubToken edge can further enrich the
semantics of a program on the graph, we also include it and
introduce extra subtoken nodes appearing in the identiﬁer of the
terminal node and connecting them to their original nodes. Hence,
the node set V is the union of the entire AST nodes and subtoken
nodes. The details of these types of edges with a simpliﬁed example
of our constructed program graph in Fig. 3 (a) and Fig. 3 (b) are
presented as follows:

• AST Edge, connects the entire AST nodes based on the abstract

syntax tree.

• NextToken, connects each leaf node in AST to its successor.
As shown in Fig. 3 (a), for the statement “fn a(x)”, there is a
NextToken edge points from “fn a” to “x”.

• SubToken, deﬁnes the connection of subtokens split from a iden-
tiﬁer i.e., variable names, function names based on camelCase
and pascal case convention. For example, “fn a” will be divided

SummaryProgram GraphSummary GraphProgramSummary Encoder fsNode Embeddings…VectorMax Pool(Program, Summary)Program Encoder fcMaximize SimilarityTraining SetNode Embeddings…Max PoolVectorBiGGNNBiGGNNMulti-Head AttentionWord Embeddings…Mean Pool…MeanPoolWord EmbeddingsSearch CodeBaseCosine Similarity“How to save list to file?”Summary EncoderProgramVectorsQueryVectorMulti-Head AttentionTraining Phase:Query Phase:QueryProgram Encoder…Program GraphConstructionSummary GraphConstructionJOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

5

Fig. 3: An example of the constructed graphs where (a) and (b) are the syntactic edges and data-ﬂow edges of the program graph with a
simple program snippet, please note that we distinguish the variable x into x1, x2, x3, x4 for ease of the clarity in data-ﬂow edges, (c) is
the constructed summary graph.

into “fn” and “a”. We further introduce the extra subtoken nodes
apart from AST nodes to describe this relation.

• LastUse, represents the immediate last read of each occurrence
of variables. As shown in Fig. 3 (b), x3 points to x1 since x1
is used as the conditional judgement, x4 points to x1 and x2
because if the conditional judgement is not satisﬁed, x4 points to
x1 directly, otherwise it will point to x2.

• LastWrite, represents the immediate last write of each occurrence
of variables. Since there is an assignment statement to update x2,
x4 points to x2 with a LastWrite edge.

• ComputedFrom, connects the left variable to all right variables
appeared in an assignment statement. In Fig. 3 (b), x2 connects
x3 and y by ComputedFrom edge.

3.2.2 Summary Graph Construction

Constituency parse tree [52] and dependency parse tree [29] are
widely used to extract the structural information for the natural
language text [30]. However, compared with constituency parse tree,
dependency parse tree follows the dependency grammars [53] and
describes the dependencies by the directed linked edges between
tokens in a sentence, hence the constructed graph describes the
relations among the tokens without redundant information i.e., no
non-terminal nodes in the constructed graph. Motivated by this, we
also construct a directed graph based on dependency parsing [29]
for the summary. Speciﬁcally, given a summary s, we extract its
dependency parsing graph as G(cid:48) = (V (cid:48), E (cid:48)), where V (cid:48) is a set
of nodes where each node is the token in the original sequence
s, E (cid:48) denotes the relations between these tokens. Different edges
represent tokens with different grammatical relations. Furthermore,
there are a total of 49 different dependency edge types [54], for
example, the edge “prep” is a prepositional modiﬁer of a verb,
adjective, or noun that serves to modify the meaning of the verb,
adjective, noun [54]. In addition, we also construct NextToken
and SubToken edge in the summary graph, which is similar to the
program graph. To summarize, given a query “How to check for
null”, which is shown in Fig. 3 (c), we can obtain its summary
graph where each token has different dependency relations with
other tokens, for example, “How”, “to” and “for” are used to
describe the relations with the verb “check” in the relations of
‘advmod”, “aux”, “prep” respectively.

3.3 Encoder

the summary s with its summary graph G(cid:48), we further feed them
into two encoders fc and fs to learn the vector representations
for the program and the summary separately. A variety of GNN
variants are proposed such as GCN [32], GGNN [24], GIN [55]
to model the graph-structured data. Considering the most existing
GNN variants ignore the direction for the message passing and
treat the graph as the undirected graph, in this work, inspired by
the recent advanced Bidirectional Gated Graph Neural Network
(BiGGNN) [30], which leverages both incoming and outgoing
message passing for the node interaction, we also use it for the
graph learning. Furthermore, to supplement the global dependency
that is missed in GNNs, we propose a multi-head attention module
to further improve the expression of BiGGNN. In the following,
we only use the program c with its constructed program graph G
for the explanation, the operations for the summary encoder fs is
the same with the program encoder fc, the difference is that we
feed the summary s with its constructed graph G(cid:48) for the summary
encoder to learn the representation.

3.3.1 Bidirectional GGNN
Given a program graph G = (V, E), BiGGNN learns the node
embeddings from both incoming and outgoing directions for the
graph. In particular, each node v ∈ V is initialized by a learnable
embedding matrix E and gets its initial representation h0
v ∈ Rd
where d is the dimensional length. We apply the message passing
function for a ﬁxed number of hops i.e., K. At each hop k ≤ K,
for the node v, we apply a summation aggregation function to
take as input a set of incoming (or outgoing) neighboring node
vectors and outputs a backward (or forward) aggregation vector.
The message passing is calculated as follows, where N(v) denotes
the neighbors of node v and (cid:97) / (cid:96) is the backward and forward
direction.

hk
hk

N(cid:97)(v)

N(cid:96)(v)

= SUM({hk−1
= SUM({hk−1

u

u

, ∀u ∈ N(cid:97)(v)})

, ∀u ∈ N(cid:96)(v)})

Then, we fuse the node embedding for both directions:

hk

N(v)

= Fuse(hk

N(cid:97)(v)

, hk

N(cid:96)(v)

)

(8)

(9)

Here the fusion function is designed as a gated sum of two inputs.

Fuse(a, b) = z (cid:12) a + (1 − z) (cid:12) b
z = σ(W z[a; b; a (cid:12) b; a − b] + bz)

(10)

By the program graph construction and summary graph construc-
tion, we can get the program c with its program graph G and

where (cid:12) is the component-wise multiplication, σ is a sigmoid
function and z is a gating vector. Finally, we feed the resulting

ExpressionModel InvocationMethodArgumentsfn_axfnyx3x2x1x4(a) Syntactic edges of the statement “fn_a(x)” (b) Data-ﬂow edges of  statements “If (x1 !=null) x2= fn_a(x3) + y;  return x4” AST EdgeSubtokenNextTokenComputedFromLastWriteLastUse(c) Summary Graph of the sentence “How to check for null”HowtocheckfornulladvmodauxpreppobjaDependency EdgeNextTokenJOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

vector to a Gated Recurrent Unit (GRU) [38] to update node
representations.

v = GRU(hk−1
hk

v

, hk

N(v)

)

(11)

v and then apply max-pooling over all nodes {hK

After K hops of computation, we obtain the ﬁnal node representa-
tion hK
v , ∀v ∈ V }
to get a d-dim graph representation hg.
hg = maxpool(FC({hK

v , ∀v ∈ V }))

(12)

where FC is a fully-connected layer.

3.3.2 Multi-Head Attention
To further capture the global interactions in the graph that BiGGNN
missed [25], [26], we design a multi-head attention module to im-
prove the model learning capacity. Note that in GraphSearchNet, the
constructed program graph consists of terminal/non-terminal nodes
and the terminal nodes are the tokens/subtokens of the original
program c, hence we directly employ multi-head attention [22]
over the sequence i.e., c to capture the global dependency relations
among these terminal nodes while ignoring the non-terminal nodes
for effective learning. Speciﬁcally, given the query1, key and value
matrix deﬁned as Q, K and V , where Q, K, V = (Et1, ..., Etl )
are the embedding matrices for the program c and ti is the i-th
token in c = {t1, ..., tl}, the output by multi-head attention can be
expressed as follows:

ht1, ..., htl = MultiHead(Q, K, V )
(13)
To get the ﬁnal representation vector hc over c, we use the mean
pool operation on ht1 , ..., htl as follows.

hc = meanpool({hti, ∀ti ∈ c})
Here we choose the meanpool operation rather than the maxpool
operation that used in BiGGNN is based on our experiments and
we found that meanpool operation can get higher results in multi-
head attention. Finally, we concatenate the vectors produced from
Bidirectional GGNN i.e., hg and multi-head attention i.e., hc to
get the ﬁnal representation r = [hg; hc] for the program.

(14)

By the program encoder fc and the summary encoder fs, we
can obtain the paired vector deﬁned as (r, r(cid:48)) where r and r(cid:48) are
the output vectors for the program and the summary respectively.

3.4 Training
Two different loss functions are used to train a model in deep
code search systems [13], [50]. The ranking loss [56], [57] that
used in Gu et al. [50] needs to determine a hyper-parameter (cid:15),
however by our preliminary experiments, we found that the value
is hard to determine manually, although it was set to 0.05 in their
experiments. Instead, following CodeSearchNet [13], we directly
use Cross-Entropy for the optimizing. Speciﬁcally, given n pairs
of (ci, si), we train fc and fs simultaneously by minimizing the
loss as follows:

−

1
n

(cid:32)

n
(cid:88)

log

(cid:33)

i r(cid:48)
rT
i
j r(cid:48)
j rT
i

(cid:80)

(15)

i
The loss function minimizes the inner product of (r(cid:48)
i, ri) between
the summary si and its corresponding program ci, while maximiz-
ing the inner product between the summary si and other distractor
programs, i.e., cj where j (cid:54)= i.

6

3.5 Code Searching

Once the model is trained, given a query q, GraphSearchNet aims
to return a set of the most relevant programs. To achieve this,
we ﬁrst embed and stored all programs into vectors by the learnt
program encoder fc in an ofﬂine manner and these programs are
from the search code database Cbase, which is independent with
the data set used for model training. At the online query phase,
for a new query q, GraphSearchNet embeds it via the summary
encoder fs and computes the cosine similarity between the query
vector with all stored program vectors in the search codebase. The
top-k programs whose vectors are the most similar i.e., the highest
top-k similarity values, to the query q are returned as the results.
In GraphSearchNet, we select 10 candidate programs i.e., k = 10
as the returned results.

4 EXPERIMENTAL SETUP

In this section, we introduce the experimental setup including
the dataset, evaluation metrics, compared baselines and the hyper-
parameter conﬁguration of GraphSearchNet.

4.1 Dataset

We select Java and Python data from CodeSearchNet [13] to evalu-
ate our approach that can be generalized to different programming
languages. The reason to choose Java and Python datasets is that
there are some existing open-sourced tools [40], [58] for Java
and Python programming language to facilitate constructing the
program graph. We follow the same train-validation-test split with
CodeSearchNet. In addition, we utilize spaCy [59] to construct
the summary graph. At the query phase, for all programs in the
search codebase, we employ the open-sourced ElasticSearch [60]
to store the vectors obtained by the learnt program encoder fc for
acceleration. Converting all programs from the search codebase
into vectors costs nearly 70 minutes for Java or Python over
756k samples in an ofﬂine manner. However, the query process
provided by ElasticSearch is fast and it only takes about 0.15
second to produce 10 candidates for each query based on the
cosine similarity. To sum up, in the evaluation section, we ﬁrst
investigate the performance of GraphSearchNet on the test set with
some automatic metrics. Then we conduct a quantitative analysis
over the real 99 queries with the programs from the search codebase
to conﬁrm the effectiveness of our approach.

4.2 Automatic Evaluation Metrics

[15],

to the previous works [12],

Similar
[16], we use
SuccessRate@k, Mean Reciprocal Rank (MRR) as our automatic
evaluation metrics. We further add Normalized Discounted Cumu-
lative Gain (NDCG) [13] as another evaluation metric.
• SuccessRate@k. SuccessRate@k measures the percentage of
the correct results existed in the top-k ranked results and it can
be calculated as follows:

SuccessRate@k =

1
|Q|

|Q|
(cid:88)

q=1

δ(FRankq ≤ k)

(16)

1. The query matrix is different with the natural language query, where the
former is a matrix used in self-attention, and the latter is used to return the
programs in code search. More explanations about the query matrix can be
found in Section 2.4.

where Q is a set of queries, δ(·) is a function, which returns 1 if
the input is true and 0 otherwise. FRank is the rank position of
the ﬁrst hit result in the result list and we set k to 1, 5, 10.

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

• MRR. MRR is the average of the reciprocal ranks of results for
a set of queries Q. The reciprocal rank of a query is the inverse
of the rank of the ﬁrst hit result, deﬁned as follows:

MRR =

1
|Q|

|Q|
(cid:88)

q=1

1
FRankq

(17)

• NDCG. NDCG measures the gain of the result based on its
position in the result list and it is the division of discounted
cumulative gain (DCG) and ideal discounted cumulative gain
(IDCG) where DCG and IDCG are calculated as follows:

p
(cid:88)

DCGp =

i=1

2reli − 1
log2(i + 1)

|REL|p
(cid:88)

IDCGp =

i=1

2reli − 1
log2(i + 1)

(18)
where p is the rank position, reli is the graded relevance of the
result at position i and |REL|p is the list of relevant results up
to position p. We set p equals to 10 for all experiments.

To sum up, for the above automatic metrics, the higher values, the
better performance the approach achieves.

4.3 Compared Baselines

To demonstrate the effectiveness of GraphSearchNet, we select the
following state-of-the-art approaches as our baselines, which are
summarized as follows.
• CodeSearchNet [13]. CodeSearchNet provided a framework to
encode programs and their summaries for code search. It consists
of different encoders, i.e., Natural Bag of Words (NBoW), 1D
Convolutional Neural Network (1D-CNN), Bidirectional RNN
(biRNN) and Self-Attention (SelfAtt) for encoding. Speciﬁcally,
SelfAtt encoder has 3 identical multi-head attention layers with
128-dimensional length and each layer has 8 heads to learn
different subspace features.

• UNIF [61]. UNIF followed the framework of CodeSearch-
Net [13], but
it replaced the self-attention mechanism by
combining each bag of code token vectors into a single code
vector for the program and averaging a bag of query token
vectors as a single vector for a query. Then it retrieved the most
similar programs on the query.

• DeepCS [12]. DeepCS jointly learnt code snippets and natural
language descriptions into a high-dimensional vector space.
For code encoding, it fused method name, API sequence, and
sequence tokens to learn the program vector. For the description,
another RNN was utilized for learning the embedding. Based on
the learnt vectors, it further used the ranking loss function [56],
[57] to train the model and retrieved the most similar programs.
• CARLCS-CNN [16]. CARLCS-CNN embedded the representa-
tions for the code and query via a co-attention mechanism and
co-attended the semantic relations of the embedded code and
query via row/column-wise max-pooling. Then the learnt vectors
were used for code search.

• TabCS [62]. TabCS incorporated the code textual features i.e.,
method name, API sequence and tokens, the code structural
feature i.e., AST and the query feature i.e., tokens with a
two-stage attention network to learn the better code and query
representation. Then two datasets were used for the evaluation.
One of them is the Java dataset from CodeSearchNet [13], which
is the same as us.

• QC-based CR [14]. The original paper ﬁrst designed a code
annotations model to take the code as an input and output an

7

annotation. Then it utilized the generated annotation with the
query for a QN-based code retrieval model. In addition, they also
added a QC-based code retrieval model to take the code with its
query as the input to distinguish the code snippets from others.
The entire model cannot run correctly by the ofﬁcial code, for
simplify, we only select QC-based code retrieval model as our
baseline.

We also select two widely used GNN variants i.e., GGNN and
GCN to investigate the performance of other GNNs as compared
to BiGGNN. We directly replace BiGGNN module with GCN or
GGNN for both encoders i.e., fc and fs and keep the other settings
unchanged for a fair comparison. For both GNN variants, since they
are not used in the code search currently, we implement them from
the scratch; for TabCS, since the evaluation dataset i.e., Java dataset
from CodeSearchNet, and the evaluation metrics are the same with
us, we directly take the values reported in their paper [62] for
comparison, for other baselines with the released source code,
we directly reproduce their methods with the default settings on
our dataset. We do not compare with MMAN [17] because their
approach was conducted on C dataset and the relevant tool to
construct the program graph for C programming language is not
available so far.

4.4 Model Settings

We embed the most frequent 150,000 tokens for the programs and
summaries in the training set with a 128-dimensional size. We set
the dropout as 0.3 after the word embedding layer for the learning
process. We use Adam [63] optimizer with an initial learning rate
of 0.01 and reduce the learning rate by a factor of 0.5 with the
patience equals to 2. We set the number of the maximum epoch as
100 and stop the training process when no improvement on MRR
for 10 epochs. We clip the gradient at length 10. The batch size
is set as 1,000, following the existing work [13]. 1000 samples
in a batch can be explained as there is a corrected program ci for
the current summary si and the remaining 999 programs in this
batch are distractors for the current si to distinguish. The number
of hops on Java and Python is set to 4 and 3, respectively. The
number of heads in multi-head attention is set to 2 on both Java
and Python datasets for efﬁciency. All experiments were run on the
DGX server with 80 cores and 180G RAM. Four 32 GB Nvidia
Graphics Tesla V100 were used to train the models and the training
process took nearly 4 hours to complete. All hyper-parameters are
tuned on the validation set.

5 EXPERIMENTAL RESULTS

In this section, we aim to answer the following research questions
by our experiments:

• RQ1: Can GraphSearchNet outperform current state-of-the-art

baselines in terms of automatic metrics?

• RQ2: What is the performance of each component used in
GraphSearchNet, are BiGGNN and multi-head attention both
beneﬁcial in improving the performance?

• RQ3: What is the impact of setting different values of hops
and heads on the performance? Whether these values perform
consistently on Java and Python datasets?

• RQ4: Can GraphSearchNet provide more accurate programs for
the real 99 queries provided by CodeSearchNet [13] compared
with other baselines?

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022
TABLE 1: Experimental results on Java and Python datasets as compared to baselines, where the marker * denotes the values are taken
from the original paper and the marker - denotes the unreported metrics.

8

Model

NBoW
biRNN
SelfAtt Encoder
1D-CNN
UNIF
DeepCS
CARLCS-CNN
TabCS
QC-based CR
GGNN
GCN
GraphSearchNet

R@1
52.11
45.65
42.09
37.91
52.79
33.40
42.60
54.70*
19.03
48.59
41.55
56.99

R@5
72.65
67.70
62.96
58.79
73.39
56.50
57.90
68.30*
40.77
70.39
66.07
76.03

Java
R@10
78.98
75.33
71.01
67.10
79.67
67.30
67.50
74.80*
51.68
77.66
72.92
81.15

NDCG MRR
61.60
54.30
55.90
62.11
52.00
67.18
47.80
54.89
62.20
46.70
44.64
48.78
43.31
47.90
53.90*
-
29.72
33.94
58.72
63.09
52.96
57.54
65.80
69.47

R@1
56.85
54.18
57.81
41.21
57.47
53.60
68.70
-
21.02
59.69
48.71
65.31

R@5
78.47
77.01
79.02
64.65
78.95
73.40
78.10
-
43.83
80.41
72.25
84.21

Python
R@10
84.21
83.29
84.46
72.55
84.43
78.90
82.80
-
54.17
85.80
79.19
89.05

NDCG MRR
66.60
55.18
65.50
70.76
67.40
76.62
52.10
63.56
67.10
44.25
62.91
66.10
67.00
70.08
-
-
32.03
36.26
69.07
72.78
59.50
63.77
73.90
77.30

5.1 RQ1: Compared with Other Baselines.

Table 1 summarizes the results of GraphSearchNet in line with
the baseline methods. Speciﬁcally, the columns R@1, R@5 and
R@10 are the results of SuccessRate@k, where k is 1, 5 and 10.
The second row presents the results by considering the program
and the summary as the sequential text with different networks for
retrieving, the third row is the results for other GNN variants. We
can observe that in the second row, the performance of different
approaches is inconsistent on different programming languages,
for example, in terms of MRR, on the Java dataset, UNIF gets
the best performance, while SelfAtt Encoder performs the best on
Python dataset. One reason we conjecture is that these sequential
approaches cannot capture the semantic mappings for both program
and summary, hence they may bias to some speciﬁc programming
languages and cannot generate well on others. Furthermore, we also
ﬁnd that QC-based CR has the worst performance on both datasets,
we believe it is due to the fact that we only employ the part of
the ofﬁcially released code for comparison, however, the entire
project cannot run correctly by our huge efforts. In addition, we
ﬁnd that GGNN outperforms GCN by a signiﬁcant margin for code
search, it is reasonable since GRU cell in GGNN has been proved
the effectiveness in ﬁltering unnecessary features than GCN [26].
This also explains why current state-of-the-art models [35], [39],
[41] select GGNN as the basic block.

The last row shows our results, we can see that GraphSearchNet
outperforms these sequential approaches (the second row) by a
signiﬁcant margin on both Java and Python datasets, indicating that
GraphSearchNet can return more relevant programs. The results
indicate that, compared with these sequential models, which treats
programs and summaries as sequences, by capturing the structural
information, GraphSearchNet could learn the semantic relations
better, making it more effective and accurate in code search.
Furthermore, we ﬁnd that GraphSearchNet has a better performance
than GGNN and GCN, we attribute it to the bidirectional message
passing and multi-head attention module that can have powerful
learning capacity. More details about the performance of each
module can be found that Section 5.2. Finally, we can see that the
overall performance on Python is superior to Java, the main reason
is that Python has simpler grammatical features and the semantic
gap is smaller than Java.
(cid:45) (cid:73)Answer to RQ1(cid:74) The performance of the sequential-
based approaches are inconsistent on Java and Python dataset,
we contribute it to the missed structural information learnt by
these networks. In contrast, GraphSearchNet outperforms them

signiﬁcantly. Furthermore, the bidirectional message passing and
multi-head attention module could improve the model learning
capacity to produce better results as compared to other GNN
variants.

5.2 RQ2: Ablation Study on Each Component in Graph-
SearchNet.

We conduct the ablation study to investigate the impact of each
component i.e., BiGGNN and multi-head attention on the separate
encoder to conﬁrm the local structural information and the global
dependency in the graph are both beneﬁcial for code search.

The experimental results are shown in Table 2, where the
checkmark denotes the used component. We can observe that
BiGGNN improves the performance signiﬁcantly as compared
to multi-head attention for the program encoder or the summary
encoder. It is an interesting ﬁnding because compared with the
graphs used in the program scenario [25], [35], [39], [41], the
transformer architecture [22], where multi-head attention is the
key component in it, is dominating the NLP community. Hence,
encoding the code into graphs is a common practice for a program,
however, it is not very common for the natural language, even
there are some existing works [30], [43], [44], [45], [46]. One
possible reason we conjecture is that the standard transformer [22]
is equipped with the encoder-decoder architecture, which is
more powerful for the text generation tasks such as machine
translation [22], [64], [65], text summarization [66], [67]. The
experimental results conﬁrm the necessity to explore the structural
information behind the query text for code search. Furthermore, we
also ﬁnd that the performance on Python dataset is still higher than
Java when turning off some components, which indicates that the
model is easier to learn the semantics on Python rather than Java.
(cid:45) (cid:73)Answer to RQ2(cid:74) BiGGNN and multi-head attention are
both effective in improving the performance, however, the local
structural information captured by BiGGNN is more critical.
When incorporating both, GraphSearchNet can get the best
performance.

5.3 RQ3: Hop&Head Analysis.

We further investigate the impact of the different number of hops
(see K in Eq. 12) and heads (see h in Eq. 5) on the capacity to
capture the local structural information and the global dependency
in the graph. Speciﬁcally, we set the number of hops in a range
of 1 to 5 and heads in a range of 1 to 8 and keep the other

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

9

TABLE 2: Ablation study of the performance on the encoder with different components on Java and Python data set.

Program

Summary

BiGGNN Multi-Head

Dataset

Java

Python

Multi-Head
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

BiGGNN
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

R@1

R@5

R@10

NDCG MRR

29.95
45.51
25.00
48.26
56.99
13.76
55.55
27.55
60.03
65.31

55.18
67.55
48.66
70.75
76.03
35.27
77.11
52.49
80.90
84.21

64.42
74.16
58.16
77.15
81.15
46.63
83.24
61.89
86.37
89.05

46.59
59.78
40.91
62.77
69.47
28.53
69.28
43.89
73.31
77.30

41.74
55.65
36.19
58.51
65.80
24.52
65.41
39.23
69.59
73.90

TABLE 3: The statistics of the graph size in the constructed
program graph and the summary graph for Java and Python dataset.

Dataset

Java

Python

Node
Edge
Node
Edge

Program Graph

Summary Graph

min max
200
55
263
54
200
10
477
9

avg
125.99
125.22
100.35
171.55

min max
200
422
199
408

3
2
3
2

avg
17.44
31.29
14.00
24.21

settings unchanged for a fair comparison. Note that we only turn on
BiGGNN with the multi-head attention closed for two encoders to
analyse the impact of hops (vice versa for head analysis), which is
different to the settings in Section 5.2, where the speciﬁc module is
closed for one encoder. Since the trend on R@1, R@5 and R@10
is similar to MRR and NDCG, we only show the results of MRR
and NDCG in Fig. 4.

We can see that in Fig. 4a and Fig. 4b, with the increasing
number of hops, MRR and NDCG improve gradually and they
reach the highest at 4 and 3 for Java and Python respectively,
after that the values begin to decrease, which is consistent with
the ﬁndings that GNNs suffer from the over-squashing [26] when
increasing the hops to learn the information from long-distant
nodes. Hence, in our experiment, we set the value to 4 and 3 for
Java and Python to achieve the best performance. The reason why
Java needs more hops than Python is that the constructed graph for
Java is larger than Python and the statistical distributions of nodes
and edges on the dataset is shown in Table 3. We can observe that
the average node size in the program graph and summary graph
on the Java dataset is 125.99 and 17.44, which is larger than the
Python dataset i.e., 100.35 and 14.00 respectively. Furthermore,
the average connections (edges) of Python on the program graph
is larger than Java i.e., 171.55 VS 125.22. Hence, the constructed
graph for Python dataset tends to be smaller and better-connected,
which makes the model require fewer hops than Java to achieve the
best performance. For mutil-head attention to capture the global
dependency, from Fig. 4c and Fig. 4d, we ﬁnd that the trend
is basically the same on Java and Python. When increasing the
number of heads to 2, the values reach the highest, which indicates
2 heads are enough to learn the global dependency in a graph.

(cid:45) (cid:73)Answer to RQ3(cid:74) The optimal value of hops is relied on
the graph size in the dataset. In GraphSearchNet, we set the hops
to 4 and 3 for Java and Python to achieve the best performance on
the used dataset. The setting of the number of heads is consistent
on both Java and Python datasets where 2 heads are enough to
capture the global dependency.

(a) MRR of Different Hops

(b) NDCG of Different Hops

(c) MRR of Different Heads

(d) NDCG of Different Heads

Fig. 4: The effect of the number of hops and heads to capture the
local structural information and the global dependency.

5.4 RQ4: Quantitative Analysis for Real 99 Queries.

We compare GraphSearchNet with the baseline CodeSearchNet,
which consists of NBoW, biRNN, SelfAtt Encoder and 1D-CNN
network, and UNIF to evaluate the performance on the real
99 queries because the selected baselines tend to have a better
performance than others in Table 1. Speciﬁcally, we return top-10
results for each query by different approaches. To measure the
quality of returned programs, we calculate the cosine similarity
between the query vector produced by the summary encoder fs
and the returned top-10 program vectors produced by the program
encoder fc and further average them over 99 queries to get the
mean score 2. The mean similarity scores for these approaches
are shown in Table 4. We can ﬁnd that the scores produced by
GraphSearchNet are higher than others, which indicates that our
approach can generate more accurate programs to the real query.
We attribute it to the effectiveness of our approach on learning the
semantic relations behind the program and the query.

Furthermore, we show two examples of the returned top-1
results for Java and Python from the search codebase in Table 5

2. The range of standard cosine similarity score belongs to [-1, 1], however,
ElasticSearch changes the score into real positive values by adding “1” to the
similarity score and updating the range to [0, 2]

12345Hops60.062.565.067.570.072.575.077.580.0MRRJavaPython12345Hops60657075808590NDCGJavaPython12345678Heads30.032.535.037.540.042.545.047.550.0MRRJavaPython12345678Heads30354045505560NDCGJavaPythonJOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

and Table 6 by GraphSearchNet and the best baseline i.e., SelfAtt
Encoder in Table 4. The completed top-10 results of 99 queries
are provided in our ofﬁcial repository. We ﬁnd that given the query
“get executable path” and “how to determine a string is a valid
word”, the generated results by SelfAtt Encoder are less relevant to
the query on both Java and Python. In particular, for the query, “get
executable path”, the results by GraphSearchNet are more accurate
than SelfAtt Encoder to produce a program with the functionality
of returning a path. For the query, “how to determine a string is
a valid word”, it aims at verifying whether a string is a word, the
produced results by SelfAtt Encoder indicate that it misunderstood
the query semantics and return the programs related to the password
veriﬁcation for both Java and Python. In contrast, GraphSearchNet
can produce more accurate and semantic-relevant programs based
on the query.
(cid:45) (cid:73)Answer to RQ4(cid:74) By calculating the average cosine
similarity score over the returned program vectors with the real
query vector, we ﬁnd that GraphSearchNet could produce more
accurate and semantic-relevant programs. In addition, we present
some examples as compared to the baselines to further conﬁrm
the effectiveness of our approach.

6 DISCUSSION
This section presents the impact of the dimensional size used in
GraphSearchNet. We further discuss the widely used pre-trained
models in the program scenario and followed by the threats to the
validity of our work.

10

(a) MRR

(b) NDCG

Fig. 5: The effect of the dimensional size with regard to MRR and
NDCG.

1,000 batches. Although these pre-trained models are often released
to the public, the improvements at the ﬁne-tuning phase for the
downstream tasks are often brought by these pre-trained models.
Hence, we claim that it is unfair to compare GraphSearchNet with
these pre-trained approaches; (3) In terms of the methodology,
the pre-trained approaches mostly rely on NLP techniques such
as BERT [70] to learn the program representation. The learning
process is to learn a model directly by the huge model parameters
e.g., CodeBERT has a total number of 125M parameters, hence the
semantics behind the program are not well explored. In contrast,
GraphSearchNet only has around 2M parameters, however, by
converting the program into a graph with different semantic
information to capture the program semantics, it still produce
promising results.

6.1 Impact of Dimensional Size

6.3 Threats to Validity

We study the impact of different dimensional size on the perfor-
mance of GraphSearchNet. We only change the dimensional size
(32/64/128/256) and keep the remaining settings unchanged for
the evaluation. The results are shown in Fig. 5. We can observe
that the performance improves greatly as the feature size increases
since the learning capacity of the model is increased with the
growing dimension size. However, we can also see that after the
128-dimensional size, MRR and NDCG on Java dataset improve
slightly, while on Python dataset, there is an obvious decrease
from the highest. We conjecture this is caused by when set to
higher dimensional size, the learning capacity is further increased
to make the model overﬁt to the Python dataset, which harms the
performance on Python dataset. In addition, with the increasing
dimensional size, the training time and GPU memory consumption
are also increased. For uniﬁcation, we set the dimensional size to
128 on both Java and Python datasets for the evaluation.

6.2 Pre-trained Models

Recently, there are some unsupervised pre-trained approaches
e.g., CodeBERT [68], GraphCodeBERT [69] that aim at learning
the general program representation for a variety of tasks. The
improvements are signiﬁcant as compared to supervised techniques.
However, in GraphSearchNet, we do not consider the pre-trained
approaches based on these points: (1) the pre-trained approaches
require the massive data (usually over million data) to pre-train e.g.,
CodeBERT included 2.1M bimodal and 6.4M unimodal data at the
pre-training phase; (2) the training process for these approaches are
much more expensive e.g., CodeBERT utilized 16 interconnected
GPU cards for parallel training and it cost 600 minutes to train

The internal threats to validity lie in our implementations, the
graph construction for the program and the summary, the model
implementation. To reduce this threat, we utilize the open-source
tool [40], [58] for Java and Python program graph construction,
Spacy [59] for the summary graph construction. To reduce the im-
plementation threat, the co-authors carefully check the correctness
of our implementation. We further make implementation public
at https://github.com/shangqing-liu/GraphSearchNet for further
investigation.

The external threats to validity include the selected datasets
and the evaluation of the baselines and the evaluation metrics.
To reduce the impact of the dataset, we select the Java and
Python datasets from CodeSearchNet [13], which has been widely
used for other works [68], [69], [71], [72] to evaluate the model
performance. For the evaluation of baselines, we select the open-
sourced baselines [12], [13], [14], [16], [61], [62] and evaluate the
performance based on the default hyper-parameters used in the
original papers. We consider these default parameters are optimal.
We carefully checked the released code to mitigate the threat
and plan to evaluate more hyper-parameters in the future. For the
evaluation metrics, there are some other metrics rather than we used
such as mean average precision (MAP) [73], which can be used
to evaluate the retrieval system, however, we follow the existing
works [12], [13], [14], [16], [61], [62] in code search and select the
widely used SuccessRate@k, MRR and NDCG for the evaluation.

7 RELATED WORK
This section summarizes the related work on code search, graph
neural networks, and program representation techniques.

3264128256Dimensional Size404550556065707580MRRJavaPython3264128256Dimensional Size404550556065707580NDCGJavaPythonJOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

11

TABLE 4: The average cosine similarity score over the 99 real queries.

Model

Avg

NBoW

biRNN

Java
0.90

Python
0.87

Java
0.84

Python
0.92

SelfAtt Encoder
Python
Java
1.04
1.03

1D-CNN

Java
0.75

Python
0.81

UNIF

Java
0.91

Python
0.87

GraphSearchNet
Python
Java
1.24
1.25

TABLE 5: Two examples of the queried top-1 results for Java from the search codebase.

Query

SelfAtt Encoder

get executable path

how to determine a string is a valid word

private Path getCollectionPath() {

Path collectionPath = PathUtils.
ReportsDirectory
.getPathForObject(getInstance());
return collectionPath.
resolve(provider.
getArtifactPath(getInstance()));

}

private static String promptForPassword

(String passwordName,
String commandLineOption, String envVarName)
throws Exception {
final Console console = System.console();
if(console == null) {

throw new Exception("Cannot allocate
a console. Set env var "+envVarName+"
or "+commandLineOption+"

on commandline in that case");

}
return new String(console.
readPassword("[%s]", passwordName+

" password:"));

}

GraphSearchNet
public static String getPathToExecutable
(File featureDir, String path) {

File scriptPath = new File(path);
if ( ! scriptPath.isAbsolute() ) {

scriptPath =
new File(featureDir, path);

}
return scriptPath.getAbsolutePath();

}

public static boolean isWord

(@Nullable final String sStr) {
if (StringHelper.hasNoText (sStr))

return false;

return isWord (sStr.toCharArray ());

}

TABLE 6: Two examples of the queried top-1 results for Python from the search codebase.

Query

SelfAtt Encoder

GraphSearchNet

get executable path

how to determine a string is a valid word

def get_mp_bin_path():

plat = platform.system()
if plat == "Linux":

return resource_filename(__name__,
"bin/metaparticle/linux/mp-compiler")

elif plat == "Windows":

return resource_filename(__name__,
"bin/metaparticle/windows/mp-compiler.exe")

elif plat == "Darwin":

return resource_filename(__name__,
"bin/metaparticle/darwin/mp-compiler")

else:

raise Exception("Your platform
is not supported.")

def _validate_admin_password(admin_password):

password_regex = r"[A-Za-z0-9@#$%ˆ&+=]{6,}"
pattern = re.compile(password_regex)
if not pattern.match(admin_password):

raise ValueError(red(

"The password must be at
least 6 characters and
contain only the
following characters:\n"
"A-Za-z0-9@#$%ˆ&+="

))

return admin_password

def get_path():

return os.path.abspath(os.path.
dirname(os.path.dirname(__file__)))

def isValid(self, text, word):

return bool(re.search(word, text,

re.IGNORECASE))

7.1 Code Search

Early works [2], [3], [4], [5], [9], [11] in code search utilized
information retrieval (IR) to query the code by extracting both
query and code characteristics. For example, Lu et al. [3] extended
a query with some synonyms generated from WordNet to improve
the hit ratio. McMillan et al. [8] proposed Portfolio, a code
search engine that combines keyword matching to return functions.
CodeHow [9] extended the query with some APIs and utilized them
with a boolean model to retrieve the matched programs. Despite
these works could produce some accurate results, the performance
of proposed approaches mainly rely on the searched code base and
the semantic gap between the program and query is not addressed.
To learn the semantics for the program and the query, some deep
learning-based approaches were proposed [12], [13], [14], [15],
[16], [18], [19], [20], [61] and these works attempted to use the
deep learning techniques e.g., LSTMs, CNNs, Transformer to learn
high-dimensional representations for programs and queries. For
example, DeepCS [50] encoded API sequence, tokens, and function
names with multiple LSTMs to get the vector representations, and
further encoded the queries into another vector with an extra
LSTM for code search. CodeSearchNet [13] explored some basic

encoders such as LSTM, CNN, SelfAtt Encoder for code search.
Furthermore, UNIF [61] proposed that using a simple bag-of-
word model with an attention mechanism can achieve state-of-
the-art performance. However, these DL-based techniques in code
search are mostly based on sequential model and ignore the rich
structural information behind the programs and queries. In contrast,
GraphSearchNet attempts to extract structural information hidden
in the text for the program and the query and further constructs the
graphs to capture the semantic relations between them. Another
work MMAN [17] encoded the program sequence, AST and CFG
on a manually collected C dataset with LSTM, Tree-LSTM and
GGNN to learn the representation of a program. It further encoded
the query with another LSTM for multi-modal learning. However,
the relevant tools for the graph construction are not available
yet and we can not reproduce the experiments on our dataset. In
addition, we encode the program and the query into graphs on Java
and Python datasets and further improve the learning capacity of
GNNs by BiGGNN and multi-head attention to achieve the best
performance.

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

12

7.2 Graph Neural Networks

REFERENCES

GNNs [24], [31], [32], [33] have attracted wide attention due to the
power in learning structure data. Various applications from different
domains such as chemistry biology [36], computer vision [74],
natural language processing [30], [33] have demonstrated the
effectiveness of GNNs. GNNs have also been applied to code-
related tasks. Compared with the early works to represent programs
with abstract syntax tree [75], [76], [77], more works have shifted
to use graphs [35] to learn the semantics for various tasks, such as
source code summarization [25], [40], vulnerability detection [39],
type inference [41], neural program repair [42]. For instance,
Allamnanis et al. [35] employed Gated Graph Neural Network
(GGNN) in predicting the wrong usage of variables in programs.
They further extended the similar approach to type inference [41].
Liu et al. [25] proposed to combine GNNs with the retrieval
information to enhance the generation of code summary. Inspired
by these state-of-the-art works that represent programs with graphs
to capture the program semantics, in GraphSearchNet, we propose
to convert the programs and queries into graphs with BiGGNN to
learn the structure information for semantic search. In addition, we
also enhance BiGGNN by capturing the global dependency for the
nodes in a graph that is missed in the conventional GNNs.

7.3 Program Representation

Program representation, aiming at capturing the syntax and seman-
tics behind the code, is a fundamental yet far-from-settled problem
for code-intelligent tasks. The early works used feature selection
and machine learning approaches such as n-gram model [78],
SVM [79] for classifying source code. For instance, Linares et
al. [79] utilized SVM and API information in classifying the
software into their categories. Later, deep learning techniques
were applied in this area to learn the general representations.
Since abstract syntax tree (AST) is the high abstraction of
programs, many works represented programs based on AST [75],
[76], [77], [80], [81]. For example, TBCNN [80] proposed a
custom convolution neural network on AST to learn the program
representation. CDLH [82] incorporated Tree-LSTM on AST for
clone detection. Code2Vec [76] and Code2Seq [75] learnt the
program representation with random sampled AST paths. In terms
of graph-based techniques, Allamanis et al. [35] innovated program
representations by deﬁning various types of edges on AST and
utilized the Gated Graph Neural Network for learning program
semantics and achieved state-of-the-art performance. Some follow-
up works also adopted this paradigm and employed it on different
tasks, such as source code summarization [25], [40], neural program
repair [42]. Different from these works, which purely built the graph
for programs, we also construct the query graph for code search.
Furthermore, we also explore to improve the learning capacity of
the conventional GNNs.

8 CONCLUSION
In this paper, we propose GraphSearchNet, a novel graph-based
approach for code search to capture the semantic relations of the
source code and the query. We construct the program graph and
the summary graph with Bidirectional Graph Neural Network i.e.,
BiGGNN to learn the local structural information hidden in the
sequential text. We further employ multi-head attention to capture
the global dependency that BiGGNN fails to learn in a graph. The
extensive experiments on Java and Python datasets demonstrate the
effectiveness of GraphSearchNet.

[1] C. Liu, X. Xia, D. Lo, C. Gao, X. Yang, and J. Grundy, “Opportunities
and challenges in code search tools,” ACM Computing Surveys (CSUR),
vol. 54, no. 9, pp. 1–40, 2021.

[2] S. K. Bajracharya and C. V. Lopes, “Analyzing and mining a code search
engine usage log,” Empirical Software Engineering, vol. 17, no. 4, pp.
424–466, 2012.

[3] M. Lu, X. Sun, S. Wang, D. Lo, and Y. Duan, “Query expansion via
wordnet for effective code search,” in 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and Reengineering (SANER).
IEEE, 2015, pp. 545–549.

[4] S. Bajracharya, T. Ngo, E. Linstead, Y. Dou, P. Rigor, P. Baldi, and
C. Lopes, “Sourcerer: a search engine for open source code supporting
structure-based search,” in Companion to the 21st ACM SIGPLAN
symposium on Object-oriented programming systems, languages, and
applications, 2006, pp. 681–682.

[5] K. Krugler, “Krugle code search architecture,” Finding Source Code on

[6]

the Web for Remix and Reuse, pp. 103–120, 2013.
S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and T. Menzies,
“Automatic query reformulations for text retrieval in software engineering,”
in 2013 35th International Conference on Software Engineering (ICSE).
IEEE, 2013, pp. 842–851.

[7] E. Hill, L. Pollock, and K. Vijay-Shanker, “Improving source code search
with natural language phrasal representations of method signatures,” in
2011 26th IEEE/ACM International Conference on Automated Software
Engineering (ASE 2011).

IEEE, 2011, pp. 524–527.

[8] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu, “Portfolio:
ﬁnding relevant functions and their usage,” in Proceedings of the 33rd
International Conference on Software Engineering, 2011, pp. 111–120.

[9] F. Lv, H. Zhang, J.-g. Lou, S. Wang, D. Zhang, and J. Zhao, “Codehow:
Effective code search based on api understanding and extended boolean
model (e),” in 2015 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE).

IEEE, 2015, pp. 260–270.

[10] G. A. Miller, WordNet: An electronic lexical database. MIT press, 1998.
[11] C. Liu, X. Xia, D. Lo, Z. Liu, A. E. Hassan, and S. Li, “Codematcher:
Searching code based on sequential semantics of important query words,”
ACM Transactions on Software Engineering and Methodology (TOSEM),
vol. 31, no. 1, pp. 1–37, 2021.

[12] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in 2018 IEEE/ACM
IEEE,

40th International Conference on Software Engineering (ICSE).
2018, pp. 933–944.

[13] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code search,”
arXiv preprint arXiv:1909.09436, 2019.

[14] Z. Yao, J. R. Peddamail, and H. Sun, “Coacor: code annotation for code
retrieval with reinforcement learning,” in The World Wide Web Conference,
2019, pp. 2203–2214.

[15] R. Haldar, L. Wu, J. Xiong, and J. Hockenmaier, “A multi-perspective
architecture for semantic code search,” in Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics. Online:
Association for Computational Linguistics, Jul. 2020, pp. 8563–8568.
[Online]. Available: https://www.aclweb.org/anthology/2020.acl-main.758
[16] J. Shuai, L. Xu, C. Liu, M. Yan, X. Xia, and Y. Lei, “Improving code
search with co-attentive representation learning,” in Proceedings of the
28th International Conference on Program Comprehension, 2020, pp.
196–207.

[17] Y. Wan, J. Shu, Y. Sui, G. Xu, Z. Zhao, J. Wu, and P. S. Yu, “Multi-modal
attention network learning for semantic source code retrieval,” arXiv
preprint arXiv:1909.13516, 2019.

[18] S. Fang, Y.-S. Tan, T. Zhang, and Y. Liu, “Self-attention networks for
code search,” Information and Software Technology, vol. 134, p. 106542,
2021.

[19] A. A. Ishtiaq, M. Hasan, M. Haque, M. Anjum, K. S. Mehrab,
T. Muttaqueen, T. Hasan, A. Iqbal, and R. Shahriyar, “Bert2code: Can
pretrained language models be leveraged for code search?” arXiv preprint
arXiv:2104.08017, 2021.

[20] L. Du, X. Shi, Y. Wang, E. Shi, S. Han, and D. Zhang, “Is a single model
enough? mucos: A multi-model ensemble learning for semantic code
search,” arXiv preprint arXiv:2107.04773, 2021.

[21] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in
neural information processing systems, 2017, pp. 5998–6008.

[23] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Convolutional neural
networks over tree structures for programming language processing,” in
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

[24] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence

neural networks,” arXiv preprint arXiv:1511.05493, 2015.

[25] S. Liu, Y. Chen, X. Xie, J. K. Siow, and Y. Liu, “Retrieval-augmented
generation for code summarization via hybrid gnn,” in International
Conference on Learning Representations, 2020.

[26] U. Alon and E. Yahav, “On the bottleneck of graph neural
networks and its practical implications,” in International Conference
on Learning Representations, 2021.
[Online]. Available: https:
//openreview.net/forum?id=i80OPhOCVH2

[27] H. Li, S. Kim, and S. Chandra, “Neural code search evaluation dataset,”

arXiv preprint arXiv:1908.09804, 2019.

[28] S. Yan, H. Yu, Y. Chen, B. Shen, and L. Jiang, “Are the code snippets
what we are searching for? a benchmark and an empirical study on code
search with natural-language queries,” in 2020 IEEE 27th International
Conference on Software Analysis, Evolution and Reengineering (SANER).
IEEE, 2020, pp. 344–354.

[29] M.-C. De Marneffe, B. MacCartney, C. D. Manning et al., “Generating
typed dependency parses from phrase structure parses.” in Lrec, vol. 6,
2006, pp. 449–454.

[30] Y. Chen, L. Wu, and M. J. Zaki, “Reinforcement learning based graph-
to-sequence model for natural question generation,” arXiv preprint
arXiv:1908.04942, 2019.

[31] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning
on large graphs,” in Advances in neural information processing systems,
2017, pp. 1024–1034.

[32] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.
[33] K. Xu, L. Wu, Z. Wang, Y. Feng, M. Witbrock, and V. Sheinin,
“Graph2seq: Graph to sequence learning with attention-based neural
networks,” arXiv preprint arXiv:1804.00823, 2018.

[34] W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin, “Graph neural
networks for social recommendation,” in The World Wide Web Conference,
2019, pp. 417–426.

[35] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to represent

programs with graphs,” arXiv preprint arXiv:1711.00740, 2017.

[36] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
A. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs
for learning molecular ﬁngerprints,” in Advances in neural information
processing systems, 2015, pp. 2224–2232.

[37] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted

boltzmann machines,” in Icml, 2010.

[38] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[39] Y. Zhou, S. Liu, J. Siow, X. Du, and Y. Liu, “Devign: Effective
vulnerability identiﬁcation by learning comprehensive program semantics
via graph neural networks,” in Advances in Neural Information Processing
Systems, 2019, pp. 10 197–10 207.

[40] P. Fernandes, M. Allamanis, and M. Brockschmidt, “Structured neural

summarization,” arXiv preprint arXiv:1811.01824, 2018.

[41] M. Allamanis, E. T. Barr, S. Ducousso, and Z. Gao, “Typilus: neural type
hints,” in Proceedings of the 41st acm sigplan conference on programming
language design and implementation, 2020, pp. 91–105.

[42] V. J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber,
“Global relational models of source code,” in International Conference on
Learning Representations, 2019.

[43] D. Su, Y. Xu, W. Dai, Z. Ji, T. Yu, and P. Fung, “Multi-hop question genera-
tion with graph convolutional network,” arXiv preprint arXiv:2010.09240,
2020.

[44] Y. Chen, L. Wu, and M. J. Zaki, “Graphﬂow: Exploiting conversation ﬂow
with graph neural networks for conversational machine comprehension,”
arXiv preprint arXiv:1908.00059, 2019.

[45] L. Song, Z. Wang, M. Yu, Y. Zhang, R. Florian, and D. Gildea, “Exploring
graph-structured passage representation for multi-hop reading compre-
hension with graph neural networks,” arXiv preprint arXiv:1809.02040,
2018.

[46] N. De Cao, W. Aziz, and I. Titov, “Question answering by reasoning
across documents with graph convolutional networks,” arXiv preprint
arXiv:1808.09920, 2018.

[47] K. Annervaz, S. B. R. Chowdhury, and A. Dukkipati, “Learning beyond
datasets: Knowledge graph augmented neural networks for natural
language processing,” arXiv preprint arXiv:1802.05930, 2018.

13

[49] K. Oono and T. Suzuki, “Graph neural networks exponentially lose ex-
pressive power for node classiﬁcation,” arXiv preprint arXiv:1905.10947,
2019.

[50] X. Gu, H. Zhang, D. Zhang, and S. Kim, “Deep api learning,” in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering, 2016, pp. 631–642.

[51] M. Cvitkovic, B. Singh, and A. Anandkumar, “Open vocabulary learning
on source code with a graph-structured cache,” in International Conference
on Machine Learning. PMLR, 2019, pp. 1475–1485.

[52] D. Jurafsky and J. H. Martin, “Speech and language processing: An
introduction to natural language processing, computational linguistics,
and speech recognition.”

[53] V. Keselj, “Speech and language processing daniel jurafsky and james h.

martin,” 2009.

[54] M.-C. De Marneffe and C. D. Manning, “Stanford typed dependencies
manual,” Technical report, Stanford University, Tech. Rep., 2008.
[55] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph

neural networks?” arXiv preprint arXiv:1810.00826, 2018.

[56] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
P. Kuksa, “Natural language processing (almost) from scratch,” Journal of
machine learning research, vol. 12, no. ARTICLE, pp. 2493–2537, 2011.
[57] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and
T. Mikolov, “Devise: A deep visual-semantic embedding model,” 2013.

[58] A. Rice. (2018) features-javac. [Online]. Available: https://github.com/

acr31/features-javac/

[59] M. Honnibal and I. Montani, “spaCy 2: Natural language understanding
with Bloom embeddings, convolutional neural networks and incremental
parsing,” 2017, to appear.

[60] C. Gormley and Z. Tong, Elasticsearch: the deﬁnitive guide: a distributed

real-time search and analytics engine.

” O’Reilly Media, Inc.”, 2015.

[61] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, “When deep
learning met code search,” in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, 2019, pp. 964–974.
[62] L. Xu, H. Yang, C. Liu, J. Shuai, M. Yan, Y. Lei, and Z. Xu, “Two-stage
attention-based model for code search with textual and structural features,”
in 2021 IEEE International Conference on Software Analysis, Evolution
and Reengineering (SANER).

IEEE, 2021, pp. 342–353.

[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[64] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473,
2014.

[65] M.-T. Luong, H. Pham, and C. D. Manning, “Effective ap-
proaches to attention-based neural machine translation,” arXiv preprint
arXiv:1508.04025, 2015.

[66] M. Maybury, Advances in automatic text summarization. MIT press,

1999.

[67] Y. Liu and M. Lapata, “Text summarization with pretrained encoders,”

arXiv preprint arXiv:1908.08345, 2019.

[68] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming
and natural languages,” arXiv preprint arXiv:2002.08155, 2020.

[69] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svy-
atkovskiy, S. Fu et al., “Graphcodebert: Pre-training code representations
with data ﬂow,” arXiv preprint arXiv:2009.08366, 2020.

[70] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[71] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identiﬁer-aware
uniﬁed pre-trained encoder-decoder models for code understanding and
generation,” arXiv preprint arXiv:2109.00859, 2021.

[72] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement,
D. Drain, D. Jiang, D. Tang et al., “Codexglue: A machine learning
benchmark dataset for code understanding and generation,” arXiv preprint
arXiv:2102.04664, 2021.

[73] H. Sch¨utze, C. D. Manning, and P. Raghavan, Introduction to information
retrieval. Cambridge University Press Cambridge, 2008, vol. 39.
[74] W. Norcliffe-Brown, S. Vafeias, and S. Parisot, “Learning conditioned
graph structures for interpretable visual question answering,” in Advances
in neural information processing systems, 2018, pp. 8334–8343.

[75] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating
sequences from structured representations of code,” arXiv preprint
arXiv:1808.01400, 2018.

[48] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
networks for semi-supervised learning,” in Thirty-Second AAAI conference
on artiﬁcial intelligence, 2018.

[76] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning
distributed representations of code,” Proceedings of the ACM on Program-
ming Languages, vol. 3, no. POPL, pp. 1–29, 2019.

14

JOURNAL OF LATEX CLASS FILES, VOL.0, NO.0, FEBRUARY 2022

[77] S. Liu, C. Gao, S. Chen, N. L. Yiu, and Y. Liu, “Atom: Commit message
generation based on abstract syntax tree and hybrid ranking,” IEEE
Transactions on Software Engineering, 2020.

[78] G. Frantzeskou, S. MacDonell, E. Stamatatos, and S. Gritzalis, “Examin-
ing the signiﬁcance of high-level programming features in source code
author classiﬁcation,” Journal of Systems and Software, vol. 81, no. 3, pp.
447–460, 2008.

[79] M. Linares-V´asquez, C. McMillan, D. Poshyvanyk, and M. Grechanik,
“On using machine learning to automatically classify software applications
into domain categories,” Empirical Software Engineering, vol. 19, no. 3,
pp. 582–618, 2014.

[80] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Convolutional neural
networks over tree structures for programming language processing,”
arXiv preprint arXiv:1409.5718, 2014.

[81] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A novel
neural source code representation based on abstract syntax tree,” in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE), 2019, pp. 783–794.

[82] H. Wei and M. Li, “Supervised deep features for software functional clone
detection by exploiting lexical and syntactical information in source code.”
in IJCAI, 2017, pp. 3034–3040.

