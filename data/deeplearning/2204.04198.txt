Modern applications of machine learning in quantum sciences

Anna Dawid1,2(cid:63), Julian Arnold3†, Borja Requena2†, Alexander Gresch4†, Marcin Płodzie´n2,
Kaelan Donatella5, Kim A. Nicoli6,7, Paolo Stornati2, Rouven Koch8, Miriam Büttner9,
Robert Okuła10,11, Gorka Muñoz–Gil12, Rodrigo A. Vargas–Hernández13,14, Alba
Cervera-Lierta15, Juan Carrasquilla14, Vedran Dunjko16, Marylou Gabrié17, Patrick
Huembeli18,19, Evert van Nieuwenburg16,20, Filippo Vicentini18, Lei Wang21,22, Sebastian J.
Wetzel23, Giuseppe Carleo18, Eliška Greplová24, Roman Krems25, Florian Marquardt26,27,
Michał Tomza1, Maciej Lewenstein2,28 and Alexandre Dauphin2(cid:63)

1 Faculty of Physics, University of Warsaw, Poland
2 ICFO - Institut de Ciències Fotòniques, The Barcelona Institute of Science and Technology,
08860 Castelldefels (Barcelona), Spain
3 Department of Physics, University of Basel, Switzerland
4 Quantum Technology Research Group, Heinrich-Heine-Universität Düsseldorf, Germany
5 Université de Paris, CNRS, Laboratoire Matériaux et Phénomènes Quantiques, France
6 Machine Learning Group, Technische Universität Berlin, Germany
7 BIFOLD, Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany
8 Department of Applied Physics, Aalto University, Espoo, Finland
9 Institute of Physics, Albert-Ludwig University of Freiburg, Germany
10 International Centre for Theory of Quantum Technologies, University of Gda´nsk, Poland
11 Department of Algorithms and System Modeling, Faculty of Electronics, Faculty of Electronics,
Telecommunications and Informatics, Gda´nsk University of Technology, Poland
12 Institute for Theoretical Physics, University of Innsbruck, Austria
13 Department of Chemistry, University of Toronto, Canada
14 Vector Institute for Artiﬁcial Intelligence, MaRS Centre, Toronto, Canada
15 Barcelona Supercomputing Center, Spain
16 LIACS, Leiden University, The Netherlands
17 CMAP, École Polytechnique, France
18 Institute of Physics, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland
19 Menten AI, Inc., Palo Alto, California, United States of America
20 Niels Bohr Institute, Copenhagen, Denmark
21 Beijing National Lab for Condensed Matter Physics
and Institute of Physics, Chinese Academy of Sciences, Beijing, China
22 Songshan Lake Materials Laboratory, Dongguan, China
23 Perimeter Institute for Theoretical Physics, Waterloo, Canada
24 Kavli Institute of Nanoscience, Delft University of Technology, NL-2600 GA Delft, The Netherlands
25 Department of Chemistry, University of British Columbia, Vancouver, Canada
26 Max Planck Institute for the Science of Light, Erlangen, Germany
27 Department of Physics, Friedrich-Alexander Universität Erlangen-Nürnberg, Germany
28 ICREA, Pg. Lluís Companys 23, 08010 Barcelona, Spain
† These authors contributed equally.
(cid:63) Anna.Dawid@fuw.edu.pl, Alexandre.Dauphin@icfo.eu

Abstract

June 23, 2022

In these Lecture Notes, we provide a comprehensive introduction to the most recent ad-
vances in the application of machine learning methods in quantum sciences. We cover
the use of deep learning and kernel methods in supervised, unsupervised, and reinforce-
ment learning algorithms for phase classiﬁcation, representation of many-body quantum
states, quantum feedback control, and quantum circuits optimization. Moreover, we in-
troduce and discuss more specialized topics such as differentiable programming, gener-
ative models, statistical approach to machine learning, and quantum machine learning.

1

In memory of Peter Wittek

2

Contents

1 Introduction

1.1 How to make computers learn?
1.2 Historical view on learning machines
1.3 Learning machines viewed by a statistical physics
1.4 Examples of tasks
1.5 Types of learning
1.6 What are these Lecture Notes about

2 Basics of machine learning

2.1 Learning as an optimization problem
2.2 Generalization and regularization
2.3 Probabilistic view on machine learning
2.4 Machine learning models

2.4.1 Linear (ridge) regression
2.4.2 Logistic regression
2.4.3 Support vector machines
2.4.4 Neural networks
2.4.5 Autoencoders
2.4.6 Autoregressive neural networks

2.5 Backpropagation

3 Phase classiﬁcation

3.1 Prototypical physical systems for the study of phases of matter

3.1.1 Ising model
3.1.2 Ising gauge theory

3.2 Unsupervised phase classiﬁcation without neural networks

3.2.1 Principal component analysis
3.2.2 t-Distributed stochastic neighbor embedding
3.3 Supervised phase classiﬁcation with neural networks
3.4 Unsupervised phase classiﬁcation with neural networks

3.4.1 Learning with autoencoders
3.4.2 Learning by confusion
3.4.3 Prediction-based method

3.5 Interpretability of machine learning models

3.5.1 Difﬁculty of interpreting parameters of a model
3.5.2 Interpretability via bottlenecks
3.5.3 Hessian-based interpretability

3.6 Outlook and open problems

4 Gaussian processes and other kernel methods

4.1 The kernel trick

4.1.1 Intuition behind the kernel trick
4.1.2 The function space as a Hilbert space
4.1.3 Reproducing kernel Hilbert spaces
4.1.4 The representer theorem
4.1.5 Consequences of the kernel trick

4.2 Kernel methods

4.2.1 Kernel ridge regression
4.2.2 Support vector machines

3

CONTENTS

7
7
8
10
12
13
15

20
20
24
27
30
31
34
35
37
39
40
42

48
48
48
50
51
52
54
56
57
57
59
60
62
63
64
68
70

71
71
72
73
74
77
77
78
79
80

CONTENTS

4.2.3 Gaussian processes
4.2.4 Training a Gaussian process

4.3 Bayesian optimization
4.4 Choosing the right model

4.4.1 Bayesian information criterion
4.4.2 Kernel search

4.5 Bayesian optimization and Gaussian processes for quantum sciences

4.5.1 Inverse problems
4.5.2 Improving quantum dynamics, physical models, and experiments
4.5.3 Extrapolation problems

4.6 Outlook and open problems

5 Neural-network quantum states

5.1 Variational methods

5.1.1 Variational states with exact expectation values
5.1.2 Variational states with approximate expectation values

5.2 Representing the wave function

5.2.1 Restricted Boltzmann machines
5.2.2 Autoregressive and recurrent neural networks
5.2.3 Capacity and entanglement
5.2.4 Implementing symmetries
5.2.5 Limitations

5.3 Applications

5.3.1 Finding the ground state
5.3.2 Real-time evolution
5.3.3 Imaginary-time evolution
5.3.4 Fermionic systems
5.3.5 Classical simulation of quantum circuits
5.3.6 Open quantum systems
5.3.7 Quantum tomography

5.4 Outlook and open problems

6 Reinforcement learning

6.1 Foundations of reinforcement learning

6.1.1 Delayed rewards
6.1.2 Exploration and exploitation
6.1.3 Markov decision process
6.1.4 Model-free vs. model-based reinforcement learning
6.1.5 Value functions and Bellman equations

6.2 Value-based methods

6.2.1 Q-learning
6.2.2 Double Q-learning
6.2.3 Implementing Q-learning with a neural network

6.3 Policy gradient methods
6.3.1 REINFORCE
6.3.2 Implementing REINFORCE with a neural network

6.4 Actor-critic methods
6.5 Projective simulation
6.6 Examples and applications
6.6.1 Toy examples
6.6.2 Go and Atari games

4

83
87
88
91
92
93
94
94
97
98
99

101
102
102
103
106
107
108
110
113
114
114
114
116
118
119
121
122
125
128

130
130
132
133
134
135
136
138
139
140
141
142
143
146
147
149
152
152
154

6.6.3 Quantum feedback control
6.6.4 Quantum circuit optimization
6.6.5 Quantum error correction
6.6.6 Quantum experiment design
6.6.7 Building optimal relaxations

6.7 Outlook and open problems

7 Deep learning for quantum sciences – selected topics

7.1 Differentiable programming

7.1.1 Automatic differentiation
7.1.2 Application to quantum physics problems

7.2 Generative models in many-body physics

7.2.1 Deep generative models
7.2.2 Applications and examples
7.3 Machine learning for experimental data

7.3.1 Automation of experimental setups
7.3.2 Machine-learning analysis of time-of-ﬂight images
7.3.3 Hamiltonian learning
7.3.4 Automated design of experiments

CONTENTS

155
156
158
160
162
163

165
165
168
173
174
177
178
183
184
189
191
194

8 Physics for deep learning

8.2 Quantum machine learning

198
198
8.1 Statistical physics for machine learning
8.1.1 Capacity of the perceptron
199
8.1.2 The teacher-student paradigm: a toy model to study the generalization 204
210
8.1.3 Models of data structure
212
8.1.4 Dynamics of learning
214
214
215
216
217
219
220
223
224

8.2.1 Gate-based quantum computing
8.2.2 What is quantum machine learning?
8.2.3 Ideal quantum computers and quantum machine learning
8.2.4 Quantum computing in the noisy intermediate-scale quantum era
8.2.5 Support vector machines with quantum kernels
8.2.6 Variational approaches
8.2.7 Parametrized quantum circuits for quantum machine learning
8.2.8 Current experimental and theoretical limitations

9 Conclusion and outlook

Acknowledgements

A Mathematical details on principal component analysis

B Derivation of the kernel trick

227

230

231

233

C Choosing the kernel matrix as the covariance matrix for a Gaussian process

235

References

List of ﬁgures and algorithms

Nomenclature

5

237

276

279

List of acronyms

Index

CONTENTS

280

282

6

Introduction

1 Introduction

Making intelligent machines, i.e., machines capable of learning and utilizing gathered knowl-
edge in thinking and reasoning, is a long-lived dream of human civilization. The more we know
about the human brain, intelligence, and psychology, the more challenging it seems. However,
despite the many obstacles and challenges in creating proper artiﬁcial intelligence (AI), the
joint effort of researchers working in natural, cognitive, mathematical, and computer sciences
has produced impressive machinery that is already revolutionizing our everyday life, industry,
and science.

1.1 How to make computers learn?

The ultimate goal of AI is to endow machines with the ability to conceptualize and create
abstractions. Both of these features are mechanisms that underlie learning representations of
knowledge and reasoning based on experience in humans. We have multiple ways to represent
ideas. For instance, we can encode a piece of music in digital format on a computer, in analog
format on a vinyl disc, or we can write it down in a music score. While the representations
have entirely different natures, the piece of music is the same. Hence, the properties of abstract
ideas do not depend on the data source.

Furthermore, conceptualization and abstraction bring the possibility to consider various
levels of details within a particular representation or the capability to switch from one level
to another while preserving the relevant information [1–5]. Our brain excels at extracting
abstract ideas from different knowledge representations. In our daily lives, we constantly pro-
cess information from multiple sources that represent the same concept in completely different
ways. For example, we can identify the concept of a dog by seeing one, hearing or smelling it,
reading the word “dog”, painting a snout on someone’s face, or even casting shadows with our
hands that resemble the shade of a dog. This level of abstraction and conceptualization allows
us to reason, connecting high-level ideas. All the aforementioned properties of our brain form
what we call intelligence. Conferring these properties to a computer would result in a general
problem-solving machine.

Today, we are at a point in our technological advances at which the human brain and
computers have a disjoint set of tasks in which they naturally excel.1 Some tasks are easy
for computers but difﬁcult for humans. These are problems that can be described by a list of
formal, mathematical rules. Therefore, computers excel at solving logic, algebra, geometry,
and optimization problems, which we can tackle with hard-coded solutions or knowledge-
based AI. However, we would like to tackle problems that are not easy to present in a formal
mathematical way, such as face recognition, or whose exact mathematical formulation is not
yet known, as detecting new quantum phases.

A particularly exciting direction is the development of algorithms that are not explicitly pro-
grammed. The main principle is to allow computers to learn from experience (or data). The
shift toward this data-driven paradigm led to the birth of machine learning (ML), schematically
depicted in Fig. 1.1. This ﬁeld leverages fundamental concepts of applied statistics, emphasiz-
ing the use of computers to estimate complicated functions and with a decreased emphasis on
proving conﬁdence intervals around them [7]. Such trend has increased even more with the
arising of deep learning (DL), where enormous and heavily parametrized hierarchical models
are used to deal with complex patterns from real-world data and do this with unprecedented

1This observation was ﬁrst made in the 1980s and it is called Moravec’s paradox. As Moravec wrote in 1988 [6],
“it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing check-
ers, and difﬁcult or impossible to give them the skills of a one-year-old when it comes to perception and mobility”.

7

Introduction

Figure 1.1: Schematic representation of the difference between the traditional pro-
gramming, based on the algorithmic approach, and the experience-based/data-
driven approach, which is the backbone of the ML paradigm. The ML paradigm
is the ﬁrst step toward learning abstractions by computers through the extraction of
common features from data.

accuracy. Interestingly, many DL architectures are designed to mimic some of the properties
of the human thinking process, such as the understanding of correlations in visual patterns
or the recurrence in sound signals. We present a schematic representation of the relationship
between these three ﬁelds (AI, ML, and DL) in Fig. 1.2.

To make a computer learn, we need three main ingredients:

1. a task to solve (section 1.4),

2. data that can be considered as an equivalent of experience. The latter can be
provided in the form of, e.g., an interacting environment, and allows for solving
the task (section 1.5),

3. a model that learns how to solve the task (section 2.4).

To check whether a computer successfully learns how to solve a task, we need to de-
ﬁne a performance measure, which can be as simple as the comparison between the
model’s prediction and the expected answer. In these terms, the learning process can
be described as the iterative minimization of the model’s error or maximization of the
model’s performance over the given task and data.

1.2 Historical view on learning machines

The foundations of the theory of learning were established already in the 1940s. Its develop-
ment has followed two parallel paths: a knowledge-based approach, which dominated the AI
research ﬁeld for decades, and a data-based one, which is currently on the rise. Throughout
the years, ML has gone under various names (like cybernetics or connectionism) and experi-
enced a few cycles of intense popularity,2 followed by criticism and disappointment, followed
by funding cuts, followed by renewed interest years or decades later [7]. To give the reader
some insight into the giants on whose arms we stand, we brieﬂy present milestones in the
development of ML, following Refs. [7, 9, 10]:

2Some argue that the “AI winter” is upon us unless we rethink AI or combine it with knowledge-based ap-
proaches [8]. It is also important to remember that such hype cycles are frequent with emerging new technologies.

8

TRADITIONAL PROGRAMMINGINPUTPROGRAMRESULTSCOMPUTATIONMACHINE LEARNINGINPUTDESIREDRESULTPROGRAMCOMPUTATIONIntroduction

Figure 1.2: Sketch of the relation between AI, ML, and DL with examples from
each ﬁeld including support vector machines (SVMs), principal component analy-
siss (PCAs), neural networks (NNs), and convolutional neural networks (CNNs).

• 1943 – Walter Pitts and Warren McCulloch create a computer model based on the neural
networks of the human brain called threshold logic. Their ﬁeld of expertise is called
cybernetics.

• 1949 – Donald Hebb hypothesizes how learning in biological systems works and for-
mulates Hebbian learning. For example, if certain neurons “ﬁre together, they wire to-
gether”.

• 1957 – Frank Rosenblatt introduces a Rosenblatt perceptron modeling a single neuron.
A perceptron is also called “an artiﬁcial neuron” and after modiﬁcations in 1969 by
Marvin Minsky and Seymour Papert to this day remains widely used as a building block
of artiﬁcial neural networks (ANNs).

• 1962 – David Hubel and Torsten Wiesel present, for the ﬁrst time, response properties

of single biological neurons recorded with a microelectrode.

• 1969 – Marvin Minsky and Seymour Papert point out the computational limitations and
disadvantages of linear models, including a single artiﬁcial neuron, contributing to the
ﬁrst “AI winter”.

• 1986 – David Rumelhart, Geoffrey Hinton, and Ronald Williams use backpropagation to
train an NN with one or two hidden layers which, next to the revival of Hebb’s ideas,
causes renewed interest in the ﬁeld that at this time is called connectionism.
In the
same year, David Rummelhart, James McClelland, et al. publish a widely discussed two-
volume book “Parallel Distributed Processing” discussing known and collecting original
contributions from the ﬁeld including backpropagation and Boltzmann machines.

• the mid-1990s – second AI winter whose appearance is ascribed [7] to exceedingly am-
bitious claims of the community, which led to the disappointment of investors, and the
simultaneous progress of kernel methods, that require less computational resources.

9

MACHINE       LEARNINGARTIFICIAL         INTELLIGENCEDEEP     LEARNINGNNsCNNsPCASVMknowledge-basedapproachesIntroduction

Interestingly, we can see how closely the development of AI was intertwined with neu-
roscience. This makes sense, as the human brain provides proof by example that intelligent
behavior is possible. A natural approach to AI would be to try to reverse engineer the brain
to reproduce its functionality. However, while the perceptron was inspired by biological neu-
rons and some ML models are loosely inspired by neurological discoveries, there is nowadays
a consensus that models should not be designed to be realistic simulators of biological func-
tions [7].3 Instead, scientists attempt to solve the mysteries of the human brain using ML.

Since 2006, DL has been thriving again thanks to a breakthrough in the efﬁcient training
of deep NNs [11] via backpropagation, followed by multiple analyses conﬁrming the impor-
tance of its depth. At the same time, there has been a rapid improvement in computational
power over the last decades which allowed the exploration of larger ML models. Here, the
development of graphics processing units (GPUs) has played a particularly important role:
highly parallelizable algorithms, such as NNs which are based on matrix and vector opera-
tions, can proﬁt immensely from the parallel architecture of GPUs allowing them to process
large amounts of data more efﬁciently than central processing units (CPUs). Additionally, we
have started producing and storing large amounts of easily accessible electronic data all over
the world [12–14], enabling data-driven programming approaches. Since then, the progress
in the ﬁeld has enabled realizations of concepts known, so far, only in science-ﬁction literature,
such as self-driving cars or robots mimicking human emotions on their artiﬁcial faces. DL has
dominated the ﬁeld of computer vision for years, and it has found great success in time-series
analysis, with applications like stock-market and weather forecasting [15]. Another fruitful
direction is natural language processing, where sequence-to-sequence models have achieved
great feats, even combining text with images [16,17]. Recently, DL-based algorithms obtained
superhuman performance in video games [18,19] and complex board games, such as Go [20].

Overall, the continuous progress in the ﬁeld of ML is supported by the steady increase of
computational power as well as its easy applicability to real-world problems. The increasing
amount of data produced by our society and the monetary beneﬁt of its processing have made
that the largest technological companies focus enormous economic efforts in the development
of ML models. It is hence not a coincidence that the most important research groups in the ﬁeld
are associated with such companies. Importantly, one should understand the extend to which
the trends of the ﬁeld are dictated by the thirst for scientiﬁc discovery or by the particular
needs of one or another technological giant. In summary, ML has become a day-by-day tool,
acting in the shades of most of the technological tools we use nowadays, with the potential of
solving some of the most important problems of the modern world and thereby contributing
to improving the quality of life of people around the globe.

1.3 Learning machines viewed by a statistical physics

It is also worth noticing that the above sketched developments of AI, data science, cognitive sci-
ence, and neuroscience, related to ML and NN, were also intertwined with the development of
the statistical physics of spin glasses and NN. A wonderful retrospective of these developments
can be found in the lecture of the late Naftali Tishby, “Statistical physics and ML: A 30-year
perspective”. Therefore, here we present a similar list of historical milestones as in section 1.2,
but focused on statistical physics achievements:

• 1975 – Philipp W. Anderson and Samuel F. Edwards formulate the Edwards-Anderson

spin glass model with short-range random interactions between Ising spins.

3Interestingly, we know that actual biological neurons compute very different functions than the perceptrons
constituting our modern NNs, but greater realism has not yet led to any improvement in model performance [7].

10

Introduction

• 1975 – a little later David Sherrington and Scott Kirpatrick formulate the Sherrington-
Kirkpatrick spin glass model with inﬁnite-range interactions, for which the mean-ﬁeld
solution should be exact. They propose to solve it using the replica trick, but this ap-
proximate solution turns out to be clearly incorrect at low temperatures.

• 1979 – Giorgio Parisi proposes an ingenious replica symmetry-breaking solution of the

Sherrington-Kirkpatrick model.

• 1982 – John J. Hopﬁeld publishes his seminal paper on attractor NNs, where by assuming
the symmetry of interneuron coupling, he relates the model to a disordered Ising model
of N spins, very much analogous to spin glasses. The maximal storage capacity is found
to be 0.14 N .

• 1985 – Daniel Amit, Hannoch Gutfreund, and Haim Sompolinski formulate the statisti-
cal physics of the Hopﬁeld model and relate limited storage capacity to the spin glass
transition.

• 1987 – Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro publish the book “Spin
glass theory And beyond: An introduction to the replica method and its applications”.
Interestingly, it is one of the ﬁrst works bringing together statistical physics and NNs but
also putting them in a more general context of complex systems like optimization and
protein folding.

• 1988 – Elisabeth Gardner formulates the, so-called, Gardner’s program to ML, where
learning abilities are related to the relative volume in the space of those NNs that realize
learning tasks and teacher-student scenarios (see section 8.1.1).

• 1989 – Daniel Amit publishes the book “Modeling brain function: The world of attractor
NNs” where he brings closer neurophysiology and artiﬁcial NNs by introducing dynam-
ical patterns whose temporal sequence encodes the information.

• 1990 – Géza Györgyi shows that sharp phase transitions from bad to good generalization

can occur in learning using the Gardner’s program on the perceptron.

• 1995 – David Saad, Sara Solla, Michael Biehl, and Holm Schwarze adapt Gardner’s idea
to study the dynamics of gradient descent in perceptrons and simple two-layer NNs called
committee machines.

• late 2010s – The statistical mechanics predictions for the perceptron and the commit-
tee machine start being made mathematically rigorous by Nicolas Macris, Jean Barbier,
Lenka Zdeborová, and Florent Krzakala.

• 2010s-today – With the explosion of DLs, interest in the statistical mechanics approach to
learning is rekindled. Analyses are developed for increasingly complex models starting
to bridge the gap from perceptrons to deep NNs.

• 2021 – Giorgio Paris receives the Nobel Prize in Physics “for the discovery of the interplay

of disorder and ﬂuctuations in physical systems from atomic to planetary scales.”

We discuss the intersection of statistical physics and ML in more detail in section 8.1.

11

Introduction

1.4 Examples of tasks

As stated above, the ﬁrst ingredient needed for a computer to learn is the notion of a learn-
ing task. The archetypical ML task is the study of a response variable, y(x), inﬂuenced by
an explanatory variable x.
In principle, there is no restriction whether y or x or both are
continuous, discrete or even categorical.4 Throughout the Lecture Notes, we restrict both
variables, possibly encoded accordingly, to be of quantitative nature. That is, we can treat the
variables straightforwardly from a numerical perspective, and we can adjust them easily to ﬁt
our needs.

Regression. We start by considering regression tasks.
In this setting, we typically assume
an immediate relationship between the two variables x and y, which is often of deterministic
nature. More precisely, we seek to express the variable y, a.k.a. the output or target, in terms
of the variable x, a.k.a. the input. In general, both variables can be multi-dimensional, as
indicated by our notation. The objective of regression is to ﬁnd the function f that yields the
mapping y = f (x) for all possible tuples of (x, y). Of course, from a practical point of view,
we can neither optimize over the set of all possible functions nor over the full domain of x.
Instead, we resort to a ﬁnite data set for which we opt to ﬁnd a model that maps every input x
to its corresponding target y. Usually, the model is predeﬁned up to some parameters,5 which
are tuned to ﬁt the data set. The most simple model assumes a linear relationship between the
input and the output. We give more details of this model archetype in section 2.4.1. From here,
there is a multitude of ways to extend the model by incorporating nonlinear dependencies
on both the model parameters and the input x. We ﬁnd interesting regression problems in
a large range of study ﬁelds, such as sociology (e.g., annual salary as a function of years of
work experience), psychology (e.g., perceived happiness relative to wealth), ﬁnance (e.g.,
housing market prices depending on socio-economic factors) or, of course, (quantum) physics
and chemistry. We cover some examples in these Lecture Notes, as for instance the prediction
of potential energy surfaces (PESs) in quantum chemistry in section 4.5, or the estimation of
the Hamiltonian’s parameters given measurement data in section 7.3.

Classiﬁcation. Another large class of tasks is classiﬁcation. In this case, our goal is to use
an algorithm to assign discrete class labels to examples.
In contrast to regression, we are
optimizing a model to ﬁnd a mapping from an input vector x to a target y, which encodes
a representation of the different possible classes. The simplest example of this kind of task is
the binary classiﬁcation, in which an algorithm has to distinguish between two classes, e.g.,
true or false. When the task involves more than two classes, we speak of multi-class classiﬁ-
cation. A canonical example for such a task is the classiﬁcation of the images of handwritten
digits contained in the famous MNIST [21] data set (named after Modiﬁed National Institute
of Standards and Technology) over ten classes, one for each number from zero to nine. Other
famous ML classiﬁcation data sets are Iris [22], CIFAR-10 and 100 [23], and ImageNet [24].6
A popular example from physics is the classiﬁcation of different classical and quantum phases
of matter, described in chapter 3. Another set of examples is provided by the classiﬁcation
subroutines in the automation of (quantum) experiments highlighted in section 7.3.

Both regression and classiﬁcation tasks require a training data set consisting of examples
of inputs x together with their corresponding labels y. Nonetheless, there are also tasks that

4When the inputs are, for example, words in a sentence as they are in the ﬁeld of natural language processing,
we can still process them by representing words by a suitable encoding which can be either continuous or discrete.

5There are also non-parametric approaches, e.g., see section 7.2.1 and section 4.4.2.
6The Iris database contains 150 data points with four features of three species of iris. The CIFAR-10 data set
consists of 60 000 32x32 color images in 10 classes and was named after the Canadian Institute for Advanced
Research. Finally, the ImageNet is a gigantic project with over 10 million labeled images whose the most popular
subset spans 1000 object classes.

12

Introduction

do not require explicit labels. An example of such is density estimation, where one aims at
inferring the probability density function of the data set. This is directly related to the ﬁeld of
generative problems, where the goal is to generate new data instances that resemble some given
input data. The distinction between the two ﬁelds is that the latter does not require explicit
knowledge or reconstruction of the underlying data distribution to sample new instances. We
present more details on density estimation in section 7.2.

In all the previous cases, we try to infer properties of a given pre-deﬁned data set. Nev-
ertheless, there are other tasks that involve starting from scratch and building a data set on
the ﬂy, from which we can then learn. A paradigmatic example of such a task is learning how
to play a game. In this case, we start tabula rasa and progressively build a data set with the
experience gathered as we play the game. From this data (or during its retrieval) our goal is
to learn a function that chooses the best possible action or move according to the current state
of the game. In this example, we can periodically alternate between collecting experience and
learning, or we can do both at the same time.

This list of tasks is, of course, not exhaustive. Other examples which do not directly fall
into the previous categories include text translation, imputation of missing values, anomaly
detection, and data denoising, to name a few.

1.5 Types of learning

The second learning ingredient is the data, whose accessibility also often determines the type
of learning we have to consider. It is clear, of course, that the notions of task, as presented in
the previous section, and data are intertwined: certain tasks can only be solved if sufﬁcient
data is available and, in turn, a richer data set allows to transition from one task to another
with seemingly low effort. Although the term data is often used for a variety of concepts
accross many ﬁelds, there is a precise deﬁnition of it in the ML community. We usually refer to
the data in terms of a data set D, containing a ﬁnite amount of data instances often called data
} or may be accompanied by predeﬁned
points xi, which may be presented as is, i.e., D = {xi
labels or targets yi, i.e. D = {(xi, yi
)}. To shorten the notation, we also represent the input
data points {xi

} by a matrix X, that can either be stacked row- or column-wise.

While the notation is clear-cut, there is much less convention and an even lesser under-
standing of how the data is represented. This is because, on one hand, the data can be arbitrarily
preprocessed (for example, the data mean is often substracted prior to any further analysis),
which already provides some degree of freedom. On the other hand, even choosing the right
descriptors to characterize our object of interest is challenging: too few might not capture all
relevant aspects of the object wheras too many can lead to spurious correlations that can in-
terfere with the conclusions that we want to gain out of the data. We refer to every element in
each data point xi as a feature. As stated before, a central problem in ML relates to the correct
representation of the data and its features. This is the core of the ﬁeld of representation learn-
ing on which we only touch, e.g., by means of autoencoders (AEs) and principal component
analysis (PCA) in chapter 3 and section 7.2.

Lastly, we emphasize that data can, loosely speaking, be identiﬁed with experience: data
can be produced as the result of a repeated interaction with an entity (such as an experiment
or a simulation) that then leaves us with a certain amount of experience about its underlying
mechanism. In some cases, this experience may be used to further interact with such entity
and learn from it. To this end, we set up a model. In summary, the type of data we have access
to effectively deﬁnes the types of learning our model can be faced with. These are usually split
in three: supervised, unsupervised and reinforcement learning.

13

Introduction

Supervised learning. Supervised learning can be seen as a generalized notion of regression
and classiﬁcation, introduced in section 1.4, and describes ML algorithms that learn from
)}. There exists various approaches to supervised learning,
labeled data, i.e., D = {(xi, yi
ranging from statistical methods to classical ML and DL, both introduced in section 2.4. The
concept of supervised learning appears repeatedly in these Lecture Notes and forms the basis
of many chapters, including phase classiﬁcation (chapter 3), Gaussian processes (chapter 4),
as well as in the selected topics of DL for quantum sciences (chapter 8). Importantly, some of
the latter are specially suited to deal with experimental data, as for instance in the efﬁcient
read-out of quantum dots or the identiﬁcation of Hamiltonian parameters describing quantum
experimental setups.
In most of these examples (but there are notable exceptions), large
amounts of data are required for the training process. On top of data, as stated, supervised
learning requires correctly labeled data. This is usually considered one of its most prominent
downsides, as perfectly matching labels are not always accessible or have to be added manually
by humans.

}).

Unsupervised learning. Supervised learning is not always the best option: the scarcity of
labeled data is one example in which a classical input-output design might fail. Instead, we
in terms of labels, is given (i.e.
often have access to data where no prior information, e.g.
D = {xi
In this case, we can employ unsupervised learning. Unsupervised learning can
either be used for preliminary preprocessing steps, such as dimensionality reduction, or for
representation learning, such as in clustering.
In the opposite way, the dimensionality can
also be increased by adding features via generative models. In these Lecture Notes, we discuss
the application of unsupervised learning for phase classiﬁcation in chapter 3 and density esti-
mation in section 7.2. This example is particularly interesting because it demonstrates how the
choice of unsupervised learning over supervised learning can aid in the automated discovery
of new physics when the interpretation of a process, e.g., the nature of two different phases
in a transition is unknown.

Reinforcement learning.
In contrast to the two previous types of learning, in reinforcement
learning (RL), we typically do not have a data set available at all. What we have instead
is an environment with which we have to interact to achieve a certain task. This interaction
is augmented with feedback, i.e., some extra information on whether the action has been
beneﬁcial or harmful in achieving the task at hand. The collection of visited environment
states, action taken and rewards or penalizations received take the role of a data set. Feedback
is very important in RL because we do not have a clear-cut route in achieving our task. In
fact, initially, we typically do not even know the necessary ingredients for achieving the task.
Often, we only know that we achieved a speciﬁc goal but not why we did it. The ﬁeld of RL is
precisely concerned with tackling the issue of how. In order to introduce it properly, we devote
to it chapter 6.

Other types of learning. While supervised, unsupervised, and reinforcement learning are the
most common learning schemes, there are ML approaches that go beyond this classiﬁcation.
An interesting example is active learning. This ﬁeld includes selection strategies that allow for
an iterative construction of a model’s training set in interaction with a human expert or envi-
ronment. The aim of active learning is to select the most informative examples and minimize
the cost of labeling [25, 26]. We touch on this topic by means of Bayesian optimization (BO)
in section 4.3 and local ensembles (LEs) in section 3.5.3. Another example of learning is the
so-called semi-supervised learning in which unlabeled data is explored to get better feature
representations and improve the models trained on the labeled data [27].

14

Introduction

Figure 1.3: The number of ML-based publications in physics, materials science, and
chemistry is growing exponentially. Adapted from Ref. [28].

1.6 What are these Lecture Notes about

We live in fascinating times where scientists start to incorporate AI algorithms for knowledge
discovery. The breakthroughs of this booming ﬁeld have led to a rapid increase in the conﬁ-
dence of the scientiﬁc community in these methods. This trend can be observed by tracking
the percentage of ML-based publications in physics, chemistry, and material science, shown
in Fig. 1.3. As the number of ML applications grows, keeping track of all advances becomes
challenging. Moreover, it is difﬁcult to ﬁnd reliable learning material of intermediate level
which allows to efﬁciently bridge the gap between the quickly developing ﬁeld of ML and
scientists interested in incorporating ML tools into their own research.

Therefore, the aim of these Lecture Notes is to give an educational and self-contained
overview of modern applications of ML in quantum sciences. We discuss in detail four ML
paradigms that have already been successfully explored in quantum physics and chemistry.
What we believe to be unique and very much needed before diving into the speciﬁcs of nu-
merous application examples is an informative introduction to each paradigm, which prepares
a quantum scientist for doing their own ML project. In particular, in chapter 3 we describe how
supervised and unsupervised learning can be utilized to classify phases of matter. In chapter 4
we introduce kernel methods with a special focus on Gaussian processes (GPs) and Bayesian
optimization (BO). Chapter 5 presents an overview of various representations of quantum
states based on neural networks (NNs). Finally, in chapter 6 we dive into the foundations of
reinforcement learning (RL) and how it can be applied to quantum experiments. To make the
Lecture Notes self-contained, we devote chapter 2 to the ML prerequisites that are necessary
to fully enjoy further, more advanced contents of this work.

Next to these well-recognized contributions of ML in quantum sciences, we observe an ex-
citing two-way interplay between the natural sciences and AI. In chapter 7 we present more
specialized examples on how ML-related methods revolutionize quantum science. In partic-
ular, we introduce the paradigm of differentiable programming (∂ P) and describe how it is
becoming an important numerical research tool. Moreover, we discuss how ML methods as-
sist researchers in tasks related to density estimation, as well as optimizations and speed-up
of scientiﬁc experiments. We also acknowledge a vibrant reverse inﬂuence on ML coming
from statistical physics (which we discuss in section 8.1) and ﬁnally, quantum computing. We
describe the promises of quantum machine learning (QML) in section 8.2. All in all, these Lec-
ture Notes discuss the fruitful interplay of AI and quantum sciences, presented schematically
in Fig. 1.4. Their content with references to relevant sections is illustrated in Fig. 1.5.

15

20002005201020152020Year0.00.51.01.52.02.5% of ML articles in the fieldmaterials sciencephysicschemistryIntroduction

Figure 1.4: Interplay of AI and quantum sciences, in particular quantum computing,
many-body physics, and quantum chemistry. Within these Lecture Notes we focus on
the inﬂuence of AI onto quantum sciences but also cover the reverse impact of the
statistical physics and quantum computing onto ML.

We encourage the reader to start with the ﬁrst two chapters, i.e., “Introduction” and “Basics
of machine learning”. Then, the reader is free to wander into any of independent chapters 3 to 6
covering four main paradigms, section 7.1 on differentiable programming, or section 8.1 dis-
cussing how statistical physics tackles puzzles of ML. Section 7.2 on generative models builds
upon chapter 5 on neural quantum states (NQSs), while section 7.3 on ML for experiments
requires knowledge of methods used for phase classiﬁcation presented in chapter 3. Finally,
section 8.2 discussing QML utilizes concepts introduced in chapters 4 to 6. The dependencies
between chapters are visualized as a tree in Fig. 1.6.

These Lecture Notes do not aim at providing an exhaustive list of ML applications in quan-
tum sciences and becoming a complete review of the ﬁeld. Such reviews already exist and
nicely summarize the latest achievements [29–31]. Instead, we aim at providing the reader
with enough knowledge, intuition, and tricks of the trade to start implementing ML methods
of choice in their own research. As such, we selected the ML applications presented in this
work that, we believe, are pedagogically appealing, while keeping a broad overview of the
ﬁeld. To this end, we focus on what a reader could do, and not only on what has been done.
To fulﬁll this ambition, we conclude each chapter with an outlook and open problems that we
recognize as important and promising.

These Lecture Notes are aimed at quantum scientists that want to familiarize themselves
with ML methods. Therefore, we assume a basic knowledge of linear algebra, probability
theory, and quantum information theory. We also expect familiarity with concepts such as
Lagrange multipliers, Hilbert space, and Monte Carlo methods. We also assume the reader is
familiar with quantum mechanics and has a basic grasp of the current challenges in quantum

16

QUANTUMCOMPUTINGARTIFICIALINTELLIGENCEMANY-BODY PHYSICS AND QUANTUM CHEMISTRY10Introduction

Figure 1.5: Content of these Lecture Notes. We cover (1) three main learning
schemes, i.e., supervised, unsupervised, and reinforcement learning (RL), (2) ex-
amples of ML tasks like classiﬁcation, regression, and density estimation, (3) various
applications in quantum chemistry and quantum many-body physics, (4) quantum
and classical ML architectures. Finally, we also dive into mechanics of ML.

17

ML INQUANTUMSCIENCESAPPLICATIONQUANTUM CHEMISTRYQUANTUMTOMOGRAPHYQUANTUMCOMPUTINGREPRESENTATIONOF QUANTUM STATESPHASECLASSIFICATIONOPTIMIZATIONAND SPEED-UPHAMILTONIANLEARNINGQUANTUMEXPERIMENTSch. 3ch. 5sec. 6.6.4 - 6.6.5sec. 5.3.7sec. 7.3.3sec. 6.6.3, 7.3.1-2sec. 4.5ARCHITECTUREFORM OFTRAININGDATAREINFORCEMENTLEARNINGSUPERVISEDLEARNINGUNSUPERVISEDLEARNINGsec. 3.2,3.4, 7.2sec. 3.3,4.5, 7.3ch. 6INTERPRETABILITYAUTOMATICDIFFERENTIATIONSTATISTICALPHYSICSFOR MLsec. 3.5sec. 2.5, 7.1sec. 8.1TASKMECHANICS OF MLCLASSIFICATIONREGRESSIONDENSITYESTIMATIONGENERATIVEPROBLEMSch. 3,ch. 4,sec. 7.2sec. 7.2sec. 7.3.2sec. 4.2.2BIAS-VARIANCE TRADE-OFFsec. 2.2CNNRNNKERNELMETHODSAECLASSICALch. 4sec. 2.4.5,3.4.1sec. 2.4.6,5.2.2sec. 2.4.4,3.3, 3.5.2NFsec. 7.2.1QUANTUMsec. 8.2Introduction

Figure 1.6: Dependencies between chapters.

sciences.

Finally, note that the idea of creating these Lecture Notes was born out of the Summer
School: Machine Learning in Quantum Physics and Chemistry which took place between Aug,
23 - Sept, 03, 2021, in Warsaw, Poland. As a result, the scientiﬁc content of this work is
inspired by the topics covered by lecturers and invited speakers of the school. We also invite
the reader to take a look at the school tutorials in Ref. [32] and to reuse ﬁgures prepared for
these Lecture Notes, which are available in Ref. [33].

Further reading

• Carleo, G. et al. (2019). Machine learning and the physical sciences. Rev. Mod. Phys.
91, 045002. This detailed review summarizes the development of ML in physics and
achievements till 2019 [30].

18

1 Introduction2 Basics of machine learning3 Phase classiﬁcation6 Reinforcementlearning4 Gaussianprocesses and other kernelmethods5 Neural-network quantum states8.2 Quantummachine learning7.2 Generativemodels in many-bodyphysics7.3 Machine learningfor experiments8.1 Statistical physics for machine learning7.1  DifferentiableprogrammingIntroduction

• Carrasquilla, J. (2020). Machine learning for quantum matter. Adv. Phys. X 5, 1. The
concise review focused on phase classiﬁcation and quantum state representation [31].

• Chollet, F. (2019). On the measure of intelligence. The review about different measures

used to quantify intelligence providing the perspective on AI development [34].

• Krenn, M. et al. (2022) On scientiﬁc understanding with artiﬁcial intelligence. arXiv:
2204.01467 [35]. A beautiful paper discussing ways in which AI could contribute to
the scientiﬁc discovery.
It touches upon the philosophy of understanding as well as
draws conclusions from dozens of anecdotes from scientists on their computer-guided
discoveries.

• Recordings of lectures of the Summer School: Machine Learning in Quantum Physics
and Chemistry which took place between Aug, 23 - Sept, 03, 2021, in Warsaw, Poland.

• Jupyter notebooks prepared as tutorials for the Summer School: Machine Learning in

Quantum Physics and Chemistry [32].

19

Basics of machine learning

2 Basics of machine learning

In this section, we describe basic machine learning (ML) concepts connected to optimization
and generalization. Moreover, we present a probabilistic view on ML that enables us to deal
with the uncertainty in the predictions we make. Finally, we discuss various ML models. To-
gether, these topics form the ML preliminaries needed for understanding the contents of the
next chapters.

2.1 Learning as an optimization problem

We have already discussed that ML can solve various tasks (e.g., classiﬁcation or regression)
and that there are different ways for the machine to access the data. The ﬁnal ingredient
is a model that learns how to solve the given task with the data at hand.
In general, it is
a function of the input data, f (x), whose output is interpreted as a prediction made for the
input data. The form of the output depends on the task. It can be, e.g., a class from a discrete
set of possible classes in the classiﬁcation task or a tensor from a continuous target distribution
in the regression task. Finding the function that provides the best mapping between data and
the desired outcome for a speciﬁc task is at the heart of ML. We start with declaring a certain
parametrization of a model (function), e.g., f (x) = w(cid:252)x + b with θ ⊃ {w, b}. Then, all
possible parametrizations of this function form the set of functions, i.e., the hypothesis class.
Section 2.4 presents speciﬁc examples of the hypothesis classes (or spaces), but for now we
focus on the learning process itself.

The mentioned learning schemes, i.e., supervised, unsupervised, or reinforcement learn-
ing, have the same underlying process of learning: ﬁnding an optimal model ˆf ≡ fθ∗ with
optimal parameters θ∗ in the hypothesis space, which minimizes the target loss function or
maximizes a model performance. For the remainder of this section, for clarity, we focus on
minimizing the loss function, L, which intuitively plays a role of a penalty for errors of a model.

Machines “learn” by minimizing the loss function of the training data, i.e., all the data
accessible to the ML model during the learning process. The minimization is done by
tuning the parameters of the model. The loss function formula varies between tasks
and there is a certain freedom of how it can be chosen. In general, the loss function
compares model predictions or a developed solution against the reality or expectations.
Therefore, learning becomes an optimization problem.

In these Lecture Notes, we use the terms of loss, error, and cost functions1 interchangeably
following Ref. [7]. Popular examples of loss functions include the mean-squared error (MSE)
and the cross-entropy (CE), used for supervised regression and classiﬁcation2 problems. The
output of the loss function depends on the model (which enters into formulas via predictions)
and the data set. They are also normalized by the number of data points n to compare their

1The literature also uses the terms of criterion or cost, error, or objective functions. Their deﬁnitions are not
very strict. Following [7]: “The function we want to minimize or maximize is called the objective function, or
criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. In this
book, we use these terms interchangeably, though some ML publications assign special meaning to some of these
term”. For example, loss function may be deﬁned for a single data point, the cost or error function may be a sum
of loss functions, so check the deﬁnitions used in each paper.

2For classiﬁcation, a more intuitive measure of the performance could be, e.g., accuracy, which is the ratio
between the number of correctly classiﬁed examples and the data set size. Note, however, that gradient-based
optimization requires measures of performance that are smooth and differentiable. These conditions distinguish
loss functions from evaluation metrics such as accuracy, recall, precision, etc.

20

Basics of machine learning

Figure 2.1: Examples of loss functions. (a) Plot of the binary cross-entropy (CE)
= 0 (blue) or 1 (purple).
for a single data point, xi, when the ground-truth label yi
(b) Intuition behind loss functions used in regression problems. Dashed lines are
differences between the labels, yi, and values predicted by a model, f (xi
). (c) Plots
of the MSE (purple) and MAE (blue) for a single data point, xi, when the ground-
truth label yi

= 0.

values between problems with different data set sizes. The MSE is a popular loss function
inherited from linear regression problems and is deﬁned as

LMSE

= 1
n

n
(cid:88)

( yi

i=1

− f (xi

))2 .

(2.1)

It has an information-theoretic justiﬁcation discussed in more detail in section 2.4.1 and sec-
tion 2.2. In the former, we also introduce the mean absolute error (MAE) as another viable
loss function, which is more sensitive to small errors than the MSE as shown in Fig. 2.1(b)-(c).
Cross-entropy (CE) is also a concept drawn from information theory and has connections to
probability theory (see section 2.3). In the binary classiﬁcation task, we can use the binary CE
(BCE) known also as the log loss (Eq. (2.2) and panel (a) in Fig. 2.1), while for the multi-class
classiﬁcation, we use the categorical CE (CCE). They are deﬁned as

LBCE

LCCE

= − 1
n

= − 1
n

n
(cid:88)

i=1
n
(cid:88)

yi

· log ( f (xi

)) + (1 − yi

) · log (1 − f (xi

)) ,

K
(cid:88)

yi,c

· log ( f (xi

)) ,

i=1

c=1

(2.2)

(2.3)

where K is the number of classes. This formula requires representing labels in a way called
one-hot encoding. For example, in a K-class problem, instead of having a label with K possible
= 1, 2, . . . , K, each label is encoded as a K-element vector with all-zero ele-
values such as yi
= [0, 0, 1, . . . , 0],
ments except for one at the index corresponding to the class. For example, yi
means a sample i belongs to the third class, as only yi,3 is non-zero.

Once we choose a loss function, we can minimize it by varying the parameters of the ML
model, using any optimization method of our choice. In general, we can reach the minimum of
the loss function either via analytical construction or optimization methods that can be either
gradient-based or gradient-free. A popular example of a gradient-based method is gradient
descent. The optimization usually starts in a random place within the loss landscape (meaning

21

0.00.20.40.60.81.0prediction f(xi)0123456BCE(yi,f(xi))(a)when yi=1when yi=002468x246810y(b)regression lineobserved valuespredicted values-2-1012prediction f(xi)012345(yi,f(xi))(c)MSEMAEBasics of machine learning

Figure 2.2: Choosing a learning rate has an impact on convergence to the minimum.
(a) If η is too small, the training needs many epochs. (b) The right η allows for a fast
convergence to a minimum and needs to be found. (c) If η is too large, optimization
can take you away from the minimum (you “overshoot”). This ﬁgure suggests that
loss function is convex which is rarely true.

with a model with randomly initialized parameters, θ = θ0).3 Using the model with θ0,
one makes prediction over the training data and from here, computes the loss function. The
next step consists of computing the gradients of the loss function with respect to each model
parameter, θ
j. The ﬁnal step is to update the parameters by subtracting the respective gradients
multiplied by a learning rate, η, i.e.,

θ

j := θ

j

− η

∂ L
∂ θ

j

.

(2.4)

These steps need to be repeated until the minimum is reached, and each repetition is called
an epoch. The intuition is that gradient descent updates model parameters by making steps
toward the minimum of the function (so in the opposite direction than the gradient, which
indicates where the function value grows). The learning rate controls the size of these steps.
Figure 2.2 presents in a simpliﬁed way the importance of the η choice. Both too large and too
small ηs make the optimization more challenging, and only an optimal η promises efﬁcient
convergence to a minimum. There is rarely an obvious way of choosing η which, therefore,
has to be found, e.g., by trial and error. As such, the learning rate is one of the so-called hy-
perparameters of the learning process. Hyperparameters are parameters whose values control
the learning process (especially speed of convergence and quality of the minimum) and are
chosen by a user (in contrast to model parameters, which are derived via training). The total
number of epochs or the choice of the loss function are hyperparameters too. We encounter
more examples of the hyperparameters in this introductory chapter.

To ﬁnd optimal hyperparameters, a good practice is to form (in addition to the training
data set) a separate validation data set. This data is only used to validate the model, and not
used for training. Then, we can set various hyperparameters and choose them in such a way
that the error on the validation set is minimized.4 Dividing the data set into smaller subsets

3In practice, parameters are usually initialized randomly but with the constraint to have a mean at zero and
constant variance across layers, otherwise we may encounter problems with vanishing or exploding gradients [36].
4One can even use optimization methods to ﬁnd optimal hyperparameters which minimize the validation error
(a popular library is Optuna [37]) but a choice of hyperparameters guided by intuition may prove to be a faster
and cheaper approach.

22

θθθTOO LOW(a)(b)(c)JUST RIGHTTOO HIGHL(θ)Basics of machine learning

can be problematic in case of the limited number of data. Alternative approaches for model
validation exist like k-fold cross-validation [7], which consists in splitting the data set into k
non-overlapping subsets. The validation error can be then estimated by taking average error
over k trials where the i-th trial uses i-th subset as a validation set and the rest as training
data. Note that the cross-validation comes at the price of increased computational cost.

Coming back to the gradient descent, note that to perform it, we must ﬁrst compute the
gradient of the loss function with respect to the parameters to be tuned, ∇θL, before each step,
see Eq. (2.4). A priori, there exist several different approaches how to compute these deriva-
tives. For example, one could work our the analytical derivatives by hand or approximate them
numerically based on ﬁnite differences. When we are concerned with the accurate numerical
evaluation of derivatives and not their symbolic form, automatic differentiation (AD) is a good
choice. AD makes use of the fact that the computer programs which compute the correspond-
ing loss function can be decomposed into a sequence of a handful of elementary arithmetic
operations (e.g., additions or multiplications) and functions (e.g., exp or sin). The numerical
value of the derivative of the program, i.e., the loss function, can therefore be computed in
an automated fashion by repeated applications of basic pre-deﬁned differentiation rules, such
as the chain rule

d f (g(x))
d x

= f

(cid:48)(g(x))g

(cid:48)(x).

(2.5)

For more details on how to compute derivatives of computer programs, in particular AD,
see section 7.1.5

The optimization procedure that we have described in the previous paragraphs and in Fig. 2.2

is very efﬁcient when the loss landscape, i.e., the representation of the loss values around the
parameter space of the model, is convex. Especially for DL, however, loss landscapes are highly
non-convex and usually exhibit multiple local minima [38,39]. Two immediate questions arise
from this non-convexity: ﬁrstly, how getting stuck in local minima corresponding to large loss
function values or in saddle points of such landscapes can be avoided? Secondly, whether
some minima are better than others? Currently, such questions concerning learning dynamics
are still being explored in various on-going research directions, but some intuitions are already
provided by statistical physics (see section 8.1). A popular approach to deal with the aforemen-
tioned problems considers a slight modiﬁcation of the gradient descent algorithm, so-called
stochastic gradient descent (SGD). This optimization method (whose pseudocode is provided
in Algorithm 1) consists in computing the loss function at each epoch on randomly selected
mini-batches (subsets) of the training data. This means that during each epoch the gradients
may point in various directions. Effectively, the resulting stochasticity has been shown to help
in escaping saddle points and narrow local minima [40].6 Moreover, computing the loss func-
tion and gradients only for a mini-batch of data instead for the whole data set provides a nice
computational speed-up for large data sets.

Let us look into the minimum reached during the optimization of DL models in more detail.
To do that, and to describe the curvature around such a minimum, we use the Hessian of the
training loss function, Hθ∗ = ∂ 2
|θ=θ∗, i.e., the square matrix of second-order partial
derivatives of L with respect to the model parameters, calculated at the minimum, θ = θ∗.
The eigenvectors of Hθ∗ corresponding to the largest positive eigenvalues indicate directions
with the steepest ascent around the minimum. A high curvature implies that the training data
strongly determines the model parameters along that direction. What may be surprising, the

Ltrain

∂ θ

θ

i

j

5The special case where AD is applied in reverse-mode to NNs is known as backpropagation and constitutes the

workhorse that enables efﬁcient NN training.

6In practice, stochasticity is helpful in avoiding saddle points but there are theoretical works showing it is not

a necessary condition for a proper convergence [41].

23

Basics of machine learning

Algorithm 1 Minibatch stochastic gradient descent (SGD)
Require: Learning rate η

Initialize θ to random values
for epoch = 1 to no_epochs do

Shufﬂe Dtrain
for i = 1 to m (where m is a minibatch size) do

i=1 L ( yi, f (xi

∼ Dtrain
xi, yi
(cid:80)m
L ← 1
m
← ∂ L
(∇L)
∂ θ
− η ∂
θ
∂ θ

j
← θ

j

j

j

L

j

(cid:46) Draw random data point from data set without replacement
(cid:46) Compute loss function on the minibatch
))
(cid:46) Compute gradients
(cid:46) Update parameters

end for

end for
return θ

training of an ML model leads to a local minimum or a saddle point7 [43–45]: the vast majority
of the eigenvalues are close to zero, indicating various ﬂat directions and some small negative
eigenvalues are also present, indicating directions with negative curvature. We present more
examples on what information one can gain out of Hθ∗ in section 3.5.3.

Up until this point, the only gradient based optimization method we have described is
SGD. Popular alterations to this scheme consist of, for example, including a momentum term
that takes previous update directions into account [46,47] or adaptive learning rates between
epochs [48] or both, culminating in the celebrated Adam optimizer [49,50]. Another different
idea is to incorporate the second derivative in the update rule as is accomplished by the limited-
memory Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) algorithm [51]. There are
also gradient-free optimization approaches which are used especially when the gradients or
loss function itself are expensive or impossible to compute, e.g., when optimizing experiments.
Examples include genetic algorithms, particle swarm optimization, random search and simu-
lated annealing [52]. Another example we discuss in more detail in section 4.3 is Bayesian
optimization.

2.2 Generalization and regularization

So far, ML looks like cleverly named function ﬁtting. This changes when we put the emphasis
on the generalization rather than merely maximizing the model performance on the training
data.

The heart of ML lies in the generalization, which is the ability to make correct predictions
on new data, never seen during the training.

We can quantify the generalization with the generalization error... Checking whether your
model generalizes well requires an additional data set, commonly referred to as the test set,

7One can wonder why we should trust a model which does not land in the global minimum. A series of empirical
results as well as applying spin-glass theory to deep learning [42] indicate, among others, that for large-size
networks, most local minima are equivalent and yield similar performance on a test set. Also, the probability of
ﬁnding a “bad” (high value) local minimum is non-zero for small-size networks and decreases quickly with network
size. Finally, attempting at ﬁnding the global minimum on the training set (as opposed to one of the many good
local ones) is not useful in practice and may lead to overﬁtting, i.e., much better performance on training set than
test set, which is equivalent to bad generalization.

24

Basics of machine learning

composed of data points that are used neither for optimization of model parameters nor for
searching for the best hyperparameters describing the learning process. The test set is only
used for reporting the ﬁnal performance of the model.8 Therefore, the original full data set
needs to be separated into a training, a validation, and a test set.9 One needs to be particularly
careful in the preparation of these data set as to prevent information leakage, i.e., the use of
information in the training process that is expected to be available at prediction time.10

A common problem encountered during the training of an ML model is a higher test error
than the training error: their difference is called the generalization error. This lower model
performance on the test set compared to the training set persists, even when all data points are
generated by an identical probability distribution (but disappears in the inﬁnite data limit).
The main reason is the large capacity of DL models.11

The capacity can be loosely understood as the measure of a model’s ability to ﬁt a variety
of functions. When the model capacity is much higher than one needs to solve the task,
the model tends to overﬁt, i.e., memorize all possible properties of the training set which
may not be true for the general distribution (and in particular, the test set).

In particular, the model can even ﬁt the noise in the training data. As a result, overﬁtting
increases the test error while keeping the training error low (or even decreasing it). An optimal
capacity allows for the lowest gap between the test and training error, so the lowest general-
ization error. However, a capacity that is too low results in an overly constrained model which
can underﬁt, i.e., have a high training error. The intuition behind the under- and overﬁtting is
schematically shown in Fig. 2.3. Therefore, we can improve the generalization of the model
by controlling its capacity.

Every modiﬁcation of the model aiming to improve the generalization of a model (pos-
sibly at the cost of the increased training error) is called a regularization technique.

One can think of regularization in terms of Occam’s razor.12 The additional motivation for
using regularization is the no free lunch theorem, which states that, when averaged over all
possible data generating distributions, every classiﬁcation algorithm has the same error rate
when classifying previously unobserved points [7, 54]. Therefore, no ML model is universally
better than another; and no regularization technique is universally better than another. This
implies that we need to design our ML algorithms to perform well on a speciﬁc tasks, e.g., by
regularizing it in a way which is tailored to this task.

A straightforward way of restricting the model’s capacity is to limit the magnitude of its
trainable parameters which effectively limits the hypothesis space of a parametrized model.
This can be done by adding a penalizing term to the training loss function which increases with
the parameters’ magnitude. Such an approach is used within the two popular regularization

8We need the test set, as the performance measure obtained from the validation test may be overestimated

because we use it to ﬁnd the best hyperparameters of the learning process.

9The ratio between sizes of these sets depends on how much data is available in total, but we suggest starting

with, e.g., 8:1:1.

10A common mistake is to ﬁrst normalize the whole data set and then separate it into a training, a validation,
and a test set. As a result, a model may “know” about the most extreme data points which are not part of the
training set.

11DL models are even able to ﬁt large data sets with random labels [53]!
12This principle states that among competing hypotheses that explain known observations equally well, one
should choose the simplest one. It is sometimes summarized as “entities should not be multiplied beyond necessity”.

25

Basics of machine learning

Figure 2.3: Scheme of under- and overﬁtting. (a) When the model capacity is too
low, the model cannot ﬁt the training data properly. (b) With the model capacity
corresponding to the task complexity, the ﬁtting is optimal. (c) When the model ca-
pacity exceeds the task complexity, the model tends to overﬁt and the generalization
error increases.

techniques, i.e., (cid:96)
detail in sections 2.4.1 and 4.2.1.

1 and (cid:96)

2 regularization. In particular, (cid:96)

2 regularization is described in more

Up to now, we have discussed the relationship between a model’s complexity and its per-
formance on the training and test set in intuitive terms. In the following, we formalize this
intuition through the bias-variance trade-off . Consider the standard situation encountered in
regression problems: We are given an ensemble of data points D = {x, f (x) + ε} that derives
from the function f (x) and some noise ε inherent to the data. The function f (x) is generally
unknown and our goal is to infer it. We do this by constructing a regression ﬁt of the data
ˆf (x). What we are eventually interested in for the test error (or generalization error) to be as
small as possible. The test error is given as an average of the loss function L evaluated over
test points

ErrT = (cid:69) (cid:2)L(y, ˆf (x)) | T (cid:3) ,
where T is a ﬁxed training set. This quantity is difﬁcult to calculate, and we can instead resort
to the expected prediction error obtained by averaging the generalization error over many
training sets

(2.6)

Err = (cid:69) (cid:2)L(y, ˆf (x))(cid:3) = (cid:69) [ErrT ] .
Let us look at the expected prediction error at a given point x0
)(cid:1)2(cid:151)

) = (cid:69) (cid:148)(cid:0) f (x0

) + ε − ˆf (x0

Err(x0

(2.7)

(2.8)

,

where, for now, we consider an MSE as the loss function (Eq. (2.1)). The averaging in Eq. (2.8)
is performed over all random variables inside the expression (cid:69) [·], namely the noise ε as well
as the model through the choice of different training sets. We can expand this expression as

Err(x0

) = (cid:69) (cid:148)(cid:0) f (x0

) − ˆf (x0

)(cid:1)2(cid:151) + (cid:69) [2ε( f ( x0

) − ˆf (x0

) )] + (cid:69) (cid:2)ε2(cid:3) ,

(2.9)

where (cid:69) (cid:2)ε2(cid:3) is the (ﬁxed) variance of the underlying noise in the data. Next, we use the
property of independent random variables (cid:69) [AB] = (cid:69) [A ] (cid:69) [B] to obtain
)(cid:3) = 0 ,

) )] = 2(cid:69) [ε] (cid:69) (cid:2) f (x0

(cid:69) [2ε( f (x0

) − ˆf (x0

) − ˆf (x0

(2.10)

26

x1UNDERFITTING(a)(b)(c)APPROPRIATE FITTINGOVERFITTINGx2x1x1x2x2Basics of machine learning

where we assumed unbiased noise (cid:69) [ε] = 0. Thus, we are left with
)(cid:1)2(cid:151)+(cid:69) (cid:2)ε2(cid:3) = (cid:69) (cid:148)(cid:0) ˆf (x0
(cid:69) (cid:148)(cid:0) f (x0
We modify Eq. (2.11) by adding and subtracting (cid:69) (cid:2) ˆf (x0

)(cid:1)2(cid:151)−2 f (x0

)(cid:3) (cid:69) (cid:2) ˆf (x0

)(cid:69) (cid:2) ˆf (x0

) − ˆf (x0

)(cid:3) to get

)(cid:3)+( f (x0

))2 +(cid:69) (cid:2)ε2(cid:3) . (2.11)

)(cid:3) (cid:69) (cid:2) ˆf (x0

)(cid:3)(cid:138) + (cid:69) (cid:2)ε2(cid:3) .

(2.12)

Err(x0

)(cid:3) − f (x0

) = (cid:0)(cid:69) (cid:2) ˆf (x0

)(cid:1)2(cid:151) − (cid:69) (cid:2) ˆf (x0
We can identify the ﬁrst term as the squared bias of our model
)(cid:3) − f (x0

)(cid:1)2 + (cid:128)(cid:69) (cid:148)(cid:0) ˆf (x0

)] := (cid:0)(cid:69) (cid:2) ˆf (x0

Bias2[ ˆf (x0

)(cid:1)2

,

and the second as its variance
Var[ ˆf (x0

)] := (cid:69) (cid:148)(cid:0) ˆf (x0

)(cid:1)2(cid:151) − (cid:69) (cid:2) ˆf (x0

)(cid:3) (cid:69) (cid:2) ˆf (x0

)(cid:3) .

This results in

Err(x0

) = Bias2[ ˆf (x0

)] + Var[ ˆf (x0

)] + (cid:69) (cid:2)ε2(cid:3) .

(2.13)

(2.14)

(2.15)

The average prediction error at a given unseen test point x0 can therefore be decomposed
into the bias of our model, its variance as well as the variance of the noise underlying our data
(which is irreducible from a model perspective).13

The more complex a model ˆf (x) is, the lower its bias is after training. However, the
increased model complexity generally also results in larger ﬂuctuations in capturing the
data points, resulting in a larger variance – a situation we refer to as overﬁtting. This
is referred to as the bias-variance trade-off.

Figure 2.4 shows an illustration of the bias-variance trade-off which makes clear that the
ideal model realizes an optimal trade-off between the training error and the model complexity.
Interestingly, empirical studies indicate that modern large DL models with enormous capacities
are able to generalize very well [57]. How overparametrized models can generalize so well
remains a challenging puzzle of the ﬁeld14 but some insight is provided with tools of statistical
physics (see section 8.1).

2.3 Probabilistic view on machine learning

The need for a probabilistic approach to ML becomes apparent when we consider that this ﬁeld
has to tackle three sources of uncertainty (following Ref. [7]). Firstly, there may be an inherent
stochasticity of the system generating the data we have access to (especially when dealing with
quantum data). Secondly, we need to account for a possible incomplete observability, i.e.,
an unavoidable lack of information regarding all possible variables inﬂuencing the system. In
other words, we have only partial access (by means of the available data) to all relevant parts
of the mechanism or distribution underlying the system. Finally, the models we use are rarely
complete and need to discard some available information. An example of incomplete modeling
may be a robot whose movement space we discretize. Such a discretization immediately makes
the robot uncertain about “omitted” parts of the space. To mathematically account for the
uncertainty of a model, we can follow the so-called Bayesian approach to probability which
interprets the probability as an expectation or quantiﬁcation of a belief.

In this section, we provide a concise reminder of basic concepts from the probability theory

which appear in the rest of these Lecture Notes:

13This does not only hold for an MSE loss as many variations of the bias-variance decomposition are known [55].
14Promising observations are provided by the lottery ticket hypothesis [58].

27

Basics of machine learning

Figure 2.4: Illustration of the bias-variance trade-off and its relation to the prediction
error observed on training (green curve) and test sets (red curve). The ideal model
which results in the lowest test error has both intermediate model complexity (e.g.
capacity) and training error. Adapted from Ref. [56].

• Random variables are variables taking random values. If they are independent and iden-
tically distributed (i.e., drawn independently from the same probability distribution),
they are called i.i.d. random variables.

• A probability distribution is a measure of how likely a random variable X is to take on
each of its possible states x 15, e.g., p(X = x). A probability distribution over discrete
(continuous) variables is called a probability mass function (probability density func-
tion). A joint probability distribution is a probability distribution over many variables at
the same time and is denoted, e.g., as p(X = x, Y = y). When the notation is clear, we
typically also drop the random variable and just write p(X = x) ≡ p(x), instead.

• Two random variables X and Y are independent if their joint probability distribution can
be expressed as a product of two factors, one involving only X and one involving only
Y :

∀x, y,

p(X = x, Y = y) = p(X = x)p(Y = y) .

(2.16)

You can denote this independence by X ⊥ Y .

• A vector whose elements consists of random variables is called a random vector and we

denote it simply with x.

• A conditional probability is a probability of one event given that some other event has
happened. We denote the conditional probability with p(Y = y | X = x), meaning the
probability of Y = y given the observation that X = x. It can be calculated as:

p(Y = y | X = x) = p(Y = y, X = x)

p(X = x)

.

(2.17)

15This notation is easily generalized to vector-valued random variables.

28

MODEL COMPLEXITYLowHighPREDICTION ERRORtest           datatraining     dataLow BiasHigh VarianceHigh BiasLow VarianceBasics of machine learning

• Any joint probability distribution over many random variables may be decomposed into
conditional distributions over only one variable each, which is called the chain rule or
product rule of probability:

p (cid:0)x

(1)

, . . . , x

(n)(cid:1) = p (cid:0)x

(1)(cid:1)

n
(cid:89)

i=2

p (cid:0)x

(i) | x

(1)

, . . . , x

(i−1)(cid:1) .

(2.18)

• Finally, let us discuss a situation where we know the conditional probability p( y | x)
and need to know the opposite one, p(x | y). Fortunately, if we also know p(x), we can
compute the desired quantity using Bayes’ rule:

p(x | y) = p( y | x)p(x)

p( y)

.

(2.19)

If we do not know p( y), we can compute it via p( y) = (cid:80)
x p( y | x)p(x), the sum rule of
probabilities. Bayes’ rule is a direct consequence of the deﬁnition of conditional proba-
bility in Eq. (2.17).

We are now armed with enough tools to look at ML models in a probabilistic way. In par-
ticular, we can reformulate the deﬁnition of supervised and unsupervised learning. Unsuper-
vised learning consists of observing some examples of a random variable X , e.g., x1, x2, . . . , xn,
and then learning the probability distribution p(X ) or some of its properties.16 Supervised
learning is about observing some examples of a random vector X and an associated vector
Y , e.g., {x1, y1
}, and learning to predict y from x, usually by estimat-
ing p(Y = y | X = x). An ideal ML model perfectly learns the probability distribution that
generates the data.17

}, . . . , {xn, yn

}, {x2, y2

Combining this with what we have discussed in section 2.1, now we know that ML models
(typically parametrized) are used to estimate probability distributions. Therefore, the concept
of likelihood enters the picture. The likelihood function is the joint probability of the observed
data as a function of the parameters of the chosen model, p(D | θ), estimating the data-
generating probability distribution.18 How it enters ML problems is explained on the example
of linear regression in section 2.4.1.

Finally, sometimes it is useful to compare two probability distributions over the same ran-
dom variable X , e.g., p(x) and q(x). Such a measure is provided by a relative entropy, called
(p||q). To be precise, it is a measure of how the prob-
the Kullback-Leibler (KL) divergence, DKL
ability distribution q is different from a reference probability distribution p. As we typically
employ it in classiﬁcation tasks where p and q are both discrete variable distributions, it is
deﬁned as:

(cid:173)

DKL

(p||q) =

log

(cid:183)

p(x)
q(x)

= 1
n

p

n
(cid:88)

i

p(xi

) log

p(xi
q(xi

)
)

.

(2.20)

(p||q) has some
For continuous distributions, the sum has to be replaced by an integral. DKL
properties of distance, i.e., is non-zero and is zero if and only if p and q are equal.19 But it

16Again, we can easily generalize this notion to random vectors or vector-valued random variables.
17If there is any kind of stochasticity in the underlying distribution, then even an ideal model can be wrong.

Such an error is called the Bayes optimal error that poses a fundamental limit on statistical models.

18Do not confuse likelihood and probability! Intuitively, probability is a property of a sample coming from some
distribution. Likelihood, on the other hand, is a property of a parametrized model. In particular, if you plot p(D | θ)
as a function of possible θ, it does not have to integrate to one.

19Equal in case of discrete variables, and equal “almost everywhere”, i.e., throughout all of space except for on

a set of measure zero, in case of continuous variables.

29

is not a proper distance measure as it is not symmetric, DKL
properties of the logarithm, DKL

DKL

(p||q) = 1
n

n
(cid:88)

i

p(xi

(p||q) can be expressed as
n
(cid:88)

) − 1
n

i

) log p(xi

p(xi

) log q(xi

) =: −S(p) + LCE

(p, q),

(2.21)

Basics of machine learning

(p||q) (cid:54)= DKL

(q||p).20 Using the

where S(p) is the Shannon entropy of the reference probability distribution p, and as the
second term we obtain the CE, which we have already introduced in Eqs. (2.2) and (2.3)! We
(p||q), i.e., the difference of p with respect to q, is
rediscover it by noting that minimizing DKL
equivalent to minimizing the cross-entropy, because q does not appear in S(p).

While the utility of comparing probability distributions is clear in the case of estimating
a true probability distribution with a parametrized one, it may not be immediately obvious
for arbitrary ML models. Let us discuss the case of supervised learning with a model f . Con-
}, where xi is a given sample
sider the labeled training data set consisting of n tuples {xi, yi
with label yi. Each label belongs to one out of K classes. Next, we can think of each one-
= k as a very speciﬁc probability distribution yi
hot-encoded label yi
k, j, where
k, j ∈ {1, . . . , K} (one-hot encoding). Next, the training data xi are fed to the model, f , and
), which gives us the prob-
as an output we obtain the probability distribution p(xi
abilities of a given sample xi belonging to each class. In the last step, we have to compare
two probability distributions p and q. Therefore, we rediscover the categorical cross-entropy
from Eq. (2.3).

) = f (xi

= q(xi

) = δ

2.4 Machine learning models

We have already described two out of three ingredients of the ML: tasks (section 1.4) and
data (section 1.5). The ﬁnal element is a model which learns how to solve a task given some
data. ML models can be broadly divided into two classes which are standard ML and DL.
In sections 2.4.1 to 2.4.3, we give the overview on the former while the latter is explained in
more depth in sections 2.4.4 to 2.4.6. Let us start by stressing the following point:

DL is a sub-ﬁeld of ML itself as depicted in Fig. 1.2. It is customary, however, to distin-
guish between ML methods based on whether they use neural networks (NNs). Hence-
forth, in the remainder of the chapter, we refer to standard ML as any algorithm which
does not make use of NNs.

The distinction here becomes more subtle: in a nutshell, what distinguishes traditional
learning from DL is the level of abstraction and the ﬂexibility the algorithm has in extracting the
features. In other words, traditional ML requires very speciﬁc algorithms speciﬁcally designed
and tailored to the problem at hand. What model is to be applied then often comes down to
experience and further intuition of the task of interest. On the other hand, NNs are a very
ﬂexible yet general tool whose core goal is to reproduce a target function without any (or
little) constraints on the functional class from which to search. As a down-side, they usually
do not support an easy interpretation of their mapping (compared to traditional ML methods)
and are often referred to as black-box functions. We explain to what extent this actually is
the case in section 3.5. The distinction we can infer is that DL does not require an explicit
set of instructions on how to connect the input with the output. Traditional ML methods, on
the other hand, are often constructed by geometric or information-theoretic arguments which
already provide intuition into the method by their very construction, hence their immediate
interpretability.

20We recommend an illustrative discussion of the asymmetry of DKL

(p||q) in Fig. 3.6 of Ref. [7].

30

Basics of machine learning

A natural question that arises now is: which approach to choose, traditional ML or DL?
As always: it depends. For instance, DL performs at the state-of-the-art level in the big-
data regime where an NN has enough available information to infer and perform the
feature extraction. The low-data limit is where traditional ML is still prevailing. Here,
we need to incorporate as much information as possible into the algorithm of choice to
make learning efﬁcient. Thus, algorithms become much less general and more problem-
dependent.

Let us now focus on the standard ML algorithms and leave the discussion about NNs for
section 2.4.4 and forward. Some prominent examples of traditional ML we encounter during
the rest of these Lecture Notes are the following: Principal component analysis (PCA) is a very
elegant approach for the task of dimensionality reduction, i.e., for data compression. It takes
multi-dimensional samples and compresses their feature space while maintaining only a few
relevant features. The compressed data can undergo further ML routines (see section 3.2.1).
Gaussian processes (GPs) are another examples of a traditional ML algorithm which deal well
with learning tasks when only limited data is available. Jointly with BO, they represent one of
the most powerful examples of the variational inference (see chapter 4). They are furthermore
an instance of so-called kernel methods which is as powerful as widely used. The elegance
comes from the efﬁcient application of a feature transformation of the input data. In this way,
the data which typically resides in a representation that is not amenable to treat, get mapped
into a domain where they are easier to analyze.

The mentioned methods are discussed in more detail across the Lecture Notes. The fol-
lowing sections constitute a primer of the standard ML models, describing basic approaches
like linear and logistic regression, linear support vector machines (SVMs), and continue into
the DL regime with description of NNs with focus on convolutional neural networks (CNNs)
and autoregressive neural networks (ARNNs).

2.4.1 Linear (ridge) regression

Before diving into the details of the topic, let us restate the problem of regression sketched in
section 1.4. We encounter a labeled data set D = {(xi, yi
≡ {(X, y)} of observations that
are derived from an underlying function f , possibly subject to some (stochastic) noise ε. The
latter is often assumed to be sampled from an unknown noise distribution E, i.e., ε ∼ E:

)}n

i=1

yi

= f (xi

) + ε

i

∀ (xi, yi

) ∈ D .

(2.22)

The function f is generally unknown and our goal is to infer it. To this end, we build a regres-
sion ﬁt of the data ˆf such that ˆf (x) ≈ y.21

Arguably, the simplest parametrized ﬁtting method one can produce is a linear model,
where we seek to ﬁnd parameters θ ∈ Rd that linearly connect the input variable x with the
prediction ˆy, i.e.,

d−1
(cid:88)

d−1
(cid:88)

ˆy =

θ

i xi

+ b ≡

θ

i xi

= x(cid:252)θ .

(2.23)

i=1

i=0

= b, the so-called bias, in the
To shorten the notation, we have absorbed the constant θ
0
= 1. Up to now, the linear model aims to ﬁnd a hy-
deﬁnition of the input x via setting x0
perplane22 through the data points. We can extend the model by a nonlinear transformation

21For the sake of simplicity, we consider one-dimensional output. The following derivations, however, can easily

be extended to multi-dimensional output as well.

22For one-dimensional input and output, the hyperplane simply is a line.

31

Basics of machine learning

φ of the input, i.e., x (cid:55)→ φ(x). This is still linear regression as we maintain linearity in the
parameters θ that we seek to optimize. As an example of a nonlinear transformation, the map
p : x (cid:55)→ (1, x, x 2, x 3, . . . , x p) promotes our model to polynomial regression up to the p-th
φ
degree. To simplify the notation in the rest of the section, we consider the case where no
feature maps are applied. The inclusion of a feature map is central element of chapter 4 and
discussed there to a far greater extent.

Once a certain hyperplane is deﬁned, by means of its parameters θ, we need to deﬁne
a quality measure that compares our predictions to their corresponding ground-truth values.
That is, we have to choose a suitable loss function L. The most conventional choice for the
loss is the mean-squared error (MSE) over the data set D as

LMSE

(θ | D) := 1
n

n
(cid:88)

i=1

(cid:0) yi

(cid:252)
− x

i θ(cid:1)2 = (cid:107)y − X (cid:252)θ(cid:107)2 .

(2.24)

To attain the right-most equation, we stack all inputs xi vertically next to each other, to form
the matrix X ∈ R(d−1)×n. The same procedure is applied to yi, now to be promoted to y.
The last step allows to ﬁnd the set of parameters θ that minimize the MSE. This yields the
least-squares estimator (LSE) (for the derivation, see the ﬁrst half of Appendix B)

θLSE

= (X (cid:252)X)−1 X (cid:252)y = X +y ,

(2.25)

where the notation X + denotes the Moore-Penrose inverse [59].

The MSE as the choice of our loss function appears to be self-evident. In fact, we can derive
it via maximizing the likelihood of the labeled data given the model’s parameters p(y | X, θ).
To this end, we assume that our targets y are actually sampled from a Gaussian with a mean
given by our linear model, i.e., x(cid:252)θ with some variance σ which models the noise on the data.
We can then write the likelihood of observing the targets y given the locations X and model
parameters θ as

p(y | X, θ) = N (y | X (cid:252)θ, σ21)
n
(cid:89)

N ( yi

(cid:252)
| x

i θ, σ2) .

=

i=1

(2.26)

(2.27)

In the last step, we furthermore assumed a data set D of i.i.d. random variables to factorize the
multivariate Gaussian. A common assumption is to regard the observed data set D as the most
probable one of the underlying linear model. We therefore seek to maximize the likelihood
in order to ﬁnd the set of parameters θ that have led to the most probable data. This is the
idea of maximum likelihood estimation (MLE). Its estimator is deﬁned as the argument of the
maximum likelihood of Eq. (2.26). We can modify this estimator by including a logarithm and
obtain:

θMLE := arg max

θ

p(y | X, θ)

= arg max
θ

= arg max
θ

= arg min
θ

log p(y | X, θ)

(cid:130)

− 1
2σ2

n
(cid:88)

i=1

(cid:0) yi

− x

(cid:252)

(cid:140)
i θ(cid:1)2 + const.

(cid:140)

(cid:0) yi

(cid:252)
− x

i θ(cid:1)2

(cid:130) n
(cid:88)

i=1

≡ arg min
θ

(LMSE

) .

(2.28)

(2.29)

(2.30)

(2.31)

32

Basics of machine learning

The constants appearing in Eq. (2.30) can be ignored as they are independent of θ. From the
previous results, we hence see that the i.i.d.-assumption, together with the concept of MLE
leads to the MSE as the preferred loss function and we conclude that the MLE θMLE coincides
with the least squares estimator θLSE of Eq. (2.25).

However, the estimator fully ignores the data noise modelled by σ2, as it was also drop
out in the maximization procedure of the p(y | X, θ). Thus, even if we correctly chose the
model, the minimization procedure of the MSE in Eq. (2.24) typically performs well on the
provided data set D but not on previously unencountered data points. The reason is overﬁtting
which we already introduced as a concept in section 2.2. This phenomenon occurs as we
incorporate the noise on the targets in our model parameters θMLE. As a way out to this issue,
we have introduced the notion of regularization. In our linear model (2.23), we can introduce
regularization by means of Bayesian inference. This means, instead of maximizing merely the
data likelihood in Eq. (2.26), we encode any prior knowledge of the model23 into a so-called
prior distribution p(θ). By virtue of the Bayes theorem from Eq. (2.19), we can calculate the
posterior distribution24 p(θ | X, y) over the parameters given the data set and maximize this
quantity, instead. This yields the maximum a posteriori estimator (MAP) deﬁned as

θMAP := arg max

θ

p(θ | X, y)

= arg max
θ

p(θ)p(y | X, θ)
p(X, y)

,

(2.32)

(2.33)

where we have used the Bayes theorem from Eq. (2.19) in the second step. The denomi-
nator does not depend on θ and can therefore be ignored. For the likelihood, we keep the
assumptions introduced for Eq. (2.26). As the prior, we now draw the parameter values from
a Gaussian distribution centered around 0 with some variance τ2, i.e.,

p(θ) = N (θ | 0, τ21) .

(2.34)

The product of two Gaussian distribution is Gaussian itself, hence allowing us to apply the
same trick with the logarithm as before in Eq. (2.29). We arrive at

θMAP

= arg min
θ

(cid:18)

LMSE

(θ | D) +

σ2
τ2

(cid:19)

(cid:18)
X (cid:252)X +

=

(cid:107)θ(cid:107)2

(cid:19)−1

1

σ2
τ2

X (cid:252)y .

(2.35)

We can picture the parameter λ = σ2/τ2 as a signal-to-noise ratio which effectively penalizes
large magnitudes of the parameter values by the additional term in the loss function. Hence,
λ is referred to as the regularization strength. This particular choice of the loss term is called
Tikhonov regularization. Its corresponding MAP is also called the linear ridge regression esti-
mator.

Let us compare the two estimators of Eqs. (2.25) and (2.35). The additional term σ2/τ21
in the estimator stems from the fact that we take both the data noise as well as a param-
eter constraint into account. Both are discarded in the limit of τ → ∞25, where we have
θMAP

→ θMLE.

In order to consider the underlying noise in the training data, we have to constrain the
linear model. Dealing with overﬁtting in such a way is usually referred to as regular-
ization.

23The prior knowledge correspond to our initial assumptions on the parameters before we are handed any data.
24We call it posterior because it is computed after the observation of the data set D.
25This corresponds to a uniform prior of the parameters θ.

33

Basics of machine learning

Finally, the choice of the prior in Eq. (2.34) is by no means unique.

In fact, there is
a plethora of regularization ideas and corresponding penalty terms [7]. An easy variation
2-norm with an (cid:96)
could, for example, be to replace the (cid:96)
1-norm. This is achieved by choosing
a Laplace distribution for the parameters as the prior. The corresponding estimator is the re-
sult of least absolute shrinkage and selection operator (LASSO) regression [60]. Because the
(cid:96)
1-norm punishes already small parameter values severely, it favors sparse solutions for the
parameters θ, instead. This can, for example, be desired to detect the signiﬁcant features out
of a pool of possible candidates in certain tasks [61].

2.4.2 Logistic regression

In the previous section, we have discussed the linear regression problem. The discussion can be
extended to the classiﬁcation task in a very straightforward way as we show in the following.

The basic idea of logistic regression is to adapt the linear model such as to estimate the
probability that a given input falls in either one of the possible classes.

Let us consider two classes K1 and K2 and an input x to classify. We introduce the class-
conditional densities p(x|Ki
).
Bayes’ theorem of Eq. (2.19) immediately gives us an expression for the posterior probability
that the input belongs to class K1. It reads as

) and the corresponding baseline class prior probabilities p(Ki

p(K1

| x) =

=

p(x | K1
)p(K1

)

)p(K1
) + p(x | K2
=: ς(θ )

p(x | K1
1
1 + exp(−θ )

)p(K2

)

(2.36)

where θ := − log

(cid:129) p(x | K2
p(x | K1

)p(K2
)p(K1

)
)

(cid:139)

and equips us with the logistic sigmoid function ς that maps any real-valued input θ to the
interval [0, 1]. We can now use the linear model (or any other ML model) to yield a value
for θ and map it to the corresponding posterior probability. This additional layer turns the
regression model into a classiﬁer.

In order to extend the situation to more than two classes, we perform a similar reformula-

tion as done in Eq. (2.36). In this case, one obtains the softmax function

p(Kk

| x) =

k

exp(θ
)
i exp(θ

i

(cid:80)

=: softmax(θ)

)

(2.37)

that maps the output score vector θ to a proper probability density over all classes at once.
Its name is derived from the fact that in the limiting case of θ
∀k (cid:54)= i, the softmax
converges to the maximum function, i.e., softmax → max.

(cid:29) θ

k

i

In both cases, the model’s parameters are trained by parsing the output scores through
either Eq. (2.36) or Eq. (2.37) to obtain and subsequently minimize the loss in Eq. (2.2) or
Eq. (2.3), respectively. An interesting aspect of any classiﬁer is how it draws a line between
data from two different phases, known as the decision boundary. In case of the linear model
the decision boundary is linear, which is a simple consequence of the model choice. Because
this boundary is derived from the likelihood of the data due to the particular choice for the
loss function, the model is highly prone to outliers. One way to circumvent this issue is to take
a geometric approach in ﬁnding the decision boundary. This is done in the next section.

34

Basics of machine learning

Figure 2.5: Geometric construction of an SVM in a 2D problem. (a) Purple line,
described fully by θ, is an exemplary hyperplane separating two classes of data (pink
and green points). (b) The optimal hyperplane maximizes the margin, M , between
itself and the support points which are training data points closest to the hyperplane.

2.4.3 Support vector machines

The alternative approach to the classiﬁcation, instead of maximizing a model likelihood, is
to analyze the geometrical properties of the data. Take a look at the the linearly separable
problem presented in Fig. 2.5.
In panel (a), you see that to classify two types of data, we
can draw a line (or more generally, a hyperplane) which separates the training data. Then,
instead of making probabilistic predictions on test data, we can just check on which side of the
hyperplane the test points are. Panel (a) contains also a simple geometric analysis which shows
that the equation for the hyperplane separating the data is θTx + θ
= 0. The unit vector in
0
the direction perpendicular to the hyperplane is θ∗ = θ/|θ|, and the shortest distance between
a point x and the hyperplane is

d(x, θ) = (cid:0)θTx + θ

0

(cid:1) /|θ| .

(2.38)

If we do not impose any additional constraints, there are many possible hyperplanes sepa-
rating the data into the two classes. How do we choose the best one? One way is by maximizing
the distance between the hyperplane and the data points. Therefore, let us formulate the con-
straint that all data points need to be at least the distance M away from the hyperplane. The
data points separated from the hyperplane exactly by M , hence the closest to the hyperplane,
become the support points presented in Fig. 2.5(b). The classiﬁcation problem boils down to
ﬁnding the θ which maximizes the margin. From Eq. (2.38), we can write:

(cid:0)θTxi

yi

+ θ
0

(cid:1) ≥ M |θ| ,

(2.39)

where elements of the vector of observations yi are ±1 in order to ensure that this formulation
is always positive, regardless of the class to which the data point belongs. Note that if we scale
each of the θ coefﬁcients by the same factor, the above (in)equality still holds. Therefore, we
can arbitrarily rescale θ and θ
M that leads to the following canonical condition
for every data point in the data set:

0 to have |θ| = 1

(cid:0)θTxi

yi

+ θ
0

(cid:1) ≥ 1 .

(2.40)

Therefore,
(cid:0)θTxi
yi

+ θ
0

to ﬁnd the optimal hyperplane, we need to minimize |θ|, while ensuring
(cid:1) ≥ 1 for every data point. This is the optimization with constraints, and we

35

projection of ( x - x0) on θ*θ*T (x - x0) = = θ*Tx - θ*Tx0 == θ*Tx +θ0| θ |1| θ |(θ*Tx + θ0)θ0 + θ*Tx = 0θ| θ |θ* =unit vectorx0(a)x(b)1| θ |M =xTθ + θ0 = 0support points1| θ |M =margincan use Langrange multipliers for that! Minimizing |θ| with constraints comes down to mini-
mizing the following Lagrange function:

Basics of machine learning

L = 1
2

|θ|2 −

n
(cid:88)

i

α

i

(cid:2) yi

(cid:0)θTxi

+ θ
0

(cid:1) − 1(cid:3) ,

where the Lagrange multipliers α
(cid:2) yi

α

i

i are chosen such that
(cid:0)θTxi

+ θ
0

(cid:1) − 1(cid:3) = 0 for each i .

(2.41)

(2.42)

Interestingly, the loss function in Eq. (2.41) with the above constraints is a so-called quadratic
program as the function itself is quadratic and the constraints are linear with respect to |θ|. It
has, therefore, a global minimum found usually via so-called sequential minimal optimization
[62] instead of any iterative gradient-based methods.

Also note that the condition put on the Lagrange multipliers in Eq. (2.42) implies the

following:

• If α

i

> 0, then (cid:2) yi
of the margin slab.
(cid:0)θTxi

+ θ
0

• If (cid:2) yi

(cid:0)θTxi

+ θ
0

(cid:1) − 1(cid:3) = 0, which means the point xi lies on the boundary

(cid:1) − 1(cid:3) > 0, the points is outside the margin and α

= 0.

i

Therefore, the ﬁnal model coefﬁcients are given only in terms of such points xi := xs,i that lie
on the boundary of the slab. These points are the support points and give the SVM its name.
The SVM problem relies then on minimizing L26 numerically to ﬁnd the coefﬁcients α
i which
are non-zero only for support points.

Therefore, classiﬁcation with support vector machines (SVMs) consists in ﬁnding the
optimal hyperplane separating the data by maximizing the margin between the hyper-
plane and the support points which are data points closest to the decision boundary.
This optimization problem with constraints is solved with Lagrange multipliers and is
convex.

With the found optimal hyperplane ˆf we can then make predictions at an arbitrary test

point x∗:

ˆf (x∗) = θT x∗ + θ
0

=

n
(cid:88)

α

i

i yixT

i x∗ + θ

0

α

= (cid:88)
i

i yixT

s,ix∗ + θ

0 ,

(2.43)

where the last summation is only over support points. Finally, in order to turn this value into
a class prediction, we take the sign of ˆf as the corresponding class label.

Until now, we only considered binary classiﬁcation problems of linearly separable data sets.
There are two obvious ways on how to extend the SVM to classiﬁcation problems that have
more than two classes, say K many. The ﬁrst, known as the one-to-one approach, breaks the
multi-class situation down to a binary classiﬁcation between every combination of two classes,
individually. This way, we are required to train O(K 2) SVMs to make predictions afterwards.
This numerical overhead is eased in the second approach: one-to-rest classiﬁcation. Here, we
only require a single SVM for each of the K classes that simply predicts whether a test point
belongs to the class or not. As a second extension possibility, we can ask about the classiﬁcation
problem that is not linearly separable. We explain this case later in section 4.2.2.

26In practice, rather than minimizing L, one maximizes a Lagrange dual, LD, which provides the lower bound

for L. We explain it in more detail in section 4.2.2.

36

Basics of machine learning

Figure 2.6: Illustration of (a) a typical fully-connected (here: two-layer) NN and (b)
one of its neurons (simple perceptron) and the computations associated with it.

2.4.4 Neural networks

Artiﬁcial neural networks (ANNs), typically referred to as neural networks (NNs), are a large
class of models used to process data in ML tasks. They are parametrized functions that are
themselves composed of many simple functions. As the name suggests, ANNs were originally
proposed by taking inspiration from the neural networks that constitute our brains. They are
typically composed of interconnected layers that sequentially process information, see Fig. 2.6.
Each layer contains multiple nodes or units, also called artiﬁcial neurons or perceptrons.27 Each
) ∈ Rm, corresponding to the activations of
node i takes as input a vector x = (x1, x2, . . . , xm
∈ R (its activation) that is computed
all nodes in the previous layer. It outputs a scalar value ai
∈ R are the weights and bias
as ai
of node i, respectively. The weights of a node control the strength of its connection to the
neurons of the previous layer. The function ς is a nonlinear function called activation function.
Common choices are the rectiﬁed linear unit (ReLU)

), where the parameters {wi, j

}m
j=1 and bi

j wi, j xi

= ς((cid:80)

+ bi

the sigmoid function (Eq. (2.36)), or the tanh function

ς(z) = max(0, z),

ς(z) = tanh(z) = ez − e−z
ez + e−z

.

(2.44)

(2.45)

The ﬁrst layer is called input layer, where the activations of its nodes are set according to
the vector x encoding the input data. The last layer is called output layer and the activations
of its nodes constitute the output of the NN. All intermediate layers are called hidden layers.
NNs where each node is by default connected to all nodes in the subsequent layer are referred
to as fully connected. The number of layers, nodes, and their connections is known as the
architecture of an NN. NNs are considered deep if they are composed of many hidden layers.28
ML methods based on deep neural networks (DNNs) as models fall under the name of DL [7].

A central question regarding NNs is what types of functions they can represent (recall our
previous discussion on traditional ML vs. DL). First, consider an NN without its nonlinear
activation functions. The function realized by such an NN is a simple afﬁne map, i.e., con-
sists of multiplying the input by a weight matrix and adding to it an additional bias vector.

27Here and in the following, we refer to the modern perceptron introduced by Minsky and Papert [63] which
can contain smooth activation functions in contrary to the Heaviside step function utilized in Rosenblatt’s original
perceptron [64].

28There is no clear consensus on the threshold of depth that divides shallow and deep NNs.

37

(b)(a)input layerhidden layer(s)output layerxa(1)(x) = ς(W(1)x + b(1))  a(2)(x) = ς(W(2)a(1) + b(2))  wij(1), bi(1)wij(2), bi(2)cell bodyoutput axonactivationfunctionw1x1w2x2x0w0w0x0axon from a neuronsynapsedendritewixi + bΣiwixi + bΣiςς((Basics of machine learning

Thus, the addition of nonlinear activation functions is crucial for NNs to be able to represent
a larger class of functions. For example, Kolmogorov and Arnold [65] have shown that any
arbitrary continuous high-dimensional function can be expressed as a linear combination of
the composition of a set of nonlinear functions

f (x) =

2m
(cid:88)

i=0

ζ

i

(cid:32) m
(cid:88)

j=1

(cid:33)
)

,

ς

(x j

i, j

(2.46)

i, ς

where ζ
i, j are nonlinear functions that act on the individual components of the input
x ∈ Rm. This means that we could represent any function f (x) with a polynomial num-
ber O(m2) of one-dimensional nonlinear functions. This strongly resembles the structure of
an NN with two hidden layers. Note, however, that the nonlinear functions must be care-
fully chosen depending on the target function. In NNs, the nonlinearities are typically ﬁxed
∀i, j. It turns out that fully-connected NNs composed of a single hidden layer and
ζ
nonlinear activation functions are also universal function approximators. That is, given that
the target function is reasonably well-behaved it can be approximated to any desired accuracy
given that its hidden layer contains enough nodes [7, 66, 67]. Note that this may still require
a hidden layer that is exponentially large in the number of nodes. This raises the question
what one can achieve with NNs that have multiple hidden layers.

= ς

i, j

i

The universal approximation theorem guarantees that there exists an NN, i.e., choice of
NN architecture, as well as weights and biases, which approximates the given target function
arbitrarily well. However, it does not guarantee that we are able to ﬁnd this choice. It turns
out that, in practice, DNNs are capable of solving many problems with much less nodes, i.e.,
trainable parameters, compared to shallow NNs. In that sense, choosing a DNN over a shallow
NN yields a useful prior over the space of functions which the NN can approximate.

The parameters of an NN are typically optimized by gradient-based methods, such as
stochastic gradient descent (SGD) or Adam, to minimize a given loss function L (see sec-
tion 2.1). Computing the gradient of the loss function with respect to the NN parameter
numerically is typically done by means of backpropagation [68] which we discuss in more de-
tail in section 2.5. In contrast, when evaluating an NN with a given input, information ﬂows
forwards through the networks. As such, this is called forward propagation.

Convolutional neural networks Convolutional neural networks (CNNs) are a special class
of NNs where, in contrast to fully-connected NNs, not every node is connected to all nodes of
the subsequent layer. Instead, convolutions replace matrix multiplications in the computation
of the activations of subsequent layers. This reduction of the number of parameters per layer
allows us to build and train deeper architectures. Moreover, this model architecture makes
use of the spatial hierarchy typically present in input data. In image-like data, pixels that are
spatially close to each other generally show more correlation than pixels which are far apart.
By replacing the full connectivity of the standard NN with multiple convolutional layers with
local connectivity, CNNs make use of this vanishing correlation at large distances.

Figure 2.7 illustrates the working principle behind a CNN – the convolutions: A ﬁlter (also
called kernel) with trainable weights is slid across a given layer. The resulting activation are
then obtained by element-wise multiplication of the neuron activations and the ﬁlter’s weights,
followed by an overall sum and the application of a nonlinear activation function. This ﬁlter-
ing causes the NN to be only locally connected (as opposed to fully connected). Note that the
number of weights, therefore, does not depend on the size of the input, but rather on the size
of the ﬁlter. The ﬁlter size controls the range over which spatial correlations in the input data
are registered. One can build one- or two-dimensional CNNs (with ﬁlters of corresponding

38

Basics of machine learning

Figure 2.7: Schematic representation of a convolutional layer in two dimensions:
A kernel/ﬁlter of ﬁxed size (here 3x3) is convolved with a two-dimensional input
image. The grey scale corresponds to the magnitude of the neuron activations and
kernel (ﬁlter) weights.

dimension) depending on whether the input data is naturally represented as a vector or a ma-
trix. In a typical CNN, after application of several such convolutional layers, the activations
are ﬂattened to a single feature vector. This corresponds to a lower-dimensional representa-
tion of the input data that is further processed using a fully-connected architecture. To reduce
the dimension of the data representation resulting from the application of convolutional lay-
ers, one typically also uses pooling operations. These combine the activations resulting from
applications of close-by ﬁlters, e.g., by taking the maximum or mean.

2.4.5 Autoencoders

Autoencoders (AEs) [69, 70] are widely used ML tools for unsupervised learning. Unlabeled
data (e.g., images, audio signals, texts) may often be high-dimensional, hence very difﬁcult
to analyze and to extract any patterns when working in the data domain. However, dimen-
sionality reduction techniques (see, e.g., section 3.2.1) represent an advantageous approach
to extract useful knowledge from such unlabeled data.
In a nutshell, the goal of AEs is to
precisely encode some knowledge, patterns, attributes of the given input data into some latent
variable29 on a lower dimensional manifold. By means of a so-called bottleneck structure (as
shown in Fig. 2.8), the latent representation of the input data is the mapped back into the
input space (decoding) by leveraging on the information extracted by the architecture at the
time of feature extraction (encoding). This bottleneck architecture is based on two NNs per-
forming the encoding and decoding parts. Such NNs are trained by minimizing the so-called
error reconstruction loss, meaning that the optimal setup for such encoder-decoder pair is the
one for which the output xrec is reconstructed as similar as possible to the original input data
x. These NNs are jointly optimized with an iterative process. In other words, for a given set of
possible encoders and decoders, we are looking for the pair that keeps the maximum of infor-
mation when encoding and, so, has the minimum of reconstruction error when decoding. This
joint optimization forces the model to maintain only the variations in the data required to re-
construct the input without holding on to redundancies within the input. Henceforth, likewise
in PCA, only the most relevant features describing the data are distilled during the learning
process. One important remark is that the bottleneck is a key attribute of such a network
design; without the presence of an information bottleneck, our network could easily learn to
simply memorize the input values by passing these values along through the network. On top

29A latent variable is a random variable that we cannot observe directly. In this case, we call variables latent

because we do not observe them in the data.

39

KERNELΣ=Basics of machine learning

Figure 2.8: Example of the bottleneck architecture of an AE. The input is connected
to the bottleneck by an encoder-NN on the left while the decoder-NN connects it with
the output on the right.

of this, by relying on such a pair of NNs, AEs are inherently more ﬂexible yet expressive com-
pared to standard dimensionality reduction algorithms (e.g., PCA) which rely on sub-manifold
projection of input data through constrained linear or nonlinear transformations.

There are several kinds and variations of AEs, all of which share this fundamental bottle-
neck property as their base structure. A concrete example of a further development of AEs
in the context of generative models are variational autoencoders (VAEs). As the name sug-
gests, VAEs [69] have to do with variational inference. What they do in practice is to train
the encoding-decoding pair in a slightly more complicated way. The knowledge extracted
from the data in the encoding part is nested into a base probability density (e.g., initialized as
a Gaussian) which is trained and tuned in such a way that it becomes a good approximation
(sampler) of the underlying data distribution. Once the training is done, the latent represen-
tation of the input data becomes thus a probability density from which one can sample new,
unseen data which resembles the one used for training, as being characterized by the same
learned features. As such, the goal here is not to reconstruct the input data from the extracted
knowledge anymore, but to produce new samples as similar as possible to the training set.
Further example of AEs are: sparse AEs [71, 72], denoising AEs [73], importance weighted
AEs [74], etc.

2.4.6 Autoregressive neural networks

To complete this section, let us brieﬂy present autoregressive neural networks (ARNNs). These
networks were originally inspired by autoregressive models in statistics and economics, which
one can employ to predict future values of a time-series (for instance, a ﬁnancial asset). ARNNs
are formalized for the general task of density estimation [75], in which the goal is to estimate
a complex, high-dimensional probability density function, see als section 7.2. They are con-
structed to satisfy the following property on the outputs of the network, satisfying a conditional

40

INPUT LAYERHIDDEN LAYER“bottleneck”OUTPUT LAYERa2a1a3x3ˆx1ˆx2ˆx4ˆx5ˆx6ˆx3x1x2x4x5x6Basics of machine learning

Figure 2.9: Pictorial representation of an recurrent neural network (RNN). One can
directly see that the model is autoregressive, as conditional probabilities only depend
on the previous input data. Here, the blue box represents a nonlinear transformation
as described in the main text. The task here is to be able to generate meaningful
sentences. The input data is a sentence, and the output is the probability for the word
“W” to be the next word in the sentence, conditioned on previous words. The hi are
the hidden vectors that take into account memory effects inherited from previous
RNN transformations. Adapted from [76].

structure

f (x) =

m
(cid:89)

i=1

(xi

fi

| xi−1, . . . , x1

)

(2.47)

) the inputs for the model.

with x = (x1, x2, . . . , xm
In the case of time-series, the inputs
xi would be values of a variable at times ti, and the model f tries to predict future values
based on past ones. A generic example of such networks is the recurrent neural network
(RNN) that was popularized in the context of natural language processing tasks. The main
idea behind this class of models is that information “loops back” into the model, introducing
correlations between different parts of the network, as opposed to feed-forward networks.
Broadly speaking, a sentence has a causal order, but the correlations between words is not
necessarily highest between words that are close together, hence the idea of introducing a back
loop, with a memory can be understood somewhat intuitively. The long-short-term memory
(LSTM) is an extension of this idea with two memory length scales (long- and short-term),
and was also found to be successful for such tasks [77]. A sketch of an RNN is presented in
Fig. 2.9, with an example use-case from a language processing task. The goal here is to predict
the next word in the sentence, based on previous words. The parameters of such a network
are hidden in the RNN cell, and take part in a nonlinear transformation given by:

hi

= ς(Whh(i−1) + Wix(i))

with Wh and Wi two weight matrices, h(i) the i-th hidden vector, that represents information
coming out of the previous cells, x(i) the i-th element of the input data, and ς some nonlinear
activation function that is applied element-wise.30 For words, x(i) represents the i-th word
of the sentence that is encoded in some form (for example, using a one-hot encoding). Note
that there exists several variants of this transformation, the most popular being the gated

30One also has to choose an initialization x(0), h(0), which are generally null vectors.

41

“May”“the”“Force”“be”P (W | “May”)P (W | “...the”)P (W | “...Force”)P (W | “...be”)h0h1h2h3h4SoftmaxRNNRNNRNNRNNSoftmaxSoftmaxSoftmaxBasics of machine learning

recurrent unit [78]. Recently, autoregressive models have been applied to different problems
in physics such as statistical mechanics [79–81], quantum tomography [82], and ground state
search [83]. In chapter 5 and section 7.2, we stress the advantages of using such models and
present impressive results for quantum physics and chemistry that have been obtained using
them.

2.5 Backpropagation

As already mentioned in section 2.1, NNs are typically trained via gradient-based methods.
These approaches require the calculation of the loss function’s derivative with respect to each
trainable parameter. In principle, given a particular NN architecture, we could derive a closed-
form solution for the gradient. However, this computation would need to be performed again
given different NN architectures. Such calculations also involve some form of human input,
which makes them tedious and prone to errors. Clearly, we would like to automate this gra-
dient calculation and make it as efﬁcient as possible. The algorithm of choice to train large
NNs is backpropagation [68]. Backpropagation belongs to a larger class of algorithms known
as automatic differentiation (AD), which allow us to evaluate the derivative of a function rep-
resented as a computer program efﬁciently, and in an automated fashion. We describe and
compare these methods in detail in section 7.1.

The basic idea behind backpropagation is to take advantage of the fact that an NN is
composed of sequences of many elementary building blocks, such as artiﬁcial neurons.
Thus, we can compute derivatives through the repeated (reverse) application of the
chain rule.

In order to provide some intuition, we exemplify the use of backpropagation on a simple
feedforward network, where nodes in each layer are connected only to nodes in the immediate
next layer. However, the main principle carries over to any other architecture, such as the
ones introduced in the sections above like CNNs, AEs, or RNNs among others. Recall that the
activations of the nodes in the l-th layer of a feedforward neural network are given by

a(l) = ς(l) (cid:0)z(l)(cid:1) = ς(l) (cid:0)W (l)a(l−1) + b(l)(cid:1) ,

(2.48)

where a(l−1) is a vector that contains the activations of the previous layer, i.e., layer l − 1.
(l)
The corresponding weight matrix is given by W (l), where w
i, j is the weight of the connection
from node j in layer l − 1 to node i in layer l, b(l) is the bias vector of layer l, and ς(l) is the
activation function of the l-th layer. The function implemented by a feedforward NN with L
layers (L − 1 hidden layers and an output layer) can be obtained by stacking up multiple such
layers

NN(x) = a(L)(x) = ς(L) (cid:0)W (L)a(L−1)(x) + b(L)(cid:1) ,
where a(0)(x) = x is the input vector. The crucial observation is that the NN output depends
on the input x solely through the activations of the previous layer a(L−1), which in turn only
depends on the input through a(L−2), and so on (see Eq. (2.48)). This simply arises from the
layer-wise processing of information in a feedforward NN.31

(2.49)

Eventually, we are interested in computing the derivatives of our loss function L with
In the following, we focus only on

respect to all weights ∂ L/∂ w

(l)
i, j and biases ∂ L/∂ b

(l)
i

.

31In fact, the concept of a feedforward network can be generalized to any directed acyclic graph. In any case,

the information processing occurs in a “forwards-directed” manner (from input nodes to output nodes).

42

Basics of machine learning

weights. However, the procedure straightforwardly generalizes to biases. For a given training
data set D = {(xi, yi

i=1, the loss function is typically given as an average

)}n

L = 1
n

n
(cid:88)

i=1

(cid:96)(NN(xi

), yi

) .

(2.50)

Here, (cid:96) measures the deviation of the prediction NN(xi
yi possibly including an additional regularization term. Thus, we have

) from the corresponding desired output

∂ L
∂ w

= 1
n

n
(cid:88)

i=1

∂ (cid:96)(NN(xi
∂ w

), yi

)

,

(2.51)

where w is a single weight of the NN. From Eq. (2.51), we see that the main task boils down
to computing derivatives for a ﬁxed input-output pair (xi, yi

∂ (cid:96)(NN(xi
∂ w

), yi

)

=

=

∂ (cid:96)(NN(xi
∂ NN
∂ (cid:96)(a(L)(xi
∂ a(L)

), yi

)

·

), yi

)

·

)

) of the form
∂ NN(xi
∂ w
∂ a(L)(xi
∂ w

)

,

,

(2.52)

The ﬁrst term can be computed manually for a given choice of loss function (see section 2.1).
For example, for an MSE loss function we have

resulting in

(cid:96)
MSE

(NN(xi

), yi

) = (cid:107)NN(xi

) − yi

(cid:107)2 ,

∂ (cid:96)

MSE

), yi

)

(NN(xi
∂ NN

= 2(NN(xi

) − yi

) .

Therefore, the central quantity of interest is

∂ NN(x)
∂ w

=

∂ a(L)(x)
∂ w

,

(2.53)

(2.54)

(2.55)

which we are going to compute via repeated application of the chain rule. In the following,
we drop the explicit dependence on x.

The key observation for the backpropagation algorithm is the fact that, due to the layer-
wise processing of information in a feedforward NN, the only way a weight in layer l inﬂuences
the loss is through the next layer l + 1. Thus, let us start by looking at the last layer L. Recall
that a(l) = ς(l)(z(l)) from Eq. (2.48). Using the chain rule, we have

∂ a(L)
∂ w

=

∂ a(L)
∂ z(L)

∂ z(L)
∂ w

=

∂ a(L)
∂ z(L)

(cid:18) ∂ W (L)
∂ w

a(L−1) + W (L) ∂ a(L−1)
∂ w

(cid:19)

,

(2.56)

where ∂ a(L)/∂ z(L) = J (L)

is the Jacobian matrix of ς(L) containing the derivative of the activa-

(z)/∂ z j. Note that J (l)

ς

is diagonal, e.g., for ReLUs (Eq. (2.44)),

tion functions

(cid:128)

J (L)
ς

(cid:138)

i, j

ς
= ∂ ς(L)

i

but not for the softmax function (Eq. (2.37)). From Eq. (2.56), if w is a weight of layer L, i.e.,
w = w

(L)
i, j , we have

∂ a(L)
∂ w

= J (L)
ς e

(L)
i a

(L−1)
j

.

(2.57)

(L)
i

Here, e
the i-th node whose activation is one. Otherwise, we have

is an activation vector of layer L where the activation of all nodes is zero except for

∂ a(L)
∂ w

= J (L)

ς W (L) ∂ a(L−1)
∂ w

.

43

(2.58)

Basics of machine learning

To evaluate this expression, one needs to go further back in the layers and compute the deriva-
tives ∂ a(L−1)/∂ w given by

∂ a(L−1)
∂ w

=

∂ a(L−1)
∂ z(L−1)

∂ z(L−1)
∂ w

= J (L−1)
ς

(cid:18) ∂ W (L−1)
∂ w

a(L−2) + W (L−1) ∂ a(L−2)
∂ w

(cid:19)

.

(2.59)

Again, if w is part of layer L − 1, i.e., w = w

(L−1)
i, j

, we have

Otherwise, we have

∂ a(L−1)
∂ w

= J (L−1)
ς

e

(L−1)
i

a

(L−2)
j

.

∂ a(L−1)
∂ w

= J (L−1)

ς W (L−1) ∂ a(L−2)
∂ w

.

Recognizing the recursive nature of the computation, we have the following relation

∂ a(l)
∂ w

= J (l)

ς W (l)J (l−1)

ς W (l−1)

. . . J (l (cid:48)+1)

ς W (l (cid:48)+1) ∂ a(l (cid:48))
∂ w

,

(2.60)

(2.61)

(2.62)

given that w is not a weight of the layers l (cid:48) through l. If w is part of layer l (cid:48), i.e., w = w
instead have

(l (cid:48))
i, j , we

∂ a(l)
∂ w

= J (l)

ς W (l)J (l−1)

ς W (l−1)

. . . J (l (cid:48)+1)

ς W (l (cid:48)+1)J (l (cid:48))
ς e

(l (cid:48))
i a

(l (cid:48)−1)
j

.

(2.63)

Finally, we have all ingredients to formulate the backpropagation algorithm. Recall that
our goal is to compute the derivative in Eq. (2.52) with respect to all tunable weights. To do
that efﬁciently, we ﬁrst initialize the following “deviation” at the output layer

∆(L) =

∂ (cid:96)(a(L)(xi
∂ a(L)

), yi

)

(cid:12) J (L)
ς

,

(2.64)

where (cid:12) denotes an element-wise (Hadamard) product.32 This intermediate quantity turns
out to be useful throughout the computation. Taking the MSE loss as an example, this would
correspond to (see Eq. (2.54))

∆(L) = 2(a(L) − yi

) (cid:12) J (L)

ς

.

(2.65)

From Eq. (2.57), it follows that the contributions to the derivative of the loss function with
respect to a weight w

in layer L are given by

(L)
i, j

2(a(L) − yi

) (cid:12) J (L)
ς e

(L)
i a

(L−1)
j

= ∆(L)e

(L)
i a

(L−1)
j

.

(2.66)

The ﬁnal derivative ∂ (cid:96)(a(L)(xi
this vector

), yi

)/∂ w

∂ (cid:96)(a(L)(xi

), yi

)/∂ w

(L)
i, j is then obtained by summing up all components of
(cid:138)

(L)
i, j

= (cid:88)
k

(cid:128)∆(L)e

(L)
i a

(L−1)
j

,

k

(2.67)

i.e., all individual contributions to the inner product given in Eq. (2.52). Having computed
the derivative with respect to all weights in layer L, we move one layer backwards, hence the

32It simpliﬁes to a regular scalar product given a single output node.

44

Basics of machine learning

name backpropagation. From Eq. (2.58), it follows that the contributions to the derivative of
a weight w

in layer L − 1 are given by

(L−1)
i, j

where

∆(L−1)e

(L−1)
i

a

(L−2)
j

,

∆(L−1) = ∆(L)W (L)J (L−1)

ς

.

(2.68)

(2.69)

Notice the intimate connection between the above procedure and the expressions in Eq. (2.62)
and Eq. (2.63). Thus, through recursion we have

∆(l−1) = ∆(l)W (l)J (l−1)

ς

.

(2.70)

This process is repeated until one arrives at the ﬁrst layer.
In the end of this reverse pass
through the NN, one has computed the desired derivative (Eq. (2.52)) with respect to all tun-
able weights. During the backpropagation algorithm, the value of all activations {a(l)(xi
l=0
and the derivatives of the corresponding activation function evaluated at that activation
{J (l)
l=1 must be known. In order to avoid any recomputation, one performs an eval-
uation of the network for the given input xi, i.e., a forward pass, and caches all the required
intermediate computation results before executing the backpropagation algorithm, i.e., the
reverse pass.

ς (xi

)}L

)}L

To further illustrate how backpropagation works, let us calculate both the forward and
reverse passes explicitly on the example of a simple two-layer NN with the MSE as the loss
function and ReLUs as activation functions, ς(l)(z) = ς(z) = ReLU(z) = max(0, z) which act
element-wise. The derivative of ReLU is 1 for z > 0 and 0 otherwise.33 We randomly initialize
the weights of this NN and ignore biases, see panel (a) of Fig. 2.10. The forward pass for
Importantly, the intermediate
an exemplary input-output pair is presented in Fig. 2.10(b).
computation results are cached. This includes the activations of all nodes, a(1) and a(2), and
the derivatives of the corresponding activation functions. Then, the backward pass starts in
panel (c) of Fig. 2.10. Here, we only focus on the calculation of the derivative of (cid:96) with respect
(1)
to two weights coming from different layers, w
1,2. The ﬁrst step is to compute the
deviation ∆(2) on the last layer from Eq. (2.65), after which a(2) can be erased from memory.
Then, using Eq. (2.66), we can calculate the derivatives of (cid:96) with respect to any weight w(2) in
the last layer, and activations a(1) can be discarded. The next step is to compute the deviations
∆(1)
1 and ∆(1)
2 on the second-last layer following Eq. (2.70). After this computation, ∆(2) can
(1)
be discarded. Using ∆(1)
1,2.
In the case of a two-layer NN, this concludes the calculation of the gradient. Note that at every
step memory can be freed by erasing cached results from the forward pass.

= x2 and following Eq. (2.68), we can compute ∂ (cid:96)/∂ w

(0)
1 and a
2

(2)
1 and w

At this point, one might raise an interesting question: Why do we compute the derivative in
a reverse pass instead of a forward pass? To answer this question, we ﬁrst have to formulate the
(l)
corresponding “forward-propagation algorithm”. For each weight w
i, j with respect to which
one want to compute a derivative (Eq. (2.52)), one ﬁrst computes

∂ a(l)
(l)
∂ w
j,i

= J (l)
ς e

(l)
i a

(l−1)
j

.

(2.71)

(l)
Now, one can directly make use of the relation in Eq. (2.62) to obtain ∂ a(L)/∂ w
i, j , from which
the ﬁnal derivative can be computed via Eq. (2.52). Notice that the difference between the
backpropagation and forward-propagation algorithm amounts to evaluating the expression
in (2.63) from left-to-right (backwards) or right-to-left (forwards), respectively.

33Formally, ReLU is non-differentiable at z = 0. In numerical practice, the derivative at z = 0 is usually set to 0.

45

Basics of machine learning

Figure 2.10: Backpropagation on a simple example of a two-layer feedforward NN
presented in (a). (b) Forward pass through the network. (c) Reverse pass calculating
derivatives.

46

(a) two-layer NN with ReLUs and MSE loss(b) forward pass for (x,y) = ([1,1,0], 0) caching intermediate results(c) backward pass for (x,y) = ([1,1,0], 0) to compute gradients taking advantage of cached results from the forward passx1aa(2)ax2x3ς’(z) = ς(l)(z) = ς(z) = ReLU(z) 0,1,z ≤ 0z > 0x1= 1x2= 1x3= 0aa(2) = 0.4= 1= 0a0.40.6-1-0.50.40.4-0.7-0.2l (NN(x),y) = (a(2) - y)2  = 0.16x1= 1x2= 1x3= 0a(2) = 0.4eq. 2.65eq. 2.69eq. 2.67eq. 2.66= 1= 0Δ(2) = 2 (a(2) -y) ς’ (z(2) ) == 2 · 0.4 · 1 = 0.8 x1= 1x3= 0a(2) = 0.4= 1= 0x2= 11,2l l Basics of machine learning

The forward-propagation algorithm also requires knowledge of the activations and deriva-
In this case, however, one does not need to cache any of
tives of the activation functions.
Instead, the computation of the derivatives can be carried out in parallel with
the results.
the forward pass (i.e., evaluation of the NN). This is because information ﬂows forwards
from layer-to-layer and no information from the earlier layers is explicitly required at later
stages. Besides this difference in memory cost, one can identify two key distinctions between
an algorithm based on forward propagation and the backpropagation algorithm. First, in for-
ward propagation algorithms, lots of redundant computations are performed given that the
derivatives in later layers have to be computed each time. This redundancy is not present in
backpropagation. Secondly, forward propagation involves unnecessary intermediate calcula-
tions where the derivatives of individual nodes are computed. Ultimately, this culminates in
the fact that the number of passes through the NN in the forward-propagation algorithm scales
with the number of tunable weights, whereas this is not the case in the backpropagation al-
gorithm. This is why backpropagation is generally preferred over forward-propagation-based
algorithms for computing gradients in NNs, in particular DNNs featuring a large number of
tunable parameters. As such, backpropagation has played a key role in the success of DL and
enabled the widespread application of NNs. For a more general in-depth discussion of these
concepts, including implementation details, see section 7.1.

Further reading

1. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer “Information

Science and Statistics” series. The standard book about standard ML [84].

2. Goodfellow, I., Bengio, Y. & Courville, A. (2016). Deep Learning. An MIT Press book.

One of the best textbooks on DL with explanation of all preliminaries [7].

3. Zhang, A. et al. (2021). Dive into Deep Learning. Interactive DL book with code, math,
and discussions. Implemented with NumPy/MXNet, PyTorch, and TensorFlow [85].

4. Mehta, P. et al. (2019). A high-bias, low-variance introduction to machine learning for
physicists. Phys. Rep. 810, 1-124. For the physicist-friendly introduction to ML [86].

5. Recordings of lectures on “ML for physicists” from 2020/21 and “Advanced ML for
physics, science, and artiﬁcial scientiﬁc discovery” from 2021/22 by Florian Marquardt.

6. Introductory ML course developed speciﬁcally with STEM students in mind: ML-
lectures.org and accompanying content: Neupert, T. et al. (2021). Introduction to ma-
chine learning for the sciences. arXiv:2102.04883 [87].

7. Carrasquilla, J. & Torlai, G. (2021). How to use neural networks to investigate quan-
tum many-body physics. PRX Quantum 2, 040201. Tutorial on ML for selected physical
problems with code [88].

47

Phase classiﬁcation

3 Phase classiﬁcation

One of the ﬁelds in physics where machine learning (ML), in particular neural networks (NNs),
could be particularly useful is condensed-matter physics [30], which revolves around the study
of the collective behavior of interacting particles. The difﬁculties associated with describing
such systems arise due to the rapid growth of the number of degrees of freedom as the particle
number grows leading to a large conﬁguration space. The “standard” approach to circumvent
these challenges is to ﬁnd suitable order parameters – quantities which represent the important
“macroscopic” degrees of freedom in a system without keeping track of all the microscopic
details. The order as quantiﬁed by these order parameters naturally separates matter into
different states, i.e., phases [89, 90]. For some systems, the order parameter is quite simple:
in ferromagnets, for example, the order parameter simply corresponds to the magnetization
which is given by a sum of local magnetic moments. In general, however, the identiﬁcation
of order parameters and the classiﬁcation of matter into distinct phases are difﬁcult tasks.
Topological phases of matter, for example, are characterized by topological properties which
are intrinsically non-local. The identiﬁcation of order parameters represents a crucial ﬁrst
step toward understanding the physics that underlies a many-body system and identifying
an appropriate order parameter for novel phases of matter typically requires lots of physical
intuition and educated guessing.

On the other hand, in ﬁelds such as computer vision, it has been demonstrated that NNs
can be trained to correctly classify intricate sets of labeled data naturally living in high dimen-
sions (see MNIST [21] or CIFAR [23]). This motivates us to explore ML techniques as a novel
tool to probe the enormous state space of relevant many-body systems that are currently in-
tractable with other algorithms [31]. Among all potential applications of ML to condensed-
matter physics, learning phases from (simulated or experimental) data is a particularly in-
triguing one: It could allow us to discover new phases and new physics without prior human
knowledge or supervision. In what follows, we aim to give the reader a ﬁrst introduction to
the ﬁeld of phase classiﬁcation using ML.

3.1 Prototypical physical systems for the study of phases of matter

In the following, we brieﬂy describe the two prototypical physical systems for which we demon-
strate the task of phase classiﬁcation in the next sections: the Ising model [91], which exhibits
a symmetry-breaking phase transition and can be characterized by a simple local order param-
eter, as well as the Ising gauge theory (IGT) [92], which shows a topological phase without
a local order parameter.1

3.1.1 Ising model

We consider the two-dimensional square-lattice ferromagnetic Ising model, which is one of the
simplest classical statistical models to show a phase transition and serves as a simple descrip-
tion of ferromagnetism. Ferromagnetism arises when a collection of spins aligns, yielding a net
magnetic moment that is macroscopic in size. In the Ising model , for each lattice site k there
∈ {+1, −1} leading to a state space of size 2N given
is a discrete (classical) spin variable σ
N lattice sites. The energy of a spin conﬁguration is speciﬁed by the following Hamiltonian

k

H(σ) = −J

σ

σ

i

j,

(cid:88)

〈i, j〉

(3.1)

1In the Landau paradigm of phase transitions [93, 94], changes between phases of matter are fundamentally
connected to changes in the underlying symmetries. Interestingly, Landau’s symmetry-breaking theory of phase
transitions breaks down for topological phases of matter [95].

48

Phase classiﬁcation

= 1 at various temperatures, where Tc

Figure 3.1: (a) Example spin conﬁguration samples of the Ising model with J = 1
≈ 2.27 (see Eq. (3.5)). Here, the blue
and kB
(orange) colored dots on each lattice site denote the value of the spin variable at
= −1). Panel reproduced from [32, Notebook A1]. (b) Mean
that site σ
= 1 (σ
magnetization per site 〈m(σ)〉
T of the Ising model as a function of the temperature
T .

k

k

where the sum runs over nearest-neighboring sites (with periodic boundary conditions) and
J is the interaction strength J > 0 (ferromagnetic interaction).2 Let us assume that the system
is at equilibrium at an inverse temperature β = 1/kBT , where kB is the Boltzmann constant
and T the temperature. Then, the probability of ﬁnding the system in a state with a spin
conﬁguration σ is described by the Boltzmann distribution

PT

(σ) = e−β H(σ)

ZT

.

(3.2)

= (cid:80)

σ e−β H(σ) is the partition function, where the sum runs over all possible spin
Here ZT
conﬁgurations. Example spin conﬁgurations of the Ising model at various temperatures are
shown in Fig. 3.1(a). Using Eq. (3.2) the expectation value of a given observable O(σ) can be
expressed as

〈O(σ)〉

T

(σ)O(σ).

PT

= (cid:88)
σ

For example, the observable corresponding to the magnetization per site is given by

m(σ) = 1
N

(cid:88)

σ

i.

i

(3.3)

(3.4)

In 1944, Onsager [91] obtained the following analytical expression for the critical temper-

ature

=

Tc

2J
kB ln(cid:0)1 +
at which a phase transition between a high-temperature paramagnetic (disordered) phase and
a low-temperature ferromagnetic (ordered) phase occurs, see Fig. 3.1. For temperatures below
the critical temperature Tc spontaneous magnetization occurs, i.e., the interaction is sufﬁ-
ciently strong to cause neighboring spins to spontaneously align leading to a non-zero mean
magnetization. At temperatures above Tc thermal ﬂuctuations completely dominate over any

(cid:112)
2(cid:1) ,

(3.5)

2The case J < 0 corresponds to the two-dimensional square-lattice antiferromagnetic Ising model which exhibits

a phase transition at the same critical temperature.

49

Phase classiﬁcation

Figure 3.2: The upper panel show example spin conﬁguration samples of IGT at
T = 0 (left panels) and T → ∞ (right panels) where local constraints are satisﬁed
for all (some) plaquettes, respectively. An exemplary plaquette is marked in red. The
lower panels show the corresponding dual representation, where the stars highlight
loop breakage. Reproduced with [32, Notebook A1].

alignment of spins and a zero magnetization is observed. As such, the magnetization serves as
an order parameter, which is zero within the disordered (paramagnetic phase) and approaches
one in the ordered (ferromagnetic phase), see Fig. 3.1(b).

3.1.2 Ising gauge theory

One of the most exciting research areas is the classiﬁcation of phases that do not have a lo-
cal order parameter, but rather a global one. Examples of systems that exhibit such phases
are band topological insulators and topological superconductors [96]. Detecting topological
phases is a challenging task from the experimental point of view because, in general, experi-
mentalists have access only to local observables. In this context, ML techniques can be of great
help [97–103].

The Ising gauge theory (IGT) [92] is the prototypical example of a system which exhibits
a topological phase of matter. Like the Ising model, the IGT is also a classical spin model
(σ
∈ {+1, −1}) deﬁned on a square lattice (with periodic boundary conditions). Here, how-
ever, the spins are placed on the lattice bonds. It is described by the following Hamiltonian

k

H(σ) = −J

(cid:88)

(cid:89)

σ

i,

p

i∈p

(3.6)

where p refers to plaquettes on the lattice, see Fig. 3.2. The ground state of this Hamiltonian
is a highly degenerate manifold spanned by all states that meet the local constraint that the

50

ෑ𝑖∈𝑝𝜎𝑖𝑧=+1ෑ𝑖∈𝑝𝜎𝑖𝑧=±1ෑ𝑖∈𝑝𝜎𝑖𝑧=+1ෑ𝑖∈𝑝𝜎𝑖𝑧=±1Phase classiﬁcation

product of spins along each plaquette is (cid:81)
= 1. As such, this ground state corresponds
to a topological phase of matter. In systems of ﬁnite size, the violations of the local constraints
are strongly suppressed, and the system exhibits a slow transition from the low-temperature
topological phase to the high-temperature phase with violated constraints. This allows for the
deﬁnition of a crossover temperature Tc deﬁned by the ﬁrst appearance of a violated local
constraint.3

i∈p

σ

i

There exists an interesting representation that highlights the topological character of the
ground state of the IGT: connect the edges of the lattice that contain spins with the same
orientation and form loops. The ground-state phase is then characterized by the property that
all these loops are closed; the violation of a constraint results in the appearance of an open
loop, see Fig. 3.2. Looking at typical spin conﬁguration samples of the IGT makes clear that its
phases are hard to distinguish visually without prior knowledge of the local constraints or the
corresponding dual representation. As such, IGT and other systems characterized by non-local
and long-range correlations pose a hard problem for any phase classiﬁcation algorithm.

3.2 Unsupervised phase classiﬁcation without neural networks

Having introduced the Ising model and the IGT, let us discuss how we can classify their re-
spective phases of matter. In particular, we are concerned with unsupervised ML algorithms.
They work with training data that do not need to be labeled (see section 1.5). Unsupervised
learning algorithms must by itself discover the relevant patterns in a training data set. As such,
these algorithms represent a primary candidate for the autonomous discovery of new phases
as they do not require prior labelling of the samples by the phase they belong to.

In particular, we discuss algorithms that perform a dimensionality reduction. In dimen-
sionality reduction, we are concerned with projecting the input data into a lower-dimensional
space. While any dimensionality reduction necessarily leads to an information loss, one aims
to discard only information in the input data that is less relevant to the problem at hand.
In particular, it is believed that real-world data often resides on a low-dimensional manifold
within the original space [104]. For example, one expects that the set of images one would
like to classify constitutes a small subspace of all possible images. In this case, the data can
be effectively described by fewer degrees of freedom. Clearly, such an approach lends itself
naturally to distinguish between different phases of matter and detect phase transitions in con-
densed matter systems: we want to discard the information-rich but complicated microscopic
description of the system for the sake of a simpler macroscopic description, e.g., in the form
of an order parameter.

Once we have performed the dimensionality reduction, we may already learn a lot about
the given problem by visualising the data within the low-dimensional representation space.
We tend to think that samples from the same phase of matter should be more similar to each
other than to samples from another phase. If the dimension reduction technique preserves
some of this similarity, we expect this to reveal itself in the data visualization. However, this
is not guaranteed to work in general. We see an example of such a failure in the following.

Going beyond visualization, we can process the data further, e.g., using clustering meth-
ods. Clustering is one of the most fundamental unsupervised learning methods used to group
unlabeled data into clusters of similar data points, where the similarity is assessed by a dis-
tance measure. In our context, the clusters would ideally correspond to the different phases of
matter present in the data. There exist many different clustering algorithms suited for differ-

3Note that as we increase the system size Tc

→ 0, i.e., the crossover temperature vanishes in the thermodynamic

limit. As such, the IGT does not exhibit a phase transition at a non-zero temperature.

51

Phase classiﬁcation

ent types of data, with k-means clustering being one of the simplest (see Ref. [86] for further
details).

Clustering can, in principle, be performed without dimensionality reduction as a pre-
processing step. However, dimensionality reduction may help in several aspects [86, 105].
Firstly, clustering typically relies on the Euclidean distance being a good measure of similarity.4
The distance between two data points in the original high-dimensional representation may,
however, not be particularly relevant as it is believed to often reside on a non-euclidean mani-
fold. Dimensionality reduction techniques can allow for the identiﬁcation of a low-dimensional
Euclidean representation of the data. The Euclidean distance between data points within this
representation is often physically more meaningful, resulting in a better clustering. Secondly,
performing dimensionality reduction as a pre-processing step helps to alleviate the problems
of the curse of dimensionality experienced when clustering data in high-dimensional spaces.
Finally, identifying a low-dimensional representation also helps to better visualize and under-
stand the clustering that is eventually obtained.

3.2.1 Principal component analysis

As an example, we consider principal component analysis (PCA) which is a common method
to perform dimensionality reduction. PCA identiﬁes mutually orthogonal directions, called
principal components (PCs), in the data space along which the linear correlation in the data
vanishes. We rank each PC based on the variance of the data along the corresponding direction.
To reduce the dimensionality of our space, we discard the PCs along which the data shows the
least variance. As such, in PCA directions along which the data exhibits a large variance are
considered to contain the most important information. In our case, ideally the data (raw spin
conﬁguration samples) naturally splits into different clusters corresponding to the individual
phases of the system when displayed in their low-dimensional representation.

(cid:80)n

i=1 xi

To be more precise, we consider the case where we are given n data points {x1, x2, ..., xn
}
each living in a m-dimensional feature space Rm with zero mean ¯x = 1
= 0. Note
n
that real-life data typically does not have zero mean.
In this case, the data ﬁrst needs to
be transformed by subtracting the mean element-wise. We deﬁne the n × m design matrix
]T. The symmetric m × m empirical covariance matrix is then given as
X = [x1, x2, ..., xn
n X (cid:252)X. Here, the i-th diagonal entry of the covariance matrix Σ
Σ = 1
ii corresponds to the
variance of the i-th feature over the entire data and the off-diagonal entries Σ
i j correspond to
the covariance between feature i and feature j. The basis in which the correlations between
features vanish corresponds to the eigenbasis of Σ in which Σ appears diagonal. Consequently,
the problem of ﬁnding directions along which the linear correlation in the data vanishes re-
duces to diagonalizing Σ, i.e., ﬁnding its eigenvectors (or PCs) {v1, v2, ..., vm
} and eigenvalues
{λ
i corresponds to the variance of the data along the di-
/ (cid:80)m
rection given by vi. We denote ˜λ
i as the ratio of explained variance contained
in the j-th PC. We refer to the appendix for a mathematical derivation of the procedure. Di-
mensionality reduction is then performed by selecting the ﬁrst k PCs with the largest ratios of
explained variance ˜λ, and projecting the data into this space of reduced dimensionality. The
projection is performed by the linear transformation ˜X = X ˜V , where ˜V = [v1, v2, ..., vk
]
and ˜X is the projected design matrix. Note that one has to choose k, the number of PCs to
keep. This can be done in an ad-hoc fashion that may be problem speciﬁc or, e.g., by choosing
the minimal number of PCs such that (cid:80)k
thresh is the desired threshold
explained variance ratio. The procedure is summarized in Algorithm 2. For an intuitive under-

}. Here, the eigenvalue λ
= λ

thresh, where ˜λ

2, ..., λ

1, λ

≥ ˜λ

i=1

i=1

˜λ

λ

m

i

j

j

4In general, whether clustering succeeds or not depends on whether the choice of distance measure (be it

Euclidean or not) is a good measure of similarity.

52

Phase classiﬁcation

Figure 3.3: (a) Illustration of the principle behind PCA applied to data points (blue)
living in a two-dimensional feature space. Orange vectors denote the ﬁrst two princi-
pal components on which we project the data (right panel). PCA applied to the spin
conﬁguration samples of (b) the Ising model and (c) IGT with k = 2. For the Ising
model, the data consists of 50 spin conﬁgurations (linear lattice size L = 30) sampled
= 3.5 in
using Monte Carlo methods at temperatures T ranging from T1
equidistant steps. For the IGT, the data consists of 1000 spin conﬁgurations (linear
lattice size L = 16) drawn within the topological phase and the disordered phase at
high temperature. Panels (b) and (c) reproduced from [32, Notebook A1].

= 1 to T20

standing of the procedure, we refer to Fig. 3.3(a) & (b): in this example, the data resides in
a two-dimensional feature space. After subtracting the data mean, PCA identiﬁes the ﬁrst PC
that contains the largest proportion of the data variance. PCA can not only be understood as
variance maximization, but also as a minimization of a reconstruction error of a linear trans-
formation. The prove of this equivalence can be found in Appendix A. For further details, see,
e.g., Ref. [86].

Now, we can readily apply PCA to our spin conﬁguration samples. Figure 3.3(b,c) shows
the results of PCA applied to spin conﬁguration samples of the Ising model and IGT, respec-
tively. For the Ising model, PCA separates the data into three clusters – a high temperature
cluster corresponding to the disordered phase, as well as two low temperature clusters corre-
sponding to the ordered phase with either positive or negative magnetization. Further analysis
shows that the ﬁrst PC corresponds to the magnetization [105]. By drawing a vertical deci-
sion boundary (perpendicular to PC1) which separates the high temperature cluster and a low
temperature cluster a rough estimate of the critical transition temperature can be obtained as
≈ 2.3 which is in agreement with the Onsager solution. In case of the IGT, PCA fails
Tc,PCA

53

Phase classiﬁcation

Algorithm 2 Principal component analysis (PCA)

Require: Hyperparameter k (dimensionality of the projected data)
Require: Design matrix X ∈ Rn×m

X ← X − mean(X)
Σ ← X (cid:252)X/n
V ← Eigenvectors(Σ)
˜V ← V [:, : k]

(cid:46) Remove mean element-wise
(cid:46) Construct empirical covariance matrix
(cid:46) Find eigenvectors and sort them descendingly by eigenvalue
(cid:46) Keep only ﬁrst k eigenvectors return ˜X ← X ˜V ∈ Rn×k

to cluster the data into the two prevalent phases [see Fig. 3.3(c)]. This is because PCA is re-
stricted to linear transformations of the input data. While this is sufﬁcient to encode simple
local order parameters [105–107], such as the magnetization in case of the Ising model, linear
transformations are not sufﬁcient to compute topological features, i.e., non-local correlations
in the data [107].

As illustrated by the failure of PCA in the case of the IGT, the restriction of PCA to linear
transformations of the input space severely limits its performance. That is, one may not be able
to ﬁnd the optimal set of directions to perform dimensionality reduction using PCA. In partic-
ular, the low-dimensional manifold on which the data resides within the original space may
not necessarily be parametrized by linear transformations of the original coordinates. In such
cases, a dimensionality reduction using PCA does not preserve the relative pairwise distance,
or similarity, between data points with respect to the manifold. However, this is a desired
property for any algorithm that aims at performing dimensionality reduction. This problem is
tackled by nonlinear dimensionality reduction techniques, such as the kernel PCA (kPCA) (see
chapter 4 on the kernel trick), the t-distributed stochastic neighbour embedding (t-SNE) [108],
or uniform manifold approximation and projection (UMAP) [109]. In the following section,
we brieﬂy describe t-SNE.

3.2.2 t-Distributed stochastic neighbor embedding

Stochastic neighbor embedding [110] and its variant called t-distributed stochastic neighbour
embedding (t-SNE) [108] are techniques for nonlinear dimensionality reduction, which aim
to preserve the local structure of the original data. That is, points which are close in the high-
dimensional data set tend to be close to one another in the low-dimensional representation.

Let us consider an initial m-dimensional space with n points, i.e., xi

∈ Rm. We deﬁne the
conditional probability pi| j that two points xi and x j are similar (i.e., close to one another) as

=

pi| j

(cid:80)

e−||xi
−x j
k(cid:54)=l e−||xk

||2/2σ2
i

−xl

||2/2σ2
i

,

(3.7)

−x j

where ||xi
|| is the Euclidean distance between the two points. The fact that Gaussian likeli-
hoods are used in pi| j implies that only points near xi contribute signiﬁcantly to its probability.
The variance σ2

i depends on the perplexity deﬁned as

= 2

Pi

− (cid:80)n

j=1 p j|i log2 p j|i ,

(3.8)

i are optimized for each point xi to have a ﬁxed perplexity value Pi

which is a measure based on Shannon entropy. In the ﬁrst step of the t-SNE algorithm, the
= const. Points
variances σ2
in regions of high density have smaller variance, while regions of low density have larger
(cid:54)= p j|i
variance.
due to the dependence on σ2
= p j|i, we deﬁne the joint

In practice, the perplexity usually is set between 5 and 50. Note that pi| j
i . To recover a symmetric relation pi| j

54

probability distribution as

=

pi j

pi| j

+ p j|i
2n

.

Phase classiﬁcation

(3.9)

The objective of the t-SNE algorithm is to ﬁnd another set of points in lower dimensional
< m and corresponding probability distribution qi j in a new

∈ Rn×dred, dred

representation yi
representation for which the KL divergence

(p||q) = (cid:88)

DK L

pi j log

i, j

pi j
qi j

(3.10)

is minimal.

The procedure starts with randomly sampling n points yi

) in a dred-
dimensional space. For each point we deﬁne the probability distribution qi j in a similar way
as in the high-dimensional space but using the t-Student probability distribution instead of
Gaussian distributions:

(z1, z2, . . . , zdred

=

qi j

(cid:80)

(1 + ||yi

− y j

(1 + ||yk

k(cid:54)=l

||2)−1
− yl

||2)−1

.

(3.11)

In the last step, we minimize the Kullback-Leibler (KL) divergence from Eq. (3.10) (see sec-
tion 2.3) by optimizing the position (z1, z2, ..., zdred
) in the
dred-dimensional space which eventually yields a low-dimensional data representation. The
t-SNE algorithm is summarized in Algorithm 3.

) of each point yi

(z1, z2, ..., zdred

= yi

The low-dimensional data representation preserves the local structure of the original data
set, i.e., similar points in original data set are now clustered in the dred-dimensional repre-
sentation space. However, the distance between the resulting clusters looses its meaning in
representation space.

Algorithm 3 t-distributed stochastic neighbour embedding (t-SNE)

Require: Hyperparameters: d (dimensionality of the projected data), perplexity P, learning

rate η

Require: Original data set of n points in m dimensional space X ∈ Rn×m
Require: Random set of n points in lower dimensional dred

< m representation Y (0) ∈ Rn×dred

for each xi do

for each x j do

Calculate pairwise conditional probability distribution pi| j with ﬁxed perplexity P

end for

end for
Calculate probability distribution pi j
for t = 1 to T do
(t−1)
for each y
i

∈ Y (t−1) do
(t−1)
j

for each y

∈ Y (t−1) do
Calculate probability distribution qi j

y

(t−1)
i

end for
Calculate gradients of the KL divergence with respect to coordinates of each
(i)
(z
1 , z
end for
Y (t) ← Y (t−1) − η ∂ DK L
∂ z

(cid:46) Update the coordinates of each point

) ∈ Y (t−1), i.e.,

(i)
2 , . . . , z

(i)
dred

∂ DK L

(p||q)

(p||q)

∂ z(i)

end for

55

Phase classiﬁcation

In general, clustering in combination with dimensionality reduction works elegantly
for simple problems, such as the Ising model. However, such approaches typically do
not perform well when applied to more difﬁcult phase classiﬁcation tasks, e.g., in the
presence of topological phases such as in the IGT [111], or when a large number of
phases is present [112].

3.3 Supervised phase classiﬁcation with neural networks

One may wonder whether the issues encountered by clustering methods introduced in the
previous section can be tackled by making use of the powerful machinery of NNs introduced
in section 2.4.4. The idea is the following [113]. We train an NN to take spin conﬁguration
samples as input and correctly label them by the phase they belong to, see Fig. 3.4(a). Typically,
the label is encoded as a binary bit string in a one-hot encoding. In case of the Ising model this
would correspond to the label 1 for all samples drawn within the ordered phase (T < Tc) or the
label 0 for all samples drawn within the disordered phase (T > Tc).5 To ensure that the output
of the NN can be used to predict a binary label, we choose the output layer to be composed of
two nodes to which we apply the softmax activation function introduced in Eq. (2.37) over the
activations x j of all nodes within the output layer. This ensures that the output layer encodes
a valid probability distribution over the classes. The predicted label is then typically chosen
based on the node which yields the maximum probability. For training, one typically employs
the binary cross-entropy (see Eq. (2.2)) which, for a ﬁxed input x is given as
L = −(cid:88)

(x) log(cid:0)NN(x)

(3.12)

(cid:1).

p j

j

j

Here, NN(x) denotes the output of an NN, which contains a softmax activation function in
its last layer, applied to the input x. The sum runs over all output nodes, i.e., the number of
(x) is the true label of the input x as speciﬁed by the one-hot encoding.
distinct classes. p j
(x) = 1 and
For example, given two classes and an input whose true label is 0, we have p0
(x) = 1. In Eq. (3.12), this is compared to NN(x)
p1
j which is the
activation of the j-th output node and corresponds to the predicted probability of the input x
to belong to class j.

(x) = 0 such that (cid:80)

j p j

In our example, the training set consists of labeled spin conﬁguration samples for a wide
range of temperatures far above and below Tc, whereas the test set is chosen over the entire
temperature range. After training the NN (see section 2.4.4) on the training set, it is evalu-
ated on the test set. In particular, we average the activation of the two nodes in the output
layer, which encode the probability of the input sample belonging to phase 0 or 1, respectively,
over the test set. Remarkably, Fig. 3.4(b) shows that these activations cross over precisely at
Tc enabling us to extract the correct critical temperature. Similarly, this method is capable
of correctly identifying the crossover temperature in the IGT [113]. The fact that NNs can
generalize to unseen input data can, for example, be exploited as follows. An NN trained on
conﬁgurations for the square-lattice ferromagnetic Ising model can also highlight the critical
temperature of Ising model with a different lattice geometry, such as a triangular lattice [113].
Note that the ferromagnetic Ising model on a triangular lattice is an typical example of a frus-
trated system.

5Of course, the opposite choice for labelling the two phases with label 0 for the ordered phase and 1 for the

disordered phase is equally good.

56

Phase classiﬁcation

Figure 3.4: Supervised phase classiﬁcation with an NN performed on the Ising model
of varying linear lattice size L (N = L2) (see [113] for further details). (a) (Convo-
lutional) NN applied to a conﬁguration sample of the Ising model. (b) The average
activation of the two nodes in the output layer are given by blue and red curves, re-
spectively. The predicted critical temperature is marked by their crossover and is in
good agreement with the Onsager solution [Eq. (3.5)] depicted as an orange vertical
line. Adapted from Ref. [113].

3.4 Unsupervised phase classiﬁcation with neural networks

In section 3.3, we showed that NNs can perform supervised phase classiﬁcation. Due to its
supervised nature this approach requires partial knowledge of the phase diagram of the system.
One can determine the critical temperature (through “interpolation”) if one knows the labels
of samples deep within two neighboring phases. Ideally, in order to discover new phases of
matter a phase classiﬁcation algorithm should not rely on such a priori knowledge about the
phases, i.e, it should be unsupervised in that regard. While clustering is unsupervised, we have
seen that its power can be limited. In the following, we discuss four methods that use NNs to
perform unsupervised phase classiﬁcation.

3.4.1 Learning with autoencoders

A natural NN-based unsupervised method is based on the analysis of the latent data repre-
sentation given by an autoencoder (AE). As we have brieﬂy explained in section 2.4.5, AEs
are NNs with a bottleneck in their center which are trained to reconstruct the input at the
output. The architecture of a typical AE is depicted in Fig. 3.5. Due to the bottleneck, the
information passing through the network needs to get compressed at the bottleneck, and then
decompressed to recover the input. As a consequence of the compression, some information
may be lost.6 However, the retained information in the bottleneck should ideally contain ev-
erything relevant for the reconstruction of the input. Therefore, the bottleneck forms a latent
space which contains a compressed representation of the input data. This is akin to the di-
mensionality reduction schemes we discussed previously (see section 3.2), which preserve the
most important features for the reconstruction. As such, we can analyze the latent represen-
tation of the input data in a similar way as the lower-dimensional representation obtained by
PCA in section 3.2.1.7

Let us apply an AE to reconstruct Monte Carlo samples of the two-dimensional Ising
model [106]. Clearly, this represents an unsupervised phase classiﬁcation scheme, because

6In general, it is possible that NNs could compress more dimensions into a single neuron. However, in practice

NNs tend to learn smooth functions, which penalizes this behavior.

7The quantum versions of AEs are also being developed and applied to phase classiﬁcation [114] and clustering

of subspaces of the Hilbert space [115]. For more details, see section 8.2.7.

57

Phase classiﬁcation

Figure 3.5: (a) Illustration of a natural bottleneck (here two neurons) in an AE ar-
chitecture. (b) Analysis of bottleneck neurons of an AE trained to reconstruct spin
conﬁgurations of a two-dimensional Ising model. Latent representation of Ising con-
ﬁgurations clusters into two phases visible as a histogram. (c) Anomaly detection
scheme allows for the recovery of the phase diagram from the reconstruction loss
of an AE trained on one phase (blue box in bottom left). Panel (b) is taken from
Ref. [106], (c) from Ref. [116].

we do not provide any labels. The relevant loss function to be minimized is given by the
reconstruction error between the input and output spin conﬁgurations (e.g., mean-squared
error (MSE)). If we look at how the latent representation of the spin conﬁgurations in the
trained AE change with the temperature (see Fig. 3.5[b]), we can immediately observe a clus-
tering of the latent parameters. The clusters correspond to the two phases of the Ising model.8
Red points correspond to the high-temperature paramagnetic phase, while yellow points cor-
respond to the low-temperature ferromagnetic phase. Note the two large yellow bins at the
edges of the histogram in Fig. 3.5(b). These are formed due to the degeneracy of the ground
state, which has either all spins pointing up or all spins pointing down.

Analysis of the AE latent representation of the input data is not the only way of an AE-based
unsupervised phase classiﬁcation. Another successful and robust scheme based on anomaly

8Beware, clustering of data in the latent space according to the phases present in the system is not a general
property of AEs. The clustering occurs when input data causes distinctive activations in the bottleneck which often
corresponds to different phases.

58

Phase classiﬁcation

detection9 was presented in Ref. [116]. The basic idea is as follows. Imagine training an AE
to reconstruct states coming from one phase. Then, the AE is used to reconstruct states com-
ing from the rest of the phase diagram. Such a task is difﬁcult, because the training data is
limited only to one phase, and the AE is bound to make reconstruction errors in other phases.
Moreover, we expect that the error is lower for phases that are similar to the “training” phase
and higher for phases which contain states that look very different. Finally, the quantum states
from the transition regimes are usually distinctive and the most unique from the rest of the
phase diagram. Altogether, the reconstruction error across the phase diagram, made by an AE
trained to reproduce states from one phase, is expected to vary according to the phases and
the phase boundaries in the system. This scheme enables the discovery of phases in a fully
unsupervised way. The authors of Ref. [116] used this scheme based on anomaly detection to
recover a full phase diagram of the extended Bose-Hubbard model in one dimension at exact
integer ﬁlling. This result is presented in panel (c) of Fig. 3.5. Interestingly, their work also re-
vealed within the phase diagram a phase-separated region10 with unexpected properties which
may be one of the ﬁrst fully unsupervised discoveries in the ML-guided phase classiﬁcation.

3.4.2 Learning by confusion

Learning by confusion [118] is another NN-based unsupervised method and works as follows.
We start by partitioning the temperature range into two regions with distinct labels. Based on
these labels, we perform supervised learning over the entire temperature range as described
in section 3.3 and keep track of the ﬁnal overall classiﬁcation accuracy of the model. This
classiﬁcation accuracy is associated with the guess for the critical temperature located at the
boundary of the two regions. We repeat this procedure systematically for multiple bi-partitions
of the temperature range, i.e., guesses for the critical temperature. Finally, we plot the clas-
siﬁcation accuracy against the guessed critical temperature. This procedure is summarized in
Algorithm 4. Note that each partitioning requires the training of a separate NN.11 The re-
sults of this algorithm applied to the Ising model are depicted in Fig. 3.6. We observe that the
classiﬁcation accuracy is W-shaped. The high classiﬁcation accuracy at the end points of the
temperature range arises due to the fact that in these cases almost all samples are assigned the

9The AE-based anomaly detection scheme was also succesfully applied to quantum dynamics problems [117].
10This phase-separated region is located between supersolid and superﬂuid phases, for more details see

Ref. [116].

11Retraining a model for each choice of a bi-partition can become computationally expensive, in particular when
increasing the resolution of the method. There has been an extension of the learning-by-confusion scheme which
uses two neural networks [119] to try to circumvent this issue by choosing bi-partition points one at a time in
a guided manner.

Algorithm 4 Learning by confusion
Require: Data set of (spin conﬁguration) samples D0

= {x}, guesses for critical temperature

T = {T1, ..., Tmax
for T ∗
c

∈ T do

}

Partition data set D0 into two regions with T ≤ T ∗
Set label y of all samples in region with T ≤ T ∗
Split resulting data set into training and test set
Perform supervised learning on the training set, i.e., train an NN to minimize loss in

c and T > T ∗
c as 0 and T ≤ T ∗

c as 1

c

Eq. (3.12)

Evaluate classiﬁcation accuracy on test set

end for
Plot accuracy vs. T ∗
c
accuracy peaks corresponds to the best guess for the location of the phase transition

(cid:46) Critical temperature T ∗

∈ T (see Fig. 3.6)

∀T ∗
c

c at which the

59

Phase classiﬁcation

Figure 3.6: Result of the learning by confusion scheme applied to the Ising model.
The data consists of 100 spin conﬁgurations (linear lattice size L = 30) sampled
= 3.5 in
using Monte Carlo methods at temperatures T ranging from T1
equidistant steps. The data set is split into equally-sized training and test sets (such
that 50 spin conﬁgurations are present at each sampled temperature). The blue curve
shows the classiﬁcation accuracy on the test set for various choices of bi-partitions.
It has a characteristic W-shape whose middle peak is at T ≈ 2.3, which is in good
agreement with the Onsager solution. Reproduced with [32, Notebook A3].

= 1 to T20

same label. In particular, in the extreme case where all samples are assigned the same label
a classiﬁcation accuracy of 1 can be achieved trivially because the NN simply needs to learn to
output the same label independent of the input. The middle peak, however, is non-trivial and
corresponds to the predicted critical temperature of the method. Here, the predicted critical
temperature is in good agreement with the Onsager solution. The presence of this middle peak
can be explained as follows. Let us assume that the data can naturally be classiﬁed into two
distinct groups realized by a particular choice for the bi-partition of the temperature range.
Then, the closer our choice of bi-partition matches the “correct” bi-partition underlying the
data the larger the classiﬁcation accuracy of our algorithm.

Here, we have discussed the case where there are precisely two distinct phases present in
the parameter range under consideration. In this case, the accuracy ideally displays a charac-
teristic W-shape, see Fig. 3.6. If there are multiple phases present, this characteristic W-shape
is modiﬁed. The shape of the signal (in particular the number of obtained peaks) could then
be used to identify the number of different phases present in the data [118, 120].

3.4.3 Prediction-based method

The learning by confusion scheme is difﬁcult to efﬁciently extend to high dimensional pa-
rameter spaces which may feature several distinct phases.12 Moreover, it has been shown
that the learning by confusion scheme has difﬁculties to correctly identify the crossover in
the IGT [111]. These limitations can be circumvented through the so-called prediction-based
method [111, 112, 121], which works as follows.

We train an NN to predict the tuning parameter (here the temperature) for each conﬁg-
uration sample. The value of the tuning parameter at which a given conﬁguration samples
has been generated is readily available both in experiment and simulation. If the system does

12In Ref. [119] an approach to extend the scheme to two-dimensional parameter spaces featuring two distinct

phases is presented.

60

1.01.52.02.53.03.5Temperature T65707580859095100Accuracy (%)Tc=2.18Phase classiﬁcation

= 1 to T20

Figure 3.7: (a,b) Illustration of the output of the prediction-based method if the
system does not undergo any phase transition. (c,d) Result of the prediction-based
method applied to the Ising model. The data consists of 100 spin conﬁgurations
(linear lattice size L = 30) sampled using Monte Carlo methods at temperatures T
= 3.5 in equidistant steps. The data set is split into
ranging from T1
equally-sized training and test sets (such that 50 spin conﬁgurations are present at
each sampled temperature). (c) Average predicted temperature for the test data as
a function of the true underlying temperature. (d) Derivative of the average predicted
temperature for the test data as a function of the true underlying temperature which
peaks at the critical temperature of the Ising model. Panels (b) and (c) reproduced
from [32, Notebook A3].

not undergo any phase transition, the predicted tuning parameter is linearly dependent on the
true tuning parameter as shown in Fig. 3.7(a). Consequently, the derivative of the predicted
tuning parameter with respect to the true tuning parameter is constant, see Fig. 3.7(b). For
In this case, the tuning
systems which exhibit a phase transition the situation is different.
parameter cannot be predicted with perfect accuracy resulting in a nonlinear relationship be-
tween the predicted and the true value of the tuning parameter. Figure 3.7(c) illustrates this
in the case of the Ising model. Consequently, the derivative is not constant and at the critical
tuning parameter the largest rate of change occurs, see Fig. 3.7(c) and (d). In other words,
the parameter value for which the NN predictions are most susceptible identiﬁes the position
of the phase transition.

Let us elaborate on this point: While the tuning parameter only changes marginally in
the vicinity of the phase transition, the system’s state and its corresponding order parameter
change dramatically as the tuning parameter crosses its critical value. As a result, the NN is the
best at distinguishing samples originating from two different phases whereas it has difﬁculties
to distinguish samples from within the same phase. That is, its predictions change the most as
the tuning parameter is swept across its critical value. Figure 3.7(c) shows that the predictions
start to saturate deep within each phase, whereas they vary most strongly with the tuning

61

True parameter ptruePredicted parameter ppred(a)True parameter ptrueDerivative dppred/dptrue(b)1.01.52.02.53.03.5Label temperature Tlabel1.02.03.0Predicted temperature Tpred(c)1.01.52.02.53.03.5 / D E H O  W H P S H U D W X U H Tlabel-1.00.01.02.0 ' H U L Y D W L Y H dTpred/dTlabel(d)Tc=2.38parameter around the transition point.

Phase classiﬁcation

So far, we have seen that phase classiﬁcations methods based on NNs are capable of lo-
cating the phase transition of the two-dimensional Ising model. To date, these methods have
successfully revealed a plethora of other phase transitions in various physical systems.13 This
fact highlights that these methods are generic and have been formulated in a system-agnostic
fashion. There may exist various physical observables (such as order parameters) that can
be used to identify a given phase transition. However, ﬁnding these quantities is typically
a hard task and requires a deep understanding of the physical system at hand. Remarkably,
the NN-based methods we showcased here can successfully classify different phases of mat-
ter in an automated fashion without a priori knowledge of the underlying physics. Note that
there exist similar system-agnostic tools which do no rely on ML, such as the speciﬁc heat for
thermal phase transitions or the ﬁdelity susceptibility [128] for quantum phase transitions.14
However, these tools can still fail for a given system and can be expensive to compute or dif-
ﬁcult to measure in an experiment. For example, the speciﬁc heat fails to locate the crossover
temperature in the IGT. In case of the ﬁdelity susceptibility, one investigates the change in the
overlap 〈Ψ
(p)〉 is the ground state of the Hamiltonian H(p), p is
the tuning parameter, and ε is an inﬁnitesimal perturbation. Because one typically does not
have access to the full wave-function, the ﬁdelity susceptibility typically remains difﬁcult to
evaluate. The NN-based methods we discussed constitute alternative tools. In particular, they
can in principle be applied using various properties of the system’s state at different values
of the tuning parameter as input. This allows them to identify phase transitions based on
experimentally accessible measurement data [129].

(p + ε)〉, where |Ψ

(p)|Ψ

0

0

0

While the phase classiﬁcations methods we discussed up to now are capable of locating
phase transitions, we have not yet gained any insights into the speciﬁc type of the phase tran-
sition that the system undergoes. The crucial question is whether one can extract physical
insights from the NNs concerning the underlying phase classiﬁcation tasks. In particular, one
can ask whether it is possible to extract novel order parameters from such NNs, which is an ul-
timate goal of the interpretable ML applied to phase classiﬁcation problems.

3.5 Interpretability of machine learning models

As seen in the previous sections, NNs are powerful tools to identify phases in physical data.
Now imagine applying these methods to a novel physical system whose phases and correspond-
ing order parameters are not yet known. The natural questions that arise in this scenario are:
Can we trust the NN predictions? In particular, how can we know that the model correctly
located a phase transition in the parameter space? Moreover, assuming that the methods cor-
rectly classiﬁed the data into different phases of matter, how can we gain physical insights
into the problem at hand? For instance, can we analyze the trained NNs to determine what
types of phase transitions the system undergoes? Or would it even be possible to extract novel
order parameters from them? When using ML (and especially DL) models, answers to these
questions are not easy to ﬁnd. Such challenges are being addressed by the research on the ML

13There exist various other ML methods for detecting phase transitions and classifying phases of matter [112,
122–127]. For example, in Ref. [124] phase transitions can be inferred by training an ML model to ﬁt the prop-
erties within one phase and extrapolating toward other regions in parameter space. Here, the model is based on
a Gaussian process (GP) utilizing kernels. We discuss kernel methods, including GPs, in detail in chapter 4.

14A quantum phase transition [89] corresponds to non-analytic behavior of the ground-state properties at the
critical value of the tuning parameter pc, where the system Hamiltonian is H(p). It emerges due to the competition
of individual terms in the Hamiltonian, which dependent on the tuning parameter.

62

Phase classiﬁcation

reliability and interpretability.15

Reliability is about trusting our ML model predictions. Our trust in the model is in-
creased, e.g., when we have access to the uncertainty of model predictions.
Inter-
pretability is about understanding what an ML model learns and how it makes its pre-
dictions. As such, these two ideas are closely intertwined.

Both concepts are particularly important on our way toward scientiﬁc discovery using ML. If
we are not able to understand what an NN learns when given a problem, our understanding
of the problem remains limited!16

We already mentioned that, a priori, DL models are usually neither reliable nor inter-
pretable. As such, they largely serve as black-box models that provide us with suitable predic-
tions (from which we, e.g., can locate phase transitions in the underlying input data). There
are several reasons for that: ﬁrstly, their learning dynamics are largely opaque and not well
understood.17 Secondly, the direct analysis of trained NNs is challenging, as we explain in
the next section. In particular, the “reasoning” of NNs does not necessarily have to be based
on the same observations on which a human would base its decisions. Tackling these chal-
lenges is important for all ML applications, but especially crucial, e.g., for medical diagnosis
or insurance and hiring decisions.

3.5.1 Difﬁculty of interpreting parameters of a model

When looking at a DNN with possibly billions of trainable parameters, it is hard for us humans
to decipher what the NN is really doing under the hood. It may be that an NN actually computes
a simple, physically relevant function, such as an order parameter, to make its predictions.
Recognizing whether that is the case is hard because the computation and relevant information
is spread over the multiple layers containing a large number of neurons each. However, if
an NN is sufﬁciently small, the direct interpretation by looking at its trainable parameters may
be possible. Consider the limiting case of a single-layer NN without any nonlinear activation
function. This corresponds to a simple linear regression model, described in section 2.4.1:

ˆy = wx + b,

(3.13)

where w is a vector of weights and b is a vector of biases. Evidently, such a linear model
allows for a direct interpretation in terms of its weights: the larger the magnitude of a given
weight (connection), the more important is the corresponding normalized feature for solv-
ing the problem at hand. For an example of weight interpretation in the context of phase
classiﬁcation, see Ref. [132].

However, this reduction in depth and loss of nonlinearity comes at the cost of expressivity.
In order for such a model to be accurate, it generally requires highly pre-processed inputs x
which “contain” the necessary nonlinearities. Moreover, the importance of a given feature has
more meaning if it is already present in a compact, physically relevant form. This largely limits
the domain of applicability of small predictive models to problems for which we (at least) have
partial knowledge.

15Note, that the formal deﬁnitions of these terms are not agreed upon in either the physical or computer science

community [130]. To circumvent the problem, here we provide intuitions about meaning of these terms.

16We can imagine a non-interpretable black-box NN that after training can give insights to the problem, e.g.,
Ref [131]. However, the model still needs to be reliable so we can trust the new insights, and we need to have
previous deep insights into the problem.

17We show you how some of these questions can be answered with tools from statistical physics in section 8.1.

63

Phase classiﬁcation

3.5.2 Interpretability via bottlenecks

As we have explained in the previous section, interpretability is an inherent characteristic of
small models. Fortunately, there are alternative approaches to interpretability that are not lim-
ited to simple small models. What we can do in large architectures is to identify bottlenecks
in the information ﬂow and focus our attention there. A bottleneck in an NN is just a layer
with fewer neurons than the layer before and after it. An example of a NN with a natural bot-
tleneck has already appeared in section 3.4.1 and in Fig. 3.5(a), namely an autoencoder (AE).
Its bottleneck forces the NN to distill the relevant information within the inputs, such that it
can ﬂow through this constriction. As such, the NN performs a dimensionality reduction and
ﬁnds a suitable low-dimensional feature representation. While the entire NN architecture can
be large and have many trainable parameters, the bottleneck itself is described only by few
parameters. Because all the relevant information for the predictions of the NN must eventu-
ally ﬂow through the bottleneck, we can limit our analysis to the small number of trainable
parameters of the bottleneck as opposed to the entire NN. In particular, we can perform a re-
gression on the output of such bottleneck neurons and extract the mapping between the input
features and the activations of the bottleneck neurons. There is a natural bottleneck in al-
most every NN – its output neuron. However, performing a regression on it without imposing
any additional bottlenecks is challenging, because you need to take into account all input fea-
tures which can grow quickly in number. Apart from the output neuron, there are also other
types of bottlenecks which can appear naturally in NNs architectures, such as in AEs (see sec-
tion 2.4.5) [106, 133] and CNNs (see section 2.4.4) [134].18 However, we can also introduce
bottlenecks into our architecture on purpose to have more interpretable ML models. This idea
gave birth to, e.g., Siamese NNs [136]. We look at these approaches in more detail in the next
paragraphs and see what information on physical systems we can extract with them.

Interpretability with autoencoders (AEs). As we have explained in sections 2.4.5 and 3.4.1,
AEs are NNs with a bottleneck in the middle that are trained to reconstruct the input at the
output. We have already shown in section 3.4.1 and in Fig. 3.5(b) that we can obtain clustering
in the latent representation corresponding to phases present in the input data, achieving the
unsupervised phase classiﬁcation. Moreover, thanks to the bottleneck, which ideally should
contain all information that is relevant for the reconstruction, we can also extract additional
information and interpret what property of the input data is preserved by an AE. In particular,
if we plot the latent parameter against the magnetization of the respective two-dimensional
Ising spin conﬁguration, as in Fig. 3.8(a), we see a linear dependence. This suggests that the
compressed representation learned by the AE is connected to the magnetization. To be more
precise, the behavior deviates from a strict linear dependence at values of the magnetization
close to −1. However, we still can make the statement that the AE learned a property related
to the magnetization given that the mapping between the latent parameter and the magneti-
zation is bijective. For example, such a statement would hold even if the latent parameter as
a function of the magnetization would vary according to a sigmoid function.

i

= λ (cid:80)

18Alternatively, a “bottleneck” can be enforced through regularization – in particular the addition of a L1 regu-
|, where λ parametrizes the regularization strength and
|wi
larization term in the loss function given by L1
the sum runs over all (trainable) weights within the NN. This term enforces the weights to vanish, i.e., for connec-
tion to be cut. Ideally this results in a sparser, and thus effectively smaller, NN which enables interpretability. For
example, the authors of Ref. [135] could extract analytical expressions for force laws and dark matter distributions
from graph neural networks trained to predict planetary and dark matter dynamics. This was achieved by perform-
ing symbolic regression on the corresponding sparse networks. Note that such a regularization is also important
when interpreting linear models as in Eq. (3.13). Often learning problems do not have a unique solution. This
means that the weights can vary given the same data and optimization procedure, which would result in different
“interpretations” of the NN’s inner workings. The regularization terms helps to remove the remaining degrees of
freedom of the weights and enforce Occam’s razor.

64

Phase classiﬁcation

Figure 3.8: Analysis of bottleneck neurons of an AE trained to reconstruct spin con-
ﬁgurations of a two-dimensional Ising model. (a) Dependence of latent space pa-
rameter on the magnetization. Red (yellow) color corresponds to samples from the
low(high)-temperature regime. (b) Absolute magnetization, absolute rescaled values
of latent parameter and reconstruction loss, averaged for ﬁxed temperature. Adapted
from Ref. [106].

Another example of analysis of latent space of AEs is work by Iten et al. [133]. They used
a special AE architecture with a question neuron, i.e., an additional neuron connected to the
ﬁrst decoding layer after the bottleneck. The input of this question neuron is provided by
a user. You can think of it as an alternative way of providing data to the network. The authors
showed that you can train such a special AE in a way that a user can ask a question via the
question neuron and the answer is encoded in the latent space.

As you see, AEs are to some degree inherently interpretable by virtue of their low-
dimensional latent space. However, the analysis of latent space does not give us any hint
on the order parameter or important features. We can only compare it against the quantities
or features we suspect to be important. If you look for a more automated way of detecting
order parameters, we can turn to very special CNNs.

Extracting order parameters with convolutional neural networks (CNNs). We have al-
ready mentioned that CNNs have natural bottlenecks in their architectures. These bottlenecks
are their ﬁlters or kernels, i.e., the structures with which they “scan” the data. Their size can
be thought of as a receptive ﬁeld size and tells us how many neighboring features (e.g., pix-
els) the network can analyze at the same time. Of course, if you have multiple convolutional
layers with multiple kernels of different sizes, intertwined with pooling layers, their analysis
is still challenging. But if you consider a simple CNN with only one or few subsequent con-
volutional layers with kernels of a ﬁxed size and only one averaging layer at the end of the
architecture, as presented in Fig. 3.9(a), such a regression becomes tractable19. The men-
tioned architecture was proposed by Wetzel et al. (2017) [134] and is called Interpretation
Net or Correlation-Probing Neural Network, see Fig. 3.9(a). Such an architecture allows us to
perform a regression on the output neuron with features extracted by kernels. Eventually, we
obtain an analytical expression for the CNN decision function. If applied to a phase classiﬁca-
tion problem, such a decision function could unravel the order parameter. It seems, however,

19It remains non-trivial and involves careful zeroing of weights, Fourier series, and other tricks.

If you are

interested, see Ref. [134]

65

Phase classiﬁcation

Figure 3.9: (a) Correlation-probing NN where the size of the receptive ﬁeld is sys-
tematically reduced in each training step. Localization Network is one or more subse-
quent convolutional layers. Averaging layer collapses convolved features to a single
number. (b) Scheme of a Siamese NN, whose two subnets are identical and share the
same tunable parameters. The input is a pair of data points and the resulting label is
either “same” or “different”. Panel (a) is adapted from Ref. [134].

that such a decision function, and therefore the order parameter which may potentially be
discovered through the CNN, depends on the choice of kernel size. What is the appropriate
choice of kernel size, and thus decision function? Occam’s razor tells us that we should be
interested in the simplest decision function. That is, one should aim to take into account only
a small number of input features. Crucially, this also makes the task of symbolic regression
easier.

Therefore, the idea of the correlation-probing convolutional network is to systematically
reduce the size of the kernel (and thus the input dimension for the symbolic regression
task) by cutting connections until there is a signiﬁcant drop in the CNN performance.
This drop corresponds to the CNN becoming “blind” to the correlations which are crucial
for detecting and distinguishing different phases.

Imagine starting from a large kernel whose size corresponds to the size of the entire input
image, e.g., 28 × 28. We train our Interpretation Net with such 28 × 28 kernels and see that
it yields good results. Now, we reduce the receptive ﬁeld size, e.g., to 20 × 20, retrain, and
observe the performance. We repeat this process of reducing the kernel size and retraining
until we see a signiﬁcant drop in the CNN performance. Such a drop occurs as soon as the

66

Phase classiﬁcation

CNN gets blind to correlations in the system which are crucial for the phase classiﬁcation, e.g.,
nearest-neighbor correlations. Finally, we can perform a regression on the output neuron of
the CNN with the smallest kernel size that still yields good performance. The decision function
we recover in the process is ideally connected to the underlying order parameter. With this
approach, the Interpretation Net is capable of successfully classifying the phases of the two-
dimensional Ising model20 or SU(2) lattice gauge theory and one extract the corresponding
decision functions. We stress that the learned decision function can strongly depend on the choice
of model architecture and training procedure (data). Therefore, various networks can detect
different order parameters, e.g., in the Ising model, they can detect the expected energy per
site, magnetization, or a scaled combination of those.

A related approach was used by Miles et al. (2021) [137] when designing a so-called
Correlator Convolutional Neural Network. This CNN performs automatic feature engineering
by probing for correlations in the ﬁrst few layers. The subsequent layers are designed to
check which correlations are the most important for the classiﬁcation. By discovering which
many-body correlations were the most important for the classiﬁcation, they could distinguish
between two distinct quantum models for their experimental results. This represents one of
the ﬁrst examples of scientiﬁc discovery with NNs.

Interpretable Siamese neural networks. Finally, you can create even more complex ar-
chitectures with artiﬁcial bottlenecks allowing for an interpretation via symbolic regression.
An interesting example is a Siamese NN [136], presented in Fig. 3.9(b). It takes two input
data points at the same time and is composed of two twin subnetworks with the same param-
eters and architecture. Their output neurons form two bottlenecks which in turn are inputs
for the third subnetwork whose aim is to connect and compare the twin outputs. The task of
the network is to determine whether two input data points are similar or not.21 This means
that Siamese NNs are able to perform a multiple-class classiﬁcation without a ﬁxed number of
classes and with relatively little training data per class. Moreover, by analyzing the bottlenecks
we can extract what the NN learns in a problem.

Let us show an example. The authors of Ref. [136] applied the Siamese NN, e.g., to the
motion of a particle in a central potential. The task was to learn whether two observations of
the same particle correspond to the same particle trajectory. After successful training, we can
perform a polynomial regression on the bottleneck with respect to the input data features. In
this case, features are a position of the particle in the two-dimensional space and its velocity
in both directions. By analyzing the dominant regression terms, they observed that the result
of regression is proportional to the angular momentum of the particle. Such an analysis of
bottlenecks of the successfully trained Siamese NN indicates that it learns conserved quantities
and invariants.22 Similar results can be observed for problems from special relativity and
electromagnetism [136].

So far, we have learned that we can interpret ML models by analyzing bottlenecks in their
architecture. These bottlenecks can either appear naturally (like in autoencoders (AEs)) or
be imposed explicitly (like in CNNs, where the kernel size is systematically reduced, or in
Siamese NNs). Another approach is based on the analysis of the minimum of the training loss
function, reached by a model during the optimization. Because it is based on the minimum, this

20Interestingly, in their paper, the quantity which leads to the better CNN performance in case of the Ising model
j) which can still be detected with a 2×1 kernel, not the magnetization

is the expected energy per site (
(cid:80)
( 1
N
21A quantum version of Siamese NNs was developed in Ref. [138] that goes beyond the distance-based metric.
22Note that it is much easier to detect invariants which can be represented as polynomial function of input

i) which can still be detected with a 1 × 1 kernel.

〈i, j〉 σ

−J
N

(cid:80)

σ

σ

i

i

features.

67

Phase classiﬁcation

Figure 3.10: (a) Low-dimensionality visualization of a non-convex loss landscape of
a DNN called VGG-56 trained on CIFAR-10 [39]. (b) Hessian-based toolbox [144] for
increasing the interpretability and the reliability of a trained ML model. It is based
on the Hessian of the training loss at the minimum (or an approximation thereof).

approach is very general and independent of the particular choice of the ML model architecture
or the learning process.

3.5.3 Hessian-based interpretability

As described in section 2.1, ML models learn by minimizing a training loss function L describ-
ing the problem through the variation of their parameters θ. The training loss landscape of
deep NNs is, however, highly non-convex. This renders the optimization problem difﬁcult, e.g.,
due to the presence of many local minima [see Fig. 3.10(a)]. Moreover, these minima may not
have equally good generalization properties that can be connected to the curvature around
a minimum.23 An analysis of how the generalization ability depends on the local curvature is
an example for a case where the shape of the reached minimum can tell us something useful
about trained ML models. The shape or curvature around the minimum θ = θ∗ is described
by a Hessian matrix calculated at the minimum, i.e.:

Hθ∗,i j

=

∂ 2
θ

∂ θ

i

Ltrain

|θ=θ∗ .

j

(3.14)

The knowledge of the curvature around the minimum also allows us to approximate how
our ML model (and as a result, its predictions) would change upon some action. Possible ac-
tions could be the removal of a single training point or a slight modiﬁcation of θ∗ → ˜θ, resulting
in a shift to an adjacent minimum with identical training error. The study of how a model re-
acts to such actions is at heart of the Hessian-based toolbox summarized in Fig. 3.10(b), which
contains inﬂuence functions [145], the resampling uncertainty estimation (RUE) [146], and
LEs [147] whose conceptual ideas we introduce in the following.

Inﬂuence functions are an approximation of the procedure known as leave-one-out train-
ing24 and estimate how the model prediction on a test point xtest change, if a certain training
point xR is removed from the training set. You can imagine three outcomes of such a removal:

23There is a general consensus that wide, ﬂat minima generalize better than sharp minima [139–142]. Keep in

mind that ﬂatness is not a well-developed concept in non-convex landscapes of deep models [143].

24Leave-one-out training for DL models with non-convex loss landscapes is tricky, because if we land in a different
local minimum, we cannot make any claims on the perturbation caused by removal of a single training point. This
is why usually we retrain carefully, starting from the minimum reached by the original model.

68

𝜽𝒟ℒ𝑧test𝑯𝜽∗ℒ𝑧testℒ𝑧testℒ𝑧test𝜽∗𝜽ℒ𝒟Phase classiﬁcation

(1) the prediction stays the same because the removed training point has no inﬂuence on the
model prediction, (2) the prediction gets better (i.e., it leads to a lower test loss for xtest),
so xR is a “harmful” training point for making a prediction on ztest, (3) the prediction gets
worse (i.e., it leads to a larger test loss for xtest), so xR is a “helpful” training point for making
a prediction on ztest, and its removal made the task of constructing an accurate model harder.
With such an analysis we can determine how inﬂuential training data points are to predictions
at test points, which may give us a hint at how the model reasons. We can even go a step
further and say that if two data points strongly inﬂuence each other, it is because they are very
similar from the model’s perspective.25 This concept of similarity learned by a ML model can
be understood as a distance between data points in the internal model representation and is
a powerful tool for detecting additional phases in mislabeled data [97,148], detecting inﬂuential
features [97], and anomaly detection [144].26

Another tool

in the Hessian-based toolbox is the resampling uncertainty estimation
(RUE) [146]. Its aim is to estimate the uncertainty of model predictions. It is an approximation
of the classical procedure known as bootstrapping. You start with your original training set
containing each training data point once. Imagine now that you create b new training sets by
drawing samples uniformly with replacement from the original data set. Due to replacement,
your new sets contain some training points in more than one copy and some points are omit-
ted. Now you can train b models on these b training sets and make b predictions on the same
test point, ztest. These predictions generally vary due to the distinct nature of the training sets.
Computing the variance of these predictions on ztest gives us an estimate for the uncertainty
of the original model prediction. A small variance signals that one can trust the prediction of
the original model, because small random modiﬁcations to the training set do not change its
prediction too much. A large variance signals that the prediction is based on a small number
of training points and is therefore not reliable. In Ref. [144], you can see how such error bars
indicate the sharpness of quantum phase transitions.

Finally, local ensembles (LEs) [147] allow us to detect the underspeciﬁcation of a given model
at the test point. A trained model is underspeciﬁed at a test input if many different predictions
at that input data are all equally consistent with the constraints posed by the training data
and the learning problem speciﬁcation (i.e., the model architecture and the loss function). As
described in section 2.1, the minimum reached within the optimization is usually surrounded
by a mostly ﬂat landscape. This means that if the model would have ended up in one of these
ﬂat neighboring points, the training error would have stayed exactly the same. Thus, such
changes should not impact the predictions – unless a prediction is underdetermined, i.e., un-
stable and not well-explained by the training data. Therefore, we can again create multiple
models by shifting the parameters of the original model by small amounts. As such, these
new models explore the ﬂat landscape around the original minimum. Eventually, we make
predictions with these new models. If a prediction on a test point ztest changed due to such
modiﬁcations, this point may be an out-of-distribution point, i.e., a point coming from a dis-
tribution that is signiﬁcantly different from the distribution underlying the training data. LEs
allow for the detection of such out-of-distribution test points which increases the reliability of
the ML model. Moreover, the authors of Ref. [147] successfully used LEs for active learning,
i.e., they built a much smaller, yet similarly informative training data set by iteratively adding
to it test points with the largest underspeciﬁcation score detected by LEs.

25This argument is well based in the geometric interpretation of inﬂuence functions, for details see section 2.3.3

in Ref. [144].

26Note that the similarity, which is arguably the central concept behind classiﬁcation tasks, has various meanings

when it comes to Siamese NNs, inﬂuence functions, and kernel methods that are covered in the next chapter.

69

Phase classiﬁcation

Therefore, we have answered our initial questions regarding interpretability: It is in-
deed possible to look inside the black box of ML models. If you focus your attention on
the bottlenecks present in NN architectures, you can determine which quantities dom-
inate in the NN prediction using regression methods. If these quantities are physically
relevant, we can argue that the NN indeed bases its predictions on physically relevant
quantities. For architectures without bottlenecks, you can turn your attention to the
curvature around the minimum of the training loss.
It contains information on the
similarity learned by a model and allows for estimating the uncertainty.

3.6 Outlook and open problems

Over the last ﬁve years, there have been many works applying supervised and unsupervised
phase classiﬁcation algorithms, including supervised learning (section 3.3), learning by con-
fusion (section 3.4.2), and the prediction-based method (section 3.4.3), to models with well-
known phases. However, there have been only few works that applied unsupervised phase
classiﬁcation methods to experimental data. Moreover, the discovery of a novel phase of mat-
ter using unsupervised phase classiﬁcation methods still remains to be demonstrated. This
would constitute a major step toward the automation of scientiﬁc discovery.

While there has been signiﬁcant progress regarding the interpretability of phase classiﬁ-
In
cation methods in recent years, we still lack a deeper understanding of these methods.
particular, it remains difﬁcult to tell when and why a given method fails or succeeds [149].
With the goal of automated scientiﬁc discovery in mind and having demonstrated that phase
classiﬁcation methods are capable of dealing with a vast range of physical systems, address-
ing these gaps in knowledge and developing corresponding interpretability tools is of crucial
importance.

Further reading

• Carleo, G. et al. (2019). Machine learning and the physical sciences. Rev. Mod. Phys.
91, 045002. An overview of the current state of the phase classiﬁcation landscape is
presented in section 4C [30].

• Neupert, T. et al. (2021). Lecture notes: Introduction to machine learning for the sciences.
An introduction to fundamentals of ML and clustering algorithms for scientists [86].

• Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models

Explainable. An introductory book on interpretable machine learning [150].

• Jupyter notebook on phase classiﬁcation [32].

70

Gaussian processes and other kernel methods

4 Gaussian processes and other kernel methods

This section deals with the so-called kernel methods of which support vector machines (SVMs)
and Gaussian processes (GPs) are prominent examples. These methods are particularly well-
suited in the case of the low availability of labeled data. This usually happens when the cre-
ation of a large data set is expensive in terms of money, time, effort, and so on. As a second
advantage, the predictions of GPs are, by construction, accompanied by their uncertainties
which other methods typically do not provide. We see how this property arises from the de-
sign choice of the models in section 4.2.

But ﬁrst, we have to introduce the notion of the kernel that is the integral part of all meth-
ods discussed in this chapter. The introduction of the kernel allows us to extend the range
of problems we are able to tackle substantially. Before properly deﬁning the mathematical
foundation of kernels, we start by providing some intuition on how to use them in practice by
means of the kernel trick and its implications. Afterwards, we show how to extend the afore-
mentioned methods via this kernel trick and discuss how to train each of them given data. For
example, it turns out that GPs can be approached from an information-theoretic perspective.
Moreover, we explain how we can make use of concepts from information theory for a guided
data acquisition procedure, as well as to select a good model among various possible ones.
We end the chapter by showcasing the power of these methods at tackling quantum chemistry
problems in section 4.5.

4.1 The kernel trick

As we have seen in section 2.4, simple approaches such as the linear regression model or the
linear SVM have severe limitations with respect to the properties of data. They are applied
to the input data as is, i.e., they are bound to the given representation of the input data. In
order to avoid confusion later on, we refer to this data space as the input space. In this input
space, it can, for example, happen that the given input data is not linearly separable. One pos-
sible remedy consists of extending the classiﬁcation power of the model by ﬁrst transforming
the input data into an alternative feature space. In contrast to what we have said earlier in
chapter 1, we explicitly distinguish between the input and the feature space in the following.1
Ideally, in this new representation, the data possesses a more convenient structure compared
to the original representation. For example, the data that was initially not linearly separa-
ble may be linearly separable in this new space. In particular, it is often useful to transform
into a higher-dimensional feature space in which our data is now nested on a manifold which
(ideally) possesses beneﬁcial additional structure.

One may think that this makes the kernel methods extremely costly (or even infeasible if
the dimension of the feature space approaches inﬁnity). However, we can make use of the fact
that the predictions of our ML algorithms of interest are often formulated in terms of distances
between data in the input space.

This is where the kernel trick comes in: instead of explicitly transforming the data into
the feature space and then calculating the distance therein, we start from the other end
and provide a closed form expression for the distance in terms of the data representation
in the input space. This typically is much more efﬁcient from a numerical perspective.

1Each element of a data point is still called a feature – in this chapter, we are, however, only interested in ﬁnding

the most convenient representation of the data whose space we hence call the feature space.

71

Gaussian processes and other kernel methods

As we will see later on, we can always associate a unique feature space to any valid distance
function. Thus, we shift the focus of ﬁnding a suitable representation to choosing a suitable
distance function. This trick is called the kernel trick because the kernel is the mathematical
object we associate with such a feature space. As such, the kernel trick allows one to retain all
the beneﬁts of high-dimensional feature spaces at a manageable computational cost. Moreover,
the mathematical foundation of kernels allows us, as we see in the next section, to enrich our
motivation with rigorous, analytical validity. Especially important from a practical point of
view is the representer theorem: so far, we have set out to ﬁnd a suitable transformation, i.e.,
function to simplify our task at hand. However, it is unclear how to optimize over functions
instead of parameters. The representer theorem endows us with both: in essence, it assures
that the optimization over the function space is equivalent to optimizing the coefﬁcients of
a closed form solution which, in turn, allows us to devise feasible numerical optimization
routines.

In the following, we start with an intuitive example to illustrate why and how the trans-
formation into the feature space can be beneﬁcial. Afterwards, we properly introduce the
mathematical notion of kernels that gives us the analytical tools at hand that are required to
understand the representer theorem.

4.1.1 Intuition behind the kernel trick

To gain some intuition, let us consider a labeled two-dimensional data set as depicted in
Fig. 4.1. In this toy example, the black line indicates the underlying decision boundary, i.e.,
the line that separates input data with different labels.
In higher dimensions, the decision
boundary generalizes to a hyperplane. In our example, one label refers to the center of the
data cloud and the other label to its outskirts. A label distribution is said to be separable if one
can draw such a decision boundary, i.e., it is separable if we can ﬁnd at least one hyperplane
separating the two class sets. If, furthermore, this decision boundary is linear, the data is called
linearly separable.2 Clearly, our toy data set is not linearly separable in the input space. As
a consequence, we cannot ﬁnd a straight line that fully separates the two data classes by means
of a simplistic linear classiﬁer. Additionally, other linear methods such as PCA (see chapter 3)
fail to cluster this data.3

2

1, x 2

) (cid:55)→ (x 2

However, as shown in the right panel in Fig. 4.1, the data set becomes linearly separable
if we transform the data in the appropriate way. Here, we have applied the transformation
φ : (x1, x2
) to map the input data nonlinearly to the so-called feature space. Hence,
the map φ is called the feature map. There are two important caveats: ﬁrstly, ﬁnding a use-
ful feature map is a highly-non-trivial task. In our toy example, the labeling procedure has
considered the data points in polar coordinates and used the radial distance r to the origin as
the label criterion. Using the connection r 2 = x 2
2, we motivate our feature map φ whose
1
choice is by no means unique. Secondly, even if we had found a good feature map, it could be
inﬁnite-dimensional.4 In that case it would not be feasible to transform the data with the fea-
ture map. In our example, we only considered a polynomial expansion of the input variables
in order to achieve linear separability. However, there can be instances where the polynomial
expansion has to be taken to inﬁnite order. We encounter such an example where the most

+ x 2

2Whether a given data set actually is (linearly) separable or not, is not easily detectable. In practice, we can
at least run algorithms such as an SVM explained in section 2.4.3 which are guaranteed to ﬁnd the corresponding
separating hyperplane if it exists.

3As described in chapter 3, PCA itself is not a clustering algorithm. However, we have also seen that it can be

used to provide a low-dimensional representation of the data in which the data is split into different clusters.

4In a sense, we do the opposite of a dimensionality reduction method such as PCA in section 3.2.1 – usually, we
drastically increase the dimension of the feature space in order to rearrange the data most conveniently. Another
difference is that we do not loose any information in the data by embedding them into a high-dimensional space.

72

Gaussian processes and other kernel methods

Figure 4.1: Toy example of a labeled two-dimensional data set. The data points are
labeled according to their position with respect to the decision boundary indicated
by the black circle in (a). In this input space, such a data set is not linearly separable.
After a transformation of the input variables into a nonlinear feature space, however,
the data becomes linearly separable as indicated by the black line in (b).

suitable feature space is inﬁnite dimensional later on when we come back to our toy example
at the end of section 4.2.2.

Fortunately, it turns out that many ML algorithms can be expressed only in terms of
inner products between data points x, y.a As such, we do not need to consider the
individual inputs x and their representation in feature space φ(x) explicitly. The kernel
trick simply consists of exchanging the inner product 〈x, y〉 = x(cid:252)y in the corresponding
algorithms by the function K(x, y) = φ(x)(cid:252)φ(y).

aWe remind the reader that these data points are, in general, vectors, i.e., they may live in a high-

dimensional space, see also section 1.5

The function K can be efﬁciently evaluated, in particular, for an inﬁnite-dimensional fea-
ture space to which φ maps and yields a single real non-negative number regardless of the
data under consideration or the dimension of the feature space.5 As we see later, the function
K is referred to as the kernel function. This way, the kernel trick allows for a feasible nonlinear
extension of a variety of ML-algorithms, such as ridge regression, SVMs, or PCA. We detail the
mathematical foundation of kernels in the next sections.

4.1.2 The function space as a Hilbert space

As we have sketched above, we want to ﬁnd a suitable choice for our feature map, i.e., search
for a function that lives in some function space. A mathematical space is a set of elements
that obey a set of common rules specifying the relationship between them. As we want to
search in a function space, it is reasonable to assume it to be a vector space. In such a space,
we are allowed to add functions to each other or rescale them by a constant without leaving

5In many cases, it is easier to work with kernels yielding inﬁnite-dimensional feature spaces than with ﬁnite
ones. Indeed, the ﬁnite dimensionality may often be a problem. We refer to section 8.2.5 for quantum kernel
methods and the references therein regarding this issue.

73

-2-1012x-2-1012y(a)0123x20123y2(b)Gaussian processes and other kernel methods

the function space. This assumption is necessary to express the unknown target function by
a weighted sum of other known functions. Fortunately, how to optimally tune these weights is
something we are familiar with in ML. As a second ingredient, we require some distance mea-
sure or, equivalently, a measure of similarity between functions. This is achieved by enriching
our vector space with an inner product 〈·, ·〉, turning the function space into an Hilbert space.
Since we have some freedom on how to deﬁne the inner product, different choices of inner
products lead to different notions of distance, i.e., different spaces altogether. We see shortly
why we need this similarity measure by the end of this section.

If we consider the space of real-valued functions and restrict ourselves to the condition of
square-integrability, the corresponding function space is such a Hilbert space. This effectively
turns the function space into something akin to the more intuitive Euclidean space. It is called
the L2-space.6 Square-integrability refers to the fact that the integral over the full domain D
remains ﬁnite, i.e., (cid:82)
D f 2(x) dx < ∞ ∀ f ∈ L2. The domain D is often given by Rm with
some dimension m in typical ML-scenarios as it corresponds to our input space. The L2-space
readily comes along with a canonical choice for the inner product given by

〈 f , g〉 =

(cid:90)

D

f (x)g(x) dx

(4.1)

that provides the notion of orthogonality as 〈 f , g〉 = 0, and a norm (cid:107) f (cid:107)2
= 〈 f , f 〉. Since
L2
a Hilbert space is a vector space, we can ﬁnd an orthogonal basis set {φ
(x)} that spans the
space. In case of the L2-space it is of inﬁnite (but countably inﬁnite) dimension. This basis
allows for any function f ∈ L2 to be decomposed as

n

f (x) = (cid:88)

φ

an

n

(x) ,

n

(4.2)

with real coefﬁcients {an
}. Lastly, any two Hilbert spaces have the same geometric structure,
regardless of their respective elements – this is the whole reason we can draw the analogies
between the L2 and the intuitive Euclidean space in the ﬁrst place. This can be made more
formal by the representation theorem of Riesz [151]: we can express certain linear functionals
by means of the Hilbert space’s inner product. The functional of interest is the evaluation of
the function f at any point x ∈ D, i.e., f ∈ L2 (cid:55)→ f (x) ∈ R since this is what we are trying
to achieve in our task. Furthermore, Riesz theorem tells us that this can, in principle, be
achieved by a particular, unique function living in the Hilbert space via the inner product, i.e.,
the evaluation is of the form 〈·, K〉 with K a member of the same Hilbert space. This function
is called the kernel function but is not guaranteed to exist in every Hilbert space – we introduce
it and its constraints properly in the next section.

4.1.3 Reproducing kernel Hilbert spaces

As already anticipated, we want to make use of Riesz theorem to connect the function space to
the evaluation of its members. That is, we want to be able, once provided with a trial function
f from the Hilbert space H, to evaluate its output f (x) at an arbitrary domain location x. In
order to use the theorem, we have to restrict the function K, which we subsequently call the
kernel function, to fulﬁll the following integral transformation:

f (x) =

(cid:90)

D

K(x, x(cid:48)) f (x(cid:48)) dx(cid:48) ∀ f ∈ H ,

(4.3)

6Of course, not all functions we are interested in ﬁnding are necessarily members of this function space. Because
we are always presented with a ﬁnite data set, however, we usually do not care what happens very far outside this
regime. Even though our target function might not be a member of this space, we should be able to ﬁnd a member
that resembles the target function in the region of interest, nevertheless. This is why we can restrict ourselves with
the L2-space in the ﬁrst place.

74

Gaussian processes and other kernel methods

while also requiring that K(·, x) = K(x, ·) =: kx ∈ H ∀ x ∈ D. The equation relates the
evaluation of the trial function f to the kernel – and we see how we can make use of it in
the following. Actually, the integral transformation resembles one of the deﬁning properties
of the δ-distribution and we would be inclined to choose K(x, x(cid:48)) ≡ δ(x − x(cid:48)). However, the
problem lies in the fact that the δ-distribution (when indexed by one of its components) is not
square-integrable, i.e., 〈δ, δ〉 (cid:54)∈ R, and thus δ (cid:54)∈ L2.7 We can solve this conundrum by setting
up a new Hilbert space H (which cannot be identical to the L2) equipped with a kernel function
K that actually fulﬁlls Eq. (4.3). As a member of the Hilbert space H, the kernel (when indexed
by either one of its arguments) can be decomposed as in Eq. (4.2). We can actually go one step
further: due to Mercer’s theorem [152], this basis set can be used to symmetrically decompose
the kernel into

K(x, x(cid:48)) = (cid:88)

φ

λ

n

n

(x)φ

n

(x(cid:48)) = K(x(cid:48), x)

(4.4)

n

with coefﬁcients {λ
to one. We can prove this by choosing f = φ
the decomposition of the kernel, we require that

n

}. Moreover, for an orthonormal basis set, the coefﬁcients λ

n are all equal
k for some integer k. Then from Eq. (4.3) and

(x) = (cid:88)

φ

k

φ

λ

n

n

(x)

n

(cid:90)

D

φ

n

(x(cid:48))φ

k

(x(cid:48)) dx(cid:48) = λ

φ

k

k

(x) .

(4.5)

Since k was chosen arbitrarily, we ﬁnd that λ
= 1 ∀n iff we select an orthonormal basis
set. We can convince ourselves that this kernel representation actually performs the integral
transformation in Eq. (4.3). Therefore,

n

(cid:90)

D

K(x, x(cid:48)) f (x(cid:48)) dx(cid:48) = (cid:88)

an

φ

k

(x)

n,k

(cid:90)

D

φ

k

(x(cid:48))φ

n

(x(cid:48)) dx(cid:48) = (cid:88)

φ

an

n

(x) = f (x)

(4.6)

n

as intended. Having introduced the kernel function K(x, x(cid:48)), let us examine more of its prop-
erties. First, let us ﬁx one of the arguments and consider the kernel only as a function of the
second argument. Due to the symmetry in the arguments, we choose x without loss of gen-
erality. We indicate this choice by a different notation as Kx(x(cid:48)) := K(x, x(cid:48)) = (cid:80)
(x(cid:48))
(x) due to Eq. (4.4). This newly
where the last equation is just Eq. (4.2) with ax,n
introduced function Kx is just another function in our Hilbert space H. However, we can ex-
press any other function f ∈ H in the same Hilbert space as an inner product with Kx: using
the integral transformation of Eq. (4.3) and the deﬁnition of the inner product as in Eq. (4.1),
we see that

n ax,n

= λ

φ

φ

n

n

n

f (x) =

(cid:90)

D

K(x, x(cid:48)) f (x(cid:48)) dx(cid:48) =

(cid:90)

D

Kx(x(cid:48)) f (x(cid:48)) dx(cid:48) = 〈Kx, f 〉 = 〈 f , Kx〉 .

(4.7)

At this point, we understand how Riesz’ theorem applies to the function evaluation: the kernel
function K turns the inner product in the Hilbert space H into the function evaluation. Due to
this reproducing capability, the function Kx is called the reproducing kernel of the Hilbert space
H, which itself is dubbed the reproducing kernel Hilbert space (RKHS). Instead of indexing
over the ﬁrst argument x, we can also deﬁne Kx(cid:48)(x) := K(x, x(cid:48)) by indexing over the second
argument x(cid:48). As both belong to the same Hilbert space H, we faithfully reproduce Kx(cid:48) by Kx
(or vice versa):

Kx(cid:48)(x) = K(x, x(cid:48)) = 〈Kx, Kx(cid:48)〉 = Kx(x(cid:48))
(4.8)
Furthermore, we see that the kernel function K(x, x(cid:48)) is just the inner product between the
reproducing kernel Kx with itself Kx(cid:48).

7At least not in the case where the domain corresponds to the whole Rm. If we restrict the frequency domain to
some bounded subset, we can, in fact, construct a kernel function. However, this function is not the δ-distribution.

75

Gaussian processes and other kernel methods

The choice of the reproducing kernel is unique as shown by the Moore-Aronszajn theo-
rem [153] granted that we have a positive deﬁnite kernel, i.e., K(x, x(cid:48)) ≥ 0 ∀x, x(cid:48) ∈ D.
This condition is necessary in order to represent the kernel function as an inner product as in
Eq. (4.8). The theorem then tells us that for every positive deﬁnite kernel, there exists a unique
RKHS and vice versa.

Furthermore, the theorem begs the question whether any positive deﬁnite function can
actually be a kernel function. This is the case as we show now if the function is symmetric
with respect to the interchange of its two arguments. To this end, consider a positive deﬁnite
function K which takes two elements from our domain D as arguments, i.e., K : D × D → R.
Mercer’s theorem also holds for positive deﬁnite functions and, thus, the decomposition in
Eq. (4.4) is valid with λ
≥ 0 (but not necessarily equal to one anymore) due to the positivity
of K. This decomposition, moreover, is unique and obeys the eigenvalue equation

n

(cid:90)

D

K(x, x(cid:48))φ

n

(x(cid:48)) dx(cid:48) = λ

φ

n

n

(x) ∀n .

(4.9)

Using Eq. (4.2), we rewrite the inner product in the L2 of Eq. (4.1) as

〈 f , g〉 =

(cid:90)

D

f (x)g(x) dx = (cid:88)

am bn

m,n

(cid:90)

D

(cid:124)

φ

(x)φ

n

m

(x) dx

=〈φ

(cid:123)(cid:122)
n

m,φ

〉=δ

(cid:125)

m,n

= (cid:88)
n

an bn

(4.10)

(cid:90)

= (cid:88)
n

φ

am

m

(x)φ

n

(x) dx

(cid:90)

bk

φ

k

(x(cid:48))φ

n

(x(cid:48)) dx(cid:48) = (cid:88)

〈 f , φ

〉 〈g, φ

〉 .

n

n

n

However, in order for K to be a valid kernel, it has to obey Eq. (4.7) as well. Plugging this into
the ﬁrst step of Eq. (4.10) together with Mercer’s decomposition as in Eq. (4.4), we see that

(cid:90)

〈 f , g〉 =

f (x)g(x) dx =

(cid:90)

〈 f , Kx〉 g(x) dx

(cid:90)

= (cid:88)
n

λ

n

φ

n

(x) 〈 f , φ

n

〉 g(x) = (cid:88)

λ

〈 f , φ

〉 〈g, φ

〉

n

n

n

n

(4.11)

Comparing the two previous results, we see a discrepancy in terms of the prefactors λ
n of the
Mercer decomposition of the (not yet) kernel function K. In order to compensate for this, we
can redeﬁne the inner product as

〈 f , g〉

H

=

∞
(cid:88)

〈 f , φ

n

n=1

〉

n

〉 〈g, φ
λ

n

,

(4.12)

which is equivalent to the previous inner product in Eq. (4.1) iff λ
of the inner product, we have

n

= 1 ∀n. With this deﬁnition

〈 f , Kx〉

H

=

∞
(cid:88)

〈 f , φ

n

n=1

〉

n

〉 〈Kx, φ
λ

n

=

∞
(cid:88)

n=1

〈 f , φ

n

〉 φ

n

(x) = f (x) .

(4.13)

n

Here, we have used Eq. (4.9) in the second step and the last equality follows from the fact
that the set {φ
} forms a complete basis, i.e., Eq. (4.2). This now fulﬁlls Eq. (4.7) as intended
and, furthermore, renders the Hilbert space H an RKHS, and K, any arbitrary positive deﬁnite
function, is indeed a kernel function. Lastly, we remark that the redeﬁnition of the inner
product is crucial in order for every RKHS to be associated with a unique positive deﬁnite
kernel.

76

Gaussian processes and other kernel methods

4.1.4 The representer theorem

Finally, we have all the ingredients necessary for the representer theorem [154]: by virtue of
Eq. (4.7), we have that f (x) = (cid:80)∞
) for any function f in general. Given a loss
function L (including a regularization term) and n training samples {(xi, yi
i=1, the theorem
asserts that

m=1 amK(x, xm

)}n

∗(x) := arg min

f

f

(cid:128) (cid:8)( f (xi
L

), yi

)(cid:9)n

i=1

(cid:138) =

n
(cid:88)

i=1

ai K(x, xi

),

(4.14)

i.e., the loss is minimized by resorting to the ﬁnite amount of training data only. Moreover, we
only require the knowledge of our kernel function K – the heavy machinery, e.g., its unique
RKHS with Riesz theorem telling us how to evaluate the unknown target function f ∗, remains
hidden in the proof and we can happily live with the resulting theorem only.

The representer theorem guarantees that we can formulate the search for a function f ∗
that minimizes a speciﬁc loss function over an inﬁnite-dimensional function space as
a search over n kernel coefﬁcients {ai
}n
i=1. Thus, it signiﬁcantly reduces the complexity
of the minimization problem at hand and renders it computationally tractable.

Finally, our mathematical efforts have come to fruition: the reproducing property of the
kernel in Eq. (4.3) allows us to implicitly embed the input data in a (possibly) high-dimensional
feature space in which we calculate the similarities to a test point x. Because we only require to
calculate the similarity measure between data points, we do not loose efﬁciency here. We see
how to practically do this in the following when we extend some of the models of section 2.4
via the kernel trick in section 4.2.

4.1.5 Consequences of the kernel trick

We have gone to somewhat lengthy details on the maths behind kernels, in particular, con-
cerning the RKHS. Despite the perceived detour through Hilbert spaces and a redeﬁnition of
the inner product in the RKHS, this leg work equips us with all the necessary foundation to the
theory of kernels: the reproducing feature of Eq. (4.3) is not a mere mathematical curiosity
but has straightforward implications in terms of the representer theorem. Secondly, the kernel
allows us to solve our initial problem (ﬁnding the unknown target function) by ﬁnding the
right kernel. The kernel can be understood as a similarity measure between functions. This
similarity between two inputs can easily be calculated even if the underlying feature space is
high- or even inﬁnite-dimensional.

One important consequence of implicitly switching from the input to the feature space by
means of the kernel trick is that we have to rethink our intuition of regularization: now, we
have to perform the regularization of the learned function in the function space given by the
RKHS as discussed in the previous section. Thus, we have to start from the inner product
in the RKHS, i.e., Eq. (4.12). By virtue of Mercer’s theorem in Eq. (4.4), the coefﬁcients
λ
n correspond to the weights of the basis functions of the kernel and are non-negative by
construction. In particular, they depend on the actual choice of the kernel function K. For
instance, our kernel decomposition could include zero entries for some of the basis functions.
This is not an issue per se: the function f we are interested in might still lie entirely in the
RKHS. Using Eq. (4.2), we can decompose it as f (x) = (cid:80)∞
Its corresponding
L2-norm is (cid:107) f (cid:107)2
2

|2. Due to Eq. (4.12), this translates to a norm in the RKHS as

n=1 an

= (cid:80)

(x).

|an

φ

n

n

(cid:107) f (cid:107)2
H

=

dim(RKHS)
(cid:88)

n=1

77

|2

.

(4.15)

|an
λ

n

Gaussian processes and other kernel methods

Table 1: Examples of kernel functions, where θ , θ

1 and θ

2 are free parameters [124].

Kernel function

Mathematical form

Linear

Radial basis
Matérn 5/2

Rational quadratic

KRQ

KLIN

KRBF

KMAT

(cid:0)x, x(cid:48)(cid:1) = x(cid:252)x(cid:48) + θ
(cid:0)x, x(cid:48)(cid:1) = exp (cid:0)− 1
(cid:107)x − x(cid:48)(cid:107)2(cid:1)
2θ 2
(cid:112)
(cid:0)x, x(cid:48)(cid:1) = (cid:128)
θ (cid:107)x − x(cid:48)(cid:107) + 5
5
1 +
3
(cid:17)−θ
(cid:16)
1 + (cid:107)x−x(cid:48)(cid:107)
(cid:0)x, x(cid:48)(cid:1) =
2θ
1

1

θ 2
2

(cid:107)x − x(cid:48)(cid:107)2(cid:138) × exp

(cid:128)−

(cid:112)

5
θ 2

(cid:107)x − x(cid:48)(cid:107)(cid:138)

Thus, we see that we potentially run in to trouble in case of zero entries for some of the coefﬁ-
cients λ
n as our norm may diverge. It remains ﬁnite iff the corresponding function coefﬁcient
an is equal to zero at the same time. This is the case if the function is entirely in the RKHS
provided by the kernel function. If not, i.e., when choosing the wrong kernel function, we
cannot regularize our model and cannot expect to learn the unknown target function f .

The theory of RKHS’ can give us an intuition on why certain choices of kernel function
seem to work while others fail: the function f we are interested to re-express with
our kernel function K has to fully lie in the RKHS uniquely deﬁned by K. If not, our
approach is doomed to fail in learning the function from the start. Because a systemic
check in the (usually) inﬁnite dimensional function space is not feasible, we can only
resort to the trial-and-error approach. Luckily for us, there are ways to systematically
build better kernels (which provide a more suitable RKHS) presented in section 4.4.2.

4.2 Kernel methods

In section 4.1, we have presented the mathematical foundation of kernels. In short, we want
to map our data to a feature space which possesses a more suitable structure for the task at
hand. Instead of explicitly deﬁning a feature map φ, we introduce a kernel function K which
provides a similarity measure between data points in the underlying feature space. As such,
we exchange the problem of searching for a (potentially) high-dimensional feature map to
ﬁnding an optimal kernel function, which is rigorously easier. This constitutes the kernel trick.

Kernel methods correspond to all classiﬁcation and regression methods that take advantage
of the kernel trick. The validity of these approaches is ensured by the representer theorem,
see section 4.1.4. The ﬁrst step of every kernel method is to choose a kernel function. As
discussed previously, any positive deﬁnite function can be used as a kernel function. Typically,
one starts by assuming some functional form (see examples in Table 1). These functions are
parametrized by a few parameters, such as θ , or θ
2 in these examples. Having chosen
a particular functional form of the kernel function, one varies these parameters to ﬁnd the best
kernel in the corresponding functional ansatz class. This makes the optimization already easier
as we now have to optimize over a set of parameters and not over a set of functions. Also note
that given a set of kernels, there exist many transformations which yield new valid kernels. For
(cid:0)x, x(cid:48)(cid:1) with positive coefﬁcients
example, any linear combination of kernel functions (cid:80)
≥ 0 constitutes a valid kernel. For a more exhaustive list of techniques for constructing new
ci
kernels, see [84]. We explicitly make use of these rules in section 4.4.2 where we discuss how
to construct good kernels systematically through composition.

1 and θ

i ci Ki

We turn to three prominent kernel methods in the remainder of this subsection: kernel

78

Gaussian processes and other kernel methods

ridge regression (KRR), support vector machines (SVMs) and Gaussian processes (GPs).

4.2.1 Kernel ridge regression

Kernel ridge regression (KRR) is an extension of ridge regression (presented in section 2.4.1)
to a nonlinear and possibly high-dimensional space. The functional we want to minimize is
lives in the RKHS H
very similar to the one of ridge regression, but this time, the model f
corresponding to the particular choice of the kernel function (note the use of MSE!):

LKRR

=

n
(cid:88)

i

( yi

− f (xi

))2 + λ(cid:107) f (cid:107)2
H

= LMSE

+ Lreg

(4.16)

with a regularizing term introduced in Eq. (4.15). Here, f is not restricted to a linear function
of its parameters (as in linear ridge regression). Instead, f can, in principle, be arbitrary. As
such, KRR is capable of building highly expressive models given an appropriate choice of the
kernel. Solving a problem with kernel methods translates to ﬁnding the optimal kernel. This
functional form in Eq. (4.16) does not yet look like it is the case. Let us then reformulate it so
it becomes apparent. A kernel model f (x) based on a kernel function K(x, x(cid:48)) can be written
as the following sum over the training data:

f (x) =

n
(cid:88)

j=1

α

j K (cid:0)x, x j

(cid:1) .

(4.17)

This formulation is an instance of the already discussed representer theorem, see Eq. (4.14).
Apart from K which we know how to compute given the data, we also have here coefﬁcients
j of our kernel model f (x) which we need to ﬁnd. Now we can express Eq. (4.16) with
α

Eq. (4.17) in matrix form:

LMSE

=

n
(cid:88)

i

( yi

− f (xi

))2 = (y − Kα)(cid:252)(y − Kα),

Lreg

= λ(cid:107) f (cid:107)2
H

= λα(cid:252)Kα,

(4.18)

(4.19)

It is a positive-semideﬁnite, square n × n
where the matrix K is called the kernel matrix.
matrix with elements K (cid:0)x, x(cid:48)(cid:1) with training points x and x(cid:48) belonging to the training set:
x1, x2, x3, . . . , xn. The vector y represents the targets for the corresponding training input
x. Finally, if we put the derivative of the sum of these two components to zero, we can ﬁnd
a solution for α which is:

ˆα = [K + λ1]−1y .

Given ˆα, we can write the estimator of the model ˆf at a test point x∗ as

ˆf (x∗) = k(cid:252) (x∗) ˆα = k(cid:252) (x∗) [K + λ1]−1y ,

(4.20)

(4.21)

where k (x∗) = [k (x∗)
)]. The analogous and thorough derivation of the kernel
trick on the example of KRR is provided in Appendix B. Finally, we can see that the prediction
of the output for an unseen input x∗ can be written in terms of:

] = [K (x∗, xi

i

• The target vector y,
• The kernel matrix K, whose elements are the kernel functions K (cid:0)xi, x j

(cid:1), which takes

advantage of the kernel trick,

• The column vector k (x∗), whose elements are the kernel functions K (x∗, xi

), which

also takes advantage of the kernel trick,

79

Gaussian processes and other kernel methods

• The regularization term of magnitude λ.

So, as anticipated earlier in the chapter, successfully applying KRR boils down to ﬁnding the
appropriate kernel function K.

4.2.2 Support vector machines

In the previous section, we have discussed how to use kernel methods for regression problems
(in particular ridge regression). In this section, we show how can we use them for classiﬁca-
tion. In this context, the intuition behind the kernel approach is to embed the input space into
the feature space in such a way that the data becomes linearly separable with a hyperplane
(as described already in section 4.1.1). The most common ML-classiﬁcation method utilizing
the kernel trick are support vector machines (SVMs).8

SVMs have been introduced already in section 2.4.3 as geometric linear classiﬁers. Before
we see how kernels enter SVMs, let us make a reminder how linear SVMs work and rephrase
the optimization problem that we have described in section 2.4.3. The problem there is to
ﬁnd an optimal hyperplane separating data from different classes. The optimal hyperplane is
deﬁned as the one with the maximal distance between the hyperplane and the data points. In
other words, we can say that all data points need to be at least the distance M away from the
hyperplane. The data points that are separated from the hyperplane exactly by M , so are the
closest to the hyperplane, become support points, xs,i. The classiﬁcation problem boils down to
ﬁnding such a hyperplane described by θ that maximizes the margin between the hyperplane
and support points xs,i (see Fig. 2.5). As we can rescale the hyperplane in an arbitrary way,
we can have |θ| = 1/M . Then maximizing a margin, becomes minimizing θ that in turn comes
down to minimizing the Lagrange function L in Eq. (2.41), which we write here again for the
readability:

L = 1
2

|θ|2 −

n
(cid:88)

i=1

α

i

[ yi

(θ(cid:252)xi

+ θ
0

) − 1] ,

where the Lagrange multipliers α

i are chosen such that

α

i

[ yi

(θ(cid:252)xi

+ θ
0

) − 1] = 0 ∀ i = 1, . . . , n .

(4.22)

(4.23)

As we have already discussed, α
i is non-zero (and positive) only for xs,i. In practice, rather
than minimizing L, we go for the dual formulation of the problem, and we maximize a La-
grange dual, LD, which provides the lower bound for L. LD remains a quadratic program
similarly as L as we have discussed in section 2.4.3. To express the problem via LD, we ﬁrstly
take the derivative of L with respect to θ and θ

0 and set it to zero. We arrive at:

θ =

0 =

n
(cid:88)

i=1
n
(cid:88)

i=1

α

i yixi

α

i yi .

(4.24)

We can see that the coefﬁcients θ are given by the Lagrange multipliers α
i, which can be found
numerically. When we plug these equations back to the Lagrange function in Eq. (4.22), we
arrive to the Lagrange dual:

=

LD

n
(cid:88)

i=1

α

i

− 1
2

n
(cid:88)

n
(cid:88)

i=1

j=1

α

α

i

(cid:252)
i x j
j yi y jx

subject to α

≥ 0 .

i

(4.25)

80

Gaussian processes and other kernel methods

Figure 4.2: The kernel form makes a difference! The same data as in Fig. 4.1 is
classiﬁed using an SVM with different kernel choices. The black line corresponds to
each underlying decision boundary. (a) As the data is not linearly separable, a linear
kernel does not work. (b) A polynomial kernel up to degree 50 does the trick with
the expense of signs of overﬁtting. (c) Due to the rotation symmetry, an RBF kernel
is best suited for classifying this data set whose decision boundary closely resembles
the actual underlying decision boundary of the data shown in (d).

(cid:252)
Finally, to put kernels in the picture, we change the notation from x

(cid:11). For
now, it remains a linear model. To deal with nonlinearities in the input space, we can now
introduce a feature map, xi

i x j to (cid:10)xi, x j

), which gives us

→ Φ(xi

=

LD

n
(cid:88)

i=1

α

i

− 1
2

n
(cid:88)

n
(cid:88)

i=1

j=1

α

α

i

j yi y j

(cid:10)Φ (xi

) , Φ (cid:0)x j

(cid:1)(cid:11) .

(4.26)

(cid:1)(cid:11), appearing. In this kernel
So we ﬁnally see our kernel function, K(xi, x j
formulation, the margin we maximize is between the hyperplane and the support points in
the feature space. Therefore, the SVM problem boils down to maximizing LD numerically to
ﬁnd the coefﬁcients α
i and the parameters of the kernel function K, e.g., using sequential
minimal optimization [62]. Once these are known, the hyperplane separating the classes in
the typically high-dimensional space is also known. With the optimal hyperplane ˆf we can

) = (cid:10)Φ (xi

) , Φ (cid:0)x j

8There is a variant of this approach designed for regression called support vector regression that is almost

identical with KRR but minimizes a different form of a loss function.

81

−101y(a)(b)−101x−101y(c)−101x(d)Gaussian processes and other kernel methods

Figure 4.3: A linear SVM applied to a data set that is not linearly separable. The
SVM tries to minimize the total distance of all misclassiﬁed data points ξ
i called slack
< 1 for points on
≥ 1 for misclassiﬁed xi (here, i = 1, 3, 4) and 0 < ξ
variables. ξ
the correct side of the decision boundary but within the margin (here, i = 2).

i

i

then make predictions at an arbitrary test point x∗:

ˆf (x∗) = θ(cid:252)φ(x) + θ
0

= (cid:88)
i

α

i yi

〈φ (xi

) , φ (xi

)〉 + θ
0

= (cid:88)
i

α

i yi K (xi, x) + θ

0 .

(4.27)

Finally, in order to turn this value into a class prediction, we take the sign of ˆf as the corre-
sponding class label. Note that the choice of the kernel function here matters as discussed in
the opening of section 4.2. We visualize this problem in Fig. 4.2.

While we can put lots of effort into ﬁnding a kernel function that renders our problem
linearly separable, we can also relax the problem by allowing some misclassiﬁcation. Let us
thus move to the problems that are not linearly separable. The derivations above still hold with
one modiﬁcation: we now allow a number of data points to be on the wrong side of the margin,
) ≥ 1 − ξ
+ θ
as shown in Fig. 4.3. This modiﬁes our constraint from Eq. (2.40) to yi
i,
0
where ξ
= 0 if the data point is on the correct side of the margin. The variables ξ
i are often
referred to as slack variables. We can incorporate the control over how “wrong” the hyperplane
can be by adding another constraint, i.e., (cid:80)
< C = const, which also adds terms to the
ξ
Lagrange function:

(θ(cid:252)xi

i

i

i

L = 1
2

|θ|2 − (cid:88)

i

α

i

[ yi

(θ(cid:252)xi

+ θ
0

) − (1 − ξ

)] + C

i

(cid:88)

ξ

i.

i

(4.28)

Now, we maximize the margin while minimizing the violation of the margin constraints. This
loss function is still a quadratic program but now has also a largely increased number of op-
timization variables (one slack variable per data point). As previously, instead of minimizing
L you can maximize LD. Note that in this case you get an additional regularization term with
magnitude C. Large C allows for more misclassiﬁed data points but promotes simpler deci-
sion boundaries. In contrary, small C forces model to better ﬁt training data, sometimes at the
expense of the validation data.9

9Beware of various deﬁnitions and notations regarding regularization strength, in particular in SVMs. For

example, in Scikit-learn decreasing a hyperparameter C corresponds to more regularization.

82

ξξξξ1*2*3*4*Gaussian processes and other kernel methods

Finally, we can make a connection between SVMs and KRR. We started by saying that we

need to minimize |θ|2 subject to the following conditions:

(θ(cid:252)xi

yi

) ≥ 1 − ξ

i with ξ

i

≥ 0

+ θ
0
(cid:88)
ξ

< C = const .

i

If we simply write

i

ξ

i

≥ 1 − yi

(θ(cid:252)xi

+ θ
0

)

and vary θ and θ

0 to ﬁnd the minimum of the following function:

L (θ, θ
0

) = 1
2

|θ|2 + C

(cid:88)

i

[1 − yi

(θ(cid:252)xi

+ θ
0

)]+ ,

the problem is equivalent to minimizing the following function:

(4.29)

(4.30)

(4.31)

which is the same as

L (θ, θ
0

) = 1
2C

|θ|2 +

L (θ, θ
0

) = 1
2

λ|θ|2 +

n
(cid:88)

i=1

n
(cid:88)

i=1

[1 − yi

(θ(cid:252)xi

+ θ
0

)]+ ,

(4.32)

max [0, 1 − yi f (xi

)] .

(4.33)

Finally, let us compare it to the functional of KRR from Eq. (4.16): the regularization term
is the same, the only difference is that KRR uses the squared error loss, while SVMs uses
a function called the Hinge loss.

4.2.3 Gaussian processes

So far, we have already covered powerful and general regression tools. We learned about
the kernel trick and what it means to learn in feature spaces rather than in the input space.
Moreover, we have seen how this is useful in particular for high dimensional features spaces
and examined in more depth some tools which manifestly use the kernel trick to perform
good learning tasks. In this section we cover an additional tool, namely Gaussian processes
(GPs). However, since we have already introduced powerful regression models such as KRR,
a natural question to ask is: why do we need another regression model?. The short answer is
that Gaussian process regression (GPR) allows to do what KRR does not: to calculate Bayesian
uncertainties on our predictions. As we shall see, this tool can be used for (at least) two
powerful applications: Bayesian optimization [155] and sampling. In the remainder of this
section we describe what GPs are and how GPR works.

Considering a linear regression function in an m-dimensional space, as in section 2.4.1:

f (x) = (θ

0, θ

1 . . . θ

)(cid:252) (1, φ

m

(x), . . . , φ

(x))

m

1

where each of the functions φ

i

(x) is some nonlinear function of x such as



φ

i

(x) = tanh

wi



x j

+ bi



m
(cid:88)

j=1

and x is m-dimensional, x = (x1, . . . , xm

)(cid:252).

83

(4.34)

(4.35)

Gaussian processes and other kernel methods

Figure 4.4: Sketch of a Bayesian NN. The features in the hidden layer (blue) are
multiplied by random variables as shown in Eq. (4.34). This way, the NN is no longer
deterministic and its output f (x) given the input x is itself a random variable.

0, . . . θ

Equation (4.34) can be visualized as the NN from Fig. 4.4. If all the weights of the net-
work are ﬁxed, the network is deterministic and maps any given data point x to a single value
f (x). However, if we now assume all the parameters θ
m to be instantiations of random
variables, distributed according to some distribution, we incorporate randomness into the net-
work’s output. The output of the NN for a ﬁxed x thus itself becomes a random variable. Such
an NN thus becomes a Bayesian NN. Assuming that all θ
i are i.i.d. random variables and
letting N → ∞, the central limit theorem10 tells us that the distribution over the output of
the Bayesian NN [156] is Gaussian. Because the distribution must remain Gaussian for any
given input x, f (x) is a ﬁrst example of a GP applied in the context of NNs. The upshot of this
preliminary example is that by incorporating noise into the model, under the assumption of
inﬁnitely many independent randomly sampled parameters, the output of the model is, again,
Gaussian. The advantage of all this, compared to what we have previously analyzed, is that
one can obtain a closed-form expression for log marginal likelihood and the predictive dis-
tribution. So, a GP can be viewed as a probability distribution over possible functions ﬁtting
a set of data points. Its output is an instantiation of a random variable, distributed according
to a multivariate density.

Our goal is to infer a function that describes a given data set. The data are usually noisy

which we model with Gaussian-distributed noise with zero mean and variance σ2:

y = f (x) + ε,

ε ∼ N (0, σ2) .

(4.36)

10The central limit theorem states that the sum of many independent random variables is approximately normally
distributed. E.g., if you roll two six-sided dice multiple times, the sum of the obtained results converges to the
Gaussian distribution centered at seven in the limit of an inﬁnite number of rolls.

84

φ1(x) φ2(x) φ3(x) φ4(x) φ5(x) φ6(x) φd(x) 1f(x)x1x2x3xmGaussian processes and other kernel methods

Let us ﬁrst consider a linear model, i.e., φ being the identity map, Eq. (4.34) becomes:

f (x) = θ(cid:252)x

(4.37)

hence a linear model. Let us now assume each data point xi is independent of the others. For
any given point, the model likelihood is expressed as p(yi
). Due to the independence,
for a labeled training data set D = (X, y) the joint likelihood reads (see also section 2.3):

| θ, xi

p(y | θ, X) =

n
(cid:89)

i=1

p(yi

| θ, xi

) .

(4.38)

Because of our assumption of Gaussian distributed noise (see Eq. (4.36)), we can explicitly
rewrite each term of the product from the right-hand side of the equation above as:

p(yi

| θ, xi

) = 1
(cid:112)

2πσ

(cid:20)

( yi

−

exp

− θT xi
2σ2

(cid:21)

)2

.

(4.39)

Hence, the model likelihood given the data D is a product of Gaussians. Thus, Eq. (4.38) can
be explicitly rewritten as:

p(y | θ, X) =

n
(cid:89)

i=1

p(yi

| θ, xi

) =

1
(2πσ2)n/2

exp

(cid:20)

−

|y − Xθ|2
2σ2

(cid:21)

.

(4.40)

Our goal now is to use the Bayes’ theorem to calculate the posterior over the weights θ (see
section 2.3). Using Eq. (2.19)), we obtain

p(θ | y, X) = p(y | θ, X)p(θ)

p(y | X)

(4.41)

where p(y | X) is just a normalization constant not depending on θ and p(θ) and represents
In principle, we have freedom over choosing the prior.
our prior (again see section 2.3).
Nevertheless, it would always be better in practice to choose this prior wisely. Hence, one
should choose it according to the prior knowledge on the problem at hand. For instance,
thinking about physical problems, one can use some context or prior knowledge on the system
to set the prior appropriately. Of course, the better the prior is chosen, the more effective the
model is. One though needs to be careful in this choice since restricting the prior or choosing
it to be subjected to unreasonable constraints would make the whole learning process much
harder. More speciﬁcally, a bad prior translates into a requirement of more data in order to
build a good model. Thus, in light of all the considerations above we set the prior to be

θ ∼ N (0, Σ

)

m+1

(4.42)

which is a joint normal distribution with zero mean and covariance matrix Σ
under the assumption of independent parameters, Σ
trix.11

m+1. For instance,
m+1 can be chosen to be a diagonal ma-

Getting back to the posterior, we can collect all the θ-independent terms under a normal-

ization constant A and rewrite the likelihood using Eq. (4.38) and (4.41) as

p(θ | y, X) = p(y | θ, X)p(θ)

p(y | X)

= 1
A

(cid:20)

−

exp

(cid:21)

|y − Xθ|2
2σ2

exp (cid:2)−θ(cid:252)Σ−1

m+1θ(cid:3) .

(4.43)

11As we include a possible bias term in Eq. (4.34), we have m + 1 tunable parameters.

85

Gaussian processes and other kernel methods

Rearranging the expressions in the exponentials, the posterior becomes:

p(θ | y, X) = 1
A

exp

(cid:149)

− 1
2

(cid:152)
(θ − µ)(cid:252)C(θ − µ)

where we used the following deﬁnitions

µ = (X (cid:252)X + σ2Σ−1
m+1
C−1 = (X (cid:252)X + σ2Σ−1
m+1

)−1X (cid:252)y
)−1

(4.44)

(4.45)

(4.46)

with µ being the posterior mean and C−1 the posterior covariance matrix. Finally, Eq. (4.44)
gives us access to the analytical form for the distribution of the model parameters p(θ | y, X)
given the data D. This, again, is a normal distribution with updated mean and covariance.
These new values intrinsically posses the information of the training data as inferred from their
analytical forms. Hence, p(θ | y, X) ∼ N (µ, C−1). Once we have our updated distribution
over our model’s parameters, the next step is to predict the output y ∗ for a previously unseen
data point x∗. To do this, we need to multiply the posterior with the likelihood for y ∗ and
integrate over all possible parameters. This yields:

(cid:90)

p( y

∗ | x∗) =

p( y

∗ | x∗

, θ)p(θ | y, X) dθ

Rm+1
(cid:90)

Rm+1

∝

exp

(cid:20)

−

(cid:21)

( y ∗ − θ(cid:252)x∗)2
2σ2

exp

(cid:149)

− 1
2

(cid:152)
(θ − µ)(cid:252)C(θ − µ)

dθ .

(4.47)

(4.48)

It is easy to notice that this distribution is again going to be Gaussian. We can also analytically
compute the conditional mean and variance:12

ˆµ = x∗(cid:252)µ = x∗(cid:252)σ−2C−1X (cid:252)y
ˆσ2 = x∗(cid:252)C−1x∗

.

(4.49)

(4.50)

The mean can be used to make predictions while the variance gives the uncertainty over such
estimation. It is now time to compare these results with previously introduced methods:

Linear Regression: x∗(cid:252) ˆθ = x∗(cid:252)(X (cid:252)X)−1X (cid:252)y
Linear Ridge Regression: x∗(cid:252) ˆθ = x∗(cid:252)(X (cid:252)X + λ1)−1X (cid:252)y

Linear GPR:

ˆµ = x∗(cid:252)(X (cid:252)X + σ2Σ−1
m+1

)−1X (cid:252)y.

(4.51)

(4.52)

(4.53)

This entire derivation, which we have been doing for the linear case, can easily be generalized
to nonlinear regression. In such case, we map x (cid:55)→ φ = φ(x), embedding our input data with
It follows that X (cid:55)→ Φ = φ(X)
a feature map into a potentially high-dimensional space.
and the conditional mean and variance can be derived accordingly. Substituting the terms
appropriately and doing a little bit of math adjustments, under the assumption of unit variance
Σ

= 1, we obtain:

m+1

ˆµ = φ∗(cid:252)Φ(cid:252) (cid:2)Φ(cid:252)Φ + σ21(cid:3)−1
ˆσ2 = φ∗(cid:252)φ∗ − φ∗(cid:252)Φ(cid:252) (cid:2)Φ(cid:252)Φ + σ21(cid:3)−1 Φφ∗

y

.

(4.54)

(4.55)

Looking closely at these expressions, we can rewrite them in terms of the kernel function K:

ˆµ = k(cid:252) (x∗) [K + σ21]−1y
ˆσ2 = K(x∗

, x∗) − k(cid:252) (x∗) (cid:2)K + σ21(cid:3)−1

k (x∗)

(4.56)

(4.57)

12with C = [σ−2X (cid:252)X + Σ−1
m+1

] already deﬁned above

86

Gaussian processes and other kernel methods

where we used the following deﬁnitions:

, x∗) ,

φ∗(cid:252)φ∗ = K(x∗
= k (x∗)
(Φφ∗)
(Φ(cid:252)Φ)
= Ki j

i j

i

= K(xi, x∗) ,

i

= K(xi, x j

).

(4.58)

(4.59)

(4.60)

One important remark with respect to the covariance matrix: though we set Σ
m+1 to be the
identity, this might not be the best choice in practice. Nevertheless, this does not pose a
problem as one can always rewrite the identities above such that they take the covariance
matrix into account (in particular, Eq. (4.58) becomes φ∗(cid:252)Σ
φ∗(cid:252) = K(x∗, x∗)). We detail
in Appendix C that the kernel trick does not interfere with the Gaussian prior assumption of
Eq. (4.42) for the parameters θ.

m+1

Thus, looking at the conditional mean from Eq. (4.56) one can directly see the parallelism
with the result of KRR from Eq. (4.21) where our regularization strength λ corresponds to
the data noise assumption, parametrized by σ2.13 This comes to no surprise as the condi-
tional mean from Eq. (4.56) simply is the MLE introduced in section 2.4.1. However, GPR
also yields the uncertainty of the prediction. Furthermore, we see, once again, the conse-
quence of the representer theorem in Eq. (4.14) on the form of the conditional mean ˆµ, i.e.,
α = (K + σ21)−1y.

4.2.4 Training a Gaussian process

In the previous section, we have seen what a GP is, how we construct it and how it allows us to
obtain an analytical expression for the conditional mean and the Bayesian uncertainty for the
output over one (or more) unseen data point(s) x∗. We understand that the performance of
the GP strongly relies on the choice of the prior and on the amount of data the model is exposed
to. So at this point one natural question which might arise is: how do we train a GP? Let us
dive into this. First and foremost, we need to choose an appropriate kernel function K which
) accordingly. The parameters of this function (e.g.,
deﬁnes the kernel matrix Ki j
as in Table 1) are tuned in order to maximize the so-called marginal likelihood p(y | X). This
name comes from the fact that this quantity is obtained from the Bayes’ theorem Eq. (4.41)
when marginalizing over the set of model parameters (i.e., taking the integral over θ). Here,
our goal is to express the marginal likelihood of a GP in terms of the kernels. To this end, we
choose the covariance matrix of the GP to be the kernel matrix such that Cov(x, x(cid:48)) = K14
).
which turns out to be equivalent to the choice of the prior made previously θprior
m+1
A mathematical justiﬁcation for this is provided in Appendix C. Our ultimate goal is to evaluate
the marginal likelihood

= K(xi, x j

∼ N (0, Σ

p(y | X) =

(cid:90)

Rm+1

p(y | θ, X)p(θ | X) dθ .

(4.61)

Given that the prior’s covariance is the kernel matrix and the integrand is a product of two
Gaussians, it is actually possible to express p(y | X) in terms of the kernel matrix K. Working
with the logarithm of the marginal likelihood it follows from Appendix C that our objective of
the training process is

log p(y | X) = − 1
2

y(cid:252) (cid:0)K + σ21(cid:1)−1

y − 1
2

log

(cid:12)
(cid:12)K + σ21(cid:12)

(cid:12) − n
2

log 2π .

(4.62)

13The difference in the two constants actually only arises from our choice of the covariance matrix. If we had
= τ21 instead, the difference would vanish completely and the two methods would yield the same

chosen Σ
estimator.

m+1

14On the notation: when providing the inputs of the covariance matrix we use Cov while when they are implicit

we just use C.

87

Gaussian processes and other kernel methods

Algorithm 5 Bayesian Optimization
Require: initial data set D0
Require: initial surrogate model GP trained on D0 with mean and variance ˆµ, ˆσ2
Require: acquisition function to be maximized

= {(X, y)}

for iteration t < Tmax do

a(x, ˆµ, ˆσ)

Sample a set of candidate points Xcand
x∗ ← maxx∈Xcand
y ∗ ← fBB
D ← Dt
Update the surrogate model’s parameters ˆµ and ˆσ2

(x∗) at x∗
(cid:83){(x∗, y ∗)}

(cid:46) batch optimization

(cid:46) update data set

end for
return ˆµ and ˆσ2

(cid:46) prediction and uncertainty of the surrogate model

When training a GP one aims to ﬁnd the parameters of the kernel function that maximize the
logarithm of marginal likelihood from Eq. (4.62). Moreover, a GP does the ﬁne-tuning of the
parameters automatically (either by gradient descent or cross validation), while in case of the
SVM one needs to validate the kernel parameters instead. On the other hand, the training
a GP requires the inversion of the kernel matrix whose size coincides with the training set.
This already gives an intuition why GPs are the tool of choice in a regime of a few data points
where they can be very effective and relatively cheap.

4.3 Bayesian optimization

In the previous section, we have discussed Gaussian process regression (GPR) and how to train
a Gaussian process (GP). Furthermore, we have shown that GPs allow to derive an analytical
form for the estimate of the output for an test data point x∗, conditioned to a set of given
data D, in a similar fashion to KRR. Moreover and contrary to KRR, GPR also comes with
a prediction uncertainty over the conditional mean. This is of a great relevance as it can
be used for Bayesian optimization (BO) [155]. As such, depending on the problem and the
use-case one approach may be preferred to the other. GPR is certainly more versatile but also
more expensive to train compared to KRR and one may want to prefer the latter toward a more
efﬁcient computation when uncertainty over the estimate is not needed.

BO is a technique used for the optimization of expensive black box functions where gra-
dients can not be easily computed or estimated (i.e., experimental setups). Here, the term
expensive is very important.
Indeed, the optimization of black box functions is the general
goal of a big set of ML-models and techniques. Such optimization usually relies on efﬁcient
computation, arbitrarily many data and so forth. However, this might not always be the case.
Sometimes, we might be facing problems where the amount of data is rather limited, or the
routine to extract additional experimental data is very expensive. In this context, the interplay
between GP and BO becomes extremely important. It is also worth mentioning that contrary to
the optimization methods used in most ML-approaches we have encountered so far (in partic-
ular NNs), BO is a gradient-free method. Therefore, it is particularly well suited for functions
that are very difﬁcult or expensive to evaluate. The way BO works in the context of GPR is that
BO takes GPs as surrogate models of the black-box function to be optimized. Recalling the results
of the previous section, GPR gives us access to a conditional prediction along with an estimate
for its uncertainty. This said, the next important thing to notice is that BO is an iterative pro-
cess. This process works as follows (see Algorithm 5 for the pseudo-code): The BO procedure
starts with a few evaluations of the black box function at some random locations. We refer
= {(X, y)}. These evaluations are used to train a ﬁrst version of
to this initial data set as D0

88

Gaussian processes and other kernel methods

a GP, hence, we can think of those to be our training data.15 Once we have our ﬁrst surrogate
model, we introduce the so-called acquisition function. The acquisition function is typically
a function of both ˆµ and ˆσ and essentially tells us where to perform the next evaluation x
in order to maximize the knowledge we gain about the underlying black-box function. In the
next section, we see how the acquisition function looks like. For now, we can just think of it
to be an arbitrary function a(x, ˆµ, ˆσ). The prediction and its uncertainty are ﬁxed given a sur-
rogate GP prior. Thus, the acquisition function is only a function of a new candidate point x.
Our goal is now to ﬁnd such candidate point to be as informative as possible. As such, the next
point to evaluate by maximizing the acquisition function, i.e., x∗ := maxx∈D a(x, ˆµ, ˆσ), where
D is the domain of x. Once the new target location x∗ is found, the next step is to evaluate
the black box function such that y ∗ = fBB
(x∗). The result of the evaluation is appended to the
(cid:83){(x∗, y ∗)} and a new, less uncertain, surrogate model
training set for GP such that D = D0
is trained on the updated D. From this point on, the iteration starts over: every time we up-
date the surrogate model, we have new predictions and uncertainties, hence a new acquisition
function. At each step of the BO a new point is thus added to D and the entire process goes
on until a maximum number of iterations Tmax is reached or some convergence criterion is
met. The plot in Fig. 4.5 shows how three subsequent steps of BO results in an increasingly
more certain surrogate model of the underlying black box function (dark green line). This
illustrates how BO can be used in the context of active learning where the training data set is
built step-by-step with the aim of minimizing the number of training points while maximizing
the information it contains.

BO with GPR has the following advantages over other regression models:

• smaller training sets are enough (rule of thumb: 10× number of trainable param-

etersa),

• provides uncertainties to the predictions,

• gradient-free so the optimization works even if you cannot compute gradients.

aA suitable choice for the kernel typically lowers the number of function calls. Moreover, it is preferable
to have smaller training sets for this method. Equation (4.56) shows that the kernel matrix needs to be
inverted for each trial kernel which adds a computational constraint.

The acquisition function

In the previous section, we have brieﬂy described the idea of BO and how it operates combined
with GPs. In this context, we have introduced the acquisition function. This quantity is very
important as it represents a mathematical technique which guides the exploration of the entire
parameter space during the BO-routine. We have previously deﬁned the acquisition function
as a general function of x, the surrogate’s prediction and its uncertainty. There are different
kinds of acquisition functions, and most of the time the choice is problem-dependent. How-
ever, most importantly, its mathematical form should always incorporate the trade-off between
exploration and exploitation. In other words, the goal of the acquisition function is to evaluate
the usefulness of the next data location to look at in order to achieve the maximization of the
surrogate model of our black box function and thus to approximate the target function with
lower uncertainty. As such, the ultimate goal in BO is to ﬁnd the next point to evaluate by
maximizing such acquisition function. One example, commonly used and easy to interpret is

15Since we work in a Bayesian setting, the data noise is taken into account in Eq. (4.36) and all predictions are

based on top of that assumption.

89

Gaussian processes and other kernel methods

Figure 4.5: Example of how BO can be applied to GPR. A GP represents a surrogate
model (light green lines) trained on an initial set of observations (red dots in the
upper left panel). At each BO-step, a new point is added to the previous set of obser-
vations such that the surrogate model becomes increasingly certain as more points
are added. The dark green line represents the underlying black box function.

the Upper Conﬁdence Bound (UCB):

aUCB

(x, ˆµ, ˆσ) = ˆµ(x) + β ˆσ(x)

(4.63)

where β ≥ 0 is an arbitrary parameter which ideally should be tuned during the optimization
routine. Here, the ﬁrst term drives the exploitation while the second drives the exploration.
In the remainder we refer to those as the exploitation and the exploration terms respectively.

Depending on the value of β, the exploration term might dominate in the maximization.
By looking at Eq. (4.63) it is immediately clear that a new candidate point with the higher
variance is preferred as the model rewards the evaluation of currently unexplored regions of
the domain. That is not surprising as the model seeks to explore what it does not know yet.
With respect to the mean, according to the UCB, higher values for the mean are preferred. That
is because by deﬁnition, we are seeking for an upper bound hence enhancing sampling in the
upper quartile of the surrogate model. In other words, in the extreme case where β (cid:29) 0, the
exploration dominates, hence regions of higher variance are preferred (see Fig. 4.6 leftmost
plot).

When instead β → 0, the acquisition function becomes far more conservative, hence sam-
ples aggressively around the best solution, i.e., exploiting the region where the surrogate model
feels conﬁdent as visible in the rightmost panel of Fig. 4.6. In Fig. 4.6 the middle plot shows
a good balance between the exploration and the exploitation. Hence, for two candidate points
with comparable predicted mean, the one with higher uncertainty is preferred. As a con-
sequence, the acquisition function, at least in the beginning of the optimization, prefers to

90

1123Gaussian processes and other kernel methods

Figure 4.6: Selection of new candidate points via Bayesian optimization (BO) using
the Upper Conﬁdence Bound acquisition function. The target function is represented
by the blue dashed line. The solid orange line is the surrogate model (GP) while the
orange shading represents its uncertainty. The left most plot exhibits exploitative be-
havior, i.e., the most selected points are around the peak(s). Contrarily, in the right-
most plot, the parameter choice for β heavily enforces exploration. As such, new sam-
pled points (red dots) are evenly distributed though some part of the domain (e.g.,
x ∼ 2 would require more exploitation). The middle plot shows a trade-off between
exploration and exploitation: the sampled candidates are well-distributed across the
entire domain thus approximating the target function efﬁciently even around the
peak(s) and boundaries.

explore rather than exploit.

Moreover, looking at the analytical form from Eq. (4.56) (which appears as the ﬁrst term
in Eq. (4.63)), the acquisition function might not always be easy to maximize (minimize) in
practice. Therefore, one needs to leverage efﬁcient numerical optimization routines. As acqui-
sition functions are highly non-convex, what is done in practice is to do batch optimization. At
each BO step, starting points xcand are randomly sampled over a speciﬁed domain D, then one
takes the best one of the sampled points (that maximizes the acquisition function) as the ac-
tual candidate. Other prominent examples of widely used acquisition functions are: Expected
Improvement, Noisy Expected Improvement, Probability of Improvement. For a deeper yet
more detailed overview on other type of acquisition functions the reader is referred to [157].

4.4 Choosing the right model

Having introduced the powerful toolbox of kernels, a natural question arises: Suppose we are
given a set of “noisy” data points, forming a data set D, and two distinct models M1 and
M2 possibly based on two different kernels, which one should we choose? This is the central
question behind the ﬁeld of model selection. To answer this question, we again take a Bayesian
approach (see section 2.3). Applying Bayes’ theorem from Eq. (2.19) to each model Mi yields

p(Mi

| D) =

p(D | Mi

)p(Mi

)

p(D)

.

(4.64)

91

−20246810x−2.50.02.55.07.510.012.515.0f(x)=0.1(a)−20246810x=4.0(b)True functionGPGP confidenceSampled points−20246810x=20(c)Gaussian processes and other kernel methods

We combine the expressions of the two models to obtain

p(M1
p(M2

| D)
| D)

=

p(D | M1
p(D | M2

)
)

p(M1
p(M2

)
)

.

(4.65)

Here, the ratio of the posterior probabilities P(Mi
abilities P(Mi

) times the so-called Bayes factor

| D) is equal to the ratio of the prior prob-

p(D | M1
p(D | M2
Equation (4.65) gives us a ﬁrst answer to our question: In a Bayesian framework, the ratio of
posterior probabilities can be used to decide which model is superior given the data at hand,
i.e., the model with the larger posterior probability is superior. In scenarios where we do not
know anything about the data, we can set the prior probabilities equal to each other, which
leaves us with the Bayes factor

(4.66)

)
)

.

p(M1
p(M2

| D)
| D)

=

p(D | M1
p(D | M2

)
)

.

(4.67)

To calculate the Bayes factor in Eq. (4.66), we need to compute p(D | Mi

) for each model
which can be viewed as a marginal likelihood, i.e., a likelihood function in which some variables
have been marginalized out (integrated out). Let us deﬁne the likelihood as p(D | θ, Mi
), such
that the marginal likelihood can be obtained as

p(D | Mi

) =

(cid:90)

Rd

p(D|θ, Mi

)p(θ | Mi

) dθ ,

(4.68)

where we integrate over the distribution of model parameters θ ∈ Rd given by p(θ | Mi
).
Unfortunately, marginal likelihoods are typically hard to compute as they involve high-
dimensional integrals. Choosing a kernel with d parameters results in a d-dimensional integral
for its marginal likelihood.

Having encountered this problem, let us take a step back: When we train a model, we
minimize a loss function (or equivalently, we maximize the log-likelihood). Therefore, why
not simply choose the model that gives the lowest loss or largest likelihood? Intuitively, this
leads to overﬁtting. This intuition is formalized by the bias-variance trade-off (see section 2.2).
In particular, the bias-variance trade-off makes clear that the ideal model realizes an optimal
trade-off between the training error and the model complexity. Rather than choosing the
model that results in the lowest loss during training, we thus need to take its complexity into
account.

4.4.1 Bayesian information criterion

A computationally tractable criterion for model selection which takes model complexity into
account is the Bayesian information criterion (BIC) [158] deﬁned as

BIC = −2 max((cid:96)) + d log(n),

(4.69)

where max((cid:96)) is the maximum of the log likelihood, n is the number of training points, and
d is the number of model parameters. The lower the BIC, the better the model. Clearly, the
BIC reﬂects the trade-off between bias, here given by max((cid:96)), and the model complexity as
measured by d log(n). Moreover, it turns out that the BIC is very close to the logarithm of the
marginal likelihood in the large n-limit [159]:

log p(D | Mi

) ≈ log p(D | θ∗

, Mi

) − d
2

log(n) ,

(4.70)

92

Gaussian processes and other kernel methods

where θ∗ are the model parameters which maximize the likelihood. This expression reveals
that the model selection criterion given in Eq. (4.65) based on the Bayesian approach does
indeed take the model complexity into account. Moreover, we see that the criterion can be
used to estimate the posterior probability of a model Mi as

(cid:1)

=

pi

exp(cid:0)− 1
2 BICi
N
Here, the normalization constant N = (cid:80)
/2 ensures that each model Mi is assigned
i e−BICi
a valid probability pi to enable comparability. As such, the BIC gives us a tractable way to select
models according to the criterion given in Eq. (4.65). In fact, BIC is asymptotically consistent
as a model selection metric: Given a family of models, including the model underlying the
data, the probability that BIC correctly selects the model underlying the data approaches one
as n → ∞.16

(4.71)

.

Inspired by these ﬁnding, we can adapt the criterion to GPR based on the log marginal
likelihood which is optimized during training (at ﬁxed kernel parameters) and the number of
kernel parameters in the GPR. This criterion is computationally tractable and thus allows one
to select between different kernels in the regression task using GPs.

4.4.2 Kernel search

Choosing the right kernel is crucial when using kernel-based method, as we have seen, e.g.,
for the performance of SVMs for different kernels in Fig. 4.2. When performing Gaussian
process regression (GPR) in a naive manner, we simply select a ﬁxed kernel from a set of
conventional kernels such as listed in Table 1. We then optimize their hyperparameters by
maximizing the marginal likelihood during training. There are now several possible routes
toward achieving a more accurate model. Clearly, we may improve the model accuracy by
providing more training points. However, keeping the number of training points low is one
of the main advantages of GPR compared to other methods and constituted our main initial
motivation. At a ﬁxed number of training data the result from GPR can only be improved
through a better kernel. Moreover, while BO is guaranteed to converge, the exact number
of iterations may vary drastically. The choice of a good kernel can signiﬁcantly speed up the
convergence of BO.

The construction of good kernels ultimately boils down to a (possibly high-dimensional)
optimization problem [161]. This happens, for example, when constructing a good kernel
through optimization of the kernel hyperparameters itself. The key challenge is posed by the
fact that the parametric form of the kernel must be proposed by the user itself. This is a non-
trivial task which relies on trial-and-error – even for experts. In Refs. [124,161,162] the kernel
learning problem was reframed as a search tree problem (see Fig. 4.7): the space of parametric
forms of kernels is constructed as a tree which can be searched systematically in an automated
fashion, where the powerful BIC is used for the kernel selection and new kernels are proposed
via composition.

We start by selecting each kernel from a set of conventional kernels and training a GP for
each of them on the same data set. Then, we select the one which achieves the lowest value of

16While the BIC criterion approximates the log marginal likelihood in the large n-limit, it can still be applied as
a heuristic model selection criterion at low values of n and can be conﬁrmed empirically to often still yield good
results. There exist many other model selection criteria (see [159] for a review), a popular one being the Akaike
information criterion [160] which closely resembles the BIC. The crucial difference between the BIC and many
other methods is its asymptotic consistency. One may question the importance of asymptotic consistency due to
the fact that the ground-truth model typically is not present in the candidate set of models in practice.

93

Gaussian processes and other kernel methods

Figure 4.7: Illustration of the search tree behind the algorithm for the optimal ker-
nel construction in GPR.
It utilizes the BIC for the model selection introduced in
Eq. (4.69). For an overview of possible kernel functions and corresponding abbrevi-
ations, see Table 1. Adapted from [124].

BIC as given by Eq. (4.69) (highlighted in blue). This kernel serves as the base kernel for the
subsequent round where it is combined with the various kernels from the starting set to create
new candidate kernels by forming products or combining them linearly. Again, the best one is
selected according to the BIC and the process is repeated. The complexity of the model, i.e., of
the composite kernel, increases as one progresses in the search tree. Eventually, increasing the
kernel complexity further leads to overﬁtting, i.e., does not improve the BIC value compared
to the kernel of the previous round and the algorithm is stopped. The algorithm can also be
stopped prematurely if the number of kernel parameters becomes large and the associated
training simply takes too long to be practical. Other than greedily searching the tree, the
reformulation of the kernel construction as a search tree problem opens up the possibility for
more advanced strategies which could yield better kernels more efﬁciently [163].

4.5 Bayesian optimization and Gaussian processes for quantum sciences

In the previous sections, we have motivated Gaussian processes (GPs) and Bayesian optimiza-
tion (BO) as powerful methods which together allow us to build expressive ML models. Im-
portantly, they are equipped with an intrinsic measure for uncertainty and can be trained using
a small amount of training data. In this section, we discuss how these two methods can be
useful in the context of quantum sciences, as sketched in Fig. 4.8. In particular, GP and BO
can be used to tackle inverse problems, to construct accurate extrapolations, and to increase
the accuracy of quantum dynamics calculations.

4.5.1 Inverse problems

As explained in section 4.3, BO is very useful when you need to optimize black-box functions
that are expensive to evaluate. This property proves extremely useful in inverse quantum prob-
lems aiming at ﬁnding theoretical description of the system by experimentally measuring its
observables. The idea is related to a popular experimental approach known as optimal control.
The optimal control approach aims at designing external ﬁeld parameters that yield the de-
sired quantum dynamics. It is usually achieved by a feedback loop which iteratively modiﬁes
experimental parameters such that they yield system dynamics advancing to the target one.

94

NokernelRBFMATRQLINRQxLINRQ+MAT.........RQ+RBF...RQxLIN+RBFRQxLINxRBFRQxLIN+MATGaussian processes and other kernel methods

Figure 4.8: Illustration of the three main classes of problems in quantum sciences
(marked in yellow) that have been successfully tackled with BO and GPs.

We can imagine applying a similar feedback loop for the inverse quantum problems. It
would consist of iterative modiﬁcations of parameters of the theoretical description (such as
Hamiltonian parameters) till the observables predicted theoretically agree with those mea-
sured experimentally. However, solving the iterative inverse quantum problem is challenging.
Each iteration requires an additional run of theoretical calculations, e.g., the numerical so-
lution of the Schrödinger equation, which is time-consuming. The optimization itself is also
difﬁcult as we do not explicitly know the range of parameters that need to be explored. Finally,
the curse of the scaling of the Hilbert space dimension with the complexity of the quantum sys-
tems is deﬁnitely not helping. How to make it more feasible? Both inverse quantum problems
and optimal control become easier when the expensive black box (either the experimental set-
up or the theoretical calculations) is replaced by a trained surrogate ML model such as a GP.
Finally, instead of a blind search for the optimal parameters, we can employ BO.

As a practical example of the inverse problems solved with BO and GPs, let us consider
the application to scattering experiments. Outcomes of such experiments are determined by
the microscopic interactions between scattered particles. We have a quantum theory that de-
scribes these interactions and can predict the outcome of such scattering events. Therefore,
our aim, in case of an inverse problem, may be to infer these microscopic interactions from
the experiment. More concretely, the authors of Ref. [166] aimed to recover a global potential
+ H, using as little
energy surface (PES)17 governing the chemical reaction H + H2

−−→ H2

17A potential energy surface (PES) describes interactions between some particles. As a result, it models land-
scapes of chemical reactions, which can be used to predict reactive pathways and ﬁnal products. Traditionally, it
is constructed as an analytic ﬁt to many, usually costly ab initio quantum chemical calculations of potential energy
for reactants for various relative positions.

95

QUANTUMPROBLEMObservablesHamiltonian parametersInverseproblemsBetter accuracyof quantumdynamicsExtrapolation in Hamiltonianparameter spaceGaussian processes and other kernel methods

Figure 4.9: Examples of feedback loops whose optimization becomes feasible when
implemented with BO and GPs. (a) x corresponds to PES, f (x) is quantum scat-
tering calculations taking the PES as an input, T is the difference between reaction
probabilities calculated by f (x) and measured in the experiment across various col-
lisional energies. Search for optimal x would require minimization of T via opti-
mization of f (x). It becomes feasible when we surrogate f (x) with one GP and x
with another GP and apply BO. (b) x are Hamiltonian parameters, f (x, t) is the
time-dependent observable f (e.g., molecular orientation or alignment), F (x) is the
time-dependent Schrödinger equation, T - difference between calculated and mea-
sured time-dependent observable f . When F (x) is surrogated by a GP, BO is used
to minimize T and ﬁnd x of the underlying Hamiltonian. Adapted from Ref. [164].

experimental measurements of the reaction rates (depending on the constituents’ translational
energy) as possible. The feedback loop that needed to be solved is presented in Fig. 4.9(a).
Firstly, they trained a GP to surrogate a quantum scattering theory on a series of PESs and
predicted reaction rates. Secondly, they modeled the PES with another GP. Finally, they used
BO to ﬁnd the three-dimensional potential energy surfaces (PESs) recovering the measured re-
action rates. Only eight iterations of BO (where every iteration rebuilds the PES completely)
were required to reach the accuracy of conventional approaches! Moreover, in this case a tra-
ditional approach of building a PES requires around 8 700 points – their GP was modeled
based only on 30 points!18 This impressive scaling is presented in Fig. 4.10. As a result, they
successfully surrogated two complex models (PES and quantum scattering calculations using
PES as an input) with two GPs trained on much smaller number of data points than needed
to build the original complex models. They also used this approach for a six-dimensional PES
of OH + H2, where BO beat the traditional approach with 290 points compared to 17 000
points.19

Another example of an inverse quantum problem is the task of inferring molecular prop-
erties from time-dependent observables. Authors of Ref. [164] tackled the reconstruction of
molecular polarizability tensors from the observed time evolutions of the orientation or align-

18Remember, these are not any 30 points, but points indicated by BO as needed for the optimal description.
19The number of points needed for efﬁcient BO scales roughly like 10 times the number of PES dimensions. We
can raise an interesting point which is, how are we even sure that we faithfully reproduce the PES if we build it
only from reaction probabilities? It may happen that we capture the reactive chemical channels accurately, but
the remaining parts of the surface are unconstrained and as a result may be nonphysical. One can argue that we
ultimately do not need a complete faithful reproduction of the underlying PES. We only need a PES that allows us
to accurately predict what we are interested in, here reaction probabilities. Note, however, that if we take a PES
built from a particular set of observables and we use it to calculate another type of observable, the result may be
wrong.

96

T(a)(b)f(x)xxf(x,t)TF(x)Gaussian processes and other kernel methods

+ H −−→ H + H2 reaction as
Figure 4.10: (a) The reaction probability for the H2
a function of the collision energy. The black solid curve is calculations from Ref. [165]
based on the surface with 8701 ab initio points. The dashed blue/orange/ green/red
curves are calculations based on the GP PES obtained with 22/23/30/37 ab initio
points. (b) GP model of the PES for the H3 reaction system constructed with 30 ab
initio points. R1 and R2 are the distances between atoms 1 and 2 and atoms 2 and 3,
respectively. Adapted from Ref. [166].

ment signals of SO3 and propylene oxide induced by strong laser pulses. The feedback loop
that was solved is in Fig. 4.9(b). They used a GP with a vector output whose each element
corresponded to a prediction of a chosen observable (orientation or alignment) in a different
time step. The GP was trained to surrogate the numerical integration of the time-dependent
Schrödinger equation given the Hamiltonian parameters. Interestingly, the authors showed
what we discussed already in section 4.4: that a proper choice of the kernel can result in
a two times faster convergence of BO. Analogous approaches were used for the reconstruc-
tion of scattering matrices of molecules from molecular hyperﬁne experiments [167] and for
optimizing energetic chemistry synthesis experiment [168].

4.5.2 Improving quantum dynamics, physical models, and experiments

GP and BO can also be used for transfer learning in the context of quantum dynamics calcu-
lations. These are typically very difﬁcult and one quickly has to rely on approximations. The
authors of Ref. [169] proposed to apply GPs to correct such approximate quantum calculations
on the example of molecular collisions and their cross-sections. The idea is to train a model
on a small number of exact results and a large number of approximate calculations, resulting
in ML models that can generalize exact quantum results to different dynamical processes.

Moreover, as the minimization of any function using BO bypasses the need of computing
gradients [155], successful applications of BO includes optimization of parameters of physical
models. The majority of such models do not have a close form solution and conventionally have
to be approximated numerically using ﬁnite differences. For example, Refs. [170,171] showed
that BO could efﬁciently optimize density functional models to improve their accuracy and
minimize the energy of the Ising model [172]. Furthermore, BO was recently used to generate
low-energy molecular conformers [173, 174], tuning the parameters of various models used
to simulate cis–trans photoisomerization of retinal in rhodopsin [175], and the optimization
of lasers [176–178]. BO has also been impactful in material science in chemical compounds
screening [179–184] and optimization of experimental setups [185–189].

97

Gaussian processes and other kernel methods

4.5.3 Extrapolation problems

The second class of problems that seems perfect for GPs are extrapolation tasks: given some
function values for data points in one regime the goal is to accurately predict the function
values of data points in different regimes. This section touches upon two possible applications
that are (1) learning PES from a possibly smallest number of ab initio calculations in one
regime and (2) predicting existence of quantum phases without knowledge of the full phase
diagram.

An example of a successful extrapolation in the case of PES-learning was shown in
Ref. [163] where authors studied the six-dimensional PES of H3O+. They trained GP mod-
els on 1000 ab initio geometries from a low-energy regime (up to ≈ 7000 cm−1) and checked
that the model predictions in higher energy regimes match the full calculations with a high
level of accuracy. If you doubt it, this result can be reproduced using the published code and
data [32]. It gets better! You can get similarly accurate extrapolations from a GP model trained
on 5000 molecular geometries of a 51-dimensional problem of a protonated imidazole dimer,
which contains 19 atoms [190]. The scaling of the extrapolation accuracy with respect to the
number of training points seems to be even more favorable for large molecules: high accuracy
was reached already for 1000 randomly sampled geometries of the 57-dimensional aspirine.20

Another example of extrapolation, this time in the space of Hamiltonians, is task of in-
ferring properties of other phases of a system given knowledge of one particular phase. In
Ref. [124] the authors proposed to train a GP on one phase of the system and expected the
model to predict phase transitions and properties of other phases. Let us start by discussing
how they achieved this for the example of the mean-ﬁeld Heisenberg spin model in the nearest-
neighbor approximation. They trained the GP on the free energy of the system in the high-
temperature regime, where the average spin magnetization is zero, far from the phase transi-
tion point. Then the trained GP was asked to extrapolate within the low-temperature regime,
and it predicted correctly both the location of the phase transition as well as the free energy
(and consequently non-zero magnetization) of the system as presented in Fig. 4.11(a).

The authors also applied this approach to a much more complex system21 whose Hamilto-

nian can be written in the following generic form:

H = H0

+ αH1

+ β H2 ,

(4.72)

where α and β are tunable parameters along which phase transitions occur. They trained
a GP in some parameter regime of the Hamiltonian and were able to successfully extrapolate
to others.22 This approach proves to be useful for such a class of Hamiltonians for another
reason. Usually, we are able to easily compute or measure the eigenspectrum in certain limits
of α and β, but not at arbitrary points within the parameter space. We can then train a GPs

20More precisely, the test energy mean absolute error was 0.177 kcal/mol. Is this error small? Are these PESs
accurate enough for modern spectroscopic applications? Spectroscopy discerns two kinds of accuracies. The spec-
troscopic accuracy is 1 cm−1, while the chemical accuracy is 1 kcal/mol ≈ 350 cm−1. Modern spectroscopic ap-
plications need a spectroscopic accuracy and this result has an error that is 60 times larger than spectroscopic
accuracy - so no, it is not yet good enough. But it is more than enough, e.g., for simulations of molecular dynamics
or reactions, especially at room temperature [191].

21The studied system was a generalized lattice polaron model [192] describing an electron in a one-dimensional
lattice with N → ∞ sites coupled to a phonon ﬁeld. The interaction between an electron and a phonon ﬁeld was
a combination of two qualitatively different terms: the Su-Schrieffer-Heeger (SSH) electron-phonon coupling and
the breathing-mode model with the Holstein coupling.

22How is this even possible? The intuition behind it is that the evolution of physical properties that are given to
the ML model as input should somehow reﬂect the fact that there is a phase transition. The model probably picks
up on the prevalent correlations within one phase and it observes that these correlations change when crossing to
other phases.

98

Gaussian processes and other kernel methods

Figure 4.11: GPs for extrapolation to non-seen quantum phases. (a) The mean-ﬁeld
Heisenberg spin model in the nearest-neighbor approximation, where black dots are
mean-ﬁeld results and blue dots are GP predictions. GP was trained on the high-
temperature shadowed regime of the data. (b)-(c) generalized lattice polaron model
with different GP training regimes (marked with white dots). In both cases, GP cor-
rectly predicted the phase transitions (color map) as compared to the quantum cal-
culations (black lines). Adapted from Ref. [124].

in these limits and can expect them to extrapolate successfully to other parameter regimes
where the direct calculation is more difﬁcult. Finally, in the same system, the authors of
Ref. [166] studied the importance of the choice of the kernel. They compared the results from
the original work [124] obtained for kernels found with the BIC as described in section 4.4.1
and section 4.4.2 to the results obtained for kernels with the same complexity (i.e., at the
same search tree level, see Fig. 4.7) but chosen at random. Predictions of such GPs were much
worse and were prone to overﬁtting, which stresses the power of the BIC as selection criterion
for kernels. The appropriate choice of the kernel is therefore crucial as it determines how far
the model can accurately extrapolate.

4.6 Outlook and open problems

• While GPs successfully surrogate PESs and need much smaller number of ab initio cal-
culations, it is challenging to reach the spectroscopic accuracy with this approach. What
is stopping us from achieving such accuracy levels with GPs? The major limitation is the
number of training data. In practice, it is often observed that the error during learning
eventually decreases by a factor of 1/n, where n is the number of training points. As
such, the number of training points required to reach a level of accuracy on the order
of 10 cm−1 for a 57-dimensional surfaces is still manageable. However, reaching spec-
troscopic accuracy requires an excessive amount of training data. In particular the size
of the training data set grows beyond the regime where GP are useful [193]. The high-
accuracy limit may be obtained if one incorporates some knowledge on the system to
the kernel. An open question of how to do that remains to be answered.

• In section 4.5 we have presented how BO and GPs can be used to tackle optimization
of expensive set-ups gradients are not accessible. Such is also the case of quantum NNs
or VAEs. Therefore, this approach may prove useful in the optimization of a quantum
model!

• An interesting research direction is combining the power of automatic differentiation
(AD), described in section 7.1, and kernel methods. Already, AD has played a major

99

ℱ𝛽/𝛼Gaussian processes and other kernel methods

role in the development of more robust kernel functions for GP models. For example,
Ref. [194] showed that by maximizing the log marginal likelihood, (Eq. (4.62)), one
could jointly optimize the weights and biases of a deep NN combined with any parameter
of a standard kernel function. A more recent work [195] also showed that learning the
composition of kernels is differentiable under the AD framework, and more complex
kernels could be parametrized. Currently, there are two main ecosystems for GPs based
on AD, GPytorch [196, 197], and GPflow [198].

• The training procedure of kernel ridge regression (KRR) could also be differentiated

using AD bypassing the need of using a cross-validation scheme [199].

• With the advent of quantum extensions of classical ML-methods for near-term quantum
devices, there are several paths on how to encode a data point x in a Hilbert space as
|x〉. As a consequence, the kernel function has to be promoted to its quantum version.
Interestingly, there is a provable advantage of such kernels based on measurement results
of the quantum state [200]. We give a bit more detail in section 8.2.

Further reading

1. Rasmussen, C. E. & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.
The MIT Press. The standard go-to reference on kernel methods and GPs in particu-
lar [201].

2. Krems, R. V. (2019). Bayesian machine learning for quantum molecular dynamics. PCCP,
21(25), 13392–13410. Discusses various applications of GPs for quantum molecular
dynamics [202].

3. Jupyter notebook allowing to faithfully reproduce a six-dimensional PES with a GP and

BO including optimal kernel search using the BIC criterion for the H3O+ [32].

4. Vargas-Hernández, R. A., & Krems, R. V. (2020). Physical extrapolation of quantum ob-
servables by generalization with Gaussian processes. Lect. Notes Phys., 968, 171–194.
In-depth review of possible applications of GPs and BO for extrapolation problems in
quantum sciences [203].

5. Huang, H. et al. (2021). Provably efﬁcient machine learning for quantum many-body
problems. arXiv:2106.12627. It introduces quantum-measurement inspired kernels for
a provable advantage of kernel methods over classical methods that do not use measure-
ment data [204].

100

Neural-network quantum states

5 Neural-network quantum states

In the early days of quantum mechanics, it soon became clear that approximation methods
would be needed to solve most relevant real-world problems [205].
Indeed, in most cases
one cannot exactly solve the Schrödinger equation for systems of more than a few interact-
ing particles. This came to be referred as the quantum many-body problem. In this chapter,
we show how neural networks (NNs) have been introduced to tackle this problem [206], in
a variety of applications including ground-state and quantum dynamics of interacting quan-
tum systems. For the sake of simplicity, we mainly focus our discussion on spin systems, and
discuss applications to fermions [207] and bosons [208] only toward the end.

According to the axioms of quantum mechanics, the state of an isolated quantum sys-
tem is encoded into a complex-valued probability amplitude commonly known as the wave
function. In the case of a single spin- 1
2 , the wave function in the computational z-basis ˆZ is
|Ψ〉 = C↑| ↑〉 + C↓| ↓〉. The coefﬁcients C↑ and C↓ are the complex-probability amplitudes of
the spin being aligned along (C↑) or opposite to (C↓) the direction of the computational basis,
and they are subject to the normalization condition |C↑|2 + |C↓|2 = 1. For many-body quantum
systems of N spins, where N can be any large number from tens to the order of the Avogadro
number ∼ 1023, the number of coefﬁcients in the wave function scales as 2N . Following up on
the spin example, the wave function can be expressed as follows

|Ψ〉 = C↑↑···↑| ↑↑ · · · ↑〉 + C↑↑···↓| ↑↑ · · · ↓〉 + · · · + C↓↓···↓| ↓↓ · · · ↓〉

= (cid:88)
1,σ
σ

2,··· ,σ

N

Cσ

1,σ

2,...,σ

|σ
1

〉 ⊗ |σ
2

N

〉 ⊗ · · · ⊗ |σ

〉 ,

N

(5.1)

〉 ⊗ |σ
where the |s〉 = |σ
2
1
the N spin system, and Cσ

〉 ⊗ · · · ⊗ |σ

〉 are the basis vectors of the Hilbert space that describes

N
are their associated amplitudes.

1,σ

2,...,σ

N

The quantum many-body problem originates from the exponential scaling of the number
of the basis elements, which leads to an exponential computational complexity in the system
size. In particular, the memory required to naively store the wave function of just 60 spins
is 16 · 260 ≈ 18 exabytes, about 500 times more than what is available on the world’s largest
supercomputer, as of 2022.

Nevertheless, while the Hilbert space of many-body quantum systems is exponentially
large, physically-relevant states are typically conﬁned to a corner of the Hilbert space that is of
limited dimension. For instance, many physical Hamiltonians only contain local interactions,
which signiﬁcantly constraints the form of the associated many-body wave functions.

The main idea behind variational methods is to ﬁnd a computationally efﬁcient repre-
sentation of the physically relevant quantum states within the Hilbert space of interest.

Variational methods circumvent the issue of an exponential complexity by encoding the
complex amplitudes of the wave function onto a parametrized function (often called the
ansatz) which depends on a set of parameters θ. If the number of parameters is polynomial
in the system size, the state can be efﬁciently stored with limited computational resources. In
general, the variational state |Ψθ〉 can be expanded onto the computational basis as

|Ψθ〉 =

2N
(cid:88)

s=1

Ψθ(s)|s〉,

(5.2)

where Ψθ(s) = 〈s|Ψθ〉 denotes the probability amplitude corresponding to the state |s〉. The

101

Neural-network quantum states

task is then to ﬁnd the parametrization θ that best describes our desired quantum state of
interest, such as the ground state of a given Hamiltonian.

5.1 Variational methods

Even when using variational states, computing expectation values can still be of exponential
complexity since one must perform sums over all the basis elements of the Hilbert space for
these calculations. Among the variational states that are practically usable, there are two pos-
sible approaches, which distinguish two families of variational ansätze: those that can be used
to compute expectation values exactly with a polynomial cost, and those that do so only ap-
proximately, with an accuracy improvable at a polynomial cost in system size. In the former,
the only source of error in the expectation value of observables comes from the truncation
of (exponentially large) regions of the Hilbert space, limiting its ability of representing wave
functions. In the latter, an additional source of error typically comes from sampling, which
however does not necessarily add a systematic error and can be improved upon for an addi-
tional computational cost.

A third category consists of parameterized quantum states whose cost for computing ex-
pectation values scales exponentially with system size. In practical applications, for example in
the case of tensor networks in two dimensions, approximate algorithms for computing expec-
tation values are introduced. Strictly speaking, however, these are not variational methods, as
we cannot compute expectation values to arbitrary accuracy in polynomial time, and introduce
a systematic bias that goes beyond the pure variational error.

5.1.1 Variational states with exact expectation values

In the ﬁrst kind of variational states, we mainly encounter locally constrained ansätze, for
which mean-ﬁeld and matrix-product states are notable examples.

Mean-ﬁeld Ansatz Mean-ﬁeld states are one of the simplest variational quantum states. With
these, we model our variational wave function by the mean-ﬁeld approximation, that is, as the
tensor product of single-spin wave functions

|Ψθ〉 = |φ
N
(cid:79)

1

=

i=1

(θ (1)
↑

, θ (1)

↓

)〉 ⊗ |φ

(θ (2)
↑

, θ (2)

↓

2

)〉 ⊗ · · · ⊗ |φ

(θ (N )
↑

, θ (N )

↓

)〉

N

|φ

i

(θ (i)

↑ , θ (i)

↓

)〉 ,

(5.3)

i

〉 are the single-spin wave functions at site i. They are subject to the orthogonality
where |φ
condition 〈φ
〉 has only two
coefﬁcients corresponding to the probability amplitudes of the spin being up or down, which
we take as variational parameters

i j denoting the Kronecker delta. This way, |φ

i j, with δ

〉 = δ

|φ

i

i

j

|φ

〉 = θ (i)
↑

| ↑〉 + θ (i)
↓

i

| ↓〉

(cid:12)
θ (i)
(cid:12)
↑
(cid:12)

(cid:12)
2
(cid:12)
(cid:12)

+

(cid:12)
θ (i)
(cid:12)
↓
(cid:12)

(cid:12)
2
(cid:12)
(cid:12)

= 1 ,

resulting into 2N complex parameters in total, i.e., θ = (cid:166)θ (i)

↑ , θ (i)

↓

(5.4)

(5.5)

(cid:12)
(cid:12)i = 1, 2, . . . , N

(cid:169)
.

With this family of wave functions, we can compute expectation values of quantum Hamil-
tonians exactly. This is a consequence of the fact that we can exploit the tensor product struc-
ture of our wave function to simplify the expectation values over many-body states to the
expectation over the corresponding single-body ones. For example, the expectation value of

102

Neural-network quantum states

i Pauli operator acting on the i-th site can be obtained as 〈Ψθ|σx

the σx
calculation is straightforward, as |φ

〉 is a two-dimensional vector and σx
i

|Ψθ〉 = 〈φ

|σx
i
is a 2 × 2 matrix.

|φ

〉. The

i

i

i

i

Tensor network states However, mean-ﬁeld states are not able to capture correlations be-
tween local degrees of freedom. Tensor network states (TNSs) are a family of quantum states
that improve upon such a limitation, and a subset of TNSs also allow to compute expectations
exactly. One of the most broadly used TNSs with this property are matrix product state (MPS),
which predominate in the study of one-dimensional systems.

Let us consider the coefﬁcients Cσ

, deﬁned in Eq. (5.1). We can consider
as a tensor with N indexes, which we can always express as the contraction of

2,...,σ

1,σ

n

Cσ
2,...,σ
1,σ
tensors Aσ

N
i , such that:

Cσ

1,...,σ

N

= (cid:88)
α,β, ... ,γ

σ
σ
σ
1
2
α,β A
A
β,δ ... A
N
γ,α

(5.6)

N

1,...,σ

where the maximal dimension of the Greek indices α, β, . . . is the bond dimension χ. This
way, an exact representation of Cσ
requires an exponentially large number of parameters.
This means that the bond dimension, χ, must increase exponentially with N . The idea of the
MPS ansatz resides in the truncation of the dimension of the indices of the tensors Aσ
i . With
the truncation, we reduce the number of parameters of our ansatz to be O(dN χ 2), where d is
the local Hilbert space dimension, e.g., two for a spin-1/2 particle. We usually truncate the
bond dimension in an elegant and controlled way using the singular value decomposition of
the tensors Aσ
i , which has a strict connection with the maximal entanglement entropy the MPS
can contain,1 as we discuss in section 5.2.3.

As a ﬁnal remark, let us mention an important algorithm proposed by S. White for the
energy minimization of variational quantum states, known as density matrix renormalization
group [209]. This algorithm is particularly well-suited for MPSs, and their combination is the
current state-of-the-art technique to compute the ground state wave function of one dimen-
sional systems. However, the description of this algorithm falls out of the scope of these lecture
notes. We refer to [210] for a complete review of the use of MPS, and to [211] for a review of
methods based on tensor networks (TNs).

5.1.2 Variational states with approximate expectation values

The second family of variational states we encounter are known as computationally tractable
states [212].

Variational ansätze must satisfy two conditions in order to be computationally tractable:

• Amplitudes for arbitrary single basis elements Ψθ(s) = 〈s|Ψθ〉 can be computed

efﬁciently.

• It is possible to efﬁciently generate samples s from the Born distribution

P(s) = |〈s|Ψθ〉|2
〈Ψθ|Ψθ〉 .

If these two conditions are met, we can estimate expectation values of arbitrary k-local
operators efﬁciently, and the statistical error due to the stochastic sampling can be rigorously
controlled by increasing the number of samples. Therefore, the computational time to compute
expectation values is polynomial in both the size of the system and the accuracy.

1The bond dimension is in fact the rank of the Schmidt decomposition of the quantum state.

103

Neural-network quantum states

A k-local operator is an operator that contains terms acting on at most k local quantum
numbers at the same time. For instance, a nearest-neighbour Hamiltonian is a 2-local
Hamiltonian because it contains terms acting on 2 qubits.

In general, given a variational state |Ψθ〉, we can obtain the expression for the expectation

value of an operator ˆO as follows

〈 ˆO〉 =

=

=

〈Ψθ| ˆO|Ψθ〉
〈Ψθ|Ψθ〉
(cid:80)
s,s(cid:48)〈Ψθ|s〉〈s| ˆO|s(cid:48)〉〈s(cid:48)|Ψθ〉
|〈Ψθ|s〉|2
(cid:80)

(cid:80)
s
〈Ψθ|s〉 〈s|Ψθ〉
〈s|Ψθ〉
(cid:80)
s

(cid:80)
s

(cid:80)
s

=

|〈Ψθ|s〉|2 (cid:80)
(cid:80)
s

s(cid:48)〈s| ˆO|s(cid:48)〉〈s(cid:48)|Ψθ〉

|〈Ψθ|s〉|2
s(cid:48)〈s| ˆO|s(cid:48)〉 〈s(cid:48)|Ψθ〉
〈s|Ψθ〉

,

(5.7)

(5.8)

(5.9)

(5.10)

|〈Ψθ|s〉|2
where we have added two identities of the form (cid:80)
|s〉〈s| = 1 in the numerator, and one in
s
〈s|Ψθ〉
〈s|Ψθ〉 in the numerator.2 We identify two main
the denominator. Then, we have multiplied by
terms:

P(s) =

|〈Ψθ|s〉|2

(cid:80)
s

|〈Ψθ|s〉|2

Oloc

(s) = (cid:88)
s(cid:48)

〈s| ˆO|s

(cid:48)〉

〈s(cid:48)|Ψθ〉
〈s|Ψθ〉

,

(5.11)

(5.12)

(s) is the so-called local estimator of ˆO. Therefore, we can write the quantum ex-
where Oloc
pectation value of an observable ˆO as the statistical expectation value of its local estimator Oloc
over the probability distribution P(s):
〈 ˆO〉 = (cid:88)

(5.13)

(s)〉

P(s)Oloc

(s) = 〈Oloc

P .

s

Let us stress that these calculations only hold for operators with the property that the number
of states s(cid:48) such that |〈s| ˆO|s(cid:48)〉| (cid:54)= 0, for arbitrary s is at most polynomial in the number of
spins. For example, it is easy to convince one-self that k-local operators satisfy this property.
(s) would not be tractable, given that the sum over s(cid:48) in Eq. (5.12)
Conversely, evaluating Oloc
would be over an exponential number of elements.

The procedure described above allows to obtain a controlled, stochastic estimate of the
expectation values by directly sampling a series of states, s(1), s(2), . . . , s(M ), from P(s), and
approximating 〈 ˆO〉 with the following arithmetic mean

〈 ˆO〉 ≈ 1
M

M
(cid:88)

Oloc

(s(i)) .

(5.14)

i=1
The statistical error associated with such an estimate is (cid:34) = (cid:112)σ2/M , and it is bounded as
long as the variance σ2 of Oloc is ﬁnite. For example, when ˆO is a k-local spin operator with
〉 = 0 never appear in the summation

2Notice that this manipulation is always valid, since amplitudes with 〈s|Ψ

θ

over s.

104

Neural-network quantum states

bounded coefﬁcients, its variance is strictly ﬁnite, since it can be shown that σ2 = 〈 ˆO2〉. 3
Therefore, the error in the estimate of expectation values decreases as (cid:34) ∼ 1/
M , which
allows us to reach arbitrary accuracy in the estimation by increasing the number of samples
M , given that limM →∞ (cid:34) = 0. However, generating a set of samples according to the Born
distribution, {s(i)} ∼ P(s), is in general a non-trivial computational task in the case where the
variational ansatz, Ψθ(s), is parameterized by an efﬁciently computable, yet arbitrary function.
One of the most commonly adopted strategies to sample from P(s) is through Markov chain
Monte Carlo methods, including the Metropolis-Hastings method, which generate a sequence
of correctly distributed samples s(i).

(cid:112)

Metropolis-Hastings methods construct a markovian stochastic process which satisﬁes the

detailed balance relation for the target probability distribution

P(s)T (s → s(cid:48)) = P(s(cid:48))T (s(cid:48) → s) ,

(5.15)

where T (s(i) → s(i+1)) is the probability that the state s(i) at step i transitions to the state
s(i+1) at the following step. As the process is markovian, the transition probability at every step
depends exclusively on the current conﬁguration. The detailed balance condition ensures that,
regardless of the initial conﬁguration s(0), the sequence eventually converges to the correct
distribution P(s) in the long time limit.

One possible choice of the transition probability T is given by the Metropolis-Hastings al-
gorithm [213]. The main idea is to express T in terms of a local transition kernel T and
an acceptance probability A such that

T (s → s(cid:48)) = T (s → s(cid:48))A(s → s(cid:48)) .

(5.16)

This way, we split the global stochastic process into the product of two local subprocesses that
we can compute efﬁciently. For instance, it is very easy to ﬁnd a normalized local transition
kernel that allows us to modify only a few degrees of freedom, like ﬂipping a single spin in
a given conﬁguration. Conversely, it is hard to ﬁnd a normalized global kernel that would act
on all spins.

The acceptance probability to go from a conﬁguration s to s(cid:48) through a local transition is

deﬁned as

A(s → s(cid:48)) = min

(cid:129)

1,

P(s(cid:48))T (s(cid:48) → s)
P(s)T (s → s(cid:48))

(cid:139)

.

(5.17)

Notice that the normalization of the Born probabilities cancel out, giving the expression

P(s(cid:48))
P(s)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

〈s(cid:48)|Ψθ〉
〈s|Ψθ〉

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

,

(5.18)

which allows us to consider unnormalized variational ansätze. Additionally, if the variational
state is computationally tractable, the transition probability also has a tractable complexity,
provided it only acts on the basis elements.

Choosing a valid transition rule T (s → s(cid:48)) is not trivial, and we must take special care in the
case of systems with symmetries. For example, if the total magnetization along the direction
of the computational basis is known, we might want to ﬁx it and use a transition rule that
does not project the Markov chain outside of a certain region. In general, a computationally

3It is also simple to prove that, when |ψθ 〉 approaches an eigenstate of ˆO, the variance vanishes. Consequently,

considering ˆO = ˆH, the statistical error vanishes as we approach the ground (or any excited) state.

105

Neural-network quantum states

expensive, yet effective choice for the transition kernel is to use the Hamiltonian itself:

T (s → s(cid:48)) =

(cid:12)〈s| ˆH|s(cid:48)〉(cid:12)
(cid:12)
(cid:12)(1 − δs,s(cid:48))
(cid:12)〈s| ˆH|s(cid:48)〉(cid:12)
(cid:12)
(cid:80)
(cid:12)

(cid:48)(cid:54)=s

,

(5.19)

which is known as Hamiltonian transition rule [206].

This way, with the Metropolis-Hastings algorithm, starting from a random conﬁguration
s(0), we can sample from P(s) by iteratively proposing local modiﬁcations s(cid:48) according
to T (s → s(cid:48)), and accepting them according to A(s → s(cid:48)).

Nonetheless, this sampling procedure is imperfect, and it can fail to converge for a reason-
able number of iterations if the sampled distribution is too complex. In addition, the procedure
suffers from the fact that the samples are correlated, since we ﬂip spins iteratively. See Algo-
rithm 6 for further details.

Algorithm 6 Metropolis-Hastings algorithm

s ← uniform∈ [1, 2N ]
for i = 1 to M do

propose s(cid:48) according to T (s → s(cid:48))
A ← P(s(cid:48))T (s(cid:48)→s)
P(s)T (s→s(cid:48))
ξ ← uniform∈ [0, 1]
if ξ ≤ A then
s ← s(cid:48)

end if

end for

5.2 Representing the wave function

(cid:46) sample initial state uniformly at random

(cid:46) calculate acceptance probability

(cid:46) update state

Now that we have seen how to compute the quantities of interest using parametrized quantum
states, let us dive into how to devise expressive variational states in practice. The main idea
is that we need to represent high-dimensional functions with a parametrization that is ﬂexible
and general enough to describe physical systems, while involving only a polynomial amount
of parameters.

Traditionally, researchers have relied on physically-inspired variational ansätze. The Jas-
trow wave function [214, 215] stands out as one of the most successful and widely used ones.
It is based on the assumption that two-body interactions are the most physically relevant, and
it assigns a trainable potential to every interacting pair. Formally,

Ψθ(s) = e

(cid:80)

− 1
2

θ

σ

σ

i

i j

i(cid:54)= j

j ,

(5.20)

where the sum runs over all possible spin pairs, and θ
i j are the parameters encoding pairwise
spin correlations. Therefore, for a system of N spins, the resulting wave function has O(N 2)
parameters. Moreover, in translationally invariant systems, the parameters θ
i j can be made
depend exclusively on the distance between i and j, resulting in a reduced number, O(N ), of
parameters.

More recently, ANNs have taken over more traditional ansätze to approximate the wave
function itself [206]. This family of variational states is known as neural quantum states

106

Neural-network quantum states

(NQSs). For instance, we can write a parametrized wave function as a feed-forward NN. In
this case, Ψθ(s) corresponds to the output of a NN that takes the conﬁguration s as input in
the form of a vector.

In a feed-forward neural network of depth D, every layer l consists of a nonlinear activation
function g (l) that acts, component-wise, on a vector resulting from applying the weight matrix
W (l) to the output of the previous layer. This way, it is possible to write the variational state as
the composition of operations g (l) · W (l), where “·” indicates point-wise operation, such that

Ψθ(s) = g

(D) · W (D)

. . . g

(2) · W (2)

(1) · W (1)s .

g

(5.21)

Hence, the output is a scalar, complex or real, representing the probability amplitude of con-
ﬁguration s.

From a mathematical perspective, these ansätze are of great interest given that NNs are
subject to universal representation theorems [65], as we explain in section 2.4.4. According
to Eq. (2.46), we could represent the many-body wave function with a polynomial number
O(N 2) of one-dimensional nonlinear functions, with N denoting the number of spins.

q, φ

However, these results hold for arbitrary nonlinear functions, Φ

q,p in Eq. (2.46), that
must be appropriately found in order to represent the target function. In practice, NNs use
a ﬁxed nonlinear activation, and we can only adjust the number of operations. In these cases,
the number of neurons does not have a strict polynomial scaling and it can be, in the worst case,
exponential in N [66]. Nevertheless, the state-of-the-art results in computer vision and natural
language processing [216–218] should be sufﬁcient motivation to employ similar techniques
to represent quantum states. Note that the NN representation of quantum states does not
〉 and
preserve the Hilbert space structure, which means that for two NN representations |ψ
|ψ
〉 represented by a NN
of the same size as the ones representing |ψ
〉 by simply adding up the parameters
1
together, as the ansatz is generally nonlinear.

〉 it is not possible to construct a valid wave function |ψ〉 = |ψ

〉 and |ψ

〉+|ψ

1

2

2

1

2

5.2.1 Restricted Boltzmann machines

NQS were ﬁrst introduced using restricted Boltzmann machines (RBMs) [206]. RBMs are shal-
low models featuring two fully-connected layers: a visible layer, consisting of N units, and
a hidden layer, consisting of M units. A scheme of an RBM architecture is presented in Fig. 5.1.
The wave function amplitudes of an RBM ansatz are given by:

Ψθ(s) = (cid:88)

h

vs+b†

eb†

hh+h†W s.

(5.22)

the visible and hidden units,

where s, h represent
respectively, and the parameters
θ = {bv, bh, W } represent the visible and hidden biases and the weight matrix, respectively.
In the NN picture, the RBM is a single-layer nonlinear feed-forward NN, with the visible units
serving as inputs and the exponential serving as the activation function. While it is common to
have biases for the hidden layer (see section 2.4.4), RBMs have also somewhat unusual biases
connected to the input values, which is explained in the next paragraph.

By construction, RBMs are designed in such a way that computing the summation over
hidden units, as in Eq. (5.22), can be done analytically. To see this, we can rewrite Eq. (5.22)
in a tractable form considering binary hidden units hi

∈ {−1, 1}, leading to

Ψθ(s) = eb†
vs

M
(cid:89)

i=1

2 cosh (cid:0)bh,i

+ Wi·s(cid:1) ,

(5.23)

107

Neural-network quantum states

Figure 5.1: Pictorial representation of a restricted Boltzmann machine (RBM) that
represents the wave function of an N -spin system, with s = (σ
) and
h = (h1, h2, . . . , hd

) the hidden units.

2, . . . , σ

1, σ

N

where bh,i and Wi· denote the i-th hidden bias and weight matrix row, respectively. To treat
spin systems, the visible units will represent the N physical spins, thus the input of the RBM is
simply the spin conﬁguration s. In this way we obtain an analytical expression to evaluate the
amplitude for a given spin conﬁguration, and thus represent the full wave function with this
ansatz. One can also interpret the hidden units as M hidden spins, and in this picture the RBM
can be thought of as an interacting spin model with interaction strengths Wi j. Moreover, we
can treat an RBM as a model with an associated energy depending on its parameters, input,
and hidden spin values. This is known as an energy-based model, and explains why input
biases are present in Eq. (5.22). In fact, the RBM is equivalent to a Hopﬁeld network, a type
of spin glass [219]. For more details on this view, see [220].

Being the ﬁrst to be introduced in this context, most of the early works about NQS em-
ploy RBMs, but other architectures have been systematically explored in more recent years.
The capacity of RBMs and its relationship to quantum entanglement has been examined in
various works [221, 222]. An extension of this architecture, the deep RBMs, has also been
introduced to solve more complex problems [223], which consists in stacking more than two
fully connected layers.

5.2.2 Autoregressive and recurrent neural networks

ARNNs, as presented in section 2.4.6, can also be used for constructing NQS, as introduced
in Ref. [83] and later applied to both quantum [224] and classical problems [79]. Their main
advantage is that their Born probability distribution is normalized, allowing for direct (autore-
gressive) sampling, which is easier to parallelize than Markov chain Monte Carlo. A pictorial
representation of both the network and the sampling algorithm is presented in Fig. 5.2.

Analogously to Eq. (2.47), we express the many-body wave function in terms of a product

108

σNzσ3zσ2zσ1zh1h3hdh2hidden layer...visible layer...Neural-network quantum states

Figure 5.2: Example of an ARNN quantum state for four spins. (a) Pictorial represen-
tation of the network. The arrows representing the weights of the model are skewed
in order not to break the conditional structure of the output probability distribution.
These layers are “masked”, due to some connections being deleted. (b) Sampling
algorithm. One samples consecutive spins using direct sampling on the conditional
probabilities at each step. Adapted from [83].

of conditional complex amplitudes:

Ψθ(s) =

N
(cid:89)

i=1

φ

i

(σ

|σ

i−1, . . . , σ
1

) ,

i

(5.24)

i

(cid:12)
(cid:12)φ

(σ|σ

)(cid:12)
which is subject to the normalization condition (cid:80)
2 = 1. With this kind
(cid:12)
σ
of architecture, we can compute expectation values by directly sampling state conﬁgurations
instead of building a Markov Chain through the Metropolis-Hastings algorithm, for exam-
ple (see Algorithm 6). We sample state conﬁgurations by iteratively sampling one spin af-
ter the other: we start sampling the ﬁrst spin σ
1 from the reduced probability distribution
)|2. Then, we sample the second one σ
|φ(σ
2 according to the conditional probability distri-
1
)|2, then the next one |φ(σ
bution |φ
N . This sampling
3
procedure is embarrassingly parallel.4

)|2, and so on until σ

i−1, . . . , σ
1

2, σ
1

(σ
2

|σ
1

|σ

2

This sampling procedure yields independent identically distributed samples. Conversely,
Markov chain Monte Carlo methods may suffer from highly correlated consecutive samples,5
which is problematic for complex probability distributions, e.g. that are far from Gaussian.
Consider a quantum state that spans several separated regions in the Hilbert space, where the
probability is concentrated. In this case, Markov chains generally remain stuck in one of the
regions, given that it must take several penalizing steps to travel from one to another, resulting
into a highly inaccurate sampling. In contrast, the direct sampling procedure can seamlessly
draw spin conﬁgurations belonging to all the regions according to the probability distribution,
yielding much better samples.

4We can use the intermediate conditional probabilities to draw samples for a low computational cost, e.g., use
the probabilities for N − 1 spins and sample from the last one, to obtain new samples; with Markov chain Monte
Carlo we cannot do this.

5Markov chain Monte Carlo methods such as the Metropolis-Hastings algorithm, generally rely on performing
modiﬁcations to the spin conﬁgurations to sample subsequent states. Therefore, this process could yield highly
correlated consecutive samples that may have a negative impact on the results.
In order to compute expecta-
tion values, we need to estimate the autocorrelation time to draw uncorrelated samples from the resulting chain.
Moreover, when approaching a phase transition points, such methods suffer from critical slowing down, making
the sampling of uncorrelated conﬁgurations unfeasible in many situations.

109

(a)σ1σ2σ3σ4In ψ1In ψ2In ψ3In ψ4In ψi (σi | ...)ΣiNEURAL AUTOREGRESSIVE QUANTUM STATESpinsMasked Convolutionsl 2- normalization(in log-space)NormalizedWave FunctionStep 1(b)SAMPLING (σ1,...,σ4) ~ | Ψ(σ1,...σ4)|2~σ1Step 2~σ2σ1Step 3~σ3σ1σ2Step 4~σ4σ1σ2σ3Neural-network quantum states

Figure 5.3: Pictorial representation of an RNN architecture for NQS. Panel (a) is for
real-valued wave functions, which can be relevant for a certain class of problems,
and panel (b) is for complex-valued wave functions. In both schemes, a local spin
conﬁguration si and a hidden vector hi are fed into an RNN cell, which performs
a nonlinear transformation. Then an activation function (ς, for softmax and/or ςς,
for softsign) is applied to obtain the ﬁnal probability and/or phase corresponding to
the conﬁguration. At the end, the probabilities (and phases) are combined to obtain
the ﬁnal wave function amplitudes ψ(s).

While the ﬁrst autoregressive models used in quantum physics were built from masked
dense or convolutional layers, mimicking the so called PixelNet architecture [225], recurrent
neural networks were later introduced [226] RNNs, inspired by natural language processing
models, are also generative models. We can draw a simple analogy between correlations in
sentences, with their elements living in a large “word space”, and spin conﬁgurations. Con-
sidering spin systems, and supposing some hidden structure, quantum states are correlated
and its base elements are elements of the Hilbert space. Following this analogy, Hibat-Allah
et al. introduced RNN wave functions [226], obtaining impressive results even for frustrated
systems. An example of such an architecture is shown in Fig. 5.3. Clearly, many different NN
architectures that can work. A plethora of different architectures have been implemented as
NQS in recent years, such as CNN wave functions [227], and, more recently, group convolu-
tional NNs [228], which are specially convenient to implement certain symmetries, something
we examine in section 5.2.4.

5.2.3 Capacity and entanglement

As we show in sections 5.1 and 5.2, there is a whole plethora of methods to represent quantum
many-body wave functions. For instance, only in NQSs, we already encounter substantial
differences between ansäzte based on different NN architectures. Hence, a natural question
arises regarding their expressive capacity and how they compare to each other.

TNs have been a recurrent tool to perform this kind of studies, provided that they are well
established and characterized, and they constitute a theoretical language to study quantum
many-body phenomena. For this reason, there has been a signiﬁcant community effort to
study the relationship between TNSs and NQSs [222, 229, 230] , which provides insight about
the expressive capacity of NQSs [231]. Following the ﬁrst introduction of NQSs implementing
RBMs [206], early works focused on ﬁnding direct relationships between various kinds of
RBM-based states and TNSs [222, 229]. Recently, it has been proven that NNs can efﬁciently
approximate, in logarithmic space, all efﬁciently contractible TNs with arbitrary precision.
Therefore, for every TNS there exists an equivalent NQS of polynomial size. Conversely, there

110

s2s1s0s2s1s0RNNP1h1RNNP2h2h0h1RNNP3h3ψ ( s )h2(a)RNNP1ςh1RNNh0h1ψ ( s )(b)ςςΦ1P2h2Φ2RNNP3h3Φ3h2ςςςςςςςςςNeural-network quantum states

Figure 5.4: Expressive capacity of different classes of variational states, as explic-
itly proven in Ref. [230] by mapping TNSs to NQS. PEPS* refers to a sub-class of
projected entangled pair states, a generalization of MPSs. Adapted from [230].

are quantum states that can be efﬁciently described by NQSs, whose representation in terms of
TNSs requires an exponential amount of parameters. Hence, TNSs are a subset of NQSs [230],
as depicted in Fig. 5.4.

As a measure of expressive capacity, we often rely on the entanglement that the different
ansätze can capture. For instance, the mean ﬁeld ansatz is, by construction, a product state
(recall Eq. (5.3)). Hence, it can capture less entanglement than TNSs or NQSs, wich do not
have such strong local limitations. This way, TNSs and NQSs have higher expressive capacity
than the mean ﬁeld ones.

More precisely, we study the entanglement scaling captured by the different ansätze. In
a generic quantum many-body system with density matrix ρ, the entanglement entropy is
deﬁned as

S(ρ) = − Tr (cid:2)ρ log2

ρ(cid:3) ,

(5.25)

which is zero for any pure state. Let us consider a partition of the system in two subsets:
I and its complementary O, as well as the reduced density matrix ρ
[ρ]. In general,
ρ
I represents a mixed state, which can have nonzero von Neumann entanglement entropy.
For a generic quantum state, the entanglement entropy of ρ
I grows with the volume of the
cut. Thus, it corresponds to a volume-law scaling. NQSs can efﬁciently capture such scaling
with architectures ranging from very basic shallow ones, such as RBMs [221], to more modern
and deeper approaches, such as CNNs or RNNs [231]. Some traditional ansätze, such as the
Jastrow wave function (see Eq. (5.20)), can also capture volume-law entanglement.6

= TrO

I

6The Jastrow ansatz is, indeed, a speciﬁc case of RBM wave function with N (N − 1)/2 hidden neurons [229].

111

Quantum      StatesNeural   Quantum      StatesPEPS*MPSGapped 1DNeural-network quantum states

Figure 5.5: Schematic representation of various ansätze inspired by [229]. (a) The
Jastrow ansatz draws connections between all possible pairs of sites. (b) The MPS
ansatz draws connections between nearest-neighbour sites along a line. (c) The RBM
ansatz connects all the sites to every hidden neuron, illustrated in different colours.
(d) The RNN ansatz processes the state sequentially, following the green arrows. The
dark blue arrows indicate the ﬂow of information within the model. Arrows without
a starting site correspond to free parameters. Sketch inspired by [234].

However, there is a subclass of states in which the entanglement entropy grows, at most, as
the boundary area between two regions. This is known as area-law scaling, and it is a property
of ground states of local and gapped Hamiltonians [232]. Due to their local nature, TNSs can
efﬁciently capture area-law entanglement [233]. For instance, in a one-dimensional chain, the
area of the cut between two subsystems is constant, meaning that the entanglement entropy
is a constant, and not an extensive quantity, in the inﬁnite volume limit. For an MPS with
bond dimension χ, the von Neumann entanglement entropy of any possible bi-partition of the
system is bounded from above as S ≤ O(log2
χ), thus making the MPS ansatz an excellent
candidate to study one-dimensional systems.

We can understand most differences between the ansätze at an intuitive level by, simply,
looking at how they are built. In Fig. 5.5, we provide a pictorial representation of the different
connections that some ansätze can draw in a bi-dimensional system. Clearly, the MPS ansatz,
depicted in Fig. 5.5(b), is the most locally restricted one, as it can only account for nearest
neighbour connections in a snake-like pattern. This effectively limits the entanglement that
MPS can capture. The RNN ansatz, illustrated in Fig. 5.5(d), while it is limited to parse the
state in the same pattern as the MPS, it has the freedom to account for additional information,
allowing it to capture richer correlations.

In contrast, other ansätze such as the Jastrow or RBM wave functions, respectively il-
lustrated in Fig. 5.5(a) and (c), can draw connections between arbitrary sites. The Jastrow

112

(b)(a)(d)(c)Neural-network quantum states

ansatz can account for all possible pairs in the system, regardless of the distance. Then, the
RBM anstaz is a generalization of the Jastrow by means of an auxiliary hidden layer of vari-
able size. Through the hidden neurons, the ansatz is no longer limited to pairs, and it can
actually consider up to all-to-all connections. This non-local character allows them to capture
volume-law entanglement.

5.2.4 Implementing symmetries

Encoding symmetries in NQSs allows us to reduce the number of parameters in the NN, re-
stricting the region of the Hilbert space that our ansatz can cover to a subspace of interest,
thus improving the accuracy of the results. Let us ﬁrst explain what we mean by symmetry
in this context. Consider a group of linear transformations: if the Hamiltonian is invariant
under those transformations, meaning that they all commute with the Hamiltonian, then the
Hamiltonian is symmetric under that group. Some of the most common symmetries in lattice
models are the translation symmetry, the rotation symmetry in two or higher dimensions, the
inversion or reﬂection symmetries, and all the compositions of those.

It is possible to show that,

if the Hamiltonian commutes with a set of operators
}K
T = { ˆTk
k=1, its ground state must also be left invariant under those transformations. There-
fore, the amplitude for two conﬁgurations |s〉 and |s(k)〉 = ˆTk
|s〉 must be invariant for any ˆTk:
Ψθ(s) = Ψθ( ˆTks) ∀k.7 One way to introduce symmetries in our NQS is to take, as output, the
sum of the ansatz evaluated on the set of symmetry-invariant conﬁgurations {s(k)}. This way,
the output is invariant by construction. However, we do not improve the performance of our
model with this approach.

A more efﬁcient approach is to build a dense layer at the beginning of the NQS model
that fulﬁlls the symmetry condition [227]. We can use this technique to encode any symmetry
group isomorphic to a polynomially large permutation group. This usually comprises the set of
all lattice symmetries (translations, rotations, reﬂections...), global discrete symmetries, such as
a global spin-ﬂip, but it cannot deal with continuous symmetries, such as SU(2). For instance,
we can implement translation symmetries through a convolution with a kernel as wide as the
system itself. Since the convolution is translationally invariant by deﬁnition, it’s easy to see
that the output of the layer is symmetry-invariant.

In the case of RBMs, we can rearrange the terms of Eq. (5.22) to make it invariant under the
j.

elements of a symmetry group. Let us denote the transformation of local spins as σ
We can write our symmetry-invariant amplitude as:

(k) = ˆTk

σ

j

Ψθ(s) = (cid:88)

(cid:40) α
(cid:88)

exp

b f
v

K
(cid:88)

N
(cid:88)

σ

j

(k) +

h

f

k

j

α
(cid:88)

b f
h

K
(cid:88)

f

k

+

h f ,k

α
(cid:88)

K
(cid:88)

N
(cid:88)

h f ,k

f

k

j

(cid:41)

W f
j

σ

(k)

j

,

(5.26)

where we have explicitly written the matrix products as sums. The important point here is that
b f
v , b f
h are now vectors in a feature space with f = 1, . . . , α, and the matrix W f is now of size
α × N .8 If we consider translational invariance, the corresponding symmetry group is made of
N translation operators. In this case, W f can be seen as a kernel acting over conﬁgurations
to which we have applied the translation operators.

There are, in fact, many ways to directly encode symmetries in NQSs. For more details,

we refer to [235] for general feedforward networks, or [227] for an example with CNNs.

7Up to a phase on the right hand side, but let us ignore it for convenience.
8Note that this expression is equivalent to Eq. (5.22) with M = K × α hidden variables.

113

Neural-network quantum states

5.2.5 Limitations

Similar to many ML methods, NQSs suffer from an interpretability problem, as we have dis-
cussed extensively in section 3.5 for generic ML approaches. However, there has been substan-
tial progress since the seminal paper from Carleo & Troyer [206]. For instance, a recent work
introduced an interpratable RBM ansatz, in which the authors add some correlation terms to
the expression of the probability distribution given by Eq. (5.22). With this, one can look at
the magnitude of the trained parameters to understand which correlations are more important
for the given physical problem [236].

Another route to gain further understanding of NQSs is through the mapping of NQS ar-
chitectures to other known ansätze, such as TNSs. By exploiting this idea, works have shown
NQSs to be capable of describing volume-law states, as opposed to TNSs, as we show in sec-
tion 5.2.3. In terms of expressive capacity, NQSs can efﬁciently represent the ground states of
one-dimensional gapped Hamiltonians, all the TNSs that are efﬁciently contractable in classi-
cal computers, and volume-law states [230]. Furthermore, there have been found exact NQS
representations of several interesting phases of matter, such as topological states and stability
codes [221,223,229,237–241]. However, not all quantum states can be efﬁciently represented
in terms of NQSs. For instance, we cannot represent random states, since there do not have
structure.

Another important aspect is choosing the right NN architecture and training strategy for
the problem. For instance, we may be interested in implementing certain symmetries, as we
discuss in section 5.2.4. However, on a given problem, a certain NQS ansatz may be well-
suited for the task, but the training procedure can fail numerically. Some works have analyzed
the training procedure involving stochastic reconﬁguration [242]. Others have found that
states involved in the dynamics of non-integrable systems are not representable by various
architectures, but their entanglement structure can be recovered, hinting at a different limit
from the built-in limitation on entanglement in TN-based ansätze [243].

These ﬁndings, along with state-of-the-art results, point toward a superior expressive
power of NQS over existing simulation methods, but many research routes have to be taken
to fully understand their capabilities, much like many ML methods discussed in these lecture
notes.

5.3 Applications

In this section, we present various applications of NQS, ranging from the ground state search to
quantum state tomography, featuring real-time dynamics, quantum circuits, and fermionic sys-
tems. In addition to presenting how the methods described previously apply to such problems,
we provide results for each application and compare them to other state-of-the-art methods.
By doing this we hope to show both the potential and versatility of NQS approaches, that is
still a young ﬁeld of research.

5.3.1 Finding the ground state

As common in many ML tasks, we deﬁne a loss function L that depends on the trainable
parameters of the NN. In this situation, this corresponds to the variational energy, i.e., the
expectation value of the Hamiltonian in the variational state:

L(θ) = E(θ) = 〈Ψθ| ˆH|Ψθ〉.

(5.27)

114

Neural-network quantum states

This choice of the loss function is naturally introduced since it follows from the variational
principle in quantum mechanics.

The variational principle states that given an Hamiltonian ˆH, the energy E(θ ) of a vari-
ational wave function |Ψθ 〉 is greater or equal than the exact ground state energy, i.e.,

E(θ) =

〈Ψθ| ˆH|Ψθ〉
〈Ψθ|Ψθ〉

≥ E0.

(5.28)

Therefore the energy is a valid loss function, as the lower the expectation value of the
energy, the better the approximation isa.

aWe stress that the principle is only valid when computing expectation values exactly. When the energy
is computed as a stochastic average, its estimated average can be lower than the exact energy. Netherthe-
less, as discussed previously, the increase of the number of samples and the use of an efﬁcient sampling
approach reduce ﬂuctuations below the exact energy systematically.

In fact, having a loss function strongly rooted in a principle of physics is crucial, since it also
allows to compare different methods. By looking at the variational energy, we can for example
understand how a method performs at solving a given problem: if the resulting approximate
ground state energy is signiﬁcantly lower than what was found by alternative techniques, we
can be reasonably sure that the solution found is of better quality. Following the general
discussion on expectation values of operators, the variational energy can be stochastically
approximated as

E(θ) ≈ 1
M

M
(cid:88)

i

(s(i)),

Eloc

(5.29)

where Eloc is the local estimator and is deﬁned as Eloc
〈s|Ψ〉 . We aim to mini-
mize this loss function by means of gradient-based optimization algorithms. The energy gra-
dients can also be written in terms of expectation values9:

(s) = (cid:80)

s(cid:48)〈s| ˆH|s〉 〈s(cid:48)|Ψ〉

∂ E(θ)
∂ θ

p

= 2 Re

= 2 Re

(cid:148)〈Eloc
(cid:148)〈(Eloc

∗
(s)O
p

(s)〉 − 〈Eloc

∗
(s)〉〈O
p

(s)〉(cid:151)

(s) − 〈Eloc

∗
(s)〉)O
p

(s)〉(cid:151)

(5.30)

(5.31)

where we have assumed that the parameters are real10 and that θ
the NQS. The diagonal operator ˆOp is deﬁned as

p is the p-th parameter of

(s) =

Op

∂
∂ θ

p

log〈s|Ψθ〉 = 〈s| ˆOp

|s〉.

(5.32)

9Computationally speaking, one does not need to store in memory the full Jacobian matrix Op

(s), but can
compute this gradient directly through the vector-Jacobian product (reverse-mode differentiation) of the vector
v = Eloc
(s). This approach lowers considerably the memory and computational
cost. For more details, see section 7.1.

(s)〉) and the Jacobian Op

(s) − 〈Eloc

10The requirement of real parameters is not actually necessary. For complex parameters the expression is very
similar, though care has to be taken in order to consider non-holomorphic ansätze. Note that many common
ansätze, in particular most autoregressive ones, are not holomorphic. A discussion of this can be found in the
appendix of Ref. [244].

115

Neural-network quantum states

We also remark that the expression used in eq. (5.31) has the form of a covariance, and
therefore it is therefore particularly stable with respect to sampling noise. Most notably, when
the wave function is close to the exact ground state, statistical ﬂuctuations in the local energy
are suppressed, implying that also statistical ﬂuctuations of the gradients are small, because
of the covariance structure.

The learning algorithm is thus straightforward. First, we initialize the weights θ(0). Next,
at each step a sequence of M conﬁgurations is sampled according to the Born distribution:
P(s; θ(s)) ∼ s(1) . . . s(M ). This can be done with a Markov chain or with direct sampling tech-
niques as explained above.

The next step is to compute the mean of the local energy E(θ), which gives us the estimate
of the expectation value of the Hamiltonian. Additionally, the gradients can also be calculated
as shown in Eq. (5.31). For the last step, we can use a gradient-based optimizer of our choice,
to update the parameters for the next step, i.e., θ (s+1)
for vanilla gradient
descent where η is the learning rate.

− η ∂ E(θ)

= θ (s)

∂ θ

p

p

p

The procedure is repeated until it converges to a minimum of the energy landscape. Here,
there is no training data set as the approach is not based on any supervised learning method.
The presented task is in fact to determine the optimal (unknown) wave function by drawing
samples from the associated Born distribution and using a neural network to model the state
itself. These steps are summarized in Algorithm 7. Note that this algorithm is not the most
commonly used, as it is less accurate than imaginary-time evolution, which is presented in
section 5.3.3.

Algorithm 7 Ground state search with NQS

Initialize θ randomly
for i = 1 to nsteps do

Generate M samples according to some algorithm (usually a Markov chain)
Calculate the gradient of the energy ∂ E(θ)/∂ θ
Update parameters as θ

− η∂ E(θ)/∂ θ

← θ

p

j (or with a more advanced update rule)

j

j

end for
return Optimized parameters θ

5.3.2 Real-time evolution

NQSs can also be used to variationally perform real-time evolution [245] through a procedure
known as time-dependent variational Monte-Carlo (t-VMC) [206, 227, 246, 247]. This is of
particular interest for non-equilibrium quantum dynamics of closed, interacting quantum sys-
tems. Studying these problems enables one to understand critical properties, entanglement
spectra, and many other physical quantities of interest in complex many-body quantum sys-
tems. The problem one wants to solve is to integrate the time-dependent Schrödinger equation
(ħh = 1 in the following) in time, using a parametrized wave function |Ψθ(t)〉:

d |Ψθ(t)〉
d t

i

= ˆH |Ψθ(t)〉 ,

(5.33)

i.e., ﬁnd the correct form of |Ψθ(t)〉 ∀t. Expanding Eq. (5.33) at ﬁrst order in δ and taking
the inner product with 〈s|, we obtain:

Ψθ(t + δ)(s) = 1 − iδ〈s| ˆH|Ψθ(t)〉 + O(δ2)

= 1 − iδEloc

(s) + O(δ2),

(5.34)

(5.35)

116

Neural-network quantum states

(s) as deﬁned in the previous section. In order to get a good variational
where we used Eloc
approximation of the state at the next time step, t + δ, it is natural to deﬁne the cost function
L( ˜θ):

L( ˜θ) = dist (cid:0)|Ψ ˜θ

〉, |Ψθ(t + δ)〉(cid:1) ,

(5.36)

with θ the variational parameters at the previous time step, and ˜θ variational parameters to
be determined. The loss function can be minimized analytically, if the time step is sufﬁciently
small. One starts by noticing that ˜θ = θ+δ ˙θ+O(δ2). One can therefore expand the variational
state |Ψ ˜θ

〉 at ﬁrst order and take its inner product with 〈s|, much like we did for Eq. (5.34):
(s) = (cid:0)1 − δ ˙θ∂θΨθ(s)(cid:1) Ψθ(s) + O(δ2).

(5.37)

Ψ

θ+τ ˙θ

We need to consider a distance measure between the two states |Ψ〉 and |φ〉 which can be efﬁ-
ciently sampled. There is a certain freedom in this choice, which can lead to slightly different
variational principles. For an extensive discussion of these issues, see [245]. By consider-
ing the inﬁdelity, keeping in mind that for many NQS architectures the quantum states are
unnormalized, we have 11:

dist (|Ψ〉, |φ〉) = 1 −

〈φ|Ψ〉〈Ψ|φ〉
〈φ|φ〉〈Ψ|Ψ〉

.

(5.38)

By plugging Eq. (5.37) and Eq. (5.34) into the distance of Eq. (5.38), minimising it, and keep-
ing the leading terms in δ one obtains an equation giving the time derivative of the variational
parameters ˙θ, enabling high-order integration methods such as Runge-Kutta integration:

with the quantum geometric tensor S and the vector f , whose elements are given by:

S ˙θ = −if

Spp(cid:48) = 〈O

∗

pOp(cid:48)〉 − 〈O

∗
p
〉 − 〈Eloc

〉〈Op(cid:48)〉
∗
〉〈O
p

〉

fp

∗
= 〈ElocO
p

(5.39)

(5.40)

(5.41)

with the Ops given by Eq. (5.32) and Eloc is the local energy. The vector f is the gradient
of the local energy with respect to the variational parameters and, in analogy with classical
mechanics, it is often called the vector of forces. The spectrum of the geometric tensor instead
encodes the (linearized) curvature of the variational space, akin to the Hessian discussed in
section 4.5. For a full derivation and an in-depth discussion of the time-dependent variational
principles, see [245]. The spectrum of S has been extensively studied in the case of ground
state optimization with RBMs [242], where it has been connected to the different regimes of
considered physical system. In practice, solving the linear system Eq. (5.39) implies either
using an iterative solver (for example, conjugate gradient) or a direct solver (for example,
QR factorization). An important pratical numerical issue is that the matrix S is often singular.
Some techniques have been found to regularize S and obtain more stable dynamics [227,248].
In all cases, since only stochastic averages for both S and f are available, stable and accurate
long time dynamics are still a challenge for NQS [248]. As an example, in Fig. 5.6 we show
the quench dynamics of a one-dimensional spin chain, subject to the Ising Hamiltonian with
a transverse ﬁeld:

ˆHTFI

= −J

ˆσz

j ˆσz

j+1

+ h

ˆσx
j .

(5.42)

(cid:88)

(cid:88)

j

j

J is the nearest-neighbor coupling, and h is the transverse ﬁeld strength. This model exhibits
a second-order phase transition in one dimension at h = J, that separates a ferromagnetic

11Rigorously, one should consider the Fubini-Study metric, but taking this distance leads to the same equations.

117

Neural-network quantum states

Figure 5.6: Critical quench dynamics with an RBM, preparing the system in the
/J = 1. This
ground state of ˆHTFI for hi
excites many eigenstates of the system at criticality (which exhibit long-range corre-
lations, making the dynamics difﬁcult to capture). Left panel: average magnetization
along x for different values of the number of hidden layers α of the RBM. Right panel:
integrated error, systematically reduced by increasing α. Taken from [206].

/J = 1/2, then suddenly quenching to h f

(for J > 0, or antiferromagnetic for J < 0) phase from a paramagnetic phase, with all spins
aligned along the transverse-ﬁeld for h (cid:29) J. Critical quench dynamics can be investigated
by preparing the system in an eigenstate of the Hamiltonian for some value of h = hi, then
/J = 1. As seen in Fig. 5.6, an RBM
suddenly switching the Hamiltonian parameters to h f
captures the dynamics up to about J t = 1.5, and increasing the number of hidden layers
α systematically improves the precision. As mentioned, more recent results have also been
obtained using a CNN on a two-dimensional system, whose dynamics are a challenge for TN
methods [227].

5.3.3 Imaginary-time evolution

The ﬁrst-order optimization scheme presented in section 5.3.1 to estimate the ground state
of many-body systems can be improved to yield more accurate results. For this purpose, it is
useful to consider imaginary time evolution, through Wick’s rotation t → iτ:

|ψθ(τ)〉 = exp(cid:0)−τ ˆH(cid:1)|ψθ(0)〉,

(5.43)

〉, with |φ

where ˆH is the Hamiltonian, and τ is a real number.
It can be shown that
〉 the exact ground state of the Hamiltonian, and provided
limτ→∞ |φ(τ)〉 = |φ
|〈ψθ (τ)|φ
〉| (cid:54)= 0. Furthermore, it can be shown that the convergence of imaginary-time evo-
lution toward the exact ground state is exponentially fast with τ, thus offering a systematic
way to ﬁnd the ground state. Analogously to real-time evolution, imaginary-time evolution
can also be performed variationally. This leads to the same type of equation as Eq. (5.39):

0

0

0

S ˙θ = −f ,

(5.44)

with the factor i missing due to the form of the exponent in Eq. (5.43). Hence, a very simi-
lar procedure is obtained as for the real time evolution in which we can update the weights
according to the update given by the equation above. To summarize both real and imagi-
nary time, the algorithm for variational time evolution is given in Algorithm 8. In the case
of imaginary-time evolution, the algorithm is typically modiﬁed in such a way that the S ma-
trix is regularized by adding a constant, Λ > 0, proportional to the identity: S → S + ΛI.
In this case, one recovers the stochastic reconﬁguration method, as originally introduced by
S. Sorella [249,250].
In Fig. 5.7, results are shown for imaginary-time evolution performed on

118

Neural-network quantum states

Algorithm 8 Real (ξ = i) or imaginary (ξ = 1) time evolution algorithm for NQS

θ ← random initialization
for i = 1 to nsteps do

Calculate Spp(cid:48) and Fp
Get ˙θ by inverting the equation (cid:80)
Update θ using an ODE integrator

end for

p(cid:48) Spp(cid:48) ˙θ

p(cid:48) = −ξFp (and possibly regularizing)

Figure 5.7: Relative error of the ground state search using imaginary time evolution
with respect to the exact solution. Left panel: relative error for the ground state of
the transverse-ﬁeld Ising model, for various values of h/J as a function of the number
of α = M /N , M being the number of hidden units for N = 80 spins. Center panel:
Same plot for the ground state of the antiferromagnetic Heisenberg model, compared
to another variational ansatz, the Jastrow wave function. Right panel: comparison
with state-of-the-art TN results, showing that NQSs perform at least as good. Taken
from [206].

the transverse-ﬁeld Ising model and the Heisenberg model [206]. These results show two im-
portant features: (i) using an RBM ansatz, the relative error can be systematically reduced by
increasing α = M /N , with M the number of hidden units and (ii) the results achieve a higher
precision than state-of-the-art TN methods.

5.3.4 Fermionic systems

The classical simulation of fermionic systems is known to be a very interesting and difﬁcult
problem. Its difﬁculty arises from the fact that fermionic operators obey anti-commutation
relations which constrain the wave functions of fermionic systems to be anti-symmetric under
particle exchange. Due to the infamous sign problem [251], all the known quantum Monte
Carlo methods become computationally extremely expensive. Nevertheless, fermionic systems
are ubiquitous in many-body quantum physics, and in high energy physics as the elementary
constituents of the matter, i.e., quarks and leptons, are fermions. Also, in chemical reactions,
electrons play a fundamental role, and they are fermionic particles as well.

One convenient approach to simulate fermionic systems is based on the mapping of the
fermionic degrees of freedom to spins. A generic protocol is the Jordan-Wigner transformation,
which enables us to map fermionic problems to interacting spin problems.12 Historically, this
technique has been used to solve spin models [253]. However, here we do the opposite: we
map fermionic operators to spin operators in order to use NQSs and the techniques presented
throughout the chapter.

12There are other transformations that have been developed in the context of quantum simulation, such as the

Bravyi-Kitaev encoding [252].

119

Neural-network quantum states

Let us consider the creation and annihilation fermionic operators acting on site j, ˆc†

j and

ˆc j, respectively. The Jordan-Wigner transformation prescribes:

=

ˆc j

=

ˆc†
j

(cid:33)

ˆσz
k

ˆσ−
j

(cid:33)

ˆσz
k

ˆσ+
j ,

(cid:32) j

(cid:89)

k=1

(cid:32) j

(cid:89)

k=1

(5.45)

(5.46)

(cid:128)

(cid:138)

ˆσx
j

= 1
2

± i ˆσ y
j

where ˆσ±
denote the spin raising and lowering operators. The ﬁrst term in
j
the transformation provides a phase that can be ±1 depending on whether the number of
occupied fermionic modes is even or odd in sites k = 1, . . . , j. We can conveniently rewrite
= ˆc†
this term using the relations ˆσz
= n j. This ensures that the
j ˆc j
j
resulting operators fulﬁll fermionic anti-commutation relations.

− 1, and ˆσ+

= 2 ˆσ+

j ˆσ−

j ˆσ−

j

j

For example, using this transformation, we can map a free fermions Hamiltonian in one

dimension:

ˆH = − 1
2

(cid:88)

j

ˆc jˆc†

j+1

+ ˆc†

j ˆc j+1

to an interacting spin Hamiltonian of the form:

ˆH = − 1
2

(cid:88)

j

ˆσ+

j ˆσ−
j+1

+ ˆσ+

j+1 ˆσ−

j

(5.47)

(5.48)

In this form, we can implement all the methods described throughout this chapter 5.

The main issue with this transformation is that it does not generalize well for arbitrary
dimensions. In higher dimensions, the Jordan-Wigner transformation makes the Hamiltonian
non-local, making it unfeasible to tackle with most standard techniques. Recently, different
mappings for fermionic degrees of freedom that work in higher dimensions have been pro-
posed. These are not general mappings, but rather are tailored to speciﬁc problems. For
example, we can map local Hamiltonians in more than one dimension to local bosonic Hamil-
tonians for certain speciﬁc gauge theories [254, 255].

As mentioned, a problem of current interest is calculating the electronic structure of
molecules [207]. This problem is important for applications in chemistry, and was in fact
known as one of the ﬁrst instances of the quantum many-body problem mentioned by Dirac
[205]. In this context one is generally interested in ﬁnding the ground state energy as a func-
tion of some physical parameter, such as the distance between two nuclei for a diatomic
molecule. This way, by looking at the minimum of the energy, one can know what the sta-
ble geometry of the molecule of interest is.

This is done by ﬁnding the ground state of an interacting fermionic Hamiltonian on a lat-

tice, usually of the form:

ˆH = (cid:88)

ti jˆc†

i ˆc j

i, j

+ (cid:88)
i, j,k,l

Ui jkl ˆc†

i ˆc†

j ˆckˆcl ,

(5.49)

where ti j is a single-body hopping term, Ui jkl is a two-body interaction strength, and ˆci
i ) is
a fermionic annihilation (creation) operator for mode i. 13 The Jordan-Wigner transformation
changes this Hamiltonian to the form

(ˆc†

ˆH = (cid:88)

ar Sr

r

(5.50)

13this can be in a real-space or a momentum-space basis, which we leave unspeciﬁed for the sake of generality.

120

Neural-network quantum states

Figure 5.8: Ground state energies of (a) C2 and (b) N2 molecules as a function of
nuclear separation, given by various techniques compared with results for an RBM
ansatz with M = 40 hidden units. CCSD(T): coupled-cluster approaches, FCI: full-
conﬁguration interaction. Taken from [207].

with ar some coefﬁcients and Sr Pauli strings formed of compositions of elements of the set
of single-qubit operators {(cid:49), ˆσx , ˆσ y , ˆσz}.
In other words, we now have an interacting spin
problem and, while the resulting Hamiltonian is not necessarily k−local, it can be shown that
the computation of the local energy estimator is still efﬁciently realized. These mappings are
therefore amenable to variational searches based of the ground states based on NQS.

In Fig. 5.8, we show the dissociation curves (ground state energies) as a function of the
nuclear separation for molecules of C2 and N2 provided by various numerical methods. By
using an RBM ansatz and a simple Jordan-Wigner transformation, one is able to recover results
that are competitive with recent full-conﬁguration interaction results, which demonstrates the
versatility and power of NQS. Finally, we mention that since the introduction of NQS, there has
been a large number of different architectures proposed for quantum chemistry, that construct
antisymmetric wave functions explicitly [256, 257].

5.3.5 Classical simulation of quantum circuits

Another promising direction for NQS is the classical simulation of quantum circuits. Indeed,
current classical simulation methods for large quantum circuits (of the order of at least 50
qubits) rely on TN methods that are explicitly restricted by entanglement. In particular, TNSs
cannot capture volume-law entanglement scaling, which quickly arises in quantum circuits,
whereas certain NQS architectures such as deep CNNs can [230]. In this context, NQSs can
be investigated in two somewhat orthogonal directions: one could use quantum circuits to
probe the limits of their capacity and trainability, and NQSs can be used to push the classical
simulation limits of quantum hardware.

Let us consider a quantum circuit deﬁned by a set of D gates G = { ˆGi

}D
i=1, each gate being
deﬁned by a unitary operator ˆGi. After each gate, the variational state must be updated so
as to capture the application of the previous gate. The following variational distance must
therefore be minimized, for each gate:

L( ˜θ) = dist (cid:0)|Ψ ˜θ

〉, ˆGi

|Ψθ〉(cid:1) ,

(5.51)

with ˜θ the parameters to be optimized, θ the parameters of the previous variational state, and
= ˆσx ). One
ˆGi the unitary operator corresponding to gate G (for instance, for a NOT gate, ˆGi
can apply this procedure for each gate G and obtain the output state at the end of a circuit,
after D optimizations. Note that ˆGi must be a k−local gate, or else the minimization procedure

121

Neural-network quantum states

cannot be carried out (this is reminiscent of ground state search). This is rarely a problem,
since universal gate sets can be constructed with only single- and two-qubit gates. With this
condition, minimizing Eq. (5.51) resembles closely the ground state optimization. One can
develop this expression using the inﬁdelity, see Eq. (5.38)], and obtain:

L( ˜θ) = 1 − 〈Gloc

(θ, ˜θ)〉|Ψ ˜θ
|2〈Gloc
(cid:12)
(cid:12)
(cid:12)s(cid:48)(cid:11)
(cid:10)s
(cid:12) ˆGi
Ψ ˜θ
(s)

(θ, ˜θ)(s) = (cid:88)
s(cid:48)

( ˜θ, θ)〉|Ψθ|2

Ψθ(s

(cid:48)),

with Gloc

(5.52)

(5.53)

where we have deﬁned Gloc as a local estimator, similarly to the procedure desribed for ground
state search. As often in ML, the minimization of L( ˜θ) may be inaccurate, which reduces the
overall ﬁdelity of the simulation. Using an RBM architecture, however not all gates need to
be approximated through minimization of the loss above. Some gates can be applied "ana-
lytically", i.e., it is possible to ﬁnd the exact update on the parameters of the network so as
to match the applied gate. In general, it is not possible to realize these exact updates for all
gates of a universal gate set, or else one could simulate any quantum circuit with an RBM with
inﬁnite precision by obtaining the exact parameter update for each gate. For example, let us
consider a Z gate acting on spin s j deﬁned by the operator ˆG = ˆσz
j . The action of such an op-
erator on a basis state |s〉 is simply ˆG |s〉 = (−1)s j |s〉. The RBM parameters before the gate are
deﬁned as θ = (bv, bh, W ) and the parameters after the gate are deﬁned as ˜θ = (˜bv, ˜bh, ˜W ).
The parameter update is given by the solution of the following equation (with C a constant):

Ψ ˜θ
(s) = C 〈s| ˆG|Ψθ〉
˜bv, j s j = C(−1)s j ebv, j s j ,
e

(5.54)

(5.55)

= bv, j

which is simply ˜bv, j
+ iπ for C = 1. The simpliﬁcation in the previous equation is
due to the fact that this gate acts trivially on the other parts of the RBM amplitude, deﬁned
in section 5.2.1. Details of how to apply other gates analytically can be found in Ref. [258]
and [259]. In this last reference, authors classically simulate the circuit corresponding to the
quantum approximate optimization algorithms (QAOA) using an RBM ansatz. This quantum
algorithm enables one to access the solution of a certain class of combinatorial optimization
problems. The corresponding circuit, which is quite shallow, can be implemented on current
hardware [260]. In Fig. 5.9, one can see that the results obtained by simulating the quantum
circuit with the RBM ansatz closely matches the result of the exact simulation, enabling one
to ﬁnd the solution of the optimization problem for large systems. Authors also estimate
a signiﬁcant advantage over TN methods.14 Indeed, in the left panel of Fig. 5.9, one can see
that the required bond dimension required to reach the same results to that of the RBM would
quickly become dauntingly large when using an MPS.

Alternatively, authors of Ref. [261] have proposed to simulate quantum circuits using
a transformer architecture. A transformer is a deep learning model that adopts the mech-
anism of self-attention, differentially weighting the signiﬁcance of each part of the input
data [262]. Using this framework, a practical algorithm to simulate quantum circuits using
a transformer ansatz responsible for the most recent breakthroughs in natural language pro-
cessing was introduced in Ref. [261]. This framework allows for the simulation of circuits that
build Greenberger-Horne-Zeilinger and linear graph states of up to 60 qubits.

5.3.6 Open quantum systems

The idea of using NNs to represent quantum states was also applied to open quantum systems.
An open quantum system is a physical system that interacts with an environment, for example

14Based on an extrapolation of numerical simulation data.

122

Neural-network quantum states

Figure 5.9: Results for the classical simulation of QAOA for a 3-planar graph. The left
panel shows 〈C〉, the approximated cost function for various values of p, the depth
of the quantum circuit. The dashed line corresponds to the exact simulation of the
p = 1 quantum circuit which the RBM simulation accurately reproduces for this task.
Right panel: Estimation of the required bond dimension in a Matrix Product State
(MPS) simulation of the QAOA circuit to match the accuracy of the RBM. For N = 54
qubits, the required bond dimension is of about 104, which amounts to using billions
of parameters, whereas the RBM uses a few hundred. Taken from [259].

an array of atoms interacting with an electromagnetic ﬁeld. Rather than describing the full
system+environment ensemble, one is generally only interested in properties of the system (in
the example above, the atoms) and one only keeps an effective description of its interaction
with the environment (the ﬁeld). This description enables one to understand effects such as
decoherence.
In the Born-Markov approximation, the time evolution of an open quantum
system is given by the Lindblad master equation [263]

∂

t ˆρ = −i[ ˆH, ˆρ] +

D
(cid:88)

i=1

(cid:0) ˆJi ˆρ ˆJ †

i

− 1
2

{ ˆJ †
i

ˆJi, ˆρ}(cid:1) ≡ L[ ˆρ],

(5.56)

where ˆρ is the system density operator, ˆH is the system Hamiltonian (ħh = 1), and ˆJi are
so-called jump operators, that describe the system-environment interaction. We have also
deﬁned L, the Liouvillian, which is to open quantum systems what the Hamiltonian is to closed
quantum systems (up to an i) – their time evolution generator. Many works focus on ﬁnding
the steady state that corresponds to the state ˆρ which satisﬁes L[ ˆρ] = 0, or the dynamics of
particular systems, which means one must in general integrate Eq. (5.56) in time, analogously
to non-equilibrium dynamics of closed systems. A ﬁrst difﬁculty one faces is ﬁnding a correct
representation for ˆρ in terms of an NN. Indeed, a density matrix is harder to represent than
a wave function, because it has to be Hermitian, semi-positive, and of trace one.
In fact,
a general method to encode a density matrix into arbitrary NNs has still not been found. The
key point of these works is that one can always purify a density matrix, and write its elements
as

(cid:10)s

(cid:12)
(cid:12) ˆρ(cid:12)
(cid:12)s

(cid:48)(cid:11) = (cid:88)
s(cid:48)

Ψ(s, s

(cid:48))Ψ∗(s, s

(cid:48)),

(5.57)

with Ψ(s, s(cid:48)) the puriﬁcation that belongs to the joint Hilbert space composed of the system
and an imaginary ancilla (whose Hilbert space is of at least the same dimension as the system’s
Hilbert space). With an RBM architecture, one can encode the puriﬁcation Ψθ(s, s(cid:48)) with an NN,
and the RBM architecture enables one to analytically trace out the ancilla spins s(cid:48) without
explicitly performing the summation which in general requires exponentially many operations.
For more details, see [264–267] and references therein.

123

Neural-network quantum states

Figure 5.10: Expectation values of observables ˆσ
i , k ∈ {x, y, z} at
the steady state of the open system described by the transverse-ﬁeld Ising Hamil-
i as a function of g/γ, with g the magnetic
tonian and jump operators ˆJi
ﬁeld strength. Results are shown for both the puriﬁed RBM approach and the POVM
approach with a Transformer network. Taken from [268].

γ ˆσ−

i ˆσk

(cid:112)

=

k

= 1/N (cid:80)

A second more recent approach proposes to view the density matrix as a probability distri-
bution over positive operator-valued measures (POVMs), and represent the resulting distribu-
tion using models employed in general density estimation, such as RNNs and ARNNs. In this
formalism, the density matrix is simply written as:

ˆρθ = (cid:88)

pθ(a) ˆMa,

a

(5.58)

with ˆMa POVMs that belong to a chosen complete set of POVMs. This could, for example,
be all the operators composed as tensor products of the Pauli operators and the identity. In
this picture, one simply needs to encode the probability distribution pθ(a) with an NN. This
POVM-based representation is motivated by the fact that the density matrix can be viewed
as an ensemble of 4N measurements, which is naively how experimentally one performs to-
mography to reconstruct the density matrix. This method alleviates the constraint on using
an RBM for open systems, but does not guarantee positivity of the density matrix, which can
lead to unphysical states. However, a certain number of results using the POVMs encoding
are promising [268,269], and understanding in which regime they work best is a key research
direction.

124

Neural-network quantum states

Finding the steady-state(s) of open quantum systems is both challenging, due to the daunt-
ing size of the Liouvillian one would need to diagonalize (4N for a spin system of N spins),
and interesting, for example, for the study of dissipative phase transitions [270]. Once
a parametrization ˆρθ of the density matrix is constructed, one can simply minimize the fol-
lowing cost function:

L(θ) = 〈 ˆρθL†L ˆρθ〉 .
The obtained state corresponds to the zero eigenvalue is zero of the Liouvillian, which is the
steady-state. For details about the procedure and how to retrieve the gradients, see [264–
267]. As one can see in Fig. 5.10, this method has been applied to the dissipative version of
the transverse-ﬁeld Ising model, with good results for both a POVM approach and an RBM
approach. In the former case, the expressive power of the network is higher, but the positivity
of the density matrix is not enforced. A clear picture of when each approach fails or succeeds
is still lacking, and is an important research direction. The dynamics of open quantum systems
is not described in detail here, but stochastic reconﬁguration can also be used for open systems
in both the RBM [267] and POVM [269] approaches.

(5.59)

5.3.7 Quantum tomography

Reconstructing a quantum state from measurement data is a challenging task, as one is faced
with an exponentially large number of snapshots in the system size to obtain a faithful recon-
struction.

Quantum tomography is the task of reconstructing a quantum state from measurement
data. Fully reconstructing a state is tedious as it requires an exponential number of
measurements in the system size. This is a major problem for experimentalists who
want to demonstrate quantum effects and protocols. Neural networks can once again
help in this task, and they have been successfully applied to reconstruct non-trivial
quantum states from a polynomial number of measurements.

Let us consider the task of reconstructing a wave function |ψ〉 from a limited number of
snapshots |ψ(s)|2 obtained by performing projective measurements in some basis spanned by
〉, with si some local quantum numbers and N the size of the system. Then,
|s〉 = |s1, s2, . . . , sN
the task, in the NQS language, is simply to minimize:

dist (|ψθ〉, |ψ〉)

min
θ

(5.60)

with θ being some variational parameters, and |ψθ〉 the variational state to optimize,
parametrized by a neural network. The architecture of this network is left unspeciﬁed here,
and all architectures work provided training can be performed efﬁciently. Many distances can
be considered, but we here focus on the Kullback-Leibler (KL) divergence (see section 2.3) as
was ﬁrst presented in the work by Torlai et al. [271]. It is deﬁned as:

DKL

(p||q) = (cid:88)
x∈P

p(x) log p(x)
log q(x)

(5.61)

for two probability distributions p and q, deﬁned on the same space P. The application to
quantum states is straightforward, as one can obtain probability distributions from the Born
rule, i.e., pθ(s) = |ψθ(s)|2, q(s) = |ψ(s)|2. By taking x to be conﬁgurations s in some set S of
snapshots, one can simply minimize:

DKL

(θ) = (cid:88)
s∈S

|ψθ(s)|2 log |ψθ(s)|2
log |ψ(s)|2

125

(5.62)

Neural-network quantum states

which concludes one possible approach.

However, recall that a quantum state is not simply a probability distribution. A probability
distribution can always be deﬁned from a quantum state, but not the reverse. More explicitly,
we want to reconstruct the full quantum state whose amplitudes are ψ(s) = (cid:112)Q(s)eiφ(s). By
minimizing Eq. (5.62), information about the phase, φ(s), is lost. This difference is crucial and
is at the heart of many issues in learning quantum states. As mentioned in chapter 5, learning
the phase of a frustrated quantum state is challenging [272]. The elegant solution to this issue
is to consider measurements performed in different bases. Indeed, the form of the quantum state
in a different basis involves the interference between amplitudes in different bases. Hence,
matching the probability distribution deﬁned by snapshots in different measurement bases
leads to the correct quantum state as long as the bases contain enough information about the
quantum state. Mathematically, one can simply replace Eq. (5.62) by:

(θ) = (cid:88)

(cid:88)

DKL

B

s∈SB

|ψB
θ

(s)|2

(s)|2
log |ψB
θ
log |ψB(s)|2

(5.63)

|ψ〉 with ˆUB
where SB is the set of snapshots of the quantum state in basis B, and ψB(s) = 〈s| ˆUB
a unitary operator. Then, gradients are found as usual, either with automatic differentiation
or analytically with simple models such as RBMs.

In Fig. 5.11, various observables are shown for a synthetic state and a reconstructed state.
The synthetic state approximates the ground state of the Heisenberg model in a triangular lat-
tice, that authors of the corresponding work generated with tensor network simulations [82].
The reconstructed state was reconstructed employing ideas presented in this section with
an RNN architecture (for more details, see the introduction or chapter 5 and section 7.2),
using a POVM representation of quantum states (for more details, see section 5.3.6). Note
that the approach has been extended to reconstructing mixed states [273], although supple-
mental care must be taken to avoid issues linked to positivity of the reconstructed density
matrix, similarly to what was presented in the above.

Experimentally, one can implement this strategy by applying rotations with, for instance,
laser pulses, and then measure the system repeatedly. In Ref. [274], authors demonstrate the
ﬁrst state reconstruction from experimental data from a programmable array of Rb atoms,
using an RBM architecture. Here snapshots of the wave function in the ˆσz basis are obtained
through site-resolved ﬂuorescence imaging. A challenge that arises when using real data is
that noise is introduced, which stems from measurement errors, leading to a set of snapshots
|ψ(s)|2 that imperfectly match the state of the system. This is taken care of in this work
by adding a noise layer to the neural network, with which the snapshots are transformed in
order to ﬁlter out the noise during training. At the expense of increasing the total number
of parameters in the network, this is a quick and easy strategy to deal with experimental
noise, that has enabled high ﬁdelity state reconstruction. Since performing quantum state
tomography with NQS has been proposed, there has been substantial efforts to implement it
on real-world experiments. For more details on experimental challenges of such proposals,
see for instance references [275–279]. The underlying principle behind these approaches
is that with a polynomial number of bases B and a polynomial number of snapshots, one
should be able to reconstruct states belonging to a certain class (not fully random states, for
instance, which contain almost no structure). As underlined previously, this class is not exactly
known, and is the subject of current research. To draw an analogy with images, images are
not fully random; they contain a lot of hidden structure, that can be learned by a properly
designed and trained neural network. The hope is that the same is true for quantum states,
and investigating the limits of such techniques could also help us understand in more detail

126

Neural-network quantum states

Figure 5.11: Various observables corresponding to the ground state of the Heisenberg
model on a triangular lattice with N = 50 spins. (a) Average magnetization along x
for each spins i. (b) Spin-spin correlation function between the spin at site 1 with
spins at site i. (c) and (d) Average spin-spin correlation between the ﬁrst and the
i-th spin. One can see that all observables are reproduced with a very high precision.
Adapted from Ref. [82].

their hidden structure, beyond what has been found with entanglement properties through
the study of TNs. As a ﬁnal consideration, let us mention that in recent years there has been
a signiﬁcant number of results that show that one can in fact predict selected properties of
complex quantum systems without reconstructing the full quantum state. This ﬁeld has been
coined under the name of “shadow tomography” (see [200, 280] and is very active. The price
to be paid in these methods however is that the number of required measurements typically
scales exponentially with the locality of the operator averages to be reconstructed, albeit with
a runtime that is typically better than the naive direct measurement of the observables from
data. In contrast to these approaches, in this section we have instead considered the task of
training a low-dimensional representation of the full wave function from a limited number of
measurements.

127

𝝈1⋅𝝈2𝜎𝑖𝑥𝜎𝑖𝑧𝜎𝑖𝑧𝝈1⋅𝝈2Neural-network quantum states

5.4 Outlook and open problems

We hope to have provided enough material to stimulate further research in the growing ﬁeld
of NQSs. Here is a non-exhaustive list of open problems and challenges related to the above
discussion:

• Capacity of NQS. Some works have proven the capability of NQSs to represent volume-
law entanglement, which means they could outperform TNSs for strongly correlated and
two- and three-dimensional systems [221,230,231]. Others have proven the equivalence
of RBMs with matrix-product states, meaning that the former cannot represent more
states than the latter [222]. Even though general theorems have been found, knowledge
about speciﬁc architectures is still rare, and understanding which architectures perform
better on which problems is a crucial point. In addition, proving representativity does
not mean that the models can be efﬁciently trained, thus understanding how the training
of NQS models works is key.

• Long-time dynamics. To this day, no method has proven to show stable long time
dynamics for NQSs [248]. However, progress has been made thanks to regularization
techniques [227]. A recent work has proposed to use an implicit method [247], enabling
one to go beyond stochastic reconﬁguration. However, ample results on large lattices
are still lacking for such an approach .

• Open quantum systems. No general method of encoding a density matrix into an arbi-
trary neural network has been found yet; one is either forced to use an RBM, which has
known limitations, or one can use a POVM approach, which may fail due to non-positive
density matrices.

• Frustrated systems. Finding the ground state of frustrated systems with an NQS ap-
proach has proved to be challenging [272], and understanding exactly how one can
improve the optimization of the procedure to learn the phase (which has a nontrivial
sign structure) is of particular interest.

• Simulation of quantum circuits. Few results have been obtained with networks other
than RBMs, and investigating how different circuits affect the accuracy of the chosen
ansatz can lead to results in two ways: understanding the complexity of a given circuit
and the limitations of the chosen ansatz.

• Quantum tomography. Quantum state tomography based on neural networks is still in
its beginnings. So far, it has only been explored numerically on toy models and small
experimental settings where traditional quantum state tomography is still feasible. It is
likely that its real beneﬁts may emerge in the context of estimation of difﬁcult quantities
in quantum simulation. In this setting, the complexity of estimation arises because even
simple quantities such as the energy and other correlation functions can have high vari-
ance. This implies that some of these quantities have a sample complexity which can
grow quickly with the size of the system.

• Extension to continuous Hilbert spaces and bosonic systems. For now most tech-
niques and works have focused on systems with discrete degrees of freedom (such as
spins). Extensions to continuous Hilbert spaces have been only recently directly ad-
dressed, for example in the context of quantum chemistry [256, 281] and nuclear mat-
ter [282]. Efﬁcient encodings for bosonic Hilbert spaces would also be of particular
interest for photonic systems, for example, that are usually treated with mean-ﬁeld-like
approaches.

128

Neural-network quantum states

• Applications in quantum information. As mentioned previously, NQS have been used
to simulate quantum circuits. More recently, they have also been applied to quantum
codes [283] for quantum error-correction and quantum communication. In this paper,
the authors demonstrate that efﬁcient quantum codes can be learned by NQS according
to which noise channels a physical system is subject to. NQS have not been widely used
for quantum information yet, and we expect them to be a useful tool for this ﬁeld in the
coming years.

Further reading

• Carleo, G. & Troyer, M. (2017). Solving the quantum many-body problem with artiﬁcial
neural networks. The original paper by Carleo and Troyer that introduced NQS [206].

• Becca F. and Sorella, S. (2017). Quantum Monte Carlo Approaches for Correlated Sys-
tems. A comprehensive book that includes details on quantum Monte-Carlo methods,
and variational states [250].

• Vicentini, F. et al. (2021). NetKet 3: Machine learning toolbox for many-body quantum
systems. The paper accompanying the open-source library NetKet 3, which contains
an extensive discussion of how to implement several algorithms introduced in this chap-
ter, as well as a collection of tutorials showing how to solve some benchmark problems
with NQS [244, 284].

• Carrasquilla, J. & Torlai, G. (2021). How to use neural networks to investigate quantum
many-body physics. A recent tutorial by Carrasquilla and Torlai that includes interesting
applications and code snippets can help anyone who wants to start in the ﬁeld [285].

• Carleo, G. (2017). Repository for example codes presented at the “Machine Learning
and Many-Body Physics” workshop. Notes, exercises, and code produced for the 2017
Beijing workshop on Machine Learning and Many-Body Physics [286].

129

Reinforcement learning

6 Reinforcement learning

So far, we have encountered multiple ML scenarios featuring supervised or unsupervised learn-
ing problems where we want to infer some labels, predict certain values, or ﬁnd patterns in
the data. In this chapter, we describe a different approach: learning strategies.

In the supervised learning framework, we can think of a student that learns from a teacher
who knows the correct answers to all possible questions within a given domain. In this scheme,
the student is limited by the knowledge of their teacher and can never surpass it or address
questions outside the teacher’s expertise. To overcome this limitation, in reinforcement learn-
ing (RL), we get rid of the teacher and let the student try things out and learn from the resulting
experience. Just like us, humans, the student learns from the interaction with an environment,
understands the consequences of its actions, and ﬁnds strategies to achieve particular goals.
In RL, we refer to the student as the agent, as it can interact with its environment.

For instance, let us consider the case in which we teach an agent to play chess. A supervised
learning approach would consist of training a ML model to reproduce the moves from recorded
chess games from the best players in the world. In this setting, given a state of the game, i.e.,
the position of the remaining pieces on the chessboard, the model predicts the move such as
reference players would do. However, this approach suffers from some major shortcomings.
For example, there is no single optimal move for every situation, and they strongly depend on
the game strategy taken by the players. As a result, the agent may be unable to consistently
execute a strategy through various actions. Additionally, the agent’s performance is ultimately
limited by the quality of the training data, meaning that it may be impossible to outperform
the reference players. We refer to section 6.6.2 for a related example.

Instead, we can let the agent play chess games, either against various opponents or even
against itself, without providing any additional knowledge besides the rules. In that case, it
develops its own understanding of the game and devise its own strategies. The resulting agent’s
potential is far superior to the previous one, as it is not limited by its teacher. Nevertheless,
learning from experience may be challenging, provided that the quality of the actions is only
assessed at the very end of the game when the outcome is decided: victory or loss.1 Hence,
the agent must develop a deep understanding of the long-term consequences of the actions
from sparse environment feedback.

In this chapter, we introduce the ﬁeld of RL. We start with an intuitive view on the concept
of learning from experience and its mathematical foundations in section 6.1. Then, we present
two main approaches: value-based RL in section 6.2, and policy gradient in section 6.3. In sec-
tion 6.4, we combine the two paradigms, introducing actor-critic algorithms. Then, we provide
an alternative approach to RL, projective simulation, in section 6.5. Finally, we present a series
of application examples of RL in section 6.6, featuring superhuman performance in games as
well as various problems in quantum technologies.

6.1 Foundations of reinforcement learning

The general setting of any RL problem consist of two main elements: an agent, and an en-
vironment that it interacts with, as illustrated in Fig. 6.1. The environment contains all the
information deﬁning the problem at hand, e.g., the rules of a game, and it provides the agent
with observations and feedback according to its actions. The environment deﬁnes the set of

1In some cases, we may be tempted to add intermediate rewards, such as a bonus for taking out a piece from
the opponent. However, in doing so we effectively change the game and its goal and, as a consequence, we might
fail to ﬁnd the optimal strategy of the original problem.

130

Reinforcement learning

all possible states, s ∈ S, which can range from an empty set, in the case of a stateless en-
vironment (see the ﬁrst example in section 6.6.1), to a multi-dimensional continuous space.
For example, these could be all the possible conﬁgurations of a board game or all the possible
combinations of joint angles in a robot.

The agent can observe (sometimes only partially) the state s of the environment, and it can
choose an action a to perform, which may include the possibility to remain idle. The action
is chosen from the set of possible actions, a ∈ A, which is deﬁned by the environment and
can be state-dependent. For instance, the action of pushing forward a pawn in chess is only
possible if there is a free position in front of it. The actions may alter the state in which the
environment is found, and they can have deterministic or stochastic outcomes. In the chess
example, all the actions are deterministic.
In contrast, in the case of a walking robot, the
action to move forward may have different results: it can succeed in doing so, the robot may
trip, or it may even remain idle with a certain probability due to a hurdle or malfunctioning.
This information is encoded in the environment, and the agent may not have access to it.

Nevertheless, every time the agent performs an action, the environment provides it with
an observation of the new state together with a feedback signal called reward, r. The reward
can take any numerical value. It may depend on the previous state, the new state, and the
action that was taken. The main purpose of the agent is to maximize the obtained rewards by
the end of the task, and it is, therefore, the quantity that deﬁnes the objective task. Hence, the
agent obtains higher rewards when accomplishing the objective task or progressing toward
the goal, e.g., winning a game, while it might receive penalties when performing harmful or
bad actions, e.g., losing a game.

The central objective of any RL problem is to learn the optimal policy, π∗, that maximizes
the obtained rewards. A policy, π, dictates which actions to take given the observations,
and thereby deﬁnes the strategy followed by the agent.

In general, the policy can take any form, as we show in forthcoming sections. For example,
it can be a table assigning the best possible action to every possible state or a ML model that,
given a state, provides a probability distribution over all the possible actions. However, the
learned policy is speciﬁc to the problem. We summarize the introduced key elements of the
RL setting in Fig. 6.1.

Let us provide some insight on the main elements of the RL setting with a couple of ex-
amples. In the case of the chess game from chapter 6, the agent is one of the players. The
environment models the rules of the game, the opponent,2 and its states, that correspond to
the piece positions on the board.3 The state space contains all the possible board conﬁgu-
rations that can be reached within a game, e.g., excluding those where one of the kings is
missing. The action space corresponds to all the possible legal moves that can be made every
turn. In this case, the agent does not obtain rewards until the game is resolved. At this point,
the agent receives a positive or negative reward upon victory or defeat, respectively. In case
of a draw, the ﬁnal reward could be zero or even negative. The goal is to learn the policy that
yields the highest possible number of victories.

2The opponent could be the same agent, which would play against itself, but each agent would perceive the
other as part of their respective environment. This is known as self-play, and it helps exploring new strategies
faster.

3The state for chess can also contain extra information, such as whether castling is still possible. For the purpose

of this example, and to keep it simpler, we restrict ourselves only to the piece positions here

131

Reinforcement learning

Figure 6.1: Overview of the basic RL setting. The agent receives an observation
from the environment. Given the observation, it chooses the next action according
to its policy. The environment determines the outcome of the action, and it returns
an observation to the agent consisting of the new state and a potential reward.

As a second example, we consider a robot trapped in a maze. The robot can only see its
immediate surroundings and has to maneuver to reach a target location.
In this case, the
agent is the robot, and the environment models the maze, its walls, and the target location.
The state is the current position of the agent plus its immediate surroundings, and the state
space comprises all the reachable locations. The action space contains the moves in all possible
directions, and the environment ensures that the agent does not cross the walls. Hence, moving
into a wall would leave the agent in the same position and, therefore, would not modify the
state. As a reward, we can provide the agent with a constant negative reward after every move
in order to encourage it to take the least amount of steps toward the goal.

6.1.1 Delayed rewards

As we have previously introduced, the reward r is a key concept in RL. The agent learns to
maximize the reward and, therefore, it is the quantity that deﬁnes the problem. At a given
discrete time t, the agent observes a state st and performs an action at according to its policy.
Then, the environment presents the agent with a new state st+1 and a reward rt+1. Hence, r is
)
= r (st−1, at−1, st
time-dependent and it may depend on any of the other three quantities rt
(see section 6.1.3 for further details).

So far, we have brieﬂy talked about maximizing the rewards. In order to formalize the
RL objective, we need to introduce the notion of delayed rewards. They introduce the idea
of “looking ahead” to the agent, allowing it to account for the future rewards obtained along
a trajectory through the state space. However, we can penalize the rewards that are far into

132

OBSERVATIONACTIONactionstateactionstaterewardENVIRONMENTRL AGENTstaterewardReinforcement learning

Figure 6.2: Impact of the discount factor, γ, in RL algorithms. (a) A myopic algo-
rithm (γ → 0) may settle for a greedy policy that leads to early immediate rewards,
even if they are smaller than possible latter ones. (b) However, a long-term oriented
algorithm (γ → 1) might sacriﬁce early rewards in favor of larger late ones.

the future with a discount factor γ ∈ [0, 1].

The discount factor weights the rewards according to their temporal separation. This
way, immediate rewards have larger weights than those that are far into the future.
The RL objective is to maximize the discounted return, deﬁned as the weighted sum of
future rewards

T −t−1
(cid:88)

=

Gt

γk rt+k+1,

(6.1)

k=0
which accounts for the rewards obtained starting at time t until the ﬁnal time T .a

aIn RL, we typically consider ﬁnite trajectories. However, a discount factor 0 ≤ γ < 1 allows us to

consider inﬁnite trajectories T = ∞ with ﬁnite returns.

Notice that the return presents a recursive form that is essential for many RL algorithms

Gt

= rt+1

+ γGt+1 .

(6.2)

This concept draws inspiration from human psychology, and it mimics our daily observa-
tion that far-term rewards, even if high, are less desired than near-term ones, e.g., we favor
procrastinating instead of reading this article. We can distinguish two limits: for a small dis-
count factor, γ → 0, the return becomes myopic, i.e., immediate rewards predominate over
any other possible future ones. On the other hand, large discount factors, γ → 1, result in
equal weights for early and late rewards, which encourage long-term oriented strategies. This
includes, in particular, the deliberate choice to perform a few seemingly sub-optimal choices
in the beginning that, however, result in a far greater ﬁnal return. We depict the two cases in
Fig. 6.2.

Hence, the discount factor strongly affects the resulting policy. In fact, it deﬁnes the RL
task, as the agent aims to maximize the return, introduced in Eq. (6.1). Nevertheless, we often
rely on trial-and-error methods to ﬁnd the discount factor that best suits our needs.

6.1.2 Exploration and exploitation

In RL we encounter a trade-off between exploration and exploitation. In order to maximize the
return, the agent must exploit its knowledge about good strategies. However, the agent must

133

reward rtime treward rtime tgreedyexploration(b)(a)Reinforcement learning

explore other different actions in order to improve them, or even discover better strategies in
the future.

However, a learning algorithm cannot rely on exploration alone, as it would be reduced
to a brute-force search algorithm. Conversely, in a case of pure exploitation, the agent would
blindly commit to the ﬁrst working strategy that it found, even if it was highly sub-optimal.
Hence, we need to ﬁnd a balance between both regimes in which the agent can try several ac-
tions and progressively favor the best ones. This way, the exploration is conducted around the
most promising areas of the state and action spaces, heavily reducing the amount of experience
that the agent must gather in order to ﬁnd the optimal policy.

A common strategy to balance exploration and exploitation is the so-called (cid:34)-greedy policy.
In this case, the agent follows its policy to perform actions (exploits), and it may take a random
action (explores) with probability (cid:34) ∈ [0, 1] at any point. This approach encompasses both
paradigms: for (cid:34) = 1, we have full exploration, whereas we have full exploitation for (cid:34) = 0.
By tuning (cid:34), we interpolate between both regimes. A common practice is to start with high (cid:34),
to enforce early exploration, and decrease it during the training process.

6.1.3 Markov decision process

All RL problems are modeled by the same underlying mathematical structure: Markov decision
processes (MDPs). They constitute a general framework to model environments in which there
exists a notion of sequentiality between states. In such environments, the future is independent
of the past given the present. This is known as the Markov property.

In essence, the Markov property means that the current state is a sufﬁcient statistic
containing all the required information relevant to the possible evolution of the en-
vironment. In particular, we do not have any memory effects from previously visited
states. Formally, at any time step t,

p(st+1

|s0, . . . , st

) = p(st+1

|st

).

(6.3)

Mathematically, an MDP is a tuple (S, A, p, G, γ), respectively denoting the state space S,
the action space A, the dynamics p, the set of total returns G, and the discount factor γ. In this
formalism, the return G, together with the discount factor γ, determines the objective, and p
describes the dynamics of the environment,

(cid:48)

p(s

, r|s, a) = p(st+1

(cid:48)

= s

, rt+1

= r|st

= s, at

= a) ,

(6.4)

which corresponds to the joint probability of observing a new state s(cid:48) and obtaining a reward
r by performing action a in state s. For fully deterministic environments, p(s(cid:48), r|s, a) is either
zero or one.

From Eq. (6.4) we can derive all the relevant information about the environment. For

instance, state-transition probabilities are a central quantity in many RL algorithms:
(cid:48)|s, a) = (cid:88)

, r|s, a) .

p(s

p(s

(cid:48)

(6.5)

r

Furthermore, it allows us to determine the reward functions. In section 6.1.1 we brieﬂy intro-
duce the reward function r(s, a, s(cid:48)). In the most general form, the reward is jointly determined
with the state s(cid:48), as shown in Eq. (6.4).4 However, in many cases, we may need to consider

4In stochastic environments, the reward can be inherently sampled from a probability distribution. Consider

134

Reinforcement learning

the expected rewards for state−action pairs and state−action−next-state triplets:
r(s, a) = (cid:88)

, r|s, a) ,

r p(s

(cid:88)

(cid:48)

r(s, a, s

r
(cid:48)) = (cid:88)
r

s(cid:48)∈S

p(s(cid:48), r|s, a)
p(s(cid:48)|s, a)

.

r

(6.6)

(6.7)

In the iterative interaction between agent and environment, the agent chooses the actions
according to a policy. The policy is a mapping from states to the probability of performing
each possible action

π(a|s) = p(at

= a |st

= s) .

(6.8)

In the limit of deterministic policies, π(a|s) is one for a single action and zero for the rest.

The goal in RL is to modify the policy with the experience gathered from the interaction

with the environment to achieve the goal. This interaction generates trajectories of the form

s0, a0, r1, s1, a1, r2, s2, a2, . . . , sT ,

where all states, actions and rewards are random variables. This way, the agent performs
a trajectory through the state-action space τ = a0, s1, a1, . . . , sT with probability

p(τ) =

T −1
(cid:89)

t=0

p(st+1

|st , at

)π(at

|st

) ,

(6.9)

starting from an initial state s0. We denote the discounted return associated to the trajectory
as G(τ) = (cid:80)T −1
t=0

γt rt+1.

This entire formalism holds assuming the Markov property from Eq. (6.3), which implies
that the environment is memory-less. However, we may encounter situations in which the
environment has certain memory effects, such as games in which the execution of a sequence
of actions yields an additional effect at the end. In these cases, we may recover the Markov
property by considering an extended state space that already includes the memory. In return,
this implies that even deterministic Markovian dynamics on the full state space can give rise
to non-deterministic and non-Markovian dynamics on the smaller state space.5

6.1.4 Model-free vs. model-based reinforcement learning

We can distinguish between two main paradigms in RL: model-free and model-based RL. In the
ﬁrst setting, the agent does not have any kind of information about the underlying mechanisms
of the environment and it must purely learn by trial and error. In the second one, the agent
either has access to a model of the environment or it builds one from the gathered experience.
Then, the agent can use this model in order to plan ahead, inferring the result of a sequence
of actions before executing any of them, in order to choose the best possible ones.

Although we focus on model-free RL in the remainder of the chapter, we brieﬂy elaborate
on how to exploit the knowledge of a model. Building a model of the environment provides
the agent with an enhanced understanding of the problem and can potentially help it face new
situations. For example, in a case where an agent juggles a set of balls, if it has a good model

the game of blackjack: with the same hand (state) the action of settling may have different rewards depending on
the opponent’s hand (environment). Hence, the reward is stochastic.

5An analogous situation is encountered in the discussion of open quantum systems: non-unitary dynamics in

the subsystems arise despite a global unitary evolution of the system and its bath.

135

Reinforcement learning

of the laws of physics, it is much easier for it to learn to juggle a new set of balls with different
shapes and weights.

These models can take various forms, but a general formulation are fully characterizable
MDPs. This way, the model approximates the dynamics of the underlying MDP of the problem.
In some situations, the true model is too complex to be grasped, and we may simply try to
approximate the parts of the dynamics that are the most relevant for the problem. An example
of a simple model would be a ML algorithm that predicts both the expected next state and the
reward (st+1, rt+1
) at any time step t. Such
a model allows us to predict the outcome of a series of future actions given the current state,
and we can train it in a supervised way directly from the experience gathered by the agent.

) given the current state and an action (st , at

In continuous-action spaces, the model provides a direct connection between the input
action and the received reward, allowing us to employ backpropagation methods to maximize
the return instead of mere sampling from the environment. See section 7.1 for examples that
illustrate the process. In the case of discrete-state spaces, the model typically takes the form
of a search tree that we can explore at our advantage. Models are especially convenient in
situations where the interaction cost with the environment is very high, such as the realiza-
tion of a physical or chemical experiment. In these cases, we try to augment our dataset of
actual samples from the environment with artiﬁcial samples drawn from the model in order
to minimize the total sampling costs.

However, we do not always have access to a model or it may not be in our interest to build
one. Building models is costly, especially in cases where we have limited knowledge about
the environment, and they are only helpful when they are accurate. Furthermore, models are
often tailored to the speciﬁc problems. On the contrary, model-free RL algorithms come with
the advantage that they are agnostic to the problem at hand and, thus, they are more versatile.
Therefore, we focus on model-free RL for the rest of the chapter for pedagogical purposes, as
they prove useful on the full range of RL tasks. In particular, we provide an introduction to
policy-based and value-based RL in section 6.3 and section 6.2.1, respectively.

6.1.5 Value functions and Bellman equations

As we mention in the previous sections, the goal in RL is to ﬁnd the optimal policy π∗ that
maximizes the return, introduced in Eq. (6.1). Such a clear objective allows us to deﬁne value
functions that estimate how convenient it is for the agent to be in a given state or to perform
a certain action to accomplish the task. For instance, consider the case in which we are looking
for a treasure on a map. Being one step away from the treasure is, overall, much better than
being ten steps away. However, not all actions in the close position are equally good, provided
that one leads to the treasure but the others move away from it. This is quantiﬁed by the
expected future return that the agent may obtain given the current conditions. However,
given that the future rewards strongly depend on the actions that the agent will take, value
functions are deﬁned with respect to the policy.

136

Reinforcement learning

The state-value function, Vπ(s), of a state s under the policy π is the expected return
when starting at state s and following the policy π thereafter. We formally deﬁne it as

Vπ(s) = E[Gt

|st

= s, π] = E

(cid:150)T −t−1
(cid:88)

k=0

γk rt+k+1

(cid:153)
= s, π

(cid:12)
(cid:12)
st
(cid:12)
(cid:12)

(6.10)

In a similar way, the action-value function, Qπ(s, a), is the expected return when starting
at state s, performing action a, and then following the policy π:

Qπ(s, a) = E[Gt

|st

= s, at

= a, π] = E

(cid:150)T −t−1
(cid:88)

k=0

γk rt+k+1

(cid:153)
= a, π

= s, at

(cid:12)
(cid:12)
st
(cid:12)
(cid:12)

(6.11)

The advantage, Aπ(s, a), is the additional expected return obtained by following an ac-
tion a at state s, over the expected policy behavior:

Aπ(s, a) = Qπ(s, a) − Vπ(s) .

(6.12)

The value functions fulﬁll a recursive relationship that is exploited by many RL algorithms,
which stems from the recursive nature of the return Eq. (6.2). This allows us to write the
state-value function Vπ(s) as a function of the next states

= s, π] = E[rt+1
(cid:48)
p(s

+ γGt+1
, r|s, a) (cid:0)r + γ E[Gt+1

= s, π]
|st+1

|st

Vπ(s) = E[Gt

|st
π(a, s)(cid:88)
s(cid:48),r
π(a, s)(cid:88)
s(cid:48),r
+ γVπ(st+1

= (cid:88)
a
= (cid:88)
a
= E[rt+1

p(s

(cid:48)

, r|s, a) (cid:0)r + γVπ(s

(cid:48))(cid:1)

)|st

= s, π] .

= s

(cid:48)

, π](cid:1)

(6.13)

We can do the analogous derivation for the action-value function Qπ(s, a)

Qπ(s, a) = E[Gt

|st
(cid:48)
p(s

= s, at
, r|s, a) (cid:0)r + γ E[Gt+1

= a, π] = E[rt+1
|st+1

|st
+ γGt+1
, π](cid:1)
(cid:48)
= s

= (cid:88)
s(cid:48),r
= (cid:88)
s(cid:48),r
= E[rt+1

p(s

(cid:48)

, r|s, a) (cid:0)r + γVπ(s

(cid:48))(cid:1)

+ γVπ(st+1

)|st

= s, at

= a, π] ,

= s, at

= a, π]

(6.14)

from which the relationship Vπ(s) = (cid:80)
π(a|s)Qπ(s, a) becomes evident. These are the Bell-
man equations for the value functions, and they lie at the core of RL as they deﬁne the relation
between the value of a state s and its successors s(cid:48), recursively capturing future information.

a

These concepts introduce the notion of partial ordering between policies. A policy π
is better than another policy π(cid:48) if it yields a higher return. Hence, π > π(cid:48) if and only if
Vπ(s) > Vπ(cid:48)(s) ∀s ∈ S. Therefore, the optimal policy π∗ is such that it is better than or equal
to all the other possible policies.6 Hence, the optimal policy maximizes the value function.

6The ordering operator is not always deﬁned between policies. Two policies π, π(cid:48) cannot be ordered iff
∃ s, s(cid:48) ∈ S : Vπ(s) > Vπ(cid:48) (s), Vπ(s(cid:48)) < Vπ(cid:48) (s(cid:48)). However, for MDPs there always exist an optimal policy π∗ s.t.
π∗ ≥ π ∀π [287].

137

Taking the Bellman equations, Eqs. (6.13) and (6.14), π∗ is such that

Reinforcement learning

Vπ∗(s) = max

a
= max
a
= max
a

E[Gt
|st
E[rt+1
Qπ∗(s, a) .

= a, π∗]

= s, at
+ γVπ∗(st+1

)|st

= s, at

= a, π∗]

(6.15)

Notice that in this new Bellman equation there is a maximization over the ﬁrst action, as
opposed to the expectation over actions from Eq. (6.13). This is because the value of a state
under the optimal policy must be equal to the expected return for the best action. In a similar
way, we can ﬁnd the Bellman equation for the action-value function Qπ(s, a) for an optimal
policy π∗. Together, they deﬁne the set of the Bellman optimality equations:

These equations fulﬁll

Vπ∗(s) = max

a

(cid:88)

s(cid:48),r

p(s

(cid:48)

, r|s, a) (cid:2)r + γVπ∗(s

(cid:48))(cid:3)

Qπ∗(s, a) =(cid:88)
s(cid:48),r

(cid:48)

p(s

, r|s, a)

(cid:104)

r + γ max

a(cid:48)

Qπ∗(s

(cid:48)

, a

(cid:105)
(cid:48))

Qπ∗(s, a) = max

π

Qπ(s, a)

Vπ∗(s) = max

π

Vπ(s) = max

a

Qπ∗(s, a) .

We can deﬁne the optimal policy π∗(a|s) and action a∗ at a given state s as:

π∗ = arg max

π

Vπ∗(s)

∗ = arg max

a

Qπ∗(s, a) .

a

(6.16)

(6.17)

(6.18)

The optimal policy π∗ corresponds to the deterministic choice of the best action a∗
for a given state s according to the optimal action-value function Qπ∗(s, a). Due to
the recursive nature of the value functions, a greedy action according to Vπ∗ or Qπ∗ is
optimal in the long term.

In order to solve them directly, we need to explicitly use p(s(cid:48), r|s, a).7

The Bellman optimality equations Eq. (6.16) are, indeed, a system of equations with one
for every state.
If
p(s(cid:48), r|s, a) is known, we know the underlying model of the system, and thus we deal with
model-based RL, as discussed in the previous section. In a general model-free RL scenario,
it is unknown and, as such, we need additional methods to solve them, such as the ones we
introduce in the following sections.

6.2 Value-based methods

In value-based RL, the goal is to obtain the optimal policy π∗(a|s) by learning the optimal
value functions, as in Eq. (6.18). This way, we start with an initial estimation of the value
function for every state, Vπ(s), or state−action pairs, Qπ(s, a). Then, we progressively update
them with the experience gathered by the agent following its policy.

Given that the value functions are deﬁned with respect to a policy (recall section 6.1.5),
we need to deﬁne a ﬁxed policy for this family of algorithms. A common choice is an (cid:34)-greedy

7Due to the maximization step in Eq. (6.16), this is a nonlinear optimization problem.

138

Reinforcement learning

policy, as introduced in section 6.1.2, provided that the optimal policy is greedy with respect
to the optimal value function. Hence, learning the value function for such policy provides us
with the optimal one in the greedy limit.

One of the most straightforward and naive approaches to learn the value function would
be to sample trajectories τ ∼ p(τ) (Eq. (6.9)), and then use the return Gt to update our value
function estimation8 for every visited state st along the way:

Vπ(st

) = Vπ(st

) + η(Gt

− Vπ(st

)) ,

(6.19)

where η is a learning rate. We can do an analogous process for every visited state and action
along the trajectory to learn Qπ(s, a) instead.

However, with this approach we can only learn at the end of each trajectory, also known
as episodes, which can be very inefﬁcient in problems involving long episodes, or even inﬁnite
ones. On the contrary, temporal-difference (TD) algorithms exploit the recursive nature of the
value functions, Eqs. (6.13) and (6.14), to learn at every time step:

Vπ(st

) = Vπ(st

) + η (rt+1

+ γVπ(st+1

) − Vπ(st

)) .

(6.20)

Notice that, while Vπ(st
) is also an estimate. This is known as a boot-
strapping method, as the update is partially based on another estimate. Nevertheless, it is
proven to converge to a unique solution. The term in brackets is known as TD error.

) is an estimate, Vπ(st+1

The algorithm implementing Eq. (6.20) is known as TD(0), which is a special case of the
TD(λ) algorithms [288]. The analogous algorithm for the action-value function is known as
SARSA [289, 290]:

Qπ(s, a) = Qπ(s, a) + η (cid:0)r + γQπ(s

(cid:48)

(cid:48)) − Qπ(s, a)(cid:1) ,

, a

(6.21)

where we have recovered the notation s(cid:48), a(cid:48), r to denote the next state, action and re-
ward. Replacing the term Qπ(s(cid:48), a(cid:48)) by an expectation over the next possible actions, such
as (cid:80)
a(cid:48) π(a(cid:48)|s(cid:48))Qπ(s(cid:48), a(cid:48)), we obtain the expected SARSA algorithm [291]. If, instead, we take
a maximization, as in Eq. (6.22) below, we obtain Q-learning [292], for which we provide
a detailed introduction in the following section 6.2.1.

6.2.1 Q-learning

Q-learning is one of the most widely used TD algorithms due to its desirable properties [292].
Most of the TD algorithms that we introduce in the previous section learn the value functions
for their given policies, mainly (cid:34)-greedy policies. These include exploratory random actions
(recall section 6.1.2) that have an impact on the learned value functions. Therefore, the pol-
icy determines the result, and we must adjust (cid:34) during the training process to ensure their
proper convergence toward the optimal value functions. However, Q-learning always learns
the optimal action-value function regardless of the policy followed during the training.9

The goal is to directly learn the optimal Q-values, Qπ∗(s, a), hence the name Q-learning,
in order to obtain π∗(s|a) by performing greedy actions over them, as in Eq. (6.18).

8The return is an unbiased estimator for the expectation Vπ(st

) = E[Gt

|st , π] from Eq. (6.10). This is known as

a sample update, as we only use a single sample to determine the expectation.

9Q-learning is an off-policy algorithm, which means that the policy it learns (optimal π∗(a|s)) is different from
the one it follows in the training episodes. Algorithms like SARSA are on-policy, and learn the value function that
corresponds to the policy with which they generate the training data.

139

Reinforcement learning

We start by arbitrarily initializing our estimates Qπ(s, a) ∀s ∈ S, a ∈ A, which are typi-
cally stored in a table (see section 6.2.3 for an implementation with NNs). Then, we sample
trajectories τ ∼ p(τ) according to the policy to progressively update our estimates with the
relation

(cid:16)

r + γ max

a(cid:48)

Qπ(s

(cid:48)

, a

(cid:17)
(cid:48)) − Qπ(s, a)

.

(6.22)

Qπ(s, a) = Qπ(s, a) + η

We illustrate the process in Algorithm 9.

Algorithm 9 Q-learning
Require: learning rate η, maximum time T , policy parameter (cid:34)

Initialize Q(s, a) ∀s ∈ S, a ∈ A
while not converged do

Initialize s0
for t = 0 to T − 1 do

ξ ← uniform∈ [0, 1]
a ← uniform a if ξ ≤ (cid:34) else arg maxa Qπ(s, a)
Move to next state s(cid:48) and obtain reward r
Q(s, a) ← Q(s, a) + η (cid:0)r + γ maxa(cid:48) Q(s(cid:48), a(cid:48)) − Q(s, a)(cid:1) .

(cid:46) (cid:34)-greedy policy

end for
end while
return Q(s, a)

(cid:46) Optimal action-value function for all states and actions

This method is guaranteed to converge to the optimal action-value function as long as
all possible state−action pairs continue to be updated. This is a necessary condition for all
the algorithms that converge to the optimal behavior and it can become an issue for fully
deterministic policies. However, with Q-learning, we can have an (cid:34)-greedy policy with (cid:34) (cid:54)= 0
that ensures that this condition is fulﬁlled.

The key element is that, while the policy determines which states and actions are visited by
the agent, the Q-value update is performed over a greedy next action, as shown in Eq. (6.22).
This way, the learned Q-values are those corresponding to the greedy policy over them, which
is the one fulﬁlling the Bellman optimality equations Eq. (6.16).

6.2.2 Double Q-learning

Most of the TD algorithms suffer from a maximization bias that results in an overestimation
of the Q-values, which can harm the performance. Specially, in Q-learning, we encounter two
maximizations: one in the (cid:34)-greedy policy and one in the greedy target policy (Eq. (6.22)).
This way, we use a maximum overestimated value (see below) to update the maximum Q-
value, which corresponds to the greedy action taken by the policy, potentially incurring into
a signiﬁcant positive bias for Qπ(s, a).

The maximization over next possible actions in Eq. (6.22) is a sample estimate for the
maximum expected value maxa(cid:48) E[Qπ(s(cid:48), a(cid:48))]. However, it is a positively biased estimator,
provided that the sample estimate actually corresponds to the expected maximum value
E[maxa(cid:48) Qπ(s(cid:48), a(cid:48))] [293]. In Ref. [287] they provide a simple example to develop intuition
on the matter: suppose that the true Q-values for all actions in a state are zero and that our
estimates Qπ(s, a) are distributed around them taking positive and negative values. The max-
imum value is positive and, hence, it is an overestimation. The overestimation of the Q-values
can prevent the algorithm from learning the optimal policy [294].

We overcome this issue with double Q-learning [295]. This way, instead of learning a single
π(s, a). However, in order to update one, we

set of Q-values, we learn two: QA

π(s, a), and QB

140

use the other to estimate the value of its corresponding next greedy action:

Reinforcement learning

QA

π(s, a) = QA

π(s, a) − η

(cid:18)

r + γQB
π

(cid:18)
s

(cid:48)

, arg max
a(cid:48)

(cid:48)

QA

π(s

, a

(cid:19)

(cid:48))

(cid:19)
π(s, a)

− QA

,

(6.23)

where A, B are interchangeable. This approach avoids using the same estimate to determine
both the maximizing action and its value, yielding an unbiased estimate.

We learn both sets of values by randomly updating one at a time at every time step. The
only additional difference with respect to standard Q-learning is that we take actions following
an (cid:34)-greedy policy that combines the information of both QA
π(s, a), e.g., using their
sum or mean. With double Q-learning, we overcome a major limitation of Q-learning at the
price of doubling the memory requirements.

π(s, a) and QB

6.2.3 Implementing Q-learning with a neural network

In Q-learning, as we have introduced it in section 6.2.1, we store the Q-values, Qπ(s, a), for
every possible state−action pair. This approach allows us to ﬁnd the exact optimal action-value
function. However, it is only viable for small problems, as the memory requirement quickly
becomes unfeasible for moderately large ones.

In these cases, we must rely on an efﬁcient way to represent Qπ(s, a) ∀s ∈ S, a ∈ A. NNs
are a prominent candidate to approximate the action-value function, as introduced in Ref. [18],
with signiﬁcantly less parameters than state−action pairs (recall section 2.4.4). Using NNs to
learn the Q-values is known as deep Q-learning and the network is commonly referred to as
deep Q-network (DQN). DQNs take a representation of state in the input layer φ(s), and have
as many neurons as possible actions in the output layer, which encode Qπ(s, a; θ) ∀a ∈ A.
Here, θ denotes the set of learnable parameters of the neural network. This way, the DQN
provides the Q-value of all possible actions given a state.

Nevertheless, DQNs may become highly unstable when directly applying Algorithm 9 with

an update rule for the network parameters:

θ = θ + η

(cid:16)

r + γ max

a(cid:48)

(cid:48)

Qπ(s

, a

(cid:48)

(cid:17)
; θ) − Qπ(s, a; θ)

∇θQπ(s, a; θ) ,

(6.24)

which is analogous to a regression problem in which we minimize the MSE loss (Eq. (2.1))
between the target, r + γ maxa(cid:48) Qπ(s(cid:48), a(cid:48); θ), and the prediction, Qπ(s, a; θ), through gradient
descent. The instabilities are mainly due to correlations in consecutive observations along the
trajectories, correlations between target and prediction, and signiﬁcant changes in the data
distribution due to small variations in the parameters. The latter happen because the agent
follows an (cid:34)-greedy policy, and small changes in the parameters may change the actions that
have the maximum Q-value for the states, abruptly altering the course of the trajectories.10
We overcome these limitations with experience replay [296], and introducing a target network.

With experience replay, instead of learning at every time step, we store the experience
gathered along the episodes in a memory, which keeps the information of every transition
(s, a, r, s(cid:48)). Then, once the agent has gathered enough experience, it replays a randomly sam-
pled batch of transitions in its memory to compute the loss and update the DQN parameters.
This way, the agent alternates between episodes to gather experience and replaying it to per-
form the learning process. This technique removes the correlation between training samples

10Consider the case of two separate paths that lead to different treasures. We initialize the Q-values arbitrarily,
and the (cid:34)-greedy policy mainly takes the path with the highest one, while casually following the other with small
probability (cid:34). However, if the second one leads to a bigger treasure, its Q-value will eventually become the highest,
and the data distribution will suddenly change to mainly sample this path and casually take the other.

141

Reinforcement learning

and mitigates the sudden changes in data distribution. Furthermore, it allows the agent to
reuse the experience to prevent forgetting and re-learning.11

In order to remove the correlation between target and prediction, we consider a target net-
work, which is a clone of the DQN that we update at a different rate. While we update the DQN
parameters, θ, at every iteration, we only update the parameters of the target network, θ−,
copying θ every few iterations. Then, we use it to predict the target term maxa(cid:48) Qπ(s(cid:48), a(cid:48); θ−),
hence the name of the network. This ensures that the prediction, Qπ(s, a; θ), and the target
are uncorrelated.

Additionally, we can go a step further and use the target network for double Q-learning
(see section 6.2.2) in order to prevent the DQN from overestimating the action-value function,
as introduced in Ref. [297]. Thus, the overall implementation consists on gathering experience
by following an (cid:34)-greedy policy on the Q-values, Qπ(s, a; θ). Then, the agent replays randomly
selected transitions from the experience to compute the MSE loss function between the target
and the prediction, but using a target network to perform double Q-learning:

L = 1
n

(cid:18)

n
(cid:88)

i=1

+ γQπ

ri

(cid:18)
s

(cid:48)
i, arg max
a(cid:48)

Qπ(s

(cid:48)
i, a

(cid:48)

; θ); θ−

(cid:19)

(cid:19)2

− Qπ(si, ai; θ)

,

(6.25)

where i denotes the index in a batch of n randomly sampled transitions from the memory.
Then, we perform a gradient descent step over the loss in Eq. (6.25) to update θ. Finally,
every few iterations, we update the target network θ− ← θ.

6.3 Policy gradient methods

The main goal of RL is to ﬁnd the optimal policy π∗(a|s) that maximizes the expected return
for a given task. In policy gradient algorithms we try to directly ﬁnd the optimal policy by
proposing a parametrized ansatz πθ(a|s) and optimizing its parameters θ, similar to the vari-
ational wave functions from chapter 5. Hence, ﬁnding the optimal policy π∗(a|s) is equivalent
to ﬁnding the optimal set of parameters θ∗ that best approximates it πθ∗(a|s) ≈ π∗(a|s). This
parametrization can take several forms, such as a NN, and controlling the shape of the policy
may allow us to leverage prior knowledge about the task to obtain better results. Furthermore,
the policies are stochastic, which have a natural exploratory character and the ﬂexibility to also
approximate deterministic policies.

In order to optimize the parameters, we use an objective function Oπ that we aim to max-
imize. This can be any ﬁgure of performance, such as the state-value function Vπ, the action-
value function Qπ, or the return G. Having continuous parametrized policies, the objective
function changes smoothly with changes in the parameters, which allows us to compute their
derivatives. We approach the optimization by a gradient ascent method: we compute the
gradient of the expectation value ∇θ E[Oπ|πθ], and perform a small update of the parame-
ters θ. The expectation value is taken over the trajectories τ sampled according to the policy
(recall Eq. (6.9)).

Directly evaluating the gradient is not straightforward because it depends on the stationary
distribution of the states, to which we do not have access in model-free RL. Hence, it is difﬁcult
to estimate the effect of the policy update on the state distribution. However, the policy gradient
theorem [298,299] provides us with an analytical form for the gradient of the objective function
that does not involve the derivative over the state distribution.

11This is specially valuable when the experience is costly to obtain. For instance, if a robot receives severe

damage, having a memory allows it to keep learning from the situation without receiving further injuries.

142

Reinforcement learning

Policy gradient theorem: For any differentiable policy πθ(a|s) and objective func-
tion Oπ, the gradient of its expectation value ∇θ E[Oπ|πθ] can be expressed in terms
of derivatives acting exclusively on the logarithmic policy ∇θ log πθ(a|s). The term
∇θ log πθ(a|s) is often referred to as the score function.

To get some additional intuition on the above theorem, let us consider an example with
the total return G(τ) as objective function (see [287] for an extended proof with Vπ(s)). Thus,
we are interested in maximizing the expectation value E[G|πθ], which is performed over the
trajectories τ ∼ pθ(τ). We restate Eq. (6.9) to explicitly show the parameter dependence

pθ(τ) =

T −1
(cid:89)

t=0

p(st+1

|st , at

)πθ(at

|st

).

Therefore, we can write the expectation as

E[G|πθ] = (cid:88)

pθ(τ)G(τ).

τ

(6.26)

(6.27)

In order to take the gradient, let us ﬁrst recall the property of logarithmic derivatives

∇θ pθ = pθ∇θ log pθ, wich we apply in the following derivation:

∇θ E[G|πθ] = (cid:88)
τ
= (cid:88)
τ

G(τ)∇θ pθ(τ)

G(τ)pθ(τ)∇θ log pθ(τ).

(6.28)

T −1
(cid:88)

Then, from Eq. (6.26), we see that the only dependence on θ from pθ(τ) is in the policy.
Therefore,

∇θ log pθ(τ) =

∇θ log πθ(at

|st

),

(6.29)

which, combined with Eq. (6.28), we obtain the expression

t=0

∇θ E[G|πθ] = (cid:88)

pθ(τ)G(τ)

T −1
(cid:88)

t=0

∇θ log πθ(at

|st

)

∇θ log πθ(at

|st

(cid:153)

(cid:12)
(cid:12)
πθ
)
(cid:12)
(cid:12)

τ
(cid:150)

= E

G(τ)

T −1
(cid:88)

t=0

(6.30)

The importance of the policy gradient theorem lies in the fact that it yields a closed form for
the gradient as an expectation value. As a consequence, we can estimate it via Monte-Carlo
sampling over different trajectories τ. Furthermore, the gradient of the objective function is
independent of the initial state s0, as it does not depend on the policy.

6.3.1 REINFORCE

The REINFORCE algorithm [300] is one of the most commonly used policy gradient algorithms
and it uses the return as objective Oπ = G(τ).12

12In section 6.1.5 we mention that the optimal policy maximizes Vπ(s) ∀s ∈ S. Taking Vπ(s) as objective, the
] (see [287]). In REINFORCE, Gt acts as an unbiased
|st , at , πθ ] from Eq. (6.11).

) to ﬁnd the optimal policy, since Qπ(at , st

gradient is ∇
θ
estimator of Qπ(at , st

] = E[Qπ(s, a)∇

E[Vπ(s)|π
θ

) = E[Gt

(a|s)|π
θ

θ log π
θ

143

Reinforcement learning

The main principle of REINFORCE is to directly modify the policy to favor series of
actions within the agent’s experience that lead to a high return. This way, previously
beneﬁcial actions are more likely to happen the next time the agent interacts with the
environment.

Formally, we solve the optimization problem θ∗ = arg maxθ

E[G|πθ ]. We ﬁnd θ∗ via
an iterative update rule in which we estimate the gradient ∇θ E[G|πθ] and perform a gradient
ascent step in its direction.
In practice, we estimate it by sampling a batch of trajectories
τ ∼ pθ(τ), also known as episodes, and then we evaluate Eq. (6.30). This way, at learning
iteration i,

∆θi

= 1
Ω

θi+1

= θi

(cid:88)

G(τ)

τ
+ η∆θi,

T −1
(cid:88)

t=0

∇θ log πθ(at

|st

)

(6.31)

(6.32)

where η is the learning rate and Ω = (cid:80)

τ G(τ).13 We illustrate the procedure in Algorithm 10.

Algorithm 10 REINFORCE
Require: learning rate η, number of trajectories n, maximum time T
Require: randomly initialized differentiable policy πθ(a|s)

while not converged do
for i = 1 to n do
Initialize s0
for t = 0 to T − 1 do
|st
Take action at
Move to next state st+1 and store reward rt+1

∼ πθ(at

) and store ∇θ log πθ(at

end for
G(i) ← (cid:80)
z(i) ← (cid:80)

t

γt rt+1
t
∇θ log πθ(at

)

|st

i G(i)

end for
Ω ← (cid:80)
∆θ ← (1/Ω) (cid:80)
θ ← θ + η∆θ

i G(i)z(i)

end while
return θ

)

|st

(cid:46) Optimal policy parameters

However, the trajectory sampling introduces signiﬁcant ﬂuctuations to the expected quan-
tities that result in large training variances, which is a general problem with any Monte-Carlo-
based approach. Some episodes may be quite successful whereas some others could be a com-
plete failure with very low returns. Such high variance results into unstable policy updates,
which increase the convergence time toward the optimal policy. A common technique to tackle
this issue is to introduce a baseline into the returns, which reduces the variance of the method
without incurring any bias, and therefore should always be used.

In order to provide a better description of the baseline, let us ﬁrst rewrite Eq. (6.30) in

13The expectation of the gradient over trajectories is a weighted sum with respect to their returns. Unlike
normalizing by the number of trajectories, this approach disregards trajectories with zero return, which do not
contribute to the gradient and would dilute the information, yielding very small updates ∆θi

→ 0.

144

Reinforcement learning

a more convenient way, and omitting the condition E[·|πθ] for the rest of the chapter:

∇θ E[G|πθ] = E

(cid:150)(cid:130) T −1
(cid:88)

t (cid:48)=0

γt (cid:48)

rt (cid:48)+1

(cid:140) T −1
(cid:88)

t

∇θ log πθ(at

|st

(cid:153)
)

= E





T −1
(cid:88)

t (cid:48)=0

γt (cid:48)

rt (cid:48)+1

t (cid:48)
(cid:88)

t=0



∇θ log πθ(at

|st

)


= E

= E

(cid:150)T −1
(cid:88)

t=0
(cid:150)T −1
(cid:88)

t=0

∇θ log πθ(at

|st

)

(cid:153)

γt (cid:48)

rt (cid:48)+1

T −1
(cid:88)

t (cid:48)=t

γt Gt

∇θ log πθ(at

|st

(cid:153)
)

,

(6.33)

where in the ﬁrst equation we write the explicit form of G(τ). In the second equation we use
the relation

∇θ E[G|πθ] = ∇θ E

(cid:150) T −1
(cid:88)

γt (cid:48)

rt (cid:48)+1

(cid:153)

=

T −1
(cid:88)

t (cid:48)=0

∇θ Eτ

t(cid:48)

(cid:148)γt (cid:48)

(cid:151)

rt (cid:48)+1



∇θ log πθ(at

|st

)


rt (cid:48)+1

t (cid:48)
(cid:88)

t=0

(6.34)

t (cid:48)=0

γt (cid:48)

t(cid:48)

Eτ

=

T −1
(cid:88)

t (cid:48)=0


= E



T −1
(cid:88)

t (cid:48)=0

γt (cid:48)

rt (cid:48)+1

t (cid:48)
(cid:88)

t=0



∇θ log πθ(at

|st

)
 ,

t(cid:48) denotes expectation over trajectories up to time t (cid:48). Then, in the third line
where Eτ
of Eq. (6.33), we rearrange the terms in the summations and we ﬁnd the explicit form of
Gt offset by a γt factor. In the ﬁnal expression, it becomes clearer how past rewards in the
trajectories do not contribute to the gradient of the policy from a given time onwards, which
recovers the Markov property.

We can reduce the variance in the gradient by introducing a state-dependent baseline b(st

)

in Eq. (6.33) such that

∇θ E[G|πθ] = E

γt (Gt

− b(st

)) ∇θ log πθ(at

|st

(cid:153)
)

.

(cid:150)T −1
(cid:88)

t=0

(6.35)

Any baseline is appropriate as long as it does not depend on the actions. This way, we do not
introduce any bias, given that
|st

[∇θ log πθ(at

)] = Eτ

(cid:2)b(st

) Eτ

)](cid:3)

|st

t

t:T

E [b(st
)∇θ log πθ(at
(cid:20)
)(cid:88)
at

= Eτ

b(st

t

πθ(at

|st

)∇θ log πθ(at

|st

)(cid:88)
st+1
(cid:124)

p(st+1

|st , at

)

(cid:88)

pθ(τ

t+1:T

(cid:123)(cid:122)
1

τ
t+1:T
(cid:124)

(cid:125)

(cid:123)(cid:122)
1

(cid:21)

)

(cid:125)

(6.36)

(cid:20)

= Eτ

t

b(st

)∇θ

(cid:88)

at
(cid:124)

(cid:21)

πθ(at

|st

)

(cid:123)(cid:122)
1

(cid:125)

= Eτ

t

[b(st

) · 0] = 0 ,

where τ
t:T indicates a trajectory from time t until the end T . We move from the second to
the third line using the property of logarithmic derivatives, as in Eq. (6.28). Notice that the
expectation remains unbiased even if the baseline depends on θ.

145

Reinforcement learning

While the expectation is unaffected, the baseline can have a major impact in the vari-
ance.14 Let us consider the case of a state-independent baseline. We can ﬁnd the optimal
baseline that minimizes the variance in the gradient for each parameter. In order to simplify
log πθ(a|s)
the notation, let zk and bk be the k-th components of the score function zk
and a state-independent baseline vector, respectively. Hence, the goal is to minimize the vari-
ance of the term (Gt
)zk,15 which is the argument of Eq. (6.35). Formally, we aim to ﬁnd
b∗
= arg minbk
k

], that is such that ∂

] = 0. Therefore,

Var [(Gt

Var [(Gt

= ∂θk

− bk

− bk

− bk

)zk

)zk

b∗
k

Var [(Gt
Var [(Gt

− bk
− bk

)zk
)zk

∂

bk

b

− bk

)zk
− bk

)2] − E[Gt zk
)z2
k

]

] = E[((Gt
] = −2 E[(Gt
E[Gt z2
]
k
E[z2
]
k

=

∗
k

,

]2

(6.37)

(6.38)

(6.39)

where in the ﬁrst equation we have used Eq. (6.36) to remove bk in the second term.

|st

) ≈ E[Gt

There are several other valid baselines that we can consider, besides the state-independent
example above, with which we may obtain better results. For instance, an estimation of
] is a common state-dependent baseline. This can ei-
the value function ˆVπ(st
ther be learned, either directly from Gt or as we show in section 6.4, or it can be estimated
through sampling in self-critic schemes (see [301]). With such baseline, actions that lead
to returns higher than expected with the current policy are reinforced, while those that lead
to lower rewards are penalized. This is equivalent to weighting the score function by the
advantage. Given that E[Gt
], from Eq. (6.11), subtracting a base-
), we obtain the expectation of the advantage (recall Eq. (6.12)). Hence,
line b(st
) = Vπ(st
)(cid:3). Directly estimating the advantage provides
∇θ E[G|πθ ] = E (cid:2)(cid:80)
)∇θ log πθ(at
the least possible variance, see [302] for further reference on this matter.

] = E[Qπ(st , at

γt A(st , at

)|st , at

|st , at

|st

t

Another common practice is to whiten the return. This consists of subtracting the mean
of the return along all the time steps of a trajectory and dividing by its standard deviation
¯Gt

G. Since this is not exactly a baseline, this method does introduce a bias.

− 〈G〉)/σ

= (Gt

6.3.2 Implementing REINFORCE with a neural network

The parametrized policy πθ is a central quantity in policy gradient methods and it can take any
form as long as it is differentiable with respect to its parameters. One of the most common
approaches in discrete action spaces is to deﬁne action probabilities according to a softmax
distribution:

e x(s,a)
a(cid:48)∈A e x(s,a(cid:48)) ,
where x(s, a) is the action preference for action a in state s.

πθ(a|s) =

(cid:80)

(6.40)

The simplest way to deﬁne action preferences is through a set of linear parameters θ ap-
plied to a feature representation of the state and action φ(s, a), such that x(s, a) = θT φ(s, a).
However, this approach may lack the expressive power to approximate the optimal policy π∗
in complex problems.

In these cases, we may need to use a deep NN to parametrize the action preferences. NNs
are a natural generalization of the linear parameter approach that we can tune to increase the
expressive power by, e.g., increasing the number of hidden layers or their size. This way, the

14Recall that Var[x] = E[x 2] − E[x]2. Hence, adding a term with null expectation does not affect the second

term but it does have an impact on the ﬁrst one Var[x − b] = E[(x − b)2] − E[x − b]2 = E[(x − b)2] − E[x]2.

15In this case, we take the approximation Var (cid:2)(cid:80)

(cid:3) ≈ (cid:80)

t X t

t Var [X t

]

146

Reinforcement learning

NN parametrizing the policy takes a state representation in the input layer φ(s), and has as
many neurons as possible actions in the output layer, which encode x(s, a) ∀a ∈ A. Applying
a softmax activation function in the output layer (see Eq. (2.37)), we obtain πθ (a|s) ∀a ∈ A,
as in Eq. (6.40).

The training process is analogous to training a supervised classiﬁer on the experience gath-
ered by the agent. Implementing REINFORCE with gradients from Eq. (6.35) is equivalent to
performing gradient descent with a modiﬁed categorical cross-entropy loss (recall Eq. (2.3)):

L = − 1
n

n
(cid:88)

T −1
(cid:88)

i=1

t=0

γt (Gt i

− b(st i

)) log πθ(at i

|st i

) ,

(6.41)

where i denotes the index in a batch of n trajectories. This way, the procedure is analogous
to training an NN classiﬁer in which the actions act as state labels. The main difference with
supervised classiﬁcation problems is that, given a state, we do not know the true probability
distribution of the actions (true labels), as that would be the optimal policy. Instead, we assign
the obtained return Gt as true label for the taken action at .16 Intuitively, in classiﬁcation
problems we aim to enhance the probability that the NN provides the right label, whereas
here we reinforce the actions with high returns.

In many situations, actions can take a range of continuous values rather than a discrete set
of categories. For instance, a robotic arm may rotate by a certain angle or we can tune various
continuous parameters in an experimental setup. Sometimes, we can discretize the action
space into small intervals at the cost of a loss in precision and an increasing amount of actions.
Nevertheless, this may not always be possible depending on the problem requirements and
the resulting number of actions.

In these cases, we model the stochastic continuous actions with a mean µ and a standard

deviation σ, such that

a = µ + σξ ,

(6.42)

where ξ is a random normal variable with unit variance. Analogously to the action preferences
above, we can parametrize µθ(s), σθ(s) in various ways, ranging from a set of linear parame-
ters, e.g., µθ(s) = θT φ(s), to an NN with two output neurons that determine both µθ(s) and
σθ(s) for the given observation. Formally,

πθ(a|s) =

1
(cid:112)
σθ(s)

2π

(cid:18)

−

exp

(cid:129) a − µθ(s)
2σθ(s)

(cid:139)2(cid:19)

.

(6.43)

In many cases, as the learning advances, and the agent becomes better at taking the right
actions (choosing µθ(s)), the deviations decrease and we obtain a quasi-deterministic policy.

6.4 Actor-critic methods

In section 6.2, we introduce value-based RL, featuring the Q-learning algorithm in sec-
tion 6.2.1. These methods excel at dealing with discrete state−action spaces, and their TD
character makes them data efﬁcient and allows them to tackle continuing tasks (inﬁnite

16The standard categorical cross entropy would be L = − 1
n

) is the true
probability distribution that we want to learn. In standard classiﬁcation problems, this is typically 1 for the true
label and 0 for the rest. Here, it corresponds to the optimal policy p(ak
|s). Since we do not have
access to π∗ (it is our goal!), we use the return Gt for the chosen action in its place, as π∗ would favor actions
with high returns. This effectively removes the expectation over actions, and we make the sum over time explicit
in Eq. (6.41).

|s), where p(ak

) = π∗(ak

) log π
θ

k p(ak

(ak

n

(cid:80)

(cid:80)

147

Reinforcement learning

episodes). However, they experience difﬁculties to deal with large state−action spaces, and
can’t deal with their continuous version. Furthermore, they are bound to implement deter-
ministic greedy policies, while many problems present stochastic optimal policies. Finally,
small changes in the value functions can cause large variations in the policy, which may cause
instabilities in learning.

On the other hand, we introduce policy-gradient methods in section 6.3, featuring the
REINFORCE algorithm in section 6.3.1. These algorithms overcome the aforementioned
limitations of value-based methods, provided that they can deal with continuous (inﬁnite)
state−action spaces, and they are based on continuous stochastic policies, which ensure
smooth changes in the policy throughout the learning process, and can become determin-
istic when needed. However, the learning happens at the end of the episodes, once we know
the return, which is an issue for long trajectories or continuing tasks.

Actor-critic algorithms combine value-based and policy-based methods in order to ob-
tain the best of both approaches. We can understand actor-critic methods as the TD
version of policy gradient, with which we retain all its advantages and overcome its
major limitation. It features two main elements: the actor, a parametrized policy that
dictates the decisions, and the critic, a model that evaluates them.

The presence of the critic allows the agent to immediately learn from each action without
waiting for the outcome at the end of the episode. Evaluating the policy mainly consists on
learning its value functions, which allows the critic to assess whether the actions are more or
less favorable. In section 6.3.1, we introduce the state-value function, Vπ(s), as the optimal
baseline to reduce the variance in policy gradient. Although, in this case, we only look at Vπ(s)
of the initial state in in the transitions, which does not allow us to evaluate the actions.17

However, we sow that, with such baseline, we can compute the gradient in terms of the
advantage A(s, a), introduced in Eq. (6.12). The explicit form of the advantage sets the foun-
dation for actor-critic methods [303–305]:

A(st , at

) = E[rt+1

+ γVπ(st+1

) − Vπ(st

)] ,

(6.44)

which is derived from Eqs. (6.12) and (6.14). This expression lies at the core of TD algorithms,
as it corresponds to the TD error from Eq. (6.20).

In Eq. (6.44), we use Vπ(s) to evaluate both the initial and ﬁnal states of a given transition,
thus constituting a critic of the action. This allows the agent to learn from every time step
in REINFORCE, processing states, actions and rewards as they occur, like the TD algorithms
from section 6.2. Nevertheless, this advantage comes as the cost of learning two models: the
policy πθ(a|s), and the state-value function Vπ(s; w), which are usually parametrized with NNs
with parameters θ and w, respectively. The NN parametrizing the state-value function takes
a feature representation of the state, φ(s), in the input layer, and has a single output neuron
encoding Vπ(s; w). The policy parametrization is the same as in section 6.3.2. We train both
models simultaneously by following Algorithm 11.

We train the actor with the methods from section 6.3, and the critic using the princi-
ples from section 6.2. Hence, all the methods in both sections apply to this algorithm. The

17In order to determine the quality of an action, we need to compare the initial and ﬁnal positions. In a game,
an action that escapes from the brink of a loss toward a less disadvantageous position may be more valuable than
one that moves from an already favorable position to a slightly better one, despite the latter providing a higher
ﬁnal state-value function.

148

Reinforcement learning

Algorithm 11 Actor-critic
Require: learning rates ηθ, ηw, maximum time T
Require: randomly initialized differentiable policy πθ(s|a)
Require: randomly initialized differentiable state-value function Vπ(s; w)

while not converged do

Initialize s0
for t = 0 to T − 1 do

Take action a ∼ πθ(a|s)
Move to next state s(cid:48) and obtain reward r
A ← r + γVπ(s(cid:48); w) − Vπ(s; w)
θ ← θ + ηθγt A∇θ log πθ(a|s)
w ← w + ηwA∇wV (s; w)

end for
end while
return θ, w

(cid:46) Update actor
(cid:46) Update critic

(cid:46) Optimal actor and critic parameters

parameter updates in Algorithm 11 come from performing gradient ascent with Eq. (6.35)
on the actor, and an analogous update rule to Eq. (6.22) for the critic, using Vπ(s) in-
stead of Qπ(s, a). The process is equivalent to perform gradient descent on the losses
Lθ = 1
), and Lw = 1
n A(s, a; w)2, respectively, in which
n
n
we omit the index for the sum over n samples. They are based on the same principles as the
ones in Eqs. (6.25) and (6.41).

γt A(st , at ; w) log πθ(at

|st

(cid:80)

(cid:80)

(cid:80)

n

t

This method is often referred to as advantage actor-critic (A2C). It has been further en-
hanced using asynchronous actors, giving raise to the asynchronous advantage actor-critic
(A3C) algorithm [306]. Other improvements rely on implementing more advanced optimiza-
tion techniques, such as the natural gradient [307], as in natural policy gradient [308], natural
actor-critic [309, 310], and trust-region methods [311–313].

6.5 Projective simulation

In recent years, there have been introduced novel approaches to RL that explore techniques
beyond the prototypical value-based and policy gradient methods that we introduce in sec-
tions 6.2 and 6.3. Among those, projective simulation (PS) [314] is of particular interest for
the physics community, due to its numerous applications in the ﬁeld.

PS considers an agent based on an episodic and compositional memory (ECM), a mathe-
matical object capable of storing the information about visited states and actions, and drawing
connections between them. This way, the ECM is continuously updated as the agent gathers
experience, and it ultimately determines the policy at any given state, as we show below. Usu-
ally, the ECM is represented as a directed weighted graph, as shown in Fig. 6.3(a). The nodes,
deﬁned here as clips, represent either visited states, actions, or hidden information learned by
the agent. As the agent explores, clips corresponding to new visited states are added to the
graph. Similarly, an agent may create additional ones to accommodate new actions, e.g., the
combination of two actions, or hidden information. The edges are weighted, and every new
node is initialized with uniform edge weights. The weights determine the transition probability
between clips, and they are updated as the agent gathers rewards.

As we have previously mentioned, the ECM deﬁnes the policy of the PS agent.

In the
vanilla version of PS, given an observed state, the agent performs a weighted random walk

149

Reinforcement learning

Figure 6.3: Schematic representation of the episodic and compositional memory
(ECM) of various projective simulation (PS) agents. (a) A multi-layer ECM with
m state nodes (pink), three hidden nodes (red) and m action nodes (yellow). The
connectivity of the graph can also be set as a (b) A three-layer ECM, used to demon-
strate the possibility of feature extraction by the PS model. See the main text for
details.

through the ECM starting on the corresponding state clip. The walk ends as soon as it lands
in an action node, and the corresponding action is chosen. The probability to jump from one
clip, ci, to another, c j, can be any normalized function of the edge weights h(ci, c j

), such as

P(ci, c j

) =

(cid:80)

)

h(ci, c j
j∈I h(ci, c j

,

)

(6.45)

where I is the set of edges of ci. Other transition functions have also been introduced, such
as softmax transitions, which allow us to have arbitrary h-values.

Following the previous scheme, training a PS agent consists on updating the ECM by adding
new nodes, and learning the edge weights. The goal is that, for every state clip, the path
through the ECM leads to the correct action with high probability. Thus, the training can then
be reduced to the update of the h-values at every time-step via

h(ci, c j

) ← h(ci, c j

) + γ(h(ci, c j

) − 1) + r

(6.46)

where ci and c j represent the clips traversed during the random walk through the ECM, γ is
a damping parameter, and r is the reward given by the environment after performing the
chosen action.

With this update rule, for every agent’s decision, i.e., every time it performs a walk from
a state node to an action node, all h-values of the visited edges are updated. In this way, the
h-values along the walk are always damped by a factor γ, and, in the case that they led to
a rewarded action, they also increase their value by a factor r.

In many practical scenarios, rewards are obtained at the end of a long series of actions,
e.g., performing various steps in a grid-world to reach a target. Hence, it is important to
“backpropagate” such reward through the sequence of all the actions that led to it. For instance,
in TD algorithms, this is achieved by considering the expected value of future states to perform
the updates, as we introduce in section 6.2. To accommodate such property, we can generalize
the update rule from Eq. (6.46) by introducing the concept of an edge glow: every time an edge
is traversed, it starts to glow decaying with time. This feature allows the agent to update all

150

S1(a)S2Sma1a2am......S1S2Sm...(b)m=1v=0m=0v=2exp. 1 (mass)exp. 2 (mass)exp. 3 (volume)...a1a2a3a1a2a3a1a2a3Reinforcement learning

the edges in the ECM involved in the decisions to describe a trajectory τ = a0, s1, a1, . . . 18
which led to a certain reward. The update rule can then be rewritten as

h(ci, c j

) ← h(ci, c j

) − γ(h(ci, c j

) − 1) + g(ci, c j

)r ,

(6.47)

where g is the glow value.

Each time a certain edge is visited, its corresponding glow value is set to 1. Then, at every

step, all the glow values are dampened via

g(ci, c j

) ← g(ci, c j

)(1 − η) ,

(6.48)

effectively decreases all of them with a rate η. This means that edges that have been recently
visited and led to a reward r (cid:54)= 0 are strengthened, while those visited earlier on received
a lesser update, analogous to TD algorithms. We refer to [315, 316] for an in-depth and
practical description of the usage of the PS models.

The presented approach to PS is a tabular method, similarly to Q-learning from sec-
tion 6.2.1, as the agent’s deliberation is saved in the adjacency matrix of the ECM, namely
the h-matrix. As commented previously, tabular methods have strong limitations when deal-
ing with large action and state spaces. Recently, non-tabular approaches for PS have been
proposed [317]. In that case, a neural network (and more precisely, an energy-based model)
is trained to output the h-value for a certain state-action pair, analogously to how DQNs are
used to predict Q-values, as we introduce in section 6.2.3.

An important feature of the PS model is its transparency and potential interpretability
power, in contrast to other approaches such as Q-learning. In the latter, the Q-values encode
the expected reward received from an action-state tuple. As the policy relies on performing
the action with largest Q-value, there is little to no room for interpretability, aside from such
maximization. Conversely, PS constructs a visible graph encoding the probabilities to hop
between nodes, which may represent both direct information from the RL task, i.e., actions
and states, but also hidden information extracted by the agent. For instance, as we describe in
section 6.6.6, the authors of Ref. [318] were able to interpret the hidden structure of the ECM,
related in that example to different optical devices. Interestingly, the PS agent was able to
create useful optical gadgets composed of multiple devices by composing actions together into
new joint nodes (see [314]). Nonetheless, when working in the so-called two-layer PS (one
layer of nodes for the states and one for the actions), PS reduces to a very similar model to Q-
learning. Indeed, recent works have extensively compared both approaches [319]. However,
we can introduce further hidden nodes to build deeper PS models, as shown in Fig. 6.3(a).

Recently, there have been multiple efforts to build such deep PS architectures and to show
that they are indeed able to extract relevant hidden features from the environment or the task
at hand [320, 321]. An enlightening example is shown in Ref. [321], which we schematically
reproduce in Fig. 6.3(b). In this work, an agent is given a set of objects with different physical
properties, such as mass, charge and volume. For simplicity, these quantities can take only one
of three values: 0, 1 or 2. The agent has access to different experiments, which measure each
of this quantities separately. The states are then different objects with certain properties, e.g.,
in Fig. 6.3(b), S2 is an object of mass 0 and volume 2, obviously in arbitrary units or categories.
On the other hand, the actions are the predictions over the various experiments. For instance,

18Be careful to not confuse the trajectories through the ECM with the trajectories through the state and action
spaces. Given a state st , the PS agent chooses the action at by performing a trajectory through the ECM that starts
on the corresponding st node until it reaches an action node. Then, the corresponding action at is performed to
move toward the next state st+1.

151

Reinforcement learning

a1 corresponds to the prediction that the object has the lowest value measured by experiment
one (related in this case to mass), a2 to an intermediate value of that same experiment, etc.
The authors show that the PS agent would assign the hidden nodes to meaningful features of
the problem. In particular, each hidden node would represent a particular value of a physical
quantity, as shown in Fig. 6.3(b). Such interesting feature is not only a valuable sign of the
interpretability of the PS model, but also was shown to increase its generalization performance.

6.6 Examples and applications

In this section, we showcase a series of prominent applications of RL. Between all the exam-
ples, we ﬁnd instances of each RL paradigm that we discuss in the previous sections. We start
with two toy examples to settle the theoretical foundations of policy gradient, as they have
analytical solutions. Then, we brieﬂy comment on some of the most famous examples of RL:
Atari video games, and Go. Finally, we highlight a few applications of RL to quantum physics,
more precisely, in the context of future quantum technologies such as quantum circuits, error
correction, and certiﬁcation.

6.6.1 Toy examples

Let us illustrate the REINFORCE algorithm, from section 6.3.1, by solving a couple of toy
examples. These simple scenarios allow us to solve all the equations analytically in order to
lay down the foundations and become familiar with the basic concepts.

= +1 and every time it goes down it receives a negative reward rt

The random walker Consider an agent that can move along a one-dimensional path with
only two actions: move up or down. Every time the agent goes up, it receives a positive reward
= −1. Considering the
rt
undiscounted case, γ = 1, the return of a trajectory of T steps is the ﬁnal position G(τ) = x T .
We can also express it in terms of the number of times the agent has taken the actions to go
up or down G(τ) = nup
− T . Clearly, the optimal policy is to always go uphill
regardless of the current position.

− ndown

= 2nup

In such a simple scenario, there is no notion of a state for the agent. Therefore, the
policy only depends on the action. Furthermore, since there are only two possible actions,
we can deﬁne the parametrized policy for one, e.g., πθ(up) ∈ [0, 1], and take the other as
πθ(down) = 1 − πθ(up). Let us consider the parametrized sigmoid policy

πθ (up) =

1

1 + e−θ , πθ (down) = 1

1 + eθ ,

(6.49)

which determine the probability to move upwards or downwards, respectively, in terms of the
single parameter θ . Their score functions are

∇θ log πθ (up) = πθ (down), ∇θ log πθ (down) = −πθ (up) .

(6.50)

With Eqs. (6.49) and (6.50), we can compute the parameter update rule from Eq. (6.31)

analytically. We can express each of its terms as a function of πθ (up):

(cid:150)

E

G(τ)

T −1
(cid:88)

t=0

∇θ log πθ (at

(cid:153)
)

= E (cid:2)(cid:0)nup

− ndown

(cid:1) (cid:0)nup

πθ (down) − ndown

πθ (up)(cid:1)(cid:3)

= E (cid:2)(cid:0)2nup
= 2 E (cid:148)(cid:0)nup
= 2Var (cid:2)nup

− T πθ (up)(cid:1)(cid:3)
− T (cid:1) (cid:0)nup
− T /2(cid:1) (cid:128)
− (cid:10)nup
(cid:3) = 2T πθ (up) (1 − πθ (up)) ,

nup

(cid:138)(cid:151)

πθ

(cid:11)

152

(6.51)

Reinforcement learning

Figure 6.4: Walkers and RL. (a) Parameter update from Eq. (6.52) for the random
walker. (b) Evolution of various policies trained on the walker with target.

where we have taken T πθ (up) as the expected number of upwards moves (cid:10)nup
. With this,
we are able to reach a closed analytical form for the parameter update rule in this simplistic
scenario, which is not the usual case in RL. This allows us to understand the way that actions
− (cid:10)nup
are reinforced. For instance, the term nup
reinforces actions that lead toward higher
upwards moves than expected following the policy, and it penalizes those that lead to fewer.

πθ

πθ

(cid:11)

(cid:11)

The parameter update is a quadratic function on the policy, such that

∆θ ∝ πθ (up)(1 − πθ (up)) ,

(6.52)

which is minimal either close to the optimal policy πθ (up) (cid:39) 1 or far from it πθ (up) (cid:39) 0, as
shown in Fig. 6.4(a). We can understand this in a very intuitive way: if the agent is already
prioritizing the action to move upwards, it has very little to learn from there on. Conversely,
if it barely takes this action, it cannot learn that it is the right choice. Hence, the agent learns
the most whenever it takes both of actions at a similar rate. This also reﬂects the importance
of the initialization. If we initialize the policy to πθ
(up) (cid:39) 0, the agent takes much longer to
converge to the optimal policy than with πθ

(up) (cid:39) 0.5.

0

0

The walker with target Consider now a slightly more complex situation in which the agent
moves along a one-dimensional path and has to stop at a target location. In this case, the two
actions are to move forward, or to stay. The agent receives a reward every time step it stays
at the target location. In this example, the optimal policy is to move forward until the agent
reaches the target, and then stop.

In contrast to the previous example, the agent is no longer blind, and the policy does
depend on the state. Notice that, even though the agent moves in space, the actual position is
completely irrelevant to the problem, and the only important information is whether the agent
is in the right position or not. Therefore, we encode this information with Boolean indicators,
assigning s = 1 when the agent is at the target location, and s = 0 elsewhere. Hence, despite
the agent moving in real space, it only navigates in a two-state MDP.19 Then, we denote the
actions “stay” and “move” with a = 0 and a = 1, respectively. This way, the optimal policy
always takes the action to move when not in target, and to stay when in target. We illustrate
the optimal policy in Table 2. Additionally, we illustrate the convergence of various policies to
the optimal one with REINFORCE in Fig. 6.4(b).

19We emphasize that, when we frame a problem as an RL instance, we only need to model and encode the
information that is relevant to the problem. Hence, the resulting state and action spaces do not need to correspond
directly to those in the “real world”. The simpler the MDP, the easier it is be for the agent.

153

Reinforcement learning

stay
a = 0

move
a = 1

0

1

1

0

out of target
s = 0
on the target
s = 1

Table 2: Optimal policy π∗(a|s) for the walker with target example.

6.6.2 Go and Atari games

Games are one of the most natural applications for RL, and they serve as a benchmark for
the state of the art methods. Most games involve long-term strategies, and early actions may
lead to completely different outcomes, even in short time scales. Furthermore, many games
involve vast state spaces, or even inﬁnite ones. Overall, they pose a great challenge that has
motivated some of the greatest advances in the ﬁeld.

The ﬁrst applications of RL to games were board games. Achieving superhuman perfor-
mance in chess was a milestone in the ﬁeld when, in 1997, Deep Blue [322] beat Garry Kas-
parov, the highest-rated chess player in the world at the time. A more recent breakthrough
has been achieving superhuman performance in the game of Go [20]. Go is a Chinese board
game which is over 3,000 years old. Two players take turns to place stones on the board.
The goal is to conquer as much space as possible, either by strategically surrounding empty
spaces, or capturing the opponent’s stones by surrounding them. Once all stones are allocated,
the player with the largest captured territory wins. Even with this simple set of rules, there
are 10172 possible board conﬁgurations, making this game order of magnitudes more complex
than chess [323].

The computer program developed by DeepMind, AlphaGo [20], combines a technique
called Monte Carlo tree search [324] with deep NNs. With this approach, the goal is to pro-
gressively build a search tree of the state space that grows as the agent gathers experience. In
the tree, each edge contains the learned action-value function Q(s, a), which partially deter-
mines the policy, similar to Q-learning from section 6.2.1. However, since the state space is
virtually inﬁnite, they implement two NNs that guide the search through the regions outside of
the tree: a parametrized policy that guides the exploration, and a parametrized value function
that predicts the probability to win from each state. See [20] for a detailed explanation.

Initially, they train the policy network by supervised learning, taking example moves from
expert games. This provides them with an early advantage with respect to starting tabula rasa
to build the search tree from already functional strategies. However, they then proceed to train
the whole pipeline through self-play, i.e., playing against itself, further reﬁning the policy via
policy gradient, as shown in section 6.3. This model defeated the world champion of Go in
2015.

This approach has been improved by removing the initial supervised training over expert
human games, and purely training through self-play from scratch. This algorithm is known
as AlphaGo Zero [325]. This new version defeated the previous one by a hundred games to
zero. In Fig. 6.5, we see the performance of AlphaGo and AlphaGo Zero with training time
in terms of Elo rating.20 Initially, AlphaGo has a substantial advantage thanks to the previous

20The Elo rating system, named after its creator Arpad Elo, is a method to calculate the relative skill level of
players in zero-sum games. After every game, the winning player takes points from the losing one. The difference
in rating between players determines the total number of points gained or lost after a game. If the higher-rated

154

Reinforcement learning

Figure 6.5: Performance comparison between AlphaGo (initial supervised learn-
ing) [20], and AlphaGo Zero (pure RL) [325] at the game of Go. Initially, AlphaGo
has an advantage thanks to the initial supervised training. However, it limits its ca-
pabilities and it is quickly outperformed by AlphaGo Zero. The horizontal dashed
line corresponds to the Elo rating of Lee Sedol during his match with AlphaGo being
a reference point for the supervised/pure RL performance. Taken from [325].

supervised learning phase. However, this pre-training ultimately limits its capabilities, and Al-
phaGo Zero outperforms it in just a few hours of training. Furthermore, while these algorithms
are generally tailored to the speciﬁc game, more general and recent approaches, defeated the
previous benchmarks in chess, shogi, and Go at the same time [326].

Another exciting avenue for RL in games are video games. One of the ﬁrst applications were
Atari games, achieving superhuman performance with deep Q-learning [18], as we explain
in section 6.2.3. In this case, the state space is also inﬁnite and the agent receives the screen
pixels as input, together with the current score. However, the action space is limited by the
game controller, which is very convenient for Q-learning. This approach achieved superhuman
performance in forty nine different games with the same algorithm.

Some other recent outstanding results in video games include competitive performance in
StarCraft II [19], Dota 2 [327], and Minecraft [328]. Furthermore, advances in model-free RL
have motivated the research on planning with model-based algorithms, with which some of
the benchmarks that we introduce above have been bested [329]. This approach does not even
require the explicit encoding of the game rules, as it builds a model of them while playing.

6.6.3 Quantum feedback control

Quantum control is a research direction in quantum technologies aiming to improve initializa-
tion and stabilization of a desired quantum state. Already, deep RL algorithms have been suc-
cessfully employed in a wide range of applications for quantum feedback control [331–334].
In general, a quantum system is controlled by an NN based on a feedback loop with some
measurements that have been performed on the system. These measurement results are fed
as an input to the NN whose output guides the control scheme. In Ref. [332], authors consid-
ered a single-mode quantum cavity. The cavity mode is leaking from the cavity and this signal

player wins, only a few rating points are taken from the lower-rated player. However, in the opposite case, the
lower-rated player takes many points from the higher-rated one.

155

Reinforcement learning

Figure 6.6: Driven single mode microcavity as an RL environment. The mode de-
cays from the cavity. Its measurement serves as the observation for the agent repre-
sented by an NN. The network converts a measurement trace into probabilities for
all the available actions, which give feedback for the displacement drive of the cavity.
Adapted from [330].

can be measured. Additionally, the cavity mode can be controlled via an external drive. The
goal is to adjust the drive amplitude of a beam entering the cavity to create and then stabilize
a cavity quantum state with a single photon as depicted in Fig. 6.6.

The input to the network, i.e., the observed state s, is the measured electric ﬁeld leaked
from the cavity. The only action in this scenario is to set the value of the driving laser amplitude
as a function of time. After a short evolution time step, the NN is queried again. Given the
updated measurement of the electric ﬁeld, the network decides on the next action probabilities.
One of these actions is selected and the next system evolution in the next step is executed. This
procedure is performed until a ﬁxed end of the trajectory, before the network’s parameters are
updated according to the policy gradient approach from section 6.3. Many different training
episodes start with the cavity vacuum state. During training, the agent (in form of an NN)
eventually ﬁnds a strategy to compete losses with the proper drive ending up in the stabilized
target cavity state.

6.6.4 Quantum circuit optimization

Quantum computing based on quantum gates requires designing a quantum circuit for a spe-
ciﬁc quantum algorithm. However, there can be many different sequences of quantum gates
implementing the same algorithm. Additionally, due to the fact that quantum gates have non-
perfect ﬁdelity, the more gate operations are performed, the more errors appear during the al-
gorithm execution. As such, quantum circuits should be designed in the most optimal way, im-

156

cavityelectric ﬁeld measurementactionset externaldrivestateENVIRONMENTAGENTdrive displacementReinforcement learning

Figure 6.7: Schematic representation of the RL framework for circuit optimization.
The agent observes a representation of a quantum circuit given by the environment.
Then, it can choose to perform a modiﬁcation to the circuit. The environment cal-
culates a reward depending on the gate count (or another metric) of the resulting
circuit, and it provides the agent with the new circuit and the reward. Adapted
from [336].

plementing the least possible number of quantum gates. This is especially important for noisy
intermediate-scale quantum (NISQ) devices, which currently allow for > 100 qubits [335]
but, at the same time do not allow for high-level logical quantum error correction.21 Quan-
tum circuit optimization utilizes the fact that there exist certain sets of transformation rules
that allow us to replace sequences of quantum gates by others that yield the same output. For
example, these transformations could involve swapping the position of two gates, or moving
one gate to a different position relative to another. Furthermore, some sequences of gates can
be shortened by merging gates without changing the output.

We can naturally formulate the problem quantum circuit optimization as an RL prob-
lem [336]. In the resulting framework, depicted in Fig. 6.7, the environment holds the quan-
tum circuit, containing information about the different gates, such as their error rates. Te agent
can observe a representation of the quantum circuit, which corresponds to the state, and it can
decide to perform a transformation to the circuit from a set of possible transformation rules.
The environment can evaluate the resulting circuit after the transformation, and provide the
agent with a reward. The reward can account for various aspects, such as the reduction in the
total gate count, the reduction in depth (the time needed for the circuit to run), or the combi-
nation of both. Additionally, the reward function can also depend on a decoherence estimate
for the whole circuit, based on the decoherence that happens on each the gates.

21In fact, we show how to employ RL methods to tackle quantum error correction in section 6.6.5

157

circuitrepresentationgate count, depthactiontransformcircuitstateENVIRONMENTAGENTcircuittransformationReinforcement learning

Figure 6.8: Schematic representation of the RL-based error correction framework.
The agent can choose the next gate or measurement to be applied to an ensemble of
a few, possibly error-affected, qubits in order to protect a single target qubit. Adapted
from [332].

This way, the resulting circuit optimization is an autonomous process that can account for
speciﬁc information about the hardware when chosing the actions, e.g., some gates involve
longer execution times, or a given qubit may be prone to further errors than others. In the
future, quantum compilers will be able to optimize circuits tailored to the hardware speciﬁca-
tions and native gate implementation.

6.6.5 Quantum error correction

Whenever we perform any kind of computation, we have to ensure that it is performed ﬂaw-
lessly.
In both classical, and quantum computation, we need mechanisms to mitigate any
possible effect of errors occurring during computations. Whereas classical error corrections
methods have long been established, the current quantum error correction schemes come with
a daunting overhead in the number of qubits. Moreover, classical correction schemes cannot
be transferred directly to the quantum case, since we can neither simply copy arbitrary quan-
tum states (known as the no-cloning-theorem [337]) nor measure the quantum computer’s
state arbitrarily to ﬁnd possible errors, as we would erase the state’s superposition. Some er-
ror correction implementations tackle these challenges using RL methods. Here, we discuss
two different approaches.

Error correction with qubit interaction The ﬁrst one proposes a suitable error correction
scheme from scratch, simply interacting with a collection of qubits [332], as sketched in
Fig. 6.8. This approach treats the actual hardware as a black-box, and therefore it is versatile
regarding the hardware’s constraints, as it does not require any prior knowledge about the task.

158

environmentmeasurementqubitsnoisepick a gateXmeasureagentactionstatepick next gate or measureorReinforcement learning

In this setting, the goal is to preserve an arbitrary single-qubit state, |φ(0)〉 = α |0〉+β |1〉, over
time. In order to do so, the agent can choose to apply gates from a given set, or to perform
measurements on auxiliary qubits. This way, any hardware limitation can readily be incorpo-
rated by a suitable choice of the avilable gates, which conforms the action space. Then, we
can measure the performance in terms of the ﬁdelity F = | 〈φ(T )|φ(0)〉 | ∈ [0, 1] after some
arbitrary, but ﬁxed time T .

However, a naive RL approach is bound to fail when we only consider the ﬁdelity as the
reward. Almost all possible circuit transformations reduce the ﬁdelity, thus making random
strategies worse than remaining idle. This happens even when considering the ﬁdelity after
each new gate or measurement, as the optimal scheme initially decreases the ﬁdelity, and
applies a recovery sequence to restore it afterwards. Hence, the chance of ﬁnding the right gate
sequence to protect the state vanishes for large times T . In order to overcome these challenges,
the authors in Ref. [332] propose a two-stage learning scheme, and a more convenient reward
function.

The two-stage learning consist of training two models. First, we train an RL agent which
has access to enhanced information with respect to what it is available in an actual
device, such as a full description of the multi-qubit state. This also allows us to use
a more convenient reward function: the recoverable quantum information

rt

= 1
2

min
(cid:126)n

(cid:107) ˆρ(cid:126)n

(t) − ˆρ−(cid:126)n

(t)(cid:107)

1 ,

(6.53)

where (cid:126)n denotes the vector in the Bloch sphere corresponding to the initial state. This
reward uses the idea that (cid:126)n and −(cid:126)n are orthogonal to provide a reward at every time-
step that guides the agent toward the optimal gate sequence. See [332] for details.
Then, we train the second model using the ﬁrst one as a teacher. The second model
only has access to the information available in a real device, such as the gates it applies,
and the occasional measurement outcomes. Instead of using RL, we train it in a super-
vised way to mimic the behavior of the ﬁrst one. The process is analogous to training
a supervised classiﬁer in which the labels are the actions of the ﬁrst model.

The overall process of two-stage learning is way faster, and much less computationally
demanding than directly solving the original problem. The main limitation is that the teacher
model requires a full state description of the multi-qubit system, which limits the application
to just a few qubits, and requires a well-characterized noise map of the device that might not
be known, in practice.

Error correction with stabilizer codes Whereas the ﬁrst approach aims at discovering the
best error correction scheme from direct qubit interaction, the second one implements a quan-
tum code to represent logical qubits [338–340]. In the example we consider here [341], the
authors use stabilizer codes to achieve error correction via redundancy. In order to properly
understand the process, let us build some basic intuition about the stabilizer formalism. Con-
sider a precursory code to correct arbitrary single bit ﬂips of the physical qubits, |0〉 ↔ |1〉
with the encoding

|0L

〉 = 1
(cid:112)
2

(|000〉 + |111〉)

for a single logical qubit state |0L
〉 in terms of three physical qubits. We can jointly measure sub-
sets of qubits without changing the state with stabilizer operations. In this case, we can apply
〉.
the operations Z1Z2, and Z2Z3 without altering the qubit state: Z1Z2

〉 = Z2Z3

〉 = |0L

|0L

|0L

159

Reinforcement learning

Moreover, we can use these operators to detect bit-ﬂip errors on one of the physical qubits,
as the stabilizer operators are designed to not alter the erroneous state either.22 The stabi-
lizer measurements have an outcome of ±1, and applying them successively we can identify
whether any qubit suffered an error to proceed with the correction. The series of outcomes is
known as the syndrome, and, in practice, these can also have errors.

In this example, we can deal with single bit-ﬂip errors, but not with phase errors repre-
sented by Zi operators. For the error correction of arbitrary single-qubit errors, we need ﬁve
physical qubits with four stabilizer operations [340]. The amount of qubit overhead grows
quickly with the number of qubit error classes to cover. The stabilizer code in Ref. [341], is
a surface code to protect a single logical qubit against arbitrary errors affecting up to d qubits
while using, at most, d 2 physical ones.

To properly perform error correction, we need a combination of accuracy, scalability, and
speed to detect and correct errors. We can formulate this as an RL task [341–344] implement-
ing the full toolbox introduced in this chapter. In the setting from [341], the environment
tracks the underlying quantum state, accounting for possible stochastic errors on the physical
qubits in the form of depolarizing and bit-ﬂip noise. The agent can choose to perform single-
qubit X -, Y -, or Z- rotations,23 or to perform syndrome measurements. Then, the environment
provides the agent with the (possibly faulty) measurement outcome, and a reward, from which
the agent can decide the new set of actions to perform. The environment employs a referee
decoder that checks whether the multi-qubit state after the agent’s actions leads to the same
logical qubit state. If it is the case, the reward is positive, otherwise, it is negative and the
episode terminates.

The authors in Ref. [341] consider an agent based on a DQN which they train with
Q-learning algorithm, as we explain section 6.2.3. After training, the average lifetime
of the encoded logical qubit can be extended drastically as shown in Fig. 6.9.
Furthermore, they implement an alternative genetic algorithm within the described
framework that results into signiﬁcantly smaller ML models, which are better suited to
run in actual devices.

6.6.6 Quantum experiment design

The design of new experiments is key for the development of the quantum sciences. The more
complex the applications become, the harder it is to ﬁnd suitable setups to test our ideas.
In the context of quantum physics, this can be illustrated in an optical experiment, where we
combine different components such that the ﬁnal quantum state has certain desired properties.
For instance, ﬁnding the appropriate set of components to create multipartite entanglement in
high dimensions is a non-trivial task, and usually relies on sophisticated previous knowledge
on the states, and involved mathematical approaches [345]. Nonetheless, such states are of
great importance in applications of quantum information and computation, and hence they
are highly coveted.

In Ref. [318], the authors propose an autonomous approach to build experiments with RL,
using the PS algorithm that we introduce in section 6.5. The goal is to create high-dimensional
many-particle entangled states, based on the orbital angular momentum of light. To do so, the
agent has access to a set of optical elements, and the actions consist on placing one of such

22In practice, we would ﬁrst devise a set of stabilizer operators, and then, we would deﬁne the logical 0 and 1

states as the simultaneous eigenstates of all of the stabilizers.

23Given that XZ = iY, we can reduce the action space in certain cases.

160

Reinforcement learning

Figure 6.9: Average lifetime of the logical qubit encoded with the surface code. The
physical qubits are affected by depolarising noise with parameter pphys that drastically
decreases the unprotected qubit lifetime (black line). With agents trained on various
noise levels, pphys, we can dramatically increase the qubit lifetime (blue dots). Taken
from [341].

components in the optical table. The states are the different conﬁgurations of optical com-
ponents in the table. After each placement, the environment analyzes the resulting quantum
state generated by the setup. If it corresponds to the desired quantum state, it provides the
agent with a reward and the episode ends. If not, the agent continues placing more elements.
It is important to note that, due to the presence of noise in optical setups, the more elements,
the harder it becomes to correctly ﬁnd the target quantum state. Hence, the agent is given
a maximal number of elements to reach its goal, after which the episode ends and the table
resets.

From a technical point of view, the agent has a 2-layer ECM: one representing the table
conﬁgurations (states), and one representing the optimal components (actions). An interesting
feature of PS is action composition: the agent can create new composite actions from simpler
ones that where found useful in previous episodes. In the current context, if the agent ﬁnds
a particular proﬁtable action sequence leading to a reward, the actions can be added combined
as a new single one in the ECM, hence allowing the agent to access rewarded experiments in
a single decision step. This way, the agent can distill combinations of components that lead to
well known setups, such as optical interferometers, as well as completely novel ones, such as
a non-local version of the Mach-Zender interferometer.

Hence, we can divide the general task of generating quantum states in two: ﬁnding the
simplest optical conﬁguration leading to the target state, and ﬁnding as many experiments as
possible that produce it. The former is crucial in terms of practical applications of quantum
technologies, as shorter experiments are less noisy, and usually easier to implement. The latter
allows us to explore to the full extent all possibles solutions to the problem, which may lead
to the discovery of new approaches to create the desired quantum states.

Automated design of quantum optical experiments has also been tackled with non-RL ap-

proaches [346–348]. We describe them in more detail in section 7.3.4.

161

Reinforcement learning

Figure 6.10: Schematic representation of the RL framework to ﬁnd optimal relax-
ations. The agent can modify the set of active constraints of a problem with its
actions. These constraints go into the environment, which solves the constrained
optimization problem. Then, the agent observes a reward that depends on both the
result of the problem and the computational cost incurred by the environment. Given
this observation, it can decide to further modify the constraints.

6.6.7 Building optimal relaxations

In physics we often encounter optimization tasks that we cannot solve in a reasonable amount
of time. In these cases, we rely on approximate methods to obtain solutions that are as close as
possible to the exact one. There are two paradigmatic approaches: variational and relaxation
methods. In the former, we parametrize a family of solutions with the hope that it contains
the exact one, such as the variational quantum states introduced in chapter 5. In the latter, we
build a relaxed (easier) version of the problem in order to provide the optimization process
with desirable properties, such as convexity.

Relaxation methods are broadly used in quantum physics and they lie at the core of quan-
tum information processing. One of the most paradigmatic examples in entanglement theory
is the relaxation from the set of separable states to those that are positive under partial trans-
position (PPT) [349]. Determining whether a state belongs to the ﬁrst class is hard, whereas
it is straightforward to check the membership to the second one. This greatly simpliﬁes the
problem of determining whether a state is entangled: we simply need to check it is not PPT.
However, while all the product states are PPT, there are some entangled states which also
belong to this class, thus resulting into an outer bound to the set of separable states.

Just like with variational methods, we often encounter a trade-off between the computa-
tional cost that we can incur and the accuracy of the method. Hence, given a limited compu-

162

quality andcost of relaxationactionmodifyconstraintsstateENVIRONMENTAGENTset ofconstraintsrelaxationSdP solverReinforcement learning

tational budget, it is crucial to ﬁnd the relaxation that best approximates the optimal solution.
Nevertheless, there is no clear way to know such optimal relaxation beforehand. The most
common practice relies on exploiting speciﬁc knowledge of the given problem, such as sym-
metries, to build hand-crafted relaxations which, in general, are suboptimal. However, we can
combine RL with semideﬁnite programming to systematically build optimal relaxations [350].

A natural way to build relaxations is to remove or relax constraints of the optimization
problem at hand. In the proposed RL framework, presented schematically in Fig. 6.10, the
states encode the active constraints of the problem, and the agent can loosen or strengthen
them with its actions. The environment acts as a black box that provides the agent with the
associated reward to the action and the new state, i.e., the new set of constraints. The rewards
are engineered to guide the agent toward the optimal relaxation, evaluating both the quality
and the cost associated to the current one.

The RL agent is completely agnostic to the problem. Therefore, the method can be ap-
plied in a wide variety of relevant problems in physics and optimization, such as building
entanglement witnesses, optimizing outer approximations to the quantum set of correlations
or ﬁnding better sum-of-squares representations of multivariate polynomials, to name a few.
In Ref. [350], the authors show a particular application to ﬁnd the ground state energy of
quantum many-body Hamiltonians. They can infer properties of the system from the resulting
optimal relaxations, such as changes in the ground state, and, even more, they can explore the
phase diagram in an autonomous way using transfer learning.

6.7 Outlook and open problems

In this chapter, we have introduced the ﬁeld of RL and its main paradigms, featuring value-
based RL (section 6.2), policy gradient methods (section 6.3), and actor-critic algorithms (sec-
tion 6.4). Additionally, we have explored other methods that present an alternative approach
to RL, such as the PS algorithm (section 6.5). These lay down the conceptual foundations to
understand a whole plethora of other advanced RL techniques, while already being competi-
tive, as we show in section 6.6.

In the context of quantum technologies, RL has been widely applied to quantum control
problems. Specially, in quantum simulation, and with the current boom in quantum computa-
tion, many problems involving state preparation, error correction, or controlling and preparing
qubits have a natural mapping to the RL framework [351–356]. Furthermore, RL serves as an
optimization tool for large problems with a clear structure, with applications as varied as quan-
tum circuit optimization, the design of experimental setups, or the construction of relaxations
in quantum information processing problems.

Similar to unsupervised learning, RL is an appealing technique for autonomous scientiﬁc
discovery, as it does not require explicit fully-characterized learning instances. However, while
we can identify some previously known strategies in the resulting RL applications, as in the sec-
tion 6.6.4 example, there is still the need to develop further analysis techniques in order to
fully understand the nature and rationale behind some of the most prominent results.

A big concern in the ﬁeld of RL algorithms is data efﬁciency, which is crucial in applications
involving costly experiments or simulations. In this regard, the ﬁeld of RL can greatly beneﬁt
from the latest advances in physics, such as devising optimal exploration strategies for the most
challenging problems, or leveraging the latest advances in quantum technologies to enhance
RL, as we show in section 8.2.7.

163

Reinforcement learning

Further reading

• Sutton, S. R. & Barto, A. G. (2018). Reinforcement Learning: An Introduction. This
textbook provides a comprehensive review on RL [287]. Speciﬁcally, chapters 7 and
12 expand the TD concept, and chapter 13 contains a full complementary derivation of
policy gradient, actor-critic, and their application to continuing problems (inﬁnite time).

• Marquardt, F. (2021). Machine learning and quantum devices. SciPost Phys. Lect. Notes

29. An introduction to RL for physicsts [330].

• Silver, D. et al. (2014). Deterministic policy gradient algorithms. PMLR, 387–395 [357].
We have introduced policy gradient methods in section 6.3 with stochastic policies. Here,
the authors introduce policy gradient with deterministic policies and its corresponding
implementation in actor-critic algorithms.

• Some of the current state-of-the-art algorithms, such as the ones we mention at the
end of section 6.4, feature additional terms in the objective function, usually in the
form of an entropy or a Kullback-Leibler (KL) divergence. This results into more robust
algorithms, and it is tightly close to the formulation of RL as probabilistic inference. We
recommend reading Ref. [358] for a tutorial, Ref. [359] for a prominent algorithm, and
Ref. [360] for another algorithm, featuring a great overview of the ﬁeld, which recently
proved its performance in experimental control [361].

164

Deep learning for quantum sciences – selected topics

7 Deep learning for quantum sciences – selected topics

Figure 7.1: There exists a dual relationship between machine learning (ML) and
physics. In this chapter, we focus on the more popular direction, where techniques
from ML, in particular deep learning (DL), are used to solve problems in physics.

So far, these Lecture Notes have focused on four broad ﬁelds at the intersection of quantum
sciences and ML: phase classiﬁcation with unsupervised and supervised ML methods in chap-
ter 3, use of kernel methods especially in quantum chemistry in chapter 4, representation of
quantum states with ML models in chapter 5, and use of reinforcement learning (RL) in quan-
tum sciences in chapter 6. We have presented each of these ideas in detail after a (hopefully)
exhaustive introduction. As such, chapter 3 through chapter 6 have highlighted a plethora of
ML applications in quantum sciences. However, they obviously do not constitute a complete
overview of the ﬁeld.

To ﬁll these gaps, the following two chapters aim at addressing more specialized topics
located at the intersection of ML and quantum sciences. This chapter, in particular, discusses
further how ML can be used to solve problems in quantum sciences (see Fig. 7.1). We start
by explaining the concept of differentiable programming (∂ P) and its use cases in quantum
sciences in section 7.1. Section 7.2 describes generative models and how they can tackle
density estimation problems in quantum physics. Finally, we describe selected ML applications
for experimental setups in section 7.3.

7.1 Differentiable programming

Differentiable programming (∂ P) represents a fundamental shift in software development that
emerged from DL [362]. In “standard” programming each instruction is explicitly speciﬁed
in the code, i.e., one speciﬁes a point in the program space with some desirable behavior
In ∂ P, computer programs are instead composed of parametrized elements
(see Fig. 7.2).
of code which can be adjusted. The programmer speciﬁes the desired behavior of the program
via a loss function. The space of programs is then searched for a suitable program by tuning the
code parameters to minimize the given loss function using derivative information. An example
of this which we have continually encountered in these notes is the use of backpropagation
to efﬁciently tune the parameters of an NN to solve a given task, such as classifying differ-
ent phases of matter or representing the ground-state wavefunction of a quantum many-body
system.

In most real-world problems, collecting data in the form of instances in which a given
task has been correctly solved is easier than writing a program that solves the task. Under

165

DEEPLEARNINGPHYSICSDeep learning for quantum sciences – selected topics

these circumstances ∂ P shines, because it allows for the program which solves the task to be
learned from data. This approach can be extremely powerful as demonstrated by the success
of programs generated through deep learning. Indeed, as we have extensively discussed in
these notes, there are nowadays many instances where ∂ P has led to algorithms which easily
outperform humans, such as in AlphaGo [20].

∂ P also has multiple other advantages compared to conventional programming. One as-
pect regards the possibility to develop customized optimization strategies. The typical instruc-
tion set of NNs consists of matrix multiplication, vector addition, element-wise application of
nonlinearities: such a set is limited and much smaller, compared to the instruction set associ-
ated with the entire class of standard computer programs. This can allow for computational
speed-ups through the design of hardware that is optimized for the limited instruction set
underlying ∂ P. Graphics processing units (GPUs) and tensor processing units (TPUs) are ex-
amples of such application-speciﬁc hardware. More recently, neuromorphic computing has
emerged as a new paradigm which promises faster and more energy-efﬁcient computation
for machine intelligence through hardware systems that mimick the neuronal and synaptic
computations of the brain [363, 364].

∂ P also allows for more ﬂexible programming: consider the situation where you had stan-
dard code that performs a certain task and someone wanted you to make it twice as fast,
possibly at the expense of its accuracy. This would be a highly non-trivial task. However, it is
easy to incorporate such constraints by means of a cost function and hyperparameters in ∂ P.
For example, given that one uses an NN this could be accomplished by cutting the network’s
size in half and retraining it. Moreover, consider the situation where programs which were
ﬁrst optimized or coded individually are merged together in a modular fashion to create a new
larger program. Then, ∂ P offers an easy solution for optimizing the performance of this new
program: simple ﬁne-tuning of the individual components in the given conﬁguration through
optimization. The beneﬁts of ∂ P come at the cost of program interpretability. At the end of the
optimization we obtain code that works well, but which is very hard to read for a human and
understand in intuitive terms. As such, we typically are left with the choice between a fairly
accurate model that is understandable in human terms, and a more accurate model that is
difﬁcult to interpret.1

In ∂ P, arbitrary computer program structures can be differentiated in an automatic
fashion. Importantly, this allows for NNs to be embedded into a plethora of existing
scientiﬁc simulations and computations, because the gradients required for training
the NNs can be computed efﬁciently. In particular, one can differentiate through the
NN, as well as surrounding non-parametrized/non-trainable parts of the program.

Recently, widespread interest in ∂ P has arisen in the area of scientiﬁc computing [365].
Examples for algorithms which have been written in a fully differentiable way are Fourier
transforms, eigenvalue solvers, singular value decompositions, or ordinary differential equa-
tions (ODEs) [366, 367]. As such, one is able to differentiate through domain-speciﬁc com-
putational processes to solve inverse problems, such as learning or control tasks: Tensor net-
works [368–370], molecular dynamics [371, 372], quantum chemistry [373–379], quantum
optimal control [380–390], or quantum circuits [391,392] have all been formulated in a fully-
differentiable manner. We discuss several examples in detail in section 7.1.2.

Notably, ∂ P enables scientiﬁc ML which combines the best of two worlds: in general, black-

1Again, interpretability appears as a central issue (see section 3.5).

166

Deep learning for quantum sciences – selected topics

Figure 7.2: Illustration of the difference between “standard” programming and dif-
ferentiable programming. In some cases, the complexity of programs found by ∂ P
exceeds human capabilities. Adapted from [362].

box ML approaches are ﬂexible but require a large amount of data to be trained successfully.
The amount of required data can be reduced by incorporating our scientiﬁc knowledge on the
structure of a problem into the program. The training of the parametrized program part is
then enabled via ∂ P. This allows for the learning task to be simpliﬁed because only the parts
of the model that are “missing” need to be learned.

Perhaps the biggest feat of ∂ P is the ability to compute gradients of loss functions with
respect to the NN parameters (see section 2.5). Recall that we require these gradients for NN
training when using gradient-based optimizers (as is typically done). Crucially, the computa-
tion is efﬁcient, precise, and occurs in an automated fashion. In particular, it allows for arbi-
trary NN architectures to be differentiated automatically without implementation overhead.
Compare this to the tedious computation of analytical gradients which needs to be performed
again given different NN architectures.

However, ∂ P is not restricted to the computation of gradients with respect to NN param-
eters for NN training.
It enables the automatic computation of gradients and higher-order
derivatives of arbitrary program variables. These can, for example, be tunable parameters of
a Hamiltonian whose ground state we are interested in. Being able to differentiate through
the eigensolver, we can tune the Hamiltonian’s parameters via a derivative-based optimizer
such that its ground state satisﬁes desired properties (as speciﬁed by a loss function), see sec-
tion 7.1.2 for details. This is an example of an inverse problem which can be solved efﬁciently
through ∂ P. However, the applicability of ∂ P goes beyond solving optimization tasks. Gra-
dients and higher-order derivatives contain highly valuable information on the relationship
between model parameters and outputs which can, e.g., facilitate the interpretation of phase
classiﬁcation methods [144] (see section 3.5.3) or help to characterize variational quantum

167

PROGRAM    SPACEoptimizationProgram    complexity“Standard”   programmingDifferentiable   programmingDeep learning for quantum sciences – selected topics

circuits [393].

7.1.1 Automatic differentiation

∂ P allows us to compute the gradients and higher-order derivatives of arbitrary computer
programs.

In general, methods for the computation of derivatives in computer programs can be
classiﬁed into four categories [394]: (1) manually working out derivatives and coding
them, (2) numerical differentiation using ﬁnite difference approximations, (3) symbolic
differentiation using expression manipulation,a and (4) automatic differentiation (AD)
which is the workhorse behind ∂ P.

aThis is done by computer algebra systems such as Mathematica, Maxima, or Maple.

Let us brieﬂy discuss these different approaches.

Manual differentiation is time consuming and prone to errors. Numerical differentiation is
quite simple to implement. Its most basic form is based on the limit deﬁnition of a derivative:
given a multivariate function f : Rm → R, the components of its gradient ∇ f = ( ∂ f
)
∂ x1
can be approximated as

∂ f
∂ xm

, . . . ,

∂ f
∂ xi

) − f (x)

f (x + hei
h
∈ Rm is the i-th unit vector and h is a small step size. Approximating ∇ f in such
where ei
a fashion requires O(m) evaluations of f . This is the main reason why numerical differen-
tiation is not useful in ML where the number of trainable parameters m can be as large as
millions or billions. Also note that for the gradient approximation to be somewhat accurate,
the step size h needs to be carefully chosen: while the truncation error of the approxima-
tion in Eq. (7.1) can be made arbitrarily small as h → 0, eventually round-off errors due to
ﬂoating-point arithmetic dominate the calculation.2

(7.1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)x

≈

,

d
d x

Symbolic differentiation is the automated manipulation of mathematical expressions for
obtaining explicit derivative expressions, e.g., by using simple derivative rules such as the
product rule

g(x) + f (x) d g(x)
d x

( f (x)g(x)) = d f (x)
d x
Symbolic expressions have the beneﬁt of being interpretable and allow for analytical treat-
ments of problems. However, symbolic derivatives generated through symbolic differentiation
typically do not allow for efﬁcient calculation of derivative values. This is because they can
quickly get substantially larger than the expression whose derivative they represent. Con-
sider a function of the form h(x) = f (x)g(x) and its derivative, which can be evaluated by
the product rule in Eq. (7.2). Note that f (x) and d f (x)
, for example, appear separately in
such an expression. A naive calculation of the derivative according to Eq. (7.2) thus involves
duplicate computations of any expressions that appear both in f (x) and d f (x)
. Moreover,
manual and symbolic methods require the underlying function to be deﬁned in a closed-form
expression. As such, they cannot easily deal with programs that involve conditional branches,

(7.2)

d x

d x

.

2In computing, ﬂoating-point numbers are typically represented approximately through a ﬁxed number of sig-
niﬁcant digits that are scaled through an exponent in some ﬁxed basis a × bc, where a, b, and c are all integers.
Because of the limited number of representable numbers, round-off errors can occur when performing computa-
tions.

168

Deep learning for quantum sciences – selected topics

Figure 7.3: Computation graph associated with the forward evaluation trace of
f (x1, x2

) + cos(x2

) = ln(x1

) − x1 x2.

loops, or recursions. That means, for symbolic differentiation to be efﬁcient there must exist
a convenient symbolic expression for computing the derivative under consideration.

When we are concerned with the accurate numerical evaluation of derivatives and not
their symbolic form, it is possible to signiﬁcantly simplify computations by storing the values
of intermediate sub-expressions in memory. This is the basic idea behind automatic differenti-
ation (AD). AD provides numerical values of derivatives (as opposed to symbolic expressions)
and it does so by using symbolic rules of differentiation (but keeping track of derivative values,
as opposed to the entire symbolic expression). As such, it may be viewed as an intermediate
between numerical and symbolic differentiation. AD makes use of the fact that every computer
program, no matter how complicated it may look, simply executes a sequence of elementary
arithmetic operations (e.g., additions or multiplications) and elementary functions (e.g., exp
or sin). We refer to the sequence of elementary operations that a computer program applies to
its input values to compute its output values as evaluation trace [395]. The derivative of every
computer program can therefore be computed in an automated fashion through repeated ap-
plication of the chain rule. As such, the number of arithmetic operations required to compute
the derivative are on the same order as for the original program. Moreover, this results in
derivatives which are accurate up to machine precision. In the following we illustrate how AD
is done in practice.

Forward-mode AD Conceptually, AD in so-called forward-mode is the simplest type. Consider
the evaluation trace of the function

f (x1, x2

) = ln(x1

) + cos(x2

) − x1 x2

(7.3)

given in Table 3(left). The associated computation graph is shown in Fig. 7.3, where the com-
putation of a function f is decomposed into variables vi. We follow the standard notation used
in Ref. [396], where v1−i, i = 1, . . . , n are the input variables, vi, i = 1, . . . , l are intermediate
variables, and vl+i, i = 1, . . . , m are output variables. For computing the derivative of f with
respect to x1, we start by associating with each variable vi a derivative

=

˙vi

∂ vi
∂ x1

.

(7.4)

Applying the chain rule to each elementary operation in the evaluation trace, we generate
the corresponding derivative trace, given in Table 3(right). In forward-mode AD the desired
derivative ˙v5
(where y is the output variable) is obtained by computing the intermediate
variables vi in sync with their corresponding derivatives ˙vi.

= ∂ y
∂ x1

This can be generalized to the computation of the full Jacobian of a function f : Rm → Rn
with m input variables xi and n output variables y j. In this case, each forward pass of AD is
= 1 for a single variable xi and zero for the rest. That is, we choose
initialized by setting ˙xi

169

Deep learning for quantum sciences – selected topics

= x1
= x2
= ln v−1
= v−1v0
= cos v0
− v2
= v1
= v3
+ v4
= y

v−1
v0

v1
v2
v3
v4
v5

v5

= 2
= 1

= ln 2
= 2
= cos 1
= −1.307
= −0.767

= −0.767

/v−1

= ˙x1
= ˙x2
= ˙v−1
= ˙v−1v0
= −˙v0 sin v0
− ˙v2
= ˙v1
= ˙v3
+ ˙v4
= ˙y

˙v−1
˙v0

˙v1
˙v2
˙v3
˙v4
˙v5

˙v5

+ ˙v−1˙v0

= 1
= 0

= 1/2
= 1
= 0
= −1/2
= −1/2

= −1/2

Table 3: Workﬂow for computation of derivatives in forward-mode AD given the
function f (x1, x2
) − x1 x2. Left: Forward evaluation trace for the
choice of initial inputs (x1, x2
) = (2, 1). Right: Forward derivative trace resulting in
the computation of

) + cos(x2

) = ln(x1

) = (2, 1).

at (x1, x2

∂ f
∂ x1

˙x = ei where ei is the i-th unit vector. The forward pass with given input values x = a then
computes

=

˙y j

∂ y j
∂ xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)x=a

for j = 1, . . . , n.

This corresponds to the i-th column column of the Jacobian matrix

=

J f






∂ y1
∂ x1
...
∂ yn
∂ x1

. . .
...
. . .

∂ y1
∂ xm
...
∂ yn
∂ xm





.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x=a

.

(7.5)

(7.6)

Thus, the full Jacobian can be computed in m forward passes, i.e., m evaluations of the function
f . As such, forward-mode AD is efﬁcient if m (cid:28) n. In the other limit, so-called reverse-mode
AD is preferred which we discuss shortly.

In practice, forward-mode AD is implemented by augmenting the algebra of real numbers
and introducing a new arithmetic: to every number one associates an additional component
which corresponds to the derivative of a function computed at that particular value. We call
this composite number a dual number

dual(v) = v + ˙vε,

(7.7)

where ε (cid:54)= 0 is a number such that ε2 = 0. The extension of all arithmetic operators to dual
numbers allows for the dual number algebra to be deﬁned. Observe, for example, that

f (dual(v)) = f (v) + ˙f (v)˙vε,

(7.8)

where we obtain the function value in the ﬁrst part and the corresponding derivative ˙f (v)˙v in
the ε part.3 This follows from expanding the function in its Taylor series and noting that terms
O(ε2) vanish due to the property that ε2 = 0. Equation (7.8) resembles the computation of
the derivative using the chain rule.

3Under the hood dual numbers are typically handled through so-called operator overloading, i.e., overloading

all functions to work appropriately on the new algebra.

170

Deep learning for quantum sciences – selected topics

Reverse-mode AD As the name suggests, in reverse-mode AD the derivatives are propagated
backwards from a given output.4 This is in contrast to forward-mode AD where we saw that
the derivatives are propagated forwards in sync with the function evaluation. Reverse-mode
AD is done by complementing each intermediate variable vi with a so-called adjoint

=

¯vi

∂ y j
∂ vi

,

(7.9)

where y j is the output variable with respect to which we desire to compute derivatives. In re-
verse mode AD, derivatives are computed in the second phase of a two-phase process. In the
ﬁrst phase, the original function code is run forward: intermediate variables vi are populated
and their dependencies in the computational graph are tracked through a bookkeeping proce-
dure. In the second phase, derivatives are calculated by propagating adjoints ¯vi in reverse, i.e.,
from the outputs to the inputs. This is illustrated in Table 4 for the function given in Eq. (7.3),
= ¯y = ∂ y
= ∂ y
where the reverse pass is started with ¯v5
∂ x1
∂ y
and ¯x2

= 1. As a result, we obtain both ¯x1

in a single reverse pass.

= ∂ y
∂ x2

v−1
v0

v1
v2
v3
v4
v5

= x1
= x2
= ln v−1
= v−1v0
= cos v0
− v2
= v1
+ v4
= v3

= ln 2
= 2
= cos 1
= −1.307
= −0.767

= y

v5

= −0.767

= ¯y

¯v5

= 2
= 1

+v4

= ∂ v3
∂ v4

¯v5

+ ∂ v3
¯v3
∂ v0
+ ∂ v2
∂ v−1

¯v1

¯v2

¯v4
¯v3
¯v2
¯v1
¯v0
¯v−1

= ∂ v5
¯v5
∂ v4
= ∂ v5
¯v5
∂ v3
= ∂ v4
¯v4
∂ v2
= ∂ v4
¯v4
∂ v1
= ∂ v2
¯v2
∂ v0
= ∂ v1
∂ v−1

¯v0
¯v−1

= ¯x2
= ¯x1

= 1

= 1
= 1
= −1
= 1
= −2.841
= −1/2

= −2.841
= −1/2

Table 4: Workﬂow for computation of derivatives in reverse-mode AD given the func-
)− x1 x2. Left: Forward evaluation trace for the choice
) = ln(x1
tion f (x1, x2
)+cos(x2
) = (2, 1). Right: Reverse (adjoint) derivative trace resulting
of initial inputs (x1, x2
∂ f
in the computation of
∂ x1

at (x1, x2

) = (2, 1).

∂ f
∂ x2

and

This example illustrates the complementary nature of the reverse mode compared to the
forward mode: The reverse-mode is cheaper to evaluate than the forward mode for functions
with a large number of inputs, i.e., where m (cid:29) n with f : Rm → Rn. As we just saw, in
the extreme case of f : Rm → R, only one application of the reverse mode is sufﬁcient to
compute the full gradient compared with the m passes of the forward mode. The typical
case encountered in ML applications corresponds to the evaluation of the derivatives of a loss
= L : Rm → R with respect to m trainable parameters, where m is typically large.
function y j
As such, reverse-mode AD is the preferred method for computing gradients automatically as
it is computationally more efﬁcient compared to forward-mode AD.5 In the context of ML,

4Historically, reverse-mode AD can be traced back to the master thesis of Seppo Linnainmaa in 1970 [397]
in which he described explicit, efﬁcient error backpropagation in arbitrary, discrete, possibly sparsely connected,
NN-like networks [398].

5Note that forward-mode and reverse-mode AD are just two (extremal) ways of applying chain rules. Finding
the optimal way to traverse the chain rule to compute a Jacobian for a given function (i.e., the choice which results
in the smallest number of arithmetic operations) is known as the optimal Jacobian accumulation problem and is
NP-complete.

171

Deep learning for quantum sciences – selected topics

reverse-mode AD applied to NNs is typically referred to as backpropagation, see section 2.5. It
is the working horse behind NN training as it allows for efﬁcient computation of the gradients
for arbitrary NN-based architectures in an automated fashion. In the following, we discuss
several ways how reverse mode AD is implemented in practice.

Static graph AD A basic implementation of reverse-mode AD makes use of static computation
graphs. This choice is natural, given that we chose to illustrate reverse-mode AD using compu-
tation graphs [399]. Tensorﬂow is an example of a platform which uses this approach. Here,
the user must deﬁne variables and operations in a graph-based language. Subsequent execu-
tions of the computation graph allow for the program to be differentiated in a straightforward
manner. However, this requires all existing programs to be rewritten as a static computation
graph which is inconvenient.

Tracing-based AD This can be circumvented by building computation graphs dynamically at
runtime which is achieved by “tracing” all the operations encountered in the forward pass given
a particular input [399]. Dynamic computation graphs are the basis of many reverse-mode
AD implementations in Julia (Tracker.jl, ReverseDiff.jl, or Autograd.jl) or Python (PyTorch,
Tensorﬂow Eager, Autograd [JAX]). The fact that it is simple to implement makes this approach
widely adopted in practice. An issue of such tracing-based implementations is that each trace is
value-dependent, meaning that each run of a program (with different inputs) can build a new
trace. Moreover, these traces can be much larger than the code itself, for example, because
loops are completely unravelled.

Source-to-source AD In source-to-source AD one overcomes these issues by generating
source code for the backward pass that is able to handle all input values [399]. In particular
branches, loops, and recursions are not explicitly unrolled. The right branch in the reverse
pass through recall of the intermediates values used in the forward pass. It turns out that the
implementation of a source-to-source AD system poses many requirements on the underlying
language.6 Source-to-source AD is used in programming languages such as Julia (Zygote.jl).7

High-level adjoint rules The advantages of reverse-mode AD in ML applications come at the
cost of increased storage requirements which (in the worst case) is proportional to the number
of operations in the evaluated function. This is because the values of the intermediate variables
populated during the forward pass need to be stored when using reverse-mode AD, whereas
they can be directly used for the derivative computation within forward-mode AD. Improving
storage requirements in reverse-mode AD implementations is an active research area. In gen-
eral, reverse-mode AD can be made more efﬁcient by deriving adjoint rules at a higher level.
Consider, for example, the case where your program involves solving a nonlinear problem
f (x, p) = 0 with an iterative method, such as Newton’s method. A naive application of the
reverse-mode AD system results in a backward pass through all iteration steps. Not only is this
computationally expensive, but also requires storing the values of all intermediate iteration
steps. Instead of unrolling the entire computation, one can analytically derive an appropriate
adjoint rule which can be used to compute the derivatives in reverse-mode via a separate linear
equation. In particular, it only requires knowledge of the ﬁnal solution x of the nonlinear prob-
lem as opposed to values in the intermediate iterations. For further details, see Ref. [199,366].
Other examples for which adjoint rules can be derived also include ODEs [367, 387, 399, 400]
and eigenvalue solvers [366]. In case of ODEs, for example, the adjoint rule involves solving
a second, augmented ODE backwards in time.

6In particular, it should possess a strong internal graph structure.
7TensorFlow considered building a source-to-source AD based on the Swift language. Older AD systems for

Fortran were also source-to-source.

172

Deep learning for quantum sciences – selected topics

(x) after ≈ 1000 iterations
Figure 7.4: (a) Optimized ground-state wave function Ψ
of the L-BFGS algorithm with box constraints (L-BFGS-B) given the target wave func-
tion Ψ(x) = 1−|x| with |x| < 0.5. (b) Optimized potential V (x) (rescaled by 1/300)
and the initial harmonic potential. Figure reproduced from [401].

0

7.1.2 Application to quantum physics problems

In this section, we illustrate the application of ∂ P to problems from quantum physics through
two simple examples.

Inverse Schrödinger problem As a concrete example of an inverse-design problem, we con-
sider the time-independent Schrödinger equation in one dimension

(cid:149)
− 1
2

d
d x 2

(cid:152)
+ V (x)

Ψ(x) = EΨ(x),

(7.10)

where we set ħh = m = 1 [366]. Typically, we are given a potential V (x) and solve for the
corresponding eigenfunctions Ψ and energies E. Here, we consider the inverse problem.
Given a particular wave function Ψ(x) we want to construct a potential V (x) with a ground-
state wave function Ψ
(x) that closely matches Ψ(x). We restrict ourselves to the domain
x ∈ [−1, 1] and deﬁne the following mean-squared error (MSE) function

0

L =

(cid:90) 1

−1

|Ψ(x) − Ψ

(x)|2d x.

0

(7.11)

The potential V (x) is discretized on a grid and each individual amplitude is tuned using
gradient-based optimization methods in order to minimize Eq. (7.11). An implementation
of this problem in JAX can be found at [401]. The results are illustrated in Fig. 7.4. Here, the
gradient is calculated by the AD system underlying JAX and involves propagating the deriva-
tive through the eigenvalue solver. For more details on ∂ P applied to inverse-design problems
in quantum mechanics and adjoints for eigensolvers, see Ref. [402].

Quantum optimal control Next, we consider a problem from quantum optimal control. We
would like to ﬁnd the time-dependent amplitudes {ui

(t)} of the following Hamiltonian

ˆH = ˆH0

+

nctrl(cid:88)

i=1

ui

(t) ˆHi,

(7.12)

such that the time-evolution under ˆH realize a CNOT gate. Here ˆHi with i ∈ [1, nctrl
independent Hamiltonians that can be tuned through the time-dependent amplitudes {ui

] are time-
(t)}.

173

1.00.50.00.51.0x0.000.040.08(x)(a)targetoptimized1.00.50.00.51.0x0.40.00.4V(x)(b)initialoptimizedDeep learning for quantum sciences – selected topics

j=1 ui j sin(π j t/T ), where we
We parametrize these amplitudes using Fourier series ui
introduce nbasis as a cutoff enabling numerical evaluations. To ﬁnd the gate which a given
(t)} implements we integrate the time-dependent Schrödinger equation from 0
choice of {ui
to T under initial conditions U(0) = 1 with

(t) = (cid:80)nbasis

dU
d t

= −i ˆH(t)U,

(7.13)

where we set ħh = 1. Ideally, U(T ) realizes a CNOT operation (Utarget
setup our loss function as

= CNOT). Hence, we

L = 1 − 1
d

|tr(U(T )†Utarget

)|,

(7.14)

where d is the dimension of the associated Hilbert space. Note that when U(T ) = Utarget we
reach the global minimum of L = 0. The coefﬁcients {ui j
} are tuned to minimize the loss
function in Eq. (7.14) using gradient-based optimization methods. An implementation of this
problem in JAX can be found at [403], where the gradient is calculated via AD in JAX and
involves propagating the derivative through the ODE solver (Eq. (7.13)). For more details on
differentiable programming applied to quantum optimal control, see Refs. [121, 381].

Outlook and open problems

In this chapter, we have introduced the novel programming paradigm that is ∂ P. Most notably,
∂ P enables NNs training via the efﬁcient, precise, and automated calculation of the correspond-
ing gradients (see section 2.5). Having the ability to differentiate arbitrary computer programs,
in addition, allows for NNs to be seemingly incorporated in scientiﬁc workﬂows. By now there
are many applications of ∂ P in scientiﬁc computing, including quantum physics. However, the
ﬁeld is still in its infancy and many open problems remain to be tackled. For instance, ﬁnd-
ing efﬁcient high-level adjoint rules for algorithms used in quantum physics problems, such
as solvers for stochastic dynamics, is still a current topic of research [390]. Another example
are chaotic systems for which standard AD methods can fail [404, 405]. The development of
AD systems is also still an ongoing effort: In Enzyme.jl [406], for example, the idea is to per-
form reverse-mode AD on the portable, low-level intermediate representation of Julia which
is language-agnostic. This allows for performance improvements due to low-level optimiza-
tions. NiLang.jl [407] on the other hand tries to build a reverse-mode AD system based on the
paradigm of reversible programming. Running the program in reverse in the backward pass
allows the overhead in memory in standard reverse-mode AD to be circumvented.

Further reading

• Baydin, A. G. et al. (2018). Automatic differentiation in machine learning: a survey.
J. Mach. Learn. Res. 18(1), 5595–5637. Good overview on AD in the context of
ML [394].

• Innes, M. et al. (2019). Differentiable programming system to bridge machine learning
and scientiﬁc computing. arXiv:1907.07587. Discussion on the role of ∂ P in scientiﬁc
computing [365].

• Google Colab notebooks by Lei Wang [401, 403].

7.2 Generative models in many-body physics

Deep generative models are (mostly) neural-network architectures designed to approximate
the probability density underlying the system we aim to describe. In this section we revise
some of the most relevant algorithms in the ﬁeld. We mainly focus on autoregressive (AR)

174

Deep learning for quantum sciences – selected topics

networks and normalizing ﬂows (NFs). With deep generative models, one often associates
the term density estimation which similarly refers to the construction of the probability density
function underlying a given problem [408]. The difﬁculty of this task comes from the fact
that the relevant probability density functions cannot be dealt with analytically in most cases.
For a large class of cases, these problems can be attributed to the difﬁculty of calculating
normalization constants (i.e., partition functions in statistical mechanics). As such, one often
has to resort to different methods and techniques that approximate the underlying probability
density functions.

Throughout this manuscript, one encounters a variety of instances of density estimation
tasks in quantum physics. For example, ﬁnding a variational representation of quantum-many
body system wave function can be viewed as a density estimation task, see chapter 5. More
generally, in quantum physics the concept of density estimation often appears in the context
of reconstructing the many-body state from measurements performed on a quantum many-
body system – a task known as quantum state tomography, which is described in more detail
in section 5.3.7. This is particularly challenging, because only a reduced set of quantities
is experimentally accessible, such as single-body density matrix or higher-order correlation
functions [409–411]. Moreover, the tomography experiment can be very demanding, and
a single experimental run can take a long time (i.e., hours or days). As such, one is faced with
the task of estimating a high-dimensional probability density function given a small number
of measurements of a restricted set of observables.

In general, one can think of two different approaches to density estimation: parametric
and non-parametric density estimation. In parametric approaches, one ﬁxes a parametrized
functional form for the density. The free parameters are then tuned such that the trial density
best matches the density of the system under consideration. This can be done by compar-
ing the trial density against training data, which corresponds to observations drawn from the
target density, or against an unnormalized target probability density (e.g., an unnormalized
Boltzmann distribution). A simple example of a parametrized trial density would be a Gaus-
In parametric approaches,
sian, where its mean and variance can be adjusted accordingly.
one reduces the problem of ﬁnding an appropriate density function to the problem of ﬁnding
appropriate parameters. Clearly, the choice of functional form of the density is crucial for the
success of this approach, as it can (potentially) restrict the family of target densities that can
be approximated well.

In contrast, in non-parametric approaches to density estimation the structure of the trial
density function is not set a priori.8 Instead, a trial density function is directly constructed
based on the training data. The simplest example of a non-parametric approach to density
estimation corresponds to the construction of a histogram. Clearly, histogram binning comes
with some drawbacks. For example, one must carefully choose the size and location of bins.
Moreover, histograms are non-differentiable functions. In general, non-parametric methods
rely on less knowledge about the system at hand compared to parametric methods. While this
makes non-parametric methods robust and generally applicable, they typically also require
more samples to reach a given level of accuracy.

In recent years, several new methods for density estimation have emerged from the inter-
play of ML and physics based on learning an approximate density underlying a given physical
system. These approaches are typically parametric in nature: the parameters θ of a suit-
able variational (parametric) ansatz for the trial density qθ are optimized systematically to
match the target density p. A natural way to achieve this is based on the maximum likelihood

8The trial density functions in non-parametric approaches to density estimation do not completely lack param-

eters. It simply means that the parametrization of the trial density functions is not ﬁxed in advance.

175

Deep learning for quantum sciences – selected topics

estimation (MLE) principle, where a likelihood function is maximized (or equivalently, a log-
likelihood is minimized) by optimizing the parameters θ (see Eq. (2.26)). For a given data set
}n
of independent observations D = {xi
i=1, the log-likelihood function of the model is deﬁned
as

(cid:96)(θ|D) = log

(cid:150)

(cid:89)

(cid:153)
qθ (x)

,

(7.15)

x∈D

where qθ(x) is the probability assigned to each independent observation of the system x. The
observations D can, for example, refer to a set of spin conﬁgurations x drawn from an Ising
model at a ﬁxed temperature. As a parametrization for the trial density function, one can
choose an NN. The fact that NNs are universal function approximators (see section 2.4.4)
makes this ML-based approach to parametric density estimation very powerful. In this context,
the approximation capabilities of NNs guarantee that any target densities can be approximated
well.

The optimization based on the MLE principle requires training data. However, in some
cases data may not be readily available, because it is difﬁcult or expensive to generate. For
such cases, the data-driven procedure described above may not be optimal – in particular be-
cause NNs typically require a generous amount of data to be trained. To this end, we discuss
an alternative approach to density estimation, sometimes referred to as Hamiltonian-driven
approach, which is based on encoding physical laws and physical prior knowledge of the sys-
tem into the learning process. Let us consider the Ising model, for example. We know the
Hamiltonian of the system and we know that spin conﬁgurations σ are distributed according
to a properly normalized Boltzmann distribution,

p(σ) = e

−H(σ)
kB T

Z

.

(7.16)

In practice, the partition function Z is difﬁcult to compute, as it involves a summation over all
possible spin conﬁgurations.9 However, one can use the knowledge that the system follows
a Boltzmann distribution to efﬁciently train an ML model that approximates the target density
p(x) successfully. In this case, one does not need training data. The parameters are optimized
to minimize the so-called reverse Kullback-Leibler (KL) divergence

KL(qθ||p) =

(cid:90)

log

D

qθ(x)
p(x)

dx,

(7.17)

where D is the relevant integration domain. This quantity measures the discrepancy between
the target density and the trial density.10 Minimizing the KL divergence is equivalent to mini-
mizing the variational free energy. As such, one could see the optimization through KL diver-
gence minimization as being an equivalent formulation for the variational principle in statisti-
cal mechanics [79,80]. Given a trial density function that can be efﬁciently sampled, Eq. (7.17)
can be approximated using Monte Carlo integration. The ratio qθ(x)/p(x) can be efﬁciently
computed, because the normalization constant inside the logarithm becomes a constant shift
and does not need to be computed when training the variational ansatz qθ. For example appli-
cations of this approach in the context of lattice physics, see Refs. [80,412,413]. For a general
overview, see Refs. [414, 415].

The remainder of this chapter is structured as follows. In the next section, we introduce
deep generative models for density estimation based on several examples. Afterwards, we

9In Eq. (7.16) σ is used to indicate the general conﬁguration of a spin system. In the remainder of the section

the more general variable x is used to indicate the point at which the probability density is evaluated.
10Note that this metric may lead to some pathological behavior, already mentioned in section 2.3.

176

Deep learning for quantum sciences – selected topics

analyze a few of those in more depth, putting additional emphasis on normalizing ﬂows (NFs)
in the last paragraph of section 7.2.2.

7.2.1 Deep generative models

The tools of choice for density estimation and synthetic data generation are the so-called
generative neural samplers (GNSs), sometimes also referred to as deep generative mod-
els. GNSs include amongst other normalizing ﬂows (NFs) [416], autoregressive neural net-
works (ARNNs) [417], variational autoencoders (VAEs) [418], and generative adversarial net-
works (GANs) [419, 420]. These methods represent a very important assortment of the whole
plethora of ML algorithms, especially when it comes to unsupervised learning. The usual set-
ting where they are deployed is when, given a set of data, one wants to learn (or approximate)
the underlying probability density of such training data. This way, one is then able to gener-
ate new artiﬁcial samples resembling the true ones. For instance, if we give to a GNS a stack
of images of dogs, once trained, we expect this to generate unseen images of dogs. Thus
what happens in practice is that the algorithm learns the salient, yet recurrent, features that
are more important to shape something which looks like a dog. Images are just a prominent
example, but of course this idea can be broadly applied, especially to physics.
In physics,
though, we have one further advantage. Sometimes we have to deal with systems of which
we know the underlying density up to a normalization factor (e.g., the partition function in
the Boltzmann distribution of a thermodynamic system). As such, we can deploy such tools
in a slightly different way, dealing better with the problem at hand. Therefore, we can follow
two distinct approaches when it comes to apply GNS to physics problems. The ﬁrst option is
to naively learn from data (e.g., data conﬁgurations) in order to learn how to generate seem-
ingly reasonable unseen conﬁgurations, the same approach we would use with dog images.
This way we could infer something on the underlying structure of the physical systems. For
instance, we can learn the underlying density to generate new conﬁgurations and relate such
density to a theoretical distribution. Doing so, one could infer on some order parameters (i.e.,
critical exponents) or the temperature at which the training conﬁgurations were generated
(or measured experimentally). The second approach, instead, relies on some prior knowledge
of the system. For example, we may know the underlying density we try to approximate is
a Boltzmann-like distribution with some unknown normalization factor. Leveraging on this
information we can structure the learning problem in a slightly different fashion though the
goal remains the same: to estimate the underlying target density of the system at hand.

As mentioned above, there are many tools that fall under the category of GNSs and all of
them come with different features and are suitable for different situations. As such, neural
samplers can be divided into two categories: the ones that give access to a tractable density,
i.e., thus allowing to compute p(x) for any x in polynomial time in the system size N , and the
ones that do not. Under the latter category we ﬁnd GANs. With GANs we can generate artiﬁcial
samples looking like the training data. These algorithms consist of two agents trained within
a game theoretic context: a generator network learns how to produce (realistic) samples to
fool a discriminator network whose task is to recognize whether samples are real or artiﬁcially
generated. The joint optimization of those two agents makes the generator more and more
capable of generating realistic samples. This approach works well in practice but unfortunately
does not allow access to tractable densities. In turn, GANs work if we want to generate unseen
conﬁgurations but do not allow much more than that. In Ref. [421] the authors used a GAN
to enhance Markov chain Monte Carlo (MCMC) methods in lattice simulations. The second
category of GNS refers to those algorithms that instead allow access to exact (or approximate)
sampling probabilities along with normalized (or unnormalized) densities. Under this category
we ﬁnd: autoregressive neural networks (ARNNs) [79–83], recurrent neural networks (RNNs)

177

Deep learning for quantum sciences – selected topics

[226, 234], restricted Boltzmann machines (RBMs) [422], and latent variable models such as
variational autoencoders (VAEs) and normalizing ﬂows (NFs) [412, 413, 423]

In the remainder of this section, we provide examples on how to deploy three kind of deep
generative models with some applications. We mention ARNNs and RNNs (already introduced
in section 2.4.6) in the context of density estimation and then focus on NFs.

7.2.2 Applications and examples

Autoregressive models As previously mentioned in chapter 5, one can resort to a certain
class of models called autoregressive (AR) models for the task of density estimation. The
main advantage in using an AR model is that it enables uncorrelated and fast [424] sampling.
The samples are uncorrelated because thanks to the AR structure of the probability distribu-
tion, a direct sampling algorithm can be performed, rather than MCMC whose performance is
hindered by the correlation between samples and can severely fail in the case of multimodal
distributions (where different blobs of probability are far apart in the conﬁguration space).
Direct sampling with AR models goes as follows: sample state x1 from qθ(x1
), then sample
from qθ(x2
). Direct sampling from a probability dis-
|xN −1, . . . , x1
tribution over an exponentially large conﬁguration space (e.g., if the xi are binary variables)
is possible with AR models, and makes them extremely powerful. The sampling can also be
made faster as many samples can be processed in parallel in a single pass. Note that imposing
an AR structure on a model constrains it: for instance, to build a deep CNN one must introduce
masked layers (to keep the AR structure), which alter the representability of such models.

), and so on until qθ(xN

|x1

Recurrent neural networks A general introduction to RNNs is provided in section 2.4.6 and
we refer the reader to that section for a more comprehensive description. RNNs are a special
type of AR models, among which one can ﬁnd a plethora of sub-models, such as the Long-Short
Term Memory (LSTM) and the Gated Recurrent Unit (GRU). The latter has been used in the
context of ground state search [226]. In the context of statistical mechanics, an RNN model
has been applied to difﬁcult problems such as spin glasses. These models are typically hard
to sample, therefore the direct sampling helped in reaching a high accuracy in a simulated
annealing task [234].

Normalizing ﬂows NFs are ML algorithms that learn complicated probability densities from
data or (when available) unnormalized probability densities (see Refs. [416, 425] for a re-
view). Recently, many works applying ﬂows to the ﬁeld of lattice physics have been pro-
posed [412, 413, 423]. A normalizing ﬂow (NF) is an algorithm that relies on the simple idea
of re-parametrizing the variable of a system using a change of coordinates transformation.
They work as follows: ﬁrst a latent random variable is sampled from a tractable, known dis-
tribution (e.g., a normal or uniform one). Successively, a bijective mapping under speciﬁc
constraints is learned. This map transforms the probability mass of the base distribution to-
wards a new one which approximates the target density we want to model. By leveraging
the change of variable formula, one can thus obtain a variational approximation of the target
density, properly normalized, and easy to handle. We refer to Fig. 7.5 for a sketch of an NF ar-
chitecture. In other words, what NFs do is to transform some Gaussian noise into real samples
from a learned bijective mapping.

In a normalizing ﬂow (NF), rather than mapping one density to another, we deﬁne
a diffeomorphic11 transport map that redistributes the probability mass from an easy-
to-treat base distribution to a more complicated target density.

11A function is diffeomorphic if it is differentiable and has a differentiable inverse.

178

Deep learning for quantum sciences – selected topics

Figure 7.5: Sketch of a normalizing ﬂow (NF) architecture. A sequence of bijective
transformations is combined in order to construct a complicated nonlinear invertible
transformation that describes the probability mass transport from a base distribution
toward a learned density that would, upon training, approximate the target density
of the problem at hand. The learned density is tractable and properly normalized,
which enables fast and efﬁcient sampling.

In the remainder of this section we dive into the mathematical intricacies of NFs. Let us
assume we have a set of unlabelled data {x} of which we want to learn the underlying density.
To make it practical, let us be more speciﬁc and assume to have a set of lattice conﬁgurations
(irrespective of the physical system). One may know that these conﬁgurations are distributed
according to a normalized density (e.g., a Boltzmann like distribution p(x) = Z −1 e−H[x])
although its normalization factor Z, known as the partition function, is generally not tractable
or easy to compute. Even knowing the unnormalized distribution ˜p = e−H[x] is typically
a hard task. Normalizing ﬂows (NFs) can overcome this issue by learning an efﬁcient
approximation of p that is properly normalized and fully tractable. Now that we have the
problem outlined, let us go back to our conﬁgurations. We deﬁne a latent variable z ∈ RD
distributed according to a known, easy-to-treat distribution. A common choice is a normal
Gaussian qz = N (0, 1) : RD → RD. The task of NFs is now to learn a bijective mapping
between the data space and the latent space that transforms the probability density of the
given base distribution N into a variational density qθ that should approximate the target
density p. The approximation ansatz qθ is parametrized by a set of parameters θ which are
optimized during training, see Fig. 7.5 for a visual representation of this scheme. What is left
now is to construct the bijective transformation and see how the parameters are encoded into
such a map. Let us deﬁne an invertible and differentiable transformation fθ : z → x = fθ(z)
with inverse f −1(x) = g(x) = z, i.e., a diffeomorphism.
In other words, learning this f
means, in practice, to redistribute the probability mass of some random signal into some
complex unknown target density.
In the context of generative models, one says that the
function f “pushes forward” the base density qz to a more complex density qθ.

To give an intuition for a one-dimensional problem, we start from the fact that the prob-
ability mass should be preserved through a change of coordinates for the random variable z.

179

x ~ qθz ~ qzzf1(z)fi (zi–1 )z1...x = fθ(z)Generative directionDensity  estimation directionzi - 1fi+1(zi )x...ziz = f-1(x)θfθ(z) = fk º …º fi º … º f1 (z)Deep learning for quantum sciences – selected topics

Therefore, one can write:

|px(x) dx| = |pz(z) dz| ,

(7.18)

where x and z represent the real samples and the Gaussian distributed random variables
respectively. It then follows that

px(x) = pz( f

−1(x))

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dz
dx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= pz( f

−1(x))

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d f (z)
dz

−1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(7.19)

where we used the deﬁnition of our bijective transformation x = f (z) and its inverse. More
speciﬁcally, for a bijection parametrized by θ, it follows that

qθ(x) = qz

(cid:0) f

−1
θ

(x)(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d fθ(z)
dz

−1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(7.20)

where fθ is invertible by construction and qz is a standard Gaussian representing our base dis-
tribution from which the latent noise z is sampled. By applying the change of variable theorem
in Eq. (7.18), one can obtain Eq. (7.20) with x ∼ qθ. The transportation of the probability
mass is encoded in the determinant of the Jacobian, last term of Eq. (7.20), which represents
the volume transformation, see Fig. 7.6 for a schematic representation. In order to make such
a learnable transformation more general and ﬂexible, one can write fθ as a composition of
many transformations (known as coupling blocks) all of which should be to invertible and dif-
ferentiable and parametrized with their own subset of parameters θi. The general composite
transformation then reads:

fθ(z) = ( fk

· · · ◦ fi

◦ · · · ◦ f1

) (z).

(7.21)

Once the mapping is learned, one can generate a new data point x∗ by sampling in the latent
space from the base distribution z ∼ qz and apply the bijection to obtain a corresponding
sample in the data space. Reversely, one can also take a sample, plug it into the inverse of f ,
being z = f −1(x), and thus obtain the probability density of the sample as a function of the
base distribution and the Jacobian of the transformation, which follows from Eq. (7.20).

In order to make the learning process smooth and efﬁcient, one additional requirement is
that the determinant of the Jacobian of such a transformation should be tractable and efﬁcient
to compute since it enters in Eq. (7.20) and is used to estimate the approximated density
of a given sample. We refer the reader to Ref. [416] for details about how to build these
transformations. By training our NF, we train the joint set of parameters for all the different
transformation (or coupling blocks) fθi
. This is usually done by minimizing the KL divergence.
(p||qθ) ≈ 0 ⇔ qθ ≈ p. The KL
In this case, we want to minimize this objective because DK L
divergence is deﬁned as

DK L

(p||qθ) =

(cid:90)

Ω

p(x) log

p(x)
qθ(x)

dx

x ∼ p(x),

(7.22)

where Ω represents the support of the target density. In other words, the KL divergence is the
expectation value of the log-ratio of the two densities with respect to the target probability p.
It is important to stress that the KL divergence is not symmetric and as such

DK L

(p||qθ) (cid:54)= DK L

(qθ||p)

(7.23)

where the ﬁrst term of the inequality is called forward KL and the right hand side is the reverse
KL. One has to be careful here as as the objective from Eq. (7.17) is different from Eq. (7.22).

180

Deep learning for quantum sciences – selected topics

Figure 7.6: The change of variables relies on the assumption that the total probability
should be preserved. As such, any arbitrary transformation should transform a base
distribution pz according to the determinant of the Jacobian which is responsible
for redistributing the probability mass from a simple base Gaussian density into a
non-trivial target probability distribution. In the one dimensional case, the red lines
identify the probability area in px and map it back to the corresponding area of the
base density pz through the bijection fθ. If the determinant of the Jacobian is one,
as in this speciﬁc case where it reduces to a one-dimensional ﬁrst-order derivative,
the volume of the transformation is preserved.

As a matter of fact, when samples are available for training the ﬂow by maximum likelihood,
one uses the forward KL, hence Eq. (7.22). In this case, samples come from the true distribution
and do not depend on the parameters θ of the ﬂow. Thus, one rewrites the objective so that
it only depends on the variational approximation qθ, because nor the target density p(x) nor
the samples x in Eq. (7.22) depend on the parameters we seek to minimize. As such, when
taking the gradient of this objective, the log p(x) term from Eq. (7.22) is going to vanish.
Given a set of unlabelled data D = {xi
}n
i=1 observed from an arbitrary distribution, we can
perform likelihood-based estimation of the parameters. The data log-likelihood in this case,
for a set of n observations, simply reads

log p(D|θ) =

n
(cid:88)

i=1

log qθ (xi

|θ).

(7.24)

One then optimizes this objective with respect to the set of θ such that qθ matches as close
as possible the target density p. Alternatively, minimizing the objective from Eq. (7.22) can
be translated into a maximization problem in Eq. (7.24). However, as mentioned before, for
physical systems it is often the case that an unnormalized target density is available. Under
this circumstance, one can train a NF by means of the so-called reverse KL divergence from
Eq. (7.17) [413]. Within this framework, one usually talks about energy based model, and

181

px(x)pz(z)zzxx x = fθ(z)Deep learning for quantum sciences – selected topics

during optimization new conﬁgurations are directly sampled from the variational density qθ
we seek to optimize. More speciﬁcally, one can expand the reverse KL objective as

DK L

(qθ||p) =

=

(cid:90)

Ω

(cid:90)

Ω

qθ(x) ln

(cid:139)

(cid:129) qθ(x)
p(x)

dx

qθ(x) (log qθ(x) + H(x)) + log Z dx,

(7.25)

where in the last step we have explicitly rewritten the objective, leveraging the fact that the
target density is known to be Boltzmann-like. From Eq. (7.25) it is clear that the unknown
normalization constant Z now becomes just a constant shift and does not play any role in the
optimization, because it vanishes when the gradient is computed. As such, the minimization
problem essentially relies on the analytical form of qθ, accessible by means of NFs, and reads

DK L

(qθ||p) =

(cid:90)

Ω

H( fθ(z)) − log

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d fθ
dz

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(z) + log qz(z)

(7.26)

where we used the deﬁnition of the target density from Eq. (7.16). The interested reader can
look for more details on how to train a NF in Ref. [413] and also an ARNN in the case of
discrete random variables in Ref. [80].

Let us now go back to the practical problem as stated above. Once we have a trained model
qθ, good enough to approximate the target p, one goal of density estimation is to generate new
samples for a physical system, e.g., sampling new, preferably uncorrelated, lattice conﬁgura-
tions. As mentioned above, one way this is usually done is by means of MCMC. However,
these chains have some pathological behavior in the proximity of a phase transitions because
the samples generated by such chains are sequential by construction and may become highly
correlated. As such, the generative process by means of MCMC shows exploding autocorrela-
tion times [426, 427]. To phrase it differently, the samples obtained by MCMC when a phase
transition is involved may not be that reliable. One should now be able to foresee where the
advantages of NFs come into play. Under certain circumstances, not difﬁcult to be realized
in practice, a ﬂow trained in a particular point in parameter space is completely agnostic to
the region of the phase space at which it is sampling. As such, when sampling from a trained
ﬂow, no matter where, one always efﬁciently samples highly decorrelated conﬁgurations for
the system at hand. For a more detailed yet mathematical description of how ﬂows work in
practice, the reader is once again referred to the review paper in Ref. [416]. Another example
of how to apply NFs in the domain of quantum ﬁeld theory is presented in Ref. [413]. In this
application, no data is required. Instead, the NF is trained by optimizing the reverse KL di-
vergence (see Eq. (7.17)). Leveraging the fact that one knows the Hamiltonian of the system
to be investigated, one knows the shape of the unnormalized underlying Boltzmann density.
Therefore, the ﬂow model can be trained like an energy based model as mentioned above. We
refer to Ref. [413] for more details on this approach.

Outlook and open problems

As stated above, in the recent years NFs have become the most promising tool to address the
problem of density estimation in physical sciences. However, many challenges still need to be
faced. First and foremost, volume scaling is currently the main issue. All the promising results
achieved so far by deploying the technique in the context of lattice ﬁeld theory, for instance,
have been obtained only on relative small lattices. One would expect that approaching the
continuum limit, hence scaling the volume of a lattice up to a reasonable shape, would have
a serious impact on the ﬂow performance. One way to tackle this problem would be to exploit

182

Deep learning for quantum sciences – selected topics

our physical knowledge by incorporating existing symmetries into the ﬂow, so that the network
does not have to learn the physics from scratch. Refs. [428, 429] give a nice overview on how
this can be achieved. Another relevant challenge brieﬂy mentioned above is the problem of
mode collapse. This has been analyzed in Refs. [414, 415] and is currently an open problem
not only in the domain of NFs in physics, but also in the entire ML community [430]. Having
a multi-modal distribution to model makes the learning problem much more complex, because
the algorithm may perfectly cover one mode of such a distribution but completely neglect the
others and get stuck. As such, when sampling from the learned variational density, we would
never cover the full statistics of the underlying density. This is a relevant problem, especially in
this context, as it may be hard to detect in some scenarios and be very harmful when accurate
estimates of observables are of interest.

Further reading

• Wang, L. (2018). Generative models for physicists [431].

• Noe, F. et al. (2019). Boltzmann generators. Science, 365, eaaw1147. Application of

normalizing ﬂows (NFs) in the context of quantum chemistry [432].

• Nicoli, K. A. et al. (2021). Machine learning of thermodynamic observables in the presence
of mode collapse. arXiv:2111.11303 [415]. Hackett, D. et al. (2021). Flow-based sam-
pling for multimodal distributions. arXiv:2107.00734 [414]. Overviews on the problem
of mode collapse.

• Köhler, J. et al. (2020). Equivariant ﬂows. arXiv:2006.02425 [428], Satorras, V. G. et
al. (2022). E(n) equivariant normalizing ﬂows. arXiv:2105.09016 [429]. Incorporating
symmetries into normalizing ﬂows (NFs).

• Albergo, M. S. et al. (2021). Flow-based sampling for fermionic lattice ﬁeld theories.
Phys. Rev. D, 104, 114507. Using normalizing ﬂows (NFs) in the context of fermion
densities [433].

7.3 Machine learning for experimental data

Performing quantum experiments poses tough technical challenges and the task of optimiz-
ing their performance while interpreting the output data can seem daunting.
It is infor-
mative to note that the output of quantum devices naturally generates large-scale data in
whose regime ML thrives. In chapter 3, we have already seen how ML can be used to detect
phases, and while such efforts are much more challenging when dealing with experimental
data, a few works managed to successfully address this real-world problem [97, 434, 435].
In section 4.5, we have discussed the applications of Gaussian processes (GPs) and Bayesian
optimization (BO) for inverse problems involving experimental data [164, 166, 167] and op-
timizing experiments [168, 176–178, 185–189]. In section 5.3.7 we have also demonstrated
how ML can boost quantum tomography [82,271,273,274] with NQSs presented in chapter 5.
Another research direction pursued in the context of quantum experiments is the application
of RL for quantum feedback control [331–334], quantum error correction [341–344], quan-
tum circuit optimization [336], and experiment design [318], all described in sections 6.6.3
to 6.6.6.

This section focuses on other ML approaches for experimental data. Firstly, we acknowl-
edge that there is an important niche of experimental physics that can be revolutionized by
ML, i.e., automation of (tedious) repetitive tasks. We show examples of successful realiza-
tions of this idea with actual experimental data in section 7.3.1. In section 7.3.2 we discuss

183

Deep learning for quantum sciences – selected topics

Figure 7.7: Experimental nanoﬂake setup. (a) A typical microscope image of hBN
from Ref. [436]. (b) Typical microscope setup with waver already placed under the
microscope. Photo credit: Klaus Ensslin Lab, ETH Zürich.

the theoretical proposal of ML-based analysis of time-of-ﬂight images which pose the standard
measurement technique in ultracold-atom setups. Then, in section 7.3.3, we describe a power-
ful scheme for quantum experiments, i.e., learning the Hamiltonian governing the system. We
conclude this section with section 7.3.4 discussing successes of the computer-guided design of
experiments that does not include RL.

7.3.1 Automation of experimental setups

This section is devoted to novel ideas for automation of physical experiments. Speciﬁcally, we
present the automated identiﬁcation of nanomaterial samples for quantum device technolo-
gies [436] and the automated tuning of double quantum dots [437] for quantum information
devices. Both of these examples have one thing in common: at some point in the execution of
their respective experiments, a large amount of human labor becomes necessary that is tedious
and repetitive with respect to the decision making process a worker employs but not trivial
enough to be replaced by a simple looping algorithm.

Automated identiﬁcation of nanomaterials In the case of the preparation of nanomaterials
for quantum devices as detailed in Ref. [436], an important step is the selection of appropri-
ate two-dimensional ﬂakes from a wafer under a microscope. The ﬂakes in question can differ
depending on their desired use in the ﬁnal device, but they all share their ﬂat shape and approx-
imate size due to the exfoliation-based technique with which they are prepared beforehand.
Examples include hexagonal boron-nitride (hBN), graphite, and bilayer-graphene. Figure 7.7
depicts the scanning setup along with a typical image for hBN. The microscope in a typical
setup can operate at different magniﬁcations and can scan a 1 cm2 wafer in roughly three
minutes. In practice, however, this takes much longer as the human operator has to slowly
move the frame across the wafer and decide for each frame which of the depicted ﬂakes are
suitable for future device building. In short, the human operator classiﬁes the ﬂakes; a well-
trained NN could do this as well. Hence, the design and training of a suitable NN architecture
was at the core of the automation scheme developed in Ref. [436]. However, it is worth not-
ing that the automation scheme did more than just the classiﬁcation task as is summarized in
Fig. 7.8. For example, prior to even implementing anything network related, they provided
the experimental team with a program with a simple GUI for click-based ﬂake labeling of pre-
processed images to simplify the generation of an adequate data set. This is pointed out here
to truly reﬂect the additional steps that need to be taken into consideration when working
with experimental setups.

184

Deep learning for quantum sciences – selected topics

Figure 7.8: The full automation procedure consisted of scanning, labeling, pre-
processing, training, classifying, and then collecting and presenting the good ﬂakes.
The steps marked with italics were done only prior to application. Figure credit:
QMAI group at TU Delft.

In this work more than one network was used to minimize classiﬁcation errors: Three
networks were used and applied consecutively, each consisting of four convolutional layers
and one dense layer. The reason this stacking of networks was necessary is the immense
imbalance between good and bad ﬂakes in the data set. While a batch optimization procedure
paired with data augmentation can usually account for this to some extent, here this was not
sufﬁcient: after passing new data that contained approximately 1000 ﬂakes, 10.8% of which
were good, through only one trained network, the classiﬁed results yielded the 86% accuracy
with 13% of false positives (bad ﬂakes classiﬁed falsely as good) and 1% of false negatives
(good ﬂakes classiﬁed falsely as bad). Since there are so few truly good ﬂakes, avoiding false
negatives is of utmost importance.

An important and more practical aspect of this number and accuracy of leftover “good”
(correctly or incorrectly classiﬁed) ﬂakes is concerned with the additional human labour that
would follow this classiﬁcation result: after classifying, the automation scheme (compare
Fig. 7.8) goes on to automatically zoom in on the good ﬂakes after which a human opera-
tor steps in again. More speciﬁcally, this means that instead of manually scanning the probe,
looking at the, e.g., 1000 ﬂakes available and zooming in on the selected ﬂakes in order to
then decide whether they are good or not, in the automated scheme the human operator only
acts after the entire wafer is scanned and the microscope is shifted to the different locations
of the good ﬂakes on which it appropriately zooms in. In this way, the experimentalist only
makes the ﬁnal decision on which ﬂakes to use; this represents a substantial workload reduc-
tion from 944 ﬂakes to 224, approximately 44% of which are actually good. However, since
there is still work involved, three instead of one networks are used to reduce the amount of
ﬂakes that need to be looked at even further. After passing the data through all three networks
only 150 ﬂakes need to be looked at, approximately 57% of which are good.

One of the reasons why the accuracy is still comparatively low is due to the discrepancies
of classiﬁcation choices among the humans who did the labeling beforehand. Given that it
was already time consuming enough to label a large enough data set even with the helpful
GUI program written for that purpose, every ﬂake was only labeled once by one of the many
experimentalists taking part in this project. Figure 7.9 captures the differences in judgement
of three different participants all looking at a selection of single shot frames. It is important
to note that while here, we were able to explicitly show one source of uncertainty and errors

185

Deep learning for quantum sciences – selected topics

Figure 7.9: Differences of human judgement visualized. Selection of 15 frames that
were to be judged on whether they contained good ﬂakes or not along with the
judgement results from three different human operators. Adapted from Ref. [436].

in preparation of the data, usually we do not see it directly. We should always assume pos-
sibility of their existence! Evidently, there is a degree of uncertainty and disagreement about
whether the individual frames contain good ﬂakes which introduced a bound on the model
performance. This is reﬂected by the classiﬁcation accuracy.

All in all, the developed automation procedure including and revolving around the NN is
still clearly a success: this type of material control is a common step in the ﬁeld of nanomaterial
device development and the method generalizes satisfyingly to, e.g., graphite and bi-layer
graphene (compare Ref. [436]). The implementation is available on GitHub [438].

Quantum dot tuning The next example is the automated tuning of double quantum dots in
quantum information technology research. A detailed discussion can be found in Ref. [437].
A quantum dot is a nanostructure that is conﬁned so heavily in all three spatial dimensions
that it is essentially zero dimensional. The conﬁnement, similar to a particle-in-a-box scenario,
leads to the emergence of quantum effects, i.e., energy quantization (as opposed to having
a continuous energy spectrum in larger structures) and thus discrete states.

In quantum information research, quantum dots are used to create qubits by putting to-
gether two dots as discrete states. There are various reasons why this technology is challenging
in the context of universal quantum computation: on the one hand, there are difﬁculties asso-
ciated with making two dots interact in a controlled manner and, on the other hand, there are
issues associated to reproducibility. Both concerns are related to the preparation techniques
and the example discussed here offers a new ML-based remedy for the former.

Like in the previous example, the experimental procedure contains a tedious step that one
can seek to automate. While in the ﬂake example this step revolved around human operators
looking at images from a probe under a microscope, in this quantum dot setup the human op-
erator looks at graphs created from changes in current measurements in the quantum device
with changing applied voltages. When conducting a measurement like this, the quantity of
interest is the occupation of the quantum dots, i.e., the state of the quantum dot. The occupa-
tion can be changed by applying a voltage to the dot: as seen in Fig. 7.10(a), every one of the
three quantum dots has a plunger gate (PG) associated with it that is used to tune the voltage.

186

Deep learning for quantum sciences – selected topics

Figure 7.10: Experimental setup for quantum dots. The device is built and then
tuned using measurements that are made possible by the quantum point contact
built into it. (a) Scanning microscope image of an example device. Base material
is a GaAs heterostructure with an electron gas embedded at the position where the
three quantum dots (QD1, QD2, QD3) are intended to be. It also contains a number
of ﬁnger gates for conﬁnement and measurement. The gates responsible for the
measurement are the three at the bottom (quantum point contact). (b) The charge-
stability diagram for a double quantum dot device that uses QD1 and QD2 in (a).
Correspondingly, the changed voltages are the ones for PG1 (corresponds to QD1)
and PG2 (corresponds to QD2). Taken from Ref. [437].

Underneath these dots, the current of a quantum point contact is measured.12 As deﬁned in
Coulomb’s law, the occupation of the dots, i.e., the negative charge of the respective electrons,
affects the electric current close to it. Hence, a change in the electron occupation causes a dis-
/∂ VPG)
crete change in the current ﬂow which corresponds to spikes in the conductance (∂ IQPC
measurement. Those spikes are the dark blue lines in the charge-stability diagram showcased
in Fig. 7.10(b). Figure 7.10(b) can then be interpreted as follows. At the bottom left corner,
both quantum dots (QDs) are unoccupied but whenever a vertical (horizontal) line is crossed,
an electron is added to QD1 (QD2).

The goal in the device preparation and tuning is to prepare different discrete states by
applying the adequate voltages that corresponds to the correct current-spike-line framed, dia-
mond shaped area in the charge-stability diagram. To do this, the operator needs the knowl-
edge of the charge-stability diagram. Thus, tuning a double quantum dot device, such as this
one, requires measuring the entire charge-stability diagram, i.e., performing many subsequent
measurements where one voltage is kept constant and the other one is gradually changed. In
this very time-consuming scenario a classiﬁcation-based ML scheme can be of help. For ML
to bring a signiﬁcant improvement, it is essential that measuring the entire charge-stability
diagram is not required for the input data. A suitable scheme should be able to produce the
two plunger gate voltages for a speciﬁed desired occupation state from any starting state (cor-
responding to a starting pair of voltages along with their current ﬂow). However, without the
charge-stability diagram, there is no way of knowing which occupational state two starting
voltages correspond to. For example, if the starting state had both voltages at 0 V , then a hu-
man operator with knowledge of Fig. 7.10(b) would know that this places the state somewhere

12The quantum point contact in Fig. 7.10(a) is formed by the three gates at the bottom responsible for the

measurement.

187

Deep learning for quantum sciences – selected topics

Figure 7.11: (a) Finding the (0,0) state using a ﬁrst classiﬁcation network: Starting
In this example the network
at random initial voltages corresponds to point “1”.
had to be run through three iterations of classiﬁcation and frame shifting until (0,0),
here marked by “3”, was reached. (b) Finding the state of desired occupation (m,n):
Depending on the different classiﬁcation results the frame can be shifted either only
to higher PG1 voltages (in case of one vertical line), only to higher PG2 voltages (in
case of one horizontal line) or diagonally, increasing both gate voltages (in case of
two lines). Adapted from Ref. [437].

in the top right of the diagram and would, by means of counting lines, be able to specify the
state.

To avoid having to measure the whole diagram, the ML scheme uses an approach for which

small, low-resolution excerpts of the diagram sufﬁce:

1. Finding the (0,0) state: In a ﬁrst step, one utilizes the fact that any state except the (0,0)
state is framed by four lines in the diagram, whereas the (0,0) state only has neighboring
lines in the positive x- and y-direction. Therefore, a ﬁrst classiﬁcation network is trained
to recognize whether there are more lines to cross in the negative x- or y-direction. The
output is either true or false. If there are more lines, both plunger voltages are lowered
by a set amount (as depicted in Fig. 7.11(a)) and the classiﬁcation is performed again
until there are no more line to be crossed.

2. Finding any desired, given state: To get from the (0,0) state to any desired state (m,n),
one has to cross exactly m vertical lines and n horizontal lines. Thus, a second network is
now trained to more accurately classify which lines there are. This network uses smaller
frames of a higher resolution that allow a more differentiated distinction between the
cases of there being no lines, there being one vertical line, there being a horizontal line
and there being both in the considered frame (compare Fig. 7.11(b)). Just like in the
ﬁrst step, each classiﬁcation is followed by a change in voltages and this 2-step procedure
is repeated until the desired state is reached.

For the training of the ﬁrst network, 470 charge stability diagrams were measured in fairly low
resolution, whereas for the second network 128 charge stability diagrams were measured in
higher resolution. The plunger gate voltage ranges were varied for the different measurements
to foster better generalization later on. In both cases, data sets were created by cutting out
numerous random frames from the diagrams and labeling them with a script. Note that while
full charge stability diagrams were measured for the generation of the training data set, the

188

Deep learning for quantum sciences – selected topics

input for the eventual application of the network only needs the small windows. Measuring
full charge stability diagrams and using many windows therein was just a convenient way to
create a data set.

When testing on the actual device, the success rates of the two loops were 90% (step 1
loop) and 63% (step 2 loop) which combines to an overall success rate of 57%. It is important
to keep in mind that those individual success rates are not the equivalents of the accuracy
rates of the two networks: each loop calls the network multiple times, so errors are doomed
to accumulate and the second loop usually requires more calls to the network than the ﬁrst
loop, because the frames are smaller, see panels (a) and (b) of Fig. 7.11. In fact, when tested
separately and only a single time on a labeled data set, the accuracy rates reached by the two
networks were 98.9% and 96%. In the article, the authors stated that the primary error source
was identiﬁed as a weak signal-to-noise ratio and improving on this would surely improve the
scheme.

In conclusion, the integration of DNNs into a larger scheme can lead to the accumulation
of errors and this needs to be taken into consideration when planning the implementation of
the automation routine. In general, the integration of ML approaches into broader automation
schemes call for different levels of network accuracy and, as it was the case in the ﬁrst example,
some scenarios might even have limited network accuracy overall. It is important to take these
things into account ahead of the implementation and gauge the beneﬁts of automation versus
the remaining workload.

Of course, the presented approaches are not the only ideas for automation of experimental
setups. For example, CNNs also help in detecting the contamination with diffraction from ice
crystals in the macromolecular diffraction data [439] or analyzing cryo-electron microscopy
maps of proteins [440].

7.3.2 Machine-learning analysis of time-of-ﬂight images

When it comes to analysis of experimental data, we present one more example related to
ultracold-atom experiments. In contrast to the two highly specialized applications to actual
experimental data discussed so far in section 7.3.1, we consider a proposal that is based on
theoretical data but is readily extendable to experiments [441]. There is a number of theory-
based, yet application-oriented proposals which are currently being published and discussing
their differences should prove insightful. The focus of this discussion is to be put on the feasi-
bility of making the transition from theory to application.

For any such transition from theory to experiment within an ML scope, the following as-

pects should be examined:

• Speciﬁcity vs. ﬂexibility of the method: as was discussed in the two earlier examples,
when the ML model does not provide truly new insights into the physics of the model,
the automation should instead yield a signiﬁcant reduction of human labour. This can
either be achieved by designing a speciﬁc scheme for one scenario that requires a large
expenditure of work or by designing a ﬂexible scheme for a large number of scenarios
of medium expenditure.

• Similarity of theoretical and experimental results: more often than not, theoretical mod-
els produce results that diverge quite signiﬁcantly from their experimental counterparts.
This can be due to experimental noise or limitations in the theoretical model. For a model
that has been trained on theoretical results only, it is important to evaluate whether fur-
ther pre-processing, such as the inclusion of artiﬁcial noise could be sufﬁcient to prepare

189

Deep learning for quantum sciences – selected topics

Figure 7.12: Implementing ML into the extraction of observables from measurements
of ultracold atoms: Conventional experimental approaches are usually only able
to extract the ﬁrst and second order density matrix through averaging approaches,
whereas the ML approach also extracts real space observables from momentum space
measurements as well as correlation functions. Taken from Ref [441].

the network architecture for an input of experimental data and/or how much the net-
work needs to be retrained.

Unlike the work done with quantum dots and ﬂakes that utilized speciﬁc schemes with high
impact, the scheme proposed now is very general and proﬁts from the ﬂexibility of the probed
system: ultracold atoms. Due to the high level of control available in such setups, ultracold
atoms represent an exemplary quantum simulator for a large variety of few to many-body
physics phenomena. It is worth noting that independently of whether the considered exper-
imental effect is a dynamic transition from superﬂuid to Mott-insulating states, the quanti-
zation of conductance through a quantum point contact, or simply the many-body nature of
condensed versus fragmented states in a double-well potential, the standard output of exper-
iments remains similar. It is namely a time-of-ﬂight image.

In an experimental setup, an initially trapped cloud of ultracold atoms is let expand and
time-of-ﬂight imaging captures snapshots of the cloud. These single shots carry an amount of
information as they can unambiguously be linked to a large variety of physical quantities and
phases. While experimentalists can usually only extract a few observables through averaging
techniques, it is shown that an ML tool should be able to harness the information contained in
the data more accurately and access a larger selection of observables (compare Fig. 7.12). The
ANN-based approach proposed by the authors exploits the shot-to-shot ﬂuctuations in order
to implicitly reconstruct the many-body state. This is promising for widespread application in
experimental realizations.

When it comes to compare theoretical predictions and experimental results, noise becomes
an important factor. In ultracold atoms, Lode et al. (Fig. 7.12) proposed a method for an op-
timized observable readout from single-shot images of ultracold atoms, arguing that the simi-
larity of theoretically simulated and experimentally detected single-shots is good enough that
the addition of artiﬁcial Gaussian noise to the theoretical data during training should sufﬁce.
Figure 7.13 shows a comparison of simulated and experimental single-shots of ultracold atoms
at different points of a phase transition in an optical cavity upon increase of one of the exter-
nal laser intensities. While this example comes from a different framework, both publications
used the same simulation method for the single-shot generation [442,443]. Noise is evidently

190

2...1@256x10016@256x10016@256x216@249x24@242x24@121x116@256x10016@2562x216@249x24@242x24@121x11@256x10016@256x10016@2562x216@249x24@242x24@121x11x401@256x10016@256x10016@2562x216@249x24@242x............TNT Library...gN bosonshσαANNExperimentsSingle ShotsMachine LearningObservablesSimulations...gN bosonshσα...UNIQORNFigure1.Machinelearningobservablesfromsingle-shotimagesofultracoldatomicsystems.Single-shotimagesaremeasuredexperimentallyorsimulatednumericallyusingMCTDH-XorotheralgorithmsandusedasinputdatainanMLtask.Otheroptionsforwavefunction-basedapproachesthatallowthesimulationofsingle-shotmeasurementsincludeITensor[62],theTensorNetworkTheory(labelTNT)Li-brary[63],ortheopenMPSsoftware[64,65].TheANNsaretrainedonthesimulatedsingle-shotdatasetobtainedhereforanN-bosontilteddouble-wellsystemtooutputmultipleobservables.Thequantities(g,h,σ,α,N)characterizingthisdoublewellarerandomized.ablescanbefoundfromreal-spacesingle-shotdataandviceversa.TheANNinusehasafewconvolutionalanddensehiddenlayers,andisdescribedindetailinSec.S1ofSupplementaryInformation(SI)[66].Theconstruc-tion,training,andevaluationofANNmodelsisimple-mentedusingTensorﬂow[67,68]andintegratedinaﬂexi-bleopen-sourcetoolkit:theUniversalNeural-networkIn-terfaceforQuantumObservableReadoutfromN-bodywavefunctions,forshort,UNIQORN[69].WeaimtodemonstratethepotentialofourMLalgo-rithmonaphysicalsystemthatconsistsofNbosonsinaone-dimensionaltilteddouble-wellpotentialdescribedbytheHamiltonianH=ZdxˆΨ†(x)[T(x)+V(x)]ˆΨ(x)+12Zdxdx0ˆΨ†(x)ˆΨ†(x0)W(x,x0)ˆΨ(x0)ˆΨ(x),(1)whereˆΨ(x),ˆΨ†(x)arebosonicﬁeldoperators.Theone-bodypartoftheHamiltoniancontainsakineticenergytermT(x)=−12∂2x,atiltedone-bodydouble-wellpo-tentialV(x)=12x2+hexp(cid:16)−x2σ2(cid:17)+αx,modeledasacombinationofanexternalharmonicconﬁnement,acen-tralGaussianbarrierofheighthandwidthσ,andatiltofslopeα.Theparticlesinteractviaatwo-bodycon-tactrepulsionW(x,x0)=gδ(x−x0),g>0[Fig.1b)].AllunitsaregivenintermsofthenaturallengthscaleL=p~/mωandenergyscaleE=~ω,wheremisthemassoftheparticlesandωistheexternalharmonictrap-pingfrequency.Thetilteddouble-wellsystemcanbeviewedasamin-imalimplementationofaquantumsimulatorofthetwo-siteHubbardmodel[7,70],whentheshapeofthestatewithineachwellisignored.Thetwolatticesitesarethengivenbythetwominimaoftheone-bodypotential,thehoppingstrengthiscontrolledbythebarrierheight,andtheon-siteinteractionandenergyoﬀsetarederivedfromtheinteractionstrengthandthetilt,respectively.Despitetheirapparentsimplicity,doublewellsfeatureawealthofmany-bodyproperties:correlations[22,71,72],numberﬂuctuations[73],self-trappingandequilibrationdynamics[74]andareofcontemporaryexperimentalin-terest[22,23,75,76].Therefore,doublewellsprovidetheperfectarenatoimplementandbenchmarkourANNapproachtoextractobservables.Asthedataset[77]forourMLtasks,wecompute3000ground-statewavefunctionsofthedouble-wellsys-temdescribedabove.Eachindividualwavefunctioncor-respondstoadouble-wellsystemwhosesystemparam-etersbarrierheighth∈[5,25],barrierwidthσ∈[0,3],interactionsg∈[0.01,0.2],tiltα∈[0,0.5],andparti-clenumberN∈[10,100]aregeneratedrandomlyac-cordingtoauniformdistributionwithinthecorrespond-ingintervals.Therangesfortherandomparametersaretunedsuchthattheobtainedgroundstatesspanarangeofphysicalphenomenathatareexpectedforin-teractingultracoldbosonsinthedouble-wellpotentials.Ourdatasetthuscontainswavefunctionsofbothcon-densedandfragmentedsystemswherethereducedone-bodydensitymatrixhasasingle[78]orseveral[79,80]macroscopic[O(N)]eigenvalues,respectively.ForeachwavefunctionΨ(x1,...,xN)inourdatasetofsolutionsoftheSchr¨odingerequationwiththeHamil-tonian(1),wegenerate1000single-shotimagesastheinputdataforourMLtasks.Thesesimulatedsingle-shotimagesarerandomsamples(˜x1,...,˜xN)thataredrawnfromtheN-particledensityP(x1,...,xN)=|Ψ(x1,...,xN)|2,seeRefs.[19,81].AsthelabelsforoursupervisedMLregressiontasks,wecomputetheone-bodyandtwo-bodydensities(1BDsand2BDs,respec-tively)thatwewanttoinferfromthesingle-shotin-putdata.Thisinputdataanditslabelsarecomputedinrealandinmomentumspaceandformourlabeleddataset[77].Forconvenience,wediscussthereal-spacedatainthefollowing;thediscussionisidenticalforthemomentum-space-case,replacingx→k.Sincewehavearangeofdiﬀerentparticlenumbers,theshot-to-shotﬂuc-tuationsinthesingle-shotimagesinourdatasetstemfrom,both,theﬁniteparticlenumberandquantumﬂuc-tuations[27,82].Hence,individualsingle-shotimagesinourdatasetmaybeverydiﬀerentfromtheirdensitylabels.Thisdeviationofsingle-shotimagesfromtheden-sityisparticularlypronouncedforsmallparticlenumbersand/orstrongquantumcorrelations[19,81].TosolvetheN-bosonSchr¨odingerequationforthegroundstateandtosimulatethesystem’sdetectioninsingle-shotimages[25–27,81,82],weusethemulticon-ﬁgurationaltime-dependentHartreemethodforindistin-guishableparticles(MCTDH-X)[25,83–89].MCTDH-XDeep learning for quantum sciences – selected topics

Figure 7.13: Comparison of experimentally measured (upper row) and simulated
(bottom row) momentum space density distributions of a system undergoing a phase
transition from a superﬂuid self-organized phase (State 1) to a superﬂuid Mott insu-
lating phase (State 6). Adapted from Ref [442].

present but the agreement is satisfactory for the different stages of the phase transition. An al-
ternative to adding artiﬁcial noise to the theoretical data is attempting to subtract noise from
experimental data, e.g., by means of denoising autoencoders. Which option is eventually cho-
sen naturally depends on the given experimental and theoretical data. In the case of single shot
images, denoising methods may not be ideal owing to the presence of quantum noise, inherent
in many-body systems, which is difﬁcult to discern from other noise sources and therefore to
selectively remove.

Overall, we have seen that ML techniques can contribute in bridging the gaps between
theoretical models and noisy or resource-constrained experimental realizations and measure-
ments. These ﬁndings represent a solid groundwork demonstrating experimental quantum
physics enhancement via ML and indicate a promising avenue toward the hybridization of ML
and the quantum realm in the coming years.

7.3.3 Hamiltonian learning

The focus of this section is the veriﬁcation of quantum simulators such as trapped ions, Ryd-
berg atoms, superconducting qubits, or ultracold atoms in optical lattices [444–447].13 These
experimental setups are well understood and can be used to simulate more complex and chal-
lenging systems governed by the same Hamiltonians. We enter exciting times when quantum
simulators start to be very complex and, in particular, not solvable with classical computers.
For example, when working with quantum simulators with 50 qubits we have to deal with
enormous Hilbert spaces of the order of 1015. Therefore, the question arises how can we
know that these simulators are working as they should be if we can not verify their results
with classical computers? One possible solution to this problem is called Hamiltonian learning
which is the main topic of this section. In particular, we discuss here the approach presented
in Ref. [448].

13Different experimental setups have different advantages and disadvantages for speciﬁc quantum simulation
problems and Hamiltonians. A difference between quantum simulation and quantum computation is that quantum
simulators are engineered for speciﬁc problems and quantum computing is more versatile and able to solve general
problems.

191

Deep learning for quantum sciences – selected topics

Figure 7.14: Illustration of the procedure of Hamiltonian learning of a one-spin sys-
tem. The spin is prepared in an initial state and driven by an unknown Hamiltonian.
The rotation frequency can be learned from experimental measurements.

The main idea of Hamiltonian learning is to reconstruct the map from experimentally
accessible measurements to the parameters of the underlying Hamiltonian.

The approach discussed in this section employs NNs to extract parameters governing the
created quantum simulator. The procedure is the following. Quantum simulators create exper-
imentally accessible data (e.g., real space images) for the corresponding Hamiltonian whose
parameter are known. Then, the NNs are trained via supervised learning to predict the pa-
rameters of these Hamiltonians. It is also possible to reverse the procedure: given the deﬁning
parameters of the Hamiltonian of a quantum system, relevant characteristics of the system can
be efﬁciently learned by an NN [449].

A very simple example to illustrate the process of Hamiltonian learning with NNs is a single
spin system as shown in Fig. 7.14. Firstly, we prepare an initial state of a known Hamiltonian,
ˆH0. In this case, it is an eigenstate of σ
z (spin “up”). Secondly, we perform a unitary evolution
under an unknown Hamiltonian, ˆH1, which leads to a precession of the spin around the axis of
the Bloch sphere. We now want to learn from measurements the unknown ˆH1, i.e., how fast
the spin precesses around the sphere. In this case, a sequence of measurements is required
to obtain the oscillation frequency, ω. This procedure can be generalized to arbitrary known
initial ˆH0 and unknown ˆH1, driving the unitary evolution of the system.

Now, we focus on another experimental setup of a quantum simulator consisting of neutral
atoms in a harmonic potential in a system of 2×50 lattice sites. The initial states of this system
are the positions of the atoms in the optical lattice and this experimental setup can be described
by the Bose-Hubbard Hamiltonian

ˆHBH

= −(cid:88)
〈i, j〉

Ji, j ˆa†

i ˆa j

+ (cid:88)
i

Ui
2

ˆa†
i ˆai

(ˆa†

i ˆai

− 1) − (cid:88)

µ

i ˆa†

i ˆai

i

(7.27)

where Ji, j describes the hopping between lattice sites i and j, Ui the onsite energies, and µ
i is
the chemical potential of the atoms in the optical lattice. If we consider only ten particles in this

192

Initial stateUnitaryevolutionMeasurement0101p=0.4p=0.6trivial HamilitonianunknownHamilitonianU= exp (- iωσyt)Deep learning for quantum sciences – selected topics

Figure 7.15: (a) Illustration of a four atom system with 25 parameters. (b) Mapping
from the experimental snapshots to the Hamiltonian parameters using one NN and
supervised learning. (c) Extension to larger system sizes by “dividing” the lattice
into subsystems with walls. (d) More efﬁcient way to map measurements to the
parameters. Adapted from Ref. [448]. Additional credit: QMAI group at TU Delft.

lattice, the corresponding Hilbert space is of dimension 1013 with 350 parameters to estimate.
This leads to two main issues: ﬁrst, the wave function is too large making it impossible to sim-
ulate this system, and, second, it leads to a 350-dimensional optimization problem. Therefore,
let us ﬁrst consider a small system consisting of 4 atoms as illustrated in Fig. 7.15(a), which
reduces the number of parameters to 25 and the Hilbert space size to 330. This eliminates the
problem of the large Hilbert space and leaves us with the optimization problem.

Now, we want to create a mapping from the measurements to the parameters of the Hamil-
tonian Eq. (7.27).14 To do so, supervised learning is used to train an NN and perform regres-
sion. The challenge in this setup is the scaling of the training data with the output size. To
train a single NN to predict all parameters, as shown in Fig. 7.15(b), several examples for
all combinations of the 25 parameters are required which is unfeasible for most applications
due to the enormous size of the required training set. The solution to this problem is quite
simple: instead of using a single NN to predict all parameters, 25 NNs are trained to predict
each parameter separately with continuous regression.

Moreover, the experimental snapshots may not be the best representation of the data set.
A more effective representation is to switch from experimental snapshot batches to the corre-
lators of the speciﬁc Hamiltonian. In this example, density correlators are used which enable
a way more efﬁcient way to train the NN by reducing the input dimension. This approach is
shown in Fig. 7.15(d). After successful training, the NN achieves around 0.1% error rates for
experimental parameters with 2500 snapshots. Using Bayesian inference as a benchmark, the
NN approach outperforms the Bayesian results for small data sets of 2500 snapshots. How-
ever, for large data sets of about 20 000 samples, both approaches achieve the same accuracy

14Here, exact simulations were used as “measurements” instead of experimental snapshots and the input images

are the real space positions of the atoms in the optical lattice.

193

Deep learning for quantum sciences – selected topics

Figure 7.16: Scaling scheme from four lattice sites to 50 for this speciﬁc system and
Hamiltonian. For each of the four wall conﬁgurations, 2500 snapshots were taken in
order to train the NN and learn all parameters. Adapted from Ref. [448].

in predictions of the parameters.

So far, we have only considered small system sizes of four atoms which can be solved
exactly with classical computers.
In the following, we present a scheme to scale to larger
system sizes of this speciﬁc Hamiltonian. In this experimental setup, it is possible to modulate
the lattice and to create walls in order to separate the chain of 50 lattice sites into four-site
units (see Fig. 7.15(c)). In this system, the parameters of the Hamiltonian are local and only
the terms of ˆH which are unaffected by the boundary have to be learned which are called the
“effective parameters” (see Fig. 7.16). Now, the boundary is shifted by one lattice site at a time
and 2500 shots are measured for each position. Once the system is shifted up to the point of
translational invariance, all parameters were at least in one conﬁguration unaffected by the
boundary wall and successfully learned by the NN.

As mentioned before, this procedure is very speciﬁc for this system and cannot easily be
generalized to different systems. The ﬁeld of Hamiltonian learning is still in its early stages
and general schemes for large systems and complex Hamiltonians have still to be developed.
However, it is a promising approach for the important task of validating if quantum simulators
work correctly which becomes more and more important with the increasing size and applica-
bility of these simulators which might have the possibility to go beyond classical computation.

7.3.4 Automated design of experiments

Amongst the proposed ML applications for experiments, we have already discussed how NNs
can be used to speed up, optimize, and verify setups, but also to analyze the generated data.
One further application is the AI-guided design of experiments which someday may, arguably,
revolutionize science.

When it comes to designing new experiments, most of the efforts have focused so far
on quantum optics [318, 346–348, 450, 451]. The design of such an experiment consists of
combining different optical lab components, e.g., beam splitters, mirrors, and crystals, so that
the ﬁnal quantum state has speciﬁc desired properties. For example, we may be interested in
obtaining a quantum state with a high-dimensional multipartite entanglement (i.e., between

194

Deep learning for quantum sciences – selected topics

Figure 7.17: Example of an algorithm for computer-inspired quantum optical exper-
iments called MELVIN. Adapted from Ref. [348].

multiple particles) which is of great importance in applications of quantum information and
computation [345]. While a trained physicist can design an experimental setup to create a
quantum state with non-trivial properties, this task can be very challenging and heavily rely
on trial and error.

In section 6.6.6 we have already presented an example [318] of an autonomous approach
to build quantum optical experiments with reinforcement learning (RL), using the projective
simulation (PS) algorithm that we introduced in section 6.5. Interestingly, there is another
AI-guided approach for designing optical experiments that has already allowed for a dozen of
new experiments in several laboratories worldwide [348]. The proposed algorithm is called
MELVIN [346] and is presented in Fig. 7.17.

To apply MELVIN, a user needs to specify a toolbox, that is, a set of available optical lab
components. Moreover, the user deﬁnes the target properties and all possible conditions that
characterize a ﬁnal quantum state. The MELVIN algorithm ﬁrst generates an experimental setup
by randomly arranging the available optical components. Each optical component is a known
symbolic modiﬁcation of the input state. Then, the resulting quantum state and its properties
are computed, as we know the initial quantum state and the symbolic transformations applied
to it. If the quantum state meets all criteria and exhibits a target property, then MELVIN reports
the setup to the user.15 More often, the generated quantum state does not match the target
one, so MELVIN starts again by generating another setup. Therefore, MELVIN relies heavily on
a random search.

However, there are two characteristics of MELVIN that grant a signiﬁcant speed-up as com-
pared to a fully random search. Firstly, a user can divide the required criteria into the cheap
and expensive ones, as presented in Fig. 7.17. The expensive criteria are then calculated only
if the cheap ones are met ﬁrst. Secondly, MELVIN is allowed to expand its initial toolbox by
adding already tested conﬁgurations and use them as basic building elements in subsequent
trials. This expansion of available tools can be thought of as a learning component of MELVIN.

For example, MELVIN has been used to ﬁnd experimental setups generating high-
dimensional multipartite entangled states as mentioned above. In Ref. [346], MELVIN iden-

15Before reporting the solution, optionally, the setup is simpliﬁed using deterministic methods predeﬁned by a
user. For example, they may include an iterative removal of a random optical component and checking whether it
changes the ﬁnal quantum state.

195

Generateexperimental setupCheap necessary setup criteria satisﬁed?Cheap necessary state criteria satisﬁed?Expensive full criteria satisﬁed?ReportsolutionExtend toolboxyesyesyesnononoSimplifyexperimental setupToolbox of quantumoptical lab componentsCalculate quantumstate ortransformationDeep learning for quantum sciences – selected topics

tiﬁed setups leading to states entangled in different ways. In particular, it found the ﬁrst ex-
perimentally realizable scheme of a so-called high-dimensional Greenberger-Horne-Zeilinger
state [346]. Moreover, as authors of Ref. [346] admit, the resulting experiments contained
interesting novel experimental techniques previously unknown to them.

Finally, studying MELVIN led to an observation that each optical setup and initial state can
be represented as weighted graphs. The successor of MELVIN, called THESEUS [347], takes
advantage of a graph representation which allows replacing random search with a gradient-
based search for optimal weights. Not only THESEUS outperforms MELVIN in terms of discovery
speed by a few orders of magnitude, but also provides interpretable solutions as long as graphs
representing discovered experimental setups are small enough.

Outlook and open problems

To conclude section 7.3, we can use ML to speed up, optimize, validate, and design experiments
as well as analyze gathered data. Proposals to apply ML to speed up and optimize experimental
work date back to 2009 [452], which may be why such applications pose one of the most widely
accepted roles for ML in experimental physics. A fascinating direction is so-called self-driving
labs [453] that combine automated experimentation platforms with AI methods to enable
autonomous experimentation. They promise an accelerated discovery rate and liberation of
experimentalists from tedious tasks.

When it comes to modern quantum technologies, the central challenges are the efﬁcient
characterization of quantum systems, the veriﬁcation of quantum devices, and the validation
of underpinning physical models. ML is expected to improve the computational cost of those
tasks. As a result, ML-based Hamiltonian learning is becoming a widely used technique to ver-
ify quantum experiments. Interesting examples are its application to nitrogen-vacancy centre
setups [454] and to nuclear magnetic resonance measurements [455].

Scaling of ML approaches to larger sizes of quantum devices remains an important chal-
lenge. Even though ML algorithms perform exceptionally well on large experimental data sets,
adding more qubits (and therefore tuning parameters) generates learning difﬁculties. These
problems are especially daunting in quantum dot systems. There are efforts toward tuning
multiple parameters at once [456, 457] or toward reducing the amount of experimental data
needed for tuning [458]. However, efﬁcient tuning of large scale quantum devices with hun-
dreds of parameters requires new methods.

In
Finally, AI promises breakthroughs when it comes to designing novel experiments.
particular, AI is argued to provide out-of-box solutions when unaware of existing human ap-
proaches [346,348]. So far, AI-guided design has been explored mainly in quantum optics with
signiﬁcant successs. However, discussed approaches (MELVIN and THESEUS) are readily extend-
able only to experiments where we can calculate how each modiﬁcation in the setup inﬂuences
the generated quantum system and its desired properties. Applying MELVIN or THESEUS to ex-
periments with very expensive (or nonexistent) theoretical description requires novel ideas.
On the other hand, it is inspiring to think about combining the proposal of self-driving labs
with AI designing novel experiments that would create an ultimate robot-scientist that never
tires and never stops seeking new solutions and discoveries.

Further reading

• King, R. D. et al. (2009). The automation of science. Science, 324, 85-89. Report on
building one of the ﬁrst “robot scientists” named “Adam” which aimed at the automation
of hypothesis forming and recording of experiments [452].

196

Deep learning for quantum sciences – selected topics

• Wiebe, N. et al. (2014). Hamiltonian learning and certiﬁcation using quantum resources.
Phys. Rev. Lett. 112, 190501. The ﬁrst proposal of Hamiltonian learning combining
quantum simulators and the Bayesian inference [459].

• Raccuglia, P. et al. (2016). Machine-learning-assisted materials discovery using failed
experiments. Nature 533, 73–76. Example of ML use for discovery of materials that
outperforms traditional human approaches [460].

• Häse, F., Roch, L. M., & Aspuru-Guzik, A. (2019). Next-generation experimentation with
self-driving laboratories. Trends Chem. 1, 282-291. Perspective concerning self-driving
laboratiories and their role in scientiﬁc discovery [453].

197

8 Physics for deep learning

Physics for deep learning

Figure 8.1: There exists a two-way inﬂuence between machine learning (ML) and
physics. In this chapter, we focus on the less known approach, i.e., physics for ML.

So far, we have discussed different applications of ML which aim at solving various prob-
lems in quantum science. In contrast, in this chapter we focus on how physics (in particular
statistical and quantum physics) inﬂuences ML research. In section 8.1, we explain fundamen-
tal theoretical challenges of ML and show how tools of statistical physics can shed some light
on these problems. In section 8.2 we discuss quantum computing and promises of quantum
machine learning (QML).

8.1 Statistical physics for machine learning

In this section, we present how to apply concepts from physics (in particular, tools of statistical
physics like the thermodynamic limit, or order parameters describing phase transitions) to
develop theory of ML (see Fig. 8.2) [461]. This idea was born already in 1980s, but the DL
revolution in 2010s has caused a renewed surge of interest in this approach.

Indeed, help from statistical physics is very needed, as we do not understand many conun-
drums in ML! For example, modern NNs can have billions of trainable parameters.1 How can
we even ﬁnd well-generalizing minima within such enormous, non-convex loss landscapes?

1One of the latest champions is Microsoft’s GPT-3 with over 175 billion parameters.

Figure 8.2: Statistical physics toolbox for understanding ML theory

198

DEEPLEARNINGPHYSICSthermodynamiclimitsimplifyingmodelsphase transitionsapproximatecomputationsMACHINELEARNINGTHEORYPhysics for deep learning

Figure 8.3: Classical and modern understanding of the generalization. (a) The clas-
sical U-shaped error curve arising from the bias-variance trade-off. (b) The double
descent error curve incorporating the classical U-shape in the classical regime and low
generalization errors of modern overparametrized models. Adapted from Ref. [462].

Another riddle is connected to the so-called bias-variance trade-off, which we have shown
in section 2.2 and which indicates that in the regime of large model complexity models should
heavily overﬁt their data sets as presented in Fig. 8.3(a). But in practice, we see that these gi-
gantic overparametrized DL models generalize very well as seen in Fig. 8.3(b). So how do they
escape this traditional bias-variance trade-off? We have a long way toward a full understand-
ing of these puzzles. A way of tackling them is to study simple, solvable models, following
a traditional approach of physicists to study new systems. Results from toy problems can give
us hints on how more complex models work.

This section has four parts. Firstly, in section 8.1.1, we go through the seminal study on
the capacity of the perceptron, which gives an idea how statistical physics can be useful for
learning problems. Then, we discuss three directions of this interdisciplinary research, i.e., the
teacher-student paradigm for studying generalization in section 8.1.2, how we can model the
structure of data in section 8.1.3, and how to study the dynamics of learning in section 8.1.4.

8.1.1 Capacity of the perceptron

The simplest ML model we can think of is a single perceptron, f , presented already in sec-
tion 2.4.4 (see Fig. 2.6(b)). In this section, we focus on its capacity, i.e., the question of how
many data points it can ﬁt. To answer it, let us make the additional assumption that the data
set is in general position.2 The assumption is reasonable – if we have many copies of the same
training point, they should not contribute to the estimation of the model capacity.

A single perceptron is only capable of learning linearly separable patterns. Therefore, we
can reformulate the question of its capacity to the question whether randomly labeled data sets
of size n with binary labels are linearly separable. The probability of such a linear separability,
(α), is a function of α, which is the ratio between the number of training points, n, and
pR
the number of data features (or data dimensionality), m. In the case of the perceptron, the
number of features is equal to the number of perceptron weights, d,3 therefore α = n
d . In this
problem, you can understand the parameter α as the difﬁculty of the classiﬁcation task, which
increases with the number of training points and decreases with the number of parameters.

2The set of points in (cid:82)d is in general position if and only if every set of (d + 1) points are not in any possible
hyperplane of dimension d. In other words, as long as there is no three data points on a single line or four points
on a single plane, etc., the set is in general position. Intuitively, any random data set is in general position.

3In general, a perceptron is parametrized by weights w and a bias b. For the remainder of this section, we

ignore biases, therefore weights are all model parameters, θ, of size d.

199

MODEL COMPLEXITYERRORTESTERRORTRAINING ERRORoverﬁttingunderﬁttingsweet spot(a)MODEL COMPLEXITYERRORTESTERRORTRAINING ERRORoverparametrizedunderparametrizedinterpolation threshold(b)“classical”regime“modern”interpolation regimePhysics for deep learning

To calculate pR we could resolve to geometric arguments. This approach was chosen by
Thomas Cover in 1960s [463]. However, here we choose to rephrase this problem in the
language of statistical physics as was done by Elizabeth Gardner in 1987 [464].

Namely, we can take the space of all possible weights, so (cid:82)d , and calculate the volume
of those weights that fulﬁll all the constraints of the random labeling.

In other words, we calculate how many sets of weights could solve the problem of sepa-

rating randomly labeled training data, D = {x(k), y (k)}n

k=1:

Vn,d

=

(cid:90)

(cid:82)d

dθ

n
(cid:89)

k=1

δ( f (x(k)

; θ) − y

(k)) .

(8.1)

The δ-function in Eq. (8.1) is 1 only when the ground-truth label is equal to the label predicted
by the perceptron. With each new data point k, we are adding a new constraint, and the
volume of possible weights shrinks. To have at least one set of such weights, the volume has
to be larger than zero, Vn,d
c, as the
value of α for which Vn,d goes down to zero. If we can calculate this, we solve the problem of
the perceptron capacity.

> 0. Therefore, we deﬁne the critical task difﬁculty, α

Let us make one modiﬁcation to the equation that leads us closer toward statistical physics.
We introduce an effective Hamiltonian counting the number of missclassiﬁed training data
points

H(θ ; D) =

Θ(− f (θ ; x

(k)) y

(k))

(8.2)

n
(cid:88)

k=1

where the Heaviside function Θ(·) is equal to 1 if its argument is positive and 0 otherwise.
We can substitute the Dirac δ-distribution above by the Boltzmann factor of H(θ ; D). Up to
a multiplicative constant, Eq. (8.1) becomes

Vn,d

∝ lim

β→+∞

β

(cid:90)

(cid:82)d

dθ e

−β (cid:80)n

k=1

Θ(− f (θ ;x (k)) y (k)) = lim

β→+∞

(cid:90)

(cid:82)d

−β H(θ ;D)

dθ e

.

(8.3)

Suddenly, the volume Vn,d in Eq. (8.3) resembles the canonical partition function4 from sta-
1
tistical physics with β playing the role of an inverse temperature, deﬁned as
kB T . The limit
β → ∞ therefore corresponds to the zero temperature limit. The problem is that this integral
is hard to calculate as it lives in a huge d-dimensional space of all real numbers.5 Moreover,
the “effective energies” in the exponent depend on the training set. As such, each training set
requires a separate calculation of the volume Vn,d .

Fortunately, the physics of disordered systems comes to the rescue.

It has been applied
to learning theory since 1980s [465–471]. Namely, if we recognize a disordered system
in Eq. (8.3), we can use solutions from statistical physics to compute this high-dimensional
integral. Let us give a brief introduction to disordered systems. A disorder system is described
by two types of random variables. The ﬁrst type concerns states of the system s ∈ (cid:82)d . For
example, for a system of d spins - 1
2 , s ∈ {−1, 1}d , because each spin can be up or down. The
i e−β(cid:34)i , where i iterates over
i is the energy of the i-th microstate. If we go to a continuous system with n identical
n, where H is

all possible microstates and (cid:34)
particles described by properties θ, the partition function is Z ∝ (cid:82) exp (cid:0)−β (cid:80)n
a classical Hamiltonian.

4A partition function for a many-body classical discrete system is equal to Z = (cid:80)

i=1 H (θi

)(cid:1) dθ

· · · dθ

1

5This is also a reason why computation of any interesting partition function is hard.

200

Physics for deep learning

second type concerns interactions between the degrees of freedom which can be parametrized
by couplings J ∈ (cid:82)n. For example, J can describe whether spins want to align or anti-align.
The distribution of states in disordered systems is then described by the Boltzmann distribu-
tion:

where H(s; J) is an energy function depending on both s and J, and ZJ
the partition function equal and plays the role of a normalization.

p(s | J) = 1
ZJ

−β H(s; J)

e

,

(8.4)

= (cid:82)

(cid:82)d dse−β H(s;J) is

− J0

) ∝ exp(cid:8)−(Ji j

)/2J 2(cid:9) where J0 and J 2 are the mean and the variance.

let us consider a spin glass [472, 473],
As an example of a disordered system,
where the energy function is H(s; J) = − (cid:80)
<i, j> Ji jsis j (resembling Ising-type interac-
tion, see Eq. (3.1)), where couplings Ji j are i.i.d. according to the normal distribution
p(Ji j
If all the
Ji j are positive, the system is ferromagnetic, and the ground state of the system is easy to
ﬁnd. With random couplings complications arise along with the frustration of the system: at
a given site, a spin can be encouraged by neighbors to point in conﬂicting directions. Finding
the ground state of such systems is a numerical challenge of its own. While in one dimen-
sion the solution is trivial and can be solved by a deterministic algorithm whose cost scales
as O(n), the complexity grows in two dimensions and reaches NP-completeness in three and
more dimensions [474].6

Now, let us tackle the exponent in Eq. (8.3) which we treat as an energy function. If we do
that, there is a property of the free energy7 which can help us in simplifying the calculations.
Namely, free energy is self-averaging.

If a random quantity is self-averaging, two conditions are met: its mean value and the
most probable value coincide in the thermodynamic limit, and ﬂuctuations around this
mean value are sufﬁciently small. In other words, the system concentrates on typical
states.

This property often holds for the free energy of disordered systems. Consider the following
argument: imagine dividing the macroscopic system into many subsystems and each subsys-
tem is still large enough to be considered macroscopic. Their interaction can be viewed as
a surface effect and is negligible when compared to the bulk. Therefore, each subsystem has
a well-deﬁned free energy and realization of disorder, even if the speciﬁc values vary between
subsystems. In the limit of an inﬁnite number of subsystems (whose interactions can be ig-
nored to ﬁrst order), the disorder average of the free energy is automatically the average free
energy across the disordered subsystems [473, 476, 477]. That is, for d large enough, the
physics of the system is independent of the disorder realization:

1
d

ln ZJ

≈ lim
d→∞

(cid:69)

J

(cid:152)

.

ln ZJ

(cid:149) 1
d

(8.5)

The free energy being extensive, note that the converging quantity in the thermodynamic limit
is the free-energy per spin. This result is highly non-trivial, tools like replica computations,
variational mean-ﬁeld methods, and high-temperature expansions are necessary to identify
where self-averaging applies and to compute the disorder averages.8 In the following para-

6There are proposals to tackle this challenge with reinforcement learning (RL) [475].
7In the thermodynamic limit, the free energy of the system is F = U − T S = − 1

β ln Z, where U is the energy of

the system and S is its entropy.

8It is interesting to note that these non-rigorous physical approaches for disordered systems developed in the

1970s [473, 476, 477] are now being put on a more rigorous footing by mathematicians [478, 479]!

201

Physics for deep learning

graph, we provide the intuition behind only one of the concepts behind Eq. (8.5), namely the
replica trick. The reader interested in more detailed explanations should turn to the tutorial
reviews [480, 481].

= − 1

Replica trick.
In statistical physics calculating averages makes sense only for extensive ob-
servables. The replica method is a way to calculate these averages with respect to disor-
der variables. We are particularly interested in the averaged value of the system free energy
[FJ
] we have to obtain the averaged
FJ
]. It turns out that averaging the log-
value of the logarithm of the partition function (cid:69)
arithm is challenging but the averages of powers of the partition function, (cid:69)
[Z n] for n ∈ (cid:78),
can be estimated. Then, by using the identity

β ln ZJ . To obtain the averaged free energy (cid:69)
J
[ln ZJ

J

J

ln x = lim
n→0

x n − 1
n

,

(8.6)

we can write

[ln Z] = lim
n→0
As we can see the limit n → 0 requires n ∈ (cid:82). However, what we can do is to calculate Z n for
n ∈ (cid:78). The partition function Z is an integral of the form (cid:82) e−β H(s,J), thus we can write Z n as

(8.7)

(cid:69)

.

J

(cid:69)

J

[Z n] − 1
n

(cid:90)

Z n =

(1)

ds

. . . ds

(n)

(cid:90)

−β H(sa,J)) =

e

n
(cid:89)

a=1

(1)

ds

. . . ds

(n)

e

−β (cid:80)n

a=1 H(sa,J))

,

(8.8)

where the exponent contains a sum over n independent samples, or replicas. The replica
trick consists in deﬁning a function φ(n) being an analytic continuation of the function in
the exponent. As such n ∈ (cid:82) becomes a continuous variable, and we can take limit n → 0
in Eq. (8.7). In summary, assuming we are able to calculate averaged value (cid:69)
[Z n], we can
calculate the averaged value of the free energy FJ .

J

Finally, having Eq. (8.5), we can come back to the volume Vd,n in Eq. (8.3).

We associate this volume Vd,n now with the partition function of a spin system. Spins
(s) are now model parameters (θ), and couplings (J) are training data (D), posing the
constraints to learn.

Applying the same analysis as in the paragraph before, we can state that the free energy for
a given realization of the data set is just the free energy averaged over the data set distribution
when we consider large data sets and large perceptron with ﬁxed ratio α = n/d:

Vd,n

(cid:39) lim
n→∞

(cid:69)D[Vd,n

| D] = V (α) .

(8.9)

Therefore, if you ﬁx the distribution of data (disorder realization), you can ﬁnd the α
c for
= 0 and as a result, the perceptron capacity. To be more exact, we can calculate
which Vd,n
it only for the large (“thermodynamic”) limit of n for an arbitrary ﬁxed α as V is actually
expressed in terms of α.

We remind you that α

c indicates the critical task difﬁculty for which the volume of per-
ceptron weights fulﬁlling the constraints of the random labelling goes to zero. It means that
for the lower task difﬁculty, α < α
c, the randomly labeled data is linearly separable, while for
the higher task difﬁculty, α > α
c, the data are no longer linearly separable. The probability,
pR, is therefore a step function of α in the thermodynamic limit. We plot pR for real-valued
parameters coming from a Gaussian distribution in blue in Fig. 8.4. To show ﬁnite-size effects,

202

Physics for deep learning

Figure 8.4: Probability of the randomly labeled data being linearly separable, pR,
as a function of the difﬁculty of the task, α = n
d . Finite size results were obtained
analytically by Cover [463].

Table 5: The capacity of the perceptron depending on the distribution of the data
= n
and type of weights. The capacity is expressed as the minimal task difﬁculty, α
d ,
for which the volume of possible solutions goes down to zero, V (α

) = 0.

c

c

Distribution of data
(k)
p(x
i
θ ∈ Rd
(k)
i

p(x
θ ∈ {−1, 1}d

Gaussian inputs
Real weights

Binary inputs
Binary weights

1

2

) = N (xi; 0, 1)

) = Bernoulli(0.5)

Critical task difﬁculty, α

c

α

c

= 2

α

c

≈ 0.83

(α) below the thermodynamic limit following Cover’s argument [463].
we can also compute pR
To vary α, we can either change n or d. In case of perceptron, it is easier to keep d ﬁxed and
calculate pR as a function of α for increasing n.

In the equivalent of the thermodynamic limit, so n → ∞, we see a phase transition for
= 2, which means that the most difﬁcult task that the perceptron is able
a critical α
to solve is when the number of training points (in general position) is twice as large as
the number of parameters.

c

Interestingly, the solution for α

= 0) depends on the setup of the problem,
namely the random data distribution and the allowed values of parameters (spin values).
While the previous discussion has been conducted for Gaussian distribution of inputs and real
perceptron parameters, θ, different critical task difﬁculty is obtained for binary inputs and
parameters, as presented in Table 5.

c (for which Vd,n

In this section we have looked at the problem of perceptron capacity which is well-known
and decades old. As such, it serves well the educational purpose. In particular, we have seen
that the statistical approach to learning focuses on simple solvable models (here, perceptrons).
Moreover, we have seen that the statistical approach aims to express learning problems in terms

203

1.01.52.02.53.0=nd0.000.250.500.751.00pRphase transitionc=2n=10n=65n=250n+Physics for deep learning

of statistical problems, e.g., disordered spin systems,9 where physicists have already developed
useful analytical tools.

In the next sections, we brieﬂy discuss selected modern results from the intersection of ML
and statistical physics. For the more thorough review on this intersection, we refer to [30].
Moreover, the outstanding retrospective of these developments can be found in the lecture
titled “Statistical physics and ML: A 30-year perspective” of the late Naftali Tishby.

8.1.2 The teacher-student paradigm: a toy model to study the generalization

Our motivation for this section is to tackle the riddle of generalization, which is the ability
of a model to make correct predictions on data unseen during training. However, our goal
for this section is not to build new useful ML models or to distinguish between bad and good
modern models in terms of generalization. Rather, we want to understand why useful modern
ML models generalize so well. To do so, let us consider all learning task elements (such as the
model, optimization method, and data) in their simplest form. The toy model which helps us
in this ambitious task falls under the teacher-student paradigm.

The teacher-student paradigm consists of two main elements: a teacher which is a data-
generating model, and a student which is a model trying to learn the data generated by
a teacher.

| x) = ft

Teacher consists of an input distribution px

(x), e.g. Gaussian or binary, and an input-
(x, θ∗). For now, let us assume the teacher is a perceptron. On top of
output rule p( yt
the input-output rule, we may assume a ground-truth distribution on the weights pθ (θ), from
which the parameters θ∗ of the teacher model were drawn. Once we decide on how a teacher
(k)
looks like, it can generate training data: D = {x(k), y
t

(x(k), θ ∗)}n

= {x(k), ft

}n
k=1

k=1.

The second element is the student whose aim is to learn the distribution underlying
the training data.
In the teacher-student scheme, we know exactly what the data-
generating distribution is. Therefore, we can easily distinguish between a student
that simply ﬁts the training data (limited generalization) and a student that recovers
a teacher’s input-output rule (perfect generalization). In other words, we can measure
the generalization ability of the student.

To continue with the teacher-student strategy, we need to decide on a model for the stu-
(x, θ), but also on a learning strategy. Let us start with the simplest scenario when
dent, fs
a student is also a perceptron (like the teacher). To train, we could use the standard empirical
loss minimization strategy, e.g.,

θ∗ = argminθ

(cid:128)

L

(k)
y
t

, fs

(cid:0)x(k)

, θ(cid:1)(cid:138)

(cid:171)

,

(cid:168) n
(cid:88)

k=1

(8.10)

(k)
where we aim to minimize a given distance between the teacher outputs y
t
outputs y (k)
posterior:

and student
(x(k), θ). Alternatively, we can instead maximize the following Bayesian

= fs

s

p(θ | D) ∝

n
(cid:89)

(cid:128)

(k)
t

y

p

| θ, x(k)(cid:138)

p(θ) .

(8.11)

9This also tells us that NNs with binary weights may be especially approachable for physicists. These are spin-

1/2 problems!

k=1

204

Physics for deep learning

Equation (8.11) denotes the posterior distribution, i.e., the belief on the student model weights
θ given the data set D and the prior assumption on the student weights p(θ).

Assuming, e.g., a MSE loss, the student generalization error for given weights θ is deﬁned

as the expected error over the entire data distribution:

Eg

(θ) = (cid:69)x, y

(cid:2)( y − fs

(x, θ))2(cid:3) .

(8.12)

(·, ·)
In the best possible scenario, the student model fs
underlying the generated data. When a student is identical to the teacher, we call the setting
Bayes optimal and deﬁne the Bayes optimal error of the student (see section 2.3),

(·, ·) is identical the teacher model ft

E opt
g

(D) = (cid:69) (cid:2)(cid:69)x, y

( y − fs

(x, θ))2 | p(θ | D)(cid:3) ,

(8.13)

which is a mean error for student parameters θ drawn from the posterior distribution
in Eq. (8.11). This is a fundamental quantity from the point of view of information theory: it
quantiﬁes how much information on the weights θ the training data set D provides assuming
the student has perfect knowledge of the form of the problem. We can use the same tools as
in the previous section (disorder average, thermodynamic limit, and replica computation) to
obtain:

E opt
g

(D) →
n→∞

(cid:69)[E opt
g

(D) | D] = E opt

g

(α) .

(8.14)

Here again, the limiting generalization error takes the form of a function of the ratio α = n
m
between the number of data points and the number of data features or weights. We no longer
interpret this ratio as the difﬁculty of the classiﬁcation task as in the capacity computation.
Instead, in generalization problems, it is more useful to think of α as the sample complexity,
that is the amount of training data available to infer the input-output rule. In the following
paragraphs, we examine generalization for a few different pairs of teacher and student.

Two perceptrons. The generalization error from Eq. (8.14) is shown in Fig. 8.5. We can
compare the limiting Bayesian optimal generalization error (red line in panels (a) and (b))
with the training of a perceptron at ﬁnite m by minimizing a loss function, such as performing
a logistic regression with gradient descent (blue squares). In panel (a), for binary weights, we
have a ﬁrst-order phase transition [469, 470]. In panel (b), for real-valued weights, there is
a smooth decrease of the generalization error [482]. In both cases, there is a computational
gap between the optimal generalization error and logistic regression with gradient descent.

Finally, the same generalization error of the student perceptron can be studied when learn-
ing occurs with algorithms called generalized approximate message passing (GAMP). For the
introduction to these methods, see Ref. [481, 483, 484]. For our needs, it is enough to know
that these algorithms provide an alternative to convex optimization and allow for efﬁcient
calculations of quantities based on graphs (like perceptrons or NNs) which are sampled from
distributions like Eqs. (8.12) to (8.14). Moreover, they are remarkable in that their asymptotic
(n, d → ∞, n/d = α) performance can be analyzed rigorously using the so-called state evo-
lution (SE). Armed with this knowledge, we now see that the generalization error obtained
using GAMP in Fig. 8.5 is much closer to the Bayes error compared to the optimization with
gradient descent.
In panel (a), there is a re-
maining computational gap between GAMP and the exact Bayes error. This regime is called
a hard phase. It comes from the fact that, in practice, our computational time is limited to
the polynomial regime. Interestingly, there is no known efﬁcient algorithm which would beat
GAMPs in the hard phase of this perceptron learning [482].

In panel (b), the gap completely disappears.

205

Physics for deep learning

Figure 8.5: Generalization error as a function of the task difﬁculty α, which is the
ratio between the number of training points and the number of (student) model
parameters for perceptrons with (a) θ ∈ (cid:82)d or (b) θ ∈ {−1, 1}. The red line is
the exact Bayes-optimal generalization error. The blue squares are for a ﬁne-tuned
perceptron with gradient-based minimization of the error. We see the computational
gap between those results. The gap (a) gets smaller or (b) disappears for message-
passing algorithms. Black circles are results for n = 104 obtained using generalized
approximate message passing (GAMP), and the green line denotes the results of state
evolution (SE) which approximates the limit n → ∞. Adapted from [482].

Two-layer NNs. So far, both the teacher and the student have been modeled with percep-
trons. We can switch to more complex models. For the reminder of this section, we use two
special two-layer NNs with a rich history in statistical physics. We start with committee ma-
chines [485, 486] shown in Fig. 8.6(a). Their analytical treatment is possible in the limit of
an inﬁnite number of input features, m, and data size, n, while keeping a ﬁnite number of
hidden units. In particular, we present here soft committee machines which allow for an even
simpler analysis. In soft committee machines, we train only parameters belonging to the ﬁrst
= d = mD, where m is the number of features and D is the
layer of the machine, θ1, of size d1
number of hidden units. The second layer is ﬁxed and identical for both the teacher and the
student. The second NN used in this section is a random feature model [487, 488] presented
in Fig. 8.6(b). Interestingly, their analytical analysis is enabled by a ﬁxed ﬁrst layer whose
= d = mD.
parameters are set to random values. The number of those parameters is also d1
Therefore, only parameters of the second layer are trainable. The idea behind the random
ﬁrst layer is that projecting a lower-dimensional input onto a much higher dimensional space
leads to better separation of the data which then can be successfully processed by a single-
layer NN.10 Also note that random feature models can have an arbitrary number of hidden
units, in particular larger than the number of input features, which allows for a study of over-
parametrization.

Two committee machines. Now, we are ready to tackle generalization with more complex
models. Here, we use soft committee machines. For now, a teacher and a student share the
same architecture. The formulation of the problem stays the same. We calculate the general-
ization error from Eq. (8.14) of the student committee machine when learning data generated
by the teacher committee machine [486]. We plot generalization errors in Fig. 8.7(a)-(b) for
committee machines with two hidden neurons. Similarly as before, we see in panel (a) that

10In other words, you can think of such a projection as mapping input data to a feature space as discussed in
section 4.2 on kernel methods. Interestingly, Refs. [487, 488] showed that random data projection onto a feature
space is not much worse compared to projecting onto an optimized feature space. However, randomization is much
cheaper than optimization.

206

𝛼𝛼Physics for deep learning

Figure 8.6: Schematic illustration of used two-layer NNs. (a) Soft committee ma-
∈ (cid:82)m×D, are trainable, whereas the
chine. Parameters belonging to the ﬁrst layer, θ1
∈ (cid:82)D×1, are chosen identical to the parameters of
parameters of the second layer, θ2
the teacher. Its analytical treatment is possible if m → ∞ and D = O(1). (b) Random
feature model. Its ﬁrst layer is ﬁxed to random parameter values. The second layer
is trainable. The number of hidden units can be varied to study overparametrization.
In the analysis, the number of hidden units D → ∞ scales linearly with the number
of inputs m → ∞, i.e., m/D = O(1).

for real-valued weights the generalization error (obtained with GAMP and SE) is equal to the
Bayes one, while for binary weights in panel (b) there is a computational gap between both
errors. This time, we also look at the overlap between hidden neurons of the student and
of the teacher, which measures the similarity neuron-by-neuron between the teacher and the
student. To be more precise, we look at the matrix Q = [q j j(cid:48)] = 1
1,i j(cid:48), where Θ
θ
1
m
and θ1 are the parameters of the teacher and student ﬁrst layers, respectively. It turns out that
there is a so-called specialization phase transition [489, 490].

(cid:80)m

Θ∗

i=1

1,i j

In the regime of low task complexity, both hidden units of the student committee ma-
chine learn the same function. After crossing the critical α, when enough data is avail-
able, hidden neurons of the student start to specialize. Each student neuron selects
a different teacher neuron to converge to. The specialized phase is associated with
lower generalization error than the non-specialized, see Fig. 8.7.

c

c

The specialization for teacher and student committee machines with two hidden neurons
(D = 2) takes place for α
≈ 1.5 for binary weights,
≈ 2 for real-value weights and for α
which means that specializing neurons require at least 2 and 1.5 times more training data
than the number of data features, m, i.e., approximately as much training data as the number
= 2m. Similar observations hold if both teacher and student
of parameters in the ﬁrst layer, d1
committee machines have a large number of hidden neurons (D (cid:29) 2). We can plot a phase
diagram of the generalization error as a function of a rescaled task difﬁculty, ˜α = α
= n
Dm
D
for real-valued weights. It is presented in Fig. 8.7(c). In total, we ﬁnd three distinct phases:
two correspond to specialized and non-specialized hidden neurons, and above the specialized
phase, there is a computational gap where a model in principle has enough information to
specialize but is unable to due to shortcomings of its optimization.

207

(a)OUTPUTD HIDDENUNITSﬁrstlayersecondlayertrainableﬁxed, same as teacherm FEATURESOUTPUTD HIDDENUNITSﬁrst layersecond layerﬁxed to randomtrainable(b)m ∞    D ∞    m/D=O(1) D=O(1)m FEATURESm ∞    Physics for deep learning

∝ n

Figure 8.7: Generalization error and specialization in committee machines as func-
tions of the task difﬁculty α = n
d , which is the ratio of the number of training
m
points and the number of input features. We consider committee machines with (a)
θ ∈ (cid:82)m×D, D = 2, or with (b) θ ∈ {−1, 1}m×D, D = 2. The black line is the exact
Bayes-optimal generalization error, black dots are obtained by studying the commit-
tee machine with GAMP. The orange and blue lines and dots indicate the overlap of
the two hidden neurons of the student committee machine with the two hidden neu-
rons of the teacher committee machine, calculated with GAMP and SE, respectively.
We see that specialization is responsible for the rapid decrease in generalization er-
ror. (c) Generalization of panel (a) to large number of hidden neurons, D. Phase
diagram calculated for the task difﬁculty, ˜α = α

D . Adapted from [486].

Overparametrization. As we have already mentioned in the introduction, one of the
most puzzling phenomena in modern ML is the generalization capability of heavily over-
In real-world setups, it is natural to think of the level of the model
parametrized models.
overparametrization as the ratio between number of model parameters, d, and number of
available training data points, n. Surprisingly, we see in practice that models with large d are
able to extract meaningful relations from much fewer training data points. In turn, with the
teacher-student scheme, we can make the deﬁnition of overparametrization more rigorous,
because we have direct access to the “ground-truth” number of parameters needed to describe
the input-output rule, which is the number of teacher parameters. Therefore, the level of
overparametrization can be understood as a ratio between the number of student and teacher
parameters. In particular, the student can have much more parameters than the teacher. To
study overparametrization, it is then a necessity to have mismatched teacher-student architec-
tures. Crucially, this mismatch of architectures means that the student cannot achieve a Bayes
optimal error anymore.

For the remainder of this section, we study the generalization error of overparametrized
student models. This time we employ as a student a random feature model, presented already
in Fig. 8.6(b). The analysis requires the model’s ﬁrst layer weights to be ﬁxed to random
values. The number of student parameters in the second layer can vary as compared to the
teacher.11 As such, we have a full control over how overparametrized the student is. We come
back to the study of overparametrization in committee machines in section 8.1.4.

11Ref. [491] interprets the same exact setting as a teacher generating labels with a perceptron, itself acting on
a low dimensional latent space, and input data generated with a one layer generative NN from this latent space.
A student perceptron is trained in the input data-label pairs.

208

ℰgoptℰgoptℰgopt(𝛼)𝛼𝛼෤𝛼ℰgopt(෤𝛼)Physics for deep learning

Figure 8.8: Generalization errors as functions of the ratio of the number of model
parameters and number of training data points for mismatched teacher-student mod-
els where the student is a random feature model. First (second) column shows the
generalization error in the case of a regression (classiﬁcation) problem. The upper
row shows results for sub-optimal regularization strengths, where the generalization
error curves exhibit a double descent. The bottom row shows results for optimal
regularization, where the double descent disappears. Adapted from Ref. [491].

α = d

With a student random feature model, we are ready to study the generalization error as
a function of overparametrization, 1
n , where d = d1 is the number of parameters in
the ﬁrst ﬁxed random layer of the student. As the student cannot achieve a Bayes optimal
error anymore, we need to change the training objective, e.g., to a MSE with (cid:96)
2 regulariza-
tion. Using various analytical tools, we can still approximate the generalization error of the
student and plot it as a function of overparametrization d
n . In Ref. [491], the generalization
error in regression and classiﬁcation tasks was analyzed for various regularization strengths.
Their results are shown in Fig. 8.8. The left (right) column shows the generalization error of
the mismatched student for optimal and sub-optimal regularization strengths in a regression
(classiﬁcation) problem.

Remarkably, in the case of mismatched student-teacher architectures, the generaliza-
tion error curve exhibits a characteristic double descent. Moreover, the optimal choice
of regularization cancels the ﬁrst error descent, resulting in the generalization error
steadily decreasing with increasing number of model parameters.

Therefore, these results on toy models give us a hint on the origin of the double descent
phenomenon.
It occurs when the student and teacher have mismatched architectures, and
the choice of regularization strength is sub-optimal. Interestingly, Ref. [491] also showed that
the magnitude of the initial generalization error ascent in the double descent phenomenon
depends on whether the problem is a classiﬁcation or regression task.

In summary, the study of toy models indicates that there are numerous reasons for the
generalization error being larger than the Bayes optimal error. In general, the generalization
capabilities depend on:

• whether a data-generating model (teacher) and learning model (student) have mis-

209

𝜆=10−8𝜆=10−4𝜆𝜆Physics for deep learning

matched architectures,

• whether the model aims at solving a regression or classiﬁcation problem,

• the choice of optimization method, target function, and available computation time,

• the sample complexity (how much training data is available and, for teacher-student

committee machines, the degree of specialization of the neurons).

8.1.3 Models of data structure

So far, while studying sources of generalization errors, we have mainly played with the archi-
tectures of teacher and student models, which speciﬁes the structure of the input-output rule
underlying the data. In particular, we have only considered random input data sets where
all input features are independent. Clearly, while such isotropic data simpliﬁes the analyti-
cal analysis, it is quite unrealistic. Ideally, we would like to study prototypical data sets, like
MNIST [21] or ImageNet [24], but these are difﬁcult to treat analytically. Instead, let us move
one step away from data sets given by white noise and use teacher-student paradigm to study
the impact of data anisotropy on the generalization error. To this end, we employ salient and
weak feature models [492]. Within these feature model, the data remains Gaussian (as in most
of the previous sections), x ∼ N (0, Σx), but the covariance is not isotropic as if Σx = Id .
Instead, it is anisotropic:

Σx =

(cid:20) σx,1Iφ
0

1d

0
σx,2Iφ

2d

(cid:21)

,

(8.15)

(cid:29) σx,2, and φ

where σx,1
1/2d denotes the number of data features (equal to the number
= 1). The
of perceptron parameters) that are affected by the variance σx,1/2 (with φ
parameters affected by the large variance, σx,1, form the salient subspace, whereas the ones
with a small variance, σx,2, form the weak subspace as presented in Fig. 8.9(a). We assume
the weak subspace to be much larger than the salient one, φ

(cid:29) φ

+ φ

1

2

2

1.

If we add such a structure to our input data and run the teacher-student scheme with
mismatched architectures (here, the teacher is a perceptron and the student is a random fea-
ture model), we can still compute the generalization error exactly in the high-dimensional
limit [492]. Importantly, this generalization error now depends on the input data anisotropy.
In particular, it depends on how the teacher perceptron is aligned with respect to the weak
and salient data subspaces as presented in Fig. 8.9(a). The dashed hyperplanes mark the sep-
aration of the input space by the teacher percepton and lie perpendicular to the perceptron
parameter vector, θ. This vector can be aligned in various ways with the data anisotropy. If
θ is aligned with the salient subspace, the hyperplane cuts along the weak subspace, and the
only subspace relevant to discriminate the data points is the salient subspace, in which the
variance of the data is concentrated, and the weak subspace can be effectively ignored. In this
case, due to the data structure, the problem has a small effective dimension corresponding
to the salient space φ
1d, therefore it is easier to solve. In turn, if θ is aligned with the weak
subspace, the impact of the data structure is negligible as the student needs to discriminate
along an axis where data has low variance comparatively to the typical variance of the data.
Here, the problem closely resembles the (fully) isotropic case.

The impact of the data structure on the generalization curve as a function of the ratio of the
m , is shown in Fig. 8.9(b).

number of student model parameters and the data dimensionality, d

210

Physics for deep learning

Figure 8.9: Data structure entering the teacher-student scheme. (a) The data space
can be separated into a weak and salient subspaces, where the data is characterized
by a small or large variance, respectively. The teacher perceptron with parameters θ
can be either aligned (blue vector) or misaligned (purple vector) with the salient sub-
space. The classiﬁcation task speciﬁed by the teacher (represented as a line separat-
ing the data) is easier (compared to the isotropic case) if θ is aligned with the salient
subspace. (b) The model can detect the structure existing in the data. As a result,
the generalization error is lower for the structured than for the fully isotropic case.
Results for the isotropic data are similar to the results for anisotropic data where the
teacher perceptron is misaligned. The generalization curve shows a double descent.
Adapted from Ref. [492].

Interestingly, the structure in the data is detected during training before the generaliza-
tion error peaks due to overﬁtting and improves the generalization error as compared
to the isotropic case.

Why does the structure help? Because, in practice, the model can ignore the weak subspace
and focus on the salient one, which lowers the dimensionality of the problem. The fact that the
generalization error is lower in the anisotropic case compared to the isotropic case remains
true even in the highly overparametrized regime ( d
= 103). Moreover, the double descent
m
phenomenon is also exacerbated in presence of data structure. Note that both these effects
take place only when the teacher perceptron is aligned with the salient subspace. Otherwise
the setup closely resembles the isotropic case.

It turns out that many more questions can be addressed with the teacher-student paradigm
using salient and weak feature models. In particular, the authors of Ref. [492] checked the
interplay between the data structure and other elements of ML problems, like the choice of
the loss function. Recalculating the quantities in Fig. 8.9(b) for the MSE and logistic loss, one
observes that the overﬁtting peak is attenuated in the case of logistic loss. Therefore, it seems
that the logistic loss takes more advantage of the existing data structure. We can conﬁrm
this further by computing the generalization error for both loss functions as a function of the
teacher-data alignment. As discussed earlier, this alignment determines how much of data
structure is effectively present in the problem. In agreement with the results described above,
when increasing the alignment, the gap between the generalization error of MSE and logistic
loss grows.

211

𝑥∈ℝ𝑚ℰg𝑑→∞𝑑=100Physics for deep learning

In
Figure 8.10: Dynamics of learning in overparametrized committee machines.
panels (a)-(b), we allow only a single trainable student layer, another layer is ﬁxed.
In panels (c)-(d), we train the whole student model. (a),(c) Generalization error
vs. student overparametrization which occurs when the number of student hidden
units, D, is larger than the number of teacher hidden units, T . (b),(d) Self-overlaps
of the student (Q matrix) and overlaps between the overparametrized student and
the teacher (R matrix). Vector w contains the second layer weights of the student.
Adapted from Ref. [493].

8.1.4 Dynamics of learning

Finally, we can investigate the dynamics of learning and its dependence on the model over-
parametrization using the teacher-student schemes described above. An example of a simpli-
ﬁed model of learning is online learning, which has been analyzed since the 1990s. In online
learning, the model is fed a stream of data, where the model sees each data point only once.
We build a loss function based on this example and perform a parameter update according to
the gradient of this loss function. In fact, we perform a parameter update after each data point
encounter. Thus, the number of optimization steps is equal to number of seen training data
points. If we take the continuous time and high-dimensional limit, and average over all ran-
dom variables (which is doable with the replica method if we assume samples at distinct times
are uncorrelated), we can again calculate the generalization error explicitly. In particular, we
can calculate how it changes during the training. In other words, we can track the quality of
the model predictions over the course of the training.

Such an analysis has already been conducted in the 1990s for perceptrons and committee
machines [494, 495].
It showed, for instance, that during online training, the generaliza-
tion error decreases with different convergence rates given different learning rates. Recently,
the same analysis was revisited for soft committee machines [493] considering the impact of
overparametrization. In the simpliﬁed case of matching teacher-student models and training
limited to only a single student layer, results show how the generalization error drops the mo-
ment the student neurons specialize and attain a large overlap with the teacher neurons. We
can also investigate the effect of overparametrization on the learning dynamics using commit-
tee machines as presented in Fig. 8.6(a) in the regime of the number of data features, m → ∞,
with the sigmoidal activation function, g(x) = erf(x/
2). Here, we study the generalization

(cid:112)

212

ℰg∗ℰg∗Physics for deep learning

error as a function of the ratio between the number of hidden units of the student D and the
teacher T given by D
T . Figure 8.10 shows two cases of online learning of such overparametrized
students. In the ﬁrst case, shown in panels (a)-(b), only the ﬁrst hidden layer of the student
model can be trained, whereas the parameters of the second hidden layer are ﬁxed and identi-
cal to the respective teacher layer. In the second case, presented in panels (c)-(d), both student
layers are trained. Panel (a) shows that the generalization error actually increases with the size
of the trainable student layer, proving that overparametrization can be detrimental in some
scenarios. To understand the reason, we analyze the teacher-student overlaps at the end of
the training in the form of the R = [Ri t
] where each matrix element measures the similarity
between the weights of the i-th student node and the t-th teacher node. We also study the
overlap of the weights of different student nodes with each other (Q = [Qi j
]). We plot both
matrices in panel (b). We see that in the case of soft committee machines only the number
of student neurons equal to the number of teacher neurons specialize. The rest simply picks
up the noise present in the available data, which impairs generalization. However, if we al-
low all layers to be trained, a very different behavior is observed. In panel (c), we see that
the generalization error decreases as one overparametrizes the student model. This time, all
neurons learn something related to the teacher neurons. Thanks to that, additional neurons
are beneﬁcial as each teacher neuron can be learned by an ensemble of student neurons which
contributes to “denoising” the estimation of the teacher parameters.

Outlook and open problems

In this section, we have seen how to use analytical tools from statistical physics to study prob-
lems in ML.
In particular, we have discussed the seminal problem of perceptron capacity.
Subsequently, we have focused on a powerful paradigm for studying the generalization error:
the teacher-student scheme. This scheme is amendable to various modiﬁcations addressing
every element of the learning problem.

We can study different teacher and student architectures, and they can be mismatched.
We have shown results for perceptrons, committee machines, and random feature models,
but in general, we can have, e.g., a pre-trained generative model (described in more detail
in section 7.2.1) playing a role of a teacher as it was done in Ref. [496]. One can also analyze
more complex data sets than those provided by a salient and weak feature model. In particular,
is is possible to conﬁrm intuitions gained from the analytical analysis of simple models with
simulations on standard benchmark data sets [492], such as MNIST [21] and CIFAR [23].
Finally, one can go beyond online gradient descent and study multi-pass stochastic gradient
descent (which involves multiple encounters of the same data points) with dynamical mean-
ﬁeld theory [497], bringing us closer and closer to modern optimization methods.

Moreover, one can investigate the capacities of large ML architectures (in contrast to sim-
ple perceptrons). Statistical tools play also an increasingly important role in the research on
quantum machine learning (QML). For example, the Gardner approach was successfully ap-
plied to quantum perceptrons [498, 499] and quantum NNs [500].
In particular, it turned
out that the quantum perceptron has some advantages over its classical counterparts when it
comes to capacity [499]. Moreover, the teacher-student scheme was proposed to systemati-
cally compare different quantum NN architectures [501]. Finally, there is an increasing body
of works searching for phases in the learning dynamics of ML models [502, 503].

Further reading

• Gabrié, M. (2020). Mean-ﬁeld inference methods for neural networks. J. Phys. A: Math.
Theor. 53, 223002. Review on the mean-ﬁeld methods mentioned within this section.
In particular, it contains principles of derivations of high-temperature expansions, the

213

Physics for deep learning

replica method, and message passing algorithms [481].

• Zdeborová, L. (2020). Understanding deep learning is also a job for physicists. Nat. Phys.

16, 602–604. Short and friendly introduction to how physics can help ML [461].

• Castellani, T. & Cavagna, A. (2005). Spin-glass theory for pedestrians. J. Stat. Mech.

P05012. Pedagogical review on mean-ﬁeld methods for spin glasses [480].

8.2 Quantum machine learning

This section explores yet another direction: how quantum mechanics and quantum technolo-
gies can be used to solve data-driven tasks. This recent ﬁeld is called quantum machine learn-
ing (QML)12. This ﬁeld started with the development of quantum algorithms aiming for a po-
tential fully quantum advantage.
In recent years, there has been an increasing interest in
another direction: studying hybrid quantum-classical algorithms (also often called quantum-
enhanced algorithms), where part of the algorithm is performed on a quantum device. With
the development of new experimental platforms for quantum computation, researchers are
now looking for applications tailored to these hybrid algorithms and trying to determine if
and how quantum advantage can arise in such systems. While the quantum advantage would
represent a breakthrough, the study of the quantum-enhanced algorithms running on these
hybrid devices is an interesting problem in itself and can potentially lead to the discovery of
exciting physics.

In the following sections, we provide an overview of the recent advances in the ﬁeld. We
do not aim to provide a complete review, but rather an introduction to selected topics. In the
last section, we refer to recent reviews of the ﬁeld for the interested reader.

8.2.1 Gate-based quantum computing

In the following sections, we focus on the description of gate-based quantum computation.
These concepts are used throughout the whole section.

The most common building blocks of gate-base quantum computation are qubits and
quantum gates. A gate-based quantum algorithm, speciﬁed as a sequence of gate op-
erations and measurements performed on qubits, can be conveniently represented as
a quantum circuit.

Qubits are two-level quantum systems that can be realized by isolating two degrees of
freedom in several experimental platforms, such as photonic platforms [504], superconducting
circuits [505], trapped ions [506], or Rydberg atoms in optical tweezers [507, 508]. When
performing a calculation, a quantum computer modiﬁes the state of the qubits or entangles
them with quantum gates.

Quantum gates are unitary operations and can be represented by unitary matrices. The
dimensions of these matrices depend on the number of qubits on which these gates act. The
scaling of their dimension is exponential in the number of qubits.

Examples of single qubit gates are the Hadamard and Pauli-X gates, which read in the

12Often in literature, quantum machine learning (QML) incorporates both quantum-enhanced ML and ML ap-
plied to quantum, e.g. ML for quantum information processing. In this section, we use QML for quantum-enhanced
ML. A detailed discussion about this convention can be found in section 8.2.2

214

Physics for deep learning

Figure 8.11: Illustration of a quantum circuit diagram with two initialized qubits,
q0 and q1, and three different quantum gates: the Hadamard gate H and the σ
x
gate X, both acting on q0, and a two-qubit gate (CNOT). The ﬁnal element is the
measurement on q0 and q1.

single qubit basis {|0〉 , |1〉}

H = 1
(cid:112)
2

(cid:19)

(cid:18)1 1
1 1

, X =

(cid:19)

(cid:18)0 1
1 0

,

or parametrized gates such as the single qubit rotation gate

RX

(θ ) = 1
(cid:112)
2

(cid:18) cos θ
−i sin θ

−i sin θ
cos θ

(cid:19)

,

(8.16)

(8.17)

parametrized in terms of the angle θ . An example of a two-qubit gate is the controlled NOT
gate (CNOT), which reads in the two qubit basis {|00〉 , |01〉 , |10〉 , |11〉}






CNOT =

1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0






.

(8.18)

In general, quantum circuits can be depicted with quantum diagrams, as exemplarily shown in
Fig. 8.11. Each line corresponds to a qubit. This circuit has two gates acting on a single qubit
(X and H) and an entangling gate (CNOT) acting on two qubits. The last part of the diagram
is the measurement, which is an interaction with individual qubits that forces their collapse
to one of the two levels. As the measurements are destructive, the careful choice of a set
a measurements is necessary to propely extract the relevant information from the quantum
circuit.

8.2.2 What is quantum machine learning?

To better understand quantum machine learning (QML), let us ﬁrst have a look at Fig. 8.12(a).
Generally, ML algorithms are run on classical data, e.g., image classiﬁcation or natural lan-
guage processing. We have thus a classical algorithm dealing with classical data (CC). These
Lectures Notes focus mainly on the case of classical ML applied to quantum data (CQ), e.g.
quantum states. On the other hand, quantum machine learning (QML) deals with the inte-
gration of quantum devices in ML algorithms. Therefore, the algorithms can be quantum, and
the data can be either classical (QC) or quantum (QQ). In this section, we focus mainly on the
QC side, as the QQ side is only at its early stage of development [509–512].

Let us discuss an elementary example to introduce the revised building blocks of ML in the
context of the QC QML. We consider the classiﬁcation problem of one-dimensional data on
a ring. We intuitively sketch each step of this QML classiﬁcation problem in Fig. 8.12(b). Firstly,
) is encoded in a quantum computer. Here, for example, we encode the
the classical data (xi, yi
, where θ are
data points on a single qubit through the action of a parametrized unitary Uθ,xi

215

==XXHPhysics for deep learning

Figure 8.12: (a) Table of the different types of data and algorithms. (b) Sketch of
an example of a classiﬁcation task on one qubit.

(θ ) introduced in section 8.2.1).
the parameters of this unitary transformation (for example RX
Then, a measurement is performed, and one can deﬁne the output of the measurement ypred
as a label (here 1 or 0). We then construct a loss function L(θ) depending on the predicted
and ground-truth labels. We can see here that the quantum-enhanced part corresponds to the
evaluation of ypred on a quantum computer. Once the loss function is deﬁned, the minimization
can be performed on a classical computer with the method of your choice, such as gradient
descent or a gradient-free optimizer (e.g., Nelder-Mead). In this simple example, the training
has a simple interpretation. Initially, the weights θ of the unitary are randomly distributed.
Consequently, the mapping of our classical data to the qubit is randomly distributed on the
Hilbert space. The optimization procedure aims to push the two classes toward the opposite
poles of the Bloch sphere. Therefore, for the weights after training θ∗, we expect that data on
the Bloch sphere is much more ordered.

8.2.3 Ideal quantum computers and quantum machine learning

Computational complexity theory is a ﬁeld of computer sciences that focuses on classifying
computational problems in terms of the resources they need. In particular, classical comput-
ers are known to excel at solving problems belonging to two complexity classes: solvable in
polynomial time (P) and bounded-error probabilistic polynomial time (BPP). Having an ideal
quantum computer, a natural question arises: what types of problems, if any, can be solved
in a polynomial time on a quantum computer while taking an exponential time on a classi-
cal computer? In this context, another complexity class was deﬁned and, roughly, includes
all problems which can be solved and veriﬁed with a quantum computer in polynomial time
(BQP).

One of the ﬁrst quantum algorithm with an exponential speed-up has been proposed in the
context of discrete Fourier transform. The quantum Fourier transform algorithm [513] per-
forms the discrete Fourier transform on 2n amplitudes using a quantum circuit consisting of
only O(n log(n)) quantum gates. The classical algorithm needs O(n2n) operations to perform
the same task. Another example of an algorithm with such a speed-up is the Shor algorithm
for efﬁcient number factorization [514]. It uses building blocks from the quantum phase esti-
mation algorithm [515] and the quantum Fourier transform to gain an exponential speed-up
with repsect to the best classical algorithm for this task. The Harrow-Hassidim-Lloyd (HHL)

216

TYPEOFALGORITHMTYPEOFDATAQCQQCCCQclassicalquantumclassicalquantum=xi,yiΣil(yi,yi)θθ*training(a)(b)yipredpred(θ)=Physics for deep learning

algorithm [516] is another very famous algorithm that was designed to solve a system of linear
equations

Ax = b,

(8.19)

where A is an n × n sparse matrix with condition number k. The algorithm is able to ﬁnd
the vector x in O(log(n) k2) time instead of the typical O(n2) for standard algorithms. This is
an exponential speed up in the size of the system, however one crucial remark to keep in mind
is that the classical algorithm returns the full solution, while the HHL can only approximate
functions of the solution vector.

Machine learning (ML) algorithms largely rely on linear algebra, which generally consti-
tutes much of machine learning computational cost. For example, the classiﬁcation problem
with a support vector machine (SVM) generally requires quadratic programming (see sec-
tion 2.4.3 for the general idea of SVM) but a special form of support vector machine (SVM)13
boils down to solving a system of linear equations. In this context, quantum computers might
speed up such costly operation. One application of the HHL algorithm have been proposed,
e.g., in the context of SVMs [517] (see [518] for a recent experimental realization on a four-
qubit quantum computer) and data ﬁtting [519]. It is worth noticing that the exact amount of
quantum speed-up provided by the HHL algorithm with respect to classical algorithms is under
debate [520]. Another caveat consist in the fact that the classical data should be efﬁciently
encoded in the quantum algorithm efﬁciently. This is another issue that must be solved by the
community.

8.2.4 Quantum computing in the noisy intermediate-scale quantum era

Until now, we have only considered ideal quantum computers to run quantum algorithms, such
as the Shor, the quantum Fourier transform, and HHL algorithms. However, the realization of
these algorithms for a number of qubits where such advantage matters is not yet feasible in
near-term quantum computers. The main reasons are that (i) quantum computers currently
contain too few qubits (nowadays in the order of hundreds) and (ii) they perform imperfect
operations (noisy).

Furthermore, algorithms such as the Shor algorithm have to be compiled on real devices.
This means that unitaries acting on several qubits have to be decomposed in elementary gates
that can be physically realized in the experimental platform. Such a transformation might lead
to complex quantum circuits with native gates [521]. For example, for the IBM-Q Washington
X , and X gates are native gates. Any other gate must be
platform, only the CNOT, ID, RZ,
decomposed into these gates. Since these gates form a set of universal quantum gates [522],
this is, in theory, sufﬁcient but, in practice, it can lead to very quantum circuits with an impor-
tant number of gates. For example, we consider the decomposition of the Shor algorithm to
these gates, as shown in Fig. 8.13. An apparently simple circuit consists, in practice, of many
operations on real quantum devices. The latter might be especially problematic due to noise
and decoherence that are intrinsically present in the physical devices.

(cid:112)

In modern quantum computers, we can identify three primary sources of errors: gate
errors (generated by a non-precise application of the desired gate), decoherence errors (loss
of coherence of the wave function as a function of time), and read-out errors (erroneous read-
out of the qubits state during the measurement procedure).

13For the special case of least-squares support-vector machine, the problem can be written as a solving a linear

system of equations.

217

Physics for deep learning

Figure 8.13: Realization of the famous Shor algorithm in a real quantum computer.
Top left diagram presents a concise theoretical circuit of this algorithm. Due to the
limitation to certain gates (CNOT and SWAP), generic gates have to be decomposed
and the circuit requires more gates and higher depths.

Due to many different noise and error sources in real quantum computers, we are far
from the fault-tolerant quantum computation. Instead, we are in the so-called noisy
intermediate-scale quantum (NISQ) era [523]. The qubits of the current quantum pro-
cessors are noisy and require quantum error correction. Nevertheless, the study of the
physics of such systems is interesting in itself.
In particular, there might be applica-
tions with a quantum speed-up within this regime, as in the case of the recent quantum
advantage experiment [505].

It is now clear that we cannot run algorithms requiring many gates or implement gates
with low error rates in NISQ circuits. If the circuit contains too many gates, the coherence
gets lost as well as the superposition and entanglement between different qubits. A natural
question arises: Can we design algorithms that perform well on NISQ devices and do not re-
quire fully error corrected quantum computers? This means one has to ﬁnd clever ways to
explore the exponentially big Hilbert space without exact algorithms. One approach is using
quantum computers to generate variational states and to ﬁnd a procedure to converge itera-
tively to the solution instead of taking a direct deterministic path (for example by performing
the optimization on a classical computer). We go into more detail into these variational ap-
proaches in section 8.2.6. Before, in section 8.2.5, we present how NISQ devices can be used
for SVM with kernels.

To sum up, the NISQ era has still many open problems in experimental quantum com-
puting and in quantum error correction. State-of-the-art devices include 50-100 qubits with
error rates of less than 0.5%14. Nonetheless, recent years showed many examples of useful
variational quantum simulations that can be performed with the near-term devices, e.g., see
Ref. [524]. Moreover, many error mitigation routines have been developed to ease the noise ef-
fects in quantum computers, allowing for extraction of useful information from noisy devices

14In the quantum advantage experiment [505], the authors reported single-qubit gate ﬁdelities of 99.85% and

two-qubit gates ﬁdelities of 99.64%

218

H...|0〉|0〉|0〉|1〉...............Ua20Ua21b0bnb1ADDERa0ana1...aa+bUa22n-1QFT-12n===CARRY=T1TT1TTHT1INTEGERFACTORIZATION:SHOR'SALGORITHMHH/n...HTPhysics for deep learning

Figure 8.14: Quantum SVM enhanced by a quantum device. (a) Sketch of the steps
of the SVM enhanced by a quantum kernel. The data are encoded in a quantum
device, such as a quantum circuit, which computes the kernel. These kernels are
then used in classical SVM. (b) Example of a dataset used in Ref. [529] to show the
capacity of quantum kernels. Blue (red) regions correspond to label 1 (0).

in the near term [525–528]. NISQ devices are also an excellent trial ﬁeld to study physics
without building a fault-tolerant quantum computer. Finally, useful applications of NISQ de-
vices can still be found, and they can be considered as a step toward fault-tolerant quantum
computing.

8.2.5 Support vector machines with quantum kernels

We have seen in section 8.2.3 that ideal quantum computers could allow one to accelerate the
numerically costly parts of the support vector machine (SVM) algorithm by implementing the
HHL algorithm. There, the key element has been to use the quantum computer to solve the lin-
ear system of equations. In 2018, three independent works [529–531] followed an interesting
alternative direction: using kernels evaluated directly on quantum devices, while performing
the rest of the SVM algorithm classically.

The idea is sketched in Fig. 8.14(a). Let us consider a dataset that is not linearly separable.
We therefore want to nonlinearly embed it in a higher dimensional space such that the data
is linearly separable in this space (see section 4.2 for more detail). We here use a quantum
device to encode classical data x into a high-dimensional Hilbert space |ψ(x)〉, or even inﬁnite
in the case of squeezed states considered in Ref. [530]. In this case, the choice of the encoding
of the classical state into the quantum state is crucial as it determines the quality of the feature
map. More importantly, quantum devices and in particular quantum circuits can allow for the
efﬁcient computation of the scalar product between two quantum states, which allows one to
deﬁne a quantum kernel

KQ

(xi, x j

) = |〈ψ(xi

)|ψ(x j

)〉|2 = (cid:88)
n

λ

φ

(xi

n

n

)φ

(x j

),

n

(8.20)

which has all the properties of a classical kernel with a feature map φ and deﬁnes an RKHS
(see section 4.1.3).15 As such, quantum kernels can be directly used in classical algorithms
such as kPCA or kSVM or Gaussian processes (GPs) [532] rendering them quantum algorithms.

15The careful reader may notice that Eq. (8.20) is the norm squared of the inner product instead of the typical
inner product expected for kernels. This becomes clearer when writing the kernel in terms of density matrices
(xi, x j
KQ
i and
ρ
j.

). The kernel is then corresponding to the Frobenius inner product of density matrices ρ

) = Tr(ρ

ρ

i

j

219

Physics for deep learning

To be more concrete, we explain the main ingredients of the quantum kernel introduced
in Ref. [529]. Given a data set of points {xi
}, the feature map is deﬁned in
terms of the unitary transformation U(xi) that can be realized in a quantum circuit of qubits

} with labels { yi

xi

(cid:55)→ |ψ(xi

)〉 = U(xi)|0〉,

(8.21)
where |0〉 stands for the product state |0〉⊗n. Typically, the classical data encoding into the
quantum circuit can be done through parametric local rotations of single qubits. The uni-
tary is then built through repeated application of these data dependent gates and other non-
parametric gates, such as entangling gates and Hadamard gates. We do not enter into the
details of the construction of the circuit, but the interested reader can have a look at the fol-
lowing Qiskit tutorial for more details [533] .

The quantum kernel can then be computed on a quantum circuit with the compute-
uncompute trick: one basically implements the following quantum circuit U †(x j
)|0〉 and
measures in the z basis. the frequency of the all-zero outcome estimates, therefore, gives an es-
timate of the kernel KQ

(xi, x j

)U(xi

).

Given these kernels, the optimization of the parameters of the SVM can be performed on
a classical computer (see section 4.2.2) using, e.g., Bayesian optimization [532] presented in
section 4.3. Ref. [529] generated a complex classiﬁcation problem, shown in Fig. 8.14(b),
where the blue (red) region corresponds to label 0 (1). They then generated a training set
by selecting random points these regions and performed the SVM enhanced by the quantum
kernel. The algorithm yields very good results with around 95% of accuracy on the test set for
this synthetic data set.

The previous example shows that quantum kernels can represent complex data sets. Nev-
ertheless, the quantum advantage has yet to be seen for a general data set [534,535]. A recent
important step in this direction has been achieved in Ref. [509], where the authors constructed
data sets that cannot be classiﬁed efﬁciently on a classical computer and in [536] where the
authors have studied supervised learning of handwritten images on quantum computers with
an improved scaling using randomized measurements. Another exciting question concerns the
best ways to build quantum kernels [537], i.e., how should one perform the encoding of the
inputs x (cid:55)→ |ψ(x)〉? This is still a very active line of research and these questions do not have
yet an answer. We refer to [529] for further information.

8.2.6 Variational approaches

This section deals with the optimization of quantum circuits that can be realized in NISQ
devices. In particular, we focus here on the so called variational quantum algorithms. This idea
generalizes the toy example we introduced in section 8.2.2. We deﬁne a parametrized quantum
circuit (PQC), a circuit that depends on a set of parameters {θ} (θ can be, for example, the
angles of single qubit rotations). Then, one deﬁnes an objective function C(θ) that we aim
to minimize. Such an objective function can always be written as a function depending on
a set of observables and on the PQC. Our goal is then to ﬁnd the optimal set of parameters θ∗
minimizing the objective function. Such variational approach has applications in many ﬁelds
such as ML (classiﬁcation, generative models), many-body physics and quantum chemistry
(ground state ﬁnding), combinatorial optimization, etc.

We illustrate the building blocks of the variational approach with the example of the
variational quantum eigensolver (VQE) [538] . Given an Hamiltonian ˆH, the goal of the VQE is
to minimize the energy E = 〈ψ| ˆH|ψ〉. The general principle of the VQE is shown in Fig. 8.15.
The process starts with an initial state which is easy to prepare on the quantum computer,
e.g., the product state |0〉⊗n, which for simplicity we denote by |0〉. This is followed by the

220

Physics for deep learning

Figure 8.15: Variational optimization of quantum circuits, including the initial state,
parametrized quantum circuit (PQC), output, objective, measurement, and classical
optimization.

parametrized quantum circuit (PQC) including, e.g., all the parameters θ of the quantum gates
and which can be seen, at this point, as a black box which prepare a quantum state. In the
ﬁrst iteration, this state is just a random state and is used as the initial state for the expecta-
tion function, which is in general the Hamiltonian of the physical system we want to study, up
to a global phase. The Hamiltonian can describe the interaction within a molecule or a spin
system, but can, in general, be any kind of cost function in operational form that can be writ-
ten in the computational basis of the quantum hardware we are using. The next step is the
minimization of the cost function with a classical subroutine to converge toward the lowest
energy state of the physical system in the space of the quantum sopate that can be reached
by our parametrized quantum circuit (PQC) in a self-consistent manner. We also know that, if
the system is gapped and the ground state unique, the minimal value of the expectation value
of the Hamiltonian is the ground-state energy and the corresponding eigenvalue is the ground
state wave function.16

Going a bit more into detail, for the PQCs we are computing the energy of the ansatz

E0

= min
θ

〈ψ(θ)| ˆH|ψ(θ)〉 = min

〈0|U †(θ) ˆH U(θ)|0〉 ,

θ

where θ are the parameters of the gates which are optimized to minimize the expectation
value. The variational state is the unitary state, i.e., our parametrized quantum circuit (PQC)
applied to the initial state |0〉. However, in order to run and work with the PQC we have to
make several assumptions: ﬁrst, we are assuming the existence of a set of parameters that
approximates the ground state and that our PQC can represent that speciﬁc solution. Second,
that it is possible to converge to the solution without being stuck in local minima and, ﬁnally,
that the circuit can be run on a NISQ computer. Taking all these assumptions into account,
there are two ways to design a PQC. The ﬁrst option is the problem-inspired design which
we can use when we exploit some physical properties of the system we want to represent,
e.g. by using the Hamiltonian representation to design the unitary operation as happens in

16The energy measured at each iteration is an upper bound of the ground state energy, according to the varia-
tional principle. The ability to reach the global minimum, of course, depends on the capacity of the circuit. If the
circuit does not contain the solution, such an ansatz will never reach the minimal energy [539].

221

InitialstateQuantum circuitthat depends on| ψ0 >Output| ψ >Expectationvalue< ψ | H | ψ >E0+ ε min E (θ)θθθNewCLASSICAL OPTIMIZATIONVariational principle: E=< ψ | H | ψ > ≥ E0E.G. VARIATIONAL QUANTUM EIGENSOLVER, CLASSIFIER, AUTOENCODER, QAOA...Parameterizedquantum circuitObjectiveHPhysics for deep learning

Figure 8.16: In this ﬁgure we show an example of a variational quantum simulation.
In sub-ﬁgure (a) we show the parametric quantum circuit used for the simulations.
In sub-ﬁgure (b) we plot the energy of the system during the optimization algorithm.

the variational quantum eigensolver (VQE) algorithm. However, these kinds of ansatz require,
in general, many gates or a particular qubit connectivity, making it unfeasible for bigger sys-
tems in current quantum computers. Another way is the hardware-efﬁcient ansatz, which is
a heuristic method that requires way less quantum gates and that consists of preparing a PQC
that uses the native gate set and respects the quantum computer connectivity.
In general,
problem-inspired ansatz use to be more precise but less feasible to implement in current quan-
tum computers, while this happens otherwise with the hardware-efﬁcient ansatz

The next step after deﬁning the PQC is the choice of the objective function which can be
everything that encodes our problem in a quantum operator, e.g., a Hamiltonian as shown
in Fig. 8.15. The objective function is then decomposed into Pauli strings whose expectation
value can be measured with the quantum computer. The expectation value of a hermitian
operator is computed on the quantum hardware by making the wave function collapse in the
computational basis. From this measurement, we can extract bit strings (i.e.
lists of 0 and
1, e.g., [(0, 0, 1, 1, 0, 0), (1, 0, 1, 0, 0, 0), (0, 0, 1, 0, 1, 0)]). From these bit strings, we can recon-
struct the expectation value of any operator that can be written as the tensor product of Pauli
matrices. This leads to the next important step in the process, the measurement. In this step,
we extract the information from our quantum computer or our quantum devices. These de-
vices project in a particular basis, normally the z-basis, and the measured expectation values
are from Pauli strings and not directly the Hamiltonian or the designed cost function.

The last step is the classical optimization in which we have to navigate through the PQC
parameter space by using, e.g., a gradient-based approach. The gradients can be written in
terms of expectation values of the quantum circuit derivatives with respect to a parameter,
and we do not have direct access to the gradients of the quantum state. In NISQ devices, the
gradient of the PQC can be computed with the parameter shift rule [540]: for each parameter
θ , one can compute exactly its partial derivative by evaluating two PQCs. The problem in
this step is the number of required measurements. In order to run the classical minimization
algorithm, measurements of all gradients are required. Therefore, this method can be very
expensive and includes a huge number of variables and multiple iterations to converge to the
ground state, and that is why other gradient-free methodologies are exploited, like genetic
algorithms or reinforcement learning strategies. After all, the combination of the variational
optimization of the quantum circuit with classical optimization algorithms is an efﬁcient way

222

0100200300400500600OptimizationStep-16-14-12-10-8-6-4-2EnergySimulateddataExactSolutioni11025913146783411121617i(a)(b)Physics for deep learning

to use NISQ devices for real world problems.

Let us discuss a concrete example: the Heisenberg Hamiltonian acting on four spins, which

reads

ˆH =

3
(cid:88)

i=1

J1

σx
i

σx

i+1

+ J2

σ y
i

σ y

i+1

+ J2

σz
i

σz

i+1

+

4
(cid:88)

j=1

h1

σx
i

+ h2

σ y
i

+ h3

σz
i .

(8.22)

We ﬁx the parameters of the Hamiltonian to J = [1, 1, −1] and h = [1, 1.5, 3]. These param-
eters are chosen to be far away from any phase transition point, not to make the problem too
difﬁcult. We can use this Hamiltonian as a benchmark for our algorithm. Let us also consider
a very easy quantum ansatz for the four-qubit case. The ansatz consists in two rotation gates,
applied to every qubits in Y and Z direction, three CNOT gates that connect every qubits, and
two more rotations in Y and Z direction. The circuit is sketched in Fig. 8.16(a).

In variational quantum eigensolvers (VQEs) we want to use the variational circuit to min-
imize the energy of the system. If we did this operation on a classical computer, the compu-
tational cost of the evaluation of the energy would scale exponentially with the number of
qubits. The expectation value of the Hamiltonian of the system can be computed efﬁciently
on a perfect quantum computer. The computational cost is linear in the number of qubits. We
can feed this cost function to an optimizer, in this case we use the Nelder-Mead optimization
routine. In Fig. 8.16(b) we plot the value of the energy of the system as a function of the
optimization step. We can ﬁnd a relatively good approximation of the ground-state energy
with few variational parameters and polynomial computational cost in the number of qubits.
Many things can be improved in these kinds of simulations, both on the design of the varia-
tional ansatz, and on the optimization routine. In the design of variational quantum circuits,
we can, for example, impose symmetries of the system we are studying. In the optimization
routine, we could use stochastic gradient descent or other more efﬁcient algorithms.

8.2.7 Parametrized quantum circuits for quantum machine learning

Classiﬁcation tasks Variational quantum circuits can be used to perform the classiﬁcation of
classical data. The ﬁrst non-trivial task in the construction of such a quantum algorithm is the
loading of the classical data on the quantum hardware. Moreover, the algorithm must be able
to process efﬁciently these data and have a way to perform the classiﬁcation. In Ref. [541] the
authors have shown that PQC with data reuploading can lead to a good classiﬁer. In Ref. [542]
the authors have introduced the concept of quantum convolutional neural networks. As for
classical CNN, these variational quantum circuits have more capacity. In particular, the authors
have shown how these circuits can be used to perform classiﬁcation on symmetry-protected
topological phase in the Haldane chain directly from the quantum state, that can be obtained
with a VQE. This idea has been realized experimentally in a recent work [510]. Moreover,
it is worth to notice that the above-mentioned classiﬁers are closely related to quantum ker-
nels [543, 544]. Finally, the authors of Ref. [545] have developed a recurrent quantum NN
that has been used for classiﬁcation and generation of handwritten digits.

Quantum reinforcement learning (RL) Parametrized quantum circuits (PQCs) can also be
used to realize action-value functions, or RL policies themselves (see section 6.2 for Q-learning
and deep Q-learning) policies (see section 6.3 for an introduction to policy gradient). Two
examples of algorithms that take advantage of PQCs in the context of RL can be found in
Ref. [546,547]. In Ref. [546], the quantum circuit is trained using a policy gradient algorithm
and is used to solve classical environments, i.e., to ﬁnd the optimal policy for the task at hand.
The choice of the action, the probability of which occurring we want to ﬁx, is going to be
encoded in the measured observable – if we have a certain set of actions, we deﬁne a certain

223

Physics for deep learning

set of observables. The RL architecture states that the agent observes a quantum state that the
circuit will produce and the expectation value of such observable is going to encode π(a|s)
– the probability of the action a in a given RL state s, i.e., it corresponds to the policy π. In
Ref. [547] the authors have used the OpenAI Gym [548] examples as benchmark environments
for the variational quantum algorithm for deep Q-learning, as for example, the Cartpole game
(a cart that can move left and right and the agent is trying to balance the pole attached to the
cart). Compared to the classical models (for which the exemplary environments were created),
the quantum models can reach a similar accuracy using much fewer parameters (which alone
does not imply better models but highlights their difference). An interesting and open question
would be to understand whether these results can be generalized to other environments.

Quantum autoencoders In the same spirit as their classical counterpart (see section 2.4.5),
quantum autoencoders [549] are used to compress quantum data on a quantum computer.
Quantum autoencoders act directly on data encoded in qubits and can thus be used to com-
press quantum data without having the need to have a classical representation that would have
an exponential cost. Since there are patterns that classical computation cannot generate, e.g.
entanglement, the quantum version of an autoencoder might be able to recognize patterns be-
yond classical capabilities. The encoding done with quantum autoencoder transform quantum
data into a latent space with fewer qubits. Let us consider a set of quantum data that can be
encoded into n qubit. We want to ﬁnd the representation of k qubit that encodes the properties
of our data. To do so, we minimize a variational cost function. The quantum simulation is per-
formed on n + k qubits. This encoding is done via a variational map represented by a quantum
circuit with a polynomial number of parameters U(θ ). Since the encoding procedure is a uni-
tary (quantum circuit), the decoding operator is just represented by the hermitian conjugate
of U †(θ ). It is still under debate whether this quantum algorithm can have a computational
advantage with respect to their classical counterpart. Variational quantum autoencoders have
also been proposed for various applications, such as quantum data denoising [550], phase
classiﬁcation [114], clustering of the Hilbert space [115], or quantum error correction [551].

Generative models Generative models are algorithms learning the distribution of a data set.
In quantum mechanics, the inherent quantum nature of the devices can be of great help for
learning probability distributions and in particular for quantum wave functions. Recently, di-
verse QML architectures have been proposed: these include quantum Hamiltonian-based mod-
els [552], quantum GANs [553, 554] and quantum Born machines [555–557]. In particular,
quantum circuit Born machines are generative model that can represent classical distribution
of data, represented as pure quantum states. In this context, variational quantum circuits can
provide a useful tool to represent these probabilities distribution and, moreover, an efﬁcient
way to sample from these distributions. The algorithm has comparable performance to its
classical counterpart. In Ref. [557] the authors have proposed yet another quantum circuit
Born machine that can learn the probability distribution of coherent thermal states, where the
probability distribution is given by the Boltzmann weights. Such algorithms can be run on
NISQ devices and are good candidates for quantum advantage in near term.

8.2.8 Current experimental and theoretical limitations

In this last section, we discuss some experimental and theoretical open problems which have
to be overcome for successful applications and use of NISQ devices. One important topic is the
quantum error mitigation, i.e., reducing or compensating errors. This includes classical post-
processing techniques as well as active operations on the hardware itself. The former approach
includes techniques as stabilizer-based approaches which rely on information associated with
conserved quantities as spin or particle number [558], and mitigation scheme based on clas-
sical post-processing of data [525–528]. These methods are, however, only post-processing

224

Physics for deep learning

tools after we ran the circuit. Another way of error mitigation are active mitigation techniques
or quantum optimal control strategies. In contrast to the post-processing techniques, these
methods are directly related to experiments and the quantum hardware [559–564].

We are not only facing experimental but also theoretical open problems which have to be
solved or overcome. One of these problems is the barren-plateau problem [565] which ap-
pears for global cost functions of quantum circuits parametrized with local unitaries. Without
prior knowledge about the solution, the parameters θ of the PQC are initialized randomly.
As a consequence, we obtain a barren-plateau: the expected value of the gradient as well
as the expected value of the variance are exponentially vanishing with the number of qubits
and/or the circuit depth. In other words, this means that the loss landscape is mainly ﬂat, with
a narrow gorge hosting the global minimum [566]. Possible solutions to the barren-plateau
problem consist in using parameters close to the solution, using a local cost function instead
of global ones, or introducing correlations between parameters [565, 566]. The downside of
the latter solutions are that these methods do not work well for strongly correlated systems.
A general solution to the barren-plateau remains still an open theoretical problem. Moreover,
it is totally unclear that there are any natural problems where a PQC will outperform a classical
learning engine [544]. The loss landscape is, furthermore, characterized by the appearance of
many local minima that can be far away from the global minimum [567]. A similar situation is
known for the training of classical NNs [38] – it is an open question whether this observation
poses an actual challenge in practical applications of PQCs.

Another theoretical obstacle includes the capacity of the PQC. When setting a PQC ansatz,
we have to be careful not to narrow the Hilbert space accessible by the PQC too much. If we
do so, we might end up in a wrong area of the Hilbert space and we cannot reach a good
approximation of the solution [539, 568]. There are some measures (e.g. Haar distributions)
but the capacity remains an open problem for the PQC ansatz.

Circuit compilation is another important challenge, involving both theory and experimen-
tal parts: the theoretical circuit, the decomposition into native gates, the simpliﬁcation, and
ﬁnally the mapping to the hardware and real qubit system. The circuit compilation relies on
the Solovay-Kitaev theorem [522,569] which states that with a universal gate set it is possible
to approximate any SU(N) with a circuit of polynomial depth up to a certain accuracy. How-
ever, when it comes to the speciﬁc hardware implementation, some gates are easier to control
than others. In PQC, one always try to use as many native gates as possible. This solution can
make the quantum circuit shorter and simpler.

As a concluding remark, much effort has been devoted toward applications with a quantum
advantage, i.e., where a quantum computer is required using less resources than the classical
counterpart. The study of such algorithms is crucial and will probably require the integration
of quantum devices in high performance computing facilities.

Outlook and open problems

We are currently in the NISQ era. Despite the complex theoretical and experimental chal-
lenges toward fault-tolerant quantum computation, the general objective in the NISQ era is
to understand the possible algorithms that can be implemented in current experimental plat-
forms. While the reduction of error rates affecting qubits and gates efﬁciency developments
is a hard task demanding fundamental scientiﬁc and technological advances, there is a need
to develop software tools to control quantum computers, develop error mitigation techniques
and deﬁne quantum optimal control strategies in the meantime, as well as tools to charac-
terize variational quantum algorithms such as the study of the loss landscape [393, 570], the
entanglement properties [571], etc. Moreover, the development of algorithms taking advan-

225

Physics for deep learning

tage of quantum computers without having a direct classical counterpart is a very interesting
but challenging direction beyond the CQ paradigm.

Further reading

• Biamonte, J. et al. (2017). Quantum machine learning. Nature 549, 195. Very famous

review on the subject of QML.

• Bharti, K. et al. (2022). Noisy intermediate-scale quantum (NISQ) algorithms. Rev. Mod.
Phys. 94, 015004. Review paper focused on the variational quantum circuits simulations
in the NISQ era.

• Li, W. & Deng, D.-L. (2022). Recent advances for quantum classiﬁers. Sci. China: Phys.
Mech. Astron. 65, 220301. Recent review paper on classiﬁcation algorithms with quan-
tum computers.

• Cerezo, M. et al. (2021). Variational quantum algorithms. Nat. Rev. Phys. 3, 625. Very
nice review paper on the recent advances in variational quantum algorithms and their
applications.

• Qiskit tutorial on quantum machine learning.

• Pennylane tutorials and demos on quantum machine learning.

• Tensorﬂow quantum tutorial on quantum reinforcement learning.

226

Conclusion and outlook

9 Conclusion and outlook

In the last decade, ML (and DL in particular) has been intensively studied and has revolution-
ized many topics, including computer vision and natural language processing.

The new toolbox and set of ideas coming from this ﬁeld have also found successful ap-
In particular, ML and DL have been used to tackle problems in
plications in the sciences.
physical and chemical sciences, both in the classical and quantum regime. Their applications
range from particle physics, ﬂuid dynamics, cosmology, many-body quantum systems [29–31],
to quantum computing and quantum information theory [572]. On the other hand, physi-
cists have started to apply tools from statistical physics to try to understand the dynamics re-
lated to the training of DL [461] and are also exploring potential hardware based on quantum
physics [573].

These Lecture Notes aim to introduce physicists and chemists to selected topics in ML
and some of their applications in physics and chemistry. As this ﬁeld is relatively new and
quickly growing, we have decided to focus on explaining key concepts in ML for scientists with
a physics or chemistry background and brieﬂy reviewed some of the possible applications. We
have also discussed how physics can help in gaining a deeper understanding of the intrinsic
mechanisms governing DL and how quantum technologies can be used for data-driven tasks.
The list of topics and applications covered in this book is, of course, not exhaustive. We nev-
ertheless hope that we have been able to convey our enthusiasm for ML applied to quantum
sciences and that we have properly introduced the necessary building blocks needed for the
keen reader to dive into this ﬁeld.

Finally, we would like to summarize the directions explored in this book and share our

view on potential exciting developments.

Characterization and classiﬁcation of trajectories and phases. Researchers have inten-
sively studied different ML and DL methods to tackle the characterization and classiﬁcation of
trajectories and phases. While many of these techniques have been very successful [31, 574],
many challenges remain. Most of the works focused on reproducing known phase diagrams
with supervised learning schemes. The ability to process unlabeled data and apply unsuper-
vised learning or self-supervised learning constitutes a big step forward in assisting physicists
in the discovery of new exotic phases of matter. Furthermore, as the most powerful models
are black boxes, interpretability techniques are essential to help physicists to discover relevant
physical concepts learned by these models. For example, Refs. [133,575] were able to discover
physical concepts or recover conservation laws from trajectories. Another very interesting di-
rection is the classiﬁcation of phases directly from experimental data [97, 131, 434, 576, 577].
It would be interesting to understand the effect of the experimental noise on the classiﬁcation
with respect to simulated data.

Gaussian processes and kernel methods. Gaussian processes (GPs) and kernel-based
regression methods are ML algorithms that are not considered to be suited for large dimen-
sional systems due to their cubic scaling with the size of the training data set [201]. Nonethe-
less, kernel-based methods have proven to be robust regression tools with comparable ac-
curacy to DL methods without the caveat of hyperparameter optimization. They have also
played a signiﬁcant role in the ﬁeld of optimization thanks to the success of Bayesian opti-
mization (BO) [155]. The advancement of kernel-based methods has been focused on two
main challenges: (i) numerical routines for matrix inversion and (ii) more robust kernel func-
tions. The rise of GPUs has allowed for the development of efﬁcient algorithms for kernel-
based methods (e.g., GPyTorch [196]). The accuracy of kernel-based methods is founded

227

Conclusion and outlook

on the learning capacity of the underlying kernel function. While algorithms similar to the
Bayesian information criterion (BIC) have proven to be very useful for the construction of ker-
nels well-suited for the data, the rise of the automatic differentiation (AD) and the ability to
parametrize more complex kernels, along the lines of Ref. [194], offer exciting alternative di-
rections. Finally, kernel-based methods have also been expanded to molecular systems where
a string-based comparison is carried as the kernel function [578] and used for computing the
similarities between Fock states [579].

Neural network quantum states. NN representation of the many-body wave function
appeared to be very successful in predicting ground state properties of the system (such as the
− J2 model [580].
energy), even outperforming state-of-the-art techniques (PEPS) for the J1
There is also great interest in using NQS for time evolution of the many-body wave func-
tion [243, 247, 581], especially in dimensions higher than one. Current challenges are the
generalization of NQS to mixed states, e.g., for open quantum systems, and the implementa-
tions of symmetries in NQS. There is a particular interest in ﬁnding strategies to extend NQS
to fermionic systems. Furthemore, the applications of NQS to ab initio studies of interacting
electrons in continuous space is a promising direction for quantum chemistry and physics ap-
plications [256, 281]. An interesting other direction is to gain a better understanding of the
internal structure and the capacity of NQS [582]. Finally, the application of NQS for quantum
state reconstruction is a very active ﬁeld.

Reinforcement learning. Reinforcement learning (RL) provides a powerful framework
with a broad range of applications in the development of quantum technologies. There is
an ongoing effort to combine RL techniques with experimental setups, which opens a variety
of research avenues. One interesting direction is the real-time control of quantum simula-
tors [351, 356, 583], which may allow us to prepare and study complex phases of matter be-
yond our current capabilities. Similarly, we can enhance NISQ devices with RL-based control
to progress toward fault-tolerant quantum computation [332,341,352,584]. Another possible
direction is the design of experimental platforms with RL, with which we may discover new
approaches for quantum experiments. In Ref. [318], the authors discover new optical setups to
prepare highly entangled quantum states. In a similar fashion, we could explore new quantum
computing architectures, or design new technical devices, for example. On a more theoretical
level, RL is a powerful optimization tool that can help us solve challenging problems either
on its own, or in combination with other established techniques [350]. A general interesting
direction is bridging theoretical and experimental advances, for which we could use RL, for
instance, to design Hamiltonians with certain desired properties of interest [585].

Differentiable programming. The application of differentiable programming (∂ P) might
be very beneﬁcial in physics.
It can be applied to different techniques such as variational
Monte Carlo (for neural quantum state (NQS), see chapter 5), tensor network [368] or mean
ﬁeld [373]. These works show that such algorithms can remove the tedious part of calculating
derivatives while retaining state-of-the-art results. The integration of automatic differentiation
(AD) in other tasks such as differential equation solving is also very promising.

Machine learning for scientiﬁc discovery. Is AI capable of scientiﬁc discovery and un-
derstanding? The hopes and prospects coming from the use of ML in science are gigantic, but
so far achievements that can be called “scientiﬁc discoveries” have been rare. In particular,
the AlphaFold [586] algorithm by DeepMind may truly revolutionize biology and medicine
thanks to its ability to predict the three-dimensional structure of a protein based solely on its
genetic sequence (known as the protein folding problem). Automated and self-driving labs
can change how we do experiments [453]. Another example is an ML-guided selection of pre-
formulated hypotheses presented in Ref. [137]. The authors ﬁrst trained a model on numerical

228

Conclusion and outlook

data coming from two different theories that were hypothesized to underlie the physical sys-
tem in question and then asked the model which theory described the experimental snapshots
of the system better. An exciting approach is using ML to guide scientists to interesting regimes
of the problem as it was done in phase classiﬁcation [116], mathematics [587], and quantum
information [588]. Finally, note that having an omniscient oracle that can predict the outcome
of any process does not a priori provide us with or prove scientiﬁc understanding [35]. Again,
this points toward the key challenge of ML interpretability.

Statistical physics for machine learning. Statistical mechanics and the physicists view
can help shed light on the inner workings of ML. Using computation methods coming from
the physics of disordered systems and the teacher-student modelling of learning problems
have already proven to be a powerful paradigm for studying a central puzzle of modern ML:
generalization of overparametrized models [491, 492].
In parallel, the dynamics of learn-
ing can be studied with these same methods [493, 497], as well as with the Langevin equa-
tion [502]. Tools from statistical mechanics can also be of great help to improve the training
of ML architectures. Recent works in this direction improved the training of restricted Boltz-
mann machines (RBMs) using a physical approach [503,589]. Both the Gardner program and
the teacher-student paradigm have been successfully used to study the capacity of quantum
architectures [498–500] and the generalization of quantum NNs [501].

Potential hardware accelerators based on physical processes. Today, NNs are run on
classical devices. While GPUs have become a game changer in the last decade in this ﬁeld,
the memory, computation time, and the energy used in the current NN architectures are con-
stantly growing and will eventually become a bottleneck. As such, there is a great effort to
ﬁnd new devices implementing NNs in physical devices [590]. The main goal of this research
direction is to construct a physical realization of NNs performing fully parallel and fast op-
erations. Examples of such platforms are optical implementations of NNs [591–598], or ex-
citon–polaritons [599–603]. Another direction has been discussed in section 8.2: the use of
hybrid classical-quantum devices to perform data driven tasks. A crucial point in this direction
is the integration of quantum devices in high performance computing facilities. Finally, recent
works have explored yet another direction coming back to ideas from the early days of classical
ML: designing quantum generalizations of Hopﬁeld networks [604, 605].

229

Conclusion and outlook

Acknowledgments

We thank Hans J. Briegel, Lorenzo Cardarelli, Kacper Cybi´nski, and Mario Krenn for useful
discussions and Fesido Studio Graﬁczne for the graphical design of the Lecture Notes.

Author contributions This manuscript is a result of a unique collaboration born between the
participants and lecturers of the Summer School: Machine Learning in Quantum Physics and
Chemistry which took place in Warsaw, Poland in August-September 2021 and which was or-
ganized by M. Tomza, A. Dauphin, A. Dawid, and M. Lewenstein. All authors of the manuscript
took part in reading and improving its content. In particular:

• “Introduction” was written by A. Dawid with help of M. Płodzie´n, M. Lewenstein,

A. Gresch, R. Koch, B. Requena, and G. Muñoz–Gil.

• “Basics of machine learning” was written by A. Dawid, A. Gresch, and J. Arnold with help

of K. Nicoli, K. Donatella, and M. Płodzie´n.

• “Phase classiﬁcation” was written by J. Arnold, A. Dawid, A. Gresch, R. Koch, and
M. Płodzie´n, based on the scientiﬁc content provided by E. Greplová, P. Huembeli, and
S. Wetzel.

• “Gaussian processes and other kernel methods” was written by A. Gresch, A. Dawid,
K. Nicoli, J. Arnold, and R. A. Vargas-Hernández based on the scientiﬁc content pro-
vided by R. Krems.

• “Representing quantum states with neural networks” was written by K. Donatella, B. Re-
quena, and P. Stornati with help of R. Okuła and M. Płodzie´n based on the scientiﬁc
content provided by G. Carleo, J. Carrasquilla, and F. Vicentini.

• “Reinforcement learning” was written by B. Requena, M. Płodzie´n, and A. Gresch with
help of R. Okuła and G. Muñoz–Gil based on the scientiﬁc content provided by V. Dunjko,
F. Marquardt, and E. van Nieuwenburg.

• “Differentiable programming” was written by J. Arnold, “Generative models” - by
K.A. Nicoli, M. Płodzie´n, and K. Donatella, “Machine learning for experimental data” -
by M. Büttner, R. Koch, and A. Dawid, based on the scientiﬁc content provided by J. Car-
rasquilla, E. Greplová, and L. Wang.

• “Statistical physics for machine learning” was written by A. Dawid with help of
M. Płodzie´n based on the scientiﬁc content of M. Gabrié, and “Quantum machine learn-
ing” was written by P. Stornati, A. Dauphin, and R. Koch with help of R. Okuła, based on
the lectures of A. Cervera-Lierta and V. Dunjko.

• Finally, “Conclusions” were written by A. Dauphin, B. Requena, M. Płodzie´n, and

A. Dawid with help of all the co-authors.

The project was led by A. Dawid and supervised by A. Dauphin with help of M. Lewenstein
and M. Tomza.

Funding information An.D. acknowledges the ﬁnancial support from the National Science
Centre, Poland, within the Preludium grant No. 2019/33/N/ST2/03123 and the Etiuda grant
No. 2020/36/T/ST2/00588. J.A. acknowledges ﬁnancial support from the Swiss National Sci-
ence Foundation (SNSF). A.G. acknowledges ﬁnancial support from the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) - project number 441423094. M.P. ac-
knowledges the support of the Polish National Agency for Academic Exchange, the Bekker

230

Mathematical details on principal component analysis

programme no: PPN/BEK/2020/1/00317. K.A.N. acknowledges support by the Federal Min-
istry of Education and Research (BMBF) for the Berlin Institute for the Foundations of Learning
and Data (BIFOLD) (01IS18037A). R.K acknowledges ﬁnancial support from the Academy of
Finland Projects No. 331342 and No. 336243. G.M-G. acknowledges support from the Aus-
trian Science Fund (FWF) through SFB BeyondC F7102. A.C-L. acknowledges the support
by the Ministry of Economic Affairs and Digital Transformation of the Spanish Government
through the QUANTUM ENIA project call - QUANTUM SPAIN project, and by the European
Union through the Recovery, Transformation and Resilience Plan - NextGenerationEU within
the framework of the Digital Spain 2025 Agenda. M.G. acknowledges funding as an Hi!Paris
Chair Holder. L.W. is supported by the Strategic Priority Research Program of the Chinese
Academy of Sciences under Grant No. XDB30000000 and National Natural Science Foun-
dation of China under Grant No. T2121001. M.T. acknowledges the ﬁnancial support from
the Foundation for Polish Science within the First Team programme co-ﬁnanced by the EU
Regional Development Fund. Al.D. acknowledges the ﬁnancial support from a fellowship
granted by la Caixa Foundation (ID 100010434, fellowship code LCF/BQ/PR20/11770012).
This project has received funding from the European Union’s Horizon 2020 research and
innovation program under the Marie Sklodowksa-Curie grant agreement No. 895439 ‘Con-
QuER’. ICFO group acknowledges support from: ERC AdG NOQIA; Agencia Estatal de Inves-
tigación (R&D project CEX2019-000910-S, funded by MCIN/ AEI/10.13039/501100011033,
Plan National FIDEUA PID2019-106901GB-I00, FPI, QUANTERA MAQS PCI2019-111828-2,
Proyectos de I+D+I “Retos Colaboración” QUSPIN RTC2019-007196-7); Fundació Cellex;
Fundació Mir-Puig; Generalitat de Catalunya through the European Social Fund FEDER
and CERCA program (AGAUR Grant No. 2017 SGR 134, QuantumCAT U16-011424, co-
funded by ERDF Operational Program of Catalonia 2014-2020); EU Horizon 2020 FET-
OPEN OPTOlogic (Grant No 899794); National Science Centre, Poland (Symfonia Grant
No. 2016/20/W/ST4/00314); European Union’s Horizon 2020 research and innovation pro-
gramme under the Marie-Skłodowska-Curie grant agreement No 101029393 (STREDCH) and
No 847648 (“La Caixa” Junior Leaders fellowships ID100010434: LCF/BQ/PI19/11690013,
LCF/BQ/PI20/11760031, LCF/BQ/PR20/11770012, LCF/BQ/PR21/11840013). Research at
Perimeter Institute is supported in part by the Government of Canada through the Department
of Innovation, Science and Economic Development Canada and by the Province of Ontario
through the Ministry of Economic Development, Job Creation and Trade. We thank the Na-
tional Research Council of Canada for their partnership with Perimeter on the PIQuIL.

A Mathematical details on principal component analysis

We can motivate PCA from two different perspectives: The ﬁrst one is sketched in the main text
and is based on retaining the largest possible data variance when reducing the dimensionality
of the data. As such, it corresponds to a constrained maximization problem. PCA can also be
motivated as the algorithm which ﬁnds a low-rank approximation ˆX to the design matrix X
such that the distance between the two matrices is minimized. We will prove that these two
approaches are equivalent. That is, we show that the projection matrix V is the solution to

V = arg min

(cid:107)X − V Z(cid:107)2

min
Z

1
N
(cid:123)(cid:122)
min. error in high-dim. space

(cid:125)

V

= arg max

(cid:107)V (cid:252)X(cid:107)2.

1
N
(cid:123)(cid:122)
max. variance in low-dim. space

(cid:125)

(cid:124)

V

(A.1)

(cid:124)

Because we deal with matrices, the norm refers to the Frobenius norm (cid:107)A(cid:107)2 := tr [A(cid:252)A].

Let us recap the approach from the main text, i.e., the variance maximization. We de-
ﬁne the design matrix X from the N p-dimensional data points by stacking them together.

231

Mathematical details on principal component analysis

However, for the mathematical proofs, we deﬁne it in its transposed version as an (p × N )
matrix. We construct the empirical covariance matrix Σ (assuming zero mean in the data) as
Σ = (XX (cid:252))/N = Σ(cid:252). It contains all covariances between any two input features. We wish to
ﬁnd a linear transformation V that preserves the maximal variance of the data. As we assume
data with zero mean, the projected data V (cid:252)X also has zero mean. The empirical variance of
the input data is given by (cid:80)N
/N . We want to ﬁnd the columns of the projection matrix
iteratively. The ﬁrst column v1 of V is obtained by maximizing the variance (cid:80)N
)2/N .
i=1
This can be summarized by the following optimization problem:

i=1 x2

(cid:252)
1xi

(v

i

max
v1

1
N

(cid:13)
(cid:13)v

(cid:252)

1X(cid:13)
2 = max
(cid:13)
v1

1
N

(cid:252)

1XX (cid:252)v1
v

= max
v1

(cid:252)
v
1

Σv1

s.t. v

(cid:252)
1v1

= 1.

(A.2)

Here, the constraint enforces a ﬁnite value for the maximum and we have inserted the
deﬁnition of the norm (of vectors here) and of the covariance matrix. We solve the con-
strained optimization problem using the method of Lagrange multipliers. To this end,
we deﬁne the Lagrangian L(v1
− 1) and calculate its differential as
− µ
(cid:252)
(cid:252)
dL = 2(v
1v1.
1v
1
1
We identify the eigenvalue problem, i.e., v1 must be an eigenvector of Σ with eigenvalue µ
1.
Choosing µ

(cid:252)
(v
1v1
)dv1. The optimal solution requires dL = 0. This is fulﬁlled, if Σv1

1 of Σ then maximizes our objective.

1 to be the largest eigenvalue λ

(cid:252)
) = v
1

Σv1

Σ−µ

= µ

1

2v

− 2µ

(cid:252)
1v2

− κv1

For the next column v2 of V , we start from Eq. (A.2) and enforce orthogonality be-
(cid:252)
= 0. We can write the modiﬁed Lagrangian
tween v1 and v2 by adding the constraint v
1v2
and calculate its differential with respect to dv2. This leaves us with the condition that
= 0 with κ being the Lagrange multiplier for the orthogonality con-
2Σv2
(cid:252)
dition. Multiplying both sides with v
1 from the left and applying the orthogonality condition
we ﬁnd that κ = 0. Plugging this into the previous condition, we again arrive at Σv2
2v2.
2 has to be the second-largest eigenvalue λ
With the same reasoning as before, we see that µ
2
of Σ with its corresponding eigenvector v2. Iteratively, we can the other entries of V as the
remaining eigenvectors of Σ ordered by their eigenvalues.

= µ

We now understand the reason behind the procedure discussed in the main text and why
we can drop the eigenvectors that carry the least variance to achieve a dimensionality reduc-
tion. The dimensionality-reduced data now has a variance spread along each axis according
to the respective PCs. This spread can also be transformed to unit variance along each axis by
modifying the projected design matrix as ˜Xwhite
which is called whitening of the
data. Here, ˜Λ = diag(λ
2, . . . ) is the diagonal matrix with the k largest eigenvalues of Σ in
1, λ
descending order.

= X ˜V ˜Λ−1/2

As mentioned earlier, there is another, equivalent approach to ﬁnd V by minimizing the
approximation error between the design matrix X and its low-rank reconstruction V ˜X. The
mean-squared error (MSE) between the two matrices is given by the following constrained
optimization problem:

min
V ,Z

(cid:107)X − V Z(cid:107)2 = min
V ,Z

tr [(X − V Z)(cid:252)(X − V Z)]

s.t. V (cid:252)V = 1

(A.3)

where we inserted the deﬁnition of the Frobenius norm. The constraint can be placed without
loss of generality: assume that V (cid:252)V = A (cid:54)= 1. Consider the eigenvalue decomposition of
A = W ΛW (cid:252) with W (cid:252)W = 1. Thus, V (cid:252)V = A = W ΛW (cid:252) ⇒ Λ = (V W )(cid:252)(V W ) and we
recover our constraint by setting ˜V = Λ−1/2V W and minimize over ˜V instead.

We solve this again with Lagrange multipliers. However, we now have a matrix constraint
and therefore introduce the matrix-valued Lagrange multiplier M (cid:252). Since distances between
matrices are given by the Frobenius norm, the Lagrangian reads as

L(V , Z) = tr [(X − V Z)(cid:252)(X − V Z)] − tr [M (V (cid:252)V − 1)] .

232

Derivation of the kernel trick

Using matrix calculus, the differential is dL = −2 tr [(X − V Z)(cid:252)V dZ]. Setting it to zero,
we require that X (cid:252)V = Z(cid:252)V (cid:252)V = Z(cid:252) and thus Z = V (cid:252)X which we plug into the objective
as tr [(X − V Z)(cid:252)(X − V Z)] = tr [X (cid:252)X − X (cid:252)V V (cid:252)X]. The ﬁrst term can be dropped
as it does not depend on the minimization parameter V . We can rewrite the second term
as −(cid:107)V (cid:252)X(cid:107) and absorb the minus sign by turning the minimization into a maximization of
(cid:107)V (cid:252)X(cid:107). This is exactly the objective of the variance maximization principle Eq. (A.2) and we
see their equivalence. In our derivation, we did not discuss how the Lagrange multiplier M
disappears. The reasoning, however, is similar to before where the eigenvalue decomposition
of M has to be considered. This effectively only adds a rotation of V which can again be
absorbed into the deﬁnition of V .

B Derivation of the kernel trick

Here, we present a derivation of the kernel trick. The training data D = {(X, y)} is deﬁned as

X =













(cid:252)
x
1
(cid:252)
x
2
...
x(cid:252)
n

=







x1,1
x2,1
...
xn,1

x1,2
x2,2
...
xn,2

· · ·
· · ·
...
· · ·







x1,m
x2,m
...
xn,m

and y =



,











y1
y2
...
yn

(B.1)

where each row of X (i.e., xi) is one data point associated with an observable yi, n is the
number of data points, and m is the number of features. Let us consider a linear model
f (x, θ) = x(cid:252)θ as in section 2.4.1. In ridge regression, the loss function is then given as

L(θ, X, y) = (cid:107) f (X, θ) − y(cid:107)2
2
+ λ (cid:107)θ(cid:107)2
2 ,

= (cid:107)Xθ − y(cid:107)2
2

+ λ (cid:107)θ(cid:107)2
2

(B.2)

where (cid:107)·(cid:107)
L(θ, X, y) with respect to θ,

2 denotes the (cid:96)

2-norm. The optimal set of parameters θ∗ is found by minimizing

θ∗ = arg min
θ

L(θ, X, y)

= arg min

θ

(cid:107)Xθ − y(cid:107)2
2

+ λ (cid:107)θ(cid:107)2
2 .

(B.3)

To validate that θ∗ is the optimal solution of L, we can verify that ∇θL(cid:12)
(cid:12)θ∗ = 0. Given the
linear dependence of θ in f , ∇θL has a closed-form solution. Before we proceed with the
derivation, let us ﬁrst expand Eq. (B.2):

L(θ, X, y) = (Xθ − y)(cid:252) (Xθ − y) + λθ(cid:252)θ

= θ(cid:252)X (cid:252)Xθ − θ(cid:252)X (cid:252)y − y(cid:252)Xθ + y(cid:252)y + λθ(cid:252)θ.

Solving for θ∗ by setting the gradient of L w.r.t. θ to zero, we get

∇θL = 0 = 2X (cid:252)Xθ − 2y(cid:252)X + y(cid:252)y + λθ.

(B.4)

(B.5)

Please consult Ref. [606] for the derivative identities needed to derive Eq. (B.5). Solving for
θ, we obtain

(X (cid:252)X + λ1) θ = X (cid:252)y,

where

θ∗ = (X (cid:252)X + λ1)−1 X (cid:252)y.

233

(B.6)

(B.7)

Derivation of the kernel trick

Before we proceed further, let us examine the X (cid:252)X term

X (cid:252)X = (cid:2)x1 x2

· · · xn

(cid:3)













(cid:252)
x
1
(cid:252)
x
2
...
x(cid:252)
n

=







x1,1
x1,2
...
x1,m

· · ·
· · ·
...
· · ·













xn,1
xn,2
...
xn,m

x1,1
x2,1
...
xn,1

· · ·
· · ·
...
· · ·



.





x1,m
x2,m
...
xn,m

(B.8)

Here, X (cid:252)X is a (m × m) matrix, where the matrix elements represent the dot-product in the
“number-of-data-points” space. Here, θ∗ is an m-dimensional vector.

The optimal solution θ∗ could be rewritten as

θ∗ = X (cid:252) (XX (cid:252) + λ1)−1 y,

where we used the following matrix identity [606]

(AB + 1)−1 A = A (BA + 1)−1 .

In the same manner, let us examine the term XX (cid:252) given by

XX (cid:252) =







x1,1
x2,1
...
xn,1

· · ·
· · ·
...
· · ·













x1,m
x2,m
...
xn,m

x1,1
x1,2
...
x1,m

· · ·
· · ·
...
· · ·



.





xn,1
xn,2
...
xn,m

(B.9)

(B.10)

(B.11)

As we can observe, the matrix elements now represent the standard dot-product between two
(cid:252)
i x j. A disadvantage of using Eq. (B.9) is that inverting XX (cid:252), a n× n
points of training data, x
matrix, becomes computationally more expensive when n (cid:29) m.

By using Eq. (B.10) to rewrite θ∗ (Eq. (B.7)) into Eq. (B.9), the prediction of a new point

xnew becomes

f (xnew, θ∗) = (θ∗)(cid:252)

xnew

= x(cid:252)

newθ∗
(XX (cid:252) + λ1)−1 y
(cid:125)
(cid:123)(cid:122)
(cid:124)
parameters

,

= x(cid:252)

newX (cid:252)
(cid:124) (cid:123)(cid:122) (cid:125)
kernel

(cid:252)
where the term (XX (cid:252) + λ1)−1 y represents the optimal parameters of the model, and x
newX (cid:252)
(cid:252)
newX (cid:252)
is the representation of xnew in feature space of the training data. For a linear model, x
is computed by the dot-product between the point where the function is evaluated and the
training data,

newX (cid:252) = (cid:2)x new
x(cid:252)

1

x new
2

· · ·

(cid:3)

x new
m







x1,1
x1,2
...
x1,m

· · ·
· · ·
...
· · ·







xn,1
xn,2
...
xn,m

=







(cid:252)
new x1
x
(cid:252)
new x2
x
...
(cid:252)
new xn
x



.





(B.12)

From Eq. (B.12) and Eq. (B.12), we can observe that a second linear model over the
(cid:252)
(cid:8)x
newxi

(cid:9)n
i=1 feature space could be deﬁned,

f (xnew, θ∗) = (θ∗)(cid:252) (cid:0)x(cid:252)

newX (cid:252)(cid:1) ,

(B.13)

where θ∗ is (XX (cid:252) + λ1)−1 y corresponding to an n-dimensional vector.

234

Choosing the kernel matrix as the covariance matrix for a Gaussian process

The initial model considered is a linear model on x, f (x, θ) = x(cid:252)θ. However, we could
1, . . . , φ(cid:96)] spanning an alternative feature
0, φ
(x), . . . , φ(cid:96)(x)], i.e., we trans-

consider a linear model over a basis-set Φ = [φ
space. If we replace in our derivation x for Φ(x) = [φ
form our data into the corresponding feature space, we end up with the following model

(x), φ

0

1

f (Φ(xnew

), θ∗) = (θ∗)(cid:252) Φ(xnew
= Φ(xnew

) = Φ(xnew

)(cid:252)θ∗

)(cid:252)Φ(X)(cid:252) (Φ(X)Φ(X)(cid:252) + λ1)−1 y.

)(cid:252)Φ(X)(cid:252) corresponds to the dot-product in the basis-set expansion between xnew
Here, Φ(xnew
and the training data X. Moreover, Φ(X)Φ(X)(cid:252) corresponds to the dot-product in the basis-
set expansion between all training data points, i.e., [Φ(X)Φ(X)(cid:252)]

).

= Φ(xi

)(cid:252)Φ(x j

i j

In the context of kernel methods Φ(X)Φ(X)(cid:252) is known as the design matrix K. It should
), θ∗) does only depend on the basis-set expan-
be stressed that, the computation of f (Φ(xnew
). This enables the kernel trick. Finally, our derivation
sion via the dot-product Φ(xi
was done for KRR, however, the logarithm of the likelihood of a GP (Eq. (4.43)) has the same
algebraic form. Therefore, our derivation also illustrates how GP models operate via kernels.

)(cid:252)Φ(x j

C Choosing the kernel matrix as the covariance matrix for a Gaus-

sian process

The covariance function is a crucial quantity in the context of Gaussian process regression
(GPR) as it encodes some preexisting assumptions on the target function we aim to learn or
on the noise affecting the targets. In chapter 4, we have discussed how the covariance function
of a GP can be expressed in terms of the kernel function. Within this context, the notion of
similarity between data points is of great relevance, and kernels are the most suitable tool to
incorporate this notion of proximity of the data in the covariance function of the regressor. As
such, one can choose the covariance of the GP prior to be the kernel matrix K given by the
kernel function K as

Cov(xi, xj) = Ki j

= K(xi, xj) ,

which is equivalent to the choice of a Gaussian prior over the parameters

p(θ)

prior

∼ N (0, Σ

) .

m+1

(C.1)

(C.2)

To see this, let us start with the linear model of Eq. (2.23) and map the input x to the

feature space using the feature map φ such that

f (x) = φ(x)(cid:252)θ .

The expectation value over the parameters θ can be computed as

(cid:69) [ f (x)] = φ(x)(cid:252)(cid:69) [θ] = 0 .

Using this result, the covariance can be expressed as

Cov(xi, xj) = (cid:69) (cid:2) f (xi) f (xj)(cid:3)

= φ(xi)(cid:252)(cid:69) [θθ(cid:252)] φ(xj)
= φ(xi)(cid:252)Σ
m+1
= φ(cid:48)(xi)(cid:252)φ(cid:48)(xi) .

φ(xj)

235

(C.3)

(C.4)

(C.5)

Choosing the kernel matrix as the covariance matrix for a Gaussian process

m+1

The last step uses the fact
Σ

i.e.
= U ΛU (cid:252) which, in turn, allows to deﬁne φ(cid:48)(x) := Λ1/2U (cid:252)φ(x). Hence, we can apply
the kernel trick to the covariance matrix and promote it to the kernel function K as done in
Eq. (C.1). Taking Eq. (C.4) together with Eq. (C.5), we also verify Eq. (C.2).

m+1 allows for an eigenvalue decomposition,

that Σ

Our goal is to evaluate the marginal likelihood p(y | X)

p(y | X) =

(cid:90)

Rm+1

p(y | θ, X)p(θ | X) dθ .

(C.6)

By marginal likelihood we refer to the marginalization over the model’s parameters θ, i.e., per-
forming the integral in Eq. (C.6). As we see shortly, the marginal likelihood can be expressed
in terms of the kernel matrix. Under the GP model, the prior is Gaussian with zero mean and
variance given by the kernel matrix K, i.e., θ | X ∼ N (0, K). We can write the logarithm of
the prior as

log p(θ | X) = log

exp(cid:8)− 1

2 θ(cid:252)K −1θ(cid:9)

(cid:112)(2 π)n |K|

= − 1
2

θ(cid:252)K−1θ − 1
2

log |K| − n
2

log 2π ,

(C.7)

where |K| denotes the determinant of K.

Since the likelihood, the ﬁrst term in the integrand in Eq. (C.6), is a factorized Gaussian
with y | θ ∼ N (θ, σ21), the integrand reduces to a product of two Gaussians, i.e., becomes
itself Gaussian and we can readily apply well known relations for a product of two Gaussians.
In particular, given two Gaussian distributions N (a, A) and N (b, B) the following result holds

N (a, A) N (b, B) = Z

−1N (c, C)

(C.8)

where c = C (cid:0)A−1a + B−1b(cid:1) and C = (A−1 + B−1)−1. In the equation above, the matrices
{A, B} represent the variances of the two Gaussian while {a, b} are the corresponding means.
The resulting Gaussian thus has a variance equal to the inverse of the sum of the inverse vari-
ances and a mean equal to the convex sum of the means weighted by their precision matrices
(inverse of the variance). The normalizing constant Z −1 looks itself like a Gaussian as

−1 = (2π)− n

2 |A + B|− 1

Z

2 exp

(cid:167)(cid:129)

− 1
2

(a − b)(cid:252)(A + B)−1(a − b)

(cid:139)(cid:170)

.

(C.9)

Leveraging this result, we perform the integration in Eq. (C.6) and refer to Ref. [201] for
further details and proofs. In order to apply the results from Eqs. (C.8) and (C.9) the set of
means and variances from Eq. (C.8), i.e. {a, b} and {A, B}, for our speciﬁc problem become
{θ, 0} and {σ21, K} respectively. Following the derivation we obtain a closed-form solution
for the marginal likelihood as

log p(y | X) = − 1
2

y(cid:252) (cid:0)K + σ21(cid:1)−1

y − 1
2

log

(cid:12)K + σ21(cid:12)
(cid:12)

(cid:12) − n
2

log 2π .

(C.10)

For completeness, we should mention that this same result could have been obtained by notic-
ing that y | X ∼ N (0, K + σ21). In conclusion, we ﬁnd that training a GP reduces to ﬁnding
the parameters of the kernel function K which maximize the logarithm of the marginal log-
likelihood from Eq. (4.62).

236

REFERENCES

References

[1] C. Williams, A brief introduction to artiﬁcial intelligence, In Proceedings OCEANS ’83, pp.

94–99, doi:10.1109/OCEANS.1983.1152096 (1983).

[2] R. Dearden and C. Boutilier, Abstraction and approximate decision-theoretic planning,

Artif. Intell. 89(1), 219 (1997), doi:10.1016/S0004-3702(96)00023-9.

[3] J.-D. Zucker, A grounded theory of abstraction in artiﬁcial intelligence, Phil. Trans. R.

Soc. Lond. B 358(1435), 1293 (2003), doi:10.1098/rstb.2003.1308.

[4] L. Saitta and J.-D. Zucker, Abstraction in Artiﬁcial Intelligence and Complex Systems,
Springer, New York, NY, ISBN 978-1-4614-7051-9, doi:10.1007/978-1-4614-7052-6
(2013).

[5] M. Mitchell, Abstraction and analogy-making in artiﬁcial intelligence, Ann. N. Y. Acad.

Sci. 1505(1), 79 (2021), doi:10.1111/nyas.14619.

[6] H. Moravec, Mind Children: The Future of Robot and Human Intelligence, Harvard Uni-
versity Press, Cambridge, Massachusetts, ISBN 0674576160, doi:10.2307/1575314, P.
15 (1988).

[7] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning, The MIT Press,

ISBN

0262035618, doi:10.5555/3086952 (2016).

[8] G. Marcus, Deep learning is hitting a wall, Nautilus, Accessed: 2022-03-11 (2022).

[9] T. J. Sejnowski, The Deep Learning Revolution: Machine Intelligence Meets Human Intel-

ligence, The MIT Press, ISBN 9780262038034 (2018).

[10] Y. LeCun, Y. Bengio and G. Hinton, Deep learning, Nature 521(7553), 436 (2015),

doi:10.1038/nature14539.

[11] G. E. Hinton, S. Osindero and Y.-W. Teh, A fast learning algorithm for deep belief nets,

Neural Comput. 18(7), 1527 (2006), doi:10.1162/neco.2006.18.7.1527.

[12] B. Marr, How much data do we create every day? The mind-blowing stats everyone should

read, Forbes, Accessed: 2018-05-21.

[13] SeedScientiﬁc, Volume of data/information created, captured, copied, and consumed

worldwide from 2010 to 2025, SeedScientiﬁc, Accessed: 2022-01-28 (2021).

[14] Statista Research Department, Volume of data/information created, captured, copied,

and consumed worldwide from 2010 to 2025, Statista, Accessed: 2022-03-18 (2022).

[15] M. F. Dixon, I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to

Practice, Springer, ISBN 9783030410674, doi:10.1007/978-3-030-41068-1 (2020).

[16] J. Eisenstein,

Introduction to Natural Language Processing, The MIT Press,

ISBN

9780262042840 (2019).

[17] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin and I. Sutskever, Formal mathe-

matics statement curriculum learning (2022), arXiv:1602.04915.

[18] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie et al., Human-
level control through deep reinforcement learning, Nature 518(7540), 529 (2015),
doi:10.1038/nature14236.

237

REFERENCES

[19] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H.
Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan et al., Grandmaster level in
StarCraft II using multi-agent reinforcement learning, Nature 575(7782), 350 (2019),
doi:10.1038/s41586-019-1724-z.

[20] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe et al.,
Mastering the game of Go with deep neural networks and tree search, Nature 529(7587),
484 (2016), doi:10.1038/nature16961.

[21] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, Gradient-based learning applied to document

recognition, Proc. IEEE 86(11), 2278 (1998), doi:10.1109/5.726791.

[22] R. A. Fisher, The use of multiple measurements in taxonomic problems, Ann. Eug. 7, 179

(1936), doi:10.1111/j.1469-1809.1936.tb02137.x.

[23] A. Krizhevsky, Learning multiple layers of features from tiny images, Tech. rep., MIT &

NYU, CiteSeerX 10.1.1.222.9220 (2009).

[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A. Khosla, M. Bernstein, A. C. Berg and L. Fei-Fei, ImageNet large scale visual recognition
challenge, Int. J. Comput. Vis. 115(3), 211 (2015), doi:10.1007/s11263-015-0816-y.

[25] D. Gissin, Active learning review, GitHub.io, Accessed: 2022-04-08 (2020).

[26] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen and X. Wang, A survey

of deep active learning, doi:10.1145/3472291 (2021).

[27] J. E. van Engelen and H. H. Hoos, A survey on semi-supervised learning, Mach. Learn.

109(2), 373 (2020), doi:10.1007/s10994-019-05855-6.

[28] B. Blaiszik, Charting ML publications in science, GitHub repository (2021).

[29] V. Dunjko and H. J. Briegel, Machine learning & artiﬁcial intelligence in the quan-
tum domain: a review of recent progress, Rep. Prog. Phys. 81(7), 074001 (2018),
doi:10.1088/1361-6633/aab406.

[30] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto and
L. Zdeborová, Machine learning and the physical sciences, Rev. Mod. Phys. 91, 045002
(2019), doi:10.1103/RevModPhys.91.045002.

[31] J. Carrasquilla, Machine learning for quantum matter, Adv. Phys.: X 5(1), 1797528

(2020), doi:10.1080/23746149.2020.1797528.

[32] Summer School: Machine Learning in Science and Technology, GitHub repository with

selected tutorials from the school (2021).

[33] A. Dawid, J. Arnold, B. Requena, A. Gresch, M. Płodzie´n, K. Donatella, K. Nicoli, P. Stor-
nati, R. Koch, M. Büttner, R. Okuła, G. Muñoz–Gil et al., GitHub repository with ﬁgures
prepared for these Lecture Notes (2022).

[34] F. Chollet, On the measure of intelligence (2019), arXiv:1911.01547.

[35] M. Krenn, R. Pollice, S. Y. Guo, M. Aldeghi, A. Cervera-Lierta, P. Friederich, G. dos
Passos Gomes, F. Häse, A. Jinich, A. Nigam, Z. Yao and A. Aspuru-Guzik, On scientiﬁc
understanding with artiﬁcial intelligence (2022), arXiv:2204.01467.

238

REFERENCES

[36] R. Bagheri, Weight initialization in deep neural networks, Towards Data Science, Ac-

cessed: 2022-02-16 (2020).

[37] T. Akiba, S. Sano, T. Yanase, T. Ohta and M. Koyama, Optuna: A next-generation hyper-

parameter optimization framework, doi:10.1145/3292500.3330701 (2019).

[38] A. L. Blum and R. L. Rivest, Training a 3-node neural network is NP-complete, Neural

Netw. 5(1), 117 (1992), doi:10.1016/S0893-6080(05)80010-3.

[39] H. Li, Z. Xu, G. Taylor, C. Studer and T. Goldstein, Visualizing the loss landscape of
In NIPS 2018 - Adv. Neural. Inf. Process. Syst., pp. 6389–6399 (2018),

neural nets,
arXiv:1712.09913.

[40] Y. Feng and Y. Tu, The inverse variance–ﬂatness relation in stochastic gradient de-
Proc. Natl. Acad. Sci. 118(9) (2021),

for ﬁnding ﬂat minima,

scent is critical
doi:10.1073/pnas.2015617118.

[41] J. D. Lee, M. Simchowitz, M. I. Jordan and B. Recht, Gradient descent only converges to

minimizers (2016), arXiv:1602.04915.

[42] A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous and Y. LeCun, The loss surfaces of
multilayer networks, In AISTATS 2015 - Int. Conf. Artif. Intell. Stat., vol. 38, pp. 192–204.
PMLR (2015), arXiv:1412.0233.

[43] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli and Y. Bengio, Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization, In
NIPS 2014 - Adv. Neural. Inf. Process. Syst., vol. 27 (2014), arXiv:1406.2572.

[44] L. Sagun, L. Bottou and Y. LeCun, Eigenvalues of the Hessian in deep learning: singularity

and beyond (2016), arXiv:1611.07476.

[45] G. Alain, N. Le Roux and P. A. Manzagol, Negative eigenvalues of the Hessian in deep
neural networks, In ICLR 2018 - Int. Conf. Learn. Represent. (2018), arXiv:1902.02366.

[46] I. Sutskever, J. Martens, G. Dahl and G. Hinton, On the importance of initialization and
momentum in deep learning, In ICML 2013 - 30th Int. Conf. Mach. Learn., vol. 28, pp.
1139–1147 (2013).

[47] Y. Liu, Y. Gao and W. Yin, An improved analysis of stochastic gradient descent with mo-

mentum (2020), arXiv:2007.07989.

[48] J. Duchi, E. Hazan and Y. Singer, Adaptive subgradient methods for online learn-
J. Mach. Learn. Res. 12, 2121–2159 (2011),

ing and stochastic optimization,
doi:10.5555/1953048.2021068.

[49] D. P. Kingma and J. Ba,

Adam: A method for stochastic optimization (2014),

arXiv:1412.6980.

[50] Z. Zhang,

Improved Adam optimizer for deep neural networks,

In 2018 IEEE/ACM
26th Int. Symp. Qual. Serv. IWQoS 2018, pp. 1–2, doi:10.1109/IWQoS.2018.8624183
(2018).

[51] C. Zhu, R. H. Byrd, P. Lu and J. Nocedal, Algorithm 778: L-BFGS-B, ACM Trans. Math.

Softw. 23(4), 550 (1997), doi:10.1145/279232.279236.

239

REFERENCES

[52] L. M. Rios and N. V. Sahinidis, Derivative-free optimization: a review of algorithms
J. Glob. Optim. 56(3), 1247 (2012),

and comparison of software implementations,
doi:10.1007/s10898-012-9951-y.

[53] C. Zhang, S. Bengio, M. Hardt, B. Recht and O. Vinyals, Understanding deep learning
requires rethinking generalization, In ICLR 2017 - Int. Conf. Learn. Represent. (2017),
arXiv:1611.03530.

[54] D. H. Wolpert, What Is Important About the No Free Lunch Theorems?, pp. 373–388,
Springer International Publishing, ISBN 978-3-030-66515-9, doi:10.1007/978-3-030-
66515-9_13 (2021).

[55] P. Domingos,

A uniﬁed bias-variance decomposition for zero-one and squared loss,
In NCAI 2000 - 17th National Conference on Artiﬁcial Intelligence, pp. 564–569,
doi:10.5555/647288.721421 (2000).

[56] J. Friedman, T. Hastie, R. Tibshirani et al., The elements of statistical learning, Springer,

ISBN 9780387848587, doi:10.1007/978-0-387-84858-7 (2001).

[57] K. Kawaguchi, L. P. Kaelbling and Y. Bengio, Generalization in deep learning (2020),

arXiv:1710.05468.

[58] J. Frankle and M. Carbin, The lottery ticket hypothesis: Finding sparse, trainable neural
networks, In ICLR 2019 - Int. Conf. Learn. Represent. (2019), arXiv:1803.03635.

[59] C. R. Rao, Generalized inverse of a matrix and its applications, pp. 601–620, University

of California Press, doi:10.1525/9780520325883-032 (1972).

[60] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R. Stat. Soc. Ser. B

Methodol. 58(1), 267 (1996), doi:10.1111/j.2517-6161.1996.tb02080.x.

[61] Z. Zhou, X. Li and R. N. Zare, Optimizing chemical reactions with deep reinforcement
learning, ACS Cent. Sci 3(12), 1337 (2017), doi:10.1021/acscentsci.7b00492.

[62] J. Platt, Sequential minimal optimization: A fast algorithm for training support vector

machines, Tech. Rep. MSR-TR-98-14, Microsoft (1998).

[63] M. Minsky and S. Papert, Perceptrons: an introduction to computational geometry, MIT

Press, ISBN 0-262-13043-2 (1969).

[64] F. Rosenblatt, The perceptron: A probabilistic model for information storage and organi-

zation in the brain, Psychol. Rev. 65(6), 386 (1958), doi:10.1037/h0042519.

[65] A. N. Kolmogorov, On the representation of continuous functions of many variables by
superposition of continuous functions of one variable and addition, In Doklady Akademii
Nauk, vol. 114, pp. 953–956. Russian Academy of Sciences (1957).

[66] G. Cybenko, Approximation by superpositions of a sigmoidal function, Math. Control

Signals Syst. 2(4), 303 (1989), doi:10.1007/BF02551274.

[67] K. Hornik, Approximation capabilities of multilayer feedforward networks, Neural Netw.

4(2), 251 (1991), doi:10.1016/0893-6080(91)90009-T.

[68] D. E. Rumelhart, G. E. Hinton and R. J. Williams, Learning representations by back-

propagating errors, Nature 323(6088), 533 (1986), doi:10.1038/323533a0.

[69] D. P. Kingma and M. Welling, Auto-encoding variational Bayes (2013), arXiv:1312.6114.

240

REFERENCES

[70] D. J. Rezende, S. Mohamed and D. Wierstra, Stochastic backpropagation and approxi-
mate inference in deep generative models, In ICML 2014 - Int. Conf. Mach. Learn., vol. 32,
pp. 1278–1286 (2014), arXiv:1401.4082.

[71] A. Ng et al., Sparse autoencoder, CS294A Lecture notes, Stanford University (2011).

[72] A. Makhzani and B. Frey, K-sparse autoencoders (2013), arXiv:1312.5663.

[73] P. Vincent, H. Larochelle, Y. Bengio and P.-A. Manzagol, Extracting and composing robust
features with denoising autoencoders, In ICML 2008 - 25th Int. Conf. Mach. Learn., pp.
1096–1103, doi:10.1145/1390156.1390294 (2008).

[74] Y. Burda, R. Grosse and R. Salakhutdinov,

Importance weighted autoencoders (2015),

arXiv:1509.00519.

[75] B. Uria, M.-A. Côté, K. Gregor,

gressive distribution estimation,
doi:10.5555/2946645.3053487.

Neural autore-
I. Murray and H. Larochelle,
J. Mach. Learn. Res. 17(1), 7184–7220 (2016),

[76] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf et al., PyTorch: an imperative style,
high-performance deep learning library, In NeurIPS 2019 - Adv. Neural. Inf. Process. Syst.
(2019), arXiv:1912.01703.

[77] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Comput. 9(8), 1735

(1997), doi:10.1162/neco.1997.9.8.1735.

[78] K. Cho, B. van Merriënboer, D. Bahdanau and Y. Bengio, On the properties of neural
machine translation: Encoder–decoder approaches, In SSST-8 - 8th Workshop on Syntax,
Semantics and Structure in Statistical Translation, pp. 103–111, doi:10.3115/v1/W14-
4012 (2014).

[79] D. Wu, L. Wang and P. Zhang,
ational autoregressive networks,
doi:10.1103/PhysRevLett.122.080602.

Solving statistical mechanics using vari-
Phys. Rev. Lett. 122(8), 080602 (2019),

[80] K. A. Nicoli, S. Nakajima, N. Strodthoff, W. Samek, K.-R. Müller and P. Kessel, Asymp-
totically unbiased estimation of physical observables with neural samplers, Phys. Rev. E
101(2), 023304 (2020), doi:10.1103/PhysRevE.101.023304.

[81] J.-G. Liu, L. Mao, P. Zhang and L. Wang, Solving quantum statistical mechanics with
variational autoregressive networks and quantum circuits, Mach. Learn.: Sci. Technol.
2(2), 025011 (2021), doi:10.1088/2632-2153/aba19d.

[82] J. Carrasquilla, G. Torlai, R. G. Melko and L. Aolita, Reconstructing quantum states with
generative models, Nat. Mach. Intell. 1(3), 155–161 (2019), doi:10.1038/s42256-019-
0028-1.

[83] O. Sharir, Y. Levine, N. Wies, G. Carleo and A. Shashua, Deep autoregressive models
for the efﬁcient variational simulation of many-body quantum systems, Phys. Rev. Lett.
124(2) (2020), doi:10.1103/PhysRevLett.124.020503.

[84] C. M. Bishop, Pattern Recognition and Machine Learning, Springer, Berlin, Heidelberg,

ISBN 0387310738, doi:10.5555/1162264 (2006).

241

REFERENCES

[85] A. Zhang, Z. C. Lipton, M. Li and A. J. Smola, Dive into deep learning (2021),

arXiv:2106.11342.

[86] P. Mehta, M. Bukov, C.-H. Wang, A. G. Day, C. Richardson, C. K. Fisher and D. J. Schwab,
A high-bias, low-variance introduction to machine learning for physicists, Phys. Rep. 810,
1 (2019), doi:10.1016/j.physrep.2019.03.001.

[87] T. Neupert, M. H. Fischer, E. Greplova, K. Choo and M. Denner, Introduction to machine

learning for the sciences (2021), arXiv:2102.04883.

[88] J. Carrasquilla and G. Torlai, How to use neural networks to investigate quantum many-
body physics, PRX Quantum 2, 040201 (2021), doi:10.1103/PRXQuantum.2.040201.

[89] S. Sachdev,

Quantum Phase Transitions,

Cambridge University Press,

doi:10.1017/cbo9780511973765 (2011).

[90] N. Goldenfeld, Lectures On Phase Transitions And The Renormalization Group, CRC

Press, doi:10.1201/9780429493492 (2018).

[91] L. Onsager, Crystal statistics. I. A two-dimensional model with an order-disorder transi-

tion, Phys. Rev. 65, 117 (1944), doi:10.1103/PhysRev.65.117.

[92] F. J. Wegner, Duality in generalized Ising models and phase transitions without local order

parameters, J. Math. Phys. 12(10), 2259 (1971), doi:10.1063/1.1665530.

[93] L. D. Landau, On the theory of phase transitions. I., Phys. Z. Sowjet. 11, 26 (1937),

Reprinted in Collected Papers of L. D. Landau.

[94] L. D. Landau, On the theory of phase transitions. II., Phys. Z. Sowjet. 11, 545 (1937),

Reprinted in Collected Papers of L. D. Landau.

[95] X.-G. Wen, Topological orders in rigid states, Int. J. Mod. Phys. B 4(02), 239 (1990),

doi:10.1142/S0217979290000139.

[96] B. Bernevig and T. Hughes, Topological Insulators and Topological Superconductors,
doi:10.1515/9781400846733

ISBN 9780691151755,

Princeton University Press,
(2013).

[97] N. Käming, A. Dawid, K. Kottmann, M. Lewenstein, K. Sengstock, A. Dauphin and
C. Weitenberg, Unsupervised machine learning of topological phase transitions from ex-
perimental data, Mach. Learn.: Sci. Technol. 2, 035037 (2021), doi:10.1088/2632-
2153/abffe7.

[98] N. Sun, J. Yi, P. Zhang, H. Shen and H. Zhai, Deep learning topological invariants of band
insulators, Phys. Rev. B 98, 085402 (2018), doi:10.1103/PhysRevB.98.085402.

[99] P. Zhang, H. Shen and H. Zhai, Machine learning topological invariants with neural
networks, Phys. Rev. Lett. 120, 066401 (2018), doi:10.1103/PhysRevLett.120.066401.

[100] M. D. Caio, M. Caccin, P. Baireuther, T. Hyart and M. Fruchart, Machine learning assisted

measurement of local topological invariants (2019), arXiv:1901.03346.

[101] N. L. Holanda and M. A. R. Grifﬁth, Machine learning topological phases in real space,

Phys. Rev. B 102, 054107 (2020), doi:10.1103/PhysRevB.102.054107.

[102] P. Baireuther, M. Płodzie´n, T. Ojanen, K. Tworzydło and T. Hyart,

Identifying Chern

numbers of superconductors from local measurements (2021), arXiv:2112.06777.

242

REFERENCES

[103] P. Huembeli, A. Dauphin and P. Wittek,

tions with adversarial neural networks,
doi:10.1103/PhysRevB.97.134109.

Identifying quantum phase transi-
Phys. Rev. B 97, 134109 (2018),

[104] C. Fefferman, S. Mitter and H. Narayanan, Testing the manifold hypothesis, J. Am. Math.

Soc. 29(4), 983 (2016), doi:10.1090/jams/852.

[105] L. Wang, Discovering phase transitions with unsupervised learning, Phys. Rev. B 94,

195105 (2016), doi:10.1103/PhysRevB.94.195105.

[106] S. J. Wetzel,

Unsupervised learning of phase transitions:

From principal com-
Phys. Rev. E 96, 022140 (2017),

ponent analysis to variational autoencoders,
doi:10.1103/PhysRevE.96.022140.

[107] W. Hu, R. R. Singh and R. T. Scalettar, Discovering phases, phase transitions, and
crossovers through unsupervised machine learning: A critical examination, Phys. Rev.
E 95(6), 062122 (2017), doi:10.1103/PhysRevE.95.062122.

[108] L. Van der Maaten and G. Hinton, Visualizing data using t-SNE, J. Mach. Learn. Res.

9(11) (2008).

[109] L. McInnes, J. Healy and J. Melville, UMAP: Uniform manifold approximation and pro-

jection for dimension reduction (2018), arXiv:1802.03426.

[110] G. E. Hinton and S. T. Roweis, Stochastic neighbor embedding,

In NIPS 2002 - Adv.

Neural. Inf. Process. Syst., vol. 15 (2002).

[111] E. Greplova, A. Valenti, G. Boschung, F. Schäfer, N. Lörch and S. D. Huber, Unsupervised
identiﬁcation of topological phase transitions using predictive models, New J. Phys. 22(4),
045003 (2020), doi:10.1088/1367-2630/ab7771.

[112] J. Arnold, F. Schäfer, M. Žonda and A. U. J. Lode,
classiﬁcation,

Phys. Rev. Res. 3,

unsupervised
doi:10.1103/PhysRevResearch.3.033052.

phase

Interpretable and
033052 (2021),

[113] J. Carrasquilla and R. G. Melko, Machine learning phases of matter, Nat. Phys. 13(5),

431 (2017), doi:10.1038/nphys4035.

[114] K. Kottmann, F. Metz, J. Fraxanet and N. Baldelli, Variational quantum anomaly detec-
tion: Unsupervised mapping of phase diagrams on a physical quantum computer, Phys.
Rev. Research 3, 043184 (2021), doi:10.1103/PhysRevResearch.3.043184.

[115] T. Szołdra, P. Sierant, M. Lewenstein and J. Zakrzewski, Unsupervised detection of de-

coupled subspaces: many-body scars and beyond (2022), arXiv:2201.07151.

[116] K. Kottmann, P. Huembeli, M. Lewenstein and A. Acín,

Unsupervised phase
Phys. Rev. Lett. 125, 170603 (2020),

discovery with deep anomaly detection,
doi:10.1103/PhysRevLett.125.170603.

[117] T. Szołdra, P. Sierant, K. Kottmann, M. Lewenstein and J. Zakrzewski, Detecting ergodic
bubbles at the crossover to many-body localization using neural networks, Phys. Rev. B
104, L140202 (2021), doi:10.1103/PhysRevB.104.L140202.

[118] E. P. Van Nieuwenburg, Y.-H. Liu and S. D. Huber, Learning phase transitions by confusion,

Nat. Phys. 13(5), 435 (2017), doi:10.1038/nphys4037.

243

[119] Y.-H. Liu and E. P. L. van Nieuwenburg,
works for detecting phase transitions,
doi:10.1103/PhysRevLett.120.176401.

cooperative net-
Phys. Rev. Lett. 120, 176401 (2018),

Discriminative

REFERENCES

[120] S. S. Lee and B. J. Kim,

phase transitions and quasi-long-range order,
doi:10.1103/PhysRevE.99.043308.

Confusion scheme in machine learning detects double
Phys. Rev. E 99, 043308 (2019),

[121] F. Schäfer and N. Lörch, Vector ﬁeld divergence of predictive model output as indication of
phase transitions, Phys. Rev. E 99, 062107 (2019), doi:10.1103/PhysRevE.99.062107.

[122] P. Ronhovde, S. Chakrabarty, D. Hu, M. Sahu, K. Sahu, K. Kelton, N. Mauro and Z. Nussi-
nov, Detecting hidden spatial and spatio-temporal structures in glasses and complex phys-
ical systems by multiresolution network clustering, Eur. Phys. J. E 34(9), 1 (2011),
doi:10.1140/epje/i2011-11105-9.

[123] P. Ronhovde, S. Chakrabarty, D. Hu, M. Sahu, K. K. Sahu, K. F. Kelton, N. A. Mauro
and Z. Nussinov, Detection of hidden structures for arbitrary scales in complex physical
systems, Sci. Rep. 2(1), 1 (2012), doi:10.1038/srep00329.

[124] R. A. Vargas-Hernández, J. Sous, M. Berciu and R. V. Krems,

ing quantum observables with machine learning:
tions from properties of a single phase,
doi:10.1103/PhysRevLett.121.255702.

Extrapolat-
Inferring multiple phase transi-
Phys. Rev. Lett. 121, 255702 (2018),

[125] A. A. Shirinyan, V. K. Kozin, J. Hellsvik, M. Pereiro, O. Eriksson and D. Yudin, Self-
organizing maps as a method for detecting phase transitions and phase identiﬁcation,
Phys. Rev. B 99, 041108 (2019), doi:10.1103/PhysRevB.99.041108.

[126] T. Mazaheri, B. Sun, J. Scher-Zagier, A. S. Thind, D. Magee, P. Ronhovde, T. Lookman,
R. Mishra and Z. Nussinov, Stochastic replica voting machine prediction of stable cubic and
double perovskite materials and binary alloys, Phys. Rev. Materials 3, 063802 (2019),
doi:10.1103/PhysRevMaterials.3.063802.

[127] O. Balabanov and M. Granath, Unsupervised learning using topological data augmenta-
tion, Phys. Rev. Research 2, 013354 (2020), doi:10.1103/PhysRevResearch.2.013354.

[128] S.-J. Gu, Fidelity approach to quantum phase transitions, Int. J. Mod. Phys. B 24(23),

4371 (2010), doi:10.1142/S0217979210056335.

[129] A. Bohrdt, S. Kim, A. Lukin, M. Rispoli, R. Schittko, M. Knap, M. Greiner and J. Léonard,
Analyzing nonequilibrium quantum states through snapshots with artiﬁcial neural net-
works, Phys. Rev. Lett. 127, 150504 (2021), doi:10.1103/PhysRevLett.127.150504.

[130] Z. C. Lipton, The mythos of model interpretability, Commun. ACM 61(10), 35 (2018),

doi:10.1145/3233231.

[131] A. Bohrdt, C. S. Chiu, G. Ji, M. Xu, D. Greif, M. Greiner, E. Demler, F. Grusdt and M. Knap,
Classifying snapshots of the doped Hubbard model with machine learning, Nat. Phys.
15(9), 921 (2019), doi:10.1038/s41567-019-0565-x.

[132] Y. Zhang, P. Ginsparg and E.-A. Kim,

topological quantum phase transitions,
doi:10.1103/PhysRevResearch.2.023283.

Interpreting machine

learning of
Phys. Rev. Res. 2, 023283 (2020),

244

REFERENCES

[133] R. Iten, T. Metger, H. Wilming, L. Del Rio and R. Renner,

Discovering phys-
Phys. Rev. Lett. 124(1), 010508 (2020),

ical concepts with neural networks,
doi:10.1103/PhysRevLett.124.010508.

[134] S. J. Wetzel and M. Scherzer, Machine learning of explicit order parameters: From
the Ising model to SU(2) lattice gauge theory, Phys. Rev. B 96(18), 184410 (2017),
doi:10.1103/PhysRevB.96.184410.

[135] M. Cranmer, A. Sanchez-Gonzalez, P. Battaglia, R. Xu, K. Cranmer, D. Spergel and S. Ho,
Discovering symbolic models from deep learning with inductive biases, In NeurIPS 2020 -
Adv. Neural. Inf. Process. Syst., pp. 1–25 (2020), arXiv:2006.11287.

[136] S. J. Wetzel, R. G. Melko, J. Scott, M. Panju and V. Ganesh, Discovering symmetry invari-
ants and conserved quantities by interpreting Siamese neural networks, Phys. Rev. Res. 2,
033499 (2020), doi:10.1103/PhysRevResearch.2.033499.

[137] C. Miles, A. Bohrdt, R. Wu, C. Chiu, M. Xu, G. Ji, M. Greiner, K. Q. Weinberger, E. Demler
and E.-A. Kim, Correlator convolutional neural networks: An interpretable architecture for
image-like quantum matter data, Nat. Commun. 12(1), 1 (2021), doi:10.1038/s41467-
021-23952-w.

[138] S. K. Radha and C. Jao,

Generalized quantum similarity learning (2022),

arXiv:2201.02310.

[139] N. S. Keskar, J. Nocedal, P. T. P. Tang, D. Mudigere and M. Smelyanskiy, On large-batch
In ICLR 2017 - Int.

training for deep learning: Generalization gap and sharp minima,
Conf. Learn. Represent. (2017), arXiv:1609.04836.

[140] L. Wu, Z. Zu and W. E, Towards understanding generalization of deep learning: Perspective

of loss landscapes (2017), arXiv:1706.10239.

[141] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov and A. G. Wilson, Averaging weights
leads to wider optima and better generalization, In UAI 2018 - 34th Conf. Uncertain. Artif.
Intell., vol. 2, pp. 876–885 (2018), arXiv:1803.05407.

[142] H. He, G. Huang and Y. Yuan, Asymmetric valleys: Beyond sharp and ﬂat local minima,

In NeurIPS 2019 - Adv. Neural. Inf. Process. Syst., vol. 32 (2019), arXiv:1902.00744.

[143] L. Dinh, R. Pascanu, S. Bengio and Y. Bengio, Sharp minima can generalize for deep
In ICML 2017 - 34th Int. Conf. Mach. Learn., vol. 3, pp. 1705–1714 (2017),

nets,
arXiv:1703.04933.

[144] A. Dawid, P. Huembeli, M. Tomza, M. Lewenstein and A. Dauphin, Hessian-based toolbox
for reliable and interpretable machine learning in physics, Mach. Learn.: Sci. Technol. 3,
015002 (2022), doi:10.1088/2632-2153/ac338d.

[145] P. W. Koh and P. Liang, Understanding black-box predictions via inﬂuence functions,
In ICML 2017 - 34th Int. Conf. Mach. Learn., vol. 70, pp. 1885–1894. PMLR (2017),
arXiv:1703.04730.

[146] P. Schulam and S. Saria, Can you trust this prediction? Auditing pointwise reliability after
learning, In AISTATS 2019 - Int. Conf. Artif. Intell. Stat., vol. 89, pp. 1022–1031. PLMR
(2020), arXiv:1901.00403.

[147] D. Madras, J. Atwood and A. D’Amour, Detecting extrapolation with local ensembles, In

ICLR 2020 - Int. Conf. Learn. Represent. (2020), arXiv:1910.09573.

245

REFERENCES

[148] A. Dawid, P. Huembeli, M. Tomza, M. Lewenstein and A. Dauphin, Phase detection
with neural networks: interpreting the black box, New J. Phys. 22(11), 115001 (2020),
doi:10.1088/1367-2630/abc463.

[149] J. Arnold and F. Schäfer, Replacing neural networks by optimal analytical predictors for

the detection of phase transitions (2022), arXiv:2203.06084.

[150] C. Molnar,

Interpretable Machine Learning: A Guide for Making Black Box Models Ex-

plainable, GitHub.io, ISBN 9780244768522 (2019).

[151] G. Bachman and L. Narici, Functional analysis, Dover Publications, Mineola, New York,

ISBN 9780486136554 (2000).

[152] J. Mercer, XVI. Functions of positive and negative type, and their connection the the-
ory of integral equations, Philos. Trans. Royal Soc. A 209(441-458), 415 (1909),
doi:10.1098/rsta.1909.0016.

[153] N. Aronszajn, Theory of reproducing kernels, Trans. Am. Math. Soc. 68(3), 337 (1950),

doi:10.2307/1990404.

[154] B. Schölkopf, R. Herbrich and A. J. Smola, A generalized representer theorem, In Com-
putational Learning Theory, pp. 416–426. Springer, doi:10.1007/3-540-44581-1_27
(2001).

[155] R. Garnett, Bayesian Optimization, Cambridge University Press, in preparation (2022).

[156] R. M. Neal, Bayesian learning for neural networks, vol. 118 of Lecture Notes in Statistics,

Springer, doi:10.1007/978-1-4612-0745-0 (2012).

[157] P. I. Frazier, A tutorial on Bayesian optimization (2018), arXiv:1807.02811.

[158] G. Schwarz, Estimating the dimension of a model, Ann. Stat. pp. 461–464 (1978),

doi:10.1214/aos/1176344136.

[159] P. Stoica and Y. Selen, Model-order selection: a review of information criterion rules, IEEE

Signal Process. Mag. 21(4), 36 (2004), doi:10.1109/MSP.2004.1311138.

[160] H. Akaike, A new look at the statistical model identiﬁcation, IEEE Trans. Automat. Contr.

19(6), 716 (1974), doi:10.1109/TAC.1974.1100705.

[161] D. Duvenaud, J. Lloyd, R. Grosse, J. Tenenbaum and G. Zoubin, Structure discovery in
nonparametric regression through compositional kernel search, In ICML 2013 - Int. Conf.
Mach. Learn., vol. 28, pp. 1166–1174. PMLR (2013), arXiv:1302.4922.

[162] D. Duvenaud, H. Nickisch and C. E. Rasmussen, Additive Gaussian processes, In NeurIPS

2011 - Adv. Neural. Inf. Process. Syst., vol. 24 (2011), arXiv:1112.4394.

[163] J. Dai and R. V. Krems, Interpolation and extrapolation of global potential energy surfaces
for polyatomic systems by Gaussian processes with composite kernels, J. Chem. Theory
Comput. 16(3), 1386 (2020), doi:10.1021/acs.jctc.9b00700.

[164] Z. Deng, I. Tutunnikov, I. S. Averbukh, M. Thachuk and R. V. Krems, Bayesian optimiza-
tion for inverse problems in time-dependent quantum dynamics, J. Chem. Phys. 153(16),
164111 (2020), doi:10.1063/5.0015896.

246

REFERENCES

[165] N. Q. Su, J. Chen, Z. Sun, D. H. Zhang and X. Xu, H + H2 quantum dynamics using poten-
tial energy surfaces based on the XYG3 type of doubly hybrid density functionals: Validation
of the density functionals, J. Chem. Phys. 142, 084107 (2015), doi:10.1063/1.4913196.

[166] R. A. Vargas-Hernández, Y. Guan, D. H. Zhang and R. V. Krems, Bayesian optimization
for the inverse scattering problem in quantum reaction dynamics, New J. Phys. 21, 22001
(2019), doi:10.1088/1367-2630/ab0099.

[167] J. T. Cantin, G. Alexandrowicz, G. Alexandrowicz and R. V. Krems, Transfer-matrix theory
of surface spin-echo experiments with molecules, Phys. Rev. A 101(6), 062703 (2020),
doi:10.1103/PhysRevA.101.062703.

[168] N. Sugisawa, H. Sugisawa, Y. Otake, R. V. Krems, H. Nakamura and S. Fuse, Rapid
and mild one-ﬂow synthetic approach to unsymmetrical sulfamides guided by Bayesian
optimization, Chem. Methods 1(11), 484 (2021), doi:10.1002/cmtd.202100053.

[169] A. Jasinski, J. Montaner, R. C. Forrey, B. H. Yang, P. C. Stancil, N. Balakr-
Machine learning
Phys. Rev. Res. 2(3), 32051 (2020),

ishnan, J. Dai, R. A. Vargas-Hernández and R. V. Krems,
corrected quantum dynamics calculations,
doi:10.1103/PhysRevResearch.2.032051.

[170] R. A. Vargas Hernandez,

hybrid-density functional models,
doi:10.1021/acs.jpca.0c01375.

Bayesian optimization for calibrating and selecting
J. Phys. Chem. A 124(20), 4053 (2020),

[171] J. Proppe, S. Gugler and M. Reiher,

Gaussian process-based reﬁnement of
J. Chem. Theory Comput. 15(11), 6046 (2019),

dispersion corrections,
doi:10.1021/acs.jctc.9b00627.

[172] R. Tamura and K. Hukushima, Bayesian optimization for computationally extensive prob-

ability distributions, PLoS One 13(3), 1 (2018), doi:10.1371/journal.pone.0193785.

[173] S. Carr, R. Garnett and C. Lo, BASC: Applying Bayesian optimization to the search for
In ICML 2016 - Int. Conf. Mach. Learn.,

global minima on potential energy surfaces,
vol. 48, pp. 898–907. PMLR (2016).

[174] L. Chan, G. R. Hutchison and G. M. Morris, Bayesian optimization for conformer gener-
ation, J. Cheminformatics 11(1), 32 (2019), doi:10.1186/s13321-019-0354-7.

[175] R. A. Vargas-Hernández, C. Chuang and P. Brumer, Multi-objective optimization for reti-
nal photoisomerization models with respect to experimental observables, J. Chem. Phys.
155(23), 234109 (2021), doi:10.1063/5.0060259.

[176] J. Duris, D. Kennedy, A. Hanuka, J. Shtalenkova, A. Edelen, P. Baxevanis, A. Egger,
T. Cope, M. McIntire, S. Ermon and D. Ratner, Bayesian optimization of a free-electron
laser, Phys. Rev. Lett. 124, 124801 (2020), doi:10.1103/PhysRevLett.124.124801.

[177] S. Jalas, M. Kirchen, P. Messner, P. Winkler, L. Hübner, J. Dirkwinkel, M. Schnepp,
R. Lehe and A. R. Maier, Bayesian optimization of a laser-plasma accelerator, Phys.
Rev. Lett. 126, 104801 (2021), doi:10.1103/PhysRevLett.126.104801.

[178] R. J. Shalloo, S. J. D. Dann, J.-N. Gruse, C. I. D. Underwood, A. F. Antoine, C. Arran,
M. Backhouse, C. D. Baird, M. D. Balcazar, N. Bourgeois, J. A. Cardarelli, P. Hatﬁeld
et al., Automation and control of laser wakeﬁeld accelerators using Bayesian optimization,
Nat. Commun. 11(1), 6355 (2020), doi:10.1038/s41467-020-20245-6.

247

REFERENCES

[179] T. Ueno, T. D. Rhone, Z. Hou, T. Mizoguchi and K. Tsuda, COMBO: An efﬁcient
Bayesian optimization library for materials science, Mater. Discov. 4, 18 (2016),
doi:10.1016/j.md.2016.04.001.

[180] R. Jalem, K. Kanamori, I. Takeuchi, M. Nakayama, H. Yamasaki and T. Saito, Bayesian-
driven ﬁrst-principles calculations for accelerating exploration of fast ion conductors for
rechargeable battery application, Sci. Rep. 8(1), 5845 (2018), doi:10.1038/s41598-
018-23852-y.

[181] S. Ju, T. Shiga, L. Feng, Z. Hou, K. Tsuda and J. Shiomi, Designing nanostructures
Phys. Rev. X 7, 021024 (2017),

for phonon transport via Bayesian optimization,
doi:10.1103/PhysRevX.7.021024.

[182] J. Kuhn, J. Spitz, P. Sonnweber-Ribic, M. Schneider and T. Böhlke,

Identifying ma-
terial parameters in crystal plasticity by Bayesian optimization, Optim. Eng. (2021),
doi:10.1007/s11081-021-09663-7.

[183] R.-R. Grifﬁths and J. M. Hernández-Lobato, Constrained Bayesian optimization for au-
tomatic chemical design using variational autoencoders, Chem. Sci. 11, 577 (2020),
doi:10.1039/C9SC04026A.

[184] A. Deshwal, C. M. Simon and J. R. Doppa, Bayesian optimization of nanoporous materi-

als, Mol. Syst. Des. Eng. 6, 1066 (2021), doi:10.1039/D1ME00093D.

[185] F. Häse, L. M. Roch, C. Kreisbeck and A. Aspuru-Guzik, Phoenics: A Bayesian optimizer

for chemistry, ACS Cent. Sci. 4(9), 1134 (2018), doi:10.1021/acscentsci.8b00307.

[186] F. Häse, M. Aldeghi, R. J. Hickman, L. M. Roch and A. Aspuru-Guzik, Gryfﬁn: An
algorithm for Bayesian optimization of categorical variables informed by expert knowledge,
Appl. Phys. Rev. 8(3), 031406 (2021), doi:10.1063/5.0048164.

[187] A. Biswas, A. N. Morozovska, M. Ziatdinov, E. A. Eliseev and S. V. Kalinin, Multi-
objective Bayesian optimization of ferroelectric materials with interfacial control for
memory and energy storage applications,
J. Appl. Phys. 130(20), 204102 (2021),
doi:10.1063/5.0068903.

[188] Y. Wang, T.-Y. Chen and D. G. Vlachos, NEXTorch: A design and bayesian optimization
toolkit for chemical sciences and engineering, J. Chem. Inf. Model. 61(11), 5312 (2021),
doi:10.1021/acs.jcim.1c00637.

[189] M. Aldeghi, F. Häse, R. J. Hickman, I. Tamblyn and A. Aspuru-Guzik, Golem: An al-
gorithm for robust experiment and process optimization, Chem. Sci. 12, 14792 (2021),
doi:10.1039/D1SC01545A.

[190] H. Sugisawa, T. Ida and R. V. Krems, Gaussian process model of 51-dimensional potential
energy surface for protonated imidazole dimer, J. Chem. Phys. 153(11), 114101 (2020),
doi:10.1063/5.0023492.

[191] C. Puzzarini, J. Bloino, N. Tasinato and V. Barone, Accuracy and interpretability: The
devil and the holy grail. New routes across old boundaries in computational spectroscopy,
Chem. Rev. 119(13), 8131 (2019), doi:10.1021/acs.chemrev.9b00007.

[192] F. Herrera, K. W. Madison, R. V. Krems and M. Berciu,

Investigating po-
Phys. Rev. Lett. 110(22), 223002 (2013),

laron transitions with polar molecules,
doi:10.1103/PhysRevLett.110.223002.

248

REFERENCES

[193] K. Asnaashari and R. V. Krems, Gradient domain machine learning with composite kernels:
improving the accuracy of PES and force ﬁelds for large molecules, Mach. learn.: sci.
technol. 3(1), 015005 (2021), doi:10.1088/2632-2153/ac3845.

[194] A. G. Wilson, Z. Hu, R. Salakhutdinov and E. P. Xing, Deep kernel learning, In AISTATS

2016 - Int. Conf. Artif. Intell. Stat. (2016), arXiv:1511.02222.

[195] S. Sun, G. Zhang, C. Wang, W. Zeng, J. Li and R. Grosse, Differentiable compositional
kernel learning for Gaussian processes, In ICML 2018 - Int. Conf. Mach. Learn. (2018),
arXiv:1806.04326.

[196] J. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel and A. G. Wilson, GPyTorch: Blackbox
matrix-matrix Gaussian process inference with GPU acceleration, In NeurIPS 2018 - Adv.
Neural Inf. Process. Syst. (2018), arXiv:1809.11165.

[197] B. Charlier, J. Feydy, J. A. Glaunès, F.-D. Collin and G. Durif, Kernel operations on the
GPU, with Autodiff, without memory overﬂows, J. Mach. Learn. Res. 22(74), 1 (2021),
arXiv:2004.11127.

[198] A. G. d. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. León-
Villagrá, Z. Ghahramani and J. Hensman, GPﬂow: A Gaussian process library using
TensorFlow, J. Mach. Learn. Res. 18(40), 1 (2017), arXiv:1610.08733.

[199] M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-López, F. Pedregosa
and J.-P. Vert, Efﬁcient and modular implicit differentiation (2021), arXiv:2105.15183.

[200] H.-Y. Huang, R. Kueng and J. Preskill,

Predicting many properties of a quan-
tum system from very few measurements, Nat. Phys. 16(10), 1050–1057 (2020),
doi:10.1038/s41567-020-0932-7.

[201] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning,
ISBN 9780262256834,

Adaptive Computation and Machine Learning. MIT Press,
doi:10.7551/mitpress/3206.001.0001 (2005).

[202] R. V. Krems, Bayesian machine learning for quantum molecular dynamics, Phys. Chem.

Chem. Phys. 21(25), 13392 (2019), doi:10.1039/c9cp01883b.

[203] R. A. Vargas-Hernández and R. V. Krems, Physical Extrapolation of Quantum Observables
by Generalization with Gaussian Processes, pp. 171–194, Springer International Publish-
ing, Cham, ISBN 978-3-030-40245-7, doi:10.1007/978-3-030-40245-7_9 (2020).

[204] H.-Y. Huang, R. Kueng, G. Torlai, V. V. Albert and J. Preskill, Provably efﬁcient machine

learning for quantum many-body problems (2021), arXiv:2106.12627.

[205] P. A. M. Dirac and R. H. Fowler, Quantum mechanics of many-electron systems, Proc. R.

Soc. A: Math. Phys. Eng. Sci. 123(792), 714 (1929), doi:10.1098/rspa.1929.0094.

[206] G. Carleo and M. Troyer, Solving the quantum many-body problem with artiﬁcial neural
networks, Science 355(6325), 602–606 (2017), doi:10.1126/science.aag2302.

[207] K. Choo, A. Mezzacapo and G. Carleo, Fermionic neural-network states for ab-initio
electronic structure, Nat. Commun. 11(1) (2020), doi:10.1038/s41467-020-15724-9.

[208] H. Saito, Solving the Bose–Hubbard model with machine learning, J. Phys. Soc. Jpn.

86(9), 093001 (2017), doi:10.7566/jpsj.86.093001.

249

REFERENCES

[209] S. R. White, Density matrix formulation for quantum renormalization groups, Phys. Rev.

Lett. 69, 2863 (1992), doi:10.1103/PhysRevLett.69.2863.

[210] U. Schollwöck, The density-matrix renormalization group in the age of matrix product

states, Ann. Phys. (N. Y.) 326(1), 96–192 (2011), doi:10.1016/j.aop.2010.09.012.

[211] R. Orús,

A practical
and projected entangled pair states,
doi:10.1016/j.aop.2014.06.013.

introduction to tensor networks: Matrix product states
Ann. Phys. (N. Y.) 349, 117–158 (2014),

[212] M. V. den Nest, Simulating quantum computers with probabilistic methods (2010),

arXiv:0911.1624.

[213] W. K. Hastings, Monte Carlo sampling methods using Markov chains and their applica-

tions, Biometrika 57(1), 97 (1970), doi:10.2307/2334940.

[214] R. Jastrow, Many-body problem with strong forces,

Phys. Rev. 98, 1479 (1955),

doi:10.1103/PhysRev.98.1479.

[215] E. Manousakis,

and its application to the cuprous oxides,
doi:10.1103/RevModPhys.63.1.

The spin-½ Heisenberg antiferromagnet on a square lattice
Rev. Mod. Phys. 63, 1 (1991),

[216] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
Language mod-
In NeurIPS 2020 - Adv. Neural Inf. Process. Syst. (2020),

P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss et al.,
els are few-shot learners,
arXiv:2005.14165.

[217] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski and A. Joulin, Emerg-
ing properties in self-supervised vision transformers, In ICCV 2021 - Int. Conf. Comput.
Vis. (2021), arXiv:2104.14294.

[218] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever and
M. Chen, GLIDE: Towards photorealistic image generation and editing with text-guided
diffusion models (2021), arXiv:2112.10741.

[219] A. Barra, A. Bernacchia, E. Santucci and P. Contucci,

of Hopﬁeld networks and Boltzmann machines,
doi:10.1016/j.neunet.2012.06.003.

On the equivalence
Neural Netw. 34, 1 (2012),

[220] G. Montufar,

Restricted Boltzmann machines:

Introduction and review (2018),

arXiv:1806.07066.

[221] D.-L. Deng, X. Li and S. Das Sarma, Quantum entanglement in neural network states,

Phys. Rev. X 7(2) (2017), doi:10.1103/PhysRevX.7.021021.

[222] J. Chen, S. Cheng, H. Xie, L. Wang and T. Xiang,
Boltzmann machines and tensor network states,
doi:10.1103/PhysRevB.97.085104.

Equivalence of restricted
Phys. Rev. B 97(8) (2018),

[223] X. Gao and L.-M. Duan, Efﬁcient representation of quantum many-body states with deep
neural networks, Nat. Commun. 8(1) (2017), doi:10.1038/s41467-017-00705-2.

[224] D. Luo, G. Carleo, B. K. Clark and J. Stokes,
works for quantum lattice gauge theories,
doi:10.1103/PhysRevLett.127.276402.

Gauge equivariant neural net-
Phys. Rev. Lett. 127, 276402 (2021),

250

REFERENCES

[225] A. Bansal, X. Chen, B. Russell, A. Gupta and D. Ramanan, PixelNet: Representation of

the pixels, by the pixels, and for the pixels (2017), arXiv:1702.06506.

[226] M. Hibat-Allah, M. Ganahl, L. E. Hayward, R. G. Melko and J. Carrasquilla, Re-
Phys. Rev. Res. 2(2), 023358 (2020),

current neural network wave functions,
doi:10.1103/PhysRevResearch.2.023358.

[227] M. Schmitt and M. Heyl,

sions with artiﬁcial neural networks,
doi:10.1103/PhysRevLett.125.100503.

Quantum many-body dynamics

in two dimen-
Phys. Rev. Lett. 125, 100503 (2020),

[228] C. Roth and A. H. MacDonald, Group convolutional neural networks improve quantum

state accuracy (2021), arXiv:2104.05085.

[229] I. Glasser, N. Pancotti, M. August, I. D. Rodriguez and J. I. Cirac, Neural-network quan-
tum states, string-bond states, and chiral topological states, Phys. Rev. X 8, 011006
(2018), doi:10.1103/PhysRevX.8.011006.

[230] O. Sharir, A. Shashua and G. Carleo, Neural tensor contractions and the expressive power

of deep neural quantum states (2021), arXiv:2103.10293.

[231] Y. Levine, O. Sharir, N. Cohen and A. Shashua,

Quantum entangle-
Phys. Rev. Lett. 122, 065301 (2019),

ment
in deep learning architectures,
doi:10.1103/PhysRevLett.122.065301.

[232] P. Calabrese and J. Cardy, Entanglement entropy and quantum ﬁeld theory, J. Stat. Mech.

2004, P06002 (2004), doi:10.1088/1742-5468/2004/06/p06002.

[233] J. Eisert, M. Cramer and M. B. Plenio, Colloquium: Area laws for the entanglement
entropy, Rev. Mod. Phys. 82, 277 (2010), doi:10.1103/RevModPhys.82.277.

[234] M. Hibat-Allah, E. M. Inack, R. Wiersema, R. G. Melko and J. Carrasquilla, Variational
neural annealing, Nat. Mach. Intell. 3, 952 (2021), doi:10.1038/s42256-021-00401-3.

[235] K. Choo, G. Carleo, N. Regnault and T. Neupert, Symmetries and many-body exci-
tations with neural-network quantum states, Phys. Rev. Lett. 121, 167204 (2018),
doi:10.1103/PhysRevLett.121.167204.

[236] A. Valenti, E. Greplova, N. H. Lindner and S. D. Huber, Correlation-enhanced neural
networks as interpretable variational quantum states, Phys. Rev. Res. 4, L012010 (2022),
doi:10.1103/PhysRevResearch.4.L012010.

[237] G. Carleo, Y. Nomura and M. Imada, Constructing exact representations of quan-
tum many-body systems with deep neural networks, Nat. Commun. 9, 5322 (2018),
doi:10.1038/s41467-018-07520-3.

[238] R. Kaubruegger, L. Pastori and J. C. Budich, Chiral topological phases from artiﬁcial
neural networks, Phys. Rev. B 97, 195136 (2018), doi:10.1103/PhysRevB.97.195136.

[239] Y. Zheng, H. He, N. Regnault and B. A. Bernevig, Restricted Boltzmann machines and
matrix product states of one-dimensional translationally invariant stabilizer codes, Phys.
Rev. B 99, 155129 (2019), doi:10.1103/PhysRevB.99.155129.

[240] S. Lu, X. Gao and L.-M. Duan,

states with restricted Boltzmann machines,
doi:10.1103/PhysRevB.99.155136.

Efﬁcient representation of topologically ordered
Phys. Rev. B 99, 155136 (2019),

251

REFERENCES

[241] Y. Huang and J. E. Moore, Neural network representation of tensor network and chiral

states, Phys. Rev. Lett. 127, 170601 (2021), doi:10.1103/PhysRevLett.127.170601.

[242] C.-Y. Park and M. J. Kastoryano, Geometry of learning neural quantum states, Phys. Rev.

Res. 2, 023232 (2020), doi:10.1103/PhysRevResearch.2.023232.

[243] S.-H. Lin and F. Pollmann, Scaling of neural-network quantum states for time evolution,

doi:10.1002/pssb.202100172 (2022).

[244] F. Vicentini, D. Hofmann, A. Szabó, D. Wu, C. Roth, C. Giuliani, G. Pescia, J. Nys,
V. Vargas-Calderon, N. Astrakhantsev and G. Carleo, NetKet 3: Machine learning toolbox
for many-body quantum systems (2021), arXiv:2112.10526.

[245] X. Yuan, S. Endo, Q. Zhao, Y. Li and S. C. Benjamin, Theory of variational quantum

simulation, Quantum 3, 191 (2019), doi:10.22331/q-2019-10-07-191.

[246] G. Carleo, F. Becca, M. Schiro and M. Fabrizio, Localization and glassy dynamics of
many-body quantum systems, Sci. Rep. 2, 243 (2012), doi:10.1038/srep00243.

[247] I. L. Gutiérrez and C. B. Mendl, Real time evolution with neural-network quantum states,

Quantum 6, 627 (2022), doi:10.22331/q-2022-01-20-627.

[248] D. Hofmann, G. Fabiani, J. Mentink, G. Carleo and M. Sentef, Role of stochastic
noise and generalization error in the time propagation of neural-network quantum states,
doi:10.21468/scipostphys.12.5.165 (2022).

[249] S. Sorella, Green function Monte Carlo with stochastic reconﬁguration, Phys. Rev. Lett.

80, 4558 (1998), doi:10.1103/PhysRevLett.80.4558.

[250] F. Becca and S. Sorella, Quantum Monte Carlo Approaches for Correlated Systems, Cam-
bridge University Press, ISBN 9781316417041, doi:10.1017/9781316417041 (2017).

[251] D. Hangleiter, I. Roth, D. Nagaj and J. Eisert, Easing the Monte Carlo sign problem, Sci.

Adv. 6, eabb8341 (2020), doi:10.1126/sciadv.abb8341.

[252] S. B. Bravyi and A. Y. Kitaev, Fermionic quantum computation, Ann. Phys. 298, 210

(2002), doi:10.1006/aphy.2002.6254.

[253] P. Jordan and E. Wigner, Über das Paulische Äquivalenzverbot, Zeitschrift für Physik 47,

631 (1928), doi:10.1007/BF01331938.

[254] E. Zohar and J. I. Cirac, Eliminating fermionic matter ﬁelds in lattice gauge theories,

Phys. Rev. B 98, 075119 (2018), doi:10.1103/PhysRevB.98.075119.

[255] U. Borla, R. Verresen, F. Grusdt and S. Moroz, Conﬁned phases of one-dimensional
spinless fermions coupled to Z2 gauge theory, Phys. Rev. Lett. 124, 120503 (2020),
doi:10.1103/PhysRevLett.124.120503.

[256] J. Hermann, Z. Schätzle and F. Noé, Deep-neural-network solution of the electronic
Schrödinger equation, Nat. Chem. 12, 891–897 (2020), doi:10.1038/s41557-020-0544-
y.

[257] D. Luo and B. K. Clark,

quantum many-body wave functions,
doi:10.1103/PhysRevLett.122.226401.

Backﬂow transformations via neural networks for
Phys. Rev. Lett. 122, 226401 (2019),

252

REFERENCES

[258] B. Jonsson, B. Bauer and G. Carleo, Neural-network states for the classical simulation of

quantum computing (2018), arXiv:1808.05232.

[259] M. Medvidovi´c and G. Carleo, Classical variational simulation of the quantum approx-
imate optimization algorithm, npj Quantum Inf. 7, 101 (2021), doi:10.1038/s41534-
021-00440-z.

[260] M. P. Harrigan, K. J. Sung, M. Neeley, K. J. Satzinger, F. Arute, K. Arya, J. Atalaya, J. C.
Bardin, R. Barends, S. Boixo and et al., Quantum approximate optimization of non-
planar graph problems on a planar superconducting processor, Nat. Phys. 17, 332–336
(2021), doi:10.1038/s41567-020-01105-y.

[261] J. Carrasquilla, D. Luo, F. Pérez, A. Milsted, B. K. Clark, M. Volkovs and L. Aolita, Prob-
abilistic simulation of quantum circuits using a deep-learning architecture, Phys. Rev. A
104, 032610 (2021), doi:10.1103/PhysRevA.104.032610.

[262] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and
In Adv. Neural. Inf. Process. Syst. (2017),

I. Polosukhin, Attention is all you need,
arXiv:1706.03762.

[263] H.-P. Breuer and F. Petruccione, The Theory of Open Quantum Systems, Oxford Univer-
sity Press, ISBN 9780198520634, doi:10.1093/acprof:oso/9780199213900.001.0001
(2007).

[264] N. Yoshioka and R. Hamazaki,
open quantum many-body
doi:10.1103/PhysRevB.99.214306.

systems,

Constructing neural

Phys. Rev. B 99,

stationary states

for
214306 (2019),

[265] A. Nagy and V. Savona, Variational quantum Monte Carlo method with a neural-
Phys. Rev. Lett. 122, 250501 (2019),

network ansatz for open quantum systems,
doi:10.1103/PhysRevLett.122.250501.

[266] F. Vicentini, A. Biella, N. Regnault and C. Ciuti, Variational neural-network ansatz
Phys. Rev. Lett. 122, 250503 (2019),

for steady states in open quantum systems,
doi:10.1103/PhysRevLett.122.250503.

[267] M.

J. Hartmann and G. Carleo,
tive quantum many-body dynamics,
doi:10.1103/PhysRevLett.122.250502.

Neural-network approach to dissipa-
Phys. Rev. Lett. 122, 250502 (2019),

[268] D. Luo, Z. Chen,
network for
doi:10.1103/PhysRevLett.128.090501 (2022).

J. Carrasquilla and B. K. Clark,

simulating open quantum systems via a probabilistic

Autoregressive neural
formulation,

[269] M. Reh, M. Schmitt and M. Gärttner, Time-dependent variational principle for open
quantum systems with artiﬁcial neural networks, Phys. Rev. Lett. 127, 230501 (2021),
doi:10.1103/PhysRevLett.127.230501.

[270] F. Minganti, A. Biella, N. Bartolo and C. Ciuti,

for dissipative phase transitions,

villians
doi:10.1103/PhysRevA.98.042118.

theory of Liou-
Phys. Rev. A 98, 042118 (2018),

Spectral

[271] G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer, R. Melko and G. Carleo, Neural-network
quantum state tomography, Nat. Phys. 14, 447–450 (2018), doi:10.1038/s41567-018-
0048-5.

253

REFERENCES

[272] A. Szabó and C. Castelnovo, Neural network wave functions and the sign problem, Phys.

Rev. Res. 2, 033075 (2020), doi:10.1103/PhysRevResearch.2.033075.

[273] T. Schmale, M. Reh and M. Gärttner, Scalable quantum state tomography with artiﬁcial

neural networks (2021), arXiv:2109.13776.

[274] G. Torlai, B. Timar, E. P. L. van Nieuwenburg, H. Levine, A. Omran, A. Keesling,
H. Bernien, M. Greiner, V. Vuleti´c, M. D. Lukin, R. G. Melko and M. Endres, Integrating
neural networks with a quantum simulator for state reconstruction, Phys. Rev. Lett. 123,
230504 (2019), doi:10.1103/PhysRevLett.123.230504.

[275] S. Lohani, B. T. Kirby, M. Brodsky, O. Danaci and R. T. Glasser, Machine learning as-
sisted quantum state estimation, Mach. Learn.: Sci. Technol. 1(3), 035007 (2020),
doi:10.1088/2632-2153/ab9a21.

[276] S. Lohani, T. A. Searles, B. T. Kirby and R. T. Glasser, On the experimental feasibility
of quantum state reconstruction via machine learning, IEEE Trans. Quantum Eng. 2, 1
(2021), doi:10.1109/TQE.2021.3106958.

[277] S. Lohani, J. M. Lukens, D. E. Jones, T. A. Searles, R. T. Glasser and B. T. Kirby, Improving
application performance with biased distributions of quantum states, Phys. Rev. Research
3, 043145 (2021), doi:10.1103/PhysRevResearch.3.043145.

[278] S. Lohani, J. M. Lukens, R. T. Glasser, T. A. Searles and B. T. Kirby, Data-centric machine

learning in quantum information science (2022), arXiv:2201.09134.

[279] O. Danaci, S. Lohani, B. T. Kirby and R. T. Glasser, Machine learning pipeline for quantum

state estimation with incomplete measurements (2020), arXiv:2012.03104.

[280] S. Aaronson, Shadow tomography of quantum states, doi:10.1145/3188745.3188802

(2018).

[281] D. Pfau, J. S. Spencer, A. G. Matthews and W. M. C. Foulkes, Ab initio solution of the
many-electron Schrödinger equation with deep neural networks, Phys. Rev. Res. 2, 033429
(2020), doi:10.1103/PhysRevResearch.2.033429.

[282] C. Adams, G. Carleo, A. Lovato and N. Rocco, Variational Monte Carlo calculations of
A ≤ 4 nuclei with an artiﬁcial neural-network correlator ansatz, Phys. Rev. Lett. 127,
022502 (2021), doi:10.1103/PhysRevLett.127.022502.

[283] J. Bausch and F. Leditzky, Quantum codes from neural networks, New J. Phys. 22(2),

023005 (2020), doi:10.1088/1367-2630/ab6cdd.

[284] F. Vicentini, Machine learning toolbox for quantum many body physics, Nat. Rev. Phys.

3, 156 (2021), doi:10.1038/s42254-021-00285-7.

[285] J. Carrasquilla and G. Torlai, Neural networks in quantum many-body physics: a hands-

on tutorial (2021), arXiv:2101.11099.

[286] G. Carleo, Beijing lecture notes and code, Lecture Notes (2017).

[287] S. R. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, Bradford Book,

ISBN 9780262352703, doi:10.5555/980651.980663 (2018).

[288] R. S. Sutton, Learning to predict by the methods of temporal differences, Mach. Learn. 3,

9 (1988), doi:10.1007/BF00115009.

254

REFERENCES

[289] G. A. Rummery and M. Niranjan,

On-line Q-learning using connectionist sys-
tems, CUED/F-INFENG/TR 166, University of Cambridge, Department of Engineering
(1994).

[290] H. van Seijen, A. R. Mahmood, P. M. Pilarski, M. C. Machado and R. S. Sut-
J. Mach. Learn. Res. 17, 1 (2016),

ton, True online temporal-difference learning,
doi:10.48550/arXiv.1512.04087.

[291] G. H. John, When the best move isn’t optimal: Q-learning with exploration, In Proc. 12th

Nat. Conf. Artif. Intell. (Vol. 2) (1994).

[292] C. J. C. H. Watkins and P. Dayan,

Q-learning, Mach. Learn. 8, 279 (1992),

doi:10.1007/BF00992698.

[293] J. E. Smith and R. L. Winkler, The optimizer’s curse: Skepticism and postdecision surprise
in decision analysis, Manag. Sci. 52, 311 (2006), doi:10.1287/mnsc.1050.0451.

[294] S. Thrun and A. Schwartz,

Issues in using function approximation for reinforcement

learning, In Proc. 4th Connectionist Models Summer School (1993).

[295] H. Van Hasselt, Double Q-learning, In Conf. Neural Inf. Process. Syst., vol. 23 (2010).

[296] L.-J. Lin, Reinforcement Learning for Robots Using Neural Networks, Ph.D. thesis,

Carnegie Mellon University, USA, UMI Order No. GAX93-22750 (1992).

[297] H. Van Hasselt, A. Guez and D. Silver, Deep reinforcement learning with double Q-

learning, In Proc. AAAI Conf. Artif. Intell. (2016), arXiv:1509.06461.

[298] P. Marbach and J. N. Tsitsiklis, Simulation-based optimization of Markov reward pro-

cesses, IEEE Trans. Automat. Contr. 46, 191 (2001), doi:10.1109/9.905687.

[299] R. S. Sutton, D. McAllester, S. Singh and Y. Mansour, Policy gradient methods for re-
In NeurIPS 2000 - Adv. Neural Inf.

inforcement learning with function approximation,
Process. Syst. (2000).

[300] R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning, Mach. Learn. 8, 229 (1992), doi:10.1007/BF00992696.

[301] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross and V. Goel, Self-critical sequence train-
In Proc. IEEE Conf. Comput. Vision and Pattern Recognition,

ing for image captioning,
doi:10.1109/CVPR.2017.131 (2017).

[302] J. Schulman, P. Moritz, S. Levine, M. Jordan and P. Abbeel, High-dimensional continuous
control using generalized advantage estimation, In ICLR 2016 - Int. Conf. Learn. Represent.
(2016), arXiv:1506.02438.

[303] A. G. Barto, R. S. Sutton and C. W. Anderson, Neuronlike adaptive elements that can solve
difﬁcult learning control problems, IEEE Trans. Syst. Man Cybern. Syst. 5, 834 (1983),
doi:10.1109/TSMC.1983.6313077.

[304] V. Konda and J. Tsitsiklis, Actor-critic algorithms,

In Conf. Neural. Inf. Process. Syst.

(1999).

[305] T. Degris, M. White and R. S. Sutton, Off-policy actor-critic, In ICML 2012 - Conf. Mach.

Learn., p. 179–186. Omnipress (2012), arXiv:1205.4839.

255

REFERENCES

[306] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver and
K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, In ICML 2016 -
33th Int. Conf. Mach. Learn., vol. 48, pp. 1928–1937 (2016), arXiv:1602.01783.

[307] S.-i. Amari, Natural gradient works efﬁciently in learning, Neural Comput. 10(2), 251

(1998), doi:10.1162/089976698300017746.

[308] S. M. Kakade, A natural policy gradient, In NeurIPS 2002 - Conf. Neural. Inf. Process.

Syst., doi:10.5555/2980539.2980738 (2002).

[309] J. Peters and S. Schaal, Natural actor-critic, Neurocomputing 71(7), 1180 (2008),

doi:10.1016/j.neucom.2007.11.026.

[310] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh and M. Lee, Natural actor–critic algorithms,

Automatica 45(11), 2471 (2009), doi:10.1016/j.automatica.2009.07.008.

[311] J. Schulman, S. Levine, P. Abbeel, M. Jordan and P. Moritz, Trust region policy optimiza-

tion, In ICML 2015 - Int. Conf. Mach. Learn. (2015), arXiv:1502.05477.

[312] Y. Wu, E. Mansimov, R. B. Grosse, S. Liao and J. Ba, Scalable trust-region method for
deep reinforcement learning using Kronecker-factored approximation, In NeurIPS 2017 -
Adv. Neural. Inf. Process. Syst., vol. 30 (2017), arXiv:1708.05144.

[313] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov, Proximal policy optimiza-

tion algorithms (2017), arXiv:1707.06347.

[314] H. J. Briegel and G. De las Cuevas, Projective simulation for artiﬁcial intelligence, Sci.

Rep. 2(1), 1 (2012), doi:10.1038/srep00400.

[315] J. Mautner, A. Makmal, D. Manzano, M. Tiersch and H. J. Briegel, Projective simulation
for classical learning agents: a comprehensive investigation, New Gener. Comput. 33(1),
69 (2015), doi:10.1007/s00354-015-0102-0.

[316] A. A. Melnikov, A. Makmal and H. J. Briegel, Benchmarking projective simulation in nav-
igation problems, IEEE Access 6, 64639 (2018), doi:10.1109/ACCESS.2018.2876494.

[317] S. Jerbi, L. M. Trenkwalder, H. P. Nautrup, H. J. Briegel and V. Dunjko, Quantum en-
hancements for deep reinforcement learning in large spaces, PRX Quantum 2(1), 010328
(2021), doi:10.1103/PRXQuantum.2.010328.

[318] A. A. Melnikov, H. P. Nautrup, M. Krenn, V. Dunjko, M. Tiersch, A. Zeilinger and H. J.
Briegel, Active learning machine learns to create new quantum experiments, Proc. Natl.
Acad. Sci. U.S.A. 115(6), 1221 (2018), doi:10.1073/pnas.1714936115.

[319] W. L. Boyajian, J. Clausen, L. M. Trenkwalder, V. Dunjko and H. J. Briegel, On the conver-
gence of projective-simulation–based reinforcement learning in Markov decision processes,
Quantum Mach. Intell. 2(2), 1 (2020), doi:10.1007/s42484-020-00023-9.

[320] A. A. Melnikov, A. Makmal, V. Dunjko and H. J. Briegel, Projective simulation with

generalization, Sci. Rep. 7(1), 1 (2017), doi:10.1038/s41598-017-14740-y.

[321] K. Ried, B. Eva, T. Müller and H. J. Briegel, How a minimal learning agent can infer the
existence of unobserved variables in a complex environment (2019), arXiv:1910.06985.

[322] M. Campbell, A. J. Hoane Jr and F.-h. Hsu, Deep blue, Artiﬁcial intelligence 134(1-2),

57 (2002), doi:10.1016/S0004-3702(01)00129-1.

256

REFERENCES

[323] A. Yee and M. Alvarado, Pattern recognition and Monte-Carlo tree search for Go gaming
In IBERAMIA 2012 – Adv. Artif. Intell., doi:10.1007/978-3-642-

better automation,
34654-5_2 (2012).

[324] R. Coulom, Efﬁcient selectivity and backup operators in monte-carlo tree search, In Inter-
national conference on computers and games, pp. 72–83. Springer, doi:10.1007/978-3-
540-75538-8_7 (2006).

[325] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap et al., Mastering the game of Go without
human knowledge, Nature 550(7676), 354 (2017), doi:10.1038/nature24270.

[326] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan et al., A general reinforcement learning
algorithm that masters chess, Shogi, and Go through self-play, Science 362(6419), 1140
(2018), doi:10.1126/science.aar6404.

[327] OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison, D. Farhi,
Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz et al., Dota 2 with large scale deep
reinforcement learning (2019), arXiv:1912.06680.

[328] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. Mohanty,
D. Perez Liebana, R. Salakhutdinov, N. Topin et al., The MineRL 2019 competition on
sample efﬁcient reinforcement learning using human priors (2019), arXiv:1904.10079.

[329] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez,
E. Lockhart, D. Hassabis, T. Graepel, T. Lillicrap and D. Silver, Mastering Atari, Go,
chess and Shogi by planning with a learned model, Nature 588(7839), 604 (2020),
doi:10.1038/s41586-020-03051-4.

[330] F. Marquardt, Machine learning and quantum devices, SciPost Phys. Lect. Notes p. 29

(2021), doi:10.21468/SciPostPhysLectNotes.29.

[331] R. Porotti, A. Essing, B. Huard and F. Marquardt, A deep reinforcement learning for quan-

tum state preparation with weak nonlinear measurements (2021), arXiv:2107.08816.

[332] T. Fösel, P. Tighineanu, T. Weiss and F. Marquardt,

with neural networks for quantum feedback,
doi:10.1103/PhysRevX.8.031084.

learning
Reinforcement
Phys. Rev. X 8, 031084 (2018),

[333] S. Borah, B. Sarma, M. Kewming, G. J. Milburn and J. Twamley, Measurement-based
feedback quantum control with deep reinforcement learning for a double-well nonlinear
potential, Phys. Rev. Lett. 127, 190403 (2021), doi:10.1103/PhysRevLett.127.190403.

[334] V. Nguyen, S. B. Orbell, D. T. Lennon, H. Moon, F. Vigneau, L. C. Camenzind, L. Yu,
D. M. Zumbühl, G. A. D. Briggs, M. A. Osborne, D. Sejdinovic and N. Ares, Deep rein-
forcement learning for efﬁcient measurement of quantum devices, npj Quantum Inf. 7(1),
100 (2021), doi:10.1038/s41534-021-00434-x.

[335] J. Preskill, Quantum computing in the NISQ era and beyond, In Quantum [523], p. 79

(2018), doi:10.22331/q-2018-08-06-79.

[336] T. Fösel, M. Yuezhen Niu, F. Marquardt and L. Li, Quantum circuit optimization with

deep reinforcement learning (2021), arXiv:2103.07585.

257

REFERENCES

[337] W. K. Wootters and W. H. Zurek, A single quantum cannot be cloned, Nature 299(5886),

802 (1982), doi:10.1038/299802a0.

[338] P. W. Shor, Scheme for reducing decoherence in quantum computer memory, Phys. Rev. A

52, R2493 (1995), doi:10.1103/PhysRevA.52.R2493.

[339] A. M. Steane, Error correcting codes in quantum theory, Phys. Rev. Lett. 77, 793 (1996),

doi:10.1103/PhysRevLett.77.793.

[340] D. Gottesman, An introduction to quantum error correction and fault-tolerant quantum

computation (2009), arXiv:0904.2557.

[341] R. Sweke, M. S. Kesselring, E. P. L. van Nieuwenburg and J. Eisert, Reinforcement learn-
ing decoders for fault-tolerant quantum computation, Mach. Learn.: Sci. Technol. 2(2),
025005 (2021), doi:10.1088/2632-2153/abc609.

[342] P. Andreasson, J. Johansson, S. Liljestrand and M. Granath, Quantum error correc-
tion for the toric code using deep reinforcement learning, Quantum 3, 183 (2019),
doi:10.22331/q-2019-09-02-183.

[343] D. Fitzek, M. Eliasson, A. F. Kockum and M. Granath,

coder for depolarizing noise on the toric code,
doi:10.1103/PhysRevResearch.2.023230.

Deep Q-learning de-
Phys. Rev. Res. 2, 023230 (2020),

[344] H. Théveniaut and E. van Nieuwenburg, A NEAT Quantum Error Decoder, SciPost Phys.

11, 5 (2021), doi:10.21468/SciPostPhys.11.1.005.

[345] M. Erhard, M. Krenn and A. Zeilinger, Advances in high-dimensional quantum entangle-

ment, Nat. Rev. Phys. 2(7), 365 (2020), doi:10.1038/s42254-020-0193-5.

[346] M. Krenn, M. Malik, R. Fickler, R. Lapkiewicz and A. Zeilinger,

Automated
Phys. Rev. Lett. 116, 090405 (2016),

search for new quantum experiments,
doi:10.1103/PhysRevLett.116.090405.

[347] M. Krenn, J. S. Kottmann, N. Tischler and A. Aspuru-Guzik, Conceptual understanding
through efﬁcient automated design of quantum optical experiments, Phys. Rev. X 11,
031044 (2021), doi:10.1103/PhysRevX.11.031044.

[348] M. Krenn, M. Erhard and A. Zeilinger, Computer-inspired quantum experiments, Nat.

Rev. Phys. 2, 649 (2020), doi:10.1038/s42254-020-0230-4.

[349] A. Peres, Separability criterion for density matrices, Phys. Rev. Lett. 77(8), 1413 (1996),

doi:10.1103/PhysRevLett.77.1413.

[350] B. Requena, G. Muñoz-Gil, M. Lewenstein, V. Dunjko and J. Tura, Certiﬁcates of quantum
many-body properties assisted by machine learning (2021), arXiv:2103.03830.

[351] M. Bukov, A. G. R. Day, D. Sels, P. Weinberg, A. Polkovnikov and P. Mehta, Reinforce-
ment learning in different phases of quantum control, Phys. Rev. X 8, 031086 (2018),
doi:10.1103/PhysRevX.8.031086.

[352] M. Y. Niu, S. Boixo, V. N. Smelyanskiy and H. Neven, Universal quantum control through
deep reinforcement learning, npj Quantum Inf. 5(33), 1 (2019), doi:10.1038/s41534-
019-0141-3.

[353] K. A. McKiernan, E. Davis, M. S. Alam and C. Rigetti, Automated quantum programming

via reinforcement learning for combinatorial optimization (2019), arXiv:1908.08054.

258

[354] Y.-H. Zhang, P.-L. Zheng, Y. Zhang and D.-L. Deng,

Topological quantum
Phys. Rev. Lett. 125, 170501 (2020),

compiling with reinforcement
doi:10.1103/PhysRevLett.125.170501.

learning,

REFERENCES

[355] C. Cao, Z. An, S.-Y. Hou, D. L. Zhou and B. Zeng,

evolution steered by reinforcement learning,
doi:10.1038/s42005-022-00837-y.

Quantum imaginary time
Commun. Phys. 5(57), 1 (2022),

[356] F. Metz and M. Bukov, Self-correcting quantum many-body control using reinforcement

learning with tensor networks (2022), arXiv:2201.11790.

[357] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra and M. Riedmiller,

De-
In ICML 2014 - Int. Conf. Mach. Learn.,

terministic policy gradient algorithms,
doi:10.5555/3044805.3044850 (2014).

[358] S. Levine, Reinforcement learning and control as probabilistic inference: Tutorial and

review (2018), arXiv:1805.00909.

[359] T. Haarnoja, A. Zhou, P. Abbeel and S. Levine, Soft actor-critic: Off-policy maximum
In ICML 2018 - Int. Conf.

entropy deep reinforcement learning with a stochastic actor,
Mach. Learn., pp. 1861–1870 (2018), arXiv:1801.01290.

[360] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess and M. Riedmiller,

Maximum a posteriori policy optimisation (2018), arXiv:1806.06920.

[361] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner,
A. Abdolmaleki, D. de las Casas, C. Donner, L. Fritz et al., Magnetic control of toka-
mak plasmas through deep reinforcement learning, Nature 602(7897), 414 (2022),
doi:10.1038/s41586-021-04301-9.

[362] A. Karpathy, Software 2.0, Medium, Accessed: 2022-04-08 (2017).

[363] C. D. Schuman, T. E. Potok, R. M. Patton, J. D. Birdwell, M. E. Dean, G. S. Rose and J. S.
Plank, A survey of neuromorphic computing and neural networks in hardware (2017),
arXiv:1808.05232.

[364] K. Roy, A. Jaiswal and P. Panda, Towards spike-based machine intelligence with neuro-
morphic computing, Nature 575(7784), 607 (2019), doi:10.1038/s41586-019-1677-2.

[365] M. Innes, A. Edelman, K. Fischer, C. Rackauckas, E. Saba, V. B. Shah and W. Tebbutt,
A differentiable programming system to bridge machine learning and scientiﬁc computing
(2019), arXiv:1907.07587.

[366] S. G. Johnson, Notes on adjoint methods for 18.335, Introduction to Numerical Methods

(2012).

[367] R. T. Chen, Y. Rubanova, J. Bettencourt and D. K. Duvenaud, Neural ordinary differential
equations, In NeurIPS 2018 - Adv. Neural. Inf. Process. Syst. (2018), arXiv:1806.07366.

[368] H.-J. Liao, J.-G. Liu, L. Wang and T. Xiang, Differentiable programming tensor networks,

Phys. Rev. X 9(3), 031041 (2019), doi:10.1103/PhysRevX.9.031041.

[369] B.-B. Chen, Y. Gao, Y.-B. Guo, Y. Liu, H.-H. Zhao, H.-J. Liao, L. Wang, T. Xiang, W. Li and
Z. Y. Xie, Automatic differentiation for second renormalization of tensor networks, Phys.
Rev. B 101, 220409 (2020), doi:10.1103/PhysRevB.101.220409.

259

REFERENCES

[370] G. Torlai, J. Carrasquilla, M. T. Fishman, R. G. Melko and M. P. A. Fisher, Wave-
function positivization via automatic differentiation, Phys. Rev. Res. 2, 032060 (2020),
doi:10.1103/PhysRevResearch.2.032060.

[371] J. Ingraham, A. Riesselman, C. Sander and D. Marks, Learning protein structure with a

differentiable simulator, In ICLR 2018 - Int. Conf. Learn. Represent. (2018).

[372] S. S. Schoenholz and E. D. Cubuk, JAX, M.D.: A framework for differentiable physics, In
NeurIPS 2020 - Adv. Neural Inf. Process. Syst., vol. 33 (2020), arXiv:1912.04232.

[373] T. Tamayo-Mendoza, C. Kreisbeck, R. Lindh and A. Aspuru-Guzik, Automatic differen-
tiation in quantum chemistry with applications to fully variational Hartree–Fock, ACS
Cent. Sci. 4(5), 559 (2018), doi:10.1021/acscentsci.7b00586.

[374] L. Zhao and E. Neuscamman, Excited state mean-ﬁeld theory without automatic differ-
entiation, J. Chem. Phys. 152(20), 204112 (2020), doi:10.1063/5.0003438.

[375] L. Li, S. Hoyer, R. Pederson, R. Sun, E. D. Cubuk, P. Riley, K. Burke et al., Kohn-sham
equations as regularizer: Building prior knowledge into machine-learned physics, Phys.
Rev. Lett. 126(3), 036401 (2021), doi:10.1103/PhysRevLett.126.036401.

[376] M. F. Kasim and S. M. Vinko, Learning the exchange-correlation functional from nature
with fully differentiable density functional theory, Phys. Rev. Lett. 127, 126403 (2021),
doi:10.1103/PhysRevLett.127.126403.

[377] A. S. Abbott, B. Z. Abbott, J. M. Turney and H. F. Schaefer, Arbitrary-order derivatives
of quantum chemical methods via automatic differentiation, J. Phys. Chem. Lett. 12(12),
3232 (2021), doi:10.1021/acs.jpclett.1c00607, PMID: 33764068.

[378] M. F. Kasim, S. Lehtola and S. M. Vinko, Dqc: A python program package for differentiable

quantum chemistry, doi:10.1063/5.0076202 (2022).

[379] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S. Alam, S. Ahmed, J. M. Arra-
zola, C. Blank, A. Delgado, S. Jahangiri, K. McKiernan, J. J. Meyer et al.,
Pen-
nyLane: Automatic differentiation of hybrid quantum-classical computations (2020),
arXiv:1811.04968.

[380] N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbrüggen and S. J. Glaser, Optimal control
of coupled spin dynamics: design of NMR pulse sequences by gradient ascent algorithms,
J. Magn. Reson. 172(2), 296 (2005), doi:10.1016/j.jmr.2004.11.004.

[381] N. Leung, M. Abdelhafez, J. Koch and D. Schuster, Speedup for quantum optimal control
from automatic differentiation based on graphics processing units, Phys. Rev. A 95(4)
(2017), doi:10.1103/PhysRevA.95.042318.

[382] M. Abdelhafez, D. I. Schuster and J. Koch, Gradient-based optimal control of open quan-
tum systems using quantum trajectories and automatic differentiation, Phys. Rev. A 99,
052327 (2019), doi:10.1103/PhysRevA.99.052327.

[383] H. Jirari, Optimal population inversion of a single dissipative two-level system, Eur. Phys.

J. B 92(12), 265 (2019), doi:10.1140/epjb/e2019-100378-x.

[384] H. Jirari, Time-optimal bang-bang control for the driven spin-boson system, Phys. Rev. A

102, 012613 (2020), doi:10.1103/PhysRevA.102.012613.

260

REFERENCES

[385] F. Schäfer, M. Kloc, C. Bruder and N. Lörch, A differentiable programming method for
quantum control, Mach. Learn.: Sci. Technol. 1(3), 035009 (2020), doi:10.1088/2632-
2153/ab9802.

[386] R. A. Vargas-Hernández, R. T. Q. Chen, K. A. Jung and P. Brumer,

Inverse design of
dissipative quantum steady-states with implicit differentiation (2020), arXiv:2011.12808.

[387] R. A. Vargas-Hernández, R. T. Q. Chen, K. A. Jung and P. Brumer, Fully differentiable
optimization protocols for non-equilibrium steady states, New J. Phys. 23(12), 123006
(2021), doi:10.1088/1367-2630/ac395e.

[388] I. Khait,

J. Carrasquilla and D. Segal,

mal machines using machine learning,
doi:10.1103/physrevresearch.4.l012029.

Optimal control of quantum ther-
Physical Review Research 4(1) (2022),

[389] L. Coopmans, D. Luo, G. Kells, B. K. Clark and J. Carrasquilla, Protocol discovery for
the quantum control of majoranas by differentiable programming and natural evolution
strategies, PRX Quantum 2(2), 020332 (2021), doi:10.1103/PRXQuantum.2.020332.

[390] F. Schäfer, P. Sekatski, M. Koppenhöfer, C. Bruder and M. Kloc, Control of stochastic
quantum dynamics by differentiable programming, Mach. Learn.: Sci. Technol. 2(3),
035004 (2021), doi:10.1088/2632-2153/abec22.

[391] X.-Z. Luo, J.-G. Liu, P. Zhang and L. Wang, Yao. jl: Extensible, efﬁcient framework for
quantum algorithm design, Quantum 4, 341 (2020), doi:10.22331/q-2020-10-11-341.

[392] O. Kyriienko, A. E. Paine and V. E. Elfving,

tions with differentiable quantum circuits,
doi:10.1103/PhysRevA.103.052416.

Solving nonlinear differential equa-
Phys. Rev. A 103(5), 052416 (2021),

[393] P. Huembeli and A. Dauphin, Characterizing the loss landscape of variational quantum
circuits, Quantum Sci. Technol. 6(2), 025011 (2021), doi:10.1088/2058-9565/abdbc9.

[394] A. G. Baydin, B. A. Pearlmutter, A. A. Radul and J. M. Siskind, Automatic differenti-
ation in machine learning: a survey, J. Mach. Learn. Res. 18(1), 5595–5637 (2018),
doi:10.5555/3122009.3242010.

[395] R. E. Wengert, A simple automatic derivative evaluation program, Commun. ACM 7(8),

463 (1964), doi:10.1145/355586.364791.

[396] A. Griewank and A. Walther,

Algorithmic Differentiation, Society for Industrial and Applied Mathematics,
9780898716597, doi:10.5555/1455489 (2008).

Evaluating Derivatives: Principles and Techniques of
ISBN

[397] S. Linnainmaa, The representation of the cumulative rounding error of an algorithm as a
taylor expansion of the local rounding errors, Master’s Thesis (in Finnish), Univ. Helsinki
pp. 6–7 (1970).

[398] A. Griewank, Who invented the reverse mode of differentiation, Documenta Math., Ac-

cessed: 2022-04-01 (2012).

[399] C. Rackauckas, Parallel computing and scientiﬁc machine learning, 18.337J/6.338J

Lecture notes, MIT Lecture (2020).

[400] Y. Ma, V. Dixit, M. Innes, X. Guo and C. Rackauckas, A comparison of automatic differen-
tiation and continuous sensitivity analysis for derivatives of differential equation solutions
(2021), arXiv:1812.01892.

261

REFERENCES

[401] L. Wang, Implementation of an inverse Schrödinger problem in JAX, Available as Google
https://colab.research.google.com/drive/1e1NFA-E1Th7nN_

Colab Notebook:
9-DzQjAaglH6bwZtVU?usp=sharing (2021).

[402] H. Xie, J.-G. Liu and L. Wang,

Automatic differentiation of dominant eigen-
solver and its applications in quantum physics, Phys. Rev. B 101, 245139 (2020),
doi:10.1103/PhysRevB.101.245139.

[403] L. Wang, Implementation of a quantum optimal control problem in JAX, Accessed: 2022-

03-11. Available as Google Colab Notebook (2021).

[404] Q. Wang, R. Hu and P. Blonigan,

ysis of chaotic limit cycle oscillations,
doi:10.1016/j.jcp.2014.03.002.

Least squares shadowing sensitivity anal-
J. Comput. Phys 267, 210 (2014),

[405] L. Metz, C. D. Freeman, S. S. Schoenholz and T. Kachman, Gradients are not all you

need (2021), arXiv:2111.05803.

[406] W. S. Moses and V. Churavy,

Instead of rewriting foreign code for machine learning,

automatically synthesize fast gradients (2020), arXiv:2010.01709.

[407] J.-G. Liu and T. Zhao, Differentiate everything with a reversible embeded domain-speciﬁc

language (2020), arXiv:2003.04617.

[408] B. Silverman,

Density Estimation for Statistics and Data Analysis,

Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis,
9781315140919, doi:10.1201/9781315140919 (1998).

Chapman &
ISBN

[409] Z. Hradil,

Quantum-state estimation,

Phys. Rev. A 55, R1561 (1997),

doi:10.1103/PhysRevA.55.R1561.

[410] M. Paris and J. Rehacek, Quantum State Estimation, Lecture Notes in Physics. Springer-
Verlag Berlin/Heidelberg, ISBN 9783642061035, doi:10.1007/b98673 (2004).

[411] Y. S. Teo,

Introduction to quantum-state estimation, World Scientiﬁc,

ISBN

9789814678865, doi:10.1142/9617 (2015).

[412] G. Kanwar, M. S. Albergo, D. Boyda, K. Cranmer, D. C. Hackett, S. Racaniere, D. J.
Rezende and P. E. Shanahan, Equivariant ﬂow-based sampling for lattice gauge theory,
Phys. Rev. Lett. 125(12), 121601 (2020), doi:10.1103/PhysRevLett.125.121601.

[413] K. A. Nicoli, C. J. Anders, L. Funcke, T. Hartung, K. Jansen, P. Kessel, S. Naka-
thermodynamic observables in lattice ﬁeld
Phys. Rev. Lett. 126(3), 032001 (2021),

jima and P. Stornati,
theories with deep generative models,
doi:10.1103/PhysRevLett.126.032001.

Estimation of

[414] D. C. Hackett, C.-C. Hsieh, M. S. Albergo, D. Boyda, J.-W. Chen, K.-F. Chen, K. Cranmer,
G. Kanwar and P. E. Shanahan, Flow-based sampling for multimodal distributions in
lattice ﬁeld theory (2021), arXiv:2107.00734.

[415] K. A. Nicoli, C. Anders, L. Funcke, T. Hartung, K. Jansen, P. Kessel, S. Nakajima and
P. Stornati, Machine learning of thermodynamic observables in the presence of mode col-
lapse (2021), arXiv:2111.11303.

[416] I. Kobyzev, S. Prince and M. Brubaker, Normalizing ﬂows: An introduction and review
IEEE Trans. Pattern Anal. Mach. Intell. 11, 3964–3979 (2021),

of current methods,
doi:10.1109/TPAMI.2020.2992934.

262

REFERENCES

[417] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves et al., Conditional
image generation with PixelCNN decoders, Advances in neural information processing
systems 29 (2016), arXiv:1606.05328.

[418] D. P. Kingma and M. Welling, An introduction to variational autoencoders, Foundations
and Trends® in Machine Learning 12(4), 307 (2019), doi:10.1561/2200000056.

[419] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta and A. A. Bharath,
Generative adversarial networks: An overview, IEEE Signal Processing Magazine 35(1),
53 (2018), doi:10.1109/MSP.2017.2765202.

[420] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville
and Y. Bengio, Generative adversarial nets, Advances in neural information processing
systems 27 (2014), arXiv:1406.2661.

[421] J. M. Pawlowski and J. M. Urban, Reducing autocorrelation times in lattice simulations
with generative adversarial networks, Mach. Learn.: Sci. Technol. 1(4), 045011 (2020),
doi:10.1088/2632-2153/abae73.

[422] R. G. Melko, G. Carleo, J. Carrasquilla and J. I. Cirac, Restricted Boltzmann machines in

quantum physics, Nat. Phys. 15(9), 887 (2019), doi:10.1038/s41567-019-0545-1.

[423] M. S. Albergo, G. Kanwar and P. E. Shanahan,

Flow-based generative models for
Markov chain Monte Carlo in lattice ﬁeld theory, Phys. Rev. D 100, 034515 (2019),
doi:10.1103/PhysRevD.100.034515.

[424] P. Ramachandran, T. L. Paine, P. Khorrami, M. Babaeizadeh, S. Chang, Y. Zhang, M. A.
Hasegawa-Johnson, R. H. Campbell and T. S. Huang, Fast generation for convolutional
autoregressive models (2017), arXiv:1704.06001.

[425] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed and B. Lakshminarayanan,

Normalizing ﬂows for probabilistic modeling and inference (2019), arXiv:1912.02762.

[426] U. Wolff, A. Collaboration et al., Monte Carlo errors with less errors, Comput. Phys.

Commun. 156(2), 143 (2004), doi:10.1016/S0010-4655(03)00467-3.

[427] P. Białas, P. Korcyl and T. Stebel, Analysis of autocorrelation times in neural Markov Chain

Monte Carlo simulations (2021), arXiv:2111.10189.

[428] J. Köhler, L. Klein and F. Noé, Equivariant ﬂows: exact likelihood generative learning
for symmetric densities, In ICML 2020 - Int. Conf. Mach. Learn., pp. 5361–5370. PMLR
(2020), arXiv:2006.02425.

[429] V. G. Satorras, E. Hoogeboom, F. B. Fuchs, I. Posner and M. Welling, E(n) equivariant

normalizing ﬂows (2021), arXiv:2105.09016.

[430] G. Jerfel, S. Wang, C. Wong-Fannjiang, K. A. Heller, Y. Ma and M. I. Jordan, Varia-
tional reﬁnement for importance sampling using the forward kullback-leibler divergence,
In Uncertainty in Artiﬁcial Intelligence, pp. 1819–1829. PMLR (2021).

[431] L. Wang, Generative models for physicists, Tech. rep., Institute of Physics, Chinese

Academy of Sciences, GitHub.io (2018).

[432] F. Noé, S. Olsson, J. Köhler and H. Wu, Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning, Science 365(6457), eaaw1147 (2019),
doi:10.1126/science.aaw1147.

263

REFERENCES

[433] M. S. Albergo, G. Kanwar, S. Racanière, D. J. Rezende, J. M. Urban, D. Boyda, K. Cran-
mer, D. C. Hackett and P. E. Shanahan, Flow-based sampling for fermionic lattice ﬁeld
theories, Phys. Rev. D 104, 114507 (2021), doi:10.1103/PhysRevD.104.114507.

[434] B. S. Rem, N. Käming, M. Tarnowski, L. Asteria, N. Fläschner, C. Becker, K. Sengstock
and C. Weitenberg, Identifying quantum phase transitions using artiﬁcial neural networks
on experimental data, Nat. Phys. 15, 917 (2019), doi:10.1038/s41567-019-0554-0.

[435] E. Lustig, O. Yair, R. Talmon and M. Segev,

Identifying topological phase transitions
in experiments using manifold learning, Phys. Rev. Lett. 125(12), 127401 (2020),
doi:10.1103/PhysRevLett.125.127401.

[436] E. Greplova, C. Gold, B. Kratochwil, T. Davatz, R. Pisoni, A. Kurzmann, P. Rick-
Fully automated identiﬁcation
Ihn and S. D. Huber,
Phys. Rev. Appl. 13(6), 064017 (2020),

haus, M. H. Fischer, T.
of two-dimensional material samples,
doi:10.1103/PhysRevApplied.13.064017.

[437] R. Durrer, B. Kratochwil, J. V. Koski, A. J. Landig, C. Reichl, W. Wegscheider,
Automated tuning of double quantum dots into spe-
T.
ciﬁc charge states using neural networks, Phys. Rev. Appl. 13(5), 054019 (2020),
doi:10.1103/PhysRevApplied.13.054019.

Ihn and E. Greplova,

[438] E. Greplova and S. Huber group, GitHub repository to “Fully automated search for 2D

material samples” (2019).

[439] P. Mostosi, H. Schindelin, P. Kollmannsberger and A. Thorn,

Haruspex:
A neural network for the automatic identiﬁcation of oligonucleotides and pro-
tein secondary structure
Angew. Chem.
doi:https://doi.org/10.1002/anie.202000421,
Int. Ed. 59(35), 14788 (2020),
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.202000421.

in cryo-electron microscopy maps,

[440] K. Nolte, Y. Gao, S. Stäb, P. Kollmannsberger and A. Thorn, Detecting ice artefacts in
processed macromolecular diffraction data with machine learning, Acta Crystallogr. D
78(2), 187 (2022), doi:10.1107/S205979832101202X.

[441] A. U. Lode, R. Lin, M. Büttner, L. Papariello, C. Lévêque, R. Chitra, M. C. Tsat-
sos, D. Jaksch and P. Molignini, Optimized observable readout from single-shot im-
ages of ultracold atoms via machine learning, Phys. Rev. A 104(4), L041301 (2021),
doi:10.1103/PhysRevA.104.L041301.

[442] R. Lin, C. Georges, J. Klinder, P. Molignini, M. Büttner, A. U. Lode, R. Chitra,
A. Hemmerich and H. Keßler, Mott transition in a cavity-boson system: A quanti-
tative comparison between theory and experiment, SciPost Phys. 11(2), 030 (2021),
doi:10.21468/SciPostPhys.11.2.030.

[443] R. Lin, P. Molignini, L. Papariello, M. C. Tsatsos, C. Lévêque, S. E. Weiner, E. Fasshauer,
R. Chitra and A. U. Lode, MCTDH-X: The multiconﬁgurational time-dependent Hartree
method for indistinguishable particles software, Quantum Sci. Technol. 5(2), 024004
(2020), doi:10.1088/2058-9565/ab788b.

[444] J. Zhang, G. Pagano, P. W. Hess, A. Kyprianidis, P. Becker, H. Kaplan, A. V. Gorshkov, Z.-X.
Gong and C. Monroe, Observation of a many-body dynamical phase transition with a 53-
qubit quantum simulator, Nature 551(7682), 601 (2017), doi:10.1038/nature24654.

264

REFERENCES

[445] H. Bernien, S. Schwartz, A. Keesling, H. Levine, A. Omran, H. Pichler, S. Choi, A. S.
Zibrov, M. Endres, M. Greiner et al., Probing many-body dynamics on a 51-atom quantum
simulator, Nature 551(7682), 579 (2017), doi:10.1038/nature24622.

[446] B. Chiaro, C. Neill, A. Bohrdt, M. Filippone, F. Arute, K. Arya, R. Babbush, D. Bacon,
J. Bardin, R. Barends, S. Boixo, D. Buell et al., Direct measurement of nonlocal in-
teractions in the many-body localized phase, doi:10.1103/PhysRevResearch.4.013148
(2022).

[447] M. Rispoli, A. Lukin, R. Schittko, S. Kim, M. E. Tai, J. Léonard and M. Greiner, Quan-
tum critical behaviour at the many-body localization transition, Nature 573(7774), 385
(2019), doi:10.1038/s41586-019-1527-2.

[448] A. Valenti, G. Jin, J. Léonard, S. D. Huber and E. Greplova, Scalable Hamiltonian learning
for large-scale out-of-equilibrium quantum dynamics, Phys. Rev. A 105, 023302 (2022),
doi:10.1103/PhysRevA.105.023302.

[449] A. Gresch, L. Bittel and M. Kliesch, Scalable approach to many-body localization via

quantum data (2022), arXiv:2202.08853.

[450] A. Cervera-Lierta, M. Krenn and A. Aspuru-Guzik, Design of quantum optical experiments

with logic artiﬁcial intelligence (2021), arXiv:2109.13273.

[451] D. Flam-Shepherd, T. Wu, X. Gu, A. Cervera-Lierta, M. Krenn and A. Aspuru-Guzik,
Learning interpretable representations of entanglement in quantum optics experiments us-
ing deep generative models (2021), arXiv:2109.02490.

[452] R. D. King, J. Rowland, S. G. Oliver, M. Young, W. Aubrey, E. Byrne, M. Liakata,
M. Markham, P. Pir, L. N. Soldatova, A. Sparkes, K. E. Whelan et al., The automation of
science, Science 324(5923), 85 (2009), doi:10.1126/science.1165620.

[453] F. Häse,

L. M. Roch and A. Aspuru-Guzik,

tation with self-driving
doi:10.1016/j.trechm.2019.02.007.

laboratories,

Trends Chem. 1(3),

Next-generation experimen-
282 (2019),

[454] A. A. Gentile, B. Flynn, S. Knauer, N. Wiebe, S. Paesani, C. E. Granade, J. G. Rarity,
R. Santagati and A. Laing, Learning models of quantum systems from experiments, Nat.
Phys. 17(7), 837 (2021), doi:10.1038/s41567-021-01201-7.

[455] T. E. O’Brien, L. B. Ioffe, Y. Su, D. Fushman, H. Neven, R. Babbush and V. Smelyanskiy,
Quantum computation of molecular structure using data from challenging-to-classically-
simulate nuclear magnetic resonance experiments (2021), arXiv:2109.02163.

[456] N. M. van Esbroeck, D. T. Lennon, H. Moon, V. Nguyen, F. Vigneau, L. C. Camenzind,
L. Yu, D. M. Zumbühl, G. A. D. Briggs, D. Sejdinovic and N. Ares, Quantum device
ﬁne-tuning using unsupervised embedding learning, New J. Phys. 22(9), 095003 (2020),
doi:10.1088/1367-2630/abb64c.

[457] B. Severin, D. T. Lennon, L. C. Camenzind, F. Vigneau, F. Fedele, D. Jirovec, A. Bal-
labio, D. Chrastina, G. Isella, M. de Kruijf, M. J. Carballido, S. Svab et al., Cross-
architecture tuning of silicon and SiGe-based quantum devices using machine learning
(2021), arXiv:2107.12975.

[458] J. P. Zwolak, T. McJunkin, S. S. Kalantre, S. F. Neyens, E. MacQuarrie, M. A. Eriksson
and J. M. Taylor, Ray-based framework for state identiﬁcation in quantum dot devices,
PRX Quantum 2, 020335 (2021), doi:10.1103/PRXQuantum.2.020335.

265

REFERENCES

[459] N. Wiebe, A. Kapoor and K. M. Svore, Quantum algorithms for nearest-neighbor methods
for supervised and unsupervised learning, Quantum Inf. Comput. 15(3–4), 316–356
(2015).

[460] P. Raccuglia, K. C. Elbert, P. D. F. Adler, C. Falk, M. B. Wenny, A. Mollo, M. Zeller, S. A.
Friedler, J. Schrier and A. J. Norquist, Machine-learning-assisted materials discovery
using failed experiments, Nature 533(7601), 73 (2016), doi:10.1038/nature17439.

[461] L. Zdeborová, Understanding deep learning is also a job for physicists, Nat. Phys. 16(6),

602 (2020), doi:10.1038/s41567-020-0929-2.

[462] M.

Belkin,

D. Hsu,

S. Ma

ern machine-learning
off,
arXiv:https://www.pnas.org/content/116/32/15849.full.pdf.

PNAS 116(32),

15849 (2019),

classical

practice

and
and

S. Mandal,
the

Reconciling mod-
trade-
bias–variance
doi:10.1073/pnas.1903070116,

[463] T. M. Cover, Geometrical and statistical properties of systems of linear inequalities
IEEE Trans. Comput. EC-14(3), 326 (1965),

with applications in pattern recognition,
doi:10.1109/PGEC.1965.264137.

[464] E. Gardner, Maximum storage capacity in neural networks, EPL 4(4), 481 (1987),

doi:10.1209/0295-5075/4/4/016.

[465] D. J. Amit, H. Gutfreund and H. Sompolinsky, Storing inﬁnite numbers of patterns
Phys. Rev. Lett. 55(14), 1530 (1985),

in a spin-glass model of neural networks,
doi:10.1103/PhysRevLett.55.1530.

[466] B. Derrida, E. Gardner and A. Zippelius, An exactly solvable asymmetric neural network

model, EPL 4(2), 167 (1987), doi:10.1209/0295-5075/4/2/007.

[467] B. Derrida and J. P. Nadal, Learning and forgetting on asymmetric, diluted neural net-

works, J. Stat. Phys. 49(5-6), 993 (1987), doi:10.1007/BF01017556.

[468] C. Peterson, A mean ﬁeld theory learning algorithm for neural networks, Complex Syst.

1, 995 (1987).

[469] W. Krauth and M. Mézard, Storage capacity of memory networks with binary couplings,

J. Phys. 50(20), 3057 (1989), doi:10.1051/jphys:0198900500200305700.

[470] G. Györgyi, First-order transition to perfect generalization in a neural network with binary
synapses, Phys. Rev. A 41(12), 7097 (1990), doi:10.1103/PhysRevA.41.7097.

[471] M. Opper and D. Haussler, Generalization performance of Bayes optimal classiﬁca-
Phys. Rev. Lett. 66(20), 2677 (1991),

tion algorithm for learning a perceptron,
doi:10.1103/PhysRevLett.66.2677.

[472] D. Sherrington and S. Kirkpatrick, Solvable model of a spin-glass, Phys. Rev. Lett. 35(26),

1792 (1975), doi:10.1103/PhysRevLett.35.1792.

[473] M. Mézard, G. Parisi and M. A. Virasoro, SK model: The replica solution without replicas,

EPL 1(2), 77 (1986), doi:10.1209/0295-5075/1/2/006.

[474] F. Barahona, On the computational complexity of Ising spin glass models, J. Phys. A:

Math. Gen. 15, 3241 (1982), doi:10.1088/0305-4470/15/10/028.

[475] C. Fan, M. Shen, Z. Nussinov, Z. Liu, Y. Sun and Y.-Y. Liu, Finding spin glass ground states

through deep reinforcement learning (2021), arXiv:2109.14411.

266

REFERENCES

[476] S. F. Edwards and P. W. Anderson, Theory of spin glasses, J. Phys. F: Met. Phys. 5, 965

(1975), doi:10.1088/0305-4608/5/5/017.

[477] D. J. Thouless, P. W. Anderson and R. G. Palmer, Solution of ‘Solvable model of a spin

glass’, Philos. Mag. 35(3), 593 (1977), doi:10.1080/14786437708235992.

[478] M. Talagrand,

The Parisi

formula,

Ann. Math. 163(1), 221 (2006),

doi:10.4007/annals.2006.163.221.

[479] D. Panchenko,

Introduction to the SK model, Current Developments in Mathematics

2014(1), 231 (2014), doi:10.4310/cdm.2014.v2014.n1.a4.

[480] T. Castellani and A. Cavagna, Spin-glass theory for pedestrians, J. Stat. Mech. p. P05012

(2005), doi:10.1088/1742-5468/2005/05/P05012.

[481] M. Gabrié, Mean-ﬁeld inference methods for neural networks, J. Phys. A: Math. Theor.

53(22), 223002 (2020), doi:10.1088/1751-8121/ab7f65.

[482] J. Barbier, F. Krzakala, N. Macris, L. Miolane and L. Zdeborová, Optimal errors and phase
transitions in high-dimensional generalized linear models, Proc. Natl. Acad. Sci. U.S.A.
116(12), 5451 (2019), doi:10.1073/pnas.1802705116.

[483] S. Rangan, Generalized approximate message passing for estimation with random linear
mixing, In 2011 IEEE International Symposium on Information Theory Proceedings. IEEE,
doi:10.1109/isit.2011.6033942 (2011).

[484] L. Zdeborová and F. Krzakala, Statistical physics of inference: thresholds and algorithms,

Adv. Phys. 65(5), 453 (2016), doi:10.1080/00018732.2016.1211393.

[485] R. Monasson and R. Zecchina, Learning and generalization theories of large committee
machines, Mod. Phys. Lett. B 09(30), 1887 (1995), doi:10.1142/s0217984995001868.

[486] B. Aubin, A. Maillard, J. Barbier, F. Krzakala, N. Macris and L. Zdeborová, The committee
machine: computational to statistical gaps in learning a two-layers neural network, J. Stat.
Mech. Theor. Exp. 2019(12), 124023 (2019), doi:10.1088/1742-5468/ab43d2.

[487] A. Rahimi and B. Recht, Random features for large-scale kernel machines, In NIPS 2007
- Adv. Neural. Inf. Process. Syst., vol. 20, doi:10.5555/2981562.2981710 (2007).

[488] A. Rahimi and B. Recht, Weighted sums of random kitchen sinks: replacing minimization
with randomization in learning, In NIPS 2008 - Adv. Neural. Inf. Process. Syst., vol. 21,
doi:10.5555/2981780.2981944 (2008).

[489] H. Schwarze and J. Hertz, Generalization in fully connected committee machines, EPL

21(7), 786 (1993), doi:10.1209/0295-5075/21/7/012.

[490] H. Schwarze, Learning a rule in a multilayer neural network, J. Phys. A: Math. Gen.

26(21), 5781 (1993), doi:10.1088/0305-4470/26/21/017.

[491] F. Gerace, B. Loureiro, F. Krzakala, M. Mézard and L. Zdeborová, Generalisation error in
learning with random features and the hidden manifold model, J. Stat. Mech. 2021(12),
124013 (2021), doi:10.1088/1742-5468/ac3ae6.

[492] S. D’Ascoli, M. Gabrié, L. Sagun and G. Biroli, More data or more parameters? Investigat-
ing the effect of data structure on generalization, In NeurIPS 2021 - Adv. Neural Process.
Syst. (2021), arXiv:2103.05524.

267

REFERENCES

[493] S. Goldt, M. S. Advani, A. M. Saxe, F. Krzakala and L. Zdeborová, Dynamics of stochastic
gradient descent for two-layer neural networks in the teacher-student setup, J. Stat. Mech.
Theor. Exp. 2020(12), 1 (2020), doi:10.1088/1742-5468/abc61e.

[494] M. Biehl and P. Riegler, On-line learning with a perceptron, EPL 28(7), 525 (1994),

doi:10.1209/0295-5075/28/7/012.

[495] M. Biehl and H. Schwarze, Learning by on-line gradient descent, J. Phys. A: Math. Gen.

28(3), 643 (1995), doi:10.1088/0305-4470/28/3/018.

[496] S. Goldt, B. Loureiro, G. Reeves, F. Krzakala, M. Mézard and L. Zdeborová, The Gaus-
sian equivalence of generative models for learning with shallow neural networks (2020),
arXiv:2006.14709.

[497] F. Mignacco, F. Krzakala, P. Urbani and L. Zdeborová, Dynamical mean-ﬁeld theory for
In NeurIPS 2020 - Conf.

stochastic gradient descent in Gaussian mixture classiﬁcation,
Neural. Inf. Process. Syst. (2020), arXiv:2006.06098.

[498] M. Lewenstein,

Quantum perceptrons,

J. Mod. Opt. 41(12), 2491 (1994),

doi:10.1080/09500349414552331.

[499] A. Gratsea, V. Kasper and M. Lewenstein, Storage properties of a quantum perceptron

(2021), arXiv:2111.08414.

[500] M. Lewenstein, A. Gratsea, A. Riera-Campeny, A. Aloy, V. Kasper and A. Sanpera, Stor-
age capacity and learning capability of quantum neural networks, doi:10.1088/2058-
9565/ac070f (2021).

[501] A. Gratsea and P. Huembeli, Exploring quantum perceptron and quantum neural network

structures with a teacher-student scheme (2021), arXiv:2105.01477.

[502] Y. Feng and Y. Tu, Phases of learning dynamics in artiﬁcial neural networks in the ab-
sence or presence of mislabeled data, Mach. Learn.: Sci. Technol. 2(4), 043001 (2021),
doi:10.1088/2632-2153/abf5b9.

[503] A. Decelle, C. Furtlehner and B. Seoane, Equilibrium and non-equilibrium regimes in the
learning of restricted Boltzmann machines, In NeurIPS 2021 - Adv. Neural Process. Syst.
(2021), arXiv:2103.05524.

[504] H.-S. Zhong, H. Wang, Y.-H. Deng, M.-C. Chen, L.-C. Peng, Y.-H. Luo, J. Qin, D. Wu,
X. Ding, Y. Hu et al., Quantum computational advantage using photons, Science
370(6523), 1460 (2020), doi:10.1126/science.abe8770.

[505] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends, R. Biswas, S. Boixo,
F. G. Brandao, D. A. Buell et al., Quantum supremacy using a programmable supercon-
ducting processor, Nature 574(7779), 505 (2019), doi:10.1038/s41586-019-1666-5.

[506] C. D. Bruzewicz, J. Chiaverini, R. McConnell and J. M. Sage, Trapped-ion quan-
tum computing: Progress and challenges, Appl. Phys. Rev. 6(2), 021314 (2019),
doi:10.1063/1.5088164, arXiv:https://doi.org/10.1063/1.5088164.

[507] M. Saffman,

Quantum computing with atomic qubits and Rydberg interactions:
J. Phys. B: At. Mol. Opt. Phys. 49(20), 202001 (2016),

progress and challenges,
doi:10.1088/0953-4075/49/20/202001.

268

REFERENCES

[508] L. Henriet, L. Beguin, A. Signoles, T. Lahaye, A. Browaeys, G.-O. Reymond and
C. Jurczak, Quantum computing with neutral atoms, Quantum 4, 327 (2020),
doi:10.22331/q-2020-09-21-327.

[509] Y. Liu, S. Arunachalam and K. Temme, A rigorous and robust quantum speed-up in
supervised machine learning, Nat. Phys. 17(9), 1013 (2021), doi:10.1038/s41567-021-
01287-z.

[510] J. Herrmann, S. Masot Llima, A. Remm, P. Zapletal, N. A. McMahon, C. Scarato,
F. Swiadek, C. Kraglund Andersen, C. Hellings, S. Krinner, N. Lacroix, S. Lazar et al., Re-
alizing quantum convolutional neural networks on a superconducting quantum processor
to recognize quantum phases (2021), arXiv:2109.05909.

[511] H.-Y. Huang, M. Broughton, J. Cotler, S. Chen, J. Li, M. Mohseni, H. Neven, R. Babbush,
R. Kueng, J. Preskill and J. R. McClean, Quantum advantage in learning from experiments
(2021), arXiv:2112.00778.

[512] M. Gong, H.-L. Huang, S. Wang, C. Guo, S. Li, Y. Wu, Q. Zhu, Y. Zhao, S. Guo, H. Qian,
Y. Ye, C. Zha et al., Quantum neuronal sensing of quantum many-body states on a 61-qubit
programmable superconducting processor (2022), arXiv:2201.05957.

[513] L. Hales and S. Hallgren, An improved quantum Fourier transform algorithm and ap-
plications, In FOCS 2000 - 41st Annu. IEEE Symp. Found. Comput. Sci., pp. 515–525,
doi:10.1109/SFCS.2000.892139 (2000).

[514] P. Shor,
ing,
doi:10.1109/SFCS.1994.365700 (1994).

Algorithms for quantum computation:

discrete logarithms and factor-
In FOCS 1994 - 35th Annu. IEEE Symp. Found. Comput. Sci., pp. 124–134,

[515] A. Y. Kitaev, Quantum measurements and the Abelian stabilizer problem (1995),

arXiv:quant-ph/9511026.

[516] A. W. Harrow, A. Hassidim and S. Lloyd, Quantum algorithm for linear systems of equa-

tions, Phys. Rev. Lett. 103, 150502 (2009), doi:10.1103/PhysRevLett.103.150502.

[517] P. Rebentrost, M. Mohseni and S. Lloyd,
for big data classiﬁcation,

chine
doi:10.1103/PhysRevLett.113.130503.

Quantum support vector ma-
Phys. Rev. Lett. 113, 130503 (2014),

[518] Z. Li, X. Liu, N. Xu and J. Du, Experimental realization of a quantum support vector
machine, Phys. Rev. Lett. 114, 140504 (2015), doi:10.1103/PhysRevLett.114.140504.

[519] N. Wiebe, D. Braun and S. Lloyd, Quantum algorithm for data ﬁtting, Phys. Rev. Lett.

109, 050505 (2012), doi:10.1103/PhysRevLett.109.050505.

[520] A. Gilyén, S. Lloyd and E. Tang, Quantum-inspired low-rank stochastic regression with

logarithmic dependence on the dimension (2018), arXiv:1811.04909.

[521] A. Ekert and R. Jozsa, Quantum computation and Shor’s factoring algorithm, Rev. Mod.

Phys. 68, 733 (1996), doi:10.1103/RevModPhys.68.733.

[522] C. M. Dawson and M. A. Nielsen, The Solovay-Kitaev algorithm (2005), arXiv:quant-

ph/0505030.

[523] J. Preskill, Quantum computing in the NISQ era and beyond, Quantum 2, 79 (2018),

doi:10.22331/q-2018-08-06-79.

269

REFERENCES

[524] Y. Y. Atas, J. Zhang, R. Lewis, A. Jahanpour, J. F. Haase and C. A. Muschik, SU(2)
hadrons on a quantum computer via a variational approach, Nat. Commun. 12(1)
(2021), doi:10.1038/s41467-021-26825-4.

[525] K. Temme, S. Bravyi and J. M. Gambetta, Error mitigation for short-depth quantum
circuits, Phys. Rev. Lett. 119, 180509 (2017), doi:10.1103/PhysRevLett.119.180509.

[526] S. Endo, S. C. Benjamin and Y. Li, Practical quantum error mitigation for near-future

applications, Phys. Rev. X 8(3) (2018), doi:10.1103/PhysRevX.8.031027.

[527] L. Funcke, T. Hartung, K. Jansen, S. Kühn, P. Stornati and X. Wang, Measure-
ment error mitigation in quantum computers through classical bit-ﬂip correction (2020),
arXiv:2007.03663.

[528] A. D. Córcoles, E. Magesan, S. J. Srinivasan, A. W. Cross, M. Steffen, J. M. Gambetta and
J. M. Chow, Demonstration of a quantum error detection code using a square lattice of four
superconducting qubits, Nat. Commun. 6(1), 6979 (2015), doi:10.1038/ncomms7979.

[529] V. Havlíˇcek, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow and
J. M. Gambetta, Supervised learning with quantum-enhanced feature spaces, Nature
567(7747), 209 (2019), doi:10.1038/s41586-019-0980-2.

[530] M. Schuld and N. Killoran, Quantum machine learning in feature Hilbert spaces, Phys.

Rev. Lett. 122, 040504 (2019), doi:10.1103/PhysRevLett.122.040504.

[531] C. M. Wilson, J. S. Otterbach, N. Tezak, R. S. Smith, A. M. Polloreno, P. J. Karalekas,
S. Heidel, M. S. Alam, G. E. Crooks and M. P. da Silva, Quantum kitchen sinks: An algo-
rithm for machine learning on near-term quantum computers (2018), arXiv:1806.08321.

[532] J. Dai and R. V. Krems, Quantum Gaussian process model of potential energy surface for a
polyatomic molecule, J. Chem. Phys. 156(18), 184802 (2022), doi:10.1063/5.0088821.

[533] Quantum feature maps and kernels, chapter of the Qiskit textbook “Introduction to

Quantum Computing”, Accessed: 2022-03-02.

[534] E. Tang, Quantum principal component analysis only achieves an exponential speedup
because of its state preparation assumptions, Phys. Rev. Lett. 127(6), 060503 (2021),
doi:10.1103/PhysRevLett.127.060503.

[535] J. M. Kübler, S. Buchholz and B. Schölkopf, The inductive bias of quantum kernels (2021),

arXiv:2106.03747.

[536] T. Haug, C. N. Self and M. S. Kim, Large-scale quantum machine learning (2021),

arXiv:2108.01039.

[537] J. Liu, F. Tacchino, J. R. Glick, L. Jiang and A. Mezzacapo, Representation learning via

quantum neural tangent kernels (2021), arXiv:2111.04225.

[538] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J. Love, A. Aspuru-Guzik
and J. L. O’Brien, A variational eigenvalue solver on a photonic quantum processor, Nat.
Comm. 5(1) (2014), doi:10.1038/ncomms5213.

[539] L. Funcke, T. Hartung, K. Jansen, S. Kühn and P. Stornati, Dimensional expressivity
analysis of parametric quantum circuits, Quantum 5, 422 (2021), doi:10.22331/q-
2021-03-29-422.

270

REFERENCES

[540] J. Li, X. Yang, X. Peng and C.-P. Sun,
proach to quantum optimal control,
doi:10.1103/PhysRevLett.118.150503.

Hybrid quantum-classical ap-
Phys. Rev. Lett. 118, 150503 (2017),

[541] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster and J. I. Latorre, Data re-uploading for
a universal quantum classiﬁer, Quantum 4, 226 (2020), doi:10.22331/q-2020-02-06-
226.

[542] I. Cong, S. Choi and M. D. Lukin, Quantum convolutional neural networks, Nat. Phys.

15(12), 1273–1278 (2019), doi:10.1038/s41567-019-0648-8.

[543] M. Schuld, Supervised quantum machine learning models are kernel methods (2021),

arXiv:2101.11020.

[544] S. Jerbi, L. J. Fiderer, H. P. Nautrup, J. M. Kübler, H. J. Briegel and V. Dunjko, Quantum

machine learning beyond kernel methods (2021), arXiv:2110.13162.

[545] J. Bausch, Recurrent quantum neural networks (2020), arXiv:2006.14619.

[546] S. Jerbi, C. Gyurik, S. C. Marshall, H. J. Briegel and V. Dunjko, Parametrized quantum

policies for reinforcement learning (2021), arXiv:2103.05577.

[547] A. Skolik, S. Jerbi and V. Dunjko, Quantum agents in the Gym: a variational quantum

algorithm for deep Q-learning (2021), arXiv:2103.15084.

[548] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang and

W. Zaremba, OpenAI Gym (2016), arXiv:1606.01540.

[549] J. Romero, J. P. Olson and A. Aspuru-Guzik, Quantum autoencoders for efﬁcient compres-
sion of quantum data, Quantum Sci. Technol. 2(4), 045001 (2017), doi:10.1088/2058-
9565/aa8072.

[550] D. Bondarenko and P. Feldmann, Quantum autoencoders to denoise quantum data, Phys.

Rev. Lett. 124(13) (2020), doi:10.1103/PhysRevLett.124.130502.

[551] D. F. Locher, L. Cardarelli and M. Müller, Quantum error correction with quantum au-

toencoders (2022), arXiv:2202.00555.

[552] G. Verdon, J. Marks, S. Nanda, S. Leichenauer and J. Hidary,

Quantum
Hamiltonian-based models and the variational quantum thermalizer algorithm (2019),
arXiv:1910.02071.

[553] S. Lloyd and C. Weedbrook, Quantum generative adversarial learning, Phys. Rev. Lett.

121, 040502 (2018), doi:10.1103/PhysRevLett.121.040502.

[554] P.-L. Dallaire-Demers and N. Killoran, Quantum generative adversarial networks, Phys.

Rev. A 98, 012324 (2018), doi:10.1103/PhysRevA.98.012324.

[555] J.-G. Liu and L. Wang, Differentiable learning of quantum circuit Born machines, Phys.

Rev. A 98, 062324 (2018), doi:10.1103/PhysRevA.98.062324.

[556] B. Coyle, D. Mills, V. Danos and E. Kasheﬁ, The Born supremacy: quantum advantage and
training of an Ising Born machine, npj Quantum Inf. 6(1) (2020), doi:10.1038/s41534-
020-00288-9.

271

REFERENCES

[557] M. Benedetti, D. Garcia-Pintos, O. Perdomo, V. Leyton-Ortega, Y. Nam and A. Perdomo-
Ortiz, A generative modeling approach for benchmarking and training shallow quantum
circuits, npj Quantum Inf. 5(1) (2019), doi:10.1038/s41534-019-0157-8.

[558] L. Viola and S. Lloyd, Dynamical suppression of decoherence in two-state quantum sys-

tems, Phys. Rev. A 58, 2733 (1998), doi:10.1103/PhysRevA.58.2733.

[559] F. Kleißler, A. Lazariev and S. Arroyo-Camejo, Universal, high-ﬁdelity quantum gates
based on superadiabatic, geometric phases on a solid-state spin-qubit at room temperature,
npj Quantum Inf. 4(1) (2018), doi:10.1038/s41534-018-0098-7.

[560] M. Taherkhani, M. Willatzen, E. V. Denning, I. E. Protsenko and N. Gregersen, High-
ﬁdelity optical quantum gates based on type-II double quantum dots in a nanowire, Phys.
Rev. B 99, 165305 (2019), doi:10.1103/PhysRevB.99.165305.

[561] E. Zahedinejad,
foli gate
doi:10.1103/PhysRevLett.114.200502.

via quantum control,

J. Ghosh and B. C. Sanders,

tof-
Phys. Rev. Lett. 114, 200502 (2015),

High-ﬁdelity single-shot

[562] D. Yu, H. Wang, D. Ma, X. Zhao and J. Qian, Adiabatic and high-ﬁdelity quantum
gates with hybrid Rydberg-Rydberg interactions, Opt. Express 27(16), 23080 (2019),
doi:10.1364/OE.27.023080.

[563] F. Haddadfarshi and F. Mintert, High ﬁdelity quantum gates of trapped ions in the pres-
ence of motional heating, New J. Phys. 18(12), 123007 (2016), doi:10.1088/1367-
2630/18/12/123007.

[564] S. Li, J. Xue, T. Chen and Z. Xue, High-ﬁdelity geometric quantum gates with short
paths on superconducting circuits, Adv. Quantum Technol. 4(5), 2000140 (2021),
doi:10.1002/qute.202000140.

[565] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush and H. Neven,

Barren
plateaus in quantum neural network training landscapes, Nat. Commun. 9(1), 1 (2018),
doi:10.1038/s41467-018-07090-4.

[566] M. Cerezo, A. Sone, T. Volkoff, L. Cincio and P. J. Coles, Cost function dependent barren
plateaus in shallow parametrized quantum circuits, Nat. Commun. 12(1), 1 (2021),
doi:10.1038/s41467-021-21728-w.

[567] L. Bittel and M. Kliesch, Training variational quantum algorithms is NP-hard, Phys. Rev.

Lett. 127, 120502 (2021), doi:10.1103/PhysRevLett.127.120502.

[568] S. Sim, P. D. Johnson and A. Aspuru-Guzik, Expressibility and entangling capability of
parameterized quantum circuits for hybrid quantum-classical algorithms, Adv. Quantum
Technol. 2(12), 1900070 (2019), doi:10.1002/qute.201900070.

[569] A. Y. Kitaev, Quantum computations: algorithms and error correction, Russ. Math. Surv.

52(6), 1191 (1997), doi:10.1070/rm1997v052n06abeh002155.

[570] M. S. Rudolph, S. Sim, A. Raza, M. Stechly, J. R. McClean, E. R. Anschuetz, L. Serrano
and A. Perdomo-Ortiz, ORQVIZ: Visualizing high-dimensional landscapes in variational
quantum algorithms (2021), arXiv:2111.04695.

[571] J. Eisert, Entangling power and quantum circuit complexity, Phys. Rev. Lett. 127, 020501

(2021), doi:10.1103/PhysRevLett.127.020501.

272

REFERENCES

[572] H.-Y. Huang, R. Kueng and J. Preskill,

tum advantage in machine learning,
doi:10.1103/PhysRevLett.126.190505.

Information-theoretic bounds on quan-
Phys. Rev. Lett. 126, 190505 (2021),

[573] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand, M. Degroote,
H. Heimonen, J. S. Kottmann, T. Menke et al., Noisy intermediate-scale quantum algo-
rithms, Rev. Mod. Phys. 94(1), 015004 (2022), doi:10.1103/RevModPhys.94.015004.

[574] G. Munoz-Gil, G. Volpe, M. A. Garcia-March, E. Aghion, A. Argun, C. B. Hong, T. Bland,
S. Bo, J. A. Conejero, N. Firbas, O. Garibo i Orts, A. Gentili et al., Objective compar-
ison of methods to decode anomalous diffusion, Nat. Commun. 12(1), 6253 (2021),
doi:10.1038/s41467-021-26320-w.

[575] Z. Liu and M. Tegmark, Machine learning conservation laws from trajectories, Phys. Rev.

Lett. 126, 180604 (2021), doi:10.1103/PhysRevLett.126.180604.

[576] G. Muñoz-Gil, M. A. Garcia-March, C. Manzo, J. D. Martín-Guerrero and M. Lewenstein,
Single trajectory characterization via machine learning, New J. Phys. 22(1), 013010
(2020), doi:10.1088/1367-2630/ab6065.

[577] G. Munoz-Gil, C. Romero-Aristizabal, N. Mateos, F. Campelo, L. I. de LLobet-Cucalon,
M. Beato, M. Lewenstein, M. Garcia-Parajo and J. A. Torreno-Pina, Particle ﬂow modu-
lates growth dynamics and nanoscale-arrested growth of transcription factor condensates
in living cells, bioRxiv (2022), doi:10.1101/2022.01.11.475940.

[578] H. B. Moss and R.-R. Grifﬁths, Gaussian process molecule property prediction with

FlowMO (2020), arXiv:2010.01118.

[579] A. Glielmo, Y. Rath, G. Csányi, A. De Vita and G. H. Booth, Gaussian process states:
A data-driven representation of quantum many-body physics, Phys. Rev. X 10, 041026
(2020), doi:10.1103/PhysRevX.10.041026.

[580] K. Choo, T. Neupert and G. Carleo,

Two-dimensional

studied with neural network quantum states,
doi:10.1103/PhysRevB.100.125124.

− J2 model
Phys. Rev. B 100(12) (2019),

frustrated J1

[581] M. Secor, A. V. Soudackov and S. Hammes-Schiffer, Artiﬁcial neural networks as
J. Phys. Chem. Lett. 12(43), 10654 (2021),

propagators in quantum dynamics,
doi:10.1021/acs.jpclett.1c03117.

[582] V. Havlicek,

Amplitude ratios and neural network quantum states (2022),

arXiv:2201.09128.

[583] J. Yao, L. Lin and M. Bukov,

preparation inspired by counterdiabatic driving,
doi:10.1103/PhysRevX.11.031070.

Reinforcement learning for many-body ground-state
Phys. Rev. X 11, 031070 (2021),

[584] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel and N. Friis, Optimizing quan-
tum error correction codes with reinforcement learning, Quantum 3, 215 (2019),
doi:10.22331/q-2019-12-16-215.

[585] P. Peng, X. Huang, C. Yin, C. Ramanathan and P. Cappellaro, Deep reinforcement learning
In APS March Meeting Abstracts, vol. 2021, p.

for quantum Hamiltonian engineering,
P33.001 (2021).

273

REFERENCES

[586] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. Žídek,
A. W. R. Nelson, A. Bridgland, H. Penedones, S. Petersen et al., Improved protein struc-
ture prediction using potentials from deep learning, Nature 577(7792), 706 (2020),
doi:10.1038/s41586-019-1923-7.

[587] A. Davies, P. Veliˇckovi´c, L. Buesing, S. Blackwell, D. Zheng, N. Tomašev, R. Tan-
burn, P. Battaglia, C. Blundell, A. Juhász, M. Lackenby, G. Williamson et al., Advanc-
ing mathematics by guiding human intuition with AI, Nature 600(7887), 70 (2021),
doi:10.1038/s41586-021-04086-x.

[588] A. Pozas-Kerstjens, N. Gisin and M.-O. Renou, Proofs of network quantum nonlocality

aided by machine learning (2022), arXiv:2203.16543.

[589] A. Pozas-Kerstjens, G. Muñoz-Gil, E. Piñol, M. Á. García-March, A. Acín, M. Lewenstein
and P. R. Grzybowski, Efﬁcient training of energy-based models via spin-glass control,
Mach. Learn.: Sci. Technol. 2(2), 025026 (2021), doi:10.1088/2632-2153/abe807.

[590] L. G. Wright, T. Onodera, M. M. Stein, T. Wang, D. T. Schachter, Z. Hu and P. L. McMahon,
Deep physical neural networks trained with backpropagation, Nature 601(7894), 549
(2022), doi:10.1038/s41586-021-04223-6.

[591] K. Wagner and D. Psaltis, Optical neural networks: an introduction by the feature editors,

Appl. Opt. 32(8), 1261 (1993), doi:10.1364/AO.32.001261.

[592] Y. Zuo, B. Li, Y. Zhao, Y. Jiang, Y.-C. Chen, P. Chen, G.-B. Jo, J. Liu and S. Du, All-
optical neural network with nonlinear activation functions, Optica 6(9), 1132 (2019),
doi:10.1364/OPTICA.6.001132.

[593] X. Sui, Q. Wu, J. Liu, Q. Chen and G. Gu, A review of optical neural networks,

IEEE

Access 8, 70773 (2020), doi:10.1109/ACCESS.2020.2987333.

[594] H. Zhang and e. a. Gu, An optical neural chip for implementing complex-valued neural
network, Nat. Commun. 12(1), 457 (2021), doi:10.1038/s41467-020-20719-7.

[595] H. e. a. Zhang, Efﬁcient on-chip training of optical neural networks using genetic algo-
rithm, ACS Photonics 8(6), 1662 (2021), doi:10.1021/acsphotonics.1c00035.

[596] X. Xu, M. Tan, B. Corcoran, J. Wu, A. Boes, T. G. Nguyen, S. T. Chu, B. E. Little, D. G.
Hicks, R. Morandotti, A. Mitchell and D. J. Moss, 11 TOPS photonic convolutional accel-
erator for optical neural networks, Nature 589(7840), 44 (2021), doi:10.1038/s41586-
020-03063-0.

[597] J. Liu, Q. Wu, X. Sui, Q. Chen, G. Gu, L. Wang and S. Li, Research progress in opti-
cal neural networks: theory, applications and developments, PhotoniX 2(1), 5 (2021),
doi:10.1186/s43074-021-00026-0.

[598] T. Wang, S.-Y. Ma, L. G. Wright, T. Onodera, B. C. Richard and P. L. McMahon, An optical
neural network using less than 1 photon per multiplication, Nat. Commun. 13(1), 123
(2022), doi:10.1038/s41467-021-27774-8.

[599] H. Xu, S. Ghosh, M. Matuszewski and T. C. Liew, Universal self-correcting computing
with disordered exciton-polariton neural networks, Phys. Rev. Appl. 13, 064074 (2020),
doi:10.1103/PhysRevApplied.13.064074.

274

REFERENCES

[600] D. Ballarini, A. Gianfrate, R. Panico, A. Opala, S. Ghosh, L. Dominici, V. Ardizzone,
M. De Giorgi, G. Lerario, G. Gigli, T. C. H. Liew, M. Matuszewski et al., Polaritonic
neuromorphic computing outperforms linear classiﬁers, Nano Lett. 20(5), 3506 (2020),
doi:10.1021/acs.nanolett.0c00435.

[601] M. Matuszewski, A. Opala, R. Mirek, M. Furman, M. Król, K. Tyszka, T. Liew, D. Bal-
Energy-efﬁcient neural network in-
Phys. Rev. Appl. 16, 024045 (2021),

larini, D. Sanvitto, J. Szczytko and B. Pi˛etka,
ference with microcavity exciton polaritons,
doi:10.1103/PhysRevApplied.16.024045.

[602] R. Mirek, A. Opala, P. Comaron, M. Furman, M. Król, K. Tyszka, B. Seredy´nski, D. Ballar-
ini, D. Sanvitto, T. C. H. Liew, W. Pacuski, J. Suffczy´nski et al., Neuromorphic binarized
polariton networks, Nano Lett. 21(9), 3715 (2021), doi:10.1021/acs.nanolett.0c04696.

[603] D. Zvyagintseva, H. Sigurdsson, V. K. Kozin, I. Iorsh, I. A. Shelykh, V. Ulyantsev and
O. Kyriienko, Machine learning of phase transitions in nonlinear polariton lattices, Com-
mun. Phys. 5(1), 8 (2022), doi:10.1038/s42005-021-00755-5.

[604] J. J. Hopﬁeld,

tive computational abilities,
doi:10.1073/pnas.79.8.2554.

Neural networks and physical

systems with emergent collec-
Proc. Natl. Acad. Sci. U.S.A. 79(8), 2554 (1982),

[605] P. Rotondo, M. Marcuzzi, J. P. Garrahan, I. Lesanovsky and M. Müller, Open quantum
generalisation of Hopﬁeld neural networks, J. Phys. A: Math. Theor. 51(11), 115301
(2018), doi:10.1088/1751-8121/aaabcb.

[606] K. B. Petersen and M. S. Pedersen, The matrix cookbook, Mathematics - Waterloo

University, Accessed: 2022-03-04 (2012).

275

List of Figures

LIST OF FIGURES

1.1 Traditional programming vs. machine learning based on data-driven program-

ming

1.2 Artiﬁcial intelligence vs. machine learning vs. deep learning
1.3 Machine learning in science is booming
1.4 Interplay of artiﬁcial intelligence, quantum computing, and physics
1.5 Content of these Lecture Notes
1.6 Tree of dependencies between chapters
2.1 Examples of loss functions
2.2 Learning rate as a hyperparameter
2.3 Under- and overﬁtting
2.4 The bias-variance trade-off
2.5 Geometric construction of a support vector machine in a two-dimensional prob-

8
9
15
16
17
18
21
22
26
28

lem

35
37
2.6 Neural network
39
2.7 Convolutional ﬁlter
40
2.8 Autoencoder
41
2.9 Recurrent neural networks
46
2.10 Backpropagation on a simple feedforward NN
49
3.1 Ising model
50
3.2 Ising gauge theory
53
3.3 Phase classiﬁcation using principal component analysis
57
3.4 Phase classiﬁcation with supervised learning
58
3.5 Unsupervised learning with autoencoders
60
3.6 Phase classiﬁcation with learning by confusion
61
3.7 Phase classiﬁcation with the prediction-based method
65
3.8 Interpretation of an autoencoder for a two-dimensional Ising model
66
3.9 Interpretation of neural networks by bottlenecks
68
3.10 Hessian-based interpretability
73
4.1 Toy example of a two-dimensional data set in the input and feature space
4.2 Classiﬁcation using support vector machines with different kernels
81
4.3 A linear support vector machine applied to a data set that is not linearly separable 82
84
4.4 Bayesian neural network
90
4.5 Bayesian optimization
91
4.6 Acquisition function example
94
4.7 Algorithm for optimal kernel construction
4.8 Three main classes of quantum problems successfully tackled with Bayesian

optimization and Gaussian processes

4.9 Gaussian processes and Bayesian optimization for feedback loops.
4.10 Gaussian process as a potential energy surface for quantum calculations.
4.11 Gaussian process for extrapolation to non-seen quantum phases
5.1 Scheme of a restricted Boltzmann machine
5.2 Autoregressive neural quantum state
5.3 Recurrent neural-network architecture as a neural quantum state
5.4 Expressive capacity of neural quantum states
5.5 Schematic representation of the of various ansätze
5.6 Dynamics of the neural quantum state
5.7 Ground state results with a Restricted Boltzmann Machine ansatz
5.8 Electronic structure calculations with neural quantum states

95
96
97
99
108
109
110
111
112
118
119
121

276

LIST OF FIGURES

5.9 Quantum Approximate Optimization Algorithm with neural quantum states
5.10 Open quantum systems with neural quantum states
5.11 Quantum tomography with a recurrent neural network
6.1 Overview of the basic reinforcement learning setting
6.2 Short-term and long-terms rewards in reinforcement learning algorithms
6.3 Schematic representation of the episodic and compositional memory of various

projective simulation agents
6.4 Walkers and reinforcement learning
6.5 Performance of AlphaGo and AlphaGo Zero
6.6 Reinforcement learning for quantum feedback of an optical cavity
6.7 Reinforcement learning for circuit optimization
6.8 Reinforcement learning for quantum error correction
6.9 Increased qubit lifetime due to reinforcement learning
6.10 Reinforcement learning to ﬁnd optimal relaxations
7.1 Machine learning inﬂuences physics
7.2 Standard vs. differentiable programming
7.3 Exemplary computation graph
7.4 Inverse Schrödinger problem solved using differential programming
7.5 Sketch of a normalizing ﬂow
7.6 Transport of probability mass through a normalizing ﬂow
7.7 Experimental nanoﬂake setup
7.8 Nanoﬂake automation scheme
7.9 Differences in human ﬂake judgement
7.10 Experimental setup for quantum dots
7.11 Quantum dots machine learning scheme
7.12 Machine learning observable extraction for ultracold atoms
7.13 Simulated and experimental p-space density
7.14 Illustration of the Hamiltonian learning of a one-spin system
7.15 Hamiltonian learning for larger systems
7.16 Scaling scheme from four lattice sites to 50 for a speciﬁc Hamiltonian
7.17 Algorithm for computer-inspired experiments
8.1 Physics inﬂuences machine learning
8.2 Statistical physics toolbox for understanding machine learning theory
8.3 Generalization error in classical and modern regimes
8.4 Capacity of the perceptron
8.5 Generalization error vs. the task difﬁculty in the teacher-student paradigm
8.6 Schemes of a committee machine and random feature model
8.7 Generalization error and specialization in committee machines
8.8 Generalization errors vs. model parametrization for mismatched teacher and

student models

8.9 Data structure entering the teacher-student scheme
8.10 Dynamics of learning in overparametrized committee machines
8.11 Illustration of a quantum circuit diagram
8.12 Quantum machine learning
8.13 Realization of the famous Shor algorithm in a real quantum computer
8.14 Quantum support vector machine enhanced by a quantum device
8.15 Variational optimization of quantum circuits
8.16 Variational quantum circuit

123
124
127
132
133

150
153
155
156
157
158
161
162
165
167
169
173
179
181
184
185
186
187
188
190
191
192
193
194
195
198
198
199
203
206
207
208

209
211
212
215
216
218
219
221
222

277

LIST OF ALGORITHMS

List of Algorithms

Minibatch stochastic gradient descent (SGD) . . . . . . . . . . . . . . . . . . . . . . 24
1
Principal component analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2
t-distributed stochastic neighbour embedding (t-SNE)
. . . . . . . . . . . . . . . . 55
3
Learning by confusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4
Bayesian Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5
Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6
Ground state search with NQS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
7
Real (ξ = i) or imaginary (ξ = 1) time evolution algorithm for NQS . . . . . . . . 119
8
Q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9
10 REINFORCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
11 Actor-critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

278

Nomenclature

Numbers and arrays

A

A

a

A

a

matrix

tensor

vector

random variable

scalar

Physical constants and quantities

β

δ

1/kBT

Kronecker delta

NOMENCLATURE

D

n

η

φ

m

ˆf

L

H

data set

size of D, i.e., number of training ex-
amples

learning rate

feature map

dimensionality of data point x, i.e.,
number of data features

model with converged θ

loss (or cost/error) function

Hessian matrix

〈x〉

p or E[x | p] Estimator of quantity x with

d

respect to distribution p

size of θ, i.e., number of model param-
eters

ˆσ

ˆH

H

σ

H

kB

m

Z

Pauli matrix

quantum Hamiltonian

Hilbert space

spin variable

classical Hamiltonian

Boltzmann constant

magnetization

partition function

Machine learning quantities

b

model biases

Ki or K i-th class or number of classes in a

classiﬁcation problem

θ

θ∗

π

π∗

(cid:96)

n

ς

model parameters

converged θ

policy

optimal policy

L(n) regularization

activation function

w or W vector or matrix of model weights

a

DKL

G

r

s

action

Kullback-Leibler divergence

return

reward

state

279

LIST OF ACRONYMS

List of acronyms

AD automatic differentiation

MAE mean absolute error

AE autoencoder

MAP maximum a posteriori estimator

AI artiﬁcial intelligence

MCMC Markov chain Monte Carlo

ANN artiﬁcial neural network

MDP Markov decision process

AR autoregressive

ML machine learning

ARNN autoregressive neural network

MLE maximum likelihood estimation

BIC Bayesian information criterion

BO Bayesian optimization

CPU central processing unit

CE cross-entropy

CNN convolutional neural network

∂ P differentiable programming

DL deep learning

DNN deep neural network

DQN deep Q-network

ECM episodic and compositional memory

GAMP generalized
passing

approximate message

MPS matrix product state

MSE mean-squared error

NF normalizing ﬂow

NISQ noisy intermediate-scale quantum

NN neural network

NQS neural quantum state

ODE ordinary differential equation

PC principal component

PCA principal component analysis

PES potential energy surface

POVM positive operator-valued measure

PPT positive under partial transposition

GAN generative adversarial network

PQC parametrized quantum circuit

GNS generative neural sampler

PS projective simulation

GP Gaussian process

GPR Gaussian process regression

GPU graphics processing unit

IGT Ising gauge theory

KRR kernel ridge regression

KL Kullback-Leibler

L-BFGS limited-memory

Broyden–Fletch-

er–Goldfarb–Shanno algorithm

LASSO least absolute shrinkage and selection

QAOA quantum approximate optimization

algorithms

QD quantum dot

QML quantum machine learning

RBM restricted Boltzmann machine

RKHS reproducing kernel Hilbert space

RL reinforcement learning

RNN recurrent neural network

RUE resampling uncertainty estimation

operator

SGD stochastic gradient descent

LE local ensemble

SE state evolution

280

LIST OF ACRONYMS

SVM support vector machine

TN tensor network

t-SNE t-distributed stochastic neighbour em-

bedding

TNS tensor network state

t-VMC time-dependent variational Monte-

Carlo

VAE variational autoencoder

TD temporal-difference

VQE variational quantum eigensolver

281

A

discount factor

Index

action

action preference

activation function
active learning
actor-critic
agent
anomaly detection
ansatz

130
146
37
14, 69, 89
148
130
58, 59, 69
101–103, 111, 112
103
102
39, 40, 57, 64
64
224
186
40, 110, 178
40, 108, 109

computationally tractable states
mean-ﬁeld

autoencoder

interpretability
quantum autoencoder

automation procedure
autoregressive models
autoregressive neural network

B

backpropagation
baseline
batch optimization
Bayes optimal error
Bayesian information criterion
Bayesian optimization

acquisition function

Bayesian posterior
Bellman equations

optimal Bellman equations

9, 10, 23, 38, 42, 171
144
185
29, 205
92, 93
88, 93, 94
89
204
137
138
26, 92, 199
39, 40, 57, 64

bias-variance trade-off
bottleneck

C

capacity
classiﬁcation
clustering
committee machine
convolutional neural network

interpretability

critical task difﬁculty
cross-validation

D

data augmentation
deep Q-network
density estimation

non parametric
parametric

differentiable programming
differentiation

automatic differentiation
manual differentiation
numerical differentiation
symbolic differentiation

dimensionality reduction

25, 199, 213
12, 184
14, 51, 52
11, 206–208, 212
38, 185
65
200
23

185
141
175
175
175
165
23, 168
99, 168, 169
168
168
168
51

282

INDEX

133

130
134
22
141
133
133

E

environment

environment dynamics

epoch
experience replay
exploitation
exploration

feature
feature space

12–14, 30, 31, 71
71

F

G

generalization error

Gaussian process
generalization

83, 88, 93, 94
24
25, 26
generalized approximate message passing 205
13, 14, 40, 110, 174
generative models
174
177
21
23
10, 166, 227, 229

generative neural samplers
gradient descent

stochastic gradient descent

graphics processing unit

deep generative models

H

Hamiltonian learning
Hamiltonian-driven approach
Hessian
hyperparameter

191
176
23, 68, 167
22

I

inﬂuence functions
interpretability

68
62–65, 67, 70, 151, 196

K

104
k-local operator
71
kernel
79
kernel ridge regression
93, 99
kernel search
kernel trick
71, 73, 78
Kullback-Leibler divergence 29, 125, 164, 176

learning by confusion
learning rate
linear regression
logistic regression
loss function
loss landscape

L

M

59
22
31, 86
34
20, 92
23, 68

Markov chain Monte Carlo
Markov decision process

105, 108, 177
131, 134

INDEX

reliability

regularization
reinforcement learning

local ensembles
resampling uncertainty estimation
underspeciﬁcation

25, 64
14, 130
model-based reinforcement learning 135
135
model-free reinforcement learning
63
69
69
69
202
51
77, 87
75, 77
107
133
133, 135
131, 132

replica trick
representation space
representer theorem
reproducing kernel Hilbert space
restricted Boltzmann machine
return

discounted return

reward

S

score function
self-averaging
semi-supervised learning
Siamese neural network
softmax
state (RL)
stochastic neighbor embedding (SNE)

t-distributed SNE

supervised learning
support vector machine

kernel support vector machine
quantum support vector machine

143
201
14
66, 67
34
131
54
54
14, 192
35
80
216

141, 142
199, 204, 213
139
139
103
103
111
97

69, 88, 94
25
14, 51, 52, 54

T

target network
teacher-student paradigm
temporal-difference learning

temporal-difference error

tensor networks

matrix product states
projected entangled pair states

transfer learning

uncertainty
underﬁtting
unsupervised learning

U

V

value function

action-value function
advantage
state-value function

136
137
137
137
220
101–107, 114, 121, 129

variational quantum eigensolver
variational state

action space
state space
trajectory

Markov property
maximum likelihood
mean squared error
minimal task difﬁculty
mode collapse

131, 134
131, 134
135
134
92, 176
26
203
183

N

neural network
neural quantum states
no free lunch theorem
noisy intermediate-scale quantum era
normalizing ﬂow

37
107
25
217
178

supervised phase classiﬁcation
unsupervised phase classiﬁcation

one hot encoding
online learning
overﬁtting
overparametrization

O

P

perceptron

perceptron capacity
quantum perceptron

perplexity
phase classiﬁcation

policy

(cid:34)-greedy policy
optimal policy

policy gradient

policy gradient theorem
REINFORCE

prediction-based method
principal component analysis

Q

Q-learning

deep Q-learning
double Q-learning

QAOA
quantum machine learning
quantum many-body problem
quantum tomography

shadow tomography

56
212
25, 33, 92
208

37, 199, 212
200
213
54
48, 98
56
51
131, 135
134
131
142
142, 143
143
60
52

139
141
140
122
214
101, 120
125, 175
127

R

random feature model
recurrent neural network
regression

206, 207
41, 110, 178
12, 26, 193

283

