0
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

2
v
3
4
0
3
0
.
2
0
0
2
:
v
i
X
r
a

Semantic Robustness of Models of Source Code

Goutham Ramakrishnan∗

Jordan Henkel∗

Zi Wang

Aws Albarghouthi

Somesh Jha

Thomas Reps

University of Wisconsin–Madison
{gouthamr,jjhenkel,zw,aws,jha,reps}@cs.wisc.edu

Abstract

Deep neural networks are vulnerable to adversarial examples—small input pertur-
bations that result in incorrect predictions. We study this problem for models of
source code, where we want the network to be robust to source-code modiﬁcations
that preserve code functionality. (1) We deﬁne a powerful adversary that can
employ sequences of parametric, semantics-preserving program transformations;
(2) we show how to perform adversarial training to learn models robust to such
adversaries; (3) we conduct an evaluation on different languages and architectures,
demonstrating signiﬁcant quantitative gains in robustness.

1

Introduction

While deep neural networks have been widely adopted in many areas of computing, it has been
repeatedly shown that they are vulnerable to adversarial examples [33, 10, 17, 27]: small, seemingly
innocuous perturbations to the input that lead to incorrect predictions. Adversarial examples raise
safety and security concerns, for example, in computer-vision models used in autonomous vehi-
cles [14, 8] or for user authentication [31]. Signiﬁcant progress has recently been made in identifying
adversarial examples and training models that are robust to such examples. However, the majority of
the research has targeted computer-vision tasks [11, 25, 33], a continuous domain.

(cid:50)

int

(Object target)

System.out.println("Begin search");

int i = 0;
for (Object elem: this.elements)
if (elem.equals(target))

In this paper, we study the problem of robustness to adversarial
examples in the discrete domain of deep neural networks for
source code. With the growing adoption of neural models
for programming tasks, robustness is becoming an important
property. Why do we want robust models of code? There are
many answers, ranging from usability to security. Consider,
for instance, a model that explains in English what a piece of
code is doing—the code-captioning task. A developer using
such a model to navigate a new code base should not receive
completely different explanations for similar pieces of code.
For a concrete example, consider the behavior of the state-of-
the-art code2seq model [3] on the Java code in Fig. 1, where
the prediction changes after logging print statements are added.
Alternatively, imagine the security-critical setting of malware classiﬁcation. We do not want a small
modiﬁcation to the malware’s binary to cause the model to deem it safe.

Figure 1: code2seq [3] correctly predicts
function name: indexOfTarget. After
the highlighted logging statements are
added, it predicts search.

System.out.println("Found");

return -1;

return i;

i++;

With images, the threat model involves small changes that are imperceptible to a human. With code,
there is no analogous notion of a change imperceptible to a human. Consequently, we consider
attacks based on semantics-preserving transformations. Because the original program’s semantics is
preserved, the program that results from the attack must have the same behavior as the original.

∗Equal Contribution

Preprint. Under review.

 
 
 
 
 
 
Most-related work. We are not the ﬁrst to study this problem. Wang and Christodorescu [35] demon-
strated the drop in accuracy of deep models over source code when applying standard transformations
and refactorings; however, they did not propose defenses in their work. Recently, Yefet et al. [37]
developed a gradient-based attack that is speciﬁc to variable-name substitution or dead-code insertion,
similar in spirit to attacks on natural-language models [13], which can efﬁciently estimate gradients
for token substitution and insertion. Yefet et al. [37] propose a defense based on outlier detection,
but do not consider arbitrary program transformations or adversarial training. Zhang et al. [39] use
the Metropolis-Hastings algorithm to perform adversarial identiﬁer renaming. However, they do
not consider other program transformations, and their defense is akin to dataset augmentation with
adversarial examples, instead of a robust optimization [25]. Parallel work [9] is discussed in Sec. 2.

Our aim in this paper is to consider the general setting of an adversary that can apply a sequence of
source-code transformations. Speciﬁcally, our goal is to answer the following question:

Can we train models that are robust to sequences of semantics-preserving transformations?

k-adversaries & k-robustness. We begin by deﬁning the notion of a k-adversary, one that can select
a sequence of k transformations from a prespeciﬁed set of semantics-preserving transformations T ,
and apply them to an input program. The adversary succeeds if it manages to change the prediction
of the neural network. For example, a transformation may add dead code to a program, replace for
loops with while loops, change variable names, replace one API call with an equivalent one, etc.

The primary challenge in implementing a k-adversary is the combinatorial search space of possible
sequences of transformations. Further, some transformations are parametric, e.g., insert a print
statement with string s, which blows up the search space even further. To implement a k-adversary in
practice, we exploit the insight that we can break up the search into two pieces: (1) enumerative search
through transformation sequences, and (2) gradient-based optimization to discover transformation
parameters. Speciﬁcally, given a sequence of transformations, we partially apply them to a given
program, without supplying parameters, resulting in a program sketch [32]: a program with unknown
holes (tokens or AST leaves). Then, gradient-based optimization, like that of Yefet et al. [37] or
Ebrahimi et al. [13], can be used to discover a worst-case instantiation of the holes.

Adversarially training k-robust models. To train a k-robust model—one that is robust to k-
adversaries—we adapt the robust-optimization objective of Madry et al. [25] to our setting: Instead
of computing the loss for a program x, we compute the worst-case loss resulting from a k-adversary.
While this approach is theoretically sound, it is hopelessly inefﬁcient for two reasons.

First, a k-adversary is an expensive operation within the training loop, because it involves enumerating
transformation sequences. Thus, even for small values of k, the search space can make training
impractically slow. We demonstrate experimentally that training on a small value, k = 1, results
in models that are robust to larger values of k. This phenomenon allows us to efﬁciently train
adversarially without incurring a combinatorial explosion in the number of transformation sequences.

Second, program transformations are typically deﬁned as tree transformations over abstract syntax
trees (ASTs). However, the neural network usually receives as input some other program format,
some of which lose program structure or entire portions of the program—for example, tokens or
subtokens of program text, or randomly sampled paths through the AST [4, 3]. Therefore, modeling an
in-training adversary is very time consuming, because it requires invoking external, non-differentiable
program-analysis tools, and converting back and forth between ASTs and training formats.

To work around training complexity, we generate sequences of transformations ofﬂine and partially
apply them to programs to generate program sketches. During training, we need only consider the
generated program sketches, performing gradient-based attacks to search for optimal parameters of
the transformations. This approach allows us to avoid applying AST transformations during training.

Evaluation. We have developed an extensible framework for writing transformations and performing
adversarial training [15]. We evaluate our approach on radically different programming languages—
Java and Python—and architectures—using tokenized and AST program representations. Our results
show that (1) adversarial training improves robustness and (2) training with a 1-adversary results in a
model that is robust on larger values of k, enabling efﬁcient training.

Contributions. We summarize our contributions below:

2

• We deﬁne k-transformation robustness for source-code tasks: robustness to an adversary that
is allowed k transformations to an input program. We show how to implement a k-adversary
by combining enumeration and gradient-based optimization over program sketches.

• We propose the ﬁrst adversarial-training method for neural models of code, adapting the
robust-optimization objective of Madry et al. [25]. To efﬁciently model the adversary during
training, we pre-generate program sketches by partially applying transformations.

• We build an extensible framework for adversarially training models of code. We thoroughly
evaluate our approach on a variety of datasets and architectures. Our results demonstrate
improvements in robustness and the power of training with weak adversaries.

2 Related Work

In concurrent work,2 Bielik and Vechev [9] combine adversarial training with abstention and AST
pruning to train robust models of code. There are a number of key differences with our work: (1)
We consider a richer space of transformations for the adversary, including inserting parameterized
dead-code. (2) We use a strong gradient-based adversary and program sketches for completing
transformations, while they use a greedy search through the space of transformations with a small
number of candidates. (3) Our adversarial training approach is more efﬁcient, as it does not solve
an expensive ILP problem to prune ASTs or train multiple models, but it is possible that we can
incorporate their AST pruning in our framework.

Adversarial examples. In test-time attacks, an adversary perturbs an example so that it is misclas-
siﬁed by a model (untargeted attack) or the perturbed example is classiﬁed as an attacker-speciﬁed
label (targeted) [5, 10, 21, 12, 11]. Initially, test-time attacks were explored in the context of images.
Our discrete domain is closer to test-time attacks in natural language processing (NLP). There are
several test-time attacks in NLP that consider discrete transformations, such as substituting words
or introducing typos [24, 26, 13, 40, 16]. A key difference between our domain and NLP is that
in the case of programs one has to worry about semantics—the program has to work even after
transformations.

Deep learning for source code. Recent years have seen huge progress in deep learning for source-
code tasks—see Allamanis et al. [2]. Here, we considered (sub-)tokenizing the program, analogous
to NLP, and using a variant of recurrent neural networks. This idea has appeared in numerous papers,
e.g., the pioneering work of Raychev et al. [29] for code completion. We have also considered the
AST paths encoding pioneered by Alon et al. [4, 3]. Researchers have considered more structured
networks, like graph neural networks [1] and tree-LSTMs [41]. These would be interesting to consider
for future experimentation in the context of adversarial training. The task we evaluated on, code
summarization, was ﬁrst introduced by Allamanis et al. [1].

3 Transformation Robustness

We now formally deﬁne k-adversaries robustness and our robust-optimization objective.

Learning problem. We assume a data distribution D over X × Y, where X is the space of samples
and Y is the space of labels. As is standard, we wish to solve the following optimization problem:

argmin
w∈H

E
(x,y)∼D

L(w, x, y)

where H is the hypothesis space and L is the loss function. Once we have solved the optimization
problem given above, we obtain a w∗, which yields a function Fw(cid:63) : X → Y.
Abstract syntax trees. We are interested in tasks where the sample space X is that of programs in
some programming language. We do not constrain the space of outputs Y—it could be a ﬁnite set of
labels for classiﬁcation, a natural-language description, etc.

We view a program x as an abstract syntax tree (AST)—a standard data structure for representing
programs. The internal nodes of an AST typically represent program constructs, such as while loops

2A preprint of our work appeared earlier on arXiv than [9].

3

if

if

if

>

=

t

<

=

>

=

x

0

y

false

0

x

y

false

1

(cid:32)

0

2

(cid:32)

false

Figure 2: Left: Example AST transformation (no parameters). Right: AST of program sketch with two holes

and conditionals, and the leaves of the tree denote variable names, constants, ﬁelds, etc. Fig. 2 (left)
shows the AST representation of the code snippet if (x > 0) y = false.
Transformations. A transformation t of a program x transforms it into another program x(cid:48). Typically,
t is deﬁned over ASTs, making it a tree-to-tree transformation. Formally, we think of a transformation
as a function t : R × X → X , where R is a space of parameters to the transformation. For example,
if t changes the name of a variable, it may need to receive a new variable name as a parameter.

For our goal of semantic robustness, we will focus on semantics-preserving transformations, i.e.,
ones that do not change the behavior of the program. Consider, e.g., the transformation t shown
in Fig. 2 (left), where we replace x > 0 with 0 < x (the transformed subtree is highlighted). Note,
however, that our approach is not tied to semantics-preserving transformations, and one could deﬁne
transformations that, for example, introduce common typos and bugs that programmers make.

Program sketches. It will be helpful for us to think of how to partially apply a transformation t
to a program x, without supplying parameters. Intuitively, this operation should result in a set of
programs, one for each possible parameter to t. We assume that parameters only affect leaves of
the transformed AST. Therefore, when we partially apply a transformation t, it results in a tree with
unknown leaves. Equivalently, we can think of such tree as a program sketch: a program with holes.
For example, if t changes names of program variables to new given names, applying t partially to our
running example if (x > 0) y = false results in the program sketch

if (

1 > 0)

2 = false

(cid:32)

(cid:32)

where
program sketch is shown in Fig. 2 (right).

(cid:32)

i are distinct unknown variable names (holes) to be ﬁlled in the sketch. The AST of this

k-adversary. Given a set of transformations T , a k-adversary is an oracle that ﬁnds a sequence of
transformations of size k that maximizes the loss function.

We use q to denote a sequence of transformations t1, . . . , tn, and their corresponding parameters
r1, . . . , rn, where ti ∈ T and ri ∈ R. We use q(x) to denote the program tn(. . . t2(r2, t1(r1, x))),
i.e., the result of applying all transformations in q to x. Let Qk denote the set of all sequences of
transformations and parameters of length k. Given a program x with label y, the goal of the adversary
is to transform x to maximize the loss; formally, the adversary solves the following objective function:

max
q∈Qk

L(w, q(x), y)

(1)

Robust-optimization objective. Now that we have formally deﬁned a k-adversary, we can solve
a robust-optimization problem [6] to learn a model that is robust to such attacks—we call this
k-robustness. Speciﬁcally, we solve the following problem:

argmin
w∈H

E
(x,y)∼D

max
q∈Qk
(cid:124)

(cid:123)(cid:122)
objective of k-adversary

(cid:125)

L(w, q(x), y)

(2)

Informally, instead of computing the loss of a pair (x, y), we consider the worst-case loss resulting
from a k-adversary-generated transformation to x. Such robust-optimization objectives have been
used for training robust deep neural networks [25]. Our setting of source code, however, results in
unique challenges, which we address next.

4 Adversaries and Efﬁcient Training

We now show how to practically implement a k-adversary and efﬁciently train k-robust models.

4

4.1 Deploying k-adversaries

It is typically quite expensive to solve the optimization objective of a k-adversary (Eq. (1)). Even
if we have k = 1 and a single transformation, the search space can be very large. For example,
say we have a transformation that changes names of function arguments. For a function with n
arguments, the space of possible parameters to the transformation is roughly |size of vocabulary|n,
and vocabulary size is easily in the thousands for recent datasets. Indeed, it can be easily shown that
the Eq. (1) is PSPACE-hard, via a reduction from the PSPACE-complete problem of checking that
the intersection of a set of automata is empty [23]. (See proof in the Appendix.) This is in contrast to
the vision domain, where ﬁnding adversarial examples is NP-complete [22].

Clearly, a naïve enumeration approach is
impractical for implementing a k-adversary.
And, unlike with robustness in the vision
domain, the space of AST transformations
is not differentiable. Nonetheless, we ob-
serve that we can break up the search
space into two pieces: (1) an enumerative
search over sequences of transformations
t1, . . . , tk, and (2) a gradient-based search
over transformation parameters, r1, . . . , rk, which allows us to efﬁciently traverse parameter space.

Algorithm 1 k-adversary for program x with label y
1: Let x(cid:63) = x
2: for all sequences t1, . . . , tk do
3:
4:
5:
6: return x(cid:63)

Let sketch z[·] be tk(. . . t2(·, t1(·, x)))
Let r = argmaxr∈R L(w, z[r], y)
if L(w, z[r], y) > L(w, x(cid:63), y) then x(cid:63) = z[r]

The full algorithm is shown in Algorithm 1. Given a sequence of transformations t1, . . . , tk, the
algorithm partially applies it to the input program x. This results in a program sketch z[·]. Recall that
a sketch is a program with holes in the leaves of the AST, and so we use [·] to denote a parameter
that z[·] can take to ﬁll its holes; as with transformations, we use R to denote the set of possible
parameters to z[·]. At this point, we can use existing algorithms to discover a complete program z[r]
that maximizes the loss function (approximately).

For example, if the sketch is represented as a sequence of tokens for an LSTM, the holes are simply
missing tokens to be inserted in certain locations. As such, attacks from natural language processing,
like HotFlip [13], can be used. In our implementation, we use a version of a recent algorithm by
Yefet et al. [37] that performs a gradient-based search for picking a token replacement. We ﬁrst ﬁll in
the holes of the program sketch z[·] with temporary special tokens s. We replace the input embedding
lookup layer with a differentiable tensor multiplication of one-hot inputs and an embedding matrix.
This ensures differentiability up to the input layer v, allowing us to (1) take a gradient-ascent step in
the direction that maximizes loss, and (2) approximate the worst token replacement; formally,

v(cid:48) ← v + η · ∇vL(w, z[s], y)

;

r = argmax v(cid:48)

In other words, we pick the replacement r as the token with the maximum value in the vector v(cid:48),
obtained after a gradient-ascent step on the one-hot input v corresponding to each special token in
z[s]. We impose additional semantic constraints, e.g., in a sketch like if (
2 = false, we
enforce that

2 receive different replacements. See appendix for full details.
(cid:32)

1 > 0)

1 and

(cid:32)

(cid:32)
4.2 Adversarial training

(cid:32)

To train a model that is robust to the k-adversary we deﬁned in Algorithm 1, we can solve the
robust-optimization problem in Eq. (2), where the inner maximization objective—the k-adversary’s
objective—is approximated using Algorithm 1.

argmin
w∈H

E
(x,y)∼D

L(w, q(x), y)

max
q∈Qk
(cid:125)
(cid:123)(cid:122)
(cid:124)
approximate using Algorithm 1

(3)

Practically, for every program x in a mini-batch, we need to run Algorithm 1 to compute a transformed
program x(cid:48) exhibiting worst-case loss. This approach is wildly inefﬁcient during training for two
reasons: (1) the combinatorial search space of the k-adversary in Algorithm 1, and (2) the mismatch
between program formats for transformation and for training. We discuss both below.
Size of the search space. Given a set of transformations T , Algorithm 1 runs for |T |k iterations, a
space that grows exponentially with k. In practice, we observe that it sufﬁces to train with a weak
k = 1 adversary, and still be quite robust to stronger adversaries. Therefore, the number of iterations

5

Table 1: Our suite of semantics-preserving transformations, along with the drops in F1 obtained by random
(Q1
G ) adversaries per transformation, for normally trained models on c2s/java-small.

R ) and gradient-based (Q1

seq2seq (F1: 37.8)

code2seq (F1: 41.4)

seq2seq (F1: 37.8)

code2seq (F1: 41.4)

Transform

∆F1 (Q1

R / Q1

G )

∆F1 (Q1

R / Q1

G )

AddDeadCode
InsertPrintStatement
RenameField
RenameLocalVariable

4.0 / 7.7
2.7 / 6.1
2.3 / 5.4
0.3 / 2.2

1.4 / 2.9
3.8 / 10.2
2.0 / 2.0
0.0 / 2.5

Transform (Cont.)

RenameParameter
ReplaceTrueFalse
UnrollWhile
WrapTryCatch

∆F1 (Q1

R / Q1

G )

∆F1 (Q1

R / Q1

G )

0.3 / 3.0
0.0 / 0.7
0.0 / 0.0
2.5 / 9.4

0.3 / 4.7
0.2 / 0.5
0.4 / 0.4
1.4 / 7.8

of Algorithm 1 is restricted to |T | per training point (x, y). Analogous observations have been made
in vision [38, 25, 36] and NLP [18].

Representation mismatch. There is usually a mismatch between the program format needed for
applying transformations and the program format needed for training a neural network. A program x
is an AST and the adversary’s transformations are deﬁned over ASTs, but the neural network expects
as input a different representation—for example, sequences of (sub)tokens, or as in a recent popular
line of work [4, 3], a sampled set of paths from one leaf of the AST to another.

Therefore, in Algorithm 1, we have to translate back and forth between ASTs and their neural
representation. To be speciﬁc, line 3 of Algorithm 1 computes a sequence of transformations over
ASTs, resulting in a program sketch z[·]. Then, line 4 requires a neural representation of z[·] to
solve the maximization problem. This approach is expensive to employ during training: in every
training step, we have to apply transformations using an external program-analysis tool and convert
the transformed AST to its neural representation.

We address this challenge as follows: To avoid calling program-analysis tools within the training
loop, we pre-generate all possible program sketches considered by the adversary in Algorithm 1. That
is, for every program x in the training set, before training, we generate the set of program sketches
Sx = {z[·] | z[·] = tk(. . . t2(·, t1(·, x)))} to avoid performing AST transformations during training.
As such, the robust-optimization objective in Eq. (3) reduces to the following:

argmin
w∈H

E
(x,y)∼D

max
r∈R,z[·]∈Sx

L(w, z[r], y)

(4)

5 Experimental Evaluation

Research questions. We designed our evaluation to answer the following research questions: (Q1)
Does adversarial training improve robustness to semantics-preserving transformations? (Q2) Does
training with a k-adversary improve robustness against stronger adversaries (larger k)?

Code-summarization task. We consider the task of automatic code summarization [1]—the predic-
tion of a method’s name given its body—which is a non-trivial task for deep-learning models. The
performance of models in this task is measured by the F1 score metric.

We experimented with two model architectures: (i) a sequence-to-sequence BiLSTM model (seq2seq)
and (ii) Alon et al.’s [3] state-of-the-art code2seq model. The seq2seq model takes sub-tokenized
programs as inputs. We built upon the implementation of IBM [20], and trained models for 10 epochs.
code2seq is a code-speciﬁc architecture that samples paths from the program AST in its encoder,
and uses a decoder with attention for predicting output tokens. We used the code2seq TensorFlow
code [3], along with their Java and Python3 path extractors. We train code2seq models for 20 epochs.

Datasets. We conducted experiments on four datasets in two languages: Java, statically typed with
types explicitly stated in the code, and Python, a dynamically typed scripting language. The datasets
originated from three different sources: (i) code2seq’s java-small dataset (c2s/java-small) [3], (ii)
GitHub’s CodeSearchNet Java and Python datasets (csn/java, csn/python) [19], and (iii) SRI Lab’s
Py150k dataset (sri/py150) [30]. For computational tractability of adversarial training, we randomly
subsample each dataset to train/validation/test sets of 150k/10k/20k each.

Program Transformations. We used two separate code-transformation pipelines: for Java, based
on Spoon [28], and for Python, based on Astor [7]. These frameworks apply program transfor-
mations to generate program sketches, as described in Sec. 4.2. We implemented a suite of 8

3 We modiﬁed the Python extractor (making it similar to the Java one), resulting in improved performance.

6

Table 2: Evaluation of our approach and baselines across four datasets. Numbers in brackets are the difference
in adversarial F1 compared to the normally trained model (higher is better).

Model

Training

seq2seq

code2seq

Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G
Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G

c2s/java-small

Nor

37.8
38.8
40.0
40.7

41.4
42.3
42.3
43.6

Q1
G
23.3
27.8 [+4.4]
27.5 [+4.2]
32.0 [+8.7]

24.6
23.4 [-1.2]
28.1 [+3.5]
31.6 [+7.0]

csn/java
Q1
G
17.2
21.4 [+4.3]
30.1 [+12.9]
32.8 [+15.6]

19.6
19.7 [+0.1]
23.5 [+3.9]
27.5 [+7.9]

Nor

32.3
32.6
38.1
37.8

39.2
39.4
40.0
40.6

csn/python
Q1
G
16.2
20.9 [+4.7]
29.8 [+13.5]
32.2 [+16.0]

21.0
20.8 [-0.2]
22.5 [+1.6]
23.8 [+2.8]

Nor

28.9
29.8
36.6
36.2

37.4
36.8
37.6
37.3

sri/py150
Q1
G
22.0
24.0 [+2.0]
33.4 [+11.4]
37.1 [+15.1]

23.7
24.4 [+0.7]
26.6 [+2.9]
29.5 [+5.8]

Nor

34.3
33.7
41.8
41.8

39.7
39.7
40.0
40.2

semantics-preserving transformations, listed in Table 1. All transformations except UnrollWhiles
are parameterized; therefore, applying the transformations creates program sketches with holes. The
transformations are conﬁgured to produce only a single hole (e.g., the WrapTryCatch transformation
encloses the target method body in a single try/catch statement and replaces the name of the variable
holding the caught exception with a single hole). Refer to appendix for exact details.

Adversaries. We consider two variants of the adversary deﬁned in Algorithm 1, a weak and a strong
adversary that search through sequences of length k:

Random (weak) adversary (Qk
randomly ﬁlls the hole in a sketch;
Gradient-based (strong) adversary (Qk
to ﬁll sketch holes with parameters that maximize loss in line 4.

R ): uses a random choice for the parameter r in line 4, i.e., it

G ): uses the gradient-based approach described in Sec. 4.1

Table 1 presents the drops in test performance from attacking normally trained seq2seq and code2seq
models with each of the 8 transformations independently, i.e., using a 1-adversary that only has
a single transformation in its set T . We observe that the gradient-based attacks are signiﬁcantly
stronger than the random attacks (most cause 2x–15x greater drops in F1). Among the most
effective transformations are AddDeadCode, InsertPrintStatements, and WrapTryCatch, which
introduce new tokens in the program that confuse the predictor. Note that while the UnrollWhiles
transformation has no effect in seq2seq’s test performance by itself, it increases the strength of the
other attacks when used in tandem with them.

5.1 Robustness evaluation

Our approach & baselines. We will use Tr-Q1
G to denote models trained using our approach
(Eq. (4)) with the gradient-based 1-adversary. We consider a series of progressively stronger baselines:

Normal training (Tr-Nor): Models are trained using a standard (non-robust) objective.
Data augmentation (Tr-Aug): Minimize the composite loss (cid:80)
where, for each xi, a program x(cid:48)
Random adversarial training (Tr-Q1
a random choice for parameters r (line 4 of Algorithm 1), instead of one that maximizes loss.

i is constructed using a random transformation from Q1.

R ): Similar to our adversarial-training method, except it uses

i L(w, xi, yi) + L(w, x(cid:48)

i, yi),

Robustness against 1-adversary. Table 2 compares our approach and the three baselines on two
metrics: (1) the test F1 score (Nor), and (2) the adversarial F1 score (denoted Q1
G ), where every test
point is attacked with the gradient-based k = 1 adversary with all the transformations in Table 1.
We make the following key observations: Tr-Q1
G models are signiﬁcantly more robust to attack than
all the baselines, especially for seq2seq. Consider the seq2seq models trained on the c2s/java-small
dataset. While the normally trained model suffers a drop in F1 of 14 points, from 37.8 to 23.3,
Tr-Q1
G only sees a drop of 8.7 points. Its ﬁnal F1 of 32.0 is markedly higher than the other models.
On the csn/python dataset, the F1 of seq2seq Tr-Nor drops 44% from 28.9 to 16.2. Tr-Q1
G starts
signiﬁcantly higher at 36.2, and experiences a drop of only 4 points to achieve an F1 score of 32.2
under a Q1
G achieves gains in F1 over Tr-Nor of around
6 (27%) and 14 (74%) points for code2seq and seq2seq, respectively.

G attack. On average across datasets, Tr-Q1

7

c2s/java-small

csn/java

csn/python

sri/py150

)
q
e
s
2
q
e
s
(

1
F

)
q
e
s
2
e
d
o
c
(

1
F

40

30

20

10

40

30

20

10

Nor Q1

R Q5

R Q1

G Q5
G

Nor Q1

R Q5

R Q1

G Q5

G Nor Q1

R Q5

R Q1

G Q5

G Nor Q1

R Q5

R Q1

G Q5
G

Figure 3: A comparison of normally trained (Tr-Nor,
adversarially trained with random parameters (Tr-Q1
on four adversaries, four datasets, and two model architectures (seq2seq and code2seq).

), and adversarially trained (Tr-Q1

), trained with dataset augmentation (Tr-Aug,

G ,

R ,

),
) models

Our results answer Q1 in the afﬁrmative: adversarial training increases robustness to attack via
semantics-preserving program transformations.

R and Q5

Robustness against a 5-adversary. We also studied the robustness of the models against a much
stronger adversary. In particular, we considered random and gradient-based 5-adversaries, denoted
by Q5
G , respectively. Because there are an intractable number (85 ≈ 32k) number of
transformation sequences of length 5, we use a randomly sampled set of sequences of length 5
(see Appendix for details). In addition, we also evaluated the models against a random 1-adversary,
denoted by Q1

R . The graphs shown in Fig. 3 summarize our ﬁndings.

G attack is often stronger than the Q5

In general, the strength of the adversaries increases along the x-axis. Interestingly we observe that
the Q1
R attack, reafﬁrming the strength of the gradient-based
approach to completing sketches and maximizing loss. While the Tr-Q1
G models achieve
comparable robustness to the random adversaries, Tr-Q1
R against
the gradient-based adversaries. The performance of all models drops signiﬁcantly against the Q5
G
attack, however the drops suffered by the Tr-Q1
G models are much less compared to the others,
thus displaying remarkable robustness to the very strong attack. For example, for the Q5
G attack on
seq2seq models for csn/java, the F1 score of Tr-Nor drops from 32.3 to 9.8, while the F1 of Tr-Q1
G
drops just 8.1 points from 37.8 to 29.7.

G consistently outperforms Tr-Q1

R and Tr-Q1

Our results answer Q2 in the afﬁrmative: adversarial training with a small k can improve robustness
against a stronger adversary—at least for the case of training with k = 1 and attacking with k = 5.

seq2seq vs. code2seq. It is interesting to compare the robustness of the seq2seq and code2seq
models. Although the baseline test F1 scores of code2seq models are much higher than the respective
seq2seq models, the models are just as vulnerable to attack. Whereas adversarial training makes
the seq2seq model signiﬁcantly more robust, the robustness gains seen in code2seq are much less.
This phenomenon is especially evident in the Python datasets, where there is a gap of ≈16 F1 points
between the Tr-Nor and Tr-Q1
G attack for seq2seq, but only 2.8–5.8 F1 points
for code2seq (see Table 2). The phenomenon can also be seen in the code2seq graphs in Fig. 3.

G models under Q1

We conjecture that this difference arises because the code2seq model uses sampled program AST
paths to make its prediction. A single program transformation may affect several AST paths at once,
thus making the code2seq model especially vulnerable to attack, even after adversarial training.

6 Conclusion

To the best of our knowledge, this work is the ﬁrst to address adversarial training for source-code
tasks via a robust-optimization objective. Our approach is general, in that it considers adversaries
that perform arbitrary and parameterized sequences of transformations. For future work, it would be

8

interesting to study the effects of adversarial training on other models, e.g., graph neural networks. It
would also be interesting to explore other source-code tasks and transformations, such as automated
refactoring.

References

[1] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. A convolutional attention network for
extreme summarization of source code. CoRR, abs/1602.03001, 2016. URL http://arxiv.
org/abs/1602.03001.

[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of
machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37,
2018.

[3] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from

structured representations of code. arXiv preprint arXiv:1808.01400, 2018.

[4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed
representations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1–29,
2019.

[5] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.

[6] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.

Princeton University Press, 2009.

[7] Berkerpeksag. berkerpeksag/astor, Jan 2020. URL https://github.com/berkerpeksag/

astor.

[8] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks on deep
neural networks using efﬁcient query mechanisms. In European Conference on Computer
Vision, pages 158–174. Springer, 2018.

[9] Pavol Bielik and Martin Vechev. Adversarial robustness for code, 2020.

[10] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndi´c, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
Joint European conference on machine learning and knowledge discovery in databases, pages
387–402. Springer, 2013.

[11] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In

Security and Privacy (SP), 2017 IEEE Symposium on, pages 39–57. IEEE, 2017.

[12] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security, pages 15–26.
ACM, 2017.

[13] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box adversarial

examples for text classiﬁcation, 2017.

[14] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1625–1634, 2018.

[15] FrameworkURL. Our data-generation framework for adversarial training, Feb 2020. URL

https://anonymous.4open.science/r/4ac75222-3673-418e-8b86-9f36ba29fc59.

[16] Siddhant Garg and Goutham Ramakrishnan. Bae: Bert-based adversarial examples for text

classiﬁcation, 2020.

9

[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[18] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal,
Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving veriﬁed robustness to symbol
substitutions via interval bound propagation. In Kentaro Inui, Jing Jiang, Vincent Ng, and
Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4081–
4091. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1419. URL
https://doi.org/10.18653/v1/D19-1419.

[19] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.

Codesearchnet challenge: Evaluating the state of semantic code search, 2019.

[20] IBM. Ibm/pytorch-seq2seq, Jan 2020. URL https://github.com/IBM/pytorch-seq2seq.

[21] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks

with limited queries and information. arXiv preprint arXiv:1804.08598, 2018.

[22] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex:
An efﬁcient smt solver for verifying deep neural networks. In International Conference on
Computer Aided Veriﬁcation, pages 97–117. Springer, 2017.

[23] Dexter Kozen. Lower bounds for natural proof systems.

In 18th Annual Symposium on

Foundations of Computer Science (sfcs 1977), pages 254–266. IEEE, 1977.

[24] Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, and Michael
Witbrock. Discrete adversarial attacks and submodular optimization with applications to text
classiﬁcation. In SysML, 2019.

[25] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.

[26] Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamdhere. Did

the model understand the question? In ACL, 2018.

[27] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Anan-
thram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017
ACM on Asia Conference on Computer and Communications Security, pages 506–519. ACM,
2017.

[28] Renaud Pawlak, Martin Monperrus, Nicolas Petitprez, Carlos Noguera, and Lionel Seinturier.
Spoon: A Library for Implementing Analyses and Transformations of Java Source Code.
Software: Practice and Experience, 46:1155–1179, 2015. doi: 10.1002/spe.2346. URL
https://hal.archives-ouvertes.fr/hal-01078532/document.

[29] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical language
models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language
Design and Implementation, pages 419–428, 2014.

[30] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision
trees. In Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented
Programming, Systems, Languages, and Applications, OOPSLA 2016, page 731–747, New
York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450344449. doi:
10.1145/2983990.2984041. URL https://doi.org/10.1145/2983990.2984041.

[31] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security, pages 1528–1540. ACM,
2016.

10

[32] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat.
Combinatorial sketching for ﬁnite programs. In Proceedings of the 12th international conference
on Architectural support for programming languages and operating systems, pages 404–415,
2006.

[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.

[34] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander
Madry. Robustness may be at odds with accuracy. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=SyxAb30cY7.

[35] Ke Wang and Mihai Christodorescu. Coset: A benchmark for evaluating neural program

embeddings, 2019.

[36] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial

training. arXiv preprint arXiv:2001.03994, 2020.

[37] Noam Yefet, Uri Alon, and Eran Yahav. Adversarial examples for models of code. arXiv

preprint arXiv:1910.07517, 2019.

[38] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only prop-
agate once: Accelerating adversarial training via maximal principle. In Advances in Neural
Information Processing Systems, pages 227–238, 2019.

[39] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. Generating adversarial
examples for holding robustness of source code processing models. 2020. URL https:
//www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhangH.6730.pdf.

[40] Wei Emma Zhang, Quan Z Sheng, AHOUD Alhazmi, and CHENLIANG LI. Adversarial
attacks on deep learning models in natural language processing: A survey. arXiv preprint
arXiv:1901.06796, 2019.

[41] Jinman Zhao, Aws Albarghouthi, Vaibhav Rastogi, Somesh Jha, and Damien Octeau. Neural-
augmented static analysis of android communication. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, pages 342–353, 2018.

11

A PSPACE-hardness of a k-adversary

Let D1, . . . , Dm be m deterministic ﬁnite automata (DFAs) over a ﬁnite alphabet Σ. Let n be the
size of the largest automaton, measured as the number of states. Let L(Di) be the language deﬁned
by Di. The problem of deciding whether ∩iL(Di) is empty is PSPACE-complete [23].

Construct a program x with m variables v1, . . . , vm with assignment statements that assign each vi
the initial state of the DFA Di. Our reduction uses just one transformation t parameterized by the
alphabet Σ. t(α, x) (where α ∈ Σ) transforms the program x by updating the rhs of the assignment
statements to the next states of all automata, i.e., t simulates a single step of all the automata Di.
We also include another transformation that is the identity function, i.e., t(·, x) = x. We assume a
special stuck state. Now we construct a 0 − 1 loss function as follows: L(x) = 1 iff the assignment
statements in x correspond to accepting states in all DFAs Di.
Note that the intersection is empty iff there is no string of length at most nm that is accepted by all
automata. (E.g., for m = 1, if D1 is non-empty, there has to be a string of length (cid:54) n, the number
of states.) Therefore, it is easy to see that maxq∈Qk L(x) is 1 iff ∩n
i=1L(Di) is not empty, where
k = nm.

Since PSPACE is closed under complement, we obtain the following theorem.

Theorem 1 The decision problem corresponding to maximizing the loss

max
q∈Qk

l(q(x))

is PSPACE-hard.

B Data Pipeline

B.1 Overview

Figure 4 shows an overview of the framework we built to support our experiments and bootstrap future
efforts in the space of semantically robust models of source code. In general, the framework supports
arbitrary input datasets. For each input dataset, a normalization procedure must be supplied to
translate the input data into a consistent encoding. Once data is normalized, the rest of the framework
can be utilized to (i) apply program transformations to arbitrary depth, (ii) train models (normally),
(iii) apply either gradient or random targeting to the holes in transformed programs (for seq2seq
and code2seq), (iv) adversarially train models, and (v) adversarially attack and evaluate the trained
models. The source code is available anonymously [15].

B.2 Details of Program Transformations

We employ a total of eight semantics preserving transforms. Each program transform produces, as
output, a program sketch (that is, a program with one or more holes). To turn a program sketch into
an complete program, each hole must be replaced with a valid token. For each of the eight transforms
we implemented, valid hole replacements take the form of either variable names or string literals.
Detailed descriptions of our eight semantics preserving transforms are given below:

1. AddDeadCode: a single statement of the form if (false) { int <HOLE> = 0; } is ap-
pended to the beginning or end of the target program. The insertion location (either begin-
ning, or end) is chosen at random.

2. RenameLocalVariables: a single, randomly selected, local variable declared in the target

program has its name replaced by a hole.

3. RenameParmeters: a single, randomly selected, parameter in the target program has its

name replaced by a hole.

4. RenameFields: a single, randomly selected, referenced ﬁeld (this.field in Java, or

self.field in Python) has its name replaced by a hole.

5. ReplaceTrueFalse: a single, randomly selected, boolean literal is replaced by an equivalent
expression containing a single hole (e.g., ("<HOLE>" == "<HOLE>") to replace true).

12

Figure 4: An overview of our framework, which enables our experimental evaluation and bootstraps efforts
towards training more robust models of source code. With its plug-and-play architecture it can, in the future, be
extended with more models, data sources, and program transforms.

6. UnrollWhiles: a single, randomly selected, while loop in the target program has its loop

body unrolled exactly one step. No holes are created by this transform.

7. WrapTryCatch: the target program is wrapped by a single try { ... } catch (...) {
... } statement. The catch statement passes along the caught exception. A hole is used in
the place of the name of the caught exception variable (e.g., catch (Exception <HOLE>)).

8. InsertPrintStatements: A single System.out.println("<HOLE>"),

in Java, and
print(’<HOLE>’) in Python, is appended to the beginning or end of the target program. The
insertion location (either beginning, or end) is chosen at random.

C Experimental Details

C.1 Datasets

We conduct our experiments on four datasets in two different languages (java, python): c2s/java-
small, csn/java, csn/python and sri/py150. Each dataset contains over 0.5M data points (method
body–method name pairs). The original sizes of the datasets are shown in Table 3.

Adversarial training in our domain of source code is an expensive process: we have to pre-generate
several transformed versions of each data point as program sketches, and then repeatedly run the
gradient-directed attack to ﬁll in the holes in the sketches. Running extensive experiments across
the 4 datasets and 2 models (seq2seq, code2seq) was computationally intractable, both in terms of
time and space. Thus, we randomly subsample the four datasets to have train/validation/test sets of
150k/10k/20k each. The datasets remain sizeable, and thus we ﬁnd that this only has a minimal effect
on model performance.

C.2 Models

seq2seq The seq2seq models were given sub-tokenized programs as input, which were obtained
by splitting up camel and snake-case, and other minor preprocessing. We trained 2-layer BiLSTM
models, with 512 units in the encoder and decoder, with embedding sizes of 512.

code2seq code2seq is the current state-of-the-art model for the task of code summarization. We built
upon the implementation from the authors of code2seq4, and use the original model parameters.

4https://github.com/tech-srl/code2seq

13

Dataset

Train Validation

Test

c2s/java-small
csn/java
sri/py150
csn/python

500.3k
454.3k
693.7k
408.2k

26.3k
15.3k
81.7k
22.8k

38.9k
26.9k
86.0k
21.9k

Table 3: The original sizes of the datasets

For both seq2seq and code2seq, we use input and output vocabularies of size 15k and 5k respectively.
All models were trained and evaluated using NVIDIA GPUs (GeForce RTX 2080 Ti and Tesla V100).

2

C.3 Adversarial Training

Adversarial training is known to improve robustness to attack at the cost of degraded performance on
clean data [34]. To maintain performance on clean data, we train with the following composite loss:

(cid:88)

i

λ · L(xi, yi) + (1 − λ) · Ladv(xi, yi)

where L is the normal training loss and Ladv is the loss from the robust optimization objective
described in Sec. 4. The λ hyperparameter controls the trade-off between performance and robustness,
we picked λ = 0.4 in our experiments after a grid search.

The robust optimization objective for adversarial training requires choosing the worst transformation
for each data point in each mini-batch of training. While this can be efﬁciently implemented in
computer vision using PGD, it is very expensive to do so in our setting. For tractability of training,
we do the following:

• We apply the gradient attack periodically during training to generate new token replacements
for the program sketches. For seq2seq we do this after every epoch, and for code2seq we do
this after every two epochs.

• Instead of picking the worst transformation for each individual point in a mini-batch, we
pick the transformation that does the worst for all the points on the whole (i.e. highest loss
over the batch).

C.4 Gradient Attack

We adapt the attack for code models from Yefet et al. [37], to ﬁnd replacements for the holes in the
program sketches. We represent holes in the program sketches by special tokens. A sketch obtained
after applying more than one transform would contain multiple variants of the special tokens, as the
holes must be ﬁlled in independently. The summary of the attack on seq2seq is described below:

• During the training of the model, the special tokens are added to the input vocabulary. The
embedding layer in the model encoder is replaced by an embedding matrix. The input tensor
of shape (batch_size, max_seq_len) is converted to its one-hot representation v, a tensor of
shape (batch_size, max_seq_len,|source_vocab|). The embedding look-up is replaced by
a multiplication between v and the embedding matrix, the rest of the encoder remains the
same. This step is necessary for differentiability upto the token-level input representation.

• For simplicity, we implement the attack to depth=1 and width=1. After one forward, we
calculate the loss and backpropagate its gradient to the v layer. We take a gradient-ascent
step on v, to yield v(cid:48). We approximate the worst token replacement for each special token by
averaging gradients over each occurrence in the input and choose the token corresponding
to the maximum. We impose an additional semantic constraint that the token replacements
made for different special tokens are distinct.

In code2seq, the subtoken embedding layer is replaced by the tensor multiplication.

14

Number of Samples Adversarial F1 ∆F1 (Baseline: 37.8)

10
20
30
40
50

29.7
28.5
27.9
27.6
27.4

-8.1
-9.3
-9.9
-10.2
-10.4

Table 4: An exploration of increasing sample counts for Q5
set. Note that the attack strength saturates very quickly.

G attacks, on the Tr-Q1

G model on the csn/java test

C.5

Implementing a tractable 5-Adversary

In Sec. 5.1, we evaluate the robustness of the trained models on the 5-adversary Q5
G, i.e. an adversary
that performs 5 semantics preserving transformations on the input. With our suite of 8 transforms, it
results in a total of 85 = 32768 possible sequences against which we need to evaluate each test data
point. This is computationally intractable for two reasons:

• For each of the 20k test points in each dataset, we choose the sequence of transforms which
results in the greatest loss as the attack. To evaluate against each of the 85 transforms, it
would require millions of forward passes through the model.

• Each sequence of transforms is generated in two steps: (i) application of transformations to
generate program sketches, and (ii) gradient attack to ﬁll in the holes of the sketches. Both
these steps are expensive and time consuming. In particular, generating 10 transforms for
20k data points requires 10 minutes for step (i) and 120 minutes for step (ii) for the code2seq
model (seq2seq is somewhat faster, but still very slow). In addition, after generating program
sketches and using a gradient attack to ﬁll holes, we still must perform adversarial evaluation
which, for code2seq on 200k data points (20k points × 10 transformed variants per point),
takes an additional 60 minutes.

Due to the above reasons, we choose to implement the 5-adversary by randomly sampling 10
sequences of transformations of length 5. We investigate the effect of increasing the number of
transforms sampled, and we ﬁnd that the strength of the attack saturates. This is seen in Table 4 for
the Tr-Q1
G model on the csn/java dataset. Because of this prominent saturation, we chose to use only
10 samples to keep total evaluation time to less than 24 hours (using 4 V100 GPUs).

D Full Robustness Evaluation Results

Table 5 contain numbers corresponding to the graphs in Fig. 2 of the main text. Each table presents
results for the different models under attacks of increasing strength (Q1
G). We note
that, in all but one case, the adversarially trained model (Tr-Q1

R, Q1
G), performs the best under attack.

G, Q5

R, Q5

15

Table 5: Evaluation of our approach and baselines across four datasets. Numbers in brackets are the difference
in adversarial F1 compared to the normally trained model (higher is better).

(a) Results for the random 1-adversary (Q1

R ).

Model

Training

seq2seq

code2seq

Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G
Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G

c2s/java-small

Nor

37.8
38.8
40.0
40.7

41.4
42.3
42.3
43.6

Q1
R
29.5
33.8 [+4.3]
33.6 [+4.1]
34.8 [+5.2]

29.5
29.4 [-0.1]
32.1 [+2.6]
33.5 [+4.0]

csn/java
Q1
R
23.5
29.1 [+5.7]
34.0 [+10.6]
34.3 [+10.8]

27.3
26.9 [-0.4]
29.6 [+2.3]
30.4 [+3.1]

Nor

32.3
32.6
38.1
37.8

39.2
39.4
40.0
40.6

csn/python
Q1
R
21.7
27.2 [+5.5]
33.3 [+11.6]
33.5 [+11.8]

25.0
25.5 [+0.5]
26.0 [+1.0]
26.7 [+1.7]

Nor

28.9
29.8
36.6
36.2

37.4
36.8
37.6
37.3

sri/py150
Q1
R
29.3
30.5 [+1.2]
37.8 [+8.5]
38.1 [+8.8]

28.7
29.1 [+0.4]
30.8 [+2.1]
31.2 [+2.4]

Nor

34.3
33.7
41.8
41.8

39.7
39.7
40.0
40.2

(b) Results for the random 5-adversary (Q5

R ).

Model

Training

seq2seq

code2seq

Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G
Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G

c2s/java-small

Nor

37.8
38.8
40.0
40.7

41.4
42.3
42.3
43.6

Q5
R
18.8
31.7 [+12.9]
30.4 [+11.6]
31.3 [+12.5]

24.4
23.4 [-1.0]
29.9 [+5.5]
31.0 [+6.6]

csn/java
Q5
R
15.9
26.3 [+10.4]
30.6 [+14.7]
32.1 [+16.1]

22.0
20.5 [-1.5]
26.2 [+4.2]
27.2 [+5.2]

Nor

32.3
32.6
38.1
37.8

39.2
39.4
40.0
40.6

csn/python
Q5
R
15.2
24.2 [+9.0]
30.8 [+15.5]
31.6 [+16.4]

22.8
22.8 [+0.0]
23.9 [+1.1]
24.6 [+1.8]

Nor

28.9
29.8
36.6
36.2

37.4
36.8
37.6
37.3

sri/py150
Q5
R
25.2
27.9 [+2.7]
34.7 [+9.5]
35.4 [+10.2]

24.4
24.8 [+0.4]
28.2 [+3.8]
28.4 [+4.0]

Nor

34.3
33.7
41.8
41.8

39.7
39.7
40.0
40.2

(c) Results for the targeted 1-adversary (Q1

G ).

Model

Training

seq2seq

code2seq

Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G
Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G

c2s/java-small

Nor

37.8
38.8
40.0
40.7

41.4
42.3
42.3
43.6

Q1
G
23.3
27.8 [+4.4]
27.5 [+4.2]
32.0 [+8.7]

24.6
23.4 [-1.2]
28.1 [+3.5]
31.6 [+7.0]

csn/java
Q1
G
17.2
21.4 [+4.3]
30.1 [+12.9]
32.8 [+15.6]

19.6
19.7 [+0.1]
23.5 [+3.9]
27.5 [+7.9]

Nor

32.3
32.6
38.1
37.8

39.2
39.4
40.0
40.6

csn/python
Q1
G
16.2
20.9 [+4.7]
29.8 [+13.5]
32.2 [+16.0]

21.0
20.8 [-0.2]
22.5 [+1.6]
23.8 [+2.8]

Nor

28.9
29.8
36.6
36.2

37.4
36.8
37.6
37.3

sri/py150
Q1
G
22.0
24.0 [+2.0]
33.4 [+11.4]
37.1 [+15.1]

23.7
24.4 [+0.7]
26.6 [+2.9]
29.5 [+5.8]

Nor

34.3
33.7
41.8
41.8

39.7
39.7
40.0
40.2

(d) Results for the targeted 5-adversary (Q5

G ).

Model

Training

seq2seq

code2seq

Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G
Tr-Nor
Tr-Aug
Tr-Q1
R
Tr-Q1
G

c2s/java-small

Nor

37.8
38.8
40.0
40.7

41.4
42.3
42.3
43.6

Q5
G
11.5
23.5 [+12.0]
21.7 [+10.2]
26.3 [+14.8]

15.5
14.5 [-0.9]
22.4 [+7.0]
27.3 [+11.9]

csn/java
Q5
G
9.8
15.6 [+5.8]
25.1 [+15.3]
29.7 [+19.9]

12.1
11.8 [-0.3]
17.6 [+5.5]
22.0 [+10.0]

Nor

32.3
32.6
38.1
37.8

39.2
39.4
40.0
40.6

csn/python
Q5
G
10.4
14.4 [+4.0]
25.3 [+14.9]
29.4 [+19.0]

15.6
15.8 [+0.2]
17.6 [+2.0]
20.6 [+5.0]

Nor

28.9
29.8
36.6
36.2

37.4
36.8
37.6
37.3

sri/py150
Q5
G
16.5
19.1 [+2.6]
28.1 [+11.6]
33.4 [+16.8]

18.2
17.8 [-0.4]
22.1 [+3.9]
25.1 [+6.9]

Nor

34.3
33.7
41.8
41.8

39.7
39.7
40.0
40.2

16

