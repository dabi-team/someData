9
1
0
2

v
o
N
7
2

]

G
L
.
s
c
[

1
v
4
2
2
2
1
.
1
1
9
1
:
v
i
X
r
a

Multi-label Classification for Automatic Tag
Prediction in the Context of Programming Challenges

Bianca Iancu
Delft University of Technology
Delft, South Holland
A.Iancu-1@student.tudelft.nl

Kyriakos Psarakis
Delft University of Technology
Delft, South Holland
K.Psarakis@student.tudelft.nl

Gabriele Mazzola
Delft University of Technology
Delft, South Holland
G.Mazzola@student.tudelft.nl

Panagiotis Soilis
Delft University of Technology
Delft, South Holland
P.Soilis@student.tudelft.nl

ABSTRACT
One of the best ways for developers to test and improve
their skills in a fun and challenging way are programming
challenges, offered by a plethora of websites. For the in-
experienced ones, some of the problems might appear too
challenging, requiring some suggestions to implement a so-
lution. On the other hand, tagging problems can be a tedious
task for problem creators. In this paper, we focus on automat-
ing the task of tagging a programming challenge description
using machine and deep learning methods. We observe that
the deep learning methods implemented outperform well-
known IR approaches such as tf-idf, thus providing a starting
point for further research on the task.

1 INTRODUCTION
In recent years, more and more people have started to show
interest in competitive programming, either for purely ed-
ucational purposes or for preparing for job interviews. Fol-
lowing this trend, we can also see an increase in the number
of online platforms providing services such as programming
challenges or programming tutorials. These problems are
based on a limited number of strategies to be employed. Un-
derstanding which strategies to apply to which problem is
the key to designing and implementing a solution. However,
it is often difficult to directly infer from the textual problem
statement the strategy to be used in the solution, especially
with little or no programming experience. Thus, assisting
the programmer by employing a system that can recom-
mend possible strategies could prove very useful, efficient
and educational.

To this end, we propose a system to automatically tag a
programming problem given its textual description. These
tags can be either generic, such as ’Math’, or more specific,
such as ’Dynamic Programming’ or ’Brute Force’. Each prob-
lem can have multiple tags associated with it, thus this re-
search is focusing on a multi-class multi-label classification

problem. This is a more challenging problem than the usual
multi-class classification, where each data point can only
have one label attached. An example of a data sample for our
problem is shown in Figure 1. A similar problem would be
to predict these tags based on the source code of a solution,
but this would restrict the application domain. To be more
specific, we are interested in applying the system in the con-
text of online programming challenges platforms where no
solution or source code is available. Thus, we only consider
the textual description of the problem statements as input to
our system.

Figure 1: Data sample example

By gathering data from two of the main online program-
ming challenges platforms, Topcoder 1 and CodeForces 2,
we approach the problem through both machine learning
and deep learning solutions, experimenting with different
architectures and data representations. Considering the com-
plexity of this problem, which is difficult even for humans,
our hypothesis is that deep learning methods should be an

1https://www.topcoder.com/
2https://codeforces.com

 
 
 
 
 
 
effective way of approaching this problem, given enough
data. Based on the aforementioned, the research question
that we are trying to answer is the following: ”Could deep
learning models learn and understand what are the strategies
to be employed in a programming challenge, given only the
textual problem statement?”

The rest of the paper is organized as follows: in Section
2 we describe the related work carried out in the literature,
both regarding multi-label classification, as well as text rep-
resentation methods. Subsequently, in Section 3 we describe
the process of gathering, understanding and pre-processing
the data, while in Section 4 we present the data representa-
tion methods and the models that we employ. Following that,
in Section 5 we discuss the experimental setup and present
the results, followed by a discussion and reflection regard-
ing those in Section 6. Finally, we conclude our research in
Section 7.

2 RELATED WORK

Multi-label classifcation
As far as we are aware, no previous work has been carried out
regarding multi-label classification in the context of tagging
programming problem statements. Additionally, most of
the papers tackling this task employ traditional machine
learning methods, rather than deep learning approaches. In
[21] the authors propose a multi-label lazy learning approach
called ML-kNN, based on the traditional k-nearest neighbor
algorithm. Moreover, in [9] the text classification problem
is approached, which consists of assigning a text document
into one or more topic categories. The paper employed a
Bayesian approach into multiclass, multi-label document
classification in which the multiple classes from a document
were represented by a mixture model. Additionally, in [6] the
authors modeled the problem of automated tag suggestion
as a multi-label text classification task in the context of the
”Tag Recommendation in Social Bookmark Systems”. The
proposed method was built using Binary Relevance (BR) and
a naive Bayes classifier as the base learner which were then
evaluated using the F-measure. Furthermore, a new method
based on the nearest-neighbor technique was proposed in
[5]. More specifically, the multi-label categorical K-nearest
neighbor approach was proposed for classifying risk factors
reported in SEC form 10-K. This is an annually filed report
published by US companies 90 days after the end of the fiscal
year.

A different strategy for approaching the multi-label classi-
fication problem proposed in the literature is active learning,
examined in [2]. Furthermore, in [18] the authors proposed
a multi-label active learning approach for text classification
based on applying a Support Vector Machine, followed by a
Logistic Regression.

Regarding deep learning approaches, in [11] the authors
analyzed the limitations of BP-MLL, a neural network (NN)
architecture aiming at minimizing the pairwise ranking error.
Additionally, they proposed replacing the ranking loss mini-
mization with the cross-entropy error function and demon-
strated that neural networks can efficiently be applied to
the multi-label text classification setting. By using simple
neural network models and employing techniques such as
rectified linear units, dropout and AdaGrad, their work out-
performed state-of-the-art approaches for this task. Further-
more, the research carried out in [8] analyzed the task of
extreme multi-label text classification (XMTC). This refers to
assigning the most relevant labels to documents, where the
labels can be chosen from an extremely large label collection
that could even reach a size of millions. The authors in [8]
represented the first deep learning approach to XMTC using
a Convolutional Neural Network. The authors showed that
the proposed CNN successfully scaled to the largest datasets
used in the experiments, while consistently producing the
best or the second-best results on all the datasets.

When it comes to evaluation metrics in multi-label classifi-
cation scenario, a widely employed metric in the literature is
the Hamming loss [6][13][16][22]. Furthermore, more tradi-
tional metrics are also used such as the F-measure [6][13][16],
as well as the average precision [16][21].

Text representation
Apart from the aforementioned, a lot of literature work has
gone into experimenting with different ways of representing
text. According to [17], the word representation that has
been traditionally used in the majority of supervised Natural
Language Processing (NLP) applications is one-hot encoding.
This term refers to the process of encoding each word into
a binary vector representation where the dimensions of the
vector represent all the unique words included in the corpus
vocabulary. While this representation is simple and robust
[10], it does not encode any notion of similarity between
words. For instance, the word ”airplane” is equally different
to the word ”university” as the word ”student”. The proposal
of Word2Vec [10] solves this issue by encoding words into
a continuous lower dimensional vector space where words
with similar semantics and syntax are grouped close to each
other. This led to the proposal of more types of text embed-
dings such as Doc2Vec [7] which encodes larger sequences
of text, such as paragraphs, sentences, and documents, into
a single vector rather than having multiple sectors, ie. one
per word. One of the more recent language models which
produced state-of-the-art results in several NLP tasks is the
Bidirectional Encoder Representations from Transformers

2

(BERT) [1]. Its main innovation comes from applying bidi-
rectional training of an attention model, the Transformer, to
language modeling.

3 DATA
In this section, we describe the steps carried out to obtain
the dataset that we have worked with. More specifically, we
focus on presenting the data source and the data collection
process, followed by an overview of the preprocessing that
we perform. Additionally, we show several descriptive sta-
tistics regarding the data and explain the steps we take for
defining the tag taxonomy for the dataset.

Data sources & collection
We investigate different competitive programming platforms
to build a dataset of programming statements and tags. Specif-
ically, the platforms of interest to us are: TopCoder3, Hack-
errank 4, CS Academy5, Codewars6, and Codeforces7. Due
to legal reasons as well as the complexity of the platform
interfaces, we manage to successfully scrape only two of
these platforms:

• Codeforces on 13/09/2019. Codeforces is a website
that offers programming contests to people inter-
ested in participating. Users are free to upload a
challenge statement, together with several tags spec-
ifying the strategies that should be used to devise
a solution. We scrape a total of 5,341 problems, to-
gether with their tags.

• Topcoder on 17/09/2019. Topcoder is a crowdsourc-
ing platform in the sense that each problem is made
available to all the developers and they provide so-
lutions. It also provides an archive with free access,
containing a set of problems with tags, similarly to
CodeForces. We scrape a total of 4,508 problems,
together with their tags.

We implement the scraping of these two platforms in Python,
creating two ad-hoc scripts, one per platform. We use Beau-
tifulSoup48 to parse the HTML of the pages, to extract only
meaningful information.

Data preprocessing
The crawled data contain a lot of redundant information or
even data that can impact the training process in a negative
way (e.g. HTML tags and LATEXsymbols). What is more, some
NLP techniques, such as stop-word removal, have proven

3https://www.topcoder.com/
4https://www.hackerrank.com/
5https://csacademy.com/
6https://www.codewars.com/
7http://codeforces.com/problemset
8https://pypi.org/project/beautifulsoup4/

to increase classification performance on textual data since
they do not provide any additional information and only
increase the dimensionality of the problem [15]. Following
the collection process, we merge the two crawled datasets
and preprocess them.

At first, HTML tags (< .∗? >) are removed since they do
not provide any descriptive information about the problem.
Afterward, since mathematical definitions are in LATEXformat
on CodeForces and as plain text on TopCoder we decide
to remove them from both to avoid introducing differences
between the two sets of data. In addition, all non-ASCII
characters, digits, stop-words, punctuation and one character
words (i.e. variable names) are removed for the same reason.
The next step of the preprocessing pipeline is to concate-
nate all textual fields into one (i.e. title and description are
concatenated into the field ”text”). Furthermore, we convert
every character in this new field to lowercase. Finally, we
run an exact duplicate removal since we observed that some
challenges on both crawled websites appear more than once.
Then we remove words with less than 10 occurrences in the
entire corpus assuming that they are not descriptive enough.
Thus, the final dataset is a JSON file with an array of coding
challenges and each entry has two fields, the ”text” and the
”tags” associated with it.

Descriptive statistics
After performing the preprocessing steps, we compute sev-
eral descriptive statistics. More specifically, we compute the
total number of problems, the average word count per prob-
lem and the average tags per problem, all separately for the
CodeForces and Topcoder data, as well as for the combined
dataset. These statistics are provided in Table 1.

CodeForces Topcoder Combined dataset

Problem count
Avg word count/problem
Avg tags/problem

4,592
120.93
1.76

4,115
94.81
1.46

8,707
108.58
1.62

Table 1: Descriptive statistics

As we can observe, the number of problems gathered from
the two data sources, as well as the additional statistics, are
quite balanced. There are no significant differences when
it comes to the average word count per problem and the
average tags per problem between the CodeForces data and
the Topcoder data. For a breakdown of the average number
of words per tag, the reader can refer to Appendix A.

Tag taxonomy
Since each of the websites uses a different set of tags, it is
necessary to create a taxonomy that maps the original tags
to new common target labels. To decide on the class labels

3

to be included in the taxonomy, we start by analyzing the
data. To this end, we generate a correlation matrix for all
the original tags present in the dataset to infer which of
them can be grouped in a more general label. The original
number of labels was 16 for the Topcoder data and 35 for
the CodeForces data. After performing the label aggregation
we result in 17 common tags for the two data sources. For a
complete list of these, the reader can refer to Appendix B.

Following preliminary experimentation, we observe that
the performance is unsatisfactory which brings us to suspect
that we have too few data instances given the associated
number of labels. Thus, we proceed by further aggregating
and reducing the number of tags. As before, we perform a
visual analysis of the correlation matrix in Figure 2 between
the different tags to decide which of them can be aggregated.

Figure 2: Problem tag correlation

Additionally, we plot a bar chart of the frequencies of the
class labels in Figure 3 to understand whether some classes
are present in too few instances to be properly learned by
the models.

After performing this additional aggregation, we decrease
the number of tags from 17 to 9. For a complete list of
these, the reader can refer to Appendix C. The main semantic
decisions taken while creating the final taxonomy for the
data are the following:

(1) We remove several general tags since we consider
that they are too general and vague and, thus, do not
give any concrete information regarding the method-
ology associated with the problem statement. Such
tags are, for example, ’implementation’, ’program-
ming’, ’arrays’ or ’interactive’.

(2) We group problem statements originally labeled with
tags related to strings, string manipulations, and reg-
ular expressions into one category under the ’String
Operations’ name.

(3) The term ’ternary search’ was found to correspond
to similar operations as ’binary search’9. Thus, we
group their associated problem statements into one
label, ’Binary Search’. During the second aggrega-
tion stage, we further combine ’Binary Search’ with
the ’Search’ label under the common name ’Search
and Binary Search’, as although they use different
methods, they refer to a similar class of problems.
(4) Since we observed that tags like ’shortest path’ and
’dfs’ were associated with both ’graphs’ and ’trees’,
we merge the latter two into the ’Graphs’ label. Ad-
ditionally, since graphs are a data structure, we fur-
ther combine them with ’Data Structures’ in the sec-
ond aggregation stage into the ’Data Structures and
Graphs’ label.

(5) Regarding the ’Math’ tag we initially intended to
remove it, since we consider it to be quite general.
However, after further analysis we observe that it is
present in a high number of data points, hence we
decide to keep it in the taxonomy. In the second ag-
gregation stage, we combine it with the ’Probabilities
and Combinatorics’ tags, since they are a mathemat-
ics field as well, under the common name ’Math and
Probabilities’.

The distribution of the final taxonomy associated with
our dataset is shown in Figure 4. We can observe that there
is an imbalance present in the dataset, with approximately
2500 data points difference between ’Math and Probabilities’
(the most common tag) and ’Geometry’ (the least common
tag). The imbalance is further taken into account when im-
plementing the machine learning and deep learning models.
Additionally, it is important to note that while we define
9 classes for classifying the data points, according to the
descriptive statistics presented before, in our data there are

Figure 3: Distribution of the original taxonomy

9https://www.geeksforgeeks.org/ternary-search/

4

less than 2 tags per data point on average. Thus, the models
will have to learn to predict significantly less 1s than 0s for
the associated labels. Therefore, to account for this matter,
in the methodology presented in the next section, we weigh
the evaluation metric, as well as the loss function for the
deep learning models to give more importance to correctly
predicting 1s during the training.

Figure 4: Distribution of the final taxonomy

4 METHODOLOGY
In this section, we first describe the data representation tech-
niques that we used to encode our data. Following that, the
machine and deep learning techniques used to classify the
programming problem statements into the target set of tags
are outlined. It has to be underlined that we only provide
the intuition and main concepts of these methods since they
do not constitute the main focus of our work.

Data representation

tf-idf. To obtain features for our baseline classifier we
employ the tf-idf approach. The Python library that we
used10 employs the following formula:

tf-idf(t,d) = tf(t,d) ∗ idf(t)

where tf(t,d) is the number of times term t appears in docu-
ment d, and idf(t) = loд 1+n
+1 where n is the total number
1+d f (t )
of documents and df(t) is the number of documents contain-
ing term t. By doing this, we obtain a number of features for
each document equal to the total number of unique terms.
For a more detailed overview of tf-idf and its usage we refer
the reader to [12].

One-hot encoding. Another representation technique we
experiment with is one-hot encoding. This method repre-
sents textual data in discrete vectors with binary values. As
mentioned previously, one-hot encoding treats all the words
in equal fashion and does not preserve any information about
the word meaning. Another thing to note is that it also leads
to a feature space with very high dimensionality. In particu-
lar, the number of dimensions of the one-hot vector is equal
to the number of unique words in the programming problem
statements dataset.

Word2Vec. One of the most popular representation tech-
niques in the field of NLP is the Word2Vec [10] model. In
particular, in their work, the authors propose two different
feedforward networks, Continuous Bag-of-Words (CBOW)
and Skip-gram, that project words into a continuous vector
space which preserves their semantics and syntax. The main
difference between the two models is that CBOW attempts
to predict the target word based on its context, whereas
Skip-gram predicts the context of the current word. In our
work, we used the Skip-gram method since it was found to
outperform CBOW in [10]. More details can be found in the
original Word2Vec paper [10].

Doc2Vec. The final representation that was tried out is the
Doc2Vec embeddings proposed by [7]. In simple terms, it ex-
tends the continuous vector space idea from Word2Vec [10]
to larger text sequences, such as sentences, paragraphs, and
documents. To be more exact, we map each programming
problem statement to a pre-defined number of dimensions.
Similarly to Word2Vec, two models are proposed called Dis-
tributed Memory (PV-DM) and Distributed Bag of Words
(PV-DBOW) which correspond to the CBOW and Skip-gram
from Word2Vec [10] respectively. In this paper, we use the
PV-DBOW model to obtain the document embeddings. More
information about Doc2Vec are available in [7].

Models
For carrying out the multi-label classification, we employ
both machine learning and deep learning approaches, thus
comparing the performances and analyzing the influence of
deep learning on this task.

The machine learning models we use for classification

are the following:

(1) Decision Tree: a non-parametric supervised learning
technique that performs classification by learning de-
cision rules based on the data features. The model is
implemented using the scikit-learn11 Python library.
(2) Random Forest: a model that fits several decision tree
classifiers on different sub-samples of the data and

10https://scikit-learn.org/stable/modules/feature extraction.html

11https://scikit-learn.org/stable/modules/tree.html

5

averages their outputs. It is also implemented using
the scikit-learn12 Python library.

(3) Random Classifier: We implement our multiclass
multilabel random classifier by predicting vectors
where each entry has an equal probability of being
either zero or one. We, therefore, expect to have, on
average, 4.5 ones predicted per sample.

The deep learning model that we employ is a Long-Short
Term Memory (LSTM) network, which is an improvement
to the traditional Recurrent Neural Networks (RNNs). By
using the Gating mechanism, it addresses the short-term
memory challenge that RNNs have. Thus, LSTMs can pre-
serve long-term memory, which is an important aspect to
consider since we are dealing with a text classification task.
When generating the labels, it is essential to preserve the
long-term dependencies in the problem statements. In order
to obtain the final predicted labels we use a logistic activa-
tion function. The reason that logistic activation function
was used over softmax is that the latter is more commonly
used for multi-class classification problems where there is
only one target label. We implement the LSTM architectures
using PyTorch13, a Python-based open-source deep learning
library.

5 EXPERIMENTS
In this section, we provide an overview of the experimental
setup. More specifically, we discuss the training and test
sets, the evaluation metrics that we employ, as well as the
training setup and the model hyperparameters. Additionally,
we present the experimental results.

Training & Test sets
Being aware of the fact that we have little data available,
8,707 data points after pre-processing and duplicate removal,
we decide to perform a single split between training and test,
without creating a validation set. We perform a 90-10 split,
creating a training set consisting of 7826 training samples,
and 881 test samples. This decision is backed up by the fact
that (i) it is not straightforward to balance the sets since we
are in a multilabel setting (one problem can have zero, one, or
multiple tags) and (ii) our goal is not devising a state-of-the-
art architecture for the given problem, but rather verifying
the applicability of neural networks to tackle it.

We mitigate the effect of this decision by avoiding ex-
tensive hyperparameter tuning using the test set, which
would otherwise cause our model to overfit on the test set14.

12https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.
RandomForestClassifier.html
13https://pytorch.org/
14https://www.kdnuggets.com/2019/05/careful-looking-model-results-
cause-information-leakage.html

Therefore, the results we present later on in this section are
obtained on the test set: the models do not have access to
this data, but we use it to make decisions and tune a limited
number of hyperparameters.

Evaluation Metrics
We present here the evaluation metrics we adopt to evaluate
the different models.

Weighted Hamming Score. The standard (non-weighted)
Hamming Score is defined as ”1 − HamminдLoss”, where the
Hamming Loss represents the fraction of the wrong labels out
of the total number of labels. However, as mentioned before,
the datapoints have, on average, less than 2 tags (out of 9).
For this reason, we decide to weight the Hamming Score
metric by weighting differently the errors in predicting 1s or
0s. The implementation of our custom metric is as follows:

W eiдht ed H amminдScor e=1−W eiдht ed H amminдLoss

where

W eiдht ed H amminдLoss=W1∗Rat io Miscl1+W0∗Rat io Miscl0

and Ratio Miscli is the ratio of misclassified entries per label
i. W0 and W1 are set to 0.18 and 0.82, respectively, to account
for the label unbalance present in our dataset.

Average Precision, Recall, F1. We implement the average
precision, recall, and F1 score using the sklearn.metrics li-
brary 15 16 17. Our implementation weights the average ob-
tained per label based on its support.

Average Number of Ones per Sample. As an additional met-
ric, we keep track of the average number of ones predicted
per sample by the model. We do this mainly to make sure
our models are not predicting either all ones or all zeros, and
to account for the label unbalance of the dataset.

Training
As explained earlier in this section, we perform a 90-10 split
of our data, obtaining training and test set. The test set is
only used to perform early stopping (to halt the training if
the model does not improve on the test set for more than 10
epochs) and to tune a very limited number of hyperparame-
ters. To obtain a test set with distribution as similar as possi-
ble to the training set, we employ the iterative stratification

15https://scikit-learn.org/stable/modules/generated/sklearn.metrics.
average precision score.html
16https://scikit-learn.org/stable/modules/generated/sklearn.metrics.
recall score.html
17https://scikit-learn.org/stable/modules/generated/sklearn.metrics.
f1 score.html

6

Model
tf-idf + Random Forest
tf-idf + Dec. tree
Random classifier
one-hot + LSTM
Doc2Vec + FFNN
Word2Vec + LSTM

Weighted Hamming Score Avg Precision Avg Recall Avg F1
0.171
0.323
0.309
0.502
0.483
0.489

-
0.111
-
0.331
-
0.525
0.163
0.7
0.173
0.79
0.158
0.793
Table 2: Model results on test set

0.719
0.326
0.228
0.407
0.387
0.375

0.27
0.423
0.518
0.708
0.754
0.762

Loss Avg # of Ones per sample N. of trainable params

0.22
1.68
4.62
2.61
3.75
4

-
-
-
402,009
649
20,505

approach18, which aims to solve the problem of balancing
the split in the case of a multiclass multilabel setting.

We implement our models using the deep learning frame-
work Pytorch13 and using some of the already implemented
models of scikit19. All of our code can be found on GitHub20.
For the training of our architectures we employ binary
cross-entropy loss 21. However, similarly to the customiza-
tion performed on the Hamming Loss, we weight the binary
cross-entropy loss to account for the label unbalance.

Model hyperparameters
With regards to the tf-idf representation, the input dimen-
sionality is 6,259, which is equal to the number of unique
terms in the entire corpus. For both Decision Tree and
Random Forest we employ the default values of Sklearn,
and set the number of estimators for the Random Forest
classifier to 500.

Similarly, with one-hot encoding, the input dimensional-
ity equals the number of uniques terms in the corpus (6,259).
The best performing LSTM network for this data represen-
tation has 16 hidden units, followed by a fully connected
layer for the output classification (9 classes). We use zero-
padding on the input sequences to create batches and speed
up training. The learning rate is 0.01.

The Doc2Vec approach results in a smaller input repre-
sentation, with a dimensionality of 30. The architectures has
two fully connected layers, with 16 and 9 neurons respec-
tively. The non-linear activation function used is ReLu. The
learning rate is again 0.01.

The Word2Vec data representation results in an input
representation of 300. The LSTM has 16 hidden units and it
is followed by a fully connected layer that maps the hidden
state to an output of dimensionality 9, for the final classifica-
tion. Again, we employ zero-padding on the input sequences.
The learning rate is 0.005.

18http://scikit.ml/api/skmultilearn.model selection.iterative stratification.
html
19https://scikit-learn.org/stable/supervised learning.html
20https://github.com/serg-ml4se-2019/group11-tagging-algorithm-
problems
21https://pytorch.org/docs/stable/nn.html#bceloss

7

Results
Table 2 shows the performance of our models on the test
set. We want to highlight again that although this data has
been used for early stopping and limited tuning, the models
have never seen it during training. It can be seen that W2V +
LSTM achieves the best Weighted Hamming Score which is,
in general, higher for the neural network models compared
to the two baselines found in the first three rows of the table.

6 DISCUSSION & REFLECTION
In this section we discuss our study, the findings arising from
our experiments and the challenges faced while conducting
them.

To our knowledge, this is the first research work that
focuses on predicting tags based on the textual description
of programming problem statements. As a result, we cannot
compare the performance we obtain with any state-of-the-art
result.

When it comes to interpreting the results shown in Table
2, we consider as baselines the tf-idf with Decision Tree,
since it achieves better performance than Random Forest
in terms of the Weighted Hamming Score and the Random
Classifier. We can observe that the deep learning models
achieve significantly higher performance compared to our
baselines, performing better on all the considered metrics.
Regarding the data representation methods employed for the
deep learning models, Word2Vec outperforms both Doc2Vec
and one-hot encoding. This is a sensible result, as Word2Vec
preserves the semantics and syntax of the words, as opposed
to one-hot encoding. Additionally, it allows for the sequential
modeling of the words in the problem statements by employ-
ing an LSTM, as opposed to Doc2Vec which encodes the data
at a paragraph level. However, as we can observe in Table 2,
the number of trainable parameters is considerably higher
for the Word2Vec model than for the Doc2Vec one, while
only achieving a slight improvement in performance. Thus,
depending on the use case, Doc2Vec might be preferred over
Word2Vec. For instance, if a system needs to be retrained reg-
ularly to account for new data, using Doc2Vec instead might
be a reasonable choice. Nevertheless, both the Word2Vec
and Doc2Vec models have a substantially lower number of
trainable parameters compared to one-hot encoding.

Moreover, we notice that in the case of the W2V+LSTM
model, our best performing one in terms of Weighted Ham-
ming Score, the average predicted number of ones per sample
is quite high in comparison to the true labels (4 compared to
1.62). This could potentially be solved by adding a custom
regularization term to this model to force the number of
predicted ones to remain lower.

Additionally, during training, we have observed that our
deep learning models were overfitting on the training set
after a few epochs, even after experimenting with regular-
ization techniques such as drop-out and weight decay. Our
reasoning behind this issue is that we do not have enough
data since the problem that we aimed at solving serves a very
specific purpose. This observation is also apparent in our
baseline creation where more complex classifiers like Ran-
dom Forest perform worse than a simpler Decision Tree. For
an example of the overfitting problem, we refer the reader to
Appendix D, where we include a training curve showing an
overfitting behaviour (one-hot encoding + LSTM). That said,
we notice that there is an emerging trend in deep learning
literature that aims at solving problems that cannot be repre-
sented by large amounts of data, called small sample learning
[14]. In particular, models try to learn concepts rather than
patterns since a concept can generalize better, thus avoiding
overfitting. Such an approach could be investigated as future
work for this task.

7 CONCLUSION
In this work we investigated how deep learning techniques
perform in a novel multi-class multi-label text classification
problem. Our findings show that deep learning approaches
significantly outperform traditional widely accepted IR tech-
niques like tf-idf. Moreover, we experimented with different
combinations of text representations and neural network
architectures, finding Word2Vec + LSTM as the option that
yields the best performance. Finally, we were able to ex-
perience first hand the challenges and issues arising when
having a limited amount of data, an issue that is common in
the deep learning literature.

Regarding possible future research directions, the need to
collect more data for this task is of crucial importance. This
could be done by either gathering data from more program-
ming challenges websites or slightly different domains that
can provide similar data samples. Another way to get more
training data is by artificially augmenting the dataset either
by the use of synonyms or adversarial networks [4][20]. The
availability of more training data will then allow researchers
to use more advanced text embeddings such as BERT [1] and
XLNet [19] which are the current state-of-the-art in the NLP
field. Last but not least, different lines of work can also be

8

tried such as avoiding preprocessing the dataset and learning
character embeddings, similar to what was performed in [3].

REFERENCES
[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
Bert: Pre-training of deep bidirectional transformers for language
understanding. arXiv preprint arXiv:1810.04805, 2018.

[2] Andrea Esuli and Fabrizio Sebastiani. Active learning strategies for
multi-label text classification. In European Conference on Information
Retrieval, pages 102–113. Springer, 2009.

[3] Ben Gelman, Bryan Hoyle, Jessica Moore, Joshua Saxe, and David
Slater. A language-agnostic model for semantic source code labeling.
In Proceedings of the 1st International Workshop on Machine Learning
and Software Engineering in Symbiosis, pages 36–44. ACM, 2018.
[4] Rahul Gupta. Data augmentation for low resource sentiment analysis
In ICASSP 2019-2019 IEEE
using generative adversarial networks.
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 7380–7384. IEEE, 2019.

[5] Ke-Wei Huang and Zhuolun Li. A multilabel text classification algo-
rithm for labeling risk factors in sec form 10-k. ACM Transactions on
Management Information Systems (TMIS), 2(3):18, 2011.

[6] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. Multil-
abel text classification for automated tag suggestion. In Proceedings of
the ECML/PKDD, volume 18, page 5, 2008.

[7] Quoc Le and Tomas Mikolov. Distributed Representations of Sentences

and Documents. page 9.

[8] Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. Deep
learning for extreme multi-label text classification. In Proceedings of the
40th International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 115–124. ACM, 2017.

[9] Andrew McCallum. Multi-label text classification with a mixture
model trained by em. In AAAI workshop on Text Learning, pages 1–7,
1999.

[10] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient
Estimation of Word Representations in Vector Space. arXiv:1301.3781
[cs], January 2013. arXiv: 1301.3781.

[11] Jinseok Nam, Jungi Kim, Eneldo Loza Menc´ıa, Iryna Gurevych, and
Johannes F¨urnkranz. Large-scale multi-label text classificationfi!?revis-
iting neural networks. In Joint european conference on machine learning
and knowledge discovery in databases, pages 437–452. Springer, 2014.
[12] Juan Ramos et al. Using tf-idf to determine word relevance in doc-
ument queries. In Proceedings of the first instructional conference on
machine learning, volume 242, pages 133–142. Piscataway, NJ, 2003.
[13] Jesse Read, Bernhard Pfahringer, and Geoffrey Holmes. Multi-label
classification using ensembles of pruned sets. In 8th IEEE international
conference on data mining, pages 995–1000. IEEE, 2008.

[14] Jun Shu, Zongben Xu, and Deyu Meng. Small sample learning in big

data era. arXiv preprint arXiv:1808.04572, 2018.

[15] Catarina Silva and Bernardete Ribeiro. The importance of stop word
removal on recall values in text categorization. In Proceedings of the
International Joint Conference on Neural Networks, 2003., volume 3,
pages 1661–1666. IEEE, 2003.

[16] Konstantinos Trohidis, Grigorios Tsoumakas, George Kalliris, and
Ioannis P Vlahavas. Multi-label classification of music into emotions.
In ISMIR, volume 8, pages 325–330, 2008.

[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word Representa-
tions: A Simple and General Method for Semi-supervised Learning.
In Proceedings of the 48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA,
2010. Association for Computational Linguistics. event-place: Uppsala,
Sweden.

APPENDIX

A. DATASET DESCRIPTIVE STATISTICS

[18] Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng Chen. Effective
multi-label active learning for text classification. In Proceedings of the
15th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 917–926. ACM, 2009.

[19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pre-
training for language understanding. arXiv preprint arXiv:1906.08237,
2019.

[20] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence
generative adversarial nets with policy gradient. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.

[21] Min-Ling Zhang and Zhi-Hua Zhou. A k-nearest neighbor based
algorithm for multi-label classification. GrC, 5:718–721, 2005.
[22] Min-Ling Zhang and Zhi-Hua Zhou. Ml-knn: A lazy learning approach

to multi-label learning. Pattern recognition, 40(7):2038–2048, 2007.

Figure 5: Average number of words per tag for the Code-
Forces data

Figure 6: Average number of words per tag for the Topcoder
data

B. LIST OF INITIAL 17 TAGS
After the initial aggregation of the tags present in the two
datasets, scraped from Topcoder and CodeForces, the 17
resulting problem statements tags are the following:

(1) Dynamic Programming
(2) Greedy
(3) Sorting
(4) Recursion
(5) Graphs
(6) String Operations

9

(7) Search and Binary Search
(8) Constructive Algorithms
(9) Math and Probabilities

Figure 8 shows the aggregation that was performed on
the initial 17 common tags in order to reduce them to the
final 9 tags.

D. OVERFITTING PROBLEM

Figure 9: Overfitting example: One-hot encoding + LSTM.

Figure 7: Average number of words per tag for the full com-
bined data

(7) Data Structures
(8) Divide and Conquer
(9) Geometry
(10) Bit Manipulation
(11) Brute Force
(12) Binary Search
(13) Search
(14) Game Theory
(15) Constructive Algorithms
(16) Math
(17) Probabilities and Combinatorics

C. LIST OF FINAL 9 TAGS
After the final aggregation of the initial 17 tags, the 9 final
problem statement tags are the following:

Figure 8: Final taxonomy

(1) Dynamic Programming
(2) Greedy and Sorting
(3) Data Structures and Graphs
(4) String Operations
(5) Geometry
(6) Brute Force

10

