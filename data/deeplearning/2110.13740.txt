1
2
0
2

t
c
O
6
2

]

V
C
.
s
c
[

1
v
0
4
7
3
1
.
0
1
1
2
:
v
i
X
r
a

DP-SSL: Towards Robust Semi-supervised Learning
with A Few Labeled Samples

Yi Xu1

Jiandong Ding2 Lu Zhang1 Shuigeng Zhou 1∗

1Shanghai Key Lab of Intelligent Information Processing,
and School of Computer Science, Fudan University, China
2Alibaba Group
{yxu17, jdding, l_zhang19, sgzhou}@fudan.edu.cn

Abstract

The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised
learning (SSL) provides a promising way to leverage unlabeled data by pseudo
labels. However, when the size of labeled data is very small (say a few labeled
samples per class), SSL performs poorly and unstably, possibly due to the low
quality of learned pseudo labels. In this paper, we propose a new SSL method called
DP-SSL that adopts an innovative data programming (DP) scheme to generate
probabilistic labels for unlabeled data. Different from existing DP methods that
rely on human experts to provide initial labeling functions (LFs), we develop a
multiple-choice learning (MCL) based approach to automatically generate LFs
from scratch in SSL style. With the noisy labels produced by the LFs, we design
a label model to resolve the conﬂict and overlap among the noisy labels, and
ﬁnally infer probabilistic labels for unlabeled samples. Extensive experiments
on four standard SSL benchmarks show that DP-SSL can provide reliable labels
for unlabeled data and achieve better classiﬁcation performance on test sets than
existing SSL methods, especially when only a small number of labeled samples
are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL
achieves 93.82% annotation accuracy on unlabeled data and 93.46% classiﬁcation
accuracy on test data, which are higher than the SOTA results.

1

Introduction

The de-facto approaches to deep learning achieve phenomenal success with the release of huge labeled
datasets. However, large manually-labeled datasets are time-consuming and expensive to acquire,
especially when expert labelers are required. Nowadays, many techniques are proposed to alleviate
the burden of manual labeling and help to train models from scratch, such as active learning [1],
crowd-labeling [2], distant supervision [3], semi [4]/weak [5]/self-supervision [6]. Among them,
semi-supervised learning (SSL) is one of the most popular techniques to cope with the scarcity of
labeled data. Two major strategies of SSL are pseudo labels [7] and consistency regularization [8].
Pseudo labels (also called self-training [9]) utilize a model’s predictions as the labels to train the
model again, while consistency of regularization forces a model to make the same prediction under
different transformations. However, when the size of labeled data is small, SSL performance degrades
drastically in both accuracy and robustness. Fig. 1 shows the change of prediction error rate with
the number of labeled samples of CIFAR-10. When the number of labeled samples reduces from
250 to 40, error rates of major existing SSL methods increase from 4.74% (USADTM) to 36.49%
(MixMatch). One possible reason of performance deterioration is the quality degradation of learnt
pseudo labels when labeled data size is small. Therefore, in this paper we address this problem by

∗Corresponding author.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
developing sophisticated labeling techniques for unlabeled data to boost SSL even when the number
of labeled samples is very small (e.g. a few labeled samples per class).

Recently, data programming (DP) was proposed as a new
paradigm of weak supervision [10]. In DP, human experts
are required to transform the decision-making process into
a series of small functions (called labeling functions, abbre-
viated as LFs), thus data can be labeled programmatically.
Besides, a label model is applied to determining the correct
labels based on consensus from the noisy and conﬂicting
labels assigned by the LFs. Such a paradigm achieves
considerable success in NLP tasks [11–14]. In addition,
DP has also been applied to computer vision tasks [15, 16].
However, current DP methods require human experts to
provide initial LFs, which is time-consuming and expen-
sive, and it is not easy to guarantee the quality of LFs.
Furthermore, LFs speciﬁcally deﬁned for one task usually
cannot be re-used for other tasks.

Figure 1: Error rate vs. #labeled samples
(CIFAR-10). Results of existing meth-
ods are from the original papers. When
only 40 labeled samples are given, all
existing SSL methods are substantially
degraded and more unstably, while our
method is still effective and robust.

In this paper, we propose a new SSL method called DP-
SSL that is effective and robust even when the number
of labeled samples is very small. In DP-SSL, an inno-
vative data programming (DP) scheme is developed to
generate probabilistic labels for unlabeled data. Different
from existing DP methods, we develop a multiple-choice
learning (MCL) based approach to automatically generate
LFs from scratch in SSL style. To remedy the over-conﬁdence problem with existing MCL methods,
we assign an additional option as abstention for each LF. After that, we design a label model to
resolve the conﬂict and overlap among the noisy labels generated by LFs, and infer a probabilistic
label for each unlabeled sample. Finally, the probabilistic labels are used to train the end model for
classifying unlabeled data. Our experiments validate the effectiveness and advantage of DP-SSL. As
shown in Fig. 1, DP-SSL performs best, and only 1.76% increase of error rate when the number of
labeled samples decreases from 250 to 40 in CIFAR-10.

Note that the pseudo labels used in existing SSL methods are quite different from the probabilistic
labels in DP-SSL, which may explain the advantage of DP-SSL over existing SSL methods. On the
one hand, pseudo labels are “hard” labels that indicate an unlabeled sample belonging to a certain
class or not, while probabilistic labels are “soft” labels that indicate the class distributions of unlabeled
samples. Obviously, the latter should be more ﬂexible and robust. On the other hand, pseudo labels
are actually generated by a single model for all unlabeled samples, while probabilistic labels are
generated from a number of diverse and specialized LFs (due to the MCL mechanism), which makes
the latter more powerful in generalization as a whole.

In summary, the contributions of this paper are as follows: 1) We propose a new SSL method DP-SSL
that employs an innovative data programming method to generate probabilistic labels for unlabeled
data, which makes DP-SSL effective and robust even when there are only a few labeled samples per
class. 2) We develop a multiple choice learning based approach to automatically generate diverse and
specialized LFs from scratch for unlabeled data in SSL manner. 3) We design a label model with a
novel potential and an unsupervised quality guidance regularizer to infer probabilistic labels from the
noisy labels generated by LFs. 4) We conduct extensive experiments on four standard benchmarks,
which show that DP-SSL outperforms the state-of-the-art methods, especially when only a small
number of labeled samples are available, DP-SSL is still effective and robust.

2 Related Work

Here we brieﬂy review the latest advances in multiple choice learning, semi-supervised learning, and
data programming, which are related to our work. Detailed information is available in [17–21].

2

402504000Number of labeled samples0102030405060Error rateMixMatchUDAReMixMatchFixMatch(RA)FixMatch(CTA)USADTMDP-SSL (Ours)2.1 Multiple Choice Learning

Multiple choice learning (MCL) [22] was proposed to overcome the low diversity problem of models
trained independently in ensemble learning. For example, stochastic multiple choice learning [23] is
for training diverse deep ensemble models. However, a crucial problem with MCL is that each model
tends to be overconﬁdent, which results in poor ﬁnal prediction. To solve this problem, [24] forces
the predictions of non-specialized models to meet a uniform distribution, so that the ﬁnal decision is
summed over diverse outputs. [25] proposes an additional network to estimate the weight of each
specialist’s output. In this paper, we develop an improved MCL based scheme to automatically
generate diverse and specialized labeling functions (LFs) from scratch in an SSL manner. These LFs
are used to generate preliminary (usually noisy) labels for unlabeled data.

2.2 Semi-supervised Learning

Semi-supervised learning (SSL) has been extensively studied in image classiﬁcation [26], object
detection [27], and semantic segmentation [28]. Two popular SSL strategies for image classiﬁcation
are pseudo labels [7] and consistency regularization [8]. Pseudo-label methods generate artiﬁcial
labels for some unlabeled images and then train the model with these artiﬁcial labels, while consistency
regularization tries to obtain an artiﬁcial distribution/label and applied it as a supervision signal with
other augmentations/views. These two strategies have been adopted by a number of recent SSL
works [4, 8, 29–39]. For example, FixMatch [4] proposes a simple combination of pseudo labels
and consistency regularization. [36] employs unsupervised learning and clustering to determine the
pseudo labels. In this paper, we propose a new SSL method that is effective and robust even when
the size of labeled data is very small. Our method employs an innovative data programming alike
method to automatically generate probabilistic labels for unlabeled data.

2.3 Data Programming

Data programming [10] is a weak supervision paradigm proposed to infer correct labels based on
the consensus among noisy labels from labeling functions (LFs), which are modules embedded with
decision-making processes for generating labels programmatically. Following the DP paradigm,
Snorkel [12] and Snuba [40] were proposed as rapid training data creation systems. Their LFs are
built with various weak supervision sources, like pattern regexes, heuristics, and external knowledge
base etc. Recently, more works are reported in the literature
[11, 13–16, 21, 41–48]. Among
them, [11, 13, 14, 45–47] focus on the adaption of label model in DP. For example, [21] aims to
reduce the computational cost and proposes a closed-formed solution for training the label model.
[15, 16, 41–43] apply DP to computer vision. Concretely, [16, 42, 43] heavily rely on the pretrained
models. [41] combines crowdsourcing, data augmentation, and DP to create weak labels for image
classiﬁcation. [15] presents a novel view for resolving infrequent data in scene graph prediction
training datasets via image-agnostic features in LFs. However, all these methods cannot be directly
applied to training models from scratch with a small number of labeled samples. Thus, in this paper
we extend DP by exploring both MCL and SSL to generate arbitrary labeling functions.

3 Method

For a C-class SSL classiﬁcation problem, assume that all training data X are divided into labeled data
Xl and unlabeled Xu, and test data are denoted as Xt. Following the notation in [4, 36], {xl, xw
l } ∈
Xl are the paired labeled samples with labels yl ∈ {1, . . . , C}, and {xu, xw
u} ∈ Xu are the triple
unlabeled samples. Here, xl and xu represent the raw images without any transformations. xw
l , xw
u ,
and xs
u are the images based on the weak and strong augmentation strategies, respectively. In this
paper, weak augmentation uses a standard ﬂip-and-shift strategy, and strong augmentation uses the
RandAugment [49] strategy with Cutout [50] augmentation operation.

u , xs

3.1 Background

In the FixMatch [4] algorithm, apart from basic cross-entropy on labeled samples,

FixMatch.
consistency regularization with pseudo labels on unlabeled samples is represented as:
u) = 1(max(p(y|xw

u )) ≥ (cid:15)) · H(ˆyw

u , p(y|xs

LF M (xw

u , xs

u)),

(1)

3

Figure 2: Framework of the DP-SSL method with four LFs.

where H is the cross-entropy, τ is the pre-deﬁned threshold, p represents the output probability of the
model, and ˆyw
u )) is the pseudo label from the weakly augmented predictions.
Multiple Choice Learning. Stochastic Multiple Choice Learning (sMCL) [23] aims to specialize
each individual model on a subset of data, by minimizing the loss as follows:

u := arg max(p(y|xw

LsM CL(xl, yl) = min

k∈{1,··· ,K}

H(yl, pk(y|xl)),

(2)

where pk is the output probability of the k-th model. For a training sample (xl, yl), sMCL feeds the
data to all K models but only chooses the most accurate model to do back-propagation. Consequently,
each model performs better on some classes than the other models, i.e., each model becomes a
specialist on some particular classes.

3.2 Framework

Fig. 2 shows the framework of our DP-SSL method, which works in three major steps as follows:

• Step 1. We employ an MCL based approach to automatically generate K LFs from scratch
in an SSL style. Here, each LF is trained on a subset of C classes in the training set based on
MCL. As shown in Fig 2, the 2nd LF is trained with samples of classes “horse” and “dog”,
and abstains from predicting when facing monkey images.

• Step 2. A graphical model is developed as the label model to aggregate the noisy labels and
produce probabilistic labels for unlabeled training data. The label model is learned in an
SSL manner with an additional regularizer.

• Step 3. The end model is trained with both provided labels and probabilistic labels generated

from Step 2. Finally, we verify the performance of the end model on the test data.

3.3 Labeling Function

In Step 1 of our method, LFs are exploited to generate noisy labels for each unlabeled image. In
previous DP works for computer vision, LFs are built via external image-agnostic knowledge [15]
or pretrained models [16, 42, 43]. However, it is difﬁcult to explicitly describe the rules of image
classiﬁcation. Instead, here we innovatively explore MCL and SSL for automatic LF generation.

As shown in Fig. 2, we share the same backbone (Wide ResNet [51] in this paper) to extract features
of images for multiple prediction heads (called LFs in this paper). To promote the diversity of LFs,
we transform the features and feed each LF with different transformed features as follows:

fk =

HW
(cid:88)

j=1

e−βkdis(f [j],ck)
k(cid:48)=1 e−βk(cid:48) dis(f [j],ck(cid:48) )

(cid:80)K

(f [j] − ck).

(3)

In this paper, K is the number of LFs, f ∈ RHW ×D denotes the feature map of input image x before
global average pooling of backbone, f [j] ∈ RD is the feature vector at the spatial position j of f .
ck ∈ RD is the learnable clustering center of the k-th LF, βk is the learnable variable of the k-th
cluster, dis(A, B) represents the distance between A and B. Thus, fk corresponds to the feature fed

4

to the k-th LF, and describes the k-th aggregated pattern of f among the K centers, it can also be
considered as a learnable weighted spatial pooling for feature f . Then, supposing Fk is the classiﬁer
in the k-th LF, which would output the probability Fk(fk) as prediction. For clarity, in the following
we denote pk(y|x) := Fk(fk) of the k-th LF with the input image x.

As depicted in [23], the classiﬁers lack diversity of prediction even trained with different protocols.
Therefore, we adopt MCL to assign a subset of labeled data for each classiﬁer automatically to
improve diversity. However, it is intuitive to observe that in Eq. (2) each category can only be
assigned to one LF, and no consensus can be exploited. Therefore, we increase the proportion of
selected models in Eq. (2) to do back-propagation, which is formulated as

LM CL
l

(xw

l , yl) =

min

M⊂{H(yl,pk(y|xw

l )}K

k=1

|M|=ρ·K

1
ρ · K

ρ·K
(cid:88)

k(cid:48)=1

Mk(cid:48),

(4)

where Mk(cid:48) indicates the k(cid:48)-th element in the set M, and ρ ∈ [1/K, 1] is a designed parameter to
represent the ratio of specialist LFs. When (cid:98)ρ · K(cid:99) is equal to 1, Eq. (4) becomes the traditional MCL
in Eq. (2). In contrast, if (cid:98)ρ · K(cid:99) is equal to K, it deteriorates to the basic ensemble learning, where
all K classiﬁers are trained with the same data.

Based on MCL, each LF is a specialist for some classes, so it can get high accuracy for samples
in these classes. While for samples from other classes not specialized by the LF, it fails to predict
due to over-conﬁdence. Thus, we take only the probabilities of specialized categories as predictions,
and allow each LF to abstain from some samples in the dataset. Formally, we denote ‘0’ as the
abstention label, and the specialized category set of the k-th LF as τk = {τ 1
}. Then, the
output label of the k-th LF ˆyk satisﬁes ˆyk ∈ {0} ∪ τk, e.g., the output of the 1st LF in Fig. 2 is among
“monkey”, “deer” and “abstention” because its specialized category set τ1 = {monkey, deer}. Then,
we denote the probability over the specialized category set τk and “abstention” option as ¯pk(y|x),
where ¯pk(y|x) ∈ R|τk|+1. The objective function over labeled samples with abstention option is

k , . . . , τ |τk|

k

Ll(xw

l , yl) =

K
(cid:88)

k=1

(cid:0)1(yl ∈ τk)H(yl, ¯pk(y|xw

l )) + 1(yl /∈ τk)H(0, ¯pk(y|xw

l ))(cid:1),

(5)

Then, for the unlabeled training data, we follow the settings in FixMatch [4], where unlabeled data
are supervised by the pseudo labels ˆyw,k

of weak augmentation data xw

u . Thus,

u

Lu(xw

u , xs

u) =

K
(cid:88)

k=1

1(max(¯pk(y|xw

u )) ≥ (cid:15))

(cid:16)

1(ˆyw,k

u ∈ τk)H(ˆyw,k

u , ¯pk(y|xs

u))+

(6)

1(ˆyw,k
u

/∈ τk)H(0, ¯pk(y|xw

u ))

(cid:17)

,

u

:= arg max(¯pk(y|xw

where ˆyw,k
u )). Speciﬁcally, we only keep samples whose largest probability
(including the abstention option) is above the predeﬁned threshold (cid:15) (0.95 in our paper), and train the
model on the kept data with pseudo label ˆyw,k
u . Accordingly, the training in this step is to minimize
the objective function as follows:

L(xw

l , yl, xw

u , xs

u) = µM CL
l

LM CL
l

(xw

l , yl) + µlLl(xw

l , yl) + µuLu(xw

u , xs

u),

(7)

l

, µl and µu are hyper-parameters. In our implementation, we ﬁrst set µM CL

where µM CL
µl = µu = 0, then adjust µM CL
Generally, in Step 1, MCL is expected to generate specialized class sets τ for LFs, with which samples
are more easily discriminated by SSL classiﬁers even there are a few labeled samples. Besides, the
abstention option is for addressing the over-conﬁdence problem of samples from non-specialized sets.

l
to 0 and µl = µu = 1 after the convergence of LM CL

= 1 and

.

l

l

3.4 Label Model

In Step 2 of our method, we utilize a graphical model to specify a single prediction by integrating
noisy labels provided by K LFs. For simpliﬁcation, we assume that the K LFs are independent (as
shown in Fig. 2). Then, suppose that ˆy = (ˆy1, · · · , ˆyK)(cid:124) ∈ RK is the vectorized form of the

5

predictions from K LFs, the joint distribution of the label model can be described as:

P (y, ˆy) =

1
Z

K
(cid:89)

k=1

φ(y, ˆyk)

(8)

where Z is the normalizer of the joint distribution, φ is the potential that couples the target y and noisy
label ˆyk. In this paper, we extend the dimension of parameters θ in label model to K × C to support
multi-class classiﬁcation. Set eky := exp(θky), which is the exponent of parameters θky. Now we
are to construct the potential function φ. Due to the specialized LFs, the potential φ should beneﬁt
the ﬁnal prediction when a noisy label agrees with the target. That is, we should have φ(y, ˆyk) > 1.
Thus, we set φ as 1 + eky for this case. On the contrary, the potential φ should negatively impact the
ﬁnal prediction when a noisy label conﬂicts with the target label in the specialized category set, i.e.,
we should have φ(y, ˆyk) < 1. Therefore, for this case we set φ to 1/(1 + eky). For the other cases,
we follow the design in [52]. In summary, the potential φ is deﬁned as follows:
if y ∈ τk, ˆyk ∈ τk, ˆyk = y
if y ∈ τk, ˆyk ∈ τk, ˆyk (cid:54)= y
if y /∈ τk, ˆyk ∈ τk, ˆyk (cid:54)= y
otherwise

1 + eky,
1/(1 + eky),
eky,
1.

φ(y, ˆyk) =





(9)

With the potential above, the normalizer Z of the joint distribution in Eq. (8) can be obtained by
summarizing over y and ˆyk:

(cid:88)

K
(cid:89)

(cid:88)

Z =

φ(y, ˆyk)

y∈Y

k=1

ˆyk∈{0}∪τk

(cid:88)

K
(cid:89)

(cid:16)

=

y∈Y

k=1

1(y ∈ τk)(2 + eky +

|τk| − 1
1 + eky

(cid:17)
) + 1(y /∈ τk)(1 + |τk|eky)

.

(10)

Then, the objective function of the label model can be expressed in an SSL manner as follows:

L(ˆyl, yl, ˆyu) =

(cid:88)

xl
(cid:124)

H(yl, P (y, ˆyl))

+ (−

(cid:88)

log

(cid:88)

P (y, ˆyu))

+R(θ, ˆyu),

(11)

(cid:123)(cid:122)
labeled samples

(cid:125)

(cid:124)

xu

y∈Y
(cid:123)(cid:122)
unlabeled samples

(cid:125)

where the ﬁrst part is the cross-entropy loss, the second is the negative log marginal likelihood on the
observed noisy labels ˆyu, and the third is a regularizer. In our method, the regularizer is utilized to
guide the label model with statistical information (the accuracy of each LF). However, the accuracy
of each LF on noisy labels is unavailable, while the accuracy on labeled training is almost 100% due
to over-ﬁtting. Thus, we have to estimate the accuracy of each LF with the observable noisy labels ˆy,
which will be presented in Sec. 3.5. After training, the label model produces probabilistic labels π by
computing the joint distribution in Eq. (8) with the noisy labels ˆy.

3.5 Accuracy Estimation

Now, we formally describe our method for estimating the accuracy of LFs. We transform the multi-
class problem into C one-versus-all tasks. For the i-th (i ∈ [1, · · · , C]) one-versus-all task, we denote
the unobserved ground-truth labels as zi ∈ {±1} (zi = +1 means y = i, and zi = −1 represents
y (cid:54)= i), noisy labels of the k-th LF as ˆzk

i ∈ {±1, 0},



1
0
−1



ˆzk
i =

if ˆyk = i,
if ˆyk = 0,
otherwise.

Then, we can write E[ˆzk
E[ˆzk

i zi] as
i zi] = P (ˆzk
= P (ˆzk
= 2P (ˆzk

i zi = −1)

i zi = 1) − P (ˆzk
i zi = 1) − (1 − P (ˆzk
i = zi) + P (ˆzk

i = 0) − 1.

i zi = 1) − P (ˆzk

i zi = 0))

6

(12)

(13)

Assume that ˆzj

i ⊥⊥ ˆzk

i |zi for distinct j and k, then
i z2
i ˆzk
i ˆzk
i ] = 1
|xu|

i ] = E[ˆzj
i ˆzk

E[ˆzj

i zi]E[ˆzk
i = 1. In Eq. (14), ˆE[ˆzj
ˆzj
i ˆzk
with the fact that z2
xu
from the noisy labels of the j-th and k-th LFs, while E[ˆzj
i zi] and E[ˆzk
true label zi is unavailable. Next, we introduce a third labeling result from the l-th LF as ˆzl
ˆE[ˆzj
i] and ˆE[ˆzk
i ˆzl
method as follows:

i is observable, which can be derived
i zi] remain to be solved due to
i, such that
izi]| can be solved by a triplet

i] are observable. Then, |ˆE[ˆzj

i ] = E[ˆzj
(cid:80)

i zi]|, |ˆE[ˆzk

i zi]|, |ˆE[ˆzl

i zi]

(14)

i ˆzl

(cid:113)

(cid:113)

(cid:113)

|ˆE[ˆzj

i zi]| =

|ˆE[ˆzk

i zi]| =

|ˆE[ˆzl

izi]| =

|ˆE[ˆzj

i ˆzk

i ] · ˆE[ˆzj

i ˆzl

i]/ˆE[ˆzk

i ˆzl

i]|,

|ˆE[ˆzj

i ˆzk

i ] · ˆE[ˆzk

i ˆzl

i]/ˆE[ˆzj

i ˆzl

i]|,

|ˆE[ˆzj

i ˆzl

i] · ˆE[ˆzk

i ˆzl

i]/ˆE[ˆzj

i ˆzk

i ]|.

(15)

We can obtain the estimated accuracy of each LF by resolving the sign of E[ˆzk
ˆP (ˆzk
regularizer of R(θ, ˆyu) can be formulated as

i :=
(cid:54)= 0) be the estimated accuracy of the k-th LF on the i-th category. Therefore, the

i zi] [21]. Let ˆak

i = zi|ˆzk
i

R(θ, ˆyu) =

C
(cid:88)

K
(cid:88)

i log Pθ(ˆzk
ˆak

i = zi|ˆzk

i (cid:54)= 0) + (1 − ˆak

i ) log(1 − Pθ(ˆzk

i = zi|ˆzk

i (cid:54)= 0))

(16)

i=1
k
i = zi|ˆzk
i

where Pθ(ˆzk
variables in the model in Eq. (8). Details of Pθ can be referred to Appendix A.3.

(cid:54)= 0) can be computed in closed form by marginalizing over all the other

3.6 End Model

In Step 3, probabilistic labels are used to train an end model under any network architecture. We
utilize noise-aware empirical risk expectation as the objective function to take annotation errors into
account. Accordingly, the ﬁnal objective function is as follows:

L(xl, yl, xu, π) =

(cid:88)

xl
(cid:124)

H(yl, p(y|xl))

+

(cid:88)

Ey∼πH(y, p(y|xu))

(17)

(cid:123)(cid:122)
labeled samples

(cid:125)

xu
(cid:124)

(cid:123)(cid:122)
unlabeled samples with probabilistic label

(cid:125)

where p(y|xl) and p(y|xu) are the predicted distributions of xl and xu, π is the distribution produced
by the label model in Sec. 3.4.

4 Experiments

4.1

Implementation Details

In the training phase, we follow the settings of previous works [4, 35, 36], augment data in weak (a
standard ﬂip-and-shift strategy) and strong forms (RandAugment [49] followed by Cutout [50]
operation), and utilize a Wide ResNet as the end model for a fair comparison. In our framework,
the batch size for labeled data and unlabeled data is set to 64 and 448, respectively. Besides, we use
the same hyperparameters (K = 50, ρ = 0.2, (cid:15) = 0.95) for all datasets. We compare DP-SSL with
major existing methods on CIFAR-10 [53], CIFAR-100 [53], SVHN [54] and STL-10 [55]. We also
analyze the effect of annotation and conduct ablation study in Sec. 4.4 and Sec. 4.5 respectively. All
experiments are implemented in Pytorch v1.7 and conducted on 16 NVIDIA RTX3090s.

4.2 Datasets

CIFAR-10 and CIFAR-100 [53] contain 50,000 training examples and 10,000 validation examples.
All images are of 32x32 pixel size and fall in 10 or 100 classes, respectively.

SVHN [54] is a digital image dataset that consists of 73,257, 26,032 and 531,131 samples in the
train, test, and extra folders. It has the same image resolution and category number as CIFAR-10.

7

Table 1: Results of error rate on CIFAR-10, CIFAR-100 and SVHN for different existing SSL methods
(Π-Model [29], Pseudo-Labeling [7], Mean Teacher [32], MixMatch [31], UDA [34], ReMixMatch
[35], FixMatch [4] and USADTM [36]) and our DP-SSL method.

CIFAR-10

CIFAR-100

SVHN

Method

40 labels

250 labels 4000 labels

400 labels 2500 labels 10000 labels

40 labels

250 labels 1000 labels

Π -Model
Pseudo-Labeling
Mean Teacher
MixMatch
UDA
ReMixMatch
USADTM

- 54.26±3.97 14.01±0.38
- 49.78±0.43 16.09±0.28
- 32.32±2.30 9.19±0.19

- 18.96±1.92 7.54±0.36
- 20.21±1.09 9.94±0.61
- 3.57±0.11 3.42±0.07
47.54±11.50 11.05±0.86 6.42±0.10 67.61±1.32 39.94±0.37 28.31±0.33 42.55±14.53 3.98±0.23 3.50±0.28
29.05±5.93 8.82±1.08 4.88±0.18 59.28±0.88 33.13±0.22 24.50±0.25 52.63±20.51 5.69±2.76 2.46±0.24
3.34±0.20 2.92±0.48 2.65±0.08
19.10±9.64 5.44±0.05 4.72±0.13 44.28±2.06 27.43±0.31 23.03±0.56
3.01±1.97 2.11±0.65 1.96±0.05
9.54±1.04 4.80±0.32 4.40±0.15 43.36±1.89 28.11±0.21 21.35±0.17

- 57.25±0.48 37.88±0.11
- 57.38±0.46 36.21±0.19
- 53.91±0.57 35.83±0.24

FixMatch (RA)
FixMatch (CTA)
DP-SSL (ours)

13.81±3.37 5.07±0.65 4.26±0.05 48.85±1.75 28.29±0.11 22.60±0.12
11.39±3.35 5.07±0.33 4.31±0.15 49.95±3.01 28.64±0.24 23.18±0.11
6.54±0.98 4.78±0.26 4.23±0.20 43.17±1.29 28.00±0.79 22.24±0.31

3.96±2.17 2.48±0.38 2.28±0.11
7.65±7.65 2.64±0.64 2.36±0.19
2.98±0.86 2.16±0.36 1.99±0.18

Fully Supervised

2.74

16.84

1.48

Table 2: Results of error rate on STL-10.
STL-10

Method

1000 labels Method

1000 labels Method

40 labels

250 labels

1000 labels

Π -Model
Pseudo-Labeling
Mean Teacher
MixMatch

26.23±0.82
27.99±0.80
21.43±2.39
10.41±0.61

UDA
ReMixMatch
FixMatch (RA)
FixMatch (CTA)

7.66±0.56
5.23±0.45
7.98±1.50
5.17±0.63

USADTM
DP-SSL (ours)
Fully Supervised

9.63±1.35
9.32±0.91

6.85±1.09
6.83±0.71
1.48

4.01±0.59
4.97±0.42

STL-10 [55] is a dataset for evaluating unsupervised and semi-supervised learning. It consists of
5000 labeled images and 8000 validation samples of 96x96 size from 10 classes. Besides, there are
100,000 unlabeled images available, including odd samples.

4.3 Comparison with Existing SSL Methods

For a fair comparison, we conduct experiments with the codebase of FixMatch and cite the results on
CIFAR-10, CIFAR-100, SVHN and STL-10 from [4, 36]. We utilize the same network architecture
(a Wide ResNet-28-2 for CIFAR-10 and SVHN, WRN-28-8 for CIFAR-100, and WRN-37-2 for
STL-10) and training protocol of FixMatch, such as optimizer and learning rate schedule. Unlabeled
data are generated by the scripts in FixMatch. Results of DP-SSL and existing methods in Tab. 1 and
Tab. 2 are presented with the mean and standard deviation (STD) of accuracy on 5 pre-deﬁned folds.

As shown in Tab. 1, our method achieves the best performance in most cases, especially when
there are only 4 labeled samples per class. Speciﬁcally, our method achieves a 93.46% accuracy on
CIFAR-10 with 4 labeled samples per category, which is 3.3% higher than that of USADTM — the
state-of-the-art method. Again on STL-10, our method surpasses USADTM and achieves the best
performance when there are 4 and 25 labeled samples per class.

i zi] is close to 0 or sign recovery of E[ˆzk

On CIFAR-100, our method performs the best for 400 labels case and the 2nd for 2500 and 10,000
labels cases. We also notice that DP-SSL has relatively large STDs for 2500 and 10,000 labels
cases, which is due to the coarse accuracy estimation. In fact, even if triplet mean is adopted in
estimation, the triplet selection in Eq. (15) still impacts accuracy estimation and regularizer a lot,
especially when E[ˆzk
i zi] is wrong. Actually, there are some
advanced approaches to unsupervised accuracy estimation [56–58] that can replace the naive triplet
mean estimation. Ideally, if we can obtain the exact accuracy of each class ˆbk
i = zi|ˆzk
i = 1)
and regularize it as R(θ, ˆyu) = (cid:80)C
ˆbk
i ) log(1 − Pθ(ˆzk
i log Pθ(ˆzk
i =
zi|ˆzk
i = 1)), we will get an end model with (27.92 ± 0.23)% error rate for 2500 labeled samples.
Comparing with USADTM, our method does not perform well enough when more labeled data
available. For USADTM, apart from the proxy label generator, unsupervised representation learning
contributes a lot for its performance. As shown in the ablation study of [36], USADTM without
unsupervised representation learning achieves around 5.73% and 4.99% error rate for 250 and 4000
labeled samples in CIFAR-10, while our method DP-SSL obtains 4.78% and 4.23% error rate.

i = 1) + (1 − ˆbk

i := ˆP (ˆzk

i = zi|ˆzk

(cid:80)K
k

i=1

8

Table 3: The macro Precision/Recall/F1 Score/Coverage of the annotated labels on CIFAR-10,
CIFAR-100, and SVHN for our method and two typical existing label models.

CIFAR-10

CIFAR-100

SVHN

Method

Metrics

40 labels 250 labels 4000 labels 400 labels 2500 labels 10000 labels 40 labels 250 labels 1000 labels

F1 Score
Majority Vote
FlyingSquid[21] F1 Score

DP-SSL (ours)

Precision
Recall
F1 Score
Coverage

85.96
90.25

93.47
93.82
93.61
99.35

94.23
94.99

95.30
95.33
95.19
99.79

95.77
95.85

95.89
95.91
95.90
99.91

49.97
48.90

55.62
56.86
54.42
99.33

69.81
69.73

71.91
72.01
71.89
99.87

76.03
74.12

75.12
78.35
76.36
99.94

90.86
93.92

95.20
96.78
95.95
99.15

95.38
97.24

97.65
97.64
97.59
99.67

96.14
97.70

97.79
97.94
97.81
99.93

4.4 Analysis

Annotation performance. Intuitively, the holistic performance of the end model in our method
highly depends on the quality of annotation results. Thus, we present the macro precision/recall/F1
score and coverage of the annotated labels of our method on CIFAR-10, CIFAR-100, and SVHN
in Tab. 3. We can see that our method achieves over 99% coverage, which means that it produces
probabilistic labels for almost all unlabeled data. Comparing to the results in [36], the label model
with 40 labeled samples outperforms the proxy label generator, FixMatch and USADTM get 88.51%
and 89.48% accuracy, respectively. Furthermore, our method achieves 97.36% accuracy for unlabeled
data with the top-500 highest probabilities in each category. Meanwhile, we also present results of
Majority Voting and FlyingSquid [21] in Tab. 3 based on the noisy labels from Step 1 of our method
for comparison. Majority Voting gets bad performance because the number of LFs triggered for
different categories is not equal. For FlyingSquid, we implement it with C one-versus-all models to
support multi-class tasks, and the large C in CIFAR-100 results in the worst performance.

Barely supervised learning. We conduct experiments to test the performance (accuracy and STD)
of our method on CIFAR-10 for some extreme cases (10, 20 and 30 labeled samples) to verify
the effectiveness of our method. Here, we select the labeled data through the scripts of FixMatch
with 5 different random seeds. As claimed in FixMatch, it reaches between 48.58% and 85.32%
test accuracy with a median of 64.28% for 10 labeled samples, while our method obtains accuracy
from 61.32% to 83.7%. As for 20 and 30 labeled samples, our method gets (85.29 ± 3.14)% and
(89.81 ± 1.59)% accuracy respectively, which have much smaller STDs than that reported in [37].

4.5 Ablation Study

In DP-SSL, LFs and the label model are the core com-
ponents to assign probabilistic labels for training the end
model. Here, we check the effects of the following factors
in the process of producing probabilistic labels by taking
CIFAR-10 as the example. For ease of exposition, only the
accuracy of predicted labels is presented in Tab. 4.

Table 4: Annotation performance for
different conﬁgurations on CIFAR-10
with 40 and 250 labels. K and ρ are set
to 50 and 0.2 by default.

Experiments

40 labels 250 labels

Exp1: w.o. MCL
Exp2: MCL w.o. FT
Exp3: MCL w. FT

MCL. Feature transformation (FT) described in Eq. (3)
can be regarded as a weighted spatial pooling for extracted
features. It is proposed to boost the diversity of generated
LFs. We conduct comparative experiments for three con-
ﬁgurations: 1) Exp1: w.o. MCL, 2) Exp2: MCL w.o. FT,
3) Exp3: MCL w. FT. The results are presented in Tab. 4. It
is interesting to see that Exp1 is better than Exp2 but worse
than Exp3. In fact, Exp1 is a simple ensemble model with a shared backbone, where each LF is
trained independently and predicts the labels within C categories. In Exp2, we observe that some
classiﬁers have never been optimized in the training phase and thus have an empty specialized set
when only a few labeled samples per class are available. Moreover, the specialized sets of many LFs
are duplicate, which incurs a negative impact on the performance. However, MCL with FT addresses
the drawbacks and helpes our method obtain versatile LFs.

Exp4: w.o. Regularizer
Exp5: Regularizer

92.46
91.61
93.82

95.02
94.98
95.33

93.19
93.82

94.94
95.33

Hyperparameters. K and ρ are the number of LFs and the ratio of specialists in Eq. (4). In our
ablation study, we focus on the variance of performance for different K and ρ with 40 labeled samples

9

(a)

(b)

Figure 3: Plots of ablation study on CIFAR-10 with 40 labels. (a) Varying the number of LFs K. (b)
Varying the specialized ratio ρ. Here, the red dashed line indicates the error rate of DP-SSL with
default hyperparameters.

on CIFAR-10. In Fig. 3a, K=50 performs the best when 40 labeled samples are available. On the
other hand, performance reaches the best when ρ=0.2 in Fig. 3b. We present more results of ρ and K
in Appendix A.5.

Regularizer. The regularizer is proposed to impose a global guidance and improve the robustness of
the label model. As shown in Tab. 4, the regularizer does boost the accuracy, especially when facing
less labeled samples. Besides, as mentioned in Sec. 4.3, the high-quality guidance of the regularizer
also reduces the label model’s performance variance, thus improves its robustness.

5 Conclusion

In this paper, we explore the data programming idea to boost SSL when only a small number of labeled
samples available by providing more accurate labels for unlabeled data. To this end, we propose
a new SSL method DP-SSL that employs an innovative DP mechanism to automatically generate
labeling functions. To make the labeling functions diverse and specialized, a multiple choice learning
based approach is developed. Furthermore, we design an effective label model by incorporating a
novel potential and a regularizer with estimated accuracy. With this model, probabilistic labels are
inferred by resolving the conﬂict and overlap among noisy labels from the labeling functions. Finally,
an end model is trained under the supervision of the probabilistic labels. Extensive experiments show
that DP-SSL can produce high-quality probabilistic labels, and outperforms the existing methods to
achieve a new SOTA, especially when only a small number of labeled samples are available.

6 Limitations of This Work

In this work, we use coarse accuracy estimation as the statistic information to guide the label model
for simplicity. As described in Sec. 3.5, we estimate the accuracy Pθ(ˆzk
i (cid:54)= 0), rather than
class-wise accuracy Pθ(ˆzk
i = 1). Besides, we do not consider the dependency between LFs
and directly assume they are independent.

i = zi|ˆzk

i = zi|ˆzk

Acknowledgments and Disclosure of Funding

This work was supported by Alibaba Group through Alibaba Innovative Research Program. Shuigeng
Zhou was also partially supported by Science and Technology Commission of Shanghai Municipality
Project (No. 19511120700), and Shanghai Artiﬁcial Intelligence Innovation and Development Projects
funded by Shanghai Municipal Commission of Economy and Informatization.

References

[1] M. Gao, Z. Zhang, G. Yu, S. Ö. Arık, L. S. Davis, and T. Pﬁster, “Consistency-based semi-supervised

active learning: Towards minimizing labeling cost,” in ECCV. Springer, 2020, pp. 510–526.

10

[2] C. Vondrick, D. Patterson, and D. Ramanan, “Efﬁciently scaling up crowdsourced video annotation,” IJCV,

vol. 101, no. 1, pp. 184–204, 2013.

[3] Y. Yao, A. Zhang, X. Han, M. Li, C. Weber, Z. Liu, S. Wermter, and M. Sun, “Visual distant supervision

for scene graph generation,” arXiv preprint arXiv:2103.15365, 2021.

[4] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li,
“Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence,” in NeurIPS, vol. 33,
2020.

[5] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Is object localization for free?-weakly-supervised learning

with convolutional neural networks,” in CVPR, 2015, pp. 685–694.

[6] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual

representations,” in ICML. PMLR, 2020, pp. 1597–1607.

[7] D.-H. Lee et al., “Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural

networks,” in Workshop on ICML, vol. 3, no. 2, 2013, p. 896.

[8] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization with stochastic transformations and perturba-

tions for deep semi-supervised learning,” in NeurIPS, 2016, pp. 1163–1171.

[9] B. Zoph, G. Ghiasi, T.-Y. Lin, Y. Cui, H. Liu, E. D. Cubuk, and Q. Le, “Rethinking pre-training and

self-training,” in NeurIPS, vol. 33, 2020.

[10] A. J. Ratner, C. De Sa, S. Wu, D. Selsam, and C. Ré, “Data programming: Creating large training sets,

quickly,” in NeurIPS, vol. 29. NIH Public Access, 2016, pp. 3567–3575.

[11] A. Awasthi, S. Ghosh, R. Goyal, and S. Sarawagi, “Learning from rules generalizing labeled exemplars,”

ICLR, 2020.

[12] A. J. Ratner, S. H. Bach, H. R. Ehrenberg, and C. Ré, “Snorkel: Fast training set generation for information

extraction,” in SIGMOD. ACM, 2017, pp. 1683–1686.

[13] A. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Ré, “Training complex models with

multi-task weak supervision,” in AAAI, vol. 33, no. 01. AAAI Press, 2019, pp. 4763–4771.

[14] O. Chatterjee, G. Ramakrishnan, and S. Sarawagi, “Data programming using continuous and quality-guided

labeling functions,” arXiv preprint arXiv:1911.09860, 2019.

[15] V. S. Chen, P. Varma, R. Krishna, M. Bernstein, C. Re, and L. Fei-Fei, “Scene graph prediction with limited

labels,” in ICCV.

IEEE, 2019, pp. 2580–2590.

[16] S. Hooper, M. Wornow, H. S. Ying, H. Kellman, Peter andXue, F. Sala, C. Langlotz, and C. Ré, “Cut out

the annotator, keep the cutout: better segmentation with weak supervision,” ICLR, 2021.

[17] N. C. Garcia, S. A. Bargal, V. Ablavsky, P. Morerio, V. Murino, and S. Sclaroff, “Distillation multiple

choice learning for multimodal action recognition,” in WACV, 2021, pp. 2754–2763.

[18] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,” Machine Learning, vol. 109,

no. 2, pp. 373–440, 2020.

[19] X. Yang, Z. Song, I. King, and Z. Xu, “A survey on deep semi-supervised learning,” arXiv preprint

arXiv:2103.00550, 2021.

[20] M. F. Chen, D. Y. Fu, F. Sala, S. Wu, R. T. Mullapudi, F. Poms, K. Fatahalian, and C. Ré, “Train and you’ll
miss it: Interactive model iteration with weak supervision and pre-trained embeddings,” arXiv preprint
arXiv:2006.15168, 2020.

[21] D. Fu, M. Chen, F. Sala, S. Hooper, K. Fatahalian, and C. Ré, “Fast and three-rious: Speeding up weak

supervision with triplet methods,” in ICML, vol. 119. PMLR, 2020, pp. 3280–3291.

[22] A. Guzman-Rivera, P. Kohli, D. Batra, and R. Rutenbar, “Efﬁciently enforcing diversity in multi-output

structured prediction,” in Artiﬁcial Intelligence and Statistics. PMLR, 2014, pp. 284–292.

[23] S. Lee, S. Purushwalkam, M. Cogswell, V. Ranjan, D. J. Crandall, and D. Batra, “Stochastic multiple

choice learning for training diverse deep ensembles,” in NeurIPS, 2016, pp. 2119–2127.

[24] K. Lee, C. Hwang, K. Park, and J. Shin, “Conﬁdent multiple choice learning,” in ICML, vol. 70. PMLR,

2017, pp. 2014–2023.

11

[25] K. Tian, Y. Xu, S. Zhou, and J. Guan, “Versatile multiple choice learning and its application to vision

computing,” in CVPR.

IEEE, 2019, pp. 6349–6357.

[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image

database,” in CVPR.

IEEE, 2009, pp. 248–255.

[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft

coco: Common objects in context,” in ECCV, vol. 8693. Springer, 2014, pp. 740–755.

[28] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and
B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016, pp. 3213–
3223.

[29] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learning,” in ICLR, 2017.

[30] A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko, “Semi-supervised learning with ladder

networks,” in NeurIPS, 2015, pp. 3546–3554.

[31] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raffel, “Mixmatch: A holistic

approach to semi-supervised learning,” in NeurIPS, 2019, pp. 5050–5060.

[32] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged consistency targets

improve semi-supervised deep learning results,” in NeurIPS, 2017, pp. 1195–1204.

[33] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial training: a regularization method for

supervised and semi-supervised learning,” TPAMI, vol. 41, no. 8, pp. 1979–1993, 2019.

[34] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, “Unsupervised data augmentation for consistency training,”

in NeurIPS, 2020.

[35] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, “Remixmatch:
Semi-supervised learning with distribution alignment and augmentation anchoring,” in ICLR, 2020.

[36] T. Han, J. Gao, Y. Yuan, and Q. Wang, “Unsupervised semantic aggregation and deformable template

matching for semi-supervised learning,” in NeurIPS, 2020.

[37] J. Li, C. Xiong, and S. C. Hoi, “Comatch: Semi-supervised learning with contrastive graph regularization,”

in ICCV, 2021, pp. 9475–9484.

[38] Z. Hu, Z. Yang, X. Hu, and R. Nevatia, “Simple: Similar pseudo label exploitation for semi-supervised

classiﬁcation,” in CVPR, 2021, pp. 15 099–15 108.

[39] B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and T. Shinozaki, “Flexmatch: Boosting
semi-supervised learning with curriculum pseudo labeling,” arXiv preprint arXiv:2110.08263, 2021.

[40] P. Varma and C. Ré, “Snuba: automating weak supervision to label training data,” in VLDB, vol. 12, no. 3.

NIH Public Access, 2018, p. 223.

[41] G. Heo, Y. Roh, S. Hwang, D. Lee, and S. E. Whang, “Inspector gadget: A data programming-based

labeling system for industrial images,” Proc. VLDB Endow., vol. 14, no. 1, pp. 28–36, 2020.

[42] A. Pal and V. N. Balasubramanian, “Adversarial data programming: Using gans to relax the bottleneck of

curated labeled data,” in CVPR.

IEEE, 2018, pp. 1556–1565.

[43] N. Das, S. Chaba, R. Wu, S. Gandhi, D. H. Chau, and X. Chu, “Goggles: Automatic image labeling with

afﬁnity coding,” in SIGMOD, 2020, pp. 1717–1732.

[44] B. Boecking, W. Neiswanger, E. Xing, and A. Dubrawski, “Interactive weak supervision: Learning useful

heuristics for data labeling,” ICLR, 2021.

[45] P. Varma, B. He, P. Bajaj, I. Banerjee, N. Khandwala, D. L. Rubin, and C. Ré, “Inferring generative model

structure with static analysis,” in NeurIPS, vol. 30. NIH Public Access, 2017, pp. 240–250.

[46] S. H. Bach, B. He, A. Ratner, and C. Ré, “Learning the structure of generative models without labeled

data,” in ICML, vol. 70. PMLR, 2017, pp. 273–282.

[47] P. Varma, F. Sala, A. He, A. Ratner, and C. Ré, “Learning dependency structures for weak supervision

models,” in ICML. PMLR, 2019, pp. 6418–6427.

12

[48] L. Zhang, J. Ding, Y. Xu, Y. Liu, and S. Zhou, “Weakly-supervised text classiﬁcation based on keyword

graph,” arXiv preprint arXiv:2110.02591, 2021.

[49] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical automated data augmentation

with a reduced search space,” in Workshop on CVPR, 2020, pp. 702–703.

[50] T. DeVries and G. W. Taylor, “Improved regularization of convolutional neural networks with cutout,”

arXiv preprint arXiv:1708.04552, 2017.

[51] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in BMVC. BMVC, 2016.

[52] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Ré, “Snorkel: Rapid training data creation

with weak supervision,” VLDB J., vol. 29, no. 2, pp. 709–730, 2020.

[53] A. Krizhevsky, “Learning multiple layers of features from tiny images,” Master’s thesis, University of

Tront, 2009.

[54] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading digits in natural images with

unsupervised feature learning,” in Workshop on NeurIPS, 2011.

[55] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks in unsupervised feature learning,” in
JMLR Workshop and Conference Proceedings, 2011, pp. 215–223.

Artiﬁcial Intelligence and Statistics.

[56] A. Jaffe, B. Nadler, and Y. Kluger, “Estimating the accuracies of multiple classiﬁers without labeled data,”

in Artiﬁcial Intelligence and Statistics. PMLR, 2015, pp. 407–415.

[57] E. Platanios, H. Poon, T. M. Mitchell, and E. J. Horvitz, “Estimating accuracy from unlabeled data: A

probabilistic logic approach,” NeurIPS, vol. 30, pp. 4361–4370, 2017.

[58] P. A. Traganitis, A. Pages-Zamora, and G. B. Giannakis, “Blind multiclass ensemble classiﬁcation,” IEEE

Transactions on Signal Processing, vol. 66, no. 18, pp. 4737–4752, 2018.

13

Table 5: Glossary of variables and symbols used in this paper.

Symbol

Used for

Labeled samples and ground-truth labels
Unlabeled samples
Weakly augmented (labeled / unlabeled) samples
Strongly augmented (labeled / unlabeled) samples
Label space. e.g., in CIFAR-10, Y = {1, 2, · · · , 10}
Number of categories. C = |Y|
RHW ×D, feature map before global average pooling of backbone
RD, feature vector at spatial position j of f

xl, yl
xu
xw
·
xs
·
Y
C
f
f (j)
dis(A, B) Distance between A and B
ck
fk
Fk
pk(y|x)
τk
¯pk(y|x)
ˆyk
ˆy
θ
eky
φ(y, ˆyk)
P (y, ˆy)
Z
R(θ, ˆyu)

RD, learnable clustering center of the k-th LF
RD, input feature of classiﬁer in the k-th LF
The classiﬁer head in the k-th LF
RC, output probability of the k-th LF
Specialized category set of the k-th LF
R|τk|+1, output probability over specialized categories and “abstention” option
= arg max(¯pk(y|x)), predicted label of the k-th LF
= (ˆy1, · · · , ˆyK)(cid:124) ∈ RK, vectorized predicted laebls of K LFs
θ ∈ RK×|Y| is the parameters of label model
eky := exp(θky), exponent of parameters θky
Potential value with the target label y and predicted label ˆyk
Joint distribution between target label y and predicted label ˆy in the label model
Normalizer item of P (y, ˆy). Z = (cid:80)
Regularizer of unlabeled data in the Label Model

ˆy∈τ P (y, ˆy)

(cid:80)

y∈Y

, latent variable of ground-truth label in the i-th one-versus-all task

zi

ˆzk
i

i zi]
i zi]

E[ˆzk
ˆE[ˆzk
ˆP (·)
ˆak
i




zi =

ˆzk
i =

yu = i
yu (cid:54)= i
ˆyk = i
ˆyk = 0
otherwise
i zi

(cid:26)+1
−1
+1
0
−1
Expectation of ˆzk
Estimated expectation of ˆzk
Probability estimated with the observable data.
Precision of the k-th LF in the i-th one-versus-all classiﬁcation



, latent variable of ˆyk in the i-th one-versus-all task

i zi over all unlabeled data without ground-truch zi.

A Appendix

A.1 Glossary

The glossary is given in Table 5.

A.2 Label Model

Label Model in Snorkel [52] is also called as “Generative Model”, which models and integrates the
noisy labels provided by K LFs. In this paper, we suppose that the K LFs are independent. Assuming
that ˆy = (ˆy1, · · · , ˆyK)(cid:124) ∈ RK is the vectorized form of the predicted labels from K LFs, and ˆyk is
the predicted label of the k-th model. For clarity, we denote ∅ as the “abstention” option in the LFs.
Then, following the deﬁnition of Snorkel, the label model in Snorkel can be represented as:

φ(y, ˆyk),

(18)

P (y, ˆy) =

1
Z

K
(cid:89)

k=1

14

with the potential function φ(y, ˆyk):

φ(y, ˆyk) =






exp(θk1 + θk2),
exp(θk1),
1.

if ˆyk = y
if ˆyk (cid:54)= y, ˆy (cid:54)= ∅
if ˆyk = ∅

(19)

where θ ∈ R2K. It can be observed that the potential function in Eq. (19) provides the same values
for all target categories y. However, each LF in our method specializes in multiple categories with
different performance. Thus, we extend the parameters θ to K ×C to support multi-class classiﬁcation.
In Eq. (19), we set exp(θk1 + θk2) to guarantee that the potential with ˆyk = y is larger than that with
ˆyk (cid:54)= y. Similarly, we set 1 + exp(θky) and 1/(1 + exp(θky)). Then, we have the potential function:

φ(y, ˆyk) =






1 + exp(θyk),
1/(1 + exp(θyk)),
1.

if ˆy ∈ τk, ˆyk = y
if ˆy ∈ τk, ˆyk (cid:54)= y
otherwise

(20)

However, the conclusion from [23, 25] tells us that MCL tends to be overconﬁdent for the samples
whose ground-truth labels are out of the specialized set. In other words, when one LF is fed with
a sample whose ground-truth label is out of the specialized set, it may still produce a label in the
specialized set with high conﬁdence. Although the cases of overconﬁdence decrease a lot due to the
introduction of the “abstention” option in Step 1, these cases still exist in our framework. Therefore,
we introduce the item exp(θyk) to represent the relationship between the predicted label ˆyk and
the target label y, even when the target labels conﬂict with the predicted labels. Based on these
considerations, we deﬁne the four-part potential function in Eq. (9.

A.3 Regularizer

We give the complementary formulation of Pθ(ˆzk
for ease of exposition. We write Pθ(ˆzk

i = zi|ˆzk

i = zi|ˆzk
i (cid:54)= 0) as follows:

i (cid:54)= 0). Set Φk(y, ˆY) := (cid:80)

ˆyk∈ ˆY φ(y, ˆyk)

Pθ(ˆzk

i = zi|ˆzk
Pθ(ˆzk

i (cid:54)= 0)
i = zi = 1) + Pθ(ˆzk
Pθ(ˆzk
i (cid:54)= 0)

i = zi = −1)

=

=

=

Pθ(y = i, ˆyk = y, ˆyk (cid:54)= 0) + Pθ(y (cid:54)= i, ˆyk (cid:54)= i, ˆyk (cid:54)= 0)
Pθ(ˆyk (cid:54)= 0)
(i, {0} ∪ τk(cid:48)) + (cid:80)

φ(i, i) (cid:81)

y(cid:54)=i(Φk(y, τk − {i}) (cid:81)

k(cid:48)(cid:54)=k Φk(cid:48)

k(cid:48)(cid:54)=k Φk(cid:48)

(cid:80)

y∈Y Φk(y, τk) (cid:81)

k(cid:48)(cid:54)=k Φk(cid:48)(y, {0} ∪ τk(cid:48))

(y, {0} ∪ τk(cid:48)))

.

(21)

As mentioned in our paper, the class-wise accuracy Pθ(ˆzk
can be written as:

i = zi|ˆzk

i = 1) without negative samples

Pθ(ˆzk

i = zi|ˆzk

i = 1) ==

k(cid:48)(cid:54)=k Φk(cid:48)

φ(i, i) (cid:81)
y∈Y φ(y, i) (cid:81)

(i, {0} ∪ τk(cid:48))
k(cid:48)(cid:54)=k Φk(cid:48)(y, {0} ∪ τk(cid:48))

(cid:80)

.

(22)

However, we can estimate ˆP (ˆzk
the class-wise accuracy ˆP (ˆzk

i = zi|ˆzk

i = 1).

i = zi|ˆzk

i (cid:54)= 0) by Eq. (15) directly, but it is more difﬁcult to estimate

A.4 Model Analysis

We give an example with 40 labeled samples in Fig. 4 to illustrate why Majority Vote falls with
K = 50 and ρ = 0.2. In the extreme case, category “automobile” and category “ship” are only
specialized by 4 LFs, while 15 LFs specialize in “dog”. If an image with category “automobile”
triggers all (4) true specialized LFs but triggers 40% (5) specialized LFs with class “dog”, Majority
Vote would misclassify it into “dog”. With our label model, we can achieve 95.22% annotation
accuracy in this case.

15

Figure 4: The number of specialized LFs for each category.

Table 6: Annotation performance for different ρ and K on CIFAR-10 with 40 labels

0.1
83.25
89.16
91.39
92.53
92.95
93.01

0.2
86.42
91.27
93.46
93.28
93.82
93.54

0.3
88.99
91.83
93.54
93.45
93.55
93.11

0.4
90.62
92.31
93.16
93.24
93.19
92.97

0.5
91.28
92.52
92.91
92.80
93.07
92.63

1.0
92.39
92.48
92.41
92.47
92.46
92.43

10
20
30
40
50
60

A.5 Hyperparameter ρ and K

We also present the complete experimental results with different ρ and K on CIFAR-10 with 40
labels in Tab. 6.

16

0246810121416airplaneautomobilebirdcatdeerdogfroghorseshiptruck# of LFs