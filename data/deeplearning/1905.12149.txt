SATNet: Bridging deep learning and logical reasoning using a differentiable
satisﬁability solver

9
1
0
2

y
a
M
9
2

]

G
L
.
s
c
[

1
v
9
4
1
2
1
.
5
0
9
1
:
v
i
X
r
a

Po-Wei Wang 1 Priya L. Donti 1 2 Bryan Wilder 3 Zico Kolter 1 4

Abstract

1. Introduction

Integrating logical reasoning within deep learning
architectures has been a major goal of modern AI
systems. In this paper, we propose a new direction
toward this goal by introducing a differentiable
(smoothed) maximum satisﬁability (MAXSAT)
solver that can be integrated into the loop of larger
deep learning systems. Our (approximate) solver
is based upon a fast coordinate descent approach
to solving the semideﬁnite program (SDP) associ-
ated with the MAXSAT problem. We show how
to analytically differentiate through the solution
to this SDP and efﬁciently solve the associated
backward pass. We demonstrate that by integrat-
ing this solver into end-to-end learning systems,
we can learn the logical structure of challenging
problems in a minimally supervised fashion. In
particular, we show that we can learn the parity
function using single-bit supervision (a tradition-
ally hard task for deep networks) and learn how to
play 9 × 9 Sudoku solely from examples. We also
solve a “visual Sudoku” problem that maps im-
ages of Sudoku puzzles to their associated logical
solutions by combining our MAXSAT solver with
a traditional convolutional architecture. Our ap-
proach thus shows promise in integrating logical
structures within deep learning.

1School of Computer Science, Carnegie Mellon University,
Pittsburgh, Pennsylvania, USA 2Department of Engineering &
Public Policy, Carnegie Mellon University, Pittsburgh, Pennsyl-
vania, USA 3Department of Computer Science, University of
Southern California, Los Angeles, California, USA 4Bosch Center
for Artiﬁcial Intelligence, Pittsburgh, Pennsylvania, USA. Cor-
respondence to: Po-Wei Wang <poweiw@cs.cmu.edu>, Priya
Donti <pdonti@cmu.edu>, Bryan Wilder <bwilder@usc.edu>,
Zico Kolter <zkolter@cs.cmu.edu>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

Although modern deep learning has produced groundbreak-
ing improvements in a variety of domains, state-of-the-art
methods still struggle to capture “hard” and “global” con-
straints arising from discrete logical relationships. Moti-
vated by this deﬁciency, there has been a great deal of recent
interest in integrating logical or symbolic reasoning into
neural network architectures (Palm et al., 2017; Yang et al.,
2017; Cingillioglu & Russo, 2018; Evans & Grefenstette,
2018). However, with few exceptions, previous work pri-
marily focuses on integrating preexisting relationships into
a larger differentiable system via tunable continuous pa-
rameters, not on discovering the discrete relationships that
produce a set of observations in a truly end-to-end fashion.
As an illustrative example, consider the popular logic-based
puzzle game Sudoku, in which a player must ﬁll in a 9 × 9
partially-ﬁlled grid of numbers to satisfy speciﬁc constraints.
If the rules of Sudoku (i.e. the relationships between prob-
lem variables) are not given, then it may be desirable to
jointly learn the rules of the game and learn how to solve
Sudoku puzzles in an end-to-end manner.

We consider the problem of learning logical structure specif-
ically as expressed by satisﬁability problems – concretely,
problems that are well-modeled as instances of SAT or
MAXSAT (the optimization analogue of SAT). This is a
rich class of domains encompassing much of symbolic AI,
which has traditionally been difﬁcult to incorporate into
neural network architectures since neural networks rely
on continuous and differentiable parameterizations. Our
key contribution is to develop and derive a differentiable
smoothed MAXSAT solver that can be embedded within
more complex deep architectures, and show that this solver
enables effective end-to-end learning of logical relationships
from examples (without hard-coding of these relationships).
More speciﬁcally, we build upon recent work in fast block
coordinate descent methods for solving SDPs (Wang et al.,
2017) to build a differentiable solver for the smoothed SDP
relaxation of MAXSAT. We provide an efﬁcient mechanism
to differentiate through the optimal solution of this SDP by
using a similar block coordinate descent solver as used in the
forward pass. Our module is amenable to GPU acceleration,
greatly improving training scalability.

 
 
 
 
 
 
SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

Using this framework, we are able to solve several problems
that, despite their simplicity, prove essentially impossible
for traditional deep learning methods and existing logical
learning methods to reliably learn without any prior knowl-
edge.
In particular, we show that we can learn the par-
ity function, known to be challenging for deep classiﬁers
(Shalev-Shwartz et al., 2017), with only single bit supervi-
sion. We also show that we can learn to play 9 × 9 Sudoku,
a problem that is challenging for modern neural network
architectures (Palm et al., 2017). We demonstrate that our
module quickly recovers the constraints that describe a fea-
sible Sudoku solution, learning to correctly solve 98.3% of
puzzles at test time without any hand-coded knowledge of
the problem structure. Finally, we show that we can embed
this differentiable solver into larger architectures, solving a
“visual Sudoku” problem where the input is an image of a
Sudoku puzzle rather than a binary representation. We show
that, in a fully end-to-end setting, our method is able to
integrate classical convolutional networks (for digit recogni-
tion) with the differentiable MAXSAT solver (to learn the
logical portion). Taken together, this presents a substan-
tial advance toward a major goal of modern AI: integrating
logical reasoning into deep learning architectures.

2. Related work

Recently, the deep learning community has given increas-
ing attention to the concept of embedding complex, “non-
traditional” layers within deep networks in order to train
systems end-to-end. Major examples have included logical
reasoning modules and optimization layers. Our work com-
bines research in these two areas by exploiting optimization-
based relaxations of logical reasoning structures, namely
an SDP relaxation of MAXSAT. We explore each of these
relevant areas of research in more detail below.

Logical reasoning in deep networks. Our work is
closely related to recent interest in integrating logical rea-
soning into deep learning architectures (Garcez et al., 2015).
Most previous systems have focused on creating differen-
tiable modules from an existing set of known relationships,
so that a deep network can learn the parameters of these re-
lationships (Dai et al., 2018; Manhaeve et al., 2018; Sourek
et al., 2018; Xu et al., 2018; Hu et al., 2016; Yang et al.,
2017; Selsam et al., 2018). For example, Palm et al. (2017)
introduce a network that carries out relational reasoning
using hand-coded information about which variables are
allowed to interact, and test this network on 9 × 9 Sudoku.
Similarly, Evans & Grefenstette (2018) integrate inductive
logic programming into neural networks by constructing
differentiable SAT-based representations for speciﬁc “rule
templates.” While these networks are seeded with prior
information about the relationships between variables, our
approach learns these relationships and their associated pa-

rameters end-to-end. While other recent work has also tried
to jointly learn rules and parameters, the problem classes
captured by these architectures have been limited. For in-
stance, Cingillioglu & Russo (2018) train a neural network
to apply a speciﬁc class of logic programs, namely the binary
classiﬁcation problem of whether a given set of propositions
entails a speciﬁc conclusion. While this approach does
not rely on prior hand-coded structure, our method applies
to a broader class of domains, encompassing any problem
reducible to MAXSAT.

Differentiable optimization layers. Our work also ﬁts
within a line of research leveraging optimization as a layer in
neural networks. For instance, previous work has introduced
differentiable modules for quadratic programs (Amos &
Kolter, 2017; Donti et al., 2017), submodular optimization
problems (Djolonga & Krause, 2017; Tschiatschek et al.,
2018; Wilder et al., 2018), and equilibrium computation
in zero-sum games (Ling et al., 2018). To our knowledge,
ours is the ﬁrst work to use differentiable SDP relaxations
to capture relationships between discrete variables.

MAXSAT SDP relaxations. We build on a long line of
research exploring SDP relaxations as a tool for solving
MAXSAT and related problems. Classical work shows that
such relaxations produce strong approximation guarantees
for MAXCUT and MAX-2SAT (Goemans & Williamson,
1995), and are empirically tighter than standard linear pro-
gramming relaxations (Gomes et al., 2006). More recent
work, e.g. Wang et al. (2017); Wang & Kolter (2019), has
developed low-rank SDP solvers for general MAXSAT prob-
lems. We extend the work of Wang et al. (2017) to create a
differentiable optimization-based MAXSAT solver that can
be employed in the loop of deep learning.

3. A differentiable satisﬁability solver

The MAXSAT problem is the optimization analogue of the
well-known satisﬁability (SAT) problem, in which the goal
is to maximize the number of clauses satisﬁed. We present
a differentiable, smoothed approximate MAXSAT solver
that can be integrated into modern deep network architec-
tures. This solver uses a fast coordinate descent approach
to solving an SDP relaxation of MAXSAT. We describe
our MAXSAT SDP relaxation as well as the forward pass
of our MAXSAT deep network layer (which employs this
relaxation). We then show how to analytically differenti-
ate through the MAXSAT SDP and efﬁciently solve the
associated backward pass.

3.1. Solving an SDP formulation of satisﬁability

Consider a MAXSAT instance with n variables and m
clauses. Let ˜v ∈ {−1, 1}n denote binary assignments
of the problem variables, where ˜vi is the truth value of

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

Figure 1. The forward pass of our MAXSAT layer. The layer takes as input the discrete or probabilistic assignments of known MAXSAT
variables, and outputs guesses for the assignments of unknown variables via a MAXSAT SDP relaxation with weights S.

variable i ∈ {1, . . . , n}, and deﬁne ˜si ∈ {−1, 0, 1}m for
i ∈ {1, . . . , n}, where ˜sij denotes the sign of ˜vi in clause
j ∈ {1, . . . , m}. We then write the MAXSAT problem as

maximize
˜v∈{−1,1}n

m
(cid:88)

n
(cid:95)

j=1

i=1

1{˜sij ˜vi > 0}.

(1)

As derived in Goemans & Williamson (1995); Wang &
Kolter (2019), to form a semideﬁnite relaxation of (1), we
ﬁrst relax the discrete variables ˜vi into associated contin-
uous variables vi ∈ Rk, (cid:107)vi(cid:107) = 1 with respect to some
“truth direction” v(cid:62) ∈ Rk, (cid:107)v(cid:62)(cid:107) = 1. Speciﬁcally, we re-
late the continuous vi to the discrete ˜vi probabilistically via
P (˜vi = 1) = cos−1(−vT
i v(cid:62))/π based on randomized round-
ing (Goemans & Williamson (1995); see Section 3.2.4). We
additionally deﬁne a coefﬁcient vector ˜s(cid:62) = {−1}m asso-
ciated with v(cid:62). Our SDP relaxation of MAXSAT is then

minimize
V ∈Rk×(n+1)

(cid:104)ST S, V T V (cid:105),

subject to (cid:107)vi(cid:107) = 1,

i = (cid:62), 1, . . . , n

(2)

√

˜sn

. . .
(cid:3) diag(1/

where V ≡ (cid:2)v(cid:62) v1
(cid:3) ∈ Rk×(n+1), and S ≡
vn
(cid:2)˜s(cid:62) ˜s1
4|˜sj |) ∈ Rm×(n+1). We note
. . .
that this problem is a low-rank (but non-convex) formu-
lation of MIN-UNSAT, which is equivalent to MAXSAT.
This formulation can be rewritten as an SDP, and has been
shown to recover the optimal SDP solution given k >
2n
(Barvinok, 1995; Pataki, 1998).

√

Despite its non-convexity, problem (2) can then be solved
optimally via coordinate descent for all i = (cid:62), 1, . . . , n. In
particular, the objective terms that depend on vi are given
by vT
i sjvj, where si is the ith column vector of
i
S. Minimizing this quantity over vi subject to the constraint
that (cid:107)vi(cid:107) = 1 yields the coordinate descent update

j=0 sT

(cid:80)n

vi = −gi/(cid:107)gi(cid:107), where gi = V ST si − (cid:107)si(cid:107)2vi.

(3)

These updates provably converge to the globally optimal
ﬁxed point of the SDP (2) (Wang et al., 2017). A more de-
tailed derivation of this update can be found in Appendix A.

// rank, num aux vars, initial weights, rand vectors
init m, naux, S
init random unit vectors v(cid:62), vrand
// smallest k for which (2) recovers SDP solution
set k =

∀i ∈ {1, . . . , n}

√

i

Algorithm 1 SATNet Layer

1: procedure INIT()
2:
3:
4:
5:
6:
7:
8: procedure FORWARD(ZI)
9:
10:
11:
12:

2n + 1

compute VI from ZI via (5)
compute VO from VI via coord. descent (Alg 2)
compute ZO from VO via (7)
return ZO

13:
14: procedure BACKWARD(∂(cid:96)/∂ZO)
15:
16:
17:
18:

compute ∂(cid:96)/∂VO via (8)
compute U from ∂(cid:96)/∂VO via coord. descent (Alg 3)
compute ∂(cid:96)/∂ZI, ∂(cid:96)/∂S from U via (12), (11)
return ∂(cid:96)/∂ZI

3.2. SATNet: Satisﬁability solving as a layer

Using our MAXSAT SDP relaxation and associated coordi-
nate descent updates, we create a deep network layer for sat-
isﬁability solving (SATNet). Deﬁne I ⊂ {1, . . . , n} to be
the indices of MAXSAT variables with known assignments,
and let O ≡ {1, . . . , n} \ I correspond to the indices of
variables with unknown assignments. Our layer admits prob-
abilistic or binary inputs zι ∈ [0, 1], ι ∈ I, and then outputs
the assignments of unknown variables zo ∈ [0, 1], o ∈ O
which are similarly probabilistic or (optionally, at test time)
binary. We let ZI ∈ [0, 1]|I| and ZO ∈ [0, 1]|O| refer to all
input and output assignments, respectively.

The outputs ZO are generated from inputs ZI via the
SDP (2), and the weights of our layer correspond to the
SDP’s low-rank coefﬁcient matrix S. This forward pass
procedure is pictured in Figure 1. We describe the steps of
layer initialization and the forward pass in Algorithm 1, and
in more detail below.

SDP relaxation(weights 𝑺)𝑣#∈ℝ&for 𝜄∈𝑣(∈ℝ&for 𝑜∈𝑧#∈0,1for 𝜄∈𝑧(∈0,1for 𝑜∈Inputs (discrete or probabilistic)relaxroundOutputs (discrete or probabilistic)MAXSAT LayerRelaxed inputsRelaxed outputsSATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

3.2.1. LAYER INITIALIZATION

When initializing SATNet, the user must specify a maximum
number of clauses m that this layer can represent. It is often
desirable to set m to be low; in particular, low-rank structure
can prevent overﬁtting and thus improve generalization.

Given this low-rank structure, a user may wish to somewhat
increase the layer’s representational ability via auxiliary
variables. The high-level intuition here follows from the
conjunctive normal form (CNF) representation of boolean
satisfaction problems; adding additional variables to a prob-
lem can dramatically reduce the number of CNF clauses
needed to describe that problem, as these variables play a
role akin to register memory that is useful for inference.

√

Finally, we set k =
2n + 1, where here n captures the
number of actual problem variables in addition to auxiliary
variables. This is the minimum value of k required for our
MAXSAT relaxation (2) to recover the optimal solution of
its associated SDP (Barvinok, 1995; Pataki, 1998).

3.2.2. STEP 1: RELAXING LAYER INPUTS

Our layer ﬁrst relaxes its inputs ZI into continuous vectors
for use in the SDP formulation (2). That is, we relax each
layer input zι, ι ∈ I to an associated random unit vector
vι ∈ Rk so that

vT
ι v(cid:62) = − cos(πzι).

(4)

(This equation is derived from the probabilistic relationship
described in Section 3.1 between discrete variables and their
continuous relaxations.) Constraint (4) can be satisﬁed by

vι = − cos(πzι)v(cid:62) + sin(πzι)(Ik − v(cid:62)vT

(cid:62))vrand
ι

,

(5)

ι

where vrand
is a random unit vector. For simplicity, we use
the notation VI ∈ Rk×|I| (i.e. the I-indexed column subset
of V ) to collectively refer to all relaxed layer inputs derived
via Equation (5).

3.2.3. STEP 2: GENERATING CONTINUOUS
RELAXATIONS OF OUTPUTS VIA SDP

Given the continuous input relaxations VI, our layer em-
ploys the coordinate descent updates (3) to compute values
for continuous output relaxations vo, o ∈ O (which we col-
lectively refer to as VO ∈ Rk×|O|). Notably, coordinate
descent updates are only computed for output variables, i.e.
are not computed for variables whose assignments are given
as input to the layer.

Our coordinate descent algorithm for the forward pass is
detailed in Algorithm 2. This algorithm maintains the term
Ω = V ST needed to compute go, and then modiﬁes it via a
rank-one update during each inner iteration. Accordingly,
the per-iteration runtime is O(nmk) (and in practice, only
a small number of iterations is required for convergence).

o

, ∀o ∈ O.

// inputs for known variables

Algorithm 2 Forward pass coordinate descent
1: input VI
2: init vo with random vector vrand
3: compute Ω = V ST
4: while not converged do
for o ∈ O do
5:
6:
7:
8:
9: output VO

// for all output variables
compute go = Ωso − (cid:107)so(cid:107)2vo as in (3)
compute vo = −go/(cid:107)go(cid:107) as in (3)
update Ω = Ω + (vo − vprev

)sT
o
// ﬁnal guess for output cols of V

o

3.2.4. STEP 3: GENERATING DISCRETE OR

PROBABILISTIC OUTPUTS

Given the relaxed outputs VO from coordinate descent, our
layer converts these outputs to discrete or probabilistic vari-
able assignments ZO via either thresholding or randomized
rounding (which we describe here).

The main idea of randomized rounding is that for every
vo, o ∈ O, we can take a random hyperplane r from the unit
sphere and assign

˜vo =

(cid:40)
1
−1

o r) = sign(vT

(cid:62)r)

if sign(vT
otherwise

, o ∈ O,

(6)

where ˜vo is the boolean output for vo.
Intuitively, this
scheme sets ˜vo to “true” if and only if vo and the truth
vector v(cid:62) are on the same side of the random hyperplane r.
Given the correct weights S, this randomized rounding pro-
cedure assures an optimal expected approximation ratio for
certain NP-hard problems (Goemans & Williamson, 1995).

During training, we do not explicitly perform randomized
rounding. We instead note that the probability that vo and
v(cid:62) are on the same side of any given r is

P (˜vo) = cos−1(−vT

o v(cid:62))/π,

(7)

and thus set zo = P (˜vo) to equal this probability.

During testing, we can either output probabilistic outputs in
the same fashion, or output discrete assignments via thresh-
olding or randomized rounding. If using randomized round-
ing, we round multiple times, and then set zo to be the
boolean solution maximizing the MAXSAT objective in
Equation (1). Prior work has observed that such repeated
rounding improves approximation ratios in practice, espe-
cially for MAXSAT problems (Wang & Kolter, 2019).

3.3. Computing the backward pass

We now derive backpropagation updates through our SAT-
Net layer to enable its integration into a neural network.
That is, given the gradients ∂(cid:96)/∂ZO of the network loss (cid:96)

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

with respect to the layer outputs, we must compute the gra-
dients ∂(cid:96)/∂ZI with respect to layer inputs and ∂(cid:96)/∂S with
respect to layer weights. As it would be inefﬁcient in terms
of time and memory to explicitly unroll the forward-pass
computations and store intermediate Jacobians, we instead
derive analytical expressions to compute the desired gra-
dients directly, employing an efﬁcient coordinate descent
algorithm. The procedure for computing these gradients is
summarized in Algorithm 1 and derived below.

3.3.1. FROM PROBABILISTIC OUTPUTS TO THEIR

CONTINUOUS RELAXATIONS

Algorithm 3 Backward pass coordinate descent

O = 0

1: input {∂(cid:96)/∂vo | o ∈ O} // grads w.r.t. relaxed outputs
2: // Compute UO from Equation (9)
3: init UO = 0 and Ψ = (UO)ST
4: while not converged do
for o ∈ O do
5:
6:
7:
8:
9: output UO

compute dgo = Ψso − (cid:107)so(cid:107)2uo − ∂(cid:96)/∂vo.
compute uo = −Podgo/(cid:107)go(cid:107).
update Ψ = Ψ + (uo − uprev

// for all output variables

)sT
o

o

Given ∂(cid:96)/∂ZO (with respect to the layer outputs), we ﬁrst
derive an expression for ∂(cid:96)/∂VO (with respect to the output
relaxations) by pushing gradients through the probability
assignment mechanism described in Section 3.2.4. That is,
for each o ∈ O,
(cid:18) ∂(cid:96)
∂zo

1
π sin(πzo)

(cid:19) (cid:18) ∂zo
∂vo

(cid:18) ∂(cid:96)
∂zo

∂(cid:96)
∂vo

v(cid:62),

(8)

=

=

(cid:19)

(cid:19)

where we obtain ∂zo/∂vo by differentiating through Equa-
tion (7) (or, more readily, by implicitly differentiating
through its rearrangement cos(πzo) = −vT

(cid:62)vo).

3.3.2. BACKPROPAGATION THROUGH THE SDP

Given the analytical form for ∂(cid:96)/∂VO (with respect to the out-
put relaxations), we next seek to derive ∂(cid:96)/∂VI (with respect
to the input relaxations) and ∂(cid:96)/∂S (with respect to the layer
weights) by pushing gradients through our SDP solution
procedure (Section 3.2.3). We describe the analytical form
for the resultant gradients in Theorem 1.
Theorem 1. Deﬁne Po ≡ Ik − vovT
o for each o ∈ O. Then,
deﬁne U ∈ Rk×n, where the columns UI = 0 and the
columns UO are given by

vec(UO) = (P ((C + D) ⊗ Ik)P )† vec

(cid:19)

(cid:18) ∂(cid:96)
∂VO

,

(9)

at a high level is quite simple: we differentiate the solution
of the SDP problem (Section 3.1) with respect to the prob-
lem’s parameters and input, which requires computing the
(relatively large) matrix-vector solve given in Equation (9).

To solve Equation (9), we use a coordinate descent approach
that closely mirrors the coordinate descent procedure em-
ployed in the forward pass, and which has similar fast con-
vergence properties. This procedure, described in Algo-
rithm 3, enables us to compute the desired gradients without
needing to maintain intermediate Jacobians explicitly. Mir-
roring the forward pass, we use rank-one updates to main-
tain and modify the term Ψ = U ST needed to compute dgo,
which again enables our algorithm to run in O(nmk) time.
We defer the derivation of Algorithm 3 to Appendix D.

3.3.3. FROM RELAXED TO ORIGINAL INPUTS

As a ﬁnal step, we must use the gradient ∂(cid:96)/∂VI (with respect
to the input relaxations) to derive the gradient ∂(cid:96)/∂ZI (with
respect to the actual inputs) by pushing gradients through
the input relaxation procedure described in Section 3.2.2.
For each ι ∈ I, we see that

∂(cid:96)
∂zι

=

=

∂(cid:96)
∂z(cid:63)
ι

∂(cid:96)
∂z(cid:63)
ι

+

−

(cid:18) ∂(cid:96)
∂vι
(cid:18) ∂vι
∂zι

(cid:19)T ∂vι
∂zι

(cid:19)T (cid:32)

(cid:88)

o∈O

(cid:33)

(12)

uosT
o

sι

where P ≡ diag(Po), where C ≡ ST
OSO − diag((cid:107)so(cid:107)2),
and where D ≡ diag((cid:107)go(cid:107)). Then, the gradient of the
network loss (cid:96) with respect to the relaxed layer inputs is

where

∂(cid:96)
∂VI

= −

(cid:16) (cid:88)

uosT
o

(cid:17)

SI,

o∈O

(10)

where SI is the I-indexed column subset of S, and the
gradient with respect to the layer weights is

∂(cid:96)
∂S

= −

(cid:16) (cid:88)

(cid:17)T

uosT
o

o∈O

V − (SV T )U.

(11)

∂vι
∂zι

= π (cid:0)sin(πzι)v(cid:62) + cos(πzι)(Ik − v(cid:62)vT

(cid:1) ,
(13)
ι captures any direct dependence of (cid:96) on z(cid:63)
and where ∂(cid:96)/∂z(cid:63)
ι
(as opposed to dependence through vι). Here, the expression
for ∂(cid:96)/∂vι comes from Equation (10), and we obtain ∂vι/∂zι
by differentiating Equation (5).

(cid:62))vrand
ι

We defer the derivation of Theorem 1 to Appendix B. Al-
though this derivation is somewhat involved, the concept

The coordinate descent updates in Algorithms 2 and 3 dom-
inate the computational costs of the forward and backward

3.4. An efﬁcient GPU implementation

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

passes, respectively. We thus present an efﬁcient, paral-
lel GPU implementation of these algorithms to speed up
training and inference. During the inner loop of coordi-
nate descent, our implementation parallelizes the compu-
tation of all go (dgo) terms by parallelizing the computa-
tion of Ω (Ψ), as well as of all rank-one updates of Ω
(Ψ). This underscores the beneﬁt of using a low-rank
SDP formulation in our MAXSAT layer, as traditional
full-rank coordinate descent cannot be efﬁciently paral-
lelized. We ﬁnd in our preliminary benchmarks that our
GPU CUDA-C implementation is up to 18 − 30x faster than
the corresponding OpenMP implementation run on Xeon
CPUs. Source code for our implementation is available at
https://github.com/locuslab/SATNet.

4. Experiments

We test our MAXSAT layer approach in three domains that
are traditionally difﬁcult for neural networks: learning the
parity function with single-bit supervision, learning 9 × 9
Sudoku solely from examples, and solving a “visual Sudoku”
problem that generates the logical Sudoku solution given an
input image of a Sudoku puzzle. We ﬁnd that in all cases,
we are able to perform substantially better on these tasks
than previous deep learning-based approaches.

4.1. Learning parity (chained XOR)

This experiment tests SATNet’s ability to differentiate
through many successive SAT problems by learning to com-
pute the parity function. The parity of a bit string is deﬁned
as one if there is an odd number of ones in the sequence
and zero otherwise. The task is to map input sequences to
their parity, given a dataset of example sequence/parity pairs.
Learning parity functions from such single-bit supervision
is known to pose difﬁculties for conventional deep learning
approaches (Shalev-Shwartz et al., 2017). However, parity
is simply a logic function – namely, a sequence of XOR
operations applied successively to the input sequence.

Hence, for a sequence of length L, we construct our model
to contain a sequence of L − 1 SATNet layers with tied
weights (similar to a recurrent network). The ﬁrst layer
receives the ﬁrst two binary values as input, and layer d
receives value d along with the rounded output of layer
d − 1. If each layer learns to compute the XOR function, the
combined system will correctly compute parity. However,
this requires the model to coordinate a long series of SAT
problems without any intermediate supervision.

Figure 2 shows that our model accomplishes this task for
input sequences of length L = 20 and L = 40. For each
sequence length, we generate a dataset of 10K random ex-
amples (9K training and 1K testing). We train our model
using cross-entropy loss and the Adam optimizer (Kingma

Figure 2. Error rate for the parity task with L = 20 (top) and
L = 40 (bottom). Solid lines denote test values, while dashed
lines represent training values.

& Ba, 2015) with a learning rate of 10−1. We compare to an
LSTM sequence classiﬁer, which uses 100 hidden units and
a learning rate of 10−3 (we tried varying the architecture
and learning rate but did not observe any improvement).
In each case, our model quickly learns the target function,
with error on the held-out set converging to zero within 20
epochs. In contrast, the LSTM is unable to learn an appro-
priate representation, with only minor improvement over
the course of 100 training epochs; across both input lengths,
it achieves a testing error rate of at best 0.476 (where a
random guess achieves value 0.5).

4.2. Sudoku (original and permuted)

In this experiment, we test SATNet’s ability to infer and
recover constraints simply from bit supervision (i.e. without
any hard-coded speciﬁcation of how bits are related). We
demonstrate this property via Sudoku. In Sudoku, given a
(typically) 9 × 9 partially-ﬁlled grid of numbers, a player
must ﬁll in the remaining empty grid cells such that each
row, each column, and each of nine 3 × 3 subgrids contains
exactly one of each number from 1 through 9. While this
constraint satisfaction problem is computationally easy to
solve once the rules of the game are speciﬁed, actually
learning the rules of the game, i.e. the hard constraints of
the puzzle, has proved challenging for traditional neural
network architectures. In particular, Sudoku problems are
often solved computationally via tree search, and while tree
search cannot be easily performed by neural networks, it is
easily expressible using SAT and MAXSAT problems.

We construct a SATNet model for this task that takes as in-
put a logical (bit) representation of the initial Sudoku board

020406080100Epoch0.00.20.4Error(L=20)020406080100Epoch0.000.250.50Error(L=40)SATNetLSTMSATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

Model

Train

Test

Model

Train

Test

Model

Train

Test

ConvNet
ConvNetMask
SATNet (ours)

72.6% 0.04%
91.4% 15.1%
99.8% 98.3%

ConvNet
ConvNetMask
SATNet (ours)

0%
0.01%
99.7% 98.3%

0%
0%

ConvNet
ConvNetMask
SATNet (ours)

0%
0.31%
89%
0.1%
93.6% 63.2%

(a) Original Sudoku.

(b) Permuted Sudoku.

(c) Visual Sudoku.
the theoretical
“best” test accuracy for our architecture is
74.7%.)

(Note:

Table 1. Results for 9 × 9 Sudoku experiments with 9K train/1K test examples. We compare our SATNet model against a vanilla convolu-
tional neural network (ConvNet) as well as one that receives a binary mask indicating which bits need to be learned (ConvNetMask).

along with a mask representing which bits must be learned
(i.e. all bits in empty Sudoku cells). This input is vectorized,
which means that our SATNet model cannot exploit the lo-
cality structure of the input Sudoku grid when learning to
solve puzzles. Given this input, the SATNet layer then out-
puts a bit representation of the Sudoku board with guesses
for the unknown bits. Our model architecture consists of a
single SATNet layer with 300 auxiliary variables and low
rank structure m = 600, and we train it to minimize a digit-
wise negative log likelihood objective (optimized via Adam
with a 2 × 10−3 learning rate).

We compare our model to a convolutional neural network
baseline modeled on that of Park (2016), which interprets
the bit inputs as 9 input image channels (one for each square
in the board) and uses a sequence of 10 convolutional lay-
ers (each with 512 3×3 ﬁlters) to output the solution. The
ConvNet makes explicit use of locality in the input represen-
tation since it treats the nine cells within each square as a
single image. We also compare to a version of the ConvNet
which receives a binary mask indicating which bits need to
be learned (ConvNetMask). The mask is input as a set of ad-
ditional image channels in the same format as the board. We
trained both architectures using mean squared error (MSE)
loss (which gave better results than negative log likelihood
for this architecture). The loss was optimized using Adam
(learning rate 10−4). We additionally tried to train an Opt-
Net (Amos & Kolter, 2017) model for comparison, but this
model made little progress even after a few days of training.
(We compare our method to OptNet on a simpler 4 × 4
version of the Sudoku problem in Appendix E.)

Our results for the traditional 9 × 9 Sudoku problem (over
9K training examples and 1K test examples) are shown in
Table 1. (Convergence plots for this experiment are shown
in Appendix F.) Our model is able to learn the constraints
of the Sudoku problem, achieving high accuracy early in
the training process (95.0% test accuracy in 22 epochs/37
minutes on a GTX 1080 Ti GPU), and demonstrating 98.3%
board-wise test accuracy after 100 training epochs (172
minutes). On the other hand, the ConvNet baseline does
poorly. It learns to correctly solve 72.6% of puzzles in the

training set but fails altogether to generalize: accuracy on
the held-out set reaches at most 0.04%. The ConvNetMask
baseline, which receives a binary mask denoting which
entries must be completed, performs only somewhat better,
correctly solving 15.1% of puzzles in the held-out set. We
note that our test accuracy is qualitatively similar to the
results obtained in Palm et al. (2017), but that our network
is able to learn the structure of Sudoku without explicitly
encoding the relationships between variables.

To underscore that our architecture truly learns the rules
of the game, as opposed to overﬁtting to locality or other
structure in the inputs, we test our SATNet architecture on
permuted Sudoku boards, i.e. boards for which we apply
a ﬁxed permutation of the underlying bit representation
(and adjust the corresponding input masks and labels ac-
cordingly). This removes any locality structure, and the
resulting Sudoku boards do not have clear visual analogues
that can be solved by humans. However, the relationships
between bits are unchanged (modulo the permutation) and
should therefore be discoverable by architectures that can
truly learn the underlying logical structure. Table 1 shows
results for this problem in comparison to the convolutional
neural network baselines. Our architecture is again able
to learn the rules of the (permuted) game, demonstrating
the same 98.3% board-wise test accuracy as in the original
game. In contrast, the convolutional neural network base-
lines perform even more poorly than in the original game
(achieving 0% test accuracy even with the binary mask as
input), as there is little locality structure to exploit. Overall,
these results demonstrate that SATNet can truly learn the
logical relationships between discrete variables.

4.3. Visual Sudoku

In this experiment, we demonstrate that SATNet can be inte-
grated into larger deep network architectures for end-to-end
training. Speciﬁcally, we solve the visual Sudoku problem:
that is, given an image representation of a Sudoku board
(as opposed to a one-hot encoding or other logical represen-
tation) constructed with MNIST digits, our network must
output a logical solution to the associated Sudoku problem.

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

In 100 epochs, our model learns to correctly solve 63.2% of
boards at test time, reaching 85% of this theoretical “best.”
Hence, our approach demonstrates strong performance in
solving visual Sudoku boards end-to-end. On the other hand,
the baseline convolutional networks make only minuscule
improvements to the training loss over the course of 100
epochs, and fail altogether to improve out-of-sample per-
formance. Accordingly, our SATNet architecture enables
end-to-end learning of the “rules of the game” directly from
pictorial inputs in a way that was not possible with previous
architectures.

5. Conclusion

In this paper, we have presented a low-rank differentiable
MAXSAT layer that can be integrated into neural network
architectures. This layer employs block coordinate descent
methods to efﬁciently compute the forward and backward
passes, and is amenable to GPU acceleration. We show that
our SATNet architecture can be successfully used to learn
logical structures, namely the parity function and the rules
of 9 × 9 Sudoku. We also show, via a visual Sudoku task,
that our layer can be integrated into larger deep network
architectures for end-to-end training. Our layer thus shows
promise in allowing deep networks to learn logical structure
without hard-coding of the relationships between variables.

More broadly, we believe that this work ﬁlls a notable gap
in the regime spanning deep learning and logical reason-
ing. While many “differentiable logical reasoning” systems
have been proposed, most of them still require fairly hand-
speciﬁed logical rules and groundings, and thus are some-
what limited in their ability to operate in a truly end-to-end
fashion. Our hope is that by wrapping a powerful yet generic
primitive such as MAXSAT solving within a differentiable
framework, our solver can enable “implicit” logical reason-
ing to occur where needed within larger frameworks, even if
the precise structure of the domain is unknown and must be
learned from data. In other words, we believe that SATNet
provides a step towards integrating symbolic reasoning and
deep learning, a long-standing goal in artiﬁcial intelligence.

Acknowledgments

Po-Wei Wang is supported by a grant from the Bosch Center
for AI; Priya Donti is supported by the Department of En-
ergy’s Computational Science Graduate Fellowship under
grant number DE-FG02-97ER25308; and Bryan Wilder is
supported by the National Science Foundation Graduate
Research Fellowship.

Figure 3. An example visual Sudoku image input, i.e. an image of
a Sudoku board constructed with MNIST digits. Cells ﬁlled with
the numbers 1-9 are ﬁxed, and zeros represent unknowns.

An example input is shown in Figure 3. This problem cannot
traditionally be represented well by neural network architec-
tures, as it requires the ability to combine multiple neural
network layers without hard-coding the logical structure of
the problem into intermediate logical layers.

Our architecture for this problem uses a convolutional neural
network connected to a SATNet layer. Speciﬁcally, we apply
a convolutional layer for digit classiﬁcation (which uses the
LeNet architecture (LeCun et al., 1998)) to each cell of the
Sudoku input. Each cell-wise probabilistic output of this
convolutional layer is then fed as logical input to the SATNet
layer, along with an input mask (as in Section 4.2). This
SATNet layer employs the same architecture and training
parameters as described in the previous section. The whole
model is trained end-to-end to minimize cross-entropy loss,
and is optimized via Adam with learning rates 2 × 10−3 for
the SATNet layer and 10−5 for the convolutional layer.

We compare our approach against a convolutional neural
network which combines two sets of convolutional layers.
First, the visual inputs are passed through the same con-
volutional layer as in our SATNet model, which outputs a
probabilistic bit representation. Next, this representation is
passed through the convolutional architecture that we com-
pared to for the original Sudoku problem, which outputs a
solution. We use the same training approach as above.

Table 1 summarizes our experimental results (over 9K train-
ing examples and 1K test examples); additional plots are
shown in Appendix F. We contextualize these results against
the theoretical “best” testing accuracy of 74.7%, which ac-
counts for the Sudoku digit classiﬁcation accuracy of our
speciﬁc convolutional architecture; that is, assuming boards
with 36.2 out of 81 ﬁlled cells on average (as in our test
set) and an MNIST model with 99.2% test accuracy (LeCun
et al., 1998), we would expect a perfect Sudoku solver to
output the correct solution 74.7% (= 0.99236.2) of the time.

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

References

Amos, B. and Kolter, J. Z. Optnet: Differentiable opti-
mization as a layer in neural networks. arXiv preprint
arXiv:1703.00443, 2017.

Barvinok, A. I. Problems of distance geometry and convex
properties of quadratic maps. Discrete & Computational
Geometry, 13(2):189–202, 1995.

Cingillioglu, N. and Russo, A. Deeplogic: End-to-end logi-
cal reasoning. arXiv preprint arXiv:1805.07433, 2018.

Dai, W.-Z., Xu, Q.-L., Yu, Y., and Zhou, Z.-H. Tunneling
neural perception and logic reasoning through abductive
learning. arXiv preprint arXiv:1802.01173, 2018.

Djolonga, J. and Krause, A. Differentiable learning of
submodular models. In Advances in Neural Information
Processing Systems, pp. 1013–1023, 2017.

Donti, P. L., Amos, B., and Kolter, J. Z. Task-based end-
to-end model learning in stochastic optimization. arXiv
preprint arXiv:1703.04529, 2017.

Evans, R. and Grefenstette, E. Learning explanatory rules
from noisy data. Journal of Artiﬁcial Intelligence Re-
search, 61:1–64, 2018.

Garcez, A., Besold, T. R., De Raedt, L., Földiak, P., Hitzler,
P., Icard, T., Kühnberger, K.-U., Lamb, L. C., Miikku-
lainen, R., and Silver, D. L. Neural-symbolic learning and
reasoning: contributions and challenges. In Proceedings
of the AAAI Spring Symposium on Knowledge Represen-
tation and Reasoning: Integrating Symbolic and Neural
Approaches, Stanford, 2015.

Goemans, M. X. and Williamson, D. P. Improved approx-
imation algorithms for maximum cut and satisﬁability
problems using semideﬁnite programming. Journal of
the ACM (JACM), 42(6):1115–1145, 1995.

Gomes, C. P., van Hoeve, W.-J., and Leahu, L. The power
of semideﬁnite programming relaxations for max-sat. In
International Conference on Integration of Artiﬁcial Intel-
ligence (AI) and Operations Research (OR) Techniques in
Constraint Programming, pp. 104–118. Springer, 2006.

Hu, Z., Ma, X., Liu, Z., Hovy, E., and Xing, E. Harnessing
deep neural networks with logic rules. In Proceedings of
the 54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), volume 1,
pp. 2410–2420, 2016.

Kingma, D. P. and Ba, J. L. Adam: Amethod for stochastic
optimization. In International Conference on Learning
Representations, 2015.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

Ling, C. K., Fang, F., and Kolter, J. Z. What game are we
playing? end-to-end learning in normal and extensive
form games. In Proceedings of the Twenty-Seventh Inter-
national Joint Conference on Artiﬁcial Intelligence, pp.
396–402, 2018. doi: 10.24963/ijcai.2018/55.

Manhaeve, R., Dumancic, S., Kimmig, A., Demeester, T.,
and De Raedt, L. Deepproblog: Neural probabilistic
logic programming. In Advances in Neural Information
Processing Systems, pp. 3749–3759, 2018.

Palm, R. B., Paquet, U., and Winther, O. Recurrent rela-
tional networks. arXiv preprint arXiv:1711.08028, 2017.

Park, K. Can neural networks crack sudoku?, 2016. URL
https://github.com/Kyubyong/sudoku.

Pataki, G. On the rank of extreme matrices in semideﬁ-
nite programs and the multiplicity of optimal eigenval-
ues. Mathematics of operations research, 23(2):339–358,
1998.

Selsam, D., Lamm, M., Bunz, B., Liang, P., de Moura, L.,
and Dill, D. L. Learning a sat solver from single-bit
supervision. arXiv preprint arXiv:1802.03685, 2018.

Shalev-Shwartz, S., Shamir, O., and Shammah, S. Failures
of gradient-based deep learning. In Proceedings of the
34th International Conference on Machine Learning, pp.
3067–3075, 2017.

Sourek, G., Aschenbrenner, V., Zelezny, F., Schockaert,
S., and Kuzelka, O. Lifted relational neural networks:
Efﬁcient learning of latent relational structures. Journal
of Artiﬁcial Intelligence Research, 62:69–100, 2018.

Tschiatschek, S., Sahin, A., and Krause, A. Differ-
arXiv preprint

entiable submodular maximization.
arXiv:1803.01785, 2018.

Wang, P.-W. and Kolter, J. Z. Low-rank semideﬁnite pro-
gramming for the max2sat problem. In AAAI Conference
on Artiﬁcial Intelligence, 2019.

Wang, P.-W., Chang, W.-C., and Kolter, J. Z. The mixing
method: coordinate descent for low-rank semideﬁnite
programming. arXiv preprint arXiv:1706.00476, 2017.

Wilder, B., Dilkina, B., and Tambe, M. Melding the data-
decisions pipeline: Decision-focused learning for combi-
natorial optimization. In AAAI Conference on Artiﬁcial
Intelligence, 2018.

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

Xu, J., Zhang, Z., Friedman, T., Liang, Y., and den Broeck,
G. V. A semantic loss function for deep learning with
symbolic knowledge. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, pp. 5498–5507,
2018. URL http://proceedings.mlr.press/
v80/xu18h.html.

Yang, F., Yang, Z., and Cohen, W. W. Differentiable learn-
ing of logical rules for knowledge base reasoning. In
Advances in Neural Information Processing Systems, pp.
2319–2328, 2017.

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

A. Derivation of the forward pass coordinate

and where Po ≡ Ik − vovT

o , o ∈ O and I (cid:48) ≡ {(cid:62)} ∪ I.

descent update

Our MAXSAT SDP relaxation (described in Section 3.1) is
given by

minimize
V ∈Rk×(n+1)

(cid:104)ST S, V T V (cid:105),

subject to (cid:107)vi(cid:107) = 1,

i = 0, . . . , n,

(A.1)

where S ∈ Rm×(n+1) and vi is the ith column vector of V .
We rewrite the objective of (A.1) as (cid:104)ST S, V T V (cid:105) ≡
tr((ST S)T (V T V )) = tr(V T V ST S) by noting that ST S
is symmetric and by cycling matrices within the trace. We
then observe that the objective terms that depend on any
given vi are given by

Rewriting as a linear system. Rewriting Equation B.1
over all o ∈ O as a linear system, we obtain

(cid:16)

diag((cid:107)go(cid:107)) ⊗ Ik + P C ⊗ Ik

(cid:17)

vec(dVO) = −P vec(ξo)

(cid:16)

P ((cid:0) diag((cid:107)go(cid:107)) + C(cid:1) ⊗ Ik)P

⇒ vec(dVO) = −

vec(ξo),
(B.3)
where C = ST
OSO − diag((cid:107)so(cid:107)2), P = diag(Po), and
the second step follows from the lemma presented in Ap-
pendix C.

(cid:17)†

We then see that by the chain rule, the gradients ∂(cid:96)/∂VI and
∂(cid:96)/∂S are given by the left matrix-vector product

vT
i

n
(cid:88)

j=0

j sivj = vT
sT
i

n
(cid:88)

j=0
(j(cid:54)=i)

j sivj + vT
sT

i sT

i sivi,

(A.2)

(cid:18)

where si is the ith column vector of S. Observe vT
last term cancels to 1, and the remaining coefﬁcient

i vi in the

gi ≡

n
(cid:88)

j=0
(j(cid:54)=i)

sT
j sivj = V ST si − (cid:107)si(cid:107)2vi

(A.3)

is constant with respect to vi. Thus, (A.2) can be simply
rewritten as

i gi + sT
vT

i si.

(A.4)

Minimizing this expression over vi with respect to the con-
straint (cid:107)vi(cid:107) = 1 yields the block coordinate descent update

(cid:19)T

vec(dVO)

∂(cid:96)
∂ vec(VO)
(cid:18)

∂(cid:96)
∂ vec(VO)

= −

(cid:19)T (cid:16)

P ((cid:0) diag((cid:107)go(cid:107)) + C(cid:1) ⊗ Ik)P

(cid:17)†

vec(ξo)

(B.4)
where the second equality comes from plugging in the result
of (B.3).
Now, deﬁne U ∈ Rk×n, where the columns UI = 0 and the
columns UO are given by

vec(UO) =

(cid:16)
P ((cid:0) diag((cid:107)go(cid:107))+C(cid:1)⊗Ik)P

(cid:17)†

vec

(cid:19)

.

(cid:18)

∂(cid:96)
∂ vec(VO)
(B.5)

Then, we see that (B.4) can be written as

vi = −gi/(cid:107)gi(cid:107).

(A.5)

(cid:19)T

(cid:18)

∂(cid:96)
∂ vec(VO)

vec(dVO) = − vec(UO)T vec(ξo),

(B.6)

B. Details on backpropagation through the

which is the implicit linear form for our gradients.

MAXSAT SDP

Given the result ∂(cid:96)/∂VO, we next seek to compute ∂(cid:96)/∂VI
and ∂(cid:96)/∂S by pushing gradients through the SDP solution
procedure described in Section 3.1. We do this by taking the
total differential through our coordinate descent updates (3)
for each output o ∈ O at the optimal ﬁxed-point solution to
which these updates converge.

Computing the total differential. Computing the total
differential of the updates (3) and rearranging, we see that
for every o ∈ O,

Computing desired gradients from implicit linear form.
Once we have obtained UO (via coordinate descent), we can
explicitly compute the desired gradients ∂(cid:96)/∂VI and ∂(cid:96)/∂S
from the implicit form (B.6). For instance, to compute the
gradient ∂(cid:96)/∂vι for some ι ∈ I, we would set dvι = 1 and
all other gradients to zero in Equation (B.6) (where these
gradients are captured within the terms ξo).

Explicitly, we compute each ∂(cid:96)/∂vιj by setting dvιj = 1 and
all other gradients to zero, i.e.

(cid:0)(cid:107)go(cid:107)Ik − (cid:107)so(cid:107)2Po

(cid:1) dvo + Po

sT
o sjdvj = −Poξo, (B.1)

(cid:88)

j∈O

∂(cid:96)
∂vιj

= − vec(UO)T vec(ξo) = −

where

ξo ≡

(cid:16) (cid:88)

j∈I(cid:48)

o sjdvj + V dST so + V ST dso − 2dsT
sT

o sovo

(cid:17)
,

= −eT
j

(cid:33)

uosT
o

sι.

(cid:32)

(cid:88)

o∈O

(cid:88)

o∈O

o ejsT
uT

ι so

(B.7)

(B.2)

Similarly, we compute each ∂(cid:96)/∂Si,j by setting dSi,j = 1

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

and all other gradients to zero, i.e.

∂(cid:96)
∂Si,j

= −

= −

(cid:88)

o∈O
(cid:88)

o∈O

uT
o ξo

o visoj − uT
uT

i (V ST )j + uT

i (sijPivi)

= −vT
i (

(cid:88)

uosoj) − uT

i (V ST )j.

o∈O

In matrix form, these gradients are
(cid:32)

(cid:33)

∂(cid:96)
∂VI

= −

(cid:88)

o∈O

∂(cid:96)
∂S

= −

(cid:32)

(cid:88)

o∈O

uosT
o

SI,

(cid:33)T

uosT
o

V − (SV T )U,

(B.10)

where ui is the ith column of U , and where SI denotes the
I-indexed column subset of S.

C. Proof of pseudoinverse computations

We prove the following lemma, used to derive the implicit
total differential for vec(dVO).
Lemma C.1. The quantity

vec(dVO) = (P ((D + C) ⊗ Ik) P )† vec(ξo)

(C.1)

is the solution of the linear system

(D ⊗ Ik + P C ⊗ Ik) vec(dVO) = P vec(ξo),

(C.2)

where P = diag(Ik − vovT
D = diag((cid:107)gi(cid:107)), and ξo is as deﬁned in Equation (B.2).

OSO − diag((cid:107)so(cid:107)2),

o ), C = ST

Proof. Examining the equation with respect to dvi gives

(cid:107)gi(cid:107)dvi + Pi





(cid:88)

j



D. Derivation of the backward pass
coordinate descent algorithm

Consider solving for UO as mentioned in Equation (B.5):

(cid:16)

P ((cid:0) diag((cid:107)go(cid:107))+C(cid:1)⊗Ik)P

(cid:17)

vec(UO) = vec

(cid:18)

∂(cid:96)
∂ vec(VO)

(cid:19)

,

(B.8)

(B.9)

where C = ST
OSO − diag((cid:107)so(cid:107)2). The linear system can
be computed using block coordinate descent. Speciﬁcally,
observe this linear system with respect to only the uo vari-
able. Since we start from UO = 0, we can assume that
P vec(Uo) = vec(Uo). This yields

(cid:107)go(cid:107)Pouo + Po

(cid:16)

UOST

Oso − (cid:107)so(cid:107)2uo

(cid:17)

= Po

(cid:18) ∂(cid:96)
∂vo

(cid:19)

.

(D.1)

Let Ψ = (UO)ST

O. Then we have

(cid:107)go(cid:107)Pouo = −Po(Ψso − (cid:107)so(cid:107)2uo − ∂(cid:96)/∂vo).

(D.2)

Deﬁne −dgi to be the terms contained in parentheses in the
right-hand side of the above equation. Note that dgi does not
depend on the variable uo. Thus, we have the closed-form
feasible solution

uo = −Podgo/(cid:107)go(cid:107).

(D.3)

After updating uo, we can maintain the term Ψ by replacing
the old uprev
o with the new uo. This yields the rank 1 update

Ψ := Ψ + (uo − uprev

o

)sT
o .

(D.4)

The above procedure is summarized in Algorithm 3. Further,
we can verify that the assumption P vec(UO) = vec(UO)
still holds after each update by the projection Po.

cijdvj − ξj

 = 0,

(C.3)

E. Results for the 4 × 4 Sudoku problem

which implies that for all i, dvi = Piyi for some yi. Substi-
tuting yi into the equality gives

(D ⊗ Ik + P C ⊗ Ik)P vec(yi)

=P ((D + C) ⊗ Ik)P vec(yi) = P vec(ξo).

(C.4)

(C.5)

Note that the last equation comes form D ⊗ IkP = D ⊗
IkP P = P (D ⊗ Ik)P due to the block diagonal structure
of the projection P . Thus, by the properties of projectors
and the pseudoinverse,

vec(Y ) = (P ((D + C) ⊗ Ik)P )†P vec(ξo)
= (P ((D + C) ⊗ Ik)P )† vec(ξo).

(C.6)

(C.7)

Note that the ﬁrst equation comes from the idempotence
property of P (that is, P P = P ). Substituting vec(dVO) =
P vec(Y ) back gives the solution of dVO.

We compare the performance of our SATNet architecture
on a 4 × 4 reduced version of the Sudoku puzzle against
OptNet (Amos & Kolter, 2017) and a convolutional neural
network architecture. These results (over 9K training and
1K testing examples) are shown in Figure E.1. We note that
our architecture converges quickly – in just two epochs – to
100% board-wise test accuracy.

OptNet takes slightly longer to converge to similar perfor-
mance, in terms of both time and epochs. In particular, we
see that OptNet takes 3-4 epochs to converge (as opposed
to 1 epoch for SATNet). Further, in our preliminary bench-
marks, OptNet required 12 minutes to run 20 epochs on a
GTX 1080 Ti GPU, whereas SATNet took only 2 minutes
to run the same number of epochs. In other words, we see
that SATNet requires fewer epochs to converge and takes
less time per epoch than OptNet.

SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

Figure E.1. Results for 4 × 4 Sudoku. Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy (% puzzles
correct) are better.

Both our SATNet architecture and OptNet outperform the
traditional convolutional neural network in this setting, as
the ConvNet somewhat overﬁts to the training set and there-
fore does not generalize as well to the test set (achieving
93% accuracy). The ConvNetMask, which additionally re-
ceives a binary input mask, performs much better (99% test
accuracy) but does not achieve perfect performance as in
the case of OptNet and SATNet.

F. Convergence plots for 9 × 9 Sudoku

experiments

Convergence plots for our 9 × 9 Sudoku experiments (orig-
inal and permuted) are shown in Figure F.1. SATNet per-
forms nearly identically in both the original and permuted
settings, generalizing well to the test set at every epoch
without overﬁtting to the training set. The ConvNet and
ConvNetMask, on the other hand, do not generalize well.
In the original setting, both architectures overﬁt to the train-
ing set, showing little-to-no improvement in generalization
performance over the course of training. In the permuted set-
ting, both ConvNet and ConvNetMask make little progress
even on the training set, as they are not able to rely on spatial
locality of inputs.

Convergence plots for the visual Sudoku experiments are
shown in Figure F.2. Here, we see that SATNet general-
izes well in terms of loss throughout the training process,
and generalizes somewhat well in terms of whole-board
accuracy. The difference in generalization performance
between the logical and visual Sudoku settings can be at-
tributed to the generalization performance of the MNIST
classiﬁer trained end-to-end with our SATNet layer. The
ConvNetMask architecture overﬁts to the training set, and
the ConvNet architecture makes little-to-no progress even
on the training set.

010203040506070Epoch101103105107109Mean NLL Loss010203040506070Epoch101102103Mean MSE Loss010203040506070Epoch020406080100% Puzzles CorrectSATNet (train)SATNet (test)ConvNet (train)ConvNet (test)ConvNetMask (train)ConvNetMask (test)OptNet (train)OptNet (test)SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver

(a) Original 9 × 9 Sudoku

Figure F.1. Results for our 9 × 9 Sudoku experiments. Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy
(% puzzles correct) are better.

(b) Permuted 9 × 9 Sudoku

Figure F.2. Results for our visual Sudoku experiments. Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy
(% puzzles correct) are better. The theoretical “best” test accuracy plotted is for our speciﬁc choice of MNIST classiﬁer architecture.

020406080100Epoch101102Mean NLL Loss020406080100Epoch101102103104Mean MSE Loss020406080100Epoch020406080100% Puzzles CorrectSATNet (train)SATNet (test)ConvNet (train)ConvNet (test)ConvNetMask (train)ConvNetMask (test)020406080100Epoch101102Mean NLL Loss020406080100Epoch101102Mean MSE Loss020406080100Epoch020406080100% Puzzles CorrectSATNet (train)SATNet (test)ConvNet (train)ConvNet (test)ConvNetMask (train)ConvNetMask (test)020406080100Epoch101102Mean NLL Loss020406080100Epoch101102103104Mean MSE Loss020406080100Epoch020406080100% Puzzles CorrectSATNet (train)SATNet (test)ConvNet (train)ConvNet (test)ConvNetMask (train)ConvNetMask (test)Theoretical "best" test accuracy