1
2
0
2

r
p
A
9
1

]

G
L
.
s
c
[

1
v
5
4
3
9
0
.
4
0
1
2
:
v
i
X
r
a

Learning to Sparsify Travelling Salesman
Problem Instances

James Fitzpatrick1

, Deepak Ajwani2

, and Paula Carroll1

1 School of Business, University College Dublin
{james.fitzpatrick1@ucdconnect.ie, paula.carroll@ucd.ie}
2 School of Computer Science, University College Dublin
deepak.ajwani@ucd.ie

Abstract. In order to deal with the high development time of exact and
approximation algorithms for NP-hard combinatorial optimisation prob-
lems and the high running time of exact solvers, deep learning techniques
have been used in recent years as an end-to-end approach to ﬁnd solu-
tions. However, there are issues of representation, generalisation, com-
plex architectures, interpretability of models for mathematical analysis
etc. using deep learning techniques. As a compromise, machine learning
can be used to improve the run time performance of exact algorithms
in a matheuristics framework. In this paper, we use a pruning heuristic
leveraging machine learning as a pre-processing step followed by an exact
Integer Programming approach. We apply this approach to sparsify in-
stances of the classical travelling salesman problem. Our approach learns
which edges in the underlying graph are unlikely to belong to an optimal
solution and removes them, thus sparsifying the graph and signiﬁcantly
reducing the number of decision variables. We use carefully selected fea-
tures derived from linear programming relaxation, cutting planes explo-
ration, minimum-weight spanning tree heuristics and various other local
and statistical analysis of the graph. Our learning approach requires
very little training data and is amenable to mathematical analysis. We
demonstrate that our approach can reliably prune a large fraction of the
variables in TSP instances from TSPLIB/MATILDA (ą 85%) while pre-
serving most of the optimal tour edges. Our approach can successfully
prune problem instances even if they lie outside the training distribu-
tion, resulting in small optimality gaps between the pruned and original
problems in most cases. Using our learning technique, we discover novel
heuristics for sparsifying TSP instances, that may be of independent
interest for variants of the vehicle routing problem.

Keywords: Travelling Salesman Problem · Graph Sparsiﬁcation · Ma-
chine Learning · Linear Programming · Integer Programming

1

Introduction

Owing to the high running time of exact solvers on many instances of NP-hard
combinatorial optimisation problems (COPs), there has been a lot of research in-
terest in leveraging machine learning techniques to speed up the computation of

 
 
 
 
 
 
2

Learning to Sparsify TSP Instances

optimisation solutions. In recent years, deep learning techniques have been used
as an end-to-end approach (see e.g., [30]) for eﬃciently solving COPs. However,
these approaches generally suﬀer from (i) limited generalisation to larger size
problem instances and limited generalisation from instances of one domain to
another domain, (ii) need for increasingly complex architectures to improve gen-
eralisability and (iii) inherent black-box nature of deep learning that comes in
the way of mathematical analysis [6]. In particular, the lack of interpretability
of these models means that (1) we do not know which properties of the input
instances are being leveraged by the deep-learning solver and (2) we cannot be
sure that the model will still work as new constraints are required, which is
typical in industry use-cases.

In contrast to the end-to-end deep learning techniques, there has been recent
work (see e.g. [13]) to use machine learning as a component to speed-up or
scale-up the exact solvers. In particular, Lauri and Dutta [19] recently proposed
a framework to use machine learning as a pre-processing step to sparsify the
maximum clique enumeration instances and scale-up the exact algorithms in
this way. In this work, we build upon this framework and show that integrating
features derived from operations research and approximation algorithms into
the learning component for sparsiﬁcation can result in reliably pruning a large
fraction of the variables in the classical Travelling Salesman Problem (TSP).
Speciﬁcally, we use carefully selected features derived from linear programming
relaxation, cutting planes exploration, minimum-weight spanning tree (MST)
heuristics and various other local and statistical analysis of the graph to sparsify
the TSP instances. With these features, we are able to prune more than 85% of
the edges on TSP instances from TSPLIB/MATILDA, while preserving most of
the optimal tour edges. Our approach can successfully prune problem instances
even if they lie outside the training distribution, resulting in small optimality
gaps between the pruned and original problems in most cases. Using our learning
technique, we discover novel heuristics for sparsifying TSP instances, that may
be of independent interest for variants of the vehicle routing problem.

Overall, our approach consists of using a pruning heuristic leveraging machine
learning (ML) as a pre-processing step to sparsify instances of the TSP, fol-
lowed by an exact Integer Programming (IP) approach. We learn which edges
in the underlying graph are unlikely to belong to an optimal TSP tour and re-
move them, thus sparsifying the graph and signiﬁcantly reducing the number
of decision variables. Our learning approach requires very little training data,
which is a crucial requirement for learning techniques dealing with NP-hard
problems. The usage of well analysed intuitive features and more interpretable
learning models means that our approach is amenable to mathematical analysis.
For instance, by inserting the edges from the double-tree approximation in our
sparsiﬁed instances, we can guarantee the same bounds on the optimality gap
as a double-tree approximation. We hypothesise that our approach, integrating
features derived from operations research and approximation algorithms into a

Learning to Sparsify TSP Instances

3

learning component for sparsiﬁcation, is likely to be useful in a range of COPs,
including but not restricted to, vehicle routing problems.

Outline. The paper is structured as follows: Section 2 describes the related
works. In Section 3 we outline the proposed sparsiﬁcation scheme: the feature
generation, the sparsiﬁcation model and post-processing techniques. Section 4
contains the experimental setup, exposition on the computational experiments
and results. Discussion and conclusions follow in Section 5.

2 Notation and Related Work
Given a graph G “ pV, E; wq with a vertex set V “ t1, ..., nu, an edge set
E “ tpu, vq|u, v P V, u ‰ vu and a weighting function wGpeq Ñ Z` that assigns
a weight to each edge e P E, the goal of the TSP is to ﬁnd a tour in G that
visits each vertex exactly once, starting and ﬁnishing at the same vertex, with
least cumulative weight. We denote by m the number of edges |E| and by n the
number of vertices |V | of the problem.

2.1 Exact, Heuristic and Approximate Approaches
The TSP is one of the most widely-studied COPs and has been for many decades;
for this reason, many very eﬀective techniques and solvers have been developed
for solving them. Concorde is a well-known, eﬀective exact solver which imple-
ments a branch and cut approach to solve a TSP IP [1], and has been used
to solve very large problems. An extremely eﬃcient implementation of the Lin-
Kernighan heuristic is available at [15], which can ﬁnd very close-to-optimal
solutions in most cases. Approximation algorithms also exist for the metric TSP
that permit the identiﬁcation of solutions, with worst-case performance guaran-
tees, in polynomial time [7, 26]. Many metaheuristic solution frameworks have
also been proposed, using the principles of ant colony optimisation, genetic al-
gorithms and simulated annealing among others [5, 9, 16]. In each of these tradi-
tional approaches to the TSP, there are lengthy development times and extensive
problem-speciﬁc knowledge is required. If the constraints of the given problem
are altered, the proposed solution method may no longer be satisfactory, possibly
requiring further development.

2.2 Learning to Solve Combinatorial Optimisation Problems
Inspired by the success of deep learning to solve natural language processing and
computer vision tasks, the question of how eﬀective similar techniques might be
in COP solution frameworks arises. Interest in this research direction emerged
following the work of Vinyals et al. [30], in which sequence-to-sequence neural
networks were used as heuristics for small instances of three diﬀerent COPs,
including the TSP. A number of works quickly followed that solve larger prob-
lem instances and avoid the need for supervised learning where access to data
is a bottleneck [3, 18, 21]. Graph neural networks [25] and transformer archi-
tectures [29] lead to signiﬁcant speedups for learned heuristics, and have been

4

Learning to Sparsify TSP Instances

demonstrated to obtain near-optimal solutions to yet larger TSP and vehicle
routing problem (VRP) instances in seconds. Although they can produce com-
petitive solutions to relatively small problems, these learning approaches appear
to fail to generalise well to larger instance sizes, and most of these approaches re-
quire that the instance is Euclidean, encoding the coordinates of the vertices for
feature computation. In cases of failure and poor solution quality, however, there
is little possibility of interpreting why mistakes were made, making it diﬃcult
to rely on these models.

2.3 Graph Sparsiﬁcation
Graph sparsiﬁcation is the process of pruning edges from a graph G “ pV, Eq
to form a subgraph H “ pV, E1 Ă Eq such that H preserves or approximates
some property of G [4, 11, 23]. Eﬀective sparsiﬁcation is achieved if |E1| ! |E|.
The running time of a TSP solve can be reduced if it is possible to sparsify the
underlying complete graph Kn so that the edges of at least one optimal Hamil-
tonian cycle are preserved. The work of Hougardy and Schroeder [17] sparsiﬁes
the graph deﬁning symmetric TSP instances exactly, removing a large fraction
of the edges, known as “useless” edges, that provably cannot exist in an optimal
tour. Another heuristic approach due to Wang and Remmel [31] sparsiﬁes sym-
metric instances by making probabilistic arguments about the likelihood that
an edge will belong to an optimal tour. Both of these approaches have proven
successful, reducing computation time signiﬁcantly for large instances, but are
unlikely to be easy to modify for diﬀerent problem variants.

Recently, the sparsiﬁcation problem has been posed as a learning problem, for
which a binary classiﬁcation model is trained to identify edges unlikely to belong
to an optimal solution. Grassia et al. [12] use supervised learning to prune edges
that are unlikely to belong to maximum cliques in a multi-step sparsiﬁcation
process. This signiﬁcantly reduces the computational eﬀort required for the task.
Sun et al. [28] train a sparsiﬁer for pruning edges from TSP instances that are
unlikely to belong to the optimal tour. These approaches have the advantage
that they can easily be modiﬁed for similar COP variants. The use of simpler,
classical ML models lends them the beneﬁts of partial interpretability and quick
inference times. In the latter case however, it is assumed that a very large number
of feasible TSP solutions can be sampled eﬃciently, which does not hold for all
routing-type problems, and in neither case are guarantees provided that the
resultant sparsiﬁed instance admits a feasible solution.

3 Sparsiﬁcation Scheme

The sparsiﬁcation problem is posed as a binary classiﬁcation task. Given some
edge e P E, we wish to assign it a label 0 or 1, where the label 0 indicates
that the associated edge variable should be pruned and the label 1 indicates
that it should be retained. We acquire labelled data for a set of graphs G “
tG1, ..., Gnu corresponding to TSP problem instances. For each graph Gi “

Learning to Sparsify TSP Instances

5

i , ..., tpi

Ť

pVi, Eiq we compute the set of all pi optimal tours Ti “ tt1
i u and for each
edge e P Ei we compute a feature representation (cid:126)qe. Each tour ti has an implied
i “ Ei with 1 and each edge
set of edges ti ùñ (cid:15)i Ă Ei. Labelling each e P
e P EizEi as 0, we train an edge classiﬁer to prune edges that do not belong to the
optimal tour. An optimal sparsiﬁer would prune all but those edges belonging
to optimal tours (potentially also solving the TSP). This classiﬁer represents
a binary-ﬁxing heuristic in the context of an IP. In the following sections, we
describe the feature representation that is computed for each edge and post-
processing steps that are taken in order to make guarantees and to alleviate the
eﬀects of over-pruning.

pi
j“1 (cid:15)j

3.1 Linear Programming Features
We pose the TSP as an IP problem, using the DFJ formulation [8]. Taking Aij
as the matrix of edge-weights, we formulate it as follows:

minimize

z “

Aijxij

i,j“1;i‰j

nÿ

subject to

nÿ

i;i‰j
nÿ

j;j‰i

xij “ 1,

j “ 1, ..., n

xij “ 1,

i “ 1, ..., n

xij ď |W | ´ 1,

W Ď V ;

|W | ě 3

xij P t0, 1u,

i, j “ 1, ..., m;

i ‰ j

ÿ

pi,jqPW

(1)

(2)

(3)

(4)

(5)

Useful information about the structure of a TSP problem can be extracted by
inspecting solution vectors to linear relaxations of this IP, we can obtain insights
into the candidacy of edges for the optimal tour. In fact, in several cases, for the
MATILDA problem set, the solution to the linear relaxation at the root of the
Branch and Bound (B&B) tree is itself an optimal solution to the TSP.

We denote the solution to the linear relaxation zLP of the integer programme
at the root node of the B&B tree by ˆ(cid:126)x0. At this point, no variables have been
branched on and no subtour elimination cuts have been introduced. That is, the
constraints (4) are dropped and the constraints (5) are relaxed as:

xij P r0, 1s,

i, j P t1, ..., mu, i ‰ j.

(6)

One can strengthen this relaxation by introducing some subtour elimination
constraints (4) at the root node. In this case, the problem to be solved remains
a linear programming problem but several rounds of subtour elimination cuts
are added. One can limit the computational eﬀort expended in this regard by

6

Learning to Sparsify TSP Instances

restricting the number of constraint-adding rounds with some upper bound k.
The solution vector for this problem after k rounds of cuts is denoted by ˜(cid:126)xk. We
can also use as features the associated reduced costs (cid:126)rk, which are computed in
the process of a Simplex solve, standardising their values as ˆ(cid:126)rk “ (cid:126)rk{ max (cid:126)rk.

In order to capture broader information about the structure of the problem,
stochasticity is introduced to the cutting planes approach. Inspired by the work
of Fischetti and Monaci [10], the objective of the problem is perturbed. Solving
the perturbed problem results in diﬀerent solution vectors, which can help us
to explore the solution space. We solve the initial relaxation zLP in order to
obtain a feasible solution, which can sometimes take a signiﬁcant amount of
computing eﬀort. Subtour-elimination constraints are added to the problem for
each subtour in the initial relaxation. Following this, k copies of this problem are
initialised. For each new problem, the objective coeﬃcients Aij are perturbed,
and the problem is re-solved. The normalised reduced costs are obtained from
each perturbed problem and for each edge pi, jq the mean reduced cost ˜rij is
computed. We use the vector ˜(cid:126)r of such values as a feature vector.

3.2 Minimum Weight Spanning Tree Features

The MST provides the basis for the Christoﬁdes–Serdyukov and double-tree ap-
proximation algorithms, which give feasible solutions with optimality guarantees
for a symmetric, metric TSP. Taking inspiration from these approximation al-
gorithms, we use multiple MSTs to extract edges from the underlying graph
that are likely to belong to the optimal solution, thereby allowing us to compute
features using them.

T “ pV, E1 Ă Eq Ð MST(G)

Algorithm 1 Minimum Spanning Tree Fea-
tures
Input: G “ pV, Eq
1: H Ð pV, E2 “ Hq
2: for k P t1, 2, ..., rlog nsu do
3:
4: G “ pV, Eq Ð pV, EzE1q
5: H “ pV, E2q Ð pV, E2 Y E1q
6: wH peq “ 1{k @e P E1
Output: H “ pV, E2q

First, a new graph H “ pV, Hq is
initialised with the vertex set but
not the edge set of the complete
graph G “ pV, Eq “ Kn. For k !
n iterations the MST T “ pV, E1q
of G is computed and the edges E1
are removed from E and added to
the edge set of H. Then, at each
step, GpV, Eq Ð GpV, EzE1q, giv-
ing a new MST at each iteration
with unique edges. The iteration
at which edges are added to the
graph H is stored, so that a feature ˆqk
ij “ 1i{k may be computed, where 1i is the
indicator, taking unit value if the edge e “ pi, jq P E was extracted at iteration
i and zero otherwise.

Since the value of k should be small, the vast majority of the edges will have
zero-valued feature-values. This edge transferal mechanism can be used as a
sparsiﬁcation method itself: the original graph can be pruned such that the only

Learning to Sparsify TSP Instances

7

remaining weighted edges are those that were identiﬁed by the successive MSTs,
with the resulting graph containing kpn ´ 1q edges.

3.3 Local Features

The work of Sun et al. [28] constructs four local features, comparing weights of
an edge pi, jq P E to the edge weights pk, jq, k P V and pi, kq, k P V . Analysis of
these local features shows that they guide the decision-making process of classi-
ﬁers most strongly. While relatively inexpensive to compute, yet less expensive
features can be computed, comparing a given edge weight pi, jq to the maximum
and minimum weights in E. That is, for each pi, jq P E we compute a set of
features qij as:

qa
ij “ p1 ` Aijq{p1 ` max
pl,kqPE

Alkq

qb
ij “ p1 ` Aijq{p1 ` max
lPV

Ailq

qc
ij “ p1 ` Aijq{p1 ` max
lPV

Aljq

qd
ij “ p1 ` min
pl,kqPE

Alkq{p1 ` Aijq

qe
ij “ p1 ` min
lPV

qf
ij “ p1 ` min
lPV

Ailq{p1 ` Aijq

Aljq{p1 ` Aijq

(7)

(8)

(9)

(10)

(11)

(12)

The motivation for the features (7) and (10) is to cheaply compute features that
relate a given edge weight to the weights of the entire graph in a global manner.
On the other hand, motivated by the work of Sun et al. [28], the features (8),
(9), (11), (12), compare a given edge weight to those in its direct neighbourhood;
the edge weight associated with edge pi, jq is related only to the weights of the
associated vertices i and j.

3.4 Postprocessing Pruned TSP Graphs

In this setting, sparsiﬁcation is posed as a set of m independent classiﬁcation
problems. The result of this is that there is no guarantee that any feasible solution
exists within a pruned problem instance. Indeed, even checking that any tour
exists within a sparsiﬁed graph is itself an NP-hard problem. One can guarantee
feasibility of the pruned graph by ensuring that the edges belonging to some
known solution exist in the pruned graph; this forces both connectivity and
Hamiltonicity (see Figure 1). The pruned graph has at least one feasible solution
and admits an optimal objective no worse than that of the solution that is
known. For the TSP we can construct feasible tours trivially by providing any
permutation of the vertices, but it is likely that such tours will be far from
optimal. Assuming the problem is metric, we can use an approximation algorithm
to construct a feasible solution to the problem that also gives a bound on the
quality of the solution.

8

Learning to Sparsify TSP Instances

Fig. 1: The pruned graph H does not admit a feasible solution. Given a known
solution, we add the edges E2 of the solution to the pruned graph in order to
guarantee the feasibility of the pruned instance. If we obtain E2 using an ap-
proximation algorithm, then we can make guarantees about the quality of the
solutions obtained from H.

4 Experiments and Results

All experiments were carried out in Python3. Graph operations were performed
using the NetworkX package [14] and the training was carried out with the Scikit-
Learn package [22]. The linear programming features were computed using the
Python interfaces for the Xpress and SCIP optimisation suites [2, 20]. Training
and feature computation was performed on a Dell laptop running Ubuntu 18.04
with 15. 6 GB of RAM an, Intel® Core™ i7-9750H 2.60GHz CPU and an Nvidia
GeForce RTX 2060/PCIe/SSE2 GPU.

4.1 Learning to Sparsify

First we train a classiﬁcation model to prune edges that are unlikely to belong
to an optimal tour. That is, given the feature representation

(cid:126)qij “ rqa

ij, qb

ij, qc

ij, qd

ij, qe

ij, qf

ij, ˆ(cid:126)rk

ij, ˜(cid:126)rij, ˆqk

ijsT

for the edge pi, jq, we aim to train a machine learning model that can classify
all the edges of a given problem instance in this manner. In each case we let
the parameter k “ rlog2pnqs, since this number grows slowly with the order n
of the graph and prevents excessive computation. In order to address the eﬀects
of class imbalance, we randomly undersample the negative class such that the
classes are equally balanced, and adopt class weights t0.01, 0.99u for the negative
and positive classes respectively to favour low false negative rates for the positive
class, favouring correctness over the sparsiﬁcation rate.

In many instances there exists more than one optimal solution. We compute all
optimal solutions and assign unit value to the label for edges that belong to any
optimal solution. Any optimal solution gives an upper bound on the solution

3 All code available at: https://github.com/JamesFitzpatrickMLLabs/optlearn

Learning to Sparsify TSP Instances

9

for any subsequent attempts to solve the problem. To compute each solution,
one can introduce a tour elimination constraint to the problem formulation to
prevent previous solutions from being feasible. The problem can then be re-
solved multiple times to obtain more solutions to the problem, until no solution
can be found that has the same objective value as that of the original optimal
tour.

Training was carried out on problem instances of the MATILDA problem set [27],
since they are small (n “ 100 for each) and permit relatively cheap labelling op-
erations. One third p63q of the CLKhard and LKCChard problem instances were
chosen for the training set, while the remaining problem instances are retained
for the testing and validation sets. These two problem categories were selected for
training once it was identiﬁed through experimentation that sparsiﬁers trained
using these problems generally performed better (with fewer infeasibilities). This
is in line with the ﬁndings of Sun et al. [28]. Since each of the linear program-
ming features must be computed for any given problem instance, it is worthwhile
checking if any solution ˆ(cid:126)x0 or ˜(cid:126)xk is an optimal TSP solution, which would allow
all computation to terminate at this point. All of the edges of a given graph
belong to exactly one of training, test or validation sets, and they must all be-
long to the same set. Each symmetric problem instance in the TSPLIB problem
set [24] for which the order n ď 4432 is retained for evaluation of the learned
sparsiﬁer only. Sample weights wij “ Aij{ maxij Aij are applied to each sample
to encourage the classiﬁer to reduce errors associated with longer edges.

Following pruning, edges of a known tour are inserted, if they are not already
elements of the edge set of the sparsiﬁed graph. In this work, we compute both
the double-tree and Christoﬁdes approximations , since they can be constructed
in polynomial time and guarantee that the pruned graph will give an optimality
ratio ˆ(cid:96)opt{(cid:96)opt ď 3{2, where ˆ(cid:96)opt is the optimal tour length for a pruned problem
and (cid:96)opt is the optimal tour length for the original problem. The performance
of the classiﬁer is evaluated using the optimality ratio ˜(cid:96) “ ˆ(cid:96)opt{(cid:96)opt and the
retention rate ˜m “ ˆm{m, where ˆm is the number of edges belonging to the
pruned problem instance. Since the pruning rate 1 ´ ˜m implies the same result
as the retention rate, we discuss them interchangeably. Introducing any known
tour guarantees feasibility at a cost in the sparsiﬁcation rate no worse than
p ˆm ` nq{ ˆm.

4.2 Performance on MATILDA Instances

The sparsiﬁcation scheme was ﬁrst evaluated on the MATILDA problem in-
stances, which are separated into seven classes, each with 190 problem instances.
Logistic regression, random forest and linear support vector classiﬁers were eval-
uated as classiﬁcation models. Although no major advantage was displayed after
a grid search over these models, here, for the sake of comparison with Sun et al.,
an L1-regularised SVM with an RBF kernel is trained as the sparsiﬁer. We can
see from Table 1 that the optimality ratios observed in the pruned problems are

10

Learning to Sparsify TSP Instances

comparable to those of Sun et al, with a small deterioration in the optimality
gap for the more diﬃcult problems (around 0.2%) when the same problem types
were used for training. However, this comes with much greater sparsiﬁcation
rates, which leads to more than 85% of the edges being sparsiﬁed in all cases
and almost 90% on average, as opposed to the around 78% observed by Sun et
al. We similarly observe that the more diﬃcult problems are pruned to a slightly
smaller extent than the easier problem instances.

Problem Class

Statistic (Mean) CLKeasy CLKhard LKCCeasy LKCChard easyCLK-hardLKCC hardCLK-easyLKCC random
1.00032
1.00385
Optimality Ratio 1.00000
0.89444
0.88298
0.90389
Pruning Rate
Table 1: Evaluation of the trained sparsiﬁer against the problem subsets of the
MATILDA instances. Here each cell indicates the mean value of the optimal-
ity ratio or pruning rate over each subset of problems (not including training
instances)

1.00176
0.89021

1.00049
0.88874

1.00035
0.90357

1.00000
0.91086

4.3 Pruning With and Without Guarantees

This section describes experiments carried out to test how much the sparsi-
ﬁer would need to rely on inserted tours to produce feasibly reduced prob-
lem instances. The optimality ratio statistics are shown in Table 2. Before
post-processing, pruning was achieved with a maximum rate of edge retention
maxt ˆm{mu “ 0.179, a minimum rate mint ˆm{mu “ 0.140 and a mean rate
x ˆm{my “ 0.143 among all problem instances. In the vast majority of cases, at
least one optimal tour is contained within the pruned instance (column 2), or one
that is within 5% of optimal (column 4). For the CLKhard problem instances,
there were one to three infeasible instances produced each time. Only in the case
of the random problem instances did the number of pruned problems containing
optimal solutions increase after inserting the approximate tour edges (column
2). In some cases (for easyCLK-hardLKCC, CLKhard and random) the pruned
instances admitting sub-optimal solutions with respect to the original problem
had improved optimality ratios (columns 3 and 4). The worst-case optimality
ratios changes little for each problem class after post-processing, except for the
case (CLKhard) where previously infeasibile pruned instances made it impossible
to compute ˜(cid:96) (column 5). The distribution of these values with post-processing
is depicted in Figure 2.
This sparsiﬁcation scheme was also tested on the TSPLIB problem instances
(see Table 3). Before post-processing, for the majority of the problem instances,
an optimal tour is contained within the sparsiﬁed graphs (column 2), and the
vast majority of the problem instances have optimality ratios no worse than
˜(cid:96) “ 1.05 (column 4). Unlike with the MATILDA instances, the pruning rate

Learning to Sparsify TSP Instances

11

Problem Class

CLKeasy
CLKhard
LKCCeasy
LKCChard
easyCLK-hardLKCC
hardCLK-easyLKCC
random

# of Problems With Below Condition True Worst Case
ˆ(cid:96)opt{(cid:96)opt “ 1.0 ˆ(cid:96)opt{(cid:96)opt ă 1.02 ˆ(cid:96)opt{(cid:96)opt ă 1.05 maxtˆ(cid:96)opt{(cid:96)optu

190 Ñ 190
122 Ñ 122
190 Ñ 190
89 Ñ 89
180 Ñ 180
184 Ñ 184
174 Ñ 179
1129 Ñ 1134

190 Ñ 190
182 Ñ 188
190 Ñ 190
184 Ñ 184
185 Ñ 190
190 Ñ 190
189 Ñ 190
1310 Ñ 1324

190 Ñ 190
185 Ñ 190
190 Ñ 190
189 Ñ 189
190 Ñ 190
190 Ñ 190
190 Ñ 190
1324 Ñ 1329

1 Ñ 1
8 Ñ 1.024
1 Ñ 1
1.054 Ñ 1.054
1.010 Ñ 1.010
1.015 Ñ 1.015
1.046 Ñ 1.0043
8 Ñ 1.054

Total
Table 2: Optimality ratio statistics before and after approximate tour insertion
following learned sparsiﬁcation (MATILDA problem instances). Each cell indi-
cates the number of problem instances of each class for which the optimality ratio
resided within the bounds stated before and after post-processing. For example,
the cell in column 2 containing 174 Ñ 179 indicates that 174 purely sparsiﬁed in-
stances admitted unit optimality ratios, but 179 did so after the post-processing.
The last column contains the maximum optimality gap for each class. If there
are infeasible sparsiﬁed graphs, this value is denoted by 8.

Fig. 2: Each point represents a problem instance, showing the pruning rate and
the optimality ratio achieved for each problem in the MATILDA benchmark set.

varies signiﬁcantly. This is in accordance with the ﬁndings of Sun et al. [28] and
indicates that the sparsiﬁer is less certain about predictions made, and fewer
edges are therefore removed in many cases. For some smaller problem instances
(smaller than the training set problems) the pruning rate approaches 0.5167.
The median pruning rate was 0.9109, with the mean pruning rate at 0.8904 and
a standard deviation of 0.086. The highest pruning rate was for the problem
d657.tsp, for which 0.9618m edges were removed, achieving an optimality ratio

12

Learning to Sparsify TSP Instances

of 1.0. Three instances were found to be infeasible under this scheme without
approximation, si535.tsp, pr299.tsp, and pr264.tsp.

Introducing the approximate tours brought the worst-case optimality ratio to
1.01186 (column 5) for pr264.tsp, with median and mean pruning rates of, re-
spectively, 0.8991 and 0.8718. The mean optimality ratio (excluding the infeasi-
ble instances) before double-tree insertion was 1.00092 and (also without these
same problems) 1.00073 after insertion, indicating that in most cases there is
little reduction in solution quality as a result of sparsiﬁcation. In Figure 3 we
can see depicted the relationship between the problem size (the order, n) and the
pruning rate. Almost all of the instances retain optimal solutions after pruning,
those that don’t are indicated by the colour scale of the points.

55 Ñ 56

Worst Case

# of Problems With Below Condition True
Problem Class ˆ(cid:96)opt{(cid:96)opt “ 1.0 ˆ(cid:96)opt{(cid:96)opt ă 1.005 ˆ(cid:96)opt{(cid:96)opt ă 1.010 maxtˆ(cid:96)opt{(cid:96)optu
8 Ñ 1.01186
69 Ñ 70
TSPLIB
Table 3: Optimality ratio statistics before and after approximate tour insertion
following learned sparsiﬁcation (TSPLIB problem instances). Results for each
symmetric problem instance with n ď 1000 using a logistic regression sparsiﬁer.
This set contained 76 problem instances, some of which are not metric TSPs.
Although we cannot make provable guarantees for the non-metric problem in-
stances, we can still use the approximate tours to ensure feasibility.

72 Ñ 73

.

4.4 Minimum Weight Spanning Tree Pruning

Empirical experiments demonstrated that using only successive MSTs, as out-
lined in Section 3.2, one can eﬀectively sparsify symmetric TSP instances. This
scheme proceeds without the need for a classiﬁer, by building a new graph H
with only the edges of the MSTs and their associated edge weights in G. This
sparsiﬁed instance achieves a retention rate of kn{m, where k is the number
of trees computed. In many cases, simply using this as a scheme for selecting
edges for retention was suﬃcient to sparsify the graph while achieving a unit
optimality ratio.

On the MATILDA problems, again with k “ rlog2pnqs, thhis achieves a worst-
case optimality ratio of 1.0713 and a uniform pruning rate of 0.86, with the
majority of the pruned instances containing an optimal solution. Under these
conditions the easier problems (CLKeasy and LKCCeasy) had optimal tours
preserved in every problem instance except for one. For the more diﬃcult prob-
lem instances, in the vast majority of cases, the ratio does not exceed 1.02 (see
Table 4). Insertion of approximate tours in this scenario does not lead to much

Learning to Sparsify TSP Instances

13

Fig. 3: Here each point represents a problem instance. The horizontal axis depicts
the problem size in terms of the number of nodes. The vertical axis shows the
sparsiﬁcation rate and the colouring of the points indicates the optimality ratio
observed.

improvement, since every instance of the MATILDA problem set is sparsiﬁed
feasibly in this scheme.

Problem Class

# of Problems With Below Condition True Worst Case
ˆ(cid:96)opt{(cid:96)opt “ 1.0 ˆ(cid:96)opt{(cid:96)opt ă 1.02 ˆ(cid:96)opt{(cid:96)opt ă 1.05 maxtˆ(cid:96)opt{(cid:96)optu

190 Ñ 190
185 Ñ 185
190 Ñ 190
183 Ñ 183
190 Ñ 190
190 Ñ 190
190 Ñ 190
1318 Ñ 1330

CLKeasy
190 Ñ 190
CLKhard
85 Ñ 88
LKCCeasy
189 Ñ 190
77 Ñ 78
LKCChard
easyCLK-hardLKCC 173 Ñ 190
hardCLK-easyLKCC 171 Ñ 173
176 Ñ 176
random
1061 Ñ 1330

1 Ñ 1
1.029 Ñ 1.029
1.002 Ñ 1.002
1.071 Ñ 1.054
1.012 Ñ 1.012
1.015 Ñ 1.015
1.006 Ñ 1.006
1.071 Ñ 1.054
All
Table 4: Optimality ratio statistics before and after approximate tour insertion
following MST sparsiﬁcation (MATILDA problem instances). Comparison be-
tween the pure multiple MST pruning scheme and the same scheme with double-
tree post-processing. In every case for the MATILDA problem set, the pruned
instances are feasible.

190 Ñ 190
190 Ñ 190
190 Ñ 190
189 Ñ 189
190 Ñ 190
190 Ñ 190
190 Ñ 190
1329 Ñ 1330

.
For the TSPLIB problems, all but two can be sparsiﬁed feasibly without the in-
sertion of approximate edges: p654.tsp and ﬂ417.tsp. Since k is a function of the
problem order, n “ 100, 86% of the edges of a graph will be removed, whereas at
order n “ 1000, the pruning rate reaches 98%. The worst optimality ratio for any

14

Learning to Sparsify TSP Instances

feasible problem was for pr226.tsp, at ˜l “ 1.106, with a retention rate of 0.071.
The majority of the sparsiﬁed instances retained an optimal tour (see Table
5), with the mean optimality ratio x˜(cid:96)y for all feasible problems taking the value
1.0061. Including approximate tour edges in the sparsiﬁed graphs results in none
having an optimality gap greater than 1.084 (pr654.tsp, which was previously in-
feasible, with a retention rate of 3.1%). The other previously infeasibly-sparsiﬁed
problem, ﬂ417.tsp admits an optimality ratio ˜(cid:96) “ 1.077, retaining just 4.3% of
its edges. The mean optimality ratio for all problems emerged as 1.0053.

# of Problems With Below Condition True Worst Case

Problem Class ˆ(cid:96)opt{(cid:96)opt “ 1.0 ˆ(cid:96)opt{(cid:96)opt ă 1.02 ˆ(cid:96)opt{(cid:96)opt ă 1.05 maxtˆ(cid:96)opt{(cid:96)optu
66 Ñ 67
TSPLIB
Table 5: Optimality ratio statistics before and after approximate tour insertion
following MST sparsiﬁcation (TSPLIB problem instances). For each problem
instance the number of trees k diﬀered, according to the order n.

8 Ñ 1.084

51 Ñ 51

71 Ñ 71

.

4.5 Specifying the Pruning Rate

One of the advantages of including the post-processing step that guarantees the
feasibility of the pruned instance is that we can exert greater control over the
pruning rate. Training a sparsiﬁer in the manner outlined above necessitates
a trade-oﬀ between the optimality ratio and the pruning rate. Higher pruning
rates result in sparsiﬁed problems that are easier to solve but that have typically
poorer optimality ratios. Indeed, if the sparsiﬁcation rate is too high, many of
the sparsiﬁed instances will also become infeasible. Including approximate tours,
however, allows us to choose eﬀectively any decision threshold for the classiﬁer,
ranging from total sparsiﬁcation and removing all edges, or choosing a threshold
that results in no sparsiﬁcation at all and the retention of all edges from the
original graph. So long as there is at least one known feasible tour, regardless of
its quality, then the pruned instance will be feasible after post-processing.

5 Discussion and Conclusions

In this work we have demonstrated that it is possible to learn to eﬀectively spar-
sify TSP instances, pruning the majority of the edges using a small classiﬁcation
model while relying on linear programming and graph-theoretic features that can
be eﬃciently computed. Providing guarantees of feasibility is possible by means
of inserting edges belonging to approximate tours, and this ensures that even
out-of-distribution problem instances can be tackled with this scheme. These fea-
tures are supplemented with local statistical features, comparing edge weights to
the global and neighbouring distributions. This scheme successfully generalises

Learning to Sparsify TSP Instances

15

to larger problem instances outside of the training distribution. Where there is
a lack of training data or where expediency is favoured over improvements that
can be obtained from training a sparsiﬁer, it has been shown that the MST ex-
traction mechanism with inserted doubletours performs well on most problem
instances.

The motivation of this work has been to use the methods of ML to aid in the
design of heuristics for solving combinatorial optimisation problems. This is in
the hope that such an approach can reduce the development time required.
Development time for ML solutions depends typically on the engineering of ei-
ther features or problem-speciﬁc models and architectures. This can eﬀectively
transfer the the engineering eﬀort from one task to another, without producing
tangible beneﬁts. In this approach, classical ML models are used, which means
that feature design is paramount to the success of the model. Here, LP features
are designed that are not dependent upon the formulation of the problem; any
TSP problem that requires formulation as binary IP with subtour-elimination
(or capacity) constraints can have features produced in the same manner. This is
attractive because it simply requires knowledge of IP formulations that give tight
relaxations. Such relaxations may also be suﬃcient to solve the desired problem
to begin with, potentially obviating the need for any further computation. Opti-
mality ratio performance does not appear to depend on the problem size, which
indicates that this pre-processing scheme might help extend the applicability of
learned solving heuristics.

The use of the double-tree approximation guarantees the feasibility of the pruned
problem instance, but it does not provide a tight bound on the optimality ratio.
Tighter bounds can be achieved by using stronger approximation algorithms,
such as that of Christoﬁdes and Serdyukov [7, 26], at a cost of greater running
times. Alternatively, one could use multiple approximation algorithms, in order
to increase the number of tours inserted into the pruned instance, or compute
successive approximations by removing from consideration edges belonging to
a tour once they have been computed, analogously to Algorithm 1. The results
obtained indicate that, in the case of the double-tree, it did not improve the
worst-case performance in general, but did improve the mean performance in
terms of the optimality ratio. This suggests that although the additional edges
do not lead to signiﬁcant improvements, it is not necessary in most cases to
rely on them to obtain acceptable optimality ratios, just to guarantee feasibility.
Therefore, insertion of any feasible tour may be suﬃcient for routing problems
where no eﬀective or eﬃcient approximation scheme exists.

This scheme has the potential to be developed in a similar manner for other
routing problems, in particular vehicle routing problems, for which solvers are
not as eﬀective in practice as they are for the TSP. To realise the beneﬁts of
scheme, an implementation would also have to be rewritten in a lower level
language. Subsequent work could be done to evaluate the smallest problem sizes

16

Learning to Sparsify TSP Instances

for which training can be carried out eﬀectively and applicability to the VRP,
for which sparsiﬁcation is already a natural process if there are time windows.

Acknowledgements

This work was funded by Science Foundation Ireland through the SFI Centre
for Research Training in Machine Learning (18/CRT/6183).

References

1. David L Applegate, Robert E Bixby, Vaˇsek Chv´atal, William Cook, Daniel G
Espinoza, Marcos Goycoolea, and Keld Helsgaun. Certiﬁcation of an optimal tsp
tour through 85,900 cities. Operations Research Letters, 37(1):11–15, 2009.

2. Robert Ashford. Mixed integer programming: A historical perspective with xpress-

mp. Annals of Operations Research, 149(1):5, 2007.

3. Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio.
Neural combinatorial optimization with reinforcement learning. arXiv preprint
arXiv:1611.09940, 2016.

4. Andr´as A Bencz´ur. Approximate st min-cuts in o (nˆ 2) time. In Proc. 28th ACM

Symposium on Theory of Computing, 1996, 1996.

5. Heinrich Braun. On solving travelling salesman problems by genetic algorithms. In
International Conference on Parallel Problem Solving from Nature, pages 129–133.
Springer, 1990.

6. G. A. Di Caro. A survey of machine learning for combinatorial optimization. In

30th European Conference on Operations Research (EURO), 2019.

7. Nicos Christoﬁdes. Worst-case analysis of a new heuristic for the travelling sales-
man problem. Technical report, Carnegie-Mellon Univ Pittsburgh Pa Management
Sciences Research Group, 1976.

8. George Dantzig, Ray Fulkerson, and Selmer Johnson. Solution of a large-scale
traveling-salesman problem. Journal of the operations research society of America,
2(4):393–410, 1954.

9. Marco Dorigo and Luca Maria Gambardella. Ant colonies for the travelling sales-

man problem. biosystems, 43(2):73–81, 1997.

10. Matteo Fischetti and Michele Monaci. Exploiting erraticism in search. Operations

Research, 62(1):114–122, 2014.

11. Wai-Shing Fung, Ramesh Hariharan, Nicholas JA Harvey, and Debmalya Pani-
grahi. A general framework for graph sparsiﬁcation. SIAM Journal on Computing,
48(4):1196–1223, 2019.

12. Marco Grassia, Juho Lauri, Sourav Dutta, and Deepak Ajwani.
multi-stage sparsiﬁcation for maximum clique enumeration.
arXiv:1910.00517, 2019.

Learning
arXiv preprint

13. Prateek Gupta, Maxime Gasse, Elias B. Khalil, Pawan Kumar Mudigonda, An-
drea Lodi, and Yoshua Bengio. Hybrid models for learning to branch. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual, 2020.

Learning to Sparsify TSP Instances

17

14. Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure,
dynamics, and function using networkx. Technical report, Los Alamos National
Lab.(LANL), Los Alamos, NM (United States), 2008.

15. Keld Helsgaun. An eﬀective implementation of the lin–kernighan traveling sales-

man heuristic. European Journal of Operational Research, 126(1):106–130, 2000.

16. John J Hopﬁeld and David W Tank. “neural” computation of decisions in opti-

mization problems. Biological cybernetics, 52(3):141–152, 1985.

17. Stefan Hougardy and Rasmus T Schroeder. Edge elimination in tsp instances. In
International Workshop on Graph-Theoretic Concepts in Computer Science, pages
275–286. Springer, 2014.

18. Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing

problems! arXiv preprint arXiv:1803.08475, 2018.

19. Juho Lauri and Sourav Dutta. Fine-grained search space classiﬁcation for hard
enumeration variants of subset problems. In The Thirty-Third AAAI Conference
on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of
Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Ed-
ucational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA,
January 27 - February 1, 2019, pages 2314–2321. AAAI Press, 2019.

20. Stephen Maher, Matthias Miltenberger, Joao Pedro Pedroso, Daniel Rehfeldt,
Robert Schwarz, and Felipe Serrano. Pyscipopt: Mathematical programming in
python with the scip optimization suite. In International Congress on Mathemat-
ical Software, pages 301–307. Springer, 2016.

21. Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak´ac.
Reinforcement learning for solving the vehicle routing problem. In Advances in
Neural Information Processing Systems, pages 9839–9849, 2018.

22. Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of
machine Learning research, 12:2825–2830, 2011.

23. David Peleg and Alejandro A Sch¨aﬀer. Graph spanners. Journal of graph theory,

13(1):99–116, 1989.

24. Gerhard Reinelt. Tsplib—a traveling salesman problem library. INFORMS Journal

on Computing, 3(4):376–384, 1991.

25. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
IEEE Transactions on Neural

Monfardini. The graph neural network model.
Networks, 20(1):61–80, 2008.

26. A.I. Serdyukov. On some extremal walks in graphs. Upravlyaemye Sistemy 17:

76–79, (17):76–79, 1978.

27. Kate Smith-Miles, Jano van Hemert, and Xin Yu Lim. Understanding tsp diﬃculty
by learning from evolved instances. In International Conference on Learning and
Intelligent Optimization, pages 266–280. Springer, 2010.

28. Yuan Sun, Andreas Ernst, Xiaodong Li, and Jake Weiner. Generalization of ma-
chine learning for problem reduction: a case study on travelling salesman problems.
OR Spectrum, pages 1–27, 2020.

29. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In Advances in neural information processing systems, pages 5998–6008, 2017.
30. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances

in neural information processing systems, pages 2692–2700, 2015.

18

Learning to Sparsify TSP Instances

31. Yong Wang and Jeﬀrey Remmel. A method to compute the sparse graphs for
In International

traveling salesman problem based on frequency quadrilaterals.
workshop on frontiers in algorithmics, pages 286–299. Springer, 2018.

