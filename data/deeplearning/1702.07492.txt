7
1
0
2

b
e
F
4
2

]

O
R
.
s
c
[

1
v
2
9
4
7
0
.
2
0
7
1
:
v
i
X
r
a

RobotgainsSocialIntelligencethroughMultimodalDeepReinforcementLearningAhmedHussainQureshi,YutakaNakamura,YuichiroYoshikawaandHiroshiIshiguroAbstract—Forrobotstocoexistwithhumansinasocialworldlikeours,itiscrucialthattheypossesshuman-likesocialinteractionskills.Programmingarobottopossesssuchskillsisachallengingtask.Inthispaper,weproposeaMultimodalDeepQ-Network(MDQN)toenablearobottolearnhuman-likeinteractionskillsthroughatrialanderrormethod.Thispaperaimstodeveloparobotthatgathersdataduringitsinteractionwithahuman,andlearnshumaninteractionbehaviorfromthehighdimensionalsensoryinformationusingend-to-endreinforcementlearning.Thispaperdemonstratesthattherobotwasabletolearnbasicinteractionskillssuccessfully,after14daysofinteractingwithpeople.I.INTRODUCTIONHuman-robotinteraction(HRI)isanemergingﬁeldofresearchwiththeaimtointegraterobotsintohumansocialenvironments.Oneofthebiggestchallengesinthedevelop-mentofsocialrobotsistounderstandhumansocialnorms[1].Itisthereforeessentialforsocialrobotstopossessdeepmodelsofsocialcognition,andbeabletolearnandadaptinaccordancewiththeirsharedexperienceswithhumanpartners.Mostofthesocialrobotstodateareeitherpre-programmed,orarecontrolledbyteleoperationorsemi-autonomousteleoperation[2],anddonotpossesstheabilitytolearnandupdatethemselves.Designinganadaptableandautonomoussociablerobotisparticularlychallenging,astherobotneedstocorrectlyinterprethumanbehaviorsaswellasrespondappropriatelytothem.Thisisnecessarytoensuresafe,naturalandeffectivehuman-robotinteraction.Arguably,mostoftheso-calledsocialrobotshavelimitedsocialinteractionskills.Oneofthemainreasonsforthislimitedcapabilityisthediversityinhumanbehavior[3].Socialinteractionbetweenhumansreliesonintentioninferencesuchasinferringtheintentionfromwalkingtrajectories,directionofeyegaze,facialexpressions,bodylanguageandactivityinprogress.Programmingarobottorecognizehumanintentionfromtheaforementionedfactorsandrespondtodiversehumanbehaviorsisnotoriouslydifﬁcult,asitishardtoenvisioneachandeveryoneofthecountlesspossibleinteractionsce-narios.Therefore,itisnecessaryforasocialrobottopossessaself-learningparadigm[4]whichenablesittolearndeep*ThisworkispartlysupportedbyJSPSGrant-in-AidforYoungScientists(B)14444719.A.H.Qureshi,Y.Nakamura,Y.YoshikawaandH.IshiguroarewithDepartmentofSystemInnovation,GraduateSchoolofEngi-neeringScience,OsakaUniversity,1-3Machikaneyama,Toyonaka,Osaka,Japan.{qureshi.ahmed,nakamura,yoshikawa,ishiguro}@irl.sys.es.osaka-u.ac.jpA.H.Qureshi,Y.YoshikawaandH.IshiguroarealsowithJSTERATOISHIGUROSymbioticHuman-RobotInteractionProject.Fig.1:Robotlearningsocialinteractionskillsfrompeople.modelsofhumansocialcognitionfromfeaturesextractedautomaticallyfromhigh-dimensionalsensoryinformation.Recently,theﬁeldofdeeplearning,alsoknownasrep-resentationlearning,hasemergedandithasachievedmanybreakthroughsonvarioustasksofcomputervision[5][6][7]andspeechrecognition[8][9].Deeplearningmethodstakerawsensoryinformationasinputandprocessittolearnmultiplelevelsofrepresentationautomatically,whereeachlevelofrepresentationcorrespondstoaslightlyhigherlevelofabstraction[5][6].Furtheradvancementsinma-chinelearninghavemergeddeeplearningwithreinforcementlearning(RL)whichhasledtothedevelopmentofthedeepQ-network(DQN)[10].DQNutilizesanautomaticfeatureextractorcalleddeepconvolutionalneuralnetwork(Con-vnets)toapproximatetheaction-valuefunctionofQ-learningmethod[10].DQNhasdemonstrateditsabilitytolearnfromhigh-dimensionalvisualinputtoplayarcadevideogamesathumanandsuperhumanlevel.However,theapplicabilityofDQNtorealworldhuman-robotinteractionproblemshasnotbeenexploredyet.Inthisresearch,weaugmentourrobotwithamultimodaldeepQ-network(MDQN)whichenablestherobottolearnsocialinteractionskillsthroughinteractionwithhumansinpublicplaces.TheproposedMDQNusestwostreamsofconvolutionalneuralnetworksforaction-valuefunctionestimation.Thedualstreamconvnetsprocessthedepthandgrayscaleimagesindependently.Weconsiderascenarioinwhichtherobotlearnstogreetpeopleusingasetoffourlegalactions,i.e.,waiting,lookingtowardshuman,wavinghandandhandshak-ing.Theobjectiveoftherobotistolearnwhichactiontoperformineachsituation.Weconductedtheexperimentatdifferentlocationssuchasacafeteria,departmentreception,variouscommonrooms,etc.,asshowninﬁgure1.After14daysofinteractingwithpeople,therobotexhibitedaremarkablelevelofbasicsocialintelligence.Therobotsocial 
 
 
 
 
 
interactionskillswerealsoevaluatedontestdatanotseenbythesystemduringtraining.Withthispaperwereleasethesourcecodeandthedepthdataset1collectedduringtheexperiment.Duringtheexperiment,wecollectedbothgrayscaleanddepthframesbutduetoprivacyconcernsweareonlyreleasingthedepthdataset.Therestofthepaperisorganizedasfollows.SectionIIprovidesdiscussionontherelatedwork,sectionIIIprovidesabriefbackgroundofreinforcementlearningandtheDQNarchitecture,sectionIVexplainstheproposedmultimodaldeepQ-learnerarchitecturewhilesectionVdescribestheexperimentalsetup.SectionVIandVIIgivetheresultsoftheexperimentandthediscussiononwhattherobothaslearnedthroughinteractionwithhumans,respectively.Finally,sectionVIIIconcludesthepaperandalsosuggestssomefutureareasofresearchinthisparticulardomain.II.RELATEDWORKTheproposedworkutilizesdeepQ-learningtoenablearobottolearnsocialinteractionskillsfromexperienceinter-actingwithpeople.Thissectiondescribesrelatedresearchfromtheﬁeldsofhuman-robotinteraction,deeplearninganddeepreinforcementlearning.ThemostrelevantpriorworkinHRIincludestheworkbyAmoretal.[11][12],Leeetal.[13],andWangetal.[14].Theproposedmethodsin[11][12][13]learnresponsiverobotbehaviorbyimitatinghumaninteractionpartners.Themovementsoftwopersons,action-reactionpairs,arerecordedduringthehuman-humaninteractionwithamotioncapturesystem.Aninteractionmodelislearnedfromthedata,whichenablesarobottocomputethebestresponsetoahumaninteractionpartner’scurrentbehavior.However,themotioncapturesystemusedfordatarecordingisnotuser-friendlyanditdoesnotyieldnaturalhumanbehavior,astheparticipantshavetoweartrack-ablemark-ers.In[14],theauthorspresentaprobabilisticgraphicalmodelwithintentionsrepresentedaslatentstates,wherethemappingfromobservationstolatentstatesisapproxi-matedbyaGaussianProcess.Theproposedmodelallowsintentioninferencefromobservedmovements.However,webelievethat,inthecaseofHRI,intentioninferenceisnotdependentonbodymovementsonlybutalsooneyegaze,bodylanguage,walkingtrajectories,activityinprogressetc.Therefore,limitingintentioninferencetobodymovementsalonedoesnotseempromising.Furthermore,thepriorartstatedaboveconsidersonlyonehumaninteractionpartnerfortherobotinthescene.Inthispaper,morecomplexscenariosareconsideredwheretherobotcanbeapproachedbyagroupofpeoplewillingornotwillingtointeractwithit.Sofar,thedeeplearninganddeepreinforcementlearning(DRL)researchhasbeenappliedtoareas,thoughincluderobotics,whichhavelittletodowiththedomainofhuman-robotinteraction.Also,mostoftheseapplicationsarelimited1Togetthesourcecodeandthedatasetpleasevisithttps://sites.google.com/a/irl.sys.es.osaka-u.ac.jp/member/home/ahmed-qureshi/deephritosimulatedenvironments.Predictinghumanintentionfromvideodatahasonlyrecentlybeenaddressedindeeplearningliterature[15].Ourworkdiffersfromaforementionedworkbecause,inourcase,therobotisactinginareal,uncontrolledenvironment;bytakinganactiontherobotmayaffectthehumanintention.Therefore,therobothastoperceivehumanbehavior,aswellas,itsownactionsaccordingtohumansocialnorms.FromthedomainofDRL,theideaofdeepQ-learninghasrecentlybeenextendedtotheroboticsﬁeldtosolvetwentycontinuouscontrolproblems;suchasleggedlocomotion,cardriving,cartpoleswing-upetc.[16].However,thesemethodshavenotyetbeenextendedtothedomainofhuman-robotinteractionorinrealworldenvi-ronments.Tothebestknowledgeoftheauthorstheredoesnotexistanyworkthatutilizesdeeplearningcoupledwithreinforcementlearningforrealizationofphysicalhuman-robotsocialinteraction.III.BACKGROUNDWeconsiderastandardreinforcementlearningformulationinwhichanagentinteractssequentiallywithanenvironmentEwithanaimofmaximizingcumulativereward.Ateachtime-step,theagentobservesastatest,takesanactionatfromthesetoflegalactionsA={1,···,K}andreceivesascalarrewardrtfromtheenvironment.Anagent’sbehaviorisformalizedbyapolicyπ,whichmapsstatestoactions.ThegoalofaRLagentistolearnapolicyπthatmaximizestheexpectedtotalreturn.Theexpectedtotalreturnisthesumofrewardsdiscountedbyfactorγ:[0,1]ateachtime-step(γ=0.99fortheproposedwork)i.e.,Rt=PTt0=tγt0−trt0,whereTisthestepatwhichtheagent’sinteractionwiththeenvironmentterminates.Furthermore,theaction-valuefunctionQπ(s,a)istheexpectedreturnwhentakingtheactionainstatesunderthepolicyπ,Qπ(s,a)=E[Rt|st=s,at=a,π].Themaximumexpectedreturnthatcanbeachievedbyfollowinganypolicyisgivenbytheoptimalaction-valuefunctionQ∗(s,a)=maxQπ(s,a).Theoptimalaction-valuefunctionobeysafundamentalrecursiverelationshipknownastheBellmanequation:Q∗(s,a)=E[r+γmax0aQ∗(s0,a0)|s,a].Theintuitionbehinditisthat:giventhattheoptimalaction-valuefunctionQ∗(s0,a0)ofthesequences0atnexttime-stepisdeterministicforallpossibleactionsa0,theoptimalpolicyistoselectanactiona0thatmaximizestheexpectedvalueofr+γQ∗(s0,a0).OneofthepracticesinRL,especiallyQ-learning,istoestimatetheaction-valuefunctionbyusingafunctionesti-matorsuchasneuralnetworksi.e.,Q(s,a)≈Q(s,a,θ).TheparametersθoftheneuralQ-networkareadjustediterativelytowardstheBellmantarget.Recently,anewapproachtoapproximateaction-valuefunction,calleddeepQ-networks(DQN),hasbeenintroducedwhichismuchmorestablethanprevioustechniques.InDQN,theaction-valuefunctionisapproximatedbyadeepconvolutionalneuralnetwork.TheDQNtechniqueforfunctionapproximationdiffersfrompreviousmethodsintwoways:1)Itusesexperiencere-play[17]i.e.,itstorestheagent’sinteractionexperience,et=(st,at,rt,st+1),withtheenvironmentintothereplaymemory,M=e1,···,et,ateachtime-step;2)ItmaintainstwoQ-networks:theBellmantargetiscomputedbythetargetnetworkwitholdparametersi.e.,Q(s,a;θ−),whilethelearningnetworkQ(s,a;θ)keepsthecurrentparameterswhichmaygetupdatedseveraltimesateachtime-step.Theoldparametersθ−areupdatedtocurrentparametersθaftereveryCiterations.InDQN,theparametersoftheQ-networkareadjustediterativelytowardstheBellmantargetbyminimizingthefollowinglossfunction:Li(θi)=E"(cid:18)r+γmax0aQ(s0,a0;θ−i)−Q(s,a;θi)(cid:19)2#(1)Foreachupdate,i,amini-batchissampledfromthereplaymemory.Thecurrentparametersθareupdatedbystochasticgradientdescentinthedirectionofthegradientofthelossfunctionwithrespecttotheparametersi.e.,5Li(θi)=Eh(cid:0)r+γmax0aQ(s0,a0;θ−i)−Q(s,a;θi)(cid:1)5θiQ(s,a;θ)i(2)Finally,theagent’sbehaviorateachtime-stepisselectedbyan(cid:15)-greedypolicywherethegreedystrategyisadoptedwithprobability1−(cid:15)whiletherandomstrategywithprobability(cid:15).IV.THEPROPOSEDALGORITHMTheproposedalgorithmconsistsoftwostreamsthatworkindependently:oneforprocessingthegrayscaleframes,andanotherforthedepthframes.Algorithm1outlinestheproposedmethod.Sincethemodelisdualstream,therefore,theparametersθandθ−consistofparametersofbothnetworks.UnlikeDQN[10],weseparatethedatagenerationandtrainingphase.Eachdayofexperimentcorrespondstoanepisodeduringwhichthealgorithmexecutesboththedatagenerationphaseandthetrainingphase.Followingisabriefdescriptionofbothphases.Datagenerationphase:Duringthedatagenerationphase,thesysteminteractswiththeenvironmentusingQ-networkQ(s,a;θ).Thesystemobservesthecurrentscene,whichcomprisesofgrayscaleanddepthframes,andtakesanactionusingthe(cid:15)-greedystrategy.Theenvironmentinreturnprovidesthescalarreward(pleaserefertosection5(2)forthedeﬁnitionofrewardfunction).Theinteractionexperiencee=(si,ai,ri,si+1)isstoredinthereplaymemoryM.ThereplaymemoryMkeepstheNmostrecentexperienceswhicharelaterusedbythetrainingphaseforupdatingthenetworkparameters.Trainingphase:Duringthetrainingphase,thesystemutilizesthecollecteddata,storedinreplaymemoryM,fortrainingthenetworks.Thehyperparameterndenotesthenumberofexperiencereplay.Foreachexperiencereplay,aminibufferBofsize2000interactionexperiencesisrandomlysampledfromtheﬁnitesizedreplaymemoryM.ThemodelistrainedontheminibatchessampledfrombufferBandthenetworkparametersareupdatediterativelyinthedirectionofthebellmantargets.Therandomsamplingfromthereplaymemorybreaksthecorrelationamongthesamplessincethestandardreinforcementlearningmethodsassumethesamplesareindependentlyandidenticallydistributed.ThereasonfordividingthealgorithmintotwoAlgorithm1:MutlimodalDeepQ-learner.1InitializereplaymemoryMtosizeN2InitializetrainingQ-networkQ(s,a;θ)withparametersθ3InitializetargetQ-networkˆQ(s,a;θ−)withweightsθ−=θ4forepisode=1,Mdo5Datagenerationphase:6Initializethestartstatetos17fori=1,Tdo8Withprobability(cid:15)selectarandomactionatotherwiseselectat=maxaQ(st,a;θ)9st+1,rt←ExecuteAction(at)10Storethetransition(st,at,rt,st+1)inM11Trainingphase:12RandomizeamemoryMforexperiencereplay13fori=1,ndo14SamplerandomminibufferBfromM15whileBdo16Sampleminibatchmoftransitions(sk,ak,rk,sk+1)fromBwithoutreplacementyk=(rk,ifstepk+1isterminalrk+γmaxaˆQ(sk+1,a;θ−),otherwisePerformgradientdescentonloss(yk−Q(sk,ak;θ))2w.r.tthenetworkparametersθ17AftereveryC-episodessyncθ−withθ.phasesistoavoidthedelaythatwouldbecausedifthenetworkweretrainedduringtheinteractionperiod.TheDQNagent[16]worksinacycleinwhichitﬁrstinteractswiththeenvironmentandstoresthetransitionintothereplaymemory,thenitsamplestheminibatchfromthereplaymemoryandtrainsthenetworkonthisminibatch.Thiscycleisrepeateduntilterminationoccurs.ThesequentialprocessofinteractionandtrainingcanbeacceptableonlyinﬁeldsotherthanHRI.InHRI,theagenthastointeractwithpeoplebasedonsocialnorms,so,anypauseordelaywhiletherobotisontheﬁeldisunacceptable.Therefore,wedividethealgorithmintotwostages:intheﬁrststage,therobotgathersdatathroughinteractionforsomeﬁniteperiodoftime,inthesecondstage,itgoestoitsrestposition.Duringtherestingperiod,thetrainingphasegetsactivatedtotrainthemultimodaldeepQ-network(MDQN).V.IMPLEMENTATIONDETAILSThissectionformallydescribesimplementationdetailsoftheresearch.TheMDQNagentwasimplementedFig.2:Dualstreamconvolutionalneuralnetworkintorch/lua2,whilerobotactionswereimplementedus-ingpython.Theentireexperimentwasperformedusing3.40GHz×8IntelCorei7processorwith32GBramandGeForceGTX980graphicprocessingunit.Therestofthesectionexplainstheroboticsystem,MDQNmodelarchitec-ture,visualinformationpre-processingdetails,experimentaldetailsandevaluationprocedure.A.RoboticsystemAldebaran’sPepperrobot3wasusedfortheproposedresearch.Pepperhastwobuilt-in2Dcamerasandone3Dsensor.AlthoughPepperhastwo2Dcameras,onlythetopcamera,locatedonPepper’sforehead,wasusedinthisresearch.TheASUSXtion3Dsensorsituatedbehindtheroboteyeswasutilizedfordepthimages.Boththetop2Dcameraandthe3Dsensorreturnedimageswithresolution320×240at10framespersecond.Moreover,therobot’srighthandwasequippedwithanexternalFSRtouchsensorwhichwashiddenundersoftwoolenglovesforaestheticreasons.Inaddition,therobotwasaugmentedwith1)foursetofactionsthroughwhichitcaninteractwiththepeople;2)arewardfunctionwithwhichtherobotcanevaluatehowwellitisperforming.Followingsubsectionsformallydescribetherobotactionsandtherewardfunction.1)Robotactions:Thisparagraphprovidestheactionsdeﬁnitionandtheirimplementationdetails.Theactionsetcomprisedoffourlegalactions,i.e.,waiting,lookingtowardshumans,wavingitshandandhandshakingwithahuman.Thedescriptionoftheactionsisasfollows:Wait:Forwaiting,therobotrandomlypickstheheadorientationfromtheallowablerangeofheadpitchandheadyaw.Duringthisaction,noattempttoengagethehumanintotheinteractionismade.Looktowardshuman:Thisactionmakestherobotsen-sitivetothestimulicomingfromtheenvironment.Ifrobotsensesanystimulus,itlooksatthestimulusoriginandchecksifthereisanyhumanthereornot.Inthecaseofhumanpresence,therobottracksthepersonwithitsheadinordertoengagehim/herfortheinteractionotherwise,therobotreturnstoitspreviousorientation.Thestimuliusedtoinstillawarenessintotherobotarethesounddetectionandthemovementdetection.2http://torch.ch/3//www.aldebaran.com/en/cool-robots/pepper/ﬁnd-out-more-about-pepper(a)Successfulhandshake.(b)Unsuccessfulhandshake.Fig.3:Instancesofsuccessfulandunsuccesfulhandshakes.Wavehand:Thisisasimplehandwavinggesture.Duringitsexecution,therobotsaysHelloorHi,andattemptstogainpeoples’attentionbytrackingthemwithitshead.Handshake:Inhandshakingaction,therobotraisesitshandtoacertainheightandwaitsatthispositionforafewseconds.Iftheexternaltouchsensor,ontherobot’shand,signalsthetouch,thentherobotgrabstheperson’shandandsaysNicetomeetyou,otherwise,therobot’shandgoesbacktoitspreviousposition.Moreover,whileperformingthisaction,therobotadjustsitsbodyrotationandheadpositioninordertotrackthetargetpersonfromwhomitmaygetthehandshake.2)Rewardfunction:Theexternaltouchsensorontherobot’srighthanddetectsifahandshakehashappenedornot.Thisformsthebasisfortherewardfunction.Therobotgetsarewardof1onthesuccessfulhandshake,-0.1onanunsuccessfulhandshakeand0fortherestofthethreeactions.Figures3(a)and3(b)depictexamplescenariosofsuccessfulandunsuccessfulhandshakesrespectively.Inthescenarioshowninﬁgure3(a),thehandshakehappenssuccessfullythereforetheagentgetstherewardvalue1,whereasinthesituationshowninﬁgure3(b),thepersonistakingtherobot’spicturewhiletherobotisattemptingtoshaketheirhand;sincethisisaninappropriatesocialreaction,theagentwillberewardedwith-0.1.B.ModelArchitectureTheproposedmodelcomprisesoftwostreams,oneforthegray-scaleinformation,andanotherforthedepthinfor-mation.Thestructureofthetwostreamsisidenticalandeachstreamcomprisesofeightlayers(excludingtheinputlayer).Theoverallmodelarchitectureisschematicallyshowninﬁgure2.Theinputstothey-channelandthedepthchannelofthemultimodalQ-networkaregrayscale(198×198×8)anddepthimages(198×198×8),respectively.Sinceeachstreamtakeseightframesasaninput,therefore,thelasteightframesfromthecorrespondingcameraarepre-processedandstackedtogethertoformtheinputforeachstreamofthenetwork.Sincethetwostreamsareidenticalsoweonlydiscussthestructureofoneofthestreams.The198×198×8inputimagesaregiventoﬁrstconvolutionallayer(C1)whichconvolves16ﬁltersof9×9withstride3,followedbyrectiﬁerlinearunitfunction(ReLU)andresultsinto16featuremapseachofsize64×64(wedenotethisby16@64×64).TheoutputfromC1isfedintosub-samplinglayerS1whichapplies2×2max-poolingwiththestrideof2×2.Thesecond(C2)andthird(C3)convolutionallayerconvolve32and64ﬁlters,respectively,ofsize5×5withstride1.Theoutput02468101214020406080100EpisodeTrue positive rate (%)Fig.4:MDQNperformanceontestdatasetovertheseriesofepisodes.fromC2andC3passesthroughthenon-linearReLUfunctionandisfedintosub-samplinglayersS2andS3,respectively.Theﬁnalhiddenlayerisfullyconnectedwith256rectiﬁerunits.Theoutputlayerisfully-connectedlinearlayerwith4units,oneunitforeachlegalaction.C.Pre-processingThepre-processfunctionpreparestheinputappropriatelyforthemodelarchitecture.Theroboticsystemprovidesthegrayscaleandthedepthimagesofsize320×240attheframerateof10fps.Thepre-processfunctionrescalesthegrayscaleandthedepthframeto198×198.Thispre-processingisexecutedontheeightmostrecentgrayscaleanddepthframes,whicharethenstackedtogethertoformtheinputforeachstreamofthedualstreamQ-network.D.ExperimentdetailsTheproposedmethodisdividedintotwophases,i.e.,thedatagenerationphaseandthetrainingphase.Foreveryepisode,thealgorithmpassesthroughthesetwophases.Duringthedatagenerationphase,therobotinteractswiththeenvironmentforaround4hours(wecallittheinteractionperiod).Duringtheinteractionperiod,thenumberofstepsi(seeAlgorithm1)executedbytherobotdependedontheinternetspeed4sincethecommunicationbetweenPepperandthecomputersystemonwhichtheMDQNwasimplementedoccuredoverthewirelessinternet.Thebehaviorstrategyduringthisphaseis(cid:15)-greedy,where(cid:15)annealslinearlyfrom1to0.1over28,000stepsandthenremainsat0.1fortherestofthesteps.FortakingthegreedyactiontheoutputsfromeachstreamofthedualstreamQ-networkwerefusedtogetherandtheactionwiththehighestQ-valuewasselected.ForthefusionofoutputsfromeachstreamoftheQ-network,thealgorithmﬁrstnormalizestheQ-valuesfromeachstreamandthentakesanaverageofthesenormalizedQ-values.Aftertheinteractionperiodisover,therobotgoestosleepandthetrainingphasebegins.Thetrainingprocedurepresentedhereisthevariantof[10].Thenetworkparametersaretrainedonminibatchesm,eachofsize25samples,usingtheRMSPropalgorithm.Itshouldbenotedthatbothnetworkstreams,grayscaleanddepth,weretrainedindependentlywithoutanyfusionofQ-values,however,theQ-valuesfromeachstreamwerefusedduringthedatagenerationphasefortakingthegreedy4Withapproximately↓37/↑23Mbpsinternetspeedtherobotcouldgatheri=2010interactionexperiencese=(si,ai,ri,si+1)in4hours.During14daysoftheinteractionperiod,therobotexecuted13938stepsintotal.TrainedModelMDQNy-channeldepth-channelAccuracy(%)95.385.982.6Truepositiverate(%)90.771.866.2Falsepositiverate(%)3.099.411.3Misclassiﬁcationrate(%)4.614.916.9TABLEI:PerformancemeasuresoftrainedQ-networks.action.Inthispresentedwork,themodelwastrainedover111,504grayscaleanddepthframes,andforeachepisode,thealgorithmperformedtenexperiencereplaysi.e.,n=10.TheparametersoftargetQ-networkθ−wereupdatedaftereveryepisodei.e.,C=1.E.EvaluationFortestingthemodelperformance,aseparatetestdataset,comprising4480grayscaleanddepthframesnotseenbythesystemduringlearningwascollected.Since,foreveryscenariotherecanbemorethanoneactionthatcanbechosenwithutmostpropriety,therefore,theagent’sdecisionwasevaluatedbythreevolunteers.Asequenceofeightframesdepictingthescenarioandtheagent’sdecisionwereshowntothevolunteers.Eachvolunteerwasaskedtojudgeiftheagent’sdecisionwasrightornot.Iftheagent’sdecisionwasconsideredwrongbythemajoritythentheevaluatorswereaskedtoconsentonthemostappropriateactionforthatparticularscenario.VI.RESULTSThissectionsummarizestheresultsofthetrainedQ-network(agent)onthetestdataset.Weevaluatedthetrainedy-channelQ-network,depth-channelQ-networkandtheMDQNonthetestdataset;table1summarizestheperformancemeasuresofthesetrainedQ-networks.Intable1,accuracycorrespondstohowoftenthepredictionsbytheQ-networkswerecorrect.Thetruepositiveratecorrespondstothepercentageofpredictingpositivetargetsaspositiveandthefalsepositiverateisthepercentageofnegativeinstancesthatwereclassiﬁedaspositive.Misclassiﬁcationratedenoteshowoftennetworkpredictionswerewrong.Intable1,itcanbeseenthatthemultimodaldeepQ-network(Fused)achievedmaximumaccuracyof95.3%,whereasthey-channelandthedepth-channelofQ-networksachieved85.9%and82.6%accuracy,respectively.Hence,theresultsintable1validatethatfusionoftwostreamsimprovesthesocialcognitiveabilityoftheagent.Figure4showstheperformanceofMDQNonthetestdatasetovertheseriesofepisodes.Theepisode0onﬁgure4correspondstotheQ-networkwithrandomlyinitializedparameters.TheplotindicatesthattheperformanceofMQDNagentontestdatasetiscontinuouslyimprovingastheagentgetsmoreandmoreinteractionexperiencewithhumans.Restofthesectionprovidesthevisualevidencesofthepropositionthattherobotgainedhuman-likesocialintelligencethroughinteractionwithhumans.Inﬁgures5-7,theactionswait,looktowardshuman,wavehand,andshake-handaredenotedasW,L,H,andSrespectively.Forﬁgures5and7,eachsub-ﬁgureshows(a)W=0.12L=0.26H=0.14S=.49(b)W=0.44L=0.22H=0.19S=0.14(c)W=0.33L=0.22H=0.25S=0.20(d)W=0.29L=0.25H=0.23S=0.24(e)W=0.30L=0.23H=0.27S=0.20(f)W=0.17L=0.26H=0.32S=0.24(g)W=0.22L=0.26H=0.34S=0.19(h)W=0.26L=0.28H=0.21S=0.24Fig.5:Successfulcasesofagentsdecision.thestart(S)andtheend(E)frameoutofthetotaleightmostrecentframesforanysituation.Figures5and6indicatetheinstancesofsuccessfulpre-dictionsbytheMQDNbasedagent.TheactionhighlightedinblueshowstheactionwithmaximumQ-value,henceindicatestheagent’sdecisionforthatparticularscenario.Inﬁgure5(a),thepersonisstandingrightinfrontoftherobot,therefore,theagentchoosesthehandshakeaction.Forscenariosdepictedinﬁgures5(b)-5(e)theagentdecidestowait.Thisisbecause,inthescenarioshowninﬁgure5(b),thereisnohumaninthescene;incaseofﬁgure5(c),thepersonisworkingontheirlaptop;incaseofﬁgure5(d),thepersoniscarryingsomethingsandtheirhandsarenotfree;andincaseofﬁgure5(e),thegroupofpeoplearewalkingawayfromtherobot.Figures5(f)and5(g)representthesituationinwhichtheagentchoosesthewave-handaction,andlookingtowardsthehumanaction,respectively.Finally,ﬁgure5(h)showsthesituationinwhichthepersonisstandinginfrontoftherobot,buttakingtherobot’spicturethereforetheagentdecidedtolooktowardshiminsteadofshaking-hand.Figure6showstheevents(A-E)thathappenedsequen-tially.Foreachevent,onlythelastframeispresented.IntheeventA,thereisnohumanfortheinteractionhencetheagentdecidestowait.IneventB,twopeopleappearedinthesceneandtheagentswitchedtothelookingtowardshumanaction.FollowingeventB,tofurthergetthehumansattention,theagentchosethewave-handactionineventC.EventDindicatesthattheagenthassuccessfullygainedtheattentionofthehumanasitledtothesuccessfulhandshake.Finally,ineventE,theperson’sheadorientationisnottowardstherobotsotheagentchoosesthelooktowardshumanactioninordertogaintheirattentionagain.Figure7representssomeofthewrongdecisionstakenbytheagent.Theactionhighlightedinredindicatestheagentdecisionwhiletheactionhighlightedingreenrepresentthedecisionconsideredappropriatebytheevaluators.VII.DISCUSSIONThissectionprovidesbriefdiscussiononi)someoftheexcitingfeaturesthattheagent(Q-network)haslearnedthroughtheexperiment;andii)theeffectoftherewardFig.6:Seriesofevents(AtoE)happenedinasequence.(a)W=0.21L=0.30H=0.26S=0.23(b)W=0.24L=0.26H=0.27S=0.23(c)W=0.22L=0.22H=0.27S=0.29(d)W=0.26L=0.25H=0.25S=0.23Fig.7:Unsuccessfulcasesofagentsdecision.functionontherobot’sbehavior.SectionsA-Chighlightthattheagenthasgainedunderstandingofsomeofthefactorsthatformthebasisforintentioninferencesuchasactivityinprogress,walkingtrajectoriesandheadorientations.SectionEprovidesadiscussionontheeffectoftherewardfunctionontherobot’ssocialinteractionskills.A.ActivityinprogressThescenariosshowninﬁgures5(c),5(d)and5(h)showapersonworkingonalaptop,apersoncarryingsomethingsandapersontakingapicturerespectively.Theagent’sdecision,duringtheseactivities,indicatesthatithaslearnedtorecognizetheactivityinprogressandhasalsolearnedthatanyinteractionduringtheseactivitieswouldnotleadtothesuccessfulhandshake,henceagentdecidestowait.B.WalkingtrajectoryTheagent’sdecisioninsituationsshowninﬁgures5(e)and5(f)showsthatithasgainedinsightaboutthewalking00.20.40.60.8120406080100Penalty on unsuccessful handshakeTrue positive rate (%)Fig.8:Effectofrewardfunctionontherobot’sbehavior.trajectories.Intheﬁgure5(e)peoplewalkedawayfromtherobotandintheﬁgure5(f)apersoniscomingdownstairsandisgettingclosertotherobot.Intheformer,MDQN-agentdecidestowaitasitisquitelessprobabletogetthehandshakeinthatsituation,whereasinthelatteritdecidestowave-handasthereisachancetogetthehandshakebygainingtheattentionoftheoncomingperson.C.HeadorientationInﬁgure6,eventDandeventEshowtwodifferentscenarios;oneinwhichtheperson’sheadorientationistowardstherobot;andotherinwhichitisnottowardstherobot.ForeventD,theagentdecidestoshake-handwhileforeventEitdecidestogainhumanattentionbylookingtowardsthem.Hence,thisgivesanindicationthattheagenthasalsolearnedimplicationsofheadorientationonsocialhuman-robotinteraction.D.Effectofrewardfunctionontherobot’sbehaviorAlltheresultspresentedsofararebasedontherewardfunctiondiscussedearlier.Thissectionformalizestheeffectoftherewardfunctionontheagent’sbehavior.Varyingthepenaltyonunsuccessfulhandshakefrom0to-1changestherobotbehaviorfromamiabletorudeaswhenthepenaltyis0therobotalwaystriestohandshakeandwhenitis-1,therobotisreluctanttohandshake.Tounderstandwhichbehaviorisacceptablebyhumans,wetrainedﬁvenetworkswithﬁvedifferentrewardfunctionsandtheseﬁvenetworkswereevaluatedfollowingtheevaluationprocedurementionedearlier.Foreachrewardfunctiontheagentgets0rewardonactionsotherthanhandshake,+1onsuccessfulhandshakeand0,-0.1,-0.2,-0.5or-1onunsuccessfulhandshake.Figure8representstheplotofthetruepositiverateofeachmodelontestdatasetversuscorrespondingpenaltygivenonunsuccessfulhandshake.Theresultshowsthattherewardfunctionwith-0.1penaltyachievedmaximumaccuracyonthetestdataset.VIII.CONCLUSIONInsocialphysicalhuman-robotinteraction,itisverydifﬁ-culttoenvisageallthepossibleinteractionscenarioswhichtherobotcanfaceinthereal-world,henceprogrammingasocialrobotisnotoriouslyhard.Totacklethischallenge,wepresentedamulti-modeldeepQ-network(MDQN)withwhichtherobotlearnsthesocialinteractionskillsthroughtrialanderrormethod.Theresultsshowthediversityofinteractionscenarios,whichweredeﬁnitelyhardtoimagine,andyettherobotwasabletolearnwhichactiontochooseateachtime-stepinthesediversescenarios.Furthermore,theresultsalsoinsinuatethattheMDQN-agenthaslearnedtogiveimportancetowalkingtrajectories,headorientation,bodylanguageandtheactivityinprogressinordertodecideitsbestaction.Inourfuturework,weplantoi)increasetheactionspaceinsteadoflimitingittojustfouractions;ii)userecurrentattentionmodelsothattherobotcanindicateitsattention;iii)evaluatetheinﬂuenceofthreeactions,otherthanhandshake,onthehumanbehavior.REFERENCES[1]C.L.Breazeal,Designingsociablerobots.MITpress,2004.[2]M.A.GoodrichandA.C.Schultz,“Human-robotinteraction:asur-vey,”Foundationsandtrendsinhuman-computerinteraction,vol.1,no.3,pp.203–275,2007.[3]V.G.Duffy,Handbookofdigitalhumanmodeling:researchforappliedergonomicsandhumanfactorsengineering.CRCpress,2008.[4]C.Breazeal,“Socialinteractionsinhri:therobotview,”IEEETrans-actionsonSystems,Man,andCybernetics,PartC(ApplicationsandReviews),vol.34,no.2,pp.181–186,2004.[5]A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks,”inAdvancesinneuralinformationprocessingsystems,2012,pp.1097–1105.[6]C.Farabet,C.Couprie,L.Najman,andY.LeCun,“Learninghier-archicalfeaturesforscenelabeling,”PatternAnalysisandMachineIntelligence,IEEETransactionson,vol.35,no.8,pp.1915–1929,2013.[7]P.Sermanet,K.Kavukcuoglu,S.Chintala,andY.LeCun,“Pedes-triandetectionwithunsupervisedmulti-stagefeaturelearning,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,2013,pp.3626–3633.[8]G.Hinton,L.Deng,D.Yu,G.E.Dahl,A.-r.Mohamed,N.Jaitly,A.Senior,V.Vanhoucke,P.Nguyen,T.N.Sainathetal.,“Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups,”SignalProcessingMagazine,IEEE,vol.29,no.6,pp.82–97,2012.[9]A.Graves,A.-r.Mohamed,andG.Hinton,“Speechrecognitionwithdeeprecurrentneuralnetworks,”in2013IEEEinternationalconferenceonacoustics,speechandsignalprocessing.IEEE,2013,pp.6645–6649.[10]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovskietal.,“Human-levelcontrolthroughdeepreinforcementlearning,”Nature,vol.518,no.7540,pp.529–533,2015.[11]H.BenAmor,D.Vogt,M.Ewerton,E.Berger,B.-I.Jung,andJ.Pe-ters,“Learningresponsiverobotbehaviorbyimitation,”inIntelligentRobotsandSystems(IROS),2013IEEE/RSJInternationalConferenceon.IEEE,2013,pp.3257–3264.[12]H.BenAmor,G.Neumann,S.Kamthe,O.Kroemer,andJ.Peters,“Interactionprimitivesforhuman-robotcooperationtasks,”inRoboticsandAutomation(ICRA),2014IEEEInternationalConferenceon.IEEE,2014,pp.2831–2837.[13]D.Lee,C.Ott,andY.Nakamura,“Mimeticcommunicationmodelwithcompliantphysicalcontactinhumanhumanoidinteraction,”TheInternationalJournalofRoboticsResearch,vol.29,no.13,pp.1684–1704,2010.[14]Z.Wang,K.M¨ulling,M.P.Deisenroth,H.B.Amor,D.Vogt,B.Sch¨olkopf,andJ.Peters,“Probabilisticmovementmodelingforintentioninferenceinhuman–robotinteraction,”TheInternationalJournalofRoboticsResearch,vol.32,no.7,pp.841–858,2013.[15]C.Vondrick,H.Pirsiavash,andA.Torralba,“Anticipatingvisualrepresentationsfromunlabeledvideo,”inComputerVisionandPatternRecognition(CVPR),2016IEEEInternationalConferenceon.IEEE,2016,pp.1063–1069.[16]T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez,Y.Tassa,D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforce-mentlearning,”arXivpreprintarXiv:1509.02971,2015.[17]L.-J.Lin,“Reinforcementlearningforrobotsusingneuralnetworks,”DTICDocument,Tech.Rep.,1993.