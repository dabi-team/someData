8
1
0
2

g
u
A
2

]
E
S
.
s
c
[

2
v
7
9
4
4
0
.
3
0
8
1
:
v
i
X
r
a

Automated software vulnerability detection with machine learning

Jacob A. Harer1,2, Louis Y. Kim1, Rebecca L. Russell1, Onur Ozdemir1, Leonard R. Kosta1,2,
Akshay Rangamani3, Lei H. Hamilton1, Gabriel I. Centeno1,4, Jonathan R. Key1,
Paul M. Ellingwood1, Erik Antelman1, Alan Mackay1, Marc W. McConley1,
Jeffrey M. Opper1, Peter Chin2, Tomo Lazovich1
1 Draper
2 Boston University, Department of Computer Science
3 ECE Department, Johns Hopkins University
4 Northeastern University, College of Computer and Information Sciences

Abstract

Thousands of security vulnerabilities are discovered in
production software each year, either reported publicly
to the Common Vulnerabilities and Exposures database
or discovered internally in proprietary code. Vulnerabil-
ities often manifest themselves in subtle ways that are
not obvious to code reviewers or the developers them-
selves. With the wealth of open source code available
for analysis, there is an opportunity to learn the patterns
of bugs that can lead to security vulnerabilities directly
In this paper, we present a data-driven ap-
from data.
proach to vulnerability detection using machine learn-
ing, speciﬁcally applied to C and C++ programs. We
ﬁrst compile a large dataset of hundreds of thousands of
open-source functions labeled with the outputs of a static
analyzer. We then compare methods applied directly to
source code with methods applied to artifacts extracted
from the build process, ﬁnding that source-based mod-
els perform better. We also compare the application of
deep neural network models with more traditional mod-
els such as random forests and ﬁnd the best performance
comes from combining features learned by deep models
with tree-based models. Ultimately, our highest perform-
ing model achieves an area under the precision-recall
curve of 0.49 and an area under the ROC curve of 0.87.

1

Introduction

Hidden ﬂaws in software often lead to security vulner-
abilities, wherein attackers can compromise a program
includ-
and force it to perform undesired behaviors,
ing crashing or exposure of sensitive user information.
Thousands of such vulnerabilities are reported publicly
to the Common Vulnerabilities and Exposures database
(CVE) each year, and many more are discovered inter-
nally in proprietary code and patched [15, 21]. As we
have seen from many recent high proﬁle exploits, includ-
ing the Heartbleed bug and the hack of the Equifax credit

history database, these security holes can often have dis-
astrous effects, both ﬁnancially and societally [3, 30].
These vulnerabilities are often due to errors made by pro-
grammers and can propagate quickly due to the preva-
lence of open source software and code re-use. While
there are existing tools for static (pre-runtime) or dy-
namic (runtime) analysis of programs, these tools typi-
cally only detect a limited subset of possible errors based
on pre-deﬁned rules. With the recent widespread avail-
ability of open source repositories, it has become pos-
sible to use data-driven techniques for discovering the
patterns of bug manifestation. In this paper, we present
techniques using machine learning for automated detec-
tion of vulnerabilities in C and C++ software1. We focus
on vulnerability detection at the individual function level
(i.e. each function within a larger codebase is analyzed
to determine whether or not it contains a vulnerability).

1.1 Related work

There currently exists a wide range of static analysis
tools which attempt to uncover common vulnerabilities.
While there are too many to list in detail here, they range
from open source implementations such as the Clang
static analyzer [29] to proprietary solutions [11]. In addi-
tion to these rule-based tools, there has been much work
in the domain of using machine learning for program
analysis. This work spans various application domains,
from malware detection [1, 4, 14, 26, 27] to intelligent
auto-completion of source code [12, 24, 25, 28]. Ad-
ditionally, they span levels of program representations,
from pure source code, to abstract syntax trees, and ﬁ-
nally compiled binaries [5, 31]. For a comprehensive re-
view of learning from “big code”, see the recent work
by Allamanis et. al. [2]. There are also several existing
tools for automated repair, which both ﬁnd and attempt
to ﬁx potential bugs. The GenProg program uses ge-

1While our work focuses on C/C++, the techniques are applicable

to any language

 
 
 
 
 
 
Figure 1: A control ﬂow graph extracted from a simple example code snippet.

netic algorithms to generate repair candidates for a given
error [18]. The Prophet program builds a probabilistic
model of potentially buggy code and then uses this model
to generate repair candidates [19]. Both of these meth-
ods require access to test cases that trigger the failure.
The DeepFix algorithm from Gupta et. al. applies deep
learning to generating ﬁxes for simple syntax errors in
student code [10]. Our unique contribution in this work
is three-fold. First, we provide a systematic exploration
of both feature-based and source-based vulnerability de-
tection. Second, we explore the effectiveness of both
traditional machine learning and deep learning methods.
Finally, we do not limit ourselves to speciﬁc error types,
instead training only on labels of buggy or not, and end
up spanning over ﬁfty different categories of weakness
in the Common Weakness Enumeration (CWE) [22].

2 Methods

In order to detect vulnerabilities in C and C++ code, we
consider two complementary approaches. The ﬁrst uses
features extracted from the intermediate representation
(IR) of programs produced during the compilation and
build process. The second operates on source code di-
rectly. These two approaches offer independent sources
of information. The source-based models can learn sta-
tistical correlations in how code is written, leveraging
techniques from natural language processing, but don’t
have any inherent knowledge about the structure or se-
mantics of the language built in (these must all be learned
from data). The build-based models take advantage of
features extracted from a compiler that understands the
structure of the language, but these features may abstract
away certain properties of the code that are useful for

vulnerability detection. By pursuing both approaches,
we allow the possibility of ultimately fusing multiple
models to increase detection performance. In this sec-
tion, we ﬁrst deﬁne the features extracted for both the
build-based and source-based features, and then discuss
the machine learning models used for vulnerability de-
tection.

2.1 Build-based feature extraction

In build-based classiﬁcation, the ﬁrst step is to extract
features from the source using Clang and LLVM
tools [16]. During the build process we extract features
at two levels of granularity. At the function level, we
extract the control ﬂow graph (CFG) of the function.
Within the control ﬂow graph, we extract features
about
the operations happening in each basic block
(opcode vector, or op-vec) and the deﬁnition and use
of variables (use-def matrix). The CFG is a graph
representation of the different paths a program can take
during execution. Each node is a basic block - a unit of
code with no branching behavior. The edges of the graph
connect basic blocks that can ﬂow into each other during
execution and occur at control ﬂow operations. At a
high-level, this representation is useful for vulnerability
detection because models can potentially learn program
execution topologies that are risky. Figure 1 shows an
example code snippet and the resulting CFG.

The high-level view of the CFG is complemented
by features extracted from each basic block within the
CFG, so that models can also learn instruction-level be-
haviors that are associated with vulnerable code. Within
each basic block, we extract features which deﬁne the
behavior of that basic block. The ﬁrst of these features is

2

vectors are representations of tokens that are useful for
predicting the surrounding context in which they appear.
Both of these representations are unsupervised, meaning
they do not require labeled examples in order to be used.
Figure 3 shows a two-dimensional representation of a
word2vec representation learned from 6.3 million exam-
ples of C and C++ functions. Notice that similar types of
tokens cluster together in the word2vec representation,
giving us conﬁdence that this feature representation ap-
propriately captures token context.

2.3 Labels

Labeling at the function level in order to train a ma-
chine learning model is a signiﬁcant challenge. We do
not know of a source of C/C++ functions with ground
truth labels that has the volume required to train most
machine learning models and also has the complexity
and diversity of real-world code. As a result, we gen-
erate labels for each function during the feature extrac-
tion process. To generate the labels, we run the Clang
static analyzer (SA) on each source repository [6]. Af-
ter pruning the Clang outputs to exclude warnings that
are not typically associated with security vulnerabilities,
we match the ﬁndings to the functions in which they oc-
curred. The labels are then binarized, meaning functions
with no SA ﬁndings are labeled as “good” and functions
with at least one SA ﬁnding are labeled as “buggy”2. In
our convention, good is the negative label and buggy is
the positive label. This means that if our detection model
labels a function as buggy and there was also a static
analysis ﬁnding, this would be counted as a true positive.
Similarly, labeling a function as buggy when it does not
have a static analysis ﬁnding is counted as a false posi-
tive. While in this work we have presented a comparison
to static analysis labels as a way of benchmarking perfor-
mance, the ultimate system we hope to provide will go
beyond static analysis label prediction. By incorporat-
ing other sources of data such as the Juliet test suite [23]
and bug report repositories, we can expand the label set
used to train models. However, in this work, we compare
solely to SA labels to evaluate performance.

2.4 Models

In constructing machine learning models for detecting
buggy functions, we consider two objectives. First, we
aim to evaluate the overall possible performance - how
well each model can predict the presence of unsafe prac-
tices in the code. Second, we aim to compare the per-
formance of the build-based and source-based features
to discover which set is more predictive of code quality.

2Future work could include using the type of SA ﬁnding to do bug

classiﬁcation rather than just detection

Figure 2: An example illustrating the lexing process.

the use-def matrix. This matrix tracks, within the basic
block, the locations of instructions where variables are
deﬁned (def) and used (use).
If a variable is deﬁned
at instruction i and used in instruction j, then both the
( j, i) entry of the use-def matrix are set to 1. The second
feature extracted for each basic block is the op-vec, or
opcode vector. LLVM assigns opcodes to instructions in
one of nine different categories: conditional, arrogate,
binary, bit binary, conversion, memory address, termina-
tion, vector operation, and other. The opcode vector for
a basic block is a binary vector with a single entry for
each of these possible classiﬁcations. If the basic block
contains an instruction with the corresponding opcode,
then that bit is set to 1 for the basic block.

2.2 Source-based feature extraction

To feed the source-based classiﬁer models, we need a
feature representation of the individual tokens that occur
in source code. We implement a custom C/C++ lexer,
which parses the code and categorizes elements into dif-
ferent bins: comments (which are currently removed),
string literals, single character literals, multiple charac-
ter literals, numbers, operators, pre-compiler directives,
and names. Names are further sub-categorized into key-
words (such as for, if, etc.), system function calls (such
as malloc), types (such as bool, int, etc.), or vari-
able names. Variable names are all mapped to the same
generic identiﬁer, but each unique variable name within
a single function gets a separate index (for the purposes
of keeping track of where variables re-appear), follow-
ing the procedure of Gupta et. al. [10]. This mapping
changes from function-to-function based on the position
in which the identiﬁer ﬁrst appears. Unique string liter-
als, on the other hand, are all mapped to the same single
identiﬁer. Finally, integer literals are read digit-by-digit,
while ﬂoat literals are mapped to a common token. Fig-
ure 2 shows an example of the lexing process.

Once the tokens are lexed, the sequence of tokens can
be converted into a vectorial representation. The ﬁrst
representation we extract is the commonly used bag-of-
words vector, where a document is represented by the
frequency with which each token in the vocabulary ap-
pears [20]. The second representation we use is learned
with the word2vec algorithm [17]. In this model, word

3

Figure 3: A word2vec embedding of tokens from C/C++ source code

Therefore, we construct separate source-based and build-
based models and also consider a combined approach.

2.4.1 Source-based models

Our source-based models take in a feature vector derived
directly from a single function’s source code and output a
score corresponding to the likelihood that the given func-
tion contains a bug. Because we have multiple possible
feature vector representations of the source code (both
the bag-of-words and a sequence of word2vec vectors),
we build multiple models. For a baseline traditional
model, we choose the extremely random trees (extra-
trees) classiﬁer3 which takes the bag-of-words vector as
input [8]. As an alternative, we also consider an en-
hanced version of the TextCNN model, a convolutional
neural network originally developed for sentence classi-
ﬁcation [13]. This model begins with a feature embed-
ding layer that is initialized with the learned word2vec
representation of section 2.2. This representation is al-
lowed to be ﬁne-tuned during training. The sequence
of feature vectors is then passed through a convolutional
layer which feeds into multiple fully connected layers.
Finally, the probability that the function is buggy is out-
put. The ﬁnal model we consider is one where the fea-

3This was found to perform better than other traditional models

such as random forest and SVM on our validation set

tures learned by the convolutional layer of the TextCNN
are passed into the extra-trees classiﬁer rather than the
fully connected layer of the neural network. Figure 4 il-
lustrates the three different model options we consider.

2.4.2 Build-based models

Our build-based models use the features extracted from
the build process described in section 2.1. One of the
major challenges with using the build-based features is
the construction of a feature vector from the CFG adja-
cency matrix and op-vec/use-def vectors as these are at
different granularities (CFG at the function level, others
at the basic block level) and very in size with the size of
the CFG and basic blocks respectively. To deal with this
variation, we construct a hand-crafted ﬁxed-size vector
from these features. In this approach, we simply average
over the number of basic blocks to create an average use-
def matrix and op-vec for the function. We combine the
average upper triangular of the use-def matrix truncated
to size 15 × 15 (105 entries), the 9 possible opcode clas-
siﬁcations, the average of the CFG’s adjacency matrix
(a measure of connectedness), and the number of basic
blocks. This results in a N = 116 dimensional feature
vector. This vector can be fed into any machine learning
model, and for this study we chose a random forest.

In addition to comparing the results of the source-

4

<identifier>(){*=calloc1,sizeof;if==NULL<string literal>return}->freelen0char<const-+>reallocmemcpyint--!memmovefor++printf[]%43unsignedstruct&2true!=#if#endif&&falsebool||elselong/enumvoidmemset?:assert.switchcase<char literal>breakdefault|<<mallocFILEgotoEOFwhile-=+=strlencontinue|=errno#ifdefsprintfstrcmpmemcmp**8strncmpfprintfstderrdouble<float literal>59>>new67strncpy~&=^staticfloatstrcpy/=douint32_tdefined#elif#ifndef#elseuint8_tregisteruint16_t::thisuint64_tvolatileunionstddeletestringshort#*=static_castexternthrowtypedefstrcattrycatchfscanf#define#undefcoutendlscanfautocinsigned#error#include%=#line#Figure 4: Illustration of the three types of source-based models tested

based and build-based models, we also produce a com-
bined model which uses a concatenation of the build-
based vector and the source-based bag of words vector.

3 Experimental results

In this section, we present the results of training and eval-
uating the different classiﬁers described in section 2.4.
We use two different datasets of open source C/C++ code
to do the evaluation, and split the data into three parts:
training, validation, and test. The training set is used to
perform supervised learning and train the models to pre-
dict the bugginess of a function. The validation set is
used for tuning hyperparameters of each model. The test
set is used only for performance evaluation. All the re-
sults shown in this section are evaluation on the test set.

3.1 Datasets

Train

Valid.

Test

Good
Buggy
Good
Buggy
Good
Buggy

Debian Github
250436
429807
9556
14616
38500
66100
1220
1878
32109
55102
1203
1857

Source
754150
31790
94268
3724
94268
3724

Table 1: Number of functions in both datasets used for
model evaluation. Debian and Github represent the sep-
arated build-based datasets, while Source represents the
dataset of all source code combined from both datasets.

We evaluate the performance of models on two dis-
tinct datasets of open source C/C++ code. The ﬁrst is
the full set of C/C++ packages distributed with the De-
bian Linux distribution [7]. The second is a large set of
C/C++ functions pulled from public Git repositories on
Github [9]. These two datasets give unique beneﬁts. The
Debian packages and releases are very well-managed and

curated, meaning that this dataset ideally consists of rel-
atively stable code. This code is still susceptible to vul-
nerabilities, but it is at least in use on many systems to-
day. The Github dataset provides a wider variety of code,
both in function and in quality. Table 1 shows the statis-
tics of each dataset, including the number of good and
buggy functions and the split between train, validation,
and test. The source-based models have more data avail-
able because the code is not required to have successfully
built. We construct one combined source-based dataset
with all of the lexed functions available in the Debian
and Github datasets. For the build-based models, we
construct separate datasets for each source. This allows
us to compare the performance on different code sam-
ples while also comparing the performance between the
build-based and source-based models. To increase the
amount of source code available for training, we process
multiple Debian releases and multiple revisions of source
repositories in Github4.

One very important aspect of splitting the dataset is
also the removal of potential duplicate functions. When
pulling open source repositories there is the possibility
of functions being duplicated across different packages.
Such duplication, when not properly accounted for, can
artiﬁcally inﬂate performance metrics if functions that
appeared in the training set also appear in validation or
test sets. In order to account for this, we perform a dupli-
cate removal routine. For the source-based dataset, any
function that has the same lexed representation as an-
other function (using anonymous variables) is removed
from the dataset. In the build-based dataset, any func-
tion with a duplicated lexed representation or a duplicate
build feature vector is removed. This is the strictest pos-
sible duplicate removal and thus gives the most conser-
vative performance results. In practice, when applying
this tool to new packages in the wild, it is indeed pos-
sible that the model would come upon a function it had

4As a result, the statistics in table 1 should not be taken as the num-
ber of functions or static analysis ﬁndings in the current Debian release,
but rather an aggregation over many releases

5

Model
BoW + ET
word2vec + CNN
CNN feat. + ET

ROC AUC P-R AUC

0.85
0.87
0.87

0.44
0.45
0.49

Table 2: Summary statistics for the performance of dif-
ferent source-based classiﬁer options

already seen in the training set. However, estimating this
duplication probability is difﬁcult and thus we choose the
most conservative approach for evaluating our models.

3.2 Source-based detection

We compare the results of the three different source-
based models described in section 2.4.1. To evaluate
them, we compute both the receiver operating charac-
teristic (ROC) curve and the precision-recall curve for
their outputs. These are constructed by choosing a score
threshold and then computing the false positive rate and
true positive rate (in the case of the ROC curve) or the
precision and recall. Thus, each curve is formed by scan-
ning through different score thresholds. As a reminder,
the labels derived from a static analysis tool as described
in section 2.3 are treated as ground truth for the purpose
of performance benchmarking. Table 2 shows the area
under the curve for both metrics in each of the three mod-
els tested.

The best performance comes from a model using fea-
tures learned by the TextCNN as inputs into an extra-
trees classiﬁer. This gives an 10% improvement in area
under the precision-recall (P-R) curve over the standard
TextCNN initialized with a pre-trained word2vec input.
However, both models perform similarly in the ROC
AUC metric. In this problem setting, the precision and
recall are more relevant metrics due to the severe label
imbalance. ROC curves are not sensitive to label imbal-
ance while precision and recall can vary greatly depend-
ing on how the labels are balanced.

3.3 Build-based detection

Here, we compare the performance of the build-based
classiﬁer on the Debian and Github datasets. The re-
sults for the ROC and P-R AUC metrics can be seen
in table 3. We note two trends here. First, we see that
the build-based classiﬁers generally perform worse than
the source-based classiﬁers. It should be noted that this
dataset has a lower number of functions available than
the full combined source dataset, as can be seen in ta-
ble 1. In section 3.4, we perform a comparison of thw
two techniques on the same dataset. Second, we see that
model performance is very similar on the two different
datasets. This suggests that the model can learn about

Dataset ROC AUC P-R AUC
0.76
Debian
0.74
Github

0.21
0.22

Table 3: Summary statistics for the performance of the
build-based classiﬁer on both the Debian and Github
datasets

Model
Build
Source
Combined

ROC AUC P-R AUC

0.74
0.81
0.82

0.22
0.29
0.32

Table 4: Comparison of build only, source only, and
combined models on the Github dataset.

different types of code, and future work will test gener-
alization across datasets.

3.4 Comparison and combined model per-

formance

As a ﬁnal test, we compare the performance of the source
and build-based models on a dataset consisting of the
same set of functions. While this does not encompass
the full set of data available to the source-based classi-
ﬁer, it allows us to perform an apples-to-apples compar-
ison of performance. We re-train a source-based model
on the Github dataset alone and compare the results to
the build-based model on that dataset. We also train a
combined model which takes both the build-based fea-
ture vector and the bag-of-words source vector. The re-
sults of these comparisons are shown in table 4.

Even when comparing on the same dataset, source-
based approaches offer an advantage over the build-
based approaches,
for both ROC AUC and in the
precision-recall space. We also illustrate that the build-
based features do add additional information that the
source code itself does not provide, as the combined
model performs better than either model does individ-
ually. Figure 6 shows the comparison of both metric
curves.

4 Conclusions and future work

In this work, we have implemented machine learning
models for the detection of bugs that can lead to secu-
rity vulnerabilities in C/C++ code. Using features de-
rived from the build process and the source code itself,
we have shown that a variety of machine learning tech-
niques can be effective at predicting the output of static
analysis tools at the function level. The main limitation
of this work is in the labeling of the functions. While we
have treated the static analysis output as ground truth in

6

Figure 5: Results of testing different source-based models on a combined Debian+Github source code dataset. The
left shows ROC curve results while the right shows the precision-recall curve.

Figure 6: Comparison of build only, source only, and combined models on the Github dataset.

this study to illustrate the effectiveness of the machine
learning approach, ultimately the techniques should be
trained on additional sources of labels to be able to func-
tion robustly in a real-world setting. This could come
from a fusion of multiple static analysis tools or from the
curation of a dedicated, large set of example programs
with known ground truth. Ultimately, we believe that
the data-driven approach can complement existing static
analysis tools and decrease the amount of time it takes to
discover potential vulnerabilities in software. By rank-
ing functions by their probability of bugginess, develop-
ers can more rapidly complete code reviews and isolate
potential issues that may not have been uncovered other-
wise.

pipeline that was used to generate the datasets for this
work. This project was sponsored by the Air Force Re-
search Laboratory (AFRL) as part of the DARPA MUSE
program.

References

[1] ABOU-ASSALEH, T., CERCONE, N., KESELJ, V., AND SWEI-
DAN, R. N-gram-based detection of new malicious code.
In
Computer Software and Applications Conference, 2004. COMP-
SAC 2004. Proceedings of the 28th Annual International (2004),
vol. 2, IEEE, pp. 41–42.

[2] ALLAMANIS, M., BARR, E. T., DEVANBU, P., AND SUTTON,
C. A Survey of Machine Learning for Big Code and Naturalness.
ArXiv e-print 1709.06182 (Sept. 2017).

Acknowledgments

The authors thank Thomas Jost, Mark Lutian, Hugh
J. Enxing, Nicolas Edwards, and Adam Zakaria for
their heroic efforts creating the build and data ingestion

[3] ARNOLD,
big

C.
changes

for
http://www.npr.org/2017/10/18/558570686/after-equifax-hack-
calls-for-big-changes-in-credit-reporting-industry, 2017.

reporting

credit

in

After

equifax

hack,

calls
industry.

[4] CESARE, S., AND XIANG, Y. Malware variant detection us-
ing similarity search over sets of control ﬂow graphs. In Trust,

7

Security and Privacy in Computing and Communications (Trust-
Com), 2011 IEEE 10th International Conference on (2011),
IEEE, pp. 181–189.

[24] RAYCHEV, V., VECHEV, M., AND KRAUSE, A. Predicting
In ACM SIGPLAN Notices

program properties from big code.
(2015), vol. 50, ACM, pp. 111–124.

[25] RAYCHEV, V., VECHEV, M., AND YAHAV, E. Code comple-
tion with statistical language models. In ACM SIGPLAN Notices
(2014), vol. 49, ACM, pp. 419–428.

[26] SCHULTZ, M. G., ESKIN, E., ZADOK, F., AND STOLFO, S. J.
Data mining methods for detection of new malicious executa-
In Security and Privacy, 2001. S&P 2001. Proceedings.
bles.
2001 IEEE Symposium on (2001), IEEE, pp. 38–49.

[27] SHABTAI, A., MOSKOVITCH, R., ELOVICI, Y., AND GLEZER,
C. Detection of malicious code by applying machine learning
classiﬁers on static features: A state-of-the-art survey. informa-
tion security technical report 14, 1 (2009), 16–29.

[28] WHITE, M., VENDOME, C., LINARES-V ´ASQUEZ, M., AND
POSHYVANYK, D. Toward deep learning software repositories.
In Mining Software Repositories (MSR), 2015 IEEE/ACM 12th
Working Conference on (2015), IEEE, pp. 334–345.

[29] XU, Z., KREMENEK, T., AND ZHANG, J. A memory model
In International Symposium
for static analysis of c programs.
On Leveraging Applications of Formal Methods, Veriﬁcation and
Validation (2010), Springer, pp. 535–548.

[30] YADRON, D. After heartbleed bug, a race to plug internet hole.

Wall Street Journal 9 (2014).

[31] YAMAGUCHI, F., LOTTMANN, M., AND RIECK, K. General-
ized vulnerability extrapolation using abstract syntax trees.
In
Proceedings of the 28th Annual Computer Security Applications
Conference (2012), ACM, pp. 359–368.

[5] CHUA, Z. L., SHEN, S., SAXENA, P., AND LIANG, Z. Neural
In 26th
nets can learn function type signatures from binaries.
USENIX Security Symposium (USENIX Security 17) (Vancouver,
BC, 2017), USENIX Association, pp. 99–116.

[6] CLANG. Clang static analyzer. https://clang-analyzer.

llvm.org/.

[7] DEBIAN. Debian - the universal operating system. https://

www.debian.org/.

[8] GEURTS, P., ERNST, D., AND WEHENKEL, L. Extremely ran-

domized trees. Machine learning 63, 1 (2006), 3–42.

[9] GITHUB. Github. https://github.com/.

[10] GUPTA, R., PAL, S., KANADE, A., AND SHEVADE, S. Deepﬁx:
In AAAI

Fixing common c language errors by deep learning.
(2017), pp. 1345–1351.

[11] HP. Hp fortify. http://www.ndm.net/sast/hp-fortify.

[12] HSIAO, C.-H., CAFARELLA, M., AND NARAYANASAMY, S.
Using web corpus statistics for program analysis. In ACM SIG-
PLAN Notices (2014), vol. 49, ACM, pp. 49–65.

[13] KIM, Y. Convolutional neural networks for sentence classiﬁca-

tion. CoRR abs/1408.5882 (2014).

[14] KOLTER, J. Z., AND MALOOF, M. A. Learning to detect and
classify malicious executables in the wild. Journal of Machine
Learning Research 7, Dec (2006), 2721–2744.

[15] LATOZA, T. D., VENOLIA, G., AND DELINE, R. Maintaining
mental models: A study of developer work habits. In Proceed-
ings of the 28th International Conference on Software Engineer-
ing (New York, NY, USA, 2006), ICSE ’06, ACM, pp. 492–501.

[16] LATTNER, C., AND ADVE, V. Llvm: A compilation framework
for lifelong program analysis & transformation. In Proceedings
of the international symposium on Code generation and optimiza-
tion: feedback-directed and runtime optimization (2004), IEEE
Computer Society, p. 75.

[17] LE, Q., AND MIKOLOV, T. Distributed representations of sen-
tences and documents. In Proceedings of the 31st International
Conference on Machine Learning (ICML-14) (2014), pp. 1188–
1196.

[18] LE GOUES, C., NGUYEN, T., FORREST, S., AND WEIMER, W.
Genprog: A generic method for automatic software repair. IEEE
Transactions on Software Engineering 38, 1 (2012), 54–72.

[19] LONG, F., AND RINARD, M. Automatic patch generation by
learning correct code. In ACM SIGPLAN Notices (2016), vol. 51,
ACM, pp. 298–312.

[20] MANNING, C. D., AND SCH ¨UTZE, H. Foundations of statistical

natural language processing. 1999.

[21] MITRE. Common vulnerabilities and exposures. cve.mitre.org.

[22] MITRE. Common Weakness Enumeration. https://cwe.

mitre.org.

[23] NIST. Juliet test suite v1.3, 2017. https://samate.nist.

gov/SRD/testsuite.php.

8

