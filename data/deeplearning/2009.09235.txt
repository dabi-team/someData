Accepted in IEEE-RAS International Conference on Humanoid Robots (Humanoids2020)

Open-Ended Fine-Grained 3D Object Categorization
by Combining Shape and Texture Features in Multiple Colorspaces

Nils Keunecke∗ and S. Hamidreza Kasaei∗

1
2
0
2

y
a
M
8
2

]

V
C
.
s
c
[

3
v
5
3
2
9
0
.
9
0
0
2
:
v
i
X
r
a

Abstract— As a consequence of an ever-increasing number of
service robots, there is a growing demand for highly accurate
real-time 3D object recognition. Considering the expansion of
robot applications in more complex and dynamic environments,
it is evident that it is not possible to pre-program all object
categories and anticipate all exceptions in advance. Therefore,
robots should have the functionality to learn about new object
categories in an open-ended fashion while working in the
environment. Towards this goal, we propose a deep transfer
learning approach to generate a scale- and pose-invariant object
representation by considering shape and texture information in
multiple color spaces. The obtained global object representation
is then fed to an instance-based object category learning
and recognition, where a non-expert human user exists in
the learning loop and can interactively guide the process
of experience acquisition by teaching new object categories,
or by correcting insufﬁcient or erroneous categories. In this
work, shape information encodes the common patterns of all
categories, while texture information is used to describes the
appearance of each instance in detail. Multiple color space
combinations and network architectures are evaluated to ﬁnd
the most descriptive system. Experimental results showed that
the proposed network architecture outperformed the selected
state-of-the-art in terms of object classiﬁcation accuracy and
scalability. Furthermore, we performed a real robot experiment
in the context of serve a beer scenario to show the real-time
performance of the proposed approach.

I. INTRODUCTION

At the end of this decade, autonomous mobile robots
are believed to be used in our life as service robots, self-
driving cars, and collaborative industrial robots. However, as
the environment becomes more complex, it is important that
the robots use an accurate and robust perception system. To
work in such a dynamic environment and safely interact with
human users, robots need to know which kind of objects exist
in the scene and where they are. Therefore, object detection
and recognition play important role in collaborative robots.
Three-Dimensional (3D) Object recognition is a fundamental
research problem in computer vision and robotic commu-
nities. Although recent approaches have shown promising
results under structure environments, there are several un-
resolved issues. In particular, most recent approaches follow
the train-then-test protocol, which means that the robot
is trained once all data has been gathered. Therefore, the
performance of the robot strongly dependent on the quality

We thank the NVIDIA Corporation for their generous donation of GPUs

which was partially used in this research.

∗Department of Artiﬁcial

Groningen,
hamidreza.kasaei@rug.nl

Netherlands.

Intelligence. University of Groningen,
n.keunecke@student.rug.nl

Fig. 1: Basic-level object categorization such as differentiating the
left three objects has been done for years and high performance
is achieved frequently. In contrast, differentiating between object
classes in ﬁne-grained object categorization tasks like the three sets
of three objects on the right side is difﬁcult if either only shape or
color information is used. This paper shows how a combination of
texture and shape signiﬁcantly increases the accuracy in such tasks.

and quantity of training data. Furthermore, the knowledge of
such robots is ﬁxed after the training phase, and any changes
in the environment require complicated, time-consuming, and
expensive robot re-programming by expert users.

A dynamic environment makes it

impossible to pre-
program all possible object categories, and anticipate all
exceptions before the robot goes into operation. Therefore,
the robot should have the ability to learn about new object
categories in an online and open-ended fashion. In order
to meet
these requirements, a robot should be able to
update the model of existing categories as new instances
are encountered and create a new model once facing a
new object category. While this procedure can be partly
supervised in the form of human-in-the-loop feedback (i.e.,
non-expert human users can interactively guide the process
of experience acquisition by teaching new categories or by
correcting insufﬁcient or erroneous categories), the robot
also has to learn independently from on-site experiences in
its environment. In human-centric environment, the robot
frequently encounters a new object that can be either not
similar (e.g., apple vs. juice box) or very similar (e.g., coke
can vs. beer can) to the previously learned object categories
in terms of texture and shape. This is a very challenging
task to learn about ﬁne-grained object categories using a few
examples since deep learning approaches often need large-
scale training data to avoid over-ﬁtting.

In this work, we propose an approach to represent an
object based on depth and colored orthographic (top, side,
and front) views. In particular, we use three depth views of an
object to represent the geometrical properties (basic-level),
and a colored orthographic view, i.e., rendered from the
direction that has maximum entropy, is used for describing
the texture of the object (ﬁne-grained). Towards this goal,

basic-level object recognitionfine-grained object recognition 
 
 
 
 
 
as more instances are presented. Among others, Kasaei et
al. [1] reported that their transfer learning approach yields
excellent results for such tasks. These network architectures
are usually pre-trained on the imageNet dataset [9].

OrthographicNet [1] is one of several recently proposed
view-based approaches [10] [11], which tend to show supe-
rior performance compared to volume-based [12] [13] and
point-based approaches [14] [15]. View-based approaches
construct 2D representations based on the point cloud of the
object. In contrast, volume-based approaches use the point
cloud to construct a 3D voxel-grid. Point-based approaches
directly operate on the point cloud, obtained from the object.
All approaches use the obtained representation as to their
respective input for a neural network (typically some form
of CNN). In contrast to other view-based approaches, the
OrthographicNet uses a partial point cloud of the object
to generate 2D orthographic projections of the object. For
robotics applications and other real-world implementations,
multi-view representations of objects are problematic due
to the lack of scenarios where objects are fully observable.
Unlike our approach, all these approaches only considered
shape information and discard color information completely.
recognition algorithms can
be divided in two groups with either focus on shape-
information [17] [18] or on color-information [19] [20].
While shape-only approaches frequently struggle with simi-
larly shaped objects [2] (Fig. 3), color-based approaches are
volatile to shadows and illumination [21] and tend to have a
bias towards texture [22].

In general, most object

Even though there exist several state-of-the-art shape-only
approaches, the neuroscientiﬁc argument has been made by
Bramao et al., that for humans color-information is essential
for object recognition [23] which can be transferred to neural
networks by several recent ﬁndings [2] [24]. Several strate-
gies to achieve the interaction between shape and texture
exist. Three approaches are presented in order of increasing
computational complexity. (i) a color constancy value can be
calculated to ﬁnd the average color of an object [2]. While
this approach already increases the performance, it lacks the
ability to detect complicated textures. (ii) Shape information
can be evaluated in parallel to an RGB image of the object
[25]. This reduces in particular the overall tendency of the
purely color-based descriptors to be biased towards texture
and will help shape-only descriptors to differentiate between
objects of similar shape (like two different soda cans).
(iii) Gowda et al. [3] have found that combining different
color spaces by transforming the RGB image improves
the reported overall accuracy as different features of an
object are represented differently in different color spaces.
The third approach combines color information with shape

Fig. 3: Representing object based on shape-only or color-only is
not enough to distinguish very similar objects (adapted from [16]).

Fig. 2: The overall system architecture: in the ﬁrst step the point
cloud of the object is processed to generate three depth orthographic
projections for encoding the shape of the object, and one colored
projection is rendered from the view that has maximum entropy. On
the top-right of the ﬁgure, the shape information is processed via
MobileNetV2. On the bottom-right, the RGB color information is
transformed into different color spaces and evaluated individually
either via a DenseNet or the MobileNetv2, depending on the setup.
The output of both sides is concatenated into a global feature vector
which is ﬁnally used for learning and recognition purposes.

we ﬁrst construct a unique Local Reference Frame (LRF)
for the object, and then, render orthographic projections by
exploiting the LRF. Each projected view is then fed to a CNN
to obtain a view-wise deep feature. The representation of the
object is ﬁnally constructed by concatenating the color and
depth representations. The ﬁnal representation is a pose- and
scale-invariant, and designed with the objective of supporting
accurate 3D object recognition. Notice the eventual deep
object representation for a juice box object as shown in
Fig. 2.

II. RELATED WORK

Three-dimensional object recognition has become a ﬁeld
of fundamental importance in computer vision, pattern recog-
nition, and robotics. Convolutional Neural Networks (CNNs)
are frequently used for image classiﬁcation purposes [4].
Recently a tendency towards deeper network architectures
can be observed [5]. However, the gained accuracy comes
at
the price of an increased model size and number of
parameters. This poses an issue in many robot applications
where computation power and memory availability may be
limited once the robot is in operation. Generally, there is
a trend towards optimizing the descriptiveness to computa-
tional complexity ratio of network architecture [3] [6].

Additionally, it has been identiﬁed that it is impossible to
pre-train neural networks for 3D object recognition entirely.
Open-Ended learning approaches have recently become more
popular [7] [8]. These approaches address the problem of
CNNs to adapt to an increasing number of categories as
this would require reshaping of the typology of the network.
CNNs tend to require a lot of training data to perform
accurately, which is unfeasible in open-ended learning as
new categories have to be learned opportunistically with very
few instances in the beginning, and incrementally updated

has max entropyColorspace TransformationConcatenated global feature vectorMobileNetDenseNetOpen-ended learning and recognitioninformation. It searches for the most optimal combination of
color spaces and combines them with a state-of-the-art shape
descriptor, such as the aforementioned OrthographicNet. It is
expected that the resulting architecture has all the advantages
of shape and color descriptors while the two parts of the
network architecture mitigate each other’s weaknesses.

III. METHOD

In this work, we assume that objects are placed on a
surface, e.g., table, and already segmented from the input
point cloud. Each object is represented by a set of points,
pi ∈ {1, ..., n}, where each point contains standard RGB
color information [R, G, B], as well as a depth value in
three axes [x, y, z].

for

For rendering orthographic images
of a 3D object, we put three virtual
cameras around the object: where the
Z axes of a camera is parallel with
one of the principal axes of the object,
and pointing towards the centroid of
the object, and perpendicular to plane
constructed by the other two axes of
the object. Therefore, it is necessary to
ﬁrst create a Local Reference Frame
(LRF)
the object. Towards this
goal, we ﬁrst compute the geometric
center of the object, which is deﬁned
as the arithmetic mean position of all
the points of the object. Afterwards, we
construct a LRF by performing eigen-
value decomposition analysis on the
normalized covariance matrix, Σ, of the object, i.e., ΣV =
EV, where E = [e1, e2, e3] contains the descending sorted
eigenvalues, and V = [(cid:126)v1, (cid:126)v2, (cid:126)v3] shows the eigenvectors. In
this work, the largest eigenvectors, (cid:126)v1 is considered as X axis.
Since we assumed the object is laying on a table, the Z axis
of the object is set to the direction that is perpendicular to the
table (gravity direction). We ﬁnally deﬁne the Y axis as the
cross product of X × Z. The object is then transformed to be
placed in the reference frame (see Fig. 4). Finally, from each
camera pose, we project the object into a depth image using
the z-buffering and orthogonal projection methods [33].

Fig. 4: An example
of object reference
frame
construction
and bounding box
estimation
a
juice box. The red,
blue
and
green,
lines show the X,
Y,
and Z axes,
respectively.

for

After rendering orthographic projects, the depth and color
views are separated into two input streams, one for color
2).
information and one for shape information (see Fig.
In particular, three scale and rotation invariant projections
are rendered based on the z-buffering technique to encode
geometrical information of the object. The obtained ortho-
graphic depth projections, namely the front-, side-, and top-
view are fed into the OrthographicNet [1]. In this work, the
MobileNetv2 [26] was used for each individual projection, as
Kasaei et al. reported the best results with this network [1].
To process the texture information, one intuitive option
is to rendered three colored orthographic projections of
the object (similar to orthographicNet). However, a set of
preliminary tests showed that no performance improvements
could be gained using all three projections but rather that

Fig. 5: An illustrative example of rendering three colored ortho-
graphic projections from the partial point cloud of a soda can.

a single projection performed signiﬁcantly better than the
other two (see Fig. 5). This is related to the aforementioned
texture-bias of color-based classiﬁers. We use viewpoint
entropy to deﬁne which of the three orthographic projections
is best for further processing (see Fig. 2). The underlying
reason for considering viewpoint entropy as the metric of
selecting the best view is that viewpoint entropy nicely takes
into account both the number of occupied pixels and their
values. Entropy is deﬁned as a metric to measure how much
information is contained in a single projection, X, and can
be calculated as:

H(X) = −

n
(cid:88)

i=1

p(xi) logp(xi)

(1)

where xi represents the normalized value of ith pixel. All
three orthographic RGB projections of an object are com-
pared and the projection with the highest entropy (e.g., most
information) is selected for further processing. Based on the
work of Gowda and Yuan [3], the following color spaces
were selected: RGB, HED, HSV, LAB, YCbCr, YIQ, and
YUV. Additionally, grayscale was included for a total of 8
color spaces. While all color spaces represent the same color,
they use different mathematical models to represent
that
color. The resulting numerical differences have an impact on
neural networks where different ﬁlters are learned depending
on the color space. Input images are transformed into each
color space using the respective transformation as displayed
exemplary in the following equation, shown for a color
transformation from RGB to the YUV color space:


 =









Y
U
V

0.299
0.587
−0.168 −0.331
0.500

0.114
0.500










 +





R
G
B





0
128
12

(2)

−0.418 −0.0813

After the color space transformations, input images are fed
into the respective network to encode the texture information.
In this paper two neural network architectures were used
to evaluate color information. A DenseNet 40-12-BC [27] is
used, which is 40 layers deep and has a growth factor of
12. The BC refers to compression layers at the end of each
dense block.

Additionally,

TABLE I: Properties of the used CNNs.

DenseNet MobileNetv2

MobileNetv2
the
Model
to as
(here
refer
Depth
was
MobileNet)
Feature length
is
used,
Input size
signiﬁcantly deeper
Parameters
Size
than the DenseNet
and has almost 10 times the parameters as the DenseNet
(Table I). Each color space was trained on both networks

88
1280 ﬂoat
224 x 224
2.25M
14.5 MB

40
132 ﬂoat
64 x 64
0.225M
3 MB

which

using Washington RGB-D dataset [28] (refer to Section IV-A
for more details).

combination of color spaces was optimized on both color
spaces.

To classify the learned representations, a MLP consist-
ing of two linear layers with output size 1024 and 256
respectively were used. Both layers use rectiﬁed linear units
(ReLU) and a dropout layer with rate 0.2. Finally, predictions
are made by a linear layer with softmax activation. Statistic
Gradient Descent (SGD) [29] is used for optimization with
a momentum of 0.9 and nesterov momentum. The initial
learning rate is set to lr0 = 0.05 and an exponential learning
rate decay lr = lr0 ∗ e−kt with k = −0.02, where t
is the number of iterations. No warm-up nor restarts were
used. This conﬁguration was used for all ofﬂine evaluations.
Training was carried out for 150 epochs with a batch size of
128. In addition, a global weight decay of 10−6 was used.

IV. RESULTS

To evaluate the proposed approach, a total of three ex-
the ofﬂine-evaluation, online-
periments were performed:
evaluation, and a real-time robot demonstration. All tests
were performed with a PC running Ubuntu 18.04 with a 3.20
GHz Intel Xeon(R) i7 CPU, and a Quadro P5000 NVIDIA.
In the case of ofﬂine and online evaluations, experiments
were carried out using the Washington RGB-D dataset [28].
This dataset contains 300 common household items, which
are organized into 51 classes. From the available 250000
views, 50000 orthographic projections were generated and
divided into a train, validation, and test set with a 70/15/15
split. During the ofﬂine evaluation, Average Class Accuracy
(ACA) over 10 trials is reported.

A. Ofﬂine Evaluation

II,

in the HSV and the YCbCr color spaces,

1) Color space evaluation: Firstly, the individual color
spaces were evaluated. From the results reported in Ta-
it can be observed that on average the Mo-
ble
bileNet performed about 2% better than the DenseNet.
However,
the
DenseNet showed better results than the MobileNet. The
best average class accuracy (ACA) was 98.56%, which
was obtained by MobileNet architecture and RGB color
space conﬁguration. In the case of DenseNet,
the best
ACA, obtained with the YCbCr color space, was 97.44%.
Notably, some color
perform
spaces
signiﬁcantly worse
in this classiﬁcation
task (i.e., Grayscale,
and HED).
YIQ,
It
that
evident
is
not all color spaces
may be beneﬁcial to
object
improve
recognition
accuracy. As
the
network architecture
both perform well, though on different color spaces, the

Colorspace DenseNet MobileNet
96.56%
93.59%
97.32%
96.37%
97.44%
92.33%
95.24%
91.55%

RGB
HED
HSV
LAB
YCbCr
YIQ
YUV
Grayscale

TABLE II: Object recognition accuracy
for all color spaces.

98.56%
96.86%
96.40%
96.87%
97.19%
92.51%
97.43%
95.80%

2) Color space optimization: Colorspace optimization
was carried out performing an exhaustive search of all
combinations of color spaces. It was found that the best
combination of color spaces for the MobileNet as well as the
DenseNet was a combination of the RGB, HSV, YCbCr, and
YUV color space at 98.84% and 98.16%, respectively. As
computational performance is a key metric of evaluation and
additional color spaces obviously impact increase the com-
putation steps, Table III summarizes the best combinations
of one to four colorspaces for both network architectures.
Neither beneﬁts from more than four colorspaces.

Once the best color space combination is obtained, the
ﬁnal and complete system architecture can be constructed.
In particular, an object representation is ﬁnally constructed
by concatenating the two individual shape and color feature
vectors of the object. It should be noted that maximum or
average pooling cannot be applied here. The underlying rea-
son is that to encode the geometrical properties of the object,
all orthographic projections are used while for representing
the texture of the object, only the orthographic view of the
object with the maximum entropy is used.

3) Evaluation of

the ﬁnal network architecture: The
Washington RGB-D dataset is known to be color biased [2]
and because of that in the previous round of experiments, all
evaluations consistently yielded higher average class accu-
racies (ACA) than the best accuracy (i.e., 86.85%)reported
for the OrthographicNet [1] which only evaluates shape
information. To investigate the optimal balance of color and
shape information, a color weight vector w is introduced and
applied to the color feature vector. Similarly, a (1−w) weight
vector is applied to the shape feature vector:

f(x, y, z, h) = (1 − w) ∗ fs(x, y, z) + w ∗ fc(h)

(3)

where fs and fc stand for shape and color feature vectors,
respectively. The combined feature vector f(x, y, z, h) is ob-
tained by weighting the shape and the color feature vectors,
and x, y, z are the views of the respective orthographic
projections, and h is the color image of the view with the
maximum entropy.

In this round of experiments, to reduce the overhead of the
color space transformation, only for this part of the evalua-
tion, all images were transformed beforehand. The training
was carried out on the Washington-D dataset following the

TABLE III: Colorspace optimization for DenseNet and MobileNet.

Model

DenseNet

MobileNet

Colorspace combination
YCbCr
YCbCr, HSV
YCbCr, HSV, RGB
YCbCr, HSV, RGB, YUV
RGB
RGB, YCbCr
RGB, YCbCr, YUV
RGB, HSV, YCbCr, YUV

ACA (%)
97.44%
97.48
97.67
98.12
98.56
98.69
98.79
98.89

TABLE IV: Average class accuracy
for the combined network of color and
shape information.

protocol laid down in Section 4. The ACA over 10 trials
is summarized in Table IV. Based on the results reported
in IV, it can be concluded that the model with DenseNet
(w = 0.6) achieved the highest ACA (99.14%) and the
MobileNet with w to 0.8 obtained the second-best result.
It should be noted
the MobileNet
that
has ≈ 9.75M pa-
rameters while the
DenseNet has only
≈ 0.75M parame-
ters. By comparing
all results, it is vis-
ible that the combi-
nation of color and
shape outperformed
both shape-only (w = 0.0) and color-only (w = 1.0)
settings. While the classiﬁer is theoretically able to learn
the optimal weight between color and shape feature vectors
during training, minor improvements can be observed from
the weight initialization. The obtained w is used to test the
open-ended capabilities of the network architecture.

weight DenseNet (%) MobileNet (%)

90.56
97.51
98.48
99.00
99.07
98.89

90.56
97.44
98.10
99.14
99.00
98.12

0
0.2
0.4
0.6
0.8
1.0

B. Online Evaluation

The scalability in the open-ended scenario was evaluated
using a recently introduced test-then-train proto-
col [2]. We developed a simulated user to interact with the
robot using three different actions, including teach, ask, and
correct. The simulated teacher should be connected to a large
object dataset, therefore, the Washington RGB-D dataset [28]
is used in this round of evaluation. The main idea is to let
the robot learns a new object category with the help of a
simulated teacher (human-in-the-learning-loop). Towards this
goal, we replaced the classiﬁcation part of the network with
an instance-based learning approach [7]. More speciﬁcally,
an instance-based learning and recognition (IBL) approach
considers category learning as a process of learning about the
instances of a category, i.e., a category is represented by a set
of known instances, C ⇐= {f1, f2, . . . , fn}, where fi is the
representation of an object, as discussed in section IV-A.3. It
should be noted that IBL is a baseline approach to evaluate
object representations. An advantage of the IBL approaches
over other machine learning methods is the ability to rapidly
adapt an object category model to an unseen instance by
storing the new instance or by throwing away an old instance.
The teacher ﬁrst teaches two new object categories to the
robot using the teach action. In the case of “teach”, the learn-
ing agent stores the object views in its perceptual memory.
In the case of ask-action, the teacher selects a previously
unseen view of a known category and asks the robot to
predict the category label of the object. If misclassiﬁcation
happens, the simulated teacher uses the correct-action to
correct the system and sends the true category of the object,
and the agent updates the respective category model using
the misclassiﬁed object. The simulated teacher sequentially
picks unseen objects from the known categories and asks the
robot to recognize them. The teacher progressively estimates

twice the error rate). Should the system fail

the recognition accuracy of the agent using a sliding window
and a new category is introduced, when all known categories
tested at least once and the recognition accuracy exceeds
threshold (τ = 0.67, meaning accuracy is
the protocol
at
to
least
reach this threshold after 100 iterations, the experiment is
aborted by the simulated user, as it can be concluded that
the robot no longer has the capability to learn additional
categories. Following this protocol, it is possible to simulate
an environment in which the robot is simultaneously learning
and recognizing.

The performance was evaluated ﬁve metrics: QCI denotes
the number of question-correct iterations, that was necessary
to learn the categories. This acts as a measure of how
fast the robot learned. NLC is the average number of all
categories learned by the robot. AIC is the average number
of instances per category. Finally, the global class accuracy
(GCA) and the Average Protocol Accuracy (APA) indicate
how well the robot performs. It is worth mentioning that
the ability to provide ‘real-time’ 3D object recognition can
not be abandoned due to its importance in applications in
service robots, and collaborative robots. Therefore, memory
usage (NLC) and computation time (#QCI) have to be used
as performance metrics during the evaluation of any robot
perception system as well.

The obtained results for these experiments in relation to
other recent approaches in open-ended object recognition
such as BoW, RACE, Open-Ended LDA, and GOOD are
reported in Table V. It can be observed that our approach
with MobileNet not only learns it the fastest with 1329.10
question-correction iterations but the average instances per
category decreased by 1 compared to the shape-only Orthop-
graphicNet. Additionally, the network architecture shows a
3−4% performance increase in the GCA and APA evaluation
metrics. The excellent scalability of the OrthographicNet can
still be observed as all 10 experiments for both new network
architectures have consistently learned all 51 categories.

However, the signiﬁcant performance improvement from
a purely shape-based descriptor to shape- and color-based
descriptor as seen in the ofﬂine evaluation (> 10%) could
not be achieved in the online evaluation. The DenseNet-
based architecture performed slightly worse than the original
OrthographicNet. The DenseNet model learned all existing
categories slower than OrthographciNet, in contrast to the
architecture using MobileNetv2. This pattern repeated for

TABLE V: Summary of the online evaluation using RGB-D
dataset, averaged over 10 trials.

Method
RACE [30]
BoW [31]
Local-LDA [32]
GOOD [18]
OrthographicNet(*)
Our + DenseNet (*)
Our + MobileNe(*)

QCI
382.10
411.80
262.60
1659.20
1342.60
1409.10
1329.10

NLC
19.90
21.80
14.40
39.20
51.00
51.00
51.00

AIC
8.88
8.20
9.14
17.28
8.97
10.28
7.97

GCA
0.67
0.71
0.66
0.66
0.77
0.75
0.81

APA
0.78
0.82
0.80
0.74
0.80
0.77
0.83

(*) indicates that the stopping condition was “lack of data”.

GCA and APA metrics. A possible cause for the slightly
lower performance of the DenseNet is the comparatively
shallow depth of the network and the small feature vector
generated by the DenseNet.

C. Real-time robot demonstrations

To show the real-time performance
the proposed approach, we per-
of
formed a real-robot experiment
in
the serve a beer scenario. The setup
(Fig. 6) consists of a table with ﬁve
different objects, namely a BeerCan,
CocktailCan, Mug, Oreo, and Vase. A
Kinect sensor is used as the percep-
tion device and a UR5e robot arm
is employed as the active device. In
this experiment, the proposed approach
is integrated into the RACE cognitive
robotic systems [30] as a learning and
recognition module. Note that, we used three similar objects
(i.e., Oreo, Mug, and Vase objects) and two very similar
objects (the two types of cans as shown in Fig. 8) to assess
the performance of the proposed approach in terms of open-
ended learning and recognition of basic and ﬁne-grained
objects using very few examples. Fig. 7 shows snapshots
of the demonstration.

Fig. 8: Two very
similar objects used
in this experiment.

In this experiment, the robot should detect and recognize
all table-top objects, as indicated by the green bounding
boxes and red texts on top of the objects. Initially, the robot
does not have any information about the objects, therefore,
all objects are recognized as unknown category. A human
user is involved in the learning loop, The user can interact
with the robot using a provided graphical menu to teach
the robot new object categories and also provide corrective
feedback whenever it is necessary (e.g., misclassiﬁcations
is able to obtain the new
happen). This way the robot
data-points from the users in an interactive way. The user
starts providing the robot with the respective category labels.
As more categories are introduced,
learns and
recognizes each of them as different categories and not
as other instances of previously introduced categories. This
demonstration showed that our approach is able to learn
about object categories in an interactive and open-ended
fashion. Furthermore, it shows the descriptive power of the
proposed approach by learning and recognizing very similar
objects using very few examples on-site. When the command
to “serve a beer” is given by the user, the robot retrieves

the robot

Fig. 6: Our experimental setup for the real-robot experiment
consists of a table, a Kinect sensor, and a UR5e robotic-arm.

the pose of the BeerCan object, goes into the pre-grasp
pose of the object, and ﬁnally grasps and picks it up. the
robot then moves the grasped BeerCan over the Mug, and
serve the drink. With this real-time robot demonstration, it
has been shown that the robot is able to recognize objects
from different orientations, learn new categories in an open-
ended fashion, and perform object recognition in real-time
(≈ 30Hz). A video of this demonstration is available at:
https://youtu.be/dB5s5x6m6WY.

V. CONCLUSIONS

In this work, we proposed a deep learning-based approach
for 3D object recognition, which enables robots to learn
about new object categories in an interactive and open-ended
fashion. We encoded both shape and texture information to
produce a global scale- and pose-invariant object descriptor.
We showed that the obtained representation is descriptive
enough to represent both basic and ﬁne-grained object cate-
gories. In particular, we rendered rotation and scale-invariant
(depth) orthographic projections of an object to encode the
shape feature, and from the view that has maximum entropy,
we rendered a color image to represent the texture of the
object. Afterward, shape and texture projections were fed
separately to two networks: one to encode the shape (i.e.,
we mainly used MobileNet), and the other one was used
for encoding the texture information (i.e., we evaluated both
MobileNet and DenseNet for this purpose). Afterward, the
obtained feature vectors from the shape encoding stream
and color encoding stream were combined in a weighted
feature vector. The obtained object description was ﬁnally
used for classiﬁcation purposes. The proposed approach was
analyzed in an ofﬂine setting and online experiment, where
a simulated human teacher was involved in the learning
loop. Experimental results showed that the proposed net-
work architecture outperformed the selected state-of-the-art
approaches in terms of object classiﬁcation accuracy and
scalability. Furthermore, we perform a real robot experiment
in the context of serve a beer scenario to show the real-time
performance of the proposed approach. In the case of ofﬂine
evaluation, DenseNet proved superior over the MobileNet
both in terms of descriptiveness and computational time. In-
terestingly, throughout online evaluation, DenseNet showed
slightly lower descriptiveness than MobileNet. For future
work, we plan to investigate how the rendered orthographic
projections can be used for object grasping purposes, and
then, develop a deep learning architecture to do simultaneous
object recognition and grasping.

REFERENCES
[1] Hamidreza Kasaei. OrthographicNet: A deep learning approach
In IEEE/ASME

for 3D object recognition in open-ended domains.
Transactions on Mechatronics, 2020.

[2] Hamidreza Kasaei, Maryam Ghorbani, Jits Schilperoort, and Wessel
Rest. Investigating the importance of shape features, color constancy,
color spaces and similarity measures in open-ended 3D object recog-
nition. In Intelligent Service Robotics 14, 2021.

[3] Shreyank Gowda and Chun Yuan.

In ColorNet: Investigating the
Importance of Color Spaces for Image Classiﬁcation, pages 581–596.
Lecture Notes in Computer Science, vol 11364. Springer, 2019.

Fig. 7: Performance of the robot during the serve a beer scenario: (a) the table and all table-top objects are detected as indicated by the
green bounding boxes. Initially, the system recognized all object as “unknown”; (b) a human user teaches all objects to the robot, then,
the robot conceptualizes all categories and recognizes all objects correctly; (c) afterwards, the user instructs the robot to perform “serve
a beer” task. The beer object is detected and grasped by the robot. (d) The robot moves the beer object over the mug object and serve
the drink.

[4] Ahmed Ali Mohammed Al-Saffar, Hai Tao, and Mohammed Ahmed
Talab. Review of deep convolution neural network in image classiﬁca-
tion. In 2017 International Conference on Radar, Antenna, Microwave,
Electronics, and Telecommunications (ICRAMET), pages 26–31. IEEE,
2017.

[5] Zhipeng Zhang and Houwen Peng. Deeper and wider siamese
In Proceedings of the IEEE
networks for real-time visual tracking.
Conference on Computer Vision and Pattern Recognition, pages 4591–
4600, 2019.

[6] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model
compression and acceleration for deep neural networks. In Artiﬁcial
Intelligence Review 53, 5113–5155, 2020.

[7] S Hamidreza Kasaei, Miguel Oliveira, Gi Hyun Lim, Lu´ıs Seabra
Lopes, and Ana Maria Tom´e.
Interactive open-ended learning for
3D object recognition: An approach and experiments. In Journal of
Intelligent and Robotic Systems, 80(3-4):537–553, 2015.

[8] Miguel Oliveira, Lu´ıs Seabra Lopes, Gi Hyun Lim, S Hamidreza
Kasaei, Angel D Sappa, and Ana Maria Tom´e. Concurrent learning
of visual codebooks and object categories in open-ended domains. In
2015 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pages 2488–2495. IEEE, 2015.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei Fei
In IEEE
Li.
Imagenet: a large-scale hierarchical image database.
Conference on Computer Vision and Pattern Recognition, Miami, FL,
pp. 248-255, 2009.

[10] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Ro-
tationnet: Joint object categorization and pose estimation using mul-
In Proceedings of the IEEE
tiviews from unsupervised viewpoints.
Conference on Computer Vision and Pattern Recognition, pages 5010–
5019, 2018.

[11] Su, H., Maji, S., Kalogerakis, E., and Learned-Miller, E. Multi-
view convolutional neural networks for 3D shape recognition.
In
Proceedings of the IEEE international conference on computer vision,
pp. 945-953. 2015.

[12] Qi, C. R., Su, H., Nießner, M., Dai, A., Yan, M., and Guibas, L. J.
Volumetric and multi-view cnns for object classiﬁcation on 3D data.
In Proceedings of the IEEE conference on computer vision and pattern
recognition. 2016.

[13] Wu, J., Zhang, C., Xue, T., Freeman, B., and Tenenbaum, J. Learning
a probabilistic latent space of object shapes via 3D generative-
adversarial modeling. Advances in neural information processing
systems, 29, 82-90. 2016.

[14] Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning
on point sets for 3D classiﬁcation and segmentation. In Proceedings
of the IEEE conference on computer vision and pattern recognition.
2017.

[15] Qi, C. R., Yi, L., Su, H., and Guibas, L. J.

Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In
Advances in neural information processing systems, 5099-5108. 2017.
[16] H. Ayoobi, H. Kasaei, M. Cao, Rineke Verbrugge, and B. Verheij.
Local-HDP : Interactive open-ended 3D object categorization.
In
ECCV 2020, 2020.

[17] Ismail Khalid Kazmi, Lihua You, and Jian Jun Zhang. A survey of
2D and 3D shape descriptors. In 2013 10th International Conference
Computer Graphics, Imaging and Visualization, pages 1–10. IEEE,
2013.

[18] Hamidreza Kasaei, Ana Tom´e, Lu´ıs Seabra Lopes, and Miguel
Oliveira. Good: A global orthographic object descriptor for 3D object
recognition and manipulation. In Pattern Recognition Letters, 83, 07
2016.

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In Advances
in neural information processing systems, pages 1097–1105, 2012.

[20] J. Krause, M. Stark, J. Deng and L. Fei-Fei. 3D Object Representations
for Fine-Grained Categorization. In IEEE International Conference on
Computer Vision Workshops, Sydney, NSW, pp. 554-561. 2013.
[21] Christian H Poth and Werner X Schneider. Breaking object correspon-
dence across saccades impairs object recognition: The role of color
and luminance. In Journal of Vision, 16(11):1–1, 2016.

[22] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge,
Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are
biased towards texture; increasing shape bias improves accuracy and
robustness. In arXiv preprint arXiv:1811.12231, 2018.

[23] Inˆes Bram˜ao, Alexandra Reis, Karl Magnus Petersson, and Lu´ıs Fa´ısca.
The role of color information on object recognition: A review and
meta-analysis. In Acta psychologica, 138:244–53, 07 2011.

[24] Chi-Yi Tsai and Shu-Hsiang Tsai. Simultaneous 3D object recognition
In IEEE Access,

and pose estimation based on RGB-D images.
6:28859–28869, 2018.

[25] Umar Asif, Mohammed Bennamoun, and Ferdous A Sohel. RGB-
D object recognition and grasp detection using hierarchical cascaded
forests. In IEEE Transactions on Robotics, 33(3):547–564, 2017.
[26] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear
bottlenecks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 4510–4520, 2018.

[27] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Wein-
In Proceedings
berger. Densely connected convolutional networks.
of the IEEE conference on computer vision and pattern recognition,
pages 4700–4708, 2017.

[28] Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. A large-scale
hierarchical multi-view RGB-D object dataset. In IEEE International
Conference on Robotics and Automation, Shanghai, pp. 1817-1824,
2011.

[29] Bottou, L. Large-scale machine learning with stochastic gradient
descent. In Proceedings of COMPSTAT’2010, pp. 177-186. 2010.
[30] Miguel Oliveira, Lu´ıs Seabra Lopes, Gi Hyun Lim, S Hamidreza
Kasaei, Ana Maria Tom´e, and Aneesh Chauhan. 3d object percep-
In Robotics and
tion and perceptual learning in the race project.
Autonomous Systems, 75:614–626, 2016.

[31] Kasaei Hamidreza, Oliveira Miguel, Gi Hyun Lim, Lu´ıs Seabra Lopes,
Ana Maria Tom´e. Towards lifelong assistive robotics: A tight coupling
between object perception and manipulation In Neurocomputing, vol.
291, pp. 151–166, 2018.

[32] S. H. Kasaei, L. S. Lopes and A. M. Tom´e. Local-LDA: Open-
Ended Learning of Latent Topics for 3D Object Recognition. In IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 42,
no. 10, pp. 2567-2580, 2020.

[33] Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao Soft
rasterizer: A differentiable renderer for image-based 3D reasoning
In Proceedings of the IEEE International Conference on Computer
Vision, pages 7708-7717, 2019.

