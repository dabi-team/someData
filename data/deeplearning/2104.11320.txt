1
2
0
2

r
p
A
2

]
I

N
.
s
c
[

1
v
0
2
3
1
1
.
4
0
1
2
:
v
i
X
r
a

Federated Double Deep Q-learning for Joint Delay
and Energy Minimization in IoT networks

Sheyda Zarandi and Hina Tabassum, Senior Member, IEEE

Abstract—In this paper, we propose a federated deep reinforce-
ment learning framework to solve a multi-objective optimization
problem, where we consider minimizing the expected long-term
task completion delay and energy consumption of IoT devices.
This is done by optimizing ofﬂoading decisions, computation
resource allocation, and transmit power allocation. Since the
formulated problem is a mixed-integer non-linear programming
(MINLP), we ﬁrst cast our problem as a multi-agent distributed
deep reinforcement learning (DRL) problem and address it using
double deep Q-network (DDQN), where the actions are ofﬂoading
decisions. The immediate cost of each agent is calculated through
solving either the transmit power optimization or local compu-
tation resource optimization, based on the selected ofﬂoading
decisions (actions). Then, to enhance the learning speed of IoT
devices (agents), we incorporate federated learning (FDL) at the
end of each episode. FDL enhances the scalability of the proposed
DRL framework, creates a context for cooperation between
agents, and minimizes their privacy concerns. Our numerical
results demonstrate the efﬁcacy of our proposed federated DDQN
framework in terms of learning speed compared to federated
deep Q network (DQN) and non-federated DDQN algorithms. In
addition, we investigate the impact of batch size, network layers,
DDQN target network update frequency on the learning speed
of the FDL.

I. INTRODUCTION

Massive connectivity is among one of the most challeng-
ing requirements of Internet-of-Things (IoT) networks which
necessitates efﬁcient, scalable, and low-complexity network
resource management. Furthermore, due to limited compu-
tation and battery capacity of the IoT devices, it is often
impossible for them to process their resource-intensive tasks
within a predeﬁned deadline. In the sequel, mobile cloud
computing (MCC) and mobile edge computing (MEC) enable
IoT devices to ofﬂoad their tasks to the cloud or edge servers to
access their substantial processing capabilities at the expense
of having to transmit the tasks over dynamic wireless channels.
Subsequently, to take full advantage of the MCC and MEC
paradigms, it becomes essential to carefully optimize ofﬂoad-
ing decisions, communication, and computation resources.

Most of the existing research works solved the joint of-
ﬂoading decision, communication, and computation resource
allocation problem leveraging on tools from optimization
theory [1], [2]. However, the algorithms were typically non-
scalable, time-consuming, and computationally expensive. Un-
like optimization frameworks, deep reinforcement
learning
(DRL) enables agents to learn by interacting with the envi-
ronment. This unique approach to learning, turns DRL into

S. Zarandi and H. Tabassum are with the Lassonde School of Engineering at
York University, ON, Canada (e-mail: shz@yorku.ca, hinat@yorku.ca). This
work is supported by the MITACS research trainee award and Discovery Grant
from the Natural Sciences and Engineering Research Council of Canada.

an ideal problem-solving tool in dynamic environments. Yet,
most DRL algorithms are centralized and thus suffer from
lack of scalability when the number of devices grow. Also,
the computational complexity of ﬁnding an optimal policy
may increase exponentially as the state space and action
space grow. Furthermore,
the centralized learning requires
IoT devices to share their information in order to train the
global model which may violate their privacy and create
unnecessary communication overhead on the already scarce
frequency spectrum.

Recently, federated learning (FDL) has emerged as a new
paradigm for cooperative learning, where multiple nodes con-
tribute in training a single global model. The devices use their
local datasets to train and then ofﬂoad their local models to
the central unit for global aggregation. FDL enhances the
cooperation between agents and scalability of the network
resource management algorithms. Furthermore, FDL does not
require local agents to share their data with any external entity,
thereby preserves the privacy of each agent [3].

To date, several research works have explored the problem
of minimizing delay and energy consumption of the IoT
devices considering a FDL system [4]–[6]. Speciﬁcally, in the
aforementioned research works, FDL features were incorpo-
rated in the problem formulation and the problem was then
solved using traditional optimization theory. Similarly, [7]–
[9] focused on optimizing different aspects of FDL, such as
compression of weights, convergence analysis, reduction in
the number of iterations, and incentive mechanisms. In these
research works, the capabilities of FDL as a part of resource
allocation solution approach were not investigated. In [10],
the problem of computation resource allocation was addressed
considering an FDL system. However, FDL is only considered
to formulate an optimization problem which is later on solved
by using a centralized actor-critic agent and without using
FDL in the solution approach.

None of the aforementioned research works applied FDL
to enhance the efﬁcacy of solving a realistic wireless resource
allocation problem. Very recently, [11], [12] adopted FDL to
facilitate the learning process in DRL, i.e., local DRL models
were trained and then integrated together to cooperatively
develop a comprehensive global DRL model. However, in [11],
a cooperative caching scheme was proposed and ofﬂoading
decisions were not considered. In [12], computational ofﬂoad-
ing was considered; however, the network was modeled as a
queuing system, transmit power was modeled as an integer
variable whose maximum value is equal to the maximum
length of the energy queue. Also, in [12], no explicit quality-
of-service (QoS) was guaranteed for users’ tasks and compu-

 
 
 
 
 
 
tation resource allocation was overlooked.

In this paper, we employ federated DRL (FedRL) to address
the problem of minimizing joint expected task completion
delay and energy consumption of IoT devices with ofﬂoading
decisions, computation resources, and transmit powers as vari-
ables. Considering the mixed-integer non-linear programming
programming (MINLP) nature of the problem, we ﬁrst refor-
mulate our problem as a multi-agent DRL problem and solve
it using double deep Q-network (DDQN). In this problem,
ofﬂoading decisions would be the actions and the immediate
cost is calculated through solving either the transmit power or
local computation resource optimization. To improve learning
quality and speed of DRL, we incorporate FDL at the end
of each episode. Using FDL results in a privacy-preserving
and scalable framework and creates a context for cooperation
between agents. Our numerical results demonstrate the efﬁcacy
of our federated DDQN framework in terms of learning
speed compared to federated deep Q-network (DQN) and non-
federated DDQN algorithms.

The rest of this paper is organized as follows: Section II
describes the system model and the problem is formulated in
Section III. Then, in Section IV, our proposed algorithm is pre-
sented. Finally, simulation results and related discussions are
provided in Section V followed by conclusions in Section VI.

II. SYSTEM MODEL AND ASSUMPTIONS

We consider a network containing one MEC server, one
cloud server, and a set of N = {1, ..., N } IoT devices with
limited computation and energy resources. We consider a given
time horizon T which is divided into T time steps. At each
time t, device i needs to process one of the tasks in its queue,
deﬁned with the tuple (Li,t, Ci,t, ˆTi,t), where Li,t is the size
of the task (in bits), Ci,t denotes the CPU cycle requirement
of the task, and ˆTi,t denotes the maximum delay threshold of
the task. At any time t, devices can either execute their task
locally or ofﬂoad it to edge or cloud server.

Let us denote local ofﬂoading decision of device i at time
t as xi,t, where xi,t = 1 means the task would be performed
locally, and xi,t = 0, otherwise. Similarly, we deﬁne MCC
and MEC ofﬂoading variable of device i by zi,t and yi,t,
respectively. If device i ofﬂoads its task to the cloud zi,t = 1
and if it ofﬂoads the task to the edge server yi,t = 1. As a
binary ofﬂoading decision is considered, we have:

xi,t + yi,t + zi,t = 1. ∀t ∈ T .

(1)

When device i ofﬂoads its task (whether to MEC server or to
the cloud), the delay and energy consumption would depend
on the channel condition, the size of the task, and the power
with which the device transmits its task and, in the case
of local computation they depend on computation resource
utilization. In what follows, we model the delay and energy
consumption that an IoT device would experience, given its
ofﬂoading decision.

If the device i decides to ofﬂoad its task, it should ﬁrst
transmit it to the MEC-enabled base station (BS) through

wireless channels. At time step t, the transmission data rate
of this user, denoted by ri,t is calculated as follows:

ri,t = B log2(1 +

pi,thi,t
σ2

),

(2)

where B and pi,t denote the bandwidth and transmit power of
device i at time step t, respectively. Also, hi,t and σ2 represent
the path-gain of device i at time t and the receiver noise. Thus,
the communication delay and energy consumption of device
i, while ofﬂoading is given, respectively, as follows:

T comm
i,t

=

Ecomm
i,t

= pi,tT comm

i,t

=

Li,t
ri,t

,

pi,tLi,t
B log2(1 + pi,thi,t

σ2

(3)

(4)

)

i,t = Ci,t

If device i ofﬂoads its task to the edge server the computation
delay would be T e
F e , where F e denotes the average
computation capacity of edge server. Also, if device i ofﬂoads
the task to cloud server, the computation delay would be T c
i,t =
Ci,t
F c , where F c represents the average computation capacity
of the cloud server.

the energy that

From the perspective of IoT device,

is
consumed for processing a task when it is ofﬂoaded to either
of the servers, is the energy spent on the transfer of the task.
Therefore, both cloud and edge computing energy utilization
i,t, would be equal to Ecomm
at step t, denoted by Ec
.
If device i chooses to perform its task locally, the local
computation delay and energy consumption would depend on
the amount of computation resource allocated to process the
task at time t, which we denote by f L
i,t. Thus, the local delay
and energy consumption of the device is modeled as follows:

i,t and Ee

i,t

T L
i,t =

Ci,t
f L
i,t

, EL

i,t = κi(f L

i,t)2,

(5)

where κ is a constant coefﬁcient that depends on the chip
architecture in devices. Note that higher resource utilization
(transmit power or computation capacity) , decreases the
task completion delay at
the expense of increased energy
consumption. Therefore, this trade-off must be carefully man-
aged through efﬁcient ofﬂoading decision making and precise
optimization of fi,t and pi,t
in the case of local computation
and ofﬂoading, respectively.

III. MULTI-OBJECTIVE PROBLEM STATEMENT

In this section, we formulate the multi-objective problem of
jointly minimizing the long-term delay and energy consump-
tion of an IoT device in a decentralized manner over a speciﬁed
time horizon T . The long-term expected cost (weighted sum
of delay and energy consumption) for each IoT device i is
formulated, respectively, as follows:

Ti(pi,fi, xi, yi, zi) = E[ lim
t→∞

t
(cid:88)

(cid:0)xi,jT L

i,j+

1
t

yi,j(T e

i,j + T comm

i,j

j=0
) + zi,j(T c

i,j + T comm

i,j

+ Ψ)(cid:1)],

(6)

Ei(pi, fi, xi, yi, zi)

= E[ lim
t→∞

1
t

t
(cid:88)

j=0

(cid:16)
xi,jEL

i,j + yi,jEe

i,j + zi,jEc
i,j

(cid:17)

],

(7)

where pi = [pi,1, pi,2, · · · , pi,t], fi = [fi,1, fi,2, · · · , fi,t],
xi = [xi,1, xi,2, · · · , xi,t], yi = [yi,1, yi,2, · · · , yi,t], and
zi = [zi,1, zi,2, · · · , zi,t], represent the vectors of transmit
powers, computation resource allocation,
local computing,
edge ofﬂoading, and cloud ofﬂoading decision of device i,
respectively. As cloud server is generally located far from
the IoT devices, the delay of accessing cloud is commonly
more than the delay of ofﬂoading to the edge server which
is located at the edge of the network. In the equation (6), Ψ
denotes the delay of accessing the cloud server, including the
time necessary to transfer the task from BS to the cloud, the
possible routing in the path, and the response delay.

For any given device i at time t, we model our problem as:

Ti + λiEi

min
pi,fi,xi,yi,zi
subject to:

• Computing and Communication Resource allocation:
Given the ofﬂoading decision, we optimize computation capac-
ity or transmit power of the devices to minimize the weighted
sum of energy consumption and delay. We use optimization
theory to address this part of the problem and then feed
the results into the DDQN framework as the immediate cost
function. In this way, we provide the learning agent with a
real sense of the quality of the adopted ofﬂoading policy that
reﬂects many important aspects of the system model (such as
limitation of resources in each device and their QoS demands).
After the DDQN agent is trained through the above men-
tioned process for one training round, we apply a feder-
ated learning framework where each IoT device will train
its DDQN models, share their models with the centralized
controller, and update their models to the central aggregating
unit. This mechanism is detailed in the ﬂowchart provided in
Fig. 1.

In what follows, we ﬁrst focus on developing local models
through DDQN algorithm and then explain how FDL would
be deployed.

i,t + yi,tEe

i,t <= Fmax,i, ∀t ∈ T ,
i,t + zi,tEc

C1 : f L
C2 : xi,tEL
C3 : Ti,t <= ˆTi,t, ∀t ∈ T ,
C4 : xi,t + yi,t + zi,t = 1, ∀i ∈ N , ∀t ∈ T ,
C5 : xi,t, yi,t, zi,t ∈ {0, 1}, ∀i ∈ N , ∀t ∈ T .

i,t <= Emax,i, ∀t ∈ T ,

In the above optimization problem, λi is a weighting factor
whose value should be carefully selected based on the hetero-
geneity of resources available at each individual IoT device. If
device i is more restricted in the energy resource compared to
computation resource, the value of λi should be set to a larger
number. Otherwise, λi should be a small number. Furthermore,
constraint C1 indicates the local computation capacity with
the maximum threshold Fmax,i. Constraint C2 represents the
restriction on the energy resource of the device and that energy
utilization should not exceed Emax,i. Furthermore, constraints
C4 and C5 deﬁne the binary ofﬂoading scheme adopted in
this paper. It can be proven that both equations (6) and (7) are
convex with respect to the variables pi,t and fi,t, respectively.
However, with binary ofﬂoading variables (xi, yi, and zi)
included, (8) turns into a MINLP that cannot be solved in
an acceptable time span.

IV. PROPOSED FEDERATED DDQN ALGORITHM

To solve (8) at each IoT device, we propose a DDQN
algorithm, and solve the problem in the following two phases:
• Ofﬂoading Decision Optimization: Since each IoT device
has three options to process a task (namely local, edge server,
or cloud server computing), there are almost 23N possible
ofﬂoading options (from the perspective of a centralized
controller) at each given time step. As the number of devices
this complexity would also surge exponentially.
increases,
To address this problem, we apply a multi-agent DDQN
framework where each IoT device would train their local
DDQN models using their local data.

A. Double deep Q-network for ofﬂoading decision making

(8)

In the ﬁrst step of our algorithm, we model our problem as
a multi-agent DDQN problem. For each device (DRL agent),
we have following components:

1) State space:

the state space for each agent i, de-
noted by si, consists of the following components: the
length of the task queue of device i (tasks that are
not yet processed or are not successfully processed,
in this queue) which is denoted by
would be kept
Li,t,
the path gain of the IoT device hi,t,
the size
of the task currently being processed Li,t,
its CPU
cycle requirement, Ci,t, and available resources. Thus,
si = {Li,t, hi,t, Li,t, Ci,t, Emax,i, Fmax,i}. If a task is
successfully processed under a given ofﬂoading decision
policy (its QoS requirement is satisﬁed), it would be
removed from the task queue of the device. Otherwise,
it will remain at the top of the queue to be processed
under another ofﬂoading decision.

2) Action space: The action space of agents, denoted by
A, contains possible ofﬂoading decisions, i.e., whether
to process the task locally or ofﬂoad.

3) Cost: (8) suggests that the cost of an agent is equal
to the weighted summation of the delay and energy
consumption given in the objective function. The value
of this objective function and thus the cost depends
on the value of pi,t in the case of ofﬂoading and fi,t
if local computation is selected. Therefore, to ensure
that the cost function accurately reﬂects the beneﬁt of
a given ofﬂoading decision, these variables should be
carefully optimized. To this end, when local computation
is selected (xi,t = 1), the cost would be calculated by
solving the instantaneous optimization problem below:

min
fi,t

i,t + λEL
T L
i,t

subject to: C1, C2, C3.

(9)

each training iteration the target value for training the online
network in device i is calculated as:
Li = ui(s, a) + γQi(s(cid:48), arg max
a(cid:48)∈A

Qi(s(cid:48), a(cid:48); θonline

), θtarget
i

)

i

While θonline
i
change in θtarget
every fupdate rounds, θtarget

i

i

(12)
is updated at every iteration, the frequency of
is typically much lower and only once in

would be set equal to θonline

.

i

As discussed before, training a DRL agent in a centralized
manner can lead to critical issues related to scalability, agents’
privacy, and additional communication overheads. On the other
hand, training a DRL agent in a distributed manner can impact
the overall performance gains (e.g., an agent might consume
a longer time to train its model). As such, we consider FDL
to combine the beneﬁts of both centralized and distributed
learning. FDL enables each agent to train its own local model,
using its own local data. Then these local models are sent to a
central aggregation unit to be combined together. This process
continues until a criterion is met.

in the case ofﬂoading is selected (yi,t = 1 or zi,t = 1),
the transmit power would be optimized by solving the
following optimization problem:

min
pi,t

yi,t(T e

i,t + T comm
+ λEcomm
i,t
i,t
+ zi,t(T c
i,t + T comm
i,t

)

+ Ψ + λEcomm

)

i,t

subject to: C2, C3.

(10)
The design of state and cost function has a signiﬁcant
impact on the success of DDQN in ﬁnding the optimal
ofﬂoading policy, π∗. By using a multi-agent approach, we
are in fact limiting the state and action space and focus on
each device separately. Also, by modeling the cost function as
an optimization problem not only can we optimize the local
resource utilization and enforce system constraints, but also we
can provide the agent with an accurate estimation of the quality
of an ofﬂoading decision. Note that it can be easily proved
that both (9) and (10) are convex single variable optimization
problems that can be solved using standard softwares.

Algorithm 1 Proposed Federated DDQN Algorithm

1:

Initialize the global model θglobal, and set maximum

FDL iterations to K.

2:

For each agent initialize online and target networks as:

θonline
i

= θglobal.

= θtarget
i
3: While (K ≥ 0):
Set θonline
4:
i
Select the set of participating devices, I, based on (13),
5:
For each device i in I do:

= θglobal, ∀i ∈ N ,

= θtarget
i

For each time step t if |Li,t| > 0 do:

Interact with environment and calculate the cost

using (9) or (10),

Save the experience in replay memory Mi,t.
Train the local model on Mi,t,
Transmit θonline

to the central aggregation unit,

i

End For

End For
update θglobal using (14).

6:
7:
8:

9:
10:

11:
12:
13:
14:

Let us denote the immediate cost of each device i obtained
from the solution of the above mentioned optimization process
as ui(s, a). Using Bellman equation, the action-state value is:

Qi(s, a) = ui(s, a) + γ

(cid:88)

s(cid:48)∈S

Pss(cid:48)(a) max
a(cid:48)∈A

Qi(s(cid:48), a(cid:48)),

(11)

where S, Pss(cid:48)(a), and γ are the set of states, the transition
probability function, and the discount factor, respectively. To
overcome the need for having a full model of environment,
calculating the transition probability function, and to acquiring
a more stable learning process, DDQN is employed in this
work. Each agent i has two neural networks working alongside
each other, one called online network with parameters θonline
i
and the other called target network with parameters θtarget
. At

i

Fig. 1: The federated reinforcement learning process

B. Federated DRL Approach

The steps to train the FedRL agents are presented in the

following:

1) Device selection strategy: At

the beginning of each
iteration of FDL, a set of IoT agents are selected to participate
in the FDL. Thus, of all N devices in the network, only a small
subset, denoted by I = {1, ..., I} is selected to contribute in
FDL. In this paper, the device selection is done based on the
following criterion:

arg max
i∈N

Var(

diPmax,i
Fmax,i

),

(13)

where di represents the distance of device from BS and
the function Var stands for variance. This criterion helps in
identifying devices whose experiences are more heterogeneous
and thus can contribute more in the the learning process.

2) Training local models: As explained previously , all IoT
devices use DDQN to train their local models. After this local
training is ﬁnished (no more unprocessed task remains in the
queue), the weights of online network, θonline
, is extracted in
each agent and is then sent to the central aggregating unit.

i

3) Model Aggregation: When central unit receives the
models of participating IoT devices, it would aggregate the
models which results in a single global model that would be
then transmitted to all agents. For the purpose of aggregation,
we utilize FedAvg [13], and perform model aggregation as:
(cid:80)

θglobal =

.

(14)

i∈I θonline
i
|I|

This global model, which has integrated the experiences
of all devices, is then transmitted back to IoT devices and
the three steps above would be repeated. The details of our
proposed framework is provided in Algorithm 1 as well as
the ﬂowchart given in Fig. 1.

V. SIMULATION RESULTS

Here, we present our simulation results and extract useful
insights related to the performance of our proposed federated
DDQN framework in comparison to federated DDQN and
distributed DDQN algorithms. In addition, we investigate the
impact of batch size, network layers, target network update
frequency on the convergence of the FDL. In what follows, we
ﬁrst focus on the impact of parameters of DRL on the learning
speed of our proposed algorithm and then the comparison of
the proposed algorithm with benchmarks would be presented.
To simulate our system, we consider a network of 100
IoT devices among which only 20 devices are selected in
each round to contribute in the FDL process. Without loss
of generality and for the sake of fair comparison, we assume
the maximum computation capacity and energy consumption
limit of the IoT devices are 1 Gbps and 23 dBm, respectively.
Fig. 2 demonstrates the effect of network architecture on
the convergence of our proposed FedRL algorithm. Here we
have some shallow networks with up to ﬁve layers and deeper
networks that are obtained by stacking multiple layers with
[16,32,32] neurons on each other. We note that by increasing
the number of layers, faster model training can be achieved.
The reason behind this observation is that, by exploiting
deeper neural networks, we can better ﬁnd the patterns in data
(here devices’ experiences), which subsequently improves the
quality of our local models. Thus, the global model is trained
much faster as its underlying components, local models, are
more accurate.

However, since our algorithm will be executed on IoT
devices that often lack necessary resources to train a deep
network, it may be infeasible to implement deeper neural
networks. Therefore, in the next ﬁgure, we select a rather
simple network architecture with [30,64,16,32,32] neurons in
each layer and instead look for other parameters that may
facilitate the learning process.

The other parameter we focus on in Fig. 3, is the batch size.
We can observe from this ﬁgure that as the batch size increases
the convergence of our proposed FedRL algorithm becomes

Fig. 2: The impact of neural network architecture on the
convergence of the proposed FedRL algorithm.

Fig. 3: The impact of batch size on the convergence of the
proposed FedRL algorithm.
faster. When batch size is equal to 10 and is extremely small
it takes up to 200 iterations to ﬁnally converge to a relative
global model, whereas in case batch size is 30, convergence
is achieved almost around iteration number 40. By increasing
the size of batches, we are basically training our model using
more data instances. which results in enhancing the quality of
local models and faster training process.

Similar to network architecture and the stated concern
regarding the limited computation capacity, memory is another
bottleneck in learning process of IoT devices. Larger batch size
means higher memory consumption. If the device is limited
both in CPU and memory capacity, neither very deep neural
networks nor increased batch size can be a proper solution to
facilitate deployment of FedRL on IoT devices.

To this end, in Fig. 4, we illustrate the effect of one of the
parameters of DDQN, namely frequency of updating target
network with online network. We can observe here that while
the effect of this parameter on performance of DDQN algo-
rithm is well investigated, this parameter is also considerably
effective in the performance of federated DDQNs. Since many
of the components in our state space, such as path-gain, QoS
of tasks, and the length of the task queue, are constantly
changing, efﬁcient choice of the frequency of updating target

050100150200250300Number of Iterations5060708090100110120Cost per Usernetwork layers= [30,64,16,32]network layers= [30,64,16,32,32]network layers= [30,64]+3*[16,32,32]network layers= [30,64]+4*[16,32,32]050100150200250300Number of Iterations60708090100110120Cost per Userbatch size = 10batch size = 20batch size = 30Fig. 4: The impact of target network update frequency on the
convergence of the proposed FedRL algorithm.

Fig. 5: Performance of federated DDQN compared to
federated DQN and distributed non-federated DDQN.

network can stabilize the environment enough for the agent to
track it better and obtain a better solution. This effect on local
models is quite notable on the FDL as well.

In Fig. 5, we compare the performance of our proposed
federated DDQN approach with those of federated DQN and
simple distributed DDQN without any aggregation. As can be
seen, the performance of federated DDQN is superior to feder-
ated DQN in terms of learning speed. As previously explained,
the main advantage of DDQN over DQN is the capability to
keep target network stationary, helping with the tractability
of states’ values and subsequently a faster convergence to the
correct estimation of them. The impact of this approach is even
more notable when DRL is combined with FDL, since if the
local models are not correctly trained, their errors would be
propagated to other devices’ local model through aggregation.
Therefore, aggregation can in fact negatively effect the result.
The comparison between distributed DDQN and federated
DDQN underlines that
the beneﬁts of federated DRL are
not limited to its scalability and privacy preservation. The
aggregation incorporated in FDL provides IoT devices a great
context to cooperatively train their models and merge their
intelligence together while preserving privacy of their infor-
mation. Exploiting federated learning, at every training round
is almost as if devices’ models are trained with I times more
data than their local information. The signiﬁcance of this share
of knowledge and not data is quite notable in Fig. 5, where as
the result of this aggregation step, federated DDQN is working
much better and faster than simple distributed DDQN. Note
that, averaging is used as the smoothing function.

VI. CONCLUSION

In this paper, we investigated the problem of joint delay
and energy minimization in an IoT network with a three-
tier ofﬂoading scheme. To solve this problem we combined
FDL, DDQN, and optimization theory. Combination of these
tools helped us to achieve a scalable, privacy-preserving,
and computationally efﬁcient framework for joint power and
computation resource allocation and ofﬂoading decision opti-
mization. In simulation results, we compared our work with

those of 1) federated DQN to demonstrate the superiority
of DDQN, especially in dynamic environments and 2) with
distributed DDQN to signify the impact of aggregation step
incorporated in FDL on the performance of the framework.
REFERENCES

[1] S. Zarandi and H. Tabassum, “Delay minimization in sliced multi-cell
mobile edge computing (mec) systems,” IEEE Communications Letters,
pp. 1–1, 2021.

[2] A. Khalili, S. Zarandi, and M. Rasti, “Joint resource allocation and
ofﬂoading decision in mobile edge computing,” IEEE Communications
Letters, vol. 23, no. 4, pp. 684–687, 2019.

[3] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y. C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks:
A comprehensive survey,” IEEE Communications Surveys Tutorials,
vol. 22, no. 3, pp. 2031–2063, 2020.

[4] S. Luo, X. Chen, Q. Wu, Z. Zhou, and S. Yu, “Hfel: Joint edge asso-
ciation and resource allocation for cost-efﬁcient hierarchical federated
edge learning,” IEEE Transactions on Wireless Communications, vol. 19,
no. 10, pp. 6535–6548, Oct 2020.

[5] J. X. X. Mo, “Energy-efﬁcient federated edge learning with joint

communication and computation design,” arXiv, Feb 2020.

[6] J. Yao and N. Ansari, “Enhancing federated learning in fog-aided iot
by cpu frequency and wireless power control,” IEEE Internet of Things
Journal, pp. 1–1, 2020.

[7] Y. Chen, X. Sun, and Y. Jin, “Communication-efﬁcient federated deep
learning with layerwise asynchronous model update and temporally
weighted aggregation,” IEEE Transactions on Neural Networks and
Learning Systems, vol. 31, no. 10, pp. 4229–4238, 2020.

[8] J. Mills, J. Hu, and G. Min, “Communication-efﬁcient federated learning
for wireless edge intelligence in iot,” IEEE Internet of Things Journal,
vol. 7, no. 7, pp. 5986–5994, 2020.

[9] Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, “A learning-based incentive
mechanism for federated learning,” IEEE Internet of Things Journal,
vol. 7, no. 7, pp. 6360–6368, 2020.

[10] Y. Zhan, P. Li, and S. Guo, “Experience-driven computational resource
allocation of federated learning by deep reinforcement learning,” in 2020
IEEE International Parallel and Distributed Processing Symposium
(IPDPS), 2020, pp. 234–243.

[11] X. Wang, C. Wang, X. Li, V. C. M. Leung, and T. Taleb, “Federated
deep reinforcement learning for internet of things with decentralized
cooperative edge caching,” IEEE Internet of Things Journal, vol. 7,
no. 10, pp. 9441–9455, 2020.

[12] J. Ren, H. Wang, T. Hou, S. Zheng, and C. Tang, “Federated learning-
based computation ofﬂoading optimization in edge computing-supported
internet of things,” IEEE Access, vol. 7, pp. 69 194–69 201, 2019.
[13] D. R. H. B. McMahan, E. Moore and B. A. Y. Arcas, “Federated
learning of deep networks using model averaging,” arXiv preprint
arXiv:1602.05629, 2016., 2016.

050100150200250300Number of Iterations60708090100110Cost per Userfrequency = 15frequency = 10frequency = 5050100150200250300Number of Iterations657075808590Smoothed Cost per UserFederated DQNFederated DDQNDistributed DDQN