Cross-modal Adversarial Reprogramming

*Paarth Neekhara, *Shehzeen Hussain, Jinglong Du, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley
University of California San Diego
{pneekhar,ssh028}@ucsd.edu
* Equal contribution

1
2
0
2

t
c
O
5
2

]
I

A
.
s
c
[

3
v
5
2
3
7
0
.
2
0
1
2
:
v
i
X
r
a

Abstract

With the abundance of large-scale deep learning models,
it has become possible to repurpose pre-trained networks
for new tasks. Recent works on adversarial reprogramming
have shown that it is possible to repurpose neural networks
for alternate tasks without modifying the network architec-
ture or parameters. However these works only consider
original and target tasks within the same data domain. In
this work, we broaden the scope of adversarial reprogram-
ming beyond the data modality of the original task. We ana-
lyze the feasibility of adversarially repurposing image clas-
siÔ¨Åcation neural networks for Natural Language Processing
(NLP) and other sequence classiÔ¨Åcation tasks. We design
an efÔ¨Åcient adversarial program that maps a sequence of
discrete tokens into an image which can be classiÔ¨Åed to the
desired class by an image classiÔ¨Åcation model. We demon-
strate that by using highly efÔ¨Åcient adversarial programs,
we can reprogram image classiÔ¨Åers to achieve competitive
performance on a variety of text and sequence classiÔ¨Åcation
benchmarks without retraining the network.

1. Introduction

Transfer

reprogram-
learning [28] and adversarial
ming [4] are two closely related techniques used for repur-
posing well-trained neural network models for new tasks.
Neural networks when trained on a large dataset for a par-
ticular task, learn features that can be useful across multi-
ple related tasks. Transfer learning aims at exploiting this
learned representation for adapting a pre-trained neural net-
work for an alternate task. Typically, the last few layers
of a neural network are modiÔ¨Åed to map to a new output
space, followed by Ô¨Åne-tuning the network parameters on
the dataset of the target task. Such techniques are espe-
cially useful when there is a limited amount of training data
available for the target task.

Adversarial reprogramming shares the same objective as
transfer learning with an additional constraint: the network
architecture or parameters cannot be modiÔ¨Åed. Instead, the

adversary can only adapt the input and output interfaces of
the network to perform the new adversarial task. This more
constrained problem setting of adversarial reprogramming
poses a security challenge to neural networks. An adver-
sary can potentially re-purpose cloud-hosted machine learn-
ing (ML) models for new tasks thereby leading to theft of
computational resources. Additionally, the attacker may re-
program models for tasks that violate the code of ethics of
the service provider. For example, an adversary can repur-
pose a cloud-hosted ML API for solving captchas to create
spam accounts.

Prior works on adversarial reprogramming [4, 21, 14, 32]
have demonstrated success in repurposing Deep Neural
Networks (DNNs) for new tasks using computationally in-
expensive input and label transformation functions. One in-
teresting Ô¨Ånding of [4] is that neural networks can be re-
programmed even if the training data for the new task has
no resemblance to the original data. The authors empiri-
cally demonstrate this by repurposing ImageNet [1] clas-
siÔ¨Åers on MNIST [16] digits with shufÔ¨Çed pixels showing
that transfer learning does not fully explain the success of
adversarial reprogramming. These results suggest that neu-
ral circuits hold properties that can be useful across multiple
tasks which are not necessarily related. Hence neural net-
work reprogramming not only poses a security threat, but
also holds the promise of more reusable and efÔ¨Åcient ML
systems by enabling shared compute of the neural network
backbone during inference time.

In existing work on adversarial reprogramming, the tar-
get adversarial task has the same data domain as the origi-
nal task. Recent work has shown that network architectures
based on the transformer model can achieve state-of-the-art
results on language [34], audio [29] and vision [2] bench-
marks suggesting that transformer networks serve as good
inductive biases in various domains. Given this common-
ality between the neural architectures in different domains,
an interesting question that arises is whether we can per-
form cross-modal adversarial reprogramming: For exam-
ple, Can we repurpose a vision transformer model for a lan-
guage task?

 
 
 
 
 
 
Cross-modal adversarial reprogramming increases the
scope of target tasks for which a neural network can be re-
purposed. In this work, we develop techniques to adversar-
ially reprogram image classiÔ¨Åcation networks for discrete
sequence classiÔ¨Åcation tasks. We propose a simple and
computationally inexpensive adversarial program that em-
beds a sequence of discrete tokens into an image and pro-
pose techniques to train this adversarial program subject to
a label remapping deÔ¨Åned between the labels of the origi-
nal and new task. We demonstrate that we can reprogram
a number of image classiÔ¨Åcation neural networks based on
both Convolutional Neural Network (CNN) [15] and Vision
Transformer [2] architectures to achieve competitive perfor-
mance on a number of sequence classiÔ¨Åcation benchmarks.
Additionally, we show that it is possible to conceal the ad-
versarial program as a perturbation in a real-world image
thereby posing a stronger security threat. The technical con-
tributions of this paper are summarized below:

‚Ä¢ We propose Cross-modal Adversarial Reprogram-
ming, a novel approach to repurpose ML models orig-
inally trained for image classiÔ¨Åcation to perform se-
quence classiÔ¨Åcation tasks. To the best of our knowl-
edge, this is the Ô¨Årst work that expands adversarial re-
programming beyond the data domain of the original
task.

‚Ä¢ We demonstrate the feasibility of our method by re-
purposing four image classiÔ¨Åcation networks for six
different sequence classiÔ¨Åcation benchmarks covering
topic, and DNA sequence classiÔ¨Åcation.
sentiment,
Our results show that a computationally-inexpensive
adversarial program can leverage the learned neural
circuits of the victim model and outperform word-
frequency based classiÔ¨Åers trained from scratch on
several tasks studied in our work.

‚Ä¢ We demonstrate for the Ô¨Årst time the threat imposed by
adversarial reprogramming to the transformer model
architecture by repurposing the Vision Transformer
model for six different sequence classiÔ¨Åcation tasks.
The reprogrammed transformer model outperforms al-
ternate architectures on Ô¨Åve out of six tasks studied in
our work.

2. Background and Related Work

2.1. Adversarial Reprogramming

Neural networks have been shown to be vulnerable to
adversarial examples [5, 26, 25, 20, 35, 33, 22, 10, 11, 9]
which are slightly perturbed inputs that cause victim models
to make a mistake. Adversarial Reprogramming was intro-
duced by [4] as a new form of adversarial threat that allows
an adversary to repurpose neural networks to perform new

tasks, which are different from the tasks they were origi-
nally trained for. The proposed technique trains a single
adversarial perturbation that can be added to all inputs in or-
der to re-purpose the target model for an attacker‚Äôs chosen
task. The adversary achieves this by Ô¨Årst deÔ¨Åning a hard-
coded one-to-one label remapping function that maps the
output labels of the adversarial task to the label space of the
classiÔ¨Åer; and learning a corresponding adversarial repro-
gramming function that transforms an input from the input
space of the new task to the input space of the classiÔ¨Åer.
The authors demonstrated the feasibility of their attack al-
gorithm by reprogramming ImageNet classiÔ¨Åcation models
for classifying MNIST and CIFAR-10 data in a white-box
setting, where the attacker has access to the victim model
parameters.

While the above attack does not require any changes to
the victim model parameters or architecture, the adversarial
program proposed [4] is only applicable to tasks where the
input space of the the original and adversarial task is con-
tinuous. To understand the feasibility of attack in a discrete
data domain, [21] proposed methods to repurpose text clas-
siÔ¨Åcation neural networks for alternate tasks, which operate
on sequences from a discrete input space. The attack algo-
rithm used a context-based vocabulary remapping method
that performs a computationally inexpensive input transfor-
mation to reprogram a victim classiÔ¨Åcation model for a new
set of sequences. This work was also the Ô¨Årst in designing
algorithms for training such an input transformation func-
tion in both white-box and black-box settings‚Äîwhere the
adversary may or may not have access to the victim model‚Äôs
architecture and parameters. They demonstrated the suc-
cess of their proposed reprogramming functions by adver-
sarially re-purposing various text-classiÔ¨Åcation models in-
cluding Long Short Term Memory networks (LSTM) [8],
bi-directional LSTMs [6] and CNNs [36] for alternate text
classiÔ¨Åcation tasks.

Recent works [14, 32] have argued that reprogramming
techniques can be viewed as an efÔ¨Åcient training method
and can be a superior alternative to transfer learning. Par-
ticularly [32] argue that one of the major limitations of cur-
rent transfer learning techniques is the requirement of large
amounts of target domain data, which is needed to Ô¨Åne-
tune pre-trained neural networks. They demonstrated the
advantage of instead using reprogramming techniques to re-
purpose existing ML models for alternate tasks, which can
be done even when training data is scarce. The authors
designed a black-box adversarial reprogramming method,
that can be trained iteratively from input-output model re-
sponses, and demonstrated its success in repurposing Im-
ageNet models for medical imaging tasks such as classiÔ¨Å-
cation of autism spectrum disorders, melanoma detection,
etc.

All of these existing reprogramming techniques are only

Figure 1. Schematic overview of our proposed cross-modal adversarial reprogramming method: The adversarial reprogramming function
fŒ∏ embeds a sequence of discrete tokens t into an image. The image can also be concealed as an additive addition to some real-world
image xc using the alternate reprogramming function f (cid:48)Œ∏. Finally, the victim model is queried with the generated image and the predicted
label is mapped to the target label using the label remapping function fL.

able to reprogram ML models when the data domain of the
target adversarial task and the original task are the same. We
address this limitation in our work by designing adversarial
input transformation functions that allow image classiÔ¨Åca-
tion models to be reprogrammed for sequence classiÔ¨Åcation
tasks such as natural language and protein sequence classi-
Ô¨Åcation.

2.2. Transformers and Image ClassiÔ¨Åers

While Convolutional Neural Networks (CNNs) have
long achieved state-of-the-art performance on vision
the recently proposed Vision Transformers
benchmarks,
(ViTs) [2] have been shown to outperform CNNs on several
image classiÔ¨Åcation tasks. Transformers [34] are known for
achieving state-of-the-art performance in natural language
processing (NLP). In order to train transformers for image
classiÔ¨Åcation tasks, the authors [2] divided an image into
patches and provide the sequence of linear embeddings of
these patches as an input to a transformer. Image patches
are treated the same way as tokens (words) in an NLP ap-
plication and the model is trained on image classiÔ¨Åcation
in a supervised manner. The authors report that when ViTs
are trained on large-scale image datasets, they are competi-
tive and also outperform state-of-the-art models on multiple
image recognition benchmarks.

3. Methodology

3.1. Problem DeÔ¨Ånition

Consider a victim image classiÔ¨Åer C trained for mapping

images x ‚àà X to a label lX ‚àà LX . That is,

C : x (cid:55)‚Üí lX

An adversary wishes to repurpose this victim image classi-
Ô¨Åer for an alternate text classiÔ¨Åcation task C (cid:48) of mapping
sequences t ‚àà T to a label lT ‚àà LT . That is,

C (cid:48) : t (cid:55)‚Üí lT

To achieve this goal, the adversary needs to learn appropri-
ate mapping functions between the input and output spaces
of the original and the new task. We solve this by Ô¨Årst deÔ¨Ån-
ing a label remapping fL that maps label spaces of the two
tasks: fL : lX (cid:55)‚Üí lT ; and then learning a corresponding
adversarial program fŒ∏ that maps a sequence t ‚àà T to an
image x ‚àà X i.e., fŒ∏ : t (cid:55)‚Üí x such that fL(C(fŒ∏(t))) acts as
the target classiÔ¨Åer C (cid:48).

We assume a white-box adversarial reprogramming set-
ting where the adversary has complete knowledge about ar-
chitecture and model parameters of the victim image clas-
siÔ¨Åer. In the next few sections we describe the adversarial
program fŒ∏, the label remapping function and the training
procedure to learn the adversarial program.

3.2. Adversarial Program

Since transformers can model both language and vision
data in a similar manner, that is, as a sequence of embed-
dings, we are curious to investigate whether a vision trans-
former can be reprogrammed for a text classiÔ¨Åcation task.
In the process, we Ô¨Ånd that CNN network architectures can
also be reprogrammed to achieve competitive performance
on discrete sequence classiÔ¨Åcation tasks. In the next section,
we discuss our cross-modal adversarial reprogramming ap-
proach.

The goal of our adversarial program is to map a sequence
of discrete tokens t ‚àà T to an image x ‚àà X. Without loss
of generalizability, we assume X = [‚àí1, 1]h√ów √óc to be
the scaled input space of the image classiÔ¨Åer C where h, w
are the height and width of the input image and c is the
number of channels. The tokens in the sequence t belong to
some vocabulary list VT . We can represent the sequence t
as t = t1, t2, . . . , tN where ti is the vocabulary index of the
ith token in sequence t in the vocabulary list VT .

fŒ∏(t)VictimImage Classifier (C)Adversarial Program (f‚ÄôŒ∏)Label Remapping (fL)ùùêxf‚ÄôŒ∏(t)Prediction: JoyEmbedding Lookup(Œ∏)amveryIexcited<pad>..Sequence (t)JoyAngerSorrowOceanSkySandBridge‚Ä¶..Original LabelsNew LabelsxcWhen designing the adversarial program it is important
to consider the computational cost of the reprogramming
function fŒ∏. This is because if a classiÔ¨Åcation model that
performs equally well can be trained from scratch for the
classiÔ¨Åcation task C (cid:48) and is computationally cheaper than
the reprogramming function, it would defeat the purpose of
adversarial reprogramming.

Keeping the above in mind, we design a reprogramming
function that looks up embeddings of the tokens ti and ar-
ranges them as contiguous patches of size p √ó p in an image
that is fed as input to the classiÔ¨Åer C. Mathematically, the
reprogramming function fŒ∏ is parameterized by a learnable
embedding tensor Œ∏|VT |√ó|p|√ó|p|√ó|c| and performs the trans-
formation fŒ∏ : t (cid:55)‚Üí x as per Algorithm 1.

Algorithm 1 Adversarial Program fŒ∏
Input: Sequence t = t1, t2, . . . , tN
Output: Reprogrammed image xh√ów√óc
Parameters: Embedding tensor Œ∏|VT |√ó|p|√ó|p|√ó|c|
x ‚Üê 0h√ów√óc
for each tk in t do
i ‚Üê (cid:98)(k √ó p)/h(cid:99)
j ‚Üê (k √ó p) mod w
x[i : i + p, j : j + p, :] ‚Üê tanh(Œ∏[tk, :, :, :])

end for
return x

The patch size p and image dimensions h, w determine
the maximum length of the sequence t that can be encoded
into the image. We pad all the input sequences t all the way
up to the maximum allowed sequence length with a padding
token to Ô¨Åll up the reprogrammed image and clip any se-
quences longer than the maximum allowed length from the
end. More details about the hyper-parameters can be found
in our experiments section.

Concealing the adversarial perturbation: Most past
works on adversarial reprogramming have considered an
unconstrained attack setting, where the reprogrammed im-
age does not necessarily need to resemble a real-world im-
age. However, as noted by [4], it is possible to conceal the
reprogrammed image in a real-world image by constrain-
ing the output of the reprogramming function. We can con-
ceal the reprogrammed image as an additive perturbation
to some real-world base image xc by deÔ¨Åning an alternate
reprogramming function f (cid:48)

Œ∏ as follows:

f (cid:48)
Œ∏ (t) = Clip[‚àí1,1](xc + (cid:15).fŒ∏(t))

(1)

Since the output of the original reprogramming function fŒ∏
is bounded between [‚àí1, 1], we can control the L‚àû norm
of the added perturbation using the parameter (cid:15) ‚àà [0, 1].

Computational Complexity: As depicted in Figure 1,
during inference, the adversarial program only looks up em-
beddings of the tokens in the sequence t and arranges them

in an image tensor which can optionally be added onto a
base image. Asymptotically, the time complexity of this
adversarial program is linear in terms of the length of the
sequence t. Since there are no matrix-vector multiplica-
tions involved in the adversarial program, it is computation-
ally equivalent to just the embedding layer of a sequence-
based neural classiÔ¨Åer. Therefore the inference cost of
the adversarial program is signiÔ¨Åcantly less than that of a
sequence-based neural classiÔ¨Åer. Table 1 in our supplemen-
tary material compares the wall-clock inference time for a
sequence of length 500 for our adversarial program and var-
ious sequence-based neural classiÔ¨Åers used in our experi-
ments.

3.3. Label Remapping and Optimization Objective

Past works [4, 21, 32] on adversarial reprogramming as-
sume that the number of labels in the target task are less
than than the number of labels in the original task. In our
work, we relax this constraint and propose label remapping
functions for both of the following scenarios:

1. Target task has fewer labels than the original task:
Initial works on adversarial reprogramming deÔ¨Åned a one-
to-one mapping between the labels of the original and new
task [4, 21]. However, recent work [32] found that mapping
multiple source labels to one target label helps improve the
performance over one-to-one mapping. Our preliminary ex-
periments on cross-modal reprogramming conÔ¨Årm this Ô¨Ånd-
ing, however, we differ in the way the Ô¨Ånal score of a target
label lt is aggregated‚Äî[32] obtained the Ô¨Ånal score for a
target label as the mean of the scores of the mapped orig-
inal labels. We found that aggregating the score by taking
the maximum rather than the mean over the mapped origi-
nal labels leads to faster training. Another advantage of us-
ing max reduction is that during inference, we can directly
map the original predicted label to our target label without
requiring access to probability scores of any other label.

Consider a target task label lt, mapped to a subset of
labels LSt ‚äÇ LS of the original task under the many-to-one
label remapping function fL. We obtain the score for this
target task label as the maximum of the scores of each label
li ‚àà LSt by classiÔ¨Åer C. That is,

Z (cid:48)

lt(t) = max
li‚ààLSt

Zli(fŒ∏(t)),

(2)

where Zk(x) and Z (cid:48)
k(t) represent the score (before softmax)
assigned to some label k by classiÔ¨Åer C and C (cid:48) respectively.
To deÔ¨Åne the label remapping fL, instead of randomly
assigning m source labels to a target label, we Ô¨Årst obtain
the model predictions on the base image xc (or a zero image
in the case of an unbounded attack) and sort the labels by
the obtained scores; We then assign the the highest scored
source labels to each target label using a round-robin strat-
egy until we have assigned m source labels to each target

label.

Note that while we need access to individual class scores
during training (where we assume a white-box attack set-
ting), during inference we can simply map the highest pre-
dicted label to the target label using the label remapping
function fL without having to know the actual scores as-
signed to different labels.

2. Original task has fewer labels than the target task:
In this scenario, we map the probability distribution over the
original labels to a distribution over target labels to class
scores for the target label space using a learnable linear
transformation. That is,

Accuracy (%)

Model

Abbr.

Type

# Params Top-1 Top-5

ViT
ViT-Base
ResNet-50
RN-50
InceptionNet-V3 IN-V3
EfÔ¨ÅcientNet-B4 EN-B4

Transformer
CNN
CNN
CNN

86.9M 84.2
25.6M 79.0
23.8M 77.5
19.3M 83.0

97.2
94.4
93.5
96.3

Table 1. Victim image classiÔ¨Åcation networks used for adversarial
reprogramming experiments. We include the number of parame-
ters of each model and also the Top-1 and Top-5 test accuracy
achieved on the ImageNet benchmark.

Z (cid:48)(t) = Œ∏(cid:48)

|LT |√ó|LX | ¬∑ softmax (Z(fŒ∏(t))).

(3)

4.2. Datasets and Reprogramming Tasks

Here Z (cid:48)(t) is a vector representing class scores (log-
its) for the target label space. Œ∏(cid:48)
|LT |√ó|LX | are the learnable
parameters of the linear transformation that are optimized
along with the parameters of the reprogramming function
fŒ∏. Note that unlike the previous scenario, in this setting,
we assume that we have access to the probability scores of
the original labels during both training and inference.

Optimization Objective: To train the parameters Œ∏ of
our adversarial program, we use a cross-entropy loss be-
tween the target label and the model score predictions ob-
tained as per Equation 2 or Equation 3. We also incorporate
an L2 regularization loss for better generalization on the test
set and to encourage more imperceptible perturbation in the
case of our bounded attack. Therefore our Ô¨Ånal optimiza-
tion objective is the following:

Plt = softmax (Z (cid:48)(t))lt

E(Œ∏) = ‚àí (cid:80)
t‚ààT

log(Plt) + Œª||Œ∏||2
2.

Here Œª is the regularization hyper-parameter and Plt is the
predicted class probability of the correct label lt for se-
quence t. We use mini-batch gradient descent using an
Adam optimizer [13] to solve the above optimization prob-
lem on the dataset of the target task.

4. Experiments

4.1. Victim Image ClassiÔ¨Åers

To demonstrate cross-modal adversarial reprogramming,
we perform experiments on four neural architectures trained
on the ImageNet dataset. We choose both CNNs and the re-
cently proposed Vision Transformers (ViT) [2] as our victim
image classiÔ¨Åers. While CNNs have long achieved state-
of-the-art performance on computer-vision benchmarks, the
recently proposed ViTs have been shown to outperform
CNNs on several image classiÔ¨Åcation tasks. We choose
the ViT-Base [2], ResNet-50 [7], InceptionNet-V3 [30] and
EfÔ¨ÅcientNet-B7 [31] architectures. The details of these ar-
chitectures are listed in Table 1. We perform experiments
on both pre-trained and randomly initialized networks.

In this work, we repurpose the aforementioned image
classiÔ¨Åers for several discrete sequence classiÔ¨Åcation tasks.
We wish to analyze the performance of cross-modal ad-
versarial reprogramming for different applications such as
understanding language and analyzing sequential biomedi-
cal data. Biomedical datasets e.g. splice-junction detection
in genes, often have fewer training samples than language
based datasets and we aim to understand whether such lim-
itations can adversely affect our proposed reprogramming
technique.

Sentiment analysis and topic classiÔ¨Åcation are popular
NLP tasks. However, analyzing the underlying semantics
of the sequence is often not necessary for solving these
tasks since word-frequency based statistics can serve as
strong discriminatory features. In contrast, tasks like DNA-
sequence classiÔ¨Åcation requires analyzing the sequential se-
mantics of the input and simple frequency analysis of the
unigrams or n-grams does not achieve competitive perfor-
mance on these tasks. To evaluate the effectiveness of ad-
versarial reprogramming in both of these scenarios, we con-
sider the following tasks and datasets in our experiments:

4.2.1 Sentiment ClassiÔ¨Åcation

1. Yelp Polarity Dataset (Yelp) [36]: This is a dataset con-
sisting of reviews from Yelp for the task of sentiment classi-
Ô¨Åcation, categorized into binary classes of positive and neg-
ative sentiment.
2. Large Movie Review Dataset (IMDB) [18]: This is a
dataset for binary sentiment classiÔ¨Åcation of positive and
negative sentiment from highly polar IMDB movie reviews.

4.2.2 Topic ClassiÔ¨Åcation

1. AG‚Äôs News Dataset (AG) [36]: is a collection of more
than 1 million news articles. News articles have been gath-
ered from more than 2000 news sources and contains 4
classes: World, Sports, Business, Sci/Tech.
2. DBPedia Ontology Dataset (DBPedia) [36]:
con-
sists of 14 non-overlapping categories from DBpedia 2014.

Dataset Statistics

Accuracy (%)

Avg

Neural Methods

TF-IDF

Dataset Task Type # Classes # Train # Test

Token

Length Bi-LSTM 1D-CNN unigram n-gram

Yelp
IMDB

Sentiment
Sentiment

AG
DBPedia

Splice
H3

Topic
Topic

DNA
DNA

2
2

4
14

3
2

560,000 38,000
25,000 25,000

120,000
7,600
560,000 70,000

word
word

word
word

135.6
246.8

57.0
47.1

2,700
13,468

490 neucleobase
1,497 neucleobase

60.0
500.0

95.94
89.43

91.45
97.78

93.26
86.84

95.18
90.02

92.09
98.09

83.87
85.43

92.50
88.52

90.92
97.12

51.42
75.68

92.93
88.43

90.69
97.16

72.24
78.89

Table 2. Statistics of the datasets used for our reprogramming tasks. We also include the test accuracy of both neural network based and
TF-IDF based benchmark classiÔ¨Åers trained from scratch on the train set.

The samples consist of the category and abstract of each
Wikipedia article.

4.2.3 DNA Sequence ClassiÔ¨Åcation

1.
This
Splice-junction Gene Sequences (Splice):
dataset [24, 3] was curated for training ML models to de-
tect splice junctions in DNA sequences. In DNA, there are
two kinds of splice junction regions: Exon-Intron (EI) junc-
tion and Intron-Exon (IE) junction. This dataset contains
sample DNA sequences of 60 base pair length categorized
into 3 classes: ‚ÄúEI‚Äù which contains exon-intron junction,
‚ÄúIE‚Äù which contains intron-exon junction, and ‚ÄúN‚Äù which
contain neither EI or IE regions.
2. Histone Protein Occupancy in DNA (H3): This dataset
from [27, 23] indicates whether certain DNA sequences
wrap around H3 histone proteins. Each sample is a se-
quence with a length of 500 neucleobases. Positive sam-
ples contain DNA regions wrapping around histone proteins
while negative samples do not contain such DNA regions.

The statistics of these datasets are included in Table 2.
To benchmark the performance that can be achieved on
these tasks, we train various classiÔ¨Åers from scratch on the
datasets for each task. We consider both neural network
based classiÔ¨Åcation models and frequency-based statistical
models (such as TF-IDF) as our benchmarks. We use word-
level tokens for sentiment and topic classiÔ¨Åcation tasks and
neucleobase level tokens for DNA sequence classiÔ¨Åcation
tasks.

The TF-IDF methods can work on either unigrams or n-
grams for creating the feature vectors from the input data.
For the n-gram model, we consider n-grams up to length 3
and choose the value of n that achieves the highest classi-
Ô¨Åcation accuracy on the hold-out set. We train a Stochas-
tic Gradient Descent (SGD) classiÔ¨Åer to classify the feature
vector as one of the target classes. Additionally, we train
DNN based text-classiÔ¨Åers: Bidirectional Long Short Term
Memory networks (Bi-LSTM) [6, 8] and 1D CNN [12]

models from scratch on the above tasks. We use randomly
initialized token embeddings for all classiÔ¨Åcation models,
which are trained along with the network parameters. For
Bi-LSTMs, we combine the outputs of the Ô¨Årst and last
time step for prediction. For the Convolutional Neural Net-
work we follow the same architecture as [12]. The hyper-
parameter details of these classiÔ¨Åers and architecture have
been included in Table 2 of the supplementary material.

We report the accuracies on the test set of the above men-
tioned classiÔ¨Åers in Table 2. We Ô¨Ånd that while both neural
and frequency based TF-IDF methods work well on senti-
ment and topic classiÔ¨Åcation tasks, neural networks signif-
icantly outperform frequency based methods on DNA se-
quence classiÔ¨Åcation tasks. This is presumably because the
latter require structural analysis of the sequence rather than
relying on keywords.

4.3. Experimental Details

Input image size and patch size: The ViT-Base model
utilized in our work is trained on images of size 384 √ó 384
and works on image patches of size 16 √ó 16. For all our
experiments, we Ô¨Åx the input image size to be 384 √ó 384.
When we use a patch of size 16 √ó 16 for encoding a sin-
gle token in our sequence, it allows for a maximum of 576
tokens to be encoded into a single image. In our initial ex-
periments we found that using larger patch sizes for smaller
sequences leads to higher performance on the target task,
since it encodes a sequence in a spatially larger area of the
image. Therefore, we choose our patch size as the largest
possible multiple of 16 that can encode the longest sequence
in our target task dataset. We list the patch size p used for
different tasks in Table 3.

Training hyper-parameters: We train each adversar-
ial program on a single Titan 1080i GPU using a batch
size of 4. We set the learning rate as 0.001 for the un-
bounded attacks and 0.001 √ó (cid:15)‚àí1 for our bounded attacks
(Equation 1). We set the L2 regularization hyper-parameter
Œª = 1e ‚àí 4 for all our experiments and train the adversarial

Unbounded

Bounded (L‚àû = 0.1)

Pre-trained

Randomly Initialized

Pre-Trained

p ViT RN-50 IN-V3 EN-B4 ViT RN-50 IN-V3 EN-B4 ViT RN-50 IN-V3 EN-B4

Task

Yelp
IMDB

16 92.82 93.29
16 86.76 85.60

AG
16 91.59 89.88
DBPedia 32 97.62 96.31

Splice
H3

48 95.31 94.48
16 82.57 78.16

89.19
80.67

89.78
95.70

95.10
80.29

93.47 92.73 68.50
87.26 88.38 81.08

90.46 91.45 82.37
96.77 97.56 30.12

92.04 54.13 48.57
80.16 77.02 73.00

65.56
52.87

50.43
52.87

91.22
64.20

52.97 88.57 81.32
50.26 82.07 72.28

24.87 86.49 83.26
19.61 92.79 80.64

50.20 95.10 94.27
51.17 76.62 72.01

81.33
71.22

78.93
81.46

94.89
75.55

81.23
81.42

84.03
79.53

91.55
75.42

Table 3. Results (% Accuracy on the test set) of adversarial reprogramming experiments targeting four image classiÔ¨Åcation models for the
six sequence classiÔ¨Åcation tasks. In the unbounded attack setting, we target both pre-trained and randomly initialized image classiÔ¨Åers. In
norm of 0.1) to a randomly
the bounded attack setting, the output of the reprogramming function is concealed as a perturbation (with L
selected ImageNet image shown in Figure 2.

‚àû

program for a maximum 100k mini-batch iterations in the
unbouned attack setting and for 200k mini-batch iterations
in the bounded attack setting. We map 10 original labels to
each target label in the scenario when there are fewer labels
for the target task than for the original task. We point the
readers to our codebase for precise implementation.1

5. Results

5.1. Pre-trained vs untrained victim models

Experimental results of our proposed cross-modal repro-
gramming method are reported in Table 3. In these experi-
ments, the original task has more labels than the target task
so we use the label remapping function given by Equation 2.
We Ô¨Årst consider the unbounded attack setting, where the
output of the adversarial program does not need to be con-
cealed in a real-world image. For these experiments, we use
the reprogramming function fŒ∏ described in Algorithm 1.
We also note that the primary evaluation of past reprogram-
ming works [4, 21, 32] is done in an unbounded attack set-
ting.

When attacking pre-trained image classiÔ¨Åers, we achieve
competitive performance (as compared to benchmark clas-
siÔ¨Åers trained from scratch, reported in Table 2) across sev-
eral tasks for all victim image classiÔ¨Åcation models. To as-
sess the importance of pre-training the victim model on the
original dataset, we also experiment with reprogramming
untrained randomly initialized networks.

Randomly initialized neural networks can potentially
have rich structure which the reprogramming functions can
exploit. Prior works [19, 17] have shown that wide neural
networks can behave as Gaussian processes, where train-
ing speciÔ¨Åc weights in the intermediate layers is not nec-
essary to perform many different tasks. However, in our

1https://github.com/paarthneekhara/multimodal_

rerprogramming

experiments, we Ô¨Ånd that for CNN-based image classiÔ¨Åers,
reprogramming pre-trained neural networks performs sig-
niÔ¨Åcantly better than reprogramming randomly initialized
networks for all tasks. This is consistent with the Ô¨Ånd-
ings of prior reprogramming work [4] which reports that
adversarial reprogramming in the image domain is more
effective when it targets pre-trained CNNs. For the ViT
model, we Ô¨Ånd that we are able to obtain competitive per-
formance on sentiment and topic classiÔ¨Åcation tasks when
reprogramming either randomly initialized or pre-trained
models. Particularly, we Ô¨Ånd that reprogramming untrained
vision transformers provides the highest accuracy on the
IMDB classiÔ¨Åcation task. However, for DNA sequence
classiÔ¨Åcation tasks (Splice and H3) that require structural
analysis of the sequence rather than token-frequency statis-
tics, we Ô¨Ånd that reprogramming pre-trained vision trans-
former model performs signiÔ¨Åcantly better than a randomly
initialized transformer model.

The ViT model outperforms other architectures on 5 out
of 6 tasks in the unbounded attack setting.
In particular,
for the task of splice-junction detection in gene sequences,
reprogramming a pre-trained ViT model outperforms both
TF-IDF and neural classiÔ¨Åers trained from scratch. For sen-
timent analysis and topic classiÔ¨Åcation tasks, which primar-
ily require keyword detection, some reprogramming meth-
ods achieve competitive performance as the benchmark
methods reported in Table 2.

Additionally, to assess the importance of the victim clas-
siÔ¨Åer for solving the target task, we study the extent to
which the task can be solved without the victim classiÔ¨Åer
and using only the adversarial reprogramming function with
a linear classiÔ¨Åcation head. We present the results and de-
tails of this experiment in Table 3 of our supplementary ma-
terial.

Concealing the adversarial perturbation: To conceal
the output of the adversarial program in a real-world image,

cause we learn an additional mapping function for the out-
put interface, which can potentially lead to better optimiza-
tion. However as a downside, this setting requires access to
all q class probability scores for predicting the adversarial
label, while in the previous many-to-one label remapping
scenario, we only need to know the highest-scored original
label for mapping it to one of the adversarial labels.

Accuracy (%)

Dataset

# Labels q ViT RN-50 IN-V3 EN-B4

AG
DBPedia
DBPedia

4
14
14

3 89.42 87.18
3 96.34 83.16
10 98.01 96.84

86.66
84.17
94.88

89.18
92.95
97.16

Table 4. Results of adversarial reprogramming in the scenario
when the target task has more labels than the original task. The
access of the adversary is constrained to class-probabilities of q
labels of the original (ImageNet) task. This evaluation is done on
pre-trained networks in an unbounded attack setting.

6. Conclusion

We propose Cross-modal Adversarial Reprogramming,
which for the Ô¨Årst time demonstrates the possibility of re-
purposing pre-trained image classiÔ¨Åcation models for se-
quence classiÔ¨Åcation tasks. We demonstrate that compu-
tationally inexpensive adversarial programs can repurpose
neural circuits to non-trivially solve tasks that require struc-
tural analysis of sequences. Our results suggest the potential
of training more Ô¨Çexible neural models that can be repro-
grammed for tasks across different data modalities and data
structures. More importantly, this work reveals a broader
security threat to public ML APIs that warrants the need for
rethinking existing security primitives.

7. Acknowledgements

This work was supported by ARO under award number
W911NF1910317, SRC under Task ID: 2899.001 and DoD
UCR W911NF2020267 (MCA S-001364).

References

[1] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-
Imagenet: A large-scale hierarchical image database.

Fei.
In CVPR, 2009.

[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2021.

[3] Dheeru Dua and Casey Graff. UCI machine learning reposi-

tory, 2017.

Figure 2. Example outputs of our adversarial reprogramming func-
tion in both unbounded (top) and bounded (bottom) attack settings
while reprogramming two different pre-trained image classiÔ¨Åers
for a DNA sequence classiÔ¨Åcation task (H3).

we follow the adversarial reprogramming function deÔ¨Åned
in Equation 1. We randomly select an image from the Ima-
geNet dataset (shown in Figure 2) as the base image xc and
train adversarial programs targeting different image classi-
Ô¨Åers for the same base image. We present the results at
L‚àû = 0.1 (on a 0 to 1 pixel value scale) distortion between
the reprogrammed image and the base image xc on the right
side of Table 3. It can be seen that for some drop in perfor-
mance, it is possible to perform adversarial reprogramming
such that the input sequence is concealed in a real-world
image. Figure 1 in our supplementary material shows the
accuracy on three target tasks for different magnitudes of al-
lowed perturbation, while reprogramming a pre-trained ViT
model.

5.2. Target task has more labels than original task

In a practical attack scenario, the adversary may only
have access to a victim image classiÔ¨Åer with fewer labels
than the target task labels. To evaluate adversarial repro-
gramming in this scenario, we constrain the adversary‚Äôs ac-
cess to the class-probability scores of just q labels of the Im-
ageNet classiÔ¨Åer. We choose the most frequent q ImageNet
labels as the original labels, that can be accessed by the ad-
versary; and perform our experiments on two tasks from
our datasets, which have the highest number of labels‚ÄîAG
News (4 labels) and DBPedia (14 labels). We use the label
remapping function given by Equation 3, and learn a linear
transformation to map the predicted probability distribution
over the q original labels to the target task label scores.

We demonstrate that we are able to perform adversarial
reprogramming even in this more constrained setting. We
achieve similar performance as compared to our many-to-
one label remapping scenario reported in Table 3 when q is
close to the number of labels in the target task. This is be-

DNA H3 Task Input SequenceACTCAGTCAGAAAACTGAATTTAGTTGATATGGGACCGCTCCAAGGTAGGAGAATACTAGATCAAGTAAAGCAACCGCACTAGTGCCTTTTTCAAACAAGGTGGTTTGATGAGGAGGCTTTCTACAATCCTAGAAATATAAGACATCTG‚Ä¶.Unbounded ViT Unbounded ResNet-50Bounded (L‚àû=0.1)  ViT Base ImageBounded (L‚àû=0.1)  ResNet-50 [4] Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-
Dickstein. Adversarial reprogramming of neural networks.
In ICLR, 2019.

[5] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In ICLR,
2015.

[6] Alex Graves, Santiago Fern¬¥andez, and J¬®urgen Schmidhuber.
Bidirectional lstm networks for improved phoneme classiÔ¨Å-
cation and recognition. In International Conference on Arti-
Ô¨Åcial Neural Networks: Formal Models and Their Applica-
tions, 2005.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[8] Sepp Hochreiter and J¬®urgen Schmidhuber. Long short-term

memory. Neural Comput., 1997.

[9] Shehzeen Hussain, Paarth Neekhara, Brian Dolhansky,
Joanna Bitton, Cristian Canton Ferrer, Julian McAuley, and
Farinaz Koushanfar. Exposing vulnerabilities of deepfake
detection systems with robust attacks. ACM Journal of Dig-
ital Threats: Research and Practice.

[10] Shehzeen Hussain, Paarth Neekhara, Shlomo Dubnov, Ju-
lian McAuley, and Farinaz Koushanfar. Waveguard: Un-
In
derstanding and mitigating audio adversarial examples.
30th USENIX Security Symposium (USENIX Security 21).
USENIX Association, 2021.

[11] Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz
Koushanfar, and Julian McAuley. Adversarial deepfakes:
Evaluating vulnerability of deepfake detectors to adversarial
examples. In WACV, 2021.

[12] Yoon Kim. Convolutional neural networks for sentence clas-

siÔ¨Åcation. In EMNLP, 2014.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. arXiv:1412.6980, 2014.

[14] Eliska Kloberdanz. Reprogramming of neural networks: A
new and improved machine learning technique. Masters The-
sis, 2020.

[15] Yann LeCun, L¬¥eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 1998.

[16] Yann LeCun and Corinna Cortes. MNIST handwritten digit

database. 2010.

[17] Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Ro-
man Novak, Sam Schoenholz, and Yasaman Bahri. Deep
neural networks as gaussian processes. In ICLR, 2018.
[18] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. Learning word
In Human Language Tech-
vectors for sentiment analysis.
nologies, 2011.

[19] Alexander Matthews, Jiri Hron, Mark Rowland, Richard E.
Turner, and Zoubin Ghahramani. Gaussian process be-
haviour in wide deep neural networks. In ICLR, 2018.
[20] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard.
Universal adversarial perturbations. In CVPR, 2017.
[21] Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and
Farinaz Koushanfar. Adversarial reprogramming of text clas-
siÔ¨Åcation neural networks. In EMNLP, 2019.

[22] Paarth Neekhara, Shehzeen Hussain, Prakhar Pandey,
Shlomo Dubnov, Julian McAuley, and Farinaz Koushanfar.
Universal Adversarial Perturbations for Speech Recognition
Systems. In Proc. Interspeech, 2019.

[23] Nguyen Ngoc Giang, Vu Tran, Duc Ngo, Dau Phan, Fa-
vorisen Lumbanraja, M Reza Faisal, Bahriddin Abapihi,
Mamoru Kubo, and Kenji Satou. Dna sequence classiÔ¨Åca-
tion by convolutional neural network. Journal of Biomedical
Science and Engineering, 2016.

[24] Michiel O. Noordewier, Geoffrey G. Towell, and Jude W.
Shavlik. Training knowledge-based neural networks to rec-
ognize genes in dna sequences. In NIPS, 1990.

[25] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Prac-
tical black-box attacks against deep learning systems using
adversarial examples. arXiv, abs/1602.02697, 2016.
[26] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt
Fredrikson, Z. Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. arXiv,
abs/1511.07528, 2015.

[27] Dmitry Pokholok, Christopher Harbison, Stuart Levine,
Megan Cole, Nancy Hannett, Tong Lee, George Bell,
Kimberly Walker, P Rolfe, Elizabeth Herbolsheimer, Julia
Zeitlinger, Fran Lewitter, David Gifford, and Richard Young.
Genome-wide map of nucleosome acetylation and methyla-
tion in yeast. Cell, 2005.

[28] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,
and Andrew Y. Ng. Self-taught learning: Transfer learning
from unlabeled data. In ICML, 2007.

[29] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou
Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and control-
lable text to speech. In Neurips, 2019.

[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, 2016.

[31] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model

scaling for convolutional neural networks. In ICML, 2019.

[32] Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho. Transfer
learning without knowing: Reprogramming black-box ma-
chine learning models with scarce data and limited resources.
In ICML, 2020.

[33] James Tu, Mengye Ren, Sivabalan Manivasagam, Ming
Liang, Bin Yang, Richard Du, Frank Cheng, and Raquel Ur-
tasun. Physically realizable adversarial examples for lidar
object detection. In CVPR, 2020.

[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Å ukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS, 2017.
[35] Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang,
and Michael I. Jordan. Greedy attack and gumbel attack:
Generating adversarial examples for discrete data. arXiv,
2018.

[36] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-
level convolutional networks for text classiÔ¨Åcation. In NIPS,
2015.

Cross-modalAdversarialReprogramming-SupplementaryMaterial1.Wall-clockinferencetimeofreprogrammingfunctionInTable1,wereportthewallclockinferencetimefortheadversarialprogramandthebenchmarktextclassiÔ¨Åersstudiedinourworkforasequenceoflength500.BoththeBi-LSTMandCNNmodeluse256hiddenunitsandanembeddingsizeof256.WeuseasinglelayerBi-LSTMnetworkand1-DCNNwithconvolutionÔ¨Åltersofsize3,4and5basedonthearchitectureproposedin(Kim,2014).Fortheadversarialprogramthepatchsizeis16X16andtheoutputimagesizeis384X384.Weaveragetheinferencetimefor100sequencesfortheseevaluations.ItcanbeseenthattheadversarialprogramissigniÔ¨ÅcantlyfasterthanbothBi-LSTMand1D-CNNmodelsforbothCPUandGPUimplementationsinPyTorch.TheCPUusedfortheseevaluationsisIntelXeonCPUandtheGPUisanNvidiaTitan1080i.ModelCPUGPUAdversarialProgram7.9ms0.2msBi-LSTM161.5ms13.9ms1DCNN383.2ms2.2msTable1.Wallclockinferencetime(inmiliseconds)fortheadver-sarialprogramandthebenchmarktextclassiÔ¨Åersstudiedinourworkforasequenceoflength500.2.Hyper-parameterdetailsofbenchmarkclassiÔ¨ÅersFortrainingthebenchmarkneural-textclassiÔ¨Åers,weusetheBi-LSTMand1D-CNNmodelwithasoftmaxclassiÔ¨Åcationhead.Forbothofthesemodels,thetokenembeddinglayerisrandomlyinitializedandtrainedwiththemodelparameters.WeuseAdamoptimizerandperformmini-batchgradientdescentusingabatchsizeof32forbothofthesemodelsforamaximum200kmini-batchiterations.Forthe1D-CNNmodelsweuseÔ¨Åltersofsize3,4and5forthethreeconvolutionallayers.Otherhyper-parameterdetailsofthesemodelsarelistedinTable2.ModelHiddenUnitsEmb.Size#LayersLRBi-LSTM25625611e-41DCNN25625631e-4Table2.Hyper-parameterdetailsfortheneuralsequenceclassiÔ¨ÅersusedasbenchmarkclassiÔ¨Åersinourwork.LR:LearningRate.Emb.Size:EmbeddingSize.3.PerturbationamountvsAccuracyinBoundedattacksFigure1showstheperformanceofcross-modaladversarialreprogrammingatdifferentmagnitudesofallowedperturba-tionintheboundedattacksetting,whileattackingtheViTmodel.Weusethesamebaseimagexcasusedinallofourboundedattackexperiments.Figure1.AccuracyvsL‚àûnormoftheperturbationwhilerepro-grammingtheViTmodelforthreetargettaskscoveringemotion,topicandDNAsequenceclassiÔ¨Åcation.4.AssessingtheimportanceofVictimModelToassesstheimportanceofthevictimmodelforsolvingthetargettask,weperformanexperimenttounderstandtheextenttowhichthetargettaskcanbesolvedbyusingonlytheadversarialreprogrammingfunctionwithalinearclassi-Ô¨Åcationheadontop.Toperformthisexperiment,wetakethemeanofalltokenembeddingsinasequenceandpassitasinputtoalinearlayerthatpredictstheclassscoresforthetargettask.Notsurprisingly,thisclassiÔ¨Åerworkswellfor