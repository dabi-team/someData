1
2
0
2

y
a
M
2

]

G
L
.
s
c
[

2
v
8
7
4
3
1
.
4
0
1
2
:
v
i
X
r
a

Geometric Deep Learning
Grids, Groups, Graphs,
Geodesics, and Gauges

Michael M. Bronstein1, Joan Bruna2, Taco Cohen3, Petar Veličković4

May 4, 2021

1Imperial College London / USI IDSIA / Twitter
2New York University
3Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm

Technologies, Inc.
4DeepMind

 
 
 
 
 
 
Contents

Preface .

.

Notation .

.

.

.

.

.

.

.

.

.

.

Introduction .

1

2

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . .

Learning in High Dimensions . . . . . . . . . . . . . . . . . .

2.1

2.2

Inductive Bias via Function Regularity . . . . . . . . .

The Curse of Dimensionality . . . . . . . . . . . . . .

3

Geometric Priors

.

.

. . . . . . . . . . . . . . . . . . . . . . .

3.1

3.2

3.3

3.4

3.5

Symmetries, Representations, and Invariance . . . . .

Isomorphisms and Automorphisms . . . . . . . . . .

Deformation Stability . . . . . . . . . . . . . . . . . .

Scale Separation . .

. . . . . . . . . . . . . . . . . . . .

The Blueprint of Geometric Deep Learning . . . . . .

4

Geometric Domains: the 5 Gs . . . . . . . . . . . . . . . . . .

4.1

4.2

4.3

Graphs and Sets . .

. . . . . . . . . . . . . . . . . . . .

Grids and Euclidean spaces . . . . . . . . . . . . . . .

Groups and Homogeneous spaces . . . . . . . . . . .

1

3

4

5

6

8

9

12

17

19

22

27

30

31

35

40

iv

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

4.4

4.5

4.6

Geodesics and Manifolds . . . . . . . . . . . . . . . .

Gauges and Bundles . . . . . . . . . . . . . . . . . . .

Geometric graphs and Meshes . . . . . . . . . . . . .

5

Geometric Deep Learning Models

. . . . . . . . . . . . . . .

5.1

5.2

5.3

5.4

5.5

5.6

5.7

5.8

Convolutional Neural Networks . . . . . . . . . . . .

Group-equivariant CNNs . . . . . . . . . . . . . . . .

Graph Neural Networks . . . . . . . . . . . . . . . . .

Deep Sets, Transformers, and Latent Graph Inference

Equivariant Message Passing Networks . . . . . . . .

Intrinsic Mesh CNNs . . . . . . . . . . . . . . . . . . .

Recurrent Neural Networks . . . . . . . . . . . . . . .

Long Short-Term Memory networks . . . . . . . . . .

44

56

61

68

69

74

77

80

83

86

89

95

6

7

Problems and Applications

. . . . . . . . . . . . . . . . . . . 102

Historic Perspective . . . . . . . . . . . . . . . . . . . . . . . . 114

0. PREFACE

1

Preface

‘geometry’ has been
For nearly two millenia since Euclid’s Elements, the word
synonymous with Euclidean geometry, as no other types of geometry existed.
Euclid’s monopoly came to an end in the nineteenth century, with examples
of non-Euclidean geometries constructed by Lobachevesky, Bolyai, Gauss,
and Riemann. Towards the end of that century, these studies had diverged
into disparate ﬁelds, with mathematicians and philosophers debating the
validity of and relations between these geometries as well as the nature of
the “one true geometry”.

A way out of this pickle was shown by a young mathematician Felix Klein,
appointed in 1872 as professor in the small Bavarian University of Erlangen.
In a research prospectus, which entered the annals of mathematics as the
Erlangen Programme, Klein proposed approaching geometry as the study of
invariants, i.e. properties unchanged under some class of transformations,
called the symmetries of the geometry. This approach created clarity by
showing that various geometries known at the time could be deﬁned by
an appropriate choice of symmetry transformations, formalized using the
language of group theory. For instance, Euclidean geometry is concerned
with lengths and angles, because these properties are preserved by the
group of Euclidean transformations (rotations and translations), while aﬃne
geometry studies parallelism, which is preserved by the group of aﬃne
transformations. The relation between these geometries is immediately
apparent when considering the respective groups, because the Euclidean
group is a subgroup of the aﬃne group, which in turn is a subgroup of the
group of projective transformations.

The impact of the Erlangen Programme on geometry was very profound.
Furthermore, it spilled to other ﬁelds, especially physics, where symmetry
principles allowed to derive conservation laws from ﬁrst principles of symme-
try (an astonishing result known as Noether’s Theorem), and even enabled
the classiﬁcation of elementary particles as irreducible representations of
the symmetry group. Category theory, now pervasive in pure mathematics,
can be “regarded as a continuation of the Klein Erlangen Programme, in the
sense that a geometrical space with its group of transformations is general-
ized to a category with its algebra of mappings”, in the words of its creators
Samuel Eilenber and Saunders Mac Lane.

At the time of writing, the state of the ﬁeld of deep learning is somewhat

According to a popular belief,
the Erlangen Programme was
delivered in Klein’s inaugural
address in October 1872.
Klein indeed gave such a talk
(though on December 7 of
the same year), but it was for
a non-mathematical audience
and concerned primarily his
ideas of mathematical
education. What is now
called the ‘Erlangen
Programme’ was actually a
research prospectus brochure

Vergleichende Betrachtungen
über neuere geometrische
Forschungen (“A comparative
review of recent researches in
geometry”) he prepared as
part of his professor
appointment. See Tobies
(2019).

See Marquis (2009).

2

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

reminiscent of the ﬁeld of geometry in the nineteenth century. There is a
veritable zoo of neural network architectures for various kinds of data, but
few unifying principles. As in times past, this makes it diﬃcult to understand
the relations between various methods, inevitably resulting in the reinvention
and re-branding of the same concepts in diﬀerent application domains. For
a novice trying to learn the ﬁeld, absorbing the sheer volume of redundant
ideas is a true nightmare.

In this text, we make a modest attempt to apply the Erlangen Programme
mindset to the domain of deep learning, with the ultimate goal of obtaining a
systematisation of this ﬁeld and ‘connecting the dots’. We call this geometri-
sation attempt ‘Geometric Deep Learning’, and true to the spirit of Felix
Klein, propose to derive diﬀerent inductive biases and network architectures
implementing them from ﬁrst principles of symmetry and invariance. In par-
ticular, we focus on a large class of neural networks designed for analysing
unstructured sets, grids, graphs, and manifolds, and show that they can be
understood in a uniﬁed manner as methods that respect the structure and
symmetries of these domains.

We believe this text would appeal to a broad audience of deep learning re-
searchers, practitioners, and enthusiasts. A novice may use it as an overview
and introduction to Geometric Deep Learning. A seasoned deep learning
expert may discover new ways of deriving familiar architectures from basic
principles and perhaps some surprising connections. Practitioners may get
new insights on how to solve problems in their respective ﬁelds.

With such a fast-paced ﬁeld as modern machine learning, the risk of writing
a text like this is that it becomes obsolete and irrelevant before it sees the light
of day. Having focused on foundations, our hope is that the key concepts
we discuss will transcend their speciﬁc realisations
— or, as Claude Adrien
Helvétius put it, “la connaissance de certains principes supplée facilement à la
connoissance de certains faits.”

“The knowledge of certain
principles easily compensates
the lack of knowledge of
certain facts.” (Helvétius,
1759)

Notation

0. NOTATION 3

Ω, u

x(u)

f (x)

G, g

(Ω,

)

∈ X

(
X

∈ F

C
(Ω))

g.u, ρ(g)

X

xu

|Ω|×s

∈ C

s

∈ C

xuj

∈ C

F(X)

τ : Ω

η : Ω

σ :

→

→

Ω

Ω(cid:48)

(cid:48)

C → C

,

,

)

,

E

E

)

F

G = (

V

= (

V

T
x (cid:63) θ

Sv

ϕi

TuΩ, T Ω

X

∈

TuΩ

gu(X, Y ) =

(cid:96)(γ), (cid:96)uv

X, Y
(cid:104)

Domain, point on domain

Signal on the domain of the form x : Ω
Functions on signals on the domain of the form
f :

→ C

(Ω)

X

→ Y

Group, element of the group

Group action, group representation

Matrix representing a signal on a discrete domain

∈

Ω

Vector representing a discrete domain signal X on
element u
Scalar representing the jth component of a dis-
crete domain signal X on element u
Function on discrete domain signals that returns
another discrete domain signal, as a matrix

Ω

∈

Automorphism of the domain

Isomorphism between two diﬀerent domains

Activation function (point-wise non-linearity)

Graph with nodes

and edges

V
, edges

V

E

E

, and faces

F

Mesh with nodes

Convolution with ﬁlter θ
Shift operator

Basis function

Tangent space at u, tangent bundle
Tangent vector

u Riemannian metric
(cid:105)

Length of a curve γ, discrete metric on edge (u, v)

4

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

1

Introduction

The last decade has witnessed an experimental revolution in data science
and machine learning, epitomised by deep learning methods. Indeed, many
high-dimensional learning tasks previously thought to be beyond reach –
such as computer vision, playing Go, or protein folding – are in fact feasi-
ble with appropriate computational scale. Remarkably, the essence of deep
learning is built from two simple algorithmic principles: ﬁrst, the notion of
representation or feature learning, whereby adapted, often hierarchical, fea-
tures capture the appropriate notion of regularity for each task, and second,
learning by local gradient-descent, typically implemented as backpropagation.

While learning generic functions in high dimensions is a cursed estimation
problem, most tasks of interest are not generic, and come with essential
pre-deﬁned regularities arising from the underlying low-dimensionality
and structure of the physical world. This text is concerned with exposing
these regularities through uniﬁed geometric principles that can be applied
throughout a wide spectrum of applications.

Exploiting the known symmetries of a large system is a powerful and classical
remedy against the curse of dimensionality, and forms the basis of most
physical theories. Deep learning systems are no exception, and since the
early days researchers have adapted neural networks to exploit the low-
dimensional geometry arising from physical measurements, e.g. grids in
images, sequences in time-series, or position and momentum in molecules,
and their associated symmetries, such as translation or rotation. Throughout
our exposition, we will describe these models, as well as many others, as
natural instances of the same underlying principle of geometric regularity.

Such a ‘geometric uniﬁcation’ endeavour in the spirit of the Erlangen Pro-
gram serves a dual purpose: on one hand, it provides a common mathemat-
ical framework to study the most successful neural network architectures,
such as CNNs, RNNs, GNNs, and Transformers. On the other, it gives a
constructive procedure to incorporate prior physical knowledge into neural
architectures and provide principled way to build future architectures yet to
be invented.

Before proceeding, it is worth noting that our work concerns representation
learning architectures and exploiting the symmetries of data therein. The
many exciting pipelines where such representations may be used (such as

2. LEARNING IN HIGH DIMENSIONS

5

self-supervised learning, generative modelling, or reinforcement learning)
are not our central focus
. Hence, we will not review in depth inﬂuential
neural pipelines such as variational autoencoders (Kingma and Welling,
2013), generative adversarial networks (Goodfellow et al., 2014), normal-
ising ﬂows (Rezende and Mohamed, 2015), deep Q-networks (Mnih et al.,
2015), proximal policy optimisation (Schulman et al., 2017), or deep mutual
information maximisation (Hjelm et al., 2019). That being said, we believe
that the principles we will focus on are of signiﬁcant importance in all of
these areas.

The same applies for
techniques used for
optimising or regularising our
architectures, such as Adam
(Kingma and Ba, 2014),
dropout (Srivastava et al.,
2014) or batch normalisation
(Ioﬀe and Szegedy, 2015).

Further, while we have attempted to cast a reasonably wide net in order to
illustrate the power of our geometric blueprint, our work does not attempt
to accurately summarise the entire existing wealth of research on Geometric
Deep Learning. Rather, we study several well-known architectures in-depth
in order to demonstrate the principles and ground them in existing research,
with the hope that we have left suﬃcient references for the reader to mean-
ingfully apply these principles to any future geometric deep architecture
they encounter or devise.

2 Learning in High Dimensions

=

Supervised machine learning, in its simplest formalisation, considers a set
drawn i.i.d. from an underlying data
of N observations
are respectively the
distribution P deﬁned over
, where
X
Y
data and the label domains. The deﬁning feature in this setup is that
is
a high-dimensional space: one typically assumes
= Rd to be a Euclidean
space of large dimension d.

(xi, yi)
}
{
X × Y

N
i=1

and

D

X

X

Let us further assume that the labels y are generated by an unknown function
f , such that yi = f (xi), and the learning problem reduces to estimating the
. Neural networks
function f using a parametrised function class
are a common realisation of such parametric function classes, in which case
Θ corresponds to the network weights. In this idealised setup, there is
θ
no noise in the labels, and modern deep learning systems typically operate
in the so-called interpolating regime, where the estimated ˜f
satisﬁes
˜f (xi) = f (xi) for all i = 1, . . . , N . The performance of a learning algorithm
on new samples drawn from
is measured in terms of the expected performance

fθ∈Θ}
{

∈ F

=

F

∈

Statistical learning theory is
concerned with more reﬁned
notions of generalisation
based on concentration
inequalities; we will review
some of these in future work.

6

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

P , using some loss L(

,
·

)

·

( ˜f ) := EP L( ˜f (x), f (x)),

R

with the squared-loss L(y, y(cid:48)) = 1
2 |
used ones.

y

y(cid:48)

2 being among the most commonly
|

−

A successful learning scheme thus needs to encode the appropriate notion
of regularity or inductive bias for f , imposed through the construction of
the function class
and the use of regularisation. We brieﬂy introduce this
concept in the following section.

F

2.1

Inductive Bias via Function Regularity

Modern machine learning operates with large, high-quality datasets, which,
together with appropriate computational resources, motivate the design of
rich function classes
with the capacity to interpolate such large data. This
F
mindset plays well with neural networks, since even the simplest choices of
architecture yields a dense class of functions.
The capacity to approximate
almost arbitrary functions is the subject of various Universal Approximation
Theorems; several such results were proved and popularised in the 1990s by
applied mathematicians and computer scientists (see e.g. Cybenko (1989);
Hornik (1991); Barron (1993); Leshno et al. (1993); Maiorov (1999); Pinkus
(1999)).

Universal Approximation, however, does not imply an absence of inductive
with universal approximation, we can
bias. Given a hypothesis space
deﬁne a complexity measure c :
R+ and redeﬁne our interpolation
problem as

F
F →

˜f

arg min
g∈F

∈

c(g)

s.t.

g(xi) = f (xi)

for i = 1, . . . , N,

making

i.e., we are looking for the most regular functions within our hypothesis
class. For standard function spaces, this complexity measure can be deﬁned
as a norm,
a Banach space and allowing to leverage a plethora
of theoretical results in functional analysis. In low dimensions, splines
are a workhorse for function approximation. They can be formulated as
above, with a norm capturing the classical notion of smoothness, such as the
squared-norm of second-derivatives

F

+∞
−∞ |

f (cid:48)(cid:48)(x)

2dx for cubic splines.
|

(cid:82)

A set A ⊂ X is said to be
dense in X if its closure

A ∪ { lim
i→∞

ai : ai ∈ A} = X .

This implies that any point in
X is arbitrarily close to a
point in A. A typical
Universal Approximation
result shows that the class of
functions represented e.g. by
a two-layer perceptron,
f (x) = c(cid:62)sign(Ax + b) is
dense in the space of
continuous functions on Rd.

Informally, a norm (cid:107)x(cid:107) can be
regarded as a “length” of
vector x. A Banach space is a
complete vector space
equipped with a norm.

2. LEARNING IN HIGH DIMENSIONS

7

Figure 1: Multilayer Perceptrons (Rosenblatt, 1958), the simplest feed-
forward neural networks, are universal approximators: with just one hidden
layer, they can represent combinations of step functions, allowing to approx-
imate any continuous function with arbitrary precision.

In the case of neural networks, the complexity measure c can be expressed
in terms of the network weights, i.e. c(fθ) = c(θ). The L2-norm of the net-
work weights, known as weight decay, or the so-called path-norm (Neyshabur
et al., 2015) are popular choices in deep learning literature. From a Bayesian
perspective, such complexity measures can also be interpreted as the neg-
ative log of the prior for the function of interest. More generally, this com-
plexity can be enforced explicitly by incorporating it into the empirical loss
(resulting in the so-called Structural Risk Minimisation), or implicitly, as
a result of a certain optimisation scheme. For example, it is well-known
that gradient-descent on an under-determined least-squares objective will
choose interpolating solutions with minimal L2 norm. The extension of such
implicit regularisation results to modern neural networks is the subject of
current studies (see e.g. Blanc et al. (2020); Shamir and Vardi (2020); Razin
and Cohen (2020); Gunasekar et al. (2017)). All in all, a natural question
arises: how to deﬁne eﬀective priors that capture the expected regularities
and complexities of real-world prediction tasks?

8

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

2.2 The Curse of Dimensionality

While interpolation in low-dimensions (with d = 1, 2 or 3) is a classic sig-
nal processing task with very precise mathematical control of estimation
errors using increasingly sophisticated regularity classes (such as spline inter-
polants, wavelets, curvelets, or ridgelets), the situation for high-dimensional
problems is entirely diﬀerent.

A function f is in the Sobolev
class Hs(Ωd) if f ∈ L2(Ωd)
and the generalised s-th
order derivative is
square-integrable:
(cid:82) |ω|2s+1| ˆf (ω)|2dω < ∞,
where ˆf is the Fourier
transform of f ; see Section 4.2.

(cid:107)

x

x

−

x(cid:48)

x(cid:48)

∈ X

| ≤ (cid:107)

X →

f (x(cid:48))

f (x)
|

R, i.e. functions satisfying

In order to convey the essence of the idea, let us consider a classical notion
of regularity that can be easily extended to high dimensions: 1-Lipschitz-
for all
functions f :
−
. This hypothesis only asks the target function to be locally smooth,
x, x(cid:48)
i.e., if we perturb the input x slightly (as measured by the norm
),
(cid:107)
the output f (x) is not allowed to change much. If our only knowledge of
the target function f is that it is 1-Lipschitz, how many observations do we
expect to require to ensure that our estimate ˜f will be close to f ? Figure
2 reveals that the general answer is necessarily exponential in the dimen-
sion d, signaling that the Lipschitz class grows ‘too quickly’ as the input
dimension increases: in many applications with even modest dimension d,
the number of samples would be bigger than the number of atoms in the
universe. The situation is not better if one replaces the Lipschitz class by a
global smoothness hypothesis, such as the Sobolev Class
. Indeed,
classic results (Tsybakov, 2008) establish a minimax rate of approximation
and learning for the Sobolev class of the order (cid:15)−d/s, showing that the extra
smoothness assumptions on f only improve the statistical picture when
s

d, an unrealistic assumption in practice.

s(Ωd)

H

−

(cid:107)

∝

Fully-connected neural networks deﬁne function spaces that enable more
ﬂexible notions of regularity, obtained by considering complexity functions
c on their weights. In particular, by choosing a sparsity-promoting regular-
isation, they have the ability to break this curse of dimensionality (Bach,
2017). However, this comes at the expense of making strong assumptions
on the nature of the target function f , such as that f depends on a collection
of low-dimensional projections of the input (see Figure 3). In most real-
world applications (such as computer vision, speech analysis, physics, or
chemistry), functions of interest tend to exhibits complex long-range corre-
lations that cannot be expressed with low-dimensional projections (Figure
3), making this hypothesis unrealistic. It is thus necessary to deﬁne an alter-
native source of regularity, by exploiting the spatial structure of the physical
domain and the geometric priors of f , as we describe in the next Section 3.

3. GEOMETRIC PRIORS

9

∈

−

±

(cid:80)

2d
j=1 zjφ(x

1, xj

xj) where
Figure 2: We consider a Lipschitz function f (x) =
Rd is placed in each quadrant, and φ a locally supported
zj =
Lipschitz ‘bump’. Unless we observe the function in most of the 2d quad-
rants, we will incur in a constant error in predicting it. This simple geo-
metric argument can be formalised through the notion of Maximum Dis-
crepancy (von Luxburg and Bousquet, 2004), deﬁned for the Lipschitz class
as κ(d) = Ex,x(cid:48) supf ∈Lip(1)
N −1/d, which mea-
(cid:39)
sures the largest expected discrepancy between two independent N -sample
(cid:80)
(cid:80)
expectations. Ensuring that κ(d)
(cid:15) requires N = Θ((cid:15)−d); the correspond-
(cid:39)
ing sample
l deﬁnes an (cid:15)-net of the domain. For a d-dimensional Eu-
}
clidean domain of diameter 1, its size grows exponentially as (cid:15)−d.

l f (x(cid:48)
l)

l f (xl)

xl
{

1
N

1
N

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

3 Geometric Priors

Modern data analysis is synonymous with high-dimensional learning. While
the simple arguments of Section 2.1 reveal the impossibility of learning from
generic high-dimensional data as a result of the curse of dimensionality,
there is hope for physically-structured data, where we can employ two fun-
damental principles: symmetry and scale separation. In the settings considered
in this text, this additional structure will usually come from the structure of
the domain underlying the input signals: we will assume that our machine
learning system operates on signals (functions) on some domain Ω. While
in many cases linear combinations of points on Ω is not well-deﬁned
, we can
linearly combine signals on it, i.e., the space of signals forms a vector space.
Moreover, since we can deﬁne an inner product between signals, this space
is a Hilbert space.

Ω must be a vector space in
order for an expression
αu + βv to make sense.

10

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

≈

g(Ax) for some unknown A

Figure 3: If the unknown function f is presumed to be well approximated as
d, then shallow neural
f (x)
∈
networks can capture this inductive bias, see e.g. Bach (2017). In typical
applications, such dependency on low-dimensional projections is unrealistic,
as illustrated in this example: a low-pass ﬁlter projects the input images
to a low-dimensional subspace; while it conveys most of the semantics,
substantial information is lost.

Rk×d with k

(cid:28)

3. GEOMETRIC PRIORS

11

When Ω has some additional
structure, we may further
restrict the kinds of signals in
X (Ω, C). For example, when
Ω is a smooth manifold, we
may require the signals to be
smooth. Whenever possible,
we will omit the range C for
brevity.

When the domain Ω is
discrete, µ can be chosen as
the counting measure, in which
case the integral becomes a
sum. In the following, we
will omit the measure and
use du for brevity.

The space of
C
structure, and

-valued signals on Ω

(for Ω a set, possibly with additional
a vector space, whose dimensions are called channels)

C

x : Ω
{
is a function space that has a vector space structure. Addition and scalar
multiplication of signals is deﬁned as:

→ C}

) =

(Ω,

X

C

(1)

(αx + βy)(u) = αx(u) + βy(u)

for all u

Ω,

∈

with real scalars α, β. Given an inner product
and a measure
µ on Ω (with respect to which we can deﬁne an integral), we can deﬁne
an inner product on

C on
(cid:105)

v, w

C

(cid:104)

(Ω,

) as

X

C

x, y
(cid:104)

(cid:105)

=

x(u), y(u)
(cid:105)

(cid:90)Ω(cid:104)

C dµ(u).

(2)

As a typical illustration, take Ω = Zn
Zn to be a two-dimensional n
n grid,
×
R3), and f a function (such as a single-
x an RGB image (i.e. a signal x : Ω
layer Perceptron) operating on 3n2-dimensional inputs. As we will see in the
following with greater detail, the domain Ω is usually endowed with certain
geometric structure and symmetries. Scale separation results from our ability
to preserve important characteristics of the signal when transferring it onto
a coarser version of the domain (in our example, subsampling the image by
coarsening the underlying grid).

→

×

We will show that both principles, to which we will generically refer as geo-
metric priors, are prominent in most modern deep learning architectures. In
the case of images considered above, geometric priors are built into Convo-
lutional Neural Networks (CNNs) in the form of convolutional ﬁlters with
shared weights (exploiting translational symmetry) and pooling (exploiting
scale separation). Extending these ideas to other domains such as graphs
and manifolds and showing how geometric priors emerge from fundamental
principles is the main goal of Geometric Deep Learning and the leitmotif of
our text.

12

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

3.1 Symmetries, Representations, and Invariance

Informally, a symmetry of an object or system is a transformation that leaves
a certain property of said object or system unchanged or invariant. Such
transformations may be either smooth, continuous, or discrete. Symmetries
are ubiquitous in many machine learning tasks. For example, in computer
vision the object category is unchanged by shifts, so shifts are symmetries in
the problem of visual object classiﬁcation. In computational chemistry, the
task of predicting properties of molecules independently of their orientation
in space requires rotational invariance. Discrete symmetries emerge naturally
when describing particle systems where particles do not have canonical
ordering and thus can be arbitrarily permuted, as well as in many dynamical
systems, via the time-reversal symmetry (such as systems in detailed bal-
ance or the Newton’s second law of motion). As we will see in Section 4.1,
permutation symmetries are also central to the analysis of graph-structured
data.

Symmetry groups The set of symmetries of an object satisﬁes a number of
properties. First, symmetries may be combined to obtain new symmetries:
if g and h are two symmetries, then their compositions g
are
also symmetries. The reason is that if both transformations leave the object
invariant, then so does the composition of transformations, and hence the
composition is also a symmetry. Furthermore, symmetries are always in-
vertible, and the inverse is also a symmetry. This shows that the collection
of all symmetries form an algebraic object known as a group. Since these
objects will be a centerpiece of the mathematical model of Geometric Deep
Learning, they deserve a formal deﬁnition and detailed discussion:

h and h

g

◦

◦

We will follow the
juxtaposition notation
convention used in group
theory, g ◦ h = gh, which
should be read right-to-left:
we ﬁrst apply h and then g.
The order is important, as in
many cases symmetries are
non-commutative. Readers
familiar with Lie groups
might be disturbed by our
choice to use the Fraktur font
to denote group elements, as
it is a common notation of Lie
algebras.

G are called commutative or Abelian After the Norwegian

mathematician Niels Henrik
Abel (1802–1829).

3. GEOMETRIC PRIORS

13

A group is a set G along with a binary operation
◦
composition (for brevity, denoted by juxtaposition g
the following axioms:

: G

G

G called
→
h = gh) satisfying

×

◦

∈

G.

Associativity: (gh)k = g(hk) for all g, h, k
Identity: there exists a unique e
Inverse: For each g
gg−1 = g−1g = e.
Closure: The group is closed under composition, i.e., for every g, h
we have gh

G satisfying eg = ge = g for all g

G there is a unique inverse g−1

G.

∈

∈

∈

G.
G such that

∈

G,

∈

∈

= hg.
.

Note that commutativity is not part of this deﬁnition, i.e. we may have gh
Groups for which gh = hg for all g, h

∈
Though some groups can be very large and even inﬁnite, they often arise
from compositions of just a few elements, called group generators. Formally,
G is said to be generated by a subset S
G (called the group generator) if
G can be written as a ﬁnite composition of the elements
every element g
of S and their inverses. For instance, the symmetry group of an equilateral
triangle (dihedral group D3) is generated by a 60◦ rotation and a reﬂection
(Figure 4). The 1D translation group, which we will discuss in detail in the
following, is generated by inﬁnitesimal displacements; this is an example of
a Lie group of diﬀerentiable symmetries.

⊆

∈

Note that here we have deﬁned a group as an abstract object, without saying
what the group elements are (e.g. transformations of some domain), only
how they compose. Hence, very diﬀerent kinds of objects may have the same
symmetry group. For instance, the aforementioned group of rotational and
reﬂection symmetries of a triangle is the same as the group of permutations
of a sequence of three elements (we can permute the corners in the triangle
.
in any way using a rotation and reﬂection – see Figure 4)

Group Actions and Group Representations Rather than considering groups
as abstract entities, we are mostly interested in how groups act on data. Since
we assumed that there is some domain Ω underlying our data, we will study
how the group acts on Ω (e.g. translation of points of the plane), and from
there obtain actions of the same group on the space of signals
(Ω) (e.g.
translations of planar images and feature maps).

X

Lie groups have a
diﬀerentiable manifold
structure. One such example
that we will study in
Section 4.3 is the special
orthogonal group SO(3),
which is a 3-dimensional
manifold.

The diagram shown in Figure
4 (where each node is
associated with a group
element, and each arrow with
a generator), is known as the
Cayley diagram.

(cid:54)
14

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 4: Left: an equilateral triangle with corners labelled by 1, 2, 3, and
all possible rotations and reﬂections of the triangle. The group D3 of rota-
tion/reﬂection symmetries of the triangle is generated by only two elements
(rotation by 60◦ R and reﬂection F) and is the same as the group Σ3 of per-
mutations of three elements. Right: the multiplication table of the group D3.
The element in the row g and column h corresponds to the element gh.

Technically, what we deﬁne
here is a left group action.

Distance-preserving
transformations are called
isometries. According to
Klein’s Erlangen Programme,
the classical Euclidean
geometry arises from this
group.

∈

∈

G and u

G and a point u

of G on a set Ω is deﬁned as a mapping (g, u)

g.u associating
A group action
(cid:55)→
Ω with some other point on Ω in a
a group element g
∈
∈
way that is compatible with the group operations, i.e., g.(h.u) = (gh).u for
all g, h
Ω. We shall see numerous instances of group actions in
the following sections. For example, in the plane the Euclidean group E(2) is
, and
the group of transformations of R2 that preserves Euclidean distances
consists of translations, rotations, and reﬂections. The same group, however,
can also act on the space of images on the plane (by translating, rotating and
ﬂipping the grid of pixels), as well as on the representation spaces learned
by a neural network. More precisely, if we have a group G acting on Ω, we
automatically obtain an action of G on the space

(Ω):

X

(g.x)(u) = x(g−1u).

(3)

Due to the inverse on g, this is indeed a valid group action, in that we have
(g.(h.x))(u) = ((gh).x)(u).

The most important kind of group actions, which we will encounter repeat-
edly throughout this text, are linear group actions, also known as group
representations. The action on signals in equation (3) is indeed linear, in the

3. GEOMETRIC PRIORS

15

sense that

g.(αx + βx(cid:48)) = α(g.x) + β(g.x(cid:48))

∈ X

(cid:55)→
Rn×n

(Ω). We can describe linear actions
for any scalars α, β and signals x, x(cid:48)
g.x that are linear in x, or equivalently, by currying,
either as maps (g, x)
as a map ρ : G
that assigns to each group element g an (invertible)
matrix ρ(g). The dimension n of the matrix is in general arbitrary and not
necessarily related to the dimensionality of the group or the dimensionality
of Ω, but in applications to deep learning n will usually be the dimensionality
of the feature space on which the group acts. For instance, we may have the
group of 2D translations acting on a space of images with n pixels.

→

As with a general group action, the assignment of matrices to group elements
should be compatible with the group action. More speciﬁcally, the matrix
representing a composite group element gh should equal the matrix product
of the representation of g and h:

A n-dimensional real representation of a group G is a map ρ : G
Rn×n,
assigning to each g
G an invertible matrix ρ(g), and satisfying the
condition ρ(gh) = ρ(g)ρ(h) for all g, h
A representation is called
unitary or orthogonal if the matrix ρ(g) is unitary or orthogonal for all
g

G.

G.

→

∈

∈

∈

Written in the language of group representations, the action of G on signals
x

(Ω) is deﬁned as ρ(g)x(u) = x(g−1u). We again verify that

∈ X

(ρ(g)(ρ(h)x))(u) = (ρ(gh)x)(u).

Invariant and Equivariant functions The symmetry of the domain Ω un-
derlying the signals
(Ω) imposes structure on the function f deﬁned on
such signals. It turns out to be a powerful inductive bias, improving learning
eﬃciency by reducing the space of possible interpolants,
(Ω)), to those
which satisfy the symmetry priors. Two important cases we will be exploring
in this text are invariant and equivariant functions.

F

X

X

(

When Ω is inﬁnte, the space
of signals X (Ω) is inﬁnite
dimensional, in which case
ρ(g) is a linear operator on
this space, rather than a ﬁnite
dimensional matrix. In
practice, one must always
discretise to a ﬁnite grid,
though.

Similarly, a complex
representation is a map
ρ : G → Cn×n satisfying the
same equation.

In general, f depends both
on the signal an the domain,
i.e., F(X (Ω), Ω). We will
often omit the latter
dependency for brevity.

A function f :
and x
input.

∈ X

(Ω)

G
(Ω), i.e., its output is unaﬀected by the group action on the

is G-invariant if f (ρ(g)x) = f (x) for all g

→ Y

X

∈

16

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 5: Three spaces of interest in Geometric Deep Learning: the (physi-
cal) domain Ω, the space of signals
(Ω)).
Symmetries of the domain Ω (captured by the group G) act on signals
(Ω) through group representations ρ(g), imposing structure on the
x
functions f

(Ω), and the hypothesis class

(Ω)) acting on such signals.

∈ X

F

X

X

(

(
X

∈ F

Note that signal processing
books routinely use the term
‘shift-invariance’ referring to
shift-equivariance, e.g. Linear
Shift-invariant Systems.

A classical example of invariance is shift-invariance,
arising in computer
vision and pattern recognition applications such as image classiﬁcation. The
function f in this case (typically implemented as a Convolutional Neural
Network) inputs an image and outputs the probability of the image to contain
an object from a certain class (e.g. cat or dog). It is often reasonably assumed
that the classiﬁcation result should not be aﬀected by the position of the
object in the image, i.e., the function f must be shift-invariant. Multi-layer
Perceptrons, which can approximate any smooth function, do not have this
property – one of the reasons why early attempts to apply these architectures
to problems of pattern recognition in the 1970s failed. The development of
neural network architectures with local weight sharing, as epitomised by
Convolutional Neural Networks, was, among other reasons, motivated by
the need for shift-invariant object classiﬁcation.

If we however take a closer look at the convolutional layers of CNNs, we
will ﬁnd that they are not shift-invariant but shift-equivariant: in other words,
a shift of the input to a convolutional layer produces a shift in the output
feature maps by the same amount.

More generally, we might
have f : X (Ω) → X (Ω(cid:48)) with
input and output spaces
having diﬀerent domains
Ω, Ω(cid:48) and representations ρ,
ρ(cid:48) of the same group G. In
this case, equivariance is
deﬁned as
f (ρ(g)x) = ρ(cid:48)(g)f (x).

A function f :
all g
way.

∈

f (ρ(g)x) = ρ(g)f (x) for
G, i.e., group action on the input aﬀects the output in the same

(Ω) is G-equivariant if

→ X

(Ω)

X

Resorting again to computer vision, a prototypical application requiring

3. GEOMETRIC PRIORS

17

shift-equivariance is image segmentation, where the output of f is a pixel-
wise image mask. Obviously, the segmentation mask must follow shifts in
the input image. In this example, the domains of the input and output are
the same, but since the input has three color channels while the output has
one channel per class, the representations (ρ,
(cid:48))) are
somewhat diﬀerent.

)) and (ρ(cid:48),

(Ω,

(Ω,

X

X

C

C

However, even the previous use case of image classiﬁcation is usually imple-
mented as a sequence of convolutional (shift-equivariant) layers, followed by
global pooling (which is shift-invariant). As we will see in Section 3.5, this
is a general blueprint of a majority of deep learning architectures, including
CNNs and Graph Neural Networks (GNNs).

3.2

Isomorphisms and Automorphisms

Subgroups and Levels of structure As mentioned before, a symmetry
is
a transformation that preserves some property or structure, and the set of
all such transformations for a given structure forms a symmetry group. It
happens often that there is not one but multiple structures of interest, and
so we can consider several levels of structure on our domain Ω. Hence, what
counts as a symmetry depends on the structure under consideration, but in
all cases a symmetry is an invertible map that respects this structure.

On the most basic level, the domain Ω is a set, which has a minimal amount
of structure: all we can say is that the set has some cardinality
. Self-maps
that preserve this structure are bijections (invertible maps), which we may
consider as set-level symmetries. One can easily verify that this is a group
by checking the axioms: a compositions of two bijections is also a bijection
(closure), the associativity stems from the associativity of the function com-
position, the map τ (u) = u is the identity element, and for every τ the inverse
exists by deﬁnition, satisfying (τ

τ −1)(u) = (τ −1

τ )(u) = u.

◦

◦

Depending on the application, there may be further levels of structure. For
instance, if Ω is a topological space, we can consider maps that preserve
continuity: such maps are called homeomorphisms and in addition to simple
bijections between sets, are also continuous and have continuous inverse.
Intuitively, continuous functions are well-behaved and map points in a neigh-
bourhood (open set) around a point u to a neighbourhood around τ (u).

Invertible and
structure-preserving maps
between diﬀerent objects
often go under the generic
name of isomorphisms (Greek
for ‘equal shape’). An
isomorphism from an object
to itself is called an
automorphism, or symmetry.

For a ﬁnite set, the cardinality
is the number of elements
(‘size’) of the set, and for
inﬁnite sets the cardinality
indicates diﬀerent kinds of
inﬁnities, such as the
countable inﬁnity of the
natural numbers, or the
uncountable inﬁnity of the
continuum R.

18

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Every diﬀerentiable function
is continuous. If the map is
continuously diﬀerentiable
‘suﬃciently many times’, it is
said to be smooth.

One can further demand that the map and its inverse are (continuously)
diﬀerentiable,
i.e., the map and its inverse have a derivative at every point
(and the derivative is also continuous). This requires further diﬀerentiable
structure that comes with diﬀerentiable manifolds, where such maps are
called diﬀeomorphisms and denoted by Diﬀ(Ω). Additional examples of struc-
tures we will encounter include distances or metrics (maps preserving them
are called isometries) or orientation (to the best of our knowledge, orientation-
preserving maps do not have a common Greek name).

A metric or distance is a function d : Ω
u, v, w

Ω:

∈

Ω

×

→

[0,

) satisfying for all

∞

Identity of indiscernibles: d(u, v) = 0 iﬀ u = v.
Symmetry: d(u, v) = d(v, u).
Triangle inequality: d(u, v)

d(u, w) + d(w, v).

≤

A space equipped with a metric (Ω, d) is called a metric space.

The right level of structure to consider depends on the problem. For example,
when segmenting histopathology slide images, we may wish to consider
ﬂipped versions of an image as equivalent (as the sample can be ﬂipped
when put under the microscope), but if we are trying to classify road signs,
we would only want to consider orientation-preserving transformations as
symmetries (since reﬂections could change the meaning of the sign).

As we add levels of structure to be preserved, the symmetry group will get
smaller. Indeed, adding structure is equivalent to selecting a subgroup, which
is a subset of the larger group that satisﬁes the axioms of a group by itself:

Let (G,
if (H,

) be a group and H
◦
⊆
) constitutes a group with the same operation.
◦

G a subset. H is said to be a subgroup of G

For instance, the group of Euclidean isometries E(2) is a subgroup of the
group of planar diﬀeomorphisms Diﬀ(2), and in turn the group of orientation-
preserving isometries SE(2) is a subgroup of E(2). This hierarchy of struc-
ture follows the Erlangen Programme philosophy outlined in the Preface:
in Klein’s construction, the Projective, Aﬃne, and Euclidean geometries

3. GEOMETRIC PRIORS

19

have increasingly more invariants and correspond to progressively smaller
groups.

Isomorphisms and Automorphisms We have described symmetries as
structure preserving and invertible maps from an object to itself. Such maps
are also known as automorphisms, and describe a way in which an object
is equivalent it itself. However, an equally important class of maps are
the so-called isomorphisms, which exhibit an equivalence between two non-
identical objects. These concepts are often conﬂated, but distinguishing them
is necessary to create clarity for our following discussion.

. An automorphism
To understand the diﬀerence, consider a set Ω =
}
of the set Ω is a bijection τ : Ω
Ω such as a cyclic shift τ (u) = u + 1 mod 3.
Such a map preserves the cardinality property, and maps Ω onto itself. If
we have another set Ω(cid:48) =
with the same number of elements, then a
Ω(cid:48) such as η(0) = a, η(1) = b, η(2) = c is a set isomorphism.
bijection η : Ω

a, b, c
{

0, 1, 2

→

}

{

→

= (

V → V

(cid:48) between two graphs

As we will see in Section 4.1 for graphs, the notion of structure includes
not just the number of nodes, but also the connectivity. An isomorphism
(cid:48)) is thus a
η :
bijection between the nodes that maps pairs of connected nodes to pairs
Two
of connected nodes, and likewise for pairs of non-connected nodes.
isomorphic graphs are thus structurally identical, and diﬀer only in the
On the other hand, a graph automorphism or
way their nodes are ordered.
symmetry is a map τ :
maps the nodes of the graph back to itself,
while preserving the connectivity. A graph with a non-trivial automorphism
(i.e., τ

= id) presents symmetries.

) and

V → V

(cid:48) = (

(cid:48),

V

V

G

G

E

E

,

I.e., (η(u), η(v)) ∈ V (cid:48) iﬀ
(u, v) ∈ V.

3.3 Deformation Stability

The symmetry formalism introduced in Sections 3.1–3.2 captures an idealised
world where we know exactly which transformations are to be considered as
symmetries, and we want to respect these symmetries exactly. For instance
in computer vision, we might assume that planar translations are exact
symmetries. However, the real world is noisy and this model falls short in
two ways.

The Folkman graph (Folkman,
1967) is a beautiful example
of a graph with 3840
automorphisms, exempliﬁed
by the many symmetric ways
to draw it.

Firstly, while these simple groups provide a way to understand global sym-

Two objects moving at
diﬀerent velocities in a video
deﬁne a transformation
outside the translation group.

1234567891011121314151617181920135791113151719(cid:54)
20

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

X

metries of the domain Ω (and by extension, of signals on it,
(Ω)), they do
not capture local symmetries well. For instance, consider a video scene with
several objects, each moving along its own diﬀerent direction. At subsequent
frames, the resulting scene will contain approximately the same semantic
information, yet no global translation explains the transformation from one
frame to another. In other cases, such as a deformable 3D object viewed by
a camera, it is simply very hard to describe the group of transformations
that preserve the object identity. These examples illustrate that in reality
we are more interested in a far larger set of transformations where global,
exact invariance is replaced by a local, inexact one. In our discussion, we
will distinguish between two scenarios: the setting where the domain Ω is
ﬁxed, and signals x
(Ω) are undergoing deformations, and the setting
∈ X
where the domain Ω itself may be deformed.

In many applications, we know a priori
Stability to signal deformations
that a small deformation of the signal x should not change the output of
f (x), so it is tempting to consider such deformations as symmetries. For
Diﬀ(Ω), or even small
instance, we could view small diﬀeomorphisms τ
bijections, as symmetries. However, small deformations can be composed
to form large deformations, so “small deformations” do not form a group,
and we cannot ask for invariance or equivariance to small deformations only.
Since large deformations can can actually materially change the semantic
content of the input, it is not a good idea to use the full group Diﬀ(Ω) as
symmetry group either.

∈

E.g., the composition of two
(cid:15)-isometries is a 2(cid:15)-isometry,
violating the closure property.

Diﬀ(Ω) is from a
A better approach is to quantify how “far” a given τ
Diﬀ(Ω) (e.g. translations) with a complexity
given symmetry subgroup G
measure c(τ ), so that c(τ ) = 0 whenever τ
G. We can now replace our
previous deﬁnition of exact invariance and equivarance under group actions
with a ‘softer’ notion of deformation stability (or approximate invariance):

⊂

∈

∈

f (ρ(τ )x)
(cid:107)

−

f (x)

(cid:107) ≤

Cc(τ )

, ,

x
(cid:107)

(cid:107)

x

∀

∈ X

(Ω)

(4)

where ρ(τ )x(u) = x(τ −1u) as before, and where C is some constant indepen-
dent of the signal x. A function f
(Ω)) satisfying the above equation
(
X
is said to be geometrically stable. We will see examples of such functions in
the next Section 3.4.

∈ F

Since c(τ ) = 0 for τ
G, this deﬁnition generalises the G-invariance prop-
erty deﬁned above. Its utility in applications depends on introducing an

∈

3. GEOMETRIC PRIORS

21

appropriate deformation cost. In the case of images deﬁned over a contin-
uous Euclidean plane, a popular choice is c2(τ ) :=
2du, which
(cid:107)
measures the ‘elasticity’ of τ , i.e., how diﬀerent it is from the displacement
by a constant vector ﬁeld. This deformation cost is in fact a norm often
called the Dirichlet energy, and can be used to quantify how far τ is from the
translation group.

Ω (cid:107)∇
(cid:82)

τ (u)

Figure 6: The set of all bijective mappings from Ω into itself forms the set
automorphism group Aut(Ω), of which a symmetry group G (shown as a circle)
is a subgroup. Geometric Stability extends the notion of G-invariance and
equivariance to ‘transformations around G’ (shown as gray ring), quantiﬁed
in the sense of some metric between transformations. In this example, a
smooth distortion of the image is close to a shift.

In many applications, the object being
Stability to domain deformations
deformed is not the signal, but the geometric domain Ω itself. Canonical
instances of this are applications dealing with graphs and manifolds: a graph
can model a social network at diﬀerent instance of time containing slightly
diﬀerent social relations (follow graph), or a manifold can model a 3D object
undergoing non-rigid deformations. This deformation can be quantiﬁed

22

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

The graph edit distance
measures the minimal cost of
making two graphs
isomorphic by a sequences of
graph edit operations. The
Gromov-Hausdorﬀ distance
measures the smallest
possible metric distortion of a
correspondence between two
metric spaces, see Gromov
(1981).

minP∈Σn trace(AP ˜AP

Two graphs can be aligned by
the Quadratic Assignment
Problem (QAP), which
considers in its simplest form
two graphs G, ˜G of the same
size n, and solves
(cid:62)
),
where A, ˜A are the
respective adjacency matrices
and Σn is the group of n × n
permutation matrices. The
graph edit distance can be
associated with such QAP
(Bougleux et al., 2015).

denotes the space of all possible variable domains (such
as follows. If
D
as the space of all graphs, or the space of Riemannian manifolds), one can
deﬁne for Ω, ˜Ω
an appropriate metric (‘distance’) d(Ω, ˜Ω) satisfying
d(Ω, ˜Ω) = 0 if Ω and ˜Ω are equivalent in some sense: for example, the graph
edit distance vanishes when the graphs are isomorphic, and the Gromov-
Hausdorﬀ distance between Riemannian manifolds equipped with geodesic
distances vanishes when two manifolds are isometric.

∈ D

A common construction of such distances between domains relies on some
˜Ω that try to ‘align’ the domains in a
family of invertible mapping η : Ω
→
way that the corresponding structures are best preserved. For example, in
the case of graphs or Riemannian manifolds (regarded as metric spaces with
the geodesic distance), this alignment can compare pair-wise adjacency or
distance structures (d and ˜d, respectively),
˜d

dD(Ω, ˜Ω) = inf

(η

d

η∈G (cid:107)

−

◦

η)
(cid:107)

×

where G is the group of isomorphisms such as bijections or isometries,
and the norm is deﬁned over the product space Ω
Ω. In other words,
a distance between elements of Ω, ˜Ω is ‘lifted’ to a distance between the
domains themselves, by accounting for all the possible alignments that
(Ω) and a deformed
preserve the internal structure.
( ˜Ω).
domain ˜Ω, one can then consider the deformed signal ˜x = x
η−1

Given a signal x

∈ X

×

◦
(Ω), Ω) : Ω

∈ X

By slightly abusing the notation, we deﬁne
as
the ensemble of possible input signals deﬁned over a varying domain. A
function f :

is stable to domain deformations if

∈ D}

(
D

(
X

) =

X

{

)

(

X

D

→ Y

C

(cid:107) ≤

∈ D

x
(cid:107)

f (˜x, ˜Ω)

f (x, Ω)
(cid:107)
, and x

dD(Ω, ˜Ω)
−
for all Ω, ˜Ω
(Ω). We will discuss this notion of stability in the
context of manifolds in Sections 4.4–4.6, where isometric deformations play
a crucial role. Furthermore, it can be shown that the stability to domain de-
formations is a natural generalisation of the stability to signal deformations,
by viewing the latter in terms of deformations of the volume form Gama
et al. (2019).

∈ X

(5)

(cid:107)

3.4 Scale Separation

While deformation stability substantially strengthens the global symmetry
priors, it is not suﬃcient in itself to overcome the curse of dimensionality, in

3. GEOMETRIC PRIORS

23

the sense that, informally speaking, there are still “too many" functions that
respect (4) as the size of the domain grows. A key insight to overcome this
curse is to exploit the multiscale structure of physical tasks. Before describ-
ing multiscale representations, we need to introduce the main elements of
Fourier transforms, which rely on frequency rather than scale.

the most famous sig-
Fourier Transform and Global invariants Arguably
nal decomposition is the Fourier transform, the cornerstone of harmonic
analysis. The classical one-dimensional Fourier transform

ˆx(ξ) =

+∞

x(u)e−iξudu

−∞
(cid:90)
expresses the function x(u)
L2(Ω) on the domain Ω = R as a linear
combination of orthogonal oscillating basis functions ϕξ(u) = eiξu, indexed by
their rate of oscillation (or frequency) ξ. Such an organisation into frequencies
reveals important information about the signal, e.g. its smoothness and
localisation. The Fourier basis itself has a deep geometric foundation and
can be interpreted as the natural vibrations of the domain, related to its
geometric structure (see e.g. Berger (2012)).

∈

The Fourier transform
dual formulation of convolution,

plays a crucial role in signal processing as it oﬀers a

(x (cid:63) θ)(u) =

x(v)θ(u

+∞

v)dv

−

−∞
a standard model of linear signal ﬁltering (here and in the following, x
denotes the signal and θ the ﬁlter). As we will show in the following, the
convolution operator is diagonalised in the Fourier basis, making it possible
to express convolution as the product of the respective Fourier transforms,

(cid:90)

a fact known in signal processing as the Convolution Theorem.

(cid:92)
(x (cid:63) θ)(ξ) = ˆx(ξ)

ˆθ(ξ),

·

As it turns out, many fundamental diﬀerential operators such as the Lapla-
cian are described as convolutions on Euclidean domains. Since such diﬀer-
ential operators can be deﬁned intrinsically over very general geometries,
this provides a formal procedure to extend Fourier transforms beyond Eu-
clidean domains, including graphs, groups and manifolds. We will discuss
this in detail in Section 4.4.

Fourier basis functions have
global support. As a result,
local signals produce energy
across all frequencies.

In the following, we will use
convolution and

(cross-)correlation
(cid:90) +∞

(x (cid:63) θ)(u) =

x(v)θ(u+v)dv

−∞

interchangeably, as it is
common in machine learning:
the diﬀerence between the
two is whether the ﬁlter is
reﬂected, and since the ﬁlter
is typically learnable, the
distinction is purely
notational.

24

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

An essential aspect of Fourier transforms is that they reveal global properties
of the signal and the domain, such as smoothness or conductance. Such
global behavior is convenient in presence of global symmetries of the domain
such as translation, but not to study more general diﬀeomorphisms. This
requires a representation that trades oﬀ spatial and frequential localisation,
as we see next.

See Mallat (1999) for a
comperehensive introduction.

Contrary to Fourier, wavelet
atoms are localised and
multi-scale, allowing to
capture ﬁne details of the
signal with atoms having
small spatial support and
coarse details with atoms
having large spatial support.
The term atom here is
synonymous with ‘basis
element’ in Fourier analysis,
with the caveat that wavelets
are redundant
(over-complete).

Multiscale representations The notion of local invariance can be articu-
lated by switching from a Fourier frequency-based representation to a scale-
based representation, the cornerstone of multi-scale decomposition methods
such as wavelets.
The essential insight of multi-scale methods is to decom-
pose functions deﬁned over the domain Ω into elementary functions that are
localised both in space and frequency.
In the case of wavelets, this is achieved
by correlating a translated and dilated ﬁlter (mother wavelet) ψ, producing a
combined spatio-frequency representation called a continuous wavelet trans-
form

(Wψx)(u, ξ) = ξ−1/2

+∞

v

ψ

u

x(v)dv.

−
ξ

(cid:90)

(cid:18)

(cid:19)

−∞
The translated and dilated ﬁlters are called wavelet atoms; their spatial po-
sition and dilation correspond to the coordinates u and ξ of the wavelet
transform. These coordinates are usually sampled dyadically (ξ = 2−j and
u = 2−jk), with j referred to as scale. Multi-scale signal representations
bring important beneﬁts in terms of capturing regularity properties beyond
global smoothness, such as piece-wise smoothness, which made them a
popular tool in signal and image processing and numerical analysis in the
90s.

Deformation stability of Multiscale representations: The beneﬁt of mul-
tiscale localised wavelet decompositions over Fourier decompositions is
revealed when considering the eﬀect of small deformations ‘nearby’ the
underlying symmetry group. Let us illustrate this important concept in the
Euclidean domain and the translation group. Since the Fourier representa-
tion diagonalises the shift operator (which can be thought of as convolution,
as we will see in more detail in Section 4.2), it is an eﬃcient representation for
translation transformations. However, Fourier decompositions are unstable
under high-frequency deformations. In contrast, wavelet decompositions
oﬀer a stable representation in such cases.

3. GEOMETRIC PRIORS

25

Aut(Ω) and its associated linear representation
Indeed, let us consider τ
∈
ρ(τ ). When τ (u) = u
v is a shift, as we will verify in Section 4.2, the
operator ρ(τ ) = Sv is a shift operator that commutes with convolution. Since
convolution operators are diagonalised by the Fourier transform, the action
of shift in the frequency domain amounts to shifting the complex phase of
the Fourier transform,

−

Svx)(ξ) = e−iξv ˆx(ξ).

(

Thus, the Fourier modulus f (x) =
removing the complex phase is a simple
(cid:100)
shift-invariant function, f (Svx) = f (x). However, if we have only approxi-
mate translation, τ (u) = u
(cid:15), the
−
situation is entirely diﬀerent: it is possible to show that

∞ = supu∈Ω (cid:107)∇
(cid:107)

|
˜τ (u) with

τ
(cid:107)∇

˜τ (u)

(cid:107) ≤

ˆx
|

f (x)

(cid:107)

f (ρ(τ )x)
x
(cid:107)

−
(cid:107)

(cid:107)

=

(1)

O

irrespective of how small (cid:15) is (i.e., how close is τ to being a shift). Conse-
quently, such Fourier representation is unstable under deformations, however
small. This unstability is manifested in general domains and non-rigid trans-
formations; we will see another instance of this unstability in the analysis
of 3d shapes using the natural extension of Fourier transforms described in
Section 4.4.

Wavelets oﬀer a remedy to this problem that also reveals the power of multi-
scale representations. In the above example, we can show (Mallat, 2012) that
the wavelet decomposition Wψx is approximately equivariant to deformations,

This notation implies that
ρ(τ ) acts on the spatial
coordinate of (Wψx)(u, ξ).

ρ(τ )(Wψx)

(cid:107)

Wψ(ρ(τ )x)
(cid:107)

−
x
(cid:107)
(cid:107)

((cid:15)).

=

O

In other words, decomposing the signal information into scales using lo-
calised ﬁlters rather than frequencies turns a global unstable representation
into a family of locally stable features. Importantly, such measurements at
diﬀerent scales are not yet invariant, and need to be progressively processed
towards the low frequencies, hinting at the deep compositional nature of
modern neural networks, and captured in our Blueprint for Geometric Deep
Learning, presented next.

Scale Separation Prior: We can build from this insight by considering
a multiscale coarsening of the data domain Ω into a hierarchy Ω1, . . . , ΩJ .
As it turns out, such coarsening can be deﬁned on very general domains,

26

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 7: Illustration of Scale Separation for image classiﬁcation tasks. The
classiﬁer f (cid:48) deﬁned on signals on the coarse grid
f (cid:48)

(Ω(cid:48)) should satisfy f

P , where P :

(Ω(cid:48)).

(Ω)

≈

X

◦

X

→ X

∈

j(Ωj,

j) :=

including grids, graphs, and manifolds. Informally, a coarsening assimilates
Ω together, and thus only requires an appropriate
nearby points u, u(cid:48)
notion of metric in the domain. If
denotes
xj : Ωj
X
{
signals deﬁned over the coarsened domain Ωj, we informally say that a
is locally stable at scale j if it admits a factorisation
function f :
X
of the form f
j(Ωj) is a non-linear coarse
≈
graining and fj :
. In other words, while the target function f
might depend on complex long-range interactions between features over
the whole domain, in locally-stable functions it is possible to separate the
interactions across scales, by ﬁrst focusing on localised interactions that are
then propagated towards the coarse scales.

Pj, where Pj :

→ Y
◦
j(Ωj)

(Ω)
fj

j
→ C

→ X

→ Y

(Ω)

X

X

C

}

Fast Multipole Method
(FMM) is a numerical
technique originally
developed to speed up the
calculation of long-ranged
forces in n-body problems.
FMM groups sources that lie
close together and treats
them as a single source.

Such principles
are of fundamental importance in many areas of physics and
mathematics, as manifested for instance in statistical physics in the so-called
renormalisation group, or leveraged in important numerical algorithms such
as the Fast Multipole Method. In machine learning, multiscale represen-
tations and local invariance are the fundamental mathematical principles
underpinning the eﬃciency of Convolutional Neural Networks and Graph
Neural Networks and are typically implemented in the form of local pooling.
In future work, we will further develop tools from computational harmonic
analysis that unify these principles across our geometric domains and will
shed light onto the statistical learning beneﬁts of scale separation.

3.5 The Blueprint of Geometric Deep Learning

3. GEOMETRIC PRIORS

27

The geometric principles of Symmetry, Geometric Stability, and Scale Sepa-
ration discussed in Sections 3.1–3.4 can be combined to provide a universal
blueprint for learning stable representations of high-dimensional data. These
representations will be produced by functions f operating on signals
)
(Ω,
deﬁned on the domain Ω, which is endowed with a symmetry group G.

X

C

The geometric priors we have described so far do not prescribe a speciﬁc
architecture for building such representation, but rather a series of necessary
conditions. However, they hint at an axiomatic construction that provably
satisﬁes these geometric priors, while ensuring a highly expressive represen-
tation that can approximate any target function satisfying such priors.

A simple initial observation is that, in order to obtain a highly expressive
representation, we are required to introduce a non-linear element, since if f
is linear and G-invariant, then for all x

(Ω),

∈ X

f (x) =

1
µ(G)

(cid:90)G

f (g.x)dµ(g) = f

1
µ(G)

(cid:18)

(cid:90)G

(g.x)dµ(g)

,

(cid:19)

which indicates that F only depends on x through the G-average Ax =
G(g.x)dµ(g). In the case of images and translation, this would entail
1
µ(G)
using only the average RGB color of the input!

(cid:82)

Here, µ(g) is known as the
Haar measure of the group G,
and the integral is performed
over the entire group.

While this reasoning shows that the family of linear invariants is not a very rich
object, the family of linear equivariants provides a much more powerful tool,
since it enables the construction of rich and stable features by composition
with appropriate non-linear maps, as we will now explain. Indeed, if B :
(cid:48)) is G-equivariant satisfying B(g.x) = g.B(x) for all
(cid:48)(cid:48) is an arbitrary (non-linear) map, then
(cid:48)(cid:48))
(cid:48)(cid:48)) is the element-wise

X
x
→ C
we easily verify that the composition U := (σ
◦
is also G-equivariant, where σ :
(Ω,
→ X
instantiation of σ given as (σ(x))(u) := σ(x(u)).

(Ω,
G, and σ :

→ X
∈

)
and g

C
∈ X

→ X

B) :

(Ω,

(Ω,

(Ω,

(Ω,

(cid:48))

X

X

C

C

C

C

C

C

)

(cid:48)

This simple property allows us to deﬁne a very general family of G-invariants,
by composing U with the group averages A
(cid:48)(cid:48). A natural
question is thus whether any G-invariant function can be approximated at
arbitrary precision by such a model, for appropriate choices of B and σ. It
is not hard to adapt the standard Universal Approximation Theorems from
unstructured vector inputs to show that shallow ‘geometric’ networks are

→ C

(Ω,

U :

X

C

◦

)

28

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 8: Geometric Deep Learning blueprint, exempliﬁed on a graph. A typi-
cal Graph Neural Network architecture may contain permutation equivariant
layers (computing node-wise features), local pooling (graph coarsening),
and a permutation-invariant global pooling layer (readout layer).

Such proofs have been
demonstrated, for example,
for the Deep Sets model by
Zaheer et al. (2017).

Meaningful metrics can be
deﬁned on grids, graphs,
manifolds, and groups. A
notable exception are sets,
where there is no predeﬁned
notion of metric.

The term ‘receptive ﬁeld’
originated in the
neuroscience literature,
referring to the spatial
domain that aﬀects the
output of a given neuron.

also universal approximators, by properly generalising the group average
to a general non-linear invariant.
However, as already described in the
case of Fourier versus Wavelet invariants, there is a fundamental tension
between shallow global invariance and deformation stability. This motivates
an alternative representation, which considers instead localised equivariant
maps.
Assuming that Ω is further equipped with a distance metric d, we
call an equivariant map U localised if (U x)(u) depends only on the values
of x(v) for
u
N
is called the receptive ﬁeld.

, for some small radius r; the latter set
}

v : d(u, v)

u =

N

≤

{

r

◦

UJ−1 · · · ◦

A single layer of local equivariant map U cannot approximate functions with
long-range interactions, but a composition of several local equivariant maps
while preserving the stability
UJ
properties of local equivariants. The receptive ﬁeld is further increased
by interleaving downsampling operators that coarsen the domain (again
assuming a metric structure), completing the parallel with Multiresolution
Analysis (MRA, see e.g. Mallat (1999)).

U1 increases the receptive ﬁeld

In summary, the geometry of the input domain, with knowledge of an un-
deryling symmetry group, provides three key building blocks: (i) a local
equivariant map, (ii) a global invariant map, and (iii) a coarsening operator.

3. GEOMETRIC PRIORS

29

These building blocks provide a rich function approximation space with
prescribed invariance and stability properties by combining them together
in a scheme we refer to as the Geometric Deep Learning Blueprint (Figure 8).

Geometric Deep Learning Blueprint

Let Ω and Ω(cid:48) be domains, G a symmetry group over Ω, and write Ω(cid:48)
if Ω(cid:48) can be considered a compact version of Ω.

Ω

⊆

We deﬁne the following building blocks:

Linear G-equivariant layer B :
B(g.x) = g.B(x) for all g

G and x

∈

(Ω,

)
C
(Ω,

X
∈ X

C

(Ω(cid:48),

C

(cid:48)) satisfying

→ X
).

Nonlinearity σ :

C → C

(cid:48) applied element-wise as (σ(x))(u) = σ(x(u)).

Local pooling (coarsening) P :

(Ω,

)

C

X

→ X

(Ω(cid:48),

C

), such that Ω(cid:48)

Ω.

⊆

G-invariant layer (global pooling) A :
A(g.x) = A(x) for all g
(Ω,

G and x

∈

∈ X

C

(Ω,

)

C

X
).

→ Y

satisfying

Using these blocks allows constructing G-invariant functions f

:

(Ω,

)

C

X

→ Y

of the form

◦

σJ

BJ

f = A

PJ−1 ◦
where the blocks are selected such that the output space of each block
matches the input space of the next one. Diﬀerent blocks may exploit
diﬀerent choices of symmetry groups G.

σ1 ◦

P1 ◦

B1

. . .

◦

◦

◦

Diﬀerent settings of Geometric Deep Learning One can make an impor-
tant distinction between the setting when the domain Ω is assumed to be ﬁxed
and one is only interested in varying input signals deﬁned on that domain,
or the domain is part of the input as varies together with signals deﬁned on
it. A classical instance of the former case is encountered in computer vision
applications, where images are assumed to be deﬁned on a ﬁxed domain

30

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

(grid). Graph classiﬁcation is an example of the latter setting, where both
the structure of the graph as well as the signal deﬁned on it (e.g. node
features) are important. In the case of varying domain, geometric stability
(in the sense of insensitivity to the deformation of Ω) plays a crucial role in
Geometric Deep Learning architectures.

This blueprint has the right level of generality to be used across a wide
range of geometric domains. Diﬀerent Geometric Deep Learning methods
thus diﬀer in their choice of the domain, symmetry group, and the speciﬁc
implementation details of the aforementioned building blocks. As we will
see in the following, a large class of deep learning architectures currently in
use fall into this scheme and can thus be derived from common geometric
principles.

In the following sections (4.1–4.6) we will describe the various geometric
domains focusing on the ‘5G’, and in Sections 5.1–5.8 the speciﬁc implemen-
tations of Geometric Deep Learning on these domains.

Architecture

Domain Ω

Symmetry group G

CNN

Grid

Translation

Spherical CNN
Intrinsic / Mesh CNN Manifold

Graph

Sphere / SO(3)

Rotation SO(3)
Isometry Iso(Ω) /
Gauge symmetry SO(2)
Permutation Σn
Permutation Σn
Complete Graph Permutation Σn
1D Grid

Time warping

Set

GNN

Deep Sets

Transformer

LSTM

4 Geometric Domains: the 5 Gs

The main focus of our text will be on graphs, grids, groups, geodesics, and
gauges. In this context, by ‘groups’ we mean global symmetry transforma-
tions in homogeneous space, by ‘geodesics’ metric structures on manifolds,
and by ‘gauges’ local reference frames deﬁned on tangent bundles (and vec-

4. GEOMETRIC DOMAINS: THE 5 GS

31

Figure 9: The 5G of Geometric Deep Learning: grids, groups & homogeneous
spaces with global symmetry, graphs, geodesics & metrics on manifolds,
and gauges (frames for tangent or feature spaces).

tor bundles in general). These notions will be explained in more detail later.
In the next sections, we will discuss in detail the main elements in common
and the key distinguishing features between these structures and describe
the symmetry groups associated with them. Our exposition is not in the
order of generality – in fact, grids are particular cases of graphs – but a way
to highlight important concepts underlying our Geometric Deep Learning
blueprint.

4.1 Graphs and Sets

In multiple branches of science, from sociology to particle physics, graphs
are used as models of systems of relations and interactions. From our per-
spective, graphs give rise to a very basic type of invariance modelled by the
group of permutations. Furthermore, other objects of interest to us, such as
grids and sets, can be obtained as a particular case of graphs.

,

E

G

V

V

= (

E ⊆ V ×V

and edges

) is a collection of nodes

A graph
between pairs
of nodes. For the purpose of the following discussion, we will further assume
the nodes to be endowed with s-dimensional node features, denoted by xu for
all u
. Social networks are perhaps among the most commonly studied
examples of graphs, where nodes represent users, edges correspond to
friendship relations between them, and node features model user properties
such as age, proﬁle picture, etc. It is also often possible to endow the edges,
or entire graphs, with features;
but as this does not alter the main ﬁndings
of this section, we will defer discussing it to future work.

∈ V

Depending on the
application ﬁeld, nodes may
also be called vertices, and
edges are often referred to as
links or relations. We will use
these terms interchangeably.

Isomorphism is an
edge-preserving bijection
between two graphs. Two
isomorphic graphs shown
here are identical up to
reordering of their nodes.

32

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

V

are usually not
The key structural property of graphs is that the nodes in
assumed to be provided in any particular order, and thus any operations
performed on graphs should not depend on the ordering of nodes. The
desirable property that functions acting on graphs should satisfy is thus
permutation invariance, and it implies that for any two isomorphic graphs, the
outcomes of these functions are identical. We can see this as a particular
setting of our blueprint, where the domain Ω =
, Rd)
is that of d-dimensional node-wise signals. The symmetry we consider is
given by the permutation group G = Σn, whose elements are all the possible
orderings of the set of node indices

1, . . . , n
{
Let us ﬁrst illustrate the concept of permutation invariance on sets, a special
case of graphs without edges (i.e.,
). By stacking the node features
=
∅
d matrix X = (x1, . . . , xn)(cid:62), we do eﬀectively specify an
as rows of the n
Σn on the set of
ordering of the nodes. The action of the permutation g
nodes amounts to the reordering of the rows of X, which can be represented
as an n
where each row and column contains
n permutation matrix ρ(g) = P,
exactly one 1 and all the other entries are zeros.

and the space

(
G

×

×

X

∈

G

E

}

.

A function f operating on this set is then said to be permutation invariant if,
for any such permutation matrix P, it holds that f (PX) = f (X). One simple
such function is

There are exactly n! such
permutations, so Σn is, even
for modest n, a very large
group.

f (X) = φ

ψ (xu)

,

(6)

(cid:32)

(cid:33)

u∈V
(cid:88)
where the function ψ is independently applied to every node’s features, and
φ is applied on its sum-aggregated outputs: as sum is independent of the order
in which its inputs are provided, such a function is invariant with respect to
the permutation of the node set, and is hence guaranteed to always return
the same output, no matter how the nodes are permuted.

We use the bold notation for
our function F(X) to
emphasise it outputs
node-wise vector features
and is hence a matrix-valued
function.

Functions like the above provide a ‘global’ graph-wise output, but very often,
we will be interested in functions that act ‘locally’, in a node-wise manner.
For example, we may want to apply some function to update the features in
every node, obtaining the set of latent node features. If we stack these latent
features into a matrix H = F(X)
is no longer permutation invariant: the
order of the rows of H should be tied to the order of the rows of X, so that
we know which output node feature corresponds to which input node. We
need instead a more ﬁne-grained notion of permutation equivariance, stating
that, once we “commit” to a permutation of inputs, it consistently permutes
the resulting objects. Formally, F(X) is a permutation equivariant function

4. GEOMETRIC DOMAINS: THE 5 GS

33

if, for any permutation matrix P, it holds that F(PX) = PF(X). A shared
node-wise linear transform

FΘ(X) = XΘ

(7)

Rd×d(cid:48), is one possible construction of such a
speciﬁed by a weight matrix Θ
permutation equivariant function, producing in our example latent features
of the form hu = Θ(cid:62)xu.

∈

This construction arises naturally from our Geometric Deep Learning blueprint.
We can ﬁrst attempt to characterise linear equivariants (functions of the form
FPX = PFX), for which it is easy to verify that any such map can be writ-
ten as a linear combination of two generators, the identity F1X = X and the
average F2X = 1
u=1 xu. As will be described in Section 5.4,
the popular Deep Sets (Zaheer et al., 2017) architecture follows precisely
this blueprint.

n 11(cid:62)X = 1
n

(cid:80)

n

We can now generalise the notions of permutation invariance and equivari-
, the graph connectivity
ance from sets to graphs. In the generic setting
=
∅
deﬁned as
n adjacency matrix A,
can be represented by the n

E (cid:54)

×

auv =

1
(u, v)
∈ E
0 otherwise.

(cid:40)

(8)

Note that now the adjacency and feature matrices A and X are “synchro-
nised”, in the sense that auv speciﬁes the adjacency information between
the nodes described by the uth and vth rows of X. Therefore, applying a
permutation matrix P to the node features X automatically implies applying
it to A’s rows and columns, PAP(cid:62).
We say that (a graph-wise function) f
is permutation invariant if

When the graph is undirected,
i.e. (u, v) ∈ E iﬀ (v, u) ∈ E,
the adjacency matrix is
symmetric, A = A(cid:62).

PAP(cid:62) is the representation
of Σn acting on matrices.

f (PX, PAP(cid:62)) = f (X, A)

and (a node-wise function) F is permutation equivariant if

F(PX, PAP(cid:62)) = PF(X, A)

for any permutation matrix P.

(9)

(10)

As a way to emphasise the
fact that our functions
operating over graphs now
need to take into account the
adjacency information, we
use the notation f (X, A).

As ob-
Here again, we can ﬁrst characterise linear equivariant functions.
served by Maron et al. (2018), any linear F satisfying equation (10) can be
expressed as a linear combination of ﬁfteen linear generators; remarkably,

This corresponds to the Bell
number B4, which counts the
number of ways to partition a
set of 4 elements, in this case
given by the 4-indices
(u, v), (u(cid:48), v(cid:48)) indexing a
linear map acting on the
adjacency matrix.

34

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Often, the node u itself is
included in its own
neighbourhood.

A multiset, denoted {{ . . . }}, is
a set where the same element
can appear more than once.
This is the case here because
the features of diﬀerent
nodes can be equal.

this family of generators is independent of n. Amongst these generators, our
blueprint speciﬁcally advocates for those that are also local, i.e., whereby the
output on node u directly depends on its neighbouring nodes in the graph.
We can formalise this constraint explicitly in our model construction, by
deﬁning what it means for a node to be neighbouring another.

A (undirected) neighbourhood of node u, sometimes also called 1-hop, is
deﬁned as

N
and the neighbourhood features as the multiset

∈ E

u =

v : (u, v)
{

or (v, u)

XNu =

xv : v

{{

.

u
∈ N

}}

∈ E}

(11)

(12)

Operating on 1-hop neighbourhoods aligns well with the locality aspect of
our blueprint: namely, deﬁning our metric over graphs as the shortest path
distance between nodes using edges in

.

E

The GDL blueprint thus yields a general recipe for constructing permutation
equivariant functions on graphs, by specifying a local function φ that operates
over the features of a node and its neighbourhood, φ(xu, XNu). Then, a
permutation equivariant function F can be constructed by applying φ to
every node’s neighbourhood in isolation (see Figure 10):

F(X, A) = 





φ(x1, XN1)
φ(x2, XN2)
...
φ(xn, XNn)








(13)

As F is constructed by applying a shared function φ to each node locally,
its permutation equivariance rests on φ’s output being independent on the
u. Thus, if φ is built to be permutation invariant,
ordering of the nodes in
then this property is satisﬁed. As we will see in future work, the choice of
φ plays a crucial role in the expressive power of such a scheme. When φ is
injective, it is equivalent to one step of the Weisfeiler-Lehman graph isomorphism
test, a classical algorithm in graph theory providing a necessary condition
for two graphs to be isomorphic by an iterative color reﬁnement procedure.

N

It is also worth noticing that the diﬀerence between functions deﬁned on sets
and more general graphs in this example is that in the latter case we need to
explicitly account for the structure of the domain. As a consequence, graphs
stand apart in the sense that the domain becomes part of the input in machine

4. GEOMETRIC DOMAINS: THE 5 GS

35

Figure 10: An illustration of constructing permutation-equivariant func-
tions over graphs, by applying a permutation-invariant function φ to every
neighbourhood. In this case, φ is applied to the features xb of node b as well
as the multiset of its neighbourhood features, XNb =
.
}}
Applying φ in this manner to every node’s neighbourhood recovers the rows
of the resulting matrix of latents features H = F(X, A).

xa, xb, xc, xd, xe

{{

learning problems, whereas when dealing with sets and grids (both particu-
lar cases of graphs) we can specify only the features and assume the domain
to be ﬁxed. This distinction will be a recurring motif in our discussion. As a re-
sult, the notion of geometric stability (invariance to domain deformation) is
crucial in most problems of learning on graphs. It straightforwardly follows
from our construction that permutation invariant and equivariant functions
produce identical outputs on isomorphic (topologically-equivalent) graphs.
These results can be generalised to approximately isomorphic graphs, and
several results on stability under graph perturbations exist (Levie et al.,
2018). We will return to this important point in our discussion on manifolds,
which we will use as an vehicle to study such invariance in further detail.

Second, due to their additional structure, graphs and grids, unlike sets,
can be coarsened in a non-trivial way
, giving rise to a variety of pooling
operations.

4.2 Grids and Euclidean spaces

The second type of objects we consider are grids. It is fair to say that the
impact of deep learning was particularly dramatic in computer vision, natural
language processing, and speech recognition. These applications all share a
geometric common denominator: an underlying grid structure. As already

More precisely, we cannot
deﬁne a non-trivial
coarsening assuming set
structure alone. There exist
established approaches that
infer topological structure
from unordered sets, and
those can admit non-trivial
coarsening.

xbxaxcxdxehbφ(xb,XNb)36

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

mentioned, grids are a particular case of graphs with special adjacency.
However, since the order of nodes in a grid is ﬁxed, machine learning models
for signals deﬁned on grids are no longer required to account for permutation
invariance, and have a stronger geometric prior: translation invariance.

As we will see later, this
makes the grid a
homogeneous space.

Circulant matrices and Convolutions Let us dwell on this point in more
detail. Assuming for simplicity periodic boundary conditions, we can think
of a one-dimensional grid as a ring graph with nodes indexed by 0, 1, . . . , n
1
−
modulo n (which we will omit for notation brevity) and the adjacency
matrix with elements au,u+1 mod n = 1 and zero otherwise. There are two
main diﬀerences from the general graph case we have discussed before.
First, each node u has identical connectivity, to its neighbours u
1 and
−
u + 1, and thus structure-wise indistinguishable from the others.
Second
and more importantly, since the nodes of the grid have a ﬁxed ordering,
we also have a ﬁxed ordering of the neighbours: we can call u
1 the ‘left
neighbour’ and u + 1 the ‘right neighbour’. If we use our previous recipe
for designing a equivariant function F using a local aggregation function
φ, we now have f (xu) = φ(xu−1, xu, xu+1) at every node of the grid: φ does
not need to be permutation invariant anymore. For a particular choice of a
linear transformation φ(xu−1, xu, xu+1) = θ−1xu−1 + θ0xu + θ1xu+1, we can
write F(X) as a matrix product,

−

F(X) =

θ0
θ−1



θ1
θ0
. . .








θ1

θ1
. . .
θ−1

. . .
θ0
θ−1

θ−1

θ1
θ0



















x0
x1
...
xn−2
xn−1










Note this very special multi-diagonal structure with one element repeated
along each diagonal, sometimes referred to as “weight sharing” in the ma-
chine learning literature.

Because of the periodic
boundary conditions, it is a
circular or cyclic convolution.
In signal processing, θ is
often referred to as the “ﬁlter,”
and in CNNs, its coeﬃcients
are learnable.

More generally, given a vector θ = (θ0, . . . , θn−1), a circulant matrix C(θ) =
(θu−v mod n) is obtained by appending circularly shifted versions of the vector
θ. Circulant matrices are synonymous with discrete convolutions,

n−1

(x (cid:63) θ)u =

xv mod n θu−v mod n

v=0
(cid:88)

4. GEOMETRIC DOMAINS: THE 5 GS

37

as one has C(θ)x = x (cid:63) θ. A particular choice of θ = (0, 1, 0, . . . , 0)(cid:62) yields a
special circulant matrix that shifts vectors to the right by one position. This
matrix is called the (right) shift or translation operator and denoted by S.

Circulant matrices can be characterised by their commutativity property: the
product of circulant matrices is commutative, i.e. C(θ)C(η) = C(η)C(θ)
for any θ and η. Since the shift is a circulant matrix, we get the familiar
translation or shift equivariance of the convolution operator,

The left shift operator is
given by S(cid:62). Obviously,
shifting left and then right
(or vice versa) does not do
anything, which means S is
orthogonal: S(cid:62)S = SS(cid:62) = I.

SC(θ)x = C(θ)Sx.

Such commutativity property should not be surprising, since the underlying
symmetry group (the translation group) is Abelian. Moreover, the opposite
direction appears to be true as well, i.e. a matrix is circulant iﬀ it commutes
with shift. This, in turn, allows us to deﬁne convolution as a translation equiv-
ariant linear operation, and is a nice illustration of the power of geometric
priors and the overall philosophy of Geometric ML: convolution emerges
from the ﬁrst principle of translational symmetry.

Note that unlike the situation on sets and graphs, the number of linearly
independent shift-equivariant functions (convolutions) grows with the size
of the domain (since we have one degree of freedom in each diagonal of a
circulant matrix). However, the scale separation prior guarantees ﬁlters can
be local, resulting in the same Θ(1)-parameter complexity per layer, as we
will verify in Section 5.1 when discussing the use of these principles in the
implementation of Convolutional Neural Network architectures.

Derivation of the discrete Fourier transform We have already mentioned
the Fourier transform and its connection to convolution: the fact that the
Fourier transform diagonalises the convolution operation is an important
property used in signal processing to perform convolution in the frequency
domain as an element-wise product of the Fourier transforms. However,
textbooks usually only state this fact, rarely explaining where the Fourier
transform comes from and what is so special about the Fourier basis. Here
we can show it, demonstrating once more how foundational are the basic
principles of symmetry.

algebra that (diagonalisable) ma-
For this purpose, recall a fact from linear
trices are joinly diagonalisable iﬀ they mutually commute. In other words,
there exists a common eigenbasis for all the circulant matrices, in which

We must additionally assume
distinct eigenvalues,
otherwise there might be
multiple possible
diagonalisations. This
assumption is satisﬁed with
our choice of S.

38

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

S is orthogonal but
non-symmetric, hence, its
eigenvectors are orthogonal
but the eigenvalues are
complex (roots of unity).

Note that the eigenvectors are
complex, so we need to take
complex conjugation when
transposing Φ.

Since the Fourier transform is
an orthogonal matrix
(Φ∗Φ = I), geometrically it
acts as a change of the system
of coordinates that amounts
to an n-dimensional rotation.
In this system of coordinates
(“Fourier domain”), the
action of a circulant C matrix
becomes element-wise
product.

they diﬀer only by their eigenvalues. We can therefore pick one circulant
matrix and compute its eigenvectors—we are assured that these will be the
eigenvectors of all other circulant matrices as well. It is convenient to pick the
shift operator, for which the eigenvectors happen to be the discrete Fourier
basis

ϕk =

1
√n

1, e

2πik
n , e

4πik
n , . . . , e

2πi(n−1)k
n

(cid:62)

,

k = 0, 1, . . . , n

1,

−

(cid:16)
which we can arrange into an n
Multiplication by Φ∗
the inverse DFT,

(cid:17)

n Fourier matrix Φ = (ϕ0, . . . , ϕn−1).
gives the Discrete Fourier Transform (DFT), and by Φ

×

ˆxk =

1
√n

n−1

u=0
(cid:88)

xue− 2πiku

n

xu =

1
√n

n−1

k=0
(cid:88)

ˆxke+ 2πiku
n .

Since all circulant matrices are jointly diagonalisable,
they are also diago-
nalised by the Fourier transform and diﬀer only in their eigenvalues. Since
the eigenvalues of the circulant matrix C(θ) are the Fourier transform of
the ﬁlter (see e.g. Bamieh (2018)), ˆθ = Φ∗θ, we obtain the Convolution
Theorem:

ˆθ0

. . .

C(θ)x = Φ 




Φ∗x = Φ( ˆθ

ˆx)

(cid:12)

ˆθn−1






Because the Fourier matrix Φ has a special algebraic structure, the prod-
ucts Φ(cid:63)x and Φx can be computed with
(n log n) complexity using a Fast
Fourier Transform (FFT) algorithm. This is one of the reasons why frequency-
domain ﬁltering is so popular in signal processing; furthermore, the ﬁlter is
typically designed directly in the frequency domain, so the Fourier transform
ˆθ is never explicitly computed.

O

Besides the didactic value of the derivation of the Fourier transform and
convolution we have done here, it provides a scheme to generalise these
concepts to graphs. Realising that the adjacency matrix of the ring graph is
exactly the shift operator, one can can develop the graph Fourier transform
and an analogy of the convolution operator by computing the eigenvectors
of the adjacency matrix (see e.g. Sandryhaila and Moura (2013)). Early

4. GEOMETRIC DOMAINS: THE 5 GS

39

In graph signal processing,
the eigenvectors of the graph
Laplacian are often used as an
alternative of the adjacency
matrix to construct the graph
Fourier transform, see
Shuman et al. (2013). On
grids, both matrices have
joint eigenvectors, but on
graphs they results in
somewhat diﬀerent though
related constructions.

attempts to develop graph neural networks by analogy to CNNs, sometimes
termed ‘spectral GNNs’, exploited this exact blueprint.
We will see in Sec-
tions 4.4–4.6 that this analogy has some important limitations. The ﬁrst
limitation comes from the fact that a grid is ﬁxed, and hence all signals on it
can be represented in the same Fourier basis. In contrast, on general graphs,
the Fourier basis depends on the structure of the graph. Hence, we cannot
directly compare Fourier transforms on two diﬀerent graphs — a problem
that translated into a lack of generalisation in machine learning problems.
Secondly, multi-dimensional grids, which are constructed as tensor products
of one-dimensional grids, retain the underlying structure: the Fourier basis
elements and the corresponding frequencies (eigenvalues) can be organised
in multiple dimensions. In images, for example, we can naturally talk about
horizontal and vertical frequency and ﬁlters have a notion of direction. On
graphs, the structure of the Fourier domain is one-dimensional, as we can
only organise the Fourier basis functions by the magnitude of the corre-
sponding frequencies. As a result, graph ﬁlters are oblivious of direction or
isotropic.

Derivation of the continuous Fourier transform For the sake of complete-
ness, and as a segway for the next discussion, we repeat our analysis in the
continuous setting. Like in Section 3.4, consider functions deﬁned on Ω = R
and the translation operator (Svf )(u) = f (u
v) shifting f by some posi-
tion v. Applying Sv to the Fourier basis functions ϕξ(u) = eiξu yields, by
associativity of the exponent,

−

Sveiξu = eiξ(u−v) = e−iξveiξu,

i.e., ϕuξ(u) is the complex eigenvector of Sv with the complex eigenvalue
e−iξv – exactly mirroring the situation we had in the discrete setting. Since
Sv is a unitary operator (i.e.,
Lp(R)),
p for any p and x
(cid:107)
any eigenvalue λ must satisfy
= 1, which corresponds precisely to the
eigenvalues e−iξv found above. Moreover, the spectrum of the translation
operator is simple, meaning that two functions sharing the same eigenvalue
must necessarily be collinear. Indeed, suppose that Svf = e−iξ0vf for some
ξ0. Taking the Fourier transform in both sides, we obtain

Svx
(cid:107)
λ
|

x
(cid:107)

p =

∈

(cid:107)

|

ξ , e−iξv ˆf (ξ) = e−iξ0v ˆf (ξ) ,

∀

which implies that ˆf (ξ) = 0 for ξ

= ξ0, thus f = αϕξ0

.

(cid:54)
40

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

For a general linear operator C that is translation equivariant (SvC = CSv),
we have

SvCeiξu = CSveiξu = e−iξvCeiξu,

of Sv with eigenvalue e−iξv, from
implying that Ceiξu is also an eigenfunction
where it follows from the simplicity of spectrum that Ceiξu = βϕξ(u); in
other words, the Fourier basis is the eigenbasis of all translation equivari-
ant operators. As a result, C is diagonal in the Fourier domain and can be
expressed as Ceiξu = ˆpC(ξ)eiξu, where ˆpC(ξ) is a transfer function acting on
diﬀerent frequencies ξ. Finally, for an arbitrary function x(u), by linearity,

(Cx)(u) = C

+∞

−∞

(cid:90)
+∞

ˆx(ξ)eiξudξ =

+∞

−∞

(cid:90)

ˆx(ξ)ˆpC(ξ)eiξudξ

=

−∞

(cid:90)

pC(v)x(u

−

v)dv = (x (cid:63) pC)(u),

where pC(u) is the inverse Fourier transform of ˆpC(ξ). It thus follows that
every linear translation equivariant operator is a convolution.

4.3 Groups and Homogeneous spaces

Our discussion of grids highlighted how shifts and convolutions are inti-
operations, and
mately connected: convolutions are linear shift-equivariant
vice versa, any shift-equivariant linear operator is a convolution. Further-
more, shift operators can be jointly diagonalised by the Fourier transform.
As it turns out, this is part of a far larger story: both convolution and the
Fourier transform can be deﬁned for any group of symmetries that we can sum
or integrate over.

Consider the Euclidean domain Ω = R. We can understand the convolution
as a pattern matching operation: we match shifted copies of a ﬁlter θ(u) with
an input signal x(u). The value of the convolution (x (cid:63) θ)(u) at a point u is
the inner product of the signal x with the ﬁlter shifted by u,

x, Suθ
(cid:104)

(x (cid:63) θ)(u) =

=

x(v)θ(u + v)dv.

(cid:105)
Note that in this case u is both a point on the domain Ω = R and also an element
of the translation group, which we can identify with the domain itself, G = R.
We will now show how to generalise this construction, by simply replacing
the translation group by another group G acting on Ω.

(cid:90)

R

Eigenfunction is synonymous
with ‘eigenvector’ and is used
when referring to
eigenvectors of continuous
operators.

The spectral characterisation
of the translation group is a
particular case of a more
general result in Functional
Analysis, the Stone’s Theorem,
which derives an equivalent
characterisation for any
one-parameter unitary group.

Technically, we need the
group to be locally compact, so
that there exists a
left-invariant Haar measure.
Integrating with respect to
this measure, we can “shift”
the integrand by any group
element and obtain the same
result, just as how we have

(cid:90) +∞

x(u)du =

(cid:90) +∞

x(u−v)du

−∞

−∞

for functions x : R → R.

Note that what we deﬁne
here is not convolution but
cross-correlation, which is
tacitly used in deep learning
under the name ‘convolution’.
We do it for consistency with
the following discussion,
since in our notation
(ρ(g)x)(u) = x(u − v) and
(ρ(g−1)x)(u) = x(u + v).

4. GEOMETRIC DOMAINS: THE 5 GS

41

Group convolution As discussed in Section 3, the action of the group G
on the domain Ω induces a representation ρ of G on the space of signals
(Ω) via ρ(g)x(u) = x(g−1u). In the above example, G is the translation
X
group whose elements act by shifting the coordinates, u + v, whereas ρ(g) is
v). Finally, in order
the shift operator acting on signals as (Svx)(u) = x(u
to apply a ﬁlter to the signal, we invoke our assumption of
(Ω) being a
Hilbert space, with an inner product

−

X

x, θ

=

(cid:105)

(cid:104)

(cid:90)Ω

x(u)θ(u)du,

where we assumed, for the sake of simplicity, scalar-valued signals,
in general the inner product has the form of equation (2).

X

(Ω, R);

Having thus deﬁned how to transform signals and match them with ﬁlters,
we can deﬁne the group convolution for signals on Ω,

The integration is done w.r.t.
an invariant measure µ on Ω.
In case µ is discrete, this
means summing over Ω.

(x (cid:63) θ)(g) =

x, ρ(g)θ
(cid:104)

=

(cid:105)
Note that x (cid:63) θ takes values on the elements g of our group G rather than points
on the domain Ω. Hence, the next layer, which takes x (cid:63) θ as input, should
act on signals deﬁned on to the group G, a point we will return to shortly.

(cid:90)Ω

x(u)θ(g−1u)du.

(14)

Just like how the traditional Euclidean convolution is shift-equivariant, the
more general group convolution is G-equivariant. The key observation is
that matching the signal x with a g-transformed ﬁlter ρ(g)θ is the same as
matching the inverse transformed signal ρ(g−1)x with the untransformed
ﬁlter θ. Mathematically, this can be expressed as
.
ρ(g−1)x, θ
(cid:105)
(cid:104)
With this insight, G-equivariance of the group convolution (14) follows
immediately from its deﬁnition and the deﬁning property ρ(h−1)ρ(g) =
ρ(h−1g) of group representations,

x, ρ(g)θ
(cid:104)

=

(cid:105)

(ρ(h)x (cid:63) θ)(g) =

ρ(h)x, ρ(g)θ
(cid:104)

(cid:105)

=

x, ρ(h−1g)θ
(cid:104)

(cid:105)

= ρ(h)(x (cid:63) θ)(g).

Let us look at some examples. The case of one-dimensional grid we have
studied above is obtained with the choice Ω = Zn =
and
{
the cyclic shift group G = Zn. The group elements in this case are cyclic
G can be identiﬁed with some u =
shifts of indices, i.e., an element g
∈
u mod n, whereas the inverse element is
0, . . . , n
g−1.v = v + u mod n. Importantly, in this example the elements of the group
(shifts) are also elements of the domain (indices). We thus can, with some

1 such that g.v = v

0, . . . , n

1
}

−

−

−

42

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

abuse of notation, identify the two structures (i.e., Ω = G); our expression
for the group convolution in this case

(x (cid:63) θ)(g) =

xv θg−1v,

n−1

v=0
(cid:88)

Actually here again, this is
cross-correlation.

leads to the familiar convolution

(x (cid:63) θ)u =

xv θv+u mod n.

n−1

v=0
(cid:88)

Spherical convolution Now consider
the two-dimensional sphere Ω = S2
with the group of rotations, the special orthogonal group G = SO(3). While
chosen for pedagogical reason, this example is actually very practical and
arises in numerous applications. In astrophysics, for example, observational
data often naturally has spherical geometry. Furthermore, spherical sym-
metries are very important in applications in chemistry when modeling
molecules and trying to predict their properties, e.g. for the purpose of
virtual drug screening.

= 1, the action of the group can be represented as a 3

Representing a point on the sphere as a three-dimensional unit vector u :
3 orthogonal
u
(cid:107)
(cid:107)
matrix R with det(R) = 1. The spherical convolution can thus be written as
the inner product between the signal and the rotated ﬁlter,

×

(x (cid:63) θ)(R) =

S2

(cid:90)

x(u)θ(R−1u)du.

The ﬁrst thing to note is than now the group is not identical to the domain:
the group SO(3) is a Lie group that is in fact a three-dimensional manifold,
whereas S2 is a two-dimensional one. Consequently, in this case, unlike the
previous example, the convolution is a function on SO(3) rather than on Ω.

This has important practical consequences: in our Geometric Deep Learn-
ing blueprint, we concatenate multiple equivariant maps (“layers” in deep
learning jargon) by applying a subsequent operator to the output of the
previous one. In the case of translations, we can apply multiple convolutions
in sequence, since their outputs are all deﬁned on the same domain Ω. In the
general setting, since x (cid:63) θ is a function on G rather than on Ω, we cannot use
exactly the same operation subsequently—it means that the next operation

Cosmic microwave
background radiation,
captured by the Planck space
observatory, is a signal on S2.

The action of SO(3) group on
S2. Note that three types of
rotation are possible; the
SO(3) is a three-dimensional
manifold.

4. GEOMETRIC DOMAINS: THE 5 GS

43

has to deal with signals on G, i.e. x
(G). Our deﬁnition of group convo-
∈ X
lution allows this case: we take as domain Ω = G acted on by G itself via the
gh deﬁned by the composition operation of G. This
group action (g, h)
(cid:55)→
.
yields the representation ρ(g) acting on x
Just like before, the inner product is deﬁned by integrating the point-wise
product of the signal and the ﬁlter over the domain, which now equals Ω = G.
In our example of spherical convolution, a second layer of convolution would
thus have the form

∈ X

(G) by (ρ(g)x)(h) = x(g−1h) The representation of G

acting on functions deﬁned
on G itself is called the
regular representation of G.

((x (cid:63) θ) (cid:63) φ)(R) =

(x (cid:63) θ)(Q)φ(R−1Q)dQ.

(cid:90)SO(3)

Since convolution involves inner product that in turn requires integrating
over the domain Ω, we can only use it on domains Ω that are small (in the
discrete case) or low-dimensional (in the continuous case). For instance,
we can use convolutions on the plane R2 (two dimensional) or special or-
thogonal group SE(3) (three dimensional), or on the ﬁnite set of nodes of
a graph (n-dimensional), but we cannot in practice perform convolution
on the group of permutations Σn, which has n! elements. Likewise, inte-
grating over higher-dimensional groups like the aﬃne group (containing
translations, rotations, shearing and scaling, for a total of 6 dimensions) is
not feasible in practice. Nevertheless, as we have seen in Section 5.3, we
can still build equivariant convolutions for large groups G by working with
signals deﬁned on low-dimensional spaces Ω on which G acts. Indeed, it is
possible to show that any equivariant linear map f :
(Ω(cid:48)) between
two domains Ω, Ω(cid:48) can be written as a generalised convolution similar to the
group convolution discussed here.

→ X

(Ω)

X

Second, we note that the Fourier transform we derived in the previous section
from the shift-equivariance property of the convolution can also be extended
to a more general case by projecting the signal onto the matrix elements of
irreducible representations of the symmetry group. We will discuss this in
future work. In the case of SO(3) studied here, this gives rise to the spherical
harmonics and Wigner D-functions, which ﬁnd wide applications in quantum
mechanics and chemistry.

Finally, we point to the assumption that has so far underpinned our discus-
sion in this section: whether Ω was a grid, plane, or the sphere, we could
transform every point into any other point, intuitively meaning that all the
points on the domain “look the same.” A domain Ω with such property is

44

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

The additional properties,
e.u = u and g(h.u) = (gh).u
are tacitly assumed here.

As well as the group of
rotations SO(3), by virtue of
it being a Lie group.

The term ‘3D’ is somewhat
misleading and refers to the
embedding space. The shapes
themselves are 2D manifolds
(surfaces).

The human body is an
example of a non-rigid object
deforming in a
nearly-isometric way.

called a homogeneous space, where for any u, v
that g.u = v

. In the next section we will try to relax this assumption.

Ω there exists g

∈

∈

G such

4.4 Geodesics and Manifolds

was a manifold, albeit a special one with a
In our last example, the sphere S2
global symmetry group due to its homogeneous structure. Unfortunately,
this is not the case for the majority of manifolds, which typically do not
have global symmetries. In this case, we cannot straightforwardly deﬁne an
action of G on the space of signals on Ω and use it to ‘slide’ ﬁlters around
in order to deﬁne a convolution as a direct generalisation of the classical
construction. Nevertheless, manifolds do have two types of invariance that
we will explore in this section: transformations preserving metric structure
and local reference frame change.

While for many machine learning readers manifolds might appear as some-
what exotic objects, they are in fact very common in various scientiﬁc do-
mains. In physics, manifolds play a central role as the model of our Universe
— according to Einstein’s General Relativity Theory, gravity arises from the
curvature of the space-time, modeled as a pseudo-Riemannian manifold. In
more ‘prosaic’ ﬁelds such as computer graphics and vision, manifolds are a
The broad spectrum of applica-
common mathematical model of 3D shapes.
tions of such models ranges from virtual and augmented reality and special
eﬀects obtained by means of ‘motion capture’ to structural biology dealing
with protein interactions that stick together (‘bind’ in chemical jargon) like
pieces of 3D puzzle. The common denominator of these applications is the
use of a manifold to represent the boundary surface of some 3D object.

First, they oﬀer a
There are several reasons why such models are convenient.
compact description of the 3D object, eliminating the need to allocate memory
to ‘empty space’ as is required in grid-based representations. Second, they
allow to ignore the internal structure of the object. This is a handy property
for example in structural biology where the internal folding of a protein
molecule is often irrelevant for interactions that happen on the molecular
surface. Third and most importantly, one often needs to deal with deformable
objects that undergo non-rigid deformations. Our own body is one such
example, and many applications in computer graphics and vision, such as
the aforementioned motion capture and virtual avatars, require deformation
invariance. Such deformations can be modelled very well as transformations

4. GEOMETRIC DOMAINS: THE 5 GS

45

that preserve the intrinsic structure of a (Riemannian) manifold, namely the
distances between points measured along the manifold, without regard to
the way the manifold is embedded in the ambient space.

We should emphasise that manifolds fall under the setting of varying domains
in our Geometric Deep Learning blueprint, and in this sense are similar
to graphs. We will highlight the importance of the notion of invariance to
domain deformations – what we called ‘geometric stability’ in Section 3.3.
Since diﬀerential geometry is perhaps less familiar to the machine learning
audience, we will introduce the basic concepts required for our discussion
and refer the reader to Penrose (2005) for their detailed exposition.

Riemannian manifolds Since the formal deﬁnition of a manifold
is some-
what involved, we prefer to provide an intuitive picture at the expense of
some precision. In this context, we can think of a (diﬀerentiable or smooth)
manifold as a smooth multidimensional curved surface that is locally Eu-
clidean, in the sense that any small neighbourhood around any point it can
be deformed to a neighbourhood of Rs; in this case the manifold is said to
be s-dimensional. This allows us to locally approximate the manifold around
point u through the tangent space TuΩ. The latter can be visualised by think-
ing of a prototypical two-dimensional manifold, the sphere, and attaching a
plane to it at a point: with suﬃcient zoom, the spherical surface will seem
planar (Figure 11).
The collection of all tangent spaces is called the tangent
bundle, denoted T Ω; we will dwell on the concept of bundles in more detail
in Section 4.5.

By ‘smooth’ we mean
diﬀerentiable suﬃent
number of times, which is
tacitly assumed for
convenience. ‘Deformed’
here means diﬀeomorphic, i.e.,
we can map between the two
neighbourhoods using a
smooth and invertible map
with smooth inverse.

Formally, the tangent bundle
is the disjoint union
(cid:71)
T Ω =

TuΩ.

u∈Ω

∈

A tangent vector, which we denote by X
TuΩ, can be thought of as a local
displacement from point u. In order to measure the lengths of tangent vectors
we need to equip the tangent space with additional
and angles between them,
structure, expressed as a positive-deﬁnite bilinear function gu : TuΩ
→
R depending smoothly on u. Such a function is called a Riemannian metric, in
honour of Bernhardt Riemann who introduced the concept in 1856, and can
u = gu(X, Y ),
be thought of as an inner product on the tangent space,
(cid:105)
which is an expression of the angle between any two tangent vectors X, Y
TuΩ. The metric also induces a norm
measure lengths of vectors.

∈
u (X, X) allowing to locally

u = g1/2
(cid:107)

X, Y
(cid:104)

TuΩ

X

×

(cid:107)

A bilinear function g is said
to be positive-deﬁnite if
g(X, X) > 0 for any non-zero
vector X (cid:54)= 0. If g is
expressed as a matrix G, it
means G (cid:31) 0. The
determinant |G|1/2 provides
a local volume element,
which does not depend on
the choice of the basis.

We must stress that tangent vectors are abstract geometric entities that exists
in their own right and are coordinate-free. If we are to express a tangent vector

46

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

X numerically as an array of numbers, we can only represent it as a list of
coordinates x = (x1, . . . , xs) relative to some local basis
TuΩ.
X1, . . . Xs
{
Similarly, the metric can be expressed as an s
s matrix G with elements
gij = gu(Xi, Xj) in that basis. We will return to this point in Section 4.5.

} ⊆

×

Unfortunately, too often
vectors are identiﬁed with
their coordinates. To
emphasise this important
diﬀerence, we use X to
denote a tangent vector and x
to denote its coordinates.

R3 :

R3 : x(cid:62)u = 0
}

Figure 11: Basic notions of Riemannian geometry illustrated on the example
of the two-dimensional sphere S2 =
, realised a subset
u
{
(sub-manifold) of R3. The tangent space to the sphere is given as TuS2 =
and is a 2D plane – hence this is a 2-dimensional
x
{
manifold. The Riemannian metric is simply the Euclidean inner product
TuS2. The
restricted to the tangent plane,
TuS2.
exponential map is given by expu(x) = cos(
(cid:107)
Geodesics are great arcs of length d(u, v) = cos−1(u(cid:62)v).

u = x(cid:62)y for any x, x
)u + sin((cid:107)x(cid:107))

∈
(cid:107)x(cid:107) x, for x

= 1
}

x, y
(cid:104)

u
(cid:107)

∈

∈

∈

x

(cid:107)

(cid:107)

(cid:105)

A manifold equipped with a metric is called a Riemannian manifold and
properties that can be expressed entirely in terms of the metric are said
to be intrinsic. This is a crucial notion for our discussion, as according to
our template, we will be seeking to construct functions acting on signals
deﬁned on Ω that are invariant to metric-preserving transformations called
If
isometries that deform the manifold without aﬀecting its local structure.
such functions can be expressed in terms of intrinsic quantities, they are
automatically guaranteed to be isometry-invariant and thus unaﬀected by
isometric deformations. These results can be further extended to dealing
with approximate isometries; this is thus an instance of the geometric stability
(domain deformation) discussed in our blueprint.

While, as we noted, the deﬁnition of a Riemannian manifold does not require
a geometric realisation in any space, it turns out that any smooth Riemannian
manifold can be realised as a subset of a Euclidean space of suﬃciently high
dimension (in which case it is said to be ‘embedded’ in that space) by using

This result is known as the
Embedding Theorem, due to
Nash (1956). The art of
origami is a manifestation of
diﬀerent isometric
embeddings of the planar
surface in R3 (Figure:
Shutterstock/300 librarians).

4. GEOMETRIC DOMAINS: THE 5 GS

47

the structure of the Euclidean space to induce a Riemannian metric. Such an
embedding is however not necessarily unique – as we will see, two diﬀerent
isometric realisations of a Riemannian metric are possible.

Scalar and Vector ﬁelds Since we are interested in signals deﬁned on Ω,
we need to provide the proper notion of scalar- and vector-valued functions
R.
on manifolds. A (smooth) scalar ﬁeld is a function of the form x : Ω
Scalar ﬁelds form a vector space
(Ω, R) that can be equipped with the inner
product

→

X

Example of a scalar ﬁeld.

Example of a vector ﬁeld.
The ﬁelds are typically
assumed to be of the same
regularity class (smoothness)
as the manifold itself.

x, y
(cid:104)

(cid:105)

=

(cid:90)Ω

x(u)y(u)du,

(15)

where du is the volume element induced by the Riemannian metric. A
T Ω assigning to
(smooth) tangent vector ﬁeld is a function of the form X : Ω
→
TuΩ.
each point a tangent vector in the respective tangent space, u
Vector ﬁelds
(Ω, T Ω) with the inner product
deﬁned through the Riemannian metric,

also form a vector space

X(u)

(cid:55)→

X

∈

X, Y

(cid:104)

=

(cid:105)

(cid:90)Ω

gu(X(u), Y (u))du.

(16)

Intrinsic gradient Another way to think of (and actually deﬁne) vector
ﬁelds is as a generalised notion of derivative. In classical calculus, one
can locally linearise a (smooth) function through the diﬀerential dx(u) =
x(u), which provides the change of the value of the function
x(u + du)
x at point u as a result of an iniﬁnitesimal displacement du. However, in
our case the naïve use of this deﬁnition is impossible, since expressions of
the form “u + du” are meaningless on manifolds due to the lack of a global
vector space structure.

−

The solution is to use tangent vectors as a model of local inﬁnitesimal displace-
ment. Given a smooth scalar ﬁeld x
(Ω, R), we can think of a (smooth)
∈ X
(Ω, R) satisfying the properties
vector ﬁeld as a linear map Y :
(Ω, R)
of a derivation: Y (c) = 0 for any constant c (corresponding to the intuition
that constant functions have vanishing derivatives), Y (x + z) = Y (x) + Y (z)
(linearity), and Y (xz) = Y (x)z + xY (z) (product or Leibniz rule), for any
smooth scalar ﬁelds x, z
(Ω, R). It can be shown that one can use these
properties to deﬁne vector ﬁelds axiomatically. The diﬀerential dx(Y ) = Y (x)
Y (x) and interpreted as follows: the
can be viewed as an operator (u, Y )

→ X

∈ X

X

(cid:55)→

48

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Importantly, this construction

does not use the Riemannian
metric whatsoever and can
thus can be extended to a
more general construction of
bundles discussed in the
Section 4.5.

This is a consequence of the
Riesz-Fréchet Representation
Theorem, by which every
dual vector can be expressed
as an inner product with a
vector.

It is tacitly assumed that
curves are given in arclength
parametrisation, such that
(cid:107)γ(cid:48)(cid:107) = 1 (constant velocity).

The Levi-Civita connection is
torsion-free and compatible
with the metric. The
Fundamental Theorem of
Riemannian geometry
guarantees its existence and
uniqueness.

change of x as the result of displacement Y
dux(Y ).

TuΩ at point u is given by
It is thus an extension of the classical notion of directional derivative.

∈

Alternatively, at each point u the diﬀerential can be regarded as a linear
TuΩ. Linear
functional dxu : TuΩ
functionals on a vector space are called dual vectors or covectors; if in addition
we are given an inner product (Riemannian metric), a dual vector can always
be represented as

R acting on tangent vectors X

→

∈

dxu(X) = gu(

∇

x(u), X).

The representation of the diﬀerential at point u is a tangent vector
∈
TuΩ called the (intrinsic) gradient of x; similarly to the gradient in classical
calculus, it can be thought of as the direction of the steepest increase of x.
The gradient considered as an operator
(Ω, T Ω) assigns at
each point x(u)
TuΩ; thus, the gradient of a scalar ﬁeld x is a
x.
vector ﬁeld

(Ω, R)

(cid:55)→ ∇

→ X

x(u)

x(u)

∇

∇

X

∈

:

∇

Ω on the manifold
Geodesics Now consider a smooth curve γ : [0, T ]
with endpoints u = γ(0) and v = γ(T ). The derivative of the curve at point
t is a tangent vector γ(cid:48)(t)
Among all the
curves connecting points u and v, we are interested in those of minimum
length, i.e., we are seeking γ minimising the length functional

Tγ(t)Ω called the velocity vector.

→

∈

(cid:96)(γ) =

T

0 (cid:107)
(cid:90)

γ(cid:48)(t)

(cid:107)γ(t)dt =

T

0
(cid:90)

g1/2
γ(t)(γ(cid:48)(t), γ(cid:48)(t))dt.

Such curves are called geodesics (from the Greek γεοδαισία, literally ‘division
of Earth’) and they play important role in diﬀerential geometry. Crucially to
our discussion, the way we deﬁned geodesics is intrinsic, as they depend
solely on the Riemannian metric (through the length functional).

Readers familiar with diﬀerential geometry might recall that geodesics are a
more general concept and their deﬁnition in fact does not necessarily require
a Riemannian metric but a connection (also called a covariant derivative, as
it generalises the notion of derivative to vector and tensor ﬁelds), which is
deﬁned axiomatically, similarly to our construction of the diﬀerential. Given
a Riemannian metric, there exists a unique special connection called the
Levi-
Civita connection which is often tacitly assumed in Riemannian geometry.
Geodesics arising from this connection are the length-minimising curves we
have deﬁned above.

4. GEOMETRIC DOMAINS: THE 5 GS

49

We will show next how to use geodesics to deﬁne a way to transport tangent
vectors on the manifold (parallel transport), create local intrinsic maps from
the manifold to the tangent space (exponential map), and deﬁne distances
(geodesic metric). This will allow us to construct convolution-like operations
by applying a ﬁlter locally in the tangent space.

TuΩ and Y

Parallel transport One issue we have already encountered when dealing
with manifolds is that we cannot directly add or subtract two points u, v
Ω.
The same problem arises when trying to compare tangent vectors at diﬀerent
points: though they have the same dimension, they belong to diﬀerent spaces,
TvΩ, and thus not directly comparable. Geodesics
e.g. X
provide a mechanism to move vectors from one point to another, in the
following way: let γ be a geodesic connecting points u = γ(0) and v = γ(T )
TuΩ. We can deﬁne a new set of tangent vectors along the
and let X
geodesic, X(t)
Tγ(t)Ω such that the length of X(t) and the angle (expressed
through the Riemannian metric) between it and the velocity vector of the
curve is constant,

∈

∈

∈

∈

∈

gγ(t)(X(t), γ(cid:48)(t)) = gu(X, γ(cid:48)(0)) = const,

As a result, we get a unique vector X(T )

∈

X(t)
(cid:107)

(cid:107)γ(t) =
TvΩ at the end point v.

X

(cid:107)

u = const.
(cid:107)

→

The map Γu→v(X) : TuΩ
TuΩ and TvΩ deﬁned as Γu→v(X) = X(T )
using the above notation is called parallel transport or connection; the latter
term implying it is a mechanism to ‘connect’ between the tangent spaces
TuΩ and TvΩ. Due to the angle and length preservation conditions, parallel
transport amounts to only rotation of the vector, so it can be associated with
an element of the special orthogonal group SO(s) (called the structure group
of the tangent bundle),
which we will denote by gu→v and discuss in further
detail in Section 4.5.

As we mentioned before, a connection can be deﬁned axiomatically inde-
pendently of the Riemannian metric, providing thus an abstract notion of
parallel transport along any smooth curve. The result of such transport,
however, depends on the path taken.

Exponential map Locally around a point u, it is always possible to deﬁne
TuΩ, i.e. such that γ(0) = u and
a unique geodesic in a given direction X
∈
0 (that is, we can shoot the
γ(cid:48)(0) = X. When γX (t) is deﬁned for all t

≥

Euclidean transport of a
vector from A to C makes no
sense on the sphere, as the
resulting vectors (red) are
not in the tangent plane.
Parallel transport from A to C
(blue) rotates the vector
along the path. It is path
dependent: going along the
path BC and ABC produces
diﬀerent results.

Assuming that the manifold
is orientable, otherwise O(s).

50

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

geodesic from a point u for as long as we like), the manifold is said to be
geodesically complete and the exponential map is deﬁned on the whole tangent
space. Since compact manifolds are geodesically complete, we can tacitly
assume this convenient property.

Note that geodesic
completeness does not
necessarily guarantee that
exp is a global
diﬀeomorphism – the largest
radius r about u for which
expu(Br(0) ⊆ TuΩ) is
mapped diﬀeomorphically is
called the injectivity radius.

Hopf-Rinow Theorem thus
estabilishes the equivalence
between geodesic and metric
completeness, the latter
meaning every Cauchy
sequence converges in the
geodesic distance metric.

Note that the term ‘metric’ is
used in two senses:
Riemannian metric g and
distance d. To avoid
confusion, we will use the
term ‘distance’ referring to
the latter. Our notation dg
makes the distance depend
on the Riemannian metric g,
though the deﬁnition of
geodesic length L.

Pushforward and pullback
are adjoint operators
(cid:104)η∗α, X(cid:105) = (cid:104)α, η∗X(cid:105) where
α ∈ T ∗Ω is a dual vector ﬁeld,
deﬁned at each point as a
linear functional acting on
TuΩ and the inner products
are deﬁned respectively on
vector and dual vector ﬁelds.

TuΩ

exp : Br(0)

This deﬁnition of geodesic provided a point and a direction gives a natural
mapping from (a subset of) the tangent space TuΩ to Ω called the exponential
Ω, which is deﬁned by taking a unit step along
map
the geodesic in the direction X, i.e., expu(X) = γX (1). The exponential map
is a local diﬀeomorphism, as it deforms the neighbourhood Br(0) (a
expu
ball or radius r) of the origin on TuΩ into a neighbourhood of u. Conversely,
one can also regard the exponential map as an intrinsic local deformation
(‘ﬂattening’) of the manifold into the tangent space.

→

⊂

guar-
Geodesic distances A result known as the Hopf-Rinow Theorem
antees that geodesically complete manifolds are also complete metric spaces,
in which one can realise a distance (called the geodesic distance or metric)
between any pair of points u, v as the length of the shortest path between
them

dg(u, v) = min

γ

(cid:96)(γ)

s.t.

γ(0) = u, γ(T ) = v,

which exists (i.e., the minimum is attained).

→

→

Isometries Consider now a deformation of our manifold Ω into another
manifold ˜Ω with a Riemannian metric h, which we assume to be a dif-
( ˜Ω, h) between the manifolds. Its diﬀerential
feomorphism η : (Ω, g)
T ˜Ω deﬁnes a map between the respective tangent bundles (re-
dη : T Ω
˜Ω,
ferred to as pushforward), such that at a point u, we have dηu : TuΩ
interpreted as before: if we make a small displacement from point u by
tangent vector X
TuΩ, the map η will be displaced from point η(u) by
tangent vector dηu(X)

Tη(u)

˜Ω.

Since the pushforward
on the two manifolds, it allows to pullback the metric h from ˜Ω to Ω,

∈
provides a mechanism to associate tangent vectors

Tη(u)

→

∈

(η∗h)u(X, Y ) = hη(u)(dηu(X), dηu(Y ))

If the pullback metric coincides at every point with that of Ω, i.e., g = η∗h,
the map η is called (a Riemannian) isometry. For two-dimensional manifolds

4. GEOMETRIC DOMAINS: THE 5 GS

51

(surfaces), isometries can be intuitively understood as inelastic deformations
that deform the manifold without ‘stretching’ or ‘tearing’ it.

By virtue of their deﬁnition, isometries preserve intrinsic structures such as
geodesic distances, which are expressed entirely in terms of the Riemannian
metric. Therefore, we can also understand isometries from the position of
metric geometry, as distance-preserving maps (‘metric isometries’) between
( ˜Ω, dh), in the sense that
metric spaces η : (Ω, dg)

→

dg(u, v) = dh(η(u), η(v))

Ω, or more compactly, dg = dh

η). In other words,
for all u, v
Riemannian isometries are also metric isometries. On connected manifolds,
the converse is also true: every metric isometry is also a Riemannian isometry.

(η

×

∈

◦

In our Geometric Deep Learning blueprint, η is a model of domain defor-
mations. When η is an isometry, any intrinsic quantities are unaﬀected by
such deformations. One can generalise exact (metric) isometries through
the notions of metric dilation

dil(η) = sup

u(cid:54)=v∈Ω

dh(η(u), η(v))
dg(u, v)

or metric distortion

dis(η) = sup

dh(η(u), η(v))

u,v∈Ω |

dg(u, v)

,
|

−

which capture the relative and absolute change of the geodesic distances
under η, respectively. The condition (5) for the stability of a function f
(Ω)) under domain deformation can be rewritten in this case as

∈

(
X

F

f (x, Ω)

(cid:107)

f (x

◦

−

η−1, ˜Ω)

(cid:107) ≤

C

x

dis(η).
(cid:107)

(cid:107)

Intrinsic symmetries A particular case of the above is a diﬀeomorphism
of the domain itself (what we termed automorphism in Section 3.2), which we
Diﬀ(Ω). We will call it a Riemannian (self-)isometry if the
will denote by τ
pullback metric satisﬁes τ ∗g = g, or a metric (self-)isometry if dg = dg
τ ).
isometries form a group with the composition operator
Not surprisingly,
denoted by Iso(Ω) and called the isometry group; the identity element is
the map τ (u) = u and the inverse always exists (by deﬁnition of τ as a
diﬀeomorphism). Self-isometries are thus intrinsic symmetries of manifolds.

(τ

×

∈

◦

This result is known as the
Myers–Steenrod Theorem.
We tacitly assume our
manifolds to be connected.

The Gromov-Hausdorﬀ
distance between metric
spaces, which we mentioned
in Section 3.2, can be
expressed as the smallest
possible metric distortion.

Continuous symmetries on
manifolds are inﬁnitesimally
generated by special tangent
vector ﬁelds called Killing
ﬁelds, named after Wilhelm
Killing.

52

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Fourier analysis on Manifolds We will now show how to construct intrin-
sic convolution-like operations on manifolds, which, by construction, will be
invariant to isometric deformations. For this purpose, we have two options:
One is to use an analogy of the Fourier transform, and deﬁne the convolution
as a product in the Fourier domain. The other is to deﬁne the convolution
spatially, by correlating a ﬁlter locally with the signal. Let us discuss the
spectral approach ﬁrst.

We remind that in the Euclidean domain the Fourier transform is obtained as
the eigenvectors of circulant matrices, which are jointly diagonalisable due to
their commutativity. Thus, any circulant matrix and in particular, diﬀerential
operator, can be used to deﬁne an analogy of the Fourier transform on general
domains. In Riemannian geometry, it is common to use the orthogonal
eigenbasis of the Laplacian operator, which we will deﬁne here.

:

X

→ X

(Ω, R)

For this purpose, recall our deﬁnition of the intrinsic gradient operator
(Ω, T Ω), producing a tangent vector ﬁeld that indicates
∇
the local direction of steepest increase of a scalar ﬁeld on the manifold. In
a similar manner, we can deﬁne the divergence operator

→
(Ω, R). If we think of a tangent vector ﬁeld as a ﬂow on the manifold, the
X
divergence measures the net ﬂow of a ﬁeld at a point, allowing to distinguish
between ﬁeld ‘sources’ and ‘sinks’. We use the notation
∗ (as opposed to
the common div) to emphasise that the two operators are adjoint,

(Ω, T Ω)

∗ :

∇

∇

X

X,
(cid:104)

x

(cid:105)

∇

=

∗X, x
,
(cid:105)

(cid:104)∇

where we use the inner products (15) and (16) between scalar and vector
ﬁelds.

From this interpretation it is
also clear that the Laplacian
is isotropic. We will see in
Section 4.6 that it is possible
to deﬁne anisotropic Laplacians
(see (Andreux et al., 2014;
Boscaini et al., 2016b)) of the
form ∇∗(A(u)∇), where
A(u) is a position-dependent
tensor determining local
direction.

X

(Ω) deﬁned as ∆ =

The Laplacian (also known as the Laplace-Beltrami operator in diﬀerential geom-
, which can be interpreted
etry) is an operator on
as the diﬀerence between the average of a function on an inﬁnitesimal sphere
around a point and the value of the function at the point itself. It is one
of the most important operators in mathematical physics, used to describe
phenomena as diverse as heat diﬀusion, quantum oscillations, and wave
propagation. Importantly in our context, the Laplacian is intrinsic, and thus
invariant under isometries of Ω.

∇

∇

∗

It is easy to see that the Laplacian is self-adjoint (‘symmetric’),

x,

(cid:104)∇

x

(cid:105)

∇

=

x, ∆x
(cid:105)
(cid:104)

=

.
∆x, x
(cid:105)

(cid:104)

4. GEOMETRIC DOMAINS: THE 5 GS

53

The quadratic form on the left in the above expression is actually the already
familiar Dirichlet energy,

c2(x) =

x

2 =
(cid:107)

(cid:107)∇

x,

(cid:104)∇

=

x
(cid:105)

∇

x(u)
(cid:107)

2
udu =

(cid:90)Ω

(cid:90)Ω (cid:107)∇

gu(

∇

x(u),

∇

x(u))du

measuring the smoothness of x.

The Laplacian operator admits an eigedecomposition

∆ϕk = λkϕk,

k = 0, 1, . . .

with countable spectrum if the manifold is compact (which we tacitly as-
sume), and orthogonal eigenfunctions,
ϕk, ϕl
(cid:104)
of ∆. The Laplacian eigenbasis can also be constructed as a set of orthogonal
minimisers of the Dirichlet energy,

= δkl, due to the self-adjointness

(cid:105)

ϕk+1 = arg min

ϕ
(cid:107)

2

s.t.

= 1 and

ϕ
(cid:107)

(cid:107)

ϕ, ϕj

(cid:104)

(cid:105)

= 0

ϕ (cid:107)∇

for j = 0, . . . , k, allowing to interpret it as the smoothest orthogonal basis
on Ω. The eigenfunctions ϕ0, ϕ1, . . . and the corresponding eigenvalues
. . . can be interpreted as the analogy of the atoms and
0 = λ0 ≤
frequencies in the classical Fourier transform.

λ1 ≤

This orthogonal basis allows to expand square-integrable functions on Ω
into Fourier series

In fact eiξu are the
eigenfunctions of the
Euclidean Laplacian d2
du2

.

x(u) =

x, ϕk
(cid:104)

ϕk(u)
(cid:105)

k≥0
(cid:88)

where ˆxk =
x, ϕk
alised) Fourier transform of x.
approximation error that can be bounded (Aﬂalo and Kimmel, 2013) by

are referred to as the Fourier coeﬃcient or the (gener-
Truncating the Fourier series results in an

(cid:105)

(cid:104)

N

−

k=0
(cid:88)

x, ϕk
(cid:104)

ϕk
(cid:105)

2

x
(cid:107)∇
(cid:107)
λN +1

.

≤

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Aﬂalo et al. (2015) further showed that no other basis attains a better error,
making the Laplacian eigenbasis optimal for representing smooth signals on
manifolds.

Note that this Fourier
transform has a discrete
index, since the spectrum is
discrete due to the
compactness of Ω.

54

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Spectral Convolution on Manifolds Spectral convolution can be deﬁned as
the product of Fourier transforms of the signal x and the ﬁlter θ,

(x (cid:63) θ)(u) =

(ˆxk

ˆθk)ϕk(u).

(17)

·

k≥0
(cid:88)
Note that here we use what is a property of the classical Fourier transform
(the Convolution Theorem) as a way to deﬁne a non-Euclidean convolution.
By virtue of its construction, the spectral convolution is intrinsic and thus
isometry-invariant. Furthermore, since the Laplacian operator is isotropic,
it has no sense of direction; in this sense, the situation is similar to that we
had on graphs in Section 4.1 due to permutation invariance of neighbour
aggregation.

Figure 12: Instability of spectral ﬁlters under domain perturbation. Left: a
signal x on the mesh Ω. Middle: result of spectral ﬁltering in the eigenbasis
of the Laplacian ∆ on Ω. Right: the same spectral ﬁlter applied to the
eigenvectors of the Laplacian ˜∆ of a nearly-isometrically perturbed domain
˜Ω produces a very diﬀerent result.

In practice, a direct computation of (17) appears to be prohibitively expensive
due to the need to diagonalise the Laplacian. Even worse, it turns out
geometrically unstable: the higher-frequency eigenfunctions of the Laplacian
can change dramatically as a result of even small near-isometric perturbations
of the domain Ω (see Figure 12). A more stable solution is provided by
realising the ﬁlter as a spectral transfer function of the form ˆp(∆),

(ˆp(∆)x)(u) =

ˆp(λk)

x, ϕk
(cid:104)

ϕk(u)
(cid:105)

k≥0
(cid:88)

=

x(v)

ˆp(λk)ϕk(v)ϕk(u) dv

(18)

(19)

(cid:90)Ω

k≥0
(cid:88)

which can be interpreted in two manners: either as a spectral ﬁlter (18),
where we identify ˆθk = ˆp(λk), or as a spatial ﬁlter (19) with a position-
k≥0 ˆp(λk)ϕk(v)ϕk(u). The advantage of this
dependent kernel θ(u, v) =

(cid:80)

4. GEOMETRIC DOMAINS: THE 5 GS

55

Geometric Deep Learning
methods based on spectral
convolution expressed
through the Fourier
transform are often referred
to as ‘spectral’ and opposed
to ‘spatial’ methods we have
seen before in the context of
graphs. We see here that
these two views may be
equivalent, so this dichotomy
is somewhat artiﬁcial and not
completely appropriate.

formulation is that ˆp(λ) can be parametrised by a small number of coeﬃcients,
r
and choosing parametric functions such as polynomials
l=0 αlλl
allows for eﬃciently computing the ﬁlter as

ˆp(λ) =

(cid:80)

(ˆp(∆)x)(u) =

r

k≥0
(cid:88)

l=0
(cid:88)

αlλl

k (cid:104)

x, ϕk

ϕk(u) =
(cid:105)

αl(∆lx)(u),

r

l=0
(cid:88)

avoiding the spectral decomposition altogether. We will discuss this con-
struction in further detail in Section 4.6.

Spatial Convolution on Manifolds A second alternative is to attempt
deﬁning convolution on manifolds is by matching a ﬁlter at diﬀerent points,
like we did in formula (14),

(x (cid:63) θ)(u) =

(cid:90)TuΩ

x(expu Y )θu(Y )dY,

(20)

where we now have to use the exponential map to access the values of the
scalar ﬁeld x from the tangent space, and the ﬁlter θu is deﬁned in the tangent
space at each point and hence position-dependent. If one deﬁnes the ﬁlter
intrinsically, such a convolution would be isometry-invariant, a property we
mentioned as crucial in many computer vision and graphics applications.

We need, however, to note several substantial diﬀerences from our previous
construction in Sections 4.2–4.3. First, because a manifold is generally not
a homogeneous space, we do not have anymore a global group structure
allowing us have a shared ﬁlter (i.e., the same θ at every u rather than θu
in expression (20)) deﬁned at one point and then move it around. An
analogy of this operation on the manifold would require parallel transport,
allowing to apply a shared θ, deﬁned as a function on TuΩ, at some other
TvΩ. However, as we have seen, this in general will depend on the path
between u and v, so the way we move the ﬁlter around matters.Third, since
we can use the exponential map only locally, the ﬁlter must be local, with
support bounded by the injectivity radius. Fourth and most crucially, we
cannot work with θ(X), as X is an abstract geometric object: in order for it
to be used for computations, we must represent it relative to some local basis
ωu : Rs
u (X). This
TuΩ, as an s-dimensional array of coordinates x = ω−1
allows us to rewrite the convolution (20) as

→

(x (cid:63) θ)(u) =

(cid:90)[0,1]s

x(expu(ωuy))θ(y)dy,

(21)

56

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

with the ﬁlter deﬁned on the unit cube. Since the exponential map is intrinsic
(through the deﬁnition of geodesic), the resulting convolution is isometry-
invariant.

The sphere S2 is an example
of a non-parallelisable
manifold, a result of the
Poincaré-Hopf Theorem, which
is colloquially stated as ‘one
cannot comb a hairy ball
without creating a cowlick.’

Example of stable gauges
constructed on
nearly-isometric manifolds
(only one axis is shown)
using the GFrames algorithm
of Melzi et al. (2019).

◦

u = dηu

Yet, this tacitly assumed we can carry the frame ωu along to another manifold,
i.e. ω(cid:48)
ωu. Obtaining such a frame (or gauge, in physics terminology)
given only a the manifold Ω in a consistent manner is however fraught with
diﬃculty. First, a smooth global gauge may not exist: this is the situation on
manifolds that are not parallelisable,
in which case one cannot deﬁne a smooth
non-vanishing tangent vector ﬁeld. Second, we do not have a canonical gauge
on manifolds, so this choice is arbitrary; since our convolution depends on
ω, if one chose a diﬀerent one, we would obtain diﬀerent results.

We should note that this is a case where practice diverges from theory: in
practice, it is possible to build frames that are mostly smooth, with a limited
number of singularities, e.g. by taking the intrinsic gradient of some intrinsic
scalar ﬁeld on the manifold.
Moreover, such constructions are stable, i.e.,
the frames constructed this way will be identical on isometric manifolds
and similar on approximately isometric ones. Such approaches were in fact
employed in the early works on deep learning on manifolds (Masci et al.,
2015; Monti et al., 2017).

Nevertheless, this solution is not entirely satisfactory because near singu-
larities, the ﬁlter orientation (being deﬁned in a ﬁxed manner relative to
the gauge) will vary wildly, leading to a non-smooth feature map even if
the input signal and ﬁlter are smooth. Moreover, there is no clear reason
why a given direction at some point u should be considered equivalent to
another direction at an altogether diﬀerent point v. Thus, despite practical
alternatives, we will look next for a more theoretically well-founded approach
that would be altogether independent on the choice of gauge.

4.5 Gauges and Bundles

Historically, ﬁbre bundles
arose ﬁrst in modern
diﬀerential geometry of Élie
Cartan (who however did not
deﬁne them explicitly), and
were then further developed
as a standalone object in the
ﬁeld of topology in the 1930s.

The notion of gauge, which we have deﬁned as a frame for the tangent space,
is quite a bit more general in physics: it can refer to a frame for any
vector
bundle, not just the tangent bundle. Informally, a vector bundle describes
a family of vector spaces parametrised by another space and consists of a
base space Ω with an identical vector space V (called the ﬁbre) attached to
Ω (for the tangent bundle these are the tangent spaces
each position u

∈

4. GEOMETRIC DOMAINS: THE 5 GS

57

TuΩ). Roughly speaking, a bundle looks as a product Ω
V locally around
u, but globally might be ‘twisted’ and have an overall diﬀerent structure. In
Geometric Deep Learning, ﬁbres can be used to model the feature spaces at
each point in the manifold Ω, with the dimension of the ﬁbre being equal to
the number of feature channels. In this context, a new and fascinating kind
of symmetry, called gauge symmetry may present itself.

×

→

Let us consider again an s-dimensional manifold Ω with its tangent bundle
T Ω, and a vector ﬁeld X : Ω
T Ω (which in this terminology is referred
to as a section on the tangent bundle). Relative to a gauge ω for the tangent
bundle, X is represented as a function x : Ω
Rs. However it is important
to realise that what we are really interested in is the underlying geometrical
object (vector ﬁeld), whose representation as a function x
(Ω, Rs) depends
on the choice of gauge ω. If we change the gauge, we also need to change x so
as to preserve the underlying vector ﬁeld being represented.

∈ X

→

Tangent bundles and the Structure group When we change the gauge, we
need to apply at each point an invertible matrix that maps the old gauge to
the new one. This matrix is unique for every pair of gauges at each point, but
possibly diﬀerent at diﬀerent points. In other words, a gauge transformation is
GL(s), where GL(s) is the general linear group of invertible
a mapping g : Ω
TuΩ to produce a new gauge
s
×
TuΩ. The gauge transformation acts on a coordinate
ω(cid:48)
u = ωu
vector ﬁeld at each point via x(cid:48)(u) = g−1
u x(u) to produce the coordinate
representation x(cid:48) of X relative to the new gauge. The underlying vector ﬁeld
remains unchanged:

s matrices. It acts on the gauge ωu : Rs

→
gu : Rs

→

→

◦

X(u) = ω(cid:48)

u(x(cid:48)(u)) = ωu(gug−1

u x(u)) = ωu(x(u)) = X(u),

which is exactly the property we desired. More generally, we may have a
ﬁeld of geometric quantities that transform according to a representation
ρ of GL(s), e.g. a ﬁeld of 2-tensors (matrices) A(u)
Rs×s that transform
u ). In this case, the gauge
like A(cid:48)(u) = ρ2(g−1
transformation gu acts via ρ(gu).

u )A(u) = ρ1(gu)A(u)ρ1(g−1

∈

Sometimes we may wish to restrict attention to frames with a certain property,
such as orthogonal frames, right-handed frames, etc. Unsurprisingly, we are
interested in a set of some property-preserving transformations that form a
group. For instance, the group that preserves orthogonality is the orthogonal
group O(s) (rotations and reﬂections), and the group that additionally

58

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

preserves orientation or ‘handedness’ is SO(s) (pure rotations). Thus, in
general we have a group G called the structure group of the bundle, and a
gauge transformation is a map g : Ω
G. A key observation is that in all
→
cases with the given property, for any two frames at a given point there exists
exactly one gauge transformation relating them.

We use s to denote the
dimension of the base space
Ω and d referring to the
dimension of the ﬁbre. For
tangent bundles, d = s is the
dimension of the underlying
manifold. For RGB image,
s = 2 and d = 3.

In this example we have
chosen G = Σ3 the
permutations of the 3 color
channels as the structure
group of the bundle. Other
choices, such as a Hue
rotation G = SO(2) are also
possible.

∈

∈

As mentioned before, gauge theory extends beyond tangent bundles, and
in general, we can consider a bundle of vector spaces whose structure and
dimensions are not necessarily related to those of the base space Ω.
For
instance, a color image pixel has a position u
Ω = Z2 on a 2D grid and
a value x(u)
R3 in the RGB space, so the space of pixels can be viewed
as a vector bundle with base space Z2 and a ﬁbre R3 attached at each point.
It is customary to express an RGB image relative to a gauge that has basis
vectors for R, G, and B (in that order), so that the coordinate representation
of the image looks like x(u) = (r(u), g(u), b(u))(cid:62). But we may equally well
permute the basis vectors (color channels) independently at each position,
as long as we remember the frame (order of channels) in use at each point
.
As a computational operation this is rather pointless, but as we will see
shortly it is conceptually useful to think about gauge transformations for the
space of RGB colors, because it allows us to express a gauge symmetry – in
this case, an equivalence between colors – and make functions deﬁned on
images respect this symmetry (treating each color equivalently).

(

X

∈ F

As in the case of a vector ﬁeld on a manifold, an RGB gauge transforma-
tion changes the numerical representation of an image (permuting the RGB
values independently at each pixel) but not the underlying image. In ma-
chine learning applications, we are interested in constructing functions
(Ω)) on such images (e.g. to perform image classiﬁcation or seg-
f
mentation), implemented as layers of a neural network. It follows that if, for
whatever reason, we were to apply a gauge transformation to our image, we
would need to also change the function f (network layers) so as to preserve
1 convolution, i.e. a map that
their meaning. Consider for simplicity a 1
takes an RGB pixel x(u)
RC. According
to our Geometric Deep Learning blueprint, the output is associated with
a group representation ρout, in this case a C-dimensional representation of
the structure group G = Σ3 (RGB channel permutations), and similarly the
input is associated with a representation ρin(g) = g. Then, if we apply a
gauge transformation to the input, we would need to change the linear map
(1
ρin(g) so that the output
feature vector y(u) = f (x(u)) transforms like y(cid:48)(u) = ρout(gu)y(u) at every

R3 to a feature vector y(u)

1 convolution) f : R3

RC to f (cid:48) = ρ−1

out(g)

→

×

×

∈

∈

◦

◦

f

4. GEOMETRIC DOMAINS: THE 5 GS

59

point. Indeed we verify:

Here the notation ρ−1(g)
should be understood as the
inverse of the group
representation (matrix) ρ(g).

y(cid:48) = f (cid:48)(x(cid:48)) = ρ−1

out(g)f (ρin(g)ρ−1

in (g)x) = ρ−1

out(g)f (x).

Gauge Symmetries To say that we consider gauge transformations to be
symmetries is to say that any two gauges related by a gauge transformation
are to be considered equivalent. For instance, if we take G = SO(d), any
two right-handed orthogonal frames are considered equivalent, because we
can map any such frame to any other such frame by a rotation. In other
words, there are no distinguished local directions such as “up” or “right”.
Similarly, if G = O(d) (the orthogonal group), then any left and right handed
orthogonal frame are considered equivalent. In this case, there is no preferred
orientation either. In general, we can consider a group G and a collection
of frames at every point u such that for any two of them there is a unique
g(u)

G that maps one frame onto the other.

∈

Regarding gauge transformations as symmetries in our Geometric Deep
Learning blueprint, we are interested in making the functions f acting on
signals deﬁned on Ω and expressed with respect to the gauge should equiv-
ariant to such transformation. Concretely, this means that if we apply a gauge
transformation to the input, the output should undergo the same transforma-
tion (perhaps acting via a diﬀerent representation of G). We noted before that
when we change the gauge, the function f should be changed as well, but for
a gauge equivariant map this is not the case: changing the gauge leaves the
mapping invariant. To see this, consider again the RGB color space example.
f , but in this case
RC is equivariant if f
The map f : R3
ρin(g) = ρout(g)
the gauge transformation applied to f has no eﬀect: ρ−1
ρin(g) = f .
In other words, the coordinate expression of a gauge equivariant map is
independent of the gauge, in the same way that in the case of graph, we
applied the same function regardless of how the input nodes were permuted.
However, unlike the case of graphs and other examples covered so far, gauge
transformations act not on Ω but separately on each of the feature vectors x(u)
by a transformation g(u)

G for each u

◦
out(g)

Ω.

→

◦

◦

◦

f

∈

∈

Further considerations enter the picture when we look at ﬁlters on manifolds
with a larger spatial support. Let us ﬁrst consider an easy example of a
(Ω, R) from scalar ﬁelds to scalar ﬁelds on an
mapping f :
→ X
s-dimensional manifold Ω. Unlike vectors and other geometric quantities,
scalars do not have an orientation, so a scalar ﬁeld x
(Ω, R) is invariant to
gauge transformations (it transforms according to the trivial representation

(Ω, R)

∈ X

X

60

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

ρ(g) = 1). Hence, any linear map from scalar ﬁelds to scalar ﬁelds is gauge
equivariant (or invariant, which is the same in this case). For example, we
could write f similarly to (19), as a convolution-like operation with a position-
dependent ﬁlter θ : Ω

R,

Ω

×

→

(x (cid:63) θ)(u) =

θ(u, v)x(v)dv.

(22)

) at each
This implies that we have a potentially diﬀerent ﬁlter θu = θ(u,
point, i.e., no spatial weight sharing — which gauge symmetry alone does
not provide.

·

(cid:90)Ω

X

∈ X

→ X

(Ω, T Ω)

(Ω, T Ω) are vector-valued functions x, y

Consider now a more interesting case of a mapping f :
(Ω, T Ω)
from vector ﬁelds to vector ﬁelds. Relative to a gauge, the input and output
vector ﬁelds X, Y
(Ω, Rs).
A general linear map between such functions can be written using the same
equation we used for scalars (22), only replacing the scalar kernel by a
matrix-valued one Θ : Ω
Rs×s. The matrix Θ(u, v) should map tan-
gent vectors in TvΩ to tangent vectors in TuΩ, but these points have diﬀerent
gauges that we may change arbitrarily and independently. That is, the ﬁlter
Ω,
would have to satisfy Θ(u, v) = ρ−1(g(u))Θ(u, v)ρ(g(v)) for all u, v
where ρ denotes the action of G on vectors, given by an s
s rotation matrix.
Since g(u) and g(v) can be chosen freely, this is an overly strong constraint
on the ﬁlter.

∈ X

→

×

×

Ω

∈

A better approach is to ﬁrst transport the vectors to a common tangent space
by means of the connection, and then impose gauge equivariance w.r.t. a
single gauge transformation at one point only. Instead of (22), we can then
deﬁne the following map between vector ﬁelds,

(x (cid:63) Θ)(u) =

Θ(u, v)ρ(gv→u)x(v)dv,

(23)

Indeed Θ would have to be
zero in this case

(cid:90)Ω

∈

G denotes the parallel transport from v to u along the geodesic
where gv→u
connecting these two points; its representation ρ(gv→u) is an s
s rotation
matrix rotating the vector as it moves between the points. Note that this
geodesic is assumed to be unique, which is true only locally and thus the
ﬁlter must have a local support. Under a gauge transformation gu, this el-
ement transforms as gu→v
u gu→vgv, and the ﬁeld itself transforms as
g−1
ρ(gv)x(v). If the ﬁlter commutes with the structure group represen-
x(v)
tation Θ(u, v)ρ(gu) = ρ(gu)Θ(u, v), equation (23) deﬁnes a gauge-equivariant
convolution, which transforms as

(cid:55)→

(cid:55)→

×

(x(cid:48) (cid:63) Θ)(u) = ρ−1(gu)(x (cid:63) Θ)(u).

4. GEOMETRIC DOMAINS: THE 5 GS

61

under the aforementioned transformation.

4.6 Geometric graphs and Meshes

We will conclude our discussion of diﬀerent geometric domains with geo-
metric graphs (i.e., graphs that can be realised in some geometric space) and
meshes. In our ‘5G’ of geometric domains, meshes fall somewhere between
graphs and manifolds: in many regards, they are similar to graphs, but their
additional structure allows to also treat them similarly to continuous objects.
For this reason, we do not consider meshes as a standalone object in our
scheme, and in fact, will emphasise that many of the constructions we derive
in this section for meshes are directly applicable to general graphs as well.

As we already mentioned in Section 4.4, two-dimensional manifolds (sur-
faces) are a common way of modelling 3D objects (or, better said, the bound-
ary surfaces of such objects). In computer graphics and vision applications,
such surfaces are often discretised as triangular meshes,
which can be roughly
thought of as a piece-wise planar approximation of a surface obtained by
gluing triangles together along their edges. Meshes are thus (undirected)
graphs with additional structure: in addition to nodes and edges, a mesh
) also have ordered triplets of nodes forming triangular faces
; the order of the

and (u, v), (u, q), (q, v)

,
V
(u, v, q) : u, v, q
{

T
F
nodes deﬁnes the face orientation.

= (
=

∈ E}

∈ V

F

E

,

It is further assumed that that each edge is shared by exactly two triangles,
and the boundary of all triangles incident on each node forms a single loop
of edges. This condition guarantees that 1-hop neighbourhoods around
each node are disk-like and the mesh thus constitutes a discrete manifold
– such meshes are referred to as manifold meshes. Similarly to Riemannian
manifolds, we can deﬁne a metric on the mesh. In the simplest instance, it can
be induced from the embedding of the mesh nodes x1, . . . , xn and expressed
. A metric deﬁned
through the Euclidean length of the edges, (cid:96)uv =
(cid:107)
in this way automatically satisﬁes properties such as the triangle inequality,
i.e., expressions of the form (cid:96)uv
and any
combination of edges. Any property that can be expressed solely in terms
of (cid:96) is intrinsic, and any deformation of the mesh preserving (cid:96) is an isometry
– these notions are already familiar to the reader from our discussion in
Section 4.4.

(cid:107)
(cid:96)uq + (cid:96)vq for any (u, v, q)

∈ F

xu

xv

≤

−

Triangular meshes are
examples of topological
structures known as simplicial
complexes.

Examples of manifold (top)
and non-manifold (bottom)
edges and nodes. For
manifolds with boundary,
one further deﬁnes boundary
edges that belong to exactly
one triangle.

62

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Laplacian matrices By analogy to our treatment of graphs, let us assume a
(manifold) mesh with n nodes, each associated with a d-dimensional feature
vector, which we can arrange (assuming some arbitrary ordering) into an
d matrix X. The features can represent the geometric coordinates of
n
the nodes as well as additional properties such as colors, normals, etc, or
in speciﬁc applications such as chemistry where geometric graphs model
molecules, properties such as the atomic number.

×

Let us ﬁrst look at the spectral convolution (17) on meshes, which we remind
the readers, arises from the Laplacian operator. Considering the mesh as
a discretisation of an underlying continuous surface, we can discretise the
Laplacian as

(∆X)u =

wuv(xu

xv),

(24)

−

n symmetric matrix ∆ = D

v∈Nu
(cid:88)
W,
or in matrix-vector notation, as an n
−
×
where D = diag(d1, . . . , dn) is called the degree matrix and du =
v wuv
the degree of node u. It is easy to see that equation (24) performs local
permutation-invariant aggregation of neighbour features φ(xu, XNu) =
v∈Nu wuvxv, and F(X) = ∆X is in fact an instance of our gen-
duxu
eral blueprint (13) for constructing permutation-equivariant functions on
graphs.

(cid:80)

(cid:80)

−

Note that insofar there is nothing speciﬁc to meshes in our deﬁnition of Lapla-
cian in (24); in fact, this construction is valid for arbitrary graphs as well,
with edge weights identiﬁed with the adjacency matrix, W = A, i.e., wuv = 1
and zero otherwise. Laplacians constructed in this way are often
if (u, v)
called combinatorial, to reﬂect the fact that they merely capture the connectiv-
ity structure of the graph.
For geometric graphs (which do not necessarily
have the additional structure of meshes, but whose nodes do have spatial
coordinates that induces a metric in the form of edge lengths), it is common
to use weights inversely related to the metric, e.g. wuv

e−(cid:96)uv .

∈ E

∝

On meshes, we can exploit the additional structure aﬀorded by the faces,
and deﬁne the edge weights in equation (24) using the cotangent formula
(Pinkall and Polthier, 1993; Meyer et al., 2003)

wuv =

cot ∠uqv + cot ∠upv
2au

(25)

where ∠uqv and ∠upv are the two angles in the triangles (u, q, v) and (u, p, v)
opposite the shared edge (u, v), and au is the local area element, typically

The degree in this case equals
the number of neighbours.

If the graph is directed, the
corresponding Laplacian is
non-symmetric.

The earliest use of this
formula dates back to the
PhD thesis of MacNeal
(1949), who developed it to
solve PDEs on the Caltech
Electric Analog Computer.

4. GEOMETRIC DOMAINS: THE 5 GS

63

computed as the area of the polygon constructed upon the barycenters of the
v,q:(u,v,q)∈F auvq.
triangles (u, p, q) sharing the node u and given by au = 1
3
The cotangent Laplacian can be shown to have multiple convenient prop-
erties (see e.g. Wardetzky et al. (2007)): it is a positive-semideﬁnite matrix,
∆ (cid:60) 0 and thus has non-negative eigenvalues λ1 ≤
λn that can be
regarded as an analogy of frequency, it is symmetric and thus has orthogonal
eigenvectors, and it is local (i.e., the value of (∆X)u depends only on 1-hop
neighbours,
u). Perhaps the most important property is the convergence
of the cotangent mesh Laplacian matrix ∆ to the continuous operator ∆
when the mesh is inﬁnitely reﬁned (Wardetzky, 2008). Equation (25) consti-
of the Laplacian operator deﬁned on
tutes thus an appropriate discretisation
Riemannian manifolds in Section 4.4.

. . .

(cid:80)

N

≤

While one expects the Laplacian to be intrinsic, this is not very obvious from
equation (25), and it takes some eﬀort to express the cotangent weights
entirely in terms of the discrete metric (cid:96) as

wuv = −

vq + (cid:96)2
uq

uv + (cid:96)2
(cid:96)2
8auvq

+ −

vp + (cid:96)2
up

uv + (cid:96)2
(cid:96)2
8auvp

where the area of the triangles aijk is given as

auvq =

suvq(suvq

(cid:96)uv)(suvq

(cid:96)vq)(suvq

(cid:96)uq)

−

−

−

(cid:113)

using Heron’s semiperimeter formula with suvq = 1
2 ((cid:96)uv + (cid:96)uq + (cid:96)vq). This
endows the Laplacian (and any quantities associated with it, such as its
eigenvectors and eigenvalues) with isometry invariance, a property for which
it is so loved in geometry processing and computer graphics (see an excellent
review by Wang and Solomon (2019)): any deformation of the mesh that
does not aﬀect the metric (cid:96) (does not ‘stretch’ or ‘squeeze’ the edges of the
mesh) does not change the Laplacian.

Some technical conditions
must be imposed on the
reﬁnement, to avoid e.g.
triangles becoming
pathological. One such
example is a bizarre
triangulation of the cylinder
known in German as the

Schwarzscher Stiefel
(Schwarz’s boot) or in
English literature as the
‘Schwarz lantern’, proposed
in 1880 by Hermann Schwarz,
a German mathematician
known from the
Cauchy-Schwarz inequality
fame.

Finally, as we already noticed,
the deﬁnition of the Laplacian (25) is invariant
u, as it involves aggregation in the form
to the permutation of nodes in
of summation. While on general graphs this is a necessary evil due to the
lack of canonical ordering of neighbours, on meshes we can order the 1-hop
neighbours according to some orientation (e.g., clock-wise), and the only
ambiguity is the selection of the ﬁrst node. Thus, instead of any possible
permutation we need to account for cyclic shifts (rotations), which intuitively
corresponds to the ambiguity arising from SO(2) gauge transformations

N

Laplacian-based ﬁlters are
isotropic. In the plane, such
ﬁlters have radial symmetry.

64

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

discussed in Section 4.5. For a ﬁxed gauge, it is possible to deﬁne an anisotropic
Laplacian that is sensitive to local directions and amounts to changing the
metric or the weights wuv. Constructions of this kind were used to design
shape descriptors by Andreux et al. (2014); Boscaini et al. (2016b) and in
early Geometric Deep Learning architectures on meshes by Boscaini et al.
(2016a).

Spectral analysis on meshes The orthogonal eigenvectors Φ = (ϕ1, . . . , ϕn)
diagonalising the Laplacian matrix (∆ = ΦΛΦ(cid:62), where Λ = diag(λ1, . . . , λn)
is the diagonal matrix of Laplacian eigenvalues), are used as the non-Euclidean
analogy of the Fourier basis, allowing to perform spectral convolution on
the mesh as the product of the respective Fourier transforms,

X (cid:63) θ = Φ diag(Φ(cid:62)θ)(Φ(cid:62)X) = Φ diag( ˆθ) ˆX,

where the ﬁlter ˆθ is designed directly in the Fourier domain. Again, nothing
in this formula is speciﬁc to meshes, and one can use the Laplacian matrix of
a generic (undirected) graph.
It is tempting to exploit this spectral deﬁnition
of convolution to generalise CNNs to graphs, which in fact was done by
one of the authors of this text, Bruna et al. (2013). However, it appears that
the non-Euclidean Fourier transform is extremely sensitive to even minor
perturbations of the underlying mesh or graph (see Figure 12 in Section 4.4)
and thus can only be used when one has to deal with diﬀerent signals on a
ﬁxed domain, but not when one wishes to generalise across diﬀerent domains.
Unluckily, many computer graphics and vision problems fall into the latter
category, where one trains a neural network on one set of 3D shapes (meshes)
and test on a diﬀerent set, making the Fourier transform-based approach
inappropriate.

As noted in Section 4.4, it is preferable to use spectral ﬁlters of the form (18)
applying some transfer function ˆp(λ) to the Laplacian matrix,

ˆp(∆)X = Φˆp(Λ)Φ(cid:62)X = Φ diag(ˆp(λ1), . . . , ˆp(λn)) ˆX.

When ˆp can be expressed in terms of matrix-vector products, the eigende-
composition of the n
can be avoided altogether. For example,
Deﬀerrard et al. (2016) used polynomials of degree r as ﬁlter functions,

n matrix ∆

×

ˆp(∆)X =

r

k=0
(cid:88)

αk∆kX = α0X + α1∆X + . . . + αr∆rX,

The fact that the graph is
assumed to be undirected is
important: in this case the
Laplacian is symmetric and
has orthogonal eigenvectors.

In the general case, the
complexity of
eigendecomposition is O(n3).

4. GEOMETRIC DOMAINS: THE 5 GS

65

d feature matrix X by the n
amounting to the multiplication of the n
n
×
Laplacian matrix r times. Since the Laplacian is typically sparse (with
)
(
|E|
O
non-zero elements)
).
this operation has low complexity of
(
|E|
∼ O
Furthermore, since the Laplacian is local, a polynomial ﬁlter of degree r is
localised in r-hop neighbourhood.

(
|E|

dr)

O

×

However, this exact property comes at a disadvantage when dealing with
meshes, since the actual support of the ﬁlter (i.e., the radius it covers) de-
pends on the resolution of the mesh. One has to bear in mind that meshes
arise from the discretisation of some underlying continuous surface, and
one may have two diﬀerent meshes
(cid:48) representing the same object.
T
In a ﬁner mesh, one might have to use larger neighbourhoods (thus, larger
degree r of the ﬁlter) than in a coarser one.

and

T

For this reason, in computer graphics applications it is more common to use
rational ﬁlters, since they are resolution-independent. There are many ways
to deﬁne such ﬁlters (see, e.g. Patanè (2020)), the most common being as a
. More generally, one can use
polynomial of some rational function, e.g., λ−1
λ+1
that maps the real line
a complex function, such as the Cayley transform
into the unit circle in the complex plane.
Levie et al. (2018) used spectral
ﬁlters expressed as Cayley polynomials, real rational functions with complex
coeﬃcients αl

λ−i
λ+i

C,

∈

ˆp(λ) = Re

r

(cid:32)

l=0
(cid:88)

λ
i
−
λ + i

l

.

(cid:33)

(cid:19)

αl

(cid:18)

When applied to matrices, the computation of the Cayley polynomial requires
matrix inversion,

ˆp(∆) = Re

r

(cid:32)

l=0
(cid:88)

αl(∆

−

iI)l(∆ + iI)−l

,

(cid:33)

which can be carried out approximately with linear complexity. Unlike
polynomial ﬁlters, rational ﬁlters do not have a local support, but have
exponential decay (Levie et al., 2018). A crucial diﬀerence compared to the
direct computation of the Fourier transform is that polynomial and rational
ﬁlters are stable under approximate isometric deformations of the underlying
graph or mesh – various results of this kind were shown e.g. by Levie et al.
(2018, 2019); Gama et al. (2020); Kenlay et al. (2021).

Meshes are nearly-regular
graphs, with each node
having O(1) neighbours,
resulting in O(n) non-zeros
in ∆.

Two-hop neighbourhoods on
meshes of diﬀerent
resolution.

Cayley transform is a
particular case of a Möbius
transformation. When applied
to the Laplacian (a
positive-semindeﬁnite
matrix), it maps its
non-negative eigenvalues to
the complex half-circle.

In signal processing,
polynomial ﬁlters are termed
ﬁnite impulse response (FIR),
whereas rational ﬁlters are
inﬁnite impulse response (IIR).

66

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Meshes as operators and Functional maps The paradigm of functional
maps suggests thinking of meshes as operators. As we will show, this allows
obtaining more interesting types of invariance exploiting the additional
structure of meshes. For the purpose of our discussion, assume the mesh
T
is constructed upon embedded nodes with coordinates X. If we construct
an intrinsic operator like the Laplacian, it can be shown that it encodes
completely the structure of the mesh, and one can recover the mesh (up to its
isometric embedding, as shown by Zeng et al. (2012)). This is also true for
some other operators (see e.g. Boscaini et al. (2015); Corman et al. (2017);
Chern et al. (2018)), so we will assume a general operator, or n
n matrix
Q(

, X), as a representation of our mesh.

×

T

In this view, the discussion of Section 4.1 of learning functions of the form
) can be rephrased as learning functions of the form f (Q). Similar to
f (X,
graphs and sets, the nodes of meshes also have no canonical ordering, i.e.,
functions on meshes must satisfy the permutation invariance or equivariance
conditions,

T

f (Q) = f (PQP(cid:62))
PF(Q) = F(PQP(cid:62))
for any permutation matrix P. However, compared to general graphs we
now have more structure: we can assume that our mesh arises from the
discretisation of some underlying continuous surface Ω. It is thus possible
(cid:48)) with n(cid:48) nodes and coordinates X(cid:48)
to have a diﬀerent mesh
(cid:48),
(cid:48),
V
representing the same object Ω as
(cid:48) can
. Importantly, the meshes
have a diﬀerent connectivity structure and even diﬀerent number of nodes
(n(cid:48)
= n). Therefore, we cannot think of these meshes as isomorphic graphs
with mere reordering of nodes and consider the permutation matrix P as
correspondence between them.

(cid:48) = (

and

E
T

F

T

T

T

Functional maps were introduced by Ovsjanikov et al. (2012) as a gener-
alisation of the notion of correspondence to such settings, replacing the
correspondence between points on two domains (a map η : Ω
Ω(cid:48)) with
→
(Ω(cid:48)), see Figure 13).
correspondence between functions (a map C :
A functional map is a linear operator C, represented as a matrix n(cid:48)
n, estab-
lishing correspondence between signals x(cid:48) and x on the respective domains
as

→ X

(Ω)

×

X

x(cid:48) = Cx.
Rustamov et al. (2013) showed that in order to guarantee area-preserving
mapping, the functional map must be orthogonal, C(cid:62)C = I, i.e., be an

In most cases the functional
map is implemented in the
spectral domain, as a k × k
map ˆC between the Fourier
coeﬃcients, x(cid:48) = Φ(cid:48) ˆCΦ(cid:62)x,
where Φ and Φ(cid:48) are the
respective n × k and n(cid:48) × k
matrices of the (truncated)
Laplacian eigenbases, with
k (cid:28) n, n(cid:48).

(cid:54)
4. GEOMETRIC DOMAINS: THE 5 GS

67

element of the orthogonal group C
map using C−1 = C(cid:62).

∈

O(n). In this case, we can invert the

Figure 13: Pointwise map (left) vs functional map (right).

The functional map also establishes a relation between the operator repre-
sentation of meshes,

Q(cid:48) = CQC(cid:62),

Q = C(cid:62)Q(cid:48)C,

Note that we read these
operations right-to-left.

This follows from the
orthogonality of permutation
matrices, P(cid:62)P = I.

which we can interpret as follows: given an operator representation Q of
and a functional map C, we can construct its representation Q(cid:48) of
mapping the signal from
then mapping back to
remeshing invariant (or equivariant) functions on meshes, satisfying

T
(cid:48) by ﬁrst
(using C(cid:62)), applying the operator Q, and
This leads us to a more general class of

(cid:48) to
T
(cid:48) (using C)

T

T

T

f (Q) = f (CQC(cid:62)) = f (Q(cid:48))
CF(Q) = F(CQC(cid:62)) = F(Q(cid:48))

for any C
invariance and equivariance is a particular case,
a trivial remeshing in which only the order of nodes is changed.

O(n). It is easy to see that the previous setting of permutation
which can be thought of as

∈

Wang et al. (2019a) showed that given an eigendecomposition of the opera-
tor Q = VΛV(cid:62), any remeshing invariant (or equivariant) function can
be expressed as f (Q) = f (Λ) and F(Q) = VF(Λ), or in other words,
remeshing-invariant functions involve only the spectrum of Q. Indeed, func-
tions of Laplacian eigenvalues have been proven in practice to be robust to
surface discretisation and perturbation, explaining the popularity of spectral
constructions based on Laplacians in computer graphics, as well as in deep
learning on graph (Deﬀerrard et al., 2016; Levie et al., 2018). Since this result
refers to a generic operator Q, multiple choices are available besides the
ubiquitous Laplacian – notable examples include the Dirac (Liu et al., 2017;
Kostrikov et al., 2018) or Steklov (Wang et al., 2018) operators, as well as
learnable parametric operators (Wang et al., 2019a).

68

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

5 Geometric Deep Learning Models

Having thoroughly studied various instantiations of our Geometric Deep
Learning blueprint (for diﬀerent choices of domain, symmetry group, and
notions of locality), we are ready to discuss how enforcing these prescriptions
can yield some of the most popular deep learning architectures.

Our exposition, once again, will not be in strict order of generality. We ini-
tially cover three architectures for which the implementation follows nearly-
directly from our preceding discussion: convolutional neural networks
(CNNs), group-equivariant CNNs, and graph neural networks (GNNs).

We will then take a closer look into variants of GNNs for cases where a
graph structure is not known upfront (i.e. unordered sets), and through
our discussion we will describe the popular Deep Sets and Transformer
architectures as instances of GNNs.

Following our discussion on geometric graphs and meshes, we ﬁrst describe
equivariant message passing networks, which introduce explicit geometric
symmetries into GNN computations. Then, we show ways in which our
theory of geodesics and gauge symmetries can be materialised within deep
learning, recovering a family of intrinsic mesh CNNs (including Geodesic
CNNs, MoNet and gauge-equivariant mesh CNNs).

Lastly, we look back on the grid domain from a temporal angle. This discus-
sion will lead us to recurrent neural networks (RNNs). We will demonstrate
a manner in which RNNs are translation equivariant over temporal grids,
but also study their stability to time warping transformations. This property
is highly desirable for properly handling long-range dependencies, and en-
forcing class invariance to such transformations yields exactly the class of
gated RNNs (including popular RNN models such as the LSTM or GRU).

While we hope the above canvasses most of the key deep learning archi-
tectures in use at the time of writing, we are well aware that novel neural
network instances are proposed daily. Accordingly, rather than aiming to
cover every possible architecture, we hope that the following sections are
illustrative enough, to the point that the reader is able to easily categorise any
future Geometric Deep Learning developments using the lens of invariances
and symmetries.

5. GEOMETRIC DEEP LEARNING MODELS

69

Figure 14: The process of convolving an image x with a ﬁlter C(θ). The ﬁlter
parameters θ can be expressed as a linear combination of generators θvw.

5.1 Convolutional Neural Networks

Convolutional Neural Networks are perhaps the earliest and most well
known example of deep learning architectures following the blueprint of
Geometric Deep Learning outlined in Section 3.5. In Section 4.2 we have
fully characterised the class of linear and local translation equivariant op-
. Let
erators, given by convolutions C(θ)x = x (cid:63) θ with a localised ﬁlter θ
us ﬁrst focus on scalar-valued (‘single-channel’ or ‘grayscale’) discretised
images, where the domain is the grid Ω = [H]
[W ] with u = (u1, u2) and
x

(Ω, R).

×

∈ X

Recall, C(θ) is a circulant
matrix with parameters θ.

Any convolution with a compactly supported ﬁlter of size H f
W f can
×
be written as a linear combination of generators θ1,1, . . . , θH f ,W f , given for
w). Any local linear
example by the unit peaks θvw(u1, u2) = δ(u1 −
equivariant map is thus expressible as

v, u2 −

H f

W f

F(x) =

αvwC(θvw)x ,

v=1
(cid:88)

w=1
(cid:88)

(26)

which, in coordinates, corresponds to the familiar 2D convolution (see Figure
14 for an overview):

Note that we usually imagine
x and θvw as 2D matrices, but
in this equation, both x and
θvw have their two coordinate
dimensions ﬂattened into
one—making x a vector, and
C(θvw) a matrix.

H f

W f

F(x)uv =

αabxu+a,v+b .

a=1
(cid:88)

b=1
(cid:88)

(27)

0111000001110000011100001100001100001100001100000x?101010101C(θ)z}|{θ11+θ13+θ22+θ31+θ33=1434112433123411331133110x?θ101010101×1×0×1×0×1×0×1×0×170

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Other choices of the basis θvw are also possible and will yield equivalent
operations (for potentially diﬀerent choices of αvw). A popular example are
directional derivatives: θvw(u1, u2) = δ(u1, u2)
= (0, 0)
δ(u1−
. In fact, directional
taken together with the local average θ0(u1, u2) = 1
derivatives can be considered a grid-speciﬁc analogue of diﬀusion processes
on graphs, which we recover if we assume each pixel to be a node connected
to its immediate neighbouring pixels in the grid.

v, u2−

w), (v, w)

Hf Wf

−

When the scalar input channel is replaced by multiple channels (e.g., RGB
colours, or more generally an arbitrary number of feature maps), the con-
volutional ﬁlter becomes a convolutional tensor expressing arbitrary linear
combinations of input features into output feature maps. In coordinates, this
can be expressed as

F(x)uvj =

H f

W f

M

a=1
(cid:88)

b=1
(cid:88)

c=1
(cid:88)

αjabcxu+a,v+b,c , j

[N ] ,

∈

(28)

where M and N are respectively the number of input and output channels.
This basic operation encompasses a broad class of neural network archi-
tectures, which, as we will show in the next section, have had a profound
impact across many areas of computer vision, signal processing, and beyond.
Here, rather than dissecting the myriad of possible architectural variants of
CNNs, we prefer to focus on some of the essential innovations that enabled
their widespread use.

Eﬃcient multiscale computation As discussed in the GDL template for
general symmetries, extracting translation invariant features out of the con-
volutional operator F requires a non-linear step.
Convolutional features are
processed through a non-linear activation function σ, acting element-wise on
the input—i.e., σ :
(Ω), as σ(x)(u) = σ(x(u)). Perhaps the most
popular example at the time of writing is the Rectiﬁed Linear Unit (ReLU):
σ(x) = max(x, 0). This non-linearity eﬀectively rectiﬁes the signals, pushing
their energy towards lower frequencies, and enabling the computation of
high-order interactions across scales by iterating the construction.

→ X

(Ω)

X

Already in the early works of Fukushima and Miyake (1982) and LeCun
et al. (1998), CNNs and similar architectures had a multiscale structure,
where after each convolutional layer (28) one performs a grid coarsening P :
(Ω(cid:48)), where the grid Ω(cid:48) has coarser resolution than Ω. This enables

(Ω)

X

→ X

ReLU, often considered a
‘modern’ architectural choice,
was already used in the
Neocognitron (Fukushima
and Miyake, 1982).
Rectiﬁcation is equivalent to
the principle of
demodulation, which is
fundamental in electrical
engineering as the basis for
many transmission protocols,
such as FM radio; and also
has a prominent role in
models for neuronal activity.

−3−2−101230123(cid:54)
5. GEOMETRIC DEEP LEARNING MODELS

71

multiscale ﬁlters with eﬀectively increasing receptive ﬁeld, yet retaining a
constant number of parameters per scale. Several signal coarsening strategies
P (referred to as pooling) may be used, the most common are applying a low-
pass anti-aliasing ﬁlter (e.g. local average) followed by grid downsampling,
or non-linear max-pooling.

In summary, a ‘vanilla’ CNN layer can be expressed as the composition of the
basic objects already introduced in our Geometric Deep Learning blueprint:

(29)

h = P(σ(F(x))) ,
i.e. an equivariant linear layer F, a coarsening operation P, and a non-
linearity σ. It is also possible to perform translation invariant global pooling
operations within CNNs. Intuitively, this involves each pixel—which, after
several convolutions, summarises a patch centered around it—proposing the
ﬁnal representation of the image
, with the ultimate choice being guided by a
form of aggregation of these proposals. A popular choice here is the average
function, as its outputs will retain similar magnitudes irrespective of the
image size (Springenberg et al., 2014).

Prominent examples following this CNN blueprint (some of which we will
discuss next) are displayed in Figure 15.

Deep and Residual Networks A CNN architecture, in its simplest form, is
therefore speciﬁed by hyperparameters (H f
k , Nk, pk)k≤K, with Mk+1 =
Nk and pk = 0, 1 indicating whether grid coarsening is performed or not.
While all these hyperparameters are important in practice, a particularly
important question is to understand the role of depth K in CNN architectures,
and what are the fundamental tradeoﬀs involved in choosing such a key
hyperparameter, especially in relation to the ﬁlter sizes (H f

k , W f

).

k , W f

k

While a rigorous answer to this question is still elusive, mounting empirical
evidence collected throughout the recent years suggests a favourable tradeoﬀ
towards deeper (large K) yet thinner (small (H f
. In this
context, a crucial insight by He et al. (2016) was to reparametrise each
convolutional layer to model a perturbation of the previous features, rather
than a generic non-linear transformation:

k )) models

k , W f

h = P (x + σ(F(x))) .
The resulting residual networks provide several key advantages over the
previous formulation. In essence, the residual parametrisation is consistent

(30)

CNNs which only consist of
the operations mentioned in
this paragraph are often
dubbed “all-convolutional”.
In contrast, many CNNs
ﬂatten the image across the
spatial axes and pass them to
an MLP classiﬁer, once
suﬃcient equivariant and
coarsening layers have been
applied. This loses
translation invariance.

Historically, ResNet models
are predated by highway
networks (Srivastava et al.,
2015), which allow for more
general gating mechanisms to
control the residual
information ﬂow.

72

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 15: Prominent examples of CNN architectures. Top-to-bottom:
LeNet (LeCun et al., 1998), AlexNet (Krizhevsky et al., 2012), ResNet (He
et al., 2016) and U-Net (Ronneberger et al., 2015). Drawn using the PlotNeu-
ralNet package (Iqbal, 2018).

1326281610112018410SOFT322496552562738413384132561314096140961000SOFT12828Input16283x3Conv16281628+162816281628+162816281628+1628322832281x13228BatchNormReLU+32281282323Averagepooling110SOFT6464I128128I/2256256I/4512512I/810241024I/16BottleneckConv512512512512I/8256256256256I/4128128128128I/264646464ISoftmax5. GEOMETRIC DEEP LEARNING MODELS

73

with the view that the deep network is a discretisation of an underlying
continuous dynamical system, modelled as an ordinary diﬀerential equation
(ODE)
. Crucially, learning a dynamical system by modeling its velocity turns
out to be much easier than learning its position directly. In our learning
setup, this translates into an optimisation landscape with more favorable
geometry, leading to the ability to train much deeper architectures than was
possible before. As will be discussed in future work, learning using deep
neural networks deﬁnes a non-convex optimisation problem, which can be
eﬃciently solved using gradient-descent methods under certain simplifying
regimes. The key advantage of the ResNet parametrisation has been rigor-
ously analysed in simple scenarios (Hardt and Ma, 2016), and remains an
active area of theoretical investigation. Finally, Neural ODEs (Chen et al.,
2018) are a recent popular architecture that pushes the analogy with ODEs
even further, by learning the parameters of the ODE ˙x = σ(F(x)) directly
and relying on standard numerical integration.

In this case, the ResNet is
performing a Forward Euler
discretisation of an ODE:
˙x = σ(F(x))

Normalisation Another important algorithmic innovation that boosted the
empirical performance of CNNs signiﬁcantly is the notion of normalisation.
In early models of neural activity, it was hypothesised that neurons perform
some form of local ‘gain control’, where the layer coeﬃcients xk are replaced
by ˜xk = σ−1
µk). Here, µk and σk encode the ﬁrst and second-
k (cid:12)
order moment information of xk, respectively. Further, they can be either
computed globally or locally.

(xk

−

In the context of Deep Learning, this principle was widely adopted through
the batch normalisation layer (Ioﬀe and Szegedy, 2015)
, followed by several
variants (Ba et al., 2016; Salimans and Kingma, 2016; Ulyanov et al., 2016;
Cooijmans et al., 2016; Wu and He, 2018). Despite some attempts to rigor-
ously explain the beneﬁts of normalisation in terms of better conditioned
optimisation landscapes (Santurkar et al., 2018), a general theory that can
provide guiding principles is still missing at the time of writing.

We note that normalising
activations of neural
networks has seen attention
even before the advent of
batch normalisation. See, e.g.,
Lyu and Simoncelli (2008).

Data augmentation While CNNs encode the geometric priors associated
with translation invariance and scale separation, they do not explicitly ac-
count for other known transformations that preserve semantic information,
e.g lightning or color changes, or small rotations and dilations. A pragmatic
approach to incorporate these priors with minimal architectural changes is
to perform data augmentation, where one manually performs said transfor-

74

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

mations to the input images and adds them into the training set.

Data augmentation has been empirically successful and is widely used—not
only to train state-of-the-art vision architectures, but also to prop up several
developments in self-supervised and causal representation learning (Chen
et al., 2020; Grill et al., 2020; Mitrovic et al., 2020). However, it is provably
sub-optimal in terms of sample complexity (Mei et al., 2021); a more eﬃcient
strategy considers instead architectures with richer invariance groups—as
we discuss next.

5.2 Group-equivariant CNNs

Recall that a homogeneous
space is a set Ω equipped
with a transitive group action,
meaning that for any u, v ∈ Ω
there exists g ∈ G such that
g.u = v.

As discussed in Section 4.3, we can generalise the convolution operation
from signals on a Euclidean space to signals on any homogeneous space Ω
acted upon by a group G
. By analogy to the Euclidean convolution where
a translated ﬁlter is matched with the signal, the idea of group convolution
is to move the ﬁlter around the domain using the group action, e.g. by
rotating and translating. By virtue of the transitivity of the group action,
we can move the ﬁlter to any position on Ω. In this section, we will discuss
several concrete examples of the general idea of group convolution, including
implementation aspects and architectural choices.

Discrete group convolution We begin by considering the case where the
domain Ω as well as the group G are discrete. As our ﬁrst example, we
consider medical volumetric images represented as signals of on 3D grids
with discrete translation and rotation symmetries. The domain is the 3D
cubical grid Ω = Z3 and the images (e.g. MRI or CT 3D scans) are modelled
(Ω). Although in practice such images
as functions x : Z3
have support on a ﬁnite cuboid [W ]
Z3, we instead prefer
×
to view them as functions on Z3 with appropriate zero padding. As our
symmetry, we consider the group G = Z3 (cid:111) Oh of distance- and orientation-
preserving transformations on Z3. This group consists of translations (Z3)
and the discrete rotations Oh generated by 90 degree rotations about the
three axes (see Figure 16).

R, i.e. x

∈ X

[H]

[D]

→

×

⊂

sequences made up of four letters:
As our second example, we consider DNA
C, G, A, and T. The sequences can be represented on the 1D grid Ω = Z as
R4, where each letter is one-hot coded in R4. Naturally, we
signals x : Z

→

DNA is a biopolymer
molecule made of four
repeating units called
nucleotides (Cytosine,
Guanine, Adenine, and
Thymine), arranged into two
strands coiled around each
other in a double helix,
where each nucleotide occurs
opposite of the
complementary one (base
pairs A/T and C/G).

5. GEOMETRIC DEEP LEARNING MODELS

75

3 ﬁlter, rotated by all 24 elements of the discrete rotation
Figure 16: A 3
group Oh, generated by 90-degree rotations about the vertical axis (red
arrows), and 120-degree rotations about a diagonal axis (blue arrows).

×

have a discrete 1D translation symmetry on the grid, but DNA sequences
have an additional interesting symmetry. This symmetry arises from the
way DNA is physically embodied as a double helix, and the way it is read by
the molecular machinery of the cell. Each strand of the double helix begins
with what is called the 5(cid:48)-end and ends with a 3(cid:48)-end, with the 5(cid:48) on one
strand complemented by a 3(cid:48) on the other strand. In other words, the two
Since the DNA molecule is always read
strands have an opposite orientation.
oﬀ starting at the 5(cid:48)-end, but we do not know which one, a sequence such as
ACCCTGG is equivalent to the reversed sequence with each letter replaced
by its complement, CCAGGGT. This is called reverse-complement symmetry
of the letter sequence. We thus have the two-element group Z2 =
0, 1
}
corresponding to the identity 0 and reverse-complement transformation 1
(and composition 1 + 1 = 0 mod 2). The full group combines translations
and reverse-complement transformations.

{

In our case, the group convolution (14) we deﬁned in Section 4.3 is given as

(x (cid:63) θ)(g) =

xuρ(g)θu,

(31)

u∈Ω
(cid:88)
the inner product between the (single-channel) input signal x and a ﬁlter θ
G via ρ(g)θu = θg−1u, and the output x (cid:63) θ is a function
transformed by g

∈

A schematic of the DNA’s
double helix structure, with
the two strands coloured in
blue and red. Note how the
sequences in the helices are
complementary and read in
reverse (from 5’ to 3’).

3’5’5’3’CGATTATACGCGTATAGCGCGCTA76

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

on G. Note that since Ω is discrete, we have replaced the integral from
equation (14) by a sum.

Transform+Convolve approach We will show that the group convolution
can be implemented in two steps: a ﬁlter transformation step, and a transla-
tional convolution step. The ﬁlter transformation step consists of creating
rotated (or reverse-complement transformed) copies of a basic ﬁlter, while
the translational convolution is the same as in standard CNNs and thus
eﬃciently computable on hardware such as GPUs. To see this, note that
G as a
in both of our examples we can write a general transformation g
H (e.g. a rotation or reverse-complement transformation)
transformation h
followed by a translation k
Zd, i.e. g = kh (with juxtaposition denoting
∈
the composition of the group elements k and h). By properties of the group
representation, we have ρ(g) = ρ(kh) = ρ(k)ρ(h). Thus,

∈

∈

(x (cid:63) θ)(kh) =

xuρ(k)ρ(h)θu

u∈Ω
(cid:88)

=

xu(ρ(h)θ)u−k

(32)

u∈Ω
(cid:88)
We recognise the last equation as the standard (planar Euclidean) convo-
lution of the signal x and the transformed ﬁlter ρ(h)θ. Thus, to implement
group convolution for these groups, we take the canonical ﬁlter θ, create
transformed copies θh = ρ(h)θ for each h
H (e.g. each rotation h
Oh
Z2), and then convolve x with
or reverse-complement DNA symmetry h
each of these ﬁlters: (x (cid:63) θ)(kh) = (x (cid:63) θh)(k). For both of our examples,
the symmetries act on ﬁlters by simply permuting the ﬁlter coeﬃcients, as
shown in Figure 16 for discrete rotations. Hence, these operations can be
implemented eﬃciently using an indexing operation with pre-computed
indices.

∈
∈

∈

While we deﬁned the feature maps output by the group convolution x (cid:63) θ
as functions on G, the fact that we can split g into h and k means that we
can also think of them as a stack of Euclidean feature maps (sometimes
called orientation channels), with one feature map per ﬁlter transformation /
orientation k. For instance, in our ﬁrst example we would associate to each
ﬁlter rotation (each node in Figure 16) a feature map, which is obtained by
convolving (in the traditional translational sense) the rotated ﬁlter. These
C array, where the number
feature maps can thus still be stored as a W

H

×

×

5. GEOMETRIC DEEP LEARNING MODELS

77

of channels C equals the number of independent ﬁlters times the number of
transformations h

H (e.g. rotations).

∈

As shown in Section 4.3, the group convolution is equivariant: (ρ(g)x) (cid:63) θ =
ρ(g)(x (cid:63) θ). What this means in terms of orientation channels is that under
the action of h, each orientation channel is transformed, and the orientation
channels themselves are permuted. For instance, if we associate one orien-
tation channel per transformation in Figure 16 and apply a rotation by 90
degrees about the z-axis (corresponding to the red arrows), the feature maps
will be permuted as shown by the red arrows. This description makes it clear
that a group convolutional neural network bears much similarity to a tradi-
tional CNN. Hence, many of the network design patterns discussed in the
Section 5.1, such as residual networks, can be used with group convolutions
as well.

Spherical CNNs in the Fourier domain For the continuous symmetry
group of the sphere that we saw in Section 4.3, it is possible to implement the
convolution in the spectral domain, using the appropriate Fourier transform
(we remind the reader that the convolution on S2 is a function on SO(3),
hence we need to deﬁne the Fourier transform on both these domains in
order to implement multi-layer spherical CNNs). Spherical harmonics are an
orthogonal basis on the 2D sphere, analogous to the classical Fourier basis
of complex exponential. On the special orthogonal group, the Fourier basis
is known as the Wigner D-functions. In both cases, the Fourier transforms
(coeﬃcients) are computed as the inner product with the basis functions,
and an analogy of the Convolution Theorem holds: one can compute the
convolution in the Fourier domain as the element-wise product of the Fourier
transforms. Furthermore, FFT-like algorithms exist for the eﬃcient compu-
tation of Fourier transform on S2 and SO(3). We refer for further details to
Cohen et al. (2018).

5.3 Graph Neural Networks

Graph Neural Networks (GNNs) are the realisation of our Geometric Deep
Learning blueprint on graphs leveraging the properties of the permutation
group. GNNs are among the most general class of deep learning architec-
tures currently in existence, and as we will see in this text, most other deep
learning architectures can be understood as a special case of the GNN with

78

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Figure 17: A visualisation of the dataﬂow for the three ﬂavours of GNN
layers, g. We use the neighbourhood of node b from Figure 10 to illustrate
this. Left-to-right: convolutional, where sender node features are multiplied
with a constant, cuv; attentional, where this multiplier is implicitly computed
via an attention mechanism of the receiver over the sender: αuv = a(xu, xv);
and message-passing, where vector-based messages are computed based
on both the sender and receiver: muv = ψ(xu, xv).

additional geometric structure.

As per our discussion in Section 4.1, we consider a graph to be speciﬁed
with an adjacency matrix A and node features X. We will study GNN
architectures that are permutation equivariant functions F(X, A) constructed
by applying shared permutation invariant functions φ(xu, XNu) over local
neighbourhoods. Under various guises, this local function φ can be referred
to as “diﬀusion”, “propagation”, or “message passing”, and the overall
computation of such F as a “GNN layer”.

The design and study of GNN layers is one of the most active areas of deep
learning at the time of writing, making it a landscape that is challenging to
navigate. Fortunately, we ﬁnd that the vast majority of the literature may be
derived from only three “ﬂavours” of GNN layers (Figure 17), which we will
present here. These ﬂavours govern the extent to which φ transforms the
neighbourhood features, allowing for varying degrees of complexity when
modelling interactions across the graph.

In all three ﬂavours, permutation invariance is ensured by aggregating fea-
(potentially transformed, by means of some function ψ)
tures from XNu
, and then updating the features
with some permutation-invariant function
of node u, by means of some function φ. Typically,
ψ and φ are learnable,
(cid:76)
is realised as a nonparametric operation such as sum, mean, or
whereas
maximum, though it can also be constructed e.g. using recurrent neural
networks (Murphy et al., 2018).

(cid:76)

Most commonly, ψ and φ are
learnable aﬃne
transformations with
activation functions; e.g.
ψ(x) = Wx + b;
φ(x, z) = σ (Wx + Uz + b),
where W, U, b are learnable
parameters and σ is an
activation function such as
the rectiﬁed linear unit. The
additional input of xu to φ
represents an optional
skip-connection, which is often
very useful.

cbacbccbdcbecbbConvolutionalxbxaxcxdxeαbaαbcαbdαbeαbbAttentionalxbxaxcxdxembambcmbdmbembbMessage-passingxbxaxcxdxe5. GEOMETRIC DEEP LEARNING MODELS

79

In the convolutional ﬂavour (Kipf and Welling, 2016a; Deﬀerrard et al.,
2016; Wu et al., 2019), the features of the neighbourhood nodes are directly
aggregated with ﬁxed weights,

hu = φ

xu,

cuvψ(xv)

.

(33)

(cid:32)

(cid:33)

v∈Nu
(cid:77)
Here, cuv speciﬁes the importance of node v to node u’s representation. It
is a constant that often directly depends on the entries in A representing
the structure of the graph. Note that when the aggregation operator
is chosen to be the summation, it can be considered as a linear diﬀusion
(cid:76)
In
or position-dependent linear ﬁltering, a generalisation of convolution.
particular, the spectral ﬁlters we have seen in Sections 4.4 and 4.6 fall under
this category, as they amount to applying ﬁxed local operators (e.g. the
Laplacian matrix) to node-wise signals.

In the attentional ﬂavour (Veličković et al., 2018; Monti et al., 2017; Zhang
et al., 2018), the interactions are implicit

hu = φ

xu,

(cid:32)

v∈Nu
(cid:77)

a(xu, xv)ψ(xv)

(cid:33)

.

(34)

Here, a is a learnable self-attention mechanism that computes the importance
coeﬃcients αuv = a(xu, xv) implicitly. They are often softmax-normalised
across all neighbours. When
is the summation, the aggregation is still a
linear combination of the neighbourhood node features, but now the weights
are feature-dependent.

(cid:76)

Finally, the message-passing ﬂavour (Gilmer et al., 2017; Battaglia et al.,
2018) amounts to computing arbitrary vectors (“messages”) across edges,

It is worthy to note that this
ﬂavour does not express every
GNN layer that is
convolutional (in the sense of
commuting with the graph
structure), but covers most
such approaches proposed in
practice. We will provide
detailed discussion and
extensions in future work.

hu = φ

xu,

ψ(xu, xv)

.

(35)

v∈Nu
(cid:77)
Here, ψ is a learnable message function, computing v’s vector sent to u, and the
aggregation can be considered as a form of message passing on the graph.

(cid:32)

(cid:33)

One important thing to note is a representational containment between these
approaches: convolution
message-passing. Indeed, attentional
⊆
GNNs can represent convolutional GNNs by an attention mechanism im-
plemented as a look-up table a(xu, xv) = cuv, and both convolutional and

attention

⊆

80

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

attentional GNNs are special cases of message-passing where the messages
are only the sender nodes’ features: ψ(xu, xv) = cuvψ(xv) for convolutional
GNNs and ψ(xu, xv) = a(xu, xv)ψ(xv) for attentional GNNs.

This does not imply that message passing GNNs are always the most use-
ful variant; as they have to compute vector-valued messages across edges,
they are typically harder to train and require unwieldy amounts of memory.
Further, on a wide range of naturally-occurring graphs, the graph’s edges
encode for downstream class similarity (i.e. an edge (u, v) implies that u
and v are likely to have the same output). For such graphs (often called
homophilous), convolutional aggregation across neighbourhoods is often a
far better choice, both in terms of regularisation and scalability. Attentional
GNNs oﬀer a “middle-ground”: they allow for modelling complex interac-
tions within neighbourhoods while computing only scalar-valued quantities
across the edges, making them more scalable than message-passing.

The “three ﬂavour” categorisation presented here is provided with brevity in
mind and inevitably neglects a wealth of nuances, insights, generalisations,
and historical contexts to GNN models. Importantly, it excludes higher-
dimensional GNN based on the Weisfeiler-Lehman hierarchy and spectral
GNNs relying on the explicit computation of the graph Fourier transform.

5.4 Deep Sets, Transformers, and Latent Graph Inference

We close the discussion on GNNs by remarking on permutation-equivariant
neural network architectures for learning representations of unordered sets.
While sets have the least structure among the domains we have discussed in
this text, their importance has been recently highlighted by highly-popular
architectures such as Transformers (Vaswani et al., 2017) and Deep Sets
(Zaheer et al., 2017). In the language of Section 4.1, we assume that we are
given a matrix of node features, X, but without any speciﬁed adjacency or
ordering information between the nodes. The speciﬁc architectures will arise
by deciding to what extent to model interactions between the nodes.

Empty edge set Unordered sets are provided without any additional struc-
ture or geometry whatsoever—hence, it could be argued that the most natural
way to process them is to treat each set element entirely independently. This
translates to a permutation equivariant function over such inputs, which

5. GEOMETRIC DEEP LEARNING MODELS

81

was already introduced in Section 4.1: a shared transformation applied to
every node in isolation. Assuming the same notation as when describing
GNNs (Section 5.3), such models can be represented as

hu = ψ(xu),

where ψ is a learnable transformation. It may be observed that this is a
special case of a convolutional GNN with
—or, equivalently, A = I.
u
}
Such an architecture is commonly referred to as Deep Sets, in recognition
of the work of Zaheer et al. (2017) that have theoretically proved several
universal-approximation properties of such architectures. It should be noted
that the need to process unordered sets commonly arises in computer vision
and graphics when dealing with point clouds; therein, such models are known
as PointNets (Qi et al., 2017).

u =

N

{

Complete edge set While assuming an empty edge set is a very eﬃcient
construct for building functions over unordered sets, often we would expect
that elements of the set exhibit some form of relational structure—i.e., that
there exists a latent graph between the nodes. Setting A = I discards any such
structure, and may yield suboptimal performance. Conversely, we could
assume that, in absence of any other prior knowledge, we cannot upfront
exclude any possible links between nodes. In this approach we assume the
complete graph, A = 11(cid:62); equivalently,
. As we do not assume access
to any coeﬃcients of interaction, running convolutional-type GNNs over such
a graph would amount to:

u =

N

V

hu = φ

(cid:32)

xu,

ψ(xv)

v∈V
(cid:77)

,

(cid:33)

, and as such
where the second input,
makes the model equivalently expressive to ignoring that input altogether;
i.e. the A = I case mentioned above.

v∈V ψ(xv) is identical for all nodes u

(cid:76)

This is a direct consequence
of the permutation invariance
of (cid:76).

This motivates the use of a more expressive GNN ﬂavour, the attentional,

hu = φ

xu,

(cid:32)

v∈V
(cid:77)

a(xu, xv)ψ(xv)

(cid:33)

(36)

which yields the self-attention operator, the core of the Transformer archi-
tecture (Vaswani et al., 2017). Assuming some kind of normalisation over

82

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

It is also appropriate to apply
the message-passing ﬂavour.
While popular for physics
simulations and relational
reasoning (e.g. Battaglia et al.
(2016); Santoro et al. (2017)),
they have not been as widely
used as Transformers. This is
likely due to the memory
issues associated with
computing vector messages
over a complete graph, or the
fact that vector-based
messages are less
interpretable than the “soft
adjacency” provided by
self-attention.

the attentional coeﬃcients (e.g. softmax), we can constrain all the scalars
a(xu, xv) to be in the range [0, 1]; as such, we can think of self-attention as
inferring a soft adjacency matrix, auv = a(xu, xv), as a byproduct of gradient-
based optimisation for some downstream task.

The above perspective means that we can pose Transformers exactly as at-
tentional GNNs over a complete graph (Joshi, 2020).
However, this is in
apparent conﬂict with Transformers being initially proposed for modelling
sequences—the representations of hu should be mindful of node u’s position in
the sequence, which complete-graph aggregation would ignore. Transform-
ers address this issue by introducing positional encodings: the node features
xu are augmented to encode node u’s position in the sequence, typically as
samples from a sine wave whose frequency is dependent on u.

On graphs, where no natural ordering of nodes exists, multiple alternatives
were proposed to such positional encodings. While we defer discussing
these alternatives for later, we note that one promising direction involves a
realisation that the positional encodings used in Transformers can be directly
related to the discrete Fourier transform (DFT), and hence to the eigenvectors
of the graph Laplacian of a “circular grid”. Hence, Transformers’ positional
encodings are implicitly representing our assumption that input nodes are
connected in a grid. For more general graph structures, one may simply
use the Laplacian eigenvectors of the (assumed) graph—an observation
exploited by Dwivedi and Bresson (2020) within their empirically powerful
Graph Transformer model.

Inferred edge set Finally, one can try to learn the latent relational structure,
leading to some general A that is neither I nor 11(cid:62). The problem of inferring
a latent adjacency matrix A for a GNN to use (often called latent graph
inference) is of high interest for graph representation learning. This is due
to the fact that assuming A = I may be representationally inferior, and
A = 11(cid:62) may be challenging to implement due to memory requirements
and large neighbourhoods to aggregate over. Additionally, it is closest to the
“true” problem: inferring an adjacency matrix A implies detecting useful
structure between the rows of X, which may then help formulate hypotheses
such as causal relations between variables.

Unfortunately, such a framing necessarily induces a step-up in modelling
complexity. Speciﬁcally, it requires properly balancing a structure learning

5. GEOMETRIC DEEP LEARNING MODELS

83

objective (which is discrete, and hence challenging for gradient-based optimi-
sation) with any downstream task the graph is used for. This makes latent
graph inference a highly challenging and intricate problem.

5.5 Equivariant Message Passing Networks

In many applications of Graph Neural Networks, node features (or parts
thereof) are not just arbitrary vectors but coordinates of geometric entities.
This is the case, for example, when dealing with molecular graphs: the nodes
representing atoms may contain information about the atom type as well
as its 3D spatial coordinates. It is desirable to process the latter part of the
features in a manner that would transform in the same way as the molecule is
transformed in space, in other words, be equivariant to the Euclidean group
E(3) of rigid motions (rotations, translations, and reﬂections) in addition to
the standard permutation equivariance discussed before.

To set the stage for our (slightly simpliﬁed) analysis, we will make a distinc-
R3; the
tion between node features fu
latter are endowed with Euclidean symmetry structure. In this setting, an
equivariant layer explicitly transforms these two inputs separately, yielding
modiﬁed node features f (cid:48)
u

Rd and node spatial coordinates xu

and coordinates x(cid:48)
u

∈

∈

.

∈

Rx(cid:48)

u + b), whereas f (cid:48)
u

We can now state our desirable equivariance property, following the Geo-
metric Deep Learning blueprint. If the spatial component of the input is
transformed by g
E(3) (represented as ρ(g)x = Rx + b, where R is an
orthogonal matrix modeling rotations and reﬂections, and b is a translation
vector), the spatial component of the output transforms in the same way (as
remains invariant.
x(cid:48)
u (cid:55)→
Much like the space of permutation equivariant functions we discussed
before in the context of general graphs, there exists a vast amount of E(3)-
equivariant layers that would satisfy the constraints above—but not all of
these layers would be geometrically stable, or easy to implement. In fact, the
space of practically useful equivariant layers may well be easily described
by a simple categorisation, not unlike our “three ﬂavours” of spatial GNN
layers. One elegant solution was suggested by Satorras et al. (2021) in the

84

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

form of equivariant message passing. Their model operates as follows:

f (cid:48)
u = φ

fu,

(cid:32)

x(cid:48)
u = xu +

v∈Nu
(cid:77)

(xu

v(cid:54)=u
(cid:88)

ψf (fu, fv,

xu

(cid:107)

−

xv

(cid:107)

2)

,

(cid:33)

xv)ψc(fu, fv,

−

xu
(cid:107)

−

xv

2)
(cid:107)

where ψf and ψc are two distinct (learnable) functions. It can be shown that
such an aggregation is equivariant under Euclidean transformations of the
on
spatial coordinates. This is due to the fact that the only dependence of f (cid:48)
u
xu is through the distances
2, and the action of E(3) necessarily
xv
(cid:107)
leaves distances between nodes unchanged. Further, the computations of
such a layer can be seen as a particular instance of the “message-passing”
GNN ﬂavour, hence they are eﬃcient to implement.

xu

−

(cid:107)

To summarise, in contrast to ordinary GNNs, Satorras et al. (2021) enable
the correct treatment of ‘coordinates’ for each point in the graph. They
are now treated as a member of the E(3) group, which means the network
outputs behave correctly under rotations, reﬂections and translations of the
input.
The features, fu, however, are treated in a channel-wise manner and
still assumed to be scalars that do not change under these transformations.
This limits the type of spatial information that can be captured within such
a framework. For example, it may be desirable for some features to be
encoded as vectors—e.g. point velocities—which should change direction
under such transformations. Satorras et al. (2021) partially alleviate this issue
by introducing the concept of velocities in one variant of their architecture.
Velocities are a 3D vector property of each point which rotates appropriately.
However, this is only a small subspace of the general representations that
could be learned with an E(3) equivariant network. In general, node features
may encode tensors of arbitrary dimensionality that would still transform
according to E(3) in a well-deﬁned manner.

Hence, while the architecture discussed above already presents an elegant
equivariant solution for many practical input representations, in some cases
it may be desirable to explore a broader collection of functions that satisfy
the equivariance property. Existing methods dealing with such settings can
be categorised into two classes: irreducible representations (of which the pre-
viously mentioned layer is a simpliﬁed instance) and regular representations.
We brieﬂy survey them here, leaving detailed discussion to future work.

While scalar features
(heatmap) do not change
under rotations, vector
features (arrows) may
change direction. The simple
E(3) equivariant GNN given
before does not take this into
account.

90°5. GEOMETRIC DEEP LEARNING MODELS

85

Irreducible representations build on the ﬁnd-
Irreducible representations
ing that all elements of the roto-translation group can be brought into an
irreducible form: a vector that is rotated by a block diagonal matrix. Cru-
cially, each of those blocks is a Wigner D-matrix (the aforementioned Fourier
basis for Spherical CNNs). Approaches under this umbrella map from one
set of irreducible representations to another using equivariant kernels. To
ﬁnd the full set of equivariant mappings, one can then directly solve the
equivariance constraint over these kernels. The solutions form a linear com-
bination of equivariant basis matrices derived by Clebsch-Gordan matrices and
the spherical harmonics.

Early examples of the irreducible representations approach include Tensor
Field Networks (Thomas et al., 2018) and 3D Steerable CNNs (Weiler et al.,
2018), both convolutional models operating on point clouds. The SE(3)-
Transformer of Fuchs et al. (2020) extends this framework to the graph
domain, using an attentional layer rather than convolutional. Further, while
our discussion focused on the special case solution of Satorras et al. (2021),
we note that the motivation for rotation or translation equivariant predic-
tions over graphs had historically been explored in other ﬁelds, including
architectures such as Dynamic Graph CNN (Wang et al., 2019b) for point
clouds and eﬃcient message passing models for quantum chemistry, such
as SchNet (Schütt et al., 2018) and DimeNet (Klicpera et al., 2020).

Regular representations While the approach of irreducible representa-
tions is attractive, it requires directly reasoning about the underlying group
representations, which may be tedious, and only applicable to groups that
are compact. Regular representation approaches are more general, but come
with an additional computational burden – for exact equivariance they re-
quire storing copies of latent feature embeddings for all group elements

.

One promising approach in this space aims to observe equivariance to Lie
groups—through deﬁnitions of exponential and logarithmic maps—with the
promise of rapid prototyping across various symmetry groups. While Lie
groups are out of scope for this section, we refer the reader to two recent
successful instances of this direction: the LieConv of Finzi et al. (2020), and
the LieTransformer of Hutchinson et al. (2020).

The approaches covered in this section represent popular ways of processing
data on geometric graphs in an way that is explicitly equivariant to the un-

This approach was, in fact,
pioneered by the group
convolutional neural
networks we presented in
previous sections.

86

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

derlying geometry. As discussed in Section 4.6, meshes are a special instance
of geometric graphs that can be understood as discretisations of continuous
surfaces. We will study mesh-speciﬁc equivariant neural networks next.

5.6

Intrinsic Mesh CNNs

Meshes, in particular, triangular ones, are the ‘bread and butter’ of computer
graphics and perhaps the most common way of modeling 3D objects. The
remarkable success of deep learning in general and CNNs in computer vision
in particular has lead to a keen interest in the graphics and geometry pro-
cessing community around the mid-2010s
to construct similar architectures
for mesh data.

Examples of geodesic patches.
In order for the resulting
patch to be a topological disk,
its radius R must be smaller
than the injectivity radius.

Construction of discrete
geodesics on a mesh.

Geodesic patches Most of the architectures for deep learning on meshes
implement convolutional ﬁlters of the form (21) by discretising or approxi-
mating the exponential map and expressing the ﬁlter in a coordinate system
Ω from a point u = γ(0)
of the tangent plane. Shooting a geodesic γ : [0, T ]
to nearby point v = γ(T ) deﬁnes a local system of geodesic polar coordinates
(r(u, v), ϑ(u, v)) where r is the geodesic distance between u and v (length of
the geodesic γ) and ϑ is the angle between γ(cid:48)(0) and some local reference
direction. This allows to deﬁne a geodesic patch x(u, r, ϑ) = x(expu ˜ω(r, ϑ)),
where ˜ωu : [0, R]

TuΩ is the local polar frame.

[0, 2π)

→

×

→

discretised as a mesh, a geodesic is a poly-line that traverses
On a surface
the triangular faces. Traditionally, geodesics have been computed using the
Fast Marching algorithm Kimmel and Sethian (1998), an eﬃcient numerical
approximation of a nonlinear PDE called the eikonal equation encountered in
physical models of wave propagation in a medium. This scheme was adapted
by Kokkinos et al. (2012) for the computation of local geodesic patches and
later reused by Masci et al. (2015) for the construction of Geodesic CNNs, the
ﬁrst intrinsic CNN-like architectures on meshes.

Importantly, in the deﬁnition of the geodesic patch we have
Isotropic ﬁlters
ambiguity in the choice of the reference direction and the patch orientation.
This is exactly the ambiguity of the choice of the gauge, and our local system
of coordinates is deﬁned up to arbitrary rotation (or a shift in the angular
coordinate, x(u, r, ϑ + ϑ0)), which can be diﬀerent at every node. Perhaps

5. GEOMETRIC DEEP LEARNING MODELS

87

the most straightforward solution is to use isotropic ﬁlters of the form θ(r)
that perform a direction-independent aggregation of the neighbour features,

(x (cid:63) θ)(u) =

x(u, r, ϑ)θ(r)drdϑ.

R

2π

0
0 (cid:90)
(cid:90)

Spectral ﬁlters discussed in Sections 4.4–4.6 fall under this category: they
are based on the Laplacian operator, which is isotropic. Such an approach,
however, discards important directional information, and might fail to extract
edge-like features.

Fixed gauge An alternative, to which we have already alluded in Sec-
tion 4.4, is to ﬁx some gauge. Monti et al. (2017) used the principal cur-
vature directions: while this choice is not intrinsic and may ambiguous at
ﬂat points (where curvature vanishes) or uniform curvature (such as on a
perfect sphere), the authors showed that it is reasonable for dealing with
deformable human body shapes, which are approximately piecewise-rigid.
Later works, e.g. Melzi et al. (2019), showed reliable intrinsic construction
of gauges on meshes, computed as (intrinsic) gradients of intrinsic func-
tions. While such tangent ﬁelds might have singularities (i.e., vanish at some
points), the overall procedure is very robust to noise and remeshing.

Angular pooling Another approach, referred to as angular max pooling,
was used by Masci et al. (2015). In this case, the ﬁlter θ(r, ϑ) is anisotropic,
but its matching with the function is performed over all the possible rotations,
which are then aggregated:

(x (cid:63) θ)(u) = max

ϑ0∈[0,2π)

R

2π

x(u, r, ϑ)θ(r, ϑ + ϑ0)drdϑ.

0
0 (cid:90)
(cid:90)
Conceptually, this can be visualised as correlating geodesic patches with a
rotating ﬁlter and collecting the strongest responses.

On meshes, the continuous integrals can be discretised using a construction
referred to as patch operators (Masci et al., 2015). In a geodesic patch around
represented in the local polar coordinates as
node u, the neighbour nodes
(ruv, ϑuv), are weighted by a set of weighting functions w1(r, ϑ), . . . , wK(r, ϑ)
(shown in Figure 18 and acting as ‘soft pixels’) and aggregated,

u,

N

Typically multi-hop
neighbours are used.

(x (cid:63) θ)u =

K
k=1 wk
K
k=1 wk

(cid:80)

(cid:80)

v∈Nu(ruv, ϑuv)xv θk
v∈Nu(ruv, ϑuv)θk

(cid:80)

(cid:80)

88

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

(here θ1, . . . , θK are the learnable coeﬃcients of the ﬁlter). Multi-channel
features are treated channel-wise, with a family of appropriate ﬁlters. Masci
et al. (2015); Boscaini et al. (2016a) used pre-deﬁned weighting functions w,
while Monti et al. (2017) further allowed them to be learnable.

Figure 18: Left-to-right: examples of patch operators used in Geodesic CNN
(Masci et al., 2015), Anisotropic CNN (Boscaini et al., 2016b) and MoNet
(Monti et al., 2017), with the level sets of the weighting functions wk(r, ϑ)
shown in red.

Gauge-equivariant ﬁlters Both isotropic ﬁlters and angular max pooling
lead to features that are invariant to gauge transformations; they transform
according to the trivial representation ρ(g) = 1 (where g
SO(2) is a rotation
of the local coordinate frame). This point of view suggests another approach,
proposed by Cohen et al. (2019); de Haan et al. (2020) and discussed in
Section 4.5, where the features computed by the network are associated with
an arbitrary representation ρ of the structure group G (e.g. SO(2) or O(2)
of rotations or rotations+reﬂections of the coordinate frame, respectively).
Tangent vectors transform according to the standard representation ρ(g) = g.
As another example, the feature vector obtained by matching n rotated copies
of the same ﬁlter transforms by cyclic shifts under rotations of the gauge;
this is known as the regular representation of the cyclic group Cn.

∈

As discussed in Section 4.5, when dealing with such geometric features
(associated to a non-trivial representation), we must ﬁrst parallel transport
them to the same vector space before applying the ﬁlter. On a mesh, this can
be implemented via the following message passing mechanism described
by de Haan et al. (2020). Let xu
Rd be a d-dimensional input feature at
mesh node u. This feature is expressed relative to an (arbitrary) choice of
gauge at u, and is assumed to transform according to a representation ρin of

∈

5. GEOMETRIC DEEP LEARNING MODELS

89

G = SO(2) under rotations of the gauge. Similarly, the output features hu of
the mesh convolution are d(cid:48) dimensional and should transform according to
ρout (which can be chosen at will by the network designer).

By analogy to Graph Neural Networks, we can implement the gauge-equivariant
convolution (23) on meshes by sending messages from the neighbours
of u (and from u itself) to u:

N

u

hu = Θself xu +

Θneigh(ϑuv)ρ(gv→u)xv,

(37)

v∈Nu
(cid:88)

∈

Rd(cid:48)×d are learned ﬁlter matrices. The structure
where Θself, Θneigh(ϑuv)
group element gv→u
SO(2) denotes the eﬀect of parallel transport from v
to u, expressed relative to the gauges at u and v, and can be precomputed
for each mesh. Its action is encoded by a transporter matrix ρ(gv→u)
Rd×d.
The matrix Θneigh(ϑuv) depends on the angle ϑuv of the neighbour v to
the reference direction (e.g. ﬁrst axis of the frame) at u, so this kernel is
anisotropic: diﬀerent neighbours are treated diﬀerently.

∈

∈

(cid:55)→

As explained in Section 4.5, for h(u) to be a well-deﬁned geometric quantity, it
ρout(g−1(u))h(u) under gauge transformations.
should transform as h(u)
This will be the case when Θselfρin(ϑ) = ρout(ϑ)Θself for all ϑ
SO(2),
and Θneigh(ϑuv
ϑ)ρin(ϑ) = ρout(ϑ)Θneigh(ϑuv). Since these constraints
are linear, the space of matrices Θself and matrix-valued functions Θneigh
satisfying these constraints is a linear subspace, and so we can parameterise
them as a linear combination of basis kernels with learnable coeﬃcients:
i βiΘi
Θself =

self and Θneigh =

i αiΘi

neigh.

−

∈

(cid:80)

(cid:80)

5.7 Recurrent Neural Networks

Our discussion has thus far always assumed the inputs to be solely spatial
across a given domain. However, in many common use cases, the inputs can
also be considered sequential (e.g. video, text or speech). In this case, we
assume that the input consists of arbitrarily many steps, wherein at each step
(Ω(t)).
t we are provided with an input signal, which we represent as X(t)

∈ X

While in general the domain can evolve in time together with the signals
on it, it is typically assumed that the domain is kept ﬁxed across all the
t, i.e. Ω(t) = Ω. Here, we will exclusively focus on this case, but note that

Note that d is the feature
dimension and is not
necessarily equal to 2, the
dimension of the mesh.

Here we abuse the notation,
identifying 2D rotations with
angles ϑ.

Whether the domain is
considered static or dynamic
concerns time scales: e.g., a
road network does change
over time (as new roads are
built and old ones are
demolished), but
signiﬁcantly slower
compared to the ﬂow of
traﬃc. Similarly, in social
networks, changes in
engagement (e.g. Twitter
users re-tweeting a tweet)
happen at a much higher
frequency than changes in
the follow graph.

90

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

exceptions are common. Social networks are an example where one often has
to account for the domain changing through time, as new links are regularly
created as well as erased. The domain in this setting is often referred to as a
dynamic graph (Xu et al., 2020a; Rossi et al., 2020).

Often, the individual X(t) inputs will exhibit useful symmetries and hence
may be nontrivially treated by any of our previously discussed architectures.
Some common examples include: videos (Ω is a ﬁxed grid, and signals are a
sequence of frames); fMRI scans (Ω is a ﬁxed mesh representing the geometry
of the brain cortex, where diﬀerent regions are activated at diﬀerent times as
a response to presented stimuli); and traﬃc ﬂow networks (Ω is a ﬁxed graph
representing the road network, on which e.g. the average traﬃc speed is
recorded at various nodes).

Let us assume an encoder function f (X(t)) providing latent representations
at the level of granularity appropriate for the problem and respectful of the
symmetries of the input domain. As an example
, consider processing video
frames: that is, at each timestep, we are given a grid-structured input repre-
d matrix X(t), where n is the number of pixels (ﬁxed in time)
sented as an n
and d is the number of input channels (e.g. d = 3 for RGB frames). Further,
we are interested in analysis at the level of entire frames, in which case it
is appropriate to implement f as a translation invariant CNN, outputting a
k-dimensional representation z(t) = f (X(t)) of the frame at time-step t.

×

We are now left with the task of appropriately summarising a sequence of
vectors z(t) across all the steps. A canonical way to dynamically aggregate this
information in a way that respects the temporal progression of inputs and
also easily allows for online arrival of novel data-points, is using a Recurrent
Neural Network (RNN).
What we will show here is that RNNs are an interest-
ing geometric architecture to study in their own right, since they implement
a rather unusual type of symmetry over the inputs z(t).

SimpleRNNs At each step, the recurrent neural network computes an m-
dimensional summary vector h(t) of all the input steps up to and including
t. This (partial) summary is computed conditional on the current step’s
features and the previous step’s summary, through a shared update function,
Rm, as follows (see Figure 19 for a summary):
R : Rk

Rm

×

→

h(t) = R(z(t), h(t−1))

(38)

We do not lose generality in
our example; equivalent
analysis can be done e.g. for
node-level outputs on a
spatiotemporal graph; the
only diﬀerence is in the
choice of encoder f (which
will then be a permutation
equivariant GNN).

Note that the z(t) vectors can
be seen as points on a
temporal grid: hence,
processing them with a CNN
is also viable in some cases.
Transformers are also
increasingly popular models
for processing generic
sequential inputs.

5. GEOMETRIC DEEP LEARNING MODELS

91

Figure 19: Illustration of processing video input with RNNs. Each input
video frame X(t) is processed using a shared function f —e.g. a transla-
tion invariant CNN—into a ﬂat representation z(t). Then the RNN update
function R is iterated across these vectors, iteratively updating a summary
vector h(t) which summarises all the inputs up to and including z(t). The
computation is seeded with an initial summary vector h(0), which may be
either pre-determined or learnable.

h(0)RRRR...z(1)z(2)z(3)z(4)h(4)h(3)h(2)h(1)ffffX(1)X(2)X(3)X(4)h(1)h(2)h(3)...92

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

In spite of their name,
SimpleRNNs are remarkably
expressive. For example, it
was shown by Siegelmann
and Sontag (1995) that such
models are Turing-complete,
meaning that they can likely
represent any computation
we may ever be able to
execute on computers.

and, as both z(t) and h(t−1) are ﬂat vector representations, R may be most
easily expressed as a single fully-connected neural network layer (often
known as SimpleRNN

; see Elman (1990); Jordan (1997)):

h(t) = σ(Wz(t) + Uh(t−1) + b)

(39)

∈

Rk×m, U

Rm×m and b

where W
Rm are learnable parameters, and
∈
σ is an activation function. While this introduces loops in the network’s
computational graph, in practice the network is unrolled for an appropriate
number of steps, allowing for backpropagation through time (Robinson and
Fallside, 1987; Werbos, 1988; Mozer, 1989) to be applied.

∈

The summary vectors may then be appropriately leveraged for the down-
stream task—if a prediction is required at every step of the sequence, then a
shared predictor may be applied to each h(t) individually. For classifying
entire sequences, typically the ﬁnal summary, h(T ), is passed to a classiﬁer.
Here, T is the length of the sequence.

Specially, the initial summary vector is usually either set to the zero-vector,
i.e. h(0) = 0, or it is made learnable. Analysing the manner in which the
initial summary vector is set also allows us to deduce an interesting form of
translation equivariance exhibited by RNNs.

Note that this construction is
extendable to grids in higher
dimensions, allowing us to,
e.g., process signals living on
images in a scanline fashion.
Such a construction powered
a popular series of models,
such as the PixelRNN from
van den Oord et al. (2016b).

Translation equivariance in RNNs Since we interpret the individual steps
t as discrete time-steps, the input vectors z(t) can be seen as living on a one-
grid of time-steps. While it might be attractive to attempt ex-
dimensional
tending our translation equivariance analysis from CNNs here, it cannot be
done in a trivial manner.

To see why, let us assume that we have produced a new sequence z(cid:48)(t) = z(t+1)
by performing a left-shift of our sequence by one step. It might be tempting to
attempt showing h(cid:48)(t) = h(t+1), as one expects with translation equivariance;
however, this will not generally hold. Consider t = 1; directly applying and
expanding the update function, we recover the following:

h(cid:48)(1) = R(z(cid:48)(1), h(0)) = R(z(2), h(0))
h(2) = R(z(2), h(1)) = R(z(2), R(z(1), h(0)))

(40)

(41)

Hence, unless we can guarantee that h(0) = R(z(1), h(0)), we will not recover
translation equivariance. Similar analysis can then be done for steps t > 1.

5. GEOMETRIC DEEP LEARNING MODELS

93

Fortunately, with a slight refactoring of how we represent z, and for a suitable
choice of R, it is possible to satisfy the equality above, and hence demonstrate
a setting in which RNNs are equivariant to shifts. Our problem was largely
one of boundary conditions: the equality above includes z(1), which our left-
shift operation destroyed. To abstract this problem away, we will observe
how an RNN processes an appropriately left-padded sequence, ¯z(t), deﬁned
as follows:

¯z(t) =

0
z(t−t(cid:48))

(cid:40)

t(cid:48)
t
≤
t > t(cid:48)

Such a sequence now allows for left-shifting
by up to t(cid:48) steps without de-
stroying any of the original input elements. Further, note we do not need to
handle right-shifting separately; indeed, equivariance to right shifts naturally
follows from the RNN equations.

Note that equivalent analyses
will arise if we use a diﬀerent
padding vector than 0.

We can now again analyse the operation of the RNN over a left-shifted verson
of ¯z(t), which we denote by ¯z(cid:48)(t) = ¯z(t+1), as we did in Equations 40–41:

h(cid:48)(1) = R(¯z(cid:48)(1), h(0)) = R(¯z(2), h(0))
h(2) = R(¯z(2), h(1)) = R(¯z(2), R(¯z(1), h(0))) = R(¯z(2), R(0, h(0)))

where the substitution ¯z(1) = 0 holds as long as t(cid:48)
padding is applied
one step (h(cid:48)(t) = h(t+1)) as long as h(0) = R(0, h(0)).

1, i.e. as long as any
. Now, we can guarantee equivariance to left-shifting by

≥

Said diﬀerently, h(0) must be chosen to be a ﬁxed point of a function γ(h) =
R(0, h). If the update function R is conveniently chosen, then not only can
we guarantee existence of such ﬁxed points, but we can even directly obtain
them by iterating the application of R until convergence; e.g., as follows:

h0 = 0

hk+1 = γ(hk),

(42)

where the index k refers to the iteration of R in our computation, as opposed
to the the index (t) denoting the time step of the RNN. If we choose R
such that γ is a contraction mapping
, such an iteration will indeed converge
to a unique ﬁxed point. Accordingly, we can then iterate Equation (42)
until hk+1 = hk, and we can set h(0) = hk. Note that this computation is
equivalent to left-padding the sequence with “suﬃciently many” zero-vectors.

It is also easy to stack multiple RNNs—simply use the
Depth in RNNs
h(t) vectors as an input sequence for a second RNN. This kind of construc-
tion is occasionally called a “deep RNN”, which is potentially misleading.

In a very similar vein, we can
derive equivariance to
left-shifting by s steps as long
as t(cid:48) ≥ s.

Contractions are functions
γ : X → X such that, under
some norm (cid:107) · (cid:107) on X ,
applying γ contracts the
distances between points: for
all x, y ∈ X , and some
q ∈ [0, 1), it holds that
(cid:107)γ(x) − γ(y)(cid:107) ≤ q(cid:107)x − y(cid:107).
Iterating such a function then
necessarily converges to a
unique ﬁxed point, as a direct
consequence of Banach’s Fixed
Point Theorem (Banach, 1922).

94

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Eﬀectively, due to the repeated application of the recurrent operation, even
a single RNN “layer” has depth equal to the number of input steps.

This often introduces uniquely challenging learning dynamics when opti-
mising RNNs, as each training example induces many gradient updates to
the shared parameters of the update network. Here we will focus on perhaps
the most prominent such issue—that of vanishing and exploding gradients
(Bengio et al., 1994)—which is especially problematic in RNNs, given their
depth and parameter sharing. Further, it has single-handedly spurred some
of the most inﬂuential research on RNNs. For a more detailed overview,
we refer the reader to Pascanu et al. (2013), who have studied the training
dynamics of RNNs in great detail, and exposed these challenges from a
variety of perspectives: analytical, geometrical, and the lens of dynamical
systems.

To illustrate vanishing gradients, consider a SimpleRNN with a sigmoidal
activation function σ
is always between 0
and 1. Multiplying many such values results in gradients that quickly tend
to zero, implying that early steps in the input sequence may not be able to
have inﬂuence in updating the network parameters at all.

, whose derivative magnitude

σ(cid:48)

|

|

Examples of such an
activation include the logistic
,
function, σ(x) =
and the hyperbolic tangent,
σ(x) = tanh x. They are
called sigmoidal due to the
distinct S-shape of their plots.

1
1+exp(−x)

For example, consider the next-word prediction task (common in e.g. predic-
tive keyboards), and the input text “Petar is Serbian. He was born on . . . [long
”. Here, predicting the next word
paragraph] . . . Petar currently lives in
as “Serbia” may only be reasonably concluded by considering the very start
of the paragraph—but gradients have likely vanished by the time they reach
this input step, making learning from such examples very challenging.

Deep feedforward neural networks have also suﬀered from the vanishing
gradient problem, until the invention of the ReLU activation (which has
gradients equal to exactly zero or one—thus ﬁxing the vanishing gradient
problem). However, in RNNs, using ReLUs may easily lead to exploding
gradients, as the output space of the update function is now unbounded,
and gradient descent will update the cell once for every input step, quickly
building up the scale of the updates. Historically, the vanishing gradient
phenomenon was recognised early on as a signiﬁcant obstacle in the use of
recurrent networks. Coping with this problem motivated the development
of more sophisticated RNN layers, which we describe next.

−6−5−4−3−2−1012345600.20.40.60.815. GEOMETRIC DEEP LEARNING MODELS

95

Figure 20: The dataﬂow of the long short-term memory (LSTM), with its
components and memory cell (M ) clearly highlighted. Based on the current
input z(t), previous summary h(t−1) and previous cell state c(t−1), the LSTM
predicts the updated cell state c(t) and summary h(t).

5.8 Long Short-Term Memory networks

A key invention that signiﬁcantly reduced the eﬀects of vanishing gradients
in RNNs is that of gating mechanisms, which allow the network to selec-
tively overwrite information in a data-driven way. Prominent examples of
these gated RNNs include the Long Short-Term Memory (LSTM; Hochreiter
and Schmidhuber (1997)) and the Gated Recurrent Unit (GRU; Cho et al.
(2014)). Here we will primarily discuss the LSTM—speciﬁcally, the variant
presented by Graves (2013)—in order to illustrate the operations of such
models. Concepts from LSTMs easily carry over to other gated RNNs.

Throughout this section, it will likely be useful to refer to Figure 20, which
illustrates all of the LSTM operations that we will discuss in text.

The LSTM augments the recurrent computation by introducing a memory
cell, which stores cell state vectors, c(t)
Rm, that are preserved between
computational steps. The LSTM computes summary vectors, h(t), directly
based on c(t), and c(t) is, in turn, computed using z(t), h(t−1) and c(t−1).
Critically, the cell is not completely overwritten based on z(t) and h(t−1),
which would expose the network to the same issues as the SimpleRNN.
Instead, a certain quantity of the previous cell state may be retained—and

∈

Wc,Uc,bcWi,Ui,biWf,Uf,bfWo,Uo,boz(t)h(t−1)×+tanh×h(t)×Mec(t)c(t−1)i(t)o(t)f(t)c(t)LSTM96

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Note that we have set the
activation function to tanh
here; as LSTMs are designed
to ameliorate the vanishing
gradient problem, it is now
appropriate to use a
sigmoidal activation.

Note that the three gates are
themselves vectors, i.e.
i(t), f (t), o(t) ∈ [0, 1]m. This
allows them to control how
much each of the m
dimensions is allowed
through the gate.

This is still compatible with
the RNN update blueprint
from Equation (38); simply
consider the summary vector
to be the concatenation of h(t)
and c(t); sometimes denoted
by h(t)(cid:107)c(t).

the proportion by which this occurs is explicitly learned from data.

Just like in SimpleRNN, we compute features by using a single fully-connected
neural network layer over the current input step and previous summary:

c(t) = tanh(Wcz(t) + Uch(t−1) + bc)
But, as mentioned, we do not allow all of this vector to enter the cell—hence
c(t). Instead,
why we call it the vector of candidate features, and denote it as
the LSTM directly learns gating vectors, which are real-valued vectors in the
range [0, 1], and decide how much of the signal should be allowed to enter,
exit, and overwrite the memory cell.

(43)

(cid:101)

(cid:101)

Three such gates are computed, all based on z(t) and h(t−1): the input gate i(t),
which computes the proportion of the candidate vector allowed to enter the
cell; the forget gate f (t), which computes the proportion of the previous cell
state to be retained, and the output gate o(t), which computes the proportion
of the new cell state to be used for the ﬁnal summary vector. Typically all of
these gates are also derived using a single fully connected layer, albeit with
, in order to guarantee
the logistic sigmoid activation logistic(x) =
that the outputs are in the [0, 1] range

1
1+exp(−x)

:

i(t) = logistic(Wiz(t) + Uih(t−1) + bi)
f (t) = logistic(Wf z(t) + Uf h(t−1) + bf )
o(t) = logistic(Woz(t) + Uoh(t−1) + bo)

(44)

(45)

(46)

Finally, these gates are appropriately applied to decode the new cell state,
c(t), which is then modulated by the output gate to produce the summary
vector h(t), as follows:

c(t) = i(t)
h(t) = o(t)

c(t) + f (t)
(cid:12)
tanh(c(t))
(cid:101)

(cid:12)

(cid:12)

c(t−1)

(47)

(48)

where
is element-wise vector multiplication. Applied together, Equations
(43)–(48) completely specify the update rule for the LSTM, which now takes
into account the cell vector c(t) as well
:

(cid:12)

(h(t), c(t)) = R(z(t), (h(t−1), c(t−1)))
Note that, as the values of f (t) are derived from z(t) and h(t−1)—and therefore
directly learnable from data—the LSTM eﬀectively learns how to appropri-
ately forget past experiences. Indeed, the values of f (t) directly appear in

5. GEOMETRIC DEEP LEARNING MODELS

97

the backpropagation update for all the LSTM parameters (W∗, U∗, b∗), al-
lowing the network to explicitly control, in a data-driven way, the degree of
vanishing for the gradients across the time steps.

Besides tackling the vanishing gradient issue head-on, it turns out that
gated RNNs also unlock a very useful form of invariance to time-warping
transformations, which remains out of reach of SimpleRNNs.

Time warping invariance of gated RNNs We will start by illustrating, in a
continuous-time setting
, what does it mean to warp time, and what is required
of a recurrent model in order to achieve invariance to such transformations.
Our exposition will largely follow the work of Tallec and Ollivier (2018),
that initially described this phenomenon—and indeed, they were among
the ﬁrst to actually study RNNs from the lens of invariances.

We focus on the continuous
setting as it will be easier to
reason about manipulations
of time there.

Let us assume a continuous time-domain signal z(t), on which we would
like to apply an RNN. To align the RNN’s discrete-time computation of
summary vectors h(t)
with an analogue in the continuous domain, h(t), we
will observe its linear Taylor expansion:

h(t + δ)

h(t) + δ

≈

dh(t)
dt

(49)

and, setting δ = 1, we recover a relationship between h(t) and h(t + 1), which
is exactly what the RNN update function R (Equation 38) computes. Namely,
the RNN update function satisﬁes the following diﬀerential equation:

dh(t)
dt

= h(t + 1)

−

h(t) = R(z(t + 1), h(t))

h(t)

−

(50)

We will use h(t) to denote a
continuous signal at time t,
and h(t) to denote a discrete
signal at time-step t.

We would like the RNN to be resilient to the way in which the signal is
sampled (e.g. by changing the time unit of measurement), in order to
account for any imperfections or irregularities therein. Formally, we denote
R+, as any monotonically increasing
a time warping
diﬀerentiable mapping between times. The notation τ is chosen because
time warping represents an automorphism of time.

operation τ : R+

→

Further, we state that a class of models is invariant to time warping if, for any
model of the class and any such τ , there exists another (possibly the same)
model from the class that processes the warped data in the same way as the
original model did in the non-warped case.

Such warping operations can
be simple, such as time
rescaling; e.g. τ (t) = 0.7t
(displayed above), which, in
a discrete setting, would
amount to new inputs being
received every ∼ 1.43 steps.
However, it also admits a
wide spectrum of
variably-changing sampling
rates, e.g. sampling may
freely accelerate or decelerate
throughout the time domain.

98

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

This is a potentially very useful property. If we have an RNN class capable
of modelling short-term dependencies well, and we can also show that this
class is invariant to time warping, then we know it is possible to train such a
model in a way that will usefully capture long-term dependencies as well (as
they would correspond to a time dilation warping of a signal with short-term
dependencies). As we will shortly see, it is no coincidence that gated RNN
models such as the LSTM were proposed to model long-range dependencies.
Achieving time warping invariance is tightly coupled with presence of gating
mechanisms, such as the input/forget/output gates of LSTMs.

When time gets warped by τ , the signal observed by the RNN at time t
is z(τ (t)) and, to remain invariant to such warpings, it should predict an
equivalently-warped summary function h(τ (t)). Using Taylor expansion
arguments once more, we derive a form of Equation 50 for the warped time,
that the RNN update R should satisfy:

dh(τ (t))
dτ (t)

= R(z(τ (t + 1)), h(τ (t)))

h(τ (t))

−

(51)

However, the above derivative is computed with respect to the warped time
τ (t), and hence does not take into account the original signal. To make our
model take into account the warping transformation explicitly, we need to
diﬀerentiate the warped summary function with respect to t. Applying the
chain rule, this yields the following diﬀerential equation:

dh(τ (t))
dt

=

dh(τ (t))
dτ (t)

dτ (t)
dt

=

dτ (t)
dt

R(z(τ (t + 1)), h(τ (t)))

h(τ (t))

dτ (t)
dt

−

(52)
and, for our (continuous-time) RNN to remain invariant to any time warping
τ (t), it needs to be able to explicitly represent the derivative dτ (t)
, which
dt
is not assumed known upfront! We need to introduce a learnable function
Γ which approximates this derivative. For example, Γ could be a neural
network taking into account z(t + 1) and h(t) and predicting scalar outputs.

Now, remark that, from the point of view of a discrete RNN model under time
warping, its input z(t) will correspond to z(τ (t)), and its summary h(t) will
correspond to h(τ (t)). To obtain the required relationship of h(t) to h(t+1)
in order to remain invariant to time warping, we will use a one-step Taylor
expansion of h(τ (t)):

h(τ (t + δ))

h(τ (t)) + δ

≈

dh(τ (t))
dt

5. GEOMETRIC DEEP LEARNING MODELS

99

and, once again, setting δ = 1 and substituting Equation 52, then discretising:

h(t+1) = h(t) +

dτ (t)
dt

R(z(t+1), h(t))

−

=

dτ (t)
dt

R(z(t+1), h(t)) +

1
(cid:18)

−

dτ (t)
dt
dτ (t)
dt

h(t)

h(t)

(cid:19)

Finally, we swap dτ (t)
dt
gives us the required form for our time warping-invariant RNN:

with the aforementioned learnable function, Γ. This

h(t+1) = Γ(z(t+1), h(t))R(z(t+1), h(t)) + (1

Γ(z(t+1), h(t)))h(t)

(53)

−

We may quickly deduce that SimpleRNNs (Equation 39) are not time warping
invariant, given that they do not feature the second term in Equation 53.
Instead, they fully overwrite h(t) with R(z(t+1), h(t)), which corresponds to
assuming no time warping at all; dτ (t)

dt = 1, i.e. τ (t) = t.

Further, our link between continuous-time RNNs and the discrete RNN
based on R rested on the accuracy of the Taylor approximation, which holds
only if the time-warping derivative is not too large, i.e., dτ (t)
(cid:46) 1. The
dt
intuitive explanation of this is: if our time warping operation ever contracts
time in a way that makes time increments (t
t + 1) large enough that
intermediate data changes are not sampled, the model can never hope to
process time-warped inputs in the same way as original ones—it simply
would not have access to the same information. Conversely, time dilations of
any form (which, in discrete terms, correspond to interspersing the input
time-series with zeroes) are perfectly allowed within our framework.

→

Combined with our requirement of monotonically increasing τ ( dτ (t)
dt >
0), we can bound the output space of Γ as 0 < Γ(z(t+1), h(t)) < 1, which
motivates the use of the logistic sigmoid activation for Γ, e.g.:

Γ(z(t+1), h(t)) = logistic(WΓz(t+1) + UΓh(t) + bΓ)

exactly matching the LSTM gating equations (e.g. Equation 44). The main
diﬀerence is that LSTMs compute gating vectors, whereas Equation 53 implies
Γ should output a scalar. Vectorised gates (Hochreiter, 1991) allow to ﬁt a
diﬀerent warping derivative in every dimension of h(t), allowing for reasoning
over multiple time horizons simultaneously.

It is worth taking a pause here to summarise what we have done. By requiring
that our RNN class is invariant to (non-destructive) time warping, we have

100

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

One case where zero-shot
transfer is possible is when
the second time warping is
assumed to be a time rescaling
of the ﬁrst one
(τ2(t) = ατ1(t)).
Transferring a gated RNN
pre-trained on τ1 to a signal
warped by τ2 merely requires
rescaling the gates:
Γ2(z(t+1), h(t)) =
αΓ1(z(t+1), h(t)). R can
retain its parameters
(R1 = R2).

This insight was already
spotted by Gers and
Schmidhuber (2000);
Jozefowicz et al. (2015), who
empirically recommended
initialising the forget-gate
bias of LSTMs to a constant
positive vector, such as 1.

derived the necessary form that it must have (Equation 53), and showed that
it exactly corresponds to the class of gated RNNs. The gates’ primary role
under this perspective is to accurately ﬁt the derivative
of the warping
transformation.

dτ (t)
dt

The notion of class invariance is somewhat distinct from the invariances
we studied previously. Namely, once we train a gated RNN on a time-
it to a signal
warped input with τ1(t), we typically cannot zero-shot transfer
warped by a diﬀerent τ2(t). Rather, class invariance only guarantees that
gated RNNs are powerful enough to ﬁt both of these signals in the same
manner, but potentially with vastly diﬀerent model parameters. That being
said, the realisation that eﬀective gating mechanisms are tightly related to
ﬁtting the warping derivative can yield useful prescriptions for gated RNN
optimisation, as we now brieﬂy demonstrate.

For example, we can often assume that the range of the dependencies we are
interested in tracking within our signal will be in the range [Tl, Th] time-steps.

By analysing the analytic solutions to Equation 52, it can be shown that
the characteristic forgetting time of h(t) by our gated RNN is proportional to
. Hence, we would like our gating values to lie between

1
, 1
Tm
Γ(z(t+1),h(t))
in order to eﬀectively remember information within the assumed range.

1
Th

(cid:104)

(cid:105)

Further, if we assume that z(t) and h(t) are roughly zero-centered—which is
a common by-product of applying transformations such as layer normali-
sation (Ba et al., 2016)—we can assume that E[Γ(z(t+1), h(t))]
logistic(bΓ).
Controlling the bias vector of the gating mechanism is hence a very powerful
way of controlling the eﬀective gate value
.

≈

Combining the two observations, we conclude that an appropriate range of
gating values can be obtained by initialising bΓ ∼ −
1), where
is the uniform real distribution. Such a recommendation was dubbed
U
chrono initialisation by Tallec and Ollivier (2018), and has been empirically
shown to improve the long-range dependency modelling of gated RNNs.

(Tl, Th)

log(

−

U

Sequence-to-sequence learning with RNNs One prominent historical ex-
ample of using RNN-backed computation are sequence-to-sequence translation
tasks, such as machine translation of natural languages. The pioneering seq2seq
work by Sutskever et al. (2014) achieved this by passing the summary vector,

5. GEOMETRIC DEEP LEARNING MODELS

101

Figure 21: One typical example of a seq2seq architecture with an RNN
encoder Renc and RNN decoder Rdec. The decoder is seeded with the ﬁnal
summary vector h(T ) coming out of the encoder, and then proceeds in an
autoregressive fashion: at each step, the predicted output from the previous
step is fed back as input to Rdec. The bottleneck problem is also illustrated
with the red lines: the summary vector h(T ) is pressured to store all relevant
information for translating the input sequence, which becomes increasingly
challenging as the input length grows.

h(T ) as an initial input for a decoder RNN, with outputs of RNN blocks being
given as inputs for the next step.

This placed substantial representational pressure on the summary vector,
h(T ). Within the context of deep learning, h(T ) is sometimes referred to as a
. Its ﬁxed capacity must be suﬃcient for representing the content of
bottleneck
the entire input sequence, in a manner that is conducive to generating a cor-
responding sequence, while also supporting input sequences of substantially
diﬀerent lengths (Figure 21).

In reality, diﬀerent steps of the output may wish to focus (attend) on diﬀerent
parts of the input, and all such choices are diﬃcult to represent via a bottle-
neck vector. Following from this observation, the popular recurrent attention
model was proposed by Bahdanau et al. (2014). At every step of processing,
a query vector is generated by an RNN; this query vector then interacts with
the representation of every time-step h(t), primarily by computing a weighted
sum over them. This model pioneered neural content-based attention and
predates the success of the Transformer model.

Lastly, while attending oﬀers a soft way to dynamically focus on parts of

The bottleneck eﬀect has
recently received substantial
attention in the graph
representation learning
community (Alon and Yahav,
2020), as well as neural
algorithmic reasoning
(Cappart et al., 2021).

h(0)RencRencRencRencz(1)z(2)z(3)z(4)h(4)RdecRdecRdec...y(0)y(1)y(2)...y(1)y(2)y(3)h(1)h(2)h(3)eh(1)eh(2)eh(3)102

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

the input content, substantial work also learnt more explicit ways to direct
attention to the input. A powerful algorithmically grounded way of doing
so is the pointer network of Vinyals et al. (2015), which proposes a simple
modiﬁcation of recurrent attention to allow for pointing over elements of
variable-sized inputs. These ﬁndings have then been generalised to the set2set
architecture (Vinyals et al., 2016), which generalises seq2seq models to
unordered sets, supported by pointer network-backed LSTMs.

6 Problems and Applications

Invariances and symmetries arise all too commonly across data originating
in the real world. Hence, it should come as no surprise that some of the
most popular applications of machine learning in the 21st century have
come about as a direct byproduct of Geometric Deep Learning, perhaps
sometimes without fully realising this fact. We would like to provide readers
with an overview—by no means comprehensive—of inﬂuential works in
Geometric Deep Learning and exciting and promising new applications.
Our motivation is twofold: to demonstrate speciﬁc instances of scientiﬁc and
industrial problems where the ﬁve geometric domains commonly arise, and
to serve additional motivation for further study of Geometric Deep Learning
principles and architectures.

Many drugs are not designed
but discovered, often
serendipitously. The historic
source of a number of drugs
from the plant kingdom is
reﬂected in their names: e.g.,
the acetylsalicylic acid,
commonly known as aspirin,
is contained in the bark of the
willow tree (Salix alba),
whose medicinal properties
are known since antiquity.

Chemistry and Drug Design One of the most promising applications of
representation learning on graphs is in computational chemistry and drug
development.
Traditional drugs are small molecules that are designed to
chemically attach (‘bind’) to some target molecule, typically a protein, in
order to activate or disrupt some disease-related chemical process. Unfor-
tunately, drug development is an extremely long and expensive process: at
the time of writing, bringing a new drug to the market typically takes more
than a decade and costs more than a billion dollars. One of the reasons is
the cost of testing where many drugs fail at diﬀerent stages – less than 5% of
candidates make it to the last stage (see e.g. Gaudelet et al. (2020)).

Since the space of chemically synthesisable molecules is very large (estimated
around 1060), the search for candidate molecules with the right combina-
tion of properties such as target binding aﬃnity, low toxicity, solubility, etc.
cannot be done experimentally, and virtual or in silico screening (i.e., the use

6. PROBLEMS AND APPLICATIONS

103

Molecular graph of Halicin.

of computational techniques to identify promising molecules), is employed.
Machine learning techniques play an increasingly more prominent role in
this task. A prominent example of the use of Geometric Deep Learning
for virtual drug screening was recently shown by Stokes et al. (2020) us-
ing a graph neural network trained to predict whether or not candidate
molecules inhibit growth
in the model bacterium Escherichia coli, they were
able to eﬀectively discover that Halicin, a molecule originally indicated for
treating diabetes, is a highly potent antibiotic, even against bacteria strains
with known antibiotic resistance. This discovery was widely covered in both
scientiﬁc and popular press.

Speaking more broadly, the application of graph neural networks to molecules
modeled as graphs has been a very active ﬁeld, with multiple specialised
architectures proposed recently that are inspired by physics and e.g. in-
corporate equivariance to rotations and translations (see e.g. Thomas et al.
(2018); Anderson et al. (2019); Fuchs et al. (2020); Satorras et al. (2021)).
Further, Bapst et al. (2020) have successfully demonstrated the utility of
GNNs for predictively modelling the dynamics of glass, in a manner that
outperformed the previously available physics-based models. Historically,
many works in computational chemistry were precursors of modern graph
neural network architectures sharing many common traits with them.

Drug Repositioning While generating entirely novel drug candidates is a
potentially viable approach, a faster and cheaper avenue for developing new
therapies is drug repositioning, which seeks to evaluate already-approved
drugs (either alone or in combinations) for a novel purpose. This often
signiﬁcantly decreases the amount of clinical evaluation that is necessary
to release the drug to the market. At some level of abstraction, the action
of drugs on the body biochemistry and their interactions between each
other and other biomolecules can be modeled as a graph, giving rise to the
concept of ‘network medicine’ coined by the prominent network scientist
Albert-László Barabási and advocating the use of biological networks (such
as protein-protein interactions and metabolic pathways) to develop new
therapies (Barabási et al., 2011).

Geometric Deep Learning oﬀers a modern take on this class of approaches. A
prominent early example is the work of Zitnik et al. (2018), who used graph
neural networks to predict side eﬀects in a form of drug repositioning known
as combinatorial therapy or polypharmacy, formulated as edge prediction in

104

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

a drug-drug interaction graph. The novel coronavirus pandemic, which
is largely ongoing at the time of writing this text, has sparked a particular
interest in attempting to apply such approaches against COVID-19 (Gysi
et al., 2020). Finally, we should note that drug repositioning is not necessarily
limited to synthetic molecules: Veselkov et al. (2019) applied similar ap-
proaches to drug-like molecules contained in food (since, as we mentioned,
many plant-based foods contain biological analogues of compounds used in
oncological therapy). One of the authors of this text is involved in a collabo-
ration adding a creative twist to this research, by partnering with a molecular
chef that designs exciting recipes based on the ‘hyperfood’ ingredients rich
in such drug-like molecules.

Protein biology Since we have already mentioned proteins as drug targets,
lets us spend a few more moments on this topic. Proteins are arguably
among the most important biomolecules that have myriads of functions
in our body, including protection against pathogens (antibodies), giving
structure to our skin (collagen), transporting oxygen to cells (haemoglobin),
catalysing chemical reactions (enzymes), and signaling (many hormones are
proteins). Chemically speaking, a protein is a biopolymer, or a chain of small
building blocks called aminoacids that under the inﬂuence of electrostatic
forces fold into a complex 3D structure. It is this structure that endows the
protein with its functions,
and hence it is crucial to the understanding of
how proteins work and what they do. Since proteins are common targets for
drug therapies, the pharmaceutical industry has a keen interest in this ﬁeld.

A typical hierarchy of problems in protein bioinformatics is going from
protein sequence (a 1D string over an alphabet of of 20 diﬀerent amino acids)
to 3D structure (a problem known as ‘protein folding’) to function (‘protein
function prediction’). Recent approaches such as DeepMind’s AlphaFold
by Senior et al. (2020) used contact graphs to represent the protein structure.
Gligorijevic et al. (2020) showed that applying graph neural networks on
such graphs allows to achieve better function prediction than using purely
sequence-based methods.

Gainza et al. (2020) developed
a Geometric Deep Learning pipeline called
MaSIF predicting interactions between proteins from their 3D structure.
MaSIF models the protein as a molecular surface discretised as a mesh, argu-
ing that this representation is advantageous when dealing with interactions
as it allows to abstract the internal fold structure. The architecture was based

A common metaphor, dating
back to the chemistry Nobel
laureate Emil Fischer is the

Schlüssel-Schloss-Prinzip
(‘key-lock principle’, 1894):
two proteins often only
interact if they have
geometrically and chemically
complementary structures.

Oncologial target PD-L1
protein surface (heat map
indicated the predicted
binding site) and the
designed binder (shown as
ribbon diagram).

6. PROBLEMS AND APPLICATIONS

105

on mesh convolutional neural network operating on pre-computed chem-
ical and geometric features in small local geodesic patches. The network
was trained using a few thousand co-crystal protein 3D structures from the
Protein Data Bank to address multiple tasks, including interface prediction,
ligand classiﬁcation, and docking, and allowed to do de novo (‘from scratch’)
design of proteins that could in principle act as biological immunotherapy
drug against cancer – such proteins are designed to inhibit protein-protein
interactions (PPI) between parts of the programmed cell death protein com-
plex (PD-1/PD-L1) and give the immune system the ability to attack the
tumor cells.

Recommender Systems and Social Networks The ﬁrst popularised large-
scale applications of graph representation learning have occurred within so-
cial networks, primarily in the context of recommender systems. Recommenders
are tasked with deciding which content to serve to users, potentially depend-
ing on their previous history of interactions on the service. This is typically
realised through a link prediction objective: supervise the embeddings of
various nodes (pieces of content) such that they are kept close together if
they are deemed related (e.g. commonly viewed together). Then the proxim-
ity of two embeddings (e.g. their inner product) can be interpreted as the
probability that they are linked by an edge in the content graph, and hence
for any content queried by users, one approach could serve its k nearest
neighbours in the embedding space.

Among the pioneers of this methodology is the American image sharing and
social media company Pinterest: besides presenting one of the ﬁrst successful
deployments of GNNs in production, their method, PinSage
, successfully
made graph representation learning scalable to graphs of millions of nodes
and billions of edges (Ying et al., 2018). Related applications, particularly
in the space of product recommendations, soon followed. Popular GNN-
backed recommenders that are currently deployed in production include
Alibaba’s Aligraph (Zhu et al., 2019) and Amazon’s P-Companion (Hao
et al., 2020). In this way, graph deep learning is inﬂuencing millions of
people on a daily level.

Within the context of content analysis on social networks, another note-
worthy eﬀort is Fabula AI, which is among the ﬁrst GNN-based startups
to be acquired (in 2019, by Twitter). The startup, founded by one of the
authors of the text and his team, developed novel technology for detecting

Pinterest had also presented
follow-up work, PinnerSage
(Pal et al., 2020), which
eﬀectively integrates
user-speciﬁc contextual
information into the
recommender.

106

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

misinformation on social networks (Monti et al., 2019). Fabula’s solution
consists of modelling the spread of a particular news item by the network
of users who shared it. The users are connected if one of them re-shared
the information from the other, but also if they follow each other on the
social network. This graph is then fed into a graph neural network, which
classiﬁes the entire graph as either ‘true’ or ‘fake’ content – with labels based
on agreement between fact-checking bodies. Besides demonstrating strong
predictive power which stabilises quickly (often within a few hours of the
news spreading), analysing the embeddings of individual user nodes re-
vealed clear clustering of users who tend to share incorrect information,
exemplifying the well-known ‘echo chamber’ eﬀect.

Traﬃc forecasting Transportation networks are another area
where Geo-
metric Deep Learning techniques are already making an actionable impact
over billions of users worldwide. For example, on road networks, we can
observe intersections as nodes, and road segments as edges connecting
them—these edges can then be featurised by the road length, current or
historical speeds along their segment, and the like.

One standard prediction problem in this space is predicting the estimated time
of arrival (ETA): for a given candidate route, providing the expected travel
time necessary to traverse it. Such a problem is essential in this space, not only
for user-facing traﬃc recommendation apps, but also for enterprises (such
as food delivery or ride-sharing services) that leverage these predictions
within their own operations.

Graph neural networks
have shown immense promise in this space as well:
they can, for example, be used to directly predict the ETA for a relevant
subgraph of the road network (eﬀectively, a graph regression task). Such
an approach was successfully leveraged by DeepMind, yielding a GNN-
based ETA predictor which is now deployed in production at Google Maps
(Derrow-Pinion et al., 2021), serving ETA queries in several major metropoli-
tan areas worldwide. Similar returns have been observed by the Baidu Maps
team, where travel time predictions are currently served by the ConSTGAT
model, which is itself based on a spatio-temporal variant of the graph atten-
tion network model (Fang et al., 2020).

A road network (top) with its
corresponding graph
representation (bottom).

Several of the metropolitan
areas where GNNs are
serving queries within
Google Maps, with indicated
relative improvements in
prediction quality (40+% in
cities like Sydney).

6. PROBLEMS AND APPLICATIONS

107

Object recognition A principal benchmark for machine learning
techniques
in computer vision is the ability to classify a central object within a provided
image. The ImageNet large scale visual recognition challenge (Russakovsky
et al., 2015, ILSVRC) was an annual object classiﬁcation challenge that pro-
pelled much of the early development in Geometric Deep Learning. Im-
ageNet requires models to classify realistic images scraped from the Web
into one of 1000 categories: such categories are at the same time diverse
(covering both animate and inanimate objects), and speciﬁc (with many
classes focused on distinguishing various cat and dog breeds). Hence, good
performance on ImageNet often implies a solid level of feature extraction
from general photographs, which formed a foundation for various transfer
learning setups from pre-trained ImageNet models.

The success of convolutional neural networks on ImageNet—particularly
the AlexNet model of Krizhevsky et al. (2012), which swept ILSVRC 2012
by a large margin—has in a large way spearheaded the adoption of deep
learning as a whole, both in academia and in industry. Since then, CNNs
have consistently ranked on top of the ILSVRC, spawning many popular
, Inception
architectures such as VGG-16 (Simonyan and Zisserman, 2014)
(Szegedy et al., 2015) and ResNets (He et al., 2016), which have successfully
surpassed human-level performance on this task. The design decisions and
regularisation techniques employed by these architectures (such as rectiﬁed
linear activations (Nair and Hinton, 2010), dropout (Srivastava et al., 2014),
skip connections (He et al., 2016) and batch normalisation (Ioﬀe and Szegedy,
2015)) form the backbone of many of the eﬀective CNN models in use today.

One example input image,
the likes of which can be
found in ImageNet,
representing the “tabby cat”
class.

Interestingly, the VGG-16
architecture has sixteen
convolutional layers and is
denoted as “very deep” by
the authors. Subsequent
developments quickly scaled
up such models to hundreds
or even thousands of layers.

Concurrently with object classiﬁcation, signiﬁcant progress had been made
on object detection; that is, isolating all objects of interest within an image,
and tagging them with certain classes. Such a task is relevant in a variety of
downstream problems, from image captioning all the way to autonomous
vehicles. It necessitates a more ﬁne-grained approach, as the predictions need
to be localised; as such, often, translation equivariant models have proven
their worth in this domain. One impactful example in this space includes
the R-CNN family of models (Girshick et al., 2014; Girshick, 2015; Ren et al.,
2015; He et al., 2017) whereas, in the related ﬁeld of semantic segmentation,
the SegNet model of Badrinarayanan et al. (2017) proved inﬂuential, with
its encoder-decoder architecture relying on the VGG-16 backbone.

108

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Game playing Convolutional neural networks also play a prominent role
as translation-invariant feature extractors in reinforcement learning (RL) envi-
ronments, whenever the observed state can be represented in a grid domain;
e.g. this is the case when learning to play video games from pixels. In this
case, the CNN is responsible for reducing the input to a ﬂat vector represen-
tation, which is then used for deriving policy or value functions that drive the
RL agent’s behaviour. While the speciﬁcs of reinforcement learning are not
the focus of this section, we do note that some of the most impactful results
of deep learning in the past decade have come about through CNN-backed
reinforcement learning.

One particular example that is certainly worth mentioning here is Deep-
Mind’s AlphaGo (Silver et al., 2016). It encodes the current state within a
game of Go by applying a CNN to the 19
19 grid representing the current
positions of the placed stones. Then, through a combination of learning from
previous expert moves, Monte Carlo tree search, and self-play, it had suc-
cessfully reached a level of Go mastery that was suﬃcient to outperform Lee
Sedol, one of the strongest Go players of all time, in a ﬁve-round challenge
match that was widely publicised worldwide.

×

While this already represented a signiﬁcant milestone for broader artiﬁcial
intelligence—with Go having a substantially more complex state-space than,
—the development of AlphaGo did not stop there. The authors
say, chess
gradually removed more and more Go-speciﬁc biases from the architecture,
with AlphaGo Zero removing human biases, optimising purely through self-
play (Silver et al., 2017), AlphaZero expands this algorithm to related two-
player games, such as Chess and Shogi; lastly, MuZero (Schrittwieser et al.,
2020) incorporates a model that enables learning the rules of the game on-
the-ﬂy, which allows reaching strong performance in the Atari 2600 console,
as well as Go, Chess and Shogi, without any upfront knowledge of the rules.
Throughout all of these developments, CNNs remained the backbone behind
these models’ representation of the input.

While several high-performing RL agents were proposed for the Atari 2600
platform over the years (Mnih et al., 2015, 2016; Schulman et al., 2017), for a
long time they were unable to reach human-level performance on all of the
57 games provided therein. This barrier was ﬁnally broken with Agent57
(Badia et al., 2020), which used a parametric family of policies, ranging
from strongly exploratory to purely exploitative, and prioritising them in
diﬀerent ways during diﬀerent stages of training. It, too, powers most of its

The game of Go is played on
a 19 × 19 board, with two
players placing white and
black stones on empty ﬁelds.
The number of legal states
has been estimated at
≈ 2 × 10170 (Tromp and
Farnebäck, 2006), vastly
outnumbering the number of
atoms in the universe.

6. PROBLEMS AND APPLICATIONS

109

computations by a CNN applied to the video game’s framebuﬀer.

Text and speech synthesis Besides images (which naturally map to a two-
dimensional grid), several of (geometric) deep learning’s strongest successes
have happened on one-dimensional grids. Natural examples of this are text
and speech, folding the Geometric Deep Learning blueprint within diverse
areas such as natural language processing and digital signal processing.

Some of the most widely applied and publicised works in this space focus
on synthesis: being able to generate speech or text, either unconditionally or
conditioned on a particular prompt. Such a setup can support a plethora of
useful tasks, such as text-to-speech (TTS), predictive text completion, and ma-
chine translation. Various neural architectures for text and speech generation
have been proposed over the past decade, initially mostly based on recurrent
neural networks (e.g. the aforementioned seq2seq model (Sutskever et al.,
2014) or recurrent attention (Bahdanau et al., 2014)). However, in recent
times, they have been gradually replaced by convolutional neural networks
and Transformer-based architectures.

One particular limitation of simple 1D convolutions in this setting is their
linearly growing receptive ﬁeld, requiring many layers in order to cover the se-
quence generated so far. Dilated
convolutions, instead, oﬀer an exponentially
growing receptive ﬁeld with an equivalent number of parameters. Owing to
this, they proved a very strong alternative, eventually becoming competitive
with RNNs on machine translation (Kalchbrenner et al., 2016), while drasti-
cally reducing the computational complexity, owing to their parallelisability
across all input positions.
The most well-known application of dilated con-
volutions is the WaveNet model from van den Oord et al. (2016a). WaveNets
demonstrated that, using dilations, it is possible to synthesise speech at the
level of raw waveform (typically 16,000 samples per second or more), produc-
ing speech samples that were signiﬁcantly more “human-like” than the best
. Subsequently, it was further demon-
previous text-to-speech (TTS) systems
strated that the computations of WaveNets can be distilled in a much simpler
model, the WaveRNN (Kalchbrenner et al., 2018)—and this model enabled
eﬀectively deploying this technology at an industrial scale. This allowed not
only its deployment for large-scale speech generation for services such as
the Google Assistant, but also allowing for eﬃcient on-device computations;
e.g. for Google Duo, which uses end-to-end encryption.

Dilated convolution is also
referred to as à trous
convolution (literally “holed”
in French).

Such techniques have also
outperformed RNNs on
problems as diverse as
protein-protein interaction
(Deac et al., 2019).

Besides this, the WaveNet
model proved capable of
generating piano pieces.

110

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Transformers (Vaswani et al., 2017) have managed to surpass the limitations
of both recurrent and convolutional architectures, showing that self-attention
is suﬃcient for achieving state-of-the-art performance in machine transla-
tion. Subsequently, they have revolutionised natural language processing.
Through the pre-trained embeddings provided by models such as BERT (De-
vlin et al., 2018), Transformer computations have become enabled for a large
amount of downstream applications of natural language processing—for
example, Google uses BERT embeddings to power its search engine.

Arguably the most widely publicised application of Transformers in the
past years is text generation, spurred primarily by the Generative Pre-trained
Transformer (GPT, Radford et al. (2018, 2019); Brown et al. (2020)) family of
models from OpenAI. In particular, GPT-3 (Brown et al., 2020) successfully
scaled language model learning to 175 billion learnable parameters, trained
on next-word prediction on web-scale amounts of scraped textual corpora.
This allowed it not only to become a highly-potent few-shot learner on a
variety of language-based tasks, but also a text generator with capability to
produce coherent and human-sounding pieces of text. This capability not
only implied a large amount of downstream applications, but also induced
a vast media coverage.

Healthcare Applications in the medical domain are another promising
area for Geometric Deep Learning. There are multiple ways in which these
methods are being used. First, more traditional architectures such as CNNs
have been applied to grid-structured data, for example, for the prediction of
length of stay in Intensive Care Units (Rocheteau et al., 2020), or diagnosis of
sight-threatening diseases from retinal scans (De Fauw et al., 2018). Winkels
and Cohen (2019) showed that using 3D roto-translation group convolutional
networks improves the accuracy of pulmonary nodule detection compared
to conventional CNNs.

Second, modelling organs as geometric surfaces, mesh convolutional neural
networks were shown to be able to address a diverse range of tasks, from
reconstructing facial structure from genetics-related information (Mahdi
et al., 2020) to brain cortex parcellation (Cucurull et al., 2018) to regressing
demographic properties from cortical surface structures (Besson et al., 2020).
The latter examples represent an increasing trend in neuroscience to consider
the brain as a surface with complex folds
giving rise to highly non-Euclidean
structures.

Such structure of the brain
cortex are called sulci and
gyri in anatomical literature.

6. PROBLEMS AND APPLICATIONS

111

At the same time, neuroscientists often try construct and analyse functional
networks of the brain representing the various regions of the brain that are
activated together when performing some cognitive function; these networks
are often constructed using functional magnetic resonance imaging (fMRI)
that shows in real time which areas of the brain consume more blood.
These
functional networks can reveal patient demographics (e.g., telling apart
males from females, Arslan et al. (2018)), as well as used for neuropathology
diagnosis, which is the third area of application of Geometric Deep Learning
in medicine we would like to highlight here. In this context, Ktena et al.
(2017) pioneered the use of graph neural networks for the prediction of
neurological conditions such as Autism Spectrum Disorder. The geometric
and functional structure of the brain appears to be intimately related, and
recently Itani and Thanou (2021) pointed to the beneﬁts of exploiting them
jointly in neurological disease analysis.

Fourth, patient networks are becoming more prominent in ML-based medical
diagnosis. The rationale behind these methods is that the information of
patient demographic, genotypic, and phenotypic similarity could improve
predicting their disease. Parisot et al. (2018) applied graph neural networks
on networks of patients created from demographic features for neurological
disease diagnosis, showing that the use of the graph improves prediction
results. Cosmo et al. (2020) showed the beneﬁts of latent graph learning
(by which the network learns an unknown patient graph) in this setting.
The latter work used data from the UK Biobank, a large-scale collection of
medical data including brain imaging (Miller et al., 2016).

A wealth of data about hospital patients may be found in electronic health
records (EHRs)
. Besides giving a comprehensive view of the patient’s pro-
gression, EHR analysis allows for relating similar patients together. This
aligns with the pattern recognition method, which is commonly used in diag-
nostics. Therein, the clinician uses experience to recognise a pattern of clinical
characteristics, and it may be the primary method used when the clinician’s
experience may enable them to diagnose the condition quickly. Along these
lines, several works attempt to construct a patient graph based on EHR data,
either by analysing the embeddings of their doctor’s notes (Malone et al.,
2018), diagnosis similarity on admission (Rocheteau et al., 2021), or even
assuming a fully-connected graph (Zhu and Razavian, 2019). In all cases,
promising results have been shown in favour of using graph representation
learning for processing EHRs.

Typically, Blood
Oxygen-Level Dependent
(BOLD) contrast imaging is
used.

Publicly available
anonymised critical-care EHR
datasets include MIMIC-III
(Johnson et al., 2016) and
eICU (Pollard et al., 2018).

112

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Particle physics and astrophysics High energy physicists were perhaps
among the ﬁrst domain experts in the ﬁeld of natural sciences to embrace
the new shiny tool, graph neural networks. In a recent review paper, Shlomi
note that machine learning has historically been heavily used in
et al. (2020)
particle physics experiments, either to learn complicated inverse functions
allowing to infer the underlying physics process from the information mea-
sured in the detector, or to perform classiﬁcation and regression tasks. For
the latter, it was often necessary to force the data into an unnatural repre-
sentation such as grid, in order to be able to used standard deep learning
architectures such as CNN. Yet, many problems in physics involve data in
the form of unordered sets with rich relations and interactions, which can
be naturally represented as graphs.

One important application in high-energy physics is the reconstruction and
classiﬁcation of particle jets – sprays of stable particles arising from multiple
successive interaction and decays of particles originating from a single initial
event. In the Large Hardon Collider, the largest and best-known particle
accelerator built at CERN, such jet are the result of collisions of protons at
nearly the speed of light. These collisions produce massive particles, such as
the long though-for Higgs boson or the top quark. The identiﬁcation and
classiﬁcation of collision events is of crucial importance, as it might provide
experimental evidence to the existence of new particles.

Multiple Geometric Deep Learning approaches
have recently been proposed
for particle jet classiﬁcation task, e.g. by Komiske et al. (2019) and Qu and
Gouskos (2019), based on DeepSet and Dynamic Graph CNN architectures,
respectively. More recently, there has also been interest in developing spe-
cialsed architectures derived from physics consideration and incorporating
inductive biases consistent with Hamiltonian or Lagrangian mechanics (see
e.g. Sanchez-Gonzalez et al. (2019); Cranmer et al. (2020)), equivariant to
the Lorentz group (a fundamental symmetry of space and time in physics)
(Bogatskiy et al., 2020), or even incorporating symbolic reasoning (Cran-
mer et al., 2019) and capable of learning physical laws from data. Such
approaches are more interpretable (and thus considered more ‘trustworthy’
by domain experts) and also oﬀer better generalisation.

Besides particle accelerators, particle detectors are now being used by as-
trophysicist for multi-messenger astronomy – a new way of coordinated obser-
vation of disparate signals, such as electromagnetic radiation, gravitational
waves, and neutrinos, coming from the same source. Neutrino astronomy is

Part of the Large Hadron
Collider detectors.

Example of a particle jet.

6. PROBLEMS AND APPLICATIONS

113

The characteristic pattern of
light deposition in IceCube
detector from background
events (muon bundles, left)
and astrophysical neutrinos
(high-energy single muon,
right). Choma et al. (2018)

of particular interest, since neutrinos interact only very rarely with matter,
and thus travel enormous distances practically unaﬀected.
Detecting neutri-
nos allows to observe objects inaccessible to optical telescopes, but requires
enormously-sized detectors – the IceCube neutrino observatory uses a cubic
kilometer of Antarctic ice shelf on the South Pole as its detector. Detecting
high-energy neutrinos can possibly shed lights on some of the most mysteri-
ous objects in the Universe, such as blazars and black holes. Choma et al.
(2018) used a Geometric neural network to model the irregular geometry of
the IceCube neutrino detector, showing signiﬁcantly better performance in
detecting neutrinos coming from astrophysical sources and separating them
from background events.

While neutrino astronomy oﬀers a big promise in the study of the Cosmos,
traditional optical and radio telescopes are still the ‘battle horses’ of as-
tronomers. With these traditional instruments, Geometric Deep Learning
can still oﬀer new methodologies for data analysis. For example, Scaife
and Porter (2021) used rotationally-equivariant CNNs for the classiﬁcation
of radio galaxies, and McEwen et al. (2021) used spherical CNNs for the
analysis of cosmic microwave background radiation, a relic from the Big
Bang that might shed light on the formation of the primordial Universe. As
we already mentioned, such signals are naturally represented on the sphere
and equivariant neural networks are an appropriate tool to study them.

Virtual and Augmented Reality Another ﬁeld of applications which served
as the motivation for the development of a large class of Geometric Deep
Learning methods is computer vision and graphics, in particular, dealing
with 3D body models for virtual and augmented reality. Motion capture tech-
nology used to produce special eﬀects in movies like Avatar often operates
in two stages: ﬁrst, the input from a 3D scanner capturing the motions of the
body or the face of the actor is put into correspondence with some canonical
shape, typically modelled as a discrete manifold or a mesh (this problem is
often called ‘analysis’). Second, a new shape is generated to repeat the mo-
tion of the input (‘synthesis’). Initial works on Geometric Deep Learning in
computer graphics and vision (Masci et al., 2015; Boscaini et al., 2016a; Monti
et al., 2017) developed mesh convolutional neural networks to address the
analysis problem, or more speciﬁcally, deformable shape correspondence.

First geometric autoencoder architectures for 3D shape synthesis were pro-
posed independently by Litany et al. (2018) and Ranjan et al. (2018). In

114

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

these architectures, a canonical mesh (of the body, face, or hand) was as-
sumed to be known and the synthesis task consisted of regressing the 3D
coordinates of the nodes (the embedding of the surface, using the jargon of
a hybrid pipeline for 3D
diﬀerential geometry). Kulon et al. (2020) showed
hand pose estimation with an image CNN-based encoder and a geometric
decoder. A demo of this system, developed in collaboration with a British
startup company Ariel AI and presented at CVPR 2020, allowed to create
realistic body avatars with fully articulated hands from video input on a
mobile phone faster than real-time. Ariel AI was acquired by Snap in 2020,
and at the time of writing its technology is used in Snap’s augmented reality
products.

Examples of complex 3D
hand poses reconstructed
from 2D images in the wild
(Kulon et al., 2020).

7 Historic Perspective

“Symmetry, as wide or as narrow as you may deﬁne its meaning, is one idea
by which man through the ages has tried to comprehend and create order,
This somewhat poetic deﬁnition of symmetry is
beauty, and perfection.”
given in the eponymous book of the great mathematician Hermann Weyl
(2015), his Schwanengesang on the eve of retirement from the Institute for
Advanced Study in Princeton. Weyl traces the special place symmetry has
occupied in science and art to the ancient times, from Sumerian symmetric
designs to the Pythagoreans who believed the circle to be perfect due to its
rotational symmetry. Plato considered the ﬁve regular polyhedra bearing
his name today so fundamental that they must be the basic building blocks
shaping the material world. Yet, though Plato is credited with coining the
term συμμετρία, which literally translates as ‘same measure’, he used it only
vaguely to convey the beauty of proportion in art and harmony in music. It
was the astronomer and mathematician Johannes Kepler to attempt the ﬁrst
rigorous analysis of the symmetric shape of water crystals. In his treatise (‘On
the Six-Cornered Snowﬂake’),
he attributed the six-fold dihedral structure of
snowﬂakes to hexagonal packing of particles – an idea that though preceded
the clear understanding of how matter is formed, still holds today as the
basis of crystallography (Ball, 2011).

In modern mathematics, symme-
Symmetry in Mathematics and Physics
try is almost univocally expressed in the language of group theory. The
origins of this theory are usually attributed to Évariste Galois, who coined

The tetrahedron, cube,
octahedron, dodecahedron,
and icosahedron are called
Platonic solids.

Fully titled Strena, Seu De
Nive Sexangula (’New Year’s
gift, or on the Six-Cornered
Snowﬂake’) was, as
suggested by the title, a small
booklet sent by Kepler in
1611 as a Christmas gift to his
patron and friend Johannes
Matthäus Wackher von
Wackenfels.

7. HISTORIC PERSPECTIVE

115

the term and used it to study solvability of polynomial equations in the
1830s. Two other names associated with group theory are those of Sophus
Lie and Felix Klein, who met and worked fruitfully together for a period of
time (Tobies, 2019). The former would develop the theory of continuous
symmetries that today bears his name; the latter proclaimed group theory to
be the organising principle of geometry in his Erlangen Program, which we
mentioned in the beginning of this text. Riemannian geometry was explicitly
excluded from Klein’s uniﬁed geometric picture, and it took another ﬁfty
years before it was integrated, largely thanks to the work of Élie Cartan in
the 1920s.

Emmy Noether, Klein’s colleague in Göttingen, proved that every diﬀer-
entiable symmetry of the action of a physical system has a corresponding
conservation law (Noether, 1918). In physics, it was a stunning result: be-
forehand, meticulous experimental observation was required to discover
fundamental laws such as the conservation of energy, and even then, it was
an empirical result not coming from anywhere. Noether’s Theorem — “a
guiding star to 20th and 21st century physics”, in the words of the Nobel
laureate Frank Wilczek — showed that the conservation of energy emerges
from the translational symmetry of time, a rather intuitive idea that the re-
sults of an experiment should not depend on whether it is conducted today
or tomorrow.

The symmetry
associated with charge conservation is the global gauge invari-
ance of the electromagnetic ﬁeld, ﬁrst appearing in Maxwell’s formulation of
electrodynamics (Maxwell, 1865); however, its importance initially remained
unnoticed. The same Hermann Weyl who wrote so dithyrambically about
symmetry is the one who ﬁrst introduced the concept of gauge invariance in
physics in the early 20th century, emphasizing its role as a principle from
which electromagnetism can be derived. It took several decades until this fun-
damental principle — in its generalised form developed by Yang and Mills
(1954) — proved successful in providing a uniﬁed framework to describe
the quantum-mechanical behavior of electromagnetism and the weak and
strong forces, ﬁnally culminating in the Standard Model that captures all the
fundamental forces of nature but gravity. We can thus join another Nobel-
winning physicist, Philip Anderson (1972), in concluding that “it is only
slightly overstating the case to say that physics is the study of symmetry.”

Weyl ﬁrst conjectured
(incorrectly) in 1919 that
invariance under the change
of scale or “gauge” was a
local symmetry of
electromagnetism. The term
gauge, or Eich in German, was
chosen by analogy to the
various track gauges of
railroads. After the
development of quantum
mechanics, Weyl (1929)
modiﬁed the gauge choice by
replacing the scale factor
with a change of wave phase.
See Straumann (1996).

116

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Shun’ichi Amari is credited
as the creator of the ﬁeld of
information geometry that
applies Riemannian
geometry models to
probability. The main object
studied by information
geometry is a statistical
manifold, where each point
corresponds to a probability
distribution.

This classical work was
recognised by the Nobel
Prize in Medicine in 1981,
which Hubel and Wiesel
shared with Roger Sperry.

Early Use of Symmetry in Machine Learning In machine learning and its
applications to pattern recognition and computer vision, the importance of
symmetry has long been recognised. Early work on designing equivariant
Kanatani
feature detectors for pattern recognition was done by Amari (1978),
(2012), and Lenz (1990). In the neural networks literature, the famous
Group Invariance Theorem for Perceptrons by Minsky and Papert (2017)
puts fundamental limitations on the capabilities of (single-layer) perceptrons
to learn invariants. This was one of the primary motivations for studying
multi-layer architectures (Sejnowski et al., 1986; Shawe-Taylor, 1989, 1993),
which ultimately led to deep learning.

In the neural network community, Neocognitron (Fukushima and Miyake,
1982) is credited as the ﬁrst implementation of shift invariance in a neural
network for “pattern recognition unaﬀected by shift in position”. His solu-
tion came in the form of hierarchical neural network with local connectivity,
drawing inspiration from the receptive ﬁelds discovered in the visual cortex
by the neuroscientists David Hubel and Torsten Wiesel two decades earlier
(Hubel and Wiesel, 1959).
These ideas culminated in Convolutional Neural
Networks in the seminal work of Yann LeCun and co-authors (LeCun et al.,
1998). The ﬁrst work to take a representation-theoretical view on invariant
and equivariant neural networks was performed by Wood and Shawe-Taylor
(1996), unfortunately rarely cited. More recent incarnations of these ideas
include the works of Makadia et al. (2007); Esteves et al. (2020) and one of
the authors of this text (Cohen and Welling, 2016).

It is diﬃcult to pinpoint exactly when the concept
Graph Neural Networks
of Graph Neural Networks began to emerge—partly due to the fact that most
of the early work did not place graphs as a ﬁrst-class citizen, partly since
GNNs became practical only in the late 2010s, and partly because this ﬁeld
emerged from the conﬂuence of several research areas. That being said, early
forms of graph neural networks can be traced back at least to the 1990s, with
examples including Alessandro Sperduti’s Labeling RAAM (Sperduti, 1994),
the “backpropagation through structure” of Goller and Kuchler (1996), and
adaptive processing of data structures (Sperduti and Starita, 1997; Frasconi
et al., 1998). While these works were primarily concerned with operating
over “structures” (often trees or directed acyclic graphs), many of the in-
variances preserved in their architectures are reminiscent of the GNNs more
commonly in use today.

7. HISTORIC PERSPECTIVE

117

Concurrently, Alessio Micheli
had proposed the neural
network for graphs (NN4G)
model, which focused on a
feedforward rather than
recurrent paradigm (Micheli,
2009).

The ﬁrst proper treatment of the processing of generic graph structures (and
the coining of the term “graph neural network”) happened after the turn of
the 21st century.
Within the Artiﬁcial Intelligence lab at the Università degli
Studi di Siena (Italy), papers led by Marco Gori and Franco Scarselli have
proposed the ﬁrst “GNN” (Gori et al., 2005; Scarselli et al., 2008). They relied
on recurrent mechanisms, required the neural network parameters to specify
contraction mappings, and thus computing node representations by searching
for a ﬁxed point—this in itself necessitated a special form of backpropagation
(Almeida, 1990; Pineda, 1988) and did not depend on node features at all.
All of the above issues were rectiﬁed by the Gated GNN (GGNN) model of
Li et al. (2015). GGNNs brought many beneﬁts of modern RNNs, such as
gating mechanisms (Cho et al., 2014) and backpropagation through time, to
the GNN model, and remain popular today.

Computational chemistry It is also very important to note an independent
and concurrent line of development for GNNs: one that was entirely driven
by the needs of computational chemistry, where molecules are most naturally
expressed as graphs of atoms (nodes) connected by chemical bonds (edges).
This invited computational techniques for molecular property prediction
that operate directly over such a graph structure, which had become present
in machine learning in the 1990s: this includes the ChemNet model of Kireev
(1995) and the work of Baskin et al. (1997). Strikingly, the “molecular graph
networks” of Merkwirth and Lengauer (2005) explicitly proposed many
of the elements commonly found in contemporary GNNs—such as edge
type-conditioned weights or global pooling—as early as 2005. The chemi-
cal motivation continued to drive GNN development into the 2010s, with
two signiﬁcant GNN advancements centered around improving molecular
ﬁngerprinting (Duvenaud et al., 2015) and predicting quantum-chemical
properties (Gilmer et al., 2017) from small molecules. At the time of writing
this text, molecular property prediction is one of the most successful applica-
tions of GNNs, with impactful results in virtual screening of new antibiotic
drugs (Stokes et al., 2020).

Node embeddings Some of the earliest success stories of deep learning
on graphs involve learning representations of nodes in an unsupervised
fashion, based on the graph structure. Given their structural inspiration,
this direction also provides one of the most direct links between graph
representation learning and network science communities. The key early

118

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Recently, a theoretical
framework was developed by
Srinivasan and Ribeiro (2019)
in which the equivalence of
structural and positional
representations was
demonstrated. Additionally,
Qiu et al. (2018) have
demonstrated that all
random-walk based
embedding techniques are
equivalent to an
appropriately-posed matrix
factorisation task.

approaches in this space relied on random walk-based embeddings: learning
node representations in a way that brings them closer together if the nodes co-
occur in a short random walk. Representative methods in this space include
DeepWalk (Perozzi et al., 2014), node2vec (Grover and Leskovec, 2016) and
LINE (Tang et al., 2015), which are all purely self-supervised. Planetoid
(Yang et al., 2016) was the ﬁrst in this space to incorporate supervision label
information, when it is available.

Unifying random walk objectives with GNN encoders
was attempted on sev-
eral occasions, with representative approaches including Variational Graph
Autoencoder (VGAE, Kipf and Welling (2016b)), embedding propagation
(García-Durán and Niepert, 2017), and unsupervised variants of GraphSAGE
(Hamilton et al., 2017). However, this was met with mixed results, and it was
shortly discovered that pushing neighbouring node representations together
is already a key part of GNNs’ inductive bias. Indeed, it was shown that
an untrained GNN was already showing performance that is competitive
with DeepWalk, in settings where node features are available (Veličković
et al., 2019; Wu et al., 2019). This launched a direction that moves away
from combining random walk objectives with GNNs and shifting towards
contrastive approaches inspired by mutual information maximisation and
aligning to successful methods in the image domain. Prominent examples of
this direction include Deep Graph Informax (DGI, Veličković et al. (2019)),
GRACE (Zhu et al., 2020), BERT-like objectives (Hu et al., 2020) and BGRL
(Thakoor et al., 2021).

Probabilistic graphical models Graph neural networks have also, con-
currently, resurged through embedding the computations of probabilistic
graphical models (PGMs, Wainwright and Jordan (2008)). PGMs are a pow-
erful tool for processing graphical data, and their utility arises from their
probabilistic perspective on the graph’s edges: namely, the nodes are treated
as random variables, while the graph structure encodes conditional indepen-
dence assumptions, allowing for signiﬁcantly simplifying the calculation and
sampling from the joint distribution. Indeed, many algorithms for (exactly
or approximately) supporting learning and inference on PGMs rely on forms
of passing messages over their edges (Pearl, 2014), with examples including
variational mean-ﬁeld inference and loopy belief propagation (Yedidia et al.,
2001; Murphy et al., 2013).

This connection between PGMs and message passing was subsequently

7. HISTORIC PERSPECTIVE

119

developed into GNN architectures, with early theoretical links established
by the authors of structure2vec (Dai et al., 2016). Namely, by posing a
graph representation learning setting as a Markov random ﬁeld (of nodes
corresponding to input features and latent representations), the authors
directly align the computation of both mean-ﬁeld inference and loopy belief
propagation to a model not unlike the GNNs commonly in use today.

The key “trick” which allowed for relating the latent representations of a
GNN to probability distributions maintained by a PGM was the usage of
Hilbert-space embeddings of distributions (Smola et al., 2007). Given φ, an ap-
propriately chosen embedding function for features x, it is possible to embed
their probability distribution p(x) as the expected embedding Ex∼p(x)φ(x).
Such a correspondence allows us to perform GNN-like computations, know-
ing that the representations computed by the GNN will always correspond
to an embedding of some probability distribution over the node features.

The structure2vec model itself is, ultimately, a GNN architecture which
easily sits within our framework, but its setup has inspired a series of GNN
architectures which more directly incorporate computations found in PGMs.
Emerging examples have successfully combined GNNs with conditional
random ﬁelds (Gao et al., 2019; Spalević et al., 2020), relational Markov
networks (Qu et al., 2019) and Markov logic networks (Zhang et al., 2020).

The Weisfeiler-Lehman formalism The resurgence of graph neural net-
works was followed closely by a drive to understand their fundamental
limitations, especially in terms of expressive power. While it was becoming
evident that GNNs are a strong modelling tool of graph-structured data,
it was also clear that they wouldn’t be able to solve any task speciﬁed on a
A canonical illustrative example of this is deciding graph
graph perfectly.
isomorphism: is our GNN able to attach diﬀerent representations to two given
non-isomorphic graphs? This is a useful framework for two reasons. If the
GNN is unable to do this, then it will be hopeless on any task requiring
the discrimination of these two graphs. Further, it is currently not known if
deciding graph isomorphism is in P
, the complexity class in which all GNN
computations typically reside.

The key framework which binds GNNs to graph isomorphism is the Weisfeiler-
Lehman (WL) graph isomorphism test (Weisfeiler and Leman, 1968). This
test generates a graph representation by iteratively passing node features

Due to their permutation
invariance, GNNs will attach
identical representations to
two isomorphic graphs, so
this case is trivially solved.

The best currently known
algorithm for deciding graph
isomorphism is due to Babai
and Luks (1983), though a
recent (not fully reviewed)
proposal by Babai (2016)
implies a quasi-polynomial
time solution.

120

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

along the edges of the graph, then randomly hashing their sums across neigh-
bourhoods. Connections to randomly-initialised convolutional GNNs are
apparent, and have been observed early on: for example, within the GCN
model of Kipf and Welling (2016a). Aside from this connection, the WL
iteration was previously introduced in the domain of graph kernels by Sher-
vashidze et al. (2011), and it still presents a strong baseline for unsupervised
learning of whole-graph representations.

While
the WL test is conceptually simple, and there are many simple exam-
ples of non-isomorphic graphs it cannot distinguish, its expressive power
is ultimately strongly tied to GNNs. Analyses by Morris et al. (2019) and
Xu et al. (2018) have both reached a striking conclusion: any GNN conform-
ing to one of the three ﬂavours we outlined in Section 5.3 cannot be more
powerful than the WL test!

In order to exactly reach this level of representational power, certain con-
straints must exist on the GNN update rule. Xu et al. (2018) have shown that,
in the discrete-feature domain, the aggregation function the GNN uses must
be injective, with summation being a key representative
. Based on the outcome
of their analysis, Xu et al. (2018) propose the Graph Isomorphism Network
(GIN), which is a simple but powerful example of a maximally-expressive
GNN under this framework. It is also expressible under the convolutional
GNN ﬂavour we propose.

Lastly, it is worth noting that these ﬁndings do not generalise to continu-
ous node feature spaces. In fact, using the Borsuk-Ulam theorem (Borsuk,
1933), Corso et al. (2020) have demonstrated that, assuming real-valued
node features, obtaining injective aggregation functions requires multiple
aggregators (speciﬁcally, equal to the degree of the receiver node)
. Their
ﬁndings have driven the Principal Neighbourhood Aggregation (PNA) ar-
chitecture, which proposes a multiple-aggregator GNN that is empirically
powerful and stable.

One simple example: the WL
test cannot distinguish a
6-cycle from two triangles.

Popular aggregators such as
maximisation and averaging
fall short in this regard,
because they would not be
able to distinguish e.g. the
neighbour multisets {{a, b}}
and {{a, a, b, b}}.

One example of such
aggregators are the moments
of the multiset of neighbours.

Which, in contrast, almost
always consider featureless or
categorically-featured graphs.

Higher-order methods The ﬁndings of the previous paragraphs do not
contradict the practical utility of GNNs. Indeed, in many real-world applica-
tions the input features are suﬃciently rich to support useful discriminative
.
computations over the graph structure, despite of the above limitations

However, one key corollary is that GNNs are relatively quite weak at de-
tecting some rudimentary structures within a graph. Guided by the speciﬁc

7. HISTORIC PERSPECTIVE

121

limitations or failure cases of the WL test, several works have provided
stronger variants of GNNs that are provably more powerful than the WL test,
.
and hence likely to be useful on tasks that require such structural detection One prominent example is
computational chemistry,
wherein a molecule’s
chemical function can be
strongly inﬂuenced by the
presence of aromatic rings in
its molecular graph.

Perhaps the most direct place to hunt for more expressive GNNs is the WL
test itself. Indeed, the strength of the original WL test can be enhanced by
considering a hierarchy of WL tests, such that k-WL tests attach represen-
tations to k-tuples of nodes (Morris et al., 2017). The k-WL test has been
directly translated into a higher-order k-GNN architecture by Morris et al.
(2019),
which is provably more powerful than the GNN ﬂavours we con-
sidered before. However, its requirement to maintain tuple representations
implies that, in practice, it is hard to scale beyond k = 3.

Concurrently, Maron et al. (2018, 2019) have studied the characterisation of
invariant and equivariant graph networks over k-tuples of nodes. Besides
demonstrating the surprising result of any invariant or equivariant graph
network being expressible as a linear combination of a ﬁnite number of
generators—the amount of which only depends on k—the authors showed
that the expressive power of such layers is equivalent to the k-WL test, and
proposed an empirically scalable variant which is provably 3-WL powerful.

Besides generalising the domain over which representations are computed,
signiﬁcant eﬀort had also went into analysing speciﬁc failure cases of 1-WL
and augmenting GNN inputs to help them distinguish such cases. One
common example is attaching identifying features to the nodes, which can
help detecting structure
. Proposals to do this include one-hot representations
(Murphy et al., 2019), as well as purely random features (Sato et al., 2020).

More broadly, there have been many eﬀorts to incorporate structural informa-
tion within the message passing process, either by modulating the message
. Several in-
function or the graph that the computations are carried over
teresting lines of work here involve sampling anchor node sets (You et al.,
2019), aggregating based on Laplacian eigenvectors (Stachenfeld et al., 2020;
Beaini et al., 2020; Dwivedi and Bresson, 2020), or performing topological data
analysis, either for positional embeddings (Bouritsas et al., 2020) or driving
message passing (Bodnar et al., 2021).

Signal processing and Harmonic analysis Since the early successes of
Convolutional Neural Networks, researchers have resorted to tools from har-
monic analysis, image processing, and computational neuroscience trying to

There have been eﬀorts, such
as the δ-k-LGNN (Morris
et al., 2020), to sparsify the
computation of the k-GNN.

For example, if a node sees its
own identiﬁer k hops away, it
is a direct indicator that it is
within a k-cycle.

In the computational
chemistry domain, it is often
assumed that molecular
function is driven by
substructures (the functional
groups), which have directly
inspired the modelling of
molecules at a motif level. For
references, consider Jin et al.
(2018, 2020); Fey et al. (2020).

122

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

provide a theoretical framework that explains their eﬃciency. M -theory is a
framework inspired by the visual cortex, pioneered by Tomaso Poggio and
collaborators (Riesenhuber and Poggio, 1999; Serre et al., 2007), based on the
notion of templates that can be manipulated under certain symmetry groups.
Another notable model arising from computational neuroscience were steer-
able pyramids, a form of multiscale wavelet decompositions with favorable
properties against certain input transformations, developed by Simoncelli
and Freeman (1995). They were a central element in early generative mod-
els for textures (Portilla and Simoncelli, 2000), which were subsequently
improved by replacing steerable wavelet features with deep CNN features
Gatys et al. (2015). Finally, Scattering transforms, introduced by Stéphane
Mallat (2012) and developed by Bruna and Mallat (2013), provided a frame-
work to understand CNNs by replacing trainable ﬁlters with multiscale
wavelet decompositions, also showcasing the deformation stability and the
role of depth in the architecture.

Signal Processing on Graph and Meshes Another important class of graph
neural networks, often referred to as spectral, has emerged from the work of
one of the authors of this text (Bruna et al., 2013), using the notion of the
Graph Fourier transform. The roots of this construction are in the signal pro-
cessing and computational harmonic analysis communities, where dealing
with non-Euclidean signals has become prominent in the late 2000s and early
2010s. Inﬂuential papers from the groups of Pierre Vandergheynst (Shuman
et al., 2013) and José Moura (Sandryhaila and Moura, 2013) popularised
the notion of “Graph Signal Processing” (GSP) and the generalisation of
Fourier transforms based on the eigenvectors of graph adjacency and Lapla-
cian matrices. The graph convolutional neural networks relying on spectral
ﬁlters by Deﬀerrard et al. (2016) and Kipf and Welling (2016a) are among
the most cited in the ﬁeld and can likely be credited) as ones reigniting the
interest in machine learning on graphs in recent years.

It is worth noting that, in the ﬁeld of computer graphics and geometry pro-
cessing, non-Euclidean harmonic analysis predates Graph Signal Processing
by at least a decade. We can trace spectral ﬁlters on manifolds and meshes
to the works of Taubin et al. (1996). These methods became mainstream in
the 2000s following the inﬂuential papers of Karni and Gotsman (2000) on
spectral geometry compression and of Lévy (2006) on using the Laplacian
eigenvectors as a non-Euclidean Fourier basis. Spectral methods have been
most prominent of which is the construction
used for a range of applications,

Learnable shape descriptors
similar to spectral graph
CNNs were proposed by
Roee Litman and Alex
Bronstein (2013), the latter
being a twin brother of the
author of this text.

7. HISTORIC PERSPECTIVE

123

of shape descriptors (Sun et al., 2009) and functional maps (Ovsjanikov
et al., 2012); these methods are still broadly used in computer graphics at
the time of writing.

Computer Graphics and Geometry Processing Models for shape analysis
based on intrinsic metric invariants were introduced by various authors in
the ﬁeld of computer graphics and geometry processing (Elad and Kimmel,
2003; Mémoli and Sapiro, 2005; Bronstein et al., 2006), and are discussed
in depth by one of the authors in his earlier book (Bronstein et al., 2008).
The notions of intrinsic symmetries were also explored in the same ﬁeld
Raviv et al. (2007); Ovsjanikov et al. (2008). The ﬁrst architecture for deep
learning on meshes, Geodesic CNNs, was developed in the team of one of
the authors of the text (Masci et al., 2015). This model used local ﬁlters
with shared weights, applied to geodesic radial patches. It was a particular
setting of gauge-equivariant CNNs developed later by another author of
the text (Cohen et al., 2019). A generalisation of Geodesic CNNs with
learnable aggregation operations, MoNet, proposed by Federico Monti et al.
(2017) from the same team, used an attention-like mechanism over the local
structural features of the mesh, that was demonstrated to work on general
graphs as well. The graph attention network (GAT), which technically
speaking can be considered a particular instance of MoNet, was introduced
by another author of this text (Veličković et al., 2018). GATs generalise
MoNet’s attention mechanism to also incorporate node feature information,
breaking away from the purely structure-derived relevance of prior work. It
is one of the most popular GNN architectures currently in use.

In the context of computer graphics, it is also worthwhile to mention that
the idea of learning on sets (Zaheer et al., 2017) was concurrently devel-
oped in the group of Leo Guibas at Stanford under the name PointNet (Qi
et al., 2017) for the analysis of 3D point clouds. This architecture has lead
to multiple follow-up works, including one by an author of this text called
Dynamic Graph CNN (DGCNN, Wang et al. (2019b)). DGCNN used a
nearest-neighbour graph to capture the local structure of the point cloud
to allow exchange of information across the nodes; the key characteristic of
this architecture was that the graph was constructed on-the-ﬂy and updated
between the layers of the neural network in relation to the downstream task.
This latter property made DGCNN one of the ﬁrst incarnations of ‘latent
graph learning’, which in its turn has had signiﬁcant follow up. Extensions to
DGCNN’s k-nearest neighbour graph proposal include more explicit control

124

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

over these graphs’ edges, either through bilevel optimisation (Franceschi
et al., 2019), reinforcement learning (Kazi et al., 2020) or direct supervi-
sion (Veličković et al., 2020). Independently, a variational direction (which
probabilistically samples edges from a computed posterior distribution) has
emerged through the NRI model (Kipf et al., 2018). While it still relies
on quadratic computation in the number of nodes, it allows for explicitly
encoding uncertainty about the chosen edges.

Another very popular direction in learning on graphs without a provided
graph relies on performing GNN-style computations over a complete graph,
letting the network infer its own way to exploit the connectivity. The need for
this arisen particularly in natural language processing, where various words
in a sentence interact in highly nontrivial and non-sequential ways. Operat-
ing over a complete graph of words brought about the ﬁrst incarnation of the
Transformer model (Vaswani et al., 2017), which de-throned both recurrent
and convolutional models as state-of-the-art in neural machine translation,
and kicked oﬀ an avalanche of related work, transcending the boundaries
between NLP and other ﬁelds. Fully-connected GNN computation has also
concurrently emerged on simulation (Battaglia et al., 2016), reasoning (San-
toro et al., 2017), and multi-agent (Hoshen, 2017) applications, and still
represents a popular choice when the number of nodes is reasonably small.

Algorithmic reasoning For most of the discussion we posed in this section,
we have given examples of spatially induced geometries, which in turn shape
the underlying domain, and its invariances and symmetries. However, plen-
tiful examples of invariances and symmetries also arise in a computational
setting. One critical diﬀerence to many common settings of Geometric Deep
Learning is that links no longer need to encode for any kind of similarity,
proximity, or types of relations—they merely specify the “recipe” for the
dataﬂow between data points they connect.

For example, one invariant of
the Bellman-Ford pathﬁnding
algorithm (Bellman, 1958) is
that, after k steps, it will
always compute the shortest
paths to the source node that
use no more than k edges.

Instead, the computations of the neural network mimic the reasoning process
of an algorithm (Cormen et al., 2009), with additional invariances induced
by the algorithm’s control ﬂow and intermediate results
. In the space of
algorithms the assumed input invariants are often referred to as preconditions,
while the invariants preserved by the algorithm are known as postconditions.

Eponymously, the research direction of algorithmic reasoning (Cappart et al.,
2021, Section 3.3.) seeks to produce neural network architectures that ap-

7. HISTORIC PERSPECTIVE

125

propriately preserve algorithmic invariants. The area has investigated the
construction of general-purpose neural computers, e.g., the neural Turing
machine (Graves et al., 2014) and the diﬀerentiable neural computer (Graves
et al., 2016). While such architectures have all the hallmarks of general
computation, they introduced several components at once, making them
often challenging to optimise, and in practice, they are almost always outper-
formed by simple relational reasoners, such as the ones proposed by Santoro
et al. (2017, 2018).

As modelling complex postconditions is challenging, plentiful work on in-
ductive biases for learning to execute (Zaremba and Sutskever, 2014) has
focused on primitive algorithms (e.g. simple arithmetic). Prominent ex-
amples in this space include the neural GPU (Kaiser and Sutskever, 2015),
neural RAM (Kurach et al., 2015), neural programmer-interpreters (Reed and
De Freitas, 2015), neural arithmetic-logic units (Trask et al., 2018; Madsen and
Johansen, 2020) and neural execution engines (Yan et al., 2020).

Emulating combinatorial algorithms of superlinear complexity was made
possible with the rapid development of GNN architectures. The algorithmic
alignment framework pioneered by Xu et al. (2019) demonstrated, theoreti-
cally, that GNNs align with dynamic programming (Bellman, 1966), which is
a language in which most algorithms can be expressed. It was concurrently
empirically shown, by one of the authors of this text, that it is possible to
design and train GNNs that align with algorithmic invariants in practice
(Veličković et al., 2019). Onwards, alignment was achieved with iterative algo-
rithms (Tang et al., 2020), linearithmic algorithms (Freivalds et al., 2019), data
structures (Veličković et al., 2020) and persistent memory (Strathmann et al.,
2021). Such models have also seen practical use in implicit planners (Deac
et al., 2020), breaking into the space of reinforcement learning algorithms.

Concurrently, signiﬁcant progress has been made on using GNNs for physics
simulations (Sanchez-Gonzalez et al., 2020; Pfaﬀ et al., 2020). This direction
yielded much of the same recommendations for the design of generalising
GNNs. Such a correspondence is to be expected: given that algorithms
can be phrased as discrete-time simulations, and simulations are typically
implemented as step-wise algorithms, both directions will need to preserve
similar kinds of invariants.

Tightly bound with the study of algorithmic reasoning are measures of
extrapolation. This is a notorious pain-point for neural networks, given that
most of their success stories are obtained when generalising in-distribution;

126

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

i.e. when the patterns found in the training data properly anticipate the ones
found in the test data. However, algorithmic invariants must be preserved
irrespective of, e.g., the size or generative distribution of the input, meaning
that the training set will likely not cover any possible scenario encountered
in practice. Xu et al. (2020b) have proposed a geometric argument for what
is required of an extrapolating GNN backed by rectiﬁer activations: its
components and featurisation would need to be designed so as to make
its constituent modules (e.g. message function) learn only linear target
functions. Bevilacqua et al. (2021) propose observing extrapolation under
the lens of causal reasoning, yielding environment-invariant representations of
graphs.

Geometric Deep Learning Our ﬁnal historical remarks regard the very
name of this text. The term ‘Geometric Deep Learning’ was ﬁrst introduced
by one of the authors of this text in his ERC grant in 2015 and popularised
in the eponymous IEEE Signal Processing Magazine paper (Bronstein et al.,
2017). This paper proclaimed, albeit “with some caution”, the signs of “a
new ﬁeld being born.” Given the recent popularity of graph neural networks,
the increasing use of ideas of invariance and equivariance in a broad range
of machine learning applications, and the very fact of us writing this text, it
is probably right to consider this prophecy at least partially fulﬁlled. The
name “4G: Grids, Graphs, Groups, and Gauges” was coined by Max Welling
for the ELLIS Program on Geometric Deep Learning, co-directed by two
authors of the text. Admittedly, the last ‘G’ is somewhat of a stretch, since
the underlying structures are manifolds and bundles rather than gauges. For
this text, we added another ‘G’, Geodesics, in reference to metric invariants
and intrinsic symmetries of manifolds.

Acknowledgements

This text represents a humble attempt to summarise and synthesise decades
of existing knowledge in deep learning architectures, through the geometric
lens of invariance and symmetry. We hope that our perspective will make
it easier both for newcomers and practitioners to navigate the ﬁeld, and for
researchers to synthesise novel architectures, as instances of our blueprint.
In a way, we hope to have presented “all you need to build the architectures that
are all you need”—a play on words inspired by Vaswani et al. (2017).

7. HISTORIC PERSPECTIVE

127

The bulk of the text was written during late 2020 and early 2021. As it often
happens, we had thousands of doubts whether the whole picture makes
sense, and used opportunities provided by our colleagues to help us break
our “stage fright” and present early versions of our work, which saw the light
of day in Petar’s talk at Cambridge (courtesy of Pietro Liò) and Michael’s
talks at Oxford (courtesy of Xiaowen Dong) and Imperial College (hosted by
Michael Huth and Daniel Rueckert). Petar was also able to present our work
at Friedrich-Alexander-Universität Erlangen-Nürnberg—the birthplace of
the Erlangen Program!—owing to a kind invitation from Andreas Maier. The
feedback we received for these talks was enormously invaluable to keeping
our spirits high, as well as polishing the work further. Last, but certainly not
least, we thank the organising committee of ICLR 2021, where our work will
be featured in a keynote talk, delivered by Michael.

We should note that reconciling such a vast quantity of research is seldom
enabled by the expertise of only four people. Accordingly, we would like to
give due credit to all of the researchers who have carefully studied aspects of
our text as it evolved, and provided us with careful comments and references:
Yoshua Bengio, Charles Blundell, Andreea Deac, Fabian Fuchs, Francesco
di Giovanni, Marco Gori, Raia Hadsell, Will Hamilton, Maksym Korablyov,
Christian Merkwirth, Razvan Pascanu, Bruno Ribeiro, Anna Scaife, Jürgen
Schmidhuber, Marwin Segler, Corentin Tallec, Ngân V ˜u, Peter Wirnsberger
and David Wong. Their expert feedback was invaluable to solidifying our
uniﬁcation eﬀorts and making them more useful to various niches. Though,
of course, any irregularities within this text are our responsibility alone. It is
currently very much a work-in-progress, and we are very happy to receive
comments at any stage. Please contact us if you spot any errors or omissions.

Bibliography

Yonathan Aﬂalo and Ron Kimmel. Spectral multidimensional scaling. PNAS,

110(45):18052–18057, 2013.

Yonathan Aﬂalo, Haim Brezis, and Ron Kimmel. On the optimality of shape
and data representation in the spectral domain. SIAM J. Imaging Sciences,
8(2):1141–1160, 2015.

Luis B Almeida. A learning rule for asynchronous perceptrons with feed-
back in a combinatorial environment. In Artiﬁcial neural networks: concept
learning, pages 102–111. 1990.

Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and

its practical implications. arXiv:2006.05205, 2020.

Sl Amari. Feature spaces which admit and detect invariant signal transfor-

mations. In Joint Conference on Pattern Recognition, 1978.

Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant

molecular neural networks. arXiv:1906.04015, 2019.

Philip W Anderson. More is diﬀerent. Science, 177(4047):393–396, 1972.
Mathieu Andreux, Emanuele Rodola, Mathieu Aubry, and Daniel Cremers.
Anisotropic Laplace-Beltrami operators for shape analysis. In ECCV, 2014.
Salim Arslan, Soﬁa Ira Ktena, Ben Glocker, and Daniel Rueckert. Graph
saliency maps through spectral convolutional networks: Application to
sex classiﬁcation with brain connectivity. In Graphs in Biomedical Image
Analysis and Integrating Medical Imaging and Non-Imaging Modalities, pages
3–13. 2018.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoﬀrey E Hinton. Layer normalization.

arXiv:1607.06450, 2016.

130

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

László Babai. Graph isomorphism in quasipolynomial time. In ACM Sympo-

sium on Theory of Computing, 2016.

László Babai and Eugene M Luks. Canonical labeling of graphs. In ACM

Symposium on Theory of computing, 1983.

Francis Bach. Breaking the curse of dimensionality with convex neural

networks. JMLR, 18(1):629–681, 2017.

Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprech-
mann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell.
Agent57: Outperforming the atari human benchmark. In ICML, 2020.

Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep
convolutional encoder-decoder architecture for image segmentation. Trans.
PAMI, 39(12):2481–2495, 2017.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
translation by jointly learning to align and translate. arXiv:1409.0473, 2014.

Philip Ball. In retrospect: On the six-cornered snowﬂake. Nature, 480(7378):

455–455, 2011.

Bassam Bamieh. Discovering transforms: A tutorial on circulant matrices,
circular convolution, and the discrete fourier transform. arXiv:1805.05533,
2018.

Stefan Banach. Sur les opérations dans les ensembles abstraits et leur appli-
cation aux équations intégrales. Fundamenta Mathematicae, 3(1):133–181,
1922.

Victor Bapst, Thomas Keck, A Grabska-Barwińska, Craig Donner, Ekin Do-
gus Cubuk, Samuel S Schoenholz, Annette Obika, Alexander WR Nelson,
Trevor Back, Demis Hassabis, et al. Unveiling the predictive power of
static structure in glassy systems. Nature Physics, 16(4):448–454, 2020.

Albert-László Barabási, Natali Gulbahce, and Joseph Loscalzo. Network
medicine: a network-based approach to human disease. Nature Reviews
Genetics, 12(1):56–68, 2011.

Andrew R Barron. Universal approximation bounds for superpositions of a
sigmoidal function. IEEE Trans. Information Theory, 39(3):930–945, 1993.

BIBLIOGRAPHY

131

Igor I Baskin, Vladimir A Palyulin, and Nikolai S Zeﬁrov. A neural device
for searching direct correlations between structures and properties of
chemical compounds. J. Chemical Information and Computer Sciences, 37(4):
715–721, 1997.

Peter W Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray
Kavukcuoglu. Interaction networks for learning about objects, relations
and physics. arXiv:1612.00222, 2016.

Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo,
Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep
learning, and graph networks. arXiv:1806.01261, 2018.

Dominique Beaini, Saro Passaro, Vincent Létourneau, William L Hamil-
ton, Gabriele Corso, and Pietro Liò. Directional graph networks.
arXiv:2010.02863, 2020.

Richard Bellman. On a routing problem. Quarterly of Applied Mathematics, 16

(1):87–90, 1958.

Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term de-
pendencies with gradient descent is diﬃcult. IEEE Trans. Neural Networks,
5(2):157–166, 1994.

Marcel Berger. A panoramic view of Riemannian geometry. Springer, 2012.
Pierre Besson, Todd Parrish, Aggelos K Katsaggelos, and S Kathleen
Bandt. Geometric deep learning on brain shape predicts sex and age.
BioRxiv:177543, 2020.

Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph
representations for graph classiﬁcation extrapolations. arXiv:2103.05045,
2021.

Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regu-
larization for deep neural networks driven by an ornstein-uhlenbeck like
process. In COLT, 2020.

Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Mon-
túfar, Pietro Liò, and Michael Bronstein. Weisfeiler and lehman go topo-
logical: Message passing simplicial networks. arXiv:2103.03212, 2021.

132

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Alexander Bogatskiy, Brandon Anderson, Jan Oﬀermann, Marwah Roussi,
David Miller, and Risi Kondor. Lorentz group equivariant neural network
for particle physics. In ICML, 2020.

Karol Borsuk. Drei sätze über die n-dimensionale euklidische sphäre. Fun-

damenta Mathematicae, 20(1):177–190, 1933.

Davide Boscaini, Davide Eynard, Drosos Kourounis, and Michael M Bron-
stein. Shape-from-operator: Recovering shapes from intrinsic operators.
Computer Graphics Forum, 34(2):265–274, 2015.

Davide Boscaini, Jonathan Masci, Emanuele Rodoià, and Michael Bronstein.
Learning shape correspondence with anisotropic convolutional neural
networks. In NIPS, 2016a.

Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael M Bronstein,
and Daniel Cremers. Anisotropic diﬀusion descriptors. Computer Graphics
Forum, 35(2):431–441, 2016b.

Sébastien Bougleux, Luc Brun, Vincenzo Carletti, Pasquale Foggia, Benoit
Gaüzere, and Mario Vento. A quadratic assignment formulation of the
graph edit distance. arXiv:1512.07494, 2015.

Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bron-
stein. Improving graph neural network expressivity via subgraph isomor-
phism counting. arXiv:2006.09252, 2020.

Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized
multidimensional scaling: a framework for isometry-invariant partial
surface matching. PNAS, 103(5):1168–1172, 2006.

Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Numerical

geometry of non-rigid shapes. Springer, 2008.

Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre
Vandergheynst. Geometric deep learning: going beyond Euclidean data.
IEEE Signal Processing Magazine, 34(4):18–42, 2017.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv:2005.14165, 2020.

BIBLIOGRAPHY

133

Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1872–
1886, 2013.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral
networks and locally connected networks on graphs. In ICLR, 2013.

Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher
Morris, and Petar Veličković. Combinatorial optimization and reasoning
with graph neural networks. arXiv:2102.09544, 2021.

Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.

Neural ordinary diﬀerential equations. arXiv:1806.07366, 2018.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A
simple framework for contrastive learning of visual representations. In
ICML, 2020.

Albert Chern, Felix Knöppel, Ulrich Pinkall, and Peter Schröder. Shape from

metric. ACM Trans. Graphics, 37(4):1–17, 2018.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bah-
danau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv:1406.1078, 2014.

Nicholas Choma, Federico Monti, Lisa Gerhardt, Tomasz Palczewski, Zahra
Ronaghi, Prabhat Prabhat, Wahid Bhimji, Michael M Bronstein, Spencer R
Klein, and Joan Bruna. Graph neural networks for icecube signal classiﬁ-
cation. In ICMLA, 2018.

Taco Cohen and Max Welling. Group equivariant convolutional networks.

In ICML, 2016.

Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge
equivariant convolutional networks and the icosahedral CNN. In ICML,
2019.

Taco S Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical cnns.

arXiv:1801.10130, 2018.

Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, and Aaron

Courville. Recurrent batch normalization. arXiv:1603.09025, 2016.

134

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Etienne Corman, Justin Solomon, Mirela Ben-Chen, Leonidas Guibas, and
Maks Ovsjanikov. Functional characterization of intrinsic and extrinsic
geometry. ACM Trans. Graphics, 36(2):1–17, 2017.

Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord Stein.

Introduction to algorithms. MIT press, 2009.

Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar
Principal neighbourhood aggregation for graph nets.

Veličković.
arXiv:2004.05718, 2020.

Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael
Bronstein. Latent-graph learning for disease prediction. In MICCAI, 2020.
Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David
Spergel, and Shirley Ho. Lagrangian neural networks. arXiv:2003.04630,
2020.

Miles D Cranmer, Rui Xu, Peter Battaglia, and Shirley Ho. Learning symbolic

physics with graph networks. arXiv:1909.05862, 2019.

Guillem Cucurull, Konrad Wagstyl, Arantxa Casanova, Petar Veličković,
Estrid Jakobsen, Michal Drozdzal, Adriana Romero, Alan Evans, and
Yoshua Bengio. Convolutional neural networks for mesh-based parcella-
tion of the cerebral cortex. 2018.

George Cybenko. Approximation by superpositions of a sigmoidal function.

Mathematics of Control, Signals and Systems, 2(4):303–314, 1989.

Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent

variable models for structured data. In ICML, 2016.

Jeﬀrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav
Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot,
Brendan O’Donoghue, Daniel Visentin, et al. Clinically applicable deep
learning for diagnosis and referral in retinal disease. Nature Medicine, 24
(9):1342–1350, 2018.

Pim de Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equiv-
ariant mesh CNNs: Anisotropic convolutions on geometric graphs. In
NeurIPS, 2020.

Andreea Deac, Petar Veličković, and Pietro Sormanni. Attentive cross-modal
paratope prediction. Journal of Computational Biology, 26(6):536–545, 2019.

BIBLIOGRAPHY

135

Andreea Deac, Petar Veličković, Ognjen Milinković, Pierre-Luc Bacon, Jian
Tang, and Mladen Nikolić. Xlvin: executed latent value iteration nets.
arXiv:2010.13146, 2020.

Michaël Deﬀerrard, Xavier Bresson, and Pierre Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral ﬁltering.
NIPS, 2016.

Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester,
Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Peter W Battaglia,
Vishal Gupta, Ang Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez, Yujia Li,
and Petar Veličković. Traﬃc Prediction with Graph Neural Networks in
Google Maps. 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understand-
ing. arXiv:1810.04805, 2018.

David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional
networks on graphs for learning molecular ﬁngerprints. NIPS, 2015.
Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer

networks to graphs. arXiv:2012.09699, 2020.

Asi Elad and Ron Kimmel. On bending invariant signatures for surfaces.

Trans. PAMI, 25(10):1285–1295, 2003.

Jeﬀrey L Elman. Finding structure in time. Cognitive Science, 14(2):179–211,

1990.

Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted

spherical CNNs. arXiv:2006.10731, 2020.

Xiaomin Fang, Jizhou Huang, Fan Wang, Lingke Zeng, Haijin Liang, and
Haifeng Wang. ConSTGAT: Contextual spatial-temporal graph attention
network for travel time estimation at baidu maps. In KDD, 2020.

Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical inter-message

passing for learning on molecular graphs. arXiv:2006.12179, 2020.

Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson.
Generalizing convolutional neural networks for equivariance to lie groups
on arbitrary continuous data. In ICML, 2020.

136

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Jon Folkman. Regular line-symmetric graphs. Journal of Combinatorial Theory,

3(3):215–232, 1967.

Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learn-

ing discrete structures for graph neural networks. In ICML, 2019.

Paolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework
for adaptive processing of data structures. IEEE Trans. Neural Networks, 9
(5):768–786, 1998.

K¯arlis Freivalds, Em¯ıls Ozolin, š, and Agris Šostaks. Neural shuﬄe-exchange
networks–sequence processing in o (n log n) time. arXiv:1907.07897, 2019.
Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling.
SE(3)-transformers: 3D roto-translation equivariant attention networks.
arXiv:2006.10503, 2020.

Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing
neural network model for a mechanism of visual pattern recognition. In
Competition and Cooperation in Neural Nets, pages 267–285. Springer, 1982.
Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola,
D Boscaini, MM Bronstein, and BE Correia. Deciphering interaction ﬁn-
gerprints from protein molecular surfaces using geometric deep learning.
Nature Methods, 17(2):184–192, 2020.

Fernando Gama, Alejandro Ribeiro, and Joan Bruna. Diﬀusion scattering

transforms on graphs. In ICLR, 2019.

Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of
graph neural networks. IEEE Trans. Signal Processing, 68:5680–5695, 2020.
Hongchang Gao, Jian Pei, and Heng Huang. Conditional random ﬁeld

enhanced graph convolutional neural networks. In KDD, 2019.

Alberto García-Durán and Mathias Niepert. Learning graph representations

with embedding propagation. arXiv:1710.03059, 2017.

Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis
using convolutional neural networks. arXiv preprint arXiv:1505.07376, 2015.
Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep,
Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian
Tang, et al. Utilising graph machine learning within drug discovery and
development. arXiv:2012.05716, 2020.

BIBLIOGRAPHY

137

Felix A Gers and Jürgen Schmidhuber. Recurrent nets that time and count.

In IJCNN, 2000.

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and
George E Dahl. Neural message passing for quantum chemistry.
arXiv:1704.01212, 2017.

Ross Girshick. Fast R-CNN. In CVPR, 2015.
Ross Girshick, Jeﬀ Donahue, Trevor Darrell, and Jitendra Malik. Rich feature
hierarchies for accurate object detection and semantic segmentation. In
CVPR, 2014.

Vladimir Gligorijevic, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler
Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor,
Ian M Fisk, Hera Vlamakis, et al. Structure-based function prediction
using graph convolutional networks. bioRxiv:786236, 2020.

Christoph Goller and Andreas Kuchler. Learning task-dependent distributed
representations by backpropagation through structure. In ICNN, 1996.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
adversarial networks. arXiv:1406.2661, 2014.

Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for

learning in graph domains. In IJCNN, 2005.

Alex Graves. Generating sequences with recurrent neural networks.

arXiv:1308.0850, 2013.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines.

arXiv:1410.5401, 2014.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Ag-
nieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefen-
stette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a
neural network with dynamic external memory. Nature, 538(7626):471–
476, 2016.

Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-
han Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own
latent: A new approach to self-supervised learning. arXiv:2006.07733,
2020.

138

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Mikhael Gromov. Structures métriques pour les variétés riemanniennes. Cedic,

1981.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for

networks. In KDD, 2016.

Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam
Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization.
In NIPS, 2017.

Deisy Morselli Gysi, Ítalo Do Valle, Marinka Zitnik, Asher Ameli, Xiao
Gan, Onur Varol, Helia Sanchez, Rebecca Marlene Baron, Dina Ghiassian,
Joseph Loscalzo, et al. Network medicine framework for identifying drug
repurposing opportunities for COVID-19. arXiv:2004.07229, 2020.

Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation

learning on large graphs. In NIPS, 2017.

Junheng Hao, Tong Zhao, Jin Li, Xin Luna Dong, Christos Faloutsos, Yizhou
Sun, and Wei Wang. P-companion: A principled framework for diversiﬁed
complementary product recommendation. In Information & Knowledge
Management, 2020.

Moritz Hardt and Tengyu Ma.

Identity matters in deep learning.

arXiv:1611.04231, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual

learning for image recognition. In CVPR, 2016.

Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.

In CVPR, 2017.

Claude Adrien Helvétius. De l’esprit. Durand, 1759.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal,
Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep repre-
sentations by mutual information estimation and maximization. In ICLR,
2019.

Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. PhD

thesis, Technische Universität München, 1991.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural

Computation, 9(8):1735–1780, 1997.

BIBLIOGRAPHY

139

Kurt Hornik. Approximation capabilities of multilayer feedforward networks.

Neural Networks, 4(2):251–257, 1991.

Yedid Hoshen.

Vain: Attentional multi-agent predictive modeling.

arXiv:1706.06122, 2017.

Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vi-
jay Pande, and Jure Leskovec. Strategies for pre-training graph neural
networks. In ICLR, 2020.

David H Hubel and Torsten N Wiesel. Receptive ﬁelds of single neurones in

the cat’s striate cortex. J. Physiology, 148(3):574–591, 1959.

Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont,
Yee Whye Teh, and Hyunjik Kim. LieTransformer: Equivariant self-
attention for Lie groups. arXiv:2012.10885, 2020.

Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.

Haris Iqbal. Harisiqbal88/plotneuralnet v1.0.0, December 2018. URL https:

//doi.org/10.5281/zenodo.2526396.

Sarah Itani and Dorina Thanou. Combining anatomical and functional net-
works for neuropathology identiﬁcation: A case study on autism spectrum
disorder. Medical Image Analysis, 69:101986, 2021.

Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational

autoencoder for molecular graph generation. In ICML, 2018.

Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation

of molecular graphs using structural motifs. In ICML, 2020.

Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling
Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care
database. Scientiﬁc Data, 3(1):1–9, 2016.

Michael I Jordan. Serial order: A parallel distributed processing approach.

In Advances in Psychology, volume 121, pages 471–495. 1997.

Chaitanya Joshi. Transformers are graph neural networks. The Gradient,

2020.

140

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical ex-

ploration of recurrent network architectures. In ICML, 2015.

Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms.

arXiv:1511.08228, 2015.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord,
Alex Graves, and Koray Kavukcuoglu. Neural machine translation in
linear time. arXiv:1610.10099, 2016.

Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman
Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord,
Sander Dieleman, and Koray Kavukcuoglu. Eﬃcient neural audio synthe-
sis. In ICML, 2018.

Ken-Ichi Kanatani. Group-theoretical methods in image understanding. Springer,

2012.

Zachi Karni and Craig Gotsman. Spectral compression of mesh geometry.

In Proc. Computer Graphics and Interactive Techniques, 2000.

Anees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Dif-
ferentiable graph module (DGM) graph convolutional networks.
arXiv:2002.04999, 2020.

Henry Kenlay, Dorina Thanou, and Xiaowen Dong. Interpretable stability

bounds for spectral graph ﬁlters. arXiv:2102.09587, 2021.

Ron Kimmel and James A Sethian. Computing geodesic paths on manifolds.

PNAS, 95(15):8431–8435, 1998.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-

tion. arXiv:1412.6980, 2014.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

arXiv:1312.6114, 2013.

Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard
Zemel. Neural relational inference for interacting systems. In ICML, 2018.
Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph

convolutional networks. arXiv:1609.02907, 2016a.

Thomas N Kipf and Max Welling. Variational graph auto-encoders.

arXiv:1611.07308, 2016b.

BIBLIOGRAPHY

141

Dmitry B Kireev. Chemnet: a novel neural network based method for
graph/property mapping. J. Chemical Information and Computer Sciences,
35(2):175–180, 1995.

Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional mes-

sage passing for molecular graphs. arXiv:2003.03123, 2020.

Iasonas Kokkinos, Michael M Bronstein, Roee Litman, and Alex M Bronstein.
Intrinsic shape context descriptors for deformable shapes. In CVPR, 2012.
Patrick T Komiske, Eric M Metodiev, and Jesse Thaler. Energy ﬂow networks:
deep sets for particle jets. Journal of High Energy Physics, 2019(1):121, 2019.
Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, and Joan

Bruna. Surface networks. In CVPR, 2018.

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁca-

tion with deep convolutional neural networks. In NIPS, 2012.

Soﬁa Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee,
Ben Glocker, and Daniel Rueckert. Distance metric learning using graph
convolutional networks: Application to functional brain networks. In
MICCAI, 2017.

Dominik Kulon, Riza Alp Guler, Iasonas Kokkinos, Michael M Bronstein,
and Stefanos Zafeiriou. Weakly-supervised mesh-convolutional hand
reconstruction in the wild. In CVPR, 2020.

Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-

access machines. arXiv:1511.06392, 2015.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-
based learning applied to document recognition. Proc. IEEE, 86(11):2278–
2324, 1998.

Reiner Lenz. Group theoretical methods in image processing. Springer, 1990.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Mul-
tilayer feedforward networks with a nonpolynomial activation function
can approximate any function. Neural Networks, 6(6):861–867, 1993.

Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cay-
leynets: Graph convolutional neural networks with complex rational spec-
tral ﬁlters. IEEE Trans. Signal Processing, 67(1):97–109, 2018.

142

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Ron Levie, Elvin Isuﬁ, and Gitta Kutyniok. On the transferability of spectral

graph ﬁlters. In Sampling Theory and Applications, 2019.

Bruno Lévy. Laplace-Beltrami eigenfunctions towards an algorithm that
“understands” geometry. In Proc. Shape Modeling and Applications, 2006.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated

graph sequence neural networks. arXiv:1511.05493, 2015.

Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh Makadia. De-
formable shape completion with graph convolutional autoencoders. In
CVPR, 2018.

Roee Litman and Alexander M Bronstein. Learning spectral descriptors for
deformable shape correspondence. Trans. PAMI, 36(1):171–180, 2013.
Hsueh-Ti Derek Liu, Alec Jacobson, and Keenan Crane. A Dirac operator for
extrinsic shape analysis. Computer Graphics Forum, 36(5):139–149, 2017.
Siwei Lyu and Eero P Simoncelli. Nonlinear image representation using

divisive normalization. In CVPR, 2008.

Richard H MacNeal. The solution of partial diﬀerential equations by means of
electrical networks. PhD thesis, California Institute of Technology, 1949.
Andreas Madsen and Alexander Rosenberg Johansen. Neural arithmetic

units. arXiv:2001.05016, 2020.

Soha Sadat Mahdi, Nele Nauwelaers, Philip Joris, Giorgos Bouritsas, Shun-
wang Gong, Sergiy Bokhnyak, Susan Walsh, Mark Shriver, Michael
Bronstein, and Peter Claes. 3d facial matching by spiral convolutional
metric learning and a biometric fusion-net of demographic properties.
arXiv:2009.04746, 2020.

VE Maiorov. On best approximation by ridge functions. Journal of Approxi-

mation Theory, 99(1):68–94, 1999.

Ameesh Makadia, Christopher Geyer,

Correspondence-free structure from motion.
2007.

and Kostas Daniilidis.
IJCV, 75(3):311–327,

Stéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999.
Stéphane Mallat. Group invariant scattering. Communications on Pure and

Applied Mathematics, 65(10):1331–1398, 2012.

BIBLIOGRAPHY

143

Brandon Malone, Alberto Garcia-Duran, and Mathias Niepert. Learn-
ing representations of missing data for predicting patient outcomes.
arXiv:1811.04752, 2018.

Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant

and equivariant graph networks. arXiv:1812.09902, 2018.

Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Prov-

ably powerful graph networks. arXiv:1905.11136, 2019.

Jean-Pierre Marquis. Category theory and klein’s erlangen program. In From

a Geometrical Point of View, pages 9–40. Springer, 2009.

Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Van-
dergheynst. Geodesic convolutional neural networks on Riemannian
manifolds. In CVPR Workshops, 2015.

James Clerk Maxwell. A dynamical theory of the electromagnetic ﬁeld.
Philosophical Transactions of the Royal Society of London, (155):459–512, 1865.
Jason D McEwen, Christopher GR Wallis, and Augustine N Mavor-Parker.
Scattering networks on the sphere for scalable and rotationally equivariant
spherical cnns. arXiv:2102.02828, 2021.

Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with
invariances in random features and kernel models. arXiv:2102.13219, 2021.
Simone Melzi, Riccardo Spezialetti, Federico Tombari, Michael M Bronstein,
Luigi Di Stefano, and Emanuele Rodolà. Gframes: Gradient-based local
reference frame for 3d shape matching. In CVPR, 2019.

Facundo Mémoli and Guillermo Sapiro. A theoretical and computational
framework for isometry invariant recognition of point cloud data. Founda-
tions of Computational Mathematics, 5(3):313–347, 2005.

Christian Merkwirth and Thomas Lengauer. Automatic generation of com-
plementary descriptors with molecular graph networks. J. Chemical Infor-
mation and Modeling, 45(5):1159–1168, 2005.

Mark Meyer, Mathieu Desbrun, Peter Schröder, and Alan H Barr. Discrete
diﬀerential-geometry operators for triangulated 2-manifolds. In Visualiza-
tion and Mathematics III, pages 35–57. 2003.

Alessio Micheli. Neural network for graphs: A contextual constructive

approach. IEEE Trans. Neural Networks, 20(3):498–511, 2009.

144

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Karla L Miller, Fidel Alfaro-Almagro, Neal K Bangerter, David L Thomas,
Essa Yacoub, Junqian Xu, Andreas J Bartsch, Saad Jbabdi, Stamatios N
Sotiropoulos, Jesper LR Andersson, et al. Multimodal population brain
imaging in the uk biobank prospective epidemiological study. Nature
Neuroscience, 19(11):1523–1536, 2016.

Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to com-

putational geometry. MIT Press, 2017.

Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles
Blundell. Representation learning via invariant causal mechanisms.
arXiv:2010.07922, 2020.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. Nature, 518(7540):529–533, 2015.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves,
Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In ICML, 2016.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan
Svoboda, and Michael M Bronstein. Geometric deep learning on graphs
and manifolds using mixture model cnns. In CVPR, 2017.

Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and
Michael M Bronstein. Fake news detection on social media using geometric
deep learning. arXiv:1902.06673, 2019.

Christopher Morris, Kristian Kersting, and Petra Mutzel. Glocalized
Weisfeiler-Lehman graph kernels: Global-local feature maps of graphs. In
ICDM, 2017.

Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton,
Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman
go neural: Higher-order graph neural networks. In AAAI, 2019.

Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Leman
go sparse: Towards scalable higher-order graph embeddings. In NeurIPS,
2020.

Michael C Mozer. A focused back-propagation algorithm for temporal

pattern recognition. Complex Systems, 3(4):349–381, 1989.

BIBLIOGRAPHY

145

Kevin Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation
for approximate inference: An empirical study. arXiv:1301.6725, 2013.

Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno
Ribeiro. Relational pooling for graph representations. In ICML, 2019.

Ryan L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno
Ribeiro. Janossy pooling: Learning deep permutation-invariant functions
for variable-size inputs. arXiv:1811.01900, 2018.

Vinod Nair and Geoﬀrey E Hinton. Rectiﬁed linear units improve restricted

boltzmann machines. In ICML, 2010.

John Nash. The imbedding problem for Riemannian manifolds. Annals of

Mathematics, 63(1):20––63, 1956.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based ca-

pacity control in neural networks. In COLT, 2015.

Emmy Noether. Invariante variationsprobleme. In König Gesellsch. d. Wiss.

zu Göttingen, Math-Phys. Klassc, pages 235–257. 1918.

Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic symme-

tries of shapes. Computer Graphics Forum, 27(5):1341–1348, 2008.

Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, and
Leonidas Guibas. Functional maps: a ﬂexible representation of maps
between shapes. ACM Trans. Graphics, 31(4):1–11, 2012.

Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosen-
berg, and Jure Leskovec. Pinnersage: Multi-modal user embedding frame-
work for recommendations at pinterest. In KDD, 2020.

Sarah Parisot, Soﬁa Ira Ktena, Enzo Ferrante, Matthew Lee, Ricardo Guer-
rero, Ben Glocker, and Daniel Rueckert. Disease prediction using graph
convolutional networks: application to autism spectrum disorder and
alzheimer’s disease. Medical Image Analysis, 48:117–130, 2018.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the diﬃculty of

training recurrent neural networks. In ICML, 2013.

Giuseppe Patanè. Fourier-based and rational graph ﬁlters for spectral pro-

cessing. arXiv:2011.04055, 2020.

146

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible

inference. Elsevier, 2014.

Roger Penrose. The road to reality: A complete guide to the laws of the universe.

Random House, 2005.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning

of social representations. In KDD, 2014.

Tobias Pfaﬀ, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W
Learning mesh-based simulation with graph networks.

Battaglia.
arXiv:2010.03409, 2020.

Fernando J Pineda. Generalization of back propagation to recurrent and

higher order neural networks. In NIPS, 1988.

Ulrich Pinkall and Konrad Polthier. Computing discrete minimal surfaces

and their conjugates. Experimental Mathematics, 2(1):15–36, 1993.

Allan Pinkus. Approximation theory of the mlp model in neural networks.

Acta Numerica, 8:143–195, 1999.

Tom J Pollard, Alistair EW Johnson, Jesse D Raﬀa, Leo A Celi, Roger G Mark,
and Omar Badawi. The eicu collaborative research database, a freely
available multi-center database for critical care research. Scientiﬁc Data, 5
(1):1–13, 2018.

Javier Portilla and Eero P Simoncelli. A parametric texture model based
on joint statistics of complex wavelet coeﬃcients. International journal of
computer vision, 40(1):49–70, 2000.

Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation. In CVPR,
2017.

Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang.
Network embedding as matrix factorization: Unifying deepwalk, line, pte,
and node2vec. In WSDM, 2018.

H Qu and L Gouskos.
arXiv:1902.08570, 2019.

Particlenet:

jet tagging via particle clouds.

Meng Qu, Yoshua Bengio, and Jian Tang. GMNN: Graph Markov neural

networks. In ICML, 2019.

BIBLIOGRAPHY

147

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-

proving language understanding by generative pre-training. 2018.

Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. Language models are unsupervised multitask learners.
OpenAI blog, 1(8):9, 2019.

Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. Gener-
ating 3D faces using convolutional mesh autoencoders. In ECCV, 2018.
Dan Raviv, Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel.

Symmetries of non-rigid shapes. In ICCV, 2007.

Noam Razin and Nadav Cohen. Implicit regularization in deep learning

may not be explainable by norms. arXiv:2005.06398, 2020.

Scott Reed and Nando De Freitas. Neural programmer-interpreters.

arXiv:1511.06279, 2015.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

Faster r-
cnn: Towards real-time object detection with region proposal networks.
arXiv:1506.01497, 2015.

Danilo Rezende and Shakir Mohamed. Variational inference with normaliz-

ing ﬂows. In ICML, 2015.

Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object

recognition in cortex. Nature neuroscience, 2(11):1019–1025, 1999.

AJ Robinson and Frank Fallside. The utility driven dynamic error propagation

network. University of Cambridge, 1987.

Emma Rocheteau, Pietro Liò, and Stephanie Hyland. Temporal pointwise
convolutional networks for length of stay prediction in the intensive care
unit. arXiv:2007.09483, 2020.

Emma Rocheteau, Catherine Tong, Petar Veličković, Nicholas Lane, and
Pietro Liò. Predicting patient outcomes with graph representation learning.
arXiv:2101.03940, 2021.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional

networks for biomedical image segmentation. In MICCAI, 2015.

Frank Rosenblatt. The perceptron: a probabilistic model for information
storage and organization in the brain. Psychological Review, 65(6):386, 1958.

148

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico
Monti, and Michael Bronstein. Temporal graph networks for deep learning
on dynamic graphs. arXiv:2006.10637, 2020.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bern-
stein, et al. Imagenet large scale visual recognition challenge. IJCV, 115
(3):211–252, 2015.

Raif M Rustamov, Maks Ovsjanikov, Omri Azencot, Mirela Ben-Chen,
Frédéric Chazal, and Leonidas Guibas. Map-based exploration of in-
trinsic shape diﬀerences and variability. ACM Trans. Graphics, 32(4):1–12,
2013.

Tim Salimans and Diederik P Kingma. Weight normalization: A sim-
ple reparameterization to accelerate training of deep neural networks.
arXiv:1602.07868, 2016.

Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia.
Hamiltonian graph networks with ODE integrators. arXiv:1909.12790,
2019.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaﬀ, Rex Ying, Jure
Leskovec, and Peter Battaglia. Learning to simulate complex physics with
graph networks. In ICML, 2020.

Aliaksei Sandryhaila and José MF Moura. Discrete signal processing on

graphs. IEEE Trans. Signal Processing, 61(7):1644–1656, 2013.

Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan
Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network
module for relational reasoning. In NIPS, 2017.

Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski,
Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and
Timothy Lillicrap. Relational recurrent neural networks. arXiv:1806.01822,
2018.

Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
How does batch normalization help optimization? arXiv:1805.11604, 2018.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features

strengthen graph neural networks. arXiv:2002.03155, 2020.

BIBLIOGRAPHY

149

Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivari-

ant graph neural networks. arXiv:2102.09844, 2021.

Anna MM Scaife and Fiona Porter. Fanaroﬀ-Riley classiﬁcation of radio
galaxies using group-equivariant convolutional neural networks. Monthly
Notices of the Royal Astronomical Society, 2021.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. The graph neural network model. IEEE Trans. Neural
Networks, 20(1):61–80, 2008.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis
Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by
planning with a learned model. Nature, 588(7839):604–609, 2020.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.
Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko,
and K-R Müller. Schnet–a deep learning architecture for molecules and
materials. The Journal of Chemical Physics, 148(24):241722, 2018.

Terrence J Sejnowski, Paul K Kienker, and Geoﬀrey E Hinton. Learning
symmetry groups with hidden units: Beyond the perceptron. Physica D:
Nonlinear Phenomena, 22(1-3):260–275, 1986.

Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent
Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson,
Alex Bridgland, et al. Improved protein structure prediction using poten-
tials from deep learning. Nature, 577(7792):706–710, 2020.

Thomas Serre, Aude Oliva, and Tomaso Poggio. A feedforward architecture
accounts for rapid categorization. Proceedings of the national academy of
sciences, 104(15):6424–6429, 2007.

Ohad Shamir and Gal Vardi. Implicit regularization in relu networks with

the square loss. arXiv:2012.05156, 2020.

John Shawe-Taylor. Building symmetries into feedforward networks. In

ICANN, 1989.

John Shawe-Taylor. Symmetries and discriminability in feedforward network

architectures. IEEE Trans. Neural Networks, 4(5):816–826, 1993.

150

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn,
and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 12(9),
2011.

Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural
networks in particle physics. Machine Learning: Science and Technology, 2
(2):021001, 2020.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and
Pierre Vandergheynst. The emerging ﬁeld of signal processing on graphs:
Extending high-dimensional data analysis to networks and other irregular
domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.

Hava T Siegelmann and Eduardo D Sontag. On the computational power of
neural nets. Journal of Computer and System Sciences, 50(1):132–150, 1995.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep
neural networks and tree search. Nature, 529(7587):484–489, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. Mastering the game of go without human knowledge. Nature,
550(7676):354–359, 2017.

Eero P Simoncelli and William T Freeman. The steerable pyramid: A ﬂex-
ible architecture for multi-scale derivative computation. In Proceedings.,
International Conference on Image Processing, volume 3, pages 444–447. IEEE,
1995.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks

for large-scale image recognition. arXiv:1409.1556, 2014.

Alex Smola, Arthur Gretton, Le Song, and Bernhard Schölkopf. A Hilbert

space embedding for distributions. In ALT, 2007.

Stefan Spalević, Petar Veličković, Jovana Kovačević, and Mladen Nikolić.
Hierachial protein function prediction with tail-GNNs. arXiv:2007.12804,
2020.

Alessandro Sperduti. Encoding labeled graphs by labeling RAAM. In NIPS,

1994.

BIBLIOGRAPHY

151

Alessandro Sperduti and Antonina Starita. Supervised neural networks for
the classiﬁcation of structures. IEEE Trans. Neural Networks, 8(3):714–735,
1997.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Ried-
miller. Striving for simplicity: The all convolutional net. arXiv:1412.6806,
2014.

Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence be-
tween positional node embeddings and structural graph representations.
arXiv:1910.00452, 2019.

Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and
Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks
from overﬁtting. JMLR, 15(1):1929–1958, 2014.

Rupesh Kumar Srivastava, Klaus Greﬀ, and Jürgen Schmidhuber. Highway

networks. arXiv:1505.00387, 2015.

Kimberly Stachenfeld, Jonathan Godwin, and Peter Battaglia. Graph net-

works with spectral message passing. arXiv:2101.00079, 2020.

Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres
Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lind-
sey A Carfrae, Zohar Bloom-Ackerman, et al. A deep learning approach
to antibiotic discovery. Cell, 180(4):688–702, 2020.

Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, and Petar

Veličković. Persistent message passing. arXiv:2103.01043, 2021.

Norbert Straumann. Early history of gauge theories and weak interactions.

hep-ph/9609230, 1996.

Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably in-
formative multi-scale signature based on heat diﬀusion. Computer Graphics
Forum, 28(5):1383–1392, 2009.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning

with neural networks. arXiv:1409.3215, 2014.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. Going deeper with convolutions. In CVPR, 2015.

152

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time?

arXiv:1804.11188, 2018.

Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, and Hao Su. Towards
scale-invariant graph-related problem solving by iterative homogeneous
gnns. In NeurIPS, 2020.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu
Mei. Line: Large-scale information network embedding. In WWW, 2015.
Gabriel Taubin, Tong Zhang, and Gene Golub. Optimal surface smoothing

as ﬁlter design. In ECCV, 1996.

Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rémi
Munos, Petar Veličković, and Michal Valko. Bootstrapped representation
learning on graphs. arXiv:2102.06514, 2021.

Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li,
Kai Kohlhoﬀ, and Patrick Riley.
Tensor ﬁeld networks: Rotation-
and translation-equivariant neural networks for 3D point clouds.
arXiv:1802.08219, 2018.

Renate Tobies. Felix Klein—-mathematician, academic organizer, educational

reformer. In The Legacy of Felix Klein, pages 5–21. Springer, 2019.

Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom.

Neural arithmetic logic units. arXiv:1808.00508, 2018.

John Tromp and Gunnar Farnebäck. Combinatorics of go. In International

Conference on Computers and Games, 2006.

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer,

2008.

Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normal-
ization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan,
Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and
Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
arXiv:1609.03499, 2016a.

Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel

recurrent neural networks. In ICML, 2016b.

BIBLIOGRAPHY

153

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In NIPS, 2017.

Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Liò, and Yoshua Bengio. Graph Attention Networks. ICLR, 2018.
Petar Veličković, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles
Blundell. Neural execution of graph algorithms. arXiv:1910.10593, 2019.
Petar Veličković, Lars Buesing, Matthew C Overlan, Razvan Pascanu, Oriol
Vinyals, and Charles Blundell. Pointer graph networks. arXiv:2006.06380,
2020.

Petar Veličković, Wiliam Fedus, William L. Hamilton, Pietro Liò, Yoshua

Bengio, and R Devon Hjelm. Deep Graph Infomax. In ICLR, 2019.

Kirill Veselkov, Guadalupe Gonzalez, Shahad Aljifri, Dieter Galea, Reza
Mirnezami, Jozef Youssef, Michael Bronstein, and Ivan Laponogov. Hy-
perfoods: Machine intelligent mapping of cancer-beating molecules in
foods. Scientiﬁc Reports, 9(1):1–12, 2019.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.

arXiv:1506.03134, 2015.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Se-

quence to sequence for sets. In ICLR, 2016.

Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation

with lipschitz functions. JMLR, 5:669–695, 2004.

Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential

families, and variational inference. Now Publishers Inc, 2008.

Yu Wang and Justin Solomon. Intrinsic and extrinsic operators for shape
In Handbook of Numerical Analysis, volume 20, pages 41–115.

analysis.
Elsevier, 2019.

Yu Wang, Mirela Ben-Chen, Iosif Polterovich, and Justin Solomon. Steklov
spectral geometry for extrinsic shape analysis. ACM Trans. Graphics, 38(1):
1–21, 2018.

Yu Wang, Vladimir Kim, Michael Bronstein, and Justin Solomon. Learning

geometric operators on meshes. In ICLR Workshops, 2019a.

154

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,
and Justin M Solomon. Dynamic graph CNN for learning on point clouds.
ACM Trans. Graphics, 38(5):1–12, 2019b.

Max Wardetzky. Convergence of the cotangent formula: An overview. Dis-

crete Diﬀerential Geometry, pages 275–286, 2008.

Max Wardetzky, Saurabh Mathur, Felix Kälberer, and Eitan Grinspun. Dis-
crete Laplace operators: no free lunch. In Symposium on Geometry Processing,
2007.

Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco
Cohen. 3d steerable cnns: Learning rotationally equivariant features in
volumetric data. arXiv:1807.02547, 2018.

Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical
form and the algebra which appears therein. NTI Series, 2(9):12–16, 1968.

Paul J Werbos. Generalization of backpropagation with application to a

recurrent gas market model. Neural Networks, 1(4):339–356, 1988.

Hermann Weyl. Elektron und gravitation. i. Zeitschrift für Physik, 56(5-6):

330–352, 1929.

Hermann Weyl. Symmetry. Princeton University Press, 2015.

Marysia Winkels and Taco S Cohen. Pulmonary nodule detection in ct scans

with equivariant cnns. Medical Image Analysis, 55:15–26, 2019.

Jeﬀrey Wood and John Shawe-Taylor. Representation theory and invariant

neural networks. Discrete Applied Mathematics, 69(1-2):33–60, 1996.

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. Simplifying graph convolutional networks. In ICML, 2019.

Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.

Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kan-
Inductive representation learning on temporal graphs.

nan Achan.
arXiv:2002.07962, 2020a.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful

are graph neural networks? arXiv:1810.00826, 2018.

BIBLIOGRAPHY

155

Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi,
and Stefanie Jegelka. What can neural networks reason about?
arXiv:1905.13211, 2019.

Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi,
and Stefanie Jegelka. How neural networks extrapolate: From feedforward
to graph neural networks. arXiv:2009.11848, 2020b.

Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and
Milad Heshemi. Neural execution engines: Learning to execute subrou-
tines. arXiv:2006.08084, 2020.

Chen-Ning Yang and Robert L Mills. Conservation of isotopic spin and

isotopic gauge invariance. Physical Review, 96(1):191, 1954.

Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-

supervised learning with graph embeddings. In ICML, 2016.

Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy,
kikuchi approximations, and belief propagation algorithms. NIPS, 2001.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamil-
ton, and Jure Leskovec. Graph convolutional neural networks for web-scale
recommender systems. In KDD, 2018.

Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural

networks. In ICML, 2019.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos,
Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In NIPS, 2017.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv:1410.4615,

2014.

Wei Zeng, Ren Guo, Feng Luo, and Xianfeng Gu. Discrete heat kernel
determines discrete riemannian metric. Graphical Models, 74(4):121–129,
2012.

Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan
Yeung. Gaan: Gated attention networks for learning on large and spa-
tiotemporal graphs. arXiv:1803.07294, 2018.

Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi,
and Le Song. Eﬃcient probabilistic logic reasoning with graph neural
networks. arXiv:2001.11850, 2020.

156

BRONSTEIN, BRUNA, COHEN & VELIČKOVIĆ

Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong
Li, and Jingren Zhou. Aligraph: A comprehensive graph neural network
platform. arXiv:1902.08730, 2019.

Weicheng Zhu and Narges Razavian. Variationally regularized graph-based
representation learning for electronic health records. arXiv:1912.03761,
2019.

Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang.
Deep graph contrastive representation learning. arXiv:2006.04131, 2020.
Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polyphar-
macy side eﬀects with graph convolutional networks. Bioinformatics, 34
(13):i457–i466, 2018.

