0
2
0
2

l
u
J

2
1

]

V
C
.
s
c
[

1
v
6
9
9
5
0
.
7
0
0
2
:
v
i
X
r
a

Diﬀerentiable Programming for Hyperspectral
Unmixing using a Physics-based Dispersion
Model

John Janiczek1, Parth Thaker1, Gautam Dasarathy1, Christopher Edwards2,
Philip Christensen1, and Suren Jayasuriya1

1 Arizona State University
2 Northern Arizona University

Abstract. Hyperspectral unmixing is an important remote sensing task
with applications including material identiﬁcation and analysis. Charac-
teristic spectral features make many pure materials identiﬁable from their
visible-to-infrared spectra, but quantifying their presence within a mix-
ture is a challenging task due to nonlinearities and factors of variation.
In this paper, spectral variation is considered from a physics-based ap-
proach and incorporated into an end-to-end spectral unmixing algorithm
via diﬀerentiable programming. The dispersion model is introduced to
simulate realistic spectral variation, and an eﬃcient method to ﬁt the pa-
rameters is presented. Then, this dispersion model is utilized as a genera-
tive model within an analysis-by-synthesis spectral unmixing algorithm.
Further, a technique for inverse rendering using a convolutional neu-
ral network to predict parameters of the generative model is introduced
to enhance performance and speed when training data is available. Re-
sults achieve state-of-the-art on both infrared and visible-to-near-infrared
(VNIR) datasets, and show promise for the synergy between physics-
based models and deep learning in hyperspectral unmixing in the future.

Keywords: hyperspectral imaging, spectral unmixing, diﬀerentiable pro-
gramming

1

Introduction

Hyperspectral imaging is a method of imaging where light radiance is densely
sampled at multiple wavelengths. Increasing spectral resolution beyond a tra-
ditional camera’s red, green, and blue spectral bands typically requires more
expensive detectors, optics, and/or lowered spatial resolution. However, hyper-
spectral imaging has demonstrated its utility in computer vision, biomedical
imaging, and remote sensing [42,8]. In particular, spectral information is criti-
cally important for understanding material reﬂectance and emission properties,
important for recognizing materials.

Spectral unmixing is a speciﬁc task within hyperspectral imaging with ap-
plication to many land classiﬁcation problems related to ecology, hydrology, and

 
 
 
 
 
 
2

J. Janiczek et al.

mineralogy [29,32]. It is particularly useful for analyzing aerial images from air-
craft or spacecraft to map the abundance of materials in a region of interest.
While pure materials have characteristic spectral features, mixtures require al-
gorithms to identify and quantify material presence.

A common model for this problem is linear mixing, which assumes electro-
magnetic waves produced from pure materials combine linearly and are scaled
by the material abundance. Mathematically this is expressed as b = Ax + η
where b is the observed spectra, A is a matrix whose columns are the pure
material spectra, η is the measurement noise, and x is the abundance of each
pure material. The model assumes that the pure material spectra, referred to
as endmember spectra, is known before-hand. Nonlinear eﬀects are known to
occur when photons interact with multiple materials within a scene for which
we refer readers to the review by Heylen et al. for techniques to account for the
non-linear mixing [29].

A key challenge that aﬀects both linear and nonlinear mixing models is that
pure materials have an inherent variability in their spectral signatures, and thus
cannot be represented by a single characteristic spectrum. Spectral variability of
endmembers is caused by subtle absorption band diﬀerences due to factors such
as diﬀerent grain sizes [43,52,47,48] or diﬀering ratios of molecular bonds [7,56]
as shown in Figure 1. Since variability causes signiﬁcant errors in unmixing
algorithms, it is an active area of research [66,18,71].

Fig. 1: Endmember Variation: Several spectra of olivine are plotted to demon-
strate it’s spectral variability. The olivine mineral is a solid solution with contin-
uous compositional variation of Iron (Fe2) and Magnesium (Mg2) bonds. This
ratio of bonds (indexed by the Fo number), causes absorption bands to shift in
frequency and strength

Recently, diﬀerentiable programming has become a popular research area
due to its potential to bridge gaps between physics-based and machine learning-
based techniques for computer vision and graphics [59,25,1]. Our key insight is to
leverage diﬀerentiable programming by modelling the variation of spectra with
a physics-based dispersion model, and incorporating this diﬀerentiable model
into an end-to-end spectral unmixing algorithm. Such an approach has the ca-
pacity to unmix scenes with a large amount of variability, while constraining
the predictions to be physically plausible. These physically plausible variations

Diﬀerentiable Programming for Hyperspectral Unmixing

3

of endmember spectra also provide additional science data as the variation of
absorption bands can reveal properties about the composition and history of the
material. To our knowledge, we are the ﬁrst to use a generative physics model
to account for spectral variability in an unmixing algorithm.

Contributions: Our speciﬁc contributions in this paper are the following:

– We introduce a physics-based dispersion model (ﬁrst presented in [53,34,61])
to generate and render spectral variation for various pure materials. We pro-
vide an eﬃcient optimization method via gradient descent to ﬁnd dispersion
model parameters for this spectral variation.

– We incorporate this dispersion model into an end-to-end spectral unmix-
ing algorithm utilizing diﬀerentiable programming to perform analysis-by-
synthesis optimization. Analysis-by-synthesis is solved via alternating mini-
mization optimization and requires no training data.

– We further design an inverse rendering algorithm consisting of a convolu-
tional neural network to jointly estimate dispersion model parameters and
mineral abundances for spectral unmixing. This method requires training
data, but is computationally eﬃcient at test time and outperforms analysis-
by-synthesis and other state-of-the-art methods.

We provide extensive analysis of our proposed methods with respect to noise
and convergence criteria. To validate our contributions, we test on both synthetic
and real datasets using hyperspectral observations in the visible and near infrared
(VNIR), and mid to far infrared (IR). The datasets also span three diﬀerent
environments from laboratory, aircraft, and satellite based spectrometers. Our
methods achieve state-of-the-art across all datasets, and we compare against
several baselines from literature. Our code is openly available and accessible here:
https://github.com/johnjaniczek/InfraRender. We hope this work inspires
more fusion between physics models and machine learning for hyperspectral
imaging and computer vision more generally in the future.

2 Related Work

Optimization-based Approaches. Standard optimization techniques for lin-
ear unmixing include projection, non-negative least squares, weighted least squares,
and interior point methods [47,28,51,13,27]. Further, sparsity-based optimization
can improve abundance prediction [69,9]. However, most optimization have not
leveraged physics priors as we do in our model.

Spectral Variability. Spectral variability has been a topic of recent inter-
est [66,6]. One approach is to augment A with multiple variations or spectra
for each endmember. To do this, multiple endmember spectral mixture anal-
ysis (MESMA) [50] and multiple-endmember linear spectral unmixing (MEL-
SUM) [17] both require labeled data of the spectral variation for each endmem-
ber. In contrast, unsupervised techniques learn endmember sets from unlabelled

4

J. Janiczek et al.

hyperspectral images, including semi-automated techniques [3], k-means cluster-
ing [3], and the sparsity promoting iterated constrained endmember algorithm
(SPICE) [64,65] which simultaneously ﬁnds endmember sets while unmixing for
material abundances. These techniques are limited by the amount of sets in
the endmember library, and computational complexity increases with more ad-
ditions. Our method by contrast ﬁnds an eﬃcient parameter set to physically
model the spectral variation.

Another category of endmember variability techniques models the endmem-
ber spectral variation as samples from a multivariate distribution P(e|θ) where
e is the endmember spectra, and θ are the distribution parameters. Common
statistical distributions proposed include the normal compositional model [54],
Gaussian mixture models [71], and the beta compositional model [18]. These
distribution models have large capacity to model spectral variations, however
sometimes they can render endmember spectra that are not physically realistic.
Deep Learning for Hyperspectral Classiﬁcation and Unmixing. Deep
learning has recently improved many hyperspectral imaging tasks [68,37]. In
particular, networks process hyperspectral pixel vectors using both deep belief
networks [40] and CNNs [30,10,62]. For spatial hyperspectral data, CNNs [12],
joint spectral-spatial feature extraction [70], and 3D CNNs [39] are used. All
these methods require large hyperspectral datasets that are annotated correctly.
One of our methods uses analysis-by-synthesis and diﬀerentiable programming
to avoid low training data issues, but our technique can be made complementary
to deep learning architectures as we show in our inverse rendering CNN.

Diﬀerentiable Programming and Rendering. Diﬀerentiable program-
ming refers to the paradigm of writing algorithms which can be fully diﬀerenti-
ated end-to-end using automatic diﬀerentiation for any parameter [59,60,4]. This
has been applied for audio [20] and 3D geometry processing [49]. In graphics,
diﬀerentiable rendering has improved ray tracing [38,44,41,67], solved analysis-
by-synthesis problems in volumetric scattering [26,25], estimated reﬂectance and
lighting [1], and performed 3D reconstruction [58]. In our paper, we write a for-
ward imaging model utilizing the physics of dispersion in spectral variation to
allow our pipeline to be diﬀerentiable end-to-end.

3 Method

Our approach to hyperspectral unmixing features two main components: (1)
use of a physically-accurate dispersion model for pure endmember spectra, and
(2) a diﬀerentiable programming pipeline to perform spectral unmixing. This ap-
proach has synergistic beneﬁts of leveraging prior domain knowledge while learn-
ing from data. Our ﬁrst algorithm solves spectral unmixing in a self-supervised
fashion using analysis-by-synthesis optimization with the dispersion model as
the synthesis step. Further, we show how inverse rendering via a convolutional
neural network (CNN) can learn parameters of this model to help speed up our
end-to-end pipeline and improves performance when training data is available.

Diﬀerentiable Programming for Hyperspectral Unmixing

5

3.1 Dispersion Model

We ﬁrst describe the dispersion model for generating endmember spectra. End-
member and/or endmember spectra is what we call the spectral curve for
emissivity (cid:15) as a function of wavenumber ω. Each pure material has a charac-
teristic endmember spectrum, although spectra can vary, which is the problem
we are trying to solve/disambiguate. Let εmeasured(ω) be endmember spectra
we have measured, typically in a lab or in the ﬁeld, whose emissivity is sam-
pled at diﬀerent wavenumbers: (cid:2)εmeasured(ω1), · · · εmeasured(ωN )(cid:3)T
. Our goal is
to propose a model εmodel(Λ; ω) with parameters Λ such that the following loss
is minimized: L(Λ) = (cid:80)N
. That is, we ﬁt the
i=1
model emissivity of an endmember spectrum to the measured spectrum. In prac-
tice, we need to add regularization and constraints to this endmember loss for
better ﬁtting which we describe after the derivation of the dispersion model.

(cid:0)εmeasured(ωi) − εmodel(Λ; ωi)(cid:1)2

Derivation of the Dispersion model: Our model of endmember spectra is
derived from an atomistic oscillator driven by electromagnetic waves impinging
on the molecular structure of the pure material [53,34]. In Figure 2, we show a
conceptual diagram of this model, and how it generates emissivity curves as a
function of wavelength. For the full derivation of the model from ﬁrst principles,
we refer the reader to Appendix A in the supplementary material. Instead, we
outline the model below based on the equations derived from that analysis.

Fig. 2: Dispersion Model Concept Figure: The insight of the dispersion
model is that optical properties can be related to molecular structure through
ﬁrst principles via an atomistic oscillator model. We use this generative model
for the formation of spectral variation in our spectral unmixing algorithm

Let Λ = [ρ, ωo, γ, (cid:15)r] be a matrix of parameters, where ρ, ω0, γ, (cid:15)r ∈ RK
and K is a model hyperparameter corresponding to the number of distinct mass-
spring equations used to model the emissivity. ρ is the band strength, ωo is
the resonant frequency, γ is the frictional force (dampening coeﬃcient), and (cid:15)r
is relative dielectric permeability. Please see the supplemental material for the
physical signiﬁcance of these parameters to the atomistic oscillator model, and
their control over the shape of spectral absorption bands. Note: usually (cid:15)r is a
constant vector which does not vary with K. Thus Λ ∈ RK×4. The refractive

6

J. Janiczek et al.

index terms n, k are given as follows [53,34]:

n(Λ; ω) =

(cid:114)

θ + b
2

,

k(Λ; ω) =

φ
n(Λ; ω)

,

where the expressions for θ, b, φ are given as follows:

θ = (cid:15)r +

K
(cid:88)

k=1

4πρkω2
0k

(ω2
− ω2)
0k
− ω2)2 + γ2

kω2
0k

ω2 ,

(ω2
0k

(cid:112)

b =

θ2 + 4φ2, φ =

K
(cid:88)

k=1

2πρkω2
0k

γkω0k ω
− ω2)2 + γ2

kω2
0k

ω2 .

(ω2
0k

(1)

(2)

(3)

We note that subscript k denotes the k-th coordinate of the corresponding vector.
Also there is another useful relation (derived in Appendix A) that n2 − k2 =
θ, nk = φ. We then deﬁne the complex refractive index as ˆn(Λ; ω) = n(Λ; ω) −
i · k(Λ; ω), where i =
−1 is the imaginary number. Hence, we can calculate the
emissivity as follows:

√

(cid:15)(Λ; ω) = 1 − R(Λ; ω),

where R(Λ; ω) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆn(Λ; ω) − 1
ˆn(Λ; ω) + 1

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(4)

When considering minerals, we introduce M ∈ N, the number of optical axes
of symmetry in crystal structures, (eg. 2 axes of symmetry in quartz [53,61]), to
deﬁne the full model:

εmodel(Λ; ω) =

M
(cid:88)

m=1

αm · (cid:15)(Λm; ω)

such that

M
(cid:88)

m=1

αm = 1, αm ≥ 0,

(5)

where we use a diﬀerent parameter matrix Λm and weight αm for each optical
axis of symmetry.

The dispersion model has been primarily used to analyze optical properties
of materials to determine n and k, which then can be subsequently applied to
optical models like radiative transfer [53,61]. After n and k are found, spectra
such as reﬂectance, emissivity, and transmissivity can be generated. In particu-
lar, we notice that ﬁne-grained control of the dispersion model parameters can
realistically render spectral variation that occurs in hyperspectral data. Our con-
tribution is to leverage these properties in a diﬀerentiable programming pipeline
for spectral unmixing.

Endmember Fitting. Using the dispersion model presented above, we want
to robustly estimate the model parameters to ﬁt the spectra εmeasured captured
in a lab or in the ﬁeld. To ﬁt the model, we wish to perform gradient descent to
eﬃciently ﬁnd these parameters. Using chain rule on the loss function, we see that
∂L
, where (ij) corresponds to that element of the parameter
∂Λij
matrix, and all expressions are scalars once the coordinate is speciﬁed. While
the partial derivatives can be calculated explicitly via symbolic toolboxes, the

∂εmodel
∂Λij

= ∂L

∂εmodel

Diﬀerentiable Programming for Hyperspectral Unmixing

7

resulting expressions are too long to be presented here. For simplicity and ease
of use, we use the autograd function [4,45] in PyTorch [46] to automatically
compute derivatives for our model as we are performing backpropagation.

One main challenge in performing endmember ﬁtting is that the dispersion
model is not an injective function, and hence is typically not identiﬁable, that
is more than one Λ can result in the same ﬁt. This can be solved, in-part,
through regularization to enforce sparsity, especially since a preference for fewer
dispersion parameters has been suggested in the literature [53,61]. In our imple-
mentation, we initialize our model with K = 50 rows of the parameter matrix.
Since the parameter ρ controls the strength of the absorption band, small val-
ues of ρ do not contribute much energy to the spectra (unnecessary absorption
bands), and can be pruned. After performing sparse regression by penalizing the
L1 norm of ρ, K is typically around 10-15 in our experiments.

Thus, our modiﬁed sparse regression problem may be written as

arg min
Λmin≤Λ≤Λmax

N
(cid:88)

i=1

(cid:0)(cid:15)real(ωi) − (cid:15)model(Λ; ωi)(cid:1)2

+ λρ||ρ||1,

(6)

where Λmin and Λmax restrict the variation of the dispersion parameters to
a plausible range. In addition, endmembers (particularly minerals) can have
multiple optical axes of symmetry described by separate spectra, which has
been noted in the literature [53,61]. Without prior knowledge of the number of
axes for every material we encounter, we run this optimization for a single and
double axes, and pick the one with the lowest error. See Section 4 for results on
endmember ﬁtting and Figure 6 for examples of modelled vs. measured spectra.
Despite the fact that this regression problem is non-convex, we solve it using
gradient descent with a random initialization; this is known to converge to a
local minimum with probability 1 [35]. A global minimum is not necessary at
this stage, since we use endmember ﬁtting to provide a good initialization point
for the subsequent alternating minimization procedure introduced in the next
subsection.

3.2 Diﬀerentiable Programming for End-to-End Spectral Unmixing

Analysis-by-Synthesis Optimization. In Figure 3, we show our full end-
to-end spectral unmixing pipeline. Here, εmodel(Λ; ω), which is initially ﬁt to
εmeasured, is then aggregated into the columns of A. Then, the observed spec-
tra b is linearly unmixed by solving a regularized least-squares optimization:
argminx(cid:107)b−Ax(cid:107)2
2 +λ(cid:107)x(cid:107)p subject to sum-to-one and non-negativity constraints
(cid:107)x(cid:107)1 = 1, x ≥ 0. Given these constraints, one cannot impose sparsity with the
usual L1 norm. Instead, we use the Lp norm to induce sparsity for the predicted
abundances; this has been proposed before for spectral unmixing [9].

The key to our pipeline is that everything is fully diﬀerentiable, and thus we

can actually minimize the following equation:

arg min
x,Λ∈[Λmin,Λmax]

(cid:107)b − A(Λ)x(cid:107)2

2 + λ(cid:107)x(cid:107)p such that (cid:107)x(cid:107)1 = 1, x ≥ 0.

(7)

8

J. Janiczek et al.

Fig. 3: Analysis-by-Synthesis: The analysis-by-synthesis algorithm uses dif-
ferentiable programming to ﬁnd optimal dispersion parameters and abundances.
The initial dispersion parameters and the target spectra are fed as inputs, and
the algorithm alternates between optimizing the abundances in the least squares
sense and updating the dispersion parameters with respect to the gradient

with respect to both the parameters of the dispersion model Λ and the abun-
dances x. This gives us our recipe for hyperspectral unmixing: ﬁrst, perform
endmember ﬁtting to initialize A(Λ), then, solve Equation 7 in an alternating
fashion for x and Λ. One could also solve this equation jointly for both unknowns,
however, we found that the alternating optimization was faster and converged
to better results.

The optimization problem established in equation (7) is an alternating min-
imization problem and is unfortunately not convex [31]. One popular approach
to tackle nonconvex problems is to ﬁnd a good initialization point [19,5], and
then execute a form of gradient descent. Inspired by this, we ﬁrst initialize A(Λ)
by performing endmember ﬁtting using Equation 6 as described in the previous
subsection. Our experiments indicate that this provides a useful initialization for
our subsequent step. We then perform alternating minimization on Equation 7
for x and Λ. Note that each iterate of the resulting alternating minimization in-
volves the solution of a subproblem which has a convergence rate which depends
on the condition number of the matrix A(Λ). For more details on this, we refer
the reader to Appendix C where we discuss on the properties of A(Λ) across
multiple runs.

In the ideal scenario, this initial matrix A(Λ) would consist of the endmem-
ber spectra that fully characterizes the mixed spectra b. However, since spectra
for the same material can signiﬁcantly vary [43,52,47,48,7,56] (see Figure 1), the
initialization can be slightly oﬀ and we follow up with (7) to obtain a better ﬁt.
Note that this optimization problem is solving for the maximum likelihood esti-
mator under a Gaussian noise model. Our optimization technique is performing
analysis-by-synthesis, as given a single observation b, the dispersion model
synthesizes endmember variation until a good ﬁt is achieved.

Inverse Rendering of Dispersion Model Parameters. The previous
analysis-by-synthesis optimization does not require training data (labeled abun-

Diﬀerentiable Programming for Hyperspectral Unmixing

9

dances in spectral mixtures) in order to perform spectral unmixing. However,
there is room for even more improvement by using labeled data to help improve
the parameter ﬁtting of the model in the synthesis step. We train a CNN to pre-
dict the parameters for a generative model, known as inverse rendering in other
domains [63]. In Figure 4, we show this inverse rendering conceptually, and how
it can be fed into our diﬀerentiable programming pipeline for end-to-end spectral
unmixing.

Our CNN architecture consists of convolutional layers followed by a series of
fully-connected layers. We refer the reader to the supplemental material for the
exact network structure and implementation details. Using a CNN for inverse
rendering is signiﬁcantly faster at test time as compared to the analysis-by-
synthesis optimization. However, it does have a drawback of requiring training
data which is unavailable for certain tasks/datasets.

Fig. 4: Inverse Rendering: A CNN is trained to “inversely render” pixels of the
hyperspectral image, by predicting both the dispersion parameters that con-
trol the spectral variability, and the abundances that control the mixing model.
During training, the reconstruction error is back-propagated through the diﬀer-
entiable dispersion model to boost the performance of the network at making
physically realistic predictions

4 Experimental Results

Datasets. We utilize three separate datasets to validate our spectral unmixing
algorithms. In Figure 5, we visually represent these datasets and their exemplar
data. For speciﬁc implementation details and dataset pre-processing, please see
Appendix D in the supplemental material.

Feely et al. Dataset. We utilize 90 samples from the Feely et al. dataset [21]
of thermal emission spectra in the infrared for various minerals measured in the
lab. Ground truth was determined via optical petrography [21], and a labeled
endmember library is provided. The limited amount of data is challenging for
machine learning methods, so we utilize the dispersion model to generate 50,000
additional synthetic spectra for dataset augmentation.

Gulfport dataset. The Gulfport dataset from Gader et al. [22] contains hy-
perspectral aerial images in the VNIR along with ground truth classiﬁcation

10

J. Janiczek et al.

labels segmenting pixels into land types (e.g. grass, road, building). Although
the dataset is for spectral classiﬁcation, it can also be used to benchmark unmix-
ing algorithms by creating synthetic mixtures of pure pixels from the Gulfport
dataset with random abundances as done by [18,71]. We perform both spectral
classiﬁcation (Gulfport) and unmixing (Gulfport synthetic) tasks in our test-
ing. Both datasets are split into a train and test set (although some methods
do not require training data), and the training data is augmented with 50,000
synthetically generated mixtures from the dispersion model.

One main diﬃculty of this dataset is the endmembers identiﬁed correspond
to coarse materials such as grass and road as opposed to pure materials. Such
endmembers can signiﬁcantly vary across multiple pixels, but this spectral vari-
ation is not physically described by the dispersion model. To solve this problem,
we utilize K-means clustering to learn examplar endmembers for each category
(e.g. grass, road, etc). Then the resulting centroid endmember can be ﬁt to the
dispersion model to allow further variation such as absorption band shifts in the
spectra. We found that K = 5 worked the best for the Gulfport dataset.

TES Martian Dataset. The Thermal Emission Spectrometer (TES) [14] uses
Fourier Transform Infrared Spectroscopy to measure the Martian surface. We
utilize pre-processing from Bandﬁeld et al. [2], and the endmember library used
by Rogers et al. to analyze Mars TES data [51]. There is no ground truth for this
dataset, as the true abundance of minerals on the Martian surface is unknown,
so other metrics such as reconstruction error of the spectra are considered.

Fig. 5: Datasets: This ﬁgure shows representative data and instrumentation for
the three datasets considered in this paper. Data includes laboratory, aircraft,
and satellite measurements, and ground truth ranges from detailed abundance
analysis under a microscope (Feely [21]) to pure pixel labels of land type for
spectral classiﬁcation (Gulfport [22]) to no ground truth for the Martian data
(TES [14])

Baselines. We compare against several state-of-the-art baselines in the lit-
erature. The basic linear unmixing algorithm is Fully Constrained Least Squares

Diﬀerentiable Programming for Hyperspectral Unmixing

11

(FCLS) [28] which solves least squares with sum-to-one and non-negativity con-
straints on the abundances. We also implement two state-of-the-art statistical
methods for modelling endmember variability as distributions: the Normal Com-
positional Model (NCM) [54] and the Beta Compositional Model (BCM) [18].
NCM and BCM use a Gaussian and Beta distribution respectively, perform
expectation-maximization for unmixing, and require a small amount of training
data to determine model parameters.

We also compare against two state-of-the-art deep learning networks by
Zhang et al. [69]. The ﬁrst network utilizes a 1D CNN (CNN-1D) architecture,
while the second network utilizes a 3D CNN (but with 1D convolutional kernels)
(CNN-3D). CNN-3D is only applicable to datasets with spatial information, and
not testable on the Feely and Gulfport synthetic data. We further created a
modiﬁed CNN architecture (CNN-1D Modiﬁed) to maximize the performance
on our datasets by changing the loss function to MSE, removing max-pooling
layers, and adding an additional fully connected layer before the output. In
the supplemental material, we provide information about the parameters, net-
work architectures, and training procedures we used for these baselines as well
as details for our own methods. We also have all of our code available here:
https://github.com/johnjaniczek/InfraRender.

Fig. 6: Endmember Fitting: (Left) Measured and modelled spectra for a
quartz sample in the IR. (Right) Cluster centroids found for pixels labelled as
grass in the Gulfport dataset, and the model ﬁt to these centroids. Note the high
ﬁdelity of ﬁt via the dispersion model for both these cases

Endmember Fitting Results. To bootstrap both the analysis-by-synthesis
and inverse rendering algorithms, good initial conditions for the dispersion pa-
rameters need to be input to the model. Determining dispersion parameters
typically required detailed molecular structure analysis or exhaustive parameter
searching methods [53,61,36]. One main advantage of our method is that we
utilize gradient descent to eﬃciently ﬁnd parameter sets for diﬀerent materials.
In Appendix B of the supplemental material, we share some of these parameter
sets and our insights using the dispersion model for the scientiﬁc community.

In Figure 6, we show qualitative results of our endmember ﬁtting by mini-
mizing the loss in Equation 6 using gradient descent. The reconstructed spectra

12

J. Janiczek et al.

achieves a low MSE with the measured spectra with an average MSE of 0.016
for the TES library, 0.0019 for the Feely library, and an MSE of 2.6e-5 on the
Gulfport cluster centroids. Note that there is noise in the measurements, and so
MSE is not an absolute metric of the ﬁt to the true unknown spectra.

Spectral Unmixing Results. In Table 1, we show results on the Feely,
Gulfport, and the Gulfport synthetic mixture datasets. For Feely, the analysis-
by-synthesis method achieved a MSE of 0.052, with the next closest method
(NCM) achieving 0.119. Due to the Feely dataset only containing 90 test samples,
the machine learning methods were trained on synthetic data which explains
their lower performance as data mismatch. Thus, the low error of analysis-by-
synthesis shows the utility of the dispersion model for modelling endmember
variability, particularly in cases with low training data.

For the Gulfport dataset, the task was to predict the material present since
the labeled data is for single coarse materials (e.g. road, grass, etc) at 100%
abundance per pixel. Here, the deep learning methods of CNNs and Inverse
Rendering have the highest performance. This is expected as there exists a large
amount of training data to learn from. Note that Inverse Rendering performs the
best at 0.272 MSE, demonstrating that the addition of a generative dispersion
model to the output of the CNN improves performance over purely learned
approaches. Also note that our analysis-by-synthesis method still has relatively
high performance (0.45 MSE) without using any training data at all.

For the Gulfport synthetic mixture dataset, Inverse Rendering achieves the
lowest MSE of 0.059, leveraging both physics-based modeling for spectral mixing
as well as learns from available training data. The BCM and the analysis-by-
synthesis methods both outperform the CNN methods, even though they do not
have access to the training data. In fact, BCM even slightly outperforms the
analysis-by-synthesis method, which could be because the sources of variation
in this data are well-described by statistical distributions.

Table 1: Results: Table - Mean squared error of the abundance predictions
vs. ground truth for Feely, Gulfport, and Gulfport synthetic datasets. The bold
entries indicate top performance

Dataset FCLS
[28]

NCM
[54]

BCM
[18]

CNN-1D
[69]

CNN-3D
[69]

CNN-1D
Modiﬁed

Analysis-
by-
synthesis
0.052

Inverse
Rendering

0.188

0.121 0.119 0.131

0.469

N/A

0.205

0.75

0.799 0.800

1.000

0.497

0.297

0.45

0.272

0.911 0.471 0.136

0.824

N/A

0.148

0.147

0.059

Feely
[21]
Gulfport
[22]
Gulfport
Synthetic

Speed of Methods. The additional capacity of adding statistical and physical
models usually has a cost of speed in implementation. Averaged over 90 mixtures,

Diﬀerentiable Programming for Hyperspectral Unmixing

13

the convergence for a single operation was FCLS - 10ms, BCM - 1.23s, NCM -
18ms, CNN - 33ms, Inverse Rendering - 39ms, and analysis-by-synthesis - 10.2s.
Future work could potentially increase the speed of analysis-by-synthesis with
parallel processing.

Noise analysis. Prior to spectral unmixing, emissivity is separated from
radiance by dividing out the black-body radiation curve at the estimated tem-
perature [47,16]. In general, a Gaussian noise proﬁle in the radiance space with
variance σ2
radiance results in wavenumber dependent noise source in the emissivity
space with the proﬁle σ2(ω) = σ2
radiance · 1/B(ω, T ) where B is the black-body
function given by Planck’s law. In our noise experiments we use a black-body
radiation curve for a 330K target, which is the approximate temperature the
Feely dataset samples were held at. In Figure 7 left, we see that the emissivity
noise is higher where the radiance signal is lower.

We simulated varying the noise power to determine the methods’ robustness
tested on 30 samples from the Feely dataset. In Figure 7, you can see that
analysis-by-synthesis still has the best performance in the presence of noise, and
is relatively ﬂat as noise increases compared to other methods. We note that
statistical methods, while having higher average error, seem to be robust to
increased noise as they can handle random perturbations of each spectral band
statistically. CNN and Inverse Rendering methods perform the worst for high
noise, as these methods were trained on data without noise.

Fig. 7: The left plot shows the radiance proﬁle of a spectra perturbed by Gaussian
noise and the resulting emissivity proﬁle after separating out the blackbody
radiance. The right ﬁgure shows the robustness of the algorithms to increasing
amounts of noise

TES Data. The Mars TES data was unmixed using our analysis-by-synthesis
method to demonstrate it’s utility on tasks where zero training data is available.
The method produces mineral maps which correctly ﬁnds abundances of the
mineral hematite at Meridiani Planum in Figure 8. This is an important Martian
mineral which provides evidence for liquid water having existed at some point on
Mars, and has been veriﬁed by NASA’s Opportunity Rover [33]. Note how FCLS
predicts many sites for hematite, while our method narrows down potential sites

14

J. Janiczek et al.

on the Martian surface, which is useful for planetary scientists. By allowing for
spectral variation through our physics-based approach, our method has lower
RMS reconstruction error than previous analysis of TES data. FCLS, which
was previously used on TES because of the zero training-data problem, has an
average RMS reconstruction error of 0.0043 while analysis-by-synthesis has an
average of 0.0038. This is an exciting result as our methods could provide a new
suite of hyperspectral analysis tools for scientists studying the Martian surface.

Fig. 8: Martian Surface Map: The images show the mineral map for hematite
of the Martian surface produced by FCLS (left) and analysis-by-synthesis (right)
using TES data. Both algorithms ﬁnd the known deposit of hematite on Merid-
iani Planum, but analysis-by-synthesis predicts a sparser map which matches
expectated distributions.

5 Discussion

This paper incorporated generative physics models into spectral unmixing al-
gorithms via diﬀerentiable programming. We adopt a physics-based dispersion
model to simulate spectral variability, and show how this model can realistically
ﬁt several real measured spectra via gradient descent. We further show how to
jointly optimize for the dispersion parameters and material abundances with an
analysis-by-synthesis optimization. A second algorithm is introduced for tasks
where additional data is available by training a CNN to “Inversely Render” a
hyperspectral image with the diﬀerentiable dispersion model in the loop.

We validate these contributions extensively on three datasets ranging from
mid to far IR and VNIR, and compared against state-of-the-art optimization,
statistical and deep learning benchmarks. From these experiments we observe
that analysis-by-synthesis has the best performance when training data is not
available, and that Inverse Rendering has the best performance when training
data is available. We also see that analysis-by-synthesis is noise resilient, and
reconstructs Mars spectra with lower error than previous techniques.

There are still limitations for the methods proposed. First, analysis-by-synthesis

has a large computational cost compared to other methods, although this could

Diﬀerentiable Programming for Hyperspectral Unmixing

15

be mitigated through parallelization of the algorithm. Secondly, the spectral un-
mixing community is limited by the training data available. This is diﬃcult to
overcome, because it is expensive to produce quality datasets, and it is not easy
for experts to label remote sensing datasets from prior knowledge alone. Future
work could investigate generating realistic synthetic data suitable for training
machine learning based algorithms for better performance. We hope using gen-
erative physics-based models inspires others to produce realistic synthetic data
as well as diﬀerentiable programming methods which require low training data.
Acknowledgements. This work was supported by NSF grant IIS-1909192
as well as GPU resources from ASU Research Computing. We thank Dr. Alina
Zare, Christopher Haberle, and Dr. Deanna Rogers for their helpful discussions,
and Kim Murray (formerly Kim Feely) for providing the laboratory measure-
ments and analysis contributing to this paper.

16

J. Janiczek et al.

References

1. Azinovic, D., Li, T.M., Kaplanyan, A., Niessner, M.: Inverse path tracing for joint
material and lighting estimation. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. pp. 2447–2456 (2019)

2. Bandﬁeld, J.L.: Global mineral distributions on mars. Journal of Geophysical Re-

search: Planets 107(E6), 9–1 (2002)

3. Bateson, C.A., Asner, G.P., Wessman, C.A.: Endmember bundles: A new approach
to incorporating endmember variability into spectral mixture analysis. IEEE Trans-
actions on Geoscience and Remote Sensing 38(2), 1083–1094 (2000)

4. Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M.: Automatic diﬀeren-
tiation in machine learning: a survey. The Journal of Machine Learning Research
18(1), 5595–5637 (2017)

5. Bhojanapalli, S., Kyrillidis, A., Sanghavi, S.: Dropping convexity for faster semi-
deﬁnite optimization. In: Conference on Learning Theory. pp. 530–582 (2016)
6. Borsoi, R.A., Imbiriba, T., Bermudez, J.C.M., Richard, C., Chanussot, J.,
Drumetz, L., Tourneret, J.Y., Zare, A., Jutten, C.: Spectral variability in hyper-
spectral data unmixing: A comprehensive review. arXiv preprint arXiv:2001.07307
(2020)

7. Burns, R.G.: Crystal ﬁeld spectra and evidence of cation ordering in olivine min-
erals. American Mineralogist: Journal of Earth and Planetary Materials 55(9-10),
1608–1632 (1970)

8. Chang, C.I.: Hyperspectral imaging: techniques for spectral detection and classiﬁ-

cation, vol. 1. Springer Science & Business Media (2003)

9. Chen, F., Zhang, Y.: Sparse hyperspectral unmixing based on constrained lp-l
2 optimization. IEEE Geoscience and Remote Sensing Letters 10(5), 1142–1146
(2013)

10. Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P.: Deep feature extraction and clas-
siﬁcation of hyperspectral images based on convolutional neural networks. IEEE
Transactions on Geoscience and Remote Sensing 54(10), 6232–6251 (2016)

11. Chen, Y., Chi, Y., Fan, J., Ma, C.: Gradient descent with random initialization:
Fast global convergence for nonconvex phase retrieval. Mathematical Programming
176(1-2), 5–37 (2019)

12. Cheng, G., Li, Z., Han, J., Yao, X., Guo, L.: Exploring hierarchical convolutional
features for hyperspectral image classiﬁcation. IEEE Transactions on Geoscience
and Remote Sensing 56(11), 6712–6722 (2018)

13. Chouzenoux, E., Legendre, M., Moussaoui, S., Idier, J.: Fast constrained least
squares spectral unmixing using primal-dual interior-point optimization. IEEE
Journal of Selected Topics in Applied Earth Observations and Remote Sensing
7(1), 59–69 (2014)

14. Christensen, P., Bandﬁeld, J., Hamilton, V.E., Ruﬀ, S.W., Kieﬀer, H.H., Titus,
T.N., Malin, M.C., Morris, R.V., Lane, M.D., Clark, R.L., Jakosky, B.M., Mel-
lon, M.T., Pearl, J.C., Conrath, B.J., Smith, M.D., Clancy, R.T., Kuzmin, R.O.,
Roush, T., Mehall, G.L., Gorelick, N., Bender, K., Murray, K., Dason, S., Greene,
E., Silverman, S., Greenﬁeld, M.: Mars global surveyor thermal emission spectrom-
eter experiment: investigation description and surface science results. Journal of
Geophysical Research: Planets 106(E10), 23823–23871 (2001)

15. Christensen, P.R., Bandﬁeld, J.L., Hamilton, V.E., Howard, D.A., Lane, M.D.,
Piatek, J.L., Ruﬀ, S.W., Stefanov, W.L.: A thermal emission spectral library of
rock-forming minerals. Journal of Geophysical Research: Planets 105(E4), 9735–
9739 (2000)

Diﬀerentiable Programming for Hyperspectral Unmixing

17

16. Christensen, P.R., Hamilton, V.E., Mehall, G., Pelham, D., ODonnell, W., Anwar,
S., Bowles, H., Chase, S., Fahlgren, J., Farkas, Z., Fisher, T., James, I., Kubik,
I., Lazbin, M., Miner, M., Rassas, L., Schulze, K., Shamordola, T., Tourville, G.,
West, R.and Woodward, D., Lauretta: The osiris-rex thermal emission spectrom-
eter (otes) instrument. Space Science Reviews 214(5), 87 (2018)

17. Combe, J.P., Le Mou´elic, S., Sotin, C., Gendrin, A., Mustard, J., Le Deit, L.,
Launeau, P., Bibring, J.P., Gondet, B., Langevin, Y., Pinet, P.: Analysis of
omega/mars express data hyperspectral data using a multiple-endmember linear
spectral unmixing model (melsum): Methodology and ﬁrst results. Planetary and
Space Science 56(7), 951–975 (2008)

18. Du, X., Zare, A., Gader, P., Dranishnikov, D.: Spatial and spectral unmixing using
the beta compositional model. IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing 7(6), 1994–2003 (2014)

19. Duchi, J.C., Ruan, F.: Solving (most) of a set of quadratic equalities: Composite
optimization for robust phase retrieval. Information and Inference: A Journal of
the IMA 8(3), 471–529 (2019)

20. Engel, J., Hantrakul, L.H., Gu, C., Roberts, A.: Ddsp: Diﬀerentiable digital signal
processing. In: International Conference on Learning Representations (2020)
21. Feely, K.C., Christensen, P.R.: Quantitative compositional analysis using thermal
emission spectroscopy: Application to igneous and metamorphic rocks. Journal of
Geophysical Research: Planets 104(E10), 24195–24210 (1999)

22. Gader, P., Zare, A., Close, R., Aitken, J., Tuell, G.: Muuﬂ gulfport hyperspectral
and lidar airborne data set. Univ. Florida, Gainesville, FL, USA, Tech. Rep. REP-
2013-570 (2013)

23. Garbuny, M.: Optical physics. Optical Physics by Max Garbuny New York, NY:

Academic Press, 1965 (1965)

24. Ge, R., Jin, C., Zheng, Y.: No spurious local minima in nonconvex low rank prob-
lems: A uniﬁed geometric analysis. In: Proceedings of the 34th International Con-
ference on Machine Learning-Volume 70. pp. 1233–1242. JMLR. org (2017)

25. Gkioulekas, I., Levin, A., Zickler, T.: An evaluation of computational imaging tech-
niques for heterogeneous inverse scattering. In: European Conference on Computer
Vision. pp. 685–701. Springer (2016)

26. Gkioulekas, I., Zhao, S., Bala, K., Zickler, T., Levin, A.: Inverse volume rendering
with material dictionaries. ACM Transactions on Graphics (TOG) 32(6), 162
(2013)

27. Goudge, T.A., Mustard, J.F., Head, J.W., Salvatore, M.R., Wiseman, S.M.: In-
tegrating crism and tes hyperspectral data to characterize a halloysite-bearing
deposit in kashira crater, mars. Icarus 250, 165–187 (2015)

28. Heinz, D.C., et al.: Fully constrained least squares linear spectral mixture analysis
method for material quantiﬁcation in hyperspectral imagery. IEEE Transactions
on Geoscience and Remote Sensing 39(3), 529–545 (2001)

29. Heylen, R., Parente, M., Gader, P.: A review of nonlinear hyperspectral unmixing
methods. IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 7(6), 1844–1868 (2014)

30. Hu, W., Huang, Y., Wei, L., Zhang, F., Li, H.: Deep convolutional neural networks

for hyperspectral image classiﬁcation. Journal of Sensors 2015 (2015)

31. Jain, P., Kar, P., et al.: Non-convex optimization for machine learning. Foundations

and Trends R(cid:13) in Machine Learning 10(3-4), 142–336 (2017)

32. Keshava, N., Mustard, J.F.: Spectral unmixing. IEEE Signal Processing Magazine

19(1), 44–57 (2002)

18

J. Janiczek et al.

33. Klingelh¨ofer, G., Morris, R.V., Bernhardt, B., Schr¨oder, C., Rodionov, D.S.,
De Souza, P., Yen, A., Gellert, R., Evlanov, E., Zubkov, B., et al.: Jarosite and
hematite at meridiani planum from opportunity’s m¨ossbauer spectrometer. Science
306(5702), 1740–1745 (2004)

34. Larkin, P.: Infrared and Raman spectroscopy: principles and spectral interpreta-

tion. Elsevier (2017)

35. Lee, J.D., Simchowitz, M., Jordan, M.I., Recht, B.: Gradient descent converges to

minimizers. arXiv preprint arXiv:1602.04915 (2016)

36. Lee, S., Tien, C.: Optical constants of soot in hydrocarbon ﬂames. In: Symposium

(international) on combustion. vol. 18, pp. 1159–1166. Elsevier (1981)

37. Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., Benediktsson, J.A.: Deep learn-
ing for hyperspectral image classiﬁcation: An overview. IEEE Transactions on Geo-
science and Remote Sensing 57(9), 6690–6709 (2019)

38. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: Diﬀerentiable monte carlo ray
tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia)
37(6), 222:1–222:11 (2018)

39. Li, Y., Zhang, H., Shen, Q.: Spectral–spatial classiﬁcation of hyperspectral imagery

with 3d convolutional neural network. Remote Sensing 9(1), 67 (2017)

40. Liu, P., Zhang, H., Eom, K.B.: Active deep learning for classiﬁcation of hyperspec-
tral images. IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 10(2), 712–724 (2016)

41. Loubet, G., Holzschuch, N., Jakob, W.: Reparameterizing discontinuous integrands
for diﬀerentiable rendering. Transactions on Graphics (Proceedings of SIGGRAPH
Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356510

42. Lu, G., Fei, B.: Medical hyperspectral imaging: a review. Journal of Biomedical

Optics 19(1), 010901 (2014)

43. Moersch, J., Christensen, P.R.: Thermal emission from particulate surfaces: A
comparison of scattering models with measured spectra. Journal of Geophysical
Research: Planets 100(E4), 7465–7477 (1995)

44. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable for-
ward and inverse renderer. Transactions on Graphics (Proceedings of SIGGRAPH
Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356498

45. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)
46. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. In: Advances in Neural Information Processing
Systems. pp. 8024–8035 (2019)

47. Ramsey, M.S., Christensen, P.R.: Mineral abundance determination: Quantitative
deconvolution of thermal emission spectra. Journal of Geophysical Research: Solid
Earth 103(B1), 577–596 (1998)

48. Ramsey M., C.P.: Mineral abundance determination: Quantitative deconvolution
of thermal emission spectra: Application to analysis of martian atmospheric par-
ticulates. Journal of Geophysical Research: Solid Earth (2000)

49. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari,
G.: Pytorch3d. https://github.com/facebookresearch/pytorch3d (2020)
50. Roberts, D.A., Gardner, M., Church, R., Ustin, S., Scheer, G., Green, R.: Map-
ping chaparral in the santa monica mountains using multiple endmember spectral
mixture models. Remote Sensing of Environment 65(3), 267–279 (1998)

Diﬀerentiable Programming for Hyperspectral Unmixing

19

51. Rogers, A., Aharonson, O.: Mineralogical composition of sands in meridiani planum
determined from mars exploration rover data and comparison to orbital measure-
ments. Journal of Geophysical Research: Planets 113(E6) (2008)

52. Salisbury, J.W., D’Aria, D.M., Sabins Jr, F.F.: Thermal infrared remote sensing

of crude oil slicks. Remote Sensing of Environment 45(2), 225–231 (1993)

53. Spitzer, W., Kleinman, D.: Infrared lattice bands of quartz. Physical Review

121(5), 1324 (1961)

54. Stein, D.: Application of the normal compositional model to the analysis of hyper-
spectral imagery. In: IEEE Workshop on Advances in Techniques for Analysis of
Remotely Sensed Data, 2003. pp. 44–51. IEEE (2003)

55. Sun, J., Qu, Q., Wright, J.: A geometric analysis of phase

retrieval.
Foundations of Computational Mathematics 18(5), 11311198 (Aug 2017).
https://doi.org/10.1007/s10208-017-9365-9,
http://dx.doi.org/10.1007/
s10208-017-9365-9

56. Sunshine, J.M., Pieters, C.M.: Determining the composition of olivine from re-
ﬂectance spectroscopy. Journal of Geophysical Research: Planets 103(E6), 13675–
13688 (1998)

57. Thaker, P., Dasarathy, G., Nedi, A.: On the sample complexity and optimization

landscape for quadratic feasibility problems (2020)

58. Tsai, C.Y., Sankaranarayanan, A.C., Gkioulekas, I.: Beyond volumetric albedo —
A surface optimization framework for non-line-of-sight imaging. In: IEEE Intl.
Conf. Computer Vision and Pattern Recognition (CVPR) (2019)

59. Wang, F., Decker, J., Wu, X., Essertel, G., Rompf, T.: Backpropagation with
callbacks: Foundations for eﬃcient and expressive diﬀerentiable programming. In:
Advances in Neural Information Processing Systems. pp. 10180–10191 (2018)
60. Wang, F., Zheng, D., Decker, J., Wu, X., Essertel, G.M., Rompf, T.: Demystifying
diﬀerentiable programming: Shift/reset the penultimate backpropagator. Proceed-
ings of the ACM on Programming Languages 3(ICFP), 1–31 (2019)

61. Wenrich, M.L., Christensen, P.R.: Optical constants of minerals derived from emis-
sion spectroscopy: Application to quartz. Journal of Geophysical Research: Solid
Earth 101(B7), 15921–15931 (1996)

62. Yang, X., Ye, Y., Li, X., Lau, R.Y., Zhang, X., Huang, X.: Hyperspectral image
classiﬁcation with deep learning models. IEEE Transactions on Geoscience and
Remote Sensing 56(9), 5408–5423 (2018)

63. Yu, Y., Smith, W.A.: Inverserendernet: Learning single image inverse rendering. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 3155–3164 (2019)

64. Zare, A., Gader, P.: Sparsity promoting iterated constrained endmember detection
in hyperspectral imagery. IEEE Geoscience and Remote Sensing Letters 4(3), 446–
450 (2007)

65. Zare, A., Gader, P.: Hyperspectral band selection and endmember detection using
sparsity promoting priors. IEEE Geoscience and Remote Sensing Letters 5(2),
256–260 (2008)

66. Zare, A., Ho, K.: Endmember variability in hyperspectral analysis: Addressing
spectral variability during spectral unmixing. IEEE Signal Processing Magazine
31(1), 95–104 (2013)

67. Zhang, C., Wu, L., Zheng, C., Gkioulekas, I., Ramamoorthi, R., Zhao, S.: A diﬀer-
ential theory of radiative transfer. ACM Transactions on Graphics (TOG) 38(6),
1–16 (2019)

20

J. Janiczek et al.

68. Zhang, L., Zhang, L., Du, B.: Deep learning for remote sensing data: A technical
tutorial on the state of the art. IEEE Geoscience and Remote Sensing Magazine
4(2), 22–40 (2016)

69. Zhang, S., Li, J., Li, H.C., Deng, C., Plaza, A.: Spectral–spatial weighted sparse
regression for hyperspectral image unmixing. IEEE Transactions on Geoscience
and Remote Sensing 56(6), 3265–3276 (2018)

70. Zhao, W., Du, S.: Spectral–spatial feature extraction for hyperspectral image clas-
siﬁcation: A dimension reduction and deep learning approach. IEEE Transactions
on Geoscience and Remote Sensing 54(8), 4544–4554 (2016)

71. Zhou, Y., Rangarajan, A., Gader, P.D.: A gaussian mixture model representation
of endmember variability in hyperspectral unmixing. IEEE Transactions on Image
Processing 27(5), 2242–2256 (2018)

Supplemental Material: Diﬀerentiable
Programming for Hyperspectral Unmixing using
a Physics-based Dispersion Model

Appendix A: Derivation of Dispersion Model

In this section, we derive the dispersion model from ﬁrst principles, modeling the
generation of spectral radiance as a sum of mass-spring oscillations driven by an
electromagnetic wave. This induces changes in the index of refraction, which gov-
erns the reﬂectance of the material with respect to light wavelength/frequency.
This is based on earlier work by Garbuny and by Spitzer et al. [23,53].

We ﬁrst start with the equation for a mass-spring oscillator driven by an

external force:

dx
dt
For a charged particle, F = qE, where q is charge and E = E0eiωt for a

d2x
dt2 + R

+ G · x(t).

F = m

(1)

propagating electromagnetic wave. Thus we can substitute these in to get:

F = qE0eiωt = m

d2x
dt2 + R

dx
dt

+ G · x(t)

which has the solution:

x =

qE0eiωt
m

1
m − ω2 + i R

G

m ω

=

qE0eiωt
m

1
ω2
0 − ω2 + iγω

.

where ω2
At the same time, we can also relate x to E via the band strength:

0 = G/m and γ = R/m.

x =

αE
q

(2)

(3)

(4)

where α is the polarizability. Using the identity (cid:15) = 1 + 4πα, we can derive the
following band strength equation:

x =

((cid:15) − 1)E0eiωt
4πq

.

Combining Eq. 3 and Eq. 5, we get

Solving for (cid:15):

((cid:15) − 1)E0eiωt
4πq

=

qE0eiωt
m

1
ω2
0 − ω2 + iγω

.

(cid:15) =

4πq2
m

1
ω2
0 − ω2 + iγω

+ 1.

(5)

(6)

(7)

22

get

Relating (cid:15) to the refractive index, (cid:15)µ = ˆn2 where µ = 1 and ˆn = n − ik, we

(cid:15) = (n − ik)2 = n2 − k2 − 2nki =

4πq2
m

1
ω2
0 − ω2 + iγω

+ 1

(8)

This yields the refractive index equations:

n2 − k2 =

4πq2
m

0 − ω2
ω2

0 − ω2)2 + γ2ω2 + 1

(ω2

2nk =

4πq2
m

γω
0 − ω2)2 + γ2ω2 .

(ω2

Using the Lorentz-Lorenz formula, we can get

ˆn2 = 1 +

4πq2
m

1
ω2
1 − ω2 + iγω

where ω2

1 = ω2

0 − 4πq2

3m where ω1 < ω0. So plugging in ω1 for ω0 yields:

n2 − k2 =

2nk =

4πq2
m

4πq2
m

1 − ω2
ω2

1 − ω2)2 + γ2ω2 + 1

(ω2

γω
1 − ω2)2 + γ2ω2 .

(ω2

(9)

(10)

(11)

(12)

(13)

This entire derivation was for a single oscillator, but in practice, there are
multiple oscillators that interact. We write this as a linear superposition given
as follows:

n2 − k2 = (cid:15)r +

(cid:88)

4πq2fi
mi

i − ω2
ω2
i − ω2)2 + γ2

i ω2

(ω2

i
4πq2fi
mi

2nk =

(cid:88)

i

γiω
i − ω2)2 + γ2

i ω2 .

(ω2

(14)

(15)

where fi is the strength of each individual oscillator. Using these equations,
we have two equations for two unknowns (n and k), which we showed in Section
3 of the main paper is the basis of calculating reﬂectance and emission.

Appendix B: Spectral Variation

As stated in the main paper, the motivation for incorporating the dispersion
model into a diﬀerentiable program for spectral unmixing is to allow for physi-
cally plausible spectral variation of pure materials. Because it is known that ab-
sorption bands can shift in frequency and strength between diﬀerent endmember
samples, the goal was to ﬁnd a model that described these changes in a physically
plausible way. That is we wanted a generative model for the formation of spectra

Title Suppressed Due to Excessive Length

23

with parameters that have ”dials” to tune the frequency, strength, and shape
of absorption bands. From the literature on analysis of the formation of spectra
from an atomistic perspective [53,61] we ﬁnd that the Lorentz-Lorenz dispersion
model is the correct approach to take. However, unlike previous works we go
further than using the model to derive optical properties of materials, we also
incorporate the dispersion model into an end-to-end spectral unmixing pipeline
that allows the parameters to be ﬁne-tuned via diﬀerentiable programming to
account for spectral variability.

In Appendix A, the dispersion model is derived from ﬁrst principles and
each absorption band is described by the parameters ρ, ωo, γ, (cid:15)r. ρ is the band
strength and as it increases the absorption band becomes deeper. ωo is the reso-
nant frequency and as it increases the absorption band shifts in wavenumber (and
also slightly shifts the shape). γ is the frictional force (dampening coeﬃcient)
and controls the shape/width of the absorption bands. (cid:15)r is relative dielectric
permeability and as it increases the entire emissivity curve is shifted downwards.
Also note that absorption bands which are close to each other interact in highly
non-linear ways.

Fig. 1: A single absorption band is initialized with (cid:15)r = 2.356, ω0 = 1161, γ =
0.1, ρ = 0.67. Then the parameters are perturbed such that ω0 is increased by
100, γ is increased to 120%, and ρ is increased to 120%. The plots show the eﬀect
of changing each parameter individually to show it’s control over the shape and
width of the absorption band.

The importance of initializing the alternating optimization with good ini-
tial dispersion parameters was emphasized in the main paper, as the problem
is non-convex and good initialization is essential. It also makes intuitive sense
to initialize with parameters ﬁt to an endmember sample to give physical sig-
niﬁcance to the generated spectra. As shown in the results of the main paper,
we achieve good ﬁts with low MSE on endmember libraries used to analyze the
Martian surface as well as a semi-urban university scene. The endmember li-
braries used to ﬁt the minerals to analyze the Mars TES data are of high quality
from the Arizona State University Thermal Emission Spectral Library [15]. The

24

resulting parameters from a few of the important materials from this endmember
library are provided in the following tables.

Table 1: Dispersion parameters found for Olivine Fo10

Axis
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1

Index
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
0
1
2
3
4
5
6
7
8
9
10

ω0
258.45
272.71
285.33
340.81
361.06
467.03
589.36
826.60
863.05
934.94
1068.56
1349.50
1400.46
1452.82
1518.96
1597.62
1694.56
1794.69
1837.96
1934.50
293.77
303.28
317.16
473.47
496.39
504.45
562.92
577.32
891.85
990.28
1108.25

γ
0.018
0.038
0.027
0.021
0.067
0.060
0.032
0.011
0.030
0.018
0.009
0.043
0.057
0.064
0.079
0.018
0.043
0.032
0.009
0.056
0.042
0.058
0.137
0.006
0.029
0.062
0.055
0.027
0.023
0.047
0.023

ρ
0.022
0.070
0.035
0.015
0.187
0.091
0.043
0.015
0.083
0.038
0.001
0.009
0.026
0.020
0.025
0.001
0.007
0.002
0.001
0.020
0.240
0.263
0.356
0.002
0.030
0.302
0.057
0.008
0.189
0.086
0.006

(cid:15)r
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.07
1.99
1.99
1.99
1.99
1.99
1.99
1.99
1.99
1.99
1.99
1.99

Appendix C: Alternating Optimization Convergence

Our ultimate goal is to solve the spectral unmixing problem which can be formu-
lated as minA,x (cid:107)b − Ax(cid:107)2
2, where the minimization occurs over both the matrix
A and the unmixing vector x. This is a standard case of alternate minimization
which is known to be nonconvex [31]. In practice, alternating minimzation are
particularly hard to tackle due to the presence of suboptimal local minimas.

Title Suppressed Due to Excessive Length

25

Table 2: Dispersion parameters found for Biotite
(cid:15)r
Axis
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
1.31
0
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1
2.61
1

ω0
235.91
432.39
439.80
446.34
451.92
594.57
954.50
1008.94
1013.39
1041.20
1075.68
1116.66
1152.61
1390.98
1460.91
1524.44
1629.72
1661.44
1687.84
1772.30
1813.27
1865.48
1964.44
268.77
294.51
313.92
337.12
362.24
400.00
462.66
492.95
510.47
653.21
718.49
873.68
928.32
991.97
1588.86
1963.15
1989.53

ρ
0.2343
0.4040
0.4131
0.0385
0.4797
0.0147
0.2510
0.0578
0.0184
0.0178
0.0198
0.0003
0.0012
0.0177
0.0280
0.0676
0.0271
0.0034
0.0723
0.0877
0.0009
0.0731
0.0131
0.4634
0.1965
0.3242
0.4930
0.1954
0.5174
0.4399
0.3498
0.0664
0.0611
0.0331
0.3343
0.0488
0.3550
0.0607
0.0023
0.0002

Index
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

γ
0.066
0.056
0.039
0.014
0.042
0.073
0.036
0.014
0.017
0.048
0.025
0.007
0.019
0.044
0.061
0.065
0.025
0.007
0.068
0.074
0.006
0.064
0.055
0.073
0.045
0.064
0.093
0.062
0.209
0.065
0.080
0.061
0.078
0.040
0.115
0.048
0.015
0.040
0.004
0.001

26

Table 3: Dispersion parameters found for Hematite
Axis
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

ω0
258.29
279.35
294.73
335.86
471.32
526.58
543.94
563.14
609.37
619.61
632.43
654.46
686.74
798.98
890.21
916.82
958.26
1002.55
1100.72
1167.07
1238.37
1282.36
234.31
238.56
312.13
356.47
430.53
444.75
457.95
486.07
577.56
727.69
748.13
773.90
1049.92
1069.60
1140.36
1197.28
1256.54

Index
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

ρ
0.110
0.141
0.149
0.130
0.098
0.029
0.062
0.067
0.041
0.041
0.067
0.054
0.038
0.011
0.009
0.005
0.014
0.010
0.022
0.010
0.005
0.019
0.007
0.031
0.255
0.032
0.085
0.032
0.011
0.019
0.160
0.049
0.040
0.013
0.058
0.003
0.012
0.022
0.010

γ
0.11
0.13
0.11
0.08
0.07
0.05
0.07
0.08
0.04
0.04
0.07
0.09
0.12
0.04
0.03
0.02
0.04
0.04
0.03
0.02
0.01
0.03
0.02
0.06
0.09
0.04
0.09
0.06
0.04
0.03
0.08
0.06
0.07
0.06
0.10
0.01
0.02
0.04
0.02

(cid:15)r
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.27
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25
1.25

Title Suppressed Due to Excessive Length

27

Recent progress on tackling nonconvex problems involves either characterizing
the optimization landscape [24,57,55] or providing initialization to descent al-
gorithms [5,11] to assure convergence to the global optimum. It is known that
gradient descent applied to alternate minimization problem faces the issue of
getting stuck at local minimas [31] and hence initialization plays an important
role in solving Equation 7 in the main paper. With that in mind, in this paper
we provide a mechanism to provide good initialization to gradient descent algo-
rithm with the hope of tackling the alternating minimization problem eﬀectively.

Initialization using dictionary A((cid:15)model): We investigate the properties of
matrix A as relates to the convergence of the alternating optimization. We de-
note the measured the emissivity spectrum of various materials in the lab as
(cid:15)measured, and the physics-based dispersion model as (cid:15)model. We then use these
emissivity spectrum to construct a dictionary A((cid:15)) which servers as the initial-
ization for A in the alternating minimization approach in Equation 7 of the main
paper. The intuition behind this revolves around the ability of matrix A as a
dictionary of known emissivity spectra and we expect that the unknown spectra
(cid:15)unknown would be described as a linear combination of columns from matrix A.

Consider the following subproblem of the alternating minimization:

min
x

(cid:107)b − A(Λ)x(cid:107)2
2

without the regularization terms. In order to ensure the uniqueness of the solu-
tion x∗, we need to ensure that the matrix A is full rank. The rate of convergence
for the above minimization is inversely dependent on the condition number of
the matrix A(Λ). While it is diﬃcult to analyze this matrix analytically, we
perform an experimental characterization of the rank, condition number, and
eigenvalues of the matrix for several diﬀerent runs of the optimization algorithm
with random initializations.

From the plots, we can note that the matrix A(Λ) has full rank with condi-
tion number of around 6000. The minimum and maximum eigenvalues are not
showing drastic diﬀerence which goes well with our motive to incorporate small
changes using alternate minimization to ﬁt the spectral diﬀerences due to geo-
graphic diﬀerences. The high condition number is the reason for the relatively
slow performance for running the alternating minimization framework, with our
method taking tens of seconds to converge.

Appendix D: Implementation Details

Fully Constrained Least Squares We compare against Fully Constrained
[28] which is a popular classical unmixing algorithm.
Least Squares (FCLS)
FCLS solves for the the (aerial) abundances, x: ˆx = arg minx ||b − Ax||2
2 subject
to ||x||1 = 1, x ≥ 0. The constraints, referred to as the sum-to-one constraint
and the non-negativity constraint, are enforced since abundances are interpreted
as percentages.

28

(a) Rank of A across runs

(b) Condition number of A across runs

(c) Max eigenvalue of A across runs

(d) Min eigenvalue of A across runs

Fig. 2: Behaviour of A across runs

Fig. 3: Random sample of A has the following eigenvalue distribution

Normal Compositional Model. The Normal Compositional Model (NCM)
[54] is one of the most popular methods for modelling endmember variability
via statistical methods. The method requires a small amount of training data
(roughly 50 samples per endmember) to learn the mean and variance of reﬂec-

Title Suppressed Due to Excessive Length

29

tivity (or emissivity) of each spectral band, and modelling the variation as a
Gaussian distribution. During unmixing, Expectation Maximization is used to
simultaneously learn the abundances and the endmember variation, subject to
the abundance sum-to-one and non-negativity constraints.

The NCM is run using the Matlab code provided by Du et al. [18]. Training
data of about 50 samples of endmember variation were provided to the NCM for
each dataset. There are no hyperparameters needed for this method.

Beta Compositional Model. The Beta Compositional Model (BCM) [18]
is a more recent method for modelling spectral variability via a statistical method.
Similar to the NCM, a small amount of training data is used to learn the beta
parameters of each spectral band, and an Expectation Maximization algorithm
is used during unmixing. The beta parameters allow each spectral band to be
modelled as a more complex distribution than the NCM and has been shown to
increase performance.

The BCM is run using the Matlab code provided by Du et al. [18]. Training
data of about 50 samples of endmember variation were provided to the BCM
for each dataset. For datasets without suﬃcient endmember samples, we gener-
ated synthetic endmember variation with the dispersion model. We search for
the optimal hyperparameters through repeated experiments and report the best
results. The optimal BCM across all datasets was run with K = 3, σV = 100,
and σM = 0.001.

CNN for Spectral Classiﬁcation and Unmixing. We baseline against
the architecture recently proposed by Zhang et al. [69] for hyperspectral unmix-
ing using both a 1D and 3D CNN. The main diﬀerence between the 1D and 3D
CNN is that the 3D CNN is operates on 3×3 bundles of pixels while the 1D CNN
is provided a single pixel. However, in both architectures the convolutional kernal
is 1D and operates along the spectral dimension. Both architectures have four
convolutional layers with Rectiﬁed Linear Unit (ReLU) and max-pooling non-
linear operations. These layers are followed by 2 fully connected layers, where the
last layer is the output abundance predictions. The ReLU of the fully connected
layers ensure non-negativity, and the sum-to-one constraint is ensured by nor-
malizing the output layer. The network is trained to minimize −ˆx log(x), where
ˆx and x are the predicted and ground truth abundances respectively. While the
1D architecture performs well on the dataset it was designed for in [69], we found
that modiﬁcations were necessary to maximize the performance on our datasets.
Namely, we found that training the network with respect to the mean squared
error (MSE) loss function, removing the max-pooling layers, and adding an ad-
ditional fully connected layer before the output improved performance of the 1D
CNN. The 3D CNN is only applicable to datasets with spatial information, thus
is ignored for the Feely and Synthetic datasets.

CNN Baselines [69]. The architecture from Zhang et al. was used to base-
line against our method (CNN-1D and CNN-3D), as well as introducing our own
modiﬁed baseline (CNN-1D modiﬁed). All CNNs were trained for 100 epochs us-
ing the Adam optimizer with learning = 0.0005, betas = (0.9, 0.999) and weight
decay = 0.

30

CNN-1D: A 1x1 hyperspectral pixel is input into the network, four convolutional
layers with alternating 1x5 and 1x4 kernels and a depth of 3, 6, 12, and 24 ker-
nels in each layer respectively. All convolutional layers have ReLU activations
a 1x2 maxpooling layer. The convolutional layers are followed by a fully con-
nected layer with 150 hidden units, and an output fully connected layer with a
size that depends on the number of abundances. Normalization is used to en-
force the abundance sum-to-one constraint and the ReLU activation enforces the
non-negativity constraint. The CNN is trained to minimize the log loss between
the predicted and ground truth abundances. The network converges in about
100 epochs with a learning rate of 1e-3.
CNN-3D: CNN-3D has an almost identical architecture, although it accepts a
3x3 set of pixels at the input. Although a spatial dimension exists at the input,
the convolutions only occur in the spectral dimension. four convolutional layers
with alternating 1x5 and 1x4 kernels and a depth of 16, 32, 64, and 128 kernels
in each layer respectively. The convolutional layers are followed by a fully con-
nected layer with 150 hidden units, and an output fully connected layer with a
size that depends on the number of abundances. Normalization is used to en-
force the abundance sum-to-one constraint and the ReLU activation enforces the
non-negativity constraint. The CNN is trained to minimize the log loss between
the predicted and ground truth abundances. The network converges in about
100 epochs with a learning rate of 1e-3.
CNN-1D Modiﬁed: Finally, a modiﬁed version of CNN-1D is baselined against
to try to ﬁnd the optimal architecture for performance on our datasets. The ﬁrst
2 max-pooling layers are removed, an additional hidden fully connected layer
with 150 units is added before the output, and a softmax operation is applied to
the output to enforce the abundance sum-to-one constraint. Also, the network is
trained to minimize the mean squared error between the predicted and ground
truth abundances. The network converges in about 100 epochs with a learning
rate of 1e-3.

Analysis-by-Synthesis Optimization. For analysis-by-synthesis, the sparse

regularization was set with p = 0.95 and λp = 0.0001. Dispersion parameters
were constrained within a tolerance of their initial conditions with ρtol = 0.05,
γtol = 0.005, (cid:15)tol = 0.001, and ωtol = 0.0001. On the Gulfport datasets γtol and
(cid:15)tol were increased to 0.05 to compensate for increased variation. Analysis-by-
Synthesis alternates between ﬁnding optimal abundances (solving a regularized
least squares problem), and updating the dispersion parameters for 100 itera-
tions using the Adam optimizer with learning rate = 0.01, betas = (0.9, 0.999),
and weight decay = 0.

Inverse Rendering CNN. The inverse rendering CNN uses the same CNN
architecture as CNN-1D modiﬁed. The input to the CNN is the spectrum (or
batch of spectra). That is there are four convolutional layers with alternating
1x5 and 1x4 kernels and a depth of 3, 6, 12, and 24 kernels in each layer re-
spectively. The convolutional layers also have ReLU activations and the last 2
layers have a 1x2 maxpooling layer. The convolutional layers are followed by
fully connected layers with 150 hidden units. The ﬁnal fully connected layer has

Title Suppressed Due to Excessive Length

31

enough units for the amount of dispersion parameters and abundances depend-
ing on the size of the endmember library and number of dispersion parameters
per endmember. Then, the dispersion parameters are used to render endmember
spectra and the mixture is reconstructed under the linear mixing model with the
predicted abundances as inputs. The network only needs the input spectra and
the abundances as inputs for training, as the reconstruction error of the spectra
is used to back-propagated through the diﬀerentiable dispersion model to teach
the network to predict good dispersion parameters. Real data (when available)
and synthetic data (around 50,000 samples) are used to train the network, which
converges after about 100 epochs. An Adam optimizer is used with learning rate
set to 1e-3, betas set to (0.9, 0.999), and weight decay set to 0.

Appendix E: Additional Results

Due to size limitations on ArXiv, we cannot display all the mineral maps of
Mars TES data here. We encourage the reader to go to Dr. Jayasuriya’s webpage
https://web.asu.edu/sites/default/files/imaging-lyceum/files/eccv2020_
hyperspectral_paperwithsupplement-min.pdf to see the full supplemental
material.

32

References

1. Azinovic, D., Li, T.M., Kaplanyan, A., Niessner, M.: Inverse path tracing for joint
material and lighting estimation. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. pp. 2447–2456 (2019)

2. Bandﬁeld, J.L.: Global mineral distributions on mars. Journal of Geophysical Re-

search: Planets 107(E6), 9–1 (2002)

3. Bateson, C.A., Asner, G.P., Wessman, C.A.: Endmember bundles: A new approach
to incorporating endmember variability into spectral mixture analysis. IEEE Trans-
actions on Geoscience and Remote Sensing 38(2), 1083–1094 (2000)

4. Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M.: Automatic diﬀeren-
tiation in machine learning: a survey. The Journal of Machine Learning Research
18(1), 5595–5637 (2017)

5. Bhojanapalli, S., Kyrillidis, A., Sanghavi, S.: Dropping convexity for faster semi-
deﬁnite optimization. In: Conference on Learning Theory. pp. 530–582 (2016)
6. Borsoi, R.A., Imbiriba, T., Bermudez, J.C.M., Richard, C., Chanussot, J.,
Drumetz, L., Tourneret, J.Y., Zare, A., Jutten, C.: Spectral variability in hyper-
spectral data unmixing: A comprehensive review. arXiv preprint arXiv:2001.07307
(2020)

7. Burns, R.G.: Crystal ﬁeld spectra and evidence of cation ordering in olivine min-
erals. American Mineralogist: Journal of Earth and Planetary Materials 55(9-10),
1608–1632 (1970)

8. Chang, C.I.: Hyperspectral imaging: techniques for spectral detection and classiﬁ-

cation, vol. 1. Springer Science & Business Media (2003)

9. Chen, F., Zhang, Y.: Sparse hyperspectral unmixing based on constrained lp-l
2 optimization. IEEE Geoscience and Remote Sensing Letters 10(5), 1142–1146
(2013)

10. Chen, Y., Jiang, H., Li, C., Jia, X., Ghamisi, P.: Deep feature extraction and clas-
siﬁcation of hyperspectral images based on convolutional neural networks. IEEE
Transactions on Geoscience and Remote Sensing 54(10), 6232–6251 (2016)

11. Chen, Y., Chi, Y., Fan, J., Ma, C.: Gradient descent with random initialization:
Fast global convergence for nonconvex phase retrieval. Mathematical Programming
176(1-2), 5–37 (2019)

12. Cheng, G., Li, Z., Han, J., Yao, X., Guo, L.: Exploring hierarchical convolutional
features for hyperspectral image classiﬁcation. IEEE Transactions on Geoscience
and Remote Sensing 56(11), 6712–6722 (2018)

13. Chouzenoux, E., Legendre, M., Moussaoui, S., Idier, J.: Fast constrained least
squares spectral unmixing using primal-dual interior-point optimization. IEEE
Journal of Selected Topics in Applied Earth Observations and Remote Sensing
7(1), 59–69 (2014)

14. Christensen, P., Bandﬁeld, J., Hamilton, V.E., Ruﬀ, S.W., Kieﬀer, H.H., Titus,
T.N., Malin, M.C., Morris, R.V., Lane, M.D., Clark, R.L., Jakosky, B.M., Mel-
lon, M.T., Pearl, J.C., Conrath, B.J., Smith, M.D., Clancy, R.T., Kuzmin, R.O.,
Roush, T., Mehall, G.L., Gorelick, N., Bender, K., Murray, K., Dason, S., Greene,
E., Silverman, S., Greenﬁeld, M.: Mars global surveyor thermal emission spectrom-
eter experiment: investigation description and surface science results. Journal of
Geophysical Research: Planets 106(E10), 23823–23871 (2001)

15. Christensen, P.R., Bandﬁeld, J.L., Hamilton, V.E., Howard, D.A., Lane, M.D.,
Piatek, J.L., Ruﬀ, S.W., Stefanov, W.L.: A thermal emission spectral library of
rock-forming minerals. Journal of Geophysical Research: Planets 105(E4), 9735–
9739 (2000)

Title Suppressed Due to Excessive Length

33

16. Christensen, P.R., Hamilton, V.E., Mehall, G., Pelham, D., ODonnell, W., Anwar,
S., Bowles, H., Chase, S., Fahlgren, J., Farkas, Z., Fisher, T., James, I., Kubik,
I., Lazbin, M., Miner, M., Rassas, L., Schulze, K., Shamordola, T., Tourville, G.,
West, R.and Woodward, D., Lauretta: The osiris-rex thermal emission spectrom-
eter (otes) instrument. Space Science Reviews 214(5), 87 (2018)

17. Combe, J.P., Le Mou´elic, S., Sotin, C., Gendrin, A., Mustard, J., Le Deit, L.,
Launeau, P., Bibring, J.P., Gondet, B., Langevin, Y., Pinet, P.: Analysis of
omega/mars express data hyperspectral data using a multiple-endmember linear
spectral unmixing model (melsum): Methodology and ﬁrst results. Planetary and
Space Science 56(7), 951–975 (2008)

18. Du, X., Zare, A., Gader, P., Dranishnikov, D.: Spatial and spectral unmixing using
the beta compositional model. IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing 7(6), 1994–2003 (2014)

19. Duchi, J.C., Ruan, F.: Solving (most) of a set of quadratic equalities: Composite
optimization for robust phase retrieval. Information and Inference: A Journal of
the IMA 8(3), 471–529 (2019)

20. Engel, J., Hantrakul, L.H., Gu, C., Roberts, A.: Ddsp: Diﬀerentiable digital signal
processing. In: International Conference on Learning Representations (2020)
21. Feely, K.C., Christensen, P.R.: Quantitative compositional analysis using thermal
emission spectroscopy: Application to igneous and metamorphic rocks. Journal of
Geophysical Research: Planets 104(E10), 24195–24210 (1999)

22. Gader, P., Zare, A., Close, R., Aitken, J., Tuell, G.: Muuﬂ gulfport hyperspectral
and lidar airborne data set. Univ. Florida, Gainesville, FL, USA, Tech. Rep. REP-
2013-570 (2013)

23. Garbuny, M.: Optical physics. Optical Physics by Max Garbuny New York, NY:

Academic Press, 1965 (1965)

24. Ge, R., Jin, C., Zheng, Y.: No spurious local minima in nonconvex low rank prob-
lems: A uniﬁed geometric analysis. In: Proceedings of the 34th International Con-
ference on Machine Learning-Volume 70. pp. 1233–1242. JMLR. org (2017)

25. Gkioulekas, I., Levin, A., Zickler, T.: An evaluation of computational imaging tech-
niques for heterogeneous inverse scattering. In: European Conference on Computer
Vision. pp. 685–701. Springer (2016)

26. Gkioulekas, I., Zhao, S., Bala, K., Zickler, T., Levin, A.: Inverse volume rendering
with material dictionaries. ACM Transactions on Graphics (TOG) 32(6), 162
(2013)

27. Goudge, T.A., Mustard, J.F., Head, J.W., Salvatore, M.R., Wiseman, S.M.: In-
tegrating crism and tes hyperspectral data to characterize a halloysite-bearing
deposit in kashira crater, mars. Icarus 250, 165–187 (2015)

28. Heinz, D.C., et al.: Fully constrained least squares linear spectral mixture analysis
method for material quantiﬁcation in hyperspectral imagery. IEEE Transactions
on Geoscience and Remote Sensing 39(3), 529–545 (2001)

29. Heylen, R., Parente, M., Gader, P.: A review of nonlinear hyperspectral unmixing
methods. IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 7(6), 1844–1868 (2014)

30. Hu, W., Huang, Y., Wei, L., Zhang, F., Li, H.: Deep convolutional neural networks

for hyperspectral image classiﬁcation. Journal of Sensors 2015 (2015)

31. Jain, P., Kar, P., et al.: Non-convex optimization for machine learning. Foundations

and Trends R(cid:13) in Machine Learning 10(3-4), 142–336 (2017)

32. Keshava, N., Mustard, J.F.: Spectral unmixing. IEEE Signal Processing Magazine

19(1), 44–57 (2002)

34

33. Klingelh¨ofer, G., Morris, R.V., Bernhardt, B., Schr¨oder, C., Rodionov, D.S.,
De Souza, P., Yen, A., Gellert, R., Evlanov, E., Zubkov, B., et al.: Jarosite and
hematite at meridiani planum from opportunity’s m¨ossbauer spectrometer. Science
306(5702), 1740–1745 (2004)

34. Larkin, P.: Infrared and Raman spectroscopy: principles and spectral interpreta-

tion. Elsevier (2017)

35. Lee, J.D., Simchowitz, M., Jordan, M.I., Recht, B.: Gradient descent converges to

minimizers. arXiv preprint arXiv:1602.04915 (2016)

36. Lee, S., Tien, C.: Optical constants of soot in hydrocarbon ﬂames. In: Symposium

(international) on combustion. vol. 18, pp. 1159–1166. Elsevier (1981)

37. Li, S., Song, W., Fang, L., Chen, Y., Ghamisi, P., Benediktsson, J.A.: Deep learn-
ing for hyperspectral image classiﬁcation: An overview. IEEE Transactions on Geo-
science and Remote Sensing 57(9), 6690–6709 (2019)

38. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: Diﬀerentiable monte carlo ray
tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia)
37(6), 222:1–222:11 (2018)

39. Li, Y., Zhang, H., Shen, Q.: Spectral–spatial classiﬁcation of hyperspectral imagery

with 3d convolutional neural network. Remote Sensing 9(1), 67 (2017)

40. Liu, P., Zhang, H., Eom, K.B.: Active deep learning for classiﬁcation of hyperspec-
tral images. IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing 10(2), 712–724 (2016)

41. Loubet, G., Holzschuch, N., Jakob, W.: Reparameterizing discontinuous integrands
for diﬀerentiable rendering. Transactions on Graphics (Proceedings of SIGGRAPH
Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356510

42. Lu, G., Fei, B.: Medical hyperspectral imaging: a review. Journal of Biomedical

Optics 19(1), 010901 (2014)

43. Moersch, J., Christensen, P.R.: Thermal emission from particulate surfaces: A
comparison of scattering models with measured spectra. Journal of Geophysical
Research: Planets 100(E4), 7465–7477 (1995)

44. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable for-
ward and inverse renderer. Transactions on Graphics (Proceedings of SIGGRAPH
Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356498

45. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)
46. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. In: Advances in Neural Information Processing
Systems. pp. 8024–8035 (2019)

47. Ramsey, M.S., Christensen, P.R.: Mineral abundance determination: Quantitative
deconvolution of thermal emission spectra. Journal of Geophysical Research: Solid
Earth 103(B1), 577–596 (1998)

48. Ramsey M., C.P.: Mineral abundance determination: Quantitative deconvolution
of thermal emission spectra: Application to analysis of martian atmospheric par-
ticulates. Journal of Geophysical Research: Solid Earth (2000)

49. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari,
G.: Pytorch3d. https://github.com/facebookresearch/pytorch3d (2020)
50. Roberts, D.A., Gardner, M., Church, R., Ustin, S., Scheer, G., Green, R.: Map-
ping chaparral in the santa monica mountains using multiple endmember spectral
mixture models. Remote Sensing of Environment 65(3), 267–279 (1998)

Title Suppressed Due to Excessive Length

35

51. Rogers, A., Aharonson, O.: Mineralogical composition of sands in meridiani planum
determined from mars exploration rover data and comparison to orbital measure-
ments. Journal of Geophysical Research: Planets 113(E6) (2008)

52. Salisbury, J.W., D’Aria, D.M., Sabins Jr, F.F.: Thermal infrared remote sensing

of crude oil slicks. Remote Sensing of Environment 45(2), 225–231 (1993)

53. Spitzer, W., Kleinman, D.: Infrared lattice bands of quartz. Physical Review

121(5), 1324 (1961)

54. Stein, D.: Application of the normal compositional model to the analysis of hyper-
spectral imagery. In: IEEE Workshop on Advances in Techniques for Analysis of
Remotely Sensed Data, 2003. pp. 44–51. IEEE (2003)

55. Sun, J., Qu, Q., Wright, J.: A geometric analysis of phase

retrieval.
Foundations of Computational Mathematics 18(5), 11311198 (Aug 2017).
https://doi.org/10.1007/s10208-017-9365-9,
http://dx.doi.org/10.1007/
s10208-017-9365-9

56. Sunshine, J.M., Pieters, C.M.: Determining the composition of olivine from re-
ﬂectance spectroscopy. Journal of Geophysical Research: Planets 103(E6), 13675–
13688 (1998)

57. Thaker, P., Dasarathy, G., Nedi, A.: On the sample complexity and optimization

landscape for quadratic feasibility problems (2020)

58. Tsai, C.Y., Sankaranarayanan, A.C., Gkioulekas, I.: Beyond volumetric albedo —
A surface optimization framework for non-line-of-sight imaging. In: IEEE Intl.
Conf. Computer Vision and Pattern Recognition (CVPR) (2019)

59. Wang, F., Decker, J., Wu, X., Essertel, G., Rompf, T.: Backpropagation with
callbacks: Foundations for eﬃcient and expressive diﬀerentiable programming. In:
Advances in Neural Information Processing Systems. pp. 10180–10191 (2018)
60. Wang, F., Zheng, D., Decker, J., Wu, X., Essertel, G.M., Rompf, T.: Demystifying
diﬀerentiable programming: Shift/reset the penultimate backpropagator. Proceed-
ings of the ACM on Programming Languages 3(ICFP), 1–31 (2019)

61. Wenrich, M.L., Christensen, P.R.: Optical constants of minerals derived from emis-
sion spectroscopy: Application to quartz. Journal of Geophysical Research: Solid
Earth 101(B7), 15921–15931 (1996)

62. Yang, X., Ye, Y., Li, X., Lau, R.Y., Zhang, X., Huang, X.: Hyperspectral image
classiﬁcation with deep learning models. IEEE Transactions on Geoscience and
Remote Sensing 56(9), 5408–5423 (2018)

63. Yu, Y., Smith, W.A.: Inverserendernet: Learning single image inverse rendering. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 3155–3164 (2019)

64. Zare, A., Gader, P.: Sparsity promoting iterated constrained endmember detection
in hyperspectral imagery. IEEE Geoscience and Remote Sensing Letters 4(3), 446–
450 (2007)

65. Zare, A., Gader, P.: Hyperspectral band selection and endmember detection using
sparsity promoting priors. IEEE Geoscience and Remote Sensing Letters 5(2),
256–260 (2008)

66. Zare, A., Ho, K.: Endmember variability in hyperspectral analysis: Addressing
spectral variability during spectral unmixing. IEEE Signal Processing Magazine
31(1), 95–104 (2013)

67. Zhang, C., Wu, L., Zheng, C., Gkioulekas, I., Ramamoorthi, R., Zhao, S.: A diﬀer-
ential theory of radiative transfer. ACM Transactions on Graphics (TOG) 38(6),
1–16 (2019)

36

68. Zhang, L., Zhang, L., Du, B.: Deep learning for remote sensing data: A technical
tutorial on the state of the art. IEEE Geoscience and Remote Sensing Magazine
4(2), 22–40 (2016)

69. Zhang, S., Li, J., Li, H.C., Deng, C., Plaza, A.: Spectral–spatial weighted sparse
regression for hyperspectral image unmixing. IEEE Transactions on Geoscience
and Remote Sensing 56(6), 3265–3276 (2018)

70. Zhao, W., Du, S.: Spectral–spatial feature extraction for hyperspectral image clas-
siﬁcation: A dimension reduction and deep learning approach. IEEE Transactions
on Geoscience and Remote Sensing 54(8), 4544–4554 (2016)

71. Zhou, Y., Rangarajan, A., Gader, P.D.: A gaussian mixture model representation
of endmember variability in hyperspectral unmixing. IEEE Transactions on Image
Processing 27(5), 2242–2256 (2018)

