1
2
0
2

v
o
N
0
3

]
I

A
.
s
c
[

4
v
5
5
7
5
0
.
4
0
1
2
:
v
i
X
r
a

Tensor Processing Primitives: A Programming Abstraction for
Efficiency and Portability in Deep Learning & HPC Workloads

EVANGELOS GEORGANAS∗, DHIRAJ KALAMKAR∗, SASIKANTH AVANCHA∗, MENACHEM
ADELMAN∗, DEEPTI AGGARWAL∗, CRISTINA ANDERSON∗, ALEXANDER BREUER#, JEREMY
BRUESTLE∗, NARENDRA CHAUDHARY∗, ABHISEK KUNDU∗, DENISE KUTNICK∗, FRANK
LAUB∗, VASIMUDDIN MD∗, SANCHIT MISRA∗, RAMANARAYAN MOHANTY∗, HANS PABST∗,
BRIAN RETFORD∗, BARUKH ZIV∗, ALEXANDER HEINECKE∗

∗Intel Corporation
#Friedrich-Schiller–Universität Jena

During the past decade, novel Deep Learning (DL) algorithms, workloads and hardware have been developed to tackle a wide
range of problems. Despite the advances in workload and hardware ecosystems, the programming methodology of DL systems
is stagnant. DL workloads leverage either highly-optimized, yet platform-specific and inflexible kernels from DL libraries,
or in the case of novel operators, reference implementations are built via DL framework primitives with underwhelming
performance. This work introduces the Tensor Processing Primitives (TPP), a programming abstraction striving for efficient,
portable implementation of DL workloads with high-productivity. TPPs define a compact, yet versatile set of 2D-tensor
operators (or a virtual Tensor ISA), which subsequently can be utilized as building-blocks to construct complex operators on
high-dimensional tensors. The TPP specification is platform-agnostic, thus code expressed via TPPs is portable, whereas the
TPP implementation is highly-optimized and platform-specific. We demonstrate the efficacy and viability of our approach
using standalone kernels and end-to-end DL & HPC workloads expressed entirely via TPPs that outperform state-of-the-art
implementations on multiple platforms.

INTRODUCTION

1
Since the advent of Deep Learning (DL) as one of the most promising machine learning paradigms almost 10
years ago, deep neural networks have advanced the fields of computer vision, natural language processing,
recommender systems, and gradually pervade an increasing number of scientific domains [1–10]. Due to the
diverse nature of the problems under consideration, these DL workloads exhibit a wide range of computational
characteristics and demands. Furthermore, due to the immense computational cost of such workloads, industry
and academia have developed specialized hardware features on commodity processors, and even specialized
accelerators in order to harness these computational needs [11].

In contrary to the fast-evolving ecosystems of DL workloads and DL-oriented hardware/accelerators, the
programming paradigm of DL systems has reached a plateau [12]. More specifically, the development of novel
DL workloads involves two types of components: i) Well-established operators within DL libraries (e.g. 2D
convolutions, inner-product, batch-norm layers in oneDNN [13] and cuDNN [14]), and ii) Unprecedented, custom
primitives which typically instantiate new algorithmic concepts/computational motifs. Unfortunately both of
these components come with their shortcomings.

On one hand, the operators within DL libraries are heavily optimized and tuned (usually by vendors) in a
platform-specific fashion, leading to monolithic, non-portable and inflexible kernels. Additionally, such opaque
and high-level operators prohibit modular design choices since the user/frameworks have to adhere to particular
interfaces that may not be adapted to fit the operation under consideration. On the other hand, the custom/un-
precedented primitives are typically implemented by the user via the available generic/reference primitives of an
ML framework which are not optimized and as such yield underwhelming performance. It is up to the user to
create optimized implementations for the custom primitives, leading again to code which is non-portable and

 
 
 
 
 
 
2

• E. Georganas et al.

potentially requires hardware expertise in order to achieve peak performance. Unfortunately, most of the times
such expertise is not available to the data/ML scientist who is developing the custom DL primitive. Therefore, the
deployment (or even the evaluation) of a new operator typically requires yet another stage in the development
cycle where low-level optimization experts are working on the re-write/fine-tuning of the operator. Later on, in
case an operator proves to be important for the community, systems researchers and vendors standardize it, and
potentially create yet another monolithic kernel within a DL library for further re-use within DL frameworks.
This entire development cycle potentially takes a considerable amount of time (up to years in some cases) and
inadvertently impedes the efficient exploration of innovative machine learning ideas [12]. An alternative approach
to optimize both types of operators is to leverage contemporary Tensor Compilers (e.g. [15–18]), however the
state-of-the-art tools are only suitable for compiling small code-blocks whereas large-scale operators require
prohibitive compilation times, and often the resulting code performs far from the achievable peak [12].

We identify that the common source of the problems mentioned in the previous paragraph is the extreme
levels of abstraction offered by the DL libraries and the Tensor Compilers. The DL libraries offer coarse-grain,
monolithic and inflexible operators whereas the Tensor Compilers usually go to the other extreme, allowing the
user to express arbitrary low-level operators without any minimal restrictions that would readily enable efficient
lifting and code-generation in their back-ends (e.g. they offer no minimal/compact set of allowed operations
on tensors). To exacerbate the challenge of optimal code generation, Tensor Compilers usually undertake the
cumbersome tasks of efficient parallelization, loop re-ordering, automatic tiling and layout transformations, which,
to date, remain unsolved in the general setup. Also, there is not a well-established way to share state-of-the-art
optimizations among the plethora of Tensor Compilers and as a result each one has its own advantages and
disadvantages, which translates eventually to sub-optimal performance on real-world scenarios [19]. We note
here the recent, promising effort of MLIR [20] towards unifying the optimization efforts in the Tensor Compiler
IR infrastructure.

In this work we introduce the Tensor Processing Primitives (TPP), a programming abstraction striving for
efficient and portable implementation of Tensor operations, with a special focus on DL workloads. TPPs define a
set of relatively low-level primitive operators on 2D Tensors, which in turn can be used as basic building blocks
to construct more complex operators on high-dimensional tensors. TPPs comprise a minimal and compact, yet
expressive set of precision-aware, 2D tensor level operators to which high-level DL operators can be reduced.
TPPs’s specification is agnostic to targeted platform, DL framework, and compiler back-end. As such the code
which is expressed in terms of TPPs is portable. Since the level of abstraction that TPPs adopt is at the sub-tensor
granularity, TPPs can be directly employed by DL workload developers within the frameworks, or could be
alternatively used to back up an IR within a Tensor Compiler stack, i.e. TPPs could form the basis of an MLIR
dialect.

While the TPP specification is agnostic of the targeted framework/platform/compiler stack, its implementation
is platform specific, and is optimized for the target architectures. This subtle detail offers a clear separation
of concerns: the user-entity of TPPs, either a developer or a compiler framework, can focus on expressing the
desired algorithm and its execution schedule (e.g. parallelization, loop orders) using the TPP tensor abstraction,
whereas the efficient, platform-specific code generation pertaining to the TPP operations belongs to the TPP
back-end. To this extent, TPPs could be also viewed as a “virtual Tensor ISA" that abstracts the actual physical
ISA of the target (e.g. SSE, AVX2, AVX512, AMX for x86, AArch64 and ARMv8 SVE , xPU).

Figure 1 shows various use-cases of TPPs within multiple software stacks. TPPs can be viewed as a layer
abstraction of the actual physical target ISA, and the user-entities can rely on the TPP layer for the code generation
pertaining to the tensor operations. Also, Figure 1 illustrates the various user-entities that might leverage TPPs.
First, the vendor-optimized DL libraries (e.g. oneDNN or oneDNN Graph) can use TPPs for optimized code
generation in their back-end. Second, the user/developer of the DL operators can directly leverage TPPs within a
DL framework extension to express the underlying tensor computations (e.g. the user may develop a framework

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

3

Fig. 1. Use-cases of TPPs in various software stacks.

extension for a novel DL operator by employing the TPPs as building blocks). Third, Tensor Compilers can
leverage TPPs (e.g. as part of an MLIR dialect) to generate high-quality code for the corresponding tensor operators.
As such, the TPP layer abstraction offers a clear separation of concerns where the Tensor Compiler may focus on
higher-level optimizations (loop tiling and re-ordering, parallelization etc) whereas the platform-specific code
generation of the tensor operations is undertaken by the TPP layer. Such a synergistic Tensor Compiler - TPP
paradigm is illustrated in Section 7. Last but not least, TPPs could be leveraged by more general Tensor Libraries
(e.g. ATen, Eigen) where tensor computations constitute the primary focus and they can be naturally mapped to
TPPs.

In our Proof-Of-Concept (POC) implementation of TPPs we leverage JIT technology to emit performant and
platform-specific code during runtime. Furthermore, in our POC we define a mini embedded Domain Specific
Language (mini-eDSL) where the TPPs can be combined via matrix equations in order to build high-level operators
without sacrificing performance.

We demonstrate the efficiency of our approach on multiple platforms using standalone kernels written entirely
with TPPs and compare the performance to vectorized-by-expert code and compiler generated code. Finally,
we showcase the expressiveness and viability of our methodology by implementing contemporary end-to-
end DL workloads using solely the TPP abstractions and show how we can outperform the state-of-the-art
implementations on multiple platforms. The main contributions of this work are:

• A TPP specification/foundation for primitive tensor operations.
• A Proof-Of-Concept implementation of the TPP specification along with a mini-eDSL (called TPP Matrix
Equations), enabling efficient fusion of TPPs that lead to portable, high-level tensor operations. We describe
in detail various standalone TPP implementations, and also we provide a detailed analysis of our TPP
Matrix Equation mini-eDSL framework.

• A demonstration of how contemporary and novel DL algorithmic motifs/workloads can be expressed in

their entirety via TPPs.

• An experimental evaluation of the TPP-based DL workloads from all relevant fields (image processing,
recommendation systems, natural language processing, graph processing and applications in science) on
multiple platforms (different instruction set architectures (ISAs) x86_64 and aarch64, and micro-architectures

Tensor Processing Primitives (TPP) as Virtual Tensor ISASSE/AVX/AVX512AMX+AVX512NEON / SVEAcceleratorVendor DL primitives APIVendor DL Graph APIXLATensor Compilers      (e.g.PlaidML)MLIR DialectsTensor Libraries (e.g.ATen, Eigen, FBGEMM)Framework extensions  Framework (e.g.PyTorch, TensorFlow)/numPy/JAX4

• E. Georganas et al.

for each ISA), including distributed-memory scaling. We show performance that matches/exceeds the
state-of-the-art implementations, while maintaining flexibility, portability and obviating the need for
low-level platform-specific optimizations.

• We show how TPPs can be leveraged as a virtual Tensor ISA within a Tensor compiler software stack,

yielding high-performance DL primitives.

• We illustrate examples of how TPPs are used outside of Deep Learning, in High Performance Computing

(HPC) applications in order to accelerate tensor computations.

Section 2 details the specification of the TPPs. Then, Section 3 illustrates a POC implementation of the TPP
specification. Section 4 presents an infrastructure that enables efficient TPP fusion. In Section 5 we exhibit how
contemporary DL motifs/algorithmic paradigms can be expressed via TPPs. Section 6 presents an experimental
evaluation of TPP-based DL kernels and workloads on multiple platforms. Section 7 outlines our POC imple-
mentation of a TPP backend within a tensor compiler (PlaidML), and also presents some results highlighting the
viability of the TPP abstraction as a virtual Tensor ISA within tensor compiler stacks. Section 8 presents exemplary
usage of TPPs within HPC applications in order to efficiently implement tensor computations. Sections 9 and 10
summarize the related work and conclude this paper.

2 THE TPP SPECIFICATION
2.1 TPP Design Principles
The TPP specification is driven by a few design principles:

1) Each TPP corresponds to a mathematical operator that takes a number of input(s) and produces an output. We
opt to specify TPPs that correspond to basic, well-defined mathematical tensor operations. In this way we keep
the set of TPPs minimal albeit expressive; basic TPPs can be combined to formulate more complex operators.

2) The inputs/outputs of the TPPs are abstract 2D tensors that can be fully specified by their shape/size, leading
dimensions, and precision. Additionally, the 2D tensors hold the following complementary runtime information: (i)
a primary field which corresponds to the memory address where the 2D (sub)tensor data resides, (ii) a secondary
field holding optional data for the tensor (e.g. a mask for the tensor), and (iii) a tertiary field holding optional,
auxiliary information of the tensor (e.g. scaling factors for a quantized tensor.)

3) TPPs are specified as “memory-to-memory" operations, or equivalently the input/output tensors are residing
in memory locations specified by the user. This design decision is critical in order to abstract the TPPs from all
physical ISAs, and enables true platform-agnostic specification. For example, if the TPPs were accepting vector
registers as inputs/outputs, then the number of physical registers, the vector length and dimensionality would be
exposed in the API of TPPs, making the specification platform-specific.

4) TPPs have declarative semantics. As such, the TPP specification does not preclude potential parallelism (e.g.

SIMD, SIMT) in the back-end implementation which is target-specific.

5) TPPs are composable in a producer-consumer fashion. Since the output of a TPP is a well-defined tensor 𝑂,
it can be fed as input to a subsequent TPP. In such a scenario, this “intermediate" tensor 𝑂 is not necessarily
exposed to the user, unless the user explicitly requires it (e.g. by combining the TPPs in a manual fashion via
an explicit temporary 𝑂 buffer/tensor which lives in the user space/application). This flexibility allows the TPP
implementation (which is platform-specific) to combine TPPs in the most efficient way for the target architecture
(e.g. the 𝑂 tensor can live at the physical register file in the composite TPP in order to avoid redundant memory
movement).

6) The TPP input/output tensors as well as the computation itself are precision aware. This feature makes mixed
precision computations (that are prominent in DL workloads) easy to express from the user point of view, and
provides information to the TPP back-end that may enable efficient implementation.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

5

/ decre-

Unary TPP
Identity
Zero
Square
Increment
ment
Square root
Reciprocal
Rcp. Sqrt.
Exp
PRNG
(De)Quantize
Reduce

Transform

Unpack

/

Replicate columns
Gather / Scatter
2D Gather / 2D Scat-
ter
2D-strided loads
stores
Tanh &Tanh_inv
RELU & RELU_inv
Sigmoid
moid_inv
GELU & GELU_inv
Dropout
Dropout_inv

Sig-

&

&

Description/Comments
Copies input to output. Given input/output datatype, it performs datatype conversions
Fills output with zeros
Squares input and stores to output
Increments / Decrements input by 1 and stores to output

Computes the square root of input and stores to output
Computes the reciprocal of input and stores to output
Computes the rcp. sqrt. of input and stores to output
Computes the exponential value of the input tensor entries and stores them to output
Generates an output tensor with pseudo-random entries
Quantizes / Dequantizes the input
Reduces the rows/columns of the input and stores to output. The reduction function can be
SUM/MUL/MIN/MAX; (optionally) reduces the squared input
Transforms input and stores to output. Transformations are: Transpose, VNNI formatting, and
VNNI to VNNI-transpose
Takes each entry 𝑥𝑖,𝑗 of the input tensor, splits it in two parts 𝑥𝑙𝑜
and stores them in two tensors 𝑋 𝑙𝑜
Takes an input column/vector, replicates it a variable number of times and forms the output
Gathers/Scatters rows/columns from input and forms the tensor
Gathers/scatters elements from input using 2D offsets

𝑖,𝑗 with same bit-width,

𝑖,𝑗 and 𝑥ℎ𝑖

, 𝑋 ℎ𝑖

Loads/stores elements from/to a tensor using primary and secondary strides

Computes the hyperbolic tangent function (or its inv used for back-propagation) on input
Apply a Rectified Linear Unit function (or its inv used for back-propagation) on input
Computes the logistic sigmoid (or its inv used for back-propagation) on input

Apply a Gaussian Error Linear Unit function (or its inv used for back-propagation) on input
Drops out values from the input tensor with probability 𝑝. For the inv/back-propagation pass,
the same dropped units are zeroed out

Table 1. Unary TPPs

2.2 TPP Arguments
As mentioned in the previous subsection, the input to TPPs are 2D tensors. Each 2D tensor can be specified by
the number of rows 𝑀, columns 𝑁 , its leading dimension 𝑙𝑑 and its datatype 𝑑𝑡𝑦𝑝𝑒. Additionally, during runtime
each tensor gets fully characterized by specifying its location/address as primary info, optional companion
tensor info as secondary (e.g. sparsity bitmask), and optionally tertiary info (e.g. in case the tensor shape is
dynamically determined at runtime, this info may contain variables specifying 𝑀/𝑁 ). Each TPP also specifies the
shape/precision of the produced/output 2D tensor.

Each TPP also supports input tensors with broadcast semantics. More specifically, TPPs accept optional flags
dictating that the input 2D tensor should be formed by broadcasting a column/row/scalar 𝑁 /𝑀/𝑀 × 𝑁 times
respectively. Finally, the TPPs accept optional flags which further specify the TPP operation. For example, in case
a TPP is computing a transcendental function, the flags may be specifying various approximation algorithms
used for the computation. In the next subsection we present the TPPs in three groups: unary, binary, and ternary
TPPs given the number of input tensors they accept.

2.3 The TPP collection
First, we highlight the ternary Batch-Reduce GEMM (BRGEMM) TPP which is the main building block for general
tensor contractions in DL kernels [21]. BRGEMM materializes the operation 𝐶 = 𝛽 · 𝐶 + (cid:205)𝑛−1
𝐴𝑖 × 𝐵𝑖 . In essence,
𝑖=0

6

• E. Georganas et al.

Binary TPP
Add
Sub
Mul
Div
Max/Min
MatMul
Pack

Compare

Description/Comments
Add two inputs
Subtracts two inputs
Multiples (elementwise) two inputs
Divides two inputs
Finds element-wise max/min of two inputs
Performs matrix multiplication of two input
Concatenates pairs of entries 𝑥𝑙𝑜
output 𝑋
Compares element-wise two inputs and stores a bitmask of the comparison

𝑖,𝑗 from the inputs 𝑋 𝑙𝑜

𝑖,𝑗 and 𝑥ℎ𝑖

, 𝑋 ℎ𝑖

into 𝑥𝑖,𝑗 and stores it to the

Table 2. Binary TPPs

Ternary TPP
GEMM
Batch-Reduce
GEMM
(N)MulAdd

Blend

Description/Comments
Performs on 2D inputs 𝐴, 𝐵, 𝐶, scalar 𝛽: 𝐶 = 𝛽𝐶 + 𝐴 × 𝐵
Performs on 2D inputs 𝐴𝑖 , 𝐵𝑖 (with 𝑖 = 0, 1,. . ., 𝑛 − 1), 𝐶, scalar 𝛽: 𝐶 = 𝛽𝐶 + (cid:205)𝑖=𝑛−1

𝑖=0

𝐴𝑖 × 𝐵𝑖

Performs on 2D inputs 𝐴, 𝐵, 𝐶: 𝐶 = 𝐶 + 𝐴 ⊙ 𝐵 (or 𝐶 = 𝐶 − 𝐴 ⊙ 𝐵 ); ⊙ denotes element-wise
multiplication
Blends 2D input tensors 𝐴, 𝐵 according to bitmask 𝐶

Table 3. Ternary TPPs

𝑖

this kernel multiplies the specified blocks 𝐴𝑀×𝐾
and reduces the partial results to a block 𝐶𝑀×𝑁 . It
and 𝐵𝐾×𝑁
𝑖
is noteworthy that tensors 𝐴 and 𝐵 can alias and also the blocks 𝐴𝑖 and 𝐵𝑖 can reside in any position in the
input (potentially high-dimensional) tensors 𝐴 and 𝐵. Previous work [21] has shown that this single building
block is sufficient to express efficiently tensor contractions in the most prevalent DL computational motifs,
namely: Convolution Neural Networks (CNN), Fully-Connected networks (FC), Multi-Layer Perceptrons (MLP),
Recurrent Neural Networks (RNN)/Long Short-Term Memory (LSTM) Networks. In Section 5 we exhibit how
BRGEMM can be further used to build efficient Attention Cells that comprise the cornerstone of modern Natural
Language Processing (NLP) workloads. BRGEMM can be specialized to one of the following three variants that
may enable more efficient implementations on various platforms: (i) address-based BRGEMM, where the addresses
of the blocks 𝐴𝑖 and 𝐵𝑖 are explicitly provided by the user, (ii) offset-based BRGEMM, where the addresses of
𝐴𝑖 and 𝐵𝑖 can be computed as 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐴𝑖 = 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐴 + 𝑜ff𝑠𝑒𝑡𝐴𝑖 and 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐵𝑖 = 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐵 + 𝑜ff𝑠𝑒𝑡𝐵𝑖 , and
(iii) stride-based BRGEMM, where the addresses of 𝐴𝑖 and 𝐵𝑖 are: 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐴𝑖 = 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐴𝑖−1 + 𝑠𝑡𝑟𝑖𝑑𝑒_𝐴 and
𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐵𝑖 = 𝑎𝑑𝑑𝑟𝑒𝑠𝑠_𝐵𝑖−1 + 𝑠𝑡𝑟𝑖𝑑𝑒_𝐵. In subsection 3.2 we present the implementation of the BRGEMM TPP in
more depth for various ISAs and platforms.

Table 1 presents the unary TPPs that accept one 2D tensor as input. Since most of these TPPs map directly
to the equivalent math function, we further elaborate only on the ones which are more complex. The Identity
TPP essentially copies the input to the output. Since the input and output are fully specified in terms of their
precision, this TPP can be also used to perform datatype conversions between tensors.

The Quantize & Dequantize TPPs are used to quantize/dequantize the input tensor whereas the exact algorithm

employed is specified by a TPP flag.

The Transform TPP uses a flag to determine the exact transformation applied on the input 2D tensor. The
Transpose transformation is the usual mathematical matrix transpose. The rest two types of transformation,
namely VNNI formatting, and VNNI to VNNI-transpose are DL specific. More specifically, modern hardware
(e.g. Intel’s Cooper Lake) requires tensors to be in specific format called VNNI in order to employ hardware
acceleration for specific operations, e.g. dot-products (see section 3.2.2 for more details). This format represents

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

7

a logical 2D tensor [𝐷1] [𝐷0] as a 3D tensor [𝐷1/𝛼] [𝐷0] [𝛼] where essentially the dimension 𝐷1 is blocked in
chunks of size 𝛼, which in turn are set as the inner-most tensor dimension. The VNNI formatting TPP performs
this exact transformation: [𝐷1] [𝐷0] → [𝐷1/𝛼] [𝐷0] [𝛼] and the VNNI to VNNI-transpose transposes a tensor
which is already laid out in VNNI format, i.e. performs [𝐷1/𝛼1] [𝐷0] [𝛼1] → [𝐷0/𝛼0] [𝐷1] [𝛼0]. In subsection 3.3.1
we outline how the Transform TPPs are implemented via Shuffle Networks.

The last four entries of Table 1 correspond to DL-specific operations. They correspond to activation functions
typically encountered in DL workloads. All these activation functions have a counterpart which is required
during the back-propagation pass of training DL networks. These DL specific TPPs could be built on top of
other TPPs, however since they are prevalent in DL workloads we opt to define them as self-contained TPPs for
ease of usage. In subsection 3.3.2 we describe the TPP implementation of non-linear approximations for several
activation functions on various ISAs.

Table 2 and Table 3 present the binary/ternary TPPs that accept two/three 2D tensor as inputs respectively.

3 TPP IMPLEMENTATION
In this Section we briefly describe our Proof-Of-Concept (POC) implementation of the TPP specification. Our
implementation targets multiple CPU architectures from various vendors that support different ISAs, but could
be readily extended to support even GPU ISAs. We build upon and extend the open source LIBXSMM [22]
library which leverages JIT techniques. Such JIT techniques have been successfully used for optimal code
generation on CPUs by taking advantage of the known (at runtime) tensor shapes/dimensions in HPC and
DL applications [21–23]. Nevertheless, the TPP specification is platform-agnostic and does not preclude any
TPP back-end implementation. In our POC implementation, the usage of TPPs is governed by two APIs: i) A
dispatch API with which the user can request the code generation of a specific TPP, and such a dispatch call JITs
a function implementing the requested operation, ii) an API to call the JITed TPP kernel. First, in Subsection 3.1
we provide a generic blueprint of our TPP implementation. Then, in subsection 3.2 we describe in more detail
the BRGEMM TPP implementation which comprises the main tensor contraction tool in the TPP abstractions.
Subsection 3.3.1 details the implementation of the unary transform TPPs via shuffle networks since their efficient
implementation diverts from the generic TPP blueprint. Finally, subsection 3.3.2 outlines the approximation
techniques we leverage in our TPP implementation of non-linear activation functions; such approximations are
essential in achieving high-performance, while at the same time their accuracy is sufficient for the purposes of
training DL workloads.

3.1 Generic TPP Implementation Blueprint
Algorithm 1 exhibits at a high-level the pseudocode that is used to implement the Unary/Binary/Ternary TPPs
in a unified fashion. The inputs of the TPPs are tensors 𝑋 , 𝑌 (in case of binary/ternary TPPs) and 𝑍 (in case of
ternary TPP), and an output tensor 𝑂. For the purposes of this simplified presentation we assume all tensors are
of size 𝑀 × 𝑁 , however, depending on the operation these might have different sizes. For example, if the unary OP
is a reduction-by-columns and the input is 𝑀 × 𝑁 , then the output is an 𝑀 × 1 vector. First, we show that the 𝑀/𝑁
loops are blocked with factors 𝑚𝑏/𝑛𝑏 such that the working sets of each microkernels fits on the available register
file. The latter is architecture specific, e.g. AVX2-enabled ISAs expose 16 256-bit vector registers, AVX512-enabled
ISAs expose 32 512-bit vector registers and Aarch64 features 32 128-bit (NEON)/512-bit (SVE) vector registers.
The “load_generic" function used in Algorithm 1 denotes the loading of a sub-tensor to a register block; this
load may imply row/column/scalar broadcast semantics if the user specified the TPP in that way, or even may
have strided-load/gather semantics if the TPP involves a strided-load/gather operation. Also, for simplicity we do
not show here the handling of “secondary" fields of the tensors that may be required (e.g. indices array for the
gather operation, bitmasks arrays). Additionally, the generic load also handles datatype conversion, for instance

8

• E. Georganas et al.

Algorithm 1 The generic unary/binary/ternary TPP algorithm

𝑀×𝑁 if binary/ternary)

𝑀×𝑁 , Z

𝑀×𝑁 , (Y
𝑀×𝑁

Inputs: X
Output: O
1: for 𝑖𝑛 = 0 . . . 𝑁 − 1 with step nb do
2:
3:
4:

for 𝑖𝑚 = 0 . . . 𝑀 − 1 with step mb do

⊲ Generic loads, may have broadcast/gather semantics,
⊲ and may perform datatype conversions
𝑋𝑏 ← load_generic 𝑚𝑏 × 𝑛𝑏 𝑋 -subblock𝑖𝑚,𝑖𝑛
if (unary TPP) then

𝑋𝑏 ← Unary_op(𝑋𝑏)

if (binary TPP) then

𝑌𝑏 ← load_generic 𝑚𝑏 × 𝑛𝑏 𝑌 -subblock𝑖𝑚,𝑖𝑛
𝑋𝑏 ← Binary_op(𝑋𝑏, 𝑌𝑏)

if (ternary TPP) then

𝑌𝑏 ← load_generic 𝑚𝑏 × 𝑛𝑏 𝑌 -subblock𝑖𝑚,𝑖𝑛
𝑍𝑏 ← load_generic 𝑚𝑏 × 𝑛𝑏 𝑍 -subblock𝑖𝑚,𝑖𝑛
𝑋𝑏 ← Ternary_op(𝑋𝑏, 𝑌𝑏, 𝑍𝑏)

⊲ Generic store, may have scatter semantics, and may
⊲ perform datatype conversion
←−−−−−−−−−−−−
𝑂-subblock𝑖𝑚,𝑖𝑛
store_generic 𝑋𝑏

5:
6:
7:

8:

9:
10:

11:
12:

13:
14:

15:
16:

17:

provided the input is in bfloat16 (BF16) [24] whereas the compute is going to happen in FP32 precision. Once
all the required sub-tensors are loaded, then the corresponding Unary/Binary/Ternary operator is applied. This
operator may be directly mapped to an available instruction (e.g. a vector add in case of binary addition), or
to a sequence of instructions for more complicated operators (e.g. reductions, random number generation via
xorshift algorithm [25], approximation algorithms for transcendental functions [26]). Last but not least, the
optimal sequence generation depends on the available instructions and this is handled by the TPP back-end/JITer.
For example, some ISAs may have masking/predicate support (e.g. AVX512 & SVE) that enable efficient handling
of loop remainders, the selected unrolling degree heavily depends on the instructions in use, their latency and
the number of available architectural registers. Once the result is computed, the resulting register block is stored
back to the corresponding output sub-tensor position. Similarly to the generic load, the “generic" store may
induce strided accesses or may be even a scatter operation. Additionally, the generic store also handles potential
datatype conversions.

3.2 The BRGEMM TPP Implementation
3.2.1 The BRGEMM kernel structure

We present in more detail the BRGEMM TPP because it comprises the tensor contraction tool in the TPP
abstraction, and is ubiquitous in the DL kernels and workloads described in Section 5. Algorithm 2 exhibits the
high-level algorithm implementing: 𝐶 = 𝛽 · 𝐶 + (cid:205)𝑛−1
𝐴𝑖 × 𝐵𝑖 . Lines 1-2 block the computation of the result 𝐶
𝑖=0
in 𝑚𝑏 × 𝑛𝑏 tensor sub-blocks. Once such a subblock is loaded into the accumulation registers (line 3), we loop
over all pairs 𝐴𝑖, 𝐵𝑖 (line 4) and we accumulate into the loaded registers the products of the corresponding
𝑚𝑏 × 𝐾 subblocks of 𝐴𝑖 with the relevant 𝐾 × 𝑛𝑏 subblocks of 𝐵𝑖 (lines 5-7). In order to calculate a partial
product of an 𝑚𝑏 × 𝑘𝑏 sub-panel of 𝐴𝑖 with a 𝑘𝑏 × 𝑛𝑏 sub-panel of 𝐵𝑖 , we follow an outer product formulation.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

9

𝑖

, 𝐵𝐾×𝑁
𝑖

Algorithm 2 The batch-reduce GEMM TPP
Inputs: 𝐴𝑀×𝐾
for 𝑖 = 0, ..., 𝑛-1, 𝐶𝑀×𝑁 , 𝛽 ∈ IR
Output: 𝐶 = 𝛽 · 𝐶 + (cid:205)𝑛−1
𝑖=0
1: for 𝑖𝑛 = 0 . . . 𝑁 − 1 with step nb do
2:
3:
4:

acc_regs ← load_generic 𝑚𝑏 × 𝑛𝑏 𝐶-subblock𝑖𝑚,𝑖𝑛
for 𝑖 = 0 . . . 𝑛 − 1 with step 1 do

for 𝑖𝑚 = 0 . . . 𝑀 − 1 with step mb do

𝐴𝑖 × 𝐵𝑖

5:
6:
7:

8:

for 𝑖𝑘 = 0 . . . 𝐾 − 1 with step kb do
⊲ Outer product GEMM microkernel
acc_regs += 𝐴𝑖 sub-panel𝑖𝑚,𝑖𝑘 × 𝐵𝑖 sub-panel𝑖𝑘 ,𝑖𝑛

𝐶-subblock𝑖𝑚,𝑖𝑛

←−−−−−−−−−−−−
store_generic acc_regs

Fig. 2. Outer product GEMM microkernels, Left: On a platform with 32 vector registers, Middle: On a platform with 16 vector
registers, Right: On a platform with 8 2D registers (tiles).

The loading of 𝐴𝑖 and 𝐵𝑖 sub-panels, and the outer-product formulation is heavily dependent on the target
platform. We provide BRGEMM implementations for multiple x86 ISAs: SSE, AVX, AVX2, AVX512, including the
recently introduced Intel AMX (Advanced Matrix Extensions) ISA [27]. Additionally, we have implemented the
BRGEMM TPP for AArch64 and ARMv8 SVE ISAs. Depending on the targeted platform, the “register" can be
either a typical vector register with varying width (e.g.128-512bit vector length), or in the case of AMX-enabled
target the “register" is a 2D tile-register. Similarly, the outer-product formulation may employ the available
Fused-Multiply-Add (FMA) instructions, or even 2D tile-multiplication instructions. In all these cases, the TPP
implementation emits the appropriate load/store/prefetch/FMA instructions, and takes into account the available
architectural registers/unrolling factors/instruction mix in order to achieve close to peak performance. Last but
not least, the BRGEMM supports multiple datatypes (FP64, FP32, BF16, INT8), and whenever possible employs
hardware acceleration, e.g. via specialized FMA instructions for INT8/BF16 datatypes. In order to highlight the
differences of the outer product GEMM microkernels that are heavily dependent on the target platform, we show
in Figure 2 three different implementations.

Figure 2-Left shows an exemplary outer product microkernel on a platform with 32 available vector registers,
for example an x86 with AVX512 or on ARM AArch64/SVE. In this case vector register v7-v30 constitute the
accumulators, vector registers v1-v6 hold a broadcasted subrow of B, and vector register v0 is used to load a
partial subcolumn of A. First, we load on v1-v6 a subrow of B via broadcasts, then we load on v0 the first chunk
of the A subcolumn and with 6 fused multiply-add (FMA) instructions (v0 with v1-v6) we multiply-and-add

SC’19,November2019,Denver,CO,USAE.Georganasetal.operationsthatariseinDLworkloads,whereasitssemanticslendthemselvestovariousoptimizations(e.g.load/storeoptimizationsoftheresultsub-tensor,prefetchingofthesub-tensorstobemulti-plied).Also,sincethekernelsupportsoperationsat￿negranularity,fusionofsubsequentoperatorsontheoutputsub-blocksisinher-entlye￿cient.ThebluelineinFigure1showstheperformanceoftheconvolutionprimitivethatleveragesournewbatch-reduceGEMMkernelachievingaveragee￿ciencyof83%,andoutperformseventheadhoc,vendor-optimizedkernel.Havingasinglekernelasbasicbuilding-blockistransformative:byimplementingandoptimizingthissinglekernelforagivenarchitecture,thedevelopmentofDLprimitivesdegeneratestomerelooptuningaroundthiskernel.Essentiallyourapproachwithasinglekerneladdressestheissueofcombinatorialexplosionoflow-leveloptimizationworkthatisrequiredforeachpair<architecture,DLprimitive>.Instead,foreacharchitectureweneedtooptimizeatlow-levelonlyonekernelforallDLprimitives.Furthermore,havingasingle,highlye￿cientbuilding-blockenablese￿cientusageoftensorcompilerframeworks.Suchframe-worksembracetensorsas￿rstclasscitizens,andprovidespeci￿cop-timizationtechniquestargetingtensoralgebraprograms.SinceDLprimitivesareinherentlytensoralgebraprograms,thereisalargeamountofongoingresearchthatleveragesspecializedtensorcom-pilersforDLworkloaddevelopment(e.g.TVM[20],GLOW[21],PlaidML[22],MLIR[23]).However,compilersstruggletooptimizesmallGEMM-￿avoredloopneststhatariseintensorprograms[24].Contemporaryarchitecturesbecomeincreasinglycomplex,andallthemicro-architecturalidiosyncrasieshavetobeconsideredinor-dertoachieveclose-to-peakperformance.Ourkernelisoptimizedforthenuancesofthearchitectureathand,andservestensorcom-pilersarobustbuildingblockthatcanbeusedduringthepolyhedraloptimizationphaseofgeneralloopnests[22,25].Toillustratetheviabilityandgeneralityofourmethodologywithasinglekernel,wedevelopDLprimitiveswhichtargettrainingandinferenceofRNN/LSTM,CNNandMLPworkloadsin⇠3,000linesofhigh-levelCcode.Ourprimitivesoutperformvendor-optimizedlibrariesonCPUs.Wealsoprovideproof-of-conceptdesignwithatensorcompilerframeworkbyshowcasinge￿cientCNNimple-mentationinTVMthatleveragesourbatch-reduceGEMMkernel.Additionally,ourmethodologyprovidesapathwayforperformanceportability;wepresentexemplary,high-performanceCNNkernelsonintegratedGPUs.Lastbutnotleast,weintegrateourprimitivesindistributedDLframeworks(Tensor￿ow[26]andGxM[27]),andshowperformanceresultsontwotrainingworkloads:Google’sNeu-ralMachineTranslation(GNMT)[5]andResNet-50training[28].TheseresultspushtheenvelopeofDLtrainingperformanceonCPUclusters.Themaincontributionsofthispaperare:•Theintroductionofthebatch-reduceGEMMkernelalongwithitse￿cientimplementation.•Thedesignandimplementationofmulti-threaded,highper-formanceDLprimitivescoveringRNN/LSTM,CNNandMLPinferenceandtrainingalgorithmswithbatch-reduceGEMMkernelbeingthebasicbuildingblock.Weneedtooptimizeatlow-levelonlythiskernelforallDLprimitives.•AdetailedperformancecomparisonofourDLprimitiveswithstate-of-the-artvendor-optimizedlibraries.A0A1A2A3B0B1B2B3CjTensor A Tensor B Tensor C !"=$∗!"+ α ∑'(∗)(*+,(-.(b)AisubcolumnCjaccumulators in 24 vector registers(a)bcastBisubrowin 6 vecregisters30 029 28 27 26 25 24 23 22 2120 19 18 1716 15 1413 12 11 10 9 8 7 65 4 3 2 1 Figure2:(a)Thebatch-reduceGEMMkernel(b)Outerprod-uctsmallGEMMmicrokernelAlgorithm1Thebatch-reduceGEMMkernelInputs:Ai2IRm⇥k,Bi2IRk⇥ni=0,...,N-1,Cj2IRm⇥n , 2IROutput:Cj= ·Cj+ PN 1i=0Ai·Bi1:forin=0...n 1withstepnbdo2:forim=0...m 1withstepmbdo3:acc_regs loadmb⇥nbCjsubblockim,in4:fori=0...N 1withstep1do5:forik=0...k 1withstep1do6:.OuterproductGEMMmicrokernel7:acc_regs+=Aisubcolumnim,ik⇥Bisubrowik,in8:Cjsubblockim,in acc_regs•DistributedmemoryresultsofLSTMandCNNtrainingwork-loadsthatleverageouroptimizedDLkernelsandoutperformthebestinclassresultsonCPUclusters.•CNNproof-of-conceptresultsonintegratedGPUsandCNNker-nelswithinTVMthatleveragethebatch-reduceGEMMkernel.2THEBATCH-REDUCEGEMMKERNELInthissection,wedescribethedesignandimplementationofthenewbatch-reduceGEMMkernelwhichcomprisesthecornerstoneofourdeeplearningprimitives.Figure2(a)illustratesthefunction-alityofthenewkernelwhichmaterializestheoperation:Cj= ·Cj+ N 1Xi=0Ai·BiThiskernelmultipliesthespeci￿edblocksAi2IRm⇥kandBi2IRk⇥nandreducesthepartialresultstoablockCj2IRm⇥nofatensorC.TensorsAandBcanaliasandalsotheblocksAiandBicanresideinanypositionintheinputtensorsAandB.Thebatch-reduceGEMMkerneltakesthefollowingarguments:(i)twoarraysofpointerstothecorrespondingblocksAiandBitobemultiplied,(ii)apointertotheoutputblockCj,(iii)thenumberNoftheblockstobemultipliedand(iv)thescalingparameters and .Ourkerneldi￿ersfromtherecentlyintroducedbatchedGEMM[19]anditsvariationstrided-batch-gemm[29]thatmaterialize:Ci= ·Ci+ ·Ai·BiThesebatchedroutinesaremissingthereductionfunctionalityandcannotoptimizefortheoutputmatrixre-use.Also,thestrided-batch-gemmkernelaccessestheAiandBisubblocksbasedon￿xedstridesandthereforeismorerestrictive.Thenewbatch-reduceGEMMkernelspeci￿cationnaturallylendsitselftoahandfulofoptimizations.First,thiskernelmin-imizestheoutputdatamovementcomparedtoGEMMorbatchedAisubcolumn015 1413 12 11 10 9 8 7 65 4 3 2 1 bcastBisubrowin 3 vecregistersCjaccumulators in 12 vector registersTile 0Tile 1Tile 2Tile 3Tile 4Tile 5Tile 6Tile 7AisubpanelBisubpanelCjaccumulators in4 tiles10

• E. Georganas et al.

the corresponding partial results on the accumulators v7-v12 (first logical row of accumulators). Then, we load
on v0 the second chunk of the A subcolumn, and subsequently with yet another 6 FMA instructions (v0 with
v1-v6) we multiply-and-add the computed partial results on the accumulators v13-v18 (second logical row of
accumulators) etc. The registers v1-v6 are reused 4 times throughout the outer product computation, and v0
is reused 6 times for each loaded A chunk. In other words, the corresponding A subcolumn and B subrow are
loaded from memory/cache into the vector registers exactly once and we get to reuse them from the register file.
Also, in such a formulation we expose 24 independent accumulation chains which is critical in order to hide the
latency of the FMA instruction. Last but not least, the platform (i.e. vector register width) and the datatype of
the microkernel determine the exact values of the blocking parameters 𝑚𝑏, 𝑛𝑏, and 𝑘𝑏. For example for single
precision datatype FP32 and an x86 AVX512 platform, each vector register can hold 16 FP32 values (the vector
registers are 512-bit wide). Therefore, this microkernel operates with blocking values 𝑚𝑏 = 64, 𝑛𝑏 = 6, and 𝑘𝑏 = 1
and it calculates a small matrix multiplication 𝐶64×6 += 𝐴64×1 × 𝐵1×6.

Figure 2-Middle shows an exemplary outer product microkernel on a platform with 16 vector registers, for
example an x86 with up to AVX2 ISA. The microkernel is similar with the previous case; since we have only 16
vector registers available, we dedicate 12 of those as 𝐶 accumulators, 3 vector register are utilized for holding a
partial B subrow, and 1 vector register is used to load a chunk of an A subcolumn. In this case 12 independent
accumulation chains are also sufficient to hide the FMA latency. Analogously to the previous case, for single
precision datatype FP32 and an x86 AVX2 platform, each vector register can hold now 8 FP32 values (the vector
registers are now 256-bit wide). Thus, this microkernel operates with blocking values 𝑚𝑏 = 32, 𝑛𝑏 = 3, and 𝑘𝑏 = 1
and it calculates a small matrix multiplication 𝐶32×3 += 𝐴32×1 × 𝐵1×3.

Figure 2-Right shows a small GEMM microkernel on a platform with 8 2D registers (tiles), for example what
is available in the recently introduced Intel AMX (Advanced Matrix Extensions) ISA. In this case each 2D tile
register has size (up to) 1KB, logically holds (up to) 16 rows of a submatrix, and can be loaded with a proper
tile-load instruction. In this particular example, tiles 0-3 comprise the 𝐶 accumulators, tiles 4-5 are used to hold a
subpanel of A and tiles 6-7 are used to hold a subpanel of B. Once we load the subpanels of A and B onto the
respective tiles, we can perform 4 tile multiply-and-add instructions: 𝑡𝑖𝑙𝑒0 += 𝑡𝑖𝑙𝑒4 × 𝑡𝑖𝑙𝑒6, 𝑡𝑖𝑙𝑒1 += 𝑡𝑖𝑙𝑒4 × 𝑡𝑖𝑙𝑒7,
𝑡𝑖𝑙𝑒2 += 𝑡𝑖𝑙𝑒5 × 𝑡𝑖𝑙𝑒6 and 𝑡𝑖𝑙𝑒3 += 𝑡𝑖𝑙𝑒5 × 𝑡𝑖𝑙𝑒7, and we update the 𝐶 accumulators. In such a microkernel,
each A/B tile is reused 2 times. Given each tile may have size up to 1KB and may hold up to 16 rows of a
submatrix, by considering BF16 datatype for A/B matrices and FP32 accumulator tiles, such a microkernel
operates with blocking values 𝑚𝑏 = 32, 𝑛𝑏 = 32, 𝑘𝑏 = 32, and can compute (up to) a small matrix multiplication
𝐶32×32 += 𝐴32×32 × 𝐵32×32. Each A/B tile represents a logical 16 × 32 BF16 A/B submatrix, and each C tile represents
a 16 × 16 FP32 accumulator. The AMX instructions will be available within the upcoming Intel Xeon processors
code-named Sapphire Rapids, and the corresponding BF16-input/FP32-output tile multiplication instructions can
deliver up to 16× more FLOPs/cycle compared to FP32 AVX512 FMA instructions on current Xeon platforms.

These considerably different GEMM microkernel variants highlight yet another aspect of the TPPs: The TPPs
specify what needs to be done rather than how it is done/implemented. In this case, the user may just specify/em-
ploy a BRGEMM TPP in order to perform a tensor contraction, whereas the TPP backend/implementation is
responsible for generating the optimal code for each platform at hand. In this methodology, all the architectural
nuances are hidden completely by the user, and the same exact user code written in terms of TPPs may be reused
across platforms with different characteristic/ISAs without sacrificing performance or portability.

3.2.2 Mixed Precision BRGEMM and its emulation

While the previous section presents the general structure of mapping matrix multiplication to various physical
ISAs, this paragraph is used to demonstrate how the idea of a virtual ISA allows to implement operations efficiently
which are not natively supported by a specific physical ISA. The example we are choosing here is our GEMM
kernel and its support for bfloat16 and int8 on architectures which don’t support these novel ISA SIMD-extension.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

11

Fig. 3. Mixed-precision dot-product instructions, Left: 16 bit integer and bfloat16 on Intel AVX512, Middle: 8bit integer using
Intel AVX512, Right: 8 bit integer using ARM ASIMD.

Before going into the details of the emulation, we first need to introduce special memory layouts which are
used by x86 and aarch64 mixed-precision dot-product instructions as shown in Figure 3. As we can see in all cases
(x86/aarch64 and bf16/int8), the overall concept is identical: Although doing mixed-precision and mixed-datatype-
length computations, these instructions are functioning from a matrix multiplication point-of-view similar to
32 bit instructions. This is achieved by having an implicit 2-wide (BF16/int16) and 4-wide (int8) dot-product of 𝐴𝑖
and 𝐵𝑖 values leading to a horizontal summation per single 32 bit 𝐶𝑖 , e.g. 𝐶0 = 𝐴0 · 𝐵0 +𝐴1 · 𝐵1 +𝐴2 · 𝐵2 +𝐴3 · 𝐵3 +𝐶0
as shown for the int8 variant. If we apply blockings with these instructions as discussed in Figure 2-Left and
Figure 2-Middle, then we realize that matrix 𝐵 is still read via 32-bit broadcast (containing 2 16-bit or 4 8-bit
values along the inner-product or common dimension). However, matrix 𝐴 is in need of reformatting. This is
due to the fact that the GEMM kernel in Figure 2-Left and Figure 2-Middle requires full SIMD-width contiguous
loads for optimal performance (which is along 𝑀 and not 𝐾). Therefore, we need to reformat 𝐴 into [𝐾𝑜 ] [𝑀] [𝐾𝑖 ]
with 𝐾𝑜 · 𝐾𝑖 = 𝐾 and 𝐾𝑖 = 2 for 16-bit and 𝐾𝑜 = 4 for 8-bit inputs. We refer to such a format as VNNI-format
throughout this paper. After such reformatting of 𝐴, we can perform full SIMD loads on 𝐴; combined with the
32-bit broadcast loads on 𝐵 we have a 32-bit GEMM kernel which has a shorter 𝐾 dimension, 2× for 16-bit
datatypes and 4× for 8-bit datatypes.

In case these novel instructions are not available, especially for bfloat16 as this is a relatively new format, one
might think, that an efficient mapping to a classic FP32 SIMD ISA is not possible. This is correct as long as the
machine does not offer int16 support. However, with int16 support and SIMD masking we can implement the
aforementioned non-trivial mixed-precision dot-product efficiently and even bit-accurately as shown in Figure 4.
This is done by processing 𝐾𝑖 in two rounds in the case of bfloat16 datatype. As shown in Figure 4 we first process
the odd (or upper) bfloat16 number. This is done by exploiting the fact that a bfoat16 number perfectly aliases
with an FP32 number in its 16 MSBs. Therefore, on AVX512 we can just execute a full SIMD load as a 16-bit-typed
load with masking. As a mask we chose 0xaaaaaaaa and as masking-mode we use zero masking. With this
trick we automatically turn on-load the upper bfloat16 numbers in 𝐴 into 16 valid FP32 numbers, and for 𝐵 we
broadcast and then perform an overriding register move. A little bit more work is needed for the lower/even
bfloat16 number: In this case we perform an unmasked load and then we use a 32-bit integer shift by 16 to
create valid FP32 numbers. A simple inspection of the instruction sequence in Figure 4 shows that we are mainly
executing fused-multiply-add instructions with little overhead compared to a classic FP32 GEMM as illustrated in
Figure 2-Left and Figure 2-Middle. Therefore, we can execute a bfloat16 GEMM with a reformatted matrix 𝐴 with
close to FP32-peak and still benefit from the smaller memory footprint (and therefore a small performance gain,

A0A1A2A3...A63B0B1B2B3...B63C0...C15A0B0+A1B1+A2B2+A3B3+C0...A60B60+A61B61+A62B62+A63B63+C15src0src1srcdestX86 AVX512: vpdbusd512bit32bit8bitA0A1...A31C0...C15A0B0+A1B1+C0...A30B30+A31B31+C15src0src1srcdestX86 AVX512: vpdwssd, vpdbf16ps512bit32bit16bitB0B1...B31A0A1A2A3...A15B0B1B2B3...B15C0...C3A0B0+A1B1+A2B2+A3B3+C0...A12B12+A13B13+A14B14+A15B15+C3src0src1srcdestARM ASIMD: sdot128bit32bit8bit12

• E. Georganas et al.

Fig. 4. Emulation of a bit accurate GEMM kernel using AVX512F instructions matching a GEMM kernel as depicted in
Figure 2 using vdpbf16ps AVX512 instructions. The glossary contains detailed descriptions of the used intrinsic functions.

as we will show later in section 6). Replacement sequences for int16 and int8 matrix inputs can be carried out in
a similar way and their detailed discussion is skipped here.

In addition to the presented emulation of mixed-precision GEMM kernels using SIMD instructions, we have
also added support for emulation of Intel AMX instructions bit-accurately on AVX512. This addition enables
running numerical accuracy experiments, such as convergence studies, before the release of a chip that supports
Intel AMX instructions. A similar path is possible for ARM’s SME instruction set and subject to future work.
These emulation capabilities further highlight the aspect of TPP as a virtual tensor ISA.

3.3 Examples of Non-Trivial Non-GEMM TPPs
The previous sections covered most of the TPP implementations: straightforward element-wise unary/bina-
ry/ternary operations and various forms of mixed precision GEMMs including their emulation on older hardware.
However, there are cases in which we are not operating on the data in an element-wise fashion, e.g. transpose, or
the Unary_op, Binary_op or Ternary_op is not an elementary operation. The goal of this section is to shed some
light on these cases by presenting the transpose TPP in detail, and sketching fast non-linear approximations on
SIMD machines that match the accuracy requirements of deep learning applications.

3.3.1 Transform-Transpose TPP via Shuffle Networks

When working with matrices, the transpose kernel is ubiquitous. It is needed to access the matrix’s elements
in various contractions along the mathematically correct dimension. However, a transpose operation is scalar at
first sight. In this subsection we exhibit how transpose can be implemented using shuffle networks in a fully

__m512 c_0_0, c_0_1, c_0_2, c_0_3;…__m512 c_5_0, c_5_1, c_5_2, c_5_3;__m512 a_0, a_1, a_2, a_3;__m512b_0,b_1,b_2,b_3,b_4,b_5;__mmask32 m_0 = 0xaaaaaaaa;/*loadC */…/* load upper part of A & B and FP32 FMAs*/a_0=_mm512_maskz_loadu_epi16( m_0, ptrA);a_1 = _mm512_maskz_loadu_epi16( m_0, ptrA+32);a_2 = _mm512_maskz_loadu_epi16( m_0, ptrA+64 );a_3 = _mm512_maskz_loadu_epi16( m_0, ptrA+96 );b_0=_mm512_set1_epi32( ptrB);b_0 = _mm512_maskz_mov_epi16( m_0, b_0 );c_0_0=_mm512_fmadd_ps( a_0, b_0, c_0_0 );c_0_1 = _mm512_fmadd_ps( a_1, b_0, c_0_1 );c_0_2 = _mm512_fmadd_ps( a_2, b_0, c_0_2 );c_0_3 = _mm512_fmadd_ps( a_3, b_0, c_0_3 );b_1 = _mm512_set1_epi32( ptrB+ldb);b_1 = _mm512_maskz_mov_epi16( m_0, b_1 );c_1_0 = _mm512_fmadd_ps( a_0, b_1, c_1_0 );c_1_1 = _mm512_fmadd_ps( a_1, b_1, c_1_1 );c_1_2 = _mm512_fmadd_ps( a_2, b_1, c_1_2 );c_1_3 = _mm512_fmadd_ps( a_3, b_1, c_1_3 );b_2 = _mm512_set1_epi32( ptrB+2*ldb);b_2 = _mm512_maskz_mov_epi16( m_0, b_2 );c_2_0 = _mm512_fmadd_ps( a_0, b_2, c_2_0 );c_2_1 = _mm512_fmadd_ps( a_1, b_2, c_2_1 );c_2_2 = _mm512_fmadd_ps( a_2, b_2, c_2_2 );c_2_3 = _mm512_fmadd_ps( a_3, b_2, c_2_3 );b_3 = _mm512_set1_epi32( ptrB+3*ldb);b_3 = _mm512_maskz_mov_epi16( m_0, b_3 );c_3_0 = _mm512_fmadd_ps( a_0, b_3, c_3_0 );c_3_1 = _mm512_fmadd_ps( a_1, b_3, c_3_1 );c_3_2 = _mm512_fmadd_ps( a_2, b_3, c_3_2 );c_3_3 = _mm512_fmadd_ps( a_3, b_3, c_3_3 );b_4 =_mm512_set1_epi32( ptrB+4*ldb);b_4 = _mm512_maskz_mov_epi16( m_0, b_4 );c_4_0 =_mm512_fmadd_ps( a_0, b_4, c_4_0 );c_4_1 = _mm512_fmadd_ps( a_1, b_4, c_4_1 );c_4_2 = _mm512_fmadd_ps( a_2, b_4, c_4_2 );c_4_3 = _mm512_fmadd_ps( a_3, b_4, c_4_3 );b_5 = _mm512_set1_epi32( ptrB+5*ldb);b_5 = _mm512_maskz_mov_epi16( m_0, b_5 );c_5_0 = _mm512_fmadd_ps( a_0, b_5, c_5_0 );c_5_1 = _mm512_fmadd_ps( a_1, b_5, c_5_1 );c_5_2 = _mm512_fmadd_ps( a_2, b_5, c_5_2 );c_5_3 = _mm512_fmadd_ps( a_3, b_5, c_5_3 );/* load lower part of A & B and FP32 FMAs*/a_0 = _mm512_loadu_epi16( ptrA);a_1 = _mm512_loadu_epi16( ptrA+32 );a_2 = _mm512_loadu_epi16( ptrA+64 );a_3 = _mm512_loadu_epi16( ptrA+96 );a_0 = _mm512_slli_epi32( a_0, 16 );a_1 = _mm512_slli_epi32( a_1, 16 );a_2 = _mm512_slli_epi32( a_2, 16 );a_3 = _mm512_slli_epi32( a_3, 16 );b_0 = _mm512_set1_epi32( ptrB);b_0 = _mm512_slli_epi32( b_0, 16 );c_0_0 = _mm512_fmadd_ps( a_0, b_0, c_0_0 );c_0_1 = _mm512_fmadd_ps( a_1, b_0, c_0_1 );c_0_2 = _mm512_fmadd_ps( a_2, b_0, c_0_2 );c_0_3 = _mm512_fmadd_ps( a_3, b_0, c_0_3 );b_1 = _mm512_set1_epi32( ptrB+ldb);b_1 = _mm512_slli_epi32( b_1, 16 );c_1_0 = _mm512_fmadd_ps( a_0, b_1, c_1_0 );c_1_1 = _mm512_fmadd_ps( a_1, b_1, c_1_1 );c_1_2 = _mm512_fmadd_ps( a_2, b_1, c_1_2 );c_1_3 = _mm512_fmadd_ps( a_3, b_1, c_1_3 );b_2 = _mm512_set1_epi32( ptrB+2*ldb);b_2 = _mm512_slli_epi32( b_2, 16 );c_2_0 = _mm512_fmadd_ps( a_0, b_2, c_2_0 );c_2_1 = _mm512_fmadd_ps( a_1, b_2, c_2_1 );c_2_2 = _mm512_fmadd_ps( a_2, b_2, c_2_2 );c_2_3 = _mm512_fmadd_ps( a_3, b_2, c_2_3 );b_3 = _mm512_set1_epi32( ptrB+3*ldb);b_3 = _mm512_slli_epi32( b_3, 16 );c_3_0 = _mm512_fmadd_ps( a_0, b_3, c_3_0 );c_3_1 = _mm512_fmadd_ps( a_1, b_3, c_3_1 );c_3_2 = _mm512_fmadd_ps( a_2, b_3, c_3_2 );c_3_3 = _mm512_fmadd_ps( a_3, b_3, c_3_3 );b_4 =_mm512_set1_epi32( ptrB+4*ldb);b_4 = _mm512_slli_epi32( b_4, 16 );c_4_0 =_mm512_fmadd_ps( a_0, b_4, c_4_0 );c_4_1 = _mm512_fmadd_ps( a_1, b_4, c_4_1 );c_4_2 = _mm512_fmadd_ps( a_2, b_4, c_4_2 );c_4_3 = _mm512_fmadd_ps( a_3, b_4, c_4_3 );b_5 = _mm512_set1_epi32( ptrB+5*ldb);b_5 = _mm512_slli_epi32( b_5, 16 );c_5_0 = _mm512_fmadd_ps( a_0, b_5, c_5_0 );c_5_1 = _mm512_fmadd_ps( a_1, b_5, c_5_1 );c_5_2 = _mm512_fmadd_ps( a_2, b_5, c_5_2 );c_5_3 = _mm512_fmadd_ps( a_3, b_5, c_5_3 );/* store C*/…Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

13

Fig. 5. Sketch of a shuffle network for a 32-bit transpose of a 16×16 matrix using Intel AVX512 instructions. Via 4 stages
(each one having 16 independent shuffles that double in width per stage), the 16×16 matrix (256 elements) can be transposed
with only 64 instructions and fully leverages the 32 architectural registers.

Fig. 6. Comparison of X86 and ARM code for a simple 4×4 single precision transpose using unpack instructions. The glossary
contains detailed descriptions of the used intrinsic functions.

vectorized fashion, e.g. Figure 5 demonstrates how a 16×16 matrix with 256 32-bit elements can be transposed in
64 cycles using AVX512 instructions.

The shuffle-network presented in Figure 5 is a blueprint for all datatype-lengths and ISAs: in log2 SIMD-Length
stages we can transpose a matrix held in a set of SIMD registers. In this particular example, we need log2 16 = 4
stages and in each stage we increase the shuffling/interleaving width of logical elements, and also increase the
distance at which we access the 32 registers grouped into two sets of 16 registers each. More specifically, we
start with registers 𝑖0 to 𝑖15 and interleave elements at the same position in a pair of registers close to each other.

0123i045678910111213141516171819202122232425262728293031016117420521824925122813292183196227231026112714301531i1o0o132 bit lowinterleave32 bit highinterleave1st stage016117o042052182492512281329324833493652375340564157446045610163248420365282440561228446011733495213753925415713294561o2i0i164 bit lowinterleave64 bit highinterleaveRepeat 8x, for remining oX and iX pairs2nd stage01632484203652824405612284460648096112688410011672881041207692108124128 bit shuffle128 bit shufflei0i401632488244056648096112728810412042036521228446068841001167692108124Repeat 8x, for remining oX and iX pairso0o43rd stage128 bit shuffle128 bit shuffleo0o8Repeat 8x, for remining oX and iX pairsi0i84th stage016324882440566480961127288104120128144160176136152168184192208224240200216232248016324864809611212814416017619220822424082440567288104120136152168184200216232248Repeat 8x, for remining oX and iX pairs__m128 r0, r1, r2, r3, t0, t1, t2, t3;r0 = _mm_loadu_ps(in + 0*ldi);  //   0   1   2   3r1 = _mm_loadu_ps(in + 1*ldi);  //   4   5   6   7r2 = _mm_loadu_ps(in + 2*ldi);  //   8   9  10  11r3 = _mm_loadu_ps(in + 3*ldi);  //  12  13  14  15t0 = _mm_unpacklo_ps(r0,r1);   //   0   4   1   5t1 = _mm_unpackhi_ps(r0,r1);   //   2   6   3   7t2 = _mm_unpacklo_ps(r2,r3);   //   8  12   9  13t3 = _mm_unpackhi_ps(r2,r3);   //  10  14  11  15r0 = _mm_unpacklo_pd(t0,t2);   //   0   4   8  12r1 = _mm_unpackhi_pd(t0,t2);   //   1   5   9  13r2 = _mm_unpacklo_pd(t1,t3);   //   2   6  10  14r3 = _mm_unpackhi_pd(t1,t3);   //   3   7  11  15_mm_storeu_ps(out + 0*ldo, r0);_mm_storeu_ps(out + 1*ldo, r1);_mm_storeu_ps(out + 2*ldo, r2);_mm_storeu_ps(out + 3*ldo, r3);float32x4_t r0, r1, r2, r3, t0, t1, t2, t3;r0 = vld1q_f32(in + 0*ldi);  //   0   1   2   3r1 = vld1q_f32(in + 1*ldi);  //   4   5   6   7r2 = vld1q_f32(in + 2*ldi);  //   8   9  10  11r3 = vld1q_f32(in + 3*ldi);  //  12  13  14  15t0 = vtrn1q_f32(r0,r1);   //   0   4   1   5t1 = vtrn2q_f32(r0,r1);   //   2   6   3   7t2 = vtrn1q_f32(r2,r3);   //   8  12   9  13t3 = vtrn2q_f32(r2,r3);   //  10  14  11  15r0 = vtrn1q_f64(t0,t2);   //   0   4   8  12r1 = vtrn2q_f64(t0,t2);   //   1   5   9  13r2 = vtrn1q_f64(t1,t3);   //   2   6  10  14r3 = vtrn2q_f64(t1,t3);   //   3   7  11  15vst1q_f32(out + 0*ldo, r0);vst1q_f32(out + 1*ldo, r1);vst1q_f32(out + 2*ldo, r2);vst1q_f32(out + 3*ldo, r3);X86 CodeARM Code14

• E. Georganas et al.

This constructs now pairs of 32 bit values in 𝑜0 and 𝑜1 which are already containing the transpose’s result for 2
out of 16 elements and we repeat this for all other 7 input register pairs. The analogous transformation is now
repeated in the second stage with 64-bit values and accessing 𝑜0 and 𝑜2 as input pair pattern. This constructs a
new set output registers 𝑖0 and 𝑖1 which are holding the transpose’s result at 128-bit granularity. After that, stage
3 is shuffling at 128-bit granularity on register pairs which have a distance of “4" and creates output registers
that hold 256-bit of transposed data. Finally, in stage 4, these 256-bit transposed input registers are shuffled once
again creating the final set of 16 register holding the transposed 16 × 16 matrix. For non-square matrices we a)
just use masked loads or set registers to zero, b) transpose the zeros as well, and then c) don’t store all registers
or employ masked stores. This basic kernel is used as a basic building block to create large transpose operators
by simply adding outer loops.

This algorithm can be implemented by any SIMD ISA which offers support for picking arbitrary values from
a pair of SIMD registers to construct a result register containing values from the two sources, i.e. a general
shuffler. However, “structured" shuffle instructions are adequate as shown in Figure 6. Both x86 and aarch64 offer
instructions exactly implementing the needs for 32-bit and 64-bit interleaves as needed in the first two stages
covered in the previous description. In the case of 128-bit-wide SIMD registers this is enough to carry out the
entire transpose of 4 × 4 matrices as shown in Figure 6.

Finally, we want to note that broadcast loads, as supported by various ISAs, can be used to implement the first
stage of the shuffle network. This has the advantage that one stage of the shuffle network can be executed faster
and in parallel to the shuffler. The shuffle operations needed in all of these networks are relatively expensive
in hardware, therefore modern CPUs often only provide one execution unit for such operations (such “shuffle-
viruses" like transposes are pretty rare in general code). However, broadcasts on the load path are cheap and
can run in parallel to the shuffle unit, hence the overall performance of the transpose operation improves. This
microkernel variation leads to relatively complex code, and as such we skip its presentation. However our TPP
implementation backend employs all these microkernel variations.

3.3.2 Approximations for non-linear TPP Activation Functions

Activation functions are used to represent non-linear behavior of neural networks. Popular known activation
functions are sigmoid, tanh and GELU. These activation functions can be approximated to increase the efficiency
of deep learning networks without effecting it’s non-linear characteristics. In this section we will discuss different
approximation techniques based on Padé rational polynomials, piecewise minimax polynomials and Taylor
expansions, along with their TPP implementation on different ISAs. For simplicity we present the relevant
algorithms in terms of x86 and arm intrinsics (see glossary for the semantics of these intrinsics), however the
actual TPP implementation relies on JIT code generation.
Rational Padé polynomials

The Padé approximation of a function 𝑓 is the ratio of two polynomials with degrees p and q:

𝑃𝑎𝑑 ´𝑒 [𝑝/𝑞 ] 𝑓 (𝑥) =

(cid:205)𝑝
(cid:205)𝑞

𝑖=0

𝑖=0

𝑎𝑖𝑥𝑖
𝑏𝑖𝑥𝑖

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

15

Fig. 7. Rational Padé 7/8 tanh approximation pseudocode with equivalent intrinsics on x86 and Arm/AArch64. We highlight
here how the FMADD instruction on x86 ISAs has an equivalent instruction sequence on AArch64.

The coefficients 𝑎𝑖 and 𝑏𝑖 can be calculated by considering the first 𝑝 + 𝑞 derivatives of 𝑓 at zero and solving the
corresponding system of equations:

𝑓 (0) =𝑃𝑎𝑑 ´𝑒 [𝑝/𝑞 ] 𝑓 (0)
𝑓 ′(0) =𝑃𝑎𝑑 ´𝑒 ′
[𝑝/𝑞 ] 𝑓 (0)
...
𝑓 (𝑝+𝑞) (0) =𝑃𝑎𝑑 ´𝑒 (𝑝+𝑞)

[𝑝/𝑞 ] 𝑓 (0)

As an example we consider the approximation of the tanh function which has two asymptotes, hence approxi-
mating it with a Taylor expansion of lower degree polynomials may not yield good results. The implementation of
the 𝑃𝑎𝑑 ´𝑒 [7/8] (𝑥) tanh approximation is shown in Figure 7. FMA operations are used to compute the numerators
and denominators via HornerâĂŹs rule. The reciprocal of the denominator is multiplied by the numerator to
get the final result. The accuracy of reciprocal instruction is different among different CPU’s. This difference
in accuracy does not affect the non-linear region of the tanh function, keeping the TPP behavior same across
different CPUâĂŹs. The sigmoid activation function can be approximated via tanh by leveraging the following

1  __m512 x2 = _mm512_mul_ps(x, x);2  __mmask16 maskHi = _mm512_cmp_ps_mask(x, hi, _CMP_GT_OQ);3  __mmask16 maskLo = _mm512_cmp_ps_mask(x, lo, _CMP_LT_OQ );4__m512 t1_nom = x2;5t1_nom = _mm512_fmadd_ps(c2_n, c3_n, t1_nom);6  t1_nom = _mm512_fmadd_ps(x2, c1_n, t1_nom);7  t1_nom = _mm512_fmadd_ps(c0, x2, t1_nom);8  __m512 nom = _mm512_mul_ps(t1_nom, x);9  __m512 t1_denom = _mm512_add_ps(c3_d, x2);10 t1_denom = _mm512_fmadd_ps(c2_d, x2, t1_denom);11 t1_denom = _mm512_fmadd_ps(c1_d, x2, t1_denom);12 t1_denom = _mm512_fmadd_ps(c0, x2, t1_denom);13 __m512 denom_rcp = _mm512_rcp14_ps(denom);14 __m512 result = _mm512_mul_ps(nom,denom_rcp);15 result = _mm512_mask_blend_ps(maskHi, result, one);16 result = _mm512_mask_blend_ps(maskLo, result, neg_one);1 float32x4_t x2 = vmul_q(x, x);2 uint32x4_t  maskHi = vcgt_q(x, _CMP_GT_OQ);3 uint32x4_t  maskLo = vcgt_q(_CMP_LT_OQ, x);4 float32x4_t t1_nom = x2;5 float32x4_t tmp = c2_n;6 c2_n = vfmaq_f32(c3_n, t1_nom);7 t1_nom = c2_n;8 c2_n = tmp;9  tmp = c1_n;10 c1_n = vfmaq_f32(x2, t1_nom);11 t1_nom = c1_n;12 c1_n = tmp;13 tmp = c0;14 c0 = vfmaq_f32(x2, t1_nom);15 t1_nom = c0;16 c0 = tmp;17 float32x4_t nom = vmul_q(t1_nom, x);18 float32x4_t t1_denom = vadd_q(c3_d, x2);19 tmp = c2_d;20 c2_d = vfmaq_f32(x2, t1_denom);21 t1_denom = c2_d;22 c2_d = tmp;23 tmp = c1_d;24 c1_d = vfmaq_f32(x2, t1_denom);25 t1_denom = c1_d;26 c1_d = tmp;27 tmp = c0;28 c0 = vfmaq_f32(x2, t1_denom);29 t1_nom = c0;30 c0 = tmp;31 float32x4_t denom_rcp = vrecpe_f32(denom);32 float32x4_t result = vmul_q(nom,denom_rcp);33 result = vbit_insert(maskHi, one);34 result = vbit_insert(maskLo, neg_one);X86 CodeARM Code16

• E. Georganas et al.

Fig. 8. Tanh minimax approximation pseudocode with equivalent intrinsics on x86 and Arm/AArch64. We highlight here how
the _mm512_range_ps instruction on x86 ISAs has an equivalent instruction sequence on AArch64. Also the permutes on x86
have equivalent Table lookup instructions on AArch64.

identity:

𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑥) = (tanh(𝑥/2) + 1)/2

Piecewise minimax polynomial approximations

In this section we discuss the minimax polynomials approach [28] with the truncated Chebyshev series [29]
for approximations of activation functions. In this approach, the input range of a function 𝑓 (𝑥) is divided into
intervals and for each interval [𝑎, 𝑏] we find a polynomial 𝑝 of degree max 𝑛 to minimize:

|𝑓 (𝑥) − 𝑝 (𝑥)|

max
𝑎 ≤𝑥 ≤𝑏

We approximate tanh and GELU activation functions using this approach in our TPP implementation. The
input range is divided into 16 intervals and for each interval we investigate a polynomial 𝑝 of 3𝑟𝑑 degree (i.e.
we find appropriate 𝑝’s coefficients c0, c1, c2 based on the minimized absolute maximum difference of 𝑓 and 𝑝).
Figure 8 shows the x86 and arm implementation of evaluating such minimax polynomials. The register index (idx)
is calculated using the exponent and MSB of the respective input values, and represents the 16 intervals where
the input values are located. The range intrinsic _mm512_range_ps(A,B) is used to generate the register index
(idx) on AVX512 platforms (Figure 8-Left, line 2). In ARM, the range functionality is emulated with equivalent
and, shlq, min and max instructions as shown in Figure 8-Right, lines 2-4. To evaluate the 3𝑟𝑑 degree polynomial
we need to locate 3 coefficients (c0,c1,c2) based on the values at the register index (idx), which holds 16 entries.
We use 3 look up operations to find the three coefficients, each involving 16 FP32 entries. The 512-bit register
length in AVX512 is sufficient to hold 16 coefficients required for each look up, resulting in using 3 registers
for 3 look up operations (see Figure 8-Left, lines 4-6). Each ARM 128-bit wide vector register can only hold 4

1 __m512 signs = _mm512_and_ps(x, ps_sign_mask);2 __m512 abs_x = _mm512_range_ps(x, lut_low,0x2);/* 32 bit addressable idx */3 __m512 idx = _mm512_and_ps(abs_x, idx);4 __m512 c0 = _mm512_permutexvar_ps(idx, c0_reg);5 __m512 c1 = _mm512_permutexvar_ps(idx, c1_reg);6 __m512 c2 = _mm512_permutexvar_ps(idx, c2_reg);7 __m512 result = _mm512_fmadd_ps(abs_x, c2, c1);8 result = _mm512_fmadd_ps(result, abs_x, c0);9 result = _mm512_fmadd_ps(result, c2, c0);10 result = _mm512_fmadd_ps(abs_x, result, const_f);11 result = _mm512_xor_ps(result, signs);1 float32x4_t signs = vand_q(x, ps_sign_mask);2 float32x4_t abs_x = vand_q(x, ps_sign_filter);3 float32x4_t idx = vshlq_u32(x,22);4 idx = vmin_q(idx, lut_high);5 idx = vmax_q(idx, lut_low);/* Convert 32-bit addressable idx to byte addressable index*/7 float32x4_t c_32b_i = vld1q_f32(32b_b_Index);8 float32x4_t c_32b_o = vld1q_f32(32b_b_Offset);9 float32x4_t c0 = vtbl1_u8(c_32b_i, idx);10 idx = vadd_q(half, x);/* Done converting*/11 float32x4_t c0 = vtbl4_u8(c00_reg-c03_reg, idx);12 float32x4_t c1 = vtbl4_u8(c10_reg-c13_reg, idx);13 float32x4_t c2 = vtbl4_u8(c20_reg-c23_reg, idx);14 float32x4_t result = vfmaq_f32(c1, abs_x, c2);15 result = vfmaq_f32(c0, result, abs_x);16 result = vfmaq_f32(c0, result, c2);17 result = vfmaq_f32(abs_x, result, const_f);15 result = vfmaq_f32(c0, abs_x, result);16 result = vbcaxq_s32(result, signs);X86 CodeARM CodeTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

17

Fig. 9. Byte addressable Table look up setup in ARM/AArch64. We highlight the conversion of 32bit indexes to byte indexes
and the use of byte indexes to get the coefficients in 16 FP32 intervals.

Fig. 10. 32Bit Addressable Table look up setup on x86 AVX512 platforms.

FP32 entries, subsequently we are using 12 vector registers to hold the 16 entries for all 3 coefficients of the
polynomial. The in-register look-up table is performed using _mm512_permutexvar_ps(A,B) instructions in
x86 AVX512 as shown in Figure 10. In ARM we have byte addressable table look up instructions which are
analogous to 32-bit addressable permutes instructions in x86. Hence, we need to convert the 32-bit addressable
(0-16) register indexes to byte addressable (0-64 bytes) indexes. In order to do that, we use a constant register A
with a table look up instruction to duplicate the register index (idx) to each byte in the 32-bit entry. A constant
offset (0,1,2,3) is added to the duplicated byte index to get the byte addressable index for each FP32 entry in
16 FP32 entries (Figure 8-Right, lines 7-9). The table look up instruction in ARM provides the 64 byte look up
capability, which is sufficient enough to search into 4 registers holding the 16 entries of each coefficient; we
are using the generated byte indexes as shown in Figure 9. Finally, 4 FMA operations are used to evaluate the
polynomial using HornerâĂŹs rule. The FMA instruction in x86 provides the user the flexibility to decide among
the sources to destroy and the ones to preserve. ARM requires mov instructions to save intermediate results in
order to avoid the data overwriting during FMA operations.
Approximation with Taylor series

As an example of approximation with Taylor series we illustrate here the exp() activation function. The 𝑒𝑥 is
approximated using the identity 𝑒𝑥 = 2𝑥 log2
𝑒 − 𝑛. We
need to calculate 2𝑛 with 𝑛 being an integer and the term 2𝑦 with |𝑦| ∈ [0, 1). A Taylor polynomial of third degree
is used to calculate the term 2𝑦 with 3 FMA instructions (see Figure 11-Left, lines 4-6). Once 2𝑦 is calculated, we
leverage the instruction _mm512_scalef_ps(A,B) which returns a vector register holding 𝑎𝑖 · 2𝑓 𝑙𝑜𝑜𝑟 (𝑏𝑖 ) for

𝑒 = 2𝑛+𝑦 = 2𝑛 · 2𝑦 with 𝑛 = 𝑟𝑜𝑢𝑛𝑑 (𝑥 log2

𝑒) and 𝑦 = 𝑥 log2

1514131211109876543210cccc8888444400003210321032103210000f0004000b0002ffff4444bbbb22223c3c3c3c101010102c2c2c2c88883f3e3d3c131211102f2e2d2cba98Input RegisterAdd the offset to byte addressable to load the 32 b valueDuplicate 32b addressable indexes to byte addressable indexesTBL  lookup with Reg A as LUT and Reg B as indexReg B: 32bit addressable index generated from rangeSet offset Reg A: Set Index for LUT to convert 32b adressable index to byte adressable Convert 32bit addressable indexes to byte addressable15141312111098765432106362616014131211585756551211109Output Coefficient64byte LUT,4 Register hold 16 entries3f3e3d3c3b3a39383736353433323130-----------fedcba987654321063626160595857565554535251504948-----------1615141312111098765432118

• E. Georganas et al.

Fig. 11. Pseudocode for 𝑒𝑥

approximation with Taylor series on AVX512 x86 and ARM.

each 𝑎𝑖 ∈ 𝐴 and 𝑏𝑖 ∈ 𝐵. This scale instruction concludes the exp() approximation on x86 with AVX512. On ARM
we calculate 2𝑛 and 2𝑦 with equivalent replacement instructions as shown in Figure 11.

4 TPP MATRIX EQUATIONS
One of the main design principles of TPPs (as described in Section 2.1) is that they can be composed in a
producer-consumer fashion to form complex operations. For example consider the scenario where a user wants
to implement the composite operation 𝐶 = 𝑇 𝑎𝑛ℎ(𝐴 + 𝐵). One way to express this via TPPs would be to allocate
an intermediate tensor 𝑡𝑚𝑝 with same shape as 𝐴 and 𝐵, and perform first 𝑡𝑚𝑝 = 𝐴𝑑𝑑 (𝐴, 𝐵) via the binary Add
TPP. Then the user can compute the final result by leveraging the Tanh Unary TPP: 𝐶 = 𝑇 𝑎𝑛ℎ(𝑡𝑚𝑝). Even though
this approach is functionally correct, it requires the explicit management of intermediate tensors/buffers by the
user and also may result in low performance since there are redundant loads/stores to the 𝑡𝑚𝑝 tensor.

In order to increase the productivity, efficiency and expressiveness pertaining to composite operators, we
implemented an embedded Domain Specific Language (eDSL) in LIBXSMM [22]. Our Proof-Of-Concept imple-
mentations allows the user to express the desired composite operator as a Matrix Equation. More specifically, the
user can express the composite operator as an equation tree, where the head and internal nodes are the available
TPPs, whereas the leaves of the tree are the input 2D tensors of the composite operation. In the next subsections
we describe in detail the methodology we employ for JITing matrix equations of TPPs.

4.1 Definitions and notations for TPP Matrix Equations
A TPP matrix equation is represented as a tree with unary/binary/ternary TPP operations as internal nodes
and the equation’s input tensors are the leaves of the tree. The inputs of a TPP tree node are essentially its
children in the equation tree. The output of an internal TPP node can be represented as a temporary intermediate
tensor which in turn can be fed as input to the parent TPP node in the tree. Depending on the TPP node type
(unary/binary/ternary), each internal node requires a number of inputs (one/two/three) to be computed/ready
before performing the corresponding TPP operation. Let’s consider for example the TPP equation tree in Figure 12-
Left that is used to express the following operator:

𝑂𝑢𝑡 = 𝑇 𝑎𝑛ℎ(𝑇0) + (𝑇1 × 𝑇2)/(𝑇3 − 𝑇4)

(1)

1 __m512 y = _mm512_roundscale_ps(y,4);2 y = _mm512_sub_ps(y,x);3 __m512 z = x;4 z = _mm512_fmadd_ps(c2, z, c3);5 z = _mm512_fmadd_ps(y, z, c1);6 z = _mm512_fmadd_ps(y, z, c0);5  __m512 exp = _mm512_scalef_ps(z, x);1  x = vmax_q(x, lo);2  x = vmin_q(x, hi);3  x = vmul_q(log2e, x);4  x = vadd_q(half, x);5  y = vrndmq_f32(x);6  y = vsub_q(y, x);7  float32x4_t z = y;8  float32x4_t x1 = x;9  z = vmul_q(c3, z);11 z = vadd_q(z, c2);12 z = vmul_q(y, z);13 z = vadd_q(z, c1);14 z = vmul_q(y, z);15 z = vadd_q(z, c0);16 x = vrndmq_f32(x);17 x = vcvtmq_s32_f32(x);18 x = vadd_q(x, exp_mask);19 x = vshlq_u32(x,23);20 x = vmul_q(x1, z);X86 CodeARM CodeTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

19

Fig. 12. Left: TPP Equation tree for 𝑂𝑢𝑡 = 𝑇 𝑎𝑛ℎ(𝑇0) + (𝑇1 × 𝑇2)/(𝑇3 − 𝑇4). Right: Assigned register scores 𝑣 on the equation
TPP nodes after running Algorithm 3.

We will illustrate with this example how our eDSL for TPP Matrix Equations works.

4.2 Optimized Execution plan for TPP Matrix Equations
The equation tree in Figure 12-Left can be naively evaluated by assigning to each intermediate node a temporary
tensor to hold the corresponding TPP output, and performing e.g. 1) the Tanh operation, 2) the Matrix Multiplica-
tion, 3) the Subtract operation, 4) the Div operation , and finally 5) the Add TPP. In such an evaluation schedule
we would need 4 intermediate tensors to hold the corresponding intermediate results. In this subsection we
illustrate how we can construct optimized execution plans for TPP Matrix Equations that minimize the number
of intermediate tensors.

For each TPP node 𝑟 we can assign a register score value 𝑣𝑟 that essentially dictates how many temporary/in-
termediate tensors are required to calculate the subtree in the equation where node 𝑟 is root. We extend the
methodology of [30] and we generate the register score values using the recursive Algorithm 3. This algorithm
calculates recursively the register scores of the children for a given node 𝑟 , and in this way we know how many
temporary tensors are required for the evaluation for each child. Now, if all of its children have the same register
score, the node 𝑟 get an increased register score value, otherwise the node gets as register score the maximum of
its children’s register score values. Intuitively this means that we can first evaluate a child 𝑐 and its subtree with
whatever intermediate tensor requirements it has, e.g. 𝑣𝑐 temporary tensors, and eventually we need only one
temporary tensor to hold 𝑐’s output. We can do the same afterwards for all other siblings of 𝑐, however we can
reuse/recycle the rest 𝑣𝑐 − 1 temporary tensors that were required by 𝑐 since 𝑐 and its subtree have been already
computed.

This algorithm optimizes the number of temporary tensors/storage that are required for the equation evaluation,
and it reuses the temporary storage as much as possible. For instance, for the equation in Figure 12-Left, after
executing Algorithm 3 on the TPP equation tree, we see that the root’s register score value is 2 (see Figure 12-
Right), meaning that only 2 intermediate tensors are required to evaluate the entire TPP tree rather than naively
assigning one temporary tensor to each internal TPP node which would result in 4 intermediate tensors.

AddTanhT0DivMatMulT1T2SubT3T4OutAddTanhT0DivMatMulT1T2SubT3T4v= 1Outv= 1v= 2v= 1v= 2Assign scores v20

• E. Georganas et al.

𝑣𝑟 ← 0

Algorithm 3 Assign_Register_Score( 𝑟 )
Input: TPP equation tree with root node 𝑟
Output: TPP equation tree with assigned register score values on its nodes
1: if is_Leaf( 𝑟 ) then
2:
3: if 𝑟 is unary TPP then
4:
5:
6:
7:

Assign_Register_Score( Left_Child( 𝑟 ) )
⊲ If child is leaf, then we assign current register score of 1, else we assign the child’s register score
if is_Leaf( Left_Child( 𝑟 ) ) then

𝑣𝑟 ← 1

𝑣𝑟 ← Register_Score(Left_Child( 𝑟 ))

else

8:
9:
10: if 𝑟 is binary TPP then
11:
12:

Assign_Register_Score( Left_Child( 𝑟 ) )
Assign_Register_Score( Right_Child( 𝑟 ) )
⊲ If the register scores of children are equal, then we get the children’s register score increased by one, otherwise

we get the max value of the children’s register score

if Register_Score(Left_Child( 𝑟 )) equals Register_Score(Right_Child( 𝑟 )) then

𝑣𝑟 ← Register_Score(Left_Child( 𝑟 )) + 1

Assign_Register_Score( Left_Child( 𝑟 ) )
Assign_Register_Score( Middle_Child( 𝑟 ) )
Assign_Register_Score( Right_Child( 𝑟 ) )
⊲ If all children are leaves, then we assign current register score of 1, otherwise we get the max value of the

if is_Leaf( Left_Child( 𝑟 ) ) AND is_Leaf( Middle_Child( 𝑟 ) ) AND is_Leaf( Right_Child( 𝑟 ) ) then

13:

14:
15:
16:
17:

25:
26:

27:
28:
29:
30:
31:

else

𝑣𝐿 ← Register_Score(Left_Child( 𝑟 ))
𝑣𝑅 ← Register_Score(Right_Child( 𝑟 ))
𝑣𝑟 ← MAX(𝑣𝐿 , 𝑣𝑅)

18:
19:
20: if 𝑟 is ternary TPP then
21:
22:
23:
24:

children’s register score

𝑣𝑟 ← 1

else

𝑣𝐿 ← Register_Score(Left_Child( 𝑟 ))
𝑣𝑀 ← Register_Score(Middle_Child( 𝑟 ))
𝑣𝑅 ← Register_Score(Right_Child( 𝑟 ))
𝑣𝑟 ← MAX(3, 𝑣𝐿, 𝑣𝑀 , 𝑣𝑅)

Now that we have assigned the register scores for each node we can devise an execution plan for the TPP
equation tree that minimizes the number of required intermediate tensors. Algorithm 4 recursively creates such
an optimal execution plan and essentially it calculates: 1) the order/traversal timestamps 𝑡 with which the TPP
equation nodes have to be evaluated, and also 2) assigns to each intermediate node 𝑟 a temporary tensor id 𝑡𝑚𝑝𝑟
that holds the intermediate resulting tensor of that TPP node. Figure 13-Right shows the optimized execution
plan by applying Algorithm 4 on our example equation. This algorithm recursively visits/evaluates the children
of a node 𝑟 in order of decreasing register score value. This means that the child/subtree with the maximum
register score value is evaluated first, one of the temporary tensors is dedicated to hold that child’s intermediate

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

21

Fig. 13. Left: TPP equation tree with assigned register scores 𝑣 on the nodes. Right: TPP equation tree with assigned traversal
timestamps 𝑡 and temporary tensor ids 𝑡𝑚𝑝 after executing Algorithm 4.

output, whereas the remaining temporary tensors can be reused for the evaluation of the siblings/subtrees, which
per definition/order of traversal, require less or equal number of intermediate tensors. Such a strategy guarantees
that the temporary tensors are optimally reused/recycled, and as a result we can leverage the minimum required
temporary tensors for the evaluation of the entire equation TPP tree. For simplicity in our description, we assumed
that all intermediate temporary tensors have the same size, however our implementation considers the actual
sizes of the intermediate output tensors and takes the maximum one as representative size for all temporary
tensors.

Implementation of Optimized Execution plan for TPP Matrix Equations

4.3
By employing Algorithm 4 we can devise an optimal execution plan for the TPP Matrix equation, and here we
describe the implementation of such a plan. We consider three implementation strategies:

• Strategy 1: Using stack-allocated buffers as intermediate temporary tensors
• Strategy 2: Using vector-register blocks as intermediate temporary tensors
• Strategy 3: Hybrid implementation where some intermediate temporary tensors are stack-allocated buffers

and some are vector-register blocks

So far in our description we have used the abstract notation “temporary tensor" without specifying how such a
temporary tensor is instantiated in the implementation. The exact instantiation of a temporary/intermediate
tensor is the differentiation factor among the 3 implementation strategies for the TPP matrix equations.

Strategy 1 considers each intermediate tensor as a physical buffer, and our TPP equation implementation
allocates on the stack some space/buffer for each temporary tensor. Then, by following the timestamp order of the
optimal execution plan (e.g. see Figure 13-Right), we emit/JIT the corresponding TPP code (e.g. see Algorithms 1
and 2) where the input tensors might be either the equation’s input buffers provided by the user, or one of the
stack allocated buffers representing an intermediate result. The fact that we have minimized the number of
intermediate temporary buffers/tensors is critical for performance since these stack-allocated buffers may remain
in some level of cache. Such a strategy is generic and can be leveraged to implement arbitrary equations. However,
Strategy 1 may suffer from store-to-load forwarding inefficiencies on modern processors. Additionally, some of

AddTanhtmp1T0Divtmp0MatMultmp0T1T2Subtmp1T3T4OutAddTanhT0DivMatMulT1T2SubT3T4v= 1Outv= 1v= 2v= 1v= 2Assign traversal timestampsand tmptensor ids⏱ t=1⏱ t=2⏱ t=3⏱ t=4⏱ t=522

• E. Georganas et al.

the intermediate tensors may spill from cache (e.g. when the intermediate outputs exceed the corresponding
cache capacity) which would make the communication of temporary tensors among TPP nodes via loads/stores
from/to stack allocated buffers quite expensive.

Strategy 2 considers each intermediate tensor as an 𝑟𝑚 × 𝑟𝑛 vector-register block. For example, on an AVX512
platform with 32 512-bit wide registers we have available 2 KBytes of register file that may be used for intermediate
tensors. Each one of such 512-bit wide vector registers can hold 16 single-precision values and by stacking e.g.
4 of these we can form a logical 16×4 intermediate tensor and in total we have available 32/4 = 8 of such
intermediate tensors that could be used by the equation. In Strategy 2 we block the computation of the equation’s
output in blocks with size 𝑟𝑚 × 𝑟𝑛, and we can calculate the corresponding 𝑟𝑚 × 𝑟𝑛 output by following the
timestamp order of the optimal execution plan. We emit/JIT the corresponding TPP code for sub-tensors with
size 𝑟𝑚 × 𝑟𝑛 where each intermediate output tensor is the assigned temporary vector-register block. Essentially
this strategy performs vertical register fusion within the equation TPP nodes and incurs no communication via
loads/stores from/to stack allocated buffers. However, such a methodology is limited by the number of available
vector registers on each platform.

Strategy 3 combines the strengths of Strategies 1 and 2 by considering some intermediate tensors as stack-
allocated buffers and some intermediate tensors as vector-register blocks. As such, in Strategy 3 the TPP
operations/subtrees which exhibit both high register pressure and reuse (e.g. transposes, GEMM/BRGEMM,
transcendental approximations), propagate the intermediate results towards the rest of the TPPs in the tree via
stack-allocated temporal tensors. On the other hand, TPP subtrees without large register pressure are implemented
using Strategy 2 that employs vertical register fusion and avoids loads/stores from/to stack-allocated buffers.

In addition to the aforementioned 3 strategies, in the TPP equation back-end we identify idioms/motifs of
combined TPPs (e.g. a gather TPP followed by a reduce TPP) and we JIT an instruction sequence which is optimal
for the composite access pattern. In subsection 5.1.5 we show an example of such a combined TPP motif that is
optimized by the TPP backend.

Even though we developed a rudimentary method/POC of combining the TPPs via Matrix Equation Trees, we
have found that it is sufficient to express all the complex operators we encountered in a wide-range of workloads
discussed further in Section 5. Nevertheless, we envision that when/if TPPs are widely adopted within Tensor
Compiler frameworks (e.g. as an MLIR dialect) then more complicated Graphs (instead of simple trees) and more
sophisticated analyses/optimization passes can be leveraged during the composition of TPPs. The key-ingredient
that makes the composition of TPPs amenable to optimization opportunities is the TPP specification itself: TPPs
comprise a small, well-defined compact set of tensor operators with declarative semantics as shown in Section 2.
We would like also to highlight one use-case of Matrix Equations that can be beneficial for specialized DL
accelerators. The BRGEMM TPP described in Section 3.2 corresponds to an output-stationary flow that is suitable
for CPUs and GPUs. Given an accelerator that favors e.g. 𝐴-stationary GEMM formulations, one could express
the following Matrix Equation: internal nodes 𝐺𝑖 would be GEMM ternary TPPs, for each GEMM node 𝐺𝑖 we
would have the same input leaf 𝐴 and a varying input 𝐵𝑖 , and the output of each node would be a result 𝐶𝑖 .
Essentially this formulation dictates an 𝐴-stationary flow, and the back-end could optimize accordingly for the
specific accelerator.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

23

Algorithm 4 Create_Execution_Plan( 𝑟 )
Input: TPP equation tree with root node 𝑟 and assigned register score values on its nodes
Output: TPP equation tree with assigned traversal timestamps 𝑡 and temporary tensor ids 𝑡𝑚𝑝
1: if is_Leaf( 𝑟 ) then
2:
3: if 𝑟 is unary TPP then
4:
5:
6:

Create_Execution_Plan( Left_Child( 𝑟 ) )
𝑡𝑟 ← global_timesteamp++
⊲ If child is leaf, reserve a new tmp, else re-use tmp from child
if is_Leaf( Left_Child( 𝑟 ) ) then

return

⊲ Recursively visit children in order of decreasing register score
Create_Execution_Plan( Child_Max_Register_Score( 𝑟 ) )
Create_Execution_Plan( Child_Min_Register_Score( 𝑟 ) )
𝑡𝑟 ← global_timesteamp++
⊲ If all children are leaves, reserve a new tmp, else re-use the tmp from a non-leaf child and recycle the tmp of

if is_Leaf( Left_Child( 𝑟 ) AND is_Leaf( Right_Child( 𝑟 ) ) ) then

𝑡𝑚𝑝𝑟 ← tmp_Left_Child( 𝑟 )

𝑡𝑚𝑝𝑟 ← Reserve_Tmp()

else

7:
8:
9:
10:
11: if 𝑟 is binary TPP then
12:
13:
14:
15:
16:

the other non-leaf child

𝑡𝑚𝑝𝑟 ← Reserve_Tmp()

else

if not_Leaf( Left_Child( 𝑟 ) then
𝑡𝑚𝑝𝑟 ← tmp_Left_Child( 𝑟 )
Recycle_Tmp( tmp_Right_Child( 𝑟 ) )

𝑡𝑚𝑝𝑟 ← tmp_Right_Child( 𝑟 )
Recycle_Tmp( tmp_Left_Child( 𝑟 ) )

else

23:
24:
25:
26: if 𝑟 is ternary TPP then
27:

⊲ Recursively visit children in order of decreasing register score
Create_Execution_Plan( Child_Max_Register_Score( 𝑟 ) )
Create_Execution_Plan( Child_Mid_Register_Score( 𝑟 ) )
Create_Execution_Plan( Child_Min_Register_Score( 𝑟 ) )
𝑡𝑟 ← global_timesteamp++
⊲ If all children are leaves, reserve a new tmp, else re-use the tmp from a non-leaf child and recycle the tmps of

the other non-leaf children

𝑡𝑚𝑝𝑟 ← Reserve_Tmp()

if is_Leaf( Left_Child( 𝑟 ) ) AND is_Leaf( Middle_Child( 𝑟 ) ) AND is_Leaf( Right_Child( 𝑟 ) ) then

else

if not_Leaf( Left_Child( 𝑟 ) then
𝑡𝑚𝑝𝑟 ← tmp_Left_Child( 𝑟 )
Recycle_Tmp( tmp_Middle_Child( 𝑟 ) ) , Recycle_Tmp( tmp_Right_Child( 𝑟 ) )

else

if not_Leaf( Right_Child( 𝑟 ) then
𝑡𝑚𝑝𝑟 ← tmp_Right_Child( 𝑟 )
Recycle_Tmp( tmp_Middle_Child( 𝑟 ) ) , Recycle_Tmp( tmp_Left_Child( 𝑟 ) )

else

𝑡𝑚𝑝𝑟 ← tmp_Middle_Child( 𝑟 )
Recycle_Tmp( tmp_Left_Child( 𝑟 ) ) , Recycle_Tmp( tmp_Right_Child( 𝑟 ) )

17:
18:
19:
20:
21:
22:

28:
29:
30:
31:
32:

33:
34:
35:
36:
37:
38:

39:
40:
41:
42:
43:
44:

45:

24

• E. Georganas et al.

Fig. 14. Softmax operator by combining TPPs.

5 TPP-BASED KERNELS & WORKLOADS
This section covers how DL kernels and workloads (image processing, recommendation systems, natural language
processing, graph processing and applications in science) can leverage TPPs to achieve high performance.
Although this paper’s work is targeting CPUs, we cover the entire training pipeline and not only inference.
The main purpose of this is to demonstrate the versatility of TPPs which is valuable in the more complicated
backward pass kernels, and to handle training’s implications to the forward pass.

5.1 TPP-based Kernels
Softmax Kernel
5.1.1

Figure 14 illustrates two Matrix Equation trees that are used to express the softmax operator [31]:

𝑌 = softmax(𝑋 ) with 𝑦𝑖 𝑗 =

(cid:16)
𝑥𝑖 𝑗 −max𝑥𝑖 𝑗 ∈𝑋 𝑥𝑖 𝑗

(cid:17)

𝑒

(cid:16)
𝑥𝑖 𝑗 −max𝑥𝑖 𝑗 ∈𝑋 𝑥𝑖 𝑗

(cid:17)

𝑒

(cid:205)
𝑥𝑖 𝑗 ∈𝑋

(2)

Equation 2 shows the formula for the softmax operator [31], which is often used as the last activation function
of a neural network, aiming to normalize its output to a probability distribution. We can represent this operator
via two TPP equation trees illustrated in Figure 14. The left tree computes the nominator of Equation 2: first
the maximum value of the input tensor 𝑋 is found (via the max-reduce TPP), then we subtract this max value
from each entry of 𝑋 (note the broadcast semantics in the second argument of the subtraction TPP), and a
new tensor 𝑋 ′ is computed by calculating the element-wise exponent on the earlier subtraction’s outcome.
Finally, in the right TPP tree each entry of the tensor 𝑋 ′ is normalized by the sum of all values in 𝑋 ′ to obtain
the softmax output, a tensor 𝑌 . This example illustrates the expressiveness of the TPP abstractions, since the
components of the mathematical formula map to TPPs in a straightforward way. At the same time, this example
highlights the separation of concerns: the user does not need to worry about the efficient implementation of this
equation on each different platform, since the TPP back-end is responsible for optimized code generation which
is target-specific (contrary to the TPP expression itself which is platform-agnostic).

5.1.2 Normalization Kernels

Batch normalization (batchnorm) is a technique [32] that normalizes neuron layer input tensors to improve
the overall training process. Batchnorm removes the need for careful parameter initialization and reduces the
required training steps [32] in the neural networks. The batchnorm computations can be divided in two stages:
i) First the mean and variance of the input tensor are computed across the “batch" dimension: 𝜇 𝑗 = (cid:205)𝑛−1
𝑥𝑖 𝑗 ,
𝑖=0

XXMAXReduceSUBBcastArg1EXPX’X’SUMReduceReci-procalMULBcastArg0Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

25

Fig. 15. Layernorm via TPPs.

𝑖=0 (𝑥𝑖 𝑗 − 𝜇𝑖 )2 where 𝑖 is the “batch" dimension and 𝑗 is the “feature" dimension, ii) then the tensor

(cid:205)𝑛−1

𝜎 2
𝑗 = 1
𝑛
entries 𝑥𝑖 𝑗 are normalized based on 𝜇 and 𝜎: 𝑥 ′

𝑖 𝑗 = (𝑥𝑖 𝑗 − 𝜇 𝑗 )/(

√︃𝜎 2

𝑗 + 𝜖).

Depending upon the workload, different TPPs and TPP equations can be employed to implement the batchnorm.
Here, we take an example of batchnorm on a ResNet50 [33] convolution layer tensor 𝑋 . The input tensor 𝑋 has a
four-dimensional shape of {N, C, H, W} with dimensions of batch (𝑁 ), feature (𝐶), height (𝐻 ), and width (𝑊 ). We
first use sum-reduce TPPs on 𝐻 and 𝑊 dimensions to compute the sum (𝑚[𝑁 , 𝐶]) and the sum of squared elements
(𝑣 [𝑁 , 𝐶]) matrices. Subsequently, we use binary add TPPs across the batch dimension of 𝑚[𝑁 , 𝐶] and 𝑣 [𝑁 , 𝐶]
matrices for eventual computation of mean (𝜇 [𝐶]) and variance (𝜎 2 [𝐶]) vectors. In the next step, we use a scaling
equation to normalize each element of the input tensor. The scaling equation 𝑌 = (𝑚′ ∗𝑋 +𝑣 ′) ∗𝐺 + 𝐵 converts the
input tensor 𝑋 into a normalized tensor 𝑌 . Here, 𝐺 [𝐶] and 𝐵 [𝐶] are scaling vector inputs to batchnorm, and 𝑚′[𝐶]
and 𝑣 ′[𝐶] are intermediate vectors that are computed from mean and variance vectors. We implement the scaling
equation by a single TPP equation containing two FMADD ternary TPPs. The second equation tree of Figure 15
shows an analogous scaling equation implementation. However, for this particular implementation, we broadcast
𝑚′, 𝑣 ′, 𝐺, 𝐵 vectors into 𝐻 , 𝑊 , and 𝑁 dimensions inside the TPP equation tree. An efficient implementation of
batchnorm uses blocking on the 𝐶, 𝐻 , and 𝑊 dimensions along with multi-threading on the 𝑁 and feature block
dimension. We do not show the details of this implementation for sake of simplicity.

𝑖 = 1
𝑚

Layer normalization (layernorm) [34] is a technique that normalizes the neurons within a layer, and
was motivated by the limitations of Batch Normalization [32] in Recurrent Neural Networks. The layernorm
computations can be divided in two stages: i) First the mean and variance of the input tensor are computed
across the “feature" dimension: 𝜇𝑖 = (cid:205)𝑚−1
(cid:205)𝑚−1
𝑗=0 (𝑥𝑖 𝑗 − 𝜇𝑖 )2 where 𝑖 is the batch dimension and 𝑗 is the
𝑥𝑖 𝑗 , 𝜎 2
𝑗=0
𝑖 + 𝜖).
“feature" dimension, ii) then the tensor entries 𝑥𝑖 𝑗 are normalized based on 𝜇 and 𝜎: 𝑥 ′
𝜎 2
Depending on the workload (e.g. attention cell in BERT), the scaled tensor may be further scaled with two other
tensors 𝛾 and 𝛽. Figure 15 illustrates two TPP equation trees that implement this composite layernorm operator.
The left equation is using the sum-reduce TPP to compute the sum and sum of squared elements of the input
tensor, namely 𝑚 and 𝑣. These two scalars are combined (not shown in the equation for simplicity), and are
fed as inputs to the right TPP tree, where the FMADD ternary TPP is used to scale the input tensor 𝑋 . Finally,
a cascading FMADD ternary TPP computes the final result via the scaling tensors 𝐺 and 𝐵. We illustrate this
layernorm via means of TPPs since all DL norming layers essentially exhibit similar computational motif, and
this specific norm is used in the BERT workload described in subsection 5.2.3.

𝑖 𝑗 = (𝑥𝑖 𝑗 − 𝜇𝑖 )/(

√︃

Group normalization (groupnorm) [35] is a technique that normalizes the neurons within a group of
features. Groupnorm was proposed as an alternative to batchnorm [32] to reduce normalization error for smaller

XSUMReduceXi ,Xi2mvXv'm’FMADDBcastArg1,2GBFMADD26

• E. Georganas et al.

Fig. 16. BF16 Split-SGD operator by combining TPPs.

batch sizes. In groupnorm, features are divided into groups, and mean and variance are computed within
each group for normalization. Groupnorm is also a generalization of the layer normalization [34] and instance
normalization [36] approach. Layernorm is groupnorm with a single group, and instance norm is groupnorm
with group size equal to one. Groupnorm can be implemented with the same set of TPPs and TPP equations that
were used in the batchnorm kernel. We again take the example of ResNet50 [33] convolution layer tensor 𝑋 and
apply groupnorm on it with 𝑔 number of groups. We can ignore the batch dimension (𝑁 ) for this discussion as
groupnorm works independently upon each batch. Therefore, the input tensor 𝑋 now has a three-dimensional
shape of {C, H, W} with dimensions of feature (𝐶), height (𝐻 ), and width (𝑊 ). We first use sum-reduce TPPs on 𝐻
and 𝑊 dimensions to compute the sum (𝑚[𝐶]) and the sum of squared elements (𝑣 [𝐶]) vectors. Subsequently,
we add 𝑚[𝐶] and 𝑣 [𝐶] values within a feature group for eventual computation of group mean (𝜇 [𝑔]) and group
variance (𝜎 2 [𝑔]) vectors. Similar to batchnorm, we use a scaling equation to normalize each element of the input
tensor. The scaling equation 𝑌 = (𝑚′ ∗ 𝑋 + 𝑣 ′) ∗ 𝐺 + 𝐵 converts input tensor 𝑋 into a normalized tensor 𝑌 .
Here, 𝐺 [𝐶] and 𝐵 [𝐶] are scaling vector inputs to groupnorm, and 𝑚′[𝐶] and 𝑣 ′[𝐶] are intermediate vectors that
are computed from group mean and group variance vectors. The second equation tree of Figure 15 shows an
analogous scaling equation implementation. However, for this particular implementation, we broadcast 𝑚′, 𝑣 ′, 𝐺, 𝐵
vectors into 𝐻 and 𝑊 dimensions inside the TPP equation tree. We can also apply the same scaling equation to
a single group or set of groups with few parameter changes. An efficient implementation of groupnorm uses
blocking on the 𝐶, 𝐻 , and 𝑊 dimensions. We do not show the details of this implementation for sake of simplicity.

5.1.3 BF16 Split-SGD Kernel

Unlike the previous kernels which are well-established in DL workloads, and as such potentially optimized
in DL libraries, we present here an example of a novel operator, which per definition is not existent in DL
libraries. BF16 split-SGD was recently introduced in the context of DLRM training with BF16 datatype [37]. The
Split-SGD-BF16 solver aims at efficiently exploiting the aliasing of BF16 and FP32 (i.e. the 16 Most Significant Bits
(MSB) on both are identical) in order to save bandwidth during the SGD-solver in training. The employed trick is
that the weights are not stored as FP32 values in a single tensor. Instead, the FP32 tensors are split into their
high and low 16bit-wide parts: the 16 MSBs of the FP32 values, and the 16 LSBs of the same values are stored as
two separate tensors 𝑋 ℎ𝑖 and 𝑋 𝑙𝑜 respectively. The 16 MSBs represent a valid BF16 number and constitute the
model/weight tensors during training. These BF16 weights are used exclusively in the forward and backward
passes, whereas the lower 16 bits are only required in optimizer. More specifically, the 𝑋 ℎ𝑖 and 𝑋 𝑙𝑜 tensors are
packed together to form an FP32 tensor, resulting in a fully FP32-accurate optimizer. Figure 16 illustrates the BF16

Grad.WeightLearn.RateFMADDBcastArg1XloXhiPACKUNPACKTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

27

, . . . , 𝑎𝑝𝑘 , . . . , 0] with entries 𝑎𝑝 = 1 for 𝑝 ∈ {𝑝1, . . . , 𝑝𝑘 } and 0 elsewhere, 𝑊 𝑀×𝐸

Algorithm 5 Sparse Gather-Reduce operation
Inputs: 𝛼𝑇 = [0, . . . , 𝑎𝑝1
Output: 𝑜𝑇 = 𝑎𝑇 × 𝑊
1: for 𝑗 = 0 . . . 𝐸 with step 𝑣𝑙𝑒𝑛 · 𝑈 do
2:

⊲ Initializing accumulator registers to 0
for 𝑢 = 0 . . . 𝑈 − 1 do
𝑣𝑒𝑐_𝑜𝑢𝑡𝑢 ← 0

3:
4:

5:
6:
7:

8:
9:

10:
11:
12:

13:

14:
15:
16:

⊲ Iterating over non-zero entries/indices in 𝛼𝑇
for 𝑖 in 1, 2, . . . , 𝑘 do

𝑖𝑑𝑥 = 𝑝𝑖
𝑛𝑒𝑥𝑡_𝑖𝑑𝑥 = 𝑝𝑖+1
⊲ Unroll innermost kernel 𝑈 times: load indexed vector, prefetch next indexed vector, accumulate loaded

vector to accumulator register
for 𝑢 = 0 . . . 𝑈 − 1 do

𝑣𝑒𝑐_𝑊 ← load_vector(𝑊 [𝑖𝑑𝑥] [ 𝑗 + 𝑢 · 𝑣𝑙𝑒𝑛 : 𝑗 + (𝑢 + 1) · 𝑣𝑙𝑒𝑛])
prefetch(𝑊 [𝑛𝑒𝑥𝑡_𝑖𝑑𝑥] [ 𝑗 + 𝑢 · 𝑣𝑙𝑒𝑛 : 𝑗 + (𝑢 + 1) · 𝑣𝑙𝑒𝑛])
𝑣𝑒𝑐_𝑜𝑢𝑡𝑢 += 𝑣𝑒𝑐_𝑊

⊲ Store accumulator registers to 𝑜𝑇
for 𝑢 = 0 . . . 𝑈 − 1 do

𝑜𝑇 [ 𝑗 + 𝑢 · 𝑣𝑙𝑒𝑛 : 𝑗 + (𝑢 + 1) · 𝑣𝑙𝑒𝑛] ← 𝑣𝑒𝑐_𝑜𝑢𝑡𝑢

Fig. 17. Sparse Embedding Lookups via TPPs

Split-SGD operator written entirely via TPPs. First the 𝑋 ℎ𝑖 and 𝑋 𝑙𝑜 are packed, and the formed FP32 tensor is
used in a cascading FMADD TPP that performs the SGD scaling with the corresponding Gradient Weight tensor
and learning rate. Finally, the resulting FP32 tensor is unpacked to the 𝑋 ℎ𝑖 and 𝑋 𝑙𝑜 tensors for further use in the
training process.

5.1.4 Convolutional Neural Network (CNN) kernel

Convolutional Neural Networks (CNN) consist of layers with multiple neurons connected by weights, and
they have been applied with success in image recognition, semantic segmentation, autonomous driving, medical
imaging and in an increasing number of scientific applications. Previous work [21, 23] has shown that CNNs,
despite their seemingly complicated loop structure due to the involved high-dimensional tensors, can be mapped
efficiently onto small 2D GEMMs and BRGEMMs. In this work, we adopt the same strategy to implement CNNs
via the BRGEMM TPP. Unlike the previous work which presents only the address-based BRGEMM formulation,
here we leverage the CNN kernels with stride-based BRGEMM for 1×1 convolutions and offset-based BRGEMM
for 3×3 convolutions to get even more performant implementations (see Section 2.3 for a brief description of the
BRGEMM variants).

GatherRows1. Emb. Table W2. IndicesSUMReduceRows28

• E. Georganas et al.

Algorithm 6 Fully-Connected Layer with Unary Activation Function
Inputs: 𝐴𝑀𝑏 ×𝐾𝑏 ×𝑏𝑘 ×𝑏𝑚 , 𝐵𝑁𝑏 ×𝐾𝑏 ×𝑏𝑛×𝑏𝑘
Output: 𝐶 𝑁𝑏 ×𝑀𝑏 ×𝑏𝑛×𝑏𝑚
1: Based on 𝑡ℎ𝑟𝑒𝑎𝑑_𝑖𝑑 calculate 𝑀𝑏_𝑠𝑡𝑎𝑟𝑡, 𝑀𝑏_𝑒𝑛𝑑, 𝑁𝑏_𝑠𝑡𝑎𝑟𝑡 and 𝑁𝑏_𝑒𝑛𝑑 to assign output work items
2: for 𝑖𝑏𝑛 = 𝑁𝑏_𝑠𝑡𝑎𝑟𝑡 . . . 𝑁𝑏_𝑒𝑛𝑑 do
3:
4:
5:
6:

for 𝑖𝑏𝑚 = 𝑀𝑏_𝑠𝑡𝑎𝑟𝑡 . . . 𝑀𝑏_𝑒𝑛𝑑 do
𝑂𝑢𝑡 = &𝐶 [𝑖𝑏𝑛] [𝑖𝑏𝑚] [0] [0]
⊲ Stride-based BRGEMM, stride_A=𝑏𝑘 · 𝑏𝑚, stride_B=𝑏𝑛 · 𝑏𝑘
BRGEMM(&𝐴[𝑖𝑏𝑚] [0] [0] [0], &𝐵 [𝑖𝑏𝑛] [0] [0] [0], 𝑂𝑢𝑡, 𝐾𝑏)
𝐶 [𝑖𝑏𝑛] [𝑖𝑏𝑚] [0] [0] ← UNARY(𝐶 [𝑖𝑏𝑛] [𝑖𝑏𝑚] [0] [0])

7:

5.1.5

Sparse Embedding Kernel

The sparse embedding kernel is comprised of multi-hot encoded lookups into an embedding table 𝑊 𝑀×𝐸 with
𝑀 being the number of rows and 𝐸 the length of each row, whereas the multi-hot weight-vector is denoted as
𝛼𝑇 = [0, . . . , 𝑎𝑝1
, . . . , 𝑎𝑝𝑘 , . . . , 0] with entries 𝑎𝑝 = 1 for 𝑝 ∈ {𝑝1, . . . , 𝑝𝑘 } and 0 elsewhere (𝑝 being the index for
the corresponding lookup items). Mathematically, the embedding lookup output vector 𝑜𝑇 can be obtained via
𝑜𝑇 = 𝑎𝑇 × 𝑊 . This operation (assuming row-major storage for 𝑊 ) is equivalent to gathering the rows of 𝑊 based
on the non-zero indices 𝑎𝑝 , and then adding them up to get the output vector 𝑜𝑇 . Figure 17 illustrates the TPP
tree that is used to express the Sparse Embedding lookup kernel.

We note that the TPP backend optimizes this sequence of TPPs, and performs register fusion across the gather
and the reduce TPP components. More specifically, given a non-zero index 𝑎𝑝 , the corresponding row of 𝑊 is
loaded in vector registers, and is added to a set of running accumulators/vector registers that hold the output 𝑜𝑇 .
Algorithm 5 illustrates the optimized JITed implementation in our TPP backend. The 𝐸 dimension is vectorized in
an SIMD-fashion with vector length 𝑣𝑙𝑒𝑛. Note that in line 13 we expose multiple independent accumulation
chains in order to hide the latency of the vector-add SIMD instructions. Since we JIT this sub-procedure, we know
the exact value of 𝐸 at runtime. As such, we can pick appropriate unrolling factor 𝑈 as well as the remainder
handling can be performed optimally via masking in case 𝐸 is not perfectly divisible by the vector length 𝑣𝑙𝑒𝑛.
Last but not least, the JITed aggregation procedure employs prefetching of the subsequent indexed vectors in 𝑊
(line 12) in order to hide the latency of these irregular accesses.

5.1.6 Multi-Layer Perceptron (MLP) kernel

Multilayer perceptrons (MLP) form a class of feed-forward artificial neural networks. An MLP consists of (at
least three) fully connected layers of neurons. Each neuron in the topology may be using a non-linear activation
function. In this section we present the implementation of the Fully Connected layers since they constitute the
cornerstone of MLP. Even though we illustrate the forward pass of Fully Connected layers, we also implement via
TPPs the kernels of the back-propagation training in an analogous fashion. Algorithm 6 shows the fully connected
layer implementation which is mapped to TPPs. First we note that the input tensors are conceptually 2D matrices
𝐴𝑀×𝐾 and 𝐵𝐾×𝑁 that need to be multiplied. We follow the approach of previous work [21] and we block the
dimensions 𝑀, 𝐾, and 𝑁 by factors 𝑏𝑚, 𝑏𝑘 , and 𝑏𝑛 respectively. Such a blocked layout is exposing better locality
and avoids large, strided sub-tensor accesses which are known to cause TLB misses and cache conflict misses
in case the leading dimensions are large powers of 2 [21]. We leverage the BRGEMM TPP in order to perform
the tensor contraction with 𝐴 and 𝐵 across their dimensions 𝐾𝑏 and 𝑏𝑘 (which constitute the 𝐾/inner-product
dimension of the original 2D matrices). We employ the stride-based BRGEMM because the sub-blocks “𝐴𝑖 " and
“𝐵𝑖 " that have to be multiplied and reduced are apart by constant strides 𝑠𝑡𝑟𝑖𝑑𝑒_𝐴 = 𝑏𝑘 · 𝑏𝑚 and 𝑠𝑡𝑟𝑖𝑑𝑒_𝐵 = 𝑏𝑛 · 𝑏𝑘

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

29

Algorithm 7 1D Dilated convolution forward pass using TPPs
Inputs: 𝐼𝐶×𝑊 , 𝑊 𝐾×𝐶×𝑆 , 𝑑 ∈ R
Output: 𝑂𝐾×𝑄
1: 𝑊 𝑇 ← TRANSPOSE(𝑊 )
2: for 𝑝𝑜𝑠 = 0 . . . 𝑄 − 1 with step bq do
3:
4:
5:
6:

⊲ Address-based BRGEMM, prepare arguments 𝐴𝑝𝑡𝑟𝑠, 𝐵𝑝𝑡𝑟𝑠
for 𝑠 = 0 . . . 𝑆 − 1 with step 1 do

𝐴𝑝𝑡𝑟𝑠 [𝑠] = &𝑊 𝑇 [𝑠, 0, 0]
𝐵𝑝𝑡𝑟𝑠 [𝑠] = &𝐼 [0, (𝑝𝑜𝑠 + 𝑠 · 𝑑)]
BRGEMM(𝐴𝑝𝑡𝑟𝑠, 𝐵𝑝𝑡𝑟𝑠, &𝑂 [0, 𝑝𝑜𝑠], 𝑆)

7:

respectively. Finally, we apply (optionally) a unary TPP corresponding to the requested activation function (e.g.
RELU) onto the just-computed output block of 𝐶.

5.2 TPP-based Workloads
5.2.1

1D Dilated Convolutions & Computational Biology

In this subsection, we show the implementation of a special type of convolution via TPPs in their entirety,
namely one-dimensional (1D) dilated convolution layer of a 1D CNN named ATACworks [38]. ATACworks is
used for de-noising and peak calling from ATAC-Seq genomic sequencing data [38]. The 1D dilated convolution
layer in ATACworks takes more than 90% of the training time, and it has input tensor width 𝑊 , output tensor
width 𝑄, 𝐶 input channels, 𝐾 output channels, filter size of 𝑆, and dilation 𝑑. We employ the transpose TPPs,
copy TPPs, and BRGEMM TPPs to optimize the forward pass and the backward pass of the PyTorch-based 1D
convolution layer. Algorithm 7 shows an example of the forward pass procedure with an input tensor 𝐼 , a weight
tensor 𝑊 , and an output tensor 𝑂.

5.2.2 Deep Learning Recommendation Model

Facebook recently proposed a deep learning recommendation model (DLRM) [39]. Its purpose is to assist the
systematic hardware-software co-design for deep learning systems. DLRM is comprised of the following major
components: (a) a sparse embedding (see subsection 5.1.5) involving tables (databases) of varying sizes, (b) a
small dense Multi-Layer Perceptron (see subsection 5.1.6), and (c) a larger and deeper MLP which is fed by the
interaction among (a) and (b). All three parts can be configured (number of features, mini-batch sizes and table
sizes) to stress different aspects of the system. We also note that in the case of training with BF16 datatype we
leverage the BF16 split-SGD optimizer (see subsection 5.1.3). For more details on the workload and CPU-oriented
optimizations we refer to prior work [37].

5.2.3 Natural Language Processing - BERT

The BERT model is a bidirectional transformer pre-trained via a combination of masked language modeling
objective, and next-sentence prediction [40]. The heart of the BERT model is comprised by sequence of BERT
layers which are built using smaller building blocks. For ease of use and implementation, we followed modular
building blocks from Hugging Face transformers library [41] and implemented four fused layers using TPP
building blocks, namely Bert-Embeddings, Bert-SelfAttention, Bert-Output/Bert-SelfOutput and Bert-Intermediate
layers.

The SelfAttention layer in turn can be formulated as a bunch of Matrix / batch Matrix-Multiplications mixed
with element-wise scale, add, dropout and softmax operators. We formulate these Matrix-Multiplications as
tensor contractions on blocked-tensors via the stride-based BRGEMM TPP (similarly to Algorithm 6). We opt to

30

• E. Georganas et al.

Fig. 18. Binary-Reduce aggregation kernel via TPPs

use blocked tensor layouts for the same reasons briefly described in Section 5.1.6. Furthermore, by working on
one small sub-tensor at a time we naturally follow a “dataflow" computation, which has been shown to maximize
the out-of-cache-reuse of tensors among cascading operators [26, 42]. The softmax operator is also formulated
entirely by TPPs as described in Section 5.1.1. We note that the sequence of Matrix-Multiplications in the attention
layer requires sub-tensors to be transposed (and VNNI transformed in case of BF16 implementation), and for this
task we leverage the transpose/transform TPPs. Bert-Output and Bert-SelfOutput layers perform GEMM over
blocked layout, and fuse bias addition, dropout, residual addition and layernorm using TPPs. The Bert-Embeddings
layer also performs layernorm and dropout after embedding lookups that are also implemented using TPPs.
Finally, Bert-Intermediate layer performs blocked GEMM followed by bias addition and GELU activation function
which we implement using the GELU TPP.

5.2.4 Emerging AI - Graph Neural Networks

Graph Neural Networks (GNN) [43] form an emerging class of Neural Networks for learning the structure of
large, population-scale graphs. Depending on the specific algorithm and task that a GNN is designed for (e.g. node
classification, link prediction), feature-vector aggregation precedes or succeeds a shallow neural network. Such a
shallow neural network typically materializes one or more linear transformations, followed by a classification
or regression mechanism [44], and the relevant TPP-based implementation is essentially the one we present in
Algorithm 6.

We focus here on the TPP-based implementation of the feature-vector aggregation. This aggregation motif
can be seen as a sequence of linear algebraic expressions involving node/edge features, along with the relevant
operators. Prior work [44] has focused on the following two algebraic sequences: Copy-Reduce and Binary-Reduce.
We elaborate here on the latter sequence Binary-Reduce (as the first is even simpler). The feature-vectors (either
pertaining to vertices or edges) are represented via dense 2D matrices/tables. At the same time, the adjacency
information in the graphs can be eventually found via arrays of indices. Therefore, by providing a set of indices
and the appropriate Tables of feature-vectors (assuming column-major storage), one can extract selectively
the desired feature-vectors via Gather-columns operations. Then, the extracted feature-vectors are fed into a
binary operator, and the outcome of the binary operations are finally reduced (the reduce operation could be
sum/max/min etc).

Figure 18 illustrates a TPP tree that is used to express the Binary-Reduce aggregation kernel. The TPP back-end
optimizes this sequence of TPPs and performs horizontal register fusion across them. More precisely, two feature-
vectors namely 𝑣0 and 𝑣1 are extracted at a time from Table 0 and Table 1 respectively by using the relevant
indices arrays, and they are combined via the proper binary op to get an intermediate vector 𝑣𝑖 . Subsequently, 𝑣𝑖
is reduced with a running reduce-vector 𝑣𝑜 that holds the output of this composite operator. Once the running

GatherColumns1. Table 02. Indices 0BinaryTPPGatherColumnsOPReduceColumns1. Table 12. Indices 1Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

31

reduction has been completed (i.e. all indexed columns from Table 0 and Table 1 have been accessed, processed
and reduced), the output vector 𝑣𝑜 is stored in the corresponding output subtensor.

6 EXPERIMENTAL RESULTS OF DL KERNELS & WORKLOADS
We use a variety of platforms that span different ISAs, different vendors and micro-architectures. More specifically,
our tested platforms include: i) a 22-core Intel Xeon E5-2699 v4 (BDX) supporting up to AVX2 ISA, ii) a 28-core
Intel Xeon 8280 (CLX) supporting up to AVX512 ISA, iii) a recently announced 40-core Intel Xeon 8380 (ICX)
supporting also up to AVX512 ISA, iv) a 28-core Intel Xeon 8380H (CPX) supporting up to AVX512 ISA, which
also offers BF16 FMA acceleration, v) a 64-core AMD EPYC 7742 (ROME) with AVX2 ISA, vi) an AWS Graviton2
instance with 64 cores at fixed 2.5 GHz and AArch64 ISA, vii) a 48-core Fujitsu A64FX at fixed 1.8 GHz with
ARMv8 SVE ISA, and viii) a 4-core client Intel i7-6700 CPU (i7) supporting up to AVX2 ISA. All Intel and AMD
chips are operating in Turbo mode. For the cluster experiments we used a 32 node CLX installation with a
dual-rail Intel Omnipath 100 pruned 2:1 fat-tree topology.

6.1 Performance of standalone DL kernels
We start the performance evaluation with standalone TPP kernels presented in subsection 5.1. First, we want to
highlight the productivity/efficiency provided by TPPs: the high-level code expressed via TPPs/trees of TPPs can
match or outperform code by compilers, and hand-vectorized (thus non-portable code) written by performance
experts. Second, we want to show the portability aspect of TPPs, since exactly the same high-level code yields
high-performance across different ISAs and micro-architectures.

Figure 19-Top shows the performance of the Softmax operator of blocked 3D tensors with size 𝑆1 × 𝑆2 × 𝑆3,
on the CLX platform (i.e. targeting AVX512 ISA). Here we perform 𝑆2 softmax operations over blocked 𝑆1 × 𝑆3
dimensions. The sizes are chosen such that some of the dimensions do not match perfectly with the vector length.
The baseline is the icc generated code with -O3 optimization level and high-zmm usage flags. The second
variant is also icc-generated code, but we propagate the tensor sizes/loop bounds via compile-time constants in
order to assist the auto-vectorization/optimize remainder handling via masking. The third code variant is the
AVX512 hand-vectorized by an expert, where the 𝑒𝑥𝑝 function uses fast Taylor approximation. Last, we evaluated
the TPP-based softmax implementation. As we can see, by propagating the tensor sizes we achieve (geo-mean)
speedup of 1.3× over the baseline. The hand-vectorized code is faster by 2.6× whereas the TPP-based variant
shows similar speedups by being 2.2× faster. The main shortcoming of the hand-vectorized code is that it is
platform-dependent and as such non-portable. More specifically, we didn’t have to our avail AVX2 hand-optimized
code in order to experiment with it on ROME. On the contrary, Figure 20-Top shows the softmax performance
on AVX2 enabled platform for the compiler-generated code and the TPP based code. The TPP-based softmax
exhibits geo-mean speedup of 2.45× over the baseline on ROME.

Figure 19-Middle shows the performance of the layernorm operator on the CLX platform. Since the layernorm
code is more straightforward (i.e. no expensive exp function is involved), we see that icc with compile-constant
bounds outperforms by 1.9× the baseline. We inspected the compiler-generated code and identified that the
reduction-loops were recognized and were heavily optimized with multiple accumulation chains etc. Similarly, the
hand-vectorized code and the TPP based code outperform the baseline by 1.3× and 1.5×. We also experimented
with gcc and the fast-math flag, and it just matched baseline performance. We want to emphasize that
propagating the tensor sizes as compile-time constants throughout the operators is not practical for real use-cases
within DL frameworks. Figure 20-Bottom shows similar performance speedups on ROME, where the TPP-based
code is 1.6× faster than the auto-vectorized baseline.

32

• E. Georganas et al.

Fig. 19. TPP kernels on CLX

Figure 19-Bottom shows the performance of the BF16 split-SGD operator on CLX. This use-case represents a
novel, mixed-precision operator where the compiler (even with compile-time constant tensor sizes) struggles to
yield good performance; the TPP-based code has geo-mean speedup of 38× over the compiler generated code.
Figure 21 illustrates the TPP-based implementation of various ResNet50 [33] Convolution layers across all
available platforms. The minibatch size used on each platform equals to the number of the corresponding cores.
It is noteworthy that the TPP-user code is identical for all targets, hence truly portable; it is merely that the TPP
backend optimizes the code generation (BRGEMM in this case) in a platform/ISA-aware fashion. The geomean
efficiencies of these convolutions are: 69% for BDX, 72% for CLX, 72% for CPX, 77% for CPX with BF16 datatype,
70% for ICX, 78% for ROME, 81% for Graviton2 and 52% for A64FX. Previous work [21] also showed on an
x86 TPP-predecessor that BRGEMM-based convolutions matched or outperformed Intel’s oneDNN library [13].
Fujitsu recently contributed an A64FX back-end to oneDNN [45] and our TPP implementation outperforms this
by 22% on the geomean. We observe that our TPP convolutions not only run on all of these different platforms
without a single line of code change, but they run at very similar hardware utilization.

0123415x11x815x11x1315x11x1615x11x2315x11x3232x32x832x32x1332x32x1632x32x2332x32x32GeomeanSpeedup over compilerTensor dimensions: S1xS2xS3Softmaxon CLXCompiler-vectorizedCompiler-vectorized with constant bounds (icc)Hand-vectorizedTPP00.511.522.5315x11x815x11x1315x11x1615x11x2315x11x3232x32x832x32x1332x32x1632x32x2332x32x32GeomeanSpeedup over compilerTensor dimensions: S1xS2xS3Layernormon CLXCompiler-vectorized without constant bounds  (icc)Compiler-vectorized without constant bounds  (gcc)Compiler-vectorized with constant bounds  (gcc)Compiler-vectorized with constant bounds (icc)Hand-vectorizedTPP01020304050607015x715x2215x3215x6327x727x2227x3227x6332x732x2232x3232x6342x742x2242x3242x6364x764x2264x3264x63GeomeanSpeedup over compilerTensor dimensions: MxNBF16 Split-SGD on CLXCompiler-vectorizedCompiler-vectorized with constant boundsHand-vectorizedTPPTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

33

Fig. 20. TPP kernels on ROME

Fig. 21. Convolutions via BRGEMM TPP

6.2 Performance of end-to-end DL workloads
6.2.1

1D Dilated Convolutions and their application to Computational Biology

Here we evaluate the oneDNN [13] and TPP-based 1D dilated convolution layer of ATACworks [38] which
takes takes more than 90% of the training time, and it has input tensor width (𝑊 ) of 60400, output tensor width
(𝑄) of 60000, 15 input channels (𝐶), 15 filters (𝐾), filter size (𝑆) of 51, and dilation (𝑑) of 8. Figure 22-Top shows
the computational efficiency results of the 1D convolution layer. oneDNN is not reaching peak performance for
these specialized convolutions, exhibiting 19.9% efficiency for the forward pass and only 4.1% for the backward
pass on CLX. Our TPP-based implementation shows 74.3% and 55.7% efficiency for the corresponding training
passes. We also highlight the performance portability of our TPP-based approach across all tested platforms.
Finally, we show training time per epoch results for ATACworks in Figure 22-Bottom. The TPP-based kernels
provide training time speedup of 6.91× on CLX when comparing to the oneDNN based implementation. We also
show that by leveraging the BF16 FMA acceleration of the CPX platform we can further obtain 1.62× speedup

01234515x11x815x11x1315x11x1615x11x2315x11x3232x32x832x32x1332x32x1632x32x2332x32x32GeomeanSpeedup over compilerTensor dimensions: S1xS2xS3Layernorm on ROMECompiler-vectorizedCompiler-vectorized with constant boundsTPP0123415x11x815x11x1315x11x1615x11x2315x11x3232x32x832x32x1332x32x1632x32x2332x32x32GeomeanSpeedup over compilerTensor dimensions: S1xS2xS3Softmax on ROMECompiler-vectorizedCompiler-vectorized with constant boundsTPP01000200030004000500060007000800090003-64-224-224-7-7-2-3-3128-128-56-56-3-3-2-1-164-64-56-56-3-3-1-1-1256-512-56-56-1-1-2-0-064-64-56-56-1-1-1-0-064-256-56-56-1-1-1-0-0256-64-56-56-1-1-1-0-0256-128-56-56-1-1-1-0-0256-256-28-28-3-3-2-1-1128-128-28-28-3-3-1-1-1512-1024-28-28-1-1-2-0-0512-256-28-28-1-1-1-0-0512-128-28-28-1-1-1-0-0128-512-28-28-1-1-1-0-0512-512-14-14-3-3-2-1-1256-256-14-14-3-3-1-1-11024-2048-14-14-1-1-2-0-0256-1024-14-14-1-1-1-0-01024-512-14-14-1-1-1-0-01024-256-14-14-1-1-1-0-0512-512-7-7-3-3-1-1-1512-2048-7-7-1-1-1-0-02048-512-7-7-1-1-1-0-0Geo MeanPeakGFLOPSWeak-scaling ResNet50 1.0 & 1.5 convolutions (C-K-H-W-R-S-stride-padh-padw)BDXCLXCPX, FP32CPX, BF16ICX, FP32ROMEGraviton2A64FX34

• E. Georganas et al.

Fig. 22. 1D Dilated Convolutions

compared to the FP32 implementation on the same platform. In total BF16 yields 12.6× speedup over the oneDNN
baseline.

6.2.2 Deep Learning Recommendation - DLRM

Figure 23-Top shows the FP32 DLRM performance on CLX using two different configurations, namely small
DLRM (blue bars) and MLPerf DLRM (orange basrs). We refer to previous work for the detailed specification
of these configurations [37]. We evaluated 4 different implementations of DLRM: i) the PyTorch reference
implementation, ii) PyTorch reference + custom Embedding extension auto-vectorized by the compiler, iii)
DLRM expressed entirely via TPPs, and iv) hand-vectorized Embedding extension + BRGEMM-TPP based
MLPs [37]. We conclude that the TPP-based implementation matches the performance of the State-Of-The-Art
implementation which is hand-vectorized specifically for AVX512 targets; both of these optimized versions
substantially outperform the PyTorch CPU reference implementation by up to 48×. Compared to the version
with the custom, auto-vectorized variant the TPP-version is up to 4.4% faster.

Figure 23-Bottom shows the DLRM performance of our TPP-based implementation across multiple platforms
and compute precisions. We want to highlight two aspects: First, we are able to run the same TPP-code without
any change across all platforms, something that is not doable with the hand-vectorized SOTA variant (iv) (since
it is not able to run on the AVX2-only BDX and ROME platforms, or on the Graviton2 platform with AArch64
ISA). Second, the TPP-based BF16 shows speedup up to 28% over the variant with auto-vectorized Embedding
extension. The culprit here is the mixed precision operations like split-SGD where the compiler struggles to yield
efficient code as shown in Section 6.1.

Figure 24 illustrates the performance breakdown of the small config on multiple platforms. The blue portions
of the bars correspond to the time spent on the Embedding component, the orange parts reflect the MLP portion,
and finally the yellow portions correspond to the remaining components of the DLRM workload. We observe that
depending on the platform, the time spent on Embedding varies from 29-37% of the total execution time, the time
spent on MLP is in the range of 33-56% of the total time, and the rest components account for 15-23% of the time.

19.971.461.674.370.569.664.14.120.123.455.756.342.244.60501001s CLX(oneDNN)1s BDX1s ROME1s CLX1s CPX1s ICX1s CPX (BF16)Efficiency (%)Efficiency of 1D dilated convolutionsForward PassBackward Pass967369192118140012409317661.001.404.576.917.8010.3912.630510150500010000150001s CLX(oneDNN)1s BDX1s ROME1s CLX1s CPX1s ICX1s CPX (BF16)SpeedupTime (sec.)ATACworks training time per epochTraining time per epochSpeedupTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

35

Fig. 23. DLRM performance on a small config (blue bars) and on the MLPerf config (orange bars)

Fig. 24. DLRM performance breakdown of small config on multiple platforms

We can also observe the correlation of the MLP performance with the compute capabilities of each platform. For
example, on CPX which has native BF16 FMA support, the BF16 MLPs are sped up by ∼2× compared to the FP32

191041.139.3539.6380.3232.7132.8632.98050100PyTorch Reference + custom Embeddingextension (scalar code)TPP-based DLRMHand-vecotrized codemsec/iterFP32 DLRM implementations on CLXSmall DLRM configMLPerf DLRM config89.8648.4252.1439.3535.9737.4629.9723.4155.1652.0546.2732.8630.9523.3929.8125.010102030405060708090100BDXGRAVITON2ROMECLXCPXICXCPX-BF16(MLP viaTPPs)CPX-BF16 (allvia TPPs)msec/iterTPP-based DLRM performance across mu>ple pla?orms/precisionsSmall DLRM configMLPerf DLRM config43.811.215.515.013.09.910.031.020.328.919.318.113.77.917.97.27.77.85.75.55.30102030405060708090100BDXGRAVITON2ROMECLXCPXICXCPX-BF16msec/iterPerformance breakdown of DLRM small configEmbeddingMLPOthers36

• E. Georganas et al.

Fig. 25. BERT Large performance

MLPs on the same platform. In regard to the time spent on the Embedding kernel which tends to be bandwidth
bound, we observe correlation with the corresponding bandwidth capabilities of the machines.

6.2.3 Natural Language Processing - BERT Large

Figure 25-Top shows end-to-end performance (in examples/second) on CLX for the BERT large SQuAD fine-
tuning task in FP32, using a max sequence length of 384 and minibatch of 24. We observe that the TPP-based
implementation (blue bar) matches the performance of the AVX512-hand-vectorized code/orange bar. At the
same time, our implementation is 1.69× faster than the Reference Hugging Faces CPU reference code [46] (green
bar).

Figure 25-Bottom shows the performance of the reference Hugging Faces code (green bars) versus the TPP-
based code (blue bars) across multiple platforms (x86 and AArch64/Graviton2) and compute precisions (FP32 for
all platforms, and BF16 for the CPX platform). The TPP-based BERT shows speedups ranging from 1.5× to 8.5×
over the Hugging Faces code. This result highlights the performance portability through the TPP abstractions. In
regard to various compute precisions, we note that with minimal changes inside the fused operators to handle
the VNNI tensor layout (required for BF16 GEMM/BRGEMM), and a couple of lines changes in the application

1.791.673.022.9800.511.522.533.5Hugging Faces ReferenceGEMMS only via TPPsall workload via TPPshand-vectorized (avx512)examples/secFP32 BERT implementations on CLX0.780.280.731.902.192.921.152.383.373.023.264.766.500.001.002.003.004.005.006.007.00BDXGRAVITON2ROMECLXCPXICXCPX-BF16examples/secBERT performance across multiple platforms/precisionsHugging Faces ReferenceTPP-based implementation1.5x8.5x4.6x1.6x1.5x1.6xTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

37

Fig. 26. BERT Large performance breakdown on multiple platforms.

code to enable BF16 training, we were able to realize 2× speed up using BF16 training on CPX (compared to FP32
training on CPX) with 28 cores, surpassing 40-core FP32-ICX performance by 37%.

In order shed light on where the benefits are coming from, we present in Figure 26 the performance breakdown
of the Hugging Faces reference code and the TPP-based implementation. In particular we focus on 4 components:

(1) GEMM which corresponds to the tensor contractions implemented via either the BRGEMM-TPP in the TPP
implementation, or it leverages optimized GEMM routines within BLAS libraries in the Hugging Faces
implementation (MKL for x86 platforms and OpenBLAS for AArch64/Graviton2).

(2) Dropout corresponding to the dropout layer in BERT, where the TPP-based implementation employs fast

random number generation via xorshift algorithm.

(3) GeLU corresponding to the Gaussian Error Linear Unit activation function in BERT, where the TPP-based

implementation leverages fast approximations as discussed in section 3.3.2.

(4) Others capturing the remaining operators: Transpose, Layer-norm, softmax, bias addition, vnni-reformatting
(in case of BF16 training), copy, add, scale, zero-kernel, reduce, optimizer. Note that all these operators map
to either unary/binary/ternary TPPs (see section 2) or the can be expressed via Matrix Equation TPPs (see
section 5).

First, we note that for the Intel x86 platforms (left part of the breakdown plot) the tensor contractions show
speedups over the highly-optimized MKL GEMM implementation in Hugging Faces in the range of 2-6%. On the
right side of the breakdown plot we observe that the BRGEMM-TPP benefits are even larger on the non-Intel
platforms. More specifically, on AMD Rome (AVX2 x86 platform) the tensor contractions are sped up by 1.9×
via the BRGEMM-TPP, and on Graviton2 (Arm AArch64 platform) the tensors contractions are 5.7× faster via
the BRGEMM-TPP compared to the implementation relying on OpenBLAS GEMM calls. To further highlight
the performance portability of the tensor contractions via the BRGEMM-TPP across multiple platforms and
precisions, Figure 27 shows the achieved GEMM performance (Left axis) on each platform for the entire training
process (blue bars), whereas the orange line (Right axis) dictates the % of machine peak. The conclusion here is
that the BRGEMM-TPP delivers high-efficiency for the corresponding tensor contractions in the range of 66-84%
for all tested ISAs and micro-architectures.

The second conclusion we can draw from the performance breakdown in Figure 26 is that our fused/dataflow
TPP implementation outlined in section 5.2.3 makes the dropout and GeLU times shrink substantially, offering
speedups in the range of 10-360×. The BERT implementation via the dropout/GeLU TPPs in tandem to the

0102030405060708090HF-refTPPHF-refTPPROMEGRAVITON205101520253035HF-refTPPHF-refTPPHF-refTPPTPP-BF16HF-refTPPBDXCLXCPXICXTime per iteration (sec)Performance breakdown of BERT Large  GEMMDropoutGeLUOthers38

• E. Georganas et al.

Fig. 27. BERT GEMM/tensor contraction efficiencies via the BRGEMM-TPP on multiple platforms.

BRGEMM TPPs take advantage of temporal locality, and virtually make the corresponding times disappear
from the overall execution time. Last but not least, the remaining components are sped-up in the TPP-based
implementation by 2.5-14× depending on the platform. As a result of these optimizations, the TPP-based BERT
implementation spends the majority of the time (75.5-88.8%) in tensor contractions which are executed at
high-efficiency as Figure 27 shows.

6.2.4 Emerging AI - Graph Neural Networks

Figure 28-Top shows end-to-end performance (in seconds/epoch, so lower is better) on CLX for the full-
batch training of the GraphSAGE workload on OGB-Products with FP32 and BF16 precision. For the CLX BF16
experiments, since CLX doesn’t have native support for BF16 FMAs, we use bit-wise accurate emulated-BF16
BRGEMM TPPs (see section 3.2.2), and we still expect savings due to the bandwidth reduction in the non-
GEMM parts, e.g., graph traversal and edge/node aggregation. We observe that the TPP-based implementation
outperforms the DGL with Xbyak JIT backend baseline version by 2.65×. The TPP-BF16 version yields another
1.66× speedup over the TPP-FP32 variant mainly due to reduced bandwidth requirements.

Figure 28-Bottom shows the performance of the TPP-based code across multiple platforms (x86 and Arm
AArch64) and compute precisions (FP32 and BF16). The relative differences in the performance can be justified by
the different compute/bandwidth specs of the benchmarked platforms. We highlight that with minimal changes
in the MLP portion to handle VNNI layout required for BF16 BRGEMM, and a couple of lines changes in the
application code to enable BF16 training, we were able to realize 1.94× speed up using BF16 training on CPX
with 28 cores compared to the FP32 training on the same platform.

In order to further analyze the behavior of the various implementations on multiple platforms, we present on
Figure 29 the relevant performance breakdown. The very left bar shows the performance breakdown of the FP32
optimized DGL implementation that leverages JITed kernels through Xbyak on the CLX platform. The blue part
corresponds to the Aggregation kernel described in subsection 5.2.4 whereas the orange portion represents the
time required by the remaining kernels, namely Multilayer-Peceptrons with Activation functions. In the DGL
implementation the activation functions are not fused within the MLP’s tensor contractions. We observe that in
this optimized DGL implementation, 82.3% is spent on the Aggregation kernel and only 17.7% is spent on the
MLPs. On the second from the left bar (annotated as CLX-FP32) we show the performance of the FP32 TPP-based
implementation on the same CLX platform. We conclude that the TPP-based Aggregation kernel exhibits a

1.072.733.064.362.083.166.8968.8%66.2%71.1%74.0%81.2%77.0%83.6%0.0%20.0%40.0%60.0%80.0%100.0%0.001.002.003.004.005.006.007.008.00BDXCLXCPXICXGRAVITON2ROMECPX-BF16% of machine peakperformance in TFlops/secGEMM efficiencies in BERT via the BRGEMM TPPTF/sEfficiencyTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

39

Fig. 28. GNN performance of GaphSAGE Full-batch training for OGB-Products

Fig. 29. GNN performance breakdown of GaphSAGE Full-batch training for OGB-Products

speedup of 3.29× compared to the DGL-Xbyak implementation, and the TPP-based MLP kernels (BRGEMM-TPP
tensor contractions with fused TPP activation functions) exhibit a speedup of 1.4× compared to the respective

23.858.995.42051015202530DGL + XBYAK JITED (FP32)TPP-based (FP32)TPP-based (BF16)secs/epochGraphSAGE Full-batch Training on single-socket for OGB-Products on CLX26.2210.707.498.997.905.075.424.08051015202530BDXGRAVITON2ROMECLXCPXICXCLX-BF16CPX-BF16secs/epochTPP-Based GraphSAGEfor OGB-Products on across multiple platforms/precisions2.65x1.66x19.635.963.3017.645.093.375.905.192.514.233.022.128.582.401.704.792.711.57051015202530CLX/ DGL-XBYAKCLX-FP32CLX-BF16BDXROMEICXGRAVITON2CPX-FP32CPX-BF16secs/epochPerformance breakdown on multiple platforms/precisions Aggregation KernelMLP + RestTPP-based implementations40

• E. Georganas et al.

Fig. 30. Distributed-memory scaling of workloads

DGL-Xbyak implementation. The FP32 TPP-based implementation spends 66.4% on the aggregation kernel and
33.6% on the fused MLP kernels.

The last 8 bars on Figure 29 illustrate the performance breakdown of the TPP-based implementation on various
platforms (CLX/BDX/ROME/ICX/GRAVITON2/CPX) and various precisions (FP32 and CPX-BF16). We want to
emphasize that all these performance numbers are obtained by employing a the same exact TPP-based code
(which is platform-agnostic); the only modification is pertaining to the BF16 TPP code where we changed the
tensor layouts in the MLP portion in order to deal with the required VNNI format. When comparing the CPX-F32
and the CPX-BF16 performance breakdowns we observe a 2× speedup on the Aggregation kernel. This kernel is
typically bandwidth bound due to its irregular/indexed accesses, and the BF16 TPP code moves half of the data
compared to the FP32 TPP code since all the tensors are halved in size (BF16 vs FP32 datatype). The MLP portion
of the TPP-based implementation is sped up by 1.73× by using the BF16 BRGEMM-TPP. The CPX platform
supports the BF16 FMA instruction which has effectively 2× the compute throughput compared to the FP32
FMA on the same platform. The BF16 BRGEMM-TPP internally leverages this BF16 FMA instruction within the
GEMM microkernel on CPX (see subsection 3.2) to speed up the tensor contraction. Finally, we highlight here
the speedup of the Aggregation kernel when e.g. comparing the CPX and the ICX FP32 TPP-based performance
numbers. The ICX platform has STREAM bandwidth of 175 GB/s whereas CPX has 97.7 GB/s, and this trend is
reflected also in the performance of the Aggregation kernel (1.54× faster on ICX than CPX).

6.3 Distributed-memory scaling of DL workloads
Even though we focused on the evaluation of the TPP-based workloads on a single node, our approach is
seamlessly incorporated into the DL frameworks, hence we can scale to multiple nodes in a cluster to accelerate
the training process employing the oneCCL library [47]. Figure 30 shows the distributed-memory scaling of
the TPP-based workloads. DLRM and BERT show almost perfect weak-scaling from 1 to 64 sockets of CLX (32
nodes) with speedups 51.7× and 57.9× respectively. Regarding the scaling of the GNN workload, the efficiency
is directly affected by the quality of the partitions produced by the graph partitioning tools. Using 64 sockets
we achieve 10× speedup compared to single socket, and further scaling improvements constitute future work.
We can conclude that TPPs for single node optimizations combined with small-size cluster level execution can
accelerate deep learning training on CPUs by up to two orders of magnitude.

7 TPP WITHIN MLIR AND A TENSOR COMPILER
In order to illustrate the viability of TPPs as a virtual Tensor ISA within MLIR and Tensor Compilers, we
implemented a rudimentary MLIR dialect corresponding to the TPPs. We also implemented lowering passes

12481632641248163264Speedup over 1 socketNumber of socketsDistributed-memory scaling of TPP-based workloadsDLRMBERTGNNIdealTensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

41

Fig. 31. Example lowering paths within the PlaidML Tensor compiler in order to achieve full network optimization from
popular frameworks. The green boxes represent the DL frameworks, the blue boxes correspond to MLIR dialects, the brown
box shows the TPP-MLIR dialect within the stack, and the purple box represents the targeted platforms.

Fig. 32. FP32 inference with PlaidML on various workloads: ResNet-152, ResNext-50, and I3D-Kinetics-400.

within the PlaidML [15] Tensor Compiler that transform intermediate MLIR representations to the TPP-MLIR
dialect. The TPP-MLIR dialect is subsequently lowered to the corresponding LIBXSMM TPP calls, therefore such
a flow is not relying on LLVM for the code generation of the corresponding tensor operations.

The current lowering path through MLIR supports a variety of front-end interfaces with LinAlg or Tile as the
lowest level common entry points, i.e. the lowest level of abstraction that inbound programs can be specified in
such that they will be subject to the full range of optimizations necessary to achieve full performance. Figure 31
details the lowering paths currently implemented in PlaidML and where key transforms map tensor operations
into the TPP dialect. The key transformation is located in the stencil pass of the PXA dialect (Parallel eXtensions for
Affine - a staging ground for PlaidML/TPP work that will be proposed upstream to the affine dialect). Operations
that cannot be matched to TPP primitives are lowered through standard affine optimization pipelines.

We experimented with the use-case of FP32 inference on a client CPU (Intel i7-6700) on three different
workloads: ResNet-152 [33], ResNext-50 [48], and I3D-Kinetics-400 [49]. Figure 32 shows the results of three

Tensor Processing Primitives (TPP) as Virtual Tensor ISAHardware TargetsMHLOTensorFlowONNXPyTorchTOSATBDLinAlgAffine/PXASCFLLVM/SPIRV11122.911.835.619.112.929.5010203040ResNet-152ResNext-50I3D-Kinetics-400SpeedupFP32 inference on Intel Corei7-6700PlaidML (MLIR + LLVM code generation)PlaidML (MLIR + TPPs as virtual tensor ISA)TensorFlow + oneDNN42

• E. Georganas et al.

implementations: i) The green bars show the performance of the code generated by PlaidML with MLIR for
intermediate representations, and LLVM for the code generation, ii) The orange bars show the performance
of the code generated by PlaidML with MLIR for intermediate representations, and the TPP-MLIR dialect as
virtual Tensor ISA for the code generation of the corresponding tensor contractions, and iii) TensorFlow FP32
inference backed-up by the vendor-optimized oneDNN library. We observe that the Tensor Compiler variant
which relies on the TPP-MLIR dialect for the tensor contractions outperforms the variant which relies exclusively
on LLVM (for loop-tiling and vectorization) up to 35.6×. At the same time, PlaidML assisted by the TPP-MLIR
dialect matches/outperforms the performance of TensorFlow which uses internally oneDNN, a highly-tuned
vendor library for this CPU target. These preliminary results highlight the viability of the synergistic Tensor
Compiler - TPP paradigm as discussed in Section 1.

8 TPP AND HPC APPLICATIONS
So far, in this paper the focus was on how the TPP abstraction can be leveraged within the Deep Learning
Domain. Tensor computations are ubiquitous, and in particular they constitute the cornerstone of many HPC
applications. As such, the TPP abstraction can be readily employed by HPC applications to accelerate tensor
computations without sacrificing portability. In the rest of this section we examine how TPPs are used within
two HPC applications, namely CP2K and EDGE.

8.1 CP2K
The tensor based formulation originated and became common in physics, and it is well adopted in the field of
engineering or applied sciences, and in electronic structure (ES) theory in particular. CP2K is an open source ES-
and MD-package (molecular dynamics) for atomistic simulations of solid-state, liquid, molecular, and biological
systems [50]. CP2K is striving for good performance on HPC and massively parallel systems. Even though the
use of novel algorithms in CP2K is the norm for scientific reasons, implementations have not widely tapped
tensors in an explicit fashion. In contrast, Machine Learning emerged with similar, yet not coherent APIs and
frameworks around the notions of tensors, layers and image processing.

While ES calculations can be formulated with tensors of ranks two to four, CP2K (and similar packages)
largely remain with matrix based formulation. Various libraries for tensor contractions gained some attraction
for scientific applications but the level of generality is key, e.g., as sparse representations are desired. CP2K
explored an API for sparse tensor contractions and published a proof of concept implementation built into the
DBCSR library [51]. Efforts targeting accelerators in CP2K, namely GPUs, are not fully booked hence hardware
specifically for Deep Learning (with focus on low and mixed precision arithmetic) is not yet a motivation of
tensors as an implementation vehicle (and source of acceleration). Therefore a collection of primitives such as
TPP is well-suited for an emerging discussion of a more general API.

CP2K 3.0 introduced LIBXSMM for Small Matrix Multiplications (SMMs). CP2K and DBCSR (previously part
of CP2K’s code base) since then additionally introduced element-wise operations (copy and transpose) with
"elements" being small matrices based on LIBXSMM. Reformulating existing code to build on (batched) GEMM
TPP and element-wise TPP operations is an established pattern for increased performance in CP2K.

To practically improve performance in CP2K one has to consider:

• Fusing kernels and increasing arithmetic intensity independent of the target being a CPU or an accelerator

(performance bound by memory bandwidth).

• Specializing code at runtime based on workload/input of the application, e.g., generating code Just-In-Time

(JIT) a.k.a. meta-programming.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

43

These objectives can be delivered by TPPs as a domain-specific language (DSL), enabling the scientist to write
more abstract code, e.g., by the means of meta-programming, and by relying on a specification which delivers
versatile primitives deferring low-level optimizations to the TPP backend.

For CP2K’s performance evaluation, we refer to BDX, CLX, ICX, and ROME as introduced earlier (section 6).
To show the portability of our approach, we augmented our results by using the Oracle Cloud Infrastructure,
namely the result for Altra processor (BM.Standard.A1.160 OCI shape). Table 4 shows the performance benefit of
LIBXSMM’s GEMM-TPP in CP2K when compared to Intel’s MKL GEMM routines.

Table 4. CP2K performance (Cases/Day) of three workloads fitting into single systems with two processors. Single-socket
performance is reported here for consistency within this paper. Intel MKL or OpenBLAS are always used for general BLAS
operations including large GEMMs. Either (BLAS-)GEMM or TPP-GEMM was used for batched multiplication of small
matrices (SMMs). Workloads utilizing CP2K’s DBCSR library for distributed block-sparse matrix multiply benefit from
(runtime-)specialized GEMM-TPP kernels where the set of matrix shapes is not known at compile-time of the application or
depends on the workload in general.

System
BDX

CLX

ICX

ROME

Altra

Workloada
H2O-256
H2O-512
H2O-256
H2O-512
H2O-DFT-LS4
H2O-256
H2O-512
H2O-DFT-LS4
H2O-256
H2O-512
H2O-DFT-LS4
H2O-256
H2O-512
H2O-DFT-LS4

BLAS-GEMMb TPP-GEMMc

91
23
154
39
45
235
60
67
225
55
65
228
60
60

101
27
162
41
47
249
65
70
244
57
65
236
62
66

TPP-Speedup
11%
17%
5%
5%
4%
6%
8%
4%
8%
4%
0%
4%
3%
10%

aH2O-256 (CP2K bench.), H2O-512 (UEABS Case A) and H2O-DFT-LS NREP=4 (UEABS Case C) from PRACE UEABS 2.1.
bIntel MKL (x86-64) or OpenBLAS (otherwise).
cLIBXSMM.

8.2 EDGE
The Extreme-Scale Discontinuous Galerkin Environment (EDGE) uses the Arbitrary high-order DERivatives
(ADER) Discontinuous Galerkin (DG) finite element method to simulate seismic wave propagation [52]. The
software uses unstructured tetrahedral meshes which are typically adapted to the used seismic velocity models.
Additionally, modelers may introduce mountain topography. A sophisticated local time stepping scheme allows
the solver to operate efficiently in very large and complex settings. The software is able to fuse multiple ensemble
simulations into one execution of the software. EDGE uses an orthogonal polynomial expansion basis to discretize
each of the considered variables in a tetrahedron of the mesh. In a typical setting, we use three relaxation
mechanisms for the viscoelastic part, resulting in a total of 27 seismic variables. Additionally using a fifth order
method gives us 35 basis functions, resulting in a total of 27 · 35 = 945 degrees of freedom per tetrahedral
element. The solver advances the degrees of freedom in time by repeatedly computing a triplet of quadrature-free

44

• E. Georganas et al.

integrators. While the actual integrators are part of EDGE, their implementation relies heavily on TPPs. The
GEMM-TPP with small and uncommon matrix sizes is the most crucial operation required by EDGE. For example,
the surface integrator requires the multiplication of a 9 × 35 matrix with a 35 × 15 matrix. The solver’s extension
with additional, performance-portable TPPs in all parts of the integrators is work-in-progress. Especially, EDGE’s
support for viscoelastic attenuation or local time stepping requires “simpler“ kernels, e.g., the unary TPPs Identity
and Zero, or the binary TPPs Mul, Sub and Add.

We evaluate EDGE’s performance-portability through the use of TPPs by studying the performance of a full
setup of the Layer Over Halfspace 3 (LOH3) benchmark with 743,066 tetrahedral elements. The same setting was
also used in [53] to study the performance of the solver on a single processor of the Frontera supercomputer
located at the Texas Advanced Computing Center (position ten in the 06/21 TOP500-list). Following this study,
a sophisticated simulation of the 2014 M𝑤 5.1 La Habra earthquake using a mesh with 237,861,634 tetrahedral
elements and EDGE’s advanced features yielded a performance of 2.20 FP32-PFLOPS on 1,536 nodes.

Table 5. Sustained 32-bit floating point performance on the studied systems. The performance is given in TFLOPS. Results are
presented for Global Time Stepping (GTS) and Local Time Stepping (LTS) when using single and fused forward simulations.

System

Cascade Lake
Ice Lake
Rome
Milan
Altra

GTS

LTS

single
1.08
1.29
1.20
1.39
1.27

fuseda
0.78
1.01
1.08
1.16
0.73

single
1.02
1.23
1.12
1.29
1.51

fuseda
0.74
0.96
1.01
1.07
0.76

aEDGE’s fused simulations use sparse matrix kernels.

For the EDGE application, we study the software’s raw floating point performance and time-to-solution by

extending our LOH3-Frontera-only study [53] with diverse processors:

• Cascade Lake (similar to CLX as introduced in section 6): 2.7 GHz 28-core Intel Xeon Platinum 8280 processor
of the Frontera system at the Texas Advanced Computing Center. We only used a single 28-core processor
of Frontera’s dual-socketed compute nodes in our tests.

• Ice Lake: 2.3 GHz 40-core Intel Xeon Platinum 8380 processor on Intel’s on-premises cluster. We only used

a single 40-core processor of the dual-socket compute nodes in our tests.

• Rome (similar to ROME as introduced in section 6): 2.25 GHz AMD EPYC 7742 (BM.Standard.E3.128 OCI

shape). We only used a single 64-core processor of the bare metal instance in our tests.

• Milan: 2.55 GHz AMD EPYC 7J13 (BM.Standard.E4.128 OCI shape). We only used single 64-core processor

of the bare metal instance in our tests.

• Altra: 3.0 GHz Ampere Altra Q80-30 processor (BM.Standard.A1.160 OCI shape). We only used a single

80-Armv8.2-core processor of the bare metal instance in our tests.

Table 5 shows the sustained floating point performance of the conducted runs. All numbers are given in
FP32-TFLOPS. Columns two and three present the performance of Global Time Stepping (GTS), whereas columns
four and five show that of Local Time Stepping (LTS). In general, the LTS configurations have a slightly lower
peak utilization when compared to their GTS counterparts. Note, however, that Table 5 only shows raw floating
point performance and does not account for time-to-solution speedups through LTS (theoretically up to 2.67× in
this case). The performance of GTS and LTS is further split into running a single forward simulation and fusing
multiple simulations. In fused mode, the solver parallelizes over the right-hand-side by concurrently simulating

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

45

seismic wave propagation for a collection of seismic sources. One of the fused mode’s unique advantages is the
opportunity for perfect vectorization of all small matrix multiplications, even when considering sparsity [52]. In
this work we matched the microarchitectures’ SIMD-length by fusing 16 simulations on Cascade Lake and Ice
Lake, eight simulations on Rome and Milan, and four simulations on Altra. Once again, note that Table 5 does not
include the respective sparsity-driven 2.49× increase of the floating point operations’ value when running fused
simulations. Comparing the performance of the different systems, we observe very high overall performance
with architectural efficiency gains originating from decreasing SIMD-lengths. This is especially noticeable when
running single forward simulations. In this case, the vectorized dimension of the small dense matrix kernels
coincides with the number of basis functions, i.e., 𝑀 = 35, which is challenging when optimizing for AVX512
(Cascade Lake and Ice Lake) and AVX2 (Rome and Milan). The short 128-bit ASIMD vector instruction (Altra)
reach a very high peak utilization of 33.2% for GTS and 39.2% in LTS. For the fused simulations, the differences in
relative peak utilization narrow further.

Table 6. Time-to-solution speedups of the studied systems when using different configurations of the solver EDGE. The
performance of the Cascade Lake system, running EDGE with Global Time Stepping (GTS) and a single forward simulation,
is used as baseline. In contrast to Table 5, the speedups include the higher algorithmic efficiencies of EDGE’s support for
Local Time Stepping (LTS) and fused forward simulations.

System

GTS

LTS

Cascade Lake
Ice Lake
Rome
Milan
Altra

single
1.00
1.19
1.11
1.28
1.18

fused single
2.50
1.80
3.02
2.33
2.76
2.48
3.18
2.67
1.69
3.71

fused
4.52
5.87
6.17
6.55
4.64

Table 6 describes the obtained performance numbers in terms of time-to-solution. Here, we use the runtime
of the studied LOH3 setting on Cascade Lake for GTS and a single forward simulation as baseline. All other
settings are given relative to this. Further, for the fused settings, we consider the per-simulation time. We observe
that EDGE’s overall performance is driven by the high floating point performance through the use of TPPs
and the solver’s advanced algorithmic features. Here, Altra performs best for single forward simulations using
LTS, accelerating the baseline by 3.71×. Milan has the best time-to-solution in all other settings and is able to
outperform the baseline by 6.55× when using LTS and fusing simulations. This performance lead originates from
Milan’s high theoretical peak combined with a high peak utilization (see Table 5).

9 RELATED WORK
The related work in terms of the development methodology of DL workloads has been referenced in the in-
troduction, so here we mention community efforts that share the same design philosophy with TPPs. Tensor
Operator Set Architecture (TOSA) is a recent work, concurrently developed with TPPs, that provides a set of
whole-tensor operations commonly employed in DL [54]. TOSA allows users to express directly operators on
up to 4D/5D tensors which are not naturally mapped even on contemporary 2D systolic hardware. We believe
that staying at the 2D primitive level is expressive and sufficient, as we can build higher-order ops with loops
around 2D operators, e.g. see Algorithm 6. Despite the similarities of TPP and TOSA specifications, the TOSA
back-end is reference C code and is not showcased in full DL-workloads. CUTLASS [55] and Triton [56] strive for
high-performance on GPUs, while also offer flexible composition that can be easily applied to solve new problems
related in DL and linear algebra, and share many design principles with TPPs. XLA [57] is a domain-specific

46

• E. Georganas et al.

compiler for linear algebra and DL that targets TensorFlow models with potentially no source code changes.
JAX [58] provides automatic differentiation of Python and NumPy functions, and the compilation of the desired
operators happens in a user-transparent way with JIT calls, yielding optimized XLA kernels. XLA and JAX share
the same philosophy with TPPs: the user is focusing on the DL kernel/workload development using high-level,
platform-agnostic, declarative-style programming, whereas the tensor-aware back-end infrastructure undertakes
the efficient and portable code generation.

Tensor Compilers (TC) [15–18] attempt to optimize DL operators in a platform-agnostics way, however their
applicability is restricted to relatively small code-blocks whereas full workload integration is cumbersome. Also,
TC undertake the tasks of efficient parallelization, loop re-ordering, automatic tiling and layout transformations,
nevertheless the obtained performance is typically underwhelming [12]. We envision that TPPs can be used as a
tool by TC in order to attain efficient platform-specific code generation, therefore TC could focus on optimizing
the higher level aspects of the tensor programs (e.g. layout transformations). Along these lines, TPPs fit in the
MLIR [20] ecosystem/stack as a lowering dialect (see Section 7), and in this way the TPP back-end could be
leveraged by multiple TC frameworks.

10 CONCLUSIONS AND FUTURE WORK
In this work we presented the Tensor Processing Primitives (TPP), a compact, yet versatile set of 2D-tensor
operators, which subsequently can be utilized as building-blocks to construct efficient, portable complex DL
operators on high-dimensional tensors. We also show how TPPs can be used within HPC applications in order
to accelerate tensor computations. We demonstrate the efficacy of our approach using standalone kernels and
end-to-end training DL-workloads (CNNs, dilated convolutions, DLRM, BERT, GNNs) expressed entirely via
TPPs that outperform state-of-the-art implementations on multiple platforms. As future work, we plan to create
a full-fledged TPP-based MLIR dialect such that Tensor Compilers can leverage the strengths of TPPs . Also, we
plan to further enrich the TPP back-end implementation by supporting more ISAs, including GPUs and POWER
architectures.

GLOSSARY
Intel Pseudo Intrinsics

(1) _mm128 Represents a vector of width 128 bits.
(2) _mm128_loadu_ps(addr) Loads 16byte of 32 bit elements.
(3) _mm128_storeu_ps(addr) Stores 16byte of 32 bit elements.
(4) _mm128_unpacklo_ps(A, B) Unpacks and interleaves 32 bit elements from the low half of A and B.
(5) _mm128_unpackhi_ps(A, B) Unpacks and interleaves 32 bit elements from the high half of A and B.
(6) _mm128_unpacklo_pd(A. B) Unpacks and interleaves 64 bit elements from the low half of A and B.
(7) _mm128_unpackhi_pd(A, B) Unpacks and interleaves 64 bit elements from the high half of A and B.
(8) _mm512 Represents a vector of width 512 bits.
(9) _mm512_permutexvar_ps(A,B) Shuffle single precision floating point elements in 512 wide vector length

using indexes specified in B.

(10) _mm512_roundscale_ps(A,B) Round single precision floating point elements to the rounding mode

specified by argument B.

(11) _mm512_sub_ps(A,B) Subtract single precision floating point elements in A from B.
(12) _mm512_scalef_ps(A,B) Scales single precision floating point elements in A using values specified in B.
(13) _mm512_range_ps(A,B, int imm8) Calculates the min, max or absolute max for each single precision-
floating point elements in A and B. Lower 2 bits of imm8[1:0] specifies the operation(min/max/absolute
max) to be performed.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

47

(14) _mm512_xor_ps(A,B) Performs XOR operation between each single precision floating point elements in

A and B vector.

(15) _mm512_and_ps(A,B) Performs AND operation between each single precision floating point elements

in A and B vector.

(16) _mm512_rcp14_ps(A,B) Calculates approximate reciprocal of each single precision floating point element

in range less then 2ˆ-14.

(17) _mm512_cmp_ps_mask(A,B,int C) Compare the single precision elements in A and B specified by the

comparison mode in C.

(18) _mm512_mask_blend_ps(mask A,B,C) Copies single precision floating point element from vector A in

vector C if the corresponding mask bit is set .

(19) _mm512_fmadd_ps(mask A,B,C) Fused-Multiply-Add: Multiplies elements from vector A and B and

adds them to elements of vector C.

(20) _mm512_maskz_loadu_epi16(mask, addr) Loads 64byte of 16bit elements under zero masking from

address addr.

(21) _mm512_set1_epi32( value ) sets a 32 bit value into all 16 entries of the vector, e.g. broadcast.
(22) _mm512_maskz_mov_epi16(mask, A) Moves 16 bit-type register A under zero-masking to a different

register.

(23) _mm512_slli_epi32(A, imm) Shifts all entries in the vector registers (typed as 32 bit elements) by value

imm to the left by shifting 0 in.

Arm Pseudo Intrinsics

(1) vld1q_f32(addr) Loads 16byte of 32 bit elements.
(2) vst1q_f32(addr) Loads 16byte of 32 bit elements.
(3) vtrn1q_f32(A, B) Unpacks and interleaves 32 bit elements from the low half of A and B.
(4) vtrn2q_f32(A, B) Unpacks and interleaves 32 bit elements from the high half of A and B.
(5) vtrn1q_f64(A. B) Unpacks and interleaves 64 bit elements from the low half of A and B.
(6) vtrn2q_f64(A, B) Unpacks and interleaves 64 bit elements from the high half of A and B.
(7) vmax_q(A,B) Calculates the maximum between each single precision floating point elements in A and B

vector.

(8) vmin_q(A,B) Calculates the minimum between each single precision floating point elements in A and B

vector.

(9) vmul_q(A,B) Multiply single precision elements in A and B vector.
(10) vsub_q(A,B) Subtract corresponding single precision elements in B from A.
(11) vadd_q(A,B) Add single precision elements in B and A.
(12) vshlq_u32(A,B) Shift left each single precision elements in A by the value specified in B.
(13) vrndmq_f32(A) Round single precision floating point elements in A using minus infinity rounding mode.
(14) vcvtmq_s32_f32(A) Converts single precision floating point elements in A to signed integers using minus

infinity rounding mode.

(15) float32x4_t Represents 4 single precision floating point elements in vector width of 128.
(16) vand_q(A,B) Performs bit-wise AND operation between A and B vector.
(17) vfmaq_f32(A,B,C) Multiply single precision elements in A and B.Add the intermediate result to C.
(18) vld1q_f32(A) Load a single precision element from scalar to all single precision element in a vector.
(19) vtbl1_u8(A,B) Performs a byte look up operation in vector A using byte addressable indexes specified in

vector B.

48

• E. Georganas et al.

(20) vtbl4_u8(A,B) Performs a 64 byte look up operation in vector A, A+1, A+2, A+3 using byte addressable

indexes specified in vector B.

(21) vbcaxq_s32(A,B) Performs XOR operation between each single precision floating point elements in A

and B vector.

(22) vcgt_q(A,B) Compare corresponding single precision elements in A and B. If B is greater then A the

corresponding bits are set in the destination vector.

(23) vrecpe_f32(A) Calculates approximate reciprocal of each single precision floating point element in vector

A.

(24) vbit_insert(A,B) Copies single precision floating point element from vector A in destination vector if the

corresponding bits are set in vector B.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

49

REFERENCES
[1] Alex Krizhevsky, I. Sutskever, and G.E. Hinton. Image classification with deep convolutional neural networks. Advances in neural

information processing systems, pages 1097–1105, 2012.

[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and
Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 1–9, 2015.

[3] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint

arXiv:1409.1556, 2014.

[4] Dong Yu, Michael L Seltzer, Jinyu Li, Jui-Ting Huang, and Frank Seide. Feature learning in deep neural networks-studies on speech

recognition tasks. arXiv preprint arXiv:1301.3605, 2013.

[5] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei
Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for
Recommender Systems, pages 7–10. ACM, 2016.

[7] Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz,
Joe Davison, Sam Shleifer, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, 2020.

[8] Erik Gawehn, Jan A Hiss, and Gisbert Schneider. Deep learning in drug discovery. Molecular informatics, 35(1):3–14, 2016.
[9] Garrett B Goh, Nathan O Hodas, and Abhinav Vishnu. Deep learning for computational chemistry. Journal of computational chemistry,

38(16):1291–1307, 2017.

[10] Maithra Raghu and Eric Schmidt. A survey of deep learning for scientific discovery. arXiv preprint arXiv:2003.11755, 2020.
[11] Md Zahangir Alom, Tarek M Taha, Chris Yakopcic, Stefan Westberg, Paheding Sidike, Mst Shamima Nasrin, Mahmudul Hasan, Brian C
Van Essen, Abdul AS Awwal, and Vijayan K Asari. A state-of-the-art survey on deep learning theory and architectures. Electronics,
8(3):292, 2019.

[12] Paul Barham and Michael Isard. Machine learning systems are stuck in a rut. In Proceedings of the Workshop on Hot Topics in Operating

Systems, pages 177–183, 2019.

[13] oneDNN. Intel onednn, https://github.com/oneapi-src/oneDNN, Accessed on 3/30/2021.
[14] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn:

Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.

[15] Tim Zerrell and Jeremy Bruestle. Stripe: Tensor compilation via the nested polyhedral model. arXiv preprint arXiv:1903.06498, 2019.
[16] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis
Ceze, et al. {TVM}: An automated end-to-end optimizing compiler for deep learning. In 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), pages 578–594, 2018.

[17] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege,
Andrew Adams, and Albert Cohen. Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions.
arXiv preprint arXiv:1802.04730, 2018.

[18] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen,
et al. Ansor: Generating high-performance tensor programs for deep learning. In 14th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 20), pages 863–879, 2020.

[19] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan, Guangwen Yang, and Depei Qian.
The deep learning compiler: A comprehensive survey. IEEE Transactions on Parallel and Distributed Systems, 32(3):708–727, 2020.

[20] MLIR. Multi-level intermediate representation, https://github.com/tensorflow/mlir, Accessed on 3/30/2021.
[21] Evangelos Georganas, Kunal Banerjee, Dhiraj Kalamkar, Sasikanth Avancha, Anand Venkat, Michael Anderson, Greg Henry, Hans
Pabst, and Alexander Heinecke. Harnessing deep learning via a single building block. In 2020 IEEE International Parallel and Distributed
Processing Symposium (IPDPS), pages 222–233. IEEE, 2020.

[22] Alexander Heinecke, Greg Henry, Maxwell Hutchinson, and Hans Pabst. LIBXSMM: Accelerating small matrix multiplications by
runtime code generation. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and
Analysis, SC ’16, pages 84:1–84:11, Piscataway, NJ, USA, 2016. IEEE Press.

[23] Evangelos Georganas, Sasikanth Avancha, Kunal Banerjee, Dhiraj Kalamkar, Greg Henry, Hans Pabst, and Alexander Heinecke. Anatomy
of high-performance deep learning convolutions on simd architectures. In SC18: International Conference for High Performance Computing,
Networking, Storage and Analysis, pages 830–841. IEEE, 2018.

[24] Bfloat16. Using bfloat16 with tensorflow models, https://cloud.google.com/tpu/docs/bfloat16, Accessed on 4/3/2019.

50

• E. Georganas et al.

[25] George Marsaglia et al. Xorshift rngs. Journal of Statistical Software, 8(14):1–6, 2003.
[26] Kunal Banerjee, Evangelos Georganas, Dhiraj D Kalamkar, Barukh Ziv, Eden Segal, Cristina Anderson, and Alexander Heinecke.

Optimizing deep learning rnn topologies on intel architecture. Supercomputing Frontiers and Innovations, 6(3):64–85, 2019.

[27] Intel-ISA. Intel architecture instruction set extensions and future features programming reference, https://software.intel.com/content/

dam/develop/public/us/en/documents/architecture-instruction-set-extensions-programming-reference.pdf, Accessed on 3/30/2021.

[28] Michael James David Powell. Approximation theory and methods. Cambridge university press, 1981.
[29] Chebyshev-Polynomials. Chebyshev polynomials, https://en.wikipedia.org/wiki/Chebyshev_polynomials, Accessed: 2021-09-26.
[30] Philippe Flajolet, Jean-Claude Raoult, and Jean Vuillemin. The number of registers required for evaluating arithmetic expressions.

Theoretical Computer Science, 9(1):99–125, 1979.

[31] J Willard Gibbs. Elementary principles in statistical mechanics. Courier Corporation, 2014.
[32] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In

International conference on machine learning, pages 448–456. PMLR, 2015.

[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE

conference on computer vision and pattern recognition, pages 770–778, 2016.

[34] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[35] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018.
[36] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv

preprint arXiv:1607.08022, 2016.

[37] Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping Chen, Mikhail Shiryaev, and Alexander Heinecke. Optimizing
deep learning recommender systems training on cpu cluster architectures. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis, pages 1–15. IEEE, 2020.

[38] Avantika Lal, Zachary D. Chiang, Nikolai Yakovenko, Fabiana M. Duarte, Johnny Israeli, and Jason D. Buenrostro. Atacworks: A deep

convolutional neural network toolkit for epigenomics. bioRxiv, 2019.

[39] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang,
Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation
systems. arXiv preprint arXiv:1906.00091, 2019.

[40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language

understanding. arXiv preprint arXiv:1810.04805, 2018.

[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃľmi Louf,
Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,
Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online,
October 2020. Association for Computational Linguistics.

[42] Minjia Zhang, Samyam Rajbhandari, Wenhan Wang, and Yuxiong He. Deepcpu: Serving rnn-based deep learning models 10x faster. In

2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18), pages 951–965, 2018.

[43] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. arXiv preprint arXiv:1706.02216,

2017.

[44] Sasikanth Avancha, Vasimuddin Md, Sanchit Misra, and Ramanarayan Mohanty. Deep graph library optimizations for intel (r) x86

architecture. arXiv preprint arXiv:2007.06354, 2020.

[45] oneDNN Fugaku. A deep dive into a deep learning library for the a64fx fugaku cpu - the development story in the developer’s own

words, https://blog.fltech.dev/entry/2020/11/19/fugaku-onednn-deep-dive-en, Accessed on 4/9/2021.
[46] Hugging-Faces. Hugging faces, https://github.com/huggingface/transformers, Accessed on 4/9/2021.
[47] oneCCL. Intel oneccl, https://github.com/oneapi-src/oneCCL, Accessed on 3/30/2021.
[48] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks.

In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017.

[49] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 6299–6308, 2017.

[50] Thomas D. KÃĳhne, Marcella Iannuzzi, Mauro Del Ben, Vladimir V. Rybkin, Patrick Seewald, Frederick Stein, Teodoro Laino, Rustam Z.
Khaliullin, Ole SchÃĳtt, Florian Schiffmann, Dorothea Golze, Jan Wilhelm, Sergey Chulkov, Mohammad Hossein Bani-Hashemian,
ValÃľry Weber, Urban BorÅątnik, Mathieu Taillefumier, Alice Shoshana Jakobovits, Alfio Lazzaro, Hans Pabst, Tiziano MÃĳller, Robert
Schade, Manuel Guidon, Samuel Andermatt, Nico Holmberg, Gregory K. Schenter, Anna Hehn, Augustin Bussy, Fabian Belleflamme,
Gloria Tabacchi, Andreas GlÃűÃ§, Michael Lass, Iain Bethune, Christopher J. Mundy, Christian Plessl, Matt Watkins, Joost VandeVondele,
Matthias Krack, and JÃĳrg Hutter. Cp2k: An electronic structure and molecular dynamics software package - quickstep: Efficient and
accurate electronic structure calculations. The Journal of Chemical Physics, 152(19):194103, 2020.

[51] Ilia Sivkov, Patrick Seewald, Alfio Lazzaro, and Jürg Hutter. DBCSR: A blocked sparse tensor algebra library. CoRR, abs/1910.13555, 2019.

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning & HPC Workloads

•

51

[52] Alexander Breuer, Alexander Heinecke, and Yifeng Cui. Edge: Extreme scale fused seismic simulations with the discontinuous galerkin
method. In Julian M. Kunkel, Rio Yokota, Pavan Balaji, and David Keyes, editors, High Performance Computing, pages 41–60, Cham, 2017.
Springer International Publishing.

[53] Alexander Breuer and Alexander Heinecke. Next-generation local time stepping for the ader-dg finite element method (submitted to

ipdps21). 2021.

[54] TOSA. Tosa, https://developer.mlplatform.org/w/tosa/, Accessed on 3/30/2021.
[55] CUTLASS. Nvidia cutlass, https://github.com/NVIDIA/cutlass, Accessed on 3/30/2021.
[56] Philippe Tillet, HT Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In
Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10–19, 2019.

[57] XLA. Xla: Optimizing compiler for machine learning, https://www.tensorflow.org/xla, Accessed on 3/30/2021.
[58] JAX. Jax: Autograd and xla, https://github.com/google/jax, Accessed on 3/30/2021.

Optimization Notice: Software and workloads used in performance tests may have been optimized for performance only on Intel microprocessors. Performance
tests, such as SYSmark and MobileMark, are measured using specific computer systems, components, software, operations and functions. Any change to any
of those factors may cause the results to vary. You should consult other information and performance tests to assist you in fully evaluating your contemplated
purchases, including the performance of that product when combined with other products. For more information go to http://www.intel.com/performance.
Intel, Xeon, and Intel Xeon Phi are trademarks of Intel Corporation in the U.S. and/or other

