NNReArch: A Tensor Program Scheduling Framework
Against Neural Network Architecture Reverse Engineering

Yukui Luo, Shijin Duan, Cheng Gongye, Yunsi Fei, and Xiaolin Xu
Department of Electrical and Computer Engineering
Northeastern University, Boston, MA, USA

2
2
0
2

r
a

M
2
2

]

R
C
.
s
c
[

1
v
6
4
0
2
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—Architecture reverse engineering has become an
emerging attack against deep neural network (DNN) implemen-
tations. Several prior works have utilized side-channel leakage to
recover the model architecture while the target is executing on a
hardware acceleration platform. In this work, we target an open-
source deep-learning accelerator, Versatile Tensor Accelerator
(VTA), and utilize electromagnetic (EM) side-channel leakage to
comprehensively learn the association between DNN architecture
conﬁgurations and EM emanations. We also consider the holistic
system – including the low-level tensor program code of the VTA
accelerator on a Xilinx FPGA, and explore the effect of such low-
level conﬁgurations on the EM leakage. Our study demonstrates
that both the optimization and conﬁguration of tensor programs
will affect the EM side-channel leakage.

Gaining knowledge of the association between the low-level
tensor program and the EM emanations, we propose NNReArch,
a lightweight tensor program scheduling framework against
side-channel-based DNN model architecture reverse engineering.
Speciﬁcally, NNReArch targets reshaping the EM traces of
different DNN operators, through scheduling the tensor program
execution of the DNN model so as to confuse the adversary.
NNReArch is a comprehensive protection framework supporting
two modes, a balanced mode that strikes a balance between the
DNN model conﬁdentiality and execution performance, and a
secure mode where the most secure setting is chosen. We imple-
ment and evaluate the proposed framework on the open-source
VTA with state-of-the-art DNN architectures. The experimental
results demonstrate that NNReArch can efﬁciently enhance the
model architecture security with a small performance overhead.
In addition, the proposed obfuscation technique makes reverse
engineering of the DNN architecture signiﬁcantly harder.

I. INTRODUCTION

Neural network (NN) has found important use in differ-
ent application domains, such as object detection, big data
analytic, and semantic recognition. To improve the infer-
ence capability of NN models, deep neural network (DNN)
is proposed, which employs larger model size for tackling
more complicated tasks. Although DNNs are demonstrated as
promising for different tasks, their large model sizes become a
bottleneck for performance and applicability. To mitigate these
issues, different methods have been proposed to accelerate the
execution of DNN models [1–5]. For example, modern FPGAs
have been widely deployed to provide acceleration for DNNs
on both edge devices and cloud infrastructures.

A DNN model can be represented as a directed acyclic
graph (DAG) composed of operation nodes and connec-
tions, thus its implementation can be mapped accordingly to
FPGA hardware components (e.g., look-up-table and DSP)
with FPGA-DNN development tools. The most commonly

used frameworks are the Xilinx deep learning processor unit
(DPU) [2], and the open-source versatile tensor accelera-
tor (VTA) [6], which can provide end-to-end optimization
for FPGA-DNN implementation. Although such FPGA-based
DNN acceleration framework provide signiﬁcant performance
improvement, they also create a new attack surface, where an
adversary can either manipulate the inference of DNN models
[7, 8], or illegally extract (i.e., using side-channel analysis)
the critical parameters of a DNN model, such as its archi-
tecture [9, 10]. Since the construction of high-performance
DNN models involves expensive data collection and training
procedures, thus their model parameters like the architecture
should be highly protected.

This paper studies the vulnerability of DNN model architec-
ture against side-channel-based DNN model extraction attack
on FPGA accelerators. Speciﬁcally, we explore the association
between electromagnetic (EM) side-channel leakage of DNN
model implementation on FPGAs. To draw generic conclu-
sions, we use state-of-the-art open-source FPGA acceleration
framework, VTA [11], and explore the internal causes of
DNN model architecture-relevant side-channel leakage from
the low-level program code. Correspondingly, we propose
defense solutions by rescheduling the DNN operator-level
tensor program and obfuscating the side-channel leakage. Note
that although this paper mainly discusses EM side-channel
the presented experimental observation and the
and VTA,
proposed methodologies can be generally extended to other
side-channels and hardware platforms.

The main contributions of this work are as follows:
• We comprehensively study the EM side-channel leakage
for DNN models deployed with VTA. To the best of our
knowledge, this is the ﬁrst work exploring the association
between low-level tensor program code and EM trace,
from the adversarial perspective.

• We systematically formulate the mathematical represen-
tation of the DNN architecture with EM emanations, fol-
lowing which we further propose security metrics for the
DNN models based on the visualized EM characteristics.
These metrics can be used to estimate the security level
of a DNN architecture against side-channel attacks.

• We present NNReArch, a ﬂexible defense framework
for DNN model architectures against EM side-channel
attacks. We evaluate the performance of NNReArch with
state-of-the-art DNN architectures, and we demonstrate
that NNReArch can signiﬁcantly complex the reverse

 
 
 
 
 
 
engineering attacks, i.e., doubling the attacking efforts
with only 3.06% performance overhead.

II. BACKGROUND AND RELATED WORK

A. Versatile Tensor Accelerator (VTA)

VTA is an open-source, generic, and FPGA-speciﬁc deep
learning acceleration framework [6], consisting of four mod-
ules to enable task-level parallelism in a pipelining fashion
(TLPP): a “fetch module” that loads the instruction stream
from the DRAM, a “load module” loading the input, weight
parameters, and intermediate results, a “store module” writing
back intermediate results, and the “compute module” accel-
erates computing. The last one is “compute module” that
relies on two kernels, the general matrix multiply (GEMM)
kernel for dense linear algebra computations, and the tensor
arithmetic logic unit (ALU) for general computing tasks. VTA
is supported by TVM [3], a compiler (AutoTVM) scheduling
the target deep learning application on a hardware platform.
In addition, two techniques, matrix multiplication blocking
(MMB) and virtual threading (VT), are used to customize the
FPGA execution to further improve the VTA performance. The
MMB technology divides large NN operators down to smaller
blocks to ﬁt the GEMM kernel, and VT manages the hardware
resources of VTA to facilitate simultaneous computing and
memory access. More technical details of the VTA can be
found in Sec. III.

B. Model Architecture Extraction Attack and Defense

Model architecture extraction has become an emerging
threat to the security of DNN. In addition to attacks from the
software side [12], different side-channels have been utilized
to extract DNN architectures on hardware platforms. Batina
et al. [13] ﬁrst demonstrated architecture extraction of multi-
layer perception (MLP) using the EM signals from both AVR
and ARM processors. In [9], Yu et al. applied EM-based
attack on an FPGA against a convolutional neural network
(CNN). Tian et al. [14] utilized an on-chip time-to-digital
(TDC) sensor to extract the NN architecture, which can be
launched remotely on a multi-tenant FPGA, but with lower
resolutions compared to EM signals. Hu et al. [15] demon-
strated extracting a complete NN architecture using multiple
side-channels of GPUs, such as the memory access pattern.
However, none of the prior works targets comprehensively
studying the tensor programmable accelerator, such as VTA.
Also, defense methods are still in their infancy.

C. EM Side-channel

[13]

EM side-channel

is an effective and contact-less
method for extracting sensitive information during the system
execution. The instantaneous EM emanation is dependent on
the dynamic current [9], as shown in Eq. 1:

Idyn(t) =

C × VDD × fclk × D(t)
2

(1)

where C denotes the capacitance of the activated metal nets,
D(t) represents the transition rate of the nets (determined by
both operations and data), VDD is the voltage supply, and fclk

Fig. 1: Experimental Platform for EM Signal Collection

stands for the execution clock frequency [16]. Therefore, an
EM trace from a hardware platform well embodies information
for its workload, data, and system computing and communi-
cation.

III. ASSOCIATING TENSOR PROGRAM ON VTA WITH EM
EMANATION

A. Threat model

We follow the same threat model as in other related side-
channel model extraction works [9, 13], with our victim
device being an edge FPGA running an VTA accelerator for
a pre-trained DNN model. The attacker is able to obtain the
EM signals of the victim device with specialized equipment
and also knows the model execution status, which can assist
in reasoning the architecture of the victim DNN model.
We assume a strong attacker, who has sufﬁcient knowledge
of the target accelerator, including the conﬁguration of the
accelerator (discussed in Sec. III-C2). The executing model
architecture is the target for reverse engineering with all the
EM traces, accelerator, and platform information.

B. Experimental Platform

We build an experimental platform using PYNQ-Z1 devel-
opment kit, an SoC with a Xilinx Zynq-7000 device and a
dual-core ARM Cortex-A9 processor (PS). To align the EM
signals with execution phases for the device characterization
purpose, we modify the VTA bitstream as shown in Fig.1 (a).
Speciﬁcally, we use two signals as the trigger signals to mark
the starting and ending of the VTA execution (and therefore
the corresponding EM segment): the general-purpose output
(GPO) manager write valid of the PS, M AXI GPO WVALID,
and the accelerator coherence port (ACP) subordinate write
valid of the VTA core, S AXI ACP WVALID. The M AXI -
GPO WVALID signal annotates the opcodes and data that have
been written into the VTA queues, and S AXI ACP WVALID
indicates that the VTA core output is valid to be written back
to the DRAM of the PYNQ-Z1 system.

Our EM trace collection setup includes an EM Probe
PBS2 [17] converting the EM signals into voltage represen-

ZYNQPS sideMSVTA coreSMTrigger signalsM_AXI_GPO_WVALIDS_AXI_ACP_WVALIDEM leakage from VTA coreEM leakage from Zynq PS side (a) VTA bitstream diagram(b) EM leakage and trigger signals(a) Conv2D-Opt:[1, 1, 1, 1, 1]. The baseline
w/o optimization.

(b) Conv2D-Opt:[2, 1, 1, 1, 1]. Blocks along
the input channel (icb = 2).

(c) Conv2D-Opt:[1, 2, 1, 1, 1]. Blocks along
the output channel (ocb = 2).

(d) Conv2D-Opt:[1, 1, 2, 1, 1]. Blocks along
the input feature height (f ihb = 2).

(e) Conv2D-Opt:[1, 1, 1, 2, 1]. Blocks along
the input feature width (f iwb = 2).

(f) Conv2D-Opt:[1, 1, 1, 1, 2]. Apply the
virtual threading (vt = 2).

Fig. 2: We use Conv2D-Wop:[256, 256, 3, 14, 14] as an example to show how the EM leakages change with the Opt-conﬁg
[icb, ocb, f ihb, f iwb, vt].

tations, an Aronia AG pre-ampliﬁer, and a Lecroy oscillo-
scope [18]. An EM trace example with the sampling rate
of 1GHz is shown in Fig. 1 (b), which is averaged from
50 measurements with the same inputs. The average pre-
processing method helps to make the EM trace stable and
ﬁlter the measurement noise.

C. Terminology and Deﬁnitions

With the EM measurement setup ﬁxed,

there are three
other factors that jointly determine the EM trace measurement
of a VTA: (1) The workload conﬁguration of the current
DNN layer’s operation, namely Wop-conﬁg; (2) The global
conﬁguration of the VTA-core, namely VTA-conﬁg; (3) To
take advantage of the FPGA parallelism and ARM multi-
thread scheduling, operators (resources) for a DNN layer are
optimized, whose conﬁguration is denoted as Opt-conﬁg.

1) Wop-conﬁg: For the most commonly used DNN ar-
layer (Conv2D) is the most
chitectures, 2D convolutional
critical component to construct the entire model architecture.
A Conv2D is speciﬁed by the number of input channels (IC),
output channels (OC), the kernel size (K), the input feature
size (F I), and the output feature size (F O). We follow the typ-
ical regulation that assumes the input/output feature is square-
shaped. The Conv2D-Wop is therefore [IC, OC, K, F I, F O].
2) VTA-conﬁg: A VTA-core is deployed on an FPGA
speciﬁed by VTA-conﬁg, the conﬁguration from [11]. The
main computing component GEMM kernel, is designed around
a tensor core performing one matrix-matrix operation in each
clock cycle. This operator is to implement the product of
a 1 × 16 input and a 16 × 16 weight matrix. The VTA
core employs hardware resources for parallel computation to
achieve high performance. The input matrix has dimension
of BAT CH × BLOCK IN , where BAT CH indicates how

many feature maps can be implemented in parallel, by the VTA
core (BAT CH = 1 by default), and BLOCK IN represents
the input channel-parallelism. For example, in our experi-
mental, BLOCK IN is set as 16, indicating that 16 input
channels can execute in parallel. The weight matrix includes
BLOCK IN × BLOCK OU T number of weights, where
BLOCK OU T represents the output channel-parallelism.
When BLOCK OU T is 16, the VTA can produce results
in 16 output channels (output BAT CH × BLOCK OU T ).
Another setup of VTA-conﬁg is the sizes of on-chip buffers,
including input, output, and weight buffer with 32KB, 128KB,
and 32KB memory sizes, respectively.

3) Opt-conﬁg: Rather than static execution, the VTA can
dynamically schedule the execution of Conv2D layers for
performance optimization. AutoTVM supports VTA to imple-
ment explicit memory latency hiding by the virtual threading
(vt) primitive, corresponding to multi-threading of the ARM
processor. As our used ARM Cortex A9 dual-core processor
allows two threads, the VTA vt can support threads up to 2. To
map the matrix multiplication efﬁciently on a VTA core, TVM
can optimally break down large workload as smaller blocks,
to achieve computation efﬁciency within limited hardware
resources. There are four scales of blocks associated with
this technology: input channel blocks (icb), output channel
blocks (ocb), and two input feature map blocks along the
height axis (f ihb) and width axis (f iwb), respectively. We
use an Opt-conﬁg vector [icb, ocb, f ihb, f iwb, vt] to represent
the optimization setting. For example, a Conv2D-Opt of [2, 2,
2, 2, 2] is applied on a convolution layer with Con2D-Wop:
[256, 256, 3, 14, 14]. The scheduler will divide the original
convolution layer into several small blocks with workload
Conv2Db-Wop:[128, 128, 3, 14, 14] because both the input
(icb) and output channel (ocb) blocks are 2. It will also separate

0   2   4   6   8  10 12 14         106   110  ………………16 𝐶𝑒𝑥and 16 𝑆, ~11.25ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)0  2   4   6   8  10 12 14         122   126  ………………32 𝐶𝑒𝑥and 32 𝑆, ~12.8ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)0  2   4   6   8  10 12         88      92  ………………8𝐶𝑒𝑥and 8𝑆, ~9.6ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)0  2   4   6   8  10 12         124    126  ………………32 𝐶𝑒𝑥and 32 𝑆, ~13.2ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)0  2   4   6   8  10 12         142    146  ………………32 𝐶𝑒𝑥and 32 𝑆, ~15.0ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)0  2   4   6   8  10 12         56      60………………8𝐶𝑒𝑥and 8𝑆, ~6.5ms𝐶𝑒𝑥𝑆100500-50100×105𝑤𝑐Time (ns)Amplitude (mv)Similarly, blocks along the feature map height and width also
affect M and wc of the EM trace, as shown in Fig. 2d and
Fig. 2e, although no obvious difference between these two EM
traces can be observed. Following our measurement results,
blocking along the width of the feature map (f iwb) induces
a longer S. Besides, we applied the virtual threading method,
which accelerates the operator by hiding the DRAM memory
access latency and enables the TLPP of VTA as mentioned in
Sec.II-A. As shown in Fig. 2f, this conﬁguration shortens the
execution time of the entire Conv2D layer compared with the
baseline by reducing the M .

From the EM leakage observation, we can draw the follow-
ing conclusions: (1) M is a function of IC, icb, OC, ocb, F O,
F I, f ihb, f iwb, and vt; (2) N is a function of IC, icb; and
(3) wc is a function of OC, ocb, F I, f ihb, and f iwb.

E. Low-level Program Code Analysis

Visually inspecting the EM traces derives general associ-
ation between the EM pattern and the two conﬁgurations,
Wop-conﬁg and Opt-conﬁg. To comprehensively understand
the execution impacts on the EM leakage, we look into the
low-level code structure shown in Fig 4. Since the Tensor ALU
operators of a Conv2D layer have low-arithmetic intensity and
therefore do not emanate high EM leakage, we focus on the
GEMM operator [6]. The GEMM code is composed by many
nested loops of operations, corresponding to the repetitive
EM pattern shown in Fig 2. We extract three parts (part 1
to 3 as shown in Fig 4) related to the Conv2DEM function
parameters, M , N , and wc, respectively. In Part 1, there are
four outer loops related to M , and their ranges indicate the
blocking parameters: icb, f ihb, f iwb and vt. Note that the part
1 program is the most outer loop, the function of M is also
determined by several other parameters, as deﬁned in Eq. 3:

M =

IC × icb × F O × f ihb × f iwb
BLOCK OU T × F I × ocb × vt

(3)

It is straightforward to determine N from the range of

ic.outer of the Part 2 code:

N =

IC
BLOCK IN × icb

(4)

Different from M and N that are discrete (integer) numbers,
wc is associated with the execution time. Hence, without
knowing the exact function, we can only leverage the Part
3 code to determine which parameters affect its quantity. In
the ﬁrst cthread.s_1 loop, if its range is larger than 1, it
will enable the TLPP. Our experiments suggest that the range
of dx is equal to K, i.e., the kernel size. If ocb = 1, the
range of dy is also 1, otherwise it is K. The range of j is a
function of F I, f iwb, and F O. Putting all these clues together,
we assume function g(·) can obtain wc from low-level tensor
program code in Eq. 5.

wc = g(K, F I, f iwb, F O, vt, fex, II)

(5)

The TLPP is conﬁgured by vt, II denotes the initiation
interval for the pipeline, and fex is the executing frequency
of the VTA core, which is 100M Hz in this paper.

Fig. 3: The scheduler divides a large Conv2D layer down to
smaller blocks along the IC and OC axes. (a) w/o optimization.
(b) Conv2D-Opt: [2, 1, 1, 1, 1], icb = 2. (c) Conv2D-Opt: [1,
2, 1, 1, 1], ocb = 2.

the 14 × 14 input feature map into 7 × 7 blocks because
f ihb and f iwb are also 2. Moreover, the Opt-conﬁg enables
dual-threading. In contrast, the non-optimized Opt-conﬁg is
Conv2D-Opt:[1, 1, 1, 1, 1]. Note that across this paper, we
use the default BAT CH setting, and the subscript b is used
to represent the detailed value of each parameter out of many
possibilities.

D. EM Leakage Observation

In our experiment, we implement a convolutional layer with
Conv2D-Wop of [256, 256, 3, 14, 14]. We choose different
Opt-conﬁgs to understand the impact of optimizations on DNN
execution, which is reﬂected in the EM leakage. Fig. 2 shows
the EM traces collected from the basic VTA setting without
optimization (Fig. 2a) and ﬁve optimized versions (Fig. 2b
to 2f). Inspecting these traces, we ﬁnd a repetitive pattern of
a segment of high-frequency activity (continuous execution,
Cex) followed by a segment of low-frequency (stalling) ac-
tivity (S). This pattern repeats M times for the convolutional
layer computation. Further, from the beginning Cex segment,
we can clearly observe several spikes, the number (N ) of
which is countable and each of them has approximately the
same width (wc), where Cex = N × wc. Thus, we can derive
a Conv2D EM trace function (Conv2DEM ) with 2 countable
parameters: M and N , and 2 measurable parameters: wc and
S.

Conv2DEM = M × (N × wc + S)

(2)

When we change the icb along the IC and ocb along the OC
of the Opt-conﬁg, the computing ﬂow for each axis’ blocks is
shown in Fig. 3, and their corresponding EM traces are shown
in Fig. 2b and Fig 2c. If dividing the Conv2D layer along
the IC axis, it will generate icb subordinated outputs OCi,
whose summation will derive the ﬁnal result. When icb = 2,
two paths are scheduled and the execution time of Cex is
reduced by almost a half. Blocks along the OC axis has a
simpler computational process. The scheduler separates OC
output channels into ocb sections, and the ﬁnal result is the
concatenation of those sub-results. Comparing Fig. 2a with
Fig. 2b and Fig. 2c, the IC axis blocking determines M
and N , and the OC axis blocking determines M and wc.

Input feature maps (𝐹𝐼×𝐹𝐼×𝐼𝐶)Input channelsOutput channelsresult(a)(b)(c)𝑂𝐶𝑂𝐶𝐼𝐶𝐼𝐶2𝐼𝐶2𝐼𝐶𝑂𝐶1𝑂𝐶2𝑂𝐶2𝑂𝐶2𝑂𝐶𝑂𝐶Fig. 4: Low-level code summary for the optimizable and high-arithmetic intensity GEMM operator, which constructs the
Conv2D layer with low-arithmetic intensity ALU.

(a) Conv2Do-Wop:[256, 256, 3, 14, 14], Conv2Do-Opt:[1, 2, 1, 1, 2].

(b) Conv2Dt-Wop:[128, 128, 3, 28, 28], Conv2Dt-Opt:[1, 4, 2, 2, 2].

Fig. 5: Conv2D EM obfuscation example. We use the subscript
o to represent the original Conv2D layer, and subscript t is the
target obfuscation Conv2D layer.

F. EM Obfuscation

With the EM leakage characterization and the low-level
code analysis, we propose to obfuscate the EM trace by
scheduling the tensor program. As a proof of concept, we
implement two different Conv2D layers: one Conv2Do with
the Wop-conﬁg of [256, 256, 3, 14, 14] and Opt-conﬁg of[1,
2, 1, 1, 2]; the other Conv2Dt with the Wop-conﬁg of [128,
128, 3, 28, 28] and Opt-conﬁg of [1, 4, 2, 2, 2]. Inspecting
their EM traces in Fig. 5, we notice these two layers can
generate similar Cex, because their GEMM operators have
equal counting result (OpCGEMM ) derived by Eq. 6.

OpCGEMM =

IC × OC × K 2 × F O2
BLOCK IN × BLOCK OU T

(6)

the stall

However,

time is different between these two
settings (So and St). Such difference can be canceled by
adding pause opcode to delay ∼ 0.35ms in every So, so
that S(cid:48)
o = St. As a result, these two EM traces become in-

distinguishable( i.e., unable to determine which setting is in
effect).

Generally, the goal of EM obfuscation for a Conv2D layer is
to ﬁnd different conﬁgurations resulting in the same operation
counts as the original one, following Eq. 6. Speciﬁcally, this
equation can assist us to ﬁnd a target Wop-conﬁg Conv2Dt −
W op, for which there exists an Opt-conﬁg Conv2Dt − Opt
satisfying Mt = Mo, Ko = Kt, and has a longer execution
time. Then we can derive ∆S = St − So by measurement,
and apply it to the original workload to mimic it as the target
workload.

IV. SECURITY METRICS AND OPTIMIZATION PROGRAM
FOR VTA IMPLEMENTATION

This section introduces NNReArch, which utilizes Opt-
conﬁg and EM obfuscation to mitigate the EM leakage of the
DNN model. For an attacker to reverse-engineer the victim
NN architecture implemented on VTA, s/he needs to derive
VTA-conﬁg and Opt-conﬁg. If Opt-conﬁg is ﬁxed, such as
Conv2D-Opt of [1, 1, 1, 1, 1], then the victim architecture is
easy to extract. Thus, a primary idea of mitigating the EM
side-channel leakage is to increase the searching space of the
Opt-conﬁg for each Conv2D layer.

A designer can formulate the scheduling of DNN execution
as an optimization problem. For a given neural network
architecture N N , we can extract a set of workload expression
E that executes on a target acceleration device. Then, for a
given workload e ∈ E, we can implement it with many differ-
ent functionally equivalent low-level program codes inducing
different EM traces, as observed in Sec. III-D and III-E. There-
fore, each workload could have multiple equivalent schedules,
i.e., Opt-conﬁg. We use P se to denote the possible schedule
space for e. For example, in VGG-19, there are 9 types of
Conv2D layers with different Wop-conﬁgs, each of whic is
denoted as ei, i ∈ [1, 9] and has a set of Opt-conﬁg P sei .

attr= {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}buffers = {res: Buffer(res_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),data: Buffer(data_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),kernel: Buffer(kernel_2: Pointer(int8), int8, [16, 16, 3, 3, 16, 16], [])}buffer_map= {data_1: data, kernel_1: kernel, res_1: res} {for (i1.outer.outer: int32, 0, 16) {for (i2.outer: int32, 0, 2) {for (i3.outer: int32, 0, 2) {for (cthread.s: int32, 0, 2) {load VTAPushGEMMOpstream to VTA queue...}}}for (ic.outer: int32, 0, 16) {Calculate the DRAM memory address and load data by VTALoadBuffer2D...}... may have additional same code blocks depending on the cthread.srangefor (cthread.s_1: int32, 0, 2) {VTAUopLoopBegin...for (dy: int32, 0, 3) {for (dx: int32, 0, 3) {for (j: int32, 0, 14) {Execute the computation…}}}... VTAUopLoopEnd}}}}}Describe the Conv2D workloadPart 1i2.outer appear if enable 𝑓𝑖ℎ𝑏i3.outer appear if enable 𝑓𝑖𝑤𝑏The range of cthread.s> 1 if enable 𝑣𝑡Related to 𝑤𝑐The range of cthread.s_1 > 1 if enable 𝑣𝑡dyappear if enable 𝑜𝑐𝑏Related to MPart 2Part 3Related to N100500-50-10002468Time (ns)Amplitude (mv)×106𝑆𝑜𝐶𝑒𝑥,𝑜100500-50-100Time (ns)Amplitude (mv)02468𝐶𝑒𝑥,𝑡𝑆𝑡× 106A. Security Metrics

When considering conﬁdentiality of a N N , brute force
attack is the most generic method and its complexity can be
represented by the size of its searching space (SSN N deﬁned
in Eq. 10). The attacker normally progresses sequentially,
i.e., from the ﬁrst Conv2D layer to the following layers,
since s/he has to utilize the results (e.g., dimensions) of the
previous layers. IC and F I of the ﬁrst layer can be directly
observed from the input image and global conﬁguration of
the accelerator. For the Wop-conﬁg of each Conv2D layer, the
adversary could build a library of combinations of Wop-conﬁg
and Opt-conﬁg, and then estimate their EM trace patterns. The
candidate with high similarity to the observed EM trace of the
target NN could be considered as the correct hypothesis with
high conﬁdence.

the

IC, F I

assume

Pooling

layer. Other

generally we

1) Search space for

parameters,
remain

individual Conv2D layers: For
a Conv2D layer,
are
derived from the previous Conv2D layer’s OC, F O,
including
or
{K, F O, OC, f ihb, f iwb, icb, ocb, vt},
be
to
discovered. Some of these parameters follow some conventions
that can be used as hints for guessing. Hint 1: The K of
the 1st Conv2D layer might be 3, 5, or 7, and the K
of the rest Conv2D layers might be 1 or 3. Hint 2: F O
depends on the F I and the stride of the kernel, which is
normally 1 or 2. When F I is smaller than 8,
the stride
will be 1. Hint 3: OC depends on IC and BLOCK IN ,
where IC is expected as a multiple of BLOCK IN .
An exception is that the 1st Conv2D layer usually has an
IC smaller than BLOCK IN , so the VTA will convert
to IC = BLOCK IN with dummy input channels.
it
If representing the relationship between OC and IC as
OC = foc × IC, then foc ∈ { 1
2 , 1, 2, 4}. Note that foc = 1
4 , 1
and 1
2 do not happen when IC = BLOCK IN , and foc = 1
4
do not occur when IC = 2 × BLOCK IN . Hence, the
searching space (SS) for K, F O, OC are
(cid:26) 3
2
(cid:26) 1
2
3
4
5

, i = 1
, i > 1
, F I < 8
, Otherwise
, IC = BLOCK IN
, IC = 2 × BLOCK IN
, Otherwise

SSOC =

SSF O =

SSK =




(7)



4

Following Eq. 7, an attacker can formulate the searching
space for the potential Wop-conifgs of Conv2Di. For a
speciﬁc Wop-conﬁg, the size of its Opt-conﬁg searching space
can be derived from Eq. 8, again we use the subscript b to
represent the detailed value of each parameter out of many
possibilities.


(cid:7)

SSf ihb = SSf iwb = (cid:6)log2
SSicb = (cid:6)log2
SSocb|OC = (cid:6)log2

IC
BLOCK IN

F I
4

(cid:7) + 1

OC
BLOCK OU T

SSvt = max(vt)




Assuming SSc = SSK ×SSF O×SSf ihb ×SSf iwb ×SSicb ×
SSvt, the searching space SSConv2Di of Conv2Di can be
derived using Eq. 9:

SSConv2Di = SSc ×

SSocb|OC

(9)

(cid:88)

OC∈ΩOC

where ΩOC denotes the corresponding values in the searching
space of OC. For example, as shown in Tab. I, the 2nd Conv2D
layer has IC = 64, thus SSOC = 5 (BLOCK IN and
BLOCK OU T are set as 16 in Sec. III-C2), and the speciﬁc
values ΩOC = {16, 32, 64, 128, 256} with possible SSocb|OC
is 1, 2, 3, 4, 5, respectively. Considering the derivation
SSc = 864 for the 2nd Conv2D layer in VGG-19, thus the
searching space for this layer is SSConv2D2 = 12960.

2) Searching space for the entire neural network: An
attacker needs to iterate all possible Wop-conﬁgs and their
Opt-conﬁgs, to compare the guessed EM leakage with the
obtained NN EM leakage starting from the 1st Conv2D layer.
We can narrow the search space based on two facts: (1)
Multiple Conv2D layers exist in one NN with the same Wop-
conﬁg; (2) A speciﬁc Opt-conﬁg may be suitable for different
Wop-conﬁgs to execute effectively. A strong and practical
scenario is that an attacker only tries to utilize the prior
knowledge before s/he really has to search the entire space.
For example, an attacker is always applying the recovered
Wop-conﬁg and Opt-conﬁg of the previous layer ﬁrst. In
other words, s/he has to checks all these already-discovered
Wop-conﬁg (SGwop) and Opt-conﬁg sets (SGopt), only if the
previous knowledge is not working. When both tests fail, s/he
iterates other possible conﬁgurations. We show an example in
Tab. I column AutoTVM Opt-conﬁg, where e3 and e4 employ
the same Opt-conﬁg. We calculate the searching space for each
Conv2D layers and derive the N N searching space SSN N ,
which also represents the basic security level.

l
(cid:88)

SSN N =

SSConv2Di

(10)

i=1

2

3) EM obfuscation scheme: For defense, one can use EM
obfuscation. In details, a designer can follow Eq. 6 to calculate
the OpCGEMM in each Conv2D layer and ﬁnd all potential
EM obfuscation Wop-conﬁgs and Opt-conﬁg, using Eq. 3
and 4. According the combination theory, the best choice of
(cid:7), where l is the number
layers to apply EM obfuscation is (cid:6) l
total layers. Therefore, the designer can randomly select 8 lay-
ers out of the 16 layers of VGG-19 to obfuscate. Consequently,
the searching space of brute force attack will be increased to a
huge number, since the Conv2D conﬁgurations reﬂected in the
EM trace does not help with reverse engineering at all. Further,
as the Eq. 2 shows, S can be changed (elongated), which will
affect the similarity calculation in brute force attack. It is hard
for an attacker to ﬁnd a unique correct Wop-conﬁg for the
current layer. If they ignore S and only compare M , N , and
wc, it will induce many possible Wop-conﬁgs.

(cid:7) + 1

(8)

B. NNReArch Framework

NNReArch is an automated tensor program generator that
can mitigate the DNN-architecture-relevant EM side-channel

TABLE I: VGG-19 under balance mode with different scheduling

ei

Conv2Di

1
2
3
4
5

6

7

8

9

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

IC

16
64
64
128
128

256

256

512

FI

224
224
112
112
56

56

28

28

Independent
SSConv2Di
2592
12960
9000
16000
10240

Wop-conﬁg
Ground Truth
[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]

AutoTVM
Opt-conﬁg
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 2, 8, 4, 2]
[1, 2, 4, 2, 2]

16000

[256, 256, 3, 56, 56]

[1, 2, 4, 2, 2]

9000

[256, 512, 3, 28, 28]

[1, 1, 1, 1, 2]

12960

[512, 512, 3, 28, 28]

[1, 4, 4, 1, 2]

512

14

5760

[512, 512, 3, 14, 14]

[1, 4, 1, 1, 2]

AutoTVM
SSConv2Di
2592
12960
9000
20
10240
20
1
1
9000
12960
1
1
5760
1
1
1

NNReArch
Opt-conﬁg
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 1, 4, 4, 2]
[1, 2, 4, 2, 2]
[1, 2, 8, 2, 2]
[1, 2, 4, 2, 2]
[1, 1, 8, 2, 2]
[1, 1, 1, 1, 2]
[1, 1, 4, 1, 2]
[1, 2, 4, 1, 2]
[1, 4, 4, 1, 2]
[1, 4, 1, 1, 2]
[1, 2, 1, 1, 2]
[1, 2, 1, 2, 2]
[1, 1, 1, 2, 2]

NNReArch
SSConv2Di
2592
12960
9000
16000
10240
16000
16000
16000
9000
12960
12960
12960
5760
5760
5760
5760

SSN N
All Conv2D Execution Time (ms)

62559
1670.02

169712
1721.19

low-level NN execution codes. In addition, they can also trade
off the security and performance through these constraints.
Two Opt-conﬁg selecting modes are provided for users: bal-
ance mode and secure mode. In balance mode, NNReArch
selects the high-performance Opt-conﬁg while ensuring the
architecture conﬁdentiality level. Speciﬁcally, the generator
will ﬁrst select the high-performance Opt-conﬁg. Secure mode
prioritizes security, by randomly choosing Opt-conﬁgs and
a certain number of layers to apply the EM obfuscation, in
order to signiﬁcantly enlarging searching space. In Fig. 6, the
“Basic security level constraint” is related to maximizing the
SSN N in both modes. The “Performance constraint” controls
the balance mode to satisfy the performance requirement,
i.e., runtime overhead. The “EM obfuscation constraint” is
customized by the user, who only needs to provide the number
of layers to obfuscate, and NNReArch will generate low-level
candidate code to satisfy all constraints.

V. EVALUATION AND DISCUSSION

In this section, we evaluate the performance of NNReArch,
and compare it with AutoTVM, using the most commonly
utilized DNN architectures,
including VGG-16, VGG-19,
ResNet-18, and ResNet-34. For sake of clarity, we list all
possible workload conﬁguration (i.e., ground truth) of the
Conv2D layers of VGG-19 in Tab. II. For example, “ei = 1”
represents the 1st type of Conv2D layer, and its corresponding
“NO. of usage = 1” means that this layer type is only used
once in VGG-19. From the brute force attack perspective, if
given the correct IC and F I, the search space SSConv2D1 of
Conv2D 1 is 2592. Therefore, the attacker will have to iterate
entire SSConv2D1, to ﬁnd out the best matching workload.

We present the technical detail of deploying NNReArch on
VGG-19 as an example, and illustrate the experimental results
of all other DNN architectures in Fig. 7.

A. Applying NNReArch on VGG-19

We ﬁrstly apply the balance mode of NNReArch on VGG-
19, which considers the tradeoff between security and per-
formance, and the results are shown in Tab. I. Speciﬁcally,

Fig. 6: NNReArch framework design overview.

TABLE II: All possible workload conﬁguration of Conv2D
layers in VGG-19 applied in NNReArch to build the Opt-
conﬁg & Performance Database (Fig. 6).

ei

1
2
3
4
5
6
7
8
9

Wop-conﬁg

[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]
[256, 256, 3, 56, 56]
[256, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 512, 3, 14, 14]

Psei
size
61
183
219
292
316
395
315
378
216

NO. of
Usage
1
1
1
1
1
3
1
3
4

leakage from the target device. Fig. 6 shows the workﬂow of
NNReArch. In step 1(cid:13), we implement the operator extraction.
The input is an abstracted DAG NN architecture, and the out-
put is the operator expression list, as shown in Tab. II column
ei. Each of these operators is for certain Conv2D layers with
different conﬁgurations. Through the TVM compiler we can
generate different Opt-conﬁgs for a speciﬁc ei workload, as
shown in the column Psei size, the number of compilable Opt-
conﬁgs (P sei). We evaluate their performance on the target de-
vice in step 2(cid:13). We record all P sei and their corresponding ex-
ecution time in an “Opt-conﬁg & Performance Database”. Step
3(cid:13) denotes the proposed NNReArch, where the user may apply
three constraints shown in 4(cid:13) to generate the corresponding

Operator extraction1NNReArchgenerator3Final configurationTarget DeviceNeural Network (NN) ArchitectureComputational graphOpt-config & Performance DatabaseBasic security level constraintPerformance evaluation2Performance constraintEM obfuscation constraint4TABLE III: NNReArch applied on 8 Conv2D layers with
secure mode (EM obfuscation)

ei

Conv2Di

1
2
3
4
5

6

7

8

9

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

NNReArch
Wop-conﬁg
[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]
[256, 64, 3, 56, 112]1
[64, 256, 3, 112, 112]2
[256, 256, 3, 112, 56]3
[256, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 2048, 3, 28, 14]4
[2048, 512, 3, 7, 7]5
[512, 2048, 3, 7, 7]6
[2048, 512, 3, 7, 7]7
[512, 512, 3, 7, 14]8

All Conv2D Execution Time (ms)

NNReArch
Opt-conﬁg
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 1, 4, 4, 2]
[1, 2, 4, 2, 2]
[1, 2, 8, 2, 2]
[1, 2, 4, 2, 2]
[1, 1, 8, 2, 2]
[1, 1, 1, 1, 2]
[1, 1, 4, 1, 2]
[1, 2, 4, 1, 2]
[1, 4, 4, 1, 2]
[1, 4, 1, 1, 2]
[1, 2, 1, 1, 2]
[1, 2, 1, 2, 2]
[1, 1, 1, 2, 2]
1927.59

we apply the security constraint as “maximizing the Secb”
and the performance constraint as “shortest execution time”.
Note that this evaluation does not include the EM obfuscation.
The performance of applying NNReArch is mainly shown in
column NNReArch Opt-conﬁg in Tab. I. The total execution
time of all VGG-19 Conv2D layers is 1721.19ms. Compared
with the original performance using “AutoTVM” scheduling
(column AutoTVM Opt-conﬁg in Tab. I), the deployment
of NNReArch only incurs 3.06% performance overhead. In
the searching space (SSConv2Di ) is signiﬁcantly
contrast,
increased, with SSN N = 169712. Thus,
in terms of the
balance mode, applying NNReArch (w/o EM obfuscation)
increases the difﬁculty of DNN architecture extraction for
about 2.71 times.

We further apply the secure mode of NNReArch (i.e., with
EM obfuscation) to prioritize the DNN model architecture
conﬁdentiality. Speciﬁcally, we select 8 layers of VGG-19
to schedule their tensor program, making them to have the
same EM trace characteristics. The Opt-conﬁg setting of
the obfuscated workload are listed in Tab. III, and here we
use superscripts (1,2,...,8) to annotate the obfuscated layers.
Compared with the ground truth conﬁguration (column Wop-
conﬁg Ground Truth in Tab. I), the obfuscated EM traces
will lead the reverse engineering attack to wrong workload.
For example, the EM obfuscation breaks the performing diver-
gence of the EM leakage on different convolution layers, when
the attacker reasons each Conv2D layer, i.e., s/he will derive
wrong workloads that negatively affect the guess on followed
layers, disturbing the consistency of adjacent Conv2D layers.
As a result, when the exteriors of different Wop-conﬁgs have
the same patterns, attackers have to handle a huge amount of
puzzles and guesses. Thus the searching space and attacking
effort will increase massively.

As discussed in Sec. III-F, the EM obfuscation needs to
add pause opcodes to obfuscate the stall time, to make the
obfuscated layer performing almost the same as the “imitated
layer”, which may brings performance loss. In our experimen-
tal evaluation on VGG-19 shown in Tab. III, the obfuscated

DNN model consumes 1927.59 ms to execute all Conv2D lay-
ers, which has an approximate 15.42% performance overhead
compared with the AutoTVM.

B. Discussion: security and performance trade-off

(a) Security level measurement

(b) Performance measurement

Fig. 7: Performance evaluation and comparison between Au-
toTVM and the NNReArch balance mode of NNReArch.

Since most existing DNN development frameworks (in-
target at
cluding both open-source and commercial) still
therefore, we evaluate the balance mode of
performance,
NNReArch on the popular DNN architectures from VGG and
ResNet to draw generic conclusions. We ﬁrst compare the
searching space (SSN N ) of DNNs from the same family. For
example, VGG-16 and VGG-19 are constructed by the same
Conv2D layer types, but only with different number of layers
of the same Wop-conﬁg. Therefore, the attacking difﬁculty for
DNNs in the same family is almost equal using AutoTVM.
As afﬁrmed in Fig. 7a, the searching space is the same for
VGG-16 and VGG-19, as well as for ResNet-18 and ResNet-
34. This indicates a vulnerability of these performance-only
optimization frameworks, i.e., using a larger DNN model does
not make it more challenging to reverse engineer the model
architecture. In contrast, the proposed NNReArch framework
constructively leverages the model size to maximizes the
searching space of each Conv2D layer, making the architecture
of deeper DNNs more secure, as shown in Fig. 7a. Fig. 7b
illustrates the performance comparison between AutoTVM
and NNReArch, which demonstrates that NNReArch only
incurs trivial execution time overhead on all evaluated DNN
architectures compared with AutoTVM.

For the secure mode of NNReArch, we demonstrate that
a carefully crafted DNN model (e.g., the one in Tab. III),
can signiﬁcantly challenge the model extraction attacks with
relatively higher performance loss (15%). Moreover, the secure
mode enables the designer to either randomly choose Conv2D
layers for EM obfuscation, or apply unexpected stall time to
break the association between the EM side-channel leakage
and workload conﬁguration, thus providing more ﬂexibility to
the DNN model security enhancement.

VGG-16VGG-19ResNet-18ResNet-34012Searching space#105AutoTVMNNReArch (w/o EMob.)VGG-16VGG-19ResNet-18ResNet-340500100015002000Execution time (ms)channel information leakage,” in 2020 IEEE Interna-
tional Symposium on Hardware Oriented Security and
Trust (HOST).

IEEE, 2020, pp. 209–218.

[10] T. Zhou, Y. Zhang, S. Duan, Y. Luo, and X. Xu, “Deep
neural network security from a hardware perspective,” in
2021 IEEE/ACM International Symposium on Nanoscale
Architectures (NANOARCH).

IEEE, 2021, pp. 1–6.

[11] T. Chen, “Pynq-z1 tvm-vta core conﬁguration.” [Online].
Available: https://github.com/apache/tvm-vta/blob/mast
er/conﬁg/pynq sample.json

[12] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and
N. Papernot, “High accuracy and high ﬁdelity extraction
of neural networks,” in 29th {USENIX} Security Sympo-
sium ({USENIX} Security 20), 2020, pp. 1345–1362.
[13] L. Batina, S. Bhasin, D. Jap, and S. Picek, “{CSI}{NN}:
Reverse engineering of neural network architectures
28th
through
{USENIX} Security Symposium ({USENIX} Security
19), 2019, pp. 515–532.

electromagnetic

channel,”

side

in

[14] S. Tian, S. Moini, A. Wolnikowski, D. Holcomb,
R. Tessier, and J. Szefer, “Remote power attacks on
the versatile tensor accelerator in multi-tenant fpgas,”
in 2021 IEEE 29th Annual International Symposium
on Field-Programmable Custom Computing Machines
(FCCM).

IEEE, 2021, pp. 242–246.
[15] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y. Ji, X. Xie,
Y. Ding, C. Liu, T. Sherwood et al., “Deepsniffer:
A dnn model extraction framework based on learning
architectural hints,” in Proceedings of the Twenty-Fifth
International Conference on Architectural Support for
Programming Languages and Operating Systems, 2020,
pp. 385–399.

[16] J. H. Anderson and F. N. Najm, “Power estimation
techniques for fpgas,” IEEE Transactions on Very Large
Scale Integration (VLSI) Systems, vol. 12, no. 10, pp.
1015–1027, 2004.

[17] “Aaronia pbs2 e & h near ﬁeld probe set sniffer dc to
6ghz with emc preampliﬁer,” https://instrumentcenter.eu/
products/emc-products/probes/aaronia-pbs2-e-h-near-fie
ld-probe-set-sniffer-dc-to-6ghz-with-emc-preamplifier,
(Accessed on 06/06/2021).

[18] “Teledyne lecroy - oscilloscope,” https://teledynelecroy

.com/oscilloscope/, (Accessed on 06/08/2021).

VI. CONCLUSION

In this paper, we study the association between DNN model
architecture conﬁguration and EM side-channel leakage. Using
an open-source deep-learning accelerator VTA as our ex-
perimental platform, we discover the low-level code causes
of the EM side-channel-enabled DNN architecture reverse
engineering attacks. Furthermore, we present NNReArch, a
DNN model architecture defense framework against side-
channel attacks. Enabling ﬂexible DNN conﬁguration between
performance and security, NNReArch integrates two modes,
balance mode that targets at increasing the searching space of
DNN model architectures, and secure mode that employs EM
obfuscation to cancel the difference between different model
layers. Different from the existing solutions, the proposed
framework is built on popular open-source DNN compilation
tools, VTA, making it a generic defense method.

REFERENCES

[1] N. P.

Jouppi, C. Young, N. Patil, D. Patterson,
G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,
A. Borchers et al., “In-datacenter performance analysis
of a tensor processing unit,” in Proceedings of the 44th
annual international symposium on computer architec-
ture, 2017, pp. 1–12.

[2] Dpu for convolutional neural network. https://www.xili

nx.com/products/intellectual-property/dpu.html.

[3] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen,
M. Cowan, L. Wang, Y. Hu, L. Ceze et al., “{TVM}:
An automated end-to-end optimizing compiler for deep
learning,” in 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), 2018,
pp. 578–594.

[4] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial ar-
chitecture for energy-efﬁcient dataﬂow for convolutional
neural networks,” ACM SIGARCH Computer Architec-
ture News, vol. 44, no. 3, pp. 367–379, 2016.

[5] T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen,
Z. Xu, O. Temam, and Y. Chen, “Dadiannao: A neural
network supercomputer,” IEEE Transactions on Comput-
ers, vol. 66, no. 1, pp. 73–88, 2016.

[6] T. Moreau, T. Chen, Z. Jiang, L. Ceze, C. Guestrin,
and A. Krishnamurthy, “Vta: an open hardware-
software stack for deep learning,” arXiv preprint
arXiv:1807.04188, 2018.

[7] Y. Luo, C. Gongye, Y. Fei, and X. Xu, “Deepstrike:
Remotely-guided fault injection attacks on dnn accel-
erator in cloud-fpga,” in 2021 58th ACM/IEEE Design
Automation Conference (DAC).
IEEE, 2021, pp. 295–
300.

[8] A. S. Rakin, Y. Luo, X. Xu, and D. Fan, “{Deep-Dup}:
An adversarial weight duplication attack framework to
crush deep neural network in {Multi-Tenant}{FPGA},”
in 30th USENIX Security Symposium (USENIX Security
21), 2021, pp. 1919–1936.

[9] H. Yu, H. Ma, K. Yang, Y. Zhao, and Y. Jin, “Deepem:
Deep neural networks model recovery through em side-

