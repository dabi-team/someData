NNReArch: A Tensor Program Scheduling Framework
Against Neural Network Architecture Reverse Engineering

Yukui Luo, Shijin Duan, Cheng Gongye, Yunsi Fei, and Xiaolin Xu
Department of Electrical and Computer Engineering
Northeastern University, Boston, MA, USA

2
2
0
2

r
a

M
2
2

]

R
C
.
s
c
[

1
v
6
4
0
2
1
.
3
0
2
2
:
v
i
X
r
a

Abstractâ€”Architecture reverse engineering has become an
emerging attack against deep neural network (DNN) implemen-
tations. Several prior works have utilized side-channel leakage to
recover the model architecture while the target is executing on a
hardware acceleration platform. In this work, we target an open-
source deep-learning accelerator, Versatile Tensor Accelerator
(VTA), and utilize electromagnetic (EM) side-channel leakage to
comprehensively learn the association between DNN architecture
conï¬gurations and EM emanations. We also consider the holistic
system â€“ including the low-level tensor program code of the VTA
accelerator on a Xilinx FPGA, and explore the effect of such low-
level conï¬gurations on the EM leakage. Our study demonstrates
that both the optimization and conï¬guration of tensor programs
will affect the EM side-channel leakage.

Gaining knowledge of the association between the low-level
tensor program and the EM emanations, we propose NNReArch,
a lightweight tensor program scheduling framework against
side-channel-based DNN model architecture reverse engineering.
Speciï¬cally, NNReArch targets reshaping the EM traces of
different DNN operators, through scheduling the tensor program
execution of the DNN model so as to confuse the adversary.
NNReArch is a comprehensive protection framework supporting
two modes, a balanced mode that strikes a balance between the
DNN model conï¬dentiality and execution performance, and a
secure mode where the most secure setting is chosen. We imple-
ment and evaluate the proposed framework on the open-source
VTA with state-of-the-art DNN architectures. The experimental
results demonstrate that NNReArch can efï¬ciently enhance the
model architecture security with a small performance overhead.
In addition, the proposed obfuscation technique makes reverse
engineering of the DNN architecture signiï¬cantly harder.

I. INTRODUCTION

Neural network (NN) has found important use in differ-
ent application domains, such as object detection, big data
analytic, and semantic recognition. To improve the infer-
ence capability of NN models, deep neural network (DNN)
is proposed, which employs larger model size for tackling
more complicated tasks. Although DNNs are demonstrated as
promising for different tasks, their large model sizes become a
bottleneck for performance and applicability. To mitigate these
issues, different methods have been proposed to accelerate the
execution of DNN models [1â€“5]. For example, modern FPGAs
have been widely deployed to provide acceleration for DNNs
on both edge devices and cloud infrastructures.

A DNN model can be represented as a directed acyclic
graph (DAG) composed of operation nodes and connec-
tions, thus its implementation can be mapped accordingly to
FPGA hardware components (e.g., look-up-table and DSP)
with FPGA-DNN development tools. The most commonly

used frameworks are the Xilinx deep learning processor unit
(DPU) [2], and the open-source versatile tensor accelera-
tor (VTA) [6], which can provide end-to-end optimization
for FPGA-DNN implementation. Although such FPGA-based
DNN acceleration framework provide signiï¬cant performance
improvement, they also create a new attack surface, where an
adversary can either manipulate the inference of DNN models
[7, 8], or illegally extract (i.e., using side-channel analysis)
the critical parameters of a DNN model, such as its archi-
tecture [9, 10]. Since the construction of high-performance
DNN models involves expensive data collection and training
procedures, thus their model parameters like the architecture
should be highly protected.

This paper studies the vulnerability of DNN model architec-
ture against side-channel-based DNN model extraction attack
on FPGA accelerators. Speciï¬cally, we explore the association
between electromagnetic (EM) side-channel leakage of DNN
model implementation on FPGAs. To draw generic conclu-
sions, we use state-of-the-art open-source FPGA acceleration
framework, VTA [11], and explore the internal causes of
DNN model architecture-relevant side-channel leakage from
the low-level program code. Correspondingly, we propose
defense solutions by rescheduling the DNN operator-level
tensor program and obfuscating the side-channel leakage. Note
that although this paper mainly discusses EM side-channel
the presented experimental observation and the
and VTA,
proposed methodologies can be generally extended to other
side-channels and hardware platforms.

The main contributions of this work are as follows:
â€¢ We comprehensively study the EM side-channel leakage
for DNN models deployed with VTA. To the best of our
knowledge, this is the ï¬rst work exploring the association
between low-level tensor program code and EM trace,
from the adversarial perspective.

â€¢ We systematically formulate the mathematical represen-
tation of the DNN architecture with EM emanations, fol-
lowing which we further propose security metrics for the
DNN models based on the visualized EM characteristics.
These metrics can be used to estimate the security level
of a DNN architecture against side-channel attacks.

â€¢ We present NNReArch, a ï¬‚exible defense framework
for DNN model architectures against EM side-channel
attacks. We evaluate the performance of NNReArch with
state-of-the-art DNN architectures, and we demonstrate
that NNReArch can signiï¬cantly complex the reverse

 
 
 
 
 
 
engineering attacks, i.e., doubling the attacking efforts
with only 3.06% performance overhead.

II. BACKGROUND AND RELATED WORK

A. Versatile Tensor Accelerator (VTA)

VTA is an open-source, generic, and FPGA-speciï¬c deep
learning acceleration framework [6], consisting of four mod-
ules to enable task-level parallelism in a pipelining fashion
(TLPP): a â€œfetch moduleâ€ that loads the instruction stream
from the DRAM, a â€œload moduleâ€ loading the input, weight
parameters, and intermediate results, a â€œstore moduleâ€ writing
back intermediate results, and the â€œcompute moduleâ€ accel-
erates computing. The last one is â€œcompute moduleâ€ that
relies on two kernels, the general matrix multiply (GEMM)
kernel for dense linear algebra computations, and the tensor
arithmetic logic unit (ALU) for general computing tasks. VTA
is supported by TVM [3], a compiler (AutoTVM) scheduling
the target deep learning application on a hardware platform.
In addition, two techniques, matrix multiplication blocking
(MMB) and virtual threading (VT), are used to customize the
FPGA execution to further improve the VTA performance. The
MMB technology divides large NN operators down to smaller
blocks to ï¬t the GEMM kernel, and VT manages the hardware
resources of VTA to facilitate simultaneous computing and
memory access. More technical details of the VTA can be
found in Sec. III.

B. Model Architecture Extraction Attack and Defense

Model architecture extraction has become an emerging
threat to the security of DNN. In addition to attacks from the
software side [12], different side-channels have been utilized
to extract DNN architectures on hardware platforms. Batina
et al. [13] ï¬rst demonstrated architecture extraction of multi-
layer perception (MLP) using the EM signals from both AVR
and ARM processors. In [9], Yu et al. applied EM-based
attack on an FPGA against a convolutional neural network
(CNN). Tian et al. [14] utilized an on-chip time-to-digital
(TDC) sensor to extract the NN architecture, which can be
launched remotely on a multi-tenant FPGA, but with lower
resolutions compared to EM signals. Hu et al. [15] demon-
strated extracting a complete NN architecture using multiple
side-channels of GPUs, such as the memory access pattern.
However, none of the prior works targets comprehensively
studying the tensor programmable accelerator, such as VTA.
Also, defense methods are still in their infancy.

C. EM Side-channel

[13]

EM side-channel

is an effective and contact-less
method for extracting sensitive information during the system
execution. The instantaneous EM emanation is dependent on
the dynamic current [9], as shown in Eq. 1:

Idyn(t) =

C Ã— VDD Ã— fclk Ã— D(t)
2

(1)

where C denotes the capacitance of the activated metal nets,
D(t) represents the transition rate of the nets (determined by
both operations and data), VDD is the voltage supply, and fclk

Fig. 1: Experimental Platform for EM Signal Collection

stands for the execution clock frequency [16]. Therefore, an
EM trace from a hardware platform well embodies information
for its workload, data, and system computing and communi-
cation.

III. ASSOCIATING TENSOR PROGRAM ON VTA WITH EM
EMANATION

A. Threat model

We follow the same threat model as in other related side-
channel model extraction works [9, 13], with our victim
device being an edge FPGA running an VTA accelerator for
a pre-trained DNN model. The attacker is able to obtain the
EM signals of the victim device with specialized equipment
and also knows the model execution status, which can assist
in reasoning the architecture of the victim DNN model.
We assume a strong attacker, who has sufï¬cient knowledge
of the target accelerator, including the conï¬guration of the
accelerator (discussed in Sec. III-C2). The executing model
architecture is the target for reverse engineering with all the
EM traces, accelerator, and platform information.

B. Experimental Platform

We build an experimental platform using PYNQ-Z1 devel-
opment kit, an SoC with a Xilinx Zynq-7000 device and a
dual-core ARM Cortex-A9 processor (PS). To align the EM
signals with execution phases for the device characterization
purpose, we modify the VTA bitstream as shown in Fig.1 (a).
Speciï¬cally, we use two signals as the trigger signals to mark
the starting and ending of the VTA execution (and therefore
the corresponding EM segment): the general-purpose output
(GPO) manager write valid of the PS, M AXI GPO WVALID,
and the accelerator coherence port (ACP) subordinate write
valid of the VTA core, S AXI ACP WVALID. The M AXI -
GPO WVALID signal annotates the opcodes and data that have
been written into the VTA queues, and S AXI ACP WVALID
indicates that the VTA core output is valid to be written back
to the DRAM of the PYNQ-Z1 system.

Our EM trace collection setup includes an EM Probe
PBS2 [17] converting the EM signals into voltage represen-

ZYNQPS sideMSVTA coreSMTrigger signalsM_AXI_GPO_WVALIDS_AXI_ACP_WVALIDEM leakage from VTA coreEM leakage from Zynq PS side (a) VTA bitstream diagram(b) EM leakage and trigger signals(a) Conv2D-Opt:[1, 1, 1, 1, 1]. The baseline
w/o optimization.

(b) Conv2D-Opt:[2, 1, 1, 1, 1]. Blocks along
the input channel (icb = 2).

(c) Conv2D-Opt:[1, 2, 1, 1, 1]. Blocks along
the output channel (ocb = 2).

(d) Conv2D-Opt:[1, 1, 2, 1, 1]. Blocks along
the input feature height (f ihb = 2).

(e) Conv2D-Opt:[1, 1, 1, 2, 1]. Blocks along
the input feature width (f iwb = 2).

(f) Conv2D-Opt:[1, 1, 1, 1, 2]. Apply the
virtual threading (vt = 2).

Fig. 2: We use Conv2D-Wop:[256, 256, 3, 14, 14] as an example to show how the EM leakages change with the Opt-conï¬g
[icb, ocb, f ihb, f iwb, vt].

tations, an Aronia AG pre-ampliï¬er, and a Lecroy oscillo-
scope [18]. An EM trace example with the sampling rate
of 1GHz is shown in Fig. 1 (b), which is averaged from
50 measurements with the same inputs. The average pre-
processing method helps to make the EM trace stable and
ï¬lter the measurement noise.

C. Terminology and Deï¬nitions

With the EM measurement setup ï¬xed,

there are three
other factors that jointly determine the EM trace measurement
of a VTA: (1) The workload conï¬guration of the current
DNN layerâ€™s operation, namely Wop-conï¬g; (2) The global
conï¬guration of the VTA-core, namely VTA-conï¬g; (3) To
take advantage of the FPGA parallelism and ARM multi-
thread scheduling, operators (resources) for a DNN layer are
optimized, whose conï¬guration is denoted as Opt-conï¬g.

1) Wop-conï¬g: For the most commonly used DNN ar-
layer (Conv2D) is the most
chitectures, 2D convolutional
critical component to construct the entire model architecture.
A Conv2D is speciï¬ed by the number of input channels (IC),
output channels (OC), the kernel size (K), the input feature
size (F I), and the output feature size (F O). We follow the typ-
ical regulation that assumes the input/output feature is square-
shaped. The Conv2D-Wop is therefore [IC, OC, K, F I, F O].
2) VTA-conï¬g: A VTA-core is deployed on an FPGA
speciï¬ed by VTA-conï¬g, the conï¬guration from [11]. The
main computing component GEMM kernel, is designed around
a tensor core performing one matrix-matrix operation in each
clock cycle. This operator is to implement the product of
a 1 Ã— 16 input and a 16 Ã— 16 weight matrix. The VTA
core employs hardware resources for parallel computation to
achieve high performance. The input matrix has dimension
of BAT CH Ã— BLOCK IN , where BAT CH indicates how

many feature maps can be implemented in parallel, by the VTA
core (BAT CH = 1 by default), and BLOCK IN represents
the input channel-parallelism. For example, in our experi-
mental, BLOCK IN is set as 16, indicating that 16 input
channels can execute in parallel. The weight matrix includes
BLOCK IN Ã— BLOCK OU T number of weights, where
BLOCK OU T represents the output channel-parallelism.
When BLOCK OU T is 16, the VTA can produce results
in 16 output channels (output BAT CH Ã— BLOCK OU T ).
Another setup of VTA-conï¬g is the sizes of on-chip buffers,
including input, output, and weight buffer with 32KB, 128KB,
and 32KB memory sizes, respectively.

3) Opt-conï¬g: Rather than static execution, the VTA can
dynamically schedule the execution of Conv2D layers for
performance optimization. AutoTVM supports VTA to imple-
ment explicit memory latency hiding by the virtual threading
(vt) primitive, corresponding to multi-threading of the ARM
processor. As our used ARM Cortex A9 dual-core processor
allows two threads, the VTA vt can support threads up to 2. To
map the matrix multiplication efï¬ciently on a VTA core, TVM
can optimally break down large workload as smaller blocks,
to achieve computation efï¬ciency within limited hardware
resources. There are four scales of blocks associated with
this technology: input channel blocks (icb), output channel
blocks (ocb), and two input feature map blocks along the
height axis (f ihb) and width axis (f iwb), respectively. We
use an Opt-conï¬g vector [icb, ocb, f ihb, f iwb, vt] to represent
the optimization setting. For example, a Conv2D-Opt of [2, 2,
2, 2, 2] is applied on a convolution layer with Con2D-Wop:
[256, 256, 3, 14, 14]. The scheduler will divide the original
convolution layer into several small blocks with workload
Conv2Db-Wop:[128, 128, 3, 14, 14] because both the input
(icb) and output channel (ocb) blocks are 2. It will also separate

0   2   4   6   8  10 12 14         106   110  â€¦â€¦â€¦â€¦â€¦â€¦16 ð¶ð‘’ð‘¥and 16 ð‘†, ~11.25msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)0  2   4   6   8  10 12 14         122   126  â€¦â€¦â€¦â€¦â€¦â€¦32 ð¶ð‘’ð‘¥and 32 ð‘†, ~12.8msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)0  2   4   6   8  10 12         88      92  â€¦â€¦â€¦â€¦â€¦â€¦8ð¶ð‘’ð‘¥and 8ð‘†, ~9.6msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)0  2   4   6   8  10 12         124    126  â€¦â€¦â€¦â€¦â€¦â€¦32 ð¶ð‘’ð‘¥and 32 ð‘†, ~13.2msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)0  2   4   6   8  10 12         142    146  â€¦â€¦â€¦â€¦â€¦â€¦32 ð¶ð‘’ð‘¥and 32 ð‘†, ~15.0msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)0  2   4   6   8  10 12         56      60â€¦â€¦â€¦â€¦â€¦â€¦8ð¶ð‘’ð‘¥and 8ð‘†, ~6.5msð¶ð‘’ð‘¥ð‘†100500-50100Ã—105ð‘¤ð‘Time (ns)Amplitude (mv)Similarly, blocks along the feature map height and width also
affect M and wc of the EM trace, as shown in Fig. 2d and
Fig. 2e, although no obvious difference between these two EM
traces can be observed. Following our measurement results,
blocking along the width of the feature map (f iwb) induces
a longer S. Besides, we applied the virtual threading method,
which accelerates the operator by hiding the DRAM memory
access latency and enables the TLPP of VTA as mentioned in
Sec.II-A. As shown in Fig. 2f, this conï¬guration shortens the
execution time of the entire Conv2D layer compared with the
baseline by reducing the M .

From the EM leakage observation, we can draw the follow-
ing conclusions: (1) M is a function of IC, icb, OC, ocb, F O,
F I, f ihb, f iwb, and vt; (2) N is a function of IC, icb; and
(3) wc is a function of OC, ocb, F I, f ihb, and f iwb.

E. Low-level Program Code Analysis

Visually inspecting the EM traces derives general associ-
ation between the EM pattern and the two conï¬gurations,
Wop-conï¬g and Opt-conï¬g. To comprehensively understand
the execution impacts on the EM leakage, we look into the
low-level code structure shown in Fig 4. Since the Tensor ALU
operators of a Conv2D layer have low-arithmetic intensity and
therefore do not emanate high EM leakage, we focus on the
GEMM operator [6]. The GEMM code is composed by many
nested loops of operations, corresponding to the repetitive
EM pattern shown in Fig 2. We extract three parts (part 1
to 3 as shown in Fig 4) related to the Conv2DEM function
parameters, M , N , and wc, respectively. In Part 1, there are
four outer loops related to M , and their ranges indicate the
blocking parameters: icb, f ihb, f iwb and vt. Note that the part
1 program is the most outer loop, the function of M is also
determined by several other parameters, as deï¬ned in Eq. 3:

M =

IC Ã— icb Ã— F O Ã— f ihb Ã— f iwb
BLOCK OU T Ã— F I Ã— ocb Ã— vt

(3)

It is straightforward to determine N from the range of

ic.outer of the Part 2 code:

N =

IC
BLOCK IN Ã— icb

(4)

Different from M and N that are discrete (integer) numbers,
wc is associated with the execution time. Hence, without
knowing the exact function, we can only leverage the Part
3 code to determine which parameters affect its quantity. In
the ï¬rst cthread.s_1 loop, if its range is larger than 1, it
will enable the TLPP. Our experiments suggest that the range
of dx is equal to K, i.e., the kernel size. If ocb = 1, the
range of dy is also 1, otherwise it is K. The range of j is a
function of F I, f iwb, and F O. Putting all these clues together,
we assume function g(Â·) can obtain wc from low-level tensor
program code in Eq. 5.

wc = g(K, F I, f iwb, F O, vt, fex, II)

(5)

The TLPP is conï¬gured by vt, II denotes the initiation
interval for the pipeline, and fex is the executing frequency
of the VTA core, which is 100M Hz in this paper.

Fig. 3: The scheduler divides a large Conv2D layer down to
smaller blocks along the IC and OC axes. (a) w/o optimization.
(b) Conv2D-Opt: [2, 1, 1, 1, 1], icb = 2. (c) Conv2D-Opt: [1,
2, 1, 1, 1], ocb = 2.

the 14 Ã— 14 input feature map into 7 Ã— 7 blocks because
f ihb and f iwb are also 2. Moreover, the Opt-conï¬g enables
dual-threading. In contrast, the non-optimized Opt-conï¬g is
Conv2D-Opt:[1, 1, 1, 1, 1]. Note that across this paper, we
use the default BAT CH setting, and the subscript b is used
to represent the detailed value of each parameter out of many
possibilities.

D. EM Leakage Observation

In our experiment, we implement a convolutional layer with
Conv2D-Wop of [256, 256, 3, 14, 14]. We choose different
Opt-conï¬gs to understand the impact of optimizations on DNN
execution, which is reï¬‚ected in the EM leakage. Fig. 2 shows
the EM traces collected from the basic VTA setting without
optimization (Fig. 2a) and ï¬ve optimized versions (Fig. 2b
to 2f). Inspecting these traces, we ï¬nd a repetitive pattern of
a segment of high-frequency activity (continuous execution,
Cex) followed by a segment of low-frequency (stalling) ac-
tivity (S). This pattern repeats M times for the convolutional
layer computation. Further, from the beginning Cex segment,
we can clearly observe several spikes, the number (N ) of
which is countable and each of them has approximately the
same width (wc), where Cex = N Ã— wc. Thus, we can derive
a Conv2D EM trace function (Conv2DEM ) with 2 countable
parameters: M and N , and 2 measurable parameters: wc and
S.

Conv2DEM = M Ã— (N Ã— wc + S)

(2)

When we change the icb along the IC and ocb along the OC
of the Opt-conï¬g, the computing ï¬‚ow for each axisâ€™ blocks is
shown in Fig. 3, and their corresponding EM traces are shown
in Fig. 2b and Fig 2c. If dividing the Conv2D layer along
the IC axis, it will generate icb subordinated outputs OCi,
whose summation will derive the ï¬nal result. When icb = 2,
two paths are scheduled and the execution time of Cex is
reduced by almost a half. Blocks along the OC axis has a
simpler computational process. The scheduler separates OC
output channels into ocb sections, and the ï¬nal result is the
concatenation of those sub-results. Comparing Fig. 2a with
Fig. 2b and Fig. 2c, the IC axis blocking determines M
and N , and the OC axis blocking determines M and wc.

Input feature maps (ð¹ð¼Ã—ð¹ð¼Ã—ð¼ð¶)Input channelsOutput channelsresult(a)(b)(c)ð‘‚ð¶ð‘‚ð¶ð¼ð¶ð¼ð¶2ð¼ð¶2ð¼ð¶ð‘‚ð¶1ð‘‚ð¶2ð‘‚ð¶2ð‘‚ð¶2ð‘‚ð¶ð‘‚ð¶Fig. 4: Low-level code summary for the optimizable and high-arithmetic intensity GEMM operator, which constructs the
Conv2D layer with low-arithmetic intensity ALU.

(a) Conv2Do-Wop:[256, 256, 3, 14, 14], Conv2Do-Opt:[1, 2, 1, 1, 2].

(b) Conv2Dt-Wop:[128, 128, 3, 28, 28], Conv2Dt-Opt:[1, 4, 2, 2, 2].

Fig. 5: Conv2D EM obfuscation example. We use the subscript
o to represent the original Conv2D layer, and subscript t is the
target obfuscation Conv2D layer.

F. EM Obfuscation

With the EM leakage characterization and the low-level
code analysis, we propose to obfuscate the EM trace by
scheduling the tensor program. As a proof of concept, we
implement two different Conv2D layers: one Conv2Do with
the Wop-conï¬g of [256, 256, 3, 14, 14] and Opt-conï¬g of[1,
2, 1, 1, 2]; the other Conv2Dt with the Wop-conï¬g of [128,
128, 3, 28, 28] and Opt-conï¬g of [1, 4, 2, 2, 2]. Inspecting
their EM traces in Fig. 5, we notice these two layers can
generate similar Cex, because their GEMM operators have
equal counting result (OpCGEMM ) derived by Eq. 6.

OpCGEMM =

IC Ã— OC Ã— K 2 Ã— F O2
BLOCK IN Ã— BLOCK OU T

(6)

the stall

However,

time is different between these two
settings (So and St). Such difference can be canceled by
adding pause opcode to delay âˆ¼ 0.35ms in every So, so
that S(cid:48)
o = St. As a result, these two EM traces become in-

distinguishable( i.e., unable to determine which setting is in
effect).

Generally, the goal of EM obfuscation for a Conv2D layer is
to ï¬nd different conï¬gurations resulting in the same operation
counts as the original one, following Eq. 6. Speciï¬cally, this
equation can assist us to ï¬nd a target Wop-conï¬g Conv2Dt âˆ’
W op, for which there exists an Opt-conï¬g Conv2Dt âˆ’ Opt
satisfying Mt = Mo, Ko = Kt, and has a longer execution
time. Then we can derive âˆ†S = St âˆ’ So by measurement,
and apply it to the original workload to mimic it as the target
workload.

IV. SECURITY METRICS AND OPTIMIZATION PROGRAM
FOR VTA IMPLEMENTATION

This section introduces NNReArch, which utilizes Opt-
conï¬g and EM obfuscation to mitigate the EM leakage of the
DNN model. For an attacker to reverse-engineer the victim
NN architecture implemented on VTA, s/he needs to derive
VTA-conï¬g and Opt-conï¬g. If Opt-conï¬g is ï¬xed, such as
Conv2D-Opt of [1, 1, 1, 1, 1], then the victim architecture is
easy to extract. Thus, a primary idea of mitigating the EM
side-channel leakage is to increase the searching space of the
Opt-conï¬g for each Conv2D layer.

A designer can formulate the scheduling of DNN execution
as an optimization problem. For a given neural network
architecture N N , we can extract a set of workload expression
E that executes on a target acceleration device. Then, for a
given workload e âˆˆ E, we can implement it with many differ-
ent functionally equivalent low-level program codes inducing
different EM traces, as observed in Sec. III-D and III-E. There-
fore, each workload could have multiple equivalent schedules,
i.e., Opt-conï¬g. We use P se to denote the possible schedule
space for e. For example, in VGG-19, there are 9 types of
Conv2D layers with different Wop-conï¬gs, each of whic is
denoted as ei, i âˆˆ [1, 9] and has a set of Opt-conï¬g P sei .

attr= {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}buffers = {res: Buffer(res_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),data: Buffer(data_2: Pointer(int8), int8, [1, 16, 14, 14, 1, 16], []),kernel: Buffer(kernel_2: Pointer(int8), int8, [16, 16, 3, 3, 16, 16], [])}buffer_map= {data_1: data, kernel_1: kernel, res_1: res} {for (i1.outer.outer: int32, 0, 16) {for (i2.outer: int32, 0, 2) {for (i3.outer: int32, 0, 2) {for (cthread.s: int32, 0, 2) {load VTAPushGEMMOpstream to VTA queue...}}}for (ic.outer: int32, 0, 16) {Calculate the DRAM memory address and load data by VTALoadBuffer2D...}... may have additional same code blocks depending on the cthread.srangefor (cthread.s_1: int32, 0, 2) {VTAUopLoopBegin...for (dy: int32, 0, 3) {for (dx: int32, 0, 3) {for (j: int32, 0, 14) {Execute the computationâ€¦}}}... VTAUopLoopEnd}}}}}Describe the Conv2D workloadPart 1i2.outer appear if enable ð‘“ð‘–â„Žð‘i3.outer appear if enable ð‘“ð‘–ð‘¤ð‘The range of cthread.s> 1 if enable ð‘£ð‘¡Related to ð‘¤ð‘The range of cthread.s_1 > 1 if enable ð‘£ð‘¡dyappear if enable ð‘œð‘ð‘Related to MPart 2Part 3Related to N100500-50-10002468Time (ns)Amplitude (mv)Ã—106ð‘†ð‘œð¶ð‘’ð‘¥,ð‘œ100500-50-100Time (ns)Amplitude (mv)02468ð¶ð‘’ð‘¥,ð‘¡ð‘†ð‘¡Ã— 106A. Security Metrics

When considering conï¬dentiality of a N N , brute force
attack is the most generic method and its complexity can be
represented by the size of its searching space (SSN N deï¬ned
in Eq. 10). The attacker normally progresses sequentially,
i.e., from the ï¬rst Conv2D layer to the following layers,
since s/he has to utilize the results (e.g., dimensions) of the
previous layers. IC and F I of the ï¬rst layer can be directly
observed from the input image and global conï¬guration of
the accelerator. For the Wop-conï¬g of each Conv2D layer, the
adversary could build a library of combinations of Wop-conï¬g
and Opt-conï¬g, and then estimate their EM trace patterns. The
candidate with high similarity to the observed EM trace of the
target NN could be considered as the correct hypothesis with
high conï¬dence.

the

IC, F I

assume

Pooling

layer. Other

generally we

1) Search space for

parameters,
remain

individual Conv2D layers: For
a Conv2D layer,
are
derived from the previous Conv2D layerâ€™s OC, F O,
including
or
{K, F O, OC, f ihb, f iwb, icb, ocb, vt},
be
to
discovered. Some of these parameters follow some conventions
that can be used as hints for guessing. Hint 1: The K of
the 1st Conv2D layer might be 3, 5, or 7, and the K
of the rest Conv2D layers might be 1 or 3. Hint 2: F O
depends on the F I and the stride of the kernel, which is
normally 1 or 2. When F I is smaller than 8,
the stride
will be 1. Hint 3: OC depends on IC and BLOCK IN ,
where IC is expected as a multiple of BLOCK IN .
An exception is that the 1st Conv2D layer usually has an
IC smaller than BLOCK IN , so the VTA will convert
to IC = BLOCK IN with dummy input channels.
it
If representing the relationship between OC and IC as
OC = foc Ã— IC, then foc âˆˆ { 1
2 , 1, 2, 4}. Note that foc = 1
4 , 1
and 1
2 do not happen when IC = BLOCK IN , and foc = 1
4
do not occur when IC = 2 Ã— BLOCK IN . Hence, the
searching space (SS) for K, F O, OC are
(cid:26) 3
2
(cid:26) 1
2
3
4
5

, i = 1
, i > 1
, F I < 8
, Otherwise
, IC = BLOCK IN
, IC = 2 Ã— BLOCK IN
, Otherwise

SSOC =

SSF O =

SSK =

ï£±
ï£²

(7)

ï£³

4

Following Eq. 7, an attacker can formulate the searching
space for the potential Wop-conifgs of Conv2Di. For a
speciï¬c Wop-conï¬g, the size of its Opt-conï¬g searching space
can be derived from Eq. 8, again we use the subscript b to
represent the detailed value of each parameter out of many
possibilities.
ï£±

(cid:7)

SSf ihb = SSf iwb = (cid:6)log2
SSicb = (cid:6)log2
SSocb|OC = (cid:6)log2

IC
BLOCK IN

F I
4

(cid:7) + 1

OC
BLOCK OU T

SSvt = max(vt)

ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

Assuming SSc = SSK Ã—SSF OÃ—SSf ihb Ã—SSf iwb Ã—SSicb Ã—
SSvt, the searching space SSConv2Di of Conv2Di can be
derived using Eq. 9:

SSConv2Di = SSc Ã—

SSocb|OC

(9)

(cid:88)

OCâˆˆâ„¦OC

where â„¦OC denotes the corresponding values in the searching
space of OC. For example, as shown in Tab. I, the 2nd Conv2D
layer has IC = 64, thus SSOC = 5 (BLOCK IN and
BLOCK OU T are set as 16 in Sec. III-C2), and the speciï¬c
values â„¦OC = {16, 32, 64, 128, 256} with possible SSocb|OC
is 1, 2, 3, 4, 5, respectively. Considering the derivation
SSc = 864 for the 2nd Conv2D layer in VGG-19, thus the
searching space for this layer is SSConv2D2 = 12960.

2) Searching space for the entire neural network: An
attacker needs to iterate all possible Wop-conï¬gs and their
Opt-conï¬gs, to compare the guessed EM leakage with the
obtained NN EM leakage starting from the 1st Conv2D layer.
We can narrow the search space based on two facts: (1)
Multiple Conv2D layers exist in one NN with the same Wop-
conï¬g; (2) A speciï¬c Opt-conï¬g may be suitable for different
Wop-conï¬gs to execute effectively. A strong and practical
scenario is that an attacker only tries to utilize the prior
knowledge before s/he really has to search the entire space.
For example, an attacker is always applying the recovered
Wop-conï¬g and Opt-conï¬g of the previous layer ï¬rst. In
other words, s/he has to checks all these already-discovered
Wop-conï¬g (SGwop) and Opt-conï¬g sets (SGopt), only if the
previous knowledge is not working. When both tests fail, s/he
iterates other possible conï¬gurations. We show an example in
Tab. I column AutoTVM Opt-conï¬g, where e3 and e4 employ
the same Opt-conï¬g. We calculate the searching space for each
Conv2D layers and derive the N N searching space SSN N ,
which also represents the basic security level.

l
(cid:88)

SSN N =

SSConv2Di

(10)

i=1

2

3) EM obfuscation scheme: For defense, one can use EM
obfuscation. In details, a designer can follow Eq. 6 to calculate
the OpCGEMM in each Conv2D layer and ï¬nd all potential
EM obfuscation Wop-conï¬gs and Opt-conï¬g, using Eq. 3
and 4. According the combination theory, the best choice of
(cid:7), where l is the number
layers to apply EM obfuscation is (cid:6) l
total layers. Therefore, the designer can randomly select 8 lay-
ers out of the 16 layers of VGG-19 to obfuscate. Consequently,
the searching space of brute force attack will be increased to a
huge number, since the Conv2D conï¬gurations reï¬‚ected in the
EM trace does not help with reverse engineering at all. Further,
as the Eq. 2 shows, S can be changed (elongated), which will
affect the similarity calculation in brute force attack. It is hard
for an attacker to ï¬nd a unique correct Wop-conï¬g for the
current layer. If they ignore S and only compare M , N , and
wc, it will induce many possible Wop-conï¬gs.

(cid:7) + 1

(8)

B. NNReArch Framework

NNReArch is an automated tensor program generator that
can mitigate the DNN-architecture-relevant EM side-channel

TABLE I: VGG-19 under balance mode with different scheduling

ei

Conv2Di

1
2
3
4
5

6

7

8

9

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

IC

16
64
64
128
128

256

256

512

FI

224
224
112
112
56

56

28

28

Independent
SSConv2Di
2592
12960
9000
16000
10240

Wop-conï¬g
Ground Truth
[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]

AutoTVM
Opt-conï¬g
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 2, 8, 4, 2]
[1, 2, 4, 2, 2]

16000

[256, 256, 3, 56, 56]

[1, 2, 4, 2, 2]

9000

[256, 512, 3, 28, 28]

[1, 1, 1, 1, 2]

12960

[512, 512, 3, 28, 28]

[1, 4, 4, 1, 2]

512

14

5760

[512, 512, 3, 14, 14]

[1, 4, 1, 1, 2]

AutoTVM
SSConv2Di
2592
12960
9000
20
10240
20
1
1
9000
12960
1
1
5760
1
1
1

NNReArch
Opt-conï¬g
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 1, 4, 4, 2]
[1, 2, 4, 2, 2]
[1, 2, 8, 2, 2]
[1, 2, 4, 2, 2]
[1, 1, 8, 2, 2]
[1, 1, 1, 1, 2]
[1, 1, 4, 1, 2]
[1, 2, 4, 1, 2]
[1, 4, 4, 1, 2]
[1, 4, 1, 1, 2]
[1, 2, 1, 1, 2]
[1, 2, 1, 2, 2]
[1, 1, 1, 2, 2]

NNReArch
SSConv2Di
2592
12960
9000
16000
10240
16000
16000
16000
9000
12960
12960
12960
5760
5760
5760
5760

SSN N
All Conv2D Execution Time (ms)

62559
1670.02

169712
1721.19

low-level NN execution codes. In addition, they can also trade
off the security and performance through these constraints.
Two Opt-conï¬g selecting modes are provided for users: bal-
ance mode and secure mode. In balance mode, NNReArch
selects the high-performance Opt-conï¬g while ensuring the
architecture conï¬dentiality level. Speciï¬cally, the generator
will ï¬rst select the high-performance Opt-conï¬g. Secure mode
prioritizes security, by randomly choosing Opt-conï¬gs and
a certain number of layers to apply the EM obfuscation, in
order to signiï¬cantly enlarging searching space. In Fig. 6, the
â€œBasic security level constraintâ€ is related to maximizing the
SSN N in both modes. The â€œPerformance constraintâ€ controls
the balance mode to satisfy the performance requirement,
i.e., runtime overhead. The â€œEM obfuscation constraintâ€ is
customized by the user, who only needs to provide the number
of layers to obfuscate, and NNReArch will generate low-level
candidate code to satisfy all constraints.

V. EVALUATION AND DISCUSSION

In this section, we evaluate the performance of NNReArch,
and compare it with AutoTVM, using the most commonly
utilized DNN architectures,
including VGG-16, VGG-19,
ResNet-18, and ResNet-34. For sake of clarity, we list all
possible workload conï¬guration (i.e., ground truth) of the
Conv2D layers of VGG-19 in Tab. II. For example, â€œei = 1â€
represents the 1st type of Conv2D layer, and its corresponding
â€œNO. of usage = 1â€ means that this layer type is only used
once in VGG-19. From the brute force attack perspective, if
given the correct IC and F I, the search space SSConv2D1 of
Conv2D 1 is 2592. Therefore, the attacker will have to iterate
entire SSConv2D1, to ï¬nd out the best matching workload.

We present the technical detail of deploying NNReArch on
VGG-19 as an example, and illustrate the experimental results
of all other DNN architectures in Fig. 7.

A. Applying NNReArch on VGG-19

We ï¬rstly apply the balance mode of NNReArch on VGG-
19, which considers the tradeoff between security and per-
formance, and the results are shown in Tab. I. Speciï¬cally,

Fig. 6: NNReArch framework design overview.

TABLE II: All possible workload conï¬guration of Conv2D
layers in VGG-19 applied in NNReArch to build the Opt-
conï¬g & Performance Database (Fig. 6).

ei

1
2
3
4
5
6
7
8
9

Wop-conï¬g

[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]
[256, 256, 3, 56, 56]
[256, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 512, 3, 14, 14]

Psei
size
61
183
219
292
316
395
315
378
216

NO. of
Usage
1
1
1
1
1
3
1
3
4

leakage from the target device. Fig. 6 shows the workï¬‚ow of
NNReArch. In step 1(cid:13), we implement the operator extraction.
The input is an abstracted DAG NN architecture, and the out-
put is the operator expression list, as shown in Tab. II column
ei. Each of these operators is for certain Conv2D layers with
different conï¬gurations. Through the TVM compiler we can
generate different Opt-conï¬gs for a speciï¬c ei workload, as
shown in the column Psei size, the number of compilable Opt-
conï¬gs (P sei). We evaluate their performance on the target de-
vice in step 2(cid:13). We record all P sei and their corresponding ex-
ecution time in an â€œOpt-conï¬g & Performance Databaseâ€. Step
3(cid:13) denotes the proposed NNReArch, where the user may apply
three constraints shown in 4(cid:13) to generate the corresponding

Operator extraction1NNReArchgenerator3Final configurationTarget DeviceNeural Network (NN) ArchitectureComputational graphOpt-config & Performance DatabaseBasic security level constraintPerformance evaluation2Performance constraintEM obfuscation constraint4TABLE III: NNReArch applied on 8 Conv2D layers with
secure mode (EM obfuscation)

ei

Conv2Di

1
2
3
4
5

6

7

8

9

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

NNReArch
Wop-conï¬g
[16, 64, 3, 224, 224]
[64, 64, 3, 224, 224]
[64, 128, 3, 112, 112]
[128, 128, 3, 112, 112]
[128, 256, 3, 56, 56]
[256, 64, 3, 56, 112]1
[64, 256, 3, 112, 112]2
[256, 256, 3, 112, 56]3
[256, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 512, 3, 28, 28]
[512, 2048, 3, 28, 14]4
[2048, 512, 3, 7, 7]5
[512, 2048, 3, 7, 7]6
[2048, 512, 3, 7, 7]7
[512, 512, 3, 7, 14]8

All Conv2D Execution Time (ms)

NNReArch
Opt-conï¬g
[1, 2, 32, 4, 2]
[1, 1, 8, 8, 2]
[1, 2, 8, 4, 2]
[1, 1, 4, 4, 2]
[1, 2, 4, 2, 2]
[1, 2, 8, 2, 2]
[1, 2, 4, 2, 2]
[1, 1, 8, 2, 2]
[1, 1, 1, 1, 2]
[1, 1, 4, 1, 2]
[1, 2, 4, 1, 2]
[1, 4, 4, 1, 2]
[1, 4, 1, 1, 2]
[1, 2, 1, 1, 2]
[1, 2, 1, 2, 2]
[1, 1, 1, 2, 2]
1927.59

we apply the security constraint as â€œmaximizing the Secbâ€
and the performance constraint as â€œshortest execution timeâ€.
Note that this evaluation does not include the EM obfuscation.
The performance of applying NNReArch is mainly shown in
column NNReArch Opt-conï¬g in Tab. I. The total execution
time of all VGG-19 Conv2D layers is 1721.19ms. Compared
with the original performance using â€œAutoTVMâ€ scheduling
(column AutoTVM Opt-conï¬g in Tab. I), the deployment
of NNReArch only incurs 3.06% performance overhead. In
the searching space (SSConv2Di ) is signiï¬cantly
contrast,
increased, with SSN N = 169712. Thus,
in terms of the
balance mode, applying NNReArch (w/o EM obfuscation)
increases the difï¬culty of DNN architecture extraction for
about 2.71 times.

We further apply the secure mode of NNReArch (i.e., with
EM obfuscation) to prioritize the DNN model architecture
conï¬dentiality. Speciï¬cally, we select 8 layers of VGG-19
to schedule their tensor program, making them to have the
same EM trace characteristics. The Opt-conï¬g setting of
the obfuscated workload are listed in Tab. III, and here we
use superscripts (1,2,...,8) to annotate the obfuscated layers.
Compared with the ground truth conï¬guration (column Wop-
conï¬g Ground Truth in Tab. I), the obfuscated EM traces
will lead the reverse engineering attack to wrong workload.
For example, the EM obfuscation breaks the performing diver-
gence of the EM leakage on different convolution layers, when
the attacker reasons each Conv2D layer, i.e., s/he will derive
wrong workloads that negatively affect the guess on followed
layers, disturbing the consistency of adjacent Conv2D layers.
As a result, when the exteriors of different Wop-conï¬gs have
the same patterns, attackers have to handle a huge amount of
puzzles and guesses. Thus the searching space and attacking
effort will increase massively.

As discussed in Sec. III-F, the EM obfuscation needs to
add pause opcodes to obfuscate the stall time, to make the
obfuscated layer performing almost the same as the â€œimitated
layerâ€, which may brings performance loss. In our experimen-
tal evaluation on VGG-19 shown in Tab. III, the obfuscated

DNN model consumes 1927.59 ms to execute all Conv2D lay-
ers, which has an approximate 15.42% performance overhead
compared with the AutoTVM.

B. Discussion: security and performance trade-off

(a) Security level measurement

(b) Performance measurement

Fig. 7: Performance evaluation and comparison between Au-
toTVM and the NNReArch balance mode of NNReArch.

Since most existing DNN development frameworks (in-
target at
cluding both open-source and commercial) still
therefore, we evaluate the balance mode of
performance,
NNReArch on the popular DNN architectures from VGG and
ResNet to draw generic conclusions. We ï¬rst compare the
searching space (SSN N ) of DNNs from the same family. For
example, VGG-16 and VGG-19 are constructed by the same
Conv2D layer types, but only with different number of layers
of the same Wop-conï¬g. Therefore, the attacking difï¬culty for
DNNs in the same family is almost equal using AutoTVM.
As afï¬rmed in Fig. 7a, the searching space is the same for
VGG-16 and VGG-19, as well as for ResNet-18 and ResNet-
34. This indicates a vulnerability of these performance-only
optimization frameworks, i.e., using a larger DNN model does
not make it more challenging to reverse engineer the model
architecture. In contrast, the proposed NNReArch framework
constructively leverages the model size to maximizes the
searching space of each Conv2D layer, making the architecture
of deeper DNNs more secure, as shown in Fig. 7a. Fig. 7b
illustrates the performance comparison between AutoTVM
and NNReArch, which demonstrates that NNReArch only
incurs trivial execution time overhead on all evaluated DNN
architectures compared with AutoTVM.

For the secure mode of NNReArch, we demonstrate that
a carefully crafted DNN model (e.g., the one in Tab. III),
can signiï¬cantly challenge the model extraction attacks with
relatively higher performance loss (15%). Moreover, the secure
mode enables the designer to either randomly choose Conv2D
layers for EM obfuscation, or apply unexpected stall time to
break the association between the EM side-channel leakage
and workload conï¬guration, thus providing more ï¬‚exibility to
the DNN model security enhancement.

VGG-16VGG-19ResNet-18ResNet-34012Searching space#105AutoTVMNNReArch (w/o EMob.)VGG-16VGG-19ResNet-18ResNet-340500100015002000Execution time (ms)channel information leakage,â€ in 2020 IEEE Interna-
tional Symposium on Hardware Oriented Security and
Trust (HOST).

IEEE, 2020, pp. 209â€“218.

[10] T. Zhou, Y. Zhang, S. Duan, Y. Luo, and X. Xu, â€œDeep
neural network security from a hardware perspective,â€ in
2021 IEEE/ACM International Symposium on Nanoscale
Architectures (NANOARCH).

IEEE, 2021, pp. 1â€“6.

[11] T. Chen, â€œPynq-z1 tvm-vta core conï¬guration.â€ [Online].
Available: https://github.com/apache/tvm-vta/blob/mast
er/conï¬g/pynq sample.json

[12] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and
N. Papernot, â€œHigh accuracy and high ï¬delity extraction
of neural networks,â€ in 29th {USENIX} Security Sympo-
sium ({USENIX} Security 20), 2020, pp. 1345â€“1362.
[13] L. Batina, S. Bhasin, D. Jap, and S. Picek, â€œ{CSI}{NN}:
Reverse engineering of neural network architectures
28th
through
{USENIX} Security Symposium ({USENIX} Security
19), 2019, pp. 515â€“532.

electromagnetic

channel,â€

side

in

[14] S. Tian, S. Moini, A. Wolnikowski, D. Holcomb,
R. Tessier, and J. Szefer, â€œRemote power attacks on
the versatile tensor accelerator in multi-tenant fpgas,â€
in 2021 IEEE 29th Annual International Symposium
on Field-Programmable Custom Computing Machines
(FCCM).

IEEE, 2021, pp. 242â€“246.
[15] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y. Ji, X. Xie,
Y. Ding, C. Liu, T. Sherwood et al., â€œDeepsniffer:
A dnn model extraction framework based on learning
architectural hints,â€ in Proceedings of the Twenty-Fifth
International Conference on Architectural Support for
Programming Languages and Operating Systems, 2020,
pp. 385â€“399.

[16] J. H. Anderson and F. N. Najm, â€œPower estimation
techniques for fpgas,â€ IEEE Transactions on Very Large
Scale Integration (VLSI) Systems, vol. 12, no. 10, pp.
1015â€“1027, 2004.

[17] â€œAaronia pbs2 e & h near ï¬eld probe set sniffer dc to
6ghz with emc preampliï¬er,â€ https://instrumentcenter.eu/
products/emc-products/probes/aaronia-pbs2-e-h-near-fie
ld-probe-set-sniffer-dc-to-6ghz-with-emc-preamplifier,
(Accessed on 06/06/2021).

[18] â€œTeledyne lecroy - oscilloscope,â€ https://teledynelecroy

.com/oscilloscope/, (Accessed on 06/08/2021).

VI. CONCLUSION

In this paper, we study the association between DNN model
architecture conï¬guration and EM side-channel leakage. Using
an open-source deep-learning accelerator VTA as our ex-
perimental platform, we discover the low-level code causes
of the EM side-channel-enabled DNN architecture reverse
engineering attacks. Furthermore, we present NNReArch, a
DNN model architecture defense framework against side-
channel attacks. Enabling ï¬‚exible DNN conï¬guration between
performance and security, NNReArch integrates two modes,
balance mode that targets at increasing the searching space of
DNN model architectures, and secure mode that employs EM
obfuscation to cancel the difference between different model
layers. Different from the existing solutions, the proposed
framework is built on popular open-source DNN compilation
tools, VTA, making it a generic defense method.

REFERENCES

[1] N. P.

Jouppi, C. Young, N. Patil, D. Patterson,
G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,
A. Borchers et al., â€œIn-datacenter performance analysis
of a tensor processing unit,â€ in Proceedings of the 44th
annual international symposium on computer architec-
ture, 2017, pp. 1â€“12.

[2] Dpu for convolutional neural network. https://www.xili

nx.com/products/intellectual-property/dpu.html.

[3] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen,
M. Cowan, L. Wang, Y. Hu, L. Ceze et al., â€œ{TVM}:
An automated end-to-end optimizing compiler for deep
learning,â€ in 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), 2018,
pp. 578â€“594.

[4] Y.-H. Chen, J. Emer, and V. Sze, â€œEyeriss: A spatial ar-
chitecture for energy-efï¬cient dataï¬‚ow for convolutional
neural networks,â€ ACM SIGARCH Computer Architec-
ture News, vol. 44, no. 3, pp. 367â€“379, 2016.

[5] T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen,
Z. Xu, O. Temam, and Y. Chen, â€œDadiannao: A neural
network supercomputer,â€ IEEE Transactions on Comput-
ers, vol. 66, no. 1, pp. 73â€“88, 2016.

[6] T. Moreau, T. Chen, Z. Jiang, L. Ceze, C. Guestrin,
and A. Krishnamurthy, â€œVta: an open hardware-
software stack for deep learning,â€ arXiv preprint
arXiv:1807.04188, 2018.

[7] Y. Luo, C. Gongye, Y. Fei, and X. Xu, â€œDeepstrike:
Remotely-guided fault injection attacks on dnn accel-
erator in cloud-fpga,â€ in 2021 58th ACM/IEEE Design
Automation Conference (DAC).
IEEE, 2021, pp. 295â€“
300.

[8] A. S. Rakin, Y. Luo, X. Xu, and D. Fan, â€œ{Deep-Dup}:
An adversarial weight duplication attack framework to
crush deep neural network in {Multi-Tenant}{FPGA},â€
in 30th USENIX Security Symposium (USENIX Security
21), 2021, pp. 1919â€“1936.

[9] H. Yu, H. Ma, K. Yang, Y. Zhao, and Y. Jin, â€œDeepem:
Deep neural networks model recovery through em side-

