2
2
0
2

b
e
F
8

]

G
L
.
s
c
[

1
v
5
9
3
5
0
.
2
0
2
2
:
v
i
X
r
a

Robust, Deep, and Reinforcement Learning for Management of
Communication and Power Networks

SUBMITTED TO THE FACULTY OF THE GRADUATE SCHOOL

A DISSERTATION

OF THE UNIVERSITY OF MINNESOTA

BY

Alireza Sadeghi

IN PARTIAL FULFILLMENT OF THE REQUIREMENTS

FOR THE DEGREE OF

DOCTOR OF PHILOSOPHY

Prof. Georgios B. Giannakis, Advisor

August, 2021

 
 
 
 
 
 
© Alireza Sadeghi 2021

ALL RIGHTS RESERVED

Acknowledgments

First and foremost, my deepest gratitude goes to my advisor Prof. Georgios B. Giannakis

for providing me with the opportunity to be a part of SPiNCOM research group, as well as

ECE/CS graduate program of University of Minnesota. He has helped me in developing clear

and scientiﬁc thought and expression, and without his support, the completion of this PhD Thesis

would not have been possible. His vision and enthusiasm about innovative research and beyond,

his broad and deep knowledge, and his unbounded energy have constantly been a true inspiration

for me. Because of him, I have been very fortunate to be always surrounded by other wonderful

students and colleagues.

Due thanks go to Profs. Mostafa Kaveh, Zhi-Li Zhang, and Mehmet Akc¸akaya for agreeing

to serve on my committee as well as all their valuable comments and feedback on my research

and thesis. Thanks also go to other professors in the Departments of Electrical Engineering and

Computer Science whose graduate level courses helped me build the necessary background to

embark on this journey.

During my PhD studies, I had the opportunity to collaborate with several excellent individuals,

and I have greatly beneﬁted from their critical thinking, brilliant ideas, and vision. Particularly,

I would like to express my greatest gratitude to my friend and collaborator Dr. Fatemeh

Sheikholeslami who was patient enough to train me in the ﬁrst couple of difﬁcult years at UMN,

and Prof. Gang Wang, with whom I was fortunate to collaborate with and learn from. I greatly

beneﬁted from his vision, ideas, and insights. I would also like to extend my due credit and

warmest thanks to Prof. Antonio G. Marques (King Juan Carlos University) for his valuable

contribution and insights to our fruitful collaborations. The material in this thesis has also

beneﬁted from collaborating with Qiuling Yang.

I would like to extend my gratitude to current and former members of the SPiNCOM group

at UMN: Dr. Siavash Ghavami, Dr. Brian Baingana, Dr. Yanning Shen, Dr. Dimitris Berberidis,

i

Dr. Jia Chen, Dr. Meng Ma, Prof. Tianyi Chen, Dr. Vassilis Ioannidis, Dr. Georgios V.

Karanikolas, Dr. Donghoon Lee, Dr. Athanasios Nikolakopoulos, Dr. Panagiotis Traganitis,

Prof. Daniel Romero, Dr. Liang Zhang, and Seth Barrash. I am truly grateful to these people for

their continuous help. I would also wish to acknowledge the grants that support ﬁnancially our

research.

I am not forgetting my friends, some of which I have already mentioned above, both the ones

here in Minneapolis, and my old-term friends that are far away, in particular: Danial Panahandeh-

Shahraki, Amirhossein Hosseini, Mojtaba Kadkhodaei Elyaderani, Mohsen Mahmoodi, Fazel

Zare Bidoky, Abolfazl Zamanpour Kiasari, Hamed Mosavat, Mehrdad Damsaz, Movahed

Jamshidi, Javad Ansari, Meysam Mohajer, and Ali Ghaffarpour.

Finally, gift of a family is incomparable. They are the source of my strength, motivation, and

sustenance. A special feeling of gratitude to my loving mother Fattaneh, whose heart I know is

sick of my long distance. Special thanks also goes to my lovely sister Maedeh, who never left

my side. I am eternally grateful to my mother, who encouraged me to pursue academic endeavor.

Without you, I would not be standing here today.

Alireza Sadeghi, Minneapolis, April, 2021.

ii

Dedication

This dissertation is dedicated to my mother and sister for their unconditional love and support.

iii

Abstract

Data-driven machine learning algorithms have effectively handled standard learning tasks.

This has become possible due mainly to the abundance of data in today’s world, ranging from

century-long climate records, high-resolution brain maps, all-sky survey observations, to billions

of texts, images, videos, and search queries that we generate daily. By learning from big data,

the state-of-the-art machine learning algorithms have successfully dealt with abundant, high-

dimensional data in various domains. Although being capable of handling classical learning

tasks, they fall short in managing and optimizing the next-generation highly complex cyber-

physical systems, operational self-deriving cars, and self-surgical systems, which desperately

need ground-breaking control, monitoring, and decision making schemes that can guarantee

robustness, scalability, and situational awareness. For example, the emergence of multimedia

services and Internet-friendly portable devices, the advent of diverse networks of intelligent

nodes such as those deployed to monitor the smart power grid, and transportation networks will

transform the cyder-physical infrastructures to an even more complex and heterogeneous one.

To cope with the arising challenges in cyber-physical system control and management

using machine learning toolkits, the present thesis ﬁrst develops principled methods to make

generic machine learning models robust against distributional uncertainties and adversarial data.

Particular focus will be on parametric models where some training data are being used to learn a

parametric model. The developed framework is of high interest especially when training and

testing data are drawn from “slightly” different distribution. We then introduce distributionally

robust learning frameworks to minimize the worst-case expected loss over a prescribed ambiguity

set of training distributions quantiﬁed via Wasserstein distance. Later, we build on this robust

framework to design robust semi-supervised learning over graph methods.

The second part of this thesis aspires to mange and control cyber-physical systems using

machine learning toolkits. Especially, to fully unleash the potential of next-generation wired

and wireless networks, we design “smart” network entities using (deep) reinforcement learning

approaches. These network entities are capable of learning, tracking, and adapting to unknown

dynamics or environments such as spatio-temporal dynamics of popular contents, network

topologies, and diverse network resource allocation policies deployed across network entities.

These smart units are such that they can pro-actively store reusable information during off-the

peak periods and then reuse it during peak time instances.

iv

Finally, this thesis enhances the power system operation and control. Our contribution

is on sustainable distribution grids with high penetration of renewable sources and demand

response programs. To account for unanticipated and rapidly changing renewable generation

and load consumption scenarios, we speciﬁcally delegate reactive power compensation to both

utility-owned control devices (e.g., capacitor banks), as well as smart inverters of distributed

generation units with cyber-capabilities. We further advocate an architectural paradigm shift

from (stochastic) optimization-based power management to data-driven “learning-to-control”

the system with reinforcements, by coupling physics-based optimization schemes with deep

reinforcement learning (DRL) advances. Our dynamic control algorithms are scalable and

adaptive to real-time changes of renewable generation and load consumption. To further enhance

the situational awareness in power network, we also develop robust power system state estimation

solvers (PSSE). Our algorithms will leverage power network topology information, as well as

data-driven priors to learn more ﬁne tuned voltages across all the buses in the network from only

limited available information. Numerical tests of pertinent tasks are provided for each chapter of

this thesis. These tests well demonstrated the merits of the proposed algorithms.

v

Contents

Acknowledgments

Dedication

Abstract

List of Tables

List of Figures

1

Introduction

1.1 Motivation and Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Learning Robust against distributional uncertainties and adversarial data . . . .

1.3 Robust semi-supervised inference over graphs. . . . . . . . . . . . . . . . . . .

1.4 Deep- and reinforced-Learning for resource management

. . . . . . . . . . . .

1.5 Data-driven, reinforced, and robust learning approaches for smart power grid .

1.6 Thesis outline .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7 Notational Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Learning Robust against Distributional Uncertainties and Adversarial Data

2.1

Introduction .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Our Contribution .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Outline and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 Problem Statement

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5 Stochastic Proximal Gradient Descent with (cid:15)-accurate Oracle . . . . . . . . . .

vi

i

iii

iv

x

xi

1

1

2

3

4

6

7

7

9

9

10

11

12

17

2.5.1 Convergence of SPGD with (cid:15)-accurate oracle . . . . . . . . . . . . . .

2.6 Stochastic Proximal Gradient Descent-Ascent

. . . . . . . . . . . . . . . . . .

2.6.1 Convergence of SPGDA . . . . . . . . . . . . . . . . . . . . . . . . .

2.7 Distributionally Robust Federated Learning . . . . . . . . . . . . . . . . . . .

2.8 Numerical Tests .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.1

SPGD with (cid:15)-accurate oracle and SPGDA . . . . . . . . . . . . . . . .

2.8.2 Distributionally robust federated learning . . . . . . . . . . . . . . . .

2.9 Conclusions .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Distributionally Robust Semi-Supervised Learning Over Graphs

3.1

Introduction .

.

.

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Distributionally robust learning . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Graph neural networks

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 Experiments .

3.6

conclusions .

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Deep and Reinforced Learning for Network Resource Management

4.1

Introduction .

.

.

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Our Contribution .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Modeling and problem statement . . . . . . . . . . . . . . . . . . . . . . . . .

4.3.1 Cost functions and caching strategies

. . . . . . . . . . . . . . . . . .

4.3.2

Popularity proﬁle dynamics

. . . . . . . . . . . . . . . . . . . . . . .

4.3.3 Reinforcement learning formulation . . . . . . . . . . . . . . . . . . .

4.4 Optimality conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.1 Optimal caching via Q-learning . . . . . . . . . . . . . . . . . . . . .

4.5 Scalable caching .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.5.1 Learning Λ .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6 Numerical tests .

4.7 Conclusions .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.8 Deep Reinforcement Learning for Adaptive Caching in Hierarchical Content

Delivery Networks

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.9

Introduction .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

19

20

21

23

24

26

27

29

29

30

32

34

35

36

37

37

38

39

40

43

44

45

46

49

51

53

56

56

56

vii

4.9.1 This section . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.10 Modeling and Problem Statement

. . . . . . . . . . . . . . . . . . . . . . . .

4.11 Two-timescale Problem Formulation . . . . . . . . . . . . . . . . . . . . . . .

4.12 Reinforcement Learning for Adaptive Caching with Dynamic Storage Pricing .

4.13 Introduction .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.14 Operating conditions and costs . . . . . . . . . . . . . . . . . . . . . . . . . .

4.14.1 Variables and constraints . . . . . . . . . . . . . . . . . . . . . . . . .

4.14.2 Prices and aggregated costs

. . . . . . . . . . . . . . . . . . . . . . .

4.15 Optimal caching with time-varying costs . . . . . . . . . . . . . . . . . . . . .

4.15.1 Bellman equations for the per-content problem . . . . . . . . . . . . .

4.15.2 Marginalized value-function . . . . . . . . . . . . . . . . . . . . . . .

4.15.3 Value function in closed form . . . . . . . . . . . . . . . . . . . . . .

4.15.4 State-action value function (Q-function):

. . . . . . . . . . . . . . . .

4.15.5 Stochastic policies: Reinforcement learning . . . . . . . . . . . . . . .

4.16 Limited storage and back-haul transmission rate via dynamic pricing . . . . . .

4.16.1 Limiting the instantaneous storage rate

. . . . . . . . . . . . . . . . .

4.16.2 Limiting the long-term storage rate

. . . . . . . . . . . . . . . . . . .

4.16.3 Limits on the back-haul transmission rate . . . . . . . . . . . . . . . .

4.16.4 Modiﬁed online solver based on Q-learning . . . . . . . . . . . . . . .

4.17 Numerical tests .

4.18 Conclusions .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Data-driven, Reinforced, and Robust Learning Approaches for

a Smarter Power Grid

5.1

Introduction .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 Voltage Control in Two Timescales . . . . . . . . . . . . . . . . . . . . . . . .

5.2.1

System model . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . .

59

60

62

63

63

64

65

66

67

68

70

72

74

75

78

78

79

83

85

86

92

95

95

98

98

5.2.2 Two-timescale voltage regulation formulation . . . . . . . . . . . . . . 100

5.3 Fast-timescale Optimization of Inverters . . . . . . . . . . . . . . . . . . . . . 101

5.3.1 Branch ﬂow model

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . 101

5.3.2 Linearized power ﬂow model

. . . . . . . . . . . . . . . . . . . . . . 103

5.4 Slow-timescale Capacitor Reconﬁguration . . . . . . . . . . . . . . . . . . . . 104

viii

5.4.1 A data-driven solution . . . . . . . . . . . . . . . . . . . . . . . . . . 104

5.4.2 A deep reinforcement learning approach . . . . . . . . . . . . . . . . . 107

5.5 Numerical Tests .

5.6 Conclusions .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

5.7 Gauss-Newton Unrolled Neural Networks and

Data-driven Priors for Regularized PSSE with Robustness

. . . . . . . . . . . 117

5.8

Introduction .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

5.9 Background and Problem Formulation . . . . . . . . . . . . . . . . . . . . . . 119

5.9.1 Gauss-Newton Iterations . . . . . . . . . . . . . . . . . . . . . . . . . 120

5.10 Unrolled Gauss-Newton with Deep Priors . . . . . . . . . . . . . . . . . . . . 121

5.10.1 Regularized PSSE with Deep Priors . . . . . . . . . . . . . . . . . . . 121

5.10.2 Graph Neural Network Deep Prior . . . . . . . . . . . . . . . . . . . . 124

5.11 Robust PSSE Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

5.12 Numerical Tests .

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

5.12.1 Simulation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

5.12.2 GNU-GNN for regularized PSSE . . . . . . . . . . . . . . . . . . . . 131

5.12.3 Robust PSSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

5.13 Conclusions .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

6 Summary and Future Directions

136

6.1 Thesis Summary .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

6.2 Future Research .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

6.2.1 Multi-agent, distributionally robust decentralized RL . . . . . . . . . . 138

6.2.2 Robust learning approach to fairness in machine learning.

. . . . . . . 138

6.2.3 Communication- and computation-efﬁcient robust federated learning . 139

References

Appendix A. Proofs for Chapter 2

141

164

A.0.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

A.0.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

A.0.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

ix

List of Tables

4.1 Run-time of the proposed caching.

. . . . . . . . . . . . . . . . . . . . . . . .

4.2 Run-time of OGA caching.

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Run-time of Myopic caching. . . . . . . . . . . . . . . . . . . . . . . . . . . .

92

92

92

x

List of Figures

2.1 Misclassiﬁcation error rate for different training methods using MNIST dataset;

Left: FGSM attack, Middle: IFGSM attack, Right: PGD attack . . . . . . . . .

24

2.2 Misclassiﬁcation error rate for different training methods using F-MNIST dataset;

Left: FGSM attack, Middle: IFGSM attack, Right: PGD attack . . . . . . . . .

24

2.3 Distributionally robust federated learning for image classiﬁcation using the non-

i.i.d. F-MNIST dataset; Left: No attack, Middle: IFGSM attack, Right: PGD

attack .

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

2.4 Federated learning for image classiﬁcation using the MNIST dataset; Left: No

attack, Middle: IFGSM attack, Right: PGD attack . . . . . . . . . . . . . . . .

26

2.5 Distributionally robust federated learning for image classiﬁcation using F-MNIST

dataset; Left: No attack, Middle: IFGSM attack, Right: PGD attack . . . . . .

27

3.1 Performance during testing for both normal (a) - (b), and perturbed input features

(c) - (d).

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

4.1 A schematic depicting the evolution of key quantities across time slots. Duration

of slots can be unequal.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Global popularity Markov chain.

. . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Local popularity Markov chain. . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 Popularity proﬁles Markov chains.

. . . . . . . . . . . . . . . . . . . . . . . .

4.5 Performance of the proposed algorithms. . . . . . . . . . . . . . . . . . . . . .

4.6 Percentage of accommodated requests via cache.

. . . . . . . . . . . . . . . .

4.7 Convergence rate of the exact and scalable Q-learning.

. . . . . . . . . . . . .

4.8 Performance in large state-action space scenaria.

. . . . . . . . . . . . . . . .

4.9 A network of caching nodes.

. . . . . . . . . . . . . . . . . . . . . . . . . . .

4.10 A hierarchical tree network cache system.

. . . . . . . . . . . . . . . . . . . .

42

54

54

54

56

57

58

59

60

60

xi

4.11 Slow and fast time slots.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.12 Structure of slots and intervals.

. . . . . . . . . . . . . . . . . . . . . . . . . .

4.13 System model and main notation. The state variables (dashed lines) are the

t and λf

t and the content request rf

storage indicator sf
and fetching prices ρf
caching and fetching decisions af
ρf
t af
and decides the values of {af
but also the cost at time instants t(cid:48) > t.

t , as well as the dynamic caching
t . The optimization variables (solid lines) are the
t =
f =1,
f =1 considering not only the cost at time t
. . . . . . .

t . The instantaneous per-ﬁle cost is cf
t }F

t and wf
t . Per slot t, the SB collects the state variables {sf

. . . . . . . . . . . . . .

t + λf

t , wf

t , λf

t ; ρf

t , rf

t wf

t }F

. . . . . . . . . . . . . . . .

4.14 Average cost versus ¯ρ for different values of p, ¯λ.
4.15 Average cost versus p for different values of ¯λ, ¯ρ.
4.16 Caching ratio vs. ¯ρ and ¯λ for p = 0.5 and s = r = 1.
4.17 Caching ratio vs. ¯ρ and ¯λ for p = 0.05 and s = r = 1.
4.18 Performance of DP versus myopic caching for ¯λ = 53.
. . . . . . . . . . . . .
4.19 Average cost versus ¯ρ for different values of ¯λ, p. Solid line is for value iteration
. . . . . . . . . . . . . . .

while dashed lines are for Q-learning based solver.

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . .

. . . . . . . . . . . . .

.

4.20 Averaged immediate cost over 1000 realizations in a non-stationary setting, and

a sample from popularities.

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1 Two-timescale partitioning of a day for joint capacitor and inverter control.

. .

63

63

69

87

88

88

88

89

89

90

99

5.2 Bus i is connected to its unique parent πi via line i.
5.3 Deep Q-network .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

. . . . . . . . . . . . . . . 102

5.4 Schematic diagram of the 47-bus industrial distribution feeder. Bus 1 is the

substation, and the 6 loads connected to it model other feeders on this substation. 110

5.5 Time-averaged instantaneous costs incurred by the four voltage control schemes. 110

5.6 Voltage magnitude proﬁles obtained by the four voltage control schemes over

the simulation period of 10, 000 slots.

. . . . . . . . . . . . . . . . . . . . . . 111

5.7 Voltage magnitude proﬁles obtained by the four voltage control schemes at buses

10 and 33 from slot 9, 900 to 10, 000.

. . . . . . . . . . . . . .

. . . . . . . . 111

5.8 Voltage magnitude proﬁles at all buses at slot 9, 900 obtained by the four voltage

control schemes.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

5.9 Hyper deep Q-network for capacitor conﬁguration.

. . . . . . . . . . . . . . . 114

xii

5.10 Time-averaged instantaneous costs incurred by the four approaches on the IEEE

123-bus feeder.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

5.11 Voltage magnitude proﬁles at all buses over the simulation period of 25, 000

slots on the IEEE 123-bus feeder.

. . . . . . . . . . . . . . . . . . . . . . . . 115

5.12 Voltage magnitude proﬁles at buses 55 and 90 from slot 24, 900 to 25, 000

obtained by the four approaches on the IEEE 123-bus feeder.

. . . . . . . . . . 115

5.13 Voltage magnitude proﬁles at all buses on slot 24, 900 obtained by four ap-

proaches on the IEEE 123-bus feeder.

. . . . . . . . . . . . . . . . . . . . . . 116

5.14 The structure of the proposed GNU-NN. . . . . . . . . . . . . . . . . . . . . . 119

5.15 The signal diffuses from layer l − 1 to l with K = 3.

. . . . . . . . . . . . . . 125

5.16 The estimated voltage magnitudes and angles by the four schemes at bus 50 from

slots 70 to 90.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

5.17 The estimated voltage magnitudes and angles by the four schemes at bus 100

from slot 70 to 90. .

.

. . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . 132

5.18 The estimated voltages magnitudes and angles by the four schemes for the ﬁrst

20 buses at slot 80.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

5.19 The estimated voltage magnitudes and angles by the four schemes under distri-

butional attacks at bus 100 from slots 70 to 90. . . . . . . . . . . . . . . . . . . 134

5.20 The estimated voltage magnitudes and angles by the four schemes under distri-

butional attacks for the ﬁrst 20 buses at slot 80.

. . . . . . . . . . . . . . . . . 134

xiii

Chapter 1

Introduction

1.1 Motivation and Context

Nowadays, overcoming emerging engineering challenges in cyber-physical systems requires

successfully performing various learning tasks. At the same time, although machine learning

algorithms have been successful in dealing with standard learning tasks with the sheer volume

and high dimensionality of data, they are defenseless against adversarially manipulated input

data, and sensitive to dynamically changing environments. Recent advancements in non-linear

function approximation, optimal transport theory, and minimax optimization techniques provide

a timely opportunity to transform machine learning algorithms to a scalable, reliable, secure,

and safe technology to control and manage complex cyber-physical systems. In this context,

the present thesis aspires to develop principled methods incorporating scalability along with

robustness in machine learning paradigms, having as ultimate goal to enhance their prediction,

control, and tracking performance in unknown, dynamic, and possibly adversarial settings.

By putting forth an analytical and algorithmic framework for learning and inferring from

data, with applications in management of cyber-physical systems, this thesis will develop a

suit of machine learning based tools to optimally control these systems. our vision is to effect

technical advances in function approximation, machine learning, optimal transport theory, and

optimization to develop state-of-the-art algorithms for managing cyber-physical systems. The

central goal is to theoretically, algorithmically, and numerically developed online, robust, and

scalable algorithms. Speciﬁcally, the following research thrusts will be pursued:

1

(T1) Robust supervised learning under distributional uncertainties due to adversaries;

(T2) Distributionally robust semi-supervised learning and inference over graphs; and,

(T3) Deep- and reinforced-learning for network resource management;

(T4) Data-driven, reinforced, and robust learning approaches for smart power grid.

2

1.2 Learning Robust against distributional uncertainties and ad-

versarial data

It has been recently recognized that learning function models is vulnerable to adversarially

manipulated input data, which discourages their use in safety-critical applications. In addition,

learning algorithms often rely on the premise that training and testing data are drawn from

the same distribution, which may not hold in practice. Major efforts have been devoted to

robustifying learning models to uncertainties arising from e.g., distributional mismatch using data

pre-processing techniques such as compression, sparsiﬁcation, and variance minimization [72,

69, 133]. While these can handle structured outliers, they are challenged by adversarial attacks,

which can be mitigated by augmenting the training set with adversarially manipulated data

[68, 98, 124]. Despite their effectiveness, the latter fall short in performance guarantees, which

motivates distributionally robust alternatives that minimize the worst-case expected loss over

a prescribed ambiguity set of training distributions [23]. Means of quantifying uncertainty

include momentum, likelihood, Kullback-Leibler (KL), and the Wasserstein distance [48, 79, 6].

Unfortunately, all robust approaches so far result in suboptimal solvers. In this context, the

distributionally robust optimization framework is developed in this thesis for training a parametric

model, both in centralized and federated learning settings. The objective is to endow the

trained model with robustness against adversarially manipulated input data, or, distributional

uncertainties, such as mismatches between training and testing data distributions, or among

datasets stored at different workers. To this aim, the data distribution is assumed unknown, and

lies within a Wasserstein ball centered around the empirical data distribution. This robust learning

task entails an inﬁnite-dimensional optimization problem, which is challenging. Leveraging a

strong duality result, a surrogate is obtained, for which three stochastic primal-dual algorithms

are developed: i) stochastic proximal gradient descent with an (cid:15)-accurate oracle, which invokes

3

an oracle to solve the convex sub-problems; ii) stochastic proximal gradient descent-ascent,

which approximates the solution of the convex sub-problems via a single gradient ascent step;

and, iii) a distributionally robust federated learning algorithm, which solves the sub-problems

locally at different workers where data are stored. Compared to the empirical risk minimization

and federated learning methods, the proposed algorithms offer robustness with little computation

overhead. Numerical tests using image datasets showcase the merits of the proposed algorithms

under several existing adversarial attacks and distributional uncertainties.

1.3 Robust semi-supervised inference over graphs.

Inference tasks over social, brain, communication, biological, transportation, and sensor networks,

have well-documented success by capitalizing on inter-dependencies captured by graphs [178,

94]. In practice however, data are only available at a subset of nodes, due to e.g. sampling

costs, and computational or privacy constraints. As inference is desired across all network nodes,

such SSL tasks over networks can beneﬁt from the underlying graph topology [39, 17, 117].

Recent advances in graph neural networks (GNNs), offer parametric models that leverage the

topology-guided structure of network data to form nested architectures that conveniently express

processes over graphs [234, 208, 60].

By succinctly encoding local graph structures and features of nodes, state-of-the-art GNNs

can scale linearly with the size of graph. Despite their success in practice, most of existing

methods are unable to handle graphs with uncertain nodal attributes. Speciﬁcally whenever

mismatches between training and testing data distribution exists, these models fail in practice.

Challenges also arise due to distributional uncertainties associated with data acquired by noisy

measurements. For instance, small perturbations to input data could signiﬁcantly deteriorate

the regression performance or result in classiﬁcation error [239, 84], just to name a couple of

undesirable consequences. Hence, it is critical to endow learning and inference of processes over

graphs with robustness against distributional uncertainties and adversarial data, especially in

safety-critical applications, such as robotics [186] and transportation [233]. In this context, a

distributionally robust learning framework is developed, where the objective is to train models

that exhibit quantiﬁable robustness against perturbations. The data distribution is considered

unknown, but lies within a Wasserstein ball centered around empirical data distribution. A

robust model is obtained by minimizing the worst expected loss over this ball. However, solving

the emerging functional optimization problem is challenging, if not impossible. Advocating a

strong duality condition, we develop a principled method that renders the problem tractable and

efﬁciently solvable. Experiments assess the performance of the proposed method.

4

1.4 Deep- and reinforced-Learning for resource management

Consider the Internet, where millions of users rely on to access millions of terabyte of content

such as Netﬂix or Amazon movies, music, social media on a daily bases. Serving end users with

high quality of service in such huge-scale is no an easy task. In reality, to meet the ever-increasing

data demand, novel technologies are required. Recognized as a key component is the so-called

caching, which refers to storing reusable popular contents across geographically distributed

storage-enabled network entities. The rationale here is to alleviate unfavorable surges of data

trafﬁc by pro-actively storing anticipated highly popular contents at local storage devices during

off-peak periods. Such resource pre-allocation is envisioned to provide signiﬁcant savings in

terms of network resources such as energy, bandwidth, and cost, in addition to increased user

satisfaction. To fully unleash its potential, a content-agnostic caching entity needs to rely on

available observations to learn what and when to cache. A part of my research is to empower

next generation networks with “smart” caching units, capable of learning, tracking, and adapting

to unknown dynamics or environments such as spatio-temporal dynamics of content popularities,

network topologies, and diverse caching policies deployed across network entities. By leveraging

contemporary (deep) reinforcement learning tools, novel algorithms will be developed which are

capable of progressively improving network performance in online and decentralized settings.

Speciﬁcally we start with considering the caching problem in wireless networks, where

small basestations (SBs) equipped with caching units have potential to handle the unprecedented

demand growth in heterogeneous networks. Through low-rate, backhaul connections with the

backbone, SBs can prefetch popular ﬁles during off-peak trafﬁc hours, and service them to the

edge at peak periods. To intelligently prefetch, each SB must learn what and when to cache,

while taking into account SB memory limitations, the massive number of available contents,

the unknown popularity proﬁles, as well as the space-time popularity dynamics of user ﬁle

requests. In this work, local and global Markov processes model user requests, and a RL (RL)

framework is put forth for ﬁnding the optimal caching policy when the transition probabilities

involved are unknown. Joint consideration of global and local popularity demands along with

5

cache-refreshing costs allow for a simple, yet practical asynchronous caching approach. The

novel RL-based caching relies on a Q-learning algorithm to implement the optimal policy in

an online fashion, thus enabling the cache control unit at the SB to learn, track, and possibly

adapt to the underlying dynamics. To endow the algorithm with scalability, a linear function

approximation of the proposed Q-learning scheme is introduced, offering faster convergence as

well as reduced complexity and memory requirements. Numerical tests corroborate the merits of

the proposed approach in various realistic settings.

Then we build on this framework and consider a network of caches. In this context, dis-

tributing the limited storage capacity across network entities calls for decentralized caching

schemes. Many practical caching systems involve a parent caching node connected to multiple

leaf nodes to serve user ﬁle requests. To model the two-way interactive inﬂuence between

caching decisions at the parent and leaf nodes, a RL framework is put forth. To handle the large

continuous state space, a scalable deep RL approach is pursued. The novel approach relies on a

hyper-deep Q-network to learn the Q-function, and thus the optimal caching policy, in an online

fashion. Reinforcing the parent node with ability to learnand- adapt to unknown policies of leaf

nodes as well as spatiotemporal dynamic evolution of ﬁle requests, results in remarkable caching

performance, as corroborated through numerical tests.

Finally, we design adaptive caching mechanism wedding tools from optimization and RL.

We introduce simple but ﬂexible generic time-varying fetching and caching costs, which are

then used to formulate a constrained minimization of the aggregate cost across ﬁles and time.

Since caching decisions per time slot inﬂuence the content availability in future slots, the novel

formulation for optimal fetch-cache decisions falls into the class of dynamic programming.

Under this generic formulation, ﬁrst by considering stationary distributions for the costs as well

as ﬁle popularities, an efﬁcient RL-based solver known as value iteration algorithm can be used

to solve the emerging optimization problem. Then, it is shown that practical limitations on cache

capacity can be handled using a particular instance of this generic dynamic pricing formulation.

Under this setting, to provide a light-weight online solver for the corresponding optimization,

the well-known RL algorithm, Q-learning, is employed to ﬁnd optimal fetch-cache decisions.

Numerical tests corroborating the merits of the proposed approach.

6
1.5 Data-driven, reinforced, and robust learning approaches for

smart power grid

Given solar generation and load consumption predictions, voltage and reactive power control aims

at optimizing reactive power injections to minimize a certain loss (e.g., power, voltage deviations),

while respecting physical and operating constraints. Proper redistribution of reactive power

sources can result in local correction of the power factor, increase system capacity, and improve

power quality. Traditionally, reactive compensation is provided by utility-owned equipment such

as tap-changing under load transformers, voltage regulators, and manually-controlled capacitor

banks [170], whose slow responses and limited lifespan render them ineffective in dealing with

the variability introduced by distributed energy resources. Advances in smart power inverters

offer new opportunities, which can provide fast and continuously-valued reactive power injection

or consumption. Methods for compensating reactive power using the inverters of PV and storage

systems have been advocated in [57, 227, 89, 237, 195, 88, 111, 125]. Unfortunately, joint control

of both traditional utility-owned devices as well as contemporary smart inverters is challenging

and has not been explored thus far, primarily because they operate in different timescales (e.g.,

hourly versus every few seconds), and involve discrete and continuous actions. In addition, several

fundamental challenges remain. How should one split the reactive power compensation duty

equitably between the smart inverters and traditionally utility devices? Should the control law be

centralized (potentially vulnerable), distributed (more robust), or hybrid? Whether centralized

or decentralized, what variables should be used as inputs to the control algorithms? e.g., what

should be the states, actions, and rewards of a RL algorithm? The main challenge arise due to

the fact that the discrete on-off commitment of capacitor units is often conﬁgured on an hourly

or daily basis, yet smart inverters can be controlled within milliseconds, thus challenging joint

control of these two types of assets. In this context, a novel two-timescale voltage regulation

scheme is developed for distribution grids by judiciously coupling data-driven with physics-based

optimization. On a faster timescale, say every second, the optimal setpoints of smart inverters

are obtained by minimizing instantaneous bus voltage deviations from their nominal values,

based on either the exact alternating current power ﬂow model or a linear approximant of it;

whereas, on the slower timescale (e.g., every hour), shunt capacitors are conﬁgured to minimize

the long-term discounted voltage deviations using a deep RL algorithm. Extensive numerical

tests on a real-world 47-bus distribution network as well as the IEEE 123-bus test feeder using

7

real data corroborate the effectiveness of the novel scheme. Finally, we ﬁnish this thesis by

considering fas and robust state estimation (SE) to maintain a comprehensive view of the system

in real time. Conventional PSSE solvers typically entail minimizing a nonlinear and nonconvex

least-squares cost using e.g., the Gauss- Newton method. Those iterative solvers however, are

sensitive to initialization, and may converge to local minima. To overcome these hurdles, this

thesis adapts and leverages recent advances on image denoising to introduce a PSSE approach

with a regularizer capturing a deep neural network (DNN) prior. For the resultant regularized

PSSE objective, a “Gauss- Newton-type” alternating minimization solver is developed. To

accommodate real-time monitoring, a novel end-to-end DNN is constructed subsequently by

unrolling the proposed alternating minimization solver. The deep PSSE architecture can further

account for the power network topology through a graph neural network (GNN) based prior. To

further endow the physics-based DNN with robustness against bad data, an adversarial DNN

training method is put forth. Numerical tests using real load data on the IEEE 118-bus benchmark

system showcase the improved estimation and robustness performance of the proposed scheme

compared with several state-of-the-art alternatives.

1.6 Thesis outline

The reminder of this thesis is organized as follows. Chapter 2 puts forth distributionally robust

supervised and federated learning methods. Chapter 3 builds on the developed distributionally

robust supervised learning framework to arrive at a distributionally robust semi-supervised

learning over graphs. Chapter 4 deals with deep and RL approaches to manage limited network

resources. Finally, the objective of Chapter 5 is to design efﬁcient learning approaches for

smart grid management and control. Finally Chapter 6 presents a concluding discussion of the

proposed approaches, along with future research directions.

1.7 Notational Conventions

Unless otherwise noted, the following notation will be used throughout the subsequent chapters.

Lower- (upper-) case boldface letters denote vectors (matrices). Calligraphic letters are reserved

for sets, e.g., S. For vectors, (cid:107)·(cid:107)2 or (cid:107)·(cid:107) represents the Euclidean norm, while (cid:107)·(cid:107)0 denotes the
(cid:96)0 pseudo-norm counting the number of nonzero entries. The n × n identity matrix is denoted by

In, and all-one vector by 1, and all-zero vector 0. The size of the matricies (vectors) is omitted
if it is obvious from the context; otherwise it is indicated by a subscript. Operator (·)(cid:62) stands for

matrix transpose, | · | the cardinality of a set, or the absolute value of a number.

8

Chapter 2

Learning Robust against Distributional

Uncertainties and Adversarial Data

2.1 Introduction

Machine learning models and tasks hinge on the premise that the training data are trustworthy,

reliable, and representative of the testing data. In practice however, data are usually generated

and stored at geographically distributed devices (a.k.a., workers) each equipped with limited

computing capability, and adhering to privacy, conﬁdentiality, and possibly cost constraints

[106]. Furthermore, the data quality is not guaranteed due to adversarially generated examples

and distribution drifts across workers or from the training to testing phases [108]. Visually

imperceptible perturbations to a dermatoscopic image of a benign mole can render the ﬁrst-ever

artiﬁcial intelligence (AI) diagnostic system approved by the U.S. Food and Drug Administration

in 2018, to classify it as cancerous with 100% conﬁdence [59]. A stranger wearing pixelated

sunglasses can fool even the most advanced facial recognition software in a home security system

to mistake it for the homeowner [174]. Hackers indeed manipulated readings of ﬁeld devices and

control centers of the Ukrainian supervisory control and data acquisition system to cause the ﬁrst

ever cyberattack-caused power outage in 2015 [36, 206]. Examples of such failures in widely

used AI-enabled safety- and security-critical systems today could put national infrastructure and
even lives at risk. 1

1Results of this Chapter are published in [169]

9

10

Recent research efforts have focused on devising defense strategies against adversarial

attacks. These strategies fall under two groups: attack detection, and attack recovery. The former

identiﬁes whether a given input is adversarially perturbed [71, 116], while the latter trains a

model to gain robustness against such adversarial inputs [72, 172], which is also the theme of

the present contribution. To robustify learning models against adversarial data, a multitude of

data pre-processing schemes have been devised [133, 175], to identify anomalies not adhering

to postulated or nominal data. Adversarial training on the other hand, adds imperceptible well-

crafted noise to clean input data to gain robustness [68]; see also e.g., [124, 136, 144], and

[38] for a recent survey. In these contributions, optimization tasks are formulated to craft

adversarial perturbations. Despite their empirical success, solving the resultant optimization

problems is challenging. Furthermore, analytical properties of these approaches have not been

well understood, which hinders explainability of the obtained models. In addition, one needs

to judiciously tune hyper parameters of the attack model, which tends to be cumbersome in

practice.

On the other hand, data are typically generated and/or stored at geographically distributed

sites, each having subsets of data with different distributions. While keeping data localized to

e.g., respect privacy, as well as reduce communication- and computation-overhead, the federated

learning (FL) paradigm targets a global model, whereby multiple devices are coordinated by

a central parameter server [106]. Existing FL approaches have mainly focused on the com-

municating versus computing tradeoff by aggregating model updates from the learners; see

e.g., [129, 107, 202, 176] and references therein. From the few works dealing with robust FL,

[109] learns from dependent data through e.g., sparsiﬁcation, while [95] entails an ensemble

of untrusted sources. These methods are rather heuristic, and rely on aggregation to gain ro-

bustness. This context, motivates well a principled approach that accounts for the uncertainties

associated with the underlying data distributions.

2.2 Our Contribution

Tapping on a distributionally robust optimization perspective, this Chapter develops robust

learning procedures that respect privacy and ensure robustness to distributional uncertainties and

adversarial attacks. Independent, identically distributed (i.i.d.) samples can be drawn from the

known data distribution. Building on [180], the adversarial input perturbations are constrained to

11

lie in a Wasserstein ball, and the sought robust model minimizes the worst-case expected loss over

this ball. As the resulting formulation leads to a challenging inﬁnite-dimensional optimization

problem, we leverage strong duality to arrive at a tractable and equivalent unconstrained mini-

mization problem, requiring solely the empirical data distribution. To solve the latter, a stochastic

proximal gradient descent (SPGD) algorithm is developed based on an (cid:15)-accurate oracle, along

with its lightweight stochastic proximal gradient descent-ascent (SPGDA) iteration. The ﬁrst

algorithm relies on the oracle to solve the emerging convex sub-problems to (cid:15)-accuracy, while

the second simply approximates its solution via a single gradient ascent step. To accommodate

communication constraints and private or possibly untrusted datasets distributed across multiple

workers, we further develop a distributionally robust federated learning (DRFL) algorithm. In a

nutshell, the main contributions of this Chapter are as follows.

• A regularized distributionally robust learning framework to endow machine learning

models with robustness against adversarial input perturbations;

• Two efﬁcient proximal-type distributionally robust optimization algorithms with ﬁnite-

sample convergence guarantees; and,

• A distributionally robust federated learning implementation to account for untrusted and

possibly anonymized data from distributed sources.

2.3 Outline and notation

Bold lowercase letters denote column vectors, while calligraphic uppercase fonts are reserved
for sets; E[·] represents expectation; ∇ denotes the gradient operator; (·)(cid:62) denotes transposition,

and (cid:107)x(cid:107) is the 2-norm of the vector x.

The rest of this Chapter is structured as follows. Problem formulation and its robust surro-

gate are the subjects of Section 2.4. The proposed SPGD with (cid:15)-accurate oracle and SPGDA

algorithms with their convergence analyses are presented in Sections 2.5 and 2.6, respectively.

The DRFL implementation is discussed in Section 2.7. Numerical tests are given in Section 2.8

with conclusions drawn in Section 2.9. Technical proofs are deferred to the Appendix.

2.4 Problem Statement

Consider the standard regularized statistical learning task

Ez∼P0

(cid:2)(cid:96)(θ; z)(cid:3) + r(θ)

min
θ∈Θ

12

(2.1)

where (cid:96)(θ; z) denotes the loss of a model parameterized by the unknown parameter vector θ

on a datum z = (x, y) ∼ P0, with feature x and label y, drawn from some nominal distribution
P0. Here, Θ denotes the feasible set for model parameters. To prevent over ﬁtting or incorporate
prior information, regularization term r(θ) is oftentimes added to the expected loss. Popular
regularizers include r(θ) := β(cid:107)θ(cid:107)2
2, where β ≥ 0 is a hyper-parameter controlling the

1 or β(cid:107)θ(cid:107)2

importance of the regularization term relative to the expected loss.

In practice, the nominal distribution P0 is typically unknown. Instead, we are given some
(a.k.a. training data), which are drawn i.i.d. from P0. Upon
in (2.1), we arrive at the empirical

data samples {zn}N
replacing P0 with the so-called empirical distribution (cid:98)P (N )
loss minimization

n=1 ∼ (cid:98)P (N )

0

0

¯E

min
θ∈Θ

(cid:2)(cid:96)(θ; z)(cid:3) + r(θ)

z∼ (cid:98)P (N )
0

(2.2)

where ¯E
n=1(cid:96)(θ; zn). Indeed, a variety of machine learning tasks can
be cast as (2.2), including e.g., ridge and Lasso regression, logistic regression, and reinforcement

[(cid:96)(θ; z)]=N −1 (cid:80)N

z∼ (cid:98)P (N )
0

learning. The resultant models obtained by solving (2.2) however, have been shown vulnerable
to adversarially corrupted data in (cid:98)P (N )
from the available (cid:98)P (N )
distributions corresponding to perturbations of the underlying data distribution, has led to the

. For this reason, targeting an adversarially robust model against a set of

. Furthermore, the testing data distribution often deviates

0

0

formulation [180]

min
θ∈Θ

sup
P ∈P

Ez∼P [(cid:96)(θ; z)] + r(θ)

(2.3)

where P represents a set of distributions centered around the data generating distribution (cid:98)P (N )
Compared with (2.1), the worst-case formulation (2.3), yields models ensuring reasonable

0

.

performance across a continuum of distributions characterized by P. In practice, different types

of ambiguity sets P can be considered, and they lead to different robustness guarantees and

computational requirements. Popular choices of P include momentum [48, 203], KL divergence

13

[79], staatistical test [7], and Wasserstein distance-based ambiguity sets [7, 180]; see e.g., [23]

for a recent overview. Among all choices, it has been shown that the Wasserstein ambiguity set

P results in a tractable realization of (2.3), thanks to the strong duality result of [7] and [180],

which also motivates this work.

To formalize this, consider two probability measures P and Q supported on set Z, and
let Π(P, Q) be the set of all joint measures supported on Z 2, with marginals P and Q. Let

c : Z × Z → [0, ∞) measure the cost of transporting a unit of mass from z in P to another
element z(cid:48) in Q. The celebrated optimal transport problem is given by [190, page 111]

Wc(P, Q) := inf
π∈Π

Eπ

(cid:2)c(z, z(cid:48))(cid:3).

(2.4)

Remark 1. If c(·, ·) satisﬁes the axioms of distance, then Wc deﬁnes a distance on the space of
probability measures. For instance, if P and Q are deﬁned over a Polish space equipped with
metric d, then choosing c(z, z(cid:48)) = dp(z, z(cid:48)) for some p ∈ [1, ∞) asserts that W 1/p
the well-known Wasserstein distance of order p between probability measures P and Q [190,

(P, Q) is

c

Deﬁnition 6.1].

For a given empirical distribution (cid:98)P (N )

, deﬁne the uncertainty set P := {P |Wc(P, (cid:98)P (N )

0
ρ} to include all probability distributions having at most ρ-distance from P (N )
this ambiguity set into (2.3), yields the following reformulation

0

0
. Incorporating

) ≤

min
θ∈Θ

sup
P

Ez∼P [(cid:96)(θ; z)] + r(θ)

s.t. Wc(P, (cid:98)P (N )

0

) ≤ ρ.

(2.5a)

(2.5b)

Observe that the inner supremum in (2.5a) runs over all joint probability measures π on Z 2

implicitly characterized by (2.5b). Intuitively, directly solving this optimization over the inﬁnite-

dimensional space of distribution functions is challenging, if not impossible. Fortunately, for a

broad range of losses as well as transport costs, it has been shown that the inner maximization

satisﬁes a strong duality condition [23]; that is, the optimal objective of this inner maximization

and its Lagrangian dual optimal objective, are equal. In addition, the dual problem involves

optimization over a one-dimensional dual variable. These two observations make it possible to

solve (2.3) in the dual domain. To formally obtain a tractable surrogate to (5.37), we make the

following assumptions.

Assumption 1. The transportation cost function c : Z ×Z → [0, ∞), is a lower semi-continuous
function satisfying c(z, z) = 0 for z ∈ Z 2.

Assumption 2. The loss function (cid:96) : Θ × Z → [0, ∞), is upper semi-continuous, and integrable.

The following proposition provides a tractable surrogate for (5.37), whose proof can be found in

[23, Theorem 1].

14

Proposition 1. Let (cid:96) : Θ × Z → [0, ∞), and c : Z × Z → [0, ∞) satisfy Assumptions 1 and 2,
respectively. Then, for any given (cid:98)P (N )

, and ρ > 0, it holds that

0

sup
P ∈P

Ez∼P [(cid:96)(θ; z)] = inf
γ≥0

(cid:8)¯E

z∼ (cid:98)P (N )
0

(cid:2) sup
ζ∈Z

{(cid:96)(θ; ζ) − γ(c(z, ζ) − ρ)} (cid:3)(cid:9)

(2.6)

where P := (cid:8)P |Wc(P, (cid:98)P (N )

0

) ≤ ρ(cid:9).

Remark 2. Thanks to strong duality, the right-hand side in (5.38) simply is a univariate dual

reformulation of the primal problem represented in the left-hand side. In sharp contrast with the
primal formulation, the expectation in the dual domain is taken only over the empirical (cid:98)P (N )
rather than any P ∈ P. In addition, since this reformulation circumvents the need for ﬁnding the

0

optimal π ∈ Π to form P, and characterizing the primal objective ∀P ∈ P, it is practically more

convenient.

Upon relying on Proposition 3, the following distributionally robust surrogate is obtained

min
θ∈Θ

inf
γ≥0

(cid:8)¯E

z∼ (cid:98)P (N )
0

(cid:2) sup
ζ∈Z

{(cid:96)(θ; ζ)+γ(ρ − c(z, ζ))} + r(θ)(cid:3)(cid:9).

(2.7)

Remark 3. The robust surrogate in (5.39) boils down to minimax (saddle-point) optimization

which has been widely studied in e.g., [110]. However, (5.39) requires the supremum to be solved

separately for each sample z, and the problem cannot be handled through existing methods.

A relaxed (hence suboptimal) version of (5.39) with a ﬁxed γ value has recently been studied

in [180]. Unfortunately, one has to select an appropriate γ value using cross validation over a grid

search that is also application dependent. Heuristically choosing a γ does not guarantee optimality

in solving the distributionally robust surrogate (5.39). Clearly, the effect of heuristically selecting

γ is more pronounced when training deep neural networks. Instead, we advocate algorithms that

optimize γ and θ simultaneously.

2A simple example satisfying these constraints is the Euclidean distance c(z, z(cid:48)) = (cid:107)z − z(cid:48)(cid:107).

15

Our approach to addressing this, relies on the structure of (5.39) to iteratively update param-
eters ¯θ := [θ(cid:62) γ](cid:62) and ζ. To end up with a differentiable function of ¯θ after maximizing over
ζ, Danskin’s theorem requires the sup-problem to have a unique solution [18]. For this reason,

we design the inner maximization to involve a strongly concave objective function through the
selection of a strongly convex transportation cost, such as c(z, z(cid:48)) := (cid:107)z − z(cid:48)(cid:107)2
p for p ≥ 1. For
the maximization over ζ to rely on a strongly concave objective, we let γ ∈ Γ := {γ|γ > γ0},
where γ0 is large enough. Since γ is the dual variable corresponding to the constraint in (5.37),
having γ ∈ Γ is tantamount to tuning ρ, which in turn controls the level of robustness. Replacing

γ ≥ 0 in (5.39) with γ ∈ Γ, our robust learning model is obtained as the solution of

min
θ∈Θ

inf
γ∈Γ

¯E

z∼ (cid:98)P (T )
0

(cid:2) sup
ζ∈Z

ψ(¯θ, ζ; z)(cid:3) + r(¯θ)

(2.8)

where ψ(¯θ, ζ; z) := (cid:96)(θ; ζ) + γ(ρ − c(z, ζ)). Intuitively, input z in (5.40) is pre-processed by
maximizing ψ accounting for the adversarial perturbation. To iteratively solve our objective in

(5.40), the ensuing sections provide efﬁcient solvers under some mild conditions. Those include

cases, every inner maximization (supremum) can be solved to (cid:15)-optimality by an oracle.

Before developing our algorithms, we make several standard assumptions; see also [180],

[110].

Assumption 3. Function c(z, ·) is Lc-Lipschitz and µ-strongly convex for any given z ∈ Z,
with respect to the norm (cid:107) · (cid:107).

Assumption 4. The loss function (cid:96)(θ; z) obeys the following Lipschitz smoothness conditions

(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lθθ(cid:107)θ − θ(cid:48)(cid:107)
(cid:107)∇θ(cid:96)(θ; z) − ∇θ(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lθz(cid:107)z − z(cid:48)(cid:107)
(cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ; z(cid:48))(cid:107)∗ ≤ Lzz(cid:107)z − z(cid:48)(cid:107)
(cid:107)∇z(cid:96)(θ; z) − ∇z(cid:96)(θ(cid:48); z)(cid:107)∗ ≤ Lzθ(cid:107)θ − θ(cid:48)(cid:107)

(2.9a)

(2.9b)

(2.9c)

(2.9d)

and it is continuously differentiable with respect to θ.

Assumption (4) guarantees that the supremum in (5.39) results in a smooth function of ¯θ;
thus, one can execute gradient descent to update θ upon solving the supremum. This will further

help to provide convergence analysis of our proposed algorithms. To elaborate more on this, the

Algorithm 1 SPGD with (cid:15)-accurate oracle
Input: Initial guess ¯θ0, step size sequence {αt > 0}T
t = 1, . . . , T Draw i.i.d samples {zn}N
Find (cid:15)-optimizer ζ(cid:15)( ¯θt; zn) via the oracle
Update:
¯θt+1 = proxαtr

n=1 ∇¯θψ(¯θ, ζ(cid:15)(¯θt; zn); zn)(cid:12)

(cid:104)¯θt − αt

(cid:80)N

n=1

N

(cid:105)

(cid:12)¯θ=¯θt

t=0, (cid:15)-accurate oracle

16

following lemma characterizes the smoothness and gradient Lipschitz properties obtained upon

solving the maximization problem in (5.40).

Lemma 1. For each z ∈ Z, deﬁne ¯ψ(¯θ; z) = supζ ψ(¯θ, ζ; z) with ζ∗(¯θ; z) = arg maxζ∈Z ψ(¯θ, ζ; z).
Then ¯ψ(·) is differentiable, and its gradient is ∇¯θ
following conditions hold

¯ψ(¯θ; z) = ∇¯θψ(¯θ, ζ∗(¯θ; z); z). Moreover, the

(cid:13)ζ∗(¯θ1; z) − ζ∗(¯θ2; z)(cid:13)
(cid:13)

(cid:13) ≤

Lzθ
λ
¯ψ(¯θ2; z)(cid:13)
LθzLzθ + LcLzθ
λ

(cid:13) ≤
(cid:17)

(cid:107)θ2 − θ1(cid:107) +

LθzLc + L2
c
λ

(cid:107)θ2 − θ1(cid:107)

¯ψ(¯θ1; z) − ∇¯θ

(cid:13)
(cid:13)∇¯θ
(cid:16)

+

Lθθ +

Lc
λ

(cid:107)γ2 − γ1(cid:107)

(cid:107)γ2 − γ1(cid:107)

(2.10a)

(2.10b)

(2.10c)

where γ1,2 ∈ Γ, and ψ(¯θ, ·; z) is λ-strongly concave.

Proof: See Appendix A.0.1 for the proof.

Lemma 1 paves the way for iteratively solving the surrogate optimization (5.40), intuitively

because it guarantees a differentiable and smooth objective upon solving the inner supremum to

its optimum.

Remark 4. Equation (2.10a) is appealing in practice. Indeed, if ¯θt = [θt, γt] is updated with a
small enough step size, the corresponding ζ∗(θt+1; z) is close enough to ζ∗(θt; z). Building
on this observation, instead of using an oracle to ﬁnd the optimum ζ∗(θt+1; z), an (cid:15)-accurate
solution ζ(cid:15)(θt+1; z) sufﬁces to obtain comparable performance. This also circumvents the need
to ﬁnd the optimum for the inner maximization per iteration, which could be computationally

demanding.

17
2.5 Stochastic Proximal Gradient Descent with (cid:15)-accurate Oracle

A standard solver of regularized optimization problems is the proximal gradient algorithm. In

this section, we develop a variant of it to tackle the robust surrogate (5.40). For convenience, let

us deﬁne

f (θ, γ) := E (cid:2) sup
ζ∈Z

{(cid:96)(θ; ζ)+γ(ρ − c(z, ζ))} (cid:3)

and rewrite our objective as

min
θ∈Θ

inf
γ∈Γ

F (θ, γ) := f (θ, γ) + r(θ)

(2.11)

(2.12)

where f (θ, γ) is the smooth function in (2.11), and r(·) is a non-smooth and convex regularizer,
such as the (cid:96)1-norm. With a slight abuse of notation, upon introducing ¯θ := [θ γ], we deﬁne
f (¯θ) := f (θ, γ) and F (¯θ) := F (θ, γ).

The proximal gradient algorithm the updates ¯θt as

¯θt+1 = arg min
θ

αtr(θ) + αt

(cid:10)θ − ¯θt, g(¯θt)(cid:11) +

(cid:13)θ − ¯θt(cid:13)
(cid:13)
2
(cid:13)

1
2

where g(¯θt) := ∇f (¯θ)|¯θ= ¯θt, and αt > 0 is some step size. The last update is expressed in the
compact form

¯θt+1 = proxαtr

(cid:2)¯θt − αtg(¯θt)(cid:3)

where the proximal gradient operator is given by

proxαr[v] := arg min

θ

αr(θ) +

(cid:107)θ − v(cid:107)2.

1
2

(2.13)

(2.14)

The working assumption is that this optimization problem can be solved efﬁciently using off-the-

shelf solvers.

Starting from the guess ¯θ0, the proposed SPGD with (cid:15)-accurate oracle executes two steps
per iteration t = 1, 2, . . .. First, it relies on an (cid:15)-accurate maximum oracle to solve the inner
problem supζ∈Z{(cid:96)(θt; ζ)−γtc(z, ζ)} for randomly drawn samples {zn}N
n=1 to yield (cid:15)-optimal
ζ(cid:15)(¯θt, zn) with the corresponding objective values ψ(¯θt, ζ(cid:15)(¯θt, zn); zn). Next, ¯θt is updated

18

using a stochastic proximal gradient step as

¯θt+1 = proxαtr

(cid:104)¯θt −

αt
N

N
(cid:88)

n=1

∇¯θψ(¯θ, ζ(cid:15)(¯θt; zn); zn)

(cid:105)
.

For implementation, the proposed SPGD algorithm with (cid:15)-accurate oracle is summarized in

Alg. 1. Convergence performance of this algorithm is analyzed in the ensuing subsection.

2.5.1 Convergence of SPGD with (cid:15)-accurate oracle

In general, the postulated model is nonlinear, and the robust surrogate F (¯θ) is nonconvex. In this
section, we characterize the convergence performance of Alg. 1 to a stationary point. However,

lack of convexity and smoothness implies that stationary points must be understood in the
sense of the Fr`echet subgradient. Speciﬁcally, the Fr`echet subgradient ∂F (ˇθ) for the composite
optimization in (2.12), is the set [152]

∂F (ˇθ) :=

(cid:110)
v (cid:12)

(cid:12) lim
¯θ→ˇθ

inf

F (¯θ) − F (ˇθ) − v(cid:62)(¯θ − ˇθ))
(cid:107)¯θ − ˇθ(cid:107)

(cid:111)
.

≥ 0

Consequently, the distance between vector 0 and the set ∂F (ˇθ) is a measure characterizing
whether a point is stationary or not. To this end, deﬁne the distance between a vector v and a set

S as dist(v, S) := mins∈S (cid:107)v − s(cid:107), and the notion of δ-stationary points as deﬁned next.

Deﬁnition 1. Given a small δ > 0, we call vector ˇθ a δ-stationary point if and only if
dist(0, ∂F (ˇθ)) ≤ δ.

Since f (·) in (2.11) is smooth, we have that ∂F (¯θ) = ∇f (¯θ) + ∂r(¯θ) [152]. Hence, it

sufﬁces to prove that the algorithm converges to a δ-stationary point ˇθ satisfying

dist(cid:0)0, ∇f (ˇθ) + ∂r(ˇθ)(cid:1) ≤ δ.

(2.15)

We further adopt the following assumption that is standard in stochastic optimization.

Assumption 5. Function f satisﬁes the next two conditions.

1. Gradient estimates are unbiased and have a bounded variance, i.e., E[g∗(¯θt)−∇f (¯θt)] =

0, and there is a constant σ2 < ∞, so that E[(cid:107)∇f (¯θt)−g∗(¯θt)(cid:107)2

2] ≤ σ2.

2. Function f (¯θ) is smooth with Lf -Lipschitz continuous gradient, i.e.,

(cid:107)∇f (¯θ1) − ∇f (¯θ2)(cid:107) ≤ Lf (cid:107)¯θ1 − ¯θ2(cid:107).

19

We are now ready to claim the convergence guarantees for Alg. 1; see Appendix A.0.2 for

the proof.

Theorem 1. Let Alg. 2 run for T iterations with constant step sizes α, η > 0. Under Assumptions
1–5, Alg. 2 generates a sequence of {¯θt} that satisﬁes

E [dist(0, ∂F (¯θt(cid:48)

))2] ≤

+

+ β

(cid:17) ∆F
(cid:16) 2
T
α
(β + 2)L2
¯θz(cid:15)
λ0

+

(cid:17)

σ2

+ 2

(cid:16) β
η

(2.16)

where t(cid:48) is uniformly sampled from {1, . . . , T }; here, ∆F := F (¯θ0) − F (¯θT +1); L2
L2
θz + λ0Lc, and β, λ0 > 0 are some constants.

¯θz :=

Theorem 1 asserts that {¯θt}T

t=1 generated by Alg. 1 converges to a stationary point on
average. The upper bound here is characterized by the initial error ∆F , which decays at the rate
of O(1/T ); and, the constant bias terms induced by the gradient estimate variance σ2 as well as

the oracle accuracy (cid:15).

Remark 5 (Oracle implementation). The (cid:15)-accurate oracle can be implemented in practice by

several optimization algorithms, with gradient ascent being a desirable one due to its simplic-

ity. Assuming γ0 ≥ Lzz/µ, gradient ascent with constant step size η obtains an (cid:15)-accurate
solution within at most O(log(d2

0/(cid:15)η)) iterations, where d0 is the diameter of set Z.

The computational complexity of Alg. 1 can grow prohibitively when dealing with large-size

datasets and complex models. This motivates lightweight, scalable, yet efﬁcient methods. To this

end, we introduce next a stochastic proximal gradient descent-ascent (SPGDA) algorithm.

2.6 Stochastic Proximal Gradient Descent-Ascent

Leveraging the strong concavity of the inner maximization problem and Lemma 1, a lightweight

variant of the SPGD with (cid:15)-accurate oracle is developed here. Instead of optimizing the inner

maximization problem to (cid:15)-accuracy by an oracle, we approximate its solution after only a single
gradient ascent step. Speciﬁcally, for a batch of data {zt
m=1 per iteration t, our SPGDA

m}M

Algorithm 2 SPGDA
Input: Initial guess ¯θ0, step size sequence {αt, ηt > 0}T
For t = 1, . . . , T Draw a batch of i.i.d samples {zm}M
m=1
m + ηt∇ζψ(¯θt, ζ; zt
Find {ζt
Update: ¯θt+1 = proxαtr
m; zt

m=1 via gradient ascent: ζt
(cid:80)M

m = zt
m=1 ∇¯θψ(¯θt, ζt

(cid:104)¯θt − αt

m}M

m)(cid:12)

(cid:12)¯θ=¯θt

M

t=0, batch size M

m)(cid:12)
(cid:105)

(cid:12)ζ=zt

m

, m = 1, . . . , M

20

algorithm ﬁrst perturbs each datum via a gradient ascent step

m = zt
ζt

m + ηt∇ζψ(¯θt, ζ; zt

m)(cid:12)

(cid:12)ζ=zt

m

, ∀m = 1, . . . , M

(2.17)

and then forms

gt(¯θt) :=

1
M

M
(cid:88)

∇¯θψ(¯θ, ζt

m; zt

m)(cid:12)

(cid:12)¯θ=¯θt.

m=1

Using (2.18), an extra proximal gradient step is taken to obtain

¯θt+1 = proxαtr

(cid:2)¯θt − αtgt(¯θt)(cid:3).

(2.18)

(2.19)

The SPGDA steps are summarized in Alg. 2. Besides its simplicity and scalability, SPGDA

enjoys convergence to a stationary point as elaborated next.

2.6.1 Convergence of SPGDA

To prove convergence of Alg. 2, let us start by deﬁning

g∗(¯θt) :=

1
M

M
(cid:88)

m=1

∇¯θψ∗(¯θt, ζ∗

m; zt

m).

(2.20)

Different from (2.18), the gradient here is obtained at the optimum ζ∗

m. To establish convergence,

one more assumption is needed.

Assumption 6. Function f satisﬁes the following conditions.

1) Gradient estimates ∇¯θψ∗(¯θt, ζ∗

m; zm) at ζ∗

That is, for m = 1 · · · M , we have E [∇¯θψ∗(θ, ζ∗
∇f (θ)(cid:107)2] ≤ σ2.

m are unbiased and have bounded variance.
m; zm)−∇¯θf (θ)] = 0 and E [(cid:107)∇¯θψ∗(θ, ζ∗

m; zm)−

2) The expected norm of gt(¯θ) is bounded, that is, E(cid:107)gt(¯θ)(cid:107)2 ≤ B2.

21

We now present a theorem on the convergence of Alg. 2; see Appendix A.0.3 for the proof.

Theorem 2 (Convergence of Alg. 2). Let ∆F := F (¯θ0)−inf ¯θ F (¯θ), and D denote the diameter
of the feasible set Θ. Under As. 1–4 and 6, for a constant step size α > 0, and a ﬁxed batch size

M > 0, after T iterations, Alg. 2 satisﬁes

E(cid:2)dist(0, ∂F (¯θT ))2(cid:3) ≤

υ
T + 1

∆F +

4σ2
M

+

2L2
θzν
M

(cid:2)(1 − αµ) D2 + α2B2(cid:3)

(2.21)

where υ, ν, and µ = γ0 − Lzz are some positive constants.

Theorem 2 implies that the sequence {¯θt}T

t=1 generated by Alg. 2 converges to a stationary
point. The upper bound in (2.21) is characterized by a vanishing term induced by initial error

∆F , and constant bias terms.

2.7 Distributionally Robust Federated Learning

In practice, massive datasets are distributed geographically across multiple sites, where scalability,

data privacy and integrity, as well as bandwidth scarcity typically discourage uploading them to

a central server. This has propelled the so-called federated learning framework, where multiple

workers exchange information with a server to learn a centralized model using data locally

generated and/or stored across workers [129, 108, 106, 41]. Workers in this learning framework

communicate iteratively with the server. Albeit appealing for its scalability, one needs to carefully

address the bandwidth bottleneck associated with server-worker links. Furthermore, the workers’

data may have (slightly) different underlying distributions, which further challenges the learning

task. To seek a model robust to distribution drifts across workers, we will adapt our novel SPGDA

approach to design a privacy-respecting and robust algorithm.

To that end, consider K workers with each worker k ∈ K collecting samples {zn(k)}N
n=1. A
globally shared model parameterized by θ is to be updated at the server by aggregating gradients

computed locally per worker. For simplicity, we consider workers having the same number

of samples N . The goal is to learn a single global model from stored data at all workers by

minimizing the following objective function

min
θ∈Θ

¯Ez∼ (cid:98)P [(cid:96)(θ; z)] + r(θ)

22

(2.22)

where ¯Ez∼ (cid:98)P [(cid:96)(θ; z)] := 1
k=1 (cid:96)(θ, zn(k)). To endow the learned model with robust-
ness against distributional uncertainties, our novel formulation will solve the following problem

n=1

N K

(cid:80)N

(cid:80)K

in a distributed fashion

min
θ∈Θ

sup
P ∈P

Ez∼P [(cid:96)(θ; z)] + r(θ)

s. to. P :=

(cid:110)

P

(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

k=1

(cid:111)
Wc(P, (cid:98)P (N )(k)) ≤ ρ

(2.23)

where Wc(P, (cid:98)P (N )(k)) denotes the Wasserstein distance between distribution P and the local
(cid:98)P (N )(k), per worker k.

Clearly, the constraint P ∈ P, couples the optimization in (6.3) across all workers. To offer

distributed implementations, we resort to Proposition 3, to arrive at the equivalent reformulation

min
θ∈Θ

inf
γ∈Γ

K
(cid:88)

k=1

¯Ez(k)∼ (cid:98)P (N )(k)

(cid:2) sup
ζ∈Z

{(cid:96)(θ; ζ)

+ γ(ρ − c(z(k), ζ))} (cid:3) + r(θ).

(2.24)

Next, we present our communication- and computation-efﬁcient DRFL that builds on the

SPGDA scheme in Sec. 2.6.

Speciﬁcally, our DRFL hinges on the fact that with ﬁxed server parameters ¯θt := [θt(cid:62), γt](cid:62)
per iteration t, the optimization problem becomes separable across all workers. Hence, upon
receiving ¯θt from the server, each worker k ∈ K:
i) samples a minibatch Bt(k) of data
from (cid:98)P (N )(k); ii) forms the perturbed loss ψk(¯θt, ζ; z) := (cid:96)(θt; ζ) + γt(ρ − c(z, ζ)) for
each z ∈ Bt(k); iii) lazily maximizes ψk(¯θt, ζ; z) over ζ using a single gradient ascent
step to yield ζ(¯θt; z) = z + ηt∇ζψk(¯θt, ζ; z)|ζ=z; and, iv) sends the stochastic gradient
|Bt(k)|−1 (cid:80)
(cid:12)¯θ=¯θt back to the server. Upon receiving all local
gradients, the server updates ¯θt using a proximal gradient descent step to ﬁnd ¯θt+1, that is

z∈Bt(k) ∇¯θψk(¯θt, ζ(¯θt; z); z)(cid:12)

¯θt+1 = proxαtr


¯θt −

αt
K

K
(cid:88)

k=1

1
|Bt(k)|

×

(cid:88)

∇¯θψk(¯θt, ζ(¯θt; z); z)(cid:12)

(cid:12)¯θ=¯θt

z∈Bt(k)





(2.25)

23

Algorithm 3 DRFL
Input: Initial guess ¯θ1, a set of workers K with data samples {zn(k)}N
step size sequence {αt, ηt > 0}T
Output: ¯θT +1
Fort = 1, . . . , T
Each worker:
Samples a minibatch Bt(k) of samples
Given ¯θt and z ∈ Bt(k), forms local perturbed loss

t=1

n=1 per worker k ∈ K,

ψk(¯θt, ζ; z) := (cid:96)(¯θt; ζ) + γt(ρ − c(z, ζ))

Lazily maximizes ψk(¯θt, ζ; z) over ζ to ﬁnd

ζ(¯θt; z) = z + ηt∇ζψk(¯θt, ζ; z)|ζ=z

Computes stochastic gradient

1
|Bt(k)|

(cid:88)

∇¯θψk(¯θt, ζ(¯θt; z); z)(cid:12)

(cid:12)¯θ=¯θt

z∈Bt(k)

and uploads to server
Server:
Updates ¯θt according to (2.25)
Broadcasts ¯θt+1 to workers

which is then broadcast to all workers to begin a new round of local updates. Our DRFL approach

is tabulated in Alg. 3.

2.8 Numerical Tests

To assess the performance in the presence of distribution drifts and adversarial perturbations, we

will rely on empirical classiﬁcation of standard MNIST and Fashion- (F-)MNIST datasets. Specif-

ically, we compare performance using models trained with empirical risk minimization (ERM),

the fast-gradient method (FGSM) [68], its iterated variant (IFGM) [98], and the Wasserstein

robust method (WRM) [180]. We further evaluate the testing performance using the projected

gradient descent (PGD) attack [124]. We ﬁrst test the performance of SPGD with (cid:15)-accurate

oracle, and the SPGDA algorithm on standard classiﬁcation tasks.

24

Figure 2.1: Misclassiﬁcation error rate for different training methods using MNIST dataset; Left:
FGSM attack, Middle: IFGSM attack, Right: PGD attack

Figure 2.2: Misclassiﬁcation error rate for different training methods using F-MNIST dataset;
Left: FGSM attack, Middle: IFGSM attack, Right: PGD attack

2.8.1 SPGD with (cid:15)-accurate oracle and SPGDA

The FGSM attack performs one step gradient update along the direction of the gradient’s sign to

ﬁnd an adversarial sample; that is,

xadv = Clip[−1,1]{x + (cid:15)advsign(∇(cid:96)x(θ; (x, y)))}

(2.26)

where (cid:15)avd controls the maximum (cid:96)∞ perturbation of adversarial samples. The element-wise
Clip[a,b]{} operator forces its input to reside in the prescribed range [−1, 1]. By running Tadv
iterations of (2.26) iterative (I) FGSM attack samples are generated [68]. Starting with an
initialization x0

adv = x, and considering the (cid:96)∞ norm, the PGD attack iterates [124]

xt+1

adv = ΠB(cid:15)(xt

adv)

adv + αsign(∇(cid:96)x(θ; (xt
xt

adv, y)))

(cid:110)

(cid:111)

(2.27)

0.00.20.40.60.81.0ε (FGSM)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 20.00.20.40.60.81.0 (IFGSM)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 212345678 (PGD)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 20.00.20.40.60.81.0ε (FGSM)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 20.00.20.40.60.81.0ε (IFGSM)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 212345678ε (PGD)0.00.20.40.60.81.0Classification errorERMWRMIFGSMAlg. 1Alg. 225

adv) := {x : (cid:107)x − xt

for Tadv steps, where Π denotes projection onto the ball B(cid:15)(xt
adv(cid:107)∞ ≤
(cid:15)adv}, and α > 0 is the stepsize set to 1 in our experiments. We use Tadv = 10 iterations for
all iterative methods both in training and attack samples. The PGD can also be interpreted
as an iterative algorithm that solves the optimization problem maxx(cid:48) (cid:96)(θ; (x(cid:48), y)) subject to
(cid:107)x(cid:48) − x(cid:107)(cid:96)∞ ≤ α. The Wasserstein attack on the other hand, generates adversarial samples
by solving a perturbed training loss with an (cid:96)2-based transportation cost associated with the
Wasserstein distance between the training and adversarial data distributions [180].

For the MNIST and F-MNIST datasets, a convolutional neural network (CNN) classiﬁer

consisting of 8 × 8, 6 × 6, and 5 × 5 ﬁlter layers with rectiﬁed linear units (ReLU) and the same

padding, is used. Its ﬁrst, second, and third layers have 64, 128, and 128 channels, respectively,

followed by a fully connected layer, and a softmax layer at the output.

CNNs with the same architecture are trained, using different adversarial samples. Speciﬁcally,

to train a Wasserstein robust CNN model (WRM), γ = 1 was used to generate Wasserstein

adversarial samples, (cid:15)adv was set to 0.1 for the other two methods, and ρ = 25 was used to
deﬁne the uncertainty set for both Algs. 1 and 2. Unless otherwise noted, we set the batch size

to 128, the number of epochs to 30, the learning rates to α = 0.001 and η = 0.02, and used the

Adam optimizer [92]. Fig. 2.1(Left) shows the classiﬁcation error on the MNIST dataset. The

error rates were obtained using testing samples generated according to the FGSM method with

(cid:15)adv. Clearly all training methods outperform ERM, and our proposed Algs. 1 and 2 offer
improved performance over competing alternatives. The testing accuracy of all methods using

samples generated according to an IFGSM attack is presented in Fig. 2.1(Middle). Likewise,

Algs. 1 and 2 outperform other methods in this case. Fig. 2.1(Right) depicts the testing accuracy

of the considered methods under different levels of a PGD attack. The plots in Fig. 2.1 showcase

the improved performance obtained by CNNs trained using Algs. 1 and 2.

The F-MNIST article image dataset is used in our second experiment. Similar to MNIST

dataset, each example in F-MNIST is also a 28 × 28 gray-scale image, associated with a

label from 10 classes. F-MNIST is a modern replacement for the original MNIST dataset for

benchmarking machine learning algorithms. Using CNNs with the same architectures as before,

the classiﬁcation error is depicted for different training methods in Fig. 2.2. Three different

attacks, namely FGSM, IFGSM, and PGD are used during testing. The proposed SPGD and

SPGDA algorithms outperform the other methods, verifying the superiority of Algs. 1 and 2 in

terms of yielding robust models.

26

Figure 2.3: Distributionally robust federated learning for image classiﬁcation using the non-i.i.d.
F-MNIST dataset; Left: No attack, Middle: IFGSM attack, Right: PGD attack

(c)
t

Figure 2.4: Federated learning for image classiﬁcation using the MNIST dataset; Left: No attack,
Middle: IFGSM attack, Right: PGD attack

2.8.2 Distributionally robust federated learning

To validate the performance of our DRFL algorithm, we considered an FL environment con-

sisting of a server and 10 workers, with local batch size 64, and assigned to every worker an

equal-sized subset of training data containing i.i.d. samples from 10 different classes. All workers

participated in each communication round. To benchmark the DRFL, we simulated the federated

averaging method [129]. The testing accuracy on the MNIST dataset per communication round

using clean (normal) images is depicted in Fig. 2.4. Clearly, both DRFL and federated averaging

algorithms exhibit reasonable performance when the data is not corrupted. The performance is

further tested against IFGSM and PGD attacks with a ﬁxed (cid:15)adv = 0.1 during each communica-
tion round, and the corresponding misclassiﬁcation error rates are shown in Figs. 2.4(Middle)

and 2.4(Right), respectively. The classiﬁcation performance using federated averaging does

not improve in Fig. 2.4(Middle), whereas the DRFL performance keeps improving across

0510152025303540Communication round0.00.20.40.60.81.0Classification errorNormalDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorIFGSMDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorPGDDRFLFederated averaging0510152025303540Communication round0.000.050.100.150.200.250.30Classification errorNormalDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorIFGSMDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorPGDDRFLFederated averaging27

Figure 2.5: Distributionally robust federated learning for image classiﬁcation using F-MNIST
dataset; Left: No attack, Middle: IFGSM attack, Right: PGD attack

communication rounds. This is a direct consequence of accounting for the data uncertainties

during the learning process. Moreover, Fig. 2.4(Right) showcases that the federated averaging

becomes even worse as the model gets progressively trained under the PGD attack. This indeed

motivates our DRFL approach when data are from untrusted entities with possibly adversarial

input perturbations. Similarly, Fig. 2.5 depicts the misclassiﬁcation rate of the proposed DRFL

method compared with federated averaging, when using the F-MNIST dataset.

As the distribution of data across devices may inﬂuence performance, we further considered

a biased local data setting. In particular, each worker k = 1, . . . , 10 has data from only one

class, so the distributions at workers are highly perturbed, and data stored across workers are

thus non-i.i.d. The testing error rate for normal inputs is reported in Fig. 2.3, while the test

error against adversarial attacks is depicted in Figs. 2.3(Middle) and 2.3(Right). This additional

set of tests shows that having distributional shifts across workers can indeed enhance testing

performance when the samples are adversarially manipulated.

2.9 Conclusions

A framework to robustify parametric machine learning models against distributional uncertainties

was put forth. The learning task was cast as a distributionally robust optimization problem, for

which two scalable stochastic optimization algorithms were developed. The ﬁrst algorithm relies

on an (cid:15)-accurate maximum-oracle to solve the inner convex subproblem, while the second

approximates its solution via a single gradient ascent step. Convergence guarantees for both

algorithms to a stationary point were obtained. The upshot of the proposed approach is that it

is amenable to federated learning from unreliable datasets across multiple workers. The novel

0510152025303540Communication round0.00.20.40.60.81.0Classification errorNormalDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorIFGSMDRFLFederated averaging0510152025303540Communication round0.00.20.40.60.81.0Classification errorPGDDRFLFederated averagingDRFL algorithm ensures data privacy and integrity, while offering robustness with minimal

computational and communication overhead. Numerical tests for classifying standard real

images showcased the merits of the proposed algorithms against distributional uncertainties and

adversaries. This work also opens up several interesting directions for future research, including

distributionally robust deep reinforcement learning.

28

Chapter 3

Distributionally Robust

Semi-Supervised Learning Over

Graphs

3.1 Introduction

Building upon but going well beyond the scope of previous robust learning paradigms, the present

Chapter puts forth a novel iterative semi-supervised learning (SSL) over graphs framework.

Relations among data in real world applications can often be captured by graphs, for instance

the analysis and inference tasks for social, brain, communication, biological, transportation, and

sensor networks [178, 94]. In practice however, the data is only available for a subset of nodes,

due to for example the cost, and computational or privacy constraints. Most of these applications

however, deal with inference of processes across all the network nodes. Such semi-supervised

learning (SSL) tasks over networks can be addressed by exploiting the underlying graph topology

[39, 17, 117].

Graph neural networks (GNNs) are parametric models that combine graph-ﬁlters and topol-

ogy information with point-wise nonlinearities, to form nested architectures to easily express

the functions deﬁned over graphs [234]. By exploiting the underlying irregular structure of

network data, the GNNs enjoy lower computational complexity, less parameters for training,

and improved generalization capabilities relative to traditional deep neural networks (DNNs),

29

30

making them appealing for learning over graphs [234, 208, 60].

Similar to other DNN models, GNNs are also susceptible to adversarial manipulated input

data or, distributional uncertainties, such as mismatches between training and testing data

distributions. For instance small perturbations to input data would signiﬁcantly deteriorate the

regression performance, or result in classiﬁcation error [239, 84], just to name a few. Hence, it

is critical to develop principled methods that can endow GNNs with robustness, especially in

safety-critical applications, such as robotics [186], and transportation [233].

Contributions. This Chapter endows SSL over graphs using GNNs with robustness against

distributional uncertainties and possibly adversarial perturbations. Assuming the data distribution

lies inside a Wasserstein ball centered at empirical data distribution, we robustify the model

by minimizing the worst expected loss over the considered ball, which is challenging to solve.

Invoking recently developed strong duality results, we develop an equivalent unconstrained and
tractable learning problem. 1.

3.2 Problem formulation

Consider a SSL task over a graph G := {V, W} with N nodes, where V := {1, . . . , N }

denotes the vertex set, and W represents the N × N weighted adjacency matrix capturing node

connectivity. The associated unnormalized graph Laplacian matrix of the undirected graph G is

L := D − A, where D := diag{W1N }, with 1N denoting the N × 1 all-one column vector.
Denote by matrix Xs ∈ RN ×F the nodal feature vectors sampled at instances s = 1, 2, · · · , with
n-th row x(cid:62)
n,s := [Xs]n: representing a feature vector of length F associated with node n ∈ V,
and (cid:62) stands for transposition. In the given graph, the labels {yn,s}n∈Os are given for only a
small subset of nodes, where Os represents the index set of observed nodes sampled at s, and Us
the index set of unobserved nodes.

Given {Xs, ys}, where ys is the vector of observed labels, the goal is to ﬁnd the labels
of unobserved nodes {yn,s}n∈Us. To this aim our objective is to learn a functional mapping
f (Xs; W) that can infer the missing labels based on available information. Such a function can

1Results of this Chapter are published in [157]

be learned by solving the following optimization problem (see e.g., [93] for more details)

31

E

min
f ∈F

(cid:104)

(cid:122)
(cid:88)

n∈Os

L0
(cid:125)(cid:124)

(cid:107)f (xn; W) − yn(cid:107)2 +λ

(cid:123)

Lreg
(cid:125)(cid:124)

(cid:123)
Wnn(cid:48)(cid:107)f (xn; W) − f (xn(cid:48); W)(cid:107)2 (cid:105)

,

(3.1)

(cid:122)
(cid:88)

n,n(cid:48)

where L0 represents the supervised loss w.r.t. the observed part of the graph, Lreg represents
the Laplacian regularization term, F denotes the feasible set of functions that we can learn, and

λ ≥ 0 is a hyper parameter. The regularization term relies on the premise that connected nodes

in the graph are likely to share similar labels. The expectation here is taken with respect to (w.r.t)

the feature and label data generating distribution.

In this work, we ﬁrst encode the graph structure using a GNN model denoted by f (X; θ, W),

where θ represents the model parameters. Such a parametric representation enables bypassing

explicit graph-based regularization Lreg represented in 3.1. The GNN model of f (·) relies on
the weighted adjacency W and therefore can easily propagate information from observed nodes

Os to unobserved ones Us. In a nutshell, objective is to learn a parametric model by solving the
following problem

min
θ∈Θ

E{X,y}∼P0 L0

(cid:0)f (X, θ; W), y(cid:1)

(3.2)

where Θ is a feasible set, and P0 is the feature and label data generating distribution. Despite
restricting the modeling capacity through parameterizing f (·) with GNNs, we may infuse addi-

tional prior information into the sought formulation through exploiting the weighted adjacency

matrix W, which does not necessarily encode node similarities.

In practice, P0 is typically unknown, instead some data samples {Xs, ys}S

s=1 are given.
Upon replacing the nominal distribution with an empirical one, we arrive at the empirical loss
(cid:1). The model obtained
minimization problem, that is minθ∈Θ S−1 (cid:80)S
by solving empirical risk minimization does not exhibit any robustness in practice, speciﬁcally if

(cid:0)f (Xs, θ; W), ys

s=1 L0

there is any mismatch between the training and testing data distributions. To endow robustness,

we reformulate this learning problem in a fresh manner as described in ensuing section.

3.3 Distributionally robust learning

To endow robustness, we consider the following optimization problem

min
θ∈Θ

sup
P ∈P

E(X,y)∼P L0

(cid:0)f (X, θ; W), y(cid:1)

32

(3.3)

where P is a set of distributions centered around the empirical data distribution (cid:98)P0. This
novel reformulation in 3.3 yields a model that performs reasonably well among a continuum of

distributions. Various ambiguity sets P can be considered in practice, and they lead to different

robustness guarantees with different computational requirements. For instance momentum, KL

divergence, statistical test, and Wasserstein distance-based sets are popular in practice; see also

[22, 179, 21] and references therein. Among possible choices, we utilize the optimal transport

theory and the Wasserstein distance to characterize the ambiguity set P. As a result, we can offer

a tractable solution for this problem, as delineated next.

To formalize our framework, let us ﬁrst deﬁne the Wasserstein distance between two prob-

ability measures. To this aim, consider probability measures P and (cid:98)P supported on some set
X , and let Π(P, (cid:98)P ) denote the set of joint measures (a.k.a coupling) deﬁned over X × X , with
marginals P and (cid:98)P , and let c : X × X → [0, ∞) measure the transportation cost for a unit of
mass from X ∈ X in P to X(cid:48) ∈ X in (cid:98)P . The so-called optimal transport problem is concerned
with the minimum cost associated with transporting all the mass from P to (cid:98)P through ﬁnding the
Eπ[c(X, X(cid:48))]. If c(·, ·) satisﬁes the axioms of distance,
optimal coupling, i.e., Wc(P, (cid:98)P ) := inf
π∈Π

then Wc deﬁnes a distance on the space of probability measures. For instance, if P and (cid:98)P are
deﬁned over a Polish space equipped with metric d, then ﬁxing c(X, X(cid:48)) = dp(X, X(cid:48)) for some
p ∈ [1, ∞) asserts that W 1/p
P and (cid:98)P .

(P, (cid:98)P ) is the well-known Wasserstein distance of order p between

c

Using the Wasserstein distance, let us deﬁne the uncertainty set P := {P |Wc(P, (cid:98)P0) ≤
ρ} to include all probability distribution functions (pdfs) having at most ρ-distance from (cid:98)P0.
Incorporating this ambiguity set into 3.3, the following robust surrogate is considered in this

work

min
θ∈Θ

sup
P ∈P

E(X,y)∼P L0

(cid:0)f (X, θ; W), y(cid:1), where P :=

(cid:110)

(cid:111)

P |Wc(P, (cid:98)P0) ≤ ρ

.

(3.4)

The inner supremum here goes after pdfs characterized by P. Solving this optimization directly

33

over the inﬁnite-dimensional space of distribution functions raises practical challenges. Fortu-

nately, under some mild conditions over losses as well as transport costs, the inner maximization

satisﬁes a strong duality condition (see [21] for a detailed discussions), which means the optimal

objective of this inner maximization and its Lagrangian dual are equal. Enticingly, the dual

reformulation involves optimization over only one-dimensional dual variable. These properties

make it practically appealing to solve 5.37 directly in the dual domain. The following proposition

highlights the strong duality result, whose proofs can be found in [22].

Proposition 2. Under some mild conditions over the loss L0(·) and cost c(·), it holds that

sup
P ∈P

EP L0

(cid:0)f (X, θ; W), y(cid:1) = inf
γ≥0

1
S

S
(cid:88)

s=1

sup
ξ∈X

{L0

(cid:0)f (ξ, θ; W), ys

(cid:1) + γ (ρ − c(Xs, ξ))}

(3.5)

where P :=

P |Wc(P, (cid:98)P0) ≤ ρ

(cid:110)

(cid:111)
.

The right-hand side in 5.38 simply is the univariate dual reformulation of the primal problem

represented in the left-hand side. Furthermore, different from the primal formulation, the

expectation in the dual domain is replaced with the summation over available training data, rather

than any P ∈ P that needs to be obtained by solving for the optimal π ∈ Π to form P. Because

of these two properties, solving the dual problem is practically more appealing. Thus, hinging

on Proposition 2, the following distributionally robust surrogate is considered in this work

min
θ∈Θ

inf
γ≥0

1
S

S
(cid:88)

s=1

sup
ξ∈X

(cid:8)L0

(cid:0)f (ξ, θ; W), ys

(cid:1) + γ(ρ − c(Xs, ξ))(cid:9)

(3.6)

This problem requires the supremum to be solved separately for each sample Xs, which
cannot be handled through existing methods. Our approach to address this relies on the structure
of this problem to iteratively update parameters ¯θ := [θ(cid:62), γ](cid:62) and ξ. Speciﬁcally, we rely on
Danskin’s theorem to ﬁrst maximize over ξ, which results in a differentiable function of ¯θ, and
then minimize the objective w.r.t. ¯θ using gradient descent. However, to guarantee convergence
to a stationary point and utilize Danskin’s theorem, we need to make sure the inner maximization

admits a unique solution (singleton). By choosing a strongly convex transportation cost such
as c(X, ξ) := (cid:107)X − ξ(cid:107)2
F , and by selecting γ ∈ Γ := {γ|γ > γ0} with a large enough γ0, we
arrive at a strongly concave objective function for the maximization over ξ. Since γ is the dual

variable associated with the constraint in 5.37, having γ ∈ Γ is tantamount to tuning ρ, which in

turn controls the level of robustness. Replacing γ ≥ 0 in 5.39 with γ ∈ Γ, our robust model can

be obtained as the solution of

34

min
θ∈Θ

inf
γ∈Γ

1
S

S
(cid:88)

s=1

sup
ξ∈X

ψ(¯θ, ξ; Xs)

(3.7)

where ψ(¯θ, ξ; Xs) = L0(f (ξ, θ; W), ys) + γ(ρ − c(Xs, ξ)). Intuitively, input Xs in 5.40 is
pre-processed by maximizing ψ(·) accounting for a perturbation. We iteratively solve 5.40,

where after sampling a mini-batch of data, we ﬁrst pre-process them by maximizing the function
ψ(·). Then, we use a simple gradient descent to update ¯θ. Notice that the θ inside function ψ(·),
represents the weights of our considered GNN, whose details are provided next.

3.4 Graph neural networks

GNNs are parametric models to represent functional relationship for graph structured data.

Speciﬁcally, the input to a GNN is a data matrix X. Upon multiplying the input X by W,
features will diffuse over the graph, giving a new graph signal ˇY = WX. To model feature
propagation, one can also replace W with the (normalized) graph Laplacian or random walk

Laplacian, since they will also preserve dependencies among nodal attributes.

During the diffusion process, the feature vector of each node is updated by a linear combina-
tion of its neighbors. Take the n-th node as an example, the shifted f -th feature [ ˇY]nf is obtained
by [ ˇY]nf = (cid:80)N
i , where Nn denotes the set of neighboring nodes
for node n. The so-called convolution operation in GNNs utilizes topology to combine features,

i=1[W]ni[X]if = (cid:80)

wnixf

i∈Nn

namely

[Y]nd := [H (cid:63) X; W]nd :=

K−1
(cid:88)

[WkX]n:[Hk]:d

k=0

(3.8)

where H := [H0 · · · HK−1] with Hk ∈ RF ×D as ﬁlter coefﬁcients; Y ∈ RN ×D the intermedi-
ate (hidden) matrix with D features per node; and WkX as the linearly combined features of

nodes within the k-hop neighborhood.

To construct a GNN with L hidden layers, ﬁrst let us denote by Xl−1 the output of the
(l − 1)-th layer, which is also the l-th layer input for l = 1, . . . , L, and X0 = X to represent
the input matrix. The hidden Yl ∈ RN ×Dl with Dl features is obtained by applying the

35
graph convolution operation 5.30 at layer l, i.e., [Yl]nd = (cid:80)Kl−1
k=0 [WkXl−1]n:[Hlk]:g, where
Hlk ∈ RFl−1×Fl is the convolution coefﬁcients for k = 0, . . . , Kl − 1. The output at layer l
is constructed by applying a graph convolution followed by a point-wise nonlinear operation

(cid:16)(cid:80)Kl−1

k=0 WkXl−1Hlk

σl(·). The input-output relationship at layer l can be represented succinctly by Xl = σl(Yl) =
σl
. Using this mapping, GNNs use a nested architecture to represent
nonlinear functional operator XL = f (X0; θ, W) that maps the GNN input X0 to label estimates
by taking into account the graph structure through W. Speciﬁcally, in a compact representation

(cid:17)

we have that

f (X0; θ, W) := σL

(cid:32)KL−1
(cid:88)

(cid:32)

(cid:32)

Wk

. . .

σ1

(cid:32)K1−1
(cid:88)

k=0

k=0

(cid:33)

(cid:33)(cid:33)

(cid:33)

WkX0H1k

. . .

HLk

(3.9)

where the parameter set θ contains all the trainable ﬁlter weights {Hlk, ∀l, k}.

3.5 Experiments

The performance of our novel distributionally robust GNN-based SSL is tested in a regression

task using real load consumption data from the 2012 Global Energy Forecasting Competition

(GEFC). Our objective here is to estimate only the amplitudes of voltages across all the nodes

in a standard IEEE 118-bus network. Utilizing this data set, the training and testing data are

prepared by solving the so-called AC power ﬂow equations using the MATPOWER toolbox

[238].

The measurements X used include all active and reactive power injections, corrupted by small

additive white Gaussian noise. Using MATPOWER we generated 1, 000 pairs of measurements

and ground-truth voltages. We used 80% of this data for training and the remaining for testing.
Throughout the training, the Adam optimizer with a ﬁxed learning rate 10−3 was employed to

minimize the H¨uber loss. Furthermore, the batch size was set to 32 during all 100 epochs.

To compare our method we employed 3 different benchmarks, namely: i) the prox-linear

network introduced in [228]; ii) a 6-layer vanilla feed-forward neural network (FNN); and, iii) an

8-layer FNN. Our considered GNN uses K = 2 with D = 8 hidden units with ReLU activation.

The ﬁrst set of tests are carried out using normal (not-corrupted) data, where the results are

depicted in Fig. 3.1. Here we show the estimated (normalized) voltage amplitudes at different

nodes, namely 105, and 20 during the given time course. The black curve represents the ground

36

Figure 3.1: Performance during testing for both normal (a) - (b), and perturbed input features (c)
- (d).

truth signal to be estimated. Clearly our GNN-based method outperforms alternative methods.

The second set of experiments are carried out over corrupted input signals, and the results

are reported in Fig. 3.1. Speciﬁcally the training samples were generated according to P0, but
during testing samples were perturbed to satisfy the constraint P ∈ P, that would yield the worst

expected loss. Fig. 3.1 depicts the estimated signals across nodes 40 and 90. Here we ﬁxed

ρ = 10 and related hyper-parameters are tuned using grid search. As the plots showcase, the our

proposed GNN-based robust method outperforms competing alternatives with corrupted inputs.

3.6 conclusions

This Chapter dealt with semi-supervised learning over graphs using GNNs. To account for

uncertainties associated with data distributions, or adversarially manipulated input data, a

principled robust learning framework was developed. Using the parametric models, we were

able to reconstruct the unobserved nodal values. Experiments corroborated the outstanding

performance of the novel method when the input data are corrupted.

7072747678808284868890index t (during testing)0.960.970.98ytEstimated label vs. ground-truth for node 105Ground truthGNNProx-linear net6-layer FNN8-layer FNN7072747678808284868890index t (during testing)0.960.970.980.99ytEstimated label vs. ground-truth for node 20Ground truthGNNProx-linear net6-layer FNN8-layer FNN6062646668707274767880index t (during testing)0.9250.9500.9751.0001.025ytEstimated label vs. ground-truth for node 40Ground truthGNNProx-linear net6-layer FNN8-layer FNN6062646668707274767880index t (during testing)0.9500.9751.0001.0251.050ytEstimated label vs. ground-truth for node 90Ground truthGNNProx-linear net6-layer FNN8-layer FNNChapter 4

Deep and Reinforced Learning for

Network Resource Management

4.1 Introduction

The advent of smart phones, tablets, mobile routers, and a massive number of devices connected

through the Internet of Things (IoT) have led to an unprecedented growth in data trafﬁc. Increased

number of users trending towards video streams, web browsing, social networking and online

gaming, have urged providers to pursue new service technologies that offer acceptable quality

of experience (QoE). One such technology entails network densiﬁcation by deploying small

pico- and femto-cells, each serviced by a low-power, low-coverage, small basestation (SB). In

this infrastructure, referred to as heterogeneous network (HetNet), SBs are connected to the

backbone by a cheap ‘backhaul’ link. While boosting the network density by substantial reuse of

scarce resources, e.g., frequency, the HetNet architecture is restrained by its low-rate, unreliable,

and relatively slow backhaul links [5].

During peak trafﬁc periods specially when electricity prices are also high, weak backhaul

links can easily become congested–an effect lowering the QoE for end users. One approach to

mitigate this limitation is to shift the excess load from peak periods to off-peak periods. Caching

realizes this shift by fetching the “anticipated” popular contents, e.g., reusable video streams,

during off-peak periods, storing this data in SBs equipped with memory units, and reusing them

during peak trafﬁc hours [145, 66, 197]. In order to utilize the caching capacity intelligently, a

content-agnostic SB must rely on available observations to learn what and when to cache. To this

37

38

end, machine learning tools can provide 5G cellular networks with efﬁcient caching, in which

a “smart” caching control unit (CCU) can learn, track, and possibly adapt to the space-time

popularities of reusable contents [145, 4].

Prior work. Existing efforts in 5G caching have focused on enabling SBs to learn unknown

time-invariant content popularity proﬁles, and cache the most popular ones accordingly. A

multi-armed bandit approach is reported in [25], where a reward is received when user requests

are served via cache; see also [173] for a distributed, coded, and convexiﬁed reformulation. A

belief propagation-based approach for distributed and collaborative caching is also investigated

in [113]. Beyond [25], [173], and [113] that deal with deterministic caching, [40] and [26]

introduce probabilistic alternatives. Caching, routing and video encoding are jointly pursued

in [149] with users having different QoE requirements. However, a limiting assumption in

[25, 173, 113, 40, 26, 149] pertains to space-time invariant modeling of popularities, which

can only serve as a crude approximation for real-world requests. Indeed, temporal dynamics

of local requests are prevalent due to user mobility, as well as emergence of new contents, or,

aging of older ones. To accommodate dynamics, Ornstein-Uhlenbeck processes and Poisson

shot noise models are utilized in [91] and [102], respectively, while context- and trend-aware

caching approaches are investigated in [138] and [104].

Another practical consideration for 5G caching is driven by the fact that a relatively small

number of users request contents during a caching period. This along with the small size of

cells can challenge SBs from estimating accurately the underlying content popularities. To

address this issue, a transfer-learning approach is advocated in [13], [20] and [102], to improve

the time-invariant popularity proﬁle estimates by leveraging prior information obtained from a

surrogate (source) domain, such as social networks.

Finally, recent studies have investigated the role of coding for enhancing performance in

cache-enabled networks [121, 123, 147]; see also [82], [83], and [51], where device-to-device

“structureless” caching approaches are envisioned.

4.2 Our Contribution

The present chapter introduces a novel approach to account for space-time popularity of user

requests by casting the caching task in a reinforcement learning (RL) framework. The CCU

of the local SB is equipped with storage and processing units for solving the emerging RL

39

optimization in an online fashion. Adopting a Markov model for the popularity dynamics, a

Q-learning caching algorithm is developed to learn the optimal policy even when the underlying

transition probabilities are unknown.

Given the geographical and temporal variability of cellular trafﬁc, global popularity proﬁles

may not always be representative of local demands. To capture this, the proposed framework

entails estimation of the popularity proﬁles both at the local as well as at the global scale. Specif-

ically, each SB estimates its local vector of popularity proﬁles based on limited observations,

and transmits it to the network operator, where an estimate of the global proﬁle is obtained by

aggregating the local ones. The estimate of the global popularity vector is then sent back to the

SBs. The SBs can adjust the cost (reward) to trade-off tracking global trends versus serving local

requests.

To obtain a scalable caching scheme, a novel approximation of the proposed Q-learning

algorithm is also developed. Furthermore, despite the stationarity assumption on the popularity

Markov models, proper selection of stepsizes broadens the scope of the proposed algorithms for
tracking demands even in non-stationary settings. 1

4.3 Modeling and problem statement

Consider a local section of a HetNet with a single SB connected to the backbone network through

a low-bandwidth, high-delay, unreliable backhaul link. Suppose further that the SB is equipped

with M units to store contents (ﬁles) that are assumed for simplicity to have unit size; see Fig. 1.

Caching will be carried out in a slotted fashion over slots t = 1, 2, . . ., where at the beginning

of each slot t, the CCU-enabled SB selects “intelligently” M ﬁles from the total of F (cid:29) M

available ones at the backbone, and prefetches them for possible use in subsequent slots. The

slots may not be of equal length, as the starting times may be set a priori, for example at 3 AM,

11 AM, or 4 PM, when the network load is low; or, slot intervals may be dictated to CCU by the

network operator on the ﬂy. Generally, a slot starts when the network is at an off-peak period,

and its duration coincides with the peak trafﬁc time when pertinent costs of serving users are

high.

During slot t, each user locally requests a subset of ﬁles from the set F := {1, 2, . . . , F }.

If a requested ﬁle has been stored in the cache, it will be simply served locally, thus incurring

1Results of this Chapter are published in [158, 164, 161, 163, 165, 166, 167, 168, 162].

40

(almost) zero cost. Conversely, if the requested ﬁle is not available in the cache, the SB must

fetch it from the cloud through its cheap backhaul link, thus incurring a considerable cost due

to possible electricity price surges, processing cost, or the sizable delay resulting in low QoE

and user dissatisfaction. The CCU wishes to intelligently select the cache contents so that costly

services from the cloud be avoided as often as possible.

Let a(t) ∈ A denote the F × 1 binary caching action vector at slot t, where A := {a|a ∈
{0, 1}F , a(cid:62)1 = M } is the set of all feasible actions; that is, [a(t)]f = 1 indicates that ﬁle f is
cached for the duration of slot t, and [a(t)]f = 0 otherwise.

Depending on the received requests from locally connected users, the CCU computes the

F × 1-vector of local popularity proﬁle pL(t) per slot t, whose f -th entry indicates the expected
local demand for ﬁle f , deﬁned as

(cid:20)

(cid:21)

pL(t)

:=

f

Number of local requests for f at slot t
Number of all local requests at slot t

.

Similarly, suppose that the backbone network estimates the F × 1 global popularity proﬁle vector

pG(t), and transmits it to all CCUs.

Having observed the local and global user requests by the end of slot t, our overall system

state is

s(t) :=

(cid:104)
G(t), p(cid:62)
p(cid:62)

L (t), a(cid:62)(t)

(cid:105)(cid:62)

.

(4.1)

Being at slot t − 1, our objective is to leverage historical observations of states, {s(τ )}t−1
τ =0,
and pertinent costs in order to learn the optimal action for the next slot, namely a∗(t). Explicit

expression of the incurred costs, and analytical formulation of the objective will be elaborated in

the ensuing subsections.

4.3.1 Cost functions and caching strategies

Efﬁciency of a caching strategy will be measured by how well it utilizes the available storage

of the local SB to keep the most popular ﬁles, versus how often local user requests are met via

fetching through the more expensive backhaul link. The overall cost incurred will be modeled as

the superposition of three types of costs.

The ﬁrst type c1,t corresponds to the cost of refreshing the cache contents. In its general form,
c1,t(·) is a function of the upcoming action a(t), and available contents at the cache according to

current caching action a(t − 1), where the subscript t captures the possibility of a time-varying

cost for refreshing the cache. A reasonable choice of c1,t(·) is

41

c1,t(a(t), a(t − 1)) := λ1,ta(cid:62)(t) [1 − a(t − 1)]

(4.2a)

which upon recalling that the action vectors a(t − 1) and a(t) have binary {0, 1} entries, implies

that c1,t counts the number of those ﬁles to be fetched and cached prior to slot t, which were not
stored according to action a(t − 1).

The second type of cost is incurred during the operational phase of slot t to satisfy user

requests. With c2,t(s(t)) denoting this type of cost, a prudent choice must: i) penalize requests
for ﬁles already cached much less than requests for ﬁles not stored; and, ii) be a non-decreasing

function of popularities [pL]f . Here for simplicity, we assume that the transmission cost of
cached ﬁles is relatively negligible, and choose

c2,t(s(t)) := λ2,t [1 − a(t)](cid:62) pL(t)

(4.2b)

which solely penalizes the non-cached ﬁles in descending order of their local popularities.

The third type of cost captures the “mismatch” between caching action a(t), and the global

popularity proﬁle pG(t). Indeed, it is reasonable to consider the global popularity of ﬁles
as an acceptable representative of what the local proﬁles will look like in the near future;

thus, keeping the caching action close to pG(t) may reduce future possible costs. Note also
that a relatively small number of local requests may only provide a crude estimate of local

popularities, while the global popularity proﬁle can serve as side information in tracking the

evolution of content popularities over the network. This has prompted the advocation of transfer

learning approaches, where content popularities in a surrogate domain are utilized for improving

estimates of popularity; see, e.g., [13] and [20]. However, this approach is limited by the degree

the surrogate (source) domain, e.g., Facebook or Twitter, is a good representative of the target

domain requests. When it is not, techniques will misguide caching decisions, while imposing

excess processing overhead to the network operator or to the SB.

To account for this issue, we introduce the third type of cost as

c3,t(s(t)) := λ3,t [1 − a(t)](cid:62) pG(t)

(4.2c)

42

Figure 4.1: A schematic depicting the evolution of key quantities across time slots. Duration of
slots can be unequal.

penalizing the ﬁles not cached according to the global popularity proﬁle pG(·) provided by the
network operator, thus promoting adaptation of caching policies close to global demand trends.

All in all, upon taking action a(t) at slot t, the aggregate cost conditioned on the popularity

vectors revealed, can be expressed as (cf. (4.2a)-(4.2c))

(cid:16)

Ct

s(t − 1), a(t)

(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)

(4.3)

:= c1,t (a(t), a(t − 1)) + c2,t (s(t)) + c3,t(s(t))
= λ1,ta(cid:62)(t)(1 − a(t − 1)) + λ2,t(1 − a(t))(cid:62)pL(t)
+ λ3,t(1 − a(t))(cid:62)pG(t).

Weights λ1,t, λ2,t, and λ3,t control the relative signiﬁcance of the corresponding summands,
whose tuning inﬂuences the optimal caching policy at the CCU. As asserted earlier, the cache-

refreshing cost at off-peak periods is considered to be less than fetching the contents during slots,

which justiﬁes the choice λ1,t (cid:28) λ2,t. In addition, setting λ3,t (cid:28) λ2,t is of interest when the
local popularity proﬁles are of acceptable accuracy, or, if tracking local popularities is of higher

importance. In particular, setting λ3,t = 0 corresponds to the special case where the caching cost
is decoupled from the global popularity proﬁle evolution. On the other hand, setting λ2,t (cid:28) λ3,t
is desirable in networks where globally popular ﬁles are of high importance, for instance when

users have high mobility and may change SBs rapidly, or, when a few local requests prevent the

SB from estimating accurately the local popularity proﬁles. Fig. 4.1 depicts the evolution of

popularity and action vectors along with the aggregate conditional costs across slots.

Remark 1. As with slot sizes, proper selection of λ1,t, λ2,t, and λ3,t is a design choice. Depend-
ing on how centralized or decentralized the network operation is desired to be, these parameters

1,1ppGLtt1tt,ppGLttat1attC1tC43

may be selected autonomously by the CCUs or provided by the network operator in a centralized

fashion. However, the overall approach requires the network service provider and the SBs to

inter-operate by exchanging relevant information. On the one hand, estimating global populari-

ties requires SBs to transmit their locally obtained pL(t) to the network operator at the end of
each slot. On the other hand, the network operator informs the CCUs of the global popularity

pG(t), and possibly weights λ1,t, λ2,t, and λ3,t. By providing the network operator with means
of parameter selection, a “master-slave” hierarchy emerges, which enables the network operator

(master) to inﬂuence SBs (slaves) caching decisions, leading to a centrally controlled adjustment

of caching policies. Interestingly, these few bytes of information exchanges occur once per slot

and at off-peak instances, thus imposing negligible overhead to the system, while enabling a

simple, yet practical and powerful optimal semi-distributed caching process; see Fig. 3.

4.3.2 Popularity proﬁle dynamics

As depicted in Fig. 3, we will model user requests (and thus popularities) at both global and local

scales using Markov chains. Speciﬁcally, global popularity proﬁles will be assumed generated by

process with

|PG|

states

collected

in

the

set

; and likewise for the set of all local popularity proﬁles PL :=

an

underlying Markov
G, . . . , p|PG|
p1

(cid:110)

(cid:111)

G

PG :=
(cid:110)

L, . . . , p|PL|
p1

L

(cid:111)

. Although PG and PL are known, the underlying transition probabilities

of the two Markov processes are considered unknown.

Given PG and PL as well as feasible caching decisions in set A, the overall set of states in

the network is

S :=

(cid:110)

s|s = [p(cid:62)

G, p(cid:62)

(cid:111)
L , a(cid:62)](cid:62), pG ∈ PG , pL ∈ PL, a ∈ A

.

The lack of knowledge on transition probabilities of the underlying Markov chains motivates

well our ensuing RL-based approach, where the learner seeks the optimal policy by interactively

making sequential decisions, and observing the corresponding costs. The caching task is

formulated in the following subsection, and an efﬁcient solver is developed to cope with the

“curse of dimensionality” typically emerging with RL problems [184].

4.3.3 Reinforcement learning formulation

As showing in Fig. 2, the CCU takes caching action a(t), at the beginning of slot t, and by the

44

end of slot t, the proﬁles pG(t) and pL(t) become available, so that the system state is updated
to s(t), and the conditional cost Ct
is revealed. Given the random
nature of user requests locally and globally, Ct in (4.3) is a random variable with mean

(cid:12)
(cid:12)
(cid:12)pG(t), pL(t)

s(t − 1), a(t)

(cid:16)

(cid:17)

Ct (s(t − 1), a(t))

:= EpG(t),pL(t)

(cid:16)

(cid:104)

Ct

(cid:12)
(cid:12)
s(t − 1), a(t)
(cid:12)pG(t), pL(t)
(cid:104)
(1 − a(t))(cid:62)pL(t)

(cid:17) (cid:105)

= λ1a(cid:62)(t) [1 − a(t − 1)] + λ2E

(4.4)

(cid:105)

+ λ3E

(cid:105)
(cid:104)
(1 − a(t))(cid:62)pG(t)

where the expectation is taken with respect to (wrt) pL(t) and pG(t), while the weights are
selected as λ1,t = λ1, λ2,t = λ2, and λ3,t = λ3 for simplicity.

Let us now deﬁne the policy function π : S → A, which maps any state s ∈ S to the

action set. Under policy π(·), for the current state s(t), caching is carried out via action

a(t + 1) = π(s(t)) dictating what ﬁles to be stored for the (t + 1)-st slot. Caching performance

is measured through the so-termed state value function

Vπ (s(t)) := lim
T →∞

E

(cid:34) T

(cid:88)

τ =t

γτ −tC (s [τ ] , π (s [τ ]))

(4.5)

(cid:35)

which is the total average cost incurred over an inﬁnite time horizon, with future terms discounted

by factor γ ∈ [0, 1). Since taking action a(t) inﬂuences the SB state in future slots, future

costs are always affected by past and present actions. Discount factor γ captures this effect,

whose tuning trades off current versus future costs. Moreover, γ also accounts for modeling

uncertainties, as well as imperfections, or dynamics. For instance, if there is ambiguity about

future costs, or if the system changes very fast, setting γ to a small value enables one to prioritize

current costs, whereas in a stationary setting one may prefer to demote future costs through a

larger γ.

The objective of this paper is to ﬁnd the optimal policy π∗ such that the average cost of any

state s is minimized (cf. (4.5))

π∗ = arg min
π∈Π

Vπ (s) ,

∀s ∈ S

45

(4.6)

where Π denotes the set of all feasible policies.

The optimization in (4.6) is a sequential decision making problem. In the ensuing section,

we present optimality conditions (known as Bellman equations) for our problem, and introduce

a Q-learning approach for solving (4.6).

4.4 Optimality conditions

Bellman equations, also known as dynamic programming equations, provide necessary conditions

for optimality of a policy in a sequential decision making problem. Being at the (t − 1)st slot,
let [P a]ss(cid:48) denote the transition probability of going from the current state s to the next state s(cid:48)
under action a; that is,

[P a]ss(cid:48) := Pr

s(t) = s(cid:48)(cid:12)
(cid:111)
(cid:110)
(cid:12)
.
(cid:12)s(t − 1) = s, π(s(t − 1)) = a

Bellman equations express the state value function by (4.5) in a recursive fashion as [184, pg.

47]

Vπ (s) = C (s, π(s)) + γ

(cid:88)

s(cid:48)∈S

[P π(s)]ss(cid:48)Vπ

(cid:0)s(cid:48)(cid:1) , ∀s, s(cid:48)

(4.7)

which amounts to the superposition of C plus a discounted version of future state value functions

under a given policy π. Speciﬁcally, after dropping the current slot index t − 1 and indicating

with prime quantities of the next slot t, C in (4.4) can be written as

C (s, π(s)) =

(cid:88)

s(cid:48):=[p(cid:48)

G,p(cid:48)

L,a(cid:48)]∈S

[P π(s)]ss(cid:48)C

(cid:16)

(cid:12)
(cid:12)p(cid:48)
(cid:12)
s, π(s)

G, p(cid:48)
L

(cid:17)

(cid:16)

(cid:12)
(cid:12)p(cid:48)
(cid:12)
s, π(s)

(cid:17)

G, p(cid:48)
L

is found as in (4.3). It turns out that, with [P a]ss(cid:48) given ∀s, s(cid:48), one
where C
can readily obtain {Vπ(s), ∀s} by solving (4.7), and eventually the optimal policy π∗ in (4.9)
using the so-termed policy iteration algorithm [184, pg. 79]. To outline how this algorithm works

in our context, deﬁne the state-action value function that we will rely on under policy π [184, pg.

62]

(cid:0)s, a(cid:48)(cid:1) := C (cid:0)s, a(cid:48)(cid:1) + γ

Qπ

[P a(cid:48)

]ss(cid:48)Vπ

(cid:0)s(cid:48)(cid:1) .

(cid:88)

s(cid:48)∈S

46

(4.8)

Commonly referred to as the “Q-function,” Qπ(s, α) basically captures the expected current
cost of taking action α when the system is in state s, followed by the discounted value of the

future states, provided that the future actions are taken according to policy π.

In our setting, the policy iteration algorithm initialized with π0, proceeds with the following

updates at the ith iteration.

• Policy evaluation: Determine Vπi(s) for all states s ∈ S under the current (ﬁxed) policy

πi, by solving the system of linear equations in (4.7) ∀s.

• Policy update: Update the policy using

πi+1(s) := arg max

α

Qπi(s, α),

∀s ∈ S.

The policy evaluation step is of complexity O(|S|3), since it requires matrix inversion for

solving the linear system of equations in (4.7). Furthermore, given Vπi(s) ∀s, the complexity
of the policy update step is O(|A||S|2), since the Q-values must be updated per state-action

pair, each subject to |S| operations; see also (4.8). Thus, the per iteration complexity of the
policy iteration algorithm is O(|S|3 + |A||S|2).

Iterations proceed until convergence, i.e.,

πi+1(s) = πi(s), ∀s ∈ S.

Clearly, the policy iteration algorithm relies on knowing [P a]ss(cid:48), which is typically not
available in practice. This motivates the use of adaptive dynamic programming (ADP) that learn
[P a]ss(cid:48) for all s, s(cid:48) ∈ S, and a ∈ A, as iterations proceed [156, pg. 834]. Unfortunately, ADP
algorithms are often very slow and impractical, as they must estimate |S|2 × |A| probabilities. In
contrast, the Q-learning algorithm elaborated next ﬁnds the optimal π∗ as well as Vπ(s), while
circumventing the need to estimate [P a]ss(cid:48), ∀s, s(cid:48); see e.g., [184, pg. 140].

4.4.1 Optimal caching via Q-learning

Q-learning is an online RL scheme to jointly infer the optimal policy π∗, and estimate the optimal
state-action value function Q∗(s, a(cid:48)) := Qπ∗(s, a(cid:48)) ∀s, a(cid:48). Utilizing (4.7) for the optimal policy
π∗,
67]

shown

[184,

that

can

pg.

be

it

π∗(s) = arg min
α

Q∗(s, α),

∀s ∈ S.

The Q-function and V (·) under π∗ are related by

V ∗(s) := Vπ∗(s) = min
α

Q∗(s, α)

which in turn yields

Q∗ (cid:0)s, a(cid:48)(cid:1) = C (cid:0)s, a(cid:48)(cid:1) + γ

(cid:88)

s(cid:48)∈S

[P a]ss(cid:48) min
α∈A

Q∗ (cid:0)s(cid:48), α(cid:1) .

47

(4.9)

(4.10)

(4.11)

Capitalizing on the optimality conditions (4.9)-(4.11), an online Q-learning scheme for
caching is listed under Alg. 1. In this algorithm, the agent updates its estimated ˆQ(s(t − 1), a(t))
is observed. That is, given s(t − 1), Q-learning takes action
as C
(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)

(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)

a(t), and upon observing s(t), it incurs cost C

. Based on the

(cid:16)

(cid:16)

instantaneous error

ε (s(t − 1), a(t)) :=

(cid:16)

1
2

C (s(t − 1), a(t)) + γ min

α

(cid:98)Q (s(t), α)

− (cid:98)Q (s(t − 1), a(t))

(cid:17)2

(4.12)

the Q-function is updated using stochastic gradient descent as

ˆQt (s(t − 1), a(t)) = (1 − βt) ˆQt−1 (s(t − 1), a(t)) +

(cid:104)

C

βt

(cid:16)

(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)

+ γ min

α

ˆQt−1 (s(t), α)

(cid:105)

while keeping the rest of the entries in ˆQt(·, ·) unchanged.

Regarding convergence of the Q-learning algorithm, a necessary condition ensuring ˆQt (·, ·) →
Q∗ (·, ·), is that all state-action pairs must be continuously updated. Under this and the usual
stochastic approximation conditions that will be speciﬁed later, ˆQt (·, ·) converges to Q∗ (·, ·)
with probability 1; see [27] for a detailed description.

To meet the requirement for continuous updates, Q-learning utilizes a probabilistic exploration-

exploitation approach to selecting actions. At slot t, exploitation happens with probability 1 − (cid:15)t
through the action a(t) = arg minα∈A ˆQt−1(s(t − 1), α), while the exploration happens with

Algorithm 4 Caching via Q-learning at CCU
Initialize s(0) randomly and ˆQ0(s, a) = 0 ∀s, a
For t = 1, 2, ...
Take action a(t) chosen probabilistically by

a(t) =

(cid:40)

arg min

a

ˆQt−1 (s(t − 1), a) w.p. 1 − (cid:15)t

random a ∈ A

w.p.

(cid:15)t

pL(t) and pG(t) are revealed based on user requests
Set s(t) := (cid:2)p(cid:62)
Incur cost C

L (t), a(t)(cid:62)(cid:3)(cid:62)
G(t), p(cid:62)
(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)

Update

(cid:16)

ˆQt (s(t − 1), a(t)) = (1 − βt) ˆQt−1 (s(t − 1), a(t))
(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)
ˆQt−1 (s(t), α)

+ γ min

+ βt

(cid:104)
C

(cid:16)

(cid:105)

α

48

(4.13)

probability (cid:15)t through a random action a ∈ A. Parameter (cid:15)t trades off exploration for exploita-
tion, and its proper selection guarantees a necessary condition for convergence. During initial

iterations or when the CCU observes considerable shifts in content popularities, setting (cid:15)t high
promotes exploration in order to learn the underlying dynamics. On the other hand, in stationary

settings and once “enough” observations are made, small values of (cid:15)t promote exploiting the
learned ˆQt−1(·, ·) by taking the estimated optimal action arg minα ˆQt−1 (s(t), α).
Regarding stochastic approximation conditions, the stepsize sequence {βt}∞
t=1 βt = ∞ and (cid:80)∞

t=1 must obey
(cid:80)∞
t < ∞ [27], both of which are satisﬁed by e.g., βt = 1 / t. However,
with a selection of constant stepsize βt = β, the mean-square error (MSE) of ˆQt+1(·, ·) is
bounded as (cf. [27])

t=1 β2

(cid:20)(cid:13)
ˆQt+1 − Q∗(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

(cid:12)
(cid:12)
(cid:12)

E

ˆQ0

(cid:21)

≤ ϕ1 (β) + ϕ2( ˆQ0) exp (−2βt)

(4.14)

where ϕ1 (β) is a positive and increasing function of β; while the second term denotes the
initialization error, which decays exponentially as the iterations proceed.

Although selection of a constant stepsize prevents the algorithm from exact convergence to
Q∗ in stationary settings, it enables CCU adaptation to the underlying non-stationary Markov

49

processes in dynamic scenaria. Furthermore, the optimal policy in practice can be obtained from

the Q-function values before convergence is achieved [184, pg. 79].

However, the main practical limitation of the Q-learning algorithm is its slow convergence,

which is a consequence of independent updates of the Q-function values. Indeed, Q-function

values are related, and leveraging these relationships can lead to multiple updates per observation

as well as faster convergence. In the ensuing section, the structure of the problem at hand will be

exploited to develop a linear function approximation of the Q-function, which in turn will endow

our algorithm not only with fast convergence, but also with scalability.

4.5 Scalable caching

Despite simplicity of the updates as well as optimality guarantees of the Q-learning algorithm,

its applicability over real networks faces practical challenges. Speciﬁcally, the Q-table is of
(cid:1) encompasses all possible selections of M from F ﬁles.
size |PG||PL||A|2, where |A| = (cid:0) F
Thus, the Q-table size grows prohibitively with F , rendering convergence of the table entries, as

M

well as the policy iterates unacceptably slow. Furthermore, action selection in minα∈A Q(s, a)
entails an expensive exhaustive search over the feasible action set A.

Linear function approximation is a popular scheme for rendering Q-learning applicable to

real-world settings [64, 126, 156]. A linear approximation for Q(s, a) in our setup is inspired by

the additive form of the instantaneous costs in (4.3). Speciﬁcally, we propose to approximate
Q(s, a(cid:48)) as

Q(s, a(cid:48)) (cid:39) QG(s, a(cid:48)) + QL(s, a(cid:48)) + QR(s, a(cid:48))

(4.15)

where QG, QL, and QR correspond to global and local popularity mismatch, and cache-refreshing
costs, respectively.

Recall that the state vector s consists of three subvectors, namely s := [p(cid:62)

L , a(cid:62)](cid:62).
Corresponding to the global popularity subvector, our ﬁrst term of the approximation in (4.15) is

G, p(cid:62)

QG(s, a(cid:48)) :=

|PG|
(cid:88)

F
(cid:88)

i=1

f =1

θG
i,f 1{pG=pi

G}1{[a(cid:48)]f =0}

(4.16)

where the sums are over all possible global popularity proﬁles as well as ﬁles, and the indicator
function 1{·} takes value 1 if its argument holds, and 0 otherwise; while θG
i,f captures the average

“overall” cost if the system is in global state pi
content. By deﬁning the |PG| × |F| matrix with (i, f )-th entry (cid:2)ΘG(cid:3)
(4.16) as

G, and the CCU decides not to cache the f th
i,f := θG
i,f , one can rewrite

QG(s, a(cid:48)) = δ(cid:62)

G(pG)ΘG(1 − a(cid:48))

(4.17)

50

where

δG(pG) :=

(cid:104)
δ(pG − p1

G), . . . , δ(pG − p|PG|
G )

(cid:105)(cid:62)

.

Similarly, we advocated the second summand in the approximation (4.15) to be

QL(s, a(cid:48)) :=

|PL|
(cid:88)

F
(cid:88)

θL
i,f 1{pL=pi

L}1{[a(cid:48)]f =0}

f =1

i=1
L (pL)ΘL(1 − a(cid:48))

= δ(cid:62)

(4.18)

where (cid:2)ΘL(cid:3)

i,f := θL

i,f , and

δL(pL) :=

(cid:104)

δ(pL − p1

L), . . . , δ(pL − p|PL|
L )

(cid:105)(cid:62)

with θL
i,f modeling the average overall cost for not caching ﬁle f when the local popularity is in
state pi
L.

Finally, our third summand in (4.15) corresponds to the cache-refreshing cost

QR(s, a(cid:48)) : =

F
(cid:88)

f =1

θR1{[a(cid:48)]f =1}1{[a]f =0}

(4.19)

= θRa(cid:48)(cid:62) (1 − a)
= θR (cid:104)

a(cid:48)(cid:62) (1 − a) + a(cid:62)1 − a(cid:48)(cid:62)1

(cid:105)

= θRa(cid:62)(1 − a(cid:48))

where θR models average cache-refreshing cost per content. The constraint a(cid:62)1 = a(cid:48)(cid:62)1 = M ,
is utilized to factor out the term 1 − a(cid:48), which will become useful later.

51
Upon deﬁning the set of parameters Λ := {ΘG, ΘL, θa}, the Q-function is readily approxi-

mated (cf. (4.15))

(cid:98)QΛ(s, a(cid:48)) :=

(cid:16)

G(pG)ΘG + δ(cid:62)
δ(cid:62)

L (pL)ΘL + θRa(cid:62)(cid:17)

(1 − a(cid:48)).

(4.20)

Thus, the original task of learning |PG||PL||A|2 parameters in Alg. 1 is now reduced to learning
Λ containing (|PG| + |PL|) |F| + 1 parameters.

4.5.1 Learning Λ

Given the current parameter estimates { (cid:98)ΘG
error is given by

t−1, (cid:98)ΘL

t−1, ˆθR

t−1} at the end of slot t, the instantaneous

(cid:98)ε (s(t − 1), a(t)) :=

+ γ min

a(cid:48) (cid:98)QΛt−1

(cid:16)

C (s(t − 1), a(t))

1
2
(cid:0)s(t), a(cid:48)(cid:1) − (cid:98)QΛt−1 (s(t − 1), a(t))

(4.21)

(cid:17)2

.

Based on this error form, the parameter update rules are obtained using stochastic gradient

descent iterations as

ˆΘG

t = ˆΘG
= ˆΘG

t−1 + αG

= ˆΘG

t−1 + αG

(cid:112)

(cid:112)

ˆΘL

t = ˆΘL
= ˆΘL

t−1 + αL

(cid:112)

(cid:112)

t−1 − αG∇ΘG (cid:98)ε (s(t − 1), a(t))

(cid:98)ε (s(t − 1), a(t)) ∇ΘG (cid:98)QΛt−1(s(t − 1), a(t))
(cid:98)ε (s(t − 1), a(t)) δG(pG(t − 1))(1 − a(t))(cid:62)

t−1 − αL∇ΘL (cid:98)ε (s(t − 1), a(t))

(cid:98)ε (s(t − 1), a(t)) ∇ΘL (cid:98)QΛt−1(s(t − 1), a(t))
(cid:98)ε (s(t − 1), a(t)) δL(pL(t − 1))(1 − a(t))(cid:62)

= ˆΘL

t−1 + αL

and

t = ˆθR
ˆθR
= ˆθR
= ˆθR

t−1 − αR∇θR (cid:98)ε (s[t − 1], a[t])
t−1 + αR

(cid:112)

(cid:98)ε (s[t − 1], a[t]) ∇θR (cid:98)QΛt−1(s [t − 1] , a [t])
(cid:98)ε (s[t − 1], a[t]) a(cid:62)[t − 1](1 − a[t]).

(cid:112)

t−1 + αR

(4.22)

(4.23)

(4.24)

52

Algorithm 5 Scalable Q-learning
Initialize s(0) randomly, (cid:98)ΘG
For t = 1, 2, ...
Take action a(t) chosen probabilistically by

0 = 0, (cid:98)ΘL

0 = 0, ˆθ

R
0 = 0, and thus (cid:98)ψ(s) = 0

a(t) =

(cid:26) M best ﬁles via (cid:98)ψ (s(t − 1)) w.p. 1 − (cid:15)t

random a ∈ A

w.p.

(cid:15)t

pG(t) and pL(t) are revealed based on user requests
L (t), a(t)(cid:62)(cid:3)(cid:62)
G(t), p(cid:62)
Set
(cid:12)
(cid:17)
(cid:12)
(cid:12)pG(t), pL(t)
s(t − 1), a(t)

s(t) := (cid:2)p(cid:62)

Incur cost C

(cid:16)

Find

(cid:98)ε (s(t − 1), a(t)) Update

(cid:98)ΘG

t , (cid:98)ΘL

R
t and ˆθ
t based on (4.22)-(4.24)

The pseudocode for this scalable approximation of the Q-learning scheme is tabulated in

Alg. 2.

The upshot of this scalable scheme is three-fold.

• The large state-action space in the Q-learning algorithm is handled by reducing the number

of parameters from |PG||PL||A|2 to (|PG| + |PL|) |F| + 1.

• In contrast to single-entry updates in the exact Q-learning Alg. 1, F − M entries in (cid:98)ΘG
and (cid:98)ΘL as well as θR, are updated per observation using (4.22)-(4.24), which leads to a
much faster convergence.

• The exhaustive search in min
a∈A

Q (s, a) required in exploitation; and also in the error

evaluation (4.21), is circumvented. Speciﬁcally, it holds that (cf. (4.20))

min
a(cid:48)∈A

Q(s, a(cid:48)) ≈ min
a(cid:48)∈A

ψ(cid:62)(s) (cid:0)1 − a(cid:48)(cid:1) = max
a(cid:48)∈A

ψ(cid:62)(s) a(cid:48)

(4.25)

where

ψ(s) := δ(cid:62)

G(pG)ΘG + δ(cid:62)

L (pL)ΘL + θRa(cid:62).

of

solution

The
by
for i = 1, . . . , M , and [a]νi = 0 for i > M , where [ψ(s)]νF
sorted entries of ψ(s).

readily

(4.25)

given

is

[a]νi

=

1

≤ · · · ≤ [ψ(s)]ν1

are

Remark 2. In the model of Sec. II-B, the state-space cardinality of the popularity vectors is

ﬁnite. These vectors can be viewed as centroids of quantization regions partitioning a state space

of inﬁnite cardinality. Clearly, such a partitioning inherently bears a complexity-accuracy trade

off, motivating optimal designs to achieve a desirable accuracy for a given affordable complexity.

This is one of our future research directions for the problem at hand.

Simulation based evaluation of the proposed algorithms for RL-based caching is now in

53

order.

4.6 Numerical tests

In this section, performance of the proposed Q-learning algorithm and its scalable approximation

is tested. To compare the proposed algorithms with the optimal ofﬂine caching policy, we ﬁrst

simulated a small network with F = 10 contents, and caching capacity M = 2 at the local SB.
Global popularity proﬁle is modeled by a two-state Markov chain with states p1
are drawn from Zipf distributions having parameters ηG

G,that
2 = 1.5, respectively [31];
see also Fig. 4.4. That is, for state i ∈ {1, 2}, the F contents are assigned a random ordering of

1 = 1 and ηG

G and p2

popularities, and then sorted accordingly in a descending order. Given this ordering and the Zipf
distribution parameter ηG
i , the popularity of the f -th content is set to

(cid:21)

(cid:20)

pi
G

=

f

f ηi

1

F
(cid:80)
l=1

1 / lηG

i

for i = 1, 2

i ≥ 0 controls the skewness of popularities. Speciﬁcally, ηG

where the summation normalizes the components to follow a valid probability mass function,
while ηG
i = 0 yields a uniform spread
of popularity among contents, while a large value of ηi generates more skewed popularities.
Furthermore, state transition probabilities of the Markov chain modeling global popularity

proﬁles are

P G :=

(cid:34)

11 pG
pG
12
21 pG
pG
22

(cid:35)

(cid:34)

=

0.8

0.2

0.75 0.25

(cid:35)

.

Similarly, local popularities are modeled by a two-state Markov chain, with states p1

L, whose entries are drawn from Zipf distributions with parameters ηL
p2

1 = 0.7 and ηL

L and
2 = 2.5,

54

Figure 4.2: Global popularity Markov chain.

Figure 4.3: Local popularity Markov chain.

Figure 4.4: Popularity proﬁles Markov chains.

respectively. The transition probabilites of the local popularity Markov chain are

P L :=

(cid:34)

pL
11 pL
12
21 pL
pL
22

(cid:35)

(cid:34)

=

(cid:35)

.

0.6 0.4

0.2 0.8

Caching performance is assessed under two cost-parameter settings: (s1) λ1 = 10, λ2 =
600, λ3 = 1000; and, (s2) λ1 = 600, λ2 = 10, λ3 = 1000. For both (s1) and (s2), the optimal
ofﬂine caching policy is found by utilizing the policy iteration with known transition probabilities.

In addition, Q-learning in Alg. 1 and its scalable approximation in Alg. 2 are run with βt = 0.8,
αG = αL = αR = 0.005, and (cid:15)t = 1 / iteration index, thus promoting exploration in the early
iterations, and exploitation in later iterations.

Fig. 4.5 depicts the observed cost versus iteration (time) index averaged over 100 realizations.

It is seen that the cashing cost via Q-learning, and through its scalable approximation converge to

that of the optimal ofﬂine policy. As anticipated, even for the small size of this network, namely

|PG| = |PL| = 2 and |A| = 45, the Q-learning algorithm converges slowly to the optimal policy,
especially under s2, while its scalable approximation exhibits faster convergence.

In order to highlight the trade-off between global and local popularity mismatches, the

percentage of accommodated requests via cache is depicted in Fig. 4.6 for settings (s3) λ1 =
λ3 = 0, λ2 = 1, 000, and (s4) λ1 = λ2 = 0, λ3 = 1, 000. Observe that penalizing local
popularity-mismatch in (s3) forces the caching policy to adapt to local request dynamics, thus

1pG12GpState 1State 21ZipfG2pG2ZipfG11Gp22Gp21Gp1pL12LpState 1State 21ZipfL2pL1ZipfL11Lp22Lp21Lp55

accommodating a higher percentage of requests via cache, while (s4) prioritizes tracking global

popularities, leading to a lower cache-hit in this setting. Due to slow convergence of the exact

Q-learning under (s3) and (s4), only the performance of the scalable solver is presented here.

Furthermore, the convergence rate of Algs. 1 and 2 is illustrated in Fig. 4.7, where average

normalized error is evaluated in terms of the “exploitation index.” Speciﬁcally, a pure exploration

is taken for the ﬁrst Texplore iterations of the algorithms, i.e., (cid:15)t = 1 for t = 1, 2, . . . , Texplore;
and a pure exploitation with (cid:15)t = 0 is adopted afterwards. We have set α = 0.005, and selected
βt = β ∈ (0, 1) so that the fastest convergence is achieved. As the plot demonstrates, the exact
Q-learning Alg. 1 exhibits slower convergence, whereas just a few iterations sufﬁce for the

scalable Alg. 2 to converge to the optimal solution, thanks to the reduced dimension of the

problem as well as the multiple updates that can be afforded per iteration.

Having established the accuracy and efﬁciency of the Alg. 2, we next simulated a larger

network with F = 1, 000 available ﬁles, and a cache capacity of M = 10, giving rise to a total
(cid:1) (cid:39) 2 × 1023 feasible caching actions. In addition, we set the local and global popularity
of (cid:0)1000
10
Markov chains to have |PL| = 40 and |PG| = 50 states, for which the underlying state transition
probabilities are drawn randomly, and Zipf parameters are drawn uniformly over the interval

(2, 4).

Fig. 4.8 plots the performance of Alg. 2 under (s5) λ1 = 100, λ2 = 20, λ3 = 20, (s6)
λ1 = 0, λ2 = 0, λ3 = 1, 000, and (s7) λ1 = 0, λ2 = 1, 000, λ3 = 600. Exploration-exploitation
parameter is set to (cid:15)t = 1 for t = 1, 2, . . . , 5, 000, in order to greedily explore the entire state-
action space in initial iterations, and (cid:15)t = 1 / iteration index for t > 5, 000. Finding the optimal
ofﬂine policy in (s6) and (s7) requires prohibitively sizable memory as well as extremely high

computational complexity, and it is thus unaffordable for this network. However, having large

cache-refreshing cost with λ1 (cid:29) λ2, λ3 in (s5) forces the optimal caching policy to freeze its
cache contents, making the optimal caching policy predictable in this setting. Despite the very

limited storage capacity, of 10 / 1, 000 = 0.01 of available ﬁles, utilization of RL-enabled

caching offers a considerable reduction in incurred costs, while the proposed approximated

Q-learning endows the approach with scalability and light-weight updates.

56

Figure 4.5: Performance of the proposed algorithms.

4.7 Conclusions

The present Chapter addressed caching in 5G cellular networks, where space-time popularity of

requested ﬁles is modeled via local and global Markov chains. By considering local and global

popularity mismatches as well as cache-refreshing costs, 5G caching is cast as a reinforcement-

learning task. A Q-learning algorithm is developed for ﬁnding the optimal caching policy in an

online fashion, and its linear approximation is provided to offer scalability over large networks.

The novel RL-based caching offers an asynchronous and semi-distributed caching scheme, where

adaptive tuning of parameters can readily bring about policy adjustments to space-time variability

of ﬁle requests via light-weight updates.

4.8 Deep Reinforcement Learning for Adaptive Caching in Hierar-

chical Content Delivery Networks

4.9 Introduction

Deep neural networks (DNNs) have lately boosted the notion of “learning from data” with

ﬁeld-changing performance improvements reported in diverse artiﬁcial intelligence tasks [67].

DNNs can cope with the ‘curse of dimensionality’ by providing compact low-dimensional

Iterationindex100200300400500600700800900100011001200Cost3004005006007008009001000110012001300Optimalo0ine,Q-learningandlinearfunctionapproximatedQ-learningpoliciesOptimalo0ineQ-learningApproximatedQ-learningScenario1Scenario257

Figure 4.6: Percentage of accommodated requests via cache.

representations of high-dimensional data [15]. Combining deep learning with RL, deep (D)

RL has created the ﬁrst artiﬁcial agents to achieve human-level performance across many

challenging domains [134, 120]. As another example, a DNN system was built to operate

Google’s data centers, and shown able to consistently achieve a 40% reduction in energy

consumption for cooling [62]. This system provides a general-purpose framework to understand

complex dynamics, which has also been applied to address other challenges including e.g.,

dynamic spectrum access [139], multiple access and handover control [222], [198], as well

as resource allocation in fog-radio access networks [183, 50] or software-deﬁned networks

[221, 73].

In realistic networks, popularities exhibit dynamics, which motivate well the so-termed

dynamic caching. A Poisson shot noise model was adopted to approximate the evolution of

popularities in [188], for which an age-based caching solution was developed in [101]. RL based

methods have been pursued in [158, 181, 164, 76]. Speciﬁcally, a Q-learning based caching

scheme was developed in [158] to model global and local content popularities as Markovian

processes. Considering Poisson shot noise popularity dynamics, a policy gradient RL based

caching scheme was devised in [181]. Assuming stationary ﬁle popularities and service costs,

a dual-decomposition based Q-learning approach was pursued in [164]. Albeit reasonable for

discrete states, these approaches cannot deal with large continuous state-action spaces. To

cope with such spaces, DRL approaches have been considered for content caching in e.g.,

Iteration index100101102103%  of accommodated requests1012141618202224262830Optimal offlineApproximated Q-learningScenario4Scenario358

Figure 4.7: Convergence rate of the exact and scalable Q-learning.

[76, 232, 77, 75, 120, 235]. Encompassing ﬁnite-state time-varying Markov channels, a deep

Q-network approach was devised in [76]. An actor-critic method with deep deterministic policy

gradient updates was used in [232]. Boosted network performance using DRL was documented

in several other applications, such as connected vehicular networks [77], and smart cities [75].

The aforementioned works focus on devising caching policies for a single caching entity. A

more common setting in next-generation networks however, involves a network of interconnected

caching nodes. It has been shown that considering a network of connected caches jointly can

further improve performance [122, 29]. For instance, leveraging network topology and the

broadcast nature of links, the coded caching strategy in [122] further reduces data trafﬁc over

a network. This idea has been extended in [148] to an online setting, where popularities are

modeled Markov processes. Collaborative and distributed online learning approaches have been

pursued [45, 29, 196]. Indeed, today’s content delivery networks such as Akamai [140], have

tree network structures. Accounting for the hierarchy of caches has become a common practice

in recent works; see also [47, 177, 187]. Joint routing and in-network content caching in a

hierarchical cache network was formulated in [47], for which greedy schemes with provable

performance guarantees can be found in [177].

We identify the following challenges that need to be addressed when designing practical

caching methods for network of caches.

c1) Networked caching. Caching decisions of a node, in a network of caches, inﬂuences

Exploitationindex,Texplore0200400600800100012001400160018002000Normalized error00.050.10.150.20.25Q-learningApproximated Q-learning59

Figure 4.8: Performance in large state-action space scenaria.

decisions of all other nodes. Thus, a desired caching policy must adapt to the network

topology and policies of neighboring nodes.

c2) Complex dynamics. Content popularities are random and exhibit unknown space-time,

heterogeneous, and often non-stationary dynamics over the entire network.

c3) Large continuous state space. Due to the shear size of available content, caching nodes,

and possible realizations of content requests, the decision space is huge.

4.9.1 This section

Prompted by the recent interest in hierarchical caching, here we focus on a two-level network

caching, where a parent node is connected to multiple leaf nodes to serve end-user ﬁle requests.

Such a two-level network constitutes the building block of the popular tree hierarchical cache

networks in e.g., [140]. To model the interaction between caching decisions of parent and leaf

nodes along with the space-time evolution of ﬁle requests, a scalable DRL approach based on

hyper deep Q-networks (DQNs) is developed. As corroborated by extensive numerical tests,

the novel caching policy for the parent node can adapt itself to local policies of leaf nodes

and space-time evolution of ﬁle requests. Moreover, our approach is simple-to-implement, and

performs close to the optimal policy.

Iterationindex10002000300040005000600070008000900010000Cost02004006008001000120014001600PerformanceofapproximatedQ-learningOptimal solutionApproximated Q-learningScenario7Scenario6Scenario560

Figure 4.9: A network of caching nodes.

Figure 4.10: A hierarchical tree network cache system.

4.10 Modeling and Problem Statement

Consider a two-level network of interconnected caching nodes, where a parent node is connected

to N leaf nodes, indexed by n ∈ N := {1, . . . , N }. The parent node is connected to the cloud

through a (typically congested) back-haul link; see Fig. 4.9. One could consider this network

as a part of a large hierarchical caching system, where the parent node is connected to a higher

level caching node instead of the cloud; see Fig. 4.10. In a content delivery network for instance,

edge servers (a.k.a. points of presence or PoPs) are the leaf nodes, and a fog server acts as the

parent node. Likewise, (small) base stations in a 5G cellular network are the leaf nodes, while a

serving gate way (S-GW) may be considered as the parent node; see also [44, p. 110].

All nodes in this network store ﬁles to serve ﬁle requests. Every leaf node serves its locally

connected end users, by providing their requested ﬁles. If a requested content is locally available

at a leaf node, the content will be served immediately at no cost. If it is not locally available

due to limited caching capacity, the content will be fetched from its parent node, at a certain

12NCentral nodeLeaf nodesCloudFeed back61

cost. Similarly, if the ﬁle is available at the parent node, it will be served to the leaf at no cost;

otherwise, the ﬁle must be fetched from the cloud at a higher cost.

To mitigate the burden with local requests on the network, each leaf node stores ‘anticipated’

locally popular ﬁles. In addition, this paper considers that each parent node stores ﬁles to serve

requests that are not locally served by leaf nodes. Since leaf nodes are closer to end users, they

frequently receive ﬁle requests that exhibit rapid temporal evolution at a fast timescale. The

parent node on the other hand, observes aggregate requests over a large number of users served

by the N leaf nodes, which naturally exhibit smaller ﬂuctuations and thus evolve at a slow

timescale.

This motivated us to pursue a two-timescale approach to managing such a network of caching

nodes. To that end, let τ = 1, 2, . . . denote the slow time intervals, each of which is further

divided into T fast time slots indexed by t = 1, . . . , T ; see Fig. 4.11 for an illustration. Each

fast time slot may be e.g., 1-2 minutes depending on the dynamics of local requests, while each

slow time interval is a period of say 4-5 minutes. We assume that the network state remains

unchanged during each fast time slot t, but can change from t to t + 1.

Consider a total of F ﬁles in the cloud, which are collected in the set F = {1, . . . , F }. At the

beginning of each slot t, every leaf node n selects a subset of ﬁles in F to prefetch and store for

possible use in this slot. To determine which ﬁles to store, every leaf node relies on a local caching

policy function denoted by πn, to take (cache or no-cache) action aaan(t + 1, τ ) = πn(sssn(t, τ ))
at the beginning of slot t + 1, based on its state vector sssn at the end of slot t. We assume this
action takes a negligible amount of time relative to the slot duration; and deﬁne the state vector
n (t, τ )](cid:62) to collect the number of requests received at leaf
sssn(t, τ ) :=rrrn(t, τ ) := [r1
node n for individual ﬁles over the duration of slot t on interval τ . Likewise, to serve ﬁle requests

n(t, τ ) · · · rF

that have not been served by leaf nodes, the parent node takes action aaa0(τ ) to store ﬁles at the
beginning of every interval τ , according to a certain policy π0. Again, as aggregation smooths
out request ﬂuctuations, the parent node observes slowly varying ﬁle requests, and can thus make

caching decisions at a relatively slow timescale. In the next section, we present a two-timescale

approach to managing such a network of caching nodes.

4.11 Two-timescale Problem Formulation

62

File transmission over any network link consumes resources, including e.g., energy, time, and

bandwidth. Hence, serving any requested ﬁle that is not locally stored at a node, incurs a cost.

Among possible choices, the present paper considers the following cost for node n ∈ N , at slot

t + 1 of interval τ

cccn(πn(sssn(t, τ )), rrrn(t + 1, τ ), aaa0(τ )) :=rrrn(t + 1, τ ) (cid:12) (1− aaa0(τ ))

(cid:12)(1− aaan(t+1, τ ))+ rrrn(t+1, τ ) (cid:12) (1− aaan(t+1, τ ))

(4.26)

n(·) · · · cF

n (·)](cid:62) concatenates the cost for serving individual ﬁles per node
where cccn(·) := [c1
n; symbol (cid:12) denotes entry-wise vector multiplication; entries of aaa0 and aaan are either 1 (cache,
hence no need to fetch), or, 0 (no-cache, hence fetch); and 1 stands for the all-one vector.

Speciﬁcally, the second summand in (4.26) captures the cost of the leaf node fetching ﬁles for

end users, while the ﬁrst summand corresponds to that of the parent fetching ﬁles from the cloud.

We model user ﬁle requests as Markov processes with unknown transition probabilities [158].

Per interval τ , a reasonable caching scheme for leaf node n ∈ N could entail minimizing the

expected cumulative cost; that is,

π∗
n,τ:= arg min
πn∈Πn

E

(cid:104) T
(cid:88)

(cid:105)
1(cid:62)cccn(πn(sssn(t, τ )), rrrn(t+1, τ ), aaa0(τ ))

(4.27)

t=1

where Πn represents the set of all feasible policies for node n. Although solving (4.27) is in
general challenging, efﬁcient near-optimal solutions have been introduced in several recent

contributions; see e.g., [158, 181, 24], and references therein. In particular, a RL based approach

using tabular Q-learning was pursued in our precursor [158], which can be employed here to

tackle this fast timescale optimization. The remainder of this paper will thus be on designing the

caching policy π0 for the parent node, that can learn, track, and adapt to the leaf node policies as
well as user ﬁle requests.

63

Figure 4.11: Slow and fast time slots.

Figure 4.12: Structure of slots and intervals.

4.12 Reinforcement Learning for Adaptive Caching with Dynamic

Storage Pricing

4.13

Introduction

To target different objectives such as content-access latency, energy, storage or bandwidth

utilization, corresponding deterministic cost parameters are deﬁned, and the aggregated cost is

minimized in [99, 150]. Deterministic cost parameters, however, may be inaccurate in modeling

practical settings, as spatio-temporal popularity evolutions, network resources such as bandwidth

and cache capacity are random and subject to change over time and space, due to e.g., time-

varying data trafﬁc over links, previous cache decisions, or channel ﬂuctuations. Therefore, this

necessitates modeling the caching problem from a stochastic optimization perspective, while

accounting for the inherently random nature of available resources and ﬁle requests.

Contributions: This Section aspires to ﬁll this gap by relying on dual decomposition tech-

niques which transform the limits on the available resources in the original (primal) optimization

into stochastic prices in the dual problem. Building on this approach, the goal is to design

more ﬂexible caching schemes by introducing a generic dynamic pricing formulation, while

enabling SBs to learn the optimal fetching-caching decisions using low-complexity techniques.

Our contributions are listed as follows.

Content deliveryCache refreshing64

1) A general formulation of the caching problem by introducing time-varying and stochastic

costs is presented, in which the fetching and caching decisions are found through a

constrained optimization with the objective of reducing the overall cost, aggregated across

ﬁles and time instants (Section 4.14).

2) Since the caching decision in a given time slot not only affects the instantaneous cost,

but also inﬂuences the cache availability in the future, the problem is indeed a dynamic

programming (DP), and therefore can be effectively solved by reinforcement learning-

based approaches. By assuming known and stationary distributions for the costs and

popularities, and upon relaxing the limited cache capacity constraint, the proposed generic

optimization problem is shown to become separable across ﬁles, and thus can be efﬁciently

solved using the value-iteration algorithm (Section 4.15).

3) Subsequently, it is shown that the particular case where the cache capacity is limited and

the distribution of the pertinent parameters are unknown can be handled by the proposed

generic formulation. Thus, in order to address these issues, a dual-decomposition technique

is developed to cope with the coupling constraint associated with the storage limitation.

Finally, an online low complexity (Q-function based) reinforcement learning solver is put

forth for learning the optimal fetch-cache decisions on-the-ﬂy (Section 4.16).

4) The separability of the objective across ﬁles together with the use of marginalized value

functions [114] enable the decomposition of the original problem into smaller-dimension

sub-problems. This in turn leads to circumventing the so-called curse of dimensionality,

which commonly arises in reinforcement learning problems (Sections 4.15 and 4.16).

The effectiveness of the proposed scheme in terms of efﬁciency as well as scalability is

corroborated by various numerical tests. Although our proposed approach enjoys theoretical

guarantees in learning the optimal fetch-cache decisions in stationary settings, numerical tests

also corroborate its merits in non-stationary scenarios.

4.14 Operating conditions and costs

Consider a memory-enabled SB responsible for serving ﬁle (content) requests denoted by

f = 1, 2, . . . , F across time. The requested contents are transmitted to users either by fetching

65

through a (costly) back-haul transmission link connecting the SB to the cloud, or, by utilizing

the local storage unit in the SB where popular contents have been proactively cached ahead of

time. The system is considered to operate in a slotted fashion with t = 1, 2, . . . denoting time.

During slot t and given the available cache contents, the SB receives a number of ﬁle

requests whose provision incurs certain costs. Speciﬁcally, for a requested ﬁle f , fetching it from

the cloud through the back-haul link gives rise to scheduling, routing and transmission costs,

whereas its availability at the cache storage in the SB will eliminate such expenses. However,

local caching also incurs a number of (instantaneous) costs corresponding to memory or energy

consumption. This gives rise to an inherent caching-versus-fetching trade-off, where one is

promoted over the other depending on their relative costs. The objective here is to propose

a simple yet sufﬁciently general framework to minimize the sum-average cost over time by

optimizing fetch-cache decisions while adhering to the constraints inherent to the operation of

the system at hand, and user-speciﬁc requirements. The variables, constraints, and costs involved

in this optimization are described in the ensuing subsections.

4.14.1 Variables and constraints

Consider the system at time slot t, where the binary variable rf
t represents the incoming request
for ﬁle f ; that is, rf
t = 0, otherwise. Here, we
assume that rf
t = 1 necessitates serving the ﬁle to the user and dropping requests is not allowed;
thus, requests must be carried out either by fetching the ﬁle from the cloud or by utilizing the

t = 1 if the ﬁle f is requested during slot t, and rf

content currently available in the cache. Furthermore, at the end of each slot, the SB will decide

if content f should be stored in the cache for its possible reuse in a subsequent slot.

t ∈ {0, 1}. Setting wf

t = 0 means “no-fetching.” Similarly, af

To formalize this, let us deﬁne the “fetching” decision variable wf

t ∈ {0, 1} along the
“caching” decision variable af
t = 1 implies “fetching” ﬁle f at time t,
while wf
t = 1 implies that content f will be stored in
cache at the end of slot t for the next slot, while af
t = 0 implies that it will not. Furthermore,
let the storage state variable sf
t ∈ {0, 1} account for the availability of ﬁles at the local cache.
In particular, sf
t = 0
otherwise. Since the availability of ﬁle f directly depends on the caching decision at time t − 1,

t = 1 if ﬁle f is available in the cache at the beginning of slot t, and sf

we have

C1:

t = af
sf

t−1,

∀f, t,

(4.28)

which will be incorporated into our optimization as constraints.

66

Moreover, since having rf

having the ﬁle in cache (sf
set of constraints

t = 1 implies transmission of ﬁle f to the user(s), it requires either
t = 1), giving rise to the second

t = 1) or fetching it from the cloud (wf

C2:

t ≤ wf
rf

t + sf
t ,

∀f, t.

(4.29)

Finally, the caching decision af
that is, only if either fetching is carried out (wf
turn implies the third set of constraints as

t can be set to 1 only when the content f is available at time t;
t = 1) or the current cache state is sf
t = 1. This in

C3: af

t ≤ sf

t + wf
t ,

∀f, t.

(4.30)

4.14.2 Prices and aggregated costs

To account for the caching and fetching costs, let ρf
with af
denoting the size of content f , a simple form for ρf

t = 1 and wf

t denote the (generic) costs associated
t = 1, respectively. Focusing for now on the caching cost and with σf

t and λf

t is

ρf
t = σf (ρ(cid:48)

t + ρ(cid:48)f

t ) + (ρ(cid:48)(cid:48)

t + ρ(cid:48)(cid:48)f

t ),

(4.31)

where the ﬁrst term is proportional to the ﬁle size σf , while the second one is constant. Note also
that we consider ﬁle-dependent costs (via variables ρ(cid:48)f
t ), as well as cost contributions
which are common across ﬁles (via ρ(cid:48)
t). In most practical setups, the latter will dominate
over the former. For example, the caching cost per bit is likely to be the same regardless of the
particular type of content, so that ρ(cid:48)f
t can
correspond to actual prices paid to an external entity (e.g., if associated with energy consumption

t = 0. From a modeling perspective, variables ρf

t and ρ(cid:48)(cid:48)f

t = ρ(cid:48)(cid:48)f

t and ρ(cid:48)(cid:48)

costs), marginal utility or cost functions, congestion indicators, Lagrange multipliers associated

with constraints, or linear combinations of those (see, e.g., [114, 63, 42, 192] and Section 4.16).

Accordingly, the corresponding form for the fetching cost is

λf
t = σf (λ(cid:48)

t + λ(cid:48)f

t ) + (λ(cid:48)(cid:48)

t + λ(cid:48)(cid:48)f

t ).

(4.32)

As before, if the transmission link from the cloud to the SB is the same for all contents, the prices
λ(cid:48)

t are expected to dominate their ﬁle-dependent counterparts λ(cid:48)f

t and λ(cid:48)(cid:48)

t and λ(cid:48)(cid:48)f
t .

Upon deﬁning the corresponding cost for a given ﬁle as cf

t (af

t , wf

t ; ρf

t , λf

t ) = ρf

t af

t + λf

67
t wf
t ,

the aggregate cost at time t is given by

ct :=

F
(cid:88)

f =1

t (af
cf

t , wf

t ; ρf

t , λf

t ) =

F
(cid:88)

f =1

t af
ρf

t + λf

t wf
t ,

(4.33)

which is the basis for the DP formulated in the next section. For future reference, Fig. 4.13 shows

a schematic of the system model and the notation introduced in this section.

4.15 Optimal caching with time-varying costs

Since decisions are coupled across time [cf. constraint (4.28)], and the future values of prices

as well as state variables are inherently random, our goal is to sequentially make fetch-cache

decisions to minimize the long-term average discounted aggregate cost





∞
(cid:88)

F
(cid:88)

(cid:16)

γtcf
t

t , wf
af

t ; ρf

t , λf

t

(cid:17)





(4.34)

¯C := E

t=0

f =1

where the expectation is taken with respect to (w.r.t.) the random variables θf
t },
and 0 < γ < 1 is the discounting factor whose tuning trades off current versus more uncertain

t := {rf

t , λf

t , ρf

future costs. To address the optimization, the following assumptions are considered:

AS1) The values of θf

t are drawn from a stationary distribution.

AS2) The distribution of θf

t is known.

AS3) The drawn value of θf
t
decisions are made.

is revealed at the beginning of each slot t, before fetch-cache

AS1 and AS2 allow ﬁnding the expectations in this section, and will be relaxed in Section

III-E to further generalize our approach to settings where the distributions are unknown. In

practice, one may estimate these distributions through e.g., historical data.

The ultimate goal here is to take real-time fetch-cache decisions by minimizing the expected

current plus future cost while adhering to operational constraints, giving rise to the following

optimization

(P1)

min
k ,af

k )}f,k≥t

{(wf

s.t.

where

68

(cid:17)(cid:105)

¯Ct :=

∞
(cid:88)

F
(cid:88)

k=t

f =1

γk−tE

(cid:16)

(cid:104)
cf
k

k, wf
af

k ; ρf

k, λf

k

(wf

k , af

k) ∈ X (rf

k , af

k−1),

∀f, k ≥ t

X (rf

k , af

(cid:110)

(w, a)

k−1) :=
k = af
sf

k−1, rf

(cid:12)
(cid:12)
(cid:12) w ∈ {0, 1}, a ∈ {0, 1},
k ≤ w + sf

k, a ≤ sf

k + w

(cid:111)
,

and the expectation is taken w.r.t. {θf
k , af
The presence of the set X (rf

k }∀k≥t+1.
k−1) in the constraints demonstrates that the cache state
at a given time depends on previous cache decisions, thus coupling the optimization variables

across time. It also implies that any instantaneous decision will inﬂuence the optimization

problem in subsequent slots, having a long-standing inﬂuence on future costs. The coupling of

the optimization variables across time indeed necessitates utilization of DP tools, motivating the

implementation to reinforcement learning algorithms to design efﬁcient solvers.

To ﬁnd the solution of the DP in (P1) we implement the following steps: a) identifying the

current and expected future aggregate costs (the latter gives rise to the so-called value functions);

b) expressing the corresponding Bellman equations over the value functions; and c) proposing a

method to estimate the value functions accordingly. This is the subject of the ensuing subsections,

which start by further exploiting the structure of our problem to reduce the complexity of the

proposed solution.

4.15.1 Bellman equations for the per-content problem

Focusing on (P1), one can readily deduce that: (i) consideration of the content-dependent

prices renders the objective in (P1) separable across f , and (ii) the constraints in (P1) are also
separable across f . Furthermore, the decisions af
t for a given f , do not affect the values
(distribution) of θf (cid:48)
k(cid:48) for ﬁles f (cid:48) (cid:54)= f and for times t(cid:48) > t. Thus, (P1) naturally gives rise to the

t and wf

69

t and the content request rf

Figure 4.13: System model and main notation. The state variables (dashed lines) are the storage
t , as well as the dynamic caching and fetching prices ρf
indicator sf
t
and λf
t and
t . The instantaneous per-ﬁle cost is cf
wf
t . Per slot t, the SB collects the state
variables {sf
t , λf
t }F
f =1 considering not only the
cost at time t but also the cost at time instants t(cid:48) > t.

t . The optimization variables (solid lines) are the caching and fetching decisions af
t af

t = ρf
f =1, and decides the values of {af

t + λf

t , wf

t ; ρf

t , rf

t wf

t }F

per-ﬁle optimization

(P2)

min
k ,af

k )}k≥t

{(wf

¯Cf
t :=

∞
(cid:88)

k=t

γk−tE

(cid:16)

(cid:104)
cf
k

k, wf
af

k ; ρf

k, λf

k

(cid:17)(cid:105)

s.t.

(wf

k , af

k) ∈ X (rf

k , af

k−1),

k ≥ t

which must be solved for f = 1, ..., F . Indeed, the aggregate cost associated with (P2) will not
depend on variables corresponding to ﬁles f (cid:48) (cid:54)= f [114]. This is the case if, for instance, the

involved variables are independent of each other (which is the setup considered here), or when

the focus is on a large system where the contribution of an individual variable to the aggregate

network behavior is practically negligible.

Bellman equations and value function: The DP in (P2) can be solved with the corresponding

Bellman equations, which require ﬁnding the associated value functions. To this end, consider

the system at time t, where the cache state as well as the ﬁle requests and cost parameters
are all given, so that we can write sf
t = θf
0 . Then, the optimal fetch-cache
decision (wf ∗
t ) is readily expressible as the solution to (4.35). The objective in (4.35)
t
is rewritten in (4.36) as the summation of current and discounted average future costs. The

0 and θf

t = sf

, af ∗

form of (4.36) is testament to the fact that problem (P2) is a DP and the caching decision a

Cloud SB with storage (Fog)Users (Edge)Limited storage,(cid:16)

wf ∗

t , af ∗

t

(cid:17)

:= arg min
f
f
t ,a
t−1

(w,a)∈X (r

= arg min
f
f
t ,a
t−1

(w,a)∈X (r






)






)



E

θ

f
k




min
(wk ,ak )∈X (r




∞
(cid:88)

f
k

,a

f
k−1



)

k=t

γk−t (cid:104)

k (af
cf

k , wf

k ; ρf

(cid:12)
(cid:12)af
k , λf
(cid:12)
k )

t = a, wf

t = w, θf

t = θf

0

70














(cid:105)



t (a, w; ρf
cf

t , λf

t ) + E
θ






f
k

min
(wk ,ak )∈X (r

∞
(cid:88)

f
k

,a

f
k−1

)

k=t+1

γk−t (cid:104)

k (af
cf

k , wf

k ; ρf

(cid:12)
(cid:12)sf
k , λf
(cid:12)
k )

(cid:105)
t+1 = a




(4.35)






V f (cid:16)

sf , rf ; ρf , λf (cid:17)

min
:=
f
t ,a
(w,a)∈X (r

f
t−1






f
k

min
(wk ,ak )∈X (r




∞
(cid:88)

f
k

,a

f
k−1



)

k=t

γk−t (cid:104)

k (af
cf

k , wf

k ; ρf

(cid:12)
(cid:12)af
k , λf
(cid:12)
k )

t = a, wf

t = w, θf

t = θf (cid:105)
















(4.36)




E

θ

)



¯V f (sf ) :=E

θf






min
f
t ,a
(w,a)∈X (r









f
t−1
(cid:110)

)

θ

E

f
k



min
(wk ,ak )∈X (r

f
k−1
(cid:111)
cf
0 (a, w; ρf , λf ) + γ ¯V f (a)

f
k

,a

=E

θf

min
(w,a)∈X (rf ,sf )




∞
(cid:88)

)



k=t

γk−t (cid:104)

k (af
cf

k , wf

k ; ρf

k , λf
k )

(cid:12)
(cid:12)af
(cid:12)

t = a, wf

t = w, θf

t = θf (cid:105)






(4.37)
















(4.38)

inﬂuences not only the current cost cf
t (·), but also future costs through the second term as well.
Bellman equations can be leveraged for tackling such a DP. Under the stationarity assumption
for variables rf
t , the term accounting for the future cost can be rewritten in terms
of the stationary value function V f (cid:0)sf , rf ; ρf , λf (cid:1). This function, formally deﬁned in (4.37),
captures the minimum sum average cost for the “state” (sf , rf ), parametrized by (λf , ρf ), where
for notational convenience, we deﬁne θf := [rf , ρf , λf ].

t and λf

t , ρf

4.15.2 Marginalized value-function

If one further assumes that price parameters and requests are i.i.d. across time, it can be shown

that the optimal solution to (P2) can be expressed in terms of the reduced value function [114]

¯V f (cid:16)

sf (cid:17)

:= Eθf

(cid:104)
V f (cid:16)

sf , rf ; ρf , λf (cid:17)(cid:105)

,

(4.39)

where the expectation is w.r.t θf . Marginalization of the value function is important not only
because it captures the average future cost of ﬁle f for cache state sf ∈ {0, 1}, but also because
¯V f (·) is a function of a binary variable, and therefore its estimation requires only estimating two
values. This is in contrast with the original four-dimensional value function in (4.37), whose

estimation is more difﬁcult due to its continuous arguments.

71
By rewriting the proposed alternative value function ¯V f (·) in a recursive fashion as the
summation of instantaneous cost and discounted future values ¯V f (·), one readily arrives at the
Bellman equation form provided in (4.38). Thus, the problem reduces to ﬁnding ¯V f (0) and
¯V f (1) for all f , after which the optimal fetch-cache decisions (wf ∗
t ) are easily found as the
t
solution to

, af ∗

(P3) min
(w,a)

s.t.

t (a, w; ρf
cf

t , λf

t ) + γ ¯V f (a)

(w, a) ∈ X (rf

t , af

t−1).

If the value-function is known, so that we have access to ¯V f (0) and ¯V f (1), the corresponding

optimal (Bellman) decisions can be found as

t = I{∆ ¯V f

γ ≥λf

t +ρf
t }

t , af
t = af
wf
t = 0, af
wf
t = 1, af
wf
t = 0, af
wf

t = I{∆ ¯V f

γ ≥ρf
t }

t = I{∆ ¯V f

γ ≥ρf
t }

t = I{∆ ¯V f

γ ≥ρf
t }

if (rf

if (rf

if (rf

t , sf
t , sf
t , sf
t , sf

if (rf

t ) = (0, 0)

t ) = (0, 1)

t ) = (1, 0)

t ) = (1, 1)

(4.40a)

(4.40b)

(4.40c)

(4.40d)

γ represents the future marginal cost, which is obtained as ∆ ¯V f

γ = γ( ¯V f (1)− ¯V f (0)),
where ∆ ¯V f
and I{·} is an indicator function that yields value one if the condition in the argument holds, and
zero otherwise.

The next subsection discusses how ¯V f (0) and ¯V f (1) can be calculated, but ﬁrst a remark is

in order.

t , ρf

t or λf

Remark 1 (Augmented value functions). The value function ¯V f (sf ) can be redeﬁned to
account for extra information on rf
t , if available. For instance, consider the case where
the distribution of rf
t can be parametrized by pf , which measures content “popularity” [31]. In
such cases, the value function can incorporate the popularity parameter as an additional input
to yield ¯V f (sf , pf ). Consequently, the optimal decisions will depend not only on the current
requests and prices, but also on the (current) popularity pf . This indeed broadens the scope
of the proposed approach, as certain types of non-stationarity in the distribution of rf
handled by allowing pf to (slowly) vary with time.

t can be

¯V1 = (1 − p)

(cid:16)

E min

a∈{0,1}

(cid:104)
γ ¯V0(1 − a) + (ρ + γ ¯V1)a

(cid:12)
(cid:12)
(cid:12)s = 1, r = 0

(cid:105) (cid:17)

+ p

(cid:16)

E min

a∈{0,1}

72

(cid:104)
γ ¯V0(1 − a) + (ρ + γ ¯V1)a

(cid:12)
(cid:12)
(cid:12)s = 1, r = 1

(cid:105) (cid:17)

= γ ¯V0 Pr (cid:0)ρ ≥ ∆ ¯Vγ

(cid:16)
(cid:1) + E
ρ + γ ¯V1

(cid:12)
(cid:12)ρ < ∆ ¯Vγ
(cid:12)

(cid:17)

Pr (cid:0)ρ < ∆ ¯Vγ

(cid:1)

(cid:16)
¯V0 = (1 − p)

E min

a∈{0,1}

(cid:104)
γ ¯V0(1 − a) + (λ + ρ + γ ¯V1)a

(cid:12)
(cid:12)
(cid:12)s = 0, r = 0

(cid:105) (cid:17)

(cid:16)

+ p

E min

a∈{0,1}

(cid:104)
(λ + γ ¯V0)(1 − a) + (λ + ρ + γ ¯V1)a

(cid:12)
(cid:12)
(cid:12)s = 0, r = 1]

(cid:105) (cid:17)

= (1 − p)

(cid:16)
γ ¯V0 Pr (cid:0)λ + ρ ≥ ∆ ¯Vγ

(cid:1) + E

+ p

(cid:16)
E[λ] + γ ¯V0 Pr (cid:0)ρ ≥ ∆ ¯Vγ

(cid:16)

(cid:1) + E

(cid:16)
λ + ρ + γ ¯V1
(cid:12)
(cid:12)ρ ≤ ∆ ¯Vγ
(cid:12)

ρ + γ ¯V1

(cid:12)
(cid:12)λ + ρ < ∆ ¯Vγ
(cid:12)

(cid:17)

(cid:17)

Pr (cid:0)ρ ≤ ∆ ¯Vγ

Pr (cid:0)λ + ρ < ∆ ¯Vγ
(cid:1) (cid:17)

(4.41)

(4.42)

(4.43)

(cid:1) (cid:17)

Algorithm 6 Value iteration for ﬁnding ¯V (·)
Initialize γ < 1, probability density function of ρ, λ and r, precision (cid:15), in order to stop
Initialize ¯V0, ¯V1
s − ¯V i+1
While | ¯V i
s
For
¯V i+1
s = Er,ρ,λ min

(cid:8)c(a, w; ρ, λ) + γ ¯V i
a

| < (cid:15); s ∈ {0, 1}

(cid:9) i = i + 1

s = 0, 1

(w,a)∈X (r,s)

4.15.3 Value function in closed form

For notational brevity, we have removed the superscript f in this subsection, and use ¯V0 and
¯V1 in lieu of ¯V (0), and ¯V (1). Denoting the long-term popularity of the content as p := E[rt],
using the expressions for the optimal actions in (4.40a)-(4.40d), and leveraging the independence

among rt, λt, and ρt, the expected cost-to-go function can be readily derived as in (4.41)-(4.43).
The expectation in (4.41) is w.r.t. ρ, while that in (4.42) is w.r.t. both λ and ρ.

Solving the system of equations in (4.41)-(4.43) yields the optimal values for ¯V1 and ¯V0. A
simple solver would be to perform exhaustive search over the range of these values since it is

only a two-dimensional search space. However, a better alternative to solving the given system

of equations is to rely on the well known value iteration algorithm. In short, this is an ofﬂine
algorithm, which per iteration i updates the estimates { ¯V i+1
cost using { ¯V i
Algorithm 1, for which the distributions of r, ρ, λ are assumed to be known.

} by computing the expected
0
1 }, until the desired accuracy is achieved. This scheme is tabulated in detail in

, ¯V i+1
1

0 , ¯V i

Remark 2 (Finite-horizon approximate policies). In the proposed algorithms, namely exhaus-

tive search as well as Algorithm 1, the solver is required to compute an expectation, which can

be burdensome in setups with limited computational resources. For such scenarios, the class

73

of ﬁnite-horizon policies emerges as a computationally affordable suboptimal alternative. The

idea behind such policies is to truncate the inﬁnite summation in the objective of (P1); thus,

only considering the impact of the current decision on a few number of future time instants

denoted by h, typically referred to as the horizon. The extreme case of a ﬁnite-horizon policy

is that of a myopic policy with h = 0, which ignores any future impact of current decision,

a.k.a. zero-horizon policy, thus taking the action which minimizes the instantaneous cost. This

is equivalent to setting the future marginal cost to zero, hence solving (4.40a)-(4.40d) with
∆ ¯V γ = ∆ ¯V h=0

γ = 0.

Another commonly used alternative is to consider the impact of the current decision for only

the next time instant, which corresponds to the so-called horizon-1 policy. This entails setting
the future cost at h = 1 as ∆ ¯V h=1
1 − ¯V h=0

γ = γ( ¯V h=0

) with

0

¯V h=0
0

¯V h=0
1

= (1 − p)E[λwh=0 + ρah=0|s = 0, r = 0]

+ pE[λwh=0 +ρah=0|s = 0, r = 1] = pE[λ]

= (1 − p)E[λwh=0 + ρah=0|s = 1, r = 0]

+ pE[λwh=0 + ρah=0|s = 1, r = 1] = 0,

(4.44)

(4.45)

which are then substituted into (4.40a)-(4.40d) to yield the actions wh=1 and ah=1. The notation
wh=0 and ah=0 in (4.44) and (4.45) is used to denote the actions obtained when (4.40a)-(4.40d)
are solved using the future marginal cost at horizon zero ∆ ¯V h=0
is zero; that is, under the myopic policy in lieu of the original optimal solution. Following an
inductive argument, the future marginal cost at h = 2 is obtained as ∆ ¯V h=2
1 − ¯V h=1
with

, which as already mentioned,

γ = γ( ¯V h=1

)

γ

0

¯V h=1
0

= (1 − p)E[λwh=1 + ρah=1 + γ ¯V h=0

a

|s = 0, r = 0]

+ pE[λwh=1 +ρah=1 + γ ¯V h=0

a

|s = 0, r = 1],

¯V h=1
1

= (1 − p)E[λwh=1 + ρah=1 + γ ¯V h=0

a

|s = 1, r = 0]

+ pE[λwh=1 + ρah=1 + γ ¯V h=0

a

|s = 1, r = 1],

which will allow to obtain the actions wh=2 and ah=2. While increasing horizons can be used,

as h grows large, solving the associated equations becomes more difﬁcult and computation of

the optimal stationary policies, is preferable.

Q (st, rt, wt, at; ρt, λt) := E






(cid:110)

min

(wk ,ak )∈X (rk ,ak−1)




∞
(cid:88)



k=t

(cid:111)∞

k=t+1

γk−t (cid:104)

(cid:12)
(cid:12)
(cid:12)at, wt, θt = θ
ck(ak, wk; ρk, λk)









(cid:105)



= ct(at, wt; ρt, λt)
(cid:125)

(cid:123)(cid:122)
Immediate cost

(cid:124)






+γ E

(cid:124)

(cid:110)

min

(wk ,ak )∈X (rk ,ak−1)

(cid:111)∞

k=t+1




∞
(cid:88)



k=t+1

γk−(t+1) (cid:104)

(cid:12)
(cid:12)
(cid:12)st+1 = at
ck(ak, wk; ρk, λk)









(cid:105)



(cid:123)(cid:122)
Average minimum future cost

(cid:125)

¯Qwt,at
rt,st

:= Eρt,λt

[Q (st, rt, wt, at; ρt, λt)] , ∀(wt, at) ∈ X (rt, at−1)

= Eρt,λt

[ct(at, wt; ρt, λt)] + γ

(cid:104)
Eθt+1

(cid:16)

(cid:104)

Q

st+1, rt+1, w∗

t+1, a∗

t+1; ρt+1, λt+1

(cid:17) (cid:12)
(cid:12)
(cid:12)θt+1, st+1 = at

(cid:105)(cid:105)

.

¯Qw,a

r,s = E[λ]w + E[ρ]a + γ(1 − p)

(cid:88)

¯Q

z1,z2
0,a Pr

(cid:16)

(w∗

t+1, a∗

t+1) = (z1, z2)|(st+1, rt+1) = (a, 0)

∀(z1 ,z2)∈X (0,a)

+γp

(cid:88)

¯Q

z1,z2
1,a Pr

∀(z1,z2)∈X (1,a)

(cid:16)

(w∗

t+1, a∗

t+1) = (z1, z2)|(st+1, rt+1) = (a, 1)

(cid:17)

(cid:17)
.

74

(4.46)

(4.47)

(4.48)

(4.49)

4.15.4 State-action value function (Q-function):

In many practical scenarios, knowing the underlying distributions for ρt, λt and rt may not
be possible, which motivates the introduction of online solvers that can learn the parameters

on-the-ﬂy. As clariﬁed in the ensuing sections, in such scenarios, the so-called Q-function (or

state-action value function) becomes helpful, since there are rigorous theoretical guarantees on

the convergence of its stochastic estimates; see [143] and [201]. Motivated by this fact, instead

of formulating our dynamic program using the value (cost-to-go) function, we can alternatively

formulate it using the Q-function. Aiming at an online solver, let us tackle the DP through the
estimation (learning) of the Q-function. Equation2 (4.46) deﬁnes the Q-function for a speciﬁc

ﬁle under a given state (st, rt), parametrized by cost parameters (ρt, λt). Under stationarity
distribution assumption for {ρt, λt, rt}, the Q-function Q (st, rt, wt, at; ρt, λt) accounts for the
minimum average aggregate cost at state (st, rt), and taking speciﬁc fetch-cache decision (wt, at)
as for the ﬁrst decision, while followed by the best possible decisions in next slots. This function

is parametrized by (ρt, λt) since while making the current cache-fetch decision, the current
values for these cost parameters are assumed to be known. The original Q-function in (4.46)

needs to be learned over all values of {st, rt, wt, at, ρt, λt, rt}, thus suffering from the curse of
dimensionality, especially due to the fact that ρt and λt are continuous variables.

To alleviate this burden, we deﬁne the marginalized Q-function Q(st, rt, wt, at) in (4.48).

2Equations (4.46)-(4.48), and (4.49) are shown at the top of page 7.

By changing the notation for clarity of exposition, the marginalized Q-function, ¯Qwt,at
rewritten in a more compact form as

rt,st , can be

75

¯Qwt,at

(cid:104)
rt,st = E

λtwt + ρtat + γ ¯Q

w∗
t+1,a∗
rt+1,at

t+1

(cid:105)

∀(wt,at) ∈ X (rt, at−1).

(4.50)

Note that, while the marginalized value-function is only a function of the state, the marginalized

Q-function depends on both the state (r, s) and the immediate action (w, a). The main reason one

prefers to learn the value-function rather than the Q-function is that the latter is computationally
more complex. To see this, note that the input space of ¯Qwt,at
is a four-dimensional binary
rt,st
space, hence the function has 24 = 16 different inputs and one must estimate the corresponding

16 outputs. Each of these possible values are called Q-factors, and under the stationarity

assumption, they can be found using (4.49) deﬁned for all (r, s, w, a). In this expression, we
have (z1, z2) ∈ {0, 1}2 and the term Pr (cid:0)(cid:0)w∗
(cid:1) = (z1, z2)(cid:1) stands for the probability of
t+1a∗
speciﬁc action (z1, z2) to be optimal at slot t + 1. This action is random because the optimal
decision at t + 1 depends on ρt+1, λt+1 and rt+1, which are not known at slot t. Although not
critical for the discussion, if needed, one can show that half of the 16 Q-factors can be discarded,

t+1

either for being infeasible – recall that (wt, at) ∈ X (rt, at−1) – or suboptimal. This means that
(4.49) needs to be computed only for 8 of the Q-factors.

From the point of view of ofﬂine estimation, working with the Q-function is more challenging

than working with the V -function, since more parameters need to be estimated. In several realistic

scenarios however, the distributions of the state variables are unknown, and one has to resort to

stochastic schemes in order to learn the parameters on-the-ﬂy. In such scenarios, the Q-function

based approach is preferable, because it enables learning the optimal decisions in an online

fashion even when the underlying distributions are unknown.

4.15.5 Stochastic policies: Reinforcement learning

As discussed in Section 4.15.3, there are scenarios where obtaining the optimal value function

(and, hence, the optimal stationary policy associated with it) is not computationally feasible. The

closing remark in that section discussed policies which, upon replacing the optimal value function

with approximations easier to compute, trade reduced complexity for loss in optimality. However,

such reduced-complexity methods still require knowledge of the state distribution [cf. (4.44) and

(4.45)]. In this section, we discuss stochastic schemes to approximate the value function under

76

unknown distributions, thus relaxing assumption AS2 made earlier. The policies resulting from

such stochastic methods offer a number of advantages since they: (a) incur a reduced complexity;

(b) do not require knowledge of the underlying state distribution; (c) are able to handle some

non-stationary environments; and in some cases, (d) they come with asymptotic optimality

guarantees. To introduce this scheme, we ﬁrst start by considering a simple method that updates

stochastic estimates of the value function itself, and then proceed to a more advanced method

which tracks the value of the Q-function. Speciﬁcally, the presented method is an instance of the

celebrated Q-learning algorithm [200], which is the workhorse of stochastic approximation in

DP.

Stochastic value function estimates

The ﬁrst method relies on current stochastic estimates of ¯V0 and ¯V1, denoted by ˆ¯V0(t) and ˆ¯V1(t)
at time t (to be deﬁned rigorously later). Given ˆ¯V0(t) and ˆ¯V1(t) at time t, the (stochastic) actions
ˆwt and ˆat are taken via solving (4.40a)-(4.40d) with ∆ ¯V γ = γ( ˆ¯V0(t) − ˆ¯V1(t)). Then, stochastic
estimates of the value functions ˆ¯V0(t) and ˆ¯V1(t) are updated as

• If st = 0, then ˆ¯V1(t+1) = ˆ¯V1(t) and ˆ¯V0(t+1) = (1−βt) ˆ¯V0(t)+βt( ˆwtλt+ˆatρt+γ ˆ¯Vˆat(t));

• If st = 1, then ˆ¯V0(t + 1) = ˆ¯V0(t) and ˆV1(t + 1) = (1 − βt) ˆ¯V1(t) + βt( ˆwtλt + ˆatρt +

γ ˆ¯Vˆat(t));

where βt > 0 denotes the stepsize. While easy to implement (only two recursions are required),
this algorithm has no optimality guarantees.

Q-learning algorithm

r,s with stochastic estimates ˆ¯Qw,a

Alternatively, one can run a stochastic approximation algorithm on the Q-function. This entails
replacing the Q-factors ¯Qw,a
r,s (t). To describe the algorithm,
suppose for now that at time t, the estimates ˆ¯Qw,a
r,s (t) are known for all (r, s, w, a). Then, in a
given slot t with (rt, st), action ( ˆw∗
t ) is obtained via either an exploration or an exploitation
step. When exploring, which happens with a small probability (cid:15)t, a random and feasible action
( ˆw∗
t ) ∈ X (rt, at−1) is taken. In contrast, in the exploitation mode, which happens with a

t , ˆa∗

t , ˆa∗

probability 1 − (cid:15)t, the optimal action according to the current estimate of ˆ¯Qw,a

r,s (t) is

+ 1 ( ˆw∗

t , ˆa∗

t ) :=

arg min
(w,a)∈X (rt,at−1)

wλt + aρt + γ ˆ¯Qw,a

rt,st(t).

77

(4.51)

After taking this action, going to next slot t + 1, and observing ρt+1, λt+1, and rt+1, the
Q-function estimate is updated as

ˆ¯Qw,a


r,s (t)

r,s (t + 1) =
ˆ¯Qw,a


(1−βt) ˆ¯Q ˆw∗

if

(r, s, w, a) (cid:54)= (rt, st, ˆw∗

t , ˆa∗
t )

t ,ˆa∗
rt,st (t) + βt
t

(cid:16)

ˆw∗

t λt + ˆa∗

t ρt + γ ˆ¯Q

ˆw∗
t+1,ˆa∗
rt+1,ˆa∗
t

t+1

(cid:17)

(t)

o.w.,

(4.52)

where “o.w.” stands for “otherwise”, ( ˆw∗

t+1) is the optimal action for the next slot and,
if needed, the stepsize βt can be adapted for each particular state-action pair. This update rule
describes one of the possible implementations of the Q-learning algorithm, which was originally

t+1, ˆa∗

introduced in [200]. This online algorithm enables making sequential decisions in an unknown

environment, and is guaranteed to learn optimal decision-making rules under certain conditions

speciﬁed next [201].

Regarding convergence of the Q-learning algorithm, the following necessary conditions

should hold [201, 28]: (c1) all feasible state (r, s) and action (w, a) pairs should be continuously

updated; and, (c2) the learning rate βt should be a diminishing step size. Under these conditions,
the factors ˆ¯Qw,a
r,s with probability 1; see [28] for details.
To satisfy (c1), various exploration-exploitation algorithms have been proposed [155, p. 839].

r,s converge to their optimal value ¯Q∗w,a

Particularly, any such scheme needs to be greedy in the limit of inﬁnite exploration, or GLIE [155,

p. 840]. A common choice to meet this property is the (cid:15)-greedy approach, as considered in this

work, with (cid:15)t = 1/t, which provides guaranteed yet slow convergence. In practice however,
(cid:15)t can be set to a small value for faster convergence [28], [143]. To satisfy the diminishing
step size rule in (c2), let us deﬁne tw,a
r,s as the index of the t-th time when the state-action pair
(r, s) and (w, a) is visited, and updated with the corresponding learning rate βtw,a
r,s . Condition
(c2) requires (cid:80)∞
= ∞, and (cid:80)∞
t=1β2
< ∞ to hold for all feasible state-action pairs,
t=1 βtw,a
tw,a
r,s
a typical choice for which is setting βtw,a
= 1/t. Similar to (cid:15)t, a constant but small learning
rate is preferred in practice as it endows the algorithm to adapt to possible changes of pertinent

r,s

r,s

r,s (1) = 0, s1 = 0, {r0, ρ0, λ0} are revealed

Algorithm 7 Q-learning algorithm to estimate ¯Qw,a
Initialize ˆ¯Qw,a
Output ˆ¯Qw,a
For
For the current state (rt, st), choose ( ˆw∗

t = 1, 2, . . .

r,s (t + 1)

r,s for a given ﬁle f

t , ˆa∗
t )
(cid:26) Solve (4.51)w.p. 1 − (cid:15)t

( ˆw∗

t , ˆa∗

t ) =

random (w, a) ∈ Xt(rt, st)w.p. (cid:15)t

78

(4.53)

Update state st+1 = ˆa∗
t
Request and cost parameters, θt+1, are revealed
Update Q factor by (4.52)

parameters in dynamic settings.

The resultant algorithm for the problem at hand is tabulated in Algorithm 7. It is important

to stress that in our particular case, we expect the algorithm to converge fast. That is the case

because, under the decomposition approach followed in this paper as well as the introduction

of the marginalized Q-function, the state-action space of the resultant Q-function has very low

dimension and hence, only a small number of Q-factors need to be estimated.

4.16 Limited storage and back-haul transmission rate via dynamic

pricing

So far, we have considered that the prices {ρf

t } are provided by the system, and we have
not assumed any explicit limits (bounds) neither on the capacity of the local storage nor on the

t , λf

back-haul transmission link between the SB and the cloud. In this section, we discuss such

limitations, and describe how by leveraging dual decomposition techniques, one can redeﬁne the
prices {ρf

t } to account for capacity constraints.

t , λf

4.16.1 Limiting the instantaneous storage rate

In this subsection, practical limitations on the cache storage capacity are explored. Suppose

that the SB is equipped with a single memory device that can store M ﬁles. Clearly, the cache

decisions should then satisfy the following constraint per time slot

79

C4:

F
(cid:88)

f =1

af
t σf ≤ M,

t = 1, 2, . . .

In order to respect such hard capacity limits, the original optimization problem in (P1) can

be simply augmented with C4, giving rise to a new optimization problem which we will refer

to as (P4). Solving (P4) is more challenging than (P1), since the constraints in C4 must be

enforced at each time instant, which subsequently couples the optimization across ﬁles. In

order to deal with this, one can dualize C4 by augmenting the cost with the primal-dual term
µt((cid:80)F
t − M ), where µt denotes the Lagrange multiplier associated with the capacity
constraint C4. The resultant problem is separable across ﬁles, but requires ﬁnding µ∗
t , the optimal

f =1 σf af

value of the Lagrange multiplier, at each and every time instant.

If the solution to the original unconstrained problem (P1) does satisfy C4, then µ∗

t = 0 due to
complementary slackness. On the other hand, if the storage limit is violated, then the constraint
is active, the Lagrange multiplier satisﬁes µ∗
t > 0, and its exact value must be found using an
iterative algorithm. Once the value of the multiplier is known, the optimal actions associated

with (P4) can be found using the expressions for the optimal solution to (P1) provided that the
original storage price ρf
t σf [cf. (4.31)].
The reason for this will be explained in detail in the following subsection, after introducing the

t is replaced with the new storage price ρf

t,aug = ρf

t + µ∗

ensemble counterpart of C4.

4.16.2 Limiting the long-term storage rate

Consider now the following constraint [cf. C4]

C5:

∞
(cid:88)

k=t

γk−tE



af
kσf

 ≤





F
(cid:88)

f =1

∞
(cid:88)

k=t

γk−tM (cid:48)

(4.54)

where the expectation is taken w.r.t. all state variables. By setting M (cid:48) = M , one can view C5 as

a relaxed version of C4. That is, while C4 enforces the limit to be respected at every time instant,

C5 only requires it to be respected on average. From a computational perspective, dealing

with C5 is easier than its instantaneous counterpart, since in the former only one constraint is

enforced and, hence, only one Lagrange multiplier, denoted by µ, must be found. This comes at

80
the price that guaranteeing C5 with M (cid:48) = M does not imply that C4 will always be satisﬁed.
Alternatively, enforcing C5 with M (cid:48) < M , will increase the probability of satisfying C4, since

the solution will guarantee that “on average” there exists free space on the cache memory. A

more formal discussion on this issue will be provided in the remark closing the subsection.

To describe in detail how accounting for C5 changes the optimal schemes, let (P5) be the

problem obtained after augmenting (P1) with C5. Suppose now that to solve (P5) we dualize

the single constraint in C5. Rearranging terms, the augmented objective associated with (P5) is

given by

∞
(cid:88)

F
(cid:88)

k=t

f =1

γk−tE

(cid:16)

(cid:104)
cf
k

k, wf
af

k ; ρf

k, λf

k

(cid:17)

+ µaf

kσf (cid:105)

−

∞
(cid:88)

k=t

γk−tM (cid:48).

(4.55)

Equation (4.55) demonstrates that after dualization and provided that the multiplier µ is known,

decisions can be optimized separately across ﬁles. To be more precise, note that the term
(cid:80)∞
k=t γk−tM (cid:48) in the objective is constant, so that it can be ignored, and deﬁne the modiﬁed

instantaneous cost as

(cid:16)

k, λf
k ; ρf
k, wf
af
k := cf
ˇcf
k
k + µσf (cid:17)
(cid:16)
k + λf
af
ρf

=

k

kwf
k .

(cid:17)

+ µσf af
k

(4.56)

The last equation not only reﬂects that the dualization indeed facilitates separate per-ﬁle op-
timization, but it also reveals that term µσf can be interpreted as an additional storage cost

associated with the long-term caching constraint. More importantly, by deﬁning the modiﬁed
(augmented) prices ρf
t + µσf for all t and f , the optimization of (4.56) can be carried
out with the schemes presented in the previous sections, provided that ρf
t,aug.
Note however that in order to run the optimal allocation algorithm, the value of µ needs to be

t is replaced with ρf

t,aug := ρf

known. Since the dual problem is always convex, one option is to use an iterative dual subgradient

method, which computes the satisfaction/violation of the constraint C5 per iteration [142], [30,

p.223]. Clearly, this requires knowledge of the state distribution, since the constraint involves an

expectation. When such knowledge is not available, or when the computational complexity to

carry out the expectations cannot be afforded, stochastic schemes are worth considering. For

the particular case of estimating Lagrange multipliers associated with long-term constraints, a

simple but powerful alternative is to resort to stochastic dual subgradient schemes [142], [30],

which for the problem at hand, estimate the value of the multiplier µ at every time instant t using

the update rule

81





ˆµt+1 =

ˆµt + ζ



F
(cid:88)

f =1





+

ˆaf ∗
t σf − M (cid:48)





.

(4.57)

In the last expression, ζ > 0 is a (small) positive constant, the update multiplied by ζ corresponds
to the violation of the constraint after removing the expectation, the notation [·]+ stands for the
max{0, ·}, and ˆaf ∗
Section 4.15 provided that ρf

t,aug = ρf
We next introduce another long-term constraint that can be considered to limit the storage

t denotes the optimal caching actions obtained with the policies described in

t is replaced by ˆρf

t + ˆµtσf .

rate. This constraint is useful not only because it gives rise to alternative novel caching-fetching

t − sf

t ]+ and αf

schemes, but also because it will allow us to establish connections with well-known algorithms
in the area of congestion control and queue management. To start, deﬁne the variables αf
in,t :=
[af
in,t = 1, then content f that
was not in the local cache at time t − 1, has been stored at time t; and as a result, less storage
space is available. On the other hand, if αf
out,t = 1, then content f was removed from the cache
at time t, thus freeing up new storage space. With this notation at hand, we can consider the long

t ]+ for all f and t. Clearly, if αf

out,t := [sf

t − af

term constraint

C6:

∞
(cid:88)

k=t

γk−tE





F
(cid:88)

f =1



αf
in,kσf

 ≤

∞
(cid:88)

k=t

γk−tE



αf
out,kσf

 ,





F
(cid:88)

f =1

(4.58)

which basically ensures the long-term stability of the local-storage. That is, the amount of data

stored in the local memory is no larger than that taken out from the memory, guaranteeing that in

the long term stored data does not grow unbounded.

To deal with C6 we can follow an approach similar to that of C5, under which we ﬁrst dualize

C6 and then use a stochastic dual method to estimate the associated dual variable. With a slight

abuse of notation, supposing that the Lagrange multiplier associated with stability is by also

denoted µ, the counterpart of (4.57) for the constraint C6 is



ˆµt+1 =

ˆµt + ζ

F
(cid:88)

[ˆaf ∗

t − sf

t ]+ − [sf

t − ˆaf ∗

t



+

]+



.

(4.59)

f =1

82

t with ˆρf

t,aug = ρf

in,t, and αf

t + ˆµtσf . However, if sf

Note that the update term in the last iteration follows after removing the expectations in C6
and replacing αf
out,t with their corresponding deﬁnitions. The modiﬁcations that the
expressions for the optimal policies require to account for this constraint are a bit more intricate.
If sf
t = 0, the problem structure is similar to that of the previous constraints, and we just need to
replace ρf
t = 1
does not require modifying the caching price, but ii) deciding ˆaf ∗
t = 0 requires considering
the negative caching price −ˆµtσf . In other words, while our formulation in Section 4.15 only
considers incurring a cost when af
t = 1 (and assumes that the instantaneous cost is zero for
af
t = 0), to fully account for C6, we would need to modify our original formulation so that costs
can be associated with the decision af
t = 0 as well. This can be done either by considering a
new cost term or, simply by replacing γ ¯V f (0) by γ ¯V f (0) − ˆµtσf in (4.40a)-(4.40d), which are
Bellman’s equations describing the optimal policies.

t = 1, it turns out that: i) deciding ˆaf ∗

Remark 3 (Role of the stochastic multipliers). It is well-established that the Lagrange multipli-

ers

can be

interpreted as

the marginal price

that

the

system must pay to

(over-)satisfy the constraint they are associated with [30, p.241]. When using stochastic methods

for estimating the multipliers, further insights on the role of the multipliers can be obtained

[63, 128, 42]. Consider for example the update in (4.57). The associated constraint C5 es-
tablishes that the long-term storage rate cannot exceed M (cid:48). To guarantee so, the stochastic

scheme updates the estimated price in a way that, if the constraint for time t is oversatisﬁed, the

price goes down, while if the constraint is violated, the price goes up. Intuitively, if the price

estimate ˆµt is far from its optimal value and the constraint is violated for several consecutive
time instants, the price will keep increasing, and eventually will take a value sufﬁciently high so

that storage decisions are penalized/avoided. How quickly the system reacts to this violation can
be controlled via the constant ζ. Interestingly, by tuning the values of M (cid:48) and ζ, and assuming

some regularity properties on the distribution of the state variables, conditions under which

deterministic short-term limits as those in C4 are satisﬁed can be rigorously derived; see, e.g.,

[42] for a related problem in the context of distributed cloud networks. A similar analysis can be

carried out for the update in (4.59) and its associated constraint C6. Every time the instantaneous

version of the constraint is violated because the amount of data stored in the memory exceeds the

amount exiting the memory, the corresponding price ˆµt increases, thus rendering future storage
decisions more costly. In fact, if we initialize the multiplier at ˆµt = 0 and set ζ = 1, then the
corresponding price is the total amount of information stored at time t in the local memory. In

other words, the update in (4.59) exempliﬁes how the dynamic prices considered in this paper

can be used to account for the actual state of the caching storage. Clearly, additional mappings

from the instantaneous storage level to the instantaneous storage price can be considered. The

connections between stochastic Lagrange multipliers and storing devices have been thoroughly

explored in the context of demand response, queuing management and congestion control. We

refer the interested readers to, e.g., [63, 128].

83

4.16.3 Limits on the back-haul transmission rate

The previous two subsections dealt with limited caching storage, and how some of those lim-
itations could be accounted for by modifying the caching price ρf
t . This section addresses
limitations on the back-haul transmission rate between the SB and the cloud as well as their
impact on the fetching price λf
t .

While our focus has been on optimizing the decisions at the SB, contemporary networks

must be designed following a holistic (cross-layer) approach that accounts for the impact of local

decisions on the rest of the network. Decomposition techniques (including those presented in

this paper) are essential to that end [142]. For the system at hand, suppose that xCD includes
all variables at the cloud network, ¯CCD(xCD) denotes the associated cost, and the feasible set
XCD accounts for the constraints that cloud variables xCD must satisfy. Similarly, let xSB,
¯CSB(xSB), and XSB denote the corresponding counterparts for the SB optimization analyzed in
this paper. Clearly, the fetching actions wf
t are included in xSB, while the variable bt representing
back-haul transmission rate (capacity) of the connecting link between the cloud and the SB, is

included in xCD. This transmission rate will depend on the resources that the cloud chooses to
allocate to that particular link, and will control the communication rate (and hence the cost of

fetching requests) between the SB and the cloud. As in the previous section, one could consider

two types of capacity constraints

C7a :

C7b :

F
(cid:88)

f =1

∞
(cid:88)

k=t

wf

t σf ≤ bt,

t = 1, . . . ,

γk−t

F
(cid:88)

f =1

E[wf

t σf ] ≤

∞
(cid:88)

k=t

γk−tE[bk],

(4.60a)

(4.60b)

depending on whether the limit is imposed in the short term or in the long term.

With these notational conventions, one could then consider the joint resource allocation

84

problem

min
xCD,xSB

¯CCD(xCD) + ¯CSB(xSB)

s.t. xCD ∈ XCD, xSB ∈ XSB, (C7)

(4.61)

where the constraint C7 – either the instantaneous one in C7a or the lon-term version in C7b –

couples both optimizations. It is then clear that if one dualizes C7, and the value of the Lagrange

multiplier associated with C7 is known, then two separate optimizations can be run: one focusing

on the cloud network and the other one on the SB. For this second optimization, consider

for simplicity that the average constraint in (4.60b) is selected and let ν denote the Lagrange

multiplier associated with such a constraint. The optimization corresponding to the SB is then

¯CSB(xSB) +

min
xSB

∞
(cid:88)

k=t

γk−t

F
(cid:88)

f =1

E[wf

t νσf ]

s.t. xSB ∈ XSB.

(4.62)

Clearly, solving this problem is equivalent to solving the original problem in Section 4.15,

provided that the original cost is augmented with the primal-dual term associated with the

coupling constraint. To address the modiﬁed optimization, we will follow steps similar to those

in Section 4.16.2, deﬁning ﬁrst a stochastic estimate of the Lagrange multiplier as





ˆνt+1 =

ˆνt + ζ



F
(cid:88)

f =1





+

ˆwf ∗

t σf − bt





,

(4.63)

and then obtaining the optimal caching-fetching decisions running the schemes in Section 4.15
after replacing the original fetching cost λf

t with the augmented one λf

t,aug = λf

t + ˆνtσf .

For simplicity, in this section we will limit our discussion to the case where ˆνt corresponds
to the value of a Lagrange multiplier corresponding to a communication constraint. However,

from a more general point of view, ˆνt represents the marginal price that the cloud network has to
pay to transmit the information requested by the SB. In that sense, there exists a broad range of

options to set the value of ˆνt, including the congestion level at the cloud network (which is also
represented by a Lagrange multiplier), or the rate (power) cost associated with the back-haul

link. While a detailed discussion on those options is of interest, it goes beyond the scope of the

present work.

85

4.16.4 Modiﬁed online solver based on Q-learning

We close this section by providing an online reinforcement-learning algorithm that modiﬁes the

one introduced in Section 4.15 to account for the multipliers introduced in Section 4.16.

By deﬁning per ﬁle cost ˆcf

k as

(cid:16)

ˆcf
k

k , af
wf

k; ρf

(cid:17)

k, λf
k, ˆµk, ˆνk
:=
k + ˆµkσf (cid:17)
(cid:16)
ρf

af
k +

(cid:16)

k + ˆνkσf (cid:17)
λf

wf
k

(4.64)

the problem of caching under limited cache capacity and back-haul link reduces to per ﬁle

optimization as follows

(P8)

min
k ,af

k )}k≥t

{(wf

s.t.

∞
(cid:88)

γk−tE

(cid:16)

(cid:104)
ˆcf
k

k, wf
af

k ; ρf

k, λf

k, ˆµk, ˆνk

(cid:17)(cid:105)

k=t
(wf

k , af

k) ∈ X (rf

k , af

k−1),

∀f, k ≥ t

where the updated dual variables ˆµk and ˆνk are obtained respectively by iteration (4.57) and
(4.63). If we plug ˆcf
k into the marginalized Q-function in (4.48), then the solution
for (P8) in current iteration k for a given ﬁle f can readily be found by solving

k instead of cf

arg min
(w,a)∈X (rt,at−1)

¯Qw,a

rt,st + w(λt + ˆνtσf ) + a(ρt + ˆµtσf ).

(4.65)

Thus, it sufﬁces to form a marginalized Q-function for each ﬁle and solve (4.65), which can be

easily accomplished through exhaustive search over 8 possible cache-fetch decisions (w, a) ∈

X (rt, at−1).

To simplify notation and exposition, we focus on the limited caching capacity constraint, and

suppose that the back-haul is capable of serving any requests, thus ˆνt = 0, ∀t. Modiﬁcations to
account also for ˆνt (cid:54)= 0 are straightforward.

The modiﬁed Q-learning (MQ-learning) algorithm, tabulated in Algorithm 8, essentially

learns to make optimal fetch-cache decisions while accounting for the limited caching capacity

constraint in C4 and/or C5. In particular, to provide a computationally efﬁcient solver the

stochastic updates corresponding to C5 are used. Subsequently, if C4 needs to be enforced,

86

the obtained solution is projected into the feasible set through projection algorithm ΠC4(·).
The projection ΠC4(.) takes the obtained solution { ˘wf ∗
t }∀f , the ﬁle sizes, as well as the
t
marginalized Q-functions as input, and generates a feasible solution {wf ∗
t }∀f satisfying C4
t
as follows: it sorts the ﬁles with ˘af ∗
t = 1 in ascending Q-function order, and caches the ﬁles
with the lowest Q-values until the cache capacity is reached. Overall, our modiﬁed algorithm

, ˘af ∗

, af ∗

performs a “double” learning: i) by using reinforcement schemes it learns the optimal policies

that map states to actions, and ii) by using a stochastic dual approach it learns the mechanism

that adapt the prices to the saturation and congestion conditions in the cache. Given the operating

conditions and the design approach considered in the paper, the proposed algorithm has moderate

complexity, and thanks to the reduced input dimensionality, it also converges in a moderate

number of iterations.

4.17 Numerical tests

In this section, we numerically assess the performance of the proposed approaches for learn-

ing optimal fetch-cache decisions. Two sets of numerical tests are provided. In the ﬁrst set,

summarized in Figs 4.14-4.18, the performance of the value iteration-based scheme in Alg.

1 is evaluated, and in the second set, summarized in Figs. 4.19-4.20, the performance of the

Q-learning solver is investigated. In both sets, the cache and fetch cost parameters are drawn
with equal probability from a ﬁnite number of values, where the mean is ¯ρf and ¯λf , respectively.
Furthermore, the request variable rf is modeled as a Bernoulli random variable with mean pf ,

whose value indicates the popularity of ﬁle f .

In the ﬁrst set, it is assumed that pf as well as the distribution of ρf , λf , are known a priori.

Simulations are carried out for a content of unit size, and can be readily extended to ﬁles of

different sizes. To help readability, we drop the superscript f in this section.

Fig. 4.14 plots the sum average cost ¯C versus ¯ρ for different values of ¯λ and p. The fetching
cost is set to ¯λ ∈ {43, 45, 50, 58} for two different values of popularity p ∈ {0.3, 0.5}. As
depicted, higher values of ¯ρ, ¯λ, p generally lead to a higher average cost. In particular, when
¯ρ (cid:28) ¯λ, caching is considerably cheaper than fetching, thus setting at = 1 is optimal for most t.
As a consequence, the total cost linearly increases with ¯ρ as most requests are met via cached

contents rather than fetching. Interestingly, if ¯ρ keeps increasing, the aggregate cost gradually

87

Figure 4.14: Average cost versus ¯ρ for different values of p, ¯λ.

saturates and does not grow anymore. The reason behind this observation is the fact that, for

very high values of ¯ρ, fetching becomes the optimal decision for meeting most ﬁle requests

and, hence, the aggregate cost no longer depends on ¯ρ. While this behavior occurs for the two

values of p, we observe that for the smallest one, the saturation is more abrupt and takes place

at a lower ¯ρ. The intuition in this case is that for lower popularity values, the ﬁle is requested

less frequently, thus the caching cost aggregated over a (long) period of time often exceeds the

“reward” obtained when (infrequent) requests are served by the local cache. As a consequence,

fetching in the infrequent case of rt = 1 incurs less cost than the caching cost aggregated over
time.

To corroborate these ﬁndings, Fig. 4.15 depicts the sum average cost versus p for different
values of ¯ρ and ¯λ. The results show that for large values of ¯ρ, fetching is the optimal action,
resulting in a linear increase in the total cost as p increases. In contrast, for small values of ¯ρ,

caching is chosen more frequently, resulting in a sub-linear cost growth.

To investigate the caching-versus-fetching trade-off for a broader range of ¯ρ and ¯λ, let us
deﬁne the caching ratio as the aggregated number of positive caching decisions (those for which

at = 1) divided by the total number of decisions. Fig. 4.16 plots this ratio for different values
of (¯ρ, ¯λ) and ﬁxed p = 0.5. As the plot demonstrates, when ¯ρ is small and ¯λ is large, ﬁles are
cached almost all the time, with the caching ratio decreasing (non-symmetrically) as ¯ρ increases
and ¯λ decreases. Similarly, the caching ratio is plotted by setting p = 0.05 in Fig. 4.17, in
which fetching is mostly preferred over a wide range of storage costs due to the small value of p.

0510152025302030405060708088

Figure 4.15: Average cost versus p for different values of ¯λ, ¯ρ.

Figure 4.16: Caching ratio vs. ¯ρ and ¯λ for p = 0.5 and s = r = 1.

Figure 4.17: Caching ratio vs. ¯ρ and ¯λ for p = 0.05 and s = r = 1.

00.20.40.60.81020406080100120Sum average cost0100800.56020401203000010600.5401202030089

Figure 4.18: Performance of DP versus myopic caching for ¯λ = 53.

Figure 4.19: Average cost versus ¯ρ for different values of ¯λ, p. Solid line is for value iteration
while dashed lines are for Q-learning based solver.

00.20.40.60.810100200300400500600MyopicDP05101520253010203040506090

Figure 4.20: Averaged immediate cost over 1000 realizations in a non-stationary setting, and a
sample from popularities.

Interestingly this is true despite high fetching costs as well, and can be intuitively explained as

follows: due to low popularity, deciding to cache may result in idle storing of the ﬁle in cache,

thus entailing an unnecessary aggregated caching cost before the stored ﬁle can be utilized to

meet user request, rendering caching suboptimal. The comparison between Fig. 4.16 and 4.17

clearly demonstrates the effect of different values of p on the performance of the cache-fetch

decisions, while the proposed approach automatically adjusts to the underlying popularities.

Finally, Fig. 4.18 compares the performance of the proposed DP-based strategy with that

of a myopic one. The myopic policy sets at = 1 if λt > ρt and the content is locally available
(either because wt = 1 or because st = 1), and sets at = 0 otherwise. The results indicate that the
proposed strategy outperforms the myopic one for all values of ¯ρ, ¯λ, p and γ.

In the second set of tests, the performance of the online Q-learning solvers is investigated. As

explained in Section 4.15, under the assumption that the underlying distributions are stationary,

the performance of the Q-learning solver should converge to the optimal one found through the
value iteration algorithm. Corroborating this statement, Fig. 4.19 plots the sum average cost ¯C
versus ¯ρ of both the marginalized value iteration and the Q-learning solver, with ¯λ ∈ {29, 36, 44}
and p ∈ {0.3, 0.5}. The solid lines are obtained when assuming a priori knowledge of the

distributions and then running the marginalized value iteration algorithm; the results and analysis

are similar to the ones reported for Fig. 4.14. The dashed curves however, are found by assuming

unknown distributions and running the Q-learning solver. Sum average cost is reported after ﬁrst

1000 iterations. As the plot suggests, despite the lack of a priori knowledge on the distributions,

0100200300400500600Time0.511.5210400.10.20.30.4PopularityMyopicProposed approachConverged resultOGA [12]Popularitythird blockfirst blocksecond block91

the Q-learning solver is able to ﬁnd the optimal decision making rule. As a result, it yields the

same sum average cost as that of value-iteration under known distributions.

The last experiment investigates the impact of the instantaneous cache capacity constraint in

C4 as well as non-stationary distributions for popularities and costs. To this end, 1,000 different

realizations (trajectories) of the random state processes are drawn, each of length T = 600. For

every realization, the cost ct [cf. (4.33)] at each and every time instant is found, and the cost
trajectory is averaged across the 1,000 realizations. Speciﬁcally, let ci
t denote the ith realization
(cid:80)1000
cost at time t, and deﬁne the averaged cost trajectory as ¯ct := 1
t. Fig. 4.20 reports
1000
the average trajectory of ¯ct in a setup where the total number of ﬁles is set to F = 500, the ﬁle
sizes are drawn uniformly at random from the interval [1, 100], and the total cache capacity is

i=1 ci

set to 40% of the aggregate ﬁle size. Adopted parameters for the MQ-learning solver are set to

βt = 0.3, and (cid:15) = 0.01. Three blocks of iterations are shown in the ﬁgure, where in each block
a speciﬁc distribution of popularities and costs are considered. For instance, the dashed line

shows the popularity of a speciﬁc ﬁle in one of the realizations, where in the ﬁst block p = 0.23,

in the second block p = 0.37, and in the third one p = 0.01. The cost parameters have means
¯λ = 44, ¯ρ = 2, ¯λ = 40, ¯ρ = 5, and ¯λ = 38, ¯ρ = 2 in the consecutive blocks, respectively.

As Fig. 4.20, the proposed MQ-learning algorithm incurs large costs during the ﬁrst few

iterations. Then, it gradually adapts to the ﬁle popularities and cost distributions, and learns

how to make optimal fetch-cache decisions, decreasing progressively the cost in each of the

blocks. To better understand the behavior of the algorithm and assess its effectiveness, we

compare it with that of Online Gradient Ascent (OGA) [146] as a representative state-of-the-art

method among the class of online expert algorithms, the myopic policy and the stationary policy

serving as the benchmark, respectively. In contrast to the OGA method, our decision variables

are not continuous, but binary. Hence, caching decisions in OGA are projected into the binary

feasible set for fair comparison. In general, since OGA and the myopic caching only use blue the

current state and requests, their performance is inferior to that of our proposed method, where

knowledge of the underlying request and price distributions is carefully utilized. During the

ﬁrst iterations however, when the MQ-learning algorithm has not adapted to the distribution

of pertinent parameters, OGA and the myopic policy perform better; on the other hand, as the

learning proceeds, the MQ-learning starts to make more precise decisions and, remarkably, in a

couple of hundreds of iterations it is able to perform very close to the optimal policy.

Furthermore, to investigate the scalability of our proposed approach, Tables 4.1, 4.2, and 4.3

%M/F

1K 2K 3K 4K 6K 8K 10K
1089
444
1052
435
980
422
1086
497

870
858
815
949

662
625
610
699

240
229
232
251

337
327
326
372

10 % 148
20 % 141
40 % 139
60 % 149

92

Table 4.1: Run-time of the proposed caching.

%M/F 1K 2K 3K 4K 6K 8K 10K
808
796
779
908

10 %
20 %
40 %
60 %

120
123
122
170

411
389
406
551

241
250
272
356

176
183
182
254

622
569
585
736

70
73
70
92

Table 4.2: Run-time of OGA caching.

%M/F 1K 2K 3K 4K 6K 8K 10K
969
1025
1105
1141

10 %
20 %
40 %
60 %

205
232
236
240

721
736
822
834

126
153
157
161

286
317
328
336

491
507
575
563

70
84
88
87

Table 4.3: Run-time of Myopic caching.

report the run-time (in seconds3) versus the number of ﬁles F as well as the storage capacity

M , set as a ratio of the total aggregated ﬁle sizes. Although the proposed approach has slightly

higher run-time due to the utilized dual-decomposition technique and the solution of the arising

integer DP, all methods scale gracefully (linearly) as the number of ﬁles increases from 1K to

10K.

4.18 Conclusions

A generic setup where a caching unit makes sequential fetch-cache decisions based on dynamic

prices and user requests was investigated. Critical constraints were identiﬁed, the aggregated cost

across ﬁles and time instants was formed, and the optimal adaptive caching was then formulated

3We run these simulations in parallel with 4 pools of workers, utilizing a machine with Intel(R) Core(TM) i7-4770

CPU @ 3.4 GHz speciﬁcations.

93

as a stochastic optimization problem. Due to the effects of the current cache decisions on

future costs, the problem was cast as a dynamic program. To address the inherent functional

estimation problem that arises in this type of programs, while leveraging the underlying problem

structure, several computationally efﬁcient algorithms were developed, including off-line (batch)

approaches, as well as online (stochastic) approaches based on Q-learning. The last part of the

paper was devoted to dynamic pricing mechanisms that allowed handling constraints both in the

storage capacity of the cache memory, as well as on the back-haul transmission link connecting

the caching unit with the cloud.

94

, ˘af ∗
t )

Algorithm 8 Modiﬁed Q-learning for online caching
Initialize 0 < γ, βt < 1, ˆµ0, ζ, (cid:15)t, M
Output ˆ¯Qwf ,af

rf ,sf (t + 1)

Set ˆ¯Qwf ,af
Set sf
For

rf ,sf (1) = 0 for all factors
0 , λf
0 = {rf
0 = 0 and variables θf
t = 0, 1 . . . For the current state (rf

0 , ρf

0 } are revealed
t , sf
(cid:26) Solve (4.51)

t ), choose ( ˘wf ∗

t

( ˘wf ∗
t

, ˘af ∗

t ) =

random (w, a) ∈ X f

t (rf

t , sf

t ) w.p. (cid:15)t

w.p. 1 − (cid:15)t

Update dual variable





ˆµt+1 =

ˆµt + ζ



F
(cid:88)

f =1





+

˘af ∗
t σf − M





Incur cost
Apply ΠC4(·) to guarantee C4 (if required)

t := cf
ˇcf

t (˘af ∗

, ˘wf ∗
t

t , λf

; ρf

t

t ) + ˆµt˘af ∗

t σf

ΠC4

(cid:20)(cid:110)

( ˘wf ∗
t

, ˘af ∗
t )

(cid:21)

(cid:111)

f

(cid:110)

wf ∗
t

, af ∗
t

→

(cid:111)

f

t+1 = af ∗

Update state sf
Request and cost parameters, θf
Update all ˆ¯Q factors as

t

t+1, are revealed

,af ∗
t

ˆ¯Qwf ∗
t
t ,sf
rf

t

(t + 1) = (1 − βt) ˆ¯Qwf ∗
t
t ,sf
rf

t

,af ∗
t

(t)

(cid:34)

+ βt

ˇcf
t + γ

min
(wf ,af )∈X f

t+1

ˆ¯Qwf ,af
rf
t+1,sf

t+1

(t)

(cid:35)

Chapter 5

Data-driven, Reinforced, and Robust

Learning Approaches for

a Smarter Power Grid

5.1 Introduction

Frequent and sizable voltage ﬂuctuations caused by the growing deployment of electric vehicles,

demand response programs, and renewable energy sources, challenge modern distribution grids.

Electric utilities are currently experiencing major issues related to the unprecedented levels of

load peaks as well as renewable penetration. For instance, a solar farm connected at the end of

a long distribution feeder in a rural area can cause voltage excursions along the feeder, while

the apparent power capability of a substation transformer is strained by frequent reverse power

ﬂows. Moreover, over-voltage happens during midday when photovoltaic (PV) generation peaks

and load demand is relatively low; whereas voltage sags occur mostly overnight due to low PV

generation even when load demand is high [35]. This motivates why voltage regulation, the task

of maintaining bus voltage magnitudes within desirable ranges, is critical in modern distribution

grids.

Early approaches to regulating the voltages at a residential level have mainly relied on utility-

owned devices, including load-tap-changing transformers, voltage regulators, and capacitor

banks, to name a few. They offer a convenient means of controlling reactive power, through which

95

96

the voltage proﬁle at their terminal buses as well as at other buses can be regulated [96, p. 678].

Obtaining the optimal conﬁguration for these devices entails solving mixed-integer programs,

which are NP-hard in general. To optimize the tap positions, a semi-deﬁnite relaxation heuristic

was used in [151, 14]. Control rules based on heuristics were developed in [189, 35]. However,

these approaches can be computationally demanding, and do not guarantee optimal performance.

A batch reinforcement learning (RL) scheme based on linear function approximation was lately
advocated in [209]. 1

Another characteristic inherent to utility-owned equipment is their limited life cycle, which

prompts control on a daily or even monthly basis. Such conﬁgurations have been effective

in traditional distribution grids without (or with low) renewable generation, and with slowly

varying load. Yet, as distributed generation grows in residential networks nowadays [182], [80],

rapid voltage ﬂuctuations occur frequently. According to a recent landmark bill, California

mandated 50% of its electricity to be powered by renewable resources by 2025 and 60% by

2030. The power generated by a solar panel can vary by 15% of its nameplate rating within

one-minute intervals [195]. Voltage control would entail more frequent switching actions, and

further installation of control devices.

Smart power inverters on the other hand, come with contemporary distributed generation

units, such as PV panels, and wind turbines. Embedded with computing and communication units,

these can be commanded to adjust reactive power output within seconds, and in a continuously-

valued fashion. Indeed, engaging smart inverters in reactive power control has recently emerged

as a promising solution [89]. Computing the optimal setpoints for inverters’ reactive power

output is an instance of the optimal power ﬂow task, which is non-convex [57]. To deal with

the renewable uncertainty as well as other communication issues (e.g., delay and packet loss),

stochastic, online, decentralized, and localized reactive control schemes have been advocated [89,

237, 90, 195, 193, 111, 231].

RL refers to a collection of tools for solving Markovian decision processes (MDPs), es-

pecially when the underlying transition mechanism is unknown [185]. In settings involving

high-dimensional, continuous action and/or state spaces however, it is well known that con-

ventional RL approaches suffer from the so-called ‘curse of dimensionality,’ which limits their

impact in practice [135]. Deep neural networks (DNNs) can address the curse of dimensionality

in the high-dimensional and continuous state space by providing compact low-dimensional

1Results of this Chapter are reported in [213, 214, 215, 216, 218, 219].

97

representations of high-dimensional inputs [67]. Wedding deep learning with RL (using a

DNN to approximate the action-value function), deep (D) RL has offered artiﬁcial agents with

human-level performance across diverse application domains [135, 159]. (D)RL algorithms

have also shown great potential in several challenging power systems control and monitoring

tasks [49, 55, 209, 225, 211, 118], and load control [43, 52]. A batch RL scheme using linear

function approximation was developed for voltage regulation in distribution systems [209]. For

voltage control of transmission networks, DRL was recently investigated to adjust generator

voltage setpoints [49]. A shortcoming of the mentioned (D)RL voltage control schemes is their

inability to cope with the curse of dimensionality in action space. Moreover, joint control of

both utility-owned devices and emerging power inverters has not been fully investigated. In

addition, the discrete variables describing the on-off operation of capacitors and slow timescale

associated with changing capacitor statuses, compared with those of fast-responding inverters

further challenges voltage regulation. As a consequence, current capacitor decisions have a

long-standing inﬂuence on future inverter setpoints. The other way around, current inverter

setpoints also affect future commitment of capacitors through the aggregate cost. Indeed, this

two-way long-term interaction is difﬁcult to model and cope with.

In this context, voltage control is dealt with in the present Chapter using shunt capacitors

and smart inverters. Preliminary results were presented in [220]. A novel two-timescale solution

combining ﬁrst principles based on physical models and data-driven advances is put forth. On

the slow timescale (e.g., hourly or daily basis), the optimal conﬁguration (corresponding to

the discrete on-off commitment) of capacitors is formulated as a Markov decision process,

by carefully deﬁning state, action, and cost according to the available control variables in the

grid. The solution of this MDP is approached by means of a DRL algorithm. This framework

leverages the merits of the so-termed target network and experience replay, which can remove the

correlation among the sequence of observations, to make the DRL stable and tractable. On the

other hand, the setpoints of the inverters’ reactive power output, are computed by minimizing the

instantaneous voltage deviation using the exact or approximate grid models on the fast timescale

(e.g., every few seconds).

Compared with past works, our contributions can be summarized as follows.

c1) Joint control of two types of assets. A hybrid data- and physics-driven approach to

managing both utility-owned equipment as well as smart inverters;

98

c2) Slow-timescale learning. Modeling demand and generation as Markovian processes,

optimal capacitor settings are learned from data using DRL;

c3) Fast-timescale optimization. Using exact or approximate grid models, the optimal setpoints

for inverters are found relying on the most recent slow-timescale solution; and,

c4) Curse of dimensionality in action space. Introducing hyper deep Q-network to handle the

curse of dimensionality emerging due to large number of capacitors.

5.2 Voltage Control in Two Timescales

In this section, we describe the system model, and formulate the two-timescale voltage regulation

problem.

5.2.1 System model

Consider a distribution grid of N + 1 buses rooted at the substation bus indexed by i = 0, whose

buses are collected into N0 := {0} ∪ N , and lines into L := {1, . . . , N }. For all i ∈ N (i.e.,
without substation bus), let vi denote their squared voltage magnitude, and pi + jqi their complex
power injected. For brevity, collect all nodal quantities into column vectors vvv, ppp, qqq. Active power
injection is split into its generation pg
i and consumption pc
i ; likewise, reactive
i = 0 and qg
i . In distribution grids, it holds that pg
power injection is qi := qg
i > 0
if bus i has a capacitor; while pg
i ≥ 0, qc
i ≥ 0,
pg
i ≥ 0 if bus i is equipped with a DG. Let us stack generation and consumption components
into vectors pppg, qqqg, pppc, and qqqc accordingly. Predictions of active power consumption and solar
generation (pppc, qqqc, pppg) can be obtained through the hourly and real-time market (see e.g., [89]),

i = 0 if bus i is a purely load bus; and pc

i as pi := pg

i = qg

i = pc

i − pc

i = qc

i − qc

or by running load demand (solar generation) prediction algorithms [229].

As mentioned earlier, there are two types of assets in modern distribution grids that can be

engaged in reactive power control; that is, utility-owned equipment featuring discrete actions

and limited lifespan, as well as smart inverters controllable within seconds and in a continuously-

valued fashion. As the aggregate load varies in a relatively slow way, traditional devices have

been sufﬁcient for providing voltage support; while fast-responding solutions using inverters

become indispensable with the increase of uncertain renewable penetration. In this context, the

present work focuses on voltage regulation by capitalizing on the reactive control capabilities of

99

Figure 5.1: Two-timescale partitioning of a day for joint capacitor and inverter control.

both capacitors and inverters, while our framework can also account for other reactive power
control devices. To this end, we divide every day into N ¯T intervals indexed by τ = 1, . . . , N ¯T .
Each of these N ¯T intervals is further partitioned into NT time slots which are indexed by
t = 1, . . . , NT , as illustrated in Fig. 5.1. To match the slow load variations, the on-off decisions
of capacitors are made (at the end of) every interval τ , which can be chosen to be e.g., an hour;

yet, to accommodate the rapidly changing renewable generation, the inverter output is adjusted
(at the beginning of) every slot t, taken to be e.g., a minute. We assume that quantities pppg(τ, t),
pppc(τ, t), and qqqc(τ, t) remain the same within each t-slot, but may change from slot t to t + 1.

Suppose there are Na shunt capacitors installed in the grid, whose bus indices are collected
in Na, and are in one-to-one correspondence with entries of K := {1, . . . , Na} (a simple
renumbering). Assume that every bus is equipped with either a shunt capacitor or a smart inverter,

but not both. The remaining buses, after removing entries in Na from N , collected in Nr, are
assumed equipped with inverters. This assumption is made without loss of generality as one can

simply set the upper and lower bounds on the reactive output to zero at buses having no inverters

installed.

As capacitor conﬁguration is performed on a slow timescale (every τ ), the reactive compen-

sation qg

i (τ, t) provided by capacitor ki ∈ K (i.e., capacitor at bus i) is represented by

i (τ, t) = ˆyki(τ )qg
qg

a,ki

,

∀i ∈ Na, τ, t

(5.1)

where ˆyki(τ ) ∈ {0, 1} is the on-off commitment of capacitor ki for the entire interval τ . Clearly,
if ˆyki(τ ) = 1, a constant amount (nameplate value) of reactive power qg
is injected in the grid
during this interval, and 0 otherwise. For convenience, the on-off decisions of capacitor units at

a,ki

interval τ are collected in a column vector ˆyyy(τ ).

Capacitor configurationInverter optimization100

r,i(τ, t)| ≤ (cid:112)(¯si)2 − (pg

On the other hand, the reactive power qg
r,i(τ, t) generated by inverter i is adjusted on the
fast timescale (every t), and it is constrained by |qg
i (τ, t))2, where ¯si
is the power capability of inverter i. Traditionally, inverter i is designed as ¯si = ¯pg
i , where
¯pg
i is the active power capacity of the renewable generation unit installed at bus i. However,
when maximum output is reached, i.e., pg
i (τ, t) = ¯pg
i , no reactive power can be provided. To
address this, oversized inverters’ nameplate capacity has been advocated such that ¯si > ¯pg
i
[89]. For instance, choosing ¯si = 1.08¯pg
i )2 instead of
(cid:112)(¯si)2 − (pg
r,i(τ, t)| ≤
0.4¯pg
r,i(τ, t) generated by
inverter i is constrained as

i and limiting qg
i (τ, t))2, the reactive power compensation provided by inverter i is |qg

i , regardless of the instantaneous PV output pg

r,i(τ, t) to (cid:112)(¯si)2 − (¯pg

i (τ, t) [89]. As such, qg

r,i(τ, t)| ≤ ¯qg
|qg

i :=

(cid:113)

(¯si)2 − (¯pg

i )2,

∀i ∈ Nr, t.

(5.2)

5.2.2 Two-timescale voltage regulation formulation

Given two-timescale load consumption and generation that we model as Markovian processes

[34], the task of voltage regulation is to ﬁnd the optimal reactive power support per slot by

conﬁguring capacitors in every interval and adjusting inverter outputs in every slot, such that the

long-term average voltage deviation is minimized. As voltage magnitudes vvv(τ, t) depend solely
on the control variables qqqg(τ, t), they are expressed as implicit functions of qqqg(τ, t), yielding
vvvτ,t(qqqg(τ, t)), whose actual function forms for postulated grid models will be given Section
5.3. The novel two-timescale voltage control scheme entails solving the following stochastic

optimization problem

minimize
g
r (τ,t)}
{yyy(τ )∈{0,1}Na }

{qqq

E

(cid:34) ∞
(cid:88)

NT(cid:88)

τ =1

t=1

γτ (cid:107)vvvτ,t(qqqg(τ, t)) − v0111(cid:107)2

(cid:35)

subject to

a,ki

i (τ, t) = ˆyki(τ )qg
qg
qg
i (τ, t) = qg
r,i(τ, t),
r,i(τ, t)| ≤ ¯qg
|qg
i ,

,

∀i ∈ Na, τ, t

∀i ∈ Nr, τ, t

∀i ∈ Nr, τ, t

(5.3a)

(5.3b)

(5.3c)

(5.3d)

for some discount factor γ ∈ (0, 1), where the expectation is taken over the joint distribution
of (pppc(τ, t), qqqc(τ, t), pppg(τ, t)) across all intervals and slots. Clearly, the optimization problem

101

(5.3) involves inﬁnitely many variables {qqqg
r(τ, t)} and {ˆyyy(τ )}, which are coupled across time
via the cost function and the constraint (5.3b). Moreover, discrete variables ˆyyy(τ ) ∈ {0, 1}Na

render problem (5.3) nonconvex and generally NP-hard. Last but not least, it is a multi-stage

optimization, whose decisions are not all made at the same stage, and must also account for the

power variability during real-time operation. In words, tackling (5.3) exactly is challenging.

Instead, our goal

is to design algorithms that sequentially observe predictions
{(pppc(τ, t), qqqc(τ, t)), qqqg(τ, t)}, and solve near optimally problem (5.3). The assumption is that,

although no distributional knowledge of those stochastic processes involved is given, their re-

alizations can be made available in real time, by means of e.g., accurate forecasting methods

[229]. In this sense, the physics governing the electric power system will be utilized together

with data to solve (5.3) in real time. Speciﬁcally, on the slow timescale, say at the end of each

interval τ − 1, the optimal on-off capacitor decisions yyy(τ ) will be set through a DRL algorithm

that can learn from the predictions collected within the current interval τ − 1; while, on the

fast timescale, namely at the beginning of each slot t within interval τ , our two-stage control

scheme will compute the optimal setpoints for inverters, by minimizing the instantaneous bus

voltage deviations while respecting physical constraints, given the current on-off commitment of

capacitor units ˆyyy(τ ) found at the very end of interval (τ − 1). These two timescales are detailed

in Sections 5.3 and 5.4, respectively.

5.3 Fast-timescale Optimization of Inverters

As alluded earlier, the actual forms of vvvτ,t(qqqg(τ, t)) will be speciﬁed in this section, relying on the
exact AC model or a linearized approximant of it. Leveraging convex relaxation to deal with the

nonconvexity, the considered AC model yields a second-order cone program (SOCP), whereas

the linearized one leads to a linearly constrained quadratic program. In contrast, the latter offers

an approximate yet computationally more affordable alternative to the former. Selecting between

these two models relies on affordable computational capabilities.

5.3.1 Branch ﬂow model

Due to the radial structure of distribution grids, every non-root bus i ∈ N has a unique parent

bus termed πi. The two are joined through the i-th distribution line represented by (πi, i) ∈ L
having impedance ri + jxi. Let Pi(τ, t) + jQi(τ, t) stand for the complex power ﬂowing from

102

Figure 5.2: Bus i is connected to its unique parent πi via line i.

buses πi to i seen at the ‘front’ end at time slot t of interval τ , as depicted in Fig. 5.2. Throughout
this section, the interval index τ will be dropped when it is clear from the context.

With further (cid:96)i denoting the squared current magnitude on line i ∈ L, the celebrated branch
ﬂow model is described by the following equations for all buses i ∈ N , and for all t within every

interval τ [8, 115]

pi(t) =

qi(t) =

(cid:88)

j∈χi
(cid:88)

j∈χi

Pj(t) − (Pi(t) − ri(cid:96)i(t))

Qj(t) − (Qi(t) − xi(cid:96)i(t))

vi(t) = vπi(t)− 2(riPi(t)+ xiQi(t))+ (r2
i (t)

(cid:96)i(t) =

i (t) + Q2
P 2
vπi(t)

i + x2

i )(cid:96)i(t)

(5.4a)

(5.4b)

(5.4c)

(5.4d)

where we have ignored the dependence on τ for brevity, and χi denotes the set of all children
buses for bus i.

Clearly, the set of equations in (5.4d) is quadratic in Pi(t) and Qi(t), yielding a nonconvex
set. To address this challenge, consider relaxing the equalities (5.4d) into inequalities (a.k.a.

hyperbolic relaxation, see e.g., [57])

i (t) + Q2
P 2

i (t) ≤ vπi(t)(cid:96)i(t),

∀i ∈ N , t

(5.5)

which can be equivalently rewritten as the following second-order cone constraints

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2Pi(t)
2Qi(t)
(cid:96)i(t) − vπi(t)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ vπi(t) + (cid:96)i(t),

∀i ∈ N .

(5.6)

Equations (5.4a)-(5.4c) and (5.6) now deﬁne a convex feasible set. The procedure of leveraging

this relaxed set (instead of the nonconvex one) is known as SOCP relaxation [115]. Interestingly,

it has been shown that under certain conditions, SOCP relaxation is exact in the sense that the set

of inequalties (5.6) holds with equalities at the optimum [61].

Given the capacitor conﬁguration ˆyyy(τ ) found at the end of the last interval τ − 1, under the

aforementioned relaxed grid model, the voltage regulation on the fast timescale based on the

exact AC model can be described as follows

103

minimize

vvv(t),qqqg

r (t),PPP (t),QQQ(t)

subject to

(cid:107)vvv(t) − v0111(cid:107)2

(5.4a) − (5.4d)
i (t) = ˆyki(τ )qg
qg
qg
i (t) = qg
r,i(t),
r,i(t)| ≤ ¯qg
|qg
i ,

a,ki

, ∀i ∈ Na

∀i ∈ Nr

∀i ∈ Nr

(5.7a)

(5.7b)

(5.7c)

(5.7d)

which is readily a convex SOCP and can be efﬁciently solved by off-the-shelf convex program-

ming toolboxes. The optimal setpoints of smart inverters for the exact AC model are found as
the qg

r -minimizer of (5.7).

However, solving SOCPs could be computationally demanding when dealing with relatively

large-scale distribution grids, say of several hundred buses. Trading off modeling accuracy for

computational efﬁciency, our next instantiation of the fast-timescale voltage control relies on an

approximate grid model.

5.3.2 Linearized power ﬂow model

As line current magnitudes {(cid:96)i} are relatively small compared to line ﬂows, the last term in
(5.4a)-(5.4c) can be ignored yielding the next set of linear equations for all i, t [9]

pi(t) =

qi(t) =

(cid:88)

j∈χi
(cid:88)

j∈χi

Pj(t) − Pi(t)

Qj(t) − Qi(t)

vi(t) = vπi(t) − 2(riPi(t) + xiQi(t))

(5.8a)

(5.8b)

(5.8c)

which is known as the linearized distribution ﬂow model. In this fashion, all squared voltage
magnitudes vvv(t) can be expressed as linear functions of qqqg(t).

Adopting the approximate model (5.8), the optimal setpoints of inverters can be found by

solving the following optimization problem per slot t in interval τ , provided ˆyyy(τ ) is available

from the last interval on the slow timescale

104

minimize

vvv(t),qqqg

r (t),PPP (t),QQQ(t)

subject to

(cid:107)vvv(t) − v0111(cid:107)2

(5.8a) − (5.8c)
i (t) = ˆyki(τ )qg
qg
qg
i (t) = qg
r,i(t),
|qg
r,i(t)| ≤ ¯qg
i ,

a,ki

, ∀i ∈ Na

∀i ∈ Nr

∀i ∈ Nr.

(5.9a)

(5.9b)

(5.9c)

(5.9d)

As all constraints are linear and the cost is quadratic, (5.9) constitutes a standard convex

quadratic program. As such, it can be solved efﬁciently by e.g., primal-dual algorithms, or

off-the-shelf convex programming solvers, whose implementation details are skipped due to

space limitations.

5.4 Slow-timescale Capacitor Reconﬁguration

Here we deal with reconﬁguration of shunt capacitors on the slow timescale. This amounts

to determining their on-off status for the ensuing interval. Past approaches to solving the

resultant integer-valued optimization were heuristic, or, relied on semideﬁnite programming

relaxation. They do not guarantee optimality, while they also incur high computational and

storage complexities. We take a different route by drawing from advances in artiﬁcial intelligence,

to develop data-driven solutions that could near optimally learn, track, as well as adapt to

unknown generation and consumption dynamics.

5.4.1 A data-driven solution

Clearly from (5.7b)–(5.9b), the capacitor decisions ˆyyy(τ ) made at the end of interval τ − 1 (slow-

timescale learning) inﬂuence inverters’ setpoints during the entire interval τ (fast-timescale

optimization). The other way around, inverters’ regulation on voltages inﬂuences the capacitor

105

Figure 5.3: Deep Q-network

commitment for the next interval. This two-way between the capacitor conﬁguration and the

optimal setpoints of inverters motivates our RL formulation. Dealing with learning policy

functions in an environment with action-dependent dynamically evolving states and costs, RL

seeks a policy function (of states) to draw actions from, in order to minimize the average

cumulative cost [185].

Modeling load demand and renewable generation as Markovian processes, the optimal

conﬁguration of capacitors can be formulated as an MDP, which can be efﬁciently solved through

RL algorithms. An MDP is deﬁned as a 5-tuple (S, A, P, c, γ), where S is a set of states; A is a

set of actions; P is a set of transition matrices; c : S × A (cid:55)→ R is a cost function such that, for
sss ∈ S and aaa ∈ A, c = (c(sss, aaa))sss∈S,aaa∈A are the real-valued instantaneous costs after the system
operator takes an action aaa at state sss; and γ ∈ [0, 1) is the discount factor. These components are

deﬁned next before introducing our voltage regulation scheme.

Action space A. Each action corresponds to one possible on-off commitment of capacitors 1

to Na, giving rise to an action vector aaa(τ ) = yyy(τ ) per interval τ . The set of binary action vectors
constitutes the action space A, whose cardinality is exponential in the number of capacitors,
meaning |A| = 2Na.

State space S. This includes per interval τ the average active power at all buses except for
the substation, along with the current capacitor conﬁgurations; that is, sss(τ ) := [ ¯ppp(cid:62)(τ ), ˆyyy(cid:62)(τ )](cid:62),
which contains both continuous and discrete variables. Clearly, it holds that S ⊆ RN × 2Na.

The action is decided according to the conﬁguration policy π that is a function of the most

recent state sss(τ − 1), given as

aaa(τ ) = π(sss(τ − 1)).

(5.10)

…………Input layerHidden layersOutput layer………Choose toconfigureCost function c. The cost on the slow timescale is

c(sss(τ − 1), aaa(τ )) =

NT(cid:88)

(cid:107)vvvτ,t(qqqg(τ, t)) − v0111(cid:107)2 .

t=1

106

(5.11)

Set of transition probability matrices P. While being at a state sss ∈ S upon taking an action
aaa, the system moves to a new state sss(cid:48) ∈ S probabilistically. Let P aaa
ssssss(cid:48) denote the transition
probability matrix from state sss to the next state sss(cid:48) under a given action aaa. Evidently, it holds that
P := (cid:8)P aaa

ssssss(cid:48)|∀aaa ∈ A(cid:9).

Discount factor γ. The discount factor γ ∈ [0, 1), trades off the current versus future costs.

The smaller γ is, the more weight the current cost has in the overall cost.

Given the current state and action, the so-termed action-value function under the control

policy π is deﬁned as

Qπ(sss(τ − 1), aaa(τ )) :=

(cid:34) ∞
(cid:88)

E

τ (cid:48)=τ

(cid:35)
(cid:12)
γτ (cid:48)−τ c(sss(τ (cid:48) − 1), aaa(τ (cid:48)))
(cid:12)
(cid:12)π, sss(τ − 1), aaa(τ )

(5.12)

where the expectation E is taken with respect to all sources of randomness.

To ﬁnd the optimal capacitor conﬁguration policy π∗, that minimizes the average voltage

deviation in the long run, we resort to the Bellman optimality equations; see e.g., [185]. Solving
those yields the action-value function under the optimal policy π∗ on the ﬂy, given by

Qπ∗(sss, aaa) = E[c(sss, aaa)] + γ

(cid:88)

sss(cid:48)∈S

P aaa
ssssss(cid:48) min
aaa∈A

Qπ∗(sss(cid:48), aaa(cid:48)).

With Qπ∗(sss, aaa) obtained, the optimal capacitor conﬁguration policy can be found as

π∗(sss) = arg min

aaa

Qπ∗(sss, aaa).

(5.13)

(5.14)

It is clear from (5.13) that if all transition probabilities {P aaa

ssssss(cid:48)} were available, we can derive
Qπ∗(sss, aaa), and subsequently the optimal policy π∗ from (5.14). Nonetheless, obtaining those
transition probabilities is impractical in practical distribution systems. This calls for approaches
that aim directly at π∗, without assuming any knowledge of {P aaa

ssssss(cid:48)}.

One celebrated approach of this kind is Q-learning, which can learn π∗ by approximating

Qπ∗(sss, aaa) ‘on-the-ﬂy’ [185, p. 107]. Due to its high-dimensional continuous state space
S however, Q-learning is not applicable for the problem at hand. This motivates function

approximation based Q-learning schemes that can deal with continuous state domains.

107

5.4.2 A deep reinforcement learning approach

DQN offers a NN function approximator of the Q-function, chosen to be e.g., a fully connected

feed-forward NN, or a convolutional NN, depending on the application [135]. It takes as input

the state vector, to generate at its output Q-values for all possible actions (one for each). As

demonstrated in [135], such a NN indeed enables learning the Q-values of all state-action pairs,

from just a few observations obtained by interacting with the environment. Hence, it effectively

addresses the challenge brought by the ‘curse of dimensionality’ [135]. Inspired by this, we

employ a feed-forward NN to approximate the Q-function in our setting. Speciﬁcally, our

DNN consists of L fully connected hidden layers with ReLU activation functions, depicted

in Fig. 5.3. At the input layer, each neuron is fed with one entry of the state vector sss(τ − 1),
which, after passing through L ReLU layers, outputs a vector ooo(τ ) ∈ R2Na , whose elements
predict the Q-values for all possible actions (i.e., capacitor conﬁgurations). Since each output
unit corresponds to a particular conﬁguration of all Na capacitors, there is a total of 2Na neurons
at the output layer. For ease of exposition, let us collect all weight parameters of this DQN into

a vector θθθ which parameterizes the input-output relationship as ooo(τ ) = Qπ(sss(τ − 1), aaa(τ ); θθθ)
(c.f. (5.12)). At the end of a given interval τ − 1, upon passing the state vector sss(τ − 1) through

this DQN, the corresponding predicted Q-values ooo(τ ) for all possible actions become available

at the output. Based on these predicted values, the system operator selects the action having the

smallest predicted Q-value to be in effect over the next interval.

Intuitively, the weights θθθ should be chosen such that the DQN outputs match well the

actual Q-values with input any state vector. Toward this objective, the popular stochastic

gradient descent (SGD) method is employed to update θθθ ‘on the ﬂy’ [135]. At the end of a

given interval τ , precisely when i) the system operator has made decision aaa(τ ), ii) the grid

has completed the transition from the state sss(τ − 1) to a new state sss(τ ), and, (iii) the network

has incurred and revealed cost c(sss(τ − 1), aaa(τ )), we perform a SGD update based on the

current estimate θθθτ to yield θθθτ +1. The so-termed temporal-difference learning [185] conﬁrms
that a sample approximation of the optimal cost-to-go from interval τ is given by c(sss(τ −

Algorithm 9 Two-timescale voltage regulation scheme.
Initialize θθθ0 randomly; weight of the target network θθθTar
state sss(0).
For
Take action aaa(τ ) through exploration-exploitation

τ = 1, 2, ...

0 = θθθ0; replay buffer R; and the initial

108

(cid:26)random aaa ∈ A

aaa(τ ) =

arg minaaa(cid:48) Q(sss(τ − 1), aaa(cid:48); θθθτ ) w.p. 1−(cid:15)τ

w.p. (cid:15)τ

where (cid:15)τ = max(cid:8)1 − 0.1 × (cid:98)τ /50(cid:99), 0(cid:9).

t = 1, 2, ..., NT

Evaluate ccc(sss(τ − 1), aaa(τ )) using (5.11).
For
Compute qqqg(τ, t) using (5.7) or (5.9).
Update sss(τ ).
Save (sss(τ −1), aaa(τ ), c(sss(τ − 1), aaa(τ )), sss(τ )) into R(τ ).
Randomly sample Mτ experiences from R(τ ).
Form the mini-batch loss LTar(θθθτ ; Mτ ) using (5.18).
Update θθθτ +1 using (5.19).
If mod(τ, B) = 0
Update the target network θθθTar

τ = θθθτ .

aaa(cid:48)

and min

1), aaa(τ )) + γ min
aaa(cid:48)∈A

Qπ(sss(τ ), aaa(cid:48); θθθτ ), where c(sss(τ − 1), aaa(τ )) is the instantaneous cost observed,
Qπ(sss(τ ), aaa(cid:48); θθθτ ) represents the smallest possible predicted cost-to-go from state sss(τ ),
which can be computed through our DQN with weights θθθτ , and is discounted by factor γ. In
Qπ(sss(τ ), aaa(cid:48); θθθτ ) is readily available at the end
words, the target value c(sss(τ − 1), aaa(τ )) + γ min
aaa(cid:48)∈A
of interval τ − 1. Adopting the (cid:96)2-norm error criterion, a meaningful approach to tuning the
weights θθθ entails minimizing the following loss function

L(θθθ) :=

(cid:104)
c(sss(τ − 1), aaa(τ )) + γ min
aaa(cid:48)∈A

Qπ(sss(τ ), aaa(cid:48); θθθτ ) − Qπ(sss(τ − 1), aaa(τ ); θθθ)

(cid:105)2

for which the SGD update is given by

θθθτ +1 = θθθτ − βτ ∇L(θθθ)|θθθτ

(5.15)

where βτ > 0 is a preselected learning rate, and ∇L(θθθ) denotes the (sub-)gradient.

However, due to the compositional structure of DNNs, the update (5.15) does not work

well in practice. In fact, the resultant DQN oftentimes does not provide a stable result; see

109

e.g., [207]. To bypass these hurdles, several modiﬁcations have been introduced. In this work,

we adopt the target network and experience replay [135]. To this aim, let us deﬁne an experience
e(τ (cid:48)) := (sss(τ (cid:48) − 1), aaa(τ (cid:48))), c(sss(τ (cid:48) − 1), aaa(τ (cid:48))), sss(τ (cid:48))), to be a tuple of state, action, cost, and

the next state. Consider also having a replay buffer R(τ ) on-the-ﬂy, which stores the most recent

R > 0 experiences visited by the agent. For instance, the replay buffer at any interval τ ≥ R is

R(τ ) := {e(τ − R + 1), . . . , e(τ )}. Furthermore, as another effective remedy to stabilizing the

DQN updates, we replicate the DQN to create a second DNN, commonly referred to as the target
network, whose weight parameters are concatenated in the vector θθθTar. It is worth highlighting
that this target network is not trained, but its parameters θθθTar are only periodically reset to
estimates of θθθ, say every B training iterations of the DQN. Consider now the temporal-difference
loss for some randomly drawn experience e(τ (cid:48)) from R(τ ) at interval τ

LTar(θθθτ ; e(τ (cid:48))) :=

(cid:104)
c(sss(τ (cid:48) − 1), aaa(τ (cid:48)))

1
2

+ γmin

aaa(cid:48)

QTar(sss(τ ), aaa(cid:48); θθθTar

τ (cid:48) ) − Q(sss(τ (cid:48) − 1), aaa(τ (cid:48)); θθθτ )

.

(5.16)

(cid:105)2

Upon taking expectation with respect to all sources of randomness generating this experience,

we arrive at

LTar(θθθτ ; R(τ ))) := Ee(τ (cid:48)) LTar(θθθτ ; e(τ (cid:48))).

(5.17)

In practice however, the underlying transition probabilities are unknown, which challenges
evaluating and hence minimizing LTar(θθθτ ; R(τ ))) exactly. A commonly adopted alternative is
to approximate the expected loss with an empirical loss over a few samples (that is, experiences

here). To this end, we draw a mini-batch of Mτ experiences uniformly at random from the
replay buffer R(τ ), whose indices are collected in the set Mτ , i.e., {e(τ (cid:48))}τ (cid:48)∈Mτ ∼ U (R(τ )).
Upon computing for each of those sampled experiences an output using the target network with
parameters θθθTar

, the empirical loss is

τ

LTar(θθθτ ; Mτ ) :=

1
2Mτ

(cid:88)

(cid:104)
c(sss(τ (cid:48) − 1), aaa(τ (cid:48)))

τ (cid:48)∈Mτ

+ γmin

aaa(cid:48)

QTar(sss(τ (cid:48)), aaa(cid:48); θθθTar

τ

) − Q(sss(τ (cid:48) − 1), aaa(τ (cid:48)); θθθτ )

(cid:105)2

.

(5.18)

In a nutshell, the weight parameter vector θθθτ of the DQN is efﬁciently updated ‘on-the-ﬂy’

110

Figure 5.4: Schematic diagram of the 47-bus industrial distribution feeder. Bus 1 is the substation,
and the 6 loads connected to it model other feeders on this substation.

Figure 5.5: Time-averaged instantaneous costs incurred by the four voltage control schemes.

using SGD over the empirical loss LTar(θθθτ ; Mτ ), with iterates given by

θθθτ +1 = θθθτ − βτ ∇LTar(θθθτ ; Mτ ).

(5.19)

Incorporating target network and experience replay remedies for stable DRL, our proposed

two-timescale voltage regulation scheme is summarized in Alg. 10.

5.5 Numerical Tests

In this section, numerical tests on a real-world 47-bus distribution feeder as well as the IEEE

123-bus benchmark system are provided to showcase the performance of our proposed DRL-

based voltage control scheme (cf. presented in Alg. 10). As has already been shown in previous

0200400600800100012001400160018002000Interval 0.10.20.30.40.50.60.70.80.9CostFixCapRandCapReal-timeDRLCap111

Figure 5.6: Voltage magnitude proﬁles obtained by the four voltage control schemes over the
simulation period of 10, 000 slots.

Figure 5.7: Voltage magnitude proﬁles obtained by the four voltage control schemes at buses 10
and 33 from slot 9, 900 to 10, 000.

0100020003000400050006000700080009000100001FixedCap0100020003000400050006000700080009000100001RandomCap0100020003000400050006000700080009000100001Real-time010002000300040005000600070008000900010000Slot t1DRLCap9900991099209930994099509960997099809990100000.9511.05Bus 10FixCapRandCapReal-timeDRLCap990099109920993099409950996099709980999010000Slot t0.9511.05Bus 33112

Figure 5.8: Voltage magnitude proﬁles at all buses at slot 9, 900 obtained by the four voltage
control schemes.

works (e.g., [89, 90, 115]), the linearized distribution ﬂow model approximates the exact AC

model very well; hence, numerical results based on the linearized model were only reported here.

The ﬁrst experiment entails the Southern California Edison 47-bus distribution feeder [57],

which is depicted in Fig. 5.4. This feeder is integrated with four shunt capacitors as well as ﬁve

smart inverters. As the voltage magnitude v0 of the substation bus is regulated to be a constant (1
in all our tests) through a voltage transformer, the capacitor at the substation was excluded from

our control. Thus, a total of three shunt capacitors along with ﬁve smart inverters embedded with

large PV plants were engaged in voltage regulation. The rest three capacitors are installed on

buses 3, 37, and 47, with capacities 120, 180, and 180 kVar, respectively, while the ﬁve large

PV plants are located on buses 2, 16, 18, 21, and 22, with capacities 300, 80, 300, 400, and 200

kW, respectively. To test our scheme in a realistic setting, real consumption as well as solar
generation data were obtained from the Smart∗ project collected on August 24, 2011 [12], which

were ﬁrst preprocessed by following the procedure described in our precursor work [89].

In our tests, to match the availability of real data, each slot t was set to a minute, and each

interval τ was set to ﬁve minutes. A power factor of 0.8 was assumed for all loads. The DQN

used here consists of three fully connected layers, which has 44 and 12 units in the ﬁrst and

second hidden layers, respectively. Although simple, it was found sufﬁcient for the task at

0510152025303540Bus Index0.950.960.970.980.9911.011.021.031.041.05Voltage magnitudeFixCapRandCapReal-timeDRLCap113

hand. ReLU activation functions (σ(x) = max(x, 0)) were employed in the hidden layers,
and logistic sigmoid functions s(x) = 1/(1 + e−x) were used at the output layer. To assess

the performance of our proposed scheme, we have simulated three capacitor conﬁguration

policies as baselines, that include a ﬁxed capacitor conﬁguration (FixCap), a random capacitor

conﬁguration (RandCap), and an (impractical) ‘real-time’ policy. Speciﬁcally, the FixCap uses

a ﬁxed capacitor conﬁguration throughout, and the RandCap implements random actions to

conﬁgure the capacitors on every slow time interval; both of which compute the inverter setpoints

by solving (5.9) per slot t. The impractical Real-time scheme however, optimizes over inverters

and capacitors on a single-timescale, namely at every slot – hence justifying its ‘real-time’
characterization. To carry out this optimization task, ﬁrst the binary constraints yki(t) ∈ {0, 1}
are relaxed to box ones yki(t) ∈ [0, 1], the resulting convex program is solved using an off-the-
shelf routine [70], which is followed by a standard rounding step to recover binary solutions for

capacitor conﬁgurations [10].

In the ﬁrst experiment, the DRL-based capacitor conﬁguration (DRLCap) voltage control

approach was examined. The replay buffer size was set to R = 10, the discount factor γ = 0.99,
the mini-batch size Mτ = 10, and the exploration-exploitation parameter (cid:15)τ = max(cid:8)1 − 0.1 ×
(cid:98)τ /50(cid:99), 0(cid:9). During training, the target network was updated every B = 5 iterations. The
time-averaged instantaneous costs

1
τ

τ
(cid:88)

i=1

c(sss(i − 1), aaa(i))

incurred by the four schemes over the ﬁrst 1 ≤ τ ≤ 2, 000 intervals are plotted in Fig. 5.5.

Evidently, the proposed scheme attains a lower cost than FixCap, RandCap, and Real-time after

a short period of learning and interacting with the environment. Even though the real-time

scheme optimizes both capacitor conﬁgurations and inverter setpoints per slot t, its suboptimal

performance in this case arises from the gap between the convexiﬁed problem and the original

nonconvex counterpart. Fig. 5.6 presents the voltage magnitude proﬁles for all buses regulated

by the four schemes sampled at every 100 slots. Again, after a short period (∼ 4, 500 slots) of

training through interacting with the environment, our DRLCap voltage control scheme quickly

learns a stable and (near-) optimal policy. In addition, voltage magnitude proﬁles regulated by

FixCap, RandCap, Real-time, and DRLCap at buses 10 and 33 from slot 9, 900 to 10, 000 are

shown in Fig. 5.7, while the voltage magnitude proﬁles at all buses at slot 9, 900 are presented

114

Figure 5.9: Hyper deep Q-network for capacitor conﬁguration.

Figure 5.10: Time-averaged instantaneous costs incurred by the four approaches on the IEEE
123-bus feeder.

in Fig. 5.8. Curves showcase the effectiveness of our DRLCap scheme in smoothing voltage

ﬂuctuations incurred due to large solar generation as well as heavy load demand.

To deal with distribution systems having a moderately large number of capacitors, we

further advocate a hyper deep Q-network implementation, that endows our DRL-based scheme
with scalability. The idea here is to ﬁrst split the total number 2Na of Q-value predictions
ooo(τ ) ∈ R2Na at the output layer into K smaller groups, each of which is of the same size
2Na/K and is to be predicted by a small-size DQN. This evidently yields the representation
K(τ )](cid:62), where oook(τ ) ∈ R2Na /Kfor k = 1, . . . , K. By running K DQNs
ooo(τ ) := [ooo(cid:62)
in parallel along with their corresponding target networks, each DQN-k generates predicted
Q-values oook(τ ) for the subset of actions corresponding to kth group. Note that all DQNs are fed

1 (τ ), . . . , ooo(cid:62)

DQN-1DQN-KMake decisionHyper Q-network Choose to configure0500100015002000250030003500400045005000Interval 00.20.40.60.811.2CostFixCapRandCapReal-timeDRLCap115

Figure 5.11: Voltage magnitude proﬁles at all buses over the simulation period of 25, 000 slots
on the IEEE 123-bus feeder.

Figure 5.12: Voltage magnitude proﬁles at buses 55 and 90 from slot 24, 900 to 25, 000 obtained
by the four approaches on the IEEE 123-bus feeder.

with the same state vector sss(τ − 1); see also Fig. 5.9 for an illustration.

To examine the scalability and performance of this hyper Q-network implementation, addi-

tional tests using the IEEE 123-bus test feeder with 9 shunt capacitors were performed. Again,
the capacitor at bus 1 was excluded from the control, rendering a total number of 28 = 256

actions (capacitor conﬁgurations). Renewable (PV) units are located on buses 47, 49, 63, 73, 104,

108, 113, with capacities 100, 16, 70, 20, 20, 30, and 10 k, respectively. The 8 shunt capacitors

are installed on buses 3, 20, 44, 93, 96, 98, 100, and 114, with capacities 50, 80, 100, 100, 100,

100, 100, and 60 kVar. In this experiment, we used a total of K = 64 equal-sized DQNs to form

the hyper Q-network, where each DQN implemented a fully connected 3-layer feed-forward

neural network, with ReLU activation functions in the hidden layers, and sigmoid functions at

116

Figure 5.13: Voltage magnitude proﬁles at all buses on slot 24, 900 obtained by four approaches
on the IEEE 123-bus feeder.

the output. The replay buffer size was set to R = 50, the batch size to Mτ = 8, and the target
network updating period to B = 10. The time-averaged instantaneous costs obtained over a

simulation period of 5, 000 intervals is plotted in Fig. 5.10. Moreover, voltage magnitude proﬁles

of all buses over the simulation period of 25, 000 slots sampled at every 100 slots under the

four schemes are plotted in Fig. 5.11; voltage magnitude proﬁles at buses 55 and 90 from slot

24, 900 to 25, 000 are shown in Fig. 5.12; and, voltage magnitude proﬁles at all buses on slot

24, 900 are depicted in 5.13. Evidently, the hyper deep Q-network based DRL scheme smooths

out the voltage ﬂuctuations after a certain period (∼ 7, 000 slots) of learning, while effectively

handling the curse of dimensionality in the control (action) space. Evidently from Figs. 5.10 and

5.13, both the time-averaged immediate cost as well as the voltage proﬁles of DRLCap converge

to those of the impractical ‘real-time’ scheme (which jointly optimizes inverter setpoints and

capacitor conﬁgurations per slot).

5.6 Conclusions

In this section, joint control of traditional utility-owned equipment and contemporary smart

inverters for voltage regulation through reactive power provision was investigated. To account for

the different response times of those assets, a two-timescale approach to minimizing bus voltage

deviations from their nominal values was put forth, by combining physics- and data-driven

stochastic optimization. Load consumption and active power generation dynamics were modeled

as MDPs. On a fast timescale, the setpoints of smart inverters were found by minimizing the

020406080100120Bus Index0.950.960.970.980.9911.011.021.031.041.05Voltage magnitudeFixCapRandCapReal-timeDRLCap117

instantaneous bus voltage deviations, while on a slower timescale, the capacitor banks were

conﬁgured to minimize the long-term expected voltage deviations using a deep reinforcement

learning algorithm. The developed two-timescale voltage regulation scheme was found efﬁcient

and easy to implement in practice, through extensive numerical tests on real-world distribution

systems using real solar and consumption data. This work also opens up several interesting

directions for future research, including deep reinforcement learning for real-time optimal power

ﬂow as well as unit commitment.

5.7 Gauss-Newton Unrolled Neural Networks and

Data-driven Priors for Regularized PSSE with Robustness

5.8 Introduction

In today’s smart grid, reliability and accuracy of state estimation are central for several system

control and optimization tasks, including optimal power ﬂow, unit commitment, economic

dispatch, and contingency analysis [2]. However, frequent and sizable state variable ﬂuctuations

caused by fast variations of renewable generation, increasing deployment of electric vehicles,

and human-in-the-loop demand response incentives, are challenging these functions.

As state variables are difﬁcult to measure directly, the supervisory control and data acquisition

(SCADA) system offers abundant measurements, including voltage magnitudes, power ﬂows,

and power injections. Given SCADA measurements, the goal of PSSE is to retrieve the state

variables, namely complex voltages at all buses [2]. PSSE is typically formulated as a (weighted)

least-squares (WLS) or a (weighted) least-absolute-value (WLAV) problem. The former can be

underdetermined, and nonconvex in general [194], while the latter can be formulated as a linear

programming problem [65, 105] if network only consists of PMUs. In practice however, power

grids must include conventional RTUs as well, which leads to highly complex and non-convex

solution.

To address these challenges, several efforts have been devoted. WLAV-based estimation

for instance can be converted into a constrained optimization, for which a sequential linear

programming solver was devised in [81], and improved (stochastic) proximal-linear solvers were

developed in [191]. On the other hand, focusing on the WLS criterion, the Gauss-Newton solver

is widely employed in practice [2]. Unfortunately, due to the nonconvexity and quadratic loss

118

function, there are two challenges facing the Gauss-Newton solver: i) sensitivity to initialization;

and ii) convergence is generally not guaranteed [236]. Semideﬁnite programming approaches

can mitigate these issues to some extent, at the price of rather heavy computational burden [236].

In a nutshell, the grand challenge of these methods, remains to develop fast and robust PSSE

solvers attaining or approximating the global optimum.

To bypass the nonconvex optimization hurdle in power system monitoring and control, recent

works have focused on developing data- (and model-) driven neural network (NN) solutions

[11, 127, 230, 224, 78, 131, 141]. Such NN-based PSSE solvers approximate the mapping from

measurements to state variables based on a training set of measurement-state pairs generated

using simulators or available from historical data [230]. However, existing NN architectures do

not directly account for the power network topology. On the other hand, a common approach

to tackling challenging ill-posed problems in image processing has been to regularize the loss

function with suitable priors [154]. Popular priors include sparsity, total variation, and low rank

[54]. Recent efforts have also focused on data-driven priors that can be learned from exemplary

data [112, 171, 3].

Permeating the beneﬁts of [112, 171] and [3] to power systems, this paper advocates a deep

(D) NN-based trainable prior for standard ill-posed PSSE, to promote physically meaningful

PSSE solutions. To tackle the resulting regularized PSSE problem, an alternating minimization-

based solver is ﬁrst developed, having Gauss-Newton iterations as a critical algorithmic compo-

nent. As with Gauss-Newton iterations, our solver requires inverting a matrix per iteration, thus

incurring a heavy computational load that may discourage its use for real-time monitoring of

large networks. To accommodate real-time operations and building on our previous works [230],

we unroll this alternating minimization solver to construct a new DNN architecture, that we term

Gauss-Newton unrolled neural networks (GNU-NN) with deep priors. As the name suggests,

our DNN model consists of a Gauss-Newton iteration as a basic building block, followed by a

proximal step to account for the regularization term. Upon incorporating a graph (G) NN-based

prior, our model exploits the structure of the underlying power network. Different from [230],

our GNU-NN method offers a systematic and ﬂexible framework to incorporate prior information

into standard PSSE tasks.

In practice, measurements collected by the SCADA system may be severely corrupted due

to e.g., parameter uncertainty, instrument mis-calibration, and unmonitored topology changes

[130, 191]. As cyber-physical systems, power networks are also vulnerable to adversarial

119

Figure 5.14: The structure of the proposed GNU-NN.

attacks [56, 205], as asserted by the ﬁrst hacker-caused Ukraine power blackout in 2015 [37].

Furthermore, it has recently been demonstrated that adversarial attacks can markedly deteriorate

NNs’ performance [97, 132]. Prompted by this, to endow our GNU-NN approach with robustness

against bad (even adversarial) data, we pursue a principled GNU-NN training method that relies

on a distributionally robust optimization formulation. Numerical tests using the IEEE 118-bus

benchmark system corroborate the performance and robustness of the proposed scheme.

Notation. Lower- (upper-) case boldface letters denote column vectors (matrices), with the

exception of vectors V , P and Q, and normal letters represent scalars. The (i, j)th entry, i-th

row, and j-th column of matrix X are [X]i,j, [X]i:, and [X]:j, respectively. Calligraphic letters
are reserved for sets except operators I and P. Symbol (cid:62) stands for transposition; 0 denotes
all-zero vectors of suitable dimensions; and (cid:107)xxx(cid:107) is the l2-norm of vector xxx.

5.9 Background and Problem Formulation

Consider an electric grid comprising N buses (nodes) with E lines (edges) that can be modeled as
a graph G := (N , E, W ), where the set N := {1, . . . , N } collects all buses, E := {(n, n(cid:48))} ⊆
N × N all lines, and W ∈ RN ×N is a weight matrix with its (n, n(cid:48))-th entry [W ]nn(cid:48) = wnn(cid:48)
modeling the impedance between buses n and n(cid:48). In particular, if (n, n(cid:48)) ∈ E, then [W ]nn(cid:48) =
wnn(cid:48); and [W ]nn(cid:48) = 0 otherwise. For each bus n ∈ N , let Vn := vr
n be its complex
voltage with magnitude denoted by |Vn|, and Pn + jQn its complex power injection. For
reference, collect the voltage magnitudes, active and reactive power injections across all buses

n + jvi

into the N -dimensional column vectors |V |, P , and Q, respectively.

System state variables v := [vr

N ](cid:62) ∈ R2N can be represented by SCADA
N vi
measurements, including voltage magnitudes, active and reactive power injections, as well as

1 . . . vr

1 vi

active and reactive power ﬂows. Let SV , SP , SQ, EP , and EQ denote the sets of buses or lines

1-st iteration1-st iteration2-nd iteration0-th iteration120

where meters of corresponding type are installed. For a compact representation, let us collect the
measurements from all meters into z := [{|Vn|2}n∈SV , {Pn}n∈SP , {Qn}n∈SQ, {Pnn(cid:48)}(n,n(cid:48))∈EP ,
{Qnn(cid:48)}(n,n(cid:48))∈EQ, ](cid:62) ∈ RM . Moreover, the m-th entry of z := {zm}M
m=1, can be described by
the following model

zm = hm(v) + (cid:15)m, ∀m = 1, . . . , M

(5.20)

where hm(v) = v(cid:62)Hmv for some symmetric measurement matrix Hm ∈ R2N ×2N , and (cid:15)m
captures the modeling error as well as the measurement noise.

The goal of PSSE is to recover the state vector v from measurements z. Speciﬁcally, adopting

the least-squares criterion and vectorizing the terms in (5.20), PSSE can be formulated as the

following nonlinear least-squares (NLS)

v∗ := arg min
v∈R2N

(cid:107)z − h(v)(cid:107)2.

(5.21)

A number of algorithms have been developed for solving (5.21), including e.g., Gauss-Newton

iterations [2], and semideﬁnite programming-based solvers [236, 100]. Starting from an initial

v0, most of these schemes (the former two) iteratively implement a mapping from vi to vi+1, in
order to generate a sequence of iterates that hopefully converges to v∗ or some point nearby. In

the ensuing subsection, we will focus on the ‘workhorse’ Gauss-Newton PSSE solver.

5.9.1 Gauss-Newton Iterations

The Gauss-Newton method is the most commonly used one for minimizing NLS [19, Sec. 1.5.1].

It relies on Taylor’s expansion to linearize the function h(v). Speciﬁcally, at a given point vi, it
linearly approximates

˜h(v, vi) ≈ h(vi) + Ji(v − vi)

(5.22)

where Ji := ∇h (vi) is the M × 2N Jacobian of h evaluated at vi, with [Ji]m,n := ∂hm/∂vn.
Subsequently, the Gauss-Newton method approximates the nonlinear term h(v) in (5.21) via

(5.22), and ﬁnds the next iterate as its minimizer; that is,

vi+1 = arg min

v

(cid:107)z − h(vi) − Ji(v − vi)(cid:107)2 .

(5.23)

Clearly, the per-iteration subproblem (5.23) is convex quadratic. If matrix J (cid:62)

i Ji is invertible,

the iterate vi can be updated in closed-form as

121

vi+1 = vi + (cid:0)J (cid:62)

i Ji

(cid:1)−1J (cid:62)

i (z − h(vi))

(5.24)

until some stopping criterion is satisﬁed. In practice however, due to the matrix inversion, the

Gauss-Newton method becomes computationally expensive; it is also sensitive to initialization,

and in certain cases it can even diverge. These limitations discourage its use for real-time

monitoring of large-scale networks. To address these limitations, instead of solving every PSSE

instance (corresponding to having a new set of measurements in z) with repeated iterations, an

end-to-end approach based on DNNs is pursued next.

5.10 Unrolled Gauss-Newton with Deep Priors

As mentioned earlier, PSSE can be underdetermined and thus ill posed due to e.g., lack of

observability. To cope with such a challenge, this section puts forth a ﬂexible topology-aware

prior that can be incorporated as a regularizer of the PSSE cost function in (5.21). To solve the

resultant regularized PSSE, an alternating minimization-based solver is developed. Subsequently,

an end-to-end DNN architecture is constructed by unrolling the alternating minimization solver.

Such a novel DNN is built using several layers of unrolled Gauss-Newton iterations followed by

proximal steps to account for the regularization term. Interestingly, upon utilizing a GNN-based

prior, the power network topology can be exploited in PSSE.

5.10.1 Regularized PSSE with Deep Priors

In practice, recovering v from z can be ill-posed, for instance when Ji is a rectangular ma-
trix. Building on the data-driven deep priors in image denoising [112, 171, 3], we advocate

regularizing any PSSE loss (here, the NLS in (5.21)) with a trainable prior information, as

min
v∈R2N

(cid:107)z − h(v)(cid:107)2 + λ (cid:107)v − D(v)(cid:107)2

(5.25)

where λ ≥ 0 is a tuning hyper-parameter, while the regularizer promotes states v residing close

to D(v). The latter could be a nonlinear ˆv estimator (obtained possibly ofﬂine) based on training

data. To encompass a large family of priors, we advocate a DNN-based estimator Dθθθ(v) with

122

weights θθθ that can be learned from historical (training) data. Taking a Bayesian view, the DNN

Dθθθ(·) can ideally output the posterior mean for a given input.

Although this regularizer can deal with ill conditioning, the PSSE objective in (5.25) remains

nonconvex. In addition, the nested structure of Dθ(·) presents further challenges. Similar to the
Gauss-Newton method for NLS in (5.21), we will cope with this challenge using an alternating

minimization algorithm to iteratively approximate the solution of (5.25). Starting with some

initial guess v0, each iteration i uses a linearized data consistency term to obtain the next iterate
vi+1; that is,

vi+1 = arg min

v

(cid:107)z − h(vi)− Ji(v − vi)(cid:107)2 + λ(cid:107)v − Dθθθ(vi)(cid:107)2

= Aiz + Biui + bi

where we deﬁne

Ai : = (J (cid:62)
Bi : = λ(J (cid:62)
bi : = (J (cid:62)

i Ji + λI)−1J (cid:62)
i
i Ji + λI)−1
i Ji + λI)−1J (cid:62)

i (Jivi − h(vi)).

The solution of (5.25) can thus be approached by alternating between the ensuing two steps

ui = Dθθθ(vi)

vi+1 = Aiz + Biui + bi.

(5.27a)

(5.27b)

Speciﬁcally, with initialization v0 = 0 and input z, the ﬁrst iteration yields v1 = A0z +
B0u0 + b0. Upon passing v1 through the DNN Dθ(·), the output u1 at the ﬁrst iteration, which
is also the input to the second iteration, is given by u1 = Dθθθ(v1) [cf. (5.27a)]. In principle, state
estimates can be obtained by repeating these alternating iterations whenever a new measurement

z becomes available. However, at every iteration i, the Jacobian matrix Ji must be evaluated,
followed by matrix inversions to form Ai, Bi, and bi. The associated computational burden
could be thus prohibitive for real-time monitoring tasks of large-scale power systems.

For fast implementation, we pursue an end-to-end learning approach that trains a DNN

constructed by unrolling iterations of this alternating minimizer to approximate directly the

mapping from measurements z to states v; see Fig. 5.14 for an illustration of the resulting GNU-

123

NN architecture. Recall that in order to derive the alternating minimizer, the DNN prior Dθθθ(·) in
(5.27a) was assumed pre-trained, with weights θθθ ﬁxed in advance. In our GNU-NN however, we
i=0, {bi}I
consider all the coefﬁcients {Ai}I
be learnable from data.

i=0, as well as the DNN weights {θi}I

i=0, {Bi}I

i=0 to

This end-to-end GNU-NN can be trained using backpropagation based on historical or

simulated measurements {zt}T

t=1. Entailing
only several matrix-vector multiplications, our GNU-NN achieves competitive PSSE performance

t=1 and corresponding ground-truth states {v∗t}T

compared with other iterative solvers such as the Gauss-Newton method. Further, relative to the

existing data-driven NN approaches, our GNU-NN can avoid vanishing and exploding gradients.

This is possible thanks to direct (a.k.a skipping) connections from the input layer to intermediate

and output layers.

Remark 6. Albeit the problem remains non-convex, and may converge to a local solution, the key

advantage of data-driven-based PSSE comes from utilizing abundant available historical training

data. Speciﬁcally, the widely used algorithms such as stochastic gradient descent algorithm and

its variants, have been successful to escape local minima while updating the NN weights. To

prevent practical challenges, such as “overﬁtting” and offer better generalization performance,

large training data sets are oftentimes used in practice. Another feature of NNs and other machine

learning approaches is that they alleviate the computational burden at the operation stage by

shifting computationally intensive ‘hard work’ to the off-line training stage. Therefore, the

sensitivity, hyper-parameter tuning, and convergence issues are to be tackled mostly during

training phase. After the mapping function between the measurement z and state vector v is

learned, estimating the states associated with a fresh set of measurements only requires very

simple operations, that is, passing the measurements through the learned NN. This would greatly

improve the efﬁciency of PSSE, bringing real-time state estimation within reach.

Interestingly, by carefully choosing the speciﬁc model for Dθθθ(·), desirable properties such as
scalability and high estimation accuracy can be also effected. For instance, if we use feed forward

NNs as Dθθθ(·), it is possible to obtain a scalable solution for large power networks. However, feed
forward NN can only leverage the grid topology indirectly through simulated MATPOWER data.

This prompts us to focus on GNNs, which can explicitly capture the topology and the physics

of the power network. The resultant Gauss-Newton unrolled with GNN priors (GNU-GNN) is

elaborated next.

124

5.10.2 Graph Neural Network Deep Prior

To allow for richly expressive state estimators to serve in our regularization term, we model Dθθθ(·)
through GNNs, that are a prudent choice for networked data. GNNs have recently demonstrated

remarkable performance in several tasks, including classiﬁcation, recommendation, and robotics

[93]. By operating directly over graphs, GNNs can explicitly leverage the power network

topology. Hence, they are attractive options for parameterization in application domains where

data adhere to a graph structure [93].

Data matrix X ∈ RN ×F with n-th row x(cid:62)

Consider a graph of N nodes with weighted adjacency matrix W capturing node connectivity.
n := [X]n: representing an F × 1 feature vector of
node n, is the GNN input. For the PSSE problem at hand, features are real and imaginary parts

of the nodal voltage (F = 2). Upon pre-multiplying the input X by W , features are propagated
over the network, yielding a diffused version ˇY ∈ RN ×F that is given by

ˇY = W X.

(5.28)

Remark 7. To model feature propagation, a common option is to rely on the adjacency matrix or
any other matrix that preserves the structure of the power network (i.e. Wnn(cid:48) = 0 if (n, n(cid:48)) /∈ E).
Examples include the graph Laplacian, the random walk Laplacian, and their normalized versions.

Basically, the shift operation in (5.28) linearly combines the f -th features of all neighbors to

obtain its propagated feature. Speciﬁcally for bus n, the shifted feature [ ˇY ]nf is

[ ˇY ]nf =

N
(cid:88)

i=1

[W ]ni[X]if =

wnixf
i

(cid:88)

i∈Nn

(5.29)

where Nn = {i ∈ N : (i, n) ∈ E} denotes the set of neighboring buses for bus n. Clearly, this
interpretation generates a diffused copy or shift of X over the graph.

The ‘graph convolution’ operation in GNNs exploits topology information to linearly com-

bine features, namely

[Y ]nd := [H (cid:63) X; W ]nd :=

K−1
(cid:88)

k=0

[W kX]n:[Hk]:d

(5.30)

where H := [H0 · · · HK−1] with Hk ∈ RF ×D concatenating all ﬁlter coefﬁcients; Y ∈ RN ×D

125

Figure 5.15: The signal diffuses from layer l − 1 to l with K = 3.

is the intermediate (hidden) matrix with D features per bus; and W kX linearly combines features

of buses within the k-hop neighborhood by recursively applying the shift operator W .

To obtain a GNN with L hidden layers, let Xl−1 denote the output of the (l − 1)-st layer,
which is also the l-th layer input for l = 1, . . . , L, and X0 = X is the input matrix. The hidden
Yl ∈ RN ×Dl with Dl features is obtained by applying the graph convolution operation (5.30) at
layer l, that is

[Yl]nd =

Kl−1
(cid:88)

[W kXl−1]n:[Hlk]:g

k=0

(5.31)

where Hlk ∈ RFl−1×Fl are the graph convolution coefﬁcients for k = 0, . . . , Kl − 1. The
output Xl at layer l is found by applying a graph convolution followed by a point-wise nonlinear
operation σl(·), such as the rectiﬁed linear unit (ReLu) σl(t) := max{0, t} for t ∈ R; see Fig.
5.15 for a depiction. Rewriting (5.31) in a compact form, we arrive at

Xl = σl(Yl) = σl

(cid:33)

W kXl−1Hlk

.

(cid:32)Kl−1
(cid:88)

k=0

(5.32)

The GNN-based PSSE provides a nonlinear functional operator XL = Φ(X0; Θ, W ) that maps
the GNN input X0 to voltage estimates by taking into account the graph structure through W ,
through

Φ(X0; Θ, W ) =
(cid:32)

(cid:32)KL−1
(cid:88)

σL

W k

. . .

σ1

(cid:32)

k=0

(cid:33)

(cid:33)(cid:33)

(cid:33)

W kX0H1k

. . .

HLk

(cid:32)K1−1
(cid:88)

k=0

(5.33)

where the parameter set Θ contains all the ﬁlter weights; that is, Θ := {Hlk, ∀l, k}, and also
recall that X0 = X.

126

i=0], v0 = 0.

t=1

i }I

i }I

i }I

i=0, {b1

i=0, {B1

i=0, {A1
i }I
t = 1, 2, . . . , T

Algorithm 10 PSSE Solver with GNN Priors.
Training phase:
Input: Training samples {(zt, v∗t)}T
Initialize:
ω1 := [{Θ1
For
Feed zt and v0 as input into GNU-GNN.
i = 0, 1, . . . , I 2
For
Reshape vi ∈ R2N to get X i
Feed X i
0 into GNN.
Vectorize the GNN output X i
Obtain vi+1 ∈ R2N using (5.27b).
Obtain vt
I+1 using (5.27b).
Minimize the loss (cid:96)(v∗t, vt
Output: ωT Inference phase:
For t = T + 1, . . . , T (cid:48)
Feed real-time zt to the trained GNU-GNN.
Obtain the estimated voltage vt.

I+1) and update ωt.

0 ∈ RN ×2.

L ∈ RN ×2 to get ui.

Remark 8. With L hidden layers, Fl features and Kl ﬁlters per layer, the total number of
parameters to be learned is |Θ| = (cid:80)L
l=1 Kl × Fl × Fl−1.

0 ∈ RN ×2. Next, we vectorize the GNN output X i

To accommodate the GNN implementation over the proposed unrolled architecture, at
the i-th iteration, we reshape the states vi ∈ R2N to form the N × 2 GNN input matrix
L ∈ RN ×2 to obtain the vector ui ∈ R2N
X i
(cf. (5.27a)). For notational brevity, we concatenate all trainable parameters of the GNU-GNN
in vector ω := [{Θi}I
i=0], and let π(z; ω) denote the end-to-end
GNU-GNN parametric model, which for given measurements z predicts the voltages across all

i=0, {Ai}I

i=0, {Bi}I

i=0, {b1

i }I

buses, meaning ˆv = π(z; ω). The GNU-GNN weights ω can be updated using backpropagation,
after specifying a certain loss (cid:96)(v∗, vI+1) measuring how well the estimated voltages vI+1 by
the GNU-GNN matches the ground-truth ones v∗. The proposed method is summarized in Alg.

10.

5.11 Robust PSSE Solver

In real-time inference, our proposed GNU-GNN that has been trained using past data, outputs
an estimate of the state vt per time slot t based on the observed measurements zt. However,

127

due to impulsive communication noise and possibly cyberattacks, our proposed GNU-GNN in

Section 5.10 can yield grossly biased estimation results. A natural extension of our approach is

to consider these imperfections in the PSSE problem. Therefore, after proposing our method to

inject prior information and training the DNN for normal input, we robustify our method in the

presence of imperfections in this section.

To obtain estimators robust to bad data, classical formulations including H¨uber estimation,

H¨uber M-estimation, and Schweppe-H¨uber generalized M-estimation, rely on the premise that

measurements obey (cid:15)-contaminated probability models; see e.g., [194]. Instead, the present

paper postulates that measured and ground-truth voltages are drawn from some nominal yet
unknown distribution P0 supported on S = Z × V, that is (z, v∗) ∼ P0. Therefore, to obtain
the end-to-end GNU-GNN parametric model π(z; ω), the trainable parameters ω are optimized

by solving min ω EP0
samples {(zt, v∗t)}T

(cid:2)(cid:96)(π(z; ω), v∗)(cid:3) [132]. In practice, P0 is unknown but i.i.d.

training
t=1 ∼ P0 are available. In this context, our PSSE amounts to solving for the

minimizer of the empirical loss as

min
ω

¯E

(cid:98)P (T )
0

[(cid:96)(π(zt; ω), v∗t)] =

1
T

T
(cid:88)

t=1

(cid:96)(π(zt; ω), v∗t).

(5.34)

To cope with uncertain and adversarial environments, the solution of (5.34) can be robustiﬁed
by optimizing over a set P of probability distributions centered around (cid:98)P (T )
the worst-case expected loss with respect to the choice of any distribution P ∈ P. Concretely,

, and minimizing

0

this can be formulated as the following distributionally robust optimization

min
ω

sup
P ∈P

EP [(cid:96)(π(z; ω), v∗)].

(5.35)

Compared with (5.34), the worst-case formulation in (5.35) ensures a reasonable performance

across a continuum of distributions in P. A broad range of ambiguity sets P could be considered

here. Featuring a strong duality enabled by the optimal transport theory [190], such distribution-

ally robust optimization approaches have gained popularity in robustifying machine learning

models [6]. Indeed, this tractability is the key impetus for this section.

Considering probability density functions P and Q deﬁned over support S, let Π(P, Q) be

the set of all joint probability distributions with marginals P and Q. Also let c : Z × Z → [0, ∞)

2For brevity the superscript t is removed from inner iteration i.

128
be some cost function representing the cost of transporting a unit of mass from (z, v∗) in P to
another element (z(cid:48), v∗) in Q (here we assume that attacker can compromise the measurements
z but not the actual system state v∗). The so-called optimal transport between two distributions

P and Q is given by [190, Page 111]

Wc(P, Q) := inf
π∈Π

Eπ

(cid:2)c(z, z(cid:48))(cid:3).

(5.36)

Intuitively, Wc(P, Q) denotes the minimum cost associated with transporting all the mass from
distribution P to Q. Under mild conditions over the cost function and distributions, Wc gives the
well-known Wasserstein distance between P and Q; see e.g., [180].

Having introduced the distance Wc, let us deﬁne an uncertainty set for the given empirical
) ≤ ρ} that includes all probability distributions hav-

0

, as P := {P |Wc(P, (cid:98)P (T )

distribution (cid:98)P (T )
ing at most ρ-distance from P (T )
for distributionally robust GNU-GNN estimation

0

0

. Incorporating P into (5.35) yields the following optimization

min
ω

sup
P

EP [(cid:96)(π(z; ω), v∗)]

s.t. Wc(P, (cid:98)P (T )

0

) ≤ ρ.

(5.37a)

(5.37b)

Notice that the inner functional optimization in (5.37a) runs over all probability distributions

P characterized by (5.37b). It is intractable to optimize directly over the inﬁnite-dimension

distribution functions. Fortunately, for continuous loss as well as transportation cost functions,

the optimal objective value of the inner maximization is equal to its dual optimal objective value.

In addition, the dual problem involves optimization over only a one-dimension variable. These

two observations prompt us to solve (5.37) in the dual domain. To formally obtain this tractable

surrogate, we call for a result from [22].

Proposition 3. Let the loss (cid:96) : ω×Z ×V → [0, ∞), and transportation cost c : Z ×Z → [0, ∞)
be continuous functions. Then, for any given (cid:98)P (T )

, and ρ > 0, it holds

0

EP [(cid:96)(π(z; ω), v∗)] =

sup
P ∈P

(5.38)

inf
γ≥0

(cid:8)¯E

(z,v∗)∼ (cid:98)P (T )

0

(cid:96)(π(ζ; ω), v∗)+γ(ρ − c(z, ζ))(cid:3)(cid:9)

(cid:2) sup
ζ∈Z

where P :=

(cid:110)
P |Wc(P, (cid:98)P (T )

0

) ≤ ρ

(cid:111)
.

129

Remark 9. Thanks to the strong duality, the right-hand side in (5.38) simply is a univariate dual

reformulation of the primal problem given on the left-hand side. In contrast with the primal

formulation, the expectation in the dual domain is taken only over the empirical distribution
(cid:98)P (T )
for ﬁnding the optimal coupling π ∈ Π to deﬁne P, and characterizing the primal objective for

rather than over any P ∈ P. Furthermore, since this reformulation circumvents the need

0

all P ∈ P, it is practically appealing and convenient.

Capitalizing on Proposition 3, the inner maximization can be replaced with its dual reformu-

lation. As a consequence, the following distributionally robust PSSE optimization can be arrived

at

min
ω

inf
γ≥0

¯E

(cid:104)
(z,v∗)∼ (cid:98)P (T )

0

sup
ζ∈Z

(cid:105)
(cid:96)(π(ζ; ω), v∗)+γ(ρ−c(z, ζ))
.

(5.39)

Finding the optimal solution (ω∗, γ∗) of (5.39) is in general challenging, because it requires

the supremum to be solved separately per observed measurements z, that cannot readily be

handled by existing minimax optimization solvers. A common approach to bypassing this
hurdle is to approximate the optimal ω∗ by solving (5.39) with a preselected and ﬁxed γ > 0

[180]. Indeed, it has been shown in [180] that for any strongly convex transportation cost
function, such as c(z, z(cid:48)) := (cid:107)z − z(cid:48)(cid:107)2
p for any p ≥ 1, a sufﬁciently large γ > 0 ensures that the
inner maximization is strongly convex, hence efﬁciently solvable. Note that having a ﬁxed γ is

tantamount to tuning ρ, which in turn controls the level of infused robustness. Fixing some large

enough γ > 0 in (5.39), our robustiﬁed GNU-GNN model can thus be obtained by solving

¯E

min
ω

(z,v∗)∼ (cid:98)P (T )

0

(cid:104)

sup
ζ∈Z

(cid:105)
ψ(ω, ζ; z, v∗)

(5.40)

where

ψ(ω, ζ; z, v∗) := (cid:96)(π(ζ; ω), v∗) + γ(ρ − c(z, ζ)).

(5.41)

Intuitively, (5.40) can be understood as ﬁrst ‘adversarially’ perturbing the measurements z
into ζ∗ by maximizing ψ(·), and then seeking a model that minimizes the empirical loss with

respect to even such perturbed inputs. Therefore, the robustness of the sought model is achieved
to future data that may be contaminated by adversaries. Initialized with some ω0, and given a

130
datum (zt, v∗), we form ψ(·) (c.f. (5.41)), and implement a single gradient ascent step for the

inner maximization as follows

ζt = zt + ηt∇ζψ(ωt, ζ; zt, v∗t)(cid:12)

(cid:12)ζ=zt

(5.42)

where ηt > 0 is the step size. Upon evaluating (5.42), the perturbed data ζt will be taken as
input (replacing the ‘healthy’ data zt) fed into Algorithm 10. Having the loss (cid:96)(π(ζt; ω), v∗t)
as solely a function of the GNU-GNN weights ω, the current iterate ωt can be updated again by

backpropagation.

5.12 Numerical Tests

This section tests the estimation performance as well as robustness of our proposed methods.

5.12.1 Simulation Setup

The simulations were carried out on an NVIDIA Titan X GPU with a 12GB RAM. For numerical

tests, we used real load consumption data from the 2012 Global Energy Forecasting Competition

(GEFC) [1]. Using this dataset, training and testing collections were prepared by solving the AC

power ﬂow equations using the MATPOWER toolbox. To match the scale of power demands, we

normalized the load data, and fed it into MATPOWER to generate 1, 000 pairs of measurements

and ground-truth voltages, 80% of which were used for training while the remaining 20% were

employed for testing. Measurements include all sending-end active power ﬂows, as well as

voltage magnitudes, corrupted by additive white Gaussian noise. Standard deviations of the noise

added to power ﬂows and voltage magnitudes were set to 0.02 and 0.01 [191], respectively.

A reasonable question to ponder is whether explicitly incorporating the power network topol-

ogy through a trainable regularizer offers improved performance over competing alternatives. In

addition, it is of interest to study how a distributionally robust training method enhances PSSE

performance in the presence of bad data and even adversaries. To this aim, four baseline PSSE

methods were numerically tested, including one optimization-based method Wirtinger-Flow

Gauss-Newton algorithm in [53], and three data-driven methods: i) the prox-linear network

in [230]; ii) a 6-layer vanilla feed-forward (F)NN; and iii) an 8-layer FNN. The weights of these

NNs were trained using the Adam optimizer to minimize the H¨uber loss. The learning rate was

131

Figure 5.16: The estimated voltage magnitudes and angles by the four schemes at bus 50 from
slots 70 to 90.

ﬁxed to 10−3 throughout 500 epochs, and the batch size was set to 32. Furthermore, the average

estimation accuracy of each algorithm is deﬁned as follows

ν =

1
N

N
(cid:88)

n=1

(cid:107)vn − v∗

n(cid:107)2
2

(5.43)

where vn is the estimated voltage proﬁle from the noisy measurements generated using v∗
n.

5.12.2 GNU-GNN for regularized PSSE

In the ﬁrst experiment, we implemented GNU-GNN by unrolling I = 6 iterations of the proposed

alternating minimizing solver, respectively. A GNN with K = 2 hops, and D = 8 hidden units

with ReLU activations per unrolled iteration was used for the deep prior of GNU-GNN. The

GNU-GNN architecture was designed to have total number of weight parameters roughly the

same as that of the prox-linear network. The Gauss-Newton algorithm is initialized using the ﬂat

voltage proﬁle. Table I tabulates the average performance of the proposed GNU-GNN approach,

the Gauss-Newton method, the prox-linear network, 6-layer FNN and 8-layer FNN over 200

testing samples. We report the accuracy of estimation on the IEEE 118-bus feeder, and the

70727476788082848688900.970.980.991.001.01Voltage mag. (p.u.)Voltages for bus 507072747678808284868890Iteration50607080Voltage angle (degree)Ground truthGNU-GNNProx-linear net6-layer FNN8-layer FNN132

Figure 5.17: The estimated voltage magnitudes and angles by the four schemes at bus 100 from
slot 70 to 90.

running time (s) on IEEE 118-bus feeder, as well as IEEE 300-bus feeder. Clearly, the proposed

GNU-GNN approach achieves superior performance where the accuracy of estimation is an

order of magnitude better than the state-of-the-art Gauss-Newton approach on 118-bus feeder. In

addition, since the GNU-GNN approach alleviates almost all the computational burden at the

estimation time by shifting it to the training time, the running time of the proposed approach

is three orders of magnitude less than the optimization-based approach on both IEEE 118-bus

feeder and IEEE 300-bus feeder.

In order to show the quality of estimates provided by the proposed GNU-GNN method, we

present the estimate of the voltage magnitudes and angles on the IEEE 118-bus feeder. As is

shown in Table I, the estimation performance of the Gauss-Newton method is too bad to depict

in the same ﬁgure with other alternatives. Therefore, we did not include the Gauss-Newton in

this set of results. Figs. 5.16 and 5.17 show the estimated voltage proﬁles obtained at buses

50 and 100 from test slots 70 to 90, respectively. The ground-truth and estimated voltages for

the ﬁrst 20 buses on the test slot 80 are presented in Fig. 5.18. These results corroborate the

improved performance of our GNU-GNN relative to the simulated PSSE solvers.

70727476788082848688900.991.001.011.02Voltage mag. (p.u.)Voltages for bus 1007072747678808284868890Iteration708090100Voltage angle (degree)Ground truthGNU-GNNProx-linear net6-layer FNN8-layer FNN133

Figure 5.18: The estimated voltages magnitudes and angles by the four schemes for the ﬁrst 20
buses at slot 80.

5.12.3 Robust PSSE

Despite their remarkable performance in standard PSSE, DNNs may fail to yield reliable and ac-

curate estimates in practice when bad data are present. Evidently, this challenges their application

in safety-critical power networks. In the experiment of this subsection we examine the robustness

of our GNU-GNN trained with the described adversarial learning method on the IEEE 118-bus

feeder. To this aim, a distributionally robust learning scheme was implemented to manipulate

the input of GNU-GNN, prox-linear net, 6-layer FNN, and 8-layer FNN models. Speciﬁcally,

under distributional attacks, an ambiguity set P comprising distributions centered at the nominal

data-generating P0 was postulated. Although the training samples were generated according to
P0, testing samples were obtained by drawing samples from a distribution P ∈ P that yields
the worst empirical loss. To this end we preprocessed test samples using (5.42) to generate

adversarially perturbed samples. Figs. 5.19 and 5.20 demonstrate the estimated voltage proﬁles

under a distributional attack with a ﬁxed γ = 0.13 (c.f. (5.40) and (5.41)) and (cid:96)2 transportation
cost. As the plots showcase, our proposed robust training method enjoys guarantees against

distributional uncertainties, especially relative to competing alternatives.

24681012141618200.960.981.001.021.041.06Voltage mag. (p.u.)Voltages for test slot 802468101214161820Bus number8090100110Voltage angle (degree)Ground truthGNU-GNNProx-linear net6-layer FNN8-layer FNN134

Figure 5.19: The estimated voltage magnitudes and angles by the four schemes under distribu-
tional attacks at bus 100 from slots 70 to 90.

Figure 5.20: The estimated voltage magnitudes and angles by the four schemes under distribu-
tional attacks for the ﬁrst 20 buses at slot 80.

70727476788082848688900.9500.9751.0001.0251.0501.075Voltage mag. (p.u.)Voltages for bus 1007072747678808284868890Iteration708090100Voltage angle (degree)Ground truthGNU-GNNProx-linear net6-layer FNN8-layer FNN24681012141618200.9250.9500.9751.0001.0251.050Voltage mag. (p.u.)Voltages for test slot 802468101214161820Bus number8090100110Voltage angle (degree)Ground truthGNU-GNNProx-linear net6-layer FNN8-layer FNN5.13 Conclusions

135

This section introduced topology-aware DNN-based regularizers to deal with the ill-posed and

nonconvex characteristics of standard PSSE approaches. An alternating minimization solver was

developed to approach the solution of the regularized PSSE objective function, which is further

unrolled to construct a DNN model. For real-time monitoring of large-scale networks, the result-

ing DNN was trained using historical or simulated measured and ground-truth voltages. A basic

building block of our GNU-GNN consists of a Gauss-Newton iteration followed by a proximal

step to deal with the regularization term. Numerical tests showcased the competitive performance

of our proposed GNU-GNN relative to several existing ones. Further, a distributionally robust

training method was presented to endow the GNU-GNN with resilience to bad data that even

come from adversarial attacks.

Chapter 6

Summary and Future Directions

Leveraging recent advances in machine learning, deep learning models in conjunction with

statistical signal processing, this thesis pioneered robust, deep, reinforced learning algorithms

with applications in management and control of cyber-physical systems. In this ﬁnal chapter, we

provide a summary of the main results discussed in this thesis, and also point out a few possible

directions for future research.

6.1 Thesis Summary

Chapter 2 dealt with distributionally robust learning, where the data distribution was considered

unknown. A framework to robustify parametric machine learning models against distributional

uncertainties was put forth, where the so-called Wasserstin distance metric was used to quantify

the distance between training and testing data generating data distributions.

Chapter 3 explored robust semi-supervised learning over graphs. To account for uncertainties

associated with data distributions, or adversarially manipulated input data, a principled robust

learning framework was developed. Using the parametric models of graph neural networks

(GNNs), we were able to reconstruct the unobserved nodal values. Experiments corroborated the

outstanding performance of the novel method when the input data are corrupted.

Chapter 4 targeted a network resource allocation problem, namely the caching. The idea

of caching was to device some entities in a wireed and wireless network with storage capacity.

These devices are to store reusable information during off-the peak instances, and then reuse them

during on-peak demand periods. By smartly storing popular contents, these devices efﬁciently

136

137

help the network to decrease the operational costs and increase user satisfaction. Especially, we

designed a generic setup where a caching unit makes sequential fetch-cache decisions based

on dynamic content popularities in local section as well as global network. Critical practical

constraints were identiﬁed, the aggregated cost across ﬁles and time instants was formed, and the

optimal adaptive caching was then formulated as: i) classical reinforcement learning algorithm;

ii) Deep reinforcement learning problem; and, iii) a stochastic optimization problem. To address

the inherent functional estimation problem that arises in each type of considered problems, while

leveraging the underlying problem structure, several computationally efﬁcient algorithms were

developed.

Finally, Chapter 5 developed a suite of methods to efﬁciently monitor, and manage the smart

grid. Especially, we started with voltage regulation problem using joint control of traditional

utility-owned equipment and contemporary smart inverters to inject reactive power. To account

for the different response times of those assets, a two-timescale approach to minimizing bus

voltage deviations from their nominal values was put forth, by combining physics- and data-

driven stochastic optimization. Load consumption and active power generation dynamics were

modeled as MDPs. On a fast timescale, the setpoints of smart inverters were found by minimizing

the instantaneous bus voltage deviations, while on a slower timescale, the capacitor banks were

conﬁgured to minimize the long-term expected voltage deviations using a deep reinforcement

learning algorithm. Then we considered the second problem of monitoring the smart grid. In

particular, topology-aware DNN-based regularizers were developed to deal with the ill-posed

and nonconvex characteristics of standard power system state estimation approaches (PSSE). An

alternating minimization solver was developed to approach the solution of the regularized PSSE

objective function, which was further unrolled to construct a DNN model.

6.2 Future Research

The contributions in this thesis opens up a broad range of interesting directions to explore and
new problems to solve. Some of such possible research directions are brieﬂy discussed next. 1

1Due to space limitations, a few works of this PhD thesis have not been reported here, including [218, 160, 217,

103, 212].

138

6.2.1 Multi-agent, distributionally robust decentralized RL

We will investigate consensus-based decentralized optimization for our scalable and distribution-

ally robust RL framework, with the ultimate goal of developing safe, multi-agent RL algorithms

operating in complex real-world environments. Autonomous driving for instance, is naturally

a multi-agent collaborative setting, where the host vehicle (a.k.a.

the planner) must apply

sophisticated negotiation skills with other road users (agents), when overtaking, giving way,

merging, taking left/right turns, or when pushing ahead in unstructured urban roadways. We will

also broaden the scope of our multi-step Lyapunov tool, to obtain non-asymptotic performance

guarantees of the proposed multi-agent, distributionally robust RL algorithms. We will also

corroborate our analytical results extensive experiments.

6.2.2 Robust learning approach to fairness in machine learning.

Fairness-aware machine learning algorithms seek methods under which the predicted outcome of

a classiﬁer is fair or non-discriminatory based on sensitive attributes such as race, sex, religion,

etc. Broadly, fairness-aware machine learning algorithms have been categorized as those pre-

processing techniques designed to modify the input data so that the outcome of any machine

learning algorithm applied to that data will be fair [153]. Preprocessing algorithms considers

training data as the cause of the discrimination. This simply is because the training data itself

captures historical discrimination or since there are more subtle patterns in the data. Feature

modiﬁcation, data set massaging, and learning unbiased data transformation are examples for

this class of methods [58, 33, 85]. The algorithm modiﬁcation techniques on the other hand

modify an existing algorithm or create a new one that will be fair under any inputs. Algorithm

modiﬁcation target speciﬁc learning algorithms, e.g., by imposing additional constraints. These

methods have been by far the most common methods to promote fairness. Among popular

techniques in this class are the regularization techniques, convex relaxation of fairness promoting

constraints, and training separate models for each value of a sensitive attribute [87, 223, 32].

Combined preprocessing and algorithm modiﬁcation methods are among effective methods at

classiﬁcation [226]. Finally, the post-processing techniques enforce the output of any model to

be fair. These methods modify the results of a previously trained classiﬁer to achieve the desired

results on different groups. For example modifying the labels of leaves in a decision tree to

satisfy fairness constraints, or modifying error proﬁles to name a few [86, 74, 204]. Despite their

139

success in dealing with some sensitive features, these proposed methods cannot handle setups

where the data is coming from unbalanced mixture of distributions where we should protect at

least one of classes. For example, consider data is comming from a mixture of distributions,
that is {xn, yn}N
n=1 ∼ P := αQ0 + (1 − α)Q1, where α ∈ [0, 1) is subpopulation portion, and
Q0 and Q1 are unknown subpopulations. The classical learning approaches do not guarantee to
ensure equitable performance for data from both Q0 and Q1, especially for small α. To offer a
fair model, we instead focus on the worst-case risk that ﬁnds model parameters θ by minimizing

the loss over the latent subpopulation Q0

minimize
θ∈Θ

Ex∼Q0E[(cid:96)(θ; (x, y))]

(6.1)

Since α and Q0 are unknown, it is impossible to compute the loss here from observed data. There-
fore we postulate a lower bound α0 ∈ (0, 1/2) on the subpopulation proportion α and consider a
set of potential minority subpopulations Pα0 := {Q0 : P = αQ0 + (1 − α)Q1 for some α ≥ α0},
and target to solve the worst case loss, formulated as follows

minimize
θ∈Θ

sup
Q0∈Pα0

E[(cid:96)(θ; (x, y))].

(6.2)

Relying on the proposed techniques described in this T2, we will provide tractable approaches

to solve this optimization problem.

6.2.3 Communication- and computation-efﬁcient robust federated learning

The recently growing need to learn from massive datasets that are distributed across multiple

sites, has propelled research to replace a single learner with multiple learners (a.k.a. workers)

exchanging information with a server to learn the sought learning model while abiding with

the privacy of local data. Albeit appealing for its scalability, to endow this so-termed federated

learning with robustness too, we must address the server-workers communication overhead that

is known to constitute the bottleneck in this setup. This becomes aggravated in deep learning,

where one may have to deal with millions of unknown parameters.

To outline our research outlook in this setting, consider M workers with each worker
m ∈ M collecting samples {zt(m)}T
t=1, and a globally shared model θ that is to be updated
at the server by aggregating gradients computed locally per worker. Bandwidth and privacy

concerns discourage uploading these distributed data to the server, which necessitates training

to be performed by having workers communicating iteratively with the server. To endow

such a distributed learning approach with robustness, we will consider solving the following

optimization problem in a distributed fashion

140

min
θ∈Θ

sup
P ∈P

Ez∼P [(cid:96)(z; θ)] ,

s. to. P :=

(cid:110)

P

(cid:12)
(cid:12)
(cid:12)

M
(cid:88)

m=1

(cid:111)
Wc(P, (cid:98)P (T )(m) ≤ ρ

(6.3)

where Wc(P, (cid:98)P (T )(m)) is the distance between P and the locally available distribution (cid:98)P (T )(m)
per learner m. A critical task here is to efﬁciently handle the communication overhead while

guaranteeing the desired robustness. To this aim, we will develop a general framework building

on the considered distributionally robust approach. Tapping into our expertise in communication-

efﬁcient decentralized learning and wireless sensor networks [137, 119, 16, 199], we will

develop methods to integrate distributional robustness in a large-scale parallel architecture with

communication-friendly learning schemes through quantization, and censoring. The quantized

gradients computed locally at the learners will be transmitted to the server at a controllably

low cost; while censoring will save communication costs in learner-server rounds by simply

skipping less informative gradients. To maximize the communication efﬁciency, we will further

investigate two-way communication compression, meaning we will compress both the upload

and download information to a limited number of bits. It will be interesting to delineate the

tradeoffs emerging between robustness, overhead reduction, and convergence rate of the learning

iterates. To this end, we will investigate performance both analytically, as well as with thorough

numerical tests.

References

[1] [Online]. Available: https://www.kaggle.com/c/global-energy-forecasting-competition-

2012- load-forecasting/data.

[2] A. Abur and A. G. Exposito, Power System State Estimation: Theory and Implementation.

New York, USA: CRC Press, 2004.

[3] H. K. Aggarwal, M. P. Mani, and M. Jacob, “MoDL: Model-based deep learning archi-

tecture for inverse problems,” IEEE Trans. Med. Imag., vol. 38, no. 2, pp. 394–405, Aug.

2018.

[4] J. G. Andrews, S. Buzzi, W. Choi, S. V. Hanly, A. Lozano, A. C. K. Soong, and J. C.

Zhang, “What will 5G be?” IEEE Journal on Selected Areas in Communications, vol. 32,

no. 6, pp. 1065–1082, June 2014.

[5] J. G. Andrews, H. Claussen, M. Dohler, S. Rangan, and M. C. Reed, “Femtocells: Past,

present, and future,” IEEE Journal on Selected Areas in Communications, vol. 30, no. 3,

pp. 497–508, Apr. 2012.

[6] C. Bandi and D. Bertsimas, “Robust option pricing,” Eur. J. Oper. Res., vol. 239, no. 3, pp.

842–853, Dec. 2014.

[7] ——, “Robust option pricing,” Eur. J. Oper. Res., vol. 239, no. 3, pp. 842–853, 2014.

[8] M. Baran and F. F. Wu, “Optimal sizing of capacitors placed on a radial distribution

system,” IEEE Trans. Power Del., vol. 4, no. 1, pp. 735–743, Jan. 1989.

[9] M. E. Baran and F. F. Wu, “Network reconﬁguration in distribution systems for loss

reduction and load balancing,” IEEE Trans. Power Del., vol. 4, no. 2, pp. 1401–1407, Apr.

1989.

141

142

[10] ——, “Optimal capacitor placement on radial distribution systems,” IEEE Trans. Power

Del., vol. 4, no. 1, pp. 725–734, Jan. 1989.

[11] P. P. Barbeiro, J. Krstulovic, H. Teixeira, J. Pereira, F. J. Soares, and J. P. Iria, “State

estimation in distribution smart grids using autoencoders,” in IEEE Intl. Power Eng. and

Opt. Conf., 2014, pp. 358–363.

[12] S. Barker, A. Mishra, D. Irwin, E. Cecchet, P. Shenoy, and J. Albrecht, “Smart*: An open

data set and tools for enabling research in sustainable homes,” SustKDD, vol. 111, no.

112, p. 108, Aug. 2012.

[13] E. Bastug, M. Bennis, and M. Debbah, “A transfer learning approach for cache-enabled

wireless networks,” in Intl. Symp. on Modeling and Optimization in Mobile, Ad Hoc, and

Wireless Networks, Mumbai, India, May 2015, pp. 161–166.

[14] M. Bazrafshan, N. Gatsis, and H. Zhu, “Optimal tap selection of step-voltage regulators

in Multi-phase distribution networks,” in Proc. of Power Syst. Comput. Conf., Dublin,

Irelands, Jun. 11-15 2018.

[15] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new

perspectives,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798–1828,

Aug. 2013.

[16] D. Berberidis, V. Kekatos, and G. B. Giannakis, “Online censoring for large-scale regres-

sions with application to streaming big data,” IEEE Trans. Signal Process., vol. 64, no. 15,

pp. 3854–3867, Aug. 2016.

[17] D. Berberidis, A. N. Nikolakopoulos, and G. B. Giannakis, “AdaDIF: Adaptive diffusions

for efﬁcient semi-supervised learning over graphs,” Intl. Conf. on Big Data, pp. 92–99,

2018.

[18] D. P. Bertsekas, “Nonlinear programming,” J. Oper. Res. Soc., vol. 48, no. 3, pp. 334–334,

1997.

[19] ——, Nonlinear Programming.

2nd ed. Belmont, MA, USA: Athena Sci., 1999.

143

[20] B. N. Bharath, K. G. Nagananda, and H. V. Poor, “A learning-based approach to caching

in heterogenous small cell networks,” IEEE Transactions on Communications, vol. 64,

no. 4, pp. 1674–1686, Apr. 2016.

[21] J. Blanchet, Y. Kang, F. Zhang, and K. Murthy, “Data-driven optimal transport cost

selection for distributionally robust optimization,” Stat., vol. 1050, pp. 1527–1554, 2006.

[22] J. Blanchet and K. Murthy, “Quantifying distributional model risk via optimal transport,”

Math. Oper. Res., vol. 44, pp. 565–600, 2019.

[23] ——, “Quantifying distributional model risk via optimal transport,” Math. Oper. Res.,

vol. 44, no. 2, pp. 565–600, 2019.

[24] P. Blasco and D. G¨und¨uz, “Learning-based optimization of cache content in a small cell

base station,” in IEEE Intl. Conf. on Commun., Sydney, Australia, June 10-14, 2014, pp.

1897–1903.

[25] P. Blasco and D. G¨und¨uz, “Content-level selective ofﬂoading in heterogeneous networks:

Multi-armed bandit optimization and regret bounds,” arXiv preprint arXiv:1407.6154,

2014.

[26] B. Blaszczyszyn and A. Giovanidis, “Optimal geographic caching in cellular networks,”

in Intl. Conf. on Communications, London, UK, June 2015, pp. 3358–3363.

[27] V. S. Borkar and S. P. Meyn, “The ODE method for convergence of stochastic approxi-

mation and reinforcement learning,” SIAM Journal on Control and Optimization, vol. 38,

no. 2, pp. 447–469, 2000.

[28] ——, “The ODE method for convergence of stochastic approximation and reinforcement

learning,” SIAM J. Control Optim., vol. 38, no. 2, pp. 447–469, 2000.

[29] S. Borst, V. Gupta, and A. Walid, “Distributed caching algorithms for content distribution

networks,” in Intl. Conf. Comput. Commun., San Diego, CA, USA, Mar. 15-19, 2010, pp.

1–9.

[30] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge, U.K.: Cambridge Univ.

Press, 2004.

144

[31] L. Breslau, P. Cao, L. Fan, G. Phillips, and S. Shenker, “Web caching and Zipf-like

distributions: Evidence and implications,” in Proc. Intl. Conf. Comput. Commun., New

York, USA, March 1999, pp. 126–134.

[32] T. Calders and S. Verwer, “Three naive bayes approaches for discrimination-free classiﬁ-

cation,” Data Mining Knwl. Discov., vol. 21, no. 2, pp. 277–292, 2010.

[33] F. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney, “Optimized

pre-processing for discrimination prevention,” in Adv. Neural Info. Process. Syst., 2017,

pp. 3992–4001.

[34] J. A. Carta, P. Ramirez, and S. Velazquez, “A review of wind speed probability distributions

used in wind energy analysis: Case studies in the Canary Islands,” Renew. Sust. Energ.

Rev., vol. 13, no. 5, pp. 933–955, Jun. 2009.

[35] P. M. Carvalho, P. F. Correia, and L. A. Ferreira, “Distributed reactive power generation

control for voltage rise mitigation in distribution networks,” IEEE Trans. Power Syst.,

vol. 23, no. 2, pp. 766–772, May 2008.

[36] D. U. Case, “Analysis of the cyber attack on the Ukrainian power grid,” 2016.

[37] ——, “Analysis of the cyber attack on the Ukrainian power grid,” E-ISAC, vol. 388, Mar.

2016.

[38] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay, “Adversarial

attacks and defences: A survey,” arXiv:1810.00069, 2018.

[39] O. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learning,” IEEE Trans. Neural

Netw., vol. 3, p. 542, 2009.

[40] B. Chen, C. Yang, and Z. Xiong, “Optimal caching and scheduling for cache-enabled

D2D communications,” IEEE Communications Letters, vol. 21, no. 5, pp. 1155–1158,

May 2017.

[41] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint learning and commu-

nications framework for federated learning over wireless networks,” arXiv:1909.07972,

2019.

145

[42] T. Chen, A. G. Marques, and G. B. Giannakis, “DGLB: Distributed stochastic geographical

load balancing over cloud networks,” IEEE Trans. Parallel Distrib. Syst., vol. 28, no. 7,

pp. 1866–1880, July 2017.

[43] B. J. Claessens, P. Vrancx, and F. Ruelens, “Convolutional neural networks for automatic

state-time feature extraction in reinforcement learning applied to residential load control,”

IEEE Trans. Smart Grid, vol. 9, no. 4, pp. 3259–3269, July 2018.

[44] E. Dahlman, S. Parkvall, and J. Skold, 4G: LTE/LTE-advanced for Mobile Broadband.

Academic press, 2013.

[45] J. Dai, Z. Hu, B. Li, J. Liu, and B. Li, “Collaborative hierarchical caching with dynamic

request routing for massive content distribution,” in Intl. Conf. Comput. Commun., Orlando,

FL, USA, Mar. 25-30, 2012, pp. 2444–2452.

[46] J. M. Danskin, “The theory of max-min, with applications,” SIAM J. Appl. Math., vol. 14,

no. 4, pp. 641–664, 1966.

[47] M. Dehghan, B. Jiang, A. Seetharam, T. He, T. Salonidis, J. Kurose, D. Towsley, and

R. Sitaraman, “On the complexity of optimal request routing and content caching in

heterogeneous cache networks,” IEEE/ACM Trans. Netw., vol. 25, no. 3, pp. 1635–1648,

June 2017.

[48] E. Delage and Y. Ye, “Distributionally robust optimization under moment uncertainty with

application to data-driven problems,” Operations research, vol. 58, no. 3, pp. 595–612,

2010.

[49] R. Diao, Z. Wang, D. Shi, Q. Chang, J. Duan, and X. Zhang, “Autonomous voltage control

for grid operation using deep reinforcement learning,” in Proc. of PESGM, Atlanta, GA,

Aug. 4-8, 2019, pp. 1–5.

[50] Y. Dong, M. Z. Hassan, J. Cheng, M. J. Hossain, and V. C. Leung, “An edge computing

empowered radio access network with UAV-mounted FSO fronthaul and backhaul: Key

challenges and approaches,” IEEE Wirel. Commun., vol. 25, no. 3, pp. 154–160, Jul. 2018.

146

[51] K. Doppler, M. Rinne, C. Wijting, C. B. Ribeiro, and K. Hugl, “Device-to-device commu-

nication as an underlay to LTE-advanced networks,” IEEE Communications Magazine,

vol. 47, no. 12, pp. 42–49, Dec. 2009.

[52] J. Duan, H. Xu, and W. Liu, “Q-learning-based damping control of wide-area power

systems under cyber uncertainties,” IEEE Trans. Smart Grid, vol. 9, no. 6, pp. 6408–6418,

Nov 2018.

[53] I. Dzaﬁc, R. A. Jabr, and T. Hrnjic, “Hybrid state estimation in complex variables,” IEEE

Trans. Power Systems, vol. PP, no. 99, pp. 1–1, 2018.

[54] H. W. Engl, M. Hanke, and A. Neubauer, Regularization of Inverse Problems. Berlin,

HR: SSBM, 1996, vol. 375.

[55] D. Ernst, M. Glavic, and L. Wehenkel, “Power systems stability control: Reinforcement

learning framework,” IEEE Trans. Power Syst., vol. 19, no. 1, pp. 427–435, Feb. 2004.

[56] P. Fairley, “Cybersecurity at U.S. utilities due for an upgrade: Tech to detect intrusions

into industrial control systems will be mandatory,” IEEE Spectr., vol. 53, no. 5, pp. 11–13,

May 2016.

[57] M. Farivar, C. R. Clarke, S. H. Low, and K. M. Chandy, “Inverter VAR control for distri-

bution systems with renewables,” in Proc. IEEE SmartGridComm., Brussels, Belgium,

Oct. 2011, pp. 457–462.

[58] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian,

“Certifying and removing disparate impact,” in Proc. ACM SIGKDD. ACM, 2015, pp.

259–268.

[59] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane,

“Adversarial attacks on medical machine learning,” Science, vol. 363, no. 6433, pp. 1287–

1289, Mar. 2019.

[60] F. Gama, A. G. Marques, G. Leus, and A. Ribeiro, “Convolutional neural network

architectures for signals supported on graphs,” IEEE Trans. Signal Process., vol. 67,

pp. 1034–1049, 2019.

147

[61] L. Gan, N. Li, U. Topcu, and S. H. Low, “Exact convex relaxation of optimal power ﬂow

in radial networks,” IEEE Trans. on Autom. Control, vol. 60, no. 1, pp. 72–87, Jan. 2015.

[62] J. Gao and R. Jamidar, “Machine learning applications for data center optimization,”

Google White Paper, Oct. 27, 2014.

[63] L. Georgiadis, M. J. Neely, and L. Tassiulas, “Resource allocation and cross-layer control

in wireless networks,” Found. Trends Netw., vol. 1, no. 1, pp. 1–144, 2006. [Online].

Available: http://dx.doi.org/10.1561/1300000001

[64] A. Geramifard, T. J. Walsh, S. Tellex, G. Chowdhary, N. Roy, and J. P. How, “A tutorial

on linear function approximators for dynamic programming and reinforcement learning,”

Foundations and Trends in Machine Learning, vol. 6, no. 4, pp. 375–451, 2013.

[65] M. G¨ol and A. Abur, “Lav based robust state estimation for systems measured by PMUs,”

IEEE Trans. Smart Grid, vol. 5, no. 4, pp. 1808–1814, 2014.

[66] N. Golrezaei, A. F. Molisch, A. G. Dimakis, and G. Caire, “Femtocaching and device-to-

device collaboration: A new architecture for wireless video distribution,” IEEE Communi-

cations Magazine, vol. 51, no. 4, pp. 142–149, Apr. 2013.

[67] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning. MIT Press, 2016,

vol. 1.

[68] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial

examples,” arXiv:1412.6572, 2014.

[69] S. Gopalakrishnan, Z. Marzi, U. Madhow, and R. Pedarsani, “Combating adversarial

attacks using sparse representations,” arXiv:1803.03880, 2018.

[70] M. Grant and S. Boyd, “CVX: Matlab software for disciplined convex programming,

version 2.1,” 2014.

[71] S. Gu and L. Rigazio, “Towards deep neural network architectures robust to adversarial

examples,” arXiv:1412.5068, 2014.

[72] C. Guo, M. Rana, M. Cisse, and L. Van Der Maaten, “Countering adversarial images

using input transformations,” arXiv:1711.00117, 2017.

148

[73] Z. Guo, W. Chen, Y.-F. Liu, Y. Xu, and Z.-L. Zhang, “Joint switch upgrade and controller

deployment in hybrid software-deﬁned networks,” IEEE J. Sel. Areas Commun., vol. 37,

no. 5, pp. 1012–1028, Mar. 2019.

[74] M. Hardt, E. Price, N. Srebro et al., “Equality of opportunity in supervised learning,” in

Adv. Neural info. Process. Syst., 2016, pp. 3315–3323.

[75] Y. He, F. R. Yu, N. Zhao, V. C. M. Leung, and H. Yin, “Software-deﬁned networks with

mobile edge computing and caching for smart cities: A big data deep reinforcement

learning approach,” IEEE Commun. Mag., vol. 55, no. 12, pp. 31–37, Dec. 2017.

[76] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. M. Leung, and Y. Zhang, “Deep-

reinforcement-learning-based optimization for cache-enabled opportunistic interference

alignment wireless networks,” IEEE Trans. Vehicular Tech., vol. 66, no. 11, pp. 10 433–

10 445, Nov. 2017.

[77] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and computing for connected

vehicles: A deep reinforcement learning approach,” IEEE Trans. Veh. Technol., vol. 67,

no. 1, pp. 44–55, Jan. 2018.

[78] X. Hu, H. Hu, S. Verma, and Z.-L. Zhang, “Physics-guided deep neural networks for

power ﬂow analysis,” arXiv:2002.00097, 2020.

[79] Z. Hu and L. J. Hong, “Kullback-leibler divergence constrained distributionally robust

optimization,” Available at Optimization Online, 2013.

[80] A. Ipakchi and F. Albuyeh, “Grid of the future,” IEEE Power Energy Mag., vol. 7, no. 2,

pp. 52–62, Feb. 2009.

[81] R. Jabr and B. Pal, “Iteratively re-weighted least absolute value method for state estima-

tion,” IET Gener., Transmiss., Distrib., vol. 150, no. 4, pp. 385–391, Jul. 2003.

[82] M. Ji, G. Caire, and A. F. Molisch, “Fundamental limits of caching in wireless D2D

networks,” IEEE Transactions on Information Theory, vol. 62, no. 2, pp. 849–869, Feb.

2016.

149

[83] ——, “Wireless device-to-device caching networks: Basic principles and system perfor-

mance,” IEEE Journal on Selected Areas in Communications, vol. 34, no. 1, pp. 176–189,

Jan. 2016.

[84] W. Jin, Y. Li, H. Xu, Y. Wang, and J. Tang, “Adversarial attacks and defenses on graphs:

A review and empirical study,” arXiv:2003.00653, 2020.

[85] F. Kamiran and T. Calders, “Classifying without discriminating,” in Intl. Conf. Comput.

Cont. Commun.

IEEE, 2009, pp. 1–6.

[86] F. Kamiran, T. Calders, and M. Pechenizkiy, “Discrimination aware decision tree learning,”

in IEEE Intl. Conf. Data Mining.

IEEE, 2010, pp. 869–874.

[87] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, “Fairness-aware classiﬁer with prej-

udice remover regularizer,” in Joint European Conf. Machine Learn. Knwl. Discov. Db.

Springer, 2012, pp. 35–50.

[88] V. Kekatos, L. Zhang, G. B. Giannakis, and R. Baldick, “Voltage regulation algorithms

for multiphase power distribution grids,” IEEE Trans. Power Syst., vol. 31, no. 5, pp.

3913–3923, Sep. 2016.

[89] V. Kekatos, G. Wang, A. J. Conejo, and G. B. Giannakis, “Stochastic reactive power

management in microgrids with renewables,” IEEE Trans. Power Syst., vol. 30, no. 6, pp.

3386–3395, Dec. 2015.

[90] V. Kekatos, L. Zhang, G. B. Giannakis, and R. Baldick, “Voltage regulation algorithms

for multiphase power distribution grids,” IEEE Trans. Power Syst., vol. 31, no. 5, pp.

3913–3923, Sep. 2016.

[91] H. Kim, J. Park, M. Bennis, S.-L. Kim, and M. Debbah, “Ultra-dense edge caching under

spatio-temporal demand and network dynamics,” arXiv preprint arXiv:1703.01038, 2017.

[92] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” Intl. Conf. Learn.

Rep., May 2015.

[93] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph convolutional

networks,” Intl. Conf. Lear. Rep., 2016.

150

[94] E. D. Kolaczyk and G. Cs´ardi, Statistical analysis of network data with R. Springer,

2014, vol. 65.

[95] N. Konstantinov and C. Lampert, “Robust learning from untrusted sources,” Intl. Conf.

Mach. Learn., June 2019.

[96] P. Kundur, N. J. Balu, and M. G. Lauby, Power System Stability and Control. Duisburg,

Germany: McGraw-hill New York, May 1994.

[97] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,”

Intl. Conf. Learn. Rep., Vancouver, BC, Canada, Apr.

[98] ——, “Adversarial machine learning at scale,” Intl. Conf. Learn. Rep., Apr. 2017.

[99] J. Kwak, G. Paschos, and G. Iosiﬁdis, “Dynamic cache rental and content caching in

elastic wireless CDNs,” in Proc. Intl. Symp. Modeling Opt. Mobile, Ad Hoc, Wireless

Netw., Shanghai, China, May 2018, pp. 1–8.

[100] Y. Lan, H. Zhu, and X. Guan, “Fast nonconvex SDP solvers for large-scale power system

state estimation,” IEEE Trans. Power Syst., 2020.

[101] M. Leconte, G. Paschos, L. Gkatzikis, M. Draief, S. Vassilaras, and S. Chouvardas, “Plac-

ing dynamic content in caches with small population,” in Intl. Conf. Comput. Commun.,

San Francisco, USA, April 10-15, 2016, pp. 1–9.

[102] ——, “Placing dynamic content in caches with small population,” in Intl. Conf. on

Computer Communications, San Francisco, USA, Apr. 2016, pp. 1–9.

[103] B. Li, A. Sadeghi, and G. Giannakis, “Heavy ball momentum for conditional gradient,”

Advances in Neural Information Processing Systems, vol. 34, 2021.

[104] S. Li, J. Xu, M. van der Schaar, and W. Li, “Trend-aware video caching through online

learning,” IEEE Transactions on Multimedia, vol. 18, no. 12, pp. 2503–2516, Dec. 2016.

[105] S. Li, A. Pandey, and L. Pileggi, “A WLAV-based robust hybrid state estimation using

circuit-theoretic approach,” arXiv:2011.06021, 2020.

[106] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning: Challenges, methods,

and future directions,” IEEE Signal Process. Mag., vol. 37, no. 3, pp. 50–60, May 2020.

151

[107] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated

optimization in heterogeneous networks,” arXiv:1812.06127, 2018.

[108] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang, D. Niyato, and

C. Miao, “Federated learning in mobile edge networks: A comprehensive survey,” IEEE

Commun. Surv. Tut., Apr. 8 2020.

[109] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion

in federated learning,” arXiv:2006.07242, 2020.

[110] T. Lin, C. Jin, and M. I. Jordan, “On gradient descent ascent for nonconvex-concave

minimax problems,” arXiv:1906.00331, 2019.

[111] W. Lin, R. Thomas, and E. Bitar, “Real-time voltage regulation in distribution systems via

decentralized PV inverter control,” in Proc. Annual Hawaii Intl. Conf. System Sciences,

Waikoloa Village, Hawaii, Jan. 2-6, 2018.

[112] S. G. Lingala and M. Jacob, “Blind compressive sensing dynamic MRI,” IEEE trans. Med.

Imag., vol. 32, no. 6, pp. 1132–1145, Mar. 2013.

[113] J. Liu, B. Bai, J. Zhang, and K. B. Letaief, “Content caching at the wireless network edge:

A distributed algorithm via belief propagation,” in Intl. Conf. on Communications, Kuala

Lumpur, Malaysia, May 2016, pp. 1–6.

[114] L. M. Lopez-Ramos, A. G. Marques, and J. Ramos, “Jointly optimal sensing and resource

allocation for multiuser interweave cognitive radios,” IEEE Trans. Wireless Commun.,

vol. 13, no. 11, pp. 5954–5967, Nov. 2014.

[115] S. H. Low, “Convex relaxation of optimal power ﬂow—Part II: Exactness,” IEEE Trans.

Control Netw. Syst., vol. 1, no. 2, pp. 177–189, May 2014.

[116] J. Lu, T. Issaranon, and D. Forsyth, “Safetynet: Detecting and rejecting adversarial

examples robustly,” in Proceedings of the IEEE International Conference on Computer

Vision, 2017, pp. 446–454.

[117] Q. Lu, V. N. Ioannidis, and G. B. Giannakis, “Semi-supervised learning of processes over

multi-relational graphs,” in IEEE Intl. Conf. Acoustics, Speech Signal Proces., 2020, pp.

5560–5564.

152

[118] R. Lu, S. H. Hong, and M. Yu, “Demand response for home energy management using

reinforcement learning and artiﬁcial neural network,” IEEE Trans. Smart Grid, Apr. 2019.

[119] X. Luo and G. B. Giannakis, “Energy-constrained optimal quantization for wireless sensor

networks,” EURASIP J. Adv. Signal Process, vol. 2008, pp. 73:1–73:12, Jan. 2008.

[120] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y. Liang, and D. I. Kim,

“Applications of deep reinforcement learning in communications and networking: A

survey,” IEEE Commun. Surv. Tutor., pp. 1–1, to appear 2019.

[121] M. A. Maddah-Ali and U. Niesen, “Fundamental limits of caching,” IEEE Transactions

on Information Theory, vol. 60, no. 5, pp. 2856–2867, May 2014.

[122] ——, “Fundamental limits of caching,” IEEE Trans. Inf. Theory, vol. 60, no. 5, pp.

2856–2867, May 2014.

[123] ——, “Decentralized coded caching attains order-optimal memory-rate tradeoff,”

IEEE/ACM Transactions on Networking, vol. 23, no. 4, pp. 1029–1040, Aug. 2015.

[124] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning

models resistant to adversarial attacks,” arXiv:1706.06083, 2017.

[125] S. Magn´usson, G. Qu, C. Fischione, and N. Li, “Voltage control using limited communi-

cation,” IEEE Trans. Control Netw. Syst., vol. 6, no. 3, pp. 993–1003, Sep. 2019.

[126] S. Mahadevan, “Learning representation and control in markov decision processes: New

frontiers,” Foundations and Trends in Machine Learning, vol. 1, no. 4, pp. 403–565, 2009.

[127] E. Manitsas, R. Singh, B. C. Pal, and G. Strbac, “Distribution system state estimation

using an artiﬁcial neural network approach for pseudo measurement modeling,” IEEE

Trans. Power Syst., vol. 27, no. 4, pp. 1888–1896, Nov. 2012.

[128] A. G. Marques, L. M. Lopez-Ramos, G. B. Giannakis, J. Ramos, and A. J. Caama˜no,

“Optimal cross-layer resource allocation in cellular networks using channel- and queue-

state information,” IEEE Trans. Veh. Technol., vol. 61, no. 6, pp. 2789–2807, July 2012.

[129] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-

efﬁcientlearning of deep networks from decentralized data,” in Proc. Intl. Conf. Artif.

Intell. Stat., vol. 54, Fort Lauderdale, FL, USA, 20–22 Apr. 2017, pp. 1273–1282.

153

[130] H. M. Merrill and F. C. Schweppe, “Bad data suppression in power system static state

estimation,” IEEE Trans. Power App. Syst., vol. PAS-90, no. 6, pp. 2718–2725, Nov. 1971.

[131] K. R. Mestav, J. Luengo-Rozas, and L. Tong, “Bayesian state estimation for unobservable

distribution systems via deep learning,” IEEE Trans. Power Syst., vol. 34, no. 6, pp.

4910–4920, May 2019.

[132] D. J. Miller, Z. Xiang, and G. Kesidis, “Adversarial learning targeting deep neural network

classiﬁcation: A comprehensive review of defenses against attacks,” Proc. IEEE, pp. 1–32,

2020 (to appear).

[133] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial training: a regu-

larization method for supervised and semi-supervised learning,” IEEE transactions on

pattern analysis and machine intelligence, vol. 41, no. 8, pp. 1979–1993, 2018.

[134] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,

M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep

reinforcement learning,” Nature, vol. 518, no. 7540, p. 529, Feb. 2015.

[135] ——, “Human-level control through deep reinforcement learning,” Nature, vol. 518, no.

7540, p. 529, Feb. 2015.

[136] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal adversarial

perturbations,” in Proceedings of the IEEE conference on computer vision and pattern

recognition, 2017, pp. 1765–1773.

[137] E. J. Msechu, S. I. Roumeliotis, A. Ribeiro, and G. B. Giannakis, “Decentralized quantized

kalman ﬁltering with scalable communication cost,” IEEE Trans. Signal Process., vol. 56,

no. 8, pp. 3727–3741, 2008.

[138] S. M¨uller, O. Atan, M. van der Schaar, and A. Klein, “Context-aware proactive content

caching with service differentiation in wireless networks,” IEEE Transactions on Wireless

Communications, vol. 16, no. 2, pp. 1024–1036, Feb. 2017.

[139] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning for dynamic spec-

trum access in multichannel wireless networks,” in Global Commun. Conf., Singapore,

Dec. 4-8, 2017, pp. 1–7.

154

[140] E. Nygren, R. K. Sitaraman, and J. Sun, “The Akamai network: A platform for high-

performance Internet applications,” ACM SIGOPS Operating Syst. Rev., vol. 44, no. 3, pp.

2–19, 2010.

[141] J. Ostrometzky, K. Berestizshevsky, A. Bernstein, and G. Zussman, “Physics-informed

deep neural network method for limited observability state estimation,” arXiv:1910.06401,

2019.

[142] D. P. Palomar and M. Chiang, “A tutorial on decomposition methods for network utility

maximization,” IEEE J. Sel. Areas Commun., vol. 24, no. 8, pp. 1439–1451, Aug. 2006.

[143] C. H. Papadimitriou and J. N. Tsitsiklis, “The complexity of Markov decision processes,”

Math. Oper. Res., vol. 12, no. 3, pp. 441–450, 1987.

[144] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical

black-box attacks against machine learning,” in Proc. Conf. Comput. Commun. Sec., 2017,

pp. 506–519.

[145] G. Paschos, E. Bastug, I. Land, G. Caire, and M. Debbah, “Wireless caching: Technical

misconceptions and business barriers,” IEEE Communications Magazine, vol. 54, no. 8,

pp. 16–22, Aug. 2016.

[146] G. Paschos, A. Destounis, L. Vigneri, and G. Iosiﬁdis, “Learning to cache with no regrets,”

in Proc. of INFOCOM Conf., Paris, France, April 2019, pp. 545–549.

[147] R. Pedarsani, M. A. Maddah-Ali, and U. Niesen, “Online coded caching,” IEEE/ACM

Transactions on Networking, vol. 24, no. 2, pp. 836–845, Apr. 2016.

[148] ——, “Online coded caching,” IEEE/ACM Trans. Netw., vol. 24, no. 2, pp. 836–845, Apr.

2016.

[149] K. Poularakis, G. Iosiﬁdis, A. Argyriou, and L. Tassiulas, “Video delivery over heteroge-

neous cellular networks: Optimizing cost and performance,” in Intl. Conf. on Computer

Communications, Toronto, Canada, Apr. 2014, pp. 1078–1086.

[150] L. Pu, L. Jiao, X. Chen, L. Wang, Q. Xie, and J. Xu, “Online resource allocation, content

placement and request routing for cost-efﬁcient edge caching in cloud radio access

networks,” IEEE J. Sel. Areas Commun., vol. 36, no. 8, pp. 1751–1767, Aug 2018.

155

[151] B. A. Robbins, H. Zhu, and A. D. Dom´ınguez-Garc´ıa, “Optimal tap setting of voltage

regulation transformers in unbalanced distribution systems,” IEEE Trans. Power Syst.,

vol. 31, no. 1, pp. 256–267, Feb. 2016.

[152] R. T. Rockafellar and R. J.-B. Wets, Variational analysis. Springer Science & Business

Media, 2009, vol. 317.

[153] A. Romei and S. Ruggieri, “A multidisciplinary survey on discrimination analysis,” The

Knowledge Engineering Review, vol. 29, no. 5, pp. 582–638, 2014.

[154] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based noise removal

algorithms,” Physica D: Nonlinear Phenomena, vol. 60, no. 1-4, pp. 259–268, Nov. 1992.

[155] S. J. Russell and P. Norvig, Artiﬁcial intelligence: a modern approach. Upper Saddle

River, NJ, USA,: Prentice-Hall, 2010.

[156] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach. Prentice-Hall,

Upper Saddle River, NJ, USA, 2010.

[157] A. Sadeghi and M. Ma, “Distributionally robust semi-supervised learning over graphs,” in

Proc. of Intl. Conf. on Learning Represenations, Workshop on Responsible AI, 2021.

[158] A. Sadeghi, F. Sheikholeslami, and G. B. Giannakis, “Optimal and scalable caching for

5G using reinforcement learning of space-time popularities,” IEEE J. Sel. Topics Signal

Process., vol. 12, no. 1, pp. 180–190, Feb. 2018.

[159] A. Sadeghi, G. Wang, and G. B. Giannakis, “Deep reinforcement learning for adaptive

caching in hierarchical content delivery networks,” IIEEE Trans. Cogn. Commun. Netw.,

to appear, 2019.

[160] A. Sadeghi, S. Ghavami, and G. B. Giannakis, “Performance bounds of estimators in

molecular communications under structural constraints,” in Proceedings of the 4th ACM

International Conference on Nanoscale Computing and Communication, 2017, pp. 1–6.

[161] A. Sadeghi, G. B. Giannakis, G. Wang, and F. Sheikholeslami, “Reinforcement learning

for caching with space-time popularity dynamics,” arXiv preprint arXiv:2005.09155,

2020.

156

[162] A. Sadeghi, A. G. Marques, and G. B. Giannakis, “Distributed network caching via dy-

namic programming,” in ICASSP 2019-2019 IEEE International Conference on Acoustics,

Speech and Signal Processing (ICASSP).

IEEE, 2019, pp. 4574–4578.

[163] A. Sadeghi, F. Sheikholeslami, and G. B. Giannakis, “Optimal dynamic caching via

reinforcement learning,” in Signal Processing Advances in Wireless Communications,

2018.

[164] A. Sadeghi, F. Sheikholeslami, A. G. Marques, and G. B. Giannakis, “Reinforcement

learning for adaptive caching with dynamic storage pricing,” IEEE J. Sel. Topics Commun.;

see arXiv:1812.08593, 2019.

[165] ——, “Reinforcement learning for adaptive caching with dynamic storage pricing,” IEEE

Journal on Selected Areas in Communications, vol. 37, no. 10, pp. 2267–2281, 2019.

[166] A. Sadeghi, F. Sheikholeslami, A. G. Matrques, and G. B. Giannakis, “Reinforcement

learning for 5g caching with dynamic cost,” in 2018 IEEE International Conference on

Acoustics, Speech and Signal Processing (ICASSP).

IEEE, 2018, pp. 6653–6657.

[167] A. Sadeghi, G. Wang, and G. B. Giannakis, “Deep reinforcement learning for adaptive

caching in hierarchical content delivery networks,” IEEE Transactions on Cognitive

Communications and Networking, vol. 5, no. 4, pp. 1024–1033, 2019.

[168] ——, “Hierarchical caching via deep reinforcement learning,” in ICASSP 2020-2020

IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).

IEEE, 2020, pp. 3532–3536.

[169] A. Sadeghi, G. Wang, M. Ma, and G. B. Giannakis, “Learning while respecting pri-

vacy and robustness to distributional uncertainties and adversarial data,” arXiv preprint

arXiv:2007.03724, 2020.

[170] M. Salem, L. Talat, and H. Soliman, “Voltage control by tap-changing transformers for a

radial distribution network,” IEE Proceedings-Generation, Transmission and Distribution,

vol. 144, no. 6, pp. 517–520, Nov. 1997.

157

[171] J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price, and D. Rueckert, “A deep cascade of

convolutional neural networks for dynamic MR image reconstruction,” IEEE Trans. Med.

Imag., vol. 37, no. 2, pp. 491–503, Oct. 2017.

[172] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry, “Adversarially robust

generalization requires more data,” in Advances in Neural Information Processing Systems,

2018, pp. 5014–5026.

[173] A. Sengupta, S. Amuru, R. Tandon, R. M. Buehrer, and T. C. Clancy, “Learning distributed

caching strategies in small cell networks,” in Proc. Intl. Symp. on Wireless Communications

Systems, Barcelona, Spain, Aug. 2014, pp. 917–921.

[174] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to a crime: Real and

stealthy attacks on state-of-the-art face recognition,” in ACM SIGSAC Conf. on Comput.

Commun. Security, 2016, pp. 1528–1540.

[175] F. Sheikholeslami, S. Jain, and G. B. Giannakis, “Minimum uncertainty based detection

of adversaries in deep neural networks,” arXiv:1904.02841, 2019.

[176] N. Shlezinger, M. Chen, Y. C. Eldar, H. V. Poor, and S. Cui, “UVeQFed: Universal vector

quantization for federated learning,” arXiv:2006.03262, 2020.

[177] S. Shukla, O. Bhardwaj, A. A. Abouzeid, T. Salonidis, and T. He, “Proactive retention-

aware caching with multi-path routing for wireless edge networks,” IEEE J. Sel. Areas

Commun., vol. 36, no. 6, pp. 1286–1299, Jun. 2018.

[178] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, “The emerging

ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to networks

and other irregular domains,” IEEE Signal Proces. Mag., vol. 30, pp. 83–98, 2013.

[179] A. Sinha, H. Namkoong, and R. V. J. Duchi, “Certifying some distributional robustness

with principled adversarial training,” Intl. Conf. Learn. Rep., 2017.

[180] A. Sinha, H. Namkoong, and J. Duchi, “Certifying some distributional robustness with

principled adversarial training,” Intl. Conf. Learn. Rep., Vancouver, BC, Canada, May

2018.

158

[181] S. O. Somuyiwa, A. Gy¨orgy, and D. G¨und¨uz, “A reinforcement-learning approach to

proactive caching in wireless networks,” IEEE J. Sel. Areas Commun., vol. 36, no. 6, pp.

1331–1344, June 2018.

[182] W. Su, J. Wang, and J. Roh, “Stochastic energy scheduling in microgrids with intermittent

renewable energy resources,” IEEE Trans. Smart Grid, vol. 5, no. 4, pp. 1876–1883, July

2014.

[183] Y. Sun, M. Peng, and S. Mao, “Deep reinforcement learning-based mode selection and

resource management for green fog radio access networks,” IEEE Internet Things J.,

vol. 6, no. 2, pp. 1960–1971, Apr. 2019.

[184] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. Cambridge, MA,

USA: MIT Press, 2016.

[185] ——, Reinforcement Learning: An Introduction. Cambridge, MA: MIT press, 2018.

[186] E. Tolstaya, F. Gama, J. Paulos, G. Pappas, V. Kumar, and A. Ribeiro, “Learning decen-

tralized controllers for robot swarms with graph neural networks,” in Conf. Robot Learn.,

2020, pp. 671–682.

[187] L. Tong, Y. Li, and W. Gao, “A hierarchical edge cloud architecture for mobile computing,”

in IEEE Intl. Conf. on Comput. Commun.

IEEE, 2016, pp. 1–9.

[188] S. Traverso, M. Ahmed, M. Garetto, P. Giaccone, E. Leonardi, and S. Niccolini, “Temporal

locality in today’s content caching: Why it matters and how to model it,” ACM SIGCOMM

Comput. Commun. Rev., vol. 43, no. 5, pp. 5–12, Nov. 2013.

[189] D. A. Tziouvaras, P. McLaren, G. Alexander, D. Dawson, J. Esztergalyos, C. Fromen,

M. Glinkowski, I. Hasenwinkle, M. Kezunovic, L. Kojovic et al., “Mathematical models

for current, voltage, and coupling capacitor voltage transformers,” IEEE Trans. Power

Del., vol. 15, no. 1, pp. 62–72, Jan. 2000.

[190] C. Villani, Optimal Transport: Old and New. Berlin, HR: SSBM, 2008, vol. 338.

[191] G. Wang, G. B. Giannakis, and J. Chen, “Robust and scalable power system state estima-

tion via composite optimization,” IEEE Trans. Smart Grid, vol. 10, no. 6, pp. 6137–6147,

Nov. 2019.

159

[192] G. Wang, V. Kekatos, A. J. Conejo, and G. B. Giannakis, “Ergodic energy management

leveraging resource variability in distribution grids,” IEEE Trans. Power Syst., vol. 31,

no. 6, pp. 4765–4775, Nov. 2016.

[193] G. Wang, G. B. Giannakis, J. Chen, and J. Sun, “Distribution system state estimation: An

overview of recent developments,” Front. Inform. Technol. Electron. Eng., vol. 20, no. 1,

pp. 4–17, Jan. 2019.

[194] ——, “Distribution system state estimation: An overview of recent developments,” Front.

Inform. Technol. Electron. Eng., vol. 20, no. 1, pp. 4–17, Jan. 2019.

[195] G. Wang, V. Kekatos, A. J. Conejo, and G. B. Giannakis, “Ergodic energy management

leveraging resource variability in distribution grids,” IEEE Trans. Power Syst., vol. 31,

no. 6, pp. 4765–4775, Nov. 2016.

[196] W. Wang, D. Niyato, P. Wang, and A. Leshem, “Decentralized caching for content delivery

based on blockchain: A game theoretic perspective,” arXiv:1801.07604, 2018.

[197] X. Wang, M. Chen, T. Taleb, A. Ksentini, and V. C. M. Leung, “Cache in the air: Ex-

ploiting content caching and delivery techniques for 5G systems,” IEEE Communications

Magazine, vol. 52, no. 2, pp. 131–139, Feb. 2014.

[198] Z. Wang, L. Li, Y. Xu, H. Tian, and S. Cui, “Handover control in wireless systems via

asynchronous multiuser deep reinforcement learning,” IEEE Internet Things J., vol. 5,

no. 6, pp. 4296–4307, Dec. 2018.

[199] Z. Wang, Z. Yu, Q. Ling, D. Berberidis, and G. B. Giannakis, “Decentralized rls with

data-adaptive censoring for regressions over large-scale networks,” IEEE Trans. Signal

Process., vol. 66, no. 6, pp. 1634–1648, Mar. 2018.

[200] C. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, King’s College, Cam-

bridge, 1989.

[201] C. Watkins and P. Dayan, “Q-learning,” Mach. learn., vol. 8, no. 3-4, pp. 279–292, May

1992.

[202] K. Wei, J. Li, M. Ding, C. Ma, H. Su, B. Zhang, and H. V. Poor, “Performance analysis

and optimization in privacy-preserving federated learning,” arXiv:2003.00229, 2020.

160

[203] W. Wiesemann, D. Kuhn, and M. Sim, “Distributionally robust convex optimization,”

Operations Research, vol. 62, no. 6, pp. 1358–1376, 2014.

[204] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro, “Learning non-

discriminatory predictors,” arXiv:1702.06081, 2017.

[205] G. Wu, J. Sun, and L. Xiong, “Optimal switching attacks and countermeasures in cyber-

physical systems,” IEEE Trans. Syst., Man, Cybern.: Syst., vol. 50, no. 5, pp. 1–10, Jun.

2020.

[206] G. Wu, G. Wang, J. Sun, and J. Chen, “Optimal partial feedback attacks in cyber-physical

power systems,” IEEE Trans. Autom. Control, pp. 1–8, 2020, to be published; DOI:

10.1109/TAC.2020.2981915.

[207] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,

Q. Gao, K. Macherey et al., “Google’s neural machine translation system: Bridging the

gap between human and machine translation,” arXiv:1609.08144, 2016.

[208] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on

graph neural networks,” IEEE Trans. Neural Netw. Learn. Syst., 2020.

[209] H. Xu, A. D. Dom´ınguez-Garc´ıa, and P. W. Sauer, “Optimal tap setting of voltage

regulation transformers using batch reinforcement learning,” arXiv:1807.10997v2, 2018.

[210] Y. Xu, R. Jin, and T. Yang, “Non-asymptotic analysis of stochastic methods for non-

smooth non-convex regularized problems,” in Adv. Neural Inf. Process. Syst., Dec. 2019,

pp. 2626–2636.

[211] Z. Yan and Y. Xu, “Data-driven load frequency control for stochastic power systems: A

deep reinforcement learning method with continuous action search,” IEEE Trans. Power

Syst., vol. 34, no. 2, pp. 1653–1656, Nov. 2018.

[212] Q. Yang, A. Sadeghi, and G. Wang, “Data-driven priors for robust psse via gauss-newton

unrolled neural networks,” IEEE Journal on Emerging and Selected Topics in Circuits

and Systems, pp. 1–1, 2022.

[213] Q. Yang, A. Sadeghi, G. Wang, G. B. Giannakis, and J. Sun, “A statistical learning

approach to reactive power control in distribution systems,” arXiv:1910.13938, 2019.

161

[214] ——, “Deep policy gradient for reactive power control in distribution systems,” in 2020

IEEE International Conference on Communications, Control, and Computing Technolo-

gies for Smart Grids (SmartGridComm).

IEEE, 2020, pp. 1–6.

[215] ——, “Power system state estimation using gauss-newton unrolled neural networks with

trainable priors,” in 2020 IEEE International Conference on Communications, Control,

and Computing Technologies for Smart Grids (SmartGridComm).

IEEE, 2020, pp. 1–6.

[216] ——, “Power system state estimation using gauss-newton unrolled neural networks with

trainable priors,” in 2020 IEEE International Conference on Communications, Control,

and Computing Technologies for Smart Grids (SmartGridComm).

IEEE, 2020, pp. 1–6.

[217] Q. Yang, A. Sadeghi, G. Wang, and J. Sun, “Learning two-layer relu networks is nearly

as easy as learning linear classiﬁers on separable data,” IEEE Transactions on Signal

Processing, vol. 69, pp. 4416–4427, 2021.

[218] Q. Yang, G. Wang, A. Sadeghi, G. B. Giannakis, and J. Sun, “Two-timescale voltage

control in distribution grids using deep reinforcement learning,” IEEE Transactions on

Smart Grid, vol. 11, no. 3, pp. 2313–2323, 2019.

[219] ——, “Two-timescale voltage regulation in distribution grids using deep reinforcement

learning,” in 2019 IEEE International Conference on Communications, Control, and

Computing Technologies for Smart Grids (SmartGridComm).

IEEE, 2019, pp. 1–6.

[220] ——, “Two-timescale voltage regulation in distribution grids using deep reinforcement

learning,” in Proc. of SmartGridComm, Beijing, CN, Oct. 21-23, 2019, pp. 1–6.

[221] C. Yu, J. Lan, Z. Guo, and Y. Hu, “DROM: Optimizing the routing in software-deﬁned

networks with deep reinforcement learning,” IEEE Access, vol. 6, pp. 64 533–64 539, Oct.

2018.

[222] Y. Yu, T. Wang, and S. C. Liew, “Deep-reinforcement learning multiple access for

heterogeneous wireless networks,” in Intl. Conf. on Comm., Kansas City, USA, May 20 -

24, 2018, pp. 1–7.

[223] M. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi, “Fairness constraints:

Mechanisms for fair classiﬁcation,” arXiv preprint arXiv:1507.05259, 2015.

162

[224] A. S. Zamzam and N. D. Sidiropoulos, “Physics-aware neural networks for distribution

system state estimation,” arXiv:1903.09669, 2019.

[225] A. S. Zamzam, B. Yang, and N. D. Sidiropoulos, “Energy storage management via deep

Q-networks,” in Proc. of PESGM, Atlanta, GA, Aug. 4-8, 2019, pp. 1–7.

[226] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, “Learning fair representations,”

in Intl. Conf. Mach. Learn., 2013, pp. 325–333.

[227] B. Zhang, A. Dominguez-Garcia, and D. Tse, “A local control approach to voltage regula-

tion in distribution networks,” in Proc. North American Power Symposium, Manhattan,

KS, 2013.

[228] L. Zhang, G. Wang, and G. B. Giannakis, “Real-time power system state estimation and

forecasting via deep unrolled neural networks,” IEEE Trans. on Signal Proces., vol. 67,

no. 15, pp. 4069–4077, 2019.

[229] ——, “Real-time power system state estimation and forecasting via deep unrolled neural

networks,” IEEE Trans. Signal Process., vol. 67, no. 15, pp. 4069–4077, Aug. 2019.

[230] ——, “Real-time power system state estimation and forecasting via deep unrolled neural

networks,” IEEE Trans. Signal Process., vol. 67, no. 15, pp. 4069–4077, Aug. 2019.

[231] Y. Zhang, M. Hong, E. Dall’Anese, S. V. Dhople, and Z. Xu, “Distributed controllers

seeking AC optimal power ﬂow solutions using ADMM,” IEEE Trans. Smart Grid, vol. 9,

no. 5, pp. 4525–4537, Sept. 2018.

[232] C. Zhong, M. C. Gursoy, and S. Velipasalar, “A deep reinforcement learning-based

framework for content caching,” in Conf. on Info. Sciences and Syst., Princeton, NJ,

March 21–23, 2018, pp. 1–6.

[233] F. Zhou, Q. Yang, T. Zhong, D. Chen, and N. Zhang, “Variational graph neural networks

for road trafﬁc prediction in intelligent transportation systems,” IEEE Trans. Ind. Inf., pp.

2802–2812, 2020.

[234] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, “Graph neural

networks: A review of methods and applications,” arXiv:1812.08434, 2018.

163

[235] H. Zhu, Y. Cao, W. Wang, T. Jiang, and S. Jin, “Deep reinforcement learning for mobile

edge caching: Review, new features, and open issues,” IEEE Netw., vol. 32, no. 6, pp.

50–57, Nov. 2018.

[236] H. Zhu and G. B. Giannakis, “Power system nonlinear state estimation using distributed

semideﬁnite programming,” IEEE J. Sel. Topics Signal Process., vol. 8, no. 6, pp. 1039–

1050, Jun. 2014.

[237] H. Zhu and H. J. Liu, “Fast local voltage control under limited reactive power: Optimality

and stability analysis,” IEEE Trans. Power Syst., vol. 31, no. 5, pp. 3794–3803, Dec. 2016.

[238] R. D. Zimmerman, C. E. Murillo-S´anchez, and R. J. Thomas, “Matpower: Steady-state

operations, planning, and analysis tools for power systems research and education,” IEEE

Trans. Power syst., vol. 26, pp. 12–19, 2010.

[239] D. Z¨ugner, O. Borchert, A. Akbarnejad, and S. Guennemann, “Adversarial attacks on

graph neural networks: Perturbations and their patterns,” ACM Trans. Knwl. Discov. from

Data, vol. 14, pp. 1–31, 2020.

Appendix A

Proofs for Chapter 2

A.0.1 Proof of Lemma 1

Since function ζ (cid:55)→ ψ(¯θ, ζ; z) is λ-strongly concave, then ζ∗(¯θ) = supζ∈Z ψ(¯θ, ζ; z) is unique.
In addition, the ﬁrst-order optimality condition gives (cid:104)∇ζψ(¯θ, ζ∗(¯θ); z), ζ − ζ∗(¯θ)(cid:105) ≤ 0. Let
us deﬁne ζ1

∗ = ζ∗(¯θ2), and use the strong concavity for any ¯θ1 and ¯θ2, to write

∗ = ζ∗(¯θ1), ζ2

ψ(¯θ2, ζ2

∗ ; z) ≤ ψ(¯θ2, ζ1

∗ ; z) + (cid:104)∇ζψ(¯θ2, ζ1

∗ ; z), ζ2

∗ − ζ1
∗ (cid:105)

−

λ
2

and

(cid:107)ζ2

∗ − ζ1

∗ (cid:107)2

ψ(¯θ2, ζ1

∗ ; z) ≤ ψ(¯θ2, ζ2

∗ ; z) + (cid:104)∇ζψ(¯θ2, ζ2

∗ ; z), ζ1

∗ − ζ2
∗ (cid:105)

∗ − ζ1

−

(cid:107)ζ2

λ
2
≤ ψ(¯θ2, ζ2

∗ (cid:107)2
λ
2

∗ ; z) −

(cid:107)ζ2

∗ − ζ1

∗ (cid:107)2

(A.1)

(A.2)

where the last inequality is a consequence of the ﬁrst-order optimality condition. Summing

(A.1) and (A.2), we ﬁnd that

λ(cid:107)ζ2

∗ − ζ1

∗ (cid:107)2 ≤ (cid:104)∇ζψ(¯θ2, ζ1
≤ (cid:104)∇ζψ(¯θ2, ζ1

∗ ; z), ζ2
∗ ; z), ζ2

− (cid:104)∇ζψ(¯θ1, ζ1

∗ ; z), ζ2

∗ − ζ1
∗ (cid:105)
∗ − ζ1
∗ (cid:105)
∗ − ζ1
∗ (cid:105)

164

(A.3)

= (cid:104)∇ζψ(¯θ2, ζ1

∗ ; z) − ∇ζψ(¯θ1, ζ1

∗ ; z), ζ2

∗ − ζ1

∗ (cid:105).

And using H¨older’s inequality, we obtain that

λ(cid:107)ζ2

∗ − ζ1

∗ (cid:107)2 ≤ (cid:107)∇ζψ(¯θ2, ζ1
∗ ; z) − ∇ζψ(¯θ1, ζ1
∗ − ζ1
× (cid:107)ζ2
∗ (cid:107)

∗ ; z)(cid:107)(cid:63)

from which we deduce

(cid:107)ζ2

∗ − ζ1

∗ (cid:107) ≤

1
λ

(cid:107)∇ζψ(¯θ2, ζ1

∗ ; z) − ∇ζψ(¯θ1, ζ1

∗ ; z)(cid:107)(cid:63).

Using ψ(¯θ, ζ; z) := (cid:96)(θ; ζ) + γ(ρ − c(z, ζ)), we have that

(cid:107)∇ζψ(¯θ2, ζ1
= (cid:107)∇ζ(cid:96)(θ2; ζ1

∗ ; z) − ∇ζψ(¯θ1, ζ1
∗ ) − ∇ζ(cid:96)(θ1; ζ1
∗ )

∗ ; z)(cid:107)(cid:63)

+ γ1∇ζc(z, ζ1

≤ (cid:107)∇ζ(cid:96)(θ2; ζ1

+ (cid:107)γ1∇ζc(z, ζ1

∗ )(cid:107)(cid:63)

∗ ) − ∇ζ(cid:96)(θ1; ζ1

∗ ) − γ2∇ζc(z, ζ1
∗ )(cid:107)(cid:63)
∗ ) − γ2∇ζc(z, ζ1

∗ )(cid:107)(cid:63)

≤ Lzθ(cid:107)θ2 − θ1(cid:107) + (cid:107)∇ζc(z, ζ1

∗ )(cid:107)(cid:63) (cid:107)γ2 − γ1(cid:107).

Substituting (A.9) into (A.6), yields

165

(A.4)

(A.5)

(A.6)

(A.7)

(A.8)

(A.9)

(cid:107)ζ2

∗ − ζ1

∗ (cid:107) ≤

≤

Lzθ
λ
Lzθ
λ

(cid:107)θ2 − θ1(cid:107) +

(cid:107)θ2 − θ1(cid:107) +

1
λ
Lc
λ

(cid:107)∇ζc(z, ζ1

∗ )(cid:107)(cid:63) (cid:107)γ2 − γ1(cid:107)

(cid:107)γ2 − γ1(cid:107)

(A.10)

where the last inequality holds because ζ (cid:55)→ c(z, ζ) is Lc-Lipschitz as per Assumption 3.

To obtain (2.10c), we ﬁrst suppose without loss of generality that only a single datum z is
given, and in order to prove existence of the gradient of ¯ψ(¯θ, z) with respect to ¯θ, we resort to
the Danskin’s theorem as follows.

Danskin’s Theorem [46]. Consider the following minimax optimization problem

min
θ∈Θ

max
ζ∈X

f (θ, ζ)

(A.11)

where X is a nonempty compact set, and f : Θ × X → [0, ∞) is such that f (·, ζ) is differ-

entiable for any ζ ∈ X , and ∇θf (θ, ζ) is continuous on Θ × X . With S(θ) := {ζ∗|ζ∗ =
arg maxζ f (θ, ζ)}, the function

166

¯f (θ) := max
ζ∈Z

f (θ, ζ)

is locally Lipschitz and directionally differentiable, where the directional derivatives satisfy

¯f (θ, d) = sup
ζ∈S(θ)

(cid:104)d, ∇θf (θ, ζ)(cid:105).

(A.12)

For a given θ, if the set S(θ) is a singleton, then the function ¯f (θ) is differentiable at θ with
gradient

∇θ

¯f (θ) = ∇θf (θ, ζ∗(θ)).

(A.13)

Given θ, and the µ-strongly convex c(z, ·), function ψ(¯θ, ·; z) is concave if Lzz − γµ < 0,
which holds true for γ0 > Lzz/µ. Replacing ¯f (θ, ζ) with ψ(¯θ, ζ; z), and given the concavity
of ζ (cid:55)→ ψ(¯θ, ζ; z), we have that ¯ψ(¯θ; z) is a continuous function with gradient

∇¯θ

¯ψ(¯θ; z) = ∇¯θ

¯ψ(¯θ, ζ∗(¯θ; z); z).

(A.14)

We can then obtain the second inequality, as

∗ ; z)(cid:107)

(cid:107)∇¯θψ(¯θ1, ζ1
≤ (cid:107)∇¯θψ(¯θ1, ζ1

∗ ; z) − ∇¯θψ(¯θ2, ζ2
∗ ; z) − ∇¯θψ(¯θ1, ζ2

∗ ; z)(cid:107)
∗ ; z) − ∇¯θψ(¯θ2, ζ2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:107)∇¯θψ(¯θ1, ζ2
(cid:34)
∇θ(cid:96)(θ1, ζ1
c(z, ζ2
(cid:34)
∇θ(cid:96)(θ1, ζ2

(cid:35)(cid:13)
∗ ) − ∇θ(cid:96)(θ1, ζ2
(cid:13)
∗ )
(cid:13)
(cid:13)
∗ ) − c(z, ζ1
∗ )
(cid:13)
∗ ) − ∇θ(cid:96)(θ2, ζ2
∗ )

≤

0

∗ ; z)(cid:107)

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∗ − ζ2
≤ Lθz(cid:107)ζ1

(cid:16)

≤

Lθθ +

∗ (cid:107) + Lc(cid:107)ζ1
LθzLzθ + LcLzθ
λ

∗ − ζ2
∗ (cid:107) + Lθθ(cid:107)θ1 − θ2(cid:107)
(cid:17)

(cid:107)θ2 − θ1(cid:107)

(A.15)

(A.16)

+

LθzLc + L2
c
λ

(cid:107)γ2 − γ1(cid:107)

167

(A.17)

where we again used inequality (A.10). As a technical note, if the considered model is a neural

network with a non-smooth activation function, the loss will not be continuously differentiable.

However, we will not encounter this challenge often in practice.

A.0.2 Proof of Theorem 1

With slight abuse of notation, deﬁne for convenience F (θ, γ) := f (θ, γ) + r(θ) + h(γ), where

h(γ) is the indicator function

h(γ) =




0,

if γ ∈ Γ



∞,

if γ /∈ Γ

(A.18)

with Γ := {γ|γ ≥ γ0}, and for ease of representation we use ¯r(¯θ) := r(θ) + h(γ). Having an
Lf –smooth function f , yields

f (¯θt+1) ≤ f (¯θt) + (cid:10)∇f (¯θt), ¯θt+1 − ¯θt(cid:11) +

Lf
2

(cid:107)¯θt+1 − ¯θt(cid:107)2.

(A.19)

For a given zt, the gradients are

and

g∗(¯θt) :=

=

(cid:34)

∇θψ(θt, γ, ζ∗(¯θt; zt); zt)
∂γψ(θt, γ, ζ∗(¯θt; zt); zt)
(cid:34)
∇θψ(θt, γ, ζ∗(¯θt; zt); zt)
ρ − c(zt, ζ∗(¯θt; zt)

(cid:35)

(cid:35)

.

g(cid:15)(¯θt) :=

=

(cid:35)
(cid:34)
∇θψ(θt, γ, ζ(cid:15)(¯θt; zt); zt)
∂γψ(θt, γ, ζ(cid:15)(¯θt; zt); zt)
(cid:35)
∇θψ(θt, γ, ζ(cid:15)(¯θt; zt); zt)
ρ − c(zt, ζ(cid:15)(¯θt; zt)

(cid:34)

obtained by an oracle at the optimal ζ∗ and the (cid:15)-optimal ζ(cid:15) solvers, respectively. Now, we deﬁne
the error vector δ(¯θt) := ∇f (¯θt) − g(cid:15)(¯θt), and replace this into (A.19), to obtain

168

f (¯θt+1) ≤ f (¯θt) + (cid:10)g(cid:15)(¯θt) + δ(¯θt), ¯θt+1 − ¯θt(cid:11)
(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2.
(cid:13)

+

Lf
2

(A.20)

The following properties hold equivalently for the proximal operator, and for any x, y

u = proxαr(x) ⇐⇒ (cid:10)x − u, y − u(cid:11) ≤ αr(y) − αr(u).

(A.21)

With u = ¯θt+1 and x = ¯θt − αtg(cid:15)(¯θt) in (A.21), it holds that

(cid:10)¯θt − αtg(cid:15)(¯θt) − ¯θt+1, ¯θt − ¯θt+1(cid:11) ≤ αt¯r(¯θt) − αt¯r(¯θt+1)

and upon rearranging, we obtain

(cid:10)g(cid:15)(¯θt), ¯θt+1 − ¯θt(cid:11) ≤ ¯r(¯θt) − ¯r(¯θt+1) −

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2.
(cid:13)

1
αt

(A.22)

Adding inequalities in (A.22) and (A.20) gives

f (¯θt+1) ≤ f (¯θt) + (cid:10)δ(¯θt), ¯θt+1 − ¯θt(cid:11) +

(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2
(cid:13)

Lf
2
(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2
(cid:13)

+ ¯r(¯θt) − ¯r(¯θt+1) −

1
αt

and with F (¯θ) := f (¯θ) + ¯r(¯θ), we can write

F (¯θt+1)−F (¯θt) ≤ (cid:10)δ(¯θt), ¯θt+1 − ¯θt(cid:11)
(cid:16) Lf
2

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2.
(cid:13)

−

+

1
αt

(A.23)

Using Young’s inequality for any η > 0 gives (cid:10)δ(¯θt), ¯θt+1−¯θt(cid:11) ≤ η
and hence

2 (cid:107)¯θt+1−¯θt(cid:107)2+ 1

2η (cid:107)δ(¯θt)(cid:107)2,

F (¯θt+1)−F (¯θt) ≤

(cid:16) Lf + η
2

−

1
αt

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2 +
(cid:13)

(cid:13)δ(¯θt)(cid:13)
(cid:13)
2
(cid:13)
2η

.

(A.24)

169
Next, we will bound δ(¯θt) := ∇f (¯θt) − g(cid:15)(¯θt). By adding and subtracting g∗(¯θt) to the

right hand side, we ﬁnd

(cid:107)δ(¯θt)(cid:107)2 ≤ 2(cid:107)∇f (¯θt) − g∗(¯θt)(cid:107)2 + 2(cid:107)g∗(¯θt) − g(cid:15)(¯θt)(cid:107)2.

(A.25)

The Lipschitz smoothness of the gradient, implies that

(cid:35)

=

(cid:13)g∗(¯θt) − g(cid:15)(¯θt)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:34)
(cid:34)
∇θψ(θt, γ, ζ∗(¯θt; zt); zt)
∇θψ(θt, γ, ζ(cid:15)(¯θt; zt); zt)
(cid:13)
(cid:13)
(cid:13)
ρ − c(zt, ζ∗(¯θt; zt)
ρ − c(zt, ζ(cid:15)(¯θt; zt)
(cid:13)
(cid:13)∇θψ(θt, γ, ζ∗(¯θt; zt); zt) − ∇θψ(θt, γ, ζ(cid:15)(¯θt; zt); zt)(cid:13)
= (cid:13)
2
(cid:13)
+ (cid:13)
(cid:16) L2
θz

∗) − c(zt, ζt
(cid:13)c(zt, ζt
(cid:17)

(cid:15))(cid:13)
2
(cid:13)

−

(a)
≤

λt + Lc

(cid:107)ζt

∗ − ζt

(cid:15)(cid:107)2

(b)
≤

(cid:16) L2
θz

λt + Lc

≤

(cid:16) L2
θz
λ0

+ Lc

(cid:17)

(cid:17)

(cid:15)

(cid:15)

(A.26)

(cid:35) (cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(A.27)

where (a) uses the λt = µγt − Lzz strong-concavity of ζ (cid:55)→ ψ(¯θ, γ, ζ; z), and the second
term is bounded by Lc(cid:107)ζt
(cid:15)(cid:107)2 according to Assumption 3. The last inequality holds for
λ0 := µγ0 − Lzz, where we used (A.18) to bound γt ≥ γ0 > Lzz. So far, we have established
that

∗ − ζt

(cid:107)g∗(¯θt) − g(cid:15)(¯θt)(cid:107)2 ≤

L2
¯θz(cid:15)
λ0

(A.28)

where for notational convenience we let L2

¯θz := L2

θz + λ0Lc. Substituting (A.28) into

(A.25), the error can be bounded as

(cid:107)δ(¯θt)(cid:107)2 ≤ 2(cid:107)∇f (¯θt) − g∗(¯θt)(cid:107)2 +

2L2
¯θz(cid:15)
λ0

.

Combining (A.24) and (A.27) yields

F (¯θt+1) − F (¯θt) ≤

(cid:16) Lf + η
2

−

1
αt

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2
(cid:13)

(A.29)

(A.30)

+

1
η

(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
(cid:13)
2 +
(cid:13)

L2
¯θz(cid:15)
ηλ0

.

170

Considering a constant step size α and summing these inequalities over t = 1, . . . , T yields

(cid:16) 1
α

−

Lf + η
2

(cid:17) T
(cid:88)

t=0

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2 ≤ F (¯θ0) − F (¯θT )
(cid:13)

+

1
η

T
(cid:88)

t=0

(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
(cid:13)
2 +
(cid:13)

(T + 1)L2
λ0

¯θz(cid:15)

.

(A.31)

From the proximal gradient update

¯θt+1 = arg min
θ

α¯r(θ) + α(cid:10)θ − ¯θt, g(cid:15)(¯θt)(cid:11) +

(cid:13)θ − ¯θt(cid:13)
(cid:13)
2
(cid:13)

1
2

(A.32)

the optimality of ¯θt+1 in (A.32), implies that

¯r(¯θt+1) + (cid:10)¯θt+1 − ¯θt, g(cid:15)(¯θt)(cid:11) +

1
2α

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2 ≤ ¯r(¯θt)
(cid:13)

which combined with the smoothness of f (c.f. (A.19)) yields

(cid:10)¯θt+1 − ¯θt, g(cid:15)(¯θt) − ∇f (¯θt)(cid:11) +

(cid:16) 1
2α

−

Lf
2

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2
(cid:13)

≤ F (¯θt) − F (¯θt+1)

Subtracting (cid:104)¯θt+1 − ¯θt, ∇f (¯θt+1)(cid:105) from both sides gives

(cid:10)¯θt+1 − ¯θt, g(cid:15)(¯θt) − ∇f (¯θt+1)(cid:11) +

(cid:16) 1
2α

−

Lf
2

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2
(cid:13)

≤ F (¯θt) − F (¯θt+1) − (cid:10)¯θt+1 − ¯θt, ∇f (¯θt+1) − ∇f (¯θt)(cid:11).

Considering (cid:13)
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) + 1
terms to the right hand side, we arrive at

α (¯θt+1 − ¯θt)(cid:13)
2
(cid:13)

on the left hand side, and adding relevant

(cid:13)
1
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)
α
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1)(cid:13)
2 +
(cid:13)

≤ (cid:13)

(cid:13)
2
(¯θt+1 − ¯θt)
(cid:13)
(cid:13)

1
α2

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2
(cid:13)

(A.33)

(cid:0)F (¯θt) − F (¯θt+1)(cid:1)

(cid:0)F (¯θt) − F (¯θt+1)(cid:1)

≤ (cid:13)

(cid:13)g(cid:15)(¯θt) − ∇f (¯θt)(cid:107)2 +

+

−

+

−

+

+

−

−

2
α

2
α

1
α2

1
α2

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2
(cid:13)

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2 +
(cid:13)

(cid:16) Lf
α
(cid:10)¯θt+1 − ¯θt, ∇f (¯θt+1) − ∇f (¯θt)(cid:11)
1
α2
(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2 +
(cid:13)

(cid:16) Lf
α
(cid:10)¯θt+1 − ¯θt), ∇f (¯θt+1) − ∇f (¯θt)(cid:11)
1
α2
(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2 +
(cid:13)
L2
f
η

(cid:16) Lf
1
α2
α
(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2 +
(cid:13)

2
α
(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2
(cid:13)

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2
(cid:13)

η
α

2
α

2
α

−

≤ (cid:13)

(cid:13)g(cid:15)(¯θt) − ∇f (¯θt)(cid:107)2 +

(cid:0)F (¯θt) − F (¯θt+1)(cid:1)

171

(A.34)

(A.35)

where the last inequality is obtained by applying Young’s inequality, and then using the Lf -
Lipschitz continuity of f (·). By simplifying the last inequality, we obtain

(cid:13)
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)

1
α
(cid:0)F (¯θt) − F (¯θt+1)(cid:1) +

+

2
α

(cid:13)
2
≤ (cid:13)
(¯θt+1− ¯θt)
(cid:13)
(cid:13)

(cid:13)g(cid:15)(¯θt) − ∇f (¯θt)(cid:13)
2
(cid:13)

(cid:16) L2
f
η

+

Lf + η
α

(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2.
(cid:13)

The ﬁrst term in the right hand side can be bounded by adding and subtracting g∗(¯θt) and using
(A.28), to arrive at

(cid:13)
2
(¯θt+1 − ¯θt)
(cid:13)
(cid:13)

(cid:13)
1
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)
α
2 2L2
¯θ(cid:15)
2
(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
≤ 2(cid:13)
(cid:13)
λ0
α
(cid:16) L2
(cid:17)(cid:13)
(cid:13)¯θt+1 − ¯θt(cid:13)
2.
f
(cid:13)
η

Lf + η
α

+

+

+

(cid:0)F (¯θt) − F (¯θt+1)(cid:1)

(A.36)

Summing these inequalities over t = 1, . . . , T , we ﬁnd

T
(cid:88)

t=0

(cid:13)
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)

(cid:13)
2
(¯θt+1 − ¯θt)
(cid:13)
(cid:13)

1
α

≤

T
(cid:88)

t=0

(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
(cid:13)
2 +
(cid:13)

2(T + 1)L2
¯θ(cid:15)
λ0

+

2
α

(cid:2)F (¯θ0) − F (¯θT )(cid:3) +

(cid:16) L2
f
η

+

Lf + η
α

(cid:17) T
(cid:88)

t=0

(cid:13)¯θt+1 − ¯θt(cid:13)
(cid:13)
2.
(cid:13)

172

(A.37)

Using (A.31) to bound the last term yields

T
(cid:88)

t=0
T
(cid:88)

≤ 2

t=0

+

β
η

T
(cid:88)

t=0

(cid:13)
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)

(¯θt+1 − ¯θt)

(cid:13)
2
(cid:13)
(cid:13)

1
α

(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
(cid:13)
2 +
(cid:13)

2(T + 1)L2
¯θ(cid:15)
λ0

+

2
α

∆F + β∆F

(cid:13)∇f (¯θt) − g∗(¯θt)(cid:13)
(cid:13)
2 +
(cid:13)

β(T + 1)L2
λ0

¯θz(cid:15)

(A.38)

where β = (

L2
η + Lf +η
f
α )

obtain

2α

2−(Lf +η)α . By taking expectation of both sides of this inequality, we

1
T + 1

E

≤

(cid:16) 2
α

+ β

(cid:104) T
(cid:88)

t=0
(cid:17) ∆F

T + 1

(cid:13)
(cid:13)g(cid:15)(¯θt) − ∇f (¯θt+1) +
(cid:13)

(cid:13)
(¯θt+1 − ¯θt)
(cid:13)
(cid:13)

2(cid:105)

1
α

+

(cid:16) β
η

(cid:17)

σ2 +

+ 2

(β + 2)L2
¯θ(cid:15)
λ0

(A.39)

where we have used E[(cid:107)∇f (¯θt) − g∗(¯θt)(cid:107)2
[152, Theorem 10] and [210], we know that

2] ≤ σ2, which holds according to Assumption 5. By

− g(cid:15)(¯θt) −

1
α

(¯θt+1 − ¯θt) ∈ ∂ ¯r(¯θt+1)

(A.40)

which gives

∇f (¯θt+1) − g(cid:15)(¯θt) −

1
α

(¯θt+1 − ¯θt) ∈ ∇f (¯θt+1) + ∂ ¯r(¯θt+1)

:= ∂F (¯θt+1).

Upon replacing the latter in the left hand side of (A.39), and recalling the deﬁnition of distance,

we deduce that

E(cid:2)dist(0, ∂ ˆF (¯θt(cid:48)

))(cid:3) ≤ (cid:0) 2
α

+β(cid:1) ∆F
T

+(cid:0) β
η

+2(cid:1)σ2 +

(β + 2)L2
λ0

¯θz(cid:15)

where t(cid:48) is randomly drawn from t(cid:48) ∈ {1, 2, . . . , T + 1}, which concludes the proof.

173

A.0.3 Proof of Theorem 2

Instead of resorting to an oracle to obtain an (cid:15)-optimal solver for the surrogate loss, here we

utilize a single step stochastic gradient ascent with mini-batch size M to solve the maximization

step. Consequently, the updates become

¯θt+1 = proxαtr

(cid:0)¯θt − αtgt(¯θt)(cid:1)

(A.41)

m=1 g(¯θt, ζt
where gt(¯θt) := 1
M
Lf -smoothness of f (¯θ), we obtain

(cid:80)M

m; zm). Letting δ(¯θt) := ∇f (¯θt) − gt(¯θt), and using the

f (¯θt+1) ≤ f (¯θt) + (cid:10)∇f (¯θt), ¯θt+1 − ¯θt(cid:11) +

≤ f (¯θt) + (cid:10)gt(¯θt) + δ(¯θt), ¯θt+1 − ¯θt(cid:11) +

Lf
2
Lf
2

(cid:107)¯θt+1 − ¯θt(cid:107)2

(cid:107)¯θt+1 − ¯θt(cid:107)2.

(A.42)

Next, we substitute ¯θt+1 → u, θt → y, and ¯θt − αtgt(¯θt) → x in (A.21), to arrive at

(cid:10)¯θt − αtgt(¯θt) − ¯θt+1, ¯θt − ¯θt+1(cid:11) ≤ αt¯r(¯θt) − αt¯r(¯θt+1)

which leads to

(cid:10)gt(¯θt), ¯θt+1 − ¯θt(cid:11) ≤ ¯r(¯θt) − ¯r(¯θt+1) −

(cid:107)¯θt+1 − ¯θt(cid:107)2.

1
αt

Substituting the latter into (A.42), gives

f (¯θt+1) ≤ f (¯θt) + (cid:10)δ(¯θt), ¯θt+1 − ¯θt(cid:11) +

(cid:107)¯θt+1 − ¯θt(cid:107)2

+ ¯r(¯θt) − ¯r(¯θt+1) −

1
αt

Lf
2
(cid:107)¯θt+1 − ¯θt(cid:107)2

and with F (θ) := f (θ) + ¯r(θ), we have

F (¯θt+1) − F (¯θt) ≤ (cid:10)δ(¯θt), ¯θt+1 − ¯θt(cid:11)

+

(cid:16) Lf
2

−

(cid:17)

1
αt

(cid:107)¯θt+1 − ¯θt(cid:107)2.

174

(A.43)

Using Young’s inequality (cid:10)δ(¯θt), ¯θt+1 − ¯θt(cid:11) ≤ 1

2 (cid:107)δ(¯θt)(cid:107)2 + 1

2 (cid:107)¯θt+1 − ¯θt(cid:107)2 implies that

F (¯θt+1)−F (¯θt) ≤

(cid:16) Lf +1
2

−

(cid:17)

1
αt

(cid:107)¯θt+1 − ¯θt(cid:107)2 +

(cid:107)δ(¯θt)(cid:107)2
2

(A.44)

and after adding the term (cid:10)¯θt+1 − ¯θt, ∇f (¯θt+1)(cid:11) to both sides in (A.44), and simplifying terms,
yields

(cid:10)¯θt+1 − ¯θt, gt(¯θt) − ∇f (¯θt+1)(cid:11)
(cid:17)
Lf
2

(cid:16) 1
2αt

≤ −

−

(cid:107)¯θt+1 − ¯θt(cid:107)2 + F (¯θt) − F (¯θt+1)

− (cid:10)¯θt+1 − ¯θt, ∇f (¯θt+1) − ∇f (¯θt)(cid:11).

(A.45)

Completing the square yields

(cid:13)
(cid:13)gt(¯θt) − ∇f (¯θt+1) +

1
αt
≤ (cid:107)gt(¯θt) − ∇f (¯θt+1)(cid:107)2 +

(¯θt+1 − ¯θt)(cid:13)
2
(cid:13)

(cid:107)¯θt+1 − ¯θt(cid:107)2

1
α2
t

+

−

(cid:17)

−

1
α2
t

(cid:107)¯θt+1 − ¯θt(cid:107)2 +

(cid:16) Lf
αt
2
(cid:10)¯θt+1 − ¯θt, ∇f (¯θt+1) − ∇f (¯θt)(cid:11)
αt

2(F (¯θt) − F (¯θt+1))
αt

(cid:17)

−

+

(cid:107)¯θt+1 − ¯θt(cid:107)2 +

≤ 2(cid:107)gt(¯θt) − ∇f (¯θt)(cid:107)2 + 2(cid:107)∇f (¯θt) − ∇f (¯θt+1)(cid:107)2
(cid:16) Lf
αt

1
α2
t
2(F (¯θt) − F (¯θt+1))
2
αt
αt
f (cid:107)¯θt+1 − ¯θt(cid:107)2
≤ 2(cid:107)gt(¯θt) − ∇f (¯θt)(cid:107)2 + 2L2
(cid:17)

(cid:107)¯θt+1 − ¯θt(cid:107)2

1
α2
t

+

−

(cid:107)¯θt+1 − ¯θt(cid:107)2 +

(cid:107)¯θt+1 − ¯θt(cid:107)2

(cid:16) Lf
αt

−

1
α2
t

+

1
α2
t

(cid:104)¯θt+1 − ¯θt, ∇f (¯θt+1) − ∇f (¯θt)(cid:105)

+

2(F (¯θt) − F (¯θt+1))
αt

≤ 2(cid:107)gt(¯θt) − ∇f (¯θt)(cid:107)2 +

+

2Lf
αt

(cid:107)¯θt+1 − ¯θt(cid:107)2

2(F (¯θt) − F (¯θt+1))
αt

+

3Lf + 2L2
αt

f αt

(cid:107)¯θt+1 − ¯θt(cid:107)2.

Recalling that δ(¯θt) := ∇f (¯θt) − gt(¯θt), we can bound the ﬁrst term as

175

(A.46)

E

(cid:12)θt(cid:105)
(cid:104)
(cid:107)gt(¯θt) − ∇f (¯θt)(cid:107)2 (cid:12)
(cid:12)θt(cid:105)
(cid:104) (cid:13)
(cid:13)g∗(¯θt) − ∇f (¯θt) + δt(cid:13)
2 (cid:12)
(cid:13)
= (cid:107)g∗(¯θt) − ∇f (¯θt)(cid:107)2 + (cid:107)δt(cid:107)2 + 2E

= E

(cid:104) (cid:10)g∗(¯θt) − ∇f (¯θt), δt(cid:11) (cid:12)

(cid:12)θt(cid:105)

(A.47)

where the third equality is obtained by expanding the square term, and using E(cid:2)(cid:104)g∗(¯θt) −
∇f (¯θt), δt(cid:105)(cid:12)
(cid:12)¯θt(cid:3) = 0. We will further bound the right hand side here as follows. Recalling that
(cid:80)M
m=1 g(¯θt, ζt
δt = 1
m; zm), it holds
M

m; zm) − g∗(¯θt), where g∗(θt) := 1
M

m=1 ∇¯θψ(¯θt, ζ∗t

(cid:80)M

that

E

(cid:104)(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

(cid:104)

m=1

g(¯θt, ζt

m; zm) − g∗(¯θt)(cid:3)(cid:13)
2(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)

(cid:105)

¯θt, ζt
m

=

1
M 2

≤

L2
θz
M 2

M
(cid:88)

E

(cid:104)(cid:13)
(cid:13)∇¯θψ(¯θt,ζt

m;zm)−∇¯θψ(¯θt,ζ∗t

2(cid:12)
m;zm)(cid:13)
(cid:12)¯θt, ζt
(cid:13)
m

m=1
M
(cid:88)

(cid:13)
(cid:13)ζt

m − ζ∗t
m

(cid:13)
2
(cid:13)

m=1

(cid:105)

(A.48)

where the second equality is because the samples {zm}M
due to the Lipschitz smoothness of ψ(·). Since ζt

m=1 are i.i.d., and last inequality holds
m is obtained by a single gradient ascent update

over a µ-strongly concave function, we have that

L2
θz
M 2

M
(cid:88)

(cid:13)
(cid:13)ζt

m − ζ∗t
m

(cid:13)
2 ≤
(cid:13)

m=1

L2
θz
M

(cid:104)
(1 − αtµ) D2 + α2

t B2(cid:105)

(A.49)

where D is the diameter of the feasible set, and αt > 0 is the step size. The following holds for

the expected error term

(cid:104)
E

(cid:107)δt(cid:107)2(cid:12)

(cid:12)¯θt, ζt
m

(cid:105)

≤

L2
θz
M

(cid:104)
(1 − αtµ) D2 + α2

t B2(cid:105)

and using it in (A.47), we arrive at

(cid:104)
E

(cid:107)gt(¯θt) − ∇f (¯θt)(cid:107)2(cid:12)

(cid:12)θt(cid:105)

+

(cid:104)

L2
¯θz
M

(1 − αtµ) D2 + α2

≤ 2(cid:107)g∗(¯θt) − ∇f (¯θt)(cid:107)2
t B2(cid:105)
.

Substituting the last inequality into (A.46) boils down to

176

(A.50)

(A.51)

(A.52)

(cid:104)(cid:13)
(cid:13)gt(¯θt) − ∇f (¯θt+1) +
E

(¯θt+1 − ¯θt)(cid:13)
(cid:13)

(cid:12)¯θt(cid:105)
2(cid:12)

≤ 4(cid:107)g∗(¯θt) − ∇f (¯θt)(cid:107)2 +

f αt

(cid:104)
(cid:107)¯θt+1 − ¯θt(cid:107)2(cid:12)

(cid:12)¯θt(cid:105)

E

1
αt
3Lf + 2L2
αt

2F (¯θt) − 2E(cid:2)F (¯θt+1)(cid:12)
αt

(cid:12)¯θt(cid:3)

+

+

(cid:104)

L2
¯θz
M

(1 − αtµ) D2 + α2

t B2(cid:105)
.

(A.53)

Taking again expectation over ¯θt on both sides, yields

(¯θt+1 − ¯θt)

(cid:13)
2
(cid:13)
(cid:13)

(A.54)

≤ 4E

(cid:13)
1
(cid:13)gt(¯θt) − ∇f (¯θt+1) +
(cid:13)
E
αt
(cid:107)g∗(¯θt) − ∇f (¯θt)(cid:107)2(cid:105)
(cid:104)
(cid:104) 2F (¯θt) − 2F (¯θt+1)
αt

+ E

+

+

L2
¯θz
M
3Lf + 2L2
αt

(cid:104)
(1 − αtµ) D2 + α2

t B2(cid:105)
(cid:107)¯θt+1 − ¯θt(cid:107)2(cid:105)

.

f αt

Recalling that E[(cid:107)ψ∗(¯θt, ζt
the ﬁrst term on the right hand side can be bounded by 4σ2
summing inequalities (A.54) from t = 0, . . . , T , yields

m; zm)−∇f (¯θt)(cid:107)2] ≤ σ2, and that g∗(¯θt) = 1
M

(cid:80)M

m=1 ψ(¯θt, ζ∗t

m; zm),

M . For a ﬁxed learning rate α > 0,

1
T + 1

(cid:104) T
(cid:88)
E

t=0

(cid:13)
(cid:13)gt(¯θt) − ∇f (¯θt+1) +

(¯θt+1 − ¯θt)(cid:13)
(cid:13)

2(cid:105)

1
αt

≤

2
α(T + 1)

(cid:0)F (θ0) − E[F (θT )](cid:1)+

2L2
¯θz
M

[(1 − αµ)D2 + α2B2]

f α

1
T + 1

(cid:104) T
(cid:88)
E

(cid:107)¯θt+1 − ¯θt(cid:107)2(cid:105)

+

4σ2
M

+

3Lf + 2L2
α
(cid:26) 2
α
(cid:26)

+

1
T + 1
2L2
¯θz
M

+

1 +

≤

t=0
f α

(cid:27)

6Lf + 4L2
[2 − α(Lf + β)]
3Lf + 2L2
f α
2(2 − α(Lf + β))

(F (¯θ0) − E[F (¯θT )]) +

4σ2
M
(1 − αµ) D2 + α2B2(cid:105)
.

(cid:27)(cid:104)

177

(A.55)

Consider now replacing F (¯θ0) − F (¯θT ) with ∆F = F (¯θ0) − inf ¯θ F (¯θ), and note that gt(¯θt) −
(¯θt+1 − ¯θt) ∈ ∂F (¯θt+1), where ∂F denotes the set of subgradients of F . It
∇f (¯θt+1) + 1
αt

then becomes clear that

E(cid:2)dist(0, ∂F )2(cid:3)

≤

1
T + 1

(cid:104) T
(cid:88)
E

t=0

(cid:13)
(cid:13)gt(¯θt) − ∇f (¯θt+1) +
(cid:13)

(cid:13)
(¯θt+1 − ¯θt)
(cid:13)
(cid:13)

2(cid:105)

1
αt

≤

ζ
T + 1

∆F +

2L2
¯θzν
N

(1 − αµ) D2 + α2B2(cid:105)
(cid:104)

+

4σ2
M

where ζ = 2

α +

6Lf +4L2
(2−α(Lf +β)) and ν = 1 +

f α

3Lf +2L2
2(2−α(Lf +β)) , which concludes the proof.

f α

