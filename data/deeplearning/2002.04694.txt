Adversarial Robustness for Code

Pavol Bielik 1 Martin Vechev 1

0
2
0
2

g
u
A
5
1

]

G
L
.
s
c
[

2
v
4
9
6
4
0
.
2
0
0
2
:
v
i
X
r
a

Abstract
Machine learning and deep learning in particu-
lar has been recently used to successfully address
many tasks in the domain of code such as ﬁnding
and ﬁxing bugs, code completion, decompilation,
type inference and many others. However, the is-
sue of adversarial robustness of models for code
has gone largely unnoticed.
In this work, we
explore this issue by: (i) instantiating adversar-
ial attacks for code (a domain with discrete and
highly structured inputs), (ii) showing that, simi-
lar to other domains, neural models for code are
vulnerable to adversarial attacks, and (iii) com-
bining existing and novel techniques to improve
robustness while preserving high accuracy.

1. Introduction

Recent years have seen an increased interest in using deep
learning to train models of code for a wide range of tasks
including code completion (Brockschmidt et al., 2019; Li
et al., 2018), code captioning (Alon et al., 2019; Allama-
nis et al., 2016; Fernandes et al., 2019), code classiﬁca-
tion (Mou et al., 2016; Zhang et al., 2019) and bug de-
tection (Allamanis et al., 2018; Pradel & Sen, 2018; Li
et al., 2019). Despite substantial progress on training ac-
curate models of code, the issue of robustness has been
overlooked. Yet, this is a very important problem shown
to affect neural models in different domains (Goodfellow
et al., 2015; Szegedy et al., 2014; Papernot et al., 2016).

Challenges in modeling code
In our work, we focus
on tasks that compute program properties (e.g., type in-
ference), usually addressed via handcrafted static analy-
sis, but for which a number of recent neural models with
high accuracy have been introduced (Hellendoorn et al.,
2018; Schrouff et al., 2019; Malik et al., 2019). Unsur-
prisingly, as these works do not consider adversarial ro-
bustness, their adversarial accuracy can drop signiﬁcantly.

1Department of Computer Science, ETH Z¨urich, Switzerland.

Correspondence to: Pavol Bielik <pavol.bielik@inf.ethz.ch>.

Learning to
Abstain

•

•

•

•

•

•

Adversarial
Training

•

•

•

•

•

•

Represenation
Reﬁnement
(cid:53)
(cid:53) (cid:53)
•
(cid:53)(cid:53) (cid:53)

•

•

•

•

•

Figure 1. Illustration of the three key components used in our
work. Each point represents a sample,
is a region where model
are regions of model
abstains from making predictions,
prediction, (cid:13) is the space of valid modiﬁcations for a given sam-
ple, and (cid:66)(cid:66)(cid:66) is the learned (reduced) space of valid modiﬁcations.

and

However, training both robust and accurate models of code
in this setting is non-trivial and requires one to address
several key challenges: (i) programs are highly structured
and long, containing hundreds of lines of code, (ii) a sin-
gle discrete program change can affect the prediction of
a large number of properties and is much more disruptive
than a slight continuous perturbation of a pixel value, and
(iii) the property prediction problem is usually undecidable
(hence, static analyzers approximate the ideal solution).

Accurate and robust models of code As a ﬁrst step to
address these challenges, we propose a novel method that
combines three key components, illustrated in Figure 1
– as we show, all of these contribute to achieving accu-
rate and robust models of code. First, we train a model
that abstains (Liu et al., 2019) from making a prediction
when uncertain, effectively partitioning the dataset into two
parts: one part where the model makes predictions ( ,
)
that should be accurate and robust, and one ( ) where the
model abstains and it is enough to be robust. Second, we
instantiate adversarial training (Goodfellow et al., 2015) to
the domain of code. Third, we develop a new method to re-
ﬁne the representation used as input to the model by learn-
ing the parts of the program relevant for the prediction.
This reduces the number of places that affect the prediction
and helps to make adversarial training for code effective.
Finally, we create a new algorithm that trains multiple mod-
els, each learning a specialized representation that makes
robust predictions on a different subset of the dataset.

Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).

We instantiate our approach to the type prediction task and
show its effectiveness – we train a model that improves ro-
bustness by 15% while preserving high accuracy.

 
 
 
 
 
 
Adversarial Robustness for Code

2. Accurate and Robust Models of Code

In this section, we present an overview of our approach.
Without loss of generality, we deﬁne an input program p
to be a sequence of words p = w1:n. The words can cor-
respond to a tokenized version of the program, nodes in
an abstract syntax tree corresponding to p or other suitable
program representations. Further, let l ∈ N be a position in
the program p that corresponds to a word wl ∈ W. A train-
ing dataset D = {(xj, yj)}N
j=1 contains a set of samples,
where x ∈ X is an input tuple x = (cid:104)p, l(cid:105) consisting of
a program p and a position in the program l, while y ∈ Y
contains the ground-truth label. As an example, the code
snippet in Figure 2a contains 12 different samples (x, y),
one for each position where a prediction should be made
(annotated with their ground-truth types y).
Our goal is to learn a function f : X → R|Y|, represented as
a neural network, which for a given input program and a po-
sition in the program, computes the probability distribution
over the labels. The model’s prediction then corresponds to
the label with the highest probability according to f .

Step 1: Augment the model with an (un)certainty score
We start by augmenting the standard neural model f with
an option to abstain from making a prediction. To achieve
this, we adopt the recently proposed approach by (Liu
et al., 2019) and introduce a selection function gh : X → R,
which measures model certainty. Then, the model is de-
ﬁned to make a prediction only if gh is conﬁdent enough
(gh(x) ≥ h) and abstain from making a prediction other-
wise. Here, h ∈ R is an associated threshold that controls
the desired level of conﬁdence. For example, using a high
threshold h = 0.9, the model learns to make only ﬁve pre-
dictions for the program in Figure 2b and will abstain from
uncertain predictions such as predicting parameter types.

The ﬁrst insight from our work is that allowing the model
to abstain is beneﬁcial for achieving robustness. This step
leads to simpler models, since learning to abstain is easier
than learning to predict the correct label. This is in con-
trast with forcing the model to learn the correct label for all
samples, which is infeasible for most practical tasks.

Step 2: Adversarial training Next, we instantiate adver-
sarial training to the domain of code. Concretely, let ∆(x)
be a set of valid modiﬁcations of sample x and let x + δ de-
note a new input obtained by applying the modiﬁcations in
δ ⊆ ∆(x) to x. As a concrete example, Figure 2c shows a
refactoring of the program from Figure 2b by renaming hex
to color. Even though this change does not affect the types
in the program, the model suddenly predicts incorrect types
for both the color parameter and the substring function.
Further, even though the types of parseInt and v are still
correct, the model became much more uncertain.

Intuitively, our goal is to address this issue and to en-
sure that the model is robust for all valid modiﬁcations
δ ⊆ ∆(x) – when evaluated on x + δ, the model either ab-
stains or predicts the correct label. Concretely, we use ad-
versarial training (Goodfellow et al., 2015), which instead
of minimizing the expected loss on the original distribu-
tion E(x,y)∼D[(cid:96)((f, gh)(x), y)] as usually done in standard
training, minimizes the expected adversarial loss:

E(x,y)∼D[ max
δ⊆∆(x)

(cid:96)((f, gh)(x + δ), y)]

(1)

That is, we minimize the worst case loss obtained by apply-
ing a valid modiﬁcation to the original sample x. Similar to
other domains, the main challenge in this setting is solving
the inner maxδ⊆∆(x) efﬁciently for the domain of code.

Standard adversarial training is insufﬁcient Although
adversarial training has been successfully applied in many
domains (Madry et al., 2018; Wong & Kolter, 2018; Sinha
et al., 2018; Raghunathan et al., 2018), in our work we
show that for code, adversarial training alone is insufﬁcient
to achieve model robustness. The key reason is that, ex-
isting neural models of code typically process the entire
program which can contain hundreds of lines of code. This
is problematic as it means that any program change will
affect all predictions and there can be inﬁnitely many pro-
gram changes in ∆(x). Further, a single discrete program
change is much more disruptive in affecting the model than
a slight continuous perturbation of a pixel value. At the
same time, while not sufﬁcient, in our evaluation we show
that adversarial training can be used to improve robustness
by 0 to 7%, depending on the model architecture.

Step 3: Representation reﬁnement To address the is-
sue that adversarial training alone does not work well, we
develop a novel technique that: (i) learns which parts of
the input program are relevant for the given prediction, and
(ii) reﬁnes the model representation such that only relevant
program parts are used as input to the neural network. Es-
sentially, the technique automatically learns an abstraction
α which given a program, produces a relevant representa-
tion of that program. Figure 2d shows an example of a
possible abstraction α that takes as input the entire pro-
gram but keeps only parts relevant for predicting the type of
parseInt – it is a method call with name parseInt which
has two arguments. To learn the abstraction α, we ﬁrst rep-
resent programs as graphs and then phrase the reﬁnement
task as an optimization problem that minimizes the num-
ber of graph edges, while ensuring that the accuracy of the
model before and after applying α stays roughly the same.

Finally, we apply adversarial training, but this time on the
abstraction α obtained via representation reﬁnement, re-
sulting in new functions f and gh. Overall, this results in
an adversarially robust model mi = (cid:104)f, gh, α(cid:105).

Adversarial Robustness for Code

(a) Training Dataset D = {(xj, yj)}N

j=1

(b) Learning to Abstain (Appendix B)

(c) Adversarial Training (Appendix C)

(hexstr, radixnum) => {
vnum = parseIntnum(

(hex, radix) => {

vnum,1.0 = parseIntnum,1.0(

(colornum,0.9, radix) => {
vnum,0.4 = parseIntnum,0.6(

hexstr.substringstr(1num),
radixnum

D

hex.substringstr,0.9(1num,1.0),
radix

(cid:104)f, gh(cid:105)

color.substringbool,0.9(1num,1.0),
radix

);
rednum = vnum >>num 16num;
...

Di+1 ⊂ Di

);

red = v >> 16num,1.0;

Learn selection function gh that makes a prediction
only if conﬁdent enough and abstains otherwise.

);

red = v >> 16num,1.0;

δ = [rename hex → color]

Train with the worst case modiﬁcations x + δ

(e) Apply & Train Next Model (Section 4)

(d) Representation Reﬁnement (Section 3)

(hex, radix) => {

v = parseInt<num>(

hex.substring(1<num>),
radix

);

red = v >> 16<num>;

robust model

mi = (cid:104)f, gh, α(cid:105)

α

(num, radix) => {
v = parseInt(

num.substring(1),
radix

=

);

red = v >> 16;

parseInt(

_,
_

);

(cid:104)f, gh(cid:105)

Retrain with
abstraction α

Annotate programs in D with predicted types

Learned abstraction α used to predict the parseInt return type

Figure 2. Overview of the main steps of our approach for learning accurate and adversarially robust models of code.

Step 4: Learning accurate models Although the
model mi is robust, it provides predictions only for a sub-
set of the samples for which it has enough conﬁdence (i.e.,
gh(x) ≥ h). To increase the ratio of samples for which
our approach makes a prediction (i.e., does not abstain),
we perform two steps: (i) generate a new dataset Di+1
by annotating the program with the predictions made by
the learned model mi, and removing successfully predicted
samples, and (ii) learn another model mi+1 on the new
dataset Di+1. We repeat this process for as long as the new
learned model predicts some of the samples in Di+1.

(i) the
Training multiple models is beneﬁcial because:
models are easier to train as well as easier to make robust as
they do not try to learn all predictions, (ii) it allows condi-
tioning on the predictions learned by earlier models which
helps both interpretability and robustness. For example,
the model mi+1 can learn that the left hand side of the
assignment v = parseInt has the same type as the right
hand side, since the type of parseInt was already pre-
dicted by mi. Interestingly, if we think of each model as a
learned set of rules, we can essentially apply the models to
a given program in a ﬁxed point style (similar to how a tra-
ditional sound static analysis works), and (iii) each model
learns a different representation α that is specialized for
the predictions it makes. For example, while predicting
the type of parseInt is independent of the argument val-
ues (parseInt( , )), predicting the second argument type
is not (parseInt( , radix)). Using a single abstraction to
predict both would lead to either reduced robustness or ac-
curacy, depending on which abstraction is used.

Summary Given a training dataset D, our approach
learns a set of robust models, each of which makes robust
predictions for a different subset of D. To achieve this,

we extend existing neural models of code with three key
components – the ability to abstain (with associated uncer-
tainty score), adversarial training, and learning to reﬁne the
representation. Given the limited space, we provide for-
mal description of the the ﬁrst two components that learn
to abstain and apply adversarial training for code in Ap-
pendix B and Appendix C, respectively. Next, we formally
describe the novel components – learning to reﬁne the rep-
resentation (Section 3) and present our training algorithm
that combines all of them together (Section 4).

3. Learning to Reﬁne Representations

As motivated in Section 2, a key issue with many existing
neural models for code is that the model prediction f (x)
depends on the full program p, even though only small parts
of p are typically relevant. We address this issue by learn-
ing an abstraction α that takes as input p and produces only
the parts relevant for the prediction. That is, α reﬁnes the
representation given as input to the neural model.

Overview Our method works as follows: (i) we convert
the program into a graph representation, (ii) then deﬁne the
model to be a graph neural network (e.g., (Veliˇckovi´c et al.,
2018; Kipf & Welling, 2017; Wu et al., 2019; Li et al.,
2016)), which at a high level works by propagating and ag-
gregating messages along graph edges, (iii) because depen-
dencies in graph neural networks are deﬁned by the struc-
ture of the graph (i.e., the edges it contains), we phrase the
problem of reﬁning the representation as an optimization
problem which removes the maximum number of graph
edges (i.e., removes the maximum number of dependen-
cies) without degrading model accuracy, and (iv) we show
how to solve the optimization problem efﬁciently by trans-
forming it to an integer linear program (ILP).

Adversarial Robustness for Code

From programs to graphs Following prior works, we
represent programs using their corresponding abstract syn-
tax trees (AST). These are further transformed into graphs,
as done in (Allamanis et al., 2018; Brockschmidt et al.,
2019), by including additional edges.

Deﬁnition 3.1. (Directed Graph) A directed graph is a tu-
ple G = (cid:104)V, E, ξV , ξE(cid:105) where V denotes a set of nodes,
E ⊆ V 2 denotes a set of directed edges, ξV : V → Nk
is a mapping from nodes to their associated attributes and
ξE : E → Nm is a mapping from edges to their attributes.

We associate two attributes with each node – type which
corresponds to the type of the AST node (e.g., Block,
Identifier, BinaryExpression, etc.) and value asso-
ciated with the AST node (e.g., +, −, 0, 1, ”GET”, x, data,
etc.). For edges we use a single attribute the edge type,
which can be: (i) ast, for the edges that correspond to those
included in the AST, (ii) last usage, for edges introduced
between any two usages (either read or write) of the same
variable, and (iii) returns-to, for edges introduced between
a return statement and the function declaration. All edges
are initially added in both directions, but can be later re-
moved during the training. Depending on the task, more
edge types can be easily added.

Representation reﬁnement Our goal is to learn an ab-
straction function α : (cid:104)V, E, ξV , ξE(cid:105) → (cid:104)V, E(cid:48) ⊆ E, ξV , ξE(cid:105)
that removes a subset of the edges from the graph. To quan-
tify the size of the abstraction, we use |α(x)| := |E(cid:48)| to
denote the number of edges after applying α on x.

Deﬁning valid graph reﬁnements Because the goal of
representation reﬁnement is to reduce the number of nodes
on which a prediction depends, we need to ensure that α
itself does not depend on all the graph nodes. This is nec-
essary as otherwise we only shift the dependency on the
entire program from the model f to the representation re-
ﬁnement α. To achieve this, the decision to include or re-
move a given edge is done locally, based only on the edge
attributes and attributes of the nodes it connects.

Concretely, for a given edge (cid:104)s, t(cid:105) ∈ E, we deﬁne an edge
feature φ((cid:104)s, t(cid:105)) := (cid:104)ξE((cid:104)s, t(cid:105)), ξV (s), ξV (t)(cid:105) to be a tu-
ple of the edge attributes and attributes of the nodes it con-
nects. As a form of regularization, we condition only on the
type attribute of each node. We denote the set of all possi-
ble edge features Φ to be the range of the function φ evalu-
ated over all edges in D. Further, we deﬁne the reﬁnement
α as a subset of edge features α ⊆ Φ. Finally, the seman-
tics of executing α over edges E is that only edges whose
features are in α are kept, i.e., {e | e ∈ E ∧ φ(e) ∈ α}.

Problem statement Minimize the expected size of the
reﬁnement α ⊆ Φ subject to the constraint that the ex-

pected loss of the model f stays approximately the same:

arg min
α⊆Φ

(cid:88)

|α(x)|

(x,y)∈D

(2)

subject to

(cid:80)

(x,y)∈D (cid:96)(f (x), y) ≈ (cid:80)

(x,y)∈D (cid:96)(f (α(x)), y)

Our problem statement is quite general and can be instan-
(i) using (cid:96)AbstainCrossEntropy as the loss (Ap-
tiated by:
pendix B), and (ii) using adversarial risk (Appendix C).

Allowing the model to abstain from making predictions is
especially important in order to obtain small α (i.e., sparse
graphs). This is because the restriction that the model accu-
racy is roughly the same is otherwise too strict and would
require that most edges are kept. Further, note that the
problem formulation is deﬁned over all samples in D, not
only those for which the model f predicts the correct label.
This is necessary since the model needs to make a predic-
tion for all samples, even if that prediction is to abstain.

Optimization via integer linear programming (ILP)
To solve Equation 8, the key idea is that for each sample
(x, y) ∈ D we ﬁrst capture the relevance of each node to the
prediction made by the model f by computing:
(cid:13)
a(f, x, y) = (cid:2)(cid:107)Gi,:(cid:107)1, . . . , (cid:13)
(cid:13)1

(cid:13)G|p|,:

(cid:3),

where G = ∇x (cid:96)(f (x), y) ∈ R|p|×emb denotes the gra-
dient with respect to the input x = (cid:104)p, l(cid:105) and a given pre-
diction y. As positions in p correspond to discrete words,
the gradient is computed with respect to their embedding
emb ∈ R. The score for each position in p is computed by
applying the L1-norm over the embedding gradients, pro-
ducing a vector of unnormalized scores a ∈ R|p|. To obtain
a probability distribution ˆa(f, x, y) over all positions in p,
we normalize the entries in a accordingly.

Then, we phrase the solution of Equation 8 as an opti-
mization problem of including the minimum number of
edges necessary for a path to exist between every relevant
node (according to ˆa) and the node where the prediction
is made. Preserving all paths between the prediction and
relevant nodes encodes the constraint that the expected loss
stays approximately the same, since it allows propagating
information throughout the graph neural network. This
optimization can be naturally encoded as minimum-cost
maximum-ﬂow problem and solved efﬁciently with off-
the-shelf ILP solvers. We provide formal deﬁnition of the
ILP encoding as well as concrete examples in Appendix D.

Even though our ILP formulation is very fast (in all our
experiments the ILP solver takes less than a second), it does
result in a more complex approach compared to an end-to-
end trainable solution. We note however that an end-to-
end trainable solution is also possible. For example, one

Adversarial Robustness for Code

αlast ← Φ
f, gh ← Train (D, tacc − (cid:15))
while true do

Algorithm 1 Training procedure used to learn a single ad-
versarially robust model (cid:104)f, gh, α(cid:105).
1: function RobustTrain(D, tacc) :
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

α ← RefineRepresentation (D, f, gh)
if |α| ≥ |αlast| then break
αlast ← α
D ← {(α(x), y) | (x, y) ∈ D}
f,gh ←AdversarialTrain (D,f,gh, tacc − (cid:15))
set threshold h in gh such that the accuracy is tacc
return (cid:104)f, gh, α(cid:105)

while true do

Algorithm 2 Training multiple adversarially robust mod-
els, each of which learns to make predictions for a different
subset of the dataset D.
1: function AccurateAndRobustTrain(D, tacc = 1.0)
2: M ← []
3:
4:
5:
6:
7:
8:
9:

(cid:104)f, gh, α(cid:105) ← RobustTrain(D, tacc)
Dabstain ← Apply(D, f, gh, α)
if |Dabstain| = |D| then break
D ← Dabstain
M ← M · (cid:104)f, gh, α(cid:105)

return M

could make α continuous by deﬁning a learnable weight
for each edge feature φ, encode the sparsity on α as part
of the loss, and extend the graph neural network such that
each message propagated along an edge e is scaled using
the corresponding value of the edge feature φ(e). We have
explored this option in the work of (Abstreiter et al., 2020).

4. Training Algorithm

We now describe our algorithm that combines learning to
abstain, adversarial training and representation reﬁnement.

Training a single adversarially robust model The train-
ing procedure used to learn a single adversarially robust
model is shown in Algorithm 1. The input is a training
dataset D and the desired accuracy tacc that the learned
model should have. Here, setting tacc = 1.0 corresponds
to a model that makes no mis-prediction (i.e., 100% accu-
racy) while tacc = 0 corresponds to training a model that
never abstains.

We start by training a model f and a selection function gh
as described in Appendix B (line 3). At this point we do not
use adversarial training and train with a weaker threshold
tacc −(cid:15), as our goal is only to obtain a fast approximation of
the samples that can be predicted with high certainty. We
use f and gh to obtain an initial representation reﬁnement α
(line 5) which is applied to the dataset D to remove edges
that are not relevant according to f and gh (line 8). After
that, we perform adversarial training (line 9) as described in
Appendix C. However, instead of training from scratch, we
reuse model f and gh learned so far, which speeds-up train-
ing. Next, we reﬁne the representation again (line 5) and if
the new representation is smaller (line 6), we repeat the
whole process. Note that the adversarial training also uses
threshold tacc − (cid:15) to account for the fact that the suitable
representation is not known in advance. After the training
loop ﬁnishes, we set the threshold h used by gh to match
the desired accuracy tacc (more details on this step are pro-

vided in Appendix B). The ﬁnal result is a model consisting
of the function f trained to make adversarially robust pre-
dictions, the selection function gh and the abstraction α.

Incorporating robust predictions Once a single model
is learned, it makes robust predictions on a subset of the
dataset Dpredict = {(x, y) | (x, y) ∈ D ∧ gh(α(x)) ≥ h} and
abstains from making a prediction on the remainder of the
samples Dabstain = D \ Dpredict. Next, for all samples
in Dpredict, we use the learned model to annotate the po-
sition l in the program p (recall that each x = (cid:104)p, l(cid:105) con-
sists of a program p and a position l) with the ground-truth
label y (denoted as Apply in Algorithm 2). Annotating
a program position corresponds to either deﬁning a new at-
tribute (as illustrated in Figure 2e) or replacing an existing
attribute (e.g., the value attribute) of a given node. Note
that annotating programs is useful only in cases where the
same program p is shared by multiple samples (x, y) ∈ D
(i.e., multiple predictions are computed for different posi-
tions in the same program).

Main training algorithm Our main training algorithm
is shown in Algorithm 2.
It takes as input the training
dataset D and learns multiple models M , each of which
makes robust predictions on a different subset of D (as mo-
tivated in Section 2). The number of models and the subsets
for which they make predictions is not ﬁxed a priori and is
learned as part of our training. Model training (line 4) and
model application (line 5) are performed as long as a non-
empty robust model exists (i.e., it makes at least one predic-
tion). If the goal is to make predictions for all the samples
in D, the Algorithm 2 is run iteratively, with decreasing
values of tacc until the full dataset is covered.

Verifying model correctness A natural extension of our
approach is to formally verify that the learned models are
correct. Even though formally verifying the correctness of
all samples is typically infeasible, it is possible to verify
a subset of them. This can be achieved since using repre-
sentation reﬁnement signiﬁcantly simpliﬁes the problem of
proving correctness of all positions (nodes) in the program

Adversarial Robustness for Code

to a much smaller set of relevant positions. In fact, for some
cases the reﬁned representation is so small that it is possible
to simply enumerate all valid modiﬁcations (e.g., a ﬁnite
set of valid variable renamings) and check that the model
is correct for all of them. Additionally, it would be possi-
ble to adapt the recently proposed techniques (Huang et al.,
2019; Jia et al., 2019), based on Interval Bound Propaga-
tion, that verify robustness to any valid word renaming and
word substitution modiﬁcations. However, applying these
techniques to realistic networks in a scalable and precise
ways is an open problem beyond the scope of our work.

5. Evaluation

We instantiated our approach to a well studied task
– predicting types for dynamically typed languages
JavaScript and TypeScript (Hellendoorn et al., 2018;
Schrouff et al., 2019; Malik et al., 2019; Raychev et al.,
2015). In this task, the need for model robustness is natural
since the model is queried each time a program is modiﬁed
by the user. Our key results are:

• Our approach learns accurate and adversarially ro-
bust models for the task of type inference, achiev-
ing 87.7% accuracy while improving robustness from
52.1% to 67.0%.

• We train highly accurate and robust models for a sub-
set of the dataset, with 99.9% accuracy and 99.9% ro-
bustness for 29% of the samples.

Our implementation uses PyTorch (Paszke et al., 2019) and
DGL library (Wang et al., 2019). We used a single Nvidia
TITAN RTX for all the experiments. For our dataset, we
collect the same top starred projects on Github and per-
form similar preprocessing steps as Hellendoorn et al. We
provide detailed description in the supplementary material.
The code and datasets are available at:

https://github.com/eth-sri/robust-code

Evaluation metrics We use two main evaluation metrics:

Accuracy is computed over the unmodiﬁed dataset D and
corresponds to the accuracy used in prior works. Con-
cretely, the accuracy is deﬁned as the ratio of samples (x, y)
for which the most likely label according to the model f ,
denoted f (x)best, is the same as the ground truth label y:

1
|D|

(cid:88)

(x,y)∈D

(cid:40)
1
0

if f (x)best = y
otherwise

Robustness is the ratio of samples (x, y) ∈ D for which
the model f evaluated on all valid modiﬁcations δ ⊆ ∆(x)

either abstains or makes a correct prediction:

1
|D|

(cid:88)

(x,y)∈D

(cid:40)

0
1

if ∃δ⊆∆(x)f (x + δ)best /∈ {y, abstain}
otherwise

Models We evaluate ﬁve neural model architectures:

LSTM is a bidirectional LSTM with attention which takes as
input a sequence of AST nodes, including both types and
values, obtained using pre-order traversal.

DeepTyper is a model proposed by Hellendoorn et al. and
consists of a bidirectional LSTM layer, followed by a single
layer graph neural network that connects all variables with
the same name (referred as consistency layer), followed by
another bidirectional LSTM layer. Our only modiﬁcation
is that the input to our model is a sequence of AST types
and values, instead of using syntactic program tokens.

GCN, GGNN and GNT are three graph neural networks that
use as input the graph program representation described
in Section 3. Here, GCN is a Graph Convolutional Net-
work (Kipf & Welling, 2017), GGNN is Gated Graph Neural
Network (Li et al., 2016) and GNT is a graph implemen-
tation of a recently proposed transformer neural network
architecture (Vaswani et al., 2017; Dehghani et al., 2019).

All models were trained with an embedding and hidden
size of 128, batch size of 32, dropout 0.1 (Srivastava et al.,
2014), initial learning rate of 0.001, using Adam opti-
mizer (Kingma & Ba, 2014) and between 10 to 20 epochs.

Reducing dependencies via dynamic halting We fur-
ther strengthen our GNT model by implementing the Adap-
tive Computation Time (ACT) (Graves, 2016) which dy-
namically learns how many computational steps each node
requires in order to make a prediction. This is in contrast to
using a ﬁxed amount of steps as done in (Allamanis et al.,
2018; Brockschmidt et al., 2019). In our experiments, ACT
signiﬁcantly reduces the number of steps each node per-
forms (half of the nodes perform ≤ 3 steps).

Program modiﬁcations We instantiate the adversar-
ial
training with both semantic preserving and label-
preserving modiﬁcations shown in Table 1. Here, expr is
either an existing expression or a new expression consisting
of a random binary expression over constants up to depth 3,
const is a randomly selected constant that results in a valid
expression and x, y, z are variables in the program scope.
Our modiﬁcations extend those used by Bielik et al. (2017)
but the list is not exhaustive and can be extended further.

To measure the model robustness, we run the adversarial at-
tack for 1000 iterations for renaming modiﬁcations and ad-
ditional 300 iterations for structural modiﬁcations. These
thresholds are rather high and were selected with the goal

Adversarial Robustness for Code

Table 1. Illustration of semantic and label preserving program modiﬁcations used in our work.

Substitutions and Renaming

Examples

Structural Modiﬁcations

Examples

Semantic Preserving

Label Preserving

variable renaming
object ﬁeld renaming
property assignment renaming

x → y
obj.x → obj.y
{x : obj} → {y : obj}

Label Preserving

number substitution
string substitution
boolean substitution

2 → 7
”get” → ”load”
true → false

new function parameters
new method arguments

def inc(x) → def inc(x, y)
inc(x) → inc(x, expr)

Semantic Preserving

ternary expressions
array access

expr1 → (expr)2 : expr1 ? expr1
expr → [expr, expr][const]

Dead Code
side-effect free expressions
adding object expressions

∅ → expr
∅ → {x : y, z : expr}

Table 2. Comparison of accuracy and robustness across various models and training techniques considered in our work for the task of
type inference. Adversarial training and the ability to abstain is applicable to all the models. The representation reﬁnement is designed
speciﬁcally to models deﬁned over graphs, including GCN, GGNN and GNT.

Standard Training

Adversarial Training

Abstain + Adversarial + Reﬁnement

(cid:96)(f (x), y)

maxδ⊆∆(x) (cid:96)(f (x + δ), y)

maxδ⊆∆(x) (cid:96)AbstainCE((f, gh)(α(x + δ)), y)

Model

Accuracy

Robustness

Accuracy

Robustness

Model

Accuracy

Robustness

LSTM
DeepTyper
GCN
GNT
GGNN

88.2 ± 0.2
88.4 ± 0.2
82.6 ± 0.6
89.3 ± 0.9
86.7 ± 0.4

44.9 ± 1.3
52.4 ± 1.2
49.1 ± 1.1
47.4 ± 1.0
52.1 ± 0.4

87.5 ± 0.4
87.1 ± 0.3
81.9 ± 0.5
88.3 ± 0.4
86.1 ± 0.2

51.9 ± 1.3
55.1 ± 2.6
49.3 ± 3.1
50.0 ± 0.5
57.9 ± 1.5

tacc = 1.00 (Abstain ≈ 70%)

GNT
GGNN
tacc = 0.00

GNT
GGNN

99.93%
99.80%

86.6%
87.7%

99.98%
99.01%

62.3%
67.0%

of closely estimating the true number of adversarial sam-
ples. Further, note that since δ ⊆ ∆(x) is a set, each itera-
tion explores a set of concrete program modiﬁcations.

5.1. Accurate and Adversarially Robust Models

We summarize the main results in Table 2. The ﬁrst col-
umn (left) shows the median test accuracy and standard
deviation of various models (across three trials trained with
different random seeds). The GCN achieves the worst accu-
racy of 82.6% and the accuracy of the remaining models is
similar with GNT model performing the best with 89.3%.

Existing models are not robust While highly accurate,
all models are also non-robust for up to half of the samples
in the dataset. In other words, for every second sample x
in our dataset, there exists a modiﬁcation δ ⊆ ∆(x) for
which f (x) predicts the type correctly while f (x + δ) mis-
predicts it. However, since these models were not trained
with the goal of adversarial robustness, it is expected for
them to be (atleast partially) non-robust.

the adversarial training increase the robustness, it does so
only slightly. The best improvement was achieved for
LSTM and GGNN models (7% and 5.8%, respectively). For
DeepTyper and GNT the robustness increased by ≈ 2.5%
while for GCN it is only 0.2%. This illustrates that while
useful, if used alone, adversarial training is not enough.

Our work: training accurate models with abstain The
models trained using our approach are shown in Table 2
(right). First, we trained our models to be both accu-
rate and robust on a subset of the dataset. This can be
achieved by setting the desired accuracy thresholds, in our
case tacc = 1.00, which corresponds to training the model
to make only correct predictions. For tacc = 1.00, our
approach learns an almost perfect model that is both ac-
curate and robust for ≈ 30.0% of samples. Here, GNT
learned 7 models and achieved 99.98% robustness while
GGNN learned 8 models with robustness of 99.01%. Learn-
ing multiple models is crucial for achieving higher cover-
age as a single model would not abstain for only 17 − 20%
of the samples, compared to 30% using multiple models.

Adversarial training alone is insufﬁcient To improve
the robustness, we next train the models using adversarial
training as described in Appendix C. Unfortunately, while

The model did not achieve 100% accuracy and robustness
for tacc = 1.00 due to several samples included in the test
set. These samples were mis-predicted because they con-

Adversarial Robustness for Code

Table 3. Robustness breakdown for the GNT and GGNN models
trained using our approach from Table 2 (right).

Robustness

Dataset

Size

∀ Correct

∃ Incorrect Abstain

GNT
Dcorrect
Dabstain
GGNN
Dcorrect
Dabstain

tacc = 1.00
29.3%
70.6%

tacc = 1.00
30.6%
69.3%

90.0%
−

75.5%
−

0.00%
0.01%

0.06%
1.46%

10.00%
99.99%

23.94%
98.54%

:= ∀δ⊆∆(x)(f, gh)(α(x + δ))best = y
∀ Correct
∃ Incorrect := ∃δ⊆∆(x)(f, gh)(α(x + δ))best /∈ {y, abstain}

tained code structure not seen during training and not cov-
ered by modiﬁcations δ ⊆ ∆(x). This illustrates that it
is important that the samples in D are diverse and contain
all the language features and corner cases of the programs,
or that the modiﬁcations ∆(x) are expressive enough such
that these can be discovered automatically during training.

Our work: improving robustness Next, we train mod-
els that take advantage of the highly accurate and robust
models trained using tacc = 1.00, but make predictions for
all the samples (i.e., do not abstain). This can be achieved
by continuing the training while reducing tacc to zero and
conditioning on all the models trained with higher tacc. In
our experiments, we train a single additional model by di-
rectly setting tacc = 0 after training with tacc = 1.00. The
results are shown in Table 2 (right) and lead to additional
robustness increase of 9.2% and 12.3% compared to using
adversarial training only for GGNN and GNT, respectively.
For GNT, the accuracy slightly decreases by 1.7% which is
expected as increasing model robustness typically comes at
the cost of reduced accuracy (Tsipras et al., 2019). Inter-
estingly, for GGNN our robust training increases the accu-
racy over both the adversarial training as well as standard
training by 1.9% and 1.0%, respectively.

Adversarial robustness breakdown Table 3 provides
a detailed breakdown of the robustness metric for the GNT
and GGNN models trained with tacc = 1.00 from Table 2
(right). Here, Dabstain contains samples for which the
model abstains from making a prediction and Dcorrect con-
tains samples for which the model evaluated on a non-
adversarial input (i.e., x without any modiﬁcation) makes
a correct prediction. We use ∀ correct to denote that a sam-
ple (x, y) is correct for all possible modiﬁcations δ ⊆ ∆(x),
the ∃ incorrect has the same deﬁnition as robustness (i.e.,
there exists a modiﬁcation that leads to an incorrect predic-
tion), and abstain denotes the remaining samples.

The GNT is precise and keeps predicting the correct label in
90% of cases and abstain in the rest. This is even though
the requirements for ∀ correct are very strict and require
that all samples are correct. When considering Dabstain,
the GNT model is also precise and produces incorrect pre-
diction for only a single sample (0.01%). For GGNN the
results are similar but the model is both less precise (keeps
the correct prediction in 75.5% of cases) and less robust
(1.46% of samples in Dabstain can be modiﬁed to cause
a mis-prediction). This shows that the majority of robust-
ness errors from Table 2 are due to mis-predicted samples
for which the model originally abstained.

6. Related Work

Our work is related to a number of different areas from
adversarial machine learning and learning over code.

Model certainty Several approaches have been recently
proposed to extend neural models with certainty mea-
sure (Gal & Ghahramani, 2016; Liu et al., 2019; Gal, 2016;
Geifman & El-Yaniv, 2017; 2019). In our work, we use the
method proposed by Liu et al. (2019) but in a novel way –
applied to the adversarial setting with the goal of training
robust models.

Learning static analyzers from data A closely related
work addresses the task of learning static analyzers (Bielik
et al., 2017): it deﬁnes a domain speciﬁc language (DSL)
to represent static analyzers, uses decision tree learning to
obtain an interpretable model, and deﬁnes a procedure that
ﬁnds counter-examples the model mis-classiﬁes (used to
re-train the model). At a high-level, some of the steps are
similar but the actual technical solution is very different as
we address a general class of neural models and do not as-
sume any prior knowledge (i.e., a DSL).

Adversarial training Even though the problem of adver-
sarial robustness of code has been overlooked, the adversar-
ial training has already been applied to related domains –
natural language processing (Miyato et al., 2017; Papernot
et al., 2016; Gao et al., 2018; Liang et al., 2018; Belinkov
& Bisk, 2017; Ebrahimi et al., 2018) and graphs (Dai et al.,
2018; Z¨ugner et al., 2018; Z¨ugner & G¨unnemann, 2019).

In the domain of graphs, existing works focus on attack-
ing the graph structure (Dai et al., 2018; Z¨ugner et al.,
2018; Z¨ugner & G¨unnemann, 2019) by considering that the
nodes are ﬁxed and edges can be added or removed. While
this setting is natural for modelling many types of graphs,
such approaches do not apply for the domain of code where
graph edges can not be added and removed arbitrarily.

In natural language processing, existing approaches gen-
erally: (i) measure the contribution of individual words or

Adversarial Robustness for Code

characters to the prediction (e.g., using gradients (Liang
et al., 2018), forward derivatives (Papernot et al., 2016)
or head/tail scores (Gao et al., 2018)), and (ii) replace or
remove those whose contribution is high (e.g., using dic-
tionaries (Jia et al., 2019), character level typos (Gao et al.,
2018; Belinkov & Bisk, 2017; Ebrahimi et al., 2018), or
handcrafted strategies (Liang et al., 2018)). The adversar-
ial training used in our work operates similarly except our
modiﬁcations are designed over programs.

Program representations A core challenge of using ma-
chine learning for code is designing a suitable program rep-
resentation used as model input. Due to its simplicity, the
most commonly used program representation is a sequence
of words, obtained either by tokenizing the program (Hel-
lendoorn et al., 2018) or by linearizing the abstract syntax
tree (Li et al., 2018). This however ignores the fact that
programs do have a rich structure – an issue addressed by
representing programs as graphs (Allamanis et al., 2018;
Brockschmidt et al., 2019) or as a combination of abstract
syntax tree paths (Alon et al., 2019). In our work, we fol-
low the approach proposed in recent works and represent
programs as graphs. More importantly, we develop a novel
technique that learns to reﬁne the representation based on
model predictions instead of ﬁxing it a priori. As shown in
our evaluation, this is crucial for learning robust models.

Adversarial attacks for code Concurrent to our work,
Yefet et al. (2019) explored the task of generating adversar-
ial attacks for code via gradient based optimization. In con-
trast, we introduce an approach to reduce the search space
an adversarial attack needs to consider by learning to re-
ﬁne the representation. Such reduced search space is useful
for both for renaming and structural modiﬁcations, whereas
gradient based optimization has been explored only for re-
naming. However, both of these approaches are orthogonal
and can be combined into one that learns both to reduce
the search space as well as to efﬁciently ﬁnd adversarial
examples in this reduced search space.

Type inference We evaluated our work on the task of
type inference for which a number of recent works improve
accuracy by proposing a new neural architectures. In con-
trast, the goal of our work is to study and improve robust-
ness of these models. To achieve this, we compare to two
prior works in our evaluation (Schrouff et al., 2019; Hel-
lendoorn et al., 2018). In addition to predicting types from
source code, Malik et al. (2019) showed that it is possible to
predict parameter types using natural language information
obtained from method docstrings. Here, existing attacks on
text (LSTM) can be used to assess its robustness but evalu-
ating text models is outside the scope of our work. Finally,
two concurrent works to ours have proposed new models to
improve accuracy: Typilus (Allamanis et al., 2020) and

LambdaNet (Wei et al., 2020). Both of these works rep-
resent programs as graphs and use graph neural networks
as the underlying model architecture, which makes our ap-
proach applicable. However, we note that for LambdaNet
we expect the model to be quite robust as the authors man-
ually designed a sparse graph representation (by designing
a static analysis to extract the type dependence graph) over
which to learn.

7. Conclusion

We presented a new technique to train accurate and ro-
bust neural models of code. Our work addresses two key
challenges inherent to the domain of code: the difﬁculty of
computing the correct label for all samples (i.e., the input is
incomplete code snippet, program semantics are unknown)
as well as the fact that programs are signiﬁcantly larger and
more structured compared to images or natural language.

To address the ﬁrst challenge, we allow the model to ab-
stain from making a prediction, rather than forcing the
model to make predictions for all samples (as done in prior
works). To address the second challenge, we learn which
parts of the program are relevant for the prediction, and ab-
stract the rest (instead of using the entire program as input).

Further, we introduce a new procedure that trains multiple
models, instead of one. This has several advantages, as
each model is simpler and thus easier to train robustly, the
learned representation is specialized to the kind of predic-
tions it makes, and the model directly conditions on predic-
tions of prior models (instead of having to re-learn them).
However, a disadvantage of our approach is that the mod-
els are learned sequentially which slows down the train-
ing (i.e., training 10 models will take 10× more time). To
speed up the training, it would be interesting to allow learn-
ing multiple models in parallel at each sequential step and
then combine them as explored by Shazeer et al. (2017).

We believe than our work is only one step in addressing the
task of adversarially robust models of code and that many
challenges remain open. For example, it remains to be seen
how effective our approach is at other tasks over code, be-
yond type inference. Further, we optimize for the worst
case adversarial robustness, which corresponds to learn-
ing a robust model for all programs. An interesting future
work is to optimize with respect to those modiﬁcation that
are common among developers, especially if it is not possi-
ble to be robust for all of them. While we checked robust-
ness for a wide range of program modiﬁcations, these are
still far from exhaustive and more work is needed in deﬁn-
ing new ones. Finally, as the number of possible modiﬁca-
tion is large and growing, an interesting area is designing
how they can be combined efﬁciently, as explored recently
by Ramakrishnan et al. (2020) and Zhang et al. (2020).

Adversarial Robustness for Code

Acknowledgements

We would like to thank the anonymous reviewers who gave
useful comments and provided interesting suggestions on
how our work can be improved and extended. Further, we
would like to acknowledge the work of (Hellendoorn et al.,
2018) which is publicly available and provided useful in-
frastructure for generating datasets used in our work. The
research leading to these results was partially supported by
an ERC Starting Grant 680358.

References

Abstreiter, K., Bielik, P., and Vechev, M.

Improving
robustness for models of code via sparse graph neural
networks. Technical report, ETH Zurich, june 2020.
https://www.research-collection.
URL
ethz.ch/handle/20.500.11850/431559.

Allamanis, M., Peng, H., and Sutton, C. A convolutional
attention network for extreme summarization of source
code. In International Conference on Machine Learning
(ICML), 2016.

Allamanis, M., Brockschmidt, M., and Khademi, M.
In In-
Learning to represent programs with graphs.
ternational Conference on Learning Representations,
ICLR’18, 2018.

Allamanis, M., Barr, E. T., Ducousso, S., and Gao, Z. Typ-
ilus: Neural type hints. In Proceedings of the 41st ACM
SIGPLAN Conference on Programming Language De-
sign and Implementation, PLDI’20, pp. 91–105, 2020.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E.
Code2Vec: Learning distributed representations of code.
In Proceedings of the 46st Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Lan-
guages, volume 3 of POPL ’19, pp. 40:1–40:29, 2019.

Belinkov, Y. and Bisk, Y.

Synthetic and natural
noise both break neural machine translation. CoRR,
abs/1711.02173, 2017.

Bielik, P., Raychev, V., and Vechev, M.

Learning a
In International Conference
static analyzer from data.
on Computer Aided Veriﬁcation, CAV’17, pp. 233–253,
2017.

Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polo-
In
zov, O. Generative code modeling with graphs.
International Conference on Learning Representations,
ICLR’19, 2019.

Dai, H., Li, H., Tian, T., Huang, X., Wang, L., Zhu, J., and
Song, L. Adversarial attack on graph structured data.

In Proceedings of the 35th International Conference on
Machine Learning, volume 80 of ICML’18, pp. 1115–
1124. PMLR, 2018.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and
Kaiser, L. Universal transformers. In International Con-
ference on Learning Representations, ICLR’19, 2019.

Ebrahimi, J., Rao, A., Lowd, D., and Dou, D. HotFlip:
White-box adversarial examples for text classiﬁcation.
In Proceedings of the 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 2: Short
Papers), ACL’18, pp. 31–36, 2018.

Fernandes, P., Allamanis, M., and Brockschmidt, M. Struc-
tured neural summarization. In International Conference
on Learning Representations, ICLR’19, 2019.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, Univer-
sity of Cambridge, Department of Engineering, 9 2016.

Gal, Y. and Ghahramani, Z. Dropout as a bayesian approx-
imation: Representing model uncertainty in deep learn-
ing. In Proceedings of The 33rd International Confer-
ence on Machine Learning, volume 48 of ICML’16, pp.
1050–1059, 2016.

Gao, J., Lanchantin, J., Soffa, M. L., and Qi, Y. Black-box
generation of adversarial text sequences to evade deep
learning classiﬁers. CoRR, abs/1801.04354, 2018.

Geifman, Y. and El-Yaniv, R. Selective classiﬁcation for
deep neural networks. In Proceedings of the 31st Inter-
national Conference on Neural Information Processing
Systems, NeurIPS’17, pp. 4885–4894, 2017.

Geifman, Y. and El-Yaniv, R. SelectiveNet: A deep neural
network with an integrated reject option. In Proceedings
of the 36th International Conference on Machine Learn-
ing, volume 97 of ICML’19, pp. 2151–2159, 2019.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
In 3rd In-
ing and harnessing adversarial examples.
ternational Conference on Learning Representations,
ICLR’15, 2015.

Graves, A. Adaptive computation time for recurrent neural

networks. CoRR, abs/1603.08983, 2016.

Gurobi Optimization, L. Gurobi optimizer reference man-

ual, 2020. URL http://www.gurobi.com.

Hellendoorn, V. J., Bird, C., Barr, E. T., and Allamanis, M.
Deep learning type inference. In Proceedings of the 2018
26th ACM Joint Meeting on European Software Engi-
neering Conference and Symposium on the Foundations
of Software Engineering, ESEC/FSE’18, 2018.

Adversarial Robustness for Code

Huang, P.-S., Stanforth, R., Welbl, J., Dyer, C., Yogatama,
D., Gowal, S., Dvijotham, K., and Kohli, P. Achieving
veriﬁed robustness to symbol substitutions via interval
In Empirical Methods in Natural
bound propagation.
Language Processing, EMNLP’19, 2019.

Mou, L., Li, G., Zhang, L., Wang, T., and Jin, Z. Con-
volutional neural networks over tree structures for pro-
In Proceedings of the
gramming language processing.
Thirtieth AAAI Conference on Artiﬁcial Intelligence,
AAAI’16, pp. 1287–1293, 2016.

Jia, R., Raghunathan, A., G¨oksel, K., and Liang, P. Cer-
tiﬁed robustness to adversarial word substitutions.
In
Empirical Methods in Natural Language Processing,
EMNLP’19, 2019.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. In International Conference on Learning
Representations, ICLR’14, 2014.

Kipf, T. N. and Welling, M.

Semi-supervised classi-
In In-
ﬁcation with graph convolutional networks.
ternational Conference on Learning Representations,
ICLR’17, 2017.

Li, J., Wang, Y., Lyu, M. R., and King, I. Code comple-
tion with neural attention and pointer networks. In Pro-
ceedings of the 27th International Joint Conference on
Artiﬁcial Intelligence, IJCAI’18, pp. 4159–25, 2018.

Li, Y., Zemel, R., Brockschmidt, M., and Tarlow, D. Gated
graph sequence neural networks. In International Con-
ference on Learning Representations, ICLR’16, 2016.

Li, Y., Wang, S., Nguyen, T. N., and Van Nguyen, S. Im-
proving bug detection via context-based code representa-
tion learning and attention-based neural networks. Proc.
ACM Program. Lang., (OOPSLA):162:1–162:30, 2019.

Liang, B., Li, H., Su, M., Bian, P., Li, X., and Shi, W.
In Proceedings
Deep text classiﬁcation can be fooled.
of the 27th International Joint Conference on Artiﬁcial
Intelligence, IJCAI’18, pp. 4208–4215, 2018.

Liu, Z., Wang, Z., Liang, P. P., Salakhutdinov, R. R.,
Morency, L.-P., and Ueda, M. Deep gamblers: Learn-
ing to abstain with portfolio theory. In Advances in Neu-
ral Information Processing Systems 32, NeurIPS’19, pp.
10622–10632. 2019.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to ad-
versarial attacks. In International Conference on Learn-
ing Representations, ICLR’18, 2018.

Malik, R. S., Patra, J., and Pradel, M. NL2Type: Inferring
javascript function types from natural language informa-
tion. In Proceedings of the 41st International Conference
on Software Engineering, ICSE ’19, pp. 304–315, 2019.

Miyato, T., Dai, A. M., and Goodfellow, I. Adversar-
ial training methods for semi-supervised text classiﬁca-
tion. In International Conference on Learning Represen-
tations, ICML’17, 2017.

Papernot, N., McDaniel, P. D., Swami, A., and Harang,
R. E. Crafting adversarial input sequences for recurrent
neural networks. CoRR, abs/1604.08275, 2016.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai-
son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
L., Bai, J., and Chintala, S. PyTorch: An imperative
In Ad-
style, high-performance deep learning library.
vances in Neural Information Processing Systems 32,
NeurIPS’19, pp. 8024–8035. 2019.

Pradel, M. and Sen, K. DeepBugs: A learning approach to
name-based bug detection. Proc. ACM Program. Lang.,
(OOPSLA):147:1–147:25, 2018.

Raghunathan, A., Steinhardt, J., and Liang, P. Cer-
In In-
tiﬁed defenses against adversarial examples.
ternational Conference on Learning Representations,
ICLR’18, 2018.

Ramakrishnan, G., Henkel, J., Wang, Z., Albarghouthi, A.,
Jha, S., and Reps, T. Semantic robustness of models of
source code, 2020.

Raychev, V., Vechev, M., and Krause, A. Predicting pro-
In Proceedings of
gram properties from ”Big Code”.
the 42Nd Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL ’15,
pp. 111–124, 2015.

Schrouff, J., Wohlfahrt, K., Marnette, B., and Atkinson, L.
Inferring javascript types using graph neural networks.
In Representation Learning on Graphs and Manifolds.
ICLR Workshop, 2019.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q. V., Hinton, G. E., and Dean, J. Outrageously large
neural networks: The sparsely-gated mixture-of-experts
layer. CoRR, abs/1701.06538, 2017.

Sinha, A., Namkoong, H., and Duchi, J. Certiﬁable dis-
tributional robustness with principled adversarial train-
ing. In International Conference on Learning Represen-
tations, ICLR’18, 2018.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res.,
15(1):1929–1958, January 2014. ISSN 1532-4435.

Adversarial Robustness for Code

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
Intriguing proper-
D., Goodfellow, I., and Fergus, R.
ties of neural networks. In International Conference on
Learning Representations, ICLR’14, 2014.

Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and
Madry, A. Robustness may be at odds with accuracy. In
International Conference on Learning Representations,
ICLR’19, 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. At-
In Advances in Neural Infor-
tention is all you need.
mation Processing Systems 30, NeurIPS’17, pp. 5998–
6008. 2017.

Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Li`o, P., and Bengio, Y. Graph Attention Networks. Inter-
national Conference on Learning Representations, 2018.

Wang, M., Yu, L., Zheng, D., Gan, Q., Gai, Y., Ye, Z.,
Li, M., Zhou, J., Huang, Q., Ma, C., Huang, Z., Guo,
Q., Zhang, H., Lin, H., Zhao, J., Li, J., Smola, A. J.,
and Zhang, Z. Deep Graph Library: Towards efﬁcient
and scalable deep learning on graphs. ICLR Workshop
on Representation Learning on Graphs and Manifolds,
2019.

Wei, J., Goyal, M., Durrett, G., and Dillig, I. Lamb-
daNet: Probabilistic type inference using graph neural
networks. In International Conference on Learning Rep-
resentations, ICLR’20, 2020.

Wong, E. and Kolter, Z. Provable defenses against adver-
sarial examples via the convex outer adversarial poly-

tope. In Proceedings of the 35th International Confer-
ence on Machine Learning, volume 80 of ICML’18, pp.
5286–5295, 2018.

Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein-
berger, K. Simplifying graph convolutional networks.
In Proceedings of the 36th International Conference on
Machine Learning, ICML’19, pp. 6861–6871. PMLR,
2019.

Yefet, N., Alon, U., and Yahav, E. Adversarial examples

for models of code, 2019.

Zhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., and
Liu, X. A novel neural source code representation based
on abstract syntax tree. In Proceedings of the 41st In-
ternational Conference on Software Engineering, ICSE
’19, pp. 783–794, 2019.

Zhang, Y., Albarghouthi, A., and D’Antoni, L. Robust-
ness to programmable string transformations via aug-
mented abstract training. In Proceedings of the 37th In-
ternational Conference on Machine Learning, ICML’20,
2020.

Z¨ugner, D. and G¨unnemann, S.

Adversarial attacks
In In-
on graph neural networks via meta learning.
ternational Conference on Learning Representations,
ICLR’19, 2019.

Z¨ugner, D., Akbarnejad, A., and G¨unnemann, S. Adver-
sarial attacks on neural networks for graph data. In Pro-
ceedings of the 24th ACM SIGKDD International Con-
ference on Knowledge Discovery & Data Mining, KDD
’18, pp. 2847–2856, 2018.

Adversarial Robustness for Code

Supplementary Material

We provide the following four appendices:

• Appendix A provides details of our dataset and addi-
tional experiments that evaluates the effect of dataset
size.

• Appendix B describes the method (introduced by Liu
et. al. 2019) used in our work for training neural mod-
els that abstain from making predictions if uncertain.

• Appendix C describes application of the adversarial
training in the domain of code via a set of program
mutations.

• Appendix D provides a formal deﬁnition of the inte-
ger linear encoding used to solve the problem in Equa-
tion 2 efﬁciently. Additionally, we provide a concrete
example illustrating the encoding.

A. Evaluation

Implementation All our models are implemented in Py-
Torch (Paszke et al., 2019). The graph neural networks are
implemented using the DGL library v0.4.3 (Wang et al.,
2019). To solve the integer linear program we use Gurobi
solver v8.11 (Gurobi Optimization, 2020).

Adaptive computation time (ACT) (Graves, 2016) Our
GNT model implements the Adaptive Computation Time
(ACT) (Graves, 2016) technique which dynamically learns
how many computational steps each node requires in or-
der to make a prediction. This is instead of using a ﬁxed
amount of steps as done for example in (Allamanis et al.,
2018; Brockschmidt et al., 2019). To achieve this, recall
that for each node vi ∈ V in the graph, a graph neural net-
t where t ∈ N
work computes sequence of hidden states si
is the timestep1. Following (Graves, 2016), the number of
timesteps that the model performs is controlled by intro-
t ∈ R(0,1) with
ducing an extra sigmoidal halting unit hi
associated learnable weight matrix Wh and bias bh:
t = σ(Whsi
hi

t + bh)

The output of the halting unit is then used to determine the
halting probability pi

t as follows:

pi
t =






k=0 hi
k
k=0 hi
k

1 − (cid:80)t−1
1 − (cid:80)t−1
hi
t

if t = T (last timestep)
if (cid:80)t
otherwise

k ≥ 1 − (cid:15)

k=0 hi

1 Note we assume only that si

t is computed for each timestep
which is independent of the concrete graph neural architecture
used to compute si
t.
where T ∈ N is the maximum allowed number of timesteps
and (cid:15) ∈ R(0,1) is a small constant introduced to allow the

network to stop after a single step (we use (cid:15) = 0.01 in our
experiments). Finally, the halting probability pi
t is used to
deﬁne the ﬁnal state si
T of a node vi as a weighted average
of its intermediate states:

si
T =

T
(cid:88)

t=0

t · si
pi
t

Dataset To obtain the datasets used in our work, we
extend the infrastructure from DeepTyper (Hellendoorn
et al., 2018), collect the same top starred projects on
GitHub, and perform similar preprocessing steps – remove
TypeScript header ﬁles, remove ﬁles with less than 100
or more than 3,000 tokens and split the projects into train,
validation and test datasets such that each project is fully
contained in one of the datasets. Additionally, we remove
exact ﬁle duplicates and ﬁles that are similar to each other
(≈ 10% of the ﬁles). We measure ﬁle similarity by collect-
ing all 3-grams (excluding comments and whitespace) and
removing ﬁles with Jaccard similarity greater than 0.7.

We compute the ground-truth types using the TypeScript
compiler version 3.4.5 based on manual
type annota-
tions, library speciﬁcations and analyzing all project ﬁles.
While we reuse the same GitHub projects and part of
DeepTyper’s infrastructure2 to obtain the dataset,
the
datasets are not directly comparable for a number of rea-
sons. First, we ﬁxed a bug due to which DeepTyper in-
correctly included some type annotations as part of the
input. Second, the projects we used are subset of those
used in DeepTyper since some are no longer available and
were removed from GitHub. Third, we additionally predict
the types corresponding to all intermediate expressions and
constants (e.g., the expression x + y contains three predic-
tions for x, y and x+y). This improves model performance
as it is explicitly trained also on the intermediate steps re-
quired to infer the types. Finally, we train all the models to
predict four primitive types (string, number, boolean,
void), four function types (() ⇒ string, () ⇒ number,
() ⇒ boolean, () ⇒ void) and a special unk label denoting
all the other types. While this is similar to types predicted
by some other works such as JSNice (Raychev et al.,
2015), it is only subset of types considered in DeepTyper.

All results presented in Tables 1 and 2 are obtained by
training our models using a dataset that contains 3000 pro-
grams split equally between training, validation and test
datasets. Because each program contain multiple type pre-
dictions, the number of training samples is signiﬁcantly
higher than the number of programs. Concretely, there
are 139, 915, 223, 912 and 121, 153 samples in training,
validation and test datasets. We note that this is only

2https://github.com/DeepTyper/DeepTyper

Adversarial Robustness for Code

a subset of the full dataset that can be obtained by pro-
cessing all the ﬁles included in the projects used by Hel-
lendoorn et al. We make the dataset available online at
https://github.com/eth-sri/robust-code.

During adversarial training, we explore 20 different mod-
iﬁcations δ ⊆ ∆(x) applied to each sample (x, y) ∈ D
which effectively increases dataset size by up to two or-
ders of magnitude since for each training epoch the modi-
ﬁcations are different. For the purposes of evaluation, we
increase the number of explored modiﬁcations to 1300 for
each sample – 1000 for renaming modiﬁcations and further
300 for renaming together with structural modiﬁcations.

B. Training Neural Models to Abstain

We now present a method for training neural models of
code that provide an uncertainty measure and can abstain
from making predictions. This is important as essen-
tially all practical tasks contain some examples for which it
is not possible to make a correct prediction (e.g., due to the
task hardness or because it contains ambiguities). In the
machine learning literature this problem is known as se-
lective classiﬁcation (supervised-learning with a reject op-
tion) and is an active area with several recently proposed
approaches (Gal & Ghahramani, 2016; Liu et al., 2019;
Gal, 2016; Geifman & El-Yaniv, 2017; 2019). In our work,
we use one of these methods (Liu et al., 2019) which is
brieﬂy summarized below. For a full description, we refer
the reader to the original paper (Liu et al., 2019).

j=1 be a training dataset and f : X → Y
Let D = {(xj, yj)}N
an existing model trained to make predictions on D. The
existing model f is augmented with an option to abstain
from making a prediction by introducing a selection func-
tion gh : X → R(0,1) with an associated threshold h ∈
R(0,1), which leads to the following deﬁnition:

(cid:40)

(f, gh)(x) :=

f (x)
abstain

if gh(x) ≥ h
otherwise

(3)

That is, the model makes a prediction only if the selec-
tion function gh is conﬁdent enough (i.e., gh(x) ≥ h) and
abstains from making a prediction otherwise. Although
conceptually the model is now deﬁned by two functions
f (the original model) and gh (the selection function), it is
possible to adapt the original classiﬁcation problem such
that a single function f (cid:48) encodes both. To achieve this,
an additional abstain label is introduced and a function
f (cid:48) : X → Y ∪ {abstain} is trained in the same way as f
(i.e., same network architecture, hyper-parameters, etc.)
with two exceptions: (i) f (cid:48) is allowed to predict the ad-
ditional abstain label, and (ii) the loss function used to
train f (cid:48) is changed to account for the additional label.
After f (cid:48) is obtained, the selection function is deﬁned as
gh := 1 − f (cid:48)(x)abstain, that is, to be the probability of

selecting any label other than abstain according to f (cid:48).
Then, f is deﬁned to be re-normalized probability distribu-
tion obtained by taking the distribution produced by f (cid:48) and
assigning zero probability to abstain label. Essentially,
as long as there is sufﬁcient probability mass h on labels
outside abstain, f decides to select one of these labels.

Loss function for abstaining To gain an intuition be-
hind the loss function used for training f (cid:48), recall that the
standard way to train neural networks is to use cross en-
tropy loss:

(cid:96)CrossEntropy(p, y) := −

|Y|
(cid:88)

i=1

yi log(pi)

(4)

Here, for a given sample (x, y) ∈ D, p = f (x) is a vector
of probabilities for each of the |Y| classes computed by the
model and y ∈ R|Y| is a vector of ground-truth probabili-
ties. Without loss of generality, assume only a single label
is correct, in which case y is a one-hot vector (i.e., yj = 1
if j-th label is correct and zero elsewhere). Then, the cross
entropy loss for an example where the j-label is correct is
− log(pj). Further, the loss is zero if the computed proba-
bility is pj = 1 (i.e., − log(1) = 0) and positive otherwise.

Now, to incorporate the additional abstain label, the ab-
stain cross entropy loss is deﬁned as follows:

(cid:96)AbstainCrossEntropy(p, y) := −

|Y|
(cid:88)

yi log(pioi + pabstain)

i=1

(5)
Here p ∈ R|Y|+1 is a distribution over the classes (includ-
ing abstain), oi ∈ R is a constant denoting the weight
of the i-th label and pabstain is the probability assigned
Intuitively, the model either: (i) learns to
to abstain.
make “safe” predictions by assigning the probability mass
to pabstain, in which case it incurs constant loss of pabstain,
or (ii) tries to predict the correct label, in which case it po-
tentially incurs smaller loss if pioi > pabstain. If the scal-
ing constant oi is high, the model is encouraged to make
predictions even if it is uncertain and potentially makes lot
of mistakes. As oi decreases, the model is penalized more
and more for making mis-predictions and learns to make
“safer” decisions by allocating more probability mass to
the abstain label.

Obtaining a model which never mis-predicts on D For
the (cid:96)AbstainCrossEntropy loss, it is possible to always ob-
tain a model f (cid:48) that never mis-predicts on samples in D.
Such a model f (cid:48) corresponds to minimizing the loss in-
curred by Equation 5 which corresponds to maximizing
pioi + pabstain (assuming i is the correct label). This can
be simpliﬁed and bounded from above to pi +pabstain ≤ 1,

Adversarial Robustness for Code

by setting oi = 1 and for any valid distribution it holds that
1 = (cid:80)
pi∈p pi. Thus, pioi + pabstain has a global optimum
trivially obtained if pabstain = 1 for all samples in D. That
is, the correctness (no mis-predictions) can be achieved by
rejecting all samples in D. However, this leads to zero re-
call and is not practically useful.

Balancing correctness and recall To achieve both cor-
rectness and high recall, similar to Liu et. al., we train our
models using a form of annealing. We start with a high
oi = |Y|, biasing the model away from abstaining, and
then train for a number of epochs n. We then gradually
decrease oi to 1 for a ﬁxed number of epochs k, slowly
nudging it towards abstaining. Finally, we keep training
with oi = 1 until convergence. We note that the threshold
h is not used during the training. Instead, it is set after the
model is trained and is used to ﬁne-tune the trade-off be-
tween recall and correctness (precision). Further, note that
oi = 1 is used only if the desired accuracy is 100% and
otherwise we use oi = 1 + (cid:15). Here, (cid:15) is selected by de-
creasing the value oi as before but stopping just before the
model abstains from making all predictions.

Summary We described an existing technique (Liu et al.,
2019) for training a model that learns to abstain from mak-
ing predictions, allowing us to trade-off correctness (preci-
sion) and recall. A key advantage of this technique is its
generality – it works with any existing neural model with
two simple changes: (i) adding an abstain label, and (ii)
using the loss function in Equation 5. To remove clutter and
keep discussion general, the rest of our work interchange-
ably uses f (x) and (f, gh)(x).

C. Adversarial Training for Code

In Section B, we described how to learn models that are
correct on subset of the training dataset D by allowing the
model to abstain from making a prediction when uncer-
tain. We now discuss how to achieve robustness (that is,
the model either abstains or makes a correct prediction)
to a much larger (potentially inﬁnite) set of samples be-
yond those included in D via so-called adversarial train-
ing (Goodfellow et al., 2015).

training The goal of adversarial

Adversarial
train-
ing (Madry et al., 2018; Wong & Kolter, 2018; Sinha et al.,
2018; Raghunathan et al., 2018) is to minimize the ex-
pected adversarial loss:

E(x,y)∼D[ max
δ⊆∆(x)

(cid:96)(f (x + δ), y)]

(6)

In practice, as we have no access to the underlying distribu-
tion but only to the dataset D, the expected adversarial loss
is approximated by adversarial risk (which training aims

to minimize):

1
|D|

|D|
(cid:88)

(x,y)∈D

max
δ⊆∆(x)

(cid:96)(f (x + δ), y)

(7)

Intuitively, instead of training on the original samples in D,
we train on the worst perturbation of each sample. Here,
δ ⊆ ∆(x) denotes an ordered sequence of modiﬁcations
while x + δ denotes a new input obtained by applying each
modiﬁcation δ ∈ δ to x. Recall that each input x = (cid:104)p, l(cid:105)
is a tuple of a program p and a position l in that program
for which we will make a prediction. Applying a modiﬁ-
cation δ : X → X to an input x corresponds to generating
both a new program as well as updating the position l if
needed (e.g., in case the modiﬁcation inserted or reordered
program statements). That is, δ can modify all positions in
p, not only those for which a prediction is made. Further,
note that the sequence of modiﬁcations δ ⊆ ∆(x) is com-
puted for each x separately, rather than having the same set
of modiﬁcations applied to all samples in D.

Using adversarial training in the domain of code requires a
set of label preserving modiﬁcations ∆(x) over programs
which preserve the output label y (deﬁned for a given task
at hand), and a technique to solve the optimization problem
maxδ⊆∆(x) efﬁciently. We elaborate on both of these next.

C.1. Label Preserving Program Modiﬁcations

We deﬁne three types of label preserving program modiﬁ-
cations – word substitutions, word renaming, and sequence
substitutions. Note that label preserving modiﬁcations are
a strict superset of semantic preserving modiﬁcations. This
is because while label preserving modiﬁcations only re-
quire that the correct label does not change, the semantic
preserving modiﬁcations require that both the label does
not change as well as that the overall program semantics
do not change. Preserving programs semantics is for many
properties unnecessarily strict and therefore we focus on
the more general label preserving modiﬁcations.

• Word substitutions are allowed to substitute a word at
a single position in the program with another word
(not necessarily contained in the program). Examples
of word substitutions include changing constants or
values of binary/unary operators.

• Word renaming is a modiﬁcation which includes re-
naming variables, parameters, ﬁelds or methods. In
order to produce valid programs, this modiﬁcation
needs to ensure that the declaration and all usages
are replaced jointly. Because of this, renaming a sin-
gle variable in practice always corresponds to making
multiple changes to the program (i.e., |δ| > 1 unless
the variable is used only once).

Adversarial Robustness for Code

• Sequence substitution is the most general type of mod-
iﬁcation which can perform any label preserving pro-
gram change such as adding dead code or reordering
independent program statements.

The main property differentiating the modiﬁcation types is
that word renaming and substitution do not change program
structure. This is used both to compute which substitution
should be made as well as to provide formal correctness
guarantees (discussed in Section 4). Further, is it used for
efﬁcient implementation that allows us to implement word
substitutions and word renaming directly on the batched
tensors, thus making them fast. In contrast, sequence sub-
stitutions require parsing batched tensors back to programs,
applying modiﬁcations on the programs and the processing
the resulting programs back to batched tensors. As a result,
word substitutions and renaming take 0.1 second to apply
once over the full training dataset while structural modiﬁ-
cations are ≈ 70× slower and take 7 seconds.

Additionally, it is also possible to deﬁne modiﬁcations that
are not label preserving (i.e., change the ground-truth la-
bel), in which case the user has to additionally provide an
oracle that computes the correct label y. However, such
oracles are typically expensive to design and run (i.e., one
would need to run a static analysis over the program or ex-
ecute the program) and therefore label preserving modiﬁ-
cation are a preferred option whenever available.

C.2. Finding Adversarial Examples

Given a program x, associated ground-truth label y, and
a set of valid modiﬁcations ∆(x) that can be applied
over x, our goal is to select a subset of them δ ⊆ ∆(x)
such that the inner term in the adversarial risk formula
maxδ⊆∆(x) (cid:96)(f (x + δ), y) is maximized. Solving for the
optimal δ is highly non-trivial since: (i) δ is an ordered
sequence rather than a single modiﬁcation, (ii) the set of
valid modiﬁcations ∆(x) is typically very large, and (iii)
the modiﬁcation can potentially perform arbitrary rewrites
of the program (due to sequence substitutions). Thus, we
focus on solving this maximization approximately, inline
with how it is solved in other domains. In what follows, we
discuss three approximate approaches to achieve this and
discuss their advantages and limitations.

C.2.1. GREEDY SEARCH

The ﬁrst approach is a greedy search that randomly samples
a sequence of modiﬁcations δ ⊆ ∆(x). The sampling can
be performed for a predeﬁned number of steps with the goal
of maximizing the adversarial risk, or until an adversarial
example is found (i.e., f (x + δ) (cid:54)= f (x)). Concretely, for a
given input x = (cid:104)p, l(cid:105), let us deﬁne the space of valid mod-
iﬁcations ∆(x) ⊆ ∆(p, l1) × ∆(p, l2) × · · · × ∆n(p, ln)
as the Cartesian product of possible modiﬁcation applied

to each position in the program l1:n. We select δ us-
sample a threshold value
ing the following procedure:
t ∼ N (0.1, 0.4) and apply the modiﬁcation at each loca-
tion with probability t. If |∆i(p, li)| > 1, then the modiﬁ-
cation to apply is sampled at random from the set ∆i(p, li).
Sampling of the threshold value t is done per each sample x
and ensures variety in the number of modiﬁcations applied.

Limitations and advantages The main advantage of this
technique is that it is simple, easy to implement, and very
fast. Given its simplicity, this technique is independent of
the actual modiﬁcation and applies equally to words substi-
tutions, word renamings as well as sequence substitutions.
However, a natural limitation of this technique is that is
uses no information about which positions and which val-
ues are important to the prediction is used.

C.2.2. GRADIENT-BASED SEARCH

Similar to prior works, gradient information can be used to
guide the search for an adversarial examples. This can done
in two ways – (i) ﬁnding a program position to change, and
(ii) ﬁnding both a program positions as well as the new
value to change. To ﬁnd a program position, we can use
gradients to measure the importance of each position a for
a given prediction in the same way as described in Sec-
tion 3. Once the attribution score a is computed, the adver-
sarial attack can be generated by sampling positions to be
modiﬁed proportionally to a, instead of the uniform sam-
pling used in the greedy search.

Additionally, as shown in the concurrent work (Yefet et al.,
2019), the gradients can also be used to select both the pro-
gram position and the new value to be used (instead of sam-
pling from all valid values uniformly at random).

Limitations and advantages The main advantage of
gradient-based approach is that the decision of which po-
sition to changes as well as what the new value should
be is guided, rather than random. Further, for renaming
modiﬁcations, such approach has shown to be quite ef-
fective (Yefet et al., 2019) at ﬁnding the adversarial ex-
amples. However, the main limitation of this approach is
that it works only for replacing single value (i.e., word sub-
stitutions and word renaming) and not when the value is
a complex structure (i.e., sequence substitution). Sequence
substitutions are important class of modiﬁcations which are
however hard to optimize for as in general, they can per-
form arbitrary changes to the program (e.g., adding dead
code, adding/removing statements, etc.).

C.2.3. REDUCING THE SEARCH SPACE

The third technique is orthogonal to the ﬁrst two and aims
to reduce the search space of relevant modiﬁcations a pri-
ori, rather than searching it efﬁciently. Concretely, for

Adversarial Robustness for Code

a position li in the program p at which the prediction is
made, it reﬁnes the set of valid program modiﬁcations as
∆(x) ⊆ (cid:81)
∆(p, lj) for all positions {lj | lj ∈ l1:n ∧
lj
reachable(lj, li)}. Here, we use reachable(lj, li) to de-
note that position lj can affect position li. When repre-
senting programs as graphs, this can be computed a priori
by checking the reachability between the two correspond-
ing nodes. Additionally, when used together with gradient
based optimization, such check is not necessary as the gra-
dients will naturally be zero. To obtain a program repre-
sentation where dependencies between many program lo-
cations are removed, we learn to reﬁne program represen-
tation as described in Section 3 and Appendix D.

Limitations and advantages The main advantage of this
approach is that it applies to both renaming and structural
modiﬁcations. The main disadvantage is that it depends
on the fact the dependencies between program locations
can be check efﬁciently and learned as part of the train-
ing. While we show how this can be done for graph neural
networks, our approach currently does not support other
models such as recurrent neural networks.

Summary
In this section, we described how adversarial
attacks can be applied to code via set of program modiﬁ-
cations. The adversarial attacks we consider are applied on
the discrete input (i.e., the attack always correspond to a
concrete program) rather than considering attacks in the la-
tent space that are not directly interpretable. We describe
two existing techniques that can be used to guide the search
for adversarial attacks (greedy search and gradient-based
search) and one makes the attacks easier by reducing the
search space. As such, these techniques are quite general
and can be applied to number of tasks over code. In our
experiments, we use the greedy search technique together
with reducing the search space.

D. Learning to Reﬁne Representation

In this section, we provide formal deﬁnition of the integer
linear program (ILP) encoding used to solve the optimiza-
tion problem presented in Section 3. Recall, that the prob-
lem statement is as follows.

Problem statement Minimize the expected size of the
reﬁnement α ⊆ Φ subject to the constraint that the ex-
pected loss of the model f stays approximately the same:

arg min
α⊆Φ

(cid:88)

|α(x)|

(x,y)∈D

(8)

subject to

(cid:80)

(x,y)∈D (cid:96)(f (x), y) ≈ (cid:80)

(x,y)∈D (cid:96)(f (α(x)), y)

Our problem statement is quite general and can be di-
rectly instantiated by: (i) using (cid:96)AbstainCrossEntropy as the
loss (Appendix B), and (ii) using adversarial risk (Ap-
pendix C).

The motivation of solving Equation 8 by phrasing it as
ILP problem is that existing off-the-shelve ILP solvers can
solve it efﬁciently and produce the optimal solution. We
discuss an alternative end-to-end solution that does not de-
pend on an external ILP solver at the end of Section 3.

Optimization via integer linear programming To solve
Equation 2 efﬁciently, the key idea is that for each sample
(x, y) ∈ D we: (i) capture the relevance of each node to the
prediction made by the model f by computing the attribu-
tion a(f, x, y) ∈ R|V | (as described in Section 3), and (ii)
include the minimum number of edges necessary for a path
to exist between every relevant node (according to the attri-
bution a) and the node where the prediction is made. Pre-
serving all paths between the prediction and relevant nodes
encodes the constraint that the expected loss stays approx-
imately the same.

Concretely, let us deﬁne a sink to be the node for which the
prediction is being made while sources are deﬁned to be all
nodes v with attribution av > t. Here, the threshold t ∈ R is
used as a form of regularization. To encode the sources and
the sink as an ILP program, we deﬁne an integer variable
rv associated with each node v ∈ V as:

rv =






v(cid:48)∈V \{v} rv(cid:48)

− (cid:80)
(cid:98)100 · av(cid:99)
0

if v is predicted node [sink]
else if av > t
otherwise

[sources]

That is, rv for a source is its attribution value converted to
an integer and rv for a sink is a negative sum of all source
values. Note that in our deﬁnition it is not possible for a sin-
gle node to be both source and a sink. For cases when the
sink node has a non-zero attribution, this attribution is sim-
ply left out since every node is trivially connected to itself.

We then deﬁne our ILP formulation of the problem as
shown in Figure 3. Here costq is an integer variable as-
sociated with each edge feature and denotes the edge ca-
pacity (i.e., the maximum amount of ﬂow allowed to go
trough the edge with this feature), fst is an integer vari-
able denoting the amount of ﬂow over the edge (cid:104)s, t(cid:105), the
constraint 0 ≤ fst ≤ costφ((cid:104)s,t(cid:105)) encodes the edge capacity,
and rv + (cid:80)
{t|(v,t)∈E} fvt encodes the
ﬂow conservation constraint which requires that the ﬂow
generated by the node rv together with the ﬂow from all
the incoming edges (cid:80)
{s|(s,v)∈E} fsv has to be the same
as the ﬂow leaving the node (cid:80)
{t|(v,t)∈E} fvt. The solu-
tion to this ILP program is a cost associated with each

{s|(s,v)∈E} fsv = (cid:80)

Adversarial Robustness for Code

minimize (cid:80)
∀((cid:104)V, E, ξV , ξE(cid:105), y) ∈ D

q∈Φ costq

subj. to

0 ≤ fst ≤ costφ((cid:104)s,t(cid:105))
rv + (cid:80)

{s|(s,v)∈E} fsv = (cid:80)

{t|(v,t)∈E} fvt

∀(cid:104)s, t(cid:105) ∈ E [edge capacity]
∀v ∈ V

[ﬂow conservation]

Figure 3. Formulation of the reﬁnement problem from Equation 8 as a minimum cost maximum ﬂow integer linear program.

(a) Original

Graph G

(d) Abstracted

(b) Graph Nodes V

Graph α(G)

V

ξV

a

rv

ﬂow conservation constraints

1

5

2

4

3

2

6

4

q3

3

1

q3

5

q7

6

A 0.3
1
A
2
0
B
3
0
C
4
0
5
B
0
6 D 0.7

r1 = −70
r2 = 0
r3 = 0
r4 = 0
r5 = 0
r6 = 70

r1 + f21 + f31 = f12 + f13
r2 + f12 + f42 + f52 = f21 + f24 + f25
r3 + f13 + f63 = f31 + f36
r4 + f24 = f42
r5 + f25 = f52
r6 + f36 = f63

Optimization Problem

minimize (cid:80)7

i=1 costqi

subj. to

edge capacity constraints
ﬂow conservation constraints

Solution

costq3,q7 = 70 ∧ costq1,q2,q4,q5,q6 = 0

α = {q3, q7}

E

(cid:104)1, 2(cid:105)
(cid:104)1, 3(cid:105)
(cid:104)3, 1(cid:105)
(cid:104)5, 2(cid:105)

(cid:104)6, 3(cid:105)

ξE

ast
ast
ast
ast

ast

(c) Graph Edges E
φ((cid:104)s, t(cid:105))

edge capacity constraints

q1 = (cid:104)ast, A, A(cid:105)
q2 = (cid:104)ast, A, B(cid:105)
q3 = (cid:104)ast, B, A(cid:105)
q3 = (cid:104)ast, B, A(cid:105)
. . .
q7 = (cid:104)ast, D, B(cid:105)

0 ≤ f12 ≤ costq1
0 ≤ f13 ≤ costq2
0 ≤ f31 ≤ costq3
0 ≤ f52 ≤ costq3

0 ≤ f63 ≤ costq7

Figure 4. Illustration of ILP encoding from Figure 3 on a single graph where the prediction should be made for node 1.

edge feature q ∈ Φ.
If the cost for a given edge fea-
ture is zero, it means that this feature was not relevant
and can be removed. As a result, we deﬁne the reﬁnement
α = {q | q ∈ Φ ∧ costq > 0} to contain all edge features
with non-zero weight.

Example As a concrete example, consider the initial
graph shown in Figure 4a and assume that the prediction
is made for node 1. For simplicity, each node has a sin-
gle attribute ξV , as shown in Figure 4b, and all edges are
of type ast. The edge feature for edge (cid:104)1, 3(cid:105) is there-
fore (cid:104)ast, A, B(cid:105), since ξE((cid:104)1, 3(cid:105)) = ast, ξV (1) = A and
ξV (3) = B, as shown in Figure 4c. The attribution a re-
veals two relevant nodes for this prediction – the node it-

self with score 0.3 and node 6 with score 0.7. We there-
fore deﬁne a single source r6 = 70 and a sink r1 = −70
and encode both the edge capacity constraints, and the ﬂow
conservation constraints as shown in Figure 4 (note that ac-
cording to Figure 3, we would encode all samples in D
jointly). The minimal cost solution assigns cost 70 to edge
features q3 and q7 which are needed to propagate the ﬂow
from node 6 to node 1. The graph obtained by applying
the abstraction α = {q3, q7} is shown in Figure 4d and
makes the prediction independent of the subtree rooted at
node 2. Notice however, that an additional edge is included
between nodes 5 and 2. This is because α is computed us-
ing local edge features φ only, which are the same for edges
(cid:104)3, 1(cid:105) and (cid:104)5, 2(cid:105).

