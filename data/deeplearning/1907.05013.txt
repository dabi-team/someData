9
1
0
2

l
u
J

1
1

]

G
L
.
s
c
[

1
v
3
1
0
5
0
.
7
0
9
1
:
v
i
X
r
a

Proﬁling based Out-of-core Hybrid Method
for Large Neural Networks

Yuki Ito ∗1, Haruki Imai2, Tung Le Duc2, Yasushi Negishi2,
Kiyokuni Kawachiya2, Ryo Matsumiya †1, and Toshio Endo1

1Tokyo Institute of Technology
2IBM Research - Tokyo

Abstract

1

INTRODUCTION

GPUs are widely used to accelerate deep learning
with NNs (NNs). On the other hand, since GPU
it is diﬃcult to im-
memory capacity is limited,
plement eﬃcient programs that compute large NNs
on GPU. To compute NNs exceeding GPU mem-
ory capacity, data-swapping method and recomput-
ing method have been proposed in existing work.
However,
in these methods, performance overhead
occurs due to data movement or increase of compu-
tation. In order to reduce the overhead, it is impor-
tant to consider characteristics of each layer such as
sizes and cost for recomputation. Based on this direc-
tion, we proposed Proﬁling based out-of-core Hybrid
method (PoocH). PoocH determines target layers of
swapping or recomputing based on runtime proﬁling.
We implemented PoocH by extending a deep learn-
ing framework, Chainer, and we evaluated its perfor-
mance. With PoocH, we successfully computed an
NN requiring 50 GB memory on a single GPU with
16 GB memory. Compared with in-core cases, per-
formance degradation was 38 % on x86 machine and
28 % on POWER9 machine.

∗Currently, Yahoo! JAPAN Corporation
†Currently, DeNA Co., Ltd.

1

Recently, neural networks (NNs) have archived high
accuracy in many ﬁelds of machine learning such as
image recognition, speech recognition, and natural
language processing [9] [10] [2] [16]. Neural networks
are computation model originally based on human
brain’s learning mechanisms.

Since computations of NNs, especially for deep net-
works, are heavy tasks, GPUs have been widely used
to accelerate them. However, the problem sizes of
NNs that can be computed are limited by GPU mem-
ory capacity. For example, ResNet50 network [3] with
the batch size of 640 requires 50 GB memory. Even
the latest GPU, NVIDIA Tesla V100, has only 16
GB or 32 GB memory, thus it is hard to support such
cases on a GPU. An approach is to use multiple GPUs
by harnessing data parallelism in mini-batched learn-
ing. However, in the case of 3D image learning[4],
memory requirement may reach dozen GBs or more
even with the batch size of 1. For such cases, a dif-
ferent approach is required.

This paper focuses on “out-of-core” methods in or-
der to exceed GPU memory capacity. There are two
well known methods, data-swapping method [8] [1]
[7] and recomputing method [15]. To support larger
computations, both of them suﬀer from performance
overhead in diﬀerent ways. The former suﬀers from
CPU-GPU communication costs and the latter intro-
duces increase in computation amounts. The better

 
 
 
 
 
 
choice depends on characteristics of each NN layer,
such as data size and computation type. Thus the
“hybrid” approach is promising, where we apply ei-
ther data-swapping or recomputing to NN layers.
However, the optimum choice is not trivial, since we
need to consider the entire NN structure, character-
istics of each layer, and also the execution environ-
ment. For example, the overhead of data-swapping
on a computer with PCI Express3.0 (the CPU-GPU
bandwidth is 16 GB/sec) is larger than a computer
with NVLINK2.0 (75 GB/sec). Therefore, the op-
timal swap/recompute targets are diﬀerent in these
two environments.

In this paper, we propose the Proﬁling-based out-
of-core Hybrid method (PoocH) for accelerating the
computation of large NNs. The objective of PoocH
is to reduce the performance overhead by optimiz-
ing targets of swapping and recomputing based on
runtime proﬁling. If memory capacity allows, some
layers may be kept on GPU memory with no perfor-
mance overhead. Also we improve the scheduling of
data movement in swapping.

We implemented PoocH by extending Chainer [13],
a deep learning framework. The performance eval-
uation is done using larger NNs on two diﬀerent
machines, an Intel Xeon based x86 machine and a
Power9 machine, each of which is equipped with a sin-
gle V100 GPU with 16GB memory. The results show
our hybrid method successfully reduce overheads; the
performance degradation with the ResNet50 network
compared with in-core cases is 38 % on x86 machine
and 28 % on POWER9 machine.

2 BACKGROUND

2.1 Neural Network

Figure 1 shows an example of the structure of NN. An
NN is composed of multiple layers, each of which is
composed of multiple feature maps. Feature maps of
a layer are computed from the previous layer’s feature
maps.

According to the computation types, each layer is
categorized into several groups: convolutional layer,
pooling layer, Batch-Normalization (BN) layer, fully-

Figure 1: Structure of NN

connected layer, and so on. In some kinds of layer
layer, parameters including
such as convolutional
weight ﬁlters are required for computation. Gener-
ally, the objective of learning process of NN is to ﬁnd
optimal values for these parameters.

The back-propagation method is a common learn-
ing method of NN, which updates parameters with
values derived from errors between NN’s output and
correct output. In this algorithm, the following pro-
cesses are performed repeatedly.

1. Forward propagation: Feature maps of each
layer are computed with training data as net-
work input. The computation goes from left to
right in the ﬁgure.

2. Backward propagation: Gradients for feature
maps, weight ﬁlter, etc. are computed using the
errors between the output of the forward step
and the correct output. The computation goes
from right to left.

3. Update: NN’s parameters are updated using the

gradients.

A sample computation timeline of an NN is drawn
in Figure 2. The length of each box represents the
computation time of a layer. The green and blue
boxes indicate forward-propagation and backward-
propagation respectively. The integer in each box
indicates the order of forward-propagation. Since up-
date computation is very short usually, it is omitted
in the ﬁgure. As in the ﬁgure, each propagation is
computed layer by layer. While forward-propagation
is conducted from input layer to output layer, back-
propagation is from output layer to input layer.

2

InputlayerConvolutionlayerOutputlayer・・・・・・・・・・・・Feature mapInputdataPoolinglayerFigure 2: Timeline of computation of NN (#layers=8)

Computation of backward-propagation of layer i
requires the feature maps, which has been computed
in forward propagation of layer i. Thus, feature maps
need to be preserved during the period.

In NN computation, “batch processing” technique
is popular in order to utilize the degree of parallelism.
With this technique, several input data (for example
several images) are simultaneously given as input of
the above algorithm. The number of input data is
called “batch size”.

2.2 Memory usage of NN

As described above, in the back-propagation method,
feature maps should be preserved until they are con-
sumed by backward computation. Also parameters
such as weight ﬁlter and gradients require memory.
Additionally, some computation layers may require
additional workspace to accelerate the computation.
Among them, the feature maps consume major
parts. The total usage is determined by the number
of layers. And the memory usage per layer increases
in proportion to the batch size and the feature map
size.

The computation would be faster if all of them are
allocated on GPU memory. However, large NN may
require larger memory then GPU memory capacity.
For example, the memory usage for computation of
ResNet50 network is shown in Figure 3. The memory
usage is proportional to bath size and exceeds 50 GB
with the batch size of 640. As another example, the
memory usage of ResNext101 for 3D data [14] [4] with
batch size of 1 is shown in Figure 4. Memory usage
increases in proportion to input data size, and reaches
58 GB in the rightmost case.

On the other hand, the memory capacity of Tesla
V100 GPU we used is 16GB and it is 32 GB even on
the latest product. Thus we require “out-of-core” ex-
ecution method to support the large NNs on a single

Figure 3: Memory usage of ResNet50

Figure 4: Memory usage of ResNext101 for 3D data
(batch size = 1)

GPU.

3 Methods to compute large

scale NN

In order to deal with data exceeding GPU memory
capacity in deep learning, data-swapping method and
recomputing method have been proposed.

3.1 Data-swapping method

In the data-swapping method, a part of the data used
in the forward computation of a layer is swapped
out to the CPU memory. An example of swap-out
is shown in Figure 5. Here forward step of a layer
computes data Y from data X on a GPU. After that
(mainly due to memory shortage) X is swapped out
to CPU memory. We cannot simply discard X since
it is necessary in the later backward computation as
shown in Figure 6. Prior to the backward computa-

3

(1) Swap-inCPU memoryGPU memoryX(2) ComputeCPU memoryGPU memorydXdYXdYBackward(1) ComputeCPU memoryGPU memoryYX(2) Swap-outCPU memoryGPU memoryYXForward723243546576swap-outcomputeswap-in745342312010time00116756Swap-out start after computation Idle timeForwardBackwardComputation start after swap-in 723456computetime01ForwardBackward054321760102030405060128256384512640Memory usage [GB]Batch size0102030405060(224*224)*128(224*224)*256(448*224)*128(448*224)*256Memory usage [GB]Input size [(height * width) * length]computation, where data dependency is considered.
We observe several idle time regions, shown in red
boxes in the ﬁgure. The largest idle region would
be eliminated simply by keeping data of layers 6, 7
on GPU without swapping, if the memory capacity
allows. However, there are still other idle regions,
which happen when computation time is too short to
hide communication costs.

3.2 Recomputing method

The recomputing method takes another approach to
deal with small memory capacity. Some parts of the
data computed in forward is simply discard without
being stored anywhere. When the discarded data is
required for backward computation, the data is re-
produced by performing forward computation again.
An example is shown in Figures 8 and 9. Here Y
is computed from X, and Z is computed from Y in
forward computation. After that, the data Y is dis-
carded. After that, when Y is needed in backward
(Figure 9), Y is recomputed by performing forward
computation by using X. If not only Y but X have
been discarded, we would need recomputation recur-
sively by using predecessor of X. This method suﬀers
not from communication costs but computation costs
for recomputing.

3.3 Hybrid method

it is possible
By using one of the two methods,
to compute the large scale NNs. However, data-
swapping method causes CPU-GPU communication
overhead, and recomputing method causes overhead
due to increase of computation. The overheads have
the following features, respectively.

• Data-swapping method:

If NN includes many
layers with large computation complexity (such
as convolution layer), the computation can hide
the communication. If layers with small compu-
tation complexity (such as BN layer) are major-
ity, overhead reduction is diﬃcult.

• Recomputing method: If NN includes many lay-
ers with small computation complexity, the out-
put data of the layer can be easily reproduced.

Figure 5: Swap-out at forward in data-swapping
method

Figure 6: Swap-in at backward in data-swapping
method (dX and dY are gradients corresponding to
X, Y)

tion of this layer, X on CPU memory is copied to
GPU memory (swapping-in).

Impact on performance

This method raises overhead for copying data be-
tween GPU and CPU. This overhead can be reduced
by a well-known technique, overlapping of communi-
cation and computation. However, the overhead is
not well hidden for the following reasons.

The computations and swapping-in/out have to be
done while keeping data dependency. Swapping-out
for certain data must wait for all the forward compu-
tations that use the data. Likewise, backward com-
putation of each layer must wait for all the necessary
data to be swapped in.

Figure 7 shows an example of the timeline of NN

4

(1) Swap-inCPU memoryGPU memoryX(2) ComputeCPU memoryGPU memorydYdXXdYdXBackward(1) ComputeCPU memoryGPU memoryYX(2) Swap-outCPU memoryGPU memoryYXForward(1) Swap-inCPU memoryGPU memoryX(2) ComputeCPU memoryGPU memorydXdYXdYBackward(1) ComputeCPU memoryGPU memoryYX(2) Swap-outCPU memoryGPU memoryYXForward723243546576swap-outcomputeswap-in745342312010time00116756Swap-out start after computation Idle timeForwardBackwardComputation start after swap-in 723456compute543210time0176ForwardBackwardFigure 7: Example of the timeline of NN computation processing with data-
swapping (#layers=8). The yellow and orange boxes represent the execution
time of swaps corresponding to each layer’s computation. In this example,
data-swapping is performed in all layers and each swap-in starts simultane-
ously with the computation of the next layer.

Figure 8: Free memory at forward in recomputing
method

Figure 9: Recomputation at backward in recomput-
ing method (dY and dZ are gradients corresponding
to Y, Z)

For layers with large computation complexity,
recomputation overhead is large.

We observe that the two methods can complement
each other. Hence, utilizing both methods is promis-
ing. In the later of this paper, we call this approach
“hybrid” methods.

One of hybrid methods is adopted in SuperNeu-
rons [6]. While this method uses both swapping
and recomputing, the decision criteria is very simple
and determined by computation types of the target
layer. For example, output of convolution layers are
swapped out, and output of BN layers are recom-
puted. This does not take into account the actual
execution time and memory usage during NN compu-
tation. On the other hand, the computation complex-
ity and memory usage of each layer diﬀers depending
on the structure of NNs. Therefore, we expect that
the performance will be improved by deciding tar-

gets of swapping and recomputing with considering
the quantitative characteristics of NNs and execution
environments.

4 POOCH

We propose Proﬁling-based out-of-core Hybrid
method (PoocH) to accelerate the computation of
large scale NNs. The key to reduce the performance
overhead is to optimize targets of swapping and re-
computing based on runtime proﬁling.

5

(1) Swap-inCPU memoryGPU memoryX(2) ComputeCPU memoryGPU memorydXdYXdYBackward(1) ComputeCPU memoryGPU memoryYX(2) Swap-outCPU memoryGPU memoryYXForward723243546576swap-outcomputeswap-in745342312010time00116756Swap-out start after computation Idle timeForwardBackwardComputation start after swap-in 723456computetime01ForwardBackward05432176(1) ComputeGPU memoryYX(2) FreeGPU memoryForwardZXZ(1) RecomputeGPU memory(2) ComputeGPU memoryXBackwarddZYdYXdZY(1) ComputeGPU memoryYX(2) FreeGPU memoryForwardZXZ(1) RecomputeGPU memory(2) ComputeGPU memoryXBackwarddZYdYXdZY4.1 Overview of PoocH

4.1.1 Optimization target

In PoocH, each data structure used in the computa-
tion of NN falls into one of following classes:

• keep : The data resides on GPU memory.

• swap : The data is swapped out to CPU memory.
When it is required later, swapped in to GPU
memory.

• recompute : The data is discarded. When it is

required, it is recomputed.

The objective of PoocH is to ﬁnd a good classiﬁcation
for data structures in a given NN, which realizes an
execution with a short execution time, while exclud-
ing out-of-memory errors.

In the current PoocH, the classiﬁcation is per-
formed only to the feature maps of NN layers. Other
data such as weight ﬁlters and gradients are retained
in the GPU memory. This is because the parameters
such as the weight ﬁlters tend to be much smaller
than feature maps, and the lifetimes of gradient data
tend to be short.

computation time, swapping time of each layer by us-
ing runtime proﬁling. To sum up, the ﬂow of PoocH
is as follows.

1. Proﬁling: PoocH runs learning iterations for sev-
eral times while recording necessary runtime in-
formation of each layer

2. Classiﬁcation: PoocH decides a classiﬁcation of
all feature maps that achieves minimal runtime

3. Execution: The system continues learning itera-
tions based on the classiﬁcation that have chosen

The rest of this chapter describes the proﬁling
phase and the classiﬁcation phase. Additionally, we
explain additional improvements in overlapped exe-
cution in Section4.3.

4.2 Runtime proﬁling

The ﬁrst step of PoocH is to collect the following
factors, which are necessary for predicting execution
time and memory usage.

• Execution time of each forward/ backward com-

putation

• Execution time of each swapping-out/ swapping-

in communication

4.1.2 Methodology of PoocH

• Execution timing of each swapping

The purpose of the optimization in PoocH is to re-
duce “the execution time of the whole computation
of NN” while satisfying a constraint “GPU memory
usage does not always exceed the GPU memory ca-
In order to perform
pacity” during the execution.
the optimization process, PoocH needs to makes sug-
gestions of classiﬁcations repeatedly and evaluate the
execution time and maximum memory usage given by
each classiﬁcation. However, it is diﬃcult to formu-
late the execution time with a simple linear equation
due to pipelined processing and data dependency.

Instead, when a classiﬁcation is chosen, PoocH sim-
ulates an execution timeline and memory manage-
ment processes. By this simulation, the execution
time and memory usage of the entire process can be
predicted. In order to achieve this simulation, PoocH
collects necessary information beforehand including

• The sizes and order of malloc/ free operations

on GPU memory

• Computation graph and dependency of the NN

The static prediction of all these factors is diﬃcult.
For example, the execution timing of each swapping
depends on the NN structure, thus prediction in com-
plex NNs with many branches such as GoogLeNet
[12] is diﬃcult. Other memory management and
order of computation depend on implementation of
deep learning program.

Thus PoocH executes a runtime proﬁling during
the ﬁrst several iterations of the learning process.
During the proﬁling execution, all feature maps are
classiﬁed into swap as the default classiﬁcation. With
this classiﬁcation, we can collect the above informa-
tion.

6

1. Each feature map falls into keep or swap tenta-

tively (section 4.4.2).

2. The targets of this phase are maps in swap in the
prior step. They are classiﬁed again into swap or
recompute.

In each search step, when PoocH takes a classiﬁ-
cation, it uses a simulation based on results of the
runtime proﬁling beforehand in order to estimate the
execution time and the memory usage with the clas-
siﬁcation.

4.4.2 Classiﬁcation of keep and swap

In the ﬁrst step of the classiﬁcation, each feature map
falls into keep or swap. Since the brute-force method
is impractical, we describe heuristics to reduce the
number of combinations to be searched.

The safest classiﬁcation, which is not likely to cause
out-of-memory, is one that classiﬁes all feature maps
into swap. From this classiﬁcation as the start point,
we ﬁnd the feature maps that cause overheads, which
are maps whose swapping time cannot be hidden by
computation even with pipelined execution described
in Section 4.3. In this step, the search is performed
as follows.

1. The timeline in case all

feature maps are
swapped is simulated. Here we obtain a time-
line as in Figure 11.

2. The timeline is examined to ﬁnd feature maps
that do NOT cause additional overhead, as ex-
plained below. They are classiﬁed into swap im-
mediately in order to reduce the search space in
the next step.

3. For other (undecided in the above step) feature
maps, PoocH executes a semi brute-force search
in order to classify them into keep or swap.

We explain examination of the timeline in the
above item 2. Here we obtain a set LO of feature
maps whose swapping-out time is NOT hidden by
computation time.
In Figure 11, LO = {5, 6, 7}
(surrounded by the red dotted line). Similarly, we
ﬁnd a set LI by examining swapping-in, which is

Figure 10: Example of swap-in scheduling

4.3 Swapping-in scheduling

This section describes improvement in pipelined
swapping-in execution to reduce communication
In a simple execution of backward com-
overhead.
putation with swapping (leftside in Figure 10),
swapping-in of data for a layer (layer 4 in the ﬁgure)
is overlapped with computation of the previous layer
(layer 5). However, this causes unnecessary stalls.
The overhead due to synchronization is reduced by
moving up the schedule of swapping-in as in right-
side in the ﬁgure. When this is done, GPU memory
usage has to be also considered.

Through this discussion, PoocH simply executes
swapping-in when there is room in the GPU memory.
The amount of free memory at each time of backward
can be judged from proﬁling result described in sec-
tion 4.2.

This improved scheduling is adopted both in the
simulations in the classiﬁcation phase and the execu-
tion phase.

4.4 Feature maps classiﬁcation

4.4.1 Classiﬁcation policy

In the classiﬁcation phase, PoocH classiﬁes each fea-
ture map into one of three classes, keep, swap, recom-
pute. The purpose is to ﬁnd a classiﬁcation that ac-
celerates NN computation. However, since the com-
bination of the classiﬁcation is too large (O(3n) where
n denotes the number of layers in the NN.), the brute-
force method may take a long time. Instead, we in-
troduce heuristics.

The classiﬁcation phase consists of the following
two steps, each of which adopts heuristics for eﬃcient
search.

7

computeswap-int453456Early swap-in453456Idle timeFigure 11: Example of feature maps that cause overhead by swapping. In this example, swap surrounded
by the red dotted line cannot be overlapped by computation.

Figure 12: Search tree in decision of keep-targets and
swap-targets

Figure 13: Reduction of swap-out. Swap-outs are
reduced in order from output layer (layer 7 → 6 →
5)

LI = {4, 6, 7} in the ﬁgure. For feature maps not
included in LO or LI (0, 1, 2 and 3 in this exam-
ple), the swapping-out/in times are hidden by com-
putation.
In our heuristics, these feature maps are
classiﬁed into swap immediately. For feature maps
in LO ∪ LI , classes are determined in the next search.

Reduction of search space considering swap-
out

Next, PoocH classiﬁes feature maps in LO ∪LI (layers
4 to 7 in the example) into keep or swap. We per-
formed the brute-force search of the target feature
maps, whose search tree looks like Figure 12. How-
ever, even with a technique described above, we have
found the search space of size O(2|LO∪LI |) is still too
large with deep NNs.

In order to solve this problem, we considered fur-
ther reduction of the search space. For this purpose,
we again investigated the execution timeline of NN
computation processing in detail. Then we found
that swapping-out tasks which cannot be overlapped
by computation tend to be continuous at the end of

forward processing on the execution timeline like the
red dotted line of Figure 13. This is because (1)
each forward computation does not wait swapping-
out and (2) each swap-out cannot started unless
the corresponding forward computation is completed.
On the other hand, the tendency is much weaker in
swapping-in in backward computation.

From this observation, we distinct LO and LI .
We adopt a light-weight greedy method for LO. As
shown on the right side of Figure 13, we can expect
that swapping-out costs can be reduced from the back
simply. In other words, those feature maps are classi-
ﬁed into keep as long as it can ﬁt in the GPU memory
in order from the output layer.

On the other hand, we still use brute-force search
for feature maps in LI as shown in Figure 14. To sum
up, we execute the classiﬁcation for given LO and LI
as follows.

• We make a binary search tree, each of whose

level corresponds to an element in LI .

• At each leaf of the search tree, feature maps in
LI have been classiﬁed, while maps in LO \ LI

8

723243546576swap-outcomputeswap-in74534231200time001167561layer 5:swapkeeplayer 6:layer 7:swap all layerslayer 4:723243546576swap-outcomputeswap-in74534231200time001167561layer 5:swapkeeplayer 6:layer 7:swap all layerslayer 4:723243546576swap-outcomputeswap-in74534231200time001167561layer 5:swapkeeplayer 6:layer 7:swap all layerslayer 4:layer 6:swapkeeplayer 7:swap all layerslayer 4:remove swap-out:745676swap-outcomputet7456Remove swap-outrecomputing X. These two functions are obtained
through the timeline simulation described in Section
4.1.2.

r(X) < 1.0 means that the overhead for X is
smaller with recomputing than with swapping. Es-
pecially, as r(X) is smaller, more overhead is likely
to be reduced by switching from swap to recompute.
On the other hand, if r(X) > 1.0, swap is better.
Considering the above, this step is performed in the
following ﬂow.

1. We let be L the set of feature maps classiﬁed

into swap in the ﬁrst step of search.

2. r(X) is evaluated for each feature map X in L.

3. All X that satisfy r(X) ≥ 1.0 are removed from

L, and classiﬁed into swap.

4. We pick up an X that satisﬁes “r(X) < 1.0” and
“r(X) is the smallest”. We remove X from L,
and classify into recompute.

5. If L is empty, the search is terminated. Other-

wise we return to (2).

The compute complexity of this step is O(|L|2),

which is much smaller than that of step 1.

5 EVALUATION

We have implemented PoocH as an extension to
Chainer version 3, a deep learning framework devel-
oped by PFN[13]. Chainer implicitly supports com-
putation on GPU by calling its own CUDA kernel
and NVIDIA’s cuDNN library APIs [11]. However,
the original Chainer fails the execution with large
NNs that require more memory than GPU memory
capacity.

We have implemented the runtime proﬁling mech-
anism and the classiﬁcation algorithm of PoocH in-
side Chainer. Especially, GPU memory management
functions (allocate and free) of Chainer are hooked
in order to keep track of GPU memory usage in the
proﬁling phase.

This section shows performance evaluation results
of this extended Chainer. As execution environ-
ments, we used two machines, an Intel Xeon based

Figure 14: Search tree in decision of keep-targets and
swap-targets (modiﬁed)

are not classiﬁed yet. We scan elements in LO \
LI linearly and switch a feature map from swap
to keep, and evaluate the entire classiﬁcation by
simulating the total execution time.

In the example we are using, LO \ LI = {5},
whose elements are scanned in the later step. Gen-
erally, the search space size of the current algorithm
is O(2|LI ||LO \ LI |). While it still includes an expo-
nential term, we have conﬁrmed that it is feasible for
practical deep NNs such as ResNet and ResNext as
shown in Section 5.

4.4.3 Classiﬁcation of recompute

The targets of the second step are feature maps that
have been classiﬁed into swap in the ﬁrst step ten-
tatively. They are examined again and some feature
maps are changed to recompute.

In this step, it is important to compare “the over-
head by data-swapping” and “overhead by recomput-
ing” for each feature map. Based on the overhead
evaluation, each feature map is classiﬁed into a class
that causes less overhead. The decision for feature
map X is done based on the following function r(X).

r(X) =

recompute overhead(X)
swap overhead(X)

(1)

Here, swap overhead(X) is the overhead caused by
swapping X when classes of other feature maps are
ﬁxed (thus its value depends not only on X but on
others). Also,
recompute overhead(X) is the overhead caused by

9

723243546576swap-outcomputeswap-in74534231200time001167561layer 5:swapkeeplayer 6:layer 7:swap all layerslayer 4:layer 6:swapkeeplayer 7:swap all layerslayer 4:remove swap-out:Table 1: The evaluation environment (x86 machine)

taneously with the previous computation.

GPU
GPU memory capacity
CPU
CPU memory capacity
CPU-GPU interconnect
CPU-GPU bandwidth
OS
CUDA
cuDNN

NVIDIA Tesla V100
16 GB
Intel Xeon Gold 6140
192 GB
PCIe gen3 x16
16 GB/sec
CentOS 7.4
CUDA 9.1
cuDNN 7.1

Table 2:
POWER9 machine)

The

evaluation environment

(IBM

GPU
GPU memory capacity
CPU
CPU memory capacity
CPU-GPU interconnect
CPU-GPU bandwidth
OS
CUDA
cuDNN

NVIDIA Tesla V100
16 GB
IBM POWER9
1 TB
NVLink2.0 x2
75 GB/sec
RHEL 7.5 (Maipo)
CUDA 9.2
cuDNN 7.1

machine (x86 machine) and a POWER9 machine. In
each machine, a single Tesla V100 GPU with 16 GB
memory is used. The details of evaluation environ-
ments are shown in the Table 1 and Table 2. Between
the two machines, it is important that CPU-GPU in-
terconnects are diﬀerent. The x86 machine adopts
ordinary PCI-Express gen3 link and the POWER9
machine adopts NVLink2.0, which is more than four
times faster than PCI-Express gen3.

5.1 Evaluation of each optimization

As described in chapter 4, PoocH has several opti-
mization steps, swapping timing, classiﬁcation into
swap/keep, and classiﬁcation into recompute. We
evaluate the impact on performance by each opti-
mization by comparing the following four methods.

• swap-all(w/o scheduling) : All feature maps are
swapped. Each swapping-in simply starts simul-

• swap-all : All feature maps are swapped. The
timing of swapping-in is improved as described
in Section 4.3.

• swap-opt : Feature maps are classiﬁed into keep
or swap by conducting only Step 1 in Section
4.4.2.

• PoocH : Feature maps are classiﬁed into keep,
swap, recompute by conducting Step 1 and Step
2 in Section 4.4.

“Swap-opt” and “PoocH” also uses optimization of

the timing of swapping.

Figure 15 shows the comparison results on x86
machine.
In the experiment, we used ResNet50
[3], and AlexNet [5]. All three are NNs for im-
age recognition and the batch sizes are conﬁgured
so that problem sizes exceed the GPU memory ca-
pacity. In this experiment, the speeds are given by
batch size divided by execution time per learning iter-
ation [#images/s]. The graph shows relative speed-
up where the base case is “swap-all(w/o scheduling)”.
In Figure 15, we observe that “swap-all” improves
the performance by 2-14 % compared to swap-all
(w/o scheduling). This is because the execution
timing of each swap-in is moved up so that the
CPU-GPU communications became likely to be over-
lapped by computation. The performance of “Swap-
opt” is improved further by keeping some data on
GPU memory and by reducing the communication
amounts. As a result, the performance is improved
by x1.4 to x3.0 compared with “swap-all”.

For all of three NNs, PoocH with all optimiza-
tions shows the highest performance. Especially
the PoocH’s performance was x1.45 swap-opt for
ResNet50. We consider this large diﬀerence comes
from the characteristics of ResNet50. ResNet50 has
many layers with small computation complexity with
respect to the feature map sizes. Therefore, recom-
puting such feature maps tends to cause less overhead
than swapping. On the other hand, for NNs with
large computational cost like AlexNet, recomputa-
tion was rarely chosen because there is computation
time enough to overlap swap. Thus, the performance

10

5.2 Comparison with existing meth-

ods

In this experiment, we computed ResNet50 and
In addition, we
AlexNet with various batch sizes.
computed ResNext101 (3D) [14] for various input
data sizes with batch size of 1. ResNext101 (3D)
is an extension of ResNext101 for video recognition
based on [4]. The input data of this NN is 3D data,
and the required memory may exceed GPU memory
capacity even with batch size of 1.

The computation of each NN was performed by the

following three methods, respectively.

• in-core : Neither data-swapping nor recomput-

ing is performed.

• superneurons : The classiﬁcation based on Su-

perNeurons [6] is used.

• PoocH : Our proposed system based on runtime

proﬁling is used.

SuperNeurons uses the following hybrid classiﬁca-

tion algorithm.

• Feature maps are stored on GPU memory pref-

erentially from output layer.

• Among the feature maps that do not ﬁt in the
GPU memory, the feature maps of convolution
layer are targets of swapping. The feature maps
of layers with other types are recomputed.

• Each swap-in starts simultaneously with the
computation of the immediately preceding con-
volution layer.

For the measurement, we have implemented this al-
gorithm by extending Chainer.

In this experiment, the experimental results do not
include proﬁling and optimization time of PoocH.
Although it gets longer with the number of layers,
we observed that it was about 2 minutes even for
resnext101 with ¿300 layers. Thus, it can be easily
amortized by speed-up of learning when the whole
learning takes hours or days.

The results for ResNet50 are shown in Figures 17
and 18. Each ﬁgure corresponds to results on the x86

Figure 15: Evaluation of each optimization on x86
machine (speed up for swap-all (w/o scheduling))

Figure 16: Evaluation of each optimization on
POWER9 machine (speed up for swap-all (w/o
scheduling))

diﬀerence between PoocH and swap-opt for Alexnet
is small.

Figure 16 shows the results on POWER9 ma-
chine. We observe PoocH showed the best perfor-
mance also on this machine.
In the ﬁgure, we ob-
serve the diﬀerences between “swap-opt” and PoocH
are small compared to those in Figure 15. This is
because the overhead of data-swapping is originally
small since NVLink accelerates CPU-GPU communi-
cation on POWER9 machine. In addition, thanks to
high speed NVLink, swapping communications can
be suﬃciently overlapped by computation even for
NNs with a small computation complexity such as
ResNet50.

11

Batch sizein-coresuperneuronsPoocH0123resnet50(256)alexnet(2432)Speed-upNN (Batch size)swap-all(w/o scheduling)swap-allswap-optPoocH00.511.5resnet50(256)googlenet(384)alexnet(2432)Speed-upNN (Batch size)swap-all(w/o scheduling)swap-allswap-optPoocHmachine and the POWER9 machine, respectively.
We observe that when the batch size is set to 256 or
more, the memory usage exceeded the GPU memory
capacity and “in-core” execution fails. When batch
size is 640, the memory usage was more than 50 GB.
PoocH successfully computes this case.

In Figure 17, the performance of in-core was 316
[#images/s], and the performance of PoocH was 195
to 316 [#images/s]. For the problem size exceed-
ing the GPU memory capacity, PoocH can perform
computation with 13 − 38% performance degradation
compared with “in-core” case.

Both in PoocH and superneurons, performances
are degraded as the batch size increases. This is
due to an increase of feature maps that cannot be
stored on the GPU memory as batch size increases.
For larger cases, we need to make more feature maps
swapped or recomputed, which leads performance
degradation.

However, comparing the two, PoocH shows x1.40 -
x1.73 better performances in the range of batch size
256 to 512. With PoocH, targets of swapping and
recomputation are determined by considering actual
each layer’s computation time and each swap commu-
nication time, not the type of layer. As a result, per-
formance was improved compared with static method
like superneurons.

Besides, execution of superneurons fails with batch
size of 640. This is because superneurons schedules
swapping-in without considering the actual memory
usage, resulting in GPU memory shortage. With
PoocH, such problems do not occur since swap-in
communications are scheduled considering memory
usage based on proﬁling.

Figure 18 shows the performance on a POWER9
machine. Here performance degradation of PoocH
compared with in-core was 2−28%. On this machine,
NVLink accelerates CPU-GPU communication, so
the overhead of data-swapping is small. As a result,
performance degradation in the case of POWER9 was
smaller than in the case of x86.

We also note that PoocH can capture the diﬀer-
ences in characteristics of the two machines. Table 3
shows the number of feature maps classiﬁed as keep,
swap or recompute for ResNet50 when using PoocH
or superneurons on both environments. From the ta-

Figure 17: Performance for ResNet50 on x86 machine

Figure 18: Performance for ResNet50 on POWER9
machine

ble, we conﬁrmed that PoocH classiﬁes more feature
maps into recompute on the x86 machine than on the
POWER9 machine. This is because PoocH tends to
prefer recomputing in the x86 machine with slower in-
terconnect. On the other hand, superneurons makes
the same classiﬁcation on the two machines.

In addition, we computed ResNet50 on x86 ma-
chine with the PoocH’s optimization results for
POWER9 machine (Figure 17).
In Figure 17, us-
ing optimization results for x86 machine resulted in
better performance than using optimization results
for POWER9 machine. Besides, execution using op-
timization results for POWER9 machine fails with
batch size of 640 on x86 machine. The order of
malloc/free are diﬀerent since the execution time of
computation/data-swapping are diﬀerent on both en-
vironments. Therefore, using the classiﬁcation ac-
cording to the diﬀerent environment may cause GPU
memory shortage.

12

Performance [#images/s]Batch sizein-coresuperneuronsPoocHPerformance [#images/s]0100200300128256384512640Performance [#images/s]Batch sizein-coresuperneuronsPoocHoptimized for POWER9 machine0100200300128256384512640Performance [#images/s]Batch sizein-coresuperneuronsPoocHTable 3: The number of feature maps classiﬁed as
keep, swap or recompute for ResNet50 (batch size =
512) when using PoocH or superneurons (on both
environments)

#keep #swap #recomp

PoocH (x86)
superneurons (x86)
PoocH (POWER9)
superneurons (POWER9)

66
66
66
66

12
21
36
21

27
18
3
18

As above, PoocH successfully optimizes the classi-
ﬁcation according to the execution environment and
shows better performance on both machines.

The results for AlexNet are shown in Figures 19
and 20. In both ﬁgures, the performance degradation
of PoocH was less than 6.1% compared with in-core.
This is because layers in AlexNet have large compu-
tation complexities, so recomputation is rarely done
when using PoocH, and swap communication could
be overlapped suﬃciently. Also, the performance dif-
ference between PoocH and superneurons was small.
Finally, the results for ResNext101 (3D) are shown
in Figures 21 and 22. With this NN, the memory
consumption depends on the sizes of 3D input data.
When we use large images as input, the memory re-
quirement exceeds GPU memory capacity even with
batch size of 1. PoocH successfully compute such
cases.

For ResNext101 (3D), the performance degrada-
tion of PoocH compared with “in-core” case was less
than 10% on both environments. Computation of
NNs for 3D data tends to have a large computa-
tion complexity, thus swap communication was suﬃ-
ciently overlapped by computation. PoocH also had
better performance compared to superneurons.

We have shown that PoocH improves the compu-
tation performance of large NNs. It is based on opti-
mizing according to the structure of the NN, the data
size and the execution environment.

6 RELATED WORK

As described in section 3.3, SuperNeurons has been
proposed by Wang et al [6]. SuperNeurons uses the

Figure 19: Performance for AlexNet on x86 machine

Figure 20: Performance for AlexNet on POWER9
machine

hybrid method of data-swapping and recomputing.
Their method statically decides classiﬁcation unlike
PoocH. We have shown that PoocH enables faster ex-
ecutions using classiﬁcation algorithm based on run-
time proﬁling.
Rhu et al.

proposed vDNN to compute large
scale NNs using data-swapping method [8]. Although
vDNN optimize classiﬁcation of swapping,
it does
not incorporate the recomputing method. Also, Cho
et al.
implemented the data-swapping method on
Chainer [7], and our Chainer is an extension of theirs.
Ito’s ooc cuDNN library performs data-swapping af-
ter dividing each computation and data [17].
ooc cuDNN supports NNs where memory consump-
tion of a single layer exceeds the GPU memory capac-
ity. By integrating PoocH and ooc cuDNN, PoocH
will support NNs of wider ranges.

Chen et al. proposed to compute large-scale NNs
by using recomputing method [15]. PoocH combines
recomputing method with data-swapping method.

13

0400800120016002304243225602688Performance [#images/s]Batch sizein-coresuperneuronsPoocHPerformance [#images/s]0400800120016002304243225602688Performance [#images/s]Batch sizein-coresuperneuronsPoocHPerformance [#images/s]References

[1] C.Meng, M.Sun, J.Yang, M.Qiu, and Y.Gu.
Training Deeper Models by GPU Memory Opti-
mization on TensorFlow. In Proceedings of ML
Systems Workshop in NIPS, 2017.

[2] F.Seide, G.Li, and D.Yu. Conversational Speech
Transcription Using Context-Dependent Deep
Neural Networks. In Proceedings of Interspeech,
pages 437–440, 2011.

[3] H.Kaiming, Z.Xiangyu, R.Shaoqing, and S.Jian.
Deep residual learning for image recognition. In
Proceedings of CVPR, 2016.

[4] K.Hara, H.Kataoka, and Y.Satoh. Can spa-
tiotemporal 3d cnns retrace the history of 2d
In Proceedings of CVPR,
cnns and imagenet?
2018.

[5] A. Krizhevsky, I. Sutskever, and G. Hinton. Im-
ageNet Classiﬁcation with Deep Convolutional
Neural Networks. In Proceedings of NIPS, pages
1097–1105, 2012.

[6] L.Wang, J.Ye, Y.Zhao, W.Wu, A.Li, S.Song,
Zenglin Xu, and Tim Kraska. SuperNeurons:
Dynamic GPU memory management for train-
ing deep neural networks.
In Proceedings of
PPoPP, 2018.

[7] M.Cho, T.Le, U.Finkler, H.Imai, Y.Negishi,
T.Sekiyama, S.Vinod, V.Zolotov, K.Kawachiya,
D.Kung, and H.Hunter. Large Model Support
for Deep Learning in Caﬀe and Chainer. In Pro-
ceedings of SysML, 2018.

[8] M.Rhu, N.Gimelshein, J.Clemons, A.Zulﬁqar,
and S.Keckler. vDNN: Virtualized Deep Neural
Networks for Scalable, Memory-Eﬃcient Neu-
ral Network Design. In Proceedings of MICRO,
pages 1–13, 2016.

[9] Q.Le,

J.Ngiam,

A.Coates,

B.Prochnow, and A.Ng.
methods for deep learning.
ICML, pages 265–272, 2011.

A.Lahiri,
On optimization
In Proceedings of

Figure 21: Performance for ResNext101 (3D) on x86
machine

Figure 22: Performance for ResNext101 (3D) on
POWER9 machine

7 SUMMARY AND FUTURE

WORK

This paper described PoocH that supports eﬃcient
execution of large neural networks that require more
memory then GPU memory capacity. It reduces the
performance overhead by determining target layers of
swapping or recomputing based on runtime proﬁling.
In addition, PoocH schedules swapping-in eﬃciently.
By using PoocH, the performance degradation when
computing NNs exceeding the GPU memory capac-
ity is reduced to 38% on x86 machine and 28% on
POWER9 machine.

The current version of PoocH targets only NNs
that compute the same problem size in each learn-
ing iteration. As future work, we will extend PoocH
in order to deal with NNs whose problem sizes change
for each iteration.

14

0500100015002000(224*224)*128(224*224)*256(448*224)*128(448*224)*256Performance [#voxels/ms]Input size[(height * width) * length]in-coresuperneuronsPoocH0500100015002000(224*224)*128(224*224)*256(448*224)*128(448*224)*256Performance [#voxels/ms]Input size[(height * width) * length]in-coresuperneuronsPoocH[10] A. Radford, L. Metz, and S. Chintala. Unsu-
pervised representation learning with deep con-
volutional generative adversarial networks.
In
Proceedings of ICLR, 2016.

[11] S.Chetlur,

C.Woolley,

P.Vandermersch,
and
B.Catanzaro,
cuDNN: Eﬃcient Primitives

J.Tran,

J.Cohen,
E.Shelhamer.
for Deep Learning. 2014.

[12] S.Christian,

L.Wei,

J.Yangqing,

S.Pierre,
R.Scott, A.Dragomir, E.Dumitru, V.Vincent,
and R.Andrew. Going deeper with convolutions.
In Proceedings of CVPR, 2015.

[13] S.Tokui, K.Oono,

S.Hido,

and J.Clayton.
a Next-Generation Open Source
In Proceedings

Chainer:
Framework for Deep Learning.
of ML Systems Workshop in NIPS, 2015.

[14] S.Xie, R.Girshick, P.Dollar, Z.Tu, and K.He.
Aggregated residual transformations for deep
neural networks. In Proceedings of CVPR, 2017.

[15] T.Chen, B.Xu, C.Zhang, and C.Guestrin. Train-
ing Deep Nets with Sublinear Memory Cost.
2016.

[16] Oriol Vinyals and Quoc V Le. A neural conver-
sational model. In Proceedings ofLearning Work-
shop in ICML, 2015.

[17] Y.Ito, R.Matsumiya, and T.Endo. ooc cuDNN:
Accommodating Convolutional Neural Networks
over GPU Memory Capacity. In Proceedings of
BigData, 2017.

15

