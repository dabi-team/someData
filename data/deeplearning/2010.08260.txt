0
2
0
2

t
c
O
6
1

]

V

I
.
s
s
e
e
[

1
v
0
6
2
8
0
.
0
1
0
2
:
v
i
X
r
a

Quantitative Digital Microscopy with Deep Learning

Benjamin Midtvedt, Saga Helgadottir, Aykut Argun, Jes´us Pineda, Daniel Midtvedt, and Giovanni Volpe
Department of Physics, University of Gothenburg, Origov¨agen 6B, SE-41296 Gothenburg, Sweden
(Dated: October 19, 2020)

Video microscopy has a long history of providing insights and breakthroughs for a broad range
Image analysis to extract quantitative information from
of disciplines, from physics to biology.
video microscopy data has traditionally relied on algorithmic approaches, which are often diﬃcult
to implement, time consuming, and computationally expensive. Recently, alternative data-driven
approaches using deep learning have greatly improved quantitative digital microscopy, potentially
oﬀering automatized, accurate, and fast image analysis. However, the combination of deep learning
and video microscopy remains underutilized primarily due to the steep learning curve involved
in developing custom deep-learning solutions. To overcome this issue, we introduce a software,
DeepTrack 2.0, to design, train and validate deep-learning solutions for digital microscopy. We use
it to exemplify how deep learning can be employed for a broad range of applications, from particle
localization, tracking and characterization to cell counting and classiﬁcation. Thanks to its user-
friendly graphical interface, DeepTrack 2.0 can be easily customized for user-speciﬁc applications,
and, thanks to its open-source object-oriented programming, it can be easily expanded to add
features and functionalities, potentially introducing deep-learning-enhanced video microscopy to a
far wider audience.

I.

INTRODUCTION

During the last century, the quantitative analysis of
microscopy images has provided important insights for
various disciplines, ranging from physics to biology. An
early example is the pioneering experiment performed
by Jean Perrin in 1910 that demonstrated beyond any
doubt the physical existence of atoms [1]: he manually
tracked the positions of microscopic colloidal particles in
a solution by projecting their image on a sheet of pa-
per (Fig. 1A) and, despite a time resolution of just 30
seconds, he managed to quantify their Brownian motion
and connect it to the atomic nature of matter.
In the
following decades, several scientists followed in Perrin’s
footsteps, improving the time resolution of the exper-
iment down to seconds [2, 8] (Fig. 1B). Despite these
improvements, manual tracking of particles intrinsically
limits the time resolution of conceivable experiments.

In the 1950s, analog electronics provided some tools
to increase acquisition and analysis speed. According to
Preston [9], the history of digital microscopy begins in
Britain in 1951 with an unlikely actor: the British Na-
tional Coal Committee convened to investigate “the pos-
sibility of making a machine to replace the human ob-
server” to measure coal dust in mining operations [10].
In 1955, Causley and Young developed a ﬂying-spot mi-
croscope to count and size particles and cells [3]: The
ﬂying-spot microscope used a cathode-ray tube to scan
a sample pixel by pixel, while the cells were counted and
sized by a simple analog integrated circuit (Fig. 1C). This
device allowed over an order of magnitude faster count-
ing than human operators, while maintaining the same
accuracy.

During the 1950s and in earnest in the 1960s, re-
searchers started employing digital computers to add
speed and functionalities to microscopic image analysis,
with a growing focus on biomedical applications. In 1965,

Prewitt and Mendelsohn managed to distinguish cells in
a blood smear by analyzing with a computer their im-
ages obtained by a ﬂying-spot microscope and recorded
In the follow-
as 8-bit values on a magnetic tape [11].
ing years, digital microscopy went from research labs to
clinical settings with the development of the computer-
ized tomography scanner (CT-scanner) in 1972 [12] and
of the automated ﬂow cytometer in 1974 [13].

In soft matter physics, despite the early success of
Perrin’s experiment [1], most studies focused on the en-
semble behavior of colloidal particles employing methods
such as selective photobleaching and image correlation
[14–16]. These methods can resolve fast dynamics, but
they can only measure the average behavior of a homo-
geneous colloidal solution [16]. To overcome these lim-
itations, Geerts et al. automated particle tracking in
1987, developing what is now known as single particle
tracking, and used it to track individual gold nanopar-
ticles on the surface of living cells from images acquired
with a diﬀerential interference contrast (DIC) microscope
[4] (Fig. 1D). In the following decades, researchers have
also used ﬂuorescent molecules [17–19] and quantum dots
[20, 21] as tracers within biological systems.

It became quickly evident that highly accurate tracking
algorithms were needed to analyze the collected data. In
1996, Crocker and Grier proposed an algorithm to deter-
mine particle positions based on the measurement of the
centroids of their images [5] (Fig. 1E). The main advan-
tage of this algorithm is that it is largely setup-agnostic,
i.e., it does not depend on the speciﬁc properties of the
imaging system and of the particle. Other setup-agnostic
approaches have been proposed in more recent years an-
alyzing, e.g., the Fourier transform of the particle image
[22], or its radial symmetry [23]. Other algorithms, in-
stead, made a model of the image based on the properties
of the imaging system and of the particle [16, 24–29].
These alternative methods were less general and often

 
 
 
 
 
 
2

Figure 1. Brief history of quantitative microscopy and particle tracking. A-B 1910–1950: The manual analysis era.
A Examples of manually tracked trajectories of colloids in a suspension from Perrin’s experiment that convinced the world of
the existence of atoms [1]. The time resolution is 30 seconds. B Kappler manually tracked the rotational Brownian motion of
a suspended micromirror to determine the Avogadro number [2]. C-E 1951-2015: The digital microscopy era. C Causley and
Young developed a computerized microscope to count particles and cells using a ﬂying-spot microscope and an analog analysis
circuit [3]. D Geerts et. al. developed an automatized method to track single gold nanoparticles on the membranes of living
cells [4]. E Crocker and Grier kickstarted modern particle tracking, achieving high accuracy using a largely setup-agnostic
approach [5]. F-I 2015-2020: The deep-learning-enhanced microscopy era. F Ronnerberger et. al. developed the U-Net, a
variation of a convolutional neural network that is particularly suited for image segmentation and has been very successful for
biomedical applications [6]. G Helgadottir et. al. developed a software to track particles using convolutional neural networks
(DeepTrack 1.0) and demonstrated that it can achieve higher tracking accuracy than traditional algorithmic approaches [7].
J This article presents DeepTrack 2.0, which provides an integrated environment to design, train and validate deep learning
solutions for quantitative digital microscopy.

more computationally expensive, but they often achieved
higher accuracy and could also provide quantitative in-
formation about the particle, such as its size [30] or its
out-of-plane position [31–33]. Despite the large number
of methods being introduced, digital video microscopy
remained a hard problem, requiring the development of
ad hoc algorithms tuned to the needs of each experiment.
In fact, a 2014 comparison of 14 tracking methods found
that, when compared on several diﬀerent simulated sce-

narios, no single algorithm performed best in all scenarios
[34].

Only in the last few years has machine learning started
to be employed for the analysis of images obtained from
digital microscopy. This comes in the wake of the deep
learning revolution [35], thanks to which computer-vision
task such as image recognition [36], semantic segmenta-
tion [37], and image generation [38], are now automatized
with relative ease. Recent results have demonstrated the

Manual analysisDigital microscopyDeep learningAtomic theoryconfirmationA1910High-resolutionBrownian motionB1931Digital microscopeC1955Single particletrackingD1987Digital videomicroscopyE1995U-NetF2015Particle trackingG2019DeepTrack 2.0H2020DetectionSegmentationClassificationpotential of applying deep learning to microscopy, vastly
improving techniques for particle tracking [7, 39, 40], cell
segmentation and classiﬁcation [6, 41–44], particle char-
acterization [39, 45, 46], object counting [47], depth-of-
ﬁeld extension [48], and image resolution [49, 50].
In
2015, Ronnerberger et. al. developed a special kind
of neural network (U-Net) for the segmentation of cell-
images [6] (Fig. 1F), which is now widely used for the
segmentation of biomedical images. In particle tracking,
Hannel et. al. employed deep learning to track and mea-
sure colloids from their holographic images [39], Newby
et. al. demonstrated how deep learning can be used for
the simultaneous tracking of multiple particles [40], and
Helgadottir et. al. achieved tracking accuracy surpass-
ing standard methods [7] (Fig. 1G). These early successes
clearly demonstrate the potential of deep learning to an-
alyze microscopy data. However, they also point to a
key limiting factor for the development and deployment
of deep-learning solutions to microscopy: the availability
of high-quality training data. In fact, training data often
need to be experimentally acquired speciﬁcally for each
application and, especially for biomedical applications, to
be manually annotated by experts, which are expensive,
time-consuming and potentially biased processes [51].

In this article, we provide a brief review of the ap-
plications of deep learning to digital microscopy and
we introduce a comprehensive software (DeepTrack 2.0,
Fig. 1H) to design, train and validate deep-learning solu-
tions for quantitative digital microscopy. In section II,
we review the main applications of deep learning to
microscopy and the most frequently employed neural-
network architectures. In section III, we introduce Deep-
Track 2.0, which greatly expands the functionalities of
the particle-tracking software DeepTrack 1.0 [7], and fea-
tures a user-friendly graphical interface and a modu-
lar (object-oriented) architecture that can be easily ex-
panded and customized for speciﬁc applications. Finally,
in section IV, we demonstrate the versatility and power
of deep learning and DeepTrack 2.0 by using it to tackle a
variety of physical and biological quantitative digital mi-
croscopy challenges, from particle localization, tracking
and characterization to cell counting and classiﬁcation.

II. DEEP LEARNING FOR MICROSCOPY

In this section, we will start by providing an overview
of machine learning and deep learning, in particular in-
troducing and comparing the deep-learning models that
are most commonly used in microscopy: fully connected
neural networks, convolutional neural networks, convo-
lutional encoder-decoders, U-Nets, and generative adver-
sarial networks. Subsequently, we will review some key
applications of deep learning in microscopy focusing on
image enhance-
image segmentation,
three key areas:
ment, and particle tracking.

Image segmentation partitions an image into multiple
segments each corresponding to a speciﬁc object (e.g.,

3

cells of diﬀerent kinds).
In this context, deep learning
has been very successful, especially in the segmentation
of biological and biomedical images. However, one limit-
ing factor is the need for high-quality training datasets,
which often need to be annotated by experts manually, a
time-consuming and tedious task.

Image enhancement includes tasks such as noise reduc-
tion, deaberration, refocusing and superresolution. Also
in this case, deep learning has been widely employed in
the last few years, especially in the context of computa-
tional microscopy. Diﬀerently from image segmentation,
image enhancement can often utilize training datasets
that are directly acquired from experiments.

Particle tracking deals with the localization of objects
(often microscopic colloidal particles or tracer molecules)
in 2D or 3D. Deep-learning-powered solutions are more
accurate than algorithmic approaches, work in extremely
diﬃcult environments with poor signal-to-noise ratio,
and can extract quantitative information about the par-
ticles. Particle-tracking algorithms can often be trained
using simulated data by employing physical simulations
of the required images.

A. Deep learning

In contrast to standard computer algorithms where
the user is required to deﬁne explicit rules to process
the data, machine-learning algorithms can learn patterns
and rules to perform speciﬁc tasks directly from series
of data. In supervised learning, machine-learning algo-
rithms learn by adjusting their behavior according to a
set of input data and corresponding desired outputs (the
ground truth). These input–output pairs constitute the
training dataset, which can be obtained either from ex-
periments or from simulations.

Deep learning is a kind of machine learning built on
artiﬁcial neural networks (ANN) [35]. ANNs were origi-
nally conceived to emulate the capabilities of the brain,
speciﬁcally its ability to learn [52]. They are constituted
by interconnected artiﬁcial neurons (simple computing
units often just returning a non-linear function of their in-
puts). Often, these artiﬁcial neurons are organized in lay-
ers (typically with tens or hundreds of artiﬁcial neurons).
In the most commonly employed architectures, each layer
receives the output of the previous layer, computes some
transformation, and feeds the result into the next layer.
In many machine vision applications, the number of lay-
ers is in the tens (this number is the “depth” of the ANN;
hence the term “deep learning”).

The weights of the connections between artiﬁcial neu-
rons and layers are the parameters that are adjusted in
the training process. The training can be broken down
into the following steps (referred to as the error back-
propagation algorithm [53]): First, the ANN receives an
input and calculates a predicted output based on its cur-
rent weights. Second, the output is compared to the true,
desired output, and the error is measured using a loss

function. Third, the ANN propagates this error back-
wards, calculating for each weight whether it should be
increased or decreased in order to reduce the error (and
a local estimate of the rate of change of the error de-
pending on that weight). Finally, the weights are up-
dated using an optimiser, which determines how much
each weight should be changed. By feeding the network
additional training data, it typically improves its per-
formance, gradually converging to some optimum weight
conﬁguration.

In microscopy applications, the most commonly em-
ployed ANN architectures are dense neural networks,
convolutional neural networks, convolutional encoder-
decoders, U-Nets, and generative adversarial networks
(Table I).

The workhorse of ANNs are dense neural networks
(DNNs), which consist of a series of layers fully connected
in sequence. While suﬃciently large DNNs can approx-
imate any function [54], the number of weights required
quickly grows to unmanageable levels, especially for large
inputs such as images. Furthermore, they present a rigid
structure, where the dimensions of both the input and
output are ﬁxed. Therefore, when analyzing images, they
are rarely used on their own, while they are often em-
ployed as the ﬁnal steps of some other network to gener-
ate the ﬁnal output from already pre-processed data.

In contrast, convolutional neural networks (CNNs) are
particularly useful to analyze images. They are primar-
ily built upon convolutional layers. In each convolutional
layer, a series of 2D ﬁlters are convolved with the input
image, producing as output a series of feature maps. The
size of the ﬁlters with respect to the input image deter-
mines the features that can be detected in each layer.
To gradually detect larger features, the feature maps are
downsampled after each convolutional layer. The down-
sampled feature maps are then fed as input to the next
network layer. There is often a dense top after the last
convolutional layer, i.e., a relatively small DNN that in-
tegrates the information contained in the output feature
maps of the last layer to determine the sought-after re-
sult.

Convolutional encoder-decoders are convolutional neu-
ral networks constituted by two paths. First, there is
the encoder path, which reduces the dimensionality of
the input through a series of convolutional layers and
downsampling layers, therefore encoding the information
about the original image. Then, there is the decoder path,
which uses the encoded information to reconstruct either
the original image or some transformed version of it (e.g.,
in segmentation tasks). Therefore, when trained to re-
construct the input image at the output, the information
at the end of the encoder path can serve as a compressed
version of the input image. When trained to reconstruct
a transformed version of the input image, the encoded
information can serve as a powerful representation of the
input image useful for the speciﬁc task at hand.

4

decoder convolutional paths, U-Nets feature also forward
concatenation steps between corresponding levels of these
two paths. This permits them to preserve positional
information lost when the image resolution is reduced.
They have been particularly successful in analyzing and
segmenting biological and biomedical images.

Diﬀerently from the previous case, generative adversar-
ial networks (GANs) combine two networks, a generator
and a discriminator, regardless of their speciﬁc architec-
tures [55]. The generator manufactures data, usually im-
ages, from some input. The discriminator, in turn, clas-
siﬁes its input as either real data or synthetic data cre-
ated by the generator. The term adversarial refers to the
fact that these two networks compete against each other:
The generator tries to fool the discriminator with manu-
factured data, while the discriminator tries to expose the
generator. The generator can be trained to either trans-
form images by feeding it with a real image as input or
to make up images by feeding it with a random input.
The generator is typically either a convolutional encoder-
decoder or a U-Net, while the discriminator is often a
convolutional neural network. While GANs are a break-
through for data generation and oﬀer many beneﬁts, they
are diﬃcult to train and highly sensitive to hyperparam-
eter tuning: Slight changes in their overall architecture
can lead to vanishing gradients, lack of convergence, and
uncorrelated generator loss and image quality [56, 57].

B.

Image segmentation

Deep learning has been extremely successful at seg-
mentation tasks, especially for biomedical applications
[44, 59–67], but also in material science [68, 69].
Im-
age segmentation is typically used to locate objects and
boundaries in images. More precisely, image segmenta-
tion assigns a label to every pixel in an image such that
pixels with the same label share certain characteristics
(e.g., represent objects of the same type).

Generally, deep-learning models performing image seg-
mentation are trained using experimental data that need
In some cases,
to be manually annotated by experts.
to alleviate the need for annotated images, pretrained
neural networks are employed (i.e., neural networks that
have been trained for classiﬁcation tasks on a large
dataset of diﬀerent images, often not directly related to
the task at hand) and ﬁne-tuned using a relatively small
set of manually annotated data [44, 59].

If the exact topography of the sample is not needed,
one can downsample the image using several convolu-
tional layers, obtaining a coarse classiﬁcation of its var-
ious regions. For example, this approach was used by
Coudray et. al.
[42] to distinguish cancerous lung cells
from normal tissue by ﬁne-tuning a pre-trained neural
network for image analysis and object detection (Incep-
tion v3 [58]) (Fig. 2A).

U-Nets are an especially useful evolution of convolu-
tional encoder-decoders. In addition to the encoder and

Much more frequently, image segmentation uses con-
In fact,

volutional encoder-decoders and U-Nets [70].

5

Architecture

Advantages

Disadvantages

Dense Neural Network (DNN)

• Can use all available information.
• Can represent any transformation

between the input and output.

• Input and output can easily have

any dimensions.

• The number of weights increases
quickly with the number of layers
and the dimension of the input.
• The input and output dimensions

must be known in advance.

Convolutional Neural Network
(CNN)

• Can be constructed with a limited

number of weights.

• Highly eﬀective at extracting local

information from images.

• Analysis is position-independent.

• Cannot access global information.
• Can be computationally expensive.
• Diﬃcult to retain an exact output

shape.

Convolutional Encoder-Decoder
(CED)

• Only the number of features needs

• Positional information is lost during

to be known in advance.

downsampling.

• Returns an image in output, which
can be more interpretable by hu-
mans.

• Can be trained as an auto-encoder

without annotated data.

• Can be hard to annotate data.

U-Net

• Retains positional information.
• Only the number of features needs

• Can quickly grow large.
• Forward concatenation layers disal-

to be known in advance.

low use as an auto-encoder.

• Returns an image in output, which
can be more interpretable by hu-
mans.

Generative Adversarial Network
(GAN)

• Can create very realistic images.
• Encourages high-frequency predic-

• Very hard to train.
• The outputs are designed to look

tions.

correct, not to be correct.

• Can be trained without annotated

• Output quality very sensitive to the

data.

details of the architecture.

Table I. A comparison of common deep learning architectures. Advantages and disadvantages of deep learning architec-
tures commonly employed for microscopy, i.e., the dense neural network (DNN), the convolutional neural network (CNN), the
convolutional encoder-decoder (CED), the U-Net, and the generative adversarial network (GAN). For each model, we also show
a miniature example of the architecture, where gray lines with orange circles represent dense layers, blue rectangles represent
convolutional layers, red rectangles represent pooling layers, and magenta rectangles represent deconvolutional layers. The
arrows depict the forward concatenation steps.

Experimental DataGeneratorDiscriminator6

Figure 2.
Image segmentation with deep learning. A Lung cell classiﬁcation and mutation prediction [42], using
Inception v3 [58]: Non-overlapping tiles in the input are analyzed, returning a low-resolution segmentation mask, containing
either just a binary classiﬁcation as tumor or healthy, or a complete prediction of the mutation type. B The U-Net architecture
as originally proposed by Ronneberger [6] diﬀers from the CED by the addition of forward concatenation steps between the
encoder and decoder parts, which allow the network to forward positional information lost during encoding. The network is
used to segment nearly overlapping cells. C A cell segmentation software, based on a model closely resembling the U-Net [59],
can be automatically retrained by feeding it additional ﬂuorescence images.

U-Nets were originally developed for cell segmentation,
where one of the key requirements is to clearly mark
the cell edges, such that neighboring cells can be dis-
tinguished [6] (Fig. 2B).

High-quality image segmentation annotations are time-
consuming to obtain, which is why many researches opt
to design networks that can be retrained for a spe-
ciﬁc task using a much smaller dataset. For example,
Sadanandan et. al. developed a neural network that can
be automatically be retrained using ﬂuorescently labeled
cells [59] (Fig. 2C). With such an approach, the neural
network can relatively easily be adapted to diﬀerent ex-
perimental setups, even though the process requires some
experimental eﬀort in acquiring the additional training
data.

Segmentation has also been used for three-dimensional
images. For example, Li et. al. used a three-dimensional
convolutional neural network to reconstruct the inter-
connections between biological neurons [71]. Similar ap-
proaches have been employed also for the volumetric re-
construction of organs [72, 73].

C.

Image enhancement

Deep learning has been widely employed for image en-
hancement. This is particularly interesting because it
permits also to perform tasks on images that would be
extremely diﬃcult or impossible to do with microscopy
because of intrinsic physical limitations.

Deep learning has been employed to achieve super-
resolution by using diﬀraction-limited images to recon-
struct images beyond the diﬀraction limit. For example,
Ouyang et. al. trained a GAN to imitate the output of
the standard super-resolution method PALM [74] signif-
icantly improving the resolution of ﬂuorescence images
[50] (Fig. 3A).

An interesting application of deep learning is to realize
cross-modality analysis, where a neural network learns
how to translate the output of a certain optical device to
that of another. For example, Wu et. al. used a U-Net to
translate between holography and brightﬁeld microscopy,
enabling volumetric imaging without the speckle and ar-
tifacts associated with holography [75] (Fig. 3B). This
method uses experimental pairs of images collected si-
multaneously by two diﬀerent optical devices.

Going one step further, deep learning can also be used
to generate images that cannot be directly obtained from

ABC7

Image enhancement with deep learning. A Fluorescence superresolution localization microscopy using deep
Figure 3.
learning [50]: It uses sparse PALM [74] (optionally together with a wideﬁeld (WF) image), to construct a super-resolved image.
We see how the quality of the produced image increases with the acquisition time. B A GAN is used to transform holography
images to brightﬁeld [75]. From the top to bottom, we see holographic images of pollen, backpropagated to the focal plane,
and ﬁnally transformed into brightﬁeld images. The bottom set of images are the real brightﬁeld images for comparison. C A
GAN is used to convert quantitative phase images (in-line holography) into virtual tissue stainings, mimicking histologically
stained brightﬁeld images [76].

the sample using optical devices, but would require some
other kind of analysis of the sample. For example, Riven-
son et al. used phase information obtained from hologra-
phy to create a virtually stained sample corresponding to
a histologically stained brightﬁeld image [76] (Fig. 3C).

Image-enhancement techniques typically train net-
works using experimental images that do not need any
manual annotation. Either the target is calculated using
known methods, or it is collected simultaneously using
an alternate path for the light. While this reduces the
amount of manual labor required, both approaches have
their drawbacks. A network trained to imitate a tradi-
tional method is unlikely to improve upon it on primary
metrics.
Instead, it can improve by allowing less ideal
inputs, or decrease execution time. On the other hand,
using a dual microscope will lead to networks specialized
for the optical devices used to acquire the training im-
ages. Moreover, such a dual-purpose microscope is usu-
ally non-standard, so the user need to alter and customize
their setup.

D. Particle tracking

Single particle tracking has become a crucial tool for
probing the microscopic world. Standard approaches are
typically limited by the complexity of the system: Higher
particle densities, higher levels of noise, and more com-
plex point-spread functions often lead to worse results.
Developments using deep learning have shown that it is
possible to largely overcome these limitations. A big ad-
vantage of deep-learning solutions for particle tracking is
that often simulated data can be used to train the net-
works.

Newby et. al. demonstrated that deep learning can
be used for the detection of particles in high-density,
low-signal-to-noise-ratio images [40]. Their method uses
a small CNN to construct a pixel-by-pixel classiﬁcation
probability map of background versus particle (Fig. 4A).
Standard algorithms can then be applied to this proba-
bility map to track the particles.

Helgadottir et. al. achieved a tracking accuracy sur-
passing that of traditional methods using a convolutional
neural network with a dense top to detect particle cen-
troids in challenging conditions [7] (Fig. 4B).

Along with particle localization, deep learning has also
been used to measure other characteristics of particles.

ABC8

Figure 4. Particle tracking with deep learning. A Particle detection in dense images of varying diﬀraction patterns [40],
where a relatively small network of three convolutional layers estimates a pixel-by-pixel probability map of background versus
particle. B High-accuracy single particle localization using a CNN with a dense top [7]. The network is scanned across the
image to detect and localize all particles (dots) and bacteria (circles) in the image. C Particle tracking and characterization
in terms of radius and refractive index using in-line holography images [46], where bounding boxes for each particle in the
ﬁeld of view are extracted and fed to a CNN. They showcase accurate measurements on data for particles between 0.5 µm and
1.5 µm. D Particle characterization in terms of radius and refractive index using oﬀ-axis holography images [45]. A CNN with
latent space temporal averaging is used to measure multiple observations of a single particle to improve accuracy. This allows
characterization of particles down to around 0.2 µm.

For example, Altman et. al. used a convolutional neu-
ral network to measure the radius and refractive index of
images of colloids acquired by an in-line holographic mi-
croscope [46] (Fig. 4C). Midtvedt et al. used an oﬀ-axis
microscope and a time-average convolutional neural net-
work to measure the radius and refractive index of even
smaller particles [45] (Fig. 4D).

Moreover, deep learning has been used for micro-
tubule tracking [77], 3d tracking of ﬂuorescence im-
ages [78, 79],
intra-cellular particle detection [80, 81],
nanoparticle sizing in transmission electron microscopy
frame-to-frame linking [83], and single-
(TEM) [82],
particle anomalous diﬀusion characterization [84–86].

III. DEEPTRACK 2.0

In this section, we introduce DeepTrack 2.0, which is
an integrated software environment to design, train, and
validate deep-learning solutions for digital microscopy
[87]. DeepTrack 2.0 builds on the particle-tracking soft-
ware package DeepTrack, which we introduced in 2019
[7], and greatly expands it beyond particle tracking to-

wards a whole new range of quantitative microscopy ap-
plications, such as classiﬁcation, segmentation, and cell
counting.

To accommodate users with any level of experience in
programming and deep learning, we provide access to
the software through several channels, from a high-level
graphical user interface that can be used without any pro-
gramming knowledge, to scripts that can be adapted for
speciﬁc applications, to a low-level set of abstract classes
to implement new functionalities. Furthermore, we pro-
vide various tutorials to use the software at each level
of complexity, including several video tutorials to guide
the user through each step of a deep-learning analysis for
microscopy: from deﬁning the training image generation
routine, to deciding the neural network model, to train-
ing and validating the network, to applying the trained
network to real data.

As main entry point, we provide a completely stand-
alone graphical user interface, which delivers all the
power of DeepTrack 2.0 without requiring programming
knowledge. This is available both for Windows and for
MacOS [88].
In fact, we recommend all users to start
with the graphical user interface, which provides a visual
approach to deep learning and an intuitive feel for how

the various software components interact.

As more precise control is desired, we recommend the
users to peruse the available Jupyter notebooks [87],
which provide complete examples of how to write scripts
for DeepTrack 2.0.

For most applications, DeepTrack 2.0 already includes
all necessary components. However, if more advanced
functionalities that are not already included are required,
it is easy to extend DeepTrack 2.0 building on its frame-
work of abstract objects and its native communication
In
with the popular deep learning package Keras [89].
fact, we expect the most advanced users to expand the
functionalities of DeepTrack 2.0 according to their needs.
All users are also welcome to report any bugs and to
propose additions to DeepTrack 2.0 through its GitHub
page [87].

A. Graphical user interface

The graphical user interface of DeepTrack 2.0 provides
an intuitive way to perform the various steps that are
necessary for the realization of a deep-learning analysis
for microscopy. Through the graphical user interface,
users can deﬁne and visualize image generation pipelines
(Fig. 5A), train models (Fig. 5B-C), and analyze experi-
mental data (Fig. 5D).

A typical workﬂow is the following:

1. Deﬁne the image generation pipeline, e.g., a
pipeline to generate images of a particle corrupted
by noise.

2. Deﬁne the ground-truth training target, e.g., the
particle image without noise (image target), or the
particle position (numeric target).

3. Deﬁne a deep learning model, e.g., U-Net.

4. Train and evaluate the deep learning model.

5. Apply the deep learning model to the user’s exper-

imental data.

Projects realized with DeepTrack 2.0 can be saved and
subsequently loaded, which is useful for archival purposes
as well as to share deep learning models and results be-
tween users and platforms. Furthermore, projects can be
automatically exported as Python code, which can then
be executed to train a network command-line or imported
into an existing project.

At a more advanced level, it is possible to extend the
capabilities of the graphical user interface by adding new
Python-coded objects. We envision that this possibility
will motivate users to create and share additional soft-
ware compatible with DeepTrack 2.0.

9

B. Scripts

We provide several Jupyter notebooks both as exam-
ples of how to write scripts using DeepTrack 2.0 and as a
foundation to create customized solutions. To facilitate
this, we provide several video tutorials [87], which detail
how the solutions are constructed and how they can be
modiﬁed.

C. Code

The software architecture of DeepTrack 2.0 (Fig. 6)
is built on four main components: features, properties,
images, and deep-learning models:

Features: They are the foundations on which Deep-
Track 2.0 is built. They receive a list of images
as input, and either apply some transformation to
all of them (e.g., adding noise), or add a new image
to the list (e.g., adding a scatterer), or merge them
into a single image (e.g., imaging a list of scatter-
ers through an optical device). By deﬁning a set of
features and how they connect, we produce a sin-
gle feature that deﬁnes the entire image creation
process.

Properties: They are the parameters that determine
how features operate. For example, a property can
control the position of a particle, the intensity of
the added noise, or the amount by which an image
is rotated. They can have a constant value or be
deﬁned by a function (e.g., to place a particle at a
random position), which can depend on the value
of other properties, either from the same feature or
from other features.

Images: They are the objects on which the features op-
erate. They behave like n-dimensional NumPy ar-
rays (ndarray) and can therefore be directly used
with most Python packages. They contain a list
of the property values used by the features that
created the image, which can be used to generate
ground-truth labels for training neural networks as
well as to evaluate how the error in deep-learning
models depends on the properties deﬁning the im-
age (e.g., signal-to-noise ratio, background gradi-
ent, illumination wavelength).

Models: They are the deep-learning models. A series
of standard models is already provided in Deep-
Track 2.0, including models for DNNs, CNNs, U-
Nets, and GANs. In each case, the parameters of
the model (e.g., number of layers and number of
artiﬁcial neurons) can be deﬁned by the user.

DeepTrack 2.0 solutions depend on the interactions be-
tween these objects. In general, there are three distinct

10

Figure 5. DeepTrack 2.0 graphical user interface. A The main interface: 1: The image generation pipeline is deﬁned
using drag and drop components. 2: An image created using the pipeline and the corresponding label are shown. 3: A
comparison image is also shown to help ensure that the generated image is similar to experimental images. B The training loss
and validation loss over time can be monitored in real time during training. It is also possible to monitor custom metrics, or
any metric as a function of some property of the image (e.g., particle size, signal-to-noise ratio, aberration strength). C The
model prediction on individual images in the validation set can be compared to the corresponding target in real time during
training, providing another way to concretely visualize the improvement of the model performance over time. D Finally, the
model can be evaluated on experimental images also during training, which can help quickly hone in on a model that correctly
handles speciﬁc experimental data.

ABCD12311

Figure 6. DeepTrack 2.0 framework. A An example image-generation pipeline composed of ﬁve features. Five dis-
tinct ellipses are generated and passed to a ﬂuorescence microscope simulator, which produces an image to which a constant
background and Poisson noise are added. B The position of the ellipses are imprinted on the image, allowing us to create a
ground-truth label, where each particle is represented by a small circle. C A generator that can continuously create images
and corresponding labels is used to train a deep-learning model. Typically many thousands of images are created to train the
neural network. D The trained model is then able to analyze experimental data. (Image shown here is not experimental, and
only used for demonstration purposes.)

typical operations a feature can perform. The ﬁrst op-
eration is to add an image to a list of images (notably
Scatterers):

[ I1,
↑
[P1]

..., In
↑
[Pn]

] → F (cid:48)(P (cid:48)) → [ I(cid:48)
1,
↑
[P1]

]

..., In,
↑
[Pn]

I(cid:48)
↑
[P (cid:48)]

Here, a list of n images are fed to the feature F (cid:48). Each
of these images has a list of properties Pi, which describe
the process used to create that image. The feature is
controlled by some properties P (cid:48), and returns a new list
of images. The ﬁrst n images are unchanged, but a new
image I (cid:48) is appended to the end, on which the properties
P (cid:48) are imprinted.

erations):

[ I1,
↑
[P1]

] → F (cid:48)(P (cid:48)) → [

..., In
↑
[Pn]

I(cid:48)
1,
↑
[P1, P (cid:48)]

...,

]

I(cid:48)
n
↑
[Pn, P (cid:48)]

Here, the feature returns a list of the same length, but
each image is altered (e.g., some noise is added or it is
rotated). The properties characterizing this alteration P (cid:48)
are imprinted on all images.

The third operation is to merge several images into a

single image:

[ I1,
↑
[P1]

] → F (cid:48)(P (cid:48)) →

..., In
↑
[Pn]

[I(cid:48)]
↑
[P1, ..., Pn, P (cid:48)]

The second operation is to transform all images in the
list in some way (the standard behavior of features, in-
cluding noise, augmentations and most mathematical op-

Here, all the properties of the input images, as well as the
feature’s own properties, are imprinted on the resulting
image (notably optical devices).

PoissonBackgroundFluorescenceEllipseDuplicateSampleToMaskABData pipelineLabelNeuralnetworkTrainedmodelCDTrainingdataExperimentaldataTraininglabelModelpredictionA typical complete image generation pipeline can look

something like:

]

[] → Fs1(Ps1) → [ Is1
↑
[Ps1]
→ Fs2(Ps2) → [ Is1,
↑
[Ps1]

]

Is2
↑
[Ps2]

→ F (cid:48)(P (cid:48)) → [

→ Fo(Po) →

]

I(cid:48)
s2
↑
[Ps2, P (cid:48)]

I(cid:48)
s1,
↑
[Ps1, P (cid:48)]
[Io]
↑
[Ps1, Ps2, P (cid:48), Po]

Here, the start is an empty list. Two initial features
(Fs1, Fs2) append images to that list, creating a list of
two images (Is1, Is2) (e.g., these could be two scattering
particles int eh ﬁeld of view). Each such image is modi-
ﬁed by a feature F (cid:48) (e.g., by adding some noise), before
being merged into a single image by Fo (e.g., representing
the output of a microscope). Note that P (cid:48) is not added
to the list of properties twice; the list is in fact a set, and
cannot contain duplicate properties.

We show an even more concrete example in Fig. 6A.
Here, we have an initial feature Ellipse which creates a
single image of an ellipse. We follow this by the feature
Duplicate, which creates a ﬁxed number of duplicates
(here ﬁve). (Note that Duplicate duplicates the feature
Ellipse, not the generated image, which is why it can
create several diﬀerent ellipses, i.e., with diﬀerent radius,
intensity, or in-image position.) This list of images is
sent to the feature Fluorescence, which images them
through a simulated ﬂuorescence microscope. After this,
a background oﬀset is added, and Poisson noise is intro-
duced.

Since the positions of all ellipses are stored as proper-
ties, they are imprinted on the ﬁnal image. This allows us
to create a segmented mask, shown in Fig. 6B, which we
can use as ground-truth label to train the deep-learning
model. These two image creation pipelines (one for the
data, one for the label), are passed to a generator which
continuously creates new images by updating the prop-
erties that control the features using user-deﬁned update
rules. These images are fed to a neural network to train
it (Fig. 6C), which results in a trained model that can
analyze experimental data (Fig. 6D).

Writing code directly using the DeepTrack 2.0 frame-
work allows the user to extend the capabilities of the
package. In most cases, it is suﬃcient to use the Lambda
feature, which allows any external function to be incor-
porated into the framework. However, certain scenarios
may require the user to write custom features. For exam-
ple, the user can extend the feature Optics (features that

12

simulate optical devices) to create a new imaging modal-
ity, the feature Scatterer (features that represent some
object in the sample) to create a custom scatter, or the
feature Augmentation (features that augment an image
to cheaply broaden the input space) to expand the range
of available augmentations. It is also straightforward to
add new neural-network models: any Keras model can
be directly merged with DeepTrack 2.0 without any con-
ﬁguration, while models from other packages can easily
be wrapped.

To help users get started writing code using Deep-
Track 2.0, we provide several comprehensive video tu-
torials [87], ranging in scope from implementing custom
features to writing complete solutions.

IV. CASE STUDIES

In this section, we use DeepTrack 2.0 to exemplify
how deep learning can be employed for a broad range
of microscopy applications. We start with a standard
benchmark for image classiﬁcation:
the MNIST digit
recognition challenge [90] (Section IV A). Afterwards,
we employ Deeptrack 2.0 to analyze microscopy images.
First, we develop a model to track particles whose im-
ages are acquired by brightﬁeld microscopy, training a
single-particle tracker whose accuracy surpasses standard
algorithmic approaches especially in noisy imaging con-
ditions [7] (Section IV B). Then, we expand this example
to also extract quantitative information about the parti-
cle, namely its size and refractive index (Section IV C).
Deep learning is especially powerful in tracking multi-
ple particles in noisy environments. As a demonstra-
tion of this, we develop a model that can detect quan-
tum dots on a living cell imaged by ﬂuorescence mi-
croscopy (Section IV D). Again, we expand this exam-
ple to demonstrate three-dimensional tracking of multi-
ple particles whose images are acquired using holography
(Section IV E). We also develop a neural network to count
the number of cells in ﬂuorescence images (Section IV F).
Finally, we train a GAN to create synthetic images of
cells from a semantic mask (Section IV G). All these ex-
amples are available both as project ﬁles for DeepTrack
2.0 graphical user interface [87] and as Jupyter notebooks
[88], and they are complemented by video tutorials.

A. MNIST digit recognition

Recognizing hand-written digits of the MNIST dataset
is a classical benchmark for machine learning [90]. The
task consists of recognizing handwritten digits from 0
to 9 in 28 × 28 pixel images. In the dataset, there are
6 · 104 training images and 1 · 104 validation images, some
examples of which are provided in Fig. 7A.

Since is a relatively simple task, we employ a DNN
(Fig. 7B). The architecture of the networks is that of
Ciersan et. al., which has achieved the best results using

13

Figure 7. A dense neural network to classify hand-written digits. A Three example images from the MNIST dataset
with their corresponding labels. B The network architecture consists of ﬁve fully connected layers of decreasing size, with
the ﬁnal layer having 10 nodes, whose outputs correspond to classiﬁcation probabilities. C Examples of augmented training
images: The network is trained on a set of 6 · 104 28 × 28 pixel images augmented by translations, rotations, shear, and elastic
distortions, using a categorical cross-entropy loss. The validation loss (magenta line) is signiﬁcantly lower than the training
loss (orange line), likely due to augmentations making the training set harder than the validation set. D Confusion matrix
showing how the 1 · 104 validation images are classiﬁed by the network: The diagonal represents the correctly classiﬁed digits,
constituting the vast majority of digits. The oﬀ-diagonal cells represent incorrectly classiﬁed digits.

DNNs amongst the attempts listed on the MNIST web-
page [90]. As a loss function, we use categorical cross-
entropy, which is a standard loss function for classiﬁca-
tion tasks.

We train the network using the 6 · 104 training im-
ages augmented using aﬃne transformations and elastic
distortions, exempliﬁed in Fig. 7C. We train it for 1000
epochs, where one epoch represents one pass through all
training images. This results in a validation loss of 0.05,
as compared to a training loss of 0.20 (Fig. 7C). The
higher training loss is likely due to the augmentations,
which make the training data harder than the validation
data. It is, as such, unlikely that the network has over-
ﬁtted the training set.

Finally, we validate the trained network on the 1 · 104
validation images. The network achieves an accuracy
of 99.34%, which is in line with the best performance
achieved by DNN on the MNIST digit recognition task

[90]. The confusion matrix (Fig. 7D) shows that the in-
correctly classiﬁed digits consist mainly of 9s classiﬁed as
7s and of 4s classiﬁed as 9s.

B. Particle localization

Determining the position of objects within an image
is a fundamental task for digital microscopy. In this ex-
ample, we aim at localizing with very high accuracy the
position of an optically trapped particle. Two videos are
captured of the same particle in the same optical trap,
one with good image quality and one with poor image
quality, from which we want to extract the particle’s x
and y positions (Fig. 8A).

To analyze these images, we ﬁrst use a CNN to trans-
form the 51 × 51 pixel input image to a 6 × 6 × 64 ten-
sor. Subsequently, we pass this result to a DNN, which

504Problem: MNISTA20001500100050010softmax5Model: DNNB1e+01e+11e+21e+31e+01e-1EpochCross-entropy lossTraining lossValidation lossTrainingCTraining set977000001200011301300010000102501014100001003010123100096904008100408861000111012951001025010010160400111100968200015007099600112233445566778899True digitPredicted digitResultsD14

Figure 8. A convolutional neural network to track a single particle. A Frames of the same particle held in the same
optical trap, but with diﬀerent illumination which results in a low-noise video (left) and a high-noise video (right). B The
network architecture consists of 3 convolutional layers, each followed by a pooling layer. The resulting tensor is ﬂattened and
passed through three fully connected layers, which return the predicted x and y position of the particle. C Five examples
of the outputs of the image generation pipeline at increasing signal-to-noise ratio (SNR). The pixel tracking error for 1000
images using the DeepTrack model (orange markers) and the radial-center algorithm (gray markers). The DeepTrack model
systematically outperforms radial center, especially for low SNR. The model was trained for 110 epochs, on a set of 1 · 104
synthetic images. The validation loss (magenta line) and the training loss (orange line) remain similar for the whole training
session. D The predicted position of the particle in the low-noise video (top panel) and the high-noise video (bottom panel) as
found by the radial center algorithm (gray line) and by the DeepTrack model (dotted orange line). In the low-noise case, they
overlap within a fraction of a pixel, while for the high-noise case, the radial center algorithm produces erratic predictions.

outputs an estimate of the particle’s in-plane position
(Fig. 8B). This model is based on the one described by
Helgadottir [7]. We use mean absolute error (MAE) as
the loss function. (Alternatively, we could also use mean
squared error (MSE), which delivers equally accurate re-
sults.)

The network is trained purely on synthetic data gen-
erated using DeepTrack 2.0. The generation of synthetic
microscopy data for training a network generally entails
the following steps. First, the optical properties of the
instrument used to capture the data is replicated (e.g.,
NA, illumination spectra, magniﬁcation, and pixel size).
This ensures that the simulated point-spread function of
the simulated optical system closely matches the exper-

imental setup. Second, the properties of the sample are
speciﬁed, including the radius and refractive index of the
particle. As a ﬁnal step, noise is added to the simulated
images to be representative of experimental data. Dur-
ing training, each parameter of the simulation (e.g., the
optical properties, the sample properties, and the noise
strength) is stochastically varied around the expected ex-
perimental values to make the network more robust. In
Fig. 8C, we show a few outputs from the image genera-
tion pipeline with SNR increasing from left to right. We
also demonstrate that the network outperforms the ra-
dial center algorithm [23]. This is achieved by training
the network for 110 epochs, on a set of 1 · 104 synthetic
images. The validation set consisted of 1000 images. We

Problem: Particle trackingAxyxy16326432322 attenxyModel: CNNB2040601e+01e-11e-21e-3Absolute error [px]Signal to noise ratioRadial centerDeepTrack model1e+01e+11e+21e+01e-1EpochMAE [px]Training lossValidation lossTrainingC605856x(t) [px]Radial centerDeepTrack0.00.20.40.60.81.0605856y(t) [px]t [s]605856x(t) [px]0.00.20.40.60.81.0605856y(t) [px]t [s]ResultsDcan see that the loss is still decreasing at this point, but
the gain is minimal (Fig 8C). No signs of overﬁtting can
be seen.

Finally, we use the network to track the two videos of
the optically trapped particle. In Fig. 8D, we see that
in the low-noise video, the radial center method and the
DeepTrack model agree, while, for the high noise-video,
the radial center method makes large, sporadic jumps.
Since the videos are of the same particle in the same op-
tical trap, we expect the dynamics of the particle to be
similar. Since only the DeepTrack method gives consis-
tent dynamics in the two cases, it indicates that Deep-
Track is better able to track this more diﬃcult case. A
more detailed discussion of this example can be found in
Ref. [7].

C. Particle characterization

Microscopy images contain quantitative information
about the morphology and optical properties of the sam-
ple. However, extracting this information using conven-
tional image analysis techniques is extremely demand-
ing. Deep learning has proven to oﬀer a remedy to this
[45, 46]. In this example, we employ DeepTrack 2.0 to
develop a model to quantify the radius and refractive
index of particles based on their complex-valued scatter-
ing patterns. As experimental veriﬁcation, we record the
scattering patterns of a heterogeneous mix of two particle
populations (150 nm polystyrene and 230 nm polystyrene
bead ﬂowing in a microﬂuidic channel) using an oﬀ-axis
holographic microscope.

In line with the previous example, we use a CNN to
downsample the 64 × 64 × 2 pixel input (the two chan-
nels corresponding to the real and the imaginary parts
of the ﬁeld) to a 8 × 8 × 128 tensor. Subsequently, we
pass this tensor to a DNN, which outputs an estimate of
the particle’s radius and refractive index (Fig. 9B). The
number of channels in each layer is doubled compared
to the previous example, which may help capture subtle
changes in the scattered ﬁeld. We used MAE loss.

To account for imperfections in the experimental sys-
tem, we approximate the experimental PSF for the sim-
ulated images by adding coma aberrations with random
strength.
In Fig. 9C we show three examples of out-
puts from the image generation pipeline. The network is
trained for 110 epochs on a set of 1·104 synthetic images.
The validation set consists of 1000 images. The valida-
tion loss diverges from the training loss after only 20
epochs suggesting that the training could be terminated
earlier, or that a larger training set could be beneﬁcial.
Finally, we evaluate the model on the experimental
dataset. In each frame, all particles are roughly localized
using a standard tracking algorithm, and focused using
the criteria described in Ref. [91]. These observations
are subsequently linked frame to frame to form traces.
We use the fact that we observe each particle multiple
times to improve the accuracy of the sizing. Speciﬁcally,

15

we predict the size and refractive index of a particle us-
ing an image from each observation of that particle. We
then average the result to obtain the ﬁnal prediction for
that particle. (This deviates slightly from the method
proposed in [45] where the averaging is performed in the
latent space, which result in a more complex and accu-
rate method.) We can see the results in Fig. 9D, showing
the radius versus the refractive index of each measured
particle. We clearly distinguish two populations, which
closely match the modal characteristics of the particles
(shown by the two circles).

D. Multiparticle tracking

The previous examples have been focused on analyz-
ing a single particle at a time. Frequently, however, mi-
croscopy involves detecting multiple particles at once. In
this example, we extract the positions of quantum dots
situated on the surface of a living cell. A small slice of
an image is shown in Fig. 10A, with each particle circled
in white.

We train a U-Net model to transform the input into a
binarized representation, where each pixel within 3 pixels
of a particle in the input is set to 1, and every other pixel
is set to 0, as shown in Fig. 10B. The network returns
a probability for each pixel, which is thresholded into
the binary representation. (Note that in this example we
can use a network that is smaller than the original U-Net
because the information is highly localized; however, if,
for example, the data were aberrated, a deeper network
would be better.) The network is compiled with a binary
cross-entropy loss.

The network is trained purely on synthetic data, sim-
ulating the appearance of a quantum dot as the PSF of
the optical device. In Fig. 10C, we show several examples
of the outputs from the image generation pipeline. The
network is trained on 2000 128 × 128 pixel images in two
sessions. The ﬁrst session consists of 10 epochs where
the loss is weighted such that setting a pixel value of 1
to 0 is penalized 10 times more than setting a value of 0
to 1. This helps the network avoid the very simple local
optimum of setting every pixel to 0. For the following
100 epochs, the two types of errors are penalized equally.
This explains the sudden change of training rate after 10
epochs seen in Fig. 10C. The validation set consists of
256 images, and shows no signs of overﬁtting.

In Fig. 10D, we show a single image tracked using the
trained network. It detects all obvious particles, as well
as a few that are hard to verify as real observations. How-
ever, for most such cases, they are detected again the next
frame, indicating that it is a real observation instead of
just random noise. (However, this method to verify the
tracking is not conclusive, since quantum dots are known
to frequently ﬂicker, meaning that they are not guaran-
teed to be visible in the next frame. Conversely, two
observations in a row do not necessarily mean that it is
a real observation. It can be a product of optical eﬀects

16

Figure 9. A convolutional neural network to measure the radius and refractive index of a single particle. A
The real and imaginary parts of the scattered ﬁeld are used to measure the radius and refractive index of a single particle. The
ﬁeld is captured using an oﬀ-axis holographic microscope and numerically propagated such that the particle is in focus. The
total datasets consists of roughly 8 · 103 such observations, belonging to 352 individual polystyrene particles with 150 nm or
230 nm radius. B The network architecture consists of 3 convolutional layers, each followed by a pooling layer. The resulting
tensor is ﬂattened and passed through three fully connected layers which return the predicted radius and refractive index of the
particle. C Three pairs of real and imaginary parts of the scattered ﬁeld from a single particle. The network is trained for 110
epochs on 1000 64 × 64 pixel images, using MAE loss. The validation loss (magenta line) stops decreasing signiﬁcantly after
only 20 epochs, while the training loss (orange line) keeps decreasing. D Measured radius versus measured refractive index for
an ensemble of particles. There are two clearly distinguished populations, which closely match the modal characteristics of the
particles (shown by the two circles).

that are consistent between frames.)

E. 3D multiparticle tracking

Similarly to single particle analysis, multi-particle
analysis can be extended to extract quantitative infor-
mation about the particles. In this example, we locate
spherical particles in 3D space from the intensity of the
scattered ﬁeld captured by an in-line holographic micro-
scope (Fig. 11A). In order to be able to validate the
out-of-plane positioning with ground-truth experimental
data, we capture the experimental data using an oﬀ-axis
holographic microscope. This allows us to accurately
track the particles in 3D space using standard methods
[91]. Oﬀ-axis holographic microscopes, unlike the inline
counterpart, retrieve the entire complex ﬁeld instead of

just the ﬁeld intensity. We approximate the conversion
from oﬀ-axis to in-line holographic microscopy by squar-
ing the amplitude of the ﬁeld. Similarly to the previous
example, we represent each particle in the input by a re-
gion of pixel values of 1s in the output. The diﬀerence
is that this network returns a volume, with each parti-
cle instead represented by a sphere with radius 3 pixels
(Fig. 11B). The last dimension of the output represents
the out-of-plane position of the particle, ranging from
2 µm to 30 µm. The network is slightly larger than the
previous example, since it needs to extract more informa-
tion about the particles. Just as in the previous example,
binary cross-entropy is used as loss function.

The network is trained purely on synthetic data. We
replicate the optical properties of the instrument used to
capture the data, simulating the appearance of a particle
using Mie theory. Each parameter of the simulation is

Problem: Particle sizingARadius rRefractiveindex n326412864642 attenrnModel: CNNB1e+01e+11e+21e-11e-2EpochsMAETraining lossValidation lossTrainingCTraining setResultsD1001502002503001.701.651.601.551.501.45Radius [nm]Refractive index150nm PSL230nm PSLRealImagRealImag150nm PSL230nm PSL17

Figure 10. A U-Net to detect quantum dots in ﬂuorescence images. A A small slice of an image depicting quantum
dots situated on a living cell, imaged through a ﬂuorescence microscope (data kindly provided by Carlo Manzo). The quantum
dots in the image are circled in white. B The network architecture is a small U-Net. A ﬁnal convolutional layer outputs a
single image, where each particle in the input is represented by a circle of 1s. C Examples of synthetic images used in the
training process. The network is trained on 2000 128 × 128 pixel image for 110 epochs using binary cross-entropy loss. The
validation loss (magenta line) and the training loss (orange line) are similar in magnitude for the entire training session. After
10 epochs both losses start decreasing more rapidly, which is explained by a change in the weighting in the loss function, which
is explained in the text. D A single frame tracked using the trained model. It detects all obvious particles, as well as a few
that are hard to conclusively verify as real observations.

stochastically varied around the experimentally expected
values, making the network more robust. Additionally,
we approximate the experimental PSF by adding coma
aberrations with random strength. In Fig. 11C, we show
a few images from this pipeline. The network is trained
for 100 epochs on a set of 1 · 103 synthetic images. The
validation set consisted of 256 images, and diverges from
the training loss after 10 epochs, suggesting that it could
be terminated earlier, or that a larger training set could
be beneﬁcial (Fig. 11C).

In Fig. 11D, we show a single particle tracked in three
dimensions, with the position found using the trained
network in orange and the oﬀ-axis method in gray. The
two methods overlap almost exactly. Moreover, we also
show the out-of-plane positioning found by the oﬀ-axis
method and the trained model for all detections. We
see that most observations fall very close to the central
line, with few outliers. Moreover, we see a divergence
from the central line at the edges of the range. This is

due a limitation of how the position is extracted from
the binarized image: A sphere close to the edge of the
volume will not be entirely contained within the image,
so that its centroid will not be the center of the sphere,
resulting in a bias.

F. Cell counting

DeepTrack 2.0 is not limited to particle analysis.
Counting the number of cells in an image has tradition-
ally been a tedious task performed manually by trained
experts. In this example, we count the number of U2OS
cells (cells cultivated from the bone tissue of a patient
suﬀering from osteosarcoma [92]) in ﬂuorescence images
shown in Fig. 12A. We use the BBBC039 dataset for
evaluation [93].

We once again use a U-net model. This time, we repre-
sent each cell by a Gaussian distribution with a standard

Problem: NanotrackingA1632641286432161Model: U-NetB1e+01e+11e+21e+01e-2EpochBinary cross-entropyTraining lossValidation lossTrainingCTraining setResultsD20 μm18

Figure 11. A U-Net to track spherical particles in three dimensions. A A sample network input, consisting of
scattering patterns of several spherical particles. The sample contains a mixture of 150 nm and 230 nm polystyrene particles.
B The network architecture is a small U-Net. A ﬁnal convolutional layer outputs a volume, where each particle in the input
is represented by a sphere of 1s. The out-of-plane direction spans 2 µm to 30 µm C Examples of synthetic images used in the
training process. The network is trained on 2000 256 × 256 pixel images for 100 epochs using a binary cross-entropy loss. The
validation loss (magenta line) diverges from the training loss (orange line) after roughly 10 epochs. D A single particle tracked
using the DeepTrack model (dotted orange line) and oﬀ-axis holography (gray line), showing the x, y, and z positions over
time. The two methods almost perfectly overlap. Moreover, we show the predicted out-of-plane position of all detections as
found using the DeepTrack model versus the oﬀ-axis holography. Most observations fall close to the central line, with a few
outliers and some deviations near the edges of the range.

deviation of 10 pixels, whose intensity values integrate to
one (Fig. 12B). In this way, the integral of the output in-
tensity corresponds to the number of cells in the image.
By representing the cell by a Gaussian proﬁle, we also
reduce the emphasis on the absolute positioning of the
cell, while retaining the ability for a human to validate
the output visually. We compile the network using MAE
loss.

The training data consists of synthetic data generated
by imaging cell-like objects through a simulated ﬂuores-
cence microscope. A few example cells, as well as a single
training input-output pair is shown in Fig. 12C. The net-
work is trained for 190 epochs on a set of 1000 synthetic
images. Since the training set of the BBBC039 dataset
is not used for training, we merge the training set and

the validation set and use the merged set for validation.
The validation loss is consistently higher than the train-
ing loss, but follows a very similar curve (Fig. 12C). This
suggests that the synthetic data is a decent approxima-
tion of the experimental images. The oﬀset can be largely
explained by a few images in the validation set that are
particularly hard for the network.

For large images, errors can average out, which can
result in deceptively accurate counting. To eliminate this
concern, we show the predicted number of cells versus the
true number of cells for smaller slices of images (256 ×
256 pixels) in the BBBC039 dataset in Fig. 12D. The
network predicts the correct number of cells within just
a few percent. As a comparison, we show that the images
cannot be analyzed by simply integrating the intensity of

Problem: 3-D TrackingA3264128256128643230Model: U-NetBTrainingset1e+01e+11e+21e-21e-3EpochBinary cross-entropyTraining lossValidation lossTrainingCResultsD20-2x [μm]50-5y [μm]0.00.51.01.52.02.53.0864Time [s]z [μm]0510152025303020100DeepTrack z [μm]Off-axis z [μm]19

Figure 12. A U-Net to count cells in a ﬂuorescence image. A Two slices from the BBBC039 dataset, with the
corresponding number of cells in the image. B The network architecture is a small U-Net. A ﬁnal convolutional layer outputs
an image with a single feature, where each cell in the input is represented by a Gaussian distribution with a standard deviation
of 10 pixels and whose intensity integrates to one. Thus, the integration of the intensity of the output corresponds to the number
of cells in the image. C Examples of the cell images created by the image simulation pipeline, followed by a sample input-output
pair containing six cells. The network is trained on 1000 256 × 256 pixel images, and validated on 150 512pixel × 688pixel
images, using MAE loss. The validation loss (magenta line) is consistently higher than the training loss (orange line), but
follows a similar curve. D The number of cells as found by the DeepTrack model compared to a naive approach based on the
summation of the values of the pixels of the image. Each data point represents a 256 × 256 pixel slice of one of the 50 images
in the test set. Three points are circled and have their corresponding input-output pair shown on the right.

the input images (Fig. 12D). In order to show a best case
scenario for the sum-of-pixels method, we transformed
each sum by an aﬃne transformation that minimizes the
square error on the test set itself.
It is apparent that
this is not suﬃcient to achieve an acceptable counting
accuracy.

G. GAN image generation

DeepTrack 2.0 can also eﬃciently handle more cases
where the training set is derived directly from experimen-
tally captured data, instead of being simulated. In this
example, we combine the two approaches, by using ex-

perimental data to train a GAN to create new data from
a semantic representation of the image (Fig. 13A). More
speciﬁcally, the GAN creates images of the drosophila
melanogaster third instar larva ventral nerve cord from a
semantic representation of background, membrane, and
mitochondria. This GAN, once trained, can subsequently
be used as a part of an image simulation pipeline, just as
any other DeepTrack feature.

The architecture of the neural network we employ is
shown in Fig. 13B. The model is composed of a generator
that learns the mapping relation between the input mask
and its corresponding cell-image, and of a discriminator
that, given the semantic segmentation, determines if the
generated image plausibly could have been drawn from a
real sample.

38Problem: Cell countingA1632641286432161sum3Model: U-NetB1e+01e+11e+22e-11e-12e-2EpochsMAETraining lossValidation lossTrainingCTraining set0102030405050403020100Predicted countTrue countSum of pixelsDeepTrack modelResultsD20

Figure 13. A conditional GAN is used to create cell images from a semantic mask. A Example masks (left)
from which images of drosophila melanogaster third instar larva ventral nerve cord (right) are generated using the segmented
anisotropic ssTEM dataset [94]. B The network architecture is a conditional generative adversarial network. The generator
transforms an input semantic mask into a realistic cell image, using a U-Net architecture with the most condensed layer being
replaced by two residual network blocks [95]. The discriminator is designed similar to the PatchGan discriminator [96], and
receives both the mask and an image as an input. The generator is trained using a MAE loss between the experimental
image and the generated image, as well as a MSE loss on the discriminator output. Conversely, the discriminator is trained
with a MSE loss. C Examples of masks and corresponding experimental images. The loss of the generator (left) and of the
discriminator (right) are shown over 1000 training epochs, each of which consists of 16 mini-batches of 7 samples. We see that
the generator loss increases towards the end of the training, a signature that continuing training beyond this point destabilizes
the generator. D Masks images from a validation set, and corresponding generated image and real image. The generated
images are qualitatively similar to the real images.

The generator follows a U-Net design with symmetric
encoder and decoder paths connected through skip con-
nections. The encoder consists of convolutional blocks
followed by strided convolutions for downsampling. Each
convolutional block contains two sequences of 3 × 3 con-
volutional layers. At each step of the encoding path, we
increase the number of feature channels by a factor of 2.
The encoder connects to the decoder through two
residual network (ResNet) blocks [95], each with 1024
feature channels. For upsampling, we use bilinear in-
terpolations, followed by a convolutional layer (stride =
1). This operation is followed by concatenation with the
corresponding feature map from the encoding path. Fur-

thermore, we add two convolutional blocks with 16 fea-
ture channels at the ﬁnal layer of the decoder. We use
a 1 × 1 convolutional layer to map each 16-component
feature vector to the output image. Herein, the hyper-
bolic tangent activation (tanh) is employed to transform
the output to the range [−1, 1]. Every layer in the gen-
erator, except the last layer, is followed by an instance
normalization (alpha = 2) and a LeakyRelu activation
layer.

The discriminator follows a PatchGan architecture
[96], which divides the input images into overlapping
patches and classiﬁes each path as real or fake, rather
than using a single descriptor. This splitting arises nat-

MitochondriaMembraneBackgroundProblem: Mask to imageAGenerator163264128256512Real?Real dataModel: GANB163264128256512102451225612864321611e+01e+11e+21e+30.80.70.60.50.4EpochGenerator loss1e+01e+11e+21e+30.40.30.20.1EpochDiscriminator lossTrainingCTraining setMaskGeneratedimageRealimageResultsDurally as a consequence of the discriminator’s convolu-
tional architecture [57]. The discriminator’s convolu-
tional blocks consist of 4 × 4 convolutional layers with a
stride of 2, which decrease the input resolution to half the
width and height. In all layers, we use instance normal-
ization (with no learnable parameters) and LeakyRelu
activation. Finally, the network outputs an 8 × 8 single-
channel tensor containing the predicted probability for
each path.

The training data consists of experimental data from
the segmented anisotropic ssTEM dataset [94]. Each
sample is normalized between -1 and 1, and augmented
by mirroring, rotating, shearing, and scaling. Moreover,
a Gaussian random noise with standard distribution ran-
domly sampled (per image) between 0 and 0.1 is added
to the mask. Adding noise to the mask qualitatively im-
proves the image quality. Speciﬁcally, without adding
noise, the network is prone to tiling very similar internal
structures, especially far away from the border of a mask.
This occurs because there is no internal structure in the
input, making two nearby regions of the input virtually
identical from the point of view of the network. By intro-
ducing some internal structure to the mask in the form
of noise, we help the network distinguish otherwise very
similar regions in the input. An additional beneﬁt is that
it is the possible to generate many images from a single
mask, just by varying the noise. A few example training
input–output pairs are shown in Fig. 12C. For this ex-
ample, we deﬁne the loss functions of the generator as,
lG = γ·MAE {zlabel, zoutput}+(1 − D(zoutput))2 , and dis-
criminator as, lD = D (zoutput)2 +(1 − D(zlabel))2 , where
D(·) denotes the discriminator network prediction, zlabel
refers to the ground truth cell-image, and zoutput is the
generated image. Note that the generator loss function,
lG, aims to minimize the MAE between the generator
output image and its target, based on the regularization
parameter γ set to 0.8. For training, we use the Adam
optimizer with a learning rate of 0.0002 and β1 = 0.5 for
1000 epochs, each of which consisting of 16 mini-batches.
The resulting model is able to create new images from
masks it has never seen before. We show ﬁve such cases
in Fig. 13D. The generated images are not identical to
the real cell images in terms of texture and appearance,
which is expected since the masks only contain spatial
information about the cells’ structures. However, the
generated images are qualitatively similar to images from
the experimental dataset.

V. OUTLOOK

The adoption of new deep-learning methods for the
analysis of microscopy data is extremely promising, but
it has been hampered by diﬃculties in generating high-
quality training datasets. While manually annotated ex-
perimental data ensures that the training set is repre-
sentative of the validation set, it is not guaranteed that
the trained network can correctly analyze data obtained

21

with another setup or annotated by another operator.
Moreover, it limits the network to human-level accuracy,
which is not suﬃcient for tasks requiring higher level of
accuracy, such as single-particle tracking. Synthetically
generated data bypasses these issues because the ground
truth can be known exactly, and the networks can be
trained with parameters that exactly match each user’s
setup.

Thanks to the increasing available inference speed, it
will become easier to perform real-time analysis of mi-
croscopy data. This can be used to make real-time deci-
sions, from simple experiment control (e.g., such as con-
trolling the sample ﬂow speed) to more complex decisions
(e.g., real-time sorting and optical force feedback sys-
tems). For example, one could imagine a completely au-
tomated experimental feedback system that applies opti-
cal forces to optimize imaging parameters and to acquire
the best possible measurements of the quantities of in-
terest.

In this article, we have introduced DeepTrack 2.0,
which provides a software environment to develop neural-
network models for quantitative digital microscopy, from
the generation of training datasets to the deployment of
deep-learning solutions tailored to the needs of each user.
We have shown that DeepTrack 2.0 is capable of train-
ing neural networks that perform a broad range of tasks
using purely synthetic training data. For tasks where it
is infeasible to simulate the training set, DeepTrack 2.0
can augment images on the ﬂy to expand the available
training set. Moreover, DeepTrack 2.0 is complemented
by a graphical user interface, allowing users with mini-
mal programming experience to explore and create deep
learning models.

We envision DeepTrack 2.0 as a open-source project,
where contributors with diﬀerent areas of expertise can
help improve and expand the framework to cover the
users’ needs.
Interesting possible directions for the fu-
ture expansion of DeepTrack 2.0 can, for example, pro-
vide tools for the analysis of time sequences using recur-
rent neural networks, understand physical processes us-
ing reservoir computing, and even support physical im-
plementations of neural networks for greater execution
speed and higher energy eﬃciency.

Deep-learning has the potential to revolutionize how
we do microscopy. However, there are still many chal-
lenges to overcome, not least of which ﬁguring out how
to obtain enough training data for the model to gener-
alize. We believe that physical simulations will play a
crucial part in overcoming this roadblock. As such, we
strongly encourage researchers and community collabora-
tors to contribute with objects and models in their area
of expertise:
from specialized in-sample structures and
improved optics simulation methods, to new and excit-
ing neural network architectures.

ACKNOWLEDGMENTS

The authors would like to thank Carlo Manzo for pro-
viding the experimental images used in the fourth case

22

study, Jose Alvarez for designing the logo of DeepTrack
2.0, as well as the European Research Council (grant
number 677511), the Knut and Alice Wallenberg Foun-
dation, and Vetenskapsr˚adet (grant numbers 2016-03523
and 2019-05071) for funding this research.

[1] J. Perrin, “Mouvement brownien et mol´ecules,” J. Phys.

Theor. Appl. 9, 5–39 (1910).

[2] E. Kappler, “Versuche zur Messung der Avogadro-
Loschmidtschen Zahl aus der Brownschen Bewegung
einer Drehwaage,” Ann. Phys. 403, 233–256 (1931).
[3] D. Causley and J. Z. Young, “Counting and sizing of
particles with the ﬂying-spot microscope,” Nature 176,
453–454 (1955).

[4] H. Geerts, M. De Brabander, R. Nuydens, S. Geuens,
M. Moeremans, J. De Mey, and P. Hollenbeck, “Nanovid
tracking: a new automatic method for the study of mo-
bility in living cells based on colloidal gold and video
microscopy,” Biophysical Journal 52, 775–782 (1987).
[5] John C. Crocker and David G. Grier, “Methods of digital
video microscopy for colloidal studies,” Journal of Colloid
and Interface Science 179, 298–310 (1996).

[6] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in Lecture Notes in Computer Science
(including subseries Lecture Notes in Artiﬁcial Intelli-
gence and Lecture Notes in Bioinformatics), Vol. 9351
(Springer Verlag, 2015) pp. 234–241.

[7] Saga Helgadottir, Aykut Argun, and Giovanni Volpe,
“Digital video microscopy enhanced by deep learning,”
Optica 6, 506 (2019).

[8] I. Nordlund, “Eine neue Bestimmung der Avogadroschen
in
Konstante aus der Brownschen Bewegung kleiner,
Wasser suspendierten Quecksilberk¨ugelchen,” Z. Phys.
Chemie 87U, 40–62 (2017).

[9] K. Preston, “Digital Image Processing in the United
States,” in Digital Processing of Biomedical Images
(Springer US, 1976) pp. 1–10.

[10] W. H. Walton, “Automatic Counting of Microscopic Par-

ticles,” Nature 169, 518–520 (1952).

[11] J. M. S. Prewitt and M. L. Mendelsohn, “The analysis of
cell images,” Ann. New York Acad. Sci. 128, 1035–1053
(1965).

[12] G. N. Hounsﬁeld, British J. Radiol., Tech. Rep. (Central
Research Laboratories of EMI Limited, Hayes, Middle-
sex, 1973).

[13] H. P. Mansberg, A. M. Saunders, and W. Groner, “The
Hemalog D white cell diﬀerential system,” J. Histochem.
Cytochem. 22, 711–24 (1974).

[14] D. Magde, E. Elson, and W. W. Webb, “Thermody-
namic ﬂuctuations in a reacting system measurement by
ﬂuorescence correlation spectroscopy,” Phys. Rev. Lett.
29, 705–708 (1972).

[15] D. Axelrod, P. Ravdin, D. E. Koppel, J. Schlessinger,
W. W. Webb, E. L. Elson, and T. R. Podleski, “Lat-
eral motion of ﬂuorescently labeled acetylcholine recep-
tors in membranes of developing muscle ﬁbers,” Proc.
Natl. Acad. Sci. U.S.A. 73, 4594–4598 (1976).

[16] C. Manzo, M. F. Garcia-Parajo, and J. A. Torreno-Pina,
from
“A review of progress in single particle tracking:

methods to biophysical insights,” Rep. Prog. Phys. 78,
124601 (2015).

[17] Th. Schmidt, G. J. Sch¨utz, W. Baumgartner, H. J. Gru-
ber, and H. Schindler, “Imaging of single molecule dif-
fusion,” Proc. Natl. Acad. Sci. U.S.A. 93, 2926–2929
(1996).

[18] G. J. Sch¨utz, G. Kada, V. Pastushenko,

and
H. Schindler, “Properties of lipid microdomains in a
muscle cell membrane visualized by single molecule mi-
croscopy.” EMBO J. 19, 892–901 (2000).

[19] D. Alcor, G. Gouzer,

and A. Triller, “Single-particle
tracking methods for the study of membrane receptors
dynamics,” Eur. J. Neurosci. 30, 987–997 (2009).

[20] M. Dahan, S. L´evi, C. Luccardini, P. Rostaing,
B. Riveau,
and A. Triller, “Diﬀusion Dynamics of
Glycine Receptors Revealed by Single-Quantum Dot
Tracking,” Science 302, 442–445 (2003).

[21] F. Pinaud, S. Clarke, A. Sittner, and M. Dahan, “Prob-
ing cellular events, one quantum dot at a time,” (2010).
[22] B. Yu, D. Chen, J. Qu, and H. Niu, “Fast Fourier domain
localization algorithm of a single molecule with nanome-
ter precision,” Opt. Lett. 36, 4317–4319 (2011).

[23] R. Parthasarathy, “Rapid, accurate particle tracking by
calculation of radial symmetry centers,” Nat. Methods 9,
724–726 (2012).

[24] R. E. Thompson, D. R. Larson, and W. W. Webb, “Pre-
cise nanometer localization analysis for individual ﬂuo-
rescent probes,” Biophys. J. 82, 2775–2783 (2002).
[25] R. J. Ober, S. Ram, and E. S. Ward, “Localization Ac-
curacy in Single-Molecule Microscopy,” Biophys. J. 86,
1185–1200 (2004).

[26] B. Zhang, J. Zerubia, and J. C. Olivo-Marin, “Gaussian
approximations of ﬂuorescence microscope point-spread
function models,” Appl. Opt., Applied Optics 46, 1819–
1829 (2007).

[27] A. V. Abraham, S. Ram, J. Chao, E. S. Ward, and R. J.
Ober, “Quantitative study of single molecule location es-
timation techniques,” Opt. Express 17, 23352 (2009).
[28] S. Stallinga and B. Rieger, “Accuracy of the Gaussian
in 2D localization mi-

Point Spread Function model
croscopy,” Opt. Express 18, 24461 (2010).

[29] S. Stallinga and B. Rieger, “Position and orientation esti-
mation of ﬁxed dipole emitters using an eﬀective Hermite
point spread function model,” Opt. Express 20, 5896
(2012).

[30] S.-H. Lee, Y. Roichman, G.-R. Yi, S.-H. Kim, S.-M.
Yang, A. van Blaaderen, P. van Oostrum, and D. G.
Grier, “Characterizing and tracking single colloidal par-
ticles with video holographic microscopy,” Opt. Express
15, 18275 (2007).

[31] L. Holtzer, T. Meckel, and T. Schmidt, “Nanometric
three-dimensional tracking of individual quantum dots
in cells,” Appl. Phys. Lett. 90, 053902 (2007).

[32] H. Deschout, F. Cella Zanacchi, M. Mlodzianoski, A. Di-

aspro, J. Bewersdorf, S. T. Hess, and K. Braeckmans,
“Precisely and accurately localizing single emitters in ﬂu-
orescence microscopy,” Nat. Methods, Nature Methods
11, 253–266 (2014).

[33] W. J. Godinez and K. Rohr, “Tracking multiple particles
in ﬂuorescence time-lapse microscopy images via proba-
bilistic data association,” IEEE Trans. Medical Imaging
34, 415–432 (2015).

[34] N. Chenouard, I. Smal, F. De Chaumont, M. Maˇska,
I. F. Sbalzarini, Y. Gong, J. Cardinale, C. Carthel,
S. Coraluppi, M. Winter, A. R. Cohen, W. J. Godinez,
K. Rohr, Y. Kalaidzidis, L. Liang, J. Duncan, H. Shen,
Y. Xu, K. E. G. Magnusson, J. Jald´en, H. M. Blau,
P. Paul-Gilloteaux, P. Roudot, C. Kervrann, F. Waharte,
J. Y. Tinevez, S. L. Shorte, J. Willemse, K. Celler, G. P.
Van Wezel, H. W. Dan, Y. S. Tsai, C. O. De Sol´orzano,
J. C. Olivo-Marin, and E. Meijering, “Objective com-
parison of particle tracking methods,” Nat. Methods 11,
281–289 (2014).

[35] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,”

Nature, Nature 521, 436–444 (2015).

[36] D. Ciresan, U. Meier,

and J. Schmidhuber, “Multi-
column deep neural networks for image classiﬁcation,” in
2012 IEEE Conf. Computer Vision Pattern Recognition
(2012) pp. 3642–3649.

[37] Evan Shelhamer, Jonathan Long, and Trevor Darrell,
“Fully Convolutional Networks for Semantic Segmenta-
tion,” IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence 39, 640–651 (2017).

[38] Mu Li, Wangmeng Zuo,

and David Zhang, “Con-
volutional Network for Attribute-driven and Identity-
preserving Human Face Generation,” (2016).

[39] Mark D. Hannel, Aidan Abdulali, Michael O’Brien, and
David G. Grier, “Machine-learning techniques for fast
and accurate feature localization in holograms of colloidal
particles,” Optics Express 26, 15221 (2018).

[40] Jay M. Newby, Alison M. Schaefer, Phoebe T. Lee,
M. Gregory Forest,
and Samuel K. Lai, “Convolu-
tional neural networks automate detection for tracking
of submicron-scale particles in 2D and 3D,” Proceedings
of the National Academy of Sciences of the United States
of America 115, 9026–9031 (2018).

[41] Claire Lifan Chen, Ata Mahjoubfar, Li-Chia Tai, Ian K.
Blaby, Allen Huang, Kayvan Reza Niazi, and Bahram
Jalali, “Deep learning in label-free cell classiﬁcation,” Sci-
entiﬁc Reports 6, 21471 (2016).

[42] Nicolas Coudray, Paolo Santiago Ocampo, Theodore
Sakellaropoulos, Navneet Narula, Matija Snuderl, David
Feny¨o, Andre L. Moreira, Narges Razavian, and Aris-
totelis Tsirigos, “Classiﬁcation and mutation prediction
from non–small cell lung cancer histopathology images
using deep learning,” Nature Medicine 24, 1559–1567
(2018).

[43] L Zhang, L Lu, I Nogues, RM Summers, S Liu, and
J Yao, “DeepPap: deep convolutional networks for cer-
vical cell classiﬁcation,” IEEE journal of biomedical and
health informatics (2017).

[44] Thorsten Falk, Dominic Mai, Robert Bensch, ¨Ozg¨un
C¸ i¸cek, Ahmed Abdulkadir, Yassine Marrakchi, Anton
B¨ohm, Jan Deubner, Zoe J¨ackel, Katharina Seiwald,
Alexander Dovzhenko, Olaf Tietz, Cristina Dal Bosco,
Sean Walsh, Deniz Saltukoglu, Tuan Leng Tay, Marco
Ilka Diester,
Prinz, Klaus Palme, Matias Simons,

23

Thomas Brox,
and Olaf Ronneberger, “U-net: deep
learning for cell counting, detection, and morphometry,”
Nature Methods 16, 67–70 (2019).

[45] Benjamin Midtvedt, Erik Ols´en, Fredrik Eklund, Fredrik
H¨o¨ok, Caroline Beck Adiels, Giovanni Volpe,
and
Daniel Midtvedt, “Holographic characterisation of sub-
wavelength particles enhanced by deep learning,” (2020).
[46] Lauren E. Altman and David G. Grier, “CATCH: Char-
acterizing and Tracking Colloids Holographically Using
Deep Neural Networks,” Journal of Physical Chemistry
B 124, 1602–1610 (2020).
[47] Weidi Xie, J Alison Noble,

and Andrew Zisserman,
“Microscopy cell counting and detection with fully con-
volutional regression networks,” COMPUTER METH-
ODS IN BIOMECHANICS AND BIOMEDICAL ENGI-
NEERING: IMAGING & VISUALIZATION 6, 283–292
(2018).

[48] Yichen Wu, Yair Rivenson, Yibo Zhang, Zhensong Wei,
Harun G¨unaydin, Xing Lin, and Aydogan Ozcan, “Ex-
tended depth-of-ﬁeld in holographic imaging using deep-
learning-based autofocusing and phase recovery,” Optica
5, 704 (2018).

[49] Elias Nehme, Lucien E. Weiss, Tomer Michaeli,
and Yoav Shechtman, “Deep-STORM: super-resolution
single-molecule microscopy by deep learning,” Optica 5,
458 (2018).

[50] Wei Ouyang, Andrey Aristov, Micka¨el Lelek, Xian Hao,
and Christophe Zimmer, “Deep learning massively accel-
erates super-resolution localization microscopy,” Nature
Biotechnology 36, 460–468 (2018).

[51] Fuyong Xing, Yuanpu Xie, Hai Su, Fujun Liu, and Lin
Yang, “Deep Learning in Microscopy Image Analysis:
A Survey,” IEEE Transactions on Neural Networks and
Learning Systems 29, 4550–4568 (2018).

[52] B. Mehlig, “Artiﬁcial neural networks,” arXiv preprint

arXiv:1901.05639 (2019).

[53] D. E. Rumelhart, G. E. Hinton, and R. J. Williams,
“Learning representations by back-propagating errors,”
Nature 323, 533–536 (1986).

[54] G. Cybenko, “Approximation by superpositions of a sig-
moidal function,” Mathematics of Control, Signals, and
Systems 2, 303–314 (1989).

[55] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio, “Generative adversarial nets,” in Ad-
vances in neural information processing systems (2014)
pp. 2672–2680.

[56] Abhay Yadav, Sohil Shah, Zheng Xu, David Jacobs, and
Tom Goldstein, “Stabilizing adversarial nets with predic-
tion methods,” arXiv preprint arXiv:1705.07364 (2017).
teaching ma-
chines to paint, write, compose, and play (O’Reilly Me-
dia, 2019).

[57] David Foster, Generative deep learning:

[58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, and
Jon Shlens, Rethinking the Inception Architecture for
Computer Vision, Tech. Rep. (2015).

[59] Sajith Kecheril Sadanandan, Petter Ranefall, Sylvie
Le Guyader, and Carolina W¨ahlby, “Automated Train-
ing of Deep Convolutional Neural Networks for Cell Seg-
mentation,” Scientiﬁc Reports 7, 1–7 (2017).

[60] Yousef Al-Kofahi, Alla Zaltsman, Robert Graves, Will
Marshall, and Mirabela Rusu, “A deep learning-based
algorithm for 2-D cell segmentation in microscopy im-
ages,” BMC Bioinformatics 19, 365 (2018).

[61] Youyi Song, Ee Leng Tan, Xudong Jiang, Jie Zhi Cheng,
Dong Ni, Siping Chen, Baiying Lei, and Tianfu Wang,
“Accurate cervical cell segmentation from overlapping
clumps in pap smear images,” IEEE Transactions on
Medical Imaging 36, 288–300 (2017).

[62] Saad Ullah Akram, Juho Kannala, Lauri Eklund, and
Janne Heikkil¨a, “Cell segmentation proposal network for
microscopy image analysis,” in Lecture Notes in Com-
puter Science (including subseries Lecture Notes in Arti-
ﬁcial Intelligence and Lecture Notes in Bioinformatics),
Vol. 10008 LNCS (Springer Verlag, 2016) pp. 21–29.
[63] Assaf Arbelle and Tammy Riklin Raviv, “Microscopy Cell
Segmentation via Adversarial Neural Networks,” Pro-
ceedings - International Symposium on Biomedical Imag-
ing 2018-April, 645–648 (2017).

[64] Nuh Hatipoglu and Gokhan Bilgin, “Cell segmentation
in histopathological images with deep learning algorithms
by utilizing spatial relationships,” Medical and Biological
Engineering and Computing 55, 1829–1848 (2017).
[65] Assaf Arbelle and Tammy Riklin Raviv, “Microscopy Cell
Segmentation via Convolutional LSTM Networks,” Pro-
ceedings - International Symposium on Biomedical Imag-
ing 2019-April, 1008–1012 (2018).

[66] Jean-Baptiste Lugagne, Haonan Lin, and Mary J. Dun-
lop, “DeLTA: Automated cell segmentation, tracking,
and lineage reconstruction using deep learning,” PLOS
Computational Biology 16, e1007673 (2020).

[67] Shan E.Ahmed Raza, Linda Cheung, David Epstein,
Stella Pelengaris, Michael Khan, and Nasir M. Rajpoot,
“MIMO-Net: A multi-input multi-output convolutional
neural network for cell segmentation in ﬂuorescence mi-
croscopy images,” in Proceedings - International Sympo-
sium on Biomedical Imaging (IEEE Computer Society,
2017) pp. 337–340.

[68] Boyuan Ma, Xiaojuan Ban, Haiyou Huang, Yulian Chen,
Wanbo Liu, and Yonghong Zhi, “Deep Learning-Based
Image Segmentation for Al-La Alloy Microscopic Im-
ages,” Symmetry 10, 107 (2018).

[69] Seyed Majid Azimi, Dominik Britz, Michael Engstler,
Mario Fritz, and Frank M¨ucklich, “Advanced steel mi-
crostructural classiﬁcation by deep learning methods,”
Scientiﬁc Reports 8, 2128 (2018).

[70] Fahad Lateef and Yassine Ruichek, “Survey on seman-
tic segmentation using deep learning techniques,” Neu-
rocomputing 338, 321–348 (2019).

[71] Rongjian Li, Tao Zeng, Hanchuan Peng,

and Shui-
wang Ji, “Deep Learning Segmentation of Optical Mi-
croscopy Images Improves 3-D Neuron Reconstruction,”
IEEE Transactions on Medical Imaging 36, 1533–1541
(2017).

[72] ¨Ozg¨un C¸ i¸cek, Ahmed Abdulkadir, Soeren S. Lienkamp,
Thomas Brox, and Olaf Ronneberger, “3D U-net: Learn-
ing dense volumetric segmentation from sparse annota-
tion,” in Lecture Notes in Computer Science (including
subseries Lecture Notes in Artiﬁcial Intelligence and Lec-
ture Notes in Bioinformatics), Vol. 9901 LNCS (Springer
Verlag, 2016) pp. 424–432.

[73] Jens Kleesiek, Gregor Urban, Alexander Hubert, Daniel
Schwarz, Klaus Maier-Hein, Martin Bendszus,
and
Armin Biller, “Deep MRI brain extraction: A 3D convo-
lutional neural network for skull stripping,” NeuroImage
129, 460–469 (2016).

[74] Eric Betzig, George H. Patterson, Rachid Sougrat,

24

O. Wolf Lindwasser, Scott Olenych, Juan S. Bonifacino,
Michael W. Davidson, Jennifer Lippincott-Schwartz, and
Harald F. Hess, “Imaging intracellular ﬂuorescent pro-
teins at nanometer resolution,” Science 313, 1642–1645
(2006).

[75] Yichen Wu, Yilin Luo, Gunvant Chaudhari, Yair Riven-
son, Ayfer Calis, Kevin de Haan, and Aydogan Ozcan,
“Bright-ﬁeld holography: cross-modality deep learning
enables snapshot 3d imaging with bright-ﬁeld contrast
using a single hologram,” Light: Science & Applications
8, 25 (2019).

[76] Yair Rivenson, Tairan Liu, Zhensong Wei, Yibo Zhang,
and Aydogan Ozcan, “PhaseStain:
Kevin de Haan,
the digital staining of label-free quantitative phase mi-
croscopy images using deep learning,” Light: Science and
Applications 8, 2047–7538 (2019).

[77] Samira Masoudi, Afsaneh Razi, Cameron H G Wright,
Jesse C Gatlin, and Ulas Bagci, IEEE transactions on
medical imaging, Tech. Rep. (2019).

[78] P Zelger, K Kaser, B Rossboth, L Velas, G J Sch¨utz,
and A Jesacher, “Three-dimensional
localization mi-
croscopy using deep learning,” Optical Express (2018),
10.1364/OE.26.033166.

[79] Simon Franchini and Samuel Krevor, “Cut, overlap and
locate: a deep learning approach for the 3D localization
of particles in astigmatic optical setups,” Experiments in
Fluids 61, 140 (2020).

[80] T. Wollmann, C. Ritter, J. N. Dohrke, J. Y. Lee,
R. Bartenschlager, and K. Rohr, “Detnet: Deep neural
network for particle detection in ﬂuorescence microscopy
images,” in Proceedings - International Symposium on
Biomedical Imaging, Vol. 2019-April (IEEE Computer
Society, 2019) pp. 517–520.

[81] C. Ritter, T. Wollmann, J. Y. Lee, R. Bartenschlager,
and K. Rohr, “Deep Learning Particle Detection for
Probabilistic Tracking in Fluorescence Microscopy Im-
ages,” in Proceedings - International Symposium on
Biomedical Imaging, Vol. 2020-April (IEEE Computer
Society, 2020) pp. 977–980.

[82] Ayse Betul Oktay and Anıl Gurses, “Automatic detec-
tion, localization and segmentation of nano-particles with
deep learning in microscopy images,” Micron 120, 113–
119 (2019).

[83] Roman Spilger, Andrea Imle, Ji Young Lee, Barbara
Muller, Oliver T. Fackler, Ralf Bartenschlager, and Karl
Rohr, “A Recurrent Neural Network for Particle Tracking
in Microscopy Images Using Future Information, Track
Hypotheses, and Multiple Detections,” IEEE Transac-
tions on Image Processing 29, 3681–3694 (2020).

[84] Naor Granik, Lucien E. Weiss, E. Nehme, Maayan Levin,
Michael Chein, Eran Perlson, Yael Roichman, and Yoav
Shechtman, “Single-Particle Diﬀusion Characterization
by Deep Learning,” Biophysical Journal 117, 185–192
(2019).

[85] Stefano Bo, Falko Schmidt, Ralf Eichhorn, and Giovanni
Volpe, “Measurement of anomalous diﬀusion using recur-
rent neural networks,” Physical Review E 100 (2019),
10.1103/PhysRevE.100.010102.

[86] Patrycja Kowalek, Hanna Loch-Olszewska, and Janusz
Szwabi´nski, “Classiﬁcation of diﬀusion modes in single-
Feature-based versus deep-
particle tracking data:
learning approach,” Physical Review E 100 (2019),
10.1103/PhysRevE.100.032410.

[87] Benjamin Midtvedt, Saga Helgadottir, Aykut Argun,

25

Jes´us Pineda, Daniel Midtvedt,
and Giovanni Volpe,
“Deeptrack-2.0,” https://github.com/softmatterlab/
DeepTrack-2.0 (2020).

[88] Benjamin Midtvedt, Saga Helgadottir, Aykut Ar-
gun, Jes´us Pineda, Daniel Midtvedt,
and Gio-
vanni Volpe, “Deeptrack-2.0-app,” https://github.
com/softmatterlab/DeepTrack-2.0-app (2020).

[89] Fran¸cois Chollet et al., “Keras,” https://keras.io

(2015).

[90] Yann LeCun, Corinna Cortes, and CJ Burges, “Mnist
handwritten digit database,” ATT Labs [Online]. Avail-
able: http://yann.lecun.com/exdb/mnist 2 (2010).
[91] Daniel Midtvedt, Fredrik Eklund, Erik Ols´en, Benjamin
Midtvedt, Jan Swenson, and Fredrik H¨o¨ok, “Size and
refractive index determination of subwavelength parti-
cles and air bubbles by holographic nanoparticle tracking
analysis,” Analytical Chemistry 92, 1908–1915 (2020).

[92] J. Pont´en and E. Saksela, “Two established in vitro cell
lines from human mesenchymal tumours,” Int. J. Cancer

2, 434–447 (1967).

[93] Vebjorn Ljosa, Katherine L. Sokolnicki, and Anne E.
Carpenter, “Annotated high-throughput microscopy im-
age sets for validation,” Nature Methods 9, 637–637
(2012).

[94] Stephan Gerhard, Jan Funke, Julien Martel, Albert
Cardona, and Richard Fetter, “Segmented anisotropic
(2013),
sstem dataset of neural
10.6084/m9.ﬁgshare.856713.v1.

tissue,” ﬁgshare

[95] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, Vol. 2016-
December (IEEE Computer Society, 2016) pp. 770–778.
[96] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros, “Image-to-image translation with conditional ad-
versarial networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition (2017)
pp. 1125–1134.

