1
2
0
2

l
u
J

0
1

]

G
L
.
s
c
[

2
v
4
8
7
0
1
.
7
0
0
2
:
v
i
X
r
a

Fast Neural Models for Symbolic Regression at Scale

Allan Costa∗
MIT
allanc@mit.edu

Rumen Dangovski∗
MIT
rumenrd@mit.edu

Owen Dugan∗
MIT
odugan@mit.edu

Samuel Kim
MIT
samkim@mit.edu

Pawan Goyal
MIT
pawan14@mit.edu

Marin Soljaˇci´c†
MIT
soljacic@mit.edu

Joseph Jacobson†
MIT
jacobson@media.mit.edu

Abstract

Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conﬂicting
with goals of ﬁnding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that ﬁnds interpretable, compact, and sparse solutions for
ﬁtting data, à la Occam’s razor. Our model deﬁnes a probability distribution
over a non-differentiable function space. We introduce a two-step optimization
method that samples functions and updates the weights with backpropagation
based on cross-entropy matching in an evolutionary strategy: we train by biasing
the probability mass toward better ﬁtting solutions. OccamNet is able to ﬁt a
variety of symbolic laws including simple analytic functions, recursive programs,
implicit functions, simple image classiﬁcation, and can outperform noticeably
state-of-the-art symbolic regression methods on real world regression datasets.
Our method requires minimal memory footprint, does not require AI accelerators
for efﬁcient training, ﬁts complicated functions in minutes of training on a single
CPU, and demonstrates signiﬁcant performance gains when scaled on a GPU. Our
implementation, demonstrations and instructions for reproducing the experiments
are available at https://github.com/druidowm/OccamNet_Public.

1

Introduction

Deep learning has revolutionized a variety of complex tasks, ranging from language modeling to
computer vision [1]. Key to this success is designing a large search space in which many local minima
sufﬁciently approximate given data [2]. This requires large, complex models, which conﬂicts with
the goals of sparsity and interpretability, making neural nets ill-suited for a myriad of physical and
computational problems that have compact and interpretable underlying mathematical structures [3].

While neural networks easily emulate data generated from symbolic laws, the resulting models are
not interpretable, might not preserve desired physical properties (e.g. time invariance), and are unable
to generalize beyond observed data. Moreover, neural networks’ reliance on complexity implies that
reproducing compact laws might require the full collection of trained weights, in opposition to a
compact form solution.

∗Equal contribution.
†Equal supervision.

Preprint. Under review.

 
 
 
 
 
 
Figure 1: (a) A two-output network model with depth L = 2, (cid:126)x = [x0, x1], user selected constants
C = [1, π], and bases functions Φ = (cid:104)+(·, ·), sin(·), (·)2, ×(·, ·)(cid:105). Highlighted are the arguments
sublayer, composed of P-nodes, and the images sublayer, composed of the bases functions from
Φ. Together, these two sublayers deﬁne a single layer of our model. (b) An example of function-
specifying directed acyclic graphs (DAGs) that can be sampled from the network in (a).

In contrast, Evolutionary Algorithms (EAs) have been successful in symbolic regression as they can
ﬁnd interpretable, compact models that explain observed data [4]. EAs have been employed as an
alternative to gradient descent for optimizing neural networks, in what is called Neuroevolution [5–7].
Traditionally, these algorithms operate over populations of candidate solutions using methods inspired
by biological evolution, such as mutations and selection of the ﬁttest [4]. More recently, evolutionary
strategies that model a probability distribution over parameters, updating this distribution according
to their own best samples (i.e. selecting the ﬁttest), were found advantageous in optimization on high-
dimensional spaces, including neural networks’ hyperparameters [8, 9]. This approach is interesting
for the purposes of Neuroevolution, as keeping a probability distribution over the weights requires
less storage than keeping a population of networks over which selection occurs.

In this paper, we consider a mixed approach of connectionist and evolutionary optimization for
symbolic regression. We use a neural network to model a probability distribution over functions, and
optimize the model through a novel two-step gradient-descent and evolutionary strategy training. We
introduce a loss function that is tunable for different tasks. Our method handles non-differentiable and
implicit functions, converges to sparse, interpretable symbolic expressions, and can even outperform
state-of-the-art symbolic regression algorithms in testing on real world regression datasets. We also
introduce a number of strategies to induce compactness and simplicity, à la Occam’s Razor.

2 Model Architecture

(0)((cid:126)xp), . . . , f ∗

Layer structure A dataset D = {(cid:104)(cid:126)xp, (cid:126)yp(cid:105)}|D|
p=1 consists of pairs of inputs (cid:126)xp and targets (cid:126)yp =
(cid:126)f ∗ ((cid:126)xp) = [f ∗
(v−1)((cid:126)xp)](cid:62). Our goal is to compose either f ∗
(i)(·) or an approximation of
f ∗
(i)(·) using a predeﬁned collection of N basis functions Φ = {φi(·)}N
i=1, which act as primitives
for programs or functions. Note that bases can be repeated, their arity (number of arguments) is
not restricted to one, they may operate over different domains, and they may involve unspeciﬁed
constants (such as in φi(x) = xc). Furthermore, the concept of bases Φ is similar to that of DSL,
domain speciﬁc languages [10]. We concatenate the input (cid:126)x with a predeﬁned collection of constants
(e.g., π, 1, e) to build constant factors in our solution.

The architecture to solve this problem resembles a conventional network with L fully connected or
sparse layers with no bias. To incorporate the bases Φ in the network, we follow a similar approach
as [11–13], in which the bases act as activation functions on the nodes of the network. Speciﬁcally,
each hidden layer consists of an arguments sublayer and an images sublayer. The bases are stacked in
the images sublayer and act as activation functions for their respective nodes. Each basis takes in
nodes from the arguments sublayer. We call nodes in the arguments sublayer P-nodes, because they
behave probabilistically, as discussed in Section 2. Figure 1 highlights this sublayer structure, while
the supplemental material (SM) describes the complete mathematical formalism behind it.

2

Temperature-controlled connectivity To make the network more interpretable, we maximize
sparsity. There are numerous approaches to inducing sparsity in neural networks, including L1, L0
and L1/2 regularization [14–16], but these methods indiscriminately regularize all weights equally
without capturing structure within layers. We propose a sparsity method which uses the probabilistic
interpretation of the softmax function by sampling sparse paths through a network.

To promote probability-based sparsity, we use a network of T -softmax layers. For any temperature
T > 0, we deﬁne a T -softmax layer as a standard T -controlled softmax layer with weighted edges
connecting an images sublayer and the subsequent arguments sublayer in which each P-node from
the arguments sublayer probabilistically samples a single edge between itself and a node in the
images sublayer. We deﬁne w(l,i) as the weights for edges leading to the ith P-node of the lth
layer and W = (cid:8)w(l,i); 1 ≤ l ≤ L, 1 ≤ i ≤ N (cid:9) . Each node’s sampling distribution is given by
p(l,i)(·; T ) = softmax(w(l,i); T ), whose limit is a delta function as T → 0. Selecting these edges
for all T -softmax layers produces a sparse DAG specifying a function (cid:126)f , as seen in Figure 1b. These
sampled edges are encoded as sparse matrices, through which a forward pass evaluates (cid:126)f . The
probability of the model sampling f(i) as its ith output, qi(f(i)|W), is the product of the probabilities
of the edges of f(i)’s DAG. Similarly, q( (cid:126)f |W), the probability of the model sampling (cid:126)f , is given by
the product of (cid:126)f ’s edges, or q( (cid:126)f |W) = (cid:81)v−1
i=0 qi(f(i)|W). In practice, we set T to a ﬁxed, typically
small, number. The last layer is usually set to a higher temperature to allow more compositionality.
Moreover, the temperature can be scheduled in the spirit of simulated annealing [17, 18], which we
did not ﬁnd to be crucial empirically and thus discuss further in the SM.

A neural network as a probability distribution over functions Our model represents a dis-
tribution q(·|W) over a function space of all functions sampleable by the network, F L
Φ =
{all function compositions up to nesting depth L of Φ}. We can then deﬁne the optimal weights W∗
of our neural network as satisfying q( (cid:126)f |W∗) = 1 for some (cid:126)f such that (cid:126)f (x) = (cid:126)f ∗(x) for all x in the
domain of (cid:126)f ∗. Note that since q(·|·) is a probability distribution we have (cid:80)
q( (cid:126)f |W) = 1
and q( (cid:126)f |W) ≥ 0 for all (cid:126)f in F L
Φ. We initialize the network with weights Wi such that
q( (cid:126)f1|Wi) = q( (cid:126)f2|Wi) for all (cid:126)f1 and (cid:126)f2 in F L
Φ. After training, discussed in Section 3, the network
has weights Wf. The network then selects the function (cid:126)ff with the highest probability q( (cid:126)ff|Wf). We
discuss our algorithms for initialization and function selection in the SM. Note that our model can be
viewed as a stochastic computational graph (SCG). SCGs with discrete stochastic nodes have been
optimized using backpropagation through continuous relaxations of the discrete nodes [19–21]. In
contrast, here we optimize the SCG via an evolutionary strategy, as we explain in Section 3.

(cid:126)f ∈F L
Φ

Skip connections We use skip connections similar
to those in DenseNet [22] and ResNet [23], concate-
nating image states with those of subsequent layers,
as depicted in Figure 2. Skip connections yield sev-
eral desirable properties: (i) The network can ﬁnd
compact solutions as it considers all levels of com-
position. This promotes solution sparsity and inter-
pretability. (ii) Shallow layers are trained before or
alongside the subsequent layers due to more direct su-
pervision, because gradients can propagate to shallow
layers more easily to avoid exploding or vanishing
gradients. This may also allow subsequent layers to
behave as higher-order corrections to the solutions found in early layers. (iii) Primitives in shallow
layers can be reused, analogous to feature reuse in DenseNet.

Figure 2: Skip connections. Dotted lines and
colour: origin of the reused neurons.

3 Training

To express a wide range of functions, we include non-differentiable bases. In symbolic regression we
are interested in ﬁnding the few global minima that correspond to the optimal solution. Although
it is possible to incorporate known constants as inputs, it is often desireable to discover functions
with unspeciﬁed constants. To address these constraints, we propose a loss function and a alternating

3

training method that combine gradient based optimization and evolutionary strategies for efﬁcient
global exploration of the function space. We also propose regularization terms to improve ﬁtting.

(cid:0)f(i)((cid:126)x), (cid:126)y(cid:1) = (2πσ2)−1/2 exp

Loss Consider a mini-batch M = (cid:104)X, Y (cid:105), and a sampled function from the network (cid:126)f (·) ∼ q(·|W).
We compute the ﬁtness of each f(i)(·) with respect to a training pair (cid:104)(cid:126)x, (cid:126)y(cid:105) by evaluating the likelihood
ki
, which is a Normal distribution with
mean y and variance σ2, and measures how close f(i)((cid:126)x) is to the target ((cid:126)y)i. The likelihood can be
also viewed as a Bayesian posterior with a noninformative prior. The total ﬁtness is determined by
summing over the entire mini-batch: Ki

− (cid:2)f(i)((cid:126)x) − ((cid:126)y)i

(cid:0)f(i)((cid:126)x), (cid:126)y(cid:1).

(cid:0)M, f(i)

(cid:1) = (cid:80)

/(2σ2)

(cid:3)2

(cid:17)

(cid:16)

((cid:126)x,(cid:126)y)∈M ki

(cid:0)f(i)((cid:126)x), (cid:126)y(cid:1) characterizes the ﬁtness function’s smoothness. As σ2 → 0, the likeli-
The variance of ki
hood is a delta function with nonzero ﬁtness for some (cid:104)(cid:126)x, (cid:126)y(cid:105) only if f(i)((cid:126)x) = ((cid:126)y)i. Similarly, a large
variance characterizes a ﬁtness in which potentially many solutions provide accurate approximations,
increasing the risk of convergence to local minima. In the former case, learning becomes harder as
few f(i)(·) out of exponentially many samplable functions result in any signal, whereas in the later
case learning might not converge to the optimal solution. We let σ2 be a network hyperparameter,
tuned for the tradeoff between ease of learning and solution optimality for different tasks.

We now introduce a loss function for backpropagating on the weights of q(·|W):

Hqi [f(i), W, M] = −Ki

(cid:0)M, f(i)

(cid:1) · log (cid:2)qi(f(i)|W)(cid:3) .

(1)

We can interpret (1) as the cross-entropy of the posterior for the target and the probability of the
sampled function f(i). If the sampled function f(i) is close to f ∗
(i), then Ki(M, f(i)) will be large
and the gradient update below, will also be large:

∇WHqi

(cid:2)f(i), W, M(cid:3) = −

∇Wqi(f(i)|W)
qi(f(i)|W)

Ki

(cid:0)M, f(i)

(cid:1) .

(2)

The ﬁrst term on the right hand side (RHS) of update (2) increases the likelihood of the function f(i).
The second term on the RHS is maximal when f(i) ≡ f ∗
(i). Importantly, the second term approaches
zero as f(i) deviates from f ∗
(i). If the sampled function is far from the target, then the likelihood
update is suppressed by Ki(M, f(i)). Therefore, we only optimize the likelihood for functions close
to the target. Note that in (2) we backpropagate only through the probability of the function f(i)
(cid:0)f(i)|W(cid:1) , whose value does not depend on the bases in Φ, implying that the bases
given by qi
can be non-differentiable. This is particularly useful for applications requiring non-differentiable
basis functions. Furthermore, this loss function allows non-differentiable regularization terms, which
greatly expands the regularization possibilities.

Evolutionary strategy We ﬁnd that sampling R functions in each step and performing a gradient
step for each sampled function as deﬁned in (2) easily converges to inadequate local minima. Instead,
we propose an evolutionary strategy to update our model. We denote W(t) as the set of weights
at training step t, and we ﬁx two hyperparameters: R, the number of functions to sample at each
training step; and λ, or the truncation parameter, which deﬁnes the number of the R paths chosen for
optimization via (2). We initialize W(0) as described in Section 2. We then proceed as follows:

1. Sample R functions (cid:126)f1, . . . , (cid:126)fR ∼ q(·|W(t)). We denote the jth output of (cid:126)fi as fi(j).
2. For each output j, sort fi(j) from greatest to least value of Kj

(cid:1) and select the top
λ functions, yielding a total of vλ selected functions g1,j, . . . , gλ,j. The total loss is then
given by (cid:80)λ

j=0 Hqj [gi,j, W, M], which yields the training step gradient update:

(cid:0)M, fi(j)

(cid:80)v−1

i=1

λ
(cid:88)

v−1
(cid:88)

−

i=1

j=0

∇Wqj(gi,j|W)
qj(gi,j|W)

Kj(M, gi,j).

(3)

Notice that through (3) we have arrived at a modiﬁed REINFORCE update [24], where the
policy is qi(·|·) and the regret is the ﬁtness Ki(·, ·).

3. Perform the gradient step (3) on W(t) for all selected paths to obtain W(t+1). In practice,

we ﬁnd that the Adam algorithm [25] works well.

4

4. Set t = t + 1 and repeat from Step 1 until a stop criterion is met.

The beneﬁt of using Equation (3) versus (2) is that accumulating over the top-vλ best ﬁts to the target
allows for explorations of function compositions that contain desired components, but are not fully
developed. For example, if we train an implicit function with OccamNet, such as the hyperbola
x0x1 = 1, then the constant function f = 1 is always a best ﬁt. However, f = 1 does not capture the
desired behavior. While a composition that contains x0 might not be fully developed to x0x1, the
probability of choosing x0 should be increased, which is possible through (3). In practice, we ﬁnd
that reweighting the importance of the top-vλ routes, substituting K (cid:48)
j(M, gi,j) = Kj(M, gi,j)/i,
improves convergence speed by biasing updates towards the best routes as demonstrated in Section 6.

Two-step training To ﬁt constants, we use activation functions with unspeciﬁed constants and
combine the training process described in Section 3 with a constant ﬁtting training process. The two-
step training process works as follows: We ﬁrst sample a batch M and a function batch ( (cid:126)f1, . . . , (cid:126)fR).
Next, for each function (cid:126)fi, we ﬁt the unspeciﬁed constants in (cid:126)fi using gradient descent. Finally, we
perform the evolutionary training step on the constant-ﬁtted function batch. To increase training
speed, we store each function’s ﬁtted constants for reuse. See the SM for more details.

Recurrence OccamNet can also be trained to ﬁnd recurrence relations. To augment the training
algorithm, for each sampled function, we compute its recurrence to a maximum depth D, obtaining
a collection of RD functions. Training continues similarly to Section 3 in which we compute the
corresponding ﬁtness, select the best vλ, and update the weights. See the SM for more details.

Regularization To improve implicit function ﬁtting, we implement novel non-differentiable regular-
ization terms which punish trivial solutions, discussed in the SM. We deﬁne a modiﬁed ﬁtness function
to incorporate regularization terms: K (cid:48)
2πσ2 is the
maximum value of Ki(M, f ), and r represents the regularization term. Because Ki(M, f ) acts as a
scaling constant for the probability term, subtracting from it reduces the gradient increase of weights
corresponding to undesired functions. If K (cid:48)
i(M, f ) < 0, the weights of these undesired functions
decrease. This is similar to reducing the variance of REINFORCE-based gradient estimators.

i(M, f ) = Ki(M, f )−s·r[f ], where s = |M| /

√

4 Related Work

Symbolic regression OccamNet was partially inspired by EQL network [11–13], a neural network-
based symbolic regression system which successfully ﬁnds simple analytic functions. Neural Arith-
metic Logic Units (NALU) and related models [26, 27] provide neural inductive bias for arithmetic in
neural networks, which in principle can ﬁt some benchmarks in Table 1. NALU updates the weights
by backpropagating through the activations, shaping the neural network towards a gating interpreta-
tion of the linear layers. However, generalizing those models to a diverse set of function bases might
be a formidable task: from our experiments, backpropagation through some activation functions
(such as division or sine) makes training considerably harder. In a different computation paradigm,
genetic programming (GP) has performed exceptionally well at symbolic regression [28, 29], and a
number of evolution-inspired, probability-based models have been explored for this goal [30].

A concurrent work [31] explores deep symbolic regression by using an RNN to search the space of
expressions by autoregressive generation of expressions. Interestingly, the authors observed that a
risk-aware reinforcement learning approach is a necessary component in their optimization, which is
similar to our approach of selecting the top λ function for optimization in Step 2 of our algorithm.
A notable difference is that OccamNet does not generate the expressions autoregressively, while it
still exhibits gradual increase in modularity during training, as discussed in Section 6. This is also a
beneﬁt both for speed and scalability. Moreover, their entropy regularization is an alternative to our
regularization. Marrying our approach with theirs is a promising direction for future work.

Program synthesis For programs, one option to ﬁt programs is to use EQL-based models with logic
activations (step functions, MIN, MAX, etc.) approximated by sigmoid activations. Another is proba-
bilistic program induction using domain-speciﬁc languages [32–34]. Neural Turing Machines [35, 36]
and their stable versions [37] are also able to discover interpretable programs, simulated by neural
networks via observations of input-output pairs by relying on an external memory. Balog et al.

5

[38] ﬁrst train a machine learning model to predict a DSL based on input output pairs and then use
a methods from satisﬁability modulo theory [39] to search the space of programs built using the
predicted DSL. In contrast, our DSL is lower level and can ﬁt components like “sort” instread of
including them in the DSL directly. Kurach et al. [40] develop a neural model for simple algorithmic
tasks by utilizing memory access for pointer manipulation and dereferencing. However, here we
achieve similar results (for example, sorting) without external memory and in only minutes on a CPU.

Integration with deep learning We are not aware of classiﬁers that predict MNIST [41]3 or
ImageNet [42]4 labels using symbolic rules as considered in the next section. The closest baseline
we found is using GP [43], which performs comparably well to our neural method, but cannot easily
integrate with deep learning. In the reinforcement learning (RL) domain, Such et al. [7], Salimans
et al. [44] propose to train deep models of millions of parameters on standard RL tasks using a
gradient-free GP, which is competitive to gradient-based RL algorithms.

SCGs and pruning Treating the problem of ﬁnding the correct function or program as a stochastic
computational graph is appealing due to efﬁcient soft approximations to discrete distributions [19–21].
Our T -softmax layers offer such an approximation and could further beneﬁt from an adaptive softmax
methodology [45], which we leave for future work. Furthermore, the sparsity induced by T -softmax
layers parallels the abundant work on pruning connections and weights in neural networks [46, 47] or
using regularizations, encouraging sparse connectivity [48, 14].

5 Experiments

To empirically validate our model, we ﬁrst develop a diverse collection of benchmarks in ﬁve
categories: Analytic functions, simple, smooth functions; Implicit functions, functions specifying
an implicit relationship between inputs; Programs, non-differentiable operations; Image/Pattern
Recognition, patterns explained by analytic expressions.

For our experiments, we terminate learning when the top-vλ sampled functions all return the same
ﬁtness K(·, f ) for 30 consecutive epochs. If this happens, these samples are equivalent function
expressions. Computing the most likely DAG allows retrieval of the ﬁnal expression. If this ﬁnal
expression matches the correct function, we determine that the network has converged. For pattern
recognition, there is no correct target composition, so we measure the accuracy of the classiﬁcation
rule on a test split, as is conventional.

In all experiments, if termination is not met in a set number of steps we consider it as not converged.
We also keep a constant temperature for all the layers except for the last one. An increased last layer
temperature allows the network to explore higher function compositionality, as shallow layers can be
further trained before the last layer probabilities become concentrated; this is particularly useful for
learning functions with high degrees of nesting. More details on hyperparameters for experiments are
in the SM. Our network converges rapidly, often in only a few seconds and at most a few minutes.

We ﬁrst compare the results with that of Eureqa, a software package for symbolic regression [28].
Eureqa uses an evolutionary strategy to ﬁt functions and is highly optimized, allowing it to evaluate
billions of functions per second. In the successful cases, Eureqa is able to ﬁnd the correct answer
on the order of 1 second, which is often signiﬁcantly faster than OccamNet. However, although
Eureqa serves as a useful baseline, we emphasize that Eureqa, unlike our architecture, cannot be
easily integrated with neural networks. Thus, the benchmark results should not be taken as an exact
one-to-one comparison. The results are shown in Table 1 and we discuss them below.

Analytic and implicit functions We highlight the large success rate for the function f (x) =
(x2 + x)/(x + 2) (line 4 in Table 1), which we originally speculated could easily trick the network
with the local minimum f (x) ≈ x − 1 for large enough x. In contrast, as with the difﬁculties faced
by Udrescu and Tegmark [29], we ﬁnd that f (x0, x1) = x2
1 (line 5) often failed to
converge because the factor x2
0; even when convergence did occur,
it required a relatively large number of steps for the network to resolve this additional constant factor.
Notably, Eureqa had difﬁculty ﬁnding the equation (cid:80)3
n=1 sin(nx) (line 3). Eureqa was unable to ﬁt

0(x0 + 1) was approximated to x3

0(x0 + 1)/x5

3Creative Commons Attribution Share Alike 3.0 License
4The Creative Commons Attribution (CC BY) License

6

Table 1: Holistic benchmarking. η is the success rate from 10 trials, the proportion of trials which
converge to the correct function. sec. is the average number of seconds for convergence. ηb is the
baseline success rate (out of 10) run for the allotted seconds in the table. A is the best accuracy from
10 trials, and Ab is the baseline accuracy. For all but “Image Recognition,” the baseline is Eureqa.
For “Image Recognition,” the baseline above the mid-line is HeuristicLab [49] and the baseline below
the mid-line is a feed-forward neural network with the same number of parameters as OccamNet.

Analytic Functions

η

1.0
0.8
0.7
0.9
0.3
0.6

1.0
0.8

η

1.0

1.0
1.0
0.9

1.0

#

Targets

1
2
3 (cid:80)3
4
5
6

2x2 + 3x
sin(3x + 2)

n=1 sin(nx)
(x2 + x)/(x + 2)
x2
0(x0 + 1)/x5
1
0/2 + (x1 + 1)2/2
x2
10.5x3.1
cos(x)

7
8

#

9

10
11
12

Implicit Functions

Targets

x0x1 = 1
0 + x2
x2
x0/ cos(x1) = 1
x1/x0 = 1

1 = 1

13 m1v1 − m2v2 = 0

Image Recognition

#

22

23
24

Targets

MNIST Binary

MNIST Trinary
ImageNet Binary

A

92.9

59.6
70.7

sec.

5
56
190
81
305
83

553
410

sec.

294

153
131
232

270

sec.

150

400
400

ηb

1.0
1.0
0.0
0.7
1.0
0.7

∗0.0
1.0

ηb

1.0

0.6
1.0
1.0

0.0

Ab

92.8

81.2
78.0

#

14
15
16
17
18

19

20

21

25
26

Programs

Targets

3x if x > 0, else x
x2 if x > 0, else −x
x if x > 0, else sin(x)
SORT(x0, x1, x2)
4LFSR(x0, x1, x2, x3)

y0((cid:126)x) = x1 if x0 < 2,

else −x1

y1((cid:126)x) = x0 if x1 < 0,

else x2
1

η

0.7
1.0
1.0
0.7
1.0

sec.

26
10
236
81
14

ηb

1.0
1.0
1.0
1.0
1.0

0.3

157

0.1

g(x) = x2 if x < 2,

y(x) = g◦4(x)

else x/2

1.0

64

0.0

g(x) = x + 2 if x < 2,

else x − 1

1.0

64

0.6

y(x) = g◦2(x)

Backprop OccamNet
Finetune ResNet

98.1
97.3

37
200

97.7
95.4

the function 10.5e3.1, because it cannot ﬁt noninteger exponents. However, Eureqa does ﬁnd close
approximations such as 11.6x3 + 0.161x4, which has an R2 value of 0.999995. For cos(x) (line
8 in Table 1) we investigated whether OccamNet could discover a formula for cosine using only
the bases sin(·), +(·, ·), and ÷(·, ·) and the constants 2 and π. We expected OccamNet to discover
cos(x) = sin(x + π/2), but, interestingly, it instead always identiﬁed the double angle identity
cos(x) = sin(2x)/(2 sin(x)). We also tested whether OccamNet could discover Taylor polynomials
of ex. OccamNet identiﬁed ex ≈ 1 + x + x2/2, but was unable to discover the subsequent x3/6 term.
We discuss implicit functions in Section 6, since on them OccamNet demonstrates sizable advantage.

Programming We benchmark the ability to ﬁnd several non-differentiable, potentially recur-
sive/iterative functions. From our experiments, we highlight both the network’s fast convergence to
the right functional form and the discovery of the correct recurrence depth of the ﬁnal expression.
This is pronounced in line 20 in Table 1, which is a challenging chaotic series on which Eureqa
struggles. We also investigated the usage of bases such as MAX and MIN for the purpose of sorting
numbers (line 17), obtaining relatively well-behaved ﬁnal solutions: the few solutions that did not
converge fail only in deciding the second component y2 of the output vector. Finally, we introduced
binary operators and discrete input sets for testing a simple 4-bit LFSR (line 18), the program
(x0, x1, x2, x3) → (x0 + x3 mod 2, x0, x1, x2), which converges fast with a high success rate.

Image Recognition We train OccamNet to classify MNIST in a binary setting between the digits 0
and 7 (MNIST Binary). For this high-dimensionality task, we implement OccamNet on an Nvidia
V100 GPU, yielding sizable 8x speed increase compared to a CPU. In line 22 one of the successful
functional ﬁts that OccamNet ﬁnds is y0 ((cid:126)x) = tanh (10(max(x715, x747) + tanh(x435) + 2x710 +
2x713)) and y1 ((cid:126)x) = tanh (10 tanh(10 (x512 + x566))) . The model learns to incorporate pixels
into the functional ﬁt that are indicative of the class: here x512 and x566 are indicative of the digit 7.
These observations hold true when we further benchmark the integration of OccamNet with deep
feature extractors (lines 24-26). We extract features from ImageNet images using a ResNet 50 model,
pre-trained on Imagenet [23]. For simplicity, we select two classes, “minivan” and “porcupine,”
(ImageNet Binary). OccamNet signiﬁcantly improves its accuracy backpropagating through our
model using a standard cross-entropy signal (lines 25-26). We either freeze the ResNet weights

7

(Backprop OccamNet) or ﬁnetune ResNet through OccamNet (Finetune ResNet). In both cases
the converged OccamNet represents simple rules (y0((cid:126)x) = x1838, y1((cid:126)x) = x1557) suggesting that
replacing the head in deep neural networks with OccamNet might be promising.

Real world regression datasets We also test OccamNet’s ability to ﬁt real world datasets, selecting
15 datasets with 1666 or fewer datapoints from the Penn Machine Learning Benchmarks (PMLB5)
regression datasets [50]. We compare OccamNet to a genetic algorithm implemented using DEAP
[51] with Epsilon-Lexicase (Eplex) selection [52], which was identiﬁed in a large benchmark study
by Orzechowski et al. [53] as the top-performing genetic method for modeling data, in terms of
validation loss for PMLB tabular regression datasets. We also compare OccamNet to AI Feynman
2.0 (AIF) [29, 54], a state of the art method in symbolic regression.6

We test OccamNet twice. For the ﬁrst test, which we label “OccamNet,” we test exactly 1,000,000
functions, the same number as we test for Eplex. For the second test, which we label “V100,”
we exploit our architecture’s integration with the deep learning framework by running OccamNet
on an Nvidia V100 GPU and testing a much larger number of functions. We allow AIF to run
for approximately as long or longer than OccamNet for each dataset. We perform grid search on
hyperparamenters, discussed in the SM, and identify the ﬁts with the best training, validation, and
testing Mean Squared Error (MSE) losses. The raw data from these experiments is shown in the SM.

Figure 3 shows the relative performance of OccamNet and comparison datasets according to several
metrics. As shown in Figure 3a,b,c, overall Eplex outperforms OccamNet in training and testing
MSE loss, but OccamNet outperforms Eplex in validation loss. We speculate that OccamNet’s
performance drop between the validation and testing datasets results from overﬁtting from the larger
set of hyperparameter combinations for OccamNet (details in the SM).

Additionally, OccamNet runs faster than Eplex in nearly all datasets tested, often by an order of
magnitude (Figure 3d). Furthermore, OccamNet is highly parallel and can easily scale on a GPU.
Thus, a major advantage of OccamNet is its speed and scalability. Comparing V100 and Eplex
demonstrates that OccamNet continues to improve when testing more functions. V100 performs
worst in the comparison based on testing MSE (see Figure 3e), but it still outperforms Eplex at 10/15
of the datasets, while running more than 9 times faster. Thus OccamNet’s speed and scalability can be
exploited to greatly increase its accuracy at symbolic regression. This demonstrates that OccamNet is
a powerful alternative to genetic algorithms for interpretable data modeling.

OccamNet also outperforms AIF for training, validation, and testing, while running faster. OccamNet
acheives a lower training and validation MSE than AIF for every dataset tested. For training
loss, OccamNet performs better than AIF in 4/7 datasets (Figure 3f). V100 performs even better.
Additionally, AIF performs polynomial ﬁtting, which OccamNet does not, giving it an additional
advantage. However, the datasets we test are a worst case for AIF; the datasets are small, have no
known underlying formula, and we normalize the data prior to training, meaning that AIF will likely
struggle not to overﬁt with its neural network and will also be less likely to ﬁnd graph modularities.

6 Discussion

Limitations Since our experimental settings did not require very large depths, we have not tested
the limits of OccamNet in terms of depth rigorously (preliminary results on increasing the depth for
pattern recognition are in the SM). We expect increasing depth to yield signiﬁcant complications,
as the search space would become exponentially large. We recognize the need of creating symbolic
regression benchmarks that would require expressions that are large in depth. We believe that
concurrent contributions [31] would also beneﬁt from such benchmarks. Another direction where
OccamNet might be improved is low level optimization that would make the method more efﬁcient
to train. For example, in our PMLB experiments, we estimate that OccamNet performs >8x as many
computations as necessary. Eplex may also beneﬁt from optimization. Finally, similarly to other
symbolic regression methods, OccamNet requires a speciﬁed basis to ﬁt a dataset. While it is a

5Creative Commons Attribution 4.0 International License
6AIF’s regression algorithm examines all possible feature subsets, which grows exponentially with the
number of features. Accordingly, we only test the datasets with 10 or fewer features. AI Feyman failed to run on
a few datasets. All remaining datasets are included in tables and ﬁgures.

8

Figure 3: A bar chart showing the relative performance between OccamNet and two baseline methods,
Eplex and AIF. The x-axis is the dataset involved. The y axis is the relative performance according
to the given metric: the MSE on the training, validation, or testing set, or the training time. To
compute this relative performance, we divide the higher (worse) performance value by the lower
(better) performance value for each dataset. The green bars represent datasets where OccamNet has a
lower (better) performance value than the comparison baseline method, and the red bars represent the
datasets where the comparison method has a better performance than OccamNet.

notable advantage of OccamNet to have non-differentiable bases, further work needs to be done to
explore optimization at a meta level that discovers appropriated bases for the datasets of interest.

Advantages OccamNet’s learning procedure allows it to combine partial solutions into bet-
ter results. For example in Figure 4,
the correct function’s probability increases by more
than 100 times before being sampled because OccamNet samples similar approximate solutions.

OccamNet successfully ﬁts many implicit func-
tions that other neurosymbolic architectures
struggle to ﬁt because of the non-differentiable
regularization terms required to avoid trivial so-
lutions. Although Eureqa also ﬁts many of these
equations, we ﬁnd that it sometimes requires the
data to be ordered by some latent variable and
struggles when the dataset is very small. This
is likely because Eureqa numerically evaluates
implicit derivatives from the dataset [55], which
can be noisy when the data is sparse. While
Schmidt and Lipson [55] proposes methods for
analyzing unordered data, it is unclear whether
these methods have been implemented in Eu-
reqa. Thus, OccamNet seems to shine in its
ability to ﬁt unordered and small datasets de-
scribed by implicit equations (e.g. momentum
conservation in line 5 in Table 1).

Figure 4: Gradual modularity with training. Dark
blue is the correct function. Light blue is a sub-
optimal ﬁt with high probability early in training.
Red corresponds to the correct function. The insets
show the ﬁrst sample of the correct function.

To our knowledge, a unique advantage of our
method is that OccamNet represents complete
analytic expressions with a single forward pass, which allows sizable gains when using an AI
accelerator, as demonstrated by our experiments on V100 (Figure 3). Furthermore, because of this
property, OccamNet, can be easily integrated with components from the standard deep learning
toolkit. For example, lines 25-26 in Table 1 demonstrate integration and joint optimization with
neural networks, which is not possible with Eureqa. We also conjecture that such integration with
autoregressive approaches [31] might be challenging as the memory and latency would increase.

9

1234567891011121314151.01.11.21.31.41.5Higher/Lowera)1.061.011.041.271.311.041.151.11.041.021.041.021.111.181.38OccamNet v. Eplex, TrainingOccamNet Performs BetterComparison Performs Better1234567891011121314151.01.52.02.53.03.5b)1.081.262.621.021.131.351.361.612.663.261.11.091.031.021.47OccamNet v. Eplex, Validation12345678910111213141512345c)1.062.471.111.151.531.911.131.141.055.081.651.142.231.293.77OccamNet v. Eplex, Testing123456789101112131415Dataset2.55.07.510.012.515.017.520.0Higher/Lowerd)2.041.0519.619.518.46.015.919.5417.94.934.7717.21.031.021.37OccamNet v. Eplex, Timing123456789101112131415Dataset1.01.52.02.53.0e)1.581.051.121.091.131.561.993.11.01.321.041.01.81.111.29OccamNetV100 v. Eplex, Testing146710111315Dataset12345f)2.951.252.081.31.521.711.265.13OccamNet v. AIF, Testing0200400Epoch106105104103102101Probabilitysin(x0+x0)sin(x0)+sin(x0)sin(x0)+sin(x0)sin(x0)+sin(x0)050100150Times Sampled7 Conclusion and Future Work

We motivated, introduced and tested OccamNet, a fast neural model for symbolic regression, on
a wide range of benchmarks. Potential stakeholders that might beneﬁt from our contributions are
institutions and scientists who would like to explain their datasets with symbolic expressions. With
OccamNet they might be able to explain their data with analytic expressions quickly and cheaply.

Currently our method is not explicitly designed against adversarial attacks. Thus, malicious stake-
holders could exploit our method and manipulate the symbolic ﬁts that OccamNet produces. A
potential direction towards alleviating the problem would be to explore ways to robustify OccamNet
by training it against an adversary. Therefore, we leave tackling adversarial robustness of neural
models for symbolic regression as an exciting direction for future work.

Acknowledgments and Disclosure of Funding

We would like to thank Thiago Bergamaschi, Kristian Georgiev, Andrew Ma, Peter Lu, Evan
Vogelbaum, Laura Zharmukhametova, Momchil Tomov, Rumen Hristov, Charlotte Loh, Ileana
Rugina and Lay Jain for fruitful discussions.

Research was sponsored in part by the United States Air Force Research Laboratory and was
accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions
contained in this document are those of the authors and should not be interpreted as representing the
ofﬁcial policies, either expressed or implied, of the United States Air Force or the U.S. Government.
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation herein.

This material is based upon work supported in part by the U.S. Army Research Ofﬁce through the
Institute for Soldier Nanotechnologies at MIT, under Collaborative Agreement Number W911NF-18-
2-0048.

Additionally, Owen Dugan would like to thank the United States Department of Defence for sponsor-
ing his research and for allowing him to attend the Research Science Institute for free.

Supplemental Material

We have organized the Supplemental Material as follows:

• In Section A we present further details about our mathematical formalism.

• In Section B we present our initialization algorithms.

• In Section C we present our function selection algorithm.

• In Section 3 we present our method for allowing OccamNet to model recurrence.

• In Section E we discuss our regularization terms.

• In Section F we discuss our methods for accounting for undeﬁned functions.

• In Section G we present our experimental hyperparameters.

• In Section H we present further details about the setup and hyperparameters for our experi-

ments with the PMLB Datasets. We also present our raw results.

• In Section I we examine the models each method provides for the PMLB Datasets.

• In Section J we present a series of ablation studies.

• In Section K we discuss neural models for sorting and pattern recognition.

• In Section L we discuss the evolutionary strategies for ﬁtting functions and programs that

we use as benchmarks.

• In Section M, we catalogue our code and video ﬁles.

10

A Mathematical Formalism

Here we introduce the full mathematical formalism behind OccamNet. As described in the main text,
we start from a predeﬁned collection of N basis functions Φ = {φi(·)}N
i=1. Each neural network
layer is deﬁned by two sublayers, the arguments and image sublayers. For a network of depth L, each
of these sublayers is reproduced L times. Now let us introduce their corresponding hidden states:
each l-th arguments sublayer deﬁnes a hidden state vector (cid:101)h(l), and similarly each l-th image sublayer
deﬁnes a hidden state h(l), as follows

(cid:101)h(l) =

(cid:104)
(cid:101)h(l)
1 , . . . , (cid:101)h(l)

M

(cid:105)

, h(l) =

(cid:104)
h(l)
1 , . . . , h(l)

N

(cid:105)

.

These vectors are related through the bases functions:

h(l)
i = φi

(cid:16)
j+1, . . . , (cid:101)h(l)
(cid:101)h(l)

j+α(φi)

(cid:17)

, j =

(cid:88)

α(φk), M =

(cid:88)

α(φk),

0≤k<i

0≤k≤N

(4)

(5)

where α(φ) is the arity of function φ(·, . . . , ·). This formally expresses how the arguments connect
to the images in any given layer, visualized as the bold edges between sublayers in Figure 1 in the
main paper. To complete the architecture and connect the images from layer l to the arguments of
layer (l + 1), we use the described softmax transformation: 7

W(T ) · h(l) =

≡








softmax(w1; T )(cid:62)
...


softmax(wMl+1; T )(cid:62)

(cid:101)h(l+1)
...



(cid:101)h(l+1)
Ml+1

= (cid:101)h(l+1),







1






h(l)
1
...
h(l)
Nl






(6)

where the hidden states h(l) and (cid:101)h(l+1) have Nl and Ml+1 coordinates, respectively.
From Equation (5), we see that Ml+1 = M = (cid:80)
0≤k≤N α(φk). If no skip connections are used,
Nl = N = |Φ|. If skip connections are used, however, Nl grows as l increases. We demonstrate how
the scaling grows as follows.

Let u be the number of inputs and v be the number of outputs. When learning connections from
images to arguments at layer l (1 ≤ l ≤ L) there will be skip connections from the images of the
previous l − 1 layers 1, . . . , l − 1. Hence each layer produces the following list of numbers of images
{u + (i + 1)N }L
i=0. We learn linear layers from these images to arguments, and the number of
arguments is always M . Thus, in total we have the following number of parameters:

v(u + (L + 1)N ) + M

L−1
(cid:88)

(u + (i + 1)N ) ∈ O(N M L2),

i=0

Along with the added inputs and constants, this description fully speciﬁes the mathematical structure
of our architecture.

B Initialization

We originally initialized all model weights to 1. However, this initializes complex functions, which
have DAGs with many more edges than simple functions, to low probabilities. As a result, we found
in practice that the network sometimes struggled to converge to complex functions with high ﬁtness
K(M, f ) because their initial low probabilities meant that they were sampled far less often than
simple functions. This is because even if complex functions have a higher probability increase than

(cid:20)

7as before, we deﬁne for any z = [z1, . . . , zNl ] the softmax function as follows softmax(z; T ) :=
exp(z1/T )
i=1 exp(zi/T )

exp(zNl
i=1 exp(zi/T )

, . . . ,

/T )

(cid:80)Nl

(cid:21)(cid:62)

(cid:80)Nl

11

simple functions when they are sampled, the initial low probabilities caused the complex functions to
be sampled far less and to have an overall lower expected probability increase.

To address this issue, we use a second initialization algorithm, which initializes all functions to equal
probability.

This initialization algorithm iterates through the layers of the network. It establishes as an invariant
that, after assigning the weights up to the lth layer, all paths leading to a given node in the lth
argument layer have equal probabilities. Then, each argument layer node has a unique corresponding
probability, the probability of all paths up to that node. We denote the probability of the ith node in
the lth argument sublayer as (cid:101)p(l)
. Because each argument layer node has a corresponding probability,
each image layer node must also have a unique corresponding probability, which, for the ith node in
the lth image sublayer, we denote as p(l)
i

. These image layer probabilities are given by

i

p(l)
i =

n+α(φi)
(cid:89)

k=n+1

(cid:101)p(l)
k , n =

i−1
(cid:88)

j=1

α(φj).

(7)

, n(l)

i )}k

i=1 representing n(l)

i nodes with probability p(cid:48)(l)

Our algorithm initializes the input image layer’s nodes to probability 1. As the algorithm iterates
through all subsequent T -Softmax layers, the invariant established above provides a system of linear
equations involving the desired connection probabilities, which the algorithm solves. The algorithm
groups the previous image layer according to the node probabilities, obtaining a set of ordered pairs
{(p(cid:48)(l)
in the lth layer. Note that if two image
i
nodes have the same probability, for each P -node in the arguments sublayer, the edges between
the image nodes and the P -node must have the same probability in order to satisfy the algorithm’s
invariant. Then, we deﬁne p(cid:48)(l,j)
as the probability of the edges between the image nodes with
probability p(cid:48)(l)
and the jth argument P -node of the lth layer. The probabilities of the edges to
i
a given P -node sum to 1, so for each j, we must have (cid:80)
= 1. Further, the algorithm
requires that the probability of a path to a P -node through a given connection is the same as the
probability of a path to that P -node through any other connection. The probability of a path to the
jth P -node through a connection with probability p(cid:48)(l,j)
, so we obtain the equations
p(cid:48)(l)

i
, for all i and j. These two constraints give the vector equation

i nip(cid:48)(l,j)

i p(cid:48)(l,j)

is p(cid:48)(l)

0 p(cid:48)(l,j)

0 = p(cid:48)(l)

i

i

i

i

i p(cid:48)(l,j)


i









1

n(l)
n(l)
0
1
p(cid:48)(l)
0 −p(cid:48)(l)
p(cid:48)(l)
0
...
p(cid:48)(l)
0

0
...
0

n(l)
2
0
−p(cid:48)(l)
2
...
0

n(l)
k
0
0
...

· · ·
· · ·
· · ·
. . .
· · · −p(cid:48)(l)
k





















p(cid:48)(l,j)
0
p(cid:48)(l,j)
1
p(cid:48)(l,j)
2
...
p(cid:48)(l,j)
k











=










1
0


0

...


0

,

for all 1 ≤ j ≤ M . The algorithm then solves for each p(cid:48)(l,j)
After determining the desired probability of each connection of the lth layer, the algorithm com-
putes the SPL weights w(cid:48)(l,j) that produce the probabilities p(cid:48)(l,j)
. Since there are inﬁnitely many
possible weights that produce the correct probabilities, the algorithm sets w(cid:48)(l,j)
0 = 0. Then, the
algorithm uses the softmax deﬁnition of the edge probabilities to determine the required value of
(cid:80)k

.

i

i

(cid:16)
m=1 exp

w(cid:48)(l,j)
i

/T (l)(cid:17)

:

p(cid:48)(l,j)

0 =

(cid:16)

exp

w(cid:48)(l,j)
0
(cid:16)

/T (l)(cid:17)

(cid:80)k

m=1 exp

w(cid:48)(l,j)

m /T (l)

=

(cid:80)k

m=1 exp

1
(cid:16)
w(cid:48)(l,j)

m /T (l)

(cid:17)

(cid:17)

so

k
(cid:88)

m=1

(cid:16)

exp

w(cid:48)(l,j)

m /T

(cid:17)

= 1/p(cid:48)(l,j)

0

.

12

Substituting this equation into the expression for the other probabilities gives

p(cid:48)(l,j)

i = exp

(cid:16)

w(cid:48)(l,j)
i

/T (l)(cid:17)
/

(cid:33)

(cid:16)

exp

wi/T (l)(cid:17)

(cid:32) k

(cid:88)

m=1

Solving for w(cid:48)(l,j)

i

gives

= p(cid:48)(l,j)
0

(cid:16)

exp

w(cid:48)(l,j)
i

/T (l)(cid:17)

.

w(cid:48)(l,j)

i = T (l) log

(cid:16)

p(cid:48)(l,j)
i

/p(cid:48)(l,j)
0

(cid:17)

,

which the algorithm uses to compute w(cid:48)(l,j)

.

i
the algorithm assigns them to the corresponding w(l,j)

After determining the weights w(cid:48)(l,j)
particular, if the ith image node has probability p(cid:48)(l)
given by w(l,j)
(cid:101)p(l+1)
process for subsequent layers until it reaches the end of the network.

. Finally, the algorithm determines p(l+1)

. In
k , the weights of edges to the ith node are
, given by
using Equation 7 and repeats the above

, for all j. The algorithm then determines the values of (cid:101)p(l+1)

= w(cid:48)(l,j)
k

i
= p(l)

1 p(l,i)

1

i

i

i

i

i

This algorithm efﬁciently equalizes the probabilities of all functions in the network. In practice,
however, we ﬁnd that perfect equalization of functions causes activation functions with two inputs to
be highly explored. This is because there are many more possible functions containing activation
functions with two inputs than with one input. In practice, therefore, we ﬁnd that a balance between
initializing all weights to one and initializing all functions to equal probability is most effective for
exploring all types of functions.

To implement this balance, we create an equalization hyperparameter, E. If E = 0, we initialize all
weights to 1 as in the original OccamNet architecture. If E (cid:54)= 1, we use the algorithm presented
above to initialize the weights, and then divide all of the weights by E. For E > 1, this has the effect
of initializing weights between the two initialization approaches. In practice, we ﬁnd that values of
E = 1 and E = 5 are most effective for exploring all types of functions (See Section H).

C Function Selection

As discussed in the main text, after training using an evolutionary strategy, the network selects the
function (cid:126)f with the highest probability q( (cid:126)f |W).

We develop a dynamic programming algorithm which determines the DAG with the highest proba-
bility. The algorithm steps sequentially through each argument layer, and at each argument layer it
determines the maximum probability path to each argument node. Knowing the maximum probability
paths to the previous argument layer nodes allows the algorithm to easily determine the maximum
probability paths to the next argument layer.

As with the network initialization algorithm, the function selection algorithm associates the ith
P -node of the lth argument sublayer with a probability, (cid:101)p(l)
, which represents the highest probability
path to that node. Similarly, we let p(l)
represent the assigned probability of the ith node of the lth
i
image sublayer, deﬁned as the highest probability path to a given image node. p(l)
can once again be
i
determined from (cid:101)p(l)
i using Equation 7. Further, the algorithm associates each node with a function,
(cid:101)f (l)
for argument nodes and f (l)
for image nodes, which represents the highest probability function
to the corresponding node. Thus, (cid:101)f (l)
. Further, f (l)
is determined from (cid:101)f (l)

has probability (cid:101)p(l)

has probability p(l)
i

, and f (l)

using

i

i

i

i

i

i

i

i

f (l)
i ((cid:126)x) = φi

(cid:16)

n+1((cid:126)x), . . . , (cid:101)f (l)
(cid:101)f (l)

(cid:17)
n+α(φj )((cid:126)x)

, n =

i−1
(cid:88)

j=1

α(φj).

(8)

13

The algorithm iterates the the networks layers. At the lth layer, it determines the maximum probability
path to each argument node, computing

(cid:101)p(l+1)

i

= MAX

(cid:16)

(cid:101)f (l+1)

i

=






f (l)
0
f (l)
1
...
f (l)
N

0

p(l)
0 p(l,i)
if (cid:101)p(l+1)
if (cid:101)p(l+1)

i

i

, . . . , p(l)

(cid:17)

N

N p(l,i)
0 p(l,i)
1 p(l,i)

1

0

.

= p(l)
= p(l)
...
= p(l)

if (cid:101)p(l+1)

i

N p(l,i)

N

and
using Equations 7 and 8, respectively. The algorithm repeats this process until it reaches the

Next, it determines the maximum probability path up to each image node, computing p(l+1)
f (l+1)
i
output layer, at which point it returns (cid:126)fmax = [ (cid:101)f (L)

N ](cid:62) and pmax = (cid:81)N

, . . . , (cid:101)f (L)

1

.

i

i

i=1 (cid:101)p(L)

D Recurrence

OccamNet can also be trained to ﬁnd recurrence relations, which is of particular interest for programs
that rely on FOR or WHILE loops. To ﬁnd such recurrence relations, we assume a maximal recursion
D. We use the following notation for recurring functions: f ◦(n+1)(x) ≡ f ◦n(f (x)), with base case
f ◦1(x) ≡ f (x).
To augment the training algorithm, we ﬁrst sample ( (cid:126)f1, . . . , (cid:126)fR) ∼ q(·|W(t)). For each (cid:126)fi, we
(cid:16) (cid:126)f ◦1
compute its recurrence to depth D as follows
, obtaining a collection of RD
functions. Training then continues as usual; we compute the corresponding Kj(M, (cid:126)f ◦n
i(j)), select the
best vλ, and update the weights. It is important to note that we consider all depths up to D since our
maximal recurrence depth might be larger than the one for the target function.

, . . . , (cid:126)f ◦D

, (cid:126)f ◦2
i

(cid:17)

i

i

Note that we do not change the network architecture to accommodate for recurrence depth D > 1.
As described in the main text, we can efﬁciently use the network architecture to evaluate a sampled
function (cid:126)f ((cid:126)x) for a given batch of (cid:126)x. To incorporate recurrence, we take the output of this forward
pass and feed it again to the network D times, similar to a recurrent neural network. The resulting
outputs are evaluations

for a given batch of (cid:126)x.

(cid:16) (cid:126)f ◦1

i ((cid:126)x), . . . , (cid:126)f ◦D

i ((cid:126)x), (cid:126)f ◦2

((cid:126)x)

(cid:17)

i

E Regularization

As discussed in the main text, to improve implicit function ﬁtting, we implement a regularized loss
function,

K (cid:48)

i(M, f ) = Ki(M, f ) − s · r[f ],

√

for some regularization function r, where s = n(M)/
Ki(M, f ). We deﬁne

2πσ2 is the maximum possible value of

r[f ] = wφ · φ[f ] + wψ · ψ[f ] + wξ · ξ[f ] + wγ · γ[f ],

where φ[f ] measures trivial operations, ψ[f ] measures trivial approximations, ξ[f ] measures the
number of constants in f , γ[f ] measures the number of activation functions in f, and wφ, wψ, wξ, and
wγ are weights for their respective regularization terms. We now discuss each of these regularization
terms in more detail.

A The Phi Term

The φ[f ] term measures whether the unsimpliﬁed form of f contains trivial operations, operations
which cause an expression to simplify. For example, division is a trivial operation in x/x, because
the expression simpliﬁes to 1. Similarly, 1 · x, x1, and x0 are all trivial operations. We punish these
trivial operations because they often produce constant outputs without ever adding meaning to an
expression.

14

To detect trivial operations, we employ two procedures. The ﬁrst uses the SymPy package [56]
to simplify f . If the simpliﬁed expression is different from the original expression, then there
are trivial operations in f and this procedure returns 1. Otherwise the ﬁrst procedure returns 0.
Unfortunately, the SymPy == function to test if functions are equal often incorrectly indicates that
nontrivial functions are trivial. For example, SymPy’s simplify function, which we use to test if a
function can be simpliﬁed, converts x + x to 2 · x, and the == function states that x + x (cid:54)= 2 · x. To
combat this, we develop a new function, sympyEquals which corrects for these issues with ==. The
sympyEquals is equivalent to ==, except that it does not take the order of terms into account, and it
does not mark expressions such as x + x and x · x as unsimpliﬁed. We ﬁnd that this greatly improves
function ﬁtting.

The constant ﬁtting procedure often produces functions which only differ from a trivial operation
because of imperfect constant ﬁtting, such as f (x0) = x0.0001
, which is likely meant to represent x0
0.
SymPy, however, will not mark this function as trivial. The second procedure addresses this issue by
counting the constant activations, such as x0.0001
, 1.001 · x0, and x0 + 0.001, which approximate
trivial operations. For the activation function f (x) = x + c, if the ﬁtted c satisﬁes −0.1 < c < 0.1,
the procedure adds 1 to its counter. Similarly, for the activation functions f (x) = cx and f (x) = xc,
if the ﬁtted c satisﬁes −0.1 < c < 0.1 or 0.9 < c < 1.1, the procedure adds 1 to its counter. We
select these ranges to capture instances of imperfect constant ﬁtting without labeling legitimate
solutions as trivial. After checking all activation functions used, the procedure returns the counter.

0

0

The φ[f ] term returns the sum of the outputs of the ﬁrst and second procedures. We ﬁnd that a weight
of wφ ≈ 0.7 for φ[f ] is most effective in our loss function. This value of wφ ensures that most trivial
f have Ki(M, f ) − s · wφ · φ[f ] < 0, thus actively reducing the weights corresponding to functions
with trivial operations, without overpunishing functions and hindering learning.

B The Psi Term

When punishing trivial operations using the φ term, we ﬁnd that the network discovers many nontrivial
operations which very closely approximate trivial functions by exploiting portions of functions with
near-zero derivatives, which can be used to artiﬁcially compress data. For example, cos(x/2) closely
approximates 1 if −1 < x < 1. Unfortunately, it is often difﬁcult to determine if a function
approximates a trivial function simply from its symbolic representation. This issue is also identiﬁed
in [55].

To detect these trivial function approximations, we develop an approach which analyzes the activation
functions’ outputs during the forward pass. The ψ[f ] term counts the number and severity of
activation functions which, during a forward pass, the network identiﬁes as possibly approximating
trivial solutions. For each basis function, the network stores values around which outputs of that
function often cluster artiﬁcially. Table 2 lists the bases which the network tests for clustering.

The procedure for determining ψ is as follows. The algorithm begins with a counter of 0. During the
forward pass, the if the network reaches a basis function φ listed in Table 2, the algorithm tests each
ordered tuple (φ, a, δ) from Table 2, where a is the point tested for clustering and δ is the clustering
tolerance. If the mean of all the outputs of the basis function, ¯y, for a given batch satisﬁes |¯y − a| < δ,
the algorithm adds min(5, 0.1/ |¯y − a|) to the counter. These expressions increase with the severity
of clustered data; the more closely the outputs are clustered, the higher the punishment term. The
minimum term ensures that ψ[f ] is never inﬁnite.

We also test for the approximation sin(x) ≈ x, by testing the inputs and outputs of the sine basis
function. If the inputs and outputs x and y to the sine basis satisfy |y − x| < 0.1, the algorithm adds
min(5, 0.05/|y − x|) to the counter. In the future, we plan to consider more approximations similar
to the small angle approximation.

ψ[f ] should not artiﬁcially punish functions involving the bases listed in Table 2 that are not trivial
approximations, because no proper use of these basis functions will always produce outputs very
close to the clustering points. Because ψ[f ] ﬂags functions based on their batch outputs, each batch
will likely give different outcomes. This allows ψ[f ] to better discriminate between trivial function
approximations and nontrivial operations: ψ[f ] should ﬂag trivial function approximations often,
but it should only ﬂag nontrivial operations rarely, when the inputs statistically ﬂuctuate to produce
clustered outputs. In practice, we ﬁnd that a weight of wψ ≈ 0.3 for ψ[f ] is most effective in our loss
function.

15

Table 2: Basis functions tested for clustering

Basis Functions Cluster Points Cluster Tolerance
(·)2
(·)3
sin(·)
cos(·)
(·)c

{0}
{0}
{1, −1}
{1, −1}
{1}

0.25
0.25
0.25
0.25
0.5

C The Xi Term

When our network converges to the correct solution, it may converge to a more complicated expression
equivalent to the desired expression. To promote simpler expressions, we slightly punish functions
based on their complexity. The ξ[f ] term counts the number of activation functions used to produce
f, which serves as a measure of f ’s complexity. We ﬁnd that a small weight of wξ ≈ 0.1 for ξ[f ]
is most effective in our loss function. This small value has little signiﬁcance when distinguishing
between a function which ﬁts a dataset well and a function which does not, but it is enough to promote
simpler functions over complex functions when they are otherwise equivalent.

D The Gamma Term

The γ[f ] term also punishes functions for their complexity. The γ[f ] term counts the number of
constants in f , which, like the number of activation functions, serves as a metric for f ’s complexity.
We ﬁnd that a weight of wγ ≈ 0.15 for γ[f ] is most effective in our loss function. Just as with
ξ[f ], this small value has little signiﬁcance when distinguishing between a function which ﬁts a
dataset well and a function which does not, but it is enough to slightly promote simpler functions
over complex functions when they are otherwise equivalent. We weight γ[f ] slightly higher than ξ[f ]
because many functions with constants can be simpliﬁed.

F Functions with Undeﬁned Outputs

One difﬁculty that may arise when training OccamNet is that many sampled functions are undeﬁned
on the input data range. Two cases of undeﬁned functions are: 1) the function is undeﬁned on
part of the input data range for all values of a set of constants or 2) the function is only undeﬁned
when the function’s constants take on certain values. An example function satisfying case 1 is
f1(x0) = c0/(x0 − x0), which divides by 0 regardless of the value of c0. An example function
satisfying case 2 is f2(x0) = xc0
0 , which is undeﬁned whenever x0 is negative and c0 is not an integer.
In the ﬁrst case, the network should abandon the function. In the second case, the network should try
other values for the constants. However, the network cannot easily determine which case an undeﬁned
function satisﬁes. To balance both cases, if the network obtains an undeﬁned result, such as NaN or
inf, for the forward pass, the network tests up to 100 more randomized sets of constants. If none
of these attempts produce deﬁned results, the network returns the array of undeﬁned outputs. For
example, with c0/(x0 − x0), the network tests a ﬁrst set of constants, determines that they produce
an undeﬁned output, and tests 100 more constants. None of these functions are deﬁned on all inputs,
so the network returns the undeﬁned outputs.
In contrast, with xc0
0 , the network might ﬁnd that the ﬁrst set of constants produces undeﬁned outputs,
but after 20 retries, the network might discover that c0 = 2 produces a function deﬁned on all inputs.
The network will then perform gradient descent, and return the ﬁtted value of c0. Further, if at
any point in the gradient descent, the forward pass yields undeﬁned results, the network returns
the well-deﬁned constants and associated output from the previous forward pass. For example, for
xc0
0 , after the network discovers that c0 = 2 works, the gradient for the constants will be undeﬁned
because c0 can only be an integer. Thus, the network will return the outputs of xc0
0 , for c0 = 2, before
the undeﬁned gradients.

We ﬁnd that if the network simply ignores functions with undeﬁned outputs, these functions will
increase in probability, because our network regularization punishes many other functions. Since
these punished functions decrease in probability during training, the functions with undeﬁned outputs

16

begin to increase in probability. To combat this, instead of ignoring undeﬁned functions, we use
a modiﬁed ﬁtness for undeﬁned functions, K (cid:48)
2πσ2 is the
maximum possible value of Ki(M, f ) and ws is a hyperparameter than can be tuned. This punishes
undeﬁned functions, causing their weights to decrease. In practice, we ﬁnd that a value of ws between
0 and 1 is most effective, depending on the application.

i(M, f ) = −wss, where s = n(M)/

√

G Experimental Hyperparameters

In Tables 3 and 4, we present and detail the hyperparameters we used for our experiments in the main
paper. We present our experimental setup for our experiments with PMLB Datasets separately, in
Section H. Note that detail about the setup for each experiment is provided in the attached code.

In Tables 3 and 4, + is addition (2 arguments); − is subtraction (2 arguments) · is multiplication (2
arguments); / is division (2 arguments); sin(·) is sine, +c is addition of a constant, ·c is multiplication
of a constant, (·)c is raising to the power of a constant, ≤ is a an if-statement (4 arguments: comparing
two numbers, one return for a true statement, and one for a false statement); −(·) is negation. MIN,
MAX and XOR all have 2 arguments. Here, SIGMOID(cid:48) is a sigmoid layer and tanh(cid:48) is a tanh layer
where the inputs to both functions are scaled by a factor of 10, +4 and +9 are the operations of adding
4 and 9 numbers respectively, and MAX4, MIN4, MAX9 and MIN9 are deﬁned likewise. The bases
for pattern recognition experiments are given as follows: ΦA consists of SIGMOID(cid:48), SIGMOID(cid:48),
tanh(cid:48), tanh(cid:48), +4, +4, +9, +9, +, +, MIN, MIN, MAX and MAX; ΦB consists of id, id, id, id, +,
+, +, +4, +4, +9, +9, +9, tanh,, tanh, SIGMOID, and SIGMOID. Additionally, the constants used
for pattern recognition are C = {−1, −1, 0, 0, 1, 1, 1}.

In Tables 3 and 4, L is the depth, T is the temperature, Tlast is the temperature of the ﬁnal layer,
σ is the variance, R is the sample size, λ is the fraction of best ﬁts, α is the learning rate, E is the
initialization parameter described in Section B, and wφ, wψ, wξ, and wγ are as deﬁned in Appendix
E. Table 3 does not include E as a listed hyperparameter, because for all experiments listed E = 0.
With ∗ we denote the experiments for which the best model is without skip connections. We do not
regularize for any experiments in Table 3. NA entries mean that the correspodning hyperparameter is
not present in the experiment.

For all experiments in Table 4, we use a learning rate of 0.01, and, when applicable, a constant-
learning rate of 0.05. We also set the temperature to 1 and the ﬁnal layer temperature to 10 for all
experiments in the table. For the equation m1v1 − m2v2 = 0, we sample m1, v1, and m2 from
[−10, 10] and compute v2 using the implicit function.

All our experiments in Table 3 use a batch size of 1000, except for Backprop OccamNet and Finetune
ResNet, for which we use batch size 128. All our experiments in Table 4 use a batch size of 200.
For each of our pattern recognition experiments we use a 90%/10% train/test random split for the
corresponding datasets. The input pixels are normalized to be in the range [0, 1]. During validation:
for MNIST Binary, MNIST Trinary and ImageNet Binary the outputs of OccamNet are thresholded at
0.5 and if the output matches the one-hot label, then the prediction is accurate and it is inaccurate
otherwise; for Backprop OccamNet and Finetune ResNet the outputs of OccamNet are viewed as
the logits of a negative log likelihood loss function, so the prediction is the argmax of the logits.
Backprop OccamNet and Finetune ResNet use an exponential decay of the learning rate with decay
factor 0.999.

H PMLB Experiment Setup and Results

As described in the main text, we test OccamNet on 15 datasets from the Penn Machine Learning
Benchmarks (PMLB) repository [50]. The 15 datasets chosen, and the corresponding numbers we use
to reference them, are shown in Table 5. We chose these datasets by selecting the ﬁrst 15 regression
datasets with fewer than 1667 datapoints. These 15 datasets are the only datasets from PMLB we
examine.

We test four methods on these datasets. OccamNet, V100, Eplex, AIF, and Extreme Gradient Boosting
(XGB) [57]. We have described all of these methods except for XGB in the main text. XGB is
a tree based method which was identiﬁed by [53] as the best machine learning method based on
validation MSE for modeling the PMLB datasets. However, XGB is not interpretable and thus

17

Table 3: Hyperparameters for Experiments Where E = 0

Target

Bases

Constants

Range

L/ T / Tlast/ σ

R/ λ/ α

2x2 + 3x
sin(3x + 2)
(cid:80)3
n=1 sin(nx)
(x2 + x)/(x + 2)
0(x0 + 1)/x5
x2
1
0/2 + (x1 + 1)2/2
x2

3x if x > 0, else x
x2 if x > 0, else −x
x if x > 0, else sin(x)

SORT(x0, x1, x2)

4LFSR(x0, x1, x2, x3)

Analytic Functions

(cid:104)·, ·, +, +(cid:105)
(cid:104)·, sin, sin, +, +(cid:105)
(cid:104)sin, sin, +, +, +(cid:105)
(cid:104)·, ·, +, +, /, /(cid:105)
(cid:104)·, ·, +, +, /, /(cid:105)
(cid:104)·, ·, +, +, /(cid:105)

∅
1, 2
1, 2
1
1
1, 2

[−10, 10]
[−10, 10]
[−20, 20]
[−6, 6]
[[−10, 10], [0.1, 3]]
[[−20, −2], [2, 20]]

Program Functions

(cid:104)≤, ≤, ·, +, +, /(cid:105)
(cid:104)≤, ≤, −(·), +, +, −, ·(cid:105)
(cid:104)≤, ≤, +, +, sin, sin(cid:105)
(cid:104)≤, +, MIN, MAX,
MAX/, ·, −(cid:105)
(cid:104)+, +, XOR, XOR(cid:105)

[−20, 20]
[−20, 20]
[−20, 20]
[−50, 50]4
{0, 1}4

2/1/1/0.01
3/1/1/0.001
5/1/1/0.001
2/1/2/0.0001
4/1/3/0.0001
3/1/2/0.1

2/1/1.5/0.1
2/1/1.5/0.1
3/1/1.5/0.01

3/1/4/0.01

50/5/0.05
50/5/0.005
50/5/0.005
100/5/0.005
100/10/0.002
150/5/0.005

100/5/0.005
100/5/0.005
100/5/0.005

100/5/0.004

2/1/1/0.1

100/5/0.005

1
1
1

1, 2

∅

1, 2

y0((cid:126)x) = x1 if x0 < 2, else −x1
y1((cid:126)x) = x0 if x1 < 0, else x2
1

(cid:104)≤, ≤, −(·), ·(cid:105)

[−5, 5]2

3/1/3/0.01

100/5/0.002

g(x) = x2 if x < 2, else x/2
y(x) = g◦4(x)

(cid:104)≤, ≤, +, ·, ·, /, /(cid:105)

1, 2

[−8, 8]

2/1/2/0.01

100/5/0.005

g(x) = x + 2 if x < 2, else x − 1
y(x) = g◦2(x)

(cid:104)≤, ≤, +, +,
+, −, −(cid:105)

1, 2

[−3, 6]

2/1/1.5/0.01

100/5/0.005

MNIST Binary
MNIST Trinary
ImageNet Binary∗
Backprop OccamNet∗
Finetune ResNet∗

Target

10.5x3.1

cos(x)
ex

Pattern Recognition

ΦA
ΦA
ΦA
ΦB
ΦB

C
C
C
C
C

[0, 1]784
[0, 1]784
[0, 1]2048
[0, 1]2048
[0, 1]3×224×224

2/1/10/0.01
2/1/10/0.01
4/1/10/10
4/1/10/NA
4/1/10/NA

150/ 10/0.05
150/ 10/0.05
150/10/0.0005
NA/NA/0.1
NA/NA/0.1

Table 4: Hyperparameters for Experiments Where E = 1
λ

Constants

Range

Bases

R

L

σ

wφ/wψ /wξ/wγ

(cid:104)+, −, ·, /, sin,
cos, +c, ·c, (·)c(cid:105)
(cid:104)+, /, sin(cid:105)
(cid:104)+, ·c, (·)c(cid:105)

Analytic Functions

∅

2, π
10

[0, 1]

[−100, 100]
[0, 1]

Implicit Functions

x0x1 = 1
x0/x1 = 1
0 + x2
x2
x0/ cos(x1) = 1
m1v1 − m2v2 = 0

1 = 1

(cid:104)+, −, ·, /, sin, cos(cid:105)
(cid:104)+, −, ·, /, sin, cos(cid:105)
(cid:104)+, −, ·, /, sin, cos(cid:105)
(cid:104)+, −, ·, /, sin, cos(cid:105)
(cid:104)+, −, ·, /, sin, cos(cid:105)

∅
∅
∅
∅
∅

[−1, 1]
[−1, 1]
[−1, 1]
[−1, 1]
[−10, 10]3

2

3
3

2
2
2
2
2

0.0005

0.01
0.05

0.01
0.01
0.01
0.01
0.01

200

400
200

400
400
200
200
200

10

50
1

1
1
10
10
10

0/0/0/0

0/0/0/0
0.7/0.3/0.05/0.03

0.7/0.3/0.15/0.1
0.7/0.3/0.15/0.1
0.7/0.3/0.15/0.1
0.7/0.3/0.15/0.1
0.7/0.3/0.15/0.1

cannot be used as a one to one comparison with OccamNet. Hence, although we provide the raw
data for XGB’s performance, we do not analyze it further. We train all methods except “V100”
on a single core of an Intel Xeon E5-2603 v4 @ 1.70GHz. For all methods, we use the basis set
Φ = (cid:104)+(·, ·), −(·, ·), ×(·, ·), ÷(·, ·), sin(·), cos(·), exp(·), log | · |(cid:105).

For each dataset, we perform grid search to identify the best hyperparameters. The hyperparameters
searched for the two OccamNet runs are shown in Table 6. The other hyperparameters not used in the
grid search are set as follows: T = 10, Tlast = 10, wφ = wψ = wξ = wγ = 0, and the dataset batch
size is the size of the training data. For OccamNet V100, we set R to be approximately as large as can
ﬁt on the V100 GPU, which varies between datasets. See Table 7 for the exact number of functions
tested for each dataset for OccamNet V100. For XGBoost, we use exactly the same hyperparameter
grid as used in Orzechowski et al. [53]. For Eplex, we use the same hyperparameter grid as used in
Orzechowski et al. [53], with the exception that we use a depth of 4 to match that of OccamNet.

We select the best run from the grid search as follows. For each hyperparameter combination, we ﬁrst
identify the models with the lowest training MSE and the lowest validation MSE:

18

Table 5: Datasets Tested

Dataset

Size

# Features

1027_ESL
1028_SWD
1029_LEV
1030_ERA
1089_USCrime
1096_FacultySalaries
192_vineyard
195_auto_price
207_autoPrice
210_cloud
228_elusage
229_pwLinear
230_machine_cpu
4544_GeographicalOriginalofMusic
485_analcatdata_vehicle

488
1000
1000
1000
47
50
52
159
159
108
55
200
209
1059
48

4
10
4
4
13
4
2
15
15
5
2
10
6
117
4

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Table 6: OccamNet Hyperparameters

Hyperparameter

OccamNet

OccamNet V100

α
σ
E
λ/R
R
N

{0.5, 1}
{0.5, 1}
{1, 5}
{0.1, 0.5, 0.9}
{500, 1000, 2000}
1000000/R

{0.5, 1}
{0.1, 0.5, 1}
{0, 1, 5}
{0.1, 0.5, 0.9}
max
1000

• For OccamNet, we examine the highest probability function after each epoch. From these
functions, we select the function with the lowest testing MSE and the function with the
lowest validation MSE.

• For Eplex, we examine the highest ﬁtness individual from each generation. From these
individuals, we select the individual with the lowest testing MSE and the individual with the
lowest validation MSE.

• For XGBoost, we train the model until the validation loss has not decreased for 100 epochs.
We then return this model as the model with the best training MSE and validation MSE.

Once we have the models with the lowest training and validation MSE for each hyperparameter
combination, we identify the overall model with the lowest training MSE from the set of lowest

Table 7: Number of Functions Sampled

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

R

17123
8333
8333
8333
178571
166666
161290
52631
52631
78125
151515
41666
40000
7874
178571

19

Table 8: Raw data from the PMLB experiments. Hyperparameters and best ﬁts are in the following
path in our code (see Section M): pmbl-experiments/pmlb-results.

Training Loss (MSE)

Validation Loss (MSE)

OccamNet V100

Eplex

AIF

XGB OccamNet V100

Eplex

AIF

0.177
0.607
0.486
0.639
0.107
0.070
0.228
0.155
0.168
0.154
0.136
0.255
0.062
0.573
0.208

0.139
0.605
0.432
0.616
0.054
0.035
0.161
0.145
0.141
0.101
0.129
0.167
0.042
0.438
0.183

0.153
0.643
0.443
0.616
0.105
0.067
0.230
0.152
0.152
0.130
0.141
0.324
0.082
0.414
0.216

0.465

0.886

0.162
0.713

0.171
0.177

0.103

0.456

0.056
0.443
0.326
0.547
0.000
0.000
0.039
0.000
0.000
0.000
0.029
0.000
0.004
0.000
0.000

0.141
0.647
0.634
0.662
0.145
0.037
0.047
0.095
0.114
0.027
0.119
0.193
0.074
0.470
0.174

0.137
0.640
0.597
0.641
0.108
0.017
0.099
0.115
0.097
0.021
0.162
0.177
0.076
0.312
0.411

0.128
0.702
0.581
0.641
0.182
0.036
0.122
0.097
0.129
0.036
0.161
0.310
0.198
0.320
0.567

0.449

1.040

0.066
0.802

0.044
0.178

0.289

0.524

XGB

0.133
0.567
0.556
0.649
0.134
0.114
0.175
0.105
0.105
0.162
0.106
0.083
0.163
0.196
0.175

Testing Loss (MSE)

Average Run Time (s)

OccamNet V100

Eplex

AIF

XGB OccamNet V100

Eplex

AIF

XGB

0.251
0.704
0.542
0.713
0.589
0.151
0.335
0.137
0.135
0.084
0.180
0.223
1.088
0.570
1.664

0.141
0.706
0.491
0.679
0.209
0.082
0.761
0.132
0.194
0.086
0.177
0.214
0.157
0.440
0.334

0.223
0.742
0.474
0.679
0.116
0.091
0.829
0.119
0.150
0.097
0.275
0.426
0.487
0.442
0.441

0.741

0.895

0.088
0.698

0.109
0.274

0.864

0.324

0.147
0.648
0.464
0.659
0.113
0.234
0.316
0.094
0.094
0.080
0.150
0.165
0.622
0.228
0.188

2246
4789
4767
4511
239
244
249
751
764
483
259
944
956
6562
264

319
302
300
308
504
479
468
359
354
373
455
335
364
354
487

4574
4651
4696
4729
4684
4755
4583
4516
4517
4605
4639
4653
4561
4785
4533

4498

4222

5076
1222

6196
1223

6899

586

5
7
6
5
3
3
1
5
5
2
2
3
5
139
29

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

training MSE models and we identify the overall model with the lowest validation MSE from the set
of lowest validation MSE models. We then record these models’ training MSE and validation MSE
as the best training MSE and validation MSE, respectively. Finally, we test the model with the overall
lowest validation MSE on the testing dataset, and record the result as the grid search testing MSE.

The raw data from this experiment is shown in Table 8. To improve readability, we use red highlighting
and bold text to illustrate the best performing model for each dataset and metric. We compare
OccamNet, Eplex, and AIF, marking the method with the lowest MSE or training time in red. We also
compare V100, Eplex, and AIF, marking the method with the lowest MSE or training time in bold.

As discussed in the main text, OccamNet is considerably faster than Eplex, often running faster
by more than an order of magnitude. This may be in part because we train Eplex with the DEAP
evolutionary computation framework [51], which is implemented in Python and utilizes NumPy
arrays for computation. Thus, our implementation of Eplex may be somewhat slower than an
implementation written in C. However, because of its selection based on many ﬁtness cases, Eplex
is also by nature considerably slower than many other genetic algorithms, running in O(T N 2),
where T is the number of ﬁtness cases and N is the population size [58]. This suggests that even
a C implementation of Eplex may not be as fast as OccamNet. More recent selection algorithms
perform comparably to Eplex but run signiﬁcantly faster, for example Batch Tournament Selection
[58]. However, because these methods did not exist at the time of Orzechowski et al. [53], they have
not been compared to other methods on the PMLB datasets. Thus, we have not tested these methods
here. On the other hand, our current implementation of sampling and the forward pass work with

20

DAGs in which an edge leads to each argument node, regardless of whether the argument node is
connected to the outputs. The result is that our implementation of OccamNet evaluates more than |Φ|
times more basis functions than is necessary, where |Φ| is the number of basis functions. In the case
of these experiments, this amounts to more than 8 times the number of calculations necessary. In
the future, we plan to implement OccamNet by constructing DAGs starting from the output nodes,
thereby greatly increasing speed.

I Analysis of Fits to PMLB Datasets

In this section, we analyze the ﬁts that the methods discussed in Section H provide for the PMLB
dataset.

OccamNet’s solutions are all short, easy to comprehend ﬁts to the data. We ﬁnd that OccamNet uses
addition, subtraction, multiplication, and division most extensively, exploiting sin(·) and cos(·) for
more nonlinearity. Interestingly, OccamNet uses exp(·) and log | · | less frequently, perhaps because
both functions can vary widely with small changes in input, making functions with these bases more
likely to represent poor ﬁts.

OccamNet’s solutions demonstrate its ability to exploit modularity and reuse components. These
solutions often have repeated components, for example in dataset #1, 1027_ESL, where the best ﬁt to
the training data is

y0 =

(sin(x2) + x3 + x1) · (sin(x2) + x3 + x1)
(sin(x2) + x3 + x1) + (x3 + x1) + (x1 + x3)

.

In this ﬁt, OccamNet builds sin(x2) + x3 + x1 in the ﬁrst two layers of the network and then
reuses it three times. Solutions like the above demonstrate OccamNet’s ability to identify successful
subcomponents of a solution and then to rearrange the subcomponents into a more useful form.
Examples like the above, however, also demonstrate that OccamNet often overuses modularity,
potentially restricting the domain of functions it can search. We suspect that the main reason that
OccamNet may rely too heavily on modularity in some ﬁts is that OccamNet uses an extremely
high learning rate of 1 for its training. We used such a large learning rate to allow OccamNet to
converge even when faced with 1030 or more functions. However, we suspect that this may also
cause OccamNet to converge to certain paths before exploring sufﬁciently. For example, with the
function above, OccamNet may have identiﬁed that sin(x2) + x3 + x1 is a useful component and,
because of its high learning rate, used this pattern in several times instead of the one time needed.
This hypothesis is supported by the fact that OccamNet V100, which samples many more functions
before taking a training step, repeats patterns less frequently than OccamNet. For example OccamNet
V100’s best ﬁt solution for the training dataset of dataset #1 is

y0 = cos(x1/x1) · cos(x1/x1) · (x2 + x3 + sin(x0) + x3 + x1 − sin(x3)),

which contains almost no repetition.

Remarkably, for dataset #4, 1030_ERA, both Eplex and OccamNet V100 discover equivalent func-
tions for both training and validation: OccamNet V100 discovers

y0 = cos(sin(x1 − x2)) · (sin(x2) + x0 + x1) · cos(x2/x2),

and Eplex discovers

y0 = cos(x1/x1) · (sin(x2) + x0 + x1) · cos(sin(x2 − x1)).
As a result, the two methods’ losses are identical up to 7 decimal places. Still, we mark Eplex as
performing better on this dataset because after the 7th decimal place it has a slightly lower loss, likely
due to differences in rounding or precision between the two approaches. Two different methods
identifying the same function is extremely unlikely; OccamNet’s search space includes 2 · 1030 paths
for this dataset, meaning that the probability of both methods identifying this function purely by
chance is miniscule. This, in combination with the fact that this function was the best ﬁt to both the
training and validation datasets for both methods, suggests that the identiﬁed function is a nearly
optimal ﬁt to the data for the given search space. Given the size of the search space, this result
thus provides further evidence that OccamNet and Eplex perform far better than brute force search.
Interestingly, although OccamNet did not discover this function, it’s best ﬁt for the validation,

y0 = sin(x2/x2) · (sin(x2) + x0 + x1) · cos(cos(x3)) · cos(sin(x3)),

21

does include several features present in the ﬁts found by V100 and Eplex, such as the the sin(x2) +
x0 + x1 term, the cos(sin(·)) term, and the x2/x2 inside of the trigonometric function. This suggests
that OccamNet may also have been close to converging to the function discovered by Eplex and
OccamNet V100. OccamNet’s loss was also always within 5% of Eplex’s loss on this dataset, again
suggesting that OccamNet had identiﬁed a function close to that of Eplex and V100.

Interestingly, AI Feynman 2.0’s ﬁts generally tend to be very simple compared to those of OccamNet.
For example, AIF’s ﬁt for the training dataset #11 is

y0 = −0.050638447726 + log(x0/ sin(x0)) − x0,

whereas OccamNet’s ﬁt is

y0 = sin(x0) · x1 · x0 · sin(x0) · log |x0| − cos(x1 · x0 − x1).
AI Feynman’s ﬁt is slightly simpler and easier to interpret, but it comes at the cost of having a 35%
higher loss. We suspect that because the PMLB datasets likely do not have modular representations,
AI Feynman must rely mainly on its brute force search, which ultimately produces shorter expressions.
AI Feynman can also produce constants because of its polynomial ﬁts, and it uses constants in nearly
every solution it proposes. We did not allow the other symbolic methods to ﬁt constants, but they still
consistently performed better than AI Feynman, suggesting that ﬁtting constants may not be essential
to accurately modeling the PMLB datasets.

J Ablation Studies

We test the performance of each hyperparameter in a collection of ablation studies, as shown in Table
9. Here, we focus on what our experiments demonstrate to be the most critical parameters to be tuned:
the collection of bases and constants, the network depth, the variance of our interpolating function,
the overall network temperature (as well as the last layer temperature), and, ﬁnally, the learning rate
of our optimizer. As before, we set the stop criterion and terminate learning when the top-λ sampled
functions all return the same ﬁtness K(·, f ) for 30 consecutive epochs. If this does not occur in a
predeﬁned, ﬁxed number of iterations, or if the network training terminates and the ﬁnal expression
does not match the correct function we aim to ﬁt, we say that the network has not converged. All
hyperparameters for baselines are speciﬁed in Section G, except for the sampling size, which is set to
R = 100.

Our benchmarks use a sampling size large enough for convergence in most experiments. It is worth
noting, however, that deeper networks failed to always converge (with convergence fraction of
η = 8/10) for the analytic function we tested. Deeper networks allow for more function composition
and let approximations emerge as local minima: in practice, we ﬁnd that increasing the last layer
temperature or reducing the variance is often needed for allowing for a larger depth L. For pattern
recognition, we found that MNIST Binary and Trinary require depth 2 for successful convergence,
while the rest of the experiments require depth 4. Shallower or deeper networks either yield subpar
accuracy or fail to converge. We also ﬁnd that for OccamNet without skip connections, larger learning
rates usually work best, i.e. 0.05 works best, while OccamNet with skip connections requires a
smaller learning rate, usually around 0.0005. We also tested different temperature and variance
schedulers, in the spirit of simulated annealing. In particular, we tested increasing or decreasing these
parameters over training epochs, as well as sinusoidally varying them with different frequencies.
Despite the increased convergence time, however, we did not ﬁnd any additional beneﬁts of using
schedulers. As we test OccamNet in larger problems spaces, we will revisit these early scheduling
studies and investigate their effects in those domains.

K Neural Approaches to Benchmarks

Since our OccamNet is a neural model that is constructed on top of a fully connected neural
architecture, below we ﬁrst consider a limitation of the standard fully connected architectures for
sorting, and then a simple application of our temperature-controlled connectivity.

A Exploring the limits of fully connected neural architectures for sorting

We made a fully connected neural network with residual connections. We used the mean squared
error (MSE) as the loss function. The output size was equal to the input size and represented the

22

Table 9: Ablation studies on representative experiments

Modiﬁcation

Convergence fraction η Convergence epochs Tc

Experiment sin(3x + 2)

baseline
added constants (2) and bases (·, (·)2, −(·))
lower last layer temperature (0.5)
higher last layer temperature (3)
lower learning rate (0.001)
higher learning rate (0.01)
deeper network (6)
lower variance (0.0001)
higher variance (0.1)
lower sampling (50)
higher sampling (250)

10/10
10/10
10/10
10/10
10/10
10/10
8/10
10/10
10/10
10/10
10/10

Experiment x2 if x > 0, else −x

baseline
added constants (1, 2) and bases (−, −(·))
lower last layer temperature (0.5)
higher last layer temperature (3)
lower learning rate (0.001)
higher learning rate (0.01)
deeper network (6)
shallower network (2)
lower variance (0.001)
higher variance (1)
lower sampling (50)
higher sampling (250)

10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10
10/10

390
710
300
450
2500
170
3100
390
450
680
200

100
290
160
150
780
90
180
160
160
180
290
140

original numbers in a sorted order. We used L2 regularization along with Adam optimization. We
tested weight decay ranging from 1e-2 to 1e-6 in which 1e-5 provided the best training and testing
accuracy. Finally learning rate for the optimiser was found to be optimum around 1e-3. We used
30, 000 data points to train the model with batch size of 200. Each of the data point was a list of
numbers between 0 and 100. For a particular value of input size x (representing number of points
to be sorted), the number of hidden units was varied from 2 to 20 and the number of hidden layers
was varied from 2 (just a input and a output layer) to x! + 2. Then, the test loss was calculated on
20,000 points, chosen from same distribution. Finally, the combination (hidden_layer, hidden_unit )
for which the loss was less than 5 and (hidden_ layer * hidden_units) was min was noted in Table 10.
As seen from the table, the system failed to ﬁnd any optimal combination for any input size greater
than or equal to 5. For example, for input size 5, the hidden units were upper capped at 20 and hidden
layers at 120 and thus 2400 parameters were insufﬁcient to sort 5 numbers.

Table 10: Minimal conﬁgurations to sort list of length “input size.”

Input Size Hidden units Hidden Layers

Parameters

2
3
4
5

6
8
18
-

2
4
4
-

12
32
72
-

Generalization The model developed above generalizes poorly on data outside the training domain.
For example, consider the model with 18 hidden units and 4 hidden layers, which is successfully
trained to sort 4 numbers chosen from range 0 to 100. It was ﬁrst tested on numbers from 0 to 100
and then on 100 to 200. The error in the ﬁrst case was around 2 while the average error in the second
case was between 6 and 8 (which is (200/100)2 = 4 times the former loss). Finally when tested
on larger ranges such as (9900, 10000), the error exploded to around 0.1 million (which is an order

23

greater than (10000/100)2 = 10000 times the original loss). This gives a hint that the error might
be scaling proportionally to the square of test domain with respect to the train domain. A possible
explanation of this comes from the use of MSE as loss function. Scaling test data by ρ scales the
absolute error by approximately the same factor and then taking a square of the error to calculate the
MSE scales the total loss by square of that factor, i.e., ρ2.

B Applying temperature-controlled connectivity to standard neural networks for MNIST

classiﬁcation

We would like to demonstrate the promise of the temperature-controlled connectivity as a regu-
larization method for the classiﬁcation heads of models with a very simple experiment. We used
the ResNet50 model to train on the standard MNIST image classiﬁcation benchmark. We studied
two variants of the model: one is the standard ResNet model and the other is the same model, but
augmented with our temperature-controlled connectivity (with T = 1) between the ﬂattened layer
and the last fully connected layer (on the lines discussed in the main paper). Then we trained both
models with a learning rate ﬁxed at 0.05 and a batch size of 64 and ran it for 10 epochs. The model
with regularization performed slightly better than one without it. The regularized model achieved
the maximum accuracy of 99.18% while the same ﬁgure for the standard one was 98.43%. Besides,
another interesting observation that we made was about the stability of the results. The regularized
model produced much more stable and consistent results across iterations as compared to one without
it. These results encourage us to study the above regularization method in larger experiments.

L Symbolic Regression Benchmarks

A Eureqa

Eureqa is a software package for symbolic regression where one can specify different target expres-
sions, building block functions (analogous to the bases in OccamNet), and loss functions [28]. For
most functions, we use the absolute error as the optimization metric. We choose formula building
blocks in Eureqa to match the bases functions used in OccamNet.

For implicit functions, we use the implicit derivative error. We also order the data to improve the
performance. For the implicit functions in lines 1, 3, and 4 in Table 2 of the main text, the data is
ordered by x0. For the equation x2
1 = 1, the data is generated by sampling θ ∈ [0, 2π) and
calculating x0 = sin(x) and x1 = cos(x), and is ordered by θ. When the data is not ordered the value
of the implicit derivative error is much higher, resulting in the algorithm favoring incorrect equations.
For equation m1v1 − m2v2, the ordering is more ambiguous because of the higher dimensionality.
We tried ordering by both m1 and the product m1v1 without success.

0 + x2

B HeuristicLab

Due to limits on the number of data points and feature columns in Eureqa, we instead use HeuristicLab
for the image recognition tasks described in Section 5.4 of the main text. HeuristicLab is a software
package for optimization and evolutionary algorithms, and includes symbolic regression and symbolic
classiﬁcation. We use the Island Genetic Algorithm with default settings.

Similar to the building block functions in Eureqa, HeuristicLab can specify the basis symbols for
each task. However, HeuristicLab does not have the bases MAX, SIGMOID, or tanh. Instead we use
the symbols IfThenElse, GreaterThan, LessThan, And, Or, and Not.

C Eplex

As discussed in Section H, Eplex [52], short for Epsilon-Lexicase selection, is a genetic programming
population selection technique which we use as a symbolic regression benchmark in our experiments
with PMLB datasets. We implement a genetic algorithm using Eplex with the DEAP [51] evolutionary
framework, using Numpy arrays [59] for computation to increase speed.

Eplex selects individuals from a population by evaluating the individuals on subsets, or ﬁtness cases,
of the full data. For a given ﬁtness case, Eplex selects the top performing individuals, and then

24

proceeds to the next ﬁtness case. This process is repeated until only one individual remains. This
individual is then used as the parent for the next generation.

D AI Feynman 2.0

We also benchmark OccamNet against AI Feynman 2.0 [54]. AI Feynman 2.0 is a mixed approach
that combines brute force symbolic regression, polynomial ﬁts, and identiﬁcation for modularity in
the data using neural networks. To identify modularities in the data, AI Feynman ﬁrst trains a neural
network on it. This serves as an interpolating function for the true data, and allows the network to
search for symmetries and other forms of modularities.

Figure 5: In this ﬁgure we present two video frames for the target sin(3x + 2), which could be
accessed via videos/sin(3x + 2).mp4 in our code ﬁles. We show the beginning of the ﬁtting
(left) and the end, where OccamNet has almost converged (right).

Figure 6: In this ﬁgure we present two video frames for the target SORT(x0, x1, x2), which could be
accessed via videos/sorting.mp4 in our code ﬁles. We show the beginning of the ﬁtting (left) and
the end, where OccamNet has almost converged (right).

M Code

We will make our code public. We have grouped our code into ﬁve main folders.
analytic-and-programs stores our network and experiments for ﬁtting analytic functions and

25

programs. implicit stores our network and experiments for implicit functions, although it also
includes the three analytic functions listed in Table 4. constant-fitting stores code very similar
to implicit but which is optimized for constant ﬁtting. image-recognition stores our network
and experiments for image classiﬁcation. pmlb-experiment stores our code for our benchmarking
against the PMLB regression datasets. Finally, videos stores several videos of our model converging
to various functions. In Figures 5 and 6 we present snapshots of the videos.

References

[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,

2015.

[2] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.

The loss surfaces of multilayer networks. In AISTATS, 2015.

[3] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In ICLR,

2020.

[4] Riccardo Poli, William B Langdon, Nicholas F McPhee, and John R Koza. A ﬁeld guide to

genetic programming. lulu.com, 2008.

[5] P. J. Angeline, G. M. Saunders, and J. B. Pollack. An evolutionary algorithm that constructs

recurrent neural networks. IEEE Transactions on Neural Networks, 5(1):54–65, 1994.

[6] Dirk V. Arnold and Nikolaus Hansen. A (1+1)-CMA-ES for constrained optimisation. In

GECCO, 2012.

[7] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley,
and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for
training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567,
2017.

[8] Nikolaus Hansen. The CMA evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772,

2016.

[9] Ilya Loshchilov and Frank Hutter. CMA-ES for hyperparameter optimization of deep neural

networks. arXiv preprint arXiv:1604.07269, 2016.

[10] Martin Fowler. Domain Speciﬁc Languages. Addison-Wesley Professional, 1st edition, 2010.

[11] Georg Martius and Christoph Lampert. Extrapolation and learning equations. arXiv preprint

arXiv:1610.02995, 2016.

[12] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation

and control. In ICML, 2018.

[13] Samuel Kim, Peter Y. Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir ˇCeperi´c, and
Marin Soljaˇci´c. Integration of neural network-based symbolic regression in deep learning for
scientiﬁc discovery. IEEE Transactions on Neural Networks and Learning Systems, pages 1–12,
2020.

[14] Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks

through L0 regularization. In ICLR, 2018.

[15] Zongben Xu, Hai Hong Zhang, Yao Wang, Xiangyu Chang, and Yong Liang. L1/2 regularization.

Science China Information Sciences, 53:1159–1169, 2010.

[16] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv

preprint arXiv:1902.09574, 2019.

[17] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science,

220(4598):671–680, 1983.

26

[18] Dimitris Bertsimas, John Tsitsiklis, et al. Simulated annealing. Statistical science, 8(1):10–15,

1993.

[19] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous

relaxation of discrete random variables. In ICLR, 2017.

[20] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.

In ICLR, 2017.

[21] George Tucker, Andriy Mnih, Chris J. Maddison, and Jascha Sohl-Dickstein. REBAR: low-
variance, unbiased gradient estimates for discrete latent variable models. In NIPS, 2017.

[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected

convolutional networks. In CVPR, 2017.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, pages 770–778, 2016.

[24] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Mach. Learn., 8(3–4):229–256, May 1992.

[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,

2015.

[26] Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural

arithmetic logic units. In NIPS. 2018.

[27] Andreas Madsen and Alexander Rosenberg Johansen. Neural arithmetic units. In ICLR, 2020.

[28] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data.

Science, 324(5923):81–85, 2009.

[29] Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A physics-inspired method for

symbolic regression. Science Advances, 6(16), 2020.

[30] Robert I Mckay, Nguyen Xuan Hoai, Peter Alexander Whigham, Yin Shan, and Michael
O’neill. Grammar-based genetic programming: a survey. Genetic Programming and Evolvable
Machines, 11(3-4):365–396, 2010.

[31] Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago,
Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical
expressions from data via risk-seeking policy gradients. In ICLR, 2021.

[32] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer

graphics programs from hand-drawn images. In NIPS. 2018.

[33] Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, and Josh Tenen-
baum. Learning libraries of subroutines for neurally–guided bayesian program induction. In
NIPS. 2018.

[34] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama.

Write, execute, assess: Program synthesis with a REPL. In NeurIPS. 2019.

[35] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint

arXiv:1410.5401, 2014.

[36] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen. King, C. Summerﬁeld, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid
computing using a neural network with dynamic external memory. Nature, 538:471–476, 2016.

[37] Mark Collier and Joeran Beel. Implementing neural turing machines. In ICANN, page 94–104,

2018.

27

[38] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.

Deepcoder: Learning to write programs. In ICLR, 2016.

[39] Armando Solar Lezama. Program Synthesis By Sketching. PhD thesis, EECS Department,

University of California, Berkeley, 2008.

[40] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In

ICLR, 2016.

[41] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning

applied to document recognition. In IEEE, 1998.

[42] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.

ImageNet: A Large-Scale

Hierarchical Image Database. In CVPR, 2009.

[43] David J. Montana and Lawrence Davis. Training feedforward neural networks using genetic

algorithms. In IJCAI. Morgan Kaufmann Publishers Inc., 1989.

[44] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies

as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.

[45] Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. Efﬁcient

softmax approximation for GPUs. In ICML, 2017.

[46] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections

for efﬁcient neural networks. 2015.

[47] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for

efﬁcient convnets. In ICLR, 2017.

[48] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsiﬁes deep

neural networks. In ICML, 2017.

[49] Stefan Wagner, Gabriel Kronberger, Andreas Beham, Michael Kommenda, Andreas
Scheibenpﬂug, Erik Pitzer, Stefan Vonolfen, Monika Koﬂer, Stephan Winkler, Viktoria Dorfer,
and Michael Affenzeller. Advanced Methods and Applications in Computational Intelligence,
volume 6, chapter Architecture and Design of the HeuristicLab Optimization Environment,
pages 197–261. Springer, 2014.

[50] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H.
Moore. PMLB: a large benchmark suite for machine learning evaluation and comparison.
BioData Mining, 10(1):36, 2017.

[51] Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, and
Christian Gagné. DEAP: Evolutionary algorithms made easy. Journal of Machine Learning
Research, 13:2171–2175, 2012.

[52] William La Cava, Lee Spector, and Kourosh Danai. Epsilon-Lexicase Selection for Regression.

In GECCO, 2016.

[53] Patryk Orzechowski, William La Cava, and Jason H. Moore. Where are we now? A large

benchmark study of recent symbolic regression methods. In GECCO, 2018.

[54] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark.
AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. 2020.

[55] Michael Schmidt and Hod Lipson. Symbolic Regression of Implicit Equations, pages 73–85.

Springer US, 2010.

[56] Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondˇrej ˇCertík, Sergey B. Kirpichev,
Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rath-
nayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta,
Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štˇepán
Rouˇcka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony
Scopatz. SymPy: symbolic computing in python. Peer J Computer Science, 3:103, 2017.

28

[57] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. arXiv e-prints,

art. arXiv:1603.02754, March 2016.

[58] Vinicius V. Melo, Danilo Vasconcellos Vargas, and Wolfgang Banzhaf. Batch Tournament

Selection for Genetic Programming. In GECCO, 2019.

[59] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,
Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fer-
nández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler
Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array
programming with NumPy. Nature, 585(7825):357–362, 2020.

29

