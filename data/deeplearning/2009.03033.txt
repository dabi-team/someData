Centralized & Distributed Deep Reinforcement

Learning Methods for Downlink Sum-Rate

1

Optimization

Ahmad Ali Khan, Student Member, IEEE, Raviraj Adve, Fellow, IEEE,
Abstract

For a multi-cell, multi-user, cellular network downlink sum-rate maximization through power allocation is

a nonconvex and NP-hard optimization problem. In this paper, we present an effective approach to solving this

problem through single- and multi-agent actor-critic deep reinforcement learning (DRL). Speciﬁcally, we use ﬁnite-

horizon trust region optimization. Through extensive simulations, we show that we can simultaneously achieve

higher spectral efﬁciency than state-of-the-art optimization algorithms like weighted minimum mean-squared error

(WMMSE) and fractional programming (FP), while offering execution times more than two orders of magnitude

faster than these approaches. Additionally, the proposed trust region methods demonstrate superior performance

and convergence properties than the Advantage Actor-Critic (A2C) DRL algorithm. In contrast to prior approaches,

the proposed decentralized DRL approaches allow for distributed optimization with limited CSI and controllable

information exchange between BSs while offering competitive performance and reduced training times.

Deep reinforcement learning, optimization, sum-rate maximization

Index Terms

I. INTRODUCTION

Improving spectral efﬁciency is a central focus of modern research in wireless communication. As

device density grows and more consumer-centric, data-intensive, applications become popular, it is clear

that coordinated resource allocation is necessary to achieve this goal. Unfortunately, developing effective

techniques remains fundamentally challenging due to the nature of sum-rate maximization: not only is the

associated optimization problem for the broadcast channel non-convex, but, as proved by Luo and Zhang

in [1], it is also NP-hard. It follows that methods to ﬁnd the globally optimal solution to this problem, such

The authors are with the Department of Electrical and Computer Engineering, University of Toronto, Ontario, ON M5S 3G4, Canada.
E-mails: (akhan, rsadve)@ece.utoronto.ca. This work has been accepted for publication in IEEE Transactions on Wireless Communications.
The authors would like to acknowledge the support of TELUS Canada and the National Science and Engineering Research Council,

Canada through its Collaborative Research and Development (CRD) program.

0
2
0
2

p
e
S
4
1

]
T
I
.
s
c
[

2
v
3
3
0
3
0
.
9
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

as outer polyblock approximation, are computationally prohibitive and generally impractical for cellular

networks involving multiple cells and multiple users [2].

The solutions proposed in the literature balance a tradeoff between exchange of channel state information

(CSI), computational complexity and performance. On the one hand, heuristic methods such as random,

greedy and max-power allocation are extremely simple to implement and require no exchange of CSI

between cooperating base stations [3]. However, since they ignore the impact of inter-cell and intra-cell

interference they tend to have a correspondingly poor performance.

In contrast, centralized optimization algorithms like weighted minimum mean-squared error mini-

mization (WMMSE) and fractional programming (FP) offer much better performance since they take

interference into account. In particular, the work by Shi et al. in [4] shows the equivalence of weighted

sum-rate maximization and weighted mean-squared error minimization and develops an iterative algorithm

for ﬁnding a resource allocation strategy. Somewhat similarly, in [5], Shen and Yu develop the fractional

programming approach, based on minorization-maximization methods, to solve the downlink power al-

location problem iteratively. Both of these approaches are guaranteed to converge to a local optimum

of the original max sum-rate problem, and as such offer excellent performance. At the same time, this

performance comes with two undesirable compromises: ﬁrst, these algorithms need to optimize across all

cooperating BSs and thus have very high (albeit polynomial-time) computational complexity; and second,

they require extensive exchange of network CSI between cooperating BSs. In other words, they assume

that all the cooperating base stations are connected to a central cloud processer via high-speed backhaul

links that receives the complete downlink CSI from the BSs and computes and forwards their coordinated

power allocation based on this information exchange [6]. As a result, due to both the computational

complexity and CSI exchange requirements, these methods do not scale well for deployment in real-

world wireless networks, where ensuring this stringent level of cooperation and shared information across

a large number of cells is infeasible. To the best of our knowledge, FP is the state-of-the art among locally

optimal schemes [6].

To reduce the information exchange requirements, distributed optimization algorithms have been studied

extensively in the literature. One approach is to model the sum-rate maximization problem as a noncoop-

erative game, in which the BSs attempt to reach a socially acceptable power allocation strategy through

best-response dynamics without having access to the complete network CSI [7], [8]. The work in [7],

for example, analyzes the conditions under which a Nash equilibrium can be reached for a discretized

3

variant of the sum-rate maximization problem in which BSs have limited access to the network CSI. On

the other hand, as noted by the authors, Nash equilibria and Pareto optimal points are not necessarily

global or even local optima of the problem; thus, game-theoretic distributed optimization methods tend to

have noticeably worse performance than centralized methods like FP and WMMSE. This is particularly

true when BSs have access to only partial or imperfect CSI [8].

Distributed methods for wireless resource management have also been developed under the frameworks

of robust and stochastic optimization [9]–[11]. Similar to game-theoretic approaches, however, these

methods achieve performance inferior to centralized optimization approaches as the number of users

becomes larger [11]. It is worth emphasizing that distributed optimization implementations of high-

performance methods like FP and WMMSE are not possible: as observed in [6], convergence to a local

optimum of the sum-rate maximization problem is only guaranteed if the BSs have access to the full

network CSI.

Resource allocation schemes based on deep supervised learning have been recently explored as a means

to overcome some of these challenges [3], [12], [13]. In [3], Sun et al. utilized supervised regression to

enable a deep feedforward neural network to approximate the locally optimal power allocation solution

achieved by the WMMSE algorithm. Similarly, in [12], the authors utilize a deep supervised learning

algorithm to approximate the globally optimal power allocation strategy for sum-rate maximization in

the uplink of a massive MIMO network. In both of these cases, the nonlinear function approximation

property of neural networks was successfully leveraged to learn a mapping from the input CSI to the

desired solution of the problem.

While such an approach reduces execution time once the trained model is deployed, it also suffers

from an obvious drawback: the level of CSI exchange required is identical to the optimization algorithms

supervised learning aims to mimic. A further disadvantage is that the supervised learning algorithm in

[3] can at best match the performance of the WMMSE algorithm, not exceed it. For the approach used

in [12], a major concern is generating samples of the globally optimal solution for the algorithm to learn

from, since, as mentioned earlier, the original optimization problem is NP-hard. In [13], Mattheisen et

al. aim to mitigate this problem by using branch-and-bound to ﬁnd globally optimal solutions to the

(also NP-hard) energy efﬁciency maximization problem; however, the computational cost of generating

a sufﬁciently large number of samples to enable effective supervised learning for a deep neural network

remains extremely high.

4

Due to the issues associated with supervised learning methods, deep reinforcement learning (DRL) has

emerged as a viable alternative to solve wireless resource management problems [14]–[17]. Crucially,

DRL methods do not require prior solutions to the problem unlike supervised learning methods; instead,

the algorithms improve over time through a mechanism of feedback and interaction. For example, in

[14], the authors present a multi-agent deep Q-learning (DQL) approach to solve the problem of link

spectral efﬁciency maximization; the proposed approach provides performance that closely matches the

FP algorithm but requires discretization of the power control variables. In a similar fashion, the work in

[15] explores the use of DQL, REINFORCE, and deep deterministic policy gradient (DDPG) algorithms

for link sum-rate maximization. The DQL and REINFORCE algorithms presented once again require

discretization of the power allocation variables; this introduces uncertainty since there is no known method

for choosing the optimal discretization factor.

A major issue with the approaches presented in [14] and [15] is that direct optimization of the network

spectral efﬁciency is not possible; instead, as a proxy, each transmitter aims to maximize the difference

between its own spectral efﬁciency and the weighted leakage to other transmitters. This proxy objective

function is only proportional to the sum-rate when the number of transmitter-receiver pairs is very large.

Additionally, the approaches proposed require CSI exchange between the transmitters to function. The

learning algorithms presented also require signiﬁcant feature engineering; for example, in [15], in addition

to the current CSI, historical power allocation values are needed in order to output current power allocation

values; this adds additional requirements to the already burdensome information exchange problem.

Finally, we observe that the aforementioned deep learning methods do not allow for distributed op-

timization with limited CSI available at each BS. For example, the supervised learning method applied

in [3] to approximate the WMMSE algorithm can only be implemented in a centralized fashion which,

once again, requires complete CSI exchange between all BSs in the network. As the authors of [3]

demonstrate, supervised learning would be unable to succeed in approximating the WMMSE algorithm in

a decentralized environment, since there would not be a one-to-one mapping between the CSI input and

power allocation output at each BS. Similarly, the reinforcement learning approaches implemented in [14]

and [15] require full CSI exchange between BSs, in addition to prior channel state information (and per-cell

spectral efﬁciency) from previous time slots. Furthermore, to compute the power allocation for each link,

the transmitter requires a ﬁxed number of the strongest interference channels as input; this necessitates the

exchange of CSI between BSs even after the agents have been deployed and training is complete. These

5

processing and information exchange requirements necessarily prevent the implementation of distributed

optimization of the network spectral efﬁciency. Thus, we conclude that the current optimization approaches,

both learning-based and otherwise, compromise by either having prohibitively high computation and

information exchange requirements or poor performance. It is this gap in the literature that we aim to

address with our work.

In this paper, we present a class of DRL methods based on Markov Decision Processes (MDP), trust-

region policy optimization (TRPO) for downlink power allocation. In contrast to the deep Q-learning

method, trust region methods are capable of directly solving continuous-valued problems; this removes

the problem of decision spaces growing exponentially larger as the discretization of the optimization

variables is increased. Compared to the REINFORCE approach, trust region methods demonstrate greater

robustness since we can ensure that updates meet practically justiﬁable constraints; thus, they have been

shown to achieve far higher performance across a variety of continuous-control reinforcement learning

problems [18], [19]. Speciﬁcally, we utilize ﬁnite-horizon trust region methods to derive centralized and

decentralized algorithms for sum-rate optimization which require varying degrees of CSI exchange between

BSs.

The contributions of this paper can be summarized as follows:

• We demonstrate that the use of ﬁnite-horizon trust region policy optimization is effective in di-

rectly solving the sum-rate maximization problem in a multi-cell, multi-user environment. Unlike

previous deep RL-based attempts which necessitate discretization of the power variables and the

use of proxy objective functions, we solve the continuous-valued power allocation problem with the

network sum-rate taken directly as the reward function, thereby simplifying the algorithm design and

implementation.

• The approaches we present provide notably higher spectral efﬁciency than the state-of-the-art FP and

WMMSE algorithms across different network sizes.

• We show that agents trained using the proposed approach produce solutions to the sum-rate maximiza-

tion problem over two orders of magnitude faster than the aforementioned conventional optimization

algorithms. The proposed TRPO methods also demonstrate lower variance and higher performance

than the conventional Advantage Actor-Critic (A2C) algorithm.

• The multi-agent distributed approaches allow us to ﬂexibly control information exchange between

BSs, which is not possible in prior distributed optimization methods or using prior deep learning

6

approaches. In particular, the partially decentralized strategy we propose allows BSs to exchange

only power allocation information to effectively improve network sum-rate while utilizing only local

CSI at each BS.

• The trained DRL methods demonstrate robust performance to changes in the reward function (i.e.,

the network sum-rate) across a range of BS transmit powers.

This paper is organized as follows: in Section II, we present the system model and formulate the

network sum-rate maximization problem. In Section III, we describe the Markov Decision Process (MDP)

framework, and derive the proposed centralized and decentralized DRL algorithms. This is followed

by training, performance, and execution time results and comparisons in Section IV. We draw some

conclusions in Section V.

II. SYSTEM MODEL AND PROBLEM FORMULATION

Our system model is similar to that used in [5]. We consider a time-division duplexed network

comprising of B single-antenna base stations and K single-antenna users per cell uniformly distributed

over the cell area. The combined downlink channel gain from BS b to user k associated with base station

b(cid:48) in the nth time slot is denoted by h(n)

b→k,b(cid:48) and given by
(cid:113)

b→k,b(cid:48)=g(n)
h(n)

b→k,b(cid:48)

β(n)
b→k,b(cid:48)

where g(n)

b→k,b(cid:48) ∼ CN (0,1) represents the complex-valued small-scale Rayleigh fading component and

β(n)
b→k,b(cid:48) represents the real-valued pathloss component. The latter is given by:

(cid:32)

β(n)
b→k,b(cid:48)=

1+

(cid:33)−α

d(n)
b→k,b(cid:48)
d0

b→k,b(cid:48) denotes the Euclidean distance in meters between the base station and user in question,

where d(n)
d0 is a reference distance and α is the pathloss exponent. Hence, h(n)
channel from BS b to user k within its cell while h(n)

b→k,b denotes the information-bearing

b→k,b(cid:48) indicates the interference channel from the same

BS to user k being served by BS b(cid:48). Furthermore, we assume that the Rayleigh fading coefﬁcients of the

information-bearing and interference channels are independent across users and time slots.

For notational convenience, the network CSI in the nth time slot is denoted by the K × B × B tensor

H(n), i.e.,

H(n)=

(cid:110)

h(n)
b→k,b(cid:48)|b,b(cid:48)=1, . . . ,B; k=1, . . . ,K

(cid:111)

The transmit power allocated by BS b to user k within its cell is denoted by pb→k and is similarly collected
in the K × B matrix P(n). Assuming a receiver noise power of z(n)

k,b at user k served by BS b, the network

7

sum-rate for the nth time slot is given by:

R (cid:0)H(n), P(n)(cid:1) =



(cid:88)

(b,k)

log2


1+

(cid:80)

p(n)
b→k

2

(cid:12)
(cid:12)
(cid:12)h(n)
(cid:12)
(cid:12)
(cid:12)
b→k,b
(cid:12)
(cid:12)h(n)
(cid:12)

b(cid:48)→k,b






2

(cid:12)
(cid:12)
(cid:12)

+z(n)
k,b

(b(cid:48),k(cid:48))(cid:54)=(b,k) p(n)

b(cid:48)→k

Based on these deﬁnitions, the goal of maximizing the network spectral efﬁciency for a single time slot

can be expressed as the following optimization problem:

maximize
P(n)

R (cid:0)H(n), P(n)(cid:1)

subject to 0 ≤ p(n)

b→k ≤ Pmax

k=1, . . . ,K; b=1, . . . ,B

(1a)

(1b)

where the constraint in (1b) ensures that the power allocated to each of the users is nonnegative and does

not exceed the maximum allowed transmit power. As stated earlier, this optimization problem has been

shown to be non-convex and NP-hard in [1].

III. PROPOSED DEEP REINFORCEMENT LEARNING APPROACH

A. Markov Decision Process (MDP)

Our proposed DRL-based algorithms employ trust-region methods to solve problem (1) in both single-

agent centralized and multi-agent distributed fashion. Both these variants are modeled using the Markov

Decision Process (MDP) framework; accordingly, in this section we deﬁne some related terminology that

will be used throughout this paper.

An agent is an entity capable of processing information from its environment and taking decisions

directed towards the maximization of a chosen reward function. The agent interacts with its environment

at discrete time instants n=1, . . . ,N (where N , the episode length, is ﬁxed). The interaction model of the

agent with its enivronment can be described as follows:

• At time step n, the agent is assumed to be in state sn ∈ S. The state comprises the information that

the agent has access to and deﬁnes its situation relative to its environment. The state space, S, is

deﬁned as the set of all possible states that can be encountered by the agent.

• Acting only upon the state at time step n, the agent takes action an ∈ A from the set of all possible

actions i.e., the action space A. As illustrated in Figure 1, the action is sampled from a conditional

probability distribution known as the policy, denoted by πθ (an|sn), where the subscript indicates that

8

Fig. 1: Graphical model illustrating transitions in a Markov Decision Process.

the policy is parameterized through the (generally tensor-valued) variable θ. In this work, we use the

terms policy and policy network interchangeably since a deep neural network is used to parametrize

the policy.

• We assume a Markov state transition model; thus, the probability of transition to sn+1 is assumed to

depend only on the current state, sn, and current action, an:

p (sn+1|sn,an, . . . ,s1,a1) =p (sn+1|sn,an)

(2)

Note that the transition probability can be completely deterministic based on the agent’s current

action; likewise, it may be completely independent of the current action [20]. This Markov transition

property is depicted further in Figure 1. The state transition likelihoods are contained in the transition

operator T . Furthermore, the state distribution arising as a result of the policy πθ and environmental

transition dynamics is denoted as dπθ and referred to as the policy state distribution.

• At the end of the time step, the agent receives a scalar reward, r(sn,an), which is a function of the

current state and current action, i.e., r(sn,an) : S × A (cid:55)→ R.

Taken together, the tuple M= {S, A, T , r} deﬁnes an MDP. Using the Markov property of state transitions,

the likelihood of observing a given sequence of states and actions in the MDP can be decomposed as:

p (s1,a1, . . . ,sN ,aN ) =p(s1)

N
(cid:89)

n(cid:48)=1

p (sn(cid:48)+1|sn(cid:48), an(cid:48)) πθ (an(cid:48)|sn(cid:48))

The average discounted reward achieved over an episode is a function of θ and is given by

¯R(θ)=

E
an(cid:48) ∼πθ, sn(cid:48) ∼dπθ

(cid:34) N
(cid:88)

(cid:35)
γn(cid:48)−1r (sn(cid:48), an(cid:48))

n(cid:48)=1

(3)

(4)

where γ ∈ [0, 1] is a discount factor controlling the value of future rewards relative to current rewards.

The subscripts an ∼ πθ and sn ∼ dπθ indicate that the expectation is with respect to actions sampled

s1a1s2a2s39

from the policy and states sampled from the policy state distribution.

We also deﬁne the state-action value function, the state value function and the advantage function.

The state-action value function Qπθ (sn, an) (also known as the Q-function), is the total expected reward

that can be accumulated by taking action an from state sn and following the policy πθ in subsequent

timesteps, i.e.,

Qπθ (sn, an) (cid:44)

E
an(cid:48) ∼πθ, sn(cid:48) ∼dπθ

γn(cid:48)−nr (sn(cid:48), an(cid:48)) |sn, an

(cid:35)

(cid:34) N
(cid:88)

n(cid:48)=n

(5)

The expectation of the state-action value function, V πθ (sn), over all possible actions with respect to

the policy and its corresponding state distribution is deﬁned as the state-value function and is given by:

V πθ (sn) (cid:44)

E
an(cid:48) ∼πθ, sn(cid:48) ∼dπθ

(cid:34) N
(cid:88)

n(cid:48)=n

(cid:35)

γn(cid:48)−nr (sn(cid:48), an(cid:48)) |sn

= E

an∼πθ

[Qπθ (sn, an)]

(6)

In other words, Qπθ (sn, an) indicates the expected value of choosing a particular action an in time

step n and afterwards following the policy whereas V πθ (sn) indicates the expected reward that can be

obtained by simply following the policy throughout.

The difference between the state-action value function and state-value function is called the advantage

function and is denoted by Aπθ (sn, an):

Aπθ (sn, an) (cid:44)Qπθ (sn, an) −V πθ (sn)

(7)

Intuitively, the advantage function can be interpreted as a measure of how much better an action an is

than the average action chosen by the policy πθ.

The goal in reinforcement learning is to ﬁnd the optimum value of the policy variable θ that leads to

maximization of the expected reward:

θ∗= arg max
θ(cid:48)
Direct differentiation of the expected reward ¯R(θ) with respect to the policy parameter yields a closed-

(8)

¯R(θ(cid:48))

form expression for the policy gradient, a Monte-Carlo sampled estimate of which can then be used to

update the policy through the well-known REINFORCE algorithm as developed by Williams [21]. In other

words, the REINFORCE algorithm attempts to change the policy parameter in the direction of increasing

reward as obtained from samples of the policy. Actor-critic algorithms employ a similar approach, but

utilize a separate function approximator (typically also a deep neural network with parameters φ), called

the value network to estimate the state-action value function Qπθ (sn, an) for gradient computation; it can

be shown that this results in estimates of the gradient with lower variance and hence improved performance

10

Fig. 2: For the centralized approach, each BS forwards its downlink CSI to a policy network which then
determines the power allocation strategy for the entire network.

[22].

Unfortunately, both the REINFORCE and standard actor-critic algorithms suffer from a serious defect:

choosing a suitable step size for the policy parameter update is extremely challenging. Depending on the

parameterization, even small changes in the policy parameter θ can lead to drastic changes in the policy

output πθ; this is especially true for policies utilizing deep neural networks, in which the output is a highly

nonlinear function of the weights and biases. It is therefore desirable to seek algorithms that modify both

the ascent direction and step size to achieve stable, incremental changes in the policy space rather than

parameter space.

B. Centralized Trust Region Policy Optimization

In this section, we consider the application of trust-region policy optimization to the sum-rate maxi-

mization problem. We begin by designing a centralized approach in which a single agent (i.e., a policy

network with parameter θ) receives the downlink CSI from all base stations, computes the network power

allocation strategy and forwards it to the base stations. Thus, it follows that the state and action for the

centralized agent are the complete network CSI and power allocation matrix respectively, i.e., sn=H(n)

and an=P(n). We parameterize a probabilistic policy over the actions as follows: the policy network

takes as input the current state and outputs the mean and log-standard deviation values for each user’s

normally-distributed power allocation, as illustrated in Figure 3. The power allocation variables are then

sampled using the following normal distribution:

p(n)
b→k ∼

(cid:104)
N

(cid:16)

µ

(cid:16)

p(n)
b→k

(cid:17)

,σ2 (cid:16)

p(n)
b→k

(cid:17)(cid:17)(cid:105)Pmax

0

(9)

Centralized Policy NetworkCSI Power11

(a) Value Network.

(b) Policy Network.

Fig. 3: The value network outputs an estimate of the value function; in contrast, the policy network
produces a mean and log-standard deviation of the action as output.

where µ

(cid:16)

p(n)
b→k

(cid:17)

and σ2 (cid:16)

p(n)
b→k

(cid:17)

are the policy network output mean and variance values for the power

allocated by the bth BS to the kth user, and the limits are set to ensure that the power constraints in

(1b) are enforced. Thus, the input size for the policy network is KB2 and the output size is 2KB. On

the other hand, the value network takes the current state and action as inputs (and hence has input size

KB2 + 2KB) and outputs an estimate of the state-action value function (which is a scalar, hence the

output size is 1).

Our goal is to optimize the policy network parameter θ to maximize the expected reward, which we

Input LayerMultiple Hidden LayersOutput LayerInput LayerMultiple Hidden LayersOutput Layer12

choose as the network sum-spectral efﬁciency averaged across multiple time slots:

¯R (θ) =

E
an∼πθ, sn∼dπθ

(cid:34)

1
N

N
(cid:88)

n=1

γnR (cid:0)H(n), P(n)(cid:1)

(cid:35)

It should be noted that for this centralized approach, the likelihood of transition to a new state (i.e., H(n))

in the nth time slot is, in fact, independent of the actions taken in the previous time slot (i.e., P(n−1)).

However, as we shall see, this does not affect the derivations of the algorithms that follow.

As illustrated in Figure 2, the centralized method is similar to the FP and WMMSE algorithms in

terms of the computation and CSI exchange. Further, we emphasize that since we consider an episode to

consist of a single time-slot, the scenario is conceptually similar to the well-known ‘multi-armed bandit’

problem in reinforcement learning [23] in which the episode length consists of a single time slot. This

single time-slot episode approach has also been utilized in prior works exploring the use of policy-based

deep reinforcement learning methods to solve resource allocation problems, such as [24] and [25].

To optimize the policy network parameter, we adopt trust-region methods; these alter the ascent direction

while utilizing decaying step sizes to avoid destructively large policy updates [26]. By bounding the

change in expected reward as a function of the change in policy parameters, we can develop an iterative

optimization approach that uses second-order curvature information, rather than just the gradient, to achieve

robust policy updates. Our approach can be viewed as a hybrid of the approaches presented in [18], [19]:

we utilize a ﬁnite-horizon approach derived from that in [19] with the bounds established in [18].

We begin by casting the policy optimization problem in the following equivalent form: suppose that the

current policy parameter is θ; then the problem of maximizing expected reward is equivalent to ﬁnding

the new policy parameter θ∗ that maximizes the difference in expected reward over the current policy:

θ∗= arg . max
θ(cid:48)

¯R(θ(cid:48))− ¯R(θ)

(10)

Instead of attempting to optimize the difference in expected reward directly, we derive the following

lower bound similar to the approach in [19]:

¯R(θ(cid:48))− ¯R(θ)= E

an(cid:48) ∼πθ(cid:48)
sn(cid:48) ∼dπθ(cid:48)

= E

an(cid:48) ∼πθ
sn(cid:48) ∼dπθ(cid:48)

(cid:34) N
(cid:88)

(cid:35)
γn(cid:48)Aπθ (sn(cid:48), an(cid:48))

n(cid:48)=1

(cid:34) N
(cid:88)

n(cid:48)=1

γn(cid:48) πθ(cid:48)(an(cid:48)|sn(cid:48))
πθ(an(cid:48)|sn(cid:48))

(cid:35)
Aπθ (sn(cid:48), an(cid:48))

≥Lθ (θ(cid:48)) −C

(cid:114) E

sn(cid:48) ∼dπθ

[DKL (πθ(cid:48) (cid:107) πθ)]

(11a)

(11b)

(11c)

where

Lθ (θ(cid:48)) (cid:44) E

an(cid:48) ∼πθ
sn(cid:48) ∼dπθ

(cid:34) N
(cid:88)

n(cid:48)=1

γn(cid:48) πθ(cid:48)(an(cid:48)|sn(cid:48))
πθ(an(cid:48)|sn(cid:48))

(cid:35)
Aπθ (sn(cid:48), an(cid:48))

13

(12)

and the optimal value of the constant C to make this inequality tight can be determined analytically

provided that γ is ﬁxed [18]; DKL (πθ(cid:48) (sn) (cid:107) πθ (sn)) represents the Kullback-Leibler (KL) divergence

between the new and old policy and is given by

DKL (πθ(cid:48) (cid:107) πθ) (cid:44) E

an∼πθ(cid:48)

(cid:20)

log

(cid:21)

πθ(cid:48)(an|sn)
πθ(an|sn)

The equality in (11a) follows from the deﬁnition of the advantage function in (7) and algebraic manipu-

lation; the equality in (11b) follows since we utilize importance sampling to change the expectation to be

over actions sampled from πθ instead of πθ(cid:48). The inequality in (11c) follows from utilizing a ﬁnite-time

horizon in Corollary 2 in [18]. Furthermore, we observe that by setting θ(cid:48) = θ, both the difference in

expected reward and the lower bound equal zero. Thus, we conclude that the given lower bound minorizes

the difference in discounted rewards, and optimizing this lower bound while ensuring that it is nonnegative,

we can always ﬁnd a value of the policy parameters that does not lead to a decrease in expected reward.

The lower bound in (11c) is useful because it allows us to bound the improvement in the expected

reward in terms of the change to the policy parameters via the KL-divergence; this is in sharp contrast

to the REINFORCE algorithm in which the effect of taking too large a step cannot be quantiﬁed. At the

same time, directly optimizing this lower bound leads to conservatively small step sizes and impractically

long training times [19]. Instead, following [18], we consider the following proxy optimization problem:

maximize
θ(cid:48)

subject to

Lθ (θ(cid:48))

E
sn(cid:48) ∼dπθ

[DKL (πθ(cid:48) (cid:107) πθ)] ≤ δ

(13a)

(13b)

In its current form, this problem is non-convex and thus mathematically intractable. However, the objective

function and constraint in (13a) can be approximated using their Taylor series expansion as follows:

Lθ (θ(cid:48)) ≈ Lθ (θ) +gT

θ (θ(cid:48)−θ)
1
2

[DKL (πθ(cid:48) (cid:107) πθ)] ≈

(θ(cid:48)−θ)T Fθ (θ(cid:48)−θ)

E
sn(cid:48) ∼dπθ

where

(14a)

(14b)

Fθ(cid:44)∇2

θ(cid:48) E

st(cid:48) ∼dπθ

[DKL (πθ(cid:48) (cid:107) πθ)] |θ= E

an(cid:48) ∼πθ
sn(cid:48) ∼dπθ

(cid:2)∇θ(cid:48) log πθ(cid:48) (an(cid:48)|sn(cid:48)) |θ∇θ(cid:48) log πθ(cid:48) (an(cid:48)|sn(cid:48)) |T

θ

(cid:3)

(15)

14

We remark at this juncture that Fθ can be interpreted as the Fisher Information matrix (FIM) of the policy

and describes the curvature as a function of the policy parameter. Utilizing these approximations to both

the objective function and constraints, we obtain the following convex proxy optimization problem:

maximize
θ(cid:48)

subject to

θ (θ(cid:48)−θ)
gT
1
2

(θ(cid:48)−θ)T Fθ (θ(cid:48)−θ) ≤ δ

(16a)

(16b)

This proxy problem in (13a) can be solved analytically to yield the optimal value of θ(cid:48) in terms of the

current parameter θ as:

(cid:115)

2δ
θ F−1
gT
θ gθ
We observe that the proxy optimization problem requires knowledge of gθ and Fθ; however, we are

θ(cid:48)=θ+

θ gθ

F−1

(17)

unable to evaluate them analytically. This can be overcome as follows: we sample M episodes ε1, . . . ,εM

from the current policy πθ , where:

εm= {sm

1 ,am

1 ,rm

1 , . . . ,sm

N ,am

N ,rm
N }

These episodes can be used to obtain unbiased estimates of the policy gradient and FIM as follows:

ˆgθ=

1
M N

M
(cid:88)

N
(cid:88)

m=1

n(cid:48)=1

γn(cid:48)∇θ(cid:48) log πθ(cid:48) (am

n(cid:48)|sm

n(cid:48)) ˆAπθ(cid:48) (sm

n(cid:48),am

n(cid:48))|θ(cid:48)=θ

ˆFθ=

1
M N

M
(cid:88)

N
(cid:88)

m=1

n(cid:48)=1

[∇θ(cid:48) log πθ(cid:48) (am

n(cid:48)|sm

n(cid:48)) ·∇θ(cid:48) log πθ(cid:48)(am

n(cid:48)|sm

n(cid:48))T ]|θ(cid:48)=θ

(18)

(19)

While directly utilizing the update in (17) is possible [27], it can lead to a violation of the KL-divergence

constraint in (13b) since we utilize Taylor approximations rather than the original objective function and

constraints. To overcome this issue, we use a backtracking line search with decaying step sizes as proposed

in [18]; thus, intead in (17) denoting:

we use the following parameter update

(cid:115)

∆θ=

2δ
ˆF−1
θ ˆgθ

ˆgT
θ

ˆF−1

θ ˆgθ

θ(cid:48)=θ+ζ j ∆θ

(20)

(21)

instead of the one in (17), where ζ ∈ (0, 1) is the step size and the integer j = 0, 1, . . . is incremented

(thus decaying the step size by a factor of ζ with each increment) until the KL-divergence constraint is

satisﬁed and the objective function from (12) is nonnegative, i.e.,

Lθ (θ(cid:48)) ≥ 0

E
st(cid:48) ∼dπθ

[DKL (πθ(cid:48) (cid:107) πθ)] ≤ δ

15

(22)

(23)

Finally, combining all these steps together, we obtain the deep trust-region reinforcement learning

algorithm in Algorithm 1. We emphasize at this point that the value network is necessary only during the

training phase; once the centralized policy has been trained using Algorithm 1, the current state can be

directly forward-propagated through the policy network to obtain the network power allocation.

Algorithm 1 Trust Region Policy Optimization for Centralized Power Allocation

1: initialize centralized policy and value network parameters θ0, φ0.
2: for i = 0, 1, . . . , Niterations do
3:

for m = 0, 1, . . . , M do

Sample πθi to collect εm=(cid:8)sm

1 ,am

1 ,Rm (cid:0)H(1), P(1)(cid:1) , . . . ,sm

N ,am

N ,Rm (cid:0)H(n), P(n)(cid:1)(cid:9).

end for
Estimate Aπθi .
Update critic network parameter φi to ﬁt estimated advantage values.
Compute policy gradient estimate gθi using (18).
Compute FIM estimate ˆFθi using (19).
Compute policy update direction ∆i using (20).
for j = 0, 1, . . . do

Compute:

θi+1=θi+ζ j∆i

if (22) and (23) are satisﬁed then

break

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

end if
end for

16:
17: end for

C. Decentralized Multi-Agent Approaches

The centralized single-agent approach we have developed so far has the same information exchange

requirements as the FP and WMMSE algorithms, which become burdensome as the number of cells

and users increases. More importantly, the increasing size of both the state and action spaces leads to

extremely slow convergence. This issue, commonly referred to as the ‘curse-of-dimensionality’ [28], [29],

renders the centralized DRL approach unsuitable for large wireless networks as the associated training

times become impractically long.

Multi-agent approaches are attractive from a computational perspective since the policy network only

computes the actions for a single agent; this circumvents the problem of the increasing size of the

16

(a) Single-agent DRL.

(b) Multi-agent DRL

Fig. 4: In multi-agent DRL methods, each agent acts individually. The collective actions of all agents
inﬂuence the common reward and the (usually distinct) states the agents will encounter in the next time
step.

state and action spaces that is inevitably incurred with a centralized approach. Furthermore, as shown

in Figures 4a and 4b, each agent processes only its own state information, but can learn under a common

reward. Intuitively, training a single policy for deployment across all BSs allows the network size to grow

without necessarily increasing the training time required. As a result, multi-agent approaches are generally

utilized to solve large-scale reinforcement learning problems as opposed to single-agent methods [30]–

[32]. However, the trust region methods employed in [18] are developed for a centralized single-agent

approach.

We overcome these challenges by modifying the trust region policy optimization approach to function

in a decentralized fashion with limited information exchange. More speciﬁcally, we consider each BS in

the network to be an individual agent with partial access to network CSI; however, we design the learning

process so that these agents learn under the common reward of network spectral efﬁciency. This is in

contrast to prior DRL approaches explored in the literature, since methods like REINFORCE are typically

not robust enough to be used in a multi-agent setting, due to the aforementioned sensitivity to changes in

the policy parameters. While deep Q-learning has been used in multi-agent settings, as mentioned earlier,

it cannot directly optimize the network spectral efﬁciency and requires signiﬁcant feature engineering. On

the other hand, constrained policy optimization algorithms like trust region policy optimization have been

successfully employed in solving multi-agent common-objective learning problems [19], [30].

Partially Decentralized Approach: Accordingly, we propose a partially decentralized multi-agent ap-

proach with reduced information exchange and per-BS CSI requirements. We propose a round-robin

approach in which the BSs sequentially allocate powers, in a randomly chosen order, to the users within

their cells, using an identical policy network deployed at each BS. Thus, the time horizon for the multi-

AgentEnvironmentActionRewardStateActionRewardAgentEnvironmentAgentActionStateStateagent approach is N = B. To limit the information exchange, each BS is allowed access to its own

downlink CSI (but not the downlink CSI of other BSs) as well as the power allocation information of the

previous BSs. Thus, the state and action for BS b are given by:

17

(cid:110)(cid:110)

sb=

h(n)
b→k,b(cid:48)|b(cid:48)=1, . . . ,B; k=1, . . . ,K
(cid:111)

(cid:110)

ab=

p(n)
b→k|k=1, . . . ,K

(cid:111)

,

(cid:110)

p(n)
b(cid:48)→k|b(cid:48)=1, . . . ,b−1; k=1, . . . ,K

(cid:111)(cid:111)

To clarify, in this round-robin allocation scheme, the initial power allocation is set to zero, and BS

1 allocates power to the users within its own cell using its policy output based on its downlink CSI. It

then forwards its power allocation to BS 2, which then uses this power information along with its own

downlink CSI to allocate power to its own users using the same policy as deployed at BS 1. This process

is halted once every BS in the network has allocated power to its users, as illustrated in Figure 5a.

To avoid notational clutter in our subsequent derivations, we do not explicitly indicate the randomized

order of BSs, and assume a single-time slot episode length for the decentralized methods1. We remark

that this procedure leads to slight differences in the deﬁnition of an episode: for the centralized setting,

an episode can encompass multiple time slots, but for the partially decentralized case we consider an

episode to be complete once all BSs have allocated the transmit powers to users for that particular time

slot. Additionally, the power allocation values are sampled from a normal distribution dependent on the

policy network output similar to the centralized setting.

Unlike the centralized approach, however, the probability of transition to a new state does depend upon

the previous action since an identical policy is utilized at each BS. Speciﬁcally, each BS’s state consists

of its own downlink CSI and the power allocated by the previous BSs; the former is independent of the

previous action as in the centralized approach, while the latter directly depends upon the previous action

chosen by the policy. With this crucial difference, the actions of all BSs are coupled; and the problem

can now be considered in the more general MDP framework as introduced in Section III earlier with

multiple time-steps. We remark once again that this decentralized setting is similar to the game-theoretic

approaches detailed earlier [7], [11] as well as prior works that have utilized DRL for solving resource

allocation problems [33].

Allowing each BS access to its downlink CSI is obviously necessary; the additional power allocation

information of the previous BSs should also allow it to avoid creating intercell interference. For instance,

1Note that an episode can alternatively be deﬁned to comprise multiple time slots; however, in this work we consider a single time-slot

per episode without loss of generality.

18

(a)

(b)

Fig. 5: For the partially decentralized approach,
the BSs sequentially exchange power allocation
information in a predetermined order until the last BS is reached. For the fully decentralized approach, no
CSI or power information is exchanged, and the BSs simultaneously and independently determine their
power allocation strategy.

if the power allocated by BS b to user k is high, BS b + 1 should avoid creating interference to that user

on channel hb+1→k,b with its own power allocation strategy since that could potentially reduce the network

sum-rate.

We emphasize that such an approach with partial information sharing is only possible in the framework

of multi-agent reinforcement learning; model-based optimization algorithms like WMMSE and FP require

full exchange of CSI between all BSs to enable cooperative power allocation. Also, the prior decentralized

approaches in the literature do not allow for ﬂexible information sharing: for example, the sequential

sharing of power information by BSs in our proposed partially decentralized algorithm would simply not

be possible in either the game-theoretic decentralized approaches presented in [7], [8] or the optimization

approaches presented in [9]–[11], as this information would be useless without the corresponding downlink

channel state information of other BSs.

As with the centralized approach, the reward function is chosen to be the network sum spectral

efﬁciency; however, to ensure that all BSs learn under a common reward, the reward is accessible only at

the end of each episode (i.e., once all BSs have allocated powers to their associated users) and intermediate

values of reward are set to zero. Hence, a complete episode is given by:

ε=(cid:8)s1,a1,0, . . . ,sB,aB,R (cid:0)H(n), P(n)(cid:1)(cid:9)

Decentralized Policy NetworkDecentralized Policy NetworkDecentralized Policy NetworkCSI PowerDecentralized Policy NetworkDecentralized Policy NetworkDecentralized Policy NetworkCSI PowerThe ascent direction is once again calculated using (20). Since an identical policy is deployed at all BSs,

the same gradient update is applied to every agent. Combining all steps together, the partially decentralized

approach is summarized in Algorithm 2.

Algorithm 2 Trust Region Policy Optimization for Partially Decentralized Power Allocation

19

1: initialize policy and value network parameters θ0, φ0 at BSs b = 1, . . . , B.
2: for i = 0, 1, . . . , Niterations do
3:

for m = 0, 1, . . . , M do
for b = 0, 1, . . . , B do
Propagate sm
b
1 , . . . , am
Forward am
b

end for
Collect εm=(cid:8)sm

through πθi at BS b to obtain am
b .

to BS b + 1.

1 ,am

1 ,0, . . . ,sm

B ,am

B ,Rm (cid:0)H(n), P(n)(cid:1)(cid:9).

end for
Estimate Aπθi .
Update critic network parameter φi at BSs b = 1, . . . , B to ﬁt estimated advantage values.
Compute policy gradient estimate gθi using (18).
Compute FIM estimate ˆFθi using (19).
Compute policy update direction ∆i using (20).
for j = 0, 1, . . . do

Compute:

θi+1=θi+ζ j∆i

if (22) and (23) are satisﬁed then

Apply policy update at BSs b = 1, . . . , B.
break

end if
end for

21:
22: end for

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

Similar to the centralized setting, we remark that once the (identical) policy network at each BS has

been trained, the value network is no longer necessary. Likewise, the common gradient update needs to

be applied only during the training phase; once the trained agents are deployed, the state information can

directly be propagated through the policy network at each BS to obtain the desired power allocation for

that cell.

Fully Decentralized Approach: For the fully decentralized approach, we eliminate all communication

between the BSs in the network; hence the state information for each BS comprises only its downlink

CSI:

(cid:110)

sb=

h(n)
b→k,b(cid:48)|b(cid:48)=1, . . . ,B; k=1, . . . ,K

(cid:111)

ab=

(cid:110)

p(n)
b→k|k=1, . . . ,K

(cid:111)

Since no information is exchanged between the agents, all BSs independently and simultaneously

20

determine their individual power allocation strategies, as illustrated in Figure 5b. The reward function is

once again chosen to be the network sum-rate and is only accessible at the end of each episode; likewise,

an identical policy is utilized at each BS and updated at the end of each training episode. The fully

decentralized algorithm is summarized in Algorithm 3.

Finally, we note that the feedback of the network sum-rate to both the centralized agent and the

distributed agents is only necessary during the training phase. Once a trained agent has been deployed,

there is no further need for this information.

Algorithm 3 Trust Region Policy Optimization for Fully Decentralized Power Allocation

1: initialize policy and value network parameters θ0, φ0 at BSs b = 1, . . . , B.
2: for i = 0, 1, . . . , Niterations do
3:

for m = 0, 1, . . . , M do
for b = 0, 1, . . . , B do
Propagate sm
b

end for
Collect εm=(cid:8)sm

1 ,am

through πθi at BS b to obtain am
b .
B ,Rm (cid:0)H(n), P(n)(cid:1)(cid:9).

1 ,0, . . . ,sm

B ,am

end for
Estimate Aπθi .
Update critic network parameter φi at BSs b = 1, . . . , B to ﬁt estimated advantage values.
Compute policy gradient estimate gθi using (18).
Compute FIM estimate ˆFθi using (19).
Compute policy update direction ∆i using (20).
for j = 0, 1, . . . do

Compute:

θi+1=θi+ζ j∆i

if (22) and (23) are satisﬁed then

Apply policy update at BSs b = 1, . . . , B.
break

end if
end for

20:
21: end for

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

IV. NUMERICAL RESULTS

To evaluate the performance of the proposed methods, we simulated different cellular networks with

the following system parameters:

21

TABLE I: Numerical values of system model parameters

Total bandwidth

W = 20 MHz

BS maximum transmit power

Pmax = 43 dBm

Noise PSD

Noise ﬁgure

N0 = −150 dBm/Hz

Nf = 9 dB

Reference distance

d0 = 0.3920 m

Cell radius

1000 m

The parameters utilized for the proposed centralized, partially and fully decentralized deep learning

algorithms are chosen identically as follows:

TABLE II: Numerical values of deep learning parameters

KL-divergence constraint

δ = 0.01

Step size

Discount factor

ζ = 0.90

γ = 0.99

Policy network hidden layers

Value network hidden layers

3

3

Neurons per hidden layer

256

Episodes per iteration

M =1000

Furthermore, the neurons in the hidden layers for both the policy and value networks are chosen to be

exponential linear units and have an activation function given by:

σ (z) =




z



ez−1

z>0

z ≤ 0

(24)

The proposed DRL schemes were implemented using Python 3.6 and TensorFlow 1.14.0. The machine

utilized for training and evaluation of all schemes was equipped with a dual-core 3.1 GHz Core i5

processer, 8 GB of memory and no discrete GPU.

22

A. Training Performance

We begin by considering training performance for the proposed approaches in a 3-cell network with the

parameters in Table I and K = 2 users per cell; additionally, the path loss exponent utilized in this scenario

was set to α = 3.76. Note that we utilize this small network size to fully evaluate training performance

and algorithm convergence for all three approaches. The policy and value networks are characterized using

the parameters in Table II. It should be noted that different random initializations of the neural networks

may converge to different ﬁnal policy and value networks; thus, we plot training curves for 10 different

parameter initializations to analyze the convergence behaviour. Furthermore, to ensure a fair comparison,

we train the centralized, partially decentralized, and fully decentralized algorithms for 4 × 106 training

steps each; this corresponds to 4 × 106 and 4/3 × 106 episodes for the centralized and decentralized

approaches respectively.

Figure 5(a) illustrates the evolution of per-episode reward for the centralized approach. Starting from an

average of around 40 Mbps, the algorithm converges to a per-episode reward of 122 Mbps. Furthermore, we

observe that the training curves for the different random initializations are highly consistent in converging

to a similar ﬁnal reward. We observe some variance during the training process; this is to be expected, since

the channel realizations (and hence the network sum-rate achieved) during each time slot are generated

randomly. The convergence behaviour can be visualized better by plotting an exponentially weighted

average of the training performance 2 which is illustrated in Figure 5(b). As we can observe, the different

random initializations follow closely matching training trajectories in converging to nearly identical ﬁnal

rewards.

In Figures 7 and 8 respectively, we plot similar training curves for 10 random initializations of the

partially and fully decentralized algorithms. In these cases too, we observe that the average reward improves

as training progresses. However, two crucial differences emerge in comparison to the centralized algorithm:

ﬁrst, the decentralized algorithms converge to a lower ﬁnal reward, and second, they demonstrate notably

higher variance across runs. In particular, considering the smoothed training curves, we observe that both

the partially and fully decentralized approaches achieve a ﬁnal reward of around 120 Mbps. The fully

decentralized algorithm also demonstrates the greatest spread of ﬁnal rewards among the different random

2The exponentially weighted average s[n] of a sequence x[n] is calculated using the relation

s[n] =

(cid:26) wx[n] + (1 − w)s[n − 1] n > 1
n = 1

x[n]

where w represents the smoothing factor. In this paper, we use w = 0.96.

23

initializations and, on average, is outperformed by the partially decentralized algorithm in this setting.

As a baseline, we also compare the training performance against the well-known A2C (Advantage Actor-

Critic) DRL algorithm ﬁrst proposed in [22]. Like TRPO, A2C utilizes a critic network to help reduce

variance in the policy gradient estimates; however, unlike TRPO, the step size is ﬁxed. We plot the training

performance of the A2C algorithm for the 3-cell centralized setting for 10 random policy initializations

with a ﬁxed step size of 7 × 10−4 and a smoothing factor of 0.96 in Figure 9. The A2C algorithm is

unable to converge in this setting and demonstrates poor performance across all initializations. Speciﬁcally,

the average reward attained ﬂuctuates between approximately 65 and 82 Mbps; this is in contrast to the

corresponding centralized TRPO method, which achieves an average reward of around 122 Mbps with

a variation of less than 1 Mbps across all initializations. This can be directly attributed to the fact that

taking even small changes in the policy parameters can lead to unexpected changes in terms of the reward

function.

B. Sum-Rate Performance

In this section, we compare the performance of the proposed approaches against the following uncoor-

dinated and state-of-the-art coordinated algorithms:

1) Maximum Power Allocation: In this scheme, each BS transmits at the maximum power Pmax to

every user within its cell. As the simplest uncoordinated scheme, this scheme necessarily achieves

poor performance; nonetheless, it is the least computationally complex and requires no exchange

of information between the BSs. Thus, it serves as a useful benchmark to evaluate the performance

and complexity of more sophisticated resource management solutions.

2) Random Power Allocation: For this scheme, the power allocated to each user by its serving base

station is chosen uniformly randomly from the interval [0, Pmax]. Like the maximum power alloca-

tion scheme, this approach is desirable from a computation and information exchange perspective

although its performance is worse than that of coordinated schemes.

3) Fractional Programming: FP is an iterative method based on minorization-maximization developed

by Shen and Yu in [5], and is the highest-performing resource allocation scheme developed thus far

in the literature [5], [6], [14]. As emphasized in the introduction, FP is guaranteed to converge to

a local optimum of the network-sum-rate optimization problem; however, the number of iterations

needed to converge is unknown a priori.

24

(a) Training convergence.

(b) Exponentially weighted average training convergence

Fig. 6: Centralized training convergence for 10 random initializations, B = 3, K = 2.

(a) Training convergence.

(b) Exponentially weighted average training convergence

Fig. 7: Partially decentralized training convergence for 10 random initializations, B = 3, K = 2.

(a) Training convergence.

(b) Exponentially weighted average training convergence

Fig. 8: Fully decentralized training convergence for 10 random initializations, B = 3, K = 2.

00.511.522.533.54Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1000.511.522.533.54Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1000.20.40.60.811.2Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1000.20.40.60.811.2Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1000.20.40.60.811.2Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1000.20.40.60.811.2Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1025

Fig. 9: Exponentially weighted centralized training convergence for A2C algorithm for 10 random
initializations; B = 3, K = 2.

4) Weighted Minimum Mean-Squared Error: WMMSE transforms the original sum-rate maximization

problem into an equivalent optimization problem of minimum mean-squared error minimization.

Like FP, WMMSE is guaranteed to converge in a monotonically nondecreasing fashion to a local

optimum of the problem. Speciﬁcally, we compare our proposed methods against the SISO variant

of the WMMSE algorithm as utilized in [3], [5], [14], [15].

Note that we do not compare against the decentralized approaches in [7], [8], [10], [11] as we consider

the state-of-the-art FP and WMMSE algorithms as the performance benchmarks.

We begin by considering the performance of the different algorithms for the aforementioned B = 3

cells, K = 2 users per cell setting considered for the training curves. Figure 10 plots the convergence of

the network sum-rate for a single random initialization of channel values according to the given system

model. As expected, the uncoordinated max-power and random-power allocation schemes achieve the

worst performance; on the other hand, both FP and WMMSE converge in a monotonically nondecreasing

fashion to local optima of the network sum-rate maximization problem as expected. In comparison, both

the coordinated and uncoordinated schemes are outperformed by the proposed deep learning approaches.

As with the training results, the centralized approach outperforms the partially decentralized approach

which, in turn, achieves a small performance advantage over the fully decentralized scheme. Also, the

00.511.522.533.54Training Episodes (x106)405060708090100110120130Average Episode Reward (Mbps)Run 1Run 2Run 3Run 4Run 5Run 6Run 7Run 8Run 9Run 1026

Fig. 10: Sum-rate convergence for a single random channel realization, B = 3, K = 2.

coordinated FP and WMMSE algorithms require multiple iterations to reach the local optimum of the

problem; this is in contrast to the proposed DRL approaches, in which the state information can be directly

propagated through the policy network to obtain the desired solution to the optimization problem.

The results in Figure 10 are for a single time slot with user locations and channels generated randomly

according to the distributions described in the system model. To compare fairly with the stated benchmarks,

in Figure 11 we consider the average network sum-rate achieved across 1000 independent random channel

realizations. These results display a similar trend: the centralized scheme achieves the highest average

network spectral efﬁciency, followed by the partially and fully decentralized schemes respectively. The

model-based optimization algorithms are outperformed by the DRL schemes while the uncoordinated

schemes once again achieve the worst performance. We also remark that even the fully decentralized

approach outperforms the state-of-the-art FP and WMMSE algorithms, despite the fact that it uses only a

fraction of the downlink CSI available to these coordinated schemes. This result is all the more remarkable

when we consider that none of the proposed approaches are trained for particular channel realizations;

instead, during training, the user locations and channel values are generated randomly from the stated

distributions in the channel model. In other words, these results indicate that the DRL approaches achieve

good generalization, as they are able to perform well on channel realizations that are drawn from the

12345678910Iterations30405060708090Network Sum-Rate (Mbps)Centralized RLPartially Decentralized RLFully Decentralized RLFPWMMSEMax PowerRandom27

Fig. 11: Averaged network sum-rate for 1000 channel realizations, B = 3, K = 2.

same distribution as, but not identical to, the training episodes [34].

As stated earlier, the chief drawback of the centralized scheme is that the training times become

impractically large compared to the decentralized schemes as the cellular network size grows. Furthermore,

the information exchange requirements are identical to the FP and WMMSE algorithms and as these

become burdensome for larger wireless networks. Accordingly, we proceed by testing the performance of

the decentralized approaches in a much larger 7-cell hexagonal network with wraparound; the network

parameters are chosen to be identical to the 3-cell setting, with the exception of the pathloss exponent α

which is set to 4.00. Figure 12 demonstrates the convergence of network sum-rate for a single random

realization of channels for this setting; similar to the 3-cell network, the partially and fully decentralized

approaches once again achieve higher network spectral efﬁciency than the benchmark schemes.

We also consider the average sum-rate achieved across a large number of independent channel realiza-

tions in Figure 13, similar to Figure 11. These results display similar trends to the 3-cell setting, with the

partially decentralized approach once again achieving the highest network spectral efﬁciency and achieving

7% higher network sum-rate than FP. The fully decentralized approach is once again outperformed by the

partially decentralized approach, but still manages to achieve higher objective function values than the

FP, WMMSE, random and max-power schemes.

Cent.   RL   Part.  Decent.      RL   Fully  Decent.      RLFPWMMSERandomMax-Power020406080100120140Average Network Sum-Rate (Mbps)  122.1119.8118.3 117.4102.744.651.328

Fig. 12: Sum-rate convergence for a single random channel realization, B = 7, K = 8.

Fig. 13: Averaged network sum-rate for 1000 channel realizations, B = 7, K = 8.

To test the robustness of the proposed RL approaches we also plot the average sum-rate achieved across

multiple time slots for Pmax values ranging between 20 and 50 dBm for the 3-cell setting in Figure 14.

051015Iterations50100150200250300350Network Sum-Rate (Mbps)Partially Decentralized RLFully Decentralized RLFPWMMSEMax PowerRandom     Partially  Decentralized          RL       Fully  Decentralized          RLFPWMMSERandomMax-Power050100150200250300350400Average Network Sum-Rate (Mbps)389.77368.63 365.2364.09 70.39 89.1229

Fig. 14: Averaged network sum-rate for different transmit powers, B = 3, K = 2.

For this test, we utilize the policy networks that have been previously trained for a transmit power of

43 dBm by simply scaling the output of the policy networks to match the transmit power constraint; this

tests the performance of the DRL approaches when the reward function is changed from that utilized

during training. As we can see, all three DRL methods demonstrate excellent performance across the

entire range, closely matching the FP and WMMSE algorithms for the lower transmit values. At higher

transmit power levels, the proposed DRL approaches demonstrate superior performance to the WMMSE

algorithm while continuing to match the FP algorithm.

For completeness, we also consider a slight variation of the original network sum-rate maximization

problem with a sum-power constraint of 43 dBm instead of a per-user power constraint for the 3-cell

setting. This power constraint is enforced for the RL algorithms by simply rescaling the power output

during training to ensure that the per-BS sum power does not exceed Pmax. As the results in Figure 15

show, the fully centralized and partially decentralized RL algorithms still outperform the FP and WMMSE

algorithms, as well as the uncoordinated approaches.

C. Channel State Information Exchange

In comparing the performance of different schemes, it is important to consider the amount of information

exchange between BSs necessary in order to achieve the resulting performance. As mentioned earlier, the

20253035404550Maximum Transmit Power Pmax (dBm)020406080100120140160180200Network Sum-Rate (Mbps)Centralized RLPartially Decentralized RLFully Decentralized RLFPWMMSEMax PowerRandom30

Fig. 15: Averaged network sum-rate for sum-power constraint, B = 3, K = 2.

FP and WMMSE algorithms require all BSs to send their complete downlink CSI to a central cloud

processor which then computes and forwards in order to compute the power allocation variables. For a

network consisting of B cells and K users per cell, this corresponds to the central processor receiving
O (cid:0)KB2(cid:1) scalars from the cooperating BSs. The centralized single-agent approach also requires identical

CSI exchange between the BSs.

For the partially decentralized approach, the state information for each BS comprises the downlink

CSI from itself to all the users in the network as well as the power allocation decisions of the previous

BSs. This corresponds to an information exchange requirement of O (KB). On the other hand, for the

fully decentralized approach, the state information comprises only the downlink channels from itself to the

users in the network. Accordingly, there is no information exchange between the BSs of the network. This

is similar to the uncoordinated max-power and random-power schemes. These results are summarized in

Table III, along with the necessary CSI per BS.

D. Execution Times

An essential aspect of comparison between different resource allocation algorithms is to consider the

time and complexity necessary to calculate a solution. A key beneﬁt of utilizing deep reinforcement

learning is that propagating the current state through the policy network to ﬁnd the action is typically

Cent.   RL   Part.  Decent.      RL   Fully  Decent.      RLFPWMMSERandomEqual Power020406080100120140Average Network Sum-Rate (Mbps)  120.1 118.8117.9115.1110.7  46.740.731

TABLE III: Comparison of CSI and information exchange requirements

Resource Allocation Scheme
Centralized RL
Partially Decentralized RL
Fully Decentralized RL
FP
WMMSE
Max-power
Random

Information exchange Per-BS CSI needed
O (cid:0)KB2(cid:1)
O (KB)
0
O (cid:0)KB2(cid:1)
O (cid:0)KB2(cid:1)
0
0

O (cid:0)KB2(cid:1)
O (KB)
O (KB)
O (cid:0)KB2(cid:1)
O (cid:0)KB2(cid:1)
0
0

TABLE IV: Average execution times of different resource allocation schemes
Resource Allocation Scheme Average Execution Time (seconds)
Centralized RL
Partially Decentralized RL
Fully Decentralized RL
FP
WMMSE
Max-power
Random

6.5 × 10−4
1.99 × 10−3
6.5 × 10−4
6.4 × 10−1
6.0 × 10−1
0
1.75 × 10−4

quite fast; thus, when the agents have been trained, power allocation solutions can be found in a fraction

of the time required by conventional optimization techniques.

Table IV illustrates the execution times of the trained models for our implementations of the different

schemes. As we can observe, the deep-RL algorithms require an execution time that is over two orders of

magnitude lower to calculate a power allocation strategy as compared to the WMMSE and FP algorithms.

All times have been measured for the 3-cell setting; although since we use identical neural network sizes

throughout, these results are equally representative for the 7-cell network.

It should be noted that the partially decentralized approach requires the longest execution time among

the DRL approaches due to the sequential sharing of power allocation information. The state information

for each BS needs to be propagated through its policy network to produce its power allocation strategy,

and then forwarded to the next BS, which repeats this process, and so on; it follows that the last BS

would have to wait until all other BSs have computed their power allocation strategies. Accordingly, the

execution time for this approach will be B times longer than the fully decentralized approach, which can

be executed in parallel at each BS. It should be noted, however, that the latency introduced in this regard

would still be much lower than that required by the FP and WMMSE algorithms; for example, projecting

from the execution times in Table III, the execution time would still be around an order of magnitude

lower than the FP and WMMSE algorithms.

V. CONCLUSIONS

32

In this paper, we employed a deep-reinforcement learning approach to directly solve the continuous-

valued, non-convex and NP-hard downlink sum-rate optimization problem. We proposed multiple vari-

ants: a fully centralized single-agent approach as well as partially and fully decentralized multi-agent

approaches. The centralized and partially decentralized TRPO approaches achieve higher network sum-

rate than the state-of-the-art FP, WMMSE and A2C algorithms while, once trained, all three approaches are

capable of ﬁnding solutions in a fraction of the time needed by the conventional optimization methods. Of

greater consequence, the framework of trust region policy optimization allows us to design decentralized

schemes enabling varying degrees of information exchange between base stations while overcoming the

‘curse-of-dimensionality’ issues associated with centralized reinforcement learning.

This work represents a preliminary step in investigating centralized and decentralized reinforcement

learning algorithms for solving wireless resource management problems, and many fruitful directions are

possible for future research. In particular, reducing the number of samples and computation necessary

to achieve effective performance is possibly the most important open problem in reinforcement learning

algorithms. Our work is, to the best of our knowledge, the ﬁrst to implement information-sharing between

DRL agents to solve optimization problems. While the exchange of power information is beneﬁcial in

helping increase network spectral efﬁciency as compared to the fully decentralized scheme, it is possible

that feature engineering may be able to improve performance further. Finally, we note that the extension

to multiple-input multiple-output (MIMO) wireless networks remains challenging for machine learning

algorithms due to input representation and prohibitively large input dimensionality; distributed DRL

algorithms such as those proposed in this work may help overcome the latter obstacle.

REFERENCES

[1] Z.-Q. Luo and S. Zhang, “Dynamic Spectrum Management: Complexity and Duality,” IEEE J. Sel. Topics Signal Process., vol. 2,

no. 1, pp. 57–73, 2008.

[2] L. Liu, R. Zhang, and K.-C. Chua, “Achieving Global Optimality for Weighted Sum-Rate Maximization in the K-User Gaussian

Interference Channel with Multiple Antennas,” IEEE Trans. Wireless Commun., vol. 11, no. 5, pp. 1933–1945, 2012.

[3] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos, “Learning to Optimize: Training Deep Neural Networks for

Interference Management,” IEEE Trans. Signal Process., vol. 66, no. 20, pp. 5438–5453, 2018.

[4] Q. Shi, M. Razaviyayn, Z.-Q. Luo, and C. He, “An Iteratively Weighted MMSE Approach to Distributed Sum-Utility Maximization

for a MIMO Interfering Broadcast Channel,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4331–4340, 2011.

[5] K. Shen and W. Yu, “Fractional Programming for Communication Systems - Part I: Power Control and Beamforming,” IEEE Trans.

Signal Process., vol. 66, no. 10, pp. 2616–2630, 2018.

33

[6] A. A. Khan, R. Adve, and W. Yu, “Optimizing Multicell Scheduling and Beamforming via Fractional Programming and Hungarian

Algorithm,” in IEEE Globecom Workshops (GC Wkshps), Abu Dhabi, Dec. 2018, pp. 1–6.

[7]

I. Menache and A. Ozdaglar, “Network Games: Theory, Models, and Dynamics,” Synthesis Lectures Commun. Netw., vol. 4, no. 1, pp.

1–159, 2011.

[8] P. De Kerret, S. Lasaulce, D. Gesbert, and U. Salim, “Best-Response Team Power Control for the Interference Channel with Local

CSI,” in IEEE Int. Conf. on Commun. (ICC), London, Jun. 2015, pp. 4132–4136.

[9] O. Tervo, H. Pennanen, D. Christopoulos, S. Chatzinotas, and B. Ottersten, “Distributed Optimization for Coordinated Beamforming

in Multicell Multigroup Multicast Systems: Power Minimization and SINR Balancing,” IEEE Trans. Signal Process., vol. 66, no. 1,

pp. 171–185, 2017.

[10] R. Fritzsche and G. P. Fettweis, “Distributed Robust Sum Rate Maximization in Cooperative Cellular Networks,” in 2013 IEEE Int.

Conf. Commun. Workshops (ICC), Jun 2013.

[11] S.-H. Park, H. Park, and I. Lee, “Distributed Beamforming Techniques for Weighted Sum-Rate Maximization in MISO Interference

Channels,” IEEE Commun. Lett., vol. 14, no. 12, pp. 1131–1133, 2010.

[12] C. D’Andrea, A. Zappone, S. Buzzi, and M. Debbah, “Uplink Power Control in Cell-Free Massive MIMO via Deep Learning,” in

IEEE Int. Workshop Comput. Advances Multi-Sensor Adaptive Process. (CAMSAP), Le gosier, Dec. 2019, pp. 554–558.

[13] B. Matthiesen, A. Zappone, E. A. Jorswieck, and M. Debbah, “Deep Learning for Real-Time Energy-Efﬁcient Power Control in Mobile

Networks,” in IEEE Int. Workshop Signal Process. Advances in Wireless Commun. (SPAWC), Cannes, Jul. 2019, pp. 1–5.

[14] Y. S. Nasir and D. Guo, “Multi-Agent Deep Reinforcement Learning for Dynamic Power Allocation in Wireless Networks,” IEEE J.

Sel. Areas Commun., vol. 37, no. 10, pp. 2239–2250, 2019.

[15] F. Meng, P. Chen, L. Wu, and J. Cheng, “Power Allocation in Multi-User Cellular Networks: Deep Reinforcement Learning Approaches,”

IEEE Trans. Wireless Commun., 2020.

[16] Y. Wei, F. R. Yu, M. Song, and Z. Han, “User Scheduling and Resource Allocation in Hetnets with Hybrid Energy Supply: An

Actor-Critic Reinforcement Learning Approach,” IEEE Trans. Wireless Commun., vol. 17, no. 1, pp. 680–692, 2017.

[17] X. Zhang, M. R. Nakhai, G. Zheng, S. Lambotharan, and B. Ottersten, “Calibrated Learning for Online Distributed Power Allocation

in Small-Cell Networks,” IEEE Trans. Commun., 2019.

[18] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained Policy Optimization,” in Int. Conf. on Mach. Learn., Sydney, Aug. 2017,

pp. 22–31.

[19] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in Int. Conf. Mach. Learn., Lille,

Jul. 2015, pp. 1889–1897.

[20] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep Reinforcement Learning: A Brief Survey,” IEEE Signal

Process. Mag., vol. 34, no. 6, pp. 26–38, 2017.

[21] R. J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,” Mach. Learn., vol. 8,

no. 3-4, pp. 229–256, 1992.

[22] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous Methods for Deep

Reinforcement Learning,” in Int. Conf. Mach. Learn., New York City, Jun. 2016, pp. 1928–1937.

[23] V. Kuleshov and D. Precup,

“Algorithms

for Multi-Armed Bandit Problems,” Feb. 2014.

[Online]. Available: https:

//arxiv.org/abs/1402.6028

[24] F. Liang, C. Shen, W. Yu, and F. Wu, “Towards Optimal Power Control via Ensembling Deep Neural Networks,” IEEE Trans. Commun.,

vol. 68, no. 3, pp. 1760–1776, 2019.

34

[25] M. Eisen, C. Zhang, L. F. Chamon, D. D. Lee, and A. Ribeiro, “Learning Optimal Resource Allocations in Wireless Systems,” IEEE

Trans. Signal Process., vol. 67, no. 10, pp. 2775–2790, 2019.

[26] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage

Estimation,” Oct. 2018. [Online]. Available: https://arxiv.org/abs/1506.02438

[27] S. M. Kakade, “A Natural Policy Gradient,” in Advances Neural Inf. Proc. Syst., 2002, pp. 1531–1538.

[28] T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor, “Learn What Not to Learn: Action Elimination with Deep

Reinforcement Learning,” in Advances Neural Inf. Process. Syst., 2018, pp. 3562–3573.

[29] J. Han, A. Jentzen, and E. Weinan, “Solving high-dimensional partial differential equations using deep learning,” Proc. Nat. Acad. Sci.,

vol. 115, no. 34, pp. 8505–8510, 2018.

[30] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I. Mordatch, “Emergent Tool Use From Multi-Agent

Autocurricula,” Aug. 2019. [Online]. Available: https://arxiv.org/abs/1909.07528

[31] S. Omidshaﬁei, J. Pazis, C. Amato, J. P. How, and J. Vian, “Deep Decentralized Multi-Task Multi-Agent Reinforcement Learning under

Partial Observability,” in Int. Conf. Mach. Learn., Sydney, Aug. 2017, pp. 2681–2690.

[32] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to Communicate with Deep Multi-Agent Reinforcement Learning,”

in Advances Neural Inf. Process. Syst., 2016, pp. 2137–2145.

[33] P. de Kerret, D. Gesbert, and M. Filippone, “Team Deep Neural Networks for Interference Channels,” in IEEE Int. Conf. Commun.

Workshops (ICC Workshops), Kansas City, May 2018, pp. 1–6.

[34] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, 2006.

