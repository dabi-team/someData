0
2
0
2

v
o
N
2
2

]

G
L
.
s
c
[

1
v
3
6
0
1
1
.
1
1
0
2
:
v
i
X
r
a

Learning a Deep Generative Model like a Program: the Free Category Prior

Eli Sennesh1
1Khoury College of Computer Science
Northeastern University
440 Huntington Ave
Boston, MA 02115
sennesh.e@northeastern.edu

Humans surpass the cognitive abilities of most other animals
in our ability to “chunk” concepts into words, and then com-
bine the words to combine the concepts. In this process, we
make “inﬁnite use of ﬁnite means”1, enabling us to learn
new concepts quickly (Lake, Salakhutdinov, and Tenen-
baum, 2013) and nest concepts within each-other
(Lake
and Piantadosi, 2019). We construct concepts to model the
world around us (Goodman, Tenenbaum, and Gerstenberg,
2014). In contrast, in artiﬁcial intelligence we still mostly fo-
cus on learning discriminative functions mapping from sen-
sory inputs to decision outputs, without modeling the data-
generating process (Zhu et al., 2019).

foundational

theories of artiﬁcial

While program induction and synthesis remain at the
intelligence
heart of
(Solomonoff, 1964; Vit´anyi and Li, 2000), only recently has
the community moved forward in attempting to use program
learning as a benchmark task itself (Chollet, 2019). Models
of program learning are most often based upon probabilis-
tic context-free grammars (Overlan, Jacobs, and Piantadosi,
2017; Romano et al., 2018), with neural networks and/or re-
inforcement learning sometimes used to aid inference (El-
lis et al., 2018, 2020). However, not every grammatical pro-
gram in the support of a PCFG yields a meaningful likeli-
hood. Indeed, a recent experiment by Bramley et al. (2018)
found that 35% of sampled hypotheses for a logical rule task
were either tautologies (true for all individual cases) or con-
tradictions (true for no individual statements).

Here we confront the assumption that a compositional
prior over complex programs must necessarily take the form
of a (probabilistic) grammar over a symbolic (probabilis-
tic) language of thought (Piantadosi, Tenenbaum, and Good-
man, 2016; Frankland and Greene, 2020). While a formal
language theorist would recognize strings of symbols, par-
ticularly bound and unbound variables, as one way among
many to encode a universal programming language, symbol
strings are the modality in which most programmers express
most programs. The cognitive science community has thus
often assumed that if the brain has simulation and reasoning
capabilities equivalent to a universal computer, then it must

Copyright © 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1Attributed to Wilhelm von Humboldt

Figure 1: The multigraph (or nerve) of a free category gen-
erated by deep generative models.

employ a serialized, symbolic representation Marcus (2018).
We provide a counterexample in which compositionality is
expressed via network structure.

Contributions Here we contribute an alternative genera-
tive model for representing programs: free categories. From
this structure, we can sample “correct by construction” prob-
abilistic programs, whose support must contain the data,
without maintaining a symbol table or performing additional
type-checking steps. We show how our formalism allows
neural networks to serve as primitives in probabilistic pro-
grams, similarly to Valkov et al. (2018). We learn both pro-
gram structure and model parameters end-to-end.

The free category prior over programs
A category C consists of a set of objects Ob(C), and for
every pair of objects A, B ∈ Ob(C) a set of morphisms
C(A, B). Individual morphisms can be denoted f : A → B,
and each object A is equipped with a unique idA : A → A.
As implied by the notation, in programming terms we can
imagine objects to be data-types and morphisms to be func-
tions between them. There are also two major laws which all
morphisms must obey to form a category: the associativity
of morphism composition, and the identity of unital compo-
sition with identity morphisms.

424×288×21616×23232×24949×2196196×2784784×196 
 
 
 
 
 
def path_between(A, B, g, P):

loc = A
f = idA
while loc != B:

probs = [Pa,B for a in g.out_arrows(loc)]
arrow = sample(Categorical(probs))
f = f >> arrow # Composition of morphisms
loc = dest(arrow)

return f

Listing 1: The loop for sampling short paths between a source A and a destination B in a free category described by the graph
g and biased random walk P .

graph G into a directed graph G(cid:48)
C = (V (cid:48), E(cid:48)), in which each
generating morphism f : A → B becomes a vertex v ∈ V (cid:48)
with a single incoming edge (from A ∈ V (cid:48)) and a single
outgoing edge (to B ∈ V (cid:48)). Denoting the adjacency matrix
of G(cid:48) as A ∈ R|V (cid:48)|×|V (cid:48)|, we can now construct transition
probabilities Pi,j for all (i, j) ∈ |V (cid:48)| × |V (cid:48)|,

p(i → j | β, W ) = softmax

log Pi,j ∝

1
β

(cid:0)eA + AW (cid:1)

i,j ,

(cid:0)eA + AW (cid:1)

(cid:18) 1
β
(cid:110) 1
(cid:0)eA + AW (cid:1)
β
(cid:110) 1

i,j
(cid:111)

i,j

(cid:80)|V (cid:48)|

j=1 exp

β (eA + AW )i,j

exp

=

(1)

(cid:19)

,

(2)

(cid:111) .

(3)

Here, we write eA for the matrix exponential of A, which
describes the “long-run” transition structure of an unbi-
ased random walk, and we also incorporate weights W ∈
R|V (cid:48)|×|V (cid:48)| that can encode arbitrary “preferences” for some
primitive arrows over others. The ﬁrst term inside the paren-
theses thus imposes an inductive bias towards primitive
arrows that belong to many multi-step paths through the
graph. The second term incorporates a learnable prefer-
ence for some edges over others. The inverse temperature
β > 0 denotes the relative “conﬁdence” of the soft opti-
mization. Rather than using − log Pi,j as an “intuitive dis-
tance” (Baram et al., 2018; Behrens et al., 2018) that com-
presses shared path segments, we can also hold j constant
across a whole path and use − log P·,j as an intuitive dis-
tance to the destination node. This observation inspires the
core of our generative model, seen in Listing 1, and can be
considered a stochastic generalization of path-based plan-
ning techniques (Blum and Furst, 1997; Blum and Langford,
2000).

Since our underlying free category includes Cartesian
product objects such as (A, B), we include “macro” arrows
(cid:62) → (A, B) in the multigraph over which we perform the
random walk. When sampled, we “expand” these macros by
sampling morphisms (cid:62) → A and (cid:62) → B and applying the
Cartesian product × on morphisms. The same type of macro
can be used to represent exponential objects BA (the inter-
nal hom-set of morphisms A → B), by recursively invoking
the probabilistic subroutine path between().

Figure 2: String diagram drawn from the posterior distribu-
tion over deep generative model architectures for Omniglot.

The closure of a directed multigraph under associative
composition and self-looping forms the free category over
that multigraph. When the number of generating morphisms
(edges in the multigraph) is ﬁnite, we say that the category
is a ﬁnitely-generated free category, and we can represent it
computationally via its underlying multigraph, G = (V, E).
In Figure 1, we show the multigraph of a free, ﬁnitely-
generated category. The vertices V correspond to (tuples of)
tensor shapes Ob(C), and the edges E correspond to proba-
bilistic decoder networks that act as generating morphisms.
There is a privileged object (cid:62), known as the “terminal” or
“unit” object; generating morphisms such as f : (cid:62) → R64
represent prior distributions pf (Z), Z ∈ R64 over the in-
dividual vector spaces. By designing a probabilistic gener-
ative model to sample paths through this graph, we thus
obtain a prior over variational autoencoder architectures
(Kingma and Welling, 2013; Zhao, Song, and Ermon, 2017),
or, equivalently, a domain-speciﬁc language for deep gener-
ative models.

Syntactically, we have replaced programs sampled from
a grammar with string diagrams sampled from a free cate-
gory. These are conventionally read from the top (leaves of
an abstract-syntax tree) to the bottom (root of the tree). An
example can be seen in Figure 2.

To sample morphisms from the free category, we conduct
a biased random walk from a “source” object i to a “destina-
tion” object j. We begin by transforming the directed multi-

4921962784p(Z49)p(Z2)p(Z196(49×2))p(Z2)p(X784(196×2))The complete generative model For data X ∈ X and
latent variables Z sampled by the parameterized morphism
fθ, itself drawn from the free category of the directed multi-
graph G, the complete generative model is

pθ(X, Z, f, β, W ) = pθ(X | Z, f )pθ(Z | f )

pG(f | β, W )p(β)p(W ),

(4)

where the prior over the inverse temperature β and edge
weights W are Gamma distributions,

β ∼ γ(1, 1),
W ∼ γ(1, 1).

Amortized variational inference in the model We im-
plement a data-driven proposal with neural networks,

qφ(f, β, W | X) = pG(f | β, W )qφ(β | X)qφ(W | X).

We also assume that each primitive morphism f comes
equipped with a trainable stochastic inverse (Stuhlm¨uller,
Taylor, and Goodman, 2013) f †
φ : X → Z (with density
qφ(Z | X; f )), so we can derive a complete joint proposal
density for all latent variables,

qφ(Z, f, β, W | X) = qφ(Z | X; f )qφ(f, β, W | X).

(5)

This proposal can then be made to approximate the true
Bayesian posterior by optimizing the Evidence Lower
Bound (ELBO),

L(θ, φ) = Eq

(cid:20) pθ(X, Z, f, β, W )
qφ(Z, f, β, W | X)

(cid:21)

.

(6)

Implementation and optimization We have imple-
mented this generative model in Pyro (Bingham et al.,
2019) atop the DisCoPy (de Felice, Toumi, and Coecke,
2020) library for applied category theory in Python and the
NetworkX (Hagberg, Schult, and Swart, 2008) library for
graph analysis. NVIL (Mnih and Gregor, 2014) and graph-
based (Schulman et al., 2015) gradient estimation techniques
were used to perform Stochastic Variational Inference (Hoff-
man et al., 2013).

The library implementing the fundamental operations de-
scribed here is called Discopyro and is available online,
along with the code for reproducing our experiments. Please
note that we regard the exponentiated adjacency matrix eA
as a constant for optimization purposes, calculating it once
upon construction of the multigraph without allowing gradi-
ents to ﬂow through it. When we have attempted to use dif-
ferentiable approximate implementations of the matrix ex-
ponential, we have found that the approximate posterior dis-
tributions collapse down to a single preferred morphism per
minibatch of input data due to the extreme effects of small
changes to the real-valued adjacency matrix.

Application: variational autoencoder
architecture search
As an example domain, we consider structured variational
autoencoder architectures for 28 × 28 grayscale image data.
The structures we consider include standard variational

VAE VLAE SPAIR

MNIST

(cid:51)

Fashion MNIST

Omniglot

(cid:51)

(cid:51)

(cid:51)

Table 1: Dataset-architecture combinations

Epochs L (validation)

L (test)

MNIST

Fashion MNIST

Omniglot

500

500

1200

8.36 × 105
4.87 × 105

n/a

n/a

n/a
1.46 × 105

Table 2: Evidence lower bounds on validation and test data
for our three example datasets

autoencoders (Kingma and Welling, 2014) with Continu-
ous Bernoulli likelihoods (Loaiza-Ganem and Cunningham,
2019), variational ladder autoencoders (Zhao, Song, and
Ermon, 2017) with continuous Bernoulli likelihoods, and
the spatially-invariant Attend-Infer-Repeat variant (Craw-
ford and Pineau, 2019) with Gaussian likelihoods. All pro-
posals and priors over vector latent variables are Gaussian.
We list the combinations of network architecture and
dataset tested in Table 1. We include MNIST (LeCun,
Cortes, and Burges, 2010) for a standard of comparison,
Fashion MNIST (Xiao, Rasul, and Vollgraf, 2017) for a
more up-to-date benchmark, and Omniglot (Lake, Salakhut-
dinov, and Tenenbaum, 2015) as a challenge dataset for
learning from few examples per class. The former two
datasets were split 90/10 into training and validation data,
while Omniglot provided a full test-set of handwritten char-
acters not present in the training dataset. All evaluations are
performed on the validation and test sets, respectively. Mini-
batching was used to subsample independent, identically
distributed samples from each class within each dataset,
treating alphabets as classes in Omniglot.

Learning a latent-variable model over 28 × 28 images
amounts to sampling a morphism f : (cid:62) → R784 whose
path-length in the underlying multigraph is at least two
(which we enforce via extra looping and edge-ﬁltering con-
ditions on the code in Listing 1).

Results

Table 2 shows training times and validation losses.

MNIST Figure 3 shows a VAE architecture sampled from
the amortized approximate posterior over the MNIST vali-
dation data. Figure 4 compares the original test images and
their reconstructions. Reconstructions are satisfactory.

Fashion MNIST Figure 5 shows a learned VLAE archi-
tecture sampled from the amortized approximate posterior
over the Fashion MNIST validation data. Figure 6 compares
the original test images and their reconstructions. Recon-
structions capture only high-level shape and texture.

Figure 3: String diagram with vanilla VAE architecture sam-
pled from MNIST’s approximate posterior.

Figure 4: Above: original MNIST digits. Below: Recon-
structions from the learned model.

Figure 5: String diagram with VLAE architecture sampled
from Fashion MNIST’s approximate posterior.

Omniglot Figure 7 shows a learned architecture sampled
from the amortized approximate posterior over the Omniglot
evaluation data. The architecture makes use of a spatial at-
tention mechanism (Jaderberg et al., 2015) to improve learn-
ing and reconstruction, and comes from the same approxi-
mate posterior distribution as the VLAE architecture above.
Figure 9 diagrams the associated dagger morphism. Figure 8
compares the original test images and their reconstructions.
Reconstructions appear to capture the relevant details of
each character with high quality, most likely thanks to spatial
attention being used to encode glimpses of whole character

Figure 6: Above: original Fashion MNIST images. Below:
Reconstructions from the learned model.

components in the inference model.
Discussion

Future work We hope that future work can extend what
we have presented here in several ways. The most clearly
necessary extension, if the intended application is amor-
tized/autoencoding inference, is to cover nonstochastic mor-
phisms, which do not sample their outputs as random vari-
ables. This would amount to the separation of a type system
(in which morphisms map from a source to a destination ob-
ject) from an effect system (in which morphisms add ran-
dom variables to a trace), with a more complex form of the
stochastic planning-graph techniques presented here.

An applied extension of this work would consider would
consider an challenging concept-learning benchmark de-
signed, such as ARC (Chollet, 2019), CURI (Vedantam
et al., 2020), or Bongard-LOGO (Nie et al., 2020). Any
program-learning problem with a well-deﬁned, differen-
tiable likelihood could be considered within our framework,
including those in which intermediate latent variables are
computed deterministically rather than sampled.

This work could also be extended by passing from a
strictly ﬁnite multigraph underlying the free category prior
to an inﬁnite multigraph. This inﬁnite multigraph would in-
clude structure created by (recursively) applying endofunc-
tors (such as universal constructions or polymorphic alge-
braic data types) and natural transformations to the base cat-
egory, and stochastic path-planning would take account of
this structure. This would extend the free category prior to
a Bayesian nonparametric model, and end-to-end training
would then require truncated or Russian Roulette (Xu, Sri-
vastava, and Sutton, 2019) ELBO estimation.

Conclusions We have presented the free category prior, a
neuro-categorical model of compositional program learning
in custom domain-speciﬁc languages. Our generative model
allows for sampling programs which are well-typed, and
therefore correct-by-construction in the sense of assigning
a nonzero, non-unity likelihood to the data. We emphasize
that our generative model does not involve any rejection-
sampling steps, nor other complex deterministic heuristics.
We have applied the free category prior to learning mod-
ular variational autoencoder architectures for three common
datasets, and have shown that the primitives in a free cate-
gory’s domain-speciﬁc language can be parameterized and
learned end-to-end by variational Bayes.

16128784p(Z16)p(Z128|Z16)p(X784|Z128)64212821962784p(Z64)p(Z2)p(Z128(64×2))p(Z2)p(Z196(128×2))p(Z2)p(X784(196×2))Figure 7: String diagram sampled from Omniglot’s approximate posterior.

Crawford, E., and Pineau, J. 2019. Spatially invariant un-
supervised object detection with convolutional neural net-
works. In Proceedings of the AAAI Conference on Artiﬁ-
cial Intelligence, volume 33, 3412–3420.

de Felice, G.; Toumi, A.; and Coecke, B. 2020. DisCoPy:
Monoidal Categories in Python. In Applied Category The-
ory Conference, 1–20.

Ellis, K.; Morales, L.; Sabl´e-meyer, M.; Paris-saclay, E.
N. S.; and Solar-lezama, A. 2018. Library Learning for
Neurally-Guided Bayesian Program Induction. In Neural
Information Processing Systems, 1–11.

Ellis, K.; Wong, C.; Nye, M.; Sable-Meyer, M.; Cary, L.;
Morales, L.; Hewitt, L.; Solar-Lezama, A.; and Tenen-
baum, J. B. 2020. Dreamcoder: Growing generalizable,
interpretable knowledge with wake-sleep bayesian pro-
gram learning. arXiv preprint arXiv:2006.08381.

Frankland, S. M., and Greene, J. D. 2020. Concepts and
Compositionality: In Search of the Brain’s Language of
Thought. Annual Review of Psychology 71(1):273–303.

Goodman, N. D.; Tenenbaum, J. B.; and Gerstenberg, T.
2014. Concepts in a Probabilistic Language of Thought.
In Concepts: New Directions, number 010. 1–25.

Hagberg, A. A.; Schult, D. A.; and Swart, P. J. 2008. Ex-
ploring network structure, dynamics, and function using
networkx.
In Varoquaux, G.; Vaught, T.; and Millman,
J., eds., Proceedings of the 7th Python in Science Confer-
ence, 11 – 15.

Hoffman, M. D.; Blei, D. M.; Wang, C.; and Paisley, J. 2013.
Stochastic variational inference. The Journal of Machine
Learning Research 14(1):1303–1347.

Jaderberg, M.; Simonyan, K.; Zisserman, A.;
2015.

and
Spatial Transformer Net-
In Advances in Neural Information Processing

Kavukcuoglu, K.
works.
Systems, volume 2.

Kingma, D. P., and Welling, M. 2013. Auto-encoding vari-

ational bayes. arXiv preprint arXiv:1312.6114.

Figure 8: Above: original Omniglot characters. Below: Re-
constructions from the learned SPAIR model.

References
Baram, A. B.; Muller, T. H.; Whittington, J. C.; and Behrens,
T. E. 2018. Intuitive planning: global navigation through
cognitive maps based on grid-like codes. bioRxiv 421461.

Behrens, T. E.; Muller, T. H.; Whittington, J. C.; Mark, S.;
Baram, A. B.; Stachenfeld, K. L.; and Kurth-Nelson, Z.
2018. What Is a Cognitive Map? Organizing Knowledge
for Flexible Behavior. Neuron 100(2):490–509.

Bingham, E.; Chen, J. P.; Jankowiak, M.; Obermeyer, F.;
Pradhan, N.; Karaletsos, T.; Singh, R.; Szerlip, P.; Hors-
fall, P.; and Goodman, N. D. 2019. Pyro: Deep univer-
sal probabilistic programming. The Journal of Machine
Learning Research 20(1):973–978.

Blum, A. L., and Furst, M. L. 1997. Fast planning through
Artiﬁcial intelligence 90(1-

planning graph analysis.
2):281–300.

Blum, A. L., and Langford, J. C. 2000. Probabilistic plan-
ning in the graphplan framework. Lecture Notes in Artiﬁ-
cial Intelligence (Subseries of Lecture Notes in Computer
Science) 1809:319–332.

Bramley, N. R.; Rothe, A.; Tenenbaum, J. B.; Xu, F.; and
Gureckis, T. M. 2018. Grounding Compositional Hy-
pothesis Generation in Speciﬁc Instances. Proceedings of
the 41st Annual Meeting of the Cognitive Science Society
1:1392–1397.

Chollet, F. 2019. On the Measure of Intelligence. 1–64.

Kingma, D. P., and Welling, M. 2014. Auto-Encoding Vari-

4921962784492196784492196784492196784p(Z49)p(Z2)p(Z196(49×2))p(Z2)p(X784(196×2))p(Z49)p(Z2)p(Z196(49×2))p(X784Z784×Z196)p(Z49)p(Z2)p(Z196(49×2))p(X784Z784×Z196)p(Z49)p(Z2)p(Z196(49×2))p(X784Z784×Z196)Figure 9: Dagger diagram learned as stochastic inverse of Figure 8.

ational Bayes. In International Conference on Learning
Representations, 1–14.

Lake, B. M., and Piantadosi, S. T. 2019. People infer recur-
sive visual concepts from just a few examples. Computa-
tional Brain & Behavior.

Lake, B. M.; Salakhutdinov, R. R.; and Tenenbaum, J. 2013.
One-shot learning by inverting a compositional causal
process.
In Burges, C. J. C.; Bottou, L.; Welling, M.;
Ghahramani, Z.; and Weinberger, K. Q., eds., Advances
in Neural Information Processing Systems (NIPS), 2526–
2534. Curran Associates, Inc.

Lake, B. M.; Salakhutdinov, R.; and Tenenbaum, J. B. 2015.
Human-level concept learning through probabilistic pro-
gram induction. Science 350(6266):1332–1338.

LeCun, Y.; Cortes, C.; and Burges, C. 2010. Mnist hand-
written digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist 2.

Loaiza-Ganem, G., and Cunningham, J. P. 2019. The con-
tinuous bernoulli: ﬁxing a pervasive error in variational
In Advances in Neural Information Pro-
autoencoders.
cessing Systems, 13287–13297.

Marcus, G. F. 2018. The algebraic mind: Integrating con-

nectionism and cognitive science. MIT press.

Mnih, A., and Gregor, K. 2014. Neural variational inference
and learning in belief networks. In International Confer-
ence on Machine Learning, 1791–1799.

Nie, W.; Yu, Z.; Mao, L.; Patel, A. B.; Zhu, Y.; and Anand-
kumar, A. 2020. Bongard-logo: A new benchmark for
human-level concept learning and reasoning. Advances in
Neural Information Processing Systems 33.

Overlan, M. C.; Jacobs, R. A.; and Piantadosi, S. T. 2017.
Learning abstract visual concepts via probabilistic pro-
gram induction in a Language of Thought. Cognition
168:320–334.

Piantadosi, S. T.; Tenenbaum, J. B.; and Goodman, N. D.
2016. The logical primitives of thought: Empirical foun-
dations for compositional cognitive models. Psychologi-
cal Review 123(4):392–424.

Romano, S.; Salles, A.; Amalric, M.; Dehaene, S.; Sigman,
M.; and Figueira, S. 2018. Bayesian validation of gram-
mar productions for the language of thought. PLoS ONE
13(7):1–20.

Schulman, J.; Heess, N.; Weber, T.; and Abbeel, P. 2015.
Gradient estimation using stochastic computation graphs.
In Advances in Neural Information Processing Systems,
3528–3536.

Solomonoff, R. J. 1964. A formal theory of inductive infer-

ence. Part I. Information and Control 7(1):1–22.
Stuhlm¨uller, A.; Taylor, J.; and Goodman, N. D.

2013.
Learning stochastic inverses. Advances in Neural Infor-
mation Processing Systems 1–9.

Valkov, L.; Chaudhari, D.; Sutton, C.; Srivastava, A.; and
Chaudhuri, S. 2018. Houdini: Lifelong learning as pro-
gram synthesis. In Advances in Neural Information Pro-
cessing Systems, volume 2018-Decem, 8687–8698.

Vedantam, R.; Szlam, A.; Nickel, M.; Morcos, A.; and Lake,
B. 2020. Curi: A benchmark for productive concept learn-
ing under uncertainty. arXiv preprint arXiv:2010.02855.
2000. Minimum descrip-
tion length induction, Bayesianism, and Kolmogorov
IEEE Transactions on Information Theory
complexity.
46(2):446–464.

Vit´anyi, P. M., and Li, M.

Xiao, H.; Rasul, K.; and Vollgraf, R. 2017. Fashion-mnist:
a novel image dataset for benchmarking machine learning
algorithms.

Xu, K.; Srivastava, A.; and Sutton, C. 2019. Variational rus-
sian roulette for deep bayesian nonparametrics. In Inter-
national Conference on Machine Learning, 6963–6972.
Zhao, S.; Song, J.; and Ermon, S. 2017. Learning Hierar-
chical Features from Generative Models. In International
Conference on Machine Learning.

Zhu, Y.; Gao, T.; Fan, L.; Huang, S.; Edmonds, M.; Liu, H.;
Gao, F.; Zhang, C.; and Qi, S. 2019. Dark, Beyond Deep:
A Paradigm Shift to Cognitive AI with Human-like Com-
monsense. Engineering.

7847841964927841964927841964921962492q(Z784×Z196Z784)q((49×2)196)p(Z2)p(Z49)q(Z784×Z196Z784)q((49×2)196)p(Z2)p(Z49)q(Z784×Z196Z784)q((49×2)196)p(Z2)p(Z49)q((196×2)784)p(Z2)q((49×2)196)p(Z2)p(Z49)