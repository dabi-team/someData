8
1
0
2

n
u
J

3
1

]
S
M

.
s
c
[

1
v
1
4
9
4
0
.
6
0
8
1
:
v
i
X
r
a

JMLR: Workshop and Conference Proceedings 1:1–8, 2018

Submitted to ICML 2018 AutoML Workshop

Far-HO: A Bilevel Programming Package for
Hyperparameter Optimization and Meta-Learning

Luca Franceschi
Istituto Italiano di Tecnologia, Genova, Italy & University College London, London, UK

luca.franceschi@iit.it

Riccardo Grazzi
Istituto Italiano di Tecnologia, Genova, Italy

riccardo.grazzi@iit.it

Massimiliano Pontil
Istituto Italiano di Tecnologia, Genova, Italy & University College London, London, UK

m.pontil@ucl.ac.uk

Saverio Salzo
Istituto Italiano di Tecnologia, Genova, Italy

Paolo Frasconi
Universit`a degli studi di Firenze, Firenze, Italy

saverio.salzo@iit.it

paolo.frasconi@unifi.it

Abstract

In (Franceschi et al., 2018) we proposed a uniﬁed mathematical framework, grounded
on bilevel programming, that encompasses gradient-based hyperparameter optimization
and meta-learning. We formulated an approximate version of the problem where the inner
objective is solved iteratively, and gave suﬃcient conditions ensuring convergence to the
exact problem. In this work we show how to optimize learning rates, automatically weight
the loss of single examples and learn hyper-representations with Far-HO, a software pack-
age based on the popular deep learning framework TensorFlow that allows to seamlessly
tackle both HO and ML problems.1
Keywords: Machine Learning; Hyperparameter Optimization; Meta-Learning; Bilevel
Programming; Optimization; Deep Learning

1. Introduction

While in the standard supervised learning problems we seek the best hypothesis in a given
space and with a given learning algorithm, in hyperparameter optimization (HO) and meta-
learning (ML) we seek a conﬁguration so that the optimized learning algorithm will produce
a model that generalizes well to new data. The search space in ML often incorporates choices
associated with the hypothesis space and the features of the learning algorithm itself (e.g.,
how optimization of the training loss is performed). Under this common perspective, both
HO and ML essentially boil down to nesting two search problems: at the inner level we seek
a good hypothesis as in standard supervised learning while at the outer level we seek a good
conﬁguration (including a good hypothesis space) where the inner search takes place.

1. This submission is a reduced version of Franceschi et al. (2018) which has been accepted at the main
ICML 2018 conference. In this paper we illustrate the software framework, material that could not be
included in the conference paper.

c(cid:13) 2018 L. Franceschi, R. Grazzi, M. Pontil, S. Salzo & P. Frasconi.

 
 
 
 
 
 
Franceschi Grazzi Pontil Salzo Frasconi

HO and ML only diﬀer substantially in terms of the experimental settings in which
they are evaluated. While in HO the available data is associated with a single task and
split2 into a training set, (used to tune the parameters) and a validation set, (used to
tune the hyperparameters) in ML we are often interested in the so-called few-shot learning
setting where data comes in the form of short episodes sampled from a common probability
distribution over supervised tasks. Algorithmic techniques for solving HO and ML can
also diﬀer substantially. Classic approaches to HO, (see e.g. Hutter et al., 2015, and
references therein) are only capable of handling up to a few hundred hyperparameters.
Recent gradient-based techniques for HO, however, have signiﬁcantly increased the number
of hyperparameters that can be optimized (Domke, 2012; Maclaurin et al., 2015; Pedregosa,
2016; Franceschi et al., 2017) making it possible to handle more hyperparameters than
parameters.

As shown in (Franceschi et al., 2018), HO and ML can be uniﬁed within the natural
mathematical framework of bilevel programming, where an outer optimization problem is
solved subject to the optimality of an inner optimization problem. The variables of the
outer objective are either the hyperparameters of a supervised learning problem in HO or
the parameters of a meta-learner in ML. In HO the inner problem is usually the minimization
of an empirical loss, while in ML it could concern classiﬁers for individual tasks. Table 1
outlines the links among bilevel programming, HO and ML.

Bilevel programming (Bard, 2013) has been suggested before in machine learning (Keerthi
et al., 2007; Kunapuli et al., 2008; Flamary et al., 2014; Pedregosa, 2016), but never in the
context of ML. The resulting framework, outlined in Section 2, encompasses some existing
approaches to ML. A technical diﬃculty arises when the solution to the inner problem can-
not be written analytically and one needs to resort to iterative optimization approaches.
We outline this approach in Section 2.3 and we brieﬂy discuss conditions that guarantee
good approximation properties.

We developed a software package. Far-HO, based on the popular deep learning frame-
work TensorFlow (Abadi et al., 2015) to facilitate the formalization and the solution of
problems arising in HO and ML in a uniﬁed manner. We present an overview of the pack-
age and showcase two possible applications in Section 3.

2. A bilevel optimization framework

We consider bilevel optimization problems (see e.g. Colson et al., 2007) of the form

min{f (λ) : λ ∈ Λ},

where

f (λ) = inf{E(w, λ) : w ∈ arg min
u∈Rd

Lλ(u)}.

(1)

Speciﬁc instances of this problem include HO and ML, which we discuss next. We call
E : Rd × Λ → R the outer objective, interpreted as a meta-train or validation error and,
for every λ ∈ Λ, we call Lλ : Rd → R the inner objective. {Lλ : λ ∈ Λ} is regarded
as a class of objective functions parameterized by λ, wherein each single function may
represent a training error, possibly averaged multiple tasks. The cartoon in Figure 1 depicts
a stereotypical scenario.

2. Data could also be split multiple times, following a cross-validation scheme.

2

Far-HO

Table 1: Links and naming conventions
among diﬀerent ﬁelds

Bilevel
programming

Hyperparameter
optimization

Meta-learning

Inner variables

Parameters

Outer variables Hyperparameters

Inner objective

Training error

Outer objective

Validation error

Ground models’
parameters

Meta-learner’s
parameters

tasks
training errors

Meta-training
error

Figure 1: Blue

lines

(average)
represent
training errors, varying w. The
blue dots are the corresponding
(inner) minimizers. The black line
represents the outer objective E,
whose minimizer is the red dot.

2.1. Hyperparameter Optimization

In the context of hyperparameter optimization, we are interested in minimizing the val-
idation error of a model gw : X → Y parameterized by a vector w, with respect to a
vector of real-valued hyperparameters λ. For example, we may consider representation or
regularization hyperparameters that control the hypothesis space or penalties, respectively.
In this setting, a prototypical choice for the inner objective is the regularized empirical

error

Lλ(w) =

(cid:88)

(cid:96)(gw(x), y) + Ωλ(w)

(x,y)∈Dtr

(2)

where Dtr = {(xi, yi)}n
Ωλ a regularizer parameterized by λ.

i=1 is a set of input/output points, (cid:96) is a prescribed loss function, and

The outer objective represents a proxy for the generalization error of gw, and it is given

by the average loss on a validation set Dval

E(w, λ) =

(cid:88)

(cid:96)(gw(x), y).

(x,y)∈Dval

(3)

Note that the outer objective E does not depend explicitly on the hyperparameters λ, since
in HO λ is instrumental in ﬁnding a good model gw, which is our ﬁnal goal.

2.2. Meta-Learning

In meta-learning (ML) the inner and outer objectives are computed by respectively summing
and averaging the training and the validation error of multiple tasks. The goal is to produce
a learning algorithm that will work well on novel tasks3.

3. This is also related to multitask learning, except in ML the goal is to extrapolate to novel tasks.

3

wErrorTrain errorsValidation errorFranceschi Grazzi Pontil Salzo Frasconi

i , yj

i )}nj

i=1 with (xj

For this purpose, we have available a meta-training set D = {Dj = Dj

j=1,
which is a collection of datasets, sampled from a meta-distribution P. Each dataset Dj =
{(xj
i ) ∈ X × Y j is linked to a speciﬁc task. Note that the output space
is task dependent (e.g. a multi-class classiﬁcation problem with variable number of classes).
The model for each task is a function gwj ,λ : X → Y j, identiﬁed by a parameter vectors wj
and hyperparameters λ. A key point here is that λ is shared between the tasks. With this
notation the inner and outer objectives are

tr ∪ Dj

val}N

i , yj

Lλ(w) = (cid:80)N

j=1 L(wj, λ, Dj

tr), and E(w, λ) =

1
N

(cid:80)N

j=1 L(wj, λ, Dj

val)

(4)

respectively, where L(w, λ, S) is the empirical loss of the pair (w, λ) on a set of examples S.
Particular examples are choosing the model gw,λ = (cid:104)w, hλ(x)(cid:105), in which case λ parameterizes
a feature mapping, or consider gwj ,λ(x) = (cid:104)w + λ, x(cid:105), in which case λ represents a common
model around which task speciﬁc models are to be found.

Note that the inner and outer losses for task j use diﬀerent train/validation splits of the
corresponding dataset Dj. Unlike in HO, in ML the ﬁnal goal is to ﬁnd a good λ and the
wj are now instrumental.

2.3. Gradient-Based Approach

We now discuss a general approach to solve Problem (1). In general there is no closed form
expression wλ, so it is not possible to directly optimize the outer objective function. A
compelling approach is to replace the inner problem with a dynamical system, as discussed
in (Domke, 2012; Maclaurin et al., 2015; Franceschi et al., 2017).

Speciﬁcally, we let [T ] = {1, . . . , T } where T is a prescribed positive integer and consider

the following approximation of Problem (1)

min
λ

fT (λ) = E(wT,λ, λ),

s.t.

w0,λ = Φ0(λ), wt,λ = Φt(wt−1,λ, λ), t ∈ [T ],

(5)

with Φ0 : Rm → Rd a smooth initialization mapping and, for every t ∈ [T ], Φt : Rd ×
Rm → Rd a smooth mapping that represents the operation performed by the t-th step of
an optimization algorithm such as gradient descent4, i.e. Φt(wt) = wt − ηt∇Lλ(·).

The approximation of the bilevel problem (1) by Procedure (5) raises the issue of the
quality of this approximation: fT may, in general, be unrelated to f . Among the possible
issues, we note that, for a chosen T , wT,λ may not even be an approximate minimizer of
Lλ, or, in the presence of multiple minimizers, the optimization dynamics may lead to a
minimizer which not necessarily achieves the inﬁmum of E. The situation is, however,
diﬀerent if the inner problem admits a unique minimizer for every λ ∈ Λ (e.g. when Lλ is
strongly convex). In this case, it is possible to show, under some regularity assumptions,
that the set of minimizers of fT converges to that of f for T → ∞ (Franceschi et al., 2018)
and that the gradient of fT converges to ∇f .

On the other hand, Procedure (5) also suggests to consider the inner dynamics as a
form of approximate empirical error minimization which is valid in its own right. From
this perspective it is possible (and indeed natural) to include among the components of

4. Other algorithms such as Adam requires auxiliary variables that need to be included in w.

4

Far-HO

λ variables associated with the optimization algorithm itself. For example, in (de Freitas,
2016; Wichrowska et al., 2017) the mapping Φt is implemented as a recurrent neural network,
while (Finn et al., 2017) focus on the initialization mapping by letting Φ0(λ) = λ.

A major advantage of the reformulation above is that it makes it possible to compute
eﬃciently the gradient of fT , called hypergradient, either in time or in memory (Maclaurin
et al., 2015; Franceschi et al., 2017), by making use of Reverse or Forward mode algorithmic
diﬀerentiation (Griewank and Walther, 2008). This makes it feasible to eﬃciently search
for a good conﬁguration in a high dimensional hyperparameter space – reverse mode having
a complexity in time independent form the size of λ.

3. Far-HO: A Gradient-Based Hyperparameter Optimization Package

We developed a software package in Python with the aim to facilitate the formalization,
implementation and numerical solution of HO and ML problems with continuous hyper-
parameters (e.g. ridge regression, logistic regression, deep neural networks, ...). Far-HO,
available at https://github.com/lucfra/FAR-HO, is based on the popular library Ten-
sorFlow (Abadi et al., 2015); by leveraging the computational power of modern GPUs,
it allows to scale up gradient-based hyperparameter optimization techniques to high di-
mensional problems (many parameters and/or hyperparameters). Notably, it implements
dynamic forward and reverse mode iterative diﬀerentiation5 for Procedure 5 and warm
restart strategies for the optimization of the inner problem and the computation of the hy-
pergradient. The package exposes utility functions to declare hyperparameters, instantiate
commonly used ﬁrst-order optimization dynamics such as gradient descent or Adam and
features single-line calls to set up approximate bilevel problems. Two practical examples are
illustrated in the remainder of this section. Experimental results obtained with Far-HO
are reported in (Franceschi et al., 2018).

Figure 2: Sample HO procedure with Far-HO.

5.

It is not required to “unroll” the computational graph of the optimization dynamics for T steps. Un-
rolling quickly becomes impractical as T grows.

5

Franceschi Grazzi Pontil Salzo Frasconi

Hyperparameter Optimization: Weighting The Examples’ Losses

The illustrative example in Figure 2 shows how to optimize hyperparameters that weigh
the contribution of each training example to the total loss. This setting is useful when part
of the training data is corrupted as in the data hyper-cleaning experiments in (Franceschi
et al., 2017). In this example, the inner objective is Lλ(w) = (cid:80)
λi(cid:96)(gw(xi), yi) We
treat also the learning rate as an hyperparameter (Lines 7 and 13). As illustrated in Line 15,
hyperparameters can be “declared” using the get hyperparameter method, which mimics
TensorFlow’s get variable, and can be placed anywhere in a computational graph. The
method minimize (closely related to minimize in TensorFlow) accepts two scalar tensors
(E and L, deﬁned in Lines 10-11) for the outer and the inner problem, respectively, and
two associated optimizers, outer opt and inner opt (one of which is directly taken from
TensorFlow in this example).

(xi,yi)∈Dtr

Figure 3: Sample ML procedure with Far-HO.

Meta-Learning: Hyper-Representation Networks

Our approach (Franceschi et al., 2018) to meta-learning consists in partitioning the model
into a cross-task intermediate hyper-representation mapping hλ : X → Rk (parametrized
by a vector λ) and task speciﬁc models gj : Rk → Y j (parametrized by vectors wj). The
ﬁnal ground model for task j is thus given by gj ◦ h; λ and wj are learned by respectively
optimizing the outer and the inner objective in (4). This method is inspired by (Baxter,
1995; Caruana, 1998) which instead jointly optimize both hyper-representation and task
speciﬁc weights.

We instantiate the approximation scheme in Problem (5) in which the weights of the
task-speciﬁc models can be learned by T iterations of gradient descent, resulting in the
val) with wj
problem: min
tr), t, j ∈
[T ], [N ]. Since, in general, the number of episodes in a meta-training set is large, we
compute a stochastic approximation of the gradient of fT by sampling a mini-batch of
episodes. Illustrative code for this example is presented in Figure 3.

t−1 − η∇wLj(wj

fT (λ) = (cid:80)N

j=1 Lj(wj

t−1, λ, Dj

T , λ, Dj

t = wj

λ

6

Far-HO

References

Mart´ın Abadi, Ashish Agarwal, and et. al. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available
from tensorﬂow.org.

Jonathan F. Bard. Practical bilevel optimization: algorithms and applications, volume 30.

Springer Science & Business Media, 2013. 01251.

Jonathan Baxter. Learning internal representations.

In Proceedings of the 8th Annual

Conference on Computational Learning Theory (COLT), pages 311–320. ACM, 1995.

Rich Caruana. Multitask learning.

In Learning to learn, pages 95–133. Springer, 1998.

02683.

Benoˆıt Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization.

Annals of operations research, 153(1):235–256, 2007.

Nando de Freitas. Learning to Learn and Compositionality with Deep Recurrent Neural
Networks: Learning to Learn and Compositionality. In Proceedings of the 22Nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.

Justin Domke. Generic Methods for Optimization-Based Modeling. In AISTATS, volume 22,
pages 318–326, 2012. URL http://www.jmlr.org/proceedings/papers/v22/domke12/
domke12.pdf.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks.
In Proceedings of the 34th International Conference on
Machine Learning, (ICML), pages 1126–1135, 2017. URL http://proceedings.mlr.
press/v70/finn17a.html.

R´emi Flamary, Alain Rakotomamonjy, and Gilles Gasso. Learning constrained task simi-
larities in graph-regularized multi-task learning. Regularization, Optimization, Kernels,
and Support Vector Machines, page 103, 2014.

Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and
In Proceedings of the 34th In-
reverse gradient-based hyperparameter optimization.
ternational Conference on Machine Learning, (ICML), pages 1165–1173, 2017. URL
http://proceedings.mlr.press/v70/franceschi17a.html.

Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In Proceedings
of The 35rd International Conference on Machine Learning (ICML), 2018.

Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques

of algorithmic diﬀerentiation. SIAM, 2008.

Frank Hutter, Jrg Lcke, and Lars Schmidt-Thieme. Beyond Manual Tuning of Hyper-
parameters. KI - K¨unstliche Intelligenz, 29(4):329–337, November 2015.
ISSN 0933-
1875, 1610-1987. doi: 10.1007/s13218-015-0381-0. URL http://link.springer.com/
10.1007/s13218-015-0381-0.

7

Franceschi Grazzi Pontil Salzo Frasconi

S Sathiya Keerthi, Vikas Sindhwani, and Olivier Chapelle. An eﬃcient method for gradient-
based adaptation of hyperparameters in svm models. In Advances in Neural Information
Processing Systems (NIPS), pages 673–680, 2007.

G. Kunapuli, K.P. Bennett, Jing Hu, and Jong-Shi Pang. Classiﬁcation model selection
via bilevel programming. Optimization Methods and Software, 23(4):475–489, August
2008. ISSN 1055-6788, 1029-4937. doi: 10.1080/10556780802102586. URL http://www.
tandfonline.com/doi/abs/10.1080/10556780802102586.

Dougal Maclaurin, David K. Duvenaud, and Ryan P. Adams. Gradient-based hyperparam-
eter optimization through reversible learning. In Proceedings of the 32nd International
Conference on Machine Learning, (ICML, pages 2113–2122, 2015.

Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings
of The 33rd International Conference on Machine Learning (ICML), pages 737–746, 2016.
URL http://proceedings.mlr.press/v48/pedregosa16.html.

Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoﬀman, Sergio G´omez Col-
menarejo, Misha Denil, Nando Freitas, and Jascha Sohl-Dickstein. Learned optimizers
that scale and generalize. In Proceedings of the 34th International Conference on Machine
Learning (ICML), pages 3751–3760, 2017.

8

