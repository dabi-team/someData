RLang: A Declarative Language for Expressing Prior Knowledge for
Reinforcement Learning

Rafael Rodriguez-Sanchez, Benjamin A. Spiegel, Jennifer Wang,
Roma Patel, Stefanie Tellex, George Konidaris
Department of Computer Science, Brown University, Providence, RI 02912 USA
{rrs, jennifer wang2, romapatel}@brown.edu, {bspiegel, steﬁe10, gdk}@cs.brown.edu

2
2
0
2

g
u
A
6
1

]
I

A
.
s
c
[

2
v
8
4
4
6
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Communicating useful background knowledge to reinforce-
ment learning (RL) agents is an important and effective
method for accelerating learning. We introduce RLang, a
domain-speciﬁc language (DSL) for communicating domain
knowledge to an RL agent. Unlike other existing DSLs pro-
posed by the RL community that ground to single elements
of a decision-making formalism (e.g., the reward function or
policy function), RLang can specify information about ev-
ery element of a Markov decision process. We deﬁne pre-
cise syntax and grounding semantics for RLang, and provide
a parser implementation that grounds RLang programs to an
algorithm-agnostic partial world model and policy that can
be exploited by an RL agent. We provide a series of example
RLang programs, and demonstrate how different RL meth-
ods can exploit the resulting knowledge, including model-free
and model-based tabular algorithms, hierarchical approaches,
and deep RL algorithms (including both policy gradient and
value-based methods).

1

Introduction

Reinforcement learning (RL) algorithms have seen impor-
tant successes such as agents that learn to play the Atari
games from pixels (Mnih et al. 2013) and learning to play
Go, chess and shogi (Silver et al. 2017, 2018; Schrittwieser
et al. 2020); demonstrating the capabilities of deep RL meth-
ods. However, they require a great amount of experience
and, thus, learning in such tabula rasa setting quickly be-
comes impractical. Moreover, to develop generalist learn-
ing agents, we cannot expect them to learn every task they
will face without instruction and task-speciﬁc knowledge. In
fact, it is reasonable to assume that we would communicate
to the agent useful task-speciﬁc knowledge, e.g, the rules of
a game, relevant permitted actions and features of the obser-
vation such as relevant observed game pieces, before they
begin learning a policy. This information will be oftentimes
partial and the agent will need to improve and complete its
knowledge in the usual RL loop, but the given prior informa-
tion should nonetheless improve the learning performance.
Languages, both formal and natural, have been used in
various ways to add prior knowledge into decision-making
(Luketina et al. 2019). Formal languages beneﬁt from unam-
biguous syntax and semantics, and can therefore be reliably
used to represent knowledge. These have proven useful in

specifying advice to agents in the form of hints about actions
(Maclin and Shavlik 1996) or policy structure (Andreas,
Klein, and Levine 2017). Communicating such knowledge
using natural language would be more intuitive, though this
approach would require converting natural language sen-
tences into grounded knowledge usable by the agent; most of
the approaches in this area restrict the possible grounding by
translating natural language into expressions of a restricted
grammar. For example, for describing task objectives (Artzi
and Zettlemoyer 2013; Patel, Pavlick, and Tellex 2020), or
other individual components of decision-making systems
such as rewards (Goyal, Niekum, and Mooney 2019; Sumers
et al. 2021) and policies (Branavan, Zettlemoyer, and Barzi-
lay 2010). All of the above approaches provide information
about a single component of a chosen decision-making for-
malism; there exists no uniﬁed framework able to express
information about all the components of a task.

We therefore introduce RLang, a domain-speciﬁc lan-
guage (DSL) with precise syntax and semantics to express
information about every component of a Markov decision
process (MDP), including ﬂat and hierarchical policies, state
factors and features, transition functions, and reward func-
tions. Moreover, we release the RLang Parser1 that inter-
prets RLang programs and produces grounded partial mod-
els in Python to be used by any learning algorithm. We then
demonstrate RLang’s versatility through a series of example
programs that express different types of domain knowledge
and how such knowledge improves learning performance.

2 Background
Domain-Speciﬁc Languages Domain-speciﬁc languages
(DSLs) are formal languages designed to specify infor-
mation relevant to a target domain. Compared to general-
purpose programming languages like Python (Van Rossum
and Drake Jr 1995) and C (Kernigham and Ritchie 1973),
DSLs typically contain a smaller set of narrower semantics
that are well-suited to a speciﬁc application. That is, DSLs
sacriﬁce computational expressivity for ease-of-use within a
particular domain. Commonly-used DSLs include the Stan-
dard Query Language (SQL) used for querying relational
databases and the Planning Domain Deﬁnition Language

1RLang source code, documentation, and examples are avail-

able at rlang.ai.

 
 
 
 
 
 
Figure 1: RLang provides users with a precise language to provide domain knowledge to RL agents. RLang programs are parsed
by the interpreter to create a partial model of the MDP (Dynamics and Task Knowledge) and its solution (Solution
Knowledge). Finally, the knowledge objects inform an RL agent that can leverage the grounded information during its learning
loop.

(PDDL; Ghallab et al. 1998) for deﬁning planning tasks.

Decision-Making Formalisms Reinforcement learning
tasks are typically modeled as Markov Decision Processes
(MDPs; Puterman 1990), which are deﬁned by a tuple
(S, A, R, T, γ), where S is a set of states, A is a set of ac-
tions, T : S × A × S → [0, 1] is a transition probability
distribution, R : S × A × S → R is a reward function,
and γ ∈ (0, 1] is a discount factor. A solution to an MDP is
a policy π : S × A → [0, 1] that maximizes the expected
discounted return E [(cid:80)∞
t=0 γtrt], where rt is the reward ob-
tained at time step t. The value function V π : S → R for
a policy π captures the expected return an agent would re-
ceive from executing π starting in a state s. The action-value
function Qπ : S × A → R of a policy is the expected return
from executing an action a in a state s and following policy
π thereafter.

Hierarchical Decision-Making Solving MDPs with
high-dimensional state and action spaces can be difﬁcult, es-
pecially in domains where long sequences of actions are re-
quired to achieve a goal. In these environments, hierarchical
reinforcement learning (Barto and Mahadevan 2003) may be
more applicable, as temporally-extended actions can reduce
the complexity of the space of solution policies. The options
framework (Sutton, Precup, and Singh 1999) formalizes this
notion by modeling high-level actions as options: closed-
loop policies deﬁned by a tuple (I, π, β), where I ⊆ S is
a set of states in which the option can be executed, π is an
option policy, and β : S → [0, 1] describes the probability
that the option will terminate upon reaching a given state.
Let O be the set of options the agent can execute, then the
MDP tuple is extended to (S, A ∪ O, R, T, γ) in the hierar-
chical setting.

3 RLang: Expressing Prior Knowledge about

Reinforcement Learning Tasks

If RL is to become widely used in practice, we must reduce
the infeasible amount of trial-and-error required to learn to
solve a task from scratch. One promising approach is to
avoid tabula rasa learning by including the sort of back-
ground knowledge that humans typically bring to a new
task. Such background knowledge is often easy to obtain—
in many cases, it is simply obvious to anyone: try not to fall
off cliffs!—and need not be perfect or complete in order to
be useful.

Unfortunately, however, there is no standardized approach
to communicating such background knowledge to an RL
agent. In most cases, the same person who implements the
learning algorithm also hand-codes the background knowl-
edge, typically in the same general-purpose programming
language in which the algorithm is implemented, typically
in an ad-hoc fashion. This has two primary drawbacks. First,
prior knowledge is often task-speciﬁc, and the lack of a
medium to express it hinders the development of general-
purpose learning algorithms that can exploit varying types
and degrees of background knowledge. Second, this ap-
proach is not accessible to end-users or other consumers
of RL agents, who do not write the algorithms themselves
and cannot necessarily be expected to master the relevant
programming languages and mathematical details, but who
might nevertheless wish to accelerate learning.

The alternative is to design a standardized, human-
interpretable DSL for expressing prior knowledge about re-
inforcement learning tasks. Such a DSL should have two im-
portant properties, which are not present in existing DSLs
(Maclin and Shavlik 1996; Denil et al. 2017; Sun, Wu, and
Lim 2020). First, it should be agnostic of the learning algo-
rithm used. Separating the question of how to express prior
knowledge from how that knowledge is exploited by a learn-
ing algorithm introduces a standardized interface that can be
used to inform a wide variety of RL agents, even ones based
on algorithms that have not yet been developed. Second,
it should be complete: able to express all the information
that could possibly be informative about a particular task.
We therefore propose RLang, a new DSL designed to fulﬁll
these criteria.

RLang can be used to prescribe features of the state space
(using Features and Propositions), specify one or
more goal states (using Goals), deﬁne abstract actions (us-
ing Options), describe solution policies and hierarchical
policy structure (using Policies), restrict the action space
(using ActionRestrictions), provide partial models
of the world (using Effects, which ground to reward
functions and transition functions), and shape reward (also
using Effects). RLang programs can be parsed using the
RLang Parser2 into an algorithm-agnostic data structure (see

2Our landing page, which includes installation instructions, can

be found at rlang.ai.

 Effect:   if at_workbench_1 and A == use:if wood >= 1:  stick’ -> stick + wood  wood’ -> 0Factor inventory := S[250:270]Feature wood := inventory[0]minecraft.rlangFeature gold := inventory[1]InterpreterDynamics & TaskKnowledgeInformed RL AgentSolution KnowledgeRL loopTable 1: RLang declarations for corresponding MDP elements. The ﬁrst column shows a component of the MDP, the second
shows an RLang expression that can inform it, while the last column contains a description of the expression.

MDP Component

RLang Declaration

Natural Language Interpretation

State Feature
φ : S → Rn

Proposition
σ : S → {(cid:62), ⊥}

Policy
π : S × A → [0, 1]

Option
(σI , π, σβ)

Reward and Transition Function
(Re : S × A × S → R,
Te : S × A × S → [0, 1])

Feature inventory value :=
5 * gold + 2 * iron

Proposition at workbench :=
position in workbench locations

Policy build bridge:
if at workbench:
Execute use

Option build axe:

init(wood >= 1 and iron >= 1)
Execute build axe policy

until(axe >= 1)

Effect resource consumption:
if wood >= 1 and A == use:
stick’ -> stick + wood
wood’ -> 0
Reward wood

The value of your inventory is 5
for each gold you have plus 2 for
each iron.
You are at a workbench if your
position is one of the workbench
locations.

If you are at a workbench, craft
using it.

When you have at least one wood
and one iron, you can build axes
until you have at least one.

Crafting will convert your wood
into sticks. You will also be
rewarded 1 for every wood you
have.

Section 3.2) that can be integrated into nearly any rein-
forcement learning algorithm. In this section, we present the
main RLang elements and their syntax. As we will show,
RLang’s syntax is inspired by Python to ensure readability
and ease-of-use, and a full formal speciﬁcation of the syntax
in Backus-Naur Form (BNF) and the grounding semantics
implemented by the RLang Parser are in Appendix A.

3.1 RLang Elements
An RLang program consists of a set of declarations, where
each one grounds to one or more components of an
(S, A, O, R, T, π) tuple. More speciﬁcally, every RLang El-
ement grounds to a function with a domain in S × A × S
and a co-domain in S, A, Rn where n ∈ N, or {(cid:62), ⊥}. We
describe the main RLang element types in the rest of this
section and summarize them in Table 1.

State Factors
In Factored MDPs (Boutilier, Dearden, and
Goldszmidt 2000), the state space is a collection of condi-
tionally independent variables: S = X1 × .. × Xn. Some
learning algorithms might ﬁnd it useful to reference these
variables individually. For example, consider a 2-D version
of Minecraft, as represented in Figure 2, where an agent has
to collect ingredients to craft new tools and objects. In this
environment the state is the concatenation of a position vec-
tor, a ﬂattened map representation, and an inventory vector:
s = (pos, map, inventory). Factors can be used to refer-
ence these independent state variables:
F a c t o r p o s i t i o n : = S [ 0 : 2 ]
F a c t o r map : = S [ 2 : 2 5 0 ]
F a c t o r i n v e n t o r y : = S [ 2 5 0 : 2 7 0 ]
S is a reserved keyword referring to the current state. A
and S’ are also keywords which refer to the current action
and the next state, respectively. Factors can be further sliced
and indexed:

F a c t o r i r o n : = i n v e n t o r y [ 0 ]

F a c t o r wood : = i n v e n t o r y [ 1 ]

State Features RLang can also be used to deﬁne more
complex functions of state. For instance, if the agent’s goal
is to build axes, we can deﬁne a Feature that captures the
number of axes that can be potentially built at the current
state:

F e a t u r e n u m b e r o f a x e s

: = wood + i r o n

Propositions Propositions in RLang, which are
functions of the form S → {(cid:62), ⊥}, identify states that share
relevant characteristics:

C o n s t a n t w o r k b e n c h l o c a t i o n s

: = [ [ 1 ,

0 ] ,

[ 1 , 3 ] ]

P r o p o s i t i o n a t w o r k b e n c h : = p o s i t i o n

i n w o r k b e n c h l o c a t i o n s

P r o p o s i t i o n h a v e b r i d g e m a t e r i a l
i r o n >= 1 and wood >= 1

: =

Goals Goals can be used to specify goal states given by
a proposition. For example, Goal get gold := gold
>= 1 encodes that the agent must collect at least one gold
unit.

Markov Features Markov Functions like the action-value
function or transition function take the form S × A × S →
R. We extend the co-domain of this function class to Rn,
where n ∈ N, and introduce Markov Features, which
allow users to compute features on an (s, a, s(cid:48)) experience
tuple. The following Markov Feature represents a change in
inventory elements.

Markov F e a t u r e i n v e n t o r y c h a n g e : =

i n v e n t o r y ’ − i n v e n t o r y

The prime (’) operator references the value of an RLang

name when evaluated on the next state.

Policies Policy functions can also be speciﬁed in RLang
using conditional expressions:

P o l i c y main :

i f

i r o n >= 2 :

i f

a t w o r k b e n c h :

E x e c u t e Use # T h i s

i s an a c t i o n

e l s e :

E x e c u t e g o t o w o r k b e n c h # T h i s

i s a p o l i c y

e l s e :

E x e c u t e

c o l l e c t

i r o n

The Execute keyword can be used to execute an action
or call another policy. The above policy instructs the agent
to craft iron tools at a workbench by ﬁrst collecting iron and
then navigating to the workbench. Policies can also be prob-
abilistic:

P o l i c y random move :

E x e c u t e up w i t h P ( 0 . 2 5 )
or E x e c u t e down w i t h P ( 0 . 2 5 )
l e f t w i t h P ( 0 . 2 5 )
or E x e c u t e
r i g h t w i t h P ( 0 . 2 5 )
or E x e c u t e

Users can specify multiple policy functions in an
RLang program and can designate a primary policy by nam-
ing it main.

Options Temporally-extended actions can be speciﬁed us-
ing Options, which include initiation and termination
propositions:

Option b u i l d b r i d g e :

i n i t h a v e b r i d g e m a t e r i a l and

a t w o r k b e n c h
E x e c u t e

c r a f t b r i d g e

u n t i l b r i d g e i n i n v e n t o r y

Action Restrictions Restrictions to the set of possible ac-
tions an agent can take in a given circumstance can be spec-
iﬁed using ActionRestrictions:

A c t i o n R e s t r i c t i o n d o n t g e t b u r n e d :

i f

( p o s i t i o n + [ 0 , 1 ] )

i n

l a v a l o c a t i o n s :
R e s t r i c t up

Effects Effects provide an interface for specifying par-
tial information about the transition and reward functions.
When using a factored MDP, RLang can also be used to
specify factored transition functions (i.e., transition func-
tions for individual factors):

E f f e c t m o v e m e n t e f f e c t :

i f x p o s i t i o n >= 1 and A == l e f t :
x p o s i t i o n ’ −> x p o s i t i o n − 1

Reward −0.1

The above Effect captures the predicted consequence of
moving left on the x position factor, stating that the x
position of the agent in the next state will be 1 less than in the
current state. This Effect also speciﬁes a −0.1 step penalty
regardless of the current state or action. In simpler MDPs,
predictions can be made about the whole state vector:

E f f e c t
i f

t i c t a c t o e :
t h r e e i n a r o w :

S ’ −> e m p t y b o a r d # Board i s

r e s e t

Effects can reference previously deﬁned effects using

similar syntax:

E f f e c t main :

−> m o v e m e n t e f f e c t
−> c r a f t i n g e f f e c t

A main Effect designates the primary environment dy-
namics, and grounds to a partial factored world model
(T , R). Similar to policies, Effects can be made probabilis-
tic using with.

Finally, it is important to note that RLang, as we have seen
across these examples, does not require the speciﬁcation of
Effects and Policies to be complete (i.e., known for
every element transition (s, a, s(cid:48)) or state-action pair (s, a),
respectively). Therefore, a user is not constrained to provide
extensive and complex programs to fully specify the MDP—
although this is a possibility with RLang—in order to accel-
erate learning. Therefore, RL agents are meant to learn to
ﬁll the missing pieces and improve over the provided knowl-
edge.

3.2 Accessing Parsed RLang Knowledge
Using the RLang’s Python API, users can parse RLang pro-
grams into the following queryable knowledge Python3
objects, which can be integrated directly into a learning al-
gorithm: (1) the Dynamics and Task Knowledge object
contains a queryable model of the environment and the task
(i.e., transition dynamics T and reward function R) that are
derived from the Effect main declaration and the col-
lection of deﬁned goals; (2) the Solution Knowledge ob-
ject that contains information about the collection of newly
deﬁned options O and the main policy π. Moreover, these
knowledge objects are implemented as partial functions and,
hence, when querying for a element of the domain where
the RLang programs provides no knowledge, it returns an
unknown ﬂag.

3.3 Specifying Complex Groundings with a

Vocabulary File

RLang comes built-in with a set of simple arithmetic,
Boolean, and set operations in addition to if,elif,else
conditional statements which can be used in various RLang
object declarations. However, we recognize that users may
want to include more complex grounding functions in their
RLang programs. For instance, when dealing with prob-
lems with high-dimensional observation spaces (e.g. pixel
frames), we might require to provide groundings that ab-
stract away low-level details of the problem. To accommo-
date these needs, we have made it possible to deﬁne RLang
objects using our RLang’s Python API which can be im-
ported and referenced in an RLang program. By specify-
ing a vocabulary ﬁle (in JSON format) and a corresponding
grounding ﬁle (a Python ﬁle deﬁning RLang objects), users
can construct RLang objects using the full features of Python
and reference them directly in RLang programs. This allows
users to provide complex expert groundings or, more gen-
erally, learned groundings that hold the necessary semantic
information to derive new grounded knowledge easily with
RLang programs.

4 Demonstrations
In this section, we demonstrate RLang use-cases, focusing
on examples that show how different types of prior infor-
mation that can be concisely and easily expressed, for vary-
ing degrees of environment complexity and different fami-

3We chose Python as the implementation language to ensure
compatibility with widely used libraries and implementation of RL
algorithms

Figure 2: Lava-Gap environment (left) and 2D Minecraft
(right)

lies of RL methods. We therefore provide examples of infor-
mation about policy hierarchical structure, policy priors and
transition dynamics, and explore how this information can
be exploited with RL methods suitable to the type of infor-
mation provided. 4 We design simple and effective RLang-
informed agents based on model-free and model-based tab-
ular RL methods such as Q-Learning (Watkins and Dayan
1992) and RMax (Brafman and Tennenholtz 2002), policy
gradient methods such as PPO (Schulman et al. 2017) and
REINFORCE (Williams 1992) and hierarchical RL meth-
ods based on options and DDQN (Van Hasselt, Guez, and
Silver 2016).

Hierarchical Policy Structure: 2D Minecraft We ﬁrst
consider a 2D version of Minecraft based on Andreas, Klein,
and Levine (2017), consisting of a gridworld (see Figure 2)
that contains workbenches where the agent can craft new ob-
jects, and raw materials like wood, stone and gold. To build
an item, the agent must have the required ingredients and be
in the correct workbench. The agent has the action use to
interact with elements, and actions to move in the cardinal
directions.

We show how providing the sub-policy structure of the
task improves performance. Speciﬁcally, we provide the
agent with initiation and termination conditions for a few
options (to collect wood, go to the three different workshops
and to build the required elements), leaving the agent to
learn the policy over options. The following program con-
cisely deﬁnes 3 options fully and 4 options with uninforma-
tive policies. This is an example of a simple RLang program
that conveys partial hierarchical structure that can effectively
help the agent improve learning.

1

2

3

4

5

6

7

8

9

10

11

Option g o t o w o r k s h o p 0 :

i n i t ( any ) :
E x e c u t e

g o t o w o r k s h o p 0 l e a r n a b l e p o l i c y
u n t i l ( a t w o r k s h o p 0 )
Option g o t o w o r k s h o p 1 :

i n i t ( any ) :
E x e c u t e

g o t o w o r k s h o p 1 l e a r n a b l e p o l i c y
u n t i l ( a t w o r k s h o p 1 )

Option g e t w o o d :

i n i t ( t h e r e i s w o o d ) :

E x e c u t e

4This section does not provide new algorithmic contribution.
Instead, it focuses in showing the advantage of providing such di-
verse kind of (partial) knowledge to an RL agent and showcases
RLang as an algorithm-agnostic medium to express such knowl-
edge.

g e t w o o d l e a r n a b l e p o l i c y
u n t i l d e l t a w o o d >= 1

Option b u i l d p l a n k :

i n i t ( wood >= 1 and a t w o r k s h o p 1 ) :

E x e c u t e u s e

u n t i l

( d e l t a p l a n k >= 1 )

Option b u i l d s t i c k :

i n i t

( wood >= 1 and a t w o r k s h o p 1 )

E x e c u t e u s e

u n t i l

( d e l t a s t i c k >= 1 )

Option b u i l d l a d d e r :

i n i t

( s t i c k >= 1 and p l a n k >= 1 )

E x e c u t e u s e

12

13

14

15

16

17

18

19

20

21

22

23

24

u n t i l

( d e l t a l a d d e r >= 1 )
To exploit this information, the agent must learn both the
policy over options to maximize reward, and the option poli-
cies that achieve each option’s termination condition. For
both the high-level and low-level agents, we use the DDQN
algorithm (Van Hasselt, Guez, and Silver 2016) (implemen-
tation details are in Appendix B.3).

Figure 3a show the average return of RLang-informed hi-
erarchical DDQN (Van Hasselt, Guez, and Silver 2016) vs.
the uninformed (ﬂat) performance of a DDQN agent. The
results show that providing a concise program partially de-
scribing a hierarchical solution was sufﬁcient to successfully
learn to solve the task, in stark contrast with the uninformed
DDQN agent.

Policy Prior: Lunar Lander Next, we consider programs
that provide prior policy knowledge. Such policy informa-
tion need not be optimal or complete, but it can still improve
learning performance. We ﬁrst consider the Lunar Lander
environment (Brockman et al. 2016), which requires learn-
ing an optimal control policy to gently land a ship on the
moon. The environment has a dense reward signal encoding
both the goal of the system and cost constraints, a continu-
ous state space, and four discrete actions that either do noth-
ing, ﬁre the main engine, or ﬁre the left or right thruster. We
provide the agent with an initial policy using the following
RLang program:

1

2

3

4

5

6

7

8

9

10

11

12

13

14

P o l i c y l a n d :

( l e f t

i f
( r i g h t
i f

l e g i n c o n t a c t == 1 . 0 ) or

l e g i n c o n t a c t == 1 . 0 )
( v e l o c i t y y / 2 * − 1 . 0 ) > 0 . 0 5 :
E x e c u t e m a i n e n g i n e

e l s e :

E x e c u t e d o n o t h i n g
r e m a i n i n g h o v e r >

e l i f
r e m a i n i n g a n g l e and r e m a i n i n g h o v e r
> −1 * r e m a i n i n g a n g l e and
r e m a i n i n g h o v e r > 0 . 0 5 :
E x e c u t e m a i n e n g i n e

e l i f

r e m a i n i n g a n g l e < − 0 . 0 5 :

E x e c u t e

r i g h t

t h r u s t e r

e l i f

r e m a i n i n g a n g l e > 0 . 0 5 :

E x e c u t e

l e f t

t h r u s t e r

e l s e :

E x e c u t e d o n o t h i n g

We implemented an RLang-informed agent using PPO
(Schulman et al. 2017), a policy gradient method, as our base
method. We probabilistically mixed the RLang-deﬁned ad-
vice policy with a learnable policy network using mixing pa-
rameter β ∈ [0, 1], following Fern´andez and Veloso (2006).

(a) Craftworld + hierarchical
information

(b) Lunar Lander +
policy prior

(c) Lava-Gap + Dynamics
information

Figure 3: Figure(a) shows the return for an RLang-informed DDQN agent (blue) that leverages hierarchical information vs.
standard (ﬂat) DDQN (red) in 2D-Minecraft. Figure(b) shows average return for Lunar Lander in which an RLang-informed
PPO agent (blue) leverages an initial advice policy (with average performance shown in yellow). Figure(c) shows the average
return curves for RLang-informed Q-Learning with information about the reward and dynamics in Lava-Gap.

This mixing parameter is annealed during learning process.
In this way, the RLang policy and the learnable policy shared
control stochastically.

1

2

3

4

5

P o l i c y gain momentum :
i f v e l o c i t y < 0 :

E x e c u t e g o l e f t

e l s e :

E x e c u t e g o r i g h t

Figure 3b shows the average return curves resulting from
an uninformed PPO agent (Schulman et al. 2017) and the
RLang-informed version. The informed agent is able to
leverage the initial performance of the given policy and learn
how to improve it further, resulting in clearly better return.

We also considered two classic control problems: Cart-
Pole and Mountain Car. For CartPole, we obtain analo-
gous results using REINFORCE (Williams 1992) as the base
method (Appendix B.4). In Mountain Car, a hard exploration
problem in RL, a very concise RLang policy results in near-
optimal performance; the simple program on the right gets
a −119 average return over 100 episodes, where the task is
considered solved with a −110 average return.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

E f f e c t m o v i n g e f f e c t :

i f A == up :

x ’ −> x + 1
y ’ −> y

e l i f A == down :
x ’ −> x − 1
y ’ −> y

e l i f A == l e f t :

x ’ −> x
y ’ −> y − 1
e l i f A == r i g h t :

x ’ −> x
y ’ −> y + 1
E f f e c t d y n a m i c s :

i f

a t w a l l :

S ’ −> S

e l s e :

−> m o v i n g e f f e c t

E f f e c t
i f

r e w a r d :

i n l a v a :
Reward −1

e l i f

a t g o a l :

Reward 1 .

24

25

26

27

28

e l s e :

Reward 0 .

E f f e c t main :

−> d y n a m i c s
−> r e w a r d

Dynamics and Rewards: Lava-Gap We now show how
to provide information about the dynamics and the rewards
to an agent using RLang. To do so we use the Lava-Gap
environment, in Figure 2, a gridworld in which an agent is
tasked to navigate to a goal position. The agent can move in
the cardinal directions but each action has a probability of
failure of 1/3. Moving into walls causes the agent to stay
in the same position and falling into a lava pit results in a
high negative reward. The agent would typically need to fall
into lava pits many times to learn to avoid it. With RLang,
however, we can easily inform agents about the dynamics
and the high cost of lava pits.
the

effect
moving effect that predicts the effect of an action
in most cases—i.e., when walls are not in the way. The
effect dynamics extends this by adding the effect of
walls and the reward function is provided through the effect
reward. Tabular Q-Learning is suitable here. We designed
a Q-Learning agent that exploits the transition dynamics
and reward information. The agent ﬁrst estimates an initial
Q-table using Value Iteration based on the partial transition
and reward models—when information is unknown for a
transition tuple (s, a, s(cid:48)) the Q-value defaults to 0 (more
details in Appendix B.1).

program on

deﬁnes

right

The

an

Average return curves for an RLang-informed Q-Learning
agent (Watkins and Dayan 1992) are in Figure 3c. This
shows that the informed agent leverages the information to
gain high return early in the training process. Analogous re-
sults for an RMax(Brafman and Tennenholtz 2002) agent, a
model-based method, are in Appendix B.1.

5 Related Work

There has been a recent surge of interest in methods that use
language to inform RL agents (Luketina et al. 2019). These
fall under methods that use natural language to instruct, or to
reward, agents as a form of supervision, or methods that use

050100150200250300350400Step Number (x1000)0.00.20.40.60.81.0Average RewardRLangUninformed050100150200250300350400Step Number (x1000)−200−1000100200Average RewardRLangUninformedAdvice Policy0150300450600750900Episode Number−1.0−0.50.00.51.0Average RewardRLangUninformedTable 2: Comparison of DSLs proposed for RL agents and the types of expressible MDP information

RL
Language

Policy
Hint

Action
Structure Constraints

Policy

State
Structure

Rewards

Transition
Dynamics

ALisp (Andre and Russell 2002)
Advice RL (Maclin and Shavlik 1996)
Program-guided Agent (Sun, Wu, and Lim 2020)
Programable Agents (Denil et al. 2017)
Policy sketches (Andreas, Klein, and Levine 2017)
GLTL (Littman et al. 2017)
SPECTRL (Jothimurugan, Alur, and Bastani 2019)
RLang

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

formal languages to represent goals or an MDP component.
Formal Languages in Reinforcement Learning In clas-
sical planning it is standard to use the Planning Domain
Description Language (PDDL; Ghallab et al. 1998) and
its probabilistic extension PPDDL (probabilistic PDDL;
Younes and Littman 2004) to specify the complete dynam-
ics of a factored-state environment. RLang is inspired by
these but it is intended for a fundamentally different task:
providing partial knowledge to a learning agent, where the
knowledge might correspond to any component of the un-
derlying MDP. The Relational Dynamic Inﬂuence Diagram
Language (RDDL; Sanner 2010) extends PPDDL capabili-
ties to compactly express factored MDPs grounded on Dy-
namic Bayesian Networks (DBNs) to allow for correlated
effects, concurrent actions and partial observability. How-
ever, it lacks expressivity for partial speciﬁcations, options
and policies which hinders its application in the RL setting.
Maclin and Shavlik (1996) propose an RL paradigm in
which the agent may request advice, as provided through
a DSL that uses propositional statements to provide policy
hints. Similarly, Sun, Wu, and Lim (2020) propose to learn
a policy conditioned on a program from a DSL. Andreas,
Klein, and Levine (2017) use a simple grammar to repre-
sent policies as a concatenation of primitives (sub-policies)
to provide RL agents with knowledge about the hierarchical
structure of the tasks.

Other languages include linear temporal

logic (LTL;
Littman et al. 2017; Jothimurugan, Alur, and Bastani 2019)
which has been used to describe goals for instruction-
following agents. These methods ground LTL formulae to
reward functions for the agent. Note that LTL formulae eas-
ily express non-Markov tasks and grounding methods mod-
ify the state space to cope with this. RLang assumes that the
state space is already Markov.

RLang expands on all of these DSLs to include informa-
tion beyond the policy and the reward function, thus allow-
ing a wider array of information to be parsed and interpreted
by the agent. Table 2 summarizes existing DSLs for RL and
shows their relative expressive power: no existing DSL is
sufﬁciently powerful to express the wide range of informa-
tion that can be used by an RL agent.

Natural Language Grounding and Learning Meth-
ods There is a signiﬁcant amount of work that attempts to
ground the semantic meaning of language instructions to
information usable by RL agents (Luketina et al. 2019).

Some approaches learn to ground natural language advice
to a single grounding function type (e.g., reward function)
directly from data. For example, for the game of Civiliza-
tion II, RL agents can be taught to ground linguistic in-
formation to features that allow better estimation of the Q-
function (Branavan, Silver, and Barzilay 2012). Other work
shows that grounding textual speciﬁcations of goals and
dynamics, allows learning a language-conditioned policy
(Zhong, Rockt¨aschel, and Grefenstette 2019). In instruction-
following, some approaches learn to map instructions di-
rectly to reward functions (Misra, Langford, and Artzi 2017;
Bahdanau et al. 2018; Goyal, Niekum, and Mooney 2020),
while others translate natural language to an intermediate
formal language that represents rewards (Artzi and Zettle-
moyer 2013; Patel, Pavlick, and Tellex 2020). In general,
these languages are restricted grammars that can be eas-
ily mapped to the desired grounding element. For example,
some methods translate instructions to sequences of prim-
itive actions (Misra and Artzi 2015) or to LTL formulae
(Williams et al. 2018; Gopalan et al. 2018; Patel, Pavlick,
and Tellex 2020). In future work, we plan to use RLang as
the semantic representation language, since it has a higher
expressive power.

6 Conclusion and Future Work

RLang is a precise, concise and unambiguous domain-
speciﬁc language designed to enable a human to provide
prior knowledge to an RL agent. It provides syntax and se-
mantics tailored for MDPs and the RL setting where par-
tial information can signiﬁcantly improve learning perfor-
mance of established RL methods, as shown in our experi-
ments. Moreover, these experiments highlight the main as-
sumption in current RL algorithm design: agents must learn
tabula rasa and, therefore, we were required to design ad-
hoc informed variations in our examples. In future work, RL
methods must also consider informed formulations in which
humans can provide information about the task deﬁnition,
the relevant dynamics and policy/action advice. We envision
RLang to enable research in more general agents that ex-
ploit the structure of programs to automatically decide how
to best exploit the knowledge provide. RLang is algorithm-
agnostic by design and we expect it to be a standard interface
that will enable research in such general informed agents.

References

simple rl: Reproducible Reinforcement

Abel, D. 2019.
Learning in Python.
State Abstraction
Andre, D.; and Russell, S. J. 2002.
for Programmable Reinforcement Learning Agents.
In
Eighteenth National Conference on Artiﬁcial Intelligence,
119–125. USA: American Association for Artiﬁcial Intel-
ligence. ISBN 0262511290.
Andreas, J.; Klein, D.; and Levine, S. 2017. Modular Multi-
task Reinforcement Learning with Policy Sketches. In Pro-
ceedings of the 34th International Conference on Machine
Learning-Volume 70, 166–175. JMLR. org.
Artzi, Y.; and Zettlemoyer, L. 2013. Weakly Supervised
Learning of Semantic Parsers for Mapping Instructions to
Actions. Transactions of the Association for Computational
Linguistics, 1: 49–62.
Bahdanau, D.; Hill, F.; Leike, J.; Hughes, E.; Hosseini, A.;
Kohli, P.; and Grefenstette, E. 2018. Learning to Understand
Goal Speciﬁcations by Modelling Reward. In International
Conference on Learning Representations.
Barto, A. G.; and Mahadevan, S. 2003. Recent advances in
Hierarchical Reinforcement Learning. Discrete Event Dy-
namic Systems, 13(1): 41–77.
Barto, A. G.; Sutton, R. S.; and Anderson, C. W. 1983. Neu-
ronlike Adaptive Elements that Can Solve Difﬁcult Learning
Control Problems. IEEE Transactions on Systems, Man, and
Cybernetics, (5): 834–846.
Boutilier, C.; Dearden, R.; and Goldszmidt, M. 2000.
Stochastic Dynamic Programming with Factored Represen-
tations. Artiﬁcial Intelligence, 121(1-2): 49–107.
Brafman, R. I.; and Tennenholtz, M. 2002. R-max: A
General Polynomial-Time Algorithm for Near-Optimal Re-
inforcement Learning. Journal of Machine Learning Re-
search, 3(Oct): 213–231.
Branavan, S.; Silver, D.; and Barzilay, R. 2012. Learning
to Win by Reading Manuals in a Monte-Carlo Framework.
Journal of Artiﬁcial Intelligence Research, 43: 661–704.
Branavan, S.; Zettlemoyer, L. S.; and Barzilay, R. 2010.
Reading Between the Lines: Learning to Map High-Level
Instructions to Commands. Association for Computational
Linguistics.
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;
Schulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI
Gym.
Denil, M.; Colmenarejo, S. G.; Cabi, S.; Saxton, D.; and
de Freitas, N. 2017. Programmable Agents. arXiv preprint
arXiv:1706.06383.
Dietterich, T. G. 1998. The MAXQ Method for Hierarchical
In ICML, volume 98, 118–126.
Reinforcement Learning.
Citeseer.
Fern´andez, F.; and Veloso, M. 2006. Probabilistic Policy
Reuse in a Reinforcement Learning Agent. In Proceedings
of the Fifth International Joint Conference on Autonomous
Agents and Multiagent Systems, 720–727.

Fujita, Y.; Nagarajan, P.; Kataoka, T.; and Ishikawa, T. 2021.
ChainerRL: A Deep Reinforcement Learning Library. Jour-
nal of Machine Learning Research, 22(77): 1–14.
Ghallab, M.; Howe, A.; Knoblock, C.; McDermott, D.; Ram,
A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL—
The Planning Domain Deﬁnition Language.
Gopalan, N.; Arumugam, D.; Wong, L.; and Tellex, S.
2018. Sequence-to-Sequence Language Grounding of Non-
In Robotics: Science and
Markovian Task Speciﬁcations.
Systems.
Goyal, P.; Niekum, S.; and Mooney, R. J. 2019. Using Natu-
ral Language for Reward Shaping in Reinforcement Learn-
ing. arXiv preprint arXiv:1903.02020.
Goyal, P.; Niekum, S.; and Mooney, R. J. 2020. PixL2R:
Guiding Reinforcement Learning Using Natural Lan-
arXiv preprint
guage by Mapping Pixels to Rewards.
arXiv:2007.15543.
Jothimurugan, K.; Alur, R.; and Bastani, O. 2019. A Com-
posable Speciﬁcation Language for Reinforcement Learn-
ing Tasks. In Wallach, H.; Larochelle, H.; Beygelzimer, A.;
d'Alch´e-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in
Neural Information Processing Systems, volume 32. Curran
Associates, Inc.
Kernigham, B. W.; and Ritchie, D. M. 1973. The C Pro-
gramming Language. Prentice Hall of India.
Littman, M. L.; Topcu, U.; Fu, J.; Isbell, C.; Wen, M.; and
MacGlashan, J. 2017. Environment-independent Task Spec-
iﬁcations via GLTL. arXiv preprint arXiv:1704.04341.
Luketina, J.; Nardelli, N.; Farquhar, G.; Foerster, J.; An-
dreas, J.; Grefenstette, E.; Whiteson, S.; and Rockt¨aschel,
T. 2019. A Survey of Reinforcement Learning Informed by
Natural Language. arXiv preprint arXiv:1906.03926.
Maas, A. L.; Hannun, A. Y.; Ng, A. Y.; et al. 2013. Rectiﬁer
Non-Linearities Improve Neural Network Acoustic Models.
In Proc. icml, volume 30, 3. Citeseer.
Maclin, R.; and Shavlik, J. W. 1996. Creating Advice-taking
Reinforcement Learners. Machine Learning, 22(1): 251–
281.
Misra, D.; and Artzi, Y. 2015. Reinforcement Learning for
Mapping Instructions to Actions with Reward Learning.
Misra, D.; Langford, J.; and Artzi, Y. 2017. Mapping In-
structions and Visual Observations to Actions with Rein-
In Proceedings of the 2017 Confer-
forcement Learning.
ence on Empirical Methods in Natural Language Process-
ing, 1004–1015.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-
ing Atari with Deep Reinforcement Learning. arXiv preprint
arXiv:1312.5602.
Nota, C. 2020. The Autonomous Learning Library. https:
//github.com/cpnota/autonomous-learning-library.
Patel, R.; Pavlick, E.; and Tellex, S. 2020. Grounding Lan-
guage to Non-Markovian Tasks with No Supervision of Task
Speciﬁcations. In Proceedings of Robotics: Science and Sys-
tems.

Zhong, V.; Rockt¨aschel, T.; and Grefenstette, E. 2019.
RTFM: Generalising to New Environment Dynamics via
Reading. In International Conference on Learning Repre-
sentations.

S.

2010.

Relational Dynamic

Http://users.cecs.anu.edu.au/

Inﬂu-
(RDDL): Language De-
ssan-

Puterman, M. L. 1990. Markov decision processes. Hand-
books in Operations Research and Management Science, 2:
331–434.
Sanner,
ence Diagram Language
scription.
ner/IPPC 2011/RDDL.pdf.
Schrittwieser, J.; Antonoglou, I.; Hubert, T.; Simonyan, K.;
Sifre, L.; Schmitt, S.; Guez, A.; Lockhart, E.; Hassabis, D.;
Graepel, T.; et al. 2020. Mastering atari, go, chess and shogi
by planning with a learned model. Nature, 588(7839): 604–
609.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal Policy Optimization Algorithms.
arXiv preprint arXiv:1707.06347.
Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,
M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,
T.; et al. 2018. A general reinforcement learning algorithm
that masters chess, shogi, and Go through self-play. Science,
362(6419): 1140–1144.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;
Huang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,
A.; et al. 2017. Mastering the Game of Go without Human
Knowledge. nature, 550(7676): 354–359.
Sumers, T. R.; Ho, M. K.; Hawkins, R. D.; Narasimhan, K.;
and Grifﬁths, T. L. 2021. Learning Rewards From Linguis-
In Proceedings of the AAAI Conference on
tic Feedback.
Artiﬁcial Intelligence, volume 35, 6002–6010.
Sun, S.-H.; Wu, T.-L.; and Lim, J. J. 2020. Program Guided
Agent. In International Conference on Learning Represen-
tations.
Sutton, R. S.; Precup, D.; and Singh, S. 1999. Between
MDPs and semi-MDPs: A Framework for Temporal Ab-
straction in Reinforcement Learning. Artiﬁcial intelligence,
112(1-2): 181–211.
Van Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep Re-
inforcement Learning with Double Q-learning. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 30.
Van Rossum, G.; and Drake Jr, F. L. 1995. Python Reference
Manual. Centrum voor Wiskunde en Informatica Amster-
dam.
Watkins, C. J.; and Dayan, P. 1992. Q-learning. Machine
learning, 8(3-4): 279–292.
Williams, E. C.; Gopalan, N.; Rhee, M.; and Tellex, S. 2018.
Learning to Parse Natural Language to Grounded Reward
Functions with Weak Supervision. In 2018 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), 1–7.
IEEE.
Williams, R. J. 1992. Simple Statistical Gradient-Following
Algorithms for Connectionist Reinforcement Learning. Ma-
chine learning, 8(3-4): 229–256.
Younes, H. L.; and Littman, M. L. 2004. PPDDL 1.0: The
language for the probabilistic part of IPC-4. In Proc. Inter-
national Planning Competition.

A RLang: Grammar and Semantics

(cid:104)option(cid:105)

::= Option (cid:104)identiﬁer(cid:105):

In this section, we deﬁne the semantics of the expressions
that compose RLang.

A.1 Grammar

(cid:104)program(cid:105)
(cid:104)declaration(cid:105)

(cid:104)constant(cid:105)

(cid:104)action(cid:105)

(cid:104)factor(cid:105)

::= import (cid:104)declarations(cid:105)
::= (cid:104)constant(cid:105)
| (cid:104)action(cid:105)
| (cid:104)factor(cid:105)
| (cid:104)proposition(cid:105)
| (cid:104)goal(cid:105)
| (cid:104)feature(cid:105)
| (cid:104)markov feature(cid:105)
| (cid:104)option(cid:105)
| (cid:104)policy(cid:105)
| (cid:104)effect(cid:105)
| (cid:104)action restriction(cid:105)
::= Constant (cid:104)identiﬁer(cid:105) :=
(cid:104)arithmetic expression(cid:105)
::= Action (cid:104)identiﬁer(cid:105) :=
(cid:104)arithmetic expression(cid:105)
::= Factor (cid:104)identiﬁer(cid:105) :=
(cid:104)special variable(cid:105)

(cid:104)proposition(cid:105)

::= Proposition (cid:104)identiﬁer(cid:105) :=

(cid:104)goal(cid:105)

(cid:104)feature(cid:105)

(cid:104)boolean expression(cid:105)
::= Goal (cid:104)identiﬁer(cid:105) :=
(cid:104)boolean expression(cid:105)
::= Feature (cid:104)identiﬁer(cid:105) :=

(cid:104)arithmetic expression(cid:105)

(cid:104)markov feature(cid:105)

::= MarkovFeature (cid:104)identiﬁer(cid:105) :=

(cid:104)policy(cid:105)

(cid:104)arithmetic expression(cid:105)
::= Policy (cid:104)identiﬁer(cid:105) :
(cid:104)policy statement(cid:105)

(cid:104)policy statement(cid:105)

::= (cid:104)execute statement(cid:105)

| (cid:104)conditional policy statement(cid:105)
| (cid:104)probabilistic policy statement(cid:105)

(cid:104)execute statement(cid:105)

::= Execute

(cid:104)arithmetic expression(cid:105)

(cid:104)option init(cid:105)(cid:104)policy statement(cid:105)
(cid:104)option until(cid:105)

(cid:104)option init(cid:105)
(cid:104)option until(cid:105)
(cid:104)effect(cid:105)

::= init (cid:104)boolean expression(cid:105)
::= until (cid:104)boolean expression(cid:105)
::= Effect (cid:104)identiﬁer(cid:105):

(cid:104)effect statements(cid:105)

(cid:104)effect statement(cid:105)

::= (cid:104)reward(cid:105)

| (cid:104)prediction(cid:105)
| (cid:104)effect reference(cid:105)
| (cid:104)conditional effect statement(cid:105)
| (cid:104)probabilistic effect statement(cid:105)
::= Reward (cid:104)arithmetic expression(cid:105)
::= (cid:104)identiﬁer(cid:105)’ -> (cid:104)arithmetic expression(cid:105)
::= -> (cid:104)identiﬁer(cid:105)
::= ActionRestriction (cid:104)identiﬁer(cid:105):

(cid:104)restrict statements(cid:105)

(cid:104)reward(cid:105)
(cid:104)prediction(cid:105)
(cid:104)effect reference(cid:105)
(cid:104)action restriction(cid:105)

A.2 Semantics: Basic Syntactic Elements
RLang allows to express information that grounds to func-
tions deﬁned over the State-Action space of an MDP. More-
over, these functions have to be Markov, in the MDP sense,
allowing to deﬁne functions with domain X that can be
S × A × S and, its simpliﬁcations, S, A, S × A, S × S. The
range of these functions can include real vectors Rd (with
d ∈ N), Booleans {(cid:62), ⊥} and sets. The following are the
basic expressions that are used to build the MDP speciﬁc
elements of RLang.

the form f

• Real Expressions (cid:104)arithmetic expression(cid:105) are functions
: X → Rd for some dimen-
of
sion d. Syntactically, RLang allows element-wise arith-
metic operations (+, −, ∗, /), numeric literals, and ref-
erences to previously deﬁned real functions to deﬁne
new functions. These functions are Markov and deﬁned
over the State-Action Space of the MDP, i.e. X ∈
{S, A, S × A, S × S, S × A × S}.

• Constant expressions (cid:104)constant(cid:105) allows to bind a name
((cid:104)identiﬁer(cid:105)) to literal value or a list of literal values.
• Boolean Expressions (cid:104)boolean expression(cid:105) Analogous
the
: X → {(cid:62), ⊥} with domain X ∈

to Real Expressions,
form f
{S, A, S × A, S × S, S × A × S}.
In order to deﬁne new Boolean expressions, RLang al-
low for logical operators (and, or, not) and order
relations of the real numbers (¡, ¡= , ¿, ¿=, =, !=).

these are functions of

• Relations and Partial Functions A relation of domains
X and Y are a subset R ⊆ X × Y . Partial functions are
speciﬁcations of functions of the form f : X → Y . A
partial function is, then, a relation such that

P F :={(x, y) ∈ X × Y and

∀(x, y), (x(cid:48), y(cid:48)) x = x(cid:48) =⇒ y = y(cid:48)}.

Therefore, it is partial because not every element of the
domain is deﬁned by the function. We consider that un-
deﬁned domain elements map to the special element
unknown.

A.3 Semantics: RLang Expressions
Core RLang Type Deﬁnitions are necessary in order to de-
rive new information from the base vocabulary.

• State Space Deﬁnitions allows to deﬁne important fea-

tures and set of states for the agent.

1. State Features (cid:104)feature(cid:105) These ground to functions of
the form φ : S → Rd. Hence, given the base vocab-
ulary and using real expressions, we can derive new
features;

2. State Factors (cid:104)factor(cid:105) In the particular case the state
space is S ⊆ Rn (n ∈ N) and factored, we have spe-
ciﬁc state features that correspond to the State Factors
whose deﬁnition is given by a list of integers that cor-
respond to the positions in the state vectors that are
part of the factor. A factorization of the state corre-
spond to a set of disjoint factors whose union is the
full state vector;

3. Propositions (cid:104)proposition(cid:105) ground to Boolean func-
tions with domain in S that represents a set of states
Sσ := {s ∈ S and s |= σ}.

• Action Deﬁnition (cid:104)action(cid:105) names a particular action.
Consider that the action space A ⊆ Rd with d ∈ N.
Then, an action deﬁnition grounds to a point in A.

allows

to deﬁne

• Option Deﬁnition (cid:104)option(cid:105)

an
temporally-abstracted action based on the options frame-
work (Sutton, Precup, and Singh 1999). Therefore, the
statement directly maps to the triple (I, πo, β), where I
is the initiation proposition deﬁned by the syntactical el-
ement (cid:104)option init(cid:105), β is the termination proposition de-
ﬁned by (cid:104)option until(cid:105) and πo is the policy function de-
ﬁned by a policy statement (cid:104)policy statement(cid:105)

• Markov Feature Deﬁnition (cid:104)markov feature(cid:105) allow to
derive real-value features of a transition tuple (s, a, s(cid:48))
of the MDP. These expressions ground to real functions
of the form f : S × A × S → Rn with n ∈ N. These
are the more general grounding function within RLang
and it allows to compose information regarding all im-
portant elements of an MDP (e.g., rewards, transitions,
value functions).

Core MDP Expressions are related to speciﬁcations of

the main functions of the MDP:

Transition Dynamics and Rewards represented syntac-
tically by (cid:104)effect statement(cid:105). Such statements ground to tu-
ples (R, T ) of reward functions and next-state probabilities.
More precisely, grounding functions are R : S ×A×S → R
and T : S ×A×S → [0, 1]. The following are the semantics
of possible effect expressions.

• Reward (cid:104)reward(cid:105) statement allows to specify a reward
value using real expressions. A reward statement grounds
to a function R : S × A × S → R deﬁned by scalar
arithmetic expressions.

• Next State Prediction (cid:104)prediction(cid:105) ground to functions
T : S × A × S → [0, 1] that gives a probability of tran-
sitioning to the next state s(cid:48) after executing action a at
state s. The following are possible groundings:
1. Null Effect: S’ -> S. This grounds to

T (s(cid:48), a, s) =

(cid:26)1
0

if s(cid:48) = s
otherwise

;

2. Singleton Prediction: S’ -> (cid:104)constant(cid:105). Let

the
constant ground to a valid state vector ˆs ∈ S. Thus,
the expression grounds to

T (s(cid:48), a, s) =

(cid:26)1
0

if s(cid:48) = ˆs
otherwise

;

3. Real Prediction: S’ -> (cid:104)arithmetic expression(cid:105).
Let the (cid:104)arithmetic expression(cid:105) ground to a real func-
tion of the form e : S × A → S. Then, the prediction
expression grounds to

T (s(cid:48), a, s) =

(cid:26)1
0

if s(cid:48) = e(s, a)
otherwise

;

4. Factor

Prediction

factor name ->
(cid:104)arithmetic expression(cid:105). Let factor name ground
to the factor φ : S → Rd where d ∈ N is the di-
mension of the factor. Let the (cid:104)arithmetic expression(cid:105)
ground to eφ : S × A → Rd. Then, a factored
prediction grounds to the function

Tφ(φ(s(cid:48)), a, s) =

(cid:26)1
0

if φ(s(cid:48)) = e(s, a)
otherwise

;

A collection of factor predictions for a set of disjoint
factors {φi}i that partition the state vector can ground
to a full transition function

T (s(cid:48), a, s) =

(cid:89)

Tφi(φi(s(cid:48)), a, s)

5. Probabilistic

i
Statements
Effect
(cid:104)probabilistic effect statement(cid:105) allows to explicitly
indicate probabilities for a collection of predictions.
Consider that the probabilistic statement is a collec-
tion of tuples {(Ti, pi)}i where Ti is the grounding
function of the prediction and pi a probability, subject
to the correctness of the probabilities (cid:80)
i pi ≤ 1 and
for all pi ≥ 0. Thus, it grounds to

T (s(cid:48), a, s) =

piTi(s(cid:48), a, s).

(cid:88)

i

i pi < 1, then the remaining probability is con-

If (cid:80)
strued to be assigned to unknown.
In the case of Factor predictions for a given factor, the
grounding function Tφ is deﬁned analogously.

6. Conditional

Statements
Effect
(cid:104)conditional effect statement(cid:105)
conditional
context allows to deﬁne subsets of
the domain
D ⊆ S × A × S through a (cid:104)boolean expression(cid:105)

The

that deﬁnes when a particular Execute statement
is valid. Hence, a (cid:104)conditional policy statement(cid:105)
grounds to partial functions R = {(Di, Ri)}i where
each tuple is the result of a branch from the parsing
of if-elif-else blocks. Analogously, for the
grounding of dynamics information of the statement
T = {(Di, Ti)}i, where the Di are disjointed subsets
of the domain deﬁned by the Boolean expressions,
the order of the conditional branches and the Ti and
Ri are deﬁned by reward and prediction statements
deﬁned above.

7. Effect References (cid:104)effect reference(cid:105) allows to refer
to previously deﬁned effect statements to compose
new ones. Each referred effect is tuple of (Ri, Ti) of
groundings for rewards and transition dynamics.
A collection of effect references ground to:
Rewards R(s, a, s(cid:48)) = (cid:80)
i∈I(s,a,s(cid:48)) Ri(s, a, s(cid:48))
where I(s, a, s(cid:48)) is the set of referred effects that have
information for rewards in the tuple (s, a, s(cid:48)). Hence,
rewards are composed additively.
Transition Dynamics Let S(cid:48)
i(s, a) = {s(cid:48) ∈ S :
Ti(s(cid:48), a, s) > 0} be the set of next states from state-
action pair (s, a) about which Ti provide knowledge.
Thus, a set of effect references are well-deﬁned if
(cid:84)
i(s,a) T (s(cid:48), a, s) ≤ 1
s(cid:48)∈(cid:83)

i(s, a) = ∅ and (cid:80)

i S(cid:48)

i S(cid:48)

for all (s, a). Hence, the grounding function is

T (s(cid:48), a, s) =

Ti(s(cid:48), a, s).

(cid:88)

i

Policies (cid:104)policy(cid:105) ground to policy functions π : S × A →
[0, 1]. The simplest expression to specify a policy is an ex-
ecute statement (cid:104)execute statement(cid:105). The name after the
Execute keyword represents either an (cid:104)action(cid:105) a(cid:48) ∈ A
and, hence, the statement grounds to

π(s, a) =

(cid:26)1

if a = a(cid:48)
0 otherwise

,

or it can refer to a previously deﬁned (cid:104)policy(cid:105) that grounds
to ˆπ and, then, the statement grounds to π = ˆπ. Therefore,
the (cid:104)execute statement(cid:105) functions analogously to a return
statement in function deﬁnitions in imperative programming
languages: when an action name is found, it maps the query-
ing state s to the ﬁrst action referenced by an execute state-
ment.

policy

Probabilistic

expressions
(cid:104)probabilistic policy statement(cid:105): Probability
statements
allow to extend the execute statement with explicit prob-
ability values. Therefore, a probabilistic policy statement
grounds to a collection of execute statements-probability
pairs {(πi, pi)}i. In this way, probabilistic policy statements
ground to

π(s, a) =

(cid:88)

i

piπi(s, a)

If (cid:80)

i pi < 1, then the remaining probability is construed

to be assigned to unknown.

Policy

conditional

Conditional

Expressions
(cid:104)conditional policy statement(cid:105): The
con-
text allows to deﬁne subsets of the domain S(cid:48) ⊆ S through
a (cid:104)proposition(cid:105) that deﬁnes when a particular execute
statement is valid. Hence, a (cid:104)conditional policy statement(cid:105)
grounds to a partial function π(cid:48)
: {(Si, πi)}i where each
pair (S(cid:48)
i, πi) the result of a branch from the parsing of
if-elif-else blocks. The S(cid:48)
i are disjointed and they are
the result of the (cid:104)proposition(cid:105), the order of the statements
and the returning semantics of the execute statements. The
πi are deﬁned by execute statements or probabilistic policy
statements.

Action Restrictions (cid:104)action restriction(cid:105) are deﬁned anal-
ogously to conditional policy statements. They reduce the
possible set of actions to consider in a given situation. They
ground to functions of the form A : S → A that deﬁnes
then subset of prohibited actions to take in state s—i.e.,
A(s) ⊂ A.

Goals (cid:104)goal(cid:105) ground to set of states that are considered
goal states for the MDP, i.e. terminating and highly reward-
ing. RLang represents goals through propositions.

B Experimental Details and Additional

Results

In this section, we extend the discussion on the implemen-
tation details of RLang demonstrations in Section 4. We
provide details about the implementation of the RLang-
informed variations of the RL algorithms, descriptions of the
environments and the hyperparameters used.

For each of the experiments below we report average re-
turn curves over 5 different random seeds and report 95%
conﬁdence intervals. Moreover, we use a running average
with window size of 50.

B.1 Lava-Gap
Lava-Gap is a 6 × 6 grid-world with coordinates x, y ∈
{1, 6}. There is a wall in position (3, 1) and 4 lava pits
in locations (3, 2), (1, 4), (2, 4), (2, 5). The goal position is
(5, 1). The agent has 4 discrete actions that allows it to move
in one of the cardinal directions by 1 step. Each action has
a probability of failure of 1/3 that would move the agent to
a random neighboring position (in the cardinal directions).
The state st at time t is represented by the x and y coordi-
nates of the position at time t. The agent receives a reward of
−1 for falling in a lava pit and the episode terminates. Sim-
ilarly, the agent receives a reward of 1 for reaching the goal
and the episode terminates. In any other case, the reward is
0. At the start of every episode, the agent begins executing
at position (1, 1). We use a discount factor of γ = 0.95.
We use simple rl’s implementation (Abel 2019) of this
gridworld and of the RMax and Q-Learning algorithms. Ex-
periments were run in a personal MacBook Pro with a 2.4
GHz Quad-Core Intel Core i5.

RLang-informed Q-Learning In the RLang-informed
example for Lava-gap, we leverage the transition and re-
ward information from the RLang program to initialize the
Q-table for Q-Learning and R-Max. We compute the Q-table
by executing value iteration considering in the state pairs

(a) RLang-informed Q-Learning

(b) RLang-informed RMax

Figure 4: Average return curves for Lava-Gap environment. We provide the agent with an RLang program that contains infor-
mation about the reward function and the transition dynamics.

where transition information is available. In Algorithm 1,
we show how the Q-table of a Q-Learning agent is initial-
ized using the information provided in a RLang program
given as input as the RLangKnowledge objects. The Updat-
eValue function computes the new value using the standard
TD-error.

Algorithm 1: Q-Table Initialization

function InitQTable(RLangKnowledge, QAgent)

for (s,a) ∈ S × A do

if RLangKnowledge.Reward(s,a) is known then

QAgent.Q(s,a) ← RLangKnowledge.Reward(s,a)

end if
end for
for N iterations do

for (s,a,s’) ∈ S × A × S do

if RLangKnowledge.Transition(s,a)
then

is known

T (s, a, s(cid:48)) ← RLangKnowledge.T(s, a)
R(s, a, s(cid:48)) ← RLangKnowledge.Reward(s,a)
QAgent.Q(s,a) ← UpdateValue(s, a, s(cid:48),
T (s, a, s(cid:48)), QAgent)

end if
end for

end for
end function

RLang-informed RMax RMax (Brafman and Tennen-
holtz 2002) is a model-based RL algorithm. Hence, we di-
rectly use the RLang-provided information to initialize the
model of the world (i.e., Transition and Reward tables) in
addition to initializing the Q-table. We set the count in the
respective tables to be a hyper-parameter K, less that the
threshold used by RMax to considered a transition known.

Hyperparameters For uninformed Q-Learning we have
the exploration (cid:15) = 0.1 and the step size α = 0.05 and
for the RLang-informed Q-Learning we use (cid:15) = 0.01 and

the same α. For RMax and RLang-informed RMax, we use
a threshold of 30 samples to consider the transition learned
and set K = 1.

RMax Results
In Figure 4b, we show the average return
curves for an RMax agent informed with the program below,
in which we see consitent results with the results obtained
for an RLang-informed Q-Learning agent.

B.2 Taxi

We use simple rl’s implementation of the Taxi environ-
ment (Dietterich 1998) with 2 passengers in a 5×5 grid. The
state vector has the position of the agent and a binary vari-
able that is 0 when the taxi does not carry a passenger and 1
otherwise. Moreover, it has the current position of the pas-
sengers, the destination of the passenger and a binary vari-
able that indicates if the passenger is in the taxi. The agent
has 4 movement actions and a special action for picking up
a passenger that is at the same position than the agent and
another for dropping off the passenger currently in the taxi.
The reward function is 1 when all passenger are in destina-
tion and 0 otherwise. The discount factor γ = 0.95. Experi-
ments were run in a personal MacBook Pro with a 2.4 GHz
Quad-Core Intel Core i5.

RLang-informed hierarchical Q-Learning In this ex-
periment, we use a hierarchical RL agent based on the op-
tions framework. In this particular case, we use Q-Learning
to learn both the policy over options and the intra-option
policies. We consider that an RLang-deﬁned option is learn-
able if the policy function is not provided, i.e. only initiation
and termination conditions are speciﬁed. We use such termi-
nation condition as a goal represented by a pseudo-reward
function that is 1 when the termination condition is achieved
and 0 otherwise that the inner agent uses to learn the intra-
option policy. We initialize the intra-option learning agents
of those learnable options deﬁned in the input RLang pro-
gram with the procedure in Algorithm 2.

0150300450600750900Episode Number−1.0−0.50.00.51.0Average RewardRLangUninformed04080120160200240280Episode Number−5−4−3−2−101Average RewardRLangUninformedAlgorithm 2: Hierarchical Agent Initialization

if o is learnable then

1: function InitializeOptions(RLangKnowledge)
for o ∈ RLangKnowledge.Options do
2:
3:
4:
end if
5:
end for
6:
7: end function

o.agent ← InitializeAgent()

Hyperparameters For our Q-Learning baseline, we use
an exploration (cid:15) = 0.1 and a step size of α = 0.1. For our
hierarchical Q-Learning agents, we have a Q-Learning agent
for each subpolicy to be learnt with (cid:15) = 0.1 and α = 0.1
and a Q-Learning agent to learn the policy over options with
(cid:15) = 0.01 and α = 0.5. We implemented our hierarchical
Q-Learning agent as Sutton, Precup, and Singh.

Results
In Figure 5a, we show the average return curves
for Taxi. The RLang program, shown below, deﬁnes the
options given to the agent—a simple variation of this pro-
gram is provided to the agent that needs to learn the intra-
option policies. The plot shows the most-informed agent, i.e.
intra-option policies are provided and it only needs to learn
the policy over options, is represented in blue, the RLang-
informed agent that only knows the initiation and termina-
tion conditions of the options (in red) and an uninformed Q-
Learning agent. We observe that both of the informed agents
are able to exploit the knowledge to gain a steeper learning
curve with respect to the uninformed agent.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Option p i c k u p p a s s e n g e r 0 :

i n i t ( n o t ( p a s s e n g e r
n o t ( p a s s e n g e r 0 i n d e s t ) )

i n t a x i ) and

E x e c u t e p i c k u p p a s s e n g e r 0

u n t i l p a s s e n g e r 0 i n t a x i

Option d r o p o f f p a s s e n g e r 0 :
i n i t ( p a s s e n g e r 0 i n t a x i )

E x e c u t e d r o p o f f p a s s e n g e r 0

u n t i l p a s s e n g e r 0 i n d e s t and n o t (
p a s s e n g e r 0 i n t a x i )

Option p i c k u p p a s s e n g e r 1 :

i n i t ( n o t ( p a s s e n g e r
n o t ( p a s s e n g e r 1 i n d e s t ) )

i n t a x i ) and

E x e c u t e p i c k u p p a s s e n g e r 1

u n t i l p a s s e n g e r 1 i n t a x i

Option d r o p o f f p a s s e n g e r 1 :
i n i t ( p a s s e n g e r 1 i n t a x i )

E x e c u t e d r o p o f f p a s s e n g e r 1

u n t i l p a s s e n g e r 1 i n d e s t and n o t (
p a s s e n g e r 1 i n t a x i )

2D Minecraft

B.3
2D Minecraft is a crafting environment based on Andreas,
Klein, and Levine implemented as a 10 × 10 grid. The state
vector include a map of the environment, an inventory vec-
tor and the change on inventory with respect to the previous
time step. The map is represented by 10 × 10 × 22 tensor
that represent with a one-hot vector the element at position

(x, y). The agent has 4 actions to move in the cardinal di-
rections by one position and a special action use to interact
with the element in front, i.e., given the current position and
orientation of the agent, the position with which the agent
interacts in the one the agent is facing. If such element is a
primitive, the agent adds it to its inventory; if the element
is a workbench and it has any of the primitive elements to
build an object in the workbench, then those objects are built
and added to the inventory (any primitive element used is re-
moved from the inventory); if the agent is in front of water
and has a bridge, it can use it and cross the water. If the
agent has to interact with stone, it needs an axe in inventory.
This is a goal-oriented environment; when the agent has the
goal object in inventory, it receives a reward of 1 and the
episode terminates. When an episode starts, the agent is ran-
domly placed in any free cell of the grid. We use a discount
factor γ = 0.99. Experiments were run on a single GPU
NVIDIA GeForce RTX 3090 Ti.

RLang-informed hierarchical DDQN Agent To solve
this environment, analogously to the Taxi environment, we
use a hierarchical agent based on options. We use DDQN
(Van Hasselt, Guez, and Silver 2016) as the algorithm to
learn both the policy over options and intra-option policies.
Algorithm 2 is used to initialized the intra-option policies.
To implement DDQN and its hierarchical variation based on
options, we based it on the Autonomous Learning Library
(Nota 2020).

Neural Network architecture and parameters We use a
CNN with 4 ReLU-activated layers with ﬁlter banks of size
32, 32, 32, 64, a kernel size of 3 and stride of 2 (padding
was used to keep the dimension 10 × 10). The inventory and
inventory change were processed with a ReLU MLP with
hidden layer of size 32 and output 32. These two vectors are
concatenated and passed through a linear layer of size 256
(for the ﬂat DDQN) and 64 for the agents in the hierarchical
DDQN implementation. Finally, this output vector is passed
through a ReLU MLP with a hidden layer of size 64 to get
the value predictions for each action.

Hyperparameters For DDQN, we use a linear schedule
for (cid:15)−greedy exploration with start with (cid:15) = 1 and (cid:15) =
0 and ﬁnal exploration step 10000. We use a learning rate
0.001 and mini-batch of size 64. We use a Prioritized replay
buffer of size 10000. The target network update frequency is
100 steps. We set a time of 1000 steps.

For hierarchical DDQN:

• Outer Agent (policy over options): we use a linear sched-
ule for (cid:15)−greedy exploration with start with (cid:15) = 1 and
(cid:15) = 0 and ﬁnal exploration step 60000. We use a learning
rate 10−5 and mini-batch of size 64. We use a Prioritized
replay buffer of size 10000. The target network update
frequency is 100 steps. This outer agent had a timeout of
1000 steps;

• Inner Agents (intra-option policies): we use a linear
schedule for (cid:15)−greedy exploration with start with (cid:15) = 1
and (cid:15) = 0.001 and ﬁnal exploration step 30000. We use a
learning rate 10−4 and mini-batch of size 128. The target

(a) Taxi

(b) 2D Minecraft

Figure 5: Average return curves for Taxi and Craftworld when information about the hierarchical structure of the problem is
provided using RLang. We provide the agents with a program that speciﬁes the initiation and termination conditions of the
options required to solve the problem and the RLang-informed agent learns the intra-option policies and policy over options. In
the particular case of Taxi, we also include the learning curve when we also provide the intra-option policies.

network update frequency is 100 steps. We use a Priori-
tized replay buffer of size 10000. Each subpolicy had a
timeout of 100 steps.

B.4 Classic Control
Environments
In this section, we consider Cartpole and
Lunar Lander, two classic environments for RL research that
have continuous state-space and discrete action space. For
both environments, we use OpenAI Gym’s implementations
(Brockman et al. 2016), i.e. CartPole-v0 and LunarLander-
v2.

Cartpole (Barto, Sutton, and Anderson 1983) consists of
an underactuated pole attached to a cart. At the beginning,
the pole starts in a vertical position (0 degrees) and the agent
has to learn a policy to keep it within 15 degrees. The state
consists of the position and the velocity of the cart, and the
angle and angular velocity of the pole. The action space is to
apply momentum to move the cart to the right or to the left.
An episode ends when the pole absolute angle is greater than
15 degrees, the position of the cart is greater than 2.4 units or
after 200 time steps. The agent gets a reward of 1 every time
step. This task is considered solved is the average return is
at least 195 over 100 episodes.

Lunar Lander simulates the task of landing a ship in a
landing pad on the moon. The state consists of the ship’s po-
sition and velocity, angle and angular velocity, and Boolean
ﬂags that indicate if the ship’s leg is in contact with the
ground. The actions are to ﬁre the main engine, the right ori-
entation engine, the left orientation engine and doing noth-
ing. The agent gets a reward of −100 if the ship crashed and
+100 if it lands correctly. It receives +10 for each leg that
touches the ground and −0.3 if the main engine is ﬁred. The
task is solved with a return of at least 200. Experiments were
run on a single GPU NVIDIA GeForce RTX 3090 Ti.

RLang-informed Policy Gradient Agent To solve these
environments, we provided non-optimal policies through

RLang programs and use policy gradients methods to lever-
age this knowledge while learning. Algorithm 3 is derived
from (Fern´andez and Veloso 2006), in which, we proba-
bilistically share control between the learning policy πθ and
the RLang-provided policy ˆπ. At each time step, we choose
which policy to follow by drawing a sample from a Bernoulli
distribution with parameter β. We use such probabilistic
mixing to collect trajectories and then optimize, θ, using
policy gradient methods. We use REINFORCE (Williams
1992) for Cartpole and PPO (Schulman et al. 2017) for Lu-
nar Lander. The mixing parameter β is annealed exponen-
tially using a decay rate α.

Algorithm 3: Hierarchical Agent Initialization

1: function PolicyMixing(RLangKnowledge,decay rate)
2:
3:
4:
5: end function

Trajectories ← Rollout(Env, πθ, ˆπ, β)
θ ←PolicyGradient(Trajectories, πθ)
β ← β ∗ decay rate

Neural Network architecture and parameters For Cart-
pole and REINFORCE, we use a policy network using
an MLP with hidden size 64 and Leaky ReLU activations
(Maas et al. 2013) with parameter 0.2

In the case of Lunar Lander and PPO, we use a policy
network based on a MLP with hidden size 64 and Leaky
ReLU activations (Maas et al. 2013) with parameter 0.2. As
a value network, we use an MLP with hidden size 64 and
Tanh activations.

Hyperparameters For Cartpole, we use PFRL’s REIN-
FORCE implementation (Fujita et al. 2021). We use an ini-
tial mixing parameter β = 0.7 and a decay rate α = 0.99.
For REINFORCE, we use a learning rate of 0.001 and a
batch size of 5. In Lunar Lander, we use PFRL’s PPO im-
plementation and use a mixing parameter β = 0.5 and a

0150300450600750900Episode Number0.00.20.40.60.81.01.2Average RewardRLang (intra-option policy known)RLang (intra-option policy unknown)Uninformed050100150200250300350400Step Number (x1000)0.00.20.40.60.81.0Average RewardRLangUninformed(a) CartPole using REINFORCE

(b) Lunar Lander using PPO

Figure 6: Average return curves for classic control tasks with continuous state spaces. We use RLang to provide the agent with
an initial policy (non-optimal) that the agent can leverage to improve learning performance. We compare with the uninformed
counterparts.

decay rate α = 0.9999. For PPO, we used a learning rate of
0.0002.

Cartpole Results For Cartpole, we provide the RLang
program below with a very simple prior policy. In Figure
6a, we show the average return curves for RLang-informed
REINFORCE and its uninformed performance, which show
a jump-start performance gain for the agent then improves
through experience.

1

2

3

4

5

P o l i c y b a l a n c e p o l e :

i f p o l e a n g u l a r v e l o c i t y > 0 :

E x e c u t e m o v e r i g h t

e l s e :

E x e c u t e m o v e l e f t

0100200300400500600700Step Number (x100)255075100125150175200Average RewardRLangUninformed050100150200250300350400Step Number (x1000)−200−1000100200Average RewardRLangUninformedAdvice Policy