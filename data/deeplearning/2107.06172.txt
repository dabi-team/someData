1
2
0
2

n
u
J

9
1

]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[

1
v
2
7
1
6
0
.
7
0
1
2
:
v
i
X
r
a

Arrhenius.jl: A Diﬀerentiable Combustion Simulation
Package

Weiqi Jia,∗, Xingyu Sub, Bin Pangc, Sean Joseph Cassadyd, Alison M.
Ferrisd, Yujuan Lic, Zhuyin Renb, Ronald Hansond, Sili Denga,∗

aDepartment of Mechanical Engineering, Massachusetts Institute of
Technology, Cambridge, MA, 02139, USA
bCenter for Combustion Energy, Tsinghua University, Haidian, Beijing, 100084, China
cWeichai Power Co. Ltd., Weifang, Shandong, China
dHigh Temperature Gasdynamics Laboratory, Department of Mechanical
Engineering, Stanford University, Stanford, CA, 94305, USA

Abstract

Combustion kinetic modeling is an integral part of combustion simulation,

and extensive studies have been devoted to developing both high ﬁdelity and

computationally aﬀordable models. Despite these eﬀorts, modeling combus-

tion kinetics is still challenging due to the demand for expert knowledge and

optimization against experiments, as well as the lack of understanding of

the associated uncertainties. Therefore, data-driven approaches that enable

eﬃcient discovery and calibration of kinetic models have received much at-

tention in recent years, the core of which is the optimization based on big

data. Diﬀerentiable programming is a promising approach for learning kinetic

models from data by eﬃciently computing the gradient of objective functions

to model parameters. However, it is often challenging to implement diﬀeren-

tiable programming in practice. Therefore, it is still not available in widely

∗Corresponding author
Email addresses: weiqiji@mit.edu (Weiqi Ji ), silideng@mit.edu (Sili Deng )

Preprint submitted to Elsevier

July 14, 2021

 
 
 
 
 
 
utilized combustion simulation packages such as CHEMKIN and Cantera.

Here, we present a diﬀerentiable combustion simulation package leveraging

the eco-system in Julia, including DiﬀerentialEquations.jl for solving diﬀer-

ential equations, ForwardDiﬀ.jl for auto-diﬀerentiation, and Flux.jl for incor-

porating neural network models into combustion simulations and optimizing

neural network models using the state-of-the-art deep learning optimizers.

We demonstrate the beneﬁts of diﬀerentiable programming in eﬃcient and

accurate gradient computations, with applications in uncertainty quantiﬁca-

tion, kinetic model reduction, data assimilation, and model discovery.

Keywords: Data-driven Modeling, Diﬀerentiable Programming, Chemical

Kinetics, Uncertainly Quantiﬁcation, Chemical Reaction Neural Network

1. Introduction

The optimization of model parameters plays a critical role in the devel-

opment of chemical kinetic simulation tools. While heuristic optimization

methods, such as genetic algorithms, have been widely employed in combus-

tion modeling, optimization algorithms based on stochastic gradient descent

(SGD) have seldom been exploited. Meanwhile, SGD has shown promise in

nonconvex optimization for complex nonlinear models, and SGD has played

a central role in driving the boom of deep learning in the last decade [1].

Existing optimization techniques in combustion modeling can be catego-

rized into heuristic algorithms and response surface techniques. Heuristic

algorithms [2, 3] usually perform well with less than 100 parameters and

small datasets. In the case of the genetic algorithm, for example, the com-

putational cost scales with the number of parameters and the number of

2

samples in the dataset. As a result, modern deep learning working in big

data regimes and deep neural network models with tens of thousands of pa-

rameters seldom employ heuristic optimization. Response surface techniques

[4] alleviate the intensive computational cost in evaluating kinetic models

by building a function approximation that maps the model parameter space

to model predictions. Similar to heuristic algorithms, response surface tech-

niques are also limited to low-dimensional model parameter space [5] and

small datasets, as the cost of building a response surface scales with the di-

mension of parameter space and the number of quantities of interest. Mean-

while, recent development in sensor techniques and experiment automation

[6] have signiﬁcantly increased the eﬃciency of experimental data generation

and driven combustion research into the big data regime. Combined with

SGD, the optimization of complex chemical models using these new datasets

becomes feasible. Furthermore, various techniques have been developed in

conjunction with SGD to increase the generalization performance of the op-

timized model. For instance, a modern deep learning optimizer not only

focuses on minimizing the loss functions but also regularizes the model to

increase the extrapolation capability. Generalization to diﬀerent conditions

and tasks is an important feature for classical physics-based chemical models,

and generalization allows us to develop a chemical model based on canonical

combustion experiments that also works reasonably well in simulating prac-

tical combustion systems. Therefore, SGD is not only more eﬃcient but also

more generalizable compared to heuristic optimization algorithms.

One of the major obstacles for exploiting SGD in combustion modeling

is the lack of software ecosystems that can eﬃciently and accurately com-

3

pute the gradient of simulation output to model parameters. For instance,

the ﬁnite diﬀerence method (often termed the brute-force method) usually

suﬀers both computational ineﬃciency, as the cost scales with the number of

parameters, and inaccuracy due to truncation error. Conversely, stochastic

gradient descent based on auto-diﬀerentiation (AD) has shown both eﬃciency

and accuracy in the training of large-scale deep neural network models [1].

Many open-source AD packages have been developed in the last decade, in-

cluding TensorFlow [7] and Jax [8] backed by Google, PyTorch [9] backed

by Facebook, ForwardDiﬀ.jl [10] and Zygote.jl [11] in Julia. To this end,

we introduce the AD-powered diﬀerentiable combustion simulation package

Arrhenius.jl [12], which incorporates combustion physics models into AD

ecosystems in Julia to facilitate the study of diﬀerentiable combustion mod-

eling.

This paper is structured as follows: we shall ﬁrst introduce the package

Arrhenius.jl in Sec. 2 and the formulas for various gradient calculations in

Sec. 3. We then present the application of Arrhenius.jl to uncertainty quan-

tiﬁcation, kinetic model reduction, data assimilation, and model discovery in

Sec. 4. Finally, we draw conclusions in Sec. 5.

2. Arrhenius.jl

Arrhenius.jl is built with the programming language of Julia to leverage

the rich ecosystems of auto-diﬀerentiation and diﬀerential equation solvers.

Arrhenius.jl does two types of diﬀerentiable programming: (i) it can diﬀer-

entiate elemental computational blocks. For example, it can diﬀerentiate the

reaction source term with respect to kinetic and thermodynamic parameters

4

as well as species concentrations. (ii) It can diﬀerentiate the entire simu-

lator in various ways, such as solving the continuous sensitivity equations

[13] as done in CHEMKIN [14] and Cantera [15] and in adjoint methods

[16, 17]. The ﬁrst type of diﬀerentiation is usually the basis of the second

type of higher-level diﬀerentiation. Arrhenius.jl oﬀers the core functionality

of combustion simulations in native Julia programming, such that users can

conveniently build applications on top of Arrhenius.jl and exploit various

approaches to do high-level diﬀerentiation.

Figure 1 shows a schematic of the structure of the Arrhenius.jl pack-

age. Arrhenius.jl reads in the chemical mechanism ﬁles in YAML format

maintained by the Cantera community; the chemical mechanism ﬁles con-

tain the kinetic model, thermodynamic, and transport databases. The core

functionality of Arrhenius.jl is to compute the reaction source terms and

mixture properties, such as heat capacities, enthalpies, entropies, Gibbs free

energies, etc. In addition, Arrhenius.jl oﬀers ﬂexible interfaces for users to

deﬁne neural network models as submodels and augment them with exist-

ing physical models. For example, one can use a neural network submodel

to represent unknown reaction pathways and exploit various scientiﬁc ma-

chine learning methods to train the neural network models, such as neural

ordinary diﬀerential equations [18, 19] and physics-informed neural network

models [20, 21]. One can then implement the governing equations for dif-

ferent applications with these core functionalities and solve the governing

equations using classical numerical methods or neural network-based solvers,

such as physics-informed neural networks [20]. Arrhenius.jl provides solvers

for canonical combustion problems, such as simulating the auto-ignition in

5

constant volume/pressure reactors and oxidation in jet-stirred reactors.

Figure 1: Schematic showing the structure of the Arrhenius.jl package.

In contrast to legacy combustion simulation packages, Arrhenius.jl can

not only provide predictions given the physical models, but also optimize

model parameters given experimental measurements. By eﬃciently and ac-

curately evaluating the gradient of the solution outputs to the model param-

eters, experimental data can be incorporated into the simulation pipeline to

enable data-driven modeling with deep learning algorithms.

3. Gradient Calculation

In this section, we brieﬂy discuss how Arrhenius.jl can enhance various

approaches for computing the gradient (sensitivity) in jet-stirred reactors,

6

shock tubes, and laminar ﬂame experiments. In general, we deal with two

kinds of gradient calculations, i.e., steady-state solutions and transient solu-

tions.

Examples of steady-state solutions are the modeling of species proﬁles in

jet-stirred reactors and steady laminar ﬂames. Without loss of generality, we

can write down the governing equations in vector form as

F (φ(α); α) = 0,

(1)

where F corresponds to the residual vector, φ corresponds to the solution

vector, and α corresponds to model parameters, such as the kinetic, thermo-

dynamic, and transport parameters. By diﬀerentiating Eq. 1 with respect

to the α, we obtain matrix equations for the gradients.

∂F
∂φ

∂φ
∂α

+

∂F
∂α

= 0,

(2)

where ∂φ

∂α are the Jacobian matrices of the solution vector with respect to
model parameters. Normally, for optimization, we have a scalar loss function

deﬁned as L = G(φ). The gradient of the loss function with respect to model

parameters can be readily achieved via

∂L
∂α

=

∂L
∂φ

∂φ
∂α

.

(3)

With Arrhenius.jl, we can leverage AD to compute the two Jacobian ma-

trices of ∂F

∂φ and ∂F

∂α . Unlike packages for calculating the analytical Jacobian,
eﬃcient computation of the Jacobian can be achieved without developing

the analytical form. In addition, general AD enables us to diﬀerentiate over

neural network submodels, which is diﬃcult to implement using analyti-

cal approaches. If the solution variables are discretized in a computational

7

domain, e.g., the one-dimensional freely propagating ﬂame, one can readily

leverage multi-threading to evaluate the two Jacobians without complex code

re-factorization using parallel computing.

Examples of transient solutions are auto-ignition and fuel pyrolysis in

shock tubes. The governing equations are in the general form of

dφ
dt

= F (φ, t; α).

(4)

A natural approach to compute the gradient W = ∂φ

∂α is solving the gov-

erning equations of W , i.e.,

dW
dt

=

∂F
∂φ

W +

∂F
∂α

.

(5)

In addition to solving Eq. 5, Arrhenius.jl leverages various adjoint sen-

sitivity algorithms provided in DiﬀerentialEquations.jl. For comprehensive

comparisons of various algorithms in computing the gradient of solutions

variables for transient solutions, readers shall consult [16].

In general, calculating the gradient for transient solutions involving stiﬀ

chemical kinetic models is expensive, as computation time usually scales with

both the number of species and the number of parameters. Meanwhile, global

combustion behaviors, such as ignition delay times (IDTs), are usually more

experimentally accessible compared to measurements of concentration pro-

ﬁles. Recent work [13, 22–24] has substantially advanced the algorithms for

computing the gradient of IDT. This work employs the sensBVP method [22],

which converts the initial value problem (IVP) to a boundary value problem

(BVP) by treating the temperature at ignition as a boundary condition and

the IDT as a free variable to solve. In other words, the problem is converted

8

from a transient problem to a steady-state problem, similar to the solution

of a one-dimensional freely propagating ﬂame.

4. Results and Discussion

We now present four case studies using Arrhenius.jl for uncertainty quan-

tiﬁcation, kinetic model reduction, data assimilation, and model discovery.

4.1. Application to Uncertainty Quantiﬁcation

We ﬁrst apply Arrhenius.jl to computing the active subspace for quanti-

fying kinetic uncertainties. Active subspace [25] is a parametric dimension

reduction approach that identiﬁes the low-dimensional subspace of high-

dimensional uncertain parameters via the singular value decomposition of

the expected parameter’s gradient.

It has been applied to evaluate com-

bustion model uncertainties in zero/one-dimensional simulations [5, 26, 27],

as well as in turbulent combustion simulations [28–30]. The active subspace

corresponds to the directions along which the simulation results change signif-

icantly, while the simulation results are almost unchanged in the kernel space.

Uncertainty quantiﬁcation in combustion simulations usually suﬀers from the

curse of dimensionality, i.e., the computational cost exponentially increases

with the number of model parameters. With the active subspace, one can

conduct various uncertainty quantiﬁcation tasks within the low-dimensional

subspace. The active subspace methodology is detailed in [25] and its key

idea is illustrated in Eq. 6:

(cid:90)

C =

∇f (x)∇f (x)T πx(x)dx = WΛWT ,

(6)

9

where x ∈ Rd represents the samples from the parameter space, πx is the

distribution of the model parameters, f (x) refers to the quantity of interest

and ∇f (x) corresponds to the gradient with respect to the model parame-

ters. The unitary matrix W consists of the d eigenvectors w1, w2, ..., wd and

Λ is a diagonal matrix whose components are the eigenvalues λ1, λ2, ..., λd,

sorted in descending order.

If there is a gap in the eigenvalues, meaning

λr >> λr+1, then the function f (x) varies mostly along the ﬁrst r eigenvec-

tors and is almost constant along the rest of the eigenvectors. The ﬁrst e

eigenvectors are selected as active directions, i.e., S = [w1, w2, ..., wr]. The

matrix C is usually approximated by Monte Carlo sampling and the num-

ber of samples required is suggested to be m = αβlog(d). The constant α

is the over-sampling factor and is recommended to be between 2 and 10,

and β is the largest dimension of the subspace allowed in subsequent utiliza-

tion of active subspace. The major challenge for exploiting active subspace

in combustion modeling is how to eﬃciently compute the gradient ∇f (x).

With Arrhenius.jl, we now can eﬃciently and accurately compute the gradi-

ents and thus make the active subspace approach more widely accessible in

combustion modeling.

We used the chemical models of GRI3.0 and LLNL’s detailed n-heptane

model (Version 3.1) [31] for demonstrations to compute eigenvalue spectra.

The ignition delay time (IDT) is speciﬁed as our quantity of interest. The

thermodynamic conditions for both cases are 40 atm, 1200 K, and stoichio-

metric fuel/air mixture. The sensBVP approach is employed to compute the

gradient of IDT to the pre-exponential factors A of the reactions in the kinetic

model. An independent log-uniform distribution is assumed for each reac-

10

tion, and ln(A/A0) ∼ U[−0.5, 0.5], in which A0 denotes the nominal value

of the pre-exponential factor A, and U denotes a uniform distribution. The

number of samples drawn from the parameter space is set as m = 20 ∗ log(d)

for methane, 10 ∗ log(d) for n-heptane, and d is the number of reactions.

In Figs. 2a and c, one-dimensional active subspace can be identiﬁed for

both cases, since the ﬁrst eigenvalue is larger than the second by two orders

of magnitude. The identiﬁed active subspace is shown in the summary plot,

where all of the samples are plotted against the ﬁrst active direction deﬁned

by the leading eigenvector. The summary plots in Figs. 2b and d further

show that the variations in IDT can be well captured by the ﬁrst active di-

rection, for all samples are distributed along a one-dimensional curve. Those

results demonstrate the capability of Arrhenius.jl in eﬀectively identifying

the active subspace. With the one-dimensional active subspace identiﬁed,

one can exploit it for global sensitivity analysis, forward and inverse uncer-

tainty quantiﬁcation, and optimization.

As far as the computational cost is concerned, our approach obtained the

active subspace within one minute on a laptop for GRI3.0, while the ﬁnite

diﬀerence approach took about an hour. In addition, the model for n-heptane

consists of 4846 reactions, and 4846 is the highest dimension that has ever

been achieved in uncertainty quantiﬁcation for chemical models in literature,

to the best knowledge of the authors.

11

Figure 2: (a, c) Eigenvalue spectra and (b, d) summary plots for the models of GRI3.0

and LLNL v3.1 for n-heptane. The thermodynamic conditions for both cases are under

40 atm, 1200 K, and stoichiometric fuel/air mixture. The uncertainties considered are the

uncertainties in the pre-exponential factor A. An independent log-uniform distribution is

assumed for all of the reactions, and ln(A/A0) ∼ U[−0.5, 0.5]. The number of samples

drawn from the parameter space is set as 20 ∗ log(nr) for methane, 10 ∗ log(nr) for n-

heptane, and nr is the number of reactions.

12

4.2. Application to Kinetic Model Reduction

Most skeletal chemical models are obtained by removing unimportant

species and the associated reactions from the master model, leaving the ki-

netic parameters of the remaining pathways unchanged after the reduction.

There has also been increasing interest in optimizing the kinetic parameters

in overly reduced reaction models to compensate for the error introduced

by over-reduction [32, 33]; this has the potential to produce a smaller model

with higher ﬁdelity than the ones obtained via traditional reduction methods.

The reduction-optimization approach proceeds by sampling from target con-

ditions and then optimizing the reduced model to achieve similar predictions

as the master mechanism under all sampled conditions.

Furthermore, our recent work [34] has shown that a chemical reaction net-

work is equivalent to a neural network with a single hidden layer. Similarly,

solving ordinary diﬀerential equations (ODEs) of reaction network models

is equivalent to solving inﬁnite-depth deep residual networks [18]. Conse-

quently, chemical model reduction is in analogy to the compression of deep

neural networks (DNNs). The compression of DNNs is essential for the wide

application of deep learning to reduce the size and hence the computational

cost of DNNs. Deep Compression [35] is one of the most famous compres-

sion techniques, which utilizes a multi-stage pipeline for compression. It ﬁrst

applies weight pruning to remove unimportant connections in the neural

network, similar to the pathway removal in skeletal mechanism reduction for

chemical models. It then applies quantization and retrains the compressed

network. Inspired by Deep Compression, we develop Deep Reduction, a two-

stage reduction scheme, with the ﬁrst step being the conventional skeletal

13

reduction and the second step being ﬁne-tuning the reduced model using an

SGD optimizer.

The Deep Reduction approach is demonstrated in reducing and optimiz-

ing two chemical models for natural gas and n-heptane, the master models of

which are the GRI3.0 mechanism [36] and the Nordin1998 mechanism [37],

respectively. As previously discussed, overly reduced models using classical

reduction approaches with a large threshold or intuition are ﬁrst obtained,

with the number of species in GRI3.0 reduced from 53 to 23, and that in

Nordin1998 reduced from 41 to 34. The reduction is targeted for simulat-

ing natural gas engines and diesel engines with the commercial software of

Converge. For GRI3.0, we ﬁrst removed the following species using an iter-

ative reduction involving DRG [38], DRGEP [39], PFA [40], and sensitivity

analysis: C, CH3OH, C2H, C2H2, HCCO, CH2CO, HCCOH, NH, NH2, NH3,

N2O, HNO, CN, HCN, H2CN, HCNN, HCNO, HOCN, HNCO, NCO, Ar, and

CH2CHO. We then further removed the species of CH3CHO, NO2, NO, NNH,

N, C2H3, and CH2OH, CH by removing NO-chemistry for Converge has a

built-in NO module and following the reduction of DRM19 [41]. The overly

reduced model is denoted as SK23 with 23 species. For Nordin1998, the fol-

lowing species were removed based on the reduction of GRI3.0 above: C3H5,

C3H4, C2H6, CH4O2, CH3O2, CH3O, and C2H2. The overly reduced model

is denoted as SK34 with 34 species. Note that the way to produce an overly

reduced model can be regarded as a hyper-parameters subject to explore.

This work focuses on the demonstration of the optimization algorithms, and

we leave the optimal choice of removed species to future studies. It should be

noted that one can also optimize an existing empirical semi-global reaction

14

model against a detailed model without consulting the skeletal mechanism

reduction.

The kinetic parameters of these overly reduced models were subsequently

optimized to retain the predictability of the master models. The ignition de-

lay times and laminar ﬂame speeds (SLs) are utilized as performance metrics

to validate the overly reduced and optimized models against the correspond-

ing master models. As shown in Fig. 3, for GRI3.0, SK23 overpredicts IDT

at high temperatures and underpredicts SL at all equivalence ratios. For

Nordin1998, SK34 signiﬁcantly overpredicts the IDT at low temperatures,

especially within the negative temperature coeﬃcient region, while the ﬂame

speeds are hardly aﬀected by the reduction.

The optimization of the kinetic parameters of the over-reduced models

was conducted on all three Arrhenius parameters, namely, A, b, Ea. Al-

though both IDT and SL could be selected as targets for optimization, we

only utilized the IDT for its relatively lower computational cost than SL.

Moreover, the top ten reactions selected based on the sensitivity analysis

for the SL were excluded from the optimization, such that the optimization

will not change these key reactions for SL. We then randomly sampled 500

initial conditions covering a wide range of mixture compositions and ther-

modynamic states for training. For GRI3.0, the range of the pressure is 1-60

atm, that of the initial temperatures is 1100-2000 K, that of the equivalence

ratios is 0.5-1.8, and the fuel composition is set as CH4 : C2H4 : C3H8

= 0.85 : 0.1 : 0.05 by volume. Similarly, for Nordin1998, the ranges of the

pressure, initial temperature, and equivalence ratio are 1-60 atm, 850-1800

K, and 0.5-1.5, respectively.

15

Figure 3: Predicted ignition delay times and ﬂame speeds: (a-b) the mixture of natu-

ral gas/air using the master mechanism GRI3.0, skeletal mechanism SK23 and optimized

SK23 OP. The ignition delay time was simulated using the fuel composition of CH4 : C2H4

: C3H8 = 0.85 : 0.1 : 0.05 by volume, pressure of 40 atm, equivalence ratio of 0.9. The

ﬂame speed was simulated at 40 atm and 300 K, and (c-d) the mixture of n-heptane/air us-

ing the master mechanism Nordin1998, skeletal mechanism SK34 and optimized SK34 OP.

The ignition delay time was simulated at pressure of 40 atm, equivalence ratio of 1.2. The

ﬂame speed was simulated at 40 atm and 500 K.

The datasets were split into training and validation datasets with a ratio

of 70:30. During each parameter update, one case was randomly sampled to

evaluate its IDT, and this process can be viewed as mini-batching with the

16

batch size of one. Instead of optimizing the Arrhenius parameters directly,

we optimized the relative changes of Arrhenius parameters compared to their

nominal values, i.e.,

p = [ln(A/A0), b − b0, Ea − Ea0],

(7)

where the subscript 0 refers to the base model. The units of Ea are speciﬁed

as cal/mol as we tried to minimize the changes in Ea. However, if one wants

to ensure the change in Ea is relatively comparable to the change in A, one

may specify the unit of Ea as kcal/mol, as a change of 2 kcal/mol in Ea is

close to change of e times in A, such that the changes in A and Ea will be

balanced and avoid stiﬀness in the parameter space.

The loss function was deﬁned as the mean square error (MSE) between

the predicted IDTs in the logarithmic scale using the reduced model and the

master model:

Loss = M SE (cid:0)log(IDT sk), log(IDT master).(cid:1)

(8)

The gradients of IDT to kinetic parameters were computed using the sens-

BVP method proposed in [22]. The Adam [42] optimizer with the default

learning rate of 0.001 was adopted. Weight decaying and early stopping were

employed to regularize the parameters, such that the optimization prefers

kinetic parameters that are close to their original values. Figure 4 shows the

training history of the loss function and the L2-norm of the model parameters

for the Nordin1998 model. We trained the reduced model for 100 epochs and

stopped the training when the loss function as well as the model parameters

reached a plateau.

17

Figure 4: Training history of loss functions, L2-norm of model parameters. Regularization

settings: weight-decay of 1e-4 with a learning rate of 1e-3.

The performance of the optimized skeletal mechanisms is also shown in

Fig. 3. For the IDT, as targeted in the optimization, the optimized models

agree with the master models very well for both two fuels. One interesting

observation is that the optimized models also work well for SL, although SL

is not targeted for optimization. This could be attributed to several rea-

sons. For GRI3.0, the optimization compensates for the errors in the high-

temperature chemistry seen in the predicted IDT, and those re-calibrated

high-temperature chemical pathways lead to accurate predictions of SL. For

Nordin1998, the skeletal models already accurately predict the IDT at high

temperatures as well as the SL, and the optimization does not degrade the

prediction of SL thanks to the regularization. It is worth noting that the opti-

mized SK23 still underpredicts SL at fuel-rich conditions, potentially because

some of the sensitive reactions for fuel-rich conditions are not ﬁxed during

18

optimization; further reﬁnement of the ﬁxed reactions is therefore suggested.

Furthermore, the optimization is computationally eﬃcient. Qualitatively

speaking, previously employed genetic algorithms have to be performed on

clusters [33], while the current work was performed on an ordinary worksta-

tion within an hour. In summary, these two case studies demonstrate the

ability of Arrhenius.jl to optimize complex reaction models with high accu-

racy, good generalization capability, and high eﬃciency. Such optimization

capability will help augment current mechanism reduction techniques.

4.3. Application to Data Assimilation

While the uncertainty in chemical models is still a standing problem in the

combustion community, a general consensus on the major pathways for small

hydrocarbons (C0-C4) has been reached. Therefore, such reaction models can

be assimilated with experimental data to estimate hidden information from

measured quantities [43] and guide further model reﬁnements.

Here, we applied Arrhenius.jl to the pyrolysis of propane in shock tubes to

assimilate the measured concentration-time series data for all major species

with the detailed kinetic mechanisms of USCMech II [44]. Again, we relied

on Arrhenius.jl to eﬃciently compute the loss functions to 333 Arrhenius

parameters in the propane pyrolysis model. The results demonstrate the

capability of Arrhenius.jl in data-assimilation for inferring the temperature

proﬁles from species proﬁles and suggesting changes of rate constants.

The pyrolysis of propane plays a crucial role in characterizing the combus-

tion behaviors of natural gas mixtures and provides insight into the cracking

pathways of larger fuels. We adopted the recently available experimental

dataset measured in [45] using a novel laser absorption technique based on

19

convex optimization. The dataset includes eight species proﬁles shown in

Figs. 5 and 6. The data includes ﬁve initial conditions, which are at 4 atm,

1250-1370 K, and with 2% propane in argon. USCMech II was adopted as the

baseline kinetic model. Similar to the comparisons in [45] to AramcoMech

3.0 [46], current kinetic models systematically over-predict the mole fractions

of propane under all temperatures. Therefore, we optimized the 111 reac-

tions that are directly related to the pyrolysis of propane. The simulations

were carried out under constant pressure, and the governing equations for

the mass fractions of species and temperature were solved. Similar to the

studies of deep reduction, we optimized all three Arrhenius parameters of

these 111 reactions. The parameters were also scaled according Eq. 7 but

with the unit of Ea speciﬁed as kcal/mol.

The loss function was deﬁned as the mean absolute error (MAE) between

the predicted and the measured species proﬁles, as shown in Eq. 9:

Loss = M AE (cid:0)X pred, X exp(cid:1) ,

(9)

where X pred and X exp correspond to the predicted and measured mole frac-

tions of species, respectively.

The ﬁve datasets are divided into four training sets and one validation

set. The case with the initial temperature of 1330 K, being the median of the

ﬁve datasets, was chosen as the validation set. Figure 8 presents the history

of loss functions, L2-norm of gradients, and L2-norm of model parameters.

While the training loss slightly increases after 500 epochs, we let the training

go to 1000 epochs to regularize the changes of parameters. The training took

four hours for 1000 epochs (15 seconds/epoch).

The predictions from the optimized model are also shown in Figs. 5

20

Figure 5: Training dataset: comparisons among measurements, predictions from the base-

line model, and predictions from the optimized model. The error-bar corresponds to the

estimated experimental uncertainties. Details on the experimental procedure and estima-

tion of experimental uncertainties can be found in [45].

and 6. The optimized model performs substantially better than the baseline

model and agrees very well with the measurements. Since the temperature

proﬁles are linked to the fuel decomposition rates, the predicted temperature

21

Figure 6: Validation dataset: comparisons among measurements, predictions from the

baseline model, and predictions from the optimized model.

proﬁles are also diﬀerent from those predicted by the baseline model. For

example, the temperature diﬀerence is as large as 20 K at 1.5 ms. Further-

more, the optimized mechanism also performs very well with respect to the

validation dataset, suggesting that there is no obvious over-ﬁtting in terms

22

Figure 7: The history of loss functions, L2-norm of model parameters. Regularization

settings: weight-decay of 1e-3 with a learning rate of 1e-3.

of the predictability.

Given the goodness of ﬁt when compared to the measured species pro-

ﬁles, one can expect that the predicted temperature proﬁles will also agree

with the actual temperature time-histories very well. The inferred temper-

ature using the optimized model is qualitatively consistent with the trends

in propane concentration as well. For instance, since the pyrolysis process is

endothermic, the faster the pyrolysis progresses, the faster the temperature

decreases. Therefore, such a data assimilation process could be utilized to

build a virtual temperature sensor based on species proﬁles. In addition, one

can utilize the same procedure to infer unknown species concentrations from

the available species measurements.

The data assimilation could also be targeted for suggesting key reactions

that potentially account for the discrepancies between modeling and measure-

23

ments. Here, we inspected reactions that have changed their rate constants

most during the optimization. To facilitate the comparisons, we lumped the

changes of the three Arrhenius parameters into the changes of pre-factor A

only, according to Eq. 7. As the changes in rate constants varies with tem-

perature, we present the changes at the reference temperature of 1300 K. The

complete list of reactions and the changes of kinetic parameters are provided

in the supplemental materials Table S1.

It is found that most of the changes in rate constants are no more than one

order of magnitude, while changes to 18 out of 111 reactions are greater than

an order of magnitude. Future experimental and theoretical studies could be

directed to those 18 reactions. It is worth noting that the large changes in

rate constants are not necessarily indicating large uncertainties/errors in the

rate constants. The large changes could also be attributed to the randomness

in the optimization. In an eﬀort to identify reaction rates that might be good

candidates for further study, one may further take the advantage of Arrhe-

nius.jl’s diﬀerential programming to exploit the Bayesian approach [47], such

as Stochastic Langevin Gradient Descent. Similar to the bound-to-bound

approach [48], prior information on these rate constants from theoretical cal-

culations and direct experimental measurements can be readily incorporated

into the Bayesian framework. Eﬃcient Bayesian inference will also enable

Bayesian experimental design [49] to better allocate experimental resources

for the determination of rate constants. While this paper focuses on demon-

strating the capability of data assimilation, we shall exploit the Bayesian

inference in future works.

In summary, the diﬀerential programming in Arrhenius.jl oﬀers us the

24

bandwidth to simultaneously optimize hundreds of model parameters for

data assimilation, and such capacity could provide new insights and tools for

kinetic model optimization and usage. For example, in the absence of direct

measurement, this data assimilation approach could be used to accurately

infer system temperature from species measurements, or could be used to

identify key reaction rates for which further experimental measurement or

modeling work is needed. Future work will include experiments to validate

the ability of the data assimilation approach to accurately infer temperature,

through direct temperature measurement, and make use of the Bayesian

approach to identify speciﬁc reaction rates for future study.

4.4. Application to Model Discovery

Finally, we present the application to Scientiﬁc Machine Learning (SciML)

[19] by using Arrhenius.jl to develop a neural-network-based pyrolysis sub-

model within the HyChem model framework [50]. Our recently developed

Chemical Reaction Neural Network (CRNN) [34] approach was employed

to develop the neural network model for its interpretability, such that the

learned model complies with fundamental physical laws and provides chem-

ical insights, as well as its compatibility with CHEMKIN/Cantera packages.

The conventional HyChem-based pyrolysis submodels require expert knowl-

edge on the chemical kinetics which takes years to develop. On the contrary,

the CRNN approach aims to autonomously discover the reaction pathways

and kinetic parameters simultaneously to accelerate high-ﬁdelity chemical

model development.

In the following demonstration, the CRNN-HyChem

approach was utilized to model the jet fuel of JP10.

As shown in Fig. 8, the CRNN-HyChem approach models the fuel chem-

25

istry of JP10 with two submodels, similar to the original HyChem concept.

The CRNN submodel models the breakdown of JP10 fuel molecules into

smaller hydrocarbons up to C6H6, and the submodel for C0-C6 describes the

oxidation chemistry. For proof-of-concept, we chose the same species in the

original HyChem pyrolysis submodel [50] to be included in the CRNN model;

however, it should be noted that the such chosen species can be treated as

hyper-parameters to circumvent the need for expert knowledge and achieve

potentially better performance.

In the original CRNN approach [34], the

Law of Mass Action and Arrhenius Law are enforced by the design of the

structure of the neural network. Reaction orders are assumed to be equal

to the stoichiometric coeﬃcients for the reactants. In the present study, el-

emental conservation is further guaranteed by projecting the stoichiometric

coeﬃcients into the elemental conservation space. For better convergence,

the stoichiometric coeﬃcients for JP10 are ﬁxed as -1, and during the train-

ing, the stoichiometric coeﬃcients are regularized to achieve better numerical

stability. The training data were generated by simulating the IDT using the

original JP10 HyChem model. A wide range of thermodynamic conditions

were considered: pressures of 1-60 atm, initial temperatures of 1100-1800 K,

and equivalence ratios of 0.5-1.5.

In total, 500 thermodynamic conditions

were randomly generated using the latin hypercube sampling method. The

dataset was split into training and validation datasets with a ratio of 70:30,

respectively.

26

Figure 8: Schematic showing the structure of the CRNN-HyChem approach.

The learned stoichiometric coeﬃcients and kinetic parameters are shown

in Table 1, where negative and positive stoichiometric coeﬃcients correspond

to reactants and products, respectively. Qualitatively, most of the learned

pathways are H-abstraction reactions, which is consistent with the expert-

derived HyChem models. However, quantitatively, the learned pathways are

not the same as those in the HyChem model, and further eﬀorts will be di-

rected to extracting physical insights from the learned pathways. Figure 9

compares the results of the learned CRNN model and the IDTs generated

using the original HyChem model [50], and they agree very well. The results

thus demonstrate the capability of Arrhenius.jl in learning CRNN models

with hundreds of parameters, which is impossible with ﬁnite diﬀerence meth-

ods. With the increasing demand for rapidly developed kinetic models for

new renewable fuels for screening and fuel design, CRNN provides an el-

egant approach to autonomously derive kinetic models from experimental

27

data. Arrhenius.jl will play a vital role in enabling such kind of autonomous

model discovery algorithms. While this manuscript focuses on learning the

reaction pathways from scratch, one can also develop a data-driven model

based on existing kinetic models for similar fuels and utilize Arrhenius.jl for

the training, as demonstrated in [51].

Figure 9: Comparisons between the predicted ignition delay times using the learned CRNN

model (Y-axis) and the HyChem model (X-axis) for both the training and validation

datasets.

Table 1: Learned stoichiometric coeﬃcients and kinetic parameters. Reaction orders are

assumed to be equal to the stoichiometric coeﬃcients for reactants.

28

5. Conclusions

This work presents a diﬀerential combustion simulation package, Arrhe-

nius.jl, which can perform eﬃcient and accurate gradient evaluations across

classical reacting ﬂow solvers. We expect that the open-source package could

greatly facilitate the integration of modern deep learning techniques into

combustion modeling, especially the physics-informed machine learning that

takes the advantage of both physics-based and data-driven modeling. We

also invite the contribution from the reacting ﬂow community to enhance

the capability of the package and explore its potential in other applications.

6. Acknowledgments

WJ and SD would like to acknowledge the funding support by Weichai

Power Co., Ltd. ZR and XS would like to acknowledge the support from

National Natural Science Foundation of China No. 52025062. WJ would

like to thank Dr. Ji-Woong Park for fruitful discussions on the deep mech-

anism reduction, Dr. Vyaas Gururajan on the implementation of the sens-

BVP method, Dr. Travis Sikes for discussions on mechanism optimization,

Dr. Christopher Rackauckas on the usages of DiﬀerentialEquations.jl, and

Matthew Johnson on sharing experience of developing ReactionMechanism-

Simulator.jl.

References

[1] Y. Bengio, I. Goodfellow, A. Courville, Deep learning, vol. 1, MIT press,

Cambridge, MA, 2017.

29

[2] G. Rein, C. Lautenberger, A. Fernandezpello, J. Torero, D. Urban,

Application of genetic algorithms and thermogravimetry to determine

the kinetics of polyurethane foam in smoldering combustion, Combust.

Flame 146 (2006) 95–108.

[3] A. Bertolino, M. F¨urst, A. Stagni, A. Frassoldati, M. Pelucchi, C. Cav-

allotti, T. Faravelli, A. Parente, An evolutionary, data-driven approach

for mechanism optimization: Theory and application to ammonia com-

bustion, Combust. Flame 229 (2021) 111366.

[4] D.A. Sheen, H. Wang, The method of uncertainty quantiﬁcation and

minimization using polynomial chaos expansions, Combust. Flame 158

(2011) 2358–2374.

[5] W. Ji, J. Wang, O. Zahm, Y.M. Marzouk, B. Yang, Z. Ren, C.K. Law,

Shared low-dimensional subspaces for propagating kinetic uncertainty

to multiple outputs, Combust. Flame 190 (2018) 146–157.

[6] M. Tao, P.T. Lynch, P. Zhao, Kinetic modeling of ignition in miniature

shock tube, Proc. Combust. Inst. 37 (2019) 593–601.

[7] Y. Yu, M. Abadi, P. Barham, E. Brevdo, M. Burrows, A. Davis, J. Dean,

S. Ghemawat, T. Harley, P. Hawkins, M. Isard, M. Kudlur, R. Monga,

D. Murray, X. Zheng, Dynamic control ﬂow in large-scale machine learn-

ing, Proceedings of the Thirteenth EuroSys Conference. ACM (2018),

pp. 265–283.

[8] J. Bradbury, R. Frostig, P. Hawkins, M.J. Johnson, C. Leary, D. Maclau-

rin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne,

30

Q. Zhang, JAX: Composable transformations of Python+NumPy pro-

grams, http://github.com/google/jax (2018).

[9] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,

T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K¨opf,

E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,

L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high-

performance deep learning library (2019), arXiv:1912.01703.

[10] J. Revels, M. Lubin, T. Papamarkou, Forward-mode automatic diﬀer-

entiation in julia (2016), arXiv:1607.07892.

[11] M. Innes, Don’t unroll adjoint: Diﬀerentiating ssa-form programs

(2018), arXiv:1810.07951.

[12] W. Ji, S. Deng, Arrhenius.jl: A diﬀerentiable combustion simulation

package, https://github.com/DENG-MIT/Arrhenius.jl (2021).

[13] W. Ji, Z. Ren, C.K. Law, Evolution of sensitivity directions during

autoignition, Proc. Combust. Inst. 37 (2019) 807–815.

[14] R.J. Kee, F.M. Rupley, J.A. Miller, Chemkin-II: A fortran chemical ki-

netics package for the analysis of gas-phase chemical kinetics, Technical

report, Sandia National Lab., Livermore, CA (1989).

[15] D.G. Goodwin, R.L. Speth, H.K. Moﬀat, B.W. Weber, Cantera: An

object-oriented software toolkit for chemical kinetics, thermodynamics,

and transport processes, https://www.cantera.org (2021), doi:10.

5281/zenodo.4527812, Version 2.5.1.

31

[16] C. Rackauckas, Y. Ma, V. Dixit, X. Guo, M. Innes, J. Revels, J. Ny-

berg, V. Ivaturi, A comparison of automatic diﬀerentiation and contin-

uous sensitivity analysis for derivatives of diﬀerential equation solutions

(2018), arXiv:1812.01892.

[17] C. Rackauckas, Q. Nie, Diﬀerentialequations.jl–a performant and

feature-rich ecosystem for solving diﬀerential equations in julia, Journal

of Open Research Software 5 (2017).

[18] R.T.Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud, Neural ordi-

nary diﬀerential equations (2018), arXiv:1806.07366.

[19] C. Rackauckas, Y. Ma, J. Martensen, C. Warner, K. Zubov, R. Supekar,

D. Skinner, A. Ramadhan, A. Edelman, Universal diﬀerential equations

for scientiﬁc machine learning (2020), arXiv:2001.04385.

[20] M. Raissi, P. Perdikaris, G. Karniadakis, Physics-informed neural net-

works: A deep learning framework for solving forward and inverse prob-

lems involving nonlinear partial diﬀerential equations, J. Comput. Phys.

378 (2019) 686–707.

[21] W. Ji, W. Qiu, Z. Shi, S. Pan, S. Deng, Stiﬀ-pinn: Physics-informed

neural network for stiﬀ chemical kinetics (2020), arXiv:2011.04520.

[22] V. Gururajan, F.N. Egolfopoulos, Direct sensitivity analysis for ignition

delay times, Combust. Flame 209 (2019) 478–480.

[23] M. Lemke, L. Cai, J. Reiss, H. Pitsch, J. Sesterhenn, Adjoint-based sen-

sitivity analysis of quantities of interest of complex combustion models,

Combust. Theory Model. 23 (2018) 180–196.

32

[24] S. Almohammadi, M. Hantouche, O.P. Le Maˆıtre, O.M. Knio, A tangent

linear approximation of the ignition delay time. i: Sensitivity to rate

parameters, Combust. Flame 230 (2021) 111426.

[25] P.G. Constantine, E. Dow, Q. Wang, Active subspace methods in theory

and practice: Applications to kriging surfaces, SIAM J. Sci. Comput.

36 (2014) A1500–A1524.

[26] M. Vohra, A. Alexanderian, H. Guy, S. Mahadevan, Active subspace-

based dimension reduction for chemical kinetics applications with epis-

temic uncertainty, Combust. Flame 204 (2019) 152–161.

[27] X. Su, W. Ji, Z. Ren, Uncertainty analysis in mechanism reduction

via active subspace and transition state analyses, Combust. Flame 227

(2021) 135–146.

[28] W. Ji, Z. Ren, Y. Marzouk, C.K. Law, Quantifying kinetic uncertainty

in turbulent combustion simulations using active subspaces, Proc. Com-

bust. Inst. 37 (2019) 2175–2182.

[29] N. Wang, Q. Xie, X. Su, Z. Ren, Quantiﬁcation of modeling uncertainties

in turbulent ﬂames through successive dimension reduction, Combust.

Flame 222 (2020) 476–489.

[30] N. Wang, T. Yang, Z. Ren, Active subspace variation and modeling

uncertainty in a supersonic ﬂame simulation, AIAA J. 59 (2021) 1798–

1807.

[31] M. Mehl, W.J. Pitz, C.K. Westbrook, H.J. Curran, Kinetic modeling

33

of gasoline surrogate components and mixtures under engine conditions,

Proc. Combust. Inst. 33 (2011) 193–200.

[32] A. Mittal, S.D. Wijeyakulasuriya, D. Probst, S. Banerjee, C.E.A.

Finney, K.D. Edwards, M. Willcox, C. Naber, Multi-dimensional com-

putational combustion of highly dilute, premixed spark-ignited opposed-

piston gasoline engine using direct chemistry with a new primary refer-

ence fuel mechanism, Internal Combustion Engine Division Fall Tech-

nical Conference, vol. 2 (2017), V002T06A022.

[33] M. Kelly, G. Bourque, S. Dooley, Toward machine learned highly reduce

kinetic models for methane/air combustion (2021), arXiv:2103.08377.

[34] W. Ji, S. Deng, Autonomous discovery of unknown reaction pathways

from data by chemical reaction neural network, J. Phys. Chem. A 125

(2021) 1082–1092.

[35] S. Han, H. Mao, W.J. Dally, Deep compression: Compressing deep

neural networks with pruning, trained quantization and huﬀman coding

(2015), arXiv:1510.00149.

[36] G.P. Smith, D.M. Golden, M. Frenklach, N.W. Moriarty, B. Eiteneer,

M. Goldenberg, C.T. Bowman, R.K. Hanson, S. Song, W.C. Gardiner,

V.V. Lissianski, Z. Qin, GRI-Mech 3.0, http://www.me.berkley.edu/

gri_mech/ (1999).

[37] N. Nordin, Numerical simulations of non-steady spray combustion us-

ing a detailed chemistry approach, thesis for the degree of Licentiate

34

of Engineering, Chalmers University of Technology, Goteborg, Sweden

(1998).

[38] T. Lu, C.K. Law, On the applicability of directed relation graphs to the

reduction of reaction mechanisms, Combust. Flame 146 (2006) 472–483.

[39] P. Pepiot, H. Pitsch, Systematic reduction of large chemical mechanisms,

4th Joint Meeting of the US Sections of the Combustion Institute, vol.

2123 (2005).

[40] W. Sun, Z. Chen, X. Gou, Y. Ju, A path ﬂux analysis method for the

reduction of detailed chemical kinetic mechanisms, Combust. Flame 157

(2010) 1298–1307.

[41] A. Kazakov, M. Frenklach, Reduced reaction sets based on GRI-mech

1.2, http://www.me.berkeley.edu/drm (1994).

[42] D.P. Kingma, J. Ba, Adam: A method for stochastic optimization

(2014), arXiv:1412.6980.

[43] M. Raissi, A. Yazdani, G.E. Karniadakis, Hidden ﬂuid mechanics:

Learning velocity and pressure ﬁelds from ﬂow visualizations, Science

367 (2020) 1026–1030.

[44] S.G. Davis, A.V. Joshi, H. Wang, F. Egolfopoulos, An optimized kinetic

model of H2/CO combustion, Proc. Combust. Inst. 30 (2005) 1283–1292.

[45] S.J. Cassady, R. Choudhary, V. Boddapati, N.H. Pinkowski, D.F. David-

son, R.K. Hanson, The pyrolysis of propane, Int. J. Chem. Kinet. 52

(2020) 725–738.

35

[46] C.W. Zhou, Y. Li, U. Burke, C. Banyon, K.P. Somers, S. Ding, S. Khan,

J.W. Hargis, T. Sikes, O. Mathieu, et al., An experimental and chemical

kinetic modeling study of 1, 3-butadiene combustion: Ignition delay time

and laminar ﬂame speed measurements, Combust. Flame 197 (2018)

423–438.

[47] R. Dandekar, K. Chung, V. Dixit, M. Tarek, A. Garcia-Valadez, K.V.

Vemula, C. Rackauckas, Bayesian neural ordinary diﬀerential equations

(2020), arXiv:2012.07244.

[48] M. Frenklach, H. Wang, M.J. Rabinowitz, Optimization and analysis of

large chemical kinetic mechanisms using the solution mapping method—

combustion of methane, Prog. Energy Combust. Sci. 18 (1992) 47–73.

[49] X. Huan, Y.M. Marzouk, Simulation-based optimal Bayesian exper-

imental design for nonlinear systems, J. Comput. Phys. 232 (2013)

288–317.

[50] Y. Tao, R. Xu, K. Wang, J. Shao, S.E. Johnson, A. Movaghar, X. Han,

J.W. Park, T. Lu, K. Brezinsky, et al., A physics-based approach to

modeling real-fuel combustion chemistry–III. reaction kinetic model of

JP10, Combust. Flame 198 (2018) 466–476.

[51] W. Ji, J. Zanders, J.W. Park, S. Deng, Machine learning approaches to

learn hychem models (2021), arXiv:2104.07875.

36

