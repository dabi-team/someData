Combining (Second-Order) Graph-Based and Headed-Span-Based
Projective Dependency Parsing

Songlin Yang, Kewei Tu∗
School of Information Science and Technology, ShanghaiTech University
Shanghai Engineering Research Center of Intelligent Vision and Imaging
{yangsl,tukw}@shanghaitech.edu.cn

2
2
0
2

r
a

M
9

]
L
C
.
s
c
[

2
v
8
3
8
5
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

Graph-based methods, which decompose the
score of a dependency tree into scores of de-
pendency arcs, are popular in dependency pars-
ing for decades. Recently, Yang and Tu (2022)
propose a headed-span-based method that de-
composes the score of a dependency tree
into scores of headed spans. They show im-
provement over ﬁrst-order graph-based meth-
ods. However, their method does not score
dependency arcs at all, and dependency arcs
are implicitly induced by their cubic-time al-
gorithm, which is possibly sub-optimal since
modeling dependency arcs is intuitively use-
ful. In this work, we aim to combine graph-
based and headed-span-based methods, incor-
porating both arc scores and headed span
scores into our model. First, we show a di-
rect way to combine with O(n4) parsing com-
plexity. To decrease complexity, inspired by
the classical head-splitting trick, we show two
O(n3) dynamic programming algorithms to
combine ﬁrst- and second-order graph-based
and headed-span-based methods. Our exper-
iments on PTB, CTB, and UD show that
combining ﬁrst-order graph-based and headed-
span-based methods is effective. We also con-
ﬁrm the effectiveness of second-order graph-
based parsing in the deep learning age, how-
ever, we observe marginal or no improve-
ment when combining second-order graph-
based and headed-span-based methods 1.

1

Introduction

Dependency parsing is an important task in natural
language processing. There are many methods to
tackle projective dependency parsing. In this paper,
we focus on two kinds of global methods: graph-
based and headed-span-based methods. They both
score all parse trees and globally ﬁnd the highest

∗ Corresponding Author
1Our
code

is
https://github.com/sustcsonglin/
span-based-dependency-parsing

publicly

available

at

reads

child

book

the

the

the

child

reads

a

a
a

book

child

book

reads

Figure 1: An example projective dependency parse tree
with all its headed spans.

scoring tree. The difference between the two is how
they score dependency trees. The simplest ﬁrst-
order graph-based methods (McDonald et al., 2005)
decompose the score of a dependency tree into the
scores of dependency arcs. Second-order graph-
based methods (McDonald and Pereira, 2006) ad-
ditionally score adjacent siblings, i.e., pairs of adja-
cent arcs with a shared head. There are many other
higher-order graph-based methods (Carreras, 2007;
Koo and Collins, 2010; Ma and Zhao, 2012). In
contrast, the headed-span-based method (Yang and
Tu, 2022) decomposes the score of a dependency
tree into the scores of headed spans: in a projective
tree, a headed span is a word-span pair such that
the subtree rooted at the word covers the span in the
surface order. Fig. 1 shows an example projective
parse tree and all its headed spans.

First-order graph-based parsers have difﬁculties
in incorporating sufﬁcient subtree information be-
fore the deep learning era. Dozat and Manning
(2017) show that ﬁrst-order graph-based parsers
with neural encoders and biafﬁne scorers can obtain
high parsing accuracy. Falenska and Kuhn (2019)
argue that powerful neural encoders—such as BiL-
STMs (Hochreiter and Schmidhuber, 1997)—can
encode rich subtree information implicitly, ques-
tioning the utility of high-order features. However,
recent works found that high-order graph-based

 
 
 
 
 
 
methods can outperform ﬁrst-order graph-based
methods (Fonseca and Martins, 2020; Zhang et al.,
2020; Wang and Tu, 2020) even with powerful
neural encoders, indicating the insufﬁcient subtree
modeling of ﬁrst-order graph-based methods. To
encode more subtree information, in contrast to the
line of work on higher-order parsing, Yang and Tu
(2022) choose to model headed spans, which con-
sist of all words within the corresponding subtrees.
Thus their model can utilize more subtree informa-
tion than ﬁrst-order graph-based methods. How-
ever, to retain the cubic parsing complexity, they
abandon modeling arcs as the parsing complexity
would be O(n4) otherwise (§3.1). Modeling de-
pendency arcs can capture the direct interactions
between two words and is thus useful. Therefore, it
is intuitively helpful to combine ﬁrst-order graph-
based and headed-span-based methods.

To decrease the parsing complexity, inspired by
the classical head-splitting trick (Eisner, 1997), we
propose to decompose the score of a headed span
into two terms, assuming that the score of the left
span boundary is independent of that of the right
span boundary for each headword. This allows us
to adapt the Eisner algorithm to parse in cubic time
considering both arc and headed span scores (§3.2)
at the cost of imposing a stronger independence as-
sumption. More interestingly, we can also combine
second-order graph-based and headed-span-based
methods and need only cubic time to parse (§3.3),
which would be much slower (to the best of our
knowledge, O(n7)) if we do not apply the head-
splitting trick.

We conduct extensive experiments on PTB, CTB,
and UD. We ﬁnd that combining ﬁrst-order graph-
based and headed-span-based methods is effective,
and applying the head-splitting trick or not result in
a similar performance, thus it is more advantageous
to apply this trick to enjoy a lower parsing complex-
ity. We also conﬁrm the effectiveness of second-
order parsing in the deep learning age, however,
we observe only marginal improvement or even no
improvement when combining second-order graph-
based and headed-span-based methods.

(i.e., averaging all subwords embeddings) to ob-
2. Then we feed
tain the word-level embeddings ei
e0, ..., en+1 into a three-layer BiLSTM network to
get c0, ..., cn+1, where ci = [fi; bi], fi and bi are
the forward and backward hidden states of the last
BiLSTM layer at position i respectively. We use
hk = [fk, bk+1] to represent the kth boundary lying
between xk and xk+1, and use ei,j = hj − hi−1 to
represent span (i, j) from position i to j inclusive
where 1 ≤ i ≤ j ≤ n. Then we compute:

• sleft

• sarc

• sspan

i,j (for arc xi → xj, used in all three models)
by feeding ci, cj into a deep biafﬁne function
(Dozat and Manning, 2017).
i,j,k (for headed-span (i, j, k) where xk is
the headword of span (i, j), used in §3.1) by
feeding ei,j, ck to a deep biafﬁne function.
k,i and sright
(for headed-span (i, j, k), used
in §3.2 and §3.3) by feeding ck, hi−1 and
ck, hj into two different deep biafﬁne func-
tions.
i,j,k (for adjacent siblings xi → {xj, xk}
with k < j < i or i < j < k, used in §3.3) by
feeding ci, ck, cj into a deep triafﬁne function
(Zhang et al., 2020).

k,j

• ssib

2.2 Learning

We decompose the training loss L into Lparse +
Llabel. For Lparse, we use the max-margin loss
(Taskar et al., 2004):

Lparse = max(0, max
y(cid:48)(cid:54)=y

(s(y(cid:48)) + ∆(y(cid:48), y) − s(y))

(1)
where ∆ measures the difference between the in-
correct tree and gold tree y. Here we let ∆ to
be the Hamming distance (i.e., the total number
of mismatches of arcs, sibling pairs, and (split)
headed-spans depending on the setting). We use
cost-augmented inference (Taskar et al., 2005) to
compute Eq. 1, which involves the use of parsing
algorithms described in the next section. We use
the same label loss Llabel in Dozat and Manning
(2017).

2 Scoring and Learning

3 Parsing

2.1 Scoring

Given an input sentences x1, ..., xn, we add <bos>
(beginning of sentence) and <eos> (end of sen-
tence) as x0 and xn+1. We apply mean-pooling
at the last layer of BERT (Devlin et al., 2019)

We use the parsing-as-deduction framework
(Pereira and Warren, 1983) to describe the pars-
ing algorithms of our proposed models.

2For some datasets requiring the use of gold POS tags, we
additionally concatenate the POS tag embedding to obtain ei

s1

s2

s1

R-COMB:

i h k

h

k + 1

j

s1 + s2

i

h

j

R-LINK:

i c

j

s1 + sarc
h,c

FINISH:

h

i

j

s1

s2

s1

s1

i

h

j

s1 + sspan
i,j,h

i

h

j

L-COMB:

i

k − 1 h

k h j

s1 + s2

i

h

j

L-LINK:

i c

j

s1 + sarc
h,c

i

j

h

Figure 2: Deduction rules for our modiﬁed Eisner-Satta algorithm (Eisner and Satta, 1999). Our modiﬁcations are
highlighted in red. All deduction items are annotated with their scores. Note that “ﬁnished” spans are marked by
double underlines, whereas “unﬁnished’ spans take the original triangle notations.

R-COMB:

L-COMB:

COMB:

s1

s2

i

k

k

j

s1 + s2

i

j

s2

s1

j

k

k

i

s1 + s2

j

i

s1

s2

j k − 1

k

i

s1 + s2

j

i

R-LINK:

L-LINK:

L-LINK-2

s1

s2

i

k

k + 1 j

s1 + s2 + sarc
i,j

i

j

s1

s2

j k − 1

k

i

s1 + s2 + sarc
i,j

j

i

s2

s1

j

k

k

i

s1 + s2 + sarc
i,j
+ssib

i,k,j

R-FINISH:

L-FINISH:

s1

i

j

s1 + sright
i,j

i

j

s1

j

i

s1 + sleft
i,j

i

j

s1

s1

R-LINK-2

i

k

k

j

s1 + s2 + sarc
i,j
+ssib

i,k,j

j

i

i

j

Figure 3: Deduction rules for our modiﬁed Eisner algorithm (Eisner, 1997) (ﬁrst two rows) and its second-order
extension (McDonald and Pereira, 2006) (all rows). Our modiﬁcations are highlighted in red. All deduction items
are annotated with their scores. Note that “ﬁnished” (in)complete spans are marked by double underlines.

3.1 O(n4) modiﬁed Eisner-Satta algorithm

In this case, we combine ﬁrst-order graph-based
parsing and headed-span-based parsing. The score
of a dependency tree y is deﬁned as:
(cid:88)
(cid:88)

s(y) =

sarc
i,j +

sspan
li,ri,i

Satta, 1999, Sec. 3) is originally deﬁned with bilex-
icalized PCFGs. Still, we can leverage its dynamic
programming substructure to incorporate both arc
scores and headed span scores, similar to the rela-
tionship between span-based constituency parsing
(Stern et al., 2017) and PCFG parsing. The ax-

(xi→xj )∈y

(li,ri,xi)∈y

We adapt the Eisner-Satta algorithm for pars-
ing. The O(n4) Eisner-Satta algorithm (Eisner and

iom items are

with initial score 0 and

i

i

i

the deduction rules are listed in Fig. 2. Unlike
the original Eisner-Satta algorithm, we distinguish
between “ﬁnished” spans and “unﬁnished” spans.
An “unﬁnished” span can absorb a child span to
form a larger span, while in a “ﬁnished” span, the
headword has already generated all its children,
so it cannot expand anymore and corresponds to
a headed-span for the given headword. By explic-
itly distinguishing between “unﬁnished“ spans and
“ﬁnished“ spans, we can incorporate headed-span
scores sspan into parsing via the newly introduced
rule FINISH. We then modify the rule L-LINK
and R-LINK accordingly as only a “ﬁnished” span
can be attached.

3.2 O(n3) modifed Eisner algorithm

In order to decrease the parsing time complexity
from O(n4) to O(n3), we decompose sspan
l,r,i into
two terms:

s(y) =

(cid:88)

sarc
i,j +

(cid:88)

(sleft
i,li

+ sright
i,ri

)

(xi→xj )∈y

(li,ri,xi)∈y

and modify the Eisner algorithm accordingly. The

axiom items are

i

i

and

i

i

with initial

score 0 and the deduction rules are shown in the
ﬁrst two rows of Fig. 3. Similar to the case in the
previous subsection, the original Eisner algorithm
does not distinguish between “ﬁnished” complete
spans and “unﬁnished” complete spans. An “unﬁn-
ished” complete span can absorb another complete
span to form a larger incomplete span, while a “ﬁn-
ished” complete span has no more child in the given
direction and thus cannot expand anymore. We in-
troduce new rules L-FINISH and R-FINISH to
incorporate the left or right span boundary scores
respectively, and adjust other rules accordingly.

3.3 O(n3) modiﬁed second-order Eisner

algorithm

We further enhance the model with adjacent sibling
information:

s(y) =

(cid:88)

sarc
i,j +

(cid:88)

ssib
i,j,k

(xi→xj )∈y
(cid:88)

+

(li,ri,xi)∈y

(xi→{xj ,xk})∈y
+ sright
i,ri

)

(sleft
i,li

2006) by distinguishing between “unﬁnished” and
“ﬁnished” complete spans. The additional deductive
rules for second-order parsing are shown in the last
row of Fig. 3 and the length of the “unﬁnished”
complete span is forced to be 1 in the rule L-LINK
and R-LINK.

4 Experiments

4.1 Setup

We conduct experiments on in Penn Treebank
(PTB) 3.0 (Marcus et al., 1993), Chinese Treebank
(CTB) 5.1 (Xue et al., 2005) and 12 languages on
Universal Dependencies (UD) 2.2. Implementation
details are shown in appendix A. The reported re-
sults are averaged over three runs with different
random seeds.

4.2 Main result

Table 1 and 2 show the results on UD, PTB
and CTB respectively. We additionally reim-
plement Biaffine+2O+MM by replacing the
TreeCRF loss of Zhang et al. (2020) with the
max-margin loss for
fair comparison. We
to our proposed models as 1O+Span
refer
(§3.1), 1O+Span+Headsplit (§3.2),
and
2O+Span+Headsplit (§3.3) respectively.

We draw the following observations. (1) Second-
order information is still helpful even with powerful
encoders (i.e., BERT). Biaffine+2O+MM out-
performs Biaffine+MM in almost all cases.
(2) Combining ﬁrst-order graph-based and
headed-span-based methods is effective. Both
1O+Span and 1O+Span+Headsplit beat
Biaffine+MM, Span in almost all cases; have
similar performance to Biaffine+2O+MM. (3)
Decomposing the headed-span scores is useful.
1O+Span+Headsplit has similar performance
to 1O+Span while manages to decrease the
parsing complexity from O(n4) to O(n3). We
speculate that powerful encoders mitigate the
issue of independent scoring.
(4) Combining
second-order graph-based and headed-span-based
methods has marginal effects. We speculate that
the utility of adjacent sibling information and
headed span information is overlapping.

where for each adjacent sibling part xi → {xj, xk},
xj and xk are two adjacent dependents of xi.

Similarly, we modify the second-order extension
of the Eisner algorithm (McDonald and Pereira,

4.3 Error analysis

Following (McDonald and Nivre, 2011), we plot
UAS as a function of sentence length; F1 scores as
functions of distance to root and dependency length

bg

ca

cs

de

en

es

fr

it

nl

no

ro

ru

Avg

+BERTmultilingual

Biafﬁne+MM†
Span
1O+Span
1O+Span+Headsplit

Biafﬁne+2O+MM
2O+Span+Headsplit

90.30
91.10
91.44
91.46

91.58
91.82

94.49
94.46
94.54
94.53

94.48
94.58

92.65
92.57
92.68
92.63

92.69
92.59

85.98
85.87
85.75
85.78

85.72
85.65

91.13
91.32
91.23
91.25

91.28
91.28

93.78
93.84
93.84
93.77

93.80
93.86

91.77
91.69
91.67
91.91

91.89
91.80

94.72
94.78
94.97
94.88

94.88
94.75

91.04
91.65
91.81
91.59

91.30
91.50

94.21
94.28
94.35
94.18

94.23
94.40

87.24
87.48
87.17
87.45

87.55
87.71

94.53
94.45
94.49
94.47

94.55
94.51

91.82
91.96
91.99
91.99

92.00
92.04

MFVI2O

91.30

93.60

92.09

82.00

90.75

92.62

89.32

93.66

91.21

91.74

86.40

92.61

90.61

For reference

Table 1: Labeled Attachment Score (LAS) on twelves languages in UD 2.2. We use ISO 639-1 codes to represent
languages. † means reported by Yang and Tu (2022). MFVI2O: Wang and Tu (2020). Span: Yang and Tu (2022).

PTB

CTB

UAS

LAS

UAS

LAS

+BERTlarge

+BERTbase

97.22
97.24
97.26
97.30
97.28
97.23

95.71
95.73
95.68
95.77
95.73
95.69

For reference

96.91
97.01

95.34
95.48

+XLNetlarge

97.20
97.42

95.72
96.26

93.18
93.33
93.56
93.46
93.42
93.57

92.10
92.30
92.49
92.42
92.34
92.47

92.55
92.65

91.69
91.47

+BERTbase
-
-
89.28
94.56

Biafﬁne+MM†
Span
1O+Span
1O+Span+HeadSplit
Biafﬁne+2O+MM
2O+Span+HeadSplit

MFVI2O
HierPtr

HPSG(cid:91)
HPSG+LAL(cid:91)

Table 2: Results on PTB and CTB. (cid:91) denotes use of ad-
ditional constituency tree data and thus not comparable
to our work. † denotes results reported by Yang and Tu
(2022). HPSG: Zhou and Zhao (2019); HPSG+LAL:
Mrini et al. (2020); HierPtr: Fernández-González and
Gómez-Rodríguez (2021).

on the CTB test set. We also follow (Yang and Tu,
2022) to plot F1 score as a function of span length.
Fig. 4a shows that compared with ﬁrst-order
graph-based method (i.e., Biafﬁne+MM), headed-
span-based method (i.e., Span) has an advantage
in predicting long sentences (of length > 30) but
has a difﬁculty in predicting short sentences (of
length < 20). By combining ﬁrst-order graph-based
and headed-span-based methods, 1O+Span can
predict both short and long sentences correctly. It
achieves the best results for all sentence length
intervals except for 30-39. Fig. 4b shows that
1O+Span achieves the best performance for almost
all cases, indicating its strong ability in predicting
complex subtrees with high tree depth. Also, Fig.
4c shows that 1O+Span achieves the best perfor-
mance for almost all cases, especially for depen-
dency arcs of length ≥ 6, showing its ability in
capturing long-range dependencies. Fig. 4d shows

96

95

94

93

92

95

90

85

80

)

%
0
0
1
(
S
A
U

)

%
0
0
1
(

e
r
o
c
s

1
F

1-9

1

2

Span
Biafﬁne+MM
1O+Span

95

94

93

92

91

90

)

%
0
0
1
(

e
r
o
c
s

1
F

Span
Biafﬁne+MM
1O+span

10-19

20-29
Sentence length

30-39

≥40

ROOT 1

3

2
5
Distance to root

4

6 ≥7

(a)

(b)

Span
Biafﬁne+MM
1O+span

)

%
0
0
1
(

e
r
o
c
s

1
F

95

90

85

80

Span
Biafﬁne+MM
1O+Span

4

3
6
Dependency length

5

7 ≥8

1-10

11-20

21-30
Span length

31-40

≥40

(c)

(d)

Figure 4: Error analysis on the CTB test set.

that Span has the best performance in identifying
the range of a subtree, although it has no direct
relation to the ﬁnal performance.

5 Conclusion

In this paper, we have studied different ways to
combine graph-based and headed-span-based meth-
ods. We found that applying the head-splitting trick
can retain the cubic parsing complexity and mean-
while improve parsing performance when combin-
ing ﬁrst-order graph-based and headed-span-based
methods. We also conﬁrmed the effectiveness
of second-order parsing, however, we observed
marginal or no improvement when combining it
with the headed-span-based method.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments. This work was supported by
the National Natural Science Foundation of China
(61976139).

References

Xavier Carreras. 2007. Experiments with a higher-
In Proceed-
order projective dependency parser.
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 957–961, Prague, Czech Republic.
Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
In 5th International Conference on Learning
ing.
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. Open-
Review.net.

Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the Fifth
International Workshop on Parsing Technologies,
pages 54–65, Boston/Cambridge, Massachusetts,
USA. Association for Computational Linguistics.

Jason Eisner and Giorgio Satta. 1999. Efﬁcient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics, pages 457–464, College Park, Maryland,
USA. Association for Computational Linguistics.

Agnieszka Falenska and Jonas Kuhn. 2019. The (non-
)utility of structural features in BiLSTM-based de-
In Proceedings of the 57th An-
pendency parsers.
nual Meeting of the Association for Computational
Linguistics, pages 117–128, Florence, Italy. Associ-
ation for Computational Linguistics.

Daniel Fernández-González

and Carlos Gómez-
Rodríguez. 2021.
Dependency parsing with
bottom-up hierarchical pointer networks. CoRR,
abs/2105.09611.

Erick Fonseca and André F. T. Martins. 2020. Re-
In Pro-
visiting higher-order dependency parsers.
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 8795–
8800, Online. Association for Computational Lin-
guistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,

ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Terry Koo and Michael Collins. 2010. Efﬁcient third-
In Proceedings of the
order dependency parsers.
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1–11, Uppsala, Sweden.
Association for Computational Linguistics.

Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
In Proceedings of COLING 2012:
dency parsing.
Posters, pages 785–796, Mumbai, India. The COL-
ING 2012 Organizing Committee.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
the 43rd
pendency parsers.
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 91–98, Ann Ar-
bor, Michigan. Association for Computational Lin-
guistics.

In Proceedings of

Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197–230.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, Trento, Italy. Association for Computational
Linguistics.

Khalil Mrini, Franck Dernoncourt, Quan Hung Tran,
Trung Bui, Walter Chang, and Ndapa Nakashole.
2020. Rethinking self-attention: Towards inter-
pretability in neural parsing. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2020, pages 731–742, Online. Association for Com-
putational Linguistics.

Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In 21st Annual Meeting of the
Association for Computational Linguistics, pages
137–144, Cambridge, Massachusetts, USA. Associ-
ation for Computational Linguistics.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A
minimal span-based neural constituency parser. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 818–827, Vancouver, Canada.
Association for Computational Linguistics.

Ben Taskar, Dan Klein, Mike Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1–8, Barcelona, Spain. Association for Com-
putational Linguistics.

Benjamin Taskar, Vassil Chatalbashev, Daphne Koller,
and Carlos Guestrin. 2005. Learning structured pre-
In Ma-
diction models: a large margin approach.
chine Learning, Proceedings of the Twenty-Second
International Conference (ICML 2005), Bonn, Ger-
many, August 7-11, 2005, volume 119 of ACM Inter-
national Conference Proceeding Series, pages 896–
903. ACM.

Xinyu Wang and Kewei Tu. 2020. Second-order neural
dependency parsing with message passing and end-
In Proceedings of the 1st Confer-
to-end training.
ence of the Asia-Paciﬁc Chapter of the Association
for Computational Linguistics and the 10th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 93–99, Suzhou, China. Association
for Computational Linguistics.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Nat. Lang.
Eng., 11(2):207–238.

Songlin Yang and Kewei Tu. 2022. Headed-span-based

projective dependency parsing. In ACL.

Yu Zhang, Zhenghua Li, and Min Zhang. 2020. Efﬁ-
cient second-order TreeCRF for neural dependency
In Proceedings of the 58th Annual Meet-
parsing.
ing of the Association for Computational Linguistics,
pages 3295–3305, Online. Association for Computa-
tional Linguistics.

Junru Zhou and Hai Zhao. 2019. Head-Driven Phrase
In
Structure Grammar parsing on Penn Treebank.
Proceedings of
the
the 57th Annual Meeting of
Association for Computational Linguistics, pages
2396–2408, Florence, Italy. Association for Compu-
tational Linguistics.

A Implementation details

We use "bert-large-cased" for PTB, "bert-base-
chinese" for CTB, and "bert-multilingual-cased"
for UD. We set the hidden size of BiLSTM to
1000. We set the hidden size of biafﬁne functions
to 600/300 for spans,arcs/labels. We set the hid-
den size of triafﬁne functions to 400. We add a
dropout layer after the embedding layer, LSTM
layers, and MLP layers with dropout rate 0.33. We
use Adam (Kingma and Ba, 2015) as the optimizer
with β1 = 0.9, β2 = 0.9 to train our model for
10 epochs with gradient clipping of 5. The max-
imal learning rate is lr = 5e − 5 for BERT and
lr = 25e − 5 for other components. We linearly
warmup the learning rate to their maximal value
for the ﬁrst epoch and gradually decay them to zero
for the rest of the epochs. We batch sentences of
similar lengths so that the token number is 4000
for each batch.

