1
2
0
2

b
e
F
1
2

]

G
L
.
s
c
[

1
v
6
9
4
0
1
.
2
0
1
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2021

TARGETED ATTACK AGAINST DEEP NEURAL NET-
WORKS VIA FLIPPING LIMITED WEIGHT BITS

Jiawang Bai 1, 2 †, Baoyuan Wu 3, 4, Yong Zhang 5, Yiming Li 1, Zhifeng Li 5, Shu-Tao Xia 1, 2
1 Tsinghua Shenzhen International Graduate School, Tsinghua University
2 PCL Research Center of Networks and Communications, Peng Cheng Laboratory
3 School of Data Science, The Chinese University of Hong Kong, Shenzhen
4 Secure Computing Lab of Big Data, Shenzhen Research Institute of Big Data
5 Tencent AI Lab

ABSTRACT

To explore the vulnerability of deep neural networks (DNNs), many attack
paradigms have been well studied, such as the poisoning-based backdoor attack
in the training stage and the adversarial attack in the inference stage. In this pa-
per, we study a novel attack paradigm, which modiﬁes model parameters in the
deployment stage for malicious purposes. Speciﬁcally, our goal is to misclassify
a speciﬁc sample into a target class without any sample modiﬁcation, while not
signiﬁcantly reduce the prediction accuracy of other samples to ensure the stealth-
iness. To this end, we formulate this problem as a binary integer programming
(BIP), since the parameters are stored as binary bits (i.e., 0 and 1) in the mem-
ory. By utilizing the latest technique in integer programming, we equivalently
reformulate this BIP problem as a continuous optimization problem, which can be
effectively and efﬁciently solved using the alternating direction method of mul-
tipliers (ADMM) method. Consequently, the ﬂipped critical bits can be easily
determined through optimization, rather than using a heuristic strategy. Extensive
experiments demonstrate the superiority of our method in attacking DNNs. The
code is available at: https://github.com/jiawangbai/TA-LBF.

1

INTRODUCTION

Due to the great success of deep neural networks (DNNs), its vulnerability (Szegedy et al., 2014;
Gu et al., 2019) has attracted great attention, especially for security-critical applications (e.g., face
recognition (Dong et al., 2019) and autonomous driving (Eykholt et al., 2018)). For example, back-
door attack (Saha et al., 2020; Xie et al., 2019) manipulates the behavior of the DNN model by
mainly poisoning some training data in the training stage; adversarial attack (Goodfellow et al.,
2015; Moosavi-Dezfooli et al., 2017) aims to fool the DNN model by adding malicious perturba-
tions onto the input in the inference stage.

Compared to the backdoor attack and adversarial attack, a novel attack paradigm, dubbed weight
attack (Breier et al., 2018), has been rarely studied. It assumes that the attacker has full access to
the memory of a device, such that he/she can directly change the parameters of a deployed model
to achieve some malicious purposes (e.g., crushing a fully functional DNN and converting it to a
random output generator (Rakin et al., 2019)). Since weight attack neither modiﬁes the input nor
control the training process, both the service provider and the user are difﬁcult to realize the existence
of the attack. In practice, since the deployed DNN model is stored as binary bits in the memory,
the attacker can modify the model parameters using some physical fault injection techniques, such
as Row Hammer Attack (Agoyan et al., 2010; Selmke et al., 2015) and Laser Beam Attack (Kim
et al., 2014). These techniques can precisely ﬂip any bit of the data in the memory. Some previous
works (Rakin et al., 2019; 2020a;b) have demonstrated that it is feasible to change the model weights
via bit ﬂipping to achieve some malicious purposes. However, the critical bits are identiﬁed mostly

†This work was done when Jiawang Bai was an intern at Tencent AI Lab.
Correspondence to: Baoyuan Wu (wubaoyuan@cuhk.edu.cn) and Shu-Tao Xia (xiast@sz.tsinghua.edu.cn).

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2021

Figure 1: Demonstration of our proposed attack against a deployed DNN in the memory. By ﬂipping
critical bits (marked in red), our method can mislead a speciﬁc sample into the target class without
any sample modiﬁcation while not signiﬁcantly reduce the prediction accuracy of other samples.

using some heuristic strategies in their methods. For example, Rakin et al. (2019) combined gradient
ranking and progressive search to identify the critical bits for ﬂipping.

This work also focuses on the bit-level weight attack against DNNs in the deployment stage, whereas
with two different goals, including effectiveness and stealthiness. The effectiveness requires that
the attacked model can misclassify a speciﬁc sample to a attacker-speciﬁed target class without
any sample modiﬁcation, while the stealthiness encourages that the prediction accuracy of other
samples will not be signiﬁcantly reduced. As shown in Fig. 1, to achieve these goals, we propose
to identify and ﬂip bits that are critical to the prediction of the speciﬁc sample but not signiﬁcantly
impact the prediction of other samples. Speciﬁcally, we treat each bit in the memory as a binary
variable, and our task is to determine its state (i.e., 0 or 1). Accordingly, it can be formulated as a
binary integer programming (BIP) problem. To further improve the stealthiness, we also limit the
number of ﬂipped bits, which can be formulated as a cardinality constraint. However, how to solve
the BIP problem with a cardinality constraint is a challenging problem. Fortunately, inspired by
an advanced optimization method, the (cid:96)p-box ADMM (Wu & Ghanem, 2018), this problem can be
reformulated as a continuous optimization problem, which can further be efﬁciently and effectively
solved by the alternating direction method of multipliers (ADMM) (Glowinski & Marroco, 1975;
Gabay & Mercier, 1976). Consequently, the ﬂipped bits can be determined through optimization
rather than the original heuristic strategy, which makes our attack more effective. Note that we
also conduct attack against the quantized DNN models, following the setting in some related works
(Rakin et al., 2019; 2020a). Extensive experiments demonstrate the superiority of the proposed
method over several existing weight attacks. For example, our method achieves a 100% attack
success rate with 7.37 bit-ﬂips and 0.09% accuracy degradation of the rest unspeciﬁc inputs in
attacking a 8-bit quantized ResNet-18 model on ImageNet. Moreover, we also demonstrate that the
proposed method is also more resistant to existing defense methods.

The main contributions of this work are three-fold. 1) We explore a novel attack scenario where the
attacker enforces a speciﬁc sample to be predicted as a target class by modifying the weights of a
deployed model via bit ﬂipping without any sample modiﬁcation. 2) We formulate the attack as a
BIP problem with the cardinality constraint and propose an effective and efﬁcient method to solve
this problem. 3) Extensive experiments verify the superiority of the proposed method against DNNs
with or without defenses.

2 RELATED WORKS

Neural Network Weight Attack. How to perturb the weights of a trained DNN for malicious
purposes received extensive attention (Liu et al., 2017a; 2018b; Hong et al., 2019). Liu et al.
(2017a) ﬁrstly proposed two schemes to modify model parameters for misclassiﬁcation without
and with considering stealthiness, which is dubbed single bias attack (SBA) and gradient descent

2

010110010100110001101001111101010010111000110101101001010100100110011000111010011001011001001110101110101100000110010100Attacker: identify and flip critical bits…A Specific SampleOther SamplesBehave NormallyClassified into the target class…A Specific SampleOther SamplesBehave NormallyDo not modify samples010110010100110001101001101101010010111000110101101001010110100110011000111011011001011001001110101110101100000110010100DNN in the memoryPublished as a conference paper at ICLR 2021

attack (GDA) respectively. After that, Trojan attack (Liu et al., 2018b) was proposed, which injects
malicious behavior to the DNN by generating a general trojan trigger and then retraining the model.
This method requires to change lots of parameters. Recently, fault sneaking attack (FSA) (Zhao
et al., 2019) was proposed, which aims to misclassify certain samples into a target class by modify-
ing the DNN parameters with two constraints, including maintaining the classiﬁcation accuracy of
other samples and minimizing parameter modiﬁcations. Note that all those methods are designed
to misclassify multiple samples instead of a speciﬁc sample, which may probably modify lots of
parameters or degrade the accuracy of other samples sharply.

Bit-Flip based Attack. Recently, some physical fault injection techniques (Agoyan et al., 2010;
Kim et al., 2014; Selmke et al., 2015) were proposed, which can be adopted to precisely ﬂip any bit
in the memory. Those techniques promote researchers to study how to modify model parameters at
the bit-level. As a branch of weight attack, the bit-ﬂip based attack was ﬁrstly explored in (Rakin
It proposed an untargeted attack that can convert the attacked DNN to a random
et al., 2019).
output generator with several bit-ﬂips. Besides, Rakin et al. (2020a) proposed the targeted bit Trojan
(TBT) to inject the fault into DNNs by ﬂipping some critical bits. Speciﬁcally, the attacker ﬂips the
identiﬁed bits to force the network to classify all samples embedded with a trigger to a certain
target class, while the network operates with normal inference accuracy with benign samples. Most
recently, Rakin et al. (2020b) proposed the targeted bit-ﬂip attack (T-BFA), which achieves malicious
purposes without modifying samples. Speciﬁcally, T-BFA can mislead samples from single source
class or all classes to a target class by ﬂipping the identiﬁed weight bits. It is worth noting that the
above bit-ﬂip based attacks leverage heuristic strategies to identify critical weight bits. How to ﬁnd
critical bits for the bit-ﬂip based attack method is still an important open question.

3 TARGETED ATTACK WITH LIMITED BIT-FLIPS (TA-LBF)

3.1 PRELIMINARIES

Storage and Calculation of Quantized DNNs. Currently, it is a widely-used technique to quantize
DNNs before deploying on devices for efﬁciency and reducing storage size. For each weight in
l-th layer of a Q-bit quantized DNN, it will be represented and then stored as the signed integer in
two’s complement representation (v = [vQ; vQ−1; ...; v1] ∈ {0, 1}Q) in the memory. Attacker can
modify the weights of DNNs through ﬂipping the stored binary bits. In this work, we adopt the
layer-wise uniform weight quantization scheme similar to Tensor-RT (Migacz, 2017). Accordingly,
each binary vector v can be converted to a real number by a function h(·), as follow:

h(v) = (−2Q−1 · vQ +

Q−1
(cid:88)

i=1

2i−1 · vi) · ∆l,

(1)

where l indicates which layer the weight is from, ∆l > 0 is a known and stored constant which
represents the step size of the l-th layer weight quantizer.

Notations. We denote a Q-bit quantized DNN-based classiﬁcation model as f : X → Y, where
X ∈ Rd being the input space and Y ∈ {1, 2, ..., K} being the K-class output space. Assuming
that the last layer of this DNN model is a fully-connected layer with B ∈ {0, 1}K×C×Q being
the quantized weights, where C is the dimension of last layer’s input. Let Bi,j ∈ {0, 1}Q be the
two’s complement representation of a single weight and Bi ∈ {0, 1}C×Q denotes all the binary
weights connected to the i-th output neuron. Given a test sample x with the ground-truth label s,
f (x; Θ, B) ∈ [0, 1]K is the output probability vector and g(x; Θ) ∈ RC is the input of the last
layer, where Θ denotes the model parameters without the last layer.

Attack Scenario. In this paper, we focus on the white-box bit-ﬂip based attack, which was ﬁrst
introduced in (Rakin et al., 2019). Speciﬁcally, we assume that the attacker has full knowledge of
the model (including it’s architecture, parameters, and parameters’ location in the memory), and can
precisely ﬂip any bit in the memory. Besides, we also assume that attackers can have access to a
small portion of benign samples, but they can not tamper the training process and the training data.

Attacker’s Goals. Attackers have two main goals, including the effectiveness and the stealthiness.
Speciﬁcally, effectiveness requires that the attacked model can misclassify a speciﬁc sample to a pre-
deﬁned target class without any sample modiﬁcation, and the stealthiness requires that the prediction
accuracy of other samples will not be signiﬁcantly reduced.

3

Published as a conference paper at ICLR 2021

3.2 THE PROPOSED METHOD

Loss for Ensuring Effectiveness. Recall that our ﬁrst target is to force a speciﬁc image to be
classiﬁed as the target class by modifying the model parameters at the bit-level. To this end, the
most straightforward way is maximizing the logit of the target class while minimizing that of the
source class. For a sample x, the logit of a class can be directly determined by the input of the last
layer g(x; Θ) and weights connected to the node of that class. Accordingly, we can modify weights
only connected to the source and target class to fulﬁll our purpose, as follows:

L1(x; Θ, B, ˆBs, ˆBt) = max (cid:0)m − p(x; Θ, ˆBt) + δ, 0(cid:1) + max (cid:0)p(x; Θ, ˆBs) − m + δ, 0(cid:1),
(2)
where p(x; Θ, ˆBi) = [h( ˆBi,1); h( ˆBi,2); ...; h( ˆBi,C)](cid:62)g(x; Θ) denotes the logit of class i (i = s
p(x; Θ, Bi), and δ ∈ R
or i = t), h(·) is the function deﬁned in Eq.

(1), m =

max
i∈{0,...,K}\{s}

indicates a slack variable, which will be speciﬁed in later experiments. The ﬁrst term of L1 aims at
increasing the logit of the target class, while the second term is to decrease the logit of the source
class. The loss L1 is 0 only when the output on target class is more than m + δ and the output on
source class is less than m − δ. That is, the prediction on x of the target model is the predeﬁned
target class. Note that ˆBs, ˆBt ∈ {0, 1}C×Q are two variables we want to optimize, corresponding to
the weights of the fully-connected layer w.r.t. class s and t, respectively, in the target DNN model.
B ∈ {0, 1}K×C×Q denotes the weights of the fully-connected layer of the original DNN model, and
it is a constant tensor in L1. For clarity, hereafter we simplify L1(x; Θ, B, ˆBs, ˆBt) as L1( ˆBs, ˆBt),
since x and Θ are also provided input and weights.

Loss for Ensuring Stealthiness. As we mentioned in Section 3.1, we assume that the attacker can
get access to an auxiliary sample set {(xi, yi)}N
i=1. Accordingly, the stealthiness of the attack can
be formulated as follows:

L2( ˆBs, ˆBt) =

N
(cid:88)

i=1

(cid:96)(f (xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt), yi),

(3)

where B{1,...,K}\{s,t} denotes {B1, B2, ..., BK}\{Bs, Bt}, and fj(xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt)
indicates the posterior probability of xi w.r.t. class j, caclulated by Softmax(p(xi; Θ, ˆBj)) or
Softmax(p(xi; Θ,Bj)). (cid:96)(·, ·) is speciﬁed by the cross entropy loss. To keep clarity, xi, Θ and
B{1,...,K}\{s,t} are omitted in L2( ˆBs, ˆBt) .
Besides, to better meet our goal, a straightforward additional approach is reducing the magnitude of
the modiﬁcation. In this paper, we constrain the number of bit-ﬂips less than k. Physical bit ﬂipping
techniques can be time-consuming as discussed in (Van Der Veen et al., 2016; Zhao et al., 2019).
Moreover, such techniques lead to abnormal behaviors in the attacked system (e.g., suspicious cache
activity of processes), which may be detected by some physical detection-based defenses (Gruss
et al., 2018). As such, minimizing the number of bit-ﬂips is critical to make the attack more efﬁcient
and practical.

Overall Objective. In conclusion, the ﬁnal objective function is as follows:

L1( ˆBs, ˆBt) + λL2( ˆBs, ˆBt),

min
ˆBs,ˆBt

(4)

s.t. ˆBs ∈ {0, 1}C×Q, ˆBt ∈ {0, 1}C×Q, dH (Bs, ˆBs) + dH (Bt, ˆBt) ≤ k,

where dH (·, ·) denotes the Hamming distance and λ > 0 is a trade-off parameter.
For the sake of brevity, Bs and Bt are concatenated and further reshaped to the vector b ∈ {0, 1}2CQ.
Similarly, ˆBs and ˆBt are concatenated and further reshaped to the vector ˆb ∈ {0, 1}2CQ. Besides,
for binary vector b and ˆb, there exists a nice relationship between Hamming distance and Euclidean
distance: dH (b, ˆb) = ||b − ˆb||2

2. The new formulation of the objective is as follows:
2 − k ≤ 0.

s.t. ˆb ∈ {0, 1}2CQ,

||b − ˆb||2

L1(ˆb) + λL2(ˆb),

(5)

min
ˆb

Problem (5) is denoted as TA-LBF (targeted attack with limited bit-ﬂips). Note that TA-LBF is a
binary integer programming (BIP) problem, whose optimization is challenging. We will introduce
an effective and efﬁcient method to solve it in the following section.

4

Published as a conference paper at ICLR 2021

3.3 AN EFFECTIVE OPTIMIZATION METHOD FOR TA-LBF

To solve the challenging BIP problem (5), we adopt the generic solver for integer programming,
dubbed (cid:96)p-Box ADMM (Wu & Ghanem, 2018). The solver presents its superior performance in
many tasks, e.g., model pruning (Li et al., 2019), clustering (Bibi et al., 2019), MAP inference (Wu
et al., 2020a), adversarial attack (Fan et al., 2020), etc.. It proposed to replace the binary constraint
equivalently by the intersection of two continuous constraints, as follows

ˆb ∈ {0, 1}2CQ ⇔ ˆb ∈ (Sb ∩ Sp),

(6)

where Sb = [0, 1]2CQ indicates the box constraint, and Sp = {ˆb : ||ˆb − 1
2 ||2
(cid:96)2-sphere constraint. Utilizing (6), Problem (5) is equivalently reformulated as
s.t. ˆb = u1, ˆb = u2, ||b − ˆb||2

L1(ˆb) + λL2(ˆb),

min
ˆb,u1∈Sb,u2∈Sp,u3∈R+

2 = 2CQ

4 } denotes the

2 − k + u3 = 0,

(7)

where two extra variables u1 and u2 are introduced to split the constraints w.r.t. ˆb. Besides, the non-
negative slack variable u3 ∈ R+ is used to transform ||b−ˆb||2
2−k+u3 =
0. The above constrained optimization problem can be efﬁciently solved by the alternating direction
method of multipliers (ADMM) (Boyd et al., 2011).

2−k ≤ 0 in (5) into ||b−ˆb||2

Following the standard procedure of ADMM, we ﬁrstly present the augmented Lagrangian function
of the above problem, as follows:

L(ˆb, u1, u2, u3, z1, z2, z3) =L1(ˆb) + λL2(ˆb) + z(cid:62)

1 (ˆb − u1) + z(cid:62)

2 (ˆb − u2)

+z3(||b − ˆb||2
||ˆb − u1||2

+

ρ1
2

2 +

ρ2
2

2 − k + u3) + c1(u1) + c2(u2) + c3(u3)

(8)

||ˆb − u2||2

2 +

(||b − ˆb||2

2 − k + u3)2,

ρ3
2

where z1, z2 ∈ R2CQ and z3 ∈ R are dual variables, and ρ1, ρ2, ρ3 > 0 are penalty factors, which
will be speciﬁed later. c1(u1) = I{u1∈Sb}, c2(u2) = I{u2∈Sp}, and c3(u3) = I{u3∈R+} capture
the constraints Sb, Sp and R+, respectively. The indicator function I{a} = 0 if a is true; otherwise,
I{a} = +∞. Based on the augmented Lagrangian function, the primary and dual variables are
updated iteratively, with r indicating the iteration index.

Given (ˆbr, zr
3), update (ur+1
pendent, and they can be optimized in parallel, as follows

, ur+1
2

, ur+1
3

1, zr

2, zr

1

). Given (ˆbr, zr

1, zr

2, zr

3), (u1, u2, u3) are inde-






ur+1

ur+1

1 = arg min
u1∈Sb
2 = arg min
u2∈Sp
ur+1
3 = arg min
u3∈R+

(zr

(zr

1)(cid:62)(ˆbr − u1) + ρ1
2)(cid:62)(ˆbr − u2) + ρ2
3(||b − ˆbr||2
zr

2 ||ˆbr − u1||2
2 ||ˆbr − u2||2

2 = PSb (ˆbr + zr
2 = PSp (ˆbr + zr

1
ρ1

2
ρ2

),

),

2 − k + u3) + ρ3

2 (||b − ˆbr||2

2 − k + u3)2

(9)

= PR+(−||b − ˆbr||2

2 + k − zr

),

3
ρ3
where PSb (a) = min((1, max(0, a)) with a ∈ Rn is the projection onto the box constraint Sb;
PSp (a) =
2 indicates the projection onto the (cid:96)2-sphere constraint Sp
(Wu & Ghanem, 2018); PR+(a) = max(0, a) with a ∈ R indicates the projection onto R+.

2 with ¯a = a − 1

||a|| + 1

n
2

√

¯a

1

, ur+1
2

3), update ˆbr+1. Although there is no closed-form solution
Given (ur+1
to ˆbr+1, it can be easily updated by the gradient descent method, as both L1(ˆb) and L2(ˆb) are
differentiable w.r.t. ˆb, as follows

, ur+1
3

1, zr

2, zr

, zr

ˆbr+1 ← ˆbr − η ·

∂L(ˆb, ur+1

1

, ur+1
, ur+1
2
3
∂ˆb

, zr

1, zr

2, zr
3)

(cid:12)
(cid:12)
(cid:12)ˆb=ˆbr

,

(10)

where η > 0 denotes the step size. Note that we can run multiple steps of gradient descent in the
above update. Both the number of steps and η will be speciﬁed in later experiments. Besides, due to
the space limit, the detailed derivation of ∂L/∂ˆb will be presented in Appendix A.

5

Published as a conference paper at ICLR 2021

1

, ur+1
3

, ur+1
2

Given (ˆbr+1, ur+1
the gradient ascent method, as follows
zr+1
1 = zr
zr+1
2 = zr
zr+1
3 = zr






), update (zr+1

1

, zr+1
2

, zr+1
3

). The dual variables are updated by

1 + ρ1(ˆbr+1 − ur+1
2 + ρ2(ˆbr+1 − ur+1
3 + ρ3(||b − ˆbr+1||2

),
),
2 − k + ur+1

2

1

3

).

(11)

1

, ur+1
3

, ur+1
2

Remarks. 1) Note that since (ur+1
) are updated in parallel, their updates belong to
the same block. Thus, the above algorithm is a two-block ADMM algorithm. We provide the
algorithm outline in Appendix B. 2) Except for the update of ˆbr+1, all other updates are very sim-
ple and efﬁcient. The computational cost of the whole algorithm will be analyzed in Appendix
C. 3) Due to the inexact solution to ˆbr+1 using gradient descent, the theoretical convergence of
the whole ADMM algorithm cannot be guaranteed. However, as demonstrated in many previous
works (Gol’shtein & Tret’yakov, 1979; Eckstein & Bertsekas, 1992; Boyd et al., 2011), the inexact
two-block ADMM often shows good practical convergence, which is also the case in our later ex-
periments. Besides, the numerical convergence analysis is presented in Appendix D. 4) The proper
adjustment of (ρ1, ρ2, ρ3) could accelerate the practical convergence, which will be speciﬁed later .

4 EXPERIMENTS

4.1 EVALUATION SETUP

Settings. We compare our method (TA-LBF) with GDA (Liu et al., 2017a), FSA (Zhao et al., 2019),
T-BFA (Rakin et al., 2020b), and TBT (Rakin et al., 2020a). All those methods can be adopted
to misclassify a speciﬁc image into a target class. We also take the ﬁne-tuning (FT) of the last
fully-connected layer as a baseline method. We conduct experiments on CIFAR-10 (Krizhevsky
et al., 2009) and ImageNet (Russakovsky et al., 2015). We randomly select 1,000 images from each
dataset as the evaluation set for all methods. Speciﬁcally, for each of the 10 classes in CIFAR-10,
we perform attacks on the 100 randomly selected validation images from the other 9 classes. For
ImageNet, we randomly choose 50 target classes. For each target class, we perform attacks on
20 images randomly selected from the rest classes in the validation set. Besides, for all methods
except GDA which does not employ auxiliary samples, we provide 128 and 512 auxiliary samples
on CIFAR-10 and ImageNet, respectively. Following the setting in (Rakin et al., 2020a;b), we
adopt the quantized ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) as the target
models. For our TA-LBF, the trade-off parameter λ and the constraint parameter k affect the attack
stealthiness and the attack success rate. We adopt a strategy for jointly searching λ and k, which is
speciﬁed in Appendix E.3. More descriptions of our settings are provided in Appendix E.

Evaluation Metrics. We adopt three metrics to evaluate the attack performance, i.e., the post attack
accuracy (PA-ACC), the attack success rate (ASR), and the number of bit-ﬂips (Nﬂip). PA-ACC
denotes the post attack accuracy on the validation set except for the speciﬁc attacked sample and the
auxiliary samples. ASR is deﬁned as the ratio of attacked samples that are successfully attacked into
the target class among all 1,000 attacked samples. Nﬂip is the number of bit-ﬂips required for an
attack. A better attack performance corresponds to a higher PA-ACC and ASR, while a lower Nﬂip.
Besides, we also show the accuracy of the original model, denoted as ACC.

4.2 MAIN RESULTS

Results on CIFAR-10. The results of all methods on CIFAR-10 are shown in Table 1. Our method
achieves a 100% ASR with the fewest Nﬂip for all the bit-widths and architectures. FT modiﬁes the
maximum number of bits among all methods since there is no limitation of parameter modiﬁcations.
Due to the absence of the training data, the PA-ACC of FT is also poor. These results indicate that
ﬁne-tuning the trained DNN as an attack method is infeasible. Although T-BFA ﬂips the second-
fewest bits under three cases, it fails to achieve a higher ASR than GDA and FSA. In terms of
PA-ACC, TA-LBF is comparable to other methods. Note that the PA-ACC of TA-LBF signiﬁcantly
outperforms that of GDA, which is the most competitive w.r.t. ASR and Nﬂip among all the baseline
methods. The PA-ACC of GDA is relatively poor, because it does not employ auxiliary samples.
Achieving the highest ASR, the lowest Nﬂip, and the comparable PA-ACC demonstrates that our
optimization-based method is more superior than other heuristic methods (TBT, T-BFA and GDA).

6

Published as a conference paper at ICLR 2021

Table 1: Results of all attack methods across different bit-widths and architectures on CIFAR-10 and
ImageNet (bold: the best; underline: the second best). The mean and standard deviation of PA-ACC
and Nﬂip are calculated by attacking the 1,000 images. Our method is denoted as TA-LBF.
Nﬂip

Dataset Method

Nﬂip

Target
Model

PA-ACC
(%)

ASR
(%)

PA-ACC
(%)

Target
Model

ASR
(%)

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

ResNet
8-bit

ACC:
92.16%

ResNet
4-bit

ACC:
91.90%

ResNet
8-bit

ACC:
69.50%

ResNet
4-bit

ACC:
66.77%

85.01±2.90
88.07±0.84
87.56±2.22
88.38±2.28
86.73±3.50
88.20±2.64
84.37±2.94
87.79±1.86
86.46±2.80
87.73±2.36
86.25±3.59
87.82±2.60

59.33±0.93
69.18±0.03
68.71±0.36
69.27±0.15
69.26±0.22
69.41±0.08
15.65±4.52
66.36±0.07
65.86±0.42
66.44±0.21
66.54±0.22
66.69±0.07

100.0
97.3
98.7
98.9
99.8
100.0
100.0
96.0
97.9
98.4
99.8
100.0

100.0
99.9
79.3
99.7
100.0
100.0
100.0
99.8
80.4
99.9
100.0
100.0

1507.51±86.54
246.70±8.19
9.91±2.33
185.51±54.93
26.83±12.50
5.57±1.58
392.48±47.26
118.20±15.03
8.80±2.01
76.83±25.27
14.08±7.94
5.25±1.09

277424.29±12136.34
577.40±19.42
24.57±20.03
441.21±119.45
18.54±6.14
7.37±2.18
135854.50±21399.94
271.24±15.98
24.79±19.02
157.53±33.66
11.45±3.82
7.96±2.50

VGG
8-bit

ACC:
93.20%

VGG
4-bit

ACC:
92.61%

VGG
8-bit

ACC:
73.31%

VGG
4-bit

ACC:
71.76%

84.31±3.10
77.79±23.35
89.83±3.92
88.80±2.86
85.51±2.88
86.06±3.17
83.31±3.76
83.90±2.63
88.74±4.52
87.58±3.06
85.08±2.82
85.91±3.29

62.08±2.33
72.99±0.02
73.09±0.12
73.28±0.03
73.29±0.02
73.28±0.03
17.76±1.71
71.18±0.03
71.49±0.15
71.69±0.09
71.73±0.03
71.73±0.03

98.7
51.6
96.7
96.8
100.0
100.0
94.5
62.4
96.2
97.5
100.0
100.0

100.0
99.2
84.5
100.0
100.0
100.0
100.0
100.0
84.3
100.0
100.0
100.0

11298.74±830.36
599.40±19.53
14.53±3.74
253.92±122.06
21.54±6.79
7.40±2.72
2270.52±324.69
266.40±18.70
11.23±2.36
75.03±29.75
10.31±3.77
6.26±2.37

1729685.22±137539.54
4115.26±191.25
363.78±153.28
1030.03±260.30
197.05±49.85
69.89±18.42
1900751.70±37329.44
3231.00±345.68
350.33±158.57
441.32±111.26
107.18±28.70
69.72±18.84

Results on ImageNet. The results on ImageNet are shown in Table 1. It can be observed that GDA
shows very competitive performance compared to other methods. However, our method obtains the
highest PA-ACC, the fewest bit-ﬂips (less than 8), and a 100% ASR in attacking ResNet. For VGG,
our method also achieves a 100% ASR with the fewest Nﬂip for both bit-widths. The Nﬂip results of
our method are mainly attributed to the cardinality constraint on the number of bit-ﬂips. Moreover,
for our method, the average PA-ACC degradation over four cases on ImageNet is only 0.06%, which
demonstrates the stealthiness of our attack. When comparing the results of ResNet and VGG, an
interesting observation is that all methods require signiﬁcantly more bit-ﬂips for VGG. One reason
is that VGG is much wider than ResNet. Similar to the claim in (He et al., 2020), increasing the
network width contributes to the robustness against the bit-ﬂip based attack.

4.3 RESISTANCE TO DEFENSE METHODS

Resistance to Piece-wise Clustering. He et al. (2020) proposed a novel training technique, called
piece-wise clustering, to enhance the network robustness against the bit-ﬂip based attack. Such
a training technique introduces an additional weight penalty to the inference loss, which has the
effect of eliminating close-to-zero weights (He et al., 2020). We test the resistance of all attack
methods to the piece-wise clustering. We conduct experiments with the 8-bit quantized ResNet
on CIFAR-10 and ImageNet. Following the ideal conﬁguration in (He et al., 2020), the clustering
coefﬁcient, which is a hyper-parameter of piece-wise clustering, is set to 0.001 in our evaluation.
For our method, the initial k is set to 50 on ImageNet and the rest settings are the same as those in
Section 4.1. Besides the three metrics in Section 4.1, we also present the number of increased Nﬂip
compared to the model without defense (i.e., results in Table 1), denoted as ∆Nﬂip.

The results of the resistance to the piece-wise clustering of all attack methods are shown in Table
2. It shows that the model trained with piece-wise clustering can improve the number of required
bit-ﬂips for all attack methods. However, our method still achieves a 100% ASR with the least
number of bit-ﬂips on both two datasets. Although TBT achieves a smaller ∆Nﬂip than ours on
CIFAR-10, its ASR is only 52.3%, which also veriﬁes the defense effectiveness of the piece-wise
clustering. Compared with other methods, TA-LBF achieves the fewest ∆Nﬂip on ImageNet and the
best PA-ACC on both datasets. These results demonstrate the superiority of our method over other
methods when attacking models trained with piece-wise clustering.

7

Published as a conference paper at ICLR 2021

Table 2: Results of all attack methods against the models with defense on CIFAR-10 and ImageNet
(bold: the best; underline: the second best). The mean and standard deviation of PA-ACC and Nﬂip
are calculated by attacking the 1,000 images. Our method is denoted as TA-LBF. ∆Nﬂip denotes
the increased Nﬂip compared to the corresponding result in Table 1.

Defense

Dataset

Method

g
n
i
r
e
t
s
u
l
C
e
s
i
w
-
e
c
e
i

P

y
t
i
c
a
p
a
C

l
e
d
o
M

r
e
g
r
a
L

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

ACC
(%)

91.01

63.62

94.29

71.35

PA-ACC
(%)

84.06±3.56
87.05±1.69
85.82±1.89
86.61±2.51
84.12±4.77
87.30±2.74
43.44±2.07
63.07±0.04
62.82±0.27
63.26±0.21
63.14±0.48
63.52±0.14

86.46±2.84
89.72±2.99
91.16±1.42
90.70±2.37
89.83±3.02
90.96±2.63
63.51±1.29
71.12±0.04
70.84±0.30
71.30±0.04
71.30±0.05
71.30±0.04

ASR
(%)

99.5
52.3
98.6
98.6
100.0
100.0
92.2
81.8
90.1
99.5
100.0
100.0

100.0
89.5
98.7
98.5
100.0
100.0
100.0
99.9
88.9
100.0
100.0
100.0

Nﬂip

∆Nﬂip

1893.55±68.98
254.20±10.22
45.51±9.47
246.11±75.36
52.76±16.29
18.93±7.11
762267.56±52179.46
1184.14±30.30
273.56±191.29
729.94±491.83
107.59±31.15
51.11±4.33

2753.43±188.27
366.90±12.09
17.91±4.64
271.27±65.18
48.96±21.03
8.79±2.44
507456.61±34517.04
1138.34±44.23
40.23±27.29
449.70±106.42
20.01±6.04
8.48±2.52

386.04
7.50
35.60
60.60
25.93
13.36
484843.27
606.74
248.99
288.73
89.05
43.74

1245.92
120.20
8.00
85.76
22.13
3.22
230032.32
560.94
15.66
8.49
1.47
1.11

Figure 2: Results of TA-LBF with different parameters λ, k, and the number of auxiliary samples
N on CIFAR-10. Regions in shadow indicate the standard deviation of attacking the 1,000 images.

Resistance to Larger Model Capacity. Previous studies (He et al., 2020; Rakin et al., 2020b)
observed that increasing the network capacity can improve the robustness against the bit-ﬂip based
attack. Accordingly, we evaluate all attack methods against the models with a larger capacity using
the 8-bit quantized ResNet on both datasets. Similar to the strategy in (He et al., 2020), we increase
the model capacity by varying the network width (i.e., 2× width in our experiments). All settings
of our method are the same as those used in Section 4.1.

The results are presented in Table 2. We observe that all methods require more bit-ﬂips to attack
the model with the 2× width. To some extent, it demonstrates that the wider network with the same
architecture is more robust against the bit-ﬂip based attack. However, our method still achieves a
100% ASR with the fewest Nﬂip and ∆Nﬂip. Moreover, when comparing the two defense methods,
we ﬁnd that piece-wise clustering performs better than the model with a larger capacity in terms of
∆Nﬂip. However, piece-wise clustering training also causes the accuracy decrease of the original
model (e.g., from 92.16% to 91.01% on CIFAR-10). We provide more results in attacking models
with defense under different settings in Appendix F.

8

1510205010020060708090100PA-ACC / ASR (%)101520Nflip51015202530k7580859095100PA-ACC / ASR (%)051015202530Nflip2550100200400800N859095100PA-ACC / ASR (%)0510NflipASRPA-ACCNflipPublished as a conference paper at ICLR 2021

Figure 3: Visualization of decision boundaries of the original model and the post attack models. The
attacked sample from Class 3 is misclassiﬁed into the Class 1 by FSA, GDA, and our method.

4.4 ABLATION STUDY

We perform ablation studies on parameters λ and k, and the number of auxiliary samples N . We use
the 8-bit quantized ResNet on CIFAR-10 as the representative for analysis. We discuss the attack
performance of TA-LBF under different values of λ while k is ﬁxed at 20, and under different values
of k while λ is ﬁxed at 10. To analyze the effect of N , we conﬁgure N from 25 to 800 and keep other
settings the same as those in Section 4.1. The results are presented in Fig. 2. We observe that our
method achieves a 100% ASR when λ is less than 20. As expected, the PA-ACC increases while the
ASR decreases along with the increase of λ. The plot of parameter k presents that k can exactly limit
the number of bit-ﬂips, while other attack methods do not involve such constraint. This advantage
is critical since it allows the attacker to identify limited bits to perform an attack when the budget is
ﬁxed. As shown in the ﬁgure, the number of auxiliary samples less than 200 has a marked positive
impact on the PA-ACC. It’s intuitive that more auxiliary samples can lead to a better PA-ACC. The
observation also indicates that TA-LBF still works well without too many auxiliary samples.

4.5 VISUALIZATION OF DECISION BOUNDARY

To further compare FSA and GDA with our method, we visualize the decision boundaries of the
original and the post attack models in Fig. 3. We adopt a four-layer Multi-Layer Perceptron trained
with the simulated 2-D Blob dataset from 4 classes. The original decision boundary indicates that
the original model classiﬁes all data points almost perfectly. The attacked sample is classiﬁed into
Class 3 by all methods. Visually, GDA modiﬁes the decision boundary drastically, especially for
Class 0. However, our method modiﬁes the decision boundary mainly around the attacked sample.
Althoug FSA is comparable to ours visually in Fig. 3, it ﬂips 10× bits than GDA and TA-LBF. In
terms of the numerical results, TA-LBF achieves the best PA-ACC and the fewest Nﬂip. This ﬁnding
veriﬁes that our method can achieve a successful attack even only tweaking the original classiﬁer.

5 CONCLUSION

In this work, we have presented a novel attack paradigm that the weights of a deployed DNN can be
slightly changed via bit ﬂipping in the memory, to give a target prediction for a speciﬁc sample, while
the predictions on other samples are not signiﬁcantly inﬂuenced. Since the weights are stored as
binary bits in the memory, we formulate this attack as a binary integer programming (BIP) problem,
which can be effectively and efﬁciently solved by a continuous algorithm. Since the critical bits are
determined through optimization, the proposed method can achieve the attack goals by ﬂipping a
few bits, and it shows very good performance under different experimental settings.

ACKNOWLEDGMENTS

This work is supported in part by the National Key Research and Development Program of China
under Grant 2018YFB1800204, the National Natural Science Foundation of China under Grant
61771273, the R&D Program of Shenzhen under Grant JCYJ20180508152204044. Baoyuan Wu is
supported by the Natural Science Foundation of China under grant No. 62076213, and the university
development fund of the Chinese University of Hong Kong, Shenzhen under grant No. 01001810.

9

Original ACC=98.0%FSA PA-ACC=89.4% Nflip=97GDA PA-ACC=61.3% Nflip=9TALBF(ours) PA-ACC=91.6% Nflip=7Class 0Class 1 (Target Class)Class 2Class 3 (Source Class)Attacked SamplePublished as a conference paper at ICLR 2021

REFERENCES

Michel Agoyan, Jean-Max Dutertre, Amir-Pasha Mirbaha, David Naccache, Anne-Lise Ribotta, and

Assia Tria. How to ﬂip a bit? In IOLTS, pp. 235–239, 2010.

Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:

A survey. IEEE Access, 6:14410–14430, 2018.

Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, and En-hui Yang.

Targeted attack for deep hashing based retrieval. In ECCV, 2020.

Adel Bibi, Baoyuan Wu, and Bernard Ghanem. Constrained k-means with general pairwise and

cardinality constraints. arXiv preprint arXiv:1907.10410, 2019.

Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the

alternating direction method of multipliers. Now Publishers Inc, 2011.

Jakub Breier, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu. Practical fault

attack on deep neural networks. In CCS, pp. 2204–2206, 2018.

Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled

data improves adversarial robustness. In NeurIPS, 2019.

Weilun Chen, Zhaoxiang Zhang, Xiaolin Hu, and Baoyuan Wu. Boosting decision-based black-
box adversarial attacks with random sign ﬂip. In Proceedings of the European Conference on
Computer Vision, 2020.

Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and Jun Zhu. Efﬁcient
decision-based black-box adversarial attacks on face recognition. In CVPR, pp. 7714–7722, 2019.

Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via

differential privacy. ICLR, 2020.

Jonathan Eckstein and Dimitri P Bertsekas. On the douglas—rachford splitting method and the
proximal point algorithm for maximal monotone operators. Mathematical Programming, 55(1-
3):293–318, 1992.

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiﬁcation. In CVPR, pp. 1625–1634, 2018.

Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang Li, Zhifeng Li, and Yujiu Yang.
Sparse adversarial attack via perturbation factorization. In European Conference on Computer
Vision, 2020.

Yan Feng, Bin Chen, Tao Dai, and Shutao Xia. Adversarial attack on deep product quantization

network for image retrieval. In AAAI, 2020.

Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via ﬁnite element approximation. Computers & mathematics with applications, 2(1):
17–40, 1976.

Roland Glowinski and A Marroco.

Sur l’approximation, par ´el´ements ﬁnis d’ordre un, et
la r´esolution, par p´enalisation-dualit´e d’une classe de probl`emes de dirichlet non lin´eaires.
ESAIM: Mathematical Modelling and Numerical Analysis-Mod´elisation Math´ematique et Anal-
yse Num´erique, 9(R2):41–76, 1975.

E Gi Gol’shtein and NV Tret’yakov. Modiﬁed lagrangians in convex programming and their gener-
alizations. In Point-to-Set Maps and Mathematical Programming, pp. 86–97. Springer, 1979.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In ICLR, 2015.

10

Published as a conference paper at ICLR 2021

Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas Jufﬁnger, Sioli O’Connell,
Wolfgang Schoechl, and Yuval Yarom. Another ﬂip in the wall of rowhammer defenses. In IEEE
S&P, pp. 245–261, 2018.

Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring

attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-

nition. In CVPR, pp. 770–778, 2016.

Zhezhi He, Adnan Siraj Rakin, Jingtao Li, Chaitali Chakrabarti, and Deliang Fan. Defending and

harnessing the bit-ﬂip based adversarial weight attack. In CVPR, pp. 14095–14103, 2020.

Sanghyun Hong, Pietro Frigo, Yi˘gitcan Kaya, Cristiano Giuffrida, and Tudor Dumitras, . Terminal
brain damage: Exposing the graceless degradation in deep neural networks under hardware fault
attacks. In USENIX Security Symposium, pp. 497–514, 2019.

Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efﬁcient integer-arithmetic-only inference. In CVPR, pp. 2704–2713, 2018.

Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk Lee, Chris Wilkerson,
Konrad Lai, and Onur Mutlu. Flipping bits in memory without accessing them: An experimental
study of dram disturbance errors. ACM SIGARCH Computer Architecture News, 42(3):361–372,
2014.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.

Technical report, 2009.

Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convo-

lutional neural networks via factorized convolutional ﬁlters. In CVPR, 2019.

Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey.

arXiv preprint arXiv:2007.08745, 2020.

Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdoor-

ing attacks on deep neural networks. In RAID, pp. 273–294, 2018a.

Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. Fault injection attack on deep neural network.

In ICCAD, pp. 131–138, 2017a.

Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu

Zhang. Trojaning attack on neural networks. In NDSS, 2018b.

Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂection backdoor: A natural backdoor

attack on deep neural networks. ECCV, 2020a.

Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In ICCD, pp. 45–48, 2017b.

Yuntao Liu, Ankit Mondal, Abhishek Chakraborty, Michael Zuzak, Nina Jacobsen, Daniel Xing,

and Ankur Srivastava. A survey on neural trojans. In ISQED, 2020b.

Szymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, 2017.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal

adversarial perturbations. In CVPR, pp. 1765–1773, 2017.

Jonathan Pan. Blackbox trojanising of deep learning models: Using non-intrusive network structure

and binary alterations. arXiv preprint arXiv:2008.00408, 2020.

Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Bit-ﬂip attack: Crushing neural network with

progressive bit search. In ICCV, pp. 1211–1220, 2019.

Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with bit trojan.

In CVPR, pp. 13198–13207, 2020a.

11

Published as a conference paper at ICLR 2021

Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali Chakrabarti, and Deliang Fan. T-bfa:

Targeted bit-ﬂip adversarial weight attack. arXiv preprint arXiv:2007.12336, 2020b.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015.

Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor at-

tacks. In AAAI, pp. 11957–11965, 2020.

Bodo Selmke, Stefan Brummer, Johann Heyszl, and Georg Sigl. Precise laser fault injections into

90 nm and 45 nm sram-cells. In CARDIS, pp. 193–205. Springer, 2015.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image

recognition. In ICLR, 2015.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,

and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS,

pp. 8000–8010, 2018.

Victor Van Der Veen, Yanick Fratantonio, Martina Lindorfer, Daniel Gruss, Cl´ementine Maurice,
Giovanni Vigna, Herbert Bos, Kaveh Razavi, and Cristiano Giuffrida. Drammer: Deterministic
rowhammer attacks on mobile platforms. In CCS, pp. 1675–1689, 2016.

Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In IEEE
S&P, pp. 707–723, 2019.

Baoyuan Wu and Bernard Ghanem. (cid:96)p-box admm: A versatile framework for integer programming.

IEEE transactions on pattern analysis and machine intelligence, 41(7):1695–1708, 2018.

Baoyuan Wu, Li Shen, Tong Zhang, and Bernard Ghanem. Map inference via l2-sphere linear

program reformulation. International Journal of Computer Vision, pp. 1–24, 2020a.

Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter:

On the transferability of adversarial examples generated with resnets. In ICLR, 2020b.

Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-

alization. In NeurIPS, 2020c.

Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against feder-

ated learning. In ICLR, 2019.

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial

effects through randomization. ICLR, 2018.

Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep

neural networks. NDSS, 2017.

Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, and Wei Liu. Exact
adversarial attack to image captioning via structured output learning with latent variables.
In
CVPR, 2019.

Fan Yao, Adnan Siraj Rakin, and Deliang Fan. Deephammer: Depleting the intelligence of deep
neural networks through targeted chain of bit ﬂips. In USENIX Security Symposium, pp. 1463–
1480, 2020.

Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In-So Kweon. Cd-uap: Class discriminative

universal adversarial perturbation. In AAAI, 2020a.

Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. Understanding adversarial exam-

ples from the mutual inﬂuence of images and perturbations. In CVPR, 2020b.

Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, and Xue Lin. Fault sneaking attack:
A stealthy framework for misleading deep neural networks. In ACM DAC, pp. 1–6. IEEE, 2019.

12

Published as a conference paper at ICLR 2021

A UPDATE ˆb BY GRADIENT DESCENT

In this section, we derive the gradient of L w.r.t. ˆb, which is adopted to update ˆbr+1 by gradient
descent (see Eq. (10)). The derivation consists of the following parts.
Derivation of ∂L1(ˆb)/∂ˆb. For clarity, here we ﬁrstly repeat some deﬁnitions,

L1( ˆBs, ˆBt) = max (cid:0)m − p(x; Θ, ˆBt) + δ, 0(cid:1) + max (cid:0)p(x; Θ, ˆBs) − m + δ, 0(cid:1),
p(x; Θ, ˆBi) = [h( ˆBi,1); h( ˆBi,2); ...; h( ˆBi,C)](cid:62)g(x; Θ),

h(v) = (−2Q−1 · vQ +

Q−1
(cid:88)

i=1

2i−1 · vi) · ∆l.

Then, we obtain that

∂p(x; Θ, ˆBi)
∂ ˆBi

= [g1(x; Θ) · (

∇h( ˆBi,1)
∂ ˆBi,1

)(cid:62); ...; gC(x; Θ) · (

∇h( ˆBi,C)
∇ ˆBi,C

)(cid:62)],

(12)

(13)

(14)

(15)

where ∇h(v)
gj(x; Θ) denotes the j-th entry of the vector g(x; Θ). Utilizing (15), we have

∇v = [20; 21, . . . , 2Q−2; −2Q−1] · ∆l is a constant, and here l indicates the last layer;

(16)

(17)

(18)

∂L1( ˆBs, ˆBt)
∂ ˆBs
∂L1( ˆBs, ˆBt)
∂ ˆBt

=

=

(cid:40)

(cid:40) ∂p(x;Θ,ˆBs)

∂ ˆBs

0, otherwise
− ∂p(x;Θ,ˆBt)
,
∂ ˆBt
0, otherwise

,

if p(x; Θ, Bs) > m − δ

if p(x; Θ, Bt) < m + δ

,

.

(cid:1)(cid:3),

Thus, we obtain that

∂L1(ˆb)
∂ˆb

= (cid:2)Reshape(cid:0) ∂L1( ˆBs, ˆBt)

∂ ˆBs

(cid:1); Reshape(cid:0) ∂L1( ˆBs, ˆBt)

∂ ˆBt

where Reshape(·) elongates a matrix to a vector along the column.
Derivation of ∂L2(ˆb)/∂ˆb. For clarity, here we ﬁrstly repeat the following deﬁnition

L2( ˆBs, ˆBt) =

N
(cid:88)

i=1

(cid:96)(cid:0)f (xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt), yi

(cid:1),

where fj(xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt) = Softmax(p(xi; Θ, ˆBj)) or Softmax(p(xi; Θ, Bj)) in-
dicates the posterior probability of xi w.r.t. class j, and we simply denote f (xi) ∈ [0, 1]K as the
posterior probability vector of xi. {(xi, yi)}N
i=1 denotes the auxiliary sample set. (cid:96)(·, ·) is speciﬁed
as the cross entropy loss. Then, we have

∂L2( ˆBs, ˆBt)
∂ ˆBs
∂L2( ˆBs, ˆBt)
∂ ˆBt

=

=

N
(cid:88)

i=1

N
(cid:88)

i=1

(cid:20)
(cid:0)I(yi = s) − fs(xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt)(cid:1) ·

(cid:20)
(cid:0)I(yi = t) − ft(xi; Θ, B{1,...,K}\{s,t}, ˆBs, ˆBt)(cid:1) ·

where I(a) = 1 of a is true, otherwise I(a) = 0. Thus, we obtain

∂p(xi; Θ, ˆBs)
∂ ˆBs
∂p(xi; Θ, ˆBt)
∂ ˆBt

(cid:21)

∂L2(ˆb)
∂ˆb

=

(cid:20)
Reshape(cid:0) ∂L2( ˆBs, ˆBt)

∂ ˆBs

(cid:1); Reshape(cid:0) ∂L2( ˆBs, ˆBt)

∂ ˆBt

(cid:21)
.

(cid:1)

(cid:21)
,

(19)

,

(20)

(21)

Derivation of ∂L(ˆb)/∂ˆb. According to Eq. (8), and utilizing Eqs. (17) and (21), we obtain
∂L(ˆb)
∂ˆb

+z1+z2+ρ1(ˆb−u1)+ρ2(ˆb−u2)+2(ˆb−b)·(cid:2)z3+ρ3||ˆb−b||2

∂L1(ˆb)
∂ˆb

∂L2(ˆb)
∂ˆb

+

=

2−k+u3

(cid:3).
(22)

13

Published as a conference paper at ICLR 2021

B ALGORITHM OUTLINE

Algorithm 1 Continuous optimization for the BIP problem (5).
Input: The original quantized DNN model f with weights Θ, B, attacked sample x with ground-
truth label s, target class t, auxiliary sample set {(xi, yi)}N
Output: ˆb.
1: Initial u0
1, z0
1, u0
2: while not converged do
Update ur+1
, ur+1
3:
2
Update ˆbr+1 as Eq. (10);
4:
Update zr+1
5:
1
r ← r + 1.
6:
7: end while

i=1, hyper-parameters λ, k, and δ.

3, ˆb0 and let r ← 0;

and zr+1
3

as Eq. (11);

and ur+1

as Eq. (9);

, zr+1
2

3, z0

2, u0

2, z0

1

3

C COMPLEXITY ANALYSIS

Table 3: Running time (seconds) of attacking one image for different methods. The mean and
standard deviation are calculated by 10 attacks.

CIFAR-10
ImageNet

FT
15.54±1.64
124.32±3.61

TBT
389.12±27.79
31425.81±540.60

T-BFA
35.05±15.79
19.16±3.52

FSA
2.71±0.48
65.28±2.49

GDA
0.67±0.54
61.97±1.59

TA-LBF
113.38±6.54
222.95±9.39

The computational complexity of the proposed algorithm (i.e., Algorithm 1) consists of two parts,
the forward and backward pass. In terms of the forward pass, since Θ, B{1,...,K}\{s,t} are ﬁxed
during the optimization, their involved terms, including g(x; Θ) and p(x; Θ, Bi)|i(cid:54)=s,t, are cal-
culated only one time. The main cost from ˆBs and ˆBt is O(2(N + 1)C 2Q) per iteration, as
there are N + 1 samples.
In terms of the backward pass, the main cost is from the update
of ˆbr+1, which is O(2(N + 1)CQ) per iteration in the gradient descent. Since all other up-
dates are very simple, their costs are omitted here. Thus, the overall computational cost is
O(cid:0)Touter[2(N + 1)CQ · (C + Tinner)](cid:1), with Touter being the iteration of the overall algorithm
and Tinner indicating the number of gradient steps in updating ˆbr+1. As shown in Section D, the
proposed method TA-LBF always converges very fast in our experiments, thus Touter is not very
large. As demonstrated in Section E.3, Tinner is set to 5 in our experiments. In short, the proposed
method can be optimized very efﬁciently.

Besides, we also compare the computational complexity of different attacks empirically. Speciﬁ-
cally, we compare the running time of attacking one image of different methods against the 8-bit
quantized ResNet on CIFAR-10 and ImageNet dataset. As shown in Table 3, TBT is the most time-
consuming method among all attacks. Although the proposed TA-LBF is not superior to T-BFA,
FSA, and GDA in running time, this gap can be tolerated when attacking a single image in the
deployment stage. Besides, our method performs better in terms of PA-ACC, ASR, and Nﬂip as
demonstrated in our experiments.

D NUMERICAL CONVERGENCE ANALYSIS

2 and ||ˆb − u2||2
We present the numerical convergence of TA-LBF in Fig. 4. Note that ||ˆb − u1||2
2
characterize the degree of satisfaction of the box and (cid:96)2-sphere constraint, respectively. For the two
examples of CIFAR-10 and ImageNet, the values of both indicators ﬁrst increase, then drop, and
ﬁnally close to 0. Another interesting observation is that L1 + λL2 ﬁrst decreases evidently and
then increases slightly. Such ﬁndings illustrate the optimization process of TA-LBF. In the early
iterations, modifying the model parameters tends to achieve the two goals mentioned in Section 3.1;
in the late iterations, ˆb is encouraged to satisfy the box and l2-sphere constraint. We also observe
that both examples stop when meeting ||ˆb − u1||2
2 ≤ 10−4 and do not

2 ≤ 10−4 and ||ˆb − u2||2

14

Published as a conference paper at ICLR 2021

Figure 4: Numerical convergence analysis of TA-LBF w.r.t. the attacked sample on CIFAR-10 and
ImageNet, respectively. We present the values of ||ˆb − u1||2
2 and L1 + λL2 at different
iterations in attacking 8-bit quantized ResNet. Note that λ in the left ﬁgure is 100 and λ in the right
ﬁgure is 104.

2, ||ˆb − u2||2

exceed the maximum number of iterations (i.e., 2000). The numerical results demonstrate the fast
convergence of our method in practice.

E EVALUATION SETUP

E.1 BASELINE METHODS

Since GDA (Liu et al., 2017a) and FSA (Zhao et al., 2019) are originally designed for attacking
the full-precision network, we adapt these two methods to attack the quantized network by applying
quantization-aware training (Jacob et al., 2018). We adopt the (cid:96)0-norm for FSA (Liu et al., 2017a)
and modiﬁcation compression for GDA (Zhao et al., 2019) to reduce the number of the modiﬁed
parameters. Among three types of T-BFA (Rakin et al., 2020b), we compare to the most comparable
method:
the 1-to-1 stealthy attack scheme. The purpose of this attack scheme is to misclassify
samples of a single source class into the target class while maintaining the prediction accuracy of
other samples. Besides, we take the ﬁne-tuning (FT) of the last fully-connected layer as a basic
attack and present its results. We perform attack once for each selected image except TBT (Rakin
et al., 2020a) and totally 1,000 attacks on each dataset. The attack objective of TBT is that the
attacked DNN model misclassiﬁes all inputs with a trigger to a certain target class. Due to such
objective, the number of attacks for TBT is equal to the number of target classes (i.e., 10 attacks on
CIFAR-10 and 50 attacks on ImageNet).

E.2 TARGET MODELS

According to the setting in (Rakin et al., 2020a;b), we adopt two popular network architectures:
ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) for evaluation. On CIFAR-10,
we perform experiments on ResNet-20 and VGG-16. On ImageNet, we use the pre-trained ResNet-
18* and VGG-16† network. We quantize all networks to the 4-bit and 8-bit quantization level using
the layer-wise uniform weight quantization scheme, which is similar to the one involved in the
Tensor-RT solution (Migacz, 2017).

E.3 PARAMETER SETTINGS OF TA-LBF

For each attack, we adopt a strategy for jointly searching λ and k. Speciﬁcally, for an initially given
k, we search λ from a relatively large initial value and divide it by 2 if the attack does not succeed.
The maximum search times of λ for a ﬁxed k is set to 8. If it exceeds the maximum search times,

*Downloaded from https://download.pytorch.org/models/resnet18-5c106cde.pth
†Downloaded from https://download.pytorch.org/models/vgg16_bn-6c64b313.pth

15

025050075010001250# iterations0.00.10.20.30.40.50.6||bu1||22   /   ||bu2||2230354045501+2CIFAR-10050010001500# iterations0.0000.0020.0040.0060.0080.010||bu1||22   /   ||bu2||2212455.012457.512460.012462.512465.012467.512470.012472.51+2ImageNet||bu1||22||bu2||221+2||bu1||22||bu2||221+2Published as a conference paper at ICLR 2021

Table 4: Results of all attack methods against models trained with piece-wise clustering on CIFAR-
10 (bold: the best; underline: the second best). We adopt different clustering coefﬁcients, including
0.0005, 0.005, and 0.01. The mean and standard deviation of PA-ACC and Nﬂip are calculated by
attacking the 1,000 images. Our method is denoted as TA-LBF. ∆Nﬂip denotes the increased Nﬂip
compared to the corresponding result in Table 1.

Clustering
Coefﬁcient

0.0005

0.005

0.01

Method

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

ACC
(%)

91.42

88.03

85.65

PA-ACC
(%)
84.28±3.49
87.97±1.75
86.20±1.96
87.17±2.44
85.28±4.16
87.92±2.54
81.08±3.61
82.96±2.18
80.80±2.64
83.10±2.75
79.23±6.25
83.63±3.47
78.73±3.54
79.86±2.04
76.67±3.41
80.45±3.14
75.33±7.83
80.51±4.39

ASR
(%)
100.0
66.1
98.5
98.5
100.0
100.0
97.9
12.7
98.1
98.4
99.9
100.0
98.3
10.1
98.1
98.0
99.7
100.0

Nﬂip

1868.26±72.48
250.30±10.97
30.95±6.50
222.70±56.52
41.33±12.84
13.47±5.34
1774.69±51.47
246.80±16.06
61.72±12.17
231.66±89.21
64.87±22.78
25.52±11.59
1748.54±46.19
236.50±10.93
55.49±11.77
220.28±101.01
59.17±23.63
24.60±13.03

∆Nﬂip

360.75
3.60
21.04
37.19
14.50
7.90
267.18
0.10
51.81
46.15
38.04
19.95
241.03
-10.20
45.58
34.77
32.34
19.03

we double k and search λ from the relatively large initial value. The maximum search times of k
is set to 4. On CIFAR-10, the initial k and λ are set to 5 and 100. On ImageNet, λ is initialized
as 104; k is initialized as 5 and 50 for ResNet and VGG, respectively. On CIFAR-10, the δ in L1
is set to 10. On ImageNet, the δ is set to 3 and increased to 10 if the attack fails. u1 and u2 are
initialized as b and u3 is initialized as 0. z1 and z2 are initialized as 0 and z3 is initialized as 0. ˆb is
initialized as b. During each iteration, the number of gradient steps for updating ˆb is 5 and the step
size is set to 0.01 on both datasets. Hyper-parameters (ρ1, ρ2, ρ3) (see Eq. (11)) are initialized as
(10−4, 10−4, 10−5) on both datasets, and increase by ρi ← ρi × 1.01, i = 1, 2, 3 after each iteration.
The maximum values of (ρ1, ρ2, ρ3) are set to (50, 50, 5) on both datasets. Besides the maximum
number of iterations (i.e., 2000), we also set another stopping criterion, i.e., ||ˆb − u1||2
2 ≤ 10−4 and
||ˆb − u2||2

2 ≤ 10−4.

F MORE RESULTS ON RESISTANCE TO DEFENSE METHODS

F.1 RESISTANCE TO PIECE-WISE CLUSTERING

We conduct experiments using the 8-bit quantized ResNet on CIFAR-10 with different clustering
coefﬁcients. We set the maximum search times of k to 5 for clustering coefﬁcient 0.005 and 0.01
and keep the rest settings the same as those in Section 4.1. The results are presented in Table 4.
As shown in the table, all values of Nﬂip are larger than attacking models without defense for all
methods, which is similar to Table 2. Our method achieves a 100% ASR with the fewest Nﬂip under
the three clustering coefﬁcients. Although TBT obtains a smaller ∆Nﬂip than our method, it fails
to achieve a satisfactory ASR. For example, TBT achieves only a 10.1% ASR when the clustering
coefﬁcient is set to 0.01. We observe that for all clustering coefﬁcients, piece-wise clustering reduces
the original accuracy. Such a phenomenon is more signiﬁcant as the clustering coefﬁcient increases.
The results also show that there is no guarantee that if the clustering coefﬁcient is larger (e.g., 0.01),
the model is more robust, which is consistent with the ﬁnding in (He et al., 2020).

16

Published as a conference paper at ICLR 2021

Table 5: Results of all attack methods against models with a larger capacity on CIFAR-10. We adopt
3× and 4× width networks. The mean and standard deviation of PA-ACC and Nﬂip are calculated
by attacking the 1,000 images. ∆Nﬂip denotes the increased Nﬂip compared to the corresponding
result in Table 1.
Model
Width

ACC
(%)

Method

∆Nﬂip

Nﬂip

3×

4×

FT
TBT
T-BFA
FSA
GDA
TA-LBF
FT
TBT
T-BFA
FSA
GDA
TA-LBF

94.90

95.02

PA-ACC
(%)
86.96±2.79
90.67±5.23
92.18±1.14
91.40±2.38
90.79±2.91
91.42±2.81
86.94±2.78
85.39±5.08
92.49±1.22
91.60±2.42
90.76±3.00
90.94±3.11

ASR
(%)
100.0
74.1
98.9
99.0
100.0
100.0
100.0
90.1
99.4
98.7
100.0
100.0

4002.52±281.24
504.70±20.44
30.50±7.52
342.20±79.44
67.53±27.45
12.29±4.18
4527.68±369.35
625.50±32.38
19.14±5.04
338.93±100.12
66.92±40.32
8.37±2.80

2495.01
258.00
20.59
156.69
40.70
6.72
3020.17
378.80
9.23
153.42
40.09
2.80

F.2 RESISTANCE TO LARGER MODEL CAPACITY

Besides the results of networks with a 2× width shown in Section 4.3, we also evaluate all methods
against models with a 3× and 4× width. All settings are the same as those used in Section 4.1. The
results are provided in Table 5. Among all attack methods, our method is least affected by increasing
the network width. Especially for the network with a 4× width, our ∆Nﬂip is only 2.80. The results
demonstrate the superiority of the formulated BIP problem and optimization. Moreover, compared
with piece-wise clustering, having a larger model capacity can improve the original accuracy, but
increases the model size and the computation complexity.

G DISCUSSIONS

G.1 COMPARING BACKDOOR, ADVERSARIAL, AND WEIGHT ATTACK

An attacker can achieve malicious purposes utilizing backdoor, adversarial, and weight attacks. In
this section, we emphasize the differences among them.

Backdoor attack happens in the training stage and requires that the attacker can tamper the training
data even the training process (Liu et al., 2020b; Li et al., 2020). Through poisoning some training
samples with a trigger, the attacker can control the behavior of the attacked DNN in the inference
stage. For example, images with reﬂections are misclassiﬁed into a target class, while benign images
are classiﬁed normally (Liu et al., 2020a). However, such an attack paradigm causes the accuracy
degradation on benign samples, which makes it detectable for users. Besides, these methods also
require to modify samples in the inference stage, which is sometimes impossible for the attacker.
Many defense methods against backdoor attack have been proposed, such as the preprocessing-
based defense (Liu et al., 2017b), the model reconstruction-based defense (Liu et al., 2018a), and
the trigger synthesis-based defense (Wang et al., 2019).

Adversarial attack modiﬁes samples in the inference stage by adding small perturbations that re-
main imperceptible to the human vision system (Akhtar & Mian, 2018). Since adversarial attack
only modiﬁes inputs while keeping the model unchanged, it has no effect on the benign samples.
Besides the basic white-box attack, the black-box attack (Wu et al., 2020b; Chen et al., 2020) and
universal attack (Zhang et al., 2020b;a) have attracted wide attention. Inspired by its success in the
classiﬁcation, it also has been extended to other tasks, including image captioning (Xu et al., 2019),
retrieval (Bai et al., 2020; Feng et al., 2020), etc.. Similarly, recent studies have demonstrated many
defense methods against adversarial attack, including the preprocessing-based defense (Xie et al.,
2018), the detection-based defense (Xu et al., 2017), and the adversarial learning-based defense
(Carmon et al., 2019; Wu et al., 2020c).

17

Published as a conference paper at ICLR 2021

Weight attack modiﬁes model parameters in the deployment stage, which is the studied paradigm in
this work. Weight attack generally aims at misleading the DNN model on the selected sample(s),
while having a minor effect on other samples (Zhao et al., 2019; Rakin et al., 2020b). Many studies
(Yao et al., 2020; Breier et al., 2018; Pan, 2020) have demonstrated that the DNN parameters can
be modiﬁed in the bit-level in memory using fault injection techniques (Agoyan et al., 2010; Kim
et al., 2014; Selmke et al., 2015) in practice. Note that the defense methods against weight attack
have been not well studied. Although some defense methods (He et al., 2020) were proposed, they
cannot achieve satisfactory performance. For example, our method can still achieve a 100% attack
success rate against two proposed defense methods. Our work would encourage further investigation
on the security of the model parameters from both attack and defense sides.

G.2 COMPARING TA-LBF WITH OTHER WEIGHT ATTACKS

We compare our TA-LBF with other weight attack methods, including TBT (Rakin et al., 2020a), T-
BFA (Rakin et al., 2020b), GDA (Liu et al., 2017a), and FSA (Zhao et al., 2019) in this section. TBT
tampers both the test sample and the model parameters. Speciﬁcally, it ﬁrst locates critical bits and
generates a trigger, and then ﬂips these bits to classify all inputs embedded with the trigger to a target
class. However, the malicious samples are easily detected by human inspection or many detection
methods (Tran et al., 2018; Du et al., 2020). We do not modify the samples to perform TA-LBF,
which makes the attack more stealthy. Rakin et al. (2020b) proposed T-BFA which misclassiﬁes
all samples (N-to-1 version) or samples from a source class (1-to-1 version) into a target class.
Our method aims at misclassifying a speciﬁc sample, which meets the attacker’s requirement in
some scenarios. For example, the attacker wants to manipulate the behavior of a face recognition
engine on a speciﬁc input. Since it affects multiple samples, T-BFA maybe not stealthy enough
in attacking real-world applications. GDA (Liu et al., 2017a) and FSA (Zhao et al., 2019) modify
model parameters at the weight-level rather than bit-level. They are designed for misclassifying
multiple samples from arbitrary classes, which makes it infeasible for them to only modify the
parameters connected to the source and target class. They modify more parameters than our method
as shown in the experiments, it might be due to the reason discussed above. Besides, TBT, T-BFA,
and GDA determine the critical weights to modify using heuristic strategies, while our TA-LBF
adopts optimization-based methods. Although FSA applies ADMM for solving the optimization
problem, it has no explicit constraint to control the number of modiﬁed parameters, which makes it
intends to modify more parameters than GDA and our TA-LBF.

H TRADE-OFF BETWEEN THREE EVALUATION METRICS

Figure 5: Curves of the trade-off between PA-ACC and Nﬂip and the trade-off between PA-ACC
and ASR for the proposed TA-LBF on two datasets.

In this section, we investigate the trade-off between three adopted evaluation metrics (i.e., PA-ACC,
ASR, and Nﬂip) for our attack. All experiments are conducted on CIFAR-10 and ImageNet dataset
in attacking the 8-bit quantized ResNet.

We ﬁrstly discuss the trade-off between PA-ACC and Nﬂip by ﬁxing the ASR as 100% using the
search strategy in Appendix E.3 and adjusting the initial λ and k to obtain different attack results.
The two curves on the left show that increasing the Nﬂip can improve the PA-ACC when Nﬂip
is relatively small; the PA-ACC decreases with the increase of Nﬂip when Nﬂip is greater than a
threshold. This phenomenon demonstrates that constraining the number of bit-ﬂips is essential to
ensure the attack stealthiness, as mentioned in Section 3.2. To study the trade-off between PA-ACC

18

10203040Nflip86878889PA-ACCCIFAR-10101520Nflip69.30069.32569.35069.37569.400PA-ACCImageNet6080100ASR (%)84868890PA-ACCCIFAR-108090100ASR (%)69.069.169.269.369.4PA-ACCImageNetPublished as a conference paper at ICLR 2021

and ASR, we ﬁx the parameter k as 10 for approximately 10 bit-ﬂips and adjust the parameter λ to
obtain different PA-ACC and ASR results. The trade-off curves between PA-ACC and ASR show
that increasing ASR can decrease the PA-ACC signiﬁcantly. Therefore, how to achieve high ASR
and PA-ACC simultaneously is still an important open problem.

19

