1

Lyapunov-guided Deep Reinforcement Learning for
Stable Online Computation Ofﬂoading in
Mobile-Edge Computing Networks

Suzhi Bi, Senior Member, IEEE, Liang Huang, Member, IEEE, Hui Wang, and Ying-Jun Angela Zhang,
Fellow, IEEE

1
2
0
2

l
u
J

7

]
I

N
.
s
c
[

3
v
0
7
3
1
0
.
0
1
0
2
:
v
i
X
r
a

Abstract—Opportunistic computation ofﬂoading is an effective
method to improve the computation performance of mobile-edge
computing (MEC) networks under dynamic edge environment.
In this paper, we consider a multi-user MEC network with
time-varying wireless channels and stochastic user task data
arrivals in sequential time frames. In particular, we aim to
design an online computation ofﬂoading algorithm to maximize
the network data processing capability subject to the long-term
data queue stability and average power constraints. The online
algorithm is practical in the sense that the decisions for each time
frame are made without the assumption of knowing the future
realizations of random channel conditions and data arrivals.
We formulate the problem as a multi-stage stochastic mixed
integer non-linear programming (MINLP) problem that jointly
determines the binary ofﬂoading (each user computes the task
either locally or at the edge server) and system resource allocation
decisions in sequential time frames. To address the coupling
in the decisions of different time frames, we propose a novel
framework, named LyDROO, that combines the advantages of
Lyapunov optimization and deep reinforcement learning (DRL).
Speciﬁcally, LyDROO ﬁrst applies Lyapunov optimization to
decouple the multi-stage stochastic MINLP into deterministic
per-frame MINLP subproblems. By doing so, it guarantees to
satisfy all the long-term constraints by solving the per-frame
subproblems that are much smaller in size. Then, LyDROO
integrates model-based optimization and model-free DRL to solve
the per-frame MINLP problems with very low computational
complexity. Simulation results show that under various network
setups, the proposed LyDROO achieves optimal computation
performance while stabilizing all queues in the system. Besides,
it induces very low computation time that is particularly suitable
for real-time implementation in fast fading environments.

Index Terms—Mobile edge computing, resource allocation,

Lyapunov optimization, deep reinforcement learning.

I. INTRODUCTION

A. Motivations and Summary of Contributions

S. Bi is with the College of Electronics and Information Engineering,
Shenzhen University, Shenzhen, China 518060 (e-mail: bsz@szu.edu.cn).
S. Bi is also with the Peng Cheng Laboratory, Shenzhen, China 518066.

L. Huang is with the College of Computer Science and Tech-
nology, Zhejiang University of Technology, Hangzhou, China (email:
lianghuang@zjut.edu.cn)

H. Wang is with the Shenzhen Institute of Information Technology, Shen-

zhen, China 518172 (email: wanghui@sziit.edu.cn)

Y-J. A. Zhang is with the Department of Information Engineering, The
(e-mail:

Chinese University of Hong Kong, Shatin, N.T., Hong Kong.
yjzhang@ie.cuhk.edu.hk).

The complete source code implementing LyDROO is available on-line at

https://github.com/revenol/LyDROO.

T HE emerging mobile-edge computing (MEC) technology

is widely recognized as a key solution to enhance the
computation performance of wireless devices (WDs) [2], es-
pecially for size-constrained IoT (Internet of Things) devices
with low on-device battery and computing capability. With
MEC servers deployed at the edge of radio access networks,
e.g., cellular base stations, WDs can ofﬂoad intensive com-
putation tasks to the edge server (ES) in the vicinity to
reduce the computation energy and time cost. Compared to
the naive scheme that ofﬂoads all the tasks for edge execu-
tion, opportunistic computation ofﬂoading, which dynamically
assigns tasks to be computed either locally or at the ES,
has shown signiﬁcant performance improvement under time-
varying network conditions, such as wireless channel gains
[3], harvested energy level [4], task input-output dependency
[5], and edge caching availability [6], etc.

There have been extensive studies on opportunistic com-
putation ofﬂoading to optimize the computation performance
of multi-user MEC networks [5]–[9]. In general, it involves
solving a mixed integer non-linear programming (MINLP) that
jointly determines the binary ofﬂoading (i.e., either ofﬂoading
the computation or not) and the communication/computation
resource allocation (e.g., task ofﬂoading time and local/edge
CPU frequencies) decisions. Solving such problems typi-
cally requires prohibitively high computational complexity
especially in large-size networks. Accordingly, many works
have focused on designing reduced-complexity sub-optimal
algorithms, such as local-search based heuristics [5], [8],
decomposition-oriented search [8], and convex relaxations
of the binary variables [9], [25], etc. However, aside from
the above sub-optimal algorithms still
performance losses,
require a large number of numerical iterations to produce
a satisfying solution. In practice, the MINLP needs to be
frequently re-solved once the system parameters, such as wire-
less link quality, vary. It is therefore too costly to implement
the conventional optimization algorithms in a highly dynamic
MEC environment.

The recent development of data-driven deep reinforcement
learning (DRL) provides a promising alternative to tackle
the online computation ofﬂoading problem. In a nutshell, the
DRL framework takes a model-free approach that uses deep
neural networks (DNNs) to directly learn the optimal mapping
from the “state” (e.g., time-varying system parameters) to the
“action” (e.g., ofﬂoading decisions and resource allocation)
to maximize the “reward” (e.g., data processing rate) via

 
 
 
 
 
 
repeated interactions with the environment [10]. It eliminates
the complicated computation of MINLP and automatically
learns from the past experience on-the-ﬂy without requiring
manually labeled training data samples, and thus is particularly
advantageous for online implementation. Many studies have
applied DRL techniques to design online ofﬂoading algo-
rithms in MEC networks [11]–[20]. In particular, our previ-
ous work [18] proposes a hybrid framework, named DROO
(Deep Reinforcement learning-based Online Ofﬂoading), to
combine the advantages of conventional model-based opti-
mization and model-free DRL methods. DROO implements
a DNN to produce binary ofﬂoading decisions based on the
input environment parameters such as channel conditions. The
candidate ofﬂoading solutions are then fed into a model-
based optimization module, which accordingly optimizes the
communication/computation resource allocation and outputs
an accurate estimate of the reward value for each candidate
ofﬂoading decision. The integrated learning and optimization
approach leads to more robust and faster convergence of the
online training process, thanks to the accurate estimation of
reward values corresponding to each sampled action.

Apart from optimizing the computation performance, it is
equally important to guarantee stable system operation, such as
data queue stability and average power consumption. However,
most of the existing DRL-based methods do not impose long-
term performance constraints (e.g., [11]–[20]). Instead, they
resort
to heuristic approaches that discourage unfavorable
actions in each time frame by introducing penalty terms related
to, for example, packet drop events [14], [15] and energy
consumption [12], [20]. A well-known framework for online
joint utility maximization and stability control is Lyapunov
optimization [21]. It decouples a multi-stage stochastic op-
timization to sequential per-stage deterministic subproblems,
while providing theoretical guarantee to long-term system
stability. Some recent works have applied Lyapunov opti-
mization to design computation ofﬂoading strategy in MEC
networks (e.g., [22]–[26]). However, it still needs to solve
a hard MINLP in each per-stage subproblem to obtain the
joint binary ofﬂoading and resource allocation decisions. To
tackle the intractability, some works have designed reduced-
complexity heuristics, such as continuous relaxation in [25]
and decoupling heuristic in [26]. This, however, suffers from
the similar performance-complexity tradeoff dilemma as in [5],
[6], [8], [9].

In this paper, we consider a multi-user MEC network in
Fig. 1, where the computation task data arrive at the WDs’
data queues stochastically in sequential time frames. We aim
to design an online computation ofﬂoading algorithm, in the
sense that the decisions for each time frame are made without
the assumption of knowing the future realizations of random
channel conditions and data arrivals. The objective is to
maximize the network data processing capability subject to the
long-term data queue stability and average power constraints.
To tackle the problem, we propose a Lyapunov-guided Deep
Reinforcement learning (DRL)-based Online Ofﬂoading (Ly-
DROO) framework that combines the advantages of Lyapunov
optimization and DRL. Under fast-varying channel fading and
dynamic task arrivals, LyDROO can make online optimal

2

(cid:38)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:70)(cid:76)(cid:85)(cid:70)(cid:88)(cid:76)(cid:87)

(cid:48)(cid:40)(cid:38) (cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)

(cid:36)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86) (cid:83)(cid:82)(cid:76)(cid:81)(cid:87) (cid:76)(cid:81)(cid:87)(cid:72)(cid:74)(cid:85)(cid:68)(cid:87)(cid:72)(cid:71)
(cid:90)(cid:76)(cid:87)(cid:75) (cid:48)(cid:40)(cid:38) (cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)

(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:82)(cid:73)(cid:73)(cid:79)(cid:82)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)

th
(cid:20)

th
(cid:22)

th
(cid:21)

(cid:58)(cid:39) (cid:22)

(cid:38)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:17) (cid:70)(cid:76)(cid:85)(cid:70)(cid:88)(cid:76)(cid:87)

(cid:58)(cid:39) (cid:20)

(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74) (cid:88)(cid:81)(cid:76)(cid:87)

(cid:38)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:17) (cid:70)(cid:76)(cid:85)(cid:70)(cid:88)(cid:76)(cid:87)

(cid:58)(cid:39) (cid:21)

(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74) (cid:88)(cid:81)(cid:76)(cid:87)

tx (cid:32)

(cid:20)

(cid:20)
(cid:55)(cid:68)(cid:86)(cid:78) (cid:82)(cid:73)(cid:73)(cid:79)(cid:82)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)

tx (cid:32)

(cid:19)
(cid:47)(cid:82)(cid:70)(cid:68)(cid:79) (cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)

(cid:21)

(cid:38)(cid:82)(cid:80)(cid:80)(cid:88)(cid:81)(cid:17) (cid:70)(cid:76)(cid:85)(cid:70)(cid:88)(cid:76)(cid:87)

(cid:55)(cid:68)(cid:86)(cid:78) (cid:84)(cid:88)(cid:72)(cid:88)(cid:72)

(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74) (cid:88)(cid:81)(cid:76)(cid:87)

tx (cid:32)
(cid:22) (cid:20)

(cid:49)(cid:72)(cid:90) (cid:87)(cid:68)(cid:86)(cid:78)
(cid:71)(cid:68)(cid:87)(cid:68) (cid:68)(cid:85)(cid:85)(cid:76)(cid:89)(cid:68)(cid:79)

(cid:40)(cid:81)(cid:72)(cid:85)(cid:74)(cid:92) (cid:86)(cid:88)(cid:83)(cid:83)(cid:79)(cid:92)

(cid:55)(cid:68)(cid:86)(cid:78) (cid:82)(cid:73)(cid:73)(cid:79)(cid:82)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)

Fig. 1: The considered multi-user MEC network in a tagged time
frame.

decisions in real time, while guaranteeing the long-term system
stability. To the authors’ best knowledge, this is the ﬁrst work
that combines Lyapunov optimization and DRL for online
computation ofﬂoading design in MEC networks. The main
contributions of the paper are:

• Online stable computation ofﬂoading design: Considering
random fading channels and data arrivals, we formulate
the problem as a multi-stage stochastic MINLP to max-
imize the long-term average weighted sum computation
rate (i.e., the number of processed bits per second) of
all the WDs, subject to the queue stability and average
power constraints. In particular, we will make the optimal
ofﬂoading and resource allocation decisions in each time
frame without the assumption of knowing the future real-
izations of random channel conditions and data arrivals.
• Integrated Lyapunov-DRL framework: To tackle the prob-
lem, we propose a novel LyDROO framework that com-
bines the advantages of Lyapunov optimization and DRL.
In particular, we ﬁrst apply Lyapunov optimization to de-
couple the multi-stage stochastic MINLP into per-frame
deterministic MINLP problems. Then in each frame, we
integrate model-based optimization and model-free DRL
to solve the per-frame MINLP problems with very low
computational complexity. In particular, we show that the
proposed LyDROO framework not only ensures the long-
term queue stability and average power constraints, but
also obtains the optimal computation rate performance in
an online fashion.

• Integrated optimization and learning: LyDROO adopts
an actor-critic structure to solve the per-frame MINLP
problem. The actor module is a DNN that learns the
optimal binary ofﬂoading action based on the input
environment parameters including the channel gains and
queue backlogs of all the WDs. The critic module eval-
uates the binary ofﬂoading action by analytically solving
the optimal resource allocation problem. Compared to
the conventional actor-critic structure that uses a model-
free DNN in the critic module, the proposed approach
takes advantage of model information to acquire accurate
evaluation of the action, and thus enjoying more robust
and faster convergence of the DRL training process.

• Balanced exploration and exploitation: LyDROO de-
ploys a noisy order-preserving quantization method to
generate ofﬂoading action, which elegantly balances the
exploration-exploitation tradeoff (i.e., performance or di-
versity oriented) in the DRL algorithm design to en-
sure fast training convergence. Besides, the quantization
method can adaptively adjust its parameter during the
training process, which yields signiﬁcant reduction in
computational complexity without compromising the con-
vergence performance.

Simulation results show that the proposed LyDROO algo-
rithm converges very fast to the optimal computation rate while
meeting all the long-term stability constraints. Compared to
a myopic benchmark algorithm that greedily maximizes the
computation rate in each time frame, the proposed LyDROO
achieves a much larger stable capacity region that can stabilize
the data queues under much heavier task data arrivals and more
stringent power constraint.

B. Related Works

Binary and partial computation ofﬂoading are two common
ofﬂoading models in edge computing systems. While the
former requires the entire dataset of a computation task to be
processed as a whole either locally at a wireless device (WD)
or remotely at the edge server, the latter allows the dataset to
be partitioned and executed in parallel at both the WD and
the edge server [2]. In this paper, we focus on the design
of online binary ofﬂoading strategy, which is widely adopted
in IoT networks for executing simple computation tasks with
non-partitionable dataset. Meanwhile, we discuss in Section
VII the application of the proposed LyDROO scheme to design
online partial ofﬂoading strategy when the computation task
consists of multiple independent subtasks.

Reduced-complexity algorithms have been widely explored
in the literature to tackle the intractability of combinatorial
computation ofﬂoading problem in multi-user MEC networks
adopting binary ofﬂoading model. For instance, [7] considers
WDs ofﬂoading their tasks to the neighboring nodes that arrive
and departure in random. It formulates an online stopping
problem and proposes a low-complexity algorithm, where each
WD individually selects the best set of neighboring nodes
in an online manner to minimize the worst-case computation
latency. The proposed method, however, is not suitable for
optimizing a long-term average objective considered in this
paper. [8] proposes a coordinate descent method that iteratively
ﬁnds the local-optimum by ﬂipping the binary ofﬂoading
decision of one user at a time. [5] applies Gibbs sampling
to search the decision space in a stochastic manner. To reduce
the search dimensions, [8] proposes an ADMM (alternating
direction method of multipliers) based method that decom-
poses the original combinatorial optimization into parallel
one-dimension sub-problems. Besides the search-based meta-
heuristic algorithms, existing work has also applied convex
relaxation to handle the binary variables, such as linear
relaxation [8], [25] and quadratic approximation [9]. The
aforementioned optimization methods, however, inevitably en-
counter the performance-complexity tradeoff dilemma when

3

handling integer variables, and are not suitable for online
implementation that requires consistently high solution quality
under fast-varying environment.

DRL has recently appeared as a promising alternative to
solve online computation ofﬂoading problems in MEC net-
works. Existing DRL-based methods take either value-based or
policy-based approach to learn the optimal mapping from the
“state” (e.g., time-varying system parameters) to the “action”
(e.g., ofﬂoading decisions and resource allocation). Commonly
used value-based DRL methods include deep Q-learning net-
work (DQN) [11]–[13], double DQN [14] and dueling DQN
[15], where a DNN is trained to estimate the state-action value
function. However, DQN-based methods are costly when the
number of possible discrete ofﬂoading actions is large, e.g.,
exponential in the number of WDs. To resolve this issue,
recent works have applied policy-based approach, such as the
actor-critic DRL [16]–[18] and the deep deterministic policy
gradient (DDPG) methods [19], [20], to directly construct the
optimal mapping policy from the input state to the output
action using a DNN. For example, [19] considers a WD taking
only discrete ofﬂoading actions, including integer ofﬂoading
decision and discredited transmit power and ofﬂoading rate,
and applies an actor-critic DRL method to learn the optimal
mapping from continuous input state to the discrete output
actions. [18] and [20] train two separate learning modules to
generate discrete ofﬂoading decision and continuous resource
allocation sequentially. Speciﬁcally, [20] applies an actor DNN
to generate the resource allocation solution, concatenated by a
DQN-based critic network to select the discrete ofﬂoading ac-
tion. Similar to [11]–[15], the estimation of state-action value
function in the critic network is difﬁcult when the number
of possible ofﬂoading actions is large. On the other hand,
the DROO framework proposed in [18] uses an actor DNN
to generate a small number of binary ofﬂoading decisions,
followed by a model-based critic module that selects the best
action by analytically solving the optimal resource allocation
problem. Thanks to the accurate evaluation of action acquired
by the critic module, DROO enjoys fast convergence to the
optimal solution even when the actor DNN provides very few
actions (e.g., two actions after sufﬁcient iterations) for the
critic to select from. In this paper, we embed DROO in the
LyDROO framework to solve the per-frame MINLP problems.
The above DRL-based methods fail to address the long-term
performance requirements, e.g., queue stability and average
power, under random environments. In this regard, recent
studies have applied Lyapunov optimization to design an on-
line ofﬂoading strategy with long-term performance guarantee
[22]–[26]. Lyapunov optimization decouples the multi-stage
stochastic problem to per-frame deterministic subproblems.
For each per-frame subproblem, [22] considers the binary
ofﬂoading decision of a single WD. Likewise, [23] schedules
only one user to ofﬂoad to one of the multiple ESs in each time
frame. In both cases, the number of binary ofﬂoading variables
is very small, and hence the optimal solution can be obtained
by brute force search. [24]–[26] consider joint ofﬂoading
decisions of multiple users. Unlike the binary ofﬂoading policy
considered in this paper, [24] allows the WDs to process
task data in parallel both locally and at the ES, and applies

(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:86)(cid:87)(cid:68)(cid:74)(cid:72) (cid:48)(cid:44)(cid:49)(cid:47)(cid:51) (cid:11)(cid:51)(cid:20)(cid:12) (cid:76)(cid:81) (cid:54)(cid:72)(cid:70)(cid:17) (cid:44)(cid:44)

(cid:51)(cid:72)(cid:85)(cid:16)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72) (cid:48)(cid:44)(cid:49)(cid:47)(cid:51) (cid:11)(cid:51)(cid:21)(cid:12) (cid:76)(cid:81) (cid:54)(cid:72)(cid:70)(cid:17) (cid:44)(cid:44)(cid:44)

(cid:47)(cid:92)(cid:39)(cid:53)(cid:50)(cid:50)

(cid:36)(cid:70)(cid:87)(cid:82)(cid:85)
(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)
(cid:38)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)
(cid:48)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:39)(cid:49)(cid:49)(cid:16)(cid:69)(cid:68)(cid:86)(cid:72)(cid:71) (cid:50)(cid:73)(cid:73)(cid:79)(cid:82)(cid:68)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:39)(cid:72)(cid:70)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81) (cid:11)(cid:51)(cid:22)(cid:12) (cid:76)(cid:81) (cid:54)(cid:72)(cid:70)(cid:17) (cid:44)(cid:57)(cid:17)(cid:36)
(cid:38)(cid:82)(cid:81)(cid:89)(cid:72)(cid:91) (cid:53)(cid:72)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)
(cid:36)(cid:79)(cid:79)(cid:82)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:11)(cid:51)(cid:23)(cid:12) (cid:76)(cid:81) (cid:54)(cid:72)(cid:70)(cid:17) (cid:44)(cid:57)(cid:17)(cid:37)

Fig. 2: Organization of the paper.

convex optimization to solve the continuous joint ofﬂoading
and resource allocation problem. In contrast, [25] and [26]
adopt binary ofﬂoading policy where the number of possible
ofﬂoading solutions grows exponentially with the user number.
To tackle the combinatorial problem, [25] relaxes the binary
variables into continuous ones. [26] proposes a two-stage
heuristic, which ﬁrst ﬁxes the resource allocation and then
obtains the binary ofﬂoading decisions using matching theory.
However, these heuristic methods cannot guarantee consis-
tently high solution quality, which may eventually degrade the
long-term performance.

In Fig. 2, we illustrate the organization of the rest of the
paper. In Section II, we formulate the stable computation
ofﬂoading problem as a multi-stage stochastic MINLP problem
(P1). In Section III, we apply the Lyapunov optimization to
decouple (P1) into per-frame deterministic MINLP subprob-
lem (P2). In Section IV, we introduce the LyDROO algorithm
to solve (P2) using an actor-critic DRL. The actor module
implements a DNN to solve the binary ofﬂoading subproblem
(P3) and the critic module applies a customized optimization
algorithm to solve the continuous resource allocation prob-
lem (P4). In Section V, we analyze the performance of the
LyDROO algorithm. In Section VI, we evaluate the proposed
algorithm via extensive simulations. Finally, we conclude the
paper in Section VII.

II. SYSTEM MODEL AND PROBLEM FORMULATION

A. System Model

As shown in Fig. 1, we consider an ES assisting the compu-
tation of N WDs in sequential time frames of equal duration
T . Within the tth time frame, we denote At
i (in bits) as the raw
task data arrival at the data queue of the ith WD. We assume
that the arrival At
i follows a general i.i.d. distribution with
bounded second order moment, i.e., E
(At
= ηi < ∞, for
i = 1, · · · , N . We assume that the value of ηi is known, e.g.,
by estimating from past observations. We denote the channel
gain between the ith WD and the ES as ht
i. Under the block
fading assumption, ht
i remains constant within a time frame
but varies independently across different frames.

i)2

h

i

In the tth time frame, suppose that a tagged WD i processes
Dt
i bits data and produces a computation output at the end of
the time frame. In particular, we assume that the WDs adopt a
binary computation ofﬂoading rule [2]. That is, within each
time frame, the raw data must be processed either locally

4

the WD or remotely at

at
the ES. For instance, WD 1
and 3 ofﬂoad their tasks while WD 2 computes locally in
Fig. 1. The ofﬂoading WDs share a common bandwidth W
for transmitting the task data to the ES in a TDMA manner.
We use a binary variable xt
i to denote the ofﬂoading decision,
where xt
i = 1 and 0 denote that WD i performs computation
ofﬂoading and local computing, respectively.

When the WD processes the data locally (xt

i = 0), we
i , which is upper bounded
. The raw data (in bits) processed locally and the

denote the local CPU frequency as f t
by f max
i
consumed energy within the time frame are [2]
i T /φ, Et

i,L = κ
respectively. Here, parameter φ > 0 denotes the number of
computation cycles needed to process one bit of raw data and
κ > 0 denotes the computing energy efﬁciency parameter.

i,L = f t

T, ∀xt

i = 0,

Dt

f t
i

(1)

(cid:1)

(cid:0)

3

Otherwise, when the data is ofﬂoaded for edge execution
(xt
i = 1), we denote P t
i as the transmit power constrained
i ≤ P max
by the maximum power P t
i T as the amount
i
of time allocated to the ith WD for computation ofﬂoading.
N
Here, τ t
i=1 τ t
i ≤ 1. The energy consumed on
i ∈ [0, 1] and
i τ t
data ofﬂoading is Et
i,O = P t
i T . Similar to [4] and [8], we
P
neglect the delay on edge computing and result downloading
such that the amount of data processed at the edge within the
time frame is

and τ t

Dt

i,O =

=

i T

i T

W τ t
vu
W τ t
vu

log2

1 +

(cid:18)

log2

1 +

i ht
P t
i
N0 (cid:19)
Et
i,Oht
i
τ t
i T N0 !

(2)

, ∀xt

i = 1,

i)Et

rt
i =

i,L + xt

where vu ≥ 1 denotes the communication overhead and N0
denotes the noise power.
i)Dt

iDt
i,L +
i,O denote the bits computed and energy consumed in
i and power

Let Dt
i
xt
iEt
time frame t. We deﬁne computation rate rt
consumption et

i,O and Et
i

, (1 − xt

, (1 − xt

i in the tth time frame as
(1 − xt
φ

W τ t
i
vu

et
i,Oht
i
τ t
i N0 !

Dt
i
T
Et
i
T
where et
i,O/T . For simplicity of exposition, we assume
T = 1 without loss of generality in the following derivations.
Let Qi(t) denote the queue length of the ith WD at the
beginning of the tth time frame. Then, the queue dynamics
can be modeled as

= (1 − xt

, Et

+ xt
i

et
i =

+ xt

i)f t
i

log2

i,O,

iet

i)κ

1 +

f t
i

(3)

i,O

=

(cid:0)

(cid:1)

3

,

Qi(t + 1) = max

Qi(t) − ˜Dt

i + At

i, 0

, i = 1, 2, · · · , (4)

n

i = min (Qi(t), Dt

where ˜Dt
i) and Qi(1) = 0. In this paper, we
consider inﬁnite queueing capacity for analytical tractability.
In the following derivation, we enforce the data causality
constraint Dt
i ≤ Qi(t), implying that Qi(t) ≥ 0 holds for
any t. Thus, the queue dynamics is simpliﬁed as

o

Qi(t + 1) = Qi(t) − Dt

i + At
i,

i = 1, 2, · · · .

(5)

Deﬁnition 1: A discrete time queue Qi(t) is strongly stable
E [Qi(t)] <

if the time average queue length limK→∞

1
K P

K
t=1

 
 
∞, where the expectation is taken with respect to the system
random events [21], i.e., channel fading and task data arrivals
in this paper.

By the Little’s law, the average delay is proportional to
the average queue length. Thus, a strongly stable data queue
translates to a ﬁnite processing delay of each task data bit.

B. Problem Formulation

In this paper, we aim to design an online algorithm to
maximize the long-term average weighted sum computation
rate of all the WDs under the data queue stability and average
power constraints. In particular, we make online decisions
in the sense that in each time frame, we optimize the task
ofﬂoading and the resource allocation decisions for the par-
ticular time frame without the assumption of knowing the
future realizations of random channel conditions and data
arrivals. We denote xt = [xt
1, · · · , xt
1, · · · , τ t
N ],
1,O, · · · , et
et
f t = [f t
, and let x =
O =
O}K
t=1, f = {f t}K
{xt}K
t=1 and eO = {et
t=1. We
(cid:2)
formulate the problem as the following multi-stage stochastic
MINLP problem (P1):

1, · · · , f t
t=1, τ = {τ t}K

N ], τ t = [τ t
N,O

N ] and et

(cid:3)

lim
K→∞

1
K

·

K
t=1

N

i=1cirt

i

maximize
x,τ ,f ,eO
subject to
i=1τ t

N

i ≤ 1, ∀t,

P
(1 − xt

i)f t

i /φ + xt
i

P

P

W τ t
i
vu

log2

1 +

i,Oht
et
i
τ t
i N0 !

≤ Qi(t), ∀i, t,

(6a)

(6b)

≤ γi, ∀i,

i

(6c)

(6d)

lim
K→∞

1
K

·

K
t=1

E

(1 − xt

i)κ

f t
i

3

+ xt

iet

i,O

P

1
K

h

(cid:1)
E [Qi(t)] < ∞, ∀i,

(cid:0)

·

(6f)

(6e)

K
lim
t=1
K→∞
i,O ≤ P max
, et
i ≤ f max
f t
P
i
i , et
i , f t
i ∈ {0, 1} , τ t
xt

τ t
i , ∀i, t,
i
i,O ≥ 0, ∀i, t.
Here, ci denotes the ﬁxed weight of the ith WD. (6a) denotes
the ofﬂoading time constraint. Notice that τ t
i = et
i,O = 0 must
hold at the optimum if xt
i = 0. Similarly, f t
i = 0 must hold
if xt
i = 1. (6b) corresponds to the data causality constraint.
(6c) corresponds to the average power constraint and γi is the
power threshold. (6d) are the data queue stability constraints.
Under the stochastic channels and data arrivals, it is hard to
satisfy the long-term constraints when the decisions are made
in each time frame without knowing the future realizations
of random channel conditions and data arrivals. Besides, the
fast-varying channel condition requires real-time decision-
making in each short time frame, e.g., within the channel
coherence time. In the following, we propose a novel LyDROO
framework that solves (P1) with both high robustness and
efﬁciency.

Remark 1: Before leaving this session, we comment on the
possible extension of the proposed LyDROO algorithm. (P1)
uses a linear utility function U (rt
in the objective.
However, we will show later in Section IV that the proposed
LyDROO framework is applicable to solve a wide range of

i ) = rt
i

5

problems as long as the resource allocation problem (P4) can
be efﬁciently solved. For instance, we can consider a general
non-decreasing concave function U (rt
i ) such that the corre-
sponding (P4) is a convex problem, e.g., α-fairness function
i )1−α with α ≥ 0 and α 6= 1, proportional fairness
(1−α)−1 (rt
function ln(rt
i ), or other suitable QoS (quality of service)
utilities (see [27] and the reference therein). For analytical
clarity, we consider in this paper a speciﬁc linear utility
function to highlight the features of the LyDROO framework.

III. LYAPUNOV-BASED DECOUPLING OF THE
MULTI-STAGE MINLP

In this section, we apply the Lyapunov optimization to
decouple (P1) into per-frame deterministic problems. To cope
with the average power constraints (6c), we introduce N vir-
tual energy queues {Yi(t)}N
i=1, one for each WD. Speciﬁcally,
we set Yi(1) = 0 and update the queue as

,

(cid:0)

(7)

Yi(t) + νet

Yi(t + 1) = max

i − νγi, 0
for i = 1, · · · , N and t = 1, · · · , K, where et
i in (3) is the
energy consumption at the tth time frame and ν is a positive
scaling factor. Yi(t) can be viewed as a queue with random
“energy arrivals” νet
i and ﬁxed “service rate” νγi. Intuitively,
when the virtual energy queues are stable, the average power
consumption et
i (i.e., the virtual queue arrival rate) does not
exceed γi, and thus the constraints in (6c) are satisﬁed.

(cid:1)

To jointly control the data and energy queues, we deﬁne
Z(t) = {Q(t), Y(t)} as the total queue backlog, where
Q(t) = {Qi(t)}N
i=1. Then, we
introduce the Lyapunov function L (Z(t)) and Lyapunov drift
∆L (Z(t)) as [21]

i=1 and Y(t) = {Yi(t)}N

N

N

(cid:16)P

i=1Yi(t)2

i=1Qi(t)2 +

L (Z(t)) = 0.5

,
∆L (Z(t)) = E {L (Z(t + 1)) − L (Z(t)) |Z(t)} .
(cid:17)
To maximize the time average computation rate while stabiliz-
ing the queue Z(t), we use the drift-plus-penalty minimization
approach [28]. Speciﬁcally, we seek to minimize an upper
bound on the following drift-plus-penalty expression at every
time frame t:

(8)

P

Λ (Z(t)) , ∆L (Z(t)) − V ·

N
i=1

E

cirt

i |Z(t)

,

(9)

where V > 0 is an “importance” weight to scale the penalty.
In the following, we derive an upper bound of Λ (Z(t)). To

P

(cid:8)

(cid:9)

begin with, we have

Qi(t + 1)2 = Qi(t)2 + 2Qi(t)
Yi(t + 1)2 = Yi(t)2 + 2Yi(t)

At
et
i − γi
(cid:0)

i − Dt
i
+
(cid:1)

At

+
et
i − γi
(cid:0)

i − Dt
i
2
.

2

,

(cid:1)

By taking the sum over the N queues on both sides, we have

N

(cid:0)
i=1Qi(t + 1)2 − 0.5
i − Dt
i

N
i=1

At

(cid:0)
(cid:1)
i=1Qi(t)2
N
At
i=1Qi(t)
P

+

N

2

0.5

=0.5

P

i − Dt
i

(cid:1)

and

P

0.5

=0.5

P
We deﬁne
P

(cid:1)

(cid:0)

N

P
i=1Yi(t + 1)2 − 0.5
et
i − γi

N
i=1

+

2

N

(cid:0)
i=1Yi(t)2
et
i − γi

N
i=1Yi(t)
P

(cid:1)
(cid:0)
P
L (Q(t)) , 0.5

(cid:0)

N

i=1Qi(t)2

P

(10)

(11)

(12)

(cid:1)

.

(cid:1)

 
and

∆L (Q(t)) , E {L (Q(t + 1)) − L (Q(t)) |Z(t)} .

(13)

By taking the conditional expectation on both sides of (10),
we have

maximize
O,rt
O

xt,τ t,f t,et

subject to

N

∆L (Q(t)) ≤ B1 +

i=1Qi(t)E
Here, B1 is a constant obtained as
At

P
i − Dt
i

≤ 0.5

0.5

N
i=1

E

2

At

i − Dt
i

|Z(t)

.

(14)

(cid:2)(cid:0)
N
i=1

E

(cid:1)
At
i

2

+

(cid:3)
Dt
i

2

P
≤ 0.5

h(cid:0)
N
i=1

i
(cid:1)
ηi + [T max {f max
P

h(cid:0)
/φ, rmax
i

i

}]2
(cid:1)

, B1,
(cid:1)
(cid:0)

i

N

i=1Yi(t)et

i

P

N

i=1 (Qi(t) + V ci) rt

i −

P

N

i=1τ t
i ≤ 1,
i /φ ≤ Qi(t), rt
f t
P
W τ t
i
vu
f t
i ≤ f max
, et
i
xt
i ∈ {0, 1} , τ t

rt
i,O ≤

log2

1 +

i,O ≤ Qi(t), ∀i,
i,Oht
et
i
τ t
i N0 !
i,O ≤ P max
τ t
i , ∀i,
i
i , f t
i , et
i,O ≥ 0, ∀i.

, ∀i, (22c)

6

(22a)

(22b)

(22d)

(22e)

where the second inequality holds because rmax
E

(cid:16)
1 + P max
N0
erage transmission rate of the ith WD.

P
log2

corresponds to the maximum av-

W
vu

,

ht
i

(cid:17)

i

i

h

(cid:16)

(cid:17)i

Similarly, we deﬁne

L (Y(t)) = 0.5

N

i=1Yi(t)2

(16)

and

P

∆L (Y(t)) , E {L (Y(t + 1)) − L (Y(t)) |Z(t)} .

(17)

We obtain the following by taking the expectation on both
sides of (11)

∆L (Y(t)) ≤ B2 +

N

i=1Yi(t)E

et
i − γi|Z(t)

,

(18)

where the constant B2 is obtained from

(cid:2)

(cid:3)

0.5

E

N
i=1

et
i − γi

P
2

P
≤ 0.5

h(cid:0)
N
i=1

max

(cid:1)

i
κ (f max
i

)3 , P max
i

2

+ γ2
i

, B2.

(cid:20)(cid:16)

n

o(cid:17)

(cid:21)

P

Summing over the two inequalities in (14) and (18), we have
i=1Qi(t)E
i − Dt
i
et
i − γi|Z(t)
(cid:1)
(cid:2)(cid:0)

∆L (Z(t)) ≤ ˆB +

N
i=1Yi(t)E
P

|Z(t)

(19)

At

+

N

(cid:3)

P

where ˆB = B1 + B2. Therefore, the upper bound of the drift-
plus-penalty expression in (9) is
ˆB +
Qi(t)E
At
i − Dt
i
+ Yi(t)E
et
i − γi|Z(t)
(cid:1)
(cid:2)(cid:0)
(cid:8)

− V E
(cid:3)

i |Z(t)

|Z(t)

cirt

N
i=1

(20)

(cid:3)

(cid:2)

.

P

(cid:2)

(cid:3) (cid:9)

(cid:3)

(cid:2)

In the tth time frame, we apply the technique of oppor-
tunistic expectation minimization [21]. That is, we observe
the queue backlogs Z(t) and decide the joint ofﬂoading and
resource allocation control action accordingly to minimize the
upper bound in (20). Notice that only the second term is related
to the control action in the tth time frame. By removing the
constant terms from the observation at the beginning of the tth
time frame, the algorithm decides the actions by maximizing
the following:
N

i=1 (Qi(t) + V ci) rt
where rt
i are in (3). Intuitively, it tends to increase the
P
computation rates of WDs that have a long data queue backlog
or a large weight, while penalizing those that have exceeded
the average power threshold. We introduce an auxiliary vari-
able rt
. Taking
into account the per-frame constraints, we solve the following
deterministic per-frame subproblem (P2) in the tth time frame

i,O for each WD i and denote rt

i=1Yi(t)et
i,

i and et
P

rt
i,O

O =

(21)

i −

i=1

(cid:9)

(cid:8)

N

N

Notice that the above constraints (22b) and (22c) are equiva-
lent to (6b) in (P1), because there is exactly one non-zero term
in the left-hand side of (6b) at the optimum. In Section V, we
will show that we can satisfy all long-term constraints in (P1)
by solving the per-frame subproblems in an online fashion.
Then, the remaining difﬁculty lies in solving the MINLP (P2)
in each time frame. In the following section, we propose a
DRL-based algorithm to solve (P2) efﬁciently.

IV. LYAPUNOV-GUIDED DRL FOR ONLINE COMPUTATION
OFFLOADING

i , et

i , f t
τ t

i,O, rt

i, Qi(t), Yi(t)}N

Recall that to solve (P2) in the tth time frame, we observe
ξt , {ht
i=1, consisting of the channel gains
i}N
i=1 and the system queue states {Qi(t), Yi(t)}N
{ht
i=1, and
accordingly decide the control action {xt, yt}, including the
binary ofﬂoading decision xt and the continuous resource allo-
N
cation yt ,
. A close observation shows
that although (P2) is a non-convex optimization problem, the
(cid:9)
resource allocation problem to optimize yt is in fact an “easy”
convex problem if xt is ﬁxed. In Section IV.B, we will propose
a customized algorithm to efﬁciently obtain the optimal yt
given xt in (P2). Here, we denote G
as the optimal
value of (P2) by optimizing yt given the ofﬂoading decision
xt and parameter ξt. Therefore, solving (P2) is equivalent to
ﬁnding the optimal ofﬂoading decision (xt)∗, where

xt, ξt

i=1

i,O

(cid:8)

(cid:0)

(cid:1)

(P3) :

∗

xt

= arg maximize
xt∈{0,1}N

G

xt, ξt

.

(23)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

In general, obtaining (xt)∗ requires enumerating 2N of-
ﬂoading decisions, which leads to signiﬁcantly high computa-
tional complexity even when N is moderate (e.g., N = 10).
Other search based methods, such as branch-and-bound and
block coordinate descent [29], are also time-consuming when
N is large. In practice, neither method is applicable to online
decision-making under fast-varying channel condition. Lever-
aging the DRL technique, we propose a LyDROO algorithm
to construct a policy π that maps from the input ξt to the
optimal action (xt)∗, i.e., π : ξt 7→ (xt)∗, with very low
complexity, e.g., tens of milliseconds computation time (i.e.,
the time duration from observing ξt to producing a control
action {xt, yt}) when N = 10.

A. Algorithm Description

As illustrated in Fig. 3, LyDROO consists of four main
modules: an actor module that accepts the input ξt and outputs
a set of candidate ofﬂoading actions {xt
i}, a critic module

 
(cid:20)(cid:12) (cid:36)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:21)(cid:12) (cid:38)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)(cid:3)(cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

7

(cid:38)(cid:75)(cid:68)(cid:81)(cid:81)(cid:72)(cid:79)(cid:3)(cid:74)(cid:68)(cid:76)(cid:81)(cid:86)
(cid:94) (cid:96)t N
ih (cid:32)

i

(cid:20)

t
(cid:32)(cid:543)

(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:82)(cid:73)(cid:3)(cid:39)(cid:49)(cid:49)
(cid:94)

t
h Q t Y t
(cid:11) (cid:12)(cid:15)
(cid:11) (cid:12)
i
i

(cid:15)

i

N

(cid:96) (cid:20)

(cid:32)

i

(cid:11)
(cid:51) (cid:543)
t
(cid:84)

t

(cid:12)

(cid:214) t(cid:91)

t(cid:91)

(cid:20)

(cid:22115)

t
M(cid:91)

t

(cid:49)(cid:50)(cid:51)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:52)(cid:88)(cid:68)(cid:81)(cid:87)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:11)
(cid:20)(cid:15)t
G (cid:91) (cid:543)

t

(cid:12)

(cid:22115)

(cid:36)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)
t(cid:91)

(cid:45)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:96)(cid:15)t
(cid:94)
(cid:91) (cid:92)

t

(cid:68)(cid:85)(cid:74)(cid:3)
(cid:48)(cid:68)(cid:91)

t

(cid:12)

t

(cid:11)
MG (cid:91)

(cid:543)

(cid:15)t
(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:3)(cid:82)(cid:83)(cid:87)(cid:76)(cid:80)(cid:68)(cid:79)(cid:3)
(cid:85)(cid:72)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:3)(cid:68)(cid:79)(cid:79)(cid:82)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:11)(cid:51)(cid:23)(cid:12)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)
(cid:39)(cid:49)(cid:49)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)

(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)
(cid:85)(cid:68)(cid:81)(cid:71)(cid:82)(cid:80)(cid:3)(cid:69)(cid:68)(cid:87)(cid:70)(cid:75)

j

(cid:96)

j
(cid:94) (cid:15)
(cid:543) (cid:91)
(cid:22115)
(cid:15)
(cid:91)
t

(cid:16)
(cid:20)

(cid:16)

(cid:21)

t

t

t

(cid:94)
(cid:543)
(cid:94)
(cid:543)

(cid:53)(cid:72)(cid:83)(cid:79)(cid:68)(cid:92)(cid:3)(cid:48)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)

(cid:16)

(cid:21)

(cid:96)

(cid:15)

(cid:91)

(cid:16)
(cid:20)

(cid:96)

(cid:22)(cid:12) (cid:51)(cid:82)(cid:79)(cid:76)(cid:70)(cid:92)(cid:3)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72) (cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

(cid:94)(cid:36) (cid:96)t N
(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:68)(cid:85)(cid:85)(cid:76)(cid:89)(cid:68)(cid:79)(cid:86)

i(cid:32)
(cid:20)

i

Q
(cid:94) (cid:11)(cid:87)(cid:12)(cid:15)
i

Y
i

(cid:11)(cid:87)(cid:12)(cid:96)N

i

(cid:32)
(cid:20)

(cid:56)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72)(cid:3)(cid:84)(cid:88)(cid:72)(cid:88)(cid:72)(cid:86)

t
(cid:94) (cid:15) (cid:96)
(cid:543) (cid:91)

t

(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:72)(cid:71)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)

N
t
D e (cid:32)
(cid:15) (cid:96)
i
i
(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:72)(cid:71)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:72)(cid:81)(cid:72)(cid:85)(cid:74)(cid:92)(cid:3)(cid:70)(cid:82)(cid:81)(cid:86)(cid:88)(cid:80)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:94)

t
i

(cid:20)

(cid:40)(cid:91)(cid:72)(cid:70)(cid:88)(cid:87)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:77)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:23)(cid:12) (cid:52)(cid:88)(cid:72)(cid:88)(cid:72)(cid:76)(cid:81)(cid:74) (cid:80)(cid:82)(cid:71)(cid:88)(cid:79)(cid:72)

Fig. 3: The schematics of the proposed LyDROO algorithm.

i} and selects the best ofﬂoading action xt, a
evaluates {xt
policy update module improves the policy of the actor module
over time, and a queueing module updates the system queue
states {Qi(t), Yi(t)}N
i=1 after executing the ofﬂoading actions.
Through repeated interactions with the random environment
{ht
i=1, the four modules operate in a sequential and
iterative manner as detailed below.

i, At

i}N

1) Actor Module: The actor module consists of a DNN
and an action quantizer. At the beginning of the tth time
frame, we denote the parameter of the DNN as θt, which is
randomly initialized following the standard normal distribution
when t = 1. Taking the observation ξt as the input, the DNN
outputs a relaxed ofﬂoading decision ˆxt ∈ [0, 1]N that is later
to be quantized into feasible binary actions. The input-output
relation is expressed as
Πθt : ξt 7→ ˆxt =

ˆxt
i ∈ [0, 1], i = 1, · · · , N

(24)

.

(cid:8)

The well-known universal approximation theorem claims that
a multi-layer perceptron with a sufﬁcient number of neurons
can accurately approximate any continuous mappings if proper
activation functions are applied at the neurons, e.g., sigmoid,
ReLu, and tanh functions [30]. Here, we use a sigmoid
activation function at the output layer.

(cid:9)

(cid:8)

j|xt
xt

j ∈ {0, 1}N , j = 1, · · · , Mt

We then quantize the continuous ˆxt into Mt feasible candi-
date binary ofﬂoading actions, where Mt is a time-dependent
design parameter. The quantization function is expressed as:
ΥMt : ˆxt 7→ Ωt =
where Ωt denotes the set of candidate ofﬂoading actions in
the tth time frames. ΥMt represents a quantization function
that generates Mt = |Ωt| binary actions. A good quantization
function should balance the exploration-exploitation tradeoff
in generating the ofﬂoading action to ensure good training con-
’s should be close to ˆxt (measured
vergence. Intuitively,
by Euclidean distance) to make effective use of the DNN’s
output and meanwhile sufﬁciently separate to avoid premature
convergence to sub-optimal solution in the training process.

, (25)

xt
j

(cid:9)

(cid:9)

(cid:8)

Here, we apply the noisy order-preserving (NOP) quantiza-
tion method [31], which can generate any Mt ≤ 2N candidate

actions. The NOP method generates the ﬁrst Mt/2 actions (Mt
is assumed an even number) by applying the order-preserving
quantizer (OPQ) in [18] to ˆxt. Speciﬁcally, the 1st action
1 = [xt
xt

1,N ] is calculated as

1,1, · · · , xt

xt
1,i =

1
0

(

ˆxt
i > 0.5,
ˆxt
i ≤ 0.5,

(26)

(1) − 0.5| ≤ |ˆxt
(N ) − 0.5|, where ˆxt

for i = 1, · · · , N . To generate the next Mt/2 − 1 actions,
we order the entries of ˆxt based on the distance to 0.5, such
(2) − 0.5| ≤ · · · ≤ |ˆxt
that |ˆxt
(i) − 0.5| · · · ≤
|ˆxt
(i) denotes the ith ordered entry of ˆxt.
(i)’s are used as the decision thresholds to quantize ˆxt,
Then, ˆxt
where the mth action xt
m, for m = 2, · · · , Mt/2, is obtained
from entry-wise comparisons of ˆxt and ˆxt

(m−1). That is,

xt
m,i =

1,

0,






i > ˆxt
ˆxt
(m−1)
i = ˆxt
ˆxt
or
i < ˆxt
ˆxt
n
(m−1)
i = ˆxt
ˆxt
or

(m−1) and ˆxt

(m−1) ≤ 0.5

(m−1) and ˆxt

(m−1) > 0.5

(27)

o

,

,

o

n

for i = 1, · · · , N . To obtain the remaining Mt/2 actions,
we ﬁrst generate a noisy version of ˆxt denoted as ˜xt =
Sigmoid (ˆxt + n), where the random Gaussian noise n ∼
N (0, IN ) with IN being an identity matrix, and Sigmoid (·)
is the element-wise Sigmoid function that bounds each entry
of ˜xt within (0, 1). Then, we produce the remaining Mt/2
actions xt
m, for m = Mt/2 + 1, · · · , Mt, by applying the
OPQ to ˜xt, i.e., replacing ˆxt with ˜xt in (26) and (27).

2) Critic Module: Followed by the actor module, the critic
module evaluates {xt
i} and selects the best ofﬂoading action
xt. Unlike the conventional actor-critic structure that uses a
model-free DNN as the critic network to evaluate the action,
LyDROO leverages the model information to evaluate the
binary ofﬂoading action by analytically solving the optimal
resource allocation problem. This enables the critic module to
have accurate evaluation of the ofﬂoading actions, and thus
achieving more robust and faster convergence of the DRL
training process.

Speciﬁcally, LyDROO selects the best action xt as

xt = arg max
j ∈Ωt

xt

G

j, ξt
xt

,

(28)

(cid:0)

(cid:1)

j, ξt
xt

(cid:1)

where G
is obtained by optimizing the resource
allocation given xt
j in (P2). We will introduce the detailed
(cid:0)
algorithm to obtain G
in Section IV.B. Notice that
the calculation of G
is performed by Mt times to
(cid:1)
obtain the best action xt. Intuitively, a larger Mt results in
better solution performance, but a larger computation time. To
balance the performance-complexity tradeoff, we propose here
an adaptive procedure to set a time-varying Mt.

j , ξt
xt
j, ξt
xt
(cid:0)

(cid:0)

(cid:1)

The key idea is that when the actor DNN gradually ap-
proaches the optimal policy over time, a small Mt sufﬁces
to ﬁnd the optimal action within a small distance to ˆxt.
Denote mt ∈ [0, Mt − 1] as the index of the best action
xt ∈ Ωt. We deﬁne m∗
t = mod(mt, Mt/2), which represents
the order of xt among either the Mt/2 noise-free or the
noise-added candidate actions. In practice, we set a maximum
M1 = 2N initially and update Mt every δM ≥ 1 time frames.
If mod (t, δM ) = 0 in time frame t, i.e., t can be divided by
δM , we set

Algorithm 1: The online LyDROO algorithm for solv-
ing (P1).

8

input : Parameters V , {γi, ci}N

i=1, K, training interval δT , Mt update

interval δM ;

output: Control actions

K
t=1
1 Initialize the DNN with random parameters θ1 and empty replay memory,

xt, yt

(cid:9)

(cid:8)

;

M1 ← 2N ;

2 Empty initial data queue Qi(1) = 0 and energy queue Yi(1) = 0, for

i = 1, · · · , N ;

3 for t = 1, 2, . . . , K do

4

5

6

7

8

9

10

11

12

13

14

15

Observe the input ξt =

ht, Qi(t), Yi(t)

(8) if mod (t, δM ) = 0;

(cid:8)

N
i=1

(cid:9)

and update Mt using

Generate a relaxed ofﬂoading action ˆxt = Πθt
Quantize ˆxt into Mt binary actions

ξt
(cid:0)
i|i = 1, · · · , Mt

with the DNN;
using the

xt

(cid:1)

(cid:9)

by optimizing resource allocation yt

i in (P2) for

(cid:8)

NOP method;
xt
i, ξt

Compute G
each xt
i;

(cid:0)

(cid:1)
Select the best solution xt = arg max
{xt
i }

xt, yt

joint action

;
(cid:1)
Update the replay memory by adding (ξt, xt);
if mod (t, δT ) = 0 then

(cid:0)

i, ξt
xt

G

(cid:0)

(cid:1)

and execute the

Uniformly sample a batch of data set {(ξτ , xτ ) | τ ∈ St} from

the memory;

Train the DNN with {(ξτ , xτ ) | τ ∈ St} and update θt using

the Adam algorithm;

end
t ← t + 1;
Update {Qi(t), Yi(t)}N
i=1 based on
N
observation nAt−1

i o

i=1

using (5) and (7).

xt−1, yt−1

(cid:0)

and data arrival

(cid:1)

Mt = 2 · min

max

m∗

t−1, · · · , m∗

t−δM

+ 1, N

.

(29)

16 end

(cid:1)

(cid:0)

(cid:0)

(cid:1)

ξt, xt

The additional 1 in the ﬁrst term within the min operator
allows Mt to increase over time. Otherwise, Mt = Mt−1 if
mod (t, δM ) 6= 0. Notice that too frequent update (small δM )
may degrade the training convergence while a too large δM
cause unnecessary computational complexity.
3) Policy Update Module: LyDROO uses

as a
labeled input-output sample for updating the policy of the
DNN. In particular, we maintain a replay memory that only
stores the most recent q data samples. In practice, with an
initially empty memory, we start
training the DNN after
collecting more than q/2 data samples. Then, the DNN is
trained periodically once every δT time slots to avoid model
over-ﬁtting. When mod (t, δT ) = 0, we randomly select a
batch of data samples {(ξτ , xτ ) , τ ∈ St}, where St denotes
the set of time indices of the selected samples. We then update
the parameter of the DNN by minimizing its average cross-
entropy loss function LS
over the data samples using the
Adam algorithm [30]

θt

(cid:1)

(cid:0)

LS(θt) = − 1/|St| ·

(cid:0)

(cid:1)

τ ∈S t

(xτ )⊺ log Πθt (ξτ )
h
+ (1 − xτ )⊺ log
P

1 − Πθt (ξτ )

(30)

,

where |St| denotes the size of the sample batch, (·)⊺ de-
notes the transpose operator, and the log function denotes
the element-wise logarithm operation of a vector. When the
training completes, we update the parameter of the actor
module in the next time frame to θt+1.

(cid:0)

(cid:1)i

4) Queueing Module: As a by-product of the critic module,
we obtain the optimal resource allocation yt associated with
xt. Accordingly, the system executes the joint computation
ofﬂoading and resource allocation action {xt, yt}, which
processes data {Dt
i}N
i=1 as given
i}N
in (3). Based on {Dt
i=1
observed in the tth time frame, the queueing module then

i=1 and consumes energy {et
i, et

i=1 and the data arrivals {At

i}N

i}N

(cid:9)

N
i=1

the beginning of

updates the data and energy queues {Qi(t + 1), Yi(t + 1)}N
i=1
using (5) and (7) at
the (t + 1)th
time frame. With the wireless channel gains observation
the system feeds the input parameter ξt+1 =
{ht+1
i
ht+1
to the DNN and starts a new
i
iteration from the actor module in Step 1).
(cid:8)

}N
i=1,
, Qi(t + 1), Yi(t + 1)

With the above actor-critic-update loop, the DNN consis-
tently learns from the best and most recent state-action pairs,
leading to a better policy πθt that gradually approximates the
optimal mapping to solve (P3). We summarize the pseudo-
code of LyDROO in Algorithm 1, where the major com-
i, ξt
xt
putational complexity is in line 7 that computes G
by solving the optimal resource allocation problems. This
(cid:1)
in fact indicates that the proposed LyDROO algorithm can
be extended to solve (P1) when considering a general non-
decreasing concave utility U (rt
i ) in the objective, because the
i, ξt
xt
per-frame resource allocation problem to compute G
is a convex problem that can be efﬁciently solved, where the
(cid:1)
detailed analysis is omitted. In the next subsection, we propose
a low-complexity algorithm to obtain G

(cid:0)

(cid:0)

.

i, ξt
xt

B. Low-complexity Optimal Resource Allocation Algorithm

(cid:0)

(cid:1)

i = 1 as Mt

Given the value of xt in (P2), we denote the index set of
users with xt
1, and the complementary user set
as Mt
0. For simplicity of exposition, we drop the superscript
t and express the optimal resource allocation problem that
computes G

as following

xt, ξt

(P4) : maximize
(cid:0)
(cid:1)
τ ,f ,eO,rO

j∈M0

ajfj/φ − Yj(t)κf 3
j

P

+

(cid:8)
i∈M1

{airi,O − Yi(t)ei,O}

(cid:9)

subject to
fj/φ ≤ Qj(t), 0 ≤ fj ≤ f max

P

j

, ∀j ∈ M0,

τi ≤ 1,

i∈M1
ei,O ≤ P max
P
i
W τi
vu

ri,O ≤

τi, ri,O ≤ Qi(t), ∀i ∈ M1,
ei,Ohi
τiN0 (cid:19)

1 +

log2

(cid:18)

, ∀i ∈ M1,

τi, ri,O, ei,O ≥ 0, ∀i ∈ M1,

where ai , Qi(t)+V ci is a parameter. Notice that (P4) can be
separately optimized for WDs in M1 and M0. In particular,
each j ∈ M0 solves an independent problem

maximize
fj
subject to

ajfj/φ − Yj(t)κf 3
j

0 ≤ fj ≤ min

φQj(t), f max

j

,

(32)

where the closed-form optimal solution is

(cid:8)

(cid:9)

f ∗
j = min

aj
3φκYj(t)

(cid:26)r

, min

φQj(t), f max

j

, ∀j ∈ M0.

(cid:27)

(33)
(cid:8)
Intuitively, the jth WD computes faster when Qj(t) is large
or Yj(t) is small, and vice versa.

(cid:9)

On the other hand, denote ˆτ = {τi, ∀i ∈ M1}, ˆeO =
{ei,O, ∀i ∈ M1} and ˆrO = {ri,O, ∀i ∈ M1}, we need to
solve the following problem for the WDs in M1,

maximize
ˆτ ,ˆeO,ˆrO
subject to

i∈M1

{airi,O − Yi(t)ei,O}

τi ≤ 1,

P
i∈M1
ei,O ≤ P max
P
i
W τi
vu

ri,O ≤

τi, ri,O ≤ Qi(t), ∀i ∈ M1,
ei,Ohi
τiN0 (cid:19)

1 +

log2

(cid:18)

, ∀i ∈ M1.

τi, ri,O, ei,O ≥ 0, ∀i ∈ M1.

We express a partial Lagrangian of the problem as

L ({ˆτ , ˆeO, ˆrO} , µ)

=

i∈M1

{airi,O − Yi(t)ei,O} + µ

1 −

i∈M1

τi

,

(35)

where µ denotes the dual variable. Furthermore, the dual
P
function is

P

(cid:1)

(cid:0)

d(µ) = maximize

L ({ˆτ , ˆeO, ˆrO} , µ)

ˆτ ,ˆeO,ˆrO
subject to
ei,O ≤ P max
i
W τi
vu

ri,O ≤

τi, ri,O ≤ Qi(t), ∀i ∈ M1,
ei,Ohi
τiN0 (cid:19)

1 +

log2

(cid:18)

, ∀i ∈ M1,

τi, ri,O, ei,O ≥ 0, ∀i ∈ M1

and the dual problem is minimize

d(µ). Notice that the dual

µ≥0
function can be decomposed into parallel sub-problems. For a
WD i ∈ M1, it solves

maximize
τi,ei,O ,ri,O
subject to

{airi,O − Yi(t)ei,O} − µτi

τi ≥ 0, 0 ≤ ei,O ≤ P max
0 ≤ ri,O ≤ Qi(t),

i

τi,

ri,O ≤

W τi
vu

log2

1 +

(cid:18)

ei,Ohi
τiN0 (cid:19)

In the following, we propose a simple algorithm that solves
(37) efﬁciently.

Notice that equality (37d) holds at the optimum because

(37a)

(37b)
(37c)

.

(37d)

9

log2

1 + ei,Ohi
τiN0

otherwise we can reduce the value of ei,O at
the opti-
mum to achieve a higher objective. By setting ri,O =
W τi
in (37d), we can equivalently write the
vu
constraint 0 ≤ ei,O ≤ P max

(cid:16)
ri,O
hi
N0 (cid:19)
τi
where Rmax
denotes the maximum transmission rate of the
ith WD. From (3), we express ei,O as a function of ri,O and
τi as

τi in (37b) as
P max
i

, Rmax
i

W
vu

log2

(38)

0 ≤

1 +

≤

(cid:18)

(cid:17)

,

i

i

2

ei,O =

ri,O vu
W τi − 1

N0τi
hi
where g(x) , N0
is a convex function. By plug-
ging (38) and (39) into (37), we can equivalently transform
(cid:1)
(37) as the following problem

ri,O
τi (cid:19)

(cid:16)
xvu
W − 1

, τi
hi

(39)

(cid:18)

(cid:17)

2

g

(cid:0)

,

maximize
ri,O,τi

airi,O − µτi − Yi(t)

subject to τi ≥ ri,O/Rmax

i

τi
hi

g

ri,O
τi (cid:19)

(cid:18)
, 0 ≤ ri,O ≤ Qi(t).

(40)

Notice that (40), and thus (37), is equivalent to the following
problem

maximize
ri,O

{Vi(ri,O)|0 ≤ ri,O ≤ Qi(t)} ,

(41)

where

Vi(ri,O) , maximize

τi
subject to

airi,O − µτi − Yi(t)

τi ≥ ri,O/Rmax

.

i

τi
hi

g

ri,O
τi (cid:19)

(cid:18)

(42)
(42) is a convex problem, where we derive the optimal solution
in the following Proposition 1.

Proposition 1: The optimal solution of (42) is

ri,O
Rmax
i

,

τ ∗
i =


W ·hW(cid:16)e−1

where ψi(µ) , N0

P max
i

ln 2vu·ri,O

µhi
Yi (t)N0

h

−1i(cid:17)+1i

if hi ≤ ψi(µ),

, otherwise,

(43)

Ai

−W(−Ai exp(−Ai)) − 1

and Ai , 1 +
µ
are ﬁxed parameters given µ. W(x) denotes the
Yi(t)Pmax
Lambert-W function, which is the inverse function of J(z) =
z exp(z) = x, i.e., z = W(x).

(cid:17)

(cid:16)

Proof : Please see the detailed proof in the Appendix A. (cid:4)

Remark 2: A close observation of (43) shows that we can
compactly express the optimal solution τ ∗
i of problem (42)
as ri,O = li(µ)τ ∗
i , where li(µ) is a ﬁxed parameter given
µ, representing the optimal communication data rate of the
ith WD. In other words, the optimal transmission time τ ∗
i of
(42) increases linearly with ri,O under a ﬁxed transmission
rate li(µ). In the following, we show how to obtain the dual
optimal solution µ∗ and retrieve the primal optimal solutions
to (34) accordingly.

From Proposition 1, we plug ri,O = li(µ)τi into (42) and

rewrite problem (41) as

maximize
ri,O

ai −

(cid:26)

µ
li(µ)

− Yi(t)

g [li(µ)]
li(µ)hi (cid:27)

ri,O

(44)

subject to 0 ≤ ri,O ≤ Qi(t),

where the optimal solution is

li(µ) − Yi(t) g[li(µ)]

li(µ)hi

< 0,

(45)

r∗
i,O =

0,
Qi(t),

if ai − µ
otherwise.
i = r∗

(
i,O/li(µ). After obtaining τ ∗
Accordingly, we have τ ∗
i ,
∀i ∈ M1, we calculate the subgradient of µ in (35) as
i . Then, we obtain the optimal dual variable µ∗
τ ∗
1 −
through the ellipsoid method (bi-section search in this case)
over the range [0, ∆], where ∆ is a sufﬁciently large value,
until a prescribed precision requirement is met.

i∈M1

P

Given the optimal µ∗, we denote the optimal ratio obtained
from (43) as li (µ∗) , r∗
i,O/τ ∗
i , ∀i ∈ M1. Notice that the
of the dual problem
optimal solution
i,O, ∀i ∈ M1
may not be primal feasible. Therefore, to ﬁnd a primal optimal
solution to (34), we substitute τi = ri,O/li (µ∗) into (34) and
simplify the problem as

i , r∗
τ ∗

(cid:9)

(cid:8)

Algorithm 2: Primal dual algorithm for optimal re-
source allocation of (P4)

10

input : xt, ξt =

Yi(t), Qi(t), At

i(cid:9)
1 initialization: σ0 ← 0.1, LB ← 0, UB ← sufﬁciently large value, convert

(cid:8)

N
i=1

xt into {M0, M1} in (P4);

2 for each WD j ∈ M0 do
Calculate f ∗
3
4 end
5 repeat

j using (33);

6

7

8

9

10

11

12

13

14

µ ← U B+LB
for each WD i ∈ M1 do

2

;

Calculate li(µ) using (43) and r∗
i ← r∗
τ ∗

i,O/li(µ);

i,O using (45);

τ ∗
i < 0 then

end
if 1 −

Pi∈M1
LB ← µ;

else

UB ← µ;

end

15
16 until |UB − LB| ≤ σ0;
17 µ∗ ← µ and obtain ˆr∗

using (47);

O by solving the LP in (46), then obtain ˆτ ∗ and ˆe∗
O

ri,O

18 Return an optimal solution of (P4) by combining (33) and (47).

maximize
ˆrO

subject to

ai −

ri,O
li(µ∗)

Xi∈M1 (cid:26)

Xi∈M1

Yi(t)g [li(µ∗)]
hili(µ∗)

(cid:27)
≤ 1, ri,O ≤ Qi(t), ∀i ∈ M1.

(46)

The above problem is a simple linear programming (LP) that
can be easily solved. With a bit abuse of notation, we denote
the optimal solution of (46) as ˆr∗
and
O =
retrieve the optimal solution to (34) as

r∗
i,O, ∀i ∈ M1

(cid:9)

i = r∗
τ ∗

i,O/li (µ∗) , e∗

i,O =

, ∀i ∈ M1.

(47)

i , ∀i ∈ M1} and ˆe∗

e∗
i,O, ∀i ∈ M1
O, µ∗} satisﬁes the KKT conditions, {ˆτ ∗, ˆe∗

Denote ˆτ ∗ = {τ ∗
. As
{ˆτ ∗, ˆe∗
O, ˆr∗
O}
is an optimal solution to (34). By combining the optimal
solutions in (33) and (47), we obtain an optimal solution of
(P4). We summarize the pseudo-code of the algorithm to solve
(P4) in Algorithm 2.

O, ˆr∗
(cid:9)

(cid:8)

(cid:8)
i g [li(µ∗)]
τ ∗
hili(µ∗)
O =

N log2

4N variables using the interior point method, the proposed
Algorithm 2 solves an LP in (46) with only N variables, which
incurs much lower computational complexity especially when
N is large. Since LyDROO executes Algorithm 2 for Mt times
in each time frame, the overall complexity of generating an of-
. Thanks to
ﬂoading action is O
the adaptive procedure in (8) that gradually reduces the value
(cid:16)
of Mt during the learning process, we observe in simulations
that a small Mt (e.g., less than 5 when N = 30) sufﬁces to
generate optimal ofﬂoading action when the learning process
converges. In the Section VI, we show in simulations that
the proposed LyDROO enjoys very low computation time and
is suitable for online implementation in time-varying edge
environment.

+ N 3 ¯L

Mt

∆
σ0

(cid:16)h

(cid:17)

(cid:17)

i

V. PERFORMANCE ANALYSIS

B. Convergence Performance

A. Computational Complexity

We ﬁrst analyze the complexity of the proposed LyDROO
scheme. The execution of LyDROO algorithm consists of two
parts: ofﬂoading action generation (line 4-9 of Algorithm 1)
and policy update (line 10-13 of Algorithm 1). In between,
ofﬂoading action generation needs to be performed in every
time frame, while policy update is performed infrequently
(e.g., once every tens of time frames) and in parallel with
task ofﬂoading and local computation. Therefore, we focus on
analyzing the complexity of generating an ofﬂoading action
in each time frame. A close observation shows that the major
complexity is on optimizing the resource allocation in line 7
of Algorithm 1, which executes Algorithm 2 to solve (P4) Mt
times in each time frame.

(cid:16)

(cid:16)

∆
σ0

+ N 3 ¯L

We show that
N log2

the time complexity of Algorithm 2 is
: the ﬁrst term corresponds to the bi-
O
section search of µ with σ0 being the small positive precision
parameter; the second term corresponds to solving the LP in
(46) using interior point method [32] with ¯L being the length
of input in binary representation to problem (46). Compared
to directly solving a general convex optimization (P4) with

(cid:17)

(cid:17)

We then analyze the asymptotic convergence performance
of the LyDROO algorithm in solving (P1). To begin with, we
ﬁrst introduce some preliminaries of Lyapunov optimization.
We denote the random event of the considered problem as an
i.i.d. process ω(t), which consists of the fading channels and
i, At
data arrivals, i.e., ω(t) = {ht
i=1. We introduce a class
of stationary and randomized policies called ω-only policy,
which observes ω(t) for each time frame t and makes control
decisions independent of the queue backlog Z(t). To ensure
that the data queue stability constraint can be satisﬁed, we
assume (P1) is feasible and the following Slater condition
holds.

i}N

Assumption 1 (Slater Condition): There are values ǫ > 0
and Φ (ǫ) ≤ Ropt and a ω-only policy Π that makes control
decisions αΠ,t in the tth time frame, which satisfy

= Φ (ǫ) ,

≤ γi − ǫ, ∀i.
(cid:1)(cid:3)
αΠ,t
Dt
i

− ǫ, ∀i.

αΠ,t
αΠ,t
(cid:0)
≤ E
(cid:1)(cid:3)

E

E

(cid:2)

Rt
et
i
At
(cid:0)
(cid:2)
i
N
i=1 cirt
(cid:3)
(cid:2)

E

(48)

Here, Rt ,
rate archived in the tth time frame. Ropt

i denotes the weighted sum computation
is the optimal

(cid:1)(cid:3)

(cid:0)

(cid:2)

P

objective of (P1) obtained over all feasible control policies
(including but not limited to ω-only policy).

We show the performance of LyDROO algorithm in the

following Theorem 1.

Theorem 1: Suppose that (P1) is feasible and satisﬁes
the Slater condition for some ǫ, Φ (ǫ) and ω-only policy Π.
Suppose that given any Z(t) in time frame t, the LyDROO
algorithm produces a value of (20) that is no larger than
a constant C ≥ 0 above the minimum, i.e., the per-frame
subproblem (P2) is solved within an optimality gap C. Then,
the following conditions hold when applying the LyDROO
algorithm in each time frame t

a) The time average computation rate satisﬁes
E

Rt

≥ Ropt − ( ˆB + C)/V. (49)

K
t=1

lim
K→∞

1/K ·

b) The average system queue length satisﬁes

(cid:3)

P

(cid:2)
K
t=1

E

N
i=1

Qt
i

lim
K→∞

1/K ·

≤ 1/ǫ ·

ˆB + C + V
P
P

Ropt − Φ (ǫ)

(cid:2)

(cid:3)

(50)

.

(cid:16)

c) All the data queues Qi(t) are strongly stable and the
(cid:2)
time average power constraint (6c) is satisﬁed with
probability 1.

(cid:3)(cid:17)

Proof : Please see the detailed proof in the Appendix B. (cid:4)
Theorem 1 indicates that if the LyDROO algorithm achieves
a limited optimality gap C when solving the per-frame sub-
problem (P2), then we satisfy all the long-term constraints and
achieve an [O(1/V ), O(V )] computation rate-delay tradeoff.
That is, by increasing V , we can improve the objective of
(P1) proportional to 1/V , but at the cost of longer data queue
length (processing delay) proportional to V , and vice versa.
Besides, a smaller C leads to both higher rate and lower delay
performance. In simulation section, we demonstrate the impact
of V on the long-term performance and show that LyDROO
achieves very small C for the per-frame subproblem. Notice
that the above analysis does not assume the speciﬁc utility
function in the objective of (P1), and thus the results hold for
any non-decreasing concave utility function U (rt

i ).

VI. SIMULATION RESULTS

de

3×108
4πfcdi

In this section, we use simulations to evaluate the perfor-
mance of the proposed LyDROO algorithm.1 All the compu-
tations are evaluated on a TensorFlow 2.0 platform with an
Intel Core i5-4570 3.2GHz CPU and 12 GB of memory. We
assume that the average channel gain ¯hi follows a path-loss
model ¯hi = Ad
, i = 1, · · · , N , where Ad = 3
denotes the antenna gain, fc = 915 MHz denotes the carrier
frequency, de = 3 denotes the path loss exponent, and di
in meters denotes the distance between the ith WD and the
ES. hi follows an i.i.d. Rician distribution with line-of-sight
link gain equal to 0.3¯hi. The noise power N0 = W υ0 with
noise power spectral density υ0 = −174 dBm/Hz. Unless
otherwise stated, we consider N = 10 WDs equally spaced
with di = 120 + 15(i − 1), for i = 1, · · · , N . The weight
ci = 1.5 if i is an odd number and ci = 1 otherwise. The task

(cid:17)

(cid:16)

1The source code is available at https://github.com/revenol/LyDROO.

TABLE I: Simulation Parameters

11

W = 2 MHz
vu = 1.1
q = 1024
γi = 0.08 watt
V = 20

= 0.3 GHz

f max
i
κ = 10−26
δT = 10
ν = 1000
λi = 3 Mbps

= 0.1 watt

P max
i
φ = 100
δM = 32
|S t| = 32
T = 1 second

data arrivals of all the WDs follow exponential distribution
with equal average rate E [At
i] = λi, i = 1, · · · , N . The values
of the other parameters are listed in Table I, which are equal
for all the WDs.

The proposed LyDROO adopts a fully-connected multilayer
perceptron in the actor module, consisting of one input layer,
two hidden layers, and one output
layer, where the ﬁrst
and second hidden layers have 120 and 80 hidden neurons,
respectively. For performance comparison, we consider two
benchmark methods:

• Lyapunov-guided Coordinate Decent (LyCD): It mini-
mizes the upper bound of drift-plus-penalty in (20), or
equivalently solves (P2), using the coordinate decent
(CD) method [8] that iteratively applies one-dimensional
search to update the binary ofﬂoading decision vector
xt. Although the optimal solution of (P2) is hard to
obtain, we have veriﬁed through extensive simulations
that the CD method achieves close-to-optimal perfor-
mance. Therefore, we consider LyCD as a target per-
formance benchmark of the LyDROO algorithm. The
major drawback of LyCD, however, lies in the signiﬁcant
computation delay when N is large. We show in the fol-
lowing simulations that the proposed LyDROO achieves
the similar computation performance as LyCD but takes
much shorter computation time.

• Myopic optimization [18]: The Myopic method neglects
the data queue backlogs and maximizes the weighted sum
computation rate in each time frame t by solving
N

(51a)

maximize
O ,rt
O

xt,τ t,f t,et

i=1cirt

i

t−1

(51b)

subject to

P
(22a) − (22e),
et
i ≤ tγi −
Here, constraint (51c) guarantees that the ith average
power constraint of (P1) is satisﬁed up to the tth time
frame, where
is the past energy consumptions
known at the tth time frame.
(cid:8)

el
i|l < t

l=1 el

i, ∀i.

(51c)

P

(cid:9)

Besides the two benchmarks above, we have also considered
using Deep Deterministic Policy Gradient (DDPG) [33], a
state-of-the-art policy-based DRL scheme, to directly learn
the optimal mapping from the input ξt to the output mixed
integer-continuous ofﬂoading action {xt
i=1 in
(P2). However, we ﬁnd through extensive simulations that
DDPG is unable to stabilize the task data queues for all the
WDs, even when the number of WDs and task arrival rates
are small, e.g., N = 3 and λi = 3 Mbps. Therefore, we do not
include DDPG as a performance benchmark in the following
simulations.

i,O}N

i , f t

i , et

i, τ t

In Fig. 4, we ﬁrst evaluate the performance of the LyDROO
algorithm in solving per-frame subproblem (P2). For fair
comparison, we ﬁrst apply the LyCD method for 30, 000 time

)
2
P
(

f
o

e
v
i
t
c
e
j
b
O
e
v
i
t
a
l
e
R

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

1.05

1

0.95

0.9

0.85

0.8

0.5

1

1.5
(cid:1)(cid:2)(cid:3)(cid:4) (cid:5)(cid:6)(cid:7)(cid:3)(cid:4) (cid:1)

2

2.5

3
(cid:1)104

Fig. 4: Performance of the LyDROO algorithm in solving per-frame
subproblem (P2). In the boxplot, the central mark (in red) indicates
the median, and the bottom and top edges of the box indicate the
25th and 75th percentiles, respectively.
frames, where we record the input to the actor module {ξ(t)}
throughout the time. Then, we use the same {ξ(t)} as the input
to the LyDROO framework in Fig. 3 only for computing the
output action {xt, yt} in each time frame without updating the
queue states. We plot the ratio between the objective values of
(P2) achieved by the LyDROO and LyCD as the time proceeds,
where each point is a moving-window average of 500 time
frames. We notice that the ratio gradually increases with time
and eventually reaches about 0.96. We also show the boxplot
of the last 500 time frames, which shows that the medium ratio
is around 0.98 and the ratio is above 0.94 for more than 75%
of the cases. As LyCD achieves close-to-optimal performance
of the per-frame subproblem (P2), this shows that LyDROO
solves (P2) with small optimality gap C, thus leading to both
higher computation rate and lower execution delay according
to Theorem 1.

We then evaluate the convergence of proposed LyDROO
algorithm and the two benchmark methods. In Fig. 5, we
consider two data arrival rates with λi = 2.5 and 3 Mbps for
all i, and plot the weighted sum computation rate, average data
queue length, and average power consumption performance
over time. We consider i.i.d. realizations of random events
ω(t) in 10, 000 time frames, where each point in the ﬁgure is
a moving-window average of 200 time frames. In Fig. 5(a),
we observe that for a low data arrival rate λi = 2.5, all the
schemes maintain the data queues stable and achieve similar
computation rate performance. Besides, they all satisfy the
average power constraint 0.08 W in Fig. 5(b). In particular,
the LyDROO and LyCD methods achieve higher data queue
lengths than the Myopic scheme, as they consume strictly
lower power than the average power requirement, meanwhile
achieving the identical rate performance in Fig. 5(c). When we
increase λi from 2.5 to 3, all the three schemes still satisfy
the average power constraints. However,
the average data
queue length of the Myopic method increases almost linearly
with time, indicating that it is unable to stabilize the data
queues. This is because the data arrival rate has surpassed the

12

(cid:1) (cid:1) (cid:2)

(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)
(cid:3)(cid:4)(cid:8)(cid:5)
(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)

50
40
30

9000 9500 10000

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

(cid:1) (cid:1) (cid:14)(cid:1)(cid:15)

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

(a) Time Frames

(cid:1) (cid:1) (cid:2)

(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)
(cid:3)(cid:4)(cid:8)(cid:5)
(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

(cid:1) (cid:1) (cid:14)(cid:1)(cid:15)

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

(b) Time Frames

(cid:1) (cid:1) (cid:2)

(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)
(cid:3)(cid:4)(cid:8)(cid:5)
(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

(cid:1) (cid:1) (cid:14)(cid:1)(cid:15)

)
b
M
(

h
t
g
n
e
L

e
u
e
u
Q

a
t
a
D

e
g
a
r
e
v
A

1000

800

600

400

200

25

20

15

10

5

)
t
t
a
w
(

n
o
i
t
p
m
u
s
n
o
C

r
e
w
o
P

e
g
a
r
e
v
A

0.08

0.078

0.076

0.074

0.08

0.075

0.07

0.065

)
s
p
b
M
(

e
t
a
R

n
o
i
t
a
t
u
p
m
o
C

m
u
S

d
e
t
h
g
i
e
W

39

38

37

36

35

33

32

31

30

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
(c) Time Frames

Fig. 5: Convergence performance comparisons of different schemes
under λi = 2.5 and 3. From top to bottom ﬁgures: a) data queue
length; b) power consumption; c) weighted sum computation rate.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
computation capacity (i.e., achievable sum computation rate)
of the Myopic algorithm. On the other hand, both the LyCD
and LyDROO methods can stabilize the data queues, indi-
cating that the proposed Lyapunov-based method can achieve
a higher computation capacity than the Myopic method. In
between, the LyCD method maintains lower data queue length
over all time frames. The LyDROO method takes time to learn
the optimal ofﬂoading policy in the early stage, where the data
queue length increases quickly when t ≤ 3, 000. However, as
the embedded DNN gradually approaches the optimal policy,
the data queue length quickly drops and eventually converges
to the similar queue length and rate performance as the LyCD
method after around t = 7, 500. For both λi’s, the data queue
lengths of the LyDROO algorithm start to drop at around
t = 3, 000, indicating its fast convergence even under highly
dynamic queueing systems. We also notice that the proposed
LyDROO achieves excellent computation performance even
before the learning algorithm fully converges. In Fig. 5(c) with
λi = 3 Mbps, the performance gap of computation rate is less
than 5% when t ≤ 3000 compared to the target benchmark
LyCD, and LyDROO even achieves higher rate than LyCD
between t = 3000 to 7000 when the learning process gradually
converges.

In Fig. 6, we evaluate the impact of system parameters.
In Fig. 6(a), we ﬁx γi = 0.08 watt and vary data ar-
rival rate λi from 2.5 to 3.2 Mbps. In Fig. 6(b), we ﬁx
λi = 3 and vary power constraint γi from 0.06 to 0.1. We
omit the results for λi ≥ 3.3 and γi ≤ 0.05 in the two
ﬁgures, respectively, because we observe that none of the
three schemes can maintain queue stability under the heavy
data arrivals and stringent power constraints, i.e., arrival rate
surpasses the achievable sum computation rate. All the three
schemes satisfy the average power constraints under different
parameters in both ﬁgures. In Fig. 6(a), the data queue lengths
of all the three schemes increase with λi. In particular, the
data queues are stable with LyCD and LyDROO under all
the considered λi, while the queue lengths of the Myopic
scheme become inﬁnite when λi ≥ 2.8. In Fig. 6(b), the
data queues are stable with LyCD and LyDROO under all
the considered λi, and the queue length decreases with γi
under the less stringent power constraint. In vivid contrast,
the Myopic scheme has inﬁnite queue length under all λi
(thus, no point appears in the queue length ﬁgure). The results
show that both LyDROO and LyCD achieve much larger
stable capacity region than the Myopic method, and thus
are more robust under heavy workload and stringent power
constraints. We also observe that LyCD and LyDROO achieve
identical computation rate performance in all the considered
cases. This is because when the data queues are long-term
stable, the average computation rate of the ith WD (departures
rate of the data queue) equals the data arrival rate λi, and
thus the achievable average weighted sum computation rate is
N
i=1 ciλi for both schemes. In fact, this also indicates that
both LyDROO and LyCD achieve the optimal computation
P
rate performance in all the considered setups. In contrast, the
Myopic method achieves lower computation rate when the data
queues are unstable, i.e., for λi > 2.7 in Fig. 6(a) and all
the considered γi ∈ [0.06, 0.1] in Fig. 6(b). Moreover, the

13

Average Data Queue Length (Mb)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)
(cid:1)(cid:2)(cid:6)(cid:3)
(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)

2.6

2.7

2.8

2.9

3

3.1

3.2

Average Energy Consumption (watt)

2.6

2.7

2.8

2.9

3

3.1

3.2

Average Weighted Sum Computation Rate (Mbps)

300

200

100

0
2.5

0.08

0.075

0.07

0.065

2.5

40
38
36
34
32
30

2.5

2.6

2.7

2.8

2.9

3

3.1

3.2

300

200

100

0
0.06

0.1

0.08

0.06

0.06

38

37

36

35

0.06

(cid:12)(cid:13)(cid:14) (cid:15)(cid:16)(cid:16)(cid:10)(cid:17)(cid:13)(cid:18) (cid:4)(cid:13)(cid:19)(cid:20) (cid:1)(cid:1)

Average Data Queue Length (Mb)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)
(cid:1)(cid:2)(cid:6)(cid:3)
(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)

0.07

0.08

0.09

0.1

Average Power Consumption (watt)

0.07

0.08

0.09

0.1

Average Weighted Sum Computation Rate (Mbps)

0.07

0.08
(cid:12)(cid:13)(cid:14) (cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:2) (cid:6)(cid:8)(cid:16)(cid:20)(cid:21)(cid:22)(cid:9)(cid:23)(cid:10)(cid:8)(cid:16) (cid:6)(cid:8)(cid:16)(cid:20)(cid:23)(cid:18)(cid:24)(cid:10)(cid:16)(cid:23) (cid:1)(cid:1)

0.09

0.1

Fig. 6: Performance comparisons under different λi and γi.
performance gap increases under heavier workload (larger λi)
and more stringent power constraints (smaller γi).

In Fig. 7, we further show the impact of the Lyapunov
control parameter V on the performance of the two Lyapunov-
based LyDROO and LyCD methods, where V ∈ [1, 1000].
All the points in the ﬁgure are the average performance after
convergence. In all the ﬁgures, the two methods achieve very
similar performance, where they both maintain data and energy
queues stable, control the average power consumption strictly
below the threshold, and achieve the optimal computation rate
performance. The parameter V controls the balance between
the sum computation rate performance and total data queue
length. Interestingly, when V is small (e.g., V ≤ 40), the
data queue length and power consumption decrease with the
increase of V , and the virtual energy queue length is close to
zero. This is because the ofﬂoading probabilities increase for
most of WDs as V becomes larger. However, when V > 40,
the data queue length, power consumption, and energy queue
length all increase with V monotonically. This is because the
ofﬂoading strategy becomes unfair when V is large, such that
the increase of ofﬂoading probabilities of some WDs is at the
cost of decreased ofﬂoading probabilities of many others. This
results in an overall increase of average data queue length and
energy consumption. In practice, we should set a moderate
V to reduce the task data buffer size required at the WDs,

Average Data Queue Length (Mb)

Average Energy Queue Length (mJoule)

14

90

80

70

60

50

40

30

20

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)
(cid:1)(cid:2)(cid:6)(cid:3)

800

600

400

200

100

101

102

103

100

101

102

103

(cid:1)

(cid:1)

Average Weighted Sum Comput. Rate (Mbps)
39

0.081

Average Power Consumption (watt)

38.5

38

37.5

37

36.5

36

100

0.08

0.079

0.078

0.077

100

101

102

103

(cid:1)

101

102

103

(cid:1)

Fig. 7: Impact of the Lyapunov control parameter V .

which depends on the speciﬁc network deployment and the
task arrival rates of all the WDs.

In Fig. 8, we show the performance of LyDROO under
different number of WDs. Speciﬁcally, we plot in Fig. 8(a)
the average queue length when the individual task arrival rate
λi varies. We observe that LyDROO can maintain stable task
data queue for λi ≤ 3.2 Mbps when N = 10, λi ≤ 2.4
Mbps when N = 20, and λi ≤ 2 Mbps when N = 30.
The points where task data queue becomes unstable are not
plotted, e.g., λi ≥ 2.5 Mbps for N = 20. As expected, the
stable capacity region shrinks with N because of the heavier
computation workload in the system under the same λi. For a
speciﬁc individual task arrival rate λi, the average data queue
length increases with N . For instance, for λi = 2 Mbps, the
queue length is less than 5 when N = 10, around 20 when
N = 20, and around 50 when N = 30. In Fig. 8(b), we
observe that the energy consumption increases with λi for all
N , and gradually reaches the power consumption threshold
0.08 Watt when λi approaches the upper boundary of stable
capacity region. The higher power consumption arises from the
more stringent resource constraint when the overall network
computation workload increases.

From the above discussions, both LyDROO and LyCD
achieve excellent computation performance under different
parameters. In Table II, we compare their computation time
under different number of WDs N . Here, we consider a
ﬁxed total network workload 30 Mbps and equally allocate
λi = 30/N to each WD for N ∈ {10, 20, 30}. The locations
of the N WDs are evenly spaced within [120, 255] meters
distance to the ES. We observe that the two methods achieve
similar computation rate performance for all N and all the

(a) Average Data Queue Length (Mb)

150

100

50

1

1.5

2

2.5

3

(b) Average Power Consumption (watt)

0.08

0.07

0.06

0.05

0.04

0.03

1

1.5

2

2.5

3

Fig. 8: Performance of LyDROO under different number of WDs
N ∈ {10, 20, 30}.
long-term constraints are satisﬁed. Besides, thanks to setting
a time-varying Mt in (8), LyDROO achieves signiﬁcant saving
in execution time compared to that when a ﬁxed Mt = 2N is
used, e.g., saves more than 80% execution time for N = 30,
without degrading the convergence. Due to the page limit, we
omit the illustrations of detailed performance and focus on
comparing the computation time between LyCD and LyDROO
methods. In Table II, LyDROO takes at most 0.156 second
to generate an ofﬂoading action in all the cases. In contrast,
LyCD consumes acceptable latency when N = 10, but signiﬁ-
cantly long latency when N = 30, e.g., around 50 times longer
than that of LyDROO method. Because the channel coherence
time of a common indoor IoT system is no larger than several

TABLE II: Computation rate and CPU computation time when N
varies.

Computation rate (Mbps)

N LyDROO
10
20
30

37.43
37.61
37.36

LyCD
37.43
37.60
37.36

CPU computation time (second)
LyCD
LyDROO
0.27
0.021
2.57
0.108
8.02
0.156

LyCD
LyDROO
12.86
23.80
51.41

seconds, the long computation time makes LyCD costly even
infeasible in a practical MEC system with online ofﬂoading
decision. The proposed LyDROO algorithm, in contract, incurs
very short latency overhead, e.g., around 3% overhead when
the time frame is 5 seconds for N = 30. Recall that after
the DNN generating a control action in a time frame, the
training process of the DNN is performed in parallel with task
ofﬂoading and computation in the remainder of the time frame,
and thus does not incur additional delay overhead. Therefore,
the LyDROO algorithm can be efﬁciently applied in an MEC
system under fast channel variation.

VII. CONCLUSIONS AND DISCUSSIONS

In this paper, we have studied an online stable computa-
tion ofﬂoading problem in a multi-user MEC network under
stochastic wireless channel and task data arrivals. We formu-
late a multi-stage stochastic MINLP problem that maximizes
the average weighted sum computation rate of all the WDs
under long-term queue stability and average power constraints.
The online design requires making joint action of binary
computation ofﬂoading and resource allocation in each short
time frame without the assumption of knowing the future
realizations of random channel conditions and data arrivals. To
tackle the problem, we proposed a LyDROO framework that
combines the advantages of Lyapunov optimization and DRL.
We show in both theory and simulations that the proposed
approach can achieve optimal computation rate performance
meanwhile satisfying all the long-term constraints. Besides,
it incurs very low computational complexity in generating an
online action, and converges within relatively small number
of iterations. The proposed LyDROO framework has wide
application in MEC networks in enhancing both the efﬁciency
and robustness of computation performance.

We conclude the paper with some potential extensions of
the proposed LyDROO scheme and future working directions.
First, besides binary computation ofﬂoading considered in this
paper, the proposed LyDROO scheme can also be extended to
design online partial computation ofﬂoading strategy where the
computation tasks consists of multiple independent subtasks
(such as in [19]). By carefully setting binary variables to
represent which subset of subtasks to be ofﬂoaded for edge
execution, LyDROO is applicable to jointly optimize the
binary ofﬂoading decisions and continuous resource allocation
for the partial ofﬂoading scheme.

Second, we consider in this paper that the task data arrivals
follow an i.i.d. process, which is a crucial assumption for
the proof of the convergence performance of the LyDROO
scheme in Algorithm 1. However, according to Theorem
4.9 in [21], the proposed LyDROO can achieve the similar
[O(1/V ), O(V )] performance guarantees as those described

15

Fig. 9: The convergence performance of LyDROO under both i.i.d.
exponential and the non-i.i.d. Markov modulated ON-OFF task arrival
models.
in Theorem 1 of this paper when the task data arrivals follow
a more general ergodic (possibly non-i.i.d.) process, such as
a Markov modulated process that the distribution of arrival
rates is time-varying and correlated in time. In Fig. 9, we
evaluate the performance of LyDROO under non-i.i.d. task
arrivals for N = 10 WDs, where the task arrivals follow
an ON-OFF Markov modulated random process. Speciﬁcally,
we consider two states for the arrival process, i.e., the ON
state and the OFF state, which are modulated by a two-
state Markov chain with transition matrix [0.1, 0.9; 0.9, 0.1].
The arrived task data size At
i at the ith WD in the tth time
frame is 0 if the system is in OFF state, and follows an i.i.d.
exponential distribution in ON state. In practice, the ON-OFF
Markov modulated random process models the bursty arrivals
of task data. We compare the convergence performance of
LyDROO under both i.i.d. exponential and the non-i.i.d. ON-
OFF task arrival models. For fair comparison, we set equal
long-term average task arrivals rate 3 Mbps for both data
arrival models. We plot in Fig. 9(a) the average task arrival
of the 10 WDs over different time frames of both i.i.d. and
the non-i.i.d. ON-OFF task arrival models. We observe in
Fig. 9(b) that LyDROO can achieve stable task data queue,
and in fact very low task queue length, for both i.i.d. and non-
i.i.d. task data arrivals after sufﬁcient training, although the
time until convergence is longer under the non-i.i.d. arrivals.
In Fig. 9(c), the average energy consumption constraint 0.08
watt is also satisﬁed under both task arrival models. The results
demonstrate the effectiveness of the proposed LyDROO under
non-i.i.d. task data arrivals.

Third, we assume a block fading channel model in this
paper. In practice, however, wireless channel may experience
small variations within a time frame. Recall that ht
i denotes the
channel gain at the beginning of the tth time frame. In case
of small channel variation, we can include a signal-to-noise
(SNR) power margin ρ ≥ 1 when setting the computation
ofﬂoading rate, i.e., Dt
in (2),
1 +
such that the channel gain is likely above ht
i/ρ throughout the
(cid:17)
time frame. Evidently, setting a larger ρ increases the robust-

i,O = W τ t
i T

Et
i,Oht
i
τ t
i T ρN0

log2

vu

(cid:16)

ness of communication against channel variation, however, at
the cost of lower spectrum efﬁciency.

where W(x) denotes the Lambert-W function. Therefore, we
have

16

Fourth, we neglect in this paper the delay on downloading
the computation result from the edge server. When the down-
loading time is non-negligible for some application, we denote
the delay on downloading the result of the ith ofﬂoading WD
in the tth time frame as

Livu

wt

i =

, ∀i ∈ Mt
1,

(52)

(cid:16)

(cid:17)

W log2

1 + P0gt
i
N0
where gt
i denotes the downlink channel gain, P0 denotes the
ﬁxed transmit power of the edge base station, and Li denotes
the ﬁxed size of computation result. During the execution
of the LyDROO algorithm, Mt
1 is the output of the actor
module, such that wt
i ’s are ﬁxed parameters when the critic
module solves the optimal resource allocation problem (P4)
given Mt
1. Therefore, we can include result downloading delay
into consideration by simply replacing the time allocation
constraint in (P4)
τi ≤ 1 with the similar linear
i ) ≤ 1, without affecting the overall
constraint
i∈Mt
1
algorithm design of LyDROO.

i∈M1
i + wt
(τ t
P

Last but not the least, in this paper, we coordinate the
computation ofﬂoading of multiple WDs using TDMA. In fact,
the proposed LyDROO is also applicable to MEC systems
using other multiple access methods, such as FDMA, CDMA,
OFDMA, and NOMA (non-orthogonal multiple access), as
long as the critic module can quickly obtain the optimal wire-
less resource allocation. Accordingly, the technical challenge
lies in the design of efﬁcient resource allocation algorithms
under different multiple access schemes considered.

P

APPENDIX A
PROOF OF PROPOSITION 1

Proof : Given ri,O, we denote the objective of the problem
(42) as Ω(τi), which is a strictly concave function within the
feasible set τi ≥ ri,O
. Accordingly, the minimum is achieved
Rmax
i
at either the boundary point ri,O
or the point v1 that satisﬁes
Rmax
i
Ω′(v1) = 0, depending on the value of v1. To obtain v1, we
take the derivative of Ω(τi) and set it equal to zero, i.e.,

Ω′(τi)

v1 =

ln 2vu · ri,O

W ·

W

e−1

µhi
Yi(t)N0

− 1

+ 1

h

(cid:16)

h

i(cid:17)

i

.

(55)

If v1 < ri,O
Rmax
i

, or equivalently Ω′(τi) = 0 is not achievable
within the feasible set, we can infer that the optimal solution
is obtained at the boundary (τi)∗ = ri,O
. Because Ω(τi) is
Rmax
i
concave, Ω′(τi) is a decreasing function. Given Ω′(v1) = 0,
the condition v1 < ri,O
< 0. By
Rmax
i
substituting τi = ri,O
when
Rmax
i

into (53), we have v1 < ri,O
(cid:17)
Rmax
i

is equivalent to Ω′

ri,O
Rmax
i

(cid:16)

µ + Yi(t)P max

i

⇒ ln (1 + di) ≤

1 − ln (1 + di)
(cid:20)
1 +

µ
Yi(t)P max

1
di

(cid:18)

+ 1

> 0

(cid:19)(cid:21)
1
1 + di (cid:19)

1 −

(56)

⇒ ln

(cid:18)
≥ −Ai +

i
Ai
1
1 + di
1 + di (cid:19)
(cid:18)
where di , hiP max
and Ai , 1 +
natural exponential operation at both sides of (56), we have

µ
Yi(t)P max

. By taking a

(cid:19) (cid:18)

i
N0

,

i

(cid:18)

−

exp

⇒ exp

≥ exp (−Ai)

1
Ai
1 + di (cid:19)
1 + di (cid:19) (cid:18)
Ai
Ai
−
1 + di (cid:19)
1 + di (cid:19) (cid:18)
Because the RHS of the above inequality satisﬁes −e−1 ≤
−Ai exp (−Ai) ≤ 0, the inequality can be equivalently ex-
pressed as

≤ −Ai exp (−Ai) .

−

(cid:18)

− Ai/(1 + di) ≤ W (−Ai exp (−Ai)) ,

(57)

where W (−Ai exp (−Ai)) ∈ [−1, 0]. The equivalence
holds because W(x) is an increasing function when x ≥
−1/e. After some simple manipulation, we obtain from
(57) that the optimal solution (τi)∗ = ri,O
when hi ≤
Rmax
i
N0
P max
i
v1 ≥ ri,O
(cid:16)
Rmax
i
solution is τ ∗

. Otherwise, we conclude that
and Ω′(τi) = 0 is achievable such that the optimal
(cid:4)
i = v1.

−W(−Ai exp(−Ai)) − 1

Ai

(cid:17)

APPENDIX B
PROOF OF THEOREM 1

= − µ −

2
(cid:18)
e−1

Yi(t)N0
hi
Yi(t)N0e
hi
ri,O vu
W τi

(cid:20)
−1

− eln 2

= −

ri,O vu
W τi − 1 − ln 2 · 2

ri,O vu
W τi

·

µhi
Yi(t)N0

− 1

(cid:18)
ln 2 ·

ri,Ovu
W τi

(cid:19)
− 1

= 0,

⇒eln 2

ri,O vu
W τi

−1

(cid:18)
ln 2 ·

(cid:18)

ri,Ovu
W τi

− 1

(cid:19)

Because e−1
equivalent to

µhi
Yi(t)N0

− 1

≥ −1,

(cid:19) (cid:21)
= e−1

µhi
Yi(t)N0

− 1

.

(cid:18)

(cid:19)
(53)
the above equality is

ln 2 ·

(cid:16)
ri,Ovu
W τi

(cid:17)

− 1 = W

e−1

(cid:18)

(cid:20)

µhi
Yi(t)N0

− 1

,

(54)

(cid:21)(cid:19)

ri,Ovu
W τi (cid:19)

To prove Theorem 1, we ﬁrst introduce the following two

lemmas.

Lemma 1: Suppose that (P1) is feasible and ω(t) is
stationary, then for any δ > 0, there exits an ω-only policy
Γ, such that the following inequalities are satisﬁed:

E

E

E

(cid:2)

(cid:2)

Rt
et
i
At
(cid:0)
i

αΓ,t
αΓ,t
(cid:0)
≤ E
(cid:1)

≥ Ropt − δ,
≤ δ, ∀i,
αΓ,t
(cid:3)

− γi
(cid:1)(cid:3)
Dt
i

+ δ, ∀i.

(58)

(cid:2)

(cid:3)

Proof : See Theorem 4.5 of [21] for detailed proof.
(cid:2)
Lemma 2: If Yi(t) is rate stable, i.e., limK→∞

(cid:4)
K = 0
holds with probability 1, then the ith average power constraint
in (6c) is satisﬁed with probability 1.

Yi(K)

(cid:1)(cid:3)

(cid:0)

Proof : Using the sample path property (Lemma 2.1 of [21]),

we have

K

K

≥

−

⇒

i −

1
K

1
K

t=1et

Yi(1)
K

Yi(K)
K

1
K
Yi(K)
P
K
By taking the limit K → ∞ on both size and substituting
i ≤ γi holds
limK→∞
(cid:4)
with probability 1, which completes the proof.

P
Yi(K)
K = 0, we have limK→∞

i ≤ γi +

K
t=1 et

K
t=1γi

t=1et

(59)

P

1
K

.

P

Proof of Theorem 1: Because (P1) is feasible and ω(t) is an
i.i.d. process, we apply Lemma 1 and consider a ﬁxed δ > 0
and the corresponding ω-only control policy Γ. Because the
minimum of (20) is obtained over all feasible control policies,
including Γ, we have
∆L (Z(t)) − V · E

Rt|Z(t)

≤ ˆB + C +

N
i=1

Qi(t)E
(cid:2)

i − Dt
At
(cid:3)
i

αΓ,t

|Z(t)

+ Yi(t)E

et
P
i

(cid:16)
αΓ,t

(cid:2)(cid:0)
− γi|Z(t)

− V · E
(cid:0)

(cid:1)(cid:1)
Rt

(cid:3)
αΓ,t

|Z(t)

(cid:0)
(cid:1)
N
i=1Qi(t)E
et
αΓ,t
(cid:2)(cid:0)
i

At

(cid:3)
i − Dt
i
− V ·
− γi
(cid:0)

αΓ,t

(cid:2)

(cid:0)

(cid:1)

(cid:3) (cid:17)

N
(cid:1)(cid:1)(cid:3)
i=1

E

Rt

αΓ,t

†
≤ ˆB + C +

(cid:2)

N

+

i=1Yi(t)E
P
‡
≤ ˆB + C + δ
P

(cid:0)

N
(cid:2)
(cid:1)
i=1 (Qi(t) + Yi(t))
i

(cid:3)

P

− V

(cid:2)
(cid:0)
Ropt − δ

,

(cid:1)(cid:3)

hP

(60)
where inequality (†) is because the control policy Γ is in-
dependent to queue backlog Z(t), and the inequality (‡) is
obtained by plugging (58). By letting δ → 0, we have
≤ ˆB + C − V Ropt.
Rt|Z(t)

∆L (Z(t)) − V · E

(61)

(cid:1)

(cid:0)

Furthermore, by summing both sizes of (61) from t = 1 to K,
and taking iterated expectations and telescoping sums, then
dividing both sizes by KV , we obtain

(cid:3)

(cid:2)

1
KV
≤ ( ˆB + C)/V − Ropt.

E [L (Z(K + 1))] − E [L (Z(1))] −
(cid:16)

(cid:3)(cid:17)
(62)
Because L (Z(K + 1)) ≥ 0 and L (Z(1)) = 0, we prove a)
by letting K → ∞ in (62).

K
t=1

E

Rt

P

(cid:2)

To prove b), we consider the ω-only policy Π that satisﬁes
the Slater condition for some values ǫ and Φ (ǫ). By plugging
the policy Π to the RHS of the inequality (†) in (60), we have

Rt|Z(t)

∆L (Z(t)) − V · E
N
≤ ˆB + C − ǫ
i=1 (Qi(t) + Yi(t))
(cid:2)
(cid:3)
where the inequality is obtained from (48). Taking iterated
expectations, summing the telescoping series, and rearranging
terms yields

− V Φ (ǫ) ,

hP

(63)

i

1/K

N
K
i=1
t=1
ˆB + C + V
P

E [(Qi(t) + Yi(t))]
1
K ·

≤

P

K
t=1
P
ǫ
By letting K → ∞ and plugging the

E [Rt] − Φ (ǫ)

E [L (Z(1))]
ǫK
fact

+

(cid:1)

(cid:0)

.

that

limK→∞

1
K

K
E [Rt] ≤ Ropt, we have
t=1
1
K

K−1
t=0

N
i=1

P
lim
K→∞

≤

P

ˆB + C + V (Ropt − Φ (ǫ))
P
ǫ

.

E [(Qi(t) + Yi(t))]

17

(64)

Then, (50) in b) is proved because Yi(t) ≥ 0. Meanwhile, (64)
also indicates that

lim
K→∞

lim
K→∞

1
K
1
K

K
t=1

E [Qi(t)] < ∞, ∀i,

P

K
t=1

E [Yi(t)] < ∞, ∀i.

(65)

P

That is, all the data queues and virtual queues are strongly
stable. Because strong stability implies rate stability (Theorem
2.8 of [21]), we have Yi(t) is rate stable. By Lemma 2, the
average power constraint (6c) is satisﬁed with probability 1,
(cid:4)
which leads to the proof of c).

REFERENCES

[1] S. Bi, L. Huang, H. Wang, and Y. J. Zhang, “Stable online computation
ofﬂoading via Lyapunov-guided deep reinforcement learning,” to appear
in Proc. IEEE ICC, Montreal, Canada, Jun. 2021, pp. 1-7, online
available: https://arxiv.org/abs/2102.03286.

[2] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey
the communication perspective,” IEEE

on mobile edge computing:
Commun. Surveys Tuts., vol. 19, no. 4, pp. 2322-2358, Aug. 2017.
[3] W. Zhang, Y. Wen, K. Guan, D. Kilper, H. Luo, and D. O. Wu,
“Energy-optimal mobile cloud computing under stochastic wireless
channel,” IEEE Trans. Wireless Commun., vol. 12, no. 9, pp. 4569-4581,
Sep. 2013.

[4] C. You, K. Huang, and H. Chae, “Energy efﬁcient mobile cloud
computing powered by wireless energy transfer,” IEEE J. Sel. Areas
Commun., vol. 34, no. 5, pp. 1757-1771, May 2016.
[5] J. Yan, S. Bi, Y. J. Zhang, and M. Tao, “Optimal

task ofﬂoading
and resource allocation in mobile-edge computing with inter-user task
dependency,” IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 235-
250, Jan. 2020.

[6] S. Bi, L. Huang, and Y. J. Zhang, “Joint optimization of service caching
placement and computation ofﬂoading in mobile edge computing sys-
tems,” IEEE Trans. Wireless Commun., vol. 19, no. 7, pp. 4947-4963,
Jul. 2020.

[7] G. Lee, W. Saad, and M. Bennis, “An online optimization framework for
distributed fog network formation with minimal latency,” IEEE Trans.
Wireless Commun., vol. 18, no. 4, pp. 2244-2258, Apr. 2019.

[8] S. Bi and Y. J. Zhang, “Computation rate maximization for wireless
powered mobile-edge computing with binary computation ofﬂoading,”
IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177-4190, Jun.
2018.

[9] T. Q. Dinh, J. Tang, Q. D. La, and T. Q. Quek, “Ofﬂoading in mobile
edge computing: task allocation and computational frequency scaling,”
IEEE Trans. Commun., vol. 65, no. 8, pp. 3571-3584, Aug. 2017.
[10] R. S. Sutton, and A. G. Barto, Reinforcement learning: An introduction,

2nd ed., Cambridge, MA: MIT press, 2018.

[11] Y. Liu, H. Yu, S. Xie, and Y. Zhang, “Deep reinforcement learning
for ofﬂoading and resource allocation in vehicle edge computing and
networks,” IEEE Trans. Veh. Technol., vol. 68, no. 11, pp. 11158-11168,
Nov. 2019.

[12] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “Learning-
based computation ofﬂoading for IoT devices with energy harvesting,”
IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1930-1941, Feb. 2019.

[13] J. Li, H. Gao, T. Lv, and Y. Lu, “Deep reinforcement learning based
computation ofﬂoading and resource allocation for MEC,” in Proc. IEEE
WCNC, pp.1-6, 2018.

[14] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized
computation ofﬂoading performance in virtual edge computing systems
via deep reinforcement learning”, IEEE Internet Things J., vol. 6, no. 3,
pp. 4005-4018, Jun. 2019.

[15] M. Tang and V. W. S. Wong, “Deep reinforcement

learning for
task ofﬂoading in mobile edge computing systems,” online available:
https://arxiv.org/abs/2005.02459, 2020.

18

[16] Y. Wei, F. R. Yu, M. Song, and Z. Han, “Joint optimization of caching,
computing, and radio resources for fog-enabled IoT using natural actor-
critic deep reinforcement learning,” IEEE Internet Things J., vol. 6,
no. 2, pp. 2061-2073, Apr. 2019.

[17] J. Du, F. R. Yu, G. Lu, J. Wang, J. Jiang, and X. Chu, “MEC-assisted
immersive VR video streaming over terahertz wireless networks: a deep
reinforcement learning approach,” IEEE Internet Things J., vol. 7, no. 10,
pp. 9517-9529, Oct. 2020.

[18] L. Huang, S. Bi, and Y. J. Zhang, “Deep reinforcement

learning
for online computation ofﬂoading in wireless powered mobile-edge
computing networks,” IEEE Trans. Mobile Compt., vol. 19, no. 11,
pp. 2581-2593, Nov. 2020.

[19] L. Xiao, X. Lu, T. Xu, X. Wan, W. Ji, and Y. Zhang, “Reinforcement
learning-based mobile ofﬂoading for edge computing against jamming
and interference,” IEEE Trans. Commun., vol. 68, no. 10, pp. 6114-6126,
Oct. 2020.

[20] J. Zhang, J. Du, Y. Shen, and J. Wang, “Dynamic computation of-
ﬂoading with energy harvesting devices: a hybrid decision based deep
reinforcement learning approach,” IEEE Internet Things J., vol. 7, no. 10,
pp. 9303-9317, Oct. 2020.

[21] M. J. Neely, “Stochastic network optimization with application to
communication and queueing systems,” Synthesis Lectures on Commu-
nication Networks, vol. 3, no. 1, pp. 1-211, 2010.

[22] Y. Mao, J. Zhang, and K. B. Letaief, “Dynamic computation ofﬂoading
for mobile-edge computing with energy harvesting devices,” IEEE J.
Sel. Areas in Commun., vol. 34, no. 12, pp. 3590-3605, Dec. 2016.
[23] Y. Sun, S. Zhou, and J. Xu, “EMM: energy-aware mobility management
for mobile edge computing in ultra dense networks,” IEEE J. Sel. Areas
in Commun., vol. 35, no. 11, pp. 2637-2646, Nov. 2017.

[24] Y. Mao, J. Zhang, S. H. Song, and K. B. Letaief, “Stochastic joint
radio and computational resource management for multi-user mobile-
edge computing systems,” IEEE Trans. Wireless Commun., vol. 16, no. 9,
pp. 5994-6009, Sep. 2017.

[25] J. Du, F. R. Yu, X. Chu, J. Feng, and G. Lu, “Computation ofﬂoading
and resource allocation in vehicular networks based on dual-side cost
minimization,” IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1079-1092,
Feb. 2019.

[26] C. Liu, M. Bennis, M. Debbah, and H. V. Poor, “Dynamic task ofﬂoading
and resource allocation for ultra-reliable low-latency edge computing,”
IEEE Trans. Commun., vol. 67, no. 6, pp. 4132-4150, Jun. 2019.
[27] C. Yang, J. Li, W. Ejaz, A. Anpalagan, and M. Guizani, “Utility function
design for strategic radio resource management games: An overview,
taxonomy, and research challenges,” Trans. Emerg. Telecommun. Tech-
nol., vol. 28, no. 5, May 2017, Art. no. e3128.

[28] L. Georgiadis, M. J. Neely, and L. Tassiulas, “Resource allocation and
cross-layer control in wireless networks,” Foundations and Trends in
Networking, vol. 1, no. 1, 2006.

[29] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization:
Algorithms and Complexity, Courier Corporation, New York, Dover,
1998.

[30] S. Marsland, Machine learning: an algorithmic perspective. CRC press,

2015.

[31] J. Yan, S. Bi, and Y. J. Zhang, “Ofﬂoading and resource allocation with
general task graph in mobile edge computing: a deep reinforcement
learning approach,” IEEE Trans. Wireless Commun., vol. 19, no. 8,
pp. 5404-5419, Aug. 2020.

[32] K. M. Anstreicher, “Linear programming in O (cid:0)[N 3/ ln n]L(cid:1) opera-

tions, SIAM J. Optimiz., vol. 9, no. 4, pp. 803-812, 1999.

[33] T. P. Lillicrap, et al., “Continuous control with deep reinforcement

learning,” in Proc. ICLR, 2016.

