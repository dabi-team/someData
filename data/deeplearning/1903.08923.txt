Learning with Batch-wise Optimal Transport Loss for 3D Shape Recognition

Lin Xu1,2∗

Han Sun1,2
1Institute of Advanced Artiﬁcial Intelligence in Nanjing, 2Horizon Robotics
{lin01.xu, han.sun, yuai.liu}@horizon.ai

Yuai Liu1,2

9
1
0
2

r
a

M
1
2

]

V
C
.
s
c
[

1
v
3
2
9
8
0
.
3
0
9
1
:
v
i
X
r
a

Abstract

Deep metric learning is essential for visual recognition.
The widely used pair-wise (or triplet) based loss objectives
cannot make full use of semantical information in training
samples or give enough attention to those hard samples dur-
ing optimization. Thus, they often suffer from a slow con-
vergence rate and inferior performance. In this paper, we
show how to learn an importance-driven distance metric
via optimal transport programming from batches of sam-
ples.
It can automatically emphasize hard examples and
lead to signiﬁcant improvements in convergence. We pro-
pose a new batch-wise optimal transport loss and combine
it in an end-to-end deep metric learning manner. We use it
to learn the distance metric and deep feature representation
jointly for recognition. Empirical results on visual retrieval
and classiﬁcation tasks with six benchmark datasets, i.e.,
MNIST, CIFAR10, SHREC13, SHREC14, ModelNet10, and
ModelNet40, demonstrate the superiority of the proposed
method. It can accelerate the convergence rate signiﬁcantly
while achieving a state-of-the-art recognition performance.
For example, in 3D shape recognition experiments, we show
that our method can achieve better recognition performance
within only 5 epochs than what can be obtained by main-
stream 3D shape recognition approaches after 200 epochs.

1. Introduction

Learning a semantical embedding metric to make similar
positive samples cluster together, while dissimilar negative
ones widen apart is an essential part for modern recognition
tasks [24, 12]. With the ﬂourish of deep learning technolo-
gies [31, 47, 53], deep metric learning has gained more at-
tention in recent years [26, 5, 44, 15, 50]. By training deep
neural networks discriminatively end-to-end, a more com-
plex highly-nonlinear deep feature representation (from the
input space to a lower dimensional semantical embedding
metric space) can be learned. The jointly learned deep fea-
ture representation and embedding metric yield signiﬁcant
improvement for recognition applications, such as 2D im-
age retrieval [59, 5, 37] or classiﬁcation [60, 42], signature

Figure 1. Schematic illustration of learning with the proposed
batch-wise loss objective as compared to pair-wise loss objec-
tive. The colors of circles represent semantical (or category) in-
formation. (a): The relationships among batches of samples of
these two loss objectives. (b): Only the semantical information
(c): The
of a pair of examples is considered at each update.
importance-driven distance metric is optimized using all available
information within training batches so that similar positive exam-
ples with large ground distances and dissimilar negative exam-
ples with small ground distances are emphasized automatically.
Arrows indicate the weights (or importance) on distances arising
from the proposed batch-wise optimal transport loss objective.

veriﬁcation [6], face recognition [12, 60, 44], and sketch-
based 3D shape cross-modality retrieval [33, 58, 63].

Despite the progress made, most of the pre-existing loss
objectives [6, 12, 26, 44, 5] do have some limitations for
metric learning. Commonly used contrastive loss [24, 12]
or triplet loss [60, 10] only considers the semantical infor-
mation within individual pairs or triplets of examples at

4321

BeforeAfterBatch-wiseLoss!"!#margin!$%Before!"!&margin!$%After!'!#!&!'(b) Distance metric learning in pair-wise case   (Normal Learning!"!#margin!$%!"!#margin(−%<0!$%BeforeAfter(−%>0!&!&!'   !'Optimal Transport Learning (a) Extend pair-wise to batch-wise correspondence via optimal transport loss       (c) Importance-driven distance metric learning in batch-wise case 
 
 
 
 
 
each update, while the interactions with the rest ones are
ignored. It would bias the learned embedding metric and
feature representation. Moreover, they do not give enough
attention to hard positive or negative examples, by cause of
the fact that these samples are often sparsely distributed and
expensive to seek out. These hard samples can strongly in-
ﬂuence parameters during the network is learned to correct
them. As a consequence, methods which neglect them of-
ten suffer from slow convergence rate and suboptimal per-
formance. Occasionally, such methods require expensive
sampling techniques to accelerate the training process and
boost the learning performance [10, 44, 36, 15].

In this paper, we propose a novel batch-wise optimal
It can
transport loss objective for deep metric learning.
learn an importance-driven distance metric via optimal
transport programming from batches of samples simulta-
neously. As we know, the fundamental idea behind met-
ric learning is minimizing the intra-category variations (or
distances) while maximizing the inter-category variations
(or distances). Thus, those semantically similar positive
samples with large ground distances and dissimilar nega-
tive examples with small ground distances should be re-
garded as hard samples. Such samples should be em-
phasized correctly to accelerate the metric learning pro-
cess. Figure 1 illustrates our main idea of proposing the
new batch-wise optimal transport loss objective. As illus-
trated, learning with the proposed loss can utilize all avail-
able semantical information of training batches simultane-
ously. The introduced importance-driven distance metric is
partly obtained as a solution to the optimal transport pro-
gram [56, 16]. It can mine and emphasize those hard sam-
ples automatically. Thus, the convergence rate of distance
metric learning process can be signiﬁcantly improved. We
further develop the new loss objective in a deep metric
learning manner. The whole network can be trained dis-
criminatively in an end-to-end fashion. The jointly learned
semantical embedding metric and deep feature representa-
tion would be more robust to intra-class and inter-class vari-
ations. We ﬁnally verify the performance of our proposed
method applying to various visual recognition tasks, includ-
ing 2D image recognition, sketch-based 3D shape cross-
modality retrieval, and 3D shape recognition. Experiment
results on six widely used benchmark datasets, i.e., MNIST,
CIFAR10, SHREC13, SHREC14, ModelNet10 and Model-
Net40, demonstrate the superiority of the proposed method.
Our method can achieve a state-of-the-art recognition per-
formance with a notably fast convergence rate.

In a nutshell, our main contributions in the present work

can be summarized as follows:

(1) We propose a novel batch-wise optimal transport loss
objective for learning an importance-driven distance metric
to improve the existing pair-wise based loss objectives.

(2) We develop a deep metric learning method based on

the proposed loss objective, which learns the importance-
driven metric and deep feature representation jointly.

(3) We verify the superiority of our proposed method on
visual recognition tasks, including 2D image recognition,
sketch-based 3D shape retrieval, and 3D shape recognition.

2. Related Work

Recognition of 3D shapes is becoming prevalent with the
advancement of modeling, digitizing, and visualizing tech-
niques for 3D objects. The increasing availability of 3D
CAD models, both on the Internet, e.g., Google 3D Ware-
house [1] and Turbosquid [2], and in the domain-speciﬁc
ﬁeld, e.g., ModelNet [3] and SHREC [34], has led to the de-
velopment of several scalable and efﬁcient methods to study
and analyze them, as well as to facilitate practical applica-
tions. For 3D shape recognition, one fundamental issue is
how to construct a determinative yet robust 3D shape de-
scriptor and feature representation. Compared to 2D im-
ages, 3D shapes have more complex geometric structures.
Their appearance can be affected signiﬁcantly by innumer-
able variations such as viewpoint, scale, and deformation.
These have brought great challenges into the recognition
task. A natural method is to construct a shape descriptor
based on the native 3D structures, e.g., point clouds, poly-
gon meshes, and volumetric grid. Then, shapes can be rep-
resented with distances, angles, triangle areas, tetrahedra
volumes, local shape diameters [38, 9], heat kernel signa-
tures [7, 29], extensions of handcrafted SIFT, SURF [28],
and learned 3D CNNs [62, 35] on 3D volumetric grids. An
alternative way is describing a 3D shape by a collection of
2D view-based projections. It can make use of CNN mod-
els, which have been pre-trained on large 2D image datasets
such as ImageNet [31] and gained a decent ability of gen-
eralization [20, 47, 23]. In this context, DeepPano [46] and
PANORAMA-NN [45] are developed to convert 3D shapes
into panoramic views, e.g., a cylinder projection around
its principal axis. Multi-view CNN (MVCNN) [52] groups
multiple CNNs with a view-pooling structure to process and
learn from all available 2D projections of a 3D shape jointly.

3. Background

Loss objective for metric learning: Metric learning aims
to learn a semantical metric from input samples. Let x ∈ X
be an input sample. The kernel function f (·; θ) : X → Rd
takes input x and generates an feature representation or em-
bedding f (x). In deep metric learning [24, 12, 50], kernel
f (·; θ) is usually deﬁned by a deep neural network, param-
eterized by a series of weights and bias θ. Metric learning
optimizes a discriminative loss objective to minimize intra-
class distances while maximizing inter-class distances. For
example, the contrastive loss in seminal Siamese network
[24, 12] takes pairs of samples as input and trains two iden-

4322

Figure 2. We formulate the proposed loss into a deep metric learning framework. Given batches of each modality samples, we use LeNet-5
[32], ResNet-50 [25], and MVCNN [52] as f 1
CNN to extract deep CNN features for 2D images, 2D sketches, and 3D shapes, respectively.
The metric network f 2
Metric consisting of four fully connected (FC) layers, i.e., 4096-2048-512-128 (two FC layers 512-256 for LeNet-5)
is used to perform dimensionality reduction of the CNN features. We add three sigmoid functions as activation among these FC layers to
generate normalized and dense feature vectors. The whole framework can be end-to-end trained discriminatively with the new batch-wise
optimal transport loss. The highlighted importance-driven distance metrics TijM +
ij are used for emphasizing hard positive
and negative samples. It jointly learns the semantic embedding metric and deep feature representation for retrieval and classiﬁcation.

ij and TijM −

tical networks to learn a deep metric Mij as

L(xi, xj; f ) = yijMij +(1−yij) max{0, ε−Mij}, (1)

where the label yij ∈ {0, 1} indicates whether a pair of
(xi, xj) is from the same class or not. The margin param-
eter ε imposes a threshold of the distances among dissim-
ilar samples. Conventionally, the Euclidian metric Mij =
||(f (xi) − f (xj)||2
2 in the feature embedding space is used
to denote the distance of a pair of samples. Triplet loss
[60, 10] shares a similar idea with contrastive loss, but ex-
tends a pair of samples to a triplet. For a given query xi, a
similar sample xj to the query xi, and a dissimilar one xk,
the triplet loss can be formulated as

L(xi, xj, xk; f ) = max{0, Mij − Mik + ε}.

(2)

Intuitively, it encourages the distance between the dissimilar
pair Mik = ||(f (xi) − f (xk)||2
2 to be larger than the dis-
tance between the similar pair Mij = ||(f (xi) − f (xj)||2
2
by at least a margin ε.
Optimal transport distances: Optimal transport distances
[16], also known as Wasserstein distances [56] or Earth
Mover’s distances [43], deﬁne a distance between two prob-
ability distributions according to principles from optimal
transport theory [57, 61]. Formally, let r and c be n-
dimensional probability measures. The set of transportation

plans between probability distributions r and c is deﬁned as
U (r, c) := {T ∈ Rn×n
+ |T 1 = r, T T 1 = c}, where 1 is an
all-ones vector. The set of transportation plans U (r, c) con-
tains all nonnegative n × n elements with row and column
sums r and c, respectively.

Give an n × n ground distance matrix M , the cost of
mapping r to c using a transport matrix T can be quanti-
ﬁed as (cid:104)T, M (cid:105), where (cid:104)., .(cid:105) stands for the Frobenius dot-
product. Then the problem deﬁned in Equation (3)

DM (r, c) := min

T ∈U (r,c)

(cid:104)T , M (cid:105),

(3)

is called an optimal transport problem between r and c
given ground cost M . The optimal transport distance
DM (r, c) measures the cheapest way to transport the mass
in probability measure r to match that in c.

Optimal

transport distances deﬁne a more powerful
cross-bin metric to measure probabilities compared with
some commonly used bin-to-bin metrics, e.g., Euclidean,
Hellinger, and Kullback-Leibler divergences. However,
the cost of computing DM is at least O(n3log(n)) when
comparing two n-dimensional probability distributions in
a general metric space [39]. To alleviate it, Cuturi [16]
formulated a regularized transport problem by adding an
entropy regularizer to Equation (3). This makes the ob-
jective function strictly convex and allows it to be solved

4323

√………………2D Render……Metric Network of 3D Shapes2D Render2D RenderCNN Network of 3D ShapesViewPooling1CNNfCNN Network of 2D Sketches1CNNfMetric Network of 2D SketchesMetric Network of 2D Images1CNNfCNN Network of 2D Images1CNNfLearned Deep Metric SpaceBatch-wise Optimal Transport Loss for Learning1CNNf1CNNfOPTBatch of  SamplesImportance-driven Distance Metric Learning for Mining Hard Positive and Negative Samplesefﬁciently. Particularly, given a transport matrix T , let
h(T ) = − (cid:80)
ij Tij log Tij be the entropy of T . For any
λ > 0, the regularized transport problem can be deﬁned as

Dλ

M (r, c) := min

T ∈U (r,c)

(cid:104)T , M (cid:105) −

1
λ

h(T ),

(4)

where the lager λ is, the closer this relaxation Dλ
M (r, c)
is to original DM (r, c). Cuturi [16] also proposed the
Sinkhorn’s algorithm [49] to solve Equation (4) for the
optimal transport T ∗. Speciﬁcally, let the matrix K =
exp(−λM ) and solve it for the scaling vectors u and v to a
ﬁxed-point by computing u = r./Kv, v = c./KT u in an
alternating way. These yield the optimal transportation plan
T ∗ = diag(u)Kdiag(v). This algorithm can be solved
with complexity O(n2) [16], which is signiﬁcantly faster
than exactly solving the original optimal transport problem.

4. Our Method

In this section, we propose a deep metric learning
scheme by using principles of the optimal transport theory
[57]. Currently, research works with optimal transport dis-
tance [16, 18, 17] mainly focus on theoretical analysis and
simulation veriﬁcation. Thus, it is hard to apply them into
a large-scale 3D shape recognition contest directly. To this
end, we have done the following three works to construct a
trainable batch-wise optimal transport loss objective.

4.1. Importance-driven Distance Metric Learning

Assuming we are given two batches of samples, each
batch has n examples X ∈ Rd×n. Let xi ∈ Rd be the rep-
resentation of the ith shape. Additionally, let r and c be the
n-dimensional probability vectors for two batches, where ri
and ci denote the number of times shape i occurs in r and c
(normalized overall samples in r and c). The optimal trans-
port introduces a transportation plan T ∈ Rn×n such that
Tij describes how much of ri should be transported to cj.
As described in Equation (4), the optimal transport distance
between batches r and c can be re-formulated as

Dλ

OT(r, c) = min
T ≥0

(cid:88)n

i.j=1

TijMij −

1
λ

h(Tij)

s.t.

(cid:88)n

j=1

Tij = r and

(cid:88)n

i=1

Tij = c ∀i, j.

(5)

The learned optimal transportation plan T ∗ is a probability
distribution [16], which aims to ﬁnd the least amount of cost
needed to transport the mass from batch r to batch c. The
unit of cost corresponds to transporting a sample by the unit
of ground distance. Thus, T ∗ solved by Equation (5) prefers
to assign higher importance values to samples with small
ground distances while leaving fewer for others.

Utilizing such property, we deﬁne the importance-driven
distance metric via imposing semantical information of

samples. Speciﬁcally, we ﬁrst deﬁne the ground distances
for a pair of similar positive samples as

M +(xi, xj; f ) = e−γ||f (xi)−f (xj )||2
2,

(6)

where γ is a hype-parameter controlling the extent of rescal-
ing. This re-scaling operator shrinks large Euclidian dis-
tance between similar samples. After re-scaling M +, the
learned T ∗ tends to put higher importance values on those
similar samples which have far Euclidian distances among
each other (a.k.a., hard postive samples), while putting
lower on the others accordingly. Thus, it would acceler-
ate the process that similar samples are getting close to each
other. For dissimilar negative samples, we deﬁne the ground
distances correspondingly as

M −(xi, xj; f ) = e−γmax{0,ε−||f (xi)−f (xj )||2

2}.

(7)

The hinge loss max{0, ε − ||f (xi) − f (xj)||2
2} penalizes
the dissimilar samples within the margin ε and ignores
the others. Thus, contrary to the above similar samples
case, here the learned T ∗ will pay higher importance values
on those dissimilar samples with small Euclidian distances
(a.k.a., hard negative samples), while assigning fewer on
the others. Thus, it could accelerate the process that dissim-
ilar samples are getting apart to each other.

4.2. Batch-wise Optimal Transport Loss Learning

Based on the deﬁned distances M +, M −, and optimal
transportation plan T ∗, now we can formulate a batch-wise
optimal transport loss for metric learning. It can be viewed
as an n-pairs extension version of the contrastive loss or
triplet loss. We deﬁne the loss objective as

L(xi, xj; f ) = L+ + L−
1
2

ij +(Iij − Yij)

TijM +

(cid:88)n
ij

1
2

= Yij

(cid:88)n
ij

TijM −
ij ,

ij and TijM −

(8)
where Yij is a binary label assigned to a pair of training
batches. Let Yij = 1 if sample xi and xj are deemed
similar, and Yij = 0 otherwise. An all-ones matrix is
denoted as I ∈ Rn×n and n is the size of each training
batch. In practice, TijM +
ij can be regarded as
the importance-driven distance metric for positive and neg-
ative samples, respectively. The optimal transportation plan
T ∗ obtained by solving Equation (5) is a probability distri-
bution of weights for emphasizing hard positive and nega-
tive samples during the loss objective optimization. We just
write the loss objective regarding only one pair of batches
here for simplicity. The overall data loss objective based on
all training batches can be easily derived as (cid:80) L.

4.3. Batch Gradient Descent Optimization

We further derive the back-propagation form of the
batch-wise optimal transport loss objective. The proposed

4324

loss objective can be embedded into a deep metric learning
framework, so that the whole network can be trained dis-
criminatively end-to-end via batch gradient descent.

Since the batch-wise optimal transport distance is a
fully connected dense matrix of pairs-wise ground dis-
tance, its gradient can be deduced as network ﬂow man-
ner. Speciﬁcally, we compute the gradient of corresponding
loss L(xi, xj; f ) with respect to embedding representations
f (xi) and f (xj) as follows,

∂L
∂f (xi)

=

n
(cid:88)

j=1

T ∗

ij(f (xi) − f (xj))(Yij − (Iij − Yij)δij)

n
(cid:88)

= −

T ∗

ij(f (xi)−f (xj))(Yij −(Iij −Yij)δij),

∂L
∂f (xj)

i=1

(9)
where T ∗ is the optimizer obtained from Equation (4).
Motivated by the fast optimal distance computation [16,
18, 21], we relax the linear program in Equation (5) us-
ing the regularized entropy as in Equation (4).
It allows
us to approximately solve Equation (4) in O(n2) time via
T ∗ = diag(u)Kdiag(v), where n is the size of batch.

The δ here is also a binary indicator assigned to the pairs.
Let δij = 1 when the Eucildian distance between shape xi
and xj is within the margin (i.e., ε − ||f (xi) − f (xj)||2
2 >
0), and δij = 0 otherwise. The f (xi) and f (xj) are fea-
ture representations obtained through deep neural networks.
Therefore, the gradient with respect to the network can be
computed easily with the chain-rule in a back-propagation
fashion, as far as
∂f (xj ) are derived. We also
note that the deﬁned ground distance M + and M − are just
used to determine the optimal transportation plan T ∗ for re-
weighting the importance of similar positive and dissimilar
negative samples. We do not consider them as variables to
compute gradient in Equation (9) for gradient updating.

∂f (xi) and

∂L

∂L

5. Experiments

In this section, we evaluated the performance of the
proposed method with applications to 2D image recog-
nition (i.e., retrieval and classiﬁcation), sketch-based 3D
shape retrieval, and 3D shape recognition tasks. Six widely
used benchmark datasets were employed in our experi-
ments, including MNIST [32], CIFAR10 [30], SHREC13
[33], SHREC14 [41], ModelNet10, and ModelNet40 [62].

5.1. Experimental settings

Architecture: Figure 2 illustrates network architecture of
deep metric learning with our batch-wise loss objective.
Datasets: The MNIST [32] is a large handwritten digits
dataset, which has 60, 000 28 × 28 black-and-white train-
ing images and 10,000 testing images. The CIFAR10 [30]
dataset consists of 60, 000 32×32 RGB images in 10 differ-
ent categories, with 6, 000 images per category. There are
50, 000 training images and 10, 000 test images. SHREC13

4325

[33] and SHREC14 [41] are two large-scale datasets for
sketch-based 3D shape retrieval. SHREC13 contains 7, 200
human-drawn sketches and 1, 258 3D shapes from 90 dif-
ferent categories. For each category, 50 sketches are used
for training and remaining 30 sketches are used for the test.
There are 14 3D shapes per category generally. SHREC14
is larger than SHREC13, which has 13, 680 sketches and
8, 987 3D shapes from 171 categories. Each of the cat-
egories has 53 3D shapes on average. There are 8, 550
sketches for training and 5, 130 for test. ModelNet [3] is
a large-scale 3D shape dataset, which contains 151, 128
3D CAD models belonging to 660 unique object categories
[62]. There are two subsets of ModelNet can be used for
evaluation. ModelNet10 contains 4, 899 3D shapes from 10
categories while ModelNet40 has 12, 311 shapes from 40
categories. In our experiments, we used the same training
and test splits as in [62]. Speciﬁcally, we randomly selected
100 unique shapes per category, where 80 shapes were cho-
sen for training and the remaining 20 shapes for the test.
Evaluations: For retrieval, we used Euclidian distance to
measure the similarity of the shapes based on their learned
feature vectors output by the metric network as shown in
Figure 2. Given a query from the test set, a ranked list of
the remaining test samples was returned according to their
distances to the query sample. We used the evaluation met-
rics for retrieval as in [63] when presenting our results. The
metrics include nearest neighbor (NN) [14], ﬁrst tier (FT)
[54], second tier (ST) [13], E-measure (E) [11], discounted
cumulated gain (DCG) [27], and mean average precision
(mAP) [40]. For classiﬁcation, we trained one-vs-all linear
SVMs [8] to classify 2D images and 3D shapes using their
features. The average category accuracy [62] was used to
evaluate the classiﬁcation performance.
Parameters settings:
In our 2D image recognition, the
learning rate and batch size were 0.01 and 64 respectively.
Our optimizer had a momentum of 0.9 and 0 weight decay
rate. The regularized parameter λ in Equation (5) was set
to be 0.01 while the re-scaling parameter γ in Equation (6)
being 10. In the sketch-based 3D shape retrieval and 3D
shape recognition experiments, the batch size was reduced
to 32. Meanwhile, the learning rate, weight decay and mo-
mentum remained the same as what has been used in 2D
experiments. We increased the regularized parameter λ to
10, which is the same as the re-scaling parameter γ.

5.2. Evaluation of Proposed Method

5.2.1

2D Image Recognition

Firstly, we empirically evaluated the effect of our pro-
posed method on two broadly used 2D images benchmark
datasets, i.e., MNIST and CIFAR10. As illustrated in Figure
2, we used a Siamese-like symmetrical network structure,
which employed Lenet-5 as its base CNN to obtain 256-
dimensional feature vectors for the images in both datasets.

Figure 3. Left: Mean average precision (mAP) and classiﬁcation accuracy curves of batch-wise optimal transport loss and pair-wise
contrastive loss on 2D MNIST dataset. Middle: Comparison of their mAP and accuracy curves on 2D CIFAR10 dataset. Right: Comparison
of their mAP curves on sketch-based 3D shapes SHREC13 and SHREC14 dataset.

Table 1. Retrieval results on the SHREC13 benchmark dataset
Method
CDMR
SBR-VC
SP
FDC
Siamese
LWBR
Our Method

DCG mAP
0.250
0.458
0.116
0.348
0.026
0.240
0.086
0.307
0.469
0.607
0.752
0.814
0.754
0.818

FT
0.203
0.097
0.016
0.069
0.403
0.725
0.728

ST
0.296
0.149
0.031
0.107
0.548
0.725
0.788

NN
0.279
0.164
0.017
0.110
0.405
0.712
0.713

E
0.166
0.085
0.018
0.061
0.287
0.369
0.366

Table 2. Retrieval results on the SHREC14 benchmark dataset
Method
CDMR
SBR-VC
DB-VLAT
Siamese
DCML
LWBR
Our Method

DCG mAP
0.054
0.328
0.050
0.319
0.131
0.376
0.228
0.496
0.286
0.498
0.401
0.581
0.591
0.712

ST
0.089
0.081
0.170
0.316
0.345
0.455
0.629

FT
0.057
0.050
0.115
0.212
0.275
0.378
0.564

E
0.041
0.037
0.079
0.140
0.171
0.236
0.305

NN
0.109
0.095
0.160
0.239
0.272
0.403
0.536

Training images were randomly shufﬂed at the start of each
epoch. In each training step, the optimal transportation T ∗
between two batches of image features was approximated
by iterating the Sinkhorn’s algorithm for 20 times. Af-
ter each epoch, we computed all image features with the
symmetrical network trained so far for classiﬁcation and re-
trieval. The categorical accuracies provided by one-vs-rest
linear SVMs and the retrieval mAPs given by the similarity
measure based on the testing samples were recorded.

The left-hand and middle subﬁgures in Figure 3 present
accuracy and mAP curves of the batch-wise optimal trans-
port loss learning concerning the number of epochs. These
ﬁgures illustrate the relationship between the convergence
rate and recognition performance. Comparing with the pair-
wise contrastive loss, our method posses a signiﬁcantly
faster convergence rate. On CIFAR10, it provides a re-
trieval mAP and a classiﬁcation accuracy which are approx-

imately 15% and 10% higher than the corresponding values
achieved by the pair-wise loss at the end of 50 epochs. The
empirical results indicate that the importance-driven dis-
tance metric learning can effectively adjust the distribution
of weights. It pays more attention to the hard positive and
negative samples during the training process.

5.2.2 Sketch-based 3D Shape Retrieval

We then evaluated our method for sketch-based 3D shape
retrieval on two large-scale benchmark datasets,
i.e.,
SHREC13 and SHREC14. The right-hand two subﬁgures
in Figure 3 demonstrate the mAP curves of our batch-wise
optimal transport loss as compared to the pair-wise loss ob-
jective. As illustrated, our method is about 5 times and 3
times faster than LBWR on SHREC13 and SHREC14 re-
spectively. Meanwhile, the retrieval performance is remark-
ably higher than the compared LBWR.

4326

01020304050Number of Epoches9092949698100Mean Average Precision (%)Batch-wise Loss on MNISTPair-wise Loss on MNIST01020304050Number of Epoches304050607080Mean Average Precision (%)Batch-wise Loss on SHREC13Pair-wise Loss on SHREC1301020304050Number of Epoches97.59898.59999.5Classification Accuracy (%)Batch-wise Loss on MNISTPair-wise Loss on MNIST01020304050Number of Epoches4045505560657075Classification Accuracy (%)Batch-wise Loss on CIFAR10Pair-wise Loss on CIFAR1001020304050Number of Epoches152025303540455055Mean Average Precision (%)Batch-wise Loss on CIFAR10Pair-wise Loss on CIFAR1001020304050Number of Epoches0102030405060Mean Average Precision (%)Batch-wise Loss on SHREC14Pair-wise Loss on SHREC14Figure 4. Mean average precision (mAP) curve with respect to the number of epochs evaluated of various methods on the ModelNet40
dataset. Left subﬁgure illustrates the mAP curves of four loss objectives for 3D shape retrieval, and right subﬁgure illustrates the mAP
curves of three weighting modesc. The mAP have been observed every ﬁve epochs for 200 epochs in ﬁgures.

Table 3. Comparisons of batch-wise optimal transport loss with other benchmark methods on the ModelNet40 dataset.

Evaluation Criteria

Methods

NN

FT

ST

DCG

E

mAP (%) Accuracy (%)

Pair-wise

Batch-wise

Individual
Triplets
Random
Mean Weighted
Random Reweighted
Optimal Reweighted

0.8287
0.8324
0.8688
0.8750
0.8688
0.8762

0.6544
0.6968
0.7948
0.7986
0.7673
0.8013

0.7891
0.8029
0.9048
0.9032
0.8846
0.8991

0.8562
0.8629
0.9140
0.9158
0.9051
0.9178

0.5668
0.5927
0.6601
0.6589
0.6445
0.6560

69.3%
74.1%
83.1%
83.3%
83.1%
83.8 %

88.6%
89.1%
89.5%
89.7%
89.0%
90.3 %

Table 4. Retrieval and classiﬁcation results on the ModelNet10 and ModelNet40 datasets.

Methods

Shape Descriptor

ModelNet10

ModelNet40

mAP (%) Accuracy (%) mAP (%) Accuracy (%)

(1) MVCNN [52]

2D View-based Descriptor (#Views=12)
2D View-based Descriptor (#Views=80)

N/A
N/A

(2) GIFT [4]

2D View-based Descriptor (#Views=64)

91.1 %

(3) 3DShapeNets [62]

3D Voxel Grid (30 × 30 × 30)

(4) Geometry Image [48]

2D Geometry Image

(5) PANORAMA-NN [45]

2D Panoramic View

(6) DeepPano [46]

2D Panoramic View

68.3%

74.9%

87.4%

84.1%

N/A
N/A

92.3%

83.5%

88.4%

91.1%

85.4%

80.2%
79.5%

81.9%

49.2%

51.3%

83.5%

76.8%

Our Method

2D View-based Descriptor (#Views=12)

87.5%

93.7 %

83.8 %

89.5%
90.1%

83.1%

77.0%

83.9%

90.7 %

77.6%

90.3 %

We also compared our method with several mainstream
approaches for 3D shape retrieval, including CDMR [22],
SBR-VC [33], SP [51], FDC [51], Siamese network [58],
DCML [19], DB-VLAT [55], and LWBR [63]. The evalua-
tion criteria include NN, FT, ST, E, DCG, and mAP.

As summarized in Table 1 and Table 2, our batch-wise
optimal transport loss based method achieved the best re-
trieval performance with respect to all evaluation metrics
on SHREC13 and SHREC14. Among compared methods,
CDMR, DCML, Siamese network, and LWBR are all deep
metric learning based approaches. They measured simi-
larity based on pairs of samples and mapped data into an

embedding metric space through different pooling schemes.
In contrast, our proposed batch-wise optimal transport loss
objective can correctly re-weight the importance value of
samples, mainly focus on those hard samples. Thus, our
approach obtained better retrieval performance.
Its mAP
reaches 0.754, which is slightly better than LBWR and sig-
niﬁcantly better than other methods. Furthermore, the ad-
vantage of our approach is enlarged on SHREC14 because
this dataset has more severe intra-class and cross-modality
variations. As a consequence, the mAP of our proposed
method is 0.591, which is 0.190, 0.305, and 0.363 higher
than LBWR, DCML and Siamese network, respectively.

4327

050100150200Number of Epoches505560657075808590Mean Average Precision (%)Batch-wise LossPair-wise LossRandom Pairs LossTriplet Loss050100150200Number of Epoches505560657075808590Mean Average Precision (%)Batch-wise Optimal ReweightedBatch-wise Random ReweightedBatch-wise Mean ReweightedIt takes only a few
rate than other benchmark methods.
epochs (i.e., 5 epochs) to achieve mAP at 83.8% and accu-
racy at 90.3%, which are better than others after 200 epochs.
It demonstrates that the learned optimal transportation plan
can correctly re-weight the training samples according to
their importance values during the metric learning process.
Moreover, solving Equation (5) to learn optimal transporta-
tion plan T ∗ is not computational expensive in practice. The
average running time required by one epoch of individual
pair loss objective is 2.51 seconds, and that of batch-wise
optimal transport loss objective takes 9.02 seconds.

Here, we analyzed the role of learned importance-driven
distance metric T ∗M ∗ in our work. It is composed of the
optimal transportation plan T ∗ ∈ Rn×n and the semanti-
cal information embedded ground distances M ∗ ∈ Rn×n.
The element M ∗
ij is ﬁlled with the distances of batch-wise
similar positive samples M +
ij and the distances of batch-
wise dissimilar negative samples M −
ij . The right-hand sub-
ﬁgure in Figure 5 shows that far similar positive and ad-
jacent dissimilar negative samples (i.e., hard samples) are
sparsely distributed. The left-hand subﬁgure is the optimal
transportation plan which is actually a probability distribu-
tion [16]. The color map reveals the learned metric ensures
higher importance weights to those few samples with small
ground distance while giving less on the remaining ones.

In the end, we compared our method with state-of-the-
art approaches for shape retrieval and classiﬁcation, includ-
ing MVCNN [52], GIFT [4], DeepPano [46] and et al.
The detailed comparison results are summarized in Table
4. Compared to these approaches, our method based on
batch-wise optimal transport loss learning can achieve the
(almost) state-of-the-art performance on both tasks.

6. Conclusion

In this paper, we proposed a novel batch-wise opti-
mal transport loss objective to learn an importance-driven
distance metric. The learned distance metric can effec-
tively emphasize hard samples according to their impor-
tance weights. We then formulated the proposed loss ob-
jective into an end-to-end deep metric learning network for
recognition. We evaluated the performance and versatil-
ity of our method with various visual recognition tasks, in-
cluding 2D image recognition, 2D sketch-based 3D shape
cross-modality retrieval, and multiview based 3D shape
recognition. The empirical results veriﬁed the proposed
method could generically accelerate the convergence rate
while achieving excellent recognition performance. Our fu-
ture work will involve facilitating such a trend and apply-
ing this importance-driven distance metric learning to more
widespread applications. For example, 3D point cloud clas-
siﬁcation, segmentation, 3D scene reconstruction, cross-
modality correspondence among visual, audio, and text.

Figure 5. Illustration of relationship between the ground distances
M ∗ and the optimal transportation plan T ∗ on the ModelNet40
dataset. For two batches (each with batch size 32) of samples,
we visualize the values of the batch-wise ground distances matrix
(i.e., 32 × 32) and the corresponding optimal transportation plan.

5.2.3 3D Shapes Recognition

We ﬁnally veriﬁed the proposed method for 3D shape
recognition on two large-scale 3D shape datasets, i.e., Mod-
elNet10 and ModelNet40. Pair-wise loss and triplet loss suf-
fer from slow convergence rate because they are not capa-
ble of exploring all available semantical information within
training batches simultaneously. To alleviate this problem,
we used random sampling techniques (i.e., recurrently shuf-
ﬂe the training batches during each epoch) to loop over as
many randomly sampled pairs as possible. It is expected
that the random pairs based loss objective could make full
use of all information so that the ﬁnally learned semantic
metric could be balanced correctly. The left-hand subﬁg-
ure in Figure 4 presents the mAP curves of batch-wise op-
timal transport loss and other compared loss objectives for
3D shape retrieval. Similarly, the batch-wise optimal trans-
port loss objective still has signiﬁcantly faster convergence
rate and can achieve a decent retrieval performance within
a small number of epochs (i.e., 5 epochs).

We also examined two different probability distributions,
i.e., uniformly distributed mean value (ν = 1
n2 ) and ran-
dom numbers in the interval (0, 1), as alternatives of the
optimal transportation plan. Uniformly distributed mean
value weights in the batch-wise loss imply that samples are
equally important for later metric learning. Uniformly dis-
tributed random weights randomly mark some samples as
hard samples within a pair of batches during the learning
process. The right-hand subﬁgure in Figure 4 illustrates
comparison results of the retrieval performance concerning
the number of epochs for these three re-weighting strate-
gies. It demonstrates that the convergence rate of optimal
re-weighted is much faster than the others.

The detailed comparison results are summarized in Table
3. We compared the batch-wise optimal transport loss with
other designed benchmark methods using NN, FT, ST, E,
DCG and mAP on the ModelNet40 dataset. As illustrated
in Figure 4 and Table 3, learning with batch-wise optimal
transport loss objective has considerably faster convergence

4328

Ground Distances51015202530Number of Batch Size51015202530Number of Batch Size0.20.40.60.8151015202530Number of Batch Size51015202530Number of Batch Size0.0050.010.0150.020.0250.03            Optimal TransportationReferences

[1] The Google 3d Warehouse. https://3dwarehouse.

sketchup.com/, 2006.

[2] The Turbosquid. https://www.turbosquid.com/,

2000.

[3] The Princeton ModelNet. http://modelnet.cs.

princeton.edu/, 2015.

[4] S. Bai, X. Bai, Z. Zhou, Z. Zhang, and L. Jan Latecki. Gift:
A real-time and scalable 3d shape search engine. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5023–5032, 2016.

[5] S. Bell and K. Bala. Learning visual similarity for product
design with convolutional neural networks. ACM Transac-
tions on Graphics (TOG), 34(4):98, 2015.

[6] J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah.
Signature veriﬁcation using a” siamese” time delay neural
network. In Advances in Neural Information Processing Sys-
tems, pages 737–744, 1994.

[7] A. M. Bronstein, M. M. Bronstein, L. J. Guibas, and M. Ovs-
janikov. Shape google: Geometric words and expressions
for invariant shape retrieval. ACM Transactions on Graphics
(TOG), 30(1):1, 2011.

[8] C.-C. Chang and C.-J. Lin. Libsvm: a library for support
vector machines. ACM transactions on intelligent systems
and technology (TIST), 2(3):27, 2011.

[9] S. Chaudhuri and V. Koltun. Data-driven suggestions for cre-
ativity support in 3d modeling. ACM Transactions on Graph-
ics (TOG), 29(6):183, 2010.

[10] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale
online learning of image similarity through ranking. Journal
of Machine Learning Research, 11(Mar):1109–1135, 2010.
[11] T. Y. Chen, H. Leung, and I. Mak. Adaptive random testing.
In Annual Asian Computing Science Conference, pages 320–
329. Springer, 2004.

[12] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-
ity metric discriminatively, with application to face veriﬁca-
In Computer Vision and Pattern Recognition, 2005.
tion.
CVPR 2005. IEEE Computer Society Conference on, vol-
ume 1, pages 539–546. IEEE, 2005.

[13] N. D. Cornea, M. F. Demirci, D. Silver, S. Dickinson, P. Kan-
tor, et al. 3d object retrieval using many-to-many matching of
curve skeletons. In Shape Modeling and Applications, 2005
International Conference, pages 366–371. IEEE, 2005.
[14] T. Cover and P. Hart. Nearest neighbor pattern classiﬁca-
tion. IEEE transactions on information theory, 13(1):21–27,
1967.

[15] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained cate-
gorization and dataset bootstrapping using deep metric learn-
In Proceedings of the IEEE
ing with humans in the loop.
Conference on Computer Vision and Pattern Recognition,
pages 1153–1162, 2016.

[16] M. Cuturi. Sinkhorn distances: Lightspeed computation of
In Advances in neural information pro-

optimal transport.
cessing systems, pages 2292–2300, 2013.

[17] M. Cuturi and D. Avis. Ground metric learning. Journal of

Machine Learning Research, 15(1):533–564, 2014.

[18] M. Cuturi and A. Doucet. Fast computation of wasserstein
barycenters. In International Conference on Machine Learn-
ing, pages 685–693, 2014.

[19] G. Dai, J. Xie, F. Zhu, and Y. Fang. Deep correlated metric
learning for sketch-based 3d shape retrieval. In AAAI, pages
4002–4008, 2017.

[20] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In International
conference on machine learning, pages 647–655, 2014.
[21] C. Frogner, C. Zhang, H. Mobahi, M. Araya, and T. A. Pog-
gio. Learning with a wasserstein loss. In Advances in Neural
Information Processing Systems, pages 2053–2061, 2015.

[22] T. Furuya and R. Ohbuchi. Ranking on cross-domain man-
In Cyberworlds
ifold for sketch-based 3d model retrieval.
(CW), 2013 International Conference on, pages 274–281.
IEEE, 2013.

[23] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
In Proceedings of the IEEE conference on
segmentation.
computer vision and pattern recognition, pages 580–587,
2014.

[24] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In Computer vision
and pattern recognition, 2006 IEEE computer society con-
ference on, volume 2, pages 1735–1742. IEEE, 2006.
[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[26] E. Hoffer and N. Ailon. Deep metric learning using triplet
In International Workshop on Similarity-Based

network.
Pattern Recognition, pages 84–92. Springer, 2015.

[27] K. J¨arvelin, S. L. Price, L. M. Delcambre, and M. L. Nielsen.
Discounted cumulated gain based evaluation of multiple-
query ir sessions. In European Conference on Information
Retrieval, pages 4–15. Springer, 2008.

[28] J. Knopp, M. Prasad, G. Willems, R. Timofte, and
L. Van Gool. Hough transform and 3d surf for robust three
dimensional classiﬁcation. Computer vision–ECCV 2010,
pages 589–602, 2010.

[29] I. Kokkinos, M. M. Bronstein, R. Litman, and A. M. Bron-
Intrinsic shape context descriptors for deformable
stein.
In Computer Vision and Pattern Recognition
shapes.
(CVPR), 2012 IEEE Conference on, pages 159–166. IEEE,
2012.

[30] A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset.

online: http://www. cs. toronto. edu/kriz/cifar. html, 2014.

[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.

[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[33] B. Li, Y. Lu, A. Godil, T. Schreck, M. Aono, H. Johan,
J. M. Saavedra, and S. Tashiro. SHREC13 track: large scale
sketch-based 3D shape retrieval. 2013.

4329

[34] B. Li, Y. Lu, C. Li, A. Godil, T. Schreck, M. Aono,
M. Burtscher, Q. Chen, N. K. Chowdhury, B. Fang, et al. A
comparison of 3d shape retrieval methods based on a large-
scale benchmark supporting multimodal queries. Computer
Vision and Image Understanding, 131:1–27, 2015.

[35] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
neural network for real-time object recognition. In Intelligent
Robots and Systems (IROS), 2015 IEEE/RSJ International
Conference on, pages 922–928. IEEE, 2015.

[36] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming
distance metric learning. In Advances in neural information
processing systems, pages 1061–1069, 2012.

[37] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4004–4012, 2016.

[38] R. Osada, T. Funkhouser, B. Chazelle, and D. Dobkin.
Shape distributions. ACM Transactions on Graphics (TOG),
21(4):807–832, 2002.

[39] O. Pele and M. Werman. Fast and robust earth mover’s dis-
tances. In Computer vision, 2009 IEEE 12th international
conference on, pages 460–467. IEEE, 2009.

[40] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-
man. Object retrieval with large vocabularies and fast spa-
In Computer Vision and Pattern Recogni-
tial matching.
tion, 2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE,
2007.

[41] D. Pickup, X. Sun, P. L. Rosin, R. Martin, Z. Cheng, Z. Lian,
M. Aono, A. Ben Hamza, A. Bronstein, M. Bronstein, et al.
Shrec14 track: Shape retrieval of non-rigid 3d human mod-
els. In Proceedings of the 7th Eurographics workshop on 3D
Object Retrieval, volume 1, page 6. Eurographics Associa-
tion, 2014.

[42] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained visual
categorization via multi-stage metric learning. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3716–3724, 2015.

[43] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s
distance as a metric for image retrieval. International journal
of computer vision, 40(2):99–121, 2000.

[44] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 815–823, 2015.

[45] K. Sﬁkas, T. Theoharis, and I. Pratikakis. Exploiting the
panorama representation for convolutional neural network
In Eurographics Workshop on
classiﬁcation and retrieval.
3D Object Retrieval, 2017.

[46] B. Shi, S. Bai, Z. Zhou, and X. Bai. Deeppano: Deep
IEEE
panoramic representation for 3-d shape recognition.
Signal Processing Letters, 22(12):2339–2343, 2015.
[47] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[48] A. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape
In European Conference

surfaces using geometry images.
on Computer Vision, pages 223–240. Springer, 2016.

[49] R. Sinkhorn. Diagonal equivalence to matrices with pre-
scribed row and column sums. The American Mathematical
Monthly, 74(4):402–405, 1967.

[50] K. Sohn. Improved deep metric learning with multi-class n-
pair loss objective. In Advances in Neural Information Pro-
cessing Systems, pages 1857–1865, 2016.

[51] P. Sousa and M. J. Fonseca. Sketch-based retrieval of draw-
ings using spatial proximity. Journal of Visual Languages &
Computing, 21(2):69–80, 2010.

[52] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-
view convolutional neural networks for 3d shape recognition.
In Proceedings of the IEEE international conference on com-
puter vision, pages 945–953, 2015.

[53] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
In Proceedings of the
Going deeper with convolutions.
IEEE conference on computer vision and pattern recogni-
tion, pages 1–9, 2015.

[54] J. W. Tangelder and R. C. Veltkamp. A survey of content
based 3d shape retrieval methods. In Shape Modeling Appli-
cations, 2004. Proceedings, pages 145–156. IEEE, 2004.
[55] A. Tatsuma, H. Koyanagi, and M. Aono. A large-scale
shape benchmark for 3d object retrieval: Toyohashi shape
benchmark. In Signal & Information Processing Association
Annual Summit and Conference (APSIPA ASC), 2012 Asia-
Paciﬁc, pages 1–10. IEEE, 2012.

[56] S. Vallender. Calculation of the wasserstein distance between
probability distributions on the line. Theory of Probability &
Its Applications, 18(4):784–786, 1974.

[57] C. Villani. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

[58] F. Wang, L. Kang, and Y. Li. Sketch-based 3d shape retrieval
using convolutional neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1875–1883, 2015.

[59] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang,
J. Philbin, B. Chen, and Y. Wu. Learning ﬁne-grained image
In Proceedings of the IEEE
similarity with deep ranking.
Conference on Computer Vision and Pattern Recognition,
pages 1386–1393, 2014.

[60] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb):207–244, 2009.
[61] Y. Wu and S. Verd´u. Witsenhausen’s counterexample: A
view from optimal transport theory. In Decision and Control
and European Control Conference (CDC-ECC), 2011 50th
IEEE Conference on, pages 5732–5737. IEEE, 2011.
[62] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1912–1920, 2015.
[63] J. Xie, G. Dai, F. Zhu, and Y. Fang. Learning barycentric
representations of 3d shapes for sketch-based 3d shape re-
trieval. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3615–3623. IEEE, 2017.

4330

