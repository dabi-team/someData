1
2
0
2

t
c
O
1
1

]
L
P
.
s
c
[

1
v
0
9
3
5
0
.
0
1
1
2
:
v
i
X
r
a

Synthesizing Machine Learning Programs with PAC
Guarantees via Statistical Sketching

OSBERT BASTANI, University of Pennsylvania, USA

We study the problem of synthesizing programs that include machine learning components such as deep
neural networks (DNNs). We focus on statistical properties, which are properties expected to hold with high
probabilityâ€”e.g., that an image classification model correctly identifies people in images with high probability.
We propose novel algorithms for sketching and synthesizing such programs by leveraging ideas from statistical
learning theory to provide statistical soundness guarantees. We evaluate our approach on synthesizing list
processing programs that include DNN components used to process image inputs, as well as case studies on
image classification and on precision medicine. Our results demonstrate that our approach can be used to
synthesize programs with probabilistic guarantees.

1 INTRODUCTION
Machine learning has recently become a powerful tool for solving challenging problems in artificial
intelligence. As a consequence, there has been a great deal of interest in incorporating machine
learning components such as deep neural networks (DNNs) into real-world systems, ranging from
healthcare decision-making [22, 27, 38], to robotics perception and control [44, 56], to improving
performance of software systems [16, 17, 39, 40, 43].

In these domains, there is often a need to ensure correctness properties of the overall system.
To reason about such properties, we need to reason about properties of the incorporated machine
learning components. However, it is in general impossible to absolutely guarantee correctness of a
machine learning componentâ€”e.g., we can never guarantee that a DNN correctly detects every
single image containing a pedestrian. Instead, we consider statistical properties, which are properties
that hold with high probability with respect to the distribution of inputsâ€”e.g., we may want to
ensure that the DNN detects 95% of pedestrians encountered by an autonomous car.

We propose a framework for synthesizing programs that incorporate machine learning com-
ponents while satisfying statistical correctness properties. Our framework consists of two com-
ponents.1 First, it includes a novel statistical sketching algorithm, which builds on the concept of
sketching [66] to provide statistical guarantees. At a high level, it takes as input a sketch annotated
with specifications encoding statistical properties that are expected to hold, as well as holes cor-
responding to real-valued thresholds for making decisions (e.g., the confidence level at which to
label an image as containing a pedestrian or to diagnose a patient with a disease). Since statistical
properties depend on the data distribution, it additionally takes as input a labeled dataset of training
examples (separate from those used to train the DNNs). Then, our algorithm selects values to fill
the holes in the sketch so all the given specifications are satisfied.

1For completeness, our framework also includes a third component for statistical verification of machine learning pro-
grams [62, 73], which is described in Appendix A.

Authorâ€™s address: Osbert Bastani, obastani@seas.upenn.edu, University of Pennsylvania, USA.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2021 Association for Computing Machinery.
XXXX-XXXX/2021/10-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: October 2021.

 
 
 
 
 
 
2

Bastani

Second, our framework uses this sketching algorithm in conjunction with a syntax-guided syn-
thesizer [2] to synthesize programs in a specific domain that provably satisfy statistical guarantees.
Our strategy is to first synthesize a sketch whose specifications encode the overall statistical cor-
rectness property, and then apply our sketching algorithm to fill the holes in the sketch so these
specifications are satisfied with high probability.

The key challenge is providing statistical guarantees for programs using DNNs. To do so, we
leverage generalization bounds from statistical learning theory [34, 69, 71]. These bounds can be
thought of as a variant of concentration inequalities to apply to parameters that are estimated based
on a dataset. Traditionally, it is hard to apply generalization bounds to obtain useful guarantees on
the performance of machine learning components. One reason is that modern machine learning
models such as DNNs do not satisfy assumptions in learning theory. However, a deeper issue is
that learning theory can only prove bounds with respect to the best model in a given family, not
the â€œtrueâ€ model. More precisely, given a model family F and a training dataset (cid:174)ğ‘§, learning theory
provides bounds on the performance of the model Ë†ğ‘“ learned using (cid:174)ğ‘§ in the following form:

ğ¿( Ë†ğ‘“ ) â‰¤ ğ¿(ğ‘“ âˆ—) + G(F , ğ‘›),
where ğ¿ a loss function (e.g., the accuracy of a model ğ‘“ ) and G is a measure of the complexity
of F in the context of the amount of available training data ğ‘› = |(cid:174)ğ‘§| (e.g., VC dimension [34] or
Rademacher complexity [7]). In particular, learning theory provides no tools for bounding the error
ğ¿(ğ‘“ âˆ—) of the optimal model given infinite training examples. In other words, learning theory cannot
guarantee that Ë†ğ‘“ is good at detecting pedestrians; at best, that given enough data, it is as good at
detecting pedestrians as the best possible DNN ğ‘“ âˆ— âˆˆ F .

However, we can provide guarantees for loss functions ğ¿ where we know that there exists some
solution ğ‘“ âˆ— with zero loss ğ¿(ğ‘“ âˆ—) = 0. In an analogy with program verification, we cannot in general
devise verification algorithms that are both sound and complete. Instead, the goal is to devise
algorithms that are as precise as possible subject to a soundness constraint. Similarly, our goal is to
learn models that perform as well as possible while satisfying a statistical propertyâ€”i.e., we want a
model that empirically minimizes the number of false alarms while still satisfying the correctness
guarantee. For instance, this approach satisfies the above condition since ğ¿(ğ‘“ âˆ—) = 0 if ğ‘“ âˆ— predicts
there is a pedestrian in every image. Thus, we might learn a model that is guaranteed to detect 95%
of pedestrians but reports many false alarms (but in practice, we often achieve good performance).
In this context, we show how to use learning theory to sketch programs with statistical guarantees.
The machine learning components (e.g., DNNs) in the given sketch have already been trained
before our sketching algorithm is applied. In particular, the only task that must be performed by
our sketching algorithm is to choose threshold values to fill the holes in the sketch in a way that
satisfies the given specifications while maximizing performanceâ€”e.g., choose the confidence level
of the DNN above which an image contains a pedestrian so we detect 95% of pedestrians. Then,
generalization bounds can give us formal guarantees because (i) we are only synthesize a handful
of parameters, so the generalization error G(F , ğ‘›) is small, and (ii) we can always choose the
thresholds to make conservative decisions, so the error ğ¿(ğ‘“ âˆ—) of the best possible model is small (we
choose the loss ğ¿ to measure whether the given specifications are satisfied, not the performance).
Next, we propose an algorithm for synthesizing machine learning programs that leverages our
statistical sketching algorithm as a subroutine. We consider a specification saying that with high
probability, the synthesized program should either return the correct answer or return â€œunknownâ€.
This specification is consistent with the above discussion since we can naÃ¯vely ensure correctness
by always returning â€œunknownâ€. Then, our goal is to synthesize a program that satisfies the desired
statistical specification but returns â€œunknownâ€ as rarely as possible. To achieve this goal, our
synthesis algorithm first uses a standard enumerative synthesis algorithm to identify a program

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

3

that is correct when given access to ground truth labels. When ground truth labels are unavailable,
this program must instead use labels predicted by machine learning components; to satisfy the
specification, these components include holes corresponding to predicted confidences below which
the component returns â€œunknownâ€. Then, our algorithm uses statistical sketching to fill these holds
with thresholds in a way that satisfies the statistical property encoded by the given specification.
Our sketching algorithm requires the holes in the sketch to be annotated with local correctness
properties; then, it fill sthe holes in a way that satisfies these annotations. Thus, the main challenge
for our synthesizer is how to label holes in the sketch with these annotations so that if the
annotations hold true, the given specification is satisfied. We instantiate such a strategy in the
context of list processing programs where the components can include learned DNNs such as object
classifiers or detectors. In particular, our algorithm analyzes the sketch to allocate allowable errors
to each hole in a way that the overall error of the program is bounded by the desired amount.

We have implemented our approach in a tool called StatCoder, and evaluate it in two ways.
First, we evaluate its ability to synthesize list processing programs satisfying statistical properties
where the program inputs are images, and DNN components are used to classify or detect objects in
these images. Our results show that our algorithm for error allocation outperforms a naÃ¯ve baseline,
and that our novel statistical learning theory bound outperforms using a more traditional bound.
Second, we perform two case studies of our sketching algorithm: one on ImageNet classification
and another on a medical prediction task, which demonstrate additional interesting applications of
our sketching algorithm. In summary, our contributions are:

â€¢ A sketching language for writing programs that incorporate machine learning components

in a way that ensures correctness guarantees (Section 3).

â€¢ Algorithms for sketching (Section 4) and synthesizing (Section 5) such programs.
â€¢ An empirical evaluation (Section 7) validating our approach in the context of our list process-

ing domain (for synthesis) as well as two case studies (for sketching).

2 OVERVIEW
We describe how our statistical sketching algorithm can construct a subroutine for detecting
whether an image contains a person, guaranteeing that if the image contains a person, then it
returns â€œtrueâ€ with high probability. Then, we describe how our synthesizer uses the sketching
algorithm to synthesize a program that counts the number of people in a sequence of images.

Statistical sketching. We assume given a DNN component ğ‘“ : X â†’ [0, 1] that, given an image
ğ‘¥ âˆˆ X, predicts whether ğ‘¥ contains a person. In particular, ğ‘“ (ğ‘¥) is a score indicating its confidence
that ğ‘¥ contains a person; higher score means more likely to contain a person. We do not assume
the the scores are reliableâ€”e.g., they may be overconfident. We assume that the ground truth label
ğ‘¦âˆ— âˆˆ Y = {0, 1} indicates whether ğ‘¥ contains a person. For example, ğ‘“ (ğ‘¥) may be the probability
that an image contains a person according to a pretrained DNN such as ResNet [29]; then, the goal
is to tailor this DNN to the current task in a way that provides correctness guarantees.

In particular, our goal is to choose a threshold ğ‘ âˆˆ [0, 1] such that the program returns that
the given image ğ‘¥ contains a person if ğ‘“ (ğ‘¥) has confidence at least 1 âˆ’ ğ‘â€”i.e., ğ‘“ (ğ‘¥) â‰¥ 1 âˆ’ ğ‘, or
equivalently, 1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘. Furthermore, we want ğ‘ to be correct in the following sense:

(ğ‘¦âˆ— = 1) â‡’ (1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘)

That is, if the image contains a person (i.e., ğ‘¦âˆ— = 1), then the classifier should say so (i.e., 1âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘).
Note that we do not require the converseâ€”i.e., the program may incorrectly conclude that an image
contains a person even if it does not. That is, we want soundness (i.e., no false negatives) but not
necessarily completeness (i.e., no false positives). However, we cannot guarantee that soundness

, Vol. 1, No. 1, Article . Publication date: October 2021.

4

Bastani

holds for every image ğ‘¥; instead, we want to guarantee it holds with high probability. There are
two ways to formulate probabilistic correctness. First, we can say ğ‘ is ğœ–-approximately correct if

Pğ‘ (ğ‘¥,ğ‘¦âˆ—)

(cid:0)ğ‘¦âˆ— = 1 â‡’ 1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘(cid:1) â‰¥ 1 âˆ’ ğœ–,

(1)

where ğ‘ (ğ‘¥, ğ‘¦âˆ—) is the data distribution and ğœ– âˆˆ R>0 is a user-provided confidence levelâ€”i.e., ğ‘“ is
correct for 1 âˆ’ ğœ– fraction of images sampled from ğ‘ (ğ‘¥, ğ‘¦âˆ—) that contain a person. Alternatively, we
can say ğ‘ is ğœ–-approximately correct if

Pğ‘ (ğ‘¥,ğ‘¦âˆ—)

(cid:0)1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘ | ğ‘¦âˆ— = 1(cid:1) â‰¥ 1 âˆ’ ğœ–.

(2)

We refer to (1) as an implication guarantee and (2) as a conditional guarantee. The difference is how
â€œirrelevant examplesâ€ (i.e., ğ‘¦âˆ— = 0) are counted: (1) counts them as being correctly handled, whereas
(2) omits them from consideration. In our example, (1) says we can count all images without people
as being correctly handled. If most images do not contain people, then we can make a large number
of mistakes on images that contain people and still achieve â‰¥ 1 âˆ’ ğœ– correctness overall. In contrast,
(2) ignores images without people, so we must obtain â‰¥ 1 âˆ’ ğœ– correctness rate on images with
people alone. However, using (2), if there are very few images with people, then estimates of the
error rate can be very noisy, making our algorithm very conservative. We allow the user choose
which guarantee to use; intuitively, (1) can be used if the goal is to bound the overall error rate,
whereas (2) should be used if it is to bound the error rate among relevant examples. Our syntax for
expressing the specification in our example is

The syntax | indicates the conditional guarantee (2); we use â‡’ to indicate (1).

1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘ {ğ‘¦âˆ— = 1} |

0.05.

We need to slightly weaken our guarantees in an additional way. The reason is that our algorithm
relies on training examples (cid:174)ğ‘§ = {(ğ‘¥1, ğ‘¦âˆ—
ğ‘– ) âˆ¼ ğ‘ are i.i.d.
samples from ğ‘ (ğ‘¥, ğ‘¦âˆ—). Thus, as with probably approximately correct (PAC) bounds from statistical
learning theory [28, 69], we need to additional allow a possibility that our algorithm fails altogether
due to the randomness in our training examples (cid:174)ğ‘§. In particular, consider an algorithm ğ´ that
chooses ğ‘ = ğ´((cid:174)ğ‘§); then, we say ğ´ is (ğœ–, ğ›¿)-PAC if

ğ‘›)} to choose ğ‘, where (ğ‘¥ğ‘–, ğ‘¦âˆ—

1), ..., (ğ‘¥ğ‘›, ğ‘¦âˆ—

Pğ‘ ( (cid:174)ğ‘§)

(cid:0)ğ´((cid:174)ğ‘§) is ğœ–-approximately correct(cid:1) â‰¥ 1 âˆ’ ğ›¿

where ğ‘ ((cid:174)ğ‘§) is the distribution over the training examples (cid:174)ğ‘§, and ğ›¿ âˆˆ R>0 is another user-provided
confidence level. Then, given the sketch, a value ğ›¿ âˆˆ R>0, and a dataset (cid:174)ğ‘§, our algorithm synthesizes
a value of ğ‘ to fill ??1 in a way that ensures that the specification holds (i.e., ğ‘ is ğœ–-approximately
correct) with probability at least 1 âˆ’ ğ›¿.

Synthesis algorithm. Next, suppose we want to synthesize a program that counts the number of
people in a list of images â„“ = (ğ‘¥1, ..., ğ‘¥ğ‘›). Intuitively, we can do so by writing a simple list processing
program around our DNN for detecting people. In particular, letting

(predictperson ğ‘¥) = 1(1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ??)
be our DNN component, where the detection threshold has been left as a hole, then the sketch
Ëœğ‘ƒex = (fold + (map predictperson â„“) 0)
counts the number of people in â„“. Given a few input-output examples along with the ground truth
labels for each image, we can use a standard enumerative synthesizer to compute the sketch Ëœğ‘ƒex,
assuming predictperson returns the ground truth label. In particular, this sketch has a single hole in
the DNN component predictperson that remains to be filled.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

5

Note that Ëœğ‘ƒex evaluates correctly if predictperson returns the ground truth label, but in general,
it may make mistakes. Thus, the correctness property for the synthesized program ğ‘ƒex needs to
account for the possibility that predictperson may return incorrectly. Mirroring the correctness
property for a single prediction, suppose we want a program ğ‘ƒex that conservatively overestimates
the number of people in â„“.2 In particular, given confidence levels ğœ–, ğ›¿ âˆˆ R>0, we say a completion
ğ‘ƒex of Ëœğ‘ƒex is ğœ–-approximately correct if

where ğ›¼ = (â„“, ğ‘¦âˆ—) is an example, and
Then, we say our synthesis algorithm is (ğœ–, ğ›¿)-probably approximately correct (PAC) if

â„“ â‰¥ ğ‘¦âˆ—) â‰¥ 1 âˆ’ ğœ–,
(cid:75)

ğ‘ƒex
(cid:74)
â„“ denotes the output of running program ğ‘ƒ on input â„“.
(cid:75)

Pğ‘ (ğ›¼) (
ğ‘ƒ
(cid:74)

Pğ‘ ( (cid:174)ğ›¼) (ğ´( Ëœğ‘ƒex, (cid:174)ğ›¼) is ğœ–-approximately correct) â‰¥ 1 âˆ’ ğ›¿,
where ğ‘ƒex = ğ´( Ëœğ‘ƒex, (cid:174)ğ›¼) is the program synthesized using our algorithm and training examples (cid:174)ğ›¼.
Using our statistical sketching algorithm, we can provide (ğœ– â€², ğ›¿ â€²)-PAC guarantees on predictperson
for any ğœ– â€², ğ›¿ â€² âˆˆ R>0; thus, the question is how to choose (i) the appropriate specification, (ii) the
parameters of this specification, and (iii) the confidence levels ğœ– â€², ğ›¿ â€². These choices depend on the
specification that we want to ensure for the synthesized program ğ‘ƒex. In our example, we can use
the specification aboveâ€”i.e., that predictperson returns 1 with high probability if there is a person:
(predictperson ğ‘¥) = 1(1 âˆ’ ğ‘“ (ğ‘¥) â‰¤??) {ğ‘¦âˆ— = 1} |
ğœ–â€².
In general, the specification on predictperson may have additional parameters (in particular, for
real-valued predictions, an error tolerance ğ‘’).

Next, we need to choose ğœ– â€², ğ›¿ â€². While there is only one hole, predictperson is executed multiple times
(assuming length(â„“) > 1). We need to choose ğœ– â€² and ğ›¿ â€² so that with high probability, predictperson
is correct for all applications. For simplicity, we assume given an upper bound ğ‘ âˆˆ N on the
maximum possible length of â„“ (we discuss how we might remove this assumption in Section 6).
Given ğ‘ , we take ğœ– â€² = ğœ–/ğ‘ and ğ›¿ â€² = ğ›¿/ğ‘ ; then, we use our sketching algorithm to synthesize ğ‘ to
fill the hole in predictperson. By a union bound, for a given list â„“, all applications of predictperson are
correct with probability at least 1 âˆ’ ğœ–, and this property holds with probability at least 1 âˆ’ ğ›¿. Under
this event, ğ‘ƒex returns correctlyâ€”i.e., ğ‘ƒex satisfies the desired (ğœ–, ğ›¿)-PAC guarantee.

3 SKETCH LANGUAGE
In this section, we describe the syntax and semantics of our sketch language, as well as the desired
correctness properties we expect that synthesized programs should satisfy.

Syntax. Our sketch language is shown in Figure 1. Intuitively, in the expression ğœ™ (ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğœ– ,
ğ‘„ is a specification that we want to ensure holds, ğ‘ƒ is a score (intuitively, it should indicate the
likelihood that ğ‘„ holds, but we make no assumptions about it), ğ‘ is a threshold below which we
consider ğ‘„ to be satisfied, ğœ– is the allowed failure probability, and ğœ” indicates whether we want
a conditional guarantee (i.e., ğœ” = |, the guarantee (2)) or implication guarantee (i.e., ğœ” = â‡’, the
guarantee (1)). We assume that ğ‘ƒ evaluates to a value in R, ğ‘ âˆˆ R, and ğ‘„ evaluates to a value in
{0, 1}. Note that ğ‘„ is itself a program; unlike programs ğ‘ƒ, it can use ground truth inputs ğ‘¦. Finally,
either ğ‘ and ğœ– in this expression can be left as a hole ?? (but not both simultaneously).

We say ğ‘ƒ is complete if it contains no holes and partial otherwise. We use P to denote the space
of programs, Â¯P âŠ† P to denote the space of complete programs, and Â¯ğ‘ƒ âˆˆ Â¯P to denote a complete
program. For ğ‘ƒ âˆˆ P, we use Î¦(ğ‘ƒ) to denote the expressions ğœ™ (ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ– in ğ‘ƒ (including cases
2In Section 5, our synthesis algorithm is presented for the case where it returns the correct answer or â€œunknownâ€ with high
probability, but as we discuss in Section 6, it can easily be modified to return an overestimate of the correct answer.

, Vol. 1, No. 1, Article . Publication date: October 2021.

6

Bastani

ğ‘ƒ ::= ğ‘ | ğ‘¥ | ğ‘“ (ğ‘ƒ, ..., ğ‘ƒ )
| ğœ™ (ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğ‘„ ::= ğ‘ | ğ‘¥ | ğ‘¦ | ğ‘“ (ğ‘„, ..., ğ‘„)

ğœ– | ğœ™ (ğ‘ƒ, ??) {ğ‘„ }ğœ”

ğœ– | ğœ™ (ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
??

ğ‘
(cid:74)
ğ‘¥
(cid:74)
ğ‘¦

(cid:74)

âˆ—
ğ›¼ = ğ‘
(cid:75)
âˆ—
ğ›¼ = ğ›¼ (ğ‘¥)
(cid:75)
âˆ—
ğ›¼ = ğ›¼ (ğ‘¦)
(cid:75)

ğ›½ = ğ‘
ğ›½ = ğ›½ (ğ‘¥)

ğ‘
(cid:74)
ğ‘¥
(cid:74)

(cid:75)

(cid:75)

ğ‘“ (ğ‘ƒ, ..., ğ‘ƒ )
(cid:74)
ğ‘“ (ğ‘„, ..., ğ‘„)
ğœ™ (ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)

(cid:74)

âˆ—
âˆ—
ğ›¼ , ...,
ğ›¼ = ğ‘“ (
ğ‘ƒ
(cid:74)
(cid:75)
(cid:75)
âˆ—
âˆ—
ğ›¼ , ...,
ğ‘„
ğ›¼ = ğ‘“ (
(cid:75)
(cid:74)
(cid:75)
âˆ—
ğ‘„
ğ›¼ =
(cid:74)
(cid:75)

âˆ—
ğ›¼
(cid:75)

ğ‘ƒ
(cid:74)

âˆ—
ğ›¼ )
(cid:75)
âˆ—
ğ‘„
ğ›¼ )
(cid:75)
(cid:74)

(cid:74)

ğ‘“ (ğ‘ƒ, ..., ğ‘ƒ )
ğœ™ (ğ‘ƒ, ğ‘£) {ğ‘„ }ğœ”
ğœ–
(cid:74)

(cid:75)

(cid:75)

ğ›½ = ğ‘“ (
ğ›½ = 1(

ğ‘ƒ
(cid:74)

ğ‘ƒ
(cid:74)

ğ›½ , ...,
ğ‘ƒ
(cid:74)
ğ›½ > ğ‘)

(cid:75)

(cid:75)

ğ›½ )

(cid:75)

Fig. 1. Syntax (left), train semantics (right, top), and test semantics (right, bottom). The production rules in
the syntax are implicitly universally quantified over constant values ğ‘ âˆˆ C, input variables ğ‘¥ âˆˆ X, ground truth
input variables ğ‘¦ âˆˆ Y, components ğ‘“ âˆˆ F where ğ‘“ : Cğ‘˜ â†’ C, ğœ– âˆˆ R>0, and ğœ” âˆˆ {|, â‡’}. The distinguished
component ğœ™ âˆˆ F is a function ğœ™ : R2 â†’ R defined by ğœ™ (ğ‘§, ğ‘¡) = 1(ğ‘§ â‰¤ ğ‘¡).

where ğ‘ or ğœ– is a hole), Î¦ğ‘
to denote the expressions ğœ™ (ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

??(ğ‘ƒ) âŠ† Î¦(ğ‘ƒ) to denote the expressions ğœ™ (ğ‘ƒ â€², ??) {ğ‘„ }ğœ”
?? in ğ‘ƒ, and Î¦?? (ğ‘ƒ) = Î¦ğ‘
?? (ğ‘ƒ).

?? (ğ‘ƒ) âˆª Î¦ğœ–

ğœ– in ğ‘ƒ, Î¦ğœ–

?? (ğ‘ƒ) âŠ† Î¦(ğ‘ƒ)

Semantics. We define two semantics for programs ğ‘ƒ, shown in Figure 1:
â€¢ Train semantics: Given a training valuation ğ›¼ âˆˆ A, where ğ›¼ : X âˆª Y â†’ C maps both
ğ›¼ evaluate ğ‘„ instead of
âˆ—
(cid:75)

inputs and ground truth inputs ğ‘¦ to values, the train semantics
ğœ™ (ğ‘ƒ, ğ‘). Since they ignore ğœ™, they can be applied to both partial and complete programs.
â€¢ Test semantics: Given a test valuation ğ›½ âˆˆ B, where ğ›½ : X â†’ C maps inputs to values, the

(cid:74)

Â·

test semantics

Â·

ğ›½ evaluate ğœ™ (ğ‘ƒ, ğ‘) instead of ğ‘„. They only apply to complete programs.

(cid:74)

(cid:75)

Correctness properties. We define what it means for a complete program to be correctâ€”i.e., satisfies

its specifications. We begin with correctness of a single specification.

Definition 3.1. Given a distribution ğ‘ (ğ›¼) over test valuations ğ›¼ âˆˆ A, ğœ™ ( Â¯ğ‘ƒ, ğ‘) {ğ‘„ } |

ğœ– is approxi-

mately sound if it satisfies the conditional guarantee3
Pğ‘ (ğ›¼)

ğœ™ ( Â¯ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
(cid:1) â‰¥ 1 âˆ’ ğœ–,
ğœ–
(cid:74)
is approximately sound if it satisfies the implication guarantee

ğœ™ ( Â¯ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)

and {ğ‘„ }â‡’
ğœ–

âˆ—
ğ›¼
(cid:75)

ğ›¼
(cid:75)

(cid:12)
(cid:12)

(cid:0)

ğœ™ ( Â¯ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)
This property can be thought of as probabilistic soundness; it says that we should have ğœ™ ( Â¯ğ‘ƒ, ğ‘) â‡’

ğœ™ ( Â¯ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)

ğ›¼ (cid:1) â‰¥ 1 âˆ’ ğœ–.
(cid:75)

âˆ—
ğ›¼ â‡’
(cid:75)

Pğ‘ (ğ›¼)

(cid:0)

ğ‘„ with high probability, which means that ğœ™ ( Â¯ğ‘ƒ, ğ‘) is a sound overapproximation of ğ‘„.

Definition 3.2. A complete program Â¯ğ‘ƒ is approximately correct (denoted Â¯ğ‘ƒ âˆˆ Â¯Pâˆ—) if every

expression ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

ğœ– in Â¯ğ‘ƒ is approximately sound.

4 STATISTICAL SKETCHING
Next, we describe our algorithm for synthesizing values ğ‘ and ğœ– to fill holes in a given sketch. Our
algorithm, shown in Figure 1, takes as input a sketch ğ‘ƒ, training valuations (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›), where
ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d. samples, and a confidence level ğ›¿ âˆˆ R>0, and outputs a complete program
ğ´( Â¯ğ‘ƒ, (cid:174)ğ›¼) âˆˆ Â¯Pâˆ— that is approximately correct with probability at least 1 âˆ’ ğ›¿ with respect to ğ‘ ( (cid:174)ğ›¼).

Our algorithm synthesizes ğ‘ and ğœ– in a bottom-up fashion, so that all subtrees of the current
expression are complete. Our sketching algorithm uses probabilistic bounds in conjunction with the
given samples (cid:174)ğ›¼ to provide guarantees. Intuitively, since we are estimating parameters from data,
our problem is a statistical learning problem [69], so we can leverage techniques from statistical
learning theory to provide guarantees on the synthesized sketch.

3Note that since ğ›¼ includes valuations of ğ‘¥ âˆˆ X, we can use it in conjunction both train semantics and test semantics.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

7

For synthesizing ğ‘â€”i.e., an expression ğ¸ = ğœ™ (ğ‘ƒ, ğ‘) {ğ‘„ }ğœ”
,
ğ¸
(cid:74)
(cid:75)
ğ›¼ = 1
then ğ‘ is ğœ–-approximately correct if ğ‘§ğ›¼ â‰¤ ğ‘ conditioned on ğ‘§âˆ—
(if ğœ” = â‡’) with probability at least 1 âˆ’ ğœ– with respect to ğ‘ (ğ›¼). In either case, synthesizing ğ‘ is
equivalent to a binary classification problem with labels ğ‘§âˆ—
ğ›¼ , with a one-dimensional hypothesis
space ğ‘ âˆˆ R and a one-dimensional feature space ğ‘§ğ›¼ âˆˆ R. Furthermore, this problem is simpleâ€”ğ‘ is
a linear classifier. Thus, we could use standard learning theory results to provide guarantees.

ğ›¼ âˆˆ R and ğ‘§âˆ—
ğ›¼ =
(cid:75)
ğ›¼ = 1 (if ğœ” = |) or whenever ğ‘§âˆ—

ğœ– . Letting ğ‘§ğ›¼ =

ğ‘ƒ
(cid:74)

However, we can obtain sharper guarantees using a learning theory bound specialized to our
setting. We build on a bound based on [28] (Section 4.1) tailored to the realizable setting, where
there exists a classifier that makes zero mistakes. Our setting is realizable, since ğ‘ = âˆ always
makes zero mistakes. The main difference is that their bound always chooses a classifier that makes
zero mistakes, which can be overly conservative. We prove a novel generalization bound that allows
for some number ğ‘˜ of mistakes that is a function of ğœ–, ğ›¿, and ğ‘›.

Synthesizing a value ğœ– is a bit different, since we are not classifying examples that depend on a
single ğ›¼, but examples that depend on (cid:174)ğ›¼. Thus, we can formulate it as a learning problem where
the examples are (cid:174)ğ›¼; however, this approach is complicated due to the need to figure out how to
divide our given samples (cid:174)ğ›¼ into multiple sub-examples (cid:174)ğ›¼1, ... (cid:174)ğ›¼ğ‘›. Instead, we use an approach based
on Hoeffdingâ€™s inequality [30] (Section 4.2) to infer ğœ–. In particular, Hoeffdingâ€™s inequality gives us
a lower bound on the correctness rate Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—
ğ›¼ â‡’ ğ‘§ğ›¼ ) â‰¥ 1 âˆ’ ğœ–
(if ğœ” = â‡’), and we can simply use this ğœ–.

ğ›¼ ) â‰¥ 1 âˆ’ ğœ– (if ğœ” = |) or Pğ‘ (ğ›¼) (ğ‘§âˆ—

Finally, our sketching algorithm uses the above two approaches to synthesize ğ‘ and ğœ– (Section 4.3).

4.1 A Learning Theory Bound

Problem formulation. We consider a unary classification problem with one-dimensional feature
and hypothesis spaces. In particular, given a probability distribution ğ‘ (ğ‘§) over ğ‘§ âˆˆ R (the feature),
the goal is to select the smallest possible threshold ğ‘¡ âˆˆ R (the hypothesis) such that

Pğ‘ (ğ‘§) (ğ‘§ â‰¤ ğ‘¡) â‰¥ 1 âˆ’ ğœ–
for a given ğœ– âˆˆ R>0. That is, we want the smallest possible ğ‘¡ such that ğ‘§ âˆˆ (âˆ’âˆ, ğ‘¡] with probability
at least 1 âˆ’ ğœ– according to ğ‘ (ğ‘§). We denote the subset of ğ‘¡ that satisfies (3) by

(3)

Tğœ– = (cid:8)ğ‘¡ âˆˆ R | Pğ‘ (ğ‘§) (ğ‘§ â‰¤ ğ‘¡) â‰¥ 1 âˆ’ ğœ–(cid:9) .
To compute such a ğ‘¡, we are given a training set of examples (cid:174)ğ‘§ = (ğ‘§1, ..., ğ‘§ğ‘›) âˆˆ Rğ‘›, where ğ‘§1, ..., ğ‘§ğ‘› âˆ¼ ğ‘
are ğ‘› i.i.d. samples from ğ‘. An estimator Ë†ğ‘¡ is a mapping Ë†ğ‘¡ : Rğ‘› â†’ R. Then, the constraint (3) is
Ë†ğ‘¡ ((cid:174)ğ‘§) âˆˆ Tğœ– ; we say such a Ë†ğ‘¡ is ğœ–-approximately correctâ€”i.e., it is correct for â€œmostâ€ samples ğ‘§ âˆ¼ ğ‘.

In general, we are unable to guarantee that Ë†ğ‘¡ is approximately correct due to the randomness in
the training examples (cid:174)ğ‘§. Thus, we additionally allow for a small probability ğ›¿ âˆˆ R>0 that Ë†ğ‘¡ is not
approximately correct.

Definition 4.1. Given ğœ–, ğ›¿ âˆˆ R>0, Ë†ğ‘¡ is (ğœ–, ğ›¿)-PAC if Pğ‘ ( (cid:174)ğ‘§) (Ë†ğ‘¡ ((cid:174)ğ‘§) âˆˆ Tğœ– ) â‰¥ 1 âˆ’ ğ›¿.
That is, Ë†ğ‘¡ ((cid:174)ğ‘§) is approximately correct with probability at least 1 âˆ’ ğ›¿ according to ğ‘ ((cid:174)ğ‘§). Our goal

is to construct an (ğœ–, ğ›¿)-PAC estimator Ë†ğ‘¡ ((cid:174)ğ‘§) that tries to minimize Ë†ğ‘¡ ((cid:174)ğ‘§).

Estimator. Given ğœ–, ğ›¿ âˆˆ R>0, consider the estimator

Ë†ğ‘¡ ((cid:174)ğ‘§) = inf
ğ‘¡ âˆˆR

(cid:8)ğ‘¡ âˆˆ R (cid:12)

(cid:12) ğ¿(ğ‘¡; (cid:174)ğ‘§) â‰¤ ğ‘˜(cid:9) + ğ›¾ ((cid:174)ğ‘§) where ğ‘˜ = max

(cid:40)
â„ âˆˆ N

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

â„
âˆ‘ï¸

ğ‘–=0

(cid:19)

(cid:18)ğ‘›
ğ‘–

ğœ–ğ‘– (1 âˆ’ ğœ–)ğ‘›âˆ’ğ‘– â‰¤ ğ›¿

(cid:41)

(4)

where the empirical loss is ğ¿(ğ‘¡; (cid:174)ğ‘§) = (cid:205)ğ‘§ âˆˆ(cid:174)ğ‘§ 1(ğ‘§ > ğ‘¡), and where ğ›¾ ((cid:174)ğ‘§) > 0 is an arbitrary positive
function. Intuitively, the empirical loss counts the number of mistakes that ğ‘¡ makes on the training

, Vol. 1, No. 1, Article . Publication date: October 2021.

8

Bastani

Algorithm 1 Use learning theory to sketch Â¯ğ‘ƒ that is approximately correct.

procedure Sketch(ğ‘ƒ, (cid:174)ğ›¼, ğ›¿)

ğ‘š â† |Î¦?? (ğ‘ƒ)|
for ğ¸ âˆˆ BottomUp(Î¦??(ğ‘ƒ)) do
if ğ¸ = ğœ™ ( Â¯ğ‘ƒ â€², ??) {ğ‘„ }ğœ”
ğœ– then

Compute (cid:174)ğ‘§ (cid:174)ğ›¼ according to (6)
Compute Ë†ğ‘¡ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) according to (4) with (ğœ–, ğ›¿/ğ‘š)
Fill the hole ?? with Ë†ğ‘¡ ((cid:174)ğ‘§ğ›¼ )

else if ğ¸ = ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

?? then
Compute (cid:174)ğ‘§ (cid:174)ğ›¼ according to (6)
Compute Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) according to (5) with ğ›¿/ğ‘š
Fill the hole ?? with 1 âˆ’ Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ )

end if

end for
return true
end procedure

dataâ€”i.e., ğ‘§ âˆˆ (cid:174)ğ‘§ such that ğ‘§ âˆ‰ (âˆ’âˆ, ğ‘¡]. To compute the solution ğ‘˜ in (4), we start with â„ = 0 and
increment it until it no longer satisfies the condition. To ensure numerical stability, this computation
is performed using logarithms. Note that ğ‘˜ does not exist if the set inside the maximum in (4) is
empty; in this case, we choose Ë†ğœ“ ((cid:174)ğ‘§) = 0, which trivially satisfies the PAC property. To compute
Ë†ğ‘¡ ((cid:174)ğ‘§), we sort the training examples ğ‘§1, ..., ğ‘§ğ‘› by magnitude, so ğ‘§1 â‰¥ ğ‘§2 â‰¥ ... â‰¥ ğ‘§ğ‘›. Finally, ğ‘§ğ‘˜+1 solves
the minimization problem in (4), so Ë†ğ‘¡ ((cid:174)ğ‘§) = ğ‘§ğ‘˜+1 + ğ›¾ ((cid:174)ğ‘§). If ğ‘˜ does not exist, then we choose Ë†ğ‘¡ ((cid:174)ğ‘§) = âˆ,
which trivially satisfies the PAC property. We have the following; see Appendix B.2 for a proof:

Theorem 4.2. The estimator Ë†ğ‘¡ ((cid:174)ğ‘§) in (4) is (ğœ–, ğ›¿)-PAC.

4.2 A Concentration Bound

Problem formulation. Consider a Bernoulli distribution ğ‘ = Bernoulli(ğœ‡) with unknown mean
ğœ‡ âˆˆ [0, 1]. Our goal is to compute a lower bound ğœˆ âˆˆ [0, 1] of ğœ‡â€”i.e., ğœ‡ â‰¥ ğœˆ. For example, if ğœ‡ is
the error rate of a classifier, then ğœˆ is a lower bound on this rate. To compute ğœˆ, we are given a
training set (cid:174)ğ‘§ = (ğ‘§1, ..., ğ‘§ğ‘›) âˆˆ {0, 1}ğ‘›, where ğ‘§1, ..., ğ‘§ğ‘› âˆ¼ ğ‘ are ğ‘› i.i.d. samples from ğ‘. An estimator is
a mapping Ë†ğœˆ : Rğ‘› â†’ R. We say Ë†ğœˆ is correct if it satisfies ğœ‡ â‰¥ Ë†ğœˆ ((cid:174)ğ‘§). We are unable to guarantee that
Ë†ğœˆ ((cid:174)ğ‘§) is correct due to the randomness in the training examples (cid:174)ğ‘§. Thus, we additionally allow for a
small probability ğ›¿ âˆˆ R>0 that Ë†ğœ“ ((cid:174)ğ‘§) is not correctâ€”i.e., it is probably correct (PC).

Definition 4.3. Given ğ›¿ âˆˆ R>0, Ë†ğœˆ is ğ›¿-PC if Pğ‘ ( (cid:174)ğ‘§)
In other words, Ë†ğœˆ ((cid:174)ğ‘§) is correct with probability at least 1 âˆ’ ğ›¿ according to the randomness in

(cid:0)ğœ‡ â‰¥ Ë†ğœˆ ((cid:174)ğ‘§)(cid:1) â‰¥ 1 âˆ’ ğ›¿.

ğ‘ ((cid:174)ğ‘§). Our goal is to construct an ğ›¿-PC estimator Ë†ğœˆ ((cid:174)ğ‘§).

Estimator. Given ğ›¿ âˆˆ R>0, consider the estimator

Ë†ğœˆ ((cid:174)ğ‘§) = Ë†ğœ‡ ((cid:174)ğ‘§) âˆ’

âˆšï¸‚ log(1/ğ›¿)
2ğ‘›

,

(5)

where Ë†ğœ‡ ((cid:174)ğ‘§) = ğ‘›âˆ’1 (cid:205)ğ‘§ âˆˆ(cid:174)ğ‘§ ğ‘§ is an estimate of ğœ‡ based on the samples (cid:174)ğ‘§; we take Ë†ğœˆ ((cid:174)ğ‘§) = 0 if (5) is
negative. Intuitively, the second term in Ë†ğœˆ ((cid:174)ğ‘§) is a correction to Ë†ğœ‡ ((cid:174)ğ‘§) to ensure it is (ğœ–, ğ›¿)-PC, based
on Hoeffdingâ€™s inequality [30]. We have the following; see Appendix B.3 for a proof:

Theorem 4.4. The estimator Ë†ğœˆ is ğ›¿-PC.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

9

4.3 Sketching Algorithm

Problem formulation. A sketching algorithm ğ´ : P Ã— Ağ‘› â†’ Â¯P takes as input a partial program
ğ‘ƒ âˆˆ P, together with a set of test valuations (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›) âˆˆ Ağ‘›, where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d.
samples from an underlying distribution ğ‘ (ğ›¼). Then, Â¯ğ‘ƒ = ğ´(ğ‘ƒ, (cid:174)ğ›¼) should be a complete program
that is approximately correct by filling each hole in expressions ğœ™ (ğ‘ƒ â€², ??) {ğ‘„ }ğœ”
??(ğ‘ƒ) with a
value ğ‘ âˆˆ R and each hole in expressions ğœ™ (ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
??(ğ‘ƒ) with a value ğœ– âˆˆ R>0. We assume
that every expression in Î¦(ğ‘ƒ) has a holeâ€”i.e., Î¦(ğ‘ƒ) = Î¦?? (ğ‘ƒ); otherwise, we cannot guarantee that
the existing thresholds in these expressions are approximately sound.

?? âˆˆ ğœ™ğœ–

ğœ– âˆˆ Î¦ğ‘

Definition 4.5. A partial program ğ‘ƒ âˆˆ P is a full sketch, denoted ğ‘ƒ âˆˆ P0, if Î¦??(ğ‘ƒ) = Î¦(ğ‘ƒ).
Then, we say ğ´ is correct if ğ´(ğ‘ƒ, (cid:174)ğ›¼) âˆˆ Â¯Pâˆ—. We cannot guarantee this property; instead, given

ğ›¿ âˆˆ R>0, we want it to hold with probability at least 1 âˆ’ ğ›¿ according to ğ‘ ( (cid:174)ğ›¼).

Definition 4.6. A sketching algorithm ğ´ : P0 Ã— Ağ‘› â†’ Â¯P is ğ›¿-probably approximately correct
(cid:0)ğ´(ğ‘ƒ, (cid:174)ğ›¼) âˆˆ Â¯Pâˆ—(cid:1) â‰¥ 1 âˆ’ ğ›¿.

(PAC) if for all ğ‘ƒ âˆˆ P0, we have Pğ‘ ( (cid:174)ğ›¼)

Note that this definition does not include ğœ– since these values are provide in the given sketch.

Algorithm. Our sketching algorithm is shown in Algorithm 1. At a high level, it fills each hole so
that the resulting expressions ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ– are all approximately sound. The order in which these
expressions are processed is important; a expression cannot be processed until all its descendants
have been processed. This order ensures that Â¯ğ‘ƒ â€² is complete, so it can be evaluated. In Algorithm 1,
the function BottomUp ensures that the expressions in Î¦?? (ğ‘ƒ) is processed in such an order. The
algorithm allocates a ğ›¿/ğ‘š probability of failure for each expression, where ğ‘š = |Î¦?? (ğ‘ƒ)|.

Synthesizing ğ‘. We describe how our algorithm synthesizes a threshold ğ‘ for an expression

ğ¸ = ğœ™ ( Â¯ğ‘ƒ â€², ??) {ğ‘„ }ğœ”

ğœ– . Given a single test valuation ğ›¼ âˆ¼ ğ‘, consider the values

ğ‘§ğ›¼ =

Â¯ğ‘ƒ â€²
(cid:74)

ğ›¼
(cid:75)

and

ğ‘§âˆ—
ğ›¼ =

ğœ™ ( Â¯ğ‘ƒ â€², ??) {ğ‘„ }ğœ”
ğœ–
(cid:74)

âˆ—
ğ›¼
(cid:75)

Given ğ‘ âˆˆ R, it follows by definition of

Â·

ğ›¼ that
(cid:75)
(cid:74)
ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)

ğ›¼ = 1(ğ‘§ğ›¼ â‰¤ ğ‘).
(cid:75)

Thus, ğ¸ is approximately sound for some ğ‘ âˆˆ R if and only if

Pğ‘ (ğ›¼) (ğ‘§ğ›¼ â‰¤ ğ‘ | ğ‘§âˆ—

ğ›¼ ) â‰¥ 1 âˆ’ ğœ–

if ğœ” = |

or

Pğ‘ (ğ›¼) (ğ‘§âˆ—

ğ›¼ â‡’ ğ‘§ğ›¼ â‰¤ ğ‘) â‰¥ 1 âˆ’ ğœ–

if ğœ” = â‡’ .

Given (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›), where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ i.i.d.,

(cid:40)

(cid:174)ğ‘§ (cid:174)ğ›¼ =

{ğ‘§ğ›¼ | ğ›¼ âˆˆ (cid:174)ğ›¼ âˆ§ ğ‘§âˆ—
{ğ‘§âˆ—

ğ›¼ }
ğ›¼ â‡’ ğ‘§ğ›¼ | ğ›¼ âˆˆ (cid:174)ğ›¼ }

if ğœ” = |
if ğœ” = â‡’

(6)

is a vector of i.i.d. samples. The estimator Ë†ğ‘¡ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) in (4) with parameters (ğœ–, ğ›¿/ğ‘š) ensures approximate
soundness with high probabilityâ€”i.e.,

Pğ‘ (ğ›¼)

(cid:0)ğ‘§ğ›¼ â‰¤ Ë†ğ‘¡ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) | ğ‘§âˆ—

ğ›¼

(cid:1) â‰¥ 1 âˆ’ ğœ–

if ğœ” = |

or Pğ‘ (ğ›¼)

(cid:0)ğ‘§âˆ—

ğ›¼ â‡’ ğ‘§ğ›¼ â‰¤ Ë†ğ‘¡ ((cid:174)ğ‘§ (cid:174)ğ›¼ )(cid:1) â‰¥ 1 âˆ’ ğœ–

if ğœ” = â‡’ .

holds with probability at least 1 âˆ’ ğ›¿/ğ‘š according to ğ‘ ( (cid:174)ğ›¼).

, Vol. 1, No. 1, Article . Publication date: October 2021.

10

Bastani

Synthesizing ğœ–. We describe how our algorithm synthesizes a confidence level ğœ– for an expression

ğ¸ = ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

??. Given a single test valuation ğ›¼ âˆ¼ ğ‘, consider the values
ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
âˆ—
ğ›¼ .
??
(cid:75)
(cid:74)
ğ›¼ and
(cid:75)

ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
??
(cid:74)
Note that we compute these values even though the ğœ– is a hole, since
ğ›¼ do not depend on
âˆ—
(cid:75)
ğœ–. Also, note that unlike the case of synthesizing ğ‘, where ğ‘§ğ›¼ âˆˆ R is a score, in this case, ğ‘§ğ›¼ âˆˆ {0, 1}
is a binary value. Given ğœ– âˆˆ R>0, ğ¸ is ğœ–-approximately sound for ğœ– if and only if

ğ‘§âˆ—
ğ›¼ =

ğ‘§ğ›¼ =

and

ğ›¼
(cid:75)

(cid:74)

(cid:74)

Â·

Â·

Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—

ğ›¼ ) â‰¥ 1 âˆ’ ğœ–

if ğœ” = |

or

Pğ‘ (ğ›¼) (ğ‘§âˆ—

ğ›¼ â‡’ ğ‘§ğ›¼ ) â‰¥ 1 âˆ’ ğœ–

if ğœ” = â‡’ .

Given (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›), where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d. samples, (cid:174)ğ‘§ (cid:174)ğ›¼ defined in (6) is a vector of i.i.d.
samples from Bernoulli(ğœ‡). Then, the estimator Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) in (5) with parameter ğ›¿/ğ‘š is a lower bound
on ğœ‡ with high probabilityâ€”i.e.,

Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—

ğ›¼ ) â‰¥ Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ )

if ğœ” = |

or

Pğ‘ (ğ›¼) (ğ‘§âˆ—

ğ›¼ â‡’ ğ‘§ğ›¼ ) â‰¥ Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ )

if ğœ” = â‡’ .

holds with probability at least 1 âˆ’ ğ›¿/ğ‘š according to ğ‘ ( (cid:174)ğ›¼). Thus, it suffices to choose 1 âˆ’ ğœ– = Ë†ğœˆ ((cid:174)ğ‘§ (cid:174)ğ›¼ ).

The following guarantee follows from Theorems 4.2 & 4.4 by a union bound over Î¦( Â¯ğ‘ƒ):

Theorem 4.7. Algorithm 1 is ğ›¿-PAC.

5 SYNTHESIS ALGORITHM
We now describe a syntax-guided synthesizer that uses our sketching algorithm to identify programs
with machine learning components while satisfying a desired error guarantee. In general, to design
such a synthesizer, we need to design a space of specifications along with a domain-specific language
(DSL) of programs. For clarity, we focus on a specific set of design choices; as we discuss in Section 6,
our approach straightforwardly generalizes in several ways. We consider the following choices:
â€¢ Specifications: We consider specifications Ëœğœ“ = (ğœ“, ğœ–, ğ‘’), consisting of both a traditional
part ğœ“ indicating the logical property that the train semantics of the program should satisfy
(provided either as a logical formula or input-output examples), and a statistical part (ğœ–, ğ‘’)
indicating that the program should have error at most ğ‘’ with probability at least 1 âˆ’ ğœ– with
respect to ğ‘ (ğ›¼), or else return âˆ….

â€¢ DSL: We consider a DSL (shown in Figure 2) of list processing programs where the inputs are
images of integers. Our DSL includes components designed to predict the integer represented
by a given image. These components return the predicted value if its confidence is above a
certain threshold, and return âˆ… otherwise. Values âˆ… are propagated as âˆ… by all components
in our DSLâ€”i.e., if any input to a function is âˆ…, then its output is also âˆ….

For clarity, we refer to specifications Ëœğœ“ as task specifications and specifications on DSL components
as component specifications. As a running example, consider the program in Figure 3. This program
predicts the value ğ‘¥ of the image input1 (as an integer) and values â„“ of the images in the list input2
(as real values), and then sums the values in â„“ that are greater than equal to ğ‘¥. It contains three
components that have component specifications: the two machine learning components predictint
and predictfloat, along with the inequality cond-â‰¤. The first two component specifications ensure
that the corresponding machine learning model returns correctly (or âˆ…) with high probability. For
the last one, note that in the expression ğ‘¦1 â‰¤ ğ‘¦2, the inputs ğ‘¦1 and ğ‘¦2 may have a small amount of
prediction error, so if they are to close together (i.e., |ğ‘¦1 âˆ’ ğ‘¦2| â‰¤ ğ‘ for some ğ‘ âˆˆ Râ‰¤0), then ğ‘¦1 â‰¤ ğ‘¦2
might be incorrect. Thus, to ensure â‰¤ returns correctly, cond-â‰¤ returns âˆ… if |ğ‘¦1 âˆ’ ğ‘¦2| â‰¤ ğ‘.

Finally, note that we use ğœ” = â‡’, indicating that our goal is to synthesize Â¯ğ‘ƒ such that the the
ğ›¼ | > ğ‘’(cid:1) â‰¥ 1 âˆ’ ğœ–. We could use
âˆ—
(cid:75)

overall success rate is boundedâ€”i.e., Pğ‘ (ğ›¼)
ğœ” = | here if we instead wanted to bound the probability of failure conditioned on

ğ›¼ = âˆ… âˆ¨ |
(cid:75)

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

(cid:0)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ â‰  âˆ….
(cid:75)

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

11

ğ‘ƒğœ ::= input1

ğœ | Â· Â· Â· | inputğ‘˜ğœ
ğœ

| (ğ‘ƒğœâ€²â†’ğœ ğ‘ƒğœâ€² )
| (fold ğ‘ƒğœâ€²â†’ğœ â†’ğœ ğ‘ƒlist(ğœâ€²) ğ‘ƒğœ )

ğ‘ƒlist(ğœ ) ::= (map ğ‘ƒğœâ€²â†’ğœ ğ‘ƒlist(ğœâ€²) )

| (filter ğ‘ƒğœ â†’bool ğ‘ƒlist(ğœ ) )
| (slice ğ‘ƒlist(ğœ ) ğ‘ƒint ğ‘ƒint)

ğ‘ƒint ::= (length ğ‘ƒlist(ğœ ) )

ğ‘ƒğœâ†’ğœâ†’ğœ ::= + | âˆ’
ğ‘ƒintâ†’intâ†’bool ::= â‰¤ | = | â‰¥

ğ‘ƒfloatâ†’floatâ†’bool ::= cond-â‰¤ | cond-â‰¥
ğ‘ƒimageâ†’ğœ ::= predictğœ
ğ‘ƒimageâ†’image ::= cond-flip

then Ë†ğ‘“ (ğ‘¥) else âˆ…)

(predictint ğ‘¥) = (if Ë†ğ‘ (ğ‘¥, Ë†ğ‘“ (ğ‘¥)) â‰¥ ??ğ‘ { Ë†ğ‘“ (ğ‘¥) = ğ‘¦âˆ— }â‡’
??ğœ–
(predictfloat ğ‘¥) = (if Ë†ğ‘ (ğ‘¥, Ë†ğ‘“ (ğ‘¥)) â‰¥ ??ğ‘ { | Ë†ğ‘“ (ğ‘¥) âˆ’ ğ‘¦âˆ— | â‰¤ ??ğ‘’ }â‡’
??ğœ–
(cond-flip ğ‘¥) = (if Ë†ğ‘flip (ğ‘¥, Ë†ğ‘“flip (ğ‘¥)) â‰¥ ??ğ‘ { Ë†ğ‘“flip (ğ‘¥) = ğ‘¦âˆ—
flip }â‡’
??ğœ–
(cond-flip0 ğ‘¥) = (if Ë†ğ‘“flip (ğ‘¥) then flip(ğ‘¥) else ğ‘¥)
1 â‰¤ ğ‘¦âˆ—
(cond-â‰¤ ğ‘¦1 ğ‘¦2) = (if |ğ‘¦1 âˆ’ ğ‘¦2 | â‰¥ ??ğ‘ {ğ‘¦âˆ—
1 â‰¥ ğ‘¦âˆ—
(cond-â‰¥ ğ‘¦1 ğ‘¦2) = (if |ğ‘¦1 âˆ’ ğ‘¦2 | â‰¥ ??ğ‘ {ğ‘¦âˆ—

2 }â‡’
??ğœ–
2 }â‡’
??ğœ–

then ğ‘¦1 â‰¥ ğ‘¦2 else âˆ…)

then ğ‘¦1 â‰¥ ğ‘¦2 else âˆ…)

then Ë†ğ‘“ (ğ‘¥) else âˆ…)

then (cond-flip0 ğ‘¥) else âˆ…)

Fig. 2. This figure shows our domain-specific language (DSL) of list processing programs over images of
inputs. The top half shows the production rules; these rules are implicitly universally quantified over the type
variables ğœ and ğœ, where ğœ ::= bool | int | float | image | list(ğœ) | ğœ â†’ ğœ and ğœ ::= int | float | image. The
bottom half shows the semantics of functions in our language that have statistical specifications.

Algorithm 2 Use learning theory to synthesize Â¯ğ‘ƒ that is approximately correct.

procedure Synthesize( (cid:174)ğ›¼,ğœ“, ğœ–, ğ‘’, ğ‘ , ğ›¿)
Ëœğ‘ƒ â† SynthesizePartialSketch(ğœ“ )
(cid:174)ğ›¼synth, (cid:174)ğ›¼sketch â† Split( (cid:174)ğ›¼)
ğ‘ƒ â† arg maxğ‘ƒ â€² âˆˆFillAll( Ëœğ‘ƒ,ğœ–,ğ‘’)
return Sketch(ğ‘ƒ, (cid:174)ğ›¼sketch, ğ›¿)

Score(Sketch(ğ‘ƒ â€², (cid:174)ğ›¼synth, ğ›¿))

end procedure

Given labeled training examples (cid:174)ğ›¼, a task specification Ëœğœ“ , a maximum list length ğ‘ , and a
confidence level ğ›¿, our algorithm shown in Algorithm 2 synthesizes a complete program Â¯ğ‘ƒ that
satisfies Ëœğœ“ with probability at least 1 âˆ’ ğ›¿. At a high level, this algorithm proceeds in three steps:
â€¢ Step 1: First, our algorithm uses the logical specification ğœ“ to identify a sketch Ëœğ‘ƒ whose
train semantics is consistent with ğœ“ . Note that the train semantics for sketches in our DSL in
Figure 2 are well-defined even when the holes left unfilled. We refer to Ëœğ‘ƒ as a partial sketch,
since it has additional holes that cannot be filled by our sketching algorithm.

â€¢ Step 2: While our algorithm uses our sketching algorithm described in Algorithm 1 to fill
holes ??ğ‘ in Ëœğ‘ƒ, it must first fill the holes ??ğœ– and ??ğ‘’ (described below), which cannot be
handled by this algorithm. To this end, it analyzes the program to identify constraints on
the values of ğœ– and ğ‘’ that can be assigned to each hole ??ğœ– and ??ğ‘’ , respectively and satisfy
the desired task specification (ğœ–, ğ‘’). Given candidate values (cid:174)ğ‘’ and (cid:174)ğœ–, it constructs the sketch
ğ‘ƒ = Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’), and evaluates the success rate Score(ğ‘ƒ) (i.e., how often
ğ›¼ â‰  âˆ…). It chooses
(cid:75)
the sketch ğ‘ƒ that maximizes this objective over a finite set of choices of (cid:174)ğœ– and (cid:174)ğ‘’.

ğ‘ƒ
(cid:74)

â€¢ Step 3: Finally, it uses a held-out set of labeled examples (cid:174)ğ›¼sketch in conjunction with our
sketching algorithm in Algorithm 1 to synthesize We use a held-out set since Theorem 4.7
only holds if the examples (cid:174)ğ›¼sketch are not used to construct the sketch ğ‘ƒ.

, Vol. 1, No. 1, Article . Publication date: October 2021.

12

Bastani

Task Specification

Ëœğœ“ex = (cid:0)ğœ“ = { [1, 2, 3] â†¦â†’ 3, [2, 4, 2] â†¦â†’ 4}, ğœ– = 0.05, ğ‘’ = 6, ğ‘ = 3, ğ›¿ = 0.05(cid:1)

Partial Sketch

Components
with Holes

Ëœğ‘ƒex = (fold + (filter (cond-â‰¤
(cid:32)(cid:32)
(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
ğ‘“1

(cid:32)(cid:32)(cid:32)(cid:32)

(predictint
(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
ğ‘“2

input1)) (map predictfloat
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

(cid:123)(cid:122)
ğ‘“3

input2) 0))

ğ‘“1 = (ğœ†ğ‘¦1 (ğœ†ğ‘¦2 (if |ğ‘¦1 âˆ’ ğ‘¦2 | â‰¥ ??ğ‘ {ğ‘¦âˆ—
1 â‰¤ ğ‘¦âˆ—
ğ‘“2 = (ğœ†ğ‘¥ (if Ë†ğ‘ (ğ‘¥, Ë†ğ‘“ (ğ‘¥)) â‰¥ ??ğ‘ { Ë†ğ‘“ (ğ‘¥) = ğ‘¦âˆ— }â‡’
??ğœ–
ğ‘“3 = (ğœ†ğ‘¥ (if Ë†ğ‘ (ğ‘¥, Ë†ğ‘“ (ğ‘¥)) â‰¥ ??ğ‘ { | Ë†ğ‘“ (ğ‘¥) âˆ’ ğ‘¦âˆ— | â‰¤ ??ğ‘’ }â‡’
??ğœ–

then ğ‘¦1 â‰¤ ğ‘¦2 else âˆ…)))

2 }â‡’
??ğœ–
then Ë†ğ‘“ (ğ‘¥) else âˆ…))

then Ë†ğ‘“ (ğ‘¥) else âˆ…))

Fig. 3. Example of a task in our list processing domain. Given Ëœğœ“ex, the goal is to synthesize a program Â¯ğ‘ƒ whose
ğ›¼ | â‰¤ ğ‘’(cid:1) â‰¥ 1 âˆ’ ğœ–.
âˆ—
train semantics satisfies ğœ“ , and whose test semantics satisfy Pğ‘ (ğ›¼)
(cid:75)

ğ›¼ = âˆ… âˆ¨ |
(cid:75)

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

(cid:0)

In Figure 3, we show the partial sketch Ëœğ‘ƒex along with two analyses which are used to help compute
the search space over (cid:174)ğœ– and (cid:174)ğ‘’. Below, we describe our DSL and synthesis algorithm in more detail.

5.1 Domain-Specific Language
Our DSL is summarized in Figure 2. To be precise, this figure shows sketches in our language; filling
holes in these sketches produces a program in our language. At a high level, the language consists
of standard list processing operators such as map, filter, and fold, along with a set of functions that
can be applied to individual integers, real numbers, or images.

Machine learning components. Our DSL has three machine learning components: predictint,
predictfloat, and cond-flip. The first two predict the value in a given image. They are identical except
for their component specification; whereas the integer predictions must be exactly correct, the
real-valued predictions are allowed to have bounded error. We describe these specifications below.
This difference gives the user flexibility in terms of what kind of guarantees they want to provide.
The third machine learning component checks if the input image is flipped along the vertical
axis. We include it to demonstrate how our approach can combine multiple machine learning
components. It only returns an image if it is confident about its prediction; otherwise, it returns âˆ….

Component specifications. Intuitively, there are two kinds of component specifications in our
language: (i) require that the output is exactly correct, and (ii) require that the error of the output
is bounded. There are four components in (i): predictint, cond-flip, cond-â‰¤, and cond-â‰¥. The first
two are straightforwardâ€”they consist of a machine learning component, and return the predicted
value if the prediction confidence is a threshold to be synthesized, and return âˆ… otherwise.

The latter two are result from challenges handling inequalities on real-valued predictions. In
particular, real-valued predictions (i.e., by predictfloat) can be wrong by a bounded amount, yet the
return value of â‰¤ and â‰¥ is a Boolean value that must be exactly correct. Thus, these components
include a component specification indicating that their output must be correct with high probability.
Note that the scoring function used in the condition is |ğ‘¦1 âˆ’ ğ‘¦2|; intuitively, if the inputs ğ‘¦1 and ğ‘¦2
are far apart (i.e., |ğ‘¦1 âˆ’ ğ‘¦2| is large), then the predicted result is less likely to be an error.

The predictfloat component is the only one in (ii). The only difference from predictint is that it only
requires that the prediction is correct to within some bounded amount of errorâ€”i.e., | Ë†ğ‘“ (ğ‘¥) âˆ’ğ‘¦âˆ—| â‰¤ ğ‘’,
for some ğ‘’ âˆˆ Râ‰¥0. Note that ğ‘’ is left as a hole to be filled.

Holes. Our language has three kinds of holes. The first two are holes ??ğ‘ and ??ğœ– ; these are in our
sketch DSL in Figure 2. Note that in that DSL, each component specification could only have either
ğ‘ or ğœ– as a hole, but here we allow both to be left as holes; our algorithm searches over choices

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

13

of ğœ– to fill holes ??ğœ– , and uses our sketching algorithm in Algorithm 1 to fill holes ??ğ‘ . The third
kind of hole is the hole ??ğ‘’ in the component specification | Ë†ğ‘“ (ğ‘¥) âˆ’ ğ‘¦âˆ—| â‰¤ ??ğ‘’ for predictfloat, which
indicates the magnitude of error allowed by the prediction of that component. As with ??ğœ– holes,
the ??ğ‘’ holes are filled by our algorithm before our sketching algorithm is applied. Intuitively, ??ğœ–
(resp., ??ğ‘’ ) holes must be filled in a way that satisfies the overall ğœ– failure probability guarantee
(resp., ğ‘’ error guarantee) in the user-provided task specification Ëœğœ“ .

5.2 Synthesis Algorithm
Our algorithm (Algorithm 2) takes as input labeled training examples (cid:174)ğ›¼, a task specification
Ëœğœ“ = (ğœ“, ğœ–, ğ‘’), and ğ›¿ âˆˆ R>0, and returns a program Â¯ğ‘ƒ that satisfies Ëœğœ“ with probability â‰¥ 1 âˆ’ ğ›¿.

Step 1: Syntax-guided synthesis. Our algorithm first synthesizes a partial sketch Ëœğ‘ƒ in our DSL
whose train semantics satisfies ğœ“ â€”i.e., UNSATğ›¼,ğ‘¦ (cid:0)ğ‘¦ =
âˆ—
ğ›¼
is well-defined even though there are holes in Ëœğ‘ƒ. We can compute Ëœğ‘ƒ using any standard synthesizer.
(cid:75)
Step 2: Sketching ğœ– and ğ‘’. Next, our algorithm fills the holes ??ğœ– in Ëœğ‘ƒ with values (cid:174)ğœ– and holes ??ğ‘’
with values (cid:174)ğ‘’ to obtain a sketch ğ‘ƒ = Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’). Since ğ‘ƒ only has holes ??ğœ– , we can use Algorithm 1
to fill these holes in a way that guarantees correctness for the given values (cid:174)ğœ– and (cid:174)ğ‘’â€”i.e.,

ğ›¼ âˆ§ Â¬ğœ“ (ğ›¼, ğ‘¦)(cid:1). Importantly, note that
âˆ—
(cid:75)

Ëœğ‘ƒ
(cid:74)

Ëœğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ | â‰¤ ğ‘’(cid:1) â‰¥ 1 âˆ’ ğœ–,
âˆ—
(7)
ğ‘ƒ
(cid:75)
(cid:74)
where Â¯ğ‘ƒ is a completion of ğ‘ƒ where the holes ??ğ‘ in ğ‘ƒ have been filled with values (cid:174)ğ‘. We need to
use Â¯ğ‘ƒ since the test semantics are not well-defined for sketches ğ‘ƒ. In particular, we need to choose
values (cid:174)ğœ– and (cid:174)ğ‘’ that ensure that (7) holds for all possible completions Â¯ğ‘ƒ of ğ‘ƒ.

ğ›¼ âˆ’
(cid:75)

Pğ‘ (ğ›¼)

(cid:0)|

Furthermore, we not only want to choose (cid:174)ğœ– and (cid:174)ğ‘’ to ensure correctness, but also to maximize
a quantitative property of Â¯ğ‘ƒ. In particular, we want to choose it in a way that maximizes the
probability that ğ‘ƒ does not return âˆ…â€”i.e., maximize the score
ğ›¼ â‰  âˆ…(cid:1)
(cid:75)

Note that the score depends critically on the choice of thresholds (cid:174)ğ‘ used to fill holes ??ğ‘ in ğ‘ƒ. Thus,
given a set of candidate choices (cid:174)ğœ– and (cid:174)ğ‘’, our algorithm constructs the corresponding sketch ğ‘ƒ â€² =
Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’), uses our sketching algorithm to fill the holes ??ğ‘ in ğ‘ƒ â€² to obtain Â¯ğ‘ƒ â€² = Sketch(ğ‘ƒ â€², (cid:174)ğ›¼, ğ›¿),
and finally scores Â¯ğ‘ƒ â€². Then, our algorithm chooses ğ‘ƒ â€² with the highest score. In Algorithm 2, we let
FillAll( Ëœğ‘ƒ, ğœ–, ğ‘’) denote the set of all sketches ğ‘ƒ â€² constructed from candidates (cid:174)ğœ– and (cid:174)ğ‘’.

Score(ğ‘ƒ) = Pğ‘ (ğ›¼)

Â¯ğ‘ƒ
(cid:74)

(cid:0)

One important detail is that Algorithm 1 requires that ğ‘ƒ is a straight-line programâ€”i.e., it cannot
handle loops. For now, we assume that we are given a bound ğ‘ âˆˆ N on the maximum length of
any input list. Then, we can unroll list operations such as map, filter, and fold into straight-line
code. Algorithm 2 uses this strategy to apply Algorithm 1 to sketches ğ‘ƒ. We describe how we can
remove the assumption that we have an upper bound ğ‘ in Section 6.

Step 3: Sketching ğ‘. Finally, we use Algorithm 1 to choose values (cid:174)ğ‘ to fill holes ??ğ‘ in the highest
scoring sketch ğ‘ƒ from the previous step, and return the result Â¯ğ‘ƒ = Sketch(ğ‘ƒ, (cid:174)ğ›¼sketch, ğ›¿). Importantly,
in the previous step, ğ‘ƒ is chosen based on a subset (cid:174)ğ›¼synth of the training examples (cid:174)ğ›¼, whereas in
this step, Â¯ğ‘ƒ is constructed based on a disjoint subset (cid:174)ğ›¼sketch. We choose these two subsets to be of
equal size since Algorithm 1 is sensitive to the number of examples in (cid:174)ğ›¼. This strategy ensures that
ğ‘ƒ does not depend on the random variable (cid:174)ğ›¼sketch, thereby ensuring that Theorem 4.7 holds.

5.3 Search Space Over (cid:174)ğœ– and (cid:174)ğ‘’
Here, we describe how we choose candidates (cid:174)ğœ– and (cid:174)ğ‘’ in Step 2 so that the candidate sketches
ğ‘ƒ â€² = Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’) satisfy (7). At a high level, for (cid:174)ğœ–, for each component ğ‘“ of Ëœğ‘ƒ with an ??ğœ– hole,

, Vol. 1, No. 1, Article . Publication date: October 2021.

14

(cid:74)

(ğ¹ ğ¿)

(cid:74)
(fold ğ¹ ğ¿ ğµ)

(map ğ¹ ğ¿)

(cid:74)
(filter ğ¹ ğ¿)

(cid:74)
(slice ğ¿ ğ¼1 ğ¼2)

(cid:74)

(cid:74)

(length ğ¿)

ğ‘“ â€²

(cid:74)
inputğ‘–
ğœ
(cid:74)

(cid:75)

#
ğ‘“

ğ¿
(cid:74)
(cid:75)
#
ğ‘“ +
(cid:75)
#
ğ‘“ +
(cid:75)
#
ğ‘“ +
ğ¼1
(cid:74)

#
#
ğ¹
ğ‘“ +
ğ‘“ =
(cid:75)
(cid:74)
(cid:75)
#
ğ‘“ = ğ‘ Â·
ğ¹
(cid:75)
(cid:74)
#
ğ‘“ = ğ‘ Â·
ğ¹
(cid:75)
(cid:74)
#
ğ‘“ = ğ‘ Â·
ğ¹
(cid:75)
(cid:74)
(cid:75)
#
#
ğ¿
ğ‘“ +
ğ‘“ =
(cid:75)
(cid:74)
(cid:75)
#
#
ğ¿
ğ‘“ =
ğ‘“
(cid:75)
(cid:75)
(cid:74)
#
ğ‘“ = 1(ğ‘“ â€² = ğ‘“ )
(cid:75)
#
ğ‘“ = 0

(cid:75)

#
ğ‘“

ğµ
(cid:74)

(cid:75)

#
ğ‘“ +
(cid:75)
#
ğ‘“

ğ¿
(cid:74)
ğ¿
(cid:74)
ğ¿
(cid:74)
(cid:75)
#
ğ‘“ +

(cid:75)
#
ğ‘“

ğ¼2
(cid:74)

#
ğ‘“

(cid:75)

Bastani

err)ğ‘› (
(cid:75)

ğ¿
(cid:74)

err,
(cid:75)

ğµ
(cid:74)

(cid:75)

err)

err)
(cid:75)
ğ¹
(
(cid:74)
err)
(cid:75)

(ğ¹ ğ¿)
(cid:74)
(fold ğ¹ ğ¿ ğµ)

(cid:74)

(map ğ¹ ğ¿)
(cid:74)
(filter ğ¹ ğ¿)
(cid:74)
(slice ğ¿ ğ¼1 ğ¼2)
(length ğ¿)

(cid:74)

(cid:74)

(cid:75)

ğ‘›âˆˆ{0,1,...,ğ‘ }
err (
ğ¿
ğ¹
(cid:74)
(cid:75)
err

err (
err =
ğ¿
ğ¹
(cid:75)
(cid:74)
(cid:74)
err = max
(cid:75)
err =
(cid:74)
(cid:75)
err =
ğ¿
(cid:74)
(cid:75)
err =
ğ¿
(cid:75)
(cid:74)
err = 0
(cid:75)

(cid:75)
err

(cid:75)

ğœ†ğœ‚.ğ‘’ğ‘“
ğœ†ğœ‚.ğœ†ğœ‚â€².ğœ‚ + ğœ‚â€²
ğœ†ğœ‚.ğœ‚

if ğ‘“ = predictfloat
if ğ‘“ âˆˆ {+, âˆ’}
otherwise

ğ‘“

(cid:74)

inputğ‘–
ğœ
(cid:74)

err =
(cid:75)

ï£±ï£´ï£´ï£²
ï£´ï£´
ï£³
err = 0
(cid:75)

err for
Fig. 4. Rules Algorithm 2 uses to compute the search space over (cid:174)ğœ– (left) and (cid:174)ğ‘’ (right). In the rule of
fold, ğ‘“ ğ‘› (â„“, ğ‘) = ğ‘“ (â„“, ğ‘“ ğ‘›âˆ’1 (â„“, ğ‘)) (and ğ‘“ 0 (â„“, ğ‘) = ğ‘) is the function ğ‘“ iterated ğ‘› times in its second argument.
(cid:75)
The definitions of ğœ‚, ğœ‚ â€², and ğœ‚ + ğœ‚ â€² in the rule for

(cid:74)

ğ‘“

Â·

err are given in Section 5.3.
(cid:75)

(cid:74)

Ëœğ‘ƒ
(cid:74)

(cid:75)

#
ğ‘“

, which is the number of times ğ‘“ occurs in the unrolled version of Ëœğ‘ƒ; then, we
we compute
consider (cid:174)ğœ– = (ğœ–ğ‘“1, ..., ğœ–ğ‘“ğ‘‘ ) such that (cid:205)ğ‘“ ğœ–ğ‘“ â‰¤ ğœ–. For (cid:174)ğ‘’, for each component ğ‘“ of Ëœğ‘ƒ with an ??ğ‘’ hole,
err : (cid:174)ğ‘’ â†¦â†’ ğ‘’ â€², which is a linear function mapping (cid:174)ğ‘’ to an upper bound ğ‘’ â€² on the error
we compute
(cid:75)
of the output; then, we consider (cid:174)ğ‘’ such that

err((cid:174)ğ‘’) â‰¤ ğ‘’. We provide details below.

Ëœğ‘ƒ
(cid:74)
Ëœğ‘ƒ
(cid:74)

(cid:75)

Search space over (cid:174)ğœ–. First, we describe our search space over parameter values (cid:174)ğœ– used to fill holes
??ğœ– so that the overall failure rate is at most ğœ–. Note that here, (cid:174)ğœ– = (ğœ–ğ‘“1, ..., ğœ–ğ‘“ğ‘˜ ), where F Ëœğ‘ƒ = {ğ‘“1, ..., ğ‘“ğ‘˜ }
are subexpressions of Ëœğ‘ƒ of the form predictint, predictfloat, cond-flip, cond-â‰¤, or cond-â‰¥, since each
of these subexpressions contains exactly one hole of the form ??ğœ– .

Intuitively, we can ensure correctness via a union boundâ€”i.e., if the sum of the ğœ–ğ‘“ is bounded by ğœ–,
then the overall failure probability is also bounded by ğœ–. The key caveat is that to apply Algorithm 1,
we need to unroll the sketch ğ‘ƒ = Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’). Thus, we need to count a value ğœ–ğ‘“ multiple times if
the corresponding subexpression ğ‘“ occurs multiple times in the unrolled version of ğ‘ƒ.

(cid:75)

ğ‘ƒ
(cid:74)

In particular, the rules

#
ğ‘“ â€² shown in Figure 4 are designed to count the number of occurrences
of the subexpression ğ‘“ â€² in the unrolled version of ğ‘ƒ. Note that in these rules, ğ‘“ â€² refers to a specific
subexpression, and 1(ğ‘“ = ğ‘“ â€²) refers to whether ğ‘“ is that specific subexpression; multiple uses of
the same construct (e.g., a program with two uses of predictint) are counted separately. These rules
are straightforward; for instance, when unrolling the fold operator, the expressions for the list ğ¿
and the initial value ğµ are included exactly once, whereas the function expression ğ¹ occurs ğ‘ times.
Then, to ensure that the failure probability is at most ğœ–, it suffices for (cid:174)ğœ– to satisfy

âˆ‘ï¸

ğ‘“ âˆˆ F Ëœğ‘ƒ

Ëœğ‘ƒ
(cid:74)

(cid:75)

#
ğ‘“ Â· ğœ–ğ‘“ â‰¤ ğœ–.

(8)

Now, let Î”F Ëœğ‘ƒ = { (cid:174)ğ‘¥ âˆˆ R| F Ëœğ‘ƒ | | âˆ€ğ‘“ . 0 â‰¤ ğ‘¥ ğ‘“ â‰¤ 1 âˆ§ (cid:205)ğ‘“ âˆˆ F Ëœğ‘ƒ
given any (cid:174)ğ‘¥ âˆˆ Î”F Ëœğ‘ƒ
finite set of points from Î”F Ëœğ‘

Ëœğ‘ƒ
, letting ğœ–ğ‘“ = ğ‘¥ ğ‘“ Â· ğœ–/
(cid:74)

#
ğ‘“

(cid:75)

, and construct the corresponding set of values (cid:174)ğœ–.

ğ‘¥ ğ‘“ = 1} be the regular simplex in R | F Ëœğ‘ƒ |. Now,
, then (8) is satisfied. In our algorithm, we search over a

In Figure 6, the rule for filter applies ğ‘“1 =cond-â‰¤ and ğ‘“2 = predictint each ğ‘ = 3 times (where
= 3. Similarly, map applies
= 3. As an example of a point in our search space,

ğ‘ is the given bound on the list length), so we have
ğ‘“3 = predictfloat a total of ğ‘ = 3 times, so
taking (cid:174)ğ‘¥ = (1/3, 1/3, 1/3) yields (cid:174)ğœ– = (1/9, 1/9, 1/9).

Ëœğ‘ƒex
(cid:74)

Ëœğ‘ƒex
(cid:74)

Ëœğ‘ƒex
(cid:74)

#
ğ‘“2

#
ğ‘“1

#
ğ‘“1

=

(cid:75)

(cid:75)

(cid:75)

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

15

Search space over (cid:174)ğ‘’. Next, we describe our search space over parameter values (cid:174)ğ‘’ used to fill holes
??ğ‘’ so the overall error is at most ğ‘’. Similar to before, (cid:174)ğ‘’ = (ğ‘’ğ‘“1, ..., ğ‘’ğ‘“â„ ), but this time G Ëœğ‘ƒ = {ğ‘“1, ..., ğ‘“â„ }
are subexpressions of Ëœğ‘ƒ of the form predictfloat, which each contain exactly one hole of the form
??ğ‘’ . In this case, we define an analysis that bounds the overall error of the output of Â¯ğ‘ƒ = Fill(ğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’)
for any (cid:174)ğœ– as a function of (cid:174)ğ‘’. More precisely,
Fill(ğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’)
(cid:74)

ğ‘ƒ
(cid:74)
for all (cid:174)ğœ– and (cid:174)ğ‘’, and for all ğ›¼ such that all component specifications in Fill(ğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’ hold for (cid:174)ğ›¼. In other
words, (9) bounds the error of the output for examples ğ›¼ such that predictions fall within the desired
error bounds (failures happen with probability at most ğœ– according to our choices of (cid:174)ğœ–).

err satisfies the following property:
err ((cid:174)ğ‘’)

ğ‘ƒ
(cid:74)
(cid:75)
ğ›¼ âˆ’
(cid:75)

(cid:13)
(cid:13)âˆ â‰¤

ğ‘ƒ
(cid:74)

âˆ—
ğ›¼
(cid:75)

(9)

(cid:13)
(cid:13)

(cid:75)

Note that (9) uses the ğ¿âˆ norm. For scalar outputs, we have âˆ¥ğ‘¥ âˆ’ ğ‘¥ â€²âˆ¥âˆ = |ğ‘¥ âˆ’ ğ‘¥ â€²|. For list outputs,
for the ğ¿âˆ norm to be well-defined, we need to ensure that ğ‘¥ =
ğ›¼ are of
âˆ—
(cid:75)
the same length (at least, when all component specifications are satisfied). In particular, the only
potential case where ğ‘¥ and ğ‘¥ â€² have unequal lengths is if Â¯ğ‘ƒ contains a filter operator. We focus on
filtering real-valued lists; filtering integer-valued lists is similar (and there are no operations to
filter list-valued lists or image-valued lists). In the real-valued case, the filter function must be either
cond-â‰¤ and cond-â‰¥. Assuming the component specifications on cond-â‰¤ and cond-â‰¥ are satisfied,
then their (Boolean) outputs are guaranteed to be equal, so the outputs of the filter operator have
equal length under train and test semantics. Thus, âˆ¥ğ‘¥ âˆ’ ğ‘¥ â€²âˆ¥âˆ is well-defined.

ğ›¼ and ğ‘¥ â€² =
(cid:75)

Fill(ğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’)

Â¯ğ‘ƒ
(cid:74)

(cid:74)

Given

ğ‘ƒ
(cid:74)

err, our goal is to compute (cid:174)ğ‘’ satisfying
(cid:75)

err((cid:174)ğ‘’) â‰¤ ğ‘’.

ğ‘ƒ
(cid:74)

(cid:75)

(cid:75)

(cid:75)

Ëœğ‘ƒ
(cid:74)

ğ‘ğ‘“ Â· ğ‘’ğ‘“ . In Figure 3, we have
ğ‘ƒ
(cid:74)

As with (cid:174)ğœ–, we can construct a candidate (cid:174)ğ‘’ for any point in ğ‘¥ âˆˆ Î”G Ëœğ‘ƒ
by taking ğ‘’ğ‘“ = ğ‘¥ ğ‘“ Â· ğ‘’/ğ‘ğ‘“ , where
Ëœğ‘ƒ
err = 3 Â· ğ‘’ğ‘“3, so there is a single candidate ğ‘’ğ‘“3 = ğ‘’/3.
err = (cid:205)ğ‘“ âˆˆ G Ëœğ‘ƒ
(cid:74)
(cid:75)
err, which are shown in Figure 4 (right). They compute an symbolic
Next, we describe the rules
expression of the form ğœ‚ = (cid:205)ğ‘“ âˆˆ G Ëœğ‘ƒ
ğ‘ğ‘“ Â· ğ‘’ğ‘“ âˆˆ E Ëœğ‘ƒ , where ğ‘ğ‘“ âˆˆ Râ‰¥0 and ğ‘’ğ‘“ is a symbol. Given (cid:174)ğ‘’, an
expression ğœ‚ can be evaluated by substituting (cid:174)ğ‘’ for the symbols ğ‘’ğ‘“ in ğœ‚. Now, the rule for function
err is the
application assumes given a function abstraction
identity function except for predictfloat, +, and âˆ’. The case predictfloat follows since we have assumed
that the component specification holes, and the component specification for ğ‘“ = predictfloat says
Â¯ğ‘“
ğ‘ğ‘“ Â·ğ‘’ğ‘“ and
exactly that |
ğ›¼ âˆ’
(cid:74)
(cid:75)
ğœ‚ â€² = (cid:205)ğ‘“ âˆˆ G Ëœğ‘ƒ
ğ‘“ Â· ğ‘’ğ‘“ , we define ğœ‚ + ğœ‚ â€² = (cid:205)ğ‘“ âˆˆ G Ëœğ‘ƒ
ğ‘“ ) Â· ğ‘’ğ‘“ . The rule for map follows since we are
(ğ‘ğ‘“ + ğ‘â€²
ğ‘â€²
using the ğ¿âˆ norm, so the bound is applied elementwise. The remaining rules are straightforward.
In Figure 3, the rule for predictfloat returns ğ‘’ğ‘“3, so the rule for map returns 3 Â· ğ‘’ğ‘“3 (since the given
err = 3 Â· ğ‘’ğ‘“3 .

ğ›¼ | â‰¤ ğ‘’ğ‘“ for any completion Â¯ğ‘“ of ğ‘“ . For + and âˆ’, letting ğœ‚ = (cid:205)ğ‘“ âˆˆ G Ëœğ‘ƒ
âˆ—
(cid:75)

bound on the list length is ğ‘ = 3). The remaining rules propagate this value, so

err : E Ëœğ‘ƒ â†’ E Ëœğ‘ƒ . In particular,
(cid:75)

ğ¹

ğ¹

(cid:75)

(cid:74)

(cid:74)

(cid:74)

ğ‘“

err is a linear function follows by structural induction. Additional compo-

(cid:75)

Finally, the fact that

Â·

Ëœğ‘ƒex
(cid:74)

nents (e.g., multiplication) can result in nonlinear expressions, but a similar approach applies.

(cid:74)

(cid:75)

Overall search space. Our overall search space consists of pairs (cid:174)ğœ– and (cid:174)ğ‘’ such that (cid:174)ğœ– satisfies (8)
and (cid:174)ğ‘’ satisfies (10); given such a pair, FillAll( Ëœğ‘ƒ, ğœ–, ğ‘’) includes the program ğ‘ƒ = Fill( Ëœğ‘ƒ, (cid:174)ğœ–, (cid:174)ğ‘’). Together,
(8) and (10) ensure the desired property (7). In particular, for any completion Â¯ğ‘ƒ of ğ‘ƒ, (10) ensures
that |
ğ›¼ | â‰¤ ğ‘’ as long as ğ›¼ satisfies all the component specifications, and (8) ensures that ğ›¼
âˆ—
(cid:75)
satisfies the component specifications with probability at least 1 âˆ’ ğœ– over ğ‘ (ğ›¼).

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

ğ‘ƒ
(cid:74)

6 DISCUSSION

Generality. In Section 5, we described a synthesizer tailored to the language in Figure 2. Our
approach generalizes straightforwardly in several ways. First, we note that the predictint and
predictfloat machine learning components are not specific to images of integers, and represent

, Vol. 1, No. 1, Article . Publication date: October 2021.

(10)

16

Bastani

general classification and regression problems, respectively. Furthermore, we can also include
err.
additional list processing components as long as we provide the abstract semantics
Thus, our algorithm can be viewed as a general algorithm for synthesizing list processing programs
with DNNs for classification and regression, where the specification is that with high probability,
the program should return the either the correct answer (within some given error tolerance) or âˆ….
We can also modify the specification in certain ways; for instance, we can ignore certain kinds
of errors by modifying the annotations on predictint and predictfloat. For instance, to allow for
one-sided errors in regression problems (e.g., it is fine to say â€œpersonâ€ when there isnâ€™t one but
not vice versa), we can simply drop the absolute values from the task specification ğœ“ and from the
annotations on predictfloat. For this case, the algorithm for allocating errors ğ‘’ works as is, but in
general, it may need to be modified to ensure the annotations imply the specification.

# and

(cid:75)

(cid:74)

(cid:74)

(cid:75)

Â·

Â·

Bound on examples. In Section 5, we assumed given a bound ğ‘ on the maximum length of any list
observed during program execution. Intuitively, we can circumvent this assumption by computing
a high probability bound ğ‘ ; the error probability can be included in the user-provided allowable
ğ›¼ denote the maximum list length observed while executing Ëœğ‘ƒ on
error rate ğœ–. In particular, let
(cid:75)
input ğ›¼. Then, suppose we can obtain ğ‘ such that

Ëœğ‘ƒ
(cid:74)

len

Pğ‘ (ğ›¼)

(cid:0)

Ëœğ‘ƒ
(cid:74)

len
ğ›¼ â‰¤ ğ‘ ) â‰¥ 1 âˆ’
(cid:75)

ğœ–
2

.

Now, if we synthesize a completion Â¯ğ‘ƒ of Ëœğ‘ƒ with overall error rate â‰¤ ğœ–/2, then by a union bound,
the total error rate is â‰¤ ğœ–. Finally, to obtain such an ğ‘ , we can use the specification
len
ğ›¼ â‰¤?? {true}â‡’
ğœ–/2.
(cid:75)
len
ğ›¼ â‰¤ ğ‘ with
Letting ğ‘ be the synthesized value used to fill the hole, the specification says that
(cid:75)
probability at least ğœ–/2 according to ğ‘ (ğ›¼), which is exactly the desired condition on ğ‘ ; thus, we
can take ğ‘ = ğ‘. Note that since the specification is true, we can use either | or â‡’.

Ëœğ‘ƒ
(cid:74)

Ëœğ‘ƒ
(cid:74)

7 EVALUATION
We describe our evaluation on synthesizing list processing programs, as well as on two case studies:
(i) a state-of-the-art image classifier, and (ii) a random forest trained to predict Warfarin drug
dosage. In addition, we describe an extension of (i) to object detection in Appendix C.

7.1 Synthesizing List Processing Programs with Image Classification

Experimental setup. We evaluate our synthesis algorithm on our list processing domain in
Section 5. Inputs are lists of MNIST digits [42]. We use a convolutional DNN (two convolutional
layers followed by two fully connected layers, with ReLU activations) [41] to predict the integer in
an image, trained on the MNIST training set; it achieves 99.2% accuracy. We also train a single layer
DNN, which is 4.04Ã— faster but only 98.5% accurate. Finally, for inputs with the flip component,
with consider input images flipped along their horizontal axis. We train a DNN to predict whether
a given image is flipped; it achieves 99.6% accuracy.

For the synthesizer, we use a standard enumerative synthesizer that returns the smallest program
in terms of depth (but chooses arbitrarily among equal depth programs). We give it 5 labeled input-
output examples as a specification ğœ“ , along with the type of the function to be synthesized [23, 50].
For the search space over each (cid:174)ğœ– and (cid:174)ğ‘’, we consider values (cid:174)ğ‘¥0 âˆˆ {1, 3, 5}ğ‘‘ , where ğ‘‘ = |F Ëœğ‘ƒ | or
ğ‘‘ = |G Ëœğ‘ƒ |, and then take (cid:174)ğ‘¥ = (cid:174)ğ‘¥0/âˆ¥ (cid:174)ğ‘¥0âˆ¥1 to normalize it to Î”ğ‘‘ . We also compare to (i) a baseline
â€œNo Searchâ€, which only considers a single (cid:174)ğ‘¥0 = (1, ..., 1), and (ii) a baseline â€œğ‘˜ = 0â€, which uses
a variant of our generalization bound that uses either ğ‘˜ = 0 (or ğ‘˜ = âˆ…, if there are insufficient
samples); this strategy captures the guarantees provided by traditional generalization bounds from

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

17

DSL Variant

Task

StatCoder

âˆ… Rate

No Search

Failure Rate

StatCoder

No Search

int

sum ğ‘¥ âˆˆ â„“
max ğ‘¥ âˆˆ â„“
sum ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜
max first ğ‘˜ elements ğ‘¥ âˆˆ â„“
count ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜

average

â€“

float

sum ğ‘¥ âˆˆ â„“
max ğ‘¥ âˆˆ â„“
sum ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜
max first ğ‘˜ elements ğ‘¥ âˆˆ â„“
count ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜

average

â€“

flip

sum ğ‘¥ âˆˆ â„“
max ğ‘¥ âˆˆ â„“
sum ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜
max first ğ‘˜ elements ğ‘¥ âˆˆ â„“
count ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜

average

â€“

fast

average

overall

sum ğ‘¥ âˆˆ â„“
max ğ‘¥ âˆˆ â„“
sum ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜
max first ğ‘˜ elements ğ‘¥ âˆˆ â„“
count ğ‘¥ âˆˆ â„“ that are â‰¤ ğ‘˜

â€“

â€“

0.000
0.000
0.001
0.000
0.001

0.000

0.000
0.000
0.000
0.000
0.000

0.000

0.015
0.015
0.025
0.063
0.025

0.029

0.033
0.033
0.039
0.035
0.039

0.036

0.016

ğ‘˜ = 0

0.177
0.177
0.206
0.195
0.206

0.000
0.000
0.022
0.008
0.022

0.010

0.192

0.000
0.000
1.000
0.005
1.000

0.000
0.000
1.000
0.177
1.000

0.401

0.435

0.016
0.016
0.085
0.046
0.085

0.230
0.230
0.265
0.258
0.265

0.050

0.250

0.033
0.033
0.127
0.061
0.127

0.706
0.706
0.755
1.000
0.755

0.076

0.784

0.134

0.415

ğ‘˜ = 0

0.001
0.001
0.001
0.000
0.000

0.018
0.008
0.010
0.007
0.000

0.009

0.001

0.001
0.000
0.000
0.000
0.000

0.001
0.000
0.000
0.000
0.000

0.000

0.000

0.012
0.006
0.004
0.004
0.000

0.001
0.001
0.001
0.000
0.000

0.005

0.001

0.026
0.008
0.005
0.007
0.000

0.000
0.000
0.000
0.000
0.000

0.009

0.000

0.006

0.000

0.018
0.008
0.016
0.007
0.000

0.010

0.001
0.000
0.010
0.000
0.000

0.002

0.012
0.006
0.012
0.005
0.000

0.007

0.026
0.008
0.023
0.010
0.000

0.013

0.008

Table 1. We show results on synthesizing list processing programs, for both our approach (StatCoder) and
the baseline that does not search over (cid:174)ğœ– and (cid:174)ğ‘’ (â€œNo Searchâ€). For each DSL variant and each task, we show
the â€œâˆ… Rateâ€ Pğ‘ (ğ›¼) (

Â¯ğ‘ƒ
(cid:74)

ğ›¼ â‰  âˆ… âˆ§ |
(cid:75)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

âˆ—
ğ›¼ = âˆ…|) > ğ‘’.
(cid:75)

ğ›¼ = âˆ…), and the â€œFailure Rateâ€ Pğ‘ (ğ›¼) (
(cid:75)

Â¯ğ‘ƒ
(cid:74)

statistical learning theory [28, 34, 71]. We use our algorithm with parameters ğœ– = ğ›¿ = 0.05, ğ‘’ = 6,
and ğ‘ = 3. We use 2500 MNIST test set images for each ğ›¼synth and ğ›¼sketch, and the remaining 5000
for evaluation. Next, we consider four variants of our DSL:

â€¢ Int: Restrict to components with integer type and omit the cond-flip component
â€¢ Float: Same as â€œintâ€, but include components with real types
â€¢ Flip: Same as â€œintâ€, but include the flip component
â€¢ Fast: Same as â€œintâ€, but use the fast neural network.

For each variant, we consider five list processing tasks, which are designed to exercise different
kinds of components. These programs all take as input a list â„“ of images ğ‘¥ âˆˆ â„“; in addition, several
of them take as input a second image ğ‘˜ that encodes some information relevant to task. Then, they
output an integer or real value (as specified by ğœ“ ). The tasks are shared across the different DSL
variants, but specific programs change based on the available components.

Results. We show results in Table 1. For the program Â¯ğ‘ƒ synthesized using each our approach

StatCoder and our baseline that does not search over (cid:174)ğœ– and (cid:174)ğ‘’, we show the following metrics:

â€¢ âˆ… Rate: The rate at which Â¯ğ‘ƒ returns âˆ…â€”i.e., Pğ‘ (ğ›¼) (
â€¢ Failure Rate: The rate at which Â¯ğ‘ƒ makes mistakesâ€”i.e.,
(cid:0)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ = âˆ…).
(cid:75)

Pğ‘ (ğ›¼)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ â‰  âˆ… âˆ§ |
(cid:75)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

ğ›¼ = âˆ…|(cid:1) > ğ‘’.
âˆ—
(cid:75)

, Vol. 1, No. 1, Article . Publication date: October 2021.

18

Bastani

(a)

(b)

(c)

Fig. 5. For list processing programs, we show âˆ… rate (black) and failure rate (red) as a function of (a) ğœ–, (b) ğ›¿,
and (c) ğ‘’, on average for (a,b) â€œIntâ€ programs and (c) â€œFloatâ€ programs. Defaults are ğœ– = ğ›¿ = 0.05 and ğ‘’ = 6.0.

Task

StatCoder

âˆ… Rate

No Search

Failure Rate

StatCoder

No Search

count the number of people in ğ‘¥
check if ğ‘¥ contains a person
count people near the center of ğ‘¥
find people near a car
minimum distance from a person to the center of ğ‘¥

average

0.054
0.054
0.290
0.901
0.149

0.290

ğ‘˜ = 0

0.901
0.901
0.901
1.000
0.901

0.054
0.054
0.290
0.901
0.149

ğ‘˜ = 0

0.003
0.003
0.003
0.000
0.000

0.124
0.124
0.032
0.003
0.023

0.124
0.124
0.032
0.003
0.023

0.061

0.290

0.921

0.061

0.002

Table 2. We show results on synthesizing list processing programs over object detection, for our approach
StatCoder. For each DSL variant and each task, we show the â€œâˆ… Rateâ€ Pğ‘ (ğ›¼) (
ğ›¼ = âˆ…), and the â€œFailure
(cid:75)
Rateâ€ Pğ‘ (ğ›¼) (

âˆ—
ğ›¼ = âˆ…|) > ğ‘’. Parameters are ğœ– = ğ›¿ = 0.2 and ğ‘’ = 20.0.
(cid:75)

ğ›¼ â‰  âˆ… âˆ§ |
(cid:75)

ğ›¼ âˆ’
(cid:75)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

Â¯ğ‘ƒ
(cid:74)

As can be seen, both StatCoder and the baseline always achieve the desired failure rate bound of
ğœ– = 0.05. Furthermore, by searching over candidates (cid:174)ğœ– and (cid:174)ğ‘’, StatCoder substantially outperforms
the baseline, achieving an 8Ã— reduction in âˆ… rate on average. For simpler programs (i.e., sum and
max), the two perform similarly since there is only a single hole, so the search space only contains
one candidate. However, for larger programs, the search improves performance by up to an order of
magnitude. There is a single case where the baseline performs better (the fourth program in the â€œflipâ€
DSL), due to random chance since the dataset ğ›¼sketch used to synthesize the final program Â¯ğ‘ƒ from Ëœğ‘ƒ
differs from the dataset ğ›¼synth used to choose (cid:174)ğœ– and (cid:174)ğ‘’. StatCoder outperforms the â€œğ‘˜ = 0â€ baseline
by an even larger margin, due to the fact that the generalization bound is overly conservative;
these results demonstrate the importance of using a generalization bound specialized to our setting
rather than a more traditional generalization bound that minimizes the empirical risk.

Next, in Figure 5, we show how these results vary as a function of the specification parameters ğœ–,
ğ›¿, and ğ‘’. As can be seen, ğœ– has the largest effect on âˆ… and failure rates, followed by ğ‘’; as expected, ğ›¿
has almost no effect since the dependence of our bound on ğ›¿ is logarithmic.

Finally, we note that the failure rates for the â€œfastâ€ DSL are very low. Thus, we could use our
technique to chain together the fast program with the slow one, along the same lines as discussed
in our case study in Section 7.3; we estimate that doing so results in a 3Ã— speedup on average.

7.2 Synthesizing List Processing Programs with Object Detection

Experimental setup. Next, we consider synthesizing programs that operate over the predictions
made by a state-of-the-art DNN for object detection. We assume given a DNN component Ë†ğ‘“ that
given an image ğ‘¥, is designed to detect people and cars in ğ‘¥. We use a pretrained state-of-the-art
object detector called Faster R-CNN [56] available in PyTorch [53], tailored to the COCO dataset [45],

, Vol. 1, No. 1, Article . Publication date: October 2021.

0.0000.0010.0020.00300.0050.010.0150.050.10.150.2failure rateâˆ…rateğœ€0.0000.0010.0020.0030.0000.0050.0100.0150.050.10.150.2failure rateâˆ…rateğ›¿0.0000.0010.0020.0030.0000.0050.0100.0153.06.09.012.0failure rateâˆ…rateeSynthesizing Machine Learning Programs

19

which is a dataset of real-world images containing people, cars, and other objects. There are multiple
variants of Faster R-CNN; we use the most accurate one, X101-FPN with 3Ã— learning rate schedule.
We represent this DNN as a component Ë†ğ‘“ : X â†’ Y = Dâˆ— Ã— R, where Ë†ğ‘“ (ğ‘¥) = ( Ë†ğ‘¦ (ğ‘¥), Ë†ğ‘ (ğ‘¥))
consists of a list of detections ğ‘‘ âˆˆ Ë†ğ‘¦ (ğ‘¥) along with a correctness score Ë†ğ‘ (ğ‘¥) that the prediction is
correct. Each detection ğ‘‘ âˆˆ D = R2 Ã— Z is itself a tuple ğ‘‘ = (ğ‘, ğ‘§) including the position ğ‘ and
predicted category of the object. The ground truth label ğ‘¦âˆ— for an image ğ‘¥ is a list of detections
ğ‘‘ âˆˆ ğ‘¦âˆ—. In general, we cannot expect to get a perfect match between the predicted bounding boxes
and the ground truth ones. Typically, two detections ğ‘‘, ğ‘‘ âˆ— match, denoted âˆ¥ğ‘‘ âˆ’ ğ‘‘ âˆ—âˆ¥ â‰¤ ğ‘’, where ğ‘’ is a
specified error tolerance, if the distance between their centers satisfies âˆ¥ğ‘ âˆ’ ğ‘âˆ—âˆ¥âˆ â‰¤ ğ‘’. Furthermore,
we write âˆ¥ Ë†ğ‘¦ (ğ‘¥) âˆ’ ğ‘¦âˆ—âˆ¥ â‰¤ ğ‘’ if | Ë†ğ‘¦ (ğ‘¥)| = |ğ‘¦âˆ—| and there exists a one-to-one correspondence between
ğ‘‘ âˆˆ Ë†ğ‘“ (ğ‘¥) and ğ‘‘ âˆ— âˆˆ ğ‘¦âˆ— such that âˆ¥ğ‘‘ âˆ’ ğ‘‘ âˆ— âˆ¥ â‰¤ ğ‘’. Then, we define predict : X â†’ (Y âˆª âˆ…) by

(predict ğ‘¥) = (if Ë†ğ‘ (ğ‘¥) â‰¥??ğ‘ {âˆ¥ Ë†ğ‘¦ (ğ‘¥) âˆ’ ğ‘¦âˆ— âˆ¥ â‰¤??ğ‘’ }â‡’
??ğœ–

then Ë†ğ‘¦ (ğ‘¥) else âˆ…).

In other words, the specification says that a correct prediction is if the error tolerance is below a
level ??ğ‘’ to be specified. Thus, given ğ‘’ and ğœ– to fill ??ğ‘’ and ??ğœ– , respectively, our sketching algorithm
synthesizes a threshold ğ‘ to fill ??ğ‘ in a way that guarantees that this specification holds. Then,
predict returns Ë†ğ‘¦ (ğ‘¥) if the DNN is sufficiently confident in its prediction, and âˆ… otherwise.

We can use this component in conjunction with our synthesis algorithm in the same way that it

uses predictfloat. In particular, we define the abstract semantics
err = ğœ†ğœ‚.ğ‘’predict.

(predict ğ‘¥)

(cid:74)

(cid:75)

These semantics enable it to select the error tolerance ğ‘’ to fill ??ğ‘’ . The remainder of the synthesis
algorithm proceeds as in Section 7.1. We use parameters ğœ– = ğ›¿ = 0.2, ğ‘’ = 20.0, and ğ‘ = 3, and
use ğ‘› = 1000 COCO validation set images for each ğ›¼synth and ğ›¼sketch and the remaining 1503 for
evaluation. We use larger ğœ– and ğ›¿ since the accuracy of the object detector is significantly lower
than that of the image classifier, so the âˆ… rates are very high for smaller choices.

(cid:75)

ğ¿â€²
(cid:74)

We evaluate our approach on synthesizing five programs, which include additional list processing
and
, (ii) (compose ğ‘“ ğ‘“ â€²), which returns the composition ğœ†ğ‘¥ .ğ‘“ (ğ‘“ â€²(ğ‘¥)), (iii) (isğ‘§â€² ğ·), which
= (ğ‘, ğ‘§) is a detection and ğ‘§ â€² âˆˆ Z is an object category, and (iv)
= (ğ‘, ğ‘§) and
ğ·
#, they each evaluate each of
(cid:74)

components: (i) (product ğ¿ ğ¿â€²), which returns the list of all pairs (ğ‘¥, ğ‘¥ â€²) such that ğ‘¥ âˆˆ
ğ‘¥ â€² âˆˆ
returns 1(ğ‘§ = ğ‘§ â€²), where
(distance ğ· ğ· â€²), which returns the distance âˆ¥ğ‘ âˆ’ ğ‘ â€²âˆ¥âˆ between two detections
ğ· â€²
(cid:74)
their arguments once, and for

= (ğ‘ â€², ğ‘§ â€²). Their abstract semantics are straightforward: for

err, the only one that propagates errors is distance, for which
(cid:75)

ğ·
(cid:74)

ğ¿
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:74)

(cid:75)

(cid:75)

Â·

Â·
(cid:74)
(distance ğ· ğ· â€²)

(cid:74)

err =
(cid:75)

ğ·
(cid:74)

err +
(cid:75)

ğ· â€²
(cid:74)

(cid:75)

err.

Results. We provide results in Table 2. The trends are similar to Section 7.1; the main difference
is that search does not help in this case, likely because there is only a single machine learning
component so optimizing the allocation does not significantly affect performance. Finally, we can
chain these programs with a faster object detector to reduce running time; see Appendix C.

7.3 Case Study 1: ImageNet Image Classification

Correctness. Consider program shown in Figure 6, which classifies images as â€œpersonâ€ (returns
true) or â€œnot personâ€ (returns false). The function is_person takes as input an image ğ‘¥, and
optionally the ground truth label ğ‘¦âˆ— (which is only used during sketching). The specification in
is_person says that the program should return true with high probability if the image is of a
person (i.e., ğ‘¦âˆ— = 1). The predicate 1(1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘) is shown in blue, where the value of ğ‘ has been
left as a hole ??1, the specification ğ‘¦âˆ— = 1 is shown in green in the curly braces, and the value
ğœ– = 0.05 is shown in green in the square braces. We perform a case study in the context of this

, Vol. 1, No. 1, Article . Publication date: October 2021.

20

Bastani

def is_person(x, y_true=None):

if 1.0 - f(x) <= ??1 {y_true} [|, 0.05]:

return True

else:

return False

def is_person_fast(x):

def monitor_correctness(x):

if np.random.uniform() <= 0.99:

return

passert 1.0 - f_fast(x) <= ??2 {is_person(x)} [|, 0.05]

if 1.0 - f_fast(x) <= ??2 {is_person(x)} [|, 0.05]:

def monitor_speed(x):

return is_person(x)

else:

return False

passert 1.0 - f_fast(x) > ??2 {true} [|, ??3]

Fig. 6. A program used to predict whether an image ğ‘¥ contains a person. Specifications are shown in green;
curly brackets is the specification and square brackets is the value of ğœ–. The corresponding inequality with a
hole in blue. Holes with the same number are filled with the same value.

program (though for labels other than â€œpersonâ€). We consider the ImageNet dataset [18], a large
image classification benchmark with over one million images in 1000 categories, including various
different animals and inanimate objects. We consider the ResNet-152 DNN architecture [29], a
state-of-the-art image classification model trained on ImageNet that achieves about 88% accuracy
overall. For both architectures, we use the implementation in PyTorch [53].

To use our system, we split the ImageNet validation set consisting of 50,000 held-out images into
(at most) 25,000 for synthesis (i.e., the synthesis set) and 25,000 for validation. Because ImageNet has
so many labels, each object category has very few examples in the validation dataset (50 on average).
Thus, we group the labels into larger, coarse-grained categories, focusing on ones that correspond
to many fine-grained ImageNet labels. We consider â€œdogâ€ (130 labels, 6,500 images) â€œbirdâ€ (59
labels, 2,950 images), â€œinsectâ€ (27 labels, 1,350 images), â€œcarâ€ (21 labels, 1050 images), â€œsnakeâ€ (17
labels, 850 images), and â€œcatâ€ (13 labels, 650 images). The default one we use is â€œcarâ€; this category
contains vehicles such as passenger cars, bikes, busses, trolleys, etc. For the scoring function, given
a coarse-grained category ğ‘Œ âŠ† Y, we use the sum of the fine-grained label probabilitiesâ€”i.e.,
ğ‘“ (ğ‘¥) = (cid:205)ğ‘¦ âˆˆğ‘Œ ğ‘ (ğ‘¥, ğ‘¦), where ğ‘ (ğ‘¥, ğ‘¦) is the predicted probability of label ğ‘¦ according to ResNet-152.
Then, we use our sketching algorithm to synthesize ğ‘ to fill ??1. We show results in the first
and fourth rows of Figure 7. Note that the red curves ideally equal the blue curves, but are slightly
conservative to account for synthesis being based on finitely many samples. The value of ğœ– has the
biggest effect on performance, since it directly governs recall; as ğœ– grows, recall drops (as desired)
and precision substantially improves. In contrast, the performance does not vary significantly with
ğ›¿. These trends match sample complexity guarantees from learning theory relevant to our setting
of ğ‘› = ğ‘‚ (log(1/ğ›¿)/ğœ–) [34, 71]. Next, as ğ‘› grows larger, recall can more closely match the desired
maximum, allowing precision to improve dramatically (the non-monotone effect is most likely due
to random chance). Finally, the dependence on the target label is also governed by the number of
synthesis images in each category.

Improving speed. Next, we describe how our framework can be used to compose ğ‘“ with a second
DNN ğ‘“fast, which is much faster than ğ‘“ but has lower accuracy. Intuitively, we want to use ğ‘“fast
when we can guarantee its prediction is correct with high probability, and use ğ‘“ otherwise. This
approach has been used to reduce running time [13, 67]; our framework can be used to do so while
providing rigorous accuracy guarantees.

The code for this approach is shown in is_person_fast in Figure 6. As before, the idea is to
compute a threshold ğ‘ â€² such that the prediction ğ‘“fast (ğ‘¥) â‰¥ 1 âˆ’ ğ‘ â€² is correct with high probability.
There are two differences. First, if we conclude that there might be a person in the image according
to ğ‘“fast, then we return the prediction according to ğ‘“ (instead of true). While ğ‘“fast is guaranteed
to detect 95% images with people with high probability, it may have more false positives than ğ‘“ ;
calling ğ‘“ after ğ‘“fast reduces these false positives. Second, the correctness guarantee is with respect

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

21

(c)

(f)

(i)

(a)

(d)

(g)

(b)

(e)

(h)

(j)

Fig. 7. For ResNet alone, we show the recall (red), desired lower bound on the recall (blue), and precision
(black) as a function of (a) ğœ–, (b) ğ›¿, (c) ğ‘¦ âˆˆ Y, and (j) the number of synthesis examples ğ‘›; the defaults are
ğœ– = ğ›¿ = 0.05, ğ‘› = 25, 000, and ğ‘¦ = â€œcarâ€, except in (c) we use ğœ– = 0.1 to facilitate the comparison with (f). We
show the same values for ResNet+AlexNet as a function of (d) ğœ–, (e) ğ›¿, and (f) ğ‘¦ âˆˆ Y. For ResNet+AlexNet
(black) compared to AlexNet alone (green), we show the running time as a function of (g) ğœ–, (h) ğ›¿, and (i)
ğ‘¦ âˆˆ Y; we omit ResNet alone since its running time (82.6 minutes) is significantly above the scale.

to the prediction Ë†ğ‘¦ = 1(ğ‘“ (ğ‘¥) â‰¥ 1 âˆ’ ğ‘) rather than ğ‘¦âˆ—. We could use ğ‘¦âˆ—, but there is no needâ€”if Ë†ğ‘¦ is
incorrect, then it is not helpful for ğ‘“fast to predict correctly since it falls back on Ë†ğ‘¦.

For ğ‘“fast, we use AlexNet, which achieves about 57% accuracy overall; in particular, we use
ğ‘“fast (ğ‘¥) = (cid:205)ğ‘¦ âˆˆğ‘Œ ğ‘fast (ğ‘¥, ğ‘¦), where ğ‘fast (ğ‘¥, ğ‘¦) is the predicted probability of label ğ‘¦ according to
AlexNet. Then, we conclude that ğ‘¥ (may) have label ğ‘¦ if ğ‘“fast (ğ‘¥) â‰¥ 1 âˆ’ ğ‘ â€², where ğ‘ â€² is synthesized
by our algorithm. We obtain results on an Nvidia GeForce RTX 2080 Ti GPU. We show results
on the second and third rows of Figure 7. All results shown are for the combined predictions (i.e.,
using both AlexNet and ResNet), and are estimated on the validation set. For running time, we omit

, Vol. 1, No. 1, Article . Publication date: October 2021.

0.600.700.800.901.000.050.10.150.2Precision/Recallğœ€0.600.700.800.901.000.050.10.150.2Precision/Recallğ›¿0.500.600.700.800.901.00DogBirdInsectCarSnakeCatPrecision/Recall0.600.700.800.901.000.050.10.150.2Precision/Recallğœ€0.600.700.800.901.000.050.10.150.2Precision/Recallğ›¿0.500.600.700.800.901.00DogBirdInsectCarSnakeCatPrecision/Recall0510150.050.10.150.2Time (Minutes)ğœ€0510150.050.10.150.2Time (Minutes)ğ›¿05101520DogBirdInsectCarSnakeCatTime (Minutes)0.200.400.600.801.00500010000150002000025000Precision/Recalln22

Bastani

results for ResNet since its running time is 82.6 minutes, which is more than 4Ã— the running time
of our combined model. For the â€œdogâ€ category, our approach reduces running time 6Ã— from 82.6
minutes to 13.8 minutes without any sacrifice in precision or recall.

Thus, our approach significantly reduces running time while achieving the desired error rate.
Furthermore, comparing to Figure 7 (d), the precision does not significantly decrease across most
labels. It does suffer for the labels â€œcarâ€ and â€œsnakeâ€. Intuitively, for these labels, there are relatively
few examples in the synthesis set, so the synthesis algorithm needs to choose more conservative
thresholds. Since the fast program has two thresholds whereas the original program only has one,
it is more conservative in the latter case. This difference is reflected in the fact that Figure 7 (e) has
higher recall than (d), especially for â€œcarâ€ and â€œsnakeâ€.

Importantly, these results rely on the fact that we are tailoring our predictions to a single
categoryâ€”i.e., our system enables the user to tailor the predictions of pretrained DNN models such
as ResNet and AlexNet to their desired task. For instance, it can focus on predicting cars rather
than achieving good performance on all 1000 ImageNet categories.

Runtime monitoring. As described in Appendix A.3, our framework can monitor the synthesized
program at runtime, which is useful since PAC guarantees are specific to the data distribution
ğ‘ (ğ‘¥, ğ‘¦âˆ—). Thus, if the program is executed on data from a different distribution, called distribution
shift [12, 55], then our guarantees may not hold. Monitoring requires us to obtain ground truth
labels ğ‘¦âˆ— for inputs ğ‘¥ encountered at run time; then, we use these ground truth labels to estimate
the failure rate of the model and ensure it is below the desired value ğœ–.

We show how we can monitor the correctness of is_person_fast. In this case, we can easily ob-
tain ground truth labels since the specification for ??2 can be obtained by evaluating ğ‘“ (ğ‘¥). We want
to avoid running ğ‘“ on every input since this would defeat the purpose of using a fast DNN; instead,
we might run it once every ğ‘ iterations for some large ğ‘ . The function monitor_correctness
implements this check, generating a ground truth label once every ğ‘ = 100 iterations on average.
Note that we formulate the check as a probabilistic assertion [60]â€”i.e.,

which has the semantics

passert 1 âˆ’ ğ‘“fast (ğ‘¥) â‰¤ ğ‘ â€² {1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘} |

0.05,

Pğ‘ (ğ‘¥,ğ‘¦âˆ—)

(cid:0)1 âˆ’ ğ‘“fast (ğ‘¥) â‰¤ ğ‘ â€² | 1 âˆ’ ğ‘“ (ğ‘¥) â‰¤ ğ‘(cid:1),

which is the specification in is_person_fast. When our framework synthesizes a value ğ‘ â€² to fill
??2 in is_person_fast, it uses the same value to fill ??2 in monitor_correctness. Then, at run
time, it accumulates pairs (ğ‘¥, Ë†ğ‘¦), where Ë†ğ‘¦ = 1(ğ‘“ (ğ‘¥) â‰¤ ğ‘), in calls to monitor_correctness and
uses them to check whether the probabilistic assertion in that function is true.

To evaluate whether monitoring can detect shifts, we select two subsets of the â€œcarâ€ category:
(i) bikes, including motor bikes, and (ii) passenger cars, excluding busses, trucks, etc., with 6 fine-
grained labels each. Then, we consider a shift from the car category to the bike categoryâ€”i.e., if we
imagine that bikes were instead labeled as cars, would the recall of our program continue to be above
the desired threshold. First, we check whether it proves correctness when the data distribution
does not shiftâ€”i.e., using the test images labeled â€œpassenger carâ€. We run our verification algorithm
on this property using the test set images labeled As expected, our verification algorithm correctly
concludes that both the recall and the running time are within the expected bounds. Then, we check
whether it proves correctness when the data distribution shiftsâ€”i.e., using the test images labeled
â€œbikeâ€. In this case, our verification algorithm concludes that recall is incorrect, but running time is
correct. Indeed, the average running time is now lowerâ€”intuitively, ğ‘“fast is incorrectly rejecting
many â€œcarâ€ images, which reduces recall (undesired) as well as running time (desired).

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

23

def predict_warfarin_dose(x, y_true=None):

def monitor_correctness(x):

y = argmax([(ys, f(x, y)) for y in [â€˜lowâ€™, â€˜medâ€™, â€˜highâ€™]])
if y == â€˜lowâ€™ and f(x, â€˜lowâ€™) >= ??1 {y_true != â€˜highâ€™} [|, 0.05]:

return â€˜lowâ€™

y = argmax([(ys, f(x, y)) for y in [â€˜lowâ€™, â€˜medâ€™, â€˜highâ€™]])
y_true = obtain_result(x)
if y == â€˜lowâ€™:

if y == â€˜highâ€™ and f(x, â€˜highâ€™) >= ??2 {y_true != â€˜lowâ€™} [|, 0.05]:

passert f(x, â€˜lowâ€™) >= ??1 {y_true != â€˜highâ€™} [|, 0.05]

return â€˜highâ€™

return â€˜medâ€™

if y == â€˜highâ€™:

passert f(x, â€˜highâ€™) >= ??2 {y_true != â€˜lowâ€™} [|, 0.05]

Fig. 8. A program that predicts the Warfarin dose for a patient with covariates ğ‘¥. Specifications are shown in
green; curly brackets is the specification and square brackets is the value of ğœ–. The corresponding inequality
with a hole in blue. Holes with the same number are filled with the same value.

(a)

(d)

(b)

(e)

(c)

(f)

Fig. 9. We show the error rate (top) and accuracy (bottom) for our program (black), the random forest (red),
always predicting â€œmediumâ€ (green) as a function of (a,d) ğœ–, (b,e) ğ›¿, and (c,f) the number of synthesis examples
ğ‘›; for the top plots, we also show the desired upper bound on the error rate (blue).

As a side note, our framework can also be used to monitor quantitative properties. For instance,
we can keep monitor how frequently the branch ğ‘“fast (ğ‘¥) > ğ‘ is takenâ€”i.e., avoiding the need to
evaluate ğ‘“ (ğ‘¥). In Figure 6, monitor_running_time includes a probabilistic assertion

passert 1 âˆ’ ğ‘“fast (ğ‘¥) > ğ‘ â€² {true} |
ğœ–
to perform this check. This assertion says that 1 âˆ’ ğ‘“fast (ğ‘¥) > ğ‘ â€² with probability at least 1 âˆ’ ğœ–â€”i.e.,
the faster branch in is_person_fast should be taken at least 1 âˆ’ ğœ– fraction of the time according
to ğ‘ (ğ‘¥, ğ‘¦âˆ—). We might not know what is a reasonable value of ğœ–â€”i.e., the rate at which ğ‘“fast predicts
there is a person in the image. Thus, we leave it as a hole ??3. Given training examples (cid:174)ğ‘§, our
framework can be used to synthesize a value of ğœ– to fill this hole.

7.4 Case Study 2: Precision Medicine

Warfarin dosing task. Next, we consider a task from precision medicine. In particular, we consider
a random forest trained to predict dosing level for the Warfarin drug based on individual covariates
such as genetic biomarkers [36]. Personalized dosing can improve patient outcomes, but significant
errors can lead to adverse events if not quickly corrected. The ideal dosage is a real-valued label.
The goal is to train a model to predict this dosage as a decision support tool for physicians. For
simplicity, we build on an approach that converts the problem into a classification problem by

, Vol. 1, No. 1, Article . Publication date: October 2021.

0.000.010.020.030.040.0500.020.040.060.080.1Error Rateğœ€0.000.010.020.030.040.0500.020.040.060.080.1Error Rateğ›¿0.000.010.020.030.040.05500100015002000Error Raten0.500.550.600.6500.020.040.060.080.1Accuracyğœ€0.500.550.600.6500.020.040.060.080.1Accuracyğ›¿0.500.550.600.65500100015002000Accuracyn24

Bastani

discretizing this value into labels Y = {high, medium, low} dose [8]. Then, the goal is to maximize
accuracy while ensuring that very few patients for whom a high dose is predicted but should have
been assigned a low dose, and vice versa.

Experimental setup. We split the dataset (5,528 examples) into training (1,658 examples), synthesis
(2,764 examples), and test (1,106 examples) sets. Then, we use scikit-learn [54] to train a random
forest ğ‘“ : X Ã— Y â†’ R with 100 trees on the training set, where ğ‘“ (ğ‘¥, ğ‘¦) âˆˆ R is the probability
assigned to label ğ‘¦ âˆˆ Y, and use ğ‘“ in conjunction with the program shown in Figure 9. This program
includes two thresholds ğ‘low and ğ‘high, and only assigns a low dose to a patient with covariates ğ‘¥ if
ğ‘“ (ğ‘¥) â‰¥ 1 âˆ’ ğ‘low, and similarly for a high doseâ€”i.e., it only assigns the riskier outcomes when ğ‘“ is
sufficiently confident in its prediction. Importantly, the specification on ğ‘low refers not to the error
rate on predictions for patients for whom ğ‘¦ = low, but for whom ğ‘¦ = highâ€”i.e., we want to choose
ğ‘low to ensure precision specifically on patients for whom ğ‘¦ = high, and conversely for ğ‘high. We
use our synthesis algorithm to synthesize values of ğ‘low and ğ‘high that satisfy these specifications.

Correctness. Figure 9 shows the results of our approach (black) compared to directly predicting
the highest probability label according to the random forest ğ‘“ (red), always predicting â€œmediumâ€
(green), and the desired error rate (blue), as a function of the maximum error rate ğœ–, the maximum
failure probability ğ›¿, and the number of synthesis examples ğ‘›. The top plots show the error rate,
which is the maximum of the rate at which patients with ğ‘¦ = low are assigned a high dose, and
the rate at which patients ğ‘¦ = high are assigned a low dose; this value should be below the blue
line. The bottom plot shows the overall accuracy of the programâ€”i.e., how often its predicted dose
equals the ground truth dose. All values are estimated on the held-out test set. As before, ğœ– has the
largest impact on performance since it directly controls the error rate; however, once it hits ğœ– = 0.06,
performance levels off since its accuracy now equals that of ğ‘“ , and the program never assigns
a dose not predicted by ğ‘“ . Performance is flat as a function of ğ›¿. Finally, performance increases
quickly as ğ‘› goes from 500 to 1000, but plateaus thereafter, again once accuracy equals that of ğ‘“ .

Runtime monitoring. In the case of Warfarin dosing, the doctor administers an initial dose to the
patient (possibly the predicted dose, depending on the doctorâ€™s judgement), and gradually adjusts
it based on the patient response. Thus, we eventually observe the ground truth dose that should
have been recommended, which we can use to monitor our program. This process is achieved
by the monitor_correctness subroutine; here, obtain_result returns the true dose eventually
observed for a patient with covariates ğ‘¥. We evaluate whether our runtime monitoring can detect
shifts in the data distribution that lead to a reduction in performance. We consider a shift in terms
of the ethnicity of the patients, which has recently been identified as an important challenge in
algorithmic healthcare [48]. In particular, we consider a model trained using non-Hispanic White
patients (2,969 examples), which we refer to as the â€œmajority patientsâ€, and test it on Black, Hispanic,
and Asian patients (2,559 examples), which we refer to as the â€œminority patientsâ€.

First, we check if it proves correctness when the data distribution does not shiftâ€”i.e., we train
the random forest, and synthesize and verify the program on majority patients. As expected, it
successfully verifies correctness. Next, we check if it proves correctness when there is a shiftâ€”i.e.,
we train the random forest and synthesize the program on majority patients, but verify the program
on minority patients. As expected, it rejects the program as incorrect.

Finally, recall that whether verification is successful depends on how many test examples are
provided; thus, we also evaluate how many test examples are needed in this setting. To make
sure we have enough examples, we use all examples in this case. Then, we find that for 2,000 test
examples, our verification algorithm successfully proves correctness, but for 500, 1,000, or 1,500
test examples, it fails. Intuitively, the number of test examples needed to verify correctness needs

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

25

to be more than the number use to synthesize the parameters, or else the synthesized thresholds
will be more precise (i.e., closer to their â€œoptimalâ€ value) and the verification algorithm will not
have enough data to validate them. In this case, we use 1,000 synthesis examples, so about 2Ã— as
many test examples are needed to verify correctness.

8 RELATED WORK

Synthesizing machine learning programs. There has been work on synthesizing programs that
include DNN components [21, 25, 65, 70, 74] and on synthesizing probabilistic programs [47, 59];
however, they do not provide guarantees on the synthesized program. There has been work on
synthesizing control policies that satisfy provable guarantees [4, 10, 72, 75]; however, they focus
on the setting where the learner can interact with the environment, and are not applicable to our
supervised learning setting. Finally, there has been work on synthesizing programs with probabilistic
constraints [20], but requires that the search space of programs has finite VC dimension.

Verified machine learning. There has been recent interest in verifying machine learning programsâ€”
e.g., verifying robustness [3, 9, 26, 31, 33], fairness [1, 11], and safety [32, 33]. More broadly, there
has been work verifying systems such as approximate computing [14, 15, 46, 57] and probabilistic
programming [60, 61]. The most closely related work is [19, 24, 35, 49], which verify semantic
properties of machine learning models by sampling synthetic inputs from a user-specified space. In
contrast, our focus is on synthesizing machine learning programs.

Statistical verification. There has been work leveraging statistical bounds to verify stochastic
systems [62, 63, 73], probabilistic programs [60, 61], and machine learning programs [11]. Our veri-
fication algorithm in Appendix A relies on bounds similar to the ones used in these approaches [73].
To the best of our knowledge, we are the first to focus on synthesis; in contrast to verification, our
approach relies on bounds from learning theory to provide correctness guarantees.

Conformal prediction. There has been work on conformal prediction [6, 58, 64, 68], including
applications of these ideas to deep learning [5, 37, 51, 52], which aim to use statistical techniques
to provide guarantees on the predictions of machine learning models. In particular, they provide
confidence sets of outputs that contain the true label with high probability. Our techniques are
inspired by these approaches, extending them to a general framework of synthesizing machine
learning programs that satisfy provable guarantees.

9 CONCLUSION
We have proposed algorithms for synthesizing machine learning programs that come with PAC
guarantees. Our technique leverages novel statistical learning bounds to achieve these guarantees.
We have empirically demonstrated how our approach can be used to synthesize list processing
programs that manipulate images using DNN components while satisfying PAC guarantees, as well
as on two case studies in image classification and precision medicine. A key direction for future
work is how to extend these ideas to settings where the underlying data distribution may shift, and
to settings beyond supervised learning such as reinforcement learning.

REFERENCES
[1] Aws Albarghouthi, Loris Dâ€™Antoni, Samuel Drews, and Aditya V Nori. 2017. FairSquare: probabilistic verification of

program fairness. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1â€“30.

[2] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund Raghothaman, Sanjit A Seshia, Rishabh Singh,

Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. IEEE.

, Vol. 1, No. 1, Article . Publication date: October 2021.

26

Bastani

[3] Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. 2019. Optimization and abstraction: A synergistic
approach for analyzing neural network robustness. In Proceedings of the 40th ACM SIGPLAN Conference on Programming
Language Design and Implementation. 731â€“744.

[4] Greg Anderson, Abhinav Verma, Isil Dillig, and Swarat Chaudhuri. 2020. Neurosymbolic Reinforcement Learning with

Formally Verified Exploration. In Advances in neural information processing systems.

[5] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. 2020. Uncertainty Sets for Image

Classifiers using Conformal Prediction. arXiv preprint arXiv:2009.14193 (2020).

[6] Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. 2014. Conformal prediction for reliable machine

learning: theory, adaptations and applications. Newnes.

[7] Peter L Bartlett and Shahar Mendelson. 2002. Rademacher and Gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research 3, Nov (2002), 463â€“482.

[8] Hamsa Bastani and Mohsen Bayati. 2015. Online decision-making with high-dimensional covariates. Operations

Research (2015).

[9] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Antonio Criminisi. 2016.
Measuring neural net robustness with constraints. In Advances in neural information processing systems. 2613â€“2621.
[10] Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. 2018. Verifiable reinforcement learning via policy extraction.

In Advances in neural information processing systems. 2494â€“2504.

[11] Osbert Bastani, Xin Zhang, and Armando Solar-Lezama. 2019. Probabilistic verification of fairness properties via

concentration. Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“27.

[12] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2007. Analysis of representations for domain

adaptation. In Advances in neural information processing systems. 137â€“144.

[13] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. 2017. Adaptive neural networks for efficient
inference. In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 527â€“536.
[14] Michael Carbin, Deokhwan Kim, Sasa Misailovic, and Martin C Rinard. 2012. Proving acceptability properties of

relaxed nondeterministic approximate programs. ACM SIGPLAN Notices 47, 6 (2012), 169â€“180.

[15] Michael Carbin, Sasa Misailovic, and Martin C Rinard. 2013. Verifying quantitative reliability for programs that execute

on unreliable hardware. ACM SIGPLAN Notices 48, 10 (2013), 33â€“52.

[16] Jia Chen, Jiayi Wei, Yu Feng, Osbert Bastani, and Isil Dillig. 2019. Relational verification using reinforcement learning.

Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“30.

[17] Yanju Chen, Chenglong Wang, Osbert Bastani, Isil Dillig, and Yu Feng. 2020. Program Synthesis Using Deduction-
Guided Reinforcement Learning. In International Conference on Computer Aided Verification. Springer, 587â€“610.
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image

database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248â€“255.

[19] Tommaso Dreossi, Daniel J Fremont, Shromona Ghosh, Edward Kim, Hadi Ravanbakhsh, Marcell Vazquez-Chanlatte,
and Sanjit A Seshia. 2019. Verifai: A toolkit for the formal design and analysis of artificial intelligence-based systems.
In International Conference on Computer Aided Verification. Springer, 432â€“442.

[20] Samuel Drews, Aws Albarghouthi, and Loris Dâ€™Antoni. 2019. Efficient Synthesis with Probabilistic Constraints. In

International Conference on Computer Aided Verification. Springer, 278â€“296.

[21] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. 2018. Learning to infer graphics programs

from hand-drawn images. In Advances in neural information processing systems. 6059â€“6068.

[22] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. 2017.
Dermatologist-level classification of skin cancer with deep neural networks. nature 542, 7639 (2017), 115â€“118.
[23] John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure transformations from input-output

examples. ACM SIGPLAN Notices 50, 6 (2015), 229â€“239.

[24] Daniel J Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L Sangiovanni-Vincentelli, and Sanjit A
Seshia. 2019. Scenic: a language for scenario specification and scene generation. In Proceedings of the 40th ACM
SIGPLAN Conference on Programming Language Design and Implementation. 63â€“78.

[25] Alexander L Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. 2017. Differentiable programs with neural

libraries. In International Conference on Machine Learning. 1213â€“1222.

[26] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. 2018.
Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on
Security and Privacy (SP). IEEE, 3â€“18.

[27] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini
Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al. 2016. Development and validation of a deep learning
algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama 316, 22 (2016), 2402â€“2410.
[28] David Haussler, Michael Kearns, Nick Littlestone, and Manfred K Warmuth. 1991. Equivalence of models for polynomial

learnability. Information and Computation 95, 2 (1991), 129â€“161.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

27

[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition. 770â€“778.

[30] Wassily Hoeffding. 1994. Probability inequalities for sums of bounded random variables. In The Collected Works of

Wassily Hoeffding. Springer, 409â€“426.

[31] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety verification of deep neural networks. In

International Conference on Computer Aided Verification. Springer, 3â€“29.

[32] Radoslav Ivanov, James Weimer, Rajeev Alur, George J Pappas, and Insup Lee. 2019. Verisig: verifying safety properties
of hybrid systems with neural network controllers. In Proceedings of the 22nd ACM International Conference on Hybrid
Systems: Computation and Control. 169â€“178.

[33] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017. Reluplex: An efficient SMT solver
for verifying deep neural networks. In International Conference on Computer Aided Verification. Springer, 97â€“117.
[34] Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. 1994. An introduction to computational learning

theory. MIT press.

[35] Edward Kim, Divya Gopinath, Corina Pasareanu, and Sanjit A Seshia. 2020. A Programmatic and Semantic Approach
to Explaining and Debugging Neural Network Based Object Detectors. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 11128â€“11137.

[36] Stephen E Kimmel, Benjamin French, Scott E Kasner, Julie A Johnson, Jeffrey L Anderson, Brian F Gage, Yves D
Rosenberg, Charles S Eby, Rosemary A Madigan, Robert B McBane, et al. 2013. A pharmacogenetic versus a clinical
algorithm for warfarin dosing. New England Journal of Medicine 369, 24 (2013), 2283â€“2293.

[37] Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. 2020. Adaptive, Distribution-Free Prediction Intervals for

Deep Networks. In International Conference on Artificial Intelligence and Statistics. 4346â€“4356.

[38] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. 2018. The artificial intelligence
clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine 24, 11 (2018), 1716â€“1720.
[39] Tim Kraska, Mohammad Alizadeh, Alex Beutel, H Chi, Ani Kristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao,

and Vikram Nathan. 2019. Sagedb: A learned database system. In CIDR.

[40] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures. In

Proceedings of the 2018 International Conference on Management of Data. 489â€“504.

[41] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural

networks. Advances in neural information processing systems 25 (2012), 1097â€“1105.

[42] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document

recognition. Proc. IEEE 86, 11 (1998), 2278â€“2324.

[43] Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating search-based program synthesis using

learned probabilistic models. ACM SIGPLAN Notices 53, 4 (2018), 436â€“449.

[44] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016. End-to-end training of deep visuomotor policies.

The Journal of Machine Learning Research 17, 1 (2016), 1334â€“1373.

[45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence
Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer,
740â€“755.

[46] Sasa Misailovic, Michael Carbin, Sara Achour, Zichao Qi, and Martin C Rinard. 2014. Chisel: Reliability-and accuracy-

aware optimization of approximate computational kernels. ACM Sigplan Notices 49, 10 (2014), 309â€“328.

[47] Aditya V Nori, Sherjil Ozair, Sriram K Rajamani, and Deepak Vijaykeerthy. 2015. Efficient synthesis of probabilistic

programs. ACM SIGPLAN Notices 50, 6 (2015), 208â€“217.

[48] Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm

used to manage the health of populations. Science 366, 6464 (2019), 447â€“453.

[49] Matthew Oâ€™Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, and John C Duchi. 2018. Scalable end-to-end
autonomous vehicle testing via rare-event simulation. In Advances in Neural Information Processing Systems. 9827â€“9838.
[50] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. ACM SIGPLAN

Notices 50, 6 (2015), 619â€“630.

[51] Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. 2020. PAC Confidence Sets for Deep Neural Networks via

Calibrated Prediction. In International Conference on Learning Representations.

[52] Sangdon Park, Shuo Li, Osbert Bastani, and Insup Lee. 2020. PAC Confidence Predictions for Deep Neural Network

Classifiers. arXiv preprint arXiv:2011.00716 (2020).

[53] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban

Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).

[54] Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the
Journal of machine Learning research 12 (2011), 2825â€“2830.

, Vol. 1, No. 1, Article . Publication date: October 2021.

28

Bastani

[55] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. 2009. Dataset shift in

machine learning. The MIT Press.

[56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster r-cnn: Towards real-time object detection with
region proposal networks. IEEE transactions on pattern analysis and machine intelligence 39, 6 (2016), 1137â€“1149.
[57] Martin Rinard. 2006. Probabilistic accuracy bounds for fault-tolerant computations that discard tasks. In Proceedings of

the 20th annual international conference on Supercomputing. 324â€“334.

[58] Yaniv Romano, Evan Patterson, and Emmanuel Candes. 2019. Conformalized quantile regression. In Advances in

Neural Information Processing Systems. 3543â€“3553.

[59] Feras A Saad, Marco F Cusumano-Towner, Ulrich Schaechtle, Martin C Rinard, and Vikash K Mansinghka. 2019.
Bayesian synthesis of probabilistic programs for automatic data modeling. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“32.

[60] Adrian Sampson, Pavel Panchekha, Todd Mytkowicz, Kathryn S McKinley, Dan Grossman, and Luis Ceze. 2014.
Expressing and verifying probabilistic assertions. In Proceedings of the 35th ACM SIGPLAN Conference on Programming
Language Design and Implementation. 112â€“122.

[61] Sriram Sankaranarayanan, Aleksandar Chakarov, and Sumit Gulwani. 2013. Static analysis for probabilistic programs:
inferring whole program properties from finitely many paths. In Proceedings of the 34th ACM SIGPLAN conference on
Programming language design and implementation. 447â€“458.

[62] Koushik Sen, Mahesh Viswanathan, and Gul Agha. 2004. Statistical model checking of black-box probabilistic systems.

In International Conference on Computer Aided Verification. Springer, 202â€“215.

[63] Koushik Sen, Mahesh Viswanathan, and Gul Agha. 2005. On statistical model checking of stochastic systems. In

International Conference on Computer Aided Verification. Springer, 266â€“280.

[64] Glenn Shafer and Vladimir Vovk. 2008. A tutorial on conformal prediction. Journal of Machine Learning Research 9,

Mar (2008), 371â€“421.

[65] Ameesh Shah, Eric Zhan, Jennifer J Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. 2020. Learning Differen-

tiable Programs with Admissible Neural Heuristics. In Advances in neural information processing systems.

[66] Armando Solar-Lezama. 2008. Program synthesis by sketching. University of California, Berkeley.
[67] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting
from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2464â€“2469.
[68] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. 2019. Conformal prediction under

covariate shift. In Advances in Neural Information Processing Systems. 2530â€“2540.

[69] Leslie G Valiant. 1984. A theory of the learnable. Commun. ACM 27, 11 (1984), 1134â€“1142.
[70] Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. 2018. Houdini: Lifelong

learning as program synthesis. In Advances in Neural Information Processing Systems.

[71] Vladimir Vapnik. 2013. The nature of statistical learning theory. Springer science & business media.
[72] Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. 2018. Programmati-

cally Interpretable Reinforcement Learning. In International Conference on Machine Learning. 5045â€“5054.

[73] HÃ¥kan LS Younes and Reid G Simmons. 2002. Probabilistic verification of discrete event systems using acceptance

sampling. In International Conference on Computer Aided Verification. Springer, 223â€“235.

[74] Halley Young, Osbert Bastani, and Mayur Naik. 2019. Learning Neurosymbolic Generative Models via Program

Synthesis. In International Conference on Machine Learning.

[75] He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. 2019. An inductive synthesis framework for verifiable
reinforcement learning. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and
Implementation. 686â€“701.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

29

A STATISTICAL VERIFICATION
We describe our algorithm for verifying a complete program. Our algorithm (Algorithm 3) takes
as input a complete program Â¯ğ‘ƒ, training valuations (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›), where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d.
samples, and a confidence level ğ›¿ âˆˆ R>0, and outputs a value ğ´( Â¯ğ‘ƒ, (cid:174)ğ›¼) âˆˆ {0, 1} indicating whether Â¯ğ‘ƒ
is approximately complete, which is correct with probability at least 1 âˆ’ ğ›¿ with respect to ğ‘ ( (cid:174)ğ›¼).

Our algorithm is based on statistical verification [62, 63, 73]. These algorithms leverage concen-
tration inequalities from probability theory to provide high-probability correctness guarantees.
Concentration inequalities are theorems that provide rigorous bounds on the rate of convergence
of statistical estimators. For instance, consider a Bernoulli distribution ğ‘ = Bernoulli(ğœ‡) with
unknown mean ğœ‡. Given samples ğ‘§1, ..., ğ‘§ğ‘› âˆ¼ ğ‘, Hoeffdingâ€™s inequality [30] says that the empirical
mean Ë†ğœ‡ ((cid:174)ğ‘§) = ğ‘›âˆ’1 (cid:205)ğ‘›

ğ‘–=1 ğ‘§ğ‘– converges to ğœ‡:

Pğ‘ ( (cid:174)ğ‘§)

(cid:0)| Ë†ğœ‡ ((cid:174)ğ‘§) âˆ’ ğœ‡| â‰¤ ğœ–(cid:1) â‰¥ 1 âˆ’ ğ›¿ where ğ›¿ = 2ğ‘’âˆ’2ğ‘›ğœ– 2,

(11)

i.e., Ë†ğœ‡ ((cid:174)ğ‘§) is a good approximation of ğœ‡ with high probability.

ğ¸
(cid:74)

ğ›¼ . Then, ğœ–-approximate soundness of ğ¸ is equivalent to ğœ‡ = Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—
âˆ—
(cid:75)

In our setting, given training valuation ğ›¼ and a specification ğ¸ = ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğ›¼ and ğ‘§âˆ—
(cid:75)

ğœ– in Â¯ğ‘ƒ, we let ğ‘§ğ›¼ =
ğ›¼ ) â‰¥ 1 âˆ’ğœ–
ğ¸
ğ›¼ =
(cid:74)
if ğœ” = |, or ğœ‡ = Pğ‘ (ğ›¼) (ğ‘§âˆ—
ğ›¼ â‡’ ğ‘§ğ›¼ ) â‰¥ 1 âˆ’ ğœ– if ğœ” = â‡’. That is, ğœ–-approximate soundness is equivalent
to ğœ‡ â‰¥ 1 âˆ’ ğœ–, where ğœ‡ is the mean of a Bernoulli random variable ğ‘§ğ›¼ that is a function of a random
variable ğ›¼ with distribution ğ‘ (ğ›¼ | ğ‘§âˆ—
ğ›¼ â‡’ ğ‘§ğ›¼
that is a function of ğ›¼ with distribution ğ‘ (ğ›¼) (if ğœ” = â‡’). However, ğ‘§ğ›¼ is potentially a complicated
function of ğ›¼ and ğ‘ (ğ›¼) is unknown, so ğœ‡ is hard to compute directly. Instead, given i.i.d. samples
and use them estimate ğœ‡:
ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ (ğ›¼), we can construct the samples ğ‘§ğ›¼1, ..., ğ‘§ğ›¼ğ‘› and ğ‘§âˆ—
ğ›¼1

ğ›¼ ) (if ğœ” = |) or the mean of a Bernoulli random variable ğ‘§âˆ—

, ..., ğ‘§âˆ—
ğ›¼ğ‘›

Ë†ğœ‡ ( (cid:174)ğ›¼) =

(cid:40) (cid:205)ğ‘›

ğ‘–=1 ğ‘§ğ›¼ğ‘– âˆ§ğ‘§âˆ—
ğ›¼ğ‘–
(cid:205)ğ‘›
ğ‘–=1 ğ‘§âˆ—
ğ›¼ğ‘–
ğ›¼ğ‘– â‡’ ğ‘§ğ›¼ğ‘–
ğ‘–=1 ğ‘§âˆ—

(cid:205)ğ‘›

if ğœ” = |
if ğœ” = â‡’ .

Then, we can use (11) to bound the error of Ë†ğœ‡ ( (cid:174)ğ›¼)â€”e.g., if Ë†ğœ‡ ( (cid:174)ğ›¼) â‰¥ 1 âˆ’ ğœ–
2 with
probability at least 1 âˆ’ ğ›¿, then ğœ‡ â‰¥ 1 âˆ’ ğœ– with probability at least 1 âˆ’ ğ›¿. However, this approach is
inefficient since Hoeffdingâ€™s inequality is not tight for our setting. Instead, our verification algorithm
(Appendix A.2) leverages a concentration inequality tailored to our setting (Appendix A.1). Finally,
we disucss how our approach can be used in the context of runtime monitoring (Appendix A.3).

2 and | Ë†ğœ‡ ( (cid:174)ğ›¼) âˆ’ ğœ‡| â‰¤ ğœ–

A.1 A Concentration Bound

Problem formulation. Consider a Bernoulli distribution ğ‘ = Bernoulli(ğœ‡) with unknown mean
ğœ‡ âˆˆ [0, 1]. Given ğœ– âˆˆ R>0, our goal is to determine whether ğœ‡ â‰¥ 1 âˆ’ ğœ–. For instance, a sample ğ‘§ âˆ¼ ğ‘
may indicate a desired outcome (e.g., a correctly classified input), in which case ğœ‡ is the correctness
rate and ğœ– is a desired bound on the error rate; then, our goal is to check whether the ğœ‡ meets the
desired error bound. More precisely, we want to compute ğœ“ âˆˆ {0, 1} such that

ğœ“ â‡’ (ğœ‡ â‰¥ 1 âˆ’ ğœ–).

(12)

That is, ğœ“ is a sound overapproximation of the property ğœ‡ â‰¥ 1 âˆ’ ğœ– (i.e., ğœ“ = 1 implies ğœ‡ â‰¥ 1 âˆ’ ğœ–).

To compute such a ğœ“ , we are given a training set of examples (cid:174)ğ‘§ = (ğ‘§1, ..., ğ‘§ğ‘›) âˆˆ {0, 1}ğ‘›, where
ğ‘§1, ..., ğ‘§ğ‘› âˆ¼ ğ‘ are ğ‘› i.i.d. samples from ğ‘. An estimator is a mapping Ë†ğœ“ : Rğ‘› â†’ R. We say such an
estimator is approximately correct if it satisfies the condition (12)â€”i.e., Ë†ğœ“ ((cid:174)ğ‘§) â‡’ (ğœ‡ â‰¥ 1 âˆ’ ğœ–).

In general, we cannot guarantee Ë†ğœ“ ((cid:174)ğ‘§) is approximately correct due to the randomness in the
training examples (cid:174)ğ‘§.4 Thus, we allow a probability ğ›¿ âˆˆ R>0 that Ë†ğœ“ ((cid:174)ğ‘§) is not approximately correct.

4Note that Ë†ğœ“ is a deterministic function; the randomness of Ë†ğœ“ ( (cid:174)ğ‘§) is entirely due to the randomness in the training data (cid:174)ğ‘§.

, Vol. 1, No. 1, Article . Publication date: October 2021.

30

Bastani

Algorithm 3 Use statistical verification to check if ğ‘ƒ is approximately correct.

procedure Verify( Â¯ğ‘ƒ, (cid:174)ğ›¼, ğ›¿)

ğ‘š â† |Î¦(ğ‘ƒ)|
for ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

ğœ– âˆˆ Î¦( Â¯ğ‘ƒ) do

Compute (cid:174)ğ‘§ (cid:174)ğ›¼ according to (14)
Compute Ë†ğœ“ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) according to (13) with (ğœ–, ğ›¿/ğ‘š)
if Â¬ Ë†ğœ“ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) then
return false

end if

end for
return true
end procedure

Definition A.1. Given ğœ–, ğ›¿ âˆˆ R>0, Ë†ğœ“ is (ğœ–, ğ›¿)-PAC if Pğ‘ ( (cid:174)ğ‘§)
In other words, Ë†ğœ“ ((cid:174)ğ‘§) is approximately correct with probability at least 1 âˆ’ ğ›¿ according to the

(cid:0) Ë†ğœ“ ((cid:174)ğ‘§) â‡’ (ğœ‡ â‰¥ 1 âˆ’ ğœ–)(cid:1) â‰¥ 1 âˆ’ ğ›¿.

randomness in ğ‘ ((cid:174)ğ‘§). Our goal is to construct an (ğœ–, ğ›¿)-PAC estimator Ë†ğœ“ ((cid:174)ğ‘§).

Ë†ğœ“ ((cid:174)ğ‘§) = 1(ğ¿((cid:174)ğ‘§) â‰¤ ğ‘˜) where ğ‘˜ = max

Estimator. Given ğœ–, ğ›¿ âˆˆ R>0, consider the estimator
(cid:40)
â„ âˆˆ N

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
and where ğ¿((cid:174)ğ‘§) = (cid:205)ğ‘›
ğ‘§ âˆˆ(cid:174)ğ‘§ (1 âˆ’ ğ‘§). Intuitively, ğ¿((cid:174)ğ‘§) counts the number of errors, so we conclude the
desired property holds as long as ğ¿((cid:174)ğ‘§) is below a threshold ğ‘˜. This threshold is chosen so Ë†ğœ“ is
(ğœ–, ğ›¿)-PACâ€”in particular, ğ›¿ upper bounds the CDF of the binomial distribution evaluated at ğ‘˜.

ğœ–ğ‘– (1 âˆ’ ğœ–)ğ‘›âˆ’ğ‘– â‰¤ ğ›¿

(cid:18)ğ‘›
ğ‘–

â„
âˆ‘ï¸

(13)

ğ‘–=0

(cid:41)

(cid:19)

To compute the solution ğ‘˜ in (13), we start with â„ = 0 and increment it until it no longer satisfies
the condition. To ensure numerical stability, this computation is performed using logarithms. Note
that ğ‘˜ does not exist if the set inside the maximum in (13) is empty; in this case, we choose Ë†ğœ“ ((cid:174)ğ‘§) = 0,
which trivially satisfies the PAC property. We have the following; see Appendix B.1 for a proof:

Theorem A.2. The estimator

Ë†ğœ“ is (ğœ–, ğ›¿)-PAC.

A.2 Verification Algorithm

Problem formulation. A verification algorithm ğ´ : Â¯P Ã— Ağ‘› â†’ {0, 1} takes as input a complete
program Â¯ğ‘ƒ âˆˆ Â¯P, and a set of test valuations (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›) âˆˆ Ağ‘›, where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d.
samples from a distribution ğ‘ (ğ›¼). For example, ğ‘ (ğ›¼) may be the distribution of input images to an
image classifier encountered while running in production, that have been manually labeled using
crowdsourcing. Then, ğ´( Â¯ğ‘ƒ, (cid:174)ğ›¼) âˆˆ {0, 1} should indicate whether Â¯ğ‘ƒ is approximately soundâ€”i.e.,
ğœ– âˆˆ Î¦( Â¯ğ‘ƒ) is approximately sound. We say that ğ´ is sound
whether every expression ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
if ğ´( Â¯ğ‘ƒ, (cid:174)ğ›¼) â‡’ Â¯ğ‘ƒ âˆˆ Â¯Pâˆ—. As before, we cannot guarantee that ğ´ is sound; instead, given ğ›¿ âˆˆ R>0, we
want this property to hold with probability at least 1 âˆ’ ğ›¿ according to ğ‘ ( (cid:174)ğ›¼).

Definition A.3. A verification algorithm ğ´ : Â¯P Ã— Ağ‘› â†’ {0, 1} is ğ›¿-probably approximately

sound if for all Â¯ğ‘ƒ âˆˆ Â¯P, Pğ‘ ( (cid:174)ğ›¼)

(cid:0)ğ´( Â¯ğ‘ƒ, (cid:174)ğ›¼) â‡’ Â¯ğ‘ƒ âˆˆ Â¯Pâˆ—(cid:1) â‰¥ 1 âˆ’ ğ›¿.

Algorithm. Our verification algorithm is shown in Algorithm 3. It check approximate correctness
of Â¯ğ‘ƒ by checking approximate soundness of each ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ– âˆˆ Î¦( Â¯ğ‘ƒ). It does so by allocating a
ğ›¿/ğ‘š probability of failure for each expression, where ğ‘š = |Î¦( Â¯ğ‘ƒ)| is the number of such expressions.

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

31

Next, we describe how our algorithm checks approximate soundness for a single expression

ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”

ğœ– . Given a single test valuation ğ›¼ âˆ¼ ğ‘, consider the indicators

ğ‘§ğ›¼ =

ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)
That is, ğ‘§ğ›¼ indicates whether ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) holds, and ğ‘§âˆ—
is approximately sound if and only if

and

ğ›¼
(cid:75)

ğ‘§âˆ—
ğ›¼ =

ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }ğœ”
ğœ–
(cid:74)

âˆ—
ğ›¼ .
(cid:75)

ğ›¼ indicates whether ğ‘„ holds. Then, ğœ™ ( Â¯ğ‘ƒ â€², ğ‘) {ğ‘„ }

Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—

ğ›¼ ) â‰¥ 1 âˆ’ ğœ–

if ğœ” = |

or

Pğ‘ (ğ›¼) (ğ‘§âˆ—

ğ›¼ â‡’ ğ‘§ğ›¼ ) â‰¥ 1 âˆ’ ğœ–

if ğœ” = â‡’ .

Next, note that ğ‘§ğ›¼ âˆˆ {0, 1} is a Bernoulli random variable with mean ğœ‡ = Pğ‘ (ğ›¼) (ğ‘§ğ›¼ | ğ‘§âˆ—
ğœ‡ = Pğ‘ (ğ›¼) (ğ‘§âˆ—

ğ›¼ ) (if ğœ” = |) or
ğ›¼ â‡’ ğ‘§ğ›¼ ) (if ğœ” = â‡’). Thus, given (cid:174)ğ›¼ = (ğ›¼1, ..., ğ›¼ğ‘›), where ğ›¼1, ..., ğ›¼ğ‘› âˆ¼ ğ‘ are i.i.d. samples,

(cid:40)

(cid:174)ğ‘§ (cid:174)ğ›¼ =

{ğ‘§ğ›¼ | ğ›¼ âˆˆ (cid:174)ğ›¼ âˆ§ ğ‘§âˆ—
{ğ‘§âˆ—

ğ›¼ }
ğ›¼ â‡’ ğ‘§ğ›¼ | ğ›¼ âˆˆ (cid:174)ğ›¼ }

if ğœ” =|
if ğœ” =â‡’

(14)

is a vector of i.i.d. samples from Bernoulli(ğœ‡). Then, the estimator Ë†ğœ“ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) in (13) with parameters
(ğœ–, ğ›¿/ğ‘š) indicates whether ğœ‡ â‰¥ 1 âˆ’ ğœ– with high probabilityâ€”i.e., if Ë†ğœ“ ((cid:174)ğ‘§ (cid:174)ğ›¼ ) = 1, then ğœ‡ â‰¥ 1 âˆ’ ğœ–
holds with probability at least 1 âˆ’ ğ›¿/ğ‘š according to ğ‘ ( (cid:174)ğ›¼). The following guarantee follows from
Theorem A.2 by a union bound over expressions in Î¦( Â¯ğ‘ƒ):

Theorem A.4. Algorithm 3 is ğ›¿-probably approximately sound.

A.3 Runtime Monitoring
One challenge is that the specifications considered by our framework depend on the distribution of
the data. As a consequence, if this distribution changes, then our correctness guarantees may no
longer hold. This potential failure mode, called distribution shift [12, 55], is a major challenge for
machine learning components. A key feature of our framework is that it can be used not only to
sketch or verify the program before it is deployed, but also to continuously re-sketch the program
based on feedback obtained in production to account for potential distribution shift. The primary
requirement for using this approach is the need for feedbackâ€”i.e., continuing to collect labeled
examples in production. In some settings, this kind of feedback is naturally available; otherwise, a
solution is to manually label a small fraction of examplesâ€”e.g., using crowdsourcing [18].

Given ground truth labels for the input examples encountered in production, our verification
algorithm can be straightforwardly adapted to the runtime setting. In particular, our system collects
examples during execution; once it collects at least ğ‘ examples, it re-runs verification or sketching.
It can do so after every subsequent example, or every ğ¾ examples. Finally, we may want to discard
an examples after ğ‘‡ steps, both for computational efficiency and to account for the fact that the data
distribution may be shifting over time so older examples are less representative. Here, ğ¾, ğ‘ ,ğ‘‡ âˆˆ N
are hyperparameters. Finally, we note that our statistical sketching algorithm can similarly be
adapted to the runtime setting.

B PROOFS

B.1 Proof of Theorem A.2
It suffices to show that if ğœ‡ < 1 âˆ’ ğœ–, then Pğ‘ ( (cid:174)ğ‘§) ( Ë†ğœ“ ((cid:174)ğ‘§)) < ğ›¿. First, note that since ğ‘§1, ..., ğ‘§ğ‘› are i.i.d.
Bernoulli random variables with mean ğœ‡, then 1 âˆ’ ğ‘§1, ..., 1 âˆ’ ğ‘§ğ‘› are i.i.d. Bernoulli random variables
with mean ğœˆ = 1 âˆ’ ğœ‡. Their sum ğ¿((cid:174)ğ‘§) is a binomial random variableâ€”i.e., ğ¿((cid:174)ğ‘§) âˆ¼ Binomial(ğ‘›, ğœˆ).

, Vol. 1, No. 1, Article . Publication date: October 2021.

32

Bastani

Also, note that the condition ğœ‡ < 1 âˆ’ ğœ– is equivalent to ğœˆ > ğœ–. Thus, we have

Pğ‘ ( (cid:174)ğ‘§) ( Ë†ğœ“ ((cid:174)ğ‘§)) =

<

ğ‘˜
âˆ‘ï¸

ğ‘–=0
ğ‘˜
âˆ‘ï¸

ğ‘–=0
â‰¤ ğ›¿,

(cid:19)

(cid:18)ğ‘›
ğ‘–

ğœˆğ‘– (1 âˆ’ ğœˆ)ğ‘›âˆ’ğ‘–

(cid:19)

(cid:18)ğ‘›
ğ‘–

ğœ–ğ‘– (1 âˆ’ ğœ–)ğ‘›âˆ’ğ‘–

where the first inequality follows by standard properties of the CDF of the Binomial distribution.
The claim follows. â–¡

B.2 Proof of Theorem 4.2
First, define

ğ‘¡ 0
ğœ– = inf
ğ‘¡ âˆˆR

Tğœ– .

Intuitively, ğ‘¡ 0
particular, it is clear that ğ‘¡ âˆˆ Tğœ– for all ğ‘¡ > ğ‘¡ 0
may not hold. Thus, it suffices to show

ğœ– âˆˆ R is the threshold that determines whether ğ‘¡ is ğœ–-approximately correct. In
ğœ– âˆˆ Tğœ– may or

ğœ– and ğ‘¡ âˆ‰ Tğœ– for all ğ‘¡ < ğ‘¡ 0

ğœ– ; in general, ğ‘¡ 0

Pğ‘ ( (cid:174)ğ‘§) (Ë†ğ‘¡ ((cid:174)ğ‘§) â‰¤ ğ‘¡ 0

ğœ– ) < ğ›¿.

To this end, note that the constraint ğ¿(ğ‘¡; (cid:174)ğ‘§) â‰¤ ğ‘˜ in (4) implies

1(ğ‘§ > Ë†ğ‘¡ ((cid:174)ğ‘§) âˆ’ ğ›¾ ((cid:174)ğ‘§)) â‰¤ ğ‘˜.

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

Thus, on event Ë†ğ‘¡ ((cid:174)ğ‘§) â‰¤ ğ‘¡ 0

ğœ– , we have Ë†ğ‘¡ ((cid:174)ğ‘§) âˆ’ ğ›¾ ((cid:174)ğ‘§) â‰¤ ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§), so

ğ‘˜ â‰¤

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

1(ğ‘§ > Ë†ğ‘¡ ((cid:174)ğ‘§) âˆ’ ğ›¾ ((cid:174)ğ‘§)) â‰¤

1(ğ‘§ > ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§)).

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

As a consequence, we have

Pğ‘ ( (cid:174)ğ‘§) (Ë†ğ‘¡ ((cid:174)ğ‘§) â‰¤ ğ‘¡ 0

ğœ– ) â‰¤ Pğ‘ ( (cid:174)ğ‘§)

1(ğ‘§ > ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§)) â‰¥ ğ‘˜

(cid:33)

.

(cid:32)

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

Next, since ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§) < ğ‘¡ 0

ğœ– , we have ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§) âˆ‰ Tğœ– â€”i.e.,

ğœ– < Pğ‘ (ğ‘§) (ğ‘§ > ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§)) = Eğ‘ (ğ‘§) (1(ğ‘§ > ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§))).

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

33

In other words, the random variables 1(ğ‘§ > ğ‘¡ 0
with mean ğœˆ > ğœ–. Thus, we have

ğœ– âˆ’ğ›¾ ((cid:174)ğ‘§)) for ğ‘§ âˆˆ (cid:174)ğ‘§ are i.i.d. Bernoulli random variables

Pğ‘ ( (cid:174)ğ‘§)

(cid:32)

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

1(ğ‘§ > ğ‘¡ 0

ğœ– âˆ’ ğ›¾ ((cid:174)ğ‘§)) â‰¥ ğ‘˜

(cid:33)

ğ‘˜
âˆ‘ï¸

ğ‘–=0
ğ‘˜
âˆ‘ï¸

ğ‘–=0
ğ‘˜
âˆ‘ï¸

ğ‘–=0
ğ‘˜
âˆ‘ï¸

=

=

<

=

Pğ‘ ( (cid:174)ğ‘§)

(cid:32)

âˆ‘ï¸

ğ‘§ âˆˆ(cid:174)ğ‘§

1(ğ‘§ > ğ‘¡ 0

ğœ– + ğ›¾ ((cid:174)ğ‘§)) = ğ‘–

(cid:33)

Binomial(ğ‘–; ğ‘›, ğœˆ)

Binomial(ğ‘–; ğ‘›, ğœ–)

(cid:19)

ğœ–ğ‘– (1 âˆ’ ğœ–)ğ‘›âˆ’ğ‘–

(cid:18)ğ‘›
ğ‘–

ğ‘–=0
â‰¤ ğ›¿,
where the first inequality follows by standard properties of the CDF of the Binomial distribution.
The claim follows. â–¡

B.3 Proof of Theorem 4.4
First, we have the following classical inequality [30]:

Theorem B.1. (Hoeffdingâ€™s inequality) We have

(cid:16)

Pğ‘ ( (cid:174)ğ‘§)

ğœ‡ âˆ’ Ë†ğœ‡ ((cid:174)ğ‘§) â‰¥ ğ‘¡

(cid:17)

â‰¤ ğ‘’âˆ’2ğ‘›ğ‘¡ 2 .

Now, letting ğ‘¡ =

âˆšï¸ƒ log(1/ğ›¿)
2ğ‘›

, we have

â‰¤ Pğ‘ ( (cid:174)ğ‘§)
where the second-to-last inequality follows from Theorem B.1. The claim follows. â–¡

ğœ‡ âˆ’ Ë†ğœ‡ ((cid:174)ğ‘§) â‰¥ ğ‘¡

ğœ‡ â‰¥ Ë†ğœˆ ((cid:174)ğ‘§)

Pğ‘ ( (cid:174)ğ‘§)

â‰¤ ğ›¿,

(cid:16)

(cid:17)

(cid:16)

(cid:17)

â‰¤ ğ‘’âˆ’2ğ‘›ğ‘¡ 2

C ADDITIONAL CASE STUDY: OBJECT DETECTION

Object detection. We assume given a DNN component ğ‘“ that given an image ğ‘¥, is designed to
detect people in ğ‘¥. Our formulation of object detection in this section is slightly different than the
previous setup. In particular, ğ‘‘ âˆˆ ğ‘“ (ğ‘¥) is a list of detections, which is a pair ğ‘‘ = (ğ‘, ğ‘) including a
bounding box ğ‘ âˆˆ R4 that encodes the center, width, and height of a rectangular region of ğ‘¥, and a
value ğ‘ âˆˆ [0, 1] that is the predicted probability that ğ‘ exists. In addition, the ground truth label
ğ‘¦âˆ— for an image ğ‘¥ is a list of bounding boxes ğ‘ âˆˆ ğ‘¦âˆ—. In general, we cannot expect to get a perfect
match between the predicted bounding boxes and the ground truth ones. Typically, two bounding
boxes ğ‘,â€² match if have significant overlapâ€”in particular, their intersection-over-union satisfies
IOU(ğ‘, ğ‘ â€²) â‰¥ ğœŒ for some threshold ğœŒ âˆˆ [0, 1]; we use a standard choice of ğœŒ = 0.5. We denote that
ğ‘ and ğ‘ â€² match in this sense by ğ‘ (cid:27) ğ‘ â€². Finally, ğ‘ approximately matches a bounding box in ğ‘¦âˆ— it
ğ‘ (cid:27) ğ‘ â€² for some ğ‘ â€² âˆˆ ğ‘¦âˆ—, which we denote by ğ‘ Ëœâˆˆ ğ‘¦âˆ—.

Experimental setup. We use a pretrained state-of-the-art object detector called Faster R-CNN [56]
available in PyTorch [53], tailored to the COCO dataset [45]. There are multiple variants of Faster
R-CNN; we use the most accurate one, termed X101-FPN with 3Ã— learning rate schedule. For each
predicted bounding box, this model additionally outputs a predicted object category (e.g., â€œpersonâ€),
as well as the size of the bounding box (â€œsmallâ€, â€œmediumâ€, and â€œlargeâ€). For most of our evaluation,

, Vol. 1, No. 1, Article . Publication date: October 2021.

34

Bastani

def detect_ppl(x):

y_hat = f(x)
return [d.box for d in y_hat if check_det(x, d)]

def check_det(x, d, d_true=None)

return d.score > ??1 {IOU(d.box, d_true) >= 0.5} [|, 0.05]

def detect_ppl_fast(x, y_true=None):

y_hat = f_fast(x)
no_ppl_score = 1.0 - max([d.score for d in y_hat])
if no_ppl_score > ??2 {len(detect_ppl(x)) != 0} [|, 0.05]:

return []

else:

return detect_ppl(x)

def monitor_correctness(x):

if np.random.uniform() <= 0.99:

return

y_hat = f_fast(x)
no_ppl_score = 1.0 - max([d.score for d in y_hat])
passert no_ppl_score > ??1 { len(detect_ppl(x)) != 0} [??3]

def monitor_speed(x):
y_hat = f_fast(x)
no_ppl_score = 1.0 - max([d.score for d in y_hat])
passert no_ppl_score > ??1 {true} [??3]

Fig. 10. A program used to detect people in a given image ğ‘¥. Specifications are shown in green; curly brackets
is the specification and square brackets is the value of ğœ–. The corresponding inequality with a hole in blue.
Holes with the same number are filled with the same value.

we use â€œpersonâ€ and â€œlargeâ€. We specify alternative choices when we used them; in particular, we
additionally consider 6 of the 91 object categories: â€œpersonâ€ (10777 bounding boxes), â€œcarâ€ (1918
bounding boxes), â€œtruckâ€ (414 bounding boxes), â€œmotorcycleâ€ (367 bounding boxes), â€œbikeâ€ (314
bounding boxes), and â€œbusâ€ (283 bounding boxes). We split the COCO validation set into 2000
synthesis images and 3000 test images.

Correctness. Our goal is to detect a majority of people. In particular, we consider synthesizing a

threshold ğ‘ and selecting all bounding boxes with probability above ğ‘â€”i.e.,

ğ‘“ (ğ‘¥, ğ‘) = {ğ‘ | (ğ‘, ğ‘) âˆˆ ğ‘“ (ğ‘¥) âˆ§ ğ‘ â‰¥ ğ‘}.
This task is more challenging to specify than our examples so far since ğ‘“ (ğ‘¥) is a structured output.
In particular, we are not reasoning about whether ğ‘“ (ğ‘¥) is correct with high probability with respect
to ğ‘ (ğ‘¥, ğ‘¦âˆ—), but whether bounding boxes (ğ‘, ğ‘) âˆˆ ğ‘“ (ğ‘¥) are correct. Thus, we need a distribution
ğ‘ (ğ‘ | ğ‘¥) over bounding boxes ğ‘ in an image ğ‘¥. Given such a distribution, our goal is to choose ğ‘ so
Pğ‘ (ğ‘¥,ğ‘¦âˆ—),ğ‘ (ğ‘ |ğ‘¥) (ğ‘ Ëœâˆˆ ğ‘“ (ğ‘¥, ğ‘) | ğ‘ Ëœâˆˆ ğ‘¦âˆ—) â‰¥ 1 âˆ’ ğœ–,
(15)
where ğ‘ (ğ‘¥, ğ‘¦âˆ—) is the data distribution. Intuitively, this property says that ğ‘“ (ğ‘¥, ğ‘) contains at least
a 1 âˆ’ ğœ– fraction of ground truth bounding boxes. A reasonable choice for ğ‘ (ğ‘ | ğ‘¥) is the uniform
distribution over ğ‘“ (ğ‘¥, 0)â€”i.e., the set of all bounding boxes predicted by ğ‘“ . One issue is when a
ground truth bounding box ğ‘ âˆˆ ğ‘¦âˆ— is completely missing from ğ‘“ (ğ‘¥, 0); in this case, ğ‘ would not
occur in ğ‘ (ğ‘ | ğ‘¥), so (15) would not count it as an error even though it is missing from ğ‘“ (ğ‘¥, ğ‘) for
any ğ‘. To handle this case, we simply add (ğ‘, 0) to ğ‘“ (ğ‘¥) during synthesis for such bounding boxes
ğ‘â€”i.e., ğ‘“ predicts ğ‘ occurs with probability zero.

The program for achieving this goal is shown in the subroutine detect_ppl in Figure 10. We
use our algorithm in conjunction with the synthesis examples to synthesize the parameter ??1 for
this program, using the default values ğœ– = ğ›¿ = 0.05 and the object category â€œpersonâ€. In Figure 11,
we show the recall (red), desired lower bound on recall (blue), and precision (black) as a function of
(a) ğœ–, (b) ğ›¿, and (c) the object category ğ‘¦. The trends are largely similar to beforeâ€”e.g., performance

, Vol. 1, No. 1, Article . Publication date: October 2021.

Synthesizing Machine Learning Programs

35

(a)

(d)

(g)

(b)

(e)

(h)

(c)

(f)

(i)

Fig. 11. For the slow model alone (top) and slow+fast model (middle), we show recall (red), the desired lower
bound on recall (blue), and precision (black) as a function of (a,d) ğœ–, (b,e) ğ›¿, and (c,f) the object category ğ‘¦. For
slow+fast (black), slow alone (red), and fast alone (green), we show running time as a function of (g) ğœ–, (h) ğ›¿,
and (i) the object category ğ‘¦.

varies significantly with ğœ– and the object category, but not very much with ğ›¿. For (c), we use ğœ– = 0.1
to facilitate comparison to our fast program described below.

Improving speed. We use a similar approach to improve speed as beforeâ€”i.e., given a fast object
detector ğ‘“fast, we want to use it to check the image, and only send it to the slow object detector ğ‘“ if
necessary. A challenge compared to image classification is that the object detection model does not
operate at the level of bounding boxes, which is the level at which we defined correctness, but at
the level of images. Thus, we cannot decide whether we want to run the slow model independently
for each detection ğ‘‘ âˆˆ ğ‘“fast (ğ‘¥); instead, we have to make such a decision for an image ğ‘¥ as a whole.
Intuitively, we check whether the fast model returns any detections in the given image ğ‘¥. To this
end, we compute the maximum score ğ‘ across all detections (ğ‘, ğ‘) âˆˆ ğ‘“fast (ğ‘¥)â€”i.e.,

Ëœğ‘“fast (ğ‘¥) = max

(ğ‘,ğ‘) âˆˆğ‘“fast (ğ‘¥)

ğ‘.

Then, we want to guarantee that ğ‘¦âˆ— = âˆ… if this score is below some threshold that ensures that
ğ‘¦âˆ— = âˆ…; this property is equivalent to its contrapositive

(ğ‘¦âˆ— â‰  âˆ…) â‡’ (1 âˆ’ Ëœğ‘“fast (ğ‘¥) â‰¤ ğ‘),

(16)

, Vol. 1, No. 1, Article . Publication date: October 2021.

0.800.850.900.951.000.050.10.150.2Precision/Recallğœ€0.800.850.900.951.000.050.10.150.2Precision/Recallğ›¿0.000.200.400.600.801.00PersonCarTruckMotorBikeBusPrecision/Recall0.600.700.800.901.000.050.10.150.2Precision/Recallğœ€0.800.850.900.951.000.050.10.150.2Precision/Recallğ›¿0.000.200.400.600.801.00PersonCarTruckMotorBikeBusPrecision/Recall048120.050.10.150.2Time (Minutes)ğœ€048120.050.10.150.2Time (Minutes)ğ›¿04812PersonCarTruckMotorBikeBusTime (Minutes)36

Bastani

where the right-hand side of the implication is equivalent to ğ‘“fast (ğ‘¥) â‰¥ 1 âˆ’ ğ‘â€”i.e., the score is above
the threshold 1 âˆ’ ğ‘. As before, we cannot ensure this property holds with probability one, so instead
we use the high-probability variant

Pğ‘ (ğ‘¥,ğ‘¦âˆ—) (1 âˆ’ Ëœğ‘“fast (ğ‘¥) â‰¤ ğ‘ | ğ‘¦âˆ— â‰  âˆ…) â‰¥ 1 âˆ’ ğœ–.
This approach is shown in the detect_ppl_fast subroutine in Figure 10. We note that this approach
does not provide guarantees as strong as the ones for image classificationâ€”in particular, there is
a chance that the false negative images ğ‘¥ of ğ‘“fast (i.e., ğ‘¥ does not satisfy (16)) will contain larger
numbers of ground truth bounding boxes compared to true positive images. Then, the recall at
the level of bounding boxes may be less than 1 âˆ’ 2ğœ–. However, we find that it works well in
practice; intuitively, ğ‘“fast is more likely to have false negative images that contain fewer ground
truth bounding boxes.

For ğ‘“fast, we use a variant of Faster R-CNN termed R50-FPN with 3Ã— learning rate schedule, which
is the fastest variant available. Then, we synthesize the parameters of ??1 and ??2 in Figure 10
using the synthesis examples. As before, all results are run on an Nvidia GeForce RTX 2080 Ti
GPU. In Figure 11, we show the recall (red), desired lower bound on recall (blue), and precision
(black) of our approach as a function of (d) ğœ–, (e) ğ›¿, and (f) the object category ğ‘¦. Similarly, we
show the running time (on the entire test set) of the combined program slow+fast (black), fast
alone (green), and slow alone (red). As can be seen, our approach reduces running time by more
than 2Ã— except in the case of â€œpersonâ€ (28% reduction) and â€œtruckâ€ (45% reduction). The person
speedup is relatively small because so many of the images in the COCO dataset contain people.
Compared to the image classification setting, we obtain a smaller speedup since the gap between
the fast and slow models is not as large, and also because we can only avoid using the slow model
for images that contain zero detections. Furthermore, comparing Figure 11 (c) and (f) (i.e., slow
alone vs. slow+fast, respectively), for categories â€œcarâ€ and â€œtruckâ€, we suffer no loss in precision,
though we suffer a small loss in precision for the others.

Finally, we note that in Figure 11 (e), for ğ›¿ = 0.15 and ğ›¿ = 0.2, the estimated recall falls slightly
below the desired lower bound on recall. This result is most likely due to random chance, either
because of randomness in the synthesis set or because these values are estimates based on a random
test set. In particular, 0.15 is a fairly high failure probability (note that the results across ğ›¿ are
correlated, since we are using the same synthesis and test sets across all ğ›¿).

Runtime monitoring. We use runtime monitors to check that our program meets the desired
bounds both in terms of error rate (the subroutine monitor_correctness in Figure 10) and running
time (the subroutine monitor_speed in Figure 10). These approaches are the same as for image
classificationâ€”the correctness monitor checks that the error rate (i.e., ğ‘“fast (ğ‘¥) concludes there are
no detections but ğ‘“ (ğ‘¥) â‰  âˆ…) is below the desired rate ğœ–, and the running time monitor checks that
ğ‘“ is not called too often (i.e., ğ‘“fast (ğ‘¥) concludes there are no detections sufficiently frequently).

To evaluate these monitors, we consider a shift from the default â€œlargeâ€ bounding boxes we use
to â€œsmallâ€ and â€œmediumâ€. Intuitively, the smaller bounding boxes correspond to objects farther in
the background, which are harder to detect but also tend to be less important (e.g., an autonomous
car may not care as much about detecting far-away pedestrians). The trends are as before. First, we
find that the monitors correctly prove correctness when there is no shift. Second, we find that the
running time does not increase due to the shift, so the running time monitor continues to prove
correctness. Finally, our correctness monitor rejects correctness for the shift to â€œsmallâ€ bounding
boxes; interestingly, it proves correctness for â€œmediumâ€ bounding boxes, which suggests that our
synthesized program generalizes to this case.

, Vol. 1, No. 1, Article . Publication date: October 2021.

