Learning Aggregations of Binary Activated Neural Networks with
Probabilities over Representations

1
2
0
2

t
c
O
9
2

]

G
L
.
s
c
[

2
v
7
3
1
5
1
.
0
1
1
2
:
v
i
X
r
a

Louis Fortier-Dubois

Gaël Letarte

Benjamin Leblanc

Pascal Germain

François Laviolette

Université Laval

Abstract

Considering a probability distribution over pa-
rameters is known as an eﬃcient strategy to
learn a neural network with non-diﬀerentiable
activation functions. We study the expecta-
tion of a probabilistic neural network as a
predictor by itself, focusing on the aggrega-
tion of binary activated neural networks with
normal distributions over real-valued weights.
Our work leverages a recent analysis derived
from the PAC-Bayesian framework that de-
rives tight generalization bounds and learning
procedures for the expected output value of
such an aggregation, which is given by an an-
alytical expression. While the combinatorial
nature of the latter has been circumvented by
approximations in previous works, we show
that the exact computation remains tractable
for deep but narrow neural networks, thanks
to a dynamic programming approach. This
leads us to a peculiar bound minimization
learning algorithm for binary activated neural
networks, where the forward pass propagates
probabilities over representations instead of
activation values. A stochastic counterpart
of this new neural networks training scheme
that scales to wider architectures is proposed.

1

INTRODUCTION

The computation graphs of deep neural networks (a.k.a.
architectures) are challenging to analyze, due to their
multiple composition of non-linearities [Ba and Caru-
ana, 2014], overparametrization [Zhang et al., 2017] and
highly non-convex learning objective [Dauphin et al.,
2014, Choromanska et al., 2015]. Studying simpler
models seems a sensible strategy to gain insight on
neural network behavior and state performance guar-
antees [e.g. Arora et al., 2019, Belkin et al., 2020]. Our
work starts from one possible simpliﬁcation, obtained

by considering the binary activation function, meaning
that each neuron outputs only one bit of information
instead of the many bits needed to represent a real
number. Note that the use of neural networks involv-
ing binary weights, with or without binary activation
[Courbariaux et al., 2015, Hubara et al., 2016], have
been suggested for reducing their resource consumption,
and these may be especially useful in view of using a
pre-trained network for forward propagation on embed-
ded systems, but this is not our primary objective.

We study aggregations of binary activated networks
with real-valued parameters. These are viewed as el-
ementary pieces over which we consider a probability
distribution. This is motivated by the recent analysis
of Letarte et al. [2019], rooted in the PAC-Bayes theory
[McAllester, 1999]: they propose a learning objective
derived from a high-conﬁdence generalization bound,
and they empirically show that minimizing this objec-
tive provides a predictor with both tight and theoreti-
cally sound guarantees. They also derived an analytical
expression for the expected output value of binary ac-
tivated networks sampled from Gaussian distributions.
Being diﬀerentiable, this expression enables gradient
descent optimization, and is to be added to methods to
train networks with non-diﬀerentiable activation func-
tions [Williams, 1992, Bengio et al., 2013, Hubara et al.,
2017]. However, in order to preserve valid PAC-Bayes
guarantees and be able to train large neural networks,
Letarte et al. [2019] relies on an approximation: neu-
rons on the same layer all treat their inputs as if they
were independent draws of the same probability distri-
bution, when in fact all the inputs must correspond
to the same draw. This recalls the mean-ﬁeld approxi-
mation performed by Soudry et al. [2014] for learning
binary activated networks in a Bayesian setting. Not
only does this make the algorithm output values that
are slightly straying from the true aggregation expec-
tation, it also fatally increases the PAC-Bayes bound
for deeper architectures.

Our contribution overcomes this limitation : we show
that it is possible to ﬁnd the exact expectation of an ag-

 
 
 
 
 
 
Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

gregation of binary activated networks in time exponen-
tial in the width of the network, and linear in its depth,
and present a stochastic version with sub-exponential
time complexity regarding the network width. The orig-
inality of our proposed learning algorithm relies on the
fact that it computes probabilities of occurrences of the
hidden layer representations. Doing so, not only our
algorithm allows us to obtain non-vacuous PAC-Bayes
bounds for very deep binary activated networks, but
reveals itself as an interesting prediction mechanism.
Noteworthy, we show that once the parameters are op-
timized, the prediction on a new example is achievable
with a time complexity that remains constant relatively
to the network depth.

2 BACKGROUND AND NOTATION

We focus our study on the task of binary classiﬁcation,
using binary activated multilayer (BAM) neural net-
works, i.e., networks where each neuron either outputs
−1 or +1, using the sign function:

sgn(x) =

(cid:40)

−1 if x < 0,
+1 if x ≥ 0.

We suppose fully connected BAMs of L ∈ N+ layers
of size dk ∈ N+, ∀k ∈ {1, 2, . . . , L}, and inputs of
size d0 ∈ N+. We ﬁx dL = 1, whose output is the
classiﬁcation output of the whole network. We call
L the depth of the network, dk the width of the kth
layer, and maxk∈{1,2,...,L} dk the width of the network.
The sequence (cid:104)dk(cid:105)L
k=0 constitutes a (fully connected)
architecture. Unlike in binary neural networks [e.g.
Courbariaux et al., 2015], the parameters of a BAM are
not constrained to be binary, but only activations are
binary-valued. We thus have weights Wk ∈ Rdk×dk−1
and biases bk ∈ Rdk , for k ∈ {1, 2, . . . , L}. That being
said, in the remaining, the equations will be stated
without loss of generality in terms of the weights Wk
only. Therefore, a BAM B is totally deﬁned by the
tuple B := (cid:104)Wk(cid:105)L
k=1. See Fig. 1a for an example of
architecture. The 0th layer is the input layer, the 1st
one is the leading hidden layer, any kth layer, with
1 < k < L, is simply called a hidden layer and the Lth
one is the output layer.

Following the premises of Letarte et al. [2019], we
consider a generalization over BAMs, which we call an
aggregation of BAMs.

Deﬁnition 1. An aggregation of BAMs with mean pa-
rameters BM = (cid:104)Wk(cid:105)L
k=1, denoted A(BM ), is given by
a Gaussian probability distribution over all parameters,
centered in BM , with identity covariance matrix.

The BAM forward propagation process consists of com-
puting the following function for an input x ∈ Rd0,

where sgn is deﬁned element-wise:

FB(x) = sgn(WLsgn(WL−1sgn(. . . sgn(W1x) . . . ))) .

In other words, the output of a BAM B is FB(x) :=
F L

B (x), supposing the recursive equations
(cid:40)sgn(W1x)

if k = 1,

F k

B (x) =

sgn(WkF k−1

B

(x)) otherwise.

On the other hand, the output of an aggregation of
BAMs is the expected output of a BAM drawn from
the parameters distribution:

FA(BM )(x) = EB∼A(BM ) FB(x).

(1)

Such aggregation of BAMs is our main object of study.
Note that the forthcoming Section 4 is dedicated to
a PAC-Bayesian treatment where A(BM ) is consid-
In line with the
ered as a posterior distribution.
(PAC-)Bayesian literature, we call the single BAM net-
work BM the Maximum-A-Posteriori (MAP) predictor.

3 THE AGGREGATION OUTPUT

We present a recursive formulation to compute the
exact aggregation output, denoted FA(BM )(x), which
is diﬀerentiable end-to-end. The following formulation
inherits from the analysis of Letarte et al. [2019], but
is expressed in terms of the individual probabilities of
representations for each layer.

First, given a ﬁxed representation (i.e., an input vector
a ∈ Rd), the expected output of a single neuron with
sign activation over an isotropic Gaussian distribution
centered on weights w ∈ Rd is

E
v∼N (w,I)

sgn(v · a) = erf

(cid:18) w · a
√
2(cid:107)a(cid:107)

(cid:19)

,

(cid:82) x
0 e−t2

where erf(x) = 2√
dt is the Gauss error func-
π
tion. However, as the input vector of a neuron relies
on the distribution of weights on the previous layers,
the representation itself is a random vector. A neuron
outputs a value s ∈ {−1, 1} with probability

Pr(fw(a) = s) =

1
2

+

s
2

erf

(cid:18) w · a
√
2(cid:107)a(cid:107)

(cid:19)

.

(2)

Thus, the probability of observing a speciﬁc representa-
tion s = (s1, . . . , sdk ) ∈ Rk := {−1, 1}dk at layer k can
be expressed in a recursive manner, having the events
k := F k
as

B (x) = s and a¯s

k−1 := F k−1

(x) = ¯s :

B

Pr(as

k) =

d1(cid:89)

(cid:18) 1
2

+

si
2

erf






i=1

(cid:88)

¯s∈Rk−1

(cid:19) (cid:19)

(cid:18) Wi
√

1 · x
2(cid:107)x(cid:107)
k−1) Pr(a¯s

Pr(as

k | a¯s

k−1) otherwise.

if k = 1,

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

...

Output layer

Hidden layer

Hidden layer

Leading hidden layer

Input layer

(a) A typical BAM architecture.

(b) The ABNet architecture, with un-
derlying BAM from (a).

(c) The Compact ABNet structure,
obtained from the ABNet of (b).

Figure 1: The BAM shown in (a) has depth L = 4, and widths (cid:104)dk(cid:105)4
k=0 = (cid:104)4, 3, 4, 2, 1(cid:105). In (b), hidden layers have
a width of 2dk (i.e., one neuron per binary representation obtainable from a layer width of dk in the underlying
BAM). In (c), all hidden layers are merged, leaving a depth of 2 (as explained in Section 5).

The base case of the above recursion corresponds to
the leading hidden layer (i.e., F 1
B). The probability of
observing the representation s ∈ R1 on this ﬁrst hidden
layer given an input x amounts to be the product of the
probability associated with the d1 individual neuron
values (Equation 2).

B, . . . , F L

The general case corresponds to the subsequent hidden
layers (i.e., F 2
B ). The probability of observing
the representation s ∈ Rk is decomposed into the sum
of the 2dk−1 probabilities of observing the representa-
tion ¯s ∈ Rk−1 on the previous hidden layer (obtained
recursively) and the conditional probability (obtained
from Equation (2), using the fact that (cid:107)¯s(cid:107)2 = dk−1) :

Pr(as

k | a¯s

k−1) =

(cid:32)

dk(cid:89)

i=1

(cid:32)

1
2

+

si
2

erf

Wi
k · ¯s
(cid:112)2dk−1

(cid:33) (cid:33)

.

B (x)=1) − Pr(F L

Finally, assuming the output layer is only one neu-
ron wide, the exact output of the aggregation can
be computed with: FA(BM )(x) = EB∼A(BM ) FB(x) =
Pr(F L
B (x)= − 1).
By contrast, the proposed PBGNet algorithm of Letarte
et al. [2019, Eq. (16)] computes F L
P BG(x) given the
following equations (equivalent to the ones above only
when L ≤ 2), with ci = (cid:0) 1
2 + ¯si
(cid:17)

P BG(x))i(cid:1):

2 (F k

if k = 1,

F k

P BG(x) =






erf

(cid:16) W1·x√

2(cid:107)x(cid:107)

(cid:88)

(cid:18)

erf

¯s∈Rk−1

Wk·¯s√
2dk−1

(cid:19) dk(cid:89)

i=1

ci

otherwise.

This comes down to outputting at each layer only the
expectation of the BAM representation given the ex-
pectation of the previous layer, instead of our com-
plete probability distribution. Their method therefore
deletes information at each layer since the expectation
does not carry the correlation between each individual

neuron output. This approximation recalls the mean-
ﬁeld one, on which the Bayesian analysis of binary
activated networks of Soudry et al. [2014] relies. We
avoid such approximations, and we compute FA(BM )(x)
by a dynamic programming approach, described next.

Dynamic program. Posing1

Ψk =

(cid:20) dk(cid:89)

i=1

(cid:18) 1
2

+

si
2

erf(cid:0) Wi

k · ¯s
(cid:112)2dk−1

(cid:19)(cid:21)

(cid:1)

, (3)

s∈Rk,¯s∈Rk−1

one can obtain straightforwardly the probability vector
Pk = (cid:2) Pr(F k
= Ψk · Pk−1 . Starting with

B (x)=s)(cid:3)

s∈Rk

P1(x) =

(cid:20) d1(cid:89)

i=1

(cid:18) 1
2

+

si
2

erf(cid:0) Wi
√

1 · x
2(cid:107)x(cid:107)

(cid:19)(cid:21)

(cid:1)

s∈R1

,

(4)

and computing Pk for k ∈ {2, 3, . . . , L} in ascending
order, it is therefore possible to compute the exact
expectation of a BAM B ∼ A in time exponential in
B’s width, but only linear in its depth.

The previous formulas lead to what stands as the for-
ward propagation process of our new neural network,
which we name ABNet for Aggregation of Binary ac-
tivated Networks (see Algorithm 1 in Appendix A).
From parameters BM , it computes FA(BM )(x). The
computation graph of ABNet is illustrated by Fig. 1b.
The width of input and hidden layers k < L are of expo-
nential size 2dk relatively to the width dk of the BAM
networks it aggregates. Each layer of ABNet outputs a
probability distribution over all possible conﬁgurations
of the underlying BAM. The next layer then multiplies
those probabilities by the conditional probabilities Ψ,
which is just a reorganization of the weights and is
totally independent of the input x. As a result, AB-
Net applies only linear functions on hidden and output

1Abusing notation a little, when writing (cid:2)g(s)(cid:3)

we

s∈R

assume all s are taken in lexicographical order.

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

layers; an observation we discuss further in Section 5.
Since every operation of ABNet is diﬀerentiable, we
can use an automatic diﬀerentiation mechanism to eﬃ-
ciently compute the derivatives, then train the network
via backpropagation and stochastic gradient descent.

Stochastic version. ABNet has many interesting
theoretical properties, but the necessity of computing
the probability of every combination of neuron outputs
at a given layer makes it too cumbersome for practi-
cal applications. We propose a stochastic version of
ABNet, which keeps its property of avoiding the mean
ﬁeld approximation while limiting the computation
complexity with regard to the width to a quadratic
one. This is achievable by picking a constant number
n of representations R(cid:48)
k uniformly from Rk at layer k,
computing only the occurring probability of those n
representations (replacing representation sets Rk by
their uniformly drawn counterparts R(cid:48)
k in Equations 3
and 4), and normalizing at each layer by dividing Pk
by (cid:80)
Pk[sk] (see Algorithm 2 in Appendix A).

sk∈R(cid:48)
k

Complexity. Assuming every layer has the same
width d at each layer, the complexity of PBGNet is
O(L2dd2) (or O(Lnd2) for its stochastic counterpart,
with n samples), while the complexity of ABNet is
O(L22dd2) (or O(Ln2d2) for the stochastic version).
See Appendix B for an empirical study of computing
times. A salient fact is that stochastic versions scale
much better on wide architectures (Fig. 4).

4 BOUNDING AND OPTIMIZING
THE GENERALIZATION LOSS

Initiated by McAllester [1999], the PAC-Bayes the-
ory allows one to bound the generalization error of a
learned predictor without requiring a validation set,
under the sole assumption that data is sampled in an
iid way from the unknown distribution D. To be eli-
gible for a PAC-Bayesian treatment, predictors must
be expressed through a posterior probability distribu-
tion over a predeﬁned class of hypotheses. Despite the
fact that neural networks are not naturally deﬁned as
such, many valuable analyses have been proposed by
applying a PAC-Bayesian theory to stochastic variants
of deterministic neural networks [Langford and Caru-
ana, 2001, Dziugaite and Roy, 2018, Zhou et al., 2019,
Pérez-Ortiz et al., 2020, Pitas, 2020] by considering
perturbations (typically Gaussian distributed noise) on
the weights. This strategy can be applied to any neu-
ral network topology and activation functions, but the
generalization bounds do not apply to the underlying
deterministic (non-perturbed) network.

over simple BAM networks (Equation 1), the PAC-
Bayesian bound applies to the output of ABNet. The
forthcoming Theorem 2 provides high conﬁdence up-
per bound for the generalization loss of a learned AB-
Net, deﬁned as LD(FA) = E(x,y)∼D (cid:96)(FA(x), y), where
(cid:96)(y(cid:48), y) = 1
2 (1 − yy(cid:48)) ∈ [0, 1] is the linear loss for the
binary classiﬁcation problem. The two main quantities
involved in the computation of the bound are the empir-
i=1 ∼ Dn,
ical loss on the learning sample S = {(xi, yi)}n

ˆLS(FA) =

1
n

n
(cid:88)

i=1

(cid:96)(FA(xi), yi) ,

(5)

and the Kullback-Leibler (KL) divergence between
the learned parameters (posterior distribution) B =
k=1 and a reference (prior distribution) Bp =
(cid:104)Wk(cid:105)L
(cid:104)Wp
k=1 which is independent of the training data.2
k(cid:105)L
By using isotropic Gaussians for both the prior and the
posterior, the KL divergence is easily obtained with

KL(B(cid:107)Bp) =

1
2

L
(cid:88)

(cid:107)Wk − Wp

k(cid:107)2.

(6)

k=1

The PAC-Bayes theorem below is borrowed from
Letarte et al. [2019], which itself is a variation from a
seminal result from Catoni [2007], but has the major
advantage to directly deal with the trade-oﬀ between
the empirical loss and the KL divergence (i.e., the value
of C in Equation (7) is the one minimizing the bound).
Theorem 2. Given a data independent prior distri-
bution Bp and δ ∈ (0, 1), with probability at least 1 −
δ over a realization of the learning sample S ∼ Dn,
the following holds for all posterior B:

(cid:26)

LD(FA) ≤ inf
C>0

(cid:16)

1 − exp

(cid:104)
−C ˆLS(FA) −

KL(B(cid:107)Bp) + ln 2

(cid:17)(cid:105)(cid:17) (cid:27)

n

.

(7)

√

δ

1
1−e−C
(cid:16)

1
n

A salient feature of the PAC-Bayesian bounds is that
they are uniformly valid (with probability at least 1−δ)
for the whole family of posterior. This is particularly
suited for the design of a bound minimization algorithm,
as the right-hand side of Equation (7) suggests an
objective to minimize, and is providing a generalization
guarantee even when the optimization procedure does
not converge to a global minimum. Thus, we propose to
train the ABNet architecture by minimizing the bound
given in Theorem 2 by stochastic gradient descent.
That is, the following objective is optimized according
to parameters B and C > 0, where d = ln 2

√

n

:

δ

(cid:16)

1
1−e−C

1 − exp

(cid:104)
−C ˆLS(FA) − KL(B(cid:107)Bp)+d

n

(cid:105)(cid:17)

.

(8)

By adopting the construction of Letarte et al. [2019]
and designing our predictor natively as a distribution

2Following Dziugaite and Roy [2018], Letarte et al. [2019],
Pitas [2020], we chose an SGD random initialization as Bp.

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

Although this objective is similar to the learning al-
gorithm PBGNet [Letarte et al., 2019], the proposed
ABNet objective diﬀers in two noticeable ways for net-
works of depth L > 2:
(1) The predictor output, and therefore the empirical
loss of Equation (5), corresponds to the exact BAM
expectation and is computed thanks to the forward
propagation routine of ABNet, instead of the more
classical computational graph of PBGNet.
(2) The KL divergence of Equation (6), acting as a reg-
ularization term, does give the same penalty to weights
of every layer in ABNet. In contrast, the corresponding
term in PBGNet penalizes weights by a growing factor
according to the layer depth.

5 COMPACTING THE ABNET

Recall from Section 3 that the aggregation output can
be computed with the matrix product
FA(B)(x) = (cid:2)1, −1(cid:3)·(ΨL(ΨL−1(. . . Ψ3(Ψ2P1(x)) . . . ))),
with (cid:104)Ψk(cid:105)L
k=2 and P1(x) computed from parameters
B = (cid:104)Wk(cid:105)L
k=1 according to Equations (3) and (4).
From this point of view, ABNet simply computes a
linear function of the leading hidden layer represen-
tation P1(x), highlighting a limitation of all binary
(and discrete-valued) activated neural networks. In-
deed, all matrices Ψk are solely based on the weights
and do not rely on the input layer. Since there is no
activation function between hidden layers, dot product
associativity allows us to state the following.
Proposition 3. The output of an aggregation of BAMs
A(B), where B = (cid:104)Wk(cid:105)L
k=1, with leading hidden layer
width d1, and other hidden layers of arbitrary width,
can be obtained by forward propagating in a compact
(with regard to depth) neural network having a leading
hidden layer of width 2d1 with erf activation and an
output layer of width 1 with identity activation:

FA(x) = H · P1(x) ,

(9)

where P1(x) is a vector of 2d1 elements deﬁning a
probability distribution on the outputs of the leading
hidden layer of B on x, and H ∈ [−1, 1]2d1 is a vector
giving the expected output of the rest of the network
given the output of the ﬁrst layer, such that
H = (cid:2)1, −1(cid:3) · ΨL · ΨL−1 · ... · Ψ3 · Ψ2 .

(10)

Since only P1(x) changes in function of x, for ﬁxed
(cid:3)
weights one can numerically precompute H := (cid:2)hs
s∈R1
once and for all x.
In the underlying BAM this is
analogous to precomputing the output for every repre-
sentation outputted by the leading hidden layer. Every
entry of H is a real number between −1 and 1 since it
represents an expectation on a BAM output. Therefore:

Corollary 4. Notwithstanding the fact that the un-
derlying BAM architecture can be arbitrarily deep, the
aggregation output can always be expressed in the fol-
lowing shallow form, with hs ∈ [−1, 1]:

FA(x) =

d1(cid:89)

(cid:88)

hs

s∈R1

i=1

(cid:18) 1
2

+

si
2

erf

(cid:18) Wi
√

1 · x
2(cid:107)x(cid:107)

(cid:19) (cid:19)

.

(11)

Thus, forward propagation of ABNet can be computed
in time constant with regard to L.

We call the algorithm that computes Equation (11) the
Compact ABNet. See Fig. 1c for a visual representation.
Interestingly, the PAC-Bayes generalization bound of
Theorem 2 is not obtainable directly from the Compact
ABNet parameters. Therefore, our bound minimization
algorithm requires the ABNet architecture. Notewor-
thy, our empirical experiments (Section 6, Fig. 3) show
that training deeper ABNet can achieve better general-
ization than a shallower architecture, even when both
share a Compact architecture of the same size. Com-
pacting our stochastic version is also possible. Since the
dot product must be executed on ﬁxed R(cid:48)
ks, the drawn
samples must be predetermined and remain the same
at each inference; this leads to a very concise classiﬁer
which performs just as well as the last learned Stochas-
tic ABNet. As can be seen in Fig. 5 from Appendix B,
compact networks are much faster at inference time
than their deep equivalent, as their complexity does
not increase with depth.

Our original approach of propagating probabilities over
representations is what brings the light on the com-
pactability phenomenon. It is a well-known result that
any function can be approximated to an arbitrary level
of accuracy with a neural network having as few as
one hidden layer, given that the layer is wide enough
[Hornik et al., 1989]. It has also been shown that a
shallow "student" network can learn to mimic a deep
"teacher" network to reach the same performance level
[Ba and Caruana, 2014]. However, typical neural net-
works do not allow such an explicit construction that
maps an initially (non-linear) deep structure to a shal-
low form. The result of Proposition 3 is a curiosity
that is worth analyzing further. Remarkably, there is a
clear dichotomy between the roles of P1(x) and H: the
former transforms data points into a probability distri-
bution over the leading hidden layer representations,
whereas the latter gives the aggregation output for each
of those representations. Put otherwise, the ﬁrst layer
serves as an embedding and the rest of the layers oper-
ate as a classiﬁer. Fig. 2 illustrates the particularity of
the prediction mechanism of ABNet.

The leading hidden layer. Equation (9) implies
that the leading hidden layer deﬁnes regions in the input
space. All subsequent hidden layers together express

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

the output value of these regions. Fig. 2a shows how
those regions divide the input space on the toy problem.
Note that each region is associated with one of the four
leading hidden layer binary representation.

Each neuron of the leading hidden layer of a BAM
deﬁnes a hyperplane in the input space, where inputs
on one side are mapped to −1, and 1 on the other side.
Considering all regions that are enclosed between the
hyperplanes yields up to 2d1 regions, corresponding
to the 2d1 output representations R1. Many of those
regions may stray very far from the actual data. For
example in Fig. 2a the region corresponding to repre-
sentation (1, −1) exists on the other side of where the
two planes meet, which is far from any existing data3.

Additional hidden layers. The vector H represents
by extension a function from {−1, 1}d1 to [−1, 1]. Its
role is to determine what sign should be outputted for
each region deﬁned by the leading hidden layer, with
a conﬁdence term. Its content is not arbitrary since it
must be obtained from the weights of the subsequent
hidden layers as in Equation (10). Depth therefore
adds expressivity to BAM aggregations by allowing
regions created by the leading hidden layer to output
uncorrelated signs, with more or less conﬁdence.

As illustrated by Figures 2c-d, taking the output of
ABNet is not equivalent as taking the output of its asso-
ciated MAP. For the same parameters, the aggregation
allows more complex regions than BAMs, taking advan-
tage that an input can belong to several regions, with
certain probabilities. Indeed, there exist points x and
parameters BM for which sgn(FA(BM )(x)) (cid:54)= FBM (x).
For instance, many incorrectly classiﬁed −1 data points
in Fig. 2d fall within the correct region in Fig. 2c be-
cause ABNet can compensate the proximity to the
central +1 region with a lesser proximity with two −1
regions. It is therefore worthy to use the aggregation
as a predictor by itself instead of its MAP, for its ex-
pressive power. Indeed, Zhu et al. [2019] observe that
ensemble methods on binary neural networks confer
stability with regard to input and parameter perturba-
tions, which leads to better generalization.

6 NUMERICAL EXPERIMENTS

We evaluated our proposed approach ABNet by follow-
ing the experimental framework of Letarte et al. [2019],
on the same six binary classiﬁcation datasets: ads and
adult from the UCI repository [Dua and Graﬀ, 2017],

3By considering the few most important region, one
could potentially interpret ABNet predictions more easily
than for classical neural networks. We consider this question
as future work, and refer the reader to Montúfar et al. [2014]
for a study of regions in the broader context of neural
networks with continuous activations.

along with four MNIST [LeCun et al., 1998] binary vari-
ants mnistLH (labels {0, 1, 2, 3, 4} form the "Low" class,
and {5, 6, 7, 8, 9} the "High" class), mnist17, mnist49
and mnist56 (only examples labeled respectively 1&7,
4&9, and 5&6 are retained). As the exact versions of
PBGNet and ABNet are limited by their exponential
complexity regarding their width, we explored narrow
network architectures (widths d ∈ {2, 4, 8}) and wider
architectures (widths d ∈ {10, 50, 100}) accessible only
to stochastic versions, all for 1 to 3 hidden layers.

We ﬁrst compare ABNet to its direct counterpart
PBGNet [Letarte et al., 2019], both directly optimizing
the PAC-Bayesian generalization bound during learn-
ing, with the prior distribution deﬁned by the network
weights random initialization. We also explore the mini-
mization of the empirical loss with the variants ABNet(cid:96)
(Eq. 5) and PBGNet(cid:96), where 20% of the training data
is used as a validation set for model selection.

Even if our work focuses on the learning of an aggrega-
tion of BAM, the optimization procedure of PBGNet
and ABNet may be used to learn a single binary acti-
vated network, as a BAM itself is not learnable with
standard gradient descent methods. We thus com-
pare the Maximum-A-Posteriori (MAP) networks of
both aggregated methods to three algorithms of the
literature for learning neural networks with binary
weights and/or activations: Expectation Backpropa-
gation [Soudry et al., 2014] (EBP) with real-valued
weights and binary activations, Binarized Neural Net-
work [Hubara et al., 2016] (BNN) with both binary
weights and activations, and BinaryConnect [Cour-
bariaux et al., 2015] (BC) with binary weights but
ReLU activations. Experiments involving EBP, BNN
or BC are performed using fully connected networks,
following the procedure used for ABNet(cid:96) and PBGNet(cid:96).
Results of the experiments are presented in Table 1.
Note that standard deviations on each bound and
train/test error are presented in Appendix C, along
with results on BC and BNN (as these models don’t rely
on aggregation according to a posterior distribution).

Narrow architectures. The PAC-Bayesian inspired
models with empirical loss minimization (PBGNet(cid:96) and
ABNet(cid:96)) obtain competitive error rates (similar to the
results achieved by BC using ReLU activations and bi-
nary weights, see Appendix C). However, the empirical
loss minimization procedure lead to non-informative
generalization bounds values. When considering the
bound for optimization and model selection for PBGNet
and ABNet, selected network architectures are smaller
with usually a single hidden layer, as the objective func-
tion contains a regularization term on the weight values
(see Eq. 8), and the error rates grow while bound values
improve to a relevant level. We also notice that bound
minimization algorithms are far less prone to overﬁt-

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

(a) For all x, the probabilities of outputting a speciﬁc representation at the leading hidden layer, obtained from P1(x),
multiplied by the expected output for each of these regions, given by H.

The

aggregation

(b)
output
FA(BM )(x) for all x, obtained by
summing over all representations.

(c) Using ABNet as a classiﬁer, one can
simply use sgn(FA(BM )(x)) as the predic-
tion for an input x.

(d) Using the BAM classiﬁer from the
MAP FBM (x), the decision boundary
is less expressive.

Figure 2: Predictions of a learned ABNet and its underlying BAM with architecture (cid:104)2, 2, 2, 1(cid:105), i.e., with
two-dimensional inputs and two hidden layers of two neurons, on a toy dataset.

Figure 3: Impact of depth for PBGNet (dashed) and ABNet (solid) on test errors and bound values according
to the width for mnistLH datasets. Results correspond to means and standard deviations over 5 repetitions.

ting than traditional optimization schemes, as their
training errors are remarkably close to their testing
errors (recall that PBGNet and ABNet are equivalent
for one hidden layer). On the larger and harder dataset
mnistLH, the narrow ABNet achieves a better error
rate and bound value than PBGNet by selecting a
deeper architecture thanks to its less penalizing KL
divergence regularization. On the performances of the
MAP induced BAM networks, error rates are usually
similar or slightly higher than the aggregated counter-
part. Hence, these approaches can be considered as
suitable algorithms to learn BAM networks.

Wide architectures. For all algorithms and most
datasets, obtained results for wide and narrow binary
neural networks are surprisingly similar. This reveals
that constraining ABNet’s width to compute the exact
aggregation output is not a major caveat. In particular,
when one seeks tight PAC-Bayesian guarantees, lower
complexity of narrow models should be favored. That
being said, the proposed stochastic training for ABNet

enables scaling to wider networks. While achieving
most of the time comparable results to the stochas-
tic PBGNet, the obtained risk on the large mnistLH
dataset suggests that the approximation scheme of AB-
Net may not be as eﬀective as the exact computation.

Deep architectures. A key improvement of AB-
Net over PBGNet is the KL divergence computation
which is not hindered by a growing factor penalizing
the weights according to the network depth. This prop-
erty should allow ABNet to learn deeper networks with
tighter generalization bounds, which we investigated
on the mnistLH dataset by extending the main exper-
iment up to 8 hidden layers. Results are presented
in Figure 3 where the diﬀerence of behavior between
the models is clearly highlighted. For a small num-
ber of hidden layers, the performances are similar, but
as the number of hidden layer grows, bound values
for PBGNet sharply rise and test error rates degrade
signiﬁcantly. On the other hand, bound values are
relatively stable for ABNet, indicating its potential to

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

Table 1: Experiment results. On narrow architectures (left column), standard versions of PBGNet and ABNet are
used, while their stochastic versions are used on wide architectures (right column). For each dataset and model,
the best-performing set of parameters over the ﬁve repetitions is retained, and we show the number of hidden
layers (L−1), hidden size (d), bound value and error rate on the train data (ErrorS) and on the test data for the
model (ErrorT ) and the associated Maximum-A-Posteriori BAM (MAP). See Appendix C for extended results.

Dataset Model

L−1 d Bound ErrorS ErrorT MAP L−1

d Bound ErrorS ErrorT MAP

Narrow architectures

Wide architectures

ads

adult

mnist17

mnist49

mnist56

mnistLH

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP

3
3
3
3
2

1
1
3
3
1

1
1
3
3
1

1
1
2
3
3

1
1
2
3
3

1
3
3
2
3

2 0.192
2 0.192
4 1.000
4 0.887
2

–

2 0.208
2 0.208
4 0.723
4 0.780
8

–

2 0.036
2 0.036
4 1.000
2 0.829
2

–

2 0.136
2 0.136
4 1.000
8 1.000
8

–

2 0.084
2 0.084
8 1.000
8 0.999
8

–

8 0.186
4 0.162
8 1.000
8 0.998
8

–

0.140
0.140
0.018
0.015
0.003

0.157
0.157
0.135
0.132
0.145

0.005
0.005
0.002
0.001
0.000

0.036
0.036
0.008
0.004
0.020

0.021
0.021
0.004
0.004
0.001

0.091
0.056
0.018
0.025
0.016

0.141
0.141
0.026
0.026
0.040

0.160
0.160
0.149
0.149
0.152

0.006
0.006
0.005
0.004
0.006

0.036
0.036
0.020
0.017
0.033

0.023
0.023
0.011
0.009
0.010

0.092
0.058
0.038
0.042
0.043

0.141
0.141
0.027
0.026
0.054

0.158
0.158
0.156
0.150
0.166

0.006
0.006
0.005
0.004
0.006

0.036
0.036
0.020
0.017
0.034

0.023
0.023
0.011
0.009
0.015

0.093
0.059
0.047
0.043
0.082

3
2
3
2
3

1
1
2
3
2

1
1
2
3
2

1
1
1
3
2

1
1
1
1
2

1
2
1
3
1

10
10
10
50
10

10
10
10
10
100

10
10
10
10
10

10
10
50
10
10

10
10
50
10
10

0.213
0.216
1.000
1.000
–

0.216
0.216
0.360
0.541
–

0.041
0.041
1.000
0.607
–

0.149
0.147
0.992
1.000
–

0.090
0.090
0.974
1.000
–

0.167
10
10
0.187
100 0.998
0.895
10
–
100

0.140
0.140
0.020
0.020
0.005

0.156
0.156
0.146
0.143
0.049

0.005
0.005
0.002
0.002
0.000

0.037
0.038
0.004
0.024
0.001

0.023
0.023
0.003
0.010
0.000

0.058
0.087
0.006
0.050
0.001

0.141
0.141
0.026
0.026
0.035

0.159
0.160
0.151
0.151
0.186

0.006
0.006
0.005
0.005
0.005

0.037
0.037
0.012
0.029
0.021

0.025
0.025
0.008
0.017
0.019

0.059
0.088
0.022
0.060
0.027

0.141
0.141
0.028
0.025
0.049

0.158
0.158
0.164
0.151
0.189

0.006
0.006
0.006
0.005
0.005

0.036
0.036
0.012
0.027
0.026

0.024
0.024
0.008
0.016
0.021

0.060
0.087
0.024
0.058
0.032

learn deep neural network architectures (the minimum
bound is achieved for 3 hidden layers of width 4, which
adequately indicates the best test error).

7 CONCLUSION

Many desirable properties stem from considering ag-
gregations over binary activated networks with our
algorithm ABNet. As PBGNet [Letarte et al., 2019],
it gives a sensible way to optimize binary activated
neural network parameters. It also provides tighter
PAC-Bayes bounds on the generalization error of deep
architectures, and leads to a surprisingly simple compu-
tation for ﬁnding an equivalent shallow network which
emphasizes the role of each layer in a binary network.
Extending the analysis to more complex prior and pos-
terior distributions, such as Gaussian with diagonal
covariance, should be explored in future works.

While we are building on the PAC-Bayes analysis of
Letarte et al. [2019], our learning algorithm, and the
analysis of the dichotomy between the ﬁrst layers versus
the others, are completely new. We believe the latter
is a powerful tool for understanding the expressivity
conferred by a network’s architecture, which could
enhance our understanding of the role of depth in neural
networks, pursuing the work of Le Roux and Bengio
[2010], Szymanski and McCane [2012] and Kidger and
Lyons [2020], but in the context of model aggregation.

Societal Impact. When machine learning algorithms
are deployed in the real world, ethical concerns must be
considered. It is hazardous to use them in automated
decision systems that might aﬀect the welfare of indi-
viduals. The same applies to our algorithm; however,
our focus on interpretability and certiﬁability is in line
with the objective of attenuating those concerns.

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

References

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei
Hu. A convergence analysis of gradient descent for
deep linear neural networks. In ICLR, 2019.

Jimmy Ba and Rich Caruana. Do deep nets really need

to be deep? In NIPS, pages 2654–2662, 2014.

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models
of double descent for weak features. SIAM J. Math.
Data Sci., 2(4):1167–1180, 2020.

Yoshua Bengio, Nicholas Léonard, and Aaron C.
Courville. Estimating or propagating gradients
through stochastic neurons for conditional computa-
tion. CoRR, abs/1308.3432, 2013.

Olivier Catoni. PAC-Bayesian supervised classiﬁcation:
the thermodynamics of statistical learning, volume 56.
Inst. of Mathematical Statistic, 2007.

Anna Choromanska, Mikael Henaﬀ, Michaël Mathieu,
Gérard Ben Arous, and Yann LeCun. The loss sur-
faces of multilayer networks. In AISTATS, volume 38,
2015.

Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural net-
works with binary weights during propagations. In
NIPS, pages 3123–3131, 2015.

Yann N. Dauphin, Razvan Pascanu, Çaglar Gülçehre,
KyungHyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifying and attacking the saddle point problem in
high-dimensional non-convex optimization. In NIPS,
pages 2933–2941, 2014.

Dheeru Dua and Casey Graﬀ. UCI machine learning
repository, 2017. URL http://archive.ics.uci.
edu/ml.

Gintare Karolina Dziugaite and Daniel M. Roy. Data-
dependent PAC-Bayes priors via diﬀerential privacy.
In NeurIPS, pages 8440–8450, 2018.

Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
Multilayer feedforward networks are universal ap-
proximators. Neural networks, 2(5):359–366, 1989.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NIPS, 2016.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. Quantized neu-
ral networks: Training neural networks with low
precision weights and activations. JMLR, 18(1):6869–
6898, 2017.

Patrick Kidger and Terry J. Lyons. Universal approxi-
mation with deep narrow networks. In COLT, volume
125, pages 2306–2327, 2020.

Diederik P Kingma and Jimmy Ba. Adam: A method

for stochastic optimization. In ICLR, 2015.

John Langford and Rich Caruana. (Not) bounding the

true error. In NIPS, pages 809–816, 2001.

Nicolas Le Roux and Yoshua Bengio. Deep belief net-
works are compact universal approximators. Neural
computation, 22(8):2192–2207, 2010.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haﬀner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE, 86(11):
2278–2324, 1998.

Gaël Letarte, Pascal Germain, Benjamin Guedj, and
François Laviolette. Dichotomize and generalize:
Pac-bayesian binary activated deep neural networks.
In NeurIPS, pages 6869–6879, 2019.

David McAllester. Some PAC-Bayesian theorems. Ma-

chine Learning, 37(3):355–363, 1999.

Guido F. Montúfar, Razvan Pascanu, KyungHyun Cho,
and Yoshua Bengio. On the number of linear regions
of deep neural networks. In NIPS, pages 2924–2932,
2014.

María Pérez-Ortiz, Omar Rivasplata, John Shawe-
Taylor, and Csaba Szepesvári. Tighter risk certiﬁ-
cates for neural networks. CoRR, abs/2007.12911,
2020.

Konstantinos Pitas. Dissecting non-vacuous generaliza-
tion bounds based on the mean-ﬁeld approximation.
In ICML, 2020.

Daniel Soudry, Itay Hubara, and Ron Meir. Expec-
tation backpropagation: Parameter-free training of
multilayer neural networks with continuous or dis-
crete weights. In NIPS, 2014.

Lech Szymanski and Brendan McCane. Deep, super-
narrow neural network is a universal classiﬁer. In
IJCNN, pages 1–8. IEEE, 2012.

Ronald J. Williams.

Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3):229–256, May 1992.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-
jamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In ICLR,
2017.

Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P.
Adams, and Peter Orbanz. Non-vacuous generaliza-
tion bounds at the imagenet scale: a PAC-bayesian
compression approach. In ICLR, 2019.

Shilin Zhu, Xin Dong, and Hao Su. Binary ensem-
ble neural network: More bits per network or more
networks per bit? In CVPR, pages 4923–4932, 2019.

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

A ALGORITHMS

We provide a detailed view on the forward propagation algorithms of ABNet and its stochastic counterpart,
presented in Section 3. We use R(cid:48) ∼ Un(R) to denote that R(cid:48) consists of n draws (with replacement) from a
uniform distribution on R.

Algorithm 1 ABNet
Require: x ∈ Rd0 and BM = (cid:104)Wk(cid:105)L
k=1, with Wk ∈ Rdk−1×dk ∀1 ≤ k ≤ L
(cid:19)(cid:21)
(cid:1)

(cid:18)

(cid:20)

1

(cid:81)d1

1: P ←

2 + si

2 erf(cid:0) Wi
1·x
2(cid:107)x(cid:107)

√

1

i=1

s1∈R1

2: for k ∈ {2, . . . , L} do
(cid:20)
2 + si

(cid:81)dk
i=1

Ψ =

3:

(cid:18)

1

P ← Ψ · P
4:
5: return (cid:2)1, −1(cid:3) · P

k

2 erf(cid:0) Wi
√

k·sk−1
2dk−1

(cid:19)(cid:21)
(cid:1)

sk∈Rk,sk−1∈Rk−1

Algorithm 2 Stochastic ABNet
Require: x ∈ Rd0 , n ∈ N and BM = (cid:104)Wk(cid:105)L
1: R(cid:48)

1 ∼ Un(R1)
(cid:18)

(cid:20)

2: P ←

(cid:81)d1

i=1

1

2 + si

2 erf(cid:0) Wi
1·x
2(cid:107)x(cid:107)

√

1

(cid:19)(cid:21)
(cid:1)

s1∈R(cid:48)
1

3: P ←

P
s1∈R(cid:48)
1
4: for k ∈ {2, . . . , L} do

P[s1]

(cid:80)

k=1, with Wk ∈ Rdk−1×dk ∀1 ≤ k ≤ L

5:

6:

7:
8:

R(cid:48)

k ∼ Un(Rk)
(cid:18)
(cid:81)dk
i=1

(cid:20)

Ψ =

1

2 + si

2 erf(cid:0) Wi
√

k

k·sk−1
2dk−1

(cid:19)(cid:21)
(cid:1)

P ← Ψ · P
P ←

(cid:80)

P
sk ∈R(cid:48)
k

P[sk]

9: return (cid:80)

sL∈R(cid:48)
L

sLP[sL]

sk∈R(cid:48)

k,sk−1∈R(cid:48)

k−1

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

B COMPUTATION TIME

All experiments were performed on NVIDIA GPUs GeForce RTX 2080 Ti. We present an empirical study of
computation time needed by our four algorithms (ABNet, Stochastic ABNet, Compact ABNet and Compact
stochastic ABNet) and by the benchmark PBGNet (and its stochastic version) in Figures 4 and 5.

Figure 4: Time needed (in seconds) for the forward propagation of our four ABNet versions and the two versions
of the PBGNet benchmark, all with 6 hidden layers and with 100 samples for stochastic versions. Computations
were executed on a batch of 32 examples of the Ads dataset, averaged on 100 repetitions. Of note, the memory
requirements of our (non-stochastic) PBGNet and ABNet implementations exceed the available resources for
layer widths greater than 13.

Figure 5: Time needed (in seconds) for the forward propagation of our four ABNet versions and the two versions
of the PBGNet benchmark, all with width dk = 12 for 1 ≤ k < L, and with 100 samples for stochastic versions.
Computations were executed on a batch of 32 examples of the Ads dataset, averaged on 100 repetitions. See
Figure 4 for the legend.

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

C DETAILED EXPERIMENTAL RESULTS

Tables 2 and 3 present complete results for the experiments of Table 1, respectively for narrow and wide
architectures. Table 4 presents details about selected hyperparameters on stochastic PBGNet and ABNet. The
source code of our experiments will be released with the paper publication.

Table 2: Extended experiment results with standard deviation on narrow architectures (with PBGNet and ABNet
exact versions) on 5 repetitions for selected models: number of hidden layers (L−1), width (d), bound value and
error rate on both train (ErrorS) and test (ErrorT ) data and the associated Maximum-A-Posteriori BAM (MAP).

Dataset Model

L−1

d

Bound

ErrorS

ErrorT

MAP

ads

adult

mnist17

mnist49

mnist56

mnistLH

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

3
3
3
3
2
1
1

1
1
3
3
1
1
1

1
1
3
3
1
2
1

1
1
2
3
3
2
1

1
1
2
3
3
1
1

1
3
3
2
3
2
1

2
2
4
4
2
4
8

2
2
4
4
8
8
2

2
2
4
2
2
4
8

2
2
4
8
8
8
2

2
2
8
8
8
8
8

8
4
8
8
8
8
2

0.192 ± 0.004
0.192 ± 0.004
1.000 ± 0.001
0.887 ± 0.064
–
–
–

0.208 ± 0.001
0.208 ± 0.001
0.723 ± 0.031
0.780 ± 0.138
–
–
–

0.036 ± 0.000
0.036 ± 0.000
1.000 ± 0.000
0.829 ± 0.078
–
–
–

0.136 ± 0.002
0.136 ± 0.002
1.000 ± 0.000
1.000 ± 0.000
–
–
–

0.084 ± 0.001
0.084 ± 0.001
1.000 ± 0.000
0.999 ± 0.001
–
–
–

0.186 ± 0.028
0.162 ± 0.001
1.000 ± 0.000
0.998 ± 0.003
–
–
–

0.140 ± 0.004
0.140 ± 0.004
0.018 ± 0.005
0.015 ± 0.003
0.003 ± 0.002
0.025 ± 0.005
0.037 ± 0.002

0.157 ± 0.001
0.157 ± 0.001
0.135 ± 0.001
0.132 ± 0.005
0.145 ± 0.002
0.142 ± 0.002
0.180 ± 0.016

0.005 ± 0.000
0.005 ± 0.000
0.002 ± 0.001
0.001 ± 0.001
0.000 ± 0.001
0.004 ± 0.000
0.003 ± 0.001

0.036 ± 0.001
0.036 ± 0.001
0.008 ± 0.003
0.004 ± 0.002
0.020 ± 0.003
0.007 ± 0.002
0.030 ± 0.006

0.021 ± 0.003
0.021 ± 0.003
0.004 ± 0.001
0.004 ± 0.002
0.001 ± 0.002
0.002 ± 0.001
0.013 ± 0.003

0.091 ± 0.037
0.056 ± 0.001
0.018 ± 0.003
0.025 ± 0.008
0.016 ± 0.002
0.023 ± 0.002
0.123 ± 0.005

0.141 ± 0.012
0.141 ± 0.012
0.026 ± 0.004
0.026 ± 0.003
0.040 ± 0.008
0.031 ± 0.004
0.038 ± 0.004

0.160 ± 0.003
0.160 ± 0.003
0.149 ± 0.002
0.149 ± 0.003
0.152 ± 0.003
0.151 ± 0.002
0.182 ± 0.017

0.006 ± 0.001
0.006 ± 0.001
0.005 ± 0.001
0.004 ± 0.001
0.006 ± 0.001
0.010 ± 0.002
0.008 ± 0.001

0.036 ± 0.004
0.036 ± 0.004
0.020 ± 0.004
0.017 ± 0.003
0.033 ± 0.003
0.016 ± 0.002
0.037 ± 0.003

0.023 ± 0.006
0.023 ± 0.006
0.011 ± 0.003
0.009 ± 0.002
0.010 ± 0.004
0.009 ± 0.004
0.023 ± 0.003

0.092 ± 0.036
0.058 ± 0.002
0.038 ± 0.002
0.042 ± 0.006
0.043 ± 0.002
0.035 ± 0.001
0.133 ± 0.004

0.141 ± 0.012
0.141 ± 0.012
0.027 ± 0.004
0.026 ± 0.003
0.054 ± 0.006
0.031 ± 0.004
0.038 ± 0.004

0.158 ± 0.003
0.158 ± 0.003
0.156 ± 0.002
0.150 ± 0.003
0.166 ± 0.005
0.151 ± 0.002
0.182 ± 0.017

0.006 ± 0.001
0.006 ± 0.001
0.005 ± 0.001
0.004 ± 0.001
0.006 ± 0.001
0.010 ± 0.002
0.008 ± 0.001

0.036 ± 0.004
0.036 ± 0.004
0.020 ± 0.004
0.017 ± 0.003
0.034 ± 0.004
0.016 ± 0.002
0.037 ± 0.003

0.023 ± 0.005
0.023 ± 0.005
0.011 ± 0.003
0.009 ± 0.002
0.015 ± 0.004
0.009 ± 0.004
0.023 ± 0.003

0.093 ± 0.036
0.059 ± 0.002
0.047 ± 0.002
0.043 ± 0.006
0.082 ± 0.005
0.035 ± 0.001
0.133 ± 0.004

All experiments were repeated with 5 diﬀerent random train/test dataset splits and weights initializations.
Networks parameters are optimized using Adam [Kingma and Ba, 2015] for the following learning rate values:
{0.1, 0.01, 0.001, 0.0001}. Training is performed for 100 epochs with early stopping after 20 epochs without
improvement. Note that for both BC and BNN algorithms, the ErrorT and MAP columns share the same results,
as these models don’t rely on aggregation.

Fortier-Dubois, Letarte, Leblanc, Germain, Laviolette

Table 3: Extended experiment results with standard deviation on wide architectures (with PBGNet and ABNet
stochastic versions) over the 5 repetitions for selected models: number of hidden layers (L−1), width (d), bound
value and error rate on the train data (ErrorS) and on the test data for the model (ErrorT ) and the associated
Maximum-A-Posteriori BAM (MAP).

Dataset Model

L−1

d

Bound

ErrorS

ErrorT

MAP

ads

adult

mnist17

mnist49

mnist56

mnistLH

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)
EBP
BC
BNN

3
2
3
2
3
1
1

1
1
2
3
2
1
1

1
1
2
3
2
3
1

1
1
1
3
2
1
1

1
1
1
1
2
3
1

1
2
1
3
1
1
1

10
10
10
50
10
10
100

10
10
10
10
100
50
100

10
10
10
10
10
50
100

10
10
50
10
10
100
100

10
10
50
10
10
50
100

10
10
100
10
100
100
100

0.213 ± 0.004
0.216 ± 0.004
1.000 ± 0.000
1.000 ± 0.000
–
–
–

0.216 ± 0.001
0.216 ± 0.000
0.360 ± 0.004
0.541 ± 0.033
–
–
–

0.041 ± 0.000
0.041 ± 0.000
1.000 ± 0.000
0.607 ± 0.084
–
–
–

0.149 ± 0.007
0.147 ± 0.007
0.992 ± 0.001
1.000 ± 0.000
–
–
–

0.090 ± 0.001
0.090 ± 0.001
0.974 ± 0.017
1.000 ± 0.000
–
–
–

0.167 ± 0.010
0.187 ± 0.003
0.998 ± 0.001
0.895 ± 0.021
–
–
–

0.140 ± 0.003
0.140 ± 0.003
0.020 ± 0.003
0.020 ± 0.005
0.005 ± 0.002
0.021 ± 0.005
0.029 ± 0.004

0.156 ± 0.001
0.156 ± 0.001
0.146 ± 0.001
0.143 ± 0.001
0.049 ± 0.002
0.160 ± 0.003
0.157 ± 0.002

0.005 ± 0.000
0.005 ± 0.000
0.002 ± 0.001
0.002 ± 0.000
0.000 ± 0.000
0.003 ± 0.001
0.004 ± 0.001

0.037 ± 0.001
0.038 ± 0.001
0.004 ± 0.000
0.024 ± 0.001
0.001 ± 0.001
0.005 ± 0.001
0.011 ± 0.002

0.023 ± 0.001
0.023 ± 0.001
0.003 ± 0.001
0.010 ± 0.002
0.000 ± 0.000
0.004 ± 0.001
0.004 ± 0.002

0.058 ± 0.013
0.087 ± 0.007
0.006 ± 0.001
0.050 ± 0.005
0.001 ± 0.000
0.013 ± 0.002
0.023 ± 0.001

0.141 ± 0.010
0.141 ± 0.010
0.026 ± 0.006
0.026 ± 0.005
0.035 ± 0.006
0.032 ± 0.005
0.032 ± 0.005

0.159 ± 0.002
0.160 ± 0.002
0.151 ± 0.002
0.151 ± 0.002
0.186 ± 0.001
0.164 ± 0.001
0.165 ± 0.002

0.006 ± 0.001
0.006 ± 0.001
0.005 ± 0.001
0.005 ± 0.001
0.005 ± 0.000
0.006 ± 0.001
0.007 ± 0.001

0.037 ± 0.004
0.037 ± 0.004
0.012 ± 0.003
0.029 ± 0.003
0.021 ± 0.004
0.015 ± 0.003
0.023 ± 0.003

0.025 ± 0.003
0.025 ± 0.003
0.008 ± 0.002
0.017 ± 0.002
0.019 ± 0.004
0.010 ± 0.003
0.012 ± 0.001

0.059 ± 0.010
0.088 ± 0.006
0.022 ± 0.001
0.060 ± 0.005
0.027 ± 0.001
0.027 ± 0.001
0.036 ± 0.001

0.141 ± 0.010
0.141 ± 0.010
0.028 ± 0.003
0.025 ± 0.004
0.049 ± 0.008
0.032 ± 0.005
0.032 ± 0.005

0.158 ± 0.002
0.158 ± 0.002
0.164 ± 0.000
0.151 ± 0.002
0.189 ± 0.003
0.164 ± 0.001
0.165 ± 0.002

0.006 ± 0.001
0.006 ± 0.001
0.006 ± 0.001
0.005 ± 0.001
0.005 ± 0.001
0.006 ± 0.001
0.007 ± 0.001

0.036 ± 0.004
0.036 ± 0.004
0.012 ± 0.003
0.027 ± 0.003
0.026 ± 0.004
0.015 ± 0.003
0.023 ± 0.003

0.024 ± 0.003
0.024 ± 0.003
0.008 ± 0.003
0.016 ± 0.002
0.021 ± 0.005
0.010 ± 0.003
0.012 ± 0.001

0.060 ± 0.010
0.087 ± 0.005
0.024 ± 0.002
0.058 ± 0.006
0.032 ± 0.002
0.027 ± 0.001
0.036 ± 0.001

Learning Aggregations of Binary Activated Neural Networks with Probabilities over Representations

Table 4: Extended details on the selected architecture in the experiments on wide architectures: number of hidden
layers (L−1), width (d), both C and KL divergence values from the computation of the corresponding bound,
learning rate (LR) and epoch providing the best model (Best epoch) given the algorithm stops after 100 epochs.

Dataset Model

L−1

ads

adult

mnist17

mnist49

mnist56

mnistLH

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

PBGNet
ABNet
PBGNet(cid:96)
ABNet(cid:96)

3
2
3
2

1
1
2
3

1
1
2
3

1
1
1
3

1
1
1
1

1
2
1
3

d

10
10
10
50

10
10
10
10

10
10
10
10

10
10
50
10

10
10
50
10

10
10
100
10

KL

C

LR

Best epoch

26
29
66262
20940

238
233
3148
10014

181
171
425542
8578

342
329
39092
158982

202
196
35699
132207

1386
1053
262046
81752

0.45
0.47
17.33
14.50

0.30
0.30
1.14
1.93

1.39
1.35
17.33
6.66

0.93
0.91
9.81
17.33

0.95
0.93
10.28
17.33

0.74
0.59
10.97
5.02

0.01
0.01
0.001
0.01

0.1
0.1
0.001
0.01

0.1
0.1
0.1
0.01

0.1
0.1
0.01
0.1

0.01
0.1
0.01
0.1

0.1
0.1
0.01
0.01

64
51
55
52

87
77
60
52

96
78
19
33

95
86
88
57

87
97
83
82

93
80
90
68

