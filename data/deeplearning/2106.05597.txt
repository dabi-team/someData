1
2
0
2

n
u
J

0
1

]

V
C
.
s
c
[

1
v
7
9
5
5
0
.
6
0
1
2
:
v
i
X
r
a

Supervising the Transfer
of Reasoning Patterns in VQA

Corentin Kervadec1∗
Orange Innovation
Cesson-Sévigné, France
corentinkervadec.github.io

Christian Wolf∗
LIRIS, INSA Lyon
UMR CNRS 5205, France
christian.wolf@insa-lyon.fr

Grigory Antipov
Orange Innovation
Cesson-Sévigné, France
grigory.antipov@orange.com

Moez Baccouche
Orange Innovation
Cesson-Sévigné, France
moez.baccouche@orange.com

Madiha Nadri
LAGEPP, Univeristé de Lyon
Univ. Claude Bernard, France
madiha.nadri@lagep.univ-lyon1.fr

Abstract

Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset
biases rather than performing reasoning, hindering generalization. It has been
recently shown that better reasoning patterns emerge in attention layers of a state-
of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This
provides evidence that deep neural networks can learn to reason when training
conditions are favorable enough. However, transferring this learned knowledge
to deployable models is a challenge, as much of it is lost during the transfer. We
propose a method for knowledge transfer based on a regularization term in our
loss function, supervising the sequence of required reasoning operations. We
provide a theoretical analysis based on PAC-learning, showing that such program
prediction can lead to decreased sample complexity under mild hypotheses. We
also demonstrate the effectiveness of this approach experimentally on the GQA
dataset and show its complementarity to BERT-like self-supervised pre-training.

1

Introduction

Reasoning over images is the main goal of Visual Question Anwering (VQA), a task where a model
is asked to answer questions over images. This problem is a test bed for the creation of agents capable
of high-level reasoning, as it involves multi-modal and high-dimensional data as well as complex
decision functions requiring latent representations and multiple hops. State-of-the-art models are
notorious for leveraging dataset biases and shortcuts in learning rather than performing reasoning,
leading to lack of generalization, as evidenced by extensive recent work on bias oriented benchmarks
for vision-and-language reasoning [1, 22, 23, 37]. Even large-scale semi-supervised pre-training
methods, which successfully managed to increase overall VQA performance, still struggle to address
questions whose answers are rare given a context [22].

∗Both authors contributed equally. 1 Also at LRIS, INSA Lyon

Preprint. Under review.

 
 
 
 
 
 
Figure 1: VQA takes visual input v and a question q and predicts a distribution over answers y. (a)
Classical discriminative training encodes the full reasoning function in the network parameters θ,
while the network activations contain latent variables necessary for reasoning over multiple hops. (b)
Additional program supervision requires intermediate network activations to contain information on
the reasoning process, simplifying learning the reasoning function g. Under the hypothesis of it’s
decomposition into multiple reasoning modes, intermediate supervision favors separately learning the
mode selector and each individual mode function. This intuition is analyzed theoretically in section 4.

It has been recently shown that reasoning patterns emerge in attention layers of a SOTA VQA model
when trained on perfect (oracle) visual inputs, which provides evidence that deep neural networks
can learn to reason, when training conditions are favorable enough [23]. In particular, uncertainty
and noise in visual inputs seems to be a major cause for shortcut learning in VQA. While this kind
of methods provide strong empirical results and insights on the bottlenecks in problems involving
learning to reason, they still suffer from signiﬁcant loss in reasoning capabilities during the transfer
phase, when the model is required to adapt from perfectly clean visual input to the noisy input it
will encounter after deployment. We conjecture, that reasoning on noisy data involves additional
functional components not necessary in the clean case due to different types of domain shifts: (1) a
presence shift, caused by imperfect object detectors, leading to missing visual objects necessary for
reasoning, or to multiple (duplicate) detections; (2) an appearance shift causing variations in object
embeddings (descriptors) for the same class of objects due to different appearance.

In this paper, we propose a new method for transferring reasoning patterns from models learned
on perfect visual input to models trained on noisy visual representations. Key to the success is a
regularization term minimizing loss of the reasoning capabilities during transfer. In particular, we
address this problem through program prediction as an additional auxiliary loss, i.e. supervision of
the sequence of reasoning operations along with their textual and/or visual arguments. To maintain a
strong link between the learned function and its objective during the knowledge transfer phase, when
inputs are switched from clean oracle inputs to noisy input, the neural model is required to continue
to predict complex reasoning programs from different types of inputs.

As a second justiﬁcation, we claim that program supervision in itself leads to a simpler learning
problem, as the underlying reasoning function is decomposed into a set of tasks, each of which is
easier to learn individually than the full joint decision function. We backup this claim through a
theoretical analysis showing decreased sample complexity under mild hypotheses.

In an experimental study, we demonstrate the effectiveness of this approach on the GQA dataset and
show its complementarity when combined to BERT-like self-supervised pre-training [36, 11].

As a summary, we present the following contributions: (i) we propose a new program supervision
module added on top of vision-language transformer models; (ii) we provide a theoretical analysis
of the beneﬁt of supervising program prediction in VQA deriving bounds on sample complexity;
(iii) we experimentally demonstrate the efﬁciency of program supervision and show that it increases
VQA performance on both in- and out-of-distribution sets, even when combined with BERT-like
pre-training [36, 11], and that it improves the quality of oracle transfer initially proposed by [23].

2 Related Work

Transformers in VQA — VQA as a task was introduced in various datasets [4, 13, 18], including
GQA [17] which is automatically-generated from real-world images. This growing amount of diverse
datasets has been accompanied by the development of more and more sophisticated VQA models.
While their exhaustive survey is out of the scope of this paper, one can mention some foundational
categories of approaches, e.g. those based on object-level attention [2] and tensor decomposition [6].
In this work, we focus on VQA models which are based on Transformers [40], due to their wide

2

!vqy(a)Classical	training,	CE	loss	on	answers	y(b)	CE	loss	on	y+	program	prediction	pLearned	knowledge	of	the	reasoning	processesLatent	variables	necessary	for	reasoning	over	multiple	hops	(used	by	reasoning	processes)vqy!pDecomposition	of	the	underlying	(unknown)	reasoning	functionadoption and their impressive results in several tasks (including VQA). In particular, we rely on the
combination of Transformers with a large-scale BERT [11]-like pretraining which was shown to be
beneﬁcial for VQA in recent works [21, 36]. More recently, [23] focused on the study of so-called
reasoning patterns in such Transformer-based VQA models. The authors analyzed how various VQA
tasks are encoded in different attention heads, by applying an energy-based analysis inspired by [30].
The analysis was performed using a perfect-sighted oracle Transformer model (which is trained with
near-to-perfect visual information, and thus, is much less prone to exploit dataset biases) in order
to identify which patterns lead to better reasoning. Then, the authors showed that these reasoning
patterns can be transferred from the oracle to a Transformer-based VQA model, thus improving both
overall accuracy and accuracy on infrequent answers. In this work, we argue that using program
prediction as an additional supervision signal is a catalyst for the transfer of these reasoning patterns.

Biases and shortcut learning in VQA — In addition to the popular VQA datasets mentioned above,
other benchmarks were proposed to evaluate speciﬁc reasoning capabilities of VQA systems. In
particular, they address the issue of shortcut learning [12] in deep learning, where models learn
decision rules which are ineffective when tested on a domain or distribution different from the training
one. For instance, VQA-CP [1] explicitly inverts the answer distribution between train and test splits.
To cope with recent criticisms regarding these evaluations [38, 34], the GQA-OOD dataset [20]
introduced a new split of GQA focusing on rare (Out-Of-Distribution / OOD) question-answer
pairs, and showed that many VQA models strongly rely on dataset biases. Following this work,
the VQA-CE dataset [9] introduced a new evaluation approach related to the VQA v2 dataset. By
studying the multi-modal shortcuts in the training set and mining some trivial predictive rules (e.g.
co-occurrences of words and visual objects), the evaluation set is generated using questions where
these mined rules lead to incorrect answers. As for GQA-OOD, this dataset demonstrated that SOTA
models do not perform well when they can’t rely on shortcuts, even models which use bias-reduction
techniques. Based on these benchmarks, several methods have been conceived to reduce shortcut
learning in VQA (see [38, 35] for a comprehensive study of the different techniques). However none
of them achieve signiﬁcant performance improvement on GQA-OOD [20] or VQA-CE [9].

Connections with symbolic VQA — Our work is also related to neuro-symbolic reasoning [42, 29]
and neural module networks (NMN) [3, 15], which generally encode a set of pre-deﬁned functions into
unique neural modules, then dynamically compose them to execute question-related programs. Several
improvements of standard NMNs have been proposed to make them end-to-end trainable through
reinforcement learning [19] or, more recently, to enhance their scalability and generalizability [7].
However, the common point of all these works is that they are generally based on the prediction of
reasoning programs whose elementary functions are learned jointly with program prediction itself,
through program supervision. In contrast to this work, our approach uses program supervision to
enrich its internal representations instead of inferring program’s execution.

Measuring complexity of learning problems — and thus generalization, has been a goal of theo-
retical machine learning since the early days, with a large body of work based on PAC-Learning
[39, 33]. Traditionally, bounds have been provided ignoring data distributions and focusing uniquely
on hypothesis classes (network structures in neural network language), e.g. as measured by VC-
dimension. Surprising experimental results on training networks on random samples have seemingly
contradicted learning theory [44], in particular Rademacher Complexity. Recently, building on
statistics of gradient descent, bounds have been proposed which take into account data distributions,
notably [32]. Algorithmic alignment between neural network structures and the decomposition of
underlying reasoning functions has been studied in [41], with a focus on algorithms based on dynamic
programming. Our theoretical contribution in section 4 builds on the latter two methodologies and
extends this type of analysis to intermediate supervision of reasoning programs.

3 Knowledge transfer and program supervision

We propose a method for transferring reasoning patterns from models learned on perfect visual inputs
to models trained on noisy visual representations. In the lines of oracle transfer [23], we ﬁrst pre-train
a model on ground-truth visual data and then ﬁne-tune on standard visual embeddings extracted with
an object detector. The underlying hypothesis is that the noise and uncertainty in visual input prevents
the model from learning to reason and leads to learning shortcuts.

3

Figure 2: A vision+language transformer with an attached program decoder. The decoder is fed
with the VL-Transformer’s penultimate embedding (just before the VQA classiﬁcation head) and
generates programs using a coarse-to-ﬁne approach: (cid:192) a coarse program is generated using a GRU,
consisting of a sequence of program operations embeddings {oi}i∈[0,n−1]. (cid:193) It is then re-ﬁned by
predicting the visual av
ij arguments using an afﬁnity score between operation and
input embeddings. Not shown: prediction of the operation’s dependencies.

ij and textual aq

To minimize the loss of reasoning capabilities during the transfer over the presence shift between
oracle and noisy visual objects, we propose a regularization technique, which supervises the prediction
of reasoning steps required to answer the question. We therefore assume the existence of the following
ground truth annotation of reasoning programs. A given data sample consists of a sequence {qi} of
input question word embeddings, a set {vi} of input visual objects, the ground truth answer class y∗
as well as the groundtruth reasoning program, which is structured as a tree involving operations and
arguments. Operations {o∗
i } are elements of a predeﬁned set {choose color, filter size, ...}.
The arguments of these operations may be taken from (i) all question words, (ii) all visual objects,
(iii) all operations — when an operation takes as argument the result of another operation. Hence,
arguments are annotated as many-to-many relationships. In the question “Is there a motorbike or a
plane?”, for instance, the operation “or” depends on the result of the two operations checking the
existence of a speciﬁc object in the image. This is denoted as aq∗
ij =1 means that
operation i is associated with question word j as argument and, similarly, av∗
ij =1 indicating a visual
argument and ad∗

ij ∈{0, 1} where aq∗

ij =1 an operation result argument.

We propose to apply the regularization on top of the VL-Transformer architecture proposed in [36],
based on sequences of self- and cross-modality attention. For this purpose, we deﬁne a trainable
module for program generation (program decoder), added to the output of the VL-Transformer model
as shown in Fig. 2 — an adaptation to other architectures would be straightforward.

Program decoder — In the lines of [7], the program decoder has been designed in a coarse-to-ﬁne
fashion. It ﬁrst generates (cid:192) a coarse sketch of the program consisting only of the operations, which
are then (cid:193) reﬁned by predicting textual and visual arguments and dependencies between operations.
(cid:192) Coarse: operation — this module only predicts the sequence of operations {oi}i∈[0,n−1] using a
recurrent neural network (RNN / GRU) [8] variant, whose initial hidden state is initialized with the
yCLS token embedding of the VQA transformer — the same embedding from which classically the
ﬁnal answer y is predicted, cf. Figure 2. Inference is stopped when the special “STOP” operation is
predicted. At each GRU time step i, a new hidden state hi is computed, from which the operation oi is
classiﬁed with a linear projection. It is supervised with a cross-entropy loss Lop = (cid:80)
iLCE(oi, o∗
i ).
(cid:193) Fine: input arguments — the coarse program is then reﬁned by predicting the operations’
arguments. We ﬁrst deal with textual and visual arguments only. Afﬁnity scores aq
ij between each
operation’s hidden embedding hi and each token embedding qj are computed with a 2-layer feed-
forward network from concatenated embeddings. They represent the probability of the word qj
to belong to the argument set of operation oi. Similar scores av
ij are computed for operations and
visual objects. They are supervised with BCE losses Lqarg = (cid:80)
ij, aq∗
ij ) and Lvarg =
(cid:80)

ijLBCE(aq

ijLBCE(av

ij, av∗

ij ).

Fine: op arguments — next the dependencies are predicted, i.e. arguments which correspond to
results of other operations, and which structure the program into a tree. We deal with these arguments
differently, and compute the set of dependency arguments for each operation oi with another GRU,

4

whose hidden state is initialized with the hidden state hi of the operation. The argument index ad
a linear projection of the hidden state and supervised with BCE: Lvarg = (cid:80)
ij ).
Program supervision — The coarse-to-ﬁne program decoder is trained with the four additional
losses weighted by hyperparameters α, β, γ, δ.

ijLBCE(ad

ij, ad∗

ij is

L = Lvqa
(cid:124)(cid:123)(cid:122)(cid:125)
VQA

,
+ α.Lop + β.Ldep + γ.Lqarg + δ.Lvarg
(cid:125)
(cid:123)(cid:122)
Program supervision

(cid:124)

(1)

Ground truth programs — We use ground truth information from the GQA [17] dataset, whose
questions have been automatically generated from real images. Each sample contains a program
describing the operations and arguments required to derive the answer for each question. However,
the GT programs have been created for GT visual arguments (GT objects), which do not exactly
match the visual input of an object detector used during training and inference [2]. We therefore
construct a soft target, by computing intersection-over-union (IoU) between GT and detected objects.

Oracle transfer — Our method uses program supervision to regularize knowledge transfer from a
visual oracle to noisy input, as introduced in [23]. Oracle transfer consists in pretraining the VL-
Transformer model on ground-truth one-hot visual inputs before performing BERT-like pre-training.
It offers training conditions which are more favorable for learning reasoning capabilities. We perform
the following steps: (1) Oracle pre-training on GT visual input on the GQA dataset; (2) (optionally)
BERT-like pre-training on data from GQA unbalanced; (3) Finetuning on the ﬁnal VQA-objective on
the GQA dataset. Each one of these steps are regularized using program supervision.

4 Sample complexity of program supervision

We provide a theoretical analysis indicating that the prediction and supervision of reasoning pro-
grams can improve learnability in vision and language reasoning under some assumptions. In what
follows, we denote with g “true” (but unknown) underlying reasoning functions, and by f functions
approximating them, implemented as neural networks. The goal is to learn a function g able to
predict a distribution y over answer classes given an input question and an input image, see Fig 1a.
While in the experimental part we use state-of-the-art tranformer based models, in this theoretical
analysis, we consider a simpliﬁed model, which takes as input the two vectorial embeddings q and v
corresponding to, respectively, the question and the visual information (image), for instance generated
by a language model and a convolutional neural network, and produces answers y∗ = g(q, v).

We restrict this analysis to two-layer MLPs, as they are easier to handle theoretically than modern
attention based models. The reasoning function g is approximated by neural network f parametrized
by a vector θ and which predicts output answers y = f (q, v, θ).

Our analysis uses PAC-learning [39] and builds on recent results providing bounds on sample
complexity taking into account the data distribution itself. We here brieﬂy reproduce Theorem 3.5.
from paper [41], which, as an extension of a result in [32], provides a bound for sample complexity
of overparametrized MLPs with vectorial outputs, i.e. MLPs with sufﬁcient capacity for learning a
given task:
Theorem 4.1 (Sample complexity for overparametrized MLPs). Let A be an overparametrized and
randomly initialized two-layer MLP trained with gradient descent for a sufﬁcient number of iterations.
Suppose g : Rd → Rm with components g(x)(i) = (cid:80)
j ∈ Rd, α(i) ∈ R,
and p(i)

x)p(i)
j = 2l, l ∈ N+. The sample complexity CA(g, (cid:15), δ) is
(cid:80)

j = 1 or p(i)

, where β(i)

j (β(i)T

j α(i)



j

j

p(i)
2 + log( m
j
δ )
||





(2)

CA(g, (cid:15), δ) = O



maxi

j p(i)

j |α(i)

j |·||β(i)
j
((cid:15)/m)2

We use the following Ansatz: since each possible input question requires a potentially different form
of reasoning over the visual content, our analysis is based on the following assumption.
Assumption 1. The unknown reasoning function g() is a mixture model which decomposes as follows

(cid:88)

y∗ =

πrhr =

r

(cid:88)

r

πrgr(v),

(3)

5

where the different mixture components r correspond to different forms of reasoning related to different
questions. The mixture components can reason on the visual input only, and the mixture weights are
determined by the question q, i.e. the weights π depend on the question q, e.g. π = gπ(q).

We call gπ(.) the reasoning mode estimator. One hypothesis underlying this analysis is that learning
to predict reasoning programs allows the model to more easily decompose into this form (3), i.e. that
the network structure closely mimicks this decomposition, as information on the different reasoning
modes r is likely to be available in the activations of intermediate layers, cf. Figure 1. This will be
formalized in assumption 3 and justiﬁed further below.
Considering the supposed “true” reasoning function y∗ = g(q, v) and its decomposition given in
(3), we suppose that each individual reasoning module gr can be approximated with a multi-variate
polynomial, in particular each component h(i)

r of the vector hr, as

h(i)

r = gr(v) =

(cid:88)

j

r,j(β(i)T
α(i)

r,j v)p(i)

r,j with parameters ω =

(cid:110)

r,j, β(i)
α(i)

r,j, p(i)
r,j.

(cid:111)

(4)

A trivial lower bound on the complexity of the reasoning mode estimator gπ(.) is the complexity of
the identity function, which is obtained in the highly unlikely case where the question embeddings q
contain the 1-in-K encoding of the choice of reasoning mode r. We adopt a more realistic case as the
following assumption.
Assumption 2. The input question embeddings q are separated into clusters according to reasoning
modes r, such that the underlying reasoning mode estimator gπ can be realized as a NN classiﬁer
with dot-product similarity in this embedding space.

Under this assumption, the reasoning mode estimator can be expressed as a generalized linear model,
i.e. a linear function followed by a soft-max σ,

π = gπ(q) = σ (cid:0)(cid:2)γT

0 q, γT

1 q, ...(cid:3)(cid:1) ,

(5)

where the different γr are the cluster centers of the different reasoning modes r in the question
embedding space. As the softmax is a monotonic non-linear function, its removal will not decrease
sample complexity2, and the complexity can be bounded by the logits πr = γT
r q. Plugging this into
(3) we obtain that each component y∗(i) of the answer is expressed as the following function:
r q(cid:1) (cid:88)

r,j v)p(i)

r,j(β(i)T
α(i)

y∗(i) =

(cid:0)γT

(cid:88)

(6)

r,j

r

j

We can reparametrize this function by concatenating the question q and the visual input v into a
single input vector x, which are then masked by two different binary masks, which can be subsumed
into the parameters γr and β(i)

r,j, respectively, giving

y∗(i) =

(cid:88)

(cid:88)

(γT

r x)α(i)

r,j(β(i)T

r,j x)p(i)

r,j

r

j

(7)

Extending Theorem 3.5. from [41], we can give our main theoretical result as the sample complexity
of this function expressed as the following theorem.
Theorem 4.2 (Sample complexity for multi-mode reasoning functions). Let A be an over-
parametrized and randomly initialized two-layer MLP trained with gradient descent for a
Suppose g : Rd → Rm with components g(x)(i) =
sufﬁcient number of
(cid:80)
r,j where γr ∈ Rd, β(i)
r,j = 1 or
r
p(i)
r,j = 2l, l ∈ N+. The sample complexity CA(g, (cid:15), δ) is


r,j ∈ R, and p(i)

r,j ∈ Rd, α(i)

r,j x)p(i)

r,j(β(i)T

r x)α(i)

iterations.

j(γT

(cid:80)



CA(g, (cid:15), δ) = O



maxi

(cid:80)
r

(cid:80)

j πp(i)

r,j|α|·||γr||2·||βr,j||
((cid:15)/m)2

p(i)
r,j
2 + log(m/δ)

 .

2In principle, there should exist special degenerate cases, where an additional softmax could reduce sample

complexity; however, in our case it is applied to a linear function and thus generates a non-linear function.

6

The proof of this theorem is given in the supplementary material (Appendix A).

Theorem 4.2 provides the sample complexity of the reasoning function g() under classical training.
In the case of program supervision, our analysis is based on the following assumption (see also Fig.
1b):
Assumption 3. Supervising reasoning programs encodes the choice of reasoning modes r into the
hidden activations of the network f . Therefore, learning is separated into several different processes,

(a) learning of the reasoning mode estimator gπ() approximated as a network branch fπ()

connected to the program output;

(b) learning of the the different reasoning modules gr() approximated as network branches
fr() connected to the different answer classes yr; each one of these modules is learned
independently.

We justify Assumption 3.a through supervision directly, which separates gπ() from the rest of the
reasoning process. We justify Assumption 3.b by the fact, that different reasoning modes r will
lead to different hidden activations of the network. Later layers will therefore see different inputs
for different modes r, and selector neurons can identify responsible inputs for each branch fr(),
effectively switching off irrelevant input.

We can see that these complexities are lower than the sample complexity of the full reasoning function

p(i)
r,j
given in theorem 4.2, since for a given combination of i, r, j, the term ||γr||2·||βr,j||
2

dominates

p(i)
r,j
. Let us recall that the different vectors γ correspond to the cluster
the corresponding term ||βr,j||
2
centers of reasoning modes in language embedding space. Under the assumption that the language
embeddings q have been created with batch normalization, a standard technique in neural vision and
language models, each value γ(i)
follows a normal distribution N (0, 1). Dropping indices i, r, j to
r
ease notation, we can then compare the expectation of the term ||γ||2·||β||p
2 over the distribution of γ
and derive the following relationship:

γ(i)∼N (0,1)||γ||2·||β||p
E

2 = C||β||p

2 =

√

2

Γ( m

2 + 1
2 )
Γ( m
2 )

||β||p
2

(8)

where Γ is the Gamma special function and m is the dimension of the language embedding γ. We
provide a proof for this equality in the supplementary material (Appendix A).

Discussion and validity of our claims — the difference in sample complexity is determined by the
factor C in equation (8), which monotonically grows with the size of the embedding space m, which
is typically in the hundreds. For the order of m=512 to m=768 used for state-of-the-art LXMERT
models [36], complexity grows by a factor of around ∼20.

We would like to point out, that this analysis very probably under-estimates the difference in complex-
ity, as the difference very much depends on the complexity of the reasoning estimator π, which we
have simpliﬁed as a linear function in equation (5). Taking into account just the necessary soft-max
alone would probably better appreciate the difference in complexity between the two methods, which
we leave for future work. Our analysis is also based on several assumptions, among which is the
simpliﬁed model (an over-parametrized MLP instead of an attention based network), as well as
assumptions of Theorem 4.2 from [41] and [32], on which our analysis is based.

Lastly, we would like to comment on the fact that we compare two different bounds: (i) the bound
on sample complexity for learning the full multi-modal reasoning given in Theorem 4.2, and (ii) the
bound for learning a single reasoning mode given by Theorem 4.1. While comparing bounds does
not provide deﬁnitive answers on the order of models, both bounds have been derived by the same
algebraic manipulations and we claim that they are comparable.

5 Experimental results

Setup + architecture: — we perform our experiments with a compact version of the Vision-
Language (VL)-Tansformer used in [36] (cf. Fig. 2), with a hidden embedding size of d=128
and h=4 heads per layer (only 26M trainable parameters). Dataset: our models are trained on the

7

Model

Oracle Prog.
sup.
transf.

GQA-OOD [20]

GQA [17]

acc-tail

acc-head test-dev

binary* open* test-std

h (a) Baseline

c
t
a
r
c
s

(b) Oracle transfer
(c) Ours

t (d) Baseline
r
e
m
x
l

(e) Oracle transfer
(f) Ours

+

(cid:88)
(cid:88)

(cid:88)
(cid:88)

42.9
48.2±0.3 54.6±1.1
(cid:88) 48.8±0.1 56.1±0.3

49.5

47.5
47.1

55.2
54.8

(cid:88) 48.0±0.6 56.6±0.6

52.4
57.0±0.3
57.8±0.2

58.5
58.4
59.3±0.3

-
74.5
75.4

-
77.1
77.3

-
42.1
43.0

-
42.6
44.1

-
57.3
58.2

-
58.8
59.7

AUC†
prog.

/
/
97.1

/
/
96.4

Table 1: Impact of program supervision on Oracle transfer [23] for vision-language transformers.
LXMERT [36] pre-training is done on the GQA unbalanced training set. We report scores on
GQA [17] (test-dev and test-std) and GQA-OOD (test). * binary and open scores are computed on
the test-std; † we evaluate visual argument prediction by computing AUC@0.66 on GQA-val.

Ablations

(1) VQA only
(2) Coarse only
(3) Coarse + dep.
(4) Full w/o v.arg
(5) Full (ours)

GQA-OOD [20] GQA [17]

acc-tail (val.)

46.9
46.5
46.8
47.3
49.9

val.

62.2
62.5
62.8
63.7
66.2

Table 2: Ablation of different
types of
program supervision (compact model, no
LXMERT/BERT pre-training, no Oracle), on
GQA val. v.arg = superv. of visual arguments.

Ablations

GQA-OOD [20] GQA [17]

acc-tail (val.)

(6) No prog
(7) Uni-modal
(8) Cross-modal

50.0
49.9
50.4

val.

66.4
66.5
67.4

Table 3: We study the impact of the program
supervision posistion: after uni-modal layers or
after cross-modal layers (standard conﬁguration).
The supervision is more efﬁcient when used after
cross-modal interactions. Setting=oracle transfer,
no lxmert

balanced GQA [17] training set (∼1M question-answer pairs). However, LXMERT pretraining is done
on the unbalanced training set (∼15M question-answer pairs). The latter contains more questions and
programs, but the same number of images (∼100K images). Evaluation: is performed on GQA [17]
and GQA-OOD [20] test sets. GQA is a dataset with question-answer pairs automatically generated
from real images, and is particularly well suited for evaluating a large variety of reasoning skills.
GQA-OOD is a benchmark dedicated to the out-of-domain VQA evaluation, and gives information
on the generalization capabilities of the model. Hyper-parameters are selected either on the testdev
(for GQA) or validation (for GQA-OOD) sets. When speciﬁed (with ±) we provide the average
accuracy and standard deviation computed on three runs with different random seeds. Visual input:
following [2], we use bottom-up visual features extracted using a pre-trained object detector (we keep
its parameters frozen during the training). If not speciﬁed, we use faster-RCNN [31] with 36 objects
per-images. In addition to that, we experiment with designed objects, and also with the VinVL [45]
features which (unlike faster-RCNN ones) are conceived speciﬁcally for vision-language tasks.

Program supervision improves visual reasoning — Tab. 1 reports the effectiveness of program
prediction when combined with oracle and BERT-like pretraining on the GQA dataset and corrobo-
rates the results found in the theoretical analysis. In addition, when using both program supervision
and LXMERT [36] but without oracle transfer, we achieve an accuracy of 58.8 on the testdev set of
GQA. This is lower than oracle transfer’s accuracy, demonstrating the complementary of the two
methods. We note that the majority of the gain is achieved on the more challenging open questions.
In addition, results on GQA-OOD (acc-tail and acc-head) suggest that the gains are obtained in, both,
out- and in-distribution settings. However, as already observed in [23], LXMERT pre-training tends
to decrease the acc-tail gains brought by oracle transfer plus program supervision. We evaluate the
program prediction performance by measuring the area under the ROC curve (AUC) on the visual
argument prediction with an IoU treshold of 2
3 =0.66. Models (c) and (e) achieve, respectively, 97.1
and 96.4 AUC scores, demonstrating the effectiveness of the program decoder.

Visual arguments are the key — We study the impact of different types of program supervision in
Tab. 2. We can see the importance of supervising arguments, in (4) and (5). The supervision of visual
arguments (5) contributes most to the gain in performance, again corroborating that visual uncertainty
is the main bottleneck for reasoning on the GQA dataset.

Program supervision enhances cross-modal interactions — In Tab. 3, we study how the inputs of
the program prediction module inﬂuence the VQA accuracy. In particular, we test two settings: (7)

8

Model

(g) Oracle transfer
(h) Ours

(i) Oracle transfer
(j) Ours
(k) Oracle transfer +lxmert
(l) Ours +lxmert

Visual
features

100 RCNN [2]

VinVL [45]

Oracle Prog.
transf.
sup.
(cid:88)
(cid:88)

57.0±0.4
(cid:88) 58.2±0.1

(cid:88)
(cid:88)
(cid:88)
(cid:88)

59.6±0.1
(cid:88) 60.9±0.2
61.4
(cid:88) 61.8

-
-

-
-
79.6
80.1

-
-

-
-
47.5
48.0

-
-

-
-
62.5
63.0

GQA [17]

test-dev

binary* open* test-std

Table 4: Impact of improved visual inputs while using program supervision on Vision-Language
Transformers. Scores on GQA [17]. *binary/open are computed on test-std.

Method

Visual
feats.

Additional
supervision

Training data (M) GQA-OOD [20]
acc-head
Img

acc-tail

Sent

RCNN [2]
BAN4 [24]
MCAN [43]
RCNN [2]
Oracle transfer [23] RCNN [2]
RCNN [2]
MMN [7]
RCNN [2]
LXMERT [36]
Ours
VinVL [45] Program
NSM [16]
OSCAR+VinVL [45] VinVL [45]

≈ 0.1 ≈1
≈ 0.1 ≈1
≈0.18 ≈1
≈0.1 ≈15
≈0.18 ≈9
≈0.1 ≈15
Scene graph ≈0.1 ≈1
≈5.7 ≈9
-

-
-
-
Program
-

SG [16]

47.2
46.5
48.3
48.0
49.8
49.1
-
-

51.9
53.4
55.5
55.5
57.7
59.7
-
-

GQA [17]

bin. open

all

76.0 40.4 57.1
75.9 42.2 58.0
75.2 44.1 58.7
78.9 44.9 60.8
77.8 45.0 60.3
80.1 48.0 63.0
78.9 49.3 63.2
82.3 48.8 64.7

Table 5: Comparison with the state of the art on the GQA [17] (test-std) and GQA-OOD [20] (test)
sets. For a fair comparison, we provide information about the required training data and supervision.

uni-modal, where the programs are predicted from the vision and language embeddings right after
the uni-modal layers (language and vision only in Fig. 2); and (8) cross-modal, where the programs
are predicted after the cross-modal layers (as shown in Fig. 2). We observe that, contrary to the
latter, the former does not improve the baseline ((7) vs (6) in Tab. 3). This highlights the fact that
the program supervision mainly impacts the operations in the cross modal layers, where the most
complex reasoning operations are performed.

Program supervision allows to take advantage of the improved visual inputs — We analyse the
impact of using our method with a better input image representation. Increasing the number of objects
from 36 to 100 per image ((g) and (h) in Tab. 4), allows to further increase the gains brought by
our method. On the contrary, the score of the baseline model remains unchanged, showing that the
program supervision allows to take advantage of a bigger number of object proposals. Similarly,
replacing the faster-RCNN features by the more recent and more accurate VinVL ones ((i)-(l) in
Tab. 4) results in better performances.

Comparison with SOTA — We report in Tab. 5 the results obtained by our approach compared to
the current SOTA on the GQA and GQA-OOD datasets. In order to ensure a fair comparison, we
also provide, for each method, information regarding the amount of data (images and sentences) used
during training. As shown in Tab. 5, our approach compares favourably with SOTA since it obtains the
second best accuracy (with a 0.2 points gap) on the GQA test-std set among the approaches which not
use extra training data. The results also remain competitive when comparing to the OSCAR+VinVL [45],
while being trained with 50 times less images. On GQA-OOD, our approach obtains the second best
acc-tail score (and the best acc-head one) with a much less complex architecture than current SOTA
(26M vs 212M trainable parameters compared to LXMERT [36]).

6 Conclusion

We have demonstrated that it is possible to improve the reasoning abilities of VQA models when
providing additional supervision of program annotations. In particular, our method is designed to
improve the transfer of reasoning patterns from models learned on perfect visual input to models
trained on noisy visual representations. Our experiments are supported by a theoretical analysis,
demonstrating that program supervision can decrease sample complexity under reasonable hypothesis.
The proposed method relies on the availability of reasoning program annotations, which are costly
to annotate, especially when dealing with human generated questions. Recent work has already

9

managed to gather such kind of annotations [10]. The next step will be to extend the method to
conﬁgurations where the program annotation is rare or incomplete.

Broader Impact — Beyond the exciting scientiﬁc reasons for exploring work on visual reasoning,
we welcome the potentially high interest for society in VQA systems targeting increased accessibility.
As an example, we have tools in mind, which allow the visually impaired to query their environment
based on visual information from a wearable camera. We are currently not aware of existing
applications abusing VQA systems for unethical goals. Potential future unethical abuse, generally and
beyond our contribution, could involve their use to automatic solving of Captchas (automatic Turing
tests), and the creation of false dialogs and discussions in online forums (“troll farms”) requiring
the examination of posted text as well as accompanying images. The latter is a risk inherent to any
powerful system capable of predicting text, but for the moment does not realistically apply to VQA,
where very short answers are predicted through classiﬁcation over the full answer dictionary (even
though our contribution is independent of the chosen VQA model).

References

[1] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don’t just assume; look and

answer: Overcoming priors for visual question answering. In CVPR, 2018.

[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR,
pages 6077–6086, 2018.

[3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR,

2016.

[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,

and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015.

[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450, 2016.

[6] Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodal tucker fusion

for visual question answering. In ICCV, 2017.

[7] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for

compositional visual reasoning. In WACV, 2021.

[8] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. In EMNLP, 2014.

[9] Corentin Dancette, Remi Cadene, Damien Teney, and Matthieu Cord. Beyond question-based biases:
Assessing multimodal shortcut learning in visual question answering. arXiv preprint arXiv:2104.03149,
2021.

[10] Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Human Attention in
Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP,
2016.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-
rectional transformers for language understanding. In Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, 2019.

[12] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence,
2(11):665–673, 2020.

[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:
Elevating the role of image understanding in visual question answering. In CVPR, pages 6904–6913, 2017.
[14] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,

2016.

[15] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:
End-to-end module networks for visual question answering. In Proceedings of the IEEE International
Conference on Computer Vision, pages 804–813, 2017.

[16] Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. In Advances

in Neural Information Processing Systems, pages 5901–5914, 2019.

[17] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and

compositional question answering. In CVPR, pages 6700–6709, 2019.

10

[18] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In
CVPR, pages 2901–2910, 2017.

[19] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
nick, and Ross Girshick. Inferring and executing programs for visual reasoning. In Proceedings of the
IEEE International Conference on Computer Vision, pages 2989–2998, 2017.

[20] C. Kervadec, G. Antipov, M. Baccouche, and C. Wolf. Roses Are Red, Violets Are Blue... but Should

VQA Expect Them To? Pre-print: arxiv:2006.05121, 2020.

[21] Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. Weak supervision helps
emergence of word-object alignment and improves vision-language tasks. In European Conference on
Artiﬁcial Intelligence, 2019.

[22] Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. Roses are red, violets are

blue... but should vqa expect them to? In CVPR, 2021.

[23] Corentin Kervadec, Theo Jaunet, Grigory Antipov, Moez Baccouche, Romain Vuillemot, and Christian

Wolf. How transferable are reasoning patterns in vqa? In CVPR, 2021.

[24] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In Advances in Neural

Information Processing Systems, pages 1564–1574, 2018.

[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2014.

[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. IJCV, 123(1):32–73, 2017.

[27] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon

emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.

[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,

and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[29] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The Neuro-Symbolic
Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In International
Conference on Learning Representations, 2019.

[30] Hubert Ramsauer, Bernhard Schäﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber,
Markus Holzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, Victor Greiff, et al. Hopﬁeld networks is all you
need. arXiv preprint arXiv:2008.02217, 2020.

[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.

[32] S.S. Du S. Arora, W. Hu, Z. Li, and R. Wang. Fine-grained Analysis of optimization and generalization for

overparametrized two-layer neural networks. In ICML, 2019.

[33] Shai S. Shalev-Shwart and S. Ben-David. Understanding Machine Learning - From Theory to Algorithms.

Cambridge University Press, 2014.

[34] Robik Shrestha, Kushal Kaﬂe, and Christopher Kanan. A negative case analysis of visual grounding
In Proceedings of the 58th Annual Meeting of the Association for Computational

methods for vqa.
Linguistics, 2020.

[35] Robik Shrestha, Kushal Kaﬂe, and Christopher Kanan. An investigation of critical issues in bias mitigation

techniques. arXiv preprint arXiv:2104.00170, 2021.

[36] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.

In EMNLP, pages 5103–5114, 2019.

[37] Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshufﬂing data for improved generaliza-

tion. arXiv preprint arXiv:2002.11894, 2020.

[38] Damien Teney, Ehsan Abbasnejad, Kushal Kaﬂe, Robik Shrestha, Christopher Kanan, and Anton van den
Hengel. On the value of out-of-distribution testing: An example of goodhart's law. In NeurIPS, 2020.

[39] L.G. Valiant. A theory of the learnable. In Communications of the ACM, volume 27(11), 1984.

[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems, pages 5998–6008, 2017.

[41] K. Xu, J. Li, M. Zhang, S.S. Du, K.-I. K., and S. Jegelka. What can Neural Networks Reason About. In

ICLR, 2020.

11

[42] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B Tenenbaum. Neural-
Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. In Advances in
Neural Information Processing Systems (NIPS), 2018.

[43] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual

question answering. In CVPR, pages 6281–6290, 2019.

[44] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking

generalization . In ICLR, 2017.

[45] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and
Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. arXiv preprint
arXiv:2101.00529.

12

Supplementary Material

A Proofs of Section 4

A.1 Proof of theorem 4.2

For the unfamiliar reader, we here brieﬂy recall the notion of sample complexity, in the context of
PAC-learning [39], which characterizes the minimum amount (=M ) of samples necessary to learn a
function with sufﬁciently low (= (cid:15)) error with sufﬁciently high (= δ) probability:
Deﬁnition A.1 (Sample complexity). Given an error threshold (cid:15)>0; a threshold on error probability
δ; a training set S = {xi, yi} of M i.i.d. training samples from D, generated from some underlying
true function yi = g(xi), and a learning algorithm A, which generates a function f from training
data, e.g. f = A(S); Then g is (M, (cid:15), δ)-learnable by A if

Px∼D [||f (x) − g(x)|| ≤ (cid:15)] ≥ 1 − δ

(9)

The case of scalar outputs
of the vector y and deﬁne the following Corollary:

In the lines of [32], we ﬁrst deﬁne the case for a single component y(i)

Corollary 0.1 (Sample complexity for multi-mode reasoning functions with a single scalar
component). Let A be an overparametrized and randomly initialized two-layer MLP trained
with gradient descent for a sufﬁcient number of iterations. Suppose g : Rd → Rm with
g(x) = (cid:80)
r,jx)pr,j where γr ∈ Rd, βr,j ∈ Rd, αr,j ∈ R, and pr,j = 1 or
r
pr,j = 2l, l ∈ N+. The sample complexity CA(g, (cid:15), δ) is

r x)αr,j(βT

j(γT

(cid:80)

CA(g, (cid:15)0, δ0) = O

(cid:32) (cid:80)
r

Proof of Corollary 0.1:

(cid:80)

j πpr,j|α|·||γr||2·||βr,j||pr,j
(cid:15)2
0

2 + log( 1
δ0

(cid:33)

)

,

Using Theorem 5.1 from [32], we know that sums of learnable functions are learnable, and can thus
focus on a single term

y = g(x) = α(γT x)(βT x)p

(10)

where we dropped indices r and j and the superscript (i) for convenience.

We proceed in the lines of the proof of Theorem 5.1 in [32]. Given a set of i.i.d data samples
S = {(xs, ys)}n
s=1 = (X, y) from the underlying function g(x), let w be the weights of the ﬁrst
layer of two layer network with ReLu activations; let H ∞ ∈ Rn,n be a Gram matrix deﬁned as
follows, with elements

H ∞

ij = Ew∼N (0,1)

(cid:2)xT

i xjI{wtxi≥0, wtxi≥0}(cid:3) .

To provide bounds on the sample complexity of g(x), using Theorem 5.1 of [32], it sufﬁces to show
that the following bound holds

(cid:113)

yT (H ∞)−1y < Mg

(11)

for a bound Mg independent of the number of samples n.
For ﬁrst introduce some notation. For matrices A = [a1, ..., an3 ] ∈ Rn1×n3 and B = [b1, ..., bn3] ∈
Rn2×n3, the Khatri-Rao product is deﬁned as A(cid:12)B = [a1⊗b1, a2⊗b2, ..., an3⊗bn3]. Let ◦ be the
Haddamard product (element wise multiplication) of two matrices. We also denote the corresponding
powers by A⊗l, A(cid:12)l, A◦l. We denote by A† = (AT A)−1AT the Moore-Penrose pseudo-inverse,
and by P A = A
From the proof of Theorem 5.1 in [32], we also know that

2 the projection matrix for the subspace spanned by A.

2 A†A

1

1

H ∞ (cid:23)

K◦2l
2π(2l − 1)2 ,

13

where K = X T X, and X is the data matrix of all row vectors xi.
Let us consider the case of p = 1. Reformulating equation (10), we get:

y = g(x) = α(γT x)(βT x)
= α(xT γ)(xT β)
= α(x⊗x)T (γ⊗β)

(12)

(13)

(14)

Now, taking the full set of input vectors xi arranged into the full data matrix X, we can perform
similar algebraic operations to get

y = g(X) = α(X T γ) ◦ (X T β)

= α(X (cid:12)2)T (γ⊗β)

(15)

(16)

Plugging (15) and (16) into (11), we need to show that the following expression is smaller than a
constant Mg:

α2((X T γ) ◦ (X T β))T (H ∞)−1(X (cid:12)2)T (γ⊗β)

=α2((X (cid:12)2)T (γ⊗β))T (H ∞)−1(X (cid:12)2)T (γ⊗β)
=α2(γ⊗β)T (X (cid:12)2)(H ∞)−1(X (cid:12)2)T (γ⊗β)
≤2πα2(γ⊗β)T (X (cid:12)2)(K◦2)†(X (cid:12)2)T (γ⊗β)
=2πα2(γ⊗β)T P X (cid:12)2
≤2πα2||(γ⊗β)||2
2
=2πα2||γ||2

)T (γ⊗β)

(X (cid:12)2

2 · ||β||2
2

where we made use of ||a⊗b||2

2 = ||a||2

2||b||2

2 for two vectors a and b and an integer n.

This ﬁnishes the proof for the case p = 1.

Let us consider the case of p = 2l+1. Reformulating equation (10), we get:

y = g(X) = α(X T γ) ◦ (X T β)p

= α(X (cid:12)2l)T (γ⊗β⊗(2l+1))

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Plugging (25) into (11), we again need to show that the following expression is smaller than a constant
Mg:

α2((X (cid:12)2l)T (γ⊗β⊗(2l+1)))T
(H ∞)−1(X (cid:12)2l)T (γ⊗β⊗(2l+1))

=α2(γ⊗β⊗(2l+1))T

(X (cid:12)2l)(H ∞)−1(X (cid:12)2l)T (γ⊗β⊗(2l+1))

≤2π(2l − 1)2α2(γ⊗β⊗(2l+1))T

(X (cid:12)2l)(K◦2)†(X (cid:12)2l)T (γ⊗β⊗(2l+1))

=2π(2l − 1)2α2(γ⊗β⊗(2l+1))T
)T (γ⊗β⊗(2l+1))

P X (cid:12)2l

(X (cid:12)2l

≤2π(2l − 1)2α2||(γ⊗β⊗(2l+1))||2
2
≤2πp2α2||(γ⊗β⊗(2l+1))||2
2
=2πp2α2||γ||2

2

2 · ||β||2p
2 and therefore ||a⊗n||2

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

where we made use of ||a⊗b||2
and an integer n.

2 = ||a||2

2||b||2

This ﬁnishes the proof for the case p = 2l+1.

14

2 = ||a||2n

2 for two vectors a and b

The case of vectorial outputs
In the lines of [41], we consider each component of the output
vector independent and apply an union bound to Corollary 0.1. If the individual components y(i)
fail to learn with probability δ0, then the full output of dimension m fails with probability mδ0 and
with an error of at most m(cid:15)0. A change of variables from ((cid:15)0, δ0) to ((cid:15), δ) gives a complexity for the
model with vectorial output of




CA(g, (cid:15), δ) = O



maxi

(cid:80)
r

(cid:80)

j πp(i)

r,j|α|·||γ||2·||βr,j||
((cid:15)/m)2

p(i)
r,j
2 + log(m/δ)

 ,

This ends the proof of Theorem 4.2.

A.2 Proof of the inequality in Eq. (8)

Let us denote by p(x) the density of normal distribution. And to make the notation more succinct
and to avoid confusion between different usages of superscripts, in this proof we will change γi
r to
γi, i.e. the ith component of the vector γ, not to be confused with γr, a vector corresponding to the
embedding of the rth reasoning mode. Then,

Eγi∼N (0,1)||γ||2·||β||p

2

=||β||p
2

Eγi∼N (0,1)

(cid:33) 1

2

(cid:32)

(cid:88)

γ2
i

i

We now perform a change of variables and introduce a new random variable

z =

(cid:88)

γ2
i .

(37)

(38)

(39)

(40)

i
Since each individual γi is distributed normal, z is distributed according to a χ2 distribution with m
degrees of freedom, and we get

Eγi∼N (0,1)||γ||2·||β||p
Ez∼χ2[z

1
2 ]

=||β||p
2

2

(41)

(42)

The expectation now corresponds to 1
2
freedom, whose kth moments are given as

th

centered moment of the χ2 distribution with m degrees of

Ez∼χ2[zk] = 2k Γ( m
2 + k)
Γ( m
2 )

(43)

This ends the proof of the equality.

B Program decoder

We provide more details on the program decoder architecture. The hidden size is set to 128 (same as
in the VL-Transformer). We use GeLU [14] as non linearity, along with layernorm [5].

Operations The maximum number of operations in one program is set to Nmaxop = 9. The
total number of operation’s labels is Nop = 212. We use a one layer GRU [8] with hidden size
equals to 128, to infer the operation’s hidden embedding hi. It is followed by a two layers MLP
(128 → 64 → Nop, projecting hi into a one-hot vector oi.

Arguments Afﬁnity scores aq
ij between each operation’s hidden embedding hi and each token
embedding qj (or vj) are computed with a 2-layer feed-forward network (256 → 64 → 1) from
concatenated embeddings. The op arguments are predicted from hi using another one layer GRU
with hidden size equals to 128 followed by a nonlinear projection (128 → Nmaxop).

15

Table 6: Training and execution time for one run. Ours corresponds to oracle transfer plus program
prediction. We also provide the approximated amount of runs done during this work (hyper parameters
search, abblation, etc.)

Run

train
train+test
train+test
train+test

train+test
train+test

train+test
train+test

Model

#GPUs

# hours

Total number of runs

Oracle
ours 36 RCNN
ours 100 RCNN
ours VinVL

ours 36 RCNN + LXMERT pretrain
ours 36 RCNN + LXMERT ﬁnetune

ours VinVL + LXMERT pretrain
ours VinVL + LXMERT ﬁnetune

1
1
2
2

2
1

3
1

30
9
10
10

100
4

180
6

≈ 5
≈ 100
≈ 5
≈ 5

≈ 20
≈ 50

2
2

Hyper-parameters

are set to α = 1, β = 1, γ = 1 and δ = 100.

C Training details

Architecture: Our VQA architecture is a compact version of the VL-Transformer introduced in
[36]3. In particular, it has 9 language only layers, 5 vision only layers, and 5 cross modal layers.
The hidden size is set to 128. In total, this compact version has 26M parameters, allowing to reduce
computation time and memory overhead.

Optimizer: All models were trained with the Adam optimizer [25], a learning rate of 10−4 with
warm starting and learning rate decay.

is performed following [23]. First, the oracle model is trained during at most 40
Oracle transfer:
epochs on the GQA balanced training set with ground-truth image representation. Then we continue
the training during 10 epochs, with visual features extracted using an object detector. We use a batch
size equals to 196 (96 when using VinVL features).

BERT/LXMERT [36] pretraining is performed during 20 epochs with a batch size of 320 (256
when using VinVL features). All pretraining losses are added from the beginning, including the
VQA one. Note that LXMERT [36] is originally pre-trained on a corpus gathering images and
sentences from MSCOCO [28] and VisualGenome [26]. In this work, we only train on the GQA [17]
unbalanced set, with VisualGenome images. After pre-trainning, we ﬁnetune on the GQA [17]
balanced set during 4 epochs, with a batch size of 32 and a learning rate equal to 10−5.

D Computing resources

Training and evaluation has been performed on several compute infrastructures, which include an
Nvidia DGX-A100 with 8× A100 GPUs and a cluster with P100 and RTX 2080 GPUs. After design
and development, the ﬁnal training and evaluation runs have been performed on Geforce RTX 2080
GPUs. We provide an estimate for the amount of compute in Table 6 — the number of GPUs and
approximate execution times for different models and experimental settings (train, validation and
test).

C02 Emission The RTX infrastructure has a carbon efﬁciency of 0.035 kgCO2eq/kWh. A cu-
mulative of 6500 hours of computation was performed on hardware of type RTX 2080 (TDP of
215W). Total emissions are estimated to be 48.9 kgCO2eq . Estimations were conducted using the
https://mlco2.github.io/impact#compute presented in [27].

3We use the code publicly available at https://github.com/airsplay/lxmert

16

Figure 3: Example of program prediction. The question is: "Does the boat to the left of the ﬂag looks
small or large?". Our model (ours+lxmert with VinVL) correctly answers "small".

Figure 4: Example of program prediction. The question is: "Who is wearing goggles?". Our model
(ours+lxmert with VinVL) correctly answers "woman".

E Visualisation of predictions

We provide example of program prediction in Fig. 3 and 4. In Fig. 3, the question is ‘does the boat
to the left of the ﬂag look small or large?’. The program decoder successfully infers the correct
program. It ﬁrst predicts the coarse operations – select, relate, choose size –, then adds the
arguments taken from the image or the question – boat, ﬂag, small, large –. Finally, the VQA model
predicts the correct answer ‘small’. In Fig. 4, the question is ‘who is wearing goggles?’. Similarly
to the ﬁrst example, the program decoder generates coarse operations – select, relate, query
name – and visual/textual arguments – woman, who, goggles, wearing–. In these two examples, the
decoder correctly predicts that the programs are chains of operations (special case of a tree). At
contrary, a question like ‘are there nuts or vegetables?’ is a not a chain because of the presence of
exist and or operations.

17

Ablations

GQA-OOD [20] GQA [17]

acc-tail (val.)

(1) VQA only
(2) Random prog.
(3) ours

46.9
45.7
49.9

val.

62.2
61.4
66.2

Table 7: Comparison with the random prog baseline, where we randomly replace the ground truth
program with a program picked from another question (compact model, no LXMERT/BERT pre-
training, no Oracle), on GQA val.

F Sanity check

As a sanity check, and to avoid the unfortunate result pinpointed in [34], we compare our model with
a random baseline random prog. in Tab. 7. In random prog., we randomly replace the ground truth
program with a program picked from another question, during the training. As expected, this random
baseline achieves low performances.

18

