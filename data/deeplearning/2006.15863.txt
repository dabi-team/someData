1

Neural Combinatorial Deep Reinforcement
Learning for Age-optimal Joint Trajectory and
Scheduling Design in UAV-assisted Networks

Aidin Ferdowsi, Mohamed A. Abd-Elmagid, Walid Saad, and Harpreet S.

Dhillon

Abstract

In this paper, an unmanned aerial vehicle (UAV)-assisted wireless network is considered in which

a battery-constrained UAV is assumed to move towards energy-constrained ground nodes to receive

status updates about their observed processes. The UAVâ€™s ï¬‚ight trajectory and scheduling of status

updates are jointly optimized with the objective of minimizing the normalized weighted sum of Age of

Information (NWAoI) values for different physical processes at the UAV. The problem is ï¬rst formulated

as a mixed-integer program. Then, for a given scheduling policy, a convex optimization-based solution

is proposed to derive the UAVâ€™s optimal ï¬‚ight trajectory and time instants on updates. However, ï¬nding

the optimal scheduling policy is challenging due to the combinatorial nature of the formulated problem.

Therefore, to complement the proposed convex optimization-based solution, a ï¬nite-horizon Markov

decision process (MDP) is used to ï¬nd the optimal scheduling policy. Since the state space of the MDP

is extremely large, a novel neural combinatorial-based deep reinforcement learning (NCRL) algorithm

using deep Q-network (DQN) is proposed to obtain the optimal policy. However, for large-scale scenarios

with numerous nodes, the DQN architecture cannot efï¬ciently learn the optimal scheduling policy

anymore. Motivated by this, a long short-term memory (LSTM)-based autoencoder is proposed to map

the state space to a ï¬xed-size vector representation in such large-scale scenarios while capturing the

spatio-temporal interdependence between the update locations and time instants. A lower bound on the

minimum NWAoI is analytically derived which provides system design guidelines on the appropriate

choice of importance weights for different nodes. Furthermore, an upper bound on the UAVâ€™s minimum

speed is obtained to achieve this lower bound value. The numerical results also demonstrate that the

proposed NCRL approach can signiï¬cantly improve the achievable NWAoI per process compared to

the baseline policies, such as weight-based and discretized state DQN policies.

Age of information, unmanned aerial vehicles, deep reinforcement learning, convex optimization.

Index Terms

0
2
0
2

n
u
J

9
2

]
T
I
.
s
c
[

1
v
3
6
8
5
1
.
6
0
0
2
:
v
i
X
r
a

The

authors

Tech, Blacksburg, VA (Emails:
{aidin,maelaziz,walids,hdhillon}@vt.edu). This research was supported in part by the U.S. National Science Foundation
under Grant CNS-1814477 and in part by the the Ofï¬ce of Naval Research (ONR) under MURI Grant N00014-19-1-2621.

are with Wireless@VT, Department

ECE, Virginia

of

 
 
 
 
 
 
2

I. INTRODUCTION

Owing to their ï¬‚exible deployment, the unmanned aerial vehicles (UAVs) have emerged as a

key component of future wireless networks. The use of UAVs as ï¬‚ying base stations (BSs), that

collect/transmit information from/to ground nodes (e.g., users, sensors or Internet of Things (IoT)

devices), has recently attracted signiï¬cant attention [1]â€“[8]. Meanwhile, introducing UAVs into

wireless networks leads to many challenging design questions related to optimal deployment,

ï¬‚ight trajectory design, and energy efï¬ciency, to name a few. So far, these challenges have mostly

been addressed in the literature using traditional performance metrics such as network coverage,

rate and delay. However, such performance metrics lack the ability of quantifying the freshness

of information collected by the UAVs since they do not account for the generation times of the

information at the ground nodes. As a result, these existing solutions are not always suitable for

many real-time monitoring applications, such as safety and IoT applications, whose quality-of-

service (QoS) depends upon the freshness of the collected information when it reaches the UAV

[9]. This necessitates the design of new freshness-aware transmission policies that can efï¬ciently

guide the UAVâ€™s ï¬‚ight trajectory as well as carefully schedule information transmissions from

the ground nodes, which is the main objective of this work.

A. Related works

Trajectory planning for UAVs has gained considerable attention in the recent past [10]â€“[20].

The works in [10]â€“[13] formulated a non-convex optimization problem to derive an optimal

trajectory of the UAV that maximizes the total throughput of the network while taking into con-

sideration the energy limitations of the UAV and ground nodes. Then, different successive convex

optimization solutions were proposed to reduce the complexity of the problem. The authors in

[14] jointly optimized the UAVâ€™s ï¬‚ight trajectory and altitude with the objective of maximizing

the total throughput of UAV-assisted backscatter networks. Using tools from stochastic geometry,

the authors in [15] characterized the performance of several canonical mobility models in an

UAV cellular network. Meanwhile, heuristic methods, ï¬‚ow-shop scheduling, dual decomposition,

shortest path, and meta reinforcement learning (RL) techniques have been proposed in [16]â€“[20]

for energy efï¬cient and maximal throughput trajectory design in UAV-assisted wireless networks.

However, the ï¬‚ight trajectories considered in [10]â€“[20] may not necessarily be optimal from the

perspective of preserving freshness of the status updates since they were obtained using traditional

performance metrics, such as throughput and delay.

3

We adopt the concept of age of information (AoI) to quantify the freshness of information at

the UAV. First introduced in [21], AoI is deï¬ned as the time elapsed since the latest received

status update packet at a destination node was generated at the source node. For a simple

queueing-theoretic model, the work in [21] characterized the average AoI, and demonstrated

that the optimal rate at which the source should generate its update packets in order to minimize

the average AoI is different from the optimal rates that either maximize throughput or minimize

delay. Then, the average AoI and other age-related metrics were investigated in the literature for

variations of the model considered in [21] (see [22] for a comprehensive survey). These early

works have inspired the adoption of AoI as a performance metric for different communication

systems that deal with time critical information [23]â€“[40]. In particular, AoI has been studied

in the context of broadcast networks (e.g., [24] and [25]), multicast networks ([26] and [27]),

transmission scheduling policies [28]â€“[34] and large-scale analysis [35]â€“[37] of IoT networks,

ultra-reliable low-latency vehicular networks [38], and social networks ([39] and [40]). Note that

the prior art in [21]â€“[40] assumed the destination node to be static, and, thus, their results cannot

be generalized to a scenario in which the destination is a mobile node such as a UAV.

The use of UAVs for maintaining freshness of information (quantiï¬ed using AoI) collected

from a set of ground nodes has been recently studied in [41]â€“[49]. The authors in [41] investigated

the role of a UAV as a mobile relay to minimize the average Peak AoI for a source-destination

pair model by jointly optimizing the UAVâ€™s ï¬‚ight trajectory as well as energy and service time

allocations for the transmission of status updates. Dynamic programming-based approaches were

proposed in [42], [43] to optimize the UAVâ€™s ï¬‚ight trajectory with the objective of minimizing

the average of the AoI values associated with different ground nodes. Furthermore, a graph

labeling-based algorithm was developed in [44] to determine the optimal scheduling of update

transmissions from the ground nodes while assuming that the UAV is equipped with a battery

of ï¬nite capacity (which needs to be recharged over time). The works in [45]â€“[49] proposed

techniques from reinforcement learning (RL) to learn age-optimal transmission policies. In

particular, in [45], the authors proposed to use Q-learning for scheduling update transmissions

from ground nodes with the objective of minimizing the expired data packets. Meanwhile, deep

Q-network (DQN) approaches with different settings were proposed in our early work [46] and

in [47]â€“[49] to ï¬nd an optimal trajectory and/or scheduling policy for the UAV in order to

minimize the AoI of ground nodes. However, these works considered discretized trajectory and

time instants in their underlying system settings, which introduces approximation errors to the

4

obtained age-optimal policies and limits their implementation in real-world scenarios.

B. Contributions

The main contribution of this paper is a novel approach that combines tools from convex

optimization and deep RL framework for optimizing the UAVâ€™s ï¬‚ight trajectory as well as the

scheduling of the status update packets from ground nodes with the objective of minimizing the

normalized weighted sum of Age of Information (NWAoI) values at the UAV. In particular, we

study a UAV-assisted wireless network, in which a UAV moves towards the ground nodes to

collect status update packets about their observed processes. For this system setup, we formulate

an NWAoI minimization problem in which the UAVâ€™s ï¬‚ight trajectory as well as scheduling

of update packet transmissions are jointly optimized. The problem is solved in two steps.

First, a convex optimization-based approach is proposed to derive the trajectory as well as

the update time instants of nodes for a speciï¬c scheduling policy. Next, in order to ï¬nd the

optimal scheduling policy, a ï¬nite-horizon Markov decision process (MDP) model with ï¬nite

state and action spaces is proposed. Due to the combinatorial nature of the problem of ï¬nding the

optimal scheduling policy, the use of a ï¬nite-horizon dynamic programming (DP) algorithm is

computationally impractical. To overcome this challenge, we propose a neural combinatorial RL

(NCRL) algorithm for this setting [50] and [51]. Unlike conventional RL problems, we show that

the state of our problem has a two dimensional matrix form with varying number of columns.

Therefore, we propose a long short-term memory (LSTM)-based autoencoder that can map the

state of the problem with varying sizes into a ï¬xed size state representation.

Several key system design insights are drawn from our analysis. For instance, we analytically

derive a lower bound on the minimum NWAoI, which is useful in deciding the importance

weights for different nodes. In particular, a key observation from the analytical expression of

the lower bound is that in order to have a similar impact from each node on the NWAoI, the

importance weight of each node should be chosen such that it is proportional to the total number

of updates transmitted by that node. Furthermore, we derive an upper bound on the UAVâ€™s

minimum speed to achieve this lower bound value. Our numerical results also demonstrate the

superiority of the proposed NCRL approach over the baseline policies, such as weight-based and

discretized state policies, in terms of the achievable NWAoI per process. They also reveal that

the NWAoI monotonically decreases with the battery sizes of the ground nodes, and the UAVâ€™s

speed and time constraint, whereas it monotonically increases with the number of nodes.

5

To the best of our knowledge, this work is the ï¬rst to combine tools from convex optimization

and deep RL to characterize the age-optimal policy in a practical scenario involving a continuous

ï¬‚ight trajectory model for the UAV.

II. SYSTEM MODEL AND PROBLEM FORMULATION

A. Network Model

Consider a wireless network in which a set M of M ground nodes are deployed to observe

potentially different physical processes (e.g., agricultural, healthcare, safety, or industrial data) of

a certain geographical region. Uplink transmissions are considered, where a UAV collects status

update packets from the ground nodes while seeking to maintain freshness of its information

status about their observed processes during the time of its operation. We assume that each

instant t is denoted by em(t) âˆˆ [0, Emax

ground node m âˆˆ M has a battery with ï¬nite capacity of Emax

m and its battery level at time
m ]. As shown in Fig. 1, the UAV ï¬‚ies at a ï¬xed height
h such that the projection of its ï¬‚ight trajectory on the ground at time instant t is denoted by
Lu(t) (cid:44) (xu(t), yu(t)), where xu(t) and yu(t) represent the projection of the UAVâ€™s location on

the x and y axes, respectively. Furthermore, we deï¬ne vu,x(t) and vu,y(t) as the UAVâ€™s velocity

in the x and y directions at time instant t such that we have:

= vu,x(t),

dxu(t)
dt
âˆ’ vmax

dyu(t)
dt
x â‰¤ vu,x(t) â‰¤ vmax

x

= vu,y(t),

, âˆ’vmax

y â‰¤ vu,y(t) â‰¤ vmax

y

,

(1)

(2)

where vmax

x

and vmax

y

represent the maximum speed of the UAV in the horizontal and vertical

directions, respectively. Due to battery constraints, the UAV can only operate for a ï¬nite time

interval. We model this fact by having a time constraint of Ï„ seconds during which the UAV

ï¬‚ies from an initial location Li

u where it can be recharged to continue its
operation. Similar to [10]â€“[12], the channels between the UAV and ground nodes are assumed

u to a ï¬nal location Lf

to be dominated by the line-of-sight (LoS) links. Therefore, at time instant t, the channel power

gain between the UAV and ground node m is modeled as:

gu,m(t) = Î²0dâˆ’2

u,m(t) =

Î²0

h2 + (cid:107)Lu(t) âˆ’ Lm(cid:107)2 , m âˆˆ M,

(3)

where du,m(t) is the distance between the UAV and node m at time instant t, Lm = [xm, ym] is

the location of node m, and Î²0 is the channel gain at a reference distance of 1 meter.

The AoI of an arbitrary physical process is deï¬ned as the time elapsed since the most recently

received update packet at the UAV was generated at the ground node observing this process. We

6

Fig. 1: An illustration of our system model.

Fig. 2: AoI evolution vs. update time instants.

let Am(t) â‰¥ Amin
t, where Amin

m be the AoI at the UAV for the process observed by node m at time instant
m is the minimum value for Am(t), which is non-zero because of the transmission
delay of the wireless link. Since we do not explicitly model this delay in our setup, we simply

interpret Amin

this is a reasonable assumption since the value of Amin

m as a constant that will correspond to the worst-case transmission delay. Note that
m is negligible compared to the difference
m is in the order of milliseconds whereas
the difference between any two consecutive update time instants is in the order of seconds). Let

between any two consecutive update time instants (Amin

ti,m be the time instant at which node m transmits an update packet for the i-th time. Hence,

the AoI dynamics for the process observed by node m will be:

Am(t) = Amin

m + t âˆ’ tiâˆ’1,m, âˆ€t âˆˆ [tiâˆ’1,m, ti,m) & i âˆˆ {1, . . . , nm} ,

(4)

where t0,m (cid:44) 0 and nm is the total number of updates transmitted by node m. Therefore, as
shown in Fig. 2, when t = ti,m, the AoI of the observed process is reset to Amin
m ; otherwise, the
AoI value increases linearly. By letting S, B, and Ïƒ2 be the size of an update packet, channel

bandwidth, and noise power at the UAV, respectively, the energy required to transmit an update

packet from node m is given according to Shannonâ€™s formula as:

Em(t) =

Ïƒ2
gu,m(t)

(cid:0)2S/B âˆ’ 1(cid:1) .

(5)

ğ‘¡=0ğ‘¡>0ğ‘¡=ğœUAV movementUplink Ground nodeğ‘¡1,ğ‘šğ´ğ‘šminğ´ğ‘šğ‘¡2,ğ‘šğ‘¡â€¦â€¦7

Clearly, when node m is scheduled to transmit an update packet at time instant t, its current

battery level em(t) should be at least equal to Em(t). Therefore, the energy level at node m is
updated as em(t) (cid:44) em(t) âˆ’ Em(t), âˆ€i : t = ti,m.

B. Problem Formulation

Our goal is to characterize the age-optimal policy which determines the UAVâ€™s velocity and

the node scheduled for transmission at every time instant over a ï¬nite horizon of time Ï„ . Let
tm (cid:44) [t1,m, . . . , tnm,m]T be an ordered vector that contains the time instants during which node
m transmits its update packets to the UAV. Then, a policy Ï€ consists of vu,x(t) and vu,y(t), for

all t âˆˆ [0, Ï„ ], and tm for all m âˆˆ M. The objective of the age-optimal policy is to minimize the

NWAoI deï¬ned as follows:

G(t1, . . . , tM ) (cid:44) 2
Ï„ 2

M
(cid:88)

(cid:18)

m=1

Î»m

(cid:90) Ï„

0

(cid:19)

Am(t)dt

,

(6)

is a normalization factor since for a given value of nm, âˆ€m âˆˆ M, we will have

where 2
Ï„ 2
0 < G(t1, . . . , tM ) â‰¤ Ï„ 2
node m with (cid:80)M
m=1 Î»m = 1. Every term of the sum in (6) can be simpliï¬ed as follows:
nm(cid:88)
(cid:90) Ï„

2 . Also, Î»m â‰¥ 0 is the importance weight of the process observed by

(cid:90) ti,m

(cid:90) Ï„

Am(t)dt =

Am(t)dt +

Am(t)dt

0

i=1

nm(cid:88)

(cid:20)

i=1

=

tiâˆ’1,m

tnm,m

Amin

m (ti,m âˆ’ tiâˆ’1,m) +

(ti,m âˆ’ tiâˆ’1,m)2
2

(cid:21)

+ Amin

m (Ï„ âˆ’ tnm,m) +

(Ï„ âˆ’ tnm,m)2
2

nm+1
(cid:88)

= Amin

m Ï„ +

(ti,m âˆ’ tiâˆ’1,m)2
2

m=1
such that tnm+1,m (cid:44) Ï„ . From (7), we can see that Amin
the optimal solution. Thus, we remove Amin
Â¯G(t1, . . . , tM ) (cid:44) 1
Ï„ 2

M
(cid:88)

Î»m

m=1

i=1

,

(7)

m is a ï¬xed value that will have no impact on
m from (7) and deï¬ne a modiï¬ed NWAoI as follows:

nm+1
(cid:88)

(ti,m âˆ’ tiâˆ’1,m)2.

Hence, our goal is to ï¬nd a policy that minimizes the NWAoI in (8) considering the time,

location, speed, and energy constraints, which translates into the following optimization problem:

Â¯G(t1, . . . tM ),

s.t.

Em(ti,m) â‰¤ Emax

m , âˆ€m âˆˆ M,

min
vu,x(t),vu,y(t),t1,...,tM
nm(cid:88)

i=1

Lu(0) = Li
u,

(8)

(9)

(10)

(11)

Lu(Ï„ ) = Lf
u,

= vu,x(t),

dxu(t)
dt
âˆ’ vmax

dyu(t)
dt
x â‰¤ vu,x(t) â‰¤ vmax

x

= vu,y(t),

, âˆ’vmax

y â‰¤ vu,y(t) â‰¤ vmax

y

8

(12)

(13)

(14)

.

Constraint (10) comes from the fact that each nodeâ€™s total energy consumption for packet

transmissions is constrained by its total available energy. The constraints on the initial and ï¬nal

location of the UAV are represented by (11) and (12) whereas the UAVâ€™s velocity constraints

are represented by (13) and (14). Solving (9) is challenging because the number of times each

node transmits its update packets is an unknown variable, thus, (9) needs to be solved for each

choice of nm to obtain the minimum NWAoI. In addition, the constraints on the UAVâ€™s speed

as well as its initial and ï¬nal locations must be satisï¬ed by the UAVâ€™s trajectory, and an energy

constraint is required to be satisï¬ed for each node. Therefore, (9) is a constrained mixed-integer

problem which is challenging to solve [52]. To this end, we provide a relaxation on the problem

that helps us to derive the exact optimal solution using a convex optimization-based approach.

III. CONVEX OPTIMIZATION-BASED AGE-OPTIMAL TRAJECTORY

In order to relax the problem in (9), let us consider ï¬xed values for n1, . . . , nM . In other words,

we will now solve problem (9) assuming that we know how many times each node should send

their update packets to the UAV. In Section IV, we will provide an algorithm to ï¬nd the optimal

values for n1, . . . , nM . We deï¬ne a mapping Î¥ : t1, . . . , tM (cid:55)â†’ tu which maps the time instants
for packet updates of each node to a sequence tu = {t1, . . . , tn} such that n (cid:44) (cid:80)M
i=1 ni and
ti â‰¤ ti+1, âˆ€i = 1, . . . , n. Mapping Î¥ indicates the order with which the nodes must transmit

their packets to the UAV. For instance if ti,j is mapped to tk and tl,q is mapped to tk+1, then

node j transmits its i-th update packet to the UAV before node q transmits its l-th update packet

to the UAV.

We deï¬ne xu (cid:44) [xu,1, . . . , xu,n]T and yu

(cid:44) [yu,1, . . . , yu,n]T such that xu,i (cid:44) xu(ti) and
yu,i (cid:44) yu(ti), 1 â‰¤ i â‰¤ n. Here, (xu,0, yu,0) represents the initial location of the UAV, and

(xu,n+1, yu,n+1) represents the ï¬nal location of the UAV. We deï¬ne t0 = 0 and tn+1 = Ï„ . Now,

from (1) we can write:

xu,i+1 âˆ’ xu,i =

yu,i+1 âˆ’ yu,i =

(cid:90) ti+1

ti
(cid:90) ti+1

ti

vu,x(t)dt, âˆ€i âˆˆ {0, . . . , n} ,

vu,y(t)dt, âˆ€i âˆˆ {0, . . . , n} ,

(15)

(16)

such that (15) and (16) are feasible if:

|xu,i+1 âˆ’ xu,i| â‰¤ vmax

x

(ti+1 âˆ’ ti),

|yu,i+1 âˆ’ yu,i| â‰¤ vmax

y

(ti+1 âˆ’ ti).

9

(17)

(18)

Equations (17) and (18) indicate that the distance between the UAVsâ€™ location in two consecutive

time instants is constrained due to the UAVsâ€™ speed limitations in (14). For example, if (17) and
(18) are satisï¬ed, one solution can be vu,x(t) = xu,i+1âˆ’xu,i
In addition, let xm (cid:44) [xm,1, . . . , xm,nm]T and ym

(cid:44) [ym,1, . . . , ym,nm]T such that xm,i (cid:44)
xu(ti,m) and ym,i (cid:44) yu(ti,m). Note that in this case, the mapping Î¥ maps x1, . . . , xM to xu, and

and vu,y(t) = yu,i+1âˆ’yu,i
ti+1âˆ’ti

ti+1âˆ’ti

.

tâˆˆTm

maps y1, . . . , yM to yu. Now, we can express node mâ€™s energy requirement for constraint (10)
(cid:2)(xm,i âˆ’ xm)2 + (ym,i âˆ’ ym)2 + h2(cid:3). Moreover, we deï¬ne
as (cid:80)
cm (cid:44) Emax
âˆ’ nmh2. Next, we can express the problem in (9) for a given scheduling policy
(order of updates) as follows:

Em(t) =
m Î²0
Ïƒ2(2S/Bâˆ’1)

Ïƒ2(2S/Bâˆ’1)
Î²0

(cid:80)nm
i=1

Â¯G(tu),

min
xu,yu,tu
nm(cid:88)

s.t.

(cid:2)(xm,i âˆ’ xm)2 + (ym,i âˆ’ ym)2(cid:3) â‰¤ cm, âˆ€m âˆˆ M,

i=1

|xu,i+1 âˆ’ xu,i| â‰¤ vmax

x

(ti+1 âˆ’ ti), âˆ€i âˆˆ {0, . . . , n},

|yu,i+1 âˆ’ yu,i| â‰¤ vmax

y

(ti+1 âˆ’ ti), âˆ€i âˆˆ {0, . . . , n},

0 â‰¤ ti â‰¤ Ï„, âˆ€i âˆˆ {1, . . . , n}.

Lemma 1. The problem in (19) is a convex optimization problem.

Proof. The term (cid:80)nm+1

i=1

(ti,m âˆ’ tiâˆ’1,m)2 in (19) can be expressed as:
nm+1
(cid:88)

(ti,m âˆ’ tiâˆ’1,m)2 = tT

mQmtm + Ï„ 2 âˆ’ 2Ï„ tnm,

such that:

1

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Qm =

2 âˆ’1

0

âˆ’1

2 âˆ’1
. . .
. . .

0 âˆ’1
...
. . .
0

Â· Â· Â·

Â· Â· Â·
0
...
. . .
. . .
0
. . . âˆ’1
2

0 âˆ’1

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

.

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Qm is a diagonally dominant matrix meaning that the magnitude of the diagonal entry in

a row is larger than or equal to the sum of the magnitudes of all the other (non-diagonal)

10

entries in that row. Moreover, Qm is symmetric and its diagonal entries are positive. There-
fore, Qm is a positive deï¬nite matrix. Hence, for any m âˆˆ M, (cid:80)nm+1
(ti,m âˆ’ tiâˆ’1,m)2 is
convex thus, (19) is convex [52]. The left hand side of the condition in (10) can be written as
(cid:80)nm
mI nmxm+yT
i=1
nmxnm
is an identity matrix, then, xT
(cid:80)nm

i=1 yi,m+
m where I nm is an identity matrix with nm columns and rows. Since I nm
mI nmxm and yT
mI nmym are convex terms. We now see that,
i=1 yi,m are linear terms and nmxnm
m is a constant value.
Therefore, the constraint in (10) is convex. Meanwhile, the constraints in (21)-(23) are linear.

(cid:2)(xm,i âˆ’ xm)2 + (ym,i âˆ’ ym)2(cid:3) = xT
m + nmynm

i=1 xi,m and âˆ’2ym

mI nmymâˆ’2xm

i=1 xi,mâˆ’2ym

m + nmynm

âˆ’2xm

(cid:80)nm

(cid:80)nm

(cid:80)nm

1

Therefore, (19) is a convex optimization problem which completes the proof.

Moreover, for some special cases, we can derive a closed-form expression for the minimum

NWAoI, as shown next.

A. NWAoI Lower Bound Analysis

A lower bound on the minimum NWAoI can be derived by considering no limits on the UAVâ€™s

speed. To derive this lower bound value, we deï¬ne Â¯nm (cid:44)

as the maximum number

(cid:22)

Emax
m Î²0
Ïƒ2(2S/Bâˆ’1)h2

(cid:23)

of times that node m can send update packets since if the UAV stays on top of node m, it requires

exactly

Ïƒ2(2S/Bâˆ’1)h2
Î²0

amount of energy for each update transmission.

Theorem 1. A lower bound on the minimum NWAoI can be expressed as follows:

Â¯G â‰¥ Â¯Gmin (cid:44)

M
(cid:88)

m=1

Î»m
Â¯nm + 1

.

(26)

Proof. See Appendix A.

Remark 1. Theorem 1 shows that the optimal scheduling policy that results in the lower bound

on NWAoI in (26) is the one that updates every node m periodically after every

Moreover, we can see from (26) that, since Â¯nm is linearly dependent on Emax

Â¯nm+1 seconds.
m , the nodes with
lower battery capacities can have a higher impact on the NWAoI. This can be helpful in deciding

Ï„

on the node importance values, Î»m. For instance, in order to have an equal impact from each

node, Î»m can be chosen to be proportional to Â¯nm + 1.

Although Theorem 1 provides a lower bound on the minimum NWAoI, this lower bound value

may not be achievable in practice because we did not account for the speed limitations of the

11

UAV while deriving this bound. That said, it is natural to wonder about the minimum speed of

the UAV required to achieve the bound in (26), which is studied next. The main idea is that the

UAV receives the updates from the nodes not exactly on top of them but at a small distance

away from them (by using the residual of the energy left from the ï¬‚oor operation in ï¬nding Â¯nm),

which reduces the distance between two update locations and, hence, minimizes the required

speed. In particular, the minimum speed requirement that allows the UAV to achieve the lower

bound in (26) is the solution of the following optimization problem:

v

vmin = min
xu,yu
iÏ„
Â¯nm + 1

s.t. ti,m =

, âˆ€m âˆˆ {1, . . . , M }, i âˆˆ {1, . . . , Â¯nm},

Â¯nm(cid:88)

i=1

(cid:2)(xm,i âˆ’ xm)2 + (ym,i âˆ’ ym)2(cid:3) â‰¤ cm, âˆ€m âˆˆ M,

|xi+1 âˆ’ xi| â‰¤ v(ti+1 âˆ’ ti), âˆ€i âˆˆ {0, . . . ,

|yi+1 âˆ’ yi| â‰¤ v(ti+1 âˆ’ ti), âˆ€i âˆˆ {0, . . . ,

M
(cid:88)

m=1

M
(cid:88)

m=1

Â¯nm},

Â¯nm}.

(27)

(28)

(29)

(30)

(31)

In problem (27), we consider that the update time instants are known and set to be the

ones derived in Theorem 1. The solution should satisfy the nodeâ€™s energy and UAVâ€™s location

constraints in (29)-(31). Also, in (30) and (31), we consider that the maximum allowable speed

of the UAV in directions x and y are equal which is a practical assumption because the UAVâ€™s

motors are usually identical. It can be easily shown that the problem in (27) is a linear program

with convex constraints which can be solved using interior point techniques [52]. However,

the solution may not give us a closed-form expression on the minimum required speed for the

UAV. A closed-form expression could be helpful in choosing the type of UAV or deï¬ning the

parameters of the optimization problem, especially the node weights. Therefore, in the following,

we derive a closed form expression for the upper bound on the UAVâ€™s minimum required speed.

To this end, we deï¬ne a scheduling policy, u, which is a vector that contains the indices of the

scheduled nodes and is ordered based on the scheduled time instants of the nodes. For instance,

letting ui and ui+1 be the i-th and i + 1-th elements of u, the node ui will be scheduled for

transmission one step prior to ui+1. Note that for every u, there exists a vector tu, and, hence,
we will have Â¯G(tu) â‰¡ Â¯G(u). Also let us deï¬ne Â¯u as the scheduling policy that keeps the order

of updates for the optimal time instants derived in Theorem 1. In the following, we derive the

12

upper bound for the UAVâ€™s minimum required speed.

Proposition 1. If no two nodes m and p exist such that Â¯nm + 1 is a divisor of Â¯np + 1 or vice

versa, then the UAVâ€™s minimum speed needed to achieve the minimum NWAoI is upper bounded

by:

(cid:40)

vmin â‰¤ Â¯vmin (cid:44) max

i

|yÂ¯ui+1 âˆ’ yÂ¯ui|
ti+1 âˆ’ ti

,

|xÂ¯ui+1 âˆ’ xÂ¯ui|
ti+1 âˆ’ ti

, âˆ€ i = 0, . . . ,

(cid:41)

Â¯nm

,

M
(cid:88)

m=1

(32)

such that âˆ€i âˆˆ {0, . . . , (cid:80)M

m=1 Â¯nm}, {ti} are the time instants derived in Theorem 1.

Proof. Let xm,i = xm and ym,i = ym for m âˆˆ M and i âˆˆ {1, . . . , Â¯nm}, meaning that the UAV

updates the nodes when it is on top of them. Then, the UAV needs to travel between the top

of two nodes in less than the difference between two consecutive time instants. Therefore, the

distances covered by the UAV between two consecutive updates in x and y directions will be

|xÂ¯ui+1 âˆ’ xÂ¯ui| and |yÂ¯ui+1 âˆ’ yÂ¯ui|, respectively. Moreover, since ti is the time instant of the i-th
update using the policy Â¯u, then the speed requirements for the travel before i-th update are
|xÂ¯ui+1âˆ’xÂ¯ui|
ti+1âˆ’ti

. Therefore, the UAVâ€™s speed has to be at least the maximum value

|yÂ¯ui+1âˆ’yÂ¯ui|
ti+1âˆ’ti

and

of the required speed for all travels, which yields (32). However, if there exists a time instant

ti such that ti = ti+1, then UAVâ€™s speed tends to be inï¬nity which is infeasible. Therefore, we
Â¯np+1 , for all pairs of nodes m and p. To this end, no two nodes m and p

Â¯nm+1 (cid:54)= jÏ„

need to have

iÏ„

must exist such that Â¯nm + 1 is divisor of Â¯np + 1 or vice versa, which completes the proof.

Proposition 1 derives a minimum value on UAVâ€™s speed so as to guarantee achieving the lower

bound on NWAoI if any two nodes do not have equal update time instants. If the UAV needs

to update two nodes at exactly the same time instant, then the required speed can be derived

by solving the problem in (32). If problem (32) does not yield a solution then the lower bound

NWAoI is not achievable. In this case, a policy different than Â¯u should be fed to the problem

in (19) to ï¬nd the update time instants and locations.

Although problem (19) can be solved, it requires the knowledge of scheduling policy, i.e.,

each nodeâ€™s number and order of updates. However, ï¬nding the scheduling policy is challenging

especially when the nodes are equipped with batteries of large capacities since the nodes may

send updates more frequently in such case. In fact, for known values of n1, . . . , nM , there exists
((cid:80)M
(cid:81)M

different orders for updating nodes. Therefore, using a brute force method, the number

m=1 nm)!
m=1 nm!

13

of times one should solve (19) to ï¬nd the optimal solution for the original problem in (9) is:

Â¯n1(cid:88)

Â· Â· Â·

Â¯nM(cid:88)

n1=1

nM =1

(cid:16)(cid:80)M

m=1 nm

(cid:17)

!

(cid:81)M

m=1 nm!

.

(33)

From (33), we can see that ï¬nding the optimal scheduling policy using brute force has a

combinatorial form which is computationally expensive. Hence, in the following, we propose a

similar NCRL method to that in [50] and [51] in order to ï¬nd the optimal scheduling policy for

the nodes without using brute force.

IV. NEURAL COMBINATORIAL BASED DEEP REINFORCEMENT LEARNING FOR OPTIMAL

SCHEDULING

In order to ï¬nd the optimal scheduling policy for the nodes, we ï¬rst propose an NCRL

approach [50] and [51]. Unlike the DRL solution proposed in our early work [46] in which

an environment is deï¬ned as the area within which the nodes are located, our proposed NCRL

considers the problem in (19) as an environment that receives a policy u and outputs the NWAoI,
Â¯G(u). In particular, we consider three main elements for this problem: state of the environment,

action of the UAV, and the reward from the environment as described in the following.

A. State, Action, Reward, and Optimal Scheduling Policy Deï¬nition

The state of the environment can be deï¬ned as a matrix Sn that has n + 1 columns: 1) the

ï¬rst column contains the initial battery levels and the time instant of operation, and 2) every

column after the ï¬rst column contains the energy levels of the nodes after an update as well as

the time instant of that update. In other words, for an update policy u, the (i âˆ’ 1)-th column

of Sn represents the energy levels of the nodes before node ui is updated. Formally, the i-th

column of Sn, will be:

sn,i (cid:44) [E1(ti), . . . , EM (ti), ti]T .

(34)

Furthermore, the initial state is deï¬ned as S0 = s0,1 (cid:44) [Emax

M , 0]T which captures
the available energy of the nodes in the beginning of the problem where the ï¬rst time instant is

, . . . , Emax

1

set to be 0. Also, note that Em(ti) and ti can be obtained for 0 â‰¤ i â‰¤ n and 1 â‰¤ m â‰¤ M by

solving the problem in (19) using the scheduling policy vector u. Therefore, the state space of

this problem is the space of all 2-D matrices with m + 1 rows such that any element at row m

for m âˆˆ M is in [0, Emax

m ] and any element at row M + 1 is in [0, Ï„ ].

14

At any state of the problem, the UAV can either choose to schedule a node for sending an

update packet or terminate the policy. Therefore, an action an at state Snâˆ’1 can get any integer
value in the action set A (cid:44) {0, . . . , M }, such that an = m > 0 means that the node m is

scheduled for transmission; an = 0 terminates the policy, i.e., no new update transmissions will

be added to the current policy. Let unâˆ’1 be a policy that contains n âˆ’ 1 node indexes such

that it transitions state S0 to Snâˆ’1. Then, at every state Snâˆ’1, action an > 0 transitions Snâˆ’1
to Sn such that Sn is the transition from S0 using policy un (cid:44) [unâˆ’1, an]. In other words, at

every state of the problem, the UAV adds a node to the end of the scheduling policy, solves the

problem in (19), and transitions the state of the problem to a new one. While transitioning the

state of the problem, the UAV receives a new NWAoI value from (19) and uses it as a reward

to derive the optimal scheduling policy. In particular, we deï¬ne the reward for every action as

the reduction in the NWAoI value, which can be expressed as:

rn(Snâˆ’1, unâˆ’1, an) = Â¯G(unâˆ’1) âˆ’ Â¯G(un).

(35)

We also deï¬ne Â¯G(u0) = 1 since when policy u is empty, i.e., none of the nodes will be

scheduled for update transmissions in that case, and, hence, the NWAoI will have a maximum

value of 1. Furthermore, we consider that the reward of the termination action an = 0 is 0, i.e.,

rn(Snâˆ’1, unâˆ’1, an = 0) = 0. Using the deï¬nition of the reward in (35), we can see that the

NWAoI for a policy un can be expressed as:
n
(cid:88)

Â¯G(un) = 1 âˆ’

k=1

rk(Skâˆ’1, ukâˆ’1, ak).

(36)

Therefore, the optimal policy that minimizes (36) (which is also the objective function of the

problem in (19)) can be written as follows:

u(cid:63) = arg maxn,un

n
(cid:88)

k=1

rk(Skâˆ’1, ukâˆ’1, ak).

(37)

Owing to the nature of evolution of the problem, represented by Snâˆ’1, an, un, Sn, and

rn(Snâˆ’1, unâˆ’1, an), the problem can be modeled as a ï¬nite-horizon MDP with ï¬nite state and

action spaces. However, due to the curse of extremely high dimensionality in the state space, it

is computationally infeasible to obtain u(cid:63) using the standard ï¬nite-horizon DP algorithm [53].

Motivated by this, we propose next a deep RL algorithm for solving (37). Deep RL is suitable

here because it can reduce the dimensionality of the large state space while learning the optimal

policy at the same time using neural combinatorial optimization methods as in [50] and [51].

15

B. Deep Reinforcement Learning Algorithm

Fig. 3: The deep RL architecture.

The proposed deep RL algorithm has two components: (i) an artiï¬cial neural network (ANN),

that reduces the dimension of the state space by extracting its useful features and (ii) an RL

component, which is used to ï¬nd the best policy based on the ANNâ€™s extracted features, as

shown in Fig. 3. To derive the policy that maximizes the total expected reward of the system,

we use a Q-learning algorithm [53]. In this algorithm, we deï¬ne a state-action value function

Q(Snâˆ’1, an) which is the expected reward of the system starting at state Snâˆ’1, performing action

an and following policy u. In Q-learning algorithm, we try to estimate the Q-function using any

policy that maximizes the future reward. To this end, we use the so-called Bellman update rule:

Qk+1 (Snâˆ’1, an) = Qk (Snâˆ’1, an) + Î²

(cid:16)

rn(Snâˆ’1, unâˆ’1, an) + Î³ max

Î±

Qk (Sn, Î±) âˆ’ Qk (Snâˆ’1, an)

(cid:17)

,

(38)

where Î² is the learning rate, and Î³ is a discount factor. The discount factor can be set to a

value between 0 and 1 if the UAVâ€™s task is continuing which means the task will never end,

and, hence, the current reward will have a higher value compared to the unknown future reward.

However, we have here two terminal cases: 1) when problem (19) does not have a solution for

a scheduling policy un and 2) when an = 0 ( the policy is terminated). Therefore, our problem

is episodic, and so we set Î³ = 1. This aligns with the optimal policy deï¬nition in (37) in which

all of the steps of an episode until the terminal state have equal weights in the evaluation of the

policy.

Since, using (38), the UAV always has an estimate of the Q-function, it can exploit the learning

by taking the action that maximizes the reward. However, when learning starts, the UAV does

Ground nodesUAVExperience 1Experience 2â€¦Replay memoryBatchFully Connected Layerğ‘„-functionPolicyUAVGradientWeight UpdateActionState RewardANN componentRL component16

not have conï¬dence on the estimated value of the Q-function since it may not have visited some

of the state-action pairs. Thus, the UAV has to explore the environment (all state-action pairs) to

some degree. To this end, an (cid:15)-greedy approach is used where (cid:15) is the probability of exploring

the environment at the current state [54], i.e., taking a random action with some probability.

Since the need for exploration goes down with time, one can reduce the value of (cid:15) to 0 as

the learning goes on to ensure that the UAV chooses the optimal action rather than explore the

environment.

The iterative method in (38) can be applied efï¬ciently for the case in which the state space is

small. However, the extremely high dimension of the state space in our problem makes such an

iterative approach impractical, since it requires a large memory and will have a slow convergence

rate. Also, this approach cannot be generalized to unobserved states, since the UAV must visit

every state and take every action to update every state-action pair [53]. Thus, we employ ANNs

which are very effective at extracting features from data points and summarizing them to smaller

dimensions. We use a DQN approach in [54]â€“[56] where the learning steps are the same as in

Q-learning, however, the Q-function is approximated using an ANN Q(Â¯s, a|Î¸), where Â¯s is a

vector representation of the state and Î¸ is the vector containing the weights of the ANN.

In our problem, the states have a matrix form with ï¬xed number of rows and varying number

of columns. However, in order to apply the DQN approach, the state matrix in our problem must

be mapped into a vector representation with ï¬xed number of elements. To do so, we propose

two methods as follows. First, for scenarios with small number of nodes, in which the size

of state matrix Sn is not very large, the last column of the state sn,n can be used as the state

representation since it captures the ï¬nal energy levels of the nodes after all of the updates. Second,

for large-scale scenarios, there will be spatio-temporal interdependencies between the nodes and

their update time instants. Thus, an ANN-based autoencoder can be used to map the varying

size states to a ï¬xed size vector (which will then be used in the DQN) [57]. This autoencoder

will be studied in detail in the following section. After deriving the state representation vector, Â¯s,

a fully connected (FC) layer, as in [54], is used to extract abstraction of the state representation.

In the FC, every artiï¬cial node of a layer is connected to every artiï¬cial node of the next layer

via the weight vector Î¸. The goal is to ï¬nd the optimal values for Î¸ such that the ANN will be

as close as possible to the optimal Q-function. To this end, we deï¬ne a loss function for any

set of (Â¯sn, an, rn, Â¯sn+1), as follows:

L(Î¸k+1) =

(cid:104)

rn + Î³ max

Î±(cid:48)

Q(Â¯sn, Î±(cid:48)|Î¸k) âˆ’ Q(Â¯snâˆ’1, an|Î¸k+1)

(cid:105)2

,

(39)

17

Select an action a:

Set n = 1, initialize an empty policy u0 = [] and NWAoI Â¯G(u0) = 1 and observe the initial state representation Â¯s1.
Repeat:

select a random action a âˆˆ A with probability Îµ ,
otherwise select a = arg minÎ± Q(Â¯sn, Î±|Î¸k).

Algorithm 1 Deep RL for NWAoI minimization
1: Initialize a replay memory that stores the past experiences of the UAV and an ANN for Q-function. Set k = 1.
2: Repeat:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
Derive the gradients for all of the episodes in the batch using (40).
Train the network Q using the average of gradients.
19:
k = k + 1.
20:
21: Until convergence.

Append action a to the end of policy unâˆ’1 as un = [unâˆ’1, a].
Solve (19) using un and ï¬nd Â¯G(un).
Observe the reward rn = Â¯G(unâˆ’1) âˆ’ Â¯G(un) and the new state Â¯sn+1.
Store experience {Â¯sn, an, rn, Â¯sn+1} in the replay memory.
n = n + 1

Until Â¯sn+1 is a terminal state.
Sample a batch of b random experiences {Â¯sÎ·, aÎ·, rÎ·, Â¯sÎ·+1} from the replay memory.
Calculate the target value t:

If the sampled experience is for terminal state then t = rÎ·,
Otherwise t = rÎ· + Î³ minÎ±(cid:48) Q(Â¯sÎ·+1, Î±(cid:48)|Î¸k).

where subscript k + 1 is the episode at which the weights are updated. In addition, we use a

replay memory that saves the evaluation of the state, action, and reward of past experiences, i.e.,

past state-actions pairs and their resulting rewards. Then, after every episode, we sample a batch

of b past experiences from the replay memory and we ï¬nd the gradient of the weights using this

batch as follows:

âˆ‡Î¸k+1L(Î¸k+1) =

(cid:104)
rn + Î³ max

Î±(cid:48)

(cid:105)
Q(Â¯sn, Î±(cid:48)|Î¸k) âˆ’ Q(Â¯snâˆ’1, an|Î¸k+1)

Ã— âˆ‡Î¸k+1Q(Â¯snâˆ’1, an|Î¸k+1).

(40)

Using this loss function, we train the weights of the ANN, Î¸. It has been shown that using

the batch method and replay memory improves the convergence of deep RL [54]. Algorithm 1

summarizes our proposed solution and Fig. 3 shows the architecture of the deep RL algorithm.

As already discussed, the proposed DQN approach can work for state representations with

ï¬xed number of elements. However, the state of the problem, Sn, in our setup has a matrix form

with varying number of columns. Although using the last column of Sn as the state representation

may work in scenarios with small number of nodes, we need to capture spatio-temporal inter-

dependence between the columns of Sn for large-scale scenarios. Therefore, we next propose

a recurrent neural network (RNN) architecture that extracts spatio-temporal interdependencies

between the node energy levels and the update time instants in order to feed into the DQN

algorithm for such large-scale scenarios.

18

C. Long Short-Term Memory-based Structure

Fig. 4: A generic LSTM block architecture.

We study a special RNN architecture, named LSTM cells [58], that can learn time interde-

pendence between the columns of the state and map them into a ï¬xed size 1-dimensional state

representation. In particular, LSTMs have three main components as shown in Fig. 4: 1) a forget

gate which receives an extra input called the cell state input and learns how much it should

memorize or forget from the past, 2) an input gate which aggregates the output of past steps and

the current input and passes it through an activation function as done in a conventional RNN,

and 3) an output gate which combines the current cell state and the output of input gate and

generates the LSTM output [59]. Formally, the relationship between different parts of the LSTM

block in Fig. 4 can be expressed as follows:
(cid:2)hT

f i = Ïƒ(Wf

iâˆ’1, sT

n,nâˆ’i

(cid:3)T

+ bf ),

(cid:2)hT
ri = Ïƒ(Wr
(cid:16)

Ëœci = tanh

Wc

iâˆ’1, sT
(cid:2)hT

(cid:3)T

+ br),

n,nâˆ’i

iâˆ’1, sT

n,nâˆ’i

(cid:17)

(cid:3)T

+ bc

ci = f i âˆ— ciâˆ’1 + ri âˆ— Ëœci,

oi = Ïƒ

(cid:16)

W o

(cid:2)hT

iâˆ’1, sT

n,nâˆ’i

(cid:17)

(cid:3)T

+ bo

hi = oi âˆ— tanh(ci),

(41)

(42)

(43)

(44)

(45)

(46)

where Ïƒ(x) (cid:44) 1

1+eâˆ’x is the sigmoid function, âˆ— represents element-wise multiplication, W f ,
W r, W c, and W o are weight matrices, and bf , br, bc, and bo are bias matrices at the forget,

input, and output gates of the LSTM. Given a state Sn, the LSTM uses every column of Sn,

sn,nâˆ’i as an input and iteratively calculates an output sequence for i âˆˆ {1, . . . , n}. Next, we

show how the cell state and output values can be used as a state representation in our problem.

ÏƒÏƒXX+XÏƒtanhtanhğ’‰ğ’Šâˆ’ğŸğ’‰ğ’Šğ’”ğ’,ğ’ŠRecurrent inputCell state inputRecurrent outputCell state outputInput gateOutput gateInputSigmoidğ’„ğ’Šâˆ’ğŸğ’„ğ’Šğ’‡ğ’Šğ’“ğ’Šà·©ğ‘ªğ’Šğ’ğ’ŠForget gate19

D. LSTM-based Autoencoder Using a Sequence-to-Sequence Model

The LSTM blocks can be used to map the matrix Sn to a vector with ï¬xed size [57]. To this

end, we use the sequence-to-sequence architecture in Fig. 5. Sequence-to-sequence models are

commonly used for translation from a language to another language [57]. In this architecture, we

use two LSTMs: one to receive an input sequence of words (a sentence) in a primary language

and one to generate a new sequence of words (a sentence) in a secondary language. Every word

in the sequence from primary language is fed to the LSTM iteratively until reaching the last word

in the sequence. Then the cell state output, cn, and recurrent output hn, are concatenated into a

vector, Â¯sn. Then, cn and hn are fed into the second LSTM as the initial cell state and recurrent

inputs. Now, the input sequence to the second LSTM will be the sequence of the words from

the secondary language. During the training of this model, the goal is to ï¬nd optimal values for

the weights and biases of the LSTMs such that, in essence, Â¯sn represents the meaning of the

sentence in the primary language. We use the same concept to learn a ï¬xed size representation

of our state space as shown in Fig. 5.

In Fig. 5, we use Sn as the input sequence for the ï¬rst and second LSTMs. In this respect,

each column of Sn represents a word of a sentence in the sequence to sequence model. As a

training trick, in [57], the authors show that the last word in the sentence always must be a

ï¬xed value that represents the end of sentence. To this end, we train our model by ï¬‚ipping the

columns of Sn left to right. In other words, sn,n is used as the ï¬rst input, sn,nâˆ’1 is used as

the second input and so on. This guarantees the last input to be sn,1 which has ï¬xed values

(the energy levels of nodes in the beginning of the problem) as shown in (34). We will use the

concatenation of vectors cn and hn, as the state representation Â¯sn in our DQN.

The size of Â¯sn is a hyperparamter of the model which requires to be optimized. To this end,

in Algorithm 2, we propose an iterative method to ï¬nd the optimal state representation. First

we deï¬ne the weight-based scheduling policy, uÎ», as the one that starts with an empty vector,

then, keep adding nodes to the policy randomly using a multinomial distribution where the

probability of choosing each node n will be its weight in NWAoI, Î»m. We use this policy to

collect experiences from the problem to train our LSTM autoencoder. In other words, for any

uÎ», we solve the problem in (19) and derive the state Sn. Afterwards, we use this state to train

the model in Fig. 5. We train the model using the back propagation method in [58] for different

sizes of Â¯sn and choose the size that has the minimum test mean squared error (MSE). Algorithm

2 shows the steps of the training process.

20

Fig. 5: An LSTM-based autoencoder architecture.

Algorithm 2 Hyperparameter Optimization for LSTM autoencoder-based State Representation
1: Set minimum and maximum sizes, kmin

for the vectors cn and hn to certain values. Set maximum number

h
of episodes Â¯e to a certain value and e = 1 and initialize a memory that stores the past states of the problem.

h , kmax

, kmax
c

, kmin

c

0 = [],

e = e + 1.

nâˆ’1, a(cid:3).

n = (cid:2)uÎ»

nâˆ’1 as uÎ»

Solve (19) using un. If (19) had a solution ï¬nd Sn+1, otherwise, break the loop.

Select an action a using a binomial distribution with probabilities equal to the weights of nodes.
Append action a to the end of policy uÎ»

Store state Sn+1 in the replay memory.
n = n + 1

Set n = 1, initialize an empty policy uÎ»
Repeat:

2: Observe the initial state S0, and store it to the memory.
3: Repeat:
4:
5:
6:
7:
8:
9:
10:
11:
12: Until e + 1 = Â¯e.
13: Split the memory randomly into training and test memories with 7 to 3 ratio.
14: Set k(cid:63)
15: Repeat:
16:
17:
18:
19:
20:
21:
22:
23:
24: Until kc + 1 = kmax
c
c and k(cid:63)
25: Output k(cid:63)
h.

Set kh = kmin
h .
Repeat:

h , and kc = kmin

Initialize an LSTM autoencoder architecture with kc and kh number of elements for cn and hn.
Train the architecture using the states in the training memory and applying back propagation[58].
Derive the average MSE between the states in the test memory and the output of LSTM autoencoder.
If the average MSE is smaller than the minimum MSE so far, set k(cid:63)
Set kh = kh + 1.
Until kh + 1 = kmax
h .
.

c = kc and k(cid:63)

h = kmin

c = kmin

h = kh.

, k(cid:63)

.

c

c

V. SIMULATION RESULTS

For our simulations we consider a rectangular area within the following coordinates: (0, 0),

(0, 1000), (1000, 0), and (1000, 1000). Unless otherwise stated, we consider B = 1 MHz, S = 10

Mbits, Ïƒ2 = âˆ’100 dBm, h = 80 meters, vmax

y = 25 m/s, and Ï„ = 900 seconds. We
randomly generate the x and y coordinates of the initial and ï¬nal location of UAV as well as the

x = vmax

location of the nodes using a uniform distribution on interval [0, 1000] meters. Also, the nodesâ€™

battery levels are drawn uniformly between 0.1 and 1 joules and the each node;s importance

value is drawn uniformly between 0 and 1 and then normalized over the sum of the importance

values. We train the UAV, using the ANN architecture in [54] with no convolutional neural

networks and only one FC layer. We use the Tensorï¬‚ow-Agents library [60] for designing the

environment, policy, and costs. In addition, we use 8 NVIDIA P100 GPU and 20 Gigabits of

memory to train the UAV. All statistical NWAoI results are averaged over 1000 episodes.

LSTM 1LSTM 1LSTM 1LSTM 1LSTM 2LSTM 2LSTM 2LSTM 2ğ’„ğŸğ’‰ğŸğ’„ğŸğ’‰ğŸğ’„ğŸğ’‰ğŸğ’„ğ’ğ’‰ğ’ğ’”ğ’,ğ’ğ’”ğ’,ğ’âˆ’ğŸğ’”ğ’,ğ’âˆ’ğŸğ’”ğ’,ğŸğ’”ğ’,ğ’ğ’”ğ’,ğ’âˆ’ğŸğ’”ğ’,ğ’âˆ’ğŸğ’”ğ’,ğŸğ’„ğŸâ€²ğ’‰ğŸâ€²ğ’„ğŸâ€²ğ’‰ğŸâ€²à´¤ğ’”ğ’21

Fig. 6: Trajectory optimization using convex optimization.

A. Convex Optimization-based Trajectory

In Fig. 6, we consider 3 nodes whose energy levels are randomly drawn between 0.1 and

0.2 joules. The initial and ï¬nal locations of the UAV are at (0, 500) and (500, 500) meters. To

study this scenario, we consider a brute force method and solve problem (19) for all of the

combinations in (33). Fig. 6 shows the optimal trajectory of the UAV as well as each nodeâ€™s

update time instant. Fig. 6 shows that each node can be updated only once during the scenario.

Therefore, the UAV tries to update the nodes as close as possible to Ï„

2 = 450 seconds which is
the optimal update time instant when each node can be updated only once due to Theorem 1.

Moreover, Fig. 6 shows that, at the update time instants, the UAV tries to be as close as possible

to the nodes in order to consume the least energy.

Fig. 7 presents the impact of the number of updates on NWAoI for a simple scenario with only

1 node that has 1 joule energy. From Fig. 7, we observe that the maximum number of times that

the UAV can update the node is 12. However, the minimum NWAoI is achieved after 6 updates.

This is due to the fact that, as seen in Fig. 7, having more updates restricts the node to use small

energy levels for each transmission. Therefore, the UAV needs to be closer to the node at each

update. This can be seen by comparing the policies with 6 (u6) and 12 (u12) updates in Fig.

7. Clearly, the update locations of u6 are more uniformly distributed on the UAVâ€™s trajectory

compared to the update locations of u12 which are distributed closely to the nodeâ€™s location.

Thus, the difference between the time instants of policy u6 are larger and its resulting NWAoI

is smaller than u12. This showcases the importance of action a = 0 which is the terminal action

in the optimal policy (since adding more updates for a node does not necessarily reduce the

0100200300400500600700800900100020030040050060070080090022

Fig. 7: NWAoI vs number of updates for a single node scenario.

Fig. 8: Comparison between the proposed NCRL and LSTM autoencoder with discretized DQN
and weight-based policy for small numbers of nodes.

NWAoI). In fact, in Fig. 7, we compare the brute force method to the proposed NCRL and show

that NCRL can ï¬nd the optimal number of updates for this scenario.

B. Learning-based scheduling policy

Fig. 8 shows the impact of the number of nodes on the NWAoI. In particular, we compare our

proposed NCRL and LSTM autoencoder with a discretized DQN approach proposed in our early

work [46] and the weight-based policy. Fig. 8 shows that both the proposed methods yield lower

NWAoI compared to the discretized DQN and weight-based policies. As the number of nodes

increases, the NWAoI increases for all four policies since: 1) each node will update its process

0246810120.150.20.250.30.350.40.450.50.550200400600800100010020030040050060070080090010001234567891000.10.20.30.40.50.623

Fig. 9: Comparison between the proposed NCRL and LSTM autoencoder with discretized DQN
and weight-based policy for large numbers of nodes.

less often than the case with smaller network, 2) the action space increases, i.e., the number

of feasible scheduling policies increases progressively as shown in (33), which makes ï¬nding

the optimal policy more challenging, and 3) the spatio-temporal interdependence between the

nodesâ€™ locations and their update time instants increases. However, as seen from Fig. 8, while

the gap between NCRL and the discretized/weight-based policy increases when the number of

nodes increases, the gap between NCRL and LSTM autoencoder reduces. This is because, for

larger number of nodes, the LSTM autoencoder starts showing its impact in learning the spatio-

temporal interdependence between the states of the problem. From Fig. 8, we can also observe

that the lower bound on NWAoI (expressed in (26)) does not depend on the number of nodes

since it is only a function of the nodesâ€™ weights and maximum number of allowable updates,

i.e., the number of nodes does not have any impact on that lower bound value.

In Fig. 9, we study the impact of having a large number of nodes on the performance of the

four policies. Fig. 9 demonstrates that, as the number of nodes increases, the proposed LSTM

autoencoder shows its impact and results in a smaller NWAoI compared to NCRL. This shows

that the proposed LSTM autoencoder can capture some interdependencies between the states

of the problem that only using the last column of the state will fail to capture. Therefore, an

LSTM autoencoder can learn better policies compared to NCRL. The reason why the LSTM

autoencoder cannot outperform NCRL for a small number of nodes is because its accuracy is

not 100% when ï¬nding the state representation for short sequence sizes. Therefore, for a small

network, the test error prevents LSTM autoencoder to outperform NCRL. However, for a large

network of nodes, the beneï¬ts of using the LSTM autoencoder is larger than its test error, and,

1015202530354045500.350.40.450.50.550.60.650.70.7524

Fig. 10: The impact of node energy levels on NWAoI.

thus, it can outperform NCRL. Fig. 9 also shows that for large-scale networks, the discretized

DQN in [46] fails to even outperform the weight-based policy since, in this method, the state

space grows exponentially which makes it harder for the DQN to learn a good policy.

Fig. 10 shows the impact of the node energy level on NWAoI. In Fig. 10, we consider 3 nodes

with energy levels randomly drawn from: [0.05, 0.15], [0.15, 0.25], . . . , [0.95, 1.05] joules. Thus,

the average energy level of the nodes will be between 0.1 and 1 joules. Fig. 10 demonstrates that

the proposed NCRL and LSTM autoencoder can outperform the discretized DQN and weight-

based policies. Moreover, Fig. 10 shows that as the energy level of the nodes increases, the

LSTM autoencoder achieves lower NWAoI compared to NCRL. This is due to the fact that

larger energy levels help the UAV update the nodes for a larger number of times which, in turn,

increases the size of the state matrix Sn. Therefore, the effect of the LSTM autoencoder can be

more obvious when the nodesâ€™ energy levels increase. From Fig. 10, we also observe that for

a larger average node energy, the discretized DQN cannot achieve a good performance and in

some cases the weight-based policy has a lower NWAoI. This is because of the nature of the

discretized DQN approach in [46] where the state space and the complexity of the problem grow

progressively with the energy levels of the nodes while the weight-based policyâ€™s complexity does

not depend on the energy levels. Fig. 10 also shows that the lower bound on NWAoI decreases

sub-linearly with respect to the average node energy level which means that the impact of energy

level reduces gradually as the node energy levels increase. Such a sub-linear behavior can be

noticed also for all four policies.

Fig. 11 compares the performance of the proposed NCRL and LSTM autoencoder with

0.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.725

Fig. 11: The impact of time constraint on NWAoI.

Fig. 12: The impact of the UAV speed on NWAoI.

discretized DQN and weight-based policies as a function on the time constraint Ï„ . We consider

3 nodes and solve the problem for different scenarios with time constraint between 5 to 15

minutes. Fig. 11 shows that, as the time constraint increases, the NWAoI becomes smaller since

a larger time constraint gives more opportunity to the UAV to move closer to the nodes and

update the node status more frequently. Moreover, Fig. 11 shows that the proposed NCRL and

LSTM autoencoder can outperform the discretized DQN and weight-based policies. Furthermore,

the performance gap between the four policies stay ï¬xed which indicates that the time constraint

has a general impact on the solution of the problem and does not depend on the policy type.

In Fig. 12, we consider three nodes while the UAV speed varies between 2 and 20 m/s. From

Fig. 12 we notice that for small values of UAV speed, NWAoI is almost similar for NCRL, LSTM

567891011121314150.320.340.360.380.40.420.440.460.480.50.5224681012141618200.30.40.50.60.70.80.926

autoencoder, discretized DQN, and weight-based policy since the UAV cannot cover large areas

and due to its time constraint it may not even update any node. However, as the UAV speed

increases, the NWAoI also decreases because the UAV can move around faster and can update

nodes more frequently. Fig. 12 demonstrates that LSTM autoencoder can achieve even lower

NWAoI values compared to NCRL for higher UAV speeds. This is due to the fact that, the

number of updates increases with the increase in speed which results in larger state matrices.

Therefore, the LSTM autoencoder can learn a better representation of the state which will result

in learning better policies.

VI. CONCLUSION

In this paper, we have investigated the problem of minimizing the NWAoI for a UAV-assisted

wireless network in which a UAV collects status update packets from energy-constrained ground

nodes. First, we have formulated the problem as a mixed-integer program. Then, for a given

scheduling policy, we have proposed a convex optimization-based approach to obtain the UAVâ€™s

optimal ï¬‚ight trajectory and time instants on updates. However, due to the combinatorial nature of

the formulated problem, it is very challenging to ï¬nd the optimal scheduling policy. To overcome

this hurdle, we have proposed a novel NCRL algorithm using DQN to reduce the system state

complexity while learning the optimal scheduling policy at the same time. However, for large-

scale networks, the DQN cannot efï¬ciently learn the optimal scheduling policy. Therefore, we

have then proposed an LSTM autoencoder that can help the proposed deep RL to learn a better

policy for such large-scale scenarios. We have analytically derived a lower bound on the minimum

NWAoI, and obtained an upper bound on the UAVâ€™s minimum speed to achieve that lower

bound value. Our numerical results have shown that the proposed NCRL algorithm signiï¬cantly

outperforms baseline policies, such as the discretized DQN and weight-based policies, in terms

of the achievable NWAoI per process. They have also demonstrated that the achievable NWAoI

by the proposed algorithm is monotonically decreasing with the time constraint of the UAV, the

battery sizes of the ground nodes, and the UAV speed.

APPENDIX

A. Proof of Theorem 1

The minimum required energy for an update from a node m is

Ïƒ2(2S/Bâˆ’1)h2
Î²0

which is the case

when the UAV requests for update from node m while it stays on top of node m, i.e. xm,i = xm

27

and ym,i = yi. In this case, every node m, will be updated Â¯nm times in the entire Ï„ seconds.

However, this requires UAV to move from the top of a node to top of another node in less than

the time difference between two optimal consecutive update time instants. Therefore, in order to

ï¬nd the lower bound on NWAoI, we neglect the limit on the UAVâ€™s speed and ï¬nd the optimal

update time instants for each node. Note that, in this case, we assume that (21) and (22) are
always satisï¬ed. Here, we deï¬ne Î´i,m (cid:44) ti,m âˆ’ tiâˆ’1,m as the difference between two update time

instants of node m. Then, we have:

Â¯G =

1
Ï„ 2

M
(cid:88)

m=1

ï£«

Î»m

ï£­

Â¯nm(cid:88)

i=1

(cid:32)

Î´2
i,m +

Ï„ âˆ’

(cid:33)2ï£¶
ï£¸ .

Î´i,m

Â¯nm(cid:88)

i=1

(47)

Since (47) is a convex function, we take the ï¬rst derivative of Â¯G with respect to Î´i,m, for
1 â‰¤ m â‰¤ M and 1 â‰¤ i â‰¤ Â¯nm, and set it equal to 0 in order to ï¬nd the optimal update time

instants which yields:

âˆ‚ Â¯G
âˆ‚Î´i,m

=

2Î»m
Ï„ 2

(cid:32)

(cid:32)

Î´i,m âˆ’

Ï„ âˆ’

(cid:33)(cid:33)

nm(cid:88)

j=1

Î´j,m

=

2Î»m
Ï„ 2

(cid:32)

2Î´i,m +

nm(cid:88)

j=1,j(cid:54)=i

(cid:33)

Î´j,m âˆ’ Ï„

.

(48)

Thus, for every node m the optimal values for Î´i,m is the solution of the following equation:
ï£¹

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

2

1

2
. . .

1
...
1 Â· Â· Â·

Â· Â· Â·
. . .
. . .

1
...
1

ï£®

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

Î´1,m
...
Î´Â¯nm,m

1

2

ï£¹

ï£®

ï£¹

Ï„
...
Ï„

ï£º
ï£º
ï£º
ï£»

=

ï£¯
ï£¯
ï£¯
ï£°

.

ï£º
ï£º
ï£º
ï£»

(49)

Now, if we subtract the ï¬rst row of the matrix in (49) from all of the other rows we will have:
ï£¹

ï£®

2

1 Â· Â· Â·

Â· Â· Â·

1

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

0
. . .
. . .

âˆ’1 1
...
0
...
...
âˆ’1 0 Â· Â· Â·

ï£®

ï£¹

Â· Â· Â·
. . .
. . .

0
...
0

ï£®

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£¹

ï£º
ï£º
ï£º
ï£»

Î´1,m
...
Î´Â¯nm,m

=

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Ï„

0
...
0

,

ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

(50)

1
which yields Î´1,m = Î´2,m = Â· Â· Â· = Î´Â¯nm,m = Ï„

0

Â¯Gmin =

1
Ï„ 2

M
(cid:88)

m=1

which can be simpliï¬ed to (26).

Â¯nm+1. Therefore, the optimal NWAoI will be:
(cid:19)2

(cid:18) Ï„

Î»m (Â¯nm + 1)

,

(51)

Â¯nm + 1

[1] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, â€œA tutorial on UAVs for wireless networks: Applications,

challenges, and open problems,â€ IEEE Commun. Surveys & Tutorials, vol. 21, pp. 2334 â€“ 2360, 2019.

REFERENCES

28

[2] U. Challita, A. Ferdowsi, M. Chen, and W. Saad, â€œMachine learning for wireless connectivity and security of cellular-

connected UAVs,â€ IEEE Wireless Commun., vol. 26, no. 1, pp. 28â€“35, Feb. 2019.

[3] R. I. Bor-Yaliniz, A. El-Keyi, and H. Yanikomeroglu, â€œEfï¬cient 3-D placement of an aerial base station in next generation

cellular networks,â€ in Proc. of IEEE Intl. Conf. on Commun. (ICC), Kuala Lumpur, Malaysia, May 2016.

[4] M. M. Azari, F. Rosas, K.-C. Chen, and S. Pollin, â€œJoint sum-rate and power gain analysis of an aerial base station,â€ in

Proc. of IEEE Global Commun. Workshops (GC Wkshps), Washington, DC, US, December 2016.

[5] M. Alzenad, A. El-Keyi, F. Lagum, and H. Yanikomeroglu, â€œ3-D placement of an unmanned aerial vehicle base station

(UAV-BS) for energy-efï¬cient maximal coverage,â€ IEEE Wireless Commun. Letters, vol. 6, no. 4, pp. 434â€“437, Aug. 2017.

[6] M. Mozaffari, A. T. Z. Kasgari, W. Saad, M. Bennis, and M. Debbah, â€œBeyond 5G with UAVs: Foundations of a 3D

wireless cellular network,â€ IEEE Trans. on Wireless Commun., vol. 18, no. 1, pp. 357â€“372, November 2018.

[7] A. Eldosouky, A. Ferdowsi, and W. Saad, â€œDrones in distress: A game-theoretic countermeasure for protecting UAVs

against GPS spooï¬ng,â€ IEEE Internet of Things Journal, vol. 7, no. 4, pp. 2840â€“2854, 2020.

[8] M. A. Kishk, A. Bader, and M.-S. Alouini, â€œOn the 3-D placement of airborne base stations using tethered UAVs,â€ IEEE

Trans. on Commun., 2020.

[9] M. A. Abd-Elmagid, N. Pappas, and H. S. Dhillon, â€œOn the role of age of information in the Internet of Things,â€ IEEE

Commun. Magazine, vol. 57, no. 12, pp. 72â€“77, December 2019.

[10] Y. Zeng, R. Zhang, and T. J. Lim, â€œThroughput maximization for UAV-enabled mobile relaying systems,â€ IEEE Trans. on

Commun., vol. 64, no. 12, pp. 4983â€“4996, Dec. 2016.

[11] L. Xie, J. Xu, and R. Zhang, â€œThroughput maximization for UAV-enabled wireless powered communication networks,â€

IEEE Internet of Things Journal, vol. 6, no. 2, pp. 1690â€“1703, October 2019.

[12] P. Li and J. Xu, â€œPlacement optimization for UAV-enabled wireless networks with multi-hop backhauls,â€ Journal of

Commun. and Information Networks, vol. 3, no. 4, pp. 64â€“73, Dec. 2018.

[13] M. Samir, S. Sharafeddine, C. M. Assi, T. M. Nguyen, and A. Ghrayeb, â€œUAV trajectory planning for data collection from

time-constrained IoT devices,â€ IEEE Trans. on Wireless Commun., vol. 19, no. 1, pp. 34â€“46, September 2020.

[14] A. Farajzadeh, O. Ercetin, and H. Yanikomeroglu, â€œUAV data collection over NOMA backscatter networks: UAV altitude

and trajectory optimization,â€ in IEEE Intl. Conf. on Commun. (ICC), Shanghai, China, May 2019.

[15] M. Banagar and H. S. Dhillon, â€œPerformance characterization of canonical mobility models in drone cellular networks,â€

IEEE Trans. on Wireless Commun., April 2020.

[16] M. Monwar, O. Semiari, and W. Saad, â€œOptimized path planning for inspection by unmanned aerial vehicles swarm with

energy constraints,â€ in Proc. of IEEE Global Commun. Conf. (GLOBECOM), Dec. 2018.

[17] Y. Du, K. Yang, K. Wang, G. Zhang, Y. Zhao, and D. Chen, â€œJoint resources and workï¬‚ow scheduling in UAV-enabled

wirelessly-powered MEC for IoT systems,â€ IEEE Trans. on Vehicular Technology, vol. 68, no. 10, pp. 10 187â€“10 200,

2019.

[18] F. Cui, Y. Cai, Z. Qin, M. Zhao, and G. Y. Li, â€œMultiple access for mobile-UAV enabled networks: Joint trajectory design

and resource allocation,â€ IEEE Trans. on Commun., vol. 67, no. 7, pp. 4980â€“4994, April 2019.

[19] Y.-J. Chen and D.-Y. Huang, â€œTrajectory optimization for cellular-enabled UAV with connectivity outage constraint,â€ IEEE

Access, vol. 8, pp. 29 205â€“29 218, 2020.

[20] Y. Hu, M. Chen, W. Saad, H. V. Poor, and S. Cui, â€œMeta-reinforcement learning for trajectory design in wireless UAV

networks,â€ Available online: arxiv.org/abs/2005.12394, 2020.

[21] S. Kaul, R. Yates, and M. Gruteser, â€œReal-time status: How often should one update?â€ in Proc. of IEEE Conf. on Computer

Commun., May 2012.

29

[22] A. Kosta, N. Pappas, and V. Angelakis, â€œAge of information: A new concept, metric, and tool,â€ Foundations and Trends

in Networking, vol. 12, no. 3, pp. 162â€“259, Nov. 2017.

[23] Y. Sun, E. Uysal-Biyikoglu, R. D. Yates, C. E. Koksal, and N. B. Shroff, â€œUpdate or wait: How to keep your data fresh,â€

IEEE Trans. on Info. Theory, vol. 63, no. 11, pp. 7492â€“7508, Nov. 2017.

[24]

I. Kadota, E. Uysal-Biyikoglu, R. Singh, and E. Modiano, â€œMinimizing the age of information in broadcast wireless

networks,â€ in Proc. of Allerton Conf. on Commun., Control, and Computing, Sept. 2016.

[25] Y.-P. Hsu, E. Modiano, and L. Duan, â€œScheduling algorithms for minimizing age of information in wireless broadcast

networks with random arrivals,â€ IEEE Trans. on Mobile Computing, August 2019.

[26] B. Buyukates, A. Soysal, and S. Ulukus, â€œAge of information in Two-hop multicast networks,â€ in Proc. of IEEE Asilomar,

2018.

[27] J. Li, Y. Zhou, and H. Chen, â€œAge of information for multicast transmission with ï¬xed and random deadlines in IoT

systems,â€ IEEE Internet of Things Journal, March 2020.

[28] B. Zhou and W. Saad, â€œJoint status sampling and updating for minimizing age of information in the Internet of Things,â€

IEEE Trans. Commun., vol. 67, no. 11, pp. 7468â€“7482, July 2019.

[29] M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas, â€œOnline age-minimal sampling policy for RF-powered IoT networks,â€

in Proc. of IEEE Global Commun. Conf. (GLOBECOM), Waikoloa, HI, USA, Dec. 2019.

[30] G. Stamatakis, N. Pappas, and A. Traganitis, â€œOptimal policies for status update generation in an IoT device with

heterogeneous trafï¬c,â€ IEEE Internet of Things Journal, vol. 7, no. 6, pp. 5315 â€“ 5328, February 2020.

[31] M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas, â€œA reinforcement learning framework for optimizing age of information

in RF-powered communication systems,â€ IEEE Trans. Commun., May 2019.

[32] B. Zhou and W. Saad, â€œMinimum age of information in the internet of things with non-uniform status packet sizes,â€ IEEE

Trans. on Wireless Commun., vol. 19, pp. 1933 â€“ 1947, December 2020.

[33] Q. Wang, H. Chen, Y. Gu, Y. Li, and B. Vucetic, â€œMinimizing the age of information of cognitive radio-based IoT systems

under a collision constraint,â€ Available online: arxiv.org/abs/2001.02482, 2020.

[34] M. A. Abd-Elmagid, H. S. Dhillon, and N. Pappas, â€œAoI-optimal joint sampling and updating for wireless powered

communication systems,â€ 2020, available online: arxiv.org/abs/2006.06339.

[35] M. Emara, H. ElSawy, and G. Bauch, â€œA spatiotemporal model for peak AoI in uplink IoT networks: Time vs event-

triggered trafï¬c,â€ Available online: arxiv.org/abs/1912.07855, 2019.

[36] P. D. Mankar, Z. Chen, M. A. Abd-Elmagid, N. Pappas, and H. S. Dhillon, â€œThroughput and age of information in a

cellular-based IoT network,â€ 2020, available online: arxiv.org/abs/2005.09547.

[37] P. D. Mankar, M. A. Abd-Elmagid, and H. S. Dhillon, â€œSpatial distribution of the mean peak age of information in wireless

networks,â€ 2020, available online: arxiv.org/abs/2006.00290.

[38] M. K. Abdel-Aziz, C.-F. Liu, S. Samarakoon, M. Bennis, and W. Saad, â€œUltra-reliable low-latency vehicular networks:

Taming the age of information tail,â€ in Proc. of IEEE Global Commun. Conf. (GLOBECOM), Abu Dhabi, United Arab

Emirates, Dec. 2018.

[39] M. Bastopcu and S. Ulukus, â€œMinimizing age of information with soft updates,â€ Journal of Commun. and Networks,

vol. 21, no. 3, pp. 233â€“243, 2019.

[40] E. Altman, R. El-Azouzi, D. S. Menasche, and Y. Xu, â€œForever young: Aging control for hybrid networks,â€ in Proc., IEEE

Intl. Symposium on Mobile Ad Hoc Networking and Computing, 2019.

[41] M. A. Abd-Elmagid and H. S. Dhillon, â€œAverage peak age-of-information minimization in UAV-assisted IoT networks,â€

IEEE Trans. on Veh. Technology, vol. 68, no. 2, pp. 2003â€“2008, Feb. 2019.

30

[42] J. Liu, X. Wang, B. Bai, and H. Dai, â€œAge-optimal trajectory planning for UAV-assisted data collection,â€ in Proc. of IEEE

Conf. on Computer Commun. Workshops (INFOCOM Wkshps), Honolulu, HI, USA, July 2018.

[43] Z. Jia, X. Qin, Z. Wang, and B. Liu, â€œAge-based path planning and data acquisition in UAV-assisted IoT networks,â€ in

Proc. of IEEE Intl. Conf. on Commun. Workshops (ICC Wkshps), July 2019.

[44] G. Ahani, D. Yuan, and Y. Zhao, â€œAge-optimal UAV scheduling for data collection with battery recharging,â€ Available

online: arxiv.org/abs/2005.00252, 2020.

[45] W. Li, L. Wang, and A. Fei, â€œMinimizing packet expiration loss with path planning in UAV-assisted data sensing,â€ IEEE

Wireless Commun. Letters, vol. 8, no. 6, pp. 1520â€“1523, July 2019.

[46] M. A. Abd-Elmagid, A. Ferdowsi, H. S. Dhillon, and W. Saad, â€œDeep reinforcement learning for minimizing age-of-

information in UAV-assisted networks,â€ in Proc. of IEEE Global Commun. Conf. (GLOBECOM), 2019.

[47] C. Zhou, H. He, P. Yang, F. Lyu, W. Wu, N. Cheng, and X. Shen, â€œDeep RL-based trajectory planning for AoI minimization

in UAV-assisted IoT,â€ in Proc. of IEEE Wireless Communications and Signal Processing (WCSP), Xiâ€™an, China, Oct. 2019.

[48] S. F. Abedin, M. Munir, N. H. Tran, Z. Han, and C. S. Hong, â€œData freshness and energy-efï¬cient UAV navigation

optimization: A deep reinforcement learning approach,â€ Available online: arxiv.org/abs/2003.04816, 2020.

[49] M. Yi, X. Wang, J. Liu, Y. Zhang, and B. Bai, â€œDeep reinforcement learning for fresh data collection in UAV-assisted IoT

networks,â€ Available online: arxiv.org/abs/2003.00391, 2020.

[50]

I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio, â€œNeural combinatorial optimization with reinforcement learning,â€

Available online: arXiv.org/abs/1611.09940, 2016.

[51] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, â€œLearning combinatorial optimization algorithms over graphs,â€ in

Advances in Neural Information Processing Systems, Long Beach, CA, USA, December 2017.

[52] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press, 2004.

[53] W. B. Powell, Approximate Dynamic Programming: Solving the curses of dimensionality.

John Wiley & Sons, 2007, vol.

703.

[54] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,

G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,

â€œHuman-level control through deep reinforcement learning,â€ Nature, vol. 518, no. 7540, p. 529, 2015.

[55] A. Ferdowsi and W. Saad, â€œDeep learning for signal authentication and security in massive Internet-of-Things systems,â€

IEEE Trans. on Commun., vol. 67, no. 2, pp. 1371â€“1387, October 2019.

[56] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, â€œRobust deep reinforcement learning for security and safety

in autonomous vehicle systems,â€ in Proc. of Intl. Conf. on Intelligent Transportation Systems (ITSC), Maui, HI, USA,

December 2018.

[57]

I. Sutskever, O. Vinyals, and Q. V. Le, â€œSequence to sequence learning with neural networks,â€ in Advances in neural

information processing systems, 2014, pp. 3104â€“3112.

[58] A. Graves, A.-r. Mohamed, and G. Hinton, â€œSpeech recognition with deep recurrent neural networks,â€ in Proc. of IEEE

Intl. Conf. on Acoustics, Speech and Signal Processing, Vancouver, BC, Canada, May 2013.

[59] A. Ferdowsi and W. Saad, â€œDeep learning-based dynamic watermarking for secure signal authentication in the Internet of

Things,â€ in Proc. of IEEE Intl. Conf. on Commun. (ICC), Kansas City, MO, USA, May 2018.

[60] S. Guadarrama, A. Korattikara, O. Ramirez, P. Castro, S. F. Ethan Holly, E. G. Ke Wang, C. Harris, V. Vanhoucke, and

E. Brevdo, â€œTF-Agents: A library for reinforcement learning in tensorï¬‚ow,â€ https://github.com/tensorï¬‚ow/agents, 2018.

