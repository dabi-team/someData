Neural Sequence-to-grid Module for Learning Symbolic Rules

Segwang Kim, 1 Hyoungwook Nam, 2 Joonyoung Kim, 1 Kyomin Jung1
1 Department of Electrical and Computer Engineering, Seoul National University, Seoul, Republic of Korea
2 Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, Illinois, USA

@snu.ac.kr, hn5@illinois.edu
ksk5693, kimjymcl, kjung
}

{

1
2
0
2

r
p
A
6
2

]

G
L
.
s
c
[

2
v
1
2
9
4
0
.
1
0
1
2
:
v
i
X
r
a

Abstract

Logical reasoning tasks over symbols, such as learning arith-
metic operations and computer program evaluations, have be-
come challenges to deep learning. In particular, even state-
of-the-art neural networks fail to achieve out-of-distribution
(OOD) generalization of symbolic reasoning tasks, whereas
humans can easily extend learned symbolic rules. To re-
solve this difﬁculty, we propose a neural sequence-to-grid
(seq2grid) module, an input preprocessor that automatically
segments and aligns an input sequence into a grid. As our
module outputs a grid via a novel differentiable mapping, any
neural network structure taking a grid input, such as ResNet
or TextCNN, can be jointly trained with our module in an
end-to-end fashion. Extensive experiments show that neural
networks having our module as an input preprocessor achieve
OOD generalization on various arithmetic and algorithmic
problems including number sequence prediction problems,
algebraic word problems, and computer program evaluation
problems while other state-of-the-art sequence transduction
models cannot. Moreover, we verify that our module en-
hances TextCNN to solve the bAbI QA tasks without external
memory.

Introduction
Symbolic reasoning tasks such as learning arithmetic opera-
tions or evaluating computer programs offer solid standards
for validating the logical inference abilities of deep learning
models. Among machine learning tasks, symbolic reason-
ing problems are apt for testing mathematical, algorithmic,
and systematic reasoning as they have strict rules mapping
a given input to a well-deﬁned unique target. In particular,
a large body of works on deep learning has considered se-
quence transduction problems for symbolic reasoning. Some
symbolic problems such as copying sequences (Dehghani
et al. 2018; Graves, Wayne, and Danihelka 2014; Grefen-
stette et al. 2015; Rae et al. 2016; Zaremba and Sutskever
2014) and arithmetic addition (Graves, Wayne, and Dani-
helka 2014; Joulin and Mikolov 2015; Kaiser and Sutskever
2015; Kalchbrenner, Danihelka, and Graves 2015; Saxton
et al. 2019; Wangperawong 2018) can be solved after un-
derstanding simple rules regardless of the inputs. Others
demand a deep learning model to discover necessary rules

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

and apply them depending on inputs given as natural lan-
guage words (Li et al. 2019; Wang, Liu, and Shi 2017; We-
ston et al. 2015), complex mathematical equations (Lample
and Charton 2019), or programming snippets (Zaremba and
Sutskever 2014).

Among them, symbolic reasoning problems can test
whether a trained deep learning model can systematically
extend rules to out-of-distribution (OOD) data that follow
a distinct distribution from the training data (Keysers et al.
2019; Lake and Baroni 2017; Saxton et al. 2019). For in-
stance, a model for the addition problem whose training in-
puts are a pair of numbers up to ﬁve digits, say 5872+13,
can face an OOD input of a pair of two 6-digit numbers
upon the testing phase, e.g., 641436+135321. Human in-
telligence with algebraic mind can naturally extend learned
rules (Marcus 2003), yet it is non-trivial to equip deep learn-
ing models for sequence transduction problems to handle
OOD generalization.

However,

it has been found that popular sequence
transduction neural networks, such as LSTM seq2seq
model (Sutskever, Vinyals, and Le 2014) and Transformer
(Vaswani et al. 2017), rarely extend learned rules in that
they are inclined to mimic the training data distribution (De-
hghani et al. 2018; Lake and Baroni 2017). There have been
signiﬁcant initial efforts to improve a model’s abilities to
extend learned rules. However, their success has been de-
pendent on the direct use of numerical values (Trask et al.
2018) or has been limited to rudimentary logic such as
copying sequences (Dehghani et al. 2018; Graves, Wayne,
and Danihelka 2014; Grefenstette et al. 2015; Rae et al.
2016; Zaremba and Sutskever 2014) and binary arithmetic
(Graves, Wayne, and Danihelka 2014; Joulin and Mikolov
2015; Kaiser and Sutskever 2015). Furthermore, OOD gen-
eralization on symbolic problems for complex or context-
dependent logic forms such as decimal arithmetic, algebraic
word problems, computer program evaluation problems has
not been tackled. Our objective is to ﬁll this gap and design
a module that helps neural networks to achieve OOD gener-
alization in these problems.

One observation from a previous study (Nam, Kim, and
Jung 2019) is that typical sequence transduction neural net-
works cannot process OOD instances of number sequence
prediction problems, such as predicting a Fibonacci se-
quence. However, when an input sequence is manually seg-

 
 
 
 
 
 
mented and aligned into a grid of digits, a CNN can easily
process OOD instances. This means providing the aligned
grid input enables to exploit inductive bias by the convo-
lution’s local and parallel computation. The grid, however,
must be handcrafted in the study, which is inapplicable for
general sequence transduction tasks. Overcoming this limi-
tation requires a new input preprocessing module that auto-
matically aligns an input sequence into a grid without super-
vision for the alignment.

In this work, we propose a neural sequence-to-grid
(seq2grid) module, an input preprocessor that learns how to
segment and align an input sequence into a grid. The grid
syntactically aligned by our module is then semantically de-
coded by a neural network. In particular, our module pro-
duces a grid by a novel differentiable mapping called nested
list operation inspired by Stack RNN (Joulin and Mikolov
2015). This mapping enables a joint training of our module
and the neural network in an end-to-end fashion via a back-
propagation.

Experimental results show that ResNets with our seq2grid
module achieve OOD generalization on various arithmetic
and algorithmic reasoning problems, such as number se-
quence prediction problems, algebraic word problems, and
computer program evaluation problems. These are nearly
impossible for other contemporary sequence-to-sequence
models including LSTM seq2seq models and Transformer-
based models. Speciﬁcally, we ﬁnd that the seq2grid can
infuse an input context into a grid so that doing arithmetic
under linguistic instructions or selecting the true branch of
if/else statements in code snippets become possible. Fur-
ther, we demonstrate that the seq2grid module can enhance
TextCNN to solve the bAbI QA tasks without the help of ex-
ternal memory. From all the aforementioned problems, we
verify the generality of the seq2grid module in that it au-
tomatically preprocesses the sequential input into the grid
input in a data-driven way.

Motivation for Sequence-to-grid Method
To demonstrate the beneﬁts of the sequence-to-grid prepro-
cessing method for symbolic reasoning tasks, we devise a
toy decimal addition problem in two different setups: se-
quential and grid-structured. Figure 1 illustrates how the
problem is deﬁned in both setups and shows why alignment
on a grid makes it easier. If the lengths of the numbers in-
crease, the temporal distances between corresponding digits,
e.g., 2 and 3, also increase in the sequential setup. How-
ever, the spatial distances between them remain constant in
the grid-structured setup since they are manually aligned ac-
cording to their digits. Therefore, we can expect that the lo-
cal and parallel nature of convolution will extend the rule
to longer inputs, while sequence transduction models will
struggle to handle the increased distances.

To see this, we trained deep learning models1 using num-
bers up to ﬁve digits and validate on six separate validation
sets, each of which contains only k-digit (k = 3, . . . , 8) num-

1The models had the same conﬁgurations used in arithmetic and
algorithmic problems (refer to experiments) except for the CNN
that was the grid decoder of the S2G-CNN.

Figure 1: The illustration of the toy decimal addition prob-
lem. Each symbol is stored with its representation vector.

Figure 2: The validation accuracy results of the toy problem.
Each column shows results from the k-digit set, where the
three rightmost sets are OOD.

bers. Hence, the validation results from the former three sets
tested in-distribution (ID) generalization, whereas the lat-
ter three tested OOD generalization. While the input and
the target in the sequential setup were sequentially fed
to sequence transduction models such as LSTM seq2seq
model (Sutskever, Vinyals, and Le 2014) and Transformer
(Vaswani et al. 2017), those in the grid-structured setup were
fed to a ResNet-based CNN model (He et al. 2016). As
expected, Figure 2 shows that extending the addition rule
to OOD validation sets is easy in the grid-structured setup,
whereas it is extremely difﬁcult in the sequential setup.

Therefore, providing aligned grid input for local and par-
allel computation can be key to achieving OOD generaliza-
tion. However, manual preprocessing that aligns an input se-
quence into a grid is impossible for most symbolic prob-
lems. For instance, in computer program evaluation prob-
lems, symbols within the code snippet can represent not only
integers but also programming instructions so that it is non-
trivial to manually align those symbols on the grid. Like-
wise, in the bAbI QA tasks, questions and stories given as
natural language have no ground-truth alignment which we

5822+13.5822+13.Sequential InputGrid InputTarget5835.can exploit for preprocessing in advance. Accordingly, we
need a data-driven preprocessing method that automatically
aligns an input symbol sequence into a grid for general sym-
bolic tasks. We implement it by designing a sequence-to-
grid module executing novel nested list operations.

Related Work
Symbolic Reasoning Tasks Symbolic reasoning requires
discovering the underlying rules of a data distribution rather
than mimicking data patterns. Hence, there have been stud-
ies to formulate symbolic reasoning tasks in machine learn-
ing problems to examine the mathematical and systematic
(rule-based) reasoning abilities of deep learning models.
Additions of unprecedented long binary numbers (Graves,
Wayne, and Danihelka 2014; Joulin and Mikolov 2015;
Kaiser and Sutskever 2015) or Number sequence predic-
tion problems (Nam, Kim, and Jung 2019) are studied. Also,
school-level math problems including algebraic word prob-
lems are uniﬁed (Saxton et al. 2019). Evaluating program
snippets (Zaremba and Sutskever 2014) further requires al-
gorithmic abilities. Besides extending mathematical rules,
systematic generalization abilities in synthetic natural lan-
guage tasks are tested by the bAbI QA tasks (Weston et al.
2015) or the SCAN problems (Lake and Baroni 2017).

Memory Augmented Neural Network Storing all input
information into external memory and querying over it is
one way to tackle symbolic reasoning tasks. Such neural net-
works, also known as memory augmented neural networks
(MANN), vary according to their memory structures and
controllers. Here, the memory controller is a neural network
that reads an input symbol and its external memory, encodes
the symbol, and write it on the memory. After the incipi-
ent MANNs like Memory network (Sukhbaatar et al. 2015)
were introduced, studies about implementing Automata with
differentiable tape as neural networks (Graves et al. 2016;
Rae et al. 2016; Joulin and Mikolov 2015; Grefenstette et al.
2015) has been carried out. We emphasize that our prepro-
cessed grid input can be seen as another representation of a
sequential input rather than a memory used in MANNs; the
RNN encoder of our module does not read the grid and the
symbol embedding is directly written to the grid rather than
passed through neural network layers.

Neural-Symbolic Learning Another approach for OOD
generalization in symbolic problems is Neural-Symbolic-
approach that integrates the connectionist and the symbolist
paradigms. Neural Programmer Interpreter (NPI) (Reed and
De Freitas 2015) and its recursion variant (Cai, Shin, and
Song 2017) have been proposed for solving compositional
programs through sequential subprograms. Also, (Chen,
Liu, and Song 2017) proposes a reinforcement learning-
based approach with structured parse-trees. Recently, Neural
Symbolic Reader (Chen et al. 2019) trains models with weak
supervision for generalization. However, our approach via
automatic alignment without domain-speciﬁc knowledge is
distinct from neural-symbolic approaches which require all
of the supervision for sequential sub-operations.

Figure 3: The sequence-input grid-output architecture.

Method
In this section, we ﬁrst describe a sequence-input grid-
output architecture consisting of a neural sequence-to-grid
(seq2grid) module and a grid decoder. Then, we introduce
how the seq2grid module preprocesses a sequence input as a
grid input. Finally, we explain nested list operations that are
executed by the seq2grid module.

Sequence-input Grid-output Architecture

The key idea of the sequence-to-grid method is to decouple
symbolic reasoning into two steps: automatically aligning
an input sequence into a grid, and doing semantic compu-
tations over the grid. Hence, we propose the sequence-input
grid-output architecture consisting of a seq2grid module and
a grid decoder as shown in Figure 3. The seq2grid module
preprocesses a sequential input into a grid input. The grid
decoder, a neural network that can handle two-dimensional
inputs, predicts the target from the grid input. Practically, we
choose the grid decoder like ResNet or TextCNN according
to problems. Note that our approach that separates the syn-
tactic (=alignment) and semantic processing is similar to the
syntactic attention (Russin et al. 2020).

Neural Sequence-to-grid Module

The main challenge for implementing the seq2grid module
is that the grid must be formed via differentiable mappings
to ensure an end-to-end training. To do so, we design the
seq2grid module with an RNN encoder that gives an action
sequence for differentiable nested list operations.

Formally, the seq2grid module works as follows. First, for
Rh
an input sequence given as symbol embeddings E(t)
where t = 1, . . . , T , the RNN encoder maps (E(t), r(t−1))
Rh. Then, a dense layer followed by a softmax
into r(t)
R3. Next, starting
layer computes an action: r(t)
a(t)
∈
(Rh)W ×H , a series
from the zero-initialized grid G(0)

(cid:55)→

∈

∈

∈

Sequence-to-gridModuleSequential Inputwhat_is_24_+_5?5+42tahwGrid InputGrid DecoderOutputof nested list operations sequentially push the input sym-
bol E(t) into the previous grid G(t−1) in different extents
under the action a(t). As a result, we obtain the grid input
(Rh)W ×H that will be fed through a grid decoder.
G(T )
Note that all aforementioned mappings are differentiable in-
cluding nested list operations which we will explain below.

∈

∈

∈

Nested List Operations
To understand how the nested list operations work, we ﬁrst
(Rh)H×W as a nested list consisting of
regard the grid G
H lists of W slots, where each slot is a vector of dimension
(Rh)W where G1 is the top
h. We denote the i-th list as Gi
list. Likewise, the j-th slot vector in the i-th list is denoted
Rh where Gi,1 is the leftmost slot of the i-th list.
as Gi,j
Now, we deﬁne a differetiable map that pushes the input
R3.
symbol E(t)
∈
Here, each component of a(t) = (a(t)
N LP , a(t)
N OP )
is the probability of performing one of three nested list
operations: top-list-update (a(t)
N LP ),
and no-op (a(t)
N OP ). As shown in Figure 4, G(t−1) with
(E(t), a(t)) grows to G(t):

Rh into the grid under the action a(t)

T LU ), new-list-push (a(t)

T LU , a(t)

∈

∈

G(t) = a(t)

T LU T LU (t) + a(t)

N LP N LP (t) + a(t)

N OP G(t−1),

i

1,j−1

for i > 1,

for j > 1,

and N LP (t)

where T LU (t)

∈
T LU (t)
T LU (t)
T LU (t)

(Rh)H×W is deﬁned as
1,1 = E(t),
1,j = G(t−1)
i = G(t−1)
(Rh)H×W is deﬁned as
1 = (E(t), E∅, . . . , E∅),
i = G(t−1)
Rh is the empty symbol
Here, E∅ := 0
. Accordingly,
∅
the zero-initialized grid G(0) = (E∅, . . . , E∅) grows to the
ﬁnal grid G(T ) as time goes. By doing so, we “preprocess”
the input sequence into the grid input in that each slot of
G(T ) holds nothing but a weighted sum of input symbols.

∈
N LP (t)
N LP (t)

for i > 1.

i−1

∈

Experimental Setup
We evaluated the seq2grid module on symbolic problems
whose targets are given as sequences or single labels. To
this end, we built neural network models, such as S2G-CNN
and S2G-TextCNN, that followed the sequence-input grid-
output architecture by varying the grid decoder according to
target modalities of problems. Refer to each problem section
for our grid decoder choices and their training losses.

We compared our models with ﬁve baselines: Trans-
former (Vaswani et al. 2017), Universal Transformer (UT)
(Dehghani et al. 2018) with dynamic halting2, a LSTM
seq2seq model (LSTM) (Sutskever, Vinyals, and Le 2014), a
LSTM seq2seq attention model with a bidirectional encoder

2The UT can take different ponder time for each position.

Figure 4: The nested list G(t−1) grows to G(t) by the action
a(t) = (a(t)
N OP ). T LU (t) and N LP (t) show
outputs of top-list-update and new-list-push operations.

N LP , a(t)

T LU , a(t)

Figure 5: The grid decoder of S2G-CNN. Only the top list of
the grid from bottleneck blocks is passed to the logit layer.
The raw target 29 is ﬂipped and padded to 92

.
∅

(LSTM-Atten) (Bahdanau, Cho, and Bengio 2014) and a Re-
lational Memory Core seq2seq model (RMC) (Santoro et al.
2018). The Transformer and the UT consisted of two lay-
ers with the hidden size 128 and four attention heads. The
LSTM, the LSTM-Atten, and the RMC had three layers with
the hidden size 1024, 512, and 512 each.

We determined conﬁgurations of our models by hyperpa-
rameter sweeping for each problem. Our implementations3
based on the open source library tensor2tensor4 con-
tain detailed training schemes and hyperparameters of our
models and the baselines. All models could ﬁt in a single
NVIDIA GTX 1080ti GPU.

The next three sections follow the same organization to
illustrate the experiments and their results. First, we intro-
duce a set of symbolic reasoning tasks by describing the lay-
outs of the inputs and the targets. Next, we describe our grid
decoder architecture to solve such problems, which is com-
bined with the seq2grid module to follow the sequence-input
grid-output architecture. Finally, we analyze the experimen-
tal results and discuss their implications.

Arithmetic and Algorithmic Problems
Arithmetic and algorithmic problems are useful to test abili-
ties to extend rules on longer inputs since the input contains

3https://github.com/SegwangKim/neural-seq2grid-module
4https://github.com/tensorﬂow/tensor2tensor

7654321876543218765432187654321)*+,(.)×)1+2(.)×)132(.)×4(.)5(.67)89:(.);9<(.)5(.67)=5(.)+bottleneck blocks92∅Flipped Sequential TargetCross-entropylogit layerPreprocessed Grid Inputdigits. We test our models on three different arithmetic and
algorithmic inference problems. Each problem consists of a
training set and two test sets randomly sampled from dis-
tributions controlled by difﬁculty parameters. Two test sets
represent in-distribution data (ID) and OOD data (OOD).
Difﬁculty parameters of the training set can be overlapped
with those of the ID test set, but instances of the two sets
are strictly separated by their hash codes. The training set
of all problems contains 1M random examples and the two
test sets contain 10K examples each. We tokenize all inputs
and targets by characters and decimal digits. We score the
output by sequence-level accuracy, i.e., whether the output
entirely matches the target sequence. For convenience, we
denote

as $.

EOS
(cid:104)

(cid:105)

Number Sequence Prediction As the name suggests, the
goal of the number sequence prediction problem (Nam,
Kim, and Jung 2019) is to predict the next term of an in-
teger sequence. After randomly choosing three initial terms,
we generate a sequence via the recursion an = 2an−1
−
an−2 + an−3 which progresses the sequence up to the nth
term. The input is the ﬁrst n terms a0, . . . , an−1 and the
target is the last term an. The difﬁculty of the instance is
parameterized by the maximum number of digits of the ini-
tial terms a0, . . . , ak−1, i.e., length, and the total number of
input integer terms n, i.e., #terms. Those two difﬁculty pa-
rameters, length and #terms, vary (1-4, 4-6), (4, 4-6), and
(6, 10-12) for the training set, the ID test set, and the OOD
test set, respectively. The input and the target of a training
example are as follows.

Input: 7008 -205 4 7221$
Target: 14233$

Algebraic Word Problem To test the arithmetic abili-
ties under linguistic instructions, we choose algebraic word
problems, i.e., add-or-sub word, (Saxton et al. 2019). The
difﬁculty of the problems is controlled by entropy, the num-
ber of digits within a question. Here, we make two differ-
ences from the original dataset. First, we only allow inte-
gers whereas ﬂoating-point numbers can appear originally.
Second, our entropy is the total number of digits in the in-
put, whereas the original entropy is the maximum number of
digits that input can have. Our entropy varies 16-20, 16-20,
and 32-40 for the training set, the ID test set, and the OOD
test set, respectively. In the OOD test, we also impose every
integer to be of length above 16 to guarantee that it is longer
than any integers in the training set. The input and the target
of a training example are as follows.

Input: What is -784518 take away 7323?$
Target: -791841$

Computer Program Evaluation Predicting the execution
results of programs requires algorithmic reasoning such as
doing arithmetic operations or following programming in-
structions like variable assignments, branches, and loops.
We use mixed strategy (Zaremba and Sutskever 2014) to

Sequence

Add-or-sub

Program

ID

OOD

ID

OOD

ID

OOD

Baselines
LSTM
LSTM-Atten
RMC
Transformer
UT

Ours
S2G-CNN
S2G-ACNN

0.21
0.68
0.01
0.97
1.00

0.96
0.90

0.00
0.00
0.00
0.00
0.00

0.99
0.92

0.99
1.00
0.99
0.97
1.00

0.98
0.96

0.00
0.00
0.00
0.00
0.00

0.53
0.55

0.25
0.37
0.33
0.37
0.62

0.51
0.44

0.07
0.01
0.01
0.00
0.00

0.33
0.35

Table 1: Best sequence-level accuracy (out of 5 runs) on
number sequence prediction problems (sequence), algebraic
word problems (Add-or-sub), and computer program evalu-
ation problems (Program)

generate the training data with nesting 2 and length 5. For
the ID test set and the OOD test set, nesting and length are
set to be (2, 5) and (2, 7), respectively. The input, a ran-
dom Python snippet, and the target, the execution result, of
a training example are as follows.

Input: j=891

for x in range(11):j-=878
print((368 if 821<874 else j))$

Target: 368$

Grid Decoder
For solving arithmetic and algorithmic problems with digits,
it is desirable to choose a grid decoder that can do local and
parallel computation. Therefore, we implemented a CNN
(He et al. 2016) consisting of three stacks of 3-layer bot-
tleneck building blocks of ResNet. Also, we implemented
its attentional variant ACNN (Ramachandran et al. 2019);
3 convolution of the CNN was substituted with a
every 3
stand-alone self-attention convolution. We used 3
25-sized
grids from the seq2grid module having 3-layered GRU en-
coder of hidden size 128 for both decoders. As shown in Fig-
ure 5, we measured cross-entropy loss between the ﬂipped-
and-padded target and the output from the logit layer. Here,
the loss for empty symbol
was included as we read out
∅
logits backward in the inference stage. We jointly trained
the seq2grid module and the CNN (ACNN) by the ADAM
optimizer (Kingma and Ba 2014) with a learning rate 1e−3.

×

×

Results
Table 1 shows that our models, S2G-CNN and S2G-ACNN,
can generalize on OOD test sets. In particular, both grid de-
coders achieve similar OOD generalization, implying that
feeding the grid input via our seq2grid module can be ben-
eﬁcial to any decoder that can do local and parallel compu-
tations. On the other hand, all baselines catastrophically fail
at the OOD test sets although they seemingly perform well
on the ID test set. This shows that extending rules to longer
numbers via sequential processing is extremely difﬁcult.

As for the number sequence prediction problems, their
OOD test results serve as unit tests for the seq2grid module

print((11*7288719))
print(((6110039 if 7327755<3501784 else
1005398)*11))
b=6367476
for x in range(19):b-=9082877
print((3569363 if 7448172<9420320 else b))

e=(450693 if 4556818<2999168 else 3618338)
for x in range(10):e-=4489485
print(e)

Figure 7: Some OOD code snippets correctly answered by
the best run of the S2G-CNN. Note that snippets contain
FOR or * instruction requiring non-linear time complexity.

Figure 8: Input examples of the bAbI QA tasks.

high. For the non-linear operations, the S2G-ACNN shows
little understanding compared with the UT on the ID test set.
However, the UT fails to extend rules of FOR and * instruc-
tions on the OOD test set while the S2G-CNN does so on
some examples as shown in Figure 7. These are surprising
in that both the seq2grid module and the ACNN grid decoder
do linear time computations in the input length.

bAbI QA Tasks
Given as natural language with a small vocabulary of around
170, the bAbI QA tasks (Weston et al. 2015) test 20 types of
simple reasoning abilities such as counting, induction, de-
duction, and path-ﬁnding. A problem instance consists of
a story, a question, and the answer. Here, the story con-
tains supporting sentences about the answer and distrac-
tors that are irrelevant sentences to the answer. We formu-
late the bAbI QA tasks (Weston et al. 2015) in a sequence
classiﬁcation setup such that an input is a concatenation of
token, and a story as shown
CLS
SEP
(cid:105)
(cid:104)
(cid:104)
in Figure 8. While previous work (Dehghani et al. 2018)
uses sentence-level encodings, we use straightforward one-

token, a question,

(cid:105)

Figure 6: Visualizations of preprocessed grid inputs of (a)
number sequence prediction problems and (b) computer pro-
gram evaluation problems. The top and the bottom row cor-
respond to S2G-CNN and S2G-ACNN, respectively.

LSTM-Atten

UT

S2G-CNN

instruction

ID

OOD

I F-E L S E
F O R
*
I F-E L S E
F O R
*
I F-E L S E
F O R
*

0.46
0.06
0.07

0.81
0.38
0.52

0.73
0.20
0.25

0.26
0.03
0.04

0.01
0.00
0.00

0.57
0.09
0.14

Table 2: Accuracy by instruction types of the best runs on
the computer program evaluation problems. For example,
the S2G-CNN correctly answers 73% of all ID snippets con-
taining IF-ELSE instructions.

since it needs to align digit symbols on the grid according
to their scales. Indeed, Figure 6a shows that our module au-
tomatically ﬁnds such alignments that resemble the tailored
grid of digits as shown in Figure 1.

For the algebraic word problems, they require context-
dependent arithmetic unlike number sequence prediction
problems using the ﬁxed progression rules. In particular,
linguistic instructions like add or take away indicates
how to add/subtract given two numbers in a speciﬁc order.
Since our grid decoders apply the ﬁxed convolutional ﬁl-
ters over the grid, linguistic instructions must be reﬂected in
the grid input beforehand for doing context-dependent arith-
metic. This shows that our seq2grid module can infuse the
instruction information into the grid input.

For the computer program evaluation problems, predict-
ing the output of a code snippet demands an understand-
ing of algorithmic rules like branching mechanisms or for-
loop given as programming instructions IF-ELSE or FOR.
Also, computing * operations has non-linear time complex-
ity, unlike addition or subtraction. Hence, we further inves-
tigate accuracy on snippets by those instructions as shown
in Table 2. For the OOD snippets containing IF-ELSE in-
structions, our S2G-CNN achieves 57% accuracy for them.
Considering that they can contain other instructions besides
branching one as shown in Figure 7, the accuracy is fairly

instructionIDOODLSTM-AttenIF-ELSE0.460.26FOR0.060.03*0.070.04UTIF-ELSE0.810.01FOR0.380.00*0.520.00S2G-ACNNIF-ELSE0.800.69FOR0.140.12*0.280.15Table2:Accuracybyinstructiontypesofthebestrunsonthecomputerprogramevaluationproblems.Eachtestsetissplitintothreebytheinstructionsused(for,multiply,if-else).print((11*9223698))print((12*(6707143if2025491>9853525else6816666)))b=8582286forxinrange(20):b-=8256733print(b)d=(1017291if7117986>9036040else5725637)forxinrange(2):d-=6827279print(d)Figure7:SomeOODcodesnippetexamplescorrectlypre-dictedbythebestrunoftheS2G-ACNN.NotethatFORor*instructionrequiringnon-lineartimecomplexity.IDtestset.Thisshowsthatextendinglearnedrulestolongernumbersisextremelydifﬁcultviasequentialprocessing.Asforthenumbersequencepredictionproblems,theirOODtestresultsserveasunittestsfortheseq2gridmodulesinceitneedstoaligndigitsymbolsonthegridaccordingtotheirscales.Indeed,Figure6ashowsthatourmoduleau-tomaticallyﬁndssuchalignmentswhicharesimilartothemanuallydesignedgridofdigitsasshowninFigure1.Asforthealgebraicwordproblems,theyrequirecontext-dependentarithmeticunliketheﬁxedprogressionrulesinnumbersequencepredictionproblems.Inparticular,linguis-ticinstructionslikeaddortakeawayindicateshowtoadd/subtractgiventwonumbersinaspeciﬁcorder.Sinceourgriddecodersapplytheﬁxedconvolutionalﬁltersoverthegrid,linguisticinstructionsmustbereﬂectedinthegridin-putbeforehandforexecutingvariousarithmetic.Thisshowsthatourseq2gridmodulecaninfusetheinstructioninforma-tiontothegridinput.Asforthecomputerprogramevaluationproblems,pre-dictingtheoutputofacodesnippetdemandsanunderstand-ingofalgorithmicruleslikebranchingmechanismsorfor-loopgivenasprogramminginstructionsIF-ELSEorFOR.Also,computing*operationshasnon-lineartimecomplex-ity,unlikeadditionorsubtraction.Hence,wefurtherinves-tigateaccuracyonsnippetsbythoseinstructionsasshowninTable2.FortheOODsnippetscontainingIF-ELSEin-Task2.two-supporting-factshCLSiWhereistheapple?hSEPiMaryjourneyedtothegar-den.Sandragotthefootballthere.Marypickeduptheapplethere.Marydroppedtheapple.Task17.basic-deductionhCLSiWhatisgertrudeafraidof?hSEPiWolvesareafraidofsheep.Gertrudeisawolf.Winonaisawolf.Sheepareafraidofmice.Miceareafraidofcats.Catsareafraidofsheep.Emilyisacat.Jessicaisawolf.Task19.path-ﬁndinghCLSiHowdoyougofromthegardentotheofﬁce?hSEPiThekitcheniswestoftheofﬁce.Theofﬁceisnorthofthehallway.Thegardeniseastofthebathroom.Thegardenissouthofthehallway.Thebedroomiseastofthehallway.Figure8:InputexamplesofthebAbIQAtasks.structions,ourS2G-ACNNachievesalmost70%accuracy,implyingthatitdoesnotrandomlypickonebranchbetweentwopossiblebranches.Forthenon-linearoperations,theS2G-ACNNshowslittleunderstandingcomparedwiththeUTontheIDtestset.However,theUTfailstoextendrulesofFORand*instructionsontheOODtestsetwhiletheS2G-ACNNdoessoonsomeexamplesasshowninFig-ure7.Thesearesurprisinginthatboththeseq2gridmoduleandtheACNNgriddecoderdolineartimecomputationsintheinputlength.bAbIQATasksGivenasnaturallanguagewiththesmallnumberofvocab-ularyabout170,thebAbIQAtasks(Westonetal.2015)test20typesofsimplereasoningabilitiessuchascounting,induction,deduction,andpath-ﬁnding.Aprobleminstanceconsistsofastory,aquestion,andtheanswer.Here,thestorycontainssupportingsentencesabouttheansweranddistractorswhichareirrelevantsentencestotheanswer.WeformulatethebAbIQAtasks(Westonetal.2015)inse-quenceclassiﬁcationsetupsuchthataninputisaconcate-nationofhCLSitoken,aquestion,hSEPitoken,andastoryasshowninFigure8.Whilepreviouswork(Dehghanietal.2018)usessentence-wiseencodings,weusestraightforwardone-hotwordencodings,yieldingtheincreaseoftheaver-agelengthofinputsequencesfrom13.6to78.9.SolvingthebAbItasksunderword-levelencodingsinsteadofsentence-levelmakesitmuchhardersinceitrequirestohandlemuchlongerdependencies.State-of-the-artmodelsdealwiththisissueviaaugmentingneuralnetworkswithexternalmem-ory(Munkhdalaietal.2019;Raeetal.2016).However,weshowthattheseq2gridmodulecanenhanceasimpleneuralnetworklikeTextCNNtoeffectivelysolvetheword-levelbAbItasks,evenintheabsenceofcomplexandexpensiveexternalmemorystructures.Task

#supps LSTM UT

S2G-
TextCNN

Baselines

Ours

1.0
1: single-supporting-fact
2.0
2: two-supporting-facts
3.0
3: three-supporting-facts
1.0
4: two-arg-relations
1.0
5: three-arg-relations
1.0
6: yes-no-questions
2.3
7: counting
1.9
8: lists-sets
1.0
9: simple-negation
1.0
10: indeﬁnite-knowledge
2.0
11: basic-coreference
12: conjunction
1.0
13: compound-coreference 2.0
2.0
14: time-reasoning
2.0
15: basic-deduction
3.0
16: basic-induction
2.0
17: positional-reasoning
2.0
18: size-reasoning
2.0
19: path-ﬁnding
1.0
20: agents-motivations

Mean error (%)
#Failed tasks

0.0
47.4
45.9
0.1
0.8
0.5
1.8
0.2
0.0
0.3
0.0
0.0
0.0
20.6
34.8
52.1
41.1
8.6
90.9
1.8

17.3
8

0.0
55.0
67.9
0.0
5.5
0.1
4.0
2.3
0.0
0.0
0.1
0.0
0.0
4.4
18.5
53.6
41.0
9.1
79.1
1.4

17.1
8

0.0
31.2
31.5
0.0
1.0
0.0
0.0
1.8
0.0
0.0
0.0
0.0
0.0
7.3
0.0
51.7
31.4
3.8
35.1
0.0

9.7
6

Table 4: Task-wise errors on the bAbI QA 10k joint tasks for
the best runs. #supps is the average number of supporting
sentences in the story.

inputs while selecting only necessary words along story arcs.
Moreover, the compression is effective in terms of the num-
ber of parameters. Indeed, the GRU encoder inside our mod-
ule is much smaller than the LSTM but enough to provide
grid inputs to our grid decoder for solving the bAbI tasks.

We highlight that our seq2grid module, not the TextCNN
decoder, leads to the superior performance of our model.
Since the attempt to use the usual TextCNN alone fails at
almost all tasks, the dramatic performance gain by the aid of
the seq2grid module is somewhat surprising.

We further analyze errors by tasks to see the possibil-
ity and the limitation of our sequence-to-grid method. The
zero variance in the number of failed tasks (Table 3) indi-
cates that the S2G-TextCNN consistently fails on the same
set of tasks, as listed in Table 4. Those failed tasks including
two-supporting-facts, positional reasoning, and path-ﬁnding
seem reasonably difﬁcult for our models in that all of them
require more than one supporting sentence for the reasoning.

Conclusion
We introduced a neural sequence-to-grid (seq2grid) module
which automatically segments and aligns an sequential input
into a grid. Our module was used as an input preprocessor
for a neural network that took a grid input. In particular, our
module executed our novel nested list operations, ensuring
an end-to-end joint training with the neural network. Em-
pirically, our module enhanced neural networks in various
symbolic reasoning tasks.

Figure 9: The grid decoder of S2G-TextCNN.

#params Error

#Failed tasks

Baselines5
LSTM
Transformer
UT
TextCNN

25.6M
0.5M
0.5M
0.2M

Ours
S2G-TextCNN 0.8M

24.9
33.1
26.8
37.8

10.8

±
±
±
±

±

5.8 12.1
1.7 18.9
6.0 15.0
0.4 19.0

3.7
0.3
4.0
0.0

±
±
±
±

0.8 6.0

0.0

±

Table 3: Error and #Failed tasks (> 5% error) on the bAbI
QA 10k joint tasks (for 10 runs).

hot word-level encodings. This setup yields the increase of
the average input length from 13.6 to 78.9, which in turn re-
quires to handle much longer dependencies. Hence, solving
the bAbI tasks under word-level encodings is much harder
than those under sentence-level encodings. State-of-the-art
models deal with longer dependencies via augmenting neu-
ral networks with external memory (Munkhdalai et al. 2019;
Rae et al. 2016). However, we will show that the seq2grid
module can enhance a simple neural network like TextCNN
to effectively solve the word-level bAbI tasks, even in the
absence of a complex and expensive memory structure.

Grid Decoder
We chose a grid decoder as a variant of TextCNN (Kim
2014). After the seq2grid module having 2-layered GRU en-
8-sized grid input,
coders of hidden size 128 gave the 4
our TextCNN predicted the label by applying k
k-CNNs
(k = 2, 3, 4), max-pooling, and dropout with the rate 0.4
as shown in the Figure 9. We used the ADAM optimizer to
jointly train the seq2grid module and the TextCNN under a
warm-up and decay learning rate scheme3.

×

×

Results
Our S2G-TextCNN outperforms sequential baseline models,
such as the LSTM, the Transformer encoder, and the UT en-
coder, as shown in Table 3. Note that we fed word-level in-
puts that require doing reasoning over distant symbols, i.e.,
the average length of inputs is 78.9, and we used the grid
that has only 32 (= 4
8) slots. From these setups, we can
conclude that our module can compress long inputs into grid

×

5We use only encoders of baselines used in arithmetic and algo-
rithmic problems. As for the logit, LSTM uses the last hidden state
while others use the hidden one corresponding to

token.

CLS
(cid:105)

(cid:104)

∙∙∙∙∙∙∙∙∙∙∙∙Grid Input2D TextCNNwestClass Label TargetCross-entropyPooling3×3Logit Layer2×2∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙4×4Acknowledgements
The authors appreciate Hyunkyung Bae for assistance with
experiments. K. Jung is with ASRI and ECE, Seoul National
University, Korea. This work was supported by Samsung
Research Funding & Incubation Center of Samsung Elec-
tronics under Project Number SRFCIT1902-06.

References
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural ma-
chine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473 .
Cai, J.; Shin, R.; and Song, D. 2017. Making neural pro-
arXiv
gramming architectures generalize via recursion.
preprint arXiv:1704.06611 .
Chen, X.; Liang, C.; Yu, A. W.; Zhou, D.; Song, D.; and
Le, Q. V. 2019. Neural symbolic reader: Scalable integra-
tion of distributed and symbolic representations for reading
In International Conference on Learning
comprehension.
Representations.
Chen, X.; Liu, C.; and Song, D. 2017. Towards synthesiz-
ing complex programs from input-output examples. arXiv
preprint arXiv:1706.01284 .
Dehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and
Kaiser, Ł. 2018. Universal transformers. arXiv preprint
arXiv:1807.03819 .
Graves, A.; Wayne, G.; and Danihelka, I. 2014. Neural tur-
ing machines. arXiv preprint arXiv:1410.5401 .
Graves, A.; Wayne, G.; Reynolds, M.; Harley, T.; Danihelka,
I.; Grabska-Barwi´nska, A.; Colmenarejo, S. G.; Grefen-
stette, E.; Ramalho, T.; Agapiou, J.; et al. 2016. Hybrid
computing using a neural network with dynamic external
memory. Nature 538(7626): 471–476.
Grefenstette, E.; Hermann, K. M.; Suleyman, M.; and Blun-
som, P. 2015. Learning to transduce with unbounded mem-
ory. In Advances in neural information processing systems,
1828–1836.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
In Proceedings of the
ual learning for image recognition.
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
Joulin, A.; and Mikolov, T. 2015. Inferring algorithmic pat-
terns with stack-augmented recurrent nets. In Advances in
neural information processing systems, 190–198.
Kaiser, Ł.; and Sutskever, I. 2015. Neural gpus learn algo-
rithms. arXiv preprint arXiv:1511.08228 .
Kalchbrenner, N.; Danihelka, I.; and Graves, A. 2015. Grid
long short-term memory. arXiv preprint arXiv:1507.01526 .
Keysers, D.; Sch¨arli, N.; Scales, N.; Buisman, H.; Furrer, D.;
Kashubin, S.; Momchev, N.; Sinopalnikov, D.; Staﬁniak, L.;
Tihon, T.; et al. 2019. Measuring Compositional General-
ization: A Comprehensive Method on Realistic Data. arXiv
preprint arXiv:1912.09713 .
Kim, Y. 2014. Convolutional neural networks for sentence
classiﬁcation. arXiv preprint arXiv:1408.5882 .

Kingma, D. P.; and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 .
Lake, B. M.; and Baroni, M. 2017.
Generalization
without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. arXiv preprint
arXiv:1711.00350 .
Lample, G.; and Charton, F. 2019. Deep learning for sym-
bolic mathematics. arXiv preprint arXiv:1912.01412 .
Li, J.; Wang, L.; Zhang, J.; Wang, Y.; Dai, B. T.; and Zhang,
D. 2019. Modeling Intra-Relation in Math Word Problems
In Pro-
with Different Functional Multi-Head Attentions.
ceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 6162–6167.
Marcus, G. 2003. The Algebraic Mind: Integrating Connec-
tionism and Cognitive Science. MIT Press.
Munkhdalai, T.; Sordoni, A.; Wang, T.; and Trischler, A.
2019. Metalearned neural memory. In Advances in Neural
Information Processing Systems, 13331–13342.
Nam, H.; Kim, S.; and Jung, K. 2019. Number Sequence
Prediction Problems for Evaluating Computational Powers
of Neural Networks. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 33, 4626–4633.
Rae, J.; Hunt, J. J.; Danihelka, I.; Harley, T.; Senior, A. W.;
Wayne, G.; Graves, A.; and Lillicrap, T. 2016. Scaling
memory-augmented neural networks with sparse reads and
writes. In Advances in Neural Information Processing Sys-
tems, 3621–3629.
Ramachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Lev-
skaya, A.; and Shlens, J. 2019. Stand-alone self-attention in
vision models. arXiv preprint arXiv:1906.05909 .
Reed, S.; and De Freitas, N. 2015. Neural programmer-
interpreters. arXiv preprint arXiv:1511.06279 .
Russin, J.; Jo, J.; O’Reilly, R.; and Bengio, Y. 2020. Compo-
sitional Generalization by Factorizing Alignment and Trans-
lation. In Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics: Student Research
Workshop, 313–327.
Santoro, A.; Faulkner, R.; Raposo, D.; Rae, J.; Chrzanowski,
M.; Weber, T.; Wierstra, D.; Vinyals, O.; Pascanu, R.; and
Lillicrap, T. 2018. Relational recurrent neural networks. In
Advances in Neural Information Processing Systems, 7299–
7310.
Saxton, D.; Grefenstette, E.; Hill, F.; and Kohli, P. 2019.
Analysing mathematical reasoning abilities of neural mod-
els. arXiv preprint arXiv:1904.01557 .
Sukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-
end memory networks. In Advances in neural information
processing systems, 2440–2448.
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence
to sequence learning with neural networks. In Advances in
neural information processing systems, 3104–3112.
Trask, A.; Hill, F.; Reed, S. E.; Rae, J.; Dyer, C.; and Blun-
som, P. 2018. Neural arithmetic logic units. In Advances in
Neural Information Processing Systems, 8035–8044.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. In Advances in neural information
processing systems, 5998–6008.
Wang, Y.; Liu, X.; and Shi, S. 2017. Deep neural solver for
math word problems. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language Process-
ing, 845–854.
Wangperawong, A. 2018. Attending to Mathematical Lan-
guage with Transformers. arXiv preprint arXiv:1812.02825
.

Weston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; van
Merri¨enboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards
ai-complete question answering: A set of prerequisite toy
tasks. arXiv preprint arXiv:1502.05698 .
Zaremba, W.; and Sutskever, I. 2014. Learning to execute.
arXiv preprint arXiv:1410.4615 .

