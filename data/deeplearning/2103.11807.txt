1
2
0
2

n
u
J

1

]

G
L
.
s
c
[

2
v
7
0
8
1
1
.
3
0
1
2
:
v
i
X
r
a

Data Cleansing for Deep Neural Networks
with Storage-eï¬ƒcient Approximation of Inï¬‚uence Functions

Kenji Suzukiâˆ—, Yoshiyuki Kobayashi, Takuya Narihira
Sony Group Corporation, Japan

Abstract

Identifying the inï¬‚uence of training data for data cleansing can improve the accuracy of deep
learning. An approach with stochastic gradient descent (SGD) called SGD-inï¬‚uence to calculate
It is necessary to
the inï¬‚uence scores was proposed, but, the calculation costs are expensive.
temporally store the parameters of the model during training phase for inference phase to calculate
inï¬‚uence sores. In close connection with the previous method, we propose a method to reduce
cache ï¬les to store the parameters in training phase for calculating inference score. We only
adopt the ï¬nal parameters in last epoch for inï¬‚uence functions calculation. In our experiments on
classiï¬cation, the cache size of training using MNIST dataset with our approach is 1.236 MB. On
the other hand, the previous method used cache size of 1.932 GB in last epoch. It means that cache
size has been reduced to 1/1,563. We also observed the accuracy improvement by data cleansing
with removal of negatively inï¬‚uential data using our approach as well as the previous method.
Moreover, our simple and general proposed method to calculate inï¬‚uence scores is available on our
auto ML tool without programing, Neural Network Console. The source code is also available.

1 Introduction

There is a growing interest in the fairness, accountability, and transparency of machine learning with
the outbreak of AI ethical issues. The machine learning is called black box systems because of very
complex networks with large amount of neurons. Understanding the behavior of neural networks
remains a signiï¬cant challenge. Explainable AI can be used not only for AI ethics applications, but
also for deep neural network debugging applications, and has the potential to extract the potential
of deep neural networks. Developers can analyze interpretations to debug models. Researchers have
developed some methods to open the black box, such as LIME [1] and Grad-CAM [2] to visualize
the basis of judgement in classiï¬cation. These methods help humans to understand the reason of
judgement.

1.1 Motivation

Although these methods can indicate the basis of judgement with heat map or super pixels, we would
like to improve accuracy without adding new data as next step. It is very crucial to suggest what to
do in explainable AI. Recently some methods for data cleansing calculating inï¬‚uence of data to deep
neural networks have been proposed [3] [4].

The inï¬‚uence scores can specify negative inï¬‚uential score to deep neural networks. These inï¬‚uence
methods help developers to clean a large amount of data for high-quality data without domain knowl-
edge for labeling. Especially, SGD-inï¬‚uence [4] using stochastic gradient descent (SGD) proposed by
Hara et al. does not require the loss function to be convex and an optimal model to be obtained.
However, the SGD-inï¬‚uence needs a large mount of cache ï¬les to store parameters in training phase.

âˆ—Kenji.B.Suzuki@sony.com

1

 
 
 
 
 
 
We are motivated to calculate inï¬‚uence with storage-eï¬ƒcient approximation, and to implement it on
Neural Network Console 1 that is our auto ML tool. Therefore, we study how to reduce the size of
cache with equivalent accuracy improvement.

2 Our Contributions

We propose a method of SGD-inï¬‚uence with cache reduction. It enables us to implement the algorithm
on Neural Network Console, our auto ML tool, that we developed. In this paper, we propose a novel
cost-eï¬€ective data inï¬‚uence algorithm for data cleansing in deep learning. Our key contributions are
summarized as follow:

1. We propose cache reduction algorithm for SGD-inï¬‚uence. The algorithm is based on SGD-
inï¬‚uence [4] proposed by Hara et al. The original work needs a large amount of HDD cache
between training phase and inference phase. We adopt only the last parameters in last epoch for
inï¬‚uence functions calculation.

2. We demonstrate that our proposed method can eï¬€ectively improve accuracy by removing the

negative inï¬‚uential data.

3. As a result, it enables us to calculate inï¬‚uence scores without large HDD cache. Therefore, we
have implemented this proposed algorithm on our auto ML tool called Neural Network Con-
sole which is available. Using this software, the inï¬‚uence scores are easily obtained without
programming.

3 Related Work

In this section, we provide an overview of existing inï¬‚uence methods. Classic technique from statis-
tics [5] has shown estimated inï¬‚uence analytically calculated. The inï¬‚uence functions for machine
learning models have not seen widespread use in spite of study in statistics. Koh and Liang [3] pro-
vided tractable approximations of inï¬‚uence functions which characterize the inï¬‚uence of each data in
terms of change loss. However, the method requires the loss function to be convex and an optimal
model, which cannot always be satisï¬ed in deep learning. Hara et al. [4] provided the inï¬‚uential
instances by retracing the steps of the SGD without these limitations.

3.1 Baseline Method

Our approach is based on SGD-inï¬‚uence [4] for deep neural networks prediction as a baseline. The
SGD-inï¬‚uence is based on counterfactual SGD where one instance is absent. The t-step of the counter-
factual SGD with j-th instance zj absence is denoted as Î¸[t+1]
âˆ’j) where
St denotes the set of instance indices used in the t-th step, Î·t is learning rate, Î¸ is the parameters of
the model, z is an instance. Let g(z; Î¸) := âˆ‡Î¸l(z; Î¸), where l is loss function. SGD-inï¬‚uence refers to
the parameter diï¬€erence Î¸[t]
âˆ’j âˆ’ Î¸[t] as the instance zi âˆˆ D at step t. Linear inï¬‚uence estimation (LIE)
is estimated liner inï¬‚uence L[T ]
âˆ’j âˆ’ Î¸[T ](cid:105) for a given query vector u[T âˆ’1] âˆˆ Rp and the
SGD-inï¬‚uence after T SGD steps.

âˆ’j (u[t]) := (cid:104)u[t], Î¸[T ]

iâˆˆSt\{j} g(zi; Î¸[t]

âˆ’j â† Î¸[t]

âˆ’j âˆ’ Î·t
|St|

(cid:80)

The SGD-inï¬‚uence consists of training phase and inference phase. The tuple of instance indices
|St|, learning rate Î·t, and parameters Î¸[t] are stored in training phase. The stored information is
retraced and u[t] in each step is computed. H [t] is the Hessian of the loss on the mini-batch St.

They evaluated both trace back all training epochs and approximate version of the trace back
only last one epoch which is storage friendly, which counterfactual SGD is used in last epoch. They

1https://dl.sony.com/

2

Figure 1: Workï¬‚ow of data cleansing. To compute inï¬‚uence scores, there are training phase, cache,
and inference phase. The cache stores parameters from the training phase. To improve accuracy, there
are stages of deleting negative data and retaining.

observed that the approximate version is also eï¬€ective for data cleansing. As a baseline method, we
study comparison of the approximate version with our approach.

4 Proposed Method

Figure 1 shows workï¬‚ow of data cleansing. To compute inï¬‚uence scores, there are training phase, cache,
and inference phase. To improve accuracy, there are stages of deleting negative data and retaining.

In order to reduce the amount of the parameters from the training phase, we propose to store only
ï¬nal parameters in last epoch from training phase. It means that cache size has been reduced to 1/T ,
where T is SGD steps. The previous study of SGD-inï¬‚uence as a baseline stores each SGD step T in
training phase. Here we assume that the ï¬nal parameters make important rules for models. Therefore,
we store the ï¬nal parameters in last epoch and load the ï¬nal parameters to calculate inï¬‚uence as
shown in Algorithm 1 and 2.

This may not be exact computation, but we regard the method as an approximation because there
are little diï¬€erence among parameters of each SGD step in last epoch. For example, when the number
of dataset is 50,000 and the batch size |St| is 32, there are T = 1, 563 SGD steps in the last epoch.
We use only ï¬nal step instead of the all parameters in each step. Therefore, we can reduce the cache
size to 1/1563 (1/T ). Since the number of data N is T Ã— |St|, cache reduction is 1/T (cid:58) |St|/N . This

3

relationship indicates that it is useful for calculating large amounts of data N in large-scale calculations.
This proposal is very simple and general. In practical use of deep learning, computation cost is an

important factor. This proposed approximation is very useful for developers.

5 Evaluations

We employ Neural Network Libraries [6]which is our developed deep learning framework under CUDA
Tool kit 10.2 and cuDNN 8.0. Neural Network Libraries are available as an OSS 2. The source code
using the Neural Network Libraries in our experiment is available at Sony AI research code repository 3.
In order to evaluate the cache reduction and data cleansing performance, we conducted image clas-
siï¬cation. We used two datasets, MNIST [7] and CIFAR10 [8], to evaluate that models of deep neural
networks are eï¬€ectively improved by removing negative inï¬‚uence data. The deep neural networks
consist of 6 conventional convolution layers.

5.1 Cache Reduction

The table 1 shows cache size of the all parameters, the previous method of Hara et al., and our
proposed method for dataset of MNIST and CIFAR10. We use 50,000 and 40,000 training data for
MNIST and CIFAR10, respectively. All parameters are accumulated through epoch k = 20. In the
previous method [4] that use the parameters in the last epoch, it is necessary to use 1.932 GB and 1.545
GB in HDD cache for MNIST and CIFAR10, respectively. Our method requires cache of 1.236 MB
for MNIST and CIFAR10. We set batch size |St| = 32, so that MNIST and CIFAR10 have T = 1, 563
steps and T = 1, 250 steps, respectively. The cache reduction is 1/T , where T is the number of SGD
steps. This result indicates the cache size reductions are 1/1,563 and 1/1,250 for MNIST and CIFAR10,
respectively.

Table 1 : Cache size of the parameters in training

Methods

Cache

Cache size (GB)
MNIST CIFAR10

All parameters
Hara et al.
Ours

Î¸ Ã— T Ã— k
Î¸ Ã— T
Î¸

38.64
1.932
0.001236

30.90
1.545
0.001236

5.2 Data Cleansing Performance

In order to check the performance of data cleansing, we adopted the original SGD-inï¬‚uence (denoted
Hara et al.) and random data removal as baselines. We set leaning rate lr = 0.05, the the number
of epoch k = 20, and batch size |St| = 32. We randomly selected 10,000 data for validation data and
used the remaining data as training dataset from the original dataset. We repeated the experiments 10
times changing diï¬€erent random seeds and the split between training and validation dataset. Another
10,000 data are set for testing accuracy.

Figure 2 shows the accuracy on the test dataset after data cleansing for (a) MNIST and (b)
CIFAR10. The data cleansing involves ï¬rst determining the inï¬‚uence of each data and then removing
the data that would negatively aï¬€ect it. In other words, Figure 2 shows how the accuracy changes
as we remove the top-n negative data. Increasing accuracy accompanied by removing top-n negative
data is an evidence of data cleansing. We observed our method has the signiï¬cant performance of data

2https://nnabla.org
3https://github.com/sony/ai-research-code

4

Figure 2: The accuracies on the test dataset after data cleansing deleted data of negative inï¬‚uence
scores for (a) MNIST and (b) CIFAR10. Comparison of Hara et al., our proposed method, randomly
removed, and no removal.

cleansing for both MNIST and CIFAR10 datasets as well as Hara et al.. On the other hand, we do not
observe the signiï¬cant accuracy improvement by randomly removed data. In the MNIST experiment
with batch size |St| = 32, the accuracy peaked at 5,000 data removals. Since the the number of original
training data is 50,000, it shows that the peak of accuracy is 10% of the data, 5,000, removed from the
training data, 50,000. Then the standard deviation Ïƒ is 0.000837. Since we repeated the experiments
10 times changing diï¬€erent random seeds, the accuracies in Fig. 2 are the mean value. The batch size
dependence experiments from |St| = 32 to 256 show data cleansing behavior at any batch sizes 4 .

6 Applications

We have implemented this proposed algorithm on Neural Network Console which is available for users.
The software runs on Windows 10 or cloud service with GPU, e.g. NVIDIA V100.
It is our auto
ML tool by GUI, which users can easily use. Users can compute deep learning without programming.
Here we provide some plugins of explainable AI on Neural Network Console. The left part in Fig. 3
shows screen shot of a part of ResNet-110 on Neural Network Console. This graphical networks help

4See Appendix for the full results.

Figure 3: The left part shows screen shot of a part of ResNet-110 on Neural Network Console. The
right part shows calculated inï¬‚uence score for each training data of CIFAR10 with ResNet-110. Our
proposed method is available on our auto ML tool, Neural Network Console.

5

user visually understand. The screen shot at right part in Fig. 3 shows calculated inï¬‚uence score for
each training data of CIFAR10 with ResNet-110.
In addition, this Neural Network Console has a
variety of explainable AI algorithms as plugins as well as this SGD-inï¬‚uence, which are LIME [1] and
Grad-CAM [2] to visualize the judgement basis in image classiï¬cation as well. User can easily examine
these out-of-state explainable AI with GUI on Windows 10 or cloud service.

7 Conclusion

Our simple and general method presented here, cache reduction of SGD-inï¬‚uence, is a step toward
cleansing data in deep neural networks. The signiï¬cant cache reduction helps developers conduct data
cleansing with less computation storage. This result enables us to implement of inï¬‚uence functions
calculation by SGD-inï¬‚uence on our auto ML tool, our Neural Network Console. We proved that our
method is an eï¬€ective approximate way to improve accuracy by removing negative data as well as the
previously proposed SGD-inï¬‚uence.

Acknowledgements

We would like to thank Associate Professor Satoshi Hara of Osaka University for fruitful advice and
discussion. We would like to thank Masato Ishii of Sony Corporation of Japan for helpful discussion,
and to thank Andrew Shin of Sony Corporation of Japan for reviewing this paper, and to thank Yukio
Oobuchi of Sony Corporation of Japan for technical support.

References

[1] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. â€why should I trust you?â€: Explaining
the predictions of any classiï¬er. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages
1135â€“1144, 2016.

[2] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pages 618â€“626,
2017.

[3] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inï¬‚uence functions. In
Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 1885â€“
1894, 2017.

[4] Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data cleansing for models trained with

sgd. In Advances in Neural Information Processing Systems, pages 4215â€“4224, 2019.

[5] R Dennis Cook and Sanford Weisberg. Characterizations of an empirical inï¬‚uence function for
detecting inï¬‚uential cases in regression. In Technometrics, volume 22, pages 495â€”-508, 1980.

[6] Takuya Narihira, Javier Alonsogarcia, Fabien Cardinaux, Akio Hayakawa, Masato Ishii, Kazunori
Iwaki, Thomas Kemp, Yoshiyuki Kobayashi, Lukas Mauch, Akira Nakamura, Yukio Obuchi,
Andrew Shin, Kenji Suzuki, Stephen Tiedmann, Stefan Uhlich, Takuya Yashima, and Kazuki
Yoshiyama. Neural network libraries: A deep learning framework designed from engineersâ€™ per-
spectives, 2021.

[7] Yann LeCun, LÂ´eon Bottou, Yoshua Bengio, and Patrick Haï¬€ner. Gradient-based learning applied
to document recognition. In Proceedings of the IEEE, volume 86(11), pages 2278â€“2324, 1998.

6

[8] Alex Krizhevsky and Geoï¬€rey Hinton. Learning multiple layers of features from tiny images. In

Technical report, Citeseer, 2009.

7

A Full Results

In this section, we provide additional experimental results to understand the eï¬€ect of data cleansing.

A.1 Standard Deviation

Since we repeated the experiments 10 times changing diï¬€erent random seeds, the accuracies with
standard deviation Ïƒ in Fig. 4. In the both MNIST and CIFAR10 experiments with batch size |St| =
32, we observed that the accuracies have increased with error bars, which mean average Â± standard
deviation Ïƒ. Compared with previous method of Hara et al., we note that our proposed method
performed similar behavior. The observation suggests that our proposed method with storage-eï¬ƒcient
approximation of inï¬‚uence functions should be useful.

Figure 4: The accuracies on the test dataset after data cleansing deleted data of negative inï¬‚uence
scores for MNIST and CIFAR10. Comparison of our proposed method and Hara et al.. The average
accuracies on the test set after data cleansing with 10 experiments. The error bars mean average Â±
standard deviation Ïƒ.

A.2 Batch Size Dependence

Figure 5 shows the full results of batch size dependence of the accuracies on the test dataset after
data cleansing that deletes data of negative inï¬‚uence scores. It is clear evident that accuracies have
increased after data cleansing with our proposed method for various batch sizes in MNIST and CI-
FAR10 experiments. These results of batch size dependence experiments from |St| = 32 to 256 conï¬rm
that our proposed method can eï¬€ectively suggest inference functions calculation for data cleansing.

8

Figure 5: Batch size dependence of the accuracies on the test dataset after data cleansing that deletes
data of negative inï¬‚uence scores for MNIST from (a) to (d) and CIFAR10 from (e) to (h). Comparison
of Hara et al., our proposed method, randomly removed, and no removal.

9

(e) CIFAR10 |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 32(f) CIFAR10 |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 64(g) CIFAR10 |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 128(h) CIFAR10 |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 256et al.et al.et al.et al.(b) MNIST |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 64(c) MNIST |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 128(d) MNIST |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 256et al.et al.et al.(a) MNIST : |ğ‘†ğ‘†ğ‘¡ğ‘¡|= 32et al.