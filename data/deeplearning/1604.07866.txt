Learning by tracking: Siamese CNN for robust target association

Laura Leal-Taix´e
TU M¨unchen
Munich, Germany

Cristian Canton Ferrer
Microsoft
Redmond (WA), USA

Konrad Schindler
ETH Zurich
Zurich, Switzerland

6
1
0
2

g
u
A
4

]

G
L
.
s
c
[

3
v
6
6
8
7
0
.
4
0
6
1
:
v
i
X
r
a

Abstract

This paper introduces a novel approach to the task of
data association within the context of pedestrian tracking,
by introducing a two-stage learning scheme to match pairs
of detections. First, a Siamese convolutional neural net-
work (CNN) is trained to learn descriptors encoding lo-
cal spatio-temporal structures between the two input image
patches, aggregating pixel values and optical ﬂow informa-
tion. Second, a set of contextual features derived from the
position and size of the compared input patches are com-
bined with the CNN output by means of a gradient boosting
classiﬁer to generate the ﬁnal matching probability. This
learning approach is validated by using a linear program-
ming based multi-person tracker showing that even a sim-
ple and efﬁcient tracker may outperform much more com-
plex models when fed with our learned matching probabili-
ties. Results on publicly available sequences show that our
method meets state-of-the-art standards in multiple people
tracking.

1. Introduction

One of the big challenges of computer vision is scene un-
derstanding from video. Humans are often the center of at-
tention of a scene, which leads to the fundamental problem
of detecting and tracking them in a video. To track multiple
people, tracking-by-detection has emerged as the preferred
method. That approach simpliﬁes the problem by dividing
it into two steps. First, ﬁnd probable pedestrian locations
independently in each frame. Second, link corresponding
detections across time to form trajectories.

The linking step, called data association is a difﬁcult
task on its own, due to missing and spurious detections, oc-
clusions, and targets interactions in crowded environments.
To address these issues, research in this area has produced
more and more complex models: global optimization meth-
ods based on network ﬂow [4, 64], minimum cliques [61]
or discrete-continuous CRF inference [1]; models of pedes-
trian interaction with social motion models [35, 44]; inte-
gration of additional motion cues such as dense point tra-
jectories [9, 23]; and person re-identiﬁcation techniques to

Figure 1: Multiple object tracking with learned detection
associations.

improve appearance models [30, 32]. Even though the mod-
els became progressively more sophisticated, the underly-
ing descriptors, which are used to decide whether two detec-
tions belong to the same trajectory, remained quite simple
and struggle in challenging scenarios (e.g., crowds, frequent
occlusions, strong illumination effects).

Recently, larger amounts of annotated data have become
available and, with the help of these data, convolutional
neural networks (CNNs) that learn feature representations
as part of their training have outperformed heuristic, hand-
engineered features in several vision problems [31]. Here,
we adapt the CNN philosophy to multi-person tracking. In
order to circumvent manual feature design for data associ-
ation, we propose to learn the decision whether two detec-
tions belong to the same trajectory. Our learning frame-
work has two stages: ﬁrst, a CNN in Siamese twin architec-
ture is trained to assess the similarity of two equally sized

1

Linear ProgrammingSiamese CNN+Gradient BoostingDetections at frame NDetections at frame MPredictedassociationsFinal trajectories 
 
 
 
 
 
image regions; second, contextual features that capture the
relative geometry and position of the two patches of inter-
est are combined with the CNN output to produce a ﬁnal
prediction, in our case using gradient boosting (GB). Given
the learned, pairwise data association score we construct a
graph that links all available detections across frames, and
solve the standard Linear Programming (LP) formulation
of multi-target tracking. We show that this simple and efﬁ-
cient linear tracker – in some sense the “canonical baseline”
of modern multi-target tracking – outperforms much more
complex models when fed with our learned edge costs.

1.1. Contributions

This paper presents three major contributions to the

pedestrian tracking task:

• Within the context of tracking, we introduce a novel
learning perspective to the data association problem.

• We propose to use a CNN in a Siamese conﬁguration
to estimate the likelihood that two pedestrian detec-
tions belong to the same tracked entity.
In the pre-
sented CNN architecture, pixel values and optical ﬂow
are combined as a multi-modal input.

• We show that formulating data association with a lin-
ear optimization model outperform complex models
when fed with accurate edge costs.

1.2. Related work

Multi-person tracking. Multi-person tracking is the in-
put for a number of computer vision applications, such as
surveillance, activity recognition or autonomous driving.
Despite the vast literature on the topic [39], it still remains
a challenging problem, especially in crowded environments
where occlusions and false detections are common. Most
modern methods use the tracking-by-detection paradigm,
which divides the task into two steps: detecting pedestri-
ans in the scene [17, 20, 25], and linking those detections
over time to create trajectories. A common formalism is
to represent the problem as a graph, where each detection
is a node, and edges indicate a possible link. The data as-
sociation can then be formulated as maximum ﬂow [4] or,
equivalently, minimum cost problem [28, 35, 45, 64], both
efﬁciently solved to (near-)global optimality with LP, with a
superior performance compared to frame-by-frame [29] or
track-by-track [3] methods. Alternative formulations typi-
cally lead to more involved optimization problems, includ-
ing minimum cliques [61] or general-purpose solvers like
MCMC [59]. There are also models that represent trajec-
tories in continuous space and use gradient-based optimiza-
tion, sometimes alternating with discrete inference for data
association [1].

A recent trend is to design ever more complex mod-
els, which include further vision routines in the hope that

they beneﬁt the tracker, including reconstruction for multi-
camera sequences [36, 54], activity recognition [11] and
segmentation [40]. In general, the added complexity seems
to exhibit diminishing returns, at signiﬁcantly higher com-
putational cost.

Other works have focused on designing more robust fea-
tures to discriminate pedestrians. Color-based appearance
models are common [30], but not always reliable, since peo-
ple can wear very similar clothes, and color statistics are
often contaminated by the background pixels and illumi-
nation changes. Kuo et al. [32], borrow ideas from per-
son re-identiﬁcation and adapt them to “re-identify” targets
during tracking. In [57], a CRF model is learned to better
distinguish pedestrians with similar appearance. A different
line of attack is to develop sophisticated motion models in
order to better predict a tracked person’s location, most no-
tably models that include interactions between nearby peo-
ple [1, 10, 35, 44, 47, 56]. A problem of such models is
that they hand-craft a term for each external inﬂuence (like
collision avoidance, or walking in groups). This limits their
applicability, because it is difﬁcult to anticipate all possible
interaction scenarios. The problem can be to some degree
alleviated by learning the motion model from data [33], al-
though this, too, only works if all relevant motion and inter-
action patterns are present in the training data. Moreover,
the motion model does not seem to be an important bottle-
neck in present tracking frameworks. By and large, more
powerful dynamic models seem to help only in a compara-
tively small number of situations, while again adding com-
plexity.

Measuring similarity with CNNs. Convolutional archi-
tectures have become the method of choice for end-to-end
learning of image representations. In relation to our prob-
lem, they have also been remarkably successful in assess-
ing the similarity of image patches for different tasks such
as optical ﬂow estimation [21], face veriﬁcation [49], and
depth estimation from multiple viewpoints [22, 60, 62].

In the context of tracking, CNNs have been used to
model appearance and scale variations of the target [18].
Recently, several authors employ them to track via online
learning, by continuously ﬁne-tuning a pre-trained CNN
model [8, 37, 51].

2. Learning to associate detections

Our tracking framework is based on the paradigm of
tracking-by-detection, i.e. ﬁrstly, we run a detector through
the sequences, and secondly, we link the detections to form
trajectories. We propose to address the data association
problem by learning a model to predict whether two de-
tections belong to the same trajectory or not. We use two
sets of features derived from the pedestrian detections to be
compared. First, local spatio-temporal features learnt using

Figure 2: Proposed two-stage learning architecture for pedestrian detection matching.

a CNN and, second, contextual features encoding the rela-
tive geometry and position variations of the two detections.
Finally, both sets of features are combined using a GB clas-
siﬁer [24] to produce the ﬁnal prediction (see Fig.2). De-
coupling local and global features processing and ensem-
bling them in a later stage allows understanding the contri-
bution of each factor plus adding robustness to the predic-
tion [15, 50].

2.1. CNN for patch similarity

A common denominator when comparing two image
patches using CNNs are Siamese architectures where two
inputs are processed simultaneously by several layers with
shared weights (convolutional and/or fully connected) that
eventually merge at some point in the network. Siamese
CNN topologies can be grouped under three main cate-
gories, depending on the point where the information from
each input patch is combined (see Fig.3):

• Cost function.

Input patches are processed by two
parallel branches featuring the same network structure
and weights. Finally, the top layers of each branch are
fed to a cost function [12, 49] that aims at learning a
manifold where different classes are easily separable.

• In-network. In this case, the top layers of the parallel
branches processing the two different inputs are con-
catenated and some more layers are added on top of
that [21, 62]. Finally, the standard softmax log-loss
function is employed.

• Joint data input. The two input patches are stacked
together forming a uniﬁed input to the CNN [21].
Again, the softmax log-loss function is used here.

While the two ﬁrst approaches have yield good results
in classiﬁcation applications, the best performance for tasks
involving comparison of detailed structures is obtained with
the joint data input strategy. As pointed out by [60] and fur-
ther corroborated by [21], jointly using information from
both patches from the ﬁrst layer tends to deliver a better
performance. In order to verify this hypothesis within the
scope of the tracking problem, we trained a Siamese net-
work using the contrastive loss function [13]:

E =

1
2N

N
(cid:88)

n=1

(y) d + (1 − y) max (τ − d, 0) ,

where d = ||an − bn||2
2, being an and bn the L2 normal-
ized responses of the top fully connected layer of the par-
allel branches processing each input image, and τ = 0.2
is the separation margin and y the label value encoded as 0
or 1. The topology of the CNN network has been the same
all through the paper and shown in Fig.2. Our early experi-
ments, showed a relative 8% AUC increase of the joint data
input case over the best performing model from the other
two topologies, given a ﬁxed number of parameters.

I1O1I2O2Stacking(Siamese junction)10x121x53C1C2C3F4F5F6Local spatio-temporal featuresContextual featuresRelative geometry & position featuresGradient Boosting ClassifierFinal Matching PredictionF7employed pedestrian detector thus increasing its capacity of
generalization.

Learning. We trained the proposed CNN as a bi-
nary classiﬁcation task, employing the standard back-
propagation on feed-forward nets by stochastic gradient de-
scent with momentum. The mini-batch size was set to 128,
with an equal learning rate for all layers set to 0.01, se-
quentially decreased every 1.5 epochs by a factor 10, ﬁ-
nally reaching 10−4. Layer weight were initialized follow-
ing [27] and we trained our CNN on a Titan GPU X for 50
epochs. The Lasagne/Theano framework was employed to
run our experiments.

Data augmentation. Even if the available training data
is fairly large, pairs of pedestrian detections tend not to have
a large range of appearances stemming from the fact that the
number of distinct people in the training corpus is limited.
Adding variety to the input data during the training phase
is a widely employed strategy to reduce overﬁtting and im-
prove generalization of CNNs [15, 16, 31]. In our particular
case, we have randomly added geometric distortions (rota-
tion, translation, skewing, scaling and vertical ﬂipping) as
well as image distortions (Gaussian blur, noise and gamma).
These transformations are applied independently for each of
the two input patches but only allowing small relative geo-
metric transformations between them (with the exception of
vertical ﬂipping that is applied to both images, when cho-
sen). Since all these transformation are performed directly
on GPU memory, the augmentation complexity cost is neg-
ligible.

2.2. Evidence aggregation with gradient boosting

The softmax output of the presented Siamese CNN
might be used directly for pedestrian detection association
but the accuracy would be low since we are not taking into
account where and when these detections originated in the
image. Therefore, the need for a set of contextual features
and a higher order classiﬁer to aggregate all this informa-
tion.

Given two pedestrian detections at different time in-
stants, It1 and It2 , encoded by its position x = (x, y) and
dimensions s = (w, h), we deﬁne our contextual features
as: the relative size change, (s1 − s2)/(s1 + s2), the posi-
tion change, (x1 − x2), and the relative velocity between
them, (x1 − x2)/(t2 − t1).

Combining the local and contextual sets of features is
carried out using gradient boosting (GB) [24]. To avoid
overﬁtting on the GB, CNN predictions for each of the train
sequences are generated in a leave-one-out fashion follow-
ing the stacked generalization concept introduced in [53].
Finally, the GB classiﬁer is trained by concatenating the
CNN and contextual features. In our case, we trained the
GB classiﬁer using 400 trees using the distributed imple-
mentation presented in [7].

(a) Cost function

(b) In-network

(c) Input stacking

Figure 3: Siamese CNN topologies

Architecture. The proposed CNN architecture takes as
input four sources of information: the pixel values in the
normalized LUV color format for each patch to be com-
pared, I1 and I2, and the corresponding x and y components
of their associated optical ﬂow [19], O1 and O2. These four
images are resized to a ﬁxed size of 121x53 and stacked
depth-wise to form a multi-modal 10-channel data blob D
to be fed to the CNN. In order to improve robustness against
varying light conditions, for each luma channel L of both I1
and I2 we perform a histogram equalization and a plane ﬁt-
ting, as introduced in [63].

The input data is processed ﬁrst by three convolutional
layers, C1,2,3, each of them followed by a PreReLU non-
linearity [27] and a max-pooling layer that renders the net
more robust to miss alignments within the components of
D. Afterwards, four fully connected layers, F4,5,6,7, aim
at capturing correlations between features in distant parts of
the image as well as cross-modal dependencies, i.e. pixel-
to-motion interactions between I1,2 and O1,2. The output
of the last fully-connected layer is fed to a binary soft-
max which produces a distribution over the class labels
(match/no match). The output of layer F6 in the network
will be used as our raw patch matching representation fea-
ture vector to be fed to the second learning stage.

t , Im

Training data generation. Pedestrian detections pro-
posed using [17] are generated for each frame and associa-
tions between detections are provided across frames during
the training phase. On one hand, positive examples, i.e.
pairs of detections corresponding to target m, (Im
t−k),
1 ≤ k < N , are directly generated from the ground truth
data, with a maximum rewind time of N = 15. On the
other hand, negative examples are generated by either pair-
ing two true detections with belonging to different people,
a true detection with a false positive or two false positive
detections; in order to increase the variety of data presented
to the CNN, we enlarged the set of false positives by ran-
domly selecting patches from the image of a given aspect
ratio that do not overlap with true positive detections. By
generating these random false positives, the CNN does not
overﬁt to the speciﬁc type of false positives generated by the

Cost FunctionOutputABCost FunctionOutputABCost FunctionOutputBA3. Tracking with Linear Programming

In this section, we present the tracking framework where
we incorporate the score deﬁned in the previous section in
order to solve the data association problem.

Let D = {dt
i} be a set of object detections with
dt
i = (x, y, t), where (x, y) is the 2D image position and
t deﬁnes the time stamp. A trajectory is deﬁned as a list of
ordered object detections Tk = {dt1
}, and
k1
the goal of multiple object tracking is to ﬁnd the set of tra-
jectories T ∗ = {Tk} that best explains the detections D.
This can be expressed as a Maximum A-Posteriori (MAP)
problem and directly mapped to a Linear Programming for-
mulation, as detailed in [35, 64].

, · · · , dtN
kN

, dt2
k2

The data association problem is therefore deﬁned by a

linear program with objective function:

very conﬁdent that two detections belong to the same tra-
jectory. We control with Vdet and Vlink the percentage of
negative edges that we want in the graph. The in/out costs,
on the other hand, are positive and they are used so that the
tracker does not indiscriminately create many trajectories.
Therefore, a trajectory will only be created if there is a set
of conﬁdent detections and conﬁdent links whose negative
costs outweigh the in/out costs. Cin = Cout, Vdet and Vlink
are learned from training data as discussed in the next sec-
tion.

The Linear Program in Eq. (1) can be efﬁciently solved
using Simplex [35] or k-shortest paths [4]. Note, that we
could use any other optimization framework, such as maxi-
mum cliques [61], or Kalman ﬁlter [44] for real-time appli-
cations.

T ∗ = argmin

T

(cid:88)

i

Cin(i)fin(i) +

(cid:88)

i

Cout(i)fout(i)

4. Experimental results

(cid:88)

+

i

Cdet(i)f (i) +

(cid:88)

i,j

Ct(i, j)f (i, j)

(1)

subject to edge capacity constraints, ﬂow conservation at
the nodes and exclusion constraints.

The costs Cin and Cout deﬁne how probable it is for a tra-
jectory to start or end. The detection cost Cdet(i) is linked
to the score that detection i was given by the detector. In-
tuitively, if the score si is very high, the cost of the edge
should be very negative, so that ﬂow will likely pass through
this edge, including the detection i in a trajectory. We nor-
malize the costs si = [0, 1] for a sequence, and deﬁne the
detection cost as:

Cdet(i) =

+ 1

(cid:40) −si
Vdet
−si+1
1−Vlink

− 1

if si < Vdet
if si ≥ Vdet

(2)

If we set, for example, Vdet = 0.5, the top half conﬁdent
detections will correspond to edges with negative cost, and
will most likely be used in some trajectory. By varying this
threshold, we can adapt to different types of detectors that
have different rates of false positives.

The cost of a link edge depends only on the probability
that the two detections i and j belong to the same trajectory,
as estimated by our classiﬁer:

Ct(i, j) =






+ 1

−sRF
i,j
Vlink
−sRF
i,j +1
1−Vlink

if sRF

i,j < Vlink

(3)

− 1

if sRF

i,j ≥ Vlink

Note in Eq. (1), that if all costs are positive, the trivial
solution will be zero ﬂow. A trajectory is only created if
its total cost is negative. We deﬁne detection costs to be
negative if we are conﬁdent that the detection is a pedes-
trian, while transition costs are negative if our classiﬁer is

This section presents the results validating the efﬁciency
of the proposed learning approach to match pairs of pedes-
trian detections as well as its performance when creating
trajectories by means of the aforementioned linear pro-
gramming tracker. In order to provide comparable results
with the rest of the state-of-the-art methods, we employed
the large MOTChallenge [34] dataset, a common reference
when addressing multi-object tracking problems. It consists
of 11 sequences for training, almost 40,000 bounding boxes,
and 11 sequences for testing, over 60,000 boxes, comprising
sequences with moving and static cameras, dense scenes,
different viewpoints, etc.

4.1. Detection matching

We ﬁrst evaluate the performance of the proposed learn-
ing approach when predicting the probability of two de-
tections belonging to the same trajectory by means of the
ROC curve computed on the training data of MOT15 [34],
as shown in Fig.4. Two result groups are depicted: ﬁrst,
when only using the CNN classiﬁer (best AUC: 0.718)
and, second, when using the two stage CNN+GB classiﬁer
(best AUC: 0.954); the later yielding to a relative 41% in-
crease in classiﬁcation performance. Oversampling the im-
age (1,2,4 and 8 ﬁxed locations) and averaging their predic-
tions proved to deliver a signiﬁcant improvement, specially
for the CNN part of the end-to-end system. However, the
impact of oversampling in the CNN+GB classiﬁer is less
relevant hence it may be avoided to reduce the overall com-
putation load.

An analysis of the ROC curve on the MOT15 training
data allowed us to ﬁnd the operation point, i.e. proba-
bility threshold Vlink within the linear programming track-
ing, that would maximize its accuracy. In our case, we set
Vlink = 0.35, after cross-validation.

Dataset

TUD-
Crossing

PETS09-
S2L2

ETH-
Jelmoli

ETH-
Linthescher

ETH-
Crossing

AVG-
TownCentre

ADL-
Rundle-1

ADL-
Rundle-3

KITTI-16

KITTI-19

Venice-1

Method
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed
MotiCon
LP2D
Proposed

TA
58.2
49.5
73.7
46.6
40.7
34.5
43.5
40.7
42.3
18.3
16.9
16.7
22.8
21.4
27.5
11.9
15.5
19.3
1.0
2.9
26.5
18.1
13.7
39.7
38.8
35.5
36.9
33.8
20.1
26.7
18.2
11.0
22.3

TP
70.8
74.1
73.0
67.6
70.2
69.7
72.9
73.5
72.8
77.7
76.4
74.2
72.9
76.3
74.1
70.3
68.5
69.0
70.3
72.2
71.6
71.8
72.8
72.9
70.1
72.0
72.6
69.9
65.2
66.2
72.9
72.4
73.0

MT ML
15.4
23.1
15.4
15.4
15.4
69.2
14.3
9.5
16.7
9.5
19.0
7.1
28.9
20.0
26.7
15.6
31.1
24.4
74.1
1.5
73.6
2.0
78.7
4.6
65.4
3.8
65.4
3.8
65.4
3.8
69.9
0.9
33.2
8.4
44.7
4.4
12.5
18.8
21.9
15.6
28.1
28.1
20.5
4.5
25.0
2.3
34.1
11.4
11.8
0.0
11.8
0.0
17.6
0.0
21.0
6.5
21.0
8.1
29.0
6.5
29.4
0.0
35.3
0.0
41.2
0.0

IDsw
403
48
197
238
319
282
37
41
30
72
77
9
8
10
4
74
260
142
136
252
33
217
400
33
36
47
24
100
97
70
74
98
4

Table 1: Detailed result on the 11 sequences of MOTChal-
lenge test, compared to two other methods that use also Lin-
ear Programming.

Programming, and the only factor that changes is the way
the edge costs are computed. In this way, we can see the
real contribution of our proposed learn-based costs. As it
can be seen in Table 1, the results indicate that our learned
data association costs are more accurate, and that this better
low-level evidence is the key factor driving the performance
improvement.

Finally we show the results on the test set of MOTChal-
lenge in Table 2, where we compare to numerous state-
of-the-art trackers. Our method is among the top per-
forming trackers, and contains less false positives than any
other method. Note, that we do not use any type of post-
processing. Again, it clearly outperforms methods based on
Linear Programming (LP2D and MotiCon), thanks to the
proposed edge costs.

Figure 4: Performance accuracy for the Siamese CNN and
the full two-stage learning approach (CNN+GB), when us-
ing an oversampling of 8,4,2 and 1 per pair at the input.

4.2. Multiple people tracking

Evaluation metrics. To evaluate multiple-object tracking
performance, we used CLEAR MOT metrics [5], track-
ing accuracy (TA) and precision (TP). TA incorporates
the three major error types (missing recall, false alarms
and identity switches (IDsw)) while TP is a measure
for the localization error, where 100% again reﬂects a
perfect alignment of the output tracks and the ground
truth. There are also two measures taken from [38] which
reﬂect the temporal coverage of true trajectories by the
tracker: mostly tracked (MT, > 80% overlap) and mostly
lost (ML, < 20%). We use only publicly available detec-
tions and evaluation scripts provided in the benchmark [34].

Determining optimal parameters. As discussed before,
the LP parameter Vlink = 0.35 is given by the opera-
tion point of the ROC curve. The other LP parameters,
Cin = Cout, Vdet are determined by parameter sweep with
cross-validation on the training MOT15 data in order to
obtain the maximum tracking accuracy.

Baselines. We compare to two tracking methods based on
Linear Programming. The ﬁrst is using only 2D distance
information as feature (LP2D), the second [33] is learning
to predict the motion of a pedestrian using image features
(MotiCon). This comparison is specially interesting, since
the optimization structure for all methods is based on Linear

0.00.20.40.60.81.0FPR0.00.20.40.60.81.0TPR1­CNN (area = 0.571)2­CNN (area = 0.630)4­CNN (area = 0.690)8­CNN (area = 0.718)1­GB (area = 0.944)2­GB (area = 0.949)4­GB (area = 0.951)8­GB (area = 0.954)Method
NOMT [9]
MHT-DAM [30]
MDP [55]
SiameseCNN (proposed)
LP-SSVM [52]
ELP [43]
JPDA-m [46]
MotiCon [33]
SegTrack [40]
LP2D (baseline)
DCO-X [41]
CEM [42]
RMOT [58]
SMOT [14]
ALExTRAC [6]
TBD [26]
TC-ODAL [2]
DP-NMS [45]
LDCT [48]

TA
33.7
32.4
30.3
29.0
25.2
25.0
23.8
23.1
22.5
19.8
19.6
19.3
18.6
18.2
17.0
15.9
15.1
14.5
4.7

TP MT ML
71.9
44.0
12.2
16.0
43.8
71.8
38.4
13.0
71.3
48.4
8.5
71.2
53.0
5.8
71.7
43.8
7.5
71.2
58.1
5.0
68.2
52.0
4.7
70.9
63.9
5.8
71.7
41.2
6.7
71.2
54.9
5.1
71.4
46.5
8.5
70.7
53.3
5.3
69.6
54.8
2.8
71.2
52.4
3.9
71.2
47.9
6.4
70.9
55.8
3.2
70.5
40.8
6.0
70.8
32.5
11.4
71.7

IDsw
442
435
680
639
849
1396
365
1018
697
1649
521
813
684
1148
1859
1939
637
4537
12348

FP
7762
9064
9717
5160
8369
7345
6373
10404
7890
11580
10652
14180
12473
8780
9233
14943
12970
13171
14066

Table 2: Results on the MOTChallenge test set.

5. Conclusions

References

In this paper we have presented a two-stage learning
based approach to associate detections within the context
of pedestrian tracking. In a ﬁrst pass, we create a multi-
dimensional input blob stacking image and optical ﬂow in-
formation from to the two patches to be compared; these
data representation allows the following Siamese convolu-
tional neural network to learn the relevant spatio-temporal
features that allow distinguishing whether these two pedes-
trian detections belong to the same tracked entity. These
local features are merged with some contextual features by
means of a gradient boosting classiﬁer yielding to a uniﬁed
prediction.

In order to highlight the efﬁciency of the proposed de-
tection association technique, we use a modiﬁed linear pro-
gramming based tracker [64] to link the proposed corre-
spondences and form trajectories. The complete system
is evaluated over the standard MOTChallenge dataset [34],
featuring enough data to ensure a satisfactory training of
the CNN and a thorough and fair evaluation. When com-
paring the proposed results with the state-of-the-art, we ob-
serve that a simple linear programming tracker fed with
accurate information reaches comparable performance than
other more complex approaches.

Future research within this ﬁeld involve applying the
proposed approach to more generic target tracking, leverag-
ing already trained models and extending the second stage
classiﬁer to deal with more complex contextual features,
e.g. social forces [35]. Evaluation of the proposed archi-
tecture over on datasets is currently under investigation.

[1] A. Andriyenko and K. Schindler. Discrete-continuous optimization

for multi-target tracking. CVPR, 2011. 1, 2

[2] S. Bae and K. Yoon. Robust online multi-object tracking based on
tracklet conﬁdence and online discriminative appearance learning.
CVPR, 2014. 7

[3] J. Berclaz, F. Fleuret, and P. Fua. Robust people tracking with global

trajectory optimization. CVPR, 2006. 2

[4] J. Berclaz, F. Fleuret, E. T¨uretken, and P. Fua. Multiple object track-
ing using k-shortest paths optimization. TPAMI, 2011. 1, 2, 5
[5] K. Bernardin and R. Stiefelhagen. Evaluating multiple object track-
ing performance: The CLEAR MOT metrics. Image and Video Pro-
cessing, 2008(1):1–10, May 2008. 6

[6] A. Bewley, L. Ott, F. Ramos, and B. Upcroft. Alextrac: Afﬁn-
ity learning by exploring temporal reinforcement within association
chains. ICRA, 2016. 7

[7] T. Chen and T. He. xgboost: extreme gradient boosting. GitHub,

2015. 4

[8] Y. Chen, X. Yang, B. Zhong, S. Pan, D. Chen, and H. Zhang. CN-
NTracker: Online discriminative object tracking via deep convolu-
tional neural network. Applied Soft Computing, 2015. 2

[9] W. Choi. Near-online multi-target tracking with aggregated local

ﬂow descriptor. ICCV, 2015. 1, 7

[10] W. Choi and S. Savarese. Multiple target tracking in world coordinate

with single, minimally calibrated camera. ECCV, 2010. 2

[11] W. Choi and S. Savarese. A uniﬁed framework for multi-target track-

ing and collective activity recognition. ECCV, 2012. 2

[12] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric
discriminatively, with application to face veriﬁcation. CVPR, 2005.
3

[13] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric
discriminatively, with application to face veriﬁcation. CVPR, 2005.
3

[14] C. Dicle, O. Camps, and M. Sznaier. The way they move: Tracking

targets with similar appearance. ICCV, 2013. 7

[15] S. Dieleman, A. van den Oord, I. Korshunova, J. Burms, J. Degrave,
L. Pigou, and P. Buteneers. Classifying plankton with deep neural
networks. Blog entry, 2015. 3, 4

[16] S. Dieleman, K. Willett, and J. Dambre. Rotation-invariant convolu-
tional neural networks for galaxy morphology prediction. Monthly
Notices of the Royal Astronomical Society, 450(10):1441–1459,
2015. 4

[17] P. Doll´ar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids

for object detection. PAMI, 2014. 2, 4

[18] J. Fan, W. Xu, Y. Wu, and Y. Gong. Human tracking using convo-
IEEE Transactions on Neural Networks,

lutional neural networks.
21(10):1610–1623, 2010. 2

[19] G. Farneb¨ack. Two-frame motion estimation based on polynomial
expansion. In Image Analysis, volume 2749 of Lecture Notes in Com-
puter Science, pages 363–370. 2003. 4

[20] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan.
Object detection with discriminatively trained part based models.
TPAMI, 2010. 2

[21] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazirbas, V. Golkov,
P. van der Smagt, D. Cremers, and T. Brox. FlowNet: Learning
Optical Flow with Convolutional Networks. ICCV, 2015. 2, 3

[22] J. Flynn,

Deep-
I. Neulander, J. Philbin, and N. Snavely.
Stereo: Learning to predict new views from the world’s imagery.
arXiv:1506.06825, 2015. 2

[23] K. Fragkiadaki, W. Zhang, G. Zhng, and J. Shi. Two-granularity
tracking: mediating trajectory and detections graphs for tracking un-
der occlusions. ECCV, 2012. 1

[24] J. Friedman. Stochastic gradient boosting. Computational Statistics

& Data Analysis, 38(4):367–378, 2002. 3, 4

[25] J. Gall, A. Yao, N. Razavi, L. van Gool, and V. Lempitsky. Hough
forests for object detection, tracking and action recognition. TPAMI,
2011. 2

[26] A. Geiger, M. Lauer, C. Wojek, C. Stiller, and R. Urtasun. 3d trafﬁc
scene understanding from movable platforms. TPAMI, 2014. 7
[27] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti-
ﬁers: Surpassing human-level performance on imagenet classiﬁca-
tion. ICCV, 2015. 4

[28] H. Jiang, S. Fels, and J. Little. A linear programming approach for

multiple object tracking. CVPR, 2007. 2

[29] Z. Khan, T. Balch, and F. Dellaert. Mcmc-based particle ﬁltering for

tracking a variable number of interacting targets. TPAMI, 2005. 2

[30] C. Kim, F. Li, A. Ciptadi, and J. Rehg. Multiple hypothesis tracking
revisited: Blending in modern appearance model. ICCV, 2015. 1, 2,
7

[31] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classiﬁcation
with deep convolutional neural networks. ANIPS, 2012. 1, 4
[32] C.-H. Kuo and R. Nevatia. How does person identity recognition

help multi-person tracking? CVPR, 2011. 1, 2

[33] L. Leal-Taix´e, M. Fenzi, A. Kuznetsova, B. Rosenhahn, and
S. Savarese. Lerning an image-based motion context for multiple
people tracking. CVPR, 2014. 2, 6, 7

[34] L. Leal-Taix´e, A. Milan, I. Reid, S. Roth, and K. Schindler.
Motchallenge 2015: Towards a benchmark for multi-target tracking.
arXiv:1504.01942, 2015. 5, 6, 7

[35] L. Leal-Taix´e, G. Pons-Moll, and B. Rosenhahn. Everybody needs
somebody: Modeling social and grouping behavior on a linear pro-
gramming multiple people tracker. ICCV. 1st Workshop on Modeling,
Simulation and Visual Analysis of Large Crowds, 2011. 1, 2, 5, 7
[36] L. Leal-Taix´e, G. Pons-Moll, and B. Rosenhahn. Branch-and-price
global optimization for multi-view multi-object tracking. CVPR,
2012. 2

[37] H. Li, Y. Li, and F. Porikli. Deeptrack: Learning discriminative fea-
ture representations online for robust visual tracking. ArXiv e-prints,
2015. 2

[38] Y. Li, C. Huang, and R. Nevatia. Learning to associate: Hybrid-
boosted multi-target tracker for crowded scene. CVPR, 2009. 6
[39] W. Luo, X. Zhao, and T.-K. Kim. Multiple object tracking: A review.

arXiv:1409.7618 [cs], Sept. 2014. 2

[40] A. Milan, L. Leal-Taix´e, K. Schindler, and I. Reid. Joint tracking and

segmentation of multiple targets. In CVPR, 2015. 2, 7

[41] A. Milan, K. Schindler, and S. Roth. Multi-target tracking by

discrete-continuous energy minimization. TPAMI, 2016. 7

[42] A. Milan and S. R. R. Schindler. Continuous energy minimization

for multitarget tracking. TPAMI, 2014. 7

[43] P. M. N. McLaughlin, J. Martinez Del Rincon. Enhancing linear pro-
gramming with motion modeling for multi-target tracking. WACV.
7

[44] S. Pellegrini, A. Ess, K. Schindler, and L. van Gool. You’ll never
walk alone: modeling social behavior for multi-target tracking.
ICCV, 2009. 1, 2, 5

[45] H. Pirsiavash, D. Ramanan, and C. Fowlkes. Globally-optimal
greedy algorithms for tracking a variable number of objects. CVPR,
2011. 2, 7

[46] H. Rezatoﬁghi, A. Milan, Z. Zhang, Q. Shi, and I. R. A. Dick. Joint

probabilistic data association revisited. ICCV, 2015. 7

[47] P. Scovanner and M. Tappen. Learning pedestrian dynamics from the

real world. ICCV, 2009. 2

[48] F. Solera, S. Calderara, and R. Cucchiara. Learning to divide and

conquer for online multi-target tracking. ICCV, 2015. 7

[49] Y. Taigman, Y. Ming, M. Ranzato, and L. Wolf. DeepFace: Clos-
ing the gap to human-level performance in face veriﬁcation. CVPR,
2014. 2, 3

[50] H. Wang, A. Cruz-Roa, A. Basavanhally, H. Gilmore, N. Shih,
M. Feldman, J. Tomaszewski, F. Gonzalez, and A. Madabhushi. Cas-
caded ensemble of convolutional neural networks and handcrafted
features for mitosis detection, 2014. 3

[51] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung. Transferring rich feature
hierarchies for robust visual tracking. ArXiv e-prints, 2015. 2
[52] S. Wang and C. Fowlkes. Learning optimal parameters for multi-

target tracking. BMVC, 2015. 7

[53] D. H. Wolpert. Stacked generalization. Neural Networks, 5:241–259,

1992. 4

[54] Z. Wu, T. Kunz, and M. Betke. Efﬁcient track linking methods for
track graphs using network-ﬂow and set-cover techniques. CVPR,
2011. 2

[55] Y. Xiang, A. Alahi, and S. Savarese. Learning to track: Online multi-

object tracking by decision making. ICCV, 2015. 7

[56] K. Yamaguchi, A. Berg, L. Ortiz, and T. Berg. Who are you with and

where are you going? CVPR, 2011. 2

[57] B. Yang and R. Nevatia. An online learned crf model for multi-target

tracking. CVPR, 2012. 2

[58] J. Yoon, H. Yang, J. Lim, and K. Yoon. Bayesian multi-object track-
ing using motion context from multiple objects. IEEE Winter Con-
ference on Applications of Computer Vision (WACV), 2015. 7
[59] Q. Yu, G. Medioni, and I. Cohen. Multiple target tracking using
spatio-temporal Markov chain Monte Carlo data association. CVPR,
2007. 2

[60] S. Zagoruyko and N. Komodakis. Learning to compare image
patches via convolutional neural networks. CVPR, 2015. 2, 3
[61] A. Zamir, A. Dehghan, and M. Shah. Gmcp-tracker: Global multi-
object tracking using generalized minimum clique graphs. ECCV,
2012. 1, 2, 5

[62] J. Zbontar and Y. LeCun. Computing the stereo matching cost with

a convolutional neural network. CVPR, 2015. 2, 3

[63] C. Zhang and Z. Zhang.

Improving multiview face detection with
multi-task deep convolutional neural networks. IEEE Winter Confer-
ence on Applications of Computer Vision, 2014. 4

[64] L. Zhang, Y. Li, and R. Nevatia. Global data association for multi-

object tracking using network ﬂows. CVPR, 2008. 1, 2, 5, 7

