Journal of Systems and Software 00 (2021) 1–17

Procedia
Computer
Science

Precise Learning of Source Code Contextual Semantics via
Hierarchical Dependence Structure and Graph Attention Networks

Zhehao Zhaoa, Bo Yangb,∗, Ge Lia, Huai Liuc, Zhi Jina,∗

aKey Laboratory of High Conﬁdence Software Technologies, Peking University, Beijing 100871, China
bSchool of Information Science and Technology, Beijing Forestry University, Beijing 100083, China
cDepartment of Computing Technologies, Swinburne University of Technology, Hawthorn VIC 3122, Australia

Abstract

Deep learning is being used extensively in a variety of software engineering tasks, e.g., program classiﬁcation and defect prediction.
Although the technique eliminates the required process of feature engineering, the construction of source code model signiﬁcantly
aﬀects the performance on those tasks. Most recent works was mainly focused on complementing AST-based source code models
by introducing contextual dependencies extracted from CFG. However, all of them pay little attention to the representation of basic
blocks, which are the basis of contextual dependencies.

In this paper, we integrated AST and CFG and proposed a novel source code model embedded with hierarchical dependencies.
Based on that, we also designed a neural network that depends on the graph attention mechanism. Speciﬁcally, we introduced
the syntactic structural of the basic block, i.e., its corresponding AST, in source code model to provide suﬃcient information and
ﬁll the gap. We have evaluated this model on three practical software engineering tasks and compared it with other state-of-the-
art methods. The results show that our model can signiﬁcantly improve the performance. For example, compared to the best
performing baseline, our model reduces the scale of parameters by 50% and achieves 4% improvement on accuracy on program
classiﬁcation task.

Keywords: Graph Neural Network, Program Analysis, Deep Learning, Abstract Syntax Tree, Control Flow
Graph
1. Introduction

Recently, deep learning has been increasingly applied
into program analysis tasks, such as program classiﬁ-
cation [1, 2, 3], software defect prediction [4, 5], and
code summarization [6, 7, 8]. However, the perfor-
mance on these tasks heavily depends on the choice
of source code model, which can be divided into three
types: abstract syntax tree- (AST-) based, control ﬂow
graph- (CFG-) based and the hybrid model of these two.
Moreover, depending on the structure of AST adopted
during analysis, AST-based source code model can be

∗Corresponding author
Email addresses: zhaozhehao@pku.edu.cn (Zhehao Zhao),

yangbo@bjfu.edu.cn (Bo Yang), lige@pku.edu.cn (Ge Li),
hliu@swin.edu.au (Huai Liu), zhijin@pku.edu.cn (Zhi Jin)

1

further divided to the whole AST [9, 10, 11] or par-
tial AST [12, 13, 14]. The syntactic structure within
AST can illustrate all the information of source code,
especially the subtle changes on it. However, the con-
textual dependencies are implicit in AST and cannot be
extracted and learnt eﬀectively. In contrast, the CFG-
based source code model [15, 16] is good at providing
contextual dependencies, which can be learnt eﬀectively
by graph neural networks. Nevertheless, CFG is un-
eﬀective to represent the information of statements lo-
cated in the basic blocks. Therefore, some researches
proposed methodologies to embed the contextual de-
pendencies from CFG into AST[17, 18, 19]. Such a de-
sign idea of the hybrid method still take AST as the core
part of the source code model. It would add the con-
textual dependencies as additional edges [17] to AST

1
2
0
2

v
o
N
0
2

]
E
S
.
s
c
[

1
v
5
3
4
1
1
.
1
1
1
2
:
v
i
X
r
a

 
 
 
 
 
 
/ Journal of Systems and Software 00 (2021) 1–17

2

or as assistant features [18]. However, the basic blocks,
which are the basis of contextual dependencies, are paid
little attention by the existing methodologies. To mine
the contextual dependencies eﬀectively, we argue that
the features of basic blocks should be prioritized. Fig-
ure 1 shows our motivational example. These two code
segments come from the PROMISE dataset used in our
study. The defect in Figure 1(a) is that returning a null
value on line 7 will cause a NullPointerException, and
the corresponding ﬁx is to return a Field type array of
length 0 here. After analyzing this example, we have
the following observations.

1 class Document {
2

3

4

5

6

7

8

public final Field [] getFields (

String name ) {

List result = new ArrayList () ;
...
if ( result . size () == 0)

return null ;

...

}

9
10 }

1 class Document {
2

private final static Field []

NO_FIELDS = new Field [0];

3

4

5

6

7

8

public final Field [] getFields (

String name ) {

List result = new ArrayList () ;
...
if ( result . size () == 0)

return NO_FIELDS ;

...

}

9
10 }

(a) The defect version of lucene-
2.2

(b) The ﬁxed version of lucene-
2.4

Figure 1: A Motivating Example from PROMISE dataset

Observation 1: This defect depends on the actual
execution path. As shown in Figure 1a, the defect is
triggered only if the condition on line 6 is met. How-
ever, if the caller of the getFields function properly han-
dles caught exceptions, this defect will not be triggered.
Thus, a reasonable source code model should reﬂect the
execution path. Furthermore, since a large number of
invocations to getFileds are outside from the Document
class, the source code model should not be limited to a
certain granularity.

Observation 2: These two source codes diﬀer
slightly but with total diﬀerent semantics. As shown
in Figure 1, the diﬀerence of these two source codes
is a choice between returning an identiﬁer NO FIELDS
or a null in line 7. The code in Figure 1b does not
cause the exception because that NO FIELDS refers to
an object (see line 2 of Figure 1b). Thus, the diﬀer-
ence of these two source code is actually the diﬀer-
ence between object and null. Moreover, for the deep
learning models with some textual features (e.g., Bag
of Words), the learning of these two words (null and
NO FIELDS) is uneven, since null occurs more fre-
quently than NO FIELDS, which would raise the dif-
ﬁculty for models to learn the real diﬀerence.

According to the Observation 1, CFG would intu-
itively become the ﬁrst choice of source code model,
since CFG can show the potential execution path and
can be constructed on any granularity. But there still
exists the issue about how to represent the basic blocks
within the CFG. In the existing CFG-based works, ba-
sic blocks are mainly represented by either line num-

2

bers [18] or Bag of Words [20, 21]. However, accord-
ing to the Observation 2, these methods only utilize
the textual features, which signiﬁcantly relies on the
frequency of occurrence. Thus they cannot eﬀectively
capture the diﬀerence shown in Figure 1 to distinguish
NO FIELDS and null. We argue that a proper source
code model should introduce semantic diﬀerences (e.g.,
the diﬀerence between object and null) into the deep
learning models more than the textual distinctions.

Motivated by these observations, we propose a novel
source code model. Speciﬁcally, to overcome the limita-
tion mentioned in the Observations 1, we choose CFG
with dataﬂow (ECFG), which can reﬂect the actual exe-
cution paths, as the backbone of the source code model.
To address the Observations 2, we use the block-level
AST, i.e., each AST subtrees correspond to each ECFG
basic blocks. Take the source codes in Figure 1 to illus-
trate the beneﬁt of such way: since NO FIELDS repre-
sents an object while null is just a keyword, the syntax
rule for them are not same, which brings diﬀerent AST
structures. To sum up, the whole model can be divided
into two levels. At the outer level, we use the inter-
procedure ECFG to express the dependencies between
the basic blocks. At the inner level, we choose AST to
express the structure of each basic block.

Our source code model has three advantages. First,
beneﬁting from the ECFG as the main body, the granu-
larity of our source code model can be ﬂexibly adjusted.
Second, also beneﬁting from the ECFG, our source code
model can show the potential execution path explicitly,
thus the contextual dependencies can be captured eﬀec-
tively by a graph neural network. Third, beneﬁting from
the substructure of AST, our source code model can
have a more informative representation of basic blocks,
hence the features within each basic blocks can be cap-
tured eﬀectively by a tree-based neural network.

Furthermore, we designed a Multi-Flow Graph Neu-
ral Network (MFGNN) to extract features from our
source code model. The calculation of MFGNN can
be divided into three steps. At the ﬁrst step, we ob-
tain features named local features through TBCNN [9]
from the collection of AST-substructures, which are cor-
responded to the basic blocks in ECFG. At the sec-
ond step, we extract features named contextual features
from ECFG, whose basic blocks has been ﬁlled with
local features. Since the ECFG is a directed graph
with multi-typed edges where we want to adopt atten-
tion mechanism, we did a slightly modiﬁcation on the
original Graph Attention Network (GAT). Speciﬁcally,
the modiﬁed model supports directed graph and multi-
typed edges, we name it as Attention-based Graph Net-
work for Directed Graph (AGN4D), and apply it in the

/ Journal of Systems and Software 00 (2021) 1–17

3

second step. At the third step, we apply a fusion layer to
coalesce these features into hybrid features, which can
be used for subsequent tasks.

makes AST widely used in a variety of software engi-
neering tasks[9, 17, 11, 12, 4, 13, 14].

To be speciﬁc, this paper has the following three ma-

2.1.2. Control Flow Graph

jor contributions:

• We propose a source code model that combines
AST and CFG with dataﬂow (ECFG). The source
code model can reﬂect both contextual dependen-
cies and syntactic structure, which allows neural
networks to learn richer program features.

• We design a learning model to obtain contextual
semantics from the source code model, namely
Multi-Flow Graph Neural Network (MFGNN).
MFGNN integrates an attention-based graph learn-
ing layer evolves from GAT.

• MFGNN is implemented and evaluated on three
typical tasks, namely the program classiﬁcation,
software defect prediction and code clone detec-
tion. The results show that MFGNN can extract
richer program features than the state-of-the-art
methods, and hence greatly improve the perfor-
mance of these tasks.

The remainder of this paper is organized as fol-
lows: Section 2 introduces the background of our work.
Section 3 describes the new source code model and
MFGNN. We report our experimental studies and re-
sults in Sections 4 and 5, respectively. The related work
is discussed in Section 6. Finally, we conclude this pa-
per in Section 7.

2. Background

In this section, we would introduce some basic con-

cepts and terms that are used in this paper.

2.1. Program representation

To represent a piece of program, there are several
ways: token sequences, AST, CFG [16]. Among all of
them, AST and CFG are adopted most widely, thus we
would introduce both of them in this section.

2.1.1. Abstract Syntax Tree

Abstract Syntax Tree (AST) is a tree representation
of the abstract syntactic structure of source code written
in a programming language [9]. Each node on the AST
represents a nonterminal symbol in the syntax rules of
the programming language. Being a near-source-level
program graph structure, AST can represent the syn-
tactic information of programs in a simple way, which

Control Flow Graph (CFG) is a directed graph in
which each node (namely basic block) represents a set
of sequentially executed instruction sequences, and the
edges represent control ﬂow paths. CFG is mostly used
in static analysis and compiler applications, as it can ac-
curately represent the ﬂow inside a program. For ex-
ample, through graph reachability analysis, CFG can
help locate inaccessible code in programs, and ﬁnd syn-
tax structures such as loops. As a source code model
for deep learning, CFG’s edges are usually considered
to represent the contextual dependencies, which have a
signiﬁcant impact on the performance of software engi-
neering tasks[18, 22, 17].

2.2. Graph Neural Networks

Graph is a generic data structure to eﬀectively ab-
stract objects and their connections [23].
It has been
widely used across multiple domains, such as social net-
works [24], chemical interaction [25] and knowledge
modeling [26].

Graph Neural Networks (GNNs) are methods used
to mine the information within a graph and obtain the
embedding vector of the graph under a learning model.
GNNs are mostly based on the message-passing mech-
anism, and consist of two functions: the Message func-
tion and the Aggregate function [23]. The Message
function is used to transform the original vector of nodes
to obtain the hidden vector; and the Aggregate function
is used to aggregate the transformed vectors of a node’s
adjacency nodes and obtain an embedding vector of the
node.

The Message function is generally represented using
. Let X = {x1, x2, ..., xn}, xi ∈
features of nodes, and H =
be the transformed features of

a parameter W ∈ RF∗F(cid:48)
RF be the initial
{h1, h2, ..., hn}, hi ∈ RF(cid:48)
nodes. Then, the Message function can be deﬁned as:

hi = Message(xi) = W xi

, where F represents the initial dimension of nodes’ fea-
tures, and F(cid:48) represents the transformed dimension of
nodes’ features.

Diﬀerent GNNs often vary in the Aggregate func-
tions. For example, GCN [27] uses summation as the
Aggregate function, which is deﬁned as follows.

h(cid:48)
i

= Aggregate(h, Ni) =

Ni(cid:88)

hi

i

3

/ Journal of Systems and Software 00 (2021) 1–17

4

, where Ni is the collection of adjacency nodes of i.

GAT [28] uses the self-attention mechanism as the
Aggregate function. GAT ﬁrst calculates self-attention
weights for all edges in the graph, as deﬁned below:

αi j =

exp(LeakyReLU(a(Whi||Wh j)))
Σk∈Niexp(LeakyReLU(a(Whi||Whk)))

, where || is the concatenation operation and a : RF(cid:48) ×
RF(cid:48) → R is the shared attention mechanism.

GAT then linearly combines the transformed features
of the neighbouring nodes according to the attention
weights, which is deﬁned as:

Ni(cid:88)

= σ(

h(cid:48)
i

αi jh j)

j

where σ is a nonlinearity function.

DefStmt

Local

NewExpr

java util List

java util ArrayList

...

BOp

==

Invoke

Const

F

Local

Method

0

int

java util List

int

T

ReturnOp

ArrayRef

StaticFieldRef

org

apache lucene document Field

...

DefStmt

Local

NewExpr

java util List

java util ArrayList

...

BOp

==

Invoke

Const

F

Local

Method

0

int

java util List

int

T

ReturnOp

NULL

...

(a) correct version

(b) faulty version

Figure 2: The comparison of the motivating example using the com-
bination of CFG and AST. Black edges for sequential execute, or-
ange edges for conditional true branch and fuchsia edges for the false
branch. The dashed-purple edges are dataﬂow edges.

3. Approach

In this section, we would introduce our source code

model and the learning model named MFGNN.

3.1. Constructing Graph through Combining ECFG

and AST

The combination of ECFG and AST can be consid-
ered as a type of program dependency graph. The back-
bone of the graph is an inter-procedural CFG. A CFG
G = (B, E) of the program is a directed graph, where B
is a collection of basic blocks and E contains all control
ﬂow relationships. We choose three-address code (e.g.,

4

LLVM IR for C/C++ and Jimple for Java) as the inter-
mediate representation when generating CFG by anal-
ysis frameworks (e.g., Clang for C/C++ and Soot for
Java).

From the motivating example (see Section 1), we can
conclude that a precise modeling for basic block is es-
sential for the following analysis. Therefore, we choose
AST to model basic blocks as its nature of expressing
syntactic structures and code information. To further
enrich the information in the AST, we introduce the data
types of variables to the subtrees of AST, which inher-
ently lacks of such information and corresponds to vari-
able usage (e.g., DeclRefExpr node in Clang). Specif-
ically, for the basic data type, we directly add it as a
leaf node of the variable node. For user-deﬁned classes,
we refer to the method mentioned in [29], and separate
these classes according to the Camel-Case naming. For
type conversion statements, we process both the original
type and the target type according to the above method
and then add them into the AST as a subtree of the type
conversion node. Another aspect we need to consider
is the constants. To handle diﬀerent constants, we dis-
assemble the constant value bitwise (e.g., The constant
nodes 456 will be disassembled to three nodes, repre-
sent 4, 5, 6,respectively.).

To address the lack of dataﬂow dependency in both
AST and CFG, in addition to the control ﬂow, call ﬂow,
and exception ﬂow relationships included in the CFG,
we also introduce data ﬂow relationship into our graph.
Speciﬁcally, dataﬂow relationship comes from the intra-
procedural dataﬂow analysis. By traversing the CFG,
we have built two collections of variables : de f ine
stores variables deﬁned in the block and use stores vari-
ables used in the block. The dataﬂow relationship be-
tween basic blocks is obtained by reaching deﬁnition
analysis later. Then we divide the edges of control ﬂow
into four categories according to their functionalities:
sequential execution, conditional true branch, condi-
tional false branch and switch branches. Diﬀerent cate-
gories of ﬂow relationship are labeled distinctly. Over-
all, there are seven types of edges in our source code
model.

The ﬁnal graph of the motivating example is shown
in Figure 2. We can observe that the red and green
blocks of the two snippets respectively containing dif-
ferent AST, representing the great diﬀerences in local
features between blocks. For the correct version (Fig.
1b), the AST of the green block indicates that a static
ﬁeld of that class is returned. For the faulty version (Fig.
1a), the AST of the red block indicates that a null value
is returned. This slight textual diﬀerence, NO FIELDS
vs. null, can be easily learnt with the help of a tree-

/ Journal of Systems and Software 00 (2021) 1–17

5

based neural network due to the signiﬁcant diﬀerence in
the AST. The control ﬂow and the dataﬂow edges (i.e.,
dashed-purple edges) jointly describe the use of vari-
ables, and the conditional true edges (i.e., orange edges)
indicate the branch and precondition of the faulty block.
Combined with the diﬀerence in local features between
green and red blocks, our source code model leads to
diﬀerent context features.

3.2. Multi-Flow Graph Neural Network

We design a neural network model to obtain the fea-
tures of our representation model and name it as Multi-
Flow Graph Neural Network (MFGNN). Figure 3 shows
the overall structure of the model. The learning process
can be divided into three stages: local features embed-
ding stage, contextual features embedding stage and fu-
sion stage. In the ﬁrst stage, the tree-based network is
used to learn the local features for each block in B. In
the second stage, attention-based graph neural network
for directed graph (AGN4D) is used to learn contextual
features in the combined graphs based on local features
of each block. In the ﬁnal stage, a fusion layer is used
to fuse local features and contextual features, and the
contextual semantics are obtained.

3.2.1. Local Features Embedding

We use Tree-based Convolutional Neural Network
(TBCNN) to obtain the local features in each block. The
original TBCNN is not suitable for our extended AST
because of the additional contents on the leaf nodes of
the extended AST. The learning process of the original
TBCNN ignores the fact that the deeper the node in the
AST, the richer the information. Therefore, we adjust
the preset weights of TBCNN to increase the weight of
deeper nodes in the convolution window of TBCNN,
i.e., the nodes with richer information would have a
more signiﬁcant impact on the training process of the
model. The formulas of the weights are shown as fol-
lows:

ηt
i

= di − 1
dmax − 1

ηl
i

= ηt
i

pi − 1
n − 1

ηr
i

= ηt

i(1 − ηl
i),

(1)

, where di is the depth of node i in the entire tree, dmax
is the depth of entire tree, pi is the position of the node
i in subtree, and n is the total number of i’s siblings.

The importance of local features is two-fold: ﬁrst, as
the features of each block, local features is the input for
AGN4D (see Section 3.2.2) for learning contextual fea-
tures; second, local features play an critical role within
the ﬁnal features, so we pass local features to the fusion
layer (see Section 3.2.3) directly.

3.2.2. Contextual Features Embedding

Our source code model can be considered as a di-
rected graph with edge types. Therefore, based on GAT
(see Section 2.2), we design a network layer nested in
MFGNN, named Attention-based Graph Neural Net-
work for Directed Graph (AGN4D), which can han-
dle directed graph and multiple types of edges. With
AGN4D, we can extract contextual features from the
combination graph.

Suppose G is an instance of the combined graph and
RG is the reverse graph of G. Let X = {x1, x2, ..., xn} rep-
resent local features of the blocks in the set B obtained
in the previous stage. Let the initial graph embedding
n} where H0 = X, the graph embed-
H0 = {h0
ding update process of G is as follows:

2, ..., h0

1, h0

kl
o,u
hl
o,u
hl
u

o(hl−1
= MS Gl
u )
= Aggl
o,u, {kl
o(kl
+ hl
= hl
r,u

o,u

+ hl−1
u

kl
r,u
o,v|v ∈ N G

= MS Gl
u }) hl
r,u

r(hl−1
u )
= Aggl

r(kl

r,u, {kl

r,v|v ∈ N RG

u

})

(2)

u

, where N G
u is the collection of successors of block u in
original graph G and N RG
for reverse graph RG. MS Gl
o
represents the MS G function of the original graph at
layer l and MS Gl
o refers
to the Agg function of the original graph at layer l and
Aggl
r for the reverse graph. Note that MS G function and
Agg function do not share parameters between diﬀerent
layers.

r for the reverse graph. Aggl

After obtaining the graph embedding from the two
graphs, the graph embedding of previous layer and cur-
rent layer are connected by a skip-connection to obtain
the ﬁnal graph representation of this layer.

The MS G function needs to transform the graph em-
bedding from the previous layer to obtain the features of
for this layer, which is parameterized by a weight matrix
W l

key which is deﬁned as follows:

kl
u

= MS Gl(hl−1

u ) = W l

keyhl−1
u .

(3)

The Agg function aggregates features in successor
blocks and current block. We add support for multiple
types of edges to the self-attention mechanism of GAT,
deﬁned as follows:

el
v
αl
v

h(cid:48)
u

5

+ Pl, f

srckl
u

= Pl
dstkl
= so f tmax({LeakyReLU(el

v < u, v, f > ∈ E

= Aggl(kl

u, {kl

v|v ∈ Nu}) = σ

v)|v ∈ Nu})

(cid:88)


vkl
αl
v

v∈Nu

(4)





/ Journal of Systems and Software 00 (2021) 1–17

<

int

int

AST

if(a<b)
c=a+b
else
c=a-b
Source Code

entry

a<b

end

F
c=a-b

T
c=a+b

our source code model

Ingoing
Edges

Outgoing
Edges

Local Features

Ki

αi

combi

Ko

αo

combo

AGN4D

Figure 3: MFGNN Structure

6

r
e
ﬁ

i
s
s
a
l
c

+

g
n
i
l
o
o
p
-
x
a
m

Contextual
Features

, where f stands for the ﬂow type of the edge from u
to v. Attention mechanism is parameterized by Pl
src and
Pl, f
dst, which indicates the importance of the f -type ﬂow
dependency between blocks u and v.

We pass the h(cid:48)

u of the last layer of AGN4D to the fu-

sion layer as contexutal features.

3.2.3. Fusion Layer

The main functionality of the fusion layer is to fuse
local features and contextual features into the hybrid
features of the program. In our design, the fusion layer
ﬁrst adds local features and contextual features, then
gets the ﬁxed size program feature vector through dy-
namic pooling. In practice, we choose max-pooling as
a pooling function. Finally, we train a classiﬁer (i.e.,
Logistic Regression (LR)) for classiﬁcation tasks.

4. Evaluation

We conducted a series of experiments to evaluate
MFGNN with comparison against some existing state-
of-art methods. Our experiments run on a 4 Tesla k40c
GPUs machine with Xeon E5-2310 32GB RAM.

4.1. Research Questions

To evaluate the eﬀectiveness of our source code
model and MFGNN, and compare them with several
state-of-the-art methods on some particularly tasks, our
experiments were particularly designed to answer the
following ﬁve research questions:

RQ1 How is the performance of MFGNN in classifying
datasets that consists of programs with small tex-
tual but large semantic diﬀerences?

RQ2 How is the performance of MFGNN in Within-
Project Defect Prediction (WPDP) task compared
with the state-of-the-art methods?

RQ3 How is the performance of MFGNN in Cross-
Project Defect Prediction (CPDP) task compared
with the state-of-the-art methods?

RQ4 How is the performance of MFGNN in Functional
Code-Clone Detection (CCD) task compared with
the state-of-the-art methods?

RQ5 To what extent do diﬀerent components

in

MFGNN inﬂuence the performance?

4.2. Datasets

For RQ1 and RQ5, we selected two datasets as the ob-
jects of our experiments, namely CodeChef and Code-
forces. The Codechef dataset is collected by Phan [15]
and composed of solutions, written in C/C++, which
are submitted by users for four challenges, namely
SUB, MNMX, FLOW, and SUM. However, these four chal-
lenges are trivial (e.g., FLOW only requires an imple-
mentation of the GCD algorithm), which cannot eval-
uate the eﬀectiveness of our tool thoroughly. Thus,
we further manually collected a dataset, namely Code-
forces, from a public website1. Speciﬁcally, it con-
sists of solutions submitted by users for ﬁve challenges,
i.e., 1062C [30], 721C [31], 731C [32], 742C [33]
and 822C [34]. The challenges involved in the Code-
forces dataset covers a variety of algorithms that are
more complicated (e.g., disjoint-union sets, Dijkstra and
greedy algorithm). Speciﬁcally, the detailed description
of these challenges are described as follows:

1https://codeforces.com

6

/ Journal of Systems and Software 00 (2021) 1–17

7

Index

CodeChef

Codeforces

Problems
Instance Count
Avg. Line of Code
Avg. Branches Count
Avg. Operators Count

SUB
2313
30
9
25

FLOW
5487
25
8
15

MNMX
9693
25
8
15

SUM
11666
36
12
35

1062C
9136
45
12
40

721C
16084
65
10
40

731C
10170
55
21
29

742C
6971
52
15
30

822C
17379
55
18
39

Table 1: The statistics of program classiﬁcation dataset for RQ1 and RQ5.

• 1062C: Given a binary-valued string and a list of
intervals.
, for each interval, the frequencies of
each value in the interval is used to calculate a for-
mula. A preﬁx sum (and product) algorithm is re-
quired to solve this challenge.

• 721C: Given a weighted directed graph, the short-
est path is found between two speciﬁc nodes. Di-
jkstra algorithm is required to solve this challenge.

• 731C: Given an undirected graph, the number of
connected components in the graph is counted. A
disjoint-union sets is required to solve this chal-
lenge.

• 742C: Given a directed graph, the least common
multiplier (LCM) is calculated for the lengths of
all the circles in the graph. To solve this challenge
correctly, circle ﬁnding algorithm and LCM algo-
rithm are required.

• 822C: Given a collection of weighted intervals, a
subset of the minimum weight sum is found to sat-
isfy some conditions (e.g., no intersect between in-
tervals). A greedy algorithm is required to solve
this challenge.

For each program in both datasets, there is a label
to indicate the running result of the corresponding pro-
gram. The meaning of labels is detailed in the follow-
ing:

• Accepted (AC): The program is able to pass all test

cases;

• Wrong Answer (WA): The program can execute

normally but output incorrect results;

• Runtime Error (RE): The program cannot exe-
cute normally on some test cases, which are gen-
erally due to illegal memory access or operation
error, e.g., divided by zero;

• Time Limited Exceeded (TLE): The program

does not response within the time limits;

7

• Memory Limited Exceeded (MLE): The con-
sumed resource, i.e., memory, exceed the require-
ment.

Except for the AC, diﬀerent running results corre-
spond to diﬀerent defects in source code. For example,
the source code with the TLE often contains redundant
steps or dead loops, while the source code with the WA
often contains functional errors. Therefore, we argue
that a reasonable source code model should reﬂect these
diﬀerences and is able to classify them eﬀectively.

Additionally, we conducted a pre-processing on both
datasets. First, we removed the source code that are
irrelevant to the corresponding challenge. Second, we
removed the duplicated ones from datasets. Third, to
avoid mislabeling, we generated some test cases accord-
ing to the requirements of the corresponding challenge.
Then, we re-ran the source code and re-labeled them
that were mis-labeled. Finally, for each dataset of chal-
lenges, we split each of them into training set, validation
set and test set in 3:1:1 ratio. Table 1 shows some met-
rics of the ﬁnal datasets.

For RQ2 and RQ3, we have selected another well-
known public dataset, namely PROMISE. The reason is
that it has been widely used for software defect predic-
tion [4, 11, 35], and it consists of several well-known
open-source Java projects. Except for the jedit (Version
3.2), which cannot be compiled properly, the remaining
10 Java projects and their corresponding versions that
we selected are identical to a previous work [4] for com-
parison. Finally, 1395 source code ﬁles, which cannot
be processed successfully by our Soot-based generator,
were removed from the dataset. The statistical descrip-
tion of the ﬁnal dataset for RQ2 and RQ3 is shown in
Table 2.

For the remaining research question, i.e., RQ4, we
have selected a public dataset, namely OJClone, which
has been adopted by several works [12, 22]. It was col-
lected from an online program judgement system for
C/C++ source code. Speciﬁcally, OJClone contains 15
program tasks, and each of them is composed of 500
source code ﬁles submitted by users. For the same task,

/ Journal of Systems and Software 00 (2021) 1–17

8

App

Ver Mean ﬁles Mean defective

Defective rate

lucene
synapse
xerces
xalan
camel
log4j
ant
jedit
poi
ivy

3
3
2
2
3
2
3
3
3
2

247
188
295
665
700
70
422
311
328
253

140
52
54
237
165
29
95
67
219
26

56.7
27.7
18.3
35.6
23.6
41.4
22.5
21.5
66.8
10.3

Table 2: The statistics of PROMISE dataset, which is specialized for
RQ2 and RQ3

diﬀerent users’ source codes could pass the test and got
AC verdict, and thus can be considered as functional
code clone. In other words, for each source code pair in
the dataset, it will be labeled by either 0 for non-cloned
pair or 1 for cloned pair. Similarly to the classifying
task, we shuﬄed and split the dataset into training, val-
idation and testing in 3:1:1 ratio.

4.3. Experiment Settings

In this section, we present the setup of each RQ’s ex-
periment, involving detailed settings about our method,
the choices of baseline methods and comparison met-
rics.

4.3.1. Settings for MFGNN

The input of MFGNN consists of four parts: 1) a col-
lection of AST nodes (represented by one-hot vectors);
2) a collection of AST’s substructures; 3) a mapping
graph (i.e., mapping the substructure to correspond-
ing basic block); and 4) an ECFG As for the hyper-
parameters, the embedding dimension of the AST nodes
is set as 50. And the dimension of AGN4D, which
is stacked with three layers, was set as 200. MFGNN
was optimized by Adamax, and trained for 200 epochs.
During the training, we selected the parameters (i.e.,
weights of MFGNN) that performed best on the vali-
dation set, and evaluated them on the test set.

4.3.2. Settings for Baselines
For RQ1. To illustrate the eﬀectiveness of MFGNN, we
choose three other well-known groups of representative
methods for comparison:

SVM-based approaches We chose SVM-based
i.e.,
approaches to demonstrate that both datasets,
CodeChef and Codeforces, do consist of source code
with small textual but large semantic distinctions.
In
terms of classifying source code ﬁles according to their
textual features, the more indistinguishable the source

code are, the worse SVM-based methods would per-
form. To show the textual distinguishability of our
dataset, we choose TF-IDF and BoW features as the tex-
tual features, and feed them into RBF-kernel SVM.

AST-based approaches To illustrate the advantages
of our source code model over AST in program classiﬁ-
cation, we chose several typical AST-based approaches.
Speciﬁcally, according to AST granularity, we can di-
vide the AST-based approaches into two categories.
One uses the entire AST of source code, like represen-
tative methods: TBCNN [9] and Tree-LSTM [36]. The
other one splits AST according to code fragments and
is known as ASTNN [12]. Moreover, code2vec [13]
adopts paths in AST to represent the source code and
learns the features contained in the paths through a
network based on attention mechanisms. Similarly,
code2seq [14] uses the same paths as code2vec but ex-
tracts the features by the seq2seq model [37].

For the settings of AST-based approaches, the AST
used in TreeLSTM, TBCNN and ASTNN is generated
by Clang, but the AST paths used by code2vec and
code2seq are generated by ASTMiner2. For code2vec,
the embedding dimension is set to 400; For code2seq,
the embedding dimension is set to 128 and the decoder
dimension is set to 320; The hidden dimension of the
other methods is set to 200.

Graph-based approaches Some recent studies fo-
cused on representing a program as a graph and adopt-
ing a graph-based learning method to extract depen-
dency features from the graph. DGCNN chooses CFG
as the source code model and obtains features with
GCN [15]. ContextGraph (CtxG) inserts extra edges
(e.g., dataﬂow edges) into the original AST, and extracts
the features with GGNN [38]. For the settings of graph-
based approaches, the number of steps of GGNN is set
to 3, and the hidden size of all graph-based approaches
was set to 200.

For RQ2. We evaluated the performance of MFGNN
on Within-Project Defect Prediction (WPDP) task. Ac-
cording to previous studies on defect prediction task [4,
11], we decided to use the same strategy, i.e., train-
ing by the earlier version and predicting on the later
version. We compared MFGNN with several typical
WPDP methods that can be divided into two types ac-
cording to their adopted source code models. Some
of defect prediction technologies used the features of
the PROMISE with traditional machine learning meth-
ods [39, 40], including Adaboost, Multi-Layer Percep-

2https://github.com/JetBrains-Research/astminer

8

/ Journal of Systems and Software 00 (2021) 1–17

9

tion (MLP) and Random Forest (RF). The others uti-
lize AST-based features, and representative methods
(e.g., DBN [4] and TreeLSTM [11]). Speciﬁcally, DBN
obtained the semantic features from AST. We classi-
ﬁed these features with three classiﬁers: Naive-Bayes
(DBNNB), Logistic Regression (DBNLR) and Decision
Tree (DBNDT ). As for TreeLSTM, after the AST was
parsed by JavaParser3, it would take the entire AST as
input for prediction. Additionally, we chose another
two well-known methods : DTL-DP [35] and BugCon-
text [18]. The former one visualized the source code ﬁle
(or binary ﬁle) as an image, and obtained the defect fea-
tures with AlexNet, while the later one acquired contex-
tual dependencies from CFG and DFG, then introduced
them into path-based AST features.

For RQ3. We conducted Cross-Project Defect Predic-
tion (CPDP) experiments to show the performance of
MFGNN. Following the previous studies [4, 11], we or-
ganized ten groups of experiments, trained models on
the source project and predicted on the target project.
For the target project, according to transfer learning
methods [41], we ﬁrst randomly selected 30% of the
data to ﬁne-tune a LR-based classiﬁer and then pre-
dicted the rest 70%. Except for DBN, which was re-
placed by its CPDP-variant: DBN-CP [4], we chose
the same set of baseline methods as RQ2. Additionally,
we added two transfer learning-based methods, namely
TCA+ [41] and TNB [42], which take the PROMISE
feature as same as the machine learning methods.

For RQ4. We conducted Functional Code-Clone De-
tection (CCD) experiments to demonstrate the distin-
guishability of semantics obtained by MFGNN. Let the
features of the two source code ﬁles within a pair that
are obtained from MFGNN be v1 and v2, respectively.
The diﬀerence can be deﬁned as d = |v1−v2|. Finally, we
use a LR-based classiﬁer (i.e., y = sigmoid(Wod + bo))
to determine whether the code pairs are similar based on
the vector d. We compared the performance of MFGNN
with several state-of-the-art models that are widely used
including RAE+ [43], Deckard [44],
on CCD task,
CDLH [45], ASTNN [12], DeepSim [46], and FCDe-
tect [22].

For RQ5. We carried out some ablation studies. Our
approach can be divided into two parts, a source code
model based on ECFG and a learning model with the
AGN4D layer. Firstly, we explored the impact of dif-
ferent choices in the design of our source code model,

3https://javaparser.org

9

which has four options: 1) representing basic blocks
with AST (A) or BoW features (B); 2) including control
ﬂow edges (C) or not; 3) including dataﬂow edges (D)
or not; and 4) embedding the source code model with
multi-typed edge (M) or with single-typed edge (S). We
have designed four variants based on the combination
of diﬀerent options.

• AST+CFG+Single: The main body of this model
is CFG with no distinction between control ﬂow
types, and its basic blocks are represented using
ASTs.

• AST+DFG+Single: The main body of this model
is DFG, with only one type of ﬂow, and its basic
blocks are represented using ASTs.

• AST+CFG+Multi: The main body of this model is
a CFG that distinguishes between diﬀerent control
ﬂows, and its basic blocks are represented using
ASTs.

• BoW+CFG+DFG+Multi: The main body of this
model is a CFG that contains the dataﬂows and
distinguishes between diﬀerent types of ﬂows. Its
basic blocks are represented using BoW.

Secondly, we explored the impact of diﬀerent graph
learning methods. We replaced the AGN4D layer with
graph convolution network (GCN) and gated-graph
neural network (GGNN), respectively. Additionally, we
compared across the diﬀerent options in AGN4D, i.e.,
summation and concatenation, to synthesize graph fea-
tures (see Eq.2) on the same source code model.

4.3.3. Metrics

For RQ1 and RQ5, we chose the accuracy and macro-
F1 [47] to evaluate the prediction result on test sets. As-
suming a task has K classes, the accuracy is deﬁned as
follow:

accuracy =

,

(5)

(cid:80)K

i=1 T Pi
N

, where T Pi refers to true positive of class i, and N is the
total number of samples.

For a binary classiﬁcation task, the F1-score (F1) is

deﬁned as follow:

F1-score = 2 ∗ precision ∗ recall
precision + recall

(6)

, where precision = T P
T P+FN , T P de-
notes the true positive, FP represents the false positive,
and FN refers to false negative.

T P+FP and recall = T P

/ Journal of Systems and Software 00 (2021) 1–17

10

A multi-label classiﬁcation task can be considered as
several binary classiﬁcation tasks on diﬀerent labels.
Based on that, assuming the task has K classes, the
macro-F1 can be deﬁned as follow:

Macro-F1 = 1
K

K(cid:88)

i=1

F1-scorei.

(7)

For RQ2 and RQ3, in addition to the F1 on the buggy
class, we also used the metric AUC (Area Under the re-
ceiver operating characteristics Curve) [11] to evaluate
the performance of defect prediction. Speciﬁcally, AUC
refers to the probability of a classiﬁer ranking a ran-
domly selected positive sample higher than a randomly
selected negative sample. Intuitively speaking, a higher
value of AUC implies a better performance.

For RQ4, following the evaluation metrics of previ-
ous works [12, 22], we choose precision (P), recall (R)
and F1 to measure the performance of the selected mod-
els on CCD task.

5. Results

In this section, we show the results of the experi-
ments, and compare the performance of diﬀerent meth-
ods.

5.1. Answer to RQ1

Table 3 illustrates the results related to RQ1, and the
best performance are highlighted in bold. In column 2,
we list the size of the corresponding model except for
the SVM-based approaches, whose size is neglectable.
According to these experimental results, we have the
following insights:

The dataset does consist of source code with a min-
imal textual diﬀerence. As we can see, SVM-based
methods did not play well in our experiments, which
is reﬂected by their corresponding F1 values. This indi-
cates that the source codes with diﬀerent labels in our
dataset cannot be eﬀectively distinguished by textual
features. In other words, it proves that the textual dif-
ferences among the source codes in out dataset are too
small to be distinguished eﬀectively.

Compared to AST-based approaches, MFGNN
achieves a better performance with fewer parameters.
Compared to the best method, i.e., TreeLSTM, among
AST-based approaches, MFGNN reduces the model pa-
rameters by up to 50%, while achieving 4.0% and 6.8%
improvements on accuracy and F1, respectively. Ad-
ditionally, we can observe that both of code2vec and
code2seq did not perform well. This is because both
of them model the source code by sampling the path of

10

the AST, which can only capture potential connections
between code tokens [48]. Program classiﬁcation task,
however, requires the identiﬁcation of the actual control
ﬂow and dataﬂow information of the program execu-
tion, which can not be achieved by their models. On
the contrary, our source code model can reﬂect the ac-
tual execution path of the program with contextual in-
formation, which can be better captured by the neural
network.

MFGNN achieves a signiﬁcant performance im-
provement while adding a limited number of pa-
rameters compared with the graph-based approaches.
Compared to the best graph-based approach, DGCNN,
MFGNN only increases the number of parameters by 4
times, but achieves 5% and 8.1% improvement on ac-
curacy and F1, respectively. Similarly, compared with
DGCNN, which has the same scale of parameters as
MFGNN, MFGNN achieves 4.8% and 8.4% improve-
ment on accuracy and F1, respectively. This result il-
lustrates that the performance of MFGNN has little cor-
relation with its number of parameters. The main dif-
ference between MFGNN and traditional graph-based
methods is two-fold. On one hand, the integration of
multiple ﬂow information in the source code model
clearly expresses the dependency features of the pro-
gram well. On the other hand, the attention mechanism
allows MFGNN to dynamically adjust the weights of
diﬀerent types of ﬂows, resulting in a better mining of
the ﬂow features.

5.2. Answer to RQ2

Table 4 shows the performance of diﬀerent ap-
proaches on the within-project defect prediction
(WPDP) task, and the best performances are highlighted
in bold. Due to the limitations of Soot (e.g., throw ex-
ceptions on some data items), our dataset lost a large
number of entries in some projects, which resulted in
the distribution of the dataset we actually used diﬀers
from the previous study [4]. To ensure the fairness of
the comparison, we re-implemented the DBN methods
and TreeLSTM mentioned in [11]. We selected multiple
groups of parameters randomly, ran all methods multi-
ple times and kept the best result.

Compared with the state-of-art method, namely
TreeLSTM, MFGNN achieved 1.6% and 4.0% im-
provements on F1 and AUC, respectively. Moreover,
MFGNN was 5% and 29.6% higher in F1 and AUC, re-
spectively, than DTLDP. Speciﬁcally, higher AUC often
means that the model has more conﬁdence in the predic-
tion results, and the main diﬀerence between MFGNN
and these methods is the use of ECFG on the source

/ Journal of Systems and Software 00 (2021) 1–17

11

Groups

Methods

SUB MNMX FLOW SUM

1062C

721C

731C

742C

822C

Avg

Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1

SVM

SVM&TF-IDF 34.7 12.9 48.0 16.2 56.6 18.1 38.4 13.9 51.0 13.6 38.7 11.2 42.9 12.0 60.0 15.0 56.2 14.4 47.4 14.1
SVM&BoW 54.5 41.5 68.6 52.5 80.0 60.8 59.9 52.6 55.7 21.5 42.6 22.6 55.4 33.1 66.9 26.2 56.8 16.6 60.0 36.4

AST

TBCNN (0.5M) 67.2 65.2 74.6 69.2 75.3 66.0 63.8 62.4 63.0 39.9 53.7 47.1 65.6 52.9 66.9 38.8 58.9 48.9 65.4 54.5
TreeLSTM (4.0M) 66.1 64.1 76.0 69.5 76.8 68.4 66.3 65.9 66.9 47.7 56.0 50.6 69.1 53.1 70.0 41.2 60.7 50.2 67.5 56.7
ASTNN (0.9M) 61.4 58.9 70.3 63.1 74.3 62.4 62.7 62.4 63.6 46.6 49.9 44.3 61.2 50.0 64.6 32.6 55.0 42.3 62.6 51.4
code2vec (173M) 29.5 24.7 36.6 25.7 29.7 21.1 31.4 24.5 41.2 18.4 28.9 18.5 28.1 18.2 49.2 18.0 30.7 14.5 33.9 20.4
code2seq (61M) 35.9 16.9 50.4 16.9 51.7 30.0 31.9 21.3 51.4 13.6 36.0 15.1 43.0 13.9 52.3 17.3 56.5 14.5 45.5 17.7

Graph

DGCNN (0.4M) 64.8 64.5 74.6 67.7 83.8 70.9 69.1 67.4 64.3 42.8 54.2 49.6 61.4 47.0 70.3 44.1 56.2 44.5 66.5 55.4
DGCNN (2.4M) 64.4 62.6 74.2 66.5 82.7 72.0 69.1 67.9 64.8 42.1 55.3 49.9 61.7 48.5 72.6 43.5 55.9 42.5 66.7 55.1
64.8 62.0 74.0 68.0 74.9 63.9 64.9 64.6 59.1 42.0 51.1 45.3 59.0 47.8 65.0 36.8 56.4 43.3 63.2 52.6

CtxG (4.9M)

MFGNN (2.1M)

74.5 74.7 83.1 81.4 81.8 71.0 72.9 73.5 68.0 53.2 59.5 54.5 70.0 61.0 73.8 51.6 59.9 50.3 71.5 63.5

Table 3: Results on program classiﬁcation task, the numbers in parentheses are the parameter sizes of methods.

Methods

Adaboost MLP

RF

DBNNB

DBNLR

DBNDT Tree-LSTM DTLDP BugContext MFGNN

Project Tr T F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC

ant

1.5 1.6 37.8 68.4 32.0 72.5 36.2 70.5 4.3 81.5 40.7 80.7 4.3 51.1 29.7 49.7 45.3 22.8 31.1 44.4 33.1 72.5
1.6 1.7 52.2 69.4 51.4 71.6 49.1 74.1 53.2 69.3 51.7 79.0 22.8 50.6 44.2 60.8 35.5 50.0 45.1 44.7 53.7 75.5

camel

1.2 1.4 40.2 70.3 39.8 68.6 47.2 75.9 12.9 53.1 16.5 40.4 9.3 51.9 53.1 82.7 32.9 34.1 36.2 52.6 54.3 83.6
1.4 1.6 40.2 70.9 30.7 68.9 45.9 70.1 13.7 58.4 32.0 58.4 8.0 44.2 55.9 79.7 34.7 44.2 27.8 50.3 56.8 84.0

ivy

1.4 2.0 14.3 66.9 14.8 67.8 23.1 69.4 47.6 61.5 27.3 57.9 26.7 57.7 15.9 45.8 21.1 18.5 31.9 44.5 22.9 60.2

jedit

4.0 4.1 57.0 80.7 54.3 80.4 54.5 79.9 41.3 45.6 41.6 50.0 0.0 50.4 62.0 78.8 23.8 35.7 38.5 63.1 65.0 84.4

lucene

2.0 2.2 58.5 63.7 59.9 63.4 59.4 65.7 32.7 65.3 36.6 65.4 35.8 53.3 60.9 59.9 58.9 48.0 43.0 58.4 64.6 64.0
2.2 2.4 64.8 56.6 68.4 57.5 64.8 62.1 25.7 47.3 37.4 73.3 14.2 71.6 68.1 59.1 68.8 40.3 68.0 60.3 68.8 63.4

log4j 1.0 1.1 66.7 78.0 73.3 82.5 75.0 84.2 75.0 88.5 60.5 90.2 72.3 64.8 73.3 75.8 24.0 46.9 75.5 66.7 73.3 77.0

poi

1.5 2.5 77.3 72.6 78.4 72.1 73.3 74.3 8.5 45.8 8.4 65.4 13.4 40.9 81.6 75.8 81.9 59.5 79.7 62.1 83.1 78.4
2.5 3.0 54.6 50.2 68.4 52.2 58.7 55.6 28.0 76.4 27.0 78.6 8.9 78.7 73.9 69.5 77.7 71.9 65.2 58.3 73.3 69.2

synapse

1.0 1.1 28.9 64.6 15.0 61.1 14.7 57.9 47.9 64.4 43.0 66.3 48.9 60.5 28.2 43.2 41.0 51.7 18.8 40.1 30.4 61.1
1.1 1.2 40.3 61.2 44.1 64.4 40.0 66.8 41.5 69.1 41.5 50.1 35.9 66.5 50.3 57.8 54.4 43.7 42.4 55.0 50.3 65.6

xalan 2.4 2.5 32.9 62.1 21.9 59.7 27.9 59.1 19.1 51.1 30.8 58.2 10.6 55.4 34.5 63.9 50.4 43.8 17.4 51.9 33.1 58.7

xerces 1.2 1.3 29.6 62.6 24.2 60.3 25.7 57.9 24.1 53.5 32.4 64.0 33.3 64.5 29.4 60.7 14.8 29.4 9.4

51.5 30.9 74.2

Avg

46.4 66.5 45.1 66.9 46.4 68.2 31.7 62.1 35.2 65.2 23.0 57.5 50.7 64.2 44.3 42.7 42.0 53.6 52.9 71.5

Table 4: The result of WPDP experiment on PROMISE.

code model allows MFGNN to capture contextual de-
pendencies.

Compared to the BugContext method, MFGNN im-
proved 7.3% and 18.7% in F1 and AUC, respectively.
We think such a signiﬁcant improvement can be at-
tributed to their structural diﬀerence, which can be di-
vided into three-fold. First, the representation of ba-
sic blocks. According to the open-source implementa-
tion of the BugContext, it only embeds line numbers
into basic blocks, while MFGNN uses AST to repre-
sent those basic blocks. Second, the process of learning
AST features. BugContext learns tree features by sam-

11

pling the paths of the tree, while TBCNN is adopted
to learn the features by MFGNN. Third, the process
of learning graph. MFGNN uses AGN4D to capture
the dependency features in the graph, while BugContext
uses node2vec to learn the information in the PDG. The
biggest advantage of AGN4D over node2vec is the in-
troduction of an attention mechanism, which allows dif-
ferent types of dependency features to be fused. In con-
clusion, the hybrid features obtained by MFGNN could
perform better on WPDP task.

/ Journal of Systems and Software 00 (2021) 1–17

12

Source

Target

Adaboost MLP
DBN-CP DTLDP BugContext MFGNN
TNB
F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC F1 AUC

RF

TCA+

ant-1.6
jedit-4.1

camel-1.4 23.9 62.9 37.2 63.3 26.8 65.9 28.0 52.9 40.0 67.5 31.9 60.7 22.8 42.0 22.6 42.5 36.3 65.3
camel-1.4 25.7 60.4 26.5 49.4 16.7 59.7 29.0 51.2 32.0 64.2 23.4 61.1 31.3 54.9 11.7 51.6 39.8 67.4

camel-1.4
poi-3.0

camel-1.4
log4j-1.1

ant-1.6
ant-1.6

54.3 69.4 32.8 38.1 38.2 70.1 25.0 42.1 59.0 79.0 56.1 74.3 47.8 18.6 22.2 66.9 50.3 71.4
48.8 68.7 62.7 80.3 55.0 73.8 28.0 46.8 53.0 73.6 48.2 63.3 44.8 26.5 51.0 68.2 56.9 75.2

jedit-4.1
jedit-4.1

34.8 57.7 13.2 30.2 37.6 71.1 50.0 63.8 53.0 76.2 32.3 59.1 38.4 36.7 45.2 70.2 41.5 64.2
57.7 78.6 56.1 72.1 56.6 78.5 18.0 35.7 62.0 79.4 48.4 67.4 39.9 62.0 38.0 49.8 57.8 78.8

jedit-4.1
lucene-2.2

log4j-1.1 26.3 63.4 0.0 13.2 12.9 84.9 61.0 61.8 71.0 84.3 37.8 61.1 59.6 49.1 31.6 24.2 57.1 67.9
log4j-1.1 64.1 74.1 60.4 92.0 70.8 82.3 52.0 60.9 63.0 79.7 45.2 53.8 46.5 41.7 53.2 62.4 54.5 63.1

lucene-2.2
xerces-1.3

xalan-2.5 63.6 57.4 68.5 60.9 61.7 61.1 58.0 54.8 45.0 53.0 57.2 61.0 37.8 54.4 43.2 48.9 67.4 66.6
xalan-2.5 38.4 50.7 62.8 59.0 21.8 56.0 59.0 53.9 57.0 53.5 26.8 46.9 64.9 40.2 23.6 55.7 63.5 61.1

xalan-2.5
log4j-1.1

xalan-2.5
ivy-2.0

lucene-2.2 46.5 54.8 74.7 64.0 51.6 59.7 64.0 63.1 54.0 57.9 56.4 60.5 74.5 50.7 65.4 54.8 64.3 56.6
lucene-2.2 49.3 62.3 37.8 57.9 55.0 60.8 60.0 55.6 54.0 63.1 52.7 55.8 76.4 56.1 62.9 50.0 70.2 63.1

xerces-1.3 35.4 55.9 0.0 37.8 39.3 64.7 23.0 39.5 31.0 49.6 32.4 57.5 15.7 43.4 34.4 62.4 50.0 74.3
xerces-1.3 12.5 64.2 0.0 33.8 20.0 52.7 45.0 66.7 37.0 60.3 36.6 59.6 29.4 51.3 32.1 53.2 47.8 71.7

xerces-1.3
synapse-1.2

ivy-2.0
ivy-2.0

34.6 71.1 39.5 79.7 35.5 70.1 30.0 68.9 34.0 77.2 30.5 57.2 11.3 54.5 25.3 67.8 37.4 79.5
33.3 74.3 51.1 78.7 34.7 74.5 24.0 62.5 38.0 82.1 29.6 62.0 22.0 18.2 40.7 71.9 39.0 78.9

ivy-1.4
poi-2.5

ivy-2.0
poi-3.0

synapse-1.1 9.4 66.1 20.9 35.6 3.4 63.2 45.0 61.4 51.0 70.0 9.7 51.9 15.7 54.1 9.4
37.4 42.7 57.8
synapse-1.1 28.3 48.1 34.9 54.2 46.5 62.9 43.0 62.7 5.0 44.4 49.0 63.4 35.2 30.0 37.0 56.4 48.5 68.6

synapse-1.2 39.7 69.7 34.5 49.7 24.2 68.9 52.0 62.3 57.0 70.7 32.4 53.6 45.7 39.8 17.5 50.2 62.0 73.3
synapse-1.2 56.3 69.9 55.8 66.0 53.8 56.2 56.0 67.6 43.0 62.8 49.5 62.3 29.4 34.0 49.8 55.2 65.7 75.1

synapse-1.2
ant-1.6

poi-3.0
poi-3.0

57.7 74.1 51.7 59.3 27.2 70.0 72.0 61.6 71.0 75.6 48.5 59.5 73.9 56.5 66.2 56.7 81.4 82.2
47.0 68.5 47.2 53.2 37.8 70.2 38.0 33.9 65.0 79.7 43.5 66.0 33.3 56.4 44.7 41.6 81.1 84.3

Avg

40.3 64.6 39.5 55.8 37.6 67.2 43.6 55.9 48.9 68.4 39.9 59.9 40.7 44.1 37.6 54.5 55.2 70.3

Table 5: The result of CPDP experiment on PROMISE

5.3. Answer to RQ3

The cross-project defect prediction (CPDP) task men-
tioned in RQ3 mainly evaluates whether the contextual
features learnt by the model can be applied to diﬀer-
ent projects. To answer this question, we compared our
proposed method, MFGNN, with several typical CPDP
methods, and the results are shown in Table 5. The best
performance among all methods are marked in bold.
Depending on the type of input data, we can further di-
vide the performance into two types:
the best perfor-
mance among metric-based methods is marked in blue
and among source code model-based methods is marked
in red .

Among all methods, MFGNN achieved the highest
overall F1 and AUC. Compare to the best metric-based
methods, MFGNN outperformed 6.3% and 1.9% in F1
and AUC, respectively. Compare to other source code
model-based methods, MFGNN achieved the highest F1
and AUC in most of the tasks. Interestingly, the Bug-
Context does not perform as well as its result on the
WPDP task (see Section 5.2). Compared with BugCon-

text, the F1 and AUC of MFGNN were improved by
up to 27.6% and 15.8%, respectively. We think the rea-
son of improvement lies behind their diﬀerence of using
context-dependent information, which could be divided
into two-fold. On one side, BugContext uses depen-
dency features to assist AST features, while MFGNN
does the opposite. Learning program context-dependent
features is critical for CPDP task, thus such a design
diﬀerence can lead to a discrepancy in performance.
On the other side, BugContext extracts features from
CFG and DFG separately, while MFGNN combines
them into the ECFG and extracts features uniformly by
AGN4D.

In conclusion, the contextual features obtained by
MFGNN are more generalized and are able to result in
better performance on the CPDP task.

5.4. Answer to RQ4

Table 6 illustrates the results related to RQ4, and the
best results are highlighted in bold. Compared with
other methods, MFGNN achieved the highest recall and

12

/ Journal of Systems and Software 00 (2021) 1–17

13

F1, as well as a relative high precision. Interestingly,
we could observe that FCDetect plays well, which ap-
ply call graph as the source code model. However, we
argue that MFGNN can capture the program context-
dependency features more eﬀectively. The main diﬀer-
ence between them is the graph learning mechanisms
they adopted. Compared to the Graph2Vec adopted by
FCDetect, MFGNN uses AGN4D based on the attention
mechanism, and thus could adjust the weights of diﬀer-
ent types of dependency information. Therefore, with
the help of more context-dependency features, MFGNN
could identify program variants more eﬀectively, lead-
ing to higher recall and F1 scores.

Figure 4: t-SNE mapping of the absolute distances of the test set’s
pairs’ features.

Figure 4 shows the absolute distances of features de-
rived from MFGNN for the data in the test set. We
can observe that there is a clear demarcation line be-
tween the red and blue dots. This illustrates the features
obtained by MFGNN can eﬀectively distinguish source
codes under the functional code-clone task. In conclu-
sion, MFGNN can improve the performance of distin-
guishing between non-cloned and cloned source code
pairs.

5.5. Answer to RQ5

To answer this question, we have adjusted the default
settings in our original methodology and compared their
performance on the program classiﬁcation task. The re-
sults are shown in Table 7, in which the default settings
are highlighted in bold. We can obtain the following
insights:

Sensitivity to the control ﬂow and dataﬂow diﬀers
from challenges. Using only DFG as the source code
model (i.e., A+D+S) works better on some challenges,

13

e.g., SUB and SUM. This is because there are much more
operators than branches within these source code (see
Table 1). In other words, these challenges have simple
control ﬂows, but complex computational logic, which
is related to data ﬂow heavily. Thus, compare to control
ﬂow edges, data ﬂow edges play a more critical role on
the test results. However, in general, CFG only (i.e.,
A+C+S) could perform better than adopting only DFG.
Introducing diﬀerent types of edges in CFG may
lead to poorer performance. Introducing diﬀerent types
of edges plays a positive role on some challenges, in-
cluding SUB, 721C, 731C, 742C, and 822C. However,
on other challenges, MFGNN performs better when the
source code model is untyped (e.g., A+C+S). This is
because these challenges require fewer branches than
the others (see Table 1). The imbalanced distribution
of types lead to ineﬀective optimization of the model on
diﬀerent types. Therefore, the uneven distribution of the
number of diﬀerent edge types prevents MFGNN from
eﬀectively fusing the features of diﬀerent types of ﬂows.
AST is the a better choice for node representation
in our experiment settings. The results show that us-
ing AST as a node representation improved the model’s
performance signiﬁcantly. Even when the other settings
in the approach were removed (e.g., A+C+S which
removed data ﬂow edges and edge types, or A+D+S
which removed control ﬂow edges), the approach still
performed better than B+C+D+M, which represents
node by Bag-of-Words (BoW) model instead of AST.
Compared to the model in BoW, i.e., B+C+D+M, our
source code model (A+C+D+M) resulted in 7.7% and
9.8% improvement on accuracy and F1, respectively.
Because the node representation is the only independent
variable here, we can conclude that AST is a better node
representation option for our task. Compared to AST,
BoW lacks both the lexical order and syntactic struc-
tures, which are essential for a proper representation of
basic blocks.

AGN4D is the best choice among the three GNNs.
To examine the eﬀectiveness of AGN4D, we altered it
into two other common GNNs, i.e., GCN and GGNN,
respectively, into our approach for a comparison study.
Table 7 shows that AGN4D outperformed the other two
GNNs, with an average of 2.7% and 5.3% higher accu-
racy and F1, respectively.

Summation is a better choice than concatenation
in contextual feature embedding stage. From the re-
sults, the use of summation as a graph feature synthe-
sis method (i.e., the last formula of 2) delivered bet-
ter performance. This is because concatenation doubles
AGN4D’s hidden dimension layer by layer, increasing
the number of model parameters and resulting in model

/ Journal of Systems and Software 00 (2021) 1–17

14

Methods RAE+ Deckard CDLH ASTNN DeepSim FCDetect MFGNN

P
R
F1

52.5
68.3
59.4

99
5
10

47
73
57

98.9
92.7
95.5

70
83
76

97
95
96

96.7
96.3
96.5

Table 6: The results of ccd task on OJClone.

Diﬀerent Settings

SUB

MNMX

FLOW

SUM

1062C

721C

731C

742C

822C

Avg

Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1

A+C+S
A+D+S
A+C+M

70.6 70.6 80.9 79.5 82.6 72.2 71.4 72.2 66.8 57.0 59.2 53.5 66.4 56.9 74.6 48.6 60.2 50.8 70.3 62.4
70.8 71.4 80.9 76.7 78.7 69.3 71.5 71.8 66.2 43.5 57.5 53.7 63.8 56.3 72.2 45.0 56.3 49.6 68.7 59.7
70.6 72.2 80.7 77.6 82.1 70.7 70.8 70.8 64.9 48.8 59.7 54.2 66.9 56.6 75.5 52.9 59.4 52.0 70.1 61.8
B+C+D+M 69.7 68.3 75.4 68.8 72.8 63.5 64.6 64.0 57.1 38.8 49.0 44.2 61.6 51.7 68.0 40.3 55.7 43.9 63.8 53.7
A+C+D+M 74.5 74.7 83.1 81.4 81.8 71.0 72.9 73.5 68.0 53.2 59.5 54.5 70.0 61.0 73.8 51.6 59.9 50.3 71.5 63.5

concatenation
summation

66.9 63.8 79.5 74.6 80.6 69.4 70.6 70.1 65.9 51.0 55.8 50.4 66.8 54.0 69.8 43.4 60.3 47.0 68.5 58.2
74.5 74.7 83.1 81.4 81.8 71.0 72.9 73.5 68.0 53.2 59.5 54.5 70.0 61.0 73.8 51.6 59.9 50.3 71.5 63.5

GCN
GGNN
AGN4D

72.3 70.4 80.4 77.2 81.3 71.8 70.7 70.8 64.6 43.5 56.6 52.2 64.3 53.2 69.7 39.9 59.8 48.8 68.9 58.6
74.7 73.9 83.2 81.7 82.8 72.1 71.5 71.2 62.7 41.6 53.7 48.1 63.6 50.6 69.5 42.7 56.7 38.0 68.7 57.8
74.5 74.7 83.1 81.4 81.8 71.0 72.9 73.5 68.0 53.2 59.5 54.5 70.0 61.0 73.8 51.6 59.9 50.3 71.5 63.5

Table 7: Results of ablation studies

overﬁtting issue.

5.6. Threats to Validity

In conducting our experiments, the following factors

existed that might aﬀect the validity of the our study.

Implementation of baselines. The internal threat
to validity is concerned with our implementation. We
reproduced TBCNN, ASTNN, CtxG, DBN, TCA+,
TreeLSTM, BugContext. Although we have imple-
mented these baseline methods as described in the orig-
inal studies, we cannot guarantee that these implemen-
tations exactly match the original ones.

Applying baselins on our dataset. In carrying out
the task, we found that many of the baseline methods
were designed speciﬁcally for a particular task, for ex-
ample code2vec’s goal was to perform function name
generation and CtxG’s goal was to perform var-misuse
detection. Although we compared these methods as
baselines, we cannot guarantee that these we can meet
the conditions for these representations of the model to
work well.

Missing projects in PROMISE dataset. Our RQ2
and RQ3 experiments are based on the PROMISE
dataset, a very early dataset in which some versions
of projects recorded are not available on the web. We
were only able to conduct experiments using projects
that could be found and could not directly use the orig-
inal experimental data from the DBN [4] and TreeL-
STM [11] studies.

14

CFG diﬀerences in diﬀerent languages.

For
C/C++, we use Clang to get the CFG, which converts
the program to LLVM IR, a kind of three-address code,
and then builds the CFG on top of that. For Java, we use
Soot to get the CFG. Soot will ﬁrst convert the program
into Jimple, a kind of SSA, and then build the CFG on
top of that. Because of the diﬀerence in the intermedi-
ate languages used, the ﬁnal CFG may not be exactly
the same for the same statements in both languages.

Conduct experiments on more tasks and more
practical datasets. To evaluate the feasibility and eﬀec-
tiveness of MFGNN, we have conducted several tasks
(e.g., program classiﬁcation and defect prediction) on
the datasets consisting of source codes from OJ and
open source projects. Though the variety of evaluated
tasks and the sources of datasets were limited, we argue
that MFGNN is robust enough even on large-scale real-
world industrial code to perform other types of tasks,
which, however, requires follow-up studies in the fu-
ture.

6. Related Works

6.1. Source Code Representation in Deep Learning

While performing program analysis with deep learn-
ing, the representation model of source code is a funda-
mental problem, which could be roughly divided into:
AST-based and CFG-based. Speciﬁcally, as for the
AST-based source code model, some studies adopted
the AST that is generated from the program directly [10,

/ Journal of Systems and Software 00 (2021) 1–17

15

9, 11] or with some modiﬁcations (e.g., inserting ad-
ditional edges between nodes [17]). Moreover, some
works [12, 13, 14] just extracted part of the generated
AST to conduct the following analysis. For examples,
Alon et al. [13, 14] chose the collection of AST’s token-
to-token path as the source code model, and learned the
features by attention-based models. Unlike these mod-
els, we chose to split the AST into subtrees based on
basic blocks. Though it would slightly broke the in-
tegrity of the AST, the explicit contextual dependencies
in the CFG could reassemble parts of the AST, making
dependencies more salient and easier to learn.

As for the CFG-based source code model, there are
two factors that signiﬁcantly aﬀect the following pro-
gram analysis with deep learning. One is the way of
representing of basic blocks; the other is the role of the
graph. To be speciﬁc, several works have tried diﬀer-
ent way to basic blocks in deep learning, e.g., assem-
bly instruction [15], Bag-of-Words model [22, 21] and
line number [18]. As for the graph, it can be utilized
as a leading role [15, 22, 21] or an auxiliary role [18]
during the analysis. For example, Wang et al. [21]
used graph as a leading role and represents basic blocks
with Bag-of-Words model composed of AST’s gram-
matical nodes. Our model similarly adopted graph as a
leading role, but represented basic blocks with the cor-
responding subtree of AST. We retained the structure
of AST, which helped us better represent the context-
independent grammatical diﬀerences than other models.

6.2. Program Classiﬁcation

Program classiﬁcation, i.e., distinguishing and classi-
fying programs by some features from various aspects,
is one of the basic software engineering tasks. For
example, as one of the applications, functional code
clone detection [12, 22, 49] is to determine whether two
code snippets implement the same functionality.
It is
achieved by classifying the functional features of the
given program. Except from functional features [9, 12],
language features [50], defect features [11, 4, 15] and
structure features [51] are also widely adopted by pro-
gram classiﬁcation tasks. In this paper, we decided to
apply defect features on classifying program test results.
Though Phan et al. [15] have done this task before, the
size of dataset and code complexity were relatively lim-
ited compared to ours, which were collected and con-
structed by crawlers and huge manual eﬀorts.

6.3. Software Defect Prediction

Software defect prediction is a challenging task that
has been researched extensively. Prior to the rise of

deep learning, researchers have adopted machine learn-
ing to achieve such a goal [41, 52, 53, 54, 55, 56, 57,
58, 59, 60]. However, these techniques require fea-
ture engineering that is normally time- and resource-
consuming. For example, Xing et al. [59] proposed a
SVM-based defect predictin methods, which depends
on both software change metrics and software com-
plexity metrics. Deep learning techniques eliminated
the process of feature engineering, and researchers be-
gan focusing on improving prediction performance us-
ing suitable source code models [52, 4, 35, 11]. Exist-
ing works have pointed out that the source code model
needs contextual dependencies [61] and should be able
to distinguish subtle changes [4]. Both of them were
taken into account in our method. Speciﬁcally, the con-
textual dependencies comes from the ECFG; and the
subtle changes, i.e., subtle grammatical diﬀerences, are
represented by the structural diﬀerences of the AST. To
the best of our knowledge, no other existing source code
models have achieved both of these goals.

7. Conclusion

In this paper, we have proposed a new source code
model based on ECFG and an attention-based model,
namely MFGNN. Our source code model restricts the
order in which MFGNN extracts features, and makes it
more eﬃcient and eﬀective for MFGNN to obtain pro-
gram features. Moreover, we have evaluated MFGNN
on three practical tasks: program classiﬁcation, soft-
ware defect prediction and code clone detection. The
results showed that MFGNN signiﬁcantly outperformed
baseline methods. For example, compared with the
well-known source code model code2seq [14], the scale
of parameters decreased more than 30-fold while the
overall accuracy was increased by 26.0%. Our research
illustrated that the performance heavily depended on
the construction of source code model. Additionally,
we highlights a few research directions for future work,
e.g., applying our method on more general real-life
projects and improving the graph and MFGNN for bet-
ter performance.

Acknowledgement

We thank anonymous reviewers for their thought-
Thanks to Ningyu He for proof-
ful comments.
reading the manuscript. This research is supported
by the National Key R&D Program of China un-
der Grant No. 2020AAA0109400, the National Nat-
ural Science Foundation of China under Grant Nos.

15

/ Journal of Systems and Software 00 (2021) 1–17

16

62072007, 61832009, 61620106007, 61502011,
the
Australian Research Council Discovery Project (Grant
No. DP210102447), and ”the Fundamental Research
Funds for the Central Universities”(BLX202003).

References

[1] K. Wang, Z. Su, Learning blended, precise semantic program

embeddings, ArXiv abs/1907.02136 (2019).

[2] J. Ott, A. Atchison, P. Harnack, N. Best, H. Anderson, C. Fir-
mani, E. Linstead, Learning lexical features of programming
languages from imagery using convolutional neural networks,
in: Proceedings of the 26th Conference on Program Compre-
hension, 2018, pp. 336–339.

[3] G. Frantzeskou, S. MacDonell, E. Stamatatos, S. Gritzalis, Ex-
amining the signiﬁcance of high-level programming features in
source code author classiﬁcation, Journal of Systems and Soft-
ware 81 (3) (2008) 447–460.

[4] S. Wang, T. Liu, L. Tan, Automatically learning semantic fea-
tures for defect prediction, in: 2016 IEEE/ACM 38th Interna-
tional Conference on Software Engineering (ICSE), IEEE, 2016,
pp. 297–308.

[5] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, K. Matsumoto,
An empirical comparison of model validation techniques for de-
fect prediction models, IEEE Transactions on Software Engi-
neering 43 (1) (2016) 1–18.

[6] X. Hu, G. Li, X. Xia, D. Lo, Z. Jin, Deep code comment gener-
ation, in: Proceedings of the 26th Conference on Program Com-
prehension, 2018, pp. 200–210.

[7] Z. Yao, J. R. Peddamail, H. Sun, Coacor: code annotation for
code retrieval with reinforcement learning, in: The World Wide
Web Conference, 2019, pp. 2203–2214.

[8] A. LeClair, S. Jiang, C. McMillan, A neural model for gener-
ating natural language summaries of program subroutines, in:
2019 IEEE/ACM 41st International Conference on Software En-
gineering (ICSE), IEEE, 2019, pp. 795–806.

[9] L. Mou, G. Li, L. Zhang, T. Wang, Z. Jin, Convolutional neural
networks over tree structures for programming language pro-
cessing, in: Thirtieth AAAI Conference on Artiﬁcial Intelli-
gence, 2016.

[10] M. White, M. Tufano, C. Vendome, D. Poshyvanyk, Deep learn-
ing code fragments for code clone detection, in: 2016 31st
IEEE/ACM International Conference on Automated Software
Engineering (ASE), IEEE, 2016, pp. 87–98.

[11] K. H. Dam, T. Pham, S. W. Ng, T. Tran, J. C. Grundy, A. K.
Ghose, T. Kim, C.-J. Kim, Lessons learned from using a deep
tree-based model for software defect prediction in practice, 2019
IEEE/ACM 16th International Conference on Mining Software
Repositories (MSR) (2019) 46–57.

[12] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, X. Liu, A novel
neural source code representation based on abstract syntax tree,
in: 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE), IEEE, 2019, pp. 783–794.

[13] U. Alon, M. Zilberstein, O. Levy, E. Yahav, Code2Vec: Learn-
ing Distributed Representations of Code, Proceedings of the
ACM on Programming Languages 3 (POPL) (2019) 1–29.
arXiv:1803.09473, doi:10.1145/3290353.

[14] U. Alon, O. Levy, E. Yahav, code2seq: Generating sequences
from structured representations of code, CoRR abs/1808.01400
(2018). arXiv:1808.01400.
URL http://arxiv.org/abs/1808.01400

[15] A. Phan, L. Nguyen, Y. Nguyen, L. Bui, Dgcnn: A convo-
lutional neural network over large-scale labeled graphs, Neu-

ral Networks 108 (09 2018). doi:10.1016/j.neunet.2018.
09.001.

[16] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White,
D. Poshyvanyk, Deep learning similarities from diﬀerent rep-
resentations of source code, in: 2018 IEEE/ACM 15th Inter-
national Conference on Mining Software Repositories (MSR),
2018, pp. 542–553.

[17] M. Allamanis, M. Brockschmidt, M. Khademi, Learning to rep-
resent programs with graphs, arXiv preprint arXiv:1711.00740
(2017).

[18] Y. Li, S. Wang, T. N. Nguyen, S. V. Nguyen, Improving bug
detection via context-based code representation learning and
attention-based neural networks, Proceedings of the ACM on
Programming Languages 3 (2019) 1–30.

[19] U. Alon, M. Zilberstein, O. Levy, E. Yahav, A general path-
based representation for predicting program properties, Pro-
ceedings of the 39th ACM SIGPLAN Conference on Program-
ming Language Design and Implementation (2018).

[20] H. Zhong, H. Mei, Learning a graph-based classiﬁer for fault

localization, 2019.

[21] Y. Wang, F. Gao, L. Wang, K. Wang, Learning Semantic Pro-
gram Embeddings with Graph Interval Neural Network 1 (Jan-
uary) (2020). arXiv:2005.09997.
URL http://arxiv.org/abs/2005.09997

[22] C. Fang, Z. Liu, Y. Shi, J. Huang, Q. Shi, Functional code
clone detection with syntax and semantics fusion learning, IS-
STA 2020 - Proceedings of the 29th ACM SIGSOFT Inter-
national Symposium on Software Testing and Analysis (2020)
516–527doi:10.1145/3395363.3397362.

[23] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li,
M. Sun, Graph neural networks: A review of methods and ap-
plications, arXiv preprint arXiv:1812.08434 (2018).

[24] W. Hamilton, Z. Ying, J. Leskovec, Inductive representation
learning on large graphs, in: Advances in neural information
processing systems, 2017, pp. 1024–1034.

[25] A. Fout, J. Byrd, B. Shariat, A. Ben-Hur, Protein interface pre-
diction using graph convolutional networks, in: Advances in
neural information processing systems, 2017, pp. 6530–6539.

[26] T. Hamaguchi, H. Oiwa, M. Shimbo, Y. Matsumoto, Knowledge
transfer for out-of-knowledge-base entities: A graph neural net-
work approach, arXiv preprint arXiv:1706.05674 (2017).
[27] J. Bruna, W. Zaremba, A. Szlam, Y. LeCun, Spectral networks
and locally connected networks on graphs, in: 2nd International
Conference on Learning Representations, ICLR 2014, Banﬀ,
AB, Canada, April 14-16, 2014, Conference Track Proceedings,
2014.
URL http://arxiv.org/abs/1312.6203

[28] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero,
P. Lio, Y. Bengio, Graph attention networks, arXiv preprint
arXiv:1710.10903 (2017).

[29] M. Cvitkovic, B. Singh, A. Anandkumar, Open Vocabulary
Learning on Source Code with a Graph-Structured Cache
(2018). arXiv:1810.08305.
URL http://arxiv.org/abs/1810.08305
Banh-mi,

https://codeforces.com/

[30] Codeforces,

problemset/problem/1062/C.

[31] Codeforces,

Journey,

https://codeforces.com/

problemset/problem/721/C.

[32] Codeforces,

Socks,

https://codeforces.com/

problemset/problem/731/C.

[33] Codeforces, Arpa’s loud Owf and Mehrdad’s evil plan, https:

//codeforces.com/problemset/problem/742/C.

[34] Codeforces, Hacker, pack your bags!, https://codeforces.

com/problemset/problem/822/C.

[35] J. Chen, K. Hu, Y. Yu, Z. Chen, Q. Xuan, Y. Liu, V. Filkov,

16

/ Journal of Systems and Software 00 (2021) 1–17

17

ing techniques for design pattern detection, Journal of Systems
and Software 103 (2015) 102–117.

[52] X. Yang, D. Lo, X. Xia, Y. Zhang, J. Sun, Deep learning for just-
in-time defect prediction, in: 2015 IEEE International Confer-
ence on Software Quality, Reliability and Security, IEEE, 2015,
pp. 17–26.

[53] J. Walden, J. Stuckman, R. Scandariato, Predicting vulnerable
components: Software metrics vs text mining, in: 2014 IEEE
25th international symposium on software reliability engineer-
ing, IEEE, 2014, pp. 23–33.

[54] X. Xia, D. Lo, X. Wang, X. Yang, Collective personalized
change classiﬁcation with multiobjective search, IEEE Trans-
actions on Reliability 65 (4) (2016) 1810–1829.

[55] L. Breiman, Random forests, Machine learning 45 (1) (2001)

5–32.

[56] L. C. Briand, W. L. Melo, J. Wust, Assessing the applicability of
fault-proneness models across object-oriented software projects,
IEEE transactions on Software Engineering 28 (7) (2002) 706–
720.

[57] T. M. Khoshgoftaar, D. L. Lanning, A neural network approach
for early detection of program modules having high risk in the
maintenance phase, Journal of Systems and Software 29 (1)
(1995) 85–91.

[58] T. M. Khoshgoftaar, X. Yuan, E. B. Allen, Balancing misclas-
siﬁcation rates in classiﬁcation-tree models of software quality,
Empirical Software Engineering 5 (4) (2000) 313–330.

[59] F. Xing, P. Guo, M. R. Lyu, A novel method for early software
quality prediction based on support vector machine, in: 16th
IEEE International Symposium on Software Reliability Engi-
neering (ISSRE’05), IEEE, 2005, pp. 10–pp.

[60] J. C. Munson, T. M. Khoshgoftaar, The detection of fault-prone
programs, IEEE transactions on Software Engineering 26 (5)
(1992) 423–433.

[61] J. Li, P. He, J. Zhu, M. R. Lyu, Software defect prediction via
convolutional neural network, 2017 IEEE International Confer-
ence on Software Quality, Reliability and Security (QRS) (2017)
318–328.

Software Visualization and Deep Transfer Learning for Eﬀective
Software Defect Prediction (2020) 578–589.

[36] M. Niepert, M. Ahmed, K. Kutzkov, Learning convolutional
neural networks for graphs, in: International conference on ma-
chine learning, 2016, pp. 2014–2023.

[37] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learn-
ing with neural networks, CoRR abs/1409.3215 (2014). arXiv:
1409.3215.
URL http://arxiv.org/abs/1409.3215

[38] Y. Li, D. Tarlow, M. Brockschmidt, R. Zemel, Gated graph
sequence neural networks, arXiv preprint arXiv:1511.05493
(2015).

[39] T. Menzies, J. Greenwald, A. Frank, Data mining static code
attributes to learn defect predictors., IEEE Trans. Software Eng.
33 (2007) 2–13. doi:10.1109/TSE.2007.10.

[40] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, A. B.
Bener, Defect prediction from static code features: current re-
sults, limitations, new approaches, Autom. Softw. Eng. 17 (4)
(2010) 375–407. doi:10.1007/s10515-010-0069-5.
URL https://doi.org/10.1007/s10515-010-0069-5

[41] J. Nam, S. J. Pan, S. Kim, Transfer defect learning, in: 2013
35th international conference on software engineering (ICSE),
IEEE, 2013, pp. 382–391.

[42] Maying, LuoGuangchun, Zengxue, ChenAiguo, Transfer learn-
ing for cross-company software defect prediction, Information
& Software Technology (2012).

[43] J. Ferrante, K. J. Ottenstein, J. D. Warren, The program depen-
dence graph and its use in optimization, ACM Trans. Program.
Lang. Syst. 9 (3) (1987) 319–349.
doi:10.1145/24039.
24041.
URL https://doi.org/10.1145/24039.24041

[44] L. Jiang, G. Misherghi, Z. Su, S. Glondu, Deckard: Scalable
and accurate tree-based detection of code clones, in: 29th Inter-
national Conference on Software Engineering (ICSE’07), 2007,
pp. 96–105. doi:10.1109/ICSE.2007.30.

[45] H.-H. Wei, M. Li, Supervised deep features for software func-
tional clone detection by exploiting lexical and syntactical in-
formation in source code, in: Proceedings of the 26th Inter-
national Joint Conference on Artiﬁcial Intelligence, IJCAI’17,
AAAI Press, 2017, p. 3034–3040.

[46] G. Zhao, J. Huang, Deepsim: Deep learning code functional
similarity, in: Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/FSE 2018,
Association for Computing Machinery, New York, NY, USA,
2018, p. 141–151. doi:10.1145/3236024.3236068.
URL https://doi.org/10.1145/3236024.3236068
[47] Y. Liu, H. T. Loh, A. Sun, Imbalanced text classiﬁcation: A term
weighting approach, Expert systems with Applications 36 (1)
(2009) 690–701.

[48] L. Jiang, H. Liu, H. Jiang, Machine learning based recommen-
dation of method names: How far are we, Proceedings - 2019
34th IEEE/ACM International Conference on Automated Soft-
ware Engineering, ASE 2019 (2019) 602–614doi:10.1109/
ASE.2019.00062.

[49] H. Yu, W. Lam, L. Chen, G. Li, T. Xie, Q. Wang, Neural
detection of semantic code clones via tree-based convolution,
in: 2019 IEEE/ACM 27th International Conference on Pro-
gram Comprehension (ICPC), 2019, pp. 70–80. doi:10.1109/
ICPC.2019.00021.

[50] S. Ugurel, R. Krovetz, C. L. Giles, What’s the code? automatic
classiﬁcation of source code archives, in: Proceedings of the
eighth ACM SIGKDD international conference on Knowledge
discovery and data mining, 2002, pp. 632–638.

[51] M. Zanoni, F. A. Fontana, F. Stella, On applying machine learn-

17

