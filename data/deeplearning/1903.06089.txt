9
1
0
2

r
a

M
5
1

]
E
S
.
s
c
[

2
v
9
8
0
6
0
.
3
0
9
1
:
v
i
X
r
a

Are My Invariants Valid? A Learning Approach

Vincent J. Hellendoorn
vhellendoorn@ucdavis.edu
UC Davis
Davis, California

Oleksandr Polozov
polozov@microsoft.com
Microsoft Research
Redmond, Washington

Premkumar T. Devanbu
ptdevanbu@ucdavis.edu
UC Davis
Davis, California

Mark Marron
marron@microsoft.com
Microsoft Research
Redmond, Washington

ABSTRACT
Ensuring that a program operates correctly is a diﬃcult task in
large, complex systems. Enshrining invariants – desired proper-
ties of correct execution – in code or comments can support main-
tainability and help sustain correctness. Tools that can automati-
cally infer and recommend invariants can thus be very beneﬁcial.
However, current invariant-suggesting tools, such as Daikon, suf-
fer from high rates of false positives, in part because they only
leverage traced program values from available test cases, rather
than directly exploiting knowledge of the source code per se. We
propose a machine-learning approach to judging the validity of
invariants, speciﬁcally of method pre- and post-conditions, based
directly on a method’s source code. We introduce a new, scalable
approach to creating labeled invariants: using programs with large
test-suites, we generate Daikon invariants using traces from sub-
sets of these test-suites, and then label these as valid/invalid by
cross-validating them with held-out tests. This process induces a
large set of labels that provide a form of noisy supervision, which is
then used to train a deep neural model, based on gated graph neu-
ral networks. Our model learns to map the lexical, syntactic, and
semantic structure of a given method’s body into a probability that
a candidate pre- or post-condition on that method’s body is correct
and is able to accurately label invariants based on the noisy signal,
even in cross-project settings. Most importantly, it performs well
on a hand-curated dataset of invariants.

1 INTRODUCTION
As software projects grow, their behavior gets harder to under-
stand. Unexpected run-time values, such as an index falling out-
side array bounds, can lead to crashes and breaches of security.
For this reason, developers often formalize invariants: statements
about the domains of values, and the relationships between them,
that are expected during execution of the program. Whether about
loops, functions or classes, good invariants have many uses, includ-
ing improving maintainability, complementing test suites, simpli-
fying debugging, and, if inserted into the code, detecting run-time
violations to ensure that faulty code “fails fast."

Because invariants can be powerful safeguards, tools that can
recommend useful invariants can be a boon to programmers. Daikon
was introduced as a landmark tool that can suggest invariants based
on run-time traces [14]. It works by running a program’s test suite
(or realistic workloads) and tracking the values in the program to
conjecture invariants about its variables and their relationships.

This can include very speciﬁc statements, such as “x is only stored
once in this array"; if an invariant has enough supporting observa-
tions, it is presented to a programmer. Unfortunately, these invari-
ants often do not hold in practice because of the incompleteness
of trace data; we ﬁnd that many of Daikon’s invariants are false
positives, especially its more complex statements.

A solution may lie in learning-based methods: a growing body
of work has shown that program properties can be inferred with
surprising accuracy from source code by supervised learners. For
instance, machine learners and neural networks are able to accu-
rately infer types for untyped code (such as JavaScript) based on
just the code syntax [16, 28, 30]. That this is possible can be ex-
plained by the high degree of “naturalness” in real-world code [18]:
due to the complexity of software development, developers tend to
rely on highly repetitive, conventional patterns; this in turn allows
the intended meaning to be easily inferred from the code itself. Ex-
amples of this phenomenon are the use of common identiﬁers and
idioms [7, 10]; these patterns can be readily mined and exploited
with natural language inspired models for a wide range of tasks
[7, 8, 16, 27, 29], all using learned models that extract rich informa-
tion from the code and/or its documentation.

We conjecture in this paper that this can extend to program be-
havior as well: expectations about the behavior of methods can
be inferred from the syntax of the method itself. However, while
training data for variable names and types can be obtained at scale,
correct invariants are diﬃcult to obtain; they are fairly uncommon
in code and using automated reasoning methods [32] for labeling
is not a very scalable proposition. At the same time, the task of
learning to label invariants given source code is a complex one,
requiring a powerful learner; therefore, to avoid over-ﬁtting, we
need lots of data. Thus, a learning approach is caught in a bind.

We introduce a novel approach out of this quandary, by creating
a large (but noisy) labeled dataset: we feed traces from subsets of
test-suites into Daikon to produce candidate invariants and then la-
bel these using the full test sets. While this approach is admittedly
unsound, it is a way to obtain labels at scale and could provide a
suﬃcient training signal to train a powerful model, which can then
be tested on real, manually-curated data.

Our contributions are:

(1) We conduct a detailed manual evaluation of the validity of
Daikon’s invariants on a diverse set of projects, showing
that most of its invariants are false positives.

 
 
 
 
 
 
(2) We propose a method for extracting both (likely) valid and
invalid (but realistic) invariants at scale using a test suite,
which can serve as a noisy, but useful training signal.
(3) We introduce a graph-based neural validator that is able to
accurately discern which invariants are valid and analyze
the characteristics of its performance.

(4) We show that our model achieves both high ROC scores on
our mined invariant corpus and is able to accurately rank
Daikon’s invariants on our manually annotated dataset, con-
trary to several compared models.

2 MOTIVATION
Much of a developer’s time is spent reading code and reasoning
about its run-time behavior. Invariants aim to simplify this task
by describing expectations about the values used in a program,
both in isolation (e.g. index >= 0, or other != null) and rel-
ative to each other (e.g. end >= start, or end == arr.Count).
The most commonly used invariants, method pre- and post-condi-
tions, specify the program’s state before and after a method’s in-
vocation respectively. Pre-conditions capture the expected state of
the method’s parameters and any relevant program state, whereas
post-conditions reﬂect how this state may have changed, as well
as properties of the method’s result.

Writing invariants, though, is tedious and often omitted. As such,
tools like Daikon oﬀer a promising solution: automatically infer-
ring method pre- and post-conditions just by executing a test suite
and monitoring the values in a program. This approach is, however,
inadequate in practice: Kim & Petersen analyzed Daikon’s behav-
ior on several programs and found its generated invariants mostly
trivial or incorrect [19]. In other words, Daikon suﬀers from a high
degree of false positives.

To better understand why, we decided to manually inspect in-
variants on real programs; we sampled 35 methods across seven
large projects on Github and used Daikon to infer nearly 500 pre-
and post-conditions for them. We then had two raters hand-annotate
these invariants (our precise methodology is detailed in Section 3.2).
Among others, we found that only 30% of the presented invariants
were valid; 40% were incorrect entirely and another 30% were in no
way relevant to the method.1 However, the real story lies beyond
the numbers: our analysis painted a complex picture of invariants
in modern software engineering.

Listing 1: Method example from Metrics.Net [3]

private AtomicLong counter = new AtomicLong();
public void Increment(long value) {

this.counter.Add(value);

}

For one, data sparsity can lead to bad invariants. Consider the
method Increment in Listing 1 (shown with the associate ﬁeld for
reference); one inferred pre-condition is this.counter != null,
which is quite appropriate. However, Daikon also inferred a pre-
condition this.counter.value == 0, which is clearly an arti-
fact of the test suite: Increment is only called once per AtomicLong
instance (despite being used many times in the whole test suite).

1This is after removing invariants on system constants, which spanned over 20% of
initial invariants, see Section 3.2.

Hellendoorn et al.

Consequently, Daikon also infers this.counter.value == value
as a post-condition, adding another false positive.

Listing 2: Method example from Mongo-C#-Driver [4]

public int CompareTo(ElectionId other) {

if (other == null) return 1;
return this._id.CompareTo(other._id);

}

Secondly, we found that the discovery of valid invariants alone
was often not the most appropriate criterion. Consider the method
in Listing 2, which implements a conventional comparator method
(for sorting object types). Here, the obvious (and only reasonable)
pre-condition is this._id != null, and Daikon does indeed in-
fer this. Unfortunately, it also infers eight other related invariants,
such as this._id._increment == this._id._timestamp. Al-
though all of these ﬁelds are associated with this method (since
CompareTo is invoked on this._id), presenting these is in no
way helpful to a programmer. This is neither a matter of validity
nor of relatedness to the method; Daikon often suggested method-
related, valid invariants that were by no means useful. A common
example was a post-condition that asserts that a value had not
changed, when this was clearly true (produced for both _id val-
ues here).

Listing 3: Method example from DotLiquid [1]

public static void RegisterSafeType(

Type type, string[] allowedMembers) {

RegisterSafeType(type, x =>

new DropProxy(x, allowedMembers));

}

Contrary to classical examples of invariants on isolated code
(e.g. stand-alone library classes [26]), our analysis showed that pre-
and post-conditions in real-world methods are often informed by
complicated interactions with their various contexts. However, cap-
turing this complexity via automatic tools appears out of reach.
The code in Listing 3 invokes an overloaded method. Daikon gen-
erates the following pre-condition:
Contract.ForAll(allowedMembers, x =>

x.ToString().OneOf("Name", "ToString"))

meaning that every element in allowedMembers is either “Name”
or “ToString”. Such elaborate invariants are not uncommon for
Daikon; we observed many instances of such “OneOf” checks (with
and without “ForAll”), all invalid, as well as checks relating ele-
ments in arrays with other values or arrays. These invariants con-
stitute Daikon’s attempts at inferring complex relationships within
the code but are nearly always incorrect. Kim & Peterson bring up
a similar issue; they ﬁnd that any semantically meaningful prop-
erties of their programs were almost never recognized. Instead,
Daikon produces a combinatorially large number of invariants on
the relationships across values in the code, with little validity [19].
Even in these simple methods, a complex picture of invariants
emerged that highlights several major challenges to useful invari-
ant inference: (1) understanding which invariants are likely to gen-
eralize, (2) recognizing which invariants are relevant, and (3) infer-
ring semantically appropriate invariants. One might argue that the
primary issue in all these cases is that the test suite did not capture

Are My Invariants Valid? A Learning Approach

Project

Core Code

Core Tests

Instrument

Preprocess

Project 
Selection

Traced Methods

Align

Valid + Invalid 
Samples

Cross-validate

Test split

Test split

Test split

Split invariants

Split invariants

Split invariants

Execute 
& Infer

Figure 1: Schematic overview of our approach to mining
valid and invalid method-aligned invariants

suﬃciently diverse uses of these methods, but in modern practice
this will always be the case: our programs are all well-tested; the
problem is that program state on such large programs is far too
complex to adequately capture based on just executions. We argue
that the heart of the issue is that Daikon infers its invariants on
trace data alone and has no mechanism in place to validate whether
these invariants make sense given the source code. We propose to
use deep neural networks to extract semantic insights from source
code in order to complement invariants extracted from trace data.
Here we speciﬁcally focus on the ﬁrst challenge above: judging
the validity of a method’s pre- and post-condition from its syntax.
Our models and results show great promise at this task, suggest-
ing that the other challenges may also be in reach. More generally,
our work demonstrates that a great deal of run-time information
can be found in the code itself and can statically be extracted by
suﬃciently intelligent models.

3 APPROACH
Our goal is to collect a large corpus of methods with their asso-
ciated pre- and post-conditions, including both valid and invalid
examples. This required a number of steps, each with substantial
hurdles to overcome, which are shown in Figure 1 and discussed
in this section. In addition, we document the process by which we
manually curated a “golden” dataset and the metrics used in our
study.

3.1 Mining Invariants
We aim to rank pre- and post-conditions produced by tools such
as Daikon based on their estimated validity. Any model that can
learn to do so requires a corpus of methods aligned with good ex-
amples of both valid and invalid invariants. The latter category
poses a challenge: tools like Daikon produce invariants by running
a program’s test suite, producing only statements that are valid
across this test suite. We propose a simple solution based on cross-
validation: we sample a subset of test cases and infer invariants on
trace data obtained by running this subset. We repeat this process
many times and compare the invariants obtained on each random
subset; invariants that hold on all of these must hold on the full test
suite as well, whereas invariants that hold on only some of the test
sets are invalid. Crucially, these invalid invariants are nonetheless
plausible, as they must have had suﬃcient support to occur within
some test sets. This section details the speciﬁcs of our data gather-
ing process.

Table 1: The projects used in this study and the character-
istics of the invariant-related data extracted from them. We
separately extract Pre- and Post-conditions and list the num-
ber of methods (#Mth) and Invariants (#Inv) collected for
each of these.

Project Tests

DotLiquid
LibGit2Sharp
Logging-Log4Net
Lucene.Net
MathNet-Numerics
Metrics.Net
Mongo-C#-Driver
Nustache
Total

356
745
184
2,585
2,005
168
1,083
146
7,272

Pre-Cond.
#Mth #Inv
1,046
136
12,709
572
7,120
395
4,379
159
9,637
880
2,598
144
4,211
187
1,611
60
43,311
2,533

Post-Cond.
#Mth #Inv
1,277
130
13,222
406
7,841
404
6,864
152
11,817
905
3,532
160
5,252
197
1,781
63
51,586
2,417

3.1.1 Data Source. Since we rely on executing unit tests for our
pre- and post-condition mining, we require a suﬃciently large and
diverse corpus of runnable projects with adequate test coverage.
An appropriate dataset was mined in recent work on loop idioms,
which collected a corpus of 11 C# projects with diverse functions,
for which they were able to run the test suite [6]. We cloned the
corresponding Git repositories and reverted them to the same com-
mit mentioned in that work. Each of these projects (or, in C# terms,
“solutions”) is comprised of several sub-projects, generally includ-
ing at least one “core” module and one primary test project with
tests for the core functionality; we focus on these two.

As part of our data extraction, we need to be able to execute
a random subset of the test suite repeatedly. Common test suite
toolkits do not support such functionality; instead, we implemented
a crawler that extracts test cases and generates code to invoke
a subset of them (including any necessary setup/teardown code).
Not all projects in our dataset were amenable to this process; in
total, we were able to instrument eight projects in this way, which
are detailed in Table 1. Since we only focus on core project tests,
manually extract test invocations, and execute a reduced number
of projects, our dataset does not span as many test cases as prior
work’s [6]. Nevertheless, we were able to extract ca. 43K pre- and
51K post-conditions, each aligned with some 2.5K methods that
average around 100 tokens, producing a dataset that is adequately
large for training a deep neural network.

3.1.2 Trace Extraction. To extract trace data from C# DLL execu-
tions, we forked and extended Celeriac, a custom Daikon front-
end for C#. It instruments every method at its entrance and (each
possible) exit by storing the value of every variable in scope, ei-
ther in terms of an object address (if applicable), or the raw data
value (for primitives, arrays, etc.). For each project, this front-end
was used to (1) instrument the core-project’s DLL on which the
test project relied to trace all its data, and (2) run the test exe-
cutable many times, writing trace ﬁles to a designated location.2
To reduce the tracing overhead on very frequently used methods,

2Some test methods got traced as well; we simply discarded these.

we switch to method call sampling on them, which uses exponen-
tial back-oﬀ in the tracing frequency for each method. Speciﬁcally,
the ﬁrst 10 calls are always traced, followed by sampling of calls
20, 30, ... , 100, 200, ..., 1000, 2000, etc.

3.1.3 Extracting Invariants. Our next goal is to extract invariants
that are “scored” based on their cross-validity between traces. Here,
we ﬁrst (separately) inferred method pre- and post-conditions on
each trace ﬁle and then cross-validated each with the other trace
ﬁles. To illustrate how this works, consider a method abs(val)
that returns the absolute value of its parameter, and an appropri-
ate post-condition for this method: val >= 0. If this method call
is observed in 25 traces and the invariant val >= 0 is supported
by each of them, it is considered “valid”. As a consequence, any in-
variant marked that was supported by the full test suite will also
be marked as “valid” in our cross-validation.3 What if, due to lim-
itations of the test suite, ﬁve of the trace ﬁles instead produced
the invariant val >= 1 (i.e. it was never invoked with val = 0)?
This invariant does support the weaker post-condition val >= 0,
but not vice versa; we would thus assign it a validity score of 0.2
(5/25) and the corresponding label “invalid”.

This “invalid” post-condition is actually a very useful training
sample to our model, for an important reason: invalid examples
such as this one provide examples of plausible yet false invariants
with respect to the project’s test suite. This matters because many
of the invariants marked as “valid” in our method are not actu-
ally so; they are only “valid” modulo the quality of the project’s
test suite. We cannot train a model to learn to judge these as valid
without good counter-examples, because it would become far too
lenient and not generalize well; randomly generated invalid invari-
ants thus won’t do. In contrast, we argue that extracting exam-
ples of invalid statements as we do simulates the inadequacy
of the test suite. This should allow a good model to extrapolate to
recognize invariants that are marked as valid but are probably not
(as we shall see in Section 5.3) and is a contribution of our work.

We instrument each project and run 10% of its test cases a to-
tal of 100 times, storing a trace ﬁle for each run. Ideally, at this
point we would run Daikon on every trace ﬁle and cross-validate
its invariants; it supports an “InvariantChecker” mode that cross-
references invariants with a collection of trace ﬁles, appropriate
for our needs. In practice, however, running Daikon this often was
prohibitively slow on larger projects (e.g. one run on Lucene took
over 5 hours), and, more importantly, Daikon’s most complex (and
error-prone) invariants were also most likely to not be marked “in-
valid” simply due to data sparsity. Rather than rely on extensive
ﬁltering of Daikon’s invariants, we implemented a simple engine
that infers a subset of its language of invariants that we found most
likely to be valid in our manual annotation. It infers the following
invariants:

• Nullity checks on objects.
• String equality checks
• Numerical constraints: equality and upper/lower-bounds, rel-
ative only to common numbers: {−1, 0, 1}, 2n and 2n − 1 for
n ≥ 4 and 10n for n ≥ 2.

3It is a remote possibility that some test case is not selected in any split, but the odds
of this are less than three in 100,000.

Hellendoorn et al.

• For arrays: all of the above if they hold true for every ele-
ment in an array (and nullity also if any element is null) and
numerical constraints on its length (same values as above).
• Relational hypotheses: equality between objects, whether
an object is contained in an array and relations between two
numerical values (where one may be an array length).

Our tool is able to rapidly process trace ﬁles, both creating invari-
ants on each split and globally cross-validated invariants (for meth-
ods that occurred in at least 10 splits).

3.2 Golden Data
We also produce a trace ﬁle based on executing every test in a
project’s test suite. This trace ﬁle can be provided to Daikon for
it to infer invariants on. Due to the aged nature of the front-end
we used, we had to recompile Daikon from source with added er-
ror handling in order to process most of the resulting trace ﬁles;
only Lucene could not be run at all.4 From the resulting invariants,
we selected ﬁve methods at random that had at least two pre- and
post-conditions for each project, yielding 217 pre-conditions and
282 post-conditions.

We manually labeled these invariants by inspecting the corre-

sponding code. Our study design was as follows:

Raters: We had access to four raters, each of whom had at least
ﬁve years of experience in software development and familiarity
with C#.5 Each method was randomly assigned to two of our raters
to separately assess all its pre- and post-conditions.

Inspection: Every method was considered in the context of its
documentation (if present) and any reasonably related code, in-
cluding ﬁelds of the surrounding class, signature of related types,
methods that invoked it, and methods that it itself invoked. Each
invariant was annotated as either “valid”, “invalid”, or “irrelevant”
according to the following criteria.

Irrelevant: Any invariant that was not in any way related to any
variable that played a role in the method (ﬁeld, parameter or local)
was marked as “irrelevant”. This includes many class invariants
that were unaﬀected by the method, but not e.g. invariants on ﬁelds
of objects that were used in the method.

Invalid: Any relevant invariant was marked as “invalid” only if
there was convincing evidence that it did not hold. This includes
many cases of test suite sparsity (some shown in Section 2), such
as arrays only ever containing one element, or a parameter object
never being set to null despite this being clearly allowed,6 as well as
overly complex invariants that were incorrect based on inspection
of the semantics of the method and class.

Valid: The remaining valid invariants are likely to be a slight
overestimate, but contain many reasonable checks. Virtually all of
these were very simple, such as nullity checks and comparisons
with simple numerals. We additionally marked many (∼17%) of

4Typical issues included method traces not matching their signature; Daikon did not
provide an option to skip such cases.
5One rater had limited experience with C# but extensive experience with Java and C,
which are closely related.
6i.e., it is either documented as such, marked as nullable, or checked for nullity in the
method body

Are My Invariants Valid? A Learning Approach

Table 2: Validity statistics of Daikon’s inferred invariants on
random method sampled from the studied projects.

Irrelevant
Invalid
Valid

Overall
25.7%
42.6%
31.7%

Pre-conditions
23.3%
50.2%
26.5%

Post-conditions
27.5%
36.8%
35.7%

these as “not useful,” when the invariant stated a property that was
in no way relevant to the semantics of the method. An example of
such an invariant was given in Section 2; nearly all of these were
post-conditions asserting that a value that was not aﬀected by the
method had indeed not changed. As this is a more subjective call,
we do not include this judgement in our analysis.

Conﬂicts: After every rater had made their pass, some 25 invari-
ants were given conﬂicting ratings. Many of these were found to
be due to the complicated nature of identifying an invariant as
“(ir)relevant”. In response, we adopted the aforementioned guide-
line regarding “irrelevant” invariants, i.e. that an invariant is rele-
vant if it contains any reference to a value used in any way in the
method (including, e.g. to a ﬁeld of an object used in the method),
thus preferring not to assign “irrelevant” unless absolutely appro-
priate. This resolved most of the conﬂicts and the guidelines were
improved to incorporate those decisions. The remaining conﬂicts
were discussed by their two raters in order to establish whether
an objective conclusion could be reached (e.g. if one of the raters
had overlooked a null-check). This left four unresolved conﬂicts,
which were omitted from our dataset.

The resulting corpus will be referred to as our “golden” data. Our
analysis identiﬁed less than one in three invariants as valid; most
others were false positives, either because they were not relevant
at all or because they were invalid. Note that, since we adopted a
low threshold for considering invariants “relevant”, a more strin-
gent analysis would mark some of the “invalid” invariants as “ir-
relevant” instead; this does not aﬀect our “valid” ratings. Note that
we mainly focus on distinguishing between the “valid” and “in-
valid” invariants in our analysis (see Section 5.3), as our model is
not trained to recognize relevancy.

3.3 Metrics
Since our objective is to rank invariants based on their validity, our
primary means of evaluation will be the Receiver Operator Char-
acteristic (ROC) curve. The output of our models is the predicted
probability of an invariant being valid. When analyzing our re-
sults, we can simulate setting a threshold on this probability, above
which we include all predictions. Each threshold will yield a dif-
ferent balance between valid and invalid invariants, where ideally
high threshold yield a very favorable balance of valid to invalid in-
variants. The ROC curve quantiﬁes this trade-oﬀ; it plots the True
Positive Rate (TPR, y-axis) against the False Positive Rate (FPR, x-
axis), to produce a curve that ranges from (0, 0) to (1, 1), where
random guessing yields a straight line between these points.7 The

7A related curve is the Cost-Eﬀectiveness Curve, which instead plots the proportion
of all inspections on the x-axis (so it produces lower AUCs); we evaluated both and
found comparable results, so we present the more common metric.

Area Under Curve (AUC) for the ROC curve is commonly used to
assess how much “lift” an ROC curve achieves compared to ran-
dom guessing, which would yield an area of 50%. We present both
this summary metric and the overall curve; see Figure 3 for an ex-
ample.

We investigate two settings for ranking: project-wide and per-
method. The latter setting computes the ROC value for ranking
invariants for each method that has both valid and invalid invari-
ants and is arguably the most realistic real-world use of our tool.
However, since many methods had only invalid invariants, we also
produce ROC curves across each full project. This corresponds to
a scenario where a developer annotates a project with invariants
without focusing on any particular method.

4 MODEL ARCHITECTURE
A complete analysis of invariant validity requires inspecting the
entire code context of the method under investigation. However,
we argue, relying on prior work [9], that such semantic analysis
can often be circumvented via inspecting the syntactic patterns
of the method’s identiﬁers, control ﬂow, and signature. This is be-
cause source code is dual-medium in nature: developers write it to
communicate some intent simultaneously to the machine (as for-
mal instructions) and to fellow humans (as natural language). We
hypothesize that a model imbued with natural-language process-
ing capabilities is able to extrapolate these syntactic patterns to ac-
curately judge semantic validity of many code properties, includ-
ing invariants. Thus, we restrict ourselves to the method source
code as the sole source of information, providing an already ac-
curate lower bound on the potential ranking performance. Other
promising data sources may be studied in future work (Section 6.2).
Capturing meaningful semantic information even from just a
method requires a powerful model. The average method in our
corpus contained over 100 AST nodes (including leafs), with many
several times larger. SE studies commonly use Recurrent Neural
Networks (RNNs), which digest a sequence of lexical tokens from
left to right (and possibly right vice versa) to emit a representation
for each token. Although these models can learn useful represen-
tations, they often struggle to aggregate data across long distances
that are very common in code [9, 17]. More importantly, they can-
not represent relational and structural properties of source code,
required to infer its semantics accurately.

Graph-based neural networks were proposed as an alternative.
These networks represent source code as a directed graph with
diﬀerent domain-speciﬁc kinds of edges, including edges between
lexically adjacent tokens (which RNNs use), but also syntax tree
edges (between parent and child nodes) and long-distance edges
between related tokens (e.g. data-ﬂow edges). By directly modeling
these relations, information can ﬂow across long distances directly
between semantically relevant locations, mitigating many of the
problems with RNNs. We adopt this model in our work, speciﬁcally
focusing on Gated Graph Neural Networks (GGNNs), which were
shown to learn generalizable, compact representations of source
code [9, 12, 21]; we also include RNNs in our ablation analysis.
For a detailed description of this model, see [9]; we brieﬂy give
an overview of its behavior here.

A GGNN takes as input a graph representation G of the method,
constructed by enriching its AST with additional kinds of edges
between related nodes, such as lexical order edges and data-ﬂow
edges. Every node v ∈ G has a state hv ∈ Rm, its vector represen-
tation. These states are initialized with node embeddings, learned
end-to-end from the nodes’ features (e.g. nonterminal kinds, data
types, and identiﬁers for leaf nodes). Like prior work, we also split
compound identiﬁers by camel-case and underscores and embed
such nodes by summing over the embeddings of their subtokens.
The GGNN performs K phases of message passing to infer the
representations of all nodes used to determine the network’s task
output. In each phase, every node v passes its state to all adjacent
nodes as a message. Every edge hv, kind,wi transforms the passed
message using a global weight matrix, shared for all edges of the
same kind. Finally, every node w aggregates its received messages
and updates its own state based on these using a recurrent cell func-
tion, allowing it to select which part of the received information
to store and which of its own state to preserve. After K ≥ 8 phases
the ﬁnal representations of each node accurately capture their role
in the method, and thus can be used to determine the task output.
In our approach, the GGNN’s inference task is invariant valida-
tion for a given hmethod, invarianti pair. To compute it, we ﬁrst in-
ject the invariant (pre- or post-condition) into the method as appro-
priate. We then perform K message passing phases of the GGNN,
after which we aggregate the hidden states belonging to all nodes
in the invariant. The resulting state is passed through a hidden
linear layer and then projected onto one dimension (with sigmoid
activation), producing a probability judgement. This architecture
can be trained end-to-end using the data described in Section 3.1,
as we outline below.

4.1 Training Setup
We implemented a highly optimized GGNN for source code in Ten-
sorFlow Eager [5], operating on top of method graphs automati-
cally extracted using the C# compiler interface. Our implementa-
tion can process around 350 methods at the time (in minibatches
of up to 40,000 tokens) on a GTX 1080Ti GPU, requiring 3-5 sec-
onds each. We were able to train ten epochs on the full corpus in
around two hours; our full set of experiments completed within a
few days.

For our cross-project setting (our main goal), a model for each
project was trained on all other projects. We held-out a small por-
tion of the test project as validation data to track overﬁtting of
our models; we stress that this was only acceptable because this
is a cross-validation experiment – holding out part of the training
data would yield a very poor estimate of overﬁtting (see intra- vs.
cross-project results in Section 5.1). In practice, our model’s per-
formance tended to saturate on the held-out data within just a few
epochs and overﬁt starting around epoch ﬁve, likely because of the
high degrees of intra-project duplication. We therefore generally
evaluated the models at epoch three, unless stated otherwise.

We trained our model using an Adam optimizer [20] with learn-
ing rate of 0.001. A vocabulary was estimated on the training cor-
pus and words occurring only once there, as well as new words in
the test data, were treated as “<unknown>” tokens. We excluded
methods with more than 500 nodes, which constituted a negligible

Hellendoorn et al.

C
O
R
_
C
U
A

100

75

50

25

0

C
O
R
_
C
U
A

100

75

50

25

0

Cross

Intra

Type

Cross

Intra

Type

Figure 2: Per-method AUC-ROC distribution with their
means for intra-project and cross-project models. Left: rank-
ing pre-conditions, right: ranking post-conditions.

portion of our data. Finally, ablations of the (twenty) edge types
proposed in prior work suggest that just six of these are necessary
for building accurate models (which greatly accelerates training):
lexical adjacency edges (next-token & previous-token), AST edges
(parent & child) and lexical use edges between occurrences of the
same identiﬁer (next-use & last-use, which approximate data-ﬂow
relatedness). We reduced our edge types to just these.

We also trained a conventional RNN model; this model was trained

to encode the method and invariant separately, aggregate each (by
averaging across their tokens) and judge the validity of the invari-
ant by passing the two encodings through a hidden layer and pro-
jecting to a probability judgement (like the GGNN’s discrimina-
tor). We used 300-dimensional embeddings (shared by the method
and invariant encoder) and two 500-dimensional bi-directional en-
coders with GRU units (one for each input); these parameters are
in line with RNNs typically used in modeling source code [16, 17].

5 RESULTS
We start our analysis with the data that we mined at scale using
the methods described under Section 3. Although this data likely
contains a high degree of false positives, it allows us to evaluate
whether our models can extrapolate from the training data, both
within and across projects, as well as the factors that contribute
to their performance (in an ablation study). Finally, we revisit our
manually annotated data to assess the true validity of our models.

5.1 Corpus Data
The simplest case for our models should be to train and test on
methods from the same project. This ensures that the identiﬁer
vernacular at train and test time is highly similar (which is always
beneﬁcial for deep learners [17]) and allows the deep learner to
proﬁt from any repeated invariants within the project (which our
manual analysis suggests is common). This setting is not unreal-
istic: when deployed as a tool, our validator could be trained on
part of a project in order to rank inferred invariants on new meth-
ods or ones with low test coverage. For most practical purposes,
however, cross-project performance is more appropriate; ideally a
model can be pre-trained on our corpus and extend well to unseen
projects. We evaluate both settings.

We show the per-method results for both intra-project and cross-
project invariant ranking in Figure 2. Intra-project predictions are
evidently very easy in our dataset; ROC values of over 90% signal

Are My Invariants Valid? A Learning Approach

near-perfect predictions. This is not to boast of the quality of our
model; it more likely signals a high degree of redundancy in the in-
variants within each project. Although that might impair the per-
formance of a machine learner, our GGNN’s cross-project perfor-
mance is actually quite good; it even perfectly ranks invariants in
a third and a quarter of pre- and post-conditions respectively. The
mean values between pre-conditions (intra: 93.8%, cross: 76.5%)
and post-conditions (intra: 91.9%, cross: 76.3%) were quite similar,
though the latter had a wider spread of success.

A closer look at the per-project performance in terms of ROC
curves is given in Figure 3. Here we rank all invariants in a project
by their projected score (see Section 3.3). Most projects achieve
quite reasonable ROC curves, with AUC values in the range of
75%-82%; only Lucene in pre-condition ranking appeared to pose
a major challenge to our model. These curves also provide more
insight into the practical ranking behavior on these projects; if a
developer were to only inspect a small portion of invariants (not
unlikely given the large number of inferred statements), the left-
most section of each graph shows the trade-oﬀ they would experi-
ence. In many cases, the ranking performance there is substantially
better than on the whole project; especially for Mathnet-Numerics
and Metrics.net, the initial section of this curve is very steep, sug-
gesting that the highest ranked invariants are predominantly valid.
We can quantify this too: at a 25% FPR, random ordering would
produce an AUC of 3.125%, but our models exceed 10% on both
conditions, outperforming random ranking by a factor three; at 5%
FPR, they outperform it by more than ﬁve times.

5.2 Ablations
To get more insight into the quality of our model and the factors
that contribute to its accuracy, we run several ablation studies. One
ablation reduces the complexity of our model: our graph-gated neu-
ral network combines several diﬀerent edge types, including adja-
cent tokens, but also syntax tree edges and edges between occur-
rences of the same identiﬁer. More commonly, work on modeling
code has used Recurrent Neural Networks (RNNs) [16, 35], which
consider only adjacent tokens. We train such a model as well (see
Section 4).

Our second ablation is aimed at better understanding whether
our GGNN actually captures any semantics from the method body.
To evaluate this, we train a GGNN (with the same architecture) that
makes a validity judgement just based on the invariant itself (ig-
noring the method body). If an invariant can be accurately judged
based on just its content, this suggests that our GGNN does not
capture much additional information from the method body, either
because it cannot or because it is often unnecessary. This might
not even be surprising: we found many invariants that were only
weakly related to the method in our analysis. In a sense, our full
model establishes a lower-bound: how much ranking performance
can be achieved with just the limited information that the method
body provides? This ablation helps us better understand this lower-
bound.

This turns out to be an interesting study, for two reasons. First,
the RNN model performs a fair bit worse than the GGNN, which
is to be expected because due to its lesser capacity. However, it
also performs worse than the “No Context” model, which does not

Table 3: Performance of ablations of our model compared
to the full model, expressed in AUC-ROC values for pre-
and post-conditions, both for ranking invariants across
the whole project (averaged over projects) and for ranking
within every method that has at least one valid and invalid
invariant (averaged across all methods).

Model
Full Model

Pre-conditions
Project Method
76.5%
RNN 72.5%
74.0%

80.4%
76.5%
77.9%

No Context

Post-conditions
Project Method
76.3%
68.6%
75.1%

77.9%
73.4%
77.1%

Golden
83.0%
60.0%
60.9%

consider the method at all, despite sharing the same training data
and achieving high training accuracy (meaning the model was able
to learn well). This strongly suggests that the RNN model learns
patterns that poorly generalize. This is not just a matter of overﬁt-
ting; its test performance tended to peak around the third epoch,
when training accuracy was far from convergence. It has been doc-
umented before that GGNNs are better at generalizing from train-
ing data [9], and this ﬁnding conﬁrms that result.8

This brings us to the second ablation: at face value, the “No Con-
text” model performs almost identically to the full GGNNs, losing
by just 2-3% on pre-conditions and 1% on post-conditions. This
would seem rather disappointing; it suggests that the naturalness
of a method betrays very little about an invariant’s validity that is
not already contained in the invariant itself. The small diﬀerence in
AUC-ROC is signiﬁcant and may be beneﬁcial to developers,9 but
would appear to make it hardly worthwhile to train the full model.
Fortunately, however, this result does not accurately reﬂect reality.
The perceptive reader may have noticed the ﬁnal column, which
does paint a stark contrast; we will discuss this next.

5.3 Golden data
All of the aforementioned results were based on the automatically
mined corpus, which we know contains many false positives still.
Training with such ﬂawed data was a deliberate decision: our goal
was not to learn to replicate the labeling, but to recognize when
plausible invariants are likely to be invalid. This is why our invalid
invariants are mined by holding out test suites; we argue that this
makes the model more likely to infer which invariants are valid
just due to sparsity of the test suite. If our deep learner is adequate,
it should be able to recover this signal despite the overwhelming
amounts of noise of invalid invariants being presented as valid at
training time. The end-goal of our learner is thus not to score well
on our automatically mined test data (although it certainly does),
but to perform well in a real-world evaluation. To accurately assess
its performance at this task, we turn to our golden data.

We use our deep learners to score all invariants that we manu-
ally annotated, removing those that were not in the scope of our in-
variant extractor (we omitted learning from certain invariant types

8Partially this may be due to GGNNs better using a limited parameter space; our
GGNNs use embedding and hidden state dimension of 128, whereas the RNN uses
300 & 500, simply because it does not learn good representations with lower values.
9The full model especially outperformed it on Lucene, our hardest project, where the
no-context model never exceeded random AUC-ROC

Hellendoorn et al.

dotliquid

libgit2sharp

logging−log4net

lucenenet

dotliquid

libgit2sharp

logging−log4net

lucenenet

R
P
T

1.00

0.75

0.50

0.25

0.00

1.00

0.75

0.50

0.25

0.00

AUC = 78.08%

AUC = 85.81%

AUC = 80.83%

AUC = 56.90%

mathnet−numerics

metrics.net

mongo−csharp−driver

nustache

AUC = 63.95%

AUC = 78.07%

AUC = 80.49%

AUC = 81.82%

R
P
T

1.00

0.75

0.50

0.25

0.00

1.00

0.75

0.50

0.25

0.00

AUC = 81.54%

AUC = 81.48%

AUC = 80.78%

AUC = 68.42%

mathnet−numerics

metrics.net

mongo−csharp−driver

nustache

AUC = 65.23%

AUC = 70.24%

AUC = 80.03%

AUC = 88.24%

0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.00
FPR

0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.00
FPR

Figure 3: Cross-project ROC curves with their AUC values; grey diagonal line shows expected values for random guessing.
Left: ranking pre-conditions, right: ranking post-conditions

because they were highly unlikely to be valid, see Section 3). The
resulting set contains 37% irrelevant, 26% invalid and 37% valid
invariants, which resembles to our overall data, but with substan-
tially fewer invalid cases (due to our ﬁltering). All invariants la-
beled “valid” in our manual annotations were also valid in our
training data (as should be the case), but false positives abounded:
nearly 30% of the invariants that we manually labeled “invalid”
were nonetheless presented as valid in our test data. The invari-
ants labeled “irrelevant” were all included in the corpus as valid.
Although this may be an accurate assessment, these were entirely
spurious and our model was not trained to recognize relevancy, so
we focus only on distinguishing “valid” from “invalid” invariants.
The ﬁnal column in Table 3 shows the AUC-ROC performance
of our various models at ranking the invariants in this dataset,
where the goal is to rank “valid” invariants above and “invalid” in-
variants. We focus just on ranking the full annotated set here, since
there are too few methods to get reliable per-method ROCs. The
diﬀerence in performance now becomes substantial; the context-
free model barely scores better than random, whereas the full GGNN
achieves quite good performance. The RNN model does not per-
form any better either, oﬀering strong evidence that the complex-
ity of the GGNN architecture is necessary to capture the semantic
cues required for disentangling valid and invalid invariants.

Listing 4: Method example from Logging-Log4Net [2]

protected override void WriteHeader() {
if (m_stream != null) {
...

}

Listing 4 shows a case where the GGNN was able to extract
semantically useful information from a method. In this method,
the ﬁrst statement evaluates the nullity of the m_stream ﬁeld, im-
plying that the inferred pre-condition m_stream != null is cer-
tainly invalid and should not be ranked highly. The no-context
model does not have access to this information and assumes the
typical validity of a nullity check: mostly valid, ranking this in the

top 20% of validity scores (the RNN makes a similar call). The full
GGNN, on the other hand, ranks it in the bottom half of its judge-
ments, well beneath almost all valid invariants. Despite having
seen many contradictory examples at training time (and indeed,
assigning it a fairly high probability), it has learned to recognize
that there is some doubt about its validity given the nullity-check
in the method body (it ranked many valid nullity checks far higher).
In doing so, it has accomplished the goal of our training setup.

5.4 Data Availability
In the interest of open science, we upload a package of our data
(manual annotations and mined graph ﬁles), code (GGNN and RNN
implementations), and output logs (intra-project performance of
GGNN, cross-project performance for all models); available at [https://doi.org/10.5281/zenodo.2574189].
We will turn this into a de-anonymized, full-ﬂedged replication
package when anonymity is lifted.

6 DISCUSSION
In Sections 2 to 5, we discussed each of the four contributions of
this paper: (1) a manually annotated dataset of Daikon invariants
on complex, real programs, (2) an approach to mining plausible
valid and invalid invariants at scale, (3) a novel neural architec-
ture that is able to assess the validity of method pre- and post-
conditions based on the method, and (4) results establishing that
our model is able to train well on our training corpus and uniquely
generalizes to manual annotations. Taking a step back, we now
discuss the implications, validity and possible extensions of this
work.

6.1 Implications
The immediate goal of our model was to rank invariants that are
proposed by a tool such as Daikon (or our subset thereof). Our re-
sults indicate that a developer who uses such a tool to infer method
pre- and post-conditions would be well-served by ﬁrst using our
tool to prioritize the output and ﬁnd invariants that are most likely

Are My Invariants Valid? A Learning Approach

to be valid. This can substantially reduce the proportion of false
positives that are presented to the developer.

More broadly, our work is one in a series of studies that aim to
learn to extract program properties, those which cannot be easily
or even feasibly be extracted by conventional static analyzers (such
as compilers), directly from its source code by exploiting its natu-
ralness. This includes work on inferring types [16, 30], learning
naming conventions [7, 8] and identifying fault-prone regions [9,
29]. Like related work on loop invariants (see Section 7), our work
focuses on properties that describe the behavior of the code, a highly
non-trivial artifact of its syntax, which even professional develop-
ers struggle to accurately assess. Unlike that body of work, we can-
not rely on an SMT solver in the process; our models are stressed
to capture a meaningful semantic signal from just the source code.
We accomplished this by using a combination of a powerful ma-
chine learner and an approach to mining data at scale that is noisy
in a way that actually encouraged our model to generalize. These
innovations help advance our ability to learn program properties.
Finally, deep learning has especially boosted this ﬁeld in recent
years by providing a family of models that can learn to both achieve
state-of-the-art performance in various tasks and learn powerful
representations of source code at the same time. The latter is ar-
guably scientiﬁcally most interesting: much like how latent repre-
sentations in computational linguistics and computer vision have
proven to capture surprisingly rich semantic features in their do-
mains, even from unlabeled data, representations of source code
have the potential to capture much of the semantic insights that
developers embed in their programs through identiﬁers, common
idioms and architecture. As such, models that can successfully ad-
dress complex tasks such as ours likely learn generalizable and
worthwhile representations of the programs they study in the pro-
cess. Our model’s success at generalizing from our training data
to real annotations, despite their very diﬀerent characteristics, is
evidence of that.

6.2 Extensions
An immediate extension of our work is to consider more context:
comments, the surrounding class, and any invoked and invoking
methods (even in other classes) can be highly valuable sources of
information. In many cases, the human annotators were not able
to conﬁdently infer the validity of the invariant from the method
body alone (though it virtually always provides some information).
In addition, many of our studied projects were well-documented;
although this cannot always be relied on, studying this documen-
tation with deep learning can be highly beneﬁcial [27, 28].

Further study of the appropriate models for our task may be
beneﬁcial. We considered several simpler ablations that were less
successful, but more complex (or entirely diﬀerent) architectures
may perform better than ours. In addition, it may be possible to
integrate trace data values directly into the model; although this
slightly changes the task from ours, it may not be unrealistic to
already have some tests in place for a method that a developer
wants to annotate. Such an extension could provide a valuable step
forward in modeling the state of programs directly [34].

6.3 Threats
Our work required a substantial number of design decisions and
solutions, which are important to restate in relation to both its in-
ternal and external validity.

We required a set of executable C# projects, for which we relied
on a previously collected dataset. Although we cannot make cer-
tain claims regarding generalization to other languages and projects,
our corpus was deliberately selected from projects with diverse
goals and implementations by prior work. In addition, C# is a pop-
ular language that closely resembles other languages that are often
used to build large systems (such as Java, Kotlin and C). However,
generalization to more script-like languages (such as Python and
JavaScript or TypeScript) is not a given and deserves further inves-
tigation.

We instrumented our projects to generate our own test execu-
tion harness, causing us to work with a reduced set of projects and
test cases. This was hard to avoid given our parameters, but future
work may investigate the eﬃcacy of more light-weight approaches
(e.g. randomly dropping method traces from one large trace ﬁle).
We were still able to work with nearly one thousand test cases per
project, which produced many gigabytes of trace data. Our instru-
mentation also required adding improved error-recovery to both
Daikon and its C# front-end (Celeriac), which may introduce some
missing values in our trace data and invariants. In practice, how-
ever, these errors were rare, occurring only on large trace ﬁles.

Our manual annotation carries the risk of errors in human judge-
ment, which we aimed to mitigate by having two annotators per
invariant, as well as strict guidelines that allowed for invariants to
be marked as “irrelevant” besides the usual categories of “valid”
and “invalid”. Nonetheless, annotating pre- and post-conditions
on large real systems was a complicated assignment and often re-
quired extensive investigation of a method’s context; we have doc-
umented evidence for all of our more complex decisions in our
replication package (see Section 5.4, andSection 7.2 for further dis-
cussion).

Our ablation analysis considers two simpler alternatives to our
full model: a bi-directional RNN and an invariants-only validator.
We designed our own ablations to address (and dismiss) various
simpler explanations for our model’s eﬃcacy; to the best of our
knowledge, no prior work has addressed this task, so no further
baselines were included. Our model generalized substantially bet-
ter than either of these, even though (bi-directional) RNNs are pow-
erful models with many applications to software engineering tasks
[16, 35].

7 RELATED WORK
Our work occupies addresses an unusual topic in aiming speciﬁ-
cally at learning method pre- and post-conditions. Although we
are not aware of any work that tackles this same issue, there is a
fair body of work that studies the learning of loop invariants. In
this setting, tools can rely on an SMT solver provided the loop and
state-space is small enough. Furthermore, we are not the ﬁrst to
analyze Daikon’s output; we discuss both these families of related
work here.

7.1 Learning (Loop) Invariants
Sharma et al. describe a data-driven approach to ﬁnding algebraic
(polynomial) invariants using a “guess and check” approach [32]:
they “guess” polynomial invariants by solving a system of linear
equations over all possible monomials up to a ﬁxed degree across
traced values of variables (one row per observed state vector). They
then “check" these using an SMT solver; any counterexamples the
solver ﬁnds are used to create another test input, after which the
process is re-run. This approach is subject to the scaling limitations
of the SMT solver, as well as the combinatorial growth of matrices
with monomial degree and state vector dimension.

A related paper describes the use of PAC-learning to learn inte-
ger loop invariants on small single-procedure programs with one
loop [31]. The PAC learner induces invariants as geometric con-
straints on a vector of loop variables. Labeled examples are ob-
tained from passing test cases that satisfy a post-condition (“good”)
and by sampling satisfying values of a weakest pre-condition that
would force a post-condition to fail after one iteration of the loop
(“bad"). This approach appears to work only on small programs
with speciﬁed post-conditions (in order to generate bad test cases).
On bigger programs, ﬁnding such a weakest precondition is not
scalable. Our approach learns a compact representation of the pro-
gram, relevant to the task, allowing it to better scale than both
these approaches.

Padhi et al. learn pre-conditions and loop invariants as boolean
combinations of a set of arithmetic conditions (“features") [23]. They
improve upon previous approaches (e.g., [15]) which could learn
such combinations, given an adequate set of features, whereas Padhi
et al. can synthesize new features. Their synthesis approach essen-
tially generates and tests all features up to a size, searching for
a new feature that can distinguish between passing and failing in-
puts, thus improving the expressive power of possible feature com-
binations. This approach requires passing and failing tests and is
agnostic with respect to the program structure. Pham et al. [25]
also describe an approach that is agnostic to the structure of the
original program, which uses a ﬁxed set of feature templates over
state vectors, together with an SVM-based approach, to learn lin-
ear inequalities that classify passing and failing state vectors. In
addition, they modify the original program to inject artiﬁcial states
near decision boundaries so as to create new labeled examples, as
a form of active learning. The paper does not show that this ap-
proach is sound. Although it is one of the few works to address
learning pre-conditions, it requires post-conditions to be in place,
as well as labeled traces corresponding to passing and failing tests.
Si et al. propose a deep reinforcement learning approach to cre-
ating loop invariants via stochastic synthesis [33]. Ultimately, an
SMT solver (Z3) is used to provide (automated) supervision. The
approach incorporates a learned GGNN to create representations
of a semantic abstraction of a given program, together with an RL
approach to learn a policy that directs the synthesis of a loop invari-
ant. The RL reward mechanism ﬁnesses the sparsity of the eventual
reward (the ﬁnal validity of the invariant, as labeled by Z3) by cre-
ating intermediate rewards; these count and normalize the propor-
tion of counterexamples produced by Z3 by candidate invariants as
the generation proceeds. This normalized count is supplemented

Hellendoorn et al.

by another intermediate reward (provided before invariant gen-
eration is complete) that is designed to reject “meaningless" and
“trivial" predicates such e == e or e < e. Although we do not focus
on generating invariants (and cannot rely on an SMT solver), the
approach proposed in this paper is quite complementary to ours,
suggesting a possible symbiosis (e.g. to tackle the third challenge
in Section 2: generating semantically meaningful invariants).

Brockschmidt et al. induce invariants over data structures for
shape analysis, using a similar approach of generating invariants
(in separation logic) production using a learned neural network
[13]. The production selection is based on hand-engineered fea-
tures over the data-structure graphs. Rather than using SMT solvers
(with RL) for supervision, they use data produced from test runs.
This approach is program agnostic and may thus beneﬁt from in-
sights in our paper, despite considering a very diﬀerent class of
invariants.

7.2 Evaluating Daikon
Daikon has become a landmark tool in the academic community
[14]. Accordingly, it is involved in much related work; it is often
used to mine a corpus of invariants on which new tools can rely for
tasks such as automated patching [24] and test generation [11, 22].
There is some related work on judging the validity of invariants,
both those Daikon generates and invariants in general. Staats et al.
study the accuracy with which programmers can classify automat-
ically generated invariants and ﬁnd that this accuracy can be re-
markably poor; users in their study misclassiﬁed between 9% and
32% of correct invariants and between 26% and 59% of incorrect in-
variants. We indeed found that classifying real-world invariants is
a complicated task, often requiring inspection of the surrounding
code. We also relied on two annotators to reduce the risk of mis-
takes and aimed to set objective guidelines for conﬂict resolution
(see Section 3.2), omitting any unresolved invariants entirely.

A few studies have assessed the validity of Daikon’s invariants
directly. Polikarpova et al. do so speciﬁcally on the Eiﬀel program-
ming language [26]. They also identify invariants in terms of both
relevance and validity, using quite similar criteria for both (although
we did not assess the validity of irrelevant invariants). They ﬁnd a
strong correlation between the size of the test suite and the valid-
ity of generated invariants. However, they also ﬁnd substantially
higher proportions of valid invariants: with a medium-sized test
suite, 54-67% of their pre- and post-conditions are relevant (this
is similar to our work), and 83-92% are valid (this is far higher
than we found). This discrepancy is likely due to the programs
they consider being much smaller (1.5KLOC at most) than ours as
they evaluate only isolated library classes. Kim & Peterson eval-
uate Daikon’s invariants on larger (C++) systems [19]; although
their technical report does not include a quantitative evaluation,
they do observe many phenomena similar to ours. Among others,
they stipulate the high degree of false positives that Daikon pro-
duces, even with ﬁlters in place, and the absence of semantically
insightful invariants, despite the presence of elaborate conditions.
Our work further conﬁrms and quantiﬁes their ﬁndings and aims
to provide a solution in the form of our learning methodology and
model.

Are My Invariants Valid? A Learning Approach

8 SUMMARY
We set out to address a complex question: are real-world method
invariants natural? Answering this involves assessing what such
invariants look like in practice and how they relate to their meth-
ods. We conducted a manual study on a widely used pre- and post-
condition inference tool (Daikon) on methods from several large
systems. Our ﬁndings show that substantial challenges exist for
practically useful invariant inference, such as needing better auto-
matic validity assessment, better identiﬁcation of relevant invari-
ants, and inference of more meaningful complex invariants. In this
work, we demonstrate the potential for a learning-based approach
that addresses the ﬁrst challenge. We contribute an automated ap-
proach to mining a large corpus of annotated invariants with noisy,
yet useful labels, and a model that can learn to meaningfully vali-
date invariants based on this data. Our graph neural network based
invariant validator is able to accurately identify valid invariants in
our automatically mined corpus, even across projects. Importantly,
it generalizes well to our human-annotated corpus, whereas no
simpler model came close to this performance. Our results set the
stage for learning of properties of a program’s behavior from its
source code.

REFERENCES
[1] [n. d.]. Source code for DotLiquid.Template.RegisterSafeType(Type type, string[]

allowedMembers). https://bit.ly/2Im6XuJ.

[2] [n. d.].

Source code for Log4Net.Appender.FileAppender.WriteHeader().

https://bit.ly/2DTsYM1.

[3] [n. d.].

Source code for Metrics ˙Net: Metrics ˙Core ˙CounterMetricİncrement().

https://bit.ly/2ttzcNY.

[4] [n. d.]. Source code for MongoDB.Driver.Core.Clusters.ElectionId.CompareTo(ElectionId

other), with slight modiﬁcations for conciseness. https://bit.ly/2ttzcNY.

[5] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al.
2016. TensorFlow: A system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16). 265–283.
[6] M. Allamanis, E. T. Barr, C. Bird, P. Devanbu, M. Marron, and C. Sutton. 2018.
Mining Semantic Loop Idioms. IEEE Transactions on Software Engineering 44, 7
(July 2018), 651–668. https://doi.org/10.1109/TSE.2018.2832048

[7] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2014. Learn-
ing Natural Coding Conventions. In Proceedings of the 22Nd ACM SIGSOFT In-
ternational Symposium on Foundations of Software Engineering (FSE 2014). ACM,
New York, NY, USA, 281–293. https://doi.org/10.1145/2635868.2635883

[8] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting accurate method and class names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. ACM, 38–49.

[9] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn-
ing to Represent Programs with Graphs. In International Conference on Learning
Representations. https://openreview.net/forum?id=BJOFETxR-

[10] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code.
In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering. ACM, 472–483.

[11] Shay Artzi, Michael D Ernst, Adam Kiezun, Carlos Pacheco, and Jeﬀ H Perkins.
2006. Finding the needles in the haystack: Generating legal test inputs for object-
oriented programs. (2006).

[16] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis.
2018. Deep Learning Type Inference. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2018). ACM, New York, NY, USA,
152–162. https://doi.org/10.1145/3236024.3236051

[17] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are deep neural net-
works the best choice for modeling source code?. In Proceedings of the 2017 11th
Joint Meeting on Foundations of Software Engineering. ACM, 763–773.

[18] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837–847.

[19] Miryung Kim and Andrew Petersen. [n. d.]. An Evaluation of Daikon: A Dy-

namic Invariant Detector. ([n. d.]).

[20] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980 (2014).

[21] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2015. Gated
graph sequence neural networks. arXiv preprint arXiv:1511.05493 (2015).
[22] Carlos Pacheco and Michael D. Ernst. 2005. Eclat: Automatic Generation and
Classiﬁcation of Test Inputs. In ECOOP 2005 - Object-Oriented Programming, An-
drew P. Black (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 504–527.

[23] Saswat Padhi, Rahul Sharma, and Todd Millstein. 2016. Data-driven precondition
inference with learned features. ACM SIGPLAN Notices 51, 6 (2016), 42–56.
[24] Jeﬀ H Perkins, Sunghun Kim, Sam Larsen, Saman Amarasinghe, Jonathan
Bachrach, Michael Carbin, Carlos Pacheco, Frank Sherwood, Stelios Sidiroglou,
Greg Sullivan, et al. 2009. Automatically patching errors in deployed software. In
Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles.
ACM, 87–102.

[25] Long H Pham, Ly Ly Tran Thi, and Jun Sun. 2017. Assertion generation
through active learning. In International Conference on Formal Engineering Meth-
ods. Springer, 174–191.

[26] Nadia Polikarpova, Ilinca Ciupa, and Bertrand Meyer. 2009. A comparative study
of programmer-written and automatically inferred contracts. In Proceedings of
the eighteenth international symposium on Software testing and analysis. ACM,
93–104.

[27] Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to
Name-based Bug Detection. Proc. ACM Program. Lang. 2, OOPSLA, Article 147
(Oct. 2018), 25 pages. https://doi.org/10.1145/3276517

[28] Rabee, Malik Sohail and Patra, Jibesh and Pradel, Michael. [n. d.]. NL2Type:
Inferring JavaScript Function Types from Natural Language Information.
([n.
d.]), to appear.

[29] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli, and Premkumar Devanbu. 2016. On the “naturalness” of buggy code.
In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE).
IEEE, 428–439.

[30] Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting program
properties from big code. In ACM SIGPLAN Notices, Vol. 50. ACM, 111–124.
[31] Rahul Sharma, Saurabh Gupta, Bharath Hariharan, Alex Aiken, and Aditya V
Nori. 2013. Veriﬁcation as learning geometric concepts. In International Static
Analysis Symposium. Springer, 388–411.

[32] Rahul Sharma, Eric Schkufza, Berkeley Churchill, and Alex Aiken. 2013. Data-
driven equivalence checking. In ACM SIGPLAN Notices, Vol. 48. ACM, 391–406.
[33] Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. 2018.
Learning loop invariants for program veriﬁcation. In Advances in Neural Infor-
mation Processing Systems. 7762–7773.

[34] Ke Wang, Zhendong Su, and Rishabh Singh. 2018. Dynamic Neural Program
Embeddings for Program Repair. In International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=BJuWrGW0Z

[35] Martin White, Christopher Vendome, Mario

In Proceedings of

Denys Poshyvanyk. 2015.
tories.
ware Repositories
(MSR ’15).
http://dl.acm.org/citation.cfm?id=2820518.2820559

and
Toward Deep Learning Software Reposi-
the 12th Working Conference on Mining Soft-
IEEE Press, Piscataway, NJ, USA, 334–345.

Linares-Vásquez,

[12] Marc

Brockschmidt, Miltiadis
2019.

and Oleksandr Polozov.
Graphs.
International
https://openreview.net/forum?id=Bke4KsA5FX

Conference

In

Allamanis,

Alexander

Gaunt,
Generative Code Modeling with
Representations.

Learning

on

L.

[13] Marc Brockschmidt, Yuxin Chen, Pushmeet Kohli, Siddharth Krishna, and
Daniel Tarlow. 2017. Learning shape analysis. In International Static Analysis
Symposium. Springer, 66–87.

[14] Michael D Ernst, Jeﬀ H Perkins, Philip J Guo, Stephen McCamant, Carlos
Pacheco, Matthew S Tschantz, and Chen Xiao. 2007. The Daikon system for
dynamic detection of likely invariants. Science of Computer Programming 69,
1-3 (2007), 35–45.

[15] Pranav Garg, Daniel Neider, Parthasarathy Madhusudan, and Dan Roth. 2016.
Learning invariants using decision trees and implication counterexamples. In
Acm Sigplan Notices, Vol. 51. ACM, 499–512.

