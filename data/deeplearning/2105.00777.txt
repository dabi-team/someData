1
2
0
2

y
a
M
4

]

V
C
.
s
c
[

2
v
7
7
7
0
0
.
5
0
1
2
:
v
i
X
r
a

Recognition of Oracle Bone Inscriptions by using Two Deep Learning
Models

Yoshiyuki Fujikawa1, Hengyi Li1, Xuebin Yue1, Aravinda C V2, Amar Prabhu G2, Lin Meng1

1Department of Electronic and Computer Engineering, Ritsumeikan University, Kusatsu,Shiga,Japan
2N.M.A.M Institute of Technology, Nitte, India

Corresponding author: Lin Meng , menglin@fc.ritsumei.ac.jp

Abstract
Oracle bone inscriptions (OBIs) contain some of the oldest characters in the world and were used in
China about 3000 years ago. As an ancient form of literature, OBIs store a lot of information that
can help us understand the world history, character evaluations, and more. However, as OBIs were
found only discovered about 120 years ago, few studies have described them, and the aging process has
made the inscriptions less legible. Hence, automatic character detection and recognition has become an
important issue. This paper aims to design a online OBI recognition system for helping preservation
and organization the cultural heritage. We evaluated two deep learning models for OBI recognition,
and have designed an API that can be accessed online for OBI recognition. In the ﬁrst stage, you only
look once (YOLO) is applied for detecting and recognizing OBIs. However, not all of the OBIs can be
detected correctly by YOLO, so we next utilize MobileNet to recognize the undetected OBIs by manually
cropping the undetected OBI in the image. MobileNet is used for this second stage of recognition as our
evaluation of ten state-of-the-art models showed that it is the best network for OBI recognition due to its
superior performance in terms of accuracy, loss and time consumption. We installed our system on an
application programming interface (API) and opened it for OBI detection and recognition.

Keywords
Oracle bone inscription recognition, Deep learning, API

I

INTRODUCTION

Oracle bone inscriptions (OBIs) were inscribed on cattle bone or turtle shells about 3000 years
ago and contain some of the oldest characters in the world. The OBIs used in the Shang Period
in China were buried in ruins due to war in 1046 B.C. and were not discovered until 1899. Very
few studies have described them, and the aging process has made the inscriptions less legible.
Understanding the inscriptions is important in terms of researching world history, character
evaluation, and more [Ochiai and Atsushi, 2008]

The most common OBI retrieval method involves rubbing, where the OBI surface is reproduced
by placing a piece of paper over the subject and then rubbing the paper with rolled ink. Figure
1 shows several examples of oracle bones(OBs), and OBIs. In (a), photo images of the front,
side and back of an OB are shown, which several characters visible in the front view image. (c)
and (d) show the rubbing image and an expert depicted image of the front side shown in (a),
and the characters in the image appear more clearly. A search example of OBIs is shown in (d)-
(g). These images were collected by AnYang Normal University and added to OBI Database
University [2019], which is a great resource for culture heritage protection and research. The
phrase “today ’s rain” is indicated by a red box in this example, and the detailed transcription
of the inscriptions in modern Chinese along with the meaning in English are shown in (h).

1

 
 
 
 
 
 
Figure 1: Example of OBIs.

The OBI characters are slightly similar to the modern Chinese characters. However, this does
not necessarily mean that we can understand the OBIs. In term of the quality of these characters,
the sizes are uneven, both small and big noises exist, some characters are broken, and the
inclinations of the characters are non-uniform. The noises refer the white area in the rubbing
image which is not a part of character. Figure 1 (d) shows a smaller noise which exists near
the character, and a bigger noise in the right side. These areas like character and interrupt the
characters detection and recognition.

Hence, OBI detection and recognition are very difﬁcult, and standard optical character recogni-
tion (OCR) for OBI recognition is not easy. Moreover, character assignments are often different
because each of oracle bone is different not only in size but also in material, which leads to dif-
ﬁculty with character detection.

This paper aims to design a online OBI recognition system for helping preservation and organi-
zation the cultural heritage. Our proposed method is intended to help archaeologists understand
OBI, it features two deep learning models for achieving better OBI detection and recognition.

In the ﬁrst step, our method applies YOLO Joseph and Ali [2018] for OBIs detection and
classiﬁcation. However, un-detected OBIs still exist. Next, MobileNet Howard et al. [2017],
Sandler et al. [2018] is applied for recognizing the undetected OBIs by manually cropping the
undetected OBI in the image. We chose MobileNet after the results of our evaluation of ten
kinds state-of-the-art models (including LeNet Lecun et al. [1998], AlexNet Krizhevsky et al.
[2012], GoogLeNet Szegedy et al. [2015], VGG16, VGG19 Simonyan and Zisserman [2015],
ResNet 152V2 He et al. [2015], Inception Szegedy et al. [2016a], InceptionResNetV2 Szegedy et al.
[2016b], Xception Cholle [2017]) showed that it achieved the best performance on loss, accu-
racy and computation times Howard et al. [2017].

Finally, the two models are designed as an API that is equip-ped on a server and operated by
users. The API can be accessed in Meng et al. [2019b].

2

Figure 2: Character evaluation.

We make three contributions in this paper:

1. This is the ﬁrst research targeting OBI detection and recognition with real rubbing images

using deep learning.

2. We developed the ﬁrst online API for OBI recognition, which contributes not only to
technology but also to culture heritage preservation and the understanding of OBIs.
3. Experiments on ten state-of-the-art recognition methods demonstrated that MobileNet is

the best model for online OBI recognition.

In section 2 of this paper, we provide an overview of OBIs and discuss related work. Section
3 describes the recognition ﬂow. In section 4, we discuss the ﬁrst stage of recognition, which
detects and recognizes OBIs by YOLO, and present the experimental results. Section 5 de-
scribes our selection of the model with the best ﬁt for the second stage of recognition through
experiments. Section 6 reports the performance results using our API. We conclude in section
7 with a brief summary and mention of future work.

II OBIS AND RELATED WORK

2.1 OBIs

This subsection deﬁnes what OBIs are and clariﬁes the importance of research on their preser-
vation. OBIs contain some of the oldest characters in the world from the Shang period of China,
about 3000 years ago. They were inscribed on cattle bone or turtle shells since no ink or paper
existed in that period.

OBIs are a kind of hieroglyph that were the original version of the modern Chinese charac-
ters, which widely used in China, Japan, and other Asian countries. One OBI character is one
hieroglyph graph, with a meaning corresponding to one thing or one object. Figure 2 shows

3

several characters and character evaluations, which are introduced in this paper and used in the
experiments. The character evaluation means the changing in writing form during its history of
use. The modern character and the English meaning are shown at the right of the ﬁgure.

The ﬁrst word in Figure 2 is “Mouse”(1), which corresponds to the image of an animal that has
four legs and a long tail. The OBI of “Mouse” has four legs and a long tail, too. The second word
is “King” (2), which corresponds to the image of a man standing on a board (line). The OBI
of “King” depicts the same meaning as well. The third word is “Rain”, which corresponding
image of rain drops falling from a cloud in the sky. The fourth word is “Sun(Day)”, and the
image is a sun. Understanding these OBIs not only helps us research history but also helps us
evaluate the Chinese character evaluations.

To protect these items of cultural heritage, researchers have recently collected OBI images and
made them available in books Zuo and P.M [2009] and an open access database University
[2019]. Ochiai also created a format database for OBIs Ochiai [2014].

2.2 Related Work

OBI recognition can be deﬁned as a technique of pattern recognition. Pattern recognition meth-
ods are typically classiﬁed into classical computer vision and deep-learning based computer
vision.

Various researchers have attempted to recognize OBIs by means of image processing. How-
ever, few English papers have reported on OBI recognition, and the recognition rate needs to
be improved. Li et al. Li and Woo [2007] presented a recognition method that treats OBIs
as a non-directed graph for recording the features of end-points, three-cross-points, ﬁve-cross-
points, blocks, net-holes, etc. Li et al.Li et al. [2011.1] presented another recognition method
using a non-directed graph too. However, due to the age of OBIs, some of the holes and cross-
points that occur are not actually part of the OBIs themselves, which increases the recognition
difﬁculty. Li et al. Li and Yang [2008.3] proposed a DNA method for recognizing OBIs. How-
ever, neither Li et al. Li et al. [2011.1] nor Li et al.Li and Yang [2008.3] provided any details
on their experiments.

Meng et al. have previously proposed several methods for recognizing OBIs by template match-
ing and by Hough transform Meng et al. [2016.2, 2015.8], Meng [2017a,b]. However, the tem-
plate matching was weak when the original character was tilted, and the tilt was not properly
processed in Meng et al. [2015.8]. In one study, there was not enough experimental work Meng
[2017a]. Later, Meng et al. proposed a two-stage recognition system to be used from noise
reduction to recognition that can consider the tilt of the character; however, only 70% of the
inscriptions on the ﬁrst-most similar templates were recognized.

Currently, researchers are attempting to recognize the characters by deep learning. However,
work by Meng et al. Meng et al. [2018] and Liu et al. Liu and Gao [2018], showed that all
of the OBIs needed to be cut in advance, which is not convenient for practical application. In
a subsequent work, Meng et al. extended the Single Shot Multibox Detector (SSD) Liu et al.
[2018], Meng et al. [2019a] for OBI detection and found through experiments that Precision,
Recall and F value achieve 0.95, 0.83 and 0.88 respectively, and indicates the effectiveness of
SSD.

Another group also aimed to recognize OBIs by deep learning Guo et al. [2016], and while they
achieved good accuracy, the experimental images of OBIs were drawn in manual by current
researchers, not the original rubbings.

4

In this paper, we uses two deep learning models for real OBI detection and recognition on real
rubbing images. In the ﬁrst step, YOLO is applied Joseph and Ali [2018] for OBI detection
and classiﬁcation. In the second step, for any OBIs that remain undetected, the user drop a
mouse on the image and draws a bounding box around these characters, which are recognized
by Howard et al. [2017], Sandler et al. [2018]. Our models are made openly available to the
public by the means of a web-facing API for any users (e.g., archaeologists) who are interested
in OBIs.

Figure 3: OBI recognition ﬂow.

III RECOGNITION FLOW

In this section, we provide an overview of the multiple deep learning processes involved in OBI
recognition.

5

Figure 4: Inscriptions detection process of YOLO model.

The left part of Figure 3 shows the proposed recognition ﬂow, which consists of two recog-
nition steps after image uploading. First, the target OBI rubbing image is uploaded, and OBI
detection and recognition by YOLO-tiny is performed using. YOLOv3 is trained and opti-
mized in advance. Then, the undetected OBIs are cropped and re-uploaded for re-recognition
by MobileNetSandler et al. [2018], Howard et al. [2017].

These models are designed as an API that is installed on the server for OBI recognition. The
API is opened for accessing and the action is shown in the right part of Fig. 3. (a) shows
the interface for uploading a target OBI rubbing image. Then in(b), when the user clicks the
blue “Recognize characters” button after the target image is uploaded, the results recognized by
YOLO are shown. The recognized characters are bounded by boxes and the recognized results
and the conﬁdence are displayed on box. For achieving better recognition results, the user
can change the conﬁdence by clicking the blue “Change conﬁdence” button. The new result
is depicted in Fig. 3 (c), which shows the number of recognized characters number increased
from two to six by optimizing the conﬁdence.

Usually, it is difﬁcult to detect and recognize all of the OBIs by YOLO-tiny, so we added
MobileNet for the character recognition. The action, “Predict Cropped Characters”, is shown
in (d) . The user crops an undetected area (Here, MobileNet is equipped for the characters
recognition. The action is shown in Fig. 3 (d) of “Predict Cropped Character”. The user
crops an undetected area (or any character that needs recognizing) and then clicks the “Predict
Cropped Character” button. The recognized results and the conﬁdence are then displayed on
the button of the web site.

IV YOLO FOR DETECTING AND RECOGNIZING OBIS

The YOLOv3 tiny network model treats the target detection as a regression problem for a spa-
tially separate target box and its category conﬁdence. A single neural network can directly
predict the conﬁdence of the target box and its category from the entire image. The sample
detection model of the YOLOv3 network model is shown in the Fig 4.

We equip a high spec GPU machine for the training, the hardware conﬁguration is GPU : GTX
1050Ti ×3, CPU : IntelCore i7 6700 3.4GHz and RAM : 32G; Operating system is Ubuntu16.04
64-bit and Deep learning framework is Darknet, Programming language is C.

• The ﬁrst step is to divide the image into an S × S grid. If an inscription is in the grid,
the grid is responsible for detecting that inscription. Each grid predicts B detection boxes

6

along with the conﬁdence of these detection boxes. The number of detection frames for
each inscription is S × S × B.

• The detection box has ﬁve predicted values– X, Y , W , H, and Conf idence – where X,Y
is the offset of the center of the prediction box relative to the cell boundary, W and H are
the ratios of the predicted box width to the entire image, and Conf idence represents the
conﬁdence of the detection box.

• Then, each grid predicts the inscription’s conditional probability, provided that the known

grid contains an inscription.

• Finally, during detection, the conditional probability is multiplied by the predictive value
of different detection box conﬁdences to obtain the score for each box’s inscription class.
These inscriptions types also include the probability of inscriptions appearing in the de-
tection frame and verify the similarity between the box and the inscription goal. This is
formulated by Eq. 1 below.

Conf idence(class) = Probablity(class) × Intersetion Over UnionT ruth

Predection

(1)

4.1 Data Set

There is no proper and standard OBI data set for training and testing purposes, so we prepared
our own. The training and test sets contain 549 and 100 image samples, respectively. We used
27 characters in the experiment. The training data set was used at the initial learning rate of
0.001 and the maximum number of training iterations was about 3000. The mini batch size
was ten. This actually one of the most interesting parts for digital/computational humanties
works. More information is necessary here: Show examples of the image samples, what is the
distribution of sampled characters, what is the distribution of count of characters per document
“bone”, is the dataset a set of bounding boxes, or concrete image samples spliced from the
document images.

4.2 Network Structure Model

In this work, we use the YOLOv3-tiny network model for training datasets. YOLOv3-tiny
follows the coordinate prediction principle of YOLOv2. YOLOv3 can easily achieve real-
time performance using just a standard computer with a graphics processing unit (GPU). The
Darknet19 structure of the YOLO v3-tiny network is detailed in Table 1 Redmon [2016]. The
given data was augmented to increase its size.

4.3 Network Training

Inscription detectors are trained in accordance with the neural network Darknet framework and
the learning rate adopts its default strategy. To reduce the training time, the convolution network
is given network parameters that are trained by the Darknet19 network model Redmon [2016].

4.4 Evaluation Metrics

We used seven metrics for our evaluation: 1) precision, 2) recall, 3) mean of average precision
measured at intersection over union (IOU), 4) F1 score, 5) model volume, 6) parameter size,
and 7) FLOP. The main conﬁdence and non-maximum suppression threshold for all models in
these experiments were set to 0.1 and 0.5, respectively. This evaluation was carried out with

7

Table 1: Darknet19 structure of YOLOv3-tiny network.

Layer
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

Type
Convolutional
Max Pooling
Convolutional
Max Pooling
Convolutional
Max Pooling
Convolutional
Max Pooling
Convolutional
Max Pooling
Convolutional
Max Pooling
Convolutional
Convolutional
Convolutional
Convolutional
YOLO
Route 13
Convolutional
Up-sampling
Route 19 8
Convolutional
Convolutional
YOLO

16

64

32

128

Filters Size/Stride
3 × 3, 1
2 × 2, 2
3 × 3, 1
2 × 2, 2
3 × 3, 1
2 × 2, 2
3 × 3, 1
2 × 2, 2
3 × 3, 1
2 × 2, 2
3 × 3, 1
2 × 2, 1
3 × 3, 1
1 × 1, 1
3 × 3, 1
1 × 1, 1

1024
256
512
255

512

256

Input
416 × 416 × 3
416 × 416 × 16
208 × 208 × 16
208 × 208 × 32
104 × 104 × 64
104 × 104 × 64
52 × 52 × 64
52 × 52 × 128
26 × 26 × 128
26 × 26 × 256
13 × 13 × 256
13 × 13 × 512
13 × 13 × 512
13 × 13 × 1024
13 × 13 × 256
13 × 13 × 512

Output
416 × 416 × 16
208 × 208 × 16
208 × 208 × 32
104 × 104 × 32
52 × 52 × 64
52 × 52 × 64
52 × 52 × 128
26 × 26 × 128
26 × 26 × 256
13 × 13 × 256
13 × 13 × 512
13 × 13 × 512
13 × 13 × 1024
13 × 13 × 256
13 × 13 × 512
13 × 13 × 255

128

256
255

1 × 1, 1
2 × 2, 1

3 × 3, 1
1 × 1, 1

13 × 13 × 256
13 × 13 × 128

13 × 13 × 128
26 × 26 × 128

13 × 13 × 384
13 × 13 × 256

13 × 13 × 256
13 × 13 × 256

no batch processing on one NVIDIA GTX1080ti GPU card using Darknet. Evaluation index
metrics were formulated by

P recision =

T P
T P + F N

Recall =

T P
T P + F P

(2)

(3)

where TP, FP, and FN respectively represent the number of samples that correctly recognized
OBIs as OBIs, the number of samples that identiﬁed non-OBIs as OBIs, and the number of
samples that identiﬁed OBIs as non-OBIs.

Six images of the experimental results are shown in Figure 5 as examples.

4.5 Experimental results of Loss and Accuracy

We used 29 classes OBI for the model selection experiment. The total number of training
images was 605. The dataset are created and stored in Meng [2020]. Since OBI are ancient

8

Figure 5: Experimental results of YOLO.

characters, some characters appeared with a low frequency. As such, the data of every class is
imbalanced, and the image number of every class varies from two to 46 pieces.

Figures 6 – 15 show the accuracy and loss of the ten models we evaluated (LeNet, AlexNet,
GoogLeNet, VGG16, VGG19, ResNet152V2, Inception, InceptionResNetV2, Xception, and
MobileNet, respectively), and the results are summarized in Table 3.

In each ﬁgure, the left side shows the accuracy of training and validation and the right side
shows the loss of training and validation. The horizontal axis shows the number of epochs and
the vertical axis shows the accuracy or loss.

The loss and accuracy of training and validation did not converge in the case of AlexNet,
VGG16, and VGG19. This was because the data set was too small to ﬁt the models.

LeNet and GoogLeNet, which are a relatively old models, achieved a better performance in
training. However, the validation accuracy was not impressive, especially for GoogLeNet. This

9

Table 2: Results of two algorithms on OBI test sets.

Sl.No

SSD

YOLOV3-tiny

Conﬁdence (Eq.1) 94.06% 90.14%
82% 94.06%
Precision (Eq.2)
88.4% 91.08%
Recall (Eq.3)

Table 3: Results for OBI classiﬁcation data

Model Epochs Training accuracy Validation accuracy

LeNet Lecun et al. [1998]
AlexNet Krizhevsky et al. [2012]
GoogLeNet Szegedy et al. [2015]
VGG19 Simonyan and Zisserman [2015]
VGG16 Simonyan and Zisserman [2015]
ResNet152V2 He et al. [2015]
Inception Szegedy et al. [2016a]
InceptionResnetV2 Szegedy et al. [2016b]
XCeption Cholle [2017]
MOBILE-NET Howard et al. [2017]

911
200
94
42
15
40
31
25
52
95

100%
5.21%
98.96%
7.52%
6.94%
96.88%
95.26%
96.09%
98.18%
99.30%

96.30%
5.70%
92.05%
3.41%
7.55%
96.59%
96.59%
97.73%
97.73%
98.89%

means that these models cannot achieve a better performance, but only increase the layer num-
bers.

In terms of ResNet and Inception, the accuracy and loss of training and validation had better
results. Since these newer models – speciﬁcally, InceptionResNetV2, Xception, and MobileNet
– achieved similarly impressive results compared to the other models, it was difﬁcult to select
the best model from among the three.

4.6 Experimental Results of Recognition Time

In this experiment, we installed the InceptionResNetV2, Xc-eption, and MobileNet models on
the API server to measure the recognition time for making the model decision. The CPU is
Intel(R) Xeon(R) CPU E5-1410 v2 @ 2.80 GHz, RAM is 8G, and OS is Ubuntu 18.04.3 LTS.
The Apache HTTP server was used for designing the API website.

Table 4 shows the computation times of the three models. We found that MobileNet consumed
the shortest time, which was three times faster than Xception and almost ﬁve times faster than
Inception ResNetv2. These results demonstrate that MobileNet is the best ﬁt for an embedded
system that has a short time consumption.

As MobileNet achieved the best performance in terms of accuracy, loss, and time consumption,
this is the model we selected for the second stage of recognition in the API.

V EXPERIMENTATION RESULTS AND DISCUSSION

Figure 3 shows examples of experimental results by the two deep learning models output by the
API. The left one shows the results by YOLO. Several characters were detected and recognized
correctly. However, a few characters were not detected, e.g., the ﬁrst and last characters of the
second line in (A). The reason is that part of the ﬁrst character is broken and the last characters

10

(a) Accuracy of LeNet

(b) Loss of LeNet

(c) Accuracy of Alexnet

(d) Loss of Alexnet

(e) Accuracy of GoogLeNet

(f) Accuracy of GoogLeNet

(g) Accuracy of  VGG16

(h) Loss of VGG16

(i) Accuracy of VGG19 

(j) Loss of VGG19 

(k) Accuracy of  ResNet

(l) Loss of ResNet

(m) Accuracy of Inception

(n) Loss of Inception

(o) Accuracy of InceptionRes

(p) Loss of InceptionRes

(q) Accuracy  of Xception

(r) Loss of Xception

(s) Accuracy  of MobileNet

(t) Loss of MobileNet

Figure 6: Accuracy and loss of ten models.

are too large. Also, the left characters in (A) were recognized as two different characters with
different sizes. The same problem occurred when MobileNet was used, as shown in the exam-
ple in (B). However, the undetected characters were recognized correctly by MobileNet after
cropping the characters in the rubbing image.

In terms of the recognition of undetected characters, currently, the small size of our dataset
limited the performance of YOLO and made it impossible for MobileNet to recognize all of the

11

Table 4: Recognition time on server machine

Model

Recognition time (seconds)

Xception
InceptionResNet v2
MobileNet

30.05
43.15
9.55

OBIs. Even so, the effectiveness of the proposed method has been demonstrated. Our future
work will involve increasing the size of the dataset.

VI CONCLUSION

Oracle bone inscriptions (OBIs), which were created around 3000 years ago, contain some of
the oldest characters in the world. Many of these OBIs are difﬁcult to recognize today due
to their age, and understanding them is important in terms of researching world history and
evaluating characters. This paper proposed two deep learning models for OBI recognition and
designed an online API on which they can be deployed. For the ﬁrst stage of recognition,
YOLO is applied for detecting and recognizing OBIs. Due to the limitations of YOLO, not all
of the characters can be detected and recognized correctly. For the recognition of the unde-
tected OBIs, we evaluated ten different models to determine which one is the best ﬁt. Results
showed that MobileNet had the best performance in terms of accuracy, loss, and short time
consumption. Hence, we also equip the API with MobileNet for cropping any undetected OBIs
and re-recognizing them again (the second stage of recognition). At present, our dataset is too
small, which limited the effectiveness of the proposed method. Increasing its size will be the
focus of future work.

ACKNOWLEDGEMENTS

This work was supported by a Grant-in-Aid for Scientists (18K18337) from the Japan Society
for the Promotion of Science (JSPS), the Ritsumeikan University Art Research Center and Key
Laboratory for Oracle Bone Script Information Processing (Anyang Normal University).

12

References

Francois Cholle. Xception: Deep learning with depthwise separable convolution. In IEEE Conference on Pattern

Recognition and Computer Vision, PRCV 2017, 2017.

J. Guo, C. Wang, E. Roman-Rangel, H. Chao, and Y. Rui. Building hierarchical representations for oracle character

and sketch recognition. IEEE Transactions on Image Processing, 25(1):104–118, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2015.

doi: arXiv:1512.03385.

G. Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Wey, Marco Andreetto,
and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. 2017.
doi: arXiv:1704.04861v1.

Redmon Joseph and Farhadi Ali. Yolov3: An incremental improvement. In Computer Vision and Pattern Recog-

nition. (2018), CVPR 2018, 2018.

A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In

Advances in Neural Information Processing Systems, NIPS 2012, 2012.

Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document

recognition. In Proceedings of the IEEE, pages 2278–2324, 1998.

Feng Li and Peng-Yung Woo. The coding principle and method for automatic recognition of jia gu wen characters.

Int. J. of Human-Computer Studies, 53(2):289–299, 2007.

Qingsheng Li and Yuxing Yang. Sticker dna algorithm of oracle-bone inscriptions retrieving. Computer Engineer-

ing and Application, 44(28):140–142, 2008.3.

Qingsheng Li, Yuxing Yang, and Aimin Wang. Recognition of inscriptions on bones or tortoise shells based on

graph isomorphism. Computer Engineering and Application, 47(8):112–114, 2011.1.

Guoying Liu and Feng Gao. Oracle-bone inscription recognition based on deep convolutional neural network.

Journal of Computers, 13(12):1442–1450, 2018.

Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.

Berg. Ssd: Single shot multibox detector. 2018. doi: 10.1007/978-3-319-46448-0 2.

Lin Meng. Recognition of oracle bone inscriptions by extracting line features on image processing. In Proceedings
of the 6th Int. Conf. on Pattern Recognition Applications and Methods, ICPRAM 2017, pages 210–217, 2017a.
Lin Meng. Two-stage recognition for oracle bone inscriptions. In Proceedings of Image Analysis and Processing,

Part II, LNCS 10485 (ICIAP 2017), ICIAP 2017, pages 672–682. Spinger, 2017b.

Lin Meng.

Oracle bone database, http://www.ihpc.se.ritsumei.ac.jp/obidataset.html, 2020.

URL

http://www.ihpc.se.ritsumei.ac.jp/OBIdataset.html.

Lin Meng, Tomonori Izumi, and Shigeru Oyanagi. Recognition of oracular bone inscriptions by clustering and
matching on the hough space. J. of the Institute of Image Electronics Engineers of Japan, 44(4):627–636,
2015.8.

Lin Meng, Yoshiyuki Fujikawa, Atsushi Ochiai, Tomonori Izumi, and Katsuhiro Yamazaki. Recognition of orac-
Int. J. of Computers Theory and Engineering, 8(1):53–57,

ular bone inscriptions using template matching.
2016.2.

Lin Meng, Aravinda C V, K R Reddy, Tomonori Izumi, and Katsuhiro Yamazaki. Ancient asian character recog-
nition for literature preservation and understanding. In 7th International Conference, EuroMed 2018, pages
741–751, 2018.

Lin Meng, Bing Lyu, Zhiyu Zhang, C. V. Aravinda, Naoto Kamitoku, and Katsuhiro Yamazaki. Oracle bone
inscription detector based on ssd. In Proceedings of New Trends in Image Analysis and Processing - ICIAP
2019, ICIAP 2019, pages 126–136, 2019a.

Lin Meng,

Aravinda

C V,

and Amar

Prabhu G.

Obi-api,

2019b.

URL

http://www.atait.se.ritsumei.ac.jp/OBI/ssdapi/.

Ochiai and Atsushi. Reading History from Oracular Bone Inscriptions. Chikuma Shobo, Japan, 2008.
Atsushi

inscriptions

database,

Ochiai.

Oracle

2014.

bone

http://koukotsu.sakura.ne.jp/top.html.
source

Joseph Redmon.

Darknet:

Open

neural

networks

in

c,

2016.

URL

URL

http://pjreddie.com/darknet/.

Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: In-
verted residuals and linear bottleneck. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, 2018.

K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In Advances

in Neural Information Processing Systems, NIPS 2015, 2015.

C. Szegedy, W. Liu, Y.Q. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.

13

Going deeper with convolutions. In 2015 IEEE Conf. on Computer Vision and Pattern Recognition, CVPR
2015, 2015.

C. Szegedy, W. Liu, Y.Q. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Rethinking the inception architecture for computer vision. In IEEE Conference on Pattern Recognition and
Computer Vision, PRCV 2016, 2016a.

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi.

Inception-v4, inception-resnet and the

impact of residual connections on learning. 2016b. doi: arXiv:1602.07261.

Anyang Normal University. Obi database, 2019. URL http://jgw.aynu.edu.cn/ajaxpage/home2.0/index.html.
Zuo and P.M. Shanghai Bo Wu Guan Cang Jia Gu Wen Zi. Shanghai bo wu guan, Shanghai, China, 2009. ISBN

9787532626915.

14

