6
1
0
2

v
o
N
1
2

]

M
Q
.
o
i
b
-
q
[

2
v
4
4
8
4
0
.
8
0
6
1
:
v
i
X
r
a

Boosting Docking-based Virtual Screening
with Deep Learning

Janaina Cruz Pereira,∗,† Ernesto Raúl Caﬀarena,∗,† and Cicero Nogueira dos
Santos∗,‡

†Fiocruz, 4365 Avenida Brasil, Rio de Janeiro, RJ 21040 900, Brazil
‡IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA

E-mail: janaina.pereira@ioc.ﬁocruz.br; ernesto@ﬁocruz.br; cicerons@us.ibm.com

Abstract

Introduction

In this work, we propose a deep learning ap-
proach to improve docking-based virtual screen-
ing. The introduced deep neural network,
DeepVS, uses the output of a docking program
and learns how to extract relevant features from
basic data such as atom and residues types ob-
tained from protein-ligand complexes. Our ap-
proach introduces the use of atom and amino
acid embeddings and implements an eﬀective
way of creating distributed vector representa-
tions of protein-ligand complexes by modeling
the compound as a set of atom contexts that
is further processed by a convolutional layer.
One of the main advantages of the proposed
method is that it does not require feature engi-
neering. We evaluate DeepVS on the Directory
of Useful Decoys (DUD), using the output of
two docking programs: AutodockVina1.1.2 and
Dock6.6. Using a strict evaluation with leave-
one-out cross-validation, DeepVS outperforms
the docking programs in both AUC ROC and
enrichment factor. Moreover, using the out-
put of AutodockVina1.1.2, DeepVS achieves an
AUC ROC of 0.81, which, to the best of our
knowledge, is the best AUC reported so far for
virtual screening using the 40 receptors from
DUD.

Drug discovery process is a time-consuming and
expensive task. The development and even the
repositioning of already known compounds is a
diﬃcult chore. 1 The scenario gets worse if we
regard the thousands or millions of molecules
capable of being synthesized in each develop-
ment stage. 2,3

In the past, experimental methods such as
high-throughput screening (HTS) could help
making this decision through the screening of
large chemical libraries against a biological tar-
get. However, the high cost of the whole pro-
cess associated with a low success rate turns
this method inaccessible to the academia. 2–4

In order to overcome these diﬃculties, the
use of low-cost computational alternatives is ex-
tensively encouraged, and it was adopted rou-
tinely as a way to aid in the development of new
drugs. 3–5

Computational virtual screening works basi-
cally as a ﬁlter (or a preﬁlter) consisting of the
virtual selection of molecules, based on a par-
ticular predeﬁned criterion of potentially active
compounds against a determined pharmacolog-
ical target. 2,4,6

Two variants of this method can be adopted:
ligand-based virtual screening and structure-
based virtual screening. The ﬁrst one deals with
the similarity and the physicochemical analysis
of active ligands to predict the activity of other
compounds with similar characteristics. The

1

 
 
 
 
 
 
second is utilized when the three-dimensional
structure of the target receptor was already elu-
cidated somehow (experimentally or computa-
tionally modeled). This approach is used to
explore molecular interactions between possi-
ble active ligands and residues of the binding
site. Structure-based methods present a better
performance when compared to methods based
solely on the structure of the ligand focusing on
the identiﬁcation of new compounds with ther-
apeutic potential. 1,7–9

One of the computational methodologies ex-
tensively used to investigate these interactions
is molecular docking. 5,8,10 The selection of more
potent ligands using Docking-based Virtual
Screening (DBVS) is made by performing the
insertion of each compound from a compound
library into a particular region of a target re-
ceptor with elucidated 3D structure. In the ﬁrst
stage of this process, a heuristic search is carried
out in which thousands of possibilities of inser-
tions are regarded. In the second, the quality
of the insertion is described via some mathe-
matical functions (scoring functions) that give
a clue about the energy complementarity be-
tween compound and target. 10,11 The last phase
became a challenge to the computational scien-
tists considering that it is easier to recover the
proper binding mode of a compound within an
active site than to assess a low energy score to
a determined pose. This hurdle constitutes a
central problem to the docking methodology. 12
Systems based on machine learning (ML)
have been successfully used to improve the out-
come of Docking-based Virtual Screening for
both, increasing the performance of score func-
tions and constructing binding aﬃnity classi-
ﬁers. 1,3 The main strategies used in virtual
screening are neural networks - NN, 13 support
vector machines - SVM 12 and random forest -
RF. 14 One of the main advantages of employing
ML is the capacity to explain the non-linear de-
pendence of the molecular interactions between
ligand and receptor. 3

Traditional Machine Learning strategies de-
pend on how the data is presented. For ex-
ample, in the virtual screening approaches, sci-
entists normally analyze the docking output to
generate or extract human engineered features.

Although this process can be eﬀective to some
degree, the manual identiﬁcation of characteris-
tics is a laborious and complex process and can
not be applied in large scale, resulting in the
loss of relevant information, consequently lead-
ing to a set of features incapable of explaining
the actual complexity of the problem. 1,3,15,16

On the other hand, recent work on Deep
Learning (DL), a family of ML approaches
that minimizes feature engineering, has demon-
strated enormous success in diﬀerent tasks from
multiple ﬁelds. 15–17 DL approaches normally
learn features (representations) directly from
the raw data with minimal or none human in-
tervention, which makes the resulting system
easier to adapt to new datasets.

In the last few years, DL is bringing the atten-
tion of the academic community and big phar-
maceutical industries, as a viable alternative to
aid in the discovery of new drugs. One of the
ﬁrst work in which DL was successfully applied
solved problems related to QSAR (Quantita-
tive Structure-Activity Relationships) by Merck
in 2012. Some years later, Dahl et al. 18 de-
veloped a multi-task deep neural network to
predict biological and chemical properties of a
compound directly from its molecular structure.
More recently, Multi-task deep neural networks
were employed to foresee the active-site directed
pharmacophore and toxicity. 19,20 Also in 2015,
Ramsundar et al. 21 predicted drug activity us-
ing Massively Multitask Neural Networks asso-
ciated with ﬁngerprints. The relevance of DL
is also highlighted by recent applications to the
discovery of new drugs and the determination of
their characteristics such as Aqueous Solubility
prediction 22 and Fingerprints. 23

In this work, we propose an approach based
on Deep Convolutional Neural Networks to im-
prove docking-based virtual screening. The
method uses docking simulation results as in-
put to a Deep Neural Network, DeepVS from
now on, which automatically learns to extract
relevant features from basic data such as com-
pound atom types, atomic partial charges, and
the distance between atoms. DeepVS learns ab-
stract features that are suitable to discriminate
between active ligands and decoys in a protein-
compound complex. To the best of our knowl-

2

edge, this work is the ﬁrst on using deep learn-
ing to improve docking-based virtual screening.
Recent works on improving docking-based vir-
tual screening have only used traditional shal-
low neural networks with human deﬁned fea-
tures. 7,13,24

We evaluated DeepVS on the Directory of
Useful Decoys, which contains 40 diﬀerent re-
ceptors. In our experiments we used the output
of two docking programs: AutodockVina1.1.2
and Dock6.6. DeepVS outperformed the dock-
ing programs in both AUC ROC and enrich-
ment factor. Moreover, when compared to the
results of other systems previously reported in
the literature, DeepVS achieved the state-of-
the-art AUC of 0.81.

The main contributions of this work are:
(1) Proposition of a new deep learning-based
approach that achieves state-of-the-art perfor-
mance on docking-based virtual screening; (2)
Introduction of the concept of atom and amino
acid embeddings, which can also be used in
other deep learning applications for computa-
tional biology; (3) Proposition of an eﬀective
method to create distributed vector represen-
tations of protein-compound complexes that
models the compound as a set of atom con-
texts that is further processed by a convolu-
tional layer.

Materials and Methods

DeepVS

DeepVS takes as input the data describing the
structure of a protein-compound complex (in-
put layer) and produces a score capable of dif-
ferentiating ligands from decoys. Figure 1 de-
tails the DeepVS architecture. First, given an
input protein-compound complex x, informa-
tion is extracted from the local context of each
compound atom (First hidden layer). The con-
text of an atom comprises basic structural data
(basic features) involving distances, neighbor
atom types, atomic partial charges and asso-
ciated residues. Next, each basic feature from
each atom context is converted to feature vec-
tors that are learned by the network. Then, a

convolutional layer is employed to summarize
information from all contexts from all atoms
and generate a distributed vector representa-
tion of the protein-compound complex r (sec-
ond hidden layer). Finally, in the last layer
(output layer), the representation of the com-
plex is given as input to softmax classiﬁer,
which is responsible for producing the score. In
Algorithm 1, we present a high-level pseudo-
code with the steps of the feedforward process
executed by DeepVS.

Atom context

First, it is necessary to perform some basic pro-
cessing on the protein-compound complex to
extract the input data for DeepVS. The in-
put layer uses information from the context of
each atom in the compound. The context of an
atom “a” is deﬁned by a set of basic features
extracted from its neighborhood. This neigh-
borhood comprises the kc atoms in the com-
pound closest to “a” (including itself) and the
kp atoms in the protein that are closest to “a”,
where kc and kp are hyperparameters that must
be deﬁned by the user. The idea of using in-
formation from closest neighbor atoms of both
compound and protein have been successfully
explored in previous work on structure-based
drug design. 25

The basic features extracted from the context
of an atom include the atom types, atomic par-
tial charges, amino acid types and the distances
from neighbors to the reference atom. For in-
stance (Figure 2), for the nitrogen atom (N3)
from THM compound, the vicinity with kc = 3
and kp = 2 is formed by N3, H and C from
the compound, and the two atoms OE and CD
from the residue Gln125 in the protein Thymi-
dine kinase (ID_PDB: 1kim). In this particular
case, the context of the atom N3 contains the
following values for each basic feature:

• Atom Type = [N; H; C; OE; CD]

• Charge = [-0.24; 0.16; 0.31; -0.61; 0.69]

• Distance = [0.00; 1.00; 1.34; 3.06; 3.90]

• Amino Acid Type = [Gln; Gln]

3

Algorithm 1 DeepVS feedforeward process

1: Input:

protein-compound complex x,

where the compound contains m atoms

2: Given:

trained network parameters
W atm ∈ Rdatm×|A|, W dist ∈ Rddist×|D|,
W chrg ∈ Rdchrg×|C|, W amino ∈ Rdamino×|R|,
W 1 ∈ Rcf ×|zi|, W 2 ∈ Rh×|cf |, W 3 ∈ R2×|h|,
b1 ∈ Rcf , b2 ∈ Rh, b3 ∈ R2

3: Z = [ ]
4: // generates the representations of atom

contexts

5: for i=1 to m do
6:

zatm = columns from Watm correspond-
ing to atom types of atomi’s neighbors
zdist = columns from Wdist correspond-
ing to distances of atomi’s neighbors
zchrg = columns from Wchrg correspond-
ing to charges of atomi’s neighbors
zamino = columns from Wamino cor-
responding to amino acids of atomi’s
neighbors
// representation of the atomi context
zi = {zatm; zdist; zchrg; zamino}
Z.add(zi)

7:

8:

9:

10:
11:
12:
13: end for
14: // U is initialized with zeros
15: U = [..] ∈ Rcf ×m
16: // convolutional layer
17: for i=1 to m do
18:
19: end for
20: // column-wise max pooling
21: r = max(U, axis = 1)
22: // hidden and output layers
23: score = W 3 (W 2r + b2) + b3
24: // returns normalized score

U [:, i] = f (W 1Z[i] + b1)

25: return

escore[1]
escore[0] + escore[1]

into real-valued vectors (aka embeddings) by
a lookup table operation. These embeddings
contain features that are automatically learned
by the network. For each type of basic fea-
ture, there is a corresponding embedding ma-
trix W that stores a column vector for each
possible value for that basic feature. The ma-
trices W atm, W dist, W chrg and W amino contain
the embeddings of the basic features atom type,
distance, atomic partial charge, and amino acid

Figure 1: DeepVS architecture scheme.
In
this ﬁgure, we use as an example the com-
pound THM (Thymidine) in complex with TK
- Thymidine kinase protein (ID_PDB: 1kim)
compound atoms are marked in dark blue and
their interactions in light blue

Representation of the atom context

The ﬁrst hidden layer in DeepVS transforms
each basic feature value of atom contexts

4

Figure 2: Context from the atom N3 (yellow)
from compound THM (Thymidine). Red circles
represent the two closest neighbors from THM
compound and the two closest neighbors from
the protein closest to N3 atom.

type, respectively. These matrices constitute
the weight matrices of the ﬁrst hidden layer
and are initialized with random numbers before
training the network.

Each column in W atm ∈ Rdatm×|A| corre-
sponds to a feature vector of a particular type of
atom, where A is the set of atom types and datm
is the dimensionality of the embedding and con-
stitutes a hyperparameter deﬁned by the user.
Given the context of an atom “a”, the network
transforms each value of the basic feature atom
type in its respective feature vector and then
concatenates these vectors to generate the vec-
tor atom type representation zatm. As illus-
trated in Figure 3, retrieving the embedding of
an atom type from W atm consists in a simple
lookup table operation. Therefore, the order of
the atom type embeddings (columns) in W atm
is arbitrary and have no inﬂuence in the result.
However, the order in which the embeddings of
the atom types are concatenated to form zatm
matters. We always concatenate ﬁrst the em-
beddings from atom types of the ligand, from
the closest to the farthest, then we concatenate
the embeddings from atom types of the protein,
from the closest to the farthest.

Likewise, zdist, zchrg, zamino vectors are cre-
ated from values of distance, charge and amino
acid types in the context of the target atom.
Values of the basic features charge and distance
need to be discretized before being used as in-

Figure 3: Illustration of the construction of the
atom type representation (zatm) for the con-
text of the atom N3 from the compound THM
(Thymidine). The symbol • indicates the con-
catenation operation.

Finally,

put for the network. We deﬁne minimum and
maximum values, cmin and cmax respectively,
to perform the discretization of charge values.
Bins equally distanced by 0.05 between mini-
mum and maximum are built. For instance,
with cmin = -1 and cmax = 1, there will be 40
bins. Similarly, to discretize distance values,
bins equally distributed by 0.3 Å will be de-
ﬁned in the range between dtmin and dtmax. For
example, with dtmin = 0 e dtmax = 5.1 Å, there
will be 18 bins.
the

con-
text of the atom “a”
is deﬁned as za =
{zatm; zdist; zchrg; zamino}, comprising the con-
catenation of the vectors previously described.
Our hypothesis is that from the basic con-
textual features, the network can learn more
abstract features (the embeddings) that are
informative about the discrimination between
compound and decoys. This type of strategy,
where basic features (words) are transformed
into more abstract features (word embeddings),
has obtained enormous success in the Natural
Language Processing (LNP) ﬁeld. 26–30

representation of

the

Representation of the protein-compound
complex

The second hidden layer in DeepVS is a con-
volutional layer, responsible for (1) extracting
more abstract features from the representations
of all atom contexts in the compound, and (2)

5

summarizing this information in a ﬁxed-length
vector r. We name the vector r the representa-
tion of the compound-protein complex, and it is
the output of the convolutional layer.

The subjacent goal in using a convolutional
layer is its ability to deal with inputs of variable
sizes. 31 In the case of virtual screening, diﬀer-
ent compounds can have a diﬀerent number of
atoms. Therefore, the number of representa-
tions of atom contexts can diﬀer for diﬀerent
complexes. In DeepVS, the convolutional layer
allows the processing of complexes of diﬀerent
sizes.

Given a complex x, whose compound is com-
posed of m atoms, the input to the convolu-
tional layer is a list of vectors {z1, z2, · · · , zm},
where zi is the representation of the context of
the i-th atom in the compound.
In the ﬁrst
stage of the convolutional layer, generation of
more abstract features from each vector zi is
carried out according to:

ui = f (W 1zi + b1)

(1)

where W 1 ∈ Rcf ×|zi| is the weight matrix corre-
sponding to the convolutional layer, b1 is a bias
term, f is the hyperbolic tangent function and
ui ∈ Rcf corresponds to the resulting feature
vector. The number of units (also called ﬁlters)
in the convolutional layer, cf , is a hyperparam-
eter deﬁned by the user.

The second stage in the convolutional layer,
summarizes
also known as pooling layer,
the features
from the various atom con-
texts. The input consists of a set of vectors
{u1, u2, · · · , um}. For the DeepVS, we use a
max-pooling layer, which produces a vector
r ∈ Rcf , where the value of the j − th ele-
ment is deﬁned as the maximum of the j − th
elements of the set of input vectors, i.e.:

[r]j = max
1≤i≤m

[ui]j

(2)

The resulting vector r from this stage is the
representation of the compound-protein com-
plex (Eq. 2). In this way, the network can learn
to generate a vector representation that sum-
marizes the information from the complex that
is relevant to discriminate between ligands and

decoys.

Scoring of Compound-Protein Complex

The vector r is processed by two usual neural
network layers: a third hidden layer that ex-
tract one more level of representation, and an
output layer, which computes a score for each
one of the two possible classiﬁcations of the
complex: (0) inactive compound and (1) active
compound. Formally, given the representation
r generated for the complex x, the third hidden
layer and the output layer execute the following
operation:

s(x) = W 3 (cid:0)W 2r + b2(cid:1) + b3

(3)

where W 2 ∈ Rh×|cf | is the weight matrix of the
third hidden layer, W 3 ∈ R2×|h| is the weight
matrix of the output layer, b2 ∈ Rh and b3 ∈
R2 are bias terms. The number of units in the
hidden layer, h, is a hyperparameter deﬁned by
the user. s(x) ∈ R2 is a vector containing the
score for each of the two classes.

Let s(x)0 and s(x)1 be the scores for the
classes 0 and 1, respectively. We transform
these scores in a probability distribution using
the softmax function, as follows:

p(0|x) =

es(x)0
es(x)0 + es(x)1

p(1|x) =

es(x)1
es(x)0 + es(x)1

(4)

(5)

where we interpret p(0|x) and p(1|x) as the
conditional probabilities of the compound be
a decoy or a ligand, respectively, given the
compound-protein complex data acquired from
docking.

The likelihood of class 1 (active ligand) is
the scoring used to rank ligands during Virtual
Screening essays. The larger the scoring, the
greater the chance the compound is an active
ligand.

Training DeepVS

The common approach for training neural net-
works is the stochastic gradient descent (SGD)

6

algorithm. 32 In our case, SGD is used to mini-
mize a loss function over a training set D that
contains both complexes of ligands and decoys.
At each iteration, a new complex (x, y) ∈ D
is randomly chosen, where y = 1 if the com-
plex contains an active ligand and y = 0, oth-
erwise. Next, the DeepVS network with pa-
rameter set θ = {W atm, W chrg, W dist, W amino,
W 1, b1, W 2, b2, W 3, b3} is used to estimate the
probability p(y|x, θ). Finally, the prediction er-
ror is computed as the negative log-likelihood,
− log (p(y|x, θ)), and the network parameters
are updated applying the backpropagation al-
gorithm. 32 In other words, the set of parame-
ters θ of the network is learned by using SGD
to select a set of values that minimize the loss
function with respect to θ:

(cid:88)

θ (cid:55)−→

(x,y)∈D

− log p(y|x, θ)

(6)

In our experiments, we applied SGD with
minibatches, which means that instead of con-
sidering only one compound-protein complex at
each iteration we consider a small set of ms
randomly selected complexes and used the av-
erage prediction loss to perform backpropaga-
tion. We set ms = 20 in our cases. Also, we
used Theano 33 to implement DeepVS and per-
form all the experiments reported in this work.

Experimental Setup

Dataset

We used the Directory of Useful Decoys
(DUD) 34 as a benchmark to evaluate our deep-
learning-based virtual screening approach. One
of the main reasons to use this dataset was
the possibility of comparing our results with
the ones from systems previously proposed for
revalidation of scoring functions and reclassi-
ﬁcation in virtual screening. 7,11,24 There is a
problem in the partial charges of the original
version of DUD that makes it trivial to discrim-
inate between ligands and decoys. Therefore,
we use the version of DUD produced by Arm-
strong et al., 35 which contains corrected atomic
partial charges.

The DUD dataset has been developed specif-

7

ically to validate VS methods in a rigorous
way.
The dataset is composed of 40 re-
ceptors distributed in six diﬀerent biological
groups: Hormone nuclear receptors, kinases,
serine proteases, metalloenzymes, ﬂavoenzymes
and other classes of enzymes. 34 Also, it pos-
sesses 2,950 annotated ligands and 95,316 de-
coys, a ratio of 36 decoys for each annotated
ligand. Each of the 36 decoys was retrieved
from the ZINC databank, to mimic some phys-
ical property of the associated ligand, such as
molecular weight, cLogP, and the number of H-
bonds groups, although diﬀering in its topol-
ogy. 7,34

Docking Programs

In this work, we used two diﬀerent com-
puter programs to perform molecular docking:
Dock 6.6 36 and AutodockVina1.1.2. 37 Both are
open access and widely used in the academia
to perform VS. Dock 6.6 oﬀers physics-based
energy score functions based on force ﬁelds
and scoring functions (GRID score & AMBER
score). 36 Autodockvina1.1.2 applies a hybrid
scoring function combining characteristics of
knowledge-based and empiric scoring functions
(Vina score). 37

Dock 6.6 setup

and

compound

structures were
Protein
prepared using computational
from
tools
Chimera. 38 Receptors were prepared using the
DockPrep module from Chimera. Ligands, non-
structural ions, solvent molecules, and cofactors
were removed. Absent atoms were added, and
Gasteiger atomic partial charges were applied.
Input ﬁles were .mol2 formatted, except those
receptors that were used to calculate molec-
ular surface,
in which Hydrogen atoms were
removed, and were ﬁnally saved in pdb for-
mat. 38 Spheres were created with radius in the
range 13.5-15.5 Å, varying according to the
size of the active site of the receptor. Box and
grid parameters were taken directly from DUD.
The docking procedure was performed accord-
ing to an available script provided by Dock6.6
program. 36

AutodockVina1.1.2 Setup

Receptors and compound were prepared follow-
ing default protocols and Gasteiger atomic par-
tial charges were applied. 37,39 A cubic Grid of
edge as 27 was deﬁned. The center of the grid
box coincided with the center of mass of the
ligand. Docking runs were performed follow-
ing the default settings deﬁned in AutoDock-
Tools. 39 The only hyperparameter we changed
was the global search exhaustiveness, which we
set to 16 as in Arciniega & Lange (2014). 39
It is worth to note that, although AutodockV-
ina1.1.2 can output more than one pose, in our
experiments, we only consider just one, which
corresponded to the pose that AutodockV-
ina1.1.2 outputs as the best one.

Evaluation Approach

The performance of the proposed method is as-
sessed using leave-one-out cross-validation with
the 40 proteins from the DUD dataset. Fig-
ure 4 illustrates the process we follow to per-
form our leave-one-out cross-validation exper-
iments. First, we applied either DOCK6.6 or
AutodockVina1.1.2 for each protein and its re-
spective set of ligands and decoys to gener-
ate the docked protein-compound complexes.
Next, in each run, one receptor was left out of
the test set while the others were employed as
the training set.

To avoid distortions in the performance re-
sults of DeepVS,
it was essential to remove
all receptors similar to the one used as a test
in a speciﬁc cross-validation iteration from the
training set.
Following Arciniega & Lange
(2014), 7 we regarded as similar receptors those
sharing the same biological class or those with
reported positive cross enrichment. 34 Once the
network was trained, it was applied to the test
receptor, producing a scoring for each of the
potential ligands. Such score was used to rank
the set compounds. The ranking was validated
using metrics that indicate the algorithm’s per-
formance.

Figure 4: Leave-one-out cross-validation.

DeepVS Hyperparameters

The main advantage in using leave-one-out
cross-validation is the possibility of tuning the
neural network hyperparameters without being
much concerned with overﬁtting. In fact, leave-
one-out cross-validation is a suitable method for
tuning hyperparameters of machine learning al-
gorithms when the dataset is small. 40 In our ex-
periments, we used the same set of hyperparam-
eters for the 40 leave-one-out cross-validation
iterations. This is equivalent to perform 40 dif-

8

ferent experiments with diﬀerent training/test
sets using the same conﬁguration for DeepVS.
The hyper-parameter values that provided our
best results and were used in our experiments
with both, AutodockVina1.1.2 and Dock6.6, are
speciﬁed in Table 1. Note that our evalua-
tion approach was stricter than the one used by
Arciniega & Lange (2014), 7 because they tuned
the hyper-parameters using a hold-out set at
each leave-one-out iteration.

In the next section, we present some experi-
mental results that detail the diﬀerence in per-
formance when we vary some of the main hyper-
parameters of DeepVS.

Table 1: Hyperparameter values for DeepVS
used to train the network.

Hyperparameter
datm
damino
dchrg
ddist
cf
h
λ
kc
kp

Value
Description
200
Atom type embedding size
200
Amino Acid emb. size
200
Charge emb. size
200
Distance emb. size
400
# conv. ﬁlters
50
# hidden units
0.075
Learning rate
# neig. atoms from comp.
6
# neig. atoms from protein 2

Evaluation Metrics

To validate DeepVS performance and compare
it with other methods previously published in
the literature, we used two wellestablished VS
performance metrics:
the enrichment factor
(EF) and the area under the ROC curve (Re-
ceiver Operating Characteristic). 41,42

ROC curves are a way to represent the rela-
tionship between the selectivity (Se) and speci-
ﬁcity (Sp) along a range of continuous values
(Equations 7 and 8). It represents the ratio of
true positives in function of the false-positives.

Se =

true positives
total actives

Sp =

true negatives
total decoys

(7)

(8)

The area under the ROC curve (AUC) rep-
resents a quantiﬁcation of the curve and facil-
itates the comparison of results. The AUC is

9

calculated as given in Eq. 9, where Nactives de-
picts the number of actives, Ndecoys represents
the number of decoys, and N i
decoys_seen describes
the number of decoys that are higher ranked
than the i-th active structure. 41 An AUC (cid:54)
0.50 indicates a random selection, whereas an
AUC of 1.0 indicates the perfect identiﬁcation
of active compounds.

AU C = 1 −

1
Nactives

Nactives(cid:88)

i

N i

decoys_seen
Ndecoys

(9)

Given the set of compounds ranked by score,
the enrichment factor at x% (Eq. 10) informs
how good is the set formed by the top x%
ranked compounds compared to a set of an
equal size selected at random from the entire
set of compounds. 41,42 The EF is computed as

EFx% =

actives at X%
compounds at X%

total compounds
total actives

(10)

Results and Discussion

DeepVS vs. Docking Programs

In Table 2 we report, for each of the 40 re-
ceptors in DUD, the virtual screening perfor-
mance for Dock6.6, AutodockVina1.1.2 (hence-
forth ADV), DeepVS using Dock6.6 output
(DeepVS-Dock) and DeepVS using AVD out-
put (DeepVS-ADV). For each system, we report
the AUC ROC and the enrichment factor (ef )
at 2% and 20%. We also report the ef maxi-
mum (efmax), which is the maximum value of
ef that can be achieved for a given ranked list of
compounds. Among the four systems, DeepVS-
ADV achieved the best average result for AUC,
ef2% and ef20%. DeepVS-ADV had the best
AUC for 20 out of the 40 DUD receptors.

Overall, the quality of the docking output im-
pacts the performance of DeepVS, which is ex-
pected. The average performance of DeepVS-
ADV, which had a better docking input from
ADV (avrg. AUC of 0.62) produced better
AUC, ef2%, ef20% and efmax than DeepVS-

Table 2: Virtual screening AUC ROC and enrichment factor (ef ) results for Dock6.6, AutodockV-
ina1.1.2 and DeepVS.

DeepVS-ADV
ef2% ef20%

Dock
auc
ef2% ef20%
1.3
0.48
0.6
0.41
0.8
0.50
3.3
0.86
0.38
0.2
1.4 0.57
0.25
0.1
0.56
2.1
0.64
2.7
0.35
0.2
0.58
1.2
0.48
1.3
0.49
1.9
0.43
1.0
0.68
2.2
0.49
2.1
0.49
1.1
0.90
4.5
0.23
0.1
0.22
0.3
0.16
0.4
0.40
1.0
0.16
0.4
0.39
1.0
0.38
1.2
0.36
1.0
0.73
3.3
0.62
2.2
0.51
1.5
0.39
0.9
0.38
1.2
0.44
0.6
0.49
0.4
0.33
0.6
0.73
2.0
0.50
1.1
0.44
1.8
1.4
0.60
1.4 0.63
0.51
1.6
0.43
1.4

5.3
3.0
0.5
13.0
0.0
4.8
0.7
11.9
17.8
0.0
0.7
3.7
12.0
2.3
5.9
8.9
3.2
7.4
0.0
1.9
0.0
4.9
4.2
0.0
5.3
3.3
13.2
11.1
10.7
3.0
5.4
4.0
1.8
1.8
12.1
1.5
12.3
2.3
0.0
4.5
10.9

efmax
20.1
36.2
12.1
24.5
1.0
5.1
7.3
36.6
20.0
1.1
1.3
36.5
34.5
18.1
7.1
36.6
36.9
10.5
1.0
18.4
1.0
36.9
18.2
2.3
36.7
18.3
36.6
33.8
36.6
36.5
36.8
4.8
2.6
3.1
36.4
1.7
38.4
3.6
2.0
36.1
36.7

average
ACE
AChE
ADA
ALR2
AmpC
AR
CDK2
COMT
COX1
COX2
DHFR
EGFr
ERagonist
ERantagonist
FGFrl
FXa
GART
GPB
GR
HIVPR
HIVRT
HMGR
HSP90
InhA
MR
NA
P38MAP
PARP
PDE5
PDGFrb
PNP
PPARg
PR
RXRa
SAHH
SRC
thrombin
TK
trypsin
VEGFr2

DeepVS-Dock
auc
ef2% ef20%
3.0
0.74
2.7 0.75
0.8
0.48
4.1 0.87
0.56
1.3
0.44
0.0
4.1
0.80
3.7 0.82
0.88
3.2
0.75
3.0
0.78
3.6
4.5
0.88
4.6 0.93
3.5
0.75
4.0 0.90
4.6 0.91
1.7
0.71
4.8 0.92
0.51
1.2
0.49
2.2
0.51
2.1
0.69
2.8
0.24
1.0
0.74
3.3
4.3
0.90
0.55
2.3
2.8 0.78
4.2 0.91
0.63
0.8
2.9
0.75
4.5 0.92
4.8 0.94
0.79
2.0
0.70
3.3
0.91
4.2
0.94
4.7
3.9 0.88
0.59
1.1
0.44
0.5
0.65
1.9
0.88
3.8

5.9
2.0
0.5
6.5
0.0
0.0
18.3
8.9
13.4
8.2
12.4
10.4
8.4
0.8
5.9
8.1
0.4
4.9
1.9
7.6
0.9
6.2
2.8
2.0
13.0
6.7
0.0
16.0
0.0
3.9
14.4
2.0
1.2
1.8
4.9
15.1
12.9
5.4
0.0
5.6
6.1

efmax
16.9
3.6
1.6
8.5
1.7
1.3
36.5
11.4
40.1
35.0
18.4
18.3
23.0
6.5
7.1
15.7
2.0
12.3
2.7
11.1
4.1
36.9
36.5
5.8
36.7
14.7
3.5
33.8
2.9
12.2
17.4
10.3
36.9
4.8
6.4
19.3
25.6
36.3
1.2
18.0
9.7

ADV
auc
ef2% ef20%
0.62
2.0
1.4
0.38
3.0 0.68
1.1
0.47
2.5 0.70
0.23
0.2
0.74
3.5
0.66
2.1
1.4
0.41
3.6 0.79
0.84
3.8
0.86
3.5
0.58
1.5
0.79
3.3
0.66
2.3
0.48
1.0
0.66
1.5
2.6
0.77
1.2 0.52
0.57
1.2
0.72
2.6
0.64
1.8
0.42
0.6
0.54
0.8
1.8
0.54
4.0 0.82
0.40
0.5
2.2
0.62
2.9 0.74
0.57
1.7
0.48
1.1
0.66
1.8
0.64
1.7
1.1
0.43
4.2 0.93
0.80
3.2
0.69
2.2
0.73
2.8
0.55
0.2
0.63
1.6
0.56
1.6

6.0
3.0
4.8
0.0
3.8
0.0
14.2
8.9
8.9
10.3
21.4
8.2
2.5
16.6
8.9
0.0
2.1
0.0
2.9
4.4
6.6
6.2
0.0
0.0
11.2
20.0
0.0
1.4
4.6
6.9
5.4
2.0
3.1
0.0
24.3
10.5
2.9
6.2
0.0
3.4
5.4

efmax
16.4
9.1
5.4
1.1
8.2
1.0
36.5
18.3
40.1
20.0
36.8
36.5
4.9
17.7
13.8
1.1
3.2
2.9
3.1
18.4
36.6
24.6
1.0
1.3
36.7
36.7
1.1
2.8
36.6
36.5
36.8
3.0
4.1
1.5
36.4
13.5
4.0
18.1
1.5
9.8
36.7

efmax
16.0
2.2
4.0
9.2
5.7
1.1
21.9
6.1
20.0
5.0
36.8
9.1
8.6
8.1
7.4
36.6
4.3
2.6
1.0
20.3
6.2
7.4
36.5
10.0
6.7
24.4
2.1
21.6
1.7
36.5
36.8
4.2
36.9
8.5
12.1
19.9
19.2
36.3
1.4
36.1
36.7

6.6
1.0
1.9
2.2
3.8
0.0
11.5
4.0
8.9
2.1
12.7
6.7
5.5
6.0
3.8
7.7
1.4
0.0
0.0
10.8
5.6
4.9
19.6
4.1
16.6
16.7
0.0
16.4
0.0
8.9
19.2
0.0
2.5
5.5
9.7
18.1
9.7
6.2
0.0
6.8
4.8

auc
3.1 0.81
0.72
1.8
0.51
1.1
0.83
3.3
0.66
1.3
0.0
0.42
4.2 0.88
2.9
0.79
4.6 0.92
0.77
2.8
4.3 0.91
4.8 0.94
3.6
0.86
3.9 0.88
0.88
3.8
3.3
0.85
3.9 0.86
0.77
2.4
0.9
0.42
4.4 0.91
4.1 0.88
2.3 0.73
4.9 0.96
5.0 0.94
4.5 0.94
3.3 0.82
0.68
1.0
0.87
3.9
0.8
0.65
3.1 0.86
0.91
3.7
4.0
0.86
4.4 0.87
2.4 0.77
3.0
0.85
4.7 0.95
3.4
0.85
3.2 0.83
0.2
0.54
2.4 0.80
4.1 0.90

Dock, whose input is based on DOCK6.6 (avrg.
AUC of 0.48). On the other hand, there were
some cases where the AUC of docking program
was very poor, but DeepVS was able to boost
the AUC result signiﬁcantly. For instance, al-
though DOCK6.6 produced an AUC < 0.40 for
the receptors AR, COX1, HSP90, InhA, PDE5,
PDGFrb and PR, DeepVS-Dock resulted in an
AUC > 0.70 for these receptors.

In Figure 5 we compare the AUC of DeepVS-
ADV and ADV for the 40 receptors. DeepVS-
ADV achieved AUC > 0.70 for 33 receptors,
while this number was only 13 for ADV. The
number of receptors with AUC < 0.50 was 2
for DeepVS-ADV and 9 for ADV. The AUC
of DeepVS-ADV was higher than the one for

ADV for 31 receptors.
In average, the AUC
of DeepVS-ADV (0.81) was 31% better than
the one for ADV (0.62). Additionally, when
we selected 20% of the data according to the
ranked compounds, in average DeepVS-ADV’s
ef (3.1) was 55% larger than the ef from the
ADV (2.0).

A comparison of the AUC from DeepVS-Dock
and Dock6.6 for the 40 receptors is presented
in Figure 6. In average, the AUC of DeepVS-
Dock (0.74) was 54% better than the one for
the Dock6.6 (0.48). While Dock6.6 achieved
AUC > 0.70 for 10% (4) of the receptors only,
DeepVS-Dock reached AUC > 0.70 for 68%
(27) of the receptors. The number of receptors
with AUC < 0.50 was 5 for DeepVS-Dock and

10

Figure 5: DeepVS-ADV vs AutodockVina1.1.2
AUC. The circles represent each of the 40 DUD
receptors.

23 for Dock6.6. The AUC of DeepVS-Dock was
higher than the one for Dock6.6 for 36 recep-
tors. Finally, when we select 20% of the data
according to the ranked compounds, in aver-
age DeepVS-Dock’s ef (3.0) was more than two
times larger than the ef from Dock6.6 (1.3).

ent docking programs, are strong evidence that
DeepVS can be an eﬀective approach for im-
proving docking-based virtual screening.

DeepVS Sensitivity to Hyperpa-
rameters

In this section, we present experimental re-
sults regarding an investigation on the sensi-
tivity of DeepVS concerning the main hyper-
parameters.
In our experiments, we used the
ADV output as input to DeepVS. Therefore, all
results reported in this section were generated
using DeepVS-ADV. However, we noticed that
DeepVS behaved in a similar manner when the
input from Dock6.6 was used.
In the experi-
ments, we varied one of the hyper-parameters
and ﬁxed all the others to the following values:
datm/amino/chrg/dist = 200, cf = 400, λ = 0.075,
kc = 6 and kp = 2.

In Table 3, we present the experimental re-
sults of varying the basic feature embedding
sizes. We can see that embedding sizes larger
than 50 improved mainly the ef . Embeddings
larger than 200 did not improve the results.

Table 3: DeepVS sensitivity to basic feature
embedding sizes.

datm/amino/chrg/dist
50
100
200

efmax
15.4
15.6
16.0

ef2% ef20% auc
3.0
6.5
3.1
6.8
3.1
6.6

0.803
0.799
0.807

The experimental results of varying the num-
ber of ﬁlters (cf ) in the convolutional layer are
presented in Table 4. Notice that the AUC
improves by increasing the number of convo-
lutional ﬁlters up to 400. On the other hand,
using cf = 200 results in the best ef2%.

Table 4: DeepVS sensitivity to # of ﬁlters in
the convolutional layer.

Figure 6: DeepVS-Dock vs DOCK6.6 AUC.
The circles represent each of the 40 DUD re-
ceptors.

The experimental results presented in this
section, which include outputs from two diﬀer-

cf
100
200
400
800

efmax
14.7
16.3
16.0
16.8

ef2% ef20% auc
3.0
6.4
3.0
7.3
3.1
6.6
3.1
7.0

0.797
0.799
0.807
0.804

11

In Table 5, we present the experimental re-
sults of DeepVS trained with diﬀerent learning
rates. We can see that larger learning rates
work better for the DUD dataset. A learn-
ing rate of 0.1 resulted in the best outcomes
in terms of AUC and ef .

Table 5: DeepVS sensitivity to the learning
rate.

λ
0.1
0.075
0.05
0.025
0.01

efmax
17.3
16.0
15.5
14.7
15.7

ef2% ef20% auc
3.2
6.9
3.1
6.6
3.0
6.6
3.0
6.4
3.1
6.2

0.809
0.807
0.801
0.795
0.800

We also investigated the impact of using a dif-
ferent number of neighbors atoms from the com-
pound (kc) and the receptor (kp). In Table 6,
we present some experimental results where we
vary both kc and kp. For instance, with kc = 0
and kp = 5, it means that no information from
the compound is used, while information from
the 5 closest atoms from the receptor is used.
In the ﬁrst half of the Table, we keep kp ﬁxed to
5, and vary kc. In the second half of the Table,
we keep kc ﬁxed to 6, and vary kp. As we can
notice in the ﬁrst half of Table 6, by increasing
the number of neighbor atoms we use from the
compound (kc), we signiﬁcantly increase both
ef and AUC. In the second half of the Ta-
ble, we can notice that using kp > 2 degrades
DeepVS-ADV performance. As we conjecture
in the next section, this behavior seems to be
related to the quality of the docking program
output.

In order to assess the robustness of DeepVS
with regard to the initialization of the weight
matrices (network parameters), we performed
10 diﬀerent runs of DeepVS-ADV using a dif-
In Table 7,
ferent random seed in each run.
we present the experimental results of these 10
runs. We can see in the Table that the standard
deviation is very small for both ef and AUC,
which demonstrates the robustness of DeepVS
to diﬀerent random seeds.

Table 6: DeepVS sensitivity to # of neighbor
atoms selected from compound/protein.

kc
0
1
2
3
4
5
6
6
6
6
6
6

kp
5
5
5
5
5
5
5
4
3
2
1
0

efmax
6.99
15.55
15.44
16.74
17.38
19.07
17.89
17.02
16.44
16.03
16.03
16.92

ef2% ef20%
1.54
2.18
2.27
4.22
2.59
5.62
2.76
6.38
2.91
6.25
3.18
6.47
3.04
6.79
3.17
6.38
2.99
6.82
3.14
6.62
3.13
6.99
3.06
6.95

auc
0.574
0.697
0.743
0.752
0.782
0.799
0.799
0.801
0.793
0.807
0.806
0.803

Table 7: DeepVS sensitivity to diﬀerent random
seeds.

Run efmax
15.97
0.73
16.53
16.32
15.30
14.92
17.08
16.82
15.90
15.59
16.08
15.18

average
stdv
1
2
3
4
5
6
7
8
9
10

ef2% ef20%
3.10
6.82
0.04
0.16
3.19
6.88
3.06
6.63
3.10
6.76
3.12
6.89
3.10
6.74
3.05
6.84
3.07
6.52
3.06
7.00
3.12
6.86
3.11
7.04

auc
0.805
0.003
0.807
0.799
0.805
0.807
0.807
0.804
0.803
0.805
0.806
0.803

Docking Quality vs. DeepVS Per-
formance

In the experimental results reported in Table 6,
we can notice that, when creating the atom
contexts, using kp > 2 (number of neighbor
atoms coming from the protein) does not lead
if we use
to improved AUC or ef .
only information from the compound (kp = 0),
which is equivalent to perform ligand based vir-
tual screening, the AUC is already very good
(0.803). We hypothesize that this behavior is
related to the quality of the docking output,
which varies a lot across the 40 DUD proteins.
In an attempting to test this hypothesis, we

In fact,

12

separately analyse the AUC of DeepVS for the
DUD proteins for which the AUC of ADV is
good (Table 8) or poor (Table 9).

In Table 8, we present the AUC of DeepVS-
ADV for proteins whose AUC of ADV is larger
than 0.75. We present results for three diﬀer-
In the
ent values of kp, namely, 0, 2 and 5.
three experiments we use kc = 6. We can notice
that for proteins that have a good docking qual-
ity (and likely the protein-compound complexes
have good structural information) the average
AUC of DeepVS-ADV increases as we increase
kp. This result suggests that if the structural in-
formation is good, the neural network can ben-
eﬁt from it.

Table 8: DeepVS-ADV results for proteins with
good docking quality.

auc > 0.75
kp = 0 kp = 2 kp = 5
0.87
0.86
0.83
0.80
0.77
0.78
0.91
0.91
0.89
0.96
0.94
0.96
0.89
0.88
0.89
0.77
0.77
0.78
0.80
0.82
0.80
0.90
0.85
0.64
0.95
0.95
0.93

average
COX1
COX2
DHFR
ERagonist
GART
MR
RXRa
SAHH

In Table 9, we present the AUC of DeepVS-
ADV for proteins whose AUC of ADV is smaller
than 0.5. For these proteins, which have a
poor docking quality (and likely the protein-
compound complexes have poor structural in-
formation) the average AUC of DeepVS-ADV
decreases as we increase kp. This result sug-
gests that if the structural information is poor,
the neural network works better without using
it.

Comparison with State-of-the-art
System

In this section, we compare DeepVS results
with the ones reported in previous work that
also employed DUD. First, we perform a de-
tailed comparison between DeepVS and the

13

Table 9: DeepVS-ADV results for proteins with
poor docking quality.

auc < 0.50
kp = 0 kp = 2 kp = 5
0.77
0.78
0.80
0.66
0.72
0.71
0.83
0.83
0.80
0.46
0.42
0.59
0.89
0.92
0.92
0.79
0.85
0.83
0.96
0.96
0.97
0.66
0.68
0.67
0.91
0.91
0.91
0.79
0.77
0.81

average
ACE
ADA
AmpC
COMT
FGFrl
HMGR
NA
PDGFrb
PR

Docking Data Feature Analysis (DDFA) sys-
tem, 7 which also applied neural networks on the
output of docking programs to perform virtual
screening. Next, we compare the average AUC
of DeepVS with one of other systems that also
report results for the 40 DUD receptors.

DDFA uses a set of human deﬁned features
that are derived from docking output data. Ex-
amples of the features employed in DDFA are:
compound eﬃciency, the best docking score of
the compound poses and the weighted average
of the best docking scores of the ﬁve most sim-
ilar compounds in the docked library. The fea-
tures are given as input to a shallow neural net-
work that classiﬁes the input protein-compound
complex as an active or inactive ligand. DDFA
uses data from the six best poses output by
the docking program, while in DeepVS we use
data from the best pose only. In Figure 7, we
compare the AUC of DeepVS-ADV vs DDFA-
ADV, which is the version of DDFA that uses
In this ﬁgure,
the output of AutodockVina.
each circle represents one of the 40 DUD recep-
tors. DeepVS-ADV produces higher AUC than
DDFA-ADV for 27 receptors, which represents
67.5% of the dataset.

DDFA-ALL is a more robust version of
DDFA 7 that uses simultaneously the output of
three diﬀerent docking programs: Autodock4.2
(AD4), AutodockVina1.1.2 (ADV) and Roset-
taLigand3.4 (RL). Therefore, DDFA-ALL uses
three times more input features than DDFA-
ADV. In Figure 8, we compare the AUC of

mercial docking softwares ICM and Glide SP.
NNScore1-ADV and NNScore2-ADV are also
based on shallow neural networks that use
human deﬁned features and the output of
AutodockVina.
It is worth to note that
NNScore1-ADV and NNScore2-ADV 11 results
are based in a diﬀerent set of decoys that are
simpler than the ones available in DUD. There-
fore, these results are not 100% comparable
with other results on the table. To the best
of our knowledge, the AUC of DeepVS-ADV is
the best reported so far for virtual screening
using the 40 receptors from DUD.

Table 10: Reported performance of diﬀerent
systems on DUD.

System
auc
DeepVS-ADV
0.81
ICM 43 b
0.79
NNScore1-ADV 11 a
0.78
Glide SP 44 b
0.77
DDFA-ALL 7
0.77
DDFA-RL 7
0.76
NNScore2-ADV 11 a
0.76
DDFA-ADV 7
0.75
DeepVS-Dock
0.74
DDFA-AD4 7
0.74
Glide HTVS 11 a
0.73
Surﬂex 44 b
0.72
Glide HTVS 44
0.72
ICM 43
0.71
RAW-ALL 7
0.70
Autodock Vina 11 a
0.70
Surﬂex 44
0.66
RosettaLigand 7
0.65
Autodock Vina 7
0.64
ICM 44
0.63
Autodock Vina
0.62
FlexX 44
0.61
Autodock4.2 7
0.60
PhDOCK 44
0.59
Dock4.0 44
0.55
Dock6.6
0.48
a Used a diﬀerent dataset of decoys.
b Tuned by expert knowledge.

Figure 7: DeepVS-ADV vs DDFA-ADV.

DeepVS-ADV vs DDFA-ALL. Despite using
data from one docking program only, DeepVS-
ADV produces higher AUC than DDFA-ALL
for 25 receptors, which represents 62.5% of
the dataset. This is a strong indication that
DeepVS-ADV result for DUD dataset is very
robust.

Figure 8: DeepVS-ADV vs DDFA-ALL.

In Table 10, we compare the average AUC
of DeepVS and the docking programs with
the ones from other systems reported in the
literature. DeepVS-ADV produced the best
AUC among all systems, outperforming com-

14

Conclusions

In this work, we introduce DeepVS, a deep
learning approach to improve the performance
of docking-based virtual screening.
Using
DeepVS on top of the docking output of
AutodockVina we were capable of producing
the best AUC reported so far for virtual screen-
ing on the DUD dataset. This result, together
with the fact that (1) DeepVS does not require
human deﬁned features, and (2) it achieves
good results using the output of a single dock-
ing program, makes DeepVS an attractive ap-
proach for virtual screening. Moreover, diﬀer-
ent from other methods that use shallow neural
networks with few parameters, DeepVS has a
larger potential for performance improvement
if more data is added to the training set. Deep
Learning systems are usually trained with large
amounts of data. Although the number of
protein-compound complexes in DUD is rela-
tively large (more than 100k), the number of
diﬀerent proteins is still very small (only 40).

Additionally, this work also brings some very
innovative ideas on how to model the protein-
compound complex raw data to be used in a
deep neural network. We introduce the idea of
atom and amino acid embeddings, which can
also be used in other deep learning applications
for bioinformatics. Moreover, our idea of mod-
eling the compound as a set of atom contexts
that is further processed by a convolution layer
proved to be an eﬀective approach to learn-
ing representations of protein-compound com-
plexes.

Acknowledgement J.C.P. is funded through
a Ph.D. scholarship from the Oswaldo Cruz
E.R.C’s research is supported
Foundation.
by following grants: Faperj/E-26/111.401/2013
and CNPq. Papes VI./407741/2012-7.

References

(1) Hecht, D.; Fogel, G. B. Computational
Intelligence Methods for Docking Scores.
Current Computer-Aided Drug Design
2009, 5, 56–68.

15

(2) Walters, W.; Stahl, M. T.; Murcko, M. A.
Virtual Screening - An Overview. Drug
Discovery Today 1998, 3, 160 – 178.

S.

(3) Cheng, T.; Li, Q.; Zhou, Z.; Wang, Y.;
Bryant,
Structure-Based Virtual
Screening for Drug Discovery: a Problem-
Centric Review. The AAPS journal 2012,
14, 133–141.

(4) Shoichet, B. K. Virtual Screening of
Chemical Libraries. Nature 2004, 432,
862–865.

(5) Ghosh, S.; Nie, A.; An, J.; Huang, Z.
Structure-Based Virtual Screening
of
Chemical Libraries for Drug Discovery.
Curr Opin Chem Biol 2006, 10, 194–202.

(6) Bissantz, C.; Folkers, G.; Rognan, D.
Protein-Based Virtual Screening of Chem-
ical Databases. 1. Evaluation of Diﬀerent
Docking/Scoring Combinations. Journal
of Medicinal Chemistry 2000, 43, 4759–
4767, PMID: 11123984.

(7) Arciniega, M.; Lange, O. F. Improvement
of Virtual Screening Results by Docking
Data Feature Analysis. Journal of Chem-
ical Information and Modeling 2014, 54,
1401–1411.

(8) Drwal, M. N.; Griﬃth, R. Combination of
Ligand- and Structure-Based Methods in
Virtual Screening. Drug Discovery Today:
Technologies 2013, 10, e395 – e401.

(9) Schneider, G. Virtual Screening: An End-
less Staircase? Nature Reviews Drug Dis-
covery 2010, 9, 273–276.

(10) Kitchen, D. B.; Decornez, H.; Furr, J. R.;
Bajorath, J. Docking and Scoring in Vir-
tual Screening for Drug Discovery: Meth-
ods and Applications. Nature Reviews
Drug Discovery 2004, 3, 935–949.

(11) Durrant,

J. D.;

Friedman, A.

J.;
Rogers, K. E.; McCammon, J. A. Com-
paring Neural-Network Scoring Functions
and the State of the Art: Applications
to Common Library Screening. Journal

of Chemical Information and Modeling
2013, 53, 1726–1735.

(12) Kinnings, S. L.; Liu, N.; Tonge, P. J.;
Jackson, R. M.; Xie, L.; Bourne, P. E. A
Machine Learning-Based Method to Im-
prove Docking Scoring Functions and Its
Application to Drug Repurposing. Jour-
nal of Chemical Information and Modeling
2011, 51, 408–419.

(13) Durrant, J. D.; McCammon, J. A.
NNScore: A Neural-Network-Based Scor-
ing Function for
the Characterization
of Protein-Ligand Complexes. Journal
of Chemical Information and Modeling
2010, 50, 1865–1871, PMID: 20845954.

(14) Ballester, P. J.; Mitchell, J. B. A Ma-
chine Learning Approach to Predicting
Protein–Ligand Binding Aﬃnity with Ap-
plications to Molecular Docking. Bioinfor-
matics 2010, 26, 1169–1175.

(15) Bengio, Y.; Courville, A.; Vincent, P. Rep-
resentation Learning: A Review and New
Perspectives. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on
2013, 35, 1798–1828.

(16) LeCun, Y.; Bengio, Y.; Hinton, G. Deep
Learning. Nature 2015, 521, 436–444.

(17) Bengio, Y. Learning Deep Architectures
for AI. Foundations and Trends R(cid:13) in Ma-
chine Learning 2009, 2, 1–127.

(18) Dahl, G. E.;

Jaitly, N.; Salakhutdi-
nov, R. Multi-Task Neural Networks
for QSAR Predictions. arXiv preprint
arXiv:1406.1231 2014,

(19) Unterthiner, T.; Mayr, A.; Klambauer, G.;
Steijaert, M.; Wegner, J. K.; Ceule-
mans, H.; Hochreiter, S. Deep Learning as
an Opportunity in Virtual Screening. Pro-
ceedings of the Deep Learning Workshop
at NIPS. 2014.

(20) Unterthiner, T.; Mayr, A.; Klam-
bauer, G.; Hochreiter, S. Toxicity Predic-
tion Using Deep Learning. arXiv preprint
arXiv:1503.01445 2015,

(21) Ramsundar, B.;

S.;

Kearnes,

Ri-
ley, P.; Webster, D.; Konerding, D.;
Pande, V. Massively Multitask Net-
works for Drug Discovery. arXiv preprint
arXiv:1502.02072 2015,

(22) Lusci, A.; Pollastri, G.; Baldi, P.
Deep Architectures and Deep Learn-
ing in Chemoinformatics:
the Predic-
tion of Aqueous Solubility for Drug-like
Molecules. Journal of Chemical Informa-
tion and Modeling 2013, 53, 1563–1575.

(23) Duvenaud, D. K.; Maclaurin, D.; Ipar-
raguirre, J.; Bombarell, R.; Hirzel, T.;
Aspuru-Guzik, A.; Adams, R. P. Convo-
lutional Networks on Graphs for Learning
Molecular Fingerprints. Advances in Neu-
ral Information Processing Systems. 2015;
pp 2215–2223.

(24) Durrant, J. D.; McCammon, J. A.
A Neural-Network
Function.
Scoring
Information and

NNScore
Receptor-Ligand
Journal of Chemical
Modeling 2011, 51, 2897–2903.

2.0:

(25) Weber, J.; Achenbach, J.; Moser, D.;
Proschak, E. VAMMPIRE: a Matched
Molecular Pairs Database for Structure-
Based Drug Design and Optimization.
Journal of medicinal chemistry 2013, 56,
5203–5207.

(26) Collobert, R.; Weston, J.; Bottou, L.;
Karlen, M.; Kavukcuoglu, K.; Kuksa, P.
Natural Language Processing (Almost)
from Acratch. The Journal of Machine
Learning Research 2011, 12, 2493–2537.

(27) Socher, R.; Huval, B.; Manning, C. D.;
Ng, A. Y. Semantic Compositionality
Through Recursive Matrix-Vector Spaces.
Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Lan-
guage Processing and Computational Nat-
ural Language Learning. 2012; pp 1201–
1211.

(28) Mikolov, T.; Sutskever, I.; Chen, K.;
Corrado, G. S.; Dean, J. Distributed

16

Representations of Words and Phrases
and their Compositionality. Advances in
Neural Information Processing Systems
(NIPS) 2013,

(29) dos Santos, C. N.; Gatti, M. Deep Con-
volutional Neural Networks for Sentiment
Analysis of Short Texts. COLING. 2014;
pp 69–78.

(30) dos Santos, C. N.; Zadrozny, B. Learn-
ing Character-Level Representations for
Part-of-Speech Tagging. Proceedings of
the 31st International Conference on Ma-
chine Learning (ICML-14). 2014; pp 1818–
1826.

(31) Waibel, A.; Hanazawa, T.; Hinton, G.;
Shikano, K.; Lang, K. J. Phoneme Recog-
nition Using Time-Delay Neural Net-
works. Acoustics, Speech and Signal Pro-
cessing, IEEE Transactions on 1989, 37,
328–339.

(32) Rumelhart, D. E.; Hinton, G. E.;
Williams, R. J. Learning Representations
by Back-Propagating Errors. Cognitive
Modeling 1988, 5, 1.

(33) Bergstra, J.; Breuleux, O.; Bastien, F.;
Lamblin, P.; Pascanu, R.; Desjardins, G.;
Turian, J.; Warde-Farley, D.; Bengio, Y.
a CPU and GPU Math Ex-
Theano:
pression Compiler. Proceedings of the
Python for Scientiﬁc Computing Confer-
ence (SciPy). 2010; p 3.

(34) Huang, N.; Shoichet, B. K.; Irwin, J. J.
Benchmarking Sets for Molecular Dock-
ing. Journal of medicinal chemistry 2006,
49, 6789–6801.

(35) Armstrong, M. S.; Morris, G. M.;
Finn, P. W.; Sharma, R.; Moretti, L.;
Cooper, R. I.; Richards, W. G. Elec-
troShape: Fast Molecular Similarity Cal-
culations Incorporating Shape, Chirality
and Electrostatics. Journal of Computer-
aided Molecular Design 2010, 24, 789–
801.

(36) Lang, P. T.; Brozell, S. R.; Mukherjee, S.;
Pettersen, E. F.; Meng, E. C.; Thomas, V.;
Rizzo, R. C.; Case, D. A.; James, T. L.;
Kuntz, I. D. DOCK 6: Combining Tech-
niques to Model RNA–Small Molecule
Complexes. Rna 2009, 15, 1219–1230.

(37) Trott, O.; Olson, A. J. AutoDock Vina:
Improving the Speed and Accuracy of
Docking with a New Scoring Function,
Eﬃcient Optimization, and Multithread-
ing. Journal of Computational Chemistry
2010, 31, 455–461.

(38) Pettersen, E. F.; Goddard, T. D.;
Huang, C. C.; Couch, G. S.; Green-
blatt, D. M.; Meng, E. C.; Ferrin, T. E.
UCSF Chimera - A Visualization Sys-
tem for Exploratory esearch and Analy-
sis. Journal of Computational Chemistry
2004, 25, 1605–1612.

(39) Morris, G. M.; Huey, R.; Lindstrom, W.;
Sanner, M. F.; Belew, R. K.; Good-
sell, D. S.; Olson, A. J. AutoDock4 and
AutoDockTools4: Automated Docking
with Selective Receptor Flexibility. Jour-
nal of Computational Chemistry 2009, 30,
2785–2791.

(40) Arlot, S.; Celisse, A. A Survey of Cross-
Validation Procedures for Model Selec-
tion. Statistics Surveys 2010, 4, 40–79.

(41) Jahn, A.; Rosenbaum, L.; Hinselmann, G.;
Zell, A. 4D Flexible Atom-Pairs: An Eﬃ-
cient Probabilistic Conformational Space
Comparison for Ligand-Based Virtual
Screening. J. Cheminformatics 2011, 3,
23.

(42) Nicholls, A. What do We Know and when
do We Know It? Journal of Computer-
Aided Molecular Design 2008, 22, 239–
255.

(43) Neves, M. A.; Totrov, M.; Abagyan, R.
Docking and Scoring with ICM: The
Benchmarking Results and Strategies for
Improvement. Journal of Computer-Aided
Molecular Design 2012, 26, 675–686.

17

(44) Cross, J. B.; Thompson, D. C.; Rai, B. K.;
Baber, J. C.; Fan, K. Y.; Hu, Y.; Hum-
blet, C. Comparison of Several Molecu-
lar Docking Programs: Pose Prediction
and Virtual Screening Accuracy. Journal
of Chemical Information and Modeling
2009, 49, 1455–1474.

18

