JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

FSD-10: A Dataset for Competitive Sports Content
Analysis

Shenglan Liu Member, IEEE, Xiang Liu, Gao Huang, Lin Feng, Lianyu Hu, Dong Jiang, Aibin Zhang, Yang Liu,

Hong Qiao Fellow, IEEE

0
2
0
2

b
e
F
9

]

V
C
.
s
c
[

1
v
2
1
3
3
0
.
2
0
0
2
:
v
i
X
r
a

Abstractâ€”Action recognition is an important and challeng-
ing problem in video analysis. Although the past decade has
witnessed progress in action recognition with the development
of deep learning, such process has been slow in competitive
sports content analysis. To promote the research on action
recognition from competitive sports video clips, we introduce a
Figure Skating Dataset (FSD-10) for ï¬negrained sports content
analysis. To this end, we collect 1484 clips from the worldwide
ï¬gure skating championships in 2017-2018, which consist of 10
different actions in men/ladies programs. Each clip is at a rate
of 30 frames per second with resolution 1080 Ã— 720. These clips
are then annotated by experts in type, grade of execution, skater
info,
.etc. To build a baseline for action recognition in ï¬gure
skating, we evaluate state-of-the-art action recognition methods
on FSD-10. Motivated by the idea that domain knowledge is
of great concern in sports ï¬eld, we propose a keyframe based
temporal segment network (KTSN) for classiï¬cation and achieve
remarkable performance. Experimental results demonstrate that
FSD-10 is an ideal dataset for benchmarking action recognition
algorithms, as it requires to accurately extract action motions
rather than action poses. We hope FSD-10, which is designed
to have a large collection of ï¬negrained actions, can serve as
a new challenge to develop more robust and advanced action
recognition models.

I. INTRODUCTION
Due to the popularity of media-sharing platforms(e.g.
YouTube), sports content analysis (SCA[31]) has become
an important research topic in computer vision [39][10][30].
A vast amount of sports videos are piled up in computer
storage, which are potential resources for deep learning. In
recent years, many enterprises (e.g. Bloomberg, SAP) have
focus on SCA[31]. In SCA, datasets are required to reï¬‚ect
characteristics of competitive sports, which is a guarantee for
training deep learning models. Generally, competitive sports
content is a series of diversiï¬ed, high professional and ulti-
mate actions. Unfortunately, existing trending human motion

Shenglan Liu, Xiang Liu, Lin Feng, Lianyu Hu, Dong Jiang, Aibin
Zhang, Yang Liu are with Faculty of Electronic Information and Elec-
trical Engineering, Dalian University of Technology, Dalian, Liaoning,
116024 China. Email: ({liusl, hly19961121, s201461147, 18247666912,
dlut liuyang}@mail.dlut.edu.cn, fenglin@dlut.edu.cn, xliudut@gmail.com).

Gao Huang is with Faculty of Tsinghua University of Technology, Beijing,

China. Email: gaohuang@tsinghua.edu.cn.

H. Qiao is with the State Key Laboratory of Management and Control for
Complex Systems, Institute of Automation, Chinese Academy of Sciences,
Beijing 100190 China. E-mail: hong.qiao@ia.ac.cn.

This work is supported in part by The National Key Research and Develop-
ment Program of China (2017YFB1300200) National Natural Science Foun-
dation of Peopleâ€™s Republic of China (No. 61672130, 61602082, 91648205,
31871106), the National Key Scientiï¬c Instrument and Equipment Develop-
ment Project (No. 61627808), the Development of Science and Technology
of Guangdong Province Special Fund Project Grants (No. 2016B090910001),
the LiaoNing Revitalization Talents Program (No.XLYC1086006).

datasets (e.g. HMDB51[18], UCF50[34]) or action datasets of
human sports (e.g. MIT Olympic sports[28], Nevada Olympic
sports[24]) are not quite representative of the richness and
complexity of competitive sports. The limitations of current
datasets could be summarised as follows: (1) Current tasks
of human action analysis are insufï¬cient, which is limited to
action types and contents of videos. (2) In most classiï¬cation
tasks, discriminant of an action largely depends on both static
human pose and background; (3) Video segmentation, as an
important task of video understanding (including classiï¬cation
and assessment
is rarely discussed in current
datasets.

in SCA ),

To address the above issues, this paper proposes a ï¬gure
skating dataset called FSD-10. FSD-10 consists of 1484 ï¬gure
skating videos with 10 different actions manually labeled.
These skating videos are segmented from around 80 hours
of competitions of worldwide ï¬gure skating championships
1 in 2017-2018. FSD-10 videos range from 3 seconds to 30
seconds, and the camera is moving to focus the skater to ensure
that he (she) appears in each frame during the process of
actions. Compared with existing datasets, our proposed dataset
has several appealing properties. First, actions in FSD-10 are
complex in content and fast in speed. For instance, the complex
2-loop-Axel jump in Fig. 1 is ï¬nished in only about 2s. Itâ€™s
worth note that the jump type heavily depends on the take off
process, which is a hard-captured moment. Second, actions of
FSD-10 are original from ï¬gure skating competitions, which
are consistent in type and sports environment (only skater
and ice around). The above two aspects create difï¬culties for
machine learning model to conclude the action types by a
single pose or background. Third, FSD-10 can be applied to
standardized action quality assessment (AQA) tasks. The AQA
of FSD-10 actions depends on base value (BV) and grade of
execution (GOE), which increases the difï¬culty of AQA by
both action qualities and rules of International Skating Union
(ISU)2. Last, FSD-10 can be extended to more tasks than the
mentioned datasets.

Along with the introduction of FSD-10, we introduce a key
frame indicator called human pose scatter (HPS). Based on
HPS, we adopt key frame sampling to improve current video
classiï¬cation methods and evaluate these methods on FSD-

1ISU Grand Prix
Four

Championships,
(https://www.isu.org/ï¬gure-skating)

of Figure Skating, European Figure Skating
Championships

Continents

Skating

Figure

2One action appearing in second half of program will earn extra rate of
10% score of BV than that appearing in ï¬rst half of program. This means
that same action may get different BV score because of the rules.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Fig. 1. An over view of the ï¬gure skating dataset. It is illustrated that warm up, take off, spinning in the air and landing are 4 steps of an Axel jump. Î¶HP S
indicates the human pose scatter of a gesture, which is introduced in Sec IV-B.

TABLE I
A SUMMARY OF EXISTING ACTION DATASETS

Dataset

UCF Sports[29]
Hollywood2[21]
UCF50[34]
HMDB51[18]
MIT Olympic sports[28]
Nevada Olympic sports[25]
AQA-7[24]
FSD-10

Year

2009
2009
2010
2011
2015
2017
2018
2019

Actions

Clips

9
12
50
51
2
3
7
10

14-35
61-278
min. 100
min. 101
150-159
150-370
83-370
91-233

Classiï¬cation
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš

Score

Temporal Segmentation

-
-
-
-
âˆš
âˆš
âˆš
âˆš

-
-
-
-
-
-
-
âˆš

10. Furthermore, experimental results validate that key frame
sampling is an important approach to improve performance of
frame-based model in FSD-10, which is in concert with cogni-
tion rules of human in ï¬gure skating. The main contributions
of this paper can be summarised as follows.

â€¢ To our best knowledge, FSD-10 is the ï¬rst challenging
dataset with high-speed, motion-focused and complex
sport actions in competitive sports ï¬eld, which introduces
multiple tasks for computer vision, such as (ï¬ne grained)
classiï¬cation, long/short temporal human motion segmen-
tation, action quality assessments, and program content
assessment in time to music.

â€¢ To set a baseline for future achievements, we also bench-
mark state-of-the-art sport classiï¬cation methods on FSD-
the key frame sampling is proposed to
10. Besides,
capture the pivotal action details in competitive sports,
which achieves better performance than state-of-the-art
methods in FSD-10.

In addition,compared to current datasets, we hope FSD-10
will be a standard dataset which promotes the research on
background-independent action recognition, allowing models
to be more robust to unexpected events and generalize to novel
situations and tasks.

II. RELATED WORKS

Current action datasets could be divided into human action
dataset (HAD) and human professional sports dataset (HPSD),
which are listed in Tab. I. HAD consists of different kinds of
broadly deï¬ned human motion clips, such as haircut and long
jump. For example, UCF101[34] and HMDB[18] are typical
cases of HAD, which have greatly promoted the process of
video classiï¬cation techniques. In contrast, HPSD is a series of
competitive sports actions, which is proposed for action quality
assessment (AQA) tasks except classiï¬cation. MIT Olympic
sports[28] and Nevada Olympic sports[25] are examples of
HPSD, which are derived from Olympic competitions.

Classiï¬cation in HPSD is important to attract peopleâ€™s atten-
tion and to highlight athleteâ€™s performance, and even to assist
referees [16][46][1]. The signiï¬cant difference between video
classiï¬cation and image classiï¬cation is logical connection
relationship in video clips [2], which involve motion besides
form (content) of frames. Therefore, human action classiï¬-
cation task should be implemented based on temporal human
information without background rather than static action frame
information. H.kuehne et al. [18] mentioned that the recogni-
tion rate of static joint locations alone in UCF sports [29]
dataset is above 98% while the use of joint kinematics is not
necessary. The result above obviously violates to the principle

Warm upğœğ»ğ‘ƒğ‘†(82.89)Take offğœğ»ğ‘ƒğ‘†(187.15)Spin in the air ğœğ»ğ‘ƒğ‘†(61.88)Landingğœğ»ğ‘ƒğ‘†(152.63)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 2. 10 ï¬gure skating actions included in FSD-10 shown with 5 frames. The color of frame borders speciï¬es to which action category they belong (Spin,
Sequence, Jump) and the labels speciï¬es their action types.

of video classiï¬cation task. Thus, both motion and pose are
indispensable modalities in human action classiï¬cation [7].

Besides classiï¬cation of HPSD, AQA is an important but
unique task in HPSD. Some AQA datasets have been proposed,
such as MIT Olympic sports dataset[28], Nevada Olympic
sports dataset[25] and AQA-7[24], which predict the action
performance of diving, gymnastic vault, etc. In this ï¬eld,
Parmar et al. [26] adopts C3D features combining regression
with the help of SVR[6] (LSTM[12]), which gets satisfactory
results. The development direction of HPSD is clearly given
by the above works while AQA still remains to be devel-
oped as the materials of these datasets is still inadequate.
the functions of datasets are
It could be concluded that
gradually expanded with the development of video techniques
[17][9][27][43][32]. However, it lacks temporal segmentation
related datasets in recent years. Our proposed FSD-10 is a
response to these issues. As far as we know, FSD-10 is the
ï¬rst dataset which gather actions task types including action
classiï¬cation, AQA and temporal segmentation.

III. FIGURE SKATING DATASET

In this section, we describe details regarding the setup and
protocol followed to capture our dataset. Then, we discuss the
temporal segmentation and assessment tasks of FSD-10 and
its future extensions.

A. Video Capture

In FSD-10, all the videos are captured from the World
Figure Skating Championships in 2017-2018, which originates
from high-deï¬nition record on YouTube. The source video
materials have large ranges of fps (frame per second) and
image size, which are standardized to 30 fps and 1080 Ã— 720
to ensure the consistency of the dataset respectively. As for
the content, the programs of ï¬gure skating can be divided into
four types, including men short program, ladies short program,
men free skating and ladies free skating. During the skating,
camera is moving with the skater to ensure that the current
action can be located center in videos. Besides, auditorium
and ice ground are collectively referred to the background,
which are similar and consistent in all videos.

B. Segmentation and Annotation

To meet the demand of deep learning and other machine
learning models, video materials are manually separated into
action clips by undergraduate students. Each clip is ranging
from 3s to 30s, the principe of the segmentation is to com-
pletely reserve action content of videos, in other words, warm-
up and ending actions are also included. Actions type, base
value (BV), grade of execution (GOE), skater name, skater
gender, skater age, coach and music are recorded in each
clip. A detailed record example of one clip is shown in Tab.
II. Unfortunately, the distribution of actions is out of tune,

ChComboSpin4FlyCamelSpin4ChoreoSequence1StepSequence33Axel3Flip 3Loop3Lutz2Axel3Lutz_3ToeloopJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Fig. 3. Number of clips per action class. The horizontal axis indicates action types while the vertical axis indicates action amout.

TABLE II
A SINGLE CLIP EXAMPLE OF FSD-10

Action

Type
BV
GOE
Skater name
Skater gender
Skater age
Coach
Music

Value

2Axel
3.3/3.63
0.58
Starr ANDREWS
F
16
Derrick Delmore, Peter Kongkasem
Fever performed by Beyonce

as different kinds of actions are totally 197 and the number
of each class of action instances is from 1 to 223. To avoid
insufï¬cient training, our FSD-10 is composed of the top 10
actions (as shown in Fig. 2, Fig. 3). Generally speaking, these
selected actions could fall into three categories: jump, spin
and sequence. And in details, ChComboSpin4, FlyCamelSpin4,
ChoreoSequence1, StepSequence3, 2Axel, 3Loop, 3Flip, 3Axel,
3Lutz and 3Lutz 3Toeloop are included.

C. Action Discriminant Analysis

Compared with existing sports, ï¬gure skating is hardly
distinguished in action types. The actions in ï¬gure skating
look like tweedledum and tweedledee, which are difï¬cult to
distinguish in classiï¬cation task. (1) It is difï¬cult to judge the
action type by human pose in a certain frame or a few frames.
For instance, Loop jump and Lutz jump are similar in warm-
up period, it is impossible to distinguish them beyond take-off
process. (2) The difference between some actions exists on a
subtle pose in ï¬xed frames. For example, Flip jump and Lutz
jump are both picking-ice actions, which are same in take-off
with left leg and picking ice with right toe. The only subtle
difference of the two types of jump is that takes-off process of
Flip jump is by inner edge while Lutz jump is by outer edge.
(3) The spin direction is uncertain. The rotational direction of
most skaters is anticlockwise while that of others is opposite
(e.g. Denise Biellmann). These factors of the above issues
make classiï¬cation task of FSD-10 a challengeable problem.

D. Temporal Segmentation and Assessment

â€¢ Action Quality Assessment (AQA). AQA would be
emerged as an imperative and challengeable issue in
ï¬gure skating, which is used to evaluate performance
level of skaters in skating programs by BV score plus
GOE score. As shown in Sec. III-B, BV and GOE are
included in our dataset, which are mainly depend on
action types and skaterâ€™s action performance respectively.
The key points of AQA are summarised as follows. (1)
BV is depend on action types and degree of action
difï¬culty. Besides, 10% bonus BV score is appended
on the second half of a program. (2) During the aerial
process, insufï¬cient number of turns and raising hands
cause GOE deduction and bonus respectively. (3) It is
caused GOE deduction of one action by hand support,
turn over, paralleling feet and trip during the landing
process.

â€¢ Action Temporal Segmentation(ATS). It is indicated
that ATS is also a signiï¬cant issue of FSD-10. A standard
action is of certain stages. For example during the process
of a jump action, four stages are commonly included,
which are preparation, take-off process, spinning in the
air and landing process. Separating these steps is a key
technical issue for AQA and further related works.

â€¢ Long Sequence Segmentation and Assessment (LSSA).
Another important
issue is LSSA, which is different
from AAS in sensitivity of actions order. Action order
and relation need to be considered in LSSA, which is
summarised as follows. (1) LSSA is the total of all the
single action score, which is calculated similar to AQA
for each action. (2) It is caused a fail case 3 when the
twice appearance of same action.

E. Future Extension

Our proposed FSD-10 is sensitive to domain knowledge and
abundant in practical tasks. Besides the tasks in Sec. III-D ,

3The twice appearance action will have a zero score whether it is successful

or not.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

FSD-10 is convenient to be extended into other ï¬elds. For
example, pair skating is also a worth noting program in ï¬gure
skating. In the pair skating, to throw one skater by another is
a classical item, and the action in the air could be illustrated
as a corresponding jump action in men/ladies programs. To
transfer single skating models to pair skating ones is an
interesting open problem, which could be deemed as a kind
of transfer learning[23][45] or meta learning[19][14]. Besides,
action reasoning[8][38] is an interesting issue. For example,
it is straightforward to conclude a 3Lutz-3Toeloop jump if
single 3Lutz jump and 3toeloop jump have been recognized. In
addition, coordination between the action and the other sports
elements such as program content assessment by rhythmical
actions in time to music is also a focused issue. Fortunately,
the corresponding music of actions is included in FSD-10. It
is to be explored whether the music ups and downs is related
to the action rhythm, which is an exciting cross-data-modality
learning task.

IV. KEYFRAME BASED TEMPORAL SEGMENT NETWORK
(KTSN)

In this section, we give a detailed description of our
keyframe based temporal segment network. Speciï¬cally, we
ï¬rst discuss the motivation of key frame sampling in Sec.
IV-A. Then, sampling method of key frame is proposed in
Sec. IV-B and IV-C. Finally, network structure of KTSN is
detailedly introduced in Sec. IV-D.

A. Motivation of Key Frame Sampling

To solve the problem of long-range temporal modeling, a
sampling method called segment based sampling is proposed
by temporal segment networks(TSN)[40]. The main idea of
segment based sampling is â€œthe dense recoded videos are
unnecessary for content which changes slowlyâ€. The method
is essentially a kind of sparse and global sampling, which
is signiï¬cant in improving recognition performance of long-
range videos. But an intractable problem is caused that it is
unbeï¬tting for fast changing videos. Especially for tasks like
ï¬gure skating, which is complex in content and fast in speed.
It is illustrated in Sec. III-C that actions of ï¬gure skating are
similar in most of frames, and the only difference lies on
certain frames. Therefore, for sampling problem, we propose
a method to sample key frames by domain knowledge and to
sample other frames by segment based sampling. It is then
focus on how to sample key frames in segmentation based
sampling.

The problem motivates us to explore an indicator of sam-
pling by domain knowledge in ï¬gure skating. For example,
by observing the process of jump, we ï¬nd that arms and legs
move following speciï¬c rules during the process. Thus, we are
encouraged to deï¬ne an indicator using anatomical keypoints
to measure this kind of rules, which is called Human Pose
Scatter and is illustrated in the next section.

B. Human Pose Scatter (HPS)

Before key frame sampling, we will introduce HPS for a
preparation. HPS is an indicator which is denoted by the

scatter of arms and legs relative to the body. The main
positions of human (left eye, right eye, etc. ) are marked by
the open-pose algorithm, which is shown in Fig. 4. Next, HPS
is introduced in the rest of this section.

Fig. 4. The sketch map of human pose scatter. A 18-anatomical-keypoints
recognition algorithm called open-pose[4][3][33][41] is adopted to get the
spatial position and anatomical structure of human in one frame.

As shown in Fig. 4, given a frame matrix of the responding
anatomical structure, X = [x1, Â· Â· Â· , xn] âˆˆ RDÃ—n (in this
paper D = 2 and n = 18), in which each row corresponds to a
particular feature while each column corresponds to a human
anatomical keypoint, and Â¯x is the column mean value of X.
Then, Ë†X can be deï¬ned as Ë†X = [x1 âˆ’ Â¯x, x2 âˆ’ Â¯x, Â· Â· Â· , xi âˆ’ Â¯x].
Motivated by the principal component analysis (PCA[42]),
HPS can be deï¬ned by Eq. 1. Projection matrix U (UDÃ—d =
[u1, u2, Â· Â· Â· , ud], satisfying U T U = I, which leads I âˆ’ U U T
the idemfactor) can be calculated by eigenvalue decomposition
(EVD) of Ë†X Ë†X T .

Î¶HP S =

n
(cid:88)

i=1

(cid:13)(xi âˆ’ Â¯x) âˆ’ U U T (xi âˆ’ Â¯x)(cid:13)
(cid:13)
2
(cid:13)
2

(1)

Actually, Eq. 1, only a formal expression of Î¶HP S, can
be easily rewritten as Eq. 2 to simplify calculation. In Eq.
2, T r (cid:2) Ë†X Ë†X T (cid:3) and T r (cid:2)U T Ë†X Ë†X T U (cid:3) = (cid:80)d
4 have been
calculated during the EVD process. Therefore, Eq. 2 offers an
efï¬cient approach for calculation of Î¶HP S.

j=1 Î»j

n
(cid:88)

Î¶HP S =

(cid:13)
(cid:13)(I âˆ’ U U T )(xi âˆ’ Â¯x)(cid:13)
2
(cid:13)
2

i=1
n
(cid:88)

(xi âˆ’ Â¯x)T (I âˆ’ U U T )(xi âˆ’ Â¯x)

=

(2)

i=1

=T r( Ë†X Ë†X T ) âˆ’

d
(cid:88)

j=1

Î»j

Î¶HP S is the sum of variance towards the principal axis
of human anatomical keypoints, which is used to reï¬‚ect
the degree of human pose stretching. Therefore, Î¶HP S is

4Î»j is the j-th eigenvalue of covariance matrix Ë†X Ë†X T , in our paper d = 1.

Ò§ğ‘¥ğ‘ˆğ‘¥1JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

a tremendous help in ï¬nding key frames of sequences. For
example, during a process of jump, Î¶HP S of spin in the air
process is lower than it of the take off process, and extreme
point maps a turning point responding to some certain actions.
This can be referred to Fig. 1.

C. Key Frame Sampling

Fig. 5. The sketch map of key frame sampling. A 2-Axel jump is performed
by Alina Ilnazovna Zagitova, which is a same action with Fig. 1. t represents
the time variable.

j

Key frame sampling is an important issue in video analysis
of ï¬gure skating, and the key frames should contain most
discriminant information of one video. In the ï¬gure skating
task, the fast changing motion frames are distinctly important
for jump actions. As illustrated in Sec. IV-B, the kind of
fast changing motion could be quantiï¬ed as Î¶HP S by using
in a T frames clip of a
anatomical keypoints. Therefore,
video, the frame corresponding to extreme frame Ojâˆ— , which
Î¶ j
satisï¬es jâˆ— = argmax
HP S, is deï¬ned as â€œthe speciï¬c key
frameâ€, where Î´ indicates the near frame radius of Ojâˆ— , j âˆˆ
{Î´ + 1, Â· Â· Â· , T âˆ’ Î´}. In addition, as nearby Î´ frames of Ojâˆ— ,
Gjâˆ— = {Gjâˆ—,jâˆ—âˆ’Î´, Â· Â· Â· , Gjâˆ—,p, Â· Â· Â· , Gjâˆ—,jâˆ—+Î´} are deï¬ned as
â€œkey framesâ€, where p âˆˆ {jâˆ— âˆ’ Î´, Â· Â· Â· , jâˆ— + Î´} \jâˆ—(see Fig. 5).
In a multi-action clip, we will get Ol
jâˆ— , where l = 1, 2, Â· Â· Â· , L,
L indicates the number of extreme frames. Then, key frame
sampling could be described as follows. First, ï¬nding the
speciï¬c key frame(s) in action sequences. Second, sampling
the video from the speciï¬c key frame(s) and its nearby frames
using the similar approach of TSN. In one batch, TSN samples
a random frame while key frame sampling involves a random
frame Gl

jâˆ—,p in Gjâˆ— âˆª Ojâˆ— .

D. Framework Structure

An effective and efï¬cient video-level framework, coined
keyframe based Temporal Segment Network (KTSN, Fig. 6),
is designed with the help of key frame sampling. Compared
with single frame or short frame pieces, KTSN works on

short snippets sampled from the videos. To make these short
snippets representative for the entire videos as well as to keep
the video is divided
computational consumption steerable,
into several segments ï¬rst. Then, sampling is continued on
each piece of snippets. As illustrated in IV-A, our sampling
methods are divided to two parts, which are segment based
sampling[40] and key frame sampling respectively. With the
sampling method, each piece contributes its snippet-level pre-
dictions of actions classes. And these snippet-level predictions
are aggregated as video-level scores by a consensus function.
The video-level prediction is more accurate than any single
snippet-level predictions for the reason that it captures details
of the whole videos. During the process of training, the model
is iteratively updated in video-level as follows.
segments

(Vvideo =
{Î±1, Î±2, Â· Â· Â· , Î±k} of equal duration pieces. One frame Tk
is randomly selected from its corresponding segment Î±k.
Similarly, One snippet Gl
jâˆ—,p is randomly selected from its
corresponding Gl
jâˆ— , which have been proposed in section
IV-B and IV-C. The sequence of snippets (T1, T2, Â· Â· Â· , Tk and
G1
jâˆ—,p) makes up of the model of keyframe
based temporal segment network as follows.

a video into k

jâˆ—,p, Â· Â· Â· , GL

We divide

jâˆ—,p, G2

jâˆ— âˆªOl

KT SN (T1,T2, Â· Â· Â· , Tk, G1

jâˆ—,p, G2

jâˆ—,p, Â· Â· Â· , GL

jâˆ—,p) =

P(G(F(T1; W ), F(T2; W ), Â· Â· Â· , F(Tk; W )),

G(F(G1

jâˆ—,p; W ), F(G2

jâˆ—,p; W ), Â· Â· Â· , F(GL

jâˆ—,p; W )))

(3)

In Eq. 3, F(Tk; W ) (or F(Gl

jâˆ—,p; W )) is the function on
behalf of a ConvNet[20] with model parameters W which plays
an important role in the snippet Tk (Gl
jâˆ—,p) and obtain a con-
sensus of classiï¬cation hypothesis. It is given the prediction of
the whole video by an aggregation function G, which combines
all the output of individual snippets. Based on the value of
G, P works out the classiï¬cation results. In this paper, P is
represented as softmax function[22].

V. EXPERIMENTS
In order to provide a benchmark for our FSD-10 dataset, we
evaluate various approaches under three different modalities:
RGB, optical ï¬‚ow and anatomical keypoints (skeleton). We
also conduct experiments on cross dataset validation. The
following describes the details of our experiments and results.

A. Evaluations of Methods on FSD-10

The state-of-the-art classiï¬cation methods of human action
are evaluated on FSD-10 in this section. In order to make
the experiments representative in action recognition ï¬eld, 2D
methods and 3D methods are selected respectively. TSN (or
KTSN) is selected as a 2D method, which is a typical two
stream structure. As for 3D methods,
two-stream inï¬‚ated
3D ConvNet (I3D) with Inception-v1[36], Inception-v3[37]
and Resnet-50, Resnet-101[11] are involved for comparison.
Besides, anatomical keypoints related methods (st-gcn[44],
densenet[15]) are tested on our dataset for reference. As listed
in Tab. III, our proposed KTSN achieves state-of-the-art results
on FSD-10. KTSN method is raised as a baseline for this high-
speed dataset.

ğœğ»ğ‘ƒğ‘†ğ‘‡â€œThe specific key frameâ€Oğ‘—âˆ—ğºğ‘—âˆ—,ğ‘—âˆ—âˆ’ğ›¿ğºğ‘—âˆ—,pğºğ‘—âˆ—,ğ‘—âˆ—+ğ›¿ğœğ»ğ‘ƒğ‘†tJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Fig. 6. Keyframe based temporal segment network. One input video is divided into k segments (here we show the k = 3 case) and a short snippet is
randomly selected from each segment. Meanwhile, n key frames are selected from the Video (here L = 1). The snippets are represented by modalities such as
RGB frames and Optical ï¬‚ow. Then these snippets are fused by an aggregation function G. Last, P combines the outputs and makes video-level predictions.
ConvNets on all snippets share parameters.

TABLE III
PERFORMANCE OF BASELINE CLASSIFICATION MODELS EVALUATES WITH
FSD-10. THE SHORT NAMES OF THESE METHODS ARE TWO-STREAM
INFLATED 3D CONVNET[5] (INCEPTION-V1[36], INCEPTION-V3[37],
RESNET-50[11] AND RESNET-101[11]), SPATIAL TEMPORAL GRAPH
CONVOLUTION NETWORKS (ST-GCN[44]), DENSENET WITH ANATOMICAL
KEYPOINTS NETWORKS[15], TEMPORAL SEGMENT NETWORK[40] AND
KEYFRAME BASED TEMPORAL SEGMENT NETWORK RESPECTIVELY.
DENSENET+ DENOTES DENSENET WITH CENTRALIZATION. BESIDES,
TSN(7) REPRESENTS TSN(k=7) AND KTSN(6,1) REPRESENTS
KTSN(k=6, L=1) CORRESPONDING TO SEC. IV-D, SIMILARLY
HEREINAFTER.

RGB

Optical ï¬‚ow

Skeleton

Fusion

Inception-v1[36]
Inception-v3[37]
Resnet-50[11]
Resnet-101[11]
St-gcn[44]
DenseNet[15]
DenseNet+[15]

TSN(7)[40]
KTSN(6, 1)

TSN(9)[40]
KTSN(8, 1)

TSN(11)[40]
KTSN(10, 1)

43.765
50.118
62.550
78.823
-
-
-

53.176
56.941

59.294
59.529

59.294
63.294

52.706
57.245
58.824
-
-
-
-

75.165
76.941

80.235
80.471

82.118
82.588

-
-
-
61.549
72.429
74.353
79.765

-
-

-
-

-
-

55.052
59.624
64.941
79.851
-
-
-

76.000
77.412

80.235
80.471

82.118
82.588

than TSN[40].

C. Cross-dataset Validations

To verify motion characteristic of a sport video, the cross-
dataset validation is conducted on both classical dataset
UCF101 and FSD-10. RGB and optical ï¬‚ow features are two
typical ways for action recognition. Generally speaking, the
RGB feature based method places emphasis on human pose
while the optical ï¬‚ow based method attaches importance to
transformation between actions. It is illustrated in Tab. IV
that the performance differences between RGB and Optical
ï¬‚ow of FSD-10 is greater than UCF101, which indicates
motion is more important than human static pose in FSD-
10. Besides, in sampling stage, FSD-10 is more sensitive to
ï¬ne-grained segmentation, as the performance raises 5.4%
compared with 2.5% in UCF101 (3-segment-sampling to 9-
segment-sampling). In FSD-10, actions are sensitive to its
motion rather than its pose, which is consistent with the
original intention of the dataset.

TABLE IV
RESULTS OF CROSS-DATASET GENERALIZATION. UCF101 AND FSD-10
ARE TRAINED WITH TSN(k=3) AND TSN(k=9) RESPECTIVELY.

B. Evaluations between TSN and KTSN

The experimental results in Tab. III show that key frame
sampling is effective in sparse sampling. As illustrated above,
key frame is crucial for competitive sports classiï¬cation tasks.
Especially for jump actions, it is vital to choose take off
frames, which enhances discriminant of different jump actions.
An example of key frame sampling of jump actions is shown
in Fig. 5. By catching these key frames instead of random
sampling, KTSN achieves better classiï¬cation performance

Dataset

Methods

RGB

Optical ï¬‚ow

Fusion

UCF101

FSD-10

TSN(3)
TSN(9)

TSN(3)
TSN(9)

85.673
86.217

48.941
59.294

85.170
89.713

75.765
82.118

92.466
94.909

76.765
82.118

VI. CONCLUSION

In this paper, we build an action dataset for competitive
sports analysis, which is characterised by high action speed

Key Frame SamplingSegment based SamplingKey-frame based Temporal Segment network ğ‘‡1ğ‘‡2ğ‘‡3ğºğ‘—âˆ—,ğ‘1â„±(ğ‘‡1;ğ‘Š)â„±(ğ‘‡2;ğ‘Š)â„±(ğ‘‡3;ğ‘Š)â„±(ğºğ‘—âˆ—,ğ‘1;ğ‘Š)ğ’¢â„±(ğ‘‡1;ğ‘Š,â„±(ğ‘‡2;ğ‘Š),â„±(ğ‘‡3;ğ‘Š))ğ’¢(â„±(ğºğ‘—âˆ—,ğ‘1;ğ‘Š))ğ’«ğ’¢â„±(ğ‘‡1;ğ‘Š,â„±(ğ‘‡2;ğ‘Š,â„±(ğ‘‡3;ğ‘Š)),ğ’¢(â„±(ğºğ‘—âˆ—,ğ‘1;ğ‘Š)))Red layerGreen layerBlue layerVideo SequenceOptical flow xOptical flow yPoolingOtherSoftmaxConvolutionInput & Outputğ’¢(),ğ’«()JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

and complex action content. We ï¬nd that motion is more
valuable than form (content and background) in this task.
Therefore, compared with other related datasets, our dataset
focuses on the action itself rather background. Our dataset
creates many interesting tasks, such as ï¬ne-grained action
classiï¬cation, action quality assessment and action temporal
segmentation. Moreover, the dataset could even be extended
to pair skating in transfer learning and coordination between
action and music in cross-data-modality learning.

We also present a baseline for action classiï¬cation in FSD-
10. Note that domain knowledge is signiï¬cant for competitive
action classiï¬cation task. Motivated by this idea, we adopt key-
frame based temporal segmentation sampling. It is illustrated
that action sampling should focus more on related frames of
motion. The experimental results demonstrate that the key
frame is effective for skating tasks.

In the future, more diversiï¬ed action types and tasks are
intended to be included to our dataset, and more interesting
action recognition methods are likely to be proposed with the
help of FSD-10. We believe that our dataset will promote the
development of action analysis and related research topics.

SUPPLEMENTARY MATERIAL

1. Training of KTSN

Architectures Inception network, chosen in KTSN, is a
milestone in CNN classiï¬er. The main characteristics of in-
ception network is the inception module, which is designed to
process the features in different dimensionality (such as the
1Ã—1, 3Ã—3 and 5Ã—5 convolution core) parallelly. The result of
Inception network is a better representation than single-layer-
networks, which is excellent in local topological structure.
Speciï¬cally, Inception v3 is selected in the experiments as
follows.

Input Our model is based on spatial and temporal level,
which are corresponding to RGB frames[35] and optical
ï¬‚ow[13] respectively. RGB mode is an industrial standard
of color, combining Red, Green and Blue into chromatic
frames, which is also called the spatial ï¬‚ow. Besides, optical
ï¬‚ow mode derives from calculating the differences between
adjacent frames, which indicates temporal ï¬‚ow.

2. Confusion Matrix of Actions

Itâ€™s signiï¬cant to involve domain knowledge in competitive
sports content analysis. It is illustrated the confusion relation-
ships between actions by examples in Sec. 3.3, and this part is
a validation, as shown in Fig. 7. First, the skating spin obtains
the best classiï¬cation results, this is because it could judge
a skating spin of video precisely with a few isolated frames
using deep networks. Second, the skating jump is the most
difï¬cult to distinguish. Itâ€™s worth note that up to 24% Lutz
jump actions are identiï¬ed as Flip jump actions.

REFERENCES

[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George
Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan.
arXiv
Youtube-8m: A large-scale video classiï¬cation benchmark.
preprint arXiv:1609.08675, 2016.

Fig. 7. The confusion matrix results of FSD-10

[2] Alan C Bovik. Handbook of image and video processing. Academic

press, 2010.

[3] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
OpenPose: realtime multi-person 2D pose estimation using Part Afï¬nity
Fields. In arXiv preprint arXiv:1812.08008, 2018.

[4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime
In CVPR,

multi-person 2d pose estimation using part afï¬nity ï¬elds.
2017.

[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition?
In proceedings of the IEEE
a new model and the kinetics dataset.
Conference on Computer Vision and Pattern Recognition, pages 6299â€“
6308, 2017.

[6] Harris Drucker, Christopher JC Burges, Linda Kaufman, Alex J Smola,
and Vladimir Vapnik. Support vector regression machines. In Advances
in neural information processing systems, pages 155â€“161, 1997.
[7] Hany El-Ghaish, Mohamed E Hussein, Amin Shoukry, and Rikio Onai.
Human action recognition based on integrating body pose, part shape,
and motion. IEEE Access, 6:49040â€“49055, 2018.

[8] Matthew L Ginsberg and David E Smith. Reasoning about action i: A
possible worlds approach. Artiï¬cial intelligence, 35(2):165â€“195, 1988.
[9] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline
Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici,
Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-
temporally localized atomic visual actions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 6047â€“
6056, 2018.

[10] Joachim Gudmundsson and Michael Horton. Spatio-temporal analysis
of team sports. ACM Computing Surveys (CSUR), 50(2):22, 2017.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
In Proceedings of the IEEE
residual learning for image recognition.
conference on computer vision and pattern recognition, pages 770â€“778,
2016.

[12] Sepp Hochreiter and JÂ¨urgen Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735â€“1780, 1997.

[13] Berthold KP Horn and Brian G Schunck. Determining optical ï¬‚ow.

Artiï¬cial intelligence, 17(1-3):185â€“203, 1981.

[14] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via

meta-learning. arXiv preprint arXiv:1810.02334, 2018.

[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
Weinberger. Densely connected convolutional networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition,
pages 4700â€“4708, 2017.

[16] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung,
Rahul Sukthankar, and Li Fei-Fei. Large-scale video classiï¬cation with
convolutional neural networks. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, pages 1725â€“1732, 2014.
[17] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier,
Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back,
Paul Natsev, et al. The kinetics human action video dataset. arXiv
preprint arXiv:1705.06950, 2017.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

Tang, and Luc Van Gool. Temporal segment networks for action
IEEE transactions on pattern analysis and
recognition in videos.
machine intelligence, 2018.

[41] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.

Convolutional pose machines. In CVPR, 2016.

[42] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component
analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37â€“
52, 1987.

[43] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video
description dataset for bridging video and language. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages
5288â€“5296, 2016.

[44] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph
convolutional networks for skeleton-based action recognition. In Thirty-
Second AAAI Conference on Artiï¬cial Intelligence, 2018.

[45] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How
transferable are features in deep neural networks? In Advances in neural
information processing systems, pages 3320â€“3328, 2014.

[46] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan,
Oriol Vinyals, Rajat Monga, and George Toderici. Beyond short
In Proceedings of
snippets: Deep networks for video classiï¬cation.
the IEEE conference on computer vision and pattern recognition, pages
4694â€“4702, 2015.

[18] Hildegard Kuehne, Hueihan Jhuang, EstÂ´Ä±baliz Garrote, Tomaso Poggio,
and Thomas Serre. Hmdb: a large video database for human motion
In 2011 International Conference on Computer Vision,
recognition.
pages 2556â€“2563. IEEE, 2011.

[19] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano
Soatto. Meta-learning with differentiable convex optimization.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 10657â€“10665, 2019.

[20] Shih-Chung B Lo, Heang-Ping Chan,

Jyh-Shyan Lin, Huai Li,
Matthew T Freedman, and Seong K Mun. Artiï¬cial convolution neural
network for medical image pattern recognition. Neural networks, 8(7-
8):1201â€“1214, 1995.

[21] Marcin MarszaÅ‚ek, Ivan Laptev, and Cordelia Schmid. Actions in
context. In CVPR 2009-IEEE Conference on Computer Vision & Pattern
Recognition, pages 2929â€“2936. IEEE Computer Society, 2009.

[22] Roland Memisevic, Christopher Zach, Marc Pollefeys, and Geoffrey E
Hinton. Gated softmax classiï¬cation. In Advances in neural information
processing systems, pages 1603â€“1611, 2010.

[23] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and
transferring mid-level image representations using convolutional neural
networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1717â€“1724, 2014.

[24] Paritosh Parmar and Brendan Morris. Action quality assessment across
multiple actions. In 2019 IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 1468â€“1476. IEEE, 2019.

[25] Paritosh Parmar and Brendan Tran Morris. What and how well you
performed? a multitask learning approach to action quality assessment.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 304â€“313, 2019.

[26] Paritosh Parmar and Brendan Tran Morris. Learning to score olympic
events. In proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pages 20â€“28, 2017.

[27] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool,
Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset
In Pro-
and evaluation methodology for video object segmentation.
ceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 724â€“732, 2016.

[28] Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. Assessing the
quality of actions. In European Conference on Computer Vision, pages
556â€“571. Springer, 2014.

[29] Mikel D Rodriguez, Javed Ahmed, and Mubarak Shah. Action mach
a spatio-temporal maximum average correlation height ï¬lter for action
recognition. In CVPR, volume 1, page 6, 2008.

[30] Seyed Morteza Safdarnejad, Xiaoming Liu, Lalita Udpa, Brooks Andrus,
John Wood, and Dean Craven. Sports videos in the wild (svw): A
In 2015 11th IEEE International
video dataset for sports analysis.
Conference and Workshops on Automatic Face and Gesture Recognition
(FG), volume 1, pages 1â€“7. IEEE, 2015.

[31] Huang-Chia Shih. A survey of content-aware video analysis for sports.
IEEE Transactions on Circuits and Systems for Video Technology,
28(5):1212â€“1231, 2017.

[32] Leonid Sigal and Michael J Black. Humaneva: Synchronized video
and motion capture dataset for evaluation of articulated human motion.
Brown Univertsity TR, 120, 2006.

[33] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand
keypoint detection in single images using multiview bootstrapping. In
CVPR, 2017.

[34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A
dataset of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[35] Sabine SÂ¨usstrunk, Robert Buckley, and Steve Swen. Standard rgb color
spaces. In Color and Imaging Conference, volume 1999, pages 127â€“134.
Society for Imaging Science and Technology, 1999.

[36] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
In Proceedings of the
Rabinovich. Going deeper with convolutions.
IEEE conference on computer vision and pattern recognition, pages 1â€“
9, 2015.

[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and
Zbigniew Wojna. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2818â€“2826, 2016.

[38] Mary Tuck and David Riley. The theory of reasoned action: A decision
theory of crime. In The reasoning criminal, pages 156â€“169. Routledge,
2017.

[39] Jacco Van Sterkenburg, Annelies Knoppers, and Sonja De Leeuw. Race,
ethnicity, and content analysis of the sports media: A critical reï¬‚ection.
Media, Culture & Society, 32(5):819â€“839, 2010.

[40] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou

