0
2
0
2

l
u
J

7

]

G
L
.
s
c
[

1
v
2
4
7
3
0
.
7
0
0
2
:
v
i
X
r
a

Meta-active Learning in Probabilistically-Safe
Optimization

Mariah L. Schrum∗
Insititute for Robots and Intelligent Machines
Georgia Institute of Technology
Atlanta, GA 30308
mschrum3@gatech.edu

Mark Connolly
Emory University
Atlanta, GA 30308
mark.connolly@emory.edu

Eric Cole
Emory University
Atlanta, GA 30308
ercole@emory.edu

Mihir Ghetiya
Emory University
Atlanta, GA 30308
mihir.v.ghetiya@emory.edu

Robert Gross
Emory University
Atlanta, GA 30308
rgross@emory.edu

Matthew Gombolay
Insititute for Robots and Intelligent Machines
Georgia Institute of Technology
Atlanta, GA 30308
matthew.gombolay@cc.gatech.edu

Abstract

Learning to control a safety-critical system with latent dynamics (e.g. for deep brain
stimulation) requires taking calculated risks to gain information as efﬁciently as
possible. To address this problem, we present a probabilistically-safe, meta-active
learning approach to efﬁciently learn system dynamics and optimal conﬁgurations.
We cast this problem as meta-learning an acquisition function, which is represented
by a Long-Short Term Memory Network (LSTM) encoding sampling history. This
acquisition function is meta-learned ofﬂine to learn high quality sampling strategies.
We employ a mixed-integer linear program as our policy with the ﬁnal, linearized
layers of our LSTM acquisition function directly encoded into the objective to trade
off expected information gain (e.g., improvement in the accuracy of the model of
system dynamics) with the likelihood of safe control. We set a new state-of-the-art
in active learning for control of a high-dimensional system with altered dynamics
(i.e., a damaged aircraft), achieving a 46% increase in information gain and a 20%
speedup in computation time over baselines. Furthermore, we demonstrate our
system’s ability to learn the optimal parameter settings for deep brain stimulation
in a rat’s brain while avoiding unwanted side effects (i.e., triggering seizures),
outperforming prior state-of-the-art approaches with a 58% increase in information
gain. Additionally, our algorithm achieves a 97% likelihood of terminating in a
safe state while losing only 15% of information gain.

∗Use footnote for providing further information about author (webpage, alternative address)—not for

acknowledging funding agencies.

Preprint. Under review.

 
 
 
 
 
 
1

Introduction

Safe and efﬁcient control of a novel systems with latent dynamics is an important objective in domains
from healthcare to robotics. In healthcare, deep brain stimulation devices implanted in the brain can
improve memory deﬁcits in patients with Alzheimers [1] and responsive neurostimulators can counter
epileptiform activity to mitigate seizures. Yet, the surgeon’s manual process of ﬁnding effective
inputs for each patient is arcane, time-consuming, and risky (e.g., the wrong input can trigger an
ictal state or brain damage). Researchers studying active learning and Bayesian optimization seek
to develop algorithms to efﬁciently and safely learn a systems’ dynamics, ﬁnding some success in
such healthcare applications [2, 3]. However, these approaches typically fail to scale up to higher-
dimensional domains. As such, researchers typically only utilize simple voltage and frequency input
controls for neural stimulation rather than fully modifying the waveform across 32 channels [2].
Similarly, high-dimensional tasks in robotics, such as learning the dynamics of a novel robotic systems
(e.g., an autopilot learning to recover control of a damaged aircraft), require active learning methods
that can succeed in these state and action spaces. What is critically needed then is a computational
mechanism to efﬁciently and safely learn across low- and high-dimensional domains.

A promising area of renewed interest is that of meta-learning for online adaptation [4, 5, 6, 7]. These
approaches include meta-learning of a robot controller that adapts to a new control task [4] and for
ﬁne-tuning a dynamics model during runtime [5]. However, these approaches do not consider the
important problem of safely controlling a system with altered dynamics, which is a requirement
for safety-critical robotic applications. Furthermore, these approaches do not actively learn. Other
researchers have sought to safely and actively learn damage models via Gaussian Processes [8, 9, 10],
including meta-learning the Bayesian prior for a Gaussian Process [11]. However these approaches
scale poorly and are sample inefﬁcient. In this paper, we seek to overcome these key limitations
of prior work by harnessing the power of meta-learning for active learning in a chance-constrained
optimization framework for safe, online adaptation.

Contributions – We develop a probabilistically safe, meta-learning approach for active learn-
ing ("meta-active learning") that achieves state-of-the-art performance against active learning and
Bayesian optimization baselines. Our acquisition function (i.e., the function that predicts the expected
information gain of a data point) is meta-learned ofﬂine, allowing the policy to beneﬁt from past
experience, and provide a more robust measure of the value of an action. The key to our approach is a
novel hybrid algorithm interweaving optimization via mixed-integer linear programming [12] (MILP)
as a policy with an acquisition function represented as a Long-Short Term Memory Network [13]
(LSTM) whose linear and piece-wise linear output layers are directly encoded into the MILP. Thus,
we achieve the best of both worlds, allowing for deep learning within chance-constrained optimization.
Speciﬁcally, the contributions of this paper are three-fold:

1. Meta-active learning for autonomously synthesizing an acquisition function to efﬁciently

infer altered or unknown system dynamics and optimize system parameters.

2. Probabilistically-safe control combined with an active-learning framework through the
integration of our deep learning-based acquisition function and integer linear programming.
3. State-of-the art results for safe, active learning for sample-efﬁcient reduction in model
error. We achieve a 46% increase in information gain in a high-dimensional environment of
controlling a damaged aircraft, and we achieve a 58% increase in information gain in our
deep brain stimulation against our baselines.

2 Preliminaries

In this section, we review the foundations of our work in active, meta-, and reinforcement learning.

Active Learning – Labelled training data is often difﬁcult to obtain due either to tight time constraints
or lack of expert resources. Active learning attempts to address this problem by utilizing a heuristic to
quantify the amount of information an unlabelled training sample would provide the model if its label
were known. A dataset D = {UD, LD} where U represent a large unlabeled set of training samples,
{xi}n
j=1, is available to the base learner
(i.e., the algorithm used to learn the target concept), ˆTψ. The base learner is initially trained on LD
and employs a metric, H(UD), commonly known as an acquisition function, to determine which
unlabelled sample to query for the label. The learner is then retrained on {LD ∪ H(UD)} [14, 15].

i=1, and LD a small set of labelled training samples, {xj, yj}m

2

Figure 1: This ﬁgure depicts our framework. Previously collected state-action pairs are fed into
an LSTM embedding layer. Mean and variance statistics are also calculated for each state and
action. The concatenated vector is fed through a fully connected layer to create the (cid:126)Z embedding.
Our Q-function consists of two, fully-connected, ReLU layers. Blue and green elements are the
meta-learned acquisition function; the grey element is our updating dynamics model, ˆTψ, which is
used to deﬁne parameters of Eq. 2 in the MILP; green and yellow elements are encoded as a MILP.

Meta-Learning – Meta-learning approaches attempt to learn a method to quickly adapt to new tasks
online. In contrast to active learning, meta-learning attempts to learn a skill or learning method, e.g.
learning an active learning function, which can be transferable to novel tasks or scenarios. These
tasks or skills are trained ofﬂine, and a common assumption is that the tasks selected at test time are
drawn from the same distribution used for training [16].

Reinforcement Learning and Q-Learning – A Markov decision process (MDP) is a stochastic
control process for decision making and can be deﬁned by the 5-tuple (cid:104)X , U, T , R, γ(cid:105). X represents
the set of states and U the set of actions. T : X × U × X (cid:48) → [0, 1] is the transition function that
returns the probability of transitioning to state x(cid:48) from state x applying action, u. R : X × U → R is
a reward function that maps a state, x and action, u, to a reward r ∈ R, and γ weights the discounting
of future rewards. Reinforcement learning seeks to synthesize a policy, π : X → U, mapping states
to actions in order to maximize the future expected reward. When π is the optimal policy, π∗, the
following Bellman condition holds: Qπ∗
(x, u) := Ex(cid:48)∼T

(x(cid:48), π∗(x))(cid:3) [17].

(cid:2)R(x, u) + γQπ∗

2.1 Problem Set-up

Our work is at the unique nexus of active learning, meta-learning and deep reinforcement learning
with the objective of learning the Q-function as an acquisition function to describe the expected
future information gained when taking action u in state x. We deﬁne information gain as the percent
decrease in the error of the objective (e.g., decrease in model error). A formal deﬁnition is provided in
Supplementary. The Q-function is trained via a meta-learning strategy in which an agent interacts in
environments sampled from a distribution of different scenarios to distill the most effective acquisition
function for active learning of system dynamics. In our context, a state X is deﬁned by the tuple
X : (cid:104)U, L, ˆTψ(cid:105) where U consists of all possible state-action pairs and L is the set of state-action
pairs that the agent has already experienced. ˆTψ is a neural network function approximator of the
transition dynamics, which is parameterized by ψ and updated online as samples are collected. ri is
proportional to the reduction of the mean squared error (MSE) loss of ˆTψ.

3 Safe Meta-Learning Architecture

Several key components are vital for learning about an unknown system in a timely manner. First, an
encoding of the context of the new dynamics is important for determining where exploration should
be focused and which actions may be beneﬁcial for gaining information about the changes to the
dynamics. Second, a range of prior experiences in active learning should be leveraged to best inform
which actions elicit the most information in a novel context within a distribution of tasks. We seek
to develop a framework with these key attributes to enable sample-efﬁcient and computationally
light-weight active learning. An overview of our system is shown in Figure 1.

3

3.1 Meta-Learning Algorithm

To infer the Q-function for an action (i.e. the acquisition function), we meta-learn over a distribution
of altered dynamics as described in Algorithm 1. For each episode, we sample from this distribution
of altered dynamics and limit each episode to the number of time steps, M , tuned to collect enough
data to accurately learn our approximate dynamics, ˆTψ, as a neural network with parameters ψ. We
utilize Q-learning to infer the Q-function which is represented by a DQN. In our training scheme we
search over the action space via a MILP, as described in Section 3.2 and select the action set, (cid:126)U (t:T ),
which maximizes the Q-function while satisfying safety constraints.

The acquisition Q-function, Qθ, is trained via Deep Q Learning [18] with target network, Qφ, and
is augmented to account for safety as described in Eq 1. The learned acquisition function, Qθ, is
utilized by our policy (Eq. 1), which is solved via a safety-constrained MILP solver. The reward, rt,
for taking a set of actions in a given state is deﬁned as the percent decrease in the MSE error of the
model, ˆTψ. This Bellman loss is backpropagated through the Q-value linear and (ReLU) output layers
through the LSTM encoding layers. µ( (cid:126)Zt+1) is the set of actions, (cid:126)Ut+1, determined by maximizing
Equation 1, which we describe in Section 3.2. The dynamics model, ˆTψ, is retrained with each new
set of state-action pairs.

Algorithm 1 Meta-learning for training

1: Randomly initialize Qθ and Qφ with weights θ = φ
2: Initialize replay buffer, D
3: for episode=1 to N do
4:
5:
6:
7:
8:
9:
10:

Initialize ˆTψ based on meta-learning distribution
Collect small initial set of state-action pairs, U0, X0
Train ˆTψ on initial set
for i=1 to M do

Forward pass on encoder to obtain (cid:126)Zi
Select Ui from Equation 1
Execute actions Ui + N according to exploration noise ; observe
states Xi+1
Retrain ˆTψ on X0:T , U0:T and observe reward ri
D ← D ∪ (cid:104)Ui,Xi,Xi+1, ri(cid:105)
Sample a batch of transitions from D
Perform forward pass through encoder to obtain (cid:126)Zt+1
(cid:16)
Calculate yt = rt + γQφ
(cid:16)

µ( (cid:126)Zt+1), (cid:126)Zt+1
yt − Qθ( (cid:126)Ut, (cid:126)Zt)

(cid:17)

(cid:17)

Update Qθ according to
Qφ ← τ Qθ + τ (1 − Qφ)

11:
12:
13:
14:
15:

16:

17:
18:
19: end for;

end for

Algorithm 2 Meta-
learning for testing

1: Draw test example from dis-
tribution of damage condi-
tions

2: Initialize ˆTψ
3: for i=1 to M do
4:

Do forward pass through
encoder to get (cid:126)Zi
Select actions, Ui, accord-
ing to Equation 1
Execute actions, Ui; ob-
serve states Xi+1 and re-
ward ri
Retrain ˆTψ on Xi+1,Ui+1

5:

6:

7:
8: end for

3.2 Mixed-Integer Linear Program

Our objective (Equation 1) is to maximize both the probability of the system remaining in a safe
conﬁguration and the information gained along the trajectory from [t, t + T ).

(cid:126)U (t:T )∗

= argmax
(cid:126)U (t:T )∈ (cid:126)U

(t:T )

(cid:104)

(cid:105)
Q( (cid:126)U (t:T ), (cid:126)Z) − ¯Qθ(·, (cid:126)Z)

λ1

(cid:110)

+ Pr

(cid:107)(cid:126)xt+T − (cid:126)xr(cid:107)1 ≤ r

(cid:111)

(1)

Q( (cid:126)U (t:T ), (cid:126)Z) describes the expected information gained along the trajectory when the set of actions
(cid:126)U (t:T ) is taken in the context of the embedding (cid:126)Z, and ¯Qθ(·, (cid:126)Z) is the expected Q-value, which we
discuss further in Section 3.3. λ1 is a hyper-parameter that can be tuned to adjust the tradeoff between
safety and information gain. We derive a linearization of this equation based on an assumption of
Gaussian dynamics which can then be solved via MILP.

Deﬁnition of Safety - We deﬁne a sphere of safety, parameterized by the safe state xr and radius
(cid:126)r which describe all safe states of the system. For example, in the case of an aircraft, (cid:126)xr would

4

Figure 2: This ﬁgure depicts the d-dimensional sphere of safety. (cid:126)xr denotes the safest state for the
system and the shaded region is the set of all safe states. Action, (cid:126)ut, is an exploratory action, which
may bring the system outside of the sphere of safety. Action, (cid:126)ut+1, estimates that the system returns
to a safe state with probability 1 − (cid:15).

be straight and level ﬂight and the sphere of safety would deﬁne any deviation from level ﬂight
that is still considered safe. Additionally, we assume that our model error comes from a Gaussian
distribution with known mean and variance. This assumption allows us to impose safety constraints
which can either be enforced with a probability of one, or this requirement can be relaxed to increase
the potential for information gain.

By assuming that our model error originates from a Gaussian distribution, we can linearize our
probability constraints described in Equation 2 to include in the MILP (See Supplementary for
full derivation). Here, Φ−1 is the inverse cumulative distribution function for the standard normal
distribution and 1 − (cid:15)d denotes the probability level. σ represents the uncertainty in the dynamics
network parameters and ¯a and ¯b are the point estimates of the dynamics. d represents the dth row
and j the columns. We compute the uncertainty, σ, of our network via bootstrapping [19, 20]. The
components of σ are calculated as described in the Supplementary.

(cid:110)

Pr

(cid:107)(cid:126)xt+T − (cid:126)xr(cid:107)1 ≤ r

(cid:111)

−→
(cid:115)(cid:88)

d(cid:48)

(cid:13)
(cid:13)Φ−1(1 − (cid:15)d)
(cid:13)

d,d(cid:48) x(t)2
σ2

j +

(cid:88)

j

d,jU (t:T )2
σ2

j

+ Γk (cid:126)U (t:T )2

− ∆(t:2)
d

(cid:13)
(cid:13)
(cid:13)1

< rd, ∀d

(2)

Our policy aims to take a set of information rich actions at time t, potentially out of the d-dimensional
sphere of safety, while guaranteeing with probability 1 − (cid:15) that the system will return to a safe state
in the next time step after T time steps. Thus, our T step propagation allows the system to deviate
from the safe region, if desired, to focus on actively learning so long as it can return to the safe region
with high conﬁdence.

3.3 An LSTM Q-Function as a Linear Program

We leverage an LSTM as a function approximator for Q-learning. To blend our Q-function with the
MILP, we pass the LSTM output embedding along with statistics of each state and action through
a fully connected layer with ReLU activations.2 This design enables us to backprop the Bellman
residual through the output layers encoded in the MILP all the way to the LSTM inputs. Thus we can
leverage the power of an LSTM and mathematical programming in a seamless framework. 3

Given our meta-learned acquisition function and chance-constrained optimization framework, we can
now perform probabilistically-safe active learning for systems such as robots to respond to damage or
other augmented-dynamics scenarios. Algorithm 2 describes how we perform our online, safe and
active learning. Intuitively, our algorithm initializes a new dynamics model to represent the unknown
or altered dynamics, and we iteratively sample information rich, safe actions via our MILP policy,
update our dynamics model, and repeat.

2We include the mean and standard deviation of previously collected states and actions as we ﬁnd the ﬁrst

and second moments of the data to be helpful, additional features.

3Details on the hyperparameter settings including learning rates and layer sizes can be found in the Supple-

mentary along with the derivation for the linearization of the Q-function for inclusion in the linear program.

5

(a) ADMETS Implantation.

(b) High-dimensional Domain; Courtesy: Flightgear.

Figure 3: Figure 3a depicts a surgical implantation in a rat brain of ADMETS device [2] (Section
4.1). Figure 3b depicts the high-dimensional domain for dynamical control (Section 4.2).

4 Experimental Evaluation

We design two experiments to validate our approach’s ability to safely and actively learn altered
or unknown dynamics in real time. We compare our approach to several baseline approaches and
demonstrate that our approach is superior in terms of both information gain and computation time.
More details on the experimental domains can be found in Supplementary.

4.1 Asynchronous Distributed Microelectrode Theta

Asynchronous distributed microelectrode theta stimulation (ADMETS) is a cutting edge deep brain
stimulation approach to treating seizure conditions that cannot be controlled via pharmacological
methods. In ADMETS, a neuro-stimulator is implanted in the brain to deliver continuous electrical
pulses to reduce seizures. However, there is no clear mapping from parameter values to reduction in
seizures that applies to all patients, as the optimal parameter settings can depend on the placement
of the device, the anatomy of an individual’s brain and other confounding factors. Further, a latent
subset of parameters can cause negative side-effects.

We test our algorithm’s ability to safely determine the optimal parameter settings on a dataset
consisting of a score quantifying memory function resulting from a sweep over the parameter settings
in six rat models, as shown in Fig 3a. Based on the work in [2], we ﬁt Guassian Processes to each
set of data to simulate the brain of each rat. The objective is to determine the amplitude of the
signal which maximizes the discrimination area (DA) without causing unwanted side effects in the
rat that typically occur when the discrimination score drops below zero. Because we do not want
the score to drop below zero, we deﬁne the time horizon over which the rat can leave the sphere
of safety as T = 0. The reward signal utilized by our meta-learner is the percent decrease in the
optimal parameter error at each time step. The optimal parameters are deﬁned as the parameters that
maximize the discrimination area.

4.2 High-dimensional Domain (Recovering Dynamics of a Damaged Aircraft)

Active learning algorithms can be ineffective in high-dimensional domains. As such, we seek to
stress-test our algorithms in just such a domain: Learning online of a damaged aircraft with nonlinear
dynamics and the tight time constraints required to learn a high-quality model of the system before
entering an unrecoverable conﬁguration (e.g., spin) or crashing. We base our simulation on theoretical
damage models from prior work describing the full equations of motion [21, 22, 23] within the
Flightgear virtual environment. The objective of this domain is to learn the difference between the
altered dynamics that result from the damage and to maintain safe ﬂight. We consider safe ﬂight to
be our designated safe state, (cid:126)xr. The aircraft takes an information rich action potentially resulting in
a deviation outside of the d-dimensional sphere of safety. The next action is constrained to guarantee
that the plane returns to the sphere of safety with probability 1 − (cid:15) via action ut+1.

4.3 Baseline Comparisons

We include the following baselines in active learning and Bayesian optimization for our evaluation.

• Epistemic Uncertainty [24] - This active learning metric quantiﬁes the uncertainty in the output

of the model for each training example and chooses the action that maximizes uncertainty.

6

• Maximizing Diversity [25] - This acquisition function selects actions which maximize the differ-

ence between previously seen states and actions.

• Bayesian Optimization (BaO) [2] - This algorithm was developed for the ADMETS domain

(Section 4.1) and is based upon a Gaussian Process model.

• Meta Bayesian Optimization (Meta BO) [26] - This approach meta-learns a Gaussian process

prior ofﬂine over previously sampled data.

• Learning Active Learning (LAL) [27] - This approach meta-learns an aquisition function, which

predicts the decrease in error of the base learner ofﬂine, with hand-engineered features.

For our ADMETS memory optimization domain, we benchmark against [2, 27, 26] as those baselines
are designed or easily adapted to optimization tasks. For our high-dimensional domain, we likewise
benchmark against [2, 24, 27, 25] as those baselines are designed or easily adapted to system
identiﬁcation (i.e., model learning) tasks. We note that we do not compare to MAML[4] or the
approach by Nagabandi et al. [5], as these algorithms have no notion of safety or active learning.

5 Results

We empirically validate that our meta-active learning approach outperforms baselines across both the
ADMETS and high-dimensional domains in terms of its ability to actively learn latent parameters, its
computation time, and its ability to safely perform these tasks.

(a)

(b)

(c)

Figure 4: This ﬁgure depicts the results of our empirical validation in the ADMETS domain bench-
marking algorithm accuracy per time step (Fig. 4a) and overall (Fig. 4b) as well as accuracy versus
computation time (Fig. 4c).

Active Learning – Results from both the ADMETS and the high-dimensional domains empirically
validate that our algorithm is able to more quickly, in terms of both computation time and information,
learn the optimal control parameters of a system (Fig. 4a) and learn a system’s dynamics (Fig. 5a)
while remaining safe. In the ADMETS domain, our model on average selects an action which results
in a 58% higher information gain than BaO and 87% higher information gain than Meta BO on

(a)

(b)

(c)

Figure 5: This ﬁgure depicts the results of our empirical validation in the high-dimensional domain
benchmarking algorithm accuracy per time step (Fig. 5a) and overall (Fig. 5b) as well as algorithm
accuracy versus computation time (Fig. 5c). Error is calculated in batches of three time steps, enabling
the robot to deviate from the safe region temporarily to gain information. We note that Meta BO is
not included in this domain as it was designed for optimization tasks, e.g. the ADMETS domain.

7

average. In the high-dimensional domain, Fig 5a depicts the advantage of our method as a function of
the number of the number of actions taken to learn the system’s dynamical model; we achieve a 49%
and 46% improvement over [24] and [25], respectively and a 46% improvement over [27] (Fig. 5b).

Computation Time – We demonstrate that our method also outperforms previous work in terms
of computation time. Across both domains, our approach not only achieves a more efﬁcient reduction
in model error and improvement in information gain, we are also faster than all baselines in the
more challenging, high-dimensional (Fig. 5c) environment. In the lower-dimensional, ADMETS
environment (Fig. 4c), BaO has a slight advantage in computation time, but our algorithm trades the
time for a 58% information gain over BaO. Additionally, we are 68x faster than LAL and 61x faster
than Meta BO, the two other meta-learning approaches we benchmark against.

Safety – We empirically validate that we achieve an 87% probability that the aircraft will return to
the sphere of safety while greatly increasing information gain at each time step as shown in Fig 5a,
compared with 99.9% safe return to the sphere when maximizing safety (i.e., λ1 ← 0, resulting in
passive learning). Furthermore, in the ADMETS domain, we are able to guarantee with a probability
of 97% that seizures will not occur while only losing 15% information gain compared to when no
safety constraints are imposed.

5.1 Discussion

Through our empirical investigation, we have demonstrated that our meta-learned acquisition function
operating within a chance-constrained optimization framework outperforms prior work in active
learning. Speciﬁcally, we are able to simultaneously achieve an improvement in information gain
via increased sample efﬁciency and decreased computation time. We achieve a 46% increase in
information gain while still achieving a 20% speedup in computation compared to active learning
baselines and 60x faster computation time compared to our meta learning baseline. Our novel,
deep learning architecture, demonstrates a unique ability to leverage the power of feature learning
across time-series data within a LSTM neural network and the utility of deep Q-learning, within
mathematical optimization with chance constraints to explicitly tradeoff safety and active learning.

6 Related Work

Our work lies at the crossroads of active learning, meta-learning and safe learning. We discuss the
contributions of our work and why our approach is novel in light of previous approaches to active
learning, meta-learning and safe learning.

Active Learning - Active learning acquisition functions provide heuristics to selecting the candidate
unlabeled training data sample that, if the label were known, would provide the most information to
the model being learned [28, 29, 30, 24]. Snoek et al. [31] discusses various Gaussian Process based
methods, and Konyushkova et al. [27] presents a method by which to predict the error reduction from
a data sample via training on prior experience. Previous literature has also investigated on-the-ﬂy
active learning and meta-active learning [32]. For example, the work by Geifman and El-Yaniv [33]
attempts to actively learn the neural network architecture that is most appropriate for the problem at
hand and can be integrated with an active learning querying strategy. Kirsch et al. [34] takes a similar
approach to Bayesian batch active learning and scores samples based on mutual information. Pang
et al. [15] proposed a method by which to learn an acquisition function which can generalize to a
variety of classiﬁcation problems. However, this work has only been demonstrated for classiﬁcation.

Meta-Learning for Dynamics - Prior work has attempted to address the problem of learning altered
dynamics via a meta-learning approach [35]. For example Belkhale et al. [36] investigates a meta-
learning approach to learn the altered dynamics of an aircraft carrying a payload; the authors train a
neural network on prior data to predict various unknown environmental and task factors allowing the
model to adapt to new payloads. Finn et al. [37] present a meta-learning approach to quickly learning
a control policy. In this approach, a distribution over prior model parameters that are most conducive
to learning of the new dynamics is meta-learned ofﬂine. While this approach provide fast policies for
learning new dynamics, it does not explicitly reason about sample efﬁciency or safety.

Safe Learning - Prior work has investigated safe learning in the context of safe Bayesian optimization
and safe reinforcement learning. For example, Yanan et al. [38] develops the algorithm SafeOpt

8

which balances exploration and exploitation to learn an unknown function. Their algorithm assumes
that the unknown function meets a variety of assumptions including that the unknown function meets
regularity conditions expressed via a Gaussian process prior and that it is Lipschitz continuous.
Turchetta et al. [39] addresses the problem of safely exploring an MDP by deﬁning an a priori
unknown safety constraint which is updated during exploration and Zimmer et al. [40] utilizes a
Gaussian process for safely learning time series data, including an active learning component to
improve sample efﬁciency. However, neither of these approaches incorporate the knowledge from
prior data to increase sample efﬁciency, limiting their ability to choose the optimal action.

7 Conclusion

In this paper we demonstrate a state of the art meta-learning approach to active learning for control.
By encoding the context of the dynamics via an LSTM and learning a Q-function – which we encode
into a mixed-integer optimization framework – that predicts the expected information gain of taking a
given action, our framework is able to efﬁciently and safely learn the nature of the altered dynamics.
We compare our approach to baseline acquisition functions and demonstrate that ours is superior in
both computation time and information gain, achieving a 46% increase in information gain while still
achieving a 20% speedup in computation time compared to state-of-the-art acquisition functions and
more than a 58% higher information compared to Bayesian approaches.

8 Broader Impact

Beneﬁciaries – This work has far reaching implications in various domains including healthcare
and robotics. The ability to learn from small heterogeneous datasets in healthcare and apply this
knowledge to future patients has the potential to greatly improve patient care and outcomes. For
instance, determining the optimal parameters for a deep brain stimulation device to reduce seizures in
epilepsy patients takes extensive time and effort when done manually by a physician. We demonstrate
that our algorithm can efﬁciently and safely determine the optimal setting for deep brain stimulation
via learning from past data, thus potentially decreasing the time that a patient must live with seizures.
Furthermore, as autonomous robotic systems become more common in our daily lives, there is an
increased need to for these systems to be able to safely and efﬁciently learn the nature of alterations
in their dynamics. We demonstrate that our algorithm can safely learn the damaged dynamics of an
aircraft in subsecond time while imposing safety constraints to prevent unrecoverable conﬁgurations
from occurring.

Negatively affected parties – We do not believe that any party will be negatively impacted by
our work. Our work seeks to push the boundaries of safe active learning and to improve upon
complementary work in the ﬁeld.

Implications of failure – A failure of our method will result in a less than optimal policy when
seeking out new data. However, due to our algorithm’s ability to impose safety constraints, as
long as these are correctly speciﬁed by the end user, such a failure would not be catastophic or
life-threatening.

Bias and Fairness – The learned acquisition function will be biased based on the distribution of
the ofﬂine training data set. This bias may be overcome by diversifying the training data set.

References

[1] S. Posporelis, A. S. David, K. Ashkan, and P. Shotbolt, “Deep brain stimulation of the memory
circuit: improving cognition in alzheimer’s disease,” Journal of Alzheimer’s Disease, vol. 64,
no. 2, pp. 337–347, 2018.

[2] O. Ashmaig, M. Connolly, R. E. Gross, and B. Mahmoudi, “Bayesian Optimization of Asyn-
chronous Distributed Microelectrode Theta Stimulation and Spatial Memory,” Proceedings of
the Annual International Conference of the IEEE Engineering in Medicine and Biology Society,
EMBS, vol. 2018-July, pp. 2683–2686, 2018.

9

[3] Y. Sui, J. Burdick, Y. Yue et al., “Stagewise safe bayesian optimization with gaussian processes,”

in International Conference on Machine Learning, 2018, pp. 4781–4789.

[4] C. Finn, P. Abbeel, and S. Levine, “Model-Agnostic Meta-Learning for Fast Adaptation of Deep

Networks,” Arxiv, 2017.

[5] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, “Learning to
Adapt in Dynamics, Real-World Environments Through Meta-Reinforcement Learning,” Arxiv,
pp. 1–17, 2019.

[6] J. Wang, Z. Kurth-Nelson, D. Turumala, and H. Soyer, “Learning to reinforcement learn,” Arxiv,

pp. 1–17, 2016.

[7] M. Andrychowicz, M. Denil, S. G. Colmenarejo, and M. W. Hoffman, “Learning to learn by
gradient descent by gradient descent,” Conference on Neural Information Processing Systems
(Conference on Neural Information Processing Systems (Nips), pp. 1–17, 2016.

[8] A. Cully, J. Clune, D. Tarapore, and J. B. Mouret, “Robots that can adapt like animals,” Nature,

vol. 521, pp. 503–507, 2015.

[9] T. Koller, F. Berkenkamp, M. Turchetta, and A. Krause, “Learning-Based Model Predictive
Control for Safe Exploration,” Proceedings of the IEEE Conference on Decision and Control,
vol. 2018-Decem, pp. 6059–6066, 2019.

[10] F. Berkenkamp, A. P. Schoellig, M. Turchetta, and A. Krause, “Safe Model-based Reinforcement

Learning with Stability Guarantees,” Nips, 2017.

[11] L. Wang, E. A. Theodorou, and M. Egerstedt, “Safe Learning of Quadrotor Dynamics Using
Barrier Certiﬁcates,” 2018 IEEE International Conference on Robotics and Automation (ICRA),
pp. 2460–2465, 2018.

[12] A. Schrijver, Theory of linear and integer programming.

John Wiley & Sons, 1998.

[13] F. A. Gers, J. Schmidhuber, and F. Cummins, Learning to forget: Continual prediction with

LSTM.

IET, 1999.

[14] I. Muslea, S. Minton, and C. A. Knoblock, “Active learning with multiple views,” Journal of

Artiﬁcial Intelligence Research, vol. 27, pp. 203–233, 2006.

[15] K. Pang, M. Dong, Y. Wu, and T. Hospedales, “Meta-Learning Transferable Active Learning
Policies by Deep Reinforcement Learning,” Arxiv, pp. 1–8, 2018. [Online]. Available:
http://arxiv.org/abs/1806.04798

[16] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-Learning in Neural Networks:
A Survey,” Arxiv, pp. 1–23, 2020. [Online]. Available: http://arxiv.org/abs/2004.05439

[17] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. The MIT

Press, 2018. [Online]. Available: http://incompleteideas.net/book/the-book-2nd.html

[18] M. Ganger, E. Duryea, and W. Hu, “Double sarsa and double expected sarsa with shallow and
deep learning,” Journal of Data Analysis and Information Processing, vol. 4, no. 4, pp. 159–176,
2016.

[19] B. Efron, “Bootstrap Methods: Another Look at the Jackknife Author(s),” Institute of Mathe-

matical Statistics Stable URL, vol. 7, no. 1, pp. 1–26, 2020.

[20] J. Franke and M. A. Nuemann, “Bootstrapping Neural Networks,” Neural Computation, vol. 12,

no. 8, pp. 1929–1949, 1998.

[21] E. J. Watkiss, “Flight dynamics of an unmanned aerial vehicle,” 1994. [Online]. Available:

https://calhoun.nps.edu/handle/10945/28222

[22] Y. Zhang, C. C. de Visser, and Q. P. Chu, “Aircraft Damage Identiﬁcation and Classiﬁcation
for Database-Driven Online Flight-Envelope Prediction,” Journal of Guidance, Control, and
Dynamics, vol. 41, no. 2, pp. 1–12, 2017.

[23] J. A. Ouellette, “Flight Dynamics and Maneuver Loads on a Commercial Aircraft with Discrete

Source Damage,” Thesis Master, Aerospace Engineering, 2010.

[24] T. Hastie, R. Tibshirani, and J. Friedman, “Model Assessment and Selection,” in The Elements
of Statistical Learning Data Mining, Inference, and Prediction. Springer-Verlag, 2017, pp.
249–252.

10

[25] M. L. Schrum and M. C. Gombolay, “When Your Robot Breaks : Active Learning During Plant

Failure,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 438–445, 2020.

[26] Z. Wang, B. Kim, and L. P. Kaelbling, “Regret bounds for meta Bayesian optimization with an
unknown Gaussian process prior,” Advances in Neural Information Processing Systems, vol.
2018-December, no. NeurIPS, pp. 10 477–10 488, 2018.

[27] K. Konyushkova, S. Raphael, and P. Fua, “Learning Active Learning from Data,” Conference

on Neural Information Processing Systems (Nips), 2017.

[28] R. Burbidge, J. J. Rowland, and R. D. King, “Active Learning for Regression Based on Query by
Committee,” Intelligent Data Engineering and Automated Learning - IDEAL 2007, pp. 209–218,
2007.

[29] M. Hasenjager and H. Ritter, “Active Learning with Local Models 1 Introduction,” Neural

Processing Letters, pp. 107–117, 1998.

[30] W. Cai, M. Zhang, and Y. Zhang, “Batch mode active learning for regression with expected
model change,” IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 7,
pp. 1668–1681, 2017.

[31] J. Snoek, H. Larochelle, and R. Adams, “Practical Bayesian Optimization of Machine Learning

Algorithms,” Conference on Neural Information Processing, 2012.

[32] P. Bachman, A. Sordoni, and A. Trischler, “Learning Algorithms for Active Learning,” Arxiv,

2016.

[33] Y. Geifman and R. El-Yaniv, “Deep Active Learning with a Neural Architecture Search,”

NeurIPS, 2018. [Online]. Available: http://arxiv.org/abs/1811.07579

[34] A. Kirsch, J. van Amersfoort, and Y. Gal, “BatchBALD: Efﬁcient and Diverse Batch
Acquisition for Deep Bayesian Active Learning,” NeurIPS, 2019. [Online]. Available:
http://arxiv.org/abs/1906.08158

[35] I. Clavera, A. Nagabandi, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, “Learning to Adapt :

Meta-Learning for Model-Based Control,” Arxiv, 2009.

[36] S. Belkhale, R. Li, G. Kahn, R. McAllister, R. Calandra, and S. Levine, “Model-Based
Meta-Reinforcement Learning for Flight with Suspended Payloads,” Arxiv, 2020. [Online].
Available: http://arxiv.org/abs/2004.11345

[37] C. Finn, K. Xu, and S. Levine, “Probabilistic Model-Agnostic Meta-Learning,” Conference on

Neural Information Processing Systems (NeurIPS), 2018.

[38] Y. Sui, A. Gotovos, J. W. Burdick, and A. Krause, “Safe exploration for optimization with
Gaussian processes,” 32nd International Conference on Machine Learning, ICML 2015, vol. 2,
pp. 997–1005, 2015.

[39] M. Turchetta, F. Berkenkamp, and A. Krause, “Safe exploration in ﬁnite Markov decision
processes with Gaussian processes,” Advances in Neural Information Processing Systems, pp.
4312–4320, 2016.

[40] C. Zimmer, M. Meister, and D. Nguyen-Tuong, “Safe active learning for time-series modeling
with Gaussian processes,” Advances in Neural Information Processing Systems, vol. 2018-
December, no. NeurIPS, pp. 2730–2739, 2018.

11

