Diﬀerentiable thermodynamic modeling

Pin-Wen Guan1, a)
Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213,
USA

(Dated: 23 February 2021)

A new framework of thermodynamic modeling is proposed by introducing the concept of differentiable program-
ming, where all the thermodynamic observables including both thermochemical quantities and phase equilibria
can be differentiated with respect to the underlying model parameters, thus allowing the models learned by
gradient-based optimization. It is shown that thermodynamic modeling and deep learning can be seamlessly in-
tegrated and uniﬁed within this framework. A preliminary successful application is demonstrated for the Cu-Rh
system. It is expected that thermodynamic modeling in a deep learning style can increase prediction power of
models, and provide more effective guidance for design, synthesis and optimization of multi-component materi-
als with complex chemistry via learning various types of data.

Various phenomena in materials have their roots in ther-
modynamics, of which the fundamental laws have been
well established very early. However, thermodynamics of
materials is far from being well studied, due to the vast
number of degrees of freedom including composition, tem-
perature, pressure, strain, external ﬁeld and dimensionality
(e.g., bulk vs nano).
In addition, the underlying micro-
scopic mechanisms contributing to macroscopic thermo-
dynamic observables are diverse, including lattice disor-
der, atomic vibration, electronic excitation, magnetic ex-
citation, etc., making accurate and comprehensive theoret-
ical descriptions difﬁcult. Therefore, developing a sufﬁ-
ciently efﬁcient methodology is highly crucial for thermo-
dynamic modeling to guide design and synthesis of mate-
rials more effectively. For this end, there have emerged
many encouraging progresses in recent years. One no-
table direction is the usage of machine learning (ML)
techniques1–13, the generalizability of which allows pre-
dictions based on limited amount of data. So far, the meth-
ods along this direction can be classiﬁed into two types
based on the types of training data and predicted quanti-
ties of the ML models. The type-I models are trained on
and predict thermochemical quantities1–8, while the type-
II models are trained on and predict phase equilibria9–13.
Naturally, one may ask if there can be a cross-type learn-
ing, i.e., learning thermochemical quantities from phase
equilibria (or learning phase equilibria from thermochem-
ical quantities, which is essentially within type-I, since the
former can be derived if the latter are fully determined,
thus this type is ignored here). Such type of learning has
the following advantages. First, it is quite often that ther-
mochemical data are scarce or not reliable enough, thus
training on phase equilibria is the desirable or even the
only option. Second, prediction of thermochemical quan-
tities like Gibbs energies allows calculations of not only
phase equilibria, but also useful parameters such as ther-
modynamic factor in diffusion. Third, phase equilibria
calculated based on thermochemical quantities are more
physics-based compared with direct predictions by ML.
Therefore, it is of great practical and scientiﬁc interests to
develop this new type of ML paradigm for thermodynam-
ics.

In this work, a new framework capable of learning ther-
modynamic potentials from both thermochemical data and

a)Electronic mail: pinweng@andrew.cmu.edu

phase equilibrium data is proposed, and it will be shown
that thermodynamic modeling and deep learning are seam-
lessly uniﬁed within this framework. This is achieved
by introducing differentiable programming (DP)14, a pro-
gramming paradigm where the computation ﬂow can
be differentiated throughout via automatic differentiation,
thus allowing gradient-based optimization of parameters
in the ﬂow. Due to its ability to incorporate physics into
deep learning, DP has increasing applications in differ-
ent ﬁelds recently, including molecular dynamics15, tensor
networks16, quantum chemistry17 and density functional
theory (DFT)18.

Consider a set of phases {Gθi}. The thermodynamic

potential of the phase θi can be represented as

Gθi(F(x),C; Aθi)

which is a function of the descriptor F(x) based on compo-
sition x, external conditions C (e.g., temperature and pres-
sure) with parameters Aθi. Usage of F(x) is also called
feature engineering. When F is identity, i.e., F(x) = x,
raw composition is directly used. Gθi can be in various
forms that are differentiable, such as the conventional one
based on polynomials19,20 and the more ML-oriented one
based on deep neural networks5.

Once {Gθi} of all the relevant phases are known, each
thermochemical quantity g j and phase equilibrium e j can
then be calculated, i.e.,

(cid:98)g j = (cid:98)g j({Gθi}), (cid:98)e j = (cid:98)e j({Gθi})
which are functionals of {Gθi}. The loss function can then
be computed as

L = ∑
j

l(g j, (cid:98)g j) +∑

j

l(e j, (cid:98)e j)

where l is a function measuring the difference between
two values, with a common choice to be the squared error
l(g j, (cid:98)g j) = (g j − (cid:98)g j)2. Finally, the parameter set {Aθi} of
thermodynamic potentials can be obtained by minimizing
the loss function

{Aθi} = arg min
{Aθi
}

L

The minimization of the loss function L relies on calcu-
}L, which is made possible by DP.
lating its gradient ∇{Aθi
Obviously, the most non-trivial part in the above compu-
tation ﬂow for differentiation is the phase equilibrium cal-
culation for e j, which generally requires minimization of

1
2
0
2

b
e
F
1
2

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

1
v
5
0
7
0
1
.

2
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
the thermodynamic potential of the whole system, the ma-
jor subject of research in computational thermodynamics.
The minimization can be usually divided into two main
steps21. The ﬁrst step is a global minimization where the
thermodynamic potential surface is sampled and an initial
solution is generated. The second step is reﬁning calcula-
tions to obtain the ﬁnal solution satisfying all the equilib-
rium conditions.

The scheme of differentiable thermodynamic modeling
is shown in ﬁg. 1. The forward mode is essentially similar
to a routine thermodynamic calculation. In the backward
mode, the computations of gradients propagate from the
loss function towards the model parameters through a se-
ries of intermediate steps, and all the single steps are ﬁnally
}L based on elementary
assembled together to obtain ∇{Aθi
rules of differentiation. It can be seen that such scheme
is very similar to that in deep learning. In fact, differen-
tiable programming can be regarded an extension of deep
learning with the use of diverse differentiable components
beyond classical neural networks.

The above general framework has been implemented for
a simple two-phase binary system Cu-Rh for demonstra-
tion, which has a liquidus and a solidus between the fcc
phase and the liquid (liq) phase besides a fcc miscibility
gap right below the solidus. To calculate phase equilib-
ria under given temperature and pressure, Gibbs energy is
used as the thermodynamic potential. The four-parameter
model by Lesnik et al.22 is taken as the target to learn:

Gθ = xθ

◦Gθ ,Rh + (1 − xθ )◦Gθ ,Cu

(cid:48)
+ RT [xlnx + (1 − x)ln(1 − x)] + x(1 − x)(Aθ + A

θ x)

where θ =fcc,liq, R is the gas constant, xθ is the Rh com-
position in the θ phase, and ◦Gθ ,Rh and ◦Gθ ,Cu are the
standard Gibbs energies of Rh and Cu in the θ phase, re-
spectively. The four model parameters have been assessed
(cid:48)
as (A f cc, A
liq) = (14.609, 11.051, 8.414, 19.799)
(in kJ/mol), which are regarded as the true values in the
present work. From this set of model parameters, the train-
ing data is generated, including phase boundaries at 1000-
2200 K.

f cc, Aliq, A

(cid:48)

The Gibbs energy minimization is divided into two
steps. The ﬁrst step is a global minimization on a grid
of compositions based on a convex-hull algorithm, gener-
ating an approximate solution which is to be reﬁned in the
second step. The second step is an iterative self-consistent
procedure, where the Newton–Raphson method is used
to solve phase equilibria under ﬁxed chemical potential
which is then updated based on the newest solved phase
equilibria in each iteration, and the iterations stop when
convergence is achieved. The loss function for this exam-
ple system is deﬁned as

L = ∑
i

Lliq− f cc,i +∑
i

L f cc− f cc,i

with

Lliq− f cc,i =






α[( (cid:99)xliq(Ti) − xliq(Ti))2 + ((cid:100)x f cc(Ti) − x f cc(Ti))2]
if (cid:99)xliq(Ti) and (cid:100)x f cc(Ti) computable
otherwise

)2

(min
x

DF
RTi

and

2

L f cc− f cc,i =
α[( (cid:91)x f cc#1(Ti) − x f cc#1(Ti))2 + ( (cid:91)x f cc#2(Ti) − x f cc#2(Ti))2]
if (cid:91)x f cc#1(Ti) and (cid:91)x f cc#2(Ti) computable
∂ 2G f cc(x,Ti)
otherwise
∂ x2

[ReLU( 1
RTi

)]2

min
x






where the rectiﬁed linear unit ReLU(z) = max(0, z), and α
is a scaling factor for improving convergence in minimiza-
tion of the loss function. In the present case, α = 100 is
used. The driving force DF of the metastable phase is the
distance in terms of Gibbs energy between the stable tan-
gent plane and a tangent plane parallel to the metastable
phase20. The above loss function contains two contribu-
tions from the liquid-fcc equilibrium and the fcc miscibil-
ity gap respectively. Since the target type of phase equilib-
rium may not be correctly reproduced if the model param-
eters have large deviation from their target values, penal-
ties are imposed instead in such scenarios to favor the re-
)]2 < 0, i.e.,
gions where min
the liquid-fcc equilibrium and the fcc miscibility gap exist
at some compositions.

∂ 2G f cc(x,Ti)
∂ x2

DF = 0 and min

x

x

A differentiable program for calculating the above loss
function is written using JAX15, a machine learning library
which can automatically differentiate Python and NumPy
functions. Notably, JAX can differentiate through control
ﬂows like loops and branches, which are key structures in
the Gibbs energy minimization. The gradient of the loss
}L, is then obtained by automatic differen-
function, ∇{Aθi
tiation of the program. Given its gradient, the loss func-
tion is minimized using a gradient-based optimization al-
gorithm, the L-BFGS-B method.

The training process for the Cu-Rh model system is
shown in ﬁg. 2. Despite starting with quite unreason-
able initial model parameters, the minimization of the loss
function is quite efﬁcient, with good convergence achieved
within a few tens of steps, which is made possible by the
explicitly calculated gradient of the loss function. In this
case, the liquid-fcc equilibrium always exists during train-
ing, thus the loss function has only two non-zero contribu-
tions from displacement of the phase boundaries (liquid-
fcc and fcc-fcc) and absence of the phase separation (fcc
miscibility gap) caused by inaccurate model parameters,
respectively. The latter contribution vanishes after the
fcc miscibility gap is made exist, and then the training
is only accompanied with quantitative adjustment of the
phase boundaries. With the loss function minimized, its
four-component gradient driving the training process ap-
proaches the zero vector, and the model parameters also
converges to the true values. To better understand how the
thermodynamic model evolves, the trajectories of Gibbs
energies of the two involved phases at 1200 K in the train-
ing are plotted. In consistence with initial absence of the
fcc miscibility gap, the Gibbs energy of the fcc phase is ini-
tially a convex function without spinodal decomposition,
but gradually trained to be non-convex leading to phase
separation. The phase diagram of the Cu-Rh system pre-
dicted by the trained model is shown in ﬁg. 3, along with
the training data. It can be seen that the predicted phase
diagram is in excellent agreement with the training data,
meaning the present model training is highly successful.

3

FIG. 1. Scheme of differentiable thermodynamic modeling. The forward computation ﬂow is indicated by black arrows. The back-
propagation gradient ﬂow is indicated by red arrows, which is used to calculate the gradient of the loss function with respect to model
parameters, ∇{Aθi }L, for minimizing the loss function. The variable parts in the differentiation are ﬁlled with colors, while the invariable
parts (the training data) have no ﬁlled colors.

The above example provides a preliminary success-
ful application of differentiable thermodynamic modeling,
which is able to learn thermodynamics from mixed types
of data on thermochemistry and phase equilibria in a deep
learning style. Due to simplicity of the binary system and
limited amount of training data, a polynomial with raw el-
emental compositions directly used as inputs is a suitable
form to represent the excess Gibbs energy of each phase,
which is also the routine practice in conventional ther-
modynamic modeling. However, such representation may
suffer “curse of dimensionality” in high-dimension chemi-
cal space. For instance, considering an extreme case where
a phase contains 100 elements, there are 100 compositional
100 = 4950 binary interactions, C3
variables, C2
100=161700
ternary interactions and C4
100=3921225 quaternary interac-
tion, totaling a daunting number of model parameters with
each interaction represented by a conventional parameter-
ized polynomial. It is therefore desirable to explore an al-
ternative representation of the Gibbs energy. Due to its
strong expressivity, the neural network has emerged as a
promising tool for this purpose, and there have been quite
a few related works in the literature. Using elemental com-
positions as inputs, a deep neural network trained on DFT
data has achieved a mean average error of 0.05 eV/atom
in predicting formation enthalpies3. There have been also
efforts in introducing physical attributes such like elec-
tronegativity and atomic radius into inputs by feature en-
gineering to alleviate the difﬁculty due to vast dimensions
of the composition space23. This is still a ﬁeld under ac-
tive research, and further discussions are beyond the scope
of the present work. However, it is quite obvious that the

present framework of differentiable thermodynamic mod-
eling can provide a necessary platform for introducing neu-
ral networks to learn thermodynamics from diverse types
of data.

From the perspective of mapping, a set of phase equilib-
rium data is actually a sample from the map f : (x,C) →
e, where e is the phase equilibrium at composition x
and external condition C. To incorporate more physics,
thermodynamic potentials {Gθ } is introduced as interme-
diate variables with two maps f1 : (x,C) → {Gθ } and
f2 : {Gθ } → e, which are usually called “thermodynamic
model” and “phase equilibrium calculation”, respectively.
The map f is just their composition:

f = f2 ◦ f1

f1 and f2 are very different in nature.

f1 is
Note that
usually complicated and sometimes even obscure, pack-
aging the whole physics of each single-phase material that
is difﬁcult to calculate explicitly without capturing all the
atomic and electronic details, but this is just the part deep
In contrast, f2 is more
learning can ﬁnd its largest use.
straightforward, thus most suitable for a direct physical
computation. Differentiable thermodynamic modeling of-
fers a seamless integration of these two components.

In summary, the present work proposes a deep learning
framework for thermodynamic modeling, which is termed
It is based on
differentiable thermodynamic modeling.
differentiable programming, which allows differentiation
throughout the computation ﬂow and therefore gradient-
based optimization of parameters. Under this framework,
thermodynamics can be learned from different types of

4

FIG. 2. Evolution of various quantities in the training process for the Cu-Rh model system: (a) loss function and the contributions from
different origins. (b) gradient of loss function. (c) model parameters. (d) Gibbs energies of involved phases at 1200 K. Heavier colors
are corresponded to larger training steps.

with complex chemistry, as well as design, synthesis and
optimization of multi-component materials.

RESEARCH DATA

The data that support the ﬁndings of this study are avail-
able from the corresponding author upon reasonable re-
quest. The data and code used in this work will be also
made publicly available on Github.

ACKNOWLEDGEMENT

acknowledge

to
like
and
discussions
Learning Webinar

inter-
author would
The
from the
esting
presentations
Series
Scientiﬁc Machine
Acknowl-
(https://www.cmu.edu/aced/sciML.html).
edgment
is also made to the Extreme Science and
Engineering Discovery Environment (XSEDE) for pro-
viding computational
through Award No.
resources
TG-CTS180061.

FIG. 3. Phase diagram of the Cu-Rh system predicted by the
trained model, in comparison with the training data.

data on thermochemistry and phase equilibria in a deep
learning style, and thermodynamic modeling and deep
learning are de facto uniﬁed and indistinguishable. Its pre-
liminary success is demonstrated by application in train-
ing a model for the Cu-Rh system.
It is expected that
differentiable thermodynamic modeling can facilitate ex-
ploration of thermodynamics of multi-component systems

1J. Schmidt, J. Shi, P. Borlido, L. Chen, S. Botti, and M. A. Marques,
“Predicting the thermodynamic stability of solids combining density
functional theory and machine learning,” Chemistry of Materials 29,
5090–5103 (2017).
2W. Ye, C. Chen, Z. Wang, I.-H. Chu, and S. P. Ong, “Deep neural
networks for accurate predictions of crystal stability,” Nature commu-
nications 9, 1–6 (2018).
3D. Jha, L. Ward, A. Paul, W.-k. Liao, A. Choudhary, C. Wolverton, and
A. Agrawal, “Elemnet: Deep learning the chemistry of materials from
only elemental composition,” Scientiﬁc reports 8, 1–13 (2018).
4S. Ubaru, A. Mi˛edlar, Y. Saad, and J. R. Chelikowsky, “Formation
enthalpies for transition metal alloys using machine learning,” Physical
Review B 95, 214102 (2017).
5G. H. Teichert, A. Natarajan, A. Van der Ven, and K. Garikipati, “Ma-
chine learning materials physics: Integrable deep neural networks en-
able scale bridging by learning free energy functions,” Computer Meth-
ods in Applied Mechanics and Engineering 353, 201–216 (2019).
6C. Lapointe, T. D. Swinburne, L. Thiry, S. Mallat, L. Proville, C. S.
Becquart, and M.-C. Marinica, “Machine learning surrogate models for
prediction of point defect vibrational entropy,” Physical Review Mate-
rials 4, 063802 (2020).
7K. Ryan, J. Lengyel, and M. Shatruk, “Crystal structure prediction via
deep learning,” Journal of the American Chemical Society 140, 10158–
10168 (2018).
8K. Kaufmann, D. Maryanovsky, W. M. Mellor, C. Zhu, A. S. Rosen-
garten, T. J. Harrington, C. Oses, C. Toher, S. Curtarolo, and K. S.
Vecchio, “Discovery of high-entropy ceramics via machine learning,”
Npj Computational Materials 6, 1–9 (2020).
9Y. Zhang, C. Wen, C. Wang, S. Antonov, D. Xue, Y. Bai, and Y. Su,
“Phase prediction in high entropy alloys with a rational selection of
materials descriptors and machine learning models,” Acta Materialia
185, 528–539 (2020).

10W. Huang, P. Martin, and H. L. Zhuang, “Machine-learning phase pre-
diction of high-entropy alloys,” Acta Materialia 169, 225–236 (2019).
11G. Pilania, J. E. Gubernatis, and T. Lookman, “Structure classiﬁca-
tion and melting temperature prediction in octet ab solids via machine

5

learning,” Physical Review B 91, 214302 (2015).

12A. Seko, T. Maekawa, K. Tsuda, and I. Tanaka, “Machine learning
with systematic density-functional theory calculations: Application to
melting temperatures of single-and binary-component solids,” Physical
Review B 89, 054303 (2014).

13P.-W. Guan and V. Viswanathan, “Meltnet: Predicting alloy melting
temperature by machine learning,” (2020), arXiv:2010.14048 [cond-
mat.mtrl-sci].

14A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind,
(2018),

“Automatic differentiation in machine learning: a survey,”
arXiv:1502.05767 [cs.SC].

15S. S. Schoenholz and E. D. Cubuk, “Jax, m.d.: A framework for differ-

entiable physics,” (2020), arXiv:1912.04232 [physics.comp-ph].

16H.-J. Liao, J.-G. Liu, L. Wang, and T. Xiang, “Differentiable program-

ming tensor networks,” Physical Review X 9, 031041 (2019).

17T. Tamayo-Mendoza, C. Kreisbeck, R. Lindh, and A. Aspuru-Guzik,
“Automatic differentiation in quantum chemistry with applications to
fully variational hartree–fock,” ACS central science 4, 559–566 (2018).
18L. Li, S. Hoyer, R. Pederson, R. Sun, E. D. Cubuk, P. Riley, K. Burke,
et al., “Kohn-sham equations as regularizer: Building prior knowledge
into machine-learned physics,” Physical Review Letters 126, 036401
(2021).

19O. Redlich and A. Kister, “Algebraic representation of thermodynamic
properties and the classiﬁcation of solutions,” Industrial & Engineering
Chemistry 40, 345–348 (1948).

20H. L. Lukas, S. G. Fries, and B. Sundman, Computational thermody-
namics: the CALPHAD method, Vol. 131 (Cambridge University Press,
Cambridge, 2007) p. 324.

21M. Piro and S. Simunovic, “Global optimization algorithms to compute
thermodynamic equilibria in large complex systems with performance
considerations,” Computational Materials Science 118, 87–96 (2016).
22D. Chakrabarti and D. Laughlin, “The cu- rh (copper-rhodium) system,”

Journal of Phase Equilibria 2, 460–462 (1982).

23L. Ward, A. Agrawal, A. Choudhary, and C. Wolverton, “A general-
purpose machine learning framework for predicting properties of inor-
ganic materials,” npj Computational Materials 2, 1–7 (2016).

