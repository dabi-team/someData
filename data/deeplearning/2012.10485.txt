RAILS: A Robust Adversarial Immune-inspired Learning System

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse
University of Michigan

0
2
0
2

c
e
D
8
1

]

G
L
.
s
c
[

1
v
5
8
4
0
1
.
2
1
0
2
:
v
i
X
r
a

Abstract

Adversarial attacks against deep neural net-
works are continuously evolving. Without ef-
fective defenses, they can lead to catastrophic
failure. The long-standing and arguably
most powerful natural defense system is the
mammalian immune system, which has suc-
cessfully defended against attacks by novel
pathogens for millions of years. In this paper,
we propose a new adversarial defense frame-
work, called the Robust Adversarial Immune-
inspired Learning System (RAILS). RAILS
incorporates an Adaptive Immune System
Emulation (AISE), which emulates in silico
the biological mechanisms that are used to
defend the host against attacks by pathogens.
We use RAILS to harden Deep k-Nearest
Neighbor (DkNN) architectures against eva-
sion attacks. Evolutionary programming is
used to simulate processes in the natural
immune system: B-cell ﬂocking, clonal ex-
pansion, and aﬃnity maturation. We show
that the RAILS learning curve exhibits sim-
ilar diversity-selection learning phases as ob-
served in our in vitro biological experiments.
When applied to adversarial image classiﬁca-
tion on three diﬀerent datasets, RAILS de-
livers an additional 5.62%/12.56%/4.74% ro-
bustness improvement as compared to apply-
ing DkNN alone, without appreciable loss of
accuracy on clean data.

1

INTRODUCTION

The state-of-the-art in supervised deep learning (Le-
Cun et al., 2015), has dramatically improved over
the past decade. Deep learning techniques have led

Copyright 2021 by the author(s).

to signiﬁcant improvements in applications such as:
face recognition (Mehdipour Ghazi and Kemal Ekenel,
2016); object detection (Zhao et al., 2019); and natu-
ral language processing (Young et al., 2018). Despite
these successes, deep learning techniques are not re-
silient to adversarial attacks such as evasion attacks on
the inputs and poisoning attacks on the training data
(Goodfellow et al., 2014; Szegedy et al., 2013; Gu et al.,
2017). The adversarial vulnerability of deep neural
networks (DNN) has restricted its applications, moti-
vating researchers to develop eﬀective defense meth-
ods.

Current defense methods can be broadly divided into
three categories:
(1) Adversarial example detection
(Metzen et al., 2017; Feinman et al., 2017; Grosse
et al., 2017; Xu et al., 2017); (2) Robust training
(Madry et al., 2017; Zhang et al., 2019; Cohen et al.,
2019; Shafahi et al., 2019; Wong et al., 2020); and
(3) Depth classiﬁers with natural robustness (Paper-
not and McDaniel, 2018; Samangouei et al., 2018).
The ﬁrst category of methods defends the deep learn-
ing algorithm using simple models for detecting ad-
versarial examples as outliers. However, it has been
shown that adversarial detection methods are not per-
fect and can be easily defeated (Carlini and Wagner,
2017). Robust training aims to harden the model to
deactivate the evasion attack. Known robust train-
ing methods are tailored to a certain level of attack
strength in the context of (cid:96)p-perturbation. Moreover,
the trade-oﬀ between accuracy and robustness presents
challenges (Zhang et al., 2019). Recently alternative
defense strategies have been proposed that implement
depth classiﬁers which are naturally resilient to eva-
sion attacks. Despite these advances, current methods
have diﬃculty providing an acceptable level of robust-
ness to novel attacks (Athalye et al., 2018).

Relative to designing eﬀective defenses to reduce a sys-
tem’s vulnerability to attacks, a natural question to
ask is the following. Can we emulate the naturally ro-
bust biological immune system to improve resiliency?
The mammalian immune system has evolved over mil-
lions of years, resulting in a life-long learning system
that continuously learns from experience to defend

 
 
 
 
 
 
RAILS: A Robust Adversarial Immune-inspired Learning System

against a constant onslaught of diverse attacks from
bacterial, viral, fungal, and other infections. The nat-
ural immune system has evolved a built-in detector to
distinguish non-self components from self components
(Farmer et al., 1986), and has a naturally robust ar-
chitecture (Mesin et al., 2016). Furthermore, the im-
mune system continuously increases its degree of ro-
bustness by adaptively learning from attacks (Mesin
et al., 2020).

Motivated by the natural immune system’s powerful
evolved defense capacities, we propose a new frame-
work, Robust Adversarial Immune-inspired Learning
System (RAILS), that can eﬀectively defend deep
learning architectures against attacks. While RAILS
can be applied to defending against various attacks on
the training/test data, in this paper we restrict atten-
tion to evasion attacks on the input.

Contributions. Compared to
methods, we make the following contributions:

existing defense

• We propose a new adversarial defense framework
(RAILS) that is inspired by the natural immune sys-
tem and show that the learning patterns exhibited by
RAILS aligns with those of the immune system.

• On three diﬀerent datasets, the RAILS implemen-
tation achieves 5.62%/12.56%/4.74% robustness im-
provement over Deep k-Nearest Neighbors (DkNN)
(Papernot and McDaniel, 2018) and higher conﬁdence
scores.

• RAILS robustiﬁes deep learning classiﬁers using an
Adaptive Immune System Emulation (AISE) that em-
ulates adaptive learning (life-long learning) in the nat-
ural immune system by adding virtual B-cell’s (mem-
ory data) to the training data. We show that AISE
hardening of a DkNN provides a 2.3% robustness im-
provement to the DkNN with only 5% augmentation
of the training data.

• The AISE emulated immune system defends against
attacks via mutation and cross-over mechanisms, thus
it is not restricted to (cid:96)p or any speciﬁc type of attack.

Related Work. After it was established that DNNs
were vulnerable to evasion attacks (Szegedy et al.,
2013), diﬀerent types of defense mechanisms have been
proposed in recent years. One intuitive idea is to elim-
inate the adversarial examples through outlier detec-
tion. The authors of Metzen et al. (2017); Grosse et al.
(2017) considered training an additional sub-network
to separate adversarial attacks from benign inputs.
The authors of Feinman et al. (2017); Xu et al. (2017)
use kernel density estimation and Bayesian uncertainty
estimation to identify adversarial examples. The above
approaches rely on the fundamental assumption that

the distributions of benign and adversarial examples
are distinct, an assumption that has been challenged
in Carlini and Wagner (2017).

In addition to adversarial attack detection, other
methods have been proposed that focus on robust
training, aiming to robustify the deep architecture
during the learning phase. Examples include pro-
jected gradient descent (PGD)-based adversarial train-
ing (Madry et al., 2017) and its many variants (Shafahi
et al., 2019; Wong et al., 2020), training via random-
ized smoothing with robustness guarantees within a
(cid:96)2 ball (Cohen et al., 2019), and TRADES which op-
timizes the standard-robust error gap (Zhang et al.,
2019). Though these defenses are eﬀective against ad-
versarial examples with a certain level of (cid:96)p attack
strength there is a sacriﬁce in overall classiﬁcation ac-
In contrast, RAILS is developed to defend
curacy.
against diverse powerful attacks with little sacriﬁce in
accuracy. Moreover, when implemented online, RAILS
can adaptive increase robustness during the inference
stage.

Another approach is to leverage on existing work on
robustifying deep classiﬁers. An example is the deep
k-Nearest Neighbor (DkNN) classiﬁer (Papernot and
McDaniel, 2018) that robustiﬁes against instance per-
turbations by applying kNN’s to features extracted
from each layer. The prediction conﬁdence of the
DkNN can be low due when there are many hidden
layers. Another relevant work is the auxiliary Genera-
tive Adversarial Network (GAN) which can be used for
defending against attacks (Samangouei et al., 2018).
However, it is diﬃcult to properly train and select hy-
perparameters for the GAN model. We will show in
this paper that the proposed RAILS method is an al-
ternative that can provide improved robustness and
conﬁdence scores.

Another line of research relevant to ours is adversar-
ial transfer learning (Liu et al., 2019; Shafahi et al.,
2020), which aims to maintain robustness when there
is covariate shift from training data to test data. We
remark that covariate shift occurs in the immune sys-
tem as it adapts to novel mutated pathogen strain.

2 FROM IMMUNE SYSTEM TO

COMPUTATIONAL SYSTEM

2.1 Learning Strategies of Immune System

Systems robustness is a property that must be de-
signed into the architecture, and one of the greatest
examples of this is within the mammalian adaptive
immune system (Rajapakse and Groudine, 2011). The
architecture of the adaptive immune system ensures a
robust response to foreign antigens, splitting the work

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

Figure 1: Simpliﬁed Immune System (left) and RAILS Computational Workﬂow (right).

between active sensing and competitive growth to pro-
duce an eﬀective antibody. Sensing of a foreign at-
tack leads to antigen-speciﬁc B-cells ﬂocking to lymph
nodes, and forming temporary structures called ger-
minal centers (De Silva and Klein, 2015; Farmer et al.,
1986).
In the expansion phase, a diverse initial set
of B-cells bearing antigen-speciﬁc immunoglobulins di-
vide symmetrically to populate the germinal center in
preparation for and optimization (aﬃnity maturation).
The B-cells with the highest aﬃnity to the antigen are
selected to asymmetrically divide and mutate, which
leads to new B-cells with higher aﬃnity to the antigen
(Mesin et al., 2016). Memory B-cells are stored within
this step, which can be used to defend against similar
attacks in the future. B-cells that reach consensus, or
achieve a threshold aﬃnity against the foreign antigen,
undergo terminal diﬀerentiation into plasma B-cells.
Plasma B-cells represent the actuators of the humoral
adaptive immune response. The adaptive immune sys-
tem is incredibly complex, but we can simplify its ro-
bust learning process into these ﬁve steps: sensing,
ﬂocking, expansion, optimization, and consensus (Fig-
ure 1) (Cucker and Smale, 2007; Rajapakse and Smale,
2017).

2.2 From Biology to Computation

Motivated by recent advances in understanding the bi-
ological immune system, we propose a new in silico
defense strategy - the Robust Adversarial Immune-
inspired Learning System (RAILS). This computa-
tional system is closely associated with the simpliﬁed
architecture of the immune system (Rajapakse et al.,
2020). Figure 1 displays a comparison between the im-
mune system workﬂow and the RAILS workﬂow. Both
systems are composed of a ﬁve-step process. For ex-
ample, RAILS emulates clonal expansion from the im-
mune system by enlarging the population of candidates
(B-cells). Similar to the plasma B-cells and memory
B-cells generated in the immune system, RAILS gener-
ates plasma data for predictions of the present inputs
and generates memory data for the defense against fu-

ture attacks.

To demonstrate that the proposed RAILS computa-
tional system captures important properties of the im-
mune system, we compare the learning curves of the
two systems in Figure 2. The green and red lines de-
pict the aﬃnity change between the population and the
antigen (test data). The data selected in the ﬂocking
step comes from antigen 1 (test data 1) in all tests,
which results in a low-aﬃnity increase for antigen 2
(test data 2). One can see that both the immune
system’s learning curve of antigen 1 and the RAILS
learning curve of antigen 1 have a small aﬃnity de-
crease at the beginning and then monotonically in-
creases (green curves). This phenomenon indicates a
two-phase learning process. The diversity phase comes
from the clonal expansion that generates diverse data
points (virtual B-Cells), which results in decreasing
aﬃnity. The selection phase arises from selection of
the high-aﬃnity data points (virtual B-Cells) gener-
ated during the diversity phase, which results in the
increasing aﬃnity.

Figure 2: Correspondence of Immune System in vitro
Analog (left) and RAILS (right) Experiments

3 RAILS: Overview

In this section, we ﬁrst introduce the Adaptive Im-
mune System Emulation (AISE), which is the key com-

RAILS: A Robust Adversarial Immune-inspired Learning System

ponent of RAILS, containing two learning stages. We
then provide details of the RAILS workﬂow. We illus-
trate RAILS and AISE in the context of hardening a
DkNN architecture against evasion attacks. For any
input and for each hidden layer, the DkNN ﬁnds the
nearest neighbors of the activation responses to the in-
put relative to the activation responses to the training
samples. Diﬀerent from the original DkNN, in RAILS
the k-nearest neighbor search is restricted to the train-
ing samples in a given class. AISE emulates the plasma
B-cell and memory B-cell defense mechanisms to ro-
bustify the architecture. The architecture of RAILS is
illustrated in Figure 3.

Figure 3: The Architecture of RAILS

3.1 Adaptive Immune System Emulation

(AISE)

The Adaptive Immune System Emulation (AISE) is
designed and implemented with a bionic process in-
spired by the mammalian immune system. Concretely,
AISE generates virtual plasma data (plasma B-cells)
and memory data (memory B-cells) through multiple
generations of evolutionary programming. The plasma
data and memory data are selected in diﬀerent ways,
thus contributing to diﬀerent model robustifying lev-
els. The plasma data contributes to the robust predic-
tions of the present inputs, and the memory data helps
to adjust the classiﬁers to eﬀectively defend against
future attacks. From the perspective of classiﬁer ad-
justment, AISE’s learning stages can be divided into
Static Learning and Adaptive Learning.

Defense with Static Learning. Static learning
helps to correct the predictions of the present in-
puts. Recall that the DkNN integrates the predicted
k-nearest neighbors of layers in the deep neural net-
work, and the ﬁnal prediction yDkNN can be obtained
by the following formula:

yDkNN = arg maxc
subject to

c ∈ [C]

(cid:80)L

l=1 pc

l (x)

(1)

where l is the l-th layer of a DNN with L layers in
total. pc
l (x) is the probability of class c predicted by
the kNN in layer l for input x. There is a ﬁnite set
of classes and the total number is C. [C] denotes the
set {1, 2, · · · , C}. Note that pc
l (x) could be small for
poisoned data, containing an adversarial training ex-
ample, even if ytrue is the true class c. The purpose of
static learning is to increase pytrue
(x) of the present in-
put x. The key idea is to generate new training exam-
ples via clonal expansion and optimization, and only
select the examples with high aﬃnity (plasma data) to
the input. Our hypothesis is that examples inherited
from parents of class ytrue have higher chance of reach-
ing high aﬃnity and, therefore, survival. After opti-
mization, a majority vote is used to make the class
prediction. We refer readers to Section 3.2 for more
implementation details and Section 4.1 for visualiza-
tion.

l

Defense with Adaptive Learning. Diﬀerent from
static learning, adaptive learning tries to harden the
classiﬁer to defend against potential future attacks.
The hardening is done by leveraging another set of
data - memory data generated during aﬃnity matura-
tion. Unlike plasma data, memory data is selected
from examples with moderate-aﬃnity to the input,
which can rapidly adapt to new variants of the cur-
rent adversarial examples. Adaptive learning is a life-
long learning process and will provide a naturally high
pytrue
(x) even if using the DkNN alone. This paper will
l
mainly focus on static learning and single-stage adap-
tive learning that implements a single cycle of classiﬁer
hardening.

3.2 RAILS Details

3.2.1 Five-Step Workﬂow

Given a mapping F : Rd → Rd(cid:48) and two vectors
x1, x2 ∈ Rd, we ﬁrst deﬁne the aﬃnity score between
x1 and x2 as A(F ; x1, x2) = −(cid:107)F (x1)−F (x2)(cid:107)2, where
A is a aﬃnity function equal to the negative Euclidean
distance. In the DNN context, F denotes the feature
mapping from input to feature representation, and A
measures the similarity between two inputs. Higher
aﬃnity score indicates higher similarity. Algorithm 1
shows the ﬁve-step workﬂow of RAILS: Sensing, Flock-
ing, Expansion, Optimization, and Consensus. We ex-
plain each step below.

Sensing. This step performs the initial discrimina-
tion between adversarial and clean inputs. This is im-
plemented using an outlier detection procedure, for
which there are several diﬀerent methods available
(Feinman et al., 2017; Xu et al., 2017). The DkNN
provides a credibility metric that can measure the la-

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

bel consistency of k-nearest neighbors in each layer.
The higher the credibility, the higher the conﬁdence
that the input is clean. The sensing stage provides a
conﬁdence score of the DkNN architecture and does
not aﬀect RAILS’s predictions.

Flocking. This step provides a starting point for
clonal expansion. For each class and each selected
layer, we ﬁnd the k-nearest neighbors that have the
highest initial aﬃnity score to the input data. Math-
ematically, we select

where Rg : [|P (G)|] → [|P (G)|] is the same ranking
function as Rc except that the domain is the set of
cardinality of the ﬁnal population P (G). γ is a per-
centage parameter and is selected as 0.05 and 0.25 for
plasma data and memory data, respectively. Note that
the memory data can be selected in each generation
and in a nonlinear way. For simplicity, we select mem-
ory data only in last generation. Memory data will be
saved in the secondary database and used for model
hardening. Details are shown in Section 3.2.2.

Consensus. Note that all examples are associated
with a label. In this step, plasma data use majority
voting for prediction of x.

(2)

l = {(ˆx, yc)|Rc(ˆx) ≤ k, (ˆx, yc) ∈ Dc}

N c
Given
i , x) ≤ A(fl; xc
A(fl; xc
∀c ∈ [C], l ∈ L, ∀i, j ∈ [nc],

j, x) ⇐ Rc(i) > Rc(j)

where x is the input. L is the set of the selected layers.
Dc is the training dataset from class c and the size
|Dc| = nc. Rc : [nc] → [nc] is a ranking function that
sorts the indices based on the aﬃnity score.
If the
memory database has been populated, we will ﬁnd the
nearest neighbors using both the training data and the
memory data generated at the previous iteration.

Expansion. This step generates new examples (oﬀ-
spring) from the existing examples (parents). The an-
cestors are nearest neighbors found by Flocking. The
process can be viewed as creating new nodes (oﬀ-
spring) linked to the existing nodes (parents), and can
be analogous to Preferential Attachment (Barab´asi
and Albert, 1999). The probability of a new node link-
ing to node i is

Π(ki) = ki
(cid:80)

j kj

,

(3)

where ki is the degree of node i.
In preferential at-
tachment models new nodes prefer to attach to ex-
isting nodes with high vertex degree. In RAILS, we
use a surrogate for the degree, which is the exponenti-
ated aﬃnity measure, and the oﬀspring are generated
by parents having high degree The diversity generated
during Expansion is provided by the operations selec-
tion, mutation, and cross-over. After new examples
are generated, RAILS calculates each new example’s
aﬃnity score relative to the input. The new exam-
ples are associated with labels that are inherited from
their parents. We refer readers to Section 3.2.2 for
additional implementation details.

Optimization (Aﬃnity Maturation).
In this
step, RAILS selects generated examples with high-
aﬃnity scores to be plasma data, and examples with
moderate-aﬃnity scores are saved as memory data.
The selection is based on a ranking function.

Sopt = {(˜x, ˜y)|Rg(˜x) ≤ γ|P (G)|, (˜x, ˜y) ∈ P (G)}

(4)

Algorithm 1 Robust Adversarial Immune-inspired
Learning System (RAILS)
Input: Test data point x; Training dataset Dtr =
{D1, D2, · · · , DC}; Number of Classes C; Model
M with feature mapping fl(·) in layer l, l ∈ L;
Aﬃnity function A.
First Step: Sensing

1 Check the conﬁdence score given by the DkNN to

detect the threat of x.
Second Step: Flocking

2 for c = 1, 2, . . . , C do
3

In each layer l ∈ L, ﬁnd the k-nearest neigh-
bors N c
l of x in Dc by ranking the aﬃnity score
A(fl; xj, x), xj ∈ Dc

4 end for

Third and Fourth Steps: Expansion and Op-
timization

5 Return plasma data Sp and memory data Sm by

using subroutine: Algorithm 2
Fifth Step: Consensus

6 Obtain the prediction y of x using the majority

vote of the plasma data

7 Return: y, the memory data

3.2.2 Clonal Expansion and Aﬃnity

Maturation

Clonal expansion and aﬃnity maturation (optimiza-
tion) are the two main steps that occur after ﬂock-
ing. Details are shown in Algorithm 2. The goal is to
promote diversity and explore the best solutions in a
broader search space. Finally, plasma data and mem-
ory data are generated. Three operations support the
creation of new examples: selection, cross-over, and
mutation. We introduce them in detail below.

Selection. The selection operation aims to decide
which candidates in the generation will be chosen to
generate the oﬀspring. We will calculate the probabil-

RAILS: A Robust Adversarial Immune-inspired Learning System

ity for each candidate through a softmax function.

P (xi) = Sof tmax(A(fl; xi, x)/τ )

=

(cid:80)

exp (A(fl;xi,x)/τ )
xj ∈S exp (A(fl;xj ,x)/τ )

(5)

where S is the set containing data points and xi ∈ S.
τ > 0 is the sampling temperature that controls the
distance after softmax operation. Given the probabil-
ity P of a candidates set S that is calculated through
(5), the selection operation is to randomly pick one ex-
ample pair (xi, yi) from S according to its probability.

(xi, yi) = Selection(S, P )

(6)

In RAILS, we select two parents for each oﬀspring,
and the second parent is selected from the same class
as that of the ﬁrst parent. The parent selection process
appears in lines 6 - 11 of Algorithm 2.

Cross-over. The crossover operator combines diﬀer-
ent candidates (parents) for generating new examples
(oﬀspring). Given two parents xp and x(cid:48)
p, the new
oﬀspring are generated by selecting each entry (e.g.,
pixel) from either xp or x(cid:48)
p via calculating the corre-
sponding probability. Mathematically,

os = Crossover(xp, x(cid:48)
x(cid:48)



with prob

p) =

x(i)
p
x(cid:48)(i)
p



with prob

A(fl;xp,x)

A(fl;xp,x)+A(fl;x(cid:48)
A(fl;x(cid:48)
A(fl;xp,x)+A(fl;x(cid:48)

p,x)

p,x)

p,x)

∀i ∈ [d]

(7)

where i represents the i-th entry of the example and d
is the dimension of the example. The cross-over oper-
ator appears in line 12 of Algorithm 2.

Mutation. This operation mutates each entry with
probability ρ by adding uniformly distributed noise in
the range [−δmax, −δmin] ∪ [δmin, δmax]. The resulting
perturbation vector is subsequently clipped to satisfy
the domain constraints.
xos = M utation(x(cid:48)
1[Bernoulli(ρ)]u([−δmax, −δmin] ∪ [δmin, δmax])(cid:1)

os) = Clip[0,1]

os+

(cid:0)x(cid:48)

(8)

where 1[Bernoulli(ρ)]
takes value 1 with proba-
bility ρ and value 0 with probability 1 − ρ.
u([−δmax, −δmin] ∪ [δmin, δmax]) is the vector that each
entry is i.i.d. chosen from the uniform distribution
U([−δmax, −δmin] ∪ [δmin, δmax]). Clip[0,1](x) is equiv-
alent to max(0, min(x, 1)). The mutation operation
appears in line 3 and line 16 in Algorithm 2.

4 EXPERIMENTAL RESULTS

We conducted experiments in the context of image
classiﬁcation. We compare RAILS to standard Con-
volutional Neural Network Classiﬁcation (CNN) and

10
11
12
13
14
15
16

Algorithm 2 clonal Expansion & Aﬃnity Maturation
in each layer
Input: x; K-nearest neighbors
N c
l , c ∈ [C], l ∈ L; Number of population
T ; Maximum generation number G; Mutation
parameters
range
probability
δmin, δmax; Sampling temperature τ

ρ; Mutation

P (0) ←− M utation(x(cid:48)) for P/CK times

l , N 2

l , · · · , N C

l }) do

1 For each layer l ∈ L, do
2 for each x(cid:48) ∈ Union({N 1
3
4 end for
5 for g = 1, 2, . . . , G do
6
7
8
9

Pg−1 = Softmax (A(fl; P (g−1), x)/τ )
for t = 1, 2, . . . , T do

(xp, yp) = Selection(Pg−1, P (g−1))
Pick all the data Syp belonging to class yp
in P (g−1)/{xp} and calculate the probability
Pyp =
if Syp (cid:54)= ∅ then

Pg−1(Syp )
(cid:80) Pg−1(Syp )

os = xp

(x(cid:48)
p, yp) = Selection(Pyp , Syp )
os = Crossover(xp, x(cid:48)
x(cid:48)
p)
else
x(cid:48)
end if
xos = M utation(x(cid:48)
P (g)
end for

p ←− xos

os)

17
18
19 end for
20 Calculate the aﬃnity score A(fl; P (G), x), ∀l ∈ L
21 Select the top 5% as plasma data Sl
p and the top
m based on the aﬃnity

25% as memory data Sl
scores, ∀l ∈ L

22 end For
23 Return:
m, S2
{S1

Sp = {S1
m, · · · , S|L|
m }

p, S2

p, · · · , S|L|

p } and Sm =

Deep k-Nearest Neighbors Classiﬁcation (DkNN) (Pa-
pernot and McDaniel, 2018) on the MNIST (Lecun
et al., 1998), SVHN (Netzer et al., 2011), and CIFAR-
10 (Krizhevsky and Hinton, 2009). We test our frame-
work using a four-convolutional-layer neural network
for MNIST, and VGG16 (Simonyan and Zisserman,
2014) for SVHN and CIFAR-10. We refer readers to
Section 2 in our Supplementary for more details of
datasets, models, and parameter selection. In addition
to the clean test examples, we also generate the same
amount of adversarial examples using a 20(10)-step
PGD attack (Madry et al., 2017) for MNIST (SVHN
and CIFAR-10). The attack strength is (cid:15) = 40/60 for
MNIST, and (cid:15) = 8 for SVHN and CIFAR-10 by de-
fault. The performance will be measured by standard
accuracy (SA) evaluated using benign (unperturbed)
test examples and robust accuracy (RA) evaluated us-
ing the adversarial (perturbed) test examples.

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

4.1 Performance in Single Layer

RAILS Improves Single Layer DkNN. We ﬁrst
test RAILS in a single layer of the CNN model and
compare the obtained accuracy with the results from
the DkNN. Table 1 shows the comparisons in the input
layer, the ﬁrst convolutional layer (Conv1), and the
second convolutional layer (Conv2) on MNIST. One
can see that for both standard accuracy and robust
accuracy, RAILS can improve DkNN in the hidden lay-
ers and achieve better results in the input layer. The
input layer results indicate that RAILS can also out-
perform supervised learning methods like kNN. The
confusion matrices in Figure 4 show that RAILS has
fewer incorrect predictions for those data that DkNN
gets wrong. Each value in Figure 4 represents the per-
centage of intersections of RAILS (correct or wrong)
and DkNN (correct or wrong).

Table 1: SA/RA Performance of RAILS versus DkNN
in Single Layer (MNIST)

SA

RA
((cid:15) = 40)
RA
((cid:15) = 60)

RAILS
DkNN
RAILS
DkNN
RAILS
DkNN

Input

Conv2
Conv1
97.53% 97.77% 97.78%
96.88%
97.42%
97.4%
93.78% 92.56% 89.29%
88.26%
90.84%
91.81%
88.83% 84.18% 73.42%
69.18%
81.01%
85.54%

Conv1

Conv2

)
0
6
=
(cid:15)
(

s
e
l
p
m
a
x
e

v
d
A

Figure 4: Confusion Matrices of Adversarial Examples
Classiﬁcation in Conv1 and Conv2 (RAILS vs. DkNN)

s
t
u
p
n
I

n
o
i
t
r
o
p
o
r
P

g
v
a

y
t
i
n
ﬃ
A

)
a
t
a
d

s
s
a
l
c

e
u
r
t
(

)
a
t
a
d

s
s
a
l
c

e
u
r
t
(

RAILS:4 VS DkNN:4
RAILS conﬁdenc: 1.0
DkNN conﬁdence: 0.6

RAILS:2 VS DkNN:3
RAILS conﬁdenc: 1.0
DkNN conﬁdence: 0.6

Figure 5: Proportion and Average Aﬃnity of True
Class Population With Respect to Generation Num-
ber (RAILS on MNIST)

examples from MNIST here. DkNN only makes a cor-
rect prediction in the ﬁrst example and gives a low
conﬁdence score for both examples. The ﬁrst row de-
picts the proportion of the true class in each genera-
tion’s population. Data from the true class occupies
the majority of the population when the generation
number increases, which indicates that RAILS can si-
multaneously obtain a correct prediction and a high
conﬁdence score. Meanwhile, clonal expansion over
multiple generations produces increased aﬃnity within
the true class, as shown in the second row. Another
observation is that RAILS requires fewer generations
when DkNN makes a correct prediction, suggesting
that aﬃnity maturation occurs in fewer generations
when the input is relatively easy to classify.

4.2 Visualization of RAILS Results

Clonal expansion within RAILS creates new examples
in each generation. To better understand the capabil-
ity of RAILS, we can visualize the changes of some key
indices during runtime. After the optimization step,
the plasma data and memory data can be compared
to the nearest neighbors found by the DkNN.

Visualization of RAILS Running Process. Fig-
ure 5 shows how the population and aﬃnity score
of the true class examples in each generation change
when the generation number increases. We show two

Visualization of RAILS Generated Examples.
Figure 6 shows the plasma data and memory data
generated by RAILS. For the ﬁrst example - digit 9,
DkNN gets 9 in four out of ﬁve nearest neighbors.
For the other two examples - digit 2 and digit 1, the
nearest neighbors only contain a small amount of data
from the true class. In contrast, the plasma data that
RAILS generated are all from the true class, which pro-
vides correct prediction with conﬁdence value 1. The
memory data captures the information of the adver-
sarial variants and is associated with the true label.
They can be used to defend future adversarial inputs.

05101520Generation number00.20.40.60.81Proportion(true class population)05101520Generation number-32-31-30-29-28-27Affinity average(true class population)01020304050Generation number00.20.40.60.81Proportion(true class population)01020304050Generation number-40-35-30-25Affinity average(true class population)RAILS: A Robust Adversarial Immune-inspired Learning System

Figure 6: The Generated Plasma Examples and Memory Examples in a Single Layer

4.3 Overall Performances on Diﬀerent

Scenarios

We compare RAILS to CNN and DkNN in terms of
SA and RA. DkNN uses 750 samples from the training
data as calibration data. RAILS leverages static learn-
ing to make the predictions. The results are shown
in Table 2. On MNIST with (cid:15) = 60, one can see
that RAILS delivers a 5.62% improvement in RA over
DkNN without appreciable loss of SA. On CIFAR-
10 (SVHN), RAILS leads to 4.74% (12.5%) and 20%
(46%) robust accuracy improvements compared to
DkNN and CNN, respectively. We also conduct ex-
periments with Square Attack (Andriushchenko et al.,
2020), which is one of the black-box attacks. The re-
sults in Table 3 show that RAILS improves the robust
accuracy of DkNN by 4.7% on CIFAR-10 with (cid:15) = 24.
We refer readers to Section 3 in Supplementary for
more results.

Table 2: SA/RA Performance of RAILS versus CNN
and DkNN

MNIST
((cid:15) = 60)

SVHN
((cid:15) = 8)

RAILS (ours)
CNN
DkNN
RAILS (ours)
CNN
DkNN

CIFAR-10 RAILS (ours)
((cid:15) = 8)

CNN
DkNN

SA

RA

97.95% 76.67%
99.16% 1.01%
97.99%
71.05%
90.62% 48.26%
94.55% 1.66%
35.7%
93.18%
43%
74%

74.98% 23.28%
38.26%

74%

Table 3: SA/RA Performance of RAILS on CIFAR-10
under Square Attack (Andriushchenko et al., 2020)

RAILS (ours)
DkNN

SA RA ((cid:15) = 24) RA ((cid:15) = 32)
74%
74%

67.2%
62.5%

57.2%
51.2%

4.4 Single-Stage Adaptive Learning

The previous experiments demonstrate that static
learning is eﬀective in predicting present adversarial

inputs. Here we show that using single-stage adaptive
learning (SSAL), i.e., one-time hardening in the test
phase, can improve the robustness of depth classiﬁer.
We generate 3000 memory data from a group of test
data by feeding them into the RAILS framework. We
then test whether the memory data can help DkNN
defend the future evasion attack. The memory data
works together with the original training data. We
then randomly select another group of test data and
generate 1000 adversarial examples for evaluation. Ta-
ble 4 shows that the SSAL improves RA of DkNN by
2.3% with no SA loss using 3000 memory data (5% of
training data).

Table 4: SA/RA Performance of DkNN (on 1000
Adversarial Examples) Before and After Hardening
the Classiﬁer Through RAILS Single-Stage Adaptive
Learning (SSAL)

DkNN
DkNN-SSAL

SA
98.5%
98.5%

RA ((cid:15) = 60)
68.3%
70.6%

5 CONCLUSION

Inspired by the immune system, we proposed a new
defense framework for deep models. The proposed
Robust Adversarial Immune-inspired Learning System
(RAILS) has a one-to-one mapping to a simpliﬁed ar-
chitecture immune system and its learning behavior
aligns with in vitro biological experiments. RAILS in-
corporates static learning and adaptive learning, con-
tributing to a robustiﬁcation of predictions and dy-
namic model hardening, respectively. The experimen-
tal results demonstrate the eﬀectiveness of RAILS. We
believe this work is fundamental and delivers valuable
principles for designing robust deep models. In future
work, we will dig deeper into the mechanisms of the
immune system’s adaptive learning (life-long learning)
and covariate shift adjustment, which will be consoli-
dated into our computational framework.

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

References

Andriushchenko, M., Croce, F., Flammarion, N., and
Hein, M. (2020). Square attack: a query-eﬃcient
black-box adversarial attack via random search. In
European Conference on Computer Vision, pages
484–501. Springer.

Athalye, A., Carlini, N., and Wagner, D. (2018). Ob-
fuscated gradients give a false sense of security: Cir-
cumventing defenses to adversarial examples. In In-
ternational Conference on Machine Learning, pages
274–283.

Barab´asi, A.-L. and Albert, R. (1999). Emergence of
scaling in random networks. science, 286(5439):509–
512.

Carlini, N. and Wagner, D. (2017). Adversarial exam-
ples are not easily detected: Bypassing ten detection
methods. In Proceedings of the 10th ACM Workshop
on Artiﬁcial Intelligence and Security, pages 3–14.
ACM.

Cohen, J., Rosenfeld, E., and Kolter, Z. (2019). Certi-
ﬁed adversarial robustness via randomized smooth-
ing. In International Conference on Machine Learn-
ing, pages 1310–1320.

Cucker, F. and Smale, S. (2007). Emergent behavior
in ﬂocks. IEEE Transactions on automatic control,
52(5):852–862.

De Silva, N. S. and Klein, U. (2015). Dynamics of b
cells in germinal centres. Nature reviews immunol-
ogy, 15(3):137–148.

Farmer, J. D., Packard, N. H., and Perelson, A. S.
(1986). The immune system, adaptation, and ma-
chine learning. Physica D: Nonlinear Phenomena,
22(1-3):187–204.

Feinman, R., Curtin, R. R., Shintre, S., and Gardner,
A. B. (2017). Detecting adversarial samples from
artifacts. arXiv preprint arXiv:1703.00410.

Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014).
Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572.

Grosse, K., Manoharan, P., Papernot, N., Backes,
M., and McDaniel, P. (2017). On the (statistical)
detection of adversarial examples. arXiv preprint
arXiv:1702.06280.

Gu, T., Dolan-Gavitt, B., and Garg, S. (2017).
Badnets:
Identifying vulnerabilities in the ma-
chine learning model supply chain. arXiv preprint
arXiv:1708.06733.

Krizhevsky, A. and Hinton, G. (2009). Learning mul-
tiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University
of Toronto.

LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep

learning. nature, 521(7553):436–444.

Lecun, Y., Bottou, L., Bengio, Y., and Haﬀner,
P. (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.

Liu, H., Long, M., Wang, J., and Jordan, M. (2019).
Transferable adversarial training: A general ap-
proach to adapting deep classiﬁers. In International
Conference on Machine Learning, pages 4013–4022.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D.,
and Vladu, A. (2017). Towards deep learning mod-
els resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083.

Mehdipour Ghazi, M. and Kemal Ekenel, H. (2016). A
comprehensive analysis of deep learning based rep-
In Proceedings of
resentation for face recognition.
the IEEE conference on computer vision and pat-
tern recognition workshops, pages 34–41.

Mesin, L., Ersching, J., and Victora, G. D. (2016).
Immunity,

Germinal center b cell dynamics.
45(3):471–482.

Mesin, L., Schiepers, A., Ersching, J., Barbulescu, A.,
Cavazzoni, C. B., Angelini, A., Okada, T., Kurosaki,
T., and Victora, G. D. (2020). Restricted clonal-
ity and limited germinal center reentry character-
ize memory b cell reactivation by boosting. Cell,
180(1):92–106.

Metzen, J. H., Genewein, T., Fischer, V., and Bischoﬀ,
B. (2017). On detecting adversarial perturbations.
In International Conference on Learning Represen-
tations.

Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu,
B., and Ng, A. Y. (2011). Reading digits in natural
images with unsupervised feature learning.

Papernot, N. and McDaniel, P.

Deep
k-nearest neighbors: Towards conﬁdent,
inter-
pretable and robust deep learning. arXiv preprint
arXiv:1803.04765.

(2018).

Rajapakse, I. and Groudine, M. (2011). On emerging
nuclear order. Journal of Cell Biology, 192(5):711–
721.

Rajapakse, I., Lindsly, S., Brockett, R., and Hartwell,
L. (2020). A mathematical theory of learning guided
by the immune system. Technical report, University
of Michigan.

Rajapakse, I. and Smale, S. (2017). Emergence of func-
tion from coordinated cells in a tissue. Proceedings
of the National Academy of Sciences, 114(7):1462–
1467.

Samangouei, P., Kabkab, M., and Chellappa, R.
(2018). Defense-gan: Protecting classiﬁers against

RAILS: A Robust Adversarial Immune-inspired Learning System

In
adversarial attacks using generative models.
International Conference on Learning Representa-
tions.

Shafahi, A., Najibi, M., Ghiasi, M. A., Xu, Z., Dick-
erson, J., Studer, C., Davis, L. S., Taylor, G., and
Goldstein, T. (2019). Adversarial training for free!
In Advances in Neural Information Processing Sys-
tems, pages 3353–3364.

Shafahi, A., Saadatpanah, P., Zhu, C., Ghiasi, A.,
Studer, C., Jacobs, D., and Goldstein, T. (2020).
Adversarially robust transfer learning. In Interna-
tional Conference on Learning Representations.

Simonyan, K. and Zisserman, A. (2014). Very deep
convolutional networks for large-scale image recog-
nition. arXiv preprint arXiv:1409.1556.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J.,
Erhan, D., Goodfellow, I., and Fergus, R. (2013).
Intriguing properties of neural networks.
arXiv
preprint arXiv:1312.6199.

Wong, E., Rice, L., and Kolter, J. Z. (2020). Fast
is better than free: Revisiting adversarial training.
In International Conference on Learning Represen-
tations.

Xu, W., Evans, D., and Qi, Y. (2017). Feature squeez-
ing: Detecting adversarial examples in deep neural
networks. arXiv preprint arXiv:1704.01155.

Young, T., Hazarika, D., Poria, S., and Cambria, E.
(2018). Recent trends in deep learning based natu-
ral language processing. ieee Computational intelli-
genCe magazine, 13(3):55–75.

Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E.,
and Jordan, M. I. (2019). Theoretically principled
trade-oﬀ between robustness and accuracy. Interna-
tional Conference on Machine Learning.

Zhao, Z.-Q., Zheng, P., Xu, S.-t., and Wu, X. (2019).
Object detection with deep learning: A review.
IEEE transactions on neural networks and learning
systems, 30(11):3212–3232.

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

Supplementary Material

1 ADDITIONAL NOTES ON RAILS

Computational complexity. The RAILS’s computational complexity is dominated by clonal expansion and
aﬃnity maturation. Given the length of the selected layers N , the population size T , and the maximum generation
number G, the computational complexity is O(T N G).

In Section 4.2 of the main text, we empirically show that RAILS requires fewer
Early stopping criterion.
generations when DkNN makes a correct prediction. Therefore, O(T N G) is the worst-case complexity. Consid-
ering the fast convergence of RAILS, one practical early stopping criterion is to check if a single class occupies
most of the population, e.g., 100%.

2 SETTINGS OF EXPERIMENTS

2.1 Datasets and Models

We test RAILS on three public datasets: MNIST (Lecun et al., 1998), SVHN (Netzer et al., 2011), and CIFAR-
10 (Krizhevsky and Hinton, 2009). The MNIST dataset is a 10-class handwritten digit database consisting of
60000 training examples and 10000 test examples. The SVHN dataset is another benchmark that is obtained
from house numbers in Google Street View images. It contains 10 classes of digits with 73257 digits for training
and 26032 digits for testing. CIFAR-10 is a more complicated dataset that consists of 60000 colour images in
10 classes. There are 50000 training images and 10000 test images. We use a four-convolutional-layer neural
network for MNIST, and VGG16 (Simonyan and Zisserman, 2014) for SVHN and CIFAR-10. For MNIST and
SVHN, we conduct the clonal expansion in the inputs. To provide better feature representations for CIFAR-10,
we use an adversarially trained model on (cid:15) = 2 and conduct the clonal expansion in a shallow layer.

2.2 Threat Models

Though out this paper, we consider three diﬀerent types of attacks: (1) Projected Gradient Descent (PGD)
attack (Madry et al., 2017) - We implement 20-step PGD attack for MNIST, and 10-step PGD attack for SVHN
and CIFAR-10. The attack strength is (cid:15) = 40/60/76.5 for MNIST, and (cid:15) = 4/8 for SVHN and CIFAR-10. (2)
Fast Gradient Sign Method (Goodfellow et al., 2014) - The attack strength is (cid:15) = 76.5 for MNIST, and (cid:15) = 4/8
for SVHN and CIFAR-10. (3) Square Attack (Andriushchenko et al., 2020) - We implement 50-step attack for
MNIST, and 30-step attack for CIFAR-10. The attack strength is (cid:15) = 76.5 for MNIST, and (cid:15) = 24/32 for
CIFAR-10.

2.3 Parameter Selection

By default, we set the size of the population T = 1000, the maximum number of generations G = 50, and the
mutation probability ρ = 0.15. To speed up the algorithm, we stop when the newly generated examples are all
from the same class. For MNIST, we set the mutation range parameters δmin = 0.05(12.75), δmax = 0.15(38.25).
The sampling temperature τ in each layer is set to 3, 18, 18, and 72. The principle of selecting τ is to make
sure that the high aﬃnity samples do not dominate the whole dataset, i.e. the algorithm will not stop after
the ﬁrst generation.
In general, we assign larger values to hidden layers with smaller length. We ﬁnd that
our method works well in a wide range of τ . Similarly, we set τ = 300 for CIFAR-10 and SVHN. Considering
CIFAR-10 and SVHN are more sensitive to small perturbations, we set the mutation range parameters δmin =
0.005(1.275), δmax = 0.015(3.825).

3 ADDITIONAL EXPERIMENTS

3.1 Additional Comparisons on MNIST

Figure S1 provides the confusion matrices for clean examples classiﬁcations and adversarial examples classiﬁ-
cations in Conv1 and Conv2 when (cid:15) = 60. The confusion matrices in Figure S1 show that RAILS has fewer

RAILS: A Robust Adversarial Immune-inspired Learning System

incorrect predictions for those data that DkNN gets wrong. Each value in Figure S1 represents the percentage
of intersections of RAILS (correct or wrong) and DkNN (correct or wrong).

Conv1

Conv2

)
0
6
=
(cid:15)
(

s
e
l
p
m
a
x
e

v
d
A

s
e
l
p
m
a
x
e

n
a
e
l
C

Figure S1: Confusion Matrices in Convolutional Layer 1 and 2 (RAILS vs. DkNN - (cid:15) = 60)

Figure S2 shows the confusion matrices of the overall performance when (cid:15) = 60. The confusion matrices indicate
that RAILS’ correct predictions agree with a majority of DkNN’s correct predictions and disagree with DkNN’s
wrong predictions.

Figure S2: Confusion Matrices (RAILS vs. DkNN - (cid:15) = 60)

We also show the SA/RA performance of RAILS under PGD attack and FGSM when (cid:15) = 76.5. The results in
Table S1 indicate that RAILS can reach higher RA than DkNN with close SA.

Table S1: SA/RA Performance of RAILS on MNIST under PGD Attack and FGSM ((cid:15) = 76.5)

RAILS (ours)
DkNN

SA
97.95%
97.99%

RA (PGD) RA (FGSM)

58.62%
47.05%

61.67%
52.23%

In Table S2, we provide the experimental results with Square Attack (Andriushchenko et al., 2020) (a black-box
attack) showing that RAILS improves the robust accuracy of DkNN by 1.35% (11% attack success rate) on
MNIST with (cid:15) = 76.5.

3.2 Additional Comparisons on CIFAR-10

In this subsection, we test RAILS on CIFAR-10 under PGD attack and FGSM with attack strength (cid:15) = 4/8.
The results are shown in Figure S3 and Figure S4. RAILS outperforms DkNN and CNN on diﬀerent attack types
and strengths. We also ﬁnd that the diﬀerence of RA between RAILS and DkNN increases when (cid:15) increases,
indicating that RAILS can defend stronger attacks.

Ren Wang, Tianqi Chen, Stephen Lindsly, Alnawaz Rehemtulla, Alfred Hero, Indika Rajapakse

Table S2: SA/RA Performance of RAILS on MNIST under Square Attack with (cid:15) = 76.5

SA

RA

RAILS (ours)
DkNN

97.95% 89.35%
97.99%

88%

Table S3: SA/RA Performance of RAILS on CIFAR-10 under PGD Attack

RAILS (ours)
CNN
DkNN

SA
74%
74.98%
74%

RA ((cid:15) = 4) RA ((cid:15) = 8)

58%
47.15%
54%

43%
23.28%
38.26%

Table S4: SA/RA Performance of RAILS on CIFAR-10 under FGSM

RAILS (ours)
CNN
DkNN

SA
74%
74.98%
74%

RA ((cid:15) = 4) RA ((cid:15) = 8)

60%
50.55%
57.5%

46.2%
31.19%
42.3%

