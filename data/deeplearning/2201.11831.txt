2
2
0
2

n
a
J

7
2

]
I

N
.
s
c
[

1
v
1
3
8
1
1
.
1
0
2
2
:
v
i
X
r
a

A Deep Reinforcement Learning Approach for

Service Migration in MEC-enabled Vehicular

Networks

Amine Abouaomar∗‡, Zoubeir Mlika∗, Abderrahime Filali∗, Soumaya Cherkaoui∗, and Abdellatif Kobbane‡
∗INTERLAB, Engineering Faculty, Universit´e de Sherbrooke, Sherbrooke (QC), Canada
†ENSIAS, Mohammed V University, Rabat, Morocco

Abstract

Multi-access edge computing (MEC) is a key enabler to reduce the latency of vehicular network. Due to the

vehicles mobility, their requested services (e.g., infotainment services) should frequently be migrated across different

MEC servers to guarantee their stringent quality of service requirements. In this paper, we study the problem of service

migration in a MEC-enabled vehicular network in order to minimize the total service latency and migration cost. This

problem is formulated as a nonlinear integer program and is linearized to help obtaining the optimal solution using

off-the-shelf solvers. Then, to obtain an efﬁcient solution, it is modeled as a multi-agent Markov decision process and

solved by leveraging deep Q learning (DQL) algorithm. The proposed DQL scheme performs a proactive services

migration while ensuring their continuity under high mobility constraints. Finally, simulations results show that the

proposed DQL scheme achieves close-to-optimal performance.

Multi-access edge computing, vehicular networks, reinforcement learning, service migration.

Index Terms

I. INTRODUCTION

Intelligent transportation systems (ITS) represents a critical component of the Internet of Things (IoT) and future

smart cities [1]. ITS will potentially provide a more secure transportation environment through effective vehicle

coordination and efﬁcient resource management [2]–[5]. In addition to safety, the ITS ecosystem will provide

entertainment services such as video streaming and gaming, which can be extended to in-vehicle augmented reality

[6]–[8]. To achieve these promising features, vehicles must be able to communicate, exchange information and access

given services with low latency. Therefore, vehicles must operate in an environment that meets these requirements.

Multi-access edge computing (MEC) is envisioned as a key component for ﬁfth-generation (5G) ultra-reliable

low-latency communications (uRLLC) services, alongside software-deﬁned networking (SDN) [9] technology. On

the one hand, MEC can be leveraged as an emerging computational paradigm that provides efﬁcient computational

capabilities to vehicles deployed in close proximity to MEC servers while ensuring a low latency. On the other

 
 
 
 
 
 
hand, SDN technology enables seamless, transparent, and efﬁcient control through the separation of the data plane

and the control plane, which simpliﬁes network operation and management [10], [11]. Therefore, a MEC-enabled

vehicular network can beneﬁt from SDN to provide efﬁcient resource management and uRLLC vehicular services

[12]–[15]. Nevertheless, due to the limited resources of MECs and the high mobility of vehicles, there are many

challenges. In particular, the requested vehicular services must be located and migrated to different MEC servers

to guarantee their continuity [16]–[18].

To address these challenges, we investigate the service placement and migration problem in a MEC-enabled

vehicular network. We leverage SDN technologies to have efﬁcient control on the MEC servers operations, with

the objective of reducing the average service latency of the vehicles. We ﬁrst, formulate the problem of service

placement and migration as a nonlinear integer program that we linearize to obtaining the optimal solution using

off-the-shelf solvers. Second, we modeled the problem as a multi-agent Markov decision process (MMDP), in

order to solve it efﬁciently using deep reinforcement learning (DRL) techniques, speciﬁcally, the deep Q-networks

(DQN). The proposed DRL-based placement and migration scheme ensures service continuity under high mobility

constraints and offer a reduced total service latency as well as the additional operational costs associated with

the migration. The proposed scheme performs proactive placement of the requested services while considering the

mobility of vehicles, the required amounts of computational and communication resources, and the overall migration

costs.

To summarize, the main contributions of this paper are synthesized as follows:

• We formulate the service placement and migration problem as a non-linear program to minimize the total service

latency (including the computing latency and the communication latency) and the cross-edge operational costs.

• We propose an MMDP framework that helps solving the problem in a distributed and scalable manner.

• We leverage DRL techniques to provide efﬁcient solution to the MMDP model. Speciﬁcally, we propose a

deep Q learning (DQL)-based solution that uses double Q network and replay buffer to improve the learning

outcome.

• We evaluate the performance of the proposed DRL-based scheme and compare it to the optimal solution

obtained by the CPLEX solver and we show that the proposed solution achieves close-to-optimal performance.

The remainder of this paper is structured as follows. In Section II, we present the system model and the problem

formulation of the service placement and migration problem. In Section III, we present the proposed multi-agent

DQL-based solution. The performance of the proposed solution is evaluated in Section IV. Last but not least, the

related works are discussed in Section V. Finally, the paper is concluded in Section VI.

A. System Model

II. SYSTEM MODEL AND PROBLEM FORMULATION

We consider an SDN-enabled MEC architecture covered with a set of gNodeBs (gNBs), each is equipped with a
MEC server 𝑛 ∈ N (cid:66) {1, 2, . . . , 𝑁 } that is connected to one gNB via high speed local-area network as illustrated in
Fig. 1. There are 𝐾 mobile users (or interchangeably called vehicles) demanding services from the MEC servers and
are denoted by the set K (cid:66) {1, 2, . . . , 𝐾 }. Each vehicle 𝑘 requests some service to fulﬁll its requirements. Without

Fig. 1: Illustration of the system model

loss of generality, we assume that all vehicles request the same vehicular service1 (e.g., an infotainment-related

service). Similar to previous works [19]–[22], we consider a MEC-based device-oriented service model contrary

to the traditional cloud-based application-oriented service model. In other words, a dedicated container or virtual

machine is assigned the vehicular service as well as the applications’ environment, which are executed on each

vehicle rather than on each application. An SDN controller is assumed to be placed on the cloud layer where it

acts as a central controller for information exchange between vehicles. Time is discrete and is divided into a set
of 𝑇 time-slots denoted by the set T = {1, 2, . . . , 𝑇 }. At each time-slot 𝑡 ∈ T , each vehicle 𝑘 ∈ K requests the
vehicular service from the MEC node 𝑛 ∈ N .

The objective of this work is to guarantee the minimum quality of service (QoS) requirements of the vehicles

while considering their erratic mobility and the computing and communication resources of MEC servers. To do

so, the requested vehicular service should be placed and migrated across different MEC servers depending on the

vehicles mobility patterns. In this work, we consider an hybrid centralized-distributed architecture where (i) each

MEC server plays the role of an agent that makes its service placement and migration decisions independently of

1The case of multiple services will be considered in our future work where network slicing will be integrated into our system model.

other MEC servers, and (ii) once each MEC agent makes its decision, it communicates it to the SDN controller

that plays the role of a central agent to coordinate the decisions of all MEC servers.

The considered QoS is represented by the vehicular service latency that includes (i) the communication delay

that is incurred by the transmission between a vehicle and a MEC server, and (ii) the computing delay that depends

on the processing capability of the MEC server as well as the size of the vehicle’s request.

1) Communication Delay: When a vehicle 𝑘 requests from a MEC server 𝑛 the vehicular service, the transmission
between 𝑘 and 𝑛 depends mainly on the wireless environment and on the size of the requested service. The channel
power gain between vehicle 𝑘 and MEC server 𝑛 at time-slot 𝑡 is denoted by 𝑔𝑡

𝑘𝑛, which includes the small-scale

fading as well as the large-scale fading. To simplify the analysis, we assume that the total available bandwidth

is divided equally between the MEC servers and each MEC server allocates its bandwidth to the vehicles in an
orthogonal manner. Accordingly, the received signal to noise ratio between MEC server 𝑛 and vehicle 𝑘 at time-slot
𝑡 is given by

𝛾𝑡
𝑘𝑛 (cid:66)

𝑝𝑛𝑔𝑡
𝑘𝑛
𝜎2

,

(1)

where 𝑝𝑛 is the transmit power of MEC server 𝑛 and 𝜎2 is the power of the noise. The achieved data rate can be

given as follows

𝑘𝑛 (cid:66) 𝑤𝑛 lg (cid:0)1 + 𝛾𝑡
Γ𝑡

𝑘𝑛

(cid:1) ,

[in bits/sec]

(2)

where 𝑤𝑛 is the allocated bandwidth of MEC server 𝑛. Consequently, the communication delay between MEC
server 𝑛 and vehicle 𝑘 at time-slot 𝑡 is given by:

𝑑𝑡
𝑘𝑛 (cid:66)

𝑠𝑘
Γ𝑡
𝑘𝑛

,

[in sec]

(3)

where 𝑠𝑘 is the size of the requested service of vehicle 𝑘.

2) Computing Delay: The computing delay depends on the processing capacity of each MEC server 𝑛, on the
total vehicles sharing MEC server 𝑛, and on the requested computing capacity of the vehicular service of vehicle
𝑘 at time-slot 𝑡. More precisely, the computing delay between MEC server 𝑛 and vehicle 𝑘 at time-slot 𝑡 is given

as follows [18]:

𝑘𝑛 (cid:66) 𝑐𝑡
𝑐𝑡

𝑘 𝑁 𝑡

𝑛/𝐹𝑛,

[in sec]

(4)

where 𝑐𝑡
𝑘 denotes the amount of computing capacity [in CPU cycles] required by the requested vehicular service
of vehicle 𝑘 at time-slot 𝑡. The computing capacity of MEC server 𝑛 [in CPU cycles/sec] is given by 𝐹𝑛 and the
number of vehicles placed on MEC server 𝑛 at time-slot 𝑡 is given by 𝑁 𝑡
𝑛.

Besides the QoS requirements, placing and migrating the vehicular service across multiple MEC servers incur

additional operational costs related, for example, to the energy consumption and the bandwidth usage. For this

reason, we consider the migration cost as an important factor into the design of our service migration solution.

3) Migration Cost: Due to the cross-edge migration, additional operational costs are incurred by the service

migration. These costs include energy consumption, expensive wide-area network bandwidth usage, etc. [18]. To
make the operational cost model general, we use 𝑚 𝑘𝑡

𝑛(cid:48)𝑛 to denote the cost of migrating the vehicular service of

vehicle 𝑘 from MEC server 𝑛(cid:48) to MEC node 𝑛 at time-slot 𝑡. Obviously, we assume that 𝑚 𝑘𝑡
and for all 𝑘, 𝑡.

𝑛(cid:48)𝑛 = 0, for all 𝑛(cid:48) = 𝑛

B. Problem Formulation

To guarantee the required QoS (communication and computing delays) and the migration cost, the optimization

problem is formulated as a multi-objective optimization problem where the aim is to optimize the communication

delay, the computing delay as well as the migration costs. To simplify the resolution of this multi-objective problem,
we transform the multi-objective problem into a single objective one by introducing the weights 𝜆𝑖 for 𝑖 ∈ {1, 2, 3}.

The formulated problem is written as a nonlinear integer program (NLP) as follows.

minimize
x

subject to

𝜆1𝐶 (x) + 𝜆2𝐷 (x) + 𝜆3 𝑀 (x)

𝑥𝑡
𝑘𝑛 ∈ {0, 1}, ∀𝑘, 𝑛, 𝑡,
𝑁
∑︁

𝑥𝑡
𝑘𝑛 = 1, ∀𝑘, 𝑡,

𝑛=1

(P1a)

(P1b)

(P1c)

where the variables 𝑥𝑡
𝑛 at time-slot 𝑡. We denote by x the multidimensional notation of the variables 𝑥𝑡

𝑘𝑛 = 1 if and only if the vehicular service requested by vehicle 𝑘 is placed at MEC server
𝑘𝑛]. The objective

𝑘𝑛, i.e., x = [𝑥𝑡

function in (P1a) is a linear combination of the communication delay, the computing delay, and the migration cost.
Constraints (P1b) guarantee that the variables 𝑥𝑡
requested by vehicle 𝑘 at time-slot 𝑡 is placed at one and only one MEC server.

𝑘𝑛 are binary. Constraints (P1c) guarantee that the vehicular service

The total computing delay 𝐶 (x) is deﬁned as follows:

𝐶 (x) (cid:66)

𝑁
∑︁

𝐾
∑︁

𝑇
∑︁

𝑛=1

𝑘=1

𝑡=1

𝑘𝑛𝑁 𝑡
𝑥𝑡

𝑛𝑐𝑡

𝑘 /𝐹𝑛,

(5)

where 𝑐𝑡
𝑘 denotes the required amount of computing capacity [in CPU cycles] of the vehicular service requested by
vehicle 𝑘 at time-slot 𝑡 and 𝐹𝑛 denotes the maximum computing capacity of MEC server 𝑛 [in CPU cycles/sec].
The term 𝑁 𝑡

𝑛 denotes the number of services placed at MEC server 𝑛, i.e.,

𝑁 𝑡

𝑛 (cid:66)

𝐾
∑︁

𝑘=1

𝑥𝑡
𝑘𝑛.

The total communication delay 𝐷 (x) is deﬁned as follows:
𝑇
∑︁

𝑁
∑︁

𝐾
∑︁

𝐷 (x) (cid:66)

𝑘𝑛𝑑𝑡
𝑥𝑡

𝑘𝑛,

where 𝑑𝑡

𝑘=1
𝑘𝑛 denotes the computing delay between MEC server 𝑛 and the vehicle 𝑘 at time-slot 𝑡 (see (3)).

𝑛=1

𝑡=1

Finally, the total migration cost 𝑀 (x) is deﬁned as follows:
𝑇
∑︁

𝐾
∑︁

𝑁
∑︁

𝑁
∑︁

𝑀 (x) (cid:66)

𝑘𝑛(cid:48) 𝑥𝑡
𝑥𝑡−1

𝑘𝑛𝑚 𝑘𝑡
𝑛(cid:48)𝑛,

𝑛=1

𝑛(cid:48)=1

𝑘=1

𝑡=1

(6)

(7)

(8)

where the migration cost 𝑚 𝑘𝑡
server 𝑛 at time-slot 𝑡. It is clear that the cost is counted inside the summation only if both 𝑥𝑡−1

𝑛(cid:48)𝑛 is used to denote the cost of migrating the service 𝑘 from MEC server 𝑛(cid:48) to MEC

𝑘𝑛(cid:48) and 𝑥𝑡

𝑘𝑛 are

equal to one, i.e., 𝑥𝑡−1
𝑘𝑛 = 1, which means that the requested service of vehicle 𝑘 is placed at MEC server
𝑛(cid:48) at time-slot 𝑡 − 1 and is placed at MEC server 𝑛 at time-slot 𝑡. This costs includes bandwidth costs incurred by

𝑘𝑛(cid:48) = 𝑥𝑡

cross-edge migration (e.g., wide-area network bandwidth usage costs) as well as energy costs caused by increased

energy consumption of network devices such as routers. To make the model general, we use a general cost term
𝑚 𝑘𝑡

𝑛(cid:48)𝑛 as done in [18].

In order to make the problem more tractable, we linearize the objective function given in (P1a). The non-linearity
of (P1) comes from the functions 𝐶 (x) and 𝑀 (x) due to the multiplication of binary variables. To linearize 𝑀 (x),
we introduce a new binary variable called 𝑧𝑘𝑡
if each term of the product of the x-variables is positive. In other words, 𝑧𝑘𝑡
𝑘𝑛(cid:48) = 𝑥𝑡
that we must add the following two constraints to force the z-variable to be zero whenever 𝑥𝑡−1

𝑘𝑛. It is clear that this new z-variable is positive if and only

𝑛(cid:48)𝑛 = 1 ⇐⇒ 𝑥𝑡−1

𝑘𝑛 = 1. This means

𝑛(cid:48)𝑛 = 𝑥𝑡−1

𝑘𝑛(cid:48) 𝑥𝑡

𝑘𝑛(cid:48) or 𝑥𝑡

𝑘𝑛 is zero:

and

𝑛(cid:48)𝑛 ≤ 𝑥𝑡−1
𝑧𝑘𝑡

𝑘𝑛(cid:48) , ∀𝑘, 𝑛, 𝑛(cid:48), 𝑡 > 1,

𝑛(cid:48)𝑛 ≤ 𝑥𝑡
𝑧𝑘𝑡

𝑘𝑛, ∀𝑘, 𝑛, 𝑛(cid:48), 𝑡.

(9)

(10)

It remains to enforce the constraints that if both 𝑥𝑡−1

𝑘𝑛(cid:48) and 𝑥𝑡

𝑘𝑛 are equal to one, then the z-variable is one. This

can be written as follows:

𝑛(cid:48)𝑛 ≥ 𝑥𝑡−1
𝑧𝑘𝑡

𝑘𝑛(cid:48) + 𝑥𝑡

𝑘𝑛 − 1, ∀𝑘, 𝑛, 𝑛(cid:48), 𝑡 > 1.

Thus, the total migration cost can be rewritten as follows:

𝑀 (z) =

𝑁
∑︁

𝑁
∑︁

𝐾
∑︁

𝑇
∑︁

𝑛=1

𝑛(cid:48)=1

𝑘=1

𝑡=1

𝑛(cid:48)𝑛𝑚 𝑘𝑡
𝑧𝑘𝑡
𝑛(cid:48)𝑛,

where z denotes the multidimensional notation of the variables 𝑧𝑘𝑡

Now, to linearize 𝐶 (x), we let 𝑦𝑡

𝑘 denote the quantity (cid:205)𝑁
𝑛=1

𝑛(cid:48)𝑛, i.e., z = [𝑧𝑘𝑡
𝑥𝑡
𝑘𝑛𝑁 𝑡
𝑘 /𝐹𝑛, i.e.,

𝑛𝑐𝑡

𝑛(cid:48)𝑛].

𝑦𝑡
𝑘 (cid:66)

𝑁
∑︁

𝑛=1

𝑘𝑛𝑁 𝑡
𝑥𝑡

𝑛𝑐𝑡

𝑘 /𝐹𝑛, ∀𝑘, 𝑡.

Thus, the total computing delay 𝐶 (y) can be rewritten as follows:

𝐶 (y) =

𝐾
∑︁

𝑇
∑︁

𝑘=1

𝑡=1

𝑦𝑡
𝑘 ,

(11)

(12)

(13)

(14)

where y denotes the multidimensional notation of the variables 𝑦𝑡

𝑘 , i.e., y = [𝑦𝑡

𝑘 ]. Now, we have to enforce that the

following constraints:

𝑘𝑛 = 1 ⇒ 𝑦𝑡
𝑥𝑡

𝑘 = 𝑁 𝑡

𝑛𝑐𝑡

𝑘 /𝐹𝑛.

(15)

These are indicator constraints that can be easily implemented in the off-the-shelf solvers such as CPLEX or Gurobi.

Nonetheless, they can be easily transformed to linear constraints using the big-M method [?], [23].

III. PROPOSED SOLUTION

In this section, we propose a deep reinforcement learning (DRL) based approach to obtain an efﬁcient solution to

the service placement and migration problem deﬁned in (P1). The proposed approach places the vehicular service

requested by the vehicles in the appropriate MEC servers to ensure the continuity of services under the mobility

constraint of vehicles while reducing the communication latency, the computing latency as well as the migration

costs of the requested service.

We use deep Q-learning (DQL) [24]—one of the most popular DRL algorithm—to efﬁciently solve the service

placement problem in the MEC-enabled vehicular network. DQL combines Q-learning with deep neural network

(DNN). It takes as input the observed state of the environment and returns as output the Q-value of all possible

actions. DQL has two main phases, namely the training phase and the inference phase. In the training phase,

the agent trains a DNN, called deep Q-network (DQN), in an ofﬂine manner. In the inference phase, the agent

takes actions in an online manner based on the trained DQN. Before describing each phase of the proposed DQL

algorithm, we model, ﬁrst, the problem as a Markov decision process (MDP).

A. The MDP Formulation

We consider a multi-agent MDP where each MEC server 𝑛 acts as an independent agent, called herein after MEC
agent 𝑛. At time-slot 𝑡, each MEC agent 𝑛 can decide either to place and instantiate the vehicular service requested
by vehicle 𝑘 or not. The key elements of the multi-agent MDP are deﬁned as follows:

1) The State Space: At time-slot 𝑡, the observed state by the MEC agent 𝑛, denoted by S𝑡

𝑛, mainly depends on

the current vehicular environment. It includes the current positions of the vehicles, their velocities, their directions,
and their service requirements (including the wireless channel gains and the distances between MEC agent 𝑛 and

the vehicles). Note that there are as many states as there are time-slots, i.e., every time-slot corresponds to a state.

In addition, a transition from one state to the next happens according to the mobility model of the vehicles.

2) The Action Space: The action set of each MEC agent 𝑛 at time-slot 𝑡 is given by the set A𝑡

𝑛 (cid:66) {0, 1}𝐾 .

𝑛 ∈ A𝑡

𝑛 is given by the row vector [𝑎𝑡

Indeed, an action 𝒂𝑡
the decision to place the service 𝑘 ∈ K at MEC server 𝑛, all happening at time-slot 𝑡. Note that the variable 𝑎𝑡
𝑥𝑡
𝑘𝑛 deﬁned in (P1) means essentially the same thing but to remove any possible confusion between the optimization
variable 𝑥𝑡
𝑘𝑛 we use two different notations. Each MEC agent 𝑛 communicates its chosen
action to the SDN controller to form a global action 𝒂𝑡 (cid:66) [𝒂𝑡
𝑁 ]. Then, the SDN controller veriﬁes if
1

𝐾 𝑛], where each element 𝑎𝑡

𝑘𝑛 and the MDP action 𝑎𝑡

𝑘𝑛 corresponds to

2𝑛, . . . , 𝑎𝑡

, . . . , 𝒂𝑡

1𝑛, 𝑎𝑡

𝑘𝑛 and

, 𝒂𝑡
2

the individual actions of the MEC agents are feasible or not according to the constraints of (P1), i.e., the individual
action 𝒂𝑡

𝑛 of MEC agent 𝑛 is considered feasible if it meets the constraints of (P1).

3) The Reward Function: A MEC agent 𝑛 chooses an action 𝒂𝑡

𝑛 ∈ A𝑡

𝑛 at time-slot 𝑡 and receives a reward 𝑅𝑡
𝑛.

Since we seek to minimize the overall vehicular service latency requested by the vehicles, the objective of MEC
agent 𝑛 must be related to the sum-latency of the services it hosts. In other words, we deﬁne the reward 𝑅𝑡
𝑛 of
MEC agent 𝑛 at time-slot 𝑡 in relation with how the placement of requested service at 𝑛 affects the latency of the
system. Therefore, the SDN controller calculates the individual reward of MEC agent 𝑛 as follows:

𝑅𝑡

𝑛 (cid:66)

𝜆1𝐶𝑡

𝑛 + 𝜆2𝐷𝑡

𝑛 + 𝜆3 𝑀 𝑡
𝑛,

−1,

if 𝒂𝑡

𝑛 is feasible

if 𝒂𝑡

𝑛 is not feasible,

(16)





where 𝐶𝑡
𝑛 = (cid:205)𝑁
𝑀 𝑡

𝑛 = (cid:205)𝐾
𝑘=1
(cid:205)𝐾

𝑎𝑡
𝑘𝑛𝑁 𝑡
𝑘𝑛(cid:48) 𝑎𝑡
𝑎𝑡−1

𝑛𝑐𝑡
𝑘𝑛𝑚 𝑘𝑡

𝑘=1

agent 𝑛 at time-slot 𝑡 is not feasible, this MEC agent should be penalized with a negative reward 𝑅𝑡

𝑛 = (cid:205)𝐾
𝑘 /𝐹𝑛 is the computation delay, 𝐷𝑡
𝑘𝑛 is the communication delay, and
𝑘=1
𝑛(cid:48)𝑛 is the migration cost of MEC agent 𝑛 at time-slot 𝑡. If the action chosen by MEC
𝑛 = −1 to prompt

𝑘𝑛𝑑𝑡
𝑎𝑡

𝑛(cid:48)=1
𝑛(cid:48)≠𝑛

it to not choose this action in future steps.

B. The Training Phase of DQL

In general, DQN approximates the Q-values 𝑄(𝑠, 𝑎, 𝜃) of each state-action pair (𝑠, 𝑎) using a DNN, where 𝜃

represents the parameters of the Q-network. Since we propose a multi-agent MDP, the proposed DQL algorithm

will be a multi-agent algorithm in which each MEC agent will have its own DQN to be approximated and trained.
When there is no confusion, we omit the index 𝑛 from the DQN of MEC agent 𝑛. In addition, the training process

of the DNN uses the experience replay memory mechanism. This mechanism helps in creating a dataset to train

the DNN once in a while by storing each MEC agent experience into a replay buffer. This experience essentially

includes the current state, the next transition state, the chosen action and the received reward. Then, each MEC

agent randomly chooses a set of samples from its replay buffer to perform the learning process. The experience

replay memory mechanism not only allows the MEC agent to learn from the past experiences, but also to provide

uncorrelated data as inputs which breaks undesirable temporal correlations. However, DQN is known to overestimate

the Q-values of stat-action pairs under certain conditions, which harms the performances. To overcome this issue,

double DQN (DDQN) [25] is proposed which reduces the overestimation and makes the training process faster and

more reliable. Indeed, DDQN uses two DNNs, called the main Q-network and the target Q-network. The former
is used to compute the Q-values 𝑄(𝑠, 𝑎, 𝜃) while the latter is used to provide the target Q-values 𝑄(𝑠, 𝑎, 𝜃−) to
train the parameters 𝜃 of the main Q-network. The training phase of our proposed multi-agent DQL algorithm is
presented in Algorithm 1, where each MEC agent 𝑛 ∈ N trains its own DDQN.

The training phase of the DQL algorithm requires as input the vehicular environment which includes the vehicles,

the requested services, the MEC servers, the computing capacity of MEC servers. It returns the trained DDQN of

each MEC agent as output. The DDQNs are trained simultaneously. The training begins by generating the vehicles

parameters and the network parameters. The vehicles parameters include the position, the velocity and the requested

service of each vehicle. The network parameters include the computing capacity of each MEC server. Then, the DQL

algorithm initializes the DDQN parameters of each MEC agent. Next, it iterates the episodes. For each episode,

the environment of each MEC agent is built by updating the position of the vehicles according to the mobility

model and generating other network parameters. For each episode, the training continues for a period of time-slots
(or steps). In each step 𝑡, each MEC agent 𝑛 observes the current state of its environment and chooses an action
from its action space A𝑡
𝑛. To select an action, the MEC agent uses the 𝜖-greedy policy. With this policy, an action
is chosen randomly with probability 𝜖. Once all MEC agents select their actions, each of them communicates its
action to the SDN controller to construct the global action 𝒂𝑡 . The SDN controller uses the constructed global

Algorithm 1: The Training Phase of DQL
Input: Agents and environment

1 Output: Trained DDQNs

Initialization: Generate vehicles and network parameters;

Initialize the DDQN of each agent 𝑛;

2 for Episode 𝑒 do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

Reset and build the agents’ environment;
for Time-slot 𝑡 do

for each MEC 𝑛 do

Observe the environment ;
Choose an action 𝑎𝑡

𝑛 using 𝜖-greedy;

end for

The SDN controller obtains the global action ;

The SDN controller calculates the individual reward of each agent;
for each MEC 𝑛 do

Receive the individual reward from the SDN controller;

Observe the next state of the environment ;
Store the experience 𝐸𝑥 𝑝𝑡

𝑛 in the replay buffer M𝑛;

if batch size then

Sample a mini-batch from M𝑛;

Do a mini-batch training;

end if

if target step then

Update the target network parameters 𝜃−
𝑛;

end if

end for

end for

24 end for

action to verify its feasibility and calculate the individual reward of each MEC agent. Then, each MEC agent 𝑛
receives its individual reward 𝑅𝑡
𝑛 from the SDN controller and moves to the next state. The obtained experience,
denoted by 𝐸𝑥 𝑝𝑛, is stored by the MEC agent 𝑛 in its replay buffer M𝑛. When the replay buffer contains enough

experiences, i.e., a certain batch size is respected, each MEC agent randomly samples a mini-batch to create a

training dataset. The latter is used by the MEC agent to perform the training process. In the training process, each

Algorithm 2: The Inference Phase of DQL
Input: The trained DDQNs

1 Output: Placement of the vehicular service of each vehicle

Initialization: Load the DDQN of each agent 𝑛;

2 for Episode 𝑒 do

3

4

5

6

7

8

9

10

11

Reset and build the environment;
for Step 𝑡 do

for each MEC 𝑛 do

Observe the environment ;
Choose an action 𝑎𝑡

𝑛 that maximize the Q-value of the tained DDQN of 𝑛;

end for

The SDN controller obtains the global action;

end for

The SDN controller calculates the objective function as in (P1a);

12 end for

MEC agent seeks to minimize a loss function, given by:

𝑛 (𝜃𝑛) = E[(𝑦𝑛 − 𝑄(S𝑡
𝐿𝑡

𝑛, 𝒂𝑡

𝑛; 𝜃𝑛))2],

(17)

𝑛, 𝒂𝑡

where 𝑄(S𝑡
with parameters 𝜃𝑛; 𝑦𝑛 is the target Q-value, which calculated using the target Q-network with parameters 𝜃−

𝑛; 𝜃𝑛) is the Q-value of action 𝒂𝑡

𝑛 which is calculated using the main Q-network

𝑛 given the state S𝑡

𝑛 and

it is given as follows:

𝑦𝑛 = 𝑅𝑡

𝑛 + 𝛾𝑄(S𝑡

𝑛, 𝑚𝑎𝑥
𝒂𝑡
𝑛

{𝑄(S𝑡

𝑛, 𝒂𝑡

𝑛; 𝜃𝑛)}; 𝜃−

𝑛),

(18)

where 0 ≤ 𝛾 ≤ 1 is the discount factor.

To update the parameters 𝜃𝑛 of the main Q-network, MEC agent 𝑛 performs a gradient descent step. Finally,

each MEC agent updates the parameters 𝜃−

𝑛 of its target Q-network at a ﬁxed target step by copying the parameters

of the main Q-network.

C. The Inference Phase of DQL

The inference phase of DQL is presented in Algorithm 2. Once the trained DDQNs are obtained, each MEC

agent uses its optimal DDQN parameters to ﬁnd an appropriate placement of the requested service by the vehicles.
In detail, at the beginning of each episode the environment of each MEC agent is built. Then, for each step 𝑡,

each MEC agent observes the current state of its environment and selects an action that maximizes its Q-value

according to its trained DDQN. Based on the selected actions of all MEC agents, the SDN controller ﬁnds the

overall communication delay, computing delay and migration costs and thus we obtain a solution to problem (P1).

IV. SIMULATION RESULTS

We consider a MEC-enabled vehicular network where three gNBs that are attached to three MEC servers are

deployed over a highway as shown in Fig. 1. The gNBs are located randomly along the highway. We assume that

the three MEC servers are deployed along the highway in a triangular fashion as depicted in Fig. 1, where the

distance between MEC 1 and MEC 2 and the distance between MEC 2 and MEC 3 is equal to 2000 m and the

distance between MEC 1 and MEC 3 is equal to 4000 m. The vehicles are drawn randomly in the highway that is

modelled as a rectangle of length 5000 m and width 18 m with two forward lanes and two backward lanes. The
vehicles move with a randomly-chosen ﬁxed speed from the range of [60, 110] km/h and once a vehicle reaches the

boundary of the highway, it reappears in the opposite side. For simplicity, all vehicles keep moving with constant

speeds with acceleration, i.e., once the random speed of a vehicle is chosen, the latter keeps moving with that speed

during the entire simulation period.

The proposed multi-agent DQL algorithm is trained on an computer with an Intel Core i7-10750H CPU, 16GB

RAM and an nVidia GeForce GTX 2070 Super graphic card. The implementation is performed using Python

and PyTorch. After performing hyper-parameters tuning, the following optimized parameters are set. Each DDQN

consists of fully connected hidden neural network with two hidden layers of 256 neurons each. The discount factor is
𝛾 = 0.99. The other DDQN and vehicular network para metes are presented in Table 1. To avoid the overestimation

problem of the Q-value, the parameters of each DDQN network are copied into the parameters of the corresponding

target DDQN every 1000 steps. According to the state of the art of deep learning models, Rectiﬁed Linear Unit

(ReLU) function accelerates the learning process since it is not vanishing gradient.

Fig. 2 illustrates the average reward per episode of one MEC agent. It is clear that the reward improves with

the training episodes as it increases when the number of episodes increases. This shows the effectiveness of the

proposed DQL algorithm. We notice that the DQL algorithm converges at approximately 1000 episodes. In other

words, the corresponding MEC agent converges to a good learning outcome, which implies that it will explore

better actions. We can notice though that the reward converges while incurring large ﬂuctuations which is mainly

due to the high mobility scenario of the vehicular network.

Fig. 3 and Fig. 4 illustrate the average cost represented by the objective function (P1a) which measures the

total service latency (the computing and communication latency) as well as the migration costs under two different

conﬁgurations. The ﬁrst conﬁguration is the computational power conﬁguration and it consists of varying the

number of cores for the three MEC servers. We considered three MEC servers with 4 cores each, or 8 cores
each, or 16 cores each, or 32 cores each, or 64 cores each, with a ﬁxed clock frequency of 2.5 GHz. The

second conﬁguration is the request size conﬁguration and it consists of varying the vehicles’ request sizes, which
we generate uniformly at random within a ﬁxed interval as follows UNIFORM(50, 100), UNIFORM(100, 150),
UNIFORM(150, 200), UNIFORM(200, 250), UNIFORM(250, 300) Kbits.

Fig. 3 shows the objective function while considering the computational power conﬁguration. We can notice that

with increasing the computational power, the average service latency as well as the migration costs are decreasing.

Regardless of different number of cores, the proposed DQL approach performs close-to-the-optimal performance.

TABLE I: Simulation parameters

Parameter

Value

Number of MEC servers

3

Transmit power of each gNB

30 dBm

Migration cost

UNIFORM (0.2, 0.3)

Number of vehicles

4

Request size

UNIFORM (50, 300) Kbits

Noise variance

−174 dBm/Hz

Bandwidth

Learning rate

Number of episodes

Discount factor

10 MHz

3𝑒 − 4

3000

0.99

Replay memory size

100000

Mini-batch size

Target update interval

1024

1000

Loss function

Minimum square error

Optimizer

Activation function

Adam

ReLU

Fig. 4 presents the objective function while considering the request size conﬁguration. The proposed approach is

shown to approximate well the optimal solution in different scenarios of the request size conﬁguration. The smaller

the request size is, the smaller the total delay and the migration costs are. This is because (i) the DQL approach

learns efﬁciently the appropriate placement of each vehicle’s service, which helps in reducing the service latency

and (ii) a small number of bits can be fulﬁlled easily by one MEC server without requiring to migrate to another

MEC server, which helps in reducing the total migration costs. We notice that as the request sizes increase, the

average service delay and the migration costs increase as well.

V. RELATED WORKS

The authors in [26] propose a quality-of-experience (QoE)-aware scheme to ensure service continuity for mobile

cloud computing environment. The scheme relays on the buffer-occupancy threshold policy that classiﬁes the new

arriving request from the mobile users. The proposed scheme protects the migrated service from trafﬁc ﬂuctuation.

In addition, the cloud server can change the buffer threshold dynamically for different categories of requests. In

[18], the authors proposed Follow-Me Chain algorithm to solve the problem of service function chaining (SFC). In

particular, the work studied the problem of inter-MEC handoffs to offer a higher satisfaction for users in mobility

scenarios. Such problem is NP-hard, and authors proposed an integer programming formulation that is solved by

Fig. 2: The training rewards for MEC server.

the Follow-Me Chain algorithm. The work in [27] investigated the relocation problem of virtual network functions

(VNF) within a cloud infrastructure under mobility and resource heterogeneity constraints. The authors studied in

particular the impact of the relocation operation on the service delay and the number of VNF relocations (i.e.,

the number of time that a single VNF is being moved from a cloud to another). The problem of relocation was

formulated as a mized integer linear programming problem and solved through a meta-heuristic approach, namely,

the ant colony optimization technique. Within the same context, the authors in [28] proposed an evaluation of three

container-based schemes for VNF migration as a mechanism to guarantee the service continuity. In particular, the

schemes consider two cases of mobility patterns, respectively, known a priori and unknown mobility patterns. For

the known a priori pattern, temporary ﬁle system and disk-less-based migration are discussed, but the main focus

was on the unknown mobility pattern, where authors proposed a solution that consists in storing the container’s

ﬁle system within the system images in a shared pool. The work in [29] considered two main logical slices

created over the same infrastructure, namely, an autonomous driving slice for safety messages, and an infotainment

slice. The authors proposed a clustering method to partition vehicles to allocate slice leaders on each cluster. A

slice leader is a serving entity using vehicle-to-vehicle (V2V) links to forward safety messages, subsequently the

road side units (RSU) forward the infotainment service using the vehicle-to-infrastructure (V2I) links. In [30], the

authors proposed an ofﬂine RL-based RAN slicing solution and a low-complexity heuristic algorithm, to satisfy

communication resources requirements of different slices with the aim to maximize the resource utilization. The

proposed approach ensures the resource availability to meet the different requirements of the slice’s trafﬁc. The

authors assume that V2V communications are either in cellular (through gNBs) or in sidelink mode (through PC5

communication). In addition, in the sidelink mode each vehicle can multicast to multiple vehicles within the same

cluster. Finally, the proposed RL approach is executed separately for each communication mode (i.e., uplink and

downlink), which means that the RL is being executed twice.

Most of the literature studied hereabove, focus on the resource provisioning at the MEC sides independently with

no consideration to the services migration problem in vehicular network, where factors such such as the mobility

patterns, the services migration costs, and the services requirements need to be considered for a more efﬁcient

service placement schemes. Further, previous works consider only heuristic or meta-heuristic methods that focus

on solving the placement problem in order to minimize only the latency without studying the cost of migration.

The works that considered the service latency as well as the migration costs leverage simple algorithmic solutions

without considering advanced machine learning approaches such as the one proposed in this paper. In this paper, we

ﬁll these gaps and we propose a service migration scheme based on DRL techniques in a MEC-enabled vehicular

network aiming to minimize the total service latency and migration cost.

VI. CONCLUSION

In this paper, we proposed a DRL-based scheme to solve the problem of vehicular service placement and migration

in a MEC-based vehicular network. First, we formulated the problem as a nonlinear integer optimization problem to

minimize the total latency (i.e., communication and computational delays ) plus migration costs in terms of energy

consumption and bandwidth usage. To solve the optimization problem we used standard solvers such as CPLEX,

and we linearize it to transform it into a linear integer optimization problem. Then, we formulate the problem

as a multi-agent Markov decision process and develop a DRL-based method by exploiting the DQL algorithm to

obtain an efﬁcient and non-complex solution. The DQL algorithm uses double DQN and replay memory strategies

to increase the learning accuracy and solve the Q-value overestimation problem. Finally, we have demonstrated

through extensive simulations that the proposed DQL algorithm achieves near-optimal performance compared to

the CPLEX solution.

The authors would like to thank the Natural Sciences and Engineering Research Council of Canada, for the

ﬁnancial support of this research.

ACKNOWLEDGMENT

REFERENCES

[1] A. Rachedi et al., “Ieee access special section editorial: The plethora of research in internet of things (iot),” IEEE Access, vol. 4, pp.

9575–9579, 2016.

[2] L. Yala et al., “Latency and Availability Driven VNF Placement in a MEC-NFV Environment,” in Proc. IEEE Global Commun. Conf.

(GLOBECOM), 2018, pp. 1–7.

[3] A. Alalewi et al., “On 5g-v2x use cases and enabling technologies: A comprehensive survey,” IEEE Access, vol. 9, pp. 107 710–107 737,

2021.

[4] A. Triwinarko et al., “Phy layer enhancements for next generation v2x communication,” Vehicular Communications, vol. 32, p. 100385,

2021.

[5] M. Azizian et al., “An optimized ﬂow allocation in vehicular cloud,” IEEE Access, vol. 4, pp. 6766–6779, 2016.

[6] S. Wang et al., “An Investigation Into the Use of Virtual Reality Technology for Passenger Infotainment in a Vehicular Environment,” in

Proc. IEEE Int. Conf. Adv. Mater. Sci. Eng. (ICAMSE), 2016, pp. 404–407.

[7] H. Khan et al., “Enhancing Video Streaming in Vehicular Networks via Resource Slicing,” IEEE Trans. Veh. Technol., vol. 69, no. 4, pp.

3513–3522, 2020.

[8] M. Azizian et al., “Vehicle software updates distribution with sdn and cloud computing,” IEEE Communications Magazine, vol. 55, no. 8,

pp. 74–79, 2017.

[9] A. Abouaomar et al., “Resource provisioning in edge computing for latency-sensitive applications,” IEEE Internet of Things Journal,

vol. 8, no. 14, pp. 11 088–11 099, 2021.

[10] A. Filali et al., “Preemptive SDN Load Balancing with Machine Learning for Delay Sensitive Applications,” IEEE Trans. Veh. Technol.,

vol. 69, no. 12, pp. 15 947–15 963, 2020.

[11] P. A. Frangoudis et al., “Service migration versus service replication in multi-access edge computing,” in 2018 14th International Wireless

Communications Mobile Computing Conference (IWCMC), 2018, pp. 124–129.

[12] Z. Mlika et al., “Network Slicing with MEC and Deep Reinforcement Learning for the Internet of Vehicles,” IEEE Network, pp. 1–7,

2021, Early Access.

[13] A. Abouaomar et al., “Service Function Chaining in MEC: A Mean-Field Game and Reinforcement Learning Approach,” 2021.

[14] A. Abouaomar et al., “Mean-Field Game and Reinforcement Learning MEC Resource Provisioning for SFC,” in IEEE GLOBECOM

Conference, 2021, pp. 1–6.

[15] A. Aissioui et al., “On enabling 5g automotive systems using follow me edge-cloud concept,” IEEE Transactions on Vehicular Technology,

vol. 67, no. 6, pp. 5302–5316, 2018.

[16] T. Ouyang et al., “Adaptive User-Managed Service Placement for Mobile Edge Computing: An Online Learning Approach,” in Proc. IEEE

Conf. Comput. Commun. (INFOCOM), 2019, pp. 1468–1476.

[17] Z. Ennya et al., “Computing tasks distribution in fog computing: Coalition game model,” in 2018 6th International Conference on Wireless

Networks and Mobile Communications (WINCOM), 2018, pp. 1–4.

[18] T. Ouyang et al., “Follow Me at the Edge: Mobility-Aware Dynamic Service Placement for Mobile Edge Computing,” IEEE J. Sel. Areas

Commun., vol. 36, no. 10, pp. 2333–2345, 2018.

[19] A. Tak et al., “Federated edge learning: Design issues and challenges,” IEEE Network, vol. 35, no. 2, pp. 252–258, 2021.

[20] A. Abouaomar et al., “A resources representation for resource allocation in fog computing networks,” in 2019 IEEE Global Communications

Conference (GLOBECOM), 2019, pp. 1–6.

[21] R. Urgaonkar et al., “Dynamic Service Migration and Workload Scheduling in Edge-Clouds,” Performance Evaluation, vol. 91, pp. 205–228,

2015.

[22] A. Abouaomar et al., “Matching-Game for User-Fog Assignment,” in Proc. IEEE Global Commun. Conf. (GLOBECOM), 2018, pp. 1–6.

[23] Z. Mlika et al., “User–Base-Station Association in HetSNets: Complexity and Efﬁcient Algorithms,” IEEE Trans. Veh. Technol., vol. 66,

no. 2, pp. 1484–1495, 2017.

[24] V. Mnih et al., “Human-Level Control Through Deep Reinforcement Learning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.

[25] H. Van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning,” in Proc. AAAI Conf. Artif. Intell., vol. 30, no. 1, 2016.

[26] Y.-R. Haung, “A QoE-Aware Strategy for Supporting Service Continuity in an MCC Environment,” Wireless Pers. Commun., vol. 116,

no. 1, pp. 629–654, 2021.

[27] P. Roy et al., “User Mobility and Quality-of-Experience Aware Placement of Virtual Network Functions in 5G,” Comput. Commun., vol.

150, pp. 367–377, 2020.

[28] R. A. Addad et al., “Towards a Fast Service Migration in 5G,” in Proc. IEEE Conf. Standards Commun. Netw. (CSCN), 2018, pp. 1–6.

[29] H. Khan et al., “Network Slicing for Vehicular Communication,” Trans. Emerg. Telecommun. Technol., vol. 32, no. 1, p. e3652, 2021.

[30] H. D. R. Albonda et al., “An Efﬁcient RAN Slicing Strategy for a Heterogeneous Network With eMBB and V2X Services,” IEEE Access,

vol. 7, pp. 44 771–44 782, 2019.

Fig. 3: The objective function vs. the computational power of the MEC servers.

Fig. 4: The objective function vs. the request sizes of the vehicles.

