2
2
0
2

n
a
J

7
2

]
I

N
.
s
c
[

1
v
1
3
8
1
1
.
1
0
2
2
:
v
i
X
r
a

A Deep Reinforcement Learning Approach for

Service Migration in MEC-enabled Vehicular

Networks

Amine Abouaomarâˆ—â€¡, Zoubeir Mlikaâˆ—, Abderrahime Filaliâˆ—, Soumaya Cherkaouiâˆ—, and Abdellatif Kobbaneâ€¡
âˆ—INTERLAB, Engineering Faculty, UniversitÂ´e de Sherbrooke, Sherbrooke (QC), Canada
â€ ENSIAS, Mohammed V University, Rabat, Morocco

Abstract

Multi-access edge computing (MEC) is a key enabler to reduce the latency of vehicular network. Due to the

vehicles mobility, their requested services (e.g., infotainment services) should frequently be migrated across different

MEC servers to guarantee their stringent quality of service requirements. In this paper, we study the problem of service

migration in a MEC-enabled vehicular network in order to minimize the total service latency and migration cost. This

problem is formulated as a nonlinear integer program and is linearized to help obtaining the optimal solution using

off-the-shelf solvers. Then, to obtain an efï¬cient solution, it is modeled as a multi-agent Markov decision process and

solved by leveraging deep Q learning (DQL) algorithm. The proposed DQL scheme performs a proactive services

migration while ensuring their continuity under high mobility constraints. Finally, simulations results show that the

proposed DQL scheme achieves close-to-optimal performance.

Multi-access edge computing, vehicular networks, reinforcement learning, service migration.

Index Terms

I. INTRODUCTION

Intelligent transportation systems (ITS) represents a critical component of the Internet of Things (IoT) and future

smart cities [1]. ITS will potentially provide a more secure transportation environment through effective vehicle

coordination and efï¬cient resource management [2]â€“[5]. In addition to safety, the ITS ecosystem will provide

entertainment services such as video streaming and gaming, which can be extended to in-vehicle augmented reality

[6]â€“[8]. To achieve these promising features, vehicles must be able to communicate, exchange information and access

given services with low latency. Therefore, vehicles must operate in an environment that meets these requirements.

Multi-access edge computing (MEC) is envisioned as a key component for ï¬fth-generation (5G) ultra-reliable

low-latency communications (uRLLC) services, alongside software-deï¬ned networking (SDN) [9] technology. On

the one hand, MEC can be leveraged as an emerging computational paradigm that provides efï¬cient computational

capabilities to vehicles deployed in close proximity to MEC servers while ensuring a low latency. On the other

 
 
 
 
 
 
hand, SDN technology enables seamless, transparent, and efï¬cient control through the separation of the data plane

and the control plane, which simpliï¬es network operation and management [10], [11]. Therefore, a MEC-enabled

vehicular network can beneï¬t from SDN to provide efï¬cient resource management and uRLLC vehicular services

[12]â€“[15]. Nevertheless, due to the limited resources of MECs and the high mobility of vehicles, there are many

challenges. In particular, the requested vehicular services must be located and migrated to different MEC servers

to guarantee their continuity [16]â€“[18].

To address these challenges, we investigate the service placement and migration problem in a MEC-enabled

vehicular network. We leverage SDN technologies to have efï¬cient control on the MEC servers operations, with

the objective of reducing the average service latency of the vehicles. We ï¬rst, formulate the problem of service

placement and migration as a nonlinear integer program that we linearize to obtaining the optimal solution using

off-the-shelf solvers. Second, we modeled the problem as a multi-agent Markov decision process (MMDP), in

order to solve it efï¬ciently using deep reinforcement learning (DRL) techniques, speciï¬cally, the deep Q-networks

(DQN). The proposed DRL-based placement and migration scheme ensures service continuity under high mobility

constraints and offer a reduced total service latency as well as the additional operational costs associated with

the migration. The proposed scheme performs proactive placement of the requested services while considering the

mobility of vehicles, the required amounts of computational and communication resources, and the overall migration

costs.

To summarize, the main contributions of this paper are synthesized as follows:

â€¢ We formulate the service placement and migration problem as a non-linear program to minimize the total service

latency (including the computing latency and the communication latency) and the cross-edge operational costs.

â€¢ We propose an MMDP framework that helps solving the problem in a distributed and scalable manner.

â€¢ We leverage DRL techniques to provide efï¬cient solution to the MMDP model. Speciï¬cally, we propose a

deep Q learning (DQL)-based solution that uses double Q network and replay buffer to improve the learning

outcome.

â€¢ We evaluate the performance of the proposed DRL-based scheme and compare it to the optimal solution

obtained by the CPLEX solver and we show that the proposed solution achieves close-to-optimal performance.

The remainder of this paper is structured as follows. In Section II, we present the system model and the problem

formulation of the service placement and migration problem. In Section III, we present the proposed multi-agent

DQL-based solution. The performance of the proposed solution is evaluated in Section IV. Last but not least, the

related works are discussed in Section V. Finally, the paper is concluded in Section VI.

A. System Model

II. SYSTEM MODEL AND PROBLEM FORMULATION

We consider an SDN-enabled MEC architecture covered with a set of gNodeBs (gNBs), each is equipped with a
MEC server ğ‘› âˆˆ N (cid:66) {1, 2, . . . , ğ‘ } that is connected to one gNB via high speed local-area network as illustrated in
Fig. 1. There are ğ¾ mobile users (or interchangeably called vehicles) demanding services from the MEC servers and
are denoted by the set K (cid:66) {1, 2, . . . , ğ¾ }. Each vehicle ğ‘˜ requests some service to fulï¬ll its requirements. Without

Fig. 1: Illustration of the system model

loss of generality, we assume that all vehicles request the same vehicular service1 (e.g., an infotainment-related

service). Similar to previous works [19]â€“[22], we consider a MEC-based device-oriented service model contrary

to the traditional cloud-based application-oriented service model. In other words, a dedicated container or virtual

machine is assigned the vehicular service as well as the applicationsâ€™ environment, which are executed on each

vehicle rather than on each application. An SDN controller is assumed to be placed on the cloud layer where it

acts as a central controller for information exchange between vehicles. Time is discrete and is divided into a set
of ğ‘‡ time-slots denoted by the set T = {1, 2, . . . , ğ‘‡ }. At each time-slot ğ‘¡ âˆˆ T , each vehicle ğ‘˜ âˆˆ K requests the
vehicular service from the MEC node ğ‘› âˆˆ N .

The objective of this work is to guarantee the minimum quality of service (QoS) requirements of the vehicles

while considering their erratic mobility and the computing and communication resources of MEC servers. To do

so, the requested vehicular service should be placed and migrated across different MEC servers depending on the

vehicles mobility patterns. In this work, we consider an hybrid centralized-distributed architecture where (i) each

MEC server plays the role of an agent that makes its service placement and migration decisions independently of

1The case of multiple services will be considered in our future work where network slicing will be integrated into our system model.

other MEC servers, and (ii) once each MEC agent makes its decision, it communicates it to the SDN controller

that plays the role of a central agent to coordinate the decisions of all MEC servers.

The considered QoS is represented by the vehicular service latency that includes (i) the communication delay

that is incurred by the transmission between a vehicle and a MEC server, and (ii) the computing delay that depends

on the processing capability of the MEC server as well as the size of the vehicleâ€™s request.

1) Communication Delay: When a vehicle ğ‘˜ requests from a MEC server ğ‘› the vehicular service, the transmission
between ğ‘˜ and ğ‘› depends mainly on the wireless environment and on the size of the requested service. The channel
power gain between vehicle ğ‘˜ and MEC server ğ‘› at time-slot ğ‘¡ is denoted by ğ‘”ğ‘¡

ğ‘˜ğ‘›, which includes the small-scale

fading as well as the large-scale fading. To simplify the analysis, we assume that the total available bandwidth

is divided equally between the MEC servers and each MEC server allocates its bandwidth to the vehicles in an
orthogonal manner. Accordingly, the received signal to noise ratio between MEC server ğ‘› and vehicle ğ‘˜ at time-slot
ğ‘¡ is given by

ğ›¾ğ‘¡
ğ‘˜ğ‘› (cid:66)

ğ‘ğ‘›ğ‘”ğ‘¡
ğ‘˜ğ‘›
ğœ2

,

(1)

where ğ‘ğ‘› is the transmit power of MEC server ğ‘› and ğœ2 is the power of the noise. The achieved data rate can be

given as follows

ğ‘˜ğ‘› (cid:66) ğ‘¤ğ‘› lg (cid:0)1 + ğ›¾ğ‘¡
Î“ğ‘¡

ğ‘˜ğ‘›

(cid:1) ,

[in bits/sec]

(2)

where ğ‘¤ğ‘› is the allocated bandwidth of MEC server ğ‘›. Consequently, the communication delay between MEC
server ğ‘› and vehicle ğ‘˜ at time-slot ğ‘¡ is given by:

ğ‘‘ğ‘¡
ğ‘˜ğ‘› (cid:66)

ğ‘ ğ‘˜
Î“ğ‘¡
ğ‘˜ğ‘›

,

[in sec]

(3)

where ğ‘ ğ‘˜ is the size of the requested service of vehicle ğ‘˜.

2) Computing Delay: The computing delay depends on the processing capacity of each MEC server ğ‘›, on the
total vehicles sharing MEC server ğ‘›, and on the requested computing capacity of the vehicular service of vehicle
ğ‘˜ at time-slot ğ‘¡. More precisely, the computing delay between MEC server ğ‘› and vehicle ğ‘˜ at time-slot ğ‘¡ is given

as follows [18]:

ğ‘˜ğ‘› (cid:66) ğ‘ğ‘¡
ğ‘ğ‘¡

ğ‘˜ ğ‘ ğ‘¡

ğ‘›/ğ¹ğ‘›,

[in sec]

(4)

where ğ‘ğ‘¡
ğ‘˜ denotes the amount of computing capacity [in CPU cycles] required by the requested vehicular service
of vehicle ğ‘˜ at time-slot ğ‘¡. The computing capacity of MEC server ğ‘› [in CPU cycles/sec] is given by ğ¹ğ‘› and the
number of vehicles placed on MEC server ğ‘› at time-slot ğ‘¡ is given by ğ‘ ğ‘¡
ğ‘›.

Besides the QoS requirements, placing and migrating the vehicular service across multiple MEC servers incur

additional operational costs related, for example, to the energy consumption and the bandwidth usage. For this

reason, we consider the migration cost as an important factor into the design of our service migration solution.

3) Migration Cost: Due to the cross-edge migration, additional operational costs are incurred by the service

migration. These costs include energy consumption, expensive wide-area network bandwidth usage, etc. [18]. To
make the operational cost model general, we use ğ‘š ğ‘˜ğ‘¡

ğ‘›(cid:48)ğ‘› to denote the cost of migrating the vehicular service of

vehicle ğ‘˜ from MEC server ğ‘›(cid:48) to MEC node ğ‘› at time-slot ğ‘¡. Obviously, we assume that ğ‘š ğ‘˜ğ‘¡
and for all ğ‘˜, ğ‘¡.

ğ‘›(cid:48)ğ‘› = 0, for all ğ‘›(cid:48) = ğ‘›

B. Problem Formulation

To guarantee the required QoS (communication and computing delays) and the migration cost, the optimization

problem is formulated as a multi-objective optimization problem where the aim is to optimize the communication

delay, the computing delay as well as the migration costs. To simplify the resolution of this multi-objective problem,
we transform the multi-objective problem into a single objective one by introducing the weights ğœ†ğ‘– for ğ‘– âˆˆ {1, 2, 3}.

The formulated problem is written as a nonlinear integer program (NLP) as follows.

minimize
x

subject to

ğœ†1ğ¶ (x) + ğœ†2ğ· (x) + ğœ†3 ğ‘€ (x)

ğ‘¥ğ‘¡
ğ‘˜ğ‘› âˆˆ {0, 1}, âˆ€ğ‘˜, ğ‘›, ğ‘¡,
ğ‘
âˆ‘ï¸

ğ‘¥ğ‘¡
ğ‘˜ğ‘› = 1, âˆ€ğ‘˜, ğ‘¡,

ğ‘›=1

(P1a)

(P1b)

(P1c)

where the variables ğ‘¥ğ‘¡
ğ‘› at time-slot ğ‘¡. We denote by x the multidimensional notation of the variables ğ‘¥ğ‘¡

ğ‘˜ğ‘› = 1 if and only if the vehicular service requested by vehicle ğ‘˜ is placed at MEC server
ğ‘˜ğ‘›]. The objective

ğ‘˜ğ‘›, i.e., x = [ğ‘¥ğ‘¡

function in (P1a) is a linear combination of the communication delay, the computing delay, and the migration cost.
Constraints (P1b) guarantee that the variables ğ‘¥ğ‘¡
requested by vehicle ğ‘˜ at time-slot ğ‘¡ is placed at one and only one MEC server.

ğ‘˜ğ‘› are binary. Constraints (P1c) guarantee that the vehicular service

The total computing delay ğ¶ (x) is deï¬ned as follows:

ğ¶ (x) (cid:66)

ğ‘
âˆ‘ï¸

ğ¾
âˆ‘ï¸

ğ‘‡
âˆ‘ï¸

ğ‘›=1

ğ‘˜=1

ğ‘¡=1

ğ‘˜ğ‘›ğ‘ ğ‘¡
ğ‘¥ğ‘¡

ğ‘›ğ‘ğ‘¡

ğ‘˜ /ğ¹ğ‘›,

(5)

where ğ‘ğ‘¡
ğ‘˜ denotes the required amount of computing capacity [in CPU cycles] of the vehicular service requested by
vehicle ğ‘˜ at time-slot ğ‘¡ and ğ¹ğ‘› denotes the maximum computing capacity of MEC server ğ‘› [in CPU cycles/sec].
The term ğ‘ ğ‘¡

ğ‘› denotes the number of services placed at MEC server ğ‘›, i.e.,

ğ‘ ğ‘¡

ğ‘› (cid:66)

ğ¾
âˆ‘ï¸

ğ‘˜=1

ğ‘¥ğ‘¡
ğ‘˜ğ‘›.

The total communication delay ğ· (x) is deï¬ned as follows:
ğ‘‡
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ¾
âˆ‘ï¸

ğ· (x) (cid:66)

ğ‘˜ğ‘›ğ‘‘ğ‘¡
ğ‘¥ğ‘¡

ğ‘˜ğ‘›,

where ğ‘‘ğ‘¡

ğ‘˜=1
ğ‘˜ğ‘› denotes the computing delay between MEC server ğ‘› and the vehicle ğ‘˜ at time-slot ğ‘¡ (see (3)).

ğ‘›=1

ğ‘¡=1

Finally, the total migration cost ğ‘€ (x) is deï¬ned as follows:
ğ‘‡
âˆ‘ï¸

ğ¾
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ‘€ (x) (cid:66)

ğ‘˜ğ‘›(cid:48) ğ‘¥ğ‘¡
ğ‘¥ğ‘¡âˆ’1

ğ‘˜ğ‘›ğ‘š ğ‘˜ğ‘¡
ğ‘›(cid:48)ğ‘›,

ğ‘›=1

ğ‘›(cid:48)=1

ğ‘˜=1

ğ‘¡=1

(6)

(7)

(8)

where the migration cost ğ‘š ğ‘˜ğ‘¡
server ğ‘› at time-slot ğ‘¡. It is clear that the cost is counted inside the summation only if both ğ‘¥ğ‘¡âˆ’1

ğ‘›(cid:48)ğ‘› is used to denote the cost of migrating the service ğ‘˜ from MEC server ğ‘›(cid:48) to MEC

ğ‘˜ğ‘›(cid:48) and ğ‘¥ğ‘¡

ğ‘˜ğ‘› are

equal to one, i.e., ğ‘¥ğ‘¡âˆ’1
ğ‘˜ğ‘› = 1, which means that the requested service of vehicle ğ‘˜ is placed at MEC server
ğ‘›(cid:48) at time-slot ğ‘¡ âˆ’ 1 and is placed at MEC server ğ‘› at time-slot ğ‘¡. This costs includes bandwidth costs incurred by

ğ‘˜ğ‘›(cid:48) = ğ‘¥ğ‘¡

cross-edge migration (e.g., wide-area network bandwidth usage costs) as well as energy costs caused by increased

energy consumption of network devices such as routers. To make the model general, we use a general cost term
ğ‘š ğ‘˜ğ‘¡

ğ‘›(cid:48)ğ‘› as done in [18].

In order to make the problem more tractable, we linearize the objective function given in (P1a). The non-linearity
of (P1) comes from the functions ğ¶ (x) and ğ‘€ (x) due to the multiplication of binary variables. To linearize ğ‘€ (x),
we introduce a new binary variable called ğ‘§ğ‘˜ğ‘¡
if each term of the product of the x-variables is positive. In other words, ğ‘§ğ‘˜ğ‘¡
ğ‘˜ğ‘›(cid:48) = ğ‘¥ğ‘¡
that we must add the following two constraints to force the z-variable to be zero whenever ğ‘¥ğ‘¡âˆ’1

ğ‘˜ğ‘›. It is clear that this new z-variable is positive if and only

ğ‘›(cid:48)ğ‘› = 1 â‡â‡’ ğ‘¥ğ‘¡âˆ’1

ğ‘˜ğ‘› = 1. This means

ğ‘›(cid:48)ğ‘› = ğ‘¥ğ‘¡âˆ’1

ğ‘˜ğ‘›(cid:48) ğ‘¥ğ‘¡

ğ‘˜ğ‘›(cid:48) or ğ‘¥ğ‘¡

ğ‘˜ğ‘› is zero:

and

ğ‘›(cid:48)ğ‘› â‰¤ ğ‘¥ğ‘¡âˆ’1
ğ‘§ğ‘˜ğ‘¡

ğ‘˜ğ‘›(cid:48) , âˆ€ğ‘˜, ğ‘›, ğ‘›(cid:48), ğ‘¡ > 1,

ğ‘›(cid:48)ğ‘› â‰¤ ğ‘¥ğ‘¡
ğ‘§ğ‘˜ğ‘¡

ğ‘˜ğ‘›, âˆ€ğ‘˜, ğ‘›, ğ‘›(cid:48), ğ‘¡.

(9)

(10)

It remains to enforce the constraints that if both ğ‘¥ğ‘¡âˆ’1

ğ‘˜ğ‘›(cid:48) and ğ‘¥ğ‘¡

ğ‘˜ğ‘› are equal to one, then the z-variable is one. This

can be written as follows:

ğ‘›(cid:48)ğ‘› â‰¥ ğ‘¥ğ‘¡âˆ’1
ğ‘§ğ‘˜ğ‘¡

ğ‘˜ğ‘›(cid:48) + ğ‘¥ğ‘¡

ğ‘˜ğ‘› âˆ’ 1, âˆ€ğ‘˜, ğ‘›, ğ‘›(cid:48), ğ‘¡ > 1.

Thus, the total migration cost can be rewritten as follows:

ğ‘€ (z) =

ğ‘
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ¾
âˆ‘ï¸

ğ‘‡
âˆ‘ï¸

ğ‘›=1

ğ‘›(cid:48)=1

ğ‘˜=1

ğ‘¡=1

ğ‘›(cid:48)ğ‘›ğ‘š ğ‘˜ğ‘¡
ğ‘§ğ‘˜ğ‘¡
ğ‘›(cid:48)ğ‘›,

where z denotes the multidimensional notation of the variables ğ‘§ğ‘˜ğ‘¡

Now, to linearize ğ¶ (x), we let ğ‘¦ğ‘¡

ğ‘˜ denote the quantity (cid:205)ğ‘
ğ‘›=1

ğ‘›(cid:48)ğ‘›, i.e., z = [ğ‘§ğ‘˜ğ‘¡
ğ‘¥ğ‘¡
ğ‘˜ğ‘›ğ‘ ğ‘¡
ğ‘˜ /ğ¹ğ‘›, i.e.,

ğ‘›ğ‘ğ‘¡

ğ‘›(cid:48)ğ‘›].

ğ‘¦ğ‘¡
ğ‘˜ (cid:66)

ğ‘
âˆ‘ï¸

ğ‘›=1

ğ‘˜ğ‘›ğ‘ ğ‘¡
ğ‘¥ğ‘¡

ğ‘›ğ‘ğ‘¡

ğ‘˜ /ğ¹ğ‘›, âˆ€ğ‘˜, ğ‘¡.

Thus, the total computing delay ğ¶ (y) can be rewritten as follows:

ğ¶ (y) =

ğ¾
âˆ‘ï¸

ğ‘‡
âˆ‘ï¸

ğ‘˜=1

ğ‘¡=1

ğ‘¦ğ‘¡
ğ‘˜ ,

(11)

(12)

(13)

(14)

where y denotes the multidimensional notation of the variables ğ‘¦ğ‘¡

ğ‘˜ , i.e., y = [ğ‘¦ğ‘¡

ğ‘˜ ]. Now, we have to enforce that the

following constraints:

ğ‘˜ğ‘› = 1 â‡’ ğ‘¦ğ‘¡
ğ‘¥ğ‘¡

ğ‘˜ = ğ‘ ğ‘¡

ğ‘›ğ‘ğ‘¡

ğ‘˜ /ğ¹ğ‘›.

(15)

These are indicator constraints that can be easily implemented in the off-the-shelf solvers such as CPLEX or Gurobi.

Nonetheless, they can be easily transformed to linear constraints using the big-M method [?], [23].

III. PROPOSED SOLUTION

In this section, we propose a deep reinforcement learning (DRL) based approach to obtain an efï¬cient solution to

the service placement and migration problem deï¬ned in (P1). The proposed approach places the vehicular service

requested by the vehicles in the appropriate MEC servers to ensure the continuity of services under the mobility

constraint of vehicles while reducing the communication latency, the computing latency as well as the migration

costs of the requested service.

We use deep Q-learning (DQL) [24]â€”one of the most popular DRL algorithmâ€”to efï¬ciently solve the service

placement problem in the MEC-enabled vehicular network. DQL combines Q-learning with deep neural network

(DNN). It takes as input the observed state of the environment and returns as output the Q-value of all possible

actions. DQL has two main phases, namely the training phase and the inference phase. In the training phase,

the agent trains a DNN, called deep Q-network (DQN), in an ofï¬‚ine manner. In the inference phase, the agent

takes actions in an online manner based on the trained DQN. Before describing each phase of the proposed DQL

algorithm, we model, ï¬rst, the problem as a Markov decision process (MDP).

A. The MDP Formulation

We consider a multi-agent MDP where each MEC server ğ‘› acts as an independent agent, called herein after MEC
agent ğ‘›. At time-slot ğ‘¡, each MEC agent ğ‘› can decide either to place and instantiate the vehicular service requested
by vehicle ğ‘˜ or not. The key elements of the multi-agent MDP are deï¬ned as follows:

1) The State Space: At time-slot ğ‘¡, the observed state by the MEC agent ğ‘›, denoted by Sğ‘¡

ğ‘›, mainly depends on

the current vehicular environment. It includes the current positions of the vehicles, their velocities, their directions,
and their service requirements (including the wireless channel gains and the distances between MEC agent ğ‘› and

the vehicles). Note that there are as many states as there are time-slots, i.e., every time-slot corresponds to a state.

In addition, a transition from one state to the next happens according to the mobility model of the vehicles.

2) The Action Space: The action set of each MEC agent ğ‘› at time-slot ğ‘¡ is given by the set Ağ‘¡

ğ‘› (cid:66) {0, 1}ğ¾ .

ğ‘› âˆˆ Ağ‘¡

ğ‘› is given by the row vector [ğ‘ğ‘¡

Indeed, an action ğ’‚ğ‘¡
the decision to place the service ğ‘˜ âˆˆ K at MEC server ğ‘›, all happening at time-slot ğ‘¡. Note that the variable ğ‘ğ‘¡
ğ‘¥ğ‘¡
ğ‘˜ğ‘› deï¬ned in (P1) means essentially the same thing but to remove any possible confusion between the optimization
variable ğ‘¥ğ‘¡
ğ‘˜ğ‘› we use two different notations. Each MEC agent ğ‘› communicates its chosen
action to the SDN controller to form a global action ğ’‚ğ‘¡ (cid:66) [ğ’‚ğ‘¡
ğ‘ ]. Then, the SDN controller veriï¬es if
1

ğ¾ ğ‘›], where each element ğ‘ğ‘¡

ğ‘˜ğ‘› and the MDP action ğ‘ğ‘¡

ğ‘˜ğ‘› corresponds to

2ğ‘›, . . . , ğ‘ğ‘¡

, . . . , ğ’‚ğ‘¡

1ğ‘›, ğ‘ğ‘¡

ğ‘˜ğ‘› and

, ğ’‚ğ‘¡
2

the individual actions of the MEC agents are feasible or not according to the constraints of (P1), i.e., the individual
action ğ’‚ğ‘¡

ğ‘› of MEC agent ğ‘› is considered feasible if it meets the constraints of (P1).

3) The Reward Function: A MEC agent ğ‘› chooses an action ğ’‚ğ‘¡

ğ‘› âˆˆ Ağ‘¡

ğ‘› at time-slot ğ‘¡ and receives a reward ğ‘…ğ‘¡
ğ‘›.

Since we seek to minimize the overall vehicular service latency requested by the vehicles, the objective of MEC
agent ğ‘› must be related to the sum-latency of the services it hosts. In other words, we deï¬ne the reward ğ‘…ğ‘¡
ğ‘› of
MEC agent ğ‘› at time-slot ğ‘¡ in relation with how the placement of requested service at ğ‘› affects the latency of the
system. Therefore, the SDN controller calculates the individual reward of MEC agent ğ‘› as follows:

ğ‘…ğ‘¡

ğ‘› (cid:66)

ğœ†1ğ¶ğ‘¡

ğ‘› + ğœ†2ğ·ğ‘¡

ğ‘› + ğœ†3 ğ‘€ ğ‘¡
ğ‘›,

âˆ’1,

if ğ’‚ğ‘¡

ğ‘› is feasible

if ğ’‚ğ‘¡

ğ‘› is not feasible,

(16)

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

where ğ¶ğ‘¡
ğ‘› = (cid:205)ğ‘
ğ‘€ ğ‘¡

ğ‘› = (cid:205)ğ¾
ğ‘˜=1
(cid:205)ğ¾

ğ‘ğ‘¡
ğ‘˜ğ‘›ğ‘ ğ‘¡
ğ‘˜ğ‘›(cid:48) ğ‘ğ‘¡
ğ‘ğ‘¡âˆ’1

ğ‘›ğ‘ğ‘¡
ğ‘˜ğ‘›ğ‘š ğ‘˜ğ‘¡

ğ‘˜=1

agent ğ‘› at time-slot ğ‘¡ is not feasible, this MEC agent should be penalized with a negative reward ğ‘…ğ‘¡

ğ‘› = (cid:205)ğ¾
ğ‘˜ /ğ¹ğ‘› is the computation delay, ğ·ğ‘¡
ğ‘˜ğ‘› is the communication delay, and
ğ‘˜=1
ğ‘›(cid:48)ğ‘› is the migration cost of MEC agent ğ‘› at time-slot ğ‘¡. If the action chosen by MEC
ğ‘› = âˆ’1 to prompt

ğ‘˜ğ‘›ğ‘‘ğ‘¡
ğ‘ğ‘¡

ğ‘›(cid:48)=1
ğ‘›(cid:48)â‰ ğ‘›

it to not choose this action in future steps.

B. The Training Phase of DQL

In general, DQN approximates the Q-values ğ‘„(ğ‘ , ğ‘, ğœƒ) of each state-action pair (ğ‘ , ğ‘) using a DNN, where ğœƒ

represents the parameters of the Q-network. Since we propose a multi-agent MDP, the proposed DQL algorithm

will be a multi-agent algorithm in which each MEC agent will have its own DQN to be approximated and trained.
When there is no confusion, we omit the index ğ‘› from the DQN of MEC agent ğ‘›. In addition, the training process

of the DNN uses the experience replay memory mechanism. This mechanism helps in creating a dataset to train

the DNN once in a while by storing each MEC agent experience into a replay buffer. This experience essentially

includes the current state, the next transition state, the chosen action and the received reward. Then, each MEC

agent randomly chooses a set of samples from its replay buffer to perform the learning process. The experience

replay memory mechanism not only allows the MEC agent to learn from the past experiences, but also to provide

uncorrelated data as inputs which breaks undesirable temporal correlations. However, DQN is known to overestimate

the Q-values of stat-action pairs under certain conditions, which harms the performances. To overcome this issue,

double DQN (DDQN) [25] is proposed which reduces the overestimation and makes the training process faster and

more reliable. Indeed, DDQN uses two DNNs, called the main Q-network and the target Q-network. The former
is used to compute the Q-values ğ‘„(ğ‘ , ğ‘, ğœƒ) while the latter is used to provide the target Q-values ğ‘„(ğ‘ , ğ‘, ğœƒâˆ’) to
train the parameters ğœƒ of the main Q-network. The training phase of our proposed multi-agent DQL algorithm is
presented in Algorithm 1, where each MEC agent ğ‘› âˆˆ N trains its own DDQN.

The training phase of the DQL algorithm requires as input the vehicular environment which includes the vehicles,

the requested services, the MEC servers, the computing capacity of MEC servers. It returns the trained DDQN of

each MEC agent as output. The DDQNs are trained simultaneously. The training begins by generating the vehicles

parameters and the network parameters. The vehicles parameters include the position, the velocity and the requested

service of each vehicle. The network parameters include the computing capacity of each MEC server. Then, the DQL

algorithm initializes the DDQN parameters of each MEC agent. Next, it iterates the episodes. For each episode,

the environment of each MEC agent is built by updating the position of the vehicles according to the mobility

model and generating other network parameters. For each episode, the training continues for a period of time-slots
(or steps). In each step ğ‘¡, each MEC agent ğ‘› observes the current state of its environment and chooses an action
from its action space Ağ‘¡
ğ‘›. To select an action, the MEC agent uses the ğœ–-greedy policy. With this policy, an action
is chosen randomly with probability ğœ–. Once all MEC agents select their actions, each of them communicates its
action to the SDN controller to construct the global action ğ’‚ğ‘¡ . The SDN controller uses the constructed global

Algorithm 1: The Training Phase of DQL
Input: Agents and environment

1 Output: Trained DDQNs

Initialization: Generate vehicles and network parameters;

Initialize the DDQN of each agent ğ‘›;

2 for Episode ğ‘’ do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

Reset and build the agentsâ€™ environment;
for Time-slot ğ‘¡ do

for each MEC ğ‘› do

Observe the environment ;
Choose an action ğ‘ğ‘¡

ğ‘› using ğœ–-greedy;

end for

The SDN controller obtains the global action ;

The SDN controller calculates the individual reward of each agent;
for each MEC ğ‘› do

Receive the individual reward from the SDN controller;

Observe the next state of the environment ;
Store the experience ğ¸ğ‘¥ ğ‘ğ‘¡

ğ‘› in the replay buffer Mğ‘›;

if batch size then

Sample a mini-batch from Mğ‘›;

Do a mini-batch training;

end if

if target step then

Update the target network parameters ğœƒâˆ’
ğ‘›;

end if

end for

end for

24 end for

action to verify its feasibility and calculate the individual reward of each MEC agent. Then, each MEC agent ğ‘›
receives its individual reward ğ‘…ğ‘¡
ğ‘› from the SDN controller and moves to the next state. The obtained experience,
denoted by ğ¸ğ‘¥ ğ‘ğ‘›, is stored by the MEC agent ğ‘› in its replay buffer Mğ‘›. When the replay buffer contains enough

experiences, i.e., a certain batch size is respected, each MEC agent randomly samples a mini-batch to create a

training dataset. The latter is used by the MEC agent to perform the training process. In the training process, each

Algorithm 2: The Inference Phase of DQL
Input: The trained DDQNs

1 Output: Placement of the vehicular service of each vehicle

Initialization: Load the DDQN of each agent ğ‘›;

2 for Episode ğ‘’ do

3

4

5

6

7

8

9

10

11

Reset and build the environment;
for Step ğ‘¡ do

for each MEC ğ‘› do

Observe the environment ;
Choose an action ğ‘ğ‘¡

ğ‘› that maximize the Q-value of the tained DDQN of ğ‘›;

end for

The SDN controller obtains the global action;

end for

The SDN controller calculates the objective function as in (P1a);

12 end for

MEC agent seeks to minimize a loss function, given by:

ğ‘› (ğœƒğ‘›) = E[(ğ‘¦ğ‘› âˆ’ ğ‘„(Sğ‘¡
ğ¿ğ‘¡

ğ‘›, ğ’‚ğ‘¡

ğ‘›; ğœƒğ‘›))2],

(17)

ğ‘›, ğ’‚ğ‘¡

where ğ‘„(Sğ‘¡
with parameters ğœƒğ‘›; ğ‘¦ğ‘› is the target Q-value, which calculated using the target Q-network with parameters ğœƒâˆ’

ğ‘›; ğœƒğ‘›) is the Q-value of action ğ’‚ğ‘¡

ğ‘› which is calculated using the main Q-network

ğ‘› given the state Sğ‘¡

ğ‘› and

it is given as follows:

ğ‘¦ğ‘› = ğ‘…ğ‘¡

ğ‘› + ğ›¾ğ‘„(Sğ‘¡

ğ‘›, ğ‘šğ‘ğ‘¥
ğ’‚ğ‘¡
ğ‘›

{ğ‘„(Sğ‘¡

ğ‘›, ğ’‚ğ‘¡

ğ‘›; ğœƒğ‘›)}; ğœƒâˆ’

ğ‘›),

(18)

where 0 â‰¤ ğ›¾ â‰¤ 1 is the discount factor.

To update the parameters ğœƒğ‘› of the main Q-network, MEC agent ğ‘› performs a gradient descent step. Finally,

each MEC agent updates the parameters ğœƒâˆ’

ğ‘› of its target Q-network at a ï¬xed target step by copying the parameters

of the main Q-network.

C. The Inference Phase of DQL

The inference phase of DQL is presented in Algorithm 2. Once the trained DDQNs are obtained, each MEC

agent uses its optimal DDQN parameters to ï¬nd an appropriate placement of the requested service by the vehicles.
In detail, at the beginning of each episode the environment of each MEC agent is built. Then, for each step ğ‘¡,

each MEC agent observes the current state of its environment and selects an action that maximizes its Q-value

according to its trained DDQN. Based on the selected actions of all MEC agents, the SDN controller ï¬nds the

overall communication delay, computing delay and migration costs and thus we obtain a solution to problem (P1).

IV. SIMULATION RESULTS

We consider a MEC-enabled vehicular network where three gNBs that are attached to three MEC servers are

deployed over a highway as shown in Fig. 1. The gNBs are located randomly along the highway. We assume that

the three MEC servers are deployed along the highway in a triangular fashion as depicted in Fig. 1, where the

distance between MEC 1 and MEC 2 and the distance between MEC 2 and MEC 3 is equal to 2000 m and the

distance between MEC 1 and MEC 3 is equal to 4000 m. The vehicles are drawn randomly in the highway that is

modelled as a rectangle of length 5000 m and width 18 m with two forward lanes and two backward lanes. The
vehicles move with a randomly-chosen ï¬xed speed from the range of [60, 110] km/h and once a vehicle reaches the

boundary of the highway, it reappears in the opposite side. For simplicity, all vehicles keep moving with constant

speeds with acceleration, i.e., once the random speed of a vehicle is chosen, the latter keeps moving with that speed

during the entire simulation period.

The proposed multi-agent DQL algorithm is trained on an computer with an Intel Core i7-10750H CPU, 16GB

RAM and an nVidia GeForce GTX 2070 Super graphic card. The implementation is performed using Python

and PyTorch. After performing hyper-parameters tuning, the following optimized parameters are set. Each DDQN

consists of fully connected hidden neural network with two hidden layers of 256 neurons each. The discount factor is
ğ›¾ = 0.99. The other DDQN and vehicular network para metes are presented in Table 1. To avoid the overestimation

problem of the Q-value, the parameters of each DDQN network are copied into the parameters of the corresponding

target DDQN every 1000 steps. According to the state of the art of deep learning models, Rectiï¬ed Linear Unit

(ReLU) function accelerates the learning process since it is not vanishing gradient.

Fig. 2 illustrates the average reward per episode of one MEC agent. It is clear that the reward improves with

the training episodes as it increases when the number of episodes increases. This shows the effectiveness of the

proposed DQL algorithm. We notice that the DQL algorithm converges at approximately 1000 episodes. In other

words, the corresponding MEC agent converges to a good learning outcome, which implies that it will explore

better actions. We can notice though that the reward converges while incurring large ï¬‚uctuations which is mainly

due to the high mobility scenario of the vehicular network.

Fig. 3 and Fig. 4 illustrate the average cost represented by the objective function (P1a) which measures the

total service latency (the computing and communication latency) as well as the migration costs under two different

conï¬gurations. The ï¬rst conï¬guration is the computational power conï¬guration and it consists of varying the

number of cores for the three MEC servers. We considered three MEC servers with 4 cores each, or 8 cores
each, or 16 cores each, or 32 cores each, or 64 cores each, with a ï¬xed clock frequency of 2.5 GHz. The

second conï¬guration is the request size conï¬guration and it consists of varying the vehiclesâ€™ request sizes, which
we generate uniformly at random within a ï¬xed interval as follows UNIFORM(50, 100), UNIFORM(100, 150),
UNIFORM(150, 200), UNIFORM(200, 250), UNIFORM(250, 300) Kbits.

Fig. 3 shows the objective function while considering the computational power conï¬guration. We can notice that

with increasing the computational power, the average service latency as well as the migration costs are decreasing.

Regardless of different number of cores, the proposed DQL approach performs close-to-the-optimal performance.

TABLE I: Simulation parameters

Parameter

Value

Number of MEC servers

3

Transmit power of each gNB

30 dBm

Migration cost

UNIFORM (0.2, 0.3)

Number of vehicles

4

Request size

UNIFORM (50, 300) Kbits

Noise variance

âˆ’174 dBm/Hz

Bandwidth

Learning rate

Number of episodes

Discount factor

10 MHz

3ğ‘’ âˆ’ 4

3000

0.99

Replay memory size

100000

Mini-batch size

Target update interval

1024

1000

Loss function

Minimum square error

Optimizer

Activation function

Adam

ReLU

Fig. 4 presents the objective function while considering the request size conï¬guration. The proposed approach is

shown to approximate well the optimal solution in different scenarios of the request size conï¬guration. The smaller

the request size is, the smaller the total delay and the migration costs are. This is because (i) the DQL approach

learns efï¬ciently the appropriate placement of each vehicleâ€™s service, which helps in reducing the service latency

and (ii) a small number of bits can be fulï¬lled easily by one MEC server without requiring to migrate to another

MEC server, which helps in reducing the total migration costs. We notice that as the request sizes increase, the

average service delay and the migration costs increase as well.

V. RELATED WORKS

The authors in [26] propose a quality-of-experience (QoE)-aware scheme to ensure service continuity for mobile

cloud computing environment. The scheme relays on the buffer-occupancy threshold policy that classiï¬es the new

arriving request from the mobile users. The proposed scheme protects the migrated service from trafï¬c ï¬‚uctuation.

In addition, the cloud server can change the buffer threshold dynamically for different categories of requests. In

[18], the authors proposed Follow-Me Chain algorithm to solve the problem of service function chaining (SFC). In

particular, the work studied the problem of inter-MEC handoffs to offer a higher satisfaction for users in mobility

scenarios. Such problem is NP-hard, and authors proposed an integer programming formulation that is solved by

Fig. 2: The training rewards for MEC server.

the Follow-Me Chain algorithm. The work in [27] investigated the relocation problem of virtual network functions

(VNF) within a cloud infrastructure under mobility and resource heterogeneity constraints. The authors studied in

particular the impact of the relocation operation on the service delay and the number of VNF relocations (i.e.,

the number of time that a single VNF is being moved from a cloud to another). The problem of relocation was

formulated as a mized integer linear programming problem and solved through a meta-heuristic approach, namely,

the ant colony optimization technique. Within the same context, the authors in [28] proposed an evaluation of three

container-based schemes for VNF migration as a mechanism to guarantee the service continuity. In particular, the

schemes consider two cases of mobility patterns, respectively, known a priori and unknown mobility patterns. For

the known a priori pattern, temporary ï¬le system and disk-less-based migration are discussed, but the main focus

was on the unknown mobility pattern, where authors proposed a solution that consists in storing the containerâ€™s

ï¬le system within the system images in a shared pool. The work in [29] considered two main logical slices

created over the same infrastructure, namely, an autonomous driving slice for safety messages, and an infotainment

slice. The authors proposed a clustering method to partition vehicles to allocate slice leaders on each cluster. A

slice leader is a serving entity using vehicle-to-vehicle (V2V) links to forward safety messages, subsequently the

road side units (RSU) forward the infotainment service using the vehicle-to-infrastructure (V2I) links. In [30], the

authors proposed an ofï¬‚ine RL-based RAN slicing solution and a low-complexity heuristic algorithm, to satisfy

communication resources requirements of different slices with the aim to maximize the resource utilization. The

proposed approach ensures the resource availability to meet the different requirements of the sliceâ€™s trafï¬c. The

authors assume that V2V communications are either in cellular (through gNBs) or in sidelink mode (through PC5

communication). In addition, in the sidelink mode each vehicle can multicast to multiple vehicles within the same

cluster. Finally, the proposed RL approach is executed separately for each communication mode (i.e., uplink and

downlink), which means that the RL is being executed twice.

Most of the literature studied hereabove, focus on the resource provisioning at the MEC sides independently with

no consideration to the services migration problem in vehicular network, where factors such such as the mobility

patterns, the services migration costs, and the services requirements need to be considered for a more efï¬cient

service placement schemes. Further, previous works consider only heuristic or meta-heuristic methods that focus

on solving the placement problem in order to minimize only the latency without studying the cost of migration.

The works that considered the service latency as well as the migration costs leverage simple algorithmic solutions

without considering advanced machine learning approaches such as the one proposed in this paper. In this paper, we

ï¬ll these gaps and we propose a service migration scheme based on DRL techniques in a MEC-enabled vehicular

network aiming to minimize the total service latency and migration cost.

VI. CONCLUSION

In this paper, we proposed a DRL-based scheme to solve the problem of vehicular service placement and migration

in a MEC-based vehicular network. First, we formulated the problem as a nonlinear integer optimization problem to

minimize the total latency (i.e., communication and computational delays ) plus migration costs in terms of energy

consumption and bandwidth usage. To solve the optimization problem we used standard solvers such as CPLEX,

and we linearize it to transform it into a linear integer optimization problem. Then, we formulate the problem

as a multi-agent Markov decision process and develop a DRL-based method by exploiting the DQL algorithm to

obtain an efï¬cient and non-complex solution. The DQL algorithm uses double DQN and replay memory strategies

to increase the learning accuracy and solve the Q-value overestimation problem. Finally, we have demonstrated

through extensive simulations that the proposed DQL algorithm achieves near-optimal performance compared to

the CPLEX solution.

The authors would like to thank the Natural Sciences and Engineering Research Council of Canada, for the

ï¬nancial support of this research.

ACKNOWLEDGMENT

REFERENCES

[1] A. Rachedi et al., â€œIeee access special section editorial: The plethora of research in internet of things (iot),â€ IEEE Access, vol. 4, pp.

9575â€“9579, 2016.

[2] L. Yala et al., â€œLatency and Availability Driven VNF Placement in a MEC-NFV Environment,â€ in Proc. IEEE Global Commun. Conf.

(GLOBECOM), 2018, pp. 1â€“7.

[3] A. Alalewi et al., â€œOn 5g-v2x use cases and enabling technologies: A comprehensive survey,â€ IEEE Access, vol. 9, pp. 107 710â€“107 737,

2021.

[4] A. Triwinarko et al., â€œPhy layer enhancements for next generation v2x communication,â€ Vehicular Communications, vol. 32, p. 100385,

2021.

[5] M. Azizian et al., â€œAn optimized ï¬‚ow allocation in vehicular cloud,â€ IEEE Access, vol. 4, pp. 6766â€“6779, 2016.

[6] S. Wang et al., â€œAn Investigation Into the Use of Virtual Reality Technology for Passenger Infotainment in a Vehicular Environment,â€ in

Proc. IEEE Int. Conf. Adv. Mater. Sci. Eng. (ICAMSE), 2016, pp. 404â€“407.

[7] H. Khan et al., â€œEnhancing Video Streaming in Vehicular Networks via Resource Slicing,â€ IEEE Trans. Veh. Technol., vol. 69, no. 4, pp.

3513â€“3522, 2020.

[8] M. Azizian et al., â€œVehicle software updates distribution with sdn and cloud computing,â€ IEEE Communications Magazine, vol. 55, no. 8,

pp. 74â€“79, 2017.

[9] A. Abouaomar et al., â€œResource provisioning in edge computing for latency-sensitive applications,â€ IEEE Internet of Things Journal,

vol. 8, no. 14, pp. 11 088â€“11 099, 2021.

[10] A. Filali et al., â€œPreemptive SDN Load Balancing with Machine Learning for Delay Sensitive Applications,â€ IEEE Trans. Veh. Technol.,

vol. 69, no. 12, pp. 15 947â€“15 963, 2020.

[11] P. A. Frangoudis et al., â€œService migration versus service replication in multi-access edge computing,â€ in 2018 14th International Wireless

Communications Mobile Computing Conference (IWCMC), 2018, pp. 124â€“129.

[12] Z. Mlika et al., â€œNetwork Slicing with MEC and Deep Reinforcement Learning for the Internet of Vehicles,â€ IEEE Network, pp. 1â€“7,

2021, Early Access.

[13] A. Abouaomar et al., â€œService Function Chaining in MEC: A Mean-Field Game and Reinforcement Learning Approach,â€ 2021.

[14] A. Abouaomar et al., â€œMean-Field Game and Reinforcement Learning MEC Resource Provisioning for SFC,â€ in IEEE GLOBECOM

Conference, 2021, pp. 1â€“6.

[15] A. Aissioui et al., â€œOn enabling 5g automotive systems using follow me edge-cloud concept,â€ IEEE Transactions on Vehicular Technology,

vol. 67, no. 6, pp. 5302â€“5316, 2018.

[16] T. Ouyang et al., â€œAdaptive User-Managed Service Placement for Mobile Edge Computing: An Online Learning Approach,â€ in Proc. IEEE

Conf. Comput. Commun. (INFOCOM), 2019, pp. 1468â€“1476.

[17] Z. Ennya et al., â€œComputing tasks distribution in fog computing: Coalition game model,â€ in 2018 6th International Conference on Wireless

Networks and Mobile Communications (WINCOM), 2018, pp. 1â€“4.

[18] T. Ouyang et al., â€œFollow Me at the Edge: Mobility-Aware Dynamic Service Placement for Mobile Edge Computing,â€ IEEE J. Sel. Areas

Commun., vol. 36, no. 10, pp. 2333â€“2345, 2018.

[19] A. Tak et al., â€œFederated edge learning: Design issues and challenges,â€ IEEE Network, vol. 35, no. 2, pp. 252â€“258, 2021.

[20] A. Abouaomar et al., â€œA resources representation for resource allocation in fog computing networks,â€ in 2019 IEEE Global Communications

Conference (GLOBECOM), 2019, pp. 1â€“6.

[21] R. Urgaonkar et al., â€œDynamic Service Migration and Workload Scheduling in Edge-Clouds,â€ Performance Evaluation, vol. 91, pp. 205â€“228,

2015.

[22] A. Abouaomar et al., â€œMatching-Game for User-Fog Assignment,â€ in Proc. IEEE Global Commun. Conf. (GLOBECOM), 2018, pp. 1â€“6.

[23] Z. Mlika et al., â€œUserâ€“Base-Station Association in HetSNets: Complexity and Efï¬cient Algorithms,â€ IEEE Trans. Veh. Technol., vol. 66,

no. 2, pp. 1484â€“1495, 2017.

[24] V. Mnih et al., â€œHuman-Level Control Through Deep Reinforcement Learning,â€ Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.

[25] H. Van Hasselt et al., â€œDeep Reinforcement Learning with Double Q-Learning,â€ in Proc. AAAI Conf. Artif. Intell., vol. 30, no. 1, 2016.

[26] Y.-R. Haung, â€œA QoE-Aware Strategy for Supporting Service Continuity in an MCC Environment,â€ Wireless Pers. Commun., vol. 116,

no. 1, pp. 629â€“654, 2021.

[27] P. Roy et al., â€œUser Mobility and Quality-of-Experience Aware Placement of Virtual Network Functions in 5G,â€ Comput. Commun., vol.

150, pp. 367â€“377, 2020.

[28] R. A. Addad et al., â€œTowards a Fast Service Migration in 5G,â€ in Proc. IEEE Conf. Standards Commun. Netw. (CSCN), 2018, pp. 1â€“6.

[29] H. Khan et al., â€œNetwork Slicing for Vehicular Communication,â€ Trans. Emerg. Telecommun. Technol., vol. 32, no. 1, p. e3652, 2021.

[30] H. D. R. Albonda et al., â€œAn Efï¬cient RAN Slicing Strategy for a Heterogeneous Network With eMBB and V2X Services,â€ IEEE Access,

vol. 7, pp. 44 771â€“44 782, 2019.

Fig. 3: The objective function vs. the computational power of the MEC servers.

Fig. 4: The objective function vs. the request sizes of the vehicles.

