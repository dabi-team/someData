1
2
0
2

g
u
A
4

]

G
L
.
s
c
[

1
v
5
9
8
1
0
.
8
0
1
2
:
v
i
X
r
a

HIGH PERFORMANCE ACROSS TWO ATARI PADDLE GAMES
USING THE SAME PERCEPTUAL CONTROL ARCHITECTURE
WITHOUT TRAINING

A PREPRINT

Tauseef Gulrez
Department of Computing and Research,
Syscon Australia Pty. Ltd.
Melbourne, VIC, Australia,
gtauseef@ieee.org

Warren Mansell
Division of Psychology and Mental Health,
School of Health Sciences,
University of Manchester, UK,
warren.mansell@manchester.ac.uk

August 5, 2021

ABSTRACT

Deep reinforcement learning (DRL) requires large samples and a long training time to operate
optimally. Yet humans rarely require long periods training to perform well on novel tasks, such as
computer games, once they are provided with an accurate program of instructions. We used perceptual
control theory (PCT) to construct a simple closed-loop model which requires no training samples
and training time within a video game study using the Arcade Learning Environment (ALE). The
model was programmed to parse inputs from the environment into hierarchically organised perceptual
signals, and it computed a dynamic error signal by subtracting the incoming signal for each perceptual
variable from a reference signal to drive output signals to reduce this error. We tested the same model
across two different Atari paddle games Breakout and Pong to achieve performance at least as high
as DRL paradigms, and close to good human performance. Our study shows that perceptual control
models, based on simple assumptions, can perform well without learning. We conclude by specifying
a parsimonious role of learning that may be more similar to psychological functioning.

Keywords Perceptual control theory · Atari games · Machine learning · Artiﬁcial agent.

1

Introduction

Gaming environments are increasingly used to develop artiﬁcial intelligence. In order to play in a gaming environment,
deep reinforcement learning (DRL) agents require a long training time to learn and execute commands. The most
successful model-free DRL agents DQN [21], A3C [20], and Rainbow [13] which achieved human-level of performance
on Atari games, go through a trial-and-error training process for 10 days (approx. 20 Million steps). Moreover, attempts
have been made at developing self-playing agents for Atari games [7, 16], but none of them were able to achieve high
performance. More recently, MuZero agent [31] shows that planning can achieve high performance on Atari games but
it requires an extensive engineering cost involving a high computational budget. MuZero agent requires more than 2
months of training to train one agent. In this letter, we propose a new gaming Agent called PCTagent. It is arguably the
ﬁrst agent to achieve human-level performance on the Atari ball and paddle games by exhibiting a control architecture
similar to the living organisms.

1.1 Perceptual Control Theory Modelling

Perceptual Control Theory (PCT) is a computational framework [26–29] to explain and model the behavior of living
organisms based on control engineering. Within a PCT system, output (efferent) signals from each control unit
within one level of a hierarchy set the goal state for input (afferent) signals at the level below. This allows actions
to vary dynamically to control input ’on the ﬂy’, so that the planning and learning of actions is unnecessary in many

 
 
 
 
 
 
High Performance on Atari using PCTagent

A PREPRINT

(a)

(b)

Figure 1: OpenAI Gym’s Atari game environments. (a) Breakout and (b) Pong’s rgb frames.

circumstances. A simple, but ’correct’, architecture is necessary to achieve a good level of performance. The nervous
system of a living organism requires the input functions to construct perceptual signals from the environment, and to
organise these with respect to one another. This architecture may have biologically prepared foundations [25], and be
further organised through verbal instruction within humans [6]; both of these pathways bypass long periods of training.

A variety of PCT-based computational models have been designed that do not require training, which have emulated
naturalistic behaviour within animals [4], human manual tracking [24], human crowd behaviour [19], ﬂyball catch-
ing [18] and robotic devices [37], [3]. However, there is little published evidence using established benchmarks within
commonly used hardware or modelling environments to compare with approaches that do require training for the
learning of actions. This was the aim of the current study.

1.2 Existing AI Methods for Arcade Games

Atari arcade games have been benchmarked primarily using model-free RL algorithms. DQN [21] utilises deep neural
network (DNN) to train Q-learning policies by incorporating replay experience and target networks. Several attempts
have been made to extend DQN by incorporating bias correction, e.g. DDQN [33], and by prioritising experience
replay [30] by architectural modiﬁcations [34], and distributional value learning [8]. Some attempts have been made to
improve performance by data collection, which increases the cost of environment steps beyond 200 million [2, 20].
Agents developed on proprioceptive inputs [12, 14], model images without using them for planning [1, 7, 23], or
combine the beneﬁts of model-based and model-free approaches [17, 22]. Most model-based agents with image inputs
have thus far been limited to relatively simple control tasks [10, 35].

SimPLe agent [16] learns a video and predicts a model in pixelated data format and utilises its predictions to train
a proximal policy agent [32]. The model tracks and establishes prediction based on four consecutive frames and
incorporates discrete latent variable as an input. The authors evaluate SimPLe on a subset of Atari games for 400k
and 2M environment steps, after which the rewards decreased, hence the model over-ﬁtted the environment. As a
direct contrast to the above designs, we have used perceptual control theory as a computational framework to show a
competitive performance with no training. PCTagent was built on a single GPU Core i7 laptop, for the paddle operated
game environment, within which it outperforms top single-GPU Atari agents Rainbow [13] and IQN [8] which rest
upon years of model-free RL research [9].

2

High Performance on Atari using PCTagent

A PREPRINT

Table 1: Ball and Paddle games results of different ML Agents

Agent Name

DQN
DBA3C
SimPLe
RADAR
Rainbow
IQN
DreamerV2
Human World record
Random
PCTagent (Ours)
*Frameskip version

Best Score
Breakout Pong
401
301
16.4
600+
120
79
312
864
2
862

21
21
5.2
21
21
21
21
21
-20
18*

Training Frames

10 million
100 million
4 million
200 million
200 million
200 million
200 million
-
-
0

2 Method

2.1 A PCT Agent and Model

PCTagent was developed based on four sub-system hierarchical model of behavior, as shown in Fig. 2 and is available
to download under GPL 1

The design of a PCT agent is based on a systematic logical analysis of the sensors and effectors of a system, and the
performance requirements of the task, that can be speciﬁed as an algorithm [11]. Within the Breakout scenario, the
sensor can estimate the distance between the paddle and ball, which needs to be controlled at zero to perform the task.
Yet the effector only commands the rate of button press left or right. Therefore, a hierarchy was constructed that bridges
between the control of button press at the bottom level and the control of paddle-ball distance at the top level. Two
intermediate levels were required. The full architecture can therefore be described as follows. The top level of PCTagent
controls the visual perception of the distance (D) to be maintained between the paddle and a moveable ball in the game
environment, as shown in Fig. 1(a)(b). Within Fig. 2, the error (e1) in the top sub-system sets the reference value (R2)
for the direction control left or right of the paddle, which is compared to the sensed direction of the paddle (MD), to
generate an error (e2) that sets the reference value (R3) for the next level down. The next level down perceives the the
position of the paddle (Px) and compares this to the reference value for movement (R3). Error in this system (e3) sets
the reference value (R4) for the left or right button press (BP ), and in turn the error of this system (e4) is transformed
to a frequency of button press which determines how far left or right user has to move to hit the ball. An inherent
embedded property of button press sub-system is the control of the velocity at which the paddle has to move to catch
the ball. Within this sub-system, rate of button press is counted by an integrator, which has a limit (currently set to
3), upon reaching max, the integrator resets and reverts to its previous position. Each level of the model contains a k
parameter which is the gain of each unit. The gain values for this PCT agent were all set to the value of 1 because this
required no a priori assumptions regarding their optimal value.

2.2 ALE Environment and Image Processing

We used the Arcade Learning Environment (ALE) as a platform to empirically assess the performance of our PCTagent.
ALE provides an interface for different Atari 2600 game environments which are challenging and engaging for humans.
ALE serves as a benchmark environment for the evaluation of AI agents. As an input to our PCTagent we obtained
raw Atari frames, which are 210X160 (height,width) pixel image with a 128 color representation. To make the game
computationally efﬁcient, we reduced the input matrix dimensionality by ﬁrst converting it to binary and then cropping
it to obtain a 100X132 region, capturing only the playing area. We do not require any speciﬁc square size images,
mostly required by 2D convolution algorithms. The tracking of the ball and the paddle were done using simple contour
detection function of OpenCV2 module running under Python 3.8.

3 Atari Paddle Games Experiments

First we present our PCTagent’s performance results and a comparison of this performance with other state-of-the-art
agent as tabulated in Table-1. In Table-1, it can be seen that all of the machine learning game playing agents require

1https://github.com/PCT-Models/PCTagent_Breakout_Atari

3

High Performance on Atari using PCTagent

A PREPRINT

Figure 2: Hierarchical PCT model for ball and paddle Atari Games.

millions of frames for training for weeks, whereas since PCTagent does not require any training, hence was ready to
play instantly. PCTagent’s highest score for Breakout and Pong was 862 and 18, respectively.

Paddle games e.g. Breakout and Pong were used to evaluate the performance of our training free PCTagent. The
games and PCTagent were run on an Intel Core-i7 single GPU machine with AMD Radeon RX 640 graphics card.
Both Pong and Breakout were tested for different Open Gym environment modes i.e. with/without frame skipping
and deterministic mode. The games were ran for 500 episodes and scores data were collected as shown in Fig. 3. In
Breakout, a negative reward of -1 for each life lost and in Pong a -1 reward for opponent’s successful score was set. It is
evident from the results (shown in Fig. 3) that in Breakout it is possible to achieve an average highest score of 400+ and
in Pong to win the game with a margin of 5-6 points, at no prior training costs. In Pong, the noframeskip mode resulted
in all wins - scores of 21, hence frameskip mode was used during data collection. A signiﬁcant noise was introduced
in the PCTagent when the game were stuck in one position. In the case of Breakout this usually happened in the end
(noframeskip mode) when one or two bricks were left to hit, which resulted most of the time in losing a life.

4 Conclusion

We produced one controller based on PCT and tested it, without training, on two different Atari paddle games. Its
performance matched or exceeded that of published benchmarks from existing reinforcement and deep learning models.
These ﬁndings complement earlier studies demonstrating the high performance of PCT controllers within robotics and

4

High Performance on Atari using PCTagent

A PREPRINT

(a)

(b)

Figure 3: Histogram score representation of the 500 episodes of (a) Pong and (b) Breakout games.

other areas of research [37], [3]. Indeed, the ﬁndings are particularly consistent with an earlier comparison between
a PCT controller and an LQR controller for an inverted pendulum robot [15]. This PCT controller required minimal
tuning and its performance metrics were superior.

PCT controllers can provide a robust control solution across environments with no training because they have no need
to learn their actions. Instead, they achieve control by varying their outputs on-the-ﬂy to control their inputs by acting
against unpredictable disturbances (e.g. turbulence, rough ground), unlike reinforcement models. The closed-loop PCT
design emulates control systems in nature, which also do not need to learn speciﬁc behaviours to operate effectively and
efﬁciently [36]. The choice of input speciﬁcations and their hierarchical organisation can be made through a systematic
analysis of the sensors, effectors and the task requirements rather than through learning, or inferences made by the
researcher [11]. This begs the question of whether reinforcement learning accounts for human skills or whether PCT
provides a more accurate model - an architecture of ’priors’ proposed to forge future advances in AI [5]. Importantly,
PCT controllers can further improve performance through training if required. The learning algorithm speciﬁed in PCT
- reorganisation - uses random-walk learning to optimise the parameters (e.g. gains) and functions (e.g. speciﬁcation

5

High Performance on Atari using PCTagent

A PREPRINT

of inputs that co-vary with target velocity) within a PCT architecture. This is a future direction for computational
modelling using PCT.

References

[1] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic

variational video prediction. arXiv preprint arXiv:1710.11252, 2017.

[2] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel
Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International Conference on
Machine Learning, pages 507–517. PMLR, 2020.

[3] Joseph W. Barter and Henry H. Yin. Achieving natural behavior in a robot using neurally inspired hierarchical

control. bioRxiv, 2021.

[4] Heather C Bell, Greg D Bell, Jeffrey A Schank, and Sergio M Pellis. Evolving the tactics of play ﬁghting: Insights

from simulating the “keep away game” in rats. Adaptive Behavior, 23(6):371–380, 2015.

[5] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai. Communications of the ACM, 64(7):58–

65, 2021.

[6] Carla Brown-Ojeda and Warren Mansell. Do perceptual instructions lead to enhanced performance relative to

behavioral instructions? Journal of motor behavior, 50(3):312–320, 2018.

[7] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators.

arXiv preprint arXiv:1704.02254, 2017.

[8] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforcement learning with

quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.

[9] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad
Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint
arXiv:1706.10295, 2017.

[10] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by

latent imagination. arXiv preprint arXiv:1912.01603, 2019.

[11] Benjamin Hawker and Roger K Moore. Robots producing their own hierarchies with dosa; the dependency-oriented

structure architect. UK-Robotics and Autonomous Systems (RAS) Network, pages 66–68, 2020.

[12] Mikael Henaff, William F Whitney, and Yann LeCun. Model-based planning with discrete and continuous actions.

arXiv preprint arXiv:1705.07177, 2017.

[13] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal
Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.

[14] Juan Camilo Gamboa Higuera, David Meger, and Gregory Dudek. Synthesizing neural network controllers with
probabilistic model-based reinforcement learning. In 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 2538–2544. IEEE, 2018.

[15] Thomas Johnson, Zhou Siteng, Wei Cheah, Warren Mansell, Rupert Young, and Simon Watson. Implementation of
a perceptual controller for an inverted pendulum robot. Journal of Intelligent & Robotic Systems, 99(3-4):683–692,
2020.

[16] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,
Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for
atari. arXiv preprint arXiv:1903.00374, 2019.

[17] Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep reinforcement

learning. In Conference on Robot Learning, pages 195–206. PMLR, 2017.

[18] Richard S Marken. Optical trajectories and the informational basis of ﬂy ball catching. 2005.

[19] Clark McPhail, William T Powers, and Charles W Tucker. Simulating individual and collective action in temporary

gatherings. Social Science Computer Review, 10(1):1–28, 1992.

[20] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David
Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International
conference on machine learning, pages 1928–1937. PMLR, 2016.

6

High Performance on Atari using PCTagent

A PREPRINT

[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. nature, 518(7540):529–533, 2015.

[22] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-
based deep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pages 7559–7566. IEEE, 2018.

[23] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction

using deep networks in atari games. arXiv preprint arXiv:1507.08750, 2015.

[24] Maximilian G Parker, Andrew BS Willett, Sarah F Tyson, Andrew P Weightman, and Warren Mansell. A systematic
evaluation of the evidence for perceptual control theory in tracking studies. Neuroscience & Biobehavioral Reviews,
112:616–633, 2020.

[25] Frans X Plooij. The phylogeny, ontogeny, causation and function of regression periods explained by reorganizations
of the hierarchy of perceptual control systems. In The Interdisciplinary Handbook of Perceptual Control Theory,
pages 199–225. Elsevier, 2020.

[26] W. T. Powers. Behavior: The control of perception. Aldine Chicago, 1973.
[27] William T Powers, RK Clark, and RL McFarland. A general feedback theory of human behavior: Part ii.

Perceptual and Motor Skills, 11(3):309–323, 1960.

[28] William Treval Powers. Living control systems iii: The fact of control. 2008.
[29] William Treval Powers, Robert K Clark, and RL Mc Farland. A general feedback theory of human behavior: Part

i. Perceptual and motor skills, 11(1):71–88, 1960.

[30] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint

arXiv:1511.05952, 2015.

[31] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur
Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning
with a learned model. Nature, 588(7839):604–609, 2020.

[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347, 2017.

[33] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In

Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016.

[34] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network
architectures for deep reinforcement learning. In International conference on machine learning, pages 1995–2003.
PMLR, 2016.

[35] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally

linear latent dynamics model for control from raw images. arXiv preprint arXiv:1506.07365, 2015.

[36] Henry Yin. The crisis in neuroscience. In The Interdisciplinary Handbook of Perceptual Control Theory, pages

23–48. Elsevier, 2020.

[37] Rupert Young. A general architecture for robotics systems: A perception-based approach to artiﬁcial life. Artiﬁcial

life, 23(2):236–286, 2017.

7

