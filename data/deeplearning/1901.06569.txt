9
1
0
2

n
a
J

9
1

]

G
L
.
s
c
[

1
v
9
6
5
6
0
.
1
0
9
1
:
v
i
X
r
a

Learning retrosynthetic planning through self-play

,
John S. Schreck,*

â€  Connor W. Coley,â€¡ and Kyle J. M. Bishop*

,
â€ 

Department of Chemical Engineering, Columbia University, New York, NY 10027, USA

â€ 
Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge,

â€¡

MA 02139, USA

E-mail: jsschreck@gmail.com; kyle.bishop@columbia.edu

Abstract

The problem of retrosynthetic planning can be framed as one player game, in

which the chemist (or a computer program) works backwards from a molecular target

to simpler starting materials though a series of choices regarding which reactions

to perform. This game is challenging as the combinatorial space of possible choices

is astronomical, and the value of each choice remains uncertain until the synthesis

plan is completed and its cost evaluated. Here, we address this problem using deep

reinforcement learning to identify policies that make (near) optimal reaction choices

during each step of retrosynthetic planning. Using simulated experience or self-play,

we train neural networks to estimate the expected synthesis cost or value of any given

molecule based on a representation of its molecular structure. We show that learned

policies based on this value network outperform heuristic approaches in synthesizing

unfamiliar molecules from available starting materials using the fewest number of

reactions. We discuss how the learned policies described here can be incorporated

into existing synthesis planning tools and how they can be adapted to changes in the

synthesis cost objective or material availability.

1

 
 
 
 
 
 
Introduction

The primary goal of computer-aided synthesis planning (CASP) is to help chemists accelerate

the synthesis of desired molecules. 1â€“3 Generally, a CASP program takes as input the structure

of a target molecule and returns a sequence of feasible reactions linking the target to

commercially available starting materials. The number of possible synthesis plans is often

astronomical, and it is therefore desirable to identify the plan(s) that minimize some user-

speciï¬ed objective function (e.g., synthesis cost ğ¶). The challenge of identifying these optimal

syntheses can be framed as a one-player gameâ€”the retrosynthesis gameâ€”to allow for useful

analogies with chess and Go, for which powerful solutions based on deep reinforcement

learning now exist. 4,5 During play, the chemist starts from the target molecule and identiï¬es

a set of candidate reactions by which to make the target in one step (Figure 1). At this

point, the chemist must decide which reaction to choose. As in other games such as chess,

the beneï¬ts of a particular decision may not be immediately obvious. Only when the game is

won or lost can one fairly assess the value of decisions that contributed to the outcome. Once

a reaction is selected, its reactant(s) become the new target(s) of successive retrosynthetic

analyses. This branching recursive process of identifying candidate reactions and deciding

which to use continues until the growing synthesis tree reaches the available substrates (a

â€œwinâ€) or it exceeds a speciï¬ed number of synthetic steps (a â€œlossâ€).

Winning outcomes are further distinguished by the cost ğ¶ of the synthesis pathway

identiï¬edâ€”the lower the better. This synthesis cost is often ambiguous and diï¬ƒcult to

evaluate as it involves a variety of uncertain or unknown quantities. For example, the

synthesis cost might include the price of the starting materials, the number of synthetic

steps, the yield of each step, the ease of product separation and puriï¬cation, the amount of

chemical waste generated, the safety or environmental hazards associated with the reactions

and reagents, et cetera. It is arguably more challenging to accurately evaluate the cost of

a proposed synthesis than it is to generate candidate syntheses. It is therefore common to

adopt simple objective functions that make use of the information available (e.g., the number

2

of reactions but not their respective yields). 6 We refer to the output of any such function as

the cost of the synthesis; optimal synthesis plans correspond to those with minimal cost.

An expert chemist excels at the retrosynthesis game for two reasons: (1) she can identify

a large number of feasible reaction candidates at each step, and (2) she can select those

candidates most likely to lead to winning syntheses. These abilities derive from the chemistâ€™s

prior knowledge and her past experience in making molecules. In contrast to games with a

ï¬xed rule set like chess, the identiï¬cation of feasible reaction candidates (i.e., the possible

â€œmovesâ€) is nontrivial: there may be thousands of possible candidates at each step using known

chemistries. To address this challenge, computational approaches have been developed to

suggest candidate reactions using libraries of reaction templates prepared by expert chemists 7

or derived from literature data. 8â€“10 Armed with these â€œrulesâ€ of synthetic chemistry, a

computer can, in principle, search the entire space of possible synthesis pathways and identify

the optimal one.

In practice, however, an exhaustive search of possible synthesis trees is not computationally

feasible or desirable owing to the exponential growth in the number of reactions with distance

from the target. 6,11 Instead, search algorithms generate a subset of possible synthesis trees,

which may or may not contain the optimal pathway(s). For longer syntheses, the subset of

pathways identiï¬ed is an increasingly small fraction of the total available. Thus, it is essential

to bias retrosynthetic search algorithms towards those regions of synthesis space most likely

to contain the optimal pathway. In the game of retrosynthesis, the player requires a strong

guiding model, or policy, for selecting the reaction at each step that leads to the optimal

synthetic pathway(s).

Prior reports on retrosynthetic planning have explored a variety of policies for guiding the

generation of candidate syntheses. 7,12â€“14 These programs select among possible reactions using

heuristic scoring functions, 7,15 crowd-sourced accessibility scores, 13,16 analogy to precedent

reactions, 17 or parametric models (e.g., neural networks) trained on literature precedents. 18,19

In particular, the Syntaurus software 7 allows for user-speciï¬ed scoring functions that can

3

âˆˆ â„›

(ğ‘š0)

(ğ‘š0) (yellow) is selected according to a policy ğœ‹(ğ‘Ÿ0

Figure 1: The objective of the retrosynthesis game is to synthesize the target product ğ‘š0 from
available substrates by way of a synthesis tree that minimizes the cost function. Molecules
and reactions are illustrated by circles and squares, respectively. Starting from the target, a
ğ‘š0) that links ğ‘š0 with
reaction ğ‘Ÿ0
|
precursors ğ‘š1, ğ‘š2, ğ‘š3. The grey squares leading to ğ‘š0 illustrate the other potential reactions
in
ğ‘Ÿ0. The game continues one move at time reducing intermediate molecules (blue)
until there are only substrates remaining, or until a maximum depth of 10 is reached. Rare
molecules (green), which are not makeable under these constraints nor commercially available,
are assigned a cost penalty of 100, while molecules at maximum depth (purple) are assigned
a cost penalty of 10. Commercially available substrates are assigned zero cost. The synthesis
cost of the product may be computed according to Eq. 1 only on completion of the game.
Here, the sampled pathway leading to the target (red arrows) has a cost of 5.

â„›

âˆ–

.

4

501212ProductIntermediateSubstrateMax depthRareSelectedPossibleReactions,Chemical, mReaction r0 selected with probability1611111d = 0d = 1d = 2d = 3dmax = 100000010010describe common strategies used by expert chemists 20 (e.g., using symmetric disconnections

to favor convergent syntheses). By contrast, Segler and Waller used literature reaction data

to train a neural network that determines which reaction templates are most likely to be

eï¬€ective on a given molecule. 18 The ability to rank order candidate reactions (by any means)

allows for guiding network search algorithms (e.g., Monte Carlo tree search 19) to generate

large numbers of possible synthesis plans. The costs of these candidates can then be evaluated

to identify the â€œbestâ€ syntheses, which are provided to the chemist (or perhaps a robotic

synthesizer).

Here, we describe a diï¬€erent approach to retrosynthetic planning based on reinforcement

learning, 21 in which the computer learns to select those candidate reactions that lead ultimately

to synthesis plans minimizing a user-speciï¬ed cost function. Our approach is inspired by

the recent success of deep reinforcement learning in mastering combinatorial games such as

Go using experience generated by repeated self-play. 4,5 In this way, DeepMindâ€™s AlphaGo

Zero learned to estimate the value of any possible move from any state in the game, thereby

capturing the title of world champion. 4,22 Similarly, by repeated plays of the retrosynthesis

game, the computer can learn which candidate reactions are most likely to lead from a given

molecule to available starting materials in an optimal fashion. This approach requires no prior

knowledge of synthetic strategy beyond the â€œrulesâ€ governing single-step reactions encoded

in a library of reaction templates. Starting from a random policy, the computer explores the

synthetic space to generate estimates of the synthesis cost for any molecule. These estimates

form the basis for improved policies that guide the discovery of synthesis plans with lower

cost. This iterative process of policy improvement converges in time to optimal policies that

identify the â€œbestâ€ pathway in a single play of the retrosynthesis game. Importantly, we show

that (near) optimal policies trained on the synthesis of âˆ¼100,000 diverse molecules generalize

well to the synthesis of unfamiliar molecules. We discuss how the learned policies described

here can be incorporated into existing synthesis planning tools and how they can be adapted

to diï¬€erent cost functions that reï¬‚ect the changing demands of organic synthesis.

5

Results and Discussion

The Retrosynthesis Game

We formulate the problem of retrosynthetic analysis as a game played by a synthetic chemist

or a computer program. At the start of the game, the player is given a target molecule ğ‘š

to synthesize starting from a set of buyable molecules denoted

. For any such molecule,

â„¬
(ğ‘š), where each reaction includes the molecule ğ‘š

there exists a set of reactions, denoted

â„›

as a product. From this set, the player chooses a particular reaction ğ‘Ÿ

(ğ‘š) according

âˆˆ â„›

to a policy ğœ‹(ğ‘Ÿ

|

ğ‘š), which deï¬nes the probability of selecting that reaction for use in the

synthesis. The cost ğ‘rxn(ğ‘Ÿ) of performing the chosen reaction is added to a running total,

which ultimately determines the overall synthesis cost. Having completed one step of the

retrosynthesis game, the player considers the reactant(s) of the chosen reaction in turn. If a

reactant ğ‘šâ€² is included among the buyable substrates, ğ‘šâ€²

, then the cost of that molecule

âˆˆ â„¬

ğ‘sub(ğ‘šâ€²) is added to the running total. Otherwise, the reactant ğ‘šâ€² must be synthesized

following the same procedure outlined above. This recursive processes results in a synthesis

tree whose root is the target molecule and whose leaves are buyable substrates. The total

cost of the resulting synthesis is

ğ‘tot =

âˆ‘ï¸

ğ‘Ÿ

ğ‘rxn(ğ‘Ÿ) +

âˆ‘ï¸

ğ‘š

ğ‘sub(ğ‘š),

(1)

where the respective sums are evaluated over all reactions ğ‘Ÿ and all leaf molecules ğ‘š included

in the ï¬nal synthesis tree. This simple cost function neglects eï¬€ects due to interactions

between successive reactions (e.g., costs incurred in switching solvents); however, it has the

useful property that the expected cost ğ‘£ğœ‹(ğ‘š) of making any molecule ğ‘š in one step via

reaction ğ‘Ÿ is directly related to the expected cost of the associated reactants ğ‘šâ€²,

â¡

ğ‘£ğœ‹(ğ‘š) =

âˆ‘ï¸

ğ‘Ÿâˆˆâ„›(ğ‘š)

ğœ‹(ğ‘Ÿ

|

ğ‘š)

â£ğ‘rxn(ğ‘Ÿ) +

â¤

ğ‘£ğœ‹(ğ‘šâ€²)

â¦ .

âˆ‘ï¸

ğ‘šâ€²(ğ‘Ÿ)

(2)

6

This recursive function terminates at buyable molecules of known cost, for which ğ‘£ğœ‹(ğ‘š) =

ğ‘sub(ğ‘š) independent of the policy.

The function ğ‘£ğœ‹(ğ‘š) denotes the expected cost or â€œvalueâ€ of any molecule ğ‘š under a

speciï¬ed policy ğœ‹. By repeating the game many times starting from many target molecules,

it is possible to learn a parametric representation of this function valid for most molecules.

Importantly, knowledge of the value function under one policy ğœ‹ enables the creation of new

and better policies ğœ‹â€² that reduce the expected cost of synthesizing a molecule. Using methods

of reinforcement learning, such iterative improvement schemes lead to the identiï¬cation of

optimal policies ğœ‹*, which identify synthesis trees of minimal cost. The value of a molecule

under such a policy is equal to the expected cost of selecting the â€œbestâ€ reaction at each step

such that

â¡

ğ‘£*(ğ‘š) = min

ğ‘Ÿ

â£ğ‘rxn(ğ‘Ÿ) +

â¤

ğ‘£*(ğ‘šâ€²)

â¦ .

âˆ‘ï¸

ğ‘šâ€²(ğ‘Ÿ)

(3)

From a practical perspective, the optimal value function takes as input a molecule (e.g., a

representation of its molecular structure) and outputs a numeric value corresponding to the

minimal cost with which it can be synthesized.

Here, we considered a set of 100,000 target molecules selected from the Reaxys database on

the basis of their structural diversity (see Methods). The set of buyable substrates

contained

â„¬

âˆ¼300,000 molecules selected from the Sigma-Aldrich, 23 eMolecules, 24 and LabNetwork 25

catalogs that have list prices less than $100/g. At each step, the possible reactions

(ğ‘š) were

â„›

identiï¬ed using a set of 60,000 reaction templates derived from more than 12 million single-step

reaction examples reported in the Reaxys database (see Methods). Because the application

of reaction templates is computationally expensive, we used a template prioritizer to identify

those templates most relevant to a given molecule. 18 On average, this procedure resulted in

up to 50 possible reactions for each molecule encountered during synthesis planning. We

assume the space of molecules and reactions implicit in these transformations is representative

of real organic chemistry while recognizing the inevitable limitations of templates culled from

7

incomplete and sometimes inaccurate reaction databases. For simplicity, the cost of each

reaction step was set to one, ğ‘rxn(ğ‘Ÿ) = 1, and the substrate costs to zero, ğ‘sub(ğ‘š) = 0. With

these assignments, the cost of making a molecule is equivalent to the number of reactions in

the ï¬nal synthesis tree.

To prohibit the formation of unreasonably deep synthesis trees, we limited our retrosyn-

thetic searches to a maximum depth of ğ‘‘max = 10. As detailed in the Methods, the addition

of this termination criterion to the recursive deï¬nition of the value function (2) requires some

minor modiï¬cations to the retrosynthesis game. In particular, the expected cost of synthesiz-

ing a molecule ğ‘š depends also on the residual depth, ğ‘£ğœ‹ = ğ‘£ğœ‹(ğ‘š, ğ›¿), where ğ›¿ = ğ‘‘max

ğ‘‘ is the

âˆ’

diï¬€erence between the maximum depth and the current depth ğ‘‘ within the tree. If a molecule

ğ‘š not included among the buyable substrates is encountered at a residual depth of zero, it is

assigned a large cost ğ‘£ğœ‹(ğ‘š, 0) = ğ‘ƒ1, thereby penalizing the failed search. Additionally, in the

event that no reactions are identiï¬ed for a given molecule ğ‘š (

(ğ‘š) = âˆ…), we assign an even

â„›

larger penalty ğ‘ƒ2, which encourages the player to avoid such molecules if possible. Below, we

use the speciï¬c numeric penalties of ğ‘ƒ1 = 10 and ğ‘ƒ2 = 100 for all games.

Heuristic Policies

En route to the development of optimal policies for retrosynthesis, we ï¬rst consider the

performance of some heuristic policies that provide context for the results below. Arguably

the simplest policy is one of complete ignorance, in which the player selects a reaction at

random at each stage of the synthesisâ€”that is, ğœ‹(ğ‘Ÿ

ğ‘š) = constant. We use this â€œstraw manâ€

|

policy to describe the general process of policy evaluation and provide a baseline from which

to measure subsequent improvements.

During the evaluation process, the computer plays the retrosynthesis game to the end

making random moves at each step of the way. After each game, the cost of each molecule

in the resulting synthesis tree is computed. This process is repeated for each of the 100,000

target molecules considered. These data pointsâ€”each containing a molecule ğ‘š at residual

8

depth ğ›¿ with cost ğ‘â€”are used to update the parametric approximation of the value function

ğ‘£ğœ‹(ğ‘š, ğ›¿). As detailed in the Methods, the value function is approximated by a neural network

that takes as input an extended-connectivity (ECFP) ï¬ngerprint of the molecule ğ‘š and the

residual depth ğ›¿ and outputs a real valued estimate of the expected cost under the policy ğœ‹. 26

This process is repeated in an iterative manner as the value estimates of the target molecules,

ğ‘£ğœ‹(ğ‘š, ğ‘‘max), approach their asymptotic values.

Figure 2a shows the total synthesis cost ğ‘tot for a single target molecule under the random

policy (markers). Each play of the retrosynthesis game has one of three possible outcomes:

a â€œwinningâ€ synthesis plan terminating in buyable substrates (blue circles), a â€œlosingâ€ plan

that exceeds the maximum depth (green triangles), and a â€œlosingâ€ plan that contains dead-

end molecules that cannot be bought or made (black pentagons). After many synthesis

attempts, the running average of the ï¬‚uctuating synthesis cost converges to the expected

cost ğ‘£ğœ‹(ğ‘š, ğ‘‘max) as approximated by the neural network (red line). Repeating this analysis

for the 100,000 target molecules, the random policy results in an average cost of âˆ¼110 per

molecule with only a 25% chance of identifying a winning synthesis in each attempt. Clearly,

there is room for improvement.

Beyond the random policy, even simple heuristics can be used to improve performance

signiï¬cantly. In one such policy, inspired by Syntaurus, 7 the player selects the reaction ğ‘Ÿ that

maximizes the quantity

ğ‘“ (ğ‘Ÿ) = ğ‘›ğ‘ (ğ‘š)ğ›¾

âˆ‘ï¸

âˆ’

ğ‘šâ€²(ğ‘Ÿ)

ğ‘›ğ‘ (ğ‘šâ€²)ğ›¾

(4)

where ğ‘›ğ‘ (ğ‘š) is the length of the canonical smiles string representing molecule ğ‘š, ğ›¾ is a

user-speciï¬ed exponent, and the sum is taken over all reactants ğ‘šâ€² associated with a reaction

ğ‘Ÿ. When ğ›¾ > 1, the reactions that maximize this function can be interpreted as those that

decompose the product into multiple parts of roughly equal size. Note that in contrast to the

random policy, this greedy heuristic is deterministic: each play of the game results in the

same outcome. Figure 2a shows the performance of this â€œsymmetric disconnectionâ€ policy

9

Figure 2: Heuristic policies. (a) Synthesis cost ğ‘tot for a single molecule ğ‘š (N-dibutyl-4-
acetylbenzeneacetamide) for successive iterations of the retrosynthesis game under the random
policy. Blue circles denote â€œwinningâ€ synthesis plans that trace back to buyable molecules.
Green triangles and black pentagons denote â€œlosingâ€ plans that exceed the maximum depth or
include unmakable molecules, respectively. The solid line shows the neural network prediction
of the value function ğ‘£ğœ‹(ğ‘š, ğ‘‘max) as it converges to the average synthesis cost. The dashed
line shows the expected cost under the deterministic â€œsymmetric disconnectionâ€ policy with
ğ›¾ = 1.5. (b) Distribution of expected costs ğ‘£ğœ‹(ğ‘š, ğ‘‘max) over the set of 100,000 target molecules
for diï¬€erent noise levels ğœ€. The red squares and black circles show the performance of the
symmetric disconnection policy (ğœ€ = 0) and the random policy (ğœ€ = 1), respectively. (c) The
average synthesis cost of the target molecules decreases with increasing noise level ğœ€, while
the average branching factor increases. Averages were estimated from 50 plays for each target
molecule.

10

11020304050Synthesisattempts410100ctot151015(cid:31)ctot(cid:30)102103104N((cid:31)ctot(cid:30))0.00.10.20.40.61.00.00.250.50.751.0Îµ25405570(cid:31)ctot(cid:30)(cid:31)ctot(cid:30)(cid:31)b(cid:30)1.401.451.501.55(cid:31)b(cid:30)104103102(a)(b)(c)100104705540251         5           10          1513040502010Synthesis attempts0.00.250.50.751.01.551.501.451.40with ğ›¾ = 1.5 for a single target molecule (dashed line). Interestingly, while the pathway

identiï¬ed by the greedy policy is much better on average than those of the random policy

(ğ‘tot = 4 vs.

ğ‘tot
âŸ¨

âŸ©

= 35.1), repeated application of the latter reveals the existence of an

even better pathway containing only three reactions. An optimal policy would allow for the

identiï¬cation of that best synthesis plan during a single play of the retrosynthesis game.

The performance of a policy is characterized by the distribution of expected costs over

the set of target molecules. Figure 2b shows the cost distribution for a series of policies that

interpolate between the greedy â€œsymmetric disconnectionâ€ policy and the random policy

(see also Figure 6). The intermediate ğœ€-greedy policies behave greedily with probability

ğœ€, selecting the reaction that maximizes ğ‘“ (ğ‘Ÿ), but behave randomly with probability ğœ€,

1

âˆ’

selecting any one of the possible reactions ğ‘Ÿ

(ğ‘š) with equal probability. On average, the

âˆˆ â„›
addition of such noise is usually detrimental to policy performance. Noisy policies are less

likely to identify a successful synthesis for a given target (Figure 7a) and result in longer

syntheses when they do succeed (Figure 7b). Consequently, the average cost

âŸ©
monotonically with increasing noise as quantiï¬ed by the parameter ğœ€ (Figure 2c).

ğ‘tot
âŸ¨

increases

The superior performance (lower synthesis costs) of the greedy policy is correlated with

the average branching factor

ğ‘
âŸ¨

âŸ©

, which represents the average number of reactants for each

reaction in the synthesis tree. Branching is largest for the greedy policy (ğœ€ = 0) and decreases

monotonically with increasing ğœ€ (Figure 2c). On average, synthesis plans with greater

branching (i.e., convergent syntheses) require fewer synthetic steps to connect the target

molecules to the set of buyable substrates. This observation supports the chemical intuition

underlying the symmetric disconnection policy: break apart each â€œcomplexâ€ molecule into

â€œsimplerâ€ precursors. However, this greedy heuristic can sometimes be short-sighted. An

optimal retrosynthetic â€œmoveâ€ may increase molecular complexity in the short run to reach

simpler precursors more quickly in the longer run (e.g., in protecting group chemistry). An

optimal policy would enable the player to identify local moves (i.e., reactions) that lead to

synthesis pathways with minimum total cost.

11

Policy improvement through self-play

Knowledge of the value function, ğ‘£ğœ‹, under a given policy ğœ‹ enables the identiï¬cation of

better policies that reduce the expected synthesis cost. To see this, consider a new policy ğœ‹â€²

that selects at each step the reaction that minimizes the expected cost under the old policy ğœ‹

ğœ‹â€²(ğ‘Ÿ

|

ğ‘š) =

â§

âªâªâªâªâ¨
âªâªâªâªâ©

â¡

1 if ğ‘Ÿ = arg min
ğ‘Ÿâˆˆâ„›(ğ‘š)

â£ğ‘rxn(ğ‘Ÿ) +

â¤

âˆ‘ï¸

ğ‘šâ€²(ğ‘Ÿ)

ğ‘£ğœ‹(ğ‘šâ€²)

â¦

.

(5)

0 otherwise

By the policy improvement theorem, 21 this greedy policy ğœ‹â€² is guaranteed to be as good or

better than the old policy ğœ‹â€”that is, ğ‘£ğœ‹â€²

â‰¤

ğ‘£ğœ‹, where equality holds only for the optimal

policy. This result provides a basis for systematically improving any policy in an iterative

procedure called policy iteration, 21 in which the value function ğ‘£ğœ‹ leads to an improved policy

ğœ‹â€² that leads to a new value function ğ‘£ğœ‹â€² and so on.

One of the challenges in using the greedy policy Eq. 5 is that it generates only a single

pathway and its associated cost for each of the target molecules. The limited exposure of

these greedy searches can result in poor estimates of the new value function ğ‘£ğœ‹â€², in particular

for molecules that are not included in the identiï¬ed pathways. A better estimate of ğ‘£ğœ‹â€² can

be achieved by exploring more of the molecule space in the neighborhood of these greedy

pathways. Here, we encourage exploration by using an ğœ€-greedy policy, which introduces

random choices with probability ğœ€ but otherwise follows the greedy policy Eq. 5. Iteration of

this ğœ€-soft policy is guaranteed to converge to an optimal policy that minimizes the expected

synthesis cost for a given noise level ğœ€ > 0. 21 Moreover, by gradually lowering the noise level,

it is possible to approach the optimal greedy policy in the limit as ğœ€

0.

â†’

12

Training protocol

Starting from the random policy, we used self-play to learn an improved policy over the

course of 1000 iterations, each comprised of âˆ¼100,000 retrosynthesis games initiated from the

target molecules. During the ï¬rst iteration, each target molecule was considered in turn using

the ğœ€-greedy policy Eq. 5 with ğœ€ = 0.2. Candidate reactions and their associated reactants

were identiï¬ed by application of reaction templates as detailed in the Methods. Absent an

initial model of the value function, the expected costs of molecules encountered during play

were selected at random from a uniform distribution on the interval [1, 100]. Following the

completion of each game, the costs of molecules in the selected pathway were computed and

stored for later use. In subsequent iterations, the values of molecules encountered previously

(at a particular depth) were estimated by their average cost. After the ï¬rst 50 iterations, the

value estimates accumulated during play were used to train a neural network, which allowed

for estimating the values of new molecules not encountered during the previous games (see

Methods for details on the network architecture and training). Policy improvement continued

in an iterative fashion guided both by the average costs (for molecules previously encountered)

and by the neural network (for new molecules), which was updated every 50-100 iterations.

During policy iteration, the noise parameter was reduced from ğœ€ = 0.2 to 0 in increments

of 0.05 every 200 iterations in an eï¬€ort to anneal the system towards an optimal policy.

Following each change in ğœ€, the saved costs were discarded such that subsequent value

estimates were generated at the current noise level ğœ€. The result of this training procedure

was a neural network approximation of the (near) optimal value function ğ‘£*(ğ‘š, ğ›¿), which

estimates the minimum cost of synthesizing any molecule ğ‘š starting from residual depth

ğ›¿. In practice, we found that a slightly better value function could be obtained using the

cumulative reaction network generated during policy iteration. Following Kowalik et al., 6 we

used dynamic programming to compute the minimum synthesis cost for each molecule in

the reaction network. These minimum costs were then used to train the ï¬nal neural network

approximation of the value function ğ‘£*.

13

computed using ğœ‹* are plotted
Figure 3: Training results. In (a) and (b)
ğ‘tot
âŸ¨
versus policy iterations, respectively (solid blue squares). Solid horizontal lines show these
quantities for the heuristic policy ğœ‹ğ‘ ğ‘‘ (red triangles) and the random policy (black circles).
The larger cyan square shows
after each tree had been searched for the best (lowest)
ğ‘tot
target cost. Dashed vertical lines show points when ğœ€ was lowered.

ğ‘tot
âŸ¨

and

âŸ©

âŸ¨

âŸ©

âŸ©

Training results

Figure 3a shows how the average synthesis cost

ğ‘tot
âŸ¨

âŸ©

decreased with each iteration over

the course of the training process. Initially, the average cost was similar to that of the

random policy (

ğ‘tot
âŸ¨

âŸ© â‰ˆ

70) but improved steadily as the computer learned to identify

â€œwinningâ€ reactions that lead quickly to buyable substrates. After 800 iterations, the cost

ğ‘tot
dropped below that of the symmetric disconnection policy (
âŸ¨

âŸ©

= 19.3) but showed little

further improvement in the absence of exploration (i.e., with ğœ€ = 0). The ï¬nal cost estimate

ğ‘tot

= 11.4, cyan square) was generated by identifying the minimum cost pathways present

(
âŸ¨
in the cumulative reaction network generated during the training process. The ï¬nal drop

âŸ©

14

1030507090(cid:31)ctot(cid:30)12004006008001000Iterations1.451.551.65(cid:31)btot(cid:30)0.2       0.15       0.1      0.05       0.0(a)90705030101.651.551.45(b)Iterations1         200       400       600       800      1000Table 1: Training and testing results for the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘ and the
learned policy ğœ‹*. Percentages were computed based the sizes of the training set (âˆ¼100,000)
and the testing set (âˆ¼25,000).

Train (100,00) Test (25,000)

ğœ‹ğ‘ ğ‘‘

19.3

1.54

64%

25%

11%

ğœ‹*

13.1

1.65

83%

11%

6%

ğœ‹ğ‘ ğ‘‘

19.2

1.54

65%

24%

11%

ğœ‹*

11.5

1.58

73%

22%

5%

âŸ©

ğ‘tot
âŸ¨
ğ‘
âŸ©
âŸ¨
ğ‘tot < ğ‘ƒ1

ğ‘ƒ1

ğ‘tot < ğ‘ƒ2
ğ‘ƒ2

â‰¤
ğ‘tot

â‰¥

in cost for ğœ€ = 0 suggests that further policy improvements are possible using improved

annealing schedules. We emphasize that the ï¬nal near-optimal policy was trained from a

state of complete ignorance, as directed by the user-speciï¬ed objective function to minimize

the synthesis cost.

During the training process, the decrease in synthesis cost was guided both by motivation,

as prescribed by the cost function, and by opportunity, as dictated by the availability of

alternate pathways. Early improvements in the average cost were achieved by avoiding

â€œunmakableâ€ molecules, which contributed the largest cost penalty, ğ‘ƒ2 = 100. Of the target

molecules, 11% reduced their synthesis cost from ğ‘tot > ğ‘ƒ2 to ğ‘ƒ2 > ğ‘tot > ğ‘ƒ1 by avoiding

such problematic molecules. By contrast, only 2% of targets improved their cost from

ğ‘ƒ2 > ğ‘tot > ğ‘ƒ1 to ğ‘ƒ1 > ğ‘tot. In other words, if a synthesis tree was not found initially at a

maximum depth of ğ‘‘max = 10, it was unlikely to be discovered during the course of training.

Perhaps more interesting are those molecules (ca. 10%) for which syntheses were more easily

found but subsequently improved (i.e., shortened) during the course of the training process.

See Table 1 for a more detailed breakdown of these diï¬€erent groups.

Consistent with our observations above, lower cost pathways were again correlated with

the degree of branching ğ‘ along the synthesis trees (Figure 3b). Interestingly, the average

branching factor for synthesis plans identiï¬ed by the learned policy was signiï¬cantly larger

15

than that of the symmetric disconnection policy (

= 1.65 vs. 1.54). While the latter favors

ğ‘

âŸ©

âŸ¨

branching, it does so locally based on limited informationâ€”namely, the heuristic score of Eq.

5. By contrast, the learned policy uses information provided in the molecular ï¬ngerprint to

select reactions that increase branching across the entire synthesis tree (not just the single

step). Furthermore, while the heuristic policy favors branching a priori, the learned policy

does so only in the service of reducing the total cost. Changes in the objective function (e.g.,

in the cost and availability of the buyable substrates) will lead to diï¬€erent learned policies.

Model validation

Figure 4 compares the performance of the learned policy evaluated on the entire set of

âˆ¼100,000 target molecules used for training and on a diï¬€erent set of âˆ¼25,000 target molecules

set aside for testing. For the training molecules, the value estimates ğ‘£*(ğ‘š) predicted by the

neural network are highly correlated with the actual costs obtained by the ï¬nal learned policy

ğœ‹* (Figure 4a). We used the same near-optimal policy to determine the synthesis cost of the

testing molecules, ğ‘tot(ğœ‹*). As illustrated in Figure 4b, these costs were correlated to the

predictions of the value network ğ‘£*(ğ‘š) albeit more weakly than those of the training data

(Pearson coeï¬ƒcient of 0.5 for testing vs. 0.99 for training). This correlation was stronger for

the data in Figure 4b, which focuses on those molecules that could actually be synthesized

(Pearson coeï¬ƒcient of 0.7 for the 73% testing molecules with â€œwinningâ€ syntheses).

Figure 4c,d compares the synthesis costs of the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘ against

that of the learned policy ğœ‹* for both the training and testing molecules. The ï¬gure shows

that the results are highly correlated (Pearson coeï¬ƒcient 0.84 and 0.86 for training and

testing, respectively), indicating that the two policies make similar predictions. However,

closer inspection reveals that the learned policy is systematically better than the heuristic as

evidenced by the portion of the histogram below the diagonal (red line). For these molecules

(42% and 31% of the training and testing sets, respectively), the learned policy identiï¬es

synthesis trees containing fewer reactions than those of the heuristic policy during single

16

Figure 4: Model Validation.
In (a) and (b) a 2D histogram illustrates the relationship
between the synthesis cost ğ‘tot determined by the learned policy ğœ‹* and that predicted by
the value network ğ‘£* for each of the âˆ¼100,000 training molecules and the âˆ¼25,000 testing
molecules, respectively. In (c) and (d), a 2D histogram compares the synthesis cost ğ‘tot
determined by the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘ to that of learned policy ğœ‹* for training
and testing molecules, respectively. In (a-d), the gray-scale intensity is linearly proportional
to the number of molecules within a given bin; the red line shows the identity relation. In (c)
and (d), the percentage of molecules for which ğœ‹* (ğœ‹ğ‘ ğ‘‘) found the cheaper pathway is listed
below (above) the red line. In (e) and (f), distributions of synthesis costs ğ‘tot determined
under policies ğœ‹ğ‘ ğ‘‘ and ğœ‹* are shown for both testing and testing molecules, respectively.

deterministic plays of the retrosynthesis game. By contrast, it is rare in both the training

and testing molecules (about 4% and 11%, respectively) that the symmetric disconnection

policy performs better than the learned policy. Additionally, the learned policy is more likely

to succeed in identifying a viable synthesis plan leading to buyable substrates (Figure 4c).

Of the âˆ¼25,000 testing molecules, â€œwinningâ€ synthesis plans were identiï¬ed for 73% using

the learn policy as compared to 64% using the heuristic. These results suggest that the

lessons gleaned from the training molecules can be used to improve the synthesis of new and

unfamiliar molecules.

17

1510cpredicted(vâˆ—)1510ctot(Ï€âˆ—)1510ctot(Ï€sd)1510151015(cid:30)ctot(cid:29)101102103104N((cid:30)ctot(cid:29))Ï€âˆ—Ï€sd(a)(i)1510cpredicted(vâˆ—)1510ctot(Ï€âˆ—)1510ctot(Ï€sd)1510151015(cid:30)ctot(cid:29)102103N((cid:30)ctot(cid:29))Ï€âˆ—Ï€sd(b)(i)(ii)(iii)(ii)(iii)True cost is higher than predictedTrue cost is lowerthan predicted4.4%42%10%31%Conclusions

We have shown that reinforcement learning can be used to identify eï¬€ective policies for the

computational design of retrosynthetic pathways. In this approach, one speciï¬es the global

objective function to be minimized (here, the synthesis cost) without the need for ad hoc

models or heuristics to guide local decisions during generation of the synthesis plan. Starting

from a random policy, repeated plays of the retrosynthesis game are used to systematically

improve performance in an iterative process that converges in time to an optimal policy. The

learned value function provides a convenient estimate for the synthesis cost of any molecule,

while the associated policy allows for rapid identiï¬cation of the synthesis path. Importantly,

the cost function can be easily adapted, for example to include speciï¬c costs for reactions

and/or buyable substrates. Policy iteration using a diï¬€erent cost function will result in a

diï¬€erent policy that reï¬‚ects the newly speciï¬ed objectives.

The chemical feasibility of synthetic pathways identiï¬ed by the learned policy are largely

determined by the quality of the reaction templates. The present templates are derived

algorithmically from reaction precedents reported in the literature; however, an identical

approach based on reinforcement learning could be applied using template libraries curated

by human experts. 7,27 Alternatively, it may be possible to forgo the use of reaction templates

altogether in favor of machine learning approaches that suggest reaction precursors by other

means. 28 Ideally, such predictions should be accompanied by recommendations regarding the

desired conditions for performing each reaction in high yield. 29â€“32

In the present approach, the deterministic policy learned during training is applied only

once to suggest one (near) optimal synthesis pathway. Additional pathways are readily

generated, for example using Monte-Carlo Tree Search (MCTS) to bias subsequent searches

away from previously identiï¬ed pathways. 18 A similar approach is used by Syntaurus, which

relies on heuristic scoring functions to guide the generation of many possible synthesis plans,

from which the â€œbestâ€ are selected. The main advantage of a strong learned policy is to

direct such exploration more eï¬€ectively towards these best syntheses, thereby reducing the

18

computational cost of exploration.

We note, however, that the computational costs of training the learned policy are signiï¬cant

(ca. several million CPU hours for the training in Figure 3). While the application of reaction

templates remains the primary bottleneck (ca. âˆ¼50%), the additional costs of computing

ECFP ï¬ngerprints and evaluating the neural network were a close second (ca. âˆ¼45%). These

costs can be greatly reduced by using simple heuristics to generate synthetic pathways, from

which stronger policies can be learned. We found that Eq. 4 performed remarkably well and

was much faster to evaluate than the neural network. Such fast heuristics could be used as

starting points for iterative policy improvement or as roll-out policies within MCTS-based

learning algorithms. 21 This approach is conceptually similar to the ï¬rst iteration of AlphaGo

introduced by DeepMind. 33 Looking forward, we anticipate that the retrosynthesis game will

soon follow the way of chess and go, in which self-taught algorithms consistently outperform

human experts.

Methods

Target molecules

Training/testing sets of 95,774/23,945 molecules were selected from the Reaxys database on

the basis of their structural diversity. Starting from more than 20 million molecules in the

database, we excluded (i) those listed in the database of buyable compounds, (ii) those with

SMILES strings shorter than 20 or longer than 100, and (iii) those with multiple fragments

(i.e., molecules with â€˜.â€™ in the SMILES string). The resulting âˆ¼16 million molecules were

then aggregated using the Taylor-Butina (TB) algorithm 34 to form âˆ¼1 million clusters, each

comprised of â€œsimilarâ€ molecules. Structural similarity between two molecules ğ‘– and ğ‘— was

determined by the Tanimoto coeï¬ƒcient

ğ‘‡ =

ğ‘šğ‘–
|

2 +
|

ğ‘šğ‘—

ğ‘šğ‘–

ğ‘šğ‘—

Â·

âˆ’ |

,

|

ğ‘šğ‘–
ğ‘šğ‘—
|

Â·
2
|

19

(6)

where ğ‘šğ‘– is the ECFP4 ï¬ngerprint for molecule ğ‘–. 35 We used ï¬ngerprints of length 1024

and radius 3. Two molecules within a common cluster were required to have a Tanimoto

coeï¬ƒcient of ğ‘‡ > 0.4. The target molecules were chosen as the centroids of the âˆ¼125, 000

largest clusters, each containing more than 20 molecules and together representing more than

âˆ¼12 million molecules. These target molecules were partitioned at random to form the ï¬nal

sets for training and testing.

Buyable molecules

A molecule is deï¬ned to be a substrate if it is listed in the commercially available Sigma-

Aldrich, 23 eMolecules, 24 or LabNetwork catalogs, 25 and does not cost more than $100/g.

The complete set of molecules in these catalogs with price per gram

$100 is denoted

â‰¤

=

ğ‘š1, . . . , ğ‘šğ‘›

with ğ‘›

300, 000. The molecules contained in each catalog can be

{

â„¬
downloaded by visiting the respective webpage.

â‰ˆ

}

Reaction templates

Given a molecule ğ‘š, we used a set of âˆ¼60, 000 reaction templates to generate sets of possible

precursors ğ‘šâ€², which can be used to synthesize ğ‘š in one step. As detailed previously, 36 these

templates were extracted automatically from literature precedents and encoded using the

SMARTS language. The application of the templates involves two main steps, substructure

matching and bond rewiring, which were implemented using RDkit. 37 Brieï¬‚y, we ï¬rst search

the molecule ğ‘š for a structural pattern speciï¬ed by the template. For each match, the

reaction template further speciï¬es the breaking and making of bonds among the constituent

atoms to produce the precursor molecule(s) ğ‘šâ€². We used the RDChiral package 38 to handle

the creation, destruction, and preservation of chiral centers during the reaction.

The application of reaction templates to produce candidate reactions represents a major

computational bottleneck in the retrosynthesis game due to the combinatorial complexity of

substructure matching. Additionally, even when a template generates a successful match, it

20

may fail to account for the larger molecular context resulting in undesired byproducts during

the forward reaction. These two challenges can be partially alleviated by use of a â€œtemplate

prioritizerâ€, 18 which takes as input a representation of the target molecule ğ‘š and generates

a probability distribution over the set of templates based on their likelihood of success. By

focusing only on the most probable templates, the prioritizer can serve to improve both

quality of the suggested reactions and the speed with which they are generated. In practice,

we trained a neural network prioritizer on 5.4 million reaction examples from Reaxys and

selected the top 99.5% of templates for each molecule ğ‘š encountered. This ï¬ltering process

drastically reduced the total number templates applied from 60,000 to less than 50 for most

molecules. The training and validation details as well as the model architecture are available

on Github. 39

Policy Iteration

As noted in the main text, the depth constraint imposed on synthesis trees generated during

the retrosynthesis requires some minor modiï¬cations to the value function of Eq. 2. The

expected cost of synthesizing a molecule ğ‘š now depends on the residual depth ğ›¿ as

ğ‘£ğœ‹(ğ‘š, ğ›¿) =

âˆ‘ï¸

ğ‘Ÿâˆˆâ„›(ğ‘š)

ğœ‹(ğ‘Ÿ

|

â¡

ğ‘š, ğ›¿)

â£ğ‘rxn(ğ‘Ÿ) +

â¤

âˆ‘ï¸

ğ‘šâ€²(ğ‘Ÿ)

ğ‘£ğœ‹(ğ‘šâ€², ğ›¿

1)

â¦ ,

âˆ’

(7)

where the ï¬rst sum is over candidate reactions with ğ‘š as product, and the second is over

all reactants ğ‘šâ€²(ğ‘Ÿ) associated with a reaction ğ‘Ÿ. For the present cost model, the expected

cost ğ‘£ğœ‹(ğ‘šâ€², ğ›¿) increases with decreasing ğ›¿ due to the increased likelihood of being penalized

(to the extent ğ‘ƒ1) for reaching the maximum depth (ğ‘‘ = ğ‘‘max such that ğ›¿ = 0). Similarly,

the ğœ€-greedy policy used in policy improvement must also account for the residual depth at

21

which a molecule is encountered

ğ‘š, ğ›¿) =

ğœ‹(ğ‘Ÿ

|

â§

âªâªâªâªâ¨
âªâªâªâªâ©

1

ğœ€

â¡

ğœ€

âˆ’

if ğ‘Ÿ = arg min
ğ‘Ÿâˆˆâ„›(ğ‘š)

â£ğ‘rxn(ğ‘Ÿ) +

ğ‘£ğœ‹(ğ‘šâ€², ğ›¿

âˆ‘ï¸

ğ‘šâ€²(ğ‘Ÿ)

â¤

1)

â¦

.

âˆ’

(8)

otherwise

These recursive functions are fully speciï¬ed by three terminating conditions introduced in the

main text: (1) buyable molecule encountered, ğ‘£(ğ‘š, ğ›¿

= 0) = ğ‘sub(ğ‘š) for ğ‘š

; (2) maximum

âˆˆ â„¬

depth reached, ğ‘£(ğ‘š, 0) = ğ‘ƒ1; and (3) unmakeable molecule encountered, ğ‘£(ğ‘š, ğ›¿

= 0) = ğ‘ƒ2

for

(ğ‘š) = âˆ….

â„›

Neural network architecture and training

We employed a multi-layer neural network illustrated schematically in Figure 5. The 17

million model parameters were learned using gradient descent on training data generated

by repeated plays of the retrosynthesis game. Training was performed using Keras with the

Theano backend and the Adam optimizer with an initial learning rate of 0.001, which decayed

with the number of model updates ğ‘˜ as 0.001/(1 + 2âˆšğ‘˜) (13 updates were used to compute

ğœ‹*). During each update, batches of 128 molecules and their computed average costs at a

ï¬xed ğœ€ were selected from the most recent data and added to a replay buï¬€er. Batches of

equivalent size were randomly selected from the buï¬€er and passed through the model for

up to 100 epochs (1 epoch was taken as the total number of new data points having passed

through the network). The mean-average error between the averaged (true) and predicted

costs was used as the loss function. The latest model weights were then used as the policy

for the next round of synthesis games.

Additional results

Figure 6 shows the full cost distribution for the heuristic policy (ğœ‹ğ‘ ğ‘‘) and the learned optimal

policy (ğœ‹*). Figure 7a shows the probability of successfully synthesizing a molecule from the

22

Ì¸
Ì¸
Figure 5: The neural model for the cost of molecules is a feed-forward neural network that
accepts as input (green) an ECFP ï¬ngerprint of size 16384 extended to include the residual
depth ğ›¿ of the molecule. The architecture includes one input later (blue) consisting of 1024
nodes, ï¬ve hidden layers (red) each containing 300 nodes, and one output layer (purple) of
size one plus a ï¬lter (also purple) that scales the initial output number to be within the range
[0, 500]. We also used batch normalization after each layer. The ï¬nal output represents the
estimated cost.

training set (ğ‘success) versus ğœ€. Figure 7b shows the average cost (

ğ‘tot

) of the training set
âŸ©
) for the target costs

âŸ¨
ğ‘tot

âŸ©

versus ğœ€. Figure 8 shows normalized probability distributions ğ‘ğœ€(
âŸ¨

in the training set for diï¬€erent values of ğœ€. Figure 9 shows the 2D probability distribution

ğ‘ğœ€(

ğ‘tot
âŸ¨

) computed with ğœ€ = 0 for the training set.
âŸ©

23

Dense: 1024, ReLUInput ECFP4: 16384 + 1Batch NormalizationDense: 300, ReLU5XDense: 1, LinearBatch Normalization500 - 500 e-|x|Table 2: Comparison of the performance of ğœ‹ğ‘ ğ‘‘ versus ğœ‹*. The values show the percent that
one policy found a lower cost than the other, or whether the two policies found pathways
with identical cost (e.g. a tie), for the molecules in the training and testing sets. All percents
were computed using the size of the training (âˆ¼100,000) or testing set (âˆ¼25,000).

Train (100,00)

Test (25,000)

ğœ‹ğ‘ ğ‘‘ (%) ğœ‹* (%) Tie (%) ğœ‹ğ‘ ğ‘‘ (%) ğœ‹* (%) Tie (%)

0.4

3.9

<1

4.4

35.0

5.2

1.6

41.8

47.9

5.1

<1

53.8

6.1

3.4

<1

9.9

21.3

8.6

1.1

31.0

47.9

9.9

1.3

59.1

ğ‘tot < ğ‘ƒ1

ğ‘ƒ1

tot < ğ‘ƒ2
ğ‘ƒ2

â‰¤
ğ‘tot

â‰¥
Bulk

Figure 6: The distribution of expected costs,
, over the set of 100,000 target molecules
ğ‘tot
âŸ©
âŸ¨
is shown for the greedy optimal policy ğ‘£*(ğ‘š) and the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘ for
diï¬€erent values of ğœ€.

24

100101102(cid:104)ctot(cid:105)101102103104N((cid:104)ctot(cid:105))Ï€sd(0.0)Ï€sd(0.05)Ï€sd(0.1)Ï€sd(0.15)Ï€sd(0.2)Ï€sd(0.3)Ï€sd(0.4)Ï€sd(0.5)Ï€sd(0.6)Ï€sd(0.8)Ï€sd(1.0)Ï€âˆ—ğ‘tot
Figure 7: (a) The probability of successfully synthesizing target molecules and (b)
âŸ©
âŸ¨
(bottom) for those synthesized are shown versus ğœ– for the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘.

Figure 8: Normalized probability distributions ğ‘ğœ€(
) are shown for diï¬€erent values of ğœ€
âŸ©
for the symmetric disconnection policy ğœ‹ğ‘ ğ‘‘. Note that the columns in the plot each sum up
to one.

ğ‘tot
âŸ¨

25

0.00.20.40.60.81.00.20.40.6psuccess0.00.20.40.60.81.0Îµ3.03.54.0(cid:31)ctot(cid:30)(a)(b)0.00.050.10.150.20.250.30.40.51.0Îµ12345678910(cid:104)ctot(cid:105)0.0000.0250.0500.0750.1000.1250.1500.1750.200pÎµ((cid:104)ctot(cid:105))Figure 9: A 2D histogram ğ‘(
âŸ¨
ğœ‹ğ‘ ğ‘‘.

ğ‘tot

ğ‘tot
, (
âŸ¨

) is shown for the symmetric disconnection policy
âŸ©

âŸ©

26

12345678910Averagecost,(cid:104)ctot(cid:105)1.001.251.501.752.002.252.502.753.00Pathwaybranching,(cid:104)b(cid:105)0.00.20.40.60.81.01.21.41.6p((cid:104)ctot(cid:105),(cid:104)b(cid:105))Acknowledgments

This work was supported by the DARPA Make-It program under contract ARO W911NF-16-

2-0023. We acknowledge computing resources from Columbia Universityâ€™s Shared Research

Computing Facility project, which is supported by NIH Research Facility Improvement Grant

1G20RR030893-01, and associated funds from the New York State Empire State Development,

Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded

April 15, 2010.

References

(1) Warr, W. A. Mol. Inf. 2014, 33, 469â€“476.

(2) Engkvist, O.; Norrby, P.-O.; Selmi, N.; Lam, Y.-h.; Peng, Z.; Sherer, E. C.; Amberg, W.;

Erhard, T.; Smyth, L. A. Drug Discovery Today 2018,

(3) Coley, C. W.; Green, W. H.; Jensen, K. F. Acc. Chem. Res. 2018, 51, 1281â€“1289.

(4) Silver, D. et al. Nature 2017, 550, 354.

(5) Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai, M.; Guez, A.; Lanctot, M.;

Sifre, L.; Kumaran, D.; Graepel, T.; Lillicrap, T.; Simonyan, K.; Hassabis, D. Science

2018, 362, 1140â€“1144.

(6) Kowalik, M.; Gothard, C. M.; Drews, A. M.; Gothard, N. A.; Weckiewicz, A.; Fuller, P. E.;

Grzybowski, B. A.; Bishop, K. J. Angew. Chem. Int. Ed. 2012, 51, 7928â€“7932.

(7) SzymkuÂ´c, S.; Gajewska, E. P.; Klucznik, T.; Molga, K.; Dittwald, P.; Startek, M.;

Bajczyk, M.; Grzybowski, B. A. Angew. Chem. Int. Ed. 2016, 55, 5904â€“5937.

(8) Law, J.; Zsoldos, Z.; Simon, A.; Reid, D.; Liu, Y.; Khew, S. Y.; Johnson, A. P.; Major, S.;

Wade, R. A.; Ando, H. Y. J. Chem. Inf. Model. 2009, 49, 593â€“602.

27

(9) Christ, C. D.; Zentgraf, M.; Kriegl, J. M. J. Chem. Inf. Model. 2012, 52, 1745â€“1756.

(10) BÃ¸gevig, A.; Federsel, H.-J.; Huerta, F.; Hutchings, M. G.; Kraut, H.; Langer, T.;

LÂ¨ow, P.; Oppawsky, C.; Rein, T.; Saller, H. Org. Process Res. Dev. 2015, 19, 357â€“368.

(11) Grzybowski, B. A.; Bishop, K. J.; Kowalczyk, B.; Wilmer, C. E. Nat. Chem. 2009, 1,

31.

(12) Bertz, S. H. J. Am. Chem. Soc. 1981, 103, 3599â€“3601.

(13) Sheridan, R. P.; Zorn, N.; Sherer, E. C.; Campeau, L.-C.; Chang, C.; Cumming, J.;

Maddess, M. L.; Nantermet, P. G.; Sinz, C. J.; Oâ€™Shea, P. D. J. Chem. Inf. Model.

2014, 54, 1604â€“1616.

(14) Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F. J. Chem. Inf. Model. 2018, 58,

252â€“261.

(15) Weininger, D. J. Chem. Inf. Comput. Sci. 1988, 28, 31â€“36.

(16) Ertl, P.; Schuï¬€enhauer, A. J. Cheminformatics 2009, 1, 8.

(17) Coley, C. W.; Rogers, L.; Green, W. H.; Jensen, K. F. ACS Central Science 2017, 3,

1237â€“1245.

(18) Segler, M. H.; Waller, M. P. Chem. Eur. J. 2017, 23, 5966â€“5971.

(19) Segler, M. H.; Preuss, M.; Waller, M. P. Nature 2018, 555, 604.

(20) Corey, E. J. The logic of chemical synthesis; John Wiley & Sons, 1991.

(21) Sutton, R.; Barto, A. Reinforcement Learning: An Introduction, second edition ed.; MIT

Press, 2017.

(22) Mnih, V. et al. Nature 2015, 518, 529.

(23) Sigma-Aldrich, Inc. https://www.sigmaaldrich.com, Accessed: 2018-12-17.

28

(24) E-molecules. https://www.emolecules.com/info/plus/download-database, Ac-

cessed: 2018-12-17.

(25) LabNetwork

Collections.

https://www.labnetwork.com/frontend-app/p/#!

/screening-sets, Accessed: 2018-12-17.

(26) Rogers, D.; Hahn, M. J. Chem. Inf. Model. 2010, 50, 742â€“754.

(27) Klucznik, T. et al. Chem 2018, 4, 522â€“532.

(28) Liu, B.; Ramsundar, B.; Kawthekar, P.; Shi, J.; Gomes, J.; Luu Nguyen, Q.; Ho, S.;

Sloane, J.; Wender, P.; Pande, V. ACS Cent. Sci. 2017, 3, 1103â€“1113.

(29) Marcou, G.; Aires de Sousa, J.; Latino, D. A.; de Luca, A.; Horvath, D.; Rietsch, V.;

Varnek, A. J. Chem. Inf. Model. 2015, 55, 239â€“250.

(30) Lin, A. I.; Madzhidov, T. I.; Klimchuk, O.; Nugmanov, R. I.; Antipin, I. S.; Varnek, A.

J. Chem. Inf. Model. 2016, 56, 2140â€“2148.

(31) Segler, M. H.; Waller, M. P. Chem. Eur. J. 2017, 23, 6118â€“6128.

(32) Gao, H.; Struble, T. J.; Coley, C. W.; Wang, Y.; Green, W. H.; Jensen, K. F. ACS

Central Science 2018, 4, 1465â€“1476.

(33) Silver, D. et al. Nature 2016, 529, 484.

(34) Butina, D. J. Chem. Inf. Comput. Sci. 1999, 39, 747â€“750.

(35) Glen, R. C.; Bender, A.; Arnby, C. H.; Carlsson, L.; Boyer, S.; Smith, J. IDrugs 2006,

9, 199.

(36) Coley, C. W.; Barzilay, R.; Jaakkola, T. S.; Green, W. H.; Jensen, K. F. ACS Cent. Sci.

2017, 3, 434â€“443.

(37) RDKit: Open-source cheminformatics. http://www.rdkit.org, Accessed: 2018-12-17.

29

(38) Coley, C. W. RDChiral. https://github.com/connorcoley/rdchiral, Accessed:

2018-07-18.

(39) Coley, C. W. Retrotemp. https://github.com/connorcoley/retrotemp/tree/

master/retrotemp, Accessed: 2018-12-17.

30

