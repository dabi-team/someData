Graph-Preserving Grid Layout:
A Simple Graph Drawing Method for Graph Classiﬁcation using CNNs

Yecheng Lyu1∗, Xinming Huang1 and Ziming Zhang2†
1Worcester Polytechnic Institute, 2Mitsubishi Electric Research Laboratories (MERL)
{ylyu,xhuang}@wpi.edu,zzhang@merl.com

9
1
0
2

p
e
S
6
2

]

G
L
.
s
c
[

1
v
3
8
3
2
1
.
9
0
9
1
:
v
i
X
r
a

Abstract

Graph convolutional networks (GCNs) suffer from the irregu-
larity of graphs, while more widely-used convolutional neural
networks (CNNs) beneﬁt from regular grids. To bridge the
gap between GCN and CNN, in contrast to previous works
on generalizing the basic operations in CNNs to graph data,
in this paper we address the problem of how to project undi-
rected graphs onto the grid in a principled way where CNNs
can be used as backbone for geometric deep learning. To this
end, inspired by the literature of graph drawing we propose
a novel graph-preserving grid layout (GPGL), an integer pro-
gramming that minimizes the topological loss on the grid.
Technically we propose solving GPGL approximately using a
regularized Kamada-Kawai algorithm, a well-known noncon-
vex optimization technique in graph drawing, with a vertex
separation penalty that improves the rounding performance
on top of the solutions from relaxation. Using GPGL we can
easily conduct data augmentation as every local minimum will
lead to a grid layout for the same graph. Together with the help
of multi-scale maxout CNNs, we demonstrate the empirical
success of our method for graph classiﬁcation.

Introduction
Graph convolutional networks (GCNs) [Defferrard, Bresson,
and Vandergheynst, 2016; Kipf and Welling, 2016; Hamilton,
Ying, and Leskovec, 2017a; Bronstein et al., 2017; Chen, Ma,
and Xiao, 2018; Gao and Ji, 2019; Wu et al., 2019a,b; Morris
et al., 2019] refer to the family of networks that generalize
well-established convolutional neural networks (CNNs) to
work on structured graphs. The irregularity of a graph, e.g.
number of nodes and degree of each node, however, brings
signiﬁcant challenges in learning deep models based on graph
data, especially from the perspectives of architecture design
and computation. In general, a GCN has two operations to
compute feature representations for the nodes in a graph,
namely aggregation and transformation. That is, the feature
representation of a node is computed as an aggregate of the
feature representations of its neighbors before it is trans-
formed by applying the weights and activation functions. To
deal with graph irregularity the adjacency matrix is fed into
the aggregation function to encode the topology. Often the
weights in the transformation function are shared by all the

∗Work was done during an internship at MERL.
†Corresponding author.

nodes in the neighborhood to handle the node orderlessness
in the convolution.

On the other hand, CNNs have gained the reputation from
success in many research areas such as speech [Abdel-Hamid
et al., 2014] and vision [He et al., 2016] due to high accuracy
and high efﬁciency that allows millions of parameters to be
trained well, thanks to the regularity of the grids leading to
tensor representations. The grid topology is naturally inher-
ent in the convolution with optimized implementation, and
thus no adjacency matrix is needed. The nodes in each local
neighborhood are well ordered by location, and thus sharing
weights among nodes is unnecessary.
Motivation. Such key differences be-
tween graph convolution and spatial
convolution make CNNs much easier
and richer in deep architectures as well
as being trained much more efﬁciently.
Most of previous works in the literature
such as GCNs, however, focus on the
blue path in Fig. 1 that aim to general-
ize the basic operations in CNNs and
apply the new operations to graph data
at the cost of, for instance, higher running time. Intuitively
there also exists another way to handle graph data, shown
as the red path in Fig. 1, that maps graphs onto grids so that
CNNs can be applied directly to graph based applications.
Such methods can easily inherit the beneﬁts from CNNs such
as computational efﬁciency. We notice, however, that there
are few papers in the literature to explore this methodology
for geometric deep learning (GDL). As a graph topology can
be richer and more ﬂexible than a grid, and often encode im-
portant message of the graph, how to preserve graph topology
on the grid becomes a key challenge for such methods.

Figure 1: Illustra-
tion of two ways to
handle graph data.
The red path shows
our methodology.

Therefore, in order to bridge the gap between GCN and
CNN and in contrast to previous works on generalizing the
basic operations in CNNs to graph input, in this paper we
aim to address the following question: Can we use CNNs as
backbone for GDL by projecting undirected graphs onto the
grid in a principled way to preserve graph topology?

In fact, how to visualize structural information as graphs
has been well studied in graph drawing, an area of mathe-
matics and computer science combining methods from geo-
metric graph theory and information visualization to derive
2D or 3D depictions of graphs with vertices and edges whose

 
 
 
 
 
 
Figure 2: Illustration of differences between (a) graph, (b) our
graph-preserving grid layout (GPGL) and (c) image. The black
color in (b) denotes no correspondence to any vertex in (a), and
other colors denote non-zero features on the grid vertices.

arrangement within a drawing affects its understandability,
usability, fabrication cost, and aesthetics [Di Battista et al.,
1994]. In the literature, the Kamada-Kawai (KK) algorithm
[Kamada and Kawai, 1989] is one of the most widely-used
undirected graph visualization techniques. In general, the
KK algorithm deﬁnes an objective function that measures
the energy of each graph layout, and searches for the (local)
minimum. This often leads to a graph layout where adjacent
nodes are near some pre-speciﬁed distance from each other,
and non-adjacent nodes are well-spaced.

To the best of our knowledge, however, such graph drawing
algorithms have never been explored for GDL. One possible
reason is that graph drawing algorithms often work in con-
tinuous spaces, while our case requires discrete spaces (i.e.
grid) where CNNs can be deployed. Overall, how to project
graphs onto the grid with topology preservation for GDL is
still elusive in the literature.
Contributions. To address the question above, in this paper
we propose a novel graph-preserving grid layout (GPGL),
an integer programming problem that minimizes the topo-
logical loss on the grid so that CNNs can be used for GDL
on undirected graphs. Technically solving such a problem
is very challenging because potentially one needs to solve a
highly nonconvex optimization problem in a discrete space.
We manage to do so by proposing a regularized KK method
with a novel vertex separation penalty, followed by the round-
ing technique. As a result, our GPGL can manage to preserve
the irregular structural information in a graph on the regular
grid as graph layout, as illustrated in Fig. 2.

In summary, our key contributions are twofold as follows:
• We are the ﬁrst, to the best of our knowledge, to explicitly
explore the usage of graph drawing algorithms in the con-
text of GDL, and accordingly propose a novel regularized
Kamada-Kawai algorithm to project graphs onto the grid
with minimum loss in topological information.

• We demonstrate the empirical success of GPGL on graph
classiﬁcation, with the help of afﬁne-invariant CNNs.

Related Work

Graph Drawing & Network Embedding. Roughly speak-
ing, graph drawing can be considered as a subdiscipline of
network embedding [Hamilton, Ying, and Leskovec, 2017b;
Cui et al., 2018; Cai, Zheng, and Chang, 2018] whose goal
is to ﬁnd a low dimensional representation of the network
nodes in some metric space so that the given similarity (or dis-
tance) function is preserved as much as possible. In summary,
graph drawing focuses on the 2D/3D visualization of graphs

[Do˘grusöz, Madden, and Madden, 1996; Eiglsperger, Fekete,
and Klau, 2001; Koren, 2005; Spielman, 2007; Tamassia,
2013], while network embedding emphasizes the learning of
low dimensional graph representations. Despite the research
goal, similar methodology has been applied to both areas.
For instance, the KK algorithm [Kamada and Kawai, 1989]
was proposed for graph visualization as a force-based lay-
out system with advantages such as good-quality results and
strong theoretical foundations, but suffering from high com-
putational cost and poor local minima. Similarly Tenenbaum,
De Silva, and Langford [2000] proposed a global geometric
framework for network embedding to preserve the intrinsic
geometry of the data as captured in the geodesic manifold
distances between all pairs of data points. There are also
some works on drawing graphs on lattice, e.g. [Freese, 2004].
In contrast to graph drawing, our focus is to project an
existing graph onto the grid with minimum topological loss
so that CNNs can be deployed efﬁciently and effectively to
handle graph data. In such a context of GDL, we are not
aware of any work that utilizes the graph drawing algorithms
to facilitate GDL, to the best of our knowledge.

Graph Synthesis & Generation. Methods in this ﬁeld, e.g.
[Grover, Zweig, and Ermon, 2018; Li et al., 2018; You et
al., 2018; Samanta et al., 2019], often aim to learn a (sophis-
ticated) generative model that reﬂects the properties of the
training graphs. Recently, Kwon and Ma [2019] proposed
learning an encoder-decoder for the graph layout generation
problem to systematically visualize a graph in diverse layouts
using deep generative model. Franceschi et al. [2019] pro-
posed jointly learning the graph structure and the parameters
of GCNs by approximately solving a bilevel program that
learns a discrete probability distribution on the edges of the
graph for classiﬁcation problems.

In contrast to such methods above, our algorithm for GPGL
is essentially a self-supervised learning algorithm that is per-
formed for each individual graph and requires no training at
all. Moreover, we focus on re-deploy each graph onto the
grid as layout while preserving its topology. This procedure
is separate from the training of CNNs later.

Geometric Deep Learning. In general GDL studies the ex-
tension of deep learning techniques to graph and manifold
structured data (e.g. [Kipf and Welling, 2016; Bronstein et
al., 2017; Monti et al., 2017; Huang, Wu, and Van Gool,
2018]). In particular in this paper we focus on graph data
only. Broadly GDL methods can be categorized into spatial
methods (e.g. [Masci et al., 2015; Boscaini et al., 2016; Monti
et al., 2017]) and spectral methods (e.g. [Defferrard, Bresson,
and Vandergheynst, 2016; Levie et al., 2018; Yi et al., 2017]).
Some nice survey on this topic can be found in [Bronstein et
al., 2017; Zhang, Cui, and Zhu, 2018; Hamilton, Ying, and
Leskovec, 2017b; Wu et al., 2019b].

Niepert, Ahmed, and Kutzkov [2016] proposed a frame-
work for learning convolutional neural networks by applying
the convolution operations to the locally connected regions
from graphs. We are different from such a work by applying
CNNs to the grids where graphs are projected to with topol-
ogy preservation. Tixier et al. [2017] proposed an ad-hoc
method to project graphs onto 2D grid and utilized CNNs

(a) graph(b) graph-preserving grid layout(c) image(a) KK before rounding

(b) KK after rounding

(c) Ours before rounding

(d) Ours after rounding

Figure 3: Illustration of layout comparison between the KK algorithm and our proposed regularized KK algorithm before and after rounding
based on a fully-connected graph with 32 vertices.

for graph classiﬁcation. Speciﬁcally each node is embedded
into a high dimensional space, then mapped to 2D space by
PCA, and ﬁnally quantized into grid. In contrast, we propose
a principled and systematical way based on graph drawing,
i.e. a nonconvex integer programming formulation, for map-
ping graphs onto 2D grid. Besides, our graph classiﬁcation
performance is much better than both works. On MUTAG
and IMDB-B data sets we can achieve 90.42% and 74.9% test
accuracy with 1.47% improvement over [Niepert, Ahmed,
and Kutzkov, 2016] and 4.5% improvement over [Tixier et
al., 2017], respectively.

Graph-Preserving Grid Layout

Problem Setup
Let G = (V, E) be an undirected graph with a vertex set V
and an edge set E ⊆ V × V, and sij ≥ 1, ∀i (cid:54)= j be the graph-
theoretic distance such as shortest-path between two vertices
vi, vj ∈ V on the graph that encodes the graph topology.

Now we would like to learn a function f : V → Z2 to
map the graph vertex set to a set of 2D integer coordinates
on the grid so that the graph topology can be preserved as
much as possible given a metric d : R2 × R2 → R and a loss
(cid:96) : R × R → R. As a result, we are seeking for f to minimize
the following objective:

min
f

(cid:88)

i(cid:54)=j

(cid:96)(d(f (vi), f (vj)), sij).

(1)

Now letting xi = f (vi) ∈ Z2 as reparametrization, we can
rewrite Eq. 1 as the following integer programming problem:

min
X ⊆Z2

(cid:88)

i(cid:54)=j

(cid:96)(d(xi, xj), sij),

(2)

where the set X = {xi} denotes the 2D grid layout of the
graph, i.e. all the vertex coordinates on the 2D grid.
Self-Supervision. Note that the problem in Eq. 2 needs to
be solved for each individual graph, which is related to self-
supervision as a form of unsupervised learning where the
data itself provides the supervision [Zisserman, 2018]. This
property is beneﬁcial for data augmentation, as every local
minimum will lead to a grid layout for the same graph.
2D Grid Layout. In this paper we are interested in learning
only 2D grid layouts for graphs, rather than higher dimen-

sional grids (even 3D) where we expect that the layouts would
be more compact in volume and would have larger variance
in conﬁguration, both bringing more challenges into training
CNNs properly later. We conﬁrm our hypothesis based on
empirical observations. Besides, the implementation of 3D
basic operations in CNNs such as convolution and pooling
are often slower than 2D counterparts, and the operations
beyond 3D are not available publicly.
Relaxation & Rounding for Integer Programming. Inte-
ger programming is NP-complete and thus ﬁnding exact so-
lutions is challenging, in general [Wolsey and Nemhauser,
2014]. Relaxation and rounding is a widely used heuristic for
solving integer programming due to its efﬁciency [Bradley,
Hax, and Magnanti, 1977], where the rounding operator is
applied to the solution from the real-number relaxed problem
as the solution for the integer programming. In this paper we
employ this heuristic to learn 2D grid layouts. For simplicity,
in the sequel we will only discuss how to solve the relaxation
problem (i.e. before rounding).

Regularized Kamada-Kawai Algorithm
In this paper we set (cid:96) and d in Eq. 2 to the least-square loss
and Euclidean distance to preserve topology, respectively.
Kamada-Kawai Algorithm. The KK graph drawing algo-
rithm [Kamada and Kawai, 1989] was designed for a (relaxed)
problem in Eq. 2 with a speciﬁc objective function as follows:

min
X ⊆R2

LKK =

1
2

(cid:18) dij
sij

(cid:88)

i(cid:54)=j

(cid:19)2

− 1

,

(3)

where dij = (cid:107)xi − xj(cid:107), ∀(i, j) denotes the Euclidean dis-
tance between vertices vi and vj. Note that there is no regular-
ization to control the distribution of nodes in 2D visualization.
Fig. 3 illustrates the problems using the KK algorithm
when projecting the fully-connected graph onto 2D grid.
Eventually KK learns a circular distribution with equal space
among the vertices as in Fig. 3(a) to minimize the topology
preserving loss in Eq. 3. When taking a close look at these
2D locations we ﬁnd that after transformation all these lo-
cations are within the square area [0, 1] × [0, 1], leading to
the square pattern in Fig. 3(b) after rounding. Such behavior
totally makes sense to KK because it does not care about the
grid layout but only the topology preserving loss. However,

Algorithm 1 Regularized Kamada-Kawai Algorithm
:graph distance {sij}, parameters α, λ, γ
Input
Output :2D grid layout X ∗
˜X ← arg minX LKK with a (randomly shufﬂed) circular layout;
β ← max(γ, min∀xi,xj ∈ ˜X ,i(cid:54)=j (cid:107)xi − xj(cid:107));
xi ← max(1, α
X ∗ ← arg minX ⊆R2 LGP GL with set {xi} as initialization;
X ∗ ← rounding(X ∗);
return X ∗;

β ) · xi, ∀xi ∈ ˜X ; // optional: rescaling

our goal is not only to preserve the topology but also to make
graphs visible on the 2D grid in terms of vertices.
Vertex Separation Penalty. To this end, we propose a novel
vertex separation penalty to regularize the vertex distribution
on the grid. The intuition behind it is that when the minimum
distance among all the vertex pairs is larger than a threshold,
say 1, it will guarantee that after rounding every vertex will
be mapped to a unique 2D location with no overlap. But
when any distance is smaller than the threshold, it should
be considered to enlarge the distance, otherwise, no penalty.
Moreover, we expect that the penalties will grow faster than
the change of distances and in such a way the vertices can
be re-distributed more rapidly. Based on these considerations
we propose the following regularizer:

Rsep = λ

(cid:26)

max

0,

(cid:27)

− 1

,

α
dij

(cid:88)

i(cid:54)=j

(4)

where α ≥ 0, λ ≥ 0 are two predeﬁned constants. From the
gradient of Rsep w.r.t. an arbitrary 2D variable xi, that is,

∂Rsep
∂xi

= −λ

(cid:88)

i(cid:54)=j

xi − xj
d3
ij

1{dij <α}

(5)

where 1{·} denotes the indicator function returning 1 if the
condition is true, otherwise 0, we can clearly see that α as a
threshold controls when penalties occur, and λ controls the
trade-off between the loss and the regularization, leading to
different step sizes in gradient based optimization.
Our Algorithm. With the help of our new regularization, we
propose a regularized KK algorithm as listed in Alg. 1, which
tries to minimize the following regularized KK loss:

min
X ⊆Z2

LGP GL = LKK + Rsep.

(6)

(1) Initialization: Note that both KK and our algorithms are
highly nonconvex, and thus good initialization is need to
make both work well, i.e. convergence to good local minima.
To this end, we ﬁrst utilize the KK algorithm to generate a
vertex distribution. To do so, we employ the implementation
in the Python library NETWORKX which uses a circular
layout as initialization by default. As discussed above, KK
has no control on the vertex distribution. This may lead to
serious vertex loss problems in the 2D grid layout where
some of vertices in the original graph merge together as a
single vertex on the grid after rounding due to small distances.
To overcome this problem, we introduce an optional step,
rescaling, to enlarge the pairwise distances linearly. Intu-
itively this step “zooms” in the vertex distribution returned

by KK when the minimum distance is smaller than the thresh-
old but cannot be too small (controlled by parameter γ ≥ 0).
In this way not only such vertices are still distributed well,
but also there will be enough space among the vertices to
search for a good local minimum for our algorithm.

By taking the scaled solution as initialization, we minimize
the regularized KK loss, where the KK objective is used to
preserve the graph topology and the regularizer monitors the
distances to prevent the vertex loss on the grid.
(2) Topology Preservation with Regularization: As we ob-
serve, the key challenge in topology preservation comes from
the node degree, and the lower degree the easier for preser-
vation. Since there are only 8 neighbors at most in the 2D
grid layout, it will induce a penalty for a graph vertex whose
degree is higher than 8. Fig. 3 illustrates such a case where
the original graph is full-connected with 32 vertices. With
the help of our regularization, we manage to map this graph
to a ball-like grid layout, as shown in Fig. 3(c) and (d).

Proposition 1. An ideal 2D grid layout with no vertex loss
for a full-connected graph with |V| vertices is a ball-like
shape with radius of (cid:100)( |V|
2 (cid:101) that minimizes Eq. 2 with
relaxation of the regularized Kamada-Kawai loss. Here (cid:100)·(cid:101)
denotes the ceiling operation.

π ) 1

Proof. Given the conditions in the proposition above, we
have sij = 1, dij ≥ 1, ∀i (cid:54)= j and Rsep = 0. Without loss
of generalization, we uniformly deploy the graph vertices
in a circle and set the circular center A to a node on the 2D
grid. Now imagine the gradient ﬁeld over all the vertices as
a sandglass centered at A where each vertex is a ball with
a unit diameter. Then it is easy to see that by the “gravity”
(i.e. gradient) all the vertices move towards the center A,
and eventually are stabilized (as a local minimum) within an
r-radius circle whose covering area should satisfy |V| ≤ πr2,
i.e. r = (cid:100)( |V|
2 (cid:101) as the smallest sufﬁcient radius to cover all
the vertices with guarantee. We now complete our proof.

π ) 1

π ) 1

Note that Fig. 3(d) exactly veriﬁes Prop. 1 with a radius
r = (cid:100)( 32
2 (cid:101) = 4. In summary, our algorithm can manage to
preserve graph topology on the 2D grid even when the node
degree is higher than 8.
(3) Computational Complexity: The KK algorithm has the
computational complexity of, at least, O(|V|2) [Kobourov,
2012] that limits the usage of KK to medium-size graphs (e.g.
50-500 vertices). Since our algorithm in Alg. 1 is based on
KK, it unfortunately inherits this limitation as well. To accel-
erate the computation for large-scale graphs, we potentially
can adopt the strategy in multi-scale graph drawing algo-
rithms such as [Harel and Koren, 2000]. However, such an
extension is out of scope of this paper, and we will consider
it in our future work.

Graph Classiﬁcation

Data Augmentation. As mentioned in self-supervision, each
local minimum from our regularized KK algorithm in Alg. 1
will lead to a grid layout for the graph, while each minimum

Table 1: Statistics of benchmark data sets for graph classiﬁcation.

Data Set

MUTAG
IMDB-B
IMDB-M
PROTEINS

Num. of
Graph

Num. of
Class

188
1000
1500
1113

2
2
3
2

Avg.
Node

17.93
19.77
13.00
39.06

Avg.
Edge

19.79
96.53
65.94
72.82

Avg.
Degree

Max
Degree

Feat.
Dim.

1.10
4.88
5.07
1.86

8
270
176
50

7
136
89
3

Figure 4: Multi-scale maxout convolution (MSM-Conv).

Table 2: Mean accuracy (%) using different combinations of α, λ.

depends on its initialization. Therefore, to augment grid lay-
out data from graphs, we do a random shufﬂe on the circular
layout when applying Alg. 1 to an individual graph.
Grid-Layout based 3D Representation. Once a grid layout
is generated, we ﬁrst crop the layout with a sufﬁciently large
ﬁxed-size window (e.g. 64 × 64), and then associate each
vertex feature vector from the graph with the projected node
within the window. All the layouts are aligned to the top-left
corner of the window. The rest of nodes in the window with
no association of feature vectors are assigned to zero vectors.
Once vertex loss occurs, we take an average, by default,
of all the vertex feature vectors (i.e. average-pooling) and
assign it to the grid node. We also compare average-pooling
with max-pooling for merging vertices, and observe similar
performance empirically in terms of classiﬁcation.
Classiﬁer: Multi-Scale Maxout CNNs (MSM-CNNs). We
apply CNNs to the 3D representations of graphs for classiﬁca-
tion. As we discussed above, once the node degree is higher
than 8, the grid layout cannot fully preserve the topology, but
rather tends to form a ball-like compact pattern with larger
neighborhood. To capture such neighborhood information
effectively, the kernel sizes in the 2D convolution need to
vary. Therefore, the problem now boils down to a feature
selection problem with convolutional kernels.

Considering these, here we propose using a multi-scale
maxout CNN as illustrated in Fig. 4. We use consecutive
convolutions with smaller kernels to approximate the convo-
lutions with larger kernels. For instance, we use two 3 × 3
kernels to approximate a 5 × 5 kernel. The maxout [Good-
fellow et al., 2013] operation selects which scale per grid
node is good for classiﬁcation and outputs the correspond-
ing features. Together with other CNN operations such as
max-pooling, we can design deep networks, if necessary.
Graph Label Prediction by Majority Voting. In our exper-
iments we perform 10-fold leave-one-out testing. For sim-
plicity we conduct data augmentation once on all the graphs.
Therefore, even a test graph still has multiple grid layouts. By
default we use the majority voting mechanism to predict the
graph label based on all the predicted labels from its grid lay-
outs. We also try to randomly sample a grid layout and utilize
its label as the graph label. We ﬁnd that statistically there is
marginal difference between the two prediction mechanisms.
In fact, we observe that the accuracy of graph prediction is
very close to that of grid layout prediction.

Experiments

Data Sets. We evaluate our method, i.e. graph-preserving
grid layout + (multi-scale maxout) CNNs, on four medium-

Data Set

α = 1.00
λ = 1000

α = 1.50
λ = 1000

α = 1.25
λ = 1000

α = 1.25
λ = 200

α = 1.25
λ = 5000

MUTAG (21x)

85.14

83.04

86.31

85.26

85.26

size benchmark data sets for graph classiﬁcation, namely
MUTAG, IMDB-B, IMDB-M and PROTEINS. Table 1 sum-
marizes some statistics of each data set. Note that the max
node degree on each data set is at least 8, indicating that ball-
like patterns as discussed in Prop. 1 may occur, especially for
IMDB-B and IMDB-M.
Implementation. By default, we set the parameters in Alg. 1
as α = 1.25, λ = 1000. In this paper we do not use rescaling
because we ﬁnd that the initial KK solutions are sufﬁcient
to learn good grid layouts. We crop all the grid layouts to a
ﬁxed-size 64 × 64 window.

Also by default, for the MSM-CNNs we utilize three
consecutive 3 × 3 kernels in the MSM-Conv, and design a
simple network of “MSM-Conv(64)→max-pooling→MSM-
Conv(128)→max-pooling→MSM-Conv(256)→global-
pooling→FC(256)→FC(128)” as hidden layers with ReLU
activations, where FC denotes a fully connected layer and
the numbers in the brackets denote the numbers of channels
in each layer. We employ Adam [Kingma and Ba, 2014] as
our optimizer, and set batch size, learning rate, and dropout
ratio to 10, 0.0001, and 0.3, respectively.

Ablation Study

Effects of α, λ on Grid Layout and Classiﬁcation. To un-
derstand their effects on grid layout generation, we visualize
some results in Fig. 5 using different combinations of α, λ.
We can see that:

• From Fig. 5(a)-(c), the diameters of grid layouts are 5 × 5,
6 × 6, 7 × 7 for α = 1.00, 1.25, 1.50, respectively. This
strongly indicates that a smaller α tends to lead to a more
compact layout at the risk of losing vertices.

• From Fig. 5(c)-(e), similarly the diameters of grid layouts
are 5×5, 6×6, 6×6 for λ = 200, 1000, 5000, respectively.
This indicates that a smaller λ tends to lead to a more
compact layout at the risk of losing vertices as well. In fact
in Fig. 5(d) node 1 and node 5 are merged together. When
λ is sufﬁciently large, the layout tends to be stable.

Such observations follow our intuition in designing Alg. 1,
and occur across all the four benchmark data sets.

We also test the effects on classiﬁcation performance. For
instance, we generate 21x grid layouts using data augmenta-
tion on MUTAG, and list our results in Table 2. Clearly our
default setting achieves the best test accuracy. It seems that
for classiﬁcation parameter α is more important.

Inputfeature map2D convolutionMulti-Scale MaxoutConvolution2D convolution2D convolutionmaxoutOutputfeature map(a) α = 1.00, λ = 1000

(b) α = 1.50, λ = 1000

(c) α = 1.25, λ = 1000
Figure 5: Illustration of effects of different combinations of α, λ on grid layout generation (IMDB-B)

(d) α = 1.25, λ = 200

(e) α = 1.25, λ = 5000

(a) MUTAG

(b) IMDB-B
Figure 6: Illustration of vertex loss on different data sets: In each subﬁgure, (left) before rounding and (right) after rounding.

(c) PROTEINS

Table 3: Vertex loss ratio (%) on each data set.

Data Set

MUTAG IMDB-B IMDB-M PROTEINS

Vertex Loss

1.06

0.99

0.40

0.90

Table 4: Ratios (%) between vertex loss and misclassiﬁcation.

Data Set

MUTAG (21x)
IMDB-B (3x)
PROTEINS (3x)

|Gv.l.|
|Gmis.|
1.06
0.99
0.90

|Gv.l. ∩ Gmis.|
|Gv.l.|
20.00
16.18
24.32

|Gn.v.l. ∩ Gmis.|
|Gn.v.l.|
16.70
37.41
29.89

Vertex Loss, Graph Topology & Misclassiﬁcation. To bet-
ter understand the problem of vertex loss, we visualize some
cases in Fig. 6. The reason for this behavior is due to the
small distances among the vertices returned by Alg. 1 that
cannot survive from rounding. Unfortunately we do not ob-
serve a pattern on when such loss will happen. Note that our
Alg. 1 cannot avoid vertex loss with guarantee, and in fact
the vertex loss ratio on each data set is very low, as shown in
Table 3.

Further we test the relationship between vertex loss and
misclassiﬁcation, and list our results in Table 4 where Gv.l.,
Gn.v.l., and Gmis. denote the sets of graphs with vertex loss,
no vertex loss, and misclassiﬁcation, respectively, ∩ denotes
the intersection of two sets, | · | denotes the cardinality of
the set, and the numbers in the brackets denote the num-
bers of grid layouts per graph in data augmentation. From
this table, we can deduce that vertex loss cannot be not the
key reason for misclassiﬁcation, because it takes only tiny
portion in misclassiﬁcation and the ratios of misclassiﬁed
graphs with/without vertex loss are very similar, indicating
that misclassiﬁcation more depends on the classiﬁer rather
than vertex loss.

Next we test the relationship between graph topology in
terms of node degree and misclassiﬁcation, and show our
results in Fig. 7. As discussed before, a larger node degree is
more difﬁcult for preserving topology. In this test we would
like to verify whether such topology loss introduces misclas-
siﬁcation. Compared with the statistics in Table 1, it seems
that topology loss does cause trouble in classiﬁcation. One of
the reasons may be that the variance of the grid layout for a
vertex with larger node degree will be higher due to perturba-
tion. Designing better CNNs will be one of our future works
to improve the performance.

CNN based Classiﬁer Comparison. We test the effective-
ness of our MSM-CNNs, compared with (1 × 1)-kernel coun-
terpart and ResNet-50 [He et al., 2016], using the same aug-
mented data. On MUTAG in terms of mean test accuracy,
MSM-CNNs can achieve 89.34%, while (1 × 1)-kernel coun-
terpart and ResNet-50 achieve 86.79% and 86.78%, respec-
tively. Similar observations are made on the other data sets.
Therefore, the feature selection mechanism seems very useful
in graph classiﬁcation with our grid layout algorithm.

Running Time. To ver-
ify the running time of
our method, we show the
inference time in Fig. 8
for both GPGL and MSM-
CNN, respectively. GPGL
is optimized on an Intel
Core i7-7700K CPU and
MSM-CNN is run on a
GTX 1080 GPU. We do
not show the training time
for MSM-CNN because
it highly depends on the
number of training samples as well, but should be linearly

Figure 8: Running time at infer-
ence for GPGL and MSM-CNN.

MUTAGIMDB-BIMDB-MPROTEINSData Set0102030405060Inference Time (ms)GPGLMSM-CNNFigure 7: Graph topology in terms of node degree vs. misclassiﬁcation on different data sets. The y-axis denotes the fractions of misclassiﬁcation
in the graphs with the same node degrees.

proportional to inference time, roughly speaking. As we see,
the inference time for MSM-CNN is roughly the same, and
much less than that for GPGL. The running time of GPGL on
PROTEINS is the highest due to its largest average number
of nodes among the four data sets, which is consistent with
our complexity analysis.
Effect of Data Augmen-
tation using Grid Lay-
outs on Classiﬁcation. In
order to train the deep clas-
the amount
siﬁers well,
of training data is crucial.
As shown in Alg. 1, our
method can easily gener-
ate tons of grid layouts that
effectively capture differ-
ent characteristics in the
graph. Given the memory
limit, we demonstrate the
test performance for data
augmentation in Fig. 9, ranging from 1x to 101x with step
10x. As we see clearly, data augmentation can signiﬁcantly
boost the classiﬁcation accuracy on MUTAG, and similar
observations have been made for the other data sets.

Figure 9: Illustration of data aug-
mentation on classiﬁcation.

State-of-the-Art Comparison

To do a fair comparison for graph classiﬁcation, we follow
the standard routine, i.e. 10-fold cross-validation with ran-
dom split. We directly cite the numbers from the leader board
(as of 09/05/2019) at https://paperswithcode.com/task/graph-
classiﬁcation on each data set without reproducing these re-
sults. All the comparisons are summarized in Table 5, where
the numbers in the brackets denote the numbers of grid lay-
outs per graph in data augmentation.

In general, our method can match or even outperform the

Table 5: State-of-the-art test accuracy (%) comparison.

Data Set
MUTAG (101x)1
IMDB-B (21x)2
IMDB-M (5x)3
PROTEINS (5x)4

Rank-1 Rank-2 Rank-3

Ours

91.20
74.45
51.50
77.90

88.95
74.20
50.27
77.26

87.87
73.50
49.50
76.40

92.48±5.30
74.90±4.01
51.99±2.36
76.44±1.56

state-of-the-art on the four data sets, achieving two rank-1,
one rank-2 and one rank-3 in terms of mean test accuracy.
The small variances indicate the stability of our method. In
summary, such results demonstrate the empirical success of
our method on graph classiﬁcation.

Conclusion
In this paper we answer the question positively that CNNs
can be used directly for graph applications by projecting
graphs onto grids properly. To this end, we propose a novel
graph drawing problem, namely graph-preserving grid lay-
out (GPGL), which is an integer programming to learn 2D
grid layouts by minimizing topology loss. We propose a
regularized Kamada-Kawai algorithm to solve the integer
programming and a multi-scale maxout CNN to work with
GPGL. We manage to demonstrate the success of our method
on graph classiﬁcation that matches or even outperforms
the state-of-the-art on four benchmark data sets. As future
work we are interested in applying this method to real-world
problems such as point cloud classiﬁcation.

1In the order of ranks (same as follows): [Lu et al., 2019; Niepert,

Ahmed, and Kutzkov, 2016; Ivanov and Burnaev, 2018]

2[Ivanov and Burnaev, 2018; Morris et al., 2019] [Morris et al.,

2019]

3[Morris et al., 2019; Xinyi and Chen, 2019; Morris et al., 2019]
4[Lu et al., 2019; Li et al., 2019; Morris et al., 2019]

1112131415161718191101Data Augmentation (times)606570758085909510-Fold Mean Test Accuracy (%)MUTAGReferences
Abdel-Hamid, O.; Mohamed, A.-r.; Jiang, H.; Deng, L.; Penn, G.;
and Yu, D. 2014. Convolutional neural networks for speech
IEEE/ACM Transactions on audio, speech, and
recognition.
language processing 22(10):1533–1545.

Boscaini, D.; Masci, J.; Rodolà, E.; and Bronstein, M. 2016. Learn-
ing shape correspondence with anisotropic convolutional neural
networks. In Advances in Neural Information Processing Sys-
tems, 3189–3197.

Bradley, S. P.; Hax, A. C.; and Magnanti, T. L. 1977. Applied

mathematical programming.

Bronstein, M. M.; Bruna, J.; LeCun, Y.; Szlam, A.; and Van-
dergheynst, P. 2017. Geometric deep learning: going beyond
euclidean data. IEEE Signal Processing Magazine 34(4):18–42.

Cai, H.; Zheng, V. W.; and Chang, K. C.-C. 2018. A comprehensive
survey of graph embedding: Problems, techniques, and applica-
tions. IEEE Transactions on Knowledge and Data Engineering
30(9):1616–1637.

Chen, J.; Ma, T.; and Xiao, C. 2018. FastGCN: Fast learning with
graph convolutional networks via importance sampling. In ICLR.

Cui, P.; Wang, X.; Pei, J.; and Zhu, W. 2018. A survey on net-
work embedding. IEEE Transactions on Knowledge and Data
Engineering 31(5):833–852.

Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convo-
lutional neural networks on graphs with fast localized spectral
ﬁltering. In NeurIPS, 3844–3852.

Di Battista, G.; Eades, P.; Tamassia, R.; and Tollis, I. G. 1994.
Algorithms for drawing graphs: an annotated bibliography. Com-
putational Geometry 4(5):235–282.

Do˘grusöz, U.; Madden, B.; and Madden, P. 1996. Circular layout in
the graph layout toolkit. In International Symposium on Graph
Drawing, 92–100. Springer.

Eiglsperger, M.; Fekete, S. P.; and Klau, G. W. 2001. Orthogonal

graph drawing. In Drawing Graphs. Springer. 121–171.

Franceschi, L.; Niepert, M.; Pontil, M.; and He, X. 2019. Learning
discrete structures for graph neural networks. In ICML, 1972–
1982.

Freese, R. 2004. Automated lattice drawing.

In International

Conference on Formal Concept Analysis, 112–127. Springer.

Gao, H., and Ji, S. 2019. Graph u-nets. In ICML, 2083–2092.

Goodfellow, I. J.; Warde-Farley, D.; Mirza, M.; Courville, A.;
arXiv preprint

2013. Maxout networks.

and Bengio, Y.
arXiv:1302.4389.

Grover, A.; Zweig, A.; and Ermon, S. 2018. Graphite: Iterative
generative modeling of graphs. arXiv preprint arXiv:1803.10459.

Hamilton, W.; Ying, Z.; and Leskovec, J. 2017a. Inductive repre-
sentation learning on large graphs. In NeurIPS, 1024–1034.

Hamilton, W. L.; Ying, R.; and Leskovec, J. 2017b. Representation
learning on graphs: Methods and applications. arXiv preprint
arXiv:1709.05584.

Harel, D., and Koren, Y. 2000. A fast multi-scale method for
In International symposium on graph

drawing large graphs.
drawing, 183–196. Springer.

He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning

for image recognition. In CVPR, 770–778.

Huang, Z.; Wu, J.; and Van Gool, L. 2018. Building deep networks

on grassmann manifolds. In AAAI.

Ivanov, S., and Burnaev, E. 2018. Anonymous walk embeddings.

arXiv preprint arXiv:1805.11921.

Kamada, T., and Kawai, S. 1989. An algorithm for drawing general
undirected graphs. Information processing letters 31(1):7–15.

Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980.

Kipf, T. N., and Welling, M.

2016. Semi-supervised classi-
ﬁcation with graph convolutional networks. arXiv preprint
arXiv:1609.02907.

Kobourov, S. G. 2012. Spring embedders and force directed graph

drawing algorithms. arXiv preprint arXiv:1201.3011.

Koren, Y. 2005. Drawing graphs by eigenvectors: theory and
practice. Computers & Mathematics with Applications 49(11-
12):1867–1888.

Kwon, O.-H., and Ma, K.-L. 2019. A deep generative model for

graph layout. arXiv preprint arXiv:1904.12225.

Levie, R.; Monti, F.; Bresson, X.; and Bronstein, M. M. 2018.
Cayleynets: Graph convolutional neural networks with complex
rational spectral ﬁlters. IEEE Transactions on Signal Processing
67(1):97–109.

Li, Y.; Vinyals, O.; Dyer, C.; Pascanu, R.; and Battaglia, P. 2018.
Learning deep generative models of graphs. arXiv preprint
arXiv:1803.03324.

Li, J.; Rong, Y.; Cheng, H.; Meng, H.; Huang, W.; and Huang, J.
2019. Semi-supervised graph classiﬁcation: A hierarchical graph
perspective. In The World Wide Web Conference, 972–982. ACM.

Lu, H.; Huang, S. H.; Ye, T.; and Guo, X.

2019. Graph
star net for generalized multi-task learning. arXiv preprint
arXiv:1906.12330.

Masci, J.; Boscaini, D.; Bronstein, M.; and Vandergheynst, P. 2015.
Geodesic convolutional neural networks on riemannian mani-
folds. In Proceedings of the IEEE international conference on
computer vision workshops, 37–45.

Monti, F.; Boscaini, D.; Masci, J.; Rodola, E.; Svoboda, J.; and
Bronstein, M. M. 2017. Geometric deep learning on graphs and
manifolds using mixture model cnns. In CVPR, 5115–5124.

Morris, C.; Ritzert, M.; Fey, M.; Hamilton, W. L.; Lenssen, J. E.;
Rattan, G.; and Grohe, M. 2019. Weisfeiler and leman go neural:
Higher-order graph neural networks. In AAAI, volume 33, 4602–
4609.

Niepert, M.; Ahmed, M.; and Kutzkov, K. 2016. Learning convolu-

tional neural networks for graphs. In ICML, 2014–2023.

Samanta, B.; Abir, D.; Jana, G.; Chattaraj, P. K.; Ganguly, N.; and
Rodriguez, M. G. 2019. Nevae: A deep generative model for
molecular graphs. In AAAI, volume 33, 1110–1117.

Spielman, D. A. 2007. Spectral graph theory and its applications.
In 48th Annual IEEE Symposium on Foundations of Computer
Science (FOCS’07), 29–38. IEEE.

Tamassia, R. 2013. Handbook of graph drawing and visualization.

Chapman and Hall/CRC.

Tenenbaum, J. B.; De Silva, V.; and Langford, J. C. 2000. A global
geometric framework for nonlinear dimensionality reduction.
science 290(5500):2319–2323.

Tixier, A. J.-P.; Nikolentzos, G.; Meladianos, P.; and Vazirgiannis,
M. 2017. Graph classiﬁcation with 2d convolutional neural
networks. arXiv preprint arXiv:1708.02218.

Wolsey, L. A., and Nemhauser, G. L. 2014. Integer and combinato-

rial optimization. John Wiley & Sons.

Wu, F.; Zhang, T.; Souza Jr, A. H. d.; Fifty, C.; Yu, T.; and Wein-
berger, K. Q. 2019a. Simplifying graph convolutional networks.
arXiv preprint arXiv:1902.07153.

Wu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Yu, P. S.
2019b. A comprehensive survey on graph neural networks. arXiv
preprint arXiv:1901.00596.

Xinyi, Z., and Chen, L. 2019. Capsule graph neural network. In

ICLR.

Yi, L.; Su, H.; Guo, X.; and Guibas, L. J. 2017. Syncspeccnn:
Synchronized spectral cnn for 3d shape segmentation. In CVPR,
2282–2290.

You, J.; Ying, R.; Ren, X.; Hamilton, W. L.; and Leskovec, J. 2018.
Graphrnn: Generating realistic graphs with deep auto-regressive
models. arXiv preprint arXiv:1802.08773.

Zhang, Z.; Cui, P.; and Zhu, W. 2018. Deep learning on graphs: A

survey. arXiv preprint arXiv:1812.04202.

Zisserman, A. 2018. Self-supervised learning. https://project.inria.

fr/paiss/ﬁles/2018/07/zisserman-self-supervised.pdf.

