Noname manuscript No.
(will be inserted by the editor)

A Controlled Experiment of Diﬀerent Code
Representations for Learning-Based Program Repair

Marjane Namavar∗ · Noor Nashid∗ · Ali
Mesbah

2
2
0
2

l
u
J

5
1

]
E
S
.
s
c
[

3
v
1
8
0
4
1
.
0
1
1
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Training a deep learning model on source code has gained signif-
icant traction recently. Since such models reason about vectors of numbers,
source code needs to be converted to a code representation before vector-
ization. Numerous approaches have been proposed to represent source code,
from sequences of tokens to abstract syntax trees. However, there is no sys-
tematic study to understand the eﬀect of code representation on learning per-
formance. Through a controlled experiment, we examine the impact of various
code representations on model accuracy and usefulness in deep learning-based
program repair. We train 21 diﬀerent generative models that suggest ﬁxes for
name-based bugs, including 14 diﬀerent homogeneous code representations,
four mixed representations for the buggy and ﬁxed code, and three diﬀerent
embeddings. We assess if ﬁx suggestions produced by the model in various
code representations are automatically patchable, meaning they can be trans-
formed to a valid code that is ready to be applied to the buggy code to ﬁx it.
We also conduct a developer study to qualitatively evaluate the usefulness of
inferred ﬁxes in diﬀerent code representations. Our results highlight the im-
portance of code representation and its impact on learning and usefulness. Our
ﬁndings indicate that (1) while code abstractions help the learning process,
they can adversely impact the usefulness of inferred ﬁxes from a developer’s
point of view; this emphasizes the need to look at the patches generated from
the practitioner’s perspective, which is often neglected in the literature, (2)
mixed representations can outperform homogeneous code representations, (3)
bug type can aﬀect the eﬀectiveness of diﬀerent code representations; although

* The ﬁrst two authors contributed equally to this work.

Marjane Namavar
The University of British Columbia, Vancouver, Canada, E-mail: marjane@ece.ubc.ca

Noor Nashid
The University of British Columbia, Vancouver, Canada, E-mail: nashid@ece.ubc.ca

Ali Mesbah
The University of British Columbia, Vancouver, Canada, E-mail: amesbah@ece.ubc.ca

 
 
 
 
 
 
2

Marjane Namavar∗ et al.

current techniques use a single code representation for all bug types, there is
no single best code representation applicable to all bug types.

Keywords Program Repair · Deep Learning · Code Representation ·
Controlled Experiment

1 Introduction

Automated Program Repair (APR) aims at generating patches for a given
bug automatically [47, 70]. Recently, there are new approaches proposed [5,
28, 97] that employ deep-learning (DL) techniques to learn bug ﬁxes from
existing code corpora [18,27,100]. Learning-based repair is also gaining traction
in practice. For example, Facebook proposed SapFix [62] that runs as part
of their continuous integration infrastructure and can suggest bug ﬁxes for
runtime crashes; Google proposed DeepDelta [66], which automatically learns
to suggest ﬁxes for build-time compilation failures. The main advantage of a
learning-based approach over more traditional techniques such as constraint-
based [60,88,108], heuristic-based [90], search-based [47,54,65], or dynamic [56]
and static [22] analysis-based repair, is that it does not require domain-speciﬁc
knowledge about bug patterns and ﬁxes in a given programming language.

To apply learning-based techniques to source code, it needs to be vector-
ized. Before vectorization, it is pertinent to ﬁrst delineate which parts of the
code and in what format will be included as input for vectorization. This is
what we refer to as code representation. As this is an emerging research area,
we still lack a clear understanding of how to best represent source code for
machine learning consumption.

Our work studies how diﬀerent code representations along with embed-
ding methods play a role in automated learning-based program repair. For
our dataset, we focus on name-related bugs, such as swapped function argu-
ments and wrong binary operator, as these have been extensively studied in
the literature [23,48,52,81–83,87,91]. Depending on the usage context of a re-
pair tool, ﬁx suggestions could also be helpful for developers [69]. While some
repair tools could be fully automated at runtime, there are repair-suggestion
tools [13, 34, 37, 41, 46] that are built for human consumption. In this study,
we consider both scenarios, i.e., some code representations could go back to
the actual source code, while others could provide developers with useful de-
bugging hints that is adequate to ﬁx a bug. Through a controlled experiment,
we train 21 models and investigate whether and to what extent changing code
representations and embedding methods can aﬀect the accuracy of the trained
model in generating repair suggestions. In addition to accuracy, we analyze the
perceived usefulness of ﬁxes generated in various code representations through
a user study. The contributions of our work include:

– The ﬁrst systematic controlled experiment to study the eﬀects of six exist-
ing and eight newly proposed code representations, and three embeddings
on the eﬃcacy of patch generation in learning-based program repair.

A Controlled Experiment of Diﬀerent Code Representations

3

– The notion of heterogeneous abstractions for representing the buggy and
ﬁxed code (instead of a homogeneous representation for both), which is
novel to the best of our knowledge for a sequence to sequence program
repair task.

– A qualitative study to assess the perceived usefulness, in terms of how

developers perceive generated ﬁxes in diﬀerent code representations.

– An evaluation on real-world bugs to verify whether the trained models are

applicable to code written by developers.

– A framework, called Reptory [25], which includes our code and dataset,
to assist future comparative experimental studies on deep learning-based
program repair.

Our ﬁndings show that, as expected, a higher level of abstraction leads to
better accuracy, however it can adversely aﬀect the usefulness of the patch.
The notion of mixed-representation is promising as it leads to better learning
accuracy depending on the bug pattern. Additionally, our results highlight
that commonly used evaluation metrics from machine learning, such as model
accuracy, cannot be a sole proxy to measure the eﬃcacy of patch generation;
patchability and perceived usefulness of an inferred ﬁx suggestion are just as
important, which is largely neglected in the literature. Even though current
techniques rely on a single code representation for all bug patterns, our study
highlights that the code representation needs to be selected depending on the
bug type as there is no single code representation that is universally eﬀective
across all bug patterns.

2 Motivating Example

Listing 1 illustrates a real-world code example from the Angular.js project of
a common name-based (aka identiﬁer-based) bug [23, 48, 52, 81–83, 87, 91]. For
the setTimeout function, the arguments must be a callback function and an
integer value as the delay time, respectively. Here, the two expected arguments
are passed in a wrong order in the buggy code, i.e. the ﬁrst argument is a delay
while the second one is a callback function. The ﬁx is to reverse the order of
these two arguments as shown in Listing 1.

Researchers have proposed various ways to represent source code for deep-
learning program repair. Some represent source code as a simple linear se-
quence of tokens [14, 35, 59, 95]. Others leverage the syntactic structure of the
source code using abstract syntax trees (AST) [19, 66].

browserSingleton.startPoller (100 ,

function (delay, fn) {

---
+++
}) ;

setTimeout (delay, fn); \\bug
setTimeout (fn, delay); \\fix

Listing 1: Example of a swapped argument bug

Table 1: Diﬀerent code representations for the example of Listing 1

Representation
ID (RID)

Category Representation

Example

WT1

WT2

DB1

DB2

DB3

FS1

FS2

FS3

FS4

TF1

AST1

AST2

AST3

AST4

d
e
s
a
b

n
e
k
o
T

d
e
s
a
b
T
S
A

Word tokenization [29, 30, 35]

setTimeout ( delay , fn )

Enhanced word tokenization [39, 59]

set <CAMEL> Timeout ( delay , fn )

DeepBugs [83]

DeepBugs - arguments with types

DeepBugs - arguments types only

Function signature

ID setTimeout ( ID delay , ID fn )

ID setTimeout ( ID number delay , ID function fn )

ID setTimeout ( ID number , ID function )

setTimeout ( number , function )

Function signature with position anchors

setTimeout ( arg0 number , arg1 function )

Function signature with LIT/ID

setTimeout ( ID number , ID function )

Function signature with position anchors and
LIT/ID

Renaming Based Abstraction - Tufano et al. [95, 97,
98]

setTimeout ( arg0 ID number , arg1 ID function )

setTimeout ( Number_1 , Method_1 )

AST of original code [1, 5, 11, 36, 66, 94]

AST - arguments with synthesized types

AST - synthesized arguments types only

Program ExpressionStatement CallExpression Identifier setTimeout
Identifier delay Identifier fn

Program ExpressionStatement CallExpression Identifier setTimeout
Identifier number delay Identifier function fn

Program ExpressionStatement CallExpression Identifier setTimeout
Identifier number Identifier function

Pre-order AST of original code [96]

CallExpression Identifier Identifier Identifier

4

M
a
r
j
a
n
e
N
a
m
a
v
a
r
∗

e
t

a
l
.

A Controlled Experiment of Diﬀerent Code Representations

5

Table 1 illustrates the code statement of the example in Listing 1 repre-
sented in various ways. For instance, the ﬁrst row depicts the representation
(WT1) with simple word tokenization, whereas AST1 represents the code us-
ing the AST of the code snippet. In addition, the way the buggy and the ﬁxed
statements are presented can also vary. For example, we can opt for repre-
senting the buggy statement via word tokenization and the ﬁxed statement as
an AST. Considering various formats and ways of combining buggy and ﬁxed
code, there are numerous permutations of the representations imaginable for
learning-based program repair.

Given the various ways code can be represented for vectorization, in this
work, we set out to explore the eﬀect of code representations on learning re-
pairs. Additionally, current studies on learning-based program repair have not
considered the interplay between code representation and the usefulness of a
ﬁx suggestion, to the best of our knowledge. We examine whether a given code
representation preserves all the ﬁx-ingredients to generate the patch automat-
ically. We further explore the importance of viewing patch generation from a
practitioner’s perspective and ask developers to evaluate the eﬀectiveness of a
code representation to write the bug ﬁxing commit.

3 Experimental Design

The goal of our study is to assess the impact of various types of program rep-
resentation on the accuracy of learning name-based bug repair, which reasons
about names based on a semantic representation to suggest a ﬁx for a piece of
code that is an instance of a name-related bug pattern. We, therefore, design
a controlled experiment [107] in which we train a large set of models to com-
pare. Using the same dataset of buggy and ﬁxed programs as input, we change
one factor of the experiment at a time, such as how the code is represented,
and control for all the other variables. We then compute the accuracy of each
trained model against the same test dataset, which the models have not seen
before. Our study addresses the following research questions:

RQ1 What is the inﬂuence of diﬀerent code representations on learning re-

pairs?

RQ2 Do mixed representations of buggy and ﬁxed statements make a diﬀer-

ence in learning repairs?

RQ3 What is the impact of using diﬀerent embeddings on the learning accu-

racy?

RQ4 To what extent are the code representations useful to generate the cor-

rect code?

To answer the research questions, we developed a framework called Rep-
tory (short for Representation Factory) that assists with the automated
name-based bug repair task in four consecutive steps: (1) Dataset Creation: ex-
tracts correct code examples from the given corpus and generates buggy code
examples using simple code transformations (see subsection 3.1). (2) Feature

Table 2: Experimental matrix. Each row trains a new model.

6

EID Independent variable Abstract representation of the buggy code

Abstract representation of the ﬁxed code

Embedding

E1

E2

E3

E4

E5

E6

E7

E8

E9

E10

E11

E12

E13

E14

E15

E16

E17

E18

E19

E20

E21

Representation

Word tokenization (WT1)

Word tokenization (WT1)

Enhanced word tokenization (WT2)

Enhanced word tokenization (WT2)

DeepBugs (DB1)

DeepBugs (DB1)

DeepBugs - arguments with types (DB2)

DeepBugs - arguments with types (DB2)

DeepBugs - arguments types only (DB3)

DeepBugs - arguments types only (DB3)

Function signature (FS1)

Function signature (FS1)

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

Function signature with position anchors (FS2)

Function signature with position anchors (FS2)

Word2Vec-CBOW

Function signature with LIT/ID (FS3)

Function signature with LIT/ID (FS3)

Word2Vec-CBOW

Function signature with position anchors and LIT/ID
(FS4)

Function signature with position anchors and LIT/ID
(FS4)

Renaming based abstraction (TF1)

Renaming based abstraction (TF1)

AST of original code (AST1)

AST of original code (AST1)

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

AST - arguments with synthesized types (AST2)

AST - arguments with synthesized types (AST2)

Word2Vec-CBOW

AST - synthesized arguments types only (AST3)

AST - synthesized arguments types only (AST3)

Word2Vec-CBOW

Pre-order AST of original code (AST4)

Pre-order AST of original code (AST4)

Word2Vec-CBOW

First most accurate representation → buggy code rep-
resentation

Second most accurate representation → ﬁxed code rep-
resentation

Word2Vec-CBOW

Mixed representation

Second most accurate representation → buggy code
representation

First most accurate representation → ﬁxed code rep-
resentation

Word tokenization

AST of original code

Most accurate combination

Embedding

Most accurate combination

Most accurate combination

AST of original code

Word tokenization

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-CBOW

Word2Vec-SG

GloVe

FastText

M
a
r
j
a
n
e
N
a
m
a
v
a
r
∗

e
t

a
l
.

A Controlled Experiment of Diﬀerent Code Representations

7

Extraction: the representations illustrated in Table 1 are used to instantiate an
experimental matrix (see subsection 3.2). The translation of each code sample
into a vector is also performed in this step. (3) Training: trains a model for
each row in our experimental matrix. (4) Evaluation: given a previously un-
seen piece of code, this step applies the trained model obtained in the previous
step to generate ﬁxes if that piece of code contains a bug.

3.1 Dataset of Bugs and Fixes

To create a dataset, we start by using the code corpus provided by Raychev
[84] They have collected open-source JavaScript projects from GitHub,
et al.
deleted duplicate ﬁles or project forks and retained only projects that parse
and are not obfuscated. This process resulted in 150,000 JavaScript ﬁles with
68.6 million lines of code. We chose JavaScript as it is one of the most popular
programming languages.1

As observed by others, the vast amount of existing code provides ample
examples of likely correct code [8,71,85]. In contrast, it is non-trivial to obtain
many code examples that suﬀer from a particular bug pattern. One possible
approach is to manually or semi-automatically search code repositories and
bug trackers for examples of bugs that match a given bug pattern. However,
extending this approach to thousands of samples is highly challenging but
necessary for deep learning models. Instead of manually creating training data,
we generate training data fully automatically from a given corpus of code.

The notion of artiﬁcially creating likely incorrect code relates to mutation
testing [38] and has been applied to introduce security vulnerabilities [21, 80].
To train models, we need a large set of data points of bugs and corresponding
ﬁxes. To obtain such a set, we follow the DeepBugs approach [83], in which the
existing code present in code repositories is considered as the correct version,
and it is automatically mutated using targeted bug patterns to create a buggy
version; it has been shown that mutants are representative of real bugs [4, 26,
40]. DeepBugs employs this synthetic dataset to train a model for predicting
whether previously unseen code is buggy. We use the code-level dataset and
the three types of bug patterns considered in DeepBugs. We use the same three
bug patterns, namely swapped arguments, wrong binary operator, and wrong
binary operands, considered in the DeepBugs work since they are among the
most prevalent name-related bugs [32, 73]. The information gathered in this
step is used in the code representation construction explained in Section 3.3.1.

Swapped arguments. Swapped argument bugs happen when developers ac-
cidentally swap the function call arguments. Listing 1 illustrates an example
of this bug pattern. Since JavaScript is a loosely typed language and type
information cannot be inferred statically, to study the impact of including
type information in code representations, we also synthesize argument types.
Synthesized Argument Types. The intent is to study the impact of including

1 https://insights.stackoverﬂow.com/survey/2021

8

Marjane Namavar∗ et al.

type information in code representations. To this end, for each function, its
arguments’ types are synthesized consistently throughout the whole dataset.
We followed the same approach as in DeepBugs to extract required informa-
tion [83]. Table 3 exhibits information for swapped function arguments.

Table 3: Collected information for swapped arguments

Extracted

Callee (C)

Base (B)

Argument 1 (A1)

Argument 2 (A2)

Arguments (A)

Argument Types (AT)

Synthesized Argument Types
(SAT)

Description

Function name being called.

Base object if the call is a method call; else empty string.

First argument passed to the function.

Second argument passed to the function.

Array containing arguments (with DeepBugs representation).

Array containing types of the arguments; “unknown” if not
inferred.

Synthesize type information assigned to arguments.

Callee, Base, Argument 1, Argument 2, Arguments and Argument Types are
common between this work and DeepBugs. The rest is new in our dataset.

Table 4: Collected information for wrong binary operator

Extracted

Description

Left Operand (LO)

The LHS of a binary operator.

Right Operand (RO)

The RHS of a binary operator.

Correct Operator (CO)

The correct operator.

Buggy Operator (BO)

The buggy operator.

Operands (OPS)

An array of left and right operands with DeepBugs represen-
tation.

Operand Types (OT)

An array containing types of left and right operands.

Wrong Binary Operator. The wrong binary operator bug pattern encom-
passes accidental developer mistakes related to binary operations. To extract
the desired data, we traverse the AST of each ﬁle in the code corpus and
gather the information in Table 4 for each binary operation. Here, Operands,
Operand Types and Operator are similar to what DeepBugs uses. The rest is
newly extracted in our dataset. Correct Operation and Buggy Operation along
with other extracted information will be used to represent code in various
forms that is described in Section 3.3.1.

A Controlled Experiment of Diﬀerent Code Representations

9

Wrong Binary Operands. Wrong binary operands bug pattern pertains to
the problem of swapped operands in binary operations. We followed the same
approach as what DeepBugs did to extract information [83]. Table 5 presents
information for wrong binary operands.

Table 5: Collected information for swapped binary operands

Extracted

Description

Left Operand (LO)

The LHS of a binary operator.

Right Operand (RO)

The RHS of a binary operator.

Operator (O)

The correct operator.

Operands (OPS)

An array of left and right operands with DeepBugs represen-
tation.

Operand Types (OT)

An array containing types of left and right operands.

Data Deduplication. To train a model and evaluate it faithfully, we need
to eliminate duplicate data points [2]. We remove all duplicates from the test
dataset as well as any recurrent data points between the test and training
datasets. Data deduplication is performed after the data extraction phase and
before converting the data points into speciﬁc code representations. For all bug
types, after data extraction explained in 3.1, if all ﬁelds retrieved for two data
points in a particular bug type are similar, they are considered as duplicate
data points and removed.

3.2 Experimental Matrix

Table 2 shows the 21 permutations considered in our controlled experiment.
Experimental runs are shown in their order of execution. Experiments employ
code representations that are introduced in Table 1. EID refers to each ex-
periment ID. In each row of the matrix, one independent variable is changed,
while all the others are kept constant (see section 3.3). For instance, we start
with word tokenization (WT1) in E1 and change the code representation in
row E2–E14. Note that in the ﬁrst 14 rows, the embedding is constant, and
only the representation changes regardless of what embedding method is used
as the goal is to compare code representations while other conditions remain
the same.

At the end of experimental run E14, the two representations that yield the
best results in terms of accuracy are selected. By the best code representations,
we mean the best in terms of accuracy, and throughout this work, the most
accurate and the best is used interchangeably. We mix and match these for
experiments 15 and 16, where we experiment with the eﬀects of heterogeneous
(mixed) representations for the buggy and ﬁxed code. As a comparison, we

10

Marjane Namavar∗ et al.

also include E17 and E18, which contain the mix and match of the two least
abstracted representations. At the end of experimental run E18, the best com-
bination of the code representations for buggy and ﬁxed versions is selected
based on the accuracy of the trained models. To assess the eﬀects of the var-
ious embedding methods, over the next three experimental runs (E19–E21),
the embedding methods are changed for the best selected code representation.
We consider all rows (E1–E21) for the swapped arguments; however, we
omit E6–E9 and E12–E13 for the wrong binary operator and operands bug
types since these code abstractions do not apply to them. Next, we elaborate
on the details of the experimental matrix Table 2.

3.3 Independent Variables (IV)

Multiple factors aﬀect the accuracy of a model that is meant to suggest code
ﬁxes. Below, we describe these factors as independent variables in our experi-
ment.

3.3.1 Code Representation

We can create countless ways of representing code, and it is not feasible to con-
sider all permutations in this work; however, we can deﬁne diﬀerent high-level
representation categories and include examples to cover them. We partition
code representations into two main classes at a high-level, namely token based
and AST based (see Table 1). The least abstracted representations of the two
classes are word tokenization and AST of original code, respectively. In token
based representations, a piece of code is represented with a sequence of tokens.
An AST based representation incorporates structural information of the code.
Both of these categories have two sub-classes where we consider a piece of code
(1) without any modiﬁcations or (2) with modiﬁcations such as adding or re-
moving information. In Table 1 WT1 and WT2 are based on word tokenization
and represent a code string as it is. The next 3 rows (DB1–DB3) related to
DeepBugs representations belong to the second subcategory where we add/re-
move some information to the features. In rows FS1–FS4, we simplify code by
removing some information and adding some high-level extra information such
as type information. TF1 refers to the code abstraction approach presented by
Tufano et al. [95, 97, 98] Finally, AST1–AST4 are tree-based representations.
The cited code representations in Table 1 have been proposed in the literature,
while the remaining code representations are newly presented in this paper.
We consider the following diﬀerent code representations in our study:

Word Tokenization. Deep neural networks applied to source code expect
a vector of tokens as input. Word tokenization is a token based representa-
tion and takes a piece of code without any modiﬁcations (E1 in Table 2) and
has been used extensively in the literature [28, 30, 35]. Enhanced word tok-
enization (E2 in Table 2) considers underscores, camel letters, and numbers as

A Controlled Experiment of Diﬀerent Code Representations

11

separators. In addition, string and number literals except for the most frequent
numbers (0 and 1) are abstracted [39, 59].

DeepBugs Variants. DeepBugs [83] speciﬁes identiﬁer tokens and literal
tokens by adding ID and LIT preﬁxes to them, respectively. The purpose of
E3 is to assess whether complementary information aﬀects learning. Besides
ID and LIT, in E4 we add variable types as well and remove variable values
in E5.

Function Signature Variations. Function signature exhibits token based
representation with modiﬁcation where we abstract away some of the details
of the source code and add some abstract information such as position anchors
to assess the learning eﬀects. E6 is a representation where we only take the
function signature. In E7, while we remove some information and keep the
function signature, we add position anchors for the arguments as well. In E8,
we add abstracted identiﬁer and literal tokens such as ID and LIT to the
function signature. Finally, in E9, we add both position anchors and ID and
LIT to the function signature.

Renaming Based Abstraction. The code presentation used in E10 is bor-
rowed from Tufano et al. [97, 98], which applies identiﬁer-renaming during
abstraction for program-repair. The same approach was also employed to
automatically learn recurring code transformation patterns [95]. Since NMT
models are likely to be ineﬀective when dealing with a large vocabulary size,
code tokens are abstracted by replacing actual identiﬁers/literals with reusable
IDs. The top-300 most frequently occurring tokens are computed ﬁrst. Sub-
sequently, other identiﬁers and literals are replaced with reusable IDs. Each
unique identiﬁer and literal is mapped to an ID, having the form of CATEGORY_#,
where CATEGORY represents the type of identiﬁer or literal (i.e., METHOD, VAR,
NUMBER, STRING) and # is a numerical ID generated sequentially for each
unique type of instance within that category. These IDs are used in place of
identiﬁers and literals in the abstracted code, while the mapping between IDs
and actual identiﬁer/literal values is saved in a map M, which allows us to
map back the IDs in the code. In particular, after the abstraction process, the
vocabulary contains: (i) JavaScript keywords; (ii) top-300 identiﬁers/literals;
(iii) reusable IDs.

AST Variants. AST provides a form of representation of the program struc-
ture that can assist to reason about program syntax and semantics. AST is
leveraged in a variety of learning-based approaches with applications in diﬀer-
ent domains such as code summarization [33] and repair [5, 19, 66]. However,
AST can be represented in a variety of ways. One simple representation is
the Depth First Search (DFS) traversal of the buggy and ﬁxed statements.
However, even this simple representation can have diﬀerent variations, such as
whether type information is embedded into the AST nodes, with or without
the arguments. E11–E14 entail AST based representations. In E11, we take the
inorder traversal of AST of a piece of code without modiﬁcations. In E12, we
embed type information in the AST. Since AST is more complicated than plain
token sequences of code, we also include E13, where arguments are removed,

12

Marjane Namavar∗ et al.

and the representation is more concise and shorter. In E14, a code fragment
is expressed as a stream of the node types that compose its AST [106]. For
extraction, the sub-tree rooted at a speciﬁc node is selected for a given code
fragment. Next, a pre-order visit of the sub-tree is performed, which for each
node encountered, prints its node type.

3.3.2 Mixed representation

The buggy and correct statements can each be represented diﬀerently. This
is what we call heterogeneous mixed representation. For instance, the correct
code can be represented using AST, whereas the buggy code can be repre-
sented using word-level tokenization. To the best of our knowledge, the notion
of mixed representations is novel and has not been explored before in any se-
quence to sequence program repair task. For E15–E16 of Table 2, we mix and
match best representations out of E1–E14. As a comparison, we also add E17
and E18, where only word tokenization (WT1) and AST of the original code
(AST1) are involved. These cover AST based and token based representations.

3.3.3 Embeddings

Our ﬁnal independent variable is embedding. Machine learning requires vector
representations of the data, and embeddings indicate how tokens are repre-
sented as multidimensional numerical vectors.

For E19–E21 in Table 2, we select the best code representation based on
accuracy and only change the embedding. There are mainly two ways for
embedding text; either encode each word present in the text (e.g., Word2Vec
or GloVe) or encode each sentence present in the text (e.g., ELMo [78] or
BERT [86]). Encoding sentences are more useful when there is a large amount
of text data and words have diﬀerent meanings in diﬀerent contexts. In this
study, we focus on learning from short buggy statements without context,
and embedding a sentence as a whole seems irrelevant here. We decided to
keep the context out for two reasons. Firstly, context adds many variations
such as the scope of the context, various abstractions for the contextual data
and diﬀerent ways of representing the context and we decided to investigate
it in more detail in the future. Secondly, some code expressions might have a
richer context while others do not, and it makes the comparison unfair because
there is this factor of context and additional information that we do not have
any control over. The purpose is to show with a speciﬁc amount of data,
which representation brings more value and helps the training process more.
Therefore we concentrate only on word embedding methods. We incorporate
the following embeddings in our experiments:
Word2Vec. Word2Vec [67, 68] is one of the most popular word embedding
methods where the meaning of a word can be derived from the various contexts
in which this word is used. Word2Vec has been studied heavily in diﬀerent
source code processing tasks [15,106]. We use both architectures of Word2Vec,
namely CBOW and Skip-gram (SG), in this work.

A Controlled Experiment of Diﬀerent Code Representations

13

GloVe. Word2Vec ignores the fact that some contextual words occur more
often than others and also it only takes into consideration the local context
and hence failing to capture the global context. GloVe [77] is an unsupervised
learning algorithm for generating word embeddings by aggregating the global
word-word co-occurrence matrix from a corpus [77]. Although GloVe is gaining
more traction in natural language processing, to the best of our knowledge, it
has not been applied to source code before.

FastText. One drawback of existing embedding methods is that they do not
take into account the morphology of words and assign a distinct vector to
each word. FastText [9] addresses this limitation by proposing a new approach
based on the skipgram model. Each word is represented as a bag of character
n-grams and a vector representation is dedicated to each character n-gram.
Then words are represented as the sum of these representations. Wainakh et
al. compare ﬁve embeddings and FastText outperforms all [103].

3.4 Dependent Variables (DV)

Dependent variables are the eﬀects of changing independent variables. We
measure the following dependent variables in our experiment:

BLEU Score. To measure ﬁx suggestion quality, we use the Bilingual Eval-
uation Understudy (BLEU) score [76]. The BLEU score is a value between
0–100. It measures how close the generated output is to the expected output.
We used BLEU-4, and smooth BLEU is computed following the method out-
lined in [51]. We show the BLEU score as reported by the NMT model on the
best-generated prediction (top-1) in the test dataset.

Position. For models that are capable of returning multiple suggestions, such
as NMT, we measure the position of the ﬁrst accurate suggestion. We collect
the position of the ﬁrst accurate suggestion in the list of all inferred sugges-
tions by the model, for each bug. We report average position for each code
representation.

Accuracy. Accuracy is measured as the percentage of unseen test dataset that
the model is able to suggest an expected output for. As we employ an NMT
model that is capable of returning multiple suggestions, we consider it correct
if any one of these suggested candidate ﬁxes matches the expected output.
The accuracy is measured based on the given code representation for the ﬁxed
version. For instance, consider E3 in Table 2 where DB1 is used as the code
representation to train the model; here the expected output is also in the DB1
format.

Accuracy, compares the produced output of the model with the expected
output while they are in the same code representation. It is an indicator that
shows how much accurate the produced outputs are but it does not express if
the produced output is automatically patchable, meaning it can be converted
to the actual ﬁxed code automatically. Considering the notion of patchability
this question arises that in case that the produced output is not automatically

14

Marjane Namavar∗ et al.

patchable, would it be useful to developers while debugging? Another impor-
tant question is how much eﬀort is needed to convert the produced output
into the actual patchable code? To address these issues, we have involved the
following three metrics.

Automatic Patchability. Automatic Patchability is a binary metric that
indicates whether a given code representation can generate a bug ﬁxing commit
automatically. It determines if the generated suggestion, in whatever code
representation, can be automatically converted to a valid patch, which in fact
can be applied to the buggy code.

Code representations can encompass varied levels of information, which
make them lossless or lossy. Some code representations embed additional in-
formation, such as type information, that might not be readily available in the
source code. For example, DB1 and DB2 enrich the source code (WT1) with
LIT/ID and type, respectively. Renaming based abstractions such as TF1 can
also be transformed back to the actual code using an additional post-processing
mapping steps. Such code representation transformations are lossless, i.e., they
preserve all the necessary ingredients for an automatic repair. There are also
representations that are lossy, i.e., they remove information through abstrac-
tion and might not contain the essential ingredients to ﬁx the bug automati-
cally. For instance, DB3 ignores arguments and only keeps type information.
We use lossless and automatically patchable interchangeably in this work.

Perceived Usefulness. While a lossy generated output might not be possible
to convert to the actual code-level ﬁx, it might still be useful as a debugging
suggestion to developers. Also, an aggressive code abstraction might help the
learning-based approach to extract repeating patterns from the code corpora,
however, the suggestions might lose their usefulness from a developer’s per-
spective as it might not contain the required repair ingredients. Diﬀerent stud-
ies [14,95] have adopted a varied level of abstraction in the literature, without
addressing how useful such abstractions can be in the real world. Therefore,
we also assess the notion of the perceived usefulness of a generated output
through a user study.

Edit Distance. Edit distance measures the number of insertions, deletions
and substitutions to transform the produced output into the actual patch-
able piece of code and it might indicate the eﬀort required to transform the
generated ﬁx suggestion into the actual valid code.

We can divide all representations in Table 1 into two categories: Cat1: to-
ken based and Cat2: AST based. The least abstracted representation in Cat1
belongs to word tokenization (WT1), while the least abstracted representation
of Cat2 is AST of original code (AST1). From the developer’s perspective, a
bug-ﬁxing commit can be generated from these least abstracted representa-
tions (WT1, AST1) with the least amount of eﬀort, and they are the closest
code representations to the actual ﬁxes. Following previous studies [20, 105],
we measure edit distance (i.e., Levenshtein distance) of Cat1 and Cat2 code
representations with WT1 and AST1, respectively, to measure the closeness to
the actual ﬁx. For example, we calculate edit distance for token-based variant

A Controlled Experiment of Diﬀerent Code Representations

15

DB1 with WT1. On the other hand, for Cat2 variant AST3 the edit-distance
calculation is performed with the least abstracted AST representation, AST1.
A distance of 0 indicates that patches are identical. The higher the value, the
more eﬀort is required to obtain the actual ﬁx in the source code.

3.5 Extraneous Variables (EV)

All variables, which are not deﬁned as independent variables, but could poten-
tially aﬀect the results (DV) of the experiment are called extraneous variables.
Such variables should be controlled where possible. In our controlled experi-
ment, model hyper-parameters such as the number of layers, units, training
steps, batch size, dropout rate, vocabulary size, and beam width are made
constants after hyperparameter optimization. The machine and environment
used for all the training and inferences also remain immutable.

3.6 Training Models

Researchers have recently leveraged neural machine translation (NMT) to
learn patterns from existing source code. For example, SequenceR [14] and
DeepDelta [66] employ NMT to learn repair patterns, while Tufano et al. [95]
use NMT to learn code changes. NMT uses the encoder-decoder architecture
with two diﬀerent neural networks. The encoder network encodes the input into
thought vectors. The decoder network takes this encoded data as input and
decodes it to produce the desired output. Once the NMT model is trained,
given previously unseen buggy code, ﬁx suggestions can be inferred. In our
experiments, we deploy Tensorﬂow NMT, which uses a deep multi-layer recur-
rent neural network that is unidirectional and LSTM as a recurrent unit [57].
Attention mechanism [6, 58] enables the neural network with a wider atten-
tion span to certain parts of inputs while generating the output. We enable
the scaled Luong attention [58] in this work. We train our models to min-
imise the cross-entropy loss and then back-propagate the loss to update the
model parameters. The Adaptive Gradient Optimizer (Adam) [45] is used to
update the network weights of our model. We use a beam width of 25 for the
inference step to generate a list of the most likely candidate sequences. All
our experimental runs are performed on an Intel(R) Xeon(R) CPU 2.50GHz
machine with 62 GB RAM. We randomly split our dataset into test and train-
ing, with 50,000 and 100,000 ﬁles, respectively. From the training set, 10% is
used as the validation set. Following the deduplication steps described in 3.1,
the test dataset for swapped arguments was reduced from 752,937 to 123,323
unique data points. After deduplicating 1,670,881 samples in the test dataset
for wrong binary operator and wrong binary operands, the ﬁnal test dataset
contained 142,503 instances. Table 7 provides the number of data points in
the training, validation, test, and real datasets for the three bug patterns in
our study. We have performed an evaluation on a real-bugs dataset explained

16

Marjane Namavar∗ et al.

Table 6: Hyperparameters and the best conﬁguration

Hyperparameter Possible Values

Tuned Value

Embedding size
Encoder layers
Decoder layers
Number of units
Learning rate
Dropout
Vocabulary size

[128, 256, 512]
[2, 3, 4]
[2, 3, 4]
[128, 256, 512]
[0.0, 0.001, 0.01, 0.1, 0.2, 1.0]
[0.0, 0.1, 0.2, 0.4, 0.5]
[5K, 10K, 20K, 30K]

512
2
2
512
0.001
0.1
30K

in subsection 5.6. We train 21 separate models, one for each row of Table 2
(E1–E21).

Hyperparameter optimization. Hyperparameters can aﬀect the perfor-
mance of an NMT model, and that is why researchers optimize for their values.
However, since we are conducting a controlled experiment, we need to control
for these values across models trained with various code representations. To
this end, we opted for tuning with the word tokenization (WT1) code rep-
resentation as it is the least abstracted. Therefore, we constructed a search
space of various conﬁgurations shown in Table 6. We selected values for each
hyperparameter by examining previous studies [7, 99]. We trained 50 diﬀerent
models with WT1 to determine the optimal values. Following that, we trained
10 models with the tuned set and compared them with multiple trained mod-
els with random values for the remaining code representations. The tuned set
outperformed the random sets in all cases. Therefore, we employed the tuned
values from Table 6 as an approximation for the optimal set of hyperparameter
values to conduct our controlled experiment. With the optimal conﬁguration
for WT1, we found little improvement in the BLEU score after 25K steps.
Although more abstracted representations may reach their best BLEU scores
earlier than 25K steps, as this is a controlled study, we decided to keep the
same stopping criterion for all models. Thus, the stopping criterion for train-
ing is the number of training steps that is ﬁxed at 25k. The training time
varies from 41.32 hours (E11) for the wrong binary operator bug type to 4.75
hours (E14) for the swapped arguments bug type. The average training time
for swapped arguments, wrong binary operator, and wrong binary operands is
21.97, 18.05, and 17.55 hours.

4 Patchability and Usefulness Study

The ultimate goal of an automated program repair system is to assist develop-
ers with bug ﬁxing tasks. As shown in Table 1, learning-based techniques could
employ a plethora of lossless and lossy code representations to represent the
buggy and the ﬁxed code. While some lossy representations can help extract
the recurring pattern, and DL models can learn recurring source code pat-
terns, they could not automatically produce the ﬁxed code. To address RQ4,
we examine whether a suggested ﬁx is (1) automatically patachable, and (2)

A Controlled Experiment of Diﬀerent Code Representations

17

useful from developers’ perspective for debugging. Fundamentally, when the
code is abstracted, it helps the model to extract ﬁx patterns, whereas the de-
veloper might ﬁnd the generated ﬁx as not useful. An end-user might tolerate
abstraction to some extent while receiving enough cues. The goal of the user
study is to ﬁnd out whether the abstracted representations provide enough
hints to the developer to write the ﬁnal patch to be useful. To understand this
notion of usefulness from a developer’s perspective, we conducted a survey.

The patchability and user study are available online as part of our replica-

tion package Reptory [25].

4.1 Bug Selection

Our selection of bugs is based on the following factors: (1) Bugs from the same
type can diﬀer in their complexity and might need a diﬀerent amount of eﬀort
to be ﬁxed. To keep the comparison of diﬀerent representations equitable,
we present the same bug in diﬀerent representations. (2) It is not feasible
to consider all bugs from our dataset. Thus we have to limit the number of
samples from each bug type.

The authors have done the automatic patchability study while developers
answered the questions of the user study. With the above design considerations
in mind, we have created two pools of samples for each study, and we explain
relevant details in the following two subsections.

4.2 Automatic Patchability Study

To assess the patchability of ﬁx suggestions, the authors manually investigated
the random samples from each code representation to understand whether
they are automatically patchable, meaning the ﬁx suggestions could be trans-
formed to the actual source code in an automated way. We have 14 diﬀerent
code representations for swapped arguments, 8 code representations for the
wrong binary operator, and 8 code representations for wrong binary operands,
which leads to a total of 30 code representations from three bug types. We have
selected ﬁve random bugs from each code representation, leading to a pool of
150 (5 ∗ 30) questions. Each question shows the buggy code and actual correct
code, and ﬁx suggestions in a given code representation. The authors exam-
ine whether the actual ﬁxed code could be generated from the ﬁx suggestion.
For example, from token-based code representation DB1, ID/LIT could be re-
moved to transform the code into the original patchable code. DB3 only keeps
type information that could not be automatically transformed back to the ac-
tual code. For the AST-based representation, type information in AST2 could
be removed from its representation, and the resulting AST could be translated
back to the original code. However, for AST3, arguments are not preserved
and cannot be transformed automatically. Each of the authors assessed and
categorized the ﬁx suggestions independently to understand whether ﬁx sug-
gestions could be converted back to the actual ﬁxed code. If the scoring diﬀered

18

Marjane Namavar∗ et al.

between authors, they discussed it and reached a consensus on the automatic
patchability of a code representation.

4.3 User Study

Despite some of the representations being lossy, they could still be helpful. The
main purpose of the user study is to assess the usefulness of lossy code repre-
sentations from the developer’s point of view because there is no automated
way or metric to investigate that. Therefore, we measure the perceived useful-
ness of each of the lossy code representations, individually. On the other side,
all lossless (automatically patchable) code representations can be converted to
the actual ﬁxed code which is similar to the word tokenization code represen-
tation. Hence, we only involve the samples from the word tokenization (WT1)
code representation as a representative of all lossless code representations.

We have 7, 2 and 2 lossy code representations for the swapped arguments,
wrong binary operator and operands, respectively. Also, we have word tok-
enization for each bug type. Therefore, there are 8, 3 and 3 code represen-
tations involved in the user study for the swapped arguments, wrong binary
operator and operands, respectively. In total, 14 (8+3+3) code representations
are involved. For each code representation, 5 samples have been randomly se-
lected. Finally, we have 70 (5 ∗ 14) samples in a pool of samples used for the
user study.

We selected 131 top JavaScript repositories that were also used by previ-
ous work [32]. These repositories are a combination of most depended-upon
npm packages, most starred-by users npm packages and most starred projects.
We chose contributors with equal to or more than 10 contributions for these
top projects. Committing to a project’s default branch, posting an issue, and
proposing a pull request are all examples of contributions. There could be mul-
tiple collaborators for a GitHub repository. If a contributor’s email address was
available in their GitHub account, we contacted them by email. We invited
405 people to participate and they were not given any beneﬁts. We provided
guidelines on (a) how to understand the survey questions, (b) how to score a
patch. 44 developers completed the study. On average, our participants had 6.8
years of general software development and 3.4 years of JavaScript experience.
Every participant received 10 randomly selected questions from our pool of
questions. In each question, a buggy code, the corresponding developer ﬁx, and
a ﬁx suggestion produced by Reptory were shown. Survey participants were
asked to give a score between 1–5 (Likert-type) to indicate to what extent
a ﬁx suggestion is helpful to convert the buggy code into the actual ﬁxed
code. As we are oﬀering a ﬁx suggestion system and what it produces is a
ﬁx suggestion that acts as a hint for developers, they need to evaluate how
diﬃcult it is to go from the buggy version to the ﬁxed version using the ﬁx
suggestion. 1 means not useful at all, and 5 means absolutely useful. There
was an open-ended question at the end of the survey to receive additional
comments about ﬁx suggestions. From the completed responses, we collected

A Controlled Experiment of Diﬀerent Code Representations

19

Table 7: Statistics for the three bug types

Bug Type

Train

Val

Test

Real

Swapped Arguments
Wrong Binary Operator
Wrong Binary Operands

1,343,669
3,051,474
3,051,474

149,297
339,053
339,053

123,323
142,503
142,503

23
37
35

perceived usefulness values from the participants and averaged them for each
representation of each bug type to achieve the perceived usefulness score of a
given code representation.

5 Results

Tables 8–10 present our results for the three diﬀerent bug patterns. The tables
are colour-coded, and in each column, the darker the colour, the better the
result for that dependent variable. For example, higher accuracies and lower
positions are darker.

EID in these tables refers to the experimental ID. For instance, EID(SA) of
1 in Table 8 presents our results of the experiment for swapped arguments with
EID as 1 from Table 2. The RID column indicates which code representations
from Table 1 are involved for that experimental run. For experiments with
mixed representations, both representations for the buggy and ﬁxed statements
are presented.

5.1 Code Representations (RQ1)

RQ1 pertains to the inﬂuence of diﬀerent code representations, and we consider
E1–E14 here. As the function signature-related representations do not apply
to the wrong binary operator and operands bug patterns, several rows are
omitted in Table 9 and Table 10.

Perceived usefulness of AST4 representation. In Tables 8–10, E14 with
AST4 representation has the highest accuracy of 100 percent. However, in
terms of edit distance and perceived usefulness, it is signiﬁcantly worse than
the second-worst outcome for all three categories of bugs. For swapped argu-
ments, AST4 achieves the lowest perceived usefulness score (1.0) in Table 11.
The developer survey deemed this representation to be ineﬀective for all bug
categories and it is not automatically patchable as well. Furthermore, devel-
opers suggested in the survey comments that this code representation does
not convey meaningful information. In Table 11 edit distance reaches its worst
value of 88.1 in swapped arguments and 70.7 in wrong binary operator and
operands, where the second-worst numbers are by far smaller. Therefore, we
contemplate E14 (shown in grey colour) as an outlier and do not consider it
to select the ﬁrst and second-best representations in terms of accuracy.

20

Marjane Namavar∗ et al.

Table 8: Results - Swapped Arguments

)
A
S
(
D
I
E

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

D
I
R

WT1
WT2
DB1
DB2
DB3
FS1
FS2 (2nd most accurate)
FS3
FS4
TF1 (1st most accurate)
AST1
AST2
AST3
AST4
TF1 → FS2
FS2 → TF1
WT1 → AST1
AST1 → WT1
TF1 (Word2Vec-SG)
TF1 (GloVe)
TF1 (FastText)

)

%
(
y
c
a
r
u
c
c
A

20.367
40.078
20.867
20.843
44.271
50.617
60.912
48.872
28.534
99.805
16.405
18.833
57.706
100.000
46.666
70.725
19.913
20.690
99.822
99.964
99.825

U
E
L
B

55.6
66.2
69.6
72.4
68.3
67.0
78.8
69.3
69.9
99.9
68.6
73.9
85.2
100.0
76.5
75.9
70.3
56.1
99.9
100.0
99.9

n
o
i
t
i
s
o
P

1.4
1.2
1.3
1.3
5.3
2.8
2.3
5.6
2.9
1.0
2.0
1.6
1.3
1.0
2.0
2.9
1.6
1.3
1.0
1.0
1.0

Representation

Mixed
Representation

Embedding

5.1.1 Swapped Arguments

Considering the ﬁrst 14 rows in Table 8, and ignoring AST4, the ﬁrst and
second highest accuracies reached are 99.805% and 60.912% for experiments
E10 and E7, which used Tufano et al. [95,97,98] (TF1) and function signature
with position anchors (FS2) as code representations, respectively. Enhanced
word tokenization in E2 leads to a better accuracy compared to E1. Although
the accuracy in DeepBugs representations with actual values (E3–E4) was
close to E1, where word tokenization was used, there was a jump in accuracy
once function arguments were abstracted away in E5. Also, function signatures
in E6–E8 improved accuracy. In E11 and E12, using AST did not help much
with accuracy, but interestingly accuracy increased sharply in E13 with the
combination of AST and abstraction. The highest BLEU score among the ﬁrst
14 experiments (E1–E14) was 99.9 in E10.

A Controlled Experiment of Diﬀerent Code Representations

21

Table 9: Results - Wrong Binary Operator

)
t
r
p
O
(
D
I
E

D
I
R

WT1
1
WT2
2
DB1
3
DB2
4
DB3 (1st most accurate)
5
10 TF1 (2nd most accurate)
11
14
15
16
17
18
19
20
21

AST1
AST4
DB3 → TF1
TF1 → DB3
WT1 → AST1
AST1 → WT1
DB3 (Word2Vec-SG)
DB3 (GloVe)
DB3 (FastText)

)

%
(
y
c
a
r
u
c
c
A

19.270
28.579
20.175
20.186
85.978
85.787
16.570
100.000
51.513
85.922
10.667
17.659
85.983
99.977
85.988

U
E
L
B

18.1
11.3
30.0
42.3
50.1
95.4
60.0
100.0
67.4
58.1
52.4
8.4
50.4
50.9
50.4

n
o
i
t
i
s
o
P

3.5
3.3
3.2
3.2
3.2
2.7
3.7
1.0
5.2
2.8
3.9
3.6
3.3
3.6
3.2

Representation

Mixed
Representation

Embedding

The best two positions for the swapped arguments belong to E10 and E2
using Tufano et al. [95, 97, 98](TF1) and enhanced word tokenization (WT2)
code representations, respectively.

5.1.2 Wrong Binary Operator and Operands

As shown in Table 9 and Table 10 in E1–E14, the ﬁrst and second top accura-
cies for both wrong binary operator and operands are yielded by experiments
E5 and E10. Analogous to swapped arguments, enhanced word tokenization
improves accuracy. In these two categories of bugs, on the one hand, accuracy
experienced minor improvement as we enriched word tokenization (WT1) with
further information in variations of DeepBugs representations (E3–E4). On the
other hand, the best accuracy was achieved once we removed arguments in E5
with DeepBugs representation with arguments types only (DB3). Using AST
in E11 decreased accuracy compared to all previous rows.

The highest BLEU score for the wrong binary operator was 95.4 (E10),
which was similar to the binary operands where the BLEU score reached 95.1
(E10).

22

Marjane Namavar∗ et al.

Table 10: Results - Wrong Binary Operands

)
d
n
p
O
(
D
I
E

D
I
R

WT1
1
WT2
2
DB1
3
DB2
4
DB3 (1st most accurate)
5
10 TF1 (2nd most accurate)
11
14
15
16
17
18
19
20
21

AST1
AST4
DB3 → TF1
TF1 → DB3
WT1 → AST1
AST1 → WT1
DB3 (Word2Vec-SG)
DB3 (GloVe)
DB3 (FastText)

)

%
(
y
c
a
r
u
c
c
A

20.474
30.625
20.899
20.912
85.993
85.817
17.789
100.000
61.582
85.995
11.363
20.673
85.993
99.996
85.992

U
E
L
B

28.4
20.6
46.2
58.1
90.4
95.1
71.8
100.0
83.1
89.5
63.9
27.6
89.5
98.9
89.4

n
o
i
t
i
s
o
P

1.4
2.1
1.4
1.4
1.0
1.0
1.8
1.0
3.4
1.0
1.6
1.4
1.0
1.0
1.0

Representation

Mixed
Representation

Embedding

5.2 Heterogeneous Representations (RQ2)

To address RQ2, we examine whether mixed representations for buggy and
ﬁxed code statements assist in learning ﬁxes. We consider E15–E18 here. For
swapped arguments, we mix and match representations TF1 and FS2 as they
provided the ﬁrst and second best accuracies of 99.805 and 60.912, respectively.
In Table 8, E16 achieved one of the highest accuracies of 70.725% among all.
However, E15 yielded the best BLEU score of 76.5, while E16 reached a BLEU
score of 75.9. Despite having a substantially lower accuracy, E17 and E18
reached the best positions.

For the wrong binary operator and operands, representations DB3 and TF1
were employed. In Table 9 and Table 10, E16 produced top accuracies of 85.922
and 85.995 along with relatively high BLEU scores of 58.1 and 89.5 for wrong
binary operator and operands, respectively. Although E15 has relatively high
accuracy, it oﬀers the worst outcome for the position for both wrong binary
operator and operands.

5.3 Embeddings (RQ3)

To answer RQ3, we compared diﬀerent embeddings for learning repairs. For
swapped arguments, E10 with abstraction as adopted by Tufano et al. [95, 97,
98] reached a high accuracy of 99.805. Diﬀerent embeddings resulted in minor

A Controlled Experiment of Diﬀerent Code Representations

23

Table 11: Results - Usefulness Study for All Bug Types

Swapped Arguments

Wrong Binary Operator

Wrong Binary Operands

Usefulness Edit Distance Usefulness Edit Distance Usefulness Edit Distance

Automatic

Patchability

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

4.4

4.4

4.4

4.4

4.4

4.4

4.4

3.3

2.7

3.4

3.6

3.2

2.1

1.0

0.0

27.7

17.1

27.1

30.0

0.0

14.4

34.5

25.0

30.2

28.6

35.3

25.3

88.1

3.2

3.2

3.2

3.2

3.2

3.2

-

3.7

-

-

-

-

-

0.0

13.9

6.7

22.0

15.5

0.0

-

23.4

-

-

-

-

-

4.2

4.2

4.2

4.2

4.2

4.2

-

3.6

-

-

-

-

-

0.0

13.9

6.7

22.0

15.5

0.0

-

23.4

-

-

-

-

-

1.1

70.7

1.2

70.7

RID

WT1

WT2

DB1

DB2

TF1

AST1

AST2

DB3

FS1

FS2

FS3

FS4

AST3

AST4

improvements in E19, E20, and E21, as E10 already has high accuracy. GloVe
(E20) yielded the best accuracy for swapped arguments. Similarly, GloVe (E20)
reached the accuracy of 99.977% and 99.996% for the wrong binary operator
and operands, respectively, which was the highest accuracy overall. The BLEU
score for the embedding experiments (E19–E21) was also relatively high for
all bug types.

5.4 Patchability and Usefulness (RQ4)

The results for our patchability and usefulness assessment are demonstrated
in Table 11. For each code representation, the table shows its automatic
patchability (binary), usefulness score, and edit distance. FS1–FS4 and AST2–
AST3 do not apply to wrong binary operator and operands. Edit distance is
measuring the eﬀort needed to transform the generated ﬁx suggestion to the
actual code.

Token based variants (WT1–WT2, DB1–DB2, TF1) and AST variants
(AST1–AST2) are automatically patchable as they preserve all the relevant in-
formation to ﬁx a bug. As lossless transformations could go back to the actual
source code, edit-distance does not apply to these representations. Lossy code
transformation in token variants (DB3, FS1–FS4) and AST variants (AST3–
AST4) are not automatically patchable as they remove information such as
function arguments.

Participants found most suggestions to be useful, which is demonstrated by
the results from our survey. For swapped arguments in Table 11, automatically
patchable code representations had the highest perceived usefulness of 4.4. FS3
achieved the second-highest perceived usefulness score of 3.6 even though its
edit distance is 28.6. Comparing FS3 and FS1, FS1 has perceived usefulness

24

Marjane Namavar∗ et al.

score of 2.7 while having a lower edit distance of 25.0. Thus closeness in string
similarity does not necessarily align with the developer’s perceived useful-
ness as including additional information could provide helpful debugging aids.
For the wrong binary operator, a lossy representation with DeepBugs variant
(DB3) yielded the best perceived usefulness of 3.7 compared to the score of 3.2
for automatically patchable code representations. The highest perceived use-
fulness score for the wrong binary operands reached 4.2 for the automatically
patchable representations.

For mixed representations (E15–E18), the usefulness score stems from the
representation of the ﬁxed code. For swapped arguments, wrong binary oper-
ator, and wrong binary operands, the best perceived usefulness score achieved
by mixed representations are 4.4, 3.7, and 4.2, respectively, which are amongst
the best in the corresponding bug types.

5.5 Overall Best and Worst Representations

Overall, the best results were achieved in EID(SA) of 20 where the code repre-
sentation for swapped arguments was TF1 with GloVe embedding and reached
the accuracy of 99.964%, BLEU score of 100, position of 1.0, edit distance of
30.0 and the highest perceived usefulness score of 4.4. TF1 is also automat-
ically patchable. Ignoring AST4, the worst overall results were yielded by
EID(Oprt) of 17 where the mix of WT1 and AST1 representations is used,
with an accuracy of 10.667%.

5.6 Evaluation on Real Bugs

To investigate whether (1) various code representations are useful in ﬁxing
real-world bugs, and (2) a model learned using artiﬁcial training data can
help to repair real bugs, we apply the trained models of all rows in Table 2
to a set of real-world bugs for all the three types of bugs. These bugs are
provided in the DeepBugs paper [83]. The number of bugs in each bug pattern
is shown in the last column of Table 7, and Table 12 illustrates our results.
For swapped arguments, we reach 100% accuracy using the renaming based
abstracted representation (TF1). For the wrong binary operator, 64.86% is
the highest accuracy achieved. 97.14% is the best accuracy for wrong binary
operands as well. Advanced embedding methods help with accuracy, and GloVe
is the best among the four embedding methods. We only reported accuracy
for the real bugs because the purpose of evaluation on them is to show that a
model learned using artiﬁcial training data can help to repair real bugs, but
complete results of all metrics for real bugs have already been included in the
replication package [25].

A Controlled Experiment of Diﬀerent Code Representations

25

Table 12: Results - Real Bugs

EID Accuracy(SA) Accuracy(oprt) Accuracy(opnd)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

52.17
30.43
52.17
52.17
39.13
34.78
43.48
43.48
21.74
100.00
52.17
52.17
43.48
100.00
21.74
60.87
52.17
52.17
100.00
100.00
100.00

64.86
40.54
62.16
64.86
59.46
-
-
-
-
59.46
59.46
-
-
100.00
18.92
59.46
00.00
59.46
59.46
59.46
59.46

88.57
68.57
91.43
88.57
94.29
-
-
-
-
60.00
88.57
-
-
100.00
40.00
94.29
00.00
88.57
91.43
97.14
91.43

6 Discussion

In this section, we will discuss the results presented in Section 5 and remark
on noticeable trends.

6.1 BLEU is not always correlated with accuracy.

BLEU score is the ﬁrst metric measured and provided by the NMT engine
itself. Our experimental results do not always indicate a constant correlation
between the BLEU score and accuracy. For instance, in Table 9 and Table 10,
going from E3 to E4 accuracy and BLEU score both increase, however, going
from E2 to E3, accuracy drops, but the BLEU score rises. The reason might
be due to the way the BLEU score is calculated. Suppose that reference is
the actual ﬁx and candidate is what we generate as the ﬁx suggestion; the
BLEU score is a fraction where the numerator is the number of words from
the candidate that are found in the reference, and the denominator is the total
number of words in the candidate. For example, if the reference has four words
while the candidate has only two words and both of them are present in the
reference the BLEU score will be 100. Although accuracy is not 100% here.

26

Marjane Namavar∗ et al.

6.2 Bug type aﬀects the functionality of the code representation.

Various code representations impact accuracy and perceived usefulness diﬀer-
ently depending on the bug type. For instance, one situation that shows the
role of the bug type can be observed where the eﬀect of abstraction on the
wrong binary operator is diﬀerent from the other two bug types. The reason
stems from the nature of this bug, i.e., the operator is important, and remov-
ing actual values does not have a negative impact. Moreover, if we only look
at the darker rows of the color-coded result tables, we can see that the be-
haviour of the wrong binary operator and operands is quite similar in terms
of accuracy, BLEU score, and position, while it is diﬀerent in swapped ar-
guments and that is another observation that indicates the essential role of
the bug type in code representation for deep-learning. Another example of
the role of bug types can be observed in the wrong binary operator results in
Table 11, where DB3 can reach a relatively high perceived usefulness of 3.7
while it is one of the most abstract representations (lossy) we have. We think
this originates from the nature of this bug type. To ﬁx binary operator bugs,
type information is enough to correct the operator, and the values of operands
are not needed. At the same time, removing actual values oﬀers a concise and
clean suggestion to developers. These results suggest that depending on the
bug type, even a high level of abstraction can be helpful. Although current
techniques [14, 19, 35, 39, 59, 66, 97] employ a single code representation for all
bug types, our ﬁndings suggest that there is no single best code representa-
tion applicable to all bug types. This result indicates that neural models with
multi-modal representations of source code could be more eﬀective for learning
repairs across bug types.

6.3 More abstraction leads to higher accuracy.

Across all the bug patterns, we observe that higher abstractions in code rep-
resentation help the model in terms of accuracy. For example, for swapped
arguments, TF1 in E10 of Table 8 yielded the best accuracy, where the model
did not need to learn the actual values. The same trend holds for the wrong
binary operator and operands where E5 produced the best results with the
DeepBugs representations with arguments types only (DB3). This is expected
as with abstraction, there is limited vocabulary, and learning is easier as the
model can exploit the recurring patterns. Also, removing actual values and
keeping types, introduces repetitions in the input that helps the model to
learn better.

6.4 More abstraction adversely inﬂuences perceived usefulness.

Developers need to interpret, transform and apply a generated ﬁx suggestion
to the buggy code. While abstraction helps accuracy, it can adversely inﬂu-
ence the perceived usefulness of ﬁx suggestions. For the swapped arguments

A Controlled Experiment of Diﬀerent Code Representations

27

bug type, FS1 has the worst perceived usefulness among token based represen-
tations in Table 11. Similarly, for the wrong binary operands in Table 11, DB3
shows the worst perceived usefulness, and we know FS1 and DB3 are among
the most abstract representations. For the AST based representations in all
bug types, AST with an aggressive level of abstraction (AST4) produces the
least useful suggestions.

6.5 Lossy representations can still be helpful.

In some code representations, such as word tokenization (WT1), a generated
ﬁx suggestion possesses all the information needed to ﬁx the bug. At the same
time, some ﬁxes generated with lossy representations, such as function signa-
ture (FS1), cannot be applied to the buggy code directly because they have
lost some information, and it is impossible to generate the exact valid code
from the ﬁx suggestion. We refer to the lossless code representations that are
convertible to the actual code as automatically patchable code representations.
Similarly, lossy code representations are not automatically patchable. There-
fore, we need another step to determine if the lossy code representations are
completely useless or not. As a result, the developer study is performed. Our
user study ﬁnds that lossy ﬁx suggestions might still be valuable during de-
bugging tasks for developers. Consider the following real-world example from
our experiment:

\\Actual buggy version:

setTimeout ( delay , fn )

\\Actual fixed version:

setTimeout ( fn , delay )

\\The developer sees the following as a fix suggestion:

setTimeout ( ID function , ID number)

Listing 2: Example of a ﬁx suggestion for Listing 1 in FS3 representation.

This inference was helpful to developers because it provides a useful de-
bugging cue about how to invoke a function with the correct argument order.
Lossy code representations abstract away some information, leading to higher
accuracy while they are not automatically patchable. Therefore, the high ac-
curacy might be misleading, and that is where the perceived usefulness comes
into play. For instance, we can assume the focus is not on the accuracy, and
we use the perceived usefulness score to select the best code representation.
Among the lossy code representations, function signature with LIT/ID (FS3)
produces the highest perceived usefulness score for the swapped arguments
bug type in Table 11. Also, among the lossy code representations, DeepBugs
with arguments types only (DB3) shows the highest perceived usefulness for
both wrong binary operator and operands. The lossless code representations
share the same perceived usefulness per bug type because all of them could be
automatically converted into the actual valid code. Generally, the perceived
usefulness of lossless code representations is higher than lossy ones except for
the wrong binary operator bug type. The reason stems from the nature of

28

Marjane Namavar∗ et al.

this bug where the incorrect operator is the bug, and it does not matter if the
operands are abstracted. Our work highlights that in addition to accuracy, the
usefulness of the model output has to be assessed, especially when a patch is
not automatically convertible. Indeed, the purpose of doing the user study was
exactly to address this issue and observe if the abstract ﬁx suggestions (lossy)
help developers.

6.6 String relatedness is not aligned with perceived usefulness.

As our results in Table 11 show, for FS1 and FS3 code representations, the
edit distance is 25.0 and 28.6, respectively. On the other hand, their perceived
usefulness scores are 2.7 and 3.6, respectively. Even though FS1 was closer to
the actual ﬁx, it yielded a lower perceived usefulness score because additional
information in FS3 helps developers with extra debugging aids. A similar ob-
servation could be made for the DB3 and FS1 code representations. DB3 has
a higher usefulness score of 3.3 even though FS1 was closer to the actual ﬁx in
terms of the edit distance. Thus, the perceived usefulness of a code representa-
tion from a real developer’s perspective does not align with the eﬀort needed
to convert the generated ﬁx suggestion to the actual code. One code represen-
tation might be more useful while it takes more eﬀort to be transformed into
the actual code.

6.7 Heterogeneous representation could outperform homogeneous code
representation.

Heterogeneous code representation could be eﬀective across all three bug types.
Mixed representations, in which ﬁxed code is more abstracted than buggy code,
could yield higher accuracy than homogeneous representations. For swapped
arguments, E16 presents a heterogeneous code representation comprised of FS2
as buggy code and TF1 as the ﬁxed code representation. FS2 representation is
enriched with function signature and position anchors, whereas TF1 abstracts
away method names, variables, and literals and does not have additional in-
formation. Hence, TF1 has less information and is more abstract compared to
FS2. When E16 (FS2 → TF1) is compared to E7 (FS2 → FS2), E16 has a
greater accuracy of 70.725, while E7 has a lower accuracy of 60.912. As we go
from a less abstract code representation (more information) to a more abstract
code representation (less information), the likelihood of improving inference
accuracy increases. However, if we go from a more abstract buggy code repre-
sentation, we do not keep all the ﬁx ingredients for less abstract representation
during inference and accuracy drops. E16 has higher accuracy than E7 because
TF1 is more abstract than FS2, and we observe that a heterogeneous code rep-
resentation outperforms a homogeneous code representation. The same trend
remains valid for the wrong binary operator and operands. Since our user study
showed that developers could tolerate some level of abstraction, heterogeneous

A Controlled Experiment of Diﬀerent Code Representations

29

representation could be chosen over homogeneous representation (with a more
abstract code representation for the ﬁxed version of code) to improve accuracy.

6.8 Ordering matters in heterogeneous code representations.

In mixed representation, using a lower abstraction (more information) for the
buggy code along with a higher abstraction for the ﬁxed code (less information)
improves the accuracy. For swapped arguments, E15 goes from TF1 (more
abstract, less information) to FS2 (less abstract, more information), yields an
accuracy of 46.666. However, E16 employs the inverse order and goes from
FS2 to TF1, resulting in a signiﬁcantly higher accuracy of 70.725. Similarly,
E18 (AST1 → WT1) yields an accuracy of 20.690, which is higher than 19.913
in E17 (WT1 → AST1). The same pattern holds for wrong binary operator
and operands, indicating that ordering of code representations play a role in
heterogeneous code representations.

6.9 Advanced embedding methods improve learning.

Word2Vec-SG does not seem to aﬀect the accuracy in comparison with Word2Vec-
CBOW. GloVe yields the best accuracy for all the bug types in our study. This
could be explained by the fact that GloVe focuses on word co-occurrences over
the whole corpus, and its embeddings relate to the probabilities that two words
appear together. Therefore, it produces more accurate embeddings for source
code. FastText does not improve the accuracy as much as GloVe does.

6.10 Correct patches can be generated for most bugs in both test and real
datasets.

Although we conducted a controlled experiment and compared various code
representations in this study, the overall results indicate that deep-learning
can learn ﬁxes for name-based bugs. With the right code representation and
embedding, for swapped arguments, wrong binary operator, and wrong binary
operands, correct patches can be generated for more than 97% of the bugs in
both test and real datasets with one exception where in the real bugs dataset,
wrong binary operator bugs are ﬁxed with the accuracy of 64.86%.

6.11 Controlled experiments in DL systems are challenging.

The performance and accuracy metrics for a learning-based model are inﬂu-
enced by the hyperparameters. While numerous hyperparameters can be tuned
for diﬀerent code representations individually, this is not feasible in a controlled
experiment where the intent is to keep everything constant and observe the

30

Marjane Namavar∗ et al.

eﬀect of changing code representations. In this work, we encountered a num-
ber of challenges pertaining to conducting controlled experiments for deep
learning-based analysis, namely (1) there is a vast number of factors that need
to be controlled for, such as neural network architectures, values for hyper-
parameters, and embedding methods (e.g., Word2Vec, GloVe, FastText), in
addition to various code abstractions and representations; the permutations
of these variables populate a large optimization learning matrix, unlike any-
thing we have seen in the literature thus far; (2) training models on large
datasets is time-consuming and computationally expensive. Overall, conduct-
ing controlled experiments to compare deep-learning models is a challenging
task due to the vast number of variables involved. However, we believe it is
an essential vehicle for proper empirical comparison in such an emerging and
fast-changing area where new ideas are proposed increasingly.

6.12 Threats to Validity

This section outlines how we mitigated potential limitations that may have
biased our ﬁndings.

6.12.1 External validity

Choice of programming languages. In this work, we study the impact of
code representation in JavaScript. Since we do not rely on language-speciﬁc
features, our ﬁndings should apply to other programming languages, although
more controlled experiments are needed in other languages to empirically as-
sess this.

Selection of bug types. Another concern could be that this study is limited
to speciﬁc bug patterns. We attempted to mitigate the generalizability threat
by incorporating three bug types used widely in the literature [10, 42, 43, 83,
103], which are also among the most prevalent JavaScript bug categories [32,
73]. In addition, many existing learning-based bug detection tools are trained
and tested on these bug-types [10, 83, 92]. As a result, studying these bug
categories is both representative and signiﬁcant. However, we do not claim or
generalize that empirical results would be directly applied to more complex
bug types, especially in conjunction with more diﬀerent deep learning models
such as Transformers [101] and Graph Neural Network [109]. However, we
believe our work is the ﬁrst step toward controlled experiementation and our
framework, Reptory, is designed to allow the addition of other bug patterns
for experimentation as described in details in Section 3.1.

Model selection. We opted for the NMT model, which is well studied in
the recent literature [14, 35, 95, 97, 98]. We did not compare our model(s) with
other ML models such as Transformers [101], Graph Neural Network [109], or
Tree-LSTM [72]. The goal of this work is not to compare diﬀerent models with
the best performance; rather, we evaluate the eﬀect of code representations in

A Controlled Experiment of Diﬀerent Code Representations

31

a controlled setting and for that, we have adopted a widely used NMT-based
architecture. Moreover, adding more models is computationally costly (for
training the models on diﬀerent code representations) but may lead to other
interesting observations. For future comparisons, our replication package can
be used as a starting point.

Contextual information, graphical representations of code and learn-
ing models. Context pertaining to the buggy statement could play a role in
the representation, which we have not considered in this study. Context can be
extracted in various levels of granularity such as the buggy statement [35, 83],
surrounding statements, enclosing function [59, 105], class [14, 97], enclosing
ﬁle [33], or encapsulating AST subtrees [96, 110].

After choosing the granularity of context from this wide array of options,
there are numerous ways of modelling source code to exploit syntactic and
semantic relations. For example, an abstract syntax tree could represent the
syntactical structure, whereas semantics information could be derived from
control ﬂow and data ﬂow graphs. Furthermore, there could be numerous
variations of combining the syntactical and semantics representations. One
can fuse semantic information, e.g., control ﬂow with the abstract syntax tree,
data ﬂow with the abstract syntax tree, or both.

Intuitively, graph neural networks could eﬀectively model such represen-
tations and learn from graphical representations of code. There are diﬀerent
graph neural networks [112] with varying levels of expressiveness. In the con-
text of GNN, expressiveness means their ability to represent diﬀerent graph
structures [89]. GraphSage [31], for example, can scale well to large source
code input but is not expressive enough since, when aggregating information,
it does not use all of the node neighbours but only samples some of them. On
the other hand, Graph Attention Network (GAT) [102] uses the information
from all its neighbours along with the attention mechanism. There are also
variants that diﬀer in how node and edge features are leveraged.

Understanding the impact of granularity of contextual information, context
representation using diﬀerent graphical representations, and the interplay with
varying deep-learning models is a non-trivial task. We believe future work
could explore the possibility of combining syntactic information with deeper
contextual information, which could then be used to more comprehensively
test the capabilities of the diﬀerent learning models to further understand
the intricacies of doing learning based program repair. We believe our work
is the ﬁrst to indicate the intricacies and challenges of performing controlled
experiments on learning-based task on source code and the importance of
such endeavours. Our replication package is available and could be expanded
in future studies on context.

Contextual embedding. The use of embeddings to represent textual words
as vectors has emerged as a powerful technique for associating meanings with
words and comparing words for similarity based on such meanings. In this
work, we focus on static word representation models to obtain embeddings
that are not dependent on their surrounding code tokens or code statements

32

Marjane Namavar∗ et al.

(context). Recently, contextual embedding models [17, 64, 79] in natural lan-
guage processing have been adopted, which can embed contextual information
as well as word representations, making them more expressive than static word
representations. Subsequently, the notion of contextual embedding is applied
to source code and contextualized embeddings such as CodeBERT [24], Cu-
BERT [42], and SCELMo [44] have shown promise in many diﬀerent source
code tasks. In contrast in this work, we compare four static word embed-
dings namely, Word2Vec-CBOW, Word2Vec-SG, FastText, and GloVe, which
is consistent with prior work [103]. A gradual progression would be to devise a
carefully designed comparative study on contextual embedding in conjunction
with diﬀerent granularities of contextual information.
Mutated dataset. Our work performs mutations to generate faulty examples
from likely correct code that may not represent real-world bugs. However, the
notion of artiﬁcially creating likely incorrect code relates to mutation testing
[38], and it is shown that mutants are quite representative of real bugs in
general [4,26,40]. Furthermore, our evaluation shows that models trained with
artiﬁcially seeded bugs are also eﬀective for repairing real-world bugs.
Real bugs dataset. The dataset of real bugs is small. The goal here is to eval-
uate whether trained models could be eﬀective on real-world bugs. The dataset
for real bugs is derived from DeepBugs [83] which performs bug prediction.
Here, we present a generative model for a program repair task to produce ﬁx
suggestions on a real bug dataset. Although the dataset of real bugs is small,
we obtain similar results.
Code representations and bug types. We investigate function signature-
related representations, given that they do not apply to all selected bug pat-
terns. In terms of applicability, two categories of code representations exist.
Some representations are applicable to all bug types [59] such as word tok-
enization, but another category of code representations has been used in the
literature, which is speciﬁc to a particular bug type [90]. To make the experi-
ment more comprehensive, we involved both kinds of representations.

6.12.2 Internal validity

Perceived usefulness. Another potential threat, which is currently present
in many learning-based techniques, would be focusing only on the accuracy
metric. While the more abstracted representation (lossy) reaches a higher
accuracy, it can produce a less useful bug ﬁx. To minimize this threat, we
conducted a user study to assess the perceived usefulness of the suggestions,
in addition to accuracy.
Selection criterion for mixed representations. We decided to use accu-
racy for the selection since it is an objective, repeatable indicator widely used
in the literature [14, 19, 29, 30, 59, 63, 66, 95]. We discuss that edit distance is
not aligned with the developer perceived usefulness. The BLEU score is not
a good proxy either, as observed in other recent studies [20]. Also, the use-
fulness metric could be perceived as subjective due to the nature of the user

A Controlled Experiment of Diﬀerent Code Representations

33

study design. If we combine all dependent variables using the same weight i.e.,
Accuracy∗BLEU Score∗(1/P osition)∗U sef ulness∗(1/EditDistance), for the swapped ar-
guments, WT1 would yield the best result. AST1 would become the second
best representation because of the lower edit distance. As we combine all the
scores, the ﬁrst best and second-best pair would change for the mixed repre-
sentations (EID15 and EID16) from (TF1, FS2) to (WT1, AST1) in Table 8.
However, if we do not consider edit distance in the combined score for auto-
matically patchable code representations, TF1 would still yield the best result.
Regardless, this would not impact our ﬁndings and discussions on mixed repre-
sentation, as to why mixed representation could be eﬀective and why ordering
is important for mixed representations.

Hyperparameter tuning. One approach for hyperparameter optimization
could be to tune each code representation from Table 2. Finding an optimal
setting can be computationally expensive due to the large search space of all
available hyper-parameters, diﬀerent variations of code representations and
diﬀerent bug types. Overall, this would incur an exorbitantly high cost for
training numerous models. Furthermore, employing diﬀerent hyperparameters
would introduce additional variability into our controlled experiment. Hence,
we opted for tuning for the code representation WT1, which is closest to the
actual code. We veriﬁed that the tuned parameters with the least abstracted
code representation work better than the random sets of hyperparameter val-
ues for other code representations. As a result, we kept the hyperparameter
values constant throughout the entire experimental matrix in our controlled
experiment.

Synthesized types. For function signature related representations, we have
incorporated synthesized types for variables. These synthesized types are not
necessarily always the actual type, which might raise a concern. However, the
goal here was not to correctly infer types in JavaScript; our goal was to observe
what would happen if we abstracted out and used higher-level information
from the code. The synthesized types for arguments are added consistently
such that argument types are always the same for a particular function.

Reproducibility. In terms of the reproducibility of our results, we have made
Reptory [25], our framework and dataset, available, along with instructions
to use it for replication or future experimental comparisons of other code
representations.

7 Related Work

We will discuss related work in this section. Firstly, the name-related pro-
gram analysis summarises previous research in the topic. We present diﬀerent
embeddings applied to source code processing tasks. Then we discuss the trans-
lation and edit-based models for learning-based program repair. Followed to
that, we outline existing comparative research on ML for SE. Since we are
conducting a controlled experiment and comparing various code representa-

34

Marjane Namavar∗ et al.

tions, we are interested to review previous research that has compared diﬀerent
components involved in the learning-based source code analysis.

Name-related program analysis. There is a wide array of research on
name-related program analysis [3, 23, 48, 52, 74, 75, 81–83, 87, 91]. There have
been attempts to use the rich information embedded in source code identiﬁers
to perform method naming [3], infer API speciﬁcations [74, 75], and detect
argument defects [52, 87]. Recent studies also applied the learning-based ap-
proach to detect name-based bugs [48,83]. To the best of our knowledge, this is
the ﬁrst work on a generative task on name-related bugs, rather than focusing
on a predictive model.

Embeddings on source code processing tasks. Word embeddings pro-
vide a way to represent words as vectors such that words that are closer to
the vector spaces are related in meaning. In natural language processing word
embedding techniques diﬀerent embedding techniques, Word2Vec, Glove, and
FastText, have been proposed. Word embedding techniques like Word2Vec
have been heavily adopted in learning-based source code processing. Learning
Word2vec embedding from token sequences is applied to diﬀerent source code
processing tasks, namely for bug detection [83], type prediction [61], or vul-
nerability detection [93]. Chen et al. [16] provide a more in-depth investigation
of source code embeddings. In natural language processing, contextual embed-
ding [55] like BERT [17], and ELMo [79] is proposed where instead of static
word embedding, higher-quality embeddings could be learned from the usage
context. Recently, these contextualized word embeddings are also adopted in
source code processing [24,42,44]. In this work, we focus on static word embed-
ding and compare 4 embedding techniques. Contextualized word embedding
with varying granularities of contextual information is a logical progression of
this work that could be built on the Reptory framework.

Deep learning-based program repair. Recently, neural machine transla-
tion (NMT) based architectures have been employed to generate bug ﬁxes [35,
95, 97, 98]. SequenceR [14] employed the sequence-to-sequence model in con-
junction with the copy mechanism to further improve the eﬀectiveness of NMT
based model. To improve the repair technique, CoCoNuT [59] adopts CNN to
encode the faulty method and generates the patch token by token. Techniques
from this stream of work consider the bug-ﬁx learning problem as sequence-
based code generation and harness the power of sequence-to-sequence based
models. In recent work, Ding et al. [20] looked critically at using a neural
translation-based approach for the program repair task. In contrast to these
translation-based models, several edit-based techniques have been proposed
which perform code edits on the buggy code. Instead of sequence-based cor-
rect code generation, edit-based approaches [12, 111] generate the sequence of
edits given the buggy piece of code. Instead of token-based representation of
source code, Hoppity [19] performs edit in the abstract syntax tree (AST).
Hoppity trains a graph edit model using GNN-based architecture and itera-
tively transforms the AST of the buggy code into the AST of the ﬁxed code.

A Controlled Experiment of Diﬀerent Code Representations

35

The goal of these approaches is to propose diﬀerent program repair tech-
niques using a varied level of code representation, embedding, and learning
models. Instead in this work, we focus on a controlled setting wherein we ex-
amine the eﬀect of code representation on the learning model in a systematic
manner.

Comparative studies on ML for SE. A few studies have attempted to
understand the eﬀect of changing factors, such as code embedding or neural
network architecture, on various software engineering tasks such as bug detec-
tion, method name prediction, and code cloning. Wainakh et al. [103] compare
ﬁve diﬀerent identiﬁer embeddings in the source code and show to what ex-
tent each of them captures the relatedness, similarity, and contextual similarity
between identiﬁers. COSET [104] oﬀers a benchmarking framework consisting
of a diverse dataset of programs in the source-code format, labeled by hu-
man experts. It trains four diﬀerent neural network architectures proposed for
software classiﬁcation tasks and uses prediction accuracy to measure the clas-
siﬁcation results. Kanade et al. [42] use pre-trained contextual embeddings of
source-code as well as Word2Vec for ﬁve classiﬁcation tasks and compare the
performance of these two approaches.

To predict if a piece of code is clean or vulnerable, Li et al. [49] compare
six diﬀerent neural network architectures and ﬁve various embedding methods.
Most of the previous studies focus on predictive tasks and classiﬁcation prob-
lems. A previous work [95] that concentrates on generative tasks, investigates
whether NMT can be used to learn code transformations on a method-level
granularity. As part of the evaluation, this study compared the eﬀect of using
small and medium-sized methods on the accuracy of the generated code.

Other studies compare diﬀerent APR tools as part of their evaluation sec-
tion [39, 50, 53, 59]. Their target is not to do a controlled experiment but to
propose a method and compare it with existing approaches. Unlike our study,
none of these studies conducts experiments in a controlled way and explores
the eﬀect of diﬀerent changing factors on learning performance as well as on
each other. Instead, they take some factors, mutate them independently and
observe the results.

A study conducted by Tufano et al. [96] applied four diﬀerent represen-
tations of source-code to detect code similarities. This work is the closest to
ours, where the impact of code representation is studied, albeit for a com-
pletely diﬀerent task. In this paper, we consider three changing factors, code
representations, mixed representations, and embeddings. The main focus is on
code representations, and once the code representation is ﬁxed, we study how
mixed representations and embeddings aﬀect code representations to generate
ﬁx suggestions. Therefore, the novelties of this work are the combination of
three ideas: (1) coverage of the main code representation categories, namely
token based and AST-based, (2) comparison in a controlled way where three
independent variables are involved, and (3) comparison for a generative task,
namely, patch generation.

36

8 Conclusion

Marjane Namavar∗ et al.

Learning-based software engineering is gaining traction in research and de-
velopment. However, there is a lack of clear understanding of how to best
represent source code for learning-based tasks. In this paper, we studied the
eﬀects of code representations and embedding methods in 21 diﬀerent ways
on name-based program repair tasks through a controlled experiment. We
demonstrated that string relatedness is not a good proxy to assess the patch-
ability of inferred ﬁxes. We qualitatively evaluated the perceived usefulness of
a code representation in ﬁx suggestions, which is overlooked in the learning-
based repair literature. Our results indicate that a higher abstraction level in-
creases accuracy while decreasing perceived usefulness depending on the bug
type. However, lossy representation could still yield useful debugging hints
to the developers. The notion of heterogeneous code representation, proposed
in this paper for a sequence to sequence repair task, can outperform homo-
geneous code representations. The results of our user study emphasize the
need to develop learning-based repair tools with practitioners in mind. The
best accuracy achieved for swapped arguments, wrong binary operator and
wrong binary operands are 99.964% (Table 8, EID(SA)-20), 99.977% (Table 9,
EID(Oprt)-20) and 99.996% (Table 10, EID(Opnd)-20), respectively, where
each test dataset has more than 120k data points in Table 7. Reptory can
be used as a benchmarking framework to perform comparative studies when
considering or proposing new code representations for learning-based tasks.

We intend to expand our work to cover other bug types and develop a
conﬁgurable infrastructure in the future. Such an infrastructure can be fed
by various independent variables, then run experiments and generate results
based on desired dependent variables. Developing an automatic evaluation
metric for source code is another promising idea to follow. Another possibility
is to combine context with single-statement bugs and contextual embedding
techniques to investigate their impact on repair tasks.

References

1. Ahmed, T., Devanbu, P., Hellendoorn, V.J.: Learning lenient parsing & typing via

indirect supervision. Empirical Software Engineering 26(2) (2021)

2. Allamanis, M.: The adverse eﬀects of code duplication in machine learning models of
code. In: ACM SIGPLAN International Symposium on New Ideas, New Paradigms,
and Reﬂections on Programming and Software, Onward! 2019, p. 143–153. Association
for Computing Machinery (2019)

3. Allamanis, M., Barr, E.T., Bird, C., Sutton, C.: Suggesting accurate method and class
names. In: 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE
2015, p. 38–49. Association for Computing Machinery (2015)

4. Andrews, J.H., Briand, L.C., Labiche, Y.: Is mutation an appropriate tool for testing
experiments? In: 27th International Conference on Software Engineering, p. 402–411.
Association for Computing Machinery (2005)

5. Bader, J., Scott, A., Pradel, M., Chandra, S.: Getaﬁx: Learning to ﬁx bugs automati-

cally. Proc. ACM Program. Lang. 3(OOPSLA) (2019)

A Controlled Experiment of Diﬀerent Code Representations

37

6. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate. In: 3rd International Conference on Learning Representations,
ICLR 2015 (2015)

7. Bengio, Y.: Practical recommendations for gradient-based training of deep architec-
tures. In: Neural Networks: Tricks of the Trade - Second Edition, Lecture Notes in
Computer Science, vol. 7700, pp. 437–478. Springer (2012)

8. Bielik, P., Raychev, V., Vechev, M.: Phog: Probabilistic model for code.

In: 33rd
International Conference on International Conference on Machine Learning - Volume
48, ICML’16, p. 2933–2942. JMLR.org (2016)

9. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with sub-
word information. Transactions of the Association for Computational Linguistics 5,
135–146 (2017)

10. Briem, J.A., Smit, J., Sellik, H., Rapoport, P., Gousios, G., Aniche, M.: Oﬀside: Learn-
ing to identify mistakes in boundary conditions. In: IEEE/ACM 42nd International
Conference on Software Engineering Workshops, ICSEW’20, p. 203–208. Association
for Computing Machinery (2020)

11. Brody, S., Alon, U., Yahav, E.: A structural model for contextual code changes. Proc.

ACM Program. Lang. 4(OOPSLA) (2020)

12. Chakraborty, S., Ding, Y., Allamanis, M., Ray, B.: Codit: Code editing with tree-based

neural models. IEEE Transactions on Software Engineering pp. 1–1 (2020)

13. Chandra, S., Torlak, E., Barman, S., Bodik, R.: Angelic debugging. In: 33rd Inter-
national Conference on Software Engineering, ICSE ’11, p. 121–130. Association for
Computing Machinery (2011)

14. Chen, Z., Kommrusch, S.J., Tufano, M., Pouchet, L., Poshyvanyk, D., Monperrus,
M.: Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE
Transactions on Software Engineering pp. 1–1 (2019)

15. Chen, Z., Monperrus, M.: The remarkable role of similarity in redundancy-based pro-

gram repair. arXiv preprint arXiv:1811.05703 (2018)

16. Chen, Z., Monperrus, M.: A literature study of embeddings on source code. arXiv

preprint arXiv:1904.03061 (2019)

17. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional
transformers for language understanding. In: North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, NAACL-HLT,
pp. 4171–4186. Association for Computational Linguistics (2019)

18. Devlin, J., Uesato, J., Singh, R., Kohli, P.: Semantic code repair using neuro-symbolic

transformation networks. arXiv preprint arXiv:1710.11054 (2017)

19. Dinella, E., Dai, H., Li, Z., Naik, M., Song, L., Wang, K.: Hoppity: Learning graph
transformations to detect and ﬁx bugs in programs. In: 8th International Conference
on Learning Representations, ICLR 2020. OpenReview.net (2020)

20. Ding, Y., Ray, B., Devanbu, P., Hellendoorn, V.J.: Patching as translation: The data
and the metaphor. In: 35th IEEE/ACM International Conference on Automated Soft-
ware Engineering, ASE ’20, p. 275–286. Association for Computing Machinery (2020)
21. Dolan-Gavitt, B., Hulin, P., Kirda, E., Leek, T., Mambretti, A., Robertson, W., Ulrich,
F., Whelan, R.: Lava: Large-scale automated vulnerability addition. In: 2016 IEEE
Symposium on Security and Privacy (SP), pp. 110–121 (2016)

22. Durieux, T., Monperrus, M.: Dynamoth: Dynamic code synthesis for automatic pro-
In: 2016 IEEE/ACM 11th International Workshop in Automation of

gram repair.
Software Test (AST), pp. 85–91 (2016)

23. Eshkevari, L.M., Arnaoudova, V., Di Penta, M., Oliveto, R., Gu´eh´eneuc, Y.G., Anto-
niol, G.: An exploratory study of identiﬁer renamings. In: 8th Working Conference on
Mining Software Repositories, MSR ’11, p. 33–42. Association for Computing Machin-
ery (2011)

24. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T.,
Jiang, D., Zhou, M.: CodeBERT: A pre-trained model for programming and natural
languages.
In: Findings of the Association for Computational Linguistics: EMNLP
2020, pp. 1536–1547. Association for Computational Linguistics (2020)

25. The impact of code representation on learning-based repair replication package.

https://github.com/annon-reptory/reptory (2021)

38

Marjane Namavar∗ et al.

26. Gopinath, R., Jensen, C., Groce, A.: Mutations: How close are they to real faults? In:
2014 IEEE 25th International Symposium on Software Reliability Engineering, ISSRE
’14, p. 189–200. IEEE Computer Society (2014)

27. Gupta, R., Kanade, A., Shevade, S.K.: Deep reinforcement learning for programming

language correction. arXiv preprint arXiv:1801.10467 (2018)

28. Gupta, R., Pal, S., Kanade, A., Shevade, S.: Deepﬁx: Fixing common c language errors
by deep learning. In: Thirty-First AAAI Conference on Artiﬁcial Intelligence, AAAI’17,
p. 1345–1351. AAAI Press (2017)

29. Gupta, R., Pal, S., Kanade, A., Shevade, S.: Deepﬁx: Fixing common c language errors
by deep learning. In: Thirty-First AAAI Conference on Artiﬁcial Intelligence, AAAI’17,
p. 1345–1351. AAAI Press (2017)

30. Hajipour, H., Bhattacharyya, A., Fritz, M.: Sampleﬁx: Learning to correct programs by
eﬃcient sampling of diverse ﬁxes. In: NeurIPS 2020 Workshop on Computer-Assisted
Programming (2020)

31. Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on large
graphs. In: 31st International Conference on Neural Information Processing Systems,
NIPS’17, p. 1025–1035. Curran Associates Inc. (2017)

32. Hanam, Q., Brito, F.S.d.M., Mesbah, A.: Discovering bug patterns in javascript. In:
24th ACM SIGSOFT International Symposium on Foundations of Software Engineer-
ing, FSE 2016, p. 144–156. Association for Computing Machinery (2016)

33. Haque, S., LeClair, A., Wu, L., McMillan, C.: Improved automatic summarization of
subroutines via attention to ﬁle context. In: 17th International Conference on Mining
Software Repositories, MSR ’20, p. 300–310. Association for Computing Machinery
(2020)

34. Hartmann, B., MacDougall, D., Brandt, J., Klemmer, S.R.: What would other pro-
grammers do: Suggesting solutions to error messages. In: SIGCHI Conference on Hu-
man Factors in Computing Systems, CHI ’10, p. 1019–1028. Association for Computing
Machinery (2010)

35. Hata, H., Shihab, E., Neubig, G.: Learning to generate corrective patches using neural

machine translation. arXiv preprint arXiv:1812.07170 (2018)

36. Hu, X., Li, G., Xia, X., Lo, D., Jin, Z.: Deep code comment generation. In: 26th Con-
ference on Program Comprehension, ICPC ’18, p. 200–210. Association for Computing
Machinery (2018)

37. Jeﬀrey, D., Feng, M., Gupta, N., Gupta, R.: Bugﬁx: A learning-based tool to assist
developers in ﬁxing bugs. In: 2009 IEEE 17th International Conference on Program
Comprehension (ICPC 2009). IEEE Computer Society (2009)

38. Jia, Y., Harman, M.: An analysis and survey of the development of mutation testing.

IEEE Transactions on Software Engineering 37(5), 649–678 (2011)

39. Jiang, N., Lutellier, T., Tan, L.: Cure: Code-aware neural machine translation for
In: 2021 IEEE/ACM 43rd International Conference on

automatic program repair.
Software Engineering (ICSE), pp. 1161–1173. IEEE Computer Society (2021)

40. Just, R., Jalali, D., Inozemtseva, L., Ernst, M.D., Holmes, R., Fraser, G.: Are mutants
a valid substitute for real faults in software testing? In: 22nd ACM SIGSOFT Inter-
national Symposium on Foundations of Software Engineering, FSE 2014. Association
for Computing Machinery (2014)

41. Kaleeswaran, S., Tulsian, V., Kanade, A., Orso, A.: Minthint: Automated synthesis of
repair hints. In: 36th International Conference on Software Engineering, ICSE 2014,
p. 266–276. Association for Computing Machinery (2014)

42. Kanade, A., Maniatis, P., Balakrishnan, G., Shi, K.: Learning and evaluating contex-
tual embedding of source code. In: 37th International Conference on Machine Learn-
ing, ICML 2020, Proceedings of Machine Learning Research, vol. 119, pp. 5110–5121.
PMLR (2020)

43. Karampatsis, R.M., Sutton, C.: How often do single-statement bugs occur? the
manysstubs4j dataset. In: 17th International Conference on Mining Software Reposi-
tories, MSR ’20, p. 573–577. Association for Computing Machinery (2020)

44. Karampatsis, R.M., Sutton, C.: Scelmo: Source code embeddings from language mod-

els. arXiv preprint arXiv:2004.13214 (2020)

45. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International

Conference on Learning Representations, ICLR (2015)

A Controlled Experiment of Diﬀerent Code Representations

39

46. Koyuncu, A., Liu, K., Bissyand´e, T.F., Kim, D., Monperrus, M., Klein, J., Le Traon,
Y.: Iﬁxr: Bug report driven program repair. In: 27th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software En-
gineering, ESEC/FSE 2019, p. 314–325. Association for Computing Machinery (2019)
47. Le Goues, C., Nguyen, T., Forrest, S., Weimer, W.: Genprog: A generic method for
IEEE Transactions on Software Engineering 38, 54–72

automatic software repair.
(2012)

48. Li, G., Liu, H., Jin, J., Umer, Q.: Deep learning based identiﬁcation of suspicious
return statements. In: 2020 IEEE 27th International Conference on Software Analysis,
Evolution and Reengineering (SANER), pp. 480–491 (2020)

49. Li, X., Wang, L., Xin, Y., Yang, Y., Chen, Y.: Automated vulnerability detection in
source code using minimum intermediate representation learning. Applied Sciences
10, 1692 (2020)

50. Li, Y., Wang, S., Nguyen, T.N.: Dlﬁx: Context-based code transformation learning for
automated program repair. In: ACM/IEEE 42nd International Conference on Software
Engineering, ICSE ’20, p. 602–614. Association for Computing Machinery (2020)
51. Lin, C.Y., Och, F.J.: Orange: a method for evaluating automatic evaluation metrics
In: The 20th International Conference on Computational

for machine translation.
Linguistics, pp. 501–507. COLING (2004)

52. Liu, H., Liu, Q., Staicu, C.A., Pradel, M., Luo, Y.: Nomen est omen: Exploring and
exploiting similarities between argument and parameter names. In: 38th International
Conference on Software Engineering, ICSE ’16, p. 1063–1073. Association for Comput-
ing Machinery (2016)

53. Liu, K., Koyuncu, A., Bissyand´e, T.F., Kim, D., Klein, J., Traon, Y.L.: You cannot
ﬁx what you cannot ﬁnd! an investigation of fault localization bias in benchmarking
automated program repair systems. 2019 12th IEEE Conference on Software Testing,
Validation and Veriﬁcation (ICST) pp. 102–113 (2019)

54. Liu, K., Koyuncu, A., Kim, K., Kim, D., Bissyand´e, T.F.: Lsrepair: Live search of
In: 2018 25th Asia-Paciﬁc Software

ﬁx ingredients for automated program repair.
Engineering Conference (APSEC), pp. 658–662. IEEE Computer Society (2018)
55. Liu, Q., Kusner, M.J., Blunsom, P.: A survey on contextual embeddings. arXiv preprint

arXiv:2003.07278 (2020)

56. Long, F., Rinard, M.: Automatic patch generation by learning correct code. In: Pro-
ceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL ’16, p. 298–312. Association for Computing Machin-
ery (2016)

57. Luong, M., Brevdo, E., Zhao, R.: Neural machine translation (seq2seq) tutorial.

https://github.com/tensorﬂow/nmt (2017)

58. Luong, T., Pham, H., Manning, C.D.: Eﬀective approaches to attention-based neural
machine translation. In: 2015 Conference on Empirical Methods in Natural Language
Processing, pp. 1412–1421. Association for Computational Linguistics (2015)

59. Lutellier, T., Pham, H.V., Pang, L., Li, Y., Wei, M., Tan, L.: Coconut: Combining
context-aware neural translation models using ensemble for program repair. In: 29th
ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA
2020, p. 101–114. Association for Computing Machinery (2020)

60. Malik, M.Z., Siddiqui, J.H., Khurshid, S.: Constraint-based program debugging using
In: 2011 Fourth IEEE International Conference on Software

data structure repair.
Testing, Veriﬁcation and Validation, pp. 190–199 (2011)

61. Malik, R.S., Patra, J., Pradel, M.: Nl2type: Inferring javascript function types from
natural language information. In: 2019 IEEE/ACM 41st International Conference on
Software Engineering (ICSE), pp. 304–315 (2019)

62. Marginean, A., Bader, J., Chandra, S., Harman, M., Jia, Y., Mao, K., Mols, A., Scott,
A.: Sapﬁx: Automated end-to-end repair at scale. In: 2019 IEEE/ACM 41st Interna-
tional Conference on Software Engineering: Software Engineering in Practice (ICSE-
SEIP), pp. 269–278 (2019)

63. Marginean, A., Bader, J., Chandra, S., Harman, M., Jia, Y., Mao, K., Mols, A., Scott,
A.: Sapﬁx: Automated end-to-end repair at scale. In: 41st International Conference on
Software Engineering: Software Engineering in Practice, ICSE-SEIP ’19, p. 269–278.
IEEE Press (2019)

40

Marjane Namavar∗ et al.

64. McCann, B., Bradbury, J., Xiong, C., Socher, R.: Learned in translation: Contextu-
alized word vectors. In: Advances in Neural Information Processing Systems, vol. 30.
Curran Associates, Inc. (2017)

65. Mehne, B., Yoshida, H., Prasad, M., Sen, K., Gopinath, D., Khurshid, S.: Accelerating
search-based program repair. 2018 IEEE 11th International Conference on Software
Testing, Veriﬁcation and Validation (ICST) pp. 227–238 (2018)

66. Mesbah, A., Rice, A., Johnston, E., Glorioso, N., Aftandilian, E.: Deepdelta: Learning
to repair compilation errors.
In: 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering,
ESEC/FSE 2019, p. 925–936. Association for Computing Machinery (2019)

67. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word representa-
tions in vector space. In: 1st International Conference on Learning Representations,
ICLR 2013, Workshop Track Proceedings (2013)

68. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed representations
of words and phrases and their compositionality. In: 26th International Conference on
Neural Information Processing Systems - Volume 2, NIPS’13, p. 3111–3119. Curran
Associates Inc. (2013)

69. Monperrus, M.: A critical review of ”automatic patch generation learned from human-
written patches”: Essay on the problem statement and the evaluation of automatic
software repair.
In: 36th International Conference on Software Engineering, ICSE
2014, p. 234–242. Association for Computing Machinery (2014)

70. Monperrus, M.: Automatic software repair: A bibliography. ACM Comput. Surv. 51(1)

(2018)

71. Nguyen, A.T., Nguyen, T.N.: Graph-based statistical language model for code. In: 37th
International Conference on Software Engineering - Volume 1, ICSE ’15, p. 858–868.
IEEE Press (2015)

72. Nguyen, X., Joty, S.R., Hoi, S.C.H., Socher, R.: Tree-structured attention with hier-
archical accumulation. In: 8th International Conference on Learning Representations,
ICLR 2020. OpenReview.net (2020)

73. Pan, K., Kim, S., Whitehead, E.J.: Toward an understanding of bug ﬁx patterns.

Empirical Software Engineering 14(3), 286–315 (2009)

74. Pandita, R., Taneja, K., Williams, L., Tung, T.: Icon: Inferring temporal constraints
from natural language api descriptions. In: 2016 IEEE International Conference on
Software Maintenance and Evolution (ICSME), pp. 378–388 (2016)

75. Pandita, R., Xiao, X., Zhong, H., Xie, T., Oney, S., Paradkar, A.: Inferring method
speciﬁcations from natural language api descriptions. In: 2012 34th International Con-
ference on Software Engineering (ICSE), pp. 815–825 (2012)

76. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: A method for automatic evalua-
tion of machine translation. In: 40th Annual Meeting on Association for Computational
Linguistics, ACL ’02, p. 311–318. Association for Computational Linguistics (2002)
77. Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word represen-
tation. In: 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 1532–1543. Association for Computational Linguistics (2014)

78. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.:
Deep contextualized word representations. In: 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguis-
tics (2018)

79. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.:
Deep contextualized word representations. In: 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguis-
tics (2018)

80. Pewny, J., Holz, T.: Evilcoder: Automated bug insertion. In: Proceedings of the 32nd
Annual Conference on Computer Security Applications, ACSAC ’16, p. 214–225. As-
sociation for Computing Machinery (2016)

81. Pradel, M., Gross, T.R.: Detecting anomalies in the order of equally-typed method ar-
guments. In: 2011 International Symposium on Software Testing and Analysis, ISSTA
’11, p. 232–242. Association for Computing Machinery (2011)

A Controlled Experiment of Diﬀerent Code Representations

41

82. Pradel, M., Gross, T.R.: Name-based analysis of equally typed method arguments.

IEEE Transactions on Software Engineering 39(8), 1127–1143 (2013)

83. Pradel, M., Sen, K.: Deepbugs: A learning approach to name-based bug detection.

Proc. ACM Program. Lang. 2(OOPSLA) (2018)

84. Raychev, V., Bielik, P., Vechev, M., Krause, A.: Learning programs from noisy data.

SIGPLAN Not. 51(1), 761–774 (2016)

85. Raychev, V., Vechev, M., Krause, A.: Predicting program properties from ”big code”.
In: 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, POPL ’15, p. 111–124. Association for Computing Machinery (2015)
86. Reimers, N., Gurevych, I.: Sentence-BERT: Sentence embeddings using Siamese
BERT-networks. In: 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 3982–3992. Association for Computational Linguistics (2019)
87. Rice, A., Aftandilian, E., Jaspan, C., Johnston, E., Pradel, M., Arroyo-Paredes, Y.:
Detecting argument selection defects. Proc. ACM Program. Lang. 1(OOPSLA) (2017)
88. Samimi, H., Sch¨afer, M., Artzi, S., Millstein, T., Tip, F., Hendren, L.: Automated
repair of html generation errors in php applications using string constraint solving. In:
34th International Conference on Software Engineering, ICSE ’12, p. 277–287. IEEE
Press (2012)

89. Sato, R.: A survey on the expressive power of graph neural networks. arXiv preprint

arXiv:2003.04078 (2020)

90. Schramm, L.: Improving performance of automatic program repair using learned
heuristics. In: 11th Joint Meeting on Foundations of Software Engineering, ESEC/FSE
2017, p. 1071–1073. Association for Computing Machinery (2017)

91. Scott, R., Ranieri, J., Kot, L., Kashyap, V.: Out of sight, out of place: Detecting and
assessing swapped arguments. In: 2020 IEEE 20th International Working Conference
on Source Code Analysis and Manipulation (SCAM), pp. 227–237 (2020)

92. Sellik, H., van Paridon, O., Gousios, G., Aniche, M.: Learning oﬀ-by-one mistakes:
An empirical study. In: 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR), pp. 58–67 (2021)

93. Tang, G., Meng, L., Wang, H., Ren, S., Wang, Q., Yang, L., Cao, W.: A comparative
study of neural network techniques for automatic software vulnerability detection.
In: 2020 International Symposium on Theoretical Aspects of Software Engineering
(TASE), pp. 1–8. IEEE Computer Society (2020)

94. Tarlow, D., Moitra, S., Rice, A., Chen, Z., Manzagol, P.A., Sutton, C., Aftandilian,
E.: Learning to ﬁx build errors with graph2diﬀ neural networks. In: Proceedings of
the IEEE/ACM 42nd International Conference on Software Engineering Workshops,
p. 19–20. Association for Computing Machinery (2020)

95. Tufano, M., Pantiuchina, J., Watson, C., Bavota, G., Poshyvanyk, D.: On learning
meaningful code changes via neural machine translation. In: 41st International Con-
ference on Software Engineering, ICSE ’19, p. 25–36. IEEE Press (2019)

96. Tufano, M., Watson, C., Bavota, G., Di Penta, M., White, M., Poshyvanyk, D.: Deep
learning similarities from diﬀerent representations of source code. In: 2018 IEEE/ACM
15th International Conference on Mining Software Repositories (MSR), pp. 542–553
(2018)

97. Tufano, M., Watson, C., Bavota, G., Di Penta, M., White, M., Poshyvanyk, D.: An
empirical investigation into learning bug-ﬁxing patches in the wild via neural machine
translation.
In: 33rd ACM/IEEE International Conference on Automated Software
Engineering, ASE 2018, p. 832–837. Association for Computing Machinery (2018)
98. Tufano, M., Watson, C., Bavota, G., Penta, M.D., White, M., Poshyvanyk, D.: An em-
pirical study on learning bug-ﬁxing patches in the wild via neural machine translation.
ACM Transactions on Software Engineering and Methodology (2019)

99. Tufano, R., Pascarella, L., Tufano, M., Poshyvanyk, D., Bavota, G.: Towards automat-
In: 2021 IEEE/ACM 43rd International Conference on

ing code review activities.
Software Engineering (ICSE), pp. 163–174. IEEE Computer Society (2021)

100. Vasic, M., Kanade, A., Maniatis, P., Bieber, D., singh, R.: Neural program repair
by jointly learning to localize and repair. In: International Conference on Learning
Representations. OpenReview.net (2019)

42

Marjane Namavar∗ et al.

101. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
In: Advances in Neural Information

L.u., Polosukhin, I.: Attention is all you need.
Processing Systems, vol. 30. Curran Associates, Inc. (2017)

102. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.: Graph
attention networks. In: International Conference on Learning Representations (2018)
103. Wainakh, Y., Rauf, M., Pradel, M.: Idbench: Evaluating semantic representations of
identiﬁer names in source code. In: 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE), pp. 562–573 (2021)

104. Wang, K., Christodorescu, M.: COSET: A benchmark for evaluating neural program

embeddings. arXiv preprint arXiv:1905.11445 (2019)

105. Watson, C., Tufano, M., Moran, K., Bavota, G., Poshyvanyk, D.: On learning meaning-
ful assert statements for unit test cases. In: ACM/IEEE 42nd International Conference
on Software Engineering, ICSE ’20, p. 1398–1409. Association for Computing Machin-
ery (2020)

106. White, M., Tufano, M., Mart´ınez, M., Monperrus, M., Poshyvanyk, D.: Sorting and
transforming program repair ingredients via deep learning code similarities. In: 2019
IEEE 26th International Conference on Software Analysis, Evolution and Reengineer-
ing (SANER), pp. 479–490 (2019)

107. Wohlin, C., Runeson, P., H¨ost, M., Ohlsson, M.C., Regnell, B., Wessl´en, A.: Experi-

mentation in software engineering. Springer Science & Business Media (2012)

108. Wotawa, F., Nica, M., Nica, I.: Automated debugging based on a constraint model of
the program and a test case. The Journal of Logic and Algebraic Programming 81,
390–407 (2012)

109. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Yu, P.S.: A comprehensive survey on
graph neural networks. IEEE Transactions on Neural Networks and Learning Systems
32(1), 4–24 (2021)

110. Zhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., Liu, X.: A novel neural source
code representation based on abstract syntax tree. In: ICSE, pp. 783–794 (2019)
111. Zhao, R., Bieber, D., Swersky, K., Tarlow, D.: Neural networks for modeling source

code edits. arXiv preprint arXiv:1904.02818 (2019)

112. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., Sun, M.: Graph
neural networks: A review of methods and applications. AI Open 1, 57–81 (2020)

Marjane Namavar is a software engineer. She received
her M.A.Sc. degree in 2021 from the School of Electrical
and Computer Engineering, University of British Columbia,
Canada, under the supervision of Professor Ali Mesbah.
Her research interests include the applications of machine
learning in source code analysis and repair.

Noor Nashid Noor Nashid is a Ph.D. student at the Uni-
versity of British Columbia (UBC). His research interests
include software engineering, with an emphasis on the ap-
plications of deep learning techniques to source code. He is
a recipient of the four-year doctoral fellowship at UBC.

A Controlled Experiment of Diﬀerent Code Representations

43

Ali Mesbah is a professor at the University of British
Columbia (UBC) where he leads the Software Analysis and
Testing (SALT) research lab. His main area of research is
in software engineering and his research interests include
software analysis and testing, web and mobile-based ap-
plications, software maintenance and evolution, debugging
and fault localization, and automated program repair. He
has published over 60 peer-reviewed papers and received
numerous best paper awards, including two ACM Distin-
guished Paper Awards at the International Conference on Software Engineer-
ing (ICSE 2009 and ICSE 2014). He is the recipient of a Killam Accelerator
Research Fellowship (KARF) award (2020), a Killam Faculty Research Prize
(2019) at UBC, and was awarded the NSERC Discovery Accelerator Supple-
ment (DAS) award in 2016. He is currently an associate editor of the IEEE
Transactions on Software Engineering (TSE) and regularly serves on the pro-
gram committees of numerous software engineering conferences such as ICSE,
FSE, ASE, ISSTA, and ICST.

