7
1
0
2

v
o
N
9
2

]

C
D
.
s
c
[

2
v
5
1
3
6
0
.
1
1
7
1
:
v
i
X
r
a

SPARCE: Sparsity aware General Purpose Core
Extensions to Accelerate Deep Neural Networks

1

Sanchari Sen, Shubham Jain, Swagath Venkataramani, Anand Raghunathan

Abstract—Deep Neural Networks (DNNs) have emerged as the method of choice for solving a wide range of machine learning tasks.
Satiating the enormous growth in computational demand posed by DNNs is a key challenge for computing system designers and has
most commonly been addressed through the design of custom accelerators. However, these specialized accelerators that utilize large
quantities of multiply-accumulate units and on-chip memory are prohibitive in many design scenarios (e.g., wearable devices and IoT
sensors), due to the stringent area/cost constraints. Therefore, accelerating DNNs on these low-power systems, comprising of mainly
the indispensable general-purpose processor (GPP) cores, requires new approaches. In this work, we focus on improving the
performance of DNNs on GPPs by exploiting a key attribute of DNNs, i.e. sparsity. We propose Sparsity aware Core Extensions
(SPARCE) - a set of micro-architectural and ISA extensions that leverage sparsity and are minimally intrusive and low-overhead. We
address the key challenges associated with exploiting sparsity in GPPs, viz., dynamically detecting whether an operand (e.g., the result
of a load instruction) is zero and subsequently skipping a set of future instructions that use it. To maximize performance beneﬁts, our
design ensures that the instructions to be skipped are prevented from even being fetched, as squashing instructions comes with a
penalty (e.g., a pipeline stall). SPARCE consists of 2 key micro-architectural enhancements. First, a Sparsity Register File (SpRF) is
utilized to track registers that are zero. Next, a Sparsity aware Skip Address (SASA) table is used to indicate instruction sequences that
can be skipped, and to associate speciﬁc SpRF registers to trigger instruction skipping. When an instruction is fetched,
SPARCE dynamically pre-identiﬁes whether the following instruction(s) can be skipped, and if so appropriately modiﬁes the program
counter, thereby skipping the redundant instructions and improving performance. We model SPARCE using the gem5 architectural
simulator, and evaluate our approach on 6 state-of-the-art image-recognition DNNs in the context of both training and inference using
the Caffe deep learning framework. On a scalar microprocessor, SPARCE achieves 1.11×-1.96× speedups across both convolution
and fully-connected layers with 10%-90% sparsity, and 19%-31% reduction in execution time at the overall application-level. We also
evaluate SPARCE on a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate that SPARCE achieves 8%-15%
reduction in the application-level execution time.

Index Terms—Deep Learning, Deep Neural Networks, Sparsity, General Purpose Processors

(cid:70)

1 INTRODUCTION

Deep neural networks (DNNs) have revitalized the ﬁeld of ma-
chine learning by achieving accuracy levels beyond human per-
ception in a variety of image, video, text and speech processing
tasks [1]–[8]. Today, they have emerged as the de facto solution
approach for challenging artiﬁcial intelligence problems and are
deployed in several products and services, including Google’s
image and voice search [9], Facebook’s DeepFace [10] and
DeepText [11], Apple’s Siri [12], to name a few. However, one
of the key challenges to the ubiquitous adoption of DNNs is
their computational demand, which far outstrips the capabilities
of modern computing platforms. For example, ResNet-152 [1],
a state-of-the-art DNN for image recognition, requires ∼11.3
giga operations to classify a single 224×224 image. As DNNs
begin to pervade the spectrum of computing devices from high
performance servers to low-power edge/IoT devices, the demands
on compute efﬁciency are expected grow even more stringent.
Consequently, there is an urgent need to explore new approaches
to improve their implementation efﬁciency.

•

•

Sanchari Sen, Shubham Jain and Anand Raghunathan are with School of
Electrical and Computer Engineering, Purdue University, West Lafayette,
IN, 47906, USA.
E-mail: {sen9,jain130,raghunathan}@purdue.edu
Swagath Venkataramani is with IBM T.J Watson Research Center, York-
town Heights, NY.
E-mail: swagath.venkataramani@ibm.com

Realizing this need, prior research efforts have explored sev-
eral key directions. A majority of research efforts exploit the
compute and communication characteristics of DNNs to efﬁciently
parallelize them on distributed platforms [13]–[19] or design spe-
cialized accelerator architectures [20]–[31]. Other efforts leverage
the intrinsic error resilience of DNNs to approximate selected
computations [32]–[39] or utilize non-CMOS technologies whose
device characteristics match the compute kernels of DNNs [40]–
[42].

Complementary to the above efforts, we focus on a different
design scenario and target deeply embedded systems such as wear-
ables and IoT edge devices where the additional area/ cost imposed
by custom-accelerators is prohibitive. For example, even a low
power DNN accelerator such as Eyeriss [31] occupies 12.25mm2
area which is 30× larger than the 0.4mm2 occupied by an ultra
low power Cortex A35 core [43]. Accelerating DNNs on these
low-power systems then comes down to accelerating them on
the indispensable general-purpose processor (GPP) cores already
present in these systems. We focus on improving the execution
time of DNNs on GPPs by leveraging sparsity in different DNN
data-structures, viz., features, weights and backpropagated errors.
Sparsity in DNNs can be both static or dynamic depending on
whether the zero values remain constant or vary across different
inputs to the network. Sparsity in weights, introduced by pruning
connections in the network after training, is static in nature. In
contrast, feature and error sparsities, caused by the thresholding
nature of the ReLU (Rectiﬁed Linear Unit) activation functions,
are dynamic in nature.

 
 
 
 
 
 
Prior efforts that exploit sparsity to accelerate DNNs can be
grouped into two classes based on whether they exploit static
or dynamic sparsity, as shown in Figure 1. Specialized acceler-
ators [31], [44]–[48] have been proposed to exploit both forms
of sparsity. However, these techniques are closely tied to a given
accelerator architecture thereby preventing their applicability to
GPPs. In contrast, static sparsity in weights can be exploited
on GPPs through software-only approaches [49]–[51] that in-
volve sparse encodings and sparse matrix multiplication routines.
However, extending them to exploit the intermediate levels of
dynamic sparsity (40%-70%) can be counter-productive because
of the encoding overhead involved at runtime. For example, the
sparse encoding overhead involved in performing sparseBLAS
based matrix multiplication on the sparse conv3 layer features of
AlexNet [52] causes a slowdown of 2.78×. We observe across
6 image-recognition DNNs that dynamic sparsity in features
results in 45.1% of the computations being rendered redundant
during inference, presenting a signiﬁcant opportunity for improv-
ing performance. We believe our effort, SPARCE, is the ﬁrst
to successfully exploit dynamic sparsity in DNNs on GPPs. To
that end, we propose micro-architectural and ISA extensions that
are minimally intrusive and low-overhead. Moreover, since static
sparsity is a special case of dynamic sparsity where the location
of zeros remains constant across inputs, the extensions also allow
SPARCE to exploit static sparsity in weights.

Fig. 1. Related work: Exploiting sparsity in DNNs

To exploit dynamic sparsity, GPPs need to be equipped to
dynamically detect if the result of an instruction (e.g., a load
from a sparse data structure) is zero and if so, skip a set of
future instructions that use its result. However, there are two
key challenges: (i) the instructions to be skipped often do not
immediately follow the instruction that determines whether they
may be; moreover, the instructions to be skipped may be non-
contiguous in the program, and (ii) to maximize performance
beneﬁts, the instructions to be skipped should be prevented from
even being fetched, as squashing the instruction in-ﬂight would
diminish the beneﬁts by introducing a pipeline stall.

Sparsity aware Core Extensions (SPARCE). To overcome the
aforementioned challenges, we propose Sparsity aware Core
Extensions (SPARCE), comprised of two key micro-architectural
enhancements. First, SPARCE contains a Sparsity Register File
(SpRF) to track which general purpose registers that contain a
zero. We achieve this by augmenting the writeback stage of the
processor to check if the update to a register is zero and appro-
priately modify the corresponding SpRF entry. Next, a Sparsity
Aware Skip Address (SASA) table is used to store instruction

2

sequences and the conditions under which they can be skipped i.e.,
the registers in the SpRF that need to be zero for the instructions
to become redundant. Whenever SPARCE fetches an instruction,
it uses the SASA table and the SpRF to pre-identify whether the
following instruction(s) can be skipped. If so, the program counter
is modiﬁed to directly fetch the next irredundant instruction.
In summary, the key contributions of this work are:
• We propose Sparsity aware Core Extensions (SPARCE) to
accelerate DNNs on general purpose processors by skipping
redundant computations borne out of sparsity in the different
DNN data-structures.

• SPARCE comprises of micro-architectural enhancements to
dynamically track when the result of an instruction is zero,
pre-identify future instructions that are rendered redundant,
and prevent them from being fetched and executed, thereby
improving performance.

• We evaluate SPARCE on a suite of 6 state-of-the-art image-
recognition DNN benchmarks using the Caffe deep learning
framework. We achieve application-level speedups of 19%-
31% on low-power embedded scalar microprocessors. We
also achieve speedups of 8%-15% over highly optimized
baseline implementations that use OpenBLAS on an ARMv8
processor with 4-way SIMD and prefetching support.
The rest of the paper is organized as follows. Section 2
explains the sources of sparsity in DNNs. Section 3 details the
key design principles of SPARCE and demonstrates them in
the context of an in-order pipelined processor. Section 4 shows
SPARCE in action using the ARM-BLAS GEMM routine as a
case study. Section 5 describes the experimental methodology and
the results are presented in Section 6. Section 7 presents related
research efforts and Section 8 concludes the paper.

2 SPARSITY IN DNNS: SOURCES AND OPPORTU-
NITY

In this section, we ﬁrst provide a brief background on DNNs. We
then explain the various static and dynamic sources of sparsity in
the different DNN data-structures, and quantify the opportunity
for performance improvement afforded by sparsity.

2.1 DNN: Background
DNNs are networks of primitive compute units called neurons,
organized into layers. Each layer is associated with a set of param-
eters called weights. DNNs operate in two phases viz. training and
inference. During the training phase, a labelled training dataset is
used to iteratively reﬁne the weights of the DNN. In the inference
phase, the trained DNN is used to classify new inputs.

Computationally, DNN executions iteratively perform 3 key
steps viz. Forward Propagation (FP), Backpropagation (BP), and
Weight Gradient and Update (WG),
that operate on 4 data-
structures, viz., features, weights, errors and gradients. All three
steps (FP, BP, WG) are performed during the training phase, while
inference involves only the FP step. In FP, inputs to the DNN are
propagated through its layers to produce the DNN outputs. In each
layer, the input features are operated on with its weights to produce
its output features, which are then fed to the next layer and so on.
In BP, errors observed at the output of the DNN are propagated
backwards through each layer of the DNN. In this case, the error
at the output of a layer is operated on with its weights to compute
the error at its inputs. In WG, the input features and the output
error of each layer are used to reﬁne its weights.

Eyeriss[31], EIE[45], SCNN[46], Cambricon-x[47], GPU ZeroSkip[48]SparseCNN[49], SSL[50], Scalpel[51],  SPARCEEyeriss[31], Cnvlutin[44], EIE[45], SCNN[46] SPARCEAcceleratorsGeneral Purpose ProcessorsStatic sparsityDynamic sparsity3

Fig. 2. Different forms of sparsity in DNNs

2.2 Sources of Sparsity in DNNs

In practice, all major DNN data-structures - features, weights,
levels of sparsity,
errors and gradients - exhibit signiﬁcant
which can be exploited for computational savings. Three of
the four data-structures (all except weight gradients) are used
as inputs to multiply-and-accumulate operations in the different
steps (FP/BP/WG), which become redundant when one of the
input operands is zero. Among these three sparse multiply-and-
accumulate operands, weights exhibit static sparsity that remains
constant across different inputs while the remaining two data-
structures (features and errors) exhibit dynamic sparsity that varies
dynamically across different inputs. Figure 2 summarizes the
sparsity in the different DNN data-structures, which we describe
in the remainder of this subsection.

2.2.1 Static Sparsity
Weight Sparsity. Sparsity in weights occurs during the inference
phase of the DNN. As shown in Figure 2, after training, connec-
tions whose weights are close to zero are pruned to compress the
model size [26], [36], [37]. The last column in Figure 2 shows
the fraction of zero weights in the different layers of the AlexNet
model trained using deep compression [36]. We ﬁnd the sparsity
to vary between 18%-85% across the different layers. Weight
sparsity is static in nature because of the fact that zero weights
are identiﬁed before the inference phase.

2.2.2 Dynamic Sparsity
Feature sparsity. Sparsity in features stems from the thresholding
nature of the activation function present at the output of each layer.
As shown in Figure 2, the predominantly used ReLU (Rectiﬁed
Linear Unit) activation function clips negative inputs to zero. Fig-
ure 2 also shows the average feature sparsity exhibited by various
DNN benchmarks, which ranges between ∼25%-60%. Feature
sparsity has a dynamic nature i.e., because neurons whose outputs

are zero vary considerably across inputs. Figure 3 illustrates this
property using feature maps obtained at the conv3 of AlexNet,
produced by two different inputs from the ImageNet dataset. We
ﬁnd the spatial variation in white pixels, which indicate zero
output, to be substantial across inputs.

Fig. 3. Variation in feature sparsity of AlexNet CONV3 layer across input
images

Error Sparsity. Sparsity in the error data-structure originates
from two sources. First, the derivative of the activation function,
such as ReLU, is zero when the error at the output of the layer is
negative. Next, when errors are propagated back through a max-
pooling layer, as shown in Equation 1, only one input of each
pooling window is set a non-zero error value.

∂E
∂yl

(x+p, y+q) =

(cid:40)

0,

if yl+1(x, y) (cid:54)= yl(x + p, y + q)

∂E
∂yl+1

, otherwise

(1)

050100GeoMeanconv3conv2conv1Types of SparsityData-structurePhaseSourcesPercentage of SparsityStaticWeightsInferenceDynamicFeaturesInference & TrainingErrorsTrainingCIFAR-10 DNN050100050100Deep Compression AlexNet[50]conv1 conv2conv3 conv4 conv5  fc1fc2fc3 GeoMeanInput 1Input 2Feature: 1Feature: 100CONV3 –Feature MapInput ImageFor example, if a pooling window of size 2×2 is used, at
least three quarters of the error values are sparse. Similar to
feature sparsity, the error sparsity is also dynamic. The average
error sparsity in different layers of a CIFAR-10 DNN is shown in
Figure 2.

2.3 Opportunity for Computational Savings

4

shows

that
redundant

Figure
the
fraction
of multiply-
accumulate
(MAC)
computations
are
for
rendered
each DNN benchmark due
to dynamic sparsity in
features during inference.
We ﬁnd that between
25%-60% (average: 45%)
of the computations can
be skipped, underscoring
the substantial opportunity
for performance improvement. Figure 5 shows how the fraction
of redundant operations varies across 1000 different inputs for
the AlexNet DNN. We observe ∼14% variation across inputs,
although each input shows considerable opportunity for reduction
in execution time (minimum: 28%).

Fig. 4. Redundant ops

4

3.1 Challenges

The key challenge in exploiting sparsity is to equip the processor
with the ability to dynamically detect if the result of an instruction
is zero and if so, skip a list of future instructions that are rendered
redundant. We illustrate this challenge using the assembly code
snippet shown in Figure 6, which computes the dot-product of
two vectors, IN P and KER, each of size N , to produce a scalar
OU T . Registers r0, r1 and r2 hold the data operands, while
p0, p1 and p2 are pointers that hold their respective memory
locations. For each instruction in the program, Figure 6 shows
the instructions that can be skipped when its result is zero. For
example, when the IN P load returns a zero (Inst. 2), the subse-
quent KER load (Inst. 4), and the multiply and add instructions
can be skipped (Insts. 6-7). It is noteworthy that the computational
savings is a weighted sum of the number of instructions skipped
and the cycles taken by each instruction. For instance, ﬂoating
point multiply and add instructions may take 3-5 cycles to execute,
while a load incurs variable cycles depending on the level of cache
hierarchy accessed.

Fig. 5. Fraction of redundant ops across different inputs of AlexNet

In summary, the dynamic sparsity present in the feature and
error data-structures offers a substantial opportunity to accelerate
DNNs. However, the levels of sparsity are not extreme enough to
completely exploit them in software, and this coupled with their
dynamic nature necessitates hardware solutions to realize beneﬁts
in the context of general purpose processors.

3 SPARCE: SPARSITY AWARE GENERAL
PURPOSE CORE EXTENSIONS

To exploit the different forms of sparsity and improve DNN
performance on GPPs, we propose Sparsity aware Core Extensions
(SPARCE), a set of micro-architectural and ISA extensions that are
general-purpose, minimally intrusive and low-overhead. In this
section, we present the key ideas behind SPARCE and describe
how they can be integrated within an in-order processor pipeline.

Fig. 6. Redundant instructions due to sparsity in vector dot-product
evaluation

The following observations highlight the challenges in detect-

ing and beneﬁting from sparsity.

• Location of redundant instructions. In a program, the
instructions that can be skipped may not immediately follow
the instruction producing the zero result. Worse, redundant
instruction sequences may be scattered non-contiguously
through the program. For instance, in the program shown
in Figure 6, when inst. 2 returns a zero, 2 non-contiguous
instruction sequences (Inst. 4 and Insts. 6-7) need to skipped.
Hence, efﬁcient ways to capture which instructions can be
skipped is key to leveraging sparsity.

• Avoiding redundant instruction fetches. To maximize per-
formance, the instructions that can be skipped need to be
prevented from even being fetched. This is especially critical
in the context of in-order pipelines, where even if the in-
struction is squashed after being fetched, it would introduce
a bubble in the pipeline. For example, if the condition for r0
or r1 being zero is checked after the F M U L instruction is
fetched (Inst. 6), it would result in a bubble ﬂowing through
the pipeline in place of Inst. 6. It is worth noting that, in the
context of multi-cycle instructions, squashing the instruction
after it is fetched can still improve performance.

00.20.40.60.8CIFAR-10AlexNetDeepCompGoogleNetVGG-16 ResNet-50GeoMeanFrac.	Redundant	Ops	à(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)LD r2, [p2]   //Load OUT LOOP :  LD r0, [p0]   //LoadINP  ADD p0, p0, #1LD r1, [p1]   //Load KERADDp1, p1, #1FMUL r3, r1, r0  //r3 = INP*KERFADD r2, r2, r3 // OUT += r3INC INDEXBNEINDEX, #N, LOOPST r2 , [p2]   Redundant	insts.if	result	==		0---4,6-7---6-7---7------------Vector	Dot-product	• Use of SIMD instructions. GPPs use vector units with
SIMD (Single Instruction, Multiple Data) execution engines
to exploit ﬁne-grained data parallelism in workloads. SIMD
instructions can be skipped only if computations performed
on all the vector lanes are redundant. This constrains the spar-
sity to be relatively coarse grained, as irregularly scattered
zero values are not beneﬁcial.

3.2 SparCE: Overview

Figure 7 shows an overview of the micro-architectural and ISA
enhancements proposed in SPARCE. We describe these extensions
in detail, and demonstrate how they address the aforementioned
challenges to leverage sparsity in DNNs.

Fig. 7. SPARCE: Design Overview

3.2.1 Micro-architectural states and ISA Extension
In SPARCE, we augment the processor with two new micro-
architectural states viz. Sparsity Register File (SpRF) and Sparsity
Aware Skip Address (SASA) table. The SpRF is used to dynam-
ically track which registers in the processor’s register ﬁle contain
zero values. The SpRF contains one entry (few bits) corresponding
to each register in the register ﬁle. When an instruction that writes
to a register retires, the SpRF is updated if the result is zero.
The SASA table stores information about which instructions can
be skipped and under what conditions. Speciﬁcally, each entry
stores the program counter (P C) of the instruction preceding
a redundant instruction sequence, the index of the register that
determines redundancy and the length of the sequence. Storing the
P C of the instruction preceding a redundant instruction sequence
allows SPARCE to pre-identify whether the next set of instructions
can be skipped and if so, skip them before the instructions are
fetched.
SASA-LD Instruction. SPARCE enables software to explicitly
identify potentially redundant instruction regions by pre-loading
the SASA table at program startup, or before the program execu-
tion enters a given code region. To this end, we extend the ISA
with a new instruction viz. SASA-LD, which loads a given region
of memory into the SASA table. As shown in Equation 2, the
SASA-LD instruction takes a register operand (Rn) that points
to the SASA table’s location in memory and an immediate operand
(size) that denotes the size of the SASA table.

SASA-LD [Rn], #size

(2)

5

At a given point in the program execution, the number of entries in
the SASA table limits the number of redundant instruction regions
that can be skipped by SPARCE. However, the SASA table can
be periodically refreshed from memory as the program execution
progresses. In the context of DNNs, we found that 20 entries in the
SASA table sufﬁce to capture all redundant instruction sequences,
since the computational kernels are captured by a small number of
library (e.g., BLAS) functions.

3.2.2 Tracking, Pre-identifying, and Skipping Redundant In-
structions

SPARCE utilizes the SpRF and the SASA table to dynamically
skip redundant instructions borne out of sparsity in the input
data-structures. As shown in Figure 7, the micro-architecture of
SPARCE is extended to support the following functions.

Track Sparse Registers. SPARCE contains a Sparse Value
Checker (SVC), which in the processor’s writeback stage compares
the result of each instruction to zero and if so, updates the entry
corresponding to the instruction’s destination register in the SpRF.

Pre-identify & Skip Redundant Instructions. SPARCE is
equipped with a Pre-identify and Skip Redundancy Unit (PSRU)
that utilizes the SASA table to identify and skip redundant instruc-
tion regions. For each instruction, we check if its P C contains an
entry in the SASA table. An entry in the SASA table indicates
that the instruction following the current instruction is the start
of a potentially redundant instruction sequence. In this case, the
PSRU checks the SpRF to identify if the registers indicated in the
SASA table entry are currently zero. If so, it increments the P C
to the end of the redundant instruction sequence, thereby skipping
instructions to beneﬁt performance. If not, SPARCE proceeds to
execute instructions in program order.

In summary, SPARCE uses the SpRF and the SASA table
to seamlessly track sparse registers, pre-identify instruction se-
quences that are redundant and dynamically skip them before they
are even fetched to improve performance.

3.3 In-order SparCE Processor Pipeline

We now explain how SPARCE can be integrated into an in-
order processor. Figure 8 shows the block diagram of the overall
SPARCE processor architecture. We start with a conventional 4-
stage (fetch, decode, execute/memory, and writeback) pipelined
processor architecture implementing a RISC-style instruction set
with at most 2 source register operands and one destination register
operand. Although the SPARCE architecture is described in this
section with a scalar execution unit for ease of illustration, it is
directly applicable to vector processors with any number of SIMD
execution lanes. We augment the processor with the following
structures.

Sparsity Register File (SpRF). The SpRF is located at the fetch
stage of the SPARCE processor. It is a multi-ported register ﬁle,
with one entry corresponding to each register in the processor’s
register ﬁle. Each entry in the SpRF contains only two single-bit
ﬁelds - isSparse and regUpdInFlight. The isSparse bit is set to 1
for registers containing zeros, and reset otherwise. The regUpdIn-
Flight bit indicates whether an instruction modifying the register
is in ﬂight in any stage of the pipeline. For instance, an instruction
modifying register Rd, sets the SpRF [Rd][regU pdInF light]
ﬁeld when it enters the decode stage and resets it after committing
its result in the writeback stage. In determining whether a future

FeDeExeWBPCIF/IDID/EXEX/WBSparse Value Checker (SVC)Processor PipelineUpdate SpRF if inst. result is zeroTrack if a register is zeroModulate PC if next set of insts. can be skippedStores which insts. become redundant & when?Identify Redundant Instruction Regions•Identify sparse data-structures•Mark insts. that operate on them Deep Neural Network (DNN) ProgramHW/SW interfaceProgram SASA table•Form SASA table entries (PC, sp.Regs, #insts)•Instrument SASA-LDinsts. in programSparCE: Software Design SparCE uArch StatesSparsity Aware Skip Address (SASA) TableSparsity Register File (SpRF)Pre-identify & Skip Redundancy Unit (PSRU)SparCE uArch Support6

Fig. 8. Block diagram of SPARCE in-order processor architecture

instruction is redundant, the regUpdInFlight ﬁeld ensures that we
do not use a stale isSparse value when a more recent instruction
updating this register is under execution.

Sparse Value Checker (SVC). The SVC is added to the write-
back stage of the SPARCE processor. It contains a compara-
tor that checks if the output of each instruction that updates
a register is zero. It then correspondingly updates the SpRF.
For example, when the output of an instruction with destina-
tion register Rd is zero, then SpRF [Rd][isSparse] is set and
SpRF [Rd][regU pdInF light] is reset.

Sparsity Aware Skip Address (SASA) table. The SASA table
is also present in the fetch stage of the SPARCE processor. As
shown in Figure 8, the SASA table is an associative memory
structure with three ﬁelds: (i) preceedingP C, which stores the
P C of the instruction prior to the redundant instruction sequence,
(ii) SpRF Condition ﬁeld which stores a Boolean combination
of 2 register indices in the SpRF that should be satisﬁed for
the instruction region to be skipped, and (iii) instsT oSkip,
which contains the length of the redundant instruction sequence.
As an example, for the code in Figure 6,
the SASA table
entry to skip instructions 6-7 would be {preceedingP C=5,
SpRF Condition=r0|r1, instsT oSkip=2}

Pre-identify and Skip Redundancy Unit (PSRU) The PSRU
is also added to the fetch stage of SPARCE processor. The
PSRU determines whether an instruction region speciﬁed in the
SASA table can be skipped, and appropriately modulates the
P C. Figure 9 shows the ﬂowchart depicting the operation of
PSRU. Given a P C, an associative lookup is performed on the
preceedingP C ﬁeld of the SASA table. If the P C is a hit in
the SASA table, then the next instruction marks the beginning
of a potentially redundant instruction sequence. To ascertain if
the instruction sequence can be skipped, the PSRU reads the
register indices speciﬁed in the SpRF Condition ﬁeld of the
SASA table (say Rs1 and Rs2) from the SpRF. If neither

Fig. 9. Flowchart for pre-identifying and skipping redundancy

SpRF [Rs1] nor SpRF [Rs2] have their regU pdInF light ﬁeld
set, then PSRU computes the Boolean condition (speciﬁed in the
SpRF Condition ﬁeld) on their respective isSparse ﬁelds. If
the condition is satisﬁed, then the instruction region is deemed
redundant and the P C is incremented by instsT oSkip to point
to the instruction immediately following the end of the redundant
region. If not, P C is incremented by 1 and the instructions are
executed in program order. However, if the regU pdInF light
ﬁeld is set for either SpRF [Rs1] or SpRF [Rs2] and the Boolean
condition in SpRF Condition cannot be deﬁnitively evaluated,
then the instruction region is temporarily marked as a skippable
region within the PSRU. The P C is incremented by 1 and the
program execution is continued. It is worth noting that continuing

IF/IDFetchDecode=0ID/EXMEM/WBReg. FileInstruction CachePCWriteback MuxPrecedingPCinstsToSkipSpRFcondition40672SpRF[Rs1]| SpRF[Rs2]41005SpRF[Rs3]&SpRF[Rs4]42502SpRF[Rs3]Sparsity Aware Skip Address (SASA) TableisSparseregUpdInFlight1001Rs2Main/ ALU #1 pipelineMACpipeline+MuxSparsity Register File (SpRF)Pre-identify & Skip Redundancy Unit (PSRU)Sparse Value Checker(SVC)Execute/Memory Rs1Load/Store Pipeline SpRF[Rs1]SpRF [Rs2]Associative MemoryCheck SpRFCond.Skippable RegionSASA Hit/MissDefaultSquashUpdate Regs. in-flightisSparseregUpdInFlightPC inc.PCSASA	Hit?YesSpRF[Rs1,Rs2]	inflight?NoIs	SpRF	[Rs1,Rs2]	sparse?YesPC	+=	SASA[PC]	instsToskipNoPC++YesMark	{PC	+	SASA[PC]	instsToskip}	as	skippablePC++Is	PC	skippable?NoYesNoSpRF[Rs1,Rs2]	inflight?PC++NoIs	SpRF	[Rs1,Rs2]	sparse?YesPC	+=	skippableregion	endNoPC++YesPC++7

Fig. 10. Zero skipping for sgemm kernel subroutine in BLAS

or aborting execution of a skippable region will not affect program
functionality.

of a highly optimized implementation of matrix multiplication
(GEMM) from the OpenBLAS library.

In the case when a P C is not present in the SASA table,
the PSRU checks if the P C is part of an active skippable
region. For the registers present in the SpRF Condition, the
regU pdInF light ﬁelds are re-checked from the SpRF. If they
are reset, the Boolean condition in SpRF Condition is evaluated.
If the skippable region is determined to be redundant, then the
remaining instructions in the region are skipped by appropriately
modifying the P C. Further, instructions belonging to the skip-
pable region prior to the current P C are squashed if they are
still in ﬂight. If the Boolean condition evaluates to a false, then
the remaining instructions in the skippable region are executed.
Finally, for a P C that misses the SASA table and is not part of
any active skippable region, the P C is incremented by 1 to fetch
the next instruction.

We note that the logic introduced in SPARCE to pre-identify
redundant instructions executes in parallel with the instruction
cache (ICache) access. The additional logic does not impact the
latency of the fetch stage, as both the SASA table and the SpRF
are signiﬁcantly smaller structures compared to the ICache.

In summary, by using the SpRF and the SASA table,
SPARCE dynamically tracks sparse registers, pre-identiﬁes if an
instruction region is redundant and skips instructions before they
are even fetched to improve performance. Thus SPARCE enables
DNN acceleration on GPPs by exploiting the sparsity resident in
their data-structures.

4 SOFTWARE FOR SPARCE
PROCESSSORS

To extract maximum performance from SPARCE, the software
needs to suitably leverage the sparsity-aware micro-architectural
extensions. In this section, we outline the key principles behind
software design for SPARCE, and demonstrate them in the context

4.1 SparCE Software Design Steps

The following steps need to be performed in software to leverage
sparsity on SPARCE processors.

Identifying sparse data-structures and redundant instructions.
One of the key requirements on software is to indicate (through the
SASA table) which instruction regions are potentially redundant
due to sparsity. To this end, sparse data-structures in the workload
need to be ﬁrst identiﬁed by the programmer. Next, when the
application is compiled, the registers that hold the sparse data-
structures and the instructions that load them from memory are
identiﬁed. Then, a static dependency analysis of the instruction
stream reveals the instructions that are affected by the sparse
data-structures, which are then marked as potentially redundant
instruction sequences. Following this, the condition for skipping
each instruction sequence is derived from: which sparse data-
structures affect the region, the registers containing them and the
type of operation performed. Finally, the application assembly is
instrumented with appropriate SASA-LD instructions.
Separate redundant instructions from instructions causing
redundancy. To identify if an instruction region can be skipped,
the instruction whose result makes them redundant should have
completed execution. Therefore, in the context of in-order proces-
sors, they need to be spaced at least few instructions (3 in our as
case as SVC is located in writeback stage) apart in the program,
so that redundant instructions are not fetched and no pipeline
bubble is introduced due to squashing redundant instructions.
To this end, the instruction stream is re-ordered by inserting
independent non-redundant instructions where needed to enable
sufﬁcient separation.

Mapping sparse data-structures on vector processors. In vector
processors, typically one of the input operands is shared by all

v12.s[i] == 0Skip 4 fmlainstructionsv0.4s == 0Skip 4 fmlainstructionsv12.s[0-3] == 0 Skip 4 ld1instructions123456789101112131415161718192021222324252627281234567891011121314151617181920212223242526272829Loads SIMD registers v0, v2, v4, v6, v8Performsfmlaon v1, v3, v5, v7, v12Loads SIMD registers v1, v3, v5, v7, v12Performsfmlaon v0, v2, v4, v6, v8vX.4s: SIMD register with 4 32-bit (single precision) elements  fmla: floating point  multiply-add to accumulator  ld1: vectorloadprfmPLDL1KEEP: prefetch for load into L1 and keep in cache512: byte offsetpB:Pointer to values in matrix B,v12, v8: SIMD registers with values from matrix BpA_x:Pointers to values in matrix Av0-v7: SIMD registers with values from matrix Ai=2SIMD lanes, while the other is different across lanes. A vector
instruction can be skipped only if computations performed on all
SIMD lanes are redundant. Hence, it is better to map a sparse
data-structure as the shared input operand, since the likelihood of
all non-shared inputs being zero is low. If both data-structures are
sparse, then the data-structure that exhibits the most block-wise
sparsity is mapped as the non-shared input operand.

8

Fig. 11. SASA table entries for kernel16x4 M1 subroutine

in the SASA table, 8 entries of size 2 for the 16 f mlas and 4
entries of size 1 for the ld1 instructions.
SparCE in action. Figure 12 depicts the sequence of events
that leads to instructions being skipped by SPARCE. First, when
register v12 is loaded by the M 1 subroutine, its SpRF entry is
updated. Note that the isSparse ﬁeld is a bit vector, one bit for
each word in the vector register. Next, when instruction M 2.12 is
fetched, it sees a hit in the SASA table and the reads SpRF [12] to
ascertain if the bit corresponding to SpRF [12][1] is sparse. Since
that is the case, the P C is incremented to directly fetch M 2.15.

10

the

Figure

assembly

two smaller

program for

4.2 Case Study: SparCE GEMM Routine
Popular deep learning frameworks, such as Caffe [53], tensor
ﬂow [54], etc., execute DNNs as a series of matrix multiply
operations, and leverage highly optimized software libraries such
as BLAS (Basic Linear Algebra Subprograms) to realize them.
Therefore, we develop a SPARCE version of the GEMM (Gener-
alized Matrix Multiply) routine from the OpenBLAS library [55]
targeting an ARM processor. We utilize the SPARCE-GEMM
routine to quantify the reduction in DNN execution time achieved
on a SPARCE processor.
shows

the
sgemm kernel l1 M16 22 subroutine, which performs single-
precision ﬂoating point matrix multiplication (B×A=C).
The sgemm routine executes
subroutines viz.
kernel16x4 M1 (which we abbreviate as M 1) and kernel16x4 M2
(M 2) sequentially in a loop. The subroutines utilize vector
input operand (B)
registers v8 and v12 to hold the ﬁrst
and registers v0-v7 to hold the second operand (A). The
intermediate results are computed in registers v16-v31. To
optimize performance, each subroutine prefetches data for the
other. For example, M 1 fetches data for operand A into the odd
registers, which are then used by M 2 in the subsequent iteration.
The memory addresses are provided by scalar registers pB and
pA 0-pA 3. The subroutines also prefetch data in the L1-cache
using the prfm instruction.
identify

the
sgemm kernel l1 M16 22 subroutine assuming one of the input
operands (say B) is sparse. The analysis can be easily extended
to cover the scenario when either A or both A and B are sparse.
Since B is sparse, it is beneﬁcial to map it as the operand shared
across the SIMD lanes, which is already the case in Figure 10.
Even when one of the words in a vector register containing B (v8
or v12) is zero, 4 f mla instructions can be skipped. For instance,
when v12.s[1] equals 0, instructions 9,10,13,14 in M 2 can be
skipped. This forms 2 redundant instruction sequences (9-10 and
13-14), each of size 2. This amounts to 16 f mlas being skipped
when the entire vector register (v8 or v12) is zero. It is worth
noting that, had the sparse data-structure (B) been mapped as the
non-shared SIMD operand in the program, only 4 f mlas could be
skipped even when the entire vector register is zero.

instruction

sequences

redundant

We

in

In addition to the f mla instructions being skipped, when the
vector register is fully zero, the load instructions for the second
operand can also be skipped. For example, when v12 is zero,
ld1 instructions (7, 11, 15 and 19) for operand A can be skipped
in M 1. Also, note that we did not re-order any instruction in
the sgemm routine, as the redundant instruction sequences were
sufﬁciently spaced apart from the instruction that triggers their
skipping. In the context of f mla instructions, the vector loads
happen in a different subroutine owing to pre-fetching, and in
the case of the ld1 instructions, they were naturally spaced >3
instructions apart.

Based on the above analysis, Figure 11 shows the SASA table
corresponding to the M 1 subroutine. We ﬁnd a total of 12 entries

Fig. 12. SPARCE in action for sgemm routine

In summary, with minimal changes to software, SPARCE pro-

cessors can leverage sparsity to improve performance.

5 EXPERIMENTAL METHODOLOGY
In this section, we present
experiments to evaluate SPARCE.

the methodology adopted in our

5.1 Performance Evaluation

We modeled the micro-architectural extensions proposed in
SPARCE using the cycle-accurate gem5 architectural simula-
tor [56]. The gem5 simulator was tightly integrated with the

PrecedingPCinstsToSkipSpRFconditionBgtinst.	in	sgemm2SpRF[v8[0]]	M1.42SpRF[v8[0]]	M1.61SpRF[v12]M1.82SpRF[v8[1]]	M1.101SpRF[v12]M1.122SpRF[v8[1]]	M1.141SpRF[v12]M1.162SpRF[v8[2]]	M1.181SpRF[v12]M1.202SpRF[v8[2]]	M1.232SpRF[v8[3]]	M1.262SpRF[v8[3]]	Time in clock cyclesProgram execution order (in instructions)FeDeExeWBPC: 0x40860x4086 M2.12: add pA_1, pA_1, #16M2.13: fmlav22.4s, v5.4s, v12.s[1]M2.14: fmlav23.4s, v7.4s, v12.s[1]M2.15: ld1 {v4.4s},  [pA_2]PrecedingPCinstsToSkipSpRFcondition0x40862SpRF[12][1]2. SASA tablelookupisSparseregUpdInFlight011005.  Bit ‘1’ is Zero!4. SpRF[12]lookup3. Hit!6. Update PC in next cycle…M1.3: ld1 {v12.4s},  pB1. Update SpRF[12]  from  writebackstageFeDeExeWBPC: 0x4098NPC = PC+(2+1) *InstrSize…It is noteworthy that all benchmarks exhibited dynamic sparsity
in features and errors, while only Deep Compression-AlexNet
exhibited static parsity in weights.

9

6 RESULTS
In this section, we present the results of our experiments that
highlight the advantages of SPARCE.

6.1 Performance and Energy Improvement

Figure 14 shows the normalized execution time beneﬁts of
SPARCE over the baseline processor for both inference and train-
ing. In the context of Dir-Conv-Scalar, the reduction in application
runtime ranges between 19%-31% across the benchmarks. In
contrast, OpenBLAS-SIMD4 demonstrates beneﬁts in the range of
8%-15% reduction in runtime. This is because f mla instructions
occupy a much smaller fraction of their runtime, as their execution
engines are more sophisticated - multiple SIMD lanes, low ﬂoating
point instruction latency etc. Also, since they support features such
as prefetching where the data is already fetched into the higher
levels of the memory subsystem, avoiding redundant data fetches
has a less prominent impact on performance.

popular Caffe [53] deep learning framework, wherein the matrices
corresponding to each layer and each input batch was formed in
Caffe and fed into the gem5 simulator to perform the matrix com-
putations. The results were fed back to Caffe to form the inputs
for the next layer (or input batch), and so on. Figure 13(a) shows
the gem5 system conﬁguration used in our experiments. All ex-
periments were run in full-system mode. We evaluate SPARCE by
measuring application level execution times under two scenarios.
The ﬁrst scenario targets embedded scalar processors that are
present in ultra-low power edge/IoT devices and lack support for
high performance libraries. We chose a scalar ARM v8 in-order
processor architecture as the baseline. We leverage the modular na-
ture of gem5 to cater to this scenario, wherein we disable support
for advanced architectural features such as SIMD processing and
pre-fetching in the ARM v8 processor. We then prototyped a direct
convolution routine (which we call Dir-Conv-Scalar) that does
not utilize the disabled features and used it in our experiments.
The second scenario targets a reasonably sophisticated mobile
processor for which we chose the ARM v8 in-order processor
architecture with 4-way SIMD as the baseline. In this case, the
matrix computations were realized using the highly-optimized
OpenBLAS [55] based GEMM routines described in Section 4.2.
We refer to this implementation as OpenBLAS-SIMD4.

5.2 Power and Area Evaluation

We implemented the hardware extensions of SPARCE at
the
Register Transfer Level (RTL) using Verilog HDL and synthesized
them to IBM 45nm technology using Synopsys Design Compiler
to measure its power and area overheads. For the conﬁguration
shown in Figure 13(a), the area overhead is 0.4% of the ARM
Cortex A35 core [43]. Thus the area overhead of SPARCE is
quite minimal allowing its deployment in the resource-constrained
embedded platforms.

Fig. 14. Improvement in execution time at the application level

Among the benchmarks, the execution time beneﬁts are largely
proportional to the amount of sparsity that they exhibit (Figure 2 in
Section 2). The Deep Compression-AlexNet benchmark achieves
the most beneﬁts because both its feature and weight data-
structures are sparse, as opposed to other benchmarks whose
weight data-structure is dense. In the context of training, the
backpropagation step achieves more improvement compared to
forward propagation. This stems from the fact that the error data-
structure is more sparse compared to features.

Execution Time Breakdown. To better appreciate the improve-
ments achieved by SPARCE, Figure 15 hierarchically breaks down
the application execution time (in the context of both Dir-Conv-
Scalar and OpenBlas-SIMD4 implementations) into components
that can and cannot be impacted by SPARCE. At the top level,
the solid yellow and gray colors represent the execution time
fraction that cannot be improved by SPARCE. This is primar-
ily constituted by auxiliary DNN operations such as activation
functions, subsampling and others, which represent 1.9% and
12.2% of the runtime for Dir-Conv-Scalar and OpenBlas-SIMD4
implementations, respectively. It is noteworthy that although these

Fig. 13. (a) Gem5 simulation parameters (b) Application benchmarks

5.3 Benchmarks

Our benchmark suite, listed in Figure 13(b), consists of 6 state-of-
the-art image-recognition DNNs viz. CIFAR-Caffe DNN using the
CIFAR-10 dataset [57], and AlexNet [52], VGG-16 [2], ResNet-
50 [1], GoogleNet [3] and Deep Compression-AlexNet [36] using
the ImageNet dataset. These benchmarks contained 5-50 layers
and took 0.01-15.4 Billion scalar operations to classify an im-
age. We utilized pre-trained models from the Caffe Model Zoo
to evaluate SPARCE in the context of inference. For training,
we utilized only the smaller CIFAR-10 benchmark, as training
ImageNet models on gem5 was prohibitively time consuming.

Processor config.ARMv8-A, In-order        SPARCEconfig.20 SASA table entries, 32 SpRFentries L1 CacheSplit I&D,  32KB I cache,  64KB D cache, 2-way set associative I & D cache, 64B line, 3-cycles/accessL2 CacheUnified 2MB 8-way set associative, 64B line, 12 cycles/ accessBenchmarkDataset# Layers#Ops CIFAR-10CIFAR-1050.01BAlexNetImageNet80.72BVGG-16ImageNet1615.4BResNet-50ImageNet503.86BGoogleNetImageNet221.59 BDeepComp.ImageNet80.72B(b)(a)0.60.70.80.911.1Normalized execution time →Dir-Conv-ScalarOpenBLAS-SIMD4BaselineCIFAR-10AlexNetDeepCompGoogleNetVGG-16ResNet-50CIFAR-10 TrainingGeoMeanoperations occupy <1% of the total DNN FLOPs, they occupy
a substantially larger fraction of the runtime for the OpenBLAS-
SIMD4 implementation. This is owed to the fact that they are
typically memory-bound (higher Bytes/FLOP ratio), which is
further ampliﬁed as matrix multiply operations are signiﬁcantly
optimized by the GEMM subroutine. Also, since the inputs to
the DNN are typically dense, the ﬁrst DNN layer exhibits little
redundancy. This occupies 14.3% and 16.9% of the total runtimes
of AlexNet for Dir-Conv-Scalar and OpenBLAS-SIMD4 imple-
mentations, respectively. The fraction grows smaller in deeper
networks such as ResNet and VGG.

In Figure 15 the green color bars denote the computations
that can accelerated by leveraging sparsity (∼71%). For Dir-
Conv-Scalar implementation, this is limited to 83.6% of the base-
line AlexNet runtime. Since AlexNet contains ∼36% redundant
the best case beneﬁts
computations (Figure 4 in Section 2),
are limited to 29.8%, of which SPARCE achieves 22.3%. For
the OpenBLAS-SIMD4 implementation, the underlying GEMM
involves supplementary operations like memory allocate, copy
and free operations, which as marked by the dotted patterns
and consumes 27% of the total execution time. This constraints
the opportunity for SPARCE to ∼44% of AlexNet runtime as
shown by the diagonally hatched portions in Figure 15. Since
AlexNet contains ∼36% redundant computations (Figure 4 in
Section 2), the best case beneﬁts are limited to ∼16%, of which
SPARCE achieves 12% improvement as other control operations
such as pointer arithmetic, prefetching etc. present within the loop
body cannot be avoided.

Fig. 15. Execution time breakdown for AlexNet

Layer-wise Beneﬁts. We now present the layer-wise breakdown of
the beneﬁts quantiﬁed in terms of the execution time, instructions
and data cache (D-Cache) accesses skipped for the convolutional
layers of AlexNet. Figure 16 shows the beneﬁts in the context of
both SIMD and scalar processor implementations. As shown in
Figure 16, we are able to achieve, on an average, 39.4% reduction
in instruction count and 35.1% reduction in D-Cache accesses for
Dir-Conv implementations on low-power embedded scalar pro-
cessors. The reduction in instruction count and D-Cache accesses
amount to 30.5% and 14% respectively for OpenBLAS-SIMD4
implementations. We also observe the beneﬁts are typically larger
for layers deeper in the DNN, as they typically exhibit more
sparsity.

Energy Beneﬁts. Power evaluation of the SPARCE reveals that
it consumes 1.74 mW at 1 GHz, which amounts to 1.9% of
the 90 mW power consumed by even the most power-efﬁcient

10

Fig. 16. Layer-wise beneﬁts breakdown for AlexNet

baseline ARM v8 processor,
the Cortex A35 processor [43].
Accordingly, the execution time beneﬁts translate to beneﬁts in the
range of 16.9%-28.7% reduction in application-level energy for a
Dir-Conv-Scalar implementation. In the context of OpenBLAS-
SIMD4 implementations, the reduction in energy ranges between
6.1%-13.2% across the benchmarks.

6.2 Performance Scaling with Sparsity

We now study how the performance of SPARCE scales with
increasing levels of sparsity. To this end, we considered a matrix
multiplication problem (B×A=C), wherein the dimensions of the
input matrices B and A were 169×3456 and 3456×384 respec-
tively. We varied the sparsity of the B matrix by constraining the
number of zero entries. The location of the zeros and other entries
of the matrices were chosen at random. Figure 17 shows how the
execution time and the fraction of instructions executed varies with
sparsity in the context of both Dir-Conv-Scalar and OpenBLAS-
SIMD4 implementations. We ﬁnd that both implementations ex-
hibit strong performance scaling with sparsity, outlining the ability
of SPARCE to efﬁciently skip computations. We ﬁnd the number
of instructions executed to be larger than ideal (dotted line in
Figure 17) due to the presence of control instructions for pointer
arithmetic, loop counts etc., in the program, which cannot be
skipped. Also, the disparity in the fraction of instructions executed
and the resultant execution time beneﬁts is more pronounced
for the OpenBLAS-SIMD4 implementation. We attribute this to
the intelligent instruction ordering in the GEMM routine utilized
in the implementation, wherein computations are aggressively
overlapped with data-fetches. Therefore, even if computations are
skipped, the improvement in performance is limited by the time
taken for the data-fetches.

6.3 Operand Ordering in SparCE OpenBLAS-SIMD4 Im-
plementations

As described in Section 4, based on the amount of spar-
sity exhibited by the data-structures, mapping the right data-
structure as the shared-SIMD operand can have a consid-
In the case of all bench-
erable impact on performance.
marks other than Deep Compression-AlexNet, only the feature

Dir-Conv-ScalarOpenBLAS-SIMD4 00.20.40.60.81BaselineSparCE00.20.40.60.81BaselineSparCENorm. exec. time→01234561Chart TitleAuxiliary ComputationsFirst DNN LayerOther LayersOther Layers - sgemm OverheadOther Layers - sgemm Compute00.250.50.751conv2conv3conv4conv5MeanNorm. scale →00.250.50.751conv2conv3conv4conv5MeanNorm. scale →Execution timeInstructionsDcache accessesOpenBLAS-SIMD4Dir-Conv-Scalar11

tations. Approximate computing techniques, such as low-precision
implementations [32]–[35], model compression [36], [37] and oth-
ers [38], [39], leverage this property to improve the computational
efﬁciency of DNNs.
Post-CMOS technology. Post-CMOS technologies such as mem-
ristors and spintronics have succeeded in realizing the computa-
tional primitives of DNNs through their intrinsic device charac-
teristics. Implementations of small scale DNNs using memristor
crossbar arrays [40], [42] and spintronic devices [41] have demon-
strated signiﬁcant promise.

All the above efforts are complementary to SPARCE, as unlike
above efforts, we attempt to leverage sparsity in the different
DNN data-structures to improve efﬁciency on cost constrained
embedded platforms comprising of mainly a GPP core.
Exploiting sparsity. Prior efforts that exploit sparsity to improve
DNN efﬁciency can be grouped to two classes based on whether
they exploit static sparsity or dynamic sparsity, as shown in
Figure 1. Specialized sparse architectures that are capable of
exploiting both static and dynamic sparsity incorporate a variety
of compression techniques and zero-skipping schemes to reduce
storage requirement and avoid redundant multiplications in accel-
erators [31], [44]–[48].

On the other hand, software approaches that exploit weight
sparsity on GPPs take advantage of sparse matrix libraries. These
sparse libraries usually yield performance improvement only un-
der extreme levels of sparsity (>95%). Since DNNs naturally
exhibit sparsity in the range of 40%-70%, a few research efforts
force more weight sparsity into DNNs using sparse decomposition
methods, etc. [49], [50] or customize the pruning to match the
underlying hardware organization [51]. These invariably come at
the cost of training overhead or loss of functional accuracy. In
contrast, we propose micro-architectural extensions to GPPs that
can exploit both dynamic and static sparsity while being effective
even under intermediate levels. Thus, SPARCE is able to exploit
feature sparsity in several state-of-the-art dense DNN models as
well as both feature and weight sparsity in existing pruned DNN
models.

8 CONCLUSION
As DNNs pervade the spectrum of computing devices, new ap-
proaches to improve their computational efﬁciency on resource-
constrained IoT/ edge devices becomes critical. In this work,
we accelerate DNNs on GPPs, which are an indispensable part
of IoT/ edge devices, by exploiting sparsity in the different
DNN data-structures. To this end, we propose sparsity aware
general purpose core extensions (SPARCE) that enable GPPs to
efﬁciently leverage sparsity, while being minimally intrusive and
low-overhead. SPARCE comprises of two key micro-architectural
enhancements. First, a Sparsity Register File (SpRF) dynamically
tracks zero-valued registers in the processor. Next, a Sparsity
Aware Skip Address (SASA) table indicates potentially redundant
instruction sequences and the conditions under which they can
be skipped. A Pre-identify and Skip Redundancy Unit (PSRU)
combines the information from the SpRF and the SASA table to
dynamically pre-identify if an instruction sequence can be skipped,
and if so masks it from being fetched and executed. We evaluate
SPARCE on 6 image-recognition DNNs in the context of both
training and inference. Our evaluations reveal that SPARCE is a
promising design that allows us to exploit all forms of static and
dynamic sparsity to accelerate DNNs on GPPs.

Fig. 17. SPARCE performance scaling with sparsity

data-structure is sparse. Therefore, mapping it as the shared-
SIMD operand (Matrix B in the sgemm subroutine) would
yield the best beneﬁts. Figure 18 shows the performance im-
provement achieved when operands are ordered in both ways
viz. Features×Weights and Weights×Features. In the context
of AlexNet, we ﬁnd that mapping features as the shared-
SIMD operand yields 1.86× better beneﬁts (∼12% vs. 6.5%)
compared to mapping the non-sparse weight data-structure.
For the Deep Compression-
AlexNet network, since both
data-
and weight
feature
structures are sparse, the dis-
parity in performance due to
operand ordering is relatively
small (<2%). Even in this
case, we ﬁnd that choosing
features as the shared-SIMD
operand is beneﬁcial. This is
attributed to the fact that some
of the weight matrices have
high degree of sparsity, and
their zero entries are typi-
cally clustered. Therefore, us-
ing weights as the non-shared
SIMD operand has less of an adverse impact on performance
compared to using features.

Fig. 18. Impact of operand ordering
on performance

7 RELATED WORK
Prior research efforts that target improving the computational
efﬁciency of DNNs can be grouped into the following broad
classes.

Software parallelization on multi-cores and GPUs. A large
number of previous efforts have been directed towards developing
techniques for efﬁcient parallelization of DNNs on multi-core
servers and GPUs [13]–[19]. However, the scalability of these
techniques is often limited by synchronization and communication
bottlenecks.

Specialized accelerators. Developing specialized hardware archi-
tectures has been an attractive approach to improve the compu-
tational efﬁciency of DNNs. These accelerators [20]–[26], [28]–
[31] utilize specialized processing cores, interconnect network and
other hardware-software co-design methodologies to leverage the
different forms of compute and data reuse patterns in DNNs.

Approximate computing. DNNs and the applications that use
them are intrinsically resilient to errors in their underlying compu-

00.20.40.60.810255075100Sparsity (%) Execution timeInstruction Count00.20.40.60.810255075100Normalized scale →Sparsity (%) Execution timeInstruction countOpenBLAS-SIMD4Dir-Conv-Scalar0.80.850.90.95W	*	FF	*	W Norm.	exec.	time	→AlexNetDeepCompF:	FeaturesW:	WeightsREFERENCES

[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual

learning for image recognition. CoRR, abs/1512.03385, 2015.

[2] K. Simonyan and A. Zisserman. Very deep convolutional networks for

large-scale image recognition. CoRR, abs/1409.1556, 2014.

[3] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E.
Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and
CoRR,
Andrew Rabinovich.
abs/1409.4842, 2014.

Going deeper with convolutions.

[4] S. Venugopalan, M. Rohrbach, J. Donahue, R. J. Mooney, T. Darrell,
In 2015 IEEE
and K. Saenko. Sequence to sequence - video to text.
International Conference on Computer Vision, ICCV 2015, Santiago,
Chile, December 7-13, 2015, pages 4534–4542, 2015.

[5] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg
Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta,
Adam Coates, and Andrew Y. Ng. Deep speech: Scaling up end-to-end
speech recognition. CoRR, abs/1412.5567, 2014.

[6] Xiang Zhang and Yann LeCun. Text understanding from scratch. CoRR,

abs/1502.01710, 2015.

[7] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman
Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for
acoustic modeling in speech recognition. Signal Processing Magazine,
2012.
Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jian-
feng Gao, and Bill Dolan. A persona-based neural conversation model.
March 2016.
Improving photo search: A step across the semantic gap. Google
Research blog, 2013.

[8]

[9]

[10] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing
the gap to human-level performance in face veriﬁcation. In 2014 IEEE
Conference on Computer Vision and Pattern Recognition, pages 1701–
1708, June 2014.

text

deeptext:

[11] Introducing

Facebook’s
https://code.facebook.com/posts/181565595577955/introducing-
deeptext-facebook-s-text-understanding-engine/.
2016.
[12] Apple
gence:
analysis/hLMSxZKVnEqO. Mashable, 2016.

Intelli-
http://mashable.com/2016/06/13/siri-sirikit-wwdc2016-

next-level Artiﬁcial

Facebook Code,

understanding

engine:

turning

into

Siri

is

a

[13] Alex Krizhevsky. One weird trick for parallelizing convolutional neural

networks. CoRR, abs/1404.5997, 2014.

[14] Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, and Kurt
Firecaffe: near-linear acceleration of deep neural network

Keutzer.
training on compute clusters. CoRR, abs/1511.00175, 2015.

[15] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang,
Quoc V. Le, and Andrew Y. Ng. Large scale distributed deep networks. In
P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bottou, and K.q. Weinberger,
editors, Advances in Neural Information Processing Systems 25, pages
1232–1240. 2012.

[16] M. Rhu, N. Gimelshein, J. Clemons, A. Zulﬁqar, and S. W. Keckler.
vdnn: Virtualized deep neural networks for scalable, memory-efﬁcient
neural network design. In 2016 49th Annual IEEE/ACM International
Symposium on Microarchitecture (MICRO), pages 1–13, Oct 2016.
[17] Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan
Vaidyanathan, Srinivas Sridharan, Dhiraj D. Kalamkar, Bharat Kaul, and
Pradeep Dubey. Distributed deep learning using synchronous stochastic
gradient descent. CoRR, abs/1602.06709, 2016.

[18] Aleksandar Zlateski, Kisuk Lee, and H. Sebastian Seung. ZNN - A
fast and scalable algorithm for training 3d convolutional networks on
In 2016 IEEE
multi-core and many-core shared memory machines.
International Parallel and Distributed Processing Symposium, IPDPS
2016, Chicago, IL, USA, May 23-27, 2016, pages 801–811, 2016.

[19] Neon, nervana systems: http://neon.nervanasys.com/docs/latest/index.html.

2016.

[20] C. Farabet, B. Martini, B. Corda, P. Akselrod, E. Culurciello, and
Y. LeCun. Neuﬂow: A runtime reconﬁgurable dataﬂow processor for
vision. In CVPR 2011 WORKSHOPS, pages 109–116, June 2011.
[21] Srimat Chakradhar, Murugan Sankaradas, Venkata Jakkula, and Srihari
Cadambi. A dynamically conﬁgurable coprocessor for convolutional
neural networks. SIGARCH Comput. Archit. News, 38(3):247–257, June
2010.

[22] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji
Chen, and Olivier Temam. Diannao: A small-footprint high-throughput
accelerator for ubiquitous machine-learning. In Proceedings of the 19th
International Conference on Architectural Support for Programming

12

Languages and Operating Systems, ASPLOS ’14, pages 269–284, New
York, NY, USA, 2014. ACM.

[23] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang,
Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam.
In Proceedings of the
Dadiannao: A machine-learning supercomputer.
47th Annual IEEE/ACM International Symposium on Microarchitecture,
MICRO-47, pages 609–622, Washington, DC, USA, 2014. IEEE Com-
puter Society.

[24] Swagath Venkataramani, Ashish Ranjan, Subarno Banerjee, Dipankar
Das, Sasikanth Avancha, Ashok Jagannathan, Ajaya Durg, Dhee-
manth Nagaraj, Bharat Kaul, Pradeep Dubey, and Anand Raghunathan.
Scaledeep: A scalable compute architecture for learning and evaluating
deep networks. In Proceedings of the 44th Annual International Sympo-
sium on Computer Architecture, ISCA ’17, pages 13–26, New York, NY,
USA, 2017. ACM.

[25] N. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P. Cantin, C.
Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V.
Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D.
Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski,
A. Kaplan, H. Khaitan, A. Koch, N. Kumar, S. Lacy, J. Laudon, J.
Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A.
Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R.
Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross,
M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J.
Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma,
E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H.
Yoon. In-datacenter performance analysis of a tensor processing unit. In
Proceedings of the 44th Annual International Symposium on Computer
Architecture, ISCA ’17, pages 1–12, New York, NY, USA, 2017. ACM.
[26] Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama,
Hyunkwang Lee, Sae Kyu Lee, Jos´e Miguel Hern´andez-Lobato, Gu-
Yeon Wei, and David Brooks. Minerva: Enabling low-power, highly-
accurate deep neural network accelerators. In Proceedings of the 43rd
International Symposium on Computer Architecture, ISCA ’16, pages
267–278, Piscataway, NJ, USA, 2016. IEEE Press.

[27] Swagath Venkataramani, Vinay K. Chippa, Srimat T. Chakradhar,
Kaushik Roy, and Anand Raghunathan. Quality programmable vec-
the
tor processors for approximate computing.
46th Annual IEEE/ACM International Symposium on Microarchitecture,
MICRO-46, pages 1–12, New York, NY, USA, 2013. ACM.

In Proceedings of

[28] S. Eldridge, A. Waterland, M. Seltzer, J. Appavoo, and A. Joshi. Towards
general-purpose neural network computing. In 2015 International Con-
ference on Parallel Architecture and Compilation (PACT), pages 99–112,
Oct 2015.

[29] Abhinandan Majumdar, Srihari Cadambi, Michela Becchi, Srimat T.
Chakradhar, and Hans Peter Graf. A massively parallel, energy efﬁcient
programmable accelerator for learning and classiﬁcation. ACM Trans.
Archit. Code Optim., 9(1):6:1–6:30, March 2012.

[30] V. Gokhale, J. Jin, A. Dundar, B. Martini, and E. Culurciello. A 240
In 2014 IEEE
g-ops/s mobile coprocessor for deep neural networks.
Conference on Computer Vision and Pattern Recognition Workshops,
pages 696–701, June 2014.

[31] Y. H. Chen, T. Krishna, J. S. Emer, and V. Sze. Eyeriss: An energy-
efﬁcient reconﬁgurable accelerator for deep convolutional neural net-
works. IEEE Journal of Solid-State Circuits, 52(1):127–138, Jan 2017.

[32] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish
In Pro-
Narayanan. Deep learning with limited numerical precision.
ceedings of the 32Nd International Conference on International Confer-
ence on Machine Learning - Volume 37, ICML’15, pages 1737–1746.
JMLR.org, 2015.

[33] Swagath Venkataramani, Ashish Ranjan, Kaushik Roy, and Anand
Raghunathan. Axnn: Energy-efﬁcient neuromorphic systems using
the 2014 International
approximate computing.
Symposium on Low Power Electronics and Design, ISLPED ’14, pages
27–32, New York, NY, USA, 2014. ACM.

In Proceedings of

[34] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained

ternary quantization. CoRR, abs/1612.01064, 2016.

[35] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Bina-
ryconnect: Training deep neural networks with binary weights during
propagations. CoRR, abs/1511.00363, 2015.

[36] Song Han, Huizi Mao, and William J. Dally. Deep compression:
Compressing deep neural network with pruning, trained quantization and
huffman coding. CoRR, abs/1510.00149, 2015.

[37] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding
up convolutional neural networks with low rank expansions. CoRR,
abs/1405.3866, 2014.

13

[55] Openblas: An optimized blas library: http://www.openblas.net/. 2017.
[56] Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K. Rein-
hardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R. Hower, Tushar
Krishna, Somayeh Sardashti, Rathijit Sen, Korey Sewell, Muhammad
Shoaib, Nilay Vaish, Mark D. Hill, and David A. Wood. The gem5
simulator. SIGARCH Comput. Archit. News, 39(2):1–7, August 2011.

[57] Alex Krizhevsky. Learning multiple layers of features from tiny images.

Technical report, 2009.

[38] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit
stochastic gradient descent and application to data-parallel distributed
training of speech dnns. In Interspeech 2014, September 2014.

[39] Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji Liu. Staleness-aware

async-sgd for distributed deep learning. CoRR, abs/1511.05950, 2015.

[40] Xiaoxiao Liu, Mengjie Mao, Beiye Liu, Hai Li, Yiran Chen, Boxun Li,
Yu Wang, Hao Jiang, Mark Barnell, Qing Wu, and Jianhua Yang. Reno:
A high-efﬁcient reconﬁgurable neuromorphic computing accelerator de-
sign. In Proceedings of the 52Nd Annual Design Automation Conference,
DAC ’15, pages 66:1–66:6, New York, NY, USA, 2015. ACM.

[41] S. G. Ramasubramanian, R. Venkatesan, M. Sharad, K. Roy, and
A. Raghunathan. Spindle: Spintronic deep learning engine for large-
scale neuromorphic computing. In Low Power Electronics and Design
(ISLPED), 2014 IEEE/ACM International Symposium on, pages 15–20,
Aug 2014.

[42] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P.
Strachan, M. Hu, R. S. Williams, and V. Srikumar. Isaac: A convolutional
neural network accelerator with in-situ analog arithmetic in crossbars. In
2016 ACM/IEEE 43rd Annual International Symposium on Computer
Architecture (ISCA), pages 14–26, June 2016.

[43] ARM Announces New Cortex-A35 CPU - Ultra-High Efﬁciency
For Wearables & More: https://www.anandtech.com/show/9769/arm-
announces-cortex-a35. Anandtech, 2015.

[44] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and
A. Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network
computing. In 2016 ACM/IEEE 43rd Annual International Symposium
on Computer Architecture (ISCA), pages 1–13, June 2016.

[45] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A.
Horowitz, and William J. Dally. EIE: efﬁcient inference engine on
compressed deep neural network. CoRR, abs/1602.01528, 2016.

[46] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli,
Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keck-
ler, and William J. Dally. Scnn: An accelerator for compressed-sparse
In Proceedings of the 44th Annual
convolutional neural networks.
International Symposium on Computer Architecture, ISCA ’17, pages
27–40, New York, NY, USA, 2017. ACM.

[47] S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo, T. Chen, and
Y. Chen. Cambricon-x: An accelerator for sparse neural networks. In
2016 49th Annual IEEE/ACM International Symposium on Microarchi-
tecture (MICRO), pages 1–12, Oct 2016.

[48] Hyunsun Park, Dongyoung Kim, Junwhan Ahn, and Sungjoo Yoo. Zero
and data reuse-aware fast convolution for deep neural networks on gpu. In
Proceedings of the Eleventh IEEE/ACM/IFIP International Conference
on Hardware/Software Codesign and System Synthesis, CODES ’16,
pages 33:1–33:10, New York, NY, USA, 2016. ACM.

[49] Baoyuan Liu, Min Wang, H. Foroosh, M. Tappen, and M. Penksy. Sparse
convolutional neural networks. In 2015 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 806–814, June 2015.
[50] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
Learning structured sparsity in deep neural networks.
In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances
in Neural Information Processing Systems 29, pages 2074–2082. Curran
Associates, Inc., 2016.

[51] Jiecao Yu, Andrew Lukefahr, David Palframan, Ganesh Dasika, Reetu-
parna Das, and Scott Mahlke. Scalpel: Customizing dnn pruning to the
In Proceedings of the 44th Annual
underlying hardware parallelism.
International Symposium on Computer Architecture, ISCA ’17, pages
548–560, New York, NY, USA, 2017. ACM.

[52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet
In F. Pereira,
classiﬁcation with deep convolutional neural networks.
C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 25, pages 1097–1105. Curran
Associates, Inc., 2012.

[53] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan
Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe:
Convolutional architecture for fast feature embedding. In Proceedings of
the 22Nd ACM International Conference on Multimedia, MM ’14, pages
675–678, New York, NY, USA, 2014. ACM.

[54] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,
J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wat-
tenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems. ArXiv e-
prints, March 2016.

