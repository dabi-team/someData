ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep
Reinforcement Learning

Naichen Shi * 1 Ruichen Li * 2 Sun Youran * 3

1
2
0
2

b
e
F
5
1

]
T
G
.
s
c
[

1
v
5
9
4
7
0
.
2
0
1
2
:
v
i
X
r
a

Abstract

People have made remarkable progress in game
AIs, especially in domain of perfect information
game. However, trick-taking poker game, as a
popular form of imperfect information game, has
been regarded as a challenge for a long time.
Since trick-taking game requires high level of
not only reasoning, but also inference to excel, it
can be a new milestone for imperfect information
game AI. We study Gongzhu, a trick-taking game
analogous to, but slightly simpler than contract
bridge. Nonetheless, the strategies of Gongzhu
are complex enough for both human and com-
puter players. We train a strong Gongzhu AI Scro-
faZero from tabula rasa by deep reinforcement
learning, while few previous efforts on solving
trick-taking poker game utilize the representation
power of neural networks. Also, we introduce
new techniques for imperfect information game
including stratiﬁed sampling, importance weight-
ing, integral over equivalent class, Bayesian in-
ference, etc. Our AI can achieve human expert
level performance. The methodologies in building
our program can be easily transferred into a wide
range of trick-taking games.

1. Introduction

We live in a world full of precariousness. Like a famous
quotation from Sherlock Holmes, “I have a turn both for
observation and for deduction.”(Doyle), one should deduce
hidden information from seemingly random observations
to make good decisions. Imperfect information game is
an abstraction of multi-agent decision making with private
information. Related theory has been found useful in auc-
tion, mechanism design, etc (Tadelis, 2013). The research
of speciﬁc examples of imperfect information game can

*Equal contribution 1IOE, University of Michigan 2EECS,
Peking University 3Yau Mathematical Sciences Center, Ts-
Sun Youran
inghua University..
<syouran0508@gmail.com>.

Correspondence to:

strengthen our abilities to navigate through the uncertainties
of the world.

People have successfully built super-human AIs for perfect
information games including chess (Silver et al., 2017) and
Go (Silver et al., 2018) by using deep reinforcement learning.
Also, by combining deep learning with imperfect sampling,
researchers have also made huge progress in Mahjong (Li
et al., 2020), StarCraft (Vinyals et al., 2019), Dota (OpenAI
et al., 2019), and Texas hold’em (Brown et al., 2019).

We study Gongzhu, a 4-player imperfect information poker
game. Gongzhu is tightly connected with a wide range of
trick-taking games. The detailed rules will be introduced in
section 2. Building a strong Gongzhu program can deepen
our understanding about imperfect information games.

We study Gongzhu for three reasons. Firstly, Gongzhu
contains medium level of randomness and requires careful
calculations to reign supreme. Compared with Mahjong and
Texas hold’em, Gongzhu is more complicated since its deci-
sion space is larger, let alone toy poker games pervasively
studied in like Leduc(Southey et al., 2005) and Kuhn(Kuhn,
1950). What’s more, it is important to read the signals from
the history of other players’ actions and update beliefs about
their private information continuously. The entanglement
of sequential decision making and imperfect information
makes it extremely nontrivial to ﬁnd a good strategy out of
high degree of noise.

Secondly, the scoring system of Gongzhu is relatively
simple compared with bridge, since players don’t bid in
Gongzhu. Thus the reward function is easier to design, and
we can focus on training highly skilled playing AI given
such reward function.

Thirdly, compared with large scale games like StarCraft or
Dota, Gongzhu is more computationally manageable: all
experiments in this paper can be done on only 2 Nvidia
2080Ti GPUs!

We train a Gongzhu program from tabula rasa by self play
without any prior human knowledge other than game rules.
Our algorithm is a combination of Monte Carlo tree search
and Bayesian inference. This method is an extension of
MCTS to imperfect information game. Our program defeats

 
 
 
 
 
 
ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

expert level human Gongzhu players in our online platform
Gongzhu Online.

We summarize our contributions below:

• We introduce the game of Gongzhu that is more difﬁ-
cult than Leduc but more manageable than StarCraft.
Gongzhu can be a benchmark for different multi-agent
reinforcement learning algorithms.

• We train a strong Gongzhu agent ScrofaZero purely by
self-play. The training of ScrofaZero requires neither
human expert data nor human guidance beyond game
rules.

• To our best knowledge, we are the ﬁrst to combine
Bayesian inferred importance sampling with deep neu-
ral network for solving trick-taking game. Our methods
can be transferred to other trick-taking games including
contract Bridge.

The paper is organized as follows. In section 2, we review
the rule of Gongzhu and its connection to other trick-taking
games. In section 3, we discuss related work in literature.
In section 4, we present an overview of our framework. In
section 5, we show some of our key methodologies including
stratiﬁed sampling and integral over equivalent class. And in
the section 6, we analyze the results of extensive empirical
experiments.

2. Rule of Gongzhu and Related Games

Before delving further into our program, we will use a
section to familiarize readers the rules of Gongzhu. We
will introduce rules from general trick-taking poker game to
speciﬁc game Gongzhu.

Gongzhu belongs to the class trick-taking, which is a large
set of games including bridge, Hearts, Gongzhu and Shengji.
For the readers unfamiliar with any of these games, see
supplementary material 8.1 for the common rules of trick
taking games. From now on, we assume readers understand
the concept of trick.

The class trick-taking is divided into two major families
according to the goal of the games: family plain-trick and
family point-trick. In family plain-trick, like Whist, Con-
tract bridge and Spades, the goal of games is to win speciﬁc
or as many as possible number of tricks. In family point-
trick, like Black Lady, Hearts, Gongzhu and Shengji, the
goal of games is to maximize the total points of cards ob-
tained.

For most of games in the family point-trick, only some cards
are associated with points. Depending on the point counting
system, family point-trick is furthermore subdivided into
two genus, evasion and attack. For genus evasion, most

Figure 1. The classiﬁcation of trick-taking games. Gongzhu be-
longs to the genus evasion, the family point-trick. As shown in this
ﬁgure, Gongzhu is tightly connected with a wide variety of games.

of points are negative, so the strategy is usually to avoid
winning tricks, while genus attack the opposite. Gongzhu
belongs to the genus evasion. The points in Gongzhu are
counted by following rules.

1. Every heart is associated with points. Their points are

shown in table 1. Notice that,

• all points of hearts are non-positive;
• the higher rank a heart card has, the greater its

point’s absolute value will be;
• the total points of hearts are -200.

2. SQ has −100 points. SQ is called zhu (scrofa) in this
game. As zhu has the most negative points, players
will try their best avoid getting it.

3. DJ has +100 points. DJ is called yang (sheep/goat), in

contrast with zhu.

4. C10 will double the points. However if one gets only
C10, this C10 is counted as +50. C10 is called trans-
former for obvious reasons.

5. If one player collects all the 13 hearts, to reward her
braveness and skill, the points of all the hearts will
be count as +200 rather than −200. This is called all
hearts. It is worth clarify that,

• to get all hearts, one need get all 13 hearts, in-

cluding zero point hearts H2, H3 and H4;

• the points are counted separately for each player

and then summed together in each team.

All the rules except all hearts are summarized in table 1.
The classiﬁcation of trick-taking games and where Gongzhu

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

belongs are shown in ﬁgure 1.

HA
HK
HQ
HJ

−50
−40
−30
−20
H5 to H10 −10
H2 to H4

0

SQ
DJ

C10

−100
+100
+50 or double
the points

Table 1. Points of cards in Gongzhu. Gongzhu is a trick-taking
class, point-trick family, evasion genus game. Note that except for
points shown in the above table, there is an extra all hearts rule
explained in Section 2.

3. Literature review

3.1. Incomplete Information Game

Different from Go, Gongzhu is a dynamic incomplete infor-
mation game (Tadelis, 2013). The research on the dynam-
ics of imperfect information game and type of equilibrium
has a long history, for example (Selten, 1975) introduces
a more reﬁned equilibrium concept called trembling hand
equilibrium. Our work applies these theoretical analysis
including Bayesian inference and sequential decision mak-
ing to the trick-taking game Gongzhu. Recently, people
proposed counterfactual regret minimization related algo-
rithms (Zinkevich et al., 2007; Brown & Sandholm, 2019)
that can be proved to ﬁnd an (cid:15)-Nash equilibrium. The ap-
plications of counterfactual regret minimization type algo-
rithms have been found successful in Texas hold’em (Brown
et al., 2019). Such algorithms are not directly applicable
to Gongzhu, which has a larger decision space and longer
decision period.

3.2. Computer Bridge

As shown in section 2, Gongzhu is tightly connected with
bridge. However unlike chess and go, computer bridge pro-
grams cannot beat human experts yet. The recent winners
of the World Computer-Bridge Championship (Wikipedia,
2021) are Wbridge51 (on 2018) and Micro Bridge2 (on
2019)3. Micro Bridge ﬁrst randomly generates unknown
hands under known conditions derived from history, then
apply tree search and pruning algorithms to make decision
under perfect information. Wbridge5 does not reveal their
algorithm to public, but it is believed to be similar to Micro
Bridge, i.e. human-crafted rules for bidding and heuristic
tree search algorithms for playing. Different from these
work, we use deep neural network to evaluate current situa-

1http://www.wbridge5.com/index.htm
2http://www.osk.3web.ne.jp/˜mcbridge/

index.html

3The 2020 Championship is cancelled. The next championship

will be in 2021.

tion and to generate actions. Recently, (Rong et al., 2019)
built a bidding program by supervised learning and then by
reinforcement learning. In contrast, we train our Gongzhu
program from tabula rasa.

3.3. Monte Carlo Tree search

The use of Monte Carlo tree search both as a good rollout
algorithm and a stable policy improvement operator is popu-
lar in perfect information game like go (Silver et al., 2018),
and chess (Silver et al., 2017). (Grill et al., 2020) analyzed
some theoretical properties of MCTS used in AlphaGo zero.
(Whitehouse, 2004; Browne et al., 2012) discusses popular
MCTS algorithms, including information set Monte Carlo
tree search (ISMCTS), an algorithm that combines MCTS
with incomplete. The Monte Carlo tree search algorithm
used in our program is the standard upper-conﬁdence bound
minimizing version, and computationally simpler than the
full ISMCTS.

3.4. Multi-agent RL

The dynamics of training multi-player game AIs by self-play
can be complicated. People have found counter-examples
where almost all gradient-based algorithms fail to converge
to Nash equilibrium (Letcher, 2021). Also, some games are
nontransitive for certain strategies (Czarnecki et al., 2020).
Instead of a Nash equilibrium strategy, we attempt to ﬁnd
a strong strategy that can beat human expert level player.
We also deﬁne a metric for the nontransitivity in the game.
Different from (Letcher et al., 2019), where nontransitivity
is deﬁned only in the parameter space, our metric can be
deﬁned for any strategy.

4. Framework

This section is an overview of our program. We start by
introducing some notations. As discussed above, Gongzhu
is a sequential game with incomplete information. The
players are denoted as N = {0, 1, 2, 3}. We denote inte-
ger u ∈ [0, 52] as the stage of game, which can also be
understand as how many cards have been played. We deﬁne
history hu ∈ H is a sequence of tuple {(it, at)}t=1,...u,
where t is stage, it is the player to take action at stage t, and
at is the card her played. History represent as the history
of card playing up to time u. We sometimes replace hu
by h for simplicity. We use h(t) denote the t-th tuple in h.
History h is publicly observable to all players. It’s natural
to assume that each player can perfectly recall, i.e. they can
remember the history exactly. After dealing of each round,
players will have their initial hand cards c = {ci}i=0,1,2,3.
ci is private information for player i, and other players can-
not see it. Also, we denote cu
i as the i-th player’s remaining
hand cards at stage u. Action a ∈ A is the card to play.
We set |A| = 52. By the rule of Gongzhu, a player may

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

not be able to play all cards depending on the suits of that
trick, thus actual legal choice of a might be smaller than the
number of remaining cards in one’s hand.

Information set is a concept in incomplete information game
theory that roughly speaking, characterizes the informa-
tion input for decision making. For standard Gongzhu
game, we deﬁne the information set for player i at time
u to be (hu, ci), i.e. public information hu combined with
player i’s private information ci. The payoff r is calculated
only at the end of the game, i.e. when all 52 cards are
played. r(hT ) = (r0(hT ), r1(hT ), r2(hT ), r3(hT ))(where
T = 52) representing the scores for each player. A strategy
of player i is a map from history and private information to
a probability distribution in action space: πi : H × Ci → A,
i.e. given a speciﬁc history h and an initial card ci, πi(ci, h)
chooses a card to play. A strategy proﬁle π is the tuple of
4 players’ strategies π = (π0, π1, π2, π3). We use π−i to
denote the strategy of other players except i.

The value of an information set on the perspective of player
i is:

vi(πi, π−i, ci, h) = Ep(c−i|h,π)
= Ep(c−i|h,π)

(cid:2)ri(h52)(cid:3)(cid:3)
(cid:2)Eπ
(cid:2)vπ(h52, ci, c−i)(cid:3)

(1)

(cid:2)ri(h52)(cid:3) for simplicity.
We use vπ(h, ci, c−i) replace Eπ
vπ(h, ci, c−i) can be interpreted as value of a state under
perfect information. h52 is the history of all possible ter-
minal state starting from h with initial hands c. The inner
expectation is taken in the following sense. Suppose there
exists an oracle that knows exactly each players’ initial cards
c and each player’s strategy π, it plays on each player’s be-
half with their strategy π starting from h till the end of
the game. Due to the randomness of mixed strategy, the
outcome, thus the payoff of the game is also random. The
inner expectation is taken over the randomness of the game
trajectory resulted from mixed strategy.

The outer expectation is taken over player i’s belief. We
deﬁne a scenario to be one possible initial hand conﬁgu-
ration c−i from the perspective of player i. In Gongzhu,
one player’s belief is the probability p(c|h, π) she assigns
to each possible scenario. At the beginning of the game, all
possible scenarios are of equal probability. This is the case
when h = h0 = ∅, which is usually referred as common
prior by game theorists. When the game proceeds, every
player can see what have been played by other players, and
this will change one’s belief about the possibility of different
scenarios. For example, clearly some initial hands conﬁg-
urations that are not compatible with the rule of Gongzhu
will have probability 0. A natural way of updating belief is
Bayes’ rule. We will cover more details on calculating the
outer expectation on section 5.3 and 5.4.

A player’s goal is to optimize its strategy on every decision

node, assuming that she knows other players’ strategy π−i:

max
πi

vi(πi, π−i, ci, h)

(2)

She will choose an action on every decision node to maxi-
mize her expected payoff. Gongzhu ends after all 52 cards
are played. The extensive form of the game can be regarded
as a 13 × 4-layer decision tree. Thus in principle, Bayes
perfect equilibrium (Tadelis, 2013) can be solved by back-
ward induction. However we do not to solve for exact Bayes
perfect equilibrium here because (i) compared with obtain-
ing a Bayes perfect equilibrium policy that is unexploitable
by others, it’s more useful to obtain a policy that can beat
most other high level players. (ii) an exact Bayes perfect
equilibrium is computationally infeasible.

To obtain a strong policy, we train a neural network by self-
play. Our neural network has a fully connected structure
(see supplementary material 8.3). We divide training and
testing into two separate processes. In training, we train this
neural network to excel under perfect information, and in
testing, we try to replicate the performance of model under
perfect information by adding Bayesian inference described
in section 5.3 and 5.4.

To train the neural network, we assume each player knows
not only his or her initial hands, but also the initial hands of
other players (see supplementary material 8.7). In the ter-
minology of computer bridge, this is called double dummy.
Then the outer expectation in equation (1) can be removed
since each player knows exactly what other players have
in their hands at any time of the game. The use of perfect
information in training has two beneﬁts, ﬁrstly the random-
ness of hidden information is eliminated thus the training
of neural network becomes more stable, secondly since
sampling hidden information is time consuming, using per-
fect information can save time. Although this treatment
may downplay the use of strategies like blufﬁng in actual
playing, the trained network performs well. Inspired by
AlphaGo Zero (Silver et al., 2018), we use Monte Carlo tree
search as a policy improvement operator to train the neural
network (see supplementary material 8.6). The loss function
is deﬁned as:

(cid:96) = DivKL (pMCTS||pnn) + λ |vMCTS − vnn| .

(3)

where pMCTS and vMCTS is the policy and value of a node
returned by Monte Carlo tree search from that node, and pnn
and vnn are the output of our neural network. The parameter
λ here weights between value loss and policy loss. Since
the value of a speciﬁc node can sometimes be as large as
several hundred while KL divergence is at most 2 or 3, this
parameter is necessary. For more details in training, see
supplementary material 8.8.

For actual testing or benchmarking, however, we must com-
ply with rules and play with honesty. On the other hand, the

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

use of Monte Carlo tree search requires each player’s hand
to allow each player perform tree search. To bridge the gap
we use stratiﬁed and importance sampling to estimate the
outer expectation of equation (1). We sample N scenarios
by our stratiﬁed sampling, then use MCTS with our policy
network as default value estimator to calculate the Q value
for each choice. Then we average these Q values with an im-
portance weight of hidden information. Finally, we choose
the cards with the highest averaged Q value to play. Details
of stratiﬁed and importance sampling will be discussed in
section 5.3 and 5.4. In ﬁgure 2, we can see that the neural
network can improve itself steadily.

Figure 2. AI trained with perfect information and tested in standard
Gongzhu game. Testing scores are calculated by WPG described
in section 5.2. Raw network means that we use MCTS for one
step, with the value network as default evaluater. Mr. Random, If
and Greed are three human experience AIs described in section
5.1. Every epoch takes less than one minute on a single Nvidia
2080Ti GPU.

5. Methodology

5.1. Human Experience Based AIs

To expand the strategy space, we build a group of human
experience based AIs using standard methods. We name
them Mr. Random, Mr. If, and Mr. Greed. Among them,
Mr. Greed is the strongest. The performance of these AIs
are shown in table 2. More details can be found in supple-
mentary material 8.2.

5.2. Evaluation System

We evaluate the performance of our AIs by letting two copies
of AI team up as partners and play against two Mr. Greeds,
and calculate their average winning points. We call this
Winning Point over Mr. Greed (WPG). In typical evaluation,
we run 1024 × 2 of games. We show that this evaluation
system is well-deﬁned in supplementary material 8.10.

R
0
-194
-275
-319

I
194
0
-80
-127

G
275
80
0
-60

SZS
319
127
60
0

R
I
G
SZS

Table 2. Combating results for 4 different AIs. In this table, R
stands for Mr. Random, I for Mr. If, G for Mr. Greed, SZS for
ScrofaZeroSimple. R, I and G are human experience AIs described
in Section 5.1 and supplementary material 8.2. SZS is ScrofaZero
without IEC algorithm described in section 5.4.

5.3. Stratiﬁed Sampling

Given a history h, initial cards on player i’s hand ci, one
should estimate what cards other players have in their hands.
As discussed before, we denote it by c−i and use c−i and
scenario interchangeably. We use C(ci) to denote the set of
all possible c−i’s.

The most natural way to calculate one’s belief about the dis-
tribution of scenarios in C(ci) is Bayesian inference (Tadelis,
2013). From Bayesian viewpoint, if player’s strategy proﬁle
is πi=0,1,2,3, which we now assume to be common knowl-
edge, player i’s belief after observing history h that initial
cards in other players’ hands are c−i is

p(c−i|h) =

p(h|c−i; π)p(c−i)
e∈C(ci) p(h|e; π)p(e)

(cid:80)

(4)

where p(h|c−i; π) is the probability that history h is gen-
erated if initial card conﬁguration is c−i, and players play
according to strategy proﬁle πi=0,1,2,3. We omit the depen-
dence on π when there is no confusion.

The belief is important because players use it to calculate
the outer expectation in equation (1)

Ec−i∼p(c−i|h) [vπ(h, ci, c−i)]

(5)

where vπ(h, ci, c−i) is the value function. The exact cal-
culation of (5) through (4) requires calculating all possible
conﬁgurations in set C(ci), which can contain 39! ∼ 1046
elements. Such size is computationally intractable, we there-
fore seek to estimate it by Monte Carlo sampling. The naive
application of Monte Carlo sampling can bring large vari-
ance to the estimation, thus we will derive a stratiﬁed im-
portance sampling approach to obtain high quality samples.

For trick-taking games, there are always cases where some
key cards are much more important than the others. These
cards are usually associated with high variance in Q value,
see section 6.2. Especially true is this for Gongzhu. We use
stratiﬁed sampling to exhaust all possible conﬁguration of
important cards.

More speciﬁcally, we ﬁrstly divide the entire C(ci) into
several mutually exclusive strata {S1, S2, ...Sp}, such that

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

C(ci) = (cid:83)t
j=1 Sj. Each stratum represents one im-
portant cards conﬁguration. To generate the partition
{S1, S2, ...St}, we identify the key cards {ck
2, ...ck
q }
in c−i based on the statistics of trained neural network,
see section 6.2, then exhaust all possible conﬁgurations
of {ck
q }. After obtaining the partition, we sample
inside each stratum. More formally, by conditional expecta-
tion rule, we can rewrite equation (5) as

2, ...ck

1, ck

1, ck

t
(cid:88)

j=1

p(Sj)Ec−i∼p(c−i|h,Sj ) [vπ(h, ci, c−i)]

(6)

where p(Sj) is the probability that c−i is in stratum Sj,
(cid:3), and p(c−i|h, Sj) is the
(cid:2)1c−i∈Sj
p(Sj) = Ec−i∼p(c−i|h)
probability distribution of c−i given the history h and the
fact that c−i is in stratum Sj. As a zero-th order approxima-
tion, we set p(Sj) = 1

t for all j.

Since the expectation in equation (6) is still analytically
intractable, we employ importance sampling to bypass the
problem. If we can obtain a sample from a simpler distri-
bution q(c−i), which has common support with p(c−i|h),
then by Radon-Nikodym theorem:

Ec−i∼p(c−i|h) [vπ(h, ci, c−i)]

= Ec−i∼q(c−i)

(cid:20)
vπ(h, ci, c−i)

(cid:21)

p(c−i|h)
q(c−i)

(7)

where we call the term p(c−i|h)
rection. If we draw N samples CN = {c(1)
from q(c−i):

q(c−i) posterior distribution cor-
−i , ...c(N )
−i }

−i , c(2)

Ec−i∼p(c−i|h) [vπ(h, ci, c−i)]

≈

1
N

N
(cid:88)

k=1

vπ(h, ci, c(k)
−i )

p(c(k)
−i |h)
q(c(k)
−i )

(8)

We take q(c−i) to be the following distribution:

(cid:40)

q(c−i) =

1/|C(c−i)|
0

if c−i is compatible with history
otherwise

(9)
i.e. q(c−i) is a uniform distribution for all c−i that is com-
patible with history. Compatible with history means under
such conﬁguration actions in history h do not violate any
rules.
Since the ratio p(c−i|h)
q(c−i)

is still intractable, we use

ˆp(c(k)

−i |h) =

p(h|c(k)
l=1 p(h|c(l)

−i )p(c(k)
−i )
−i)p(c(l)
−i)

(cid:80)N

,

ˆq(c−i) =

1
N

(10)

to approximate p(c(k)
−i ). Equation (10) change
the the scope of summation on the denominator of (4) from

−i |h) and q(c(k)

the entire population to only samples. Then equation (7)
reduces to:

Ec−i∼p(c−i|h) [vπ(h, ci, c−i)]

≈

1
k=1 s(c(k)
−i )

(cid:80)N

N
(cid:88)

l=1

vπ(h, ci, c(l)

−i)s(c(l)
−i)

(11)

where s(c(k)
−i ) = p(h|c(k)
deﬁned as s(c(k)
algorithm to calculate the score in section 5.4.

−i ) is the score we assign to scenario c(k)
−i )p(c(k)

−i , it is
−i ). We will introduce an

5.4. Integral over Equivalent Class

In this section, we will focus on how to compute s(c−i).
We assume that other players are using similar strategies
to player i. Then the policy network of ScrofaZero can be
used to estimate p(h|ci). To continue, we deﬁne correction
factor γ for a single action as

γ(a, h, cj) = e−β·regret = e−β(qmax−qa),

(12)

to be the unnormalized probability of player j taking action
a under the assumption of j using similar strategies to player
i. In deﬁnition (12), h is the history before action a, cj the
hand cards for player j, qa the policy network output for
player j’s action a and qmax the greatest value in outputs
of legal choices in cj, β a temperature controlling level of
certainty of our belief. Then the p(h|c−i) in formula (4) can
be written as

p(h|c−i) = p(hu|c−i) =

u−1
(cid:89)

t=0

p(at+1|ht, cj(t+1))

=

u−1
(cid:89)

t=0

γ(at+1, ht, cj(t+1))
α∈lc(t) γ(α, ht, cj(t+1))

(cid:80)

,

(13)

where lc(t) is legal choices at stage t. As a generalized
approach of Bayesian treatment, we estimate p(h|c−i) with
products of correction factors. We call this algorithm Inte-
gral over Equivalent Class (IEC). The pseudocode for IEC
is as algorithm 1. As an attention mechanism and to save
computation resources, some “important” history slices are
selected based on statistics in section 6 in calculating the
scenario’s score in algorithm 1, see supplementary material
8.12 for detail.

Compared with naive Bayes weighting, our IEC weighting
is insensitive to variations of number of legal choices thus
more stable. Experiments show that IEC can outperform
naive Bayes weighting by a lot, see table 3.

In rest of this section we will explain the intuition of integral
over equivalent class. We begin by introducing the concept
of irrelevant cards. Irrelevant cards should be the cards
which (i) will not change both its correction factor and other

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

Algorithm 1 Integral over Equivalent Class (IEC)

unchanged by deﬁnition, denoted by J.

Input: history hu, player i’s initial card ci, one possible
scenario c−i.
s(c−i) ← 1
for t = u − 1, u − 2, ..., 0 do

h ← ht, c ← cj(t+1), a ← at+1
if a is important then

s(c−i) ← γ(a, h, c)s(c−i)

end if
end for
Output: Score for scenario s(c−i).

Techniques
US
SS
US with IEC
SS with IEC
SS with IEC
(against US)

Performance Win(+Draw) Rate
59.6 ± 3.3
59.7 ± 3.3
54.5 ± 4.7
73.9 ± 3.3

62.2(+2.8)%
64.4(+2.0)%
-
67.7(+2.5)%

15.5 ± 4.1

53.5(+3.3)%

Table 3. Performance after different methods. US stands for Uni-
formly Sampling, SS for Stratiﬁed Sampling, IEC for Integral over
Equivalent Class. The sampling number of US is set to 9 such that
the sampling number will equal to that of SS. The last line of this
table is ScrofaZero with the strongest sampling technique, SS with
IEC, against itself without any method.

cards’ correction factors if it is moved to others’ hand, and
(ii) will not change its correction factor if other cards are
moved. The existence of approximate irrelevant cards can
be conﬁrmed both from experiences of playing games or
from the statistics in section 6. In ﬁgure 4 of section 6.1, we
see that there are some cards whose variance of values are
small. These cards are candidates approximate irrelevant
cards. See supplementary material 8.11 for an concrete
example.

We call two distributions of cards only different in irrelevant
cards equivalent. This equivalent relation divides all scenar-
ios C(c−i) into equivalent classes. We denote the equivalent
class of scenario c−i as [c−i]. We should integrate over
the whole equivalent class once we get the result of one
represent element, because the MCTS procedure for each
scenario is expensive. The weight of one equivalent class
should be

p(hu|c−i)p([c−i]) =

(cid:88)

all permutations of
irrelevant cards

u−1
(cid:89)

t=0

γ(at+1, ht, cj(t+1))
Yt+1 + Jj(t+1)

(14)
where j(t) is the player who played at stage t, Jj(t) the
sum of correction factors of irrelevant cards in j(t) and
Yt the sum of correction factors of other cards in j(t). Y
may change in different scenarios but (cid:80)3
j=1 Jj will keep

Follow the steps in supplementary material 8.14, we can
obtain the result of the summation in equation (14)

p(hu|c−i)p([c−i]) = 3N

u−1
(cid:89)

γ(at+1, ht, cj(t+1))
Yt+1 + J/3

+ O(ξ3)

t=0

(15)
where N is the number of irrelevant cards, J the sum of
correction factor of all irrelevant cards and ξ a real number
between 0 and 2/3. One can see the supplementary material
8.14 for detail.

But notice that, the denominators in the result of (15) are
insensitive to change of Y because both Y and J are always
greater than 1 (see section 6.1 for the magnitude of Y and J).
For scenarios in different equivalent classes, their Y ’s might
be different but the J is always the same. So the integral
remains approximately the same. Thus we can ignore the
denominators when calculating scenario’s score. Or in other
words, we can use the product of unnormalized correction
factors as scenario’s score. This is exactly the procedure of
IEC.

6. Empirical Analysis

6.1. Neural Network Output Statistics

To begin with, we present some basic statistics for neural
network. They include mean and variance of value and
correction factor γ of playing a card. The average correction
factor γ shown in ﬁgure 3 reﬂect cards are “good” or not:
the higher correction factor, the better to take such action.
We can ﬁnd in the ﬁgure that, SQ is not a “good” card,
one would better not play it. Another ﬁnding is that the
correction factor of four suits peak at around 10, thus when
one player choose between cards lower than 10, she should
play the largest one.

However, for Gongzhu, the value of a card highly depends
on the situation. Hence it’s important to study the vari-
ance of correction factor. For example, a SQ will bring
large negative points ends in your team but will bring large
proﬁts ends in your opponent. Variance of values shown
in ﬁgure 4 illustrates the magnitude of risk when dealing
with corresponding cards. We can see that SQ’s variance is
large, which is in line with our analysis. Meanwhile, heart
cards should be dealt differently under different situations.
Among heart suit, HK and HA are especially important.
This may be the result of ﬁnesse technique and all hearts in
Gongzhu.

These statistics from ScrofaZero reveal which cards are im-
portant. This information is used in stratiﬁed sampling in
section 5.3. Also, they are consistent with human experi-
ence.

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

positive value has the opposite meaning. We can see that
Mr. Greed and ScrofaZero agrees with each other very well.

Cards Mr. Greed

SA
SK
CA
CK
CQ
CJ
DA
DK
DQ

−50
−30
−20
−15
−10
−5
30
20
10

ScrofaZero
−20 ∼ −40
−20 ∼ −40
−20
−20
−10
−10
20
10
10

Figure 3. Average value of correction factor for different cards.
The β used here is equal to 0.015.

Table 4. The experience parameters in Mr. Greed and output of
ScrofaZero. Negative means that card is a burden, positive the op-
posite. ScrofaZero’s values are estimated under typical situations.

7. Conclusion

In this work we introduce trick-taking game Gongzhu as a
new benchmark for incomplete information game. We train
ScrofaZero, a human expert level AI capable of distilling
information and updating belief from history observations.
The training starts from tabula rasa and does not need do-
main of human knowledge. We introduce stratiﬁed sampling
and IEC to boost the performance of ScrofaZero.

Future research directions may include designing better
sampling techniques, incorporating sampling into neural
network, and applying our methods to other trick-taking
games like contract bridge. Also, we believe the knowledge
in training ScrofaZero can be transferred to other real world
applications where imperfect information plays a key role
for decision making.

Figure 4. Variance of values for different cards.

6.2. Comparison between Neural Network and Human

References

Experience

The best classical AI Mr. Greed explained in Section 5.1 has
many ﬁne-tuned parameters, including the value for each
card. For example, although SA and SK are not directly
associated with points in the rule of Gongzhu, they have
a great chance to get the most negative card SQ which
weights −100. So SA and SK are counted as −50 and
−30 points respectively in Mr. Greed. These parameters
are ﬁne-tuned by human experience. In the area of chess,
people have compared the difference for chess piece relative
value between human convention and deep neural network
AI trained from zero (Tomaˇsev et al., 2020). Here we will
conduct a similar analysis to our neural network AI and Mr.
Greed’s parameters. Table 4 shows experience parameters
in Mr. Greed for some important cards and ScrofaZero’s
output under typical situations. Negative means that card
is a risk or hazard, thus it is better to get rid of it, while

Brown, N. and Sandholm, T. Solving imperfect-information
games via discounted regret minimization. In The Thirty-
Third AAAI Conference on Artiﬁcial Intelligence, 2019.

Brown, N., , and Sandholm, T. Superhuman ai for multi-

player poker. Science, 2019.

Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowl-
ing, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samoth-
rakis, S., and Colton, S. A survey of monte carlo tree
search methods. IEEE Transactions on Computational
Intelligence and AI in Games, 4, 2012.

Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochas-
tic and nonstochastic multi-armed bandit problems. Foun-
dations and Trends in Machine Learning, 5, 2012.

Czarnecki, W. M., Gidel, G., Tracey, B., Tuyls, K., Omid-

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

Southey, F., Bowling, M. P., Larson, B., Piccione, C., Burch,
N., Billings, D., and Rayner, C. Bayes’ bluff: Opponent
modelling in poker. Proceedings of the Twenty-First Con-
ference on Uncertainty in Artiﬁcial Intelligence, 2005.

Tadelis, S. Game Theory: An Introduction. Princeton Uni-
versity press, 41 William Street, Princeton, New Jersey,
2013.

Tomaˇsev, N., Paquet, U., Hassabis, D., and Kramnik, V.
Assessing game balance with alphazero: Exploring alter-
native rule sets in chess, 2020.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,
Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,
T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Dani-
helka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P.,
Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T.,
Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine,
T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring,
R., Yogatama, D., W¨unsch, D., McKinney, K., Smith,
O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis,
D., and an David Silver, C. A. Grandmaster level in star-
craft ii using multi-agent reinforcement learning. Nature,
2019.

Whitehouse, D. Monte Carlo Tree Search for games with
Hidden Information and Uncertainty. PhD thesis, Univer-
sity of York, 7 2004.

Wikipedia. Computer bridge, 2021. URL https://en.

wikipedia.org/wiki/Computer_bridge.

Zinkevich, M., Johanson, M., Bowling, M., and Piccione,
C. Regret minimization in games with incomplete in-
formation. In 20th Conference on Neural Information
Processing Systems, 2007.

shaﬁei, S., Balduzzi, D., and Jaderberg, M. Real world
games look like spinning tops. In NeurIPS, 2020.

Doyle, A. C. The Sign of Four.

Grill, J.-B., Altch´e, F., Tang, Y., Hubert, T., Valko, M.,
Antonoglou, I., and Munos, R. Monte-carlo tree search
as regularized policy optimization. In International Con-
ference on Machine Learning, 2020.

Kuhn, H. W. Simpliﬁed two-person poker. Contributions to

the Theory of Games, 1950.

Letcher, A. On the impossibility of global convergence
in multi-loss optimization. In International Conference
on Learning Representations, 2021. URL https://
openreview.net/forum?id=NQbnPjPYaG6.

Letcher, A., Balduzzi, D., Racani‘ere, S., Martens, J., Foer-
ster, J., Tuyls, K., and Graepel, T. Differentiable game
mechanics. Journal of Machine Learning Research, 2019.

Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang,
R., Zhao, L., Qin, T., Liu, T.-Y., and Hon, H.-W.
Suphx: Mastering mahjong with deep reinforcement
learning. arXiv, 2020. URL https://arxiv.org/
abs/2003.13590.

OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V.,
Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme,
S., Hesse, C., J´ozefowicz, R., Gray, S., Olsson, C., Pa-
chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman,
J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S.,
Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2
with large scale deep reinforcement learning. 2019. URL
https://arxiv.org/abs/1912.06680.

Rong, J., Qin, T., and An, B. Competitive bridge bidding
with deep neural networks. Proceedings of the 18th Inter-
national Conference on Autonomous Agents and MultiA-
gent Systems, 2019.

Selten, R. Reexamination of the perfectness concept for
International

equilibrium points in extensive games.
journal of game theory, 1975.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I.,
Aja Huang, A. G., Hubert, T., Baker, L., Lai, M., Bolton,
A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den
Driessche, G., Graepel, T., and Hassabis, D. Mastering
the game of go without human knowledge. Nature, 2017.

Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-
pel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. A
general reinforcement learning algorithm that masters
chess, shogi, and go through self-play. Science, 2018.

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

8. Experimental Details and Extend Data

8.3. Network Architecture

8.1. Trick-taking Games

Gongzhu belongs to the class trick-taking, which is a large
set of games including bridge, Hearts, Gongzhu and Shengji.
We dedicate this section to familiarizing readers with trick-
taking games. Trick-taking games share the following com-
mon rules.

1. A standard 52-card deck is used in most cases.

2. Generally, there are four players paired in partnership,
with partners sitting opposite to each other around a
table.

3. Cards are shufﬂed and dealt to four players at the be-

ginning.

4. As the name suggests, trick-taking game consists of a
number of tricks. In a trick, four players play one card
sequentially by the following rules:

• The player leading the ﬁrst trick is chosen ran-
domly or by turns. The ﬁrst card of each trick can
be any card in that player’s hand.

• Following players should follow the suit if pos-
sible. There are no limits on the ranking of the
cards played.

• At the end of each trick, four cards played are
ranked and the player who played the card of
highest rank becomes the winner.

• The winner of the last trick leads the next trick.
• The playing order is usually clockwise.

5. The cards are usually ranked by: A K Q J 10 9 8 7 6 5

4 3 2.

8.2. Details of Human Experience Based AIs

We build a group of human experience based AIs using
standard methods. The group includes

1. Mr. Random: Random player choosing cards from

legal choices randomly.

2. Mr. If: A program with 33 if statements representing
human experience. Mr. If can outperform Mr. Random
a lot with such a few number of ifs.

3. Mr. Greed: AI with hundreds of if statements and par-
tial backward induction. It contains many handcrafted
hyper-parameters to complete the search. Mr. Greed
can outperform Mr. If, but not proportional to their
number of if statements.

The performances of these AIs are shown in table 2. More
details can be found in our repository.

We use fully connected layer with skip connection as our
model basic block. The input of our neural network is
encoded into a 434-dimensional onehot vector (this will be
explained in detail in the next subsection). The output is
a 53 dimensional vector with the ﬁrst 52 elements to be p
vector and the last element to be v.

We also tried other network topologies, including shallower
fully connection layers and ResNet. Their performance are
shown in table 5.

Network Topology
Fully Connection 16
Fully Connection 24
ResNet 18

#Parameters
9315381
11416299
11199093

Performance
0 ± 8
32 ± 5
−66 ± 8

Table 5. Performance of different networks. These scores are eval-
uated by WPG introduced in section 5.2. Sufﬁcient rounds of
games are played to make sure the variance is small enough.

Since ResNet does not show signiﬁcant improvement in
performance, we stick to fully connected neural network for
most of our experiments.

8.4. Network Inputs

The input of our neural network is a 52×4+54×3+16×4 =
434 dimension vector.

• 52 × 4 denotes the hands of 4 players. 52 because
Gongzhu uses standard 52-card deck. Cards of the
other 3 players are guessed by the methods discussed
in Section 5.3.

• 54×3 denotes the cards played in this trick. We choose
these format because at most 3 cards are played before
the speciﬁed player can play. We use 54 instead of
52 to represent the cards due to the diffuse technique
described in the next subsection.

• 16 × 4 denotes the cards associated with points that
is already played. In the game Gongzhu, there are 16
cards which have scores.

8.5. Diffuse Technique in Inputs

We use diffuse technique for representing cards in this trick
when preparing inputs. Normally we should set one spe-
ciﬁc element of onehot vector corresponding to a card to 1.
However, we want to amplify the input signal. Hence we
set not only the element in the onehot vector correspond-
ing to this card, but also the two adjacent elements, to 1.
Also we extend length for representing each card from 52
and to 54 to diffuse the element at two endpoints. Figure
5 shows how diffuse technique works. Input in this form

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

can be transformed to and from standard input with a single
fully connection layer. In experiments, diffuse technique
accelerates training.

2 × {legal choice number} searches, MCTS can only pre-
dict the future approximately after two cards are played. It
is quite surprising that the neural network can improve and
ﬁnally acquire “intuition” for long term future.

Figure 5. We use diffuse technique when representing cards in this
trick to accelerate training. Above shows the normal input, while
below shows the input after using diffuse technique.

8.6. MCTS

We used the standard MCTS using the value network to
evaluate positions in the tree. We use UCB (Upper Conﬁ-
dence Bound)(Bubeck & Cesa-Bianchi, 2012) method to
select a child node. More speciﬁcally, for each selection,
we choose node i = argmaxj vj + c(cid:112)ln N/nj, where vj is
the value of node j, nj is how many times node j has been
visited, N is the total visit round, c is the exploration con-
stant. There are two hyperparameters important in MCTS:
exploration constant c and search number TMCTS. We keep
exploration constant to c = 30. As the search number, we
set TMCTS = 2 × {legal choice number} when training and
TMCTS = 10 + 2 × {legal choice number} when evaluat-
ing. The search number is crucial in training and will be
discussed in the next subsection.

8.7. Collect Training Data under Perfect Information

The network is trained under perfect information where it
can see cards in others’ hands. Or in other words, we do
not need to sample hidden information during training. This
setting might be preferred because

1. without sampling, training is more robust;

2. by this means, more diversiﬁed middlegames and end-
ings can be reached, which helps neural network im-
prove faster by learning from different circumstances.

For example, we ﬁnd neural network trained with perfect
information masters the techniques in all hearts much more
faster than one trained with imperfect information.

After each MCTS search and play, the input and the re-
wards for different cards will be saved in buffer for train-
ing. The search number in MCTS is crucial. When it
is small, the neural network of course will not improve.
However, we ﬁnd that, when search number in MCTS is
too large, the neural network will again not improve, or
even degrade! We ﬁnd 2 × {legal choice number} the most
suitable number for MCTS searching. Notice that, with

8.8. Training

We typically let four AIs play 64 games then train for one
batch. What’s more, inspired by online learning, we will
also let neural network review data of last two batches.
So the number of data points in each batch is 64 × 52 ×
3 = 9984. Then the target function (3) is optimized by
Adam optimizer with lr = 0.001 and β = (0.3, 0.999) for
3 iterations.

There is one thing special in our dealing with loss function
which deserves some explanations. Normally, it’s natural to
mask the illegal choice in the probability output of network
(i.e. mask them after the softmax layer). However, we mask
the output before softmax layer. Or in coding language,
we use softmax(p × legal mask) rather than legal mask ×
softmax(p). We ﬁnd this procedure much better than the
other one. A possible explanation is that, if we multiply the
mask before softmax, the information of illegal choice is
still preserved in the loss, which can help the layer before
output to ﬁnd a more reasonable feature representation.

8.9. Arena

We provide an arena for different AIs to combat. We deﬁne
standard protocols for AIs and arena to commute. We set
up an server welcoming AIs from all around the world.
Every AI obeying our protocol can combat with our AIs
and download detailed statistics. We also provide data and
standard utility functions for others training usage. More
details can be found in our GitHub repository.

8.10. Details for Evaluation System

In section 5.2, we introduced Winning Point over Mr. Greed
(WPG). For this evaluation system to be well-deﬁned, WPG
should be transitive, or at least approximately transitive,
i.e. program with higher WPG should play better. In this
section, We will introduce a statistics ε that measures the
intransitivity of an evaluation function, then show that WPG
is nearly transitive by numerical experiments.

We ﬁrst deﬁne ξij to be the average winning score of play-
ing strategy πi against strategy πj. Then let us start by
considering two extreme cases. A game is transitive on Π,
if

ξij + ξjk = ξik ∀πi, πj, πk ∈ Π,

(16)

where Π is a subspace of the strategy space. As a
famous example for intransitive game, 2-player rock-
paper-scissor game is not transitive for strategy tuple
(Always play rock, Always play paper, Always play scissor).

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

In the middle of the totally transitive and intransitive games,
we want to build a function εΠ to describe the transitivity
of policy tuple Π = (π1, π2, ...πn). To better character-
ize the intransitivity, the function ε should have following
properties:

a) take value inside [0, 1], and equal 0 when evaluation
system is totally transitive, 1 when totally intransitive;

b) be invariant under translation and inﬂation of scores;

c) be invariant under reindexing of πi’s in Π;

d) take use of combating results of every triple strategies
n = n(n − 1)(n − 2)/6

(πi, πj, πk) in Π. There are C 3
such triples.

e) will not degenerate (i.e. approach to 0 or 1) under

inﬁnitely duplication of any πi ∈ Π;

f) be stable under adding similar level strategies into Π.

We deﬁne ε to be

(cid:80)
i<j<k

|ξij + ξjk − ξik|2

|ξij + ξjk − ξik| (|ξij| + |ξjk| + |ξik|)

.

εΠ =

(cid:80)
i<j<k

(17)
When there are only three strategies i, j and k, this deﬁni-
tion of (cid:15) reduces to the form of

εijk =

|ξij + ξjk − ξik|
|ξij| + |ξjk| + |ξik|

.

(18)

Equation (17) can be seen as average of εijk with weights
|ξij + ξjk − ξik| (|ξij| + |ξjk| + |ξik|). If one strategy πi0 ∈
Π is duplicated inﬁnity times, deﬁnition (17) will approach

εΠ with inﬁnite πi0
(cid:80)
j<k

=
|ξi0j + ξjk − ξi0k|2

(cid:80)
j<k

|ξi0j + ξjk − ξi0k| (|ξi0j| + |ξjk| + |ξi0k|)

.

(19)

In other words, the |ξij + ξjk − ξik| factor in weights guar-
antees property (e). The (|ξij| + |ξjk| + |ξik|) factor guar-
antees property (f), because, by deﬁnition, “similar level
strategies” means (|ξij| + |ξjk| + |ξik|) is small. One can
easily verify that this ε obeys other properties in the list
above.

Using the deﬁnition in equation (17), we calculate the tran-
sitivity of the strategy tuple of 4 different AIs as shown in
Table. 2.

ε{R,I,G,ZTS} = 0.03 ± 0.02
(20)
The 0.02 in equation above is the standard deviation comes
from evaluation of two agents. The statistics in (20) is not
signiﬁcantly from 0. Therefore, playing with Mr.Greed
and calculating average winning score is a good choice for
evaluating the strength of AI.

8.11. Examples for Irrelevant Cards

Table 6 shows an example for irrelevant cards. Roughly
speaking, irrelevant cards are those cards (i) that don’t result
in large change in both their own correction factors and
other cards’ correction factors if they are switched to other
player, and (ii) whose correction factors don’t change much
if other cards are switched. Correction factor is deﬁned in
equation (12). We can see from table 6 that, with or without
D4, the correction factors of other cards are almost the same.
Also different combinations of other cards (e.g. “D7, D8”,
“D6, D7, DJ” or “D10, DK”) will not affect the correction
factor of D4.

Cards Guessed
D7
D8
D4
D6
D7
DJ
D4
D10
DK
D4

γ Without D4
1.0000
0.9897
-
0.9010
1.0000
0.7398
-
1.0000
0.9538
-

γ With D4
1.0000
0.9912
0.9188
0.9043
1.0000
0.7321
0.9233
1.0000
0.9548
0.9519

Table 6. An example for irrelevant card. Here D4 is the irrelevant
card. γ is correction factor of corresponding cards.

From ﬁgure 4, we know that irrelevant cards are most likely
appear in spade. Table 7 is an example for irrelevant card SJ.
Apart from SJ, other small cards, S2 to S7 are also highly
likely to be irrelevant cards in this situation.

Cards Guessed
S5
S10
SJ
S8
S10
SQ
SJ
S7
SK
SJ

γ Without SJ
0.9424
1.0000
-
0.9560
1.0000
0.4818
-
1.0000
0.8302
-

γ With SJ
0.9464
1.0000
0.9598
0.9585
1.0000
0.4950
0.9528
1.0000
0.8331
0.9677

Table 7. An example for irrelevant card. Here SJ is the irrelevant
card. γ is correction factor of corresponding cards.

8.12. Select Important History in IEC

Like what we have discussed in section 5.4, as an atten-
tion mechanism and to save computation resources, some
“important” history slices are selected based on statistics in
section 6 in IEC algorithm. From ﬁgure 3 and 4, we can see

ScrofaZero: Mastering Trick-taking Poker Game Gongzhu by Deep Reinforcement Learning

that, the cards smaller than 8 always have small correction
factors in its suit and lower value variance. This means
that the history slices with card played smaller than 8 are
highly likely to be unimportant. So we only select history
slices with the card played greater than 7 in IEC algorithm.
Another selection rule is that, when a player is following
the suit, history slices of other suits are also not important.

8.13. Average over unknown Information in IEC

In section 5.4, we introduced IEC algorithm. However, our
network’s input requests other’s hands. We should not give
hands of this scenario directly to neural network, because
that player cannot know the exact correct hands of other
players. As a work around, we average the hands informa-
tion and give it to neural network, shown in ﬁgure 6. In
other words, we replace the “onehot” in input representing
other’s hands with “triple-1/3-hot”.

should keep unchanged in different scenarios for this deci-
sion node, denoted by J.

J1 + J2 + J3 = Const (cid:44) J.

(22)

The distribution of Jj is polynomial. In most situations,
there are always many (5 or more) irrelevant cards. By
central limit theorem, a polynomial distribution can be ap-
proximated by a multivariate normal distribution. Since
J3 = J − J1 − J2, we can derive the marginal distribution
of J1 and J2. We adopt this approximation and replace
the summation over all permutations of irrelevant cards in
equation (14) by an integral

p(hu|c−i) ≈

3N
2π|Σ|
(cid:18)

exp

−

(cid:90) (cid:90) x1+x2<J/3

x1,2>−J/3

(x1, x2)Σ−1(x1, x2)T

(cid:19)

1
2

u−1
(cid:89)

t=0

γ(at+1, ht, cj(t+1))
Yt+1 + J/3 + xj(t+1)

dx1dx2

(23)

Figure 6. The network input representing other’s hands is averaged
in IEC.

= 3N

u−1
(cid:89)

=

3N J 2
4π|Σ|

γ(at+1, ht, cj(t+1))
Yt+1 + J/3

+ O(ξ3)

t=0
γ(at+1, ht, cj(t+1))
Yt+1 + J/3

u−1
(cid:89)

t=0

+ O(ξ3)

where N is the number of irrelevant cards, J the sum of
correction factor of all irrelevant cards, xj = Jj − J/3 and
ξ a real number between 0 and 2/3. The last equality is
because Σ satisﬁes

(cid:90) (cid:90) x1+x2< J

3

x1,2> −J
3

1
2π|Σ|
=1 − O(ξ3).

e− 1

2 (x1,x2)Σ−1(x1,x2)T

dx1dx2

The result of equation (23) is equation (15) in main text.

(24)

The replacement from “one” to “triple-1/3”is not standard.
Table 8 shows that the performance will not deteriorate
under this nonstandard input.

Input method
Standard input (“onehot”)
Averaged Input (“triple-1/3-hot”)

WPG
1 ± 16
7 ± 4.6

Table 8. Raw network performance under standard input and av-
eraged input. Raw network means that we directly use policy
network in playing rather than MCTS.

8.14. Proof of Equation (15)

Once irrelevant cards are deﬁned, c−i is divided in to three
parts: cards already played, relevant cards and approxi-
mately irrelevant cards. Form now on, we will refer ap-
proximately irrelevant cards as irrelevant cards. The Jj in
equation (14) is the sum of all of irrelevant cards’ correc-
tion factors in player j, while the Yt in equation (14) is the
sum of the other cards’ correction factors of the player who
played stage t.

(cid:88)

γ(ck) (cid:44) Jj ,

(cid:88)

γ(ck) (cid:44) Yt

ck∈irrelevant cards
in player j

ck∈cards played
∪ relevant cards

(21)

Notice that by deﬁnition of irrelevant cards, (cid:80)3

j=1 Jj

