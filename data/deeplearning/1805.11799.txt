8
1
0
2

y
a
M
0
3

]
I

A
.
s
c
[

1
v
9
9
7
1
1
.
5
0
8
1
:
v
i
X
r
a

Automated proof synthesis for propositional logic with deep neural networks

TARO SEKIYAMA, National Institute of Informatics, Japan
KOHEI SUENAGA, Kyoto University, Japan and JST PRESTO, Japan

This work explores the application of deep learning, a machine learning technique that uses deep neural networks (DNN) in its core, to
an automated theorem proving (ATP) problem. To this end, we construct a statistical model which quantifies the likelihood that a proof

is indeed a correct one of a given proposition. Based on this model, we give a proof-synthesis procedure that searches for a proof in

the order of the likelihood. This procedure uses an estimator of the likelihood of an inference rule being applied at each step of a proof.
As an implementation of the estimator, we propose a proposition-to-proof architecture, which is a DNN tailored to the automated
proof synthesis problem. To empirically demonstrate its usefulness, we apply our model to synthesize proofs of propositional logic.

We train the proposition-to-proof model using a training dataset of proposition–proof pairs. The evaluation against a benchmark set

shows the very high accuracy and an improvement to the recent work of neural proof synthesis.

Additional Key Words and Phrases: Deep Learning, Deep Neural Networks, Automatic Theorem Proving

1 INTRODUCTION

Theorem proving is an essential activity in formal reasoning. Needless to say, mathematics has become the reliable
foundation of modern natural science, including several branches of theoretical computer science, by justifying theorems

with proofs. The importance of correct proofs leads to the study of software called proof assistants [Nipkow et al. 2002;

Norell 2009; The Coq Development Team 2017], which allow users to state theorems and their proofs formally in the

form of certain programming languages and automatically check that the proofs correctly prove the theorems. The

realm of the areas that rely on theorem proving is expanding beyond mathematics; for example, it is being applied

for system verification [Klein et al. 2009; Leroy 2009], where one states the correctness of a system as a theorem and

justifies it in the form of proofs.

Automated theorem proving (ATP) [Bibel 2013; Fitting 2012; Pfenning 2004] is a set of techniques that prove logical
formulas automatically. We are concerned with the following form of ATP called automated proof synthesis (APS): Given
a logical formula P, if P holds, return a proof M of P. In the light of the importance of theorem proving, APS serves
as a useful tool for activities based on formal reasoning. For example, from the perspective of the aforementioned

system verification, APS serves for automating system verification; indeed, various methods for (semi)automated static

program verification [Barnett et al. 2005; Chalin et al. 2007; Filliâtre and Paskevich 2013] can be seen as APS procedures.

We also remark another important application of APS: automated program synthesis. An APS algorithm can be seen as

an automated program synthesis procedure via the Curry–Howard isomorphism [Sørensen and Urzyczyn 2006], in
which M can be seen as a program and P can be seen as a specification. Not only is APS interesting from the practical
viewpoint, it is also interesting from the theoretical perspective in that it investigates the algorithmic aspect of theorem

proving.

Traditionally, the main weapon from the programming-language community to tackle APS has been symbolic
methods; an APS algorithm inspects the syntactic structure of the formula P and, using the obtained information, tries

Authors’ addresses: Taro Sekiyama, National Institute of Informatics, Japan, sekiyama@nii.ac.jp; Kohei Suenaga, Kyoto University, Japan, ksuenaga@kuis.
kyoto-u.ac.jp, JST PRESTO, Japan, ksuenaga@kuis.kyoto-u.ac.jp.

2018.

1

 
 
 
 
 
 
2

Taro Sekiyama and Kohei Suenaga

to construct a proof derivation of P. A seminal work in this regard is by Ben-Yelles [1979]; they proposed a sound and
complete APS algorithm for an implicational fragment of the propositional logic.

This paper tackles the APS problem using another emerging technology: statistical machine learning. In particular,
we explore an application of deep neural networks (DNN) [Goodfellow et al. 2016]. DNNs have seen a great success in
recent years for solving various tasks; to name a few, image recognition [He et al. 2016], speech recognition [Hinton

et al. 2012], and natural language processing [Bahdanau et al. 2014; Cho et al. 2014; Wu et al. 2016]. To this end, we
propose a novel DNN architecture named proposition-to-proof model1 tailored to the APS problem.

Concretely, we statistically model the APS problem in terms of probabilities. This statistical model serves for
quantifying how a partially constructed proof is likely to lead to a correct proof of the given proposition P. Based on
this statistical model, we define a proof-synthesis procedure that searches for a proof of given proposition P in the
order of the likelihood. This proof synthesis procedure requires a function to estimate the likelihood of an inference

rule being applied at a specific step of a proof (or, equivalently, a specific position of a partially constructed proof). For

this estimation, we use a DNN based on the proposition-to-proof architecture that we propose. We empirically evaluate

the performance of our network, which reveals that it can predict the inference rules that fill the rest of a partially
constructed proof of a propositional-logic formula with 96.79% accuracy.

This work is not the first one that applies DNNs to APS. Among them, Sekiyama et al. [2017] reports an application

of DNN equipped with long-short term memory (LSTM) to the APS problem. The idea in their work is to view the APS

problem as a machine translation problem from the language of logical formulas to the language of proofs. Based on

this view, they applied an off-the-shelf neural machine translation framework to APS. They report that their network,

which is a vanilla one for neural machine translation, proved around 50% of the propositional-logic formulas in the

benchmark they used. In contrast to their approach of trying to synthesize an entire proof at once, we designed our
proof-synthesis procedure so that it gradually constructs a proof based on the likelihood of each inference rule.

The contributions of this work are summarized as follows.

• We construct a statistical model for the APS problem in terms of probability. This model formally quantifies the
likelihood of a proof being a correct one of a given proposition. Applying the laws of probability to this model,

we derive how the computation of the likelihood of an entire proof is reduced to the successive computations of

the likelihood of inference rules.

• Based on this statistical model, we design a proof-synthesis procedure that searches for a proof of a given
proposition in the descending order of the likelihood. This algorithm gradually constructs a proof by repeating

the estimation of the likelihood of occurrences of inference rules in the proof.

• We propose a novel DNN architecture which we call proposition-to-proof model that estimates the above likelihood
of inference rules. This network takes a proposition P, the position in a partially constructed proof to be filled,
and contextual information as input and outputs the likelihood of inference rules being applied at the position to

be filled.

• We implemented the proof-synthesis procedure with a trained proposition-to-proof model and empirically
confirmed its effectiveness compared to Sekiyama et al. [2017]. In addition to measuring the accuracy of the

trained proposition-to-proof model, we conducted in-depth analyses of the model. We confirmed that our model
estimates the proof constructor with 96.79% accuracy.

1In the community of neural network research, there is a habit to call a trained DNN “model”. Following this convention, we abuse the word “model” for
a trained DNN.

3

Types
P, Q, R

::= a | P → Q | P × Q | P + Q

Terms
L, M, N ::= [ ] | x | λx.M | M N | (M, N ) | case M of (x, y) → N |

Left M | Right M | case L of { Left x → M; Right y → N }

Typing contexts

Γ

::= ∅ | Γ, x:P

Fig. 1. Syntax.

Currently, we do not claim that our procedure outperforms the state-of-the-art APS method for propositional logic.

Rather, our contribution consists in the statistical reformulation of the APS problem and application of deep learning,

which exposes superhuman performance in many areas. We believe that deep learning is also useful in the APS problem

possibly in combination with symbolic methods and that the present work opens up a new research direction in this

regard.

The rest of this paper is organized as follows: Section 2 defines the logic and the proof system that we use in this

paper; Section 3 reviews statistical machine learning briefly; Section 4 defines the proof-synthesis algorithm; Section 5

gives a brief introduction to deep learning and introduces the proposition-to-proof architecture; Section 6 describes the

result of the experiments; Section 7 discusses related work; and Section 8 concludes.

We assume the readers’ familiarity to the Curry–Howard isomorphism [Sørensen and Urzyczyn 2006]. We sometimes

abuse the terminologies in the simply typed lambda-calculus for those of the proof theory of the propositional logic. We

also assume that the readers are familiar with the probability theory although we do not deal with measure-theoretic

discussions in the present paper.

2 THE SIMPLY TYPED LAMBDA CALCULUS AS PROPOSITIONAL LOGIC

In this work, we identify the simply typed lambda calculus with the intuitionistic propositional logic via the Curry–

Howard isomorphism [Sørensen and Urzyczyn 2006]. This view is indeed beneficial for us: (1) a term of the simply

typed lambda calculus is the concise representation of a derivation tree, which is essentially the proof of a proposition
and (2) we can express a partially constructed proof as a term with holes, which denote positions in a proof that needs
to be filled. In the rest of this section, we introduce the simply typed lambda calculus extended with product types and

sum types. The Curry–Howard isomorphism allows us to identify a product type with the conjunction of propositions

and a sum type with the disjunction.

Figure 1 shows the syntax of the simply typed lambda calculus. Types (or propositions) are represented by the
metavariables P, Q, and R; terms (or proofs) are represented by the metavariables L, M, and N ; and typing contexts (or
collections of assumptions) are represented by the metavariable Γ. The definition of types is standard: they consist of
type variables (or propositional variables), function types P → Q, product types P × Q, and sum types P + Q. We use the
metavariables a, b, c, and d for type variables. The syntax of terms is that of the simply typed lambda calculus. Products
are constructed by (M, N ) and destructed by case M of (x, y) → M; sums are constructed by Left M and Right N and
destructed by case L of { Left x → M; Right y → N }. The term syntax is equipped with a hole [ ] to express partially
constructed terms. A hole denotes a position in a term that needs to be filled (see below). We use metavariables x, y,
and z for term variables.

4

Γ ⊢ M : P

Taro Sekiyama and Kohei Suenaga

Γ ⊢ [ ] : P

Hole

Γ, x:P ⊢ M : Q
Γ ⊢ λx.M : P → Q
Γ ⊢ N : Q

Γ ⊢ M : P

Γ ⊢ (M, N ) : P × Q
Γ ⊢ M : P
Γ ⊢ Left M : P + Q

Abs

Pair

Left

Var

x:P ∈ Γ
Γ ⊢ x : P
Γ ⊢ M : P → Q Γ ⊢ N : P
Γ ⊢ M N : Q

App

Γ ⊢ M : P × Q Γ, x:P, y:Q ⊢ N : R
Γ ⊢ case M of (x, y) → N : R

CasePair

Γ ⊢ M : Q
Γ ⊢ Right M : P + Q

Right

Γ ⊢ L : P + Q Γ, x:P ⊢ M : R Γ, y:Q ⊢ N : R
Γ ⊢ case L of { Left x → M; Right y → N } : R

CaseSum

Fig. 2. Inference rules.

The notions of free variables, bound variables, and substitution for terms are defined as usual. λx.M binds x in
M; case M of (x, y) → N binds x and y in N ; and case L of { Left x → M; Right y → N } binds x in M and y in N ,
respectively. Types have no binders. We write FV (M) for the set of term variables that occur freely in M. We write
[N /x] M for the capture-avoiding substitution of N for x in M. We say two terms are α-equivalent if they are different
only in the use of bound variable names. We identify two α-equivalent terms.

A term that contains holes represents a partially constructed proof. Our proof-synthesis procedure introduced in

Section 4 maintains a set of partially constructed terms and fills a hole inside a term in the set at each step. We assume
that holes in a term are uniquely identified by natural numbers. We write [ ]i
for a hole with number i. We write M[N ]i
for the term obtained by filling the hole [ ]i

in M with N .

We also define the typing relation Γ ⊢ M : P as the least relation that satisfies the inference rules in Figure 2. This
relation means that term M is typed at P under Γ or, equivalently, M is a proof of P under assumptions Γ. We write
Γ ⊬ M : P to denote that Γ ⊢ M : P does not hold. The rules in Figure 2 are standard except for the rule Hole for holes.
This rule allows any type to be given to a hole. We call the inference rules except for the rule Hole proof inference rules.
We say that M is a (complete) proof of P if ∅ ⊢ M : P is derived and M has no holes. M is said to be partial or partially
constructed if ∅ ⊢ M : P but M contains holes.

To define the notion of normal forms, we introduce β-reduction (−→β ) and η-reduction (−→η ), which are the least
compatible relations satisfying the rules in Figure 3; the last η-reduction rule for sums is given by Ghani [1995]. Term
M is a βη normal form when there does not exist N such that M −→β N nor M −→η N .

3 BACKGROUND: STATISTICAL MACHINE LEARNING

In order to make the present paper self-contained, we explain basic concepts on statistical machine learning and

probabilities that appear in this paper. The exposition about machine learning in this section is not intended to be

exhaustive; for detail, see the standard textbooks, e.g., Bishop [2006].

Machine learning is a generic term for a set of techniques to make software “learn” how to behave from data without
being explicitly programmed. The machine-learning task in this paper is of a type called supervised learning. In this

5

M −→β N

β-reduction

(λx.M) N −→β
case (L, M) of (x, y) → N −→β
case (Left L) of { Left x → M; Right y → N } −→β
case (Right L) of { Left x → M; Right y → N } −→β

[N /x] M

[L/x, M/y] N

[L/x] M

[L/y] N

M −→η N

η-reduction

(case L of (x, y) → x, case L of (x, y) → y) −→η
case M of { Left x → [Left x/z] N ; Right y → [Right y/z] N } −→η [M/z] N
(x, y (cid:60) FV (N ))

(λx.M x) −→η M (x (cid:60) FV (M))
L

Fig. 3. Reduction rules.

type of tasks, a learner needs to synthesize a function f : X → Y for certain sets X and Y .2 In a typical setting, the
learner is given a set D := {(x1, y1), . . . , (xn, yn )} of sample input–output pairs of f as a hint for this learning task; the
set D is called a training dataset.

A popular strategy to tackle this problem is to use statistics. In order to explain application of statistics in machine
learning, we fix notation about probabilities. We designate a random variable, say Bx , that evaluates to an element of
Y following a certain probabilistic distribution parameterized by an element x ∈ X ; we use the bold face for random
variables in this paper. For this random variable, one can consider, for example, the probability p(Bx = y) of Bx being
evaluated to y. More generally, given a predicate φ(x, Bx ) over x and Bx , one can define the probability p(φ) that φ
holds for x and a value of Bx .3 Notice that the truth value of the predicate φ is a random variable; it may hold or may
not hold depending on the result of the evaluation of Bx in general. We can also define the probability p(φ1 | φ2) for
given two predicates φ1 and φ2, which is the probability of φ1 holding under the condition that φ2 is true. A probability
distribution of a random variable Bx conditioned on φ, written p(Bx | φ), is a function that maps an element y ∈ Y to
the probability p(Bx = y | φ).

We view the statistical machine learning in the following way. Ideally, it is desirable to discover the “true” probability
distribution, especially probability distribution dx behind the random variable Bx , that explains how the training
dataset D is generated. If we have this distribution, then we can guess a highly probable output y to x as one that
y ∈Y dx (y)). However, it is actually hard to identify dx precisely. An alternative
maximizes the value of dx (i.e., arg max
promising way is to approximate dx by a parameterized function Fx,w1, ...,wn
over parameters w1, . . . , wn that maps an
element of Y to the (approximated) probability of Bx being evaluated to y. The parameters are tuned using numerical
optimization so that the probability of D being generated is maximized. Then, the function f is synthesized so that
f (x) = arg max

y ∈Y Fx,w1, ...,wn (y).

The performance of f synthesized as above depends on several factors, including: (1) whether the set {Fx,w1, ...,wn |
w1, . . . , wn are possible parameters} contains a function that is sufficiently “close” to the true distribution dx and (2)

2We only consider the case where X and Y are discrete in this paper.
3Strictly speaking, we need to define the structure of measurable sets on Y and argue that {y ∈ Y | φ(x, y)} is measurable on this structure to formally
define p(φ). We do not discuss such measure-theoretic issues in this paper.

6

Taro Sekiyama and Kohei Suenaga

x (cid:55)→ Fx,w1, ...,wn
a promising methodology to address these issues.

is not overfitted to D and performs well for unobserved data. Deep learning, as seen in Section 5.1, is

4 AUTOMATED PROOF SYNTHESIS WITH STATISTICAL MODELING

This section starts with making a statistical model for the APS problem by rephrasing the concepts introduced in

Section 3 using the terms of our setting. Based on this model, we define a proof-search procedure that takes the
likelihood of a proof term into account. We show that we can decompose the probability distribution p(MP |P) for APS
to the multiplication of easier-to-approximate fine-grained probability distributions in terms of term constructors that

occur in proof terms. We show the derivation of these fine-grained distributions and then proceed to the definition of

the procedure of proof synthesis.

4.1 Automated proof synthesis, statistically
In our setting, the training dataset D is a set of proposition–proof pairs {(P ′
proof of P ′
i
propositions P as possible, that is, our f has to satisfy ∅ ⊢ f (P) : P for many propositions P.

is a complete
for each i. We are to synthesize a function f that takes a proposition P and returns its proof M for as many

n)}, where M ′
i

1), . . . , (P ′

n, M ′

1, M ′

To statistically model the APS problem, we designate several random variables. P is a random variable that evaluates
to a type.4
MP is a family of random variables indexed by type P. Then, for a predicate φ(P) on P, the probability
distribution p(MP | φ) is a distribution on the set of proof terms conditioned by the predicate φ(P). As we outlined in
Section 3, if we have a good approximation of the conditional probability distribution p(MP | P = P), then we obtain
M p(MP = M | P = P). Therefore, a function that maps P to
a highly probable proof term of P by computing arg max
arg max

M p(MP = M | P = P) is expected to be a good proof synthesizer under this model.

4.2 Derivation of fine-grained distributions
We could directly learn p(MP | P = P) using a certain machine leaning technique along with the training dataset D;
this is the strategy taken by Sekiyama et al. [2017]. However, we found that such monolithic approximation of the

probabilistic distribution often leads to a bad approximation; indeed, the accuracy of the automated proof synthesizer by
Sekiyama et al. was around 50% at best. In this paper, we instead convert p(MP | P = P) using the laws of probabilities so
that the learning task is reduced to a set of fine-grained ones. Under this strategy, one can compute an approximation of

the probability distribution by combining several easier-to-approximate distributions. We discover that the combination
leads to a better proof synthesizer than learning M monolithically.

In order to derive the fine-grained distributions, we first introduce several notions that enable us to specify occurrences

of term constructors in proofs.

Definition 4.1 (One-depth contexts). The set of one-depth contexts is defined by the following BNF:

C ∈ Ctx

::= λx. [ ] | [ ] [ ] | ([ ] , [ ]) | case [ ] of (x, y) → [ ] |

Left [ ] | Right [ ] | case [ ] of { Left x → [ ] ; Right y → [ ]}.

We assume that each hole in a one-depth context is equipped with a unique identifier. We write C[Mi]i for the term
obtained by filling holes [ ]0 , ..., [ ]n

in C with terms M0, ..., Mn, respectively.

4 We could actually build a statistical model that does not treat P as a random variable as long as we focus on the contents of this paper. However, in order
to easily extend our framework to one that consider nontrivial probabilistic distributions over propositions in future, we model P as a random variable.

Definition 4.2 (Paths). A path ρ is a finite sequence of pairs (C, i) where i is a natural number that identifies a hole in

C. We write ⟨ρ, (C, i)⟩ for the path obtained by postpending (C, i) to path ρ.

A one-depth context represents a term constructor other than variables. Using one-depth contexts, ρ = ⟨(C0, i0), (C1, i1), . . . , (Cn, in)⟩

specifies a path in a term, whose top-level constructor is identical to C0, from its root node in the following way: C0;
the hole in C0 with the identifier i0; C1; the hole in C1 with the identifier i1; and so on. For example, let M be a term
λx.case x of (y, z) → (z, y). Then, a path from the root of M to the reference to variable y is represented by the path

7

⟨(λx. [ ]0 , 0), (case [ ]0 of (y, z) → [ ]1 , 1), (([ ]0 , [ ]1), 1)⟩.

We show that the probability p(MP = M | P = P) is equal to ϕ(M, ⟨⟩) if every subterm M ′ of M is annotated with its

type (which we write typeof(M ′)), where ϕ is defined by induction on the structure of M:

ϕ (x, ρ) = p(x = x | P = P, Q = typeof(x), ρ = ρ)
ϕ (C[Mi]i, ρ) = p(C = C | P = P, Q = typeof(C[Mi]i), ρ = ρ) ×

(1)

(cid:206)

i ϕ (Mi, ⟨ρ, (C, i)⟩).

The function ϕ (M, ρ) computes p(M = M |P = P, Q = typeof(M), ρ = ρ), the probability of M being a subterm of
typeof(M) at the position specified by ρ within a proof of P, by induction on the structure of M using two auxiliary
probabilities: p(x = x | P = P, Q = Q, ρ = ρ) and p(C = C | P = P, Q = Q, ρ = ρ). In the definition, we use the
following random variables: x evaluates to a term variable; C evaluates to a one-depth context; Q evaluates to the
type to be proved by M; and ρ evaluates to a path that specifies the position where x or C is placed. Note that P
evaluates to the type that is supposed to be proved by the root node, not by M. The conditional-probability expression
p(x = x | P = P, Q = Q, ρ = ρ) quantifies the probability of x being a proof of Q under the condition that it appears at
the position specified by ρ; and p(C = C | P = P, Q = Q, ρ = ρ) is the probability of C being the top-level constructor of
a proof term of Q if it appears at the position specified by ρ. We will explain how to model type annotations typeof(M)
later.

If M is a variable x, then ϕ uses the value of the former probability as the answer. If M is not a variable, then it can
be written in the form of C[Mi]i using some one-depth context C and terms M1, . . . , Mn . The definition argues that this
probability is the multiplication of (1) the likelihood of C conditioned by P = P, Q = typeof(M), and ρ = ρ; and (2) the
probabilities of ϕ(Mi , ⟨(C, i), ρ⟩). Notice that the information of Mj does not appear in the probability calculation of Mk
if j (cid:44) k; this greatly simplifies the definition of ϕ.

We informally show that ϕ(M, ρ) is indeed equal to p(M = M | P = P, Q = typeof(M), ρ = ρ) by induction on the
structure of M.5 The case of M = x is easy. Consider the case of M = C[Mi]i. We start from p(M = C[Mi]i | P = P, Q =
typeof (M), ρ = ρ). This probability is equal to the following probability:

(cid:16)
C = C, M1 = M1, . . . , Mn = Mn

p

(cid:12)
(cid:12) P = P, Q = typeof (M), ρ = ρ
(cid:12)

(cid:17)

.

Here, each Mi is the random variable that evaluates to the term to be filled in the i-th hole in C. By using the chain rule
of conditional probabilities [Koller and Friedman 2009] this probability is equal to the following.

p (C = C | P = P, Q = typeof(M), ρ = ρ) ×
p (M1 = M1, . . . , Mn = Mn | C = C, P = P, Q = typeof(M), ρ = ρ) .

(2)

5In order to formally prove this fact, we need to define the random variables and the probability distributions in this paper in more formal style, which
we decide to defer to future work.

8

Taro Sekiyama and Kohei Suenaga

We decompose the second expression in Equation 2. Let us first refine the condition of this expression with Qi

, a family

of the random variables that evaluate to the type of the i-th hole in C, and ρi
−→
Mi ) for Q1 = typeof(M1), ..., Qn
to the paths of the i-th hole in C. For short, let us write
and −→
Equation 2 is equal to

, a family of random variables that evaluate
= typeof(Mn )
= ⟨ρ, (C, n)⟩. Then, with these random variables, the second expression in

ρi for ρ1 = ⟨ρ, (C, 1)⟩, ..., ρn

ρi = −→

= typeof(

−→
Q i

(cid:16)

p

−→
Q i

= typeof(

M1 = M1, . . . , Mn = Mn

(cid:12)
(cid:12)C = C, P = P, Q = typeof(M), ρ = ρ,
(cid:12)
−→
Mi ) does not affect the distributions of M1, . . . , Mn since the type of every
ρi does not affect the distributions either since the
are uniquely determined by C and ρ. Then, an important observation here is that under the condition that
−→
−→
ρ i are known, each of the random variables M1, . . . , Mn is independent of the rest; in other words,
Q i and
ρ i . Hence, the above

because (1) the condition
subexpression is known by assumption and (2) the condition −→
ρ i
values of −→
ρ i
the values of
each of the random variables M1, . . . , Mn is conditionally independent of the rest under
probability is equal to

−→
Q i and −→

= typeof(

−→
Mi ),

= −→
ρi

−→
Q i

= −→

−→
ρ i

(cid:17)

(cid:214)

1≤j ≤n

(cid:16)

p

M j = Mj

(cid:12)
(cid:12)C = C, P = P, Q = typeof(M), ρ = ρ,
(cid:12)

−→
Q i

= typeof(

−→
Mi ),

−→
ρ i

(cid:17)

= −→
ρi

.

In the condition part of this expression, noting that (1) the values of C, Q, and ρ are irrelevant since we have the values
, the above probability is equal to
of

; and (2) the distribution of M j depends only on the values of ρ j

and −→
ρ i

−→
Q i

(cid:214)

1≤j ≤n

(cid:16)

p

M j = Mj

(cid:12)
(cid:12)P = P, Q j
(cid:12)

= typeof(Mj ), ρ j

and Q j
(cid:17)

= ρj

.

By the induction hypothesis, this is equal to (cid:206)1≤j ≤n ϕ(Mj , ⟨ρ, (C, j)⟩); substituting this to Equation 2, we have Equa-
tion 1.

We back up the aforementioned observation that is a key to decompose the second expression by an example. Let C
be ([ ]0 , [ ]1) under a path that binds f to a → b, д to a × b → a, x to a, and y to b. Suppose we know that [ ]0 should be
filled with a term of type a → b and [ ]1 with a term of type a. Obviously, in our context of proof synthesis, the fact that
f is filled in [ ]0 does not add any restriction on the set of possible terms in [ ]1, since any term of the type of each hole
works as a proof of each type. This observation can be generalized to an arbitrary case in our type system.6

4.3 Proof synthesis procedure

Based on the discussion in Section 4.2, we design a proof-synthesis procedure. Procedure 1 shows the definition of our
procedure ProofSynthesize, which takes proposition P to be proved. This procedure maintains a priority queue Q of
partially constructed terms. The priority associated with M by Q denotes the likelihood of M forming a proof of P. In
each iteration of Lines 4–15, ProofSynthesize picks a term M with the highest likelihood and fills a hole in M with a
one-depth context. It returns a proof if it encounters a correct proof of P. We write p∗(φ1 | φ2) for an approximation of
p(φ1 | φ2).

Before going into the detail, we remark a gap between the procedure ProofSynthesize and the statistical model in
Section 4.2. In that statistical model, we defined the likelihood of a variable p(x | P = P, Q = Q, ρ = ρ) and that of a
one-depth context p(C | P = P, Q = Q, ρ = ρ) as separate probability distributions. Although this separation admits the

6We also expect this observation to be generalized to various type systems. This observation essentially comes from the fact that the interfacing by a type
separates certain dependency between a context and a term, which is often true for many type systems.

Procedure 1 Proof synthesis

9

1: procedure ProofSynthesize(P)
2:

Initialize priority queue Q that contains partial proofs constructed so far.
Push [ ] to Q with priority 1.0.
while Q is not empty do

Pop M with the highest priority P from Q.
Let ρ = arg max
for each Cx ∈ Ctx ∪ BV (M, ρ) such that ∅ ⊢ M[Cx ]ρ : P do

ρ ∈hole (M) maxr p∗(r = r | ρ = ρ).

if hole (M[Cx ]ρ ) = ∅ then

return M[Cx ]ρ

else

Let Q be a proof obligation to be discharged at [ ]ρ
Push M[Cx ]ρ to Q with priority p∗(r = rCx | P = P, ρ = ρ, Q = Q) P

.

end if

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

end for
end while

15:
16: end procedure

inductive definition of the function ϕ, it is not necessarily plausible from the viewpoint of proof synthesis since, in
filling a hole, we do not know whether it should be filled with a variable or with a one-depth context.

In order to solve this problem, we assume that we have an approximation of the likelihood of an proof inference
rule that should be applied to a hole. Concretely, we assume that we can approximate the probability distribution
p(r | P, ρ, Q), where r is a random variable that evaluates to the name of an proof inference rule in Figure 2. This
assumption requires that we estimate the likelihood of Var being applied for a hole, which can be done in the same

way as estimation of those of other inference rules.

Let us explain the inside of the procedure in more detail. A proof is synthesized by the while loop, where the
pointed by ρ in the partial proof M that has the highest likelihood P (Lines 4–15). We
procedure fills the hole [ ]ρ
write hole (M) for the set of paths to holes in M. We select path ρ such that the inference rule applied at the position
pointed by the path has the highest probability. After finding the hole to be filled, we replace it with Cx , which denotes
one-depth contexts or variables. BV (M, ρ) is the set of bound variables that can be referred to at [ ]ρ
and M[Cx ]ρ is the
in M with Cx . Note that Ctx are the set of all one-depth contexts. If M[Cx ]ρ is a proof of P,
term obtained by filling [ ]ρ
which can be checked using an off-the-shelf type checker, then the procedure returns it as the synthesis result (Line 9).
Otherwise, M[Cx ]ρ is added to Q with priority p∗(r = rCx | P = P, ρ = ρ, Q = Q) P, which is the likelihood of M[Cx ]ρ
forming a proof (Line 12). rCx
; how to
find it is discussed in Section 6.4.2.

is the proof inference rule corresponding to Cx . Q is a proof obligation at [ ]ρ

We make a few remarks about the procedure:

• In the current implementation, we have not implemented the approximator of p(x | P, ρ, Q); instead, if the
procedure decides to fill a hole with a variable, we assume that p(x | P, ρ, Q) is the uniformly distribution on the
set of variables that are available at this scope. Although this may look like a naive strategy, our implementation

still works quite well for many propositions; see Section 6.4.2. The problem of estimating the likelihood of a
variable is similar to the premise selection problem, for which various work has been done [Irving et al. 2016;
Kaliszyk et al. 2017; Loos et al. 2017; Wang et al. 2017]. Combining our synthesizer with such a technique is an

interesting future direction.

10

Taro Sekiyama and Kohei Suenaga

• In the current implementation, we assume that the type checking conducted in Line 7 infers the type of each
subexpression of M[Cx ]ρ and annotates these types to them; this is indeed how we handle the typeof (M) in
Section 4.2. This is a reasonable assumption as far as we are concerned with the propositional logic. For more

expressive logics, we may need some auxiliary methods to guess the type of each expression.

• The procedure ProofSynthesize is not an algorithm. If it is fed with an unsatisfiable proposition, then it does
not terminate. Even if it is fed with a valid proposition, it may not be able to discover a proof of the proposition
depending on the performance of the estimator of p∗.

5 NEURAL PROPOSITION-TO-PROOF MODEL

In order to implement ProofSynthesize, we are to approximate the probability distribution p(r | P, ρ, Q) that produces
the likelihood of a proof inference rule being applied at a given position in a proof. To this end, we design a new DNN
model, which we call a proposition-to-proof model, tailored to the classification task of inference rules. We start with a
brief review of deep learning for making this paper self-contained; see, e.g., Goodfellow et al. [2016] for the details.

Then we describe a basic architecture of the proposition-to-proof model.

5.1 Deep learning, briefly

Deep learning is a generic term for machine learning methods based on deep neural networks. Usually, as other machine
learning technologies, the first common step of deep learning is to build a vector space of features and a mapping from
a datum to the space of the features. An element of the vector space of the features is called a feature vector. It is a
multidimensional vector each dimension of which is a numerical value that represents certain information of the datum.

By embedding data to a vector space and working on this space, we can apply various numerical optimization techniques

of machine learning such as support vector machine and random forest. The design of the feature representation is

known to have a great influence on the performance of a machine learning method. However, developing feature

representations, known as feature-engineering, requires deep expertise in the application domains; it is in general

difficult to generalize a design of a feature representation to other tasks.

Deep learning allows us to avoid hard feature-engineering. In contrast with other methods, deep learning can

learn the feature representation from data without manual engineering, which makes it possible to find a good

feature representation with less effort. Deep learning is also very expressive in the sense that it can approximate any
continuous function f with a given degree of accuracy by appropriately tuning the set of parameters [Hornik et al.
1989]. Furthermore, it is known that deep learning tends to generalize to unseen data without overfitting to a given
training dataset if the set is sufficiently large.7

Models in deep learning are represented by (artificial) neural networks, which consist of an input layer that converts
data to a feature vector; an output layer that converts a feature vector to a human-readable format; and one or more
hidden layers that learn feature representations. Neural networks with multiple hidden layers are especially called
deep neural networks (DNNs). The basic building block of hidden layers is a perceptron, which is a function over
multidimensional vectors with learnable parameters. Given an n-dimensional vector vx = [x1, ..., xn ] in the vector

7The reason why a deep learning model tends not to overfit is still not fully understood; see Neyshabur et al. [2017] for recent remarkable development
on this issue.

11

space Rn on reals, a perceptron produces an m-dimensional vector vy = [y1, ..., ym ] ∈ Rm such that:

yj =

n
(cid:213)

i=1

Wj,i xi + bj

where Wj,i ∈ R, a matrix called a weight, is a learnable coefficient parameter; and bj ∈ R, a vector called a bias, is a
learnable parameter independent of the input. We simply write the behavior of a perceptron as the following linear

operation:

vy = W vx + b
where W is a real matrix in Rm×n , b is a real vector in Rm , W vx is the matrix product of W and the transpose of vx , and
+ is the element-wise addition. Since a perceptron is a linear function, any multilayer perceptron is also represented

by a single perceptron since the composition of several linear maps is also linear. To give DNN models the ability to
approximate any nonlinear function, each hidden layer postpends the application of a nonlinear function f , called
activation, and produces the result vz of an element-wise application of vy to f :

vz = f (vy).

fc for clarification.
A hidden layer of this type is called a fully connected layer. We write W and b in it as W
Hidden layers are supposed to extract abstract features of data and a use of multiple layers makes DNNs powerful.

fc and b

However, training a model with an excessive number of layers requires expensive learning cost. This is one of the

reasons why many variants of DNNs have been studied for effective learning; this work can be seen as a new DNN

model tailored to proof synthesis.

As other machine learning methods, deep learning requires training to tune the learnable parameters. The parameters

are tuned so that a model approximates the distribution of a given dataset as closely as possible using numerical

optimization techniques such as stochastic gradient descent [Robbins and Monro 1951]. These numerical optimization

methods adjust learnable parameters so that the difference between an expected output and the actual response from
the model is minimized; this difference is called a loss value and a function to calculate loss values is called a loss function.
Splitting a training dataset into multiple small collections called mini-batches is a popular strategy in the training of a

DNN. In this strategy, the numerical optimization is applied to each mini-batch to update the learnable parameters.
After the training, one evaluates the performance of the trained model by using a validation dataset, which is different
but supposed to origin from the same distribution as the training dataset.

5.2 Proposition-to-proof model

We design a DNN model that takes three arguments, proposition P to be proven, path ρ pointing to the hole to be
filled, and proof obligation Q a term of which should be placed in the hole, and approximates the probability of a proof
inference rule being applied at the position specified by ρ in a proof of P. Following the standard manner in deep
learning, the model represents features of the three arguments as real vectors and then approximates the likelihood of
each proof inference rule with them. We first explain how we learn feature representations of propositions P and Q.
The features of the path are obtained by extracting those of P along the path. We finally integrate all features into a
single feature vector and use it to estimate a proof inference rule that should be applied.

5.2.1 Proposition encoder. To obtain informative features from proposition P, we consider an abstract syntax
tree (AST) representation of P. Each node of the AST is equipped with a proposition constructor (→, ×, or +) or a

12

Taro Sekiyama and Kohei Suenaga

Fig. 4. Proposition representations.

propositional variable. We first give a simple feature to each node in the AST and then design a new layer to learn an

effective feature representation of the AST. In what follows, we suppose that each node in an AST is associated with a

feature vector.

One possible way to provide vectors that distinguish nodes of an AST is to use one-hot vectors, which are used
broadly in natural language processing and represent a word as an n-dimensional vector (n is the number of unique
words considered) that only the element corresponding to the word has scalar value 1 and the others have 0. In this

work, the information of proposition constructor is embedded into a vector as in one-hot vectors, while propositional

variables embed their numerical scale values into a fixed element in the vector.

Definition 5.1 (Vector representation of proposition node). Let f be a bijective function that maps propositional variables

to positive numbers. Then, Enc gives a vector to node t as follows.

= [f (a), 0, 0, 0]

Enc (a)
Enc (→) = [0, 1, 0, 0]
= [0, 0, 1, 0]
Enc (×)
= [0, 0, 0, 1]
Enc (+)

Figure 4 illustrates feature vectors given by Enc, which are similar to one-hot vectors in that each dimension of them
represents a class of a node. We consider that all propositional variables belong to the same class, so Enc assigns their
numerical values to the same dimension. On the other hand, different propositional variables should be distinguished;
e.g., if a (cid:44) b, proofs generated for a → b → a and a → b → b should be different. Thus, Enc assigns different numbers
to different propositional variables.

We expect that this encoding of nodes is more informative, especially, for propositional variables that do not occur

in a training dataset—we call such variables unknown—than one-hot vectors. How to handle unknown entities is a

common issue also in natural language processing, which addresses the issue by a workaround that maps all unknown

words to a special symbol “unknown”. However, this workaround has the problems that (1) the feature vector for the

“unknown” is not related to propositional variables at the training phase since the “unknown” does not occur in the

training dataset and (2) all unknown propositional variables are mapped to a single symbol “unknown” and so they

𝑎	×	𝑏→𝑐→𝑏	×	𝑎	→𝑐→×→𝑎𝑏𝑐→𝑐×𝑎𝑏𝟚𝟙𝟙𝟚𝟙𝟙𝟚𝟚𝟙𝟛𝟛𝕟= [n, 0, 0, 0]𝟙= [0, 1, 0, 0]𝟚= [0, 0, 1, 0]SynaxASTrepresentationFeature vectors in AST13

Fig. 5. Encoder.

are not distinguished. Fortunately, we know as a domain knowledge that “unknown” comes from only propositional

variables and we can assign unique positive numbers to all propositional variables, which should make feature vectors
of unknown propositional variables more informative. We expect the encoding by Enc to be helpful for the issue of
unknown propositional variables.

After giving a vector to each node by Enc, we obtain features of P by two steps (Figure 5). The first step gains a
feature representation of each node from nodes around it by using AST convolution layers. The second step aggregates
feature vectors of nodes into a single vector by an aggregation layer.

AST convolution layer. An AST convolution layer updates a feature vector of each node t in an AST by using vectors
of nodes around t. Suppose that parent (t) is the parent and child (t, i) is the i-th child of t. Let vt be an n-dimensional
feature vector of t. Then, the AST convolution layer updates all vectors of nodes in a given AST simultaneously as
follows. Let ςt be a class of node t, that is, a proposition constructor (→, ×, or +) or a class to denote propositional
variables.

conv

vt ← F

W

conv
conv
ςt,i vchild (t,i) + W
ςt

vt + W

conv
ςt,p vparent (t) + b

conv
ςt

(3)

(cid:33)

(cid:32)

(cid:213)

i

conv

conv
ςt,i ∈ Rm×n is a weight parameter which is a coefficient of the feature vector of the i-th child, W
ςt
conv
ςt,p ∈ Rm×n is for the parent, b

∈ Rm×n
where W
conv is an activation function. Each
is for t, W
parameter is shared between nodes with the same ςt . If t is the root node, then vparent (t) denotes the zero vector. We
use multiple AST convolution layers to learn features of a node.

∈ Rm is a bias parameter for ςt , and F

conv
ςt

The update (3) is inspired by tree-based convolution proposed by Mou et al. [2016], but there are a few differences.

First, our update rule involves the feature vector of a parent node to capture features of the context where a node is

used, whereas Mou et al. do not. Second, Mou et al. regard an AST as a binary tree, which is possible, e.g., by left-child
right-sibling binary trees,8 which makes it possible to fix the number of weight parameters for children to be only two.
This view is useful when one deals with ASTs where a node may have an arbitrary number of children. However, a

different tree representation may affect a feature representation learned by DNNs—especially, it may not preserve the

locality of the original AST representation. Thus, instead of binary trees, we deal with ASTs as they are. Fortunately,

the syntax of propositions is defined rigorously and the number of children of each node is fixed. Hence, we can fix the

number of learnable weight parameters for children: two for each proposition constructor.

Aggregation layer. An aggregation layer integrates features of nodes in an AST to a single vector.

8They did not clarify what binary tree is considered, though.

𝟚𝟙𝟙𝟚𝟙𝟙𝟚𝟚𝟙𝟛𝟛Tree convolutionv4v2v3v6v1v8v9v10v11v5v7Tree convolution…v4v2v3v6v1v8v9v10v11v5v7‘‘‘‘‘‘‘‘‘‘‘AggregationvP14

Taro Sekiyama and Kohei Suenaga

Definition 5.2 (Aggregation layer). Let t be a node of an AST where nodes are augmented with n-dimensional vectors.

Function Agg (t) produces an n-dimensional vector from t as follows:

Agg (t) = F

(cid:32)

(cid:213)

agg

i

agg

W

ςt,i Agg (child (t, i)) + W

agg
ςt vt + b

(cid:33)

agg
ςt

agg

agg
where W
ςt
agg
b
ςt

and W

∈ Rn is a bias for ςt , and F

ςt,i ∈ Rn×n are weight parameters which are coefficients of vectors of t and its i-th child, respectively,
agg is an activation function. Each parameter is shared between nodes with the same ςt .

Another way to produce a single feature vector from an AST is a max-pool [Mou et al. 2016], which, for each

dimension, takes the maximum scalar value among all feature vectors in the AST. While max-pools are used by usual

convolutional neural networks [Krizhevsky et al. 2012], it is not clear that gathering only maximum values captures

features of the whole of the AST. By contrast, an aggregation layer can be considered as “fold” on trees with feature
vectors, and we expect that Agg (t) learns a feature representation of the AST because it takes not only maximum values
but also the other elements of feature vectors of all nodes into account.

In what follows, we write vP for the feature vector of P that is achieved by applying Enc, multiple AST convolution

layers, and an aggregation layer sequentially.

5.2.2 Path encoder. To achieve good performance, we have to know what assumptions are available at the position
for which an inference rule is estimated. For example, if a variable of type a → b can be referred to, we expect the
variable to be useful to prove b. We can access to information of assumptions via proposition P and path ρ. The
proposition-to-proof model thus extracts features of assumptions from the feature vector vP of P along the given ρ.

Definition 5.3 (Extraction). Extract (ρ, v) extracts features in the position to which ρ points from v.

Extract (⟨⟩, v)
Extract (⟨(C, i), ρ⟩, v) = Extract (ρ, v′) where v′ = F

= W

ext v + b

ext

ext (cid:16)

W

ext
C,i v + b

(cid:17)

ext
C,i

ext, W

∈ Rn×n and b
where W
the addition of (C, i) to path ρ at the beginning.

ext, b

ext
C,i

ext
C,i

∈ Rn are learnable parameters and F

ext is an activation function. ⟨(C, i), ρ⟩ is

Figure 6 illustrates the process of computing Extract (ρ, v), where features are extracted along the path. We write
vP, ρ for Extract (ρ, vP ). The weight parameters in Definition 5.3 have a role of extracting features necessary to capture
assumptions from vP . The biases are expected to capture information of the context around the node to which the path
points.

5.2.3 Classification. We estimate what proof inference rule is most likely to be applied by using two feature vectors
vP, ρ , the extracted features from P along ρ, and vQ, the features of proof obligation Q. For that, as usual, we concatenate
vP, ρ and vQ and apply multiple fully connected layers to the concatenation result so that the number of dimensions
of the final output vo is equal to that of proof inference rules, that is, eight. Using vo, we approximate the likelihood
of a proof inference rule r being applied by softmax. For vector v ∈ Rn , we write v[i] for the real number of the i-th
dimension of v. Let nr ∈ {1, ..., 8} be an index corresponding to proof inference rule r in vo. Then, the approximation
probability p∗(r = r | P = P, ρ = ρ, Q = Q) is calculated by:

exp(vo[nr ])
j=1 exp(vo[j])

(cid:205)8

15

Fig. 6. A running example of extraction.

6 EXPERIMENTS

This section reports the performance of our proposition-to-proof model and the proof synthesis procedure with it. We

train the model on a dataset that contains pairs of a proposition and its proof by supervised learning. After explaining

the detailed architecture of our model (Section 6.1), we detail creation of the dataset (Section 6.2). We evaluate the

trained model on the basis of accuracy, that is, we check, given a proposition, a partially constructed proof, and a hole

from a validation dataset, how accurately the model estimates the inference rule to be applied at the hole; we also

conduct an in-depth analysis of the model to confirm how influential depths of hole positions are on the accuracy

(Section 6.4.1). Finally, we evaluate the proof synthesis procedure given in Section 4.3 (Section 6.4.2).

We implemented the procedure ProofSynthesize and our model on Python 3 (version 3.6.3) with the deep learning

framework Chainer [Tokui et al. 2015] (version 2.1.0). We use the Haskell interpreter GHCi (version 8.0.1) for a type

checker in ProofSynthesize. All experiments are conducted on a machine equipped with 12 CPU cores (Intel i7-6850K

3.60GHz), 32 GB RAM, and NVIDIA GPUs (Quadro P6000).

6.1 Network configuration

Figure 7 shows the architecture of our proposition-to-proof model in the experiments. We use three AST convolution
layers to encode proposition P to be proven and one for proof obligation Q. The concatenation result of vP, ρ from
Extract and vQ from Agg2 is fed to three fully connected layers. The detailed specification of each layer is shown in
Table 1. We use a rectified linear unit (ReLU) [Glorot et al. 2011] as activation functions throughout the architecture.

6.2 Dataset

The power of deep learning rests on datasets used to train DNN models. In this work, we need a dataset of pairs of a
proposition and its proof. We make dataset Dall by generating small proofs exhaustively and large proofs at random.
Small proofs are generated by Procedure 2, which produces a set D of pairs of a proposition and its proof the size
of which is equal to or less than s. Procedure 2 generates proofs by filling the leftmost holes [ ]i
in terms M of queue
S. If proposition P of a generated proof M is already included jointly with N in D, we choose the proof the size of

vvv1vv1v2vv1v2v3vv1v2v3vP,ρthe node pointed by the path16

Taro Sekiyama and Kohei Suenaga

Fig. 7. The architecture of our DNN model. P is a proposition to be proven, ρ is a path specifying the hole to be filled, and Q is a
proof obligation to be discharged at the hole.

Layer

Learnable parameters

Number of dimensions of output vectors

AST conv1

AST conv2

AST conv3

Agg1

Extract

AST conv4

Agg2

FC1
FC2
FC3

,W

conv

ς,p ∈ R200×4

,W

conv

ς,p ∈ R500×200

,W

conv

ς,p ∈ R1000×500

conv
W
ς
conv
ς

b

conv
W
ς
conv
ς

b

conv
W
ς
conv
ς

b

conv
,W
ς,i
∈ R200
conv
,W
ς,i
∈ R500
conv
,W
ς,i
∈ R1000
agg

b

,W

agg
,W
W
ς
ext, W
ext
W
C,i
ext
ext, b
∈ R1000
C,i
conv
conv
,W
W
ς
ς,i
conv
∈ R16
ς
agg

agg
W
ς
fc ∈ R1016×1016
fc ∈ R1016×1016
fc ∈ R8×1016

,W

W

W

b

W

ς,i ∈ R1000×1000
∈ R1000×1000

agg
ς

b

∈ R1000

conv

ς,p ∈ R16×4

ς,i ∈ R16×16

agg
∈ R16
b
ς
fc ∈ R1016
fc ∈ R1016
fc ∈ R8

b

b

b

200

500

1000

1000

1000

16

16

1016

1016

8

Table 1. Learnable parameters and the number of dimensions of vectors in the output for each layer. ς is a class of a node in a
proposition AST.

which is smaller (Lines 10–11) in order to decrease the number of estimations performed by ProofSynthesize—the
error by approximation becomes larger as more estimations are performed. We call P principal when, for any Q such
that ∅ ⊢ M : Q, there exists some map from propositional variables to propositions such that f (P) = Q [Milner
1978]; well-typed terms in the simply typed lambda calculus have principal types. Following Sekiyama et al. [2017],
we have constructed only βη normal forms. The dataset Dall that we use in this work includes a dataset produced by
SmallProofGen(9).

ASTconv1ASTconv2ASTconv3Agg1ExtractASTconv4Agg2PQρEncEncFC1FC2FC3+concatsoftmax17

Procedure 2 Small proof generation
1: procedure SmallProofGen(s)
2:

Initialize D with the empty set and S with the empty queue
Push [ ] to S
while S is not empty do

Pop M from S
Let [ ]i
for each C ∈ Cnstr (M, [ ]i) such that M[C]i is a βη normal form,

be the leftmost hole in M

size (M[C]i) ≤ s, and ∅ ⊢ M[C]i

: P for principal P do

if M[C]i has a hole then
Push M[C]i to S

else if D contains (P, N ) such that size (M) < size (N ) then

D ← (D \{(P, N ))} ∪ {(P, M[C]i)}

else if D does not contain (P, N ) for any N then

D ← D ∪ {(P, M[C]i)}

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

end if

end for
end while
return D

17:
18: end procedure

Size

Number of proofs

1–10
11–20
21–30
31–40
41–50
1–50

136877
14885
7910
5848
2224
167744

Table 2. The number of proofs per size in Dall

Generating large proofs is not so easy due to the huge space to be searched. We generate a large proof efficiently, as
follows. Suppose that a lower bound l and an upper bound u of the size of a proof generated are given and let M be a βη
normal proof partially constructed so far. We start with M = [ ]. We gradually fill holes in M with term constructors
chosen randomly and keep M to be the partial proof produced last. If M becomes a complete proof with a smaller size
than l, we restart the proof generation from the beginning with M = [ ]. If the size of M becomes larger than l, we
preferentially choose variables as term constructors substituted for holes to finish the proof generation as soon as
possible. If M becomes a complete proof with size s such that l ≤ s ≤ u, we produce M as the result. If the size of M
becomes large than u, we restart the proof generation. This approach may appear rather ad-hoc, but we could generate
many large proofs by it. For (l, u) ∈ {(10, 30), (20, 40), (30, 50)}, we generate 15000, 15000, and 10000 proofs, respectively.
The dataset Dall contains proofs shown in Table 2. Since our DNN model feeds a proposition P, a path ρ, and a proof
obligation Q to be discharged at the hole specified by ρ and estimates an inference rule r that should be applied at the
hole, we make quadruples (P, Q, ρ, r) from Dall and split them into training dataset Dt and validation dataset Dv. Dt
contains 90% of quadruples generated from Dall (1731998 quadruples) and Dv does the remaining 10% (193108 ones).
Table 3 shows the number of training data in Dt for each inference rule.

18

Taro Sekiyama and Kohei Suenaga

Inference rule Number of training data

Var
Abs
App
Pair
CasePair
Left
Right
CaseEither

453655
473338
29621
172268
27272
269613
269480
36750

Table 3. The number of training data for each inference rule.

Depth

#

All

Var

Abs

App

Pair

CasePair

Left

Right CaseEither

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16–20
21–26
1–26

16774
18427
21932
25250
27262
26107
20719
13466
7616
4639
3478
2460
1735
1221
790
1158
74
193108

100.0
99.32
98.34
97.72
96.92
96.63
96.68
95.27
92.57
90.54
90.80
89.59
88.59
90.17
90.13
90.50
90.54
96.79

N/A
N/A
98.38
97.46
97.36
98.53
98.76
98.45
97.90
96.62
96.88
96.51
96.26
98.08
98.00
97.91
97.50
98.03

100.0
99.77
99.61
99.50
99.30
99.10
98.82
98.35
96.16
97.19
97.04
97.30
95.50
97.06
97.89
95.75
85.71
99.27

N/A
94.17
92.54
89.13
90.95
85.87
80.46
35.48
30.51
37.66
20.97
21.67
23.91
12.00
29.41
19.05
0.00
78.21

100.0
99.69
99.62
99.52
98.82
98.63
97.39
95.33
92.46
91.11
93.60
91.45
83.15
89.39
97.62
91.11
100.0
98.50

N/A
89.31
76.09
72.97
62.87
53.42
50.52
37.05
34.10
24.80
26.00
15.39
16.67
21.88
6.67
8.33
0.00
57.25

100.0
99.61
99.21
98.81
97.50
97.12
97.93
96.59
93.88
93.18
92.57
91.41
94.24
93.97
87.14
95.05
75.00
98.05

100.0
99.76
98.97
98.78
97.75
96.25
97.31
96.58
95.86
92.44
93.70
93.07
95.35
89.92
88.75
86.79
100.0
97.95

N/A
84.68
79.19
83.41
82.08
70.96
48.11
38.05
33.00
32.87
29.41
26.92
16.98
17.14
12.00
3.70
N/A
67.34

Table 4. Validation accuracy of the trained model for each inference rule per depth. The column “#” shows the number of validation
data and “All” does the accuracy for all inference rules. “N/A” means that there are no validation data.

6.3 Training

We train the proposition-to-proof model with the architecture given in Section 6.1 on dataset Dt by stochastic gradient
descent with a mini-batch size of 1000 for 20 epochs.9 Weights in each layer of the model are initialized by the values

independently drawn from the Gaussian distribution with mean 0 and standard deviation

where n is the number of
dimensions of vectors in the input to the layer. The biases are initialized with 0. We use the softmax cross entropy as
the loss function. As an optimizer, we use Adam [Kingma and Ba 2014] with parameters α = 0.001, β1 = 0.9, β2 = 0.999,
and ϵ = 10−8. We lower α, which controls the learning rate, by 10 times when the training converges. We regularize
our model by a weight decay with penalty rate λ = 0.0001.

(cid:113) 1
n

6.4 Evaluation

6.4.1 Accuracy. Table 4 shows the accuracy of the trained model on the validation dataset Dv. The bottom row in
the table reports the summarized accuracy and presents that the trained model achieves total accuracy 96.79%. Looking

9Epoch is the unit that means how many times the dataset is scanned during the training.

19

at results per inference rule, we achieve the very high accuracy for Var, Abs, Pair, Left, and Right. It is interesting

that the train model chooses either of Left or Right appropriately according to problem instances. It means that, given
proposition P + Q, the proof synthesis procedure with this trained model can select whichever of P and Q should be
proven with high probability. The accuracy for App, CasePair, and CaseEither is not so bad, but the estimation of

these rules is more difficult than that of other rules. This may be due to the training dataset. As shown in Table 3,

the numbers of training data for App, CasePair, and CaseEither are much smaller than those of other rules. Since

the model is trained so that inference rules that often occur in the training dataset are more likely to be estimated

in order to minimize the loss, the trained model may prefer to choose inference rules other than App, CasePair, and

CaseEither. Furthermore, it may be possible that the training data for those rules are insufficient to learn feature

representation of the likelihood of them being applied. In either case, data augmentation would be useful, though we

need to establish effective augmentation of proofs.

Our model is supposed to access the assumptions via the P and ρ. Since ρ becomes larger as the position of the hole
does deeper, the depth of the hole is expected to affect the performance of the model. We thus investigate the accuracy

of the trained model for each depth of holes in the validation data, which is shown in Table 4. Seeing the column “All”,

we can find that the accuracy at a greater depth tends to be lower. The accuracy of Abs, Pair, Left, and Right is still

high even if holes are at deep positions. We consider that this is because, rather than assumptions, proof obligations

play an important role to choose those inference rules. By contrast, the accuracy of App, CasePair, and CaseEither is

not high, especially, when holes are at very deep positions. Since these rules need information about assumptions to

judge whether they should be applied, their accuracy may be improved by representing features of assumptions better.

The accuracy of Var is very high at any depth, though whether we can apply Var should depend on assumptions.

This may be due to the large number of training data for Var (Table 3), which may make it possible to learn feature

representation of assumptions only for Var.

Finally, we confirm the power of explicit use of proof obligations. To this end, we train a model that does not use the
feature vector of a proof obligation; we call such a model obligation-free. The architecture of the obligation-free model is
the same as Figure 7 except that it does not refer to the feature vector of proof obligation Q. We train the obligation-free
model in the same way as Section 6.3. The validation result of the trained obligation-free model is shown in Table 5.

Compared with Table 4, the accuracy of the obligation-free model is lower than that of the proposition-to-proof model

for all inference rules, especially, at great depth. The use of proof obligations thus improves the performance of the

DNN model.

6.4.2 Proof synthesis. This section evaluates ProofSynthesize (Procedure 1) with the trained proposition-to-proof
model. We make two test datasets for evaluation by choosing 500 propositions from Dv respectively. One dataset Dsmall
consists of propositions that are generated by SmallProofGen(9), that is, the sizes of their proofs can be equal to or
lower than 9. The other dataset Dlarge includes propositions that are generated at random so that the sizes of their
proofs are larger than 9. We abort the proof synthesis if a proof is not generated within three minutes. We use the

principal proposition for a proof obligation that is required by ProofSynthesize.

We compare our procedure with an existing method of APS with deep learning by Sekiyama et al. [2017]. They

view proof generation as a translation task from a proposition language to a proof language and apply a so-called

sequence-to-sequence model [Sutskever et al. 2014], a popular DNN model in machine translation, in order to produce

a token sequence expected to be a proof from a token sequence of a proposition. They find that, though the response

from the DNN model may not be a proof of the proposition, the response is often “close” to a correct proof and, based

20

Taro Sekiyama and Kohei Suenaga

Depth

#

All

Var

Abs

App

Pair

CasePair

Left

Right CaseEither

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16–20
21–26
1–26

16774
18427
21932
25250
27262
26107
20719
13466
7616
4639
3478
2460
1735
1221
790
1158
74
193108

99.99
99.07
97.24
95.17
92.19
89.12
85.18
78.15
66.19
56.33
56.67
57.24
57.06
59.46
58.48
61.66
64.86
88.52

N/A
N/A
97.73
96.28
96.05
96.71
97.18
95.93
93.62
87.81
88.19
89.43
86.18
86.38
85.71
86.50
85.00
94.06

100.0
99.68
98.92
97.49
95.15
91.77
86.90
77.79
66.67
67.95
69.03
73.28
72.97
75.98
72.54
74.06
85.71
94.06

N/A
94.17
88.06
87.11
85.56
77.81
64.52
13.71
7.63
7.79
8.07
6.67
4.35
4.00
5.88
9.52
0.00
69.40

100.0
99.15
97.99
96.20
91.99
82.93
60.79
28.31
15.28
10.28
9.20
7.24
9.95
10.61
4.76
11.11
0.00
83.90

N/A
86.16
76.09
63.29
53.22
39.21
26.12
13.84
4.05
6.40
1.00
3.85
10.42
3.13
0.00
8.33
0.00
44.42

99.97
99.36
98.12
96.33
91.54
86.47
76.89
59.69
28.79
19.89
14.59
11.34
9.95
10.35
5.71
3.96
12.5
85.69

99.97
99.55
96.93
95.21
89.97
83.08
74.80
55.87
20.86
20.48
13.85
12.04
13.95
7.56
4.76
5.66
11.11
84.37

Table 5. Validation accuracy of the trained obligation-free model.

N/A
85.96
82.14
81.61
76.63
59.35
18.21
9.74
5.08
0.70
2.35
2.56
1.89
5.71
8.00
0.00
N/A
57.41

ProofSynthesize

Sekiyama et al. [2017]

Dsmall

Dlarge

Number of successes
Average time in success

Number of successes
Average time in success

500
0.45

466
4.56

500
1.85

157
29.03

Table 6. The evaluation result of the proof synthesis procedures: number of propositions that succeed in generation of proofs and
average of elapsed times of the generation.

on this observation, propose a proof synthesis procedure that uses the response from the DNN model as a guide of
proof search. We train the sequence-to-sequence model on Dt for 200 epochs in the same way as Sekiyama et al. and
apply their proof synthesis procedure to propositions in Dsmall and Dlarge.

Table 6 shows the number of propositions that succeed in generation of proofs by each procedure and the average of

elapsed times taken by the procedure when proofs are generated successfully (the unit is second). Both procedures
succeed in generating proofs for all propositions in Dsmall, which indicates that they work well, at least, for propositions
that have small proofs. As for Dlarge, ProofSynthesize successfully generates proofs for 93.2% of propositions in
Dlarge, while the procedure of Sekiyama et al. does for only 31.4%. Since ProofSynthesize calculates the likelihood of
a proof being a correct one by the joint probability of inference rules in the proof, we can generate a correct proof even

in a case that the likelihoods of a few instances of inference rules in the correct proof are estimated to be low, if the

likelihoods of other instances are to be high. By contrast, the procedure of Sekiyama et al. uses only a single term as a

guide, so it is hard to recover the mistake of the estimation by the DNN model. This would also lead to a difference of

elapsed times taken by two proof synthesis procedures—the procedure of Sekiyama et al. takes four times and six times
as long as ProofSynthesize for propositions in Dsmall and Dlarge, respectively.

21

7 RELATED WORK

7.1 Automated theorem proving with deep learning

Application of deep learning to ATP is becoming in trend recently. Roughly speaking, there have been two research
directions for ATP with deep learning: enhancing existing solvers with deep learning and implementing ATP procedures
using deep learning. We discuss these two lines of work in the following.

7.1.1 Enhancing existing provers. Existing automated theorem provers rely on many heuristics. Applying deep
learning to improve these hand-crafted heuristics, aiming at enhancing them, is an interesting direction. Premise
selection, a task to select premises needed to prove a given conjecture, is an important heuristic to narrow the search
space of proofs. Irving et al. [2016] show the possibility of the application of deep learning to this area using various

DNN models to encode premises and a conjecture to be proven in first-order logic. Kaliszyk et al. [2017] make a dataset

in the HOL Light theorem prover [Harrison 2009] for several tasks, including premise selection, related to ATP. Wang

et al. [2017] tackles the premise selection problem in higher-order logic. Their key idea is to regard logical formulas as

graphs by connecting a propositional variable to its binder, while the other work such as Irving et al. [2016] and Kaliszyk

et al. [2017] deals with them as token sequences. This idea allows a DNN model to utilize structural information of

formulas and be invariant to names of bound variables.

Loos et al. [2017] apply several off-the-shelf DNN architectures to guide clause selection of a saturation-based first-
order logic prover E [Schulz 2013]. Given a conjecture to be proven, E generates a set of clauses from logical formulas

including the negated conjecture and investigates whether a contradiction is derivable by processing the clauses one

by one; if a contradiction is found, the conjecture holds; otherwise, it does not. If E processes clauses that derive a

contradiction early, the proof search finishes in a small number of search steps. Hence, clause selection is an important

task in saturation-based theorem provers including E. Loos et al. use DNNs to rank clauses that are not processed yet

and succeed in accelerating the proof search by combining the DNN-guided clause selection with existing heuristics.

This direction of enhancing the existing provers is orthogonal to our present work. Although our goal is to generate

proofs directly with deep learning, rather than focusing on specific subproblems that are important in theorem proving,

we expect (as we discussed in Section 4.3) that the combination of our approach with these techniques is also beneficial

to our technique.

7.1.2

Formula proving. Solving the Boolean satisfiability (SAT) problem by encoding problem instances into neural
networks has been attempted in early days [Johnson 1989]. Recent work uses DNNs as a binary classifier of Boolean

logical formulas. Bünz and Lamm [2017] represent a Boolean formula in conjunctive normal form (CNF) as a graph

where variable nodes are connected to nodes that represent disjunctive clauses referring to the variables and apply a

graph neural network [Scarselli et al. 2009] to classify the satisfiability of the formula. Similarly NeuroSAT [Selsam

et al. 2018] regards CNF formulas as graphs, but it adopts a message passing model and can often (not always) produce

a Boolean assignment, which makes it possible to check that the formula is truly satisfied. Evans et al. [2018] tackles

the entailment problem in the propositional logic, that is, whether a propositional conjecture can be proven under

considered assumptions. They also develop a new DNN model that classifies whether a given entailment holds. These

lines of work do not guarantee the correctness of the solution. Our work, although the procedure may not terminate,

guarantees the correctness of the returned proof.

Sekiyama et al. [2017] applied deep learning to proof synthesis. Their key idea is that the task of proof synthesis

can be seen as a translation task from propositions to proofs. Based on this idea, they use a sequence-to-sequence

22

Taro Sekiyama and Kohei Suenaga

model [Sutskever et al. 2014], which is widely used in machine translation with deep learning, in order to translate a

proposition to its proof. As shown in Section 6.4.2, our proposition-to-proof model outperforms their model from the

perspectives of (1) the number of propositions that are successfully proved and (2) the time spent by the proof-synthesis

procedures.

7.2 Neural program synthesis

Synthesizing proofs from propositions can be regarded as synthesizing programs from types via the Curry–Howard

isomorphism [Sørensen and Urzyczyn 2006]. Program synthesis is one of the classical AI problems, and synthesis with
deep learning, dubbed neural program synthesis, has been studied recently. A typical task of the neural program synthesis
is to produce programs satisfying given input-output examples [Balog et al. 2016; Devlin et al. 2017; Parisotto et al.

2016; Yin and Neubig 2017]. Although it appears to be difficult to transfer their approaches to proof synthesis directly

since the task of proof synthesis represents a specification of a program by types (i.e., propositions), not input–output

pairs, there are similarities between them. For example, the AST decoder based on the syntax of the target language by

Yin and Neubig [2017] is similar to that used in our work in that we also construct an AST of a proof gradually, whereas

our proposition-to-proof model effectively uses the proof obligations which do not appear in neural proof synthesis.

We expect that the ideas in neural program synthesis work in proof synthesis as well to achieve better performance.

7.3 Deep neural networks for tree structures

Propositions and proofs have variable sizes, and a major way to handle such variable-length data, especially, in natural

language processing is to deal with them as sequences. However, such sequence representation collapses the structural

information contained in inputs. Indeed, our work takes advantage of the fact that proofs can be interpreted as derivation
trees, which makes it possible to synthesize proofs gradually. Besides propositions and proofs, many objects are tree-
structured—e.g., parse trees, hierarchical dependency graphs, and index structures in databases—and recently there are
many studies on tree generation with DNNs. Zhang et al. [2016] propose tree long short-term memory (TreeLSTM) to
construct tree structures. TreeLSTM relates a parent and its children by a dependency path, which connects a child

node to the parent via the siblings. This representation of node relationships needs more steps to pass encoding features

to a node. As shown in Section 6.4.1, it would cause degradation of the performance. Dong and Lapata [2016] generate

tree-structured logical formulas from natural sentences by a top-down decoder. Their decoding method provides special

nodes that link a parent to its children, whereas our work does not need such nodes because we know whether a node

has children by looking at the inference rule of it. Alvarez-Melis and Jaakkola [2017] also study a decoder for generation

of trees where each node has an arbitrary number of children. Their decoder performs two predictions: one is whether

a node has a child; and the other is whether it has a sibling. Unlike the task that they address, the number of children of

an AST node in the propositional logic is fixed and it is enough to predict the kind of a node. Mou et al. [2016] develop

a tree-based convolutional neural network (TBCNN), which calculates a feature vector of a node by using vectors of

nodes near it. While it is similar to the AST convolution and the aggregation layer in our work, there is a difference

for each. First, the AST convolution refers to all adjacent nodes including the parent, whereas the TBCNN considers

only children. Second, the aggregation layer can be seen as “fold” on trees with feature vectors and the produced

single vector should contain features of all nodes in a tree. The TBCNN uses a max-pool to integrate feature vectors

of nodes into a single vector, that is, it produces a vector each dimension of which has the value maximum among

the corresponding dimensions of feature vectors of the nodes. Although max-pools are commonly used in usual (not

23

tree-based) convolutional neural networks [Krizhevsky et al. 2012], it is unclear that gathering only maximum values

does not drop any important feature of nodes in a tree.

8 CONCLUSION

We present an approach to applying deep learning to the APS problem. We statistically formulate the APS problem in

terms of probabilities so that we can quantify the likelihood of a term being a correct proof of a proposition. From this

formulation, we show that this likelihood can be calculated by using the likelihood of an inference rule being applied

at a specified position in a proof, which enables us to synthesize proofs gradually. To approximate this likelihood,

we develop a DNN that we call a proposition-to-proof model. Our DNN model encodes the tree representation of a

proposition and decodes it to estimate an inference rule to be applied by using the proof obligation to be discharged

effectively. We train the proposition-to-proof model on a dataset of automatically generated proposition-proof pairs

and confirmed that the trained model achieves 96.79% accuracy in the inference-rule estimation, though there is still

room for improvement. We also develop a proof synthesis procedure with the trained DNN model and show that it can

synthesize many proofs of a proposition in short time.

Our exploration of APS along with deep learning is still at the early stage; there are many challenging tasks to be

addressed. One of the important challenges is to extend the target logic to more expressive ones such as first-order

logic and higher-order logic. For example, first-order logic introduces the notions of predicates and quantification.

To learn a feature representation of a predicate, we may need a DNN model that takes the “meaning” of a predicate

into account. Quantification not only makes formulas complicated but also requires us to deal with the problem of

instantiation. Another important notion that we need to deal with is the induction principle. With the extension of

the logic, it is expected that a problem with datasets happen. One promising way to address it is, as done by Kaliszyk

et al. [2017], making a dataset from publicly available proofs. Furthermore, the creation of a benchmark collecting

challenging tasks related to APS is crucial for the development of APS with deep learning, as ImageNet [Russakovsky

et al. 2015] contributes to the advance of image processing.

Another future direction is improvement of a model. Our model is expected to have access to assumptions via the

feature vector of a given proposition. However, it may be more useful to encode a set of assumptions directly, as we

encoded proof obligations in this work. A problem with it is that the number of assumptions is not fixed; DNNs are

good at handling objects with a fixed size but require efforts to deal with variable-sized data. Another possible issue is

the vanishing gradient problem; gradients in very deep neural networks often vanish, which makes learning difficult.

Since our tree-structured model can be considered to have variable-length nonlinear layers and become deeper as

propositions and/or proofs are larger, that problem would be more serious when we deal with larger propositions and

proofs than the present work. We expect that the recent progress in research to address this problem works well also in

our settings; especially, residual blocks [He et al. 2016] and LSTMs [Hochreiter and Schmidhuber 1997] are promising

workarounds.

ACKNOWLEDGMENTS

This work is partially supported by JST PRESTO Grant Number JPMJPR15E5, Japan.

REFERENCES
David Alvarez-Melis and Tommi S. Jaakkola. 2017. Tree-structured Decoding with Doubly-Recurrent Neural Networks. International Conference on

Learning Representations.

24

Taro Sekiyama and Kohei Suenaga

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473

(2014). arXiv:1409.0473 http://arxiv.org/abs/1409.0473

Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016. DeepCoder: Learning to Write Programs. CoRR

abs/1611.01989 (2016). arXiv:1611.01989

Michael Barnett, Robert DeLine, Manuel Fähndrich, Bart Jacobs, K. Rustan M. Leino, Wolfram Schulte, and Herman Venter. 2005. The Spec# Programming
System: Challenges and Directions. In Verified Software: Theories, Tools, Experiments, First IFIP TC 2/WG 2.3 Conference, VSTTE 2005, Zurich, Switzerland,
October 10-13, 2005, Revised Selected Papers and Discussions. 144–152. https://doi.org/10.1007/978-3-540-69149-5_16

Ch. Ben-Yelles. 1979. Type Assignment in the Lambda-Calculus: Syntax and Semantics. Ph.D. Dissertation. Department of Pure Mathematics, University

College of Swansea.

Wolfgang Bibel. 2013. Automated theorem proving. Springer Science & Business Media.
Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.
Benedikt Bünz and Matthew Lamm. 2017. Graph Neural Networks and Boolean Satisfiability. CoRR abs/1702.03592 (2017). arXiv:1702.03592
Patrice Chalin, Perry R. James, and George Karabotsos. 2007. An Integrated Verification Environment for JML: Architecture and Early Results. In

Proceedings of the 2007 Conference on Specification and Verification of Component-based Systems: 6th Joint Meeting of the European Conference on
Software Engineering and the ACM SIGSOFT Symposium on the Foundations of Software Engineering (SAVCBS ’07). ACM, New York, NY, USA, 47–53.
https://doi.org/10.1145/1292316.1292322

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the Properties of Neural Machine Translation: Encoder-Decoder

Approaches. CoRR abs/1409.1259 (2014). arXiv:1409.1259 http://arxiv.org/abs/1409.1259

Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. RobustFill: Neural Program
Learning under Noisy I/O. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017. 990–998.

Li Dong and Mirella Lapata. 2016. Language to Logical Form with Neural Attention. In Proceedings of the 54th Annual Meeting of the Association for

Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.

Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. 2018. Can Neural Networks Understand Logical Entailment? CoRR

abs/1802.08535 (2018). arXiv:1802.08535

Jean-Christophe Filliâtre and Andrei Paskevich. 2013. Why3 - Where Programs Meet Provers. In Programming Languages and Systems - 22nd European
Symposium on Programming, ESOP 2013, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2013, Rome, Italy,
March 16-24, 2013. Proceedings. 125–128.

Melvin Fitting. 2012. First-order logic and automated theorem proving. Springer Science & Business Media.
Neil Ghani. 1995. ßn-Equality for Coproducts. In Typed Lambda Calculi and Applications, Second International Conference on Typed Lambda Calculi and

Applications, TLCA ’95, Edinburgh, UK, April 10-12, 1995, Proceedings. 171–185. https://doi.org/10.1007/BFb0014052

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep Sparse Rectifier Neural Networks. In Proceedings of the Fourteenth International Conference

on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011. 315–323.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.
John Harrison. 2009. HOL Light: An Overview. In Theorem Proving in Higher Order Logics, 22nd International Conference, TPHOLs 2009, Munich, Germany,

August 17-20, 2009. Proceedings. 60–66.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on

computer vision and pattern recognition. 770–778.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen,
Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal
Processing Magazine 29, 6 (2012), 82–97.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735–1780.
Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural Networks 2, 5

(1989), 359–366.

Geoffrey Irving, Christian Szegedy, Alexander A. Alemi, Niklas Eén, François Chollet, and Josef Urban. 2016. DeepMath - Deep Sequence Models for
Premise Selection. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December
5-10, 2016, Barcelona, Spain. 2235–2243.

James L. Johnson. 1989. A Neural Network Approach to the 3-Satisfiability Problem. J. Parallel Distrib. Comput. 6, 2 (1989), 435–449.
Cezary Kaliszyk, François Chollet, and Christian Szegedy. 2017. HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving. CoRR

abs/1703.00426 (2017). arXiv:1703.00426

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2014).
Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski,
Michael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. 2009. seL4: Formal Verification of an OS Kernel. In Proceedings of the ACM
SIGOPS 22Nd Symposium on Operating Systems Principles (SOSP ’09). ACM, New York, NY, USA, 207–220. https://doi.org/10.1145/1629575.1629596
Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. The MIT

Press.

25

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6,
2012, Lake Tahoe, Nevada, United States. 1106–1114.

Xavier Leroy. 2009. Formal Verification of a Realistic Compiler. Commun. ACM 52, 7 (July 2009), 107–115. https://doi.org/10.1145/1538788.1538814
Sarah M. Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. 2017. Deep Network Guided Proof Search. In LPAR-21, 21st International

Conference on Logic for Programming, Artificial Intelligence and Reasoning, Maun, Botswana, May 7-12, 2017. 85–105.

Robin Milner. 1978. A Theory of Type Polymorphism in Programming. J. Comput. Syst. Sci. 17, 3 (1978), 348–375.
Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural Networks over Tree Structures for Programming Language Processing. In

Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 1287–1293.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. 2017. Exploring Generalization in Deep Learning. In Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA.
5949–5958.

Tobias Nipkow, Markus Wenzel, and Lawrence C. Paulson. 2002. Isabelle/HOL: A Proof Assistant for Higher-order Logic. Springer-Verlag, Berlin, Heidelberg.
Ulf Norell. 2009. Dependently Typed Programming in Agda. In Proceedings of the 4th International Workshop on Types in Language Design and Implementation

(TLDI ’09). ACM, New York, NY, USA, 1–2. https://doi.org/10.1145/1481861.1481862

Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. 2016. Neuro-Symbolic Program Synthesis.

CoRR abs/1611.01855 (2016). arXiv:1611.01855

Frank Pfenning. 2004. Automated theorem proving. Lecture notes, March (2004).
Herbert Robbins and Sutton Monro. 1951. A Stochastic Approximation Method. The Annals of Mathematical Statistics 22, 3 (09 1951), 400–407.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 3
(2015), 211–252.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The Graph Neural Network Model. IEEE Trans.

Neural Networks 20, 1 (2009), 61–80. https://doi.org/10.1109/TNN.2008.2005605

Stephan Schulz. 2013. System Description: E 1.8. In Logic for Programming, Artificial Intelligence, and Reasoning - 19th International Conference, LPAR-19,

Stellenbosch, South Africa, December 14-19, 2013. Proceedings. 735–743.

Taro Sekiyama, Akifumi Imanishi, and Kohei Suenaga. 2017. Towards Proof Synthesis Guided by Neural Machine Translation for Intuitionistic Propositional

Logic. CoRR abs/1706.06462 (2017). arXiv:1706.06462 http://arxiv.org/abs/1706.06462

Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. 2018. Learning a SAT Solver from Single-Bit

Supervision. CoRR abs/1802.03685 (2018). arXiv:1802.03685

Morten Heine Sørensen and Pawel Urzyczyn. 2006. Lectures on the Curry-Howard Isomorphism, Volume 149 (Studies in Logic and the Foundations of

Mathematics). Elsevier Science Inc., New York, NY, USA.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing

Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. 3104–3112.

The Coq Development Team. 2017. The Coq Proof Assistant Reference Manual – Version 8.7.2. http://coq.inria.fr.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning. In Proceedings of

workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS).

Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. 2017. Premise Selection for Theorem Proving by Deep Graph Embedding. In Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA.
2783–2793.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey,
Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,
George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and
Jeffrey Dean. 2016. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. CoRR abs/1609.08144
(2016). arXiv:1609.08144 http://arxiv.org/abs/1609.08144

Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General-Purpose Code Generation. In Proceedings of the 55th Annual Meeting of

the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers. 440–450.

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2016. Top-down Tree Long Short-Term Memory Networks. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016.
310–320.

