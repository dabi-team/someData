Software Vulnerability Detection via Deep Learning
over Disaggregated Code Graph Representation

Yufan Zhuang, Sahil Suneja, Veronika Thost, Giacomo Domeniconi, Alessandro Morari, Jim Laredo
IBM Research, Yorktown Heights, NY, USA

Abstract

Identifying vulnerable code is a precautionary measure to
counter software security breaches. Tedious expert effort has
been spent to build static analyzers, yet insecure patterns are
barely fully enumerated. This work explores a deep learning
approach to automatically learn the insecure patterns from
code corpora. Because code naturally admits graph structures
with parsing, we develop a novel graph neural network (GNN)
to exploit both the semantic context and structural regularity
of a program, in order to improve prediction performance.
Compared with a generic GNN, our enhancements include a
synthesis of multiple representations learned from the several
parsed graphs of a program, and a new training loss metric
that leverages the ﬁne granularity of labeling. Our model
outperforms multiple text, image and graph-based approaches,
across two real-world datasets.

1 Introduction

Recently, AI research has made inroads in source code un-
derstanding, and being able to perform tasks such as program
invariant inferencing [33], code categorization [56], code vari-
able prediction [11], and function naming [10], amongst oth-
ers. Similarly, in case of software vulnerability detection, what
used to be a domain traditionally dominated by static and dy-
namic analysis is seeing assistance and competition from AI
models. Shortcomings in existing techniques, such as the high
false positives of static analyzers, and the lack of complete-
ness of dynamic analysis, are a few reasons promoting the
entry of AI into this ﬁeld [52, 62].

In particular, Graph Neural Networks [48] are surfacing as
the state of the art in AI-based vulnerability detection. This is
because of their ability to operate directly on graph-structured
input, combined with the graph-level representation that can
be mapped naturally on to source code. These approaches
typically operate on source code converted to compiler-level
graph structures, incorporating the syntactic and semantic

relationships between program statements via abstract syntax
tree (AST), data ﬂow graph (DFG), and control ﬂow graph
(CFG).

Previous work [61] demonstrated that manually-created
graph-level vulnerability templates are useful in searching for
similar instances elsewhere in a project. With source code
represented as a graph, a template can be deﬁned as a control-
ﬂow and data-dependency relationship between locations of
interest in source code (i.e. edge connectivity between nodes
of a graph). For example, the absence of a ‘variable sanitiza-
tion template’ [61], i.e. value-range validation prior to being
used as a memory allocation size argument, may indicate a
potential buffer overﬂow vulnerability. In this work, we ex-
plore the possibility to automatically learn such vulnerability
templates by analyzing multiple programs, and then match
them across unseen source code graphs to catch vulnerabili-
ties in them. In a machine learning setting, the vulnerability
detection problem can be translated to a supervised classiﬁ-
cation task. This boils down to training a model over several
examples of source code snippets, labeled as being vulnera-
ble or healthy, with the goal of getting the model to extract
vulnerability patterns or signals from them.

In this paper, we present a novel graph neural network
(GNN) architecture designed to assist the model in concen-
trating on speciﬁc program constructs independently. Unlike
existing GNN-based approaches which are based on a single
multi-graph (fusing one or more graphs together) [19, 62],
our model operates on different representations of the source
code graph ({AST, CFG, DFG}) separately. By focusing on
the graphs individually, the model is afforded more oppor-
tunity to better learn vulnerable patterns which each graph
captures. Then, the aggregated information for each graph is
only composed at the end, when forming the representation
of the entire program for the downstream classiﬁcation task.
Our model, which we call 3GNN, is based on Crystal Graph
Convolutional Networks (CGCN) [60] and Self Attention
Graph (SAG) Pooling [31]. As shown in ﬁgure 1, the three
source code graph components are used to obtain initial vec-
tor representations. These are used as input for three separate

1

Figure 1: Our 3GNN model architecture with its 4 stages. Input source code is ﬁrst transformed into AST, CFG and DFG graphs to feed to the
model. Each graph is processed in three independent pipelines. Each pipeline follows the same Vectorization and Information Exchange steps.
The ﬁnal vector representations across the three pipelines are aggregated together, and fed to the ﬁnal classiﬁcation MLP layer, which generates
the prediction for the sample in terms of relative probability of a sample being vulnerable or healthy.

pipelines with shared weights, which are learned by the model
while being trained across multiple examples of vulnerable
and healthy source code. These vector representations are in
turn the input of multiple hierarchical modules comprised
of CGCN and SAG Pooling layers. Each layer drops irrel-
evant graph nodes to assist the next layer to narrow down
towards the set of nodes and edges better representative of
vulnerabilities. Finally, the three pipelines converge into a
multi-layer perceptron, which performs the ﬁnal classiﬁcation
of input code into vulnerable or healthy, based upon the rela-
tive probability the model prescribes the input code to each
class. Along with a novel model architecture, we also present
a new loss function speciﬁcally designed for the vulnerability
detection task, to measure (and improve upon) how far off
the model prediction is for the input source code sample. We
evaluate our model on two publicly available C/C++ datasets:
Draper [36], and QEMU+FFmpeg [62]. Our results show that
3GNN achieves superior performance compared to various
text (BiLSTM), image (CNN) and graph-based (GGNN) base-
lines, recording a 6.9% better F1 than the next-best model
(GGNN).

2 Motivation

Traditional Approaches: Traditionally, vulnerability detec-
tion has been grouped into the categories of static analyses and
dynamic analyses [20]. Static analysis tools, such as Clang
static analyzer [2] and Infer [4], typically build a model of
program states and reason over all possible behaviors that
might happen in the real execution. Constrained by a vast
search space of possible executions and behaviors, static anal-
yses make trade-offs in favor of scalability and abstract some
details, lose information and hence produce False Positives
(code tagged as being vulnerable but isn’t so). For rule-based
static analyses, such as linters [1] and taint analysis tools [3],
the quality of the results depend on the coverage of the de-
fect types and the quality of the rules. By contrast, dynamic
analyses execute the program and observe the execution be-

haviors. Testing, symbolic execution, concolic testing and
fuzzing [5–8, 15, 51] are commonly used for this purpose.
While it can concretely expose the defects in the execution,
it requires a precise input that can drive the execution to the
places of interests. Unfortunately, it’s usually challenging to
prepare inputs that achieve either good program coverages or
satisfying the particular path conditions.

AI Augmentation Approach: In contrast, with AI-based
approaches, the aim is to learn such code-to-vulnerability
mapping heuristics automatically. The idea is pretty
straightforward–presenting a large dataset of tagged examples
to a learning-based model so that it can ﬁgure out the prop-
erties which differentiate vulnerable from healthy code. For
example, when a model is exposed to enough examples (and
counter-examples) like the one shown in Figure 2(a), it may
learn that the existence of an if construct is an important sig-
nal in the context of avoiding buffer overﬂow vulnerabilities.
Similarly, by being exposed (trained) to examples of other
vulnerability types, the model can be taught to pick up the
corresponding signals or templates [61], such as for pointer
dereference, resource leak, use-after-free, and failure to re-
lease lock, amongst others. The learned heuristics from such
a macroscopic approach to vulnerability detection can then
augment the microscopic approach of static analyzers. The
end result is a more ﬁnely curated set of alerts to the developer
to aid in secure code development with hopefully lesser False
Positives.

The promise of this has been shown in previous work, with
learning-based approaches outperforming static analyzers in
vulnerability detection. For example, [45,52] report F11 in the
range of 0.02 to 0.45 for different static analyzers, compared
to 0.84 for a neural network which treats source code simply
as photos. These results are for the NIST Juliet test suite [40],
developed speciﬁcally for measuring the effectiveness of static
analysis tools.

Our Approach: Unlike existing approaches which treat

1F1 is the harmonic mean of Precision and Recall.

2

Figure 2: A sample code snippet with its corresponding AST, CFG and DFG graphs.

source code as images [27], or as a sequence of tokens [35,42],
we focus on the graph nature of source code which we feel is
a more natural representation of code, encapsulating semanti-
cally rich information. Unlike existing graph neural network
approaches [19, 62] which are based on a single graph com-
ponent, or a fused multi-graph, our model operates on the
AST, CFG, and DFG separately. Each graph excels at expos-
ing particular vulnerabilities through concentrating on the
respective program constructs while omitting irrelevant infor-
mation. For example, the CFG pairing mallocs and frees
detects potential resource leaks in program branches. Hence,
when constructing vector representations of a program by
using a graph neural network, it is more beneﬁcial to perform
graph convolution and message passing on each graph sepa-
rately, maximally exploiting vulnerable patterns a specialized
graph captures. This way, information of a node in the single
multi-graph, but irrelevant to a particular sub-graph targeting
speciﬁc vulnerability types, will not be aggregated blindly for
this sub-graph, enhancing its opportunity of exposing the vul-
nerability. Aggregated information for each graph will only be
composed at the end, when forming the representation of the
entire program for the downstream classiﬁcation task. Further-
more, with bigger program size, which is quite often the case
with real-world code, the corresponding graphs can become
quite big. This makes it harder for the model to efﬁciently
gather information across the graph, during learning. Treating
the graph components separately limits this information ﬂow

complexity, assisting in more efﬁcient learning.

3 Design

Figure 1 shows our 3GNN model architecture, which takes in
raw source code and classiﬁes it as being vulnerable or not.
It consists of the following 4 stages: (i) converting code to
graphs, (ii) encoding graphs as vectors, (iii) propagating infor-
mation through the graphs, and (iv) classifying ﬁnal vectors
as representing vulnerable or healthy code.

3.1 Code-to-Graph Transformation

The ﬁrst step is the conversion of source code into a represen-
tation amenable for consumption by neural networks. With
graph neural networks, which can operate on graph inputs
directly, we are able to preserve syntactic and semantic rela-
tionships of source code components, by using compiler-level
graph structures to represent code. In particular, the code’s
syntax information is captured by the AST, as shown in Fig-
ure 2(b). The AST models the structures of the function but
is insufﬁcient to capture the program behavior. Semantic in-
formation is encapsulated by the data ﬂow (DFG) and control
ﬂow (CFG) graphs. In Figure 2(c), the Use-Def edges with
tag “D_a” represent the data dependency from the variable
a deﬁnition in line 3, to it’s use in lines 5 and 7 of the code
snippet shown in Figure 2(a). Similarly, the edges of the CFG

3

(Figure 2(d)) capture the execution order such as conditional
branches. We use the Joern open-source tool [21] to convert
each code sample into its graph representation.

3.2 Vectorization

After converting source code to graphs, the next step is to
convert its nodes and edges into a vector representation con-
sumable by neural networks.

Node Vectors: Each node in the code graph is composed
of a node type and its source code component. While us-
ing Joern to convert source code to graph, we get 69 node
types, for example Identiﬁer, CallExpression, IfStatement etc.
For each node, the type is encoded into a one-hot vector2.
Each node’s source code component, on the other hand, is
encoded through a transformer encoder [55], which preserves
semantic information between the source code tokens. We
select transformer [55] as the encoder to encode the source
code since it has been shown to provide better contextualized
embeddings than another semantic-preserving embedding al-
ternatives, such as Word2vec [38] (see Section 4.2.1 for more
embedding alternatives).

These two vector encodings (node type vector and node
code vector) are then concatenated to form the node repre-
sentation vector. More concretely, considering the ith node
of the nth graph, we denote its type as tn,i ∈ {1, 2, . . . , 69}, its
code as cn,i ∈ {1, 2, . . . , D}ln,i where D denotes the size of the
dictionary and ln,i denotes the number of tokens of the node
code cn,i. The resulting node vector is denoted as xn,i, that is
obtained through the following process, where we use [·] to
denote concatenation:

xn,i = [e(tn,i), Transformer(cn,i)]

Edge Vectors: Joern’s conversion of source code to graph
yields 12 different types of edges, for example USE, DEF,
CONTROLS, etc. To this set, we add the Next Consecutive
Sequence (NCS) edges of the AST leaf nodes to maintain the
sequential information in the graph [11]. Each edge is then
encoded as a 13-dimensional one-hot vector. Formally, we
denote the edge vector between the ith node and the jth node
of the nth graph as en,i, j ∈ {0, 1}13.

3.3

Information Exchange

This step takes the information encoded in the nodes at the
local statement-level from step 3.2, and performs a global
information exchange throughout the source code graph. This
is done for each graph in the pipeline ({AST, CFG, DFG})
separately. The idea is to gather information around each
node’s neighborhood, weighted according to the different
edge types. After this information exchange, a graph-level

2An encoding scheme to convert categorical features into a numeric array.
Each category gets encoded in a num_categories dimension array, with a ‘1’
only at the location of the corresponding category, and ‘0’s elsewhere.

representation is formed from the information accumulated at
the relevant nodes. Finally, graph-level representations from
the three pipelines are aggregated, and sent to the next (classi-
ﬁcation) layer.

Graph Convolution: The core advantage of our approach
comes from the disaggregated treatment of the source code
graph. This, combined with a pooling module (described next)
which drops irrelevant nodes, allows our model to focus on the
nodes and edges better representative of vulnerabilities. As
such, it is independent of the module responsible for message
massing, with available alternatives including R-GCN [50],
CGCN [60], GGNN [33], for example. We chose the crys-
tal graphical convolutional neural network (CGCN) [60] for
aggregating neighbouring nodes and edges’ information dur-
ing message passing. Our rationale behind this decision is
two-fold: First convolutional graph neural networks (such as
CGCN) are more scalable than recurrent graph neural net-
works (such as GGNN) [59], since the latter needs to com-
pute and store several states for one node at the same time,
while in CGCN each node vector is computed and updated
immediately once per-layer. Second, CGCN is more efﬁcient
compared to other convolutional graph neural networks which
also take in edge information, such as R-GCN. Unlike the
latter, which computes update from the neighbouring nodes
repetitively for each type of connections, CGCN takes only
one aggregation step for all types of neighbouring edges and
nodes.

The actual information exchange between the nodes of

each code graph is governed by the following equation:

′

n,i = xn,i + ∑
x
j∈N(i)

σ([xn,i, xn, j, en,i, j]W1 + b1)

⊙g([xn,i, xn, j, en,i, j]W2 + b2)

′

n,i and xn,i denote updated and original ith node’s
where x
vector of the nth, Ni denotes the neighbours of the ithnode
respectively, σ denotes the Sigmoid activation [41] and g de-
notes the Softplus activation [41]. W1,W2, b1, b2 are learnable
weights.

Hierarchical Modules: Vulnerability analysis requires a
model to produce graph-level predictions. However, typical
vulnerabilities such as buffer overﬂow or pointer dereference
usually would only take place in few statements, thus the
relevant nodes are often the minority and may not be directly
connected to each other. Instead of just one message passing
layer, we use a hierarchical approach to ﬁlter out irrelevant
nodes, to allow the our model to focus more on the set of
nodes and edges better representative of vulnerabilities.

Speciﬁcally, at each layer, node vectors are updated by a
CGCN module, and are then passed to a Self Attention Graph
(SAG) Polling module [31] which will drop half of the nodes
in the current graph. The rest of the nodes will be passed to
the next layer. We read out the graphical representations at
the end of each layer by a soft attention module, and sum all

4

where θ(L)

1 , θ(L)

2

are learnable weights.

BinaryLoss(y, ˆy) =

the layer-wise representations together to form the ﬁnal graph
representation (per pipeline).

The precise steps to narrow down on the relevant nodes and
edges is as follows. The steps are the same for each pipleline.

1. At each layer (L), the node representations are updated

according to the aforementioned CGCN module.

n,i = ˜x(L)
x(L+1)

n,i + ∑
j∈N(i)

σ([ ˜x(L)

n,i , ˜x(L)

n, j , en,i, j]W(L)

1 + b(L)
1 )

⊙g([ ˜x(L)

n,i , ˜x(L)

n, j , en,i, j]W(L)

2 + b(L)
2 )

2. Then we select a subset of the nodes to the next layer
by SAG Pooling, in which the attention scores are calcu-
lated via a vanilla graph convolutional operator:

α(L+1)
n,i = θ(L)

1 x(L)

n,i + ∑
j∈N(i)

2 x(L)
θ(L)

n, j

3. We apply the attention score to the nodes in the top-k
subset, while the other nodes and their edges are dropped
from the current graph:

n,i = x(L+1)
˜x(L+1)

n,i ⊙ tanh(α(L+1)

n,i

)

if i ∈ topk(α(L+1)

n,i

)

4. Graph-level soft-attention read-out is performed at the

end of each layer, as follows:

O(L+1)
n

= ∑
i∈Vn

Softmax[MLP1( ˜x(L+1)

n,i

)] ⊙ MLP2( ˜x(L+1)

n,i

)

where MLP1 and MLP2 denote two distinct multi-layer
perceptrons [44] with learnable weights and standard
ReLU activation [26].

5. Finally, the graph level read-outs are summed together

to get the ﬁnal graph representation:

O(L)
n

g = ∑
L

3.4 Classiﬁcation Layer
In order to provide the class predictions, the ﬁnal graph rep-
resentations across the three pipelines are concatenated and
then fed to a classiﬁcation MLP, as follows:

ˆyn = σ(MLP([gAST, gDFG, gCFG])
(cid:1)

where σ denotes the Sigmoid activation, and gAST, gDFG, gCFG
represent the graph representations for AST, DFG and CFG
respectively. Essentially the model assigns probabilities to the
sample of belonging to different classes, based on the sam-
ple possessing the characteristics of a healthy vs. vulnerable

5

code. These characteristics or signals is what the model learns
during training as it gets exposed to multiple examples.

Loss Function: For vulnerability analysis datasets with bi-
nary labels, Cross Entropy Loss [24] with re-balancing class
weights is often sufﬁcient to provide good results. In case of
multi-label datasets such as Draper [36], this is not sufﬁcient,
due to the highly unbalanced label distribution and the possi-
bility that one function could fall into multiple vulnerability
classes. We designed a novel loss function for such scenario
that combines the Multi-class Binary Cross-Entropy (MBCE)
loss and the Binary loss that only takes in the most conﬁ-
dent prediction score. For a given ground truth y ∈ {0, 1}M
and prediction ˆy ∈ [0, 1]M pair, the proposed loss function is
computed as follows:

MBCELoss(y, ˆy) = −

1
M

M
∑
i=1

yi · log ˆyi + (1 − yi) · log(1 − ˆyi)

−yi · log ˆyi − (1 − yi) · log(1 − ˆyi),

i = arg maxi ˆyi;



−yi · log ˆyi − (1 − yi) · log(1 − ˆyi),


i = arg maxi ˆyi ∗ yi;

if max(y) = 0

if max(y) > 0

Loss(y, ˆy) = w1 · MBCELoss(y, ˆy) + w2 · BinaryLoss(y, ˆy)

where w1, w2 denote the balancing coefﬁcients for the two
components, and M denotes the number of possible classes, in
the case of Draper [36] M = 5. Note that just as normal cross
entropy loss functions, class weights can be easily added to
adjust for imbalanced classes.

The two-factor loss function has a number of appealing fea-
tures for multi-class multi-label vulnerability detection. First,
it addresses the binary classiﬁcation and multi-class multi-
label classiﬁcation at the same time, thus spares the effort
to train two classiﬁcation modules respectively. Second, it
offers the option to weigh more binary accuracy than getting
all the class labels correct. This option is relevant to our ap-
plication domain since vulnerability detection itself is a fairly
difﬁcult problem, and identifying the right class of vulnera-
bilities would be arguably more difﬁcult. Being able to point
out more vulnerabilities with some class missing labels is typ-
ically better than pointing out less vulnerabilities with their
correct class labels. Third, from our empirical evaluations we
found out that training with such fused targets gave superior
performance than training with each target separately.

4 Evaluation
We ﬁrst describe the datasets used for vulnerability detection.
We then present a qualitative comparison of the different
alternatives to vectorize source code. Next, we describe the
baseline models we compare our 3GNN model against, and
the model conﬁgurations. Finally, we present the results of
the vulnerability prediction performance comparison for the
the different models.

Table 1: Datasets overview.

Draper

QEMU+FFmpeg

Total Samples
Vulnerabilities
Non-Vulnerabilities
Avg. No. of Nodes
Max. No. of Nodes

1,192,509
76,188
1,116,321
162
500

21,020
9,116
11,904
48
355

4.1 Datasets

Draper [36] is a large and highly imbalanced dataset with
over a million C/C++ functions collected from open source
repositories on Github, and the Debian Linux distribution.
Each function was checked by three static analyzers and la-
beled by security experts. Note that the quality of the labels
strongly depends on the static analyzers—an imperfect but
acceptable approximation.

QEMU+FFmpeg [62] is a much smaller but balanced
dataset collected from Github repositories. The labelling is
done based on commit messages and domain experts.

For both datasets, we removed the functions which Joern
could not parse correctly, and those with more than 500 nodes
due to memory limits. For Draper, we use the 80/10/10 split
provided by [36], for QEMU+FFmpeg, we use 3-fold cross
validation and a 75/25 split as suggested in [62]. The datasets
are summarized in Table 1.

4.2 Baselines

Here we describe the multiple source code embedding
schemes we experimented with, as well as various text, image
and graph baseline models we compare 3GNN with.

4.2.1 Embedding Alternatives

We experimented with the following source code embedding
schemes:

1. Random embeddings atop a dictionary of tokens ex-

tracted from source code across the dataset

2. Normalizing all identiﬁers to a generic ID token to avoid
learning an incorrect association of identiﬁers (which
change across samples) with vulnerabilities

3. Symbolically normalizing identiﬁers as per their ap-
pearance order in each sample, e.g. VARIABLE_0,
FUNCTION_1, etc.

4. Vectorizing tokens using Word2vec [38] to preserve
the semantic attributes of the code and language. For
example, the vectors assigned to say memcpy and memset
will be closer to each other, and distant to those for digits.

In our preliminary experiments we observed that ID nor-
malization did not show any signiﬁcant performance improve-
ment over dictionary-based embeddings. In fact, it performed
worse for non-graph-based models, because they did not have
any auxiliary signals (like a graph’s edge dependency) to com-
pensate for the lost identiﬁer context. Instead, such models
performed better with the less-extreme symbolic normaliza-
tion which was able to preserve enough signals, while reduc-
ing identiﬁer-induced noise. Finally, the choice of whether
or not to use Word2vec was highly dependent on the dataset
and the model being trained. In light of these observations,
we use the best performing encoding scheme for evaluating
the baseline models described next.

4.2.2 Models

We use multiple state-of-the-art neural network baselines to
compare 3GNN against. These operate upon different repre-
sentations of source code: code-as-sequence, code-as-photo,
and code-as-graph. These have already been shown to be su-
perior to traditional approaches, as well as classical machine
learning approaches in prior work.

BiLSTM is a kind of recurrent neural network (RNN). It
processes source code as a sequence of tokens and tries to
extract temporal signals. It has been shown to have good
vulnerability detection performance [35]. BiLSTM performed
best with Word2Vec embeddings for QEMU+FFmpeg dataset.
No extra beneﬁts were observed for Draper.

CNN treats source code as images, and then try to extract
pixel signals from it using Convolutional Neural Network
models from the image domain. This model, borrowed from
[45], is a standard 1D-convolutional network and uses random
token embeddings.

GGNN: Instead of borrowing techniques from image and
time-series domain, this model operates at a more natural
graph-level representation of source code. Amongst the exist-
ing graph neural network alternatives, we choose GGNN [33]
as a comparison baseline since it has been shown to be quite
effective for source code understanding tasks [11]. It uses mes-
sage passing for transferring information from adjacent nodes
over the connecting edges, and aggregating such information
at each node via Gated Recurrent Units, a kind of RNN. For
this model, the same node encoder as for our 3GNN model
performed the best. There is one more recent work [62] which
is a slight variant of the vanilla GGNN mentioned above,
adding a convolution layer after the GGNN. Reproducibility
and correspondence issues (discussed in Section 5) prevents
its inclusion into the comparison baselines.

4.3 Model Conﬁgurations and Training

We use the hyper-parameters settings as suggested in [45, 62]
for the baseline models. On top of the already extensively
tuned hyper-parameters, we performed further tuning and

6

Table 2: Comparison of our model 3GNN with baselines for Draper
and QEMU+FFmpeg datasets.

Draper

QEMU+FFmpeg

Model

Acc

F1

MCC

Acc

F1

BiLSTM 92.04
92.26
CNN
93.49
GGNN
93.26
3GNN

49.35
49.40
50.80
54.32

0.461
0.460
0.474
0.512

60.32
59.01
61.86
61.03

61.71
61.36
59.0
61.83

MCC

0.243
0.227
0.239
0.252

Table 3: Extra performance squeezed by adding a Random Forest
classiﬁer on the ﬁnal feature representation generated by the models
just prior to the Classiﬁcation layer. Results for the Draper dataset.
Improvement observed irrespective of the models’ treatment of code
as linear sequence, or code as photo, or code as graph.

Model

Acc

F1

BiLSTM+RF
CNN+RF
3GNN+RF

92.79
92.70
93.22

51.13
52.72
54.94

MCC

0.477
0.497
0.520

conﬁrmed the goodness of the conﬁgurations. For our model,
we tuned hyper-parameters such as dropout, text embedding
and hidden dimensions. We set the class weights according
to the inverse class ratio for each dataset. For the transformer
encoder, one layer of transformer is used with 2 attention
heads and 64 hidden forward dimensions. Three attention
read-out heads are used with Softmax activation at the end of
the node encoding module for Draper while one attention head
is used for QEMU+FFmpeg due to its smaller data size. As
for the CGCN convolutional module, input and output dimen-
sions are set to the number of node types plus the dimension
of transformer encoder output. All of the intermediate SAG
Pooling modules are conﬁgured to drop 50% of the nodes
upon taking inputs; they keep the output dimensions of the
same size as the input. In the end, the three 128-dimensional
vectors representing each sub-graph are concatenated and fed
into the MLP to generate the ﬁnal predictions.

In the Draper experiments, we trained for 100 epochs and
evaluated performance on the test set according to the best
validation F1 score. For QEMU+FFmpeg we used an early-
stop mechanism with patience of 100 epochs (as [62]), and
we report the averaged cross-validation performances.

4.4 Results and Discussion

Table 2 summarizes the results on both datasets. Shown are
the accuracy, the F1 score, and the Matthews correlation coef-
ﬁcient (MCC). The classes in Draper are highly imbalanced
and thus the last two metrics more reliably measure the per-
formance [13, 16].

As can be seen, 3GNN outperforms all baselines. Being
able to extract 6.9% more F1 performance from even a highly
imbalanced dataset as Draper, shows the ability of our model
to enhance a generic graph neural network so as to better cap-
ture vulnerability signals. Considering that the node encoder
is the same for both GGNN and 3GNN, this performance
improvement is attributable to the independent use of the
sub-graphs combined with the hierarchical modules, allow-
ing our model to better localize attention towards potential
vulnerability templates.

which makes it hard to differentiate between the potential
of each model. This is corroborated by the signiﬁcant over-
ﬁtting we saw for all models on this dataset, even with heavy
regularization. Still, the performance relative to the Draper
dataset is better, indicating potential for practical use.

To measure the contribution of each individual graph com-
ponent, we evaluate the prediction performance achieved by
our model using each graph in isolation- AST vs. CFG vs.
DFG. Learning based only on the CFG representation of
source code leads to a 37% drop in F1, compared to incor-
porating information from all three pipelines in our 3GNN
model. DFG alone fairs better with a 14% decline in F1. AST
provides the most relevant information coming close to within
3.5% of 3GNN’s F1.

The large size of the Draper dataset enables stacking more
layers to squeeze some more performance from the mod-
els [45]. For example, as shown in Table 3, adding a Random
Forest layer [14] on top of the models improves model per-
formance across the board, by up to 6.7% as observed for the
CNN model. The absolute numbers are still low, as opposed to
the model performance (> 0.8 F1) seen on synthetic datasets
in previous work [45, 52], highlighting the complexity of a
noisy real-world dataset. Another reason for the lower perfor-
mance on Draper could be the quality of labels, having been
derived from static analyzers. A potentially erroneous ground
truth can greatly affect the ability of the models to learn the
differentiating properties of vulnerable vs. healthy code.

5 Related Work

Recently, several ML models have been proposed to help
alleviate some of the issues of traditional techniques, as dis-
cussed in Section 2. These methods tend to get signals from
source code features such as number of lines or conditional
statements, use of sensitive library functions or system calls,
call-stack depth, complexity measures like cyclomatic or Hal-
stead complexity, etc. [9, 23, 25, 37, 47, 53]. This can be aug-
mented with meta features such as commit messages and bug
reports [46, 63].

In case of QEMU+FFmpeg, the margin is not that signif-
icant. A possible reason is the limited size of this dataset,

Another approach to automated source code understand-
ing is via statistical language models. These methods capture

7

regularities in source code [28] at the token level. N-gram
approaches and Hidden Markov Models have been used for
tasks such as code completion, fault localization, and code
search [30, 39, 43]. Speciﬁc to the bug detection task, [57]
leverages n-gram language models to calculate the probability
distribution of program tokens in a project, and ﬂag low prob-
ability token sequences as potential bugs. More recently, deep
learning models have emerged as same or better performers
than n-gram models [58].

Instead of feature engineering as in classical ML, deep
learning approaches automatically extract signals from code
by treating it as an image or a linear sequence [17, 27, 34, 35,
42, 45]. We believe more meaningful signals are preserved in
a graph-level encoding of source code and thus experiment
with a code-as-a-graph representation. We are motivated by
the template matching work (non-learning-based) introduced
by Yamaguchi et al. [61]. Unlike their work which uses a
multi-graph, we process the individual graph components in
separate pipelines for more ﬁne-grained signal extraction.

From an encoding perspective, recent works have also
looked into deep learning over code-as-graph as in [11, 12,
54, 56]. [56] uses graph neural network based auto-encoders
for unsupervised learning over source code ASTs to automat-
ically cluster Java classes into different categories (business
logic, interface, and utility classes). [11] uses Gated Graph
Neural Networks to learn fundamental program structures,
such as predicting variable names given its usage, or selecting
the correct variable given a program location. Similarly, [54]
uses GNN to learn loop invariants for program veriﬁcation.
Code2vec [12] uses a collection of path-contexts (AST paths
+ leaf-node values) to encode source code, and an attention-
based neural network to learn appropriate names for a function
from its code. In our case, we operate at a macroscopic level
of automatically learning vulnerability templates from source
code graphs.

Most relevant work to ours which also uses deep learning
over code-as-graph in the software vulnerability domain in-
cludes [19, 32, 62]. [32] uses a combination of AST, PDG
and DFG graphs to encode Java methods. Attention GRU as
well as attention convolutional layer is used to focus on buggy
paths in the code. A direct comparison is not straightforward
due to a difference in the target language (the Joern graph
extractor we use is limited to C/C++) and the type of soft-
ware bugs in focus (we don’t target named-based bugs). The
contrast is similar with Hoppity [19], which adds a pointer
mechanism to a GNN [49] for bug localization. However, un-
like 3GNN’s focus and disaggregated graph methodology, it
targets general programming bugs speciﬁc to Javascript code,
and operates primarily on the AST. Devign [62] comes clos-
est to our approach, using GNNs and operating upon Joern
graphs on C/C++ code. It uses a multi-graph representation
tying different source code graphs together, as opposed to
our philosophy of assisting the model in concentrating on
speciﬁc program constructs independently. While we train

on all Joern edges, Devign uses a subset of it’s edges, while
adding a few other edge types inspired by [11]. The Devign
paper does not present results on already existing datasets
but introduces it’s own dataset, half of which has been pub-
licly released. We do not use this method as a baseline for
comparison because we are not convinced about our model
reproduction based on the paper details. We were unable to
achieve the results presented in the paper despite our best
efforts. Several attempts of correspondence with the authors
for clariﬁcations also failed.

Finally, there is yet another alternative of treating code-
as-natural-language, where approaches such as BERT [18],
which have shown great promise in the Natural Language Pro-
cessing (NLP) domain, can be put to use. Recently, it has been
adopted for source code understanding to perform tasks such
as variable misuse prediction, incorrect operator/operand de-
tection, code search, documentation generation, and function-
docstring mismatch [22, 29]. We are still exploring this di-
mension of code representation for vulnerability detection,
and do not compare with it in this work.

6 Acknowledgement

The authors would like to thank Dr. Jie Chen (MIT-IBM
Watson AI Lab, IBM Research) for his guidance in this work.

7 Conclusion

We presented a novel graph neural network architecture for
automatically detecting vulnerabilities in source code. We
enhance a generic GNN with the ability to better localize at-
tention towards potential vulnerability templates by (i) using
multiple graph representations of a program separately, (ii)
employing hierarchical layers of convolutional and pooling
modules, enabling the network to drop irrelevant nodes, and
(iii) developing a new training loss metric tailored for vulner-
ability analysis type tasks with multi-class / multi-labels. Our
model outperforms several text (BiLSTM), image (CNN) and
graph (GGNN) baseline models, recording a 6.9% better F1
than the next-best model (GGNN) for the real-world Draper
dataset. Future work includes the investigation of the explain-
ability for this model, and its application to new vulnerability
detection datasets.

References

[1] A list of awesome linters.

https://github.com/

caramelomartins/awesome-linters.

[2] Facebook Infer.

https://clang-analyzer.llvm.

org.

8

[3] HCL AppScan Source.

https://www.hcltechsw.

com/wps/portal/products/appscan/offerings/
source.

[4] Infer static analyzer. https://fbinfer.com/.

[5] libFuzzer – a library for coverage-guided fuzz testing.

https://llvm.org/docs/LibFuzzer.html.

[6] OSS-Fuzz - continuous fuzzing of open source software.

https://github.com/google/oss-fuzz.

[7] Pin - A Dynamic Binary Instrumentation Tool.

https://software.intel.com/en-us/articles/
pin-a-dynamic-binary-instrumentation-tool.

[8] Valgrind, an instrumentation framework for building
dynamic analysis tools. http://valgrind.org/.

[9] Mamoun Alazab, Sitalakshmi Venkatraman, Paul Wat-
ters, and Moutaz Alazab. Zero-day malware detection
based on supervised learning algorithms of api call sig-
natures. In Proceedings of the Ninth Australasian Data
Mining Conference-Volume 121, pages 171–182. Aus-
tralian Computer Society, Inc., 2011.

[10] Miltiadis Allamanis, Earl T Barr, Christian Bird, and
Charles Sutton. Suggesting accurate method and class
names. In Proceedings of the 2015 10th Joint Meeting
on Foundations of Software Engineering, pages 38–49,
2015.

[11] Miltiadis Allamanis, Marc Brockschmidt, and Mah-
moud Khademi. Learning to represent programs with
graphs. arXiv preprint arXiv:1711.00740, 2017.

[12] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Ya-
hav. code2vec: Learning distributed representations of
code. Proceedings of the ACM on Programming Lan-
guages, 3(POPL):40, 2019.

[13] Sabri Boughorbel, Fethi Jarray, and Mohammed El-
Anbari. Optimal classiﬁer for imbalanced data us-
ing matthews correlation coefﬁcient metric. PloS one,
12(6):e0177678, 2017.

[14] Leo Breiman. Random forests. Machine learning,

45(1):5–32, 2001.

[15] Cristian Cadar, Daniel Dunbar, and Dawson Engler.
Klee: Unassisted and automatic generation of high-
coverage tests for complex systems programs. In Pro-
ceedings of the 8th USENIX Conference on Operating
Systems Design and Implementation, OSDI’08, pages
209–224, Berkeley, CA, USA, 2008. USENIX Associa-
tion.

[16] Davide Chicco and Giuseppe Jurman. The advantages
of the matthews correlation coefﬁcient (mcc) over f1
score and accuracy in binary classiﬁcation evaluation.
BMC genomics, 21(1):6, 2020.

[17] Min-je Choi, Sehun Jeong, Hakjoo Oh, and Jaegul Choo.
End-to-end prediction of buffer overruns from raw
source code via neural memory networks. arXiv preprint
arXiv:1703.02458, 2017.

[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: Pre-training of deep bidi-
rectional transformers for language understanding. In
Proceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics, pages 4171–4186, 2019.

[19] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik,
Le Song, and Ke Wang. Hoppity: Learning graph trans-
formations to detect and ﬁx bugs in programs. In In-
ternational Conference on Learning Representations,
2019.

[20] Michael D. Ernst. Static and dynamic analysis: Synergy
and duality. In WODA 2003: Workshop on Dynamic
Analysis, pages 24–27, Portland, OR, USA, May 2003.

[21] Fabian Yamaguchi. Joern: A Robust Code Analysis
Platform for C/C++. http://www.mlsec.org/joern/
docs.shtml.

[22] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting
Liu, Daxin Jiang, et al. Codebert: A pre-trained model
for programming and natural languages. arXiv preprint
arXiv:2002.08155, 2020.

[23] Iker Gondra. Applying machine learning to software
fault-proneness prediction. Journal of Systems and Soft-
ware, 81(2):186–195, 2008.

[24] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and
Yoshua Bengio. Deep learning, volume 1. MIT press
Cambridge, 2016.

[25] Gustavo Grieco, Guillermo Luis Grinblat, Lucas Uzal,
Sanjay Rawat, Josselin Feist, and Laurent Mounier. To-
ward large-scale vulnerability discovery using machine
learning. In Proceedings of the Sixth ACM Conference
on Data and Application Security and Privacy, pages
85–96. ACM, 2016.

[26] Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Ma-
howald, Rodney J Douglas, and H Sebastian Seung. Dig-
ital selection and analogue ampliﬁcation coexist in a
cortex-inspired silicon circuit. Nature, 405(6789):947–
951, 2000.

9

[27] Jacob A Harer, Louis Y Kim, Rebecca L Russell, Onur
Ozdemir, Leonard R Kosta, Akshay Rangamani, Lei H
Hamilton, Gabriel I Centeno, Jonathan R Key, Paul M
Ellingwood, et al. Automated software vulnerabil-
ity detection with machine learning. arXiv preprint
arXiv:1803.04497, 2018.

[28] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. On the naturalness of soft-
ware. In 2012 34th International Conference on Soft-
ware Engineering (ICSE), pages 837–847. IEEE, 2012.

[29] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. Learning and evaluating contextual
embedding of source code. ICML’20, 2020.

[30] Wei Ming Khoo, Alan Mycroft, and Ross Anderson.
Rendezvous: A search engine for binary code. In 2013
10th Working Conference on Mining Software Reposito-
ries (MSR), pages 329–338. IEEE, 2013.

[31] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-
attention graph pooling. In International Conference on
Machine Learning, pages 3734–3743, 2019.

[32] Yi Li, Shaohua Wang, Tien N Nguyen, and Son
Van Nguyen. Improving bug detection via context-based
code representation learning and attention-based neural
networks. Proceedings of the ACM on Programming
Languages, 3(OOPSLA):1–30, 2019.

[33] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and
Richard Zemel. Gated graph sequence neural networks.
arXiv preprint arXiv:1511.05493, 2015.

[34] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu,
Zhaoxuan Chen, Sujuan Wang, and Jialai Wang. Sysevr:
a framework for using deep learning to detect software
vulnerabilities. arXiv preprint arXiv:1807.06756, 2018.

[35] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin,
Sujuan Wang, Zhijun Deng, and Yuyi Zhong. Vuldeep-
ecker: A deep learning-based system for vulnerability
detection. arXiv preprint arXiv:1801.01681, 2018.

[36] Louis Kim, Rebecca Russell. Draper VDISC Dataset -
Vulnerability Detection in Source Code. https://osf.
io/d45bw/.

[39] Syeda Nessa, Muhammad Abedin, W Eric Wong, Latifur
Khan, and Yu Qi. Software fault localization using n-
gram analysis. In International Conference on Wireless
Algorithms, Systems, and Applications, pages 548–559.
Springer, 2008.

[40] NIST. Juliet test suite v1.3. https://samate.nist.

gov/SRD/testsuite.php, 2017.

[41] Chigozie Nwankpa, Winifred Ijomah, Anthony Gacha-
gan, and Stephen Marshall. Activation functions: Com-
parison of trends in practice and research for deep learn-
ing. arXiv preprint arXiv:1811.03378, 2018.

[42] Michael Pradel and Koushik Sen. Deepbugs: A learning
approach to name-based bug detection. Proceedings of
the ACM on Programming Languages, 2(OOPSLA):147,
2018.

[43] Veselin Raychev, Martin Vechev, and Eran Yahav. Code
completion with statistical language models. In Pro-
ceedings of the 35th ACM SIGPLAN Conference on
Programming Language Design and Implementation,
pages 419–428, 2014.

[44] David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning internal representations by error
propagation. Technical report, California Univ San
Diego La Jolla Inst for Cognitive Science, 1985.

[45] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo La-
zovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood,
and Marc McConley. Automated vulnerability detection
in source code using deep representation learning. In
2018 17th IEEE International Conference on Machine
Learning and Applications (ICMLA), pages 757–762.
IEEE, 2018.

[46] Antonino Sabetta and Michele Bezzi. A practical
approach to the automatic classiﬁcation of security-
relevant commits. In 2018 IEEE International Confer-
ence on Software Maintenance and Evolution (ICSME),
pages 579–582. IEEE, 2018.

[47] Riccardo Scandariato, James Walden, Aram Hovsepyan,
and Wouter Joosen. Predicting vulnerable software com-
ponents via text mining. IEEE Transactions on Software
Engineering, 40(10):993–1006, 2014.

[37] Ruchika Malhotra. Comparative analysis of statisti-
cal and machine learning methods for predicting faulty
modules. Applied Soft Computing, 21:286–297, 2014.

[38] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In Ad-
vances in neural information processing systems, pages
3111–3119, 2013.

[48] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE Transactions on Neural
Networks, 20(1):61–80, 2008.

[49] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE Transactions on Neural
Networks, 20(1):61–80, 2008.

10

[60] Tian Xie and Jeffrey C Grossman. Crystal graph con-
volutional neural networks for an accurate and inter-
pretable prediction of material properties. Physical re-
view letters, 120(14):145301, 2018.

[61] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad
Rieck. Modeling and discovering vulnerabilities with
In 2014 IEEE Symposium on
code property graphs.
Security and Privacy, pages 590–604. IEEE, 2014.

[62] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du,
and Yang Liu. Devign: Effective vulnerability identiﬁ-
cation by learning comprehensive program semantics
via graph neural networks. In Advances in Neural Infor-
mation Processing Systems, pages 10197–10207, 2019.

[63] Yaqin Zhou and Asankhaya Sharma. Automated iden-
tiﬁcation of security issues from commit messages and
In Proceedings of the 2017 11th Joint
bug reports.
Meeting on Foundations of Software Engineering, pages
914–919. ACM, 2017.

[50] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne Van Den Berg, Ivan Titov, and Max Welling.
Modeling relational data with graph convolutional net-
works. In European Semantic Web Conference, pages
593–607. Springer, 2018.

[51] Koushik Sen, Darko Marinov, and Gul Agha. Cute: A
concolic unit testing engine for c. In Proceedings of
the 10th European Software Engineering Conference
Held Jointly with 13th ACM SIGSOFT International
Symposium on Foundations of Software Engineering,
ESEC/FSE-13, pages 263–272, New York, NY, USA,
2005. ACM.

[52] Carson D Sestili, William S Snavely, and Nathan M
VanHoudnos. Towards security defect prediction with
ai. arXiv preprint arXiv:1808.09897, 2018.

[53] Yonghee Shin, Andrew Meneely, Laurie Williams, and
Jason A Osborne. Evaluating complexity, code churn,
and developer activity metrics as indicators of software
vulnerabilities. IEEE Transactions on Software Engi-
neering, 37(6):772–787, 2010.

[54] Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur
Naik, and Le Song. Learning loop invariants for pro-
gram veriﬁcation. In Advances in Neural Information
Processing Systems, pages 7751–7762, 2018.

[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Ad-
vances in neural information processing systems, pages
5998–6008, 2017.

[56] Petr Vytovtov and Kirill Chuvilin. Unsupervised clas-
sifying of software source code using graph neural net-
works. In 2019 24th Conference of Open Innovations
Association (FRUCT), pages 518–524. IEEE, 2019.

[57] Song Wang, Devin Chollak, Dana Movshovitz-Attias,
and Lin Tan. Bugram: bug detection with n-gram lan-
guage models. In Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engi-
neering, pages 708–719, 2016.

[58] Martin White, Christopher Vendome, Mario Linares-
Vásquez, and Denys Poshyvanyk. Toward deep learning
software repositories. In 2015 IEEE/ACM 12th Working
Conference on Mining Software Repositories, pages 334–
345. IEEE, 2015.

[59] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong
Long, Chengqi Zhang, and S Yu Philip. A comprehen-
sive survey on graph neural networks. IEEE Transac-
tions on Neural Networks and Learning Systems, 2020.

11

