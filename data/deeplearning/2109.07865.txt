1
2
0
2

v
o
N
2
2

]

G
L
.
s
c
[

2
v
5
6
8
7
0
.
9
0
1
2
:
v
i
X
r
a

OMPQ: Orthogonal Mixed Precision Quantization

Yuexiao Ma1, Taisong Jin1*, Xiawu Zheng1, Yan Wang2, Huixia Li1,
Yongjian Wu3, Yunsheng Wu3, Guannan Jiang4, Wei Zhang4, Rongrong Ji1,5

1School of Informatics, Xiamen University

2Pinterest, USA

3Tencent Youtu Lab

4Contemporary Amperex Technology Co., Limited

5Institute of Artiﬁcial Intelligence, Xiamen University

Abstract

To bridge the ever increasing gap between deep neu-
ral networks’ complexity and hardware capability, network
quantization has attracted more and more research atten-
tion. The latest trend of mixed precision quantization takes
advantage of hardware’s multiple bit-width arithmetic oper-
ations to unleash the full potential of network quantization.
However, this also results in a difﬁcult integer programming
formulation, and forces most existing approaches to use an
extremely time-consuming search process even with various
Instead of solving a problem of the original
relaxations.
integer programming, we propose to optimize a proxy met-
ric, the concept of network orthogonality, which is highly
correlated with the loss of the integer programming but
also easy to optimize with linear programming. This ap-
proach reduces the search time and required data amount
by orders of magnitude, with little compromise on quan-
tization accuracy. Speciﬁcally, we achieve 72.08% Top-1
accuracy on ResNet-18 with 6.7Mb, which does not require
any searching iterations. Given the high efﬁciency and low
data dependency of our algorithm, we used it for the post-
training quantization, which achieve 71.27% Top-1 accu-
racy on MobileNetV2 with only 1.5Mb. Our code is avail-
able at https://github.com/MAC-AutoML/OMPQ.

1. Introduction

Recently, we witness an obvious trend in deep learn-
ing that the models have rapidly increasing complexity
[18, 19, 29–31, 37]. But due to practical limits such as la-
tency, battery, and temperature, the host hardware where
the models are deployed cannot keep up with this trend. It
results in a large and ever increasing gap between the com-
putational demands and resources. To address this issue,
network quantization [11, 28], which maps single precision

*Corresponding Author: jintaisong@xmu.edu.cn

Figure 1. Comparison of the resources used to obtain the optimal
bit conﬁguration between our algorithm and other mixed precision
algorithms (FracBits [34], HAWQ [12], BRECQ [23]) on ResNet-
18. “Searching Data” is the number of input images.

ﬂoating point weights or activations to lower bits integers
for compression and acceleration, has attracted consider-
able research attention. Network quantization can be nat-
urally formulated as an integer programming problem and a
straightforward approach is to relax the constraints to make
it a tractable optimization problem, at a cost of an approxi-
mated solution, e.g. Straight Through Estimation (STE) [4].
With the recent development of inference hardware,
arithmetic operations with variable bit-width become a pos-
sibility, and bring further ﬂexibility to the network quanti-
zation. To take full advantage of the hardware capability,
mixed precision quantization [12, 23, 33, 34] aims to quan-
tize different network layers to different bit conﬁgurations,
so as to achieve a better trade-off between compression ratio
and accuracy compared to traditional uniﬁed quantization.
While beneﬁting from the extra ﬂexibility, mixed precision

1

Searching Cost (iterations)FracBitsHAWQBRECQOurs1050120010040080120~10401080160Searching Data3000600900~1.1M1.2M1200FracBitsHAWQBRECQOurs1.2M256641024 
 
 
 
 
 
quantization also suffers from a more complicated and chal-
lenging optimization problem, with a non-differentiable and
extremely non-convex objective function. Therefore, exist-
ing approaches [12, 23, 33, 34] often require numerous data
and computing resources to search for the optimal bit con-
ﬁguration. For instance, FracBits [34] approximates the bit-
width by performing a ﬁrst-order Taylor expansion at the
adjacent integer, making the bit variable differentiable. This
allows it to integrate search process into training to obtain
the optimal bit conﬁguration. However, to derive a decent
solution, it still requires a large amount of computation re-
sources in the searching and training process. To resolve the
large demand on training data, Dong et al. [12] use the av-
erage eigenvalue of the hessian matrix of each layer as the
metric for bit allocation. However, the matrix-free Hutchin-
son algorithm for implicitly calculating the average of the
eigenvalues of the hessian matrix still needs 50 iterations
for each network layer. Another direction is black box opti-
mization. For instance, Wang et al. [33] use reinforcement
learning for the bit allocation of each layer. Li et al. [23] use
evolutionary search algorithm [17] to derive the optimal bit
conﬁguration, together with a block reconstruction strategy
to efﬁciently optimize the quantized model, which is called
Post-Training Quantization (PTQ). But the population evo-
lution process requires 1, 024 input data and 100 iterations,
and thus still demands on computation and data.

Different from the existing approaches of black box op-
timization or constraint relaxation, we propose to construct
a proxy metric, which could be substantially different form,
but highly correlated with the objective function of the orig-
inal integer programming. Speciﬁcally, we propose an in-
tuitive metric named ORthogonality Metric (ORM) as the
proxy metric. The intuition here is that the network layer
with low orthogonality is more likely represented by a linear
combination of the remaining layers while the orthogonal
network layer is irreplaceable. As illustrated in Fig. 1, we
only need a single-pass search process on a small amount
of data with ORM. In general, we propose to obtain the op-
timal bit conﬁguration by using the orthogonality of neural
network. According to the research of representation learn-
ing [3], the representation capability of the model is cor-
related to its number of parameters. The mixed precision
quantization can be viewed as a redistribution of network
parameters, so the bit-width of each layer is also positively
correlated with the representation capability of the model.
Therefore, we deconstruct the neural network model into a
set of functions, and deﬁne the orthogonality of the model
by extending its deﬁnition from a function f : R → R
to the entire network f : Rm → Rn. The measurement of
the orthogonality could be efﬁciently performed with Monte
Carlo sampling and Cauchy-Schwarz inequality, based on
which we can then assign a larger bit-width to the layer with
larger orthogonality to maximize the layers’ representation

capability and the mutual information during the quantiza-
tion process. We then further integrate the network orthog-
onality with speciﬁc constraints to construct a linear pro-
gramming problem to obtain the optimal bit conﬁguration.
In summary, our contributions are listed as follows:

• We introduce a novel metric of layer orthogonality to
model the mutual information among layers, and lever-
age it as a proxy metric to efﬁciently solve the mixed
precision quantization problem, which is the ﬁrst at-
tempt in the community and can easily be integrated
into any quantization schemes.

• We explore the theoretical properties of the proposed
orthogonality and prove that network orthogonality is
orthogonal invariant and scale invariant, which demon-
strates its robustness.

• We use orthogonality metric to construct a linear pro-
gramming problem, which can derive the optimal bit-
width conﬁguration in a few seconds without iteration.

• We also provide extensive experiments on ImageNet,
which demonstrate that proposed orthogonality based
approach could provide state-of-the-art quantization
performance with orders of magnitude’s speed up.

2. Related Work

Quantized Neural Networks: Existing neural network
quantization algorithms can be divided into two categories
based on their training strategy: post-training quantiza-
tion (PTQ) and quantization-aware training (QAT). Post-
training quantization [6, 23, 25] is an ofﬂine quantization
method, which only needs a small amount of data to com-
plete quantization process. Therefore, PTQ could obtain an
optimal quantized model efﬁciently, at a cost of accuracy
drop from quantization.
In contrast, quantization-aware
training [7, 8, 10, 12, 38] adopts online quantization strat-
egy. This type of methods utilize the whole training dataset
during quantization process. As a result, it has superior ac-
curacy but limited efﬁciency.

If viewed from a perspective of bit-width allocation strat-
egy, neural network quantization can also be divided into
uniﬁed and mixed precision quantization. Traditionally,
network quantization means uniﬁed quantization. Choi et
al. [10] aim to optimize the parameterized clip boundary
of activation value of each layer during training process.
Cai et al. [7] derive the optimal ﬁxed point quantization un-
der different bit-widths based on the Gaussian distribution
of activation values, thereby minimizing quantization error.
Chen et al. [8] introduce the meta-quantization block to ap-
proximate the derivative of non-derivable function. Zhou
et al. [38] use an incremental quantization scheme, which

2

Figure 2. An overview of OMPQ. Left: Deconstruct the model into a set of functions F. Middle: ORM symmetric matrix calculated from
F. Right: Linear programming problem constructed by the importance factor θ to derive optimal bit conﬁguration.

ﬁrst quantizes a part of the weights and optimizes the re-
maining real-valued weights to minimize the quantization
error until the entire neural network is quantized. Recently,
some works [12, 23, 34] that explore assigning different bit-
widths to different layers begin to emerge. Yang et al. [34]
approximate the derivative of bit-width by ﬁrst-order Tay-
lor expansion at adjacent integer points, thereby fusing the
optimal bit-width selection with the training process. How-
ever, its optimal bit-width searching takes 80% of the train-
ing epochs, which consumes lots of time and computation
power. Dong et al. [12] take the average eigenvalues of the
hessian matrix of each layer as the basis for the bit allo-
cation of that layer. However, the matrix-free Hutchinson
algorithm for calculating the average eigenvalues needs to
be iterated 50 times for each layer. Li et al. [23] propose
a post-training quantization method based on block recon-
struction strategy. Although the resource consumption to
get the optimal bit-width is smaller than the methods men-
tioned above, the evolutionary algorithm [17] used in [23]
still needs 1, 024 images as input and iterates 100 times.

Network Similarity: Given a neural network can natu-
rally be deconstructed into functions, assigning each layer
a bit-width is highly related to the problem of studying the
mutual information each layer can provide. And the key
lies in how to deﬁne the independence between those func-
tions. Previous works [2, 14–16, 21, 22] deﬁne covariance
and cross-covariance operators in the Reproducing Kernel
Hilbert Spaces (RKHSs), and derive mutual information
criteria based on these operators. Gretton et al. [15] pro-
pose the Hilbert-Schmidt Independence Criterion (HSIC),
and gave a ﬁnite-dimensional approximation of it. Further-
more, Kornblith et al. [21] propose the similarity criterion
CKA based on HSIC, and studies its relationship with other
similarity criteria. These works are enlightening and theo-
retically sound, but are highly complicated and hard to im-

3

plement. We propose a metric from the perspective of net-
work orthogonality, and give a simple and clear derivation.
Simultaneously, we use it to guide the network quantization.

3. Methodology

In this section, we will introduce our mixed precision
quantization algorithm from three aspects: how to deﬁne
the orthogonality, how to efﬁciently measure it, and how to
construct a linear programming model to obtain the optimal
bit conﬁguration.

3.1. Network Orthogonality

A neural network can be naturally decomposed into a
set of layers or functions. Formally, for the given input
x ∈ R1×(C×H×W ), we decompose a neural network into
F = {f1, f2, . . . , fL}, where fi represents the transforma-
tion from input x to the result of the i-th layer.
In other
words, if gi represents the function of the i-th layer, then
gi−1
fi(x) = gi
. Here we in-
troduce the inner product [1] between functions fi and fj,
which is formally deﬁned as,

(cid:0) . . . g1(x)(cid:1)(cid:17)

(cid:0)fi−1(x)(cid:1) = gi

(cid:16)

(cid:104)fi, fj(cid:105)P (x) =

(cid:90)

D

fi(x)P (x)fj(x)T dx,

(1)

i

If we set f (m)

where fi(x) ∈ R1×(Ci×Hi×Wi), fj(x) ∈ R1×(Cj ×Hj ×Wj )
are known functions when the model is given, and D is
(x) to be the m-th ele-
the domain of x.
ment of fi(x), then P (x) ∈ R(Ci×Hi×Wi)×(Cj ×Hj ×Wj )
is the probability density matrix between fi(x) and fj(x),
where Pm,n(x) is the probability density function of the
random variable f (m)
(x). According to the def-
j
inition in [1], (cid:104)fi, fj(cid:105)P (x) = 0 means that fi and fj are
weighted orthogonal. In other words, (cid:104)fi, fj(cid:105)P (x) is nega-

(x) · f (n)

i

32 bit32 bit32 bit32 bit𝑓1(𝑋)𝑓2(𝑋)𝑓𝐿−1(𝑋)𝜃𝑖=𝑒−𝛽(σ𝑗ORM𝑖,𝑗−1)ORM Matrix𝜃1𝜃𝑖𝜃𝑖+1𝜃𝐿3 bit8 bit7 bit4 bitMini-Batch Data𝑓𝐿(𝑋)(a) Deconstruct the Network(b) Calculate ORM Matrix(c) ConstructLinear Programming ProblemFeasible RegionObjective Function GradientInterior PointD |h∗(x) − (cid:80)

tively correlated with the orthogonality between fi and fj.
When we have a known set of functions to be quantized
F = {fi}L
i=1, with the goal to approximate an arbitrary
function h∗, the quantization error can then be expressed
by the mean square error: ξ (cid:82)
i ψifi(x)|2dx,
where ξ and ψi are error coefﬁcient and combination coef-
ﬁcient, respectively. According to Parseval equality [32], if
F is orthogonal basis functions set, then the mean square
error could achieve 0. Furthermore, the orthogonality be-
tween the basis functions is stronger, the mean square er-
ror is smaller, i.e., the model corresponding to the linear
combination of basis functions has a stronger representa-
tion capability. Here we further introduce this insight to
network quantization. Speciﬁcally, we propose to assign
a larger bit-width to the layer with stronger orthogonality
against all other layers to maximize the representation ca-
pability of the model. However, Eq. (1) has the integral of a
continuous function which is untractable in practice. There-
fore, we derive a new metric to efﬁciently approximate the
orthogonality of each layer in Sec. 3.2.

3.2. Efﬁcient Orthogonality Metric

We propose to leverage the Monte Carlo sampling to ap-
proximate the orthogonality of the layers, to avoid the in-
tractable integral. Speciﬁcally, from the Monte Carlo inte-
gration perspective in [5], Eq. (1) can be rewritten as

(cid:104)fi, fj(cid:105)P (x) =

=

(cid:90)

D

fi(x)P (x)fj(x)T dx
(cid:13)
(cid:13)
(cid:13)EP (x)[fj(x)T fi(x)]
(cid:13)
(cid:13)
(cid:13)F

(2)

.

We randomly get N samples x1, x2, . . . , xN from a train-
ing dataset with the probability density matrix P (x), which
allows the expectation EP (x)[fj(x)T fi(x)] to be further ap-
proximated as,

(cid:13)
(cid:13)
(cid:13)EP (x)[fj(x)T fi(x)]
(cid:13)
(cid:13)
(cid:13)F

≈

=

1
N

1
N

N
(cid:88)

(cid:13)
(cid:13)
fj(xn)T fi(xn)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)fj(X)T fi(X)(cid:13)

n=1

(cid:13)F ,

(3)

where fi(X) ∈ RN ×(Ci×Hi×Wi) represents the output of
the i-th layer, fj(X) ∈ RN ×(Cj ×Hj ×Wj ) represents the
output of the j-th layer, and || · ||F is the Frobenius norm.
From Eqs. (2) and (3), we have,

(cid:90)

N

D

fi(x)P (x)fj(x)T dx ≈ (cid:13)

(cid:13)fj(X)T fi(X)(cid:13)

(cid:13)F .

(4)

However, the comparison of orthogonality between differ-
ent layers is difﬁcult due to the differences in dimensional-
ity. To this end, we use the Cauchy-Schwarz inequality to

4

normalize it in [0, 1] for different layers. Applying Cauchy-
Schwarz inequality to the left side of Eq. (4), we can get

(cid:19)2

fi(x)P (x)fj(x)T dx
(cid:90)

(cid:18)

0 ≤

N

(cid:90)

D

(cid:90)

≤

D

N fi(x)Pi(x)fi(x)T dx

N fj(x)Pj(x)fj(x)T dx.

D

(5)

We substitute Eq. (4) into Eq. (5) and perform some simpli-
ﬁcations to derive our ORthogonality Metric (ORM):

ORM(X, fi, fj) =

2
||fj(X)T fi(X)||
F
||fi(X)T fi(X)||F ||fj(X)T fj(X)||F

,

(6)
where ORM ∈ [0, 1]. fi and fj is orthogonal when ORM =
0. On the contrary, fi and fj is dependent when ORM = 1.
Therefore, ORM is negatively correlated to orthogonality.
In addition, ORM is invariant to orthogonal transformation
Q and scale transformation α, which shows that ORM is
more stable and robust than other independence criteria [21,
24]. Speciﬁcally, ORM satisﬁes

ORM(X, fiQ, fj)

=

2
||fj(X)T fi(X)Q||
F
||QT fi(X)T fi(X)Q||F ||fj(X)T fj(X)||F

(7)

= ORM(X, fi, fj),

which indicates that ORM is invariant to orthogonal trans-
formations. Meanwhile, ORM also satisﬁes

ORM(X, αfi, fj)

=

2
||fj(X)T αfi(X)||
F
||αfi(X)T αfi(X)||F ||fj(X)T fj(X)||F

(8)

= ORM(X, fi, fj),

which indicates the invariance of scale transformation. Or-
thogonal transformation breaks the permutation symmetry
of a network and eliminates the singularity of network units
[26]. Scale transformation which exists widely in the Batch-
Norm layer [20] greatly impacts the network presentation.
Therefore, the orthogonal invariance and scale invariance of
ORM are necessary. The detailed proofs of Eqs. (7) and (8)
are provided in the appendix. Besides, we also provide ex-
periment comparison on the scale invariance and orthog-
onality invariance between ORM and other independence
criteria, the results are reported in Sec. 4.2.

3.3. Mixed Precision Quantization

For a speciﬁc neural network, we can calculate an or-
thogonality matrix K, where kij = ORM(X, fi, fj). Ob-

viously, K is a symmetric matrix and the diagonal ele-
ments are 1. Furthermore, we show some ORM matrices
on widely used models with different samples N in the ap-
pendix. We add up the non-diagonal elements of each row
of the matrix,

Algorithm 1 OMPQ
Input: Pre-trained model M with L layers, small batch of
data X sampled independently and identically.
Output: Optimal bit allocation b of the model.

1: Input X to M and derive function set F =

γi =

L
(cid:88)

j=1

kij − 1.

(9)

Smaller γi means stronger orthogonality between fi and
other functions in the function set F, and it also means that
former i layers of the neural network is more independent.
Thus, we leverage the monotonically decreasing function
e−x to model this relationship:

θi = e−βγi,

(10)

where β is a hyper-parameter to control the bit-width dif-
ference between different layers. We also investigate some
other monotonically decreasing functions (For the details,
please refer to Sec. 4.2) . θi is used as the importance fac-
tor for the former i layers of the network, then we deﬁne a
linear programming problem as follows:

Objective: max

b





L
(cid:88)

i=1

bi
L − i + 1



θj

 ,

L
(cid:88)

j=i

Constraints:

L
(cid:88)

i

M (bi) ≤ T .

(11)

M (bi) is the model size of the i-th layer under bi bit quan-
tization and T represents the target model size. b is the op-
timal bit conﬁguration. Maximizing the objective function
means assigning larger bit-width to more independent layer,
which implicitly maximizes the model’s representation ca-
pability. More details of network deconstruction, linear pro-
gramming construction and the impact of β are provided in
the appendix.

Note that it is extremely efﬁcient to solve the linear pro-
gramming problem in Eq. (18), which only takes a few
seconds on a single CPU. In other words, our method is
extremely efﬁcient (9s on MobileNetV2) when compar-
ing to the previous methods [12, 23, 34] that require lots
of data or iterations for searching.
In addition, our algo-
rithm can be used as a plug-and-play module to combine
with quantization-aware training or post-training quantiza-
tion schemes thanking to the high efﬁciency and low data
requirements. In other words, our approach is capable of
improving accuracy of SOTA methods, where experiment
results are reported in Secs. 4.3 and 4.4. The proposed al-
gorithm is summarized in Algorithm 1.

{f1, · · · , fL};

end for

for fj = f1, · · · , fL do

Calculate ki,j = ORM(X, fi, fj) by Eq. (6);

2: for fi = f1, · · · , fL do
3:
4:
5:
6: end for
7: Calculate layer orthogonality γi by Eq. (9);
8: Calculate importance factor θi by Eq. (17);
9: Solve the linear programming problem as Eq. (18);
10: return b.

3.4. Calculation Acceleration

Given a speciﬁc model, the dimension of features in-
creases with the depth of the layer. Thus, calculating
Eq. (6) involves huge matrices. In the following, we ana-
lyze the computation complexity of the ORM between fi
and fj. Suppose that fi(X) ∈ RN ×(Ci×Hi×Wi), fj(X) ∈
RN ×(Cj ×Hj ×Wj ), and the dimension of features in the
j-th layer is larger than that of the i-th layer. Further-
more, the time complexity of computing ORM(X, fi, fj) is
O(N C 2
j ). The huge matrix occupies a lot of mem-
ory resources, and also increases the time complexity of the
entire algorithm by several orders of magnitude. Therefore,
we derive an equivalent form to accelerate calculation. If
we take Y = fi(X), Z = fj(X) as an example, then
Y Y T , ZZ T ∈ RN ×N . We have:

j W 2

j H 2

||Z T Y ||2

F = (cid:10)vec(Y Y T ), vec(ZZ T )(cid:11) ,

(12)

where vec(·) represents the operation of ﬂattening matrix
into vector. From Eq. (13), we can observe that calcu-
lating Frobenius norm of matrix is transformed into inner
product of vectors, and the time complexity of calculat-
ing ORM(X, fi, fj) becomes O(N 2CjHjWj). When the
number of samples N is larger than the dimension of fea-
tures C ×H ×W , the norm form is faster to calculate thank-
ing to lower time complexity, vice versa. We have demon-
strated the speciﬁc acceleration ratio and proof of Eq. (13)
in appendix. Speciﬁcally, when the dimension of features is
100× larger than the number of samples, the average accel-
eration ratio can reach about 70×.

4. Experiments

In this section, we conduct a series of experiments to val-
idate the effectiveness of OMPQ on ImageNet. We ﬁrst in-
troduce the implementation details of our experiments. Ab-
lation experiments about the invariance of ORM, monotoni-

5

cally decreasing function and deconstruction granularity are
then conducted to demonstrate the importance of each com-
ponent. Finally, we combine OMPQ with widely-used QAT
and PTQ schemes, which shows a better compression and
accuracy trade-off comparing to the SOTA methods.

4.1. Implementation Details

The ImageNet dataset includes 1.2M training data and
50,000 validation data. We randomly obtain 64 training data
for ResNet-18/50 and 32 training data for MobileNetV2 fol-
lowing similar data pre-processing [18] to get the set of
functions F. OMPQ is extremely efﬁcient which only needs
a piece of Nvidia Geforce GTX 1080Ti and a single In-
tel(R) Xeon(R) CPU E5-2620 v4. For the models that have
a large amount of parameters, we directly adopt round func-
tion to convert the bit-width into an integer after linear pro-
gramming. Meanwhile, we adopt depth-ﬁrst search (DFS)
to ﬁnd the bit conﬁguration that strictly meets the different
constraints for small model, e.g. ResNet-18. The aforemen-
tioned processes are extremely efﬁcient, which only take a
few seconds on these devices. Besides, OMPQ is ﬂexible,
which is capable of leveraging different search spaces with
QAT and PTQ under different requirements. Finetuning im-
plementation details are listed as follow.

For experiments on QAT quantization scheme, we use
two NVIDIA Tesla V100 GPUs. Our quantization frame-
work does not exist integer division or ﬂoating point num-
bers in the network. We thus fuse the batch normaliza-
tion [20] layer into the convolution layer during the training
process. In the training process, initial learning rate is set to
1e-4, and the batch size is set to 128. We use cosine learning
rate scheduler and SGD optimizer with 1e-4 weight decay
during 90 epochs without distillation. We ﬁx the weight and
activation values of ﬁrst and last layer at 8 bit following the
previous works, where the search space is 4-8 bit.

For experiments on PTQ quantization scheme, we per-
form OMPQ on an NVIDIA Geforce GTX 1080Ti and com-
bine it with the ﬁnetuning block reconstruction algorithm
BRECQ [23]. In particular, the activation precision of all
layers are ﬁxed to 8 bit. In other words, only weights are
quantized, which is allocated in the 2-4 bit search space.

4.2. Ablation Study

Invariance. We compare the scale invariance and or-
thogonality invariance of ORM to Projection-Weighted
Canonical CorrelAtion (PWCCA) [24] and Linear Hilbert-
Schmidt Independence Criterion (Linear HSIC) [21].
In
order to check the scale and orthogonal invariance, we
randomly generate X, Y ∈ R100×10, orthogonal matrix
Q ∈ R10×10, and take scale α = 1.5. Then, we perform or-
thogonal transformation XQ and scale transformation αX.
Finally, we calculate absolute difference of the criterion be-
fore and after the transformation. The results are listed in

Tab. 1. It can be seen that PWCCA does not have orthogo-
nal invariance, and Linear HSIC is sensitive to scale trans-
formation. Meanwhile, ORM is more stable and robust to
perturbation than other criteria.

Independence
Criterion

Orthogonal
Transformation

Scale
Transformation

PWCCA [24]
Linear HSIC [21]
ORM

0.02
0
0

0
15.42
0

Table 1. Comparison of independence criterion on orthogonal in-
variance and scale invariance. The result is represented as the ab-
solute difference of the criterion before and after the transforma-
tion.

(%)

(%)

Decreasing ResNet-18 MobileNetV2 Changing
Function
e−x
−logx
−x
−x3
−ex

Rate
e−x
x−2
0
6x
ex

72.30
72.26
72.36
71.71
-

63.51
63.20
63.0
-
-

Table 2. The Top-1 accuracy (%) with different monotonically
decreasing functions on ResNet-18 and MobileNetV2.

Monotonically Decreasing Function. We then investigate
the monotonically decreasing function in Eq. (17). Obvi-
ously, second-order derivatives of monotonically decreas-
ing functions in Eq. (17) inﬂuence the changing rate of or-
thogonality differences. In other words, the variance of the
orthogonality between different layers becomes larger as
the rate becomes faster. We test the accuracy of ﬁve dif-
ferent monotonically decreasing functions on quantization-
aware training of ResNet-18 (6.7Mb) and post-training
quantization of MobileNetV2 (0.9Mb).

It can be observed from Tab. 2 that the accuracy grad-
ually decreases with the increasing of changing rate.
In
the corresponding bit conﬁguration, we also observe that
a larger changing rate also means a more aggressive bit al-
location strategy.
In other words, OMPQ tends to assign
more different bits between layers under a large changing
rate, which leads to a worse performance in network quan-
tization. Another interesting observation is the accuracy on
ResNet-18 and MobileNetV2. Speciﬁcally, quantization-
aware training on ResNet-18 requires numerous data, which
makes the change of accuracy insigniﬁcant. On the con-
trary, post-training quantization on MobileNetV2 is inca-
pable of assigning bit conﬁguration that meets the model
constraints when the functions are set to −x3 or −ex. To

6

Method

Baseline

RVQuant [27]
HAWQ-V3 [35]
OMPQ

PACT [10]
LQ-Nets [36]
HAWQ-V3 [35]
OMPQ

Method

Baseline

PACT [10]
LQ-Nets [36]
RVQuant [27]
HAQ [33]
OneBitwidth [9]
HAWQ-V3 [35]
OMPQ
OMPQ

W bit

A bit

Int-Only

Uniform

Model Size (Mb)

BOPs (G)

Top-1 (%)

(a) ResNet-18

32

8
8
mixed

5
4
mixed
mixed

32

8
8
8

5
32
mixed
6

(cid:37)

(cid:37)
(cid:34)
(cid:34)

(cid:37)
(cid:37)
(cid:34)
(cid:34)

-

(cid:37)
(cid:34)
(cid:34)

(cid:34)
(cid:37)
(cid:34)
(cid:34)

(b) ResNet-50

44.6

11.1
11.1
6.7

7.2
5.8
6.7
6.7

1, 858

116
116
97

74
225
72
75

73.09

70.01
71.56
72.30

69.80
70.00
70.22
72.08

W bit

A bit

Int-Only

Uniform

Model Size (Mb)

BOPs (G)

Top-1 (%)

32

32

5
4
5
mixed
mixed
mixed
mixed
mixed

5
32
5
32
8
mixed
5
5

(cid:37)

(cid:37)
(cid:37)
(cid:37)
(cid:37)
(cid:37)
(cid:34)
(cid:34)
(cid:34)

-

(cid:34)
(cid:37)
(cid:37)
(cid:37)
(cid:34)
(cid:34)
(cid:34)
(cid:34)

97.8

16.0
13.1
16.0
9.62
12.3
18.7
16.0
18.7

3, 951

133
486
101
520
494
154
141
156

77.72

76.70
76.40
75.60
75.48
76.70
75.39
76.20
76.28

Table 3. Mixed precision quantization results of ResNet-18 and ResNet-50. “Int-Only” means only including integer during quantization
process. “Uniform” represents uniform quantization.

Model

W bit Layer Block Stage

Net

4.3. Quantization-Aware Training

ResNet-18
MobileNetV2

5∗
3∗

72.51
69.37

72.52
69.10

72.47
68.86

72.31
63.99

Table 4. Top-1 accuracy (%) of different deconstruction granu-
larity. The activations of MobileNetV2 and ResNet-18 are both
quantized to 8 bit. ∗ means mixed bit.

this end, we select e−x as our monotonically decreasing
function in the following experiments.
Deconstruction Granularity. We study the impact of
different deconstruction granularity on model accuracy.
Speciﬁcally, we test four different granularity includ-
ing layer-wise, block-wise, stage-wise and net-wise on
the quantized-aware training of ResNet-18 and the post-
training quantization of MobileNetV2. As reported in
Tab. 4, the accuracy of the two models is increasing with
ﬁner granularities. Such difference is more signiﬁcant on
MobileNetV2 due to the different sensitiveness between the
point-wise and depth-wise convolution. We thus employ
layer-wise granularity in the following experiments.

We perform quantization-aware training on ResNet-
18/50, where the results and compress ratio are compared
with the previous uniﬁed quantization methods [10, 27, 36]
and mixed precision quantization [9, 33, 35]. As shown
in Tab. 3, OMPQ shows the best trade-off between accu-
racy and compress ratio on ResNet-18/50. For example,
we achieve 72.08% on ResNet-18 with only 6.7Mb and
75BOPs. Comparing with HAWQ-V3 [35], the difference
of the model size are negligible (6.7Mb, 75BOPs vs 6.7Mb,
72BOPs). Meanwhile, the model compressed by OMPQ is
1.86% higher than HAWQ-V3 [35]. Similarly, we achieve
76.28% on ResNet-50 with 18.7Mb and 156BOPs. And
OMPQ is 0.89% higher than HAWQ-V3 with similar model
size (18.7Mb, 156BOPs vs 18.7Mb, 154BOPs).

4.4. Post-Training Quantization

As we mentioned before, OMPQ can also be combined
with PTQ scheme to further improve the quantization ef-
ﬁciency thanking to its low data dependence and search
efﬁciency. Previous PTQ method-BRECQ [23] proposes
block reconstruction quantization strategy to reduce quan-

7

W bit

A bit Model Size (Mb)

Top-1 (%)

Searching Data

Searching Iterations

(a) ResNet-18

Method

Baseline

32

32

FracBits-PACT [34] mixed mixed

OMPQ
OMPQ

mixed
mixed

ZeroQ [6]
BRECQ† [23]
PACT [10]
HAWQ-V3 [35]

4
4
4
4

4
8

4
4
4
4

FracBits-PACT [34] mixed mixed

OMPQ

BRECQ [23]
OMPQ

mixed

mixed
mixed

4

8
8

44.6

4.5
4.5
4.5

5.81
5.81
5.81
5.81
5.81
5.5

4.0
4.0

71.08

69.10
68.69
69.73

21.20
69.32
69.20
68.45
69.70
69.38

68.82
69.34

-

1.2M
64
64

-
-
-
-
1.2M
64

1, 024
64

-

120
0
0

-
-
-
-
120
0

100
0

(b) MobileNetV2

Method

Baseline

BRECQ [23]
OMPQ

FracBits [34]
BRECQ [23]
OMPQ

W bit

A bit

Model Size (Mb)

Top-1 (%)

Searching Data

Searching Iterations

32

mixed
mixed

mixed
mixed
mixed

32

8
8

mixed
8
8

13.4

1.3
1.3

1.84
1.5
1.5

72.49

68.99
69.51

69.90
70.28
71.27

-

1, 024
32

1.2M
1, 024
32

-

100
0

120
100
0

Table 5. Mixed precision post-training quantization experiments on ResNet-18 and MobileNetV2. † means using distilled data in the
ﬁnetuning process.

tization errors. However, BRECQ also leverages evolu-
tionary search strategy [17] to select the bit conﬁguration,
which requires a large amount of computation resources.
The proposed bit search algorithm is orthogonal to the
BRECQ ﬁnetuning process. We replace the evolutionary
search algorithm with OMPQ and combine it with the ﬁne-
tuning process of BRECQ, which rapidly reduces the search
cost and achieves the better performance. Experiment re-
sults are demonstrated in Tab. 5, we can observe that OMPQ
clearly shows the superior performance to uniﬁed quantiza-
tion and mixed precision quantization methods under dif-
ferent model constraints. In particular, OMPQ outperforms
BRECQ by 0.52% on ResNet-18 under the same model size
(4.0Mb). OMPQ also outperforms FracBits by 1.37% on
MobileNetV2 with a smaller model size (1.5Mb vs 1.8Mb).

We also compare OMPQ with BRECQ and uniﬁed quan-
tization, where the results are reported in Fig. 3. Obviously,
the accuracy of OMPQ is generally higher than BRECQ
on ResNet-18 and MobileNetV2 with different model con-
straints. Furthermore, OMPQ and BRECQ are both better
than uniﬁed quantization, which shows that mixed precision
quantization is superior.

Figure 3. Mixed precision quantization comparison of OMPQ and
BRECQ on ResNet-18 and MobileNetV2.

5. Conclusion

In this paper, we have proposed a novel mixed precision
algorithm, termed OMPQ, to effectively search the optimal
bit conﬁguration on the different constraints. Firstly, we
derive the orthogonality metric of neural network by gener-
alizing the orthogonality of the function to the neural net-
work. Secondly, we leverage the proposed orthogonality

8

                   0 R G H O  6 L ] H   0 E                  $ F F X U D F \                                                                 5 H V 1 H W                   0 R G H O  6 L ] H   0 E                                                                                        0 R E L O H 1 H W 9  2 X U V % 5 ( & 4 8 Q L I L H G   E L W   E L Wmetric to design a linear programming problem, which is
capable of ﬁnding the optimal bit conﬁguration. Both or-
thogonality generation and linear programming solving are
extremely efﬁcient, which are ﬁnished within a few seconds
on a single CPU and GPU. Meanwhile, OMPQ also outper-
forms the previous mixed precision quantization and uniﬁed
quantization methods. For future work, we will study the
mixed precision quantization method combining multiple
knapsack problem with the network orthogonality metric.

References

[1] George B Arfken and Hans J Weber. Mathematical methods

for physicists, 1999. 3

[2] Francis R Bach and Michael I Jordan. Kernel independent
component analysis. Journal of Machine Learning Research
(JMLR), 3:1–48, 2002. 3

[3] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-
resentation learning: A review and new perspectives. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 35:1798–1828, 2013. 2

[4] Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients through stochastic
arXiv preprint
neurons for conditional computation.
arXiv:1308.3432, 2013. 1

[5] Russel E Caﬂisch. Monte carlo and quasi-monte carlo meth-

ods. Acta numerica, 7:1–49, 1998. 4

[6] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel
zero shot quantization framework. In Computer Vision and
Pattern Recognition (CVPR), pages 13169–13178, 2020. 2,
8

[7] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconce-
los. Deep learning with low precision by half-wave gaussian
quantization. In Computer Vision and Pattern Recognition
(CVPR), pages 5918–5926, 2017. 2

[8] Shangyu Chen, Wenya Wang, and Sinno Jialin Pan.
Metaquant: Learning to quantize by learning to penetrate
non-differentiable quantization. Neural Information Pro-
cessing Systems(NeurIPS), 32:3916–3926, 2019. 2

[9] Ting-Wu Chin, I Pierce, Jen Chuang, Vikas Chandra, and
Diana Marculescu. One weight bitwidth to rule them all.
In European Conference on Computer Vision (ECCV), pages
85–103, 2020. 7

[10] Jungwook Choi, Zhuo Wang, Swagath Venkataramani,
Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash
Pact: Parameterized clipping activa-
Gopalakrishnan.
arXiv preprint
tion for quantized neural networks.
arXiv:1805.06085, 2018. 2, 7, 8

[11] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. Binarized neural networks:
Training deep neural networks with weights and activations
constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830,
2016. 1

[12] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. Hawq-v2: Hes-
sian aware trace-weighted quantization of neural networks.

In Neural Information Processing Systems(NeurIPS), pages
18518–18529, 2020. 1, 2, 3, 5

[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions (ICLR), 2021. 13

[14] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Di-
mensionality reduction for supervised learning with repro-
ducing kernel hilbert spaces. Journal of Machine Learning
Research (JMLR), 5:73–99, 2004. 3

[15] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard
Sch¨olkopf. Measuring statistical dependence with hilbert-
In Algorithmic Learning Theory (ALT),
schmidt norms.
pages 63–77, 2005. 3

[16] Arthur Gretton, Ralf Herbrich, and Alexander J Smola. The
kernel mutual information. In International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages
IV–880, 2003. 3

[17] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,
Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot
neural architecture search with uniform sampling. In Euro-
pean Conference on Computer Vision (ECCV), pages 544–
560, 2020. 2, 3, 8

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition (CVPR), pages 770–778,
2016. 1, 6, 13

[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017. 1, 13

[20] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International Conference on Machine Learn-
ing (ICML), pages 448–456, 2015. 4, 6, 12

[21] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and
Geoffrey Hinton. Similarity of neural network represen-
In International Conference on Machine
tations revisited.
Learning (ICML), pages 3519–3529, 2019. 3, 4, 6

[22] Sue E Leurgans, Rana A Moyeed, and Bernard W Silverman.
Canonical correlation analysis when the data are curves.
Journal of the Royal Statistical Society: Series B (Method-
ological), 55:725–740, 1993. 3

[23] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi
Zhang, Fengwei Yu, Wei Wang, and Shi Gu. {BRECQ}:
Pushing the limit of post-training quantization by block re-
construction. In International Conference on Learning Rep-
resentations (ICLR), 2021. 1, 2, 3, 5, 6, 7, 8

[24] Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights
on representational similarity in neural networks with canon-
In Neural Information Processing Sys-
ical correlation.
tems(NeurIPS), page 5732–5741, 2018. 4, 6

9

[25] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and
Max Welling. Data-free quantization through weight equal-
ization and bias correction. In International Conference on
Computer Vision (ICCV), pages 1325–1334, 2019. 2
[26] Emin Orhan and Xaq Pitkow. Skip connections eliminate
singularities. In International Conference on Learning Rep-
resentations (ICLR), 2018. 4

[27] Eunhyeok Park, Sungjoo Yoo, and Peter Vajda. Value-aware
quantization for training and inference of neural networks.
In European Conference on Computer Vision (ECCV), pages
580–595, 2018. 7

[28] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In European Conference
on Computer Vision (ECCV), pages 525–542, 2016. 1
[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
In Computer Vision and
residuals and linear bottlenecks.
Pattern Recognition (CVPR), pages 4510–4520, 2018. 1
[30] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 1

[31] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Computer Vision and Pattern Recognition
(CVPR), pages 1–9, 2015. 1

[32] James Tanton. Encyclopedia of Mathematics. Facts on ﬁle,

2005. 4

[33] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.
Haq: Hardware-aware automated quantization with mixed
In Computer Vision and Pattern Recognition
precision.
(CVPR), pages 8612–8620, 2019. 1, 2, 7

[34] Linjie Yang and Qing Jin. Fracbits: Mixed precision quanti-
zation via fractional bit-widths. AAAI Conference on Artiﬁ-
cial Intelligence (AAAI), 35:10612–10620, 2021. 1, 2, 3, 5,
8

[35] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gho-
lami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida
Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural net-
work quantization. In International Conference on Machine
Learning (ICML), pages 11875–11886, 2021. 7, 8

[36] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang
Hua. Lq-nets: Learned quantization for highly accurate and
compact deep neural networks. In European Conference on
Computer Vision (ECCV), pages 365–382, 2018. 7

[37] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
In Computer Vision and Pattern
work for mobile devices.
Recognition (CVPR), pages 6848–6856, 2018. 1

[38] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong
Chen. Incremental network quantization: Towards lossless
In International Confer-
cnns with low-precision weights.
ence on Learning Representations (ICLR), 2017. 2

10

Appendix

6. Calculation Acceleration

Suppose that fi(X) ∈ RN ×(Ci×Hi×Wi) and fj(X) ∈
RN ×(Cj ×Hj ×Wj ) are the output of the i-th layer and the j-
th layer in the neural network, respectively. We set Y =
fi(X), Z = fj(X), then Y Y T , ZZ T ∈ RN ×N . The cal-
culation is accelerated through:

||Z T Y ||2

F = (cid:10)vec(Y Y T ), vec(ZZ T )(cid:11) .

(13)

Proof. We set p1 = Ci × Hi × Wi, p2 = Cj × Hj × Wj,
and zi,m, yi,m are the i-th row and m-th column entry of the
matrix Z and Y , then

||Z T Y ||2
F

Calculation Strategy
Inner Product Norm

Acceleration Ratio

N >p
N <p

18.42
0.12

0.34
8.40

54×
70×

Table 6. The ORM calculation time (second) of different calcula-
tion strategy between matrix Y and Z which have the same size.
There are two cases for the size of Y or Z, i.e., feature number p
is larger/smaller than the number of the samples N .

7. Invariance

Due to the homogeneity of the Frobenius norm, we can
easily derive the scale invariance of the ORM. Here we give
the proof of orthogonal invariance. Firstly, we introduce a
lemma to better demonstrate the proof.

Lemma 1. Given matrix Y ∈ RN ×p1, Z ∈ RN ×p2, we
have

||Z T Y ||2

F = tr(Y Y T ZZ T ).

(14)

(cid:33)2

zm,iym,j

zm,iym,j

(cid:33) (cid:32) N
(cid:88)

n=1

(cid:33)

zn,iyn,j

zm,iym,jzn,iyn,j

Proof.

=

=

=

=

=

p2
(cid:88)

p1
(cid:88)

(cid:32) N
(cid:88)

i=1

j=1

m=1

p2
(cid:88)

p1
(cid:88)

(cid:32) N
(cid:88)

i=1

j=1

m=1

p2
(cid:88)

p1
(cid:88)

N
(cid:88)

N
(cid:88)

i=1

j=1

m=1

n=1

N
(cid:88)

N
(cid:88)

p2
(cid:88)

p1
(cid:88)

m=1

n=1

N
(cid:88)

N
(cid:88)

i=1




p1
(cid:88)

m=1

n=1

j=1

tr(Y Y T ZZ T )


N
(cid:88)

N
(cid:88)

p1
(cid:88)





ym,jyn,j



(cid:33)

zm,izn,i

(cid:32) p2
(cid:88)

i=1

zm,iym,jzn,iyn,j

=

j=1

m=1

n=1

j=1



ym,jyn,j



(cid:33)

zm,izn,i

(cid:32) p2
(cid:88)

i=1

= (cid:10)vec(Y Y T ), vec(ZZ T )(cid:11)
= ||Z T Y ||2
F .

= (cid:10)vec(Y Y T ), vec(ZZ T )(cid:11) .

From Eq. (13), we can see that two computation forms have
different time complexities when dealing with different sit-
uations. Speciﬁcally, the time complexities of calculating
||Z T Y ||2
F and inner product of vectors are O(N p1p2) and
O(N 2(p1+p2+1)), respectively. In other words, when fea-
ture number (p1 or p2) is larger than the number of samples
N , we take the inner product form to speed up the calcu-
lation. Conversely, using ||Z T Y ||2
F is clearly more faster
than inner product form.

Concretely, we take N, p ∈ {10000, 100} as an exam-
ple and randomly generate matrix Y, Z ∈ RN ×p. In our
experiment, we calculate the ORM of Y, Z in vector inner
product form and norm form, respectively. The results are
reported in Tab. 6. From this table, when the number of fea-
tures is less than the number of samples, calculating ORM
in the norm form is much faster (54×) than the inner prod-
uct form. On the contrary, when the number of features is
greater than the number of samples, the inner product form
calculates the ORM faster (70×).

11

Therefore, we perform orthogonality transformation
fiQ, where Q is the orthogonal matrix which satisﬁes
QQT = I. Meanwhile, we set Y = fi(X), Z = fj(X).
The orthogonality invariance of the ORM as follows:

Proof.

ORM(X, fiQ, fj)
(cid:13)
(cid:13)Z T Y Q(cid:13)
2
(cid:13)
F
(cid:107)QT Y T Y Q(cid:107)F (cid:107)Z T Z(cid:107)F

=

=

=

=

tr(Y QQT Y T ZZ T )
(cid:112)tr(Y QQT Y T Y QQT Y T )(cid:107)Z T Z(cid:107)F

tr(Y Y T ZZ T )
(cid:112)tr(Y Y T Y Y T )(cid:107)Z T Z(cid:107)F
(cid:13)
(cid:13)Z T Y (cid:13)
2
(cid:13)
F
(cid:107)Y T Y (cid:107)F (cid:107)Z T Z(cid:107)F

= ORM(X, fi, fj).

8. Network Deconstruction and Linear Pro-

gramming

In this section, we elaborate on why the network is de-
constructed by the former i layers instead of the i-th layer.
Furthermore, we will illustrate how to construct a linear
programming problem in details.

8.1. Network Deconstruction

Fig. 4 demonstrates two approaches to the deconstruc-
tion of the network: the former i layers deconstruction and
the i-th layer deconstruction, and we naturally accept that
the latter deconstruction is more intuitive. We give the deﬁ-
nition of orthogonality for the i-th and j-th layers under the
latter deconstruction:

(cid:104)gi, gj(cid:105)P (x) =

(cid:90)

D

gi(xi)P (x)gj(xj)T dx,

(15)

where xi, xj are the outputs of the former layer, respec-
tively. To unify the inputs, we further expand Eq. (15) as
follows:

(cid:104)gi, gj(cid:105)P (x)

(cid:90)

=

D

gi(gi−1(. . . g1(x)))P (x)gj(gj−1(. . . g1(x)))T dx.

(16)

To facilitate the mathematical analysis and the proof, we set
fi(x) = gi(gi−1(. . . g1(x))), fj(x) = gj(gj−1(. . . g1(x)))
and the composition function f (x) represents the former
i layers of the network. Therefore, the uniﬁed input re-
stricts us to deconstruct the network according to the former
i layers only. Moreover, two deconstruction approaches are
equivalent in the context of uniﬁed input. Having deter-
mined the network deconstruction, we next explain that how
to construct a linear programming problem associate to the
network deconstruction.

8.2. Linear Programming

According to Eq. (17), we calculate the importance fac-

tor θi corresponding to the former i layers of the network,

θi = e−βγi.

(17)

The stronger orthogonality of the former i layers implies
that the importance factor θi is larger. Therefore, we take
θi as the weight coefﬁcient for the bit-width assignment of
the former i layers, which means that the important layers
are assigned a larger bit-width to retain more information.
However, this will lead to different scales of weight coefﬁ-
cient for different layers due to accumulation, e.g. the range
of weight coefﬁcients for the ﬁrst layer and the last layer
is [Le−β(L−1), L] and [e−β(L−1), 1], respectively. We thus
rescale the i-th layer by multiplying the factor 1/(L−i+1).

Figure 4. The former i layers deconstruction(left) and the i-th
layer deconstruction(right). We omit the BatchNorm layer [20]
and the activation layer for simplicity while X1 = X, X2 =
g1(X1), . . . , XL = gL−1(XL−1).

Figure 5. Compositions of linear programming objective function.
bi represents the bit-width variable of the i-th layer. θi represents
the importance factor of the former i layers.

Finally, we sum up all the terms to obtain the objective func-
tion of the linear programming in Eq. (18). More details are
intuitively illustrated in Fig. 5.

Objective: max

b





L
(cid:88)

i=1

bi
L − i + 1



θj

 ,

L
(cid:88)

j=i

Constraints:

L
(cid:88)

i

M (bi) ≤ T .

(18)

12

32 bit32 bit32 bit32 bit𝑓1(𝑋)𝑓2(𝑋)𝑓𝐿−1(𝑋)𝑓𝐿(𝑋)𝑋32 bit32 bit32 bit32 bit𝑔1(𝑋1)𝑔2(𝑋2)𝑔𝐿−1(𝑋𝐿−1)𝑔𝐿(𝑋𝐿)𝑋𝜃1𝑏1𝜃2𝑏1+𝑏2𝜃𝐿𝑏1+𝑏2+⋯+𝑏𝐿−1+𝑏𝐿32 bit32 bit32 bit32 bit𝑋𝜃𝐿−1𝑏1+𝑏2+⋯+𝑏𝐿−1×1𝐿×1𝐿−1×12Figure 6. Layerwise bit conﬁguration of MobileNetV2, which is quantized to 0.9Mb

able as follows:

(cid:20)

lim
β→+∞
(cid:20)

lim
β→0+

e−β(L−1), 1

e−β(L−1), 1

(cid:21)

(cid:21)

= [0, 1] ,

= {1} .

(19)

From Eq. (19), when β increases, the range of impor-
tance factor θ becomes larger. On the contrary, when β ap-
proaches zero, the range of θ is greatly compressed, so the
variance of importance factors of different layers is greatly
reduced. We explore the relationship between accuracy and
β on MobileNetV2. We quantize MobileNetV2 to 0.9Mb
and ﬁx all activation value at 8 bit. As shown in Fig. 8,
when β increases, the accuracy gradually increases as the
variance of importance factors of different layers becomes
larger, and stabilizes when β is large enough. According
to our experiments, the low variance of importance factor
θ may lead the linear programming algorithm to choose an
aggressive bit conﬁguration resulting in sub-optimal accu-
racy.

11. Bit Conﬁgurations

We also give the layer-wise bit conﬁgurations of ResNet-
18 [18] and MobileNetV2 [19] on PTQ. As shown in Figs. 6
and 7, we observe that MobileNetV2 allocates more bits to
the depthwise convolution, and allocates fewer bits to the
In other words, the 3 × 3 convo-
pointwise convolution.
lutional layers are more orthogonal than the 1 × 1 convo-
lutional layers, which somehow explains the fact that net-
works containing a large number of 1 × 1 convolutional
layers are difﬁcult to quantize. Therefore, this phenomenon
can potentially be instructive for low precision quantiza-
tion and binarization of ResNet-50, MobileNetV2 and even
Transformer [13]. Besides, the bit-width of each layer on
ResNet-18 and MobileNetV2 decreases as the depth of the
layer increases.

Figure 7. Layerwise bit conﬁguration of ResNet-18, which is
quantized to 3.5Mb

Figure 8. The relationship between accuracy and hyperparameter
β on MobileNetV2, which is quantized to 0.9Mb.

9. ORM Matrix

We present the ORM matrices of ResNet-18, ResNet-50
and MobileNetV2 for different samples N in Fig. 9. We
can obtain the following conclusions: (a) the orthogonality
between the former i and the former j layers generally be-
comes stronger as the absolute difference |i − j| increases.
(b) the orthogonality discrepancy of the network which out-
put activation lies within the same block is tiny. (c) as the
samples N increases, the approximation of expectation be-
comes more accurate and the orthogonality discrepancy of
different layers becomes more and more obvious.

10. Importance Variance

According to Sec. 8.2, the hyperparameter β may also
affect the range of the weight coefﬁcients of bit-width vari-

13

1234#bitsLayer#weight bits (depthwise)#weight bits (pointwise)110203040501234#bitsLayer151015  H    H    H    H                     $ F F X U D F \                                             Figure 9. The ORM matrices of ResNet-18, ResNet-50 and MobileNetV2 with different samples N .

14

            / D \ H U            / D \ H U          / D \ H U          6 D P S O H V N                                                                                                                                                                             / D \ H U                                  / D \ H U                   6 D P S O H V N                                                                                                                                                                                                              / D \ H U                   6 D P S O H V N                                                                                                                                                                                                              / D \ H U                   6 D P S O H V N                                                                                                                                                                                                             / D \ H U                   6 D P S O H V N                                                                                                                                                                                                             / D \ H U                   6 D P S O H V N                                                                                                                                                                                                             / D \ H U  D   5 H V Q H W                      6 D P S O H V N                                                                                                                                                                 E   5 H V Q H W                           F   0 R E L O H Q H W 9                   