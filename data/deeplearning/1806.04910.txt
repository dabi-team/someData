Bilevel Programming for Hyperparameter Optimization and Meta-Learning

8
1
0
2

l
u
J

3

]
L
M

.
t
a
t
s
[

2
v
0
1
9
4
0
.
6
0
8
1
:
v
i
X
r
a

Luca Franceschi 1 2 Paolo Frasconi 3 Saverio Salzo 1 Riccardo Grazzi 1 Massimiliano Pontil 1 2

Abstract

We introduce a framework based on bilevel pro-
gramming that uniﬁes gradient-based hyperparam-
eter optimization and meta-learning. We show
that an approximate version of the bilevel prob-
lem can be solved by taking into explicit account
the optimization dynamics for the inner objective.
Depending on the speciﬁc setting, the outer vari-
ables take either the meaning of hyperparameters
in a supervised learning problem or parameters of
a meta-learner. We provide sufﬁcient conditions
under which solutions of the approximate prob-
lem converge to those of the exact problem. We
instantiate our approach for meta-learning in the
case of deep learning where representation lay-
ers are treated as hyperparameters shared across
a set of training episodes. In experiments, we
conﬁrm our theoretical ﬁndings, present encour-
aging results for few-shot learning and contrast
the bilevel approach against classical approaches
for learning-to-learn.

1. Introduction

While in standard supervised learning problems we seek the
best hypothesis in a given space and with a given learning
algorithm, in hyperparameter optimization (HO) and meta-
learning (ML) we seek a conﬁguration so that the optimized
learning algorithm will produce a model that generalizes
well to new data. The search space in ML often incorpo-
rates choices associated with the hypothesis space and the
features of the learning algorithm itself (e.g., how optimiza-
tion of the training loss is performed). Under this common
perspective, both HO and ML essentially boil down to nest-
ing two search problems: at the inner level we seek a good

1Computational Statistics and Machine Learning, Istituto
Italy 2Department of Com-
Italiano di Tecnologia, Genoa,
puter Science, University College London, London, UK
3Department of Information Engineering, Universit`a degli Studi
di Firenze, Florence, Italy. Correspondence to: Luca Franceschi
<luca.franceschi@iit.it>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

hypothesis (as in standard supervised learning) while at
the outer level we seek a good conﬁguration (including a
good hypothesis space) where the inner search takes place.
Surprisingly, the literature on ML has little overlap with
the literature on HO and in this paper we present a uniﬁed
framework encompassing both of them.

Classic approaches to HO (see e.g. Hutter et al., 2015,
for a survey) have been only able to manage a relatively
small number of hyperparameters, from a few dozens using
random search (Bergstra and Bengio, 2012) to a few hun-
dreds using Bayesian or model-based approaches (Bergstra
et al., 2013; Snoek et al., 2012). Recent gradient-based tech-
niques for HO, however, have signiﬁcantly increased the
number of hyperparameters that can be optimized (Domke,
2012; Maclaurin et al., 2015; Pedregosa, 2016; Franceschi
et al., 2017) and it is now possible to tune as hyperparame-
ters entire weight vectors associated with a neural network
layer. In this way, it becomes feasible to design models
that possibly have more hyperparameters than parameters.
Such an approach is well suited for ML, since parameters
are learned from a small dataset, whereas hyperparameters
leverage multiple available datasets.

HO and ML only differ substantially in terms of the experi-
mental settings in which they are evaluated. While in HO the
available data is associated with a single task and split into a
training set (used to tune the parameters) and a validation set
(used to tune the hyperparameters), in ML we are often in-
terested in the so-called few-shot learning setting where data
comes in the form of short episodes (small datasets with few
examples per class) sampled from a common probability
distribution over supervised tasks.

Early work on ML dates back at least to the 1990’s (Schmid-
huber, 1992; Baxter, 1995; Thrun and Pratt, 1998) but this
research area has received considerable attention in the last
few years, mainly driven by the need in real-life and indus-
trial scenarios for learning quickly a vast multitude of tasks.
These tasks, or episodes, may appear and evolve continu-
ously over time and may only contain few examples (Lake
et al., 2017). Different strategies have emerged to tackle ML.
Although they do overlap in some aspects, it is possible to
identify at least four of them. The metric strategy attempts
to use training episodes to construct embeddings such that
examples of the same class are mapped into similar repre-

 
 
 
 
 
 
Bilevel Programming for Hyperparameter Optimization and Meta-Learning

Table 1. Links and naming conventions among different ﬁelds.

Bilevel
programming

Hyperparameter
optimization

Meta-learning

Inner variables

Parameters

Outer variables

Hyperparameters

Inner objective

Training error

Outer objective

Validation error

Parameters of
Ground models
Parameters of
Meta-learner
Training errors
on tasks (Eq. 3)
Meta-training
error (Eq. 4)

sentations. It has been instantiated in several variants that
involve non-parametric (or instance-based) predictors (Koch
et al., 2015; Vinyals et al., 2016; Snell et al., 2017). In the re-
lated memorization strategy, the meta-learner learns to store
and retrieve data points representations in memory. It can
be implemented either using recurrent networks (Santoro
et al., 2016) or temporal convolutions (Mishra et al., 2018).
The use of an attention mechanism (Vaswani et al., 2017) is
crucial both in (Vinyals et al., 2016) and in (Mishra et al.,
2018). The initialization strategy (Ravi and Larochelle,
2017; Finn et al., 2017) uses training episodes to infer a good
initial value for the model’s parameters so that new tasks can
be learned quickly by ﬁne tuning. The optimization strat-
egy (Andrychowicz et al., 2016; Ravi and Larochelle, 2017;
Wichrowska et al., 2017) forges an optimization algorithm
that will ﬁnd it easier to learn on novel related tasks.

A main contribution of this paper is a uniﬁed view of HO
and ML within the natural mathematical framework of
bilevel programming, where an outer optimization problem
is solved subject to the optimality of an inner optimization
problem. In HO the outer problem involves hyperparame-
ters while the inner problem is usually the minimization of
an empirical loss. In ML the outer problem could involve
a shared representation among tasks while the inner prob-
lem could concern classiﬁers for individual tasks. Bilevel
programming (Bard, 2013) has been suggested before in ma-
chine learning in the context of kernel methods and support
vector machines (Keerthi et al., 2007; Kunapuli et al., 2008),
multitask learning (Flamary et al., 2014), and more recently
HO (Pedregosa, 2016), but never in the context of ML. The
resulting framework outlined in Sec. 2 encompasses some
existing approaches to ML, in particular those based on the
initialization and the optimization strategies.

A technical difﬁculty arises when the solution to the inner
problem cannot be written analytically (for example this hap-
pens when using the log-loss for training neural networks)
and one needs to resort to iterative optimization approaches.
As a second contribution, we provide in Sec. 3 sufﬁcient
conditions that guarantee good approximation properties.
We observe that these conditions are reasonable and apply

to concrete problems relevant to applications.

In Sec. 4, by taking inspiration on early work on representa-
tion learning in the context of multi-task and meta-learning
(Baxter, 1995; Caruana, 1998), we instantiate the framework
for ML in a simple way treating the weights of the last layer
of a neural network as the inner variables and the remaining
weights, which parametrize the representation mapping, as
the outer variables. As shown in Sec. 5, the resulting ML
algorithm performs well in practice, outperforming most of
the existing strategies on MiniImagenet.

2. A bilevel optimization framework

In this paper, we consider bilevel optimization problems
(see e.g. Colson et al., 2007) of the form

min{f (λ) : λ ∈ Λ},

(1)

where function f : Λ → R is deﬁned at λ ∈ Λ as

f (λ) = inf{E(wλ, λ) : wλ ∈ arg min
u∈Rd

Lλ(u)}.

(2)

We call E : Rd × Λ → R the outer objective and, for every
λ ∈ Λ, we call Lλ : Rd → R the inner objective. Note that
{Lλ : λ ∈ Λ} is a class of objective functions parameterized
by λ. Speciﬁc instances of this problem include HO and ML,
which we discuss next. Table 1 outlines the links among
bilevel programming, HO and ML.

2.1. Hyperparameter Optimization

In the context of hyperparameter optimization, we are in-
terested in minimizing the validation error of a model
gw : X → Y parameterized by a vector w, with respect
to a vector of hyperparameters λ. For example, we may
consider representation or regularization hyperparameters
that control the hypothesis space or penalties, respectively.
In this setting, a prototypical choice for the inner objective
is the regularized empirical error

Lλ(w) =

(cid:88)

(cid:96)(gw(x), y) + Ωλ(w),

(x,y)∈Dtr

where Dtr = {(xi, yi)}n
i=1 is a set of input/output points, (cid:96)
is a prescribed loss function, and Ωλ a regularizer param-
eterized by λ. The outer objective represents a proxy for
the generalization error of gw, and it may be given by the
average loss on a validation set Dval

E(w, λ) =

(cid:88)

(cid:96)(gw(x), y).

(x,y)∈Dval

or, in more generality, by a cross-validation error, as detailed
in Appendix B. Note that in this setting, the outer objective
E does not depend explicitly on the hyperparameters λ,

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

inner and outer losses for task j use different train/validation
splits of the corresponding dataset Dj. Furthermore, unlike
in HO, in ML the ﬁnal goal is to ﬁnd a good λ and the wj
are now instrumental.

The cartoon in Figure 1 illustrates ML as a bilevel problem.
The parameter λ indexes an hypothesis space within which
the inner objective is minimized. A particular example, de-
tailed in Sec. 4, is to choose the model gw,λ = (cid:104)w, hλ(x)(cid:105),
in which case λ parameterizes a feature mapping. Yet an-
other choice would be to consider gwj ,λ(x) = (cid:104)w + λ, x(cid:105),
in which case λ represents a common model around which
task speciﬁc models are to be found (see e.g. Evgeniou et al.,
2005; Finn et al., 2017; Khosla et al., 2012; Kuzborskij et al.,
2013, and reference therein).

2.3. Gradient-Based Approach

We now discuss a general approach to solve Problem (1)-(2)
when the hyperparameter vector λ is real-valued. To sim-
plify our discussion let us assume that the inner objective
has a unique minimizer wλ. Even in this simpliﬁed scenario,
Problem (1)-(2) remains challenging to solve. Indeed, in
general there is no closed form expression wλ, so it is not
possible to directly optimize the outer objective function.
While a possible strategy (implicit differentiation) is to ap-
ply the implicit function theorem to ∇Lλ = 0 (Pedregosa,
2016; Koh and Liang, 2017; Beirami et al., 2017), another
compelling approach is to replace the inner problem with a
dynamical system. This point, discussed in (Domke, 2012;
Maclaurin et al., 2015; Franceschi et al., 2017), is developed
further in this paper.

Speciﬁcally, we let [T ] = {1, . . . , T } where T is a pre-
scribed positive integer and consider the following approxi-
mation of Problem (1)-(2)

min
λ

fT (λ) = E(wT,λ, λ),

(5)

where E is a smooth scalar function, and2

w0,λ = Φ0(λ), wt,λ = Φt(wt−1,λ, λ), t ∈ [T ],

(6)

with Φ0 : Rm → Rd a smooth initialization mapping and,
for every t ∈ [T ], Φt : Rd × Rm → Rd a smooth mapping
that represents the operation performed by the t-th step of
an optimization algorithm. For example, the optimization
dynamics could be gradient descent: Φt(wt, λ) = wt −
ηt∇Lλ(·) where (ηt)t∈[T ] is a sequence of steps sizes.

The approximation of the bilevel problem (1)-(2) by the
procedure (5)-(6) raises the issue of the quality of this ap-
proximation and we return to this issue in the next section.

2In general, the algorithm used to minimize the inner objective
may involve auxiliary variables, e.g., velocities when using gradi-
ent descent with momentum, so w should be intended as a larger
vector containing both model parameters and auxiliary variables.

Figure 1. Each blue line represents the average training error when
varying w in gw,λ and the corresponding inner minimizer is shown
as a blue dot. The validation error evaluated at each minimizer
yields the black curve representing the outer objective f (λ), whose
minimizer is shown as a red dot.

since in HO λ is instrumental in ﬁnding a good model gw,
which is our ﬁnal goal. As a more speciﬁc example, consider
linear models, gw(x) = (cid:104)w, x(cid:105), let (cid:96) be the square loss and
let Ωλ(w) = λ(cid:107)w(cid:107)2, in which case the inner objective is
ridge regression (Tikhonov regularization) and the bilevel
problem optimizes over the regularization parameter the
validation error of ridge regression.

2.2. Meta-Learning

val}N

tr ∪ Dj

In meta-learning (ML) the inner and outer objectives are
computed by averaging a training and a validation error
over multiple tasks, respectively. The goal is to produce
a learning algorithm that will work well on novel tasks1.
For this purpose, we have available a meta-training set
D = {Dj = Dj
j=1, which is a collection of
datasets, sampled from a meta-distribution P. Each dataset
Dj = {(xj
i , yj
i ) ∈ X × Y j is linked to
a speciﬁc task. Note that the output space is task depen-
dent (e.g. a multi-class classiﬁcation problem with variable
number of classes). The model for each task is a function
gwj ,λ : X → Y j, identiﬁed by a parameter vectors wj and
hyperparameters λ. A key point here is that λ is shared
between the tasks. With this notation the inner and outer
objectives are

i=1 with (xj

i )}nj

i , yj

Lλ(w) =

E(w, λ) =

N
(cid:88)

Lj(wj, λ, Dj

tr),

j=1

N
(cid:88)

j=1

Lj(wj, λ, Dj

val)

(3)

(4)

respectively. The loss Lj(wj, λ, S) represents the empirical
error of the pair (wj, λ) on a set of examples S. Note that the

1The ML problem is also related to multitask learning, however

in ML the goal is to extrapolate from the given tasks.

wErrorTrain errorsValidation errorBilevel Programming for Hyperparameter Optimization and Meta-Learning

However, it also suggests to consider the inner dynamics
as a form of approximate empirical error minimization (e.g.
early stopping) which is valid in its own right. From this
perspective – conversely to the implicit differentiation strat-
egy – it is possible to include among the components of λ
variables which are associated with the optimization algo-
rithm itself. For example, λ may include the step sizes or
momentum factors if the dynamics Φt in Eq. (6) is gradient
descent with momentum; in (Andrychowicz et al., 2016;
Wichrowska et al., 2017) the mapping Φt is implemented as
a recurrent neural network, while (Finn et al., 2017) focus
on the initialization mapping by letting Φ0(λ) = λ.

A major advantage of this reformulation is that it makes it
possible to compute efﬁciently the gradient of fT , which we
call hypergradient, either in time or in memory (Maclaurin
et al., 2015; Franceschi et al., 2017), by making use of re-
verse or forward mode algorithmic differentiation (Griewank
and Walther, 2008; Baydin et al., 2017). This allows us to
optimize a number of hyperparameters of the same order of
that of parameters, a situation which arise in ML.

3. Exact and Approximate Bilevel

Programming

In this section, we provide results about the existence of
solutions of Problem (1)-(2) and the approximation proper-
ties of Procedure (5)-(6) with respect to the original bilevel
problem. Proofs of these results are provided in the supple-
mentary material.

Procedure (5)-(6), though related to the bilevel problem
(1)-(2), may not be, in general, a good approximation of
it. Indeed, making the assumptions (which sound perfectly
reasonable) that, for every λ ∈ Λ, wT,λ → wλ for some
wλ ∈ arg max Lλ, and that E(·, λ) is continuous, one can
only assert that limT →∞ fT (λ) = E(wλ, λ) ≥ f (λ). This
is because the optimization dynamics converge to some
minimizer of the inner objective Lλ, but not necessarily
to the one that also minimizes the function E. This is
illustrated in Figure 2. The situation is, however, different
if the inner problem admits a unique minimizer for every
λ ∈ Λ. Indeed in this case, it is possible to show that the
set of minimizers of the approximate problems converge,
as T → +∞ and in an appropriate sense, to the set of
minimizers of the bilevel problem. More precisely, we
make the following assumptions:

Figure 2. In this cartoon,
λ , w(2)
{w(1)
converge to w(1)

for a ﬁxed λ, argmin Lλ =
λ }; the iterates of an optimization mapping Φ could
λ , λ) > E(w(2)

λ with E(w(1)

λ , λ).

Then, problem (1)-(2) becomes

min
λ∈Λ

f (λ) = E(wλ, λ),

wλ = argminuLλ(u).

(7)

Under the above assumptions, in the following we give
results about the existence of solutions of problem (7) and
the (variational) convergence of the approximate problems
(5)-(6) towards problem (7) — relating the minima as well
as the set of minimizers. In this respect we note that, since
both f and fT are nonconvex, argmin fT and argmin f are
in general nonsingleton, so an appropriate deﬁnition of set
convergence is required.

Theorem 3.1 (Existence). Under Assumptions (i)-(iv) prob-
lem (7) admits solutions.
Proof See Appendix A.

The result below follows from general facts on the stabil-
ity of minimizers in optimization problems (Dontchev and
Zolezzi, 1993).

Theorem 3.2 (Convergence).
tions (i)-(iv), suppose that:

In addition to Assump-

(v) E(·, λ) is uniformly Lipschitz continuous;
(vi) The iterates (wT,λ)T ∈N converge uniformly to wλ on

Λ as T → +∞.

Then

(a) inf fT → inf f ,

(b) argmin fT → argmin f , meaning that, for every
(λT )T ∈N such that λT ∈ argmin fT , we have that:

- (λT )T ∈N admits a convergent subsequence;
- for every subsequence (λKT )T ∈N such that

λKT → ¯λ, we have ¯λ ∈ argmin f .

(i) Λ is a compact subset of Rm;

Proof See Appendix A.

(ii) E : Rd × Λ → R is jointly continuous;

(iii) the map (w, λ) (cid:55)→ Lλ(w) is jointly continuous and

such that arg min Lλ is a singleton for every λ ∈ Λ;

(iv) wλ = arg min Lλ remains bounded as λ varies in Λ.

We stress that assumptions (i)-(vi) are very natural and sat-
isﬁed by many problems of practical interests. Thus, the
above results provide full theoretical justiﬁcation to the pro-
posed approximate procedure (5)-(6). The following remark
discusses assumption (vi), while the subsequent example
will be relevant to the experiments in Sec. 5.

wLλE (·,λ)λλ(1)(2)wwBilevel Programming for Hyperparameter Optimization and Meta-Learning

Algorithm 1. Reverse-HG for Hyper-representation

Input: λ, current values of the hyperparameter, T num-
ber of iteration of GD, η ground learning rate, B mini-
batch of episodes from D
Output: Gradient of meta-training error w.r.t. λ on B
for j = 1 to |B| do
0 = 0

wj
for t = 1 to T do

wj

t ← wt−1 − η∇wLj(wj

t−1, λ, Dj
tr)

αj
T ← ∇wLj(wj
pj ← ∇λLj(wj
for t = T − 1 downto 0 do

T , λ, Dval)
T , λ, Dval)

pj ← pj − αj
(cid:104)
αj
t ← αj
j pj

t+1

return (cid:80)

t+1η∇λ∇wLj(wj
I − η∇w∇wLj(wj

t , λ, Dj
tr)
t , λ, Dj
tr)

(cid:105)

Remark 3.3. If Lλ is strongly convex, then many gradient-
based algorithms (e.g., standard and accelerated gradient de-
scent) yield linear convergence of the iterates wT,λ’s. More-
over, in such cases, the rate of linear convergence is of type
(νλ − µλ)/(νλ + µλ), where νλ and µλ are the Lipschitz
constant of the gradient and the modulus of strong convexity
of Lλ respectively. So, this rate can be uniformly bounded
from above by ρ ∈ ]0, 1[, provided that supλ∈Λ νλ < +∞
and inf λ∈Λ µλ > 0. Thus, in these cases wT,λ converges
uniformly to wλ on Λ (at a linear rate).
Example 3.4. Let us consider the following form of the
inner objective:

LH (w) = (cid:107)y − XHw(cid:107)2 + ρ(cid:107)w(cid:107)2,

(8)

where ρ > 0 is a ﬁxed regularization parameter and
H ∈ Rd×d is the hyperparameter, representing a linear fea-
ture map. LH is strongly convex, with modulus µ = ρ > 0
(independent on the hyperparameter H), and Lipschitz
smooth with constant νH = 2(cid:107)(XH)(cid:62)XH + ρI(cid:107), which is
bounded from above, if H ranges in a bounded set of square
matrices. In this case assumptions (i)-(vi) are satisﬁed.

4. Learning Hyper-Representations

In this section, we instantiate the bilevel programming ap-
proach for ML outlined in Sec. 2.2 in the case of deep learn-
ing where representation layers are shared across episodes.
Finding good data representations is a centerpiece in ma-
chine learning. Classical approaches (Baxter, 1995; Caru-
ana, 1998) learn both the weights of the representation map-
ping and those of the ground classiﬁers jointly on the same
data. Here we follow the bilevel approach and split each
dataset/episode in training and validation sets.

Our method involves the learning of a cross-task intermedi-

ate representation hλ : X → Rk (parametrized by a vector
λ) on top of which task speciﬁc models gj : Rk → Y j
(parametrized by vectors wj) are trained. The ﬁnal ground
model for task j is thus given by gj ◦ h. To ﬁnd λ, we solve
Problem (1)-(2) with inner and outer objectives as in Eqs.
(3) and (4), respectively. Since, in general, this problem
cannot be solved exactly, we instantiate the approximation
scheme in Eqs. (5)-(6) as follows:

min
λ

fT (λ) =

N
(cid:88)

j=1

Lj(wj

T , λ, Dj

val)

(9)

wj

t−1−η∇wLj(wj

t−1, λ, Dj
t = wj
tr), t, j ∈ [T ], [N ]. (10)
Starting from an initial value, the weights of the task-speciﬁc
models are learned by T iterations of gradient descent.
The gradient of fT can be computed efﬁciently in time
by making use of an extended reverse-hypergradient pro-
cedure (Franceschi et al., 2017) which we present in Al-
gorithm 1. Since, in general, the number of episodes in a
meta-training set is large, we compute a stochastic approx-
imation of the gradient of fT by sampling a mini-batch of
episodes. At test time, given a new episode ¯D, the represen-
tation h is kept ﬁxed, and all the examples in ¯D are used to
tune the weights ¯w of the episode-speciﬁc model ¯g.

Like other initialization and optimization strategies for
ML, our method does not require lookups in a support
set as the memorization and metric strategies do (Santoro
et al., 2016; Vinyals et al., 2016; Mishra et al., 2018). Un-
like (Andrychowicz et al., 2016; Ravi and Larochelle, 2017)
we do not tune the optimization algorithm, which in our case
is plain empirical loss minimization by gradient descent,
and rather focus on the hypothesis space. Unlike (Finn
et al., 2017), that aims at maximizing sensitivity of new task
losses to the model parameters, we aim at maximizing the
generalization to novel examples during training episodes,
with respect to λ. Our assumptions about the structure of
the model are slightly stronger than in (Finn et al., 2017)
but still mild, namely that some (hyper)parameters deﬁne
the representation and the remaining parameters deﬁne the
classiﬁcation function. In (Munkhdalai and Yu, 2017) the
meta-knowledge is distributed among fast and slow weights
and an external memory; our approach is more direct, since
the meta-knowledge is solely distilled by λ. A further advan-
tage of our method is that, if the episode-speciﬁc models are
linear (e.g. logistic regressors) and each loss Lj is strongly
convex in w, the theoretical guarantees of Theorem 3.2 ap-
ply (see Remark 3.3). These assumptions are satisﬁed in the
experiments reported in the next section.

5. Experiments

The aim of the following experiments is threefold. First,
we investigate the impact of the number of iterations of the
optimization dynamics on the quality of the solution on a

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

Figure 3. Optimization of the outer objectives f and fT for exact
and approximate problems. The optimization of H is performed
with gradient descent with momentum, with same initialization,
step size and momentum factor for each run.

Figure 4. Accuracy on Dtest of exact and approximated solutions
during optimization of H. Training and validation accuracies
reach almost 100% already for T = 4 and after few hundred
hyperiterations, and therefore are not reported.

simple multiclass classiﬁcation problem. Second, we test
our hyper-representation method in the context of few-shot
learning on two benchmark datasets. Finally, we constrast
the bilevel ML approach against classical approaches to
learn shared representations 3.

5.1. The Effect of T

Motivated by the theoretical ﬁndings of Sec. 3, we empir-
ically investigate how solving the inner problem approxi-
mately (i.e. using small T ) affects convergence, generaliza-
tion performances, and running time. We focus in particular
on the linear feature map described in Example 3.4, which
allows us to compare the approximated solution against the
closed-form analytical solution given by

wH = [(XH)T XH + ρI]−1(XH)T Y.

In this setting, the bilevel problem reduces to a (non-convex)
optimization problem in H.

We use a subset of 100 classes extracted from Omniglot
dataset (Lake et al., 2017) to construct a HO problem aimed
at tuning H. A training set Dtr and a validation set Dval,
each consisting of three randomly drawn examples per class,
were sampled to form the HO problem. A third set Dtest,
consisting of ﬁfteen examples per class, was used for testing.
Instead of using raw images as input, we employ feature
vectors x ∈ R256 computed by the convolutional network
trained on one-shot ﬁve-ways ML setting as described in
Sec. 5.2.

For the approximate problems we compute the hypergra-
dient using Algorithm 1, where it is intended that B =
{(Dtr, Dval)}. Figure 3 shows the values of functions f and
fT (see Eqs. (1) and (5), respectively) during the optimiza-
tion of H. As T increases, the solution of the approximate

Table 2. Execution times on a NVidia Tesla M40 GPU.

T
Time (sec)

1
60

4
119

16
356

64
1344

256
5532

Exact
320

problem approaches the true bilevel solution. However, per-
forming a small number of gradient descent steps for solving
the inner problem acts as implicit regularizer. As it is evi-
dent from Figure 4, the generalization error is better when
T is smaller than the value yielding the best approximation
of the inner solution. This is to be expected since, in this
setting, the dimensions of parameters and hyperparameters
are of the same order, leading to a concrete possibility of
overﬁtting the outer objective (validation error). An appro-
priate, problem dependent, choice of T may help avoiding
this issue (see also Appendix C). As T increases, the number
of hyperiterations required to reach the maximum test accu-
racy decreases, further suggesting that there is an interplay
between the number of iterations used to solve the inner and
the outer objective. Finally, the running time of Algorithm
1, is linear in T and the size of w and independent of the
size of H (see also Table 2), making it even more appealing
to reduce the number of iterations.

5.2. Few-shot Learning

We new turn our attention to learning-to-learn, precisely to
few-shot supervised learning, implementing the ML strategy
outlined in Sec. 4 on two different benchmark datasets:

• OMNIGLOT (Lake et al., 2015), a dataset that contains
examples of 1623 different handwritten characters from 50
alphabets. We downsample the images to 28 × 28.

• MINIIMAGENET (Vinyals et al., 2016), a subset of Ima-
geNet (Deng et al., 2009), that contains 60000 downsampled
images from 100 different classes.

3The code for reproducing the experiments, based on the
package FAR-HO (https://bit.ly/far-ho), is available
at https://bit.ly/hyper-repr

Following the experimental protocol used in a number of re-
cent works, we build a meta-training set D, from which we
sample datasets to solve Problem (9)-(10), a meta-validation

010002000300040005000Hyperiterations6080100120140160180SquarederrorExactsolutonT=1T=4T=16T=64T=256010002000300040005000Hyperiterations858687888990919293Accuracy%ExactsolutonT=1T=4T=16T=64T=256Bilevel Programming for Hyperparameter Optimization and Meta-Learning

tr ∪ Dj

set V for tuning ML hyperparameters, and ﬁnally a meta-
test set T which is used to estimate accuracy. Operationally,
each meta-dataset consists of a pool of samples belonging to
different (non-overlapping between separate meta-dataset)
classes, which can be combined to form ground classiﬁca-
tion datasets Dj = Dj
val with 5 or 20 classes (for
Omniglot). The Dj
tr’s contain 1 or 5 examples per class
which are used to ﬁt wj (see Eq. 10). The Dj
val’s, contain-
ing 15 examples per class, is used either to compute fT (λ)
(see Eq. (9)) and its (stochastic) gradient if Dj ∈ D or to
provide a generalization score if Dj comes from either V or
T . For MiniImagenet we use the same split and images pro-
posed in (Ravi and Larochelle, 2017), while for Omniglot
we use the protocol deﬁned by (Santoro et al., 2016).

As ground classiﬁers we use multinomial logistic regressors
and as task losses (cid:96)j we employ cross-entropy. The inner
problems, being strongly convex, admit unique minimizers,
yet require numerical computation of the solutions. We ini-
tialize ground models parameters wj to 0 and, according to
the observation in Sec. 5.1, we perform T gradient descent
steps, where T is treated as a ML hyperparameter that has to
be validated. Figure 6 shows an example of meta-validation
of T for one-shot learning on MiniImagenet. We compute a
stochastic approximation of ∇fT (λ) with Algorithm 1 and
use Adam with decaying learning rate to optimize λ.

Regarding the speciﬁc implementation of the representation
mapping h, we employ for Omniglot a four-layers convo-
lutional neural network with strided convolutions and 64
ﬁlters per layer as in (Vinyals et al., 2016) and other suc-
cessive works. For MiniImagenet we tried two different
architectures:

• C4L, a four-layers convolutional neural network with max-
pooling and 32 ﬁlters per layer;

• RN: a residual network (He et al., 2016) built of four
residual blocks followed by two convolutional layers.

The ﬁrst network architecture has been proposed in (Ravi
and Larochelle, 2017) and then used in (Finn et al., 2017),
while a similar residual network architecture has been em-
ployed in a more recent work (Mishra et al., 2018). Further
details on the architectures of h, as well as other ML hyper-
parameters, are speciﬁed in the supplementary material. We
report our results, using RN for MiniImagenet, in Table 3,
alongside scores from various recently proposed methods
for comparison.

The proposed method achieves competitive results highlight-
ing the relative importance of learning a task independent
representation, on the top of which logistic classiﬁers trained
with very few samples generalize well. Moreover, utilizing
more expressive models such as residual network as repre-
sentation mappings, is beneﬁcial for our proposed strategy
and, unlike other methods, does not result in overﬁtting

of the outer objective, as reported in (Mishra et al., 2018).
Indeed, compared to C4L, RN achieves a relative improve-
ment of 6.5% on one-shot and 4.2% on ﬁve-shot. Figure 5
provides a visual example of the goodness of the learned rep-
resentation, showing that MiniImagenet examples (the ﬁrst
from meta-training, the second from the meta-testing sets)
from similar classes (different dog breeds) are mapped near
each other by h and, conversely, samples from dissimilar
classes are mapped afar.

Figure 5. After sampling two datasets D ∈ D and D(cid:48) ∈ T , we
show on the top the two images x ∈ D, x(cid:48) ∈ D(cid:48) that minimize
||hλ(x) − hλ(x(cid:48))|| and on the bottom those that maximize it. In
between each of the two couples we compare a random subset of
components of hλ(x) (blue) and hλ(x(cid:48)) (green).

Figure 6. Meta-validation of the number of gradient descent steps
(T ) of the ground models for MiniImagenet using the RN repre-
sentation. Early stopping on the accuracy on meta-validation set
during meta-training resulted in halting the optimization of λ after
42k, 40k, 22k, and 15k hyperiterations for T equal to 3, 5, 8 and
12 respectively; in line with our observation in Sec. 5.1.

5.3. On Variants of Representation Learning Methods

In this section, we show the beneﬁts of learning a represen-
tation within the proposed bilevel framework compared to
other possible approaches that involve an explicit factoriza-
tion of a classiﬁer as gj ◦ h. The representation mapping h
is either pretrained or learned with different meta-learning
algorithms. We focus on the problem of one-shot learning
on MiniImagenet and we use C4L as architecture for the
representation mapping. In all the experiments the ground
models gj are multinomial logistic regressor as in Sec. 5.2,
tuned with 5 steps of gradient descent. We ran the following
experiments:
• Multiclass: the mapping h : X → R64 is given by the

3581266687072747678AccuracyonDtr358124950515253AccuracyonDtestAccuracyonDvalBilevel Programming for Hyperparameter Optimization and Meta-Learning

Table 3. Accuracy scores, computed on episodes from T , of various methods on 1-shot and 5-shot classiﬁcation problems on Omniglot
and MiniImagenet. For MiniImagenet 95% conﬁdence intervals are reported. For Hyper-representation the scores are computed over 600
randomly drawn episodes. For other methods we show results as reported by their respective authors.

Method

OMNIGLOT 5 classes OMNIGLOT 20 classes
1-shot

5-shot

1-shot

5-shot

Siamese nets (Koch et al., 2015)
Matching nets (Vinyals et al., 2016)
Neural stat. (Edwards and Storkey, 2016)
Memory mod. (Kaiser et al., 2017)
Meta-LSTM (Ravi and Larochelle, 2017)
MAML (Finn et al., 2017)
Meta-networks (Munkhdalai and Yu, 2017)
Prototypical Net. (Snell et al., 2017)
SNAIL (Mishra et al., 2018)
Hyper-representation

97.3
98.1
98.1
98.4
−
98.7
98.9
98.8
99.1
98.6

98.4
98.9
99.5
99.6
−
99.9
−
99.7
99.8
99.5

88.2
93.8
93.2
95.0
−
95.8
97.0
96.0
97.6
95.5

97.0
98.5
98.1
98.6
−
98.9
−
98.9
99.4
98.4

MINIIMAGENET 5 classes

1-shot

5-shot

−
43.44 ± 0.77
−
−
43.56 ± 0.84
48.70 ± 1.75
49.21 ± 0.96
49.42 ± 0.78
55.71 ± 0.99
50.54 ± 0.85

−
55.31 ± 0.73
−
−
60.60 ± 0.71
63.11 ± 0.92
−
68.20 ± 0.66
68.88 ± 0.92
64.53 ± 0.68

linear outputs before the softmax operation of a network4
pretrained on the totality of examples contained in the train-
ing meta-dataset (600 examples for each of the 64 classes).
In this setting, we found that using the second last layer or
the output after the softmax yields worst results;

• Bilevel-train: we use a bilevel approach but, unlike in Sec.
4, we optimize the parameter vector λ of the representation
mapping by minimizing the loss on the training sets of each
episode. The hypergradient is still computed with Algorithm
1, albeit we set Dj

tr for each training episodes;

val = Dj

• Approx and Approx-train: we consider an approxima-
tion of the hypergradient ∇fT (λ) by disregarding the op-
timization dynamics of the inner objectives (i.e. we set
∇λwj
T = 0). In Approx-train we just use the training sets;
• Classic: as in (Baxter, 1995), we learn h by jointly opti-
mize ˆf (λ, w1, . . . , wN ) = (cid:80)N
tr) and treat
the problem as standard multitask learning, with the ex-
ception that we evaluate ˆf on mini-batches of 4 episodes,
randomly sampled every 5 gradient descent iterations.

j=1 Lj(wj, λ, Dj

In settings where we do not use the validation sets, we let
the training sets of each episode contain 16 examples per
class. Using training episodes with just one example per
class resulted in performances just above random chance.
While the ﬁrst experiment constitutes a standard baseline,
the others have the speciﬁc aim of assessing (i) the impor-
tance of splitting episodes of meta-training set into training
and validation and (ii) the importance of computing the
hypergradient of the approximate bilevel problem with Al-
gorithm 1. The results reported in Table 4 suggest that
both the training/validation splitting and the full computa-
tion of the hypergradient constitute key factors for learning
a good representation in a meta-learning context. On the
other side, using pretrained representations, especially in
a low-dimensional space, turns out to be a rather effective
baseline. One possible explanation is that, in this context,

4The network is similar to C4L but has 64 ﬁlters per layer.

Table 4. Performance of various methods where the representation
is either transfered or learned with variants of hyper-representation
methods. The last raw reports, for comparison, the score obtained
with hyper-representation.

Method

# ﬁlters Accuracy 1-shot

Multiclass
Bilevel-train
Approx
Approx-train
Classic-train
Hyper-representation-C4L

64
32
32
32
32
32

43.02
29.63
41.12
38.80
40.46
47.51

some classes in the training and testing meta-datasets are
rather similar (e.g. various dog breeds) and thus ground
classiﬁers can leverage on very speciﬁc representations.
6. Conclusions

We have shown that both HO and ML can be formulated in
terms of bilevel programming and solved with an iterative
approach. When the inner problem has a unique solution
(e.g. is strongly convex), our theoretical results show that the
iterative approach has convergence guarantees, a result that
is interesting in its own right. In the case of ML, by adapting
classical strategies (Baxter, 1995) to the bilevel framework
with training/validation splitting, we present a method for
learning hyper-representations which is experimentally ef-
fective and supported by our theoretical guarantees.

Our framework encompasses recently proposed methods
for meta-learning, such as learning to optimize, but also
suggests different design patterns for the inner learning
algorithm which could be interesting to explore in future
work. The resulting inner problems may not satisfy the
assumptions of our convergence analysis, raising the need
for further theoretical investigations. An additional future
direction of research is the study of the statistical properties
of bilevel strategies where outer objectives are based on the
generalization ability of the inner model to new (validation)
data. Ideas from (Maurer et al., 2016; Denevi et al., 2018)
may be useful in this direction.

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

References

Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W.,
Pfau, D., Schaul, T., and de Freitas, N. (2016). Learning
to learn by gradient descent by gradient descent. In Ad-
vances in Neural Information Processing Systems (NIPS),
pages 3981–3989.

Bard, J. F. (2013). Practical bilevel optimization: algo-
rithms and applications, volume 30. Springer Science &
Business Media. 01251.

Baxter, J. (1995). Learning internal representations. In Pro-
ceedings of the 8th Annual Conference on Computational
Learning Theory (COLT), pages 311–320. ACM.

Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind,
J. M. (2017). Automatic differentiation in machine learn-
ing: a survey. Journal of Machine Learning Research,
18:153:1–153:43.

Beirami, A., Razaviyayn, M., Shahrampour, S., and Tarokh,
V. (2017). On optimal generalizability in parametric
learning. In Advances in Neural Information Processing
Systems (NIPS), pages 3458–3468.

Bergstra, J. and Bengio, Y. (2012). Random search for hyper-
parameter optimization. Journal of Machine Learning
Research, 13(Feb):281–305.

Bergstra, J., Yamins, D., and Cox, D. D. (2013). Making
a science of model search: Hyperparameter optimiza-
tion in hundreds of dimensions for vision architectures.
In Proceedings of the 30th International Conference on
Machine Learning, (ICML), pages 115–123.

Caruana, R. (1998). Multitask learning. In Learning to

learn, pages 95–133. Springer. 02683.

Colson, B., Marcotte, P., and Savard, G. (2007). An
overview of bilevel optimization. Annals of operations
research, 153(1):235–256.

Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M. (2018).
Incremental learning-to-learn with statistical guarantees.
arXiv preprint arXiv:1803.08089. To appear in UAI 2018.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. (2009). Imagenet: A large-scale hierarchical image
database. In Computer Vision and Pattern Recognition
(CVPR), pages 248–255.

Domke, J. (2012). Generic Methods for Optimization-Based
Modeling. In AISTATS, volume 22, pages 318–326.

Edwards, H. and Storkey, A. (2016). Towards a Neural
Statistician. arXiv:1606.02185 [cs, stat]. 00027 arXiv:
1606.02185.

Evgeniou, T., Micchelli, C. A., and Pontil, M. (2005). Learn-
ing multiple tasks with kernel methods. Journal of Ma-
chine Learning Research, 6(Apr):615–637.

Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic
meta-learning for fast adaptation of deep networks. In
Proceedings of the 34th International Conference on Ma-
chine Learning, (ICML), pages 1126–1135.

Flamary, R., Rakotomamonjy, A.,

and Gasso, G.
(2014). Learning constrained task similarities in graph-
regularized multi-task learning. Regularization, Opti-
mization, Kernels, and Support Vector Machines, page
103.

Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
(2017). Forward and reverse gradient-based hyperparam-
eter optimization. In Proceedings of the 34th Interna-
tional Conference on Machine Learning, (ICML), pages
1165–1173.

Glorot, X. and Bengio, Y. (2010). Understanding the difﬁ-
culty of training deep feedforward neural networks. In
Proceedings of the 13th International Conference on Arti-
ﬁcial Intelligence and Statistics (AIStat), pages 249–256.

Griewank, A. and Walther, A. (2008). Evaluating deriva-
tives: principles and techniques of algorithmic differenti-
ation. SIAM.

He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual
learning for image recognition. In Computer Vision and
Pattern Recognition (CVPR), pages 770–778.

Hutter, F., Lcke, J., and Schmidt-Thieme, L. (2015). Beyond
Manual Tuning of Hyperparameters. KI - K¨unstliche
Intelligenz, 29(4):329–337.

Kaiser, L., Nachum, O., Roy, A., and Bengio, S. (2017).

Learning to remember rare events.

Keerthi, S. S., Sindhwani, V., and Chapelle, O. (2007). An
efﬁcient method for gradient-based adaptation of hyper-
parameters in svm models. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 673–680.

Khosla, A., Zhou, T., Malisiewicz, T., Efros, A. A., and
Torralba, A. (2012). Undoing the damage of dataset bias.
In European Conference on Computer Vision, pages 158–
171. Springer.

Dontchev, A. L. and Zolezzi, T. (1993). Well-posed op-
timization problems, volume 1543 of Lecture Notes in
Mathematics. Springer-Verlag, Berlin.

Koch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese
neural networks for one-shot image recognition. In ICML
Deep Learning Workshop, volume 2. 00127.

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

Snell, J., Swersky, K., and Zemel, R. S. (2017). Prototypical
networks for few-shot learning. In Advances in neural
information processing systems, pages 4080–4090.

Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical
bayesian optimization of machine learning algorithms.
In Advances in Neural Information Processing Systems
(NIPS), pages 2951–2959.

Thrun, S. and Pratt, L. (1998). Learning to learn. Springer.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).
Attention is all you need. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 6000–6010.

Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K.,
and Wierstra, D. (2016). Matching networks for one shot
learning. In Advances in Neural Information Processing
Systems (NIPS), pages 3630–3638.

Wichrowska, O., Maheswaranathan, N., Hoffman, M. W.,
Colmenarejo, S. G., Denil, M., Freitas, N., and Sohl-
Dickstein, J. (2017). Learned optimizers that scale and
generalize. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML), pages 3751–3760.

Koh, P. W. and Liang, P. (2017). Understanding black-box
predictions via inﬂuence functions. In Proceedings of
the 34th International Conference on Machine Learning
(ICML), pages 1885–1894.

Kunapuli, G., Bennett, K., Hu, J., and Pang, J.-S. (2008).
Classiﬁcation model selection via bilevel programming.
Optimization Methods and Software, 23(4):475–489.

Kuzborskij, I., Orabona, F., and Caputo, B. (2013). From
n to n+ 1: Multiclass transfer incremental learning. In
Computer Vision and Pattern Recognition (CVPR), 2013
IEEE Conference on, pages 3358–3365.

Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.
(2015). Human-level concept learning through probabilis-
tic program induction. Science, 350(6266):1332–1338.

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-
man, S. J. (2017). Building machines that learn and think
like people. Behavioral and Brain Sciences, 40. 00152.

Maclaurin, D., Duvenaud, D. K., and Adams, R. P. (2015).
Gradient-based hyperparameter optimization through re-
versible learning. In Proceedings of the 32nd Interna-
tional Conference on Machine Learning, (ICML, pages
2113–2122.

Maurer, A., Pontil, M., and Romera-Paredes, B. (2016). The
beneﬁt of multitask representation learning. The Journal
of Machine Learning Research, 17(1):2853–2884.

Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.

(2018). A simple neural attentive meta-learner.

Munkhdalai, T. and Yu, H. (2017). Meta networks.

In
Proceedings of the 34th International Conference on Ma-
chine Learning, (ICML), pages 2554–2563.

Pedregosa, F. (2016). Hyperparameter optimization with
approximate gradient. In Proceedings of The 33rd Inter-
national Conference on Machine Learning (ICML), pages
737–746.

Ravi, S. and Larochelle, H. (2017). Optimization as a model
for few-shot learning. In In International Conference on
Learning Representations (ICLR).

Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D.,
and Lillicrap, T. (2016). Meta-learning with memory-
augmented neural networks. In Proceedings ofthe 33rd
International Conference on Machine Learning, pages
1842–1850.

Schmidhuber, J. (1992). Learning to Control Fast-Weight
Memories: An Alternative to Dynamic Recurrent Net-
works. Neural Computation, 4(1):131–139. 00082.

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

A. Proofs of the Results in Sec. 3

Therefore, using also the continuity of ϕ, we have

Proof of Theorem 3.1. Since Λ is compact, it follows from
Weierstrass theorem that a sufﬁcient condition for the exis-
tence of minimizers is that f is continuous. Thus, let ¯λ ∈ Λ
and let (λn)n∈N be a sequence in Λ such that λn → ¯λ.
We prove that f (λn) = E(wλn , λn) → E(w¯λ, ¯λ) = f (¯λ).
Since (wλn )n∈N is bounded, there exists a subsequence
(wkn )n∈N such that wkn → ¯w for some ¯w ∈ Rd. Now,
since λkn → ¯λ and the map (w, λ) (cid:55)→ Lλ(w) is jointly
continuous, we have

∀ w ∈ Rd, L¯λ( ¯w) = lim
n
≤ lim
n

Lλkn

(wkn )

Lλkn

(w) = L¯λ(w).

Therefore, ¯w is a minimizer of L¯λ and hence ¯w = w¯λ.
This prove that (wλn)n∈N is a bounded sequence having
a unique cluster point. Hence (wλn)n∈N is convergence
to its unique cluster point, which is w¯λ. Finally, since
(wλn , λn) → (w¯λ, ¯λ) and E is jointly continuous, we have
E(wλn , λn) → E(w¯λ, ¯λ) and the statement follows.
We recal a fundamental fact concerning the stability of min-
ima and minimizers in optimization problems (Dontchev
and Zolezzi, 1993). We provide the proof for completeness.

Theorem A.1 (Convergence). Let ϕT and ϕ be lower semi-
continuous functions deﬁned on a compact set Λ. Suppose
that ϕT converges uniformly to ϕ on Λ as T → +∞. Then

(a) inf ϕT → inf ϕ,

(b) argmin ϕT → argmin ϕ, meaning that, for every
(λT )T ∈N such that λT ∈ argmin ϕT , we have that:

- (λT )T ∈N admits a convergent subsequence;
- for every subsequence (λKT )T ∈N such that

λKT → ¯λ, we have ¯λ ∈ argmin ϕ.

Proof. Let (λT )T ∈N be a sequence in Λ such that, for every
T ∈ N, λT ∈ argmin ϕT . We prove that

1) (λT )T ∈N admits a convergent subsequence.

2) for every subsequence (λKT )T ∈N such that λKT → ¯λ,
we have ¯λ ∈ argmin ϕ and ϕKT (λKT ) → inf ϕ.

3) inf ϕT → inf ϕ.

The ﬁrst point follows from the fact that Λ is compact.
Concerning the second point, let (λKT )T ∈N be a subse-
quence such that λKT → ¯λ. Since ϕKT converge uniformly
to ϕ, we have

|ϕKT (λKT ) − ϕ(λKT )| ≤ sup
λ∈Λ

|ϕKT (λ) − ϕ(λ)| → 0.

∀ λ ∈ Λ, ϕ(¯λ) = lim
T
≤ lim
T

ϕ(λKT ) = lim
T

ϕKT (λKT )

ϕKT (λ) = ϕ(λ).

So, ¯λ ∈ argmin ϕ and ϕ(¯λ) = limT ϕKT (λKT ) ≤ inf ϕ =
ϕ(¯λ), that is, limT ϕKT (λKT ) = inf ϕ.
Finally, as regards the last point, we proceed by contradic-
tion. If (ϕT (λT ))T ∈N does not convergce to inf f , then
there exists an ε > 0 and a subsequence (ϕKT (λKT ))T ∈N
such that

|ϕKT (λKT ) − inf ϕ| ≥ ε,

∀ T ∈ N

(11)

T

) be a convergent subsequence of (λKT )T ∈N.
Now, let (λK(1)
→ ¯λ. Clearly (λK(1)
Suppose that λK(1)
) is also a subse-
quence of (λT )T ∈N. Then, it follows from point 2) above
) → inf ϕ. This latter ﬁnding together
that ϕK(1)
with equation (11) gives a contradiction.

(λK(1)

T

T

T

T

Proof of Theorem 3.2. Since E(·, λ) is uniformly Lipschitz
continuous, there exists ν > 0 such that for every T ∈ N
and every λ ∈ Λ

|fT (λ) − f (λ)| = |E(wT,λ, λ) − E(wλ, λ)|

≤ ν(cid:107)wT,λ − wλ(cid:107).

It follows from assumption (vi) that fT (λ) converges to
f (λ) uniformly on Λ as T → +∞. Then the statement
follows from Theorem A.1

B. Cross-validation and Bilevel Programming

We note that the (approximate) bilevel programming frame-
work easily accommodates also estimations of the gener-
alization error generated by a cross-validation procedures.
We describe here the case of K-fold cross-validation, which
includes also leave-one-out cross validation.

Let D = {(xi, yi)}n
i=1 be the set of available data; K-fold
cross validation, with K ∈ {1, 2, . . . , N } consists in parti-
tioning D in K subsets {Dj}K
j=1 and ﬁt as many models
gwj on training data Dj
i(cid:54)=j Di. The models are then
evaluated on Dj
j=1 the
vector of stacked weights, the K-fold cross validation error
is given by

val = Dj. Denoting by w = (wj)K

tr = (cid:83)

E(w, λ) =

1
K

K
(cid:88)

j=1

Ej(wj, λ)

where Ej(wj, λ) = (cid:80)
(x,y)∈Dj (cid:96)(gwj (x), y). E can be
treated as the outer objective in the bilevel framework, while
the inner objective Lλ may be given by the sum of regu-
larized empirical errors over each Dj
tr for the K models.

Bilevel Programming for Hyperparameter Optimization and Meta-Learning

Under this perspective, a K-fold cross-validation procedure
closely resemble the bilevel problem for ML formulated
in Sec. 2.2, where, in this case, the meta-distribution col-
lapses on the data (ground) distribution and the episodes are
sampled from the same dataset of points.

hyperparameter optimization, as reported in the last row of
the Table 5.

Table 5. Validation and test mean absolute percentage error
(MAPE) for various values of T .

(cid:80)

j Ej(wj

By following the procedure outlined in Sec. 2.3 we can
approximate the minimization of Lλ with T steps of an
optimization dynamics and compute the hypergradient of
fT (λ) = 1
T , λ) by training the K models and
K
proceed with either forward or reverse differentiation. The
models may be ﬁtted sequentially, in parallel or stochas-
tically. Speciﬁcally, in this last case, one can repeatedly
sample one fold at a time (or possibly a mini-batch of folds)
and compute a stochastic hypergradient that can be used in
a SGD procedure in order to minimize fT . At last, we note
that similar ideas for leave-one out cross-validation error are
developed in (Beirami et al., 2017), where the hypergradient
of an approximation of the outer objective is computed by
means of the implicit function theorem.

C. The Effect of T : Ridge Regression

In Sec. 5.1 we showed that increasing the number of itera-
tions T leads to a better optimization of the outer objective f
through the approximations fT , which converge uniformly
to f by Proposition 3.2. This, however, does not necessary
results in better test scores due to possible overﬁtting of
the training and validation errors, as we noted for the linear
hyper-representation multiclass classiﬁcation experiment in
Sec. 5.1. The optimal choice of T appears to be, indeed,
problem dependent: if, in the aforementioned experiment, a
quite small T led to the best test accuracy (after hyperparam-
eter optimization), in this section we present a small scale
linear regression experiment that beneﬁts from an increasing
number of inner iterations.

We generated 90 noisy synthetic data points with 30 features,
of which only 5 were informative, and divided them equally
to form training, validation and test sets. As outer objective
we set the mean squared error E(w) = ||Xvalw − yval||2,
where Xval and yval are the validation design matrix and
targets respectively and w ∈ R30 is the weight vector of the
model. We set as inner inner objective

Lλ(w) = ||Xtrw − ytr||2 +

30
(cid:88)

i=1

eλiw2
i

and optimized the L2 vector of regularization coefﬁcients λ
(equivalent to a diagonal Tikhonov regularization matrix).
The results, reported in Table 5, show that in this scenario
overﬁtting is not an issue and 250 inner iterations yield the
best test result. Note that, as in Sec. 5.1, also this problem
admits an analytical solution, from which it is possible to
compute the hypergradient analytically and perform exact

T

Validation MAPE Test MAPE

10
50
100
250
Exact

11.35
1.28
0.55
0.47
0.37

43.49
5.22
1.26
0.50
0.57

D. Further Details on Few-shot Experiments

This appendix contains implementation details of the repre-
sentation mapping h and the meta-learning hyperparameters
used in the few-shot learning experiments of Sec. 5.2.

To optimize the representation mapping, in all the exper-
iments, we use Adam with learning rate set to 10−3 and
a decay-rate of 10−5. We used the initialization strategy
proposed in (Glorot and Bengio, 2010) for all the weights λ
of the representation.

For Omniglot experiments we used a meta-batch size of 32
episodes for ﬁve-way and of 16 episodes for 20-way. To
train the episode-speciﬁc classiﬁers we set the learning rate
to 0.1.

For one set of experiments with Mini-imagenet we used an
hyper-representation (C4L) consisting of 4 convolutional
layers where each layer is composed by a convolution with
32 ﬁlters, a batch normalization followed by a ReLU acti-
vation and a 2x2 max-pooling. The classiﬁers were trained
using mini-batches of 4 episodes for one-shot and 2 episodes
for ﬁve-shot with learning rate set to 0.01.

The other set of experiments with Mini-imagenet employed
a Residual Network (RN) as the representation mapping,
built of 4 residual blocks (with 64, 96, 128, 256 ﬁlters) and
then the block that follows {1 × 1 conv (2048 ﬁlters), avg
pooling, 1 × 1 conv (512 ﬁlters) }. Each residual block
repeats the following block 3 times {1 × 1 conv, batch
normalization, leaky ReLU (leak 0.1)} before the residual
connection. In this case the classiﬁers were optimized using
mini-batches of 2 episodes for both one and ﬁve-shot with
learning rate set to 0.04.

