Learning to Track Objects from Unlabeled Videos

Jilai Zheng1 Chao Ma1* Houwen Peng2 Xiaokang Yang1
1 MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University
2 Microsoft Research
{zhengjilai,chaoma,xkyang}@sjtu.edu.cn, houwen.peng@microsoft.com

1
2
0
2

g
u
A
8
2

]

V
C
.
s
c
[

1
v
1
1
7
2
1
.
8
0
1
2
:
v
i
X
r
a

Abstract

In this paper, we propose to learn an Unsupervised Sin-
gle Object Tracker (USOT) from scratch. We identify that
three major challenges, i.e., moving object discovery, rich
temporal variation exploitation, and online update, are the
central causes of the performance bottleneck of existing
unsupervised trackers. To narrow the gap between unsu-
pervised trackers and supervised counterparts, we propose
an effective unsupervised learning approach composed of
three stages. First, we sample sequentially moving objects
with unsupervised optical ﬂow and dynamic programming,
instead of random cropping. Second, we train a naive
Siamese tracker from scratch using single-frame pairs.
Third, we continue training the tracker with a novel cy-
cle memory learning scheme, which is conducted in longer
temporal spans and also enables our tracker to update on-
line. Extensive experiments show that the proposed USOT
learned from unlabeled videos performs well over the state-
of-the-art unsupervised trackers by large margins, and on
par with recent supervised deep trackers. Code is available
at https://github.com/VISION-SJTU/USOT.

1. Introduction

Visual object tracking is one of the most fundamental
computer vision tasks with numerous applications, such
as autonomous driving, intelligent surveillance, robot, and
human-computer interaction. The past few years have
witnessed considerable progress in visual object tracking,
thanks to the powerful representation of deep learning. In
spite of the success, the state-of-the-art deep tracking algo-
rithms are data-hungry, requiring a huge number of anno-
tated data for supervised training. As manually labeled data
are expensive and time-consuming, exploiting ubiquitous
unlabeled videos for visual tracking has drawn increasing
attention recently. Following the classic pipeline of unsu-
pervised learning, existing unsupervised trackers randomly
crop template regions on unlabeled videos and employ ei-

* Corresponding author.

Figure 1: Comparison on the VOT2017/18 benchmark with
recent deep trackers. The proposed trackers, USOT and
USOT*, perform well over the state-of-the-art unsupervised
deep trackers, including LUDT [38], LUDT+ [38], and
S2SiamFC [34], and on par with recent supervised trackers.
Notation: SiamFC [2], SiamDW [47], SiamRPN [26], C-
RPN [12], DaSiamRPN [50], ATOM [8], SiamRPN++ [25],
DiMP [3], KYS [4].

ther self consistency [34] or cycle consistency [37] as a pre-
text task for learning to track. Despite the promising results,
there still exists a large performance gap between unsuper-
vised and supervised trackers. In view of the great success
of unsupervised learning on a number of other vision tasks,
such as video object segmentation [23], optical ﬂow [28]
and depth estimation [14], it is of great interest to narrow
the gap between unsupervised and supervised trackers.

We identify three critical challenges that cause the per-
formance bottleneck of unsupervised trackers. 1) Mov-
ing object discovery. As ground truth bounding boxes are
not available, existing unsupervised trackers randomly sam-
ple regions in frames as pseudo templates [37, 34]. Ran-
dom samples are far from precisely locating objects, not to
mention learning to distinguish between objects and back-
ground. Moreover, as random samples do not contain clear
edges of objects, they are not suitable for bounding box re-

 
 
 
 
 
 
gression learning. The lack of bounding box regression for
scale change estimation heavily limits the performance of
unsupervised trackers. 2) Rich temporal variation exploita-
tion. Due to the lack of labels in the temporal span, exist-
ing unsupervised trackers struggle to learn from rich motion
clues. For example, UDT [37] performs forward and back-
ward tracking within at most 10 frames.
In such a short
clip, the foreground objects show highly correlated appear-
ances with little variations, causing a failure to exploit rich
temporal variations over a long span for training. 3) Online
update. Online update helps to exploit the temporal smooth-
ness and has demonstrated great success in leading super-
vised tracking methods [35, 3, 46, 48]. While supervised
trackers usually collect multiple object samples in separated
frames for learning online modules [3, 13], it is more chal-
lenging to train online branches for unsupervised trackers,
due to lack of even coarse object locations in videos.

To address these challenges, we propose to train a robust
tracker from unlabeled videos. First, for data preparation,
we develop a sequential box sampling algorithm to coarsely
discover moving objects from unlabeled videos. Speciﬁ-
cally, we use unsupervised optical ﬂow to detect moving ob-
jects and apply dynamic programming to sequentially link
candidate boxes. Second, we naively train from scratch
an unsupervised Siamese tracker using single-frame pairs.
That is, we train with each Siamese pair cropped based on
the sampled box in a single frame. Despite its simplicity,
we show that this strategy provides a great initialization for
the unsupervised tracker, thus beneﬁcial to future training
in longer temporal spans. Third, we propose a cycle mem-
ory learning scheme to continue training the naive tracker.
Speciﬁcally, we divide the whole video into a number of
fragments according to the detected moving object trajec-
tory. We then conduct forward tracking from a single frame
to several other frames in the same fragment, and store all
intermediate tracking results in a memory queue. We then
track backward to the initial frame to compute the consis-
tency loss. Since the length of video fragments is quite
long (averaged 64.6 frames on VID [33]) compared with
UDT [37] (<10 frames), our tracker can capture large mo-
tion and appearance variations. More importantly, the pro-
posed cycle memory scheme allows updating the memory
queue online for model update (see Sec. 4.3).

We evaluate the proposed unsupervised tracker on six
large-scale benchmarks. Extensive experiments show that
our proposed tracker performs well over the state-of-the-art
unsupervised trackers by large margins, and on par with the
recent supervised trackers (see Fig. 1). The main contribu-
tions of this work are summarized as follows:

• We coarsely discover moving objects from unlabeled

videos for unsupervised learning.

• We train a naive Siamese tracker with single-frame
pairs and gradually extend it to longer temporal spans.

• We propose a cycle memory learning scheme, allowing

unsupervised trackers to update online.

2. Related Work

Supervised Tracking. Deep learning has revolutionized
the ﬁeld of visual tracking by powerful representation.
In the past few years, template-based deep trackers with
Siamese networks have received increasing attention due to
the promising results on benchmark datasets. These track-
ers regard the target object as a template and search over
a cropped window to locate the target. SiamFC [2] ﬁrst
utilizes the same backbone network to extract deep fea-
tures from both the template patch and the search patch,
and computes the response map with cross-correlation.
Since then, considerable efforts have been made to ex-
tend Siamese trackers. SiamRPN [26] incorporates a re-
gion proposal network (RPN) [31] into the Siamese frame-
work. DiMP [3] proposes to attach an online module
to Siamese trackers for template update. Other notice-
able improvements in the Siamese framework involve ad-
vanced backbone network [47], correlation method [25],
attention mechanism [45], re-detection module [36], mask
generation [40], feature alignment [48], anchor-free regres-
sor [6, 15], etc. With these efforts, Siamese trackers have
shown the superior tracking performance thus far. However,
training Siamese trackers requires a huge number of labeled
training data. In this work, we aim at a novel unsupervised
learning scheme, which helps to learn template-based track-
ers from unlabeled videos in the wild.
Unsupervised Tracking. The pioneer unsupervised deep
tracker (UDT) [37] suggests that a robust tracker is able to
track an object forward and backward in a video and ﬁnally
return to its initial location in the starting frame. UDT de-
velops a tracker based on DCFNet [39], and computes a
cycle consistency loss between forward and backward tra-
jectories in the training phase. The contemporary method of
UDT is TimeCycle [41], which proposes cycle consistency
to generate unsupervised video representation. JSLTC [27]
proposes to calculate an inter-frame afﬁnity matrix to model
the transitions between video frames and use such corre-
spondence to track objects. S2SiamFC [34] adopts the self-
Siamese pipeline to train a foreground/background classi-
ﬁer like SiamFC [2] from single-frame pairs, showing com-
parable results to the supervised counterpart. Despite the
promising results, there exists a large performance gap be-
tween the state-of-the-art unsupervised trackers and the top-
performing supervised ones. We identify three critical chal-
lenges, moving object discovery, rich temporal variation ex-
ploitation, and online update, that cause the performance
bottleneck of unsupervised trackers. By effectively tackling
these challenges, the proposed unsupervised tracker out-
performs the state-of-the-art unsupervised trackers by large
margins, and is on par with recent supervised trackers.

3. Proposed Method

In this section, we present the proposed unsupervised
tracker in detail. The unsupervised training scheme involves
three stages. The ﬁrst stage in Sec. 3.1 aims to produce a
trajectory of moving objects from unlabeled videos. The
second stage in Sec. 3.2 learns a naive Siamese tracker us-
ing single-frame pairs. The third stage in Sec. 3.3 continues
training the naive tracker by means of cycle memory learn-
ing, which is performed in longer temporal spans and also
enables the unsupervised tracker to update online.

3.1. Moving Object Discovery

Instead of randomly cropping objects, we propose to
generate a smooth bounding box sequence for moving fore-
ground objects in unlabeled videos. For discovering moving
objects, we have two key observations:

• Foreground objects tend to have distinguishing motion
patterns in contrast to the background surroundings.
This inspires us to discover candidate foreground ob-
jects by means of unsupervised optical ﬂow.

• The trajectories of moving objects tend to be smooth.
This motivates us to employ dynamic programming
(DP) to get temporally reliable box sequences.

Candidate Box Generation. Let an arbitrary video be I
including L successive frames with the same size W × H,
namely I = {It | 1 ≤ t ≤ L}, where It is the tth frame
in I. To locate the potential foreground object in frame It,
we ﬁrst compute the optical ﬂow map Ft with frame It and
frame It+Tf , i.e., Ft = F lowt→t+Tf . Tf is an interval for
computing optical ﬂow. As is illustrated in Fig. 2, we obtain
Ft from frame It and frame It+Tf using the off-the-shelf
unsupervised ARFlow [28] algorithm, and then transform
Ft to a distance map Dt. We binarize Dt to get a mask Mt
as follows:
(cid:26) 1
0

t ) + (1 − α) · meanj(Dj
t )
o.w.

t ≥ α · maxj(Dj

if Di

t =

M i

where Di

t =

(cid:13)
(cid:13)F i
(cid:13)

t − meanj(F j
t )

(cid:13)
(cid:13)
(cid:13)2

,

(1)
and α ∈ (0, 1) is a hyper-parameter. Here superscript de-
notes pixel-wise index. The maximum value and mean
value within the spatial dimension are respectively indicated
by max and mean.

Every connected area with all internal pixels satisfying
M i
t = 1 corresponds to an area which has distinguishing
motion compared with background in It, and this area is
more likely to cover a foreground object. To further ﬁlter
out unreliable areas from these candidates, we take the cir-
cumscribed rectangles of all these areas as initial candidate
boxes, and score the boxes according to their sizes and po-
sitions. Due to center bias, larger bounding boxes in the

Figure 2: Candidate box generation via optical ﬂow. The
ﬂow map Ft contains the distinguishing motion patterns of
moving objects. We binarize the ﬂow map Ft using a dis-
tance metric Dt to generate the candidate box Bt.

middle of the image should have higher quality scores. Let
B = (x0, y0, x1, y1) denote the top-left and bottom-right
corners of a box. The quality score Sc of the box B is de-
ﬁned as:

Sc(B) =(x1 − x0)(y1 − y0)+

β · min(x0, W − x1) min(y0, H − y1),

(2)

where β is a weight parameter. The box with the highest
score is selected as the ﬁnal candidate box Bt for frame It.
We denote the set of all these selected candidate boxes in
video I as B = {Bt | 1 ≤ t ≤ L}.
Box Sequence Generation.
The generated candidate
bounding boxes B may contain noisy boxes due to camera
shake, occlusion, etc. To remove unreliable boxes, we ap-
ply dynamic programming to create a more reliable bound-
ing box sequence B(cid:48). According to the second observation
that the trajectory of a moving object in a video should be
smooth, we select a subset of candidate bounding boxes
from B, where the trajectory of the selected boxes is as
smooth as possible. For dynamic programming, the most
critical issue is how to measure the reward of transition in
the box trajectory from one bounding box to another. We
modify the DIoU [49] metric, which originally considers
the overlap and distance between two boxes. Formally, the
reward Rdp for dynamic programming is deﬁned as:

Rdp(Bt, Bt(cid:48)) = IoU (Bt, Bt(cid:48)) − γ · RDIoU (Bt, Bt(cid:48)), (3)

where γ is a hyper-parameter. To encourage a smooth
trajectory, we set γ > 1 for the distance penalty on
RDIoU [49]. Note that dynamic programming aims at dis-
covering an optimal path in the box sequence B with the
highest reward accumulation (see the supplementary docu-
ment for the complete algorithm). As shown in Fig. 3, for
the frames whose candidate boxes are not selected by DP,
we use linear interpolation to generate pseudo boxes based
on their adjacent candidate boxes selected by DP.

FtDtMtItBtIt+TfFigure 3: Box sequence generation. We use dynamic programming to generate a smooth and reliable box trajectory from
candidate boxes in yellow. Pseudo boxes in green in the remaining frames are generated through linear interpolation.

3.2. Naive Siamese Tracker

With the generated box sequences, we train a naive
Siamese tracker using single-frame pairs from scratch. This
pretext task is based on a simple observation that an image
and any of its sub-region naturally form a training pair of the
Siamese network [34]. However, randomly sampled pseudo
boxes as in [34] fail to cover foreground objects for effec-
tively training Siamese networks. Moreover, random sam-
ples are not suitable for learning bounding box regression.
This signiﬁcantly hinders the performance of unsupervised
trackers. We propose to utilize the reliable box sequence
B(cid:48) as training data. To ensure that the most precise bound-
ing boxes in B(cid:48) are sampled by the data loader, we adopt a
two-level scoring mechanism to ﬁlter out low-quality boxes
at both the sequence and frame levels. We ﬁnd that denser
frame selection by DP in video I tends to imply a more
successful moving object discovery. As such, we deﬁne the
quality score Qv of video I = {It | 1 ≤ t ≤ L} as:

Qv(I) =

Ndp
L

,

(4)

where Ndp indicates the number of frames in video I se-
lected by DP.

Similarly, the frame quality score evaluating the box B(cid:48)
t
in frame It can be measured by the percentage of frames
selected by DP within all its adjacent frames. Let Ts be a
ﬁxed frame interval. We formally deﬁne the frame quality
score Qf as:

Qf (B(cid:48)

N (cid:48)
dp
2Ts + 1
dp indicates the number of frames between frame

t) =

(5)

,

where N (cid:48)
It−Ts and frame It+Ts selected by DP.

When loading training pairs, we sequentially conduct
video sampling and frame sampling. We sample a video
only if its quality score Qv(I) ≥ θ1, where θ1 is a thresh-
old. During frame sampling, we randomly sample several
frames with their total number positively correlated with
1/Qv(I) from the selected video, and then select the frame
with the highest frame quality score Qf (B(cid:48)

t) for training.

We follow the conventional training paradigm as in
SiamFC [2]. The input template zt and the search area xt
are respectively of size 127 × 127 and 255 × 255, both

cropped from It based on B(cid:48)
t. After extracting deep fea-
tures from the input pair, we adopt PrPool [19] to pool the
template feature, and then compute the multi-scale corre-
lation map [48]. The output response map Rcls is of size
25 × 25 × 1 for foreground/background classiﬁcation. The
other output response map Rreg is of size 25 × 25 × 4 for
regressing the distances from the center location to the four
sides of the bounding box. The loss function Lnaive is the
sum of both the regression and classiﬁcation losses:

Lnaive = Lreg + λ1Lcls,

(6)

where Lreg and Lcls are respectively the IoU loss [44] and
the binary cross-entropy (BCE) loss [10]. λ1 is a weight
parameter.

3.3. Cycle Memory Training

We view the above unsupervised Siamese tracker as a
naive tracker as it incurs two limitations. First, as the tem-
plate and search area are cropped in the same frame, the
tracker is not learned with large motion and appearance
variations. Second, this tracker cannot update itself online,
thus fails to track objects in long temporal spans or under
complex scenes.

We propose to continue training the naive tracker us-
ing a cycle memory learning scheme, aiming to enable the
tracker to handle large variations as well as update the mem-
ory queue online. The main idea of cycle memory can be
In brief, we ﬁrst conduct for-
summarized as in Fig. 4.
ward tracking from a template zt to Nmem adjacent mem-
ory frames, then store features of all intermediate tracking
results as a memory queue, and ﬁnally conduct backward
tracking to the original search area xt. A cycle memory loss
Lmem is computed using the same ground truth as Lcls.

Speciﬁcally, at every training step, we simultaneously
crop a training pair zt and xt
in frame It (the same
as training the naive Siamese tracker), as well as Nmem
memory search areas sampled from {xt | Tl ≤ t ≤ Tu}.
These memory search areas are cropped from Nmem
adjacent frames of It according to the box sequence
{B(cid:48)
t | Tl ≤ t ≤ Tu}. Here Tl and Tu are the lower and up-
per frame indices for sampling memory frames. Select-
ing these two indices is quite important. To learn from
long-term variations, the frame interval between Tl and Tu

Trajectory via dynamic programmingLinear interpolationCandidate boxesItIt+3It+6It+9Figure 4: Overview of the proposed unsupervised tracking framework. Left: The overall training pipeline. Right: The
detailed structure of the naive Siamese tracker for self tracking and forward tracking, and the online module learned with the
cycle memory scheme. The naive tracker is trained with a template and a search area cropped from the same frame, while
the online module aims to track backward from the memory search areas to the template frame following the cycle learning
pipeline. The circle notations with ∗ denote multi-scale correlation [48] for deep features, where the same color indicates
weight sharing. The circle with W refers to the conﬁdence-value module for integrating the correlation maps (Eqn. 8).

should be large enough. However, excessive frame interval
does harm to the learning process as the target object may
disappear in frames far from It. In practice, we dynami-
cally set Tl and Tu at frame It. Since they are two mirror
variables, we formally deﬁne Tu as follows:

{k}

Tu(It) = max
t≤k≤L
s.t. ∀t < t(cid:48) ≤ k, Rdp(B(cid:48)
∀t < t(cid:48) ≤ k, Qf (B(cid:48)

t(cid:48)) ≥ θ2

t(cid:48)−1, B(cid:48)
t(cid:48)) ≥ θ3,

(7)

k can be connected to B(cid:48)

where θ2 and θ3 are two thresholds. The main idea of Eqn. 7
is that, as long as a box B(cid:48)
t through a
smooth and reliable box sequence in B(cid:48), search area cropped
from B(cid:48)
k can be used for cycle memory training. In other
words, we take step changes in B(cid:48) to divide I into frag-
ments, and the pseudo boxes of all frames in the same frag-
ment tend to locate the same object. This scheme helps our
tracker exploit long-term variations, while still ensuring the
reliability of pseudo bounding boxes in the memory frames
(see Sec. 4.3 for quantitative analysis).

Let Nmem denote the number of memory frames. We
ﬁrst utilize the tracker to predict Nmem intermediate bound-
ing boxes in the memory frames for the template zt. We
adopt PrPool [19] to pool Nmem features based on the in-

termediate boxes. Then we use the pooled features as tem-
plates to conduct multi-scale correlation [48] with the deep
feature of xt. Note that the original classiﬁcation branch
and the memory branch share the same weights in terms
of the multi-scale correlation module. All Nmem correla-
tion maps, denoted as {C u
corr | 1 ≤ u ≤ Nmem}, are inte-
grated together by a conﬁdence-value strategy. Speciﬁcally,
given a correlation map C u
corr, we utilize two 3 × 3 con-
volution layers to generate a conﬁdence map C u
conf and a
value map C u
val with the same dimension. We then nor-
malize C u
conf element-wise across all conﬁdence maps as
weights on C u
val. The ﬁnally integrated correlation map C
is formulated as follows:

C =

(cid:88)

1≤u≤Nmem

softmax (C u

conf ) (cid:12) C u

val,

(8)

where (cid:12) denotes Hadamard product. As shown in Fig. 4,
the integrated map C is converted to 25 × 25 × 1 via con-
volution, yielding the response map Rmem of the object in
search area xt. The total loss function L for training is:

L = Lreg + λ1Lcls + λ2Lmem,

(9)

where λ1 and λ2 are weight parameters. We use the BCE
loss [10] as the cycle memory loss Lmem, which shares the
same pseudo ground truth label as in Lcls.

Forward trackMemory queueBackward trackSelf trackTemplateSearch areaPooling featuresMemorysearchareas***conv3 × 3conv3 × 3wNaive Siamese TrackerCycle Memory Learning Training PipelineSame frameconvconvconvCcorrCconfCvalMemory   queueSearchareamemclsregTable 1: Results on the VOT benchmarks. The proposed trackers perform well over the state-of-the-art unsupervised trackers.
Boldface denotes the best performance among all trackers built without video labels for ofﬂine training (the same below).

Tracker

Unsup.

VOT2016

VOT2017/18

VOT2020

A ↑

0.532
0.61
-
-

0.419
0.407
0.489
0.55

0.493
0.544
0.570

0.593
0.600

R ↓

0.461
0.22
-
-

1.109
0.727
0.569
0.20

0.639
0.422
0.331

0.336
0.233

EAO ↑

0.235
0.411
-
-

0.115
0.165
0.192
0.375

0.215
0.232
0.299

0.351
0.402

A ↑

0.503
0.56
0.590
0.597

0.400
0.394
0.447
0.48

0.463
0.463
0.490

0.564
0.578

R ↓

0.585
0.34
0.204
0.153

1.639
1.011
0.773
0.27

0.782
0.693
0.412

0.435
0.304

EAO ↑

0.188
0.326
0.401
0.440

0.076
0.118
0.135
0.280

0.180
0.154
0.230

0.290
0.344

A ↑

0.418
-
0.462
0.457

0.345
0.367
0.407
-

-
-
-

R ↑

0.502
-
0.734
0.740

0.244
0.322
0.432
-

-
-
-

EAO ↑

0.179
-
0.271
0.274

0.092
0.113
0.154
-

-
-
-

0.458
0.448

0.600
0.600

0.222
0.219

No
No
No
No

Yes
Yes
Yes
Yes

Yes
Yes
Yes

Yes
Yes

SiamFC [2]
DaSiamRPN [50]
ATOM [8]
DiMP [3]

IVT [32]
MIL [1]
KCF [17]
ECO [7]

S2SiamFC [34]
LUDT [38]
LUDT+ [38]

USOT (Ours)
USOT* (Ours)

4. Experiments

This section presents the results of our unsupervised
tracker on multiple benchmarks, with comparisons to the
state-of-the-art tracking algorithms. Extensive ablation
studies are provided to analyze the effectiveness of our de-
sign choices.

4.1. Implementation Details

Training. Our tracker is trained on the data collected from
the training sets of four datasets including GOT-10k [18],
ImageNet VID [33], LaSOT [11] and YouTube-VOS [43].
Note that the ground-truth labels of these training sets are
not available in our method. The hyper-parameters for
extracting the video box sequences are set as α = 0.3,
β = 0.5, γ = 4.1 respectively. Our network adopts ResNet-
50 [16] as the backbone network, and uses the third convo-
lutional block to extract deep features for input images. We
notice that existing CNN backbones pretrained on the Ima-
geNet dataset [33] contain information from manual labels.
For the sake of solid comparisons, we conduct all the ex-
periments in two settings (i.e., w/o and w/ supervised back-
bone pretraining on ImageNet). During training, we use
synchronized SGD [24] on 4 NVIDIA GeForce RTX 3090
GPUs. Each GPU hosts 12 groups of training instances.
The whole end-to-end training phase takes 30 epochs in to-
tal, in which cycle memory is conducted only within the
last 25 epochs. We start with a warm-up learning rate from
2.5 × 10−3 to 5 × 10−3 in the ﬁrst 6 epochs, while the re-
maining epochs adopt an exponentially decreasing learning
rate from 5 × 10−3 down to 2 × 10−5.

At each training step, we sample a template bounding
box from B(cid:48) with θ1 = 0.4 and crop a template patch and
a search area as SiamFC [2]. This image pair is augmented

with horizontal and vertical ﬂips. For training the online
module with cycle memory, we input extra Nmem = 4
memory search areas together with the template-search pair.
Other hyper-parameters for cycle memory are set to γ =
2.5, θ2 = 0.45 and θ3 = 0.40. The trade-off parameter
in the loss function is ﬁxed to λ1 = 0.2 for naive Siamese
training, and for cycle memory we keep λ1 + λ2 = 0.9 and
gradually decrease λ1 from 0.3, in order to make the tracker
gently suit to the tracking task in longer temporal spans.
Inference. The inference is performed on both the ofﬂine
and online branches. The ofﬂine branch follows the conven-
tional inference methodology of Siamese networks [25, 48].
Based on the template feature from frame I1, the ofﬂine re-
sponse maps Rcls and bounding boxes Rreg in subsequent
frames are generated by the ofﬂine classiﬁer and regressor.
On the other hand, the online branch maintains a memory
queue of templates and dynamically updates this queue with
new predictions. In practice, when tracking the object in
frame It, the memory queue consists of totally Nq tem-
plates, including two ground-truth templates from frame I1
(i.e., the original template and its horizontal ﬂip), the latest
predicted template from frame It−1, and Nq − 3 histori-
cal templates with the highest scores in R from frame I2 to
It−2. The ﬁnal response map R is a weighted sum of Rcls
and Rmem, namely R = (1 − w)Rcls + wRmem, where
the weight w is set to 0.7 throughout our experiments.

4.2. State-of-the-art Comparison

We compare our method with the state-of-the-art un-
supervised and supervised trackers. The comparisons are
conducted on six benchmarks, including VOT2016 [20],
VOT2017/18 [22], VOT2020 [21], TrackingNet
[29],
OTB2015 [42] and LaSOT [11]. We denote by USOT the

Table 2: Results on the TrackingNet [29] dataset. The pro-
posed unsupervised trackers, USOT and USOT*, perform
well over the state-of-the-arts.

Trackers

Unsup.

Succ. ↑

Prec. ↑ Norm P. ↑

SiamFC [2]
MDNet [30]
UpdateNet [46]
ATOM [8]
SiamRPN++ [25]

KCF [17]
DSST [9]
ECO [7]

LUDT [38]
LUDT+ [38]

USOT (Ours)
USOT* (Ours)

No
No
No
No
No

Yes
Yes
Yes

Yes
Yes

Yes
Yes

57.1
61.4
67.7
70.3
73.3

44.7
46.4
56.1

54.3
56.3

59.9
61.5

53.3
55.5
62.5
64.8
69.4

41.9
46.0
48.9

46.9
49.5

55.1
56.6

66.3
71.0
75.2
77.1
80.0

54.6
58.8
62.1

59.3
63.3

68.2
69.1

proposed tracker trained with the unsupervised backbone
initialization [5], while denoting by USOT* that with su-
pervised ImageNet pretraining. It is worth mentioning that
we keep all the hyper-parameters ﬁxed throughout the ex-
periments to report results on all the datasets.
VOT2016. The VOT2016 dataset contains 60 video se-
quences. We adopt the Accuracy (A), Robustness (R) and
Expected Average Overlap (EAO) [20] as the VOT-toolkit
to evaluate the overall performance. Tab. 1 shows that the
proposed trackers, both USOT and USOT*, signiﬁcantly
outperform the state-of-the-art unsupervised trackers, with
respectively 5.2 and 10.3 points increase in EAO over the
top-performing unsupervised tracker LUDT+.
VOT2017/18. The VOT2017/18 dataset consists of 60 more
challenging video sequences. Tab. 1 shows that our USOT*
obtains 8.8 and 11.4 points increase respectively in Accu-
racy and EAO compared with LUDT+. Our USOT without
supervised backbone still outperforms LUDT+ by respec-
tively 7.4 and 6.0 points in Accuracy and EAO.
VOT2020. The VOT2020 dataset encodes targets with
segmentation masks, and updates the calculation of Accu-
racy (A), Robustness (R) and Expected Average Overlap
(EAO) [21]. Tab. 1 shows that our USOT* outperforms
SiamFC by 3.0, 9.8 and 4.0 points respectively in A, R and
EAO. Our USOT even achieves better performance com-
pared with USOT* with an EAO of 0.222.
TrackingNet.
The TrackingNet dataset contains over
30000 videos, with 511 videos for testing. Tab. 2 shows that
our USOT* outperforms LUDT+ with 5.2 points in Success
and 7.1 points in Precision. Our USOT is comparable with
USOT* on the TrackingNet dataset. This suggests that un-
supervised representation learning has the potential to per-
form as well as supervised ImageNet pretraining.
OTB2015. The OTB2015 dataset contains 100 videos.
Tab. 3 shows that our USOT outperforms USOT*. This

Figure 5: Success plot and Precision plot on the LaSOT
testing set [11].

afﬁrms the potential of unsupervised representation learn-
ing from scratch. Furthermore, the proposed unsupervised
trackers achieve comparable performance with LUDT and
SiamFC. Our trackers perform slightly worse than LUDT+,
because LUDT+ adopts some online tracking techniques in
[7] over LUDT for better performance.
LaSOT. The LaSOT testing set consists of 280 videos with
an average length over 2000. LaSOT is important for mea-
suring long-term tracking performance. Fig. 5 shows that
our USOT* signiﬁcantly outperforms LUDT+ with an in-
crease of 5.3 and 5.2 on Success and Precision, respectively.
Our USOT achieves comparable results to the supervised
trackers SiamFC [2] and SiamDW [47].

4.3. Ablation Studies

Training Stage Indispensability. We do extensive ablation
studies on the importance of different stages in the train-
ing phase. Experiments are conducted on the VOT2017/18
benchmark with USOT*. Tab. 4 shows that training a
naive tracker from single-frame pairs with random cropping
causes a signiﬁcant accuracy drop compared with our pro-
posed box sequence generation. In addition, directly con-
ducting cycle memory training without the naive Siamese
tracker initialization also causes a large performance drop.
Video Utilization Rate. We use the video quality score
Qv(I) in Eqn. 4 to ﬁlter out noisy sequences during unsu-
pervised training. On the GOT-10k dataset and the VID
dataset, we utilize 50.8% and 56.3% videos respectively
within all videos available for training, covering rich va-
rieties of unlabeled videos.
Frame Interval. Our proposed training method can learn
the appearance information across long intervals. This fa-
cilitates unsupervised trackers to adapt to temporal appear-
ance changes. Compared to the very short frame intervals
in previous deep unsupervised trackers S2SiamFC [34] (i.e.,
0 frame) and UDT [37] (i.e., < 10 frames), the training in-
stances sampled by our method possess an averaged long
frame interval of 41.1 and 64.6 respectively on the GOT-
10k and VID datasets.

Table 3: Results on the OTB2015 [42] dataset. Our unsupervised trackers achieve comparable results with the previous
supervised and unsupervised trackers.

Tracker

DCFNet
[39]

SiamFC
[2]

SiamRPN ATOM DiMP DSST KCF
[17]

[26]

[8]

[9]

[3]

LUDT
[38]

LUDT+ USOT USOT*
(Ours)
(Ours)

[38]

AUC success
Distance precision

58.0
-

58.2
77.1

63.7
85.1

66.7
87.9

68.6
89.9

51.8
68.9

48.5
69.6

60.2
76.9

63.9
84.3

58.9
80.6

57.4
77.5

Table 4: Ablation studies on our pseudo box generation
module and naive Siamese tracker initialization before cy-
cle memory training on the VOT2017/18 dataset.

Table 6: Ablation studies on training data. With more
unlabeled videos used for training, the proposed USOT*
achieves better results on the VOT2017/18 dataset.

Operations

A ↑

R ↓

EAO ↑

Training Data

Random boxes
Our gnereated boxes

w/o naive Siamese learning
w/ naive Siamese learning

0.488
0.567

0.575
0.578

0.646
0.520

0.389
0.304

0.195
0.263

0.306
0.344

Table 5: Quantitative results on the IoU success rates of the
pseudo boxes in template frames and memory frames.

Dataset \ IoU

Template

Memory

0.3

0.5

0.3

0.5

GOT-10k
VID

63.2% 45.5% 62.0% 43.8%
64.4% 42.1% 63.9% 42.0%

Pseudo Bounding Box Generation. To better investigate
the precision of the pseudo bounding boxes, we collect over
104 training instances and compute the IoU scores between
the output pseudo bounding boxes and the ground truth
bounding boxes on both the GOT-10k and VID datasets.
Tab. 5 shows the success rates of the pseudo bounding boxes
over different IoU scores in both template frames and mem-
ory frames. On both datasets, over 63% sampled boxes in
template frames cover at least parts of the foreground ob-
jects (IoU > 0.3), while over 42% sampled boxes in tem-
plate frames are precise enough to cover approximately the
entire objects (IoU > 0.5). Besides, from the small dif-
ference between the IoU success rates of pseudo boxes in
template frames and memory frames on both datasets, we
conclude that using large frame intervals for cycle memory
training only slightly decreases the reliability of memory
frames compared to template frames. This explains why
our unsupervised tracker can learn from large motions.
Training Dataset. Since most existing unsupervised deep
trackers are trained on the VID dataset, we investigate the
impact of training data on USOT* on the VOT2017/18
benchmark. As is shown in Tab. 6, when only using VID
as the training set, the proposed tracker still achieves 0.315
in EAO, with an 8.5 points increase over the state-of-the-art
unsupervised tracker LUDT+ (i.e., 0.230 in EAO). Besides,
our tracker beneﬁts from training on more unlabeled videos,
inferring the great potential of unsupervised tracking.

VID GOT-10k LaSOT YT-VOS

(cid:34)
(cid:34)
(cid:34)
(cid:34)

(cid:34)
(cid:34)
(cid:34)

(cid:34)
(cid:34)

(cid:34)

A ↑

R ↓

EAO ↑

0.576 0.337
0.587 0.323
0.579 0.328
0.578 0.304

0.315
0.320
0.337
0.344

Table 7: Parameter sensitivity of the length and weight of
the online memory queue on the VOT2017/18 dataset.

Nq \ w

0.3

5
6
7
8

0.289
0.294
0.310
0.302

0.5

0.302
0.312
0.318
0.300

0.6

0.323
0.312
0.336
0.319

0.7

0.313
0.329
0.344
0.341

0.8

0.323
0.322
0.331
0.338

Online Update. We study the parameter sensitivity of Nq
and w in the online memory module. Nq indicates the num-
ber of memorized features collected online in the memory
queue, while w indicates the weight for Rmem. Tab. 7
reports the EAO scores of USOT* on the VOT2017/18
dataset. The cooperation of ofﬂine and online modules with
w = 0.7 beneﬁts the proposed tracker most, and setting the
length of the memory queue Nq to 7 is most suitable.

5. Concluding Remarks

In this paper, we propose learning a robust tracker from
unlabeled videos from scratch. We ﬁrst generate candidate
box sequences to cover moving objects in videos. We then
train a naive Siamese tracker using single-frame pairs. We
ﬁnally continue training the naive tracker in longer tem-
poral spans with a novel cycle memory scheme, enabling
the tracker to update online. Extensive experiments demon-
strate that the proposed unsupervised tracker sets new state-
of-the-art unsupervised tracking results, and even performs
on par with recent supervised deep trackers. This work un-
veils the power of unsupervised learning for object tracking.

Acknowledgements. This work was supported by NSFC
(61906119, U19B2035), Shanghai Municipal Science
and Technology Major Project (2021SHZDZX0102), and
Shanghai Pujiang Program.

References

[1] Boris Babenko, Ming-Hsuan Yang, and Serge J. Belongie.
Robust object tracking with online multiple instance learn-
ing. TPAMI, 2011.

[2] Luca Bertinetto, Jack Valmadre, Jo˜ao F. Henriques, Andrea
Vedaldi, and Philip H. S. Torr. Fully-convolutional siamese
networks for object tracking. In ECCV Workshop, 2016.
[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu
Timofte. Learning discriminative model prediction for track-
ing. In ICCV, 2019.

[4] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu
Timofte. Know your surroundings: Exploiting scene infor-
mation for object tracking. In ECCV, 2020.

[5] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming
He. Improved baselines with momentum contrastive learn-
ing. arXiv:2003.04297, 2020.

[6] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang,
and Rongrong Ji. Siamese box adaptive network for visual
tracking. In CVPR, 2020.

[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. ECO: efﬁcient convolution operators for
tracking. In CVPR, 2017.

[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. ATOM: accurate tracking by overlap max-
imization. In CVPR, 2019.

[9] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and
Michael Felsberg. Accurate scale estimation for robust vi-
sual tracking. In BMVC, 2014.

[10] Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and
Reuven Y. Rubinstein. A tutorial on the cross-entropy
method. Ann. Oper. Res., 2005.

[11] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia
Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
Lasot: A high-quality benchmark for large-scale single ob-
ject tracking. In CVPR, 2019.

[12] Heng Fan and Haibin Ling. Siamese cascaded region pro-
posal networks for real-time visual tracking. In CVPR, 2019.
[13] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang.
Stmtrack: Template-free visual tracking with space-time
memory networks. In CVPR, 2021.

[14] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, 2017.

[15] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and
Shengyong Chen. Siamcar: Siamese fully convolutional
classiﬁcation and regression for visual tracking. In CVPR,
2020.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016.

[17] Jo˜ao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge
Batista. High-speed tracking with kernelized correlation ﬁl-
ters. TPAMI, 2015.

[18] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A
large high-diversity benchmark for generic object tracking in
the wild. TPAMI, 2019.

[19] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-
ing Jiang. Acquisition of localization conﬁdence for accurate
object detection. In ECCV, 2018.

[20] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,
Roman P. Pﬂugfelder, Luka Cehovin, Tom´as Voj´ır, Gustav
H¨ager, Alan Lukezic, Gustavo Fern´andez, et al. The visual
object tracking VOT2016 challenge results. In ECCV Work-
shop, 2016.

[21] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-
berg, Roman P. Pﬂugfelder, Joni-Kristian K¨am¨ar¨ainen, Mar-
tin Danelljan, Luka Cehovin Zajc, Alan Lukezic, Ondrej Dr-
bohlav, Linbo He, et al. The eighth visual object tracking
VOT2020 challenge results. In ECCV Workshop, 2020.
[22] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-
berg, Roman P. Pﬂugfelder, Luka Cehovin Zajc, Tom´as
Voj´ır, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,
Gustavo Fern´andez, ´Alvaro Garc´ıa-Mart´ın, ´Alvaro Iglesias-
Arias, et al. The sixth visual object tracking VOT2018 chal-
lenge results. In ECCV Workshop, 2018.

[23] Zihang Lai, Erika Lu, and Weidi Xie. MAST: A memory-

augmented self-supervised tracker. In CVPR, 2020.

[24] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie
Henderson, Richard E. Howard, Wayne E. Hubbard, and
Lawrence D. Jackel. Backpropagation applied to handwrit-
ten zip code recognition. Neural Comput., 1989.

[25] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,
and Junjie Yan. Siamrpn++: Evolution of siamese visual
tracking with very deep networks. In CVPR, 2019.

[26] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
High performance visual tracking with siamese region pro-
posal network. In CVPR, 2018.

[27] Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang,
Jan Kautz, and Ming-Hsuan Yang. Joint-task self-supervised
learning for temporal correspondence. In NeurIPS, 2019.
[28] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao
Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and
Feiyue Huang. Learning by analogy: Reliable supervision
from transformations for unsupervised optical ﬂow estima-
tion. In CVPR, 2020.

[29] Matthias M¨uller, Adel Bibi, Silvio Giancola, Salman Al-
Subaihi, and Bernard Ghanem. Trackingnet: A large-scale
dataset and benchmark for object tracking in the wild.
In
ECCV, 2018.

[30] Hyeonseob Nam and Bohyung Han. Learning multi-domain
convolutional neural networks for visual tracking. In CVPR,
2016.

[31] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In NeurIPS, 2015.

[32] David A. Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-
Hsuan Yang. Incremental learning for robust visual tracking.
IJCV, 2008.

[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael S. Bernstein, Alexander C. Berg,
and Fei-Fei Li. Imagenet large scale visual recognition chal-
lenge. IJCV, 2015.

[34] Chon-Hou Sio, Yu-Jen Ma, Hong-Han Shuai, Jun-Cheng
Chen, and Wen-Huang Cheng. S2siamfc: Self-supervised
fully convolutional siamese network for visual tracking. In
ACM MM, 2020.

[35] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao
Bao, Wangmeng Zuo, Chunhua Shen, Rynson W. H. Lau,
and Ming-Hsuan Yang. VITAL: visual tracking via adver-
sarial learning. In CVPR, 2018.

[36] Paul Voigtlaender, Jonathon Luiten, Philip H. S. Torr, and
Bastian Leibe. Siam R-CNN: visual tracking by re-detection.
In CVPR, 2020.

[37] Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei
Liu, and Houqiang Li. Unsupervised deep tracking.
In
CVPR, 2019.

[38] Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei
Liu, and Houqiang Li. Unsupervised deep representation
learning for real-time tracking. IJCV, 2021.

[39] Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, and
Weiming Hu. Dcfnet: Discriminant correlation ﬁlters net-
work for visual tracking. arXiv:1704.04057, 2017.

[40] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and
Philip H. S. Torr. Fast online object tracking and segmenta-
tion: A unifying approach. In CVPR, 2019.

[41] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learn-
ing correspondence from the cycle-consistency of time. In
CVPR, 2019.

[42] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-

ing benchmark. TPAMI, 2015.

[43] Ning Xu, Linjie Yang, Yuchen Fan,

Jianchao Yang,
Dingcheng Yue, Yuchen Liang, Brian L. Price, Scott Cohen,
and Thomas S. Huang. Youtube-vos: Sequence-to-sequence
video object segmentation. In ECCV, 2018.

[44] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and
Thomas S. Huang. Unitbox: An advanced object detection
network. In ACM MM, 2016.

[45] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.
Scott. Deformable siamese attention networks for visual ob-
ject tracking. In CVPR, 2020.

[46] Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer,
Martin Danelljan, and Fahad Shahbaz Khan. Learning the
model update for siamese trackers. In ICCV, 2019.

[47] Zhipeng Zhang and Houwen Peng. Deeper and wider
In CVPR,

siamese networks for real-time visual tracking.
2019.

[48] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and
Weiming Hu. Ocean: Object-aware anchor-free tracking. In
ECCV, 2020.

[49] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang
Ye, and Dongwei Ren. Distance-iou loss: Faster and better
learning for bounding box regression. In AAAI, 2020.
[50] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and
Weiming Hu. Distractor-aware siamese networks for visual
object tracking. In ECCV, 2018.

