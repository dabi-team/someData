0
2
0
2

n
u
J

5
1

]
T
G
.
s
c
[

1
v
2
4
7
8
0
.
6
0
0
2
:
v
i
X
r
a

Certifying Strategyproof Auction Networks

Michael J. Curry∗
curry@cs.umd.edu
Computer Science Department
University of Maryland
College Park, MD 20742

Tom Goldstein
tomg@cs.umd.edu
Computer Science Department
University of Maryland
College Park, MD 20742

Ping-Yeh Chiang∗
pchiang@cs.umd.edu
Computer Science Department
University of Maryland
College Park, MD 20742

John P. Dickerson
john@cs.umd.edu
Computer Science Department
University of Maryland
College Park, MD 20742

Abstract

Optimal auctions maximize a seller’s expected revenue subject to individual ra-
tionality and strategyproofness for the buyers. Myerson’s seminal work in 1981
settled the case of auctioning a single item; however, subsequent decades of work
have yielded little progress moving beyond a single item, leaving the design of
revenue-maximizing auctions as a central open problem in the ﬁeld of mechanism
design. A recent thread of work in “differentiable economics” has used tools
from modern deep learning to instead learn good mechanisms. We focus on the
RegretNet architecture, which can represent auctions with arbitrary numbers of
items and participants; it is trained to be empirically strategyproof, but the property
is never exactly veriﬁed leaving potential loopholes for market participants to
exploit. We propose ways to explicitly verify strategyproofness under a particular
valuation proﬁle using techniques from the neural network veriﬁcation literature.
Doing so requires making several modiﬁcations to the RegretNet architecture in
order to represent it exactly in an integer program. We train our network and
produce certiﬁcates in several settings, including settings for which the optimal
strategyproof mechanism is not known.

1

Introduction

Auctions are an important mechanism for allocating scarce goods, and drive billions of dollars of
revenue annually in online advertising [10], sourcing [32], spectrum auctions [7, 21], and myriad
other verticals [27, 31]. In the typical auction setting, agents who wish to bid on one or more items
are presumed to have private valuations of the items, which are drawn at random from some prior
valuation distribution. They then bid in the auction, possibly lying strategically about their valuation
while attempting to anticipate the strategic behavior of others. The result is a potentially complicated
Bayes-Nash equilibrium; even predicting these equilibria can be difﬁcult, let alone designing auctions
that have good equilibria.

This motivates the design of strategyproof auctions: these are auctions where players are incen-
tivized to simply and truthfully reveal their private valuations to the auctioneer. Subject to the
strategyproofness constraint, which makes player behavior predictable, it is then possible to optimize
the mechanism to maximize revenue. A classic strategyproof auction design is the second-price

∗equal contributions

Preprint. Under review.

 
 
 
 
 
 
auction—coinciding with the celebrated Vickrey-Clarke-Groves (VCG) mechanism [5, 15, 36]—in
which participants bid only once and the highest bidder wins, but the price paid by the winner is only
the amount of the second-highest bid.

In a groundbreaking work, Myerson [28] characterized the revenue-maximizing strategyproof auction
for selling one item to many bidders. However, there has been very little progress in characterizing
optimal strategyproof auctions in more general settings. Optimal mechanisms are known for some
cases of auctioning two items to a single bidder [8, 17, 24, 30]. The optimal strategyproof auction
even for 2 agents buying 2 items is still unknown.

The difﬁculty of designing optimal auctions has motivated attempts to formulate the auction design
problem as a learning problem. Duetting et al. [9] provide the general end-to-end approach which
we build on in this paper. In brief, they design a neural network architecture to encode an auction
mechanism, and train it on samples from the valuation distribution to maximize revenue. While
they describe a number of network architectures which work in restricted settings, we focus on their
RegretNet architecture, which can be used in settings with any number of agents or items.

The training procedure for RegretNet involves performing gradient ascent on the network inputs, to
ﬁnd a good strategic bid for each player; the network is then trained to minimize the difference in
utility between strategic and truthful bids—this quantity is the eponymous “regret”. The process is
remarkably similar to adversarial training [23], which applies robust optimization schemes to neural
networks, and the desired property of strategyproofness can be thought of as a kind of adversarial
robustness. Motivated by this connection, we use techniques from the adversarial robustness literature
to compute certiﬁable upper bounds on the amount by which a player can improve their utility by
strategically lying.

While the adversarial training approach seems effective in approximating strategyproof auction
mechanisms, neural network training is fraught with local minima and suboptimal stationary points.
One can discover strategic behaviors by using simple gradient methods on RegretNet auctions, but we
note that it is known from the adversarial examples literature that such results are often sub-optimal
[2] and depend strongly on the optimizer [37]. For this reason, it is unclear how strategyproof the
results of RegretNet training are, and how much utility can be gained through strategic behavior in
such auctions.

Our goal here is to learn auction mechanisms that are not only approximately strategyproof, but that
come with rigorous bounds on how much they can be exploited by strategic agents, regardless of
the strategy used. We achieve this by leveraging recent ideas developed for certifying adversarial
robustness of neural classiﬁers [4, 22, 35], and adapting them to work within an auction framework.

Our contributions.

• We initiate the study of certiﬁably strategyproof learned auction mechanisms. We see this as
a step toward achieving the best of both worlds in auction design—maintaining provable
properties while expanding to more general settings than could be analyzed by traditional
methods.

• We develop a method for formulating an integer-programming-based certiﬁer for general
learned auctions with additive valuations. This requires changes to the RegretNet archi-
tecture. We replace its softmax activation with the piecewise linear sparsemax [26], and
we present two complementary techniques for dealing with the requirement of individual
rationality: either formulating a nonconvex integer program, or removing this constraint
from the network architecture and adding it as a learned penalty instead.

• We provide the ﬁrst certiﬁable learned auctions for several settings, including a 2 agent,
2 item case where no known optimal auction exists; modulo computational scalability,
our techniques for learning auctions and producing certiﬁcates work for settings with any
number of items or (additive) bidders.

2 Background

Below, we describe the RegretNet approach for learning auction mechanisms, as well as the neural
network veriﬁcation techniques that we adapt to the auction setting. The RegretNet architecture
originated the idea of parameterizing mechanisms as neural networks and training them using tech-

2

niques from modern deep learning. This approach has been termed “differentiable economics”, and
several other papers have expanded on this approach in various settings beyond revenue-maximizing
sealed-bid auctions [12, 13, 25, 33, 34].

2.1 RegretNet

In the standard auction setting, it is assumed that there are n agents (indexed by i) buying k items
(indexed by j), and that the agents value the items according to values drawn from some distribution
P (vi). This distribution is assumed to be public, common knowledge (it is essentially the data-
generating distribution). However, the speciﬁc sampled valuations are assumed to be private to each
agent.

The auctioneer solicits a bid bij from all agents on all items. The auction mechanism f (b1, · · · , bn)
aggregates bids and outputs the results of the auction. This consists of an allocation matrix aij,
representing each player’s probability of winning each item, and a payment vector pi, representing
the amount players are charged. Players receive some utility ui based on the results; in this work, we
focus on the case of additive utility, where ui = (cid:80)
As previously mentioned, players are allowed to choose bids strategically to maximize their own
utility, but it is often desirable to disincentivize this and enforce strategyproofness. The auctioneer
also wants to maximize the amount of revenue paid. Duetting et al. [9] present the RegretNet
approach: the idea is to represent the mechanism f as a neural network, with architectural constraints
to ensure that it represents a valid auction, and a training process to encourage strategyproofness
while maximizing revenue.

j aijvij − pi.

(We note that [9] presents other architectures, RochetNet and MyersonNet, which are completely
strategyproof by construction, but only work in speciﬁc settings: selling to one agent, or selling only
one item. In our work, we focus only on certifying the general RegretNet architecture.)

2.1.1 Network architecture

The RegretNet architecture is essentially an ordinary feedforward network that accepts vectors of bids
as input and has two output heads: one is the matrix of allocations and one is the vector of payments
for each agent.

The network architecture is designed to make sure that the allocation and payments output are feasible.
First, it must ensure that no item is overallocated: this amounts to ensuring that each column of the
allocation matrix is a valid categorical distribution, which can be enforced using a softmax activation.

Second, it must ensure that no bidder (assuming they bid their true valuations) is charged more than
the expected value of their allocation. It accomplishes this by using a sigmoid activation on the
payment output head to make values lie between 0 and 1 – call these ˜pi. Then the ﬁnal payment for
each player is

˜pi; this guarantees that utility can at worst be 0.

(cid:16)(cid:80)

(cid:17)

j vijaij

Both of these architectural features pose problems for certiﬁcation, which we describe below.

2.1.2 Training procedure

The goal of the auctioneer is to design a mechanism that maximizes the expected sum of payments
received Ev∼P (v) [(cid:80)
i pi(v)], while ensuring that each player has low regret, deﬁned as the difference
in utility between the truthful bid and their best strategic bid:

rgti(v) = max

bi

ui(bi, v−i) − ui(vi, v−i)

(1)

Note that this deﬁnition of regret allows only player i to change their bid. However, if Ev[rgti(v)] is
low for all players then the mechanism must be approximately strategyproof; this is because every
possible strategic bid by players other than i could also be observed as a truthful bid from the support
of P (v).

[9] approximates regret using an approach very similar to adversarial training [23]. They deﬁne a
quantity (cid:100)rgti by approximately solving the maximization problem using gradient ascent on the input –

3

essentially ﬁnding an adversarial input for each player. Given this approximate quantity, they can
then deﬁne an augmented Lagrangian loss function to maximize revenue while forcing (cid:99)rgt to be
close to 0:

L(v, λ) = −

(cid:88)

pi +

(cid:88)

i

i

λi (cid:99)rgti(v) +

ρ
2

(cid:32)

(cid:88)

i

(cid:33)2

(cid:99)rgti(v)

(2)

They then perform stochastic gradient descent on this loss function, occasionally increasing the
Lagrange multipliers λ, ρ and recomputing (cid:99)rgt at each iteration using gradient ascent. At test time,
they compute revenue under the truthful valuation and regret against a stronger attack of 1000 steps.

2.2 Mixed integer programming for certiﬁable robustness

Modern neural networks with ReLU activations are piecewise linear, allowing the use of integer
programming techniques to verify properties of these networks. Bunel et al. [4] present a good
overview of various techniques use to certify adversarial robustness, along with some new methods.
The general approach they describe is to deﬁne variables in the integer program representing neural
network activations, and constrain them to be equal to each network layer’s output:

ˆxi+1 = Wixi + bi
xi+1 = max(0, ˆxi+1)

(3)

With input constraints x0 ∈ S representing the set over which the adversary is allowed to search,
solving the problem to maximize some quantity will compute the actual worst-case input. In most
cases, this is some proxy for the classiﬁcation error, and the input set is a ball around the true
input; in our case, computing a certiﬁcate for player i involves maximizing ui(bi, v−i) over all
bi ∈ Supp(P (vi)), i.e. explicitly solving (1).

The program is linear except for the ReLU term, but this can be represented by adding some auxiliary
integer variables. In particular, Tjeng et al. [35] present the following set of constraints (supposing a
d-dimensional layer output), which are feasible iff xi = max( ˆxi, 0):

δi ∈ {0, 1}d, xi ≥ 0,

xi ≤ uiδi
xi ≥ ˆxi, xi ≤ ˆxi − li(1 − δi)

(4)

The ui, li are upper and lower bounds on each layer output that are known a priori – these can be
derived, for instance, by solving some linear relaxation of the program representing the network. In
particular, an approach called Planet due to Ehlers [11] involves resolving the relaxation to compute
tighter bounds for each layer in turn. Bunel et al. [4] provide a Gurobi-based [16] integer program
formulation that uses the Planet relaxations, later updated for use by Lu and Kumar [22]; we modify
that version of the code for our own approach.

3 Techniques

These neural network veriﬁcation techniques cannot be immediately applied to the RegretNet archi-
tecture directly. We describe modiﬁcations to both the network architecture and the mathematical
programs that allow for their use: a replacement for the softmax activation that can be exactly
represented via a bilevel optimization approach, and two techniques for coping with the individual
rationality requirement. We also use a regularizer from the literature to promote ReLU stability,
which empirically makes solving the programs faster.

3.1 Sparsemax

The RegretNet architecture applies a softmax to the network output to produce an allocation distri-
bution where no item is overallocated. In an integer linear program, there is no easy way to exactly
represent the softmax. While a piecewise linear overapproximation might be possible, we elect

4

instead to replace the softmax with the sparsemax [26]. Both softmax and sparsemax project vectors
onto the simplex, but the sparsemax performs a Euclidean projection:

sparsemax(x) = arg min

z

1
2

(cid:107)x − z(cid:107)2

2 s. t. 1T z − 1 = 0, 0 < z < 1

(5)

([26] describes a cheap exact solution to this optimization problem and its gradient which are used
during training. We use a PyTorch layer provided in [19, 20].)

In order to encode this activation function in our integer program, we can write down its KKT
conditions and add them as constraints (a standard approach for bilevel optimization [6]), as shown
in (6).

These constraints are all linear, except for the complemen-
tary slackness constraints – however, these can be repre-
sented as SOS1 constraints in Gurobi and other solvers.

The payment head also uses a sigmoid nonlinearity; we
simply replace this with a piecewise linear function similar
to a sigmoid.

3.2 Enforcing individual rationality

(z − x) + µ1 − µ2 + λ1 = 0
z − 1 ≤ 0, −z ≤ 0, 1T z − 1 = 0
µ1 ≥ 0, µ2 ≥ 0
µ1(z − 1) = 0, µ2(−z) = 0

(6)

The RegretNet architecture imposes individual rationality – the requirement that no agent should pay
more than they win – by multiplying with a fractional payment head, so that each player’s payment is
always some fraction of the value of their allocation distribution.

When trying to maximize utility (in order to maximize regret), this poses a problem. The utility
for player i, with input bids bi, is ui(bi) = (cid:80)
j aijvij − pi. The value of the allocation is a linear
(cid:17)
combination of variables with ﬁxed coefﬁcients. But pi = ˜pi
– this involves products
of variables, which cannot be easily represented in standard integer linear programs.

j aijbij

(cid:16)(cid:80)

We propose two solutions: we can either formulate and solve a nonconvex integer program (with
bilinear equality constraints), or remove the IR constraint from the architecture and attempt to enforce
it via training instead.

Nonconvex integer programs The latest version of Gurobi can solve programs with bilinear
optimality constraints to global optimality. By adding a dummy variable, we can chain together two
such constraints to represent the ﬁnal payment: pi = ˜piy, and yi = (cid:80)
j aijbij. It is desirable to
enforce IR constraints at the architectural level, but as described in the experiments section, it can
potentially be much slower.

Individual rationality penalty As opposed to constraining the model architecture to enforce indi-
vidual rationality constraint, we also experiment with enforcing the constraint through an additional
term in the Lagrangian (a similar approach was used in an earlier version of [9]). We can compute
the extent to which individual rationality is violated:

irvi = max(pi −

(cid:88)

j

aibi, 0)

(7)

We then allow the network to directly output a payment, but add another penalty term to encourage
individual rationality:

L(v, λ, µ) = −

(cid:88)

pi +

(cid:88)

i

i

λi (cid:99)rgti(v) +

ρ
2

(cid:32)

(cid:88)

i

(cid:33)2

(cid:99)rgti(v)

(cid:88)

+

µi irv2
i

i

(8)

With this approach, the ﬁnal payment no longer involves a product between allocations, bids, and
payment head, so the MIP formulation does not have any quadratic constraints.

5

Distillation loss Training becomes quite unstable after adding the individual rationality penalty; we
stabilize the process using distillation [18]. Speciﬁcally, we train a teacher network using the original
RegretNet architecture, and use a mean squared error loss between the student and the teacher’s
output to train our network. The teacher may have an approximately correct mechanism, but is
difﬁcult to certify; using distillation, we can train a similar student network with an architecture more
favorable for certiﬁcation.

We allow the payments to vary freely during training, to avoid vanishing gradients, and simply clip
the payments to the feasible range after the training is done. Through this method, empirically, we
are able to train student networks that are comparable to the teachers in performance.

3.3 Regularization for fast certiﬁcates

Xiao et al. [38] point out that a major speed limitation in integer-programming based veriﬁcation
comes from the need to branch on integer variables to represent the ReLU nonlinearity (see Equation
4). However, if a ReLU unit is stable, meaning its input is always only positive or only negative, then
there is no need for integer variables, as its output is either linear or constant respectively.

We adopt the approach in [38], which at train time uses interval bound propagation [14] to compute
loose upper and lower bounds on each activation, and adds a regularization term − tanh(1 + ul) to
encourage them to have the same sign. At veriﬁcation time, variables where upper and lower bounds
(computed using the tighter Planet relaxation) are both positive or both negative do not use the costly
formulation of Equation 4.

4 Experiments

We experiment on two auction settings: 1 agent, 2 items, with valuations uniformly distributed on
[0, 1] (the true optimal mechanism is derived analytically and presented by [24]); and 2 agents, 2
items, with valuations uniformly distributed on [0, 1], which is unsolved analytically but shown to be
empirically learnable in [9].

For each of these settings, we train 3 networks:

• A network with a sparsemax allocation head which enforces individual rationality using the

fractional payment architecture, and uses the ReLU stability regularizer of [38]

• The same architecture, without ReLU regularization

• A network that does not enforce IR, trained via distillation on a network with the original

RegretNet architecture

Additionally, to investigate how solve time scales for larger auctions, we consider settings with up to
3 agents and 3 items for the architecture without IR enforcement. All training code is implemented
using the PyTorch framework [29].

4.1 Training procedure

We generate 600,000 valuation proﬁles as training set and 3,000 valuation proﬁles as the testing
set. We use a batch size of 20,000 for training, and we train the network for a total of 1000 epochs.
At train time, we generate misreports through 25 steps of gradient ascent on the truthful valuation
proﬁles with learning rate of .02; at test time, we use 1000 steps. Architecturally, all our networks
use a shared trunk followed by separate payment and allocation heads; we ﬁnd the use of a shared
embedding makes the network easier to certify. We generally use more layers for larger auctions, and
the detailed architectures, along with hyperparameters of the augmented Lagrangian, are reported in
Appendix A.

4.2 Results

Our results for regret, revenue and solve time are summarized in Table 1. We show the relationship
between truthful and strategic bids for a learned 1 agent, 2 item mechanism in Figure 1.

6

Auction
Setting

IR

Relu
Reg.

Solve
time (s)

Revenue

Empirical
Regret

Certiﬁed
Regret

Emp./Cert.
Regret

0.593 (0.404)
0.569 (0.390)
0.568 (0.398)
0.876 (0.286)
—
0.874 (0.285)

25.6 (72.0)
7.2 (17.5)
0.034 (0.007)
13.9 (37.0)
17.4 (51.9)
5.8 (16.3)
7.520 (24.2) —
5.480 (5.577)
2.495 (2.271) —

1x2
Yes No
1x2
Yes Yes
1x2
No
Yes
Yes No
2x2
2x2 (2nd) Yes No
2x2
Yes Yes
2x2 (2nd) Yes Yes
Yes
2x2
No
Yes
2x2 (2nd) No
Table 1: Summary of experimental results. Empirical regret is computed on 3000 random points and
certiﬁed regret is computed on 1000 different points. (2nd) denotes the second agent in a multi-agent
auction. Note that average empirical regret is only about 60-80% of the average true regret. The
number in the parenthesis represents the standard deviation.

0.019 (0.016)
0.004 (0.003)
0.011 (0.004)
0.014 (0.016)
0.011 (0.013)
0.013 (0.015)
0.012 (0.014)
0.011 (0.011)
0.017 (0.017)

0.014 (0.012)
0.003 (0.002)
0.009 (0.005)
0.009 (0.013)
0.007 (0.011)
0.008 (0.012)
0.008 (0.012)
0.006 (0.007)
0.011 (0.010)

0.731
0.700
0.839
0.637
0.676
0.626
0.680
0.533
0.666

0.882 (0.334)

Figure 1: For the 1 agent, 2 item setting (regularized, IR enforced), this plot shows truthful bids (blue
circle), with an arrow to the best strategic bid computed by the certiﬁer (red ﬁlled). Only points with
regret at least 0.005 are shown; the size of markers is proportional to the magnitude of regret. While
the truthful and strategic bids are often far apart, this does not necessarily mean that violations of
strategyproofness are large; in this plot, the highest regret of any point is still only 0.014.

Regret certiﬁcate quality We are able to train and certify networks with reasonably low regret –
usually less than one percent of the maximum utility an agent can receive in these settings. Although
mean regrets are small, the distributions are right skewed (particularly in the 2 agent, 2 item case)
and there are a few points with high (approximately 0.1) regret. Crucially, we ﬁnd that our certiﬁed
regrets tend to be larger on average than PGD-based empirical regret, suggesting that our method
reveals strategic behaviors that gradient-based methods miss.

Trained revenue As a baseline we consider the mean revenue from selling each item individually
in a Myerson auction – in the 1 agent 2 item setting, this is 0.5; in the 2 agent 2 item setting, it is
0.8333. Our trained revenues exceed these baselines. For the 1 agent 2 item settings, the optimal
mechanism [24] has a revenue of 0.55; our mechanisms can only exceed this because they are not
perfectly strategyproof.

Individual rationality Empirically, the individual rationality penalty is very effective. On average,
less than 5.53% of points give an allocation violating the individual rationality constraint, and even if

7

0.00.20.40.60.81.0Item 1 Valuation0.00.20.40.60.81.0Item 2 ValuationFigure 2: Certiﬁed regret and solve time for 1000 random points for IR and non-IR network
architectures (regularized). Maximum utility in these settings is 2.0, so regrets are relatively small in
most regions. At points with high regret, our certiﬁcates are able to detect this deﬁciency.

it is violated, the magnitude of violation is on average less than .0002 (Table 4, Appendix B). Filtering
out IR-violating points after the fact results in lower revenue but by less than one percent.

Solve time The time required to solve the MIP is also quite important. In general, we ﬁnd that
ReLU stability regularization helps speed up solve time, and that solving the bilinear MIP (required
for architectural enforcement of IR) is much harder than solving the mixed-integer linear program for
the other architecture.

Auction setting Mean solve time (s)

Solve time std Regret Regret std

2x3
3x2
3x3

160.66
5.039
71.81

142.86
3.40
54.24

0.0342
0.0209
0.0243

0.0169
0.0152
0.0204

Table 2: Solve times and regrets for non-IR architecture without clipped payments in larger settings on
250 random points. In general, increasing the number of items signiﬁcantly slows down certiﬁcation.

To investigate scalability, we also consider solve times and certiﬁed regrets for settings with larger
numbers of agents and items; results are summarized in Table 2. Our experiments use the non-IR-
enforcing architecture; additionally, for these experiments we do not apply hard clipping of payments.
In general, increasing the number of items signiﬁcantly increases the solve time – this is not too
surprising, as increasing the number of items increases the dimensionality of the space that must be
certiﬁed (while the same is not true for increasing the number of agents, because certiﬁcates are for
one agent only). The larger solve time for 2 rather than 3 agents is harder to explain – it may simply
be the result of different network properties or a more complex learned mechanism.

We note that both solve time and regret are heavily right-skewed, as shown in Figure 2. We also
ﬁnd that the difference between allocations, payments, and utilities computed by the actual network
and those from the integer program is on the order of 10−6 – the constraints in the model correctly
represent the neural network.

5 Conclusion and Future Work

Our MIP solution method is relatively naive. Using more advanced techniques for presolving, and
specially-designed heuristics for branching, have resulted in signiﬁcant improvements in speed and
scalability for certifying classiﬁers [22, 35]. Our current work serves as strong proof-of-concept
validation that integer-programming-based certiﬁability can be useful in the auction setting, and it is
likely that these techniques could be applied in a straightforward way to yield a faster certiﬁer.

The performance of our learned mechanisms is also not as strong as those presented in [9], both in
terms of regret and revenue. It is unclear to us whether this is due to differences in the architecture or
to hyperparameter tuning. We observe that our architecture has the capacity to represent the same
class of functions as RegretNet, so we are hopeful that improved training might close the gap.

8

1x2 distill1x2 IR2x2 IR2x2 distill0.000.020.040.060.080.10Certified regret1x2 distill1x2 IR2x2 IR2x2 distill050100150200250Solve time (s)In addition to generalization bounds provided by Duetting et al. [9], other work has dealt with the
problem of estimating expected strategyproofness given only regret estimated on samples from the
valuation distribution [3]. The methods presented in this work for solving the utility maximization
problem have the potential to complement these bounds and techniques.

In this paper, we have described a modiﬁed version of the RegretNet architecture for which we can
produce certiﬁable bounds on the maximum regret which a player suffers under a given valuation
proﬁle. Previously, this regret could only be estimated empirically using a gradient ascent approach
which is not guaranteed to reach global optimality. We hope that these techniques can help both
theorists and practitioners have greater conﬁdence in the correctness of learned auction mechanisms.

Broader Impact

The immediate social impact of this work will likely be limited. Learned auction mechanisms are
of interest to people who care about auction theory, and may eventually be used as part of the
design of auctions that will be deployed in practice, but this has not yet happened to our knowledge.
We note, however, that the design of strategyproof mechanisms is often desirable from a social
good standpoint. Making the right move under a non-strategyproof mechanism may be difﬁcult for
real-world participants who are not theoretical agents with unbounded computational resources. The
mechanism may impose a real burden on them: the cost of ﬁguring out the correct move. By contrast,
a strategyproof mechanism simply requires truthful reports—no burden at all.

Moreover, the knowledge and ability to behave strategically may not be evenly distributed, with the
result that under non-strategyproof mechanisms, the most sophisticated participants may game the
system to their own beneﬁt. This has happened in practice: in Boston, some parents were able to game
the school choice assignment system by misreporting their preferences, while others were observed
not to do this; on grounds of fairness, the system was replaced with a redesigned strategyproof
mechanism [1].

Thus, we believe that in general, the overall project of strategyproof mechanism design is likely to
have a positive social impact, both in terms of making economic mechanisms easier to participate in
and ensuring fair treatment of participants with different resources, and we hope we can make a small
contribution to it.

Acknowledgments

Dickerson and Curry were supported in part by NSF CAREER Award IIS-1846237, DARPA GARD,
DARPA SI3-CMD #S4761, DoD WHS Award #HQ003420F0035, and a Google Faculty Research
Award. Goldstein and his students were supported by the DARPA GARD and DARPA QED4RML
programs. Additional support was provided by the National Science Foundation DMS division, and
the JP Morgan Fellowship program.

References

[1] Atila Abdulkadiroglu, Parag Pathak, Alvin E Roth, and Tayfun Sonmez. Changing the boston
school choice mechanism. Working Paper 11965, National Bureau of Economic Research,
January 2006.

[2] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning (ICML), 2018.

[3] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Estimating approximate incentive

compatibility. In Economics and Computation (EC), 2019.

[4] Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and Pawan Kumar Mudigonda.
A uniﬁed view of piecewise linear neural network veriﬁcation. In Neural Information Processing
Systems (NeurIPS), 2018.

[5] Edward H Clarke. Multipart pricing of public goods. Public choice, pages 17–33, 1971.

9

[6] Benoît Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization.

Annals of Operations Research, 153(1):235–256, April 2007.

[7] Peter Cramton. The FCC spectrum auctions: An early assessment. Journal of Economics &

Management Strategy, 6(3):431–495, 1997.

[8] Constantinos Daskalakis, Alan Deckelbaum, and Christos Tzamos. Strong duality for a multiple-

good monopolist. In Economics and Computation (EC), pages 449–450, 2015.

[9] Paul Duetting, Zhe Feng, Harikrishna Narasimhan, David C. Parkes, and Sai Srivatsa Ravin-
dranath. Optimal auctions through deep learning. In International Conference on Machine
Learning (ICML), 2019.

[10] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the
generalized second-price auction: Selling billions of dollars worth of keywords. American
Economic Review, 97(1):242–259, 2007.

[11] Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In

Automated Technology for Veriﬁcation and Analysis. Springer, 2017.

[12] Zhe Feng, Harikrishna Narasimhan, and David C. Parkes. Deep learning for revenue-optimal
auctions with budgets. In International Conference on Autonomous Agents and MultiAgent
Systems (AAMAS), 2018.

[13] Noah Golowich, Harikrishna Narasimhan, and David C. Parkes. Deep learning for multi-facility
location mechanism design. In International Joint Conference on Artiﬁcial Intelligence (IJCAI),
2018.

[14] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy A. Mann, and Pushmeet Kohli. On the effectiveness of
interval bound propagation for training veriﬁably robust models. CoRR, abs/1810.12715, 2018.

[15] Theodore Groves. Incentives in teams. Econometrica: Journal of the Econometric Society,

pages 617–631, 1973.

[16] Gurobi Optimization, LLC. Gurobi optimizer reference manual, 2020. URL http://www.

gurobi.com.

[17] Nima Haghpanah and Jason D. Hartline. Reverse mechanism design. CoRR, abs/1404.1341,

2014.

[18] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural

network. CoRR, abs/1503.02531, 2015.

[19] Kris Korrel. sparsemax-pytorch, May 2020. URL https://doi.org/10.5281/zenodo.

3860669.

[20] Kris Korrel, Dieuwke Hupkes, Verna Dankers, and Elia Bruni. Transcoding compositionally:
Using attention to ﬁnd more generalizable solutions. In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019.

[21] Kevin Leyton-Brown, Paul Milgrom, and Ilya Segal. Economics and computer science of a
radio spectrum reallocation. Proceedings of the National Academy of Sciences (PNAS), 114
(28):7202–7209, 2017.

[22] Jingyue Lu and M. Pawan Kumar. Neural network branching for neural network veriﬁcation. In

International Conference on Learning Representations (ICLR), 2020.

[23] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018.

[24] Alejandro M. Manelli and Daniel R. Vincent. Bundling as an optimal selling mechanism for a

multiple-good monopolist. J. Econ. Theory, 127(1):1–35, 2006.

10

[25] Padala Manisha, CV Jawahar, and Sujit Gujar. Learning optimal redistribution mechanisms
through neural networks. In International Conference on Autonomous Agents and MultiAgent
Systems (AAMAS), 2018.

[26] André F. T. Martins and Ramón Fernández Astudillo. From softmax to sparsemax: A sparse
model of attention and multi-label classiﬁcation. In International Conference on Machine
Learning (ICML), 2016.

[27] Paul Milgrom. Discovering prices: auction design in markets with complex constraints.

Columbia University Press, 2017.

[28] Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):58–73,

1981.

[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Neural Information Processing Systems (NeurIPS).
2019.

[30] Gregory Pavlov. Optimal mechanism for selling two goods. The BE Journal of Theoretical

Economics, 11(1), 2011.

[31] Alvin E Roth. Marketplaces, markets, and market design. American Economic Review, 108(7):

1609–58, 2018.

[32] Tuomas Sandholm. Expressive commerce and its application to sourcing: How we conducted

$35 billion of generalized combinatorial auctions. AI Magazine, 28(3):45–45, 2007.

[33] Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan Hong, Zhi Guo,
Zongyao Ding, Pengjun Lu, and Pingzhong Tang. Reinforcement mechanism design, with
applications to dynamic pricing in sponsored search auctions. CoRR, abs/1711.10279, 2017.

[34] Andrea Tacchetti, DJ Strouse, Marta Garnelo, Thore Graepel, and Yoram Bachrach. A neural
architecture for designing truthful and efﬁcient auctions. CoRR, abs/1907.05181, 2019.

[35] Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference on Learning Representations (ICLR),
2019.

[36] William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of

ﬁnance, 16(1):8–37, 1961.

[37] Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Enhancing gradient-based attacks

with symbolic intervals. CoRR, abs/1906.02282, 2019.

[38] Kai Y. Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shaﬁullah, and Aleksander Madry. Train-
ing for faster adversarial robustness veriﬁcation via inducing relu stability. In International
Conference on Learning Representations (ICLR), 2019.

11

A Architectural and Training Details

We initialized the Lagrange multiplier of regret (λi) as 5, and update it every 6 batches, and we
experiment with values for the constant ρrgt ranging between 0.5 to 2 (reporting the choice that gave
the lowest regret). For the IR violation penalty, we initialize the Lagrange multiplier of IR violation
(µi) as 20, and update the Lagrange multiplier every 6 iterations. µ is initialized as 5, and then
incremented by 5 every 5 batches. For distillation, we take a mean squared error loss between the
student and teacher’s output, and use a multiplier of 1
400 . Speciﬁcally, the Lagrange multipliers are
updated as follows.

λi+1 = λi + ρrgt
(cid:99)rgti
µi+1 = µi + ρirv irvi

ρrgt
i+1 = ρrgt
i+1 = ρirv
ρirv

i + ρrgt
inc
i + ρirv
inc

Auction Setting

Inner Product Relu Stability Regularizer Embedding Layer

Yes
1 Agent x 2 Items
Yes
1 Agent x 2 Items
1 Agent x 2 Items
No
2 Agents x 2 Items Yes
2 Agents x 2 Items Yes
2 Agents x 2 Items No

No
Yes
Yes
No
Yes
Yes

1 hidden layer x 128 units
1 hidden layer x 128 units
1 hidden layer x 128 units
2 hidden layer x 128 units
2 hidden layer x 128 units
2 hidden layer x 128 units

B Additional Experimental Information

Hardware All certiﬁcation experiments were conducted on an AMD Ryzen 3600X CPU with
32GB RAM. Training of the network was conducted with a 2080 GPU on a university compute
cluster.

Additional experiments Table 4 shows more detailed results for the non-IR-enforcing architecture.
IR violations are relatively small, and ﬁltering out these cases (sacriﬁcing revenue) does not harm
overall revenue too much.

Table 3 shows the results of scaling experiments for settings with more agents and items, in a setting
where payment clipping is applied. Again, increasing the dimensionality of the input space by
increasing the number of items seems to impose a greater cost than increasing the number of agents.

Auction setting Mean solve time (s) Regret

2x3
3x2
3x3

109.749 (159.212)
3.033 (2.377)
59.173 (53.431)
Table 3: Solve times and regrets for non-IR architecture with clipped payments in larger settings on
250 random points. In general, increasing the number of items signiﬁcantly slows down certiﬁcation.
Standard deviations are in parentheses.

0.027 (0.016)
0.019 (0.016)
0.022 (0.020)

12

Auction
Setting

% of
IR violation

Max
IR violation

Mean
IR violation

Revenue before
enforcing IR

Revenue after
enforcing IR

5.53%
4.60%

0.0001 (0.0003)
1x2
2x2
0.0002 (0.0007)
Table 4: IR violation for the 1x2/2x2 auction settings. Note that the mean IR violation is small, and
revenue after enforcing IR drops only slightly. The number in parenthesis represents the standard
deviation.

0.0053
0.0083

0.5738
0.8874

0.5681
0.8824

13

