9
1
0
2

n
u
J

4

]
L
M

.
t
a
t
s
[

1
v
8
7
4
1
0
.
6
0
9
1
:
v
i
X
r
a

What do AI algorithms actually learn? – On
false structures in deep learning

Laura Thesing∗

Vegard Antun†

Anders C. Hansen∗ †

June 5, 2019

Abstract

There are two big unsolved mathematical questions in artiﬁcial intelligence (AI): (1)
Why is deep learning so successful in classiﬁcation problems and (2) why are neural nets
based on deep learning at the same time universally unstable, where the instabilities make
the networks vulnerable to adversarial attacks. We present a solution to these questions
that can be summed up in two words; false structures.
Indeed, deep learning does not
learn the original structures that humans use when recognising images (cats have whiskers,
paws, fur, pointy ears, etc), but rather diﬀerent false structures that correlate with the
original structure and hence yield the success. However, the false structure, unlike the
original structure, is unstable. The false structure is simpler than the original structure,
hence easier to learn with less data and the numerical algorithm used in the training will
more easily converge to the neural network that captures the false structure. We formally
deﬁne the concept of false structures and formulate the solution as a conjecture. Given that
trained neural networks always are computed with approximations, this conjecture can only
be established through a combination of theoretical and computational results similar to
how one establishes a postulate in theoretical physics (e.g. the speed of light is constant).
Establishing the conjecture fully will require a vast research program characterising the false
structures. We provide the foundations for such a program establishing the existence of the
false structures in practice. Finally, we discuss the far reaching consequences the existence
of the false structures has on state-of-the-art AI and Smale’s 18th problem.

1 Introduction

It is now well established through the vast literature on adversarial attacks [7, 8, 17, 19, 20, 21, 29,
31, 33, 36] (we can only cite a small subset here) on neural networks for image classiﬁcation that
deep learning provides highly successful, yet incredibly unstable neural networks for classiﬁcation
problems. Moreover, recently, the instability phenomenon has also been shown [2] for deep
learning in image reconstruction and inverse problems [1, 13, 15, 18, 30, 32, 40]. Thus, this
phenomenon of instability seems to be universal. What is fascinating is that despite intense
research trying to solve the instability issue [6, 10, 14, 16, 23, 37], the problem is still open.
As a result, there is a growing concern regarding the consequences of the instabilities of deep
learning methods in the sciences. Indeed, Science [9] recently reported on researchers warning
about potential fatal consequences in medicine due to the instabilities. Hence, we are left with
the following fundamental question:

Why are current deep learning methods so successful in image classiﬁcation, yet uni-
versally unstable and, hence, vulnerable to adversarial attacks?

In this paper we provide a radical conjecture answering the above question in classiﬁcation and
explaining why this problem will not be solved unless there is a fundamental rethink of how to
approach learning. We provide the ﬁrst steps towards such a theory.

∗Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK
†Department of Mathematics, University of Oslo, Norway

1

 
 
 
 
 
 
Figure 1: Image of a ﬁre truck with a horizontal blue line in the lower right corner, as suggested
in the thought experiment in Example 2.1. As discussed in the example, the blue line yields
many potential false structures.

Conjecture 1.1 (False structures in classiﬁcation). The current training process in deep learning
for classiﬁcation forces the neural network to learn a diﬀerent (false) structure and not the actual
structure of the classiﬁcation problem. There are three main components:

(i) (Success) The false structure correlates well with the original structure, hence one gets a

high success rate.

(ii) (Instability) The false structure is unstable, and thus the network is susceptible to adver-

sarial attacks.

(iii) (Simplicity) The false structure is simpler than the desired structure, and hence easier
to learn e.g. fewer data is needed and the numerical algorithm used in the training easily
converges to the neural network that captures the false structure.

Remark 1.2 (Structure). One can think of the word structure to mean the concept of what would
describe a classiﬁcation problem. Considering an image, we could think of what makes humans
recognise a cat or a ﬁre truck. In particular, a structure describing a cat would encompass all the
features that make humans recognise cats. However, classiﬁcation problems extend beyond image
recognition. For example, one may want to classify sound patterns or patters in meteorological
data, seismic data etc. Thus, we need a proper mathematical deﬁnition of what we mean by
structure and also false structure.

1.1 Smale’s 18th problem

Based on a request from V. Arnold, inspired by Hilbert’s list of mathematical problems for the
20th century, S. Smale created a list of mathematical problems for the 21st century [28]. The
last problem on this list is more relevant than ever and echoes Turing’s paper from 1950 [35]
on the question of artiﬁcial intelligence. Turing asks if a computer can think, and suggests the
imitation game as a test for his question about artiﬁcial intelligence. Smale takes the question
even further and asks in his 18th problem the following:

“ What are the limits of intelligence, both artiﬁcial and human?”

— Smale’s 18th problem (from mathematical problems for the 21st century [28]).

Smale formulates the question in connection to foundations of computational mathematics and
discusses diﬀerent models of computation appropriate for the problem [5, 34]. The results in

2

this paper should be viewed in connection with Smale’s 18th problem and the foundations
of computation (see §2.2). Our contribution is part of a larger program on foundations of
computational mathematics and the Solvability Complexity Index hierarchy [3, 4, 11, 12, 22, 25]
established to determine the boundaries of computational mathematics and scientiﬁc computing.

2 Why the concept of false structures is needed

To illustrate the concept of false structure and the conjecture we continue with a thought ex-
periment.

Example 2.1 (A thought experiment explaining false structures and Conjecture 1.1). Suppose
a human is put in a room in a foreign country with an unknown language. The person is to be
trained to recognise the label ”ﬁre truck”, and in order to do so, she is given a large complex
collection of images with diﬀerent types of ﬁre trucks and images without ﬁre trucks. Each time
an image with a ﬁre truck is shown to the person, she hears the foreign language word for ﬁre
truck. In particular, the person is trying to learn the function f : M Ñ t0, 1u, where 1 means
that there is a ﬁre truck in the image, and M denotes a large set of images. We will refer to f
as the original structure. However, on the set T Ă M of images shown in the training process
there is a small, but clearly visible, horizontal blue line on each of the images showing a ﬁre
truck (this is visualised in Figure 1). On the images without ﬁre trucks there is no line. The
question is: will the person think that the foreign language word for ﬁre truck means horizontal
blue line, blue line, line or actual ﬁre truck. This is an example of the original structure f
(describing ﬁre trucks) and three false structures g1 (horizontal blue line), g2 (blue line, not
necessary horizontal), and g3 (line, colour and geometry are irrelevant). All of them could have
been learned from the same data. However, the structure or false structure that the person has
learned will yield wildly diﬀerent results on diﬀerent tests:

(i) (False structure g1): Suppose the person learns the false structure g1 describing the hor-
izontal blue-line structure that is chosen. Suppose also that the test set of images is chosen
such that every image containing a ﬁre truck also has a horizontal blue line. On this test set
one will have 100% success, yet the false structure g1 will give incredible instabilities in several
diﬀerent ways. First, a tiny perturbation in terms of removing the blue line will yield a miss
classiﬁcation. Second, a slight rotation of the blue line would mean wrong output, and a slight
change in colour will result in an incorrect decision. Thus, there are at least three types of
adversarial attacks that would succeed in reducing the success rate from 100% to 0% on the
test set.

(ii) (False structure g3): Suppose it is just the line structure described by the false structure
g3 that is learned. Repeating the experiment in (i) with all images in the test set containing
a ﬁre truck also having a small visible green line would yield the same success as in (i) as well
as instabilities, however, now rotations of the line would not have an eﬀect nor changes in the
colour. Thus, two less adversarial attacks would be successful. If we did the experiment with
the false structure g2, there would have been at least two forms of adversarial attacks available.

(iii) (The original structure f ): If the actual ﬁre truck structure, described by f , was cho-
sen, one would be as successful and stable as would be expected from a human when given any
test set. Note that the original structure f is much more complex and likely harder to learn
than the false structures g1, g2, g3.

Motivated by the above thought experiment we can now formally deﬁne the original structure

and false structures.

Deﬁnition 2.2 (The original structure and false structures). Consider M Ă Rd and a string of
unique (see Remark 2.3) predicates L “ tα0, . . . , αN u on M, where N P N such that for each
x P M there is a unique αj P L such that αjpxq “ true. For such x deﬁne f pxq “ j. We say that
the pair pf, Lq is the original structure on M. A false structure for pf, Lq relative to T Ĺ M is

3

a pair pg, L1q, where L1 “ tβ0, . . . , βN u a string of unique predicates with g : M Ñ t0, . . . , N u
such that gpxq “ j iﬀ βjpxq “ true. Moreover, βj ‰ αj for all j P t0, . . . , N u and

gpxq “ j iﬀ f pxq “ j @ x P T ,

as well as

C “ tx P MzT | f pxq “ i, gpxq “ j, for some i ‰ ju ‰ H.

(1)

(2)

We say that g is a partial false structure if αj ‰ βj for at least two diﬀerent j P t0, . . . , N u (as
opposed to all).

Remark 2.3 (Unique predicates). By unique predicates L “ tα0, . . . , αN u we mean that the
support of the characteristic functions induced by the predicates in L do not intersect.

Remark 2.4 (How bad is the false structure?). Note that Deﬁnition 2.2 does not consider how
’far’ the false structure g is from the original structure f . This is beyond the scope of this paper,
however, this can easily be done. For example, suppose M is equipped with some probability
measure P , then assuming C from (2) is measurable, P pCq would indicate how severe it would
be to learn the false structure instead of the original structure.

The motivation behind the idea of a false structure can be understood as follows. Sup-
pose one is interested in learning the original structure pf, Lq as in Deﬁnition 2.2.
In Ex-
ample 2.1 the list L “ tα0, α1u of predicates are α1pxq “ x demonstrates a ﬁre truck, and
α0pxq “ x does not demonstrate a ﬁre truck. In order to learn pf, Lq we have a training set
T Ă M. However, if we have a false structure relative to T , as in Example 2.1, pg1, L1q where
L1 “ tβ0, β1u with

β1pxq “ x demonstrates a horizontal blue line,
β0pxq “ x does not demonstrate a horizontal blue line,

how do we know that we have not learned pg1, L1q instead? In Example 2.1 there are three
diﬀerent false structures, each with its diﬀerent instability issues.

Remark 2.5 (Formulation of the predicate). By ”x demonstrates a z” (where z was a horizontal
blue line above) in the previous predicate we mean that the main object showing in x is z, and
that there is only one main object. The word demonstrate is slightly ambiguous, however, for
simplicity we use this formulation.

Example 2.1 illustrates the issues in Conjecture 1.1 very simply.

Indeed, a simple false
structure could give great performance yet incredible instabilities. Let us continue with the
thought experiment, however, now we will replace the human in Example 2.1 with a machine,
and in particular, we consider the deep learning technique.

Example 2.6. We consider the same problem as in Example 2.1, however, we replace the human
by a neural network that we shall train. Indeed, we let f : M Ă Rd Ñ t0, 1u be the function
deciding if there is a ﬁre truck in the image, where M is as in Example 2.1. The training set
T “ tx1, . . . , xru and test set C “ ty1, . . . , ysu consists of images xj and yj with and without
ﬁre trucks. However, all ﬁre truck images also contain a small blue horizontal line, and there is
no blue line in the images without ﬁre trucks. We choose a cost function C, a class of neural
networks N N and approximate the optimisation problem

Ψ P argmin
ΦPN N

Cpv, wq where vj “ Φpxjq, wj “ f pxjq for 1 ď j ď r.

(3)

The question is now, given that there are three false structures pg1, L1
predicates in L1

2q, pg3, L1
j would come from the description in Example 2.1) that would also ﬁt (3):

1q, pg2, L1

3q (the

Why should we think that the trained neural network has picked up the correct struc-
ture, and not any of the the false structures?

Remark 2.7 (Simplicity). Note that there could be a minimiser Ψ of (3) such that Ψ “ f ,
however, this minimiser may be hard to reach and hence one ﬁnds another minimiser ˜Ψ of (3)
such that ˜Ψ “ g1, say. We will see examples of this phenomenon below.

4

2.1 Support for the conjecture and how to establish it

Unlike common conjectures in mathematics, Conjecture 1.1 can never be proven with standard
mathematical tools. The issue is that all neural networks that are created are done so with
inaccurate computations. Thus, actual minimisers are rarely found, if ever, but rather approx-
imations in one form or another. Thus Conjecture 1.1 should be treated more like a postulate
in theoretical physics, like ’the speed of light is constant’. One can never establish this with
a mathematical proof, however, mathematical theory and experiments can help support the
postulate.

Note that there is already an overwhelming amount of numerical evidence that Conjecture
1.1 is true based on the myriad of experiments done over the last ﬁve years. Indeed, we have
the following documented cases: (I) Unrecognizable and bizarre structures are labeled as natural
images [21]. Trained successful neural nets classify unrecognizable and bizarre structures as
natural images with standard labels with high conﬁdence. Such mistakes would not be possible
if the neural network actually captured the correct structure that allows for image recognition in
the human brain. (II) Perturbing one pixel changes the label [31, 36]. It has been veriﬁed that
trained and successful networks change the label of the classiﬁcation even when only one pixel
is perturbed. Clearly, the structures in an image that allows for recognition by humans are not
aﬀected by a change in a single pixel. (III) Universal invisible perturbations change more than
90% of the labels [19, 20]. The DeepFool software [8] demonstrates how a single almost invisible
perturbation to the whole test set dramatically changes the failure rate. Diﬀerent structures in
images, allowing for successful human recognition, are clearly not susceptible to misclassiﬁcation
by a single near-invisible perturbation. However, the false structure learned through training is
clearly unstable.

There is quite a bit of work on establishing which part of the data is crucial for the decision of
the classiﬁer [24, 26, 27, 38, 39]. This is a rather diﬀerent program compared to establishing our
conjecture. Indeed, our conjecture is about the unstable false structures. However, one should
not rule out that there might be connections that could help detecting and understanding the
false structures.

2.2 Consequences of Conjecture 1.1

The correctness of Conjecture 1.1 may have several consequences both negative and positive.
Negative consequences:

(i) The success of deep learning in classiﬁcation is not due to networks learning the structures
that humans associate with image recognition, but rather that the network picks up unstable
false structures in images that are potentially impossible for humans to detect. This means
that instability, and hence vulnerability to adversarial attacks, can never be removed until one
guarantees that no false structure is learned. This means a potential complete overhaul of
modern AI.

(ii) The success is dependent of the simple yet unstable structures, thus the AI does not
capture the intelligence of a human.

(iii) Since one does not know which structure the network picks up, it becomes hard to conclude
what the neural network actually learns, and thus harder to trust its prediction. What if the
false structure gives wrong predictions?

Positive consequences:

(i) Deep learning captures structures that humans cannot detect, and these structures require
very little data and computing power in comparison to the true original structures, however,
they generalise rather well compared to the original structure. Thus, from an eﬃciency point
of view, the human brain may be a complete overkill for certain classiﬁcation problems, and
deep learning ﬁnds a mysterious eﬀective way of classifying.

(ii) The structure learned by deep learning may have information that the human may not
capture. This structure could be useful if characterised properly. For example, what if there is

5

structural information in the data that allows for accurate prediction that the original structure
could not do?

Consequences - Smale’s 18th problem:

(i) Conjecture 1.1 suggests that there is a fundamental diﬀerence between state-of-the-art AI
and human intelligence as neural networks based on deep learning learn completely diﬀerent
structures compared to what humans learn. Hence, in view of Smale’s 18th problem, correct-
ness of Conjecture 1.1 implies both limitations of AI as well as human intelligence. Indeed,
the false unstable structures learned by modern AI limits its abilities to match human intel-
ligence regarding stability. However, in view of the positive consequences mentioned above,
correctness of Conjecture 1.1 implies that there is a limitation to human intelligence when it
comes to detecting other structures that may provide diﬀerent information than the structure
detected by humans.

3 Establishing Conjecture 1.1 - Do false structures exist

in practice?

Our starting point for establishing Conjecture 1.1 is Theorem 3.1 below, for which the proof cap-
tures all the three components of the conjecture. We will demonstrate how this happens in actual
computations. To introduce some notation, we let N N N,L, with N “ pNL, NL´1, . . . , N1, N0q
denote the set of all L-layer neural networks. That is, all mappings Φ : CN0 Ñ CNLof the form

Φpxq “ WLpρpWL´1pρp. . . ρpW1pxqqqqqq,

with x P RN0, where the Wjs are aﬃne maps with dimensions given by N, and ρ is a ﬁxed
non-linear function acting component-wise on a vector. We consider a binary classiﬁer f : M Ă
Rd Ñ t0, 1u, where M is some subset. To make sure that we consider stable problems we deﬁne
the family of well separated and stable sets S f

δ with separation at least δ ą 0:

S f
δ “ ttx1, . . . , xru | }xj} ď 1, min
i‰j

}xi ´ xj}8 ě δ, f pxj ` yq “ f pxjq for }y}8 ă δu.

(4)

Moreover, the cost function C used in the training is in

CF :“ tC : RrNL ˆ RrNL Ñ R` : Cpv, wq “ 0 iﬀ v “ wu.

(5)

Theorem 3.1 (Bastounis, Hansen, Vlacic [3]). There is an uncountable family F of classi-
ﬁcation functions f : RN0 Ñ t0, 1u such that for each f P F and neural network dimen-
sions N “ pNL, NL´1, . . . , N0q with N0, L ě 2, any (cid:15) ą 0, and any integers s, r with r ě
3pN1 ` 1q ¨ ¨ ¨ pNL´1 ` 1q, there exist uncountably many non-intersecting training sets T “
tx1, . . . , xru P S f
εpr`sq of size r (where εpnq :“ rp4n ` 3qp2n ` 2qs´1) and uncountably many
non-intersecting classiﬁcation sets C “ ty1, . . . , ysu P S f
εpr`sq of size s such that we have the
following. For every C P CF there is a neural net

Ψ P argmin
ΦPN N N,L

Cpv, wq where vj “ Φpxjq, wj “ f pxjq for 1 ď j ď r

(6)

such that Ψpxq “ f pxq @x P T Y C. However, there exist uncountably many v P RN0 such that

|Ψpvq ´ f pvq| ě 1{2,

}v ´ x}8 ď (cid:15) for some x P T .

Moreover, there exists a stable neural network

ˆΨ R N N N,L with ˆΨpxq “ f pxq, @ x P B8

εpr`sqpT Y Cq,

where B8

εpr`sqpT Y Cq denotes the εpr ` sq neighbourhood, in the l8 norm, of T Y C.

(7)

(8)

6

The message of Theorem 3.1 and its proof [3] (which serves as a basis for §3.1) is summarised

below.

(i) (Success) The successful neural network learns a false structures that correlates well with
the true structure of the problem and hence the great success (100% success on an arbitrarily
large test set).

(ii) (Instability) The false structure is completely unstable despite the original problem being
stable (see S f
δ in (4)). Because of the training process of seeking a minimiser of the optimisation
problem, the neural network learns the false structure, and hence becomes completely unstable.
Indeed, it will fail on uncountably many instances that are (cid:15)-away from the training set.
Moreover, (cid:15) can be made arbitrarily small.

(ii) (Simplicity) The false structure is very simple and easy to learn. Moreover, paradoxically,
there exists another neural network with diﬀerent dimensions that becomes stable and has the
same success rate as the unstable network, however, there is no known way to construct this
network.

Theorem 3.1 and its proof provides the starting point in the program on establishing Conjecture
1.1. However, the missing part is to show that: the false structure is learned in practice, it is
much easier to learn than the original structure, and that the original structure is very unlikely
to be learned in the presence of the false structure in the training set. This is done in the next
section.

3.1 Establishing the conjecture: Case 1

We will ﬁrst start by demonstrating that Conjecture 1.1 is true in the many unique classiﬁcation
cases suggested by Theorem 3.1. A simple example is the inﬁnite class of functions

fa : M Ñ t0, 1u , M “ rb, 1s ˆ r0, 1s,

fapxq “

V

R

a
x1

mod 2,

(9)

for a ą 0 and 0 ă b ă 1. Consider the predicates

α0pxq “

V

R

a
x1

is odd,

α1pxq “

V

R

a
x1

is even.

a

Let L “ tα1, α2u then pfa, Lq is the original structure, as in Deﬁnition 2.2. We note that fa is
k , where k P N, and hence may be viewed as a very
constant on each of the intervals
simple classiﬁcation problem: given x, is fapxq “ 1 or fapxq “ 0? To simplify the learning and
analysis further we will assume that a P N, a ă K for some chosen K P N, and we let b “ a
K`1 .
This means that fa has K ´ a jump discontinuities on the interval rb, 1q. To ensure fa is stable
with respect to perturbations of size (cid:15) ą 0 on its input, we will ensure that each of our samples
of fa lies at least (cid:15) away from each of these jump discontinuities. Hence, we deﬁne

k`1 ď x1 ă a

S(cid:15) “

ˆ

Kď

k“a

a
k ` 1

` (cid:15),

a
k

˙

´ (cid:15)

.

(10)

choose the samples from the set S(cid:15) ˆ r0, 1s. This is similar to (4) used in Theorem 3.1. To avoid
that S(cid:15) is a union of empty sets we will always assume that (cid:15) ă b2{p2pa ´ bqq. It will not be a
goal in itself to learn fa for any x, but rather to learn the right value of fa within the (cid:15)-stable
region S(cid:15). Indeed, given that this is a decision problem, inputs close to the boundary are always
going to be hard to classify. However, the decision problem stays stable on S(cid:15) ˆ r0, 1s.

For the learning task we now consider two sets of size r,

0 “ tpxi
T r

1, 0qur

i“1,

δ “ tpxi
T r

1, xi

2qur

i“1, xi

2 “ δfapxi

1q,

xi
1 P S(cid:15),

(11)

where 0 ă δ ă (cid:15). Note that T r

δ gives rise to a false structure as the next proposition shows.

7

T 7
δ

T 5000
0

T 5000
δ

T 10000
0

C0

Cδ

Figure 2:
(Experiment I) The graphs shows the output of fapxq and tσpΨipxqqs for x in the
two sets C0 (top row) and Cδ (bottom row). The networks Ψi, i “ 1, . . . , 4 have been trained on
the sets T 7

, respectively, and are shown from left to right.

δ , T 5000
0

, T 5000
δ

and T 10000
δ

Proposition 3.2. Consider the predicates β0 and β1 on M deﬁned by

β0pxq “ x2 is 0,

β1pxq “ x2 is not 0.

Note that for small δ, g becomes unstable on T r

Deﬁne g : M Ñ t0, 1u by gpxq “ 0 when β0pxq “ true and gpxq “ 1 when β1pxq “ true. Let
L1 “ tβ0, β1u. Then pg, L1q is a false structure for pfa, Lq relative to T r
δ .
δ . Moreover, the false structure pg, L1q appears
much simpler than the original structure fa. In order to train a neural network to learn the
original structure fa we choose the set of networks (architecture) to be N N N,2. That is all fully
connected 2-layer networks with dimensions N “ r1, 4K, 2s and non-linear function ρ : R Ñ R
being the ReLU function. What is crucial is that the set N N N,2 is rich enough to predict the
value of fa. By prediction we here mean that for a network φ the value of tσ ˝ φs, agrees with fa,
where σpxq “ 1{p1 ` expp´xqq, x P R is the sigmoid function, and t¨s means rounding to nearest
integer. For the function class N N N,2 above, it is indeed, possible to ﬁnd such a network whose
prediction agrees with fa on the stable area S(cid:15) ˆ r0, 1s in (10). This is formalised in the following
statement.

Proposition 3.3 (Existence of stable and accurate network). Let σpxq “ 1{p1`expp´xqq, x P R
be the sigmoid function, and let C : Rr ˆ Rr Ñ R be the cross entropy cost function for binary
classiﬁcation, that is

rÿ

Cpv, wq “

´wj logpσpvjqq ´ p1 ´ wjq logp1 ´ σpvjqq.

(12)

j“1

Let fa be as in (9). Then, for any η ą 0, there exists a two layer neural network Ψ P N N N,2
with N “ r1, 4K, 2s using the ReLU activation function, such that

fapxq “ tσpΨpxqqs,

x P S(cid:15) ˆ r0, 1s,

where t¨s denotes rounding to the closest integer, and for any subset T “ txp1q, . . . , xprqu Ă
S(cid:15) ˆ r0, 1s, v “ tΨpxquxPT and w “ tfapxquxPT we have Cpv, wq ď η.

In particular, Proposition 3.3 states, similarly to the last statement in Theorem 3.1, that
there is a stable and accurate network approximating fa that provides arbitrarily small values
of the cost function C. The problem, however, as we will see in the next experiment, is that it
is very unlikely to be found it in the presence of the false structure.

8

Φpx1q “ vertical

Φpx2q “ horizontal

Φpx3q “ vertical

Φpx4q “ horizontal

Φpx5q “ horizontal

Φpx6q “ vertical

Φpx7q “ horizontal

Φpx8q “ vertical

j“1 Ă ˜Cb,c (b “ 0.009, c “ 0.01), as
Figure 3: (Experiment II) Upper ﬁgures: Elements txju4
j“5 P ˆCb,c as well as the
well as the network classiﬁcations Φpxjq . Lower ﬁgures: Elements txju8
network classiﬁcations Φpxjq. Note how the network is completely unstable as the label changes
with perturbations that are not visible to the human eye.

3.1.1 Experiment I

The experiment is done as follows. We ﬁxed a “ 20, K “ 26, (cid:15) “ 10´2 and δ “ 10´4 and
trained four neural networks Ψ1, Ψ2, Ψ3, Ψ4 P N N N,2 with N “ r1, 4K, 2s and ReLU activation
function. The networks Ψi, i “ 1, . . . , 4 were trained on the sets T 7
and T 10000
,
0
respectively. For the set T 7
δ , we ensured that the ﬁrst components xi
1 were located in separate
intervals of S(cid:15). Otherwise it would be infeasible to learn fa from 7 samples. For the other sets
we distributed the ﬁrst components approximately equally between the disjoint intervals of S(cid:15).
To investigate which structure the networks had learned, we deﬁne the two sets,

δ , T 5000
0

, T 5000
δ

C0 “ tpx1, 0q | x1 P rb, 1su and Cδ “ tpx1, x2q | x1 P rb, 1s, x2 “ δfapx1qu.

(13)

0

We have plotted fapxq and tσpψipxqqs for x in C0 and Cδ. The results are displayed in Figure
2 and give the following conclusions. If a network has learned the false structure pg, L1q, then
tσpΨipxqqs should agree with fa on Cδ, while it should be all zero on C0. On the other hand if
the network has not learned pg, L1q, then it should have the same output on both Cδ and C0. If
the network has learned pfa, Lq, it should agree with fa on both Cδ and C0.

Conclusion (Exp I): Ψ1, trained on T 7

δ (7 samples) learns the false structure pg, L1q. Ψ2,
trained on T 5000
, does not learn the original structure pfa, Lq nor the false structure pg, L1q. Ψ3,
trained on T 5000
, learns the false structure pg, L1q. Ψ4, trained on T 10000
, learns the original
structure pfa, Lq. Note that the conclusion supports Conjecture 1.1: The false structure pg, L1q
for pfa, Lq relative to T r
It also
δ
gives fantastic success on Cδ. The original structure pfa, Lq is diﬃcult to learn (10000 samples
are needed to succeed, yet 5000 samples are too few). Moreover, when training on T r
δ , it is
impossible to learn pfa, Lq, even with an excessive amount of samples, and the false structure
pg, L1q is always learned. In particular, despite Proposition 3.3 assuring that the architecture
chosen is rich enough to include stable networks that approximate fa well, and should be found
when the cost function is small, the good network is not found in nearly all cases.

is unstable and simple and is learned with only 7 samples.

δ

δ

9

´a

a

1 ´ a

a

a

a

1 ` a

1 ` a

´a ´a ´a 1 ´ a

´a

´a

a

a

Figure 4a: The colour code for the images
deﬁning ˜M. Horizontal stripe: the light
coloured pixels have value 1 ´ a and the
dark coloured pixels have value ´a. Ver-
tical stripe: the light coloured pixels have
value 1 ` a and the dark coloured pixels
have value a.

Figure 4b: The colour code for the images
deﬁning xM. Horizontal stripe: the light
coloured pixels have value 1 ` a and the
dark coloured pixels have value a. Vertical
stripe: the light coloured pixels have value
1´a and the dark coloured pixels have value
´a.

3.2 Establishing the conjecture: Case 2

Consider M to be the collection of 32ˆ32 grey scale images with a 3-pixel wide either horizontal
or vertical light stripe on a dark background as shown in Figure 3. The colour code is as follows:
´0.01 is black and 1 ` 0.01 is white. Hence, numbers between ´0.01 and 0.01 yield variations
of black and numbers between 1 ´ 0.01 and 1 ` 0.01 give variations of white. Thus, there are
slight diﬀerences in the black and white colours, however, they are typically not visible to the
human eye.

Deﬁne the original structure pf, Lq on M with L “ tα0, α1u, where

α0pxq “ x has a light horizontal stripe, α1pxq “ x has a light vertical stripe,

(14)

and note that the original structure pf, Lq is very robust to any small perturbations.

3.2.1 Experiment II

The experiment is inspired by an example from [8], and is done as follows. Deﬁne the two sets

˜M “ tx P M | x has a speciﬁc colour code described in Figure 4a with a ą 0u,
xM “ tx P M | x has a speciﬁc colour code described in Figure 4b with a ą 0u,

and notice that both are non-intersecting subsets of M. Next let ˜Cb,c Ă ˜M and pCb,c Ă
xM, each
contain 1000 elements from their respective sets and where the value of a P rb, cs. Each element
of ˜Cb,c and pCb,c, is chosen with an equal probability of being a vertical or horizontal stripe, and
the value of a is chosen from the uniform distribution on rb, cs.

We have trained a neural network Φ on a training set T which contains exactly the 60
unique elements in ˜M for which a “ 0.01. The network Φ has a 100% success rate on the set
˜Cb,c, b “ 0.009, c “ 0.01, yet its success rate on pCb,c is 0%. As is evident from Figure 3, Φ
misclassiﬁes images that look exactly the same as the ones that it successfully classiﬁes. Hence,
it is completely unstable and Φ has clearly not learned the original structure pf, Lq. Thus, a
pertinent question is:

What is the false structure that Φ has learned?

To answer the question we begin with the following proposition.

Proposition 3.4. Consider the predicates β0 and β1 on M deﬁned by

β0pxq “ The sum of the pixel values of x are ď 96,

β1pxq “ The sum of the pixel values of x are ą 96,
and let L1 “ tβ0, β1u. Let gpxq “ 0 when β0pxq “ true, and gpxq “ 1 when β1pxq “ true. Then
pg, L1q is a false structure for pf, Lq relative to ˜M.

10

b

0.009

0.008

0.007

0.006

c

0.010

0.009

0.008

0.007

pCb,c
˜Cb,c
100% 0.0%

100% 0.0%

98.7% 0.0%

98.5% 0.6%

Table 1: Accuracy of the network Φ, on the two test sets ˜Cb,c and pCb,c, for various values of b
and c. In all cases the two sets contain 1000 elements.

Note that, contrary to the original structure pf, Lq, that considers the geometry of the prob-
lem, the false structure pg, Lq is clearly completely unstable on ˜M. Indeed, a tiny perturbation
in the pixel values will change the label. The question is whether it is this false structure that
is actually learned by the network Φ. This turns out to be a rather delicate question. Indeed,
the fact that we get 100% success rate on ˜Cb,c, b “ 0.009, c “ 0.01 and 0% success rate on pCb,c
suggests that the false structure that Φ learns is pg, L1q. However, by choosing smaller values of
c and d, we see from Table 1 that the success rate of Φ on ˜Cb,c decreases, whereas the success
rate of pCb,c increases. This implies that this is not entirely the case; if Φ had learned the false
structure pg, L1q it should have 100% success rate on ˜Cb,c and 0% success rate on pCb,c for all
0 ă b ă c ď 0.01. This example illustrates how delicate the task of determining exactly the false
structure actually is, even on the simplest examples. The actual false structure learned by Φ is
likely not too far from pg, L1q, but making this statement mathematically rigorous, as well as a
full test is beyond the scope of this paper.

Conclusion (Exp II): The above numerical examples suggest that Φ learns a false structure,
however, it may not always be pg, L1q from Proposition 3.4. It should also be noted that if the
experiments are done with larger test sets ˜Cb,c and ˆCb,c the conclusion stays the same. The
experiments support Conjecture 1.1 as the false structures are simple to learn (only 60 samples
needed) and completely unstable. Indeed, tiny perturbations make the network change its label.
Moreover, the network Φ that learned the false structures become successful on large test sets.

4 Final conclusion

The correctness of Conjecture 1.1 may have far reaching consequences on how we understand
modern AI, and in particular on how to get to the heart of the problem of universal instability
throughout neural networks based on deep learning. The conjecture is inspired by Theorem 3.1
and its proof in addition to the many numerical examples demonstrating instabilities in deep
learning and suggesting learning of false structures. This paper provides the foundations for
a larger program to establish the conjecture fully. However, as we have demonstrated in this
paper, Conjecture 1.1 appears to be true even in the simplest cases.

11

References

[1] J. Adler and O. ¨Oktem. Solving ill-posed inverse problems using iterative deep neural

networks. Inverse Problems, 33(12):124007, 2017.

[2] V. Antun, F. Renna, C. Poon, B. Adcock, and A. C. Hansen. On instabilities of deep
learning in image reconstruction – Does AI come at a cost? arXiv:1902.05300, 2019.

[3] A. Bastounis, A. C. Hansen, and V. Vlacic. On computational barriers and paradoxes in

estimation, regularisation, learning and computer assisted proofs. Preprint, 2019.

[4] J. Ben-Artzi, A. C. Hansen, O. Nevanlinna, and M. Seidel. New barriers in complexity
theory: On the solvability complexity index and the towers of algorithms. Comptes Rendus
Mathematique, 353(10):931 – 936, 2015.

[5] L. Blum, F. Cucker, M. Shub, and S. Smale. Complexity and Real Computation. Springer-

Verlag New York, Inc., Secaucus, NJ, USA, 1998.

[6] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning

augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.

[7] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno,
and D. Song. Robust physical-world attacks on deep learning visual classiﬁcation.
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1625–1634,
2018.

[8] A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard. The robustness of deep networks: A

geometrical perspective. IEEE Signal Processing Magazine, 34:50–62, 2017.

[9] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S. Kohane. Adver-

sarial attacks on medical machine learning. Science, 363(6433):1287–1289, 2019.

[10] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.

International conference on learning representations, 2015.

[11] A. C. Hansen. On the approximation of spectra of linear operators on hilbert spaces. J.

Funct. Anal., 254(8):2092 – 2126, 2008.

[12] A. C. Hansen. On the solvability complexity index, the n-pseudospectrum and approxima-

tions of spectra of operators. J. Amer. Math. Soc., 24(1):81–124, 2011.

[13] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep convolutional neural network
for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9):4509–4522,
2017.

[14] J. Lu, T. Issaranon, and D. Forsyth. Safetynet: Detecting and rejecting adversarial examples
robustly. In IEEE International Conference on Computer Vision, pages 446–454, 2017.

[15] A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos. Using deep neural networks for
inverse problems in imaging: beyond analytical methods. IEEE Signal Processing Magazine,
35(1):20–36, 2018.

[16] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. Wijewickrema, G. Schoenebeck, M. E. Houle,
D. Song, and J. Bailey. Characterizing adversarial subspaces using local intrinsic dimen-
sionality. In International Conference on Learning Representations, 2018.

[17] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations,
2018.

[18] M. T. McCann, K. H. Jin, and M. Unser. Convolutional neural networks for inverse problems

in imaging: A review. IEEE Signal Processing Magazine, 34(6):85–95, Nov 2017.

12

[19] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial pertur-
bations. In IEEE Conference on computer vision and pattern recognition, pages 86–94, 07
2017.

[20] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 2574–2582, 2016.

[21] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High con-
ﬁdence predictions for unrecognizable images. IEEE Conference on Computer Vision and
Pattern Recognition, pages 427–436, 2015.

[22] P. Odifreddi. Classical Recursion Theory (Volume I). North–Holland Publishing Co., Am-

sterdam, 1989.

[23] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to
adversarial perturbations against deep neural networks. In IEEE Symposium on Security
and Privacy, pages 582–597. IEEE, 2016.

[24] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the predic-
tions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pages 1135–1144. ACM, 2016.

[25] H. Rogers, Jr. Theory of recursive functions and eﬀective computability. MIT Press, Cam-

bridge, MA, USA, 1987.

[26] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-
In IEEE

cam: Visual explanations from deep networks via gradient-based localization.
International Conference on Computer Vision, pages 618–626, 2017.

[27] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visual-
ising image classiﬁcation models and saliency maps. International conference on learning
representations, 2013.

[28] S. Smale. Mathematical problems for the next century. Mathematical Intelligencer, 20:7–15,

1998.

[29] D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, F. Tramer, A. Prakash,
In 12th tUSENIXu

and T. Kohno. Physical adversarial examples for object detectors.
Workshop on Oﬀensive Technologies (tWOOTu 18), 2018.

[30] R. Strack. AI transforms image reconstruction. Nature Methods, 15:309, 04 2018.

[31] J. Su, D. V. Vargas, and K. Sakurai. One pixel attack for fooling deep neural networks.

IEEE Transactions on Evolutionary Computation, 2019.

[32] J. Sun, H. Li, Z. Xu, et al. Deep ADMM-Net for compressive sensing MRI. In Advances in

Neural Information Processing Systems, pages 10–18, 2016.

[33] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus.
Intriguing properties of neural networks. In International conference on learning represen-
tations, 2014.

[34] A. M. Turing. On Computable Numbers, with an Application to the Entscheidungsproblem.

Proc. London Math. Soc., S2-42(1):230, 1936.

[35] A. M. Turing. I.-Computing machinery and intelligence. Mind, LIX(236):433–460, 1950.

[36] D. V. Vargas and J. Su. Understanding the one-pixel attack: Propagation maps and locality

analysis. arXiv preprint arXiv:1902.02947, 2019.

13

[37] H. Wang and C.-N. Yu. A direct approach to robust deep learning using adversarial net-

works. In International Conference on Learning Representations, 2019.

[38] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. European

conference on computer vision, pages 818–833, 2014.

[39] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for dis-
criminative localization. In IEEE Conference on Computer Vision and Pattern Recognition,
pages 2921–2929, 2016.

[40] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen. Image reconstruction by

domain-transform manifold learning. Nature, 555(7697):487, 03 2018.

5 Appendix

5.1 Proofs

5.1.1 Proof of Proposition 3.2

δ . We have that x2 “ δfapx1q for all x P T r

Proof of Proposition 3.2. First, we notice that the predicates in L1 are unique, i.e. x2 is either
0 or not and hence the classiﬁcation is unambiguous. Next, we want to see that gpxq “ j ðñ
f pxq “ j on T r
δ . Hence for the ﬁrst case that we get
gpxq “ 0 when x2 “ 0, which means that f pxq “ 0 since δ ą 0. The other case follows from
the same argument. Last, we see that C is non-empty. Let x2 “ 0, and choose x1 P rb, 1s such
that fapxq “ 1 for x “ px1, x2q. Such an x1 will always exists since K ą a. Then f pxq “ 1 and
gpxq “ 0.

5.1.2 Proof of Proposition 3.3

Proof of Proposition 3.3. Let x “ px1, x2q P rb, 1s ˆ r0, 1s, and let

φc,d

(cid:15) pxq “ ρ

ˆ

x1 ´ c
2(cid:15)

`

1
2

˙

ˆ

´ ρ

x1 ´ c
2(cid:15)

´

1
2

˙

ˆ

´ ρ

x1 ´ d
2(cid:15)

`

1
2

˙

ˆ

` ρ

x1 ´ d
2(cid:15)

´

1
2

˙

,

(15)

where (cid:15) ą 0 and ρptq “ maxt0, tu, t P R is the ReLU activation function. We start by noticing
that φc,d
is a two layered neural network lying in N N N,2 with N “ r1, 4, 2s, where the coeﬃcients
in front of x2 are all zero. Hence the support of φc,d
equals rc ´ (cid:15), d ` (cid:15)s ˆ r0, 1s. We also have
that φc,d

(cid:15) pxq “ 1 for x P rc, ds ˆ r0, 1s and that the range of φc,d

is r0, 1s. Next let

(cid:15)

(cid:15)

(cid:15)

Φpxq “

Kÿ

k“a

1
2

pp´1qk ` 1qφcpkq,dpkq

(cid:15)

pxq

(16)

with

cpkq “

a
k ` 1

` (cid:15)

and

dpkq “

a
k

´ (cid:15)

be a sum of K ´ a ` 1 smaller networks with non-overlapping support in the ﬁrst variable. We
note that the coeﬃcients in front of some of the functions will be zero, and hence could have
been removed, but we will not bother to do so. We notice that Φ P N N r1,4pK´a`1qs,2 Ă N N N,2
with N “ r1, 4K, 2s, where the inclusion holds since K ą a.

Let x1 P pa{pk ` 1q ` (cid:15), a{k ´ (cid:15)q and notice that if k ` 1 is odd, then Φpxq “ fapxq “ 1, and if
k ` 1 is even, then Φpxq “ fapxq “ 0. Hence we conclude that Φ P N N N,2, with N “ r1, 4K, 2s,
is a neural network such that Φpxq “ fapxq for all x P S(cid:15) ˆ r0, 1s.

Next let N1 ą 0 be a constant so that ´ logpσpN1qq ă η{r and let N2 ă 0 be a constant so

that ´ logp1 ´ σpN2qq ă η{r. Furthermore let N “ maxtN1, ´N2u. Then then

Ψpxq “ 2N Φpxq ´ N,

x P rb, 1s ˆ r0, 1s

14

is a neural network in N N N,2, N “ r1, 4K, 2s. Finally notice that for any T “ tx1, . . . , xru Ă
S(cid:15) ˆ r0, 1s we have w “ tf pxiqur
i“1 Ă t0, 1ur. Hence if wj “ 0, then Ψpxiq ď ´N and if wj “ 1
then Ψpxjq ě N . letting v “ tΨpxiqur

i“1, we readily see that

rÿ

rÿ

Cpw, vq “

´wj logpσpvjqq ´ p1 ´ wjqσp1 ´ vjq ă

η{r “ η,

j“1

j“1

moreover, by the same argument as above we see that tσpΨpxqqs “ f pxq for all x P S(cid:15) ˆr0, 1s.

5.1.3 Proof of Proposition 3.4

Proof of Proposition 3.4. The proof is similar to the proof of Proposition 3.2. We again start
with the uniqueness of L1. It is clear that the number of pixels is either larger than 96 or smaller
or equal. Hence, g is well deﬁned by the predicates β0 and β1. Next, we recognise that for images
in ˜M the images with a vertical line sum up to 3 ¨ 32 ` 322a and for the horizontal lines they
sum up to 3 ¨ 32 ´ 322a. Hence, for all all images x P ˜M we have that those with horizontal lines
have values ă 96 and for vertical lines ą 96 and therefore coincide with the evaluation of the
structure f . Last, we see that C in non-empty. All images in x P Mz ˜M with vertical lines have
that the pixel sum is 96. Therefore, they also get classiﬁed as having a horizontal line, which is
then diﬀerent to the classiﬁcation by f .

5.2 Description of training procedures

In this section we intend to describe the training procedure in detail, so that all the experiments
becomes reproducible. A complete overview of the code, and the weights of the trained net-
works described in this paper can be downloaded from https://github.com/vegarant/false_
structures. Before we start we would also like to point out that in each of the experiments
there is an inherit randomness due to random initialization of the network weights, and “on the
ﬂy” generation of some of the training and test sets. Hence, rerunning the code, might result
in slightly diﬀerent results. It is beyond the scope of this paper, to investigate how often, and
under what circumstances a false structure is learned.

All the code have been implemented in Tensorﬂow version 1.13.1, and all layers have used
Tensorﬂows default weight initializer, the so called “glorot uniform” initializer. If not otherwise
stated, the default options for other parameters is always assumed.

5.2.1 Experiment I

Network architecture. We considered two-layer neural networks with a ReLU activation
function between the two layers. The output dimension from the ﬁrst layer was set to 4K, and
the output dimension of the ﬁnal layers was 1. An observant reader, who reads the proof of
Proposition 3.3, might notice that it would be possible to decrease the output dimension of the
ﬁrst layer. In our initial tests, we did this, but doing so made it substantially harder to learn
the true structure fa.

Training parameters. We trained the networks using the cross entropy loss function for
binary classiﬁcation as described in Proposition 3.3. All networks was trained using the ADAM
optimizer, running 30000 epochs. The network trained on the set containing only 7 samples used
a batch size of 7, the three other networks used a batch size of 50, with a shuﬄing of the data
samples in each epoch.

Training data. The training sets considered are T 7
, and is described
in the main document. We point out that for each of the training steps, each of these sets where
drawn at random. Hence there is some randomness in the experiment itself.

and T 10000
δ

δ , T 5000
0

, T 5000
δ

5.2.2 Experiment II

Network architecture. The trained network Φ had the following architecture.

15

Input

Conv2D

ReLU

MaxPool

Conv2D

ReLU

MaxPool

Dense

Dense

where the to 2D convolutional layers (Conv2D) had a kernel size of 5, strides equal p1, 1q, padding
equal “same”. The ﬁrst of the convolutional layers had 24 ﬁlters, whereas the last had 48 ﬁlters.
After each of the convolutional layers we used a ReLU activation function. For the max pooling
layers a pool size of p2, 2q was used, with padding equal “same”. After the ﬁnal max pool
layer the output of the layer was reshaped into a vector and feed to a dense layer. The output
dimension of the ﬁrst dense layer was 10, whereas for the last it was 1.

Training parameters. We trained the networks using the cross entropy loss function for
binary classiﬁcation as described in Proposition 3.3. The network was trained using the ADAM
optimizer for 10 epochs, with a batch size of 60. We ran the code a few times to capture the exact
false structure as described in the main document, as it sometimes capture a slightly diﬀerent
false structure, which does not have exactly 0% success rate on pCb,c, b “ 0.009 and c “ 0.01.
Training data. The network was trained on the training set T Ă ˜M which contains exactly
the 60 unique elements in ˜M for which a “ 0.01.

16

