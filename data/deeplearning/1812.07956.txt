0
2
0
2

n
a
J

7

]

C
O
.
h
t
a
m

[

5
v
6
5
9
7
0
.
2
1
8
1
:
v
i
X
r
a

On Lazy Training in Differentiable Programming

Lénaïc Chizat
CNRS, Université Paris-Sud
Orsay, France
lenaic.chizat@u-psud.fr

Edouard Oyallon
CentraleSupelec, INRIA
Gif-sur-Yvette, France
edouard.oyallon@centralesupelec.fr

Francis Bach
INRIA, ENS, PSL Research University
Paris, France
francis.bach@inria.fr

Abstract

theoretical works,

it was shown that strongly over-
In a series of recent
parameterized neural networks trained with gradient-based methods could converge
exponentially fast to zero training loss, with their parameters hardly varying. In
this work, we show that this “lazy training” phenomenon is not speciﬁc to over-
parameterized neural networks, and is due to a choice of scaling, often implicit,
that makes the model behave as its linearization around the initialization, thus
yielding a model equivalent to learning with positive-deﬁnite kernels. Through a
theoretical analysis, we exhibit various situations where this phenomenon arises
in non-convex optimization and we provide bounds on the distance between the
lazy and linearized optimization paths. Our numerical experiments bring a critical
note, as we observe that the performance of commonly used non-linear deep con-
volutional neural networks in computer vision degrades when trained in the lazy
regime. This makes it unlikely that “lazy training” is behind the many successes of
neural networks in difﬁcult high dimensional tasks.

1

Introduction

Differentiable programming is becoming an important paradigm in signal processing and machine
learning that consists in building parameterized models, sometimes with a complex architecture and
a large number of parameters, and adjusting these parameters in order to minimize a loss function
using gradient-based optimization methods. The resulting problem is in general highly non-convex.
It has been observed empirically that, for ﬁxed loss and model class, changes in the parameterization,
optimization procedure, or initialization could lead to a selection of models with very different
properties [36]. This paper is about one such implicit bias phenomenon, that we call lazy training,
which corresponds to the model behaving like its linearization around the initialization.

This work is motivated by a series of recent articles [11, 22, 10, 2, 37] where it is shown that over-
parameterized neural networks could converge linearly to zero training loss with their parameters
hardly varying. With a slightly different approach, it was shown in [17] that inﬁnitely wide neural
networks behave like the linearization of the neural network around its initialization. In the present
work, we argue that this behavior is not speciﬁc to neural networks, and is not so much due to
over-parameterization than to an implicit choice of scaling. By introducing an explicit scale factor,
we show that essentially any parametric model can be trained in this lazy regime if its output is
close to zero at initialization. This shows that guaranteed fast training is indeed often possible, but

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
(a) Non-lazy training (τ = 0.1)

(b) Lazy training (τ = 2)

(c) Generalization properties

Figure 1: Training a two-layer ReLU neural network initialized with normal random weights of
variance τ 2: lazy training occurs when τ is large. (a)-(b) Trajectory of weights during gradient
descent in 2-D (color shows sign of output layer). (c) Generalization in 100-D: it worsens as τ
increases. The ground truth is generated with 3 neurons (arrows in (a)-(b)). Details in Section 3.

at the cost of recovering a linear method1. Our experiments on two-layer neural networks and deep
convolutional neural networks (CNNs) suggest that this behavior is undesirable in practice.

1.1 Presentation of lazy training

We consider a parameter space2 Rp, a Hilbert space F, a smooth model h : Rp → F (such as a neural
network) and a smooth loss R : F → R+. We aim to minimize, with gradient-based methods, the
objective function F : Rp → R+ deﬁned as

F (w) := R(h(w)).
With an initialization w0 ∈ Rp, we deﬁne the linearized model ¯h(w) = h(w0) + Dh(w0)(w − w0)
around w0, and the corresponding objective ¯F : Rp → R+ as

¯F (w) := R(¯h(w)).
It is a general fact that the optimization path of F and ¯F starting from w0 are close at the beginning
of training. We call lazy training the less expected situation where these two paths remain close until
the algorithm is stopped.

Showing that a certain non-convex optimization is in the lazy regime opens the way for surprisingly
precise results, because linear models are rather well understood. For instance, when R is strongly
convex, gradient descent on ¯F with an appropriate step-size converges linearly to a global mini-
mizer [4]. For two-layer neural networks, we show in Appendix A.2 that the linearized model is a
random feature model [26] which lends itself nicely to statistical analysis [6]. Yet, while advantageous
from a theoretical perspective, it is not clear a priori whether this lazy regime is desirable in practice.

This phenomenon is illustrated in Figure 1 where lazy training for a two-layer neural network with
rectiﬁed linear units (ReLU) is achieved by increasing the variance τ 2 at initialization (see next
section). While in panel (a) the ground truth features are identiﬁed, this is not the case for lazy
training on panel (b) that manages to interpolate the observations with just a small displacement
in parameter space (in both cases, near zero training loss was achieved). As seen on panel (c), this
behavior hinders good generalization in the teacher-student setting [30]. The plateau reached for
large τ corresponds exactly to the performance of the linearized model, see Section 3.1 for details.

1.2 When does lazy training occur?

A general criterion. Let us start with a formal computation. We assume that w0 is not a minimizer
so that F (w0) > 0, and not a critical point so that ∇F (w0) (cid:54)= 0. Consider a gradient descent step
w1 := w0 − η∇F (w0), with a small stepsize η > 0. On the one hand, the relative change of the
objective is ∆(F ) := |F (w1)−F (w0)|
. On the other hand, the relative change of the

≈ η (cid:107)∇F (w0)(cid:107)2

F (w0)

F (w0)

1Here we mean a prediction function linearly parameterized by a potentially inﬁnite-dimensional vector.
2Our arguments could be generalized to the case where the parameter space is a Riemannian manifold.

2

circle of radius 1gradient flow (+)gradient flow (-)1021011001010.00.51.01.52.02.53.03.5Test lossend of trainingbest throughout training≤ η (cid:107)∇F (w0)(cid:107)·(cid:107)D2h(w0)(cid:107)
differential of h measured in operator norm is ∆(Dh) := (cid:107)Dh(w1)−Dh(w0)(cid:107)
.
Lazy training refers to the case where the differential of h does not sensibly change while the loss
enjoys a signiﬁcant decrease, i.e., ∆(F ) (cid:29) ∆(Dh). Using the above estimates, this is guaranteed
when

(cid:107)Dh(w0)(cid:107)

(cid:107)Dh(w0)(cid:107)

(cid:107)∇F (w0)(cid:107)
F (w0)

(cid:29)

(cid:107)D2h(w0)(cid:107)
(cid:107)Dh(w0)(cid:107)

.

For the square loss R(y) = 1

2 (cid:107)y − y(cid:63)(cid:107)2 for some y(cid:63) ∈ F, this leads to the simpler criterion
(cid:107)D2h(w0)(cid:107)
(cid:107)Dh(w0)(cid:107)2 (cid:28) 1,

κh(w0) := (cid:107)h(w0) − y(cid:63)(cid:107)

(1)

using the approximation (cid:107)∇F (w0)(cid:107) = (cid:107)Dh(w0)(cid:124)(h(w0) − y(cid:63))(cid:107) ≈ (cid:107)Dh(w0)(cid:107) · (cid:107)h(w0) − y(cid:63)(cid:107).
This quantity κh(w0) could be called the inverse relative scale of the model h at w0. We prove
in Theorem 2.3 that it indeed controls how much the training dynamics differs from the linearized
training dynamics when R is the square loss3. For now, let us explore situations in which lazy training
can be shown to occur, by investigating the behavior of κh(w0).

Rescaled models. Considering a scaling factor α > 0, it holds

καh(w0) =

1
α

(cid:107)αh(w0) − y(cid:63)(cid:107)

(cid:107)D2h(w0)(cid:107)
(cid:107)Dh(w0)(cid:107)2 .

Thus, καh(w0) simply decreases as α−1 when α grows and (cid:107)αh(w0) − y(cid:63)(cid:107) is bounded, leading
to lazy training for large α. Training dynamics for such rescaled models are studied in depth in
Section 2. For neural networks, there are various ways to ensure h(w0) = 0, see Section 3.

Homogeneous models.
is equivalent to multiplying the scale factor α by λq. In equation,

If h is q-positively homogeneous4 then multiplying the initialization by λ

κh(λw0) =

1
λq (cid:107)λqh(w0) − y(cid:63)(cid:107)
This formula applies for instance to q-layer neural networks consisting of a cascade of homogenous
non-linearities and linear, but not afﬁne, operators. Such networks thus enter the lazy regime as the
variance of initialization increases, if one makes sure that the initial output has bounded norm (see
Figures 1 and 2(b) for 2-homogeneous examples).

(cid:107)D2h(w0)(cid:107)
(cid:107)Dh(w0)(cid:107)2 .

Two-layer neural networks. For m, d ∈ N, consider functions hm : (Rd)m → F of the form

hm(w) = α(m)

m
(cid:88)

i=1

φ(θi),

where α(m) > 0 is a normalization, w = (θ1, . . . , θm) and φ : Rd → F is a smooth function.
This setting covers the case of two-layer neural networks (see Appendix A.2). When initializing
i=1 satisfying Eφ(θi) = 0, and under the
with independent and identically distributed variables (θi)m
assumption that Dφ is not identically 0 on the support of the initialization, we prove in Appendix A.2
that for large m it holds

E[κhm(w0)] (cid:46) m− 1

2 + (mα(m))−1.

As a consequence, as long as mα(m) → ∞ when m → ∞, such models are bound to reach the
lazy regime. In this case, the norm of the initial output becomes negligible in front of the scale as m
grows due to the statistical cancellations that follow from the assumption Eφ(θi) = 0. In contrast, the
critical scaling α(m) = 1/m, allows to converge as m → ∞ to a non degenerate dynamic described
by a partial differential equation and referred to as the mean-ﬁeld limit [24, 7, 29, 33].

3Note that lazy training could occur even when κh(w0) is large, i.e. Eq. (1) only gives a sufﬁcient condition.
4That is, for q ≥ 1, it holds h(λw) = λqh(w) for all λ > 0 and w ∈ Rp.

3

1.3 Content and contributions

The goal of this paper is twofold: (i) understanding in a general optimization setting when lazy
training occurs, and (ii) investigating the practical usefulness of models in the lazy regime. It is
organized as follows:

• in Section 2, we study the gradient ﬂows for rescaled models αh and prove in various
situations that for large α, they are close to gradient ﬂows of the linearized model. When
the loss is strongly convex, we also prove that lazy gradient ﬂows converge linearly, either
to a global minimizer for over-parameterized models, or to a local minimizer for under-
parameterized models.

• in Section 3, we use numerical experiments on synthetic cases to illustrate how lazy training
differs from other regimes of training (see also Figure 1). Most importantly, we show
empirically that CNNs used in practice could be far from the lazy regime, with their
performance not exceeding that of some classical linear methods as they become lazy.

Our focus is on general principles and qualitative description.

Updates of the paper. This article is an expanded version of “A Note on Lazy Training in Su-
pervised Differential Programming” that appeared online in December 2018. Compared to the
ﬁrst version, it has been complemented with ﬁnite horizon bounds in Section 2.2 and numerical
experiments on CNNs in Section 3.2 while the rest of the material has just been slightly reorganized.

2 Analysis of Lazy Training Dynamics

2.1 Theoretical setting

Our goal in this section is to show that lazy training dynamics for the scaled objective

Fα(w) :=

1
α2 R(αh(w))

(2)

are close, when the scaling factor α is large, to those of the scaled objective for the linearized model

¯Fα(w) :=

1
α2 R(α¯h(w)),

(3)

where ¯h(w) := h(w0) + Dh(w0)(w − w0) and w0 ∈ Rp is a ﬁxed initialization. Multiplying the ob-
jective by 1/α2 does not change the minimizers, and corresponds to the proper time parameterization
of the dynamics for large α. Our basic assumptions are the following:
Assumption 2.1. The parametric model h : Rp → F is differentiable with a locally Lipschitz
differential5 Dh. Moreover, R is differentiable with a Lipschitz gradient.

This setting is mostly motivated by supervised learning problems, where one considers a probability
distribution ρ ∈ P(Rd × Rk) and deﬁnes F as the space L2(ρx; Rk) of square-integrable functions
with respect to ρx, the marginal of ρ on Rd. The risk R is then built from a smooth loss function
(cid:96) : (Rk)2 → R+ as R(g) = E(X,Y )∼ρ(cid:96)(g(X), Y ). This corresponds to empirical risk minimization
when ρ is a ﬁnite discrete measure, and to population risk minimization otherwise (in which case
only stochastic gradients are available to algorithms). Finally, one deﬁnes h(w) = f (w, ·) where
f : Rp × Rd → Rk is a parametric model, such as a neural network, which outputs in Rk depend on
parameters in Rp and input data in Rd.

In the rest of this section, we study the gradient ﬂow of the objective function
Gradient ﬂows.
Fα which is an approximation of (accelerated) gradient descent [12, 31] and stochastic gradient
descent [19, Thm. 2.1] with small enough step sizes. With an initialization w0 ∈ Rp, the gradient

5Dh(w) is a continuous linear map from Rp to F. The Lipschitz constant of Dh : w (cid:55)→ Dh(w) is deﬁned
with respect to the operator norm. When F has a ﬁnite dimension, Dh(w) can be identiﬁed with the Jacobian
matrix of h at w.

4

ﬂow of Fα is the path (wα(t))t≥0 in the space of parameters Rp that satisﬁes wα(0) = w0 and solves
the ordinary differential equation

w(cid:48)

1
α
where Dh(cid:124) denotes the adjoint of the differential Dh. We will study this dynamic for itself, and will
also compare it to the gradient ﬂow ( ¯wα(t))t≥0 of ¯Fα that satisﬁes ¯wα(0) = w0 and solves

Dh(wα(t))(cid:124)∇R(αh(wα(t))),

α(t) = −∇Fα(wα(t)) = −

(4)

α(t) = −∇ ¯Fα( ¯wα(t)) = −
¯w(cid:48)

1
α
Note that when h(w0) = 0, the renormalized dynamic w0 + α( ¯wα(t) − w0) does not depend on α,
as it simply follows the gradient ﬂow of w (cid:55)→ R(Dh(w0)(w − w0)) starting from w0.

Dh(w0)(cid:124)∇R(α¯h( ¯wα(t))).

(5)

2.2 Bounds with a ﬁnite time horizon

We start with a general result that conﬁrms that when h(w0) = 0, taking large α leads to lazy training.
We do not assume convexity of R.
Theorem 2.2 (General lazy training). Assume that h(w0) = 0. Given a ﬁxed time horizon T > 0, it
holds supt∈[0,T ] (cid:107)wα(t) − w0(cid:107) = O(1/α),

(cid:107)wα(t) − ¯wα(t)(cid:107) = O(1/α2)

and

sup
t∈[0,T ]

sup
t∈[0,T ]

(cid:107)αh(wα(t)) − α¯h( ¯wα(t))(cid:107) = O(1/α).

For supervised machine learning problems, the bound on (cid:107)wα(t) − ¯wα(t)(cid:107) implies that αh(wα(T ))
also generalizes like α¯h( ¯wα(T )) outside of the training set for large α, see Appendix A.3. Note
that the generalization behavior of linear models has been widely studied, and is particularly well
understood for random feature models [26], which are recovered when linearizing two layer neural
networks, see Appendix A.2. It is possible to track the constants in Theorem 2.2 but they would
depend exponentially on the time horizon T . This exponential dependence can however be discarded
for the speciﬁc case of the square loss, where we recover the scale criterion informally derived in
Section 1.2.
Theorem 2.3 (Square loss, quantitative). Consider the square loss R(y) = 1
2 (cid:107)y − y(cid:63)(cid:107)2 for some
y(cid:63) ∈ F and assume that for some (potentially small) r > 0, h is Lip(h)-Lipschitz and Dh is
Lip(Dh)-Lipschitz on the ball of radius r around w0. Then for an iteration number K > 0 and
corresponding time T := K/Lip(h)2, it holds

(cid:107)αh(wα(T )) − α¯h( ¯wα(T ))(cid:107)
(cid:107)αh(w0) − y(cid:63)(cid:107)

≤

K 2
α

Lip(Dh)
Lip(h)2 (cid:107)αh(w0) − y(cid:63)(cid:107)

as long as α ≥ K(cid:107)αh(w0) − y(cid:63)(cid:107)/(rLip(h)).

We can make the following observations:

• For the sake of interpretability, we have introduced a quantity K, analogous to an iteration
number, that accounts for the fact that the gradient ﬂow needs to be integrated with a step-
size of order 1/Lip(∇Fα) = 1/Lip(h)2. For instance, with this step-size, gradient descent
at iteration K approximates the gradient ﬂow at time T = K/Lip(h)2, see, e.g., [12, 31].
• Laziness only depends on the local properties of h around w0. These properties may vary a
lot over the parameter space, as is the case for homogeneous functions seen in Section 1.2.

For completeness, similar bounds on (cid:107)wα(T ) − w0(cid:107) and (cid:107)wα(T ) − ¯wα(T )(cid:107) are also provided in
Appendix B.2. The drawback of the bounds in this section is the increasing dependency in time, which
is removed in the next section. Yet, the relevance of Theorem 2.2 remains because it does not depend
on the conditioning of the problem. Although the bound grows as K 2, it gives an informative estimate
for large or ill-conditioned problems, where training is typically stopped much before convergence.

2.3 Uniform bounds and convergence in the lazy regime

This section is devoted to uniform bounds in time and convergence results under the assumption
that R is strongly convex. In this setting, the function ¯Fα is strictly convex on the afﬁne hyperspace

5

w0 + ker Dh(w0)⊥ which contains the linearized gradient ﬂow ( ¯wα(t))t≥0, so the latter converges
linearly to the unique global minimizer of ¯Fα. In particular, if h(w0) = 0 then this global minimizer
does not depend on α and supt≥0 (cid:107) ¯wα(t) − w0(cid:107) = O(1/α). We will see in this part how these
properties reﬂect on the gradient ﬂow wα(t) for large α.

Over-parameterized case. The following proposition shows global convergence of lazy training
under the condition that Dh(w0) is surjective. As rank Dh(w0) gives the number of effective
parameters or degrees of freedom of the model around w0, this over-parameterization assumption
guarantees that any model around h(w0) can be ﬁtted. Of course, this can only happen if F is
ﬁnite-dimensional.
Theorem 2.4 (Over-parameterized lazy training). Consider a M -smooth and m-strongly convex
loss R with minimizer y(cid:63) and condition number κ := M/m. Assume that σmin, the smallest
singular value of Dh(w0)(cid:124) is positive and that the initialization satisﬁes (cid:107)h(w0)(cid:107) ≤ C0 :=
min/(32κ3/2(cid:107)Dh(w0)(cid:107)Lip(Dh)) where Lip(Dh) is the Lipschitz constant of Dh. If α > (cid:107)y∗(cid:107)/C0,
σ3
then for t ≥ 0, it holds

(cid:107)αh(wα(t)) − y∗(cid:107) ≤

κ(cid:107)αh(w0) − y∗(cid:107) exp(−mσ2

mint/4).

√

If moreover h(w0) = 0, it holds as α → ∞, supt≥0 (cid:107)wα(t) − w0(cid:107) = O(1/α),

(cid:107)αh(wα(t)) − α¯h( ¯wα(t))(cid:107) = O(1/α)

and

sup
t≥0

(cid:107)wα(t) − ¯wα(t)(cid:107) = O(log α/α2).

sup
t≥0

The proof of this result relies on the fact that αh(wα(t)) follows the gradient ﬂow of R in a time-
dependent and non degenerate metric: the pushforward metric [21] induced by h on F. For the ﬁrst
part, we do not claim improvements over [11, 22, 10, 2, 37], where a lot of effort is also put in dealing
with the non-smoothness of h, which we do not study here. As for the uniform in time comparison
with the tangent gradient ﬂow, it is new and follows mostly from Lemma B.2 in Appendix B where
the constants are given and depend polynomially on the characteristics of the problem.

Under-parameterized case. We now remove the over-parameterization assumption and show
again linear convergence for large values of α. This covers in particular the case of population loss
minimization, where F is inﬁnite-dimensional. For this setting, we limit ourselves to a qualitative
statement6.
Theorem 2.5 (Under-parameterized lazy training). Assume that F is separable, R is strongly convex,
h(w0) = 0 and rank Dh(w) is constant on a neighborhood of w0. Then there exists α0 > 0 such
that for all α ≥ α0 the gradient ﬂow (4) converges at a geometric rate (asymptotically independent
of α) to a local minimum of Fα.

Thanks to lower-semicontinuity of the rank function, the assumption that the rank is locally constant
holds generically, in the sense that it is satisﬁed on an open dense subset of Rp. In this under-
parameterized case, the limit limt→∞ wα(t) is for α large enough a strict local minimizer, but in
general not a global minimizer of Fα because the image of Dh(w0) does not a priori contain the
global minimizer of R. Thus it cannot be excluded that there exists parameters w farther from w0
with a smaller loss. This fact is clearly observed experimentally in Section 3, Figure 2-(b). Finally, a
comparison with the linearized gradient ﬂow as in Theorem 2.4 could be shown along the same lines,
but would be technically slightly more involved because differential geometry comes into play.

Relationship to the global convergence result in [7]. A consequence of Theorem 2.5 is that in
the lazy regime, the gradient ﬂow of the population risk for a two-layer neural network might get
stuck in a local minimum. In contrast, it is shown in [7] that such gradient ﬂows converge to global
optimality in the inﬁnite over-parameterization limit p → ∞ if initialized with enough diversity in
the weights. This is not a contradiction since Theorem 2.5 assumes a ﬁnite number p of parameters.
In the lazy regime, the population loss might also converge to its minimum when p increases: this is
guaranteed if the tangent kernel Dh(w0)Dh(w0)(cid:124) [17] converges (after normalization) to a universal
kernel as p → ∞. However, this convergence might be unreasonably slow in high-dimension, as
Figure 1-(c) suggests. As a side note, we stress that the global convergence result in [7] is not limited
to lazy dynamics but also covers non-linear dynamics, such as seen on Figure 1 where neurons move.

6In contrast to the ﬁnite horizon bound of Theorem 2.3, quantitative statements would here involve the

smallest positive singular value of Dh(w0), which is anyways hard to control.

6

(a)

(b)

√

Figure 2: (a) Test loss at convergence for gradient descent, when α depends on m as α = 1/m or
m, the latter leading to lazy training for large m (not symmetrized). (b) Population loss at
α = 1/
convergence versus τ for SGD with a random N(0, τ 2) initialization (symmetrized). In the hatched
area the loss was still slowly decreasing.

3 Numerical Experiments

We realized two sets of experiments, the ﬁrst with two-layer neural networks conducted on syn-
thetic data and the second with convolutional neural networks (CNNs) conducted on the CIFAR-10
dataset [18]. The code to reproduce these experiments is available online7.

3.1 Two-layer neural networks in the teacher-student setting

We consider the following two-layer student neural network hm(w) = fm(w, ·) with fm(w, x) =
(cid:80)m
j=1 aj max(bj · x, 0) where aj ∈ R and bj ∈ Rd for j = 1, . . . , m. It is trained to minimize the
square loss with respect to the output of a two-layer teacher neural network with same architecture
and m0 = 3 hidden neurons, with random weights normalized so that (cid:107)ajbj(cid:107) = 1 for j ∈ {1, 2, 3}.
For the student network, we use random Gaussian weights, except when symmetrized initialization
is mentioned, in which case we use random Gaussian weights for j ≤ m/2 and set for j > m/2,
bj = bj−m/2 and aj = −aj−m/2. This amounts to training a model of the form h(wa, wb) =
hm/2(wa) − hm/2(wb) with wa(0) = wb(0) and guaranties zero output at initialization. The training
data are n input points uniformly sampled on the unit sphere in Rd and we minimize the empirical
risk, except for Figure 2b(b) where we directly minimize the population risk with Stochastic Gradient
Descent (SGD).

Cover illustration. Let us detail the setting of Figure 1 in Section 1. Panels (a)-(b) show gradient
descent dynamics with n = 15, m = 20 with symmetrized initialization (illustrations with more
neurons can be found in Appendix C). To obtain a 2-D representation, we plot |aj(t)|bj(t) throughout
training (lines) and at convergence (dots) for j ∈ {1, . . . , m}. The blue or red colors stand for the
signs of aj(t) and the unit circle is displayed to help visualizing the change of scale. On panel (c), we
set n = 1000, m = 50 with symmetrized initialization and report the average and standard deviation
of the test loss over 10 experiments. To ensure that the bad performances corresponding to large τ are
not due to a lack of regularization, we display also the best test error throughout training (for kernel
methods, early stopping is a form of regularization [34]).

√

Increasing number of parameters. Figure 2-(a) shows the evolution of the test error when in-
creasing m as discussed in Section 1.2, without symmetrized initialization. We report the results
for two choices of scaling functions α(m), averaged over 5 experiments with d = 100. The scaling
1/
m leads to lazy training, with a poor generalization as m increases, in contrast to the scaling
1/m for which the test error remains relatively close to 0 for large m (more experiments with this
scaling can be found in [7, 29, 24]).

7https://github.com/edouardoyallon/lazy-training-CNN

7

100101102103m01Test lossscaling 1/mscaling 1/m1021011001010123Population loss at convergencenot yet convergedUnder-parameterized case. Finally, Figure 2-(b) illustrates the under-parameterized case, with
d = 100, m = 50 with symmetrized initialization. We used SGD with batch-size 200 to minimize
the population square loss, and displayed average and standard deviation of the ﬁnal population loss
(estimated with 2000 samples) over 5 experiments. As shown in Theorem 2.5, SGD converges to a a
priori local minimum in the lazy regime (i.e., here for large τ ). In contrast, it behaves well when τ is
small, as in Figure 1. There is also an intermediate regime (hatched area) where convergence is very
slow and the loss was still decreasing when the algorithm was stopped.

3.2 Deep CNNs experiments

We now study whether lazy training is relevant to understand the good performances of convolutional
neural networks (CNNs).

Interpolating from standard to lazy training. We ﬁrst study the effect of increasing the scale
factor α on a standard pipeline for image classiﬁcation on the CIFAR10 dataset. We consider the
VGG-11 model [32], which is a widely used model on CIFAR10. We trained it via mini-batch SGD
with a momentum parameter of 0.9. For the sake of interpretability, no extra regularization (e.g.,
BatchNorm) is incorporated, since a simple framework that outperforms linear methods baselines
with some margin is sufﬁcient to our purpose (see Figure 3(b)). An initial learning rate η0 is linearly
decayed at each epoch, following ηt = η0
1+βt . The biases are initialized with 0 and all other weights
are initialized with normal Xavier initialization [13]. In order to set the initial output to 0 we use the
centered model h, which consists in replacing the VGG model ˜h by h(w) := ˜h(w) − ˜h(w0). Notice
that this does not modify the differential at initialization.
The model h is trained for the square loss multiplied by 1/α2 (as in Section 2), with standard
data-augmentation, batch-size of 128 [35] and η0 = 1 which gives the best test accuracies over the
grid 10k, k ∈ {−3, 3}, for all α. The total number of epochs is 70, adjusted so that the performance
reaches a plateau for α = 1. Figure 3(a) reports the accuracy after training αh for increasing values
of α ∈ 10k for k = {0, 1, 2, 3, 4, 5, 6, 7} (α = 1 being the standard setting). For α < 1, the training
loss diverges with η0 = 1. We also report the stability of activations, which is the share of neurons
over ReLU layers that, after training, are activated for the same inputs than at initialization, see
Appendix C. Values close to 100% are strong indicators of an effective linearization.

We observe a signiﬁcant drop in performance as α grows, and then the accuracy reaches a plateau,
suggesting that the CNN progressively reaches the lazy regime. This demonstrates that the linearized
model (large α) is not sufﬁcient to explain the good performance of the model for α = 1. For large α,
we obtain a low limit training accuracy and do not observe overﬁtting, a surprising fact since this
amounts to solving an over-parameterized linear system. This behavior is due to a poorly conditioned
linearized model, see Appendix C.

Performance of linearized CNNs.
In this second set of experiments, we investigate whether varia-
tions of the models trained above in a lazy regime could increase the performance and, in particular,
could outperform other linear methods which also do not involve learning a representation [26, 25].
To this end, we train widened CNNs in the lazy regime, as widening is a well-known strategy to boost
performances of a given architecture [35]. We multiply the number of channels of each layer by 8 for
the VGG model and 7 for the ResNet model [16] (these values are limited by hardware constraints).
We choose α = 107 to train the linearized models, a batch-size of 8 and, after cross-validation,
η0 = 0.01, 1.0 for respectively the standard and linearized model. We also multiply the initial
weights by respectively 1.2 and 1.3 for the ResNet-18 and VGG-11, as we found that it slightly
boosts the training accuracies. Each model is trained with the cross-entropy loss divided by α2 until
the test accuracy stabilizes or increases, and we check that the average stability of activations (see
Appendix C) was 100%.

As seen on Figure 3(b), widening the VGG model slightly improves the performances of the linearized
model compared to the previous experiment but there is still a substantial gap of performances from
other non-learned representations [28, 25] methods, not to mention the even wider gap with their
non-lazy counterparts. This behavior is also observed on the state-of-the-art ResNet architecture.
Note that [3] reports a test accuracy of 77.4% without data augmentation for a linearized CNN with
a specially designed architecture which in particular solves the issue of ill-conditioning. Whether

8

Model
ResNet wide, linearized
VGG-11 wide, linearized
Prior features [25]
Random features [28]
VGG-11 wide, standard
ResNet wide, standard

Train acc. Test acc.
56.7
61.7
82.3
84.2
89.7
91.0

55.0
61.0
-
-
99.9
99.4

(b)

(a)

Figure 3: (a) Accuracies on CIFAR10 as a function of the scaling α. The stability of activations
suggest a linearized regime when high. (b) Accuracies on CIFAR10 obtained for α = 1 (standard,
non-linear) and α = 107 (linearized) compared to those reported for some linear methods without
data augmentation: random features and prior features based on the scattering transform.

variations of standard architectures and pipelines can lead to competitive performances with linearized
CNNs, remains an open question.

Remark on wide NNs.
It was proved [17] that neural networks with standard initialization (random
independent weights with zero mean and variance O(1/n(cid:96)) at layer (cid:96), where n(cid:96) is the size of the
previous layer), are bound to reach the lazy regime as the sizes of all layers grow unbounded.
Moreover, for very large neural networks of more than 2 layers, this choice of initialization is
essentially mandatory to avoid exploding or vanishing initial gradients [15, 14] if the weights are
independent with zero mean. Thus we stress that we do not claim that wide neural networks do
not show a lazy behavior, but rather that those which exhibit good performances are far from this
asymptotic behavior.

4 Discussion

Lazy training is an implicit bias phenomenon in differentiable programming, that refers to the
situation when a non-linear parametric model behaves like a linear one. This arises when the scale
of the model becomes large which, we have shown, happens implicitly under some choices of
hyper-parameters governing normalization, initialization and number of iterations. While the lazy
training regime provides some of the ﬁrst optimization-related theoretical insights for deep neural
networks [10, 2, 37, 17], we believe that it does not explain yet the many successes of neural networks
that have been observed in various challenging, high-dimensional tasks in machine learning. This is
corroborated by numerical experiments where it is seen that the performance of networks trained
in the lazy regime degrades and in particular does not exceed that of some classical linear methods.
Instead, the intriguing phenomenon that still deﬁes theoretical understanding is the one illustrated on
Figure 1(c) for small τ and on Figure 3(a) for α = 1: neural networks trained with gradient-based
methods (and neurons that move) have the ability to perform high-dimensional feature selection
through highly non-linear dynamics.

Acknowledgments

We acknowledge supports from grants from Région Ile-de-France and the European Research Council
(grant SEQUOIA 724063). Edouard Oyallon was supported by a GPU donation from NVIDIA. We
thank Alberto Bietti for interesting discussions and Brett Bernstein for noticing an error in a previous
version of this paper.

References

[1] Ralph Abraham, Jerrold E. Marsden, and Tudor Ratiu. Manifolds, Tensor Analysis, and

Applications, volume 75. Springer Science & Business Media, 2012.

9

101103105107 (scale of the model)60708090100%train accuracytest accuracystability of activations[2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning
via over-parameterization. In Proceedings of the 36th International Conference on Machine
Learning, volume 97, pages 242–252, 2019.

[3] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955,
2019.

[4] Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine

learning. SIAM Review, 60(2):223–311, 2018.

[5] Youness Boutaib. On Lipschitz maps and their ﬂows. arXiv preprint arXiv:1510.07614, 2015.

[6] Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with SGD and random
features. In Advances in Neural Information Processing Systems, pages 10192–10203, 2018.

[7] Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, 2018.

[8] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in

Neural Information Processing Systems, pages 342–350, 2009.

[9] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pages 2253–2261, 2016.

[10] Simon S. Du, Lee Jason D., Li Haochuan, Wang Liwei, and Zhai Xiyu. Gradient descent ﬁnds
global minima of deep neural networks. In International Conference on Machine Learning
(ICML), 2019.

[11] Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient descent provably
In International Conference on Learning

optimizes over-parameterized neural networks.
Representations, 2019.

[12] Walter Gautschi. Numerical analysis. Springer Science & Business Media, 1997.

[13] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedfor-
ward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial
intelligence and statistics, pages 249–256, 2010.

[14] Boris Hanin and David Rolnick. How to start training: The effect of initialization and architec-

ture. In Advances in Neural Information Processing Systems, pages 571–581, 2018.

[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision, pages 1026–1034, 2015.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[17] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, 2018.

[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report, Citeseer, 2009.

[19] Harold Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and

Applications, volume 35. Springer Science & Business Media, 2003.

[20] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In International
Conference on Learning Representations, 2018.

[21] John M. Lee. Smooth manifolds. In Introduction to Smooth Manifolds, pages 1–29. Springer,

2003.

[22] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems,
pages 8167–8176, 2018.

10

[23] Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. In International
Conference on Learning Representations, 2018.

[24] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–
E7671, 2018.

[25] Edouard Oyallon and Stéphane Mallat. Deep roto-translation scattering for object classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2865–2873, 2015.

[26] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems, pages 1177–1184, 2008.

[27] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimiza-
tion with randomization in learning. In Advances in neural information processing systems,
pages 1313–1320, 2009.

[28] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet
classiﬁers generalize to ImageNet? In Proceedings of the 36th International Conference on
Machine Learning, pages 5389–5400, 2019.

[29] Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error. In
Advances in neural information processing systems, 2018.

[30] David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E,

52(4):4225, 1995.

[31] Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d’Aspremont. Integration methods
and optimization algorithms. In Advances in Neural Information Processing Systems, pages
1109–1118, 2017.

[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

[33] Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks: A

central limit theorem. Stochastic Processes and their Applications, 2019.

[34] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent

learning. Constructive Approximation, 26(2):289–315, 2007.

[35] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British

Machine Vision Conference (BMVC), pages 87.1–87.12, 2016.

[36] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.

[37] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes

over-parameterized deep ReLU networks. Machine Learning Journal, 2019.

11

Supplementary material

Supplementary material for the paper: “On Lazy Training in Differentiable Programming” authored
by Lénaïc Chizat , Edouard Oyallon and Francis Bach (NeurIPS 2019). This supplementary material
is organized as follows:

• Appendix A: Remarks on the linearized model
• Appendix B: Proofs of the theoretical results
• Appendix C: Experimental details and additional results

A The linearized model in supervised machine learning

A.1 Differentiable models and their linearization

In this section, we give some details on the interpretation of the linearized model in the case
of supervised machine learning. In this setting, a differentiable model is a typically a function
f : Rp × Rd → Rk where Rp is the parameter space, Rd is the input space and Rk the output space.
One deﬁnes a Hilbert space F of functions from Rd to Rk, typically L2(ρx, Rk) where ρx is the
distribution of input samples. The function h : Rp → F considered in the article is then the function
which to a vector of parameters associates a predictor h : w (cid:55)→ f (w, ·).
In ﬁrst order approximation around the initial parameters w0 ∈ Rp, the parametric model f (w, x)
reduces to the following linearized or tangent model :

¯f (w, x) = f (w0, x) + Dwf (w0, x)(w − w0).
where Dwf is the differential of f in the variable w. The corresponding hypothesis class is afﬁne
in the space of predictors. It should be stressed that when f is a neural network, ¯f is generally not
a linear neural network because it is not linear in x ∈ Rd, but in the features Dwf (w0, x) ∈ Rp×k
which generally depend non-linearly on x. For large neural networks, the dimension of the features
might be much larger than d, which makes ¯f similar to a non-parametric method. Finally, if f is
already a linear model, then f and ¯f are identical.

(6)

Kernel method with an offset.
equivalent to training a linear model in the variables

In the case of the square loss, training the afﬁne model (6) is

(˜x, ˜y) := (Dwf (w0, x), y − f (w0, x)).

When k = 1, this is equivalent to a kernel method with the tangent kernel [17] deﬁned as K :
Rd × Rd → R

K(x, x(cid:48)) = Dwf (w0, x)Dwf (w0, x(cid:48))(cid:124).
(7)
This kernel is different from the one traditionally associated to neural networks [27, 9] which involve
the derivative with respect to the output layer only. Also, the output data is shifted by the initialization
of the model h(w0) = f (w0, ·). This term inherits from the randomness due to the initialization: it
is for instance shown in [20, 23] that the distribution of h(w0) converges to a Gaussian process for
certain over-parameterized neural networks initialized with random normal weights.

A.2 Two-layer neural networks

Lazy training has some interesting consequences when looking more particularly at two-layer neural
networks. These are functions of the form

fm(w, x) = α(m)

m
(cid:88)

j=1

bj · σ(aj · x),

where m ∈ N is the size of the hidden layer and σ : R → R is an activation function and
j=1 where θj = (aj, bj) ∈ Rd+1, so here the number of parameters is
the parameters8 are (θj)m
p = m(d + 1). We have also introduced a scaling α(m) > 0 as in Section 1.2.

8We have omitted the bias/intercept, which is recovered by ﬁxing the last coordinate of x to 1.

12

Justiﬁcation for asymptotics. In this paragraph, we justify the formula for the asymptotic upper
bound on κhm(w0) given for such models in Section 1.2. Using the assumption that Eφ(θi) = 0 and
the fact that the parameters are independents, one has E(cid:107)h(w0)(cid:107)2 = mα(m)2E(cid:107)φ(θ)(cid:107)2. For the
differential, from the law of large numbers, we have the estimate

1

mα(m)2 Dh(w0) Dh(w0)(cid:124) =

1
m

m
(cid:88)

i=1

Dφ(θi)Dφ(θi)(cid:124) −→
m→∞

E [Dφ(θ)Dφ(θ)(cid:124)] .

It follows that E(cid:107)Dh(w0)(cid:107)2 = E(cid:107)Dh(w0)Dh(w0)(cid:124)(cid:107) ∼ mα(m)2(cid:107)E[Dφ(θ)Dφ(θ)(cid:124)](cid:107) because we
have assumed that Dφ is not identically 0 on the support of θ. One also has

(cid:107)D2h(w0)(cid:107) = sup

α(m)

u∈Rd×m
(cid:107)u(cid:107)≤1

m
(cid:88)

i=1

(cid:124)
i D2φ(θi)ui ≤ α(m) sup
u
θi

(cid:107)D2φ(θi)(cid:107) ≤ α(m)Lip(Dφ).

From the deﬁnition of κhm(w0) and the upper bound (cid:107)hm(w0)−y(cid:63)(cid:107) ≤ (cid:107)h(w0)(cid:107)+(cid:107)y(cid:63)(cid:107) we conclude
that

E[κhm(w0)] (cid:46) m− 1

2 + (mα(m))−1.

√

Limit kernels and random feature. In this section, we show that the tangent kernel is a random
feature kernel for neural networks with a single hidden layer. For simplicity, we consider the scaling
m as in [11] which leads to a non-degenerated limit of the kernel9 as m → ∞. The
α(m) = 1/
associated tangent kernel in Eq. (7) is the sum of two kernels Km(x, x(cid:48)) = K (a)
m (x, x(cid:48)),
one for each layer, where

m (x, x(cid:48))+K (b)

K (a)

m (x, x(cid:48)) =

1
m

m
(cid:88)

(x · x(cid:48))b2

j σ(cid:48)(aj · x)σ(cid:48)(aj · x(cid:48))

j=1

and K (b)

m (x, x(cid:48)) =

1
m

m
(cid:88)

j=1

σ(aj · x)σ(aj · x(cid:48)).

If we assume that the initial weights aj (resp. bj) are independent samples of a distribution on Rd
(resp. a distribution on R), these are random feature kernels [26] that converge as m → ∞ to the
kernels
K (a)(x, x(cid:48)) = E(a,b)

and K (b)(x, x(cid:48)) = Ea [σ(a · x)σ(a · x(cid:48))] .

(cid:2)(x · x(cid:48))b2σ(cid:48)(a · x)σ(cid:48)(a · x(cid:48))(cid:3)

The second component K (b), corresponding to the differential with respect to the output layer, is
the one traditionally used to make the link between these networks and random features [27]. When
σ(s) = max{s, 0} is the rectiﬁed linear unit activation and the distribution of the weights aj is
rotation invariant in Rd, one has the following explicit formulae [8]:

K (a)(x, x(cid:48)) =

(x · x(cid:48))E(b2)
2π

(π − ϕ), K (b)(x, x(cid:48)) =

(cid:107)x(cid:107)(cid:107)x(cid:48)(cid:107)E((cid:107)a(cid:107)2)
2πd

((π − ϕ) cos ϕ + sin ϕ)

(8)

where ϕ ∈ [0, π] is the angle between the two vectors x and x(cid:48). See Figure 4 for an illustration of
this kernel and the convergence of its random approximations. The link with (independent) random
sampling is lost for deeper neural networks, but it is shown in [17] that tangent kernels still converge
when the size of networks increase, for certain architectures.

A.3 Generalization for the lazy model

As noted in the main text, in supervised machine learning, F is often a Hilbert space of functions
on Rd and the model h is often of the form h(w) = f (w, ·) where f : Rp × Rd → Rk. A natural
question that arises in this context and that is not directly answered by the theorems of Section 2,
is whether the trained lazy model and the trained tangent model also generalize the same way, i.e.
whether at training time T , it holds f (w(T ), x) ≈ ¯f ( ¯w(T ), x) for points x ∈ Rd that are not in the
training set, where ¯f (w, x) = f (w0, x) + Dwf (w0, x)(w − w0). We will see here that it is actually
a simple consequence of the bounds.

9Since the deﬁnition of gradients depends on the choice of a metric, this scaling is not of intrinsic importance.
Rather, it reﬂects that we work with the Euclidean metric on Rp. The choice of scaling however becomes
important when dealing with training (see also discussion in Section 1.2).

13

Figure 4: Random realizations of the kernels Km and the limit kernel K of Eq. (8). We display the
value of K(x, x(cid:48)) as a function of ϕ = angle(x, x(cid:48)) with x ﬁxed, on a section of the sphere in R10.
Parameters are normal random variables of variance 1, so E(b2) = 1 and E((cid:107)a(cid:107)2) = d.

Proposition A.1 (Generalizing like the tangent model). Assume that for some C > 0 it holds
(cid:107)wα(T ) − ¯w(T )(cid:107) ≤ C log(α)/α2. Assume moreover that there exists a set X ⊂ Rd such that
M1 := supx∈X (cid:107)Dwf (w0, x)(cid:107) < ∞ and M2 := supx∈X Lip(w (cid:55)→ Dwf (w, x)) < ∞. Then it
holds

(cid:107)αf (wα(T ), x) − α ¯f ( ¯wα(T ), x)(cid:107) ≤ C

sup
x∈X

(cid:18)

log α
α

M1 +

1
2

(cid:19)

C · M2 · log(α)

−→
α→∞

0.

Proof. Let us call A the quantity to be upper bounded, and start with the decomposition

A ≤ sup
x∈X

(cid:107)αf (wα(T ), x) − α ¯f (wα(T ), x)(cid:107) + sup
x∈X

(cid:107)α ¯f (wα(T ), x) − α ¯f ( ¯wα(T ), x)(cid:107) = A1 + A2

By Taylor’s theorem applied at each point x ∈ X, one has

It also holds

A1 ≤

α
2

M2(cid:107)wα(T ) − ¯wα(T )(cid:107)2 ≤

C 2 · M2 log(α)2
2α

.

A2 = α sup
x∈X

(cid:107)Dwf (w0, x)(wα(T ) − ¯wα(T ))(cid:107) ≤

M1C log(α)
α

and the conclusion follows.

B Proofs of the theoretical results

In all the forthcoming proofs, we use the notations y(t) = αh(wα(t)) and ¯y(t) = α¯h( ¯wα(t)) for the
dynamics in F (they also depend on α although this is not reﬂected in the notation). We also write
Σ(w) := Dh(w)Dh(w)(cid:124) for the so-called tangent kernel [17], which is a quadratic form on F. By
using the chain rule, we ﬁnd that the trajectories in F solve the differential equation

d
dt
d
dt

y(t) = −Σ(wα(t))∇R(y(t)),

¯y(t) = −Σ(w(0))∇R(¯y(t)).

with y(0) = ¯y(0) = αh(w0). Remark that the ﬁrst differential equation is coupled with wα(t).

B.1 Proof for Theorem 2.2 (ﬁnite horizon, non-quantitative)

For this ﬁrst proof, we only track the dependency in α, and we use C to denote a quantity independent
of α, that may vary from line to line. For T > 0, it holds

(cid:90) T

0

(cid:107)w(cid:48)

α(t)(cid:107)dt =

(cid:90) T

0

(cid:107)∇Fα(wα(t))(cid:107)dt ≤

(cid:32)(cid:90) T

√

T

0

(cid:33) 1

2

(cid:107)∇Fα(wα(t))(cid:107)2dt

.

14

It follows, by using the fact that d

dt Fα(wα(t)) = −(cid:107)∇Fα(wα(t))(cid:107)2, that supt∈[0,T ] (cid:107)wα(t) −
α . In particular, we deduce that supt∈[0,T ] (cid:107)y(t) − y(0)(cid:107) ≤ C and

1
2 (cid:46) 1

w(0)(cid:107) ≤ (T · Fα(wα(t)))
supt∈[0,T ] (cid:107)∇R(y(t))(cid:107) ≤ C.

Let us now consider the evolution of ∆(t) := (cid:107)y(t) − ¯y(t)(cid:107). It satisﬁes ∆(0) = 0 and

∆(cid:48)(t) ≤ (cid:107)Σ(wα(t))∇R(y(t)) − Σ(w(0))∇R(¯y(t))(cid:107)

≤ (cid:107)(Σ(wα(t)) − Σ(w(0)))∇R(y(t))(cid:107) + (cid:107)Σ(w(0))(∇R(y(t)) − ∇R(¯y(t))(cid:107)
≤ C1/α + C2∆(t)

The ordinary differential equation u(cid:48)(t) = C1/α + C2u(t) with initial condition u(0) = 0 admits the
unique solution u(t) = C1
(exp(C2t) − 1). Since ∆(t) is a sub-solution of this system, it follows
αC2
that ∆(t) ≤ C1
(exp(C2t) − 1) ≤ C/α (notice the exponential dependence in the ﬁnal time and
αC2
some other characteristics of the problem). Finally, consider the quantity δ(t) = (cid:107)wα(t) − ¯wα(t)(cid:107).
It holds

δ(cid:48)(t) ≤ α−1(cid:107)Dh(wα(t))(cid:124)∇R(f (t)) − Dh(w0)(cid:124)∇R(¯y(t))(cid:107)

≤ α−1(cid:107)Dh(wα(t))(cid:124) − Dh(w0)(cid:124)(cid:107)(cid:107)∇R(y(t))(cid:107) + α−1(cid:107)Dh(w0)(cid:107)(cid:107)∇R(y) − ∇R(¯y(t))(cid:107)
≤ Cα−2

We thus conclude, since δ(0) = 0, that supt∈[0,T ] (cid:107)δ(t)(cid:107) ≤ α−2.

B.2 Proof of Theorem 2.3 (ﬁnite horizon, square loss)

Step 1. With the square loss, the objective is still potentially non-convex, but we have the property

d
dt

(cid:107)y(t) − y∗(cid:107)2 = −(cid:104)Σ(w(t))(y(t) − y(cid:63)), y(t) − y(cid:63)(cid:105) ≤ 0.

The proof scheme is otherwise similar as above, but we carry all constants. Let us denote Texit =
inf{t > 0 ; (cid:107)wα(t) − w0(cid:107) > r}. For t ≤ Texit it holds

(cid:107)w(cid:48)

α(t)(cid:107) = (cid:107)∇Fα(wα(t))(cid:107) ≤ α−1(cid:107)y(t) − y(cid:63)(cid:107)(cid:107)Dh(wα(t))(cid:107) ≤ α−1(cid:107)y(0) − y(cid:63)(cid:107)Lip(h)
It follows that (cid:107)wα(t) − w(0)(cid:107) ≤ tα−1(cid:107)y(0) − y(cid:63)(cid:107)Lip(h) (this bound is tighter for small times,
t used in the previous proof). Since we have assumed that α ≥
compared to the bound in
k(cid:107)y(0) − y(cid:63)(cid:107)/(rLip(h)), it holds (cid:107)wα(t) − w0(cid:107) ≤ (t/K) · rLip(h)2 = r so Texit > T .
Step 2. Now we consider ∆(t) = (cid:107)y(t) − ¯y(t)(cid:107). It holds

√

1
2

d
dt

∆(t)2 = (cid:104)y(cid:48)(t) − ¯y(cid:48)(t), y(t) − ¯y(t)(cid:105)

≤ −(cid:104)Σ(wα(t))∇R(y(t)) − Σ(w(0))∇R(¯y(t)), y(t) − ¯y(t)(cid:105)
≤ −(cid:104)(Σ(wα(t)) − Σ(w(0)))∇R(y(t)), y(t) − ¯y(t)(cid:105)

where we have used the fact that (cid:104)Σ(w(0))(∇R(y(t)) − ∇R(¯y(t)), y(t) − ¯y(t)(cid:105) ≥ 0, which is
speciﬁc to the square loss. Taking the norms and dividing both sides by ∆(t), it follows

∆(cid:48)(t) ≤ Lip(Σ) · (cid:107)wα(t) − w(0)(cid:107)(cid:107)y(0) − y(cid:63)(cid:107) ≤ 2Lip(h)2Lip(Dh)tα−1(cid:107)y(0) − y(cid:63)(cid:107)2

where we have used Lip(Σ) ≤ 2Lip(h)Lip(Dh). Since ∆(0) = 0, it follows

∆(t) ≤

t2
α

Lip(h)2Lip(Dh)(cid:107)y(0) − y(cid:63)(cid:107)2.

The bound in the statement then follows by writing this upper bound at time T = K/Lip(h)2.

Step 3. Finally, consider δ(t) = (cid:107)wα(t) − ¯wα(t)(cid:107). The bound that we will obtain is not reported
in the main text due to space constraints, but proved here for the sake of completeness. As in the
previous proof, it holds
αδ(cid:48)(t) ≤ (cid:107)Dh(wα(t))(cid:124)−Dh(w0)(cid:124)(cid:107)(cid:107)∇R(y(t))(cid:107)+(cid:107)Dh(w0)(cid:107)(cid:107)∇R(y)−∇R(¯y(t))(cid:107) = A(t)+B(t).

15

Let us bound these two quantities separately. On the one hand, it holds for t ∈ [0, T ],

A(t) ≤ Lip(Dh)(cid:107)wα(t) − w0(cid:107)(cid:107)y(0) − y(cid:63)(cid:107) ≤

t
α

Lip(h)Lip(Dh)(cid:107)y(0) − y(cid:63)(cid:107)2.

On the other hand, it holds for t ∈ [0, T ],

B(t) ≤

t2
α

Lip(h)3Lip(Dh)(cid:107)y(0) − y(cid:63)(cid:107)2.

By integrating these two bounds and summing, we get

δ(T ) ≤

≤

T 2
α2 Lip(h)2Lip(Dh)(cid:107)y(0) − y(cid:63)(cid:107)2
K 2
α2

Lip(Dh)
Lip(h)3 (cid:107)y(0) − y(cid:63)(cid:107)2 (2 + 4K/3) .

(cid:18) 2

Lip(h)

+

4T
3

(cid:19)

Lip(h)

After rearranging the terms, we obtain

αLip(h)
(cid:107)y(0) − y(cid:63)(cid:107)

(cid:107)wα(T ) − ¯wα(T )(cid:107) ≤

K 2
α

Lip(Dh)
Lip(h)2 (cid:107)y(0) − y(cid:63)(cid:107) (2 + 4K/3)

Note that this bound is arranged so that both sides of the inequality are dimensionless, in the sense that
they would not change under a simple rescaling of either the norm on F or on Rp. The left-hand side
should be understood as the relative difference between the non-linear and the linearized dynamics,
while the right-hand side involves the scale of Section 1.2.

B.3 Proof of Theorem 2.4 (over-parameterized case)

Consider the radius r0 := σmin/(2Lip(Dh)). By smoothness of h, it holds Σ(w) (cid:23) σ2
minId/4 as
long as (cid:107)w − w0(cid:107) < r0. Thus Lemma B.1 below guarantees that y(t) converges linearly, up to time
T := inf{t ≥ 0 ; (cid:107)wα(t) − w0(cid:107) > r0}. It only remains to ﬁnd conditions on α so that T = +∞.
The variation of the parameters wα(t) can be bounded for 0 ≤ t ≤ T as

(cid:107)w(cid:48)

α(t)(cid:107) ≤

1
α

(cid:107)Dh(wα(t))(cid:107)(cid:107)∇R(y(t))(cid:107) ≤

2M
α

(cid:107)Dh(w0)(cid:107)(cid:107)y(t) − y∗(cid:107).

By Lemma B.1, it follows that for 0 ≤ t ≤ T ,

(cid:107)wα(t) − w0(cid:107) ≤

≤

2M 3/2
αm
8κ3/2
ασ2

min

(cid:107)Dh(w0)(cid:107)(cid:107)y(0) − y∗(cid:107)

(cid:90) t

0

e−(mσ2

min/4)sds

(cid:107)Dh(w0)(cid:107)(cid:107)y(0) − y∗(cid:107).

This quantity is smaller than r0, and thus T = ∞, if (cid:107)y(0) − y∗(cid:107) ≤ 2αC0. This is in particular
guaranteed by the conditions on h(w0) and α in the theorem.
When h(w0) = 0, the previous bound also implies the “laziness” property supt≥0 (cid:107)wα(t) − w0(cid:107) =
O(1/α) since in that case y(0) does not depend on α. For the comparison with the tangent gradient
ﬂow, the ﬁrst bound is obtained by applying the stability Lemma B.2, and noticing that the quantity
denoted by K in that lemma is in O(1/α) thanks to the previous bound on (cid:107)wα(t) − w0(cid:107). For the
last bound, we compute the integral over [0, +∞) of the bound

α(cid:107)w(cid:48)

α(t) − ¯w(cid:48)

α(t)(cid:107) = (cid:107)Dh(wα(t))(cid:124)∇R(y(t)) − Dh(w0)(cid:124)∇R(¯y(t))(cid:107)

≤ (cid:107)Dh(wα(t)) − Dh(w0)(cid:107)(cid:107)∇R(y(t))(cid:107) + (cid:107)Dh(w0)(cid:107)(cid:107)∇R(y(t)) − ∇R(¯y(t))(cid:107).

It is easy to see from the derivations above that the integral of the ﬁrst term is in O(1/α). For the
second term, we deﬁne t0 := 4 log α/(µσ2

min) and on [0, t0] we use the smoothness bound

(cid:107)∇R(y(t)) − ∇R(¯y(t))(cid:107) ≤ M (cid:107)y(t) − ¯y(t)(cid:107)

which integral over [0, t0] is in O(log α/α), while on [t0, +∞) we use the crude bound

(cid:107)∇R(y(t)) − ∇R(¯y(t))(cid:107) ≤ (cid:107)∇R(y(t))(cid:107) + (cid:107)∇R(¯y(t))(cid:107)

16

which integral over [t0, +∞) is in O(1/α) thanks to the deﬁnition of t0 and the exponential decrease
of ∇R along both trajectories. This is sufﬁcient to conclude. As a side note, we remark that
the assumption that Dh is globally Lipschitz could be avoided by considering the more technical
deﬁnition

Lip(Dh) := inf

(cid:110)

L > 0 ; Dh is L-Lipschitz on a ball centered at w0 of radius

(cid:111)

σmin
2L

> 0,

because then the path wα(t) never escapes the ball of radius σmin
Lemma B.1 (Strongly-convex gradient ﬂow in a time-dependent metric). Let F : F → R be a
m-strongly-convex function with M -Lipschitz continuous gradient and with global minimizer y∗ and
let Σ(t) : F → F be a time dependent continuous self-adjoint linear operator with eigenvalues lower
bounded by λ > 0 for 0 ≤ t ≤ T . Then solutions on [0, T ] to the differential equation

2L around w0 for α > (cid:107)y∗(cid:107)/C0.

satisfy, for 0 ≤ t ≤ T ,

y(cid:48)(t) = −Σ(t)∇F (y(t)),

(cid:107)y(t) − y∗(cid:107) ≤ (M/m)1/2(cid:107)y(0) − y∗(cid:107) exp (−mλt) .

Proof. By strong convexity, it holds ¯F (y) := F (y) − F (y∗) ≤ 1

2m (cid:107)∇F (y)(cid:107)2. It follows

d
dt

¯F (y(t)) = −∇F (y(t))(cid:124) Σ(t) ∇F (y(t)) ≤ −λ(cid:107)∇F (y(t))(cid:107)2 ≤ −2mλ ¯F (y),

and thus ¯F (y(t)) ≤ exp (−2mλ) ¯F (y(0)) by Grönwall’s Lemma. We now use the strong convexity
¯F (y) in the left-hand side and the smoothness inequality ¯F (y) ≤
inequality (cid:107)y − y∗(cid:107)2 ≤ 2
m
2 M (cid:107)y − y∗(cid:107)2 in the right-hand side. This yields (cid:107)y(t) − y∗(cid:107)2 ≤ M
1

m exp (−2mλ) (cid:107)y(0) − y∗(cid:107)2.

B.4 Stability Lemma

The following stability lemma is at the basis of the equivalence between lazy training and linearized
model training in Theorem 2.4. We limit ourselves to a rough estimate sufﬁcient for our purposes.
Lemma B.2. Let R : F → R+ be a m-strongly convex function and let Σ(t) be a time dependent
positive deﬁnite operator on F such that Σ(t) (cid:23) λId for t ≥ 0. Consider the paths y(t) and ¯y(t) on
F that solve for t ≥ 0,

y(cid:48)(t) = −Σ(t)∇R(y(t))

and

¯y(cid:48)(t) = −Σ(0)∇R(¯y(t)).

Deﬁning K := supt≥0 (cid:107)(Σ(t) − Σ(0))∇R(y(t))(cid:107), it holds for t ≥ 0,

(cid:107)y(t) − ¯y(t)(cid:107) ≤

K(cid:107)Σ(0)(cid:107)1/2
λ3/2m

.

Proof. Let Σ1/2
and let h : R+ → R+ be the function deﬁned as h(t) = 1

be the positive deﬁnite square root of Σ(0), let z(t) = Σ−1/2

2 (cid:107)z(t) − ¯z(t)(cid:107)2. It holds

0

0

y(t), ¯z(t) = Σ−1/2

¯y(t)

0

h(cid:48)(t) = (cid:104)z(cid:48)(t) − ¯z(cid:48)(t), z(t) − ¯z(t)(cid:105)

= −(cid:104)Σ−1/2
= −(cid:104)Σ1/2

0

Σ(t)∇R(Σ1/2

0 z(t)) − Σ1/2

0 ∇R(Σ1/2

0 ¯z(t)), z(t) − ¯z(t)(cid:105)

0 ∇R(Σ1/2

0 z(t)) − Σ1/2
(Σ(t) − Σ(0))∇R(Σ1/2

0 ∇R(Σ1/2

− (cid:104)Σ−1/2
0

0 z(t)), z(t) − ¯z(t)(cid:105).

0 ¯z(t)), z(t) − ¯z(t)(cid:105)

(A(t))

(B(t))

Since the function z (cid:55)→ R(Σ1/2
quantity K introduced in the statement, one has also (cid:107)B(t)(cid:107) ≤ K(cid:107)z(t) − ¯z(t)(cid:107)/
Summing these two terms yields the bound

0 z) is λm-strongly convex, one has that A(t) ≤ −2λmh(t). Using the
λ = K(cid:112)2h(t)/λ.

√

h(cid:48)(t) ≤ K(cid:112)2h(t)/λ − 2λmh(t).
The right-hand side is a concave function of h(t) which is nonnegative for h(t) ∈ [0, K 2/(2λ3m2)]
and negative for higher values of h(t). Since h(0) = 0 it follows that for all t ≥ 0, one has
h(t) ≤ K 2/(2λ3µ2) and the result follows since (cid:107)y(t) − ¯y(t)(cid:107) ≤ (cid:107)Σ(0)(cid:107)1/2(cid:112)2h(t).

17

•

•

W0

×

w0

h

y∗
×

•

•

×
h(w0)

¯h(W0)

h(W0)

Figure 5: There is a small neighborhood W0 ⊂ Rp of the initialization w0, which image by h is a
differentiable manifold in F. In the lazy regime, the optimization paths (both in W and in F) for the
non-linear model h (dashed gray paths) are close to those of the linearized model ¯h (dashed black
paths) until convergence or stopping time (Section 2). This ﬁgure illustrates the under-parameterized
case where p < dim(F).

B.5 Proof of Theorem 2.5 (under-parameterized case)

The setting of this theorem is depicted on Figure 5. By the rank theorem (a result of differential
geometry, see [21, Thm. 4.12] or [1] for a statement in separable Hilbert spaces), there exists open
sets W0, ¯W0 ⊂ Rp and F0, ¯F0 ⊂ F and diffeomorphisms ϕ : W0 → ¯W0 and ψ : F0 → ¯F0 such
that ϕ(w0) = 0, ψ(h(w0)) = 0 and ψ ◦ h ◦ ϕ−1 = πr, where πr is the map that writes, in suitable
bases, (x1, . . . , xp) (cid:55)→ (x1, . . . , xr, 0, . . . ). Up to restricting these domains, we may assume that
¯F0 is convex. We also denote by Πr the r-dimensional hyperplan in F that is spanned by the ﬁrst r
vectors of the basis. The situation is is summarized in the following commutative diagram:

W0

ϕ

¯W0

h

πr

F0

ψ

¯F0

In the rest of the proof, we denote by C > 0 any quantity that depends on m, M and Lipschitz
smoothness constants of h, ψ, ϕ, ψ−1, ϕ−1, but not on α. Although we do not do so, this could be
translated into explicit constants that depends on the smoothness of h and R, on the strong convexity
constant of R and on the smallest positive singular value of Dh(w0) using quantitative versions of
the rank theorem [5, Thm. 2.7].

Step 1. Our proof is along the same lines as that of Theorem 2.4, but performed in Πr which can
be thought of as a straighten up version of h(W0). Consider the function Gα deﬁned for g ∈ ¯F0 as
Gα(g) = R(αψ−1(g))/α2. The gradient and Hessian of Gα satisfy, for v1, v2 ∈ Rp,

∇Gα(g) =

1
(Dψ(g)−1)(cid:124)∇R(αψ−1(g)),
α
(cid:124)
1 (Dψ(g)−1)(cid:124)∇2R(αψ−1(g))Dψ(g)−1v2
D2Gα(g)(v1, v2) = v
1
D2ψ(g)−1(v1, v2)(cid:124)∇R(αψ−1(g)).
α

+

The second order derivative of Gα is the sum of a ﬁrst term with eigenvalues in an interval [C −1, C],
and a second term that goes to 0 as α increases. It follows that Gα is smooth and strongly convex for
α large enough. Note that if R or ψ−1 are not twice continuously differentiable, then the Hessian
computations should be understood in the distributional sense (this is sufﬁcient because the functions
involved are Lipschitz smooth). Also, let g∗ be a minimizer of the lower-semicontinuous closure of

18

Gα on the closure of ¯F0. By strong convexity of R and our assumptions, it holds
2
m

(Gα(0) − Gα(g∗)) ≤

2R(0)
α2m

(cid:107)g∗(cid:107)2 ≤

,

so g∗ is in the interior of ¯F0 for α large enough and is then the unique minimizer of Gα.

Step 2. Now consider T := inf{t ≥ 0 ; wα(t) /∈ W0}. For t ∈ [0, T ), the trajectory wα(t) of the
gradient ﬂow (4) has “mirror” trajectories in the four spaces in the diagram above. Let us look more
particularly at g(t) := πr ◦ ϕ(wα(t)) = ψ ◦ h(wα(t)) for t < T . In the following computation, we
write Dϕ for the value of the differential at the corresponding point of the dynamic Dϕ(wα(t)) (and
similarly for other differentials). By noticing that Dh = Dψ−1DπrDϕ, we have

g(cid:48)(t) = −

= −

1
α
1
α

DψDhDh(cid:124)∇R(αψ−1(g(t))

DπrDϕDϕ(cid:124)Dπr

(cid:124)(Dψ−1)(cid:124)∇R(αψ−1(g(t)).

so g(t) remains in Πr. Also, the ﬁrst r × r block of DπrDϕDϕ(cid:124)Dπr
(cid:124) is positive deﬁnite on Πr,
with a positive lower bound (up to taking W0 and F0 smaller if necessary). Thus by Lemma B.1,
there are constants C1, C2 > 0 independent of α such that, for t ∈ [0, T ), (cid:107)g(t) − g∗(cid:107) ≤ C1(cid:107)g(0) −
g∗(cid:107) exp (−C2t) .

Step 3. Now we want to show that T = +∞ for α large enough. It holds

1
α
and, by Lipschitz-smoothness of Gα (Step 1), (cid:107)∇Gα(g(t))(cid:107) ≤ C

Dh(cid:124)∇R(αh(wα(t)) = Dϕ(cid:124)Dπr

w(cid:48)(t) = −

(cid:124)∇Gα(g(t))

α (cid:107)g(t) − g∗(cid:107) hence

(cid:107)wα(t) − w0(cid:107) ≤

(cid:90) t

exp(−C2s)ds ≤

C
α

C
αC2

.

0
Thus, by choosing α large enough, one has wα(t) ∈ W0 for all t ≥ 0, so T = ∞ and the theorem
follows.

C Experimental details and additional results

C.1 Many neurons dynamics visualized

The setting of Figure 6 is the same as for panels (a)-(b) in Figure 1 except that m = 200, n = 200: it
allows to visualize behavior of the training dynamics for a larger number of neurons. Symmetrized
initialization to set f (w0, ·) = 0 was used on panel (c) but not on panel (b), where we see that the
neurons need to move slightly more in order to compensate for the non-zero initialization. As on
Figure 1, we observe a good behavior in the non-lazy regime for small τ .

(a) Non-lazy training (τ = 0.1)

(b) Lazy (τ = 2, not symmetrized)

(c) Lazy (τ = 2, symmetrized)

Figure 6: Training a two-layer ReLU neural network initialized with normal random weights of
variance τ 2, as in Figure 1, but with more neurons. In this 2-homogeneous setting, changing τ 2 is
equivalent to changing α by the same amount so lazy training occurs for large τ .

19

C.2 Stability of activations

We deﬁne here the “stability of activations” mentioned in Section 3.2. We consider a ReLU layer
(cid:96) of size n(cid:96) in a neural network and the test input data (xi)N
i=1 (the test images of CIFAR10 in our
case). We call zij(T ) ∈ R the value of the pre-activation (i.e. the value that goes through the ReLU
function as an input) of index j on the data sample i, obtained with the parameters of the network at
epoch T . The “stability of activations” for this layer is deﬁned as s(cid:96) := Q
n(cid:96)×N where L is the number
of ReLU layers, Q is the number of indices (i, j) that satisfy sign(zij(Tlast)) = sign(zij(Tinit))
for i ∈ {1, . . . , B} and j ∈ {1, . . . , n(cid:96)}, where Tinit refers to initialization and Tlast to the end of
training. The quantity that we report on Figure 3(a) is the average of s(cid:96) over all ReLU layers of the
VGG-11 network, for various values of α.

C.3 Spectrum of the tangent kernel

In the setting of Figure 3(a), we want to understand why the linearized model (that is, trained for large
α) could not reach low training accuracies in spite of being highly over-parameterized. Figure 7(a)
shows the train and test losses after 70 epochs where we see that the training loss is far from 0 for all
α ≥ 10. We report on Figure 7(b) the normalized and sorted eigenvalues σ2
i of the tangent kernel
Dh(w0)Dh(w0)(cid:124) (notice the log-log scale) evaluated for two distinct input data sets (xi)n
i=1 of
size n = 500: (i) images randomly sampled from the training set of CIFAR10 and (ii) images with
uniform random pixel values. Since there are 10 output channels, the corresponding space F has
10 × 500 dimensions. We observe that there is a gap of 1 order of magnitude between the 0.2% largest
eigenvalues and the remaining ones—which causes the ill conditionning—and then a decrease of
order approximately O(1/i). We observe a similar pattern with the CIFAR10 inputs and completely
random inputs, which suggests that this conditioning is intrinsic to the linearized VGG model. Note
that modifying the neural network architecture to improve this conditioning, or using optimization
methods that are better adapted to ill-conditionned models, is beyond the scope of the present paper.

(a)

(b)

Figure 7: (a) End-of training train and test loss. (b) Spectrum of the tangent kernel Dh(w0)Dh(w0)(cid:124)
for the VGG11 model on two data sets.

20

101103105107 (scale of the model)0.000.010.020.030.040.050.060.07Loss Mean Square Errortrain losstest loss100101102103i1041031021011002iCIFAR10Random