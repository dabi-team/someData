Revisiting Code Search in a Two-Stage Paradigm

Fan Hua,â€  Yanlin Wangb,Â§,â€¡ Lun Duc Xirong Lia Hongyu Zhangd Shi Hanc Dongmei Zhangc
aSchool of Information, Renmin University of China
bSchool of Software Engineering, Sun Yat-sen University
dThe University of Newcastle
cMicrosoft Research Asia

{hufan_hf, xirong}@ruc.edu.cn {lun.du, shihan, dongmeiz}@microsoft.com
hongyu.zhang@newcastle.edu.au, wangylin36@mail.sysu.edu.cn

2
2
0
2

g
u
A
4
2

]
E
S
.
s
c
[

1
v
4
7
2
1
1
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
With a good code search engine, developers can reuse existing code
snippets and accelerate software development process. Current
code search methods can be divided into two categories: tradi-
tional information retrieval (IR) based and deep learning (DL) based
approaches. DL-based approaches include the cross-encoder par-
adigm and the bi-encoder paradigm. However, both approaches
have certain limitations. The inference of IR-based and bi-encoder
models are fast, however, they are not accurate enough; while cross-
encoder models can achieve higher search accuracy but consume
more time. In this work, we propose TOSS, a two-stage fusion code
search framework that can combine the advantages of different code
search methods. TOSS first uses IR-based and bi-encoder models to
efficiently recall a small number of top-k code candidates, and then
uses fine-grained cross-encoders for finer ranking. Furthermore, we
conduct extensive experiments on different code candidate volumes
and multiple programming languages to verify the effectiveness
of TOSS. We also compare TOSS with six data fusion methods. Ex-
perimental results show that TOSS is not only efficient, but also
achieves state-of-the-art accuracy with an overall mean reciprocal
ranking (MRR) score of 0.763, compared to the best baseline result
on the CodeSearchNet benchmark of 0.713.

1 INTRODUCTION

Code search is an important task in software engineering as
it helps software development and maintenance [17, 27]. With a
well-developed code search system, developers can search for code
snippets with natural language and reuse previously written code to
accelerate software development. With the advent of a large corpus
of open source code, finding semantic relevant code snippets among
massive code snippets given natural language queries has become
one of the key challenges in this field [1].

Â§Yanlin Wang is the corresponding author.
â€ Work done during internship at Microsoft Research Asia
â€¡Work done at Microsoft Research Asia

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, 2022, Woodstock, NY
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

(a) Bi-encoder.

(b) Cross-encoder.

Figure 1: The concept diagram of bi-encoder and cross-
encoder code search models. Bi-encoder models are fast as
the code embeddings can be pre-calculated offline. While
cross-encoder models perform full-attention over the input
pair of query and code, which could gain more information.

Existing code search methods can be mainly divided into two
categories, traditional information retrieval (IR) based (text match-
ing) methods [11, 16, 18, 18, 21, 23, 29, 32] and deep learning (DL)
based methods [4, 5, 8â€“10, 12, 14, 22]. However, neither is ideal for
applying the techniques to real world scenarios, especially when
the size of the searched codebase is huge. (1) IR-based methods are
fast in terms of inference time, however, they are often not accurate.
(2) On the contrary, DL-based methods can achieve higher perfor-
mance, however, they are usually slower because of large models.
Specifically, there are mainly two paradigms of model architecture
in existing DL-based code search methods: cross-encoder paradigm
[5, 12] and bi-encoder paradigm [9, 10, 14, 22]. As described in Fig-
ure 1, cross-encoders perform full-attention over the input pairs of
query and code, while bi-encoders map each input (query or code)
independently into a dense vector space. The trade-off of efficiency
and effectiveness also exist between these two paradigms: cross-
encoders achieves better accuracy than bi-encoders. As the code
embeddings can be pre-calculated and stored, bi-encoders are more
efficient than cross-encoders. For many applications cross-encoders
are not practical as they cannot produce separate embeddings for
effective comparing or searching.

In this work, we systematically study the trade-offs between
effectiveness and efficiency of existing code search methods. We
aim to find out a combined solution that can not only keep the high
search performance of cross-encoders but also reduce searching
time. We select and reproduce nine code search methods with pub-
licly available implementations on the CodeSearchNet [14] dataset.

â€¦FCğŸÃ—ğŸLinearCode BaseQuery EncoderCode EncoderQueryCodeLinearSimilarityQueryCross-modal EncoderQueryCodeClassification scoreâ€¦FCğŸÃ—ğŸLinearCode BaseQuery EncoderCode EncoderQueryCodeLinearSimilarityQueryCross-modal EncoderQueryCodeClassification score 
 
 
 
 
 
Conference acronym â€™XX, 2022, Woodstock, NY

Fan Hu et al.

Firstly, we study the effect of different pre-processing operations
in the code search task and find a proper pre-processing method
that can boost the code search performance of text matching meth-
ods. Secondly, we systematically analyze the search performance,
inference time, and recall results of these methods. We find that
different methods are complementary to a certain extent. Therefore,
adopting the classic two-stage or multi-stage retrieval idea in the
field of information retrieval, we design TOSS, a TwO-Stage fuSion
code Search framework. The first stage recalls a certain number
(top-k) of code snippets using text matching or bi-encoder methods
and the second stage uses fine-grained cross-encoders for rerank-
ing the code snippets recalled in the first stage. Finally, through
extensive experiments, we find that improving recall diversity by
combining the recalled code snippets from different first-stage mod-
els can improve the overall code search performance. Our proposed
method in the two-stage paradigm is not only efficient but also
effective - it achieves the state-of-the-art results with an overall
mean reciprocal ranking (MRR) score of 0.763, which is 7.1% higher
than the best baseline method. Besides, with the retrieval results
of different models, one may consider the data fusion methods of
IR, such as Max, Min, and so on. We also compare TOSS with the
six data fusion methods described in Section 2.2. We position this
work as a starting point research for exploring multi-stage retrieval
in the code search task, and we provide tools as the playground for
the community to do more exploration in our framework.
The contributions of this work can be summarized as:

â€¢ We build a two-stage recall & rerank framework for the code
search task and adapt existing methods to this framework
to improve the effectiveness and efficiency of code search.
â€¢ We propose a multi-channel recall method that improves
recall diversity and code search performance in the two-stage
paradigm.

â€¢ Through extensive experiments, we show the effectiveness of
the proposed two-stage paradigm under different scenarios
and data volumes.

2 RELATED WORK
2.1 Code search methods
Early explorations [11, 16, 18, 21, 23, 29, 32] on code search mainly
apply information retrieval (IR) techniques directly, which regard
code search as a text matching task. Queries and code snippets
are both regarded as plain text. The traditional text matching al-
gorithms include BOW (bag-of-words) [24], Jaccard [15], TF-IDF
(term frequency-inverse document frequency) [20], BM25 [19], and
extended boolean model [16].

Since the cross-modal semantic gap is the major challenge for
IR-based code search methods, researchers have explored many
machine learning/deep learning based approaches [5, 9, 10, 12â€“
14, 22, 26, 28] to capture the correlation between query and code
from large-scale training data. Machine learning based code search
models [9, 22] typically learn an embedding for query and code,
and then calculate cross-modal similarity in a shared vector space.
Early work on neural approaches to code search includes CODEnn
[9], which uses RNN to jointly embed code snippets and natural
language queries into a high-dimensional vector space. Husain et
al. [14] proposed the CodeSearchNet benchmark and four baselines

for code search, i.e., NBoW, 1D-CNN, biRNN, and SelfAtt. After the
large-scale pre-training model BERT [3] was proposed, Feng et al.
[5] proposed CodeBERT, which is a model pre-trained on unlabeled
source code and comments.

For transformer-based code search methods, there are mainly
two types of methods: bi-encoder and cross-encoder architecture.
GraphCodeBERT [10] is a bi-encoder method, which encodes query
and code into dense embeddings independently. CodeBERT [5] and
CoCLR [12] are cross-encoders, where query and code are jointly
encoded and we get a score that predicts whether a code answers
a given query. Cross-encoder models process the query paired
with each candidate code sequence, which is helpful to capture
the relationship between two modalities (natural language and
programming language). The concept diagram of bi-encoder and
cross-encoder model is shown on Figure 1.

2.2 Fusion methods
In the code search field, the fusion of IR, bi-encoder and cross-
encoder code search methods are rarely explored. Some researchers
have tried to use IR-based methods to improve the ML code search
models. Sachdev et al. [22] used Word2vec method to get word
embedding and aggregate representation of all the words with TF-
IDF weights. Xie et al. [31] calculated code similarity based on
Siamese Neural Network. The weights of the word embeddings are
fitted by TF-IDF. Recently, Gotmare et al. [7] proposed CasCode
to fuse bi-encoder and cross-encoder models. However, CasCode
only fuses the recall results of one bi-encoder model. In contrast,
we propose a two-stage paradigm to fuse IR, bi-encoder and cross-
encoder code search methods, which significantly improve the
search performance and have a relatively fast search speed.

For fusion of the retrieval results of different source, one may
consider the unsupervised data fusion method proposed in the
information retrieval community, such as CombANZ [6, 25], Max,
Min, CombMNZ [6, 25], CombSUM [6, 25] and BordaCount [2]. The
CombANZ method combines the similarity scores by computing
the average of the non-zero scores. The Max and Min methods
combine the similarity score sets by selecting the maximum or
minimum one as the final score. The CombMNZ combines the
similarity scores by multiplying the summation of all scores with the
number of non-zero scores assigned to the method. The CombSUM
combines the similarity scores by simply summing up their scores.
The Borda count converts the similarity scores into ranks - codes
with higher scores would obtain smaller ranks. For each element,
this method sums up the ranking points of an element given by a set
of models. The ranking point of a code is defined as the substraction
of the codeâ€™s rank in the list from the total number of codes in the
codebase.

Our two-stage code search paradigm TOSS is different from data
fusion methods. Instead of fusing similarity score sets, TOSS first
recalls high-quality code snippets with first stage models, and then
re-ranks this candidate set according to the second stage model.

Revisiting Code Search in a Two-Stage Paradigm

Conference acronym â€™XX, 2022, Woodstock, NY

Figure 2: The overall framework of our two-stage paradigm TOSS.

3 FRAMEWORK
3.1 Preliminaries
Given a query ğ‘ and a codebase ğ¶, the goal of code search is to find
the best code snippet that matches the query ğ‘ from the codebase
ğ¶. Current code search methods can be uniformly formulated as
follows:

M (ğ‘, ğ‘),

max
ğ‘ âˆˆğ¶

(1)

where M (Â·, Â·) is a similarity function to compute the match degree
of the given query and a code snippet. The main difference of
various code search methods is the design of the similarity function
M (Â·, Â·).

For a traditional IR-based code search method, the similarity
function Mğ¼ğ‘… (Â·, Â·) is designed as a text-based matching function.
For instance, BOW (bag of words) is to count the number of differ-
ent words within the query and the code snippets, respectively. It
then forms vector representations and conducts cosine similarity
calculation.

For a bi-encoder deep-learning model, we first transform the
textual representations of the query ğ‘ and the code snippets ğ‘ to
vector representations ğ‘’ğ‘ and ğ‘’ğ‘ by neural network encoders, and
then calculate the similarity (or distance) measures in Euclidean
space such as Cosine similarity or Euclidean distance to obtain the
cross-modal similarity score ğ‘ ğ‘ğ‘– . The calculation can be formalized
as follows:

(2)

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

eq = Î“(ğ‘)
ec = Î“â€²(ğ‘), ğ‘ âˆˆ ğ¶
ğ‘ ğ‘ğ‘– = ğ‘ ğ‘–ğ‘š(eq, ec)
where Î“ and Î“â€² are two well-trained neural network encoders that
can be the same networks or different networks according to the
customized settings. So the measure function in a bi-encoder deep-
learning model is defined as ğ‘€ğµğ¼ (Â·, Â·) = ğ‘ ğ‘–ğ‘š(Î“(Â·), Î“â€²(Â·)). The neural
network encoders Î“ and Î“â€² are learned from labeled paired data.
For a cross-encoder deep-learning model, we concatenate the
query and the code snippet, and input them to a neural network
model to directly calculate the cross-modal similarity ğ‘ ğ‘œ :

ğ‘ ğ‘œ = Mğ‘‚ ([ğ‘, < ğ‘†ğ¸ğ‘ƒ >, ğ‘]),

(3)

câˆˆCâ€²
Mrank (c, q),

where < ğ‘†ğ¸ğ‘ƒ > represents the separator between query and code.
Since the embeddings of code snippets cannot be calculated offline,
the search speed is always slower than the other two types of
methods.

3.2 Two-stage code search paradigm
Figure 2 depicts the overall framework of our two-stage code search
paradigm TOSS. Instead of directly selecting the best code snippet
according to the overall target Eq. (1), the two-stage code search
paradigm firstly recalls a candidate set of code snippets with a
recall similarity function Mğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ and then re-ranks the candidate
set according to a ranking similarity function Mğ‘Ÿğ‘ğ‘›ğ‘˜ . Formally:

First Stage : Csub = arg max
Câ€² âŠ‚C, |Câ€² |=K

âˆ‘ï¸

Mrecall (c, q),

(4)

(5)

Second Stage :

câˆ— = arg max
câˆˆCsub
where ğ¶ğ‘ ğ‘¢ğ‘ is the candidate set that contains top ğ¾ code snippets
with the recall similarity function Mğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ , and ğ‘âˆ— is the final se-
lected code snippet after re-ranking. The recall method Mğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
would be fast but lack accuracy, while the ranking method Mğ‘Ÿğ‘ğ‘›ğ‘˜
could be complex. This code search paradigm enables a more ac-
curate but time-consuming similarity function Mğ‘Ÿğ‘ğ‘›ğ‘˜ , especially
when the code base is very large.

In addition, by applying multiple search methods in the recall
stage, we can incorporate the advantages of different methods and
increase the diversity of the candidate set. TOSS uses ğ‘š fast first-
stage models for recall, and then combine those returned code
snippet sets as the final candidate set. It can be formalized as:

ğ¶ (ğ‘–)
ğ‘ ğ‘¢ğ‘ = arg max

ğ¶â€² âŠ‚ğ¶

âˆ‘ï¸

ğ‘ âˆˆğ¶â€², |ğ¶â€² |=ğ¾

M (ğ‘–)

ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ (ğ‘, ğ‘), for 0 < i â‰¤ m (6)

ğ‘š
(cid:216)

ğ¶ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ =

ğ¶ (ğ‘–)
ğ‘ ğ‘¢ğ‘

,

(7)

ğ‘–
ğ‘ ğ‘¢ğ‘ is the recalled top ğ¾ candidate set according to the ğ‘–-th

where ğ¶ (ğ‘–)
recall method, and ğ¶ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ is the final candidate set.

In conclusion, we expect this two-stage framework to accomplish
two objectives: improved search performance and reduced search

Stage 1CodeBaseOnline & Local codesPre-processingBM25Query: Return the bubble sort.GraphCodeBERTâ€¦â€¦Top-K recalled code snippetsCodeBERTReranked codesStage 1 modelsâ€¦â€¦â€¦Combined CandidatesStage 2Stage 2 modelConference acronym â€™XX, 2022, Woodstock, NY

Fan Hu et al.

time. More specifically: (1) High-quality code snippets can be re-
called in the first stage and ranked higher in the fine-grained second
stage re-ranking. (2) The two-stage paradigm should considerably
reduce research time by recalling a small set of code candidates for
second stage re-ranking.

4 EXPERIMENTAL DESIGN
4.1 Datasets
We conduct experiments on the widely used CodeSearchNet [14]
dataset. Following Guo et al. [10], we filter low-quality queries and
expand the retrieval set to the whole code corpus. Note that we use
the CodeSearchNet Python dataset, which contains 14,918 queries
and 43,827 code candidates, in most of the experiments. We also
generalize our findings to multiple programming languages with
the full CodeSearchNet dataset in Section 5.5.

4.2 Baselines
We select code search baselines to study based on their represen-
tativeness and their availability. The baselines that satisfy the fol-
lowing three criteria are chosen: 1) The source code is publicly
available. 2) The overall model is adaptable to all the six program-
ming languages in the CodeSearchNet dataset. 3) The paper is peer
reviewed if it is proposed in a research paper. As a result, we select
nine code search baselines and divide them into three categories: IR-
based text matching methods (Jaccard [15], BOW [24], TFIDF [20],
and BM25 [19]), cross-encoder deep learning based methods (Code-
BERT [5] and CoCLR [12]), and bi-encoder deep learning based
methods (CODEnn [9], GraphCodeBERT [10] and CodeBERT-bi).
â€¢ Jaccard. We use jaccard_score1 from the sklearn python

package to implement the Jaccard similarity method.

â€¢ BOW. We use CountVectorizer2 to convert a query or code
to a vector of token counts. The cross-modal similarity is then
calculated by cosine similarity of code and query vectors.
â€¢ TFIDF. We use TfidfVectorizer3 to convert a query or
code to a vector of TF-IDF features. The cross-modal sim-
ilarity is calculated by cosine similarity of code and query
TF-IDF features.

â€¢ BM25. We use the python package Rank-BM254 to imple-

ment the BM25 similarity.

â€¢ CODEnn. CODEnn is proposed in DeepCS [9], which jointly
embeds code snippets and natural language descriptions into
a high-dimensional vector space with LSTM. We use the
implementation from their released repository5.

â€¢ CodeBERT. A bi-encoder Transformer-based pre-trained
model for programming language and natural language. We
use the implementation from their released repository.6

1https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.
html
2https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.
CountVectorizer.html
3https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.
TfidfVectorizer.html
4https://pypi.org/project/rank-bm25/
5https://github.com/guxd/deep-code-search
6https://github.com/microsoft/CodeBERT

â€¢ CoCLR. A CodeBERT-based contrastive learning method for
code search. We use the implementation from their released
repository.7

â€¢ GraphCodeBERT. A dataflow aware pre-trained model
based on CodeBERT. We use the implementation from their
released repository.8

â€¢ CodeBERT-bi. CodeBERT bi-encoder variant described in
the Appendix of the original paper [5], where CodeBERT
first encodes query and code separately, and then calculates
the similarity by dot product.

4.3 Evaluation metrics
To evaluate the performance of code search models, we apply two
popular evaluation metrics on the test set: MRR (Mean Reciprocal
Ranking) and R@K (top-K accuracy, K=1, 5, 10, 100, 1000). They are
commonly used for in previous code search studies [5, 9, 10, 12, 14,
16, 22]. The higher the MRR and R@K values, the better the code
search performance. In addition, we report per query search time
as the efficiency measure.

4.4 Experimental settings
For training, we use the default hyper-parameter settings provided
by each method. For evaluation, the batch size is 256. For bi-encoder
models, since source code can be processed offline, we do not in-
clude this computation cost in search time calculation. All exper-
iments are conducted on a machine with Intel Xeon E5-2698v4
2.2Ghz 20-Core CPU and one Tesla V100 32GB GPU.

4.5 Research questions
To evaluate the effectiveness of the proposed approach, we answer
the following research questions:

â€¢ RQ1: How effective are different code pre-processing meth-

ods for text matching methods?

â€¢ RQ2: How do different baseline methods perform in terms

of search performance and search efficiency?

â€¢ RQ3: How effective is the proposed paradigm TOSS in dif-

ferent settings?

â€¢ RQ4: How effective is the proposed paradigm TOSS under

different code volumes?

â€¢ RQ5: How effective is the proposed two stage paradigm for
multiple programming languages compared to the baseline
methods?

5 EXPERIMENTAL RESULTS

5.1 The impact of different pre-processing

operations (RQ1)

With the advent of deep learning approaches for code search, tradi-
tional IR-based code search approaches are often considered weak
baselines [10, 14]. However, the IR-based approaches also have
their own advantages, such as simplicity, training-free, and fast
search speed, etc. Since text search methods are very sensitive to
data pre-processing methods [30], we investigate the effect of four

7https://github.com/Jun-jie-Huang/CoCLR
8https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/codesearch

Revisiting Code Search in a Two-Stage Paradigm

Conference acronym â€™XX, 2022, Woodstock, NY

Table 1: The code search performance (MRR) of different
text matching methods with different text pre-processing
tools.

Pre-processing tool
SPS DS RS POS

Text matching method

Jaccard

BOW TF-IDF

BM25 Overall

Ã—
âœ“
Ã—
Ã—
Ã—
âœ“
âœ“
âœ“

Ã—
Ã—
âœ“
Ã—
Ã—
âœ“
âœ“
âœ“

Ã—
Ã—
Ã—
âœ“
Ã—
Ã—
âœ“
âœ“

Ã—
Ã—
Ã—
Ã—
âœ“
Ã—
Ã—
âœ“

0.1122
0.1823
0.1353
0.1842
0.1162
0.2321
0.2366
0.2425

0.0949
0.1580
0.1079
0.1627
0.1010
0.1955
0.2012
0.2220

0.1341
0.2274
0.1251
0.2326
0.1384
0.2194
0.2257
0.2397

0.2914
0.4250
0.3056
0.4215
0.2921
0.4438
0.4444
0.4523

0.1582
0.2482 (56.9% â†‘)
0.1685 (6.5% â†‘)
0.2503 (58.2% â†‘)
0.1619 (2.39% â†‘)
0.2727 (72.4% â†‘)
0.2770 (75.1% â†‘)
0.2891 (82.8% â†‘)

data pre-processing operations that are commonly used in previous
work on code search:

matching methods (Jaccard, BOW, TFIDF, and BM25). This result is
consistent with previous work, as DL-based code search methods
are data driven, which could learn embedding and find patterns
from training data of different modalities (programming languages
and natural languages). While text matching methods are rule based,
which are sub-optimal compared with deep search methods espe-
cially in the code search scenario with different modalities. How-
ever, rule-based methods need no training data and less comput-
ing resource, which is more practical in industrial scenarios. The
cross-encoder model CodeBERT performs the best among the nine
methods in terms of search performance. However, in terms of time
efficiency (inference time), bi-encoders and text matching methods
greatly outperforms cross-encoder methods CodeBERT and CoCLR.
Since cross-encoder methods cannot calculate code features offline,
they require a lot of computing resources and may not be suitable
for code search in industry scenarios with a large codebase.

â€¢ SPS: Split pascal and snake case. For example, it splits â€œTwoStageMethodâ€

to â€œTwo Stage Methodâ€ and â€œvectorizer_paramâ€ to â€œvector-
izer paramâ€. We use the regular expression (re) package9 to
implement SPS.

â€¢ DS: Delete the English stop-words, such as â€œbothâ€, â€œmoreâ€,
â€œsomeâ€ and so on. We use stop-words implementation from
the NLTK package.10

â€¢ RS: Ronin Split, such as splitting "showtraceback" to "show

trace back". We use ronin from spiral11.

â€¢ POS: Part-of-speech restoration. For example, it restores
â€œconfigsâ€ to â€œconfigâ€. We use the WordNetLemmatizer from
the NLTK package12 for implementation.

To study the influence of different data pre-processing operations
and find a suitable combination, we conduct a series of experiments
on their combinations and evaluate the search performance (in
MRR). As shown in Table 1, different data pre-processing methods
can affect the overall performance by a large margin. The effect of
using RS alone is the most obvious, with an average MRR improve-
ment of 58.2%. With all pre-processing methods, four text matching
methods achieve best overall search performance.

In summary, we find that:
â€¢ Different text pre-processing methods can affect the overall

performance by a noticeable margin.

â€¢ SPS+DS+RS+POS is the recommended code pre-processing
method, as the overall performance of four text matching meth-
ods is best.

5.2 Analysis of baseline methods (RQ2)
In order to explore the complementary of various baselines, we
study the nine baseline methods in the following aspects:
â€¢ Search performance measure: MRR and R@k.
â€¢ Need training: whether the method requires training data.
â€¢ Search efficiency measure: per query time.

As shown in Table 2, we observe that in general, most deep
learning-based code search methods (CodeBERT, CodeBERT-bi,
CoCLR, and GraphCodeBERT) perform better than IR-based text

9https://docs.python.org/3/library/re.html
10https://www.nltk.org/api/nltk.corpus.html
11https://github.com/casics/spiral
12https://www.nltk.org/api/nltk.stem.html

With the above analysis, we conclude following key points:
â€¢ Most deep code search methods perform better than text match-
ing methods, while the latter methods need no training data
and less computing resource.

â€¢ The cross-encoder model CodeBERT performs the best among

the nine methods.

â€¢ Cross-encoder models are time consuming and not suitable for

retrieval in large codebases.

â€¢ Different baselines have their own characteristics. If we can
integrate them well, the search performance and efficiency
could be improved.

5.3 Effectiveness of the proposed paradigm

TOSS (RQ3)

In this section, we want to explore the effectiveness of our pro-
posed two-stage framework. The first-stage models are chosen
from text matching and bi-encoder models. The second-stage mod-
els are chosen from cross-encoder models, which have the slow
time performance but full attention to the input pair of query and
code. We first explore the performance of a single first-stage model
+ second-stage model. Then we try multiple first-stage models
+ second-stage model. We find the best two-stage model TOSS
[ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ . Finally, we analyze the recall
results of different first-stage models and find that multi-channel
recalls have more overlap with the ground truth (GT), which leads
to the boosting search performance of our proposed TOSS.

For first-stage models selection, We select 4 models with better
accuracy and time performance from IR and bi-encoder models,
i.e. GraphCodeBERT, CodeBERT-bi, BM25 and Jaccard. We put the
top-K (K=5, 10, 100) code snippets recalled from the first-stage
model into the second-stage model for re-ranking. We report MRR
for search performance, per query time for efficiency measurement.
The results of CodeBERT as the second-stage model are
shown in Table 3. Due to space limit, the results of CoCLR
as the second-stage model are shown in the replication pack-
age. We observe that compared with the model used alone, the
TOSS method can significantly improve the code search perfor-
mance. TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ achieves best per-
formance. Compared with the best single-model CodeBERT, the

Conference acronym â€™XX, 2022, Woodstock, NY

Fan Hu et al.

Table 2: The code search performance of nine baseline methods. Per query time is the average retrieval seconds for randomly
selected 100 queries. We repeat each time calculating experiment three times and report the mean and standard deviation. The
cross-encoder DL models are time-consuming because the code features cannot be calculated offline.

Model

MRR Need training Per query time / s R@1 R@5 R@10 R@100 R@1000

Text matching IR model
Jaccard
BOW
TFIDF
BM25

Bi-encoder DL model
CODEnn
CodeBERT-bi
GraphCodeBERT

0.2425 No
0.2220 No
0.2397 No
0.4523 No

0.1775 Yes
0.6669 Yes
0.6948 Yes

Cross-encoder DL model
CodeBERT
CoCLR

0.7015 Yes
0.6349 Yes

0.0130 Â± 0.0004
0.0011 Â± 0.0000
0.0011 Â± 0.0001
0.0062 Â± 0.0003

0.0033 Â± 0.0001
0.0021 Â± 0.0003
0.0048 Â± 0.0002

17.7
16.1
16.9
35.6

11.1
57.4
59.3

30.7
28.1
30.8
56.4

23.9
77.9
82.1

802.43 Â± 51.29
766.27 Â± 47.79

62.4
51.6

79.2
78.3

36.7
33.6
37.1
63.4

30.7
83.3
87.3

83.7
84.6

59.4
56.6
62.9
81.0

57.3
94.6
96.5

94.5
95.7

83.0
82.6
87.1
92.0

82.3
98.8
99.1

98.7
99.0

Figure 3: Visualization results of the top-1 recalled samples based on the four baselines being used and ground truth (GT) in
the CodeSearchNet python test set. The diversity of recalled code candidates is higher for methods of different paradigms. The
coincident number of recalls for fusing (GraphCodeBERT and BM25) is 5,108, which is less than text matching methods (BM25
and Jaccad) (5,222) and deep code search methods (GraphCodeBERT and CodeBERT-bi) (8,071). Besides, different methods can
recall a part of unique ground truth code snippets. Best viewed in color.

MRR is improved by 8.6%, and the retrieval time is reduced to 1/1400
of the original method.

We also find that using multiple stage1 models for recall per-
forms better than using single stage1 models. We attribute it to
the complementarity of multiple models in the first stage, which
can improve the recall ratio of the ground truth. To validate our
assumption and explore how much complementarity the first-stage
model recall have, we visualize the three combinations top 1 re-
call of the first-stage models, i.e., ground truth (GT) with two text
matching methods (BM25 and Jaccad), two bi-encoder DL methods
(GraphCodeBERT and CodeBERT-bi) and fusing methods (Graph-
CodeBERT and BM25). The result is shown in Figure 3. We obtain
two keypoints. First, the recall results of different models vary
greatly, and the intersection accounts for a small proportion of
the total. Second, for the diversity of recalled results, the model
from the two different paradigms are more diverse. The coincident
number of recalls for fusing (GraphCodeBERT and BM25) is 5,108,

which is less than text matching methods (BM25 and Jaccad) (5,222)
and deep code search methods (GraphCodeBERT and CodeBERT-bi)
(8,071). The performace of TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
is better than that of TOSS [ğµğ‘€25+ğ½ ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ and TOSS
[ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ âˆ’ğ‘ğ‘–+ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ , which proves that the
fusion of two different channel recall is effective. The performance
of TOSS [ğ´ğ¿ğ¿ ğ‘†ğ‘¡ğ‘ğ‘”ğ‘’1 ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ğ‘  ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ is not the best, which
means that using too many first-stage models may introduce more
negative samples, resulting in lower performance. The visualization
of the speed versus accuracy trade-off of nine baselines and TOSS
is shown on Figure 5.

We also evaluate the search performance of TOSS with six IR
data fusion methods presented in Section 2.2. We try to fuse the
similarity scores of GraphCodeBERT and BM25 with the top-K (K=5,
10, 100) re-ranking CodeBERT similarity scores. Note that we use
zero-one normalization to make the similarity scores comparable.

(a) GT, BM25 and Jaccard.(b) GT, GraphCodeBERTand CodeBERT-bi(c) GT, GraphCodeBERTand BM25GTGraphCodeBERTBM25GTGraphCodeBERTCodeBERT-biGTBM25Jaccard(a) GT+BM25+Jaccard(b) GT+GraphCodeBERT+CodeBERT-bi(c) GT+GraphCodeBERT+BM25Revisiting Code Search in a Two-Stage Paradigm

Conference acronym â€™XX, 2022, Woodstock, NY

Table 3: The two-stage code search performance of different baseline combinations. Per query time is the average retrieval
seconds for randomly selected 100 queries. We repeat each experiment three times and report the mean and standard deviation.
Stage 2 model: CodeBERT.

Combination

Single stage1 model + CodeBERT
TOSS [ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
TOSS [ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ âˆ’ğ‘ğ‘– ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
TOSS [ğ½ ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡

MRR
Top 5 Top 10 Top 100

Per Query Time / s

Top 5

Top 10

Top 100

0.5428
0.7392
0.6911
0.3093

0.5852
0.7512
0.7163
0.3530

0.6884
0.7589
0.7566
0.5264

0.076 Â± 0.002 0.127 Â± 0.002 1.149 Â± 0.006
0.539 Â± 0.001 0.593 Â± 0.001 1.619 Â± 0.007
0.163 Â± 0.006 0.219 Â± 0.005 1.210 Â± 0.047
0.064 Â± 0.001 0.123 Â± 0.002 1.137 Â± 0.009

Multiple stage1 models + CodeBERT
0.5706
TOSS [ğµğ‘€25+ğ½ ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
TOSS [ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ âˆ’ğ‘ğ‘–+ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ 0.7548
TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡
TOSS [ğ´ğ¿ğ¿ ğ‘†ğ‘¡ğ‘ğ‘”ğ‘’1 ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ğ‘  ]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡

0.6110
0.7588
0.7553 0.7595
0.7462
0.7293

0.7058
0.7608
0.7607
0.7593

0.133 Â± 0.003 0.241 Â± 0.001 2.278 Â± 0.017
0.695 Â± 0.004 0.806 Â± 0.007 2.847 Â± 0.010
0.598 Â± 0.001 0.712 Â± 0.000 2.746 Â± 0.009
0.819 Â± 0.003 1.043 Â± 0.003 5.087 Â± 0.049

(a) MRR curves.

(b) Search time (log) curves.

Figure 4: Performance curves with different code volumes. We set the code volume to be from 200 to 40000. TOSS refers to TOSS
[ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ . Since we randomly select a specific number of codes from the CSN python code candidates,
we repeat each calculation three times and report the average results and the error bounds.

Table 4: MRR performance of different IR data fusion meth-
ods. Fusion data includes the similarity scores of Graph-
CodeBERT and BM25, and the re-ranking score of Code-
BERT with top-K code snippets recalled.

Fusion methods

CombANZ
Max
Min
CombMNZ
CombSUM
BordaCount

TOSS

Top 5

0.5897
0.4892
0.5524
0.5770
0.5825
0.5406

MRR
Top 10

0.5846
0.4923
0.6286
0.6154
0.6175
0.6061

Top 100

0.4787
0.4938
0.6290
0.5787
0.5787
0.6138

0.7553

0.7595

0.7607

The Mean Reciprocal Ranking (MRR) results are shown in Table 4.
As we can see, TOSS outperforms all six fusion methods.
With above analysis, we conclude four key points:
â€¢ TOSS can significantly improve the code search accuracy with

acceptable time consumption.

â€¢ The variant TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡

achieves the best code search performance.

â€¢ Compared to single-channel method, multi-channel methods
can recall more high-quality code candidates and boost the
search performance.

â€¢ TOSS is more effective than the six selected data fusion meth-

ods.

5.4 Analysis of different code volumes (RQ4)
In this section, to demonstrate the advantage of TOSS in differ-
ent scenarios with different codebase sizes, especially when code

0500010000150002000025000300003500040000Codebase volumes0.30.40.50.60.70.80.9MRRTOSSBM25CodeBERTGraphCodeBERT0500010000150002000025000300003500040000Codebase volumes5.02.50.02.55.07.510.012.515.0Total time (log)TOSSBM25CodeBERTGraphCodeBERTConference acronym â€™XX, 2022, Woodstock, NY

Fan Hu et al.

Table 5: MRR performance on six languages of the CodeSearchNet dataset. TOSS refers to TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ .

Model / Method Ruby

JavaScript

Go

Python

0.2303
BOW
0.2390
TF
0.2202
Jaccard
0.5054
BM25
0.3420
CODEnn
0.6790
CodeBERT-bi
GraphCodeBERT 0.7030

0.1841
0.2042
0.1913
0.3932
0.3550
0.6200
0.6440

0.3502
0.3625
0.3453
0.5723
0.4951
0.8820
0.8970

0.2220
0.2397
0.2425
0.4523
0.1775
0.6669
0.6920

Java

0.2447
0.2620
0.2354
0.4261
0.1083
0.6760
0.6910

PHP

0.1929
0.2149
0.1822
0.3352
0.1407
0.6280
0.6490

Overall

0.2374
0.2537
0.2362
0.4474
0.2698
0.6920
0.7127

TOSS

0.7645 (8.7%â†‘) 0.6962 (8.1%â†‘) 0.9181 (2.4%â†‘) 0.7595 (9.8%â†‘) 0.7497 (8.5%â†‘) 0.6922 (6.7%â†‘) 0.7634 (7.1%â†‘)

For MRR performance, as shown in Figure 4(a), among these
models TOSS has consistently the best accuracy under different
code volumes. Under small code volume, GraphCodeBERT performs
well. While as the code volume increases, the search performance of
GraphCodeBERT drops rapidly, which indicates that it is reasonable
in our model TOSS to put GraphCodeBERT in the first stage and
re-rank the small number of code candidates that are recalled.

For search time performance, as shown in Figure 4(b), text match-
ing method BM25 has the shortest total time and cross-encoder
method CodeBERT has the longest total time. Since the calculation
time of the second stage is much larger than that of the first stage,
and the calculation time of the second stage only depends on the
size of k and has no dependency on the size of the original codebase,
the search time of TOSS basically does not change much with the
code volume.

In summary, we obtain two key findings:
â€¢ As the code volume increases, the search performance of each

method decreases and the search time increases.

â€¢ The code search performance of TOSS is robust across differ-
ent code volumes, and the computation time is fast and stable.
Therefore, TOSS is a promising paradigm for real-world appli-
cations with large codebases.

5.5 Compare with the baselines in multiple

programming languages (RQ5)

In this section, we compare the overall search performance of TOSS
with baselines and study whether the performance improvement of
TOSS still holds for other programming languages. As CodeBERT
and CoCLR are time-consuming and doesnâ€™t suitable for large-scale
retrieval, these two cross-encoder methods are not included. We
show the Mean Reciprocal Ranking (MRR) results in Table 5. As
we can see, TOSS outperforms all baseline methods on all the six
programming languages. The average MRR of TOSS is 0.763, bring
a 7.1% gain to the best baseline method GraphCodeBERT. Note that
the conclusion still hold for other metrics, due to space limitation,
we only show the results in MRR. The recall results can be found
in our replication package.

6 CONCLUSION
In this paper, we present TOSS, a two-stage recall & rerank frame-
work for code search. It adapts existing methods to this framework
to improve both effectiveness and efficiency of code search. With
multi-channel first stage method, we improve recall diversity and

Figure 5: Visualization of the speed versus accuracy trade-
off of nine baselines and our two-stage method. Dataset:
CodeSearchNet python test set. The area of the circle is pro-
portional to the size of the model. The two-stage method
TOSS refers to TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ . With
two-stage method, we are able to achieve top performance
comparable to the best single model CodeBERT, while re-
quiring substantially lesser inference time.

search techniques are applied to real world applications with huge
codebase, we study the impact of different code volumes for TOSS
and baselines. We choose four methods for comparison, i.e., BM25
(text matching), GraphCodeBERT (bi-encoder), CodeBERT (cross-
encoder) and our TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ (our two-
stage paradigm). Each method is the best model in their respective
categories. Due to the size limitation of the CodeSearchNet python
dataset, the codebase cannot grow infinitely. We set the code vol-
ume to increase from 200 to 40,000. Since we randomly select a
specific number of codes from the CodeSearchNet python code
candidates, we repeat each calculation three times and report the
average results and the error bounds.

We show the search performance MRR and per query time (log)
with different code volumes in Figure 4. From the results, we can
observe that as the code volume increases, the search performance
of each method decreases and the search time increases.

123456Total time (log)0.20.30.40.50.60.70.8MRRJaccardBowTFIDFBM25CODEnnCodeBERT-biGraphCodeBERTCodeBERTCoCLRTOSSRevisiting Code Search in a Two-Stage Paradigm

Conference acronym â€™XX, 2022, Woodstock, NY

[24] Hinrich SchÃ¼tze, Christopher D Manning, and Prabhakar Raghavan. 2008. Intro-
duction to information retrieval. Vol. 39. Cambridge University Press Cambridge.
[25] Joseph A Shaw and Edward A Fox. 1995. Combination of multiple searches. NIST

SPECIAL PUBLICATION SP (1995), 105â€“105.

[26] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Im-
proving code search with co-attentive representation learning. In Proceedings of
the 28th International Conference on Program Comprehension.

[27] Janice Singer, Timothy C. Lethbridge, Norman G. Vinson, and Nicolas Anquetil.
1997. An examination of software engineering work practices. In CASCON. IBM,
21.

[28] Zhensu Sun, Li Li, Yan Liu, and Xiaoning Du. 2022. On the Importance of Building

High-quality Training Datasets for Neural Code Search. arXiv (2022).

[29] Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang Phan, Trong Duc Nguyen,
and Tien N Nguyen. 2017. Combining word2vec with revised vector space model
for better code retrieval. In ICSE-C.

[30] S Vijayarani, Ms J Ilamathi, Ms Nithya, et al. 2015. Preprocessing techniques for
text mining-an overview. International Journal of Computer Science & Communi-
cation Networks (2015), 7â€“16.

[31] Chunli Xie, Xia Wang, Cheng Qian, and Mengqi Wang. 2020. A source code
similarity based on Siamese neural network. Applied Sciences 10, 21 (2020), 7519.
IECS: Intent-enforced code search via
extended Boolean model. Journal of Intelligent & Fuzzy Systems (2017), 2565â€“
2576.

[32] Yangrui Yang and Qing Huang. 2017.

further improve code search performance in the two-stage para-
digm. We evaluate different two stage model combinations and find
the best two-stage model TOSS [ğºğ‘Ÿğ‘ğ‘â„ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ +ğµğ‘€25]+ğ¶ğ‘œğ‘‘ğ‘’ğµğ¸ğ‘…ğ‘‡ ,
which means that GraphCodeBERT and BM25 are used as the first
stage methods and CodeBERT is used as the second stage method.
We conduct extensive experiments on large-scale benchmark Code-
SearchNet with six programming languages (Ruby, JavaScript, Go,
Python, Java, PHP) and the results confirm its effectiveness in dif-
ferent scenarios and with different data volumes.

REFERENCES
[1] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton.
2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput.
Surv. 51, 4 (2018), 81:1â€“81:37.

[2] Javed A Aslam and Mark Montague. 2001. Models for metasearch. In SIGIR.

276â€“284.

[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL-HLT.

[4] Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and Dongmei Zhang.
2021. Is a single model enough? mucos: A multi-model ensemble learning ap-
proach for semantic code search. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management. 2994â€“2998.

[5] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In EMNLP.
[6] Edward A Fox, M Prabhakar Koushik, Joseph Shaw, Russell Modlin, Durgesh Rao,
et al. 1993. Combining evidence from multiple searches. In The first text retrieval
conference (TREC-1). 319â€“328.

[7] Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and Steven CH Hoi. 2021.
Cascaded Fast and Slow Models for Efficient Semantic Code Search. arXiv (2021).
[8] Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang,
and Michael R Lyu. 2022. Accelerating Code Search with Deep Hashing and
Code Classification. In ACL.

[9] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In

ICSE.

[10] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In ICLR.

[11] Emily Hill, Lori Pollock, and K Vijay-Shanker. 2011.

Improving source code
search with natural language phrasal representations of method signatures. In
ASE.

[12] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming
Zhou, and Nan Duan. 2021. CoSQA: 20,000+ Web Queries for Code Search and
Question Answering. In ACL.

[13] Qing Huang, An Qiu, Maosheng Zhong, and Yuan Wang. 2020. A code-description
representation learning model based on attention. In 2020 IEEE 27th International
Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE.

[14] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv (2019).

[15] Paul Jaccard. 1901. Ã‰tude comparative de la distribution florale dans une portion

des Alpes et des Jura. Bull Soc Vaudoise Sci Nat (1901), 547â€“579.

[16] Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei Wang, Dongmei Zhang, and
Jianjun Zhao. 2015. Codehow: Effective code search based on api understanding
and extended boolean model (e). In ASE.

[17] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query
Expansion Based on Crowd Knowledge for Code Search. IEEE Trans. Serv. Comput.
9, 5 (2016), 771â€“783.

[18] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query
IEEE Transactions on

expansion based on crowd knowledge for code search.
Services Computing (2016), 771â€“783.

[19] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance frame-

work: BM25 and beyond. Now Publishers Inc.

[20] Stephen E Robertson and K Sparck Jones. 1976. Relevance weighting of search

terms. Journal of the American Society for Information science (1976), 129â€“146.

[21] Barbara Rosario. 2000. Latent semantic indexing: An overview. Techn. rep.

INFOSYS (2000), 1â€“16.

[22] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In MAPL.
[23] Abdus Satter and Kazi Sakib. 2016. A search log mining based query expansion

technique to improve effectiveness in code search. In ICCIT.

