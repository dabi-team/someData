Boda-RTC: Productive Generation of Portable,
Efﬁcient Code for Convolutional Neural Networks
on Mobile Computing Platforms

Matthew W. Moskewicz

Forrest N. Iandola

Kurt Keutzer

University of California, Berkeley
{moskewcz,forresti,keutzer}@eecs.berkeley.edu

Abstract—The popularity of neural networks (NNs) spans
academia [1], industry [2], and popular culture [3]. In particular,
convolutional neural networks (CNNs) have been applied to many
image based machine learning tasks and have yielded strong
results [4]. The availability of hardware/software systems for
efﬁcient training and deployment of large and/or deep CNN
models is critical for the continued success of the ﬁeld [5] [1].
Early systems for NN computation focused on leveraging existing
dense linear algebra techniques and libraries [6] [7]. Current
approaches use low-level machine speciﬁc programming [8]
and/or closed-source, purpose-built vendor libraries [9]. In this
work, we present an open source system that, compared to
existing approaches, achieves competitive computational speed
while achieving signiﬁcantly greater portability. We achieve this
by targeting the vendor-neutral OpenCL platform [10] using a
code-generation approach. We argue that our approach allows
for both: (1) the rapid development of new computational kernels
for existing hardware targets, and (2) the rapid tuning of existing
computational kernels for new hardware targets. Results are pre-
sented for a case study of targeting the Qualcomm Snapdragon
820 mobile computing platform [11] for CNN deployment.

Index Terms—computer vision; code generation; neural net-

works; mobile computing; convolution

I. INTRODUCTION AND MOTIVATION

6
1
0
2

p
e
S
3
1

]

C
D
.
s
c
[

2
v
4
9
0
0
0
.
6
0
6
1
:
v
i
X
r
a

Fig. 1. Boda overview: from NNs to mobile computing platforms.

Convolutional neural network (CNN) based approaches
in a broad variety of computer
have become dominant
vision applications, including object detection [12], video
classiﬁcation [13], and human action recognition [14]. On
a mobile phone or wearable device, energy-efﬁcient computer
vision is necessary to put these and other applications into
production in order to enable novel functionality and achieve
the promises of augmented reality. More critically, the safety
of autonomous vehicles hinges on their ability to understand

1

their surroundings in real-time under tight energy, power, and
price constraints. To support ongoing research, development,
and deployment of systems that include NNs, it is desirable to
nurture a diverse enabling ecosystem of tools and approaches.
In particular, we feel it is desirable to support many hardware
and software platforms to enable new applications across
many areas, including mobile, IoT, transportation, medical,
and others. Imagine that, for a given task, high-performance
vendor libraries exist for at least one platform. Currently, for
CNNs, the vendor is Nvidia, the platform is Maxwell, and
the library is cuDNN [9]. Why not simply use that vendor’s
platform and libraries for the task and be satisﬁed? One issue
is quite simple: in industrial use cases, choice of platform may
be dictated by business concerns. Further, those same business
concerns may preclude dependence on any single vendor. For
example, the ﬂagship Samsung Galaxy S7 mobile phone ships
in two versions: one using a Samsung-proprietary Exynos
8890 System-on-Chip (SoC), the other using the Qualcomm
Snapdragon 820 [11] SoC. Neither of these SoCs contains
Nvidia GPUs or are otherwise capable of running cuDNN.
Further, Nvidia, Qualcomm, and Samsung have engaged in a
long running patent dispute over GPU technologies. Based on
the uncertainties associated with such litigation, SoC and/or
GPU alternatives are subject to constant change. Further, even
once a hardware platform is chosen, business needs may
dictate the speciﬁc software tasks that must be supported.
For example, in an effort to provide differentiation, vendors
have a strong desire to implement novel functionality not
present in existing publicly available libraries. Together, these
uncertainties about both target hardware and particular use-case
create a strong pressure for portability: the ability to quickly
achieve reasonably performance for a variety of tasks across a
variety of platforms.

This work focuses on the challenges associated with
achieving portable, efﬁcient computation of the key primitive
operations needed by convolutional neural networks. We frame
the problem as follows:

Consider a use-case consisting of a speciﬁc combination of:
• a target computational device (i.e. a hardware architecture),

and

• a set of convolutional neural network primitives and an

input size (i.e. a CNN architecture and task).

 
 
 
 
 
 
For such a use-case, achieving good computational efﬁciency
and/or meeting particular performance requirements is, in
general, difﬁcult. Building systems based on existing compu-
tational libraries (e.g. BLAS) generally achieves only limited
efﬁciencies [15]. Improving on such approaches requires tuning
multiple computational kernels for the particular use-case at
hand. This generally requires months of effort by a very
specialized programmer. This programmer must be one that
is both capable of producing high-efﬁciency code for a target
platform as well as being familiar with the details of CNN
computations. Such programmers are not common and thus
their time is a very limited resource.

In this work, we present an open-source vertically-integrated
framework for developing and deploying CNN computations.
The framework combines parts of the functionality of CNN
middleware frameworks such as Caffe [7] or TensorFlow [16]
with the functionality of CNN computation libraries such
as Nvidia’s cuDNN [9] or Nervana System’s NervanaGPU/-
neon [17] [18]. “Out-of-the-box,” our framework does not
attempt to have the breadth or depth of features of a typical
general-use middleware such as Caffe or TensorFlow. Also, it
does not attempt to achieve the same efﬁciency as a highly-
target-speciﬁc computational library such as cuDNN. Instead,
we aim to allow for the rapid development and deployment of
new use-cases/ﬂows of the form described above. In particular,
the framework enables productive, ﬂexible, modular code
generation of the full set of needed CNN operations for any
given use case.

This allows for both:

• the rapid development of new computational kernels for

existing hardware targets, and

• the rapid tuning of existing computational kernels for new

hardware targets.

To provide support for these claims, we present a case study
of using the framework to target the Qualcomm Snapdragon 820
mobile computing platform [11] for DNN/CNN deployment
(see Figure 1). We show that the framework enabled a quick
path to initial functional correctness, and from there helped
navigate a smooth path to reasonably efﬁcient computational
performance.

The rest of the paper is organized as follows. In Section II
we review the semantics of the key computational operations
needed to support CNNs, focusing particularly on convolution.
Then, in Section III we discuss performing CNN calculations
on modern computational hardware, focusing particularly
on the issue of data reuse. We take a historical approach
to this explanation, citing related work along the way. In
Section IV, we consider the general topic of programming
GPUs and performance portability. In Section V we introduce
our framework using the example kernel of single-precision
matrix-matrix multiply (SGEMM). The use of such a relatively
simple, existing computational kernel allows us to:

• illustrate in general what our framework does and how to

apply it,

• to show in particular how we approach a new hardware

2

target, and

• to provide a good baseline for general performance

evaluation.

Then, we present our core contribution of productive, portable,
efﬁcient, code generation for CNN operations (via case study)
in Sections VI and VII. Finally, we conclude in Section VIII.

II. INTRODUCTION TO CNN COMPUTATIONS
Different CNN architectures can contain a wide variety of
computational primitives. For completeness, it is important
that any CNN computation system supports a reasonably
broad set of such primitives. Further, it is desirable that
support for new operations, particularly those that are simple,
can be easily added. The most common operations needed
for deployment (also known as testing or inference, and as
opposed to training) include convolution, pooling and softmax.
However, across many common networks, such as AlexNet [6],
GoogLeNetV1 [19], and the VGG networks [20], convolution
operations dominate the overall computation. Thus, in this
section, we focus on the convolution operation. In particular,
we consider the commonly occurring forms of convolutions
from the above (and similar) CNNs. Given the nature of
this work, we will focus on practical, operational deﬁnitions
of the relevant computations and the data on which those
computations act. Although various types of data are used
in CNNs, the most common are 32-bit (and, increasingly,
16-bit) ﬂoating point numbers. Regardless of exact type or
precision, the numeric values used by CNNs are grouped
into named ND-Arrays (i.e. collections of numbers with N
indexes, sometimes also called N-D Matrices or tensors).
Thus, we deﬁne convolution operationally as the function
out = conv(in, f ilts) where out, in, and f ilts are all N-
D arrays. For simplicity, we omit discussion of the common
practice of special-case handling of biases here. Further, we
will restrict the discussion to convolution over 2D images,
which is the only type used in the above example networks.
Thus, for a single 2D multi-channel image, both in and out
are 3D arrays, with their dimensions being the image height,
the image width, and the number of image channels. For
example, the overall input to a network might be an RGB
(3 channel) image with a size of 205
205 pixels. Thus the
dimensions of the 3D array storing this image (dims(in)) could
be written compactly as Y
3, where the
X
×
Y, X, and C name the dimensions, and the 205, 205, and 3
are the concrete sizes of those dimensions. Often, particularly
for training, it is desirable to process a batch of multiple
images. In this case, an additional B dimension is added to
in and out. For the example of a 32 image batch, dims(in)
3. The results of the
becomes B
×
convolution are fully independent across input/output image
pairs. Thus, for simplicity, we assume a single image for the
remainder of this discussion. The semantics of convolutions
are determined by several architecturally speciﬁed values: the
number of output channels, the kernel size, and the stride.
While in general the kernel size and stride can be different
in each spatial dimension (i.e. X and Y in the 2D case), for

C = 205

C = 32

205

205

205

X

×

×

×

×

×

×

×

×

×

Y

(cid:99)

×

×

−

×

×

×

7
×

7
×

KSZ

KSZ

C = 100

IC = 96

simplicity, here we consider only the case where kernel size
and stride are the same for all dimensions (i.e. are scalars).
The kernel size and number of output channels, combined
with the number of channels in in, determine dims(f ilts).
In our running example, recall that the number of input
channels IC = 3. If the number of output channels OC = 96,
and the kernel size KSZ = 7, then dims(f ilts) will be
3. Finally, the width of
OC
the output will be given by outX = 1 + (inX
KSZ)/stride
(and similarly for the height). Thus, if the stride for this
7)/2 = 100,
example is 2, then we have outX = 1 + (205
−
X
and dims(out) will be Y
96. Note that
100
×
×
×
in is often padded by
KSZ/2
elements (usually zeros)
(cid:98)
in each spatial dimension. When such padding is applied
(assuming stride = 1), out will have the same spatial
dimensions (height and width) as in. We can calculate each
output channel oc using in and the 3D slice of f ilts where
OC = oc. That is, we use each slice f ilts[OC = oc]
3) to compute each slice
(dims KSZ
×
out[OC = oc] (dims Y
100). Then, for each
output point out[OC = oc, Y = oy, X = ox], we extract a
KSZ window of in (sometimes called an input patch
KSZ
or patch) starting at Y = oy
stride, across
∗
all input channels, to yield the slice in[Y = [oy
stride, oy
∗
stride + KSZ)] (dims
stride + KSZ), X = [ox
Y
3). The ﬁnal value of each output point is
the sum of all elements of the element-wise product of the
per-output-channel slice of f ilts and the per-output-x-y-point
slice of in. Typically some activation function, such as ReLU
(max(0, x)) [21], is next applied to each output value, and
this operation is often fused into the convolution operation
itself for efﬁciency. For later reference, note here that each
in slice is reused across all output channels, and each f ilts
slice is reused across all output x-y points. Similarly, note that
if multiple images are processed in a batch, the number of
input-patches/output-x-y-points is multiplied by B.

7
×
×
X = 100

stride, X = ox

stride, ox

IC = 7

C = 7

KSZ

7
×

X

×

×

×

×

×

×

×

∗

∗

∗

∗

III. CNN COMPUTATIONS ON MODERN HARDWARE AND
RELATED WORK

In the prior section, the ﬁnal computation of each (scalar)
output value in a convolution consisted of an element-wise
product of two ND-Arrays (one a slice of the input, one a
slice of the ﬁlters, with exactly equal dimensions), followed
by a sum over all the elements of the product. If we were
to conceptually reshape or view the two 3D slices as 1D
arrays (i.e. vectors, with size equal to the product of the 3
3D-Array dimension sizes), we can see that this operation
is simply a vector dot-product. Thus, the core computational
operation of a large class of NNs (including CNNs) is to
perform many dot-product operations between a large set of
input vectors and model parameter vectors (also known as
ﬁlter weights, ﬁlters, or weights). In NNs with multiple layers,
the outputs of one operation may become inputs to another.
In CNNs, the slices that form the inputs to each dot-product
may spatially overlap and thus share data with each other.
Recall that in our running example, the we have dims(out) of

3

∗

∗

×

×

×

×

X

100

100

C = 100

96, yielding 100

Y
96 = 960000
output points, each requiring one dot-product to compute. If
we consider the set of all input-slice/ﬁlter-slice pairs to these
960000 dot-products, we see that it is formed from the cross
product of 100
100 = 10000 input slices and 96 (per-output
channel) ﬁlter slices. Thus, each dot-product input is reused
across many dot-products: 96 dot-products for each input-slice,
and 10000 dot-products for each ﬁlter-slice.

∗

At least as early as 2004, work using GPUs to accelerate NNs
made the key observation that this data reuse pattern of NNs
makes then well suited for calculation using GPUs [5]. Based
on the amount of calculation hardware available, and the rate at
which it can perform calculations, each computational system
has a peak computational rate. Computations which can achieve
this peak rate on a given system are termed compute limited.
But, on modern computational systems (CPUs and GPUs),
computations such as batches of independent dot-product
operations are instead limited by the time taken to transfer
operands between different storage locations in the system.
This is termed a bandwidth limited computation. Depending
on the ratio of communication resources to compute resources
in a system, it is necessary to reuse data to varying degrees at
different levels of the storage hierarchy of the system to avoid
becoming bandwidth limited [22]. Over time, the relative cost of
communication has increased compared to that of computation,
so systems that have high absolute peak computational rates (for
a given power level) require increasing amounts of data reuse
to achieve those rates [23]. The terms Arithmetic Intensity (or
sometimes more generally Operational Intensity), abbreviated
AI in this work, is used to refer ratios between an amount
of computation and an amount of communication. The most
common units for AI are F LOP S/byte (F/B), and care must
be taken to properly convert any quantities of communication
from elements (e.g. 32-bit or 16-bit ﬂoats) into bytes (e.g. 4
bytes per 32-bit ﬂoat, 2 bytes per 16-bit ﬂoat). For a given ﬂow,
there are two key top-level AI values of interest: the AI required
by the hardware to achieve peak compute rate (the knee AI), and
the best-case AI available in the computation to be performed.
To ﬁnd the top-level knee AI for a given hardware device, we
simply divide the peak compute rate of the device by the peak
off-chip bandwidth of the device. However, note that since most
hardware systems have multiple levels of memory hierarchy
and have many quirks that complicate issues, this top-level
knee AI provides only (perhaps unachievable) upper bounds
on performance with respect to any given computation’s AI.
To calculate the best-case available AI for a given computation,
we take the number of FLOPS to be performed divided by the
(minimal) total number of bytes to transfer.

×

7
×

Returning to our working example, recall that the size of each
3, or 147 elements. If we form a 2D matrix
input-slice is 7
using all 10000 input slices as rows, we will have a 10000
147
matrix inmat. Similarly, we can form 2D matrix using all 96
96 matrix f iltsmat.
ﬁlter slices as columns, yielding a 147
We can now express the entire convolution operation as a single
matrix-matrix multiply operation: outmat = inmat
f iltsmat.
96 = 960000 elements, so
Note that outmat has 10000

×

×

∗

∗

×

×

100

C =
we can reshape it to the desired dims(out) of Y
96. Due to the dot-product-input reuse discussed
100
earlier, such matrix-matrix multiplies are generally amenable
to high-efﬁciency GPU implementations [24]. Quantitatively,
this is seen by the fact that such computations have high AI.
In Figure 2, we show the calculation of the AI for the

X

×

×

matrix-multiplication of our running example.

Early CNN frameworks such as cuda-convnet [6] and
Caffe [7] originally performed CNN convolutions in exactly
this fashion, leveraging Nvidia’s cuBLAS [25] matrix library.
However, there are several limitations of this BLAS-based

approach:

• When inX , inY >> KSZ,

roughly
(KSZ/stride)2 larger than in; thus explicitly creating
the matrix inmat may require signiﬁcant intermediate
memory, and to a lesser extent, non-negligible time.
• It does not allow for data reuse between spatially over-

inmat

is

lapping input slices.

• The underlying matrix-matrix multiply library may not

be well optimized for the problem sizes required.

• It is not possible to fuse an activation function with the

convolution operation.

• It is not possible to use various other optimizations, such

as Winograd convolution [26].
Regardless of the exact reasons,
it became clear to the
community of researchers working on high-performance im-
plementations of CNNs that BLAS-based approaches were
often falling far short of peak compute. Overall community
efforts then turned to purpose-built libraries for convolution,
so it remains an open research question what the limits of
BLAS-based approaches are in various cases.

In particular, Nvidia soon released the cuDNN [9] library,
which exposes an API for directly performing convolutions
using a variety of approaches. Due to the closed-source nature
of the library, and the fact that it has evolved rapidly, it
is difﬁcult to analyze in detail. However, from community
benchmarking it is clear that it achieves much higher efﬁciency
than BLAS-based approaches [15]. Concurrently, a family of
libraries based on an assembly-language-level programming
ﬂow appeared [8] [17] [18], offering similar performance to
cuDNN. Historically, the assembly language level approach has
offered the best performance at any given time, with cuDNN
catching up in its next release. Since the above mentioned
assembly-language kernels are open source, one could speculate
that Nvidia is copying or reimplementing them internally. If
true, it would imply that the entire high-performance CNN
computation ecosystem is anchored by only a few high-
performance programmers at Nervana Systems and Nvidia.
One result of this state of affairs is that there is effectively
only one usable hardware platform for high efﬁciency CNN
computations: Nvidia Maxwell GPUs. For completeness, we
brieﬂy consider the state of the art for performing convolution
on CPUs. Recently, Intel has made signiﬁcant advances in
supporting convolutions, and is competitive with GPUs at
large scale [27]. However, in practice, most use cases are still
single socket, or at most single node, and current CPUs cannot

4

offer competitive performance at this scale, particularly using
consumer (as opposed to server) parts. Also, it is unclear that
CPUs are currently competitive with GPUs on a power or cost
basis for CNN computation. Note that a detailed comparison
of CPUs vs. GPUs is outside the scope of this paper. While in
this work we focus on targeting GPUs, extending our results
to CPUs is a reasonable topic for future work.

IV. PROGRAMMING GPUS AND PERFORMANCE
PORTABILITY
GPUs are generally considered more difﬁcult to program
than CPUs. This is not an issue for an end-user of a library
like cuDNN, since all interactions with the GPU are hidden
behind a C library interface. However, if one wishes to write
such a library, one must face the complexity and issues of
GPU programming and portability. For Nvidia hardware, the
proprietary CUDA language is the ofﬁcially blessed program-
ming language. Alternately, the OpenCL [10] standard is the
only portable option for targeting a variety of GPUs (including
Nvidia GPUs). In terms of low-level features, memory model,
and threading model, CUDA and OpenCL are quite similar.
For example, many GPUs allow explicit loads and stores to
the L2 cache memory as an alternative or adjunct to traditional
caching and prefetching. OpenCL and CUDA both expose this;
OpenCL calls it local memory whereas CUDA calls it shared
memory. For the program itself, both languages are based on
deﬁning a single function (termed a kernel in both languages)
that is run by many threads in parallel. Kernels are compiled to
sequences of machine code and cached in instruction caches on
GPUs similarly to on CPUs. In practice, when running the same
program on different GPUs, OpenCL provides only functional
portability. That is, a given (correct) OpenCL program will
produce consistent results on all supported OpenCL platforms.
However, an OpenCL program that is tuned for high efﬁciency
on one platform (e.g. an Nvidia Maxwell GPU) will not
necessarily deliver high efﬁciency when run on a different
platform (e.g. a Qualcomm Snapdragon SoC’s GPU). Perhaps
due to this issue, no OpenCL library comparable to cuDNN or
NervanaGPU exists. Thus, OpenCL support for CNN operations
on AMD or any other GPU platforms is generally lacking.
Currently, in the caffe framework, support for OpenCL uses
only the BLAS-based approach, and even that support is
marginalized and fragmented across various forks [28] and
pull requests. In this paper, we aim to bridge the performance-
portability gap for CNN operations, and bring such operations
to a more even footing across various hardware platforms when
compared with existing high efﬁciency approaches.

For the two platforms we consider in this work, Figure 3
shows the basic rooﬂine curves formed by considering peak
compute rate and off-chip memory bandwidth. The knee
Arithmetic Intensity (AI) for each platform is marked; note
that although the two platforms have very different absolute
performance, their knee AIs are similar. Peak values are taken
from documentation in the case of the Nvidia hardware, but
(due to lack of documentation) are approximated using a
microbenchmark (clpeak [29]) for the Qualcomm hardware.








o1,1
o2,1
...
o10000,1
outmat : 96

o1,2
o2,2
...
o10000,2
10K



o1,96
. . .
o2,96
. . .
...
. . .
. . .
o10000,96
4B = 3.84M B






=








i1,1
i2,1
...
i10000,1

inmat : 10K

i1,2
i2,2
...
i10000,2
147

∗

∗





i1,147
. . .
i2,147
. . .
...
. . .
. . .
i10000,147
4B = 5.88M B ×











f1,1
f2,1
...
f147,1
f iltsmat : 147

f1,2
f2,2
...
f147,2

96



f1,96
. . .
f2,96
. . .
...
. . .
. . .
f147,96
4B = 0.06M B






∗
AI = 141M F/9.78M B = 14.4F/B

∗

→

F LOP S = 10K

147 = 141GF ; Bytes = 3.84 + 5.88 + 0.06 = 9.78M B

∗

∗
96

∗

∗

Fig. 2. Example calculation of data reuse / Arithmetic Intensity (AI) in Matrix-Matrix Multiplication

with separate per-image calls to im2col() and SGEM M ().
SGEMM is a simple and well studied function across many
hardware architectures. The main required tasks for achieving
an efﬁcient SGEMM implementation for a given hardware
target are as follows:

1) Arranging accesses to storage to make best use of

system’s communication potential.

2) Achieving sufﬁcient data reuse at each level of the
system’s storage hierarchy to avoid being bandwidth
limited.

3) Ensuring computational units are continually active; (2)

is necessary but not sufﬁcient for this.

Fig. 3.
platforms used in this work.

Rooﬂine plot of peak compute rate and off-chip bandwidth for

V. WARMUP: CODE GENERATION FOR SGEMM

Fig. 4. Boda ﬂow: CNN → compute graph → code generation.

At a high level, our framework employs a code generation
ﬂow summarized in Figure 4. To explain the details of how
this ﬂow works in practice, we start with the application of our
approach to a simple example: matrix-matrix multiplication.
As discussed earlier, early approaches to CNN convolutions
employed matrix-matrix multiplication as the key computational
primitive. This allowed the usage of high-quality vendor
provided BLAS libraries (such as cuBLAS from Nvidia) for
the bulk of the computation. In particular, one key BLAS
function, single precision matrix-matrix multiply, or SGEMM,
implements the core computation. Also, following the Caffe
naming convention, an auxiliary function inmat = im2col(in)
provides the conversion from the ND-Array in to the 2D-
matrix-of-input-slices inmat. Note that f iltsmat is typically
simply a reshape of f ilts. As mentioned earlier, the expansive
nature of im2col() can be problematic. In particular, for large
batches of images, the size of inmat (for a single layer) may
exceed GPU memory. To avoid this, batches may be handled

4) Repeating (1), (2), and (3) for all interesting input sizes.
Unfortunately, these goals are often both interrelated and in
conﬂict with each other. For example, it is often the case that
accessing storage contiguously, or in certain patterns, achieves
higher bandwidth than others. Thus, there may be a tradeoff
between achieving good communication bandwidth and reading
the best set of data for reuse at the next level. In general, (1) and
(2) directly balance each other. So, if a factor of N additional
reuse can be achieved at anything less than a factor of N cost in
bandwidth, it is favorable to do so. The primary goal is (3); if
the maximum compute rate can be achieved, other concerns are
secondary. The input sizes for SGEMM are expressed as M , K,
and N , all scalars, where if c = SGEM M (a, b), then c is an
N . The general-case AI
M
K, and b is K
2M N K
calculation for SGEMM is AI =
4(M N +M K+KN ) . Typically,
it is difﬁcult to achieve good efﬁciency across a range of sizes
and hardware targets without some form of metaprogramming.
We deﬁne metaprogramming as the collection of techniques
where, instead of directly writing code, some higher level
facility is used to create the desired ﬁnal code. C Macros,
C++ templates, and ad-hoc code generation are all examples
of metaprogramming, and all offer similar key beneﬁts:

N matrix, a is M

×

×

×

• In general, values that are known ahead of time can be
constant in the ﬁnal code without (potentially repeatedly)
hard-coding them at the source level.

• Loops with static bounds are easier to unroll and/or require

less instructions or registers to implement.

• Offsets, strides, scales, and other values can often can
statically combined and/or used as immediates to reduce
register usage and instruction count.

• Conditionals depending on known-constant values can be

eliminated.

5

110100ArithmeticIntensity[Flops/Byte]10100103104Performance[GFlops/second]Adreno530Compute(256.0GF/s)Titan-XCompute(6700.0GF/s)Adreno530BW(20.0GB/s)Titan-XBW(336.0GB/s)Adreno530kneeAI=12.8Titan-XkneeAI=19.9• Many variants of a single version of an algorithm can be
generated simply by varying parameters at the meta level.
• Repetitive sections of code can be generated, rather than
manually written. This is particularly important when
simpler techniques such as compiler-driven loop unrolling
are insufﬁcient or cumbersome.

The usage of metaprogramming for SGEMM for GPUs
seems to be commonplace; as far the authors are aware,
it is used to varying degrees by all modern, efﬁcient GPU
BLAS libraries including cuBLAS [25], MAGMA [30], and
clBLAS [31].

Our framework uses a combination of string-replacement
templates, speciﬁc support for known-size ND-Array access,
and ad-hoc unrestricted programmatic code generation. This
places it toward the more ﬂexible/extreme end of the space
of metaprogramming techniques. Although metaprogramming
is inherently complex, we nonetheless attempt to achieve a
good balance between ﬂexibility, power, simplicity, and ease
of use. To implement SGEMM in OpenCL for GPU targets,
we employ the following standard techniques:

• Register Tiling [32]
• Explicit Local Memory Blocking
• Inner Loop Unrolling

Listing 1 shows pseudocode for our SGEMM template.

void SGEMM( nda M:K a, nda N:K bt, nda M:N c ) {

Listing 1. SGEMM code template

// dims work Mg:Ng:Mb:Nb:Kb:Mt:Nt // blocking values
local a_lm[%(work_Kb_dim)*%(work_Mb_dim)*%(work_Mt_dim)];
local b_lm[%(work_Kb_dim)*%(work_Nb_dim)*%(work_Nt_dim)];
// per-thread tile of output to compute, stored in registers
float c_t[%(work_Mt_dim)*%(work_Nt_dim)] = {0};
float a_r[%(work_Mt_dim)];
float b_r[%(work_Nt_dim)];
for( int32_t k = 0; k < %(a_K_dim); k += %(work_Kb_dim) ) {

BARRIER_SYNC;
// workgroup-wide load of local memory for this iteration
%(lm_loads);
BARRIER_SYNC;
for( uint32_t subk = 0; subk < %(work_Kb_dim); ++subk ) {

%(loads); // load per-thread slice of a, b into a_r, b_r
%(fmas); // perform fused multiply-adds

}

}

// iterate over rows of the Mt * Nt registers in c_t

for( int32_t m = 0; m < %(work_Mt_dim); ++m ) {

%(transpose_c_t_row); // transpose row of c_t into b_r
%(store_c_t_row); // store transposed row of c_t to c

}

}

For a given input size and hardware platform, the ﬁrst main
task of the code generation is to determine a good blocking
strategy. This can be accomplished with a combination of
heuristic calculations, manual parameter tuning, or automated
tuning. In this work, we use only the ﬁrst two approaches;
use of automated tuning is a good subject for future work.
The general ﬂow from operation (SGEMM or convolution) to
blocking constants is illustrated in Figure 5. The main blocking
parameters we must choose are:

• M t, N t: The number of output elements computed per
thread in the M and N dimensions. Typically in the range

6

Fig. 5. Boda codegen: CNN op → Variant → blocking and GPU code.

[1, 8]. Note that the kernel will require at least M t
registers per thread.

×

N t

• Kb: The inner loop unrolling factor. Typically in the range
[1, 8]. If too small, loop overhead becomes excessive. If
too large, Instruction or Local memory will be insufﬁcient,
as both scale linearly with Kb.

• M b, N b: Determines the total number of output el-
ements computed per workgroup. Valid and/or desir-
able values vary by hardware architecture, but often
M b
threads_per_workgroup should be in the
N b, both Local memory
range [32, 256]. When M b
≈
usage and the Global/Local memory reuse factor scale
linearly as M b (or N b).

N b

≡

×

(cid:100)

. M g

M/M b/M t

N/N b/N t
(cid:101)

• M g, N g: Based on M, N and the prior choices of
M t, N t, M b, N b, we have M g =
and
N g =
N g workgroups will be needed
to compute the ﬁnal result. For many targets, the total
number of workgroups must be above a threshold to
saturate all computational elements in the device. Small
values may be subject to performance limitations if they
are not a multiple of some particular constant.

×

(cid:101)

(cid:100)

After determining the blocking constants, the code generator

must emit code for various blocks:

• %(lm_loads): workgroup-wide cooperative global memory

register

→

local memory loads

→

• %(loads):M t + N t per-thread local memory

loads into a_r[m], b_r[n]

• %(fmas):

N t
c_t[m][n]+ = a_r[m]

M t

×

in-register

multiply-adds:

b_r[n]
• %(transpose_c_t_row), %(store_c_t_row): transpose and

∗

write per-thread outputs into c

The general challenges when emitting these blocks are to
minimize conditionals and choose particular constructs that
can execute efﬁciently. Careful data layout at the global, local,
and register levels may be required, potentially with additional
code to reorganize, shufﬂe, or transpose data at each level.
With a few days of effort, reasonable SGEMM performance
on an Nvidia Titan-X GPU was achieved. See Table I for a
comparison of this template’s performance with that of the
SGEMM from Nvidia’s highly-tuned cuBLAS library. In short,
with moderate effort our framework achieved
80% of the
performance of the vendor library, albeit only for a few simple
cases.

∼

Next, we turn our attention to our main focus: our new
hardware target, the Snapdragon 820 (SD820). The ﬁrst key
thing we learned about this platform is that manual vectorization
of load and stores yields an improvement of 2x or more in

TABLE I
SGEMM OPERATION SPEED AND EFFICIENCY: CUBLAS VS. BODA

Size

Communication/Compute

cuBLAS Performance

Boda Performance

MKN
128
256
384
512
768
1024
1536
2048

Bytes
197KB
786KB
1.77MB
3.15MB
7.08MB
12.6MB
28.3MB
50.3MB

FLOPs
4.19MF
33.6MF
113MF
268MF
906MF
2.15GF
7.25GF
17.2GF

F/B
21.3
42.7
64.0
85.3
128
171
256
341

Runtime
49.3us
49.2us
63.3us
107us
202us
541us
1.39ms
3.56ms

GF/s
85.0GF/s
681GF/s
1.79TF/s
2.51TF/s
4.49TF/s
3.97TF/s
5.22TF/s
4.83TF/s

Runtime
36.7us
54.2us
80.1us
120us
255us
602us
1.63ms
4.08ms

GF/s
114GF/s
619GF/s
1.41TF/s
2.24TF/s
3.55TF/s
3.57TF/s
4.44TF/s
4.21TF/s

Speedup
1.34x
0.91x
0.79x
0.89x
0.79x
0.90x
0.85x
0.87x

TABLE II
SGEMM OPERATION SPEED AND EFFICIENCY: QSML (CPU; NO VENDOR-PROVIDED GPU SUPPORT) VS. BODA (GPU)

Size

Communication/Compute

QSML Performance

Boda Performance

MKN
128
256
384
512
768
1024
1536
2048

Bytes
197KB
786KB
1.77MB
3.15MB
7.08MB
12.6MB
28.3MB
50.3MB

FLOPs
4.19MF
33.6MF
113MF
268MF
906MF
2.15GF
7.25GF
17.2GF

F/B
21.3
42.7
64.0
85.3
128
171
256
341

Runtime
560us
1.7ms
3.8ms
8.0ms
23ms
54ms
175ms
401ms

GF/s
7.5GF/s
20GF/s
30GF/s
34GF/s
39GF/s
40GF/s
41GF/s
42GF/s

Runtime
264us
507us
2.1ms
3.6ms
11ms
27ms
89ms
223ms

GF/s
16GF/s
66GF/s
54GF/s
74GF/s
78GF/s
80GF/s
81GF/s
77GF/s

Speedup
2.1x
3.3x
1.8x
2.2x
1.9x
2.0x
1.9x
1.8x

load/store bandwidth. Further, at least for SGEMM on the
SD820 platform, we ﬁnd that it is not generally proﬁtable to
explicitly move data from global to local memory. Instead,
we apply our usual work-blocking strategy, but simply omit
the loads and stores to local memory, and read/write global
memory directly. Due to the access pattern of the blocking,
the hardware cache appears to provide data reuse similar
to that of using local memory explicitly, and we avoid the
overhead of both the local memory accesses and related
synchronization overheads. However, note that the resulting
bandwidth ampliﬁcation is limited, perhaps due to overall
limited bandwidth from cache/local-memory. Unfortunately,
the SD820 development platform does not provide sufﬁcient
proﬁling information, hardware/ISA documentation, or other
tools (such as a disassembler) to perform a more in depth
performance analysis. As a result, days were required to
improve results through blind experimentation, guesswork, and
tuning. In the end, we adapted our approach for the SD820
using the following techniques:

• Manual vectorization of loads/stores
• Local-memory based output buffers (which seems to a
compiler optimization triggered by high register use)
• Direct global memory access with cache Blocking (i.e.

don’t attempt to use local memory explicitly)

The resulting speed of our SGEMM on the SD820 platform is
2X that of the SGEMM from the vendor provided QSML [33]
library; see Table II. Note, however, that there is currently no
vendor provided GPU-based SGEMM, so this comparison is
against code running on the CPU portion of the SD820 SoC.
Currently, it is not clear if any other OpenCL BLAS libraries
can target the SD820 platform without some signiﬁcant addi-

7

tional efforts. Thus, research of, proﬁling of, and comparison
against other OpenCL-based BLAS libraries is a good topic
for future work.

VI. CODE GENERATION FOR CNN CONVOLUTIONS

As

the

earlier,

discussed

BLAS-based

or
im2col()/SGEM M () approach to performing convolutions
has various limitations. Now, we turn our attention to using
our framework to generate functions that directly perform
convolutions. The ﬁrst variant we will discuss is a simple
fusion of im2col() and SGEM M (). We apply the same
basic techniques as in the SGEM M () discussion above,
but we fold the behavior of im2col() into the %(lm_loads)
code block. Or, in other words, we only implicitly create the
matrix inmat; we just read the correct elements from in as
needed. While this approach is simple, and avoids the memory
overhead of creating inmat explicitly, it makes task (1) much
more difﬁcult. In particular, both the overhead of additional
indexing logic and the resultant poor access patterns reading
global memory can make this variant
than
im2col() + SGEM M (). However, it provides a starting place
for further explorations, and can function as a fallback method
for convolutions not handled by more specialized variants,
especially for cases with large kernel sizes where the overhead
of im2col() is larger. We term this the implicit-SGEMM
or conv variant. The next variant we consider exploits
the common case where the convolution kernel size KSZ
is 1. In this case, various simpliﬁcations are possible, and
it is relatively easy to use a transformation function over
in to ensure a good global memory access pattern. Note
that, for KSZ = 1 convolutions, per-image im2col() is the

less efﬁcient

identity function; thus the k1conv variant is quite similar to
SGEMM. The ﬁnal variant we consider, termed tconv (tiled
convolution), is targeted at the commonly occurring cases of
11], with reasonable widths for
kernel sizes in the range [2,
in (perhaps in the range [KSZ
50]). In this case,
we can perform some additional optimizations:

5, KSZ

∼

∗

∗

• We can fully unroll over the X dimension of the kernel.
This uses signiﬁcant but limited extra local memory and
registers, but allows sharing of in row data in registers
across unrollings of the inner loop.

• We can load entire X/Y tiles of in at the workgroup level.
Even for kernels as small as 2x2, this vastly reduces the
amount of data that must be loaded from global memory
for in. The reduction is a factor of (KSZ/stride)2; this is
naturally the same factor by which im2col() is expansive.

Again, an input transformation must be applied to in to help
simplify indexing logic and improve memory access patterns.
For benchmarking, we consider a range of convolutions
drawn from three common CNNs: AlexNet [6], NiN [34], and
GoogLeNetV1 [19]. For each network, we consider batch sizes
B of 1, 5, and 20. We then gather all the unique convolution
180. While some operations
operations, of which there are
∼
are duplicated within some networks, and the mixing together
of operations from different batch sizes is perhaps not ideal,
this set of operations represents a reasonable set over which
low total runtime (summed over all operations) is desired. That
is, absolute efﬁciency is generally less important than absolute
runtime, and total runtime tends to be dominated by the larger
convolutions. That said, high efﬁciency across a broad range
of problems sizes is still desirable. Each particular network,
batch size, and overall use case requires some particular subset
of convolutions. As with SGEMM, we initially compare our
convolutions against Nvidia’s cuDNN library on an Nvidia
Titan-X. Due to space limitations, we do not present the full
results of that experiment here, but only summarize them. In
brief, as with SGEMM, we achieve reasonable performance,
albeit with a few weeks of effort rather than a few days.
Compared to SGEMM, tuning convolutions took more time
for various reasons. This was the both the author’s ﬁrst
experience at implementing high efﬁciency code on Maxwell
GPUs as well as the author’s ﬁrst attempt at implementing
convolutions; both of these incurred a substantial learning curve
penalty. Beyond that, the design space for convolution seems
to be more complex and varied than that of SGEMM; while
this does offer more potential for optimization, it certainly
also increases development time to the degree one attempts
180
to explore and exploit the space. Note that of the
convolutions, almost all are handled by either the k1conv or
tconv variant. Only a few cases fall though to the conv
variant. Both k1conv and tconv provide a 2x or more
speedup over conv. Now, we turn to our main focus in this
work of targeting the SD820 platform. As with the SGEMM
case, our main task is to manually vectorize loads and stores.
Additionally, as with SGEMM, we avoid the explicit use of
local memory, and instead rely on cache for global memory

∼

bandwidth ampliﬁcation. So far, we have only implemented
two new variants for the SD820 platform: conv_simd and
k1conv_simd, which are manually load/store vectorized
versions of their non-simd counterparts. As with the Nvidia
case, the k1conv_simd variant provides signiﬁcant speedup
over the fallback conv_simd variant. Detailed speed results
are presented for the SD820 platform in the next section.
Given the knowledge gained from implementing SGEMM
on the SD820, it took only a few days to create these new
variants. As with SGEMM, progress was hindered due to
lack of documentation and tools. Such things are undoubtedly
available inside of corporations, and we hope the potential
for higher performance CNN implementations will encourage
vendors to make them available to programmers.

VII. RESULTS

Fig. 6. Speedup of k1conv_simd over conv_simd on SD820 platform.

Here, for the SD820 platform, we show the speed of the
OpenCL code generated by our framework for our benchmark
set of convolutions. In Figure 6, we show the beneﬁt of the
k1conv_simd variant for the convolutions to which it can be
applied: those with size 1 kernels. In Figures 7 and 8, we show
the absolute performance of our framework’s generated code for
each benchmark convolution. As per the graph legend, for each
convolution, we indicate which variant was selected. Inspecting
the results, it can be seen that there are many convolutions with
high arithmetic intensity (AI) that perform worse than those
with lower AI. This is due to the fact that the higher performing
cases are using the higher-efﬁciency k1conv_simd variant.
Based on our experience with the Nvidia platform, we predict
that implementing a tconv_simd variant for the SD820
platform will greatly improve the performance of most of
the cases currently using the fallback conv_simd variant.
Given the lack of a vendor CNN library or other libraries to

8

3.2×1071×1083.2×1081×1093.2×109#-of-FLOPS0.0010.00320.010.032RUNTIME(seconds)10.0GF/s20.0GF/s50.0GF/s100.0GF/sRUNTIME(seconds)vs#-of-FLOPS[log/logscale]conv_simdk1conv_simd(Comp)VIII. CONCLUSIONS
Convolutional Neural Nets are of growing importance in a
broad range of applications, and particularly in computer vision
systems used in self-driving cars, medical imaging, and a variety
of consumer-facing applications such as face identiﬁcation in
social media. As both the research and development commu-
nities continue to grow, interest in productive deployment of
CNNs across many platforms and application domains will
only increase; however, the number of programmers capable of
efﬁciently implementing CNN operations is very limited, and
the keys for efﬁcient implementations are not widely known.
As a result, support for high efﬁciency CNN calculation is
currently limited to only a few hardware platforms. In the Boda
framework described in this paper we have aspired to bridge
the performance-portability gap for the key CNN operations
and to bring such operations to a more even footing across
various hardware platforms when compared with existing high
efﬁciency approaches. We have demonstrated our approach
with a case study of tuning CNN deployment computations for
the Snapdragon 820 mobile computing platform. By offering
competitive performance and superior portability, we feel this
work will positively impact the ability of the research and
development communities to experiment with the deployment
of CNNs across a wide range of platforms for an ever-
broadening range of applications.

Acknowledgments: Research partially funded by DARPA
Award Number HR0011-12-2-0016, the Berkeley Deep Drive
(BDD) Industry Consortium, and ASPIRE industrial sponsors
and afﬁliates Intel, Google, Hewlett-Packard, Huawei, LGE,
Nvidia, Nokia, Oracle, and Samsung.

REFERENCES

[1] J. Schmidhuber, “Deep learning in neural networks: An overview,” Neural

Networks, 2015.

[2] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan-
zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., “Deep
speech 2: End-to-end speech recognition in english and mandarin,”
arXiv:1512.02595, 2015.

[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den
Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot
et al., “Mastering the game of go with deep neural networks and tree
search,” Nature, 2016.

[4] A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn features
off-the-shelf: an astounding baseline for recognition,” in Computer Vision
and Pattern Recognition Workshops, 2014.

[5] K.-S. Oh and K. Jung, “Gpu implementation of neural networks,” Pattern

Recognition, 2004.

[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation

with Deep Convolutional Neural Networks,” in NIPS, 2012.

[7] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” arXiv:1408.5093, 2014.

[8] A. Lavin, “maxDNN: an efﬁcient convolution kernel for deep learning

with maxwell gpus,” arXiv:1501.06633, 2015.

[9] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cuDNN: efﬁcient primitives for deep learning,”
arXiv:1410.0759, 2014.

[10] J. E. Stone, D. Gohara, and G. Shi, “Opencl: A parallel programming
standard for heterogeneous computing systems,” Computing in Science
& Engineering, 2010.

[11] Qualcomm, “Snapdragon 820 processor,” https://www.qualcomm.com/
products/snapdragon/processors/820, 2016, [Online; accessed 4-April-
2016].

Fig. 7. Per-convolution-size speed on SD820 platform.

Fig. 8. Same data as Fig 7, plotted as speed vs. arithmetic intensity.

directly compare against, it is difﬁcult to know how close our
performance is to optimal. From the SD820 rooﬂine curve in
Figure 3, we know there is signiﬁcant headroom over our results
in terms of peak compute performance. While our limited
knowledge of the SD820’s on-chip memory subsystems makes
a determination difﬁcult, in many cases it seems likely we
are limited by cache and/or global memory bandwidth. Thus,
usage of smaller data types for storage (e.g. half-precision
ﬂoats) and/or using hardware support for texture access are
natural candidates to achieve additional improvements.

9

1×1071×1081×1091×1010#-of-FLOPS0.0010.00320.010.0320.10.32RUNTIME(seconds)10.0GF/s20.0GF/s50.0GF/s100.0GF/sRUNTIME(seconds)vs#-of-FLOPS[log/logscale]conv_simdk1conv_simd100200300400500600700800ArithmeticIntensity1234567F/s×1010F/svsArithmeticIntensityconv_simdk1conv_simd106Flops107Flops108Flops109Flops1010Flops[12] R. Girshick, F. Iandola, T. Darrell, and J. Malik, “Deformable part
models are convolutional neural networks,” in Computer Vision and
Pattern Recognition, 2015.

[13] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-
Fei, “Large-scale video classiﬁcation with convolutional neural networks,”
in Computer Vision and Pattern Recognition, 2014.

[14] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks
for human action recognition,” IEEE Trans. on Pattern Analysis and
Machine Intelligence (PAMI), 2013.

[15] S. Chintala,

“convnet-benchmarks,”

https://github.com/soumith/

convnet-benchmarks, 2016, [Online; accessed 4-April-2016].

[16] M. Abadi et al., “TensorFlow: Large-scale machine learning on
heterogeneous systems,” 2015, software available from tensorﬂow.org.
[Online]. Available: http://tensorﬂow.org/

[17] S. Gray and N. Systems, “NervanaTM library for gpus,” https://github.
com/NervanaSystems/nervanagpu, 2016, [Online; accessed 4-April-2016].
[18] N. Systems, “Fast, scalable, easy-to-use python based deep learning
framework by nervanaTM,” https://github.com/NervanaSystems/neon,
2016, [Online; accessed 4-April-2016].

[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
arXiv:1409.4842, 2014.

[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

large-scale image recognition,” arXiv:1409.1556, 2014.

[21] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted

boltzmann machines,” in ICML, 2010.

[22] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: an insightful
visual performance model for multicore architectures,” Communications
of the ACM, 2009.

[23] J. Demmel, “Communication-avoiding algorithms for linear algebra and

beyond.” in IPDPS, 2013.

[24] V. Volkov and J. W. Demmel, “Benchmarking gpus to tune dense linear

algebra,” in Supercomputing, 2008.

[25] NVIDIA, “cublas,” https://developer.nvidia.com/cublas, 2016, [Online;

accessed 27-May-2016].
[26] A. Lavin, “Fast algorithms
arXiv:1509.09308, 2015.

for convolutional neural networks,”

[27] D. Das, S. Avancha, D. Mudigere, K. Vaidynathan, S. Sridharan,
D. Kalamkar, B. Kaul, and P. Dubey, “Distributed deep learning using
synchronous stochastic gradient descent,” arXiv:1602.06709, 2016.
[28] AMD, “Opencl version of caffe developed by amd research lab,” https:
//github.com/amd/OpenCL-caffe, 2016, [Online; accessed 7-April-2016].
[29] K. Bhat, “A tool which proﬁles opencl devices to ﬁnd their peak ca-
pacities,” https://github.com/krrishnarraj/clpeak, 2016, [Online; accessed
30-May-2016].

[30] Y. Li, J. Dongarra, and S. Tomov, “A note on auto-tuning gemm for
gpus,” in International Conference on Computational Science (ICCS),
2009.

[31] AMD et al., “a software library containing blas functions written
in opencl,” https://github.com/clMathLibraries/clBLAS, 2016, [Online;
accessed 31-May-2016].

[32] F. N. Iandola, D. Shefﬁeld, M. J. Anderson, P. M. Phothilimthana, and
K. Keutzer, “Communication-minimizing 2d convolution in gpu registers,”
in International Conference on Image Processing (ICIP), 2013.

[33] Qualcomm, “Snapdragon math libraries,” https://developer.qualcomm.
com/software/snapdragon-math-libraries, 2016, [Online; accessed 29-
Auguest-2016].

[34] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv:1312.4400,

2013.

10

