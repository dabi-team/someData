TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

Tianqi Chen1, Thierry Moreau1, Ziheng Jiang1,2, Lianmin Zheng3, Eddie Yan1

Meghan Cowan1, Haichen Shen1, Leyuan Wang4,2, Yuwei Hu5, Luis Ceze1, Carlos Guestrin1, Arvind Krishnamurthy1
1Paul G. Allen School of Computer Science & Engineering, University of Washington

2 AWS, 3Shanghai Jiao Tong University, 4UC Davis, 5Cornell

8
1
0
2

t
c
O
5

]

G
L
.
s
c
[

3
v
9
9
7
4
0
.
2
0
8
1
:
v
i
X
r
a

Abstract

There is an increasing need to bring machine learn-
ing to a wide diversity of hardware devices. Current
frameworks rely on vendor-speciﬁc operator libraries
and optimize for a narrow range of server-class GPUs.
Deploying workloads to new platforms – such as mo-
bile phones, embedded devices, and accelerators (e.g.,
FPGAs, ASICs) – requires signiﬁcant manual effort.
We propose TVM, a compiler that exposes graph-level
and operator-level optimizations to provide performance
portability to deep learning workloads across diverse
hardware back-ends. TVM solves optimization chal-
lenges speciﬁc to deep learning, such as high-level op-
erator fusion, mapping to arbitrary hardware primitives,
and memory latency hiding. It also automates optimiza-
tion of low-level programs to hardware characteristics by
employing a novel, learning-based cost modeling method
for rapid exploration of code optimizations. Experimen-
tal results show that TVM delivers performance across
hardware back-ends that are competitive with state-of-
the-art, hand-tuned libraries for low-power CPU, mo-
bile GPU, and server-class GPUs. We also demonstrate
TVM’s ability to target new accelerator back-ends, such
as the FPGA-based generic deep learning accelerator.
The system is open sourced and in production use inside
several major companies.

1

Introduction

Deep learning (DL) models can now recognize images,
process natural language, and defeat humans in challeng-
ing strategy games. There is a growing demand to deploy
smart applications to a wide spectrum of devices, rang-
ing from cloud servers to self-driving cars and embed-
ded devices. Mapping DL workloads to these devices is
complicated by the diversity of hardware characteristics,
including embedded CPUs, GPUs, FPGAs, and ASICs
(e.g., the TPU [21]). These hardware targets diverge in

Figure 1: CPU, GPU and TPU-like accelerators re-
quire different on-chip memory architectures and com-
pute primitives. This divergence must be addressed when
generating optimized code.

terms of memory organization, compute functional units,
etc., as shown in Figure 1.

Current DL frameworks, such as TensorFlow, MXNet,
Caffe, and PyTorch, rely on a computational graph in-
termediate representation to implement optimizations,
e.g., auto differentiation and dynamic memory man-
agement [3, 4, 9]. Graph-level optimizations, however,
are often too high-level to handle hardware back-end-
speciﬁc operator-level transformations. Most of these
frameworks focus on a narrow class of server-class
GPU devices and delegate target-speciﬁc optimizations
to highly engineered and vendor-speciﬁc operator li-
braries. These operator-level libraries require signiﬁcant
manual tuning and hence are too specialized and opaque
to be easily ported across hardware devices. Providing
support in various DL frameworks for diverse hardware
back-ends presently requires signiﬁcant engineering ef-
fort. Even for supported back-ends, frameworks must
make the difﬁcult choice between: (1) avoiding graph
optimizations that yield new operators not in the prede-
ﬁned operator library, and (2) using unoptimized imple-
mentations of these new operators.

To enable both graph- and operator-level optimiza-

1

L1DL1IL1DL1IL2L2L3RFRFRFRFL1/TXL1/TXL2SMSMActivationBuﬀerAccum.Register FileWgt. FIFOCPUGPU‘TPU’implicitly managedmixedexplicitly managedMemory Subsystem Architecture···Compute  Primitivescalarvectortensor 
 
 
 
 
 
tions for diverse hardware back-ends, we take a fun-
damentally different, end-to-end approach. We built
TVM, a compiler that takes a high-level speciﬁcation of
a deep learning program from existing frameworks and
generates low-level optimized code for a diverse set of
hardware back-ends. To be attractive to users, TVM
needs to offer performance competitive with the multi-
tude of manually optimized operator libraries across di-
verse hardware back-ends. This goal requires addressing
the key challenges described below.

Leveraging Speciﬁc Hardware Features and Abstrac-
tions. DL accelerators introduce optimized tensor com-
pute primitives [1, 12, 21], while GPUs and CPUs con-
tinuously improve their processing elements. This poses
a signiﬁcant challenge in generating optimized code for
a given operator description. The inputs to hardware in-
structions are multi-dimensional, with ﬁxed or variable
lengths; they dictate different data layouts; and they have
special requirements for memory hierarchy. The system
must effectively exploit these complex primitives to ben-
eﬁt from acceleration. Further, accelerator designs also
commonly favor leaner control [21] and ofﬂoad most
scheduling complexity to the compiler stack. For spe-
cialized accelerators, the system now needs to gener-
ate code that explicitly controls pipeline dependencies to
hide memory access latency – a job that hardware per-
forms for CPUs and GPUs.

Large Search Space for Optimization Another chal-
lenge is producing efﬁcient code without manually tun-
ing operators. The combinatorial choices of memory ac-
cess, threading pattern, and novel hardware primitives
creates a huge conﬁguration space for generated code
(e.g., loop tiles and ordering, caching, unrolling) that
would incur a large search cost if we implement black
box auto-tuning. One could adopt a predeﬁned cost
model to guide the search, but building an accurate cost
model is difﬁcult due to the increasing complexity of
modern hardware. Furthermore, such an approach would
require us to build separate cost models for each hard-
ware type.

TVM addresses these challenges with three key mod-
(1) We introduce a tensor expression language
ules.
to build operators and provide program transformation
primitives that generate different versions of the pro-
gram with various optimizations. This layer extends
Halide [32]’s compute/schedule separation concept by
also separating target hardware intrinsics from transfor-
mation primitives, which enables support for novel ac-
celerators and their corresponding new intrinsics. More-
over, we introduce new transformation primitives to ad-
dress GPU-related challenges and enable deployment to
specialized accelerators. We can then apply different se-
quences of program transformations to form a rich space

of valid programs for a given operator declaration. (2)
We introduce an automated program optimization frame-
work to ﬁnd optimized tensor operators. The optimizer is
guided by an ML-based cost model that adapts and im-
proves as we collect more data from a hardware back-
(3) On top of the automatic code generator, we
end.
introduce a graph rewriter that takes full advantage of
high- and operator-level optimizations.

By combining these three modules, TVM can take
model descriptions from existing deep learning frame-
works, perform joint high- and low-level optimizations,
and generate hardware-speciﬁc optimized code for back-
ends, e.g., CPUs, GPUs, and FPGA-based specialized
accelerators.

This paper makes the following contributions:

• We identify the major optimization challenges in pro-
viding performance portability to deep learning work-
loads across diverse hardware back-ends.

• We introduce novel schedule primitives that take ad-
vantage of cross-thread memory reuse, novel hardware
intrinsics, and latency hiding.

• We propose and implement a machine learning based
optimization system to automatically explore and
search for optimized tensor operators.

• We build an end-to-end compilation and optimiza-
tion stack that allows the deployment of deep learning
workloads speciﬁed in high-level frameworks (includ-
ing TensorFlow, MXNet, PyTorch, Keras, CNTK) to
diverse hardware back-ends (including CPUs, server
GPUs, mobile GPUs, and FPGA-based accelerators).
The open-sourced TVM is in production use inside
several major companies.

We evaluated TVM using real world workloads on a
server-class GPU, an embedded GPU, an embedded
CPU, and a custom generic FPGA-based accelerator.
Experimental results show that TVM offers portable
performance across back-ends and achieves speedups
ranging from 1.2× to 3.8× over existing frameworks
backed by hand-optimized libraries.

2 Overview

This section describes TVM by using an example to walk
through its components. Figure 2 summarizes execu-
tion steps in TVM and their corresponding sections in
the paper. The system ﬁrst takes as input a model from
an existing framework and transforms it into a computa-
tional graph representation. It then performs high-level
dataﬂow rewriting to generate an optimized graph. The
operator-level optimization module must generate efﬁ-
cient code for each fused operator in this graph. Oper-
ators are speciﬁed in a declarative tensor expression lan-

2

Figure 3: Example computational graph of a two-layer
convolutional neural network. Each node in the graph
represents an operation that consumes one or more ten-
sors and produces one or more tensors. Tensor operations
can be parameterized by attributes to conﬁgure their be-
havior (e.g., padding or strides).

an example computational graph representation of a two-
layer convolutional neural network. The main differ-
ence between this high-level representation and a low-
level compiler intermediate representation (IR), such as
LLVM, is that the intermediate data items are large,
multi-dimensional tensors. Computational graphs pro-
vide a global view of operators, but they avoid specifying
how each operator must be implemented. Like LLVM
IRs, a computational graph can be transformed into func-
tionally equivalent graphs to apply optimizations. We
also take advantage of shape speciﬁcity in common DL
workloads to optimize for a ﬁxed set of input shapes.

TVM exploits a computational graph representation to
apply high-level optimizations: a node represents an op-
eration on tensors or program inputs, and edges represent
data dependencies between operations.
It implements
many graph-level optimizations, including: operator fu-
sion, which fuses multiple small operations together;
constant-folding, which pre-computes graph parts that
can be determined statically, saving execution costs; a
static memory planning pass, which pre-allocates mem-
ory to hold each intermediate tensor; and data layout
transformations, which transform internal data layouts
into back-end-friendly forms. We now discuss operator
fusion and the data layout transformation.

Operator Fusion. Operator fusion combines multiple
operators into a single kernel without saving the interme-
diate results in memory. This optimization can greatly
reduce execution time, particularly in GPUs and spe-
cialized accelerators. Speciﬁcally, we recognize four
categories of graph operators: (1) injective (one-to-one
map, e.g., add), (2) reduction (e.g., sum), (3) complex-
out-fusable (can fuse element-wise map to output, e.g.,
conv2d), and (4) opaque (cannot be fused, e.g., sort). We
provide generic rules to fuse these operators, as follows.
Multiple injective operators can be fused into another in-
jective operator. A reduction operator can be fused with
input injective operators (e.g., fuse scale and sum). Op-
erators such as conv2d are complex-out-fusable, and we

Figure 2: System overview of TVM. The current stack
supports descriptions from many deep learning frame-
works and exchange formats, such as CoreML and
ONNX, to target major CPU, GPU and specialized ac-
celerators.

guage; execution details are unspeciﬁed. TVM identiﬁes
a collection of possible code optimizations for a given
hardware target’s operators. Possible optimizations form
a large space, so we use an ML-based cost model to ﬁnd
optimized operators. Finally, the system packs the gen-
erated code into a deployable module.

End-User Example.
In a few lines of code, a user can
take a model from existing deep learning frameworks and
call the TVM API to get a deployable module:

This compiled runtime module contains three compo-
nents: the ﬁnal optimized computational graph (graph),
generated operators
and module parame-
ters (params). These components can then be used to
deploy the model to the target back-end:

(lib),

TVM supports multiple deployment back-ends in lan-
guages such as C++, Java and Python. The rest of this
paper describes TVM’s architecture and how a system
programmer can extend it to support new back-ends.

3 Optimizing Computational Graphs

Computational graphs are a common way to represent
programs in DL frameworks [3, 4, 7, 9]. Figure 3 shows

3

FrameworksHigh Level Graph RewritingMachine Learning Based Automated OptimizerOptimized Computational GraphComputational GraphHardware-Aware Optimization PrimitivesDeclarativeTensor ExpressionsOptimized Low Level Loop ProgramLLVM IRCUDA/Metal/OpenCLAccelerator BackendDeployable ModuleOperator-level Optimization and Code GenerationSection 3Section 4Section 5import tvm as t# Use keras framework as example, import modelgraph, params = t.frontend.from_keras(keras_model)target = t.target.cuda()graph, lib, params = t.compiler.build(graph, target, params)import tvm.runtime as tmodule = runtime.create(graph, lib, t.cuda(0))module.set_input(**params)module.run(data=data_array)output = tvm.nd.empty(out_shape, ctx=t.cuda(0))module.get_output(0, output)conv2dreluconv2dreluﬂattendensesoftmaxoperationinputsdataﬂowdependencyw1w2w3datachannels=32,kernel_size=(3,3), padding=(1,1),use_bias=0example attributesshape=(1,10)Figure 4: Performance comparison between fused and
non-fused operations. TVM generates both operations.
Tested on NVIDIA Titan X.

can fuse element-wise operators to its output. We can
apply these rules to transform the computational graph
into a fused version. Figure 4 demonstrates the impact
of this optimization on different workloads. We ﬁnd that
fused operators generate up to a 1.2× to 2× speedup by
reducing memory accesses.

Data Layout Transformation. There are multiple
ways to store a given tensor in the computational graph.
The most common data layout choices are column major
and row major. In practice, we may prefer to use even
more complicated data layouts. For instance, a DL ac-
celerator might exploit 4 × 4 matrix operations, requiring
data to be tiled into 4 × 4 chunks to optimize for access
locality.

Data layout optimization converts a computational
graph into one that can use better internal data layouts
for execution on the target hardware. It starts by spec-
ifying the preferred data layout for each operator given
the constraints dictated by memory hierarchies. We then
perform the proper layout transformation between a pro-
ducer and a consumer if their preferred data layouts do
not match.

While high-level graph optimizations can greatly im-
prove the efﬁciency of DL workloads, they are only as
effective as what the operator library provides. Cur-
rently, the few DL frameworks that support operator fu-
sion require the operator library to provide an implemen-
tation of the fused patterns. With more network opera-
tors introduced on a regular basis, the number of possible
fused kernels can grow dramatically. This approach is
no longer sustainable when targeting an increasing num-
ber of hardware back-ends since the required number
of fused pattern implementations grows combinatorially
with the number of data layouts, data types, and accel-
erator intrinsics that must be supported. It is not feasi-
ble to handcraft operator kernels for the various opera-
tions desired by a program and for each back-end. To

Figure 5: Example schedule transformations that opti-
mize a matrix multiplication on a specialized accelerator.

this end, we next propose a code generation approach
that can generate various possible implementations for a
given model’s operators.

4 Generating Tensor Operations

TVM produces efﬁcient code for each operator by gen-
erating many valid implementations on each hardware
back-end and choosing an optimized implementation.
This process builds on Halide’s idea of decoupling de-
scriptions from computation rules (or schedule optimiza-
tions) [32] and extends it to support new optimizations
(nested parallelism, tensorization, and latency hiding)
and a wide array of hardware back-ends. We now high-
light TVM-speciﬁc features.

4.1 Tensor Expression and Schedule Space

We introduce a tensor expression language to support au-
tomatic code generation. Unlike high-level computation
graph representations, where the implementation of ten-
sor operations is opaque, each operation is described in

4

conv+bn+relu128x28x281x1x128x256depthwise-conv+bn+relu512x14x143x3x512rnncellhidden:128lstmcellhidden:1280.000.501.001.502.00RelativeSpeedupw/ofusionw/fusionfor y in range(1024):  for x in range(1024):    C[y][x] = 0    for k in range(1024):      C[y][x] += A[k][y] * B[k][x]for yo in range(128):  for xo in range(128):    C[yo*8:yo*8+8][xo*8:xo*8+8] = 0    for ko in range(128):      for yi in range(8):        for xi in range(8):          for ki in range(8):            C[yo*8+yi][xo*8+xi] +=                A[ko*8+ki][yo*8+yi] * B[ko*8+ki][xo*8+xi]inp_buffer AL[8][8], BL[8][8]acc_buffer CL[8][8]for yo in range(128):  for xo in range(128):    vdla.fill_zero(CL)    for ko in range(128):      vdla.dma_copy2d(AL, A[ko*8:ko*8+8][yo*8:yo*8+8])      vdla.dma_copy2d(BL, B[ko*8:ko*8+8][xo*8:xo*8+8])              vdla.fused_gemm8x8_add(CL, AL, BL)    vdla.dma_copy2d(C[yo*8:yo*8+8,xo*8:xo*8+8], CL)+ Cache Data on Accelerator Special Buﬀer A = t.placeholder((1024, 1024))B = t.placeholder((1024, 1024))k = t.reduce_axis((0, 1024))C = t.compute((1024, 1024), lambda y, x:                 t.sum(A[k, y] * B[k, x], axis=k))s = t.create_schedule(C.op)scheduleschedule transformationcorresponding low-level code+ Map to Accelerator Tensor InstructionsCL = s.cache_write(C, vdla.acc_buffer)AL = s.cache_read(A, vdla.inp_buffer)# additional schedule steps omitted …s[CL].tensorize(yi, vdla.gemm8x8)+ Loop Tilingyo, xo, ko, yi, xi, ki = s[C].tile(y, x, k, 8, 8, 8)Figure 6: TVM schedule lowering and code generation
process. The table lists existing Halide and novel TVM
scheduling primitives being used to optimize schedules
for CPUs, GPUs and accelerator back-ends. Tensoriza-
tion is essential for accelerators, but it can also be used
for CPUs and GPUs. Special memory-scope enables
memory reuse in GPUs and explicit management of on-
chip memory in accelerators. Latency hiding is speciﬁc
to TPU-like accelerators.

an index formula expression language. The following
code shows an example tensor expression to compute
transposed matrix multiplication:

Each compute operation speciﬁes both the shape of
the output tensor and an expression describing how to
compute each element of it. Our tensor expression
language supports common arithmetic and math oper-
ations and covers common DL operator patterns. The
language does not specify the loop structure and many
other execution details, and it provides ﬂexibility for
adding hardware-aware optimizations for various back-
ends. Adopting the decoupled compute/schedule princi-
ple from Halide [32], we use a schedule to denote a spe-
ciﬁc mapping from a tensor expression to low-level code.
Many possible schedules can perform this function.

We build a schedule by incrementally applying basic
transformations (schedule primitives) that preserve the
program’s logical equivalence. Figure 5 shows an ex-
ample of scheduling matrix multiplication on a special-
ized accelerator. Internally, TVM uses a data structure
to keep track of the loop structure and other information
as we apply schedule transformations. This information
can then help generate low-level code for a given ﬁnal
schedule.

Our tensor expression takes cues from Halide [32],
Darkroom [17], and TACO [23]. Its primary enhance-
ments include support for the new schedule optimiza-
tions discussed below. To achieve high performance
on many back-ends, we must support enough schedule
primitives to cover a diverse set of optimizations on dif-
ferent hardware back-ends. Figure 6 summarizes the
operation code generation process and schedule primi-

5

Figure 7: Performance comparison between TVM with
and without cooperative shared memory fetching on ma-
trix multiplication workloads. Tested on an NVIDIA Ti-
tan X.

tives that TVM supports. We reuse helpful primitives
and the low-level loop program AST from Halide, and
we introduce new primitives to optimize GPU and ac-
celerator performance. The new primitives are neces-
sary to achieve optimal GPU performance and essen-
tial for accelerators. CPU, GPU, TPU-like accelerators
are three important types of hardware for deep learning.
This section describes new optimization primitives for
CPUs, GPUs and TPU-like accelerators, while section 5
explains how to automatically derive efﬁcient schedules.

4.2 Nested Parallelism with Cooperation

Parallelism is key to improving the efﬁciency of
compute-intensive kernels in DL workloads. Modern
GPUs offer massive parallelism, requiring us to bake par-
allel patterns into schedule transformations. Most exist-
ing solutions adopt a model called nested parallelism, a
form of fork–join. This model requires a parallel sched-
ule primitive to parallelize a data parallel task; each task
can be further recursively subdivided into subtasks to ex-
ploit the target architecture’s multi-level thread hierarchy
(e.g., thread groups in GPU). We call this model shared-
nothing nested parallelism because one working thread
cannot look at the data of its sibling within the same par-
allel computation stage.

An alternative to the shared-nothing approach is to
fetch data cooperatively. Speciﬁcally, groups of threads
can cooperatively fetch the data they all need and place
it into a shared memory space. 1 This optimization can
take advantage of the GPU memory hierarchy and en-

1 Halide recently added shared memory support but without general

memory scope for accelerators.

Schedule primitives  used in various hardware backendsCPU ScheduleGPU Schedule eeAccel. Schedule ule[Halide] Loop Transformations✔✔✔[Halide] Thread Binding✔✔✔[Halide] Compute Locality✔✔✔[TVM] Special Memory Scope✔✔[TVM] Tensorization✔✔✔[TVM] Latency Hiding✔Tensor ExpressionCode LoweringSelect Schedule PrimitivesFinal ScheduleLow level codem, n, h = t.var('m'), t.var('n'), t.var('h')A = t.placeholder((m, h), name='A')B = t.placeholder((n, h), name='B')k = t.reduce_axis((0, h), name='k')C = t.compute((m, n), lambda y, x:                    t.sum(A[k, y] * B[k, x], axis=k))result shapecomputing rule10242048MatrixSize012345678Time(ms)cuBLASTVMw/ocoop.TVMable data reuse across threads through shared memory
regions. TVM supports this well-known GPU optimiza-
tion using a schedule primitive to achieve optimal per-
formance. The following GPU code example optimizes
matrix multiplication.

Figure 7 demonstrates the impact of this optimiza-
tion. We introduce the concept of memory scopes to the
schedule space so that a compute stage (AS and BS in the
code) can be marked as shared. Without explicit memory
scopes, automatic scope inference will mark compute
stages as thread-local. The shared task must compute
the dependencies of all working threads in the group.
Additionally, memory synchronization barriers must be
properly inserted to guarantee that shared loaded data is
visible to consumers. Finally, in addition to being use-
ful to GPUs, memory scopes let us tag special memory
buffers and create special lowering rules when targeting
specialized DL accelerators.

4.3 Tensorization

DL workloads have high arithmetic intensity, which
can typically be decomposed into tensor operators like
matrix-matrix multiplication or 1D convolution. These
natural decompositions have led to the recent trend of
adding tensor compute primitives
[1, 12, 21]. These
new primitives create both opportunities and challenges
for schedule-based compilation; while using them can
improve performance, the compilation framework must
seamlessly integrate them. We dub this tensorization: it
is analogous to vectorization for SIMD architectures but
has signiﬁcant differences. Instruction inputs are multi-
dimensional, with ﬁxed or variable lengths, and each has
different data layouts. More importantly, we cannot sup-
port a ﬁxed set of primitives since new accelerators are
emerging with their own variations of tensor instructions.
We therefore need an extensible solution.

We make tensorization extensible by separating the
target hardware intrinsic from the schedule with a mech-
anism for tensor-intrinsic declaration. We use the same
tensor expression language to declare both the behavior
of each new hardware intrinsic and the lowering rule as-
sociated with it. The following code shows how to de-
clare an 8 × 8 tensor hardware intrinsic.

6

Additionally, we introduce a tensorize schedule primi-
tive to replace a unit of computation with the correspond-
ing intrinsics. The compiler matches the computation
pattern with a hardware declaration and lowers it to the
corresponding hardware intrinsic.

Tensorization decouples the schedule from speciﬁc
hardware primitives, making it easy to extend TVM
to support new hardware architectures. The generated
code of tensorized schedules aligns with practices in
high-performance computing: break complex operations
into a sequence of micro-kernel calls. We can also use
the tensorize primitive to take advantage of handcrafted
micro-kernels, which can be beneﬁcial in some plat-
forms. For example, we implement ultra low precision
operators for mobile CPUs that operate on data types
that are one- or two-bits wide by leveraging a bit-serial
matrix vector multiplication micro-kernel. This micro-
kernel accumulates results into progressively larger data
types to minimize the memory footprint. Presenting the
micro-kernel as a tensor intrinsic to TVM yields up to a
1.5× speedup over the non-tensorized version.

4.4 Explicit Memory Latency Hiding

Latency hiding refers to the process of overlapping mem-
ory operations with computation to maximize utilization
of memory and compute resources. It requires different
strategies depending on the target hardware back-end.
On CPUs, memory latency hiding is achieved implic-
itly with simultaneous multithreading [14] or hardware
prefetching [10, 20]. GPUs rely on rapid context switch-
ing of many warps of threads [44]. In contrast, special-
ized DL accelerators such as the TPU [21] usually favor
leaner control with a decoupled access-execute (DAE)
architecture [35] and ofﬂoad the problem of ﬁne-grained
synchronization to software.

Figure 9 shows a DAE hardware pipeline that reduces
runtime latency. Compared to a monolithic hardware de-
sign, the pipeline can hide most memory access over-
heads and almost fully utilize compute resources. To
achieve higher utilization, the instruction stream must be
augmented with ﬁne-grained synchronization operations.
Without them, dependencies cannot be enforced, leading
to erroneous execution. Consequently, DAE hardware
pipelines require ﬁne-grained dependence enqueuing/d-
equeuing operations between the pipeline stages to guar-

Barrier inserted automaticallyby compilerAll threads cooperativelyload AS and BS in diﬀerentparallel patternsfor thread_group (by, bx) in cross(64, 64):  for thread_item (ty, tx) in cross(2, 2):    local CL[8][8] = 0    shared AS[2][8], BS[2][8]     for k in range(1024):      for i in range(4):        AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx]      for each i in 0..4:        BS[ty][i*4+tx] = B[k][bx*64+ty*8+i*4+tx]      memory_barrier_among_threads()      for yi in range(8):        for xi in range(8):          CL[yi][xi] += AS[yi] * BS[xi]      for yi in range(8):        for xi in range(8):          C[yo*8+yi][xo*8+xi] = CL[yi][xi]w, x = t.placeholder((8, 8)), t.placeholder((8, 8))k = t.reduce_axis((0, 8))y = t.compute((8, 8), lambda i, j:                t.sum(w[i, k] * x[j, k], axis=k))def gemm_intrin_lower(inputs, outputs):   ww_ptr = inputs[0].access_ptr(“r")   xx_ptr = inputs[1].access_ptr("r")   zz_ptr = outputs[0].access_ptr("w")   compute = t.hardware_intrin("gemm8x8", ww_ptr, xx_ptr, zz_ptr)   reset = t.hardware_intrin("fill_zero", zz_ptr)   update = t.hardware_intrin("fuse_gemm8x8_add", ww_ptr, xx_ptr, zz_ptr)   return compute, reset, updategemm8x8 = t.decl_tensor_intrin(y.op, gemm_intrin_lower)declare behaviorlowering rule to generatehardware intrinsics to carry out the computationFigure 8: TVM virtual thread lowering transforms a virtual thread-parallel program to a single instruction stream; the
stream contains explicit low-level synchronizations that the hardware can interpret to recover the pipeline parallelism
required to hide memory access latency.

Figure 9: Decoupled Access-Execute in hardware hides
most memory access latency by allowing memory and
computation to overlap. Execution correctness is en-
forced by low-level synchronization in the form of de-
pendence token enqueueing/dequeuing actions, which
the compiler stack must insert in the instruction stream.

Figure 10: Rooﬂine [47] of an FPGA-based DL ac-
celerator running ResNet inference. With latency hid-
ing enabled by TVM, performance of the benchmarks is
brought closer to the rooﬂine, demonstrating higher com-
pute and memory bandwidth efﬁciency.

available pipeline parallelism dictated by the low-level
synchronizations in the instruction stream.

antee correct execution, as shown in Figure 9’s instruc-
tion stream.

Programming DAE accelerators that require explicit
low-level synchronization is difﬁcult. To reduce the
programming burden, we introduce a virtual threading
scheduling primitive that lets programmers specify a
high-level data parallel program as they would a hard-
ware back-end with support for multithreading. TVM
then automatically lowers the program to a single in-
struction stream with low-level explicit synchronization,
as shown in Figure 8. The algorithm starts with a high-
level multi-threaded program schedule and then inserts
the necessary low-level synchronization operations to
guarantee correct execution within each thread. Next,
it interleaves operations of all virtual threads into a sin-
gle instruction stream. Finally, the hardware recovers the

Hardware Evaluation of Latency Hiding. We now
demonstrate the effectiveness of latency hiding on a cus-
tom FPGA-based accelerator design, which we describe
in depth in subsection 6.4. We ran each layer of ResNet
on the accelerator and used TVM to generate two sched-
ules: one with latency hiding, and one without. The
schedule with latency hiding parallelized the program
with virtuals threads to expose pipeline parallelism and
therefore hide memory access latency. Results are shown
in Figure 10 as a rooﬂine diagram [47]; rooﬂine perfor-
mance diagrams provide insight into how well a given
system uses computation and memory resources for dif-
ferent benchmarks. Overall, latency hiding improved
performance on all ResNet layers. Peak compute utiliza-
tion increased from 70% with no latency hiding to 88%
with latency hiding.

7

ldldexexldld…exexvthread 0vthread 1barrierldex…ldldexldex…ldldexbarriervthread 0vthread 1ldexldex……ldldldexldexInput: High-level Threaded ProgramFinal Single Instruction StreamInject Synchronization Instructionsfor vthread tx in range(2):  acc_buffer CL[8]  inp_buffer AL[8]  for k in range(128):    ld.dma_copy2d(AL, AL[k][tx*8:tx*8+8])    ex.accumulate(AL, CL)  acc_buffer CL[2][8]  inp_buffer AL[2][8]  ex.push_dep_to(ld)  ex.push_dep_to(ld)  for k in range(128):    ld.pop_dep_from(ex)    ld.dma_copy2d(AL[0], AL[k][0:8])    ld.push_dep_to(ex)    ld.pop_dep_from(ex)    ld.dma_copy2d(AL[1], AL[k][8:16])    ld.push_dep_to(ex)    ex.pop_dep_from(ld)    ex.accumulate(AL[0], CL[0])    ex.push_dep_to(ld)    ex.pop_dep_from(ld)    ex.accumulate(AL[1], CL[1])    ex.push_dep_to(ld)  ld.pop_dep_from(ex)  ld.pop_dep_from(ex)for vthread tx in range(2):  acc_buffer CL[8]  inp_buffer AL[8]  ex.push_dep_to(ld)  for k in range(128):    ld.pop_dep_from(ex)    ld.dma_copy2d(AL, AL[k][tx*8:tx*8+8])    ld.push_dep_to(ex)    ex.pop_dep_from(ld)    ex.accumulate(AL, CL)    ex.push_dep_to(ld)  ld.pop_dep_from(ex)read after write  (RAW) dependenceread after write  (RAW) dependencepush RAW dependencepush WAR dependencepop RAW dependencepop WAR dependenceld0ex 0ld1ex 1ld2ld3ex 2ex 3ld 0ex 0ld 1ex 1ld 2ex 2ld3ex 3execution savingsDecoupled Access-Execute PipelinetMonolithic Pipelinewrite after read  (WAR) dependenceread after write  (RAW) dependenceld.perform_action(ld0)ld.push_dep_to(ex)ld.perform_action(ld1)ld.push_dep_to(ex)ex.pop_dep_from(ld)ex.perform_action(ex0)ex.push_dep_to(ld)ex.pop_dep_from(ld)ex.perform_action(ex1)ex.push_dep_to(ld)ld.pop_dep_from(ex)ld.perform_action(ld2)...ld.perform_action(ld0)ex.perform_action(ex0)ld.perform_action(ld1)ex.perform_action(ex1)...Instruction Streamcompute boundmemory bound5 Automating Optimization

Given the rich set of schedule primitives, our remaining
problem is to ﬁnd optimal operator implementations for
each layer of a DL model. Here, TVM creates a special-
ized operator for the speciﬁc input shape and layout as-
sociated with each layer. Such specialization offers sig-
niﬁcant performance beneﬁts (in contrast to handcrafted
code that would target a smaller diversity of shapes and
layouts), but it also raises automation challenges. The
system needs to choose the schedule optimizations –
such as modifying the loop order or optimizing for the
memory hierarchy – as well as schedule-speciﬁc param-
eters, such as the tiling size and the loop unrolling factor.
Such combinatorial choices create a large search space of
operator implementations for each hardware back-end.
To address this challenge, we built an automated sched-
ule optimizer with two main components: a schedule ex-
plorer that proposes promising new conﬁgurations, and
a machine learning cost model that predicts the perfor-
mance of a given conﬁguration. This section describes
these components and TVM’s automated optimization
ﬂow (Figure 11).

5.1 Schedule Space Speciﬁcation

We built a schedule template speciﬁcation API to let a
developer declare knobs in the schedule space. The tem-
plate speciﬁcation allows incorporation of a developer’s
domain-speciﬁc knowledge, as necessary, when specify-
ing possible schedules. We also created a generic mas-
ter template for each hardware back-end that automati-
cally extracts possible knobs based on the computation
description expressed using the tensor expression lan-
guage. At a high level, we would like to consider as many
conﬁgurations as possible and let the optimizer manage
the selection burden. Consequently, the optimizer must
search over billions of possible conﬁgurations for the real
world DL workloads used in our experiments.

5.2 ML-Based Cost Model

One way to ﬁnd the best schedule from a large conﬁgu-
ration space is through blackbox optimization, i.e., auto-
tuning. This method is used to tune high performance
computing libraries [15, 46]. However, auto-tuning re-
quires many experiments to identify a good conﬁgura-
tion.

An alternate approach is to build a predeﬁned cost
model to guide the search for a particular hardware back-
end instead of running all possibilities and measuring
their performance.
Ideally, a perfect cost model con-
siders all factors affecting performance: memory access
patterns, data reuse, pipeline dependencies, and thread-

Figure 11: Overview of automated optimization frame-
work. A schedule explorer examines the schedule space
using an ML-based cost model and chooses experiments
to run on a distributed device cluster via RPC. To im-
prove its predictive power, the ML model is updated pe-
riodically using collected data recorded in a database.

Method Category

Blackbox auto-tuning
Predeﬁned cost model
ML based cost model

Data
Cost

high
none
low

Model
Bias

none
high
low

Need
Hardware
Info

no
yes
no

Learn
from
His-
tory
no
no
yes

Table 1: Comparison of automation methods. Model bias
refers to inaccuracy due to modeling.

ing patterns, among others. This approach, unfortu-
nately, is burdensome due to the increasing complexity
of modern hardware. Furthermore, every new hardware
target requires a new (predeﬁned) cost model.

We instead take a statistical approach to solve the cost
modeling problem. In this approach, a schedule explorer
proposes conﬁgurations that may improve an operator’s
performance. For each schedule conﬁguration, we use
an ML model that takes the lowered loop program as in-
put and predicts its running time on a given hardware
back-end. The model, trained using runtime measure-
ment data collected during exploration, does not require
the user to input detailed hardware information. We up-
date the model periodically as we explore more conﬁg-
urations during optimization, which improves accuracy
for other related workloads, as well. In this way, the qual-
ity of the ML model improves with more experimental
trials. Table 1 summarizes the key differences between
automation methods. ML-based cost models strike a bal-
ance between auto-tuning and predeﬁned cost modeling
and can beneﬁt from the historical performance data of
related workloads.

Machine Learning Model Design Choices. We must
consider two key factors when choosing which ML
model the schedule explorer will use: quality and speed.
The schedule explorer queries the cost model frequently,
which incurs overheads due to model prediction time
and model reﬁtting time. To be useful, these overheads
must be smaller than the time it takes to measure per-

8

Raspberry PiTrackerMali GPUNvidia GPUTensorOp SpeciﬁcationSchedule Space TemplateDatabaseDevice ClusterSchedule ExplorerML Cost Modellogquerytraining dataFPGA Boardrpcget_perf…updateclude the memory access count and reuse ratio of each
memory buffer at each loop level, as well as a one-hot
encoding of loop annotations such as “vectorize”, “un-
roll”, and “parallel.” We also evaluate a neural network
model that uses TreeRNN [38] to summarize the loop
program’s AST without feature engineering. Figure 13
summarizes the workﬂow of the cost models. We found
that tree boosting and TreeRNN have similar predictive
quality. However, the former performs prediction twice
as fast and costs much less time to train. As a result, we
chose gradient tree boosting as the default cost model in
our experiments. Nevertheless, we believe that both ap-
proaches are valuable and expect more future research on
this problem.

On average, the tree boosting model does prediction
in 0.67 ms, thousands of times faster than running a real
measurement. Figure 12 compares an ML-based opti-
mizer to blackbox auto-tuning methods; the former ﬁnds
better conﬁgurations much faster than the latter.

5.3 Schedule Exploration

Once we choose a cost model, we can use it to select
promising conﬁgurations on which to iteratively run real
measurements. In each iteration, the explorer uses the
ML model’s predictions to select a batch of candidates
on which to run the measurements. The collected data is
then used as training data to update the model. If no ini-
tial training data exists, the explorer picks random candi-
dates to measure.

The simplest exploration algorithm enumerates and
runs every conﬁguration through the cost model, se-
lecting the top-k predicted performers. However, this
strategy becomes intractable with large search spaces.
Instead, we run a parallel simulated annealing algo-
rithm [22]. The explorer starts with random conﬁgura-
tions, and, at each step, randomly walks to a nearby con-
ﬁguration. This transition is successful if cost decreases
as predicted by the cost model. It is likely to fail (reject)
if the target conﬁguration has a higher cost. The random
walk tends to converge on conﬁgurations that have lower
costs as predicted by the cost model. Exploration states
persist across cost model updates; we continue from the
last conﬁguration after these updates.

5.4 Distributed Device Pool and RPC

A distributed device pool scales up the running of on-
hardware trials and enables ﬁne-grained resource sharing
among multiple optimization jobs. TVM implements a
customized, RPC-based distributed device pool that en-
ables clients to run programs on a speciﬁc type of de-
vice. We can use this interface to compile a program
on the host compiler, request a remote device, run the

Figure 12: Comparison of different automation methods
for a conv2d operator in ResNet-18 on TITAN X. The
ML-based model starts with no training data and uses
the collected data to improve itself. The Y-axis is the
speedup relative to cuDNN. We observe a similar trend
for other workloads.

Figure 13: Example workﬂow of ML cost models. XG-
Boost predicts costs based on loop program features.
TreeRNN directly summarizes the AST.

formance on real hardware, which can be on the order
of seconds depending on the speciﬁc workload/hardware
target. This speed requirement differentiates our problem
from traditional hyperparameter tuning problems, where
the cost of performing measurements is very high rela-
tive to model overheads, and more expensive models can
In addition to the choice of model, we need
be used.
to choose an objective function to train the model, such
as the error in a conﬁguration’s predicted running time.
However, since the explorer selects the top candidates
based only on the relative order of the prediction (A runs
faster than B), we need not predict the absolute execution
times directly. Instead, we use a rank objective to predict
the relative order of runtime costs.

We implement several types of models in our ML opti-
mizer. We employ a gradient tree boosting model (based
on XGBoost [8]), which makes predictions based on fea-
tures extracted from the loop program; these features in-

9

0100200300400500600700800NumberofTrials0.000.250.500.751.001.251.50RelativeSpeedupTVM:ML-basedModelTVM:BlackboxGeneticAlgorithmTVM:RandomSearchBaseline:cuDNNfor yo in range(4):  for xo in range(4):    C[yo*2:yo*2+2][xo*2:xo*2+2] = 0    for ko in range(8):     for yi in range(2):        for xi in range(2):          C[yo*2+yi][xo*2+xi] +=              A[k][yo*2+yi] * B[k][xo*2+xi]xiyikxoyoC2441664A12161664B22166464Feature ExtractionQuery: Loop ASTcost predictione.g. touched memory sizeSchedule ExplorerXGBoostalternatively, we can feed AST to TreeRNNH,W
224, 224
56, 56
56, 56
56, 56
56, 56
28, 28
28, 28
28, 28
14, 14
14, 14
14, 14
7, 7

Name
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12

Name
D1
D2
D3
D4
D5
D6
D7
D8
D9

Operator
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
Operator
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d
depthwise conv2d

IC, OC
3,64
64,64
64,64
64,128
64,128
128,128
128,256
128,256
256,256
256,512
256,512
512,512

K, S
7, 2
3, 1
1, 1
3, 2
1, 2
3, 1
3, 2
1, 2
3, 1
3, 2
1, 2
3, 1

H,W
112, 112
112, 112
56, 56
56, 56
28, 28
28, 28
14, 14
14, 14
7, 7

IC
32
64
128
128
256
256
512
512
1024

K, S
3, 1
3, 2
3, 1
3, 2
3, 1
3, 2
3, 1
3, 2
3, 1

Table 2: Conﬁgurations of all conv2d operators in
ResNet-18 and all depthwise conv2d operators in Mo-
bileNet used in the single kernel experiments. H/W
denotes height and width, IC input channels, OC out-
put channels, K kernel size, and S stride size. All ops
use “SAME” padding. All depthwise conv2d operations
have channel multipliers of 1.

function remotely, and access results in the same script
on the host. TVM’s RPC supports dynamic upload and
runs cross-compiled modules and functions that use its
runtime convention. As a result, the same infrastruc-
ture can perform a single workload optimization and
end-to-end graph inference. Our approach automates the
compile, run, and proﬁle steps across multiple devices.
This infrastructure is especially critical for embedded de-
vices, which traditionally require tedious manual effort
for cross-compilation, code deployment, and measure-
ment.

6 Evaluation

TVM’s core is implemented in C++ (∼50k LoC). We
provide language bindings to Python and Java. Earlier
sections of this paper evaluated the impact of several in-
dividual optimizations and components of TVM, namely,
operator fusion in Figure 4, latency hiding in Figure 10,
and the ML-based cost model in Figure 12. We now fo-
cus on an end-to-end evaluation that aims to answer the
following questions:

• Can TVM optimize DL workloads over multiple

platforms?

• How does TVM compare to existing DL frame-

10

Figure 14: GPU end-to-end evaluation for TVM,
MXNet, Tensorﬂow, and Tensorﬂow XLA. Tested on the
NVIDIA Titan X.

works (which rely on heavily optimized libraries)
on each back-end?

• Can TVM support new, emerging DL workloads
(e.g., depthwise convolution, low precision opera-
tions)?

• Can TVM support and optimize for new specialized

accelerators?

To answer these questions, we evaluated TVM on four
types of platforms: (1) a server-class GPU, (2) an embed-
ded GPU, (3) an embedded CPU, and (4) a DL accelera-
tor implemented on a low-power FPGA SoC. The bench-
marks are based on real world DL inference workloads,
including ResNet [16], MobileNet [19], the LSTM Lan-
guage Model [48], the Deep Q Network (DQN) [28] and
Deep Convolutional Generative Adversarial Networks
(DCGAN) [31]. We compare our approach to exist-
ing DL frameworks, including MxNet [9] and Tensor-
Flow [2], that rely on highly engineered, vendor-speciﬁc
libraries. TVM performs end-to-end automatic optimiza-
tion and code generation without the need for an external
operator library.

6.1 Server-Class GPU Evaluation

We ﬁrst compared the end-to-end performance of
deep neural networks TVM, MXNet (v1.1), Tensor-
ﬂow (v1.7), and Tensorﬂow XLA on an Nvidia Titan
X. MXNet and Tensorﬂow both use cuDNN v7 for con-
volution operators; they implement their own versions
of depthwise convolution since it is relatively new and
not yet supported by the latest libraries. They also use
cuBLAS v8 for matrix multiplications. On the other
hand, Tensorﬂow XLA uses JIT compilation.

Figure 14 shows that TVM outperforms the base-
lines, with speedups ranging from 1.6× to 3.8× due to
both joint graph optimization and the automatic opti-
mizer, which generates high-performance fused opera-

ResNet-18MobileNet0.01.02.03.04.05.06.07.0Time(ms)LSTMLMDQNDCGAN0.00.10.20.30.40.50.60.70.80.9TensorﬂowXLATensorﬂowMXNetTVMw/ographoptTVMFigure 16: ARM A53 end-to-end evaluation of TVM and
TFLite.

Figure 15: Relative speedup of all conv2d operators in
ResNet-18 and all depthwise conv2d operators in Mo-
bileNet. Tested on a TITAN X. See Table 2 for op-
erator conﬁgurations. We also include a weight pre-
transformed Winograd [25] for 3x3 conv2d (TVM PT).

tors. DQN’s 3.8 x speedup results from its use of un-
conventional operators (4×4 conv2d, strides=2) that are
not well optimized by cuDNN; the ResNet workloads are
more conventional. TVM automatically ﬁnds optimized
operators in both cases.

To evaluate the effectiveness of operator level opti-
mization, we also perform a breakdown comparison for
each tensor operator in ResNet and MobileNet, shown in
Figure 15. We include TensorComprehension (TC, com-
mit: ef644ba) [42], a recently introduced auto-tuning
framework, as an additional baseline. 2 TC results in-
clude the best kernels it found in 10 generations × 100
population × 2 random seeds for each operator (i.e.,
2000 trials per operator). 2D convolution, one of the
most important DL operators, is heavily optimized by
cuDNN. However, TVM can still generate better GPU
kernels for most layers. Depthwise convolution is a
newly introduced operator with a simpler structure [19].
In this case, both TVM and TC can ﬁnd fast kernels com-
pared to MXNet’s handcrafted kernels. TVM’s improve-
ments are mainly due to its exploration of a large sched-
ule space and an effective ML-based search algorithm.

6.2 Embedded CPU Evaluation

We evaluated the performance of TVM on an ARM Cor-
tex A53 (Quad Core 1.2GHz). We used Tensorﬂow Lite
(TFLite, commit: 7558b085) as our baseline system.
Figure 17 compares TVM operators to hand-optimized

2According to personal communication [41], TC is not yet meant
to be used for compute-bound problems. However, it is still a good
reference baseline to include in the comparison.

Figure 17: Relative speedup of all conv2d operators in
ResNet-18 and all depthwise conv2d operators in mo-
bilenet. Tested on ARM A53. See Table 2 for the con-
ﬁgurations of these operators.

ones for ResNet and MobileNet. We observe that TVM
generates operators that outperform the hand-optimized
TFLite versions for both neural network workloads. This
result also demonstrates TVM’s ability to quickly opti-
mize emerging tensor operators, such as depthwise con-
volution operators. Finally, Figure 16 shows an end-to-
end comparison of three workloads, where TVM outper-
forms the TFLite baseline.3

Ultra Low-Precision Operators We demonstrate
TVM’s ability to support ultra low-precision infer-
ence [13, 33] by generating highly optimized operators
for ﬁxed-point data types of less than 8-bits. Low-
precision networks replace expensive multiplication with
vectorized bit-serial multiplication that is composed of
bitwise and popcount reductions [39]. Achieving efﬁ-
cient low-precision inference requires packing quantized
data types into wider standard data types, such as int8
or int32. Our system generates code that outperforms
hand-optimized libraries from Caffe2 (commit: 39e07f7)

3DCGAN and LSTM results are not presented because they are not

yet supported by the baseline.

11

C1C2C3C4C5C6C7C8C9C10C11C120.00.51.01.52.02.53.0RelativeSpeedupcuDNNTensorComprehensionsMXKernelTVMTVMPTD1D2D3D4D5D6D7D8D90.01.02.03.04.05.0RelativeSpeedupResNet-18MobileNet0.0100.0200.0300.0400.0500.0600.0700.0800.0Time(ms)DQN0.02.04.06.08.010.012.0TensorﬂowLiteTVMw/ographoptTVMC1C2C3C4C5C6C7C8C9C10C11C120.00.51.01.52.02.53.0Relative SpeedupTensorflow LiteTVMD1D2D3D4D5D6D7D8D90.01.02.03.0Relative SpeedupFigure 18: Relative speedup of single- and multi-
threaded low-precision conv2d operators in ResNet.
Baseline was a single-threaded, hand-optimized imple-
mentation from Caffe2 (commit: 39e07f7). C5, C3 are
1x1 convolutions that have less compute intensity, result-
ing in less speedup by multi-threading.

[39]. We implemented an ARM-speciﬁc tensorization
intrinsic that leverages ARM instructions to build an ef-
ﬁcient, low-precision matrix-vector microkernel.We then
used TVM’s automated optimizer to explore the schedul-
ing space.

Figure 18 compares TVM to the Caffe2 ultra low-
precision library on ResNet for 2-bit activations, 1-bit
weights inference. Since the baseline is single threaded,
we also compare it to a single-threaded TVM version.
Single-threaded TVM outperforms the baseline, particu-
larly for C5, C8, and C11 layers; these are convolution
layers of kernel size 1×1 and stride of 2 for which the ul-
tra low-precision baseline library is not optimized. Fur-
thermore, we take advantage of additional TVM capa-
bilities to produce a parallel library implementation that
shows improvement over the baseline. In addition to the
2-bit+1-bit conﬁguration, TVM can generate and opti-
mize for other precision conﬁgurations that are unsup-
ported by the baseline library, offering improved ﬂexi-
bility.

6.3 Embedded GPU Evaluation

For our mobile GPU experiments, we ran our end-to-end
pipeline on a Fireﬂy-RK3399 board equipped with an
ARM Mali-T860MP4 GPU. The baseline was a vendor-
provided library, the ARM Compute Library (v18.03).
As shown in Figure 19, we outperformed the baseline on
three available models for both float16 and float32
(DCGAN and LSTM are not yet supported by the base-
line). The speedup ranged from 1.2× to 1.6×.

6.4 FPGA Accelerator Evaluation

Vanilla Deep Learning Accelerator We now relate
how TVM tackled accelerator-speciﬁc code generation
on a generic inference accelerator design we prototyped
on an FPGA. We used in this evaluation the Vanilla Deep

Figure 19: End-to-end experiment results on Mali-
T860MP4. Two data types, ﬂoat32 and ﬂoat16, were
evaluated.

Learning Accelerator (VDLA) – which distills charac-
teristics from previous accelerator proposals [12, 21, 27]
into a minimalist hardware architecture – to demonstrate
TVM’s ability to generate highly efﬁcient schedules that
can target specialized accelerators. Figure 20 shows the
high-level hardware organization of the VDLA architec-
ture. VDLA is programmed as a tensor processor to
efﬁciently execute operations with high compute inten-
sity (e.g, matrix multiplication, high dimensional con-
volution). It can perform load/store operations to bring
blocked 3-dimensional tensors from DRAM into a con-
tiguous region of SRAM. It also provides specialized on-
chip memories for network parameters, layer inputs (nar-
row data type), and layer outputs (wide data type). Fi-
nally, VDLA provides explicit synchronization control
over successive loads, computes, and stores to maximize
the overlap between memory and compute operations.

Methodology. We implemented the VDLA design on a
low-power PYNQ board that incorporates an ARM Cor-
tex A9 dual core CPU clocked at 667MHz and an Artix-7
based FPGA fabric. On these modest FPGA resources,
we implemented a 16 × 16 matrix-vector unit clocked at
200MHz that performs products of 8-bit values and accu-
mulates them into a 32-bit register every cycle. The the-
oretical peak throughput of this VDLA design is about
102.4GOPS/s. We allocated 32kB of resources for ac-
tivation storage, 32kB for parameter storage, 32kB for
microcode buffers, and 128kB for the register ﬁle. These
on-chip buffers are by no means large enough to provide
sufﬁcient on-chip storage for a single layer of ResNet and
therefore enable a case study on effective memory reuse
and latency hiding.

We built a driver library for VDLA with a C runtime
API that constructs instructions and pushes them to the
target accelerator for execution. Our code generation al-
gorithm then translates the accelerator program to a se-
ries of calls into the runtime API. Adding the specialized
accelerator back-end took ∼2k LoC in Python.

12

C2C3C4C5C6C7C8C9C10C11C120.02.04.06.08.010.0RelativeSpeedupHandoptimizedTVMsingle-threadedTVMmulti-threadedﬂoat32ﬂoat16ResNet-18ﬂoat32ﬂoat16MobileNet0.050.0100.0150.0200.0250.0Time(ms)ﬂoat32ﬂoat16DQN0.01.02.03.04.05.0ARMComputeLibTVMw/ographoptTVMFigure 20: VDLA Hardware design overview.

End-to-End ResNet Evaluation. We used TVM to
generate ResNet inference kernels on the PYNQ plat-
form and ofﬂoaded as many layers as possible to VDLA.
We also used it to generate both schedules for the CPU
only and CPU+FPGA implementation. Due to its shal-
low convolution depth, the ﬁrst ResNet convolution layer
could not be efﬁciently ofﬂoaded on the FPGA and was
instead computed on the CPU. All other convolution lay-
ers in ResNet, however, were amenable to efﬁcient of-
ﬂoading. Operations like residual layers and activations
were also performed on the CPU since VDLA does not
support these operations.

Figure 21 breaks down ResNet inference time into
CPU-only execution and CPU+FPGA execution. Most
computation was spent on the convolution layers that
could be ofﬂoaded to VDLA. For those convolution lay-
ers, the achieved speedup was 40×. Unfortunately, due
to Amdahl’s law, the overall performance of the FPGA
accelerated system was bottlenecked by the sections of
the workload that had to be executed on the CPU. We
envision that extending the VDLA design to support
these other operators will help reduce cost even further.
This FPGA-based experiment showcases TVM’s ability
to adapt to new architectures and the hardware intrinsics
they expose.

7 Related Work

Deep learning frameworks [3, 4, 7, 9] provide convenient
interfaces for users to express DL workloads and deploy
them easily on different hardware back-ends. While ex-
isting frameworks currently depend on vendor-speciﬁc
tensor operator libraries to execute their workloads, they
can leverage TVM’s stack to generate optimized code for
a larger number of hardware devices.

High-level computation graph DSLs are a typical
way to represent and perform high-level optimiza-
tions. Tensorﬂow’s XLA [3] and the recently introduced
DLVM [45] fall into this category. The representations

13

Figure 21: We ofﬂoaded convolutions in the ResNet
workload to an FPGA-based accelerator. The grayed-out
bars correspond to layers that could not be accelerated
by the FPGA and therefore had to run on the CPU. The
FPGA provided a 40x acceleration on ofﬂoaded convo-
lution layers over the Cortex A9.

of computation graphs in these works are similar, and a
high-level computation graph DSL is also used in this
paper. While graph-level representations are a good ﬁt
for high-level optimizations, they are too high level to
optimize tensor operators under a diverse set of hard-
ware back-ends. Prior work relies on speciﬁc lowering
rules to directly generate low-level LLVM or resorts to
vendor-crafted libraries. These approaches require sig-
niﬁcant engineering effort for each hardware back-end
and operator-variant combination.

Halide [32] introduced the idea of separating comput-
ing and scheduling. We adopt Halide’s insights and reuse
its existing useful scheduling primitives in our compiler.
Our tensor operator scheduling is also related to other
work on DSL for GPUs [18, 24, 36, 37] and polyhedral-
based loop transformation [6,43]. TACO [23] introduces
a generic way to generate sparse tensor operators on
CPU. Weld [30] is a DSL for data processing tasks. We
speciﬁcally focus on solving the new scheduling chal-
lenges of DL workloads for GPUs and specialized accel-
erators. Our new primitives can potentially be adopted
by the optimization pipelines in these works.

High-performance libraries such as ATLAS [46] and
FFTW [15] use auto-tuning to get
the best perfor-
mance. Tensor comprehension [42] applied black-box
auto-tuning together with polyhedral optimizations to
optimize CUDA kernels. OpenTuner [5] and existing
hyper parameter-tuning algorithms [26] apply domain-
agnostic search. A predeﬁned cost model is used to
automatically schedule image processing pipelines in
Halide [29]. TVM’s ML model uses effective domain-
aware cost modeling that considers program structure.
The based distributed schedule optimizer scales to a
larger search space and can ﬁnd state-of-the-art kernels
on a large range of supported back-ends. More impor-
tantly, we provide an end-to-end stack that can take de-
scriptions directly from DL frameworks and jointly opti-
mize together with the graph-level stack.

MEMORY LOADUNITMEMORY STOREUNITDRAMMICRO-OP SRAMINPUT MEMWEIGHT MEMLOAD BUFFERSTORE BUFFERREGISTER FILEGEMMV_ALULOAD→EXE QEXE→LOAD QEXE→STORE QSTORE→EXE QINSTRUCTION FETCH UNITLOAD CMD QCOMPUTE CMD QCOMPUTESTORE CMD Qcontroller0.00.51.01.52.02.53.0ResNet18 Inference Time (s)TVM ARMTVM ARM+FPGAotherlayer_0convDespite the emerging popularity of accelerators for
deep learning [11, 21], it remains unclear how a com-
pilation stack can be built to effectively target these de-
vices. The VDLA design used in our evaluation provides
a generic way to summarize the properties of TPU-like
accelerators and enables a concrete case study on how
to compile code for accelerators. Our approach could
potentially beneﬁt existing systems that compile deep
learning to FPGA [34,40], as well. This paper provides a
generic solution to effectively target accelerators via ten-
sorization and compiler-driven latency hiding.

8 Conclusion

We proposed an end-to-end compilation stack to solve
fundamental optimization challenges for deep learning
across a diverse set of hardware back-ends. Our system
includes automated end-to-end optimization, which is
historically a labor-intensive and highly specialized task.
We hope this work will encourage additional studies of
end-to-end compilation approaches and open new op-
portunities for DL system software-hardware co-design
techniques.

Acknowledgement

We would like to thank Ras Bodik, James Bornholt, Xi
Wang, Tom Anderson and Qiao Zhang for their thorough
feedback on earlier versions of this paper. We would also
like to thank members of Sampa, SAMPL and Systems
groups at the Allen School for their feedback on the work
and manuscript. We would like to thank the anonymous
OSDI reviewers, and our shepherd, Ranjita Bhagwan, for
helpful feedbacks. This work was supported in part by a
Google PhD Fellowship for Tianqi Chen, ONR award
#N00014-16-1-2795, NSF under grants CCF-1518703,
CNS-1614717, and CCF-1723352, and gifts from Intel
(under the CAPA program), Oracle, Huawei and anony-
mous sources.

References

[1] NVIDIA Tesla V100 GPU Architecture: The World’s Most Ad-

vanced Data Center GPU, 2017.

[2] ABADI, M., AGARWAL, A., BARHAM, P., BREVDO, E., CHEN,
Z., CITRO, C., CORRADO, G. S., DAVIS, A., DEAN, J.,
DEVIN, M., GHEMAWAT, S., GOODFELLOW, I., HARP, A.,
IRVING, G., ISARD, M., JIA, Y., JOZEFOWICZ, R., KAISER,
L., KUDLUR, M., LEVENBERG, J., MAN ´E, D., MONGA,
R., MOORE, S., MURRAY, D., OLAH, C., SCHUSTER, M.,
SHLENS, J., STEINER, B., SUTSKEVER, I., TALWAR, K.,
TUCKER, P., VANHOUCKE, V., VASUDEVAN, V., VI ´EGAS, F.,
VINYALS, O., WARDEN, P., WATTENBERG, M., WICKE, M.,
YU, Y., AND ZHENG, X. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. Software available
from tensorﬂow.org.

[3] ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A.,
DEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD,
M., KUDLUR, M., LEVENBERG, J., MONGA, R., MOORE, S.,
MURRAY, D. G., STEINER, B., TUCKER, P., VASUDEVAN, V.,
WARDEN, P., WICKE, M., YU, Y., AND ZHENG, X. Tensor-
ﬂow: A system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implementation
(OSDI 16) (2016), pp. 265–283.

[4] AGARWAL, A., AKCHURIN, E., BASOGLU, C., CHEN, G.,
CYPHERS, S., DROPPO, J., EVERSOLE, A., GUENTER, B.,
HILLEBRAND, M., HOENS, R., HUANG, X., HUANG, Z.,
IVANOV, V., KAMENEV, A., KRANEN, P., KUCHAIEV, O.,
MANOUSEK, W., MAY, A., MITRA, B., NANO, O., NAVARRO,
G., ORLOV, A., PADMILAC, M., PARTHASARATHI, H., PENG,
B., REZNICHENKO, A., SEIDE, F., SELTZER, M. L., SLANEY,
M., STOLCKE, A., WANG, Y., WANG, H., YAO, K., YU, D.,
ZHANG, Y., AND ZWEIG, G. An introduction to computational
networks and the computational network toolkit. Tech. Rep.
MSR-TR-2014-112, August 2014.

[5] ANSEL, J., KAMIL, S., VEERAMACHANENI, K., RAGAN-
KELLEY, J., BOSBOOM, J., O’REILLY, U.-M., AND AMARAS-
INGHE, S. Opentuner: An extensible framework for program au-
totuning. In International Conference on Parallel Architectures
and Compilation Techniques (Edmonton, Canada, August 2014).

[6] BAGHDADI, R., BEAUGNON, U., COHEN, A., GROSSER, T.,
KRUSE, M., REDDY, C., VERDOOLAEGE, S., BETTS, A.,
DONALDSON, A. F., KETEMA, J., ABSAR, J., HAASTREGT,
S. V., KRAVETS, A., LOKHMOTOV, A., DAVID, R., AND HA-
JIYEV, E. Pencil: A platform-neutral compute intermediate lan-
guage for accelerator programming. In Proceedings of the 2015
International Conference on Parallel Architecture and Compila-
tion (PACT) (Washington, DC, USA, 2015), PACT ’15, IEEE
Computer Society, pp. 138–149.

[7] BASTIEN, F., LAMBLIN, P., PASCANU, R., BERGSTRA, J.,
GOODFELLOW, I. J., BERGERON, A., BOUCHARD, N., AND
BENGIO, Y. Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning NIPS 2012
Workshop, 2012.

[8] CHEN, T., AND GUESTRIN, C. Xgboost: A scalable tree boost-
In Proceedings of the 22Nd ACM SIGKDD Inter-
ing system.
national Conference on Knowledge Discovery and Data Mining
(New York, NY, USA, 2016), KDD ’16, ACM, pp. 785–794.

[9] CHEN, T., LI, M., LI, Y., LIN, M., WANG, N., WANG, M.,
XIAO, T., XU, B., ZHANG, C., , AND ZHANG, Z. MXNet:
A ﬂexible and efﬁcient machine learning library for heteroge-
neous distributed systems. In Neural Information Processing Sys-
tems, Workshop on Machine Learning Systems (LearningSys’15)
(2015).

[10] CHEN, T.-F., AND BAER, J.-L. Effective hardware-based data
prefetching for high-performance processors. IEEE Transactions
on Computers 44, 5 (May 1995), 609–623.

[11] CHEN, Y., LUO, T., LIU, S., ZHANG, S., HE, L., WANG, J., LI,
L., CHEN, T., XU, Z., SUN, N., AND TEMAM, O. Dadiannao:
A machine-learning supercomputer. In Proceedings of the 47th
Annual IEEE/ACM International Symposium on Microarchitec-
ture (Washington, DC, USA, 2014), MICRO-47, IEEE Computer
Society, pp. 609–622.

[12] CHEN, Y.-H., EMER, J., AND SZE, V. Eyeriss: A spatial ar-
chitecture for energy-efﬁcient dataﬂow for convolutional neural
networks. In Proceedings of the 43rd International Symposium
on Computer Architecture (Piscataway, NJ, USA, 2016), ISCA
’16, IEEE Press, pp. 367–379.

[13] COURBARIAUX, M., BENGIO, Y., AND DAVID, J. Binarycon-
nect: Training deep neural networks with binary weights during
propagations. CoRR abs/1511.00363 (2015).

14

[14] EGGERS, S. J., EMER, J. S., LEVY, H. M., LO, J. L., STAMM,
R. L., AND TULLSEN, D. M. Simultaneous multithreading: a
platform for next-generation processors. IEEE Micro 17, 5 (Sept
1997), 12–19.

[15] FRIGO, M., AND JOHNSON, S. G. Fftw: an adaptive software ar-
chitecture for the fft. In Acoustics, Speech and Signal Processing,
1998. Proceedings of the 1998 IEEE International Conference on
(May 1998), vol. 3, pp. 1381–1384 vol.3.

[16] HE, K., ZHANG, X., REN, S., AND SUN, J. Identity mappings
arXiv preprint arXiv:1603.05027

in deep residual networks.
(2016).

[17] HEGARTY, J., BRUNHAVER, J., DEVITO, Z., RAGAN-KELLEY,
J., COHEN, N., BELL, S., VASILYEV, A., HOROWITZ, M., AND
HANRAHAN, P. Darkroom: Compiling high-level image pro-
cessing code into hardware pipelines. ACM Trans. Graph. 33, 4
(July 2014), 144:1–144:11.

[18] HENRIKSEN, T., SERUP, N. G. W., ELSMAN, M., HENGLEIN,
F., AND OANCEA, C. E.
Futhark: Purely functional gpu-
programming with nested parallelism and in-place array updates.
In Proceedings of the 38th ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation (New York,
NY, USA, 2017), PLDI 2017, ACM, pp. 556–571.

[19] HOWARD, A. G., ZHU, M., CHEN, B., KALENICHENKO, D.,
WANG, W., WEYAND, T., ANDREETTO, M., AND ADAM, H.
Mobilenets: Efﬁcient convolutional neural networks for mobile
vision applications. CoRR abs/1704.04861 (2017).

[20] JOUPPI, N. P.

Improving direct-mapped cache performance
by the addition of a small fully-associative cache and prefetch
buffers. In [1990] Proceedings. The 17th Annual International
Symposium on Computer Architecture (May 1990), pp. 364–373.

[21] JOUPPI, N. P., YOUNG, C., PATIL, N., PATTERSON, D.,
AGRAWAL, G., BAJWA, R., BATES, S., BHATIA, S., BODEN,
N., BORCHERS, A., BOYLE, R., CANTIN, P.-L., CHAO, C.,
CLARK, C., CORIELL, J., DALEY, M., DAU, M., DEAN, J.,
GELB, B., GHAEMMAGHAMI, T. V., GOTTIPATI, R., GUL-
LAND, W., HAGMANN, R., HO, C. R., HOGBERG, D., HU,
J., HUNDT, R., HURT, D., IBARZ, J., JAFFEY, A., JAWORSKI,
A., KAPLAN, A., KHAITAN, H., KILLEBREW, D., KOCH, A.,
KUMAR, N., LACY, S., LAUDON, J., LAW, J., LE, D., LEARY,
C., LIU, Z., LUCKE, K., LUNDIN, A., MACKEAN, G., MAG-
GIORE, A., MAHONY, M., MILLER, K., NAGARAJAN, R.,
NARAYANASWAMI, R., NI, R., NIX, K., NORRIE, T., OMER-
NICK, M., PENUKONDA, N., PHELPS, A., ROSS, J., ROSS, M.,
SALEK, A., SAMADIANI, E., SEVERN, C., SIZIKOV, G., SNEL-
HAM, M., SOUTER, J., STEINBERG, D., SWING, A., TAN, M.,
THORSON, G., TIAN, B., TOMA, H., TUTTLE, E., VASUDE-
VAN, V., WALTER, R., WANG, W., WILCOX, E., AND YOON,
D. H. In-datacenter performance analysis of a tensor processing
unit. In Proceedings of the 44th Annual International Symposium
on Computer Architecture (New York, NY, USA, 2017), ISCA
’17, ACM, pp. 1–12.

[22] KIRKPATRICK, S., GELATT, C. D., AND VECCHI, M. P. Op-
timization by simulated annealing. Science 220, 4598 (1983),
671–680.

[23] KJOLSTAD, F., KAMIL, S., CHOU, S., LUGATO, D., AND
AMARASINGHE, S. The tensor algebra compiler. Proc. ACM
Program. Lang. 1, OOPSLA (Oct. 2017), 77:1–77:29.

[24] KL ¨OCKNER, A. Loo.py: transformation-based code generation
for GPUs and CPUs. In Proceedings of ARRAY ‘14: ACM SIG-
PLAN Workshop on Libraries, Languages, and Compilers for Ar-
ray Programming (Edinburgh, Scotland., 2014), Association for
Computing Machinery.

[25] LAVIN, A., AND GRAY, S. Fast algorithms for convolutional
neural networks. In 2016 IEEE Conference on Computer Vision

and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June
27-30, 2016 (2016), pp. 4013–4021.

[26] LI, L., JAMIESON, K. G., DESALVO, G., ROSTAMIZADEH,
A., AND TALWALKAR, A. Efﬁcient hyperparameter optimiza-
tion and inﬁnitely many armed bandits. CoRR abs/1603.06560
(2016).

[27] LIU, D., CHEN, T., LIU, S., ZHOU, J., ZHOU, S., TEMAN, O.,
FENG, X., ZHOU, X., AND CHEN, Y. Pudiannao: A polyvalent
In Proceedings of the Twentieth
machine learning accelerator.
International Conference on Architectural Support for Program-
ming Languages and Operating Systems (New York, NY, USA,
2015), ASPLOS ’15, ACM, pp. 369–381.

[28] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A.,
VENESS, J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER,
M., FIDJELAND, A. K., OSTROVSKI, G., ET AL. Human-level
control through deep reinforcement learning. Nature 518, 7540
(2015), 529.

[29] MULLAPUDI, R. T., ADAMS, A., SHARLET, D., RAGAN-
KELLEY, J., AND FATAHALIAN, K. Automatically scheduling
halide image processing pipelines. ACM Trans. Graph. 35, 4
(July 2016), 83:1–83:11.

[30] PALKAR, S., THOMAS, J. J., NARAYANAN, D., SHANBHAG,
A., PALAMUTTAM, R., PIRK, H., SCHWARZKOPF, M., AMA-
RASINGHE, S. P., MADDEN, S., AND ZAHARIA, M. Weld: Re-
thinking the interface between data-intensive applications. CoRR
abs/1709.06416 (2017).

[31] RADFORD, A., METZ, L., AND CHINTALA, S. Unsupervised
representation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434 (2015).

[32] RAGAN-KELLEY, J., BARNES, C., ADAMS, A., PARIS, S., DU-
RAND, F., AND AMARASINGHE, S. Halide: A language and
compiler for optimizing parallelism, locality, and recomputation
in image processing pipelines. In Proceedings of the 34th ACM
SIGPLAN Conference on Programming Language Design and
Implementation (New York, NY, USA, 2013), PLDI ’13, ACM,
pp. 519–530.

[33] RASTEGARI, M., ORDONEZ, V., REDMON, J., AND FARHADI,
A. Xnor-net: Imagenet classiﬁcation using binary convolutional
In European Conference on Computer Vision
neural networks.
(2016), Springer, pp. 525–542.

[34] SHARMA, H., PARK, J., MAHAJAN, D., AMARO, E., KIM,
J. K., SHAO, C., MISHRA, A., AND ESMAEILZADEH, H. From
high-level deep neural models to fpgas. In Microarchitecture (MI-
CRO), 2016 49th Annual IEEE/ACM International Symposium on
(2016), IEEE, pp. 1–12.

[35] SMITH, J. E. Decoupled access/execute computer architectures.
In Proceedings of the 9th Annual Symposium on Computer Archi-
tecture (Los Alamitos, CA, USA, 1982), ISCA ’82, IEEE Com-
puter Society Press, pp. 112–119.

[36] STEUWER, M., REMMELG, T., AND DUBACH, C. Lift: A func-
tional data-parallel ir for high-performance gpu code generation.
In Proceedings of the 2017 International Symposium on Code
Generation and Optimization (Piscataway, NJ, USA, 2017), CGO
’17, IEEE Press, pp. 74–85.

[37] SUJEETH, A. K., LEE, H., BROWN, K. J., CHAFI, H., WU, M.,
ATREYA, A. R., OLUKOTUN, K., ROMPF, T., AND ODERSKY,
M. Optiml: An implicitly parallel domain-speciﬁc language for
machine learning. In Proceedings of the 28th International Con-
ference on International Conference on Machine Learning (USA,
2011), ICML’11, pp. 609–616.

[38] TAI, K. S., SOCHER, R., AND MANNING, C. D.

Improved
semantic representations from tree-structured long short-term
memory networks. arXiv preprint arXiv:1503.00075 (2015).

15

[39] TULLOCH, A., AND JIA, Y. High performance ultra-low-
arXiv preprint

precision convolutions on mobile devices.
arXiv:1712.02427 (2017).

[40] UMUROGLU, Y., FRASER, N. J., GAMBARDELLA, G., BLOTT,
M., LEONG, P. H. W., JAHRE, M., AND VISSERS, K. A. FINN:
A framework for fast, scalable binarized neural network infer-
ence. CoRR abs/1612.07119 (2016).

[41] VASILACHE, N. personal communication.

[42] VASILACHE, N., ZINENKO, O., THEODORIDIS, T., GOYAL, P.,
DEVITO, Z., MOSES, W. S., VERDOOLAEGE, S., ADAMS,
A., AND COHEN, A. Tensor comprehensions: Framework-
agnostic high-performance machine learning abstractions. CoRR
abs/1802.04730 (2018).

[43] VERDOOLAEGE, S., CARLOS JUEGA, J., COHEN, A., IGNA-
CIO G ´OMEZ, J., TENLLADO, C., AND CATTHOOR, F. Polyhe-
dral parallel code generation for cuda. ACM Trans. Archit. Code
Optim. 9, 4 (Jan. 2013), 54:1–54:23.

[44] VOLKOV, V. Understanding Latency Hiding on GPUs. PhD

thesis, University of California at Berkeley, 2016.

[45] WEI, R., ADVE, V., AND SCHWARTZ, L. Dlvm: A mod-
ern compiler infrastructure for deep learning systems. CoRR
abs/1711.03016 (2017).

[46] WHALEY, R. C., AND DONGARRA, J. J. Automatically tuned
linear algebra software. In Proceedings of the 1998 ACM/IEEE
Conference on Supercomputing (Washington, DC, USA, 1998),
SC ’98, IEEE Computer Society, pp. 1–27.

[47] WILLIAMS, S., WATERMAN, A., AND PATTERSON, D.
Rooﬂine: An insightful visual performance model for multicore
architectures. Commun. ACM 52, 4 (Apr. 2009), 65–76.

[48] ZAREMBA, W., SUTSKEVER, I., AND VINYALS, O. Recurrent
neural network regularization. arXiv preprint arXiv:1409.2329
(2014).

16

