Doc-StartDLBI: Deep learning guided Bayesian inference for structure reconstruction
of super-resolution ﬂuorescence microscopy
Yu Li1,†, Fan Xu2,†, Fa Zhang2, Pingyong Xu3, Mingshu Zhang3, Ming Fan4, Lihua Li4, Xin Gao1,∗, Renmin Han1,∗
1King Abdullah University of Science and Technology (KAUST), Computational Bioscience Research Center (CBRC), Computer,
Electrical and Mathematical Sciences and Engineering (CEMSE) Division, Thuwal, 23955-6900, Saudi Arabia. 2High Performance
Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China. 3Key
Laboratory of RNA Biology, Institute of Biophysics, Chinese Academy of Sciences, Beijing 100101, China. 4Institute of Biomedical
Engineering and Instrumentation, Hangzhou Dianzi University, Hangzhou, 310018, China.

8
1
0
2

p
e
S
1

]

V
C
.
s
c
[

3
v
7
7
7
7
0
.
5
0
8
1
:
v
i
X
r
a

ABSTRACT
Motivation: Super-resolution ﬂuorescence microscopy, with a
resolution beyond the diffraction limit of
light, has become an
indispensable tool to directly visualize biological structures in living
cells at a nanometer-scale resolution. Despite advances in high-
density super-resolution ﬂuorescent techniques, existing methods still
have bottlenecks, including extremely long execution time, artiﬁcial
thinning and thickening of structures, and lack of ability to capture
latent structures.
Results: Here we propose a novel deep learning guided Bayesian
inference approach, DLBI, for the time-series analysis of high-density
ﬂuorescent
images. Our method combines the strength of deep
learning and statistical inference, where deep learning captures the
underlying distribution of the ﬂuorophores that are consistent with the
observed time-series ﬂuorescent images by exploring local features
and correlation along time-axis, and statistical inference further reﬁnes
the ultrastructure extracted by deep learning and endues physical
meaning to the ﬁnal image. In particular, our method contains three
main components. The ﬁrst one is a simulator that takes a high-
resolution image as the input, and simulates time-series low-resolution
ﬂuorescent images based on experimentally calibrated parameters,
which provides supervised training data to the deep learning model.
The second one is a multi-scale deep learning module to capture
information in each input low-resolution image as well
both spatial
as temporal information among the time-series images. And the third
one is a Bayesian inference module that takes the image from the
deep learning module as the initial localization of ﬂuorophores and
removes artifacts by statistical inference. Comprehensive experimental
results on both real and simulated datasets demonstrate that our
method provides more accurate and realistic local patch and large-
ﬁeld reconstruction than the state-of-the-art method, the 3B analysis,
while our method is more than two orders of magnitude faster.
Availability: The main program is available at https://github.com/
lykaust15/DLBI.
1 INTRODUCTION
Fluorescence microscopy with a resolution beyond the diﬀraction limit of
light (i.e., super-resolution) has played an important role in biological
sciences. The application of super-resolution ﬂuorescence microscope
techniques to living-cell
imaging promises dynamic information on
complex biological structures with nanometer-scale resolution.

Recent development of ﬂuorescence microscopy takes advantages of
both the development of optical theories and computational methods.
Living cell stimulated emission depletion (STED) (Hein et al., 2008),
reversible saturable optical linear ﬂuorescence transitions (RESOLFT)
(A Schwentker et al., 2007), and structured illumination microscopy
(SIM) (Gustafsson, 2005) mainly focus on the innovation of instruments,
which requires sophisticated, expensive optical setups and specialized

†These authors contributed equally to this work.
∗All
(xin.gao@kaust.edu.sa) and Renmin Han (renmin.han@kaust.edu.sa).

correspondence

addressed

should

to Xin Gao

be

expertise for accurate optical alignment. The time-series analysis
based on localization microscopy techniques, such as photoactivatable
localization microscopy (PALM) (Hess et al., 2006) and stochastic optical
reconstruction microscopy (STORM) (Rust et al., 2006), is mainly based
on the computational methods, which build a super-resolution image
from the localized positions of single molecules in a large number of
images. Though compared with STED, RESOLFT and SIM, PALM and
STORM do not need specialized microscopes, the localization techniques
of PALM and STORM require the ﬂuorescence emission from individual
ﬂuorophores to not overlap with each other, leading to long imaging time
and increased damage to live samples (Lippincott-Schwartz and Manley,
2009). More recent methods (Holden et al., 2011; Huang et al., 2011;
Quan et al., 2011; Zhu et al., 2012) alleviate the long exposure problem
by developing multiple-ﬂuorophore ﬁtting techniques to allow relatively
dense ﬂuorescent data, but still do not solve the problem completely.

Bayesian-based time-series analysis of high-density ﬂuorescent
images (Cox et al., 2012; Xu et al., 2015, 2017) further pushes the
limit. By using data from overlapping ﬂuorophores as well as information
it extends the super-resolution
from blinking and bleaching events,
imaging to the large-ﬁeld imaging of living cells. Despite its potential
to resolve ultrastructures and fast cellular dynamics in living cells,
several bottlenecks still remain. The state-of-the-art methods, such as
Bayesian analysis of the blinking and bleaching (i.e., the 3B analysis)
(Cox et al., 2012), are computationally expensive, and may cause
artiﬁcial thinning and thickening of structures due to local sampling.
Signiﬁcant improvements on runtime and accuracy have been achieved by
single molecule-guided Bayesian localization microscopy (SIMBA) (Xu
et al., 2017) with the introduction of dual-channel ﬂuorescent imaging
and single molecule-guided Bayesian inference. However, the enhanced
process is severely limited by the specialized class of proteins.

Deep learning has accomplished great success in various ﬁelds,
including super-resolution imaging (Ledig et al., 2016; Kim et al., 2016;
Lim et al., 2017). Among diﬀerent deep learning architectures,
the
generative adversarial network (GAN) (Goodfellow et al., 2014) achieved
the state-of-the-art performance on single image super-resolution (SISR)
(Ledig et al., 2016). However, there are two fundamental diﬀerences
between the SISR and super-resolution ﬂuorescence microscopy. First,
the input of SISR is a downsampled (i.e.,
low-resolution) image of
a static high-resolution image and the expected output is the original
image, whereas the input of super-resolution ﬂuorescence microscopy is
a time-series of low-resolution ﬂuorescent images and the output is the
high-resolution image containing estimated locations of the ﬂuorophores
(i.e., the reconstructed structure). Second, the nature of SISR ensures that
there are readily a huge amount of existing data to train deep learning
models, whereas for ﬂuorescence microscopy,
there are only limited
time-series datasets. Furthermore, most of these datasets do not have
the ground-truth high-resolution images, which make supervised deep
learning infeasible.

In this paper, we propose a novel deep learning guided Bayesian
inference framework, DLBI,
for structure reconstruction of high-
resolution ﬂuorescent microcopy. Our framework combines the strength
of stochastic simulation, deep learning and statistical inference. To
this is the ﬁrst deep learning-based super-resolution
our knowledge,

c(cid:13) The Author 2017.

1

 
 
 
 
 
 
2

Li et al.

ﬂuorescent microscopy method. In particular, the stochastic simulation
module simulates time-series low-resolution images from high-resolution
images based on experimentally calibrated parameters of ﬂuorophores
and stochastic modeling, which provides supervised training data for
deep learning models. The deep learning module takes the simulated
time-series low-resolution images as inputs, captures the underlying
distribution that generates the ground-truth super-resolutions images by
exploring local features and correlation along time-axis of the low-
resolution images, and outputs a predicted high-resolution image. To
achieve this goal, we develop a generative adversarial network (GAN)
in which a generator network and a discriminator network contest with
each other. The generator network tries to learn the distribution of the
high-resolution images in a multi-scale manner, whereas the discriminator
network tries to discriminate the ground-truth images and the images
produced by the generator network. In order to capture the deep features
in the images, we further ease the degradation issue by integrating residual
networks (He et al., 2016) into our GAN model, where degradation
means that stacking more network layers does not lead to better accuracy.
The high-resolution image produced by the deep learning module is
often very close to the ground-truth image. However, it can still contain
some artifacts, and more importantly, lacks the physical meaning. Thus,
we develop the Bayesian inference module to take the predicted high-
resolution image from deep learning, run Bayesian inference from the
initial locations of ﬂuorophores in the predicted image, and predict a more
accurate high-resolution image.

Comprehensive experimental results on two simulated and three
real-world datasets demonstrate that DLBI provides more accurate and
realistic local-patch as well as large-ﬁeld reconstruction than the state-
of-the-art method, the 3B analysis (Cox et al., 2012). Meanwhile, our
method is more than 100 times faster than the 3B analysis.

2 MATERIALS AND METHODS

(i) stochastic
As shown in Fig. 1, DLBI contains three modules:
simulation (Section 2.1), (ii) deep neural networks (Section 2.2), and (iii)
Bayesian inference (Section 2.3).

Fig. 1: The overall workﬂow of DLBI. The three modules are shown in solid boxes: the
simulation module (green), the deep learning module (red), and the Bayesian inference
module (blue). The training (orange) and testing (purple) procedures are shown in dashed
boxes. For training,
time-series low-resolution images simulated from the simulation
module are used to train the deep learning module in a multi-scale manner, from 2X,
4X, to 8X resolution. For testing, given certain time-series low-resolution images, the
deep learning module predicts the 2X to 8X super-resolution images, and the Bayesian
inference module takes the predicted 8X image (which contains artifacts) and produces the
ﬁnal high-resolution image.

Although deep learning has proved its great superiority in various
ﬁelds, it has not been used for ﬂuorescent microscopy image analysis.
One of the possible reasons is the lack of supervised training data,
which means the number of time-series low-resolution image datasets
is limited and even for the existing datasets,
the ground-truth high-
resolution images are often unknown. Here, a stochastic simulation based
on the experimentally calibrated parameters is designed to solve this
issue, without the need of collecting a massive amount of real ﬂuorescent
images. This empowers our deep neural networks to eﬀectively learn
the latent structures under the low-resolution, high-noise and stochastic
ﬂuorescing conditions. The primitive super-resolution images produced

by deep neural networks still contain artifacts and lack physical meaning,
we ﬁnally develop a Bayesian inference module based on the mechanism
of ﬂuorophore switching to produce high-conﬁdent images.

Our method combines the strength of deep learning and statistical
inference, where deep learning captures the underlying distribution that
generates the training super-resolution images by exploring local features
and correlation along time-axis, and statistical inference removes artifacts
and reﬁnes the ultrastructure extracted by deep learning, and further
endues physical meaning to the ﬁnal image.

2.1 The stochastic simulation module

The input of our simulation module is a high-resolution image that depicts
the distribution of the ﬂuorophores and the output is a time-series of low-
resolution ﬂuorescent images with diﬀerent ﬂuorescing states. We refer
the readers to Section S1 for terminologies in ﬂuorescence microscopy.

In our simulation, Laplace-ﬁltered natural

images and sketches
are used as the ground-truth high-resolution images that contain the
ﬂuorophore distribution. If a gray-scale image is given,
the depicted
shapes are considered as the distribution of ﬂuorophores and each pixel
value on the image is considered as the density of ﬂuorophores at the
location. We then create a number of simulated ﬂuorophores that are
distributed according to the distribution and the densities. For each
ﬂuorophore,
i.e., among
it switches according to a Markov model,
states of emitting (activated), not emitting (inactivated), and bleached.
The emitting state means that the ﬂuorophore emits photons and a
spot according to the point spread function (PSF) is depicted on the
canvas. All the spots of the emitting ﬂuorophores thus result in a high-
resolution ﬂuorescent image. Applying the Markov model on the initial
high-resolution image generates a time-series of high-resolution images.
After adding the background to the high-resolution images,
they are
downsampled to low-resolution images and noise is ﬁnally added. Fig.
2 summarizes the stochastic simulation procedure.

Here, the success of simulation relies on three factors: (i) the principal
of the linear optical system, (ii) experimentally calibrated parameters of
ﬂuorophores, and (iii) stochastic modeling.

2.1.1 Linear optics A ﬂuorescence microscope is considered as a
linear optical system, in which the superposition principle is valid, i.e.,
Image(Obj1 + Obj2) = Image(Obj1) + Image(Obj2). The behavior of
ﬂuorophores is considered invariant to mutual interaction. Therefore,
for high-density ﬂuorescent images, the pixel density can be directly
calculated from the light emitted from its surrounding ﬂuorophores.

When a ﬂuorophore is activated, an observable spot can be recorded
by the sensor, the shape of which is called the point spread function (PSF).
Considering the limitation of sensor capability, the PSF of an isotropic
point source is often approximated as a Gaussian function:

I(x, y) = I0 exp(−

1
2σ2

((x − x0)2 + (y − y0)2)),

(1)

where σ is calculated from the ﬂuorophore in the specimen that speciﬁes
the width of the PSF, I0 is the peak intensity and is proportional to the

Fig. 2: The workﬂow of stochastic simulation. Firstly, a high-resolution image is inputted
as the distribution and density of ﬂuorophores. Then, the emitting of photons is simulated
based on the stochastic parameters for each time frame. A random background (DC oﬀset)
is added to each image. The images are then downsampled to low-resolution and noise is
added, which results in a time-series of low-resolution images.

Deep learning guided Bayesian inference for super-resolution ﬂuorescence microscopy

3

photon emission rate and the single-frame acquisition time, (x0, y0) is the
location of the ﬂuorophore.

While PSF describes the shape,

the full width at half maximum
(FWHM) describes the distinguishability. It is deﬁned to be the half width
of the maximum amplitude of PSF. If PSF is modeled as a Gaussian
function, the relationship between FWHM and σ is given by
√

FWHM = 2

2 ln 2 σ ≈ 2.355 σ.

(2)

Considering the probability of linear optics, a high-density ﬂuorescent

image is composed by PSFs of the ﬂuorophores.

2.1.2 Calibrated parameters of ﬂuorophores
In most imaging systems,
the characteristics of a ﬂuorescent protein can be calibrated by
experimental techniques. With all the calibrated parameters, it is not
diﬃcult to describe and simulate the ﬂuorescent switching of a specialized
protein.

The ﬁrst characteristic of a ﬂuorophore is its switching probability. A
ﬂuorophore always transfers among three states, emitting, not emitting
and bleached, which can be speciﬁed by a Markov model (Fig. 3). If
the ﬂuorophore transfers from not emitting to bleached, it will not emit
any photon anymore. As linear optics, each ﬂuorophore’s transitions are
assumed to be independent.

Fig. 3: The Markov model describing state transition of a ﬂuorophore.

The second characteristic of a ﬂuorophore is its PSF. When a real-
world ﬂuorophore is activated, the emitted photons and its corresponding
PSF will not stay unchanged over time. The stochasticity of the PSF
and photon strength describes the characteristics of a ﬂuorescent protein.
To simulate the ﬂuorescence, we should not ignore these properties.
Fortunately, the related parameters can be well-calibrated. The PSF and
FWHM of a ﬂuorescent protein can be measured in low molecule density.
In an instrument for PALM or STORM, the PSF of the microscope can
be measured by acquiring image frames, ﬁtting the ﬂuorescent spots
parameter, normalizing and then averaging the aligned single-molecule
images. The distribution of FWHM can be obtained from statistical
analysis. The principle of linear optics ensures that the parameters
measured in single-molecule conditions is also applicable to high-density
conditions.

In our simulation, a log-normal distribution (Cox et al., 2012; Zhu
et al., 2012) is used to approximate the experimentally measured single
ﬂuorophore photon number distribution. Firstly, a table of ﬂuorophore’s
experimentally calibrated FWHM parameters is used to initialize the
PSF table in our simulation, according to Eq.1 and Eq.2. Then for each
ﬂuorophore recorded in the high-resolution image, the state of the current
image frame is calculated according to the transfer table [P1, P2, P3, P4,
P5] (Fig. 3) and a random PSF shape is produced if the corresponding
ﬂuorophore is at the “emitting” state. This procedure is repeated for each
ﬂuorophore, which results in the ﬁnal ﬂuorescent image.

Stochastic modeling The illumination of real-world objects is
2.1.3
diﬀerent at diﬀerent time. In general, the illumination change of real-
world objects can be suppressed by high-pass ﬁltering with a large
Gaussian kernel. However,
this operation will sharpen the random
noise and cannot remove the background (or DC oﬀset1). To make
our simulation more realistic, several stochastic factors are introduced.
First, for a series of simulated ﬂuorescent images, a background value
calculated from the multiplication between a random strength factor

1 DC oﬀset, DC bias or DC component denotes the mean value of a
signal. If the mean amplitude is zero, there is no DC oﬀset. For most
microscopy, the DC oﬀset can be calibrated but cannot be completely
removed.

and the average image intensity is added to the ﬂuorescent images to
simulate the DC oﬀset. For the same time-series, the strength factor
remains unchanged but the background strength changes with the image
intensity. Second, the high-resolution ﬂuorescent image is downsampled
and random Gaussian noise is added to the low-resolution image. Here,
the noise is also stochastic for diﬀerent time-series and close to the noise
strength that is measured from the real-world microscopy.

The default setting of our simulation takes a 480 × 480 pixel high-
resolution image as the input and simulates 200 frames of 60 × 60 pixel
(i.e., 8× binned) low-resolution images.
2.2 The deep learning module

We build a deep residual network under the generative adversarial
network (GAN) framework (Goodfellow et al., 2014; Ledig et al.,
2016) to estimate the primitive super-resolution image IS R (the latent
structure features) from time-series of low-resolution ﬂuorescent images
T = {IFL
}k=1,...,K . Instead of building just one generative model, our
approach builds a pair of models, a generator model, G, which produces
the estimation of the underling structure of the training images, and a
discriminator model, D, which is trained to distinguish the reconstructed
super-resolution image from the ground-truth one. Fig. 4 demonstrates
the overview of our deep learning framework.

k

Fig. 4: Overview of our deep learning framework. There are two main components of the
model, the generator network (G) and the discriminator network (D). G is used to convert
the time-series noisy, low-resolution images into a noise-free, super-resolution image while
D is used to distinguish the ground-truth high-resolution image from the one produced by
G. G and D are designed to contest each other, which are trained simultaneously. As the
training goes on, both G’s ability to generate better super-resolution images and D’s ability
to distinguish the generated images are improved, which results in more and more similar
images to the ground-truth from G. During testing, only G is used.

2.2.1 Basic concepts The goal of training a generator neural network
is to obtain the optimized parameters, θG, for the generating function, G,
with the minimum diﬀerence between the output super-resolution image,
IS R, and ground-truth, IHR:

ˆθG = arg min

θG

1
N

N(cid:88)

n=1

lS R(G(Tn, θG), IHR

n

),

(3)

where G(Tn, θG) is the generated super-resolution image by G for the
nth training sample, N is the number of training images, and lS R is a loss
function that will be speciﬁed later.

For the discriminator network D, D(x) represents the probability of
the data being the real high-resolution image rather than from G. When
training D, we try to maximize its ability to diﬀerentiate ground-truth
to force G to learn better details. When
from the generated image,
training G, we try to minimize log(1 − D(G(Tn, θG), θD)), which is the
log likelihood of D being able to tell that the image generated by G is not
ground-truth. That is, we minimax the following function:

E

min
θG
+E

IHR∼ptrain(IHR)[log(D(IHR, θD))]

max
θD
IHR∼pG (T )[log(1 − D(G(T , θG), θD))].
In this way, we force the generator to optimize the generative loss, which
is composed of perceptual loss, content loss and adversarial loss (more
details of the loss function will be introduced in Section 2.2.3).

(4)

2.2.2 Model architecture Our network is specialized for the analysis
of time-series images through: (1) 3D ﬁlters in the neural network that

4

Li et al.

take all the image frames into consideration, which extracts the time
dependent information naturally, (2) two speciﬁcally designed modules
in the generator residual network, i.e., Monte Carlo dropout (Gal and
Ghahramani, 2015) and denoise shortcut, to cope with the stochastic
switching of ﬂuorophores and random noise, and (3) a novel incremental
multi-scale architecture and parameter tuning scheme, which is designed
to suppress the error accumulation in large upscaling factor neural
networks.

features. Section S2 provides detailed descriptions of the generator and
discriminator networks.
2.2.3 Model training and testing GAN is known to be diﬃcult to train
(Salimans et al., 2016). We use the following techniques to obtain stable
models. For the generator model, we do not train GAN immediately after
initialization. Instead, we pretrain the model. During the pretrain process,
we minimize the mean squared error between the super-resolution image
and the ground-truth, i.e., with the pixel-wise MSE loss as

lS R
MS Eµ

=

1
µ2WH

µW(cid:88)

µH(cid:88)

x

y

(G(T , θGµ ) − IHR

x,y )2,

(5)

where W is the width of the low-resolution image, H is the height of
the low-resolution image, and µ = 2, 4, 8 is the upscaling factor. During
pretraining, we optimize lS R
simultaneously, instead of
optimizing the sum of them.

, lS R

, lS R

MS E8

MS E2

MS E4

Only after the model has been well-pretrained do we start training
the GAN. During that process, we also use VGG19 (Simonyan and
Zisserman, 2014) to calculate the perceptual loss (Johnson et al., 2016)
and use Adam optimizer (Kingma and Ba, 2014) with learning rate decay
as the optimizer. When feeding an image to the VGG model, we resize
the image to fulﬁll the dimensionality requirement:

lS R
VGGµ

=

V(cid:88)

i=1

(VGG(G(T , θGµ ))i − VGG(IHR)i)2,

(6)

where V is the dimensionality of the VGG embedding output.

During ﬁnal tuning, we simultaneously optimize the 2×, 4×, and 8×

upscaling by the generative loss:

lS R
GANµ

= 0.4 ∗ lS R

MS Eµ

+ 10−6 ∗ lS R

VGGµ

,

and

lS R
GAN8

= 0.5 ∗ lS R

MS E8

+ 10−3 ∗ lS R

ADV8

+ 10−6 ∗ lS R

VGG8

,

(7)

(8)

where µ = 2, 4 and the 8× upscaling has an additional

the adversarial loss lS R
discriminator network, we use the following loss function:

ADV8

= (cid:80)N

term,
n=1 log(1 − D(G(Tn, θG), θD)). For the

lS R
DIS

=

N(cid:88)

n=1

log(D(G(Tn, θG), θD)) +

N(cid:88)

n=1

log(1 − D(IHR

n

, θD)).

(9)

During testing, for the same input time-series images, we run the
model multiple times to get a series of super-resolution images. Because
of the Monte Carlo dropout layer in the generator model, all of the super-
resolution images are not identical. We then compute the average of these
images as the ﬁnal prediction, with another map showing the p-value of
each pixel. We use Tensorﬂow combined with TensorLayer (Dong et al.,
2017) to implement the deep learning module. Trained on a workstation
with one Pascal Titan X, the model gets converged in around 8 hours.

2.3 The Bayesian inference module

Our Bayesian inference module takes both the time-series low-resolution
images and the primitive super-resolution image produced by the deep
learning module as inputs, and generates a set of optimized ﬂuorophore
locations, which are further interpreted as a high-conﬁdent super-
resolution image. Since the deep learning module has already depicted the
ultrastructures in the image, we use these structures as the initialization of
the ﬂuorophore locations, re-sampling with a random punishment against
Ix,y
artifacts. For each pixel, we re-sample the ﬂuorophore intensity by
and the location by (x, y) ± rand(x, y), where Ix,y is the pixel value in the
image produced by deep learning, rand(x, y) is limited in ±8. In this way,
the extremely high illumination can be suppressed and fake structures will
be re-estimated.

(cid:112)

Fig. 5: Architecture of the generator network, which is composed of a residual network
component and a multi-scale upsampling component. The low-resolution images are ﬁrstly
fed to the residual network to extract information from the original 3D space, during which
denoising is performed by the Monte Carlo dropout and denoise shortcut. The extracted
feature maps are fed into the multi-scale upsampling component to increase the resolution
gradually. We increase the resolution by a factor of 2 during each upsampling, resulting in
three parameter tuning interfaces. Using the three interfaces, we can use all the 2×, 4× and
8× ground-truth images to train the generator network, reducing the artifacts in the ﬁnal
8× super-resolution images greatly.

Fig. 5 illustrates the entire architecture of the generator model. The
input is time-series low-resolution images. We ﬁrst use a convolutional
layer with the ﬁlter size as 7 by 72, which is larger than the commonly
used ﬁlter,
to capture meaningful features of the input ﬂuorescence
microscope images. The Monte Carlo dropout layer, which dropouts
some pixels from the input feature maps during both training and testing,
is applied to the output of the ﬁrst layer to suppress noise. To further
alleviate the noise issue, we use another technique, the denoise shortcut. It
is similar to the identical shortcut in the residual network block. However,
instead of being exactly the same as the input, we set each channel of the
input feature map as the average of all the channels. The denoise shortcut
is added to the output of the convolutional layer, which is after 16 residual
blocks (Section S2), element-wise. After this feature map extraction
process, we use a pixel shuﬄe layer combined with the convolutional
layer to increase the dimensionality of the image gradually (upsampling
Conv X2 in Fig. 5).

Here we adopt a novel multi-scale tuning procedure to stabilize the 8×
images. As shown in Fig. 5, our generator can output and thus calculate
the training error of multi-scale super-resolution images, ranging from
2× to 8×, which means that our model has multiple training interfaces
for back propagation. Thus during training, we use the 2×, 4×, 8×
high-resolution ground-truth images to tune the model simultaneously
to ensure that the dimensionality of the images increases smoothly and
gradually without introducing too much fake detail.

For the discriminator network, we adopt the traditional convolutional
neural network, which contains eight convolutional layers, one residual
block and one sigmoid layer (Section S2). The convolutional layers
increase the number of channels gradually to 2048 and then decrease
it using 1 by 1 ﬁlters. Those convolutional layers are followed by a
residual block, which further increases the model ability of extracting

2 Here,
the ﬁlter size depends on the FWHM of a PSF. Generally,
a ﬂuorescence microscope produces low-resolution images with PSF
spanning 3 ∼ 7 pixels. The specially designed ﬁlter size can balance
between the computational time and physical meaning.

2.3.1 Basic concepts As shown in Fig. 3, a ﬂuorophore has three
states: emitting (light), not emitting and bleached. In classic Bayesian-
based time-series analysis, the switching procedure of ﬂuorophores is
modeled by Bayesian inference, i.e., given an observed region R, deciding

Deep learning guided Bayesian inference for super-resolution ﬂuorescence microscopy

5

3 EXPERIMENTAL RESULTS
3.1 Training deep learning

To train our deep learning module, the stochastic simulation module was
used to simulate time-series low-resolution images from 12000 gray-
scale high-resolution images. These images were downloaded from two
databases: (i) 4000 natural images were downloaded from ILSVRC
(Russakovsky et al., 2015) and Laplace ﬁltered, and (ii) 8000 sketches
were downloaded from the Sketchy Database (Sangkloy et al., 2016).
Note that our simulation is a generic method, which does not depend on
the type of the input images. Thus any gray-scale image can be interpreted
as the ﬂuorophore distribution and used to generate the corresponding
time-series low-resolution images.

To initialize all the weights of the deep learning models, we used the
random normal initializer with the mean as 0 and standard deviation as
0.02. As for the Monte Carlo dropout layer, we set the keep ratio as 0.8.
In terms of the Adam optimizer (Kingma and Ba, 2014), we followed
the setting in (Li et al., 2018; Dai et al., 2017) and set the learning rate
as 1 ∗ 10−4 and the beta 1, which is the exponential decay rate for the
ﬁrst moment estimates, as 0.9. During training, we set the batch size as
8, the initialization training epoch as 2 and the GAN training epoch as
40. When performing the real GAN training, we utilized the learning rate
decay technique, reducing the learning rate by half every 10 epochs.

3.2 Evaluation datasets

Two simulated datasets and three real-world datasets are used to evaluate
the performance of the proposed method. Simulated datasets are used due
to the availability of ground-truth.

The ﬁrst two datasets are simulated datasets, for which the ground-
truth (i.e., high-resolution images) is downloaded from the Single-
Molecule Localization Microscopy (SMLM) challenge3 (Sage et al.,
2015). The two datasets correspond to two structures: MT0.N1.HD
(abbr. MT) and Tubulin ConjAL647 (abbr. Tub). For each structure,
single molecule positions were downloaded and then transformed to
ﬂuorophore densities according to Section 2.1. For simulation, the photo-
convertible ﬂuorescent protein (PCFP) mEos3.2 (Zhang et al., 2012) and
its associated PSF, FWHM and state transfer table were used. For the
convenience of calculation, we cropped the large-ﬁeld structure into four
separate areas, each with 480 × 480 pixels (1px = 20nm). For each high-
resolution image, 200 frames of low-resolution ﬂuorescent images were
generated, each with 60 × 60 pixels.

The third dataset is a real-world dataset, which was used in recent
work (Xu et al., 2017). The actin was labeled with mEos3.24 in U2OS
cells (abbr. Actin1) and taken with an exposure time of 50 ms per image
frame. The actin network is highly dynamic and exhibits diﬀerent subtype
structures criss-crossing at various distances and angles, including stress
ﬁbers and bundles with diﬀerent sizes and diameters. The dataset has 200
frames of high-density ﬂuorescent images, each with 249 × 395 pixels
(1px = 160nm) in the green channel. This is a good benchmark set that
has been well tested which can compare our method with SIMBA (Xu
et al., 2017), a recent Bayesian approach based on dual-channel imaging
and photo-convertible ﬂuorescent proteins.

Two other real-world datasets labeled with mEos3.2 were also used.
One is an actin cytoskeleton network (abbr. Actin2), which is labeled and
taken under a similar exposure condition with Actin1, but is completely
new and has not been used by previous works. The other one is an
Endoplasmic reticulum structure (abbr. ER), which has a more complex
structure. It is a type of organelle that forms an interconnected network
of ﬂattened, membrane-enclosed sacs or tubes known as cisternae, which
exhibits diﬀerent circular-structures and connections at diﬀerent scales.

3 http://bigwww.epﬂ.ch/smlm/datasets/
4 For the convenience of cellular labeling and instrument setup, here all
the experiments were carried out by mEos3.2.

Fig. 6: The Bayesian inference model used for ﬂuorophore switching. (A) For one
ﬂuorophore, its transition between diﬀerent states can be modeled by a hidden Markov
model (HMM). (B) For high-density ﬂuorophores, the observed ﬂuorescence and their
underlying transitions can be modeled by a factorial hidden Markov model (FHMM).

whether there is a ﬂuorophore (F) or not (N) by

P(F|R)
P(N|R)

= P(R|F)P(F)
P(R|N)P(N)

,

(10)

where P(F) and P(N) are constants which are based on experimental
prior, P(R|F) is the probability of the observed data region R given the
location of the ﬂuorophore, P(R|N) is the probability of the observed data
region R if there is no ﬂuorophore, which can be calculated by integrating
all the probability of observing pixels given the noise model.

For a single ﬂuorophore, the switching procedure can be modeled by
a hidden Markov model (HMM) (Rabiner, 1989), as shown in Fig.6(A).
However, for high-density ﬂuorophores, each ﬂuorophore transfers the
state independently with a stable probability (Cox et al., 2012) and all
the ﬂuorophores together can be modeled by a factorial hidden Markov
model (FHMM) (Ghahramani and Jordan, 1996), as shown in Fig.6(B),
which has been used and proved in (Xu et al., 2017).

2.3.2 Reﬁning results with physical meaning Although HMM and
FHMM are capable of modeling the ﬂuorophore switching process,
they are localization-guided, which often ignore the global information,
and are computationally expensive to learn. Thus, we initialize the
ﬂuorophores’ locations by using the image generated by deep learning
and use Bayesian inference to further reﬁne the results.

We apply the FHMM model to deal with high-density ﬂuorescent
microscopy. The parameters of FHMM are estimated by the expectation-
maximization (EM) algorithm:

Q (cid:0)φnew|φ(cid:1) = E (cid:8)log P (cid:0){Ft, Dt} |φnew(cid:1) |φ, {Dt}(cid:9) ,

(11)

where the observation sequence has T frames, {Dt}, t = 1, ..., T . The
hidden states are {Ft}, where each ﬂuorophore has three possible states
in the model. Q is a function of the ﬂuorophore parameters φnew given
the current parameter estimation and the observation sequence {Dt}. The
procedure iterates between a step that ﬁxes the current parameters and
computes posterior probabilities over the hidden states (the E-step), and a
step that uses these probabilities to maximize the expected log likelihood
of the observations as a function of the parameters (the M-step).

In the E-step, we ﬁx the ﬂuorophore parameters in the model and
utilize the hybrid of Markov chain Monte Carlo and forward algorithm
to sample the initial model. When a new ﬂuorophore is determined, we
take samples of this ﬂuorophore using the forward ﬁltering backward
sampling algorithm (Godsill et al., 2004). Thus,
the sampled image
sequence contains this ﬂuorophore. In the M-step, we optimize the
ﬂuorophore parameters and ﬁnd the maximum a posteriori (MAP)
ﬂuorophore positions using the conjugate gradient. Then, based on
already known positions of ﬂuorophores, the surrounding ﬂuorophores
with high probability are expended. The ﬁnal super-resolution image is
obtained by iterating these two steps until convergence.

The detailed method description and parameter setting are given in

Section S3.

6

Li et al.

Fig. 7: Visualization of the ground-truth high-resolution images, representative low-resolution input images, the reconstruction results of the 3B analysis (Cox et al., 2012), and the results
of our method on three representative areas of each simulated dataset: MT (columns 1-3) and Tub (columns 4-6). The four rows show the ground-truth high-resolution images, the ﬁrst
frames of the simulated time-series low-resolution images, the reconstruction results of the 3B analysis, and the reconstruction results of DLBI, respectively.

For the ER dataset, the exposure time is 6.7 ms per frame. The resolution
of each image in Actin2 is 263 × 337 pixels (1px = 160nm) and that in
ER is 256 × 170 pixels (1px = 100nm). Both datasets have 200 frames of
high-density ﬂuorescent images and the same photographing parameters
as Actin1. These datasets were used to demonstrate the power of our
method in diverse ultrastructures. The detailed procedure for collecting
the real-world datasets is given in Section S4.

Since the 3B analysis (Cox et al., 2012) is one of the most widely used
high-density ﬂuorescent super-resolution techniques, which can deal with
high temporal and spatial resolutions (Lidke, 2012; Cox et al., 2012), it
was chosen to compare with our method.
3.3 Performance on simulated datasets

3.3.1 Visual performance Fig. 7 shows the visualization of the ground-
truth high-resolution images, representative low-resolution input images,
the reconstruction results of the 3B analysis, and the results of our method
on the simulated datasets. Due to the space limitation, we illustrate three
representative areas of each dataset and leave the fourth in Section S5.

As shown in Fig.7, the ground-truth images have very clear structures
while the low-resolution image frames are very blurry and noisy (8×
downsampled). To reconstruct the ultrastructures, we ran the 3B analysis
with 240 iterations and ran our Bayesian inference module after the
deep learning module with 60 iterations. In each iteration, the Bayesian
inference module of our method searches four neighbor points for each
ﬂuorophore, whereas the 3B analysis takes isolated estimation strategy.
Thus the diﬀerence in iteration numbers is comparable. Due to the high
computational expense of the 3B analysis, each 60 × 60 image was
subdivided into nine overlapped subareas for multi-core process, whereas
for our method, the entire image was processed by a single CPU core.

It is clear that the reconstructions of our method are very similar to
the ground-truth in terms of smoothness, continuity, and thickness. On
the other hand, the reconstructions of the 3B analysis consist of a number
of interrupted short lines and points with thin structures. In general, two
conclusions can be drawn from the visual inspection.

First, DLBI discovered much more natural structures than the 3B
analysis. For example, in the bottom part of Fig. 7(B), there are two
lines overlapping with each other and a bifurcation at the tail. Due to

the very low resolution in the input time-series images (e.g., Fig. 7(H)),
neither DLBI nor the 3B analysis was able to recover the overlapping
structure. However, DLBI reconstructed the proper thickness of that
structure (Fig. 7(T)), whereas the 3B analysis only recovered a very
thin line structure (Fig. 7(N)). Moreover, the bifurcation structure was
reconstructed naturally by DLBI. Similar conclusions can be drawn on
the more complex structures in the Tub dataset (columns 4-6 in Fig. 7).

Second, DLBI discovered much more latent structures than the 3B
analysis. The Tub dataset consists of a lot of lines (tubulins) with
diverse curvature degrees (Fig. 7(D),(E),(F)). The reconstructions of
the 3B analysis successfully revealed most of the tubulin structures but
left the crossing parts interrupted (Fig. 7(P),(Q),(R)). As a comparison,
the reconstruction results of DLBI recovered both the line-like tubulin
structures and most of the crossing parts accurately (Fig. 7(V),(W),(X)).

3.3.2 Quantitative performance For single-molecule super-resolution
ﬂuorescence microscopy, the quantitative performance has been measured
by assessing the localization accuracy of single-emitters in each frame
(Ram et al., 2006; Small, 2009; Huang et al., 2011). For high-density
super-resolution ﬂuorescence microscopy,
the entire time-series are
analyzed and the production is the probability map of the locations of
ﬂuorophores.

Table 1. Performance comparison between the 3B analysis and DLBI on the four areas
of the two simulated datasets in terms of peak signal-to-noise ratio (PSNR) and structural
similarity (SSIM). The best performance is shown in bold.

Datasets

PSNR
(dB)

SSIM

3B
DLBI
3B
DLBI

01

17.99
18.59

0.89
0.92

MT0.N1.HD
03
02

17.62
19.16

0.89
0.92

17.84
18.51

0.90
0.93

Tubulin ConjAL647

04

17.89
20.42

0.90
0.94

01

13.42
18.72

0.74
0.82

02

15.49
19.17

0.81
0.85

03

15.00
18.72

0.75
0.80

04

13.21
16.63

0.69
0.76

Since the ground-truth is known for the simulated datasets, here we
use peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) to
measure the reconstruction performance, both of which are widely-used
criteria for image reconstruction in computer vision. The performance
of the 3B analysis and DLBI on the two simulated datasets are given in
Table 1. Here, we denote the four areas (Section 3.2) of each dataset as

Deep learning guided Bayesian inference for super-resolution ﬂuorescence microscopy

7

“01”, “02”, “03” and “04”, respectively. It can be seen that DLBI clearly
outperforms the 3B analysis in terms of both PSNR and SSIM on all the
areas of the two datasets.
3.4 Performance on real datasets

Fig. 8 shows the ﬁrst frame of the time-series ﬂuorescent images for
each of the three real-world datasets. Here we evaluate the performance
of our method for both local-patch reconstruction (areas selected by
green rectangles) and large-ﬁeld reconstruction (areas selected by yellow
rectangles).

Fig. 8: The ﬁrst frame of the time-series ﬂuorescent images for each of the three real-world
datasets: (A) Actin1 (Xu et al., 2017), (B) Actin2, and (C) ER.

3.4.1 Local-patch reconstruction Fig. 9 shows the ﬁrst frames of the
low-resolution images of the three local-patches, and the reconstruction
results of the 3B analysis and DLBI. The regions of interests of the
selected patches are (60, 60, 60, 60), (120, 120, 60, 60) and (60, 60, 60, 60)
for the three datasets, respectively5. The temporal resolutions for the
two actin datasets and the ER dataset were 10s and 1.34s respectively,
according to the exposure time of the image frames.

It can be seen that the reconstruction results of the 3B analysis
capture the main structures in the ﬂuorescent images, but mainly consist
of isolated high-illuminating spots, with details being interrupted (Fig.
9(D),(E),(F)). In contrast,
the results of DLBI recover most of the
latent ultrastructures, and the reconstructed structures have well-estimated
ﬂuorophore distribution and continuous depiction (Fig. 9(G),(H),(I)).
Table 2. Performance comparison between the 3B analysis and DLBI on the real datasets
in terms of RSP and RSE with SQUIRREL. The higher RSP and lower RSE values, the
higher the image quality is. The best performance is shown in bold.

Dataset

Actin1-patch

Actin2-patch

ER-patch

Criteria

RSP

RSE

3B
DLBI

0.583
0.721

3915.096
3326.007

RSP

0.770
0.878

RSE

2196.068
1648.919

RSP

0.827
0.916

RSE

4077.037
2904.707

We further assessed the reconstruction quality of the 3B analysis
and DLBI by SQUIRREL (super-resolution quantitative image rating
and reporting of error locations) (Culley et al., 2018). SQUIRREL
compares the diﬀraction-limited image (the reference image) and the
reconstructed equivalents to generate a quantitative map, in which two
scores are calculated: the resolution-scaled Pearson coeﬃcient (RSP) and
the resolution-scaled error (RSE). The higher RSP and lower RSE values,
the higher the image quality is. Table 2 shows the RSP and RSE scores for
the 3B analysis and DLBI. It is clear that DLBI signiﬁcantly outperforms
the 3B analysis. More detailed comparisons are given in Section S5.3.

3.4.2 From deep learning to Bayesian inference Our method combines
inference, where deep
the strength of deep learning and statistical
learning captures both local features in the images and the time-course
correlation, and statistical inference removes artifacts from deep learning
and enhances physical meaning to the ﬁnal results. Conceptually, this

5 Region of interest is usually denoted as (X, Y, W, H), where (X, Y) are
the coordinates of the top left point of the rectangle, W is the width of the
rectangle, and H is the height of the rectangle.

Fig. 9: Reconstructions of the local patches of the three real datasets. First column: the
Actin1 dataset (the green box in Fig.8(A)). Second column: the Actin2 dataset (the green
box in Fig.8(B)). Third column: the ER dataset (the green box in Fig.8(C)). The ﬁrst row
shows the ﬁrst frames of the time-series low-resolution images. The second row shows the
reconstructions of the 3B analysis. The third row shows the reconstructions of DLBI. The
reconstructed images are 480 × 480 pixels and the local-patch images are 60 × 60 pixels.

is equivalent to using the power of deep learning to automatically and
systematically explore and extract spatial and temporal features, and
taking advantages of the explicit and rigorous mathematical foundation
of probabilistic graphical models. Here we investigate the eﬀectiveness of
this combination.

Fig. 10 demonstrates the outputs of the deep learning module and
the Bayesian inference module. It can be seen that the super-resolution
images outputted from the deep learning module are very close to the ﬁnal
images from the Bayesian inference module, except for some artifacts
and false structures. This is due to two reasons: (i) the abundance of
training data provided by our simulation module, which are simulated
under the real experimentally-calibrated parameters, enable deep learning
to eﬀectively learn spatial and temporal features; and (ii) the high diversity
of biological structures is still a challenge, which causes the artifacts and
false structures to be learned by deep learning.

Fig. 10: Reconstructions of the three local patches of the three real datasets (the ﬁrst
column) by the deep learning module (the second column) and by deep learning guided
Bayesian inference (the third column).

After the deep learning module generates the super-resolution image,
the Bayesian inference module uses both the original time-series low-
resolution images and the deep learning image to statistically infer a
“false/true” determination on each ﬂuorophore location and produce the
ﬁnal image. In particular, the false structures are not directly rejected but
used as seeds to search for true structures. Therefore, as shown in Fig. 10,
although the deep learning module outputted some unnatural structures

8

Li et al.

for the Actin2 and ER datasets, these structures were further corrected by
the Bayesian inference module.

3.4.3 Runtime analysis After being trained, running the deep learning
model is very computationally inexpensive. Furthermore,
the results
of deep learning provide a close-to-optimal initialization for Bayesian
inference, which also signiﬁcantly reduces trial-and-error and leads to
faster convergence. Fig. 11 shows the runtime comparison of the deep
learning module, the entire DLBI pipeline, and the 3B analysis on the nine
reconstruction tasks (i.e., the six areas of the simulated datasets shown in
Fig.7 and the three local patches of the real datasets shown in Fig.9). It
can be seen that the runtime for the deep learning module ranges between
1 to 3 minutes and that of DLBI ranges between 30 to 40 minutes. In
contrast, the runtime for the 3B analysis is around 75 hours, which is more
than 110 times higher than that for DLBI. Our results have demonstrated
that the super-resolution images from the deep learning module alone is a
good estimation to the ground-truth. Therefore, for users who value time
and can compromise accuracy, the results from the deep learning module
provide a good tradeoﬀ, and thus a good estimation of the ground-truth.

Fig. 11: Runtime comparison of the deep learning module (DNN), the entire DLBI pipeline
(DLBI), and the 3B analysis (3B) on the nine reconstruction tasks (i.e., the six areas of the
simulated datasets shown in Fig.7 and the three local patches of the real datasets shown
in Fig.9). The runtime was measured on a Fedora 25 system with 128 Gb memory and
E5-2667v4 (3.2 GHz) CPU.

3.4.4 Large-ﬁeld reconstruction To analyze a dataset with 200 frames,
each with about 200 × 300 pixels, it takes our method about 7 ∼ 10 hours
on a single CPU core. Therefore, our method is able to achieve large-ﬁeld
reconstruction on the yellow areas shown in Fig. 8. Fig. 12 shows the
large-ﬁeld reconstruction images of the three real datasets. For the Actin1
dataset, the selected area is 200 × 300 pixels and the reconstructed super-
resolution image is 1600×2400 pixels. For the Actin2 dataset, the selected
area is 250×240 pixels and the reconstructed image is 2000×1920 pixels.
And for the ER dataset, the selected area is 200 × 150 pixels and the
reconstructed image is 1600 × 1200 pixels.

As shown in Fig. 12(A) and (B), the actin networks in the two
datasets have been successfully recovered by DLBI. The thinning and
thickening trends of the cytoskeleton have been clearly depicted, as well
as the small latent structures, including actin ﬁlaments, actin bundles and
ruﬄes. For the endoplasmic reticulum structure (Fig.12(C)), the circular-
structures and connections of the cytoskeleton have also been accurately
reconstructed.

For the Actin1 dataset,

the single-molecule reconstruction of the
red channel is available (Fig. 12(D)). This reconstruction was produced
by PALM (Hess et al., 2006) using 20,000 frames, whereas the
reconstruction image of DLBI (Fig. 12(A)) used only 200 frames. We
further overlayed the image produced by DLBI with that of PALM to
check how well they overlap (Fig. 12(E)). It is clear that the main
structures of the two images almost perfectly agree with each other. In
addition, our method was able to recover the latent structure on the top-left
part which was not photographed by PALM due to out of range of views
in dual-channel photographing. If we carefully check the original low-
resolution ﬂuorescent images, we could ﬁnd that this predicted structure
indeed exists, which is consistent with our reconstruction.

4 DISCUSSION AND CONCLUSION

In this paper, we proposed a deep learning guided Bayesian inference
method for structure reconstruction of super-resolution ﬂuorescence
microscopy. Our method combines the strength of deep learning and
statistical inference. We further overcame the high data requirement
bottleneck of deep learning by a novel stochastic simulation module
based on the experimentally-calibrated parameters and problem-speciﬁc
physical models. It should be noted that although our simulation provides
close-to-realistic data to train the deep learning module, it still contains
bias and unrealistic parts, which will be learned by the deep learning
module. Bayesian inference, on the other hand, can correct and reﬁne
the ultrastructure learned by deep learning, and thus enhance the physical
meaning of the ﬁnal super-resolution image.

We have comprehensively evaluated the quality of the reconstructed
super-resolution images. The future work includes evaluating how well
the framework can rediscover the parameters used in the simulation
module.

ACKNOWLEDGEMENTS

This work was supported by the Kind Abdullah Unviersity of Science and
Technology (KAUST) Oﬃce of Sponsored Research (OSR) under Awards
No. FCC/1/1976-04, URF/1/2602-01, URF/1/3007-01, URF/1/3412-
01 and URF/1/3450-01, the National Key Reaseach and Development
the National natural Science
Program of China (2017YFA0504702),
Foundation of China (Grant No. U1611263, U1611261, 61232001,
61472397, 61502455, 61672493).

REFERENCES

A Schwentker, M., Bock, H., Hofmann, M., Jakobs, S., Bewersdorf, J., Eggeling,
C., and Hell, S. W. (2007). Wide-ﬁeld subdiﬀraction resolft microscopy using
ﬂuorescent protein photoswitching. Microscopy research and technique, 70(3),
269–280.

Cox, S., Rosten, E., Monypenny, J., Jovanovic-Talisman, T., Burnette, D. T.,
Lippincott-Schwartz, J., Jones, G. E., and Heintzmann, R. (2012). Bayesian
localization microscopy reveals nanoscale podosome dynamics. Nat. methods,
9(2), 195–200.

Culley, S., Albrecht, D., Jacobs, C., Pereira, P. M., Leterrier, C., Mercer,
J., and Henriques, R. (2018). Quantitative mapping and minimization of
super-resolution optical imaging artifacts. Nat. methods.

Dai, H., Umarov, R., Kuwahara, H., Li, Y., Song, L., and Gao, X. (2017).
Sequence2vec: a novel embedding approach for modeling transcription factor
binding aﬃnity landscape. Bioinformatics (Oxford, England), 33, 3575–3583.
Dong, H., Supratak, A., Mai, L., Liu, F., Oehmichen, A., Yu, S., and Guo, Y.
(2017). Tensorlayer: A versatile library for eﬃcient deep learning development.
In Proceedings of the 2017 ACM on Multimedia Conference, pages 1201–1204.
ACM.

Gal, Y. and Ghahramani, Z. (2015). Dropout as a Bayesian Approximation:

Representing Model Uncertainty in Deep Learning. arXiv.

Ghahramani, Z. and Jordan, M. I. (1996). Factorial hidden markov models.

In

Advances in Neural Information Processing Systems, pages 472–478.

Godsill, S. J., Doucet, A., and West, M. (2004). Monte carlo smoothing for nonlinear
time series. Journal of the american statistical association, 99(465), 156–168.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances
in neural information processing systems, pages 2672–2680.

Gustafsson, M. G. (2005). Nonlinear structured-illumination microscopy: wide-ﬁeld
ﬂuorescence imaging with theoretically unlimited resolution. P. Natl. Acad. Sci.
USA, 102(37), 13081–13086.

He, K. M., Zhang, X. Y., Ren, S. Q., and Sun, J. (2016). Deep residual learning
for image recognition. 2016 Ieee Conference on Computer Vision and Pattern
Recognition (Cpvr), pages 770–778.

Hein, B., Willig, K. I., and Hell, S. W. (2008). Stimulated emission depletion
(sted) nanoscopy of a ﬂuorescent protein-labeled organelle inside a living cell.
Proceedings of the National Academy of Sciences, 105(38), 14271–14276.

Hess, S. T., Girirajan, T. P., and Mason, M. D. (2006). Ultra-high resolution imaging
by ﬂuorescence photoactivation localization microscopy. Biophys. J., 91(11),
4258–4272.

Holden, S. J., Uphoﬀ, S., and Kapanidis, A. N. (2011). Daostorm: an algorithm for

high-density super-resolution microscopy. Nat. methods, 8(4), 279–280.

Huang, F., Schwartz, S. L., Byars, J. M., and Lidke, K. A. (2011). Simultaneous
multiple-emitter ﬁtting for single molecule super-resolution imaging. Biomed.

Deep learning guided Bayesian inference for super-resolution ﬂuorescence microscopy

9

Fig. 12: Large-ﬁeld reconstructions of the three real datasets: (A) Actin1, (B) Actin2, and (C) ER. (D) The single-molecule reconstruction of the Actin1 dataset by PALM based on 20,000
frames. (E) Overlap of the reconstruction images by DLBI (in red) and PALM (in green).

Opt. Express, 2(5), 1377–1393.

Acad. Sci. USA, 103(12), 4457–4462.

Johnson, J., Alahi, A., and Fei-Fei, L. (2016). Perceptual losses for real-time
style transfer and super-resolution. In European Conference on Computer Vision,
pages 694–711. Springer.

Kim, J., Kwon Lee, J., and Mu Lee, K. (2016). Accurate image super-resolution
using very deep convolutional networks. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 1646–1654.

Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980.

Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta, A.,
Aitken, A., Tejani, A., Totz, J., Wang, Z., et al. (2016). Photo-realistic single
image super-resolution using a generative adversarial network. arXiv preprint
arXiv:1609.04802.

Li, Y., Wang, S., Umarov, R., Xie, B., Fan, M., Li, L., and Gao, X. (2018). Deepre:
sequence-based enzyme ec number prediction by deep learning. Bioinformatics,
34(5), 760–769.
Lidke, K. A. (2012).

Super resolution for common probes and common

microscopes. Nat. methods, 9(2), 139.

Lim, B., Son, S., Kim, H., Nah, S., and Lee, K. M. (2017). Enhanced deep
In The IEEE Conference

residual networks for single image super-resolution.
on Computer Vision and Pattern Recognition (CVPR) Workshops, volume 2.

Lippincott-Schwartz,

J. and Manley, S.
ﬂuorescence microscopy to work. Nat. methods, 6(1), 21–23.

(2009).

Putting super-resolution

Quan, T., Zhu, H., Liu, X., Liu, Y., Ding, J., Zeng, S., and Huang, Z.-L. (2011).
High-density localization of active molecules using structured sparse model and
bayesian information criterion. Opt. express, 19(18), 16963–16974.

Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications

in speech recognition. Proc. IEEE, 77(2), 257–286.

Ram, S., Ward, E. S., and Ober, R. J. (2006). Beyond rayleigh’s criterion: a
resolution measure with application to single-molecule microscopy. P. Natl.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015).
Int. J. Comput. Vision,
ImageNet Large Scale Visual Recognition Challenge.
115(3), 211–252.

Rust, M. J., Bates, M., and Zhuang, X. (2006). Sub-diﬀraction-limit imaging by
stochastic optical reconstruction microscopy (storm). Nat. methods, 3(10), 793–
796.

Sage, D., Kirshner, H., Pengo, T., Stuurman, N., Min, J., Manley, S., and Unser,
M. (2015). Quantitative evaluation of software packages for single-molecule
localization microscopy. Nat. methods, 12(8), 717–724.

Salimans, T., Goodfellow, I. J., Zaremba, W., Cheung, V., Radford, A., and Chen,
X. (2016). Improved techniques for training gans. CoRR, abs/1606.03498.
Sangkloy, P., Burnell, N., Ham, C., and Hays, J. (2016). The sketchy database:

learning to retrieve badly drawn bunnies. ACM T. Graphic., 35(4), 119.

Simonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for

large-scale image recognition. arXiv preprint arXiv:1409.1556.

Small, A. R. (2009). Theoretical limits on errors and acquisition rates in localizing

switchable ﬂuorophores. Biophys. J., 96(2), L16–L18.

Xu, F., Zhang, M., Liu, Z., Xu, P., and Zhang, F. (2015). Bayesian localization
microscopy based on intensity distribution of ﬂuorophores. Protein & cell, 6(3),
211–220.

Xu, F., Zhang, M., He, W., Han, R., Xue, F., Liu, Z., Zhang, F., Lippincott-Schwartz,
J., and Xu, P. (2017). Live cell single molecule-guided bayesian localization
super resolution microscopy. Cell Res., 27(5), 713.

Zhang, M., Chang, H., Zhang, Y., Yu, J., Wu, L., Ji, W., Chen, J., Liu, B., Lu, J., Liu,
Y., et al. (2012). Rational design of true monomeric and bright photoactivatable
ﬂuorescent proteins. Nat. methods, 9(7), 727–729.

Zhu, L., Zhang, W., Elnatan, D., and Huang, B. (2012). Faster storm using

compressed sensing. Nat. methods, 9(7), 721–723.

