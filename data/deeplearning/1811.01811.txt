Active Deep Learning Attacks under Strict Rate
Limitations for Online API Calls

Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, and Jason H. Li
Intelligent Automation, Inc.
Rockville, MD 20855, USA
Email:{yshi, ysagduyu, kdavaslioglu, jli}@i-a-i.com

8
1
0
2

v
o
N
5

]

G
L
.
s
c
[

1
v
1
1
8
1
0
.
1
1
8
1
:
v
i
X
r
a

Abstract—Machine learning has been applied to a broad
range of applications and some of them are available online as
application programming interfaces (APIs) with either free (trial)
or paid subscriptions. In this paper, we study adversarial machine
learning in the form of back-box attacks on online classiﬁer
APIs. We start with a deep learning based exploratory (inference)
attack, which aims to build a classiﬁer that can provide similar
classiﬁcation results (labels) as the target classiﬁer. To minimize
the difference between the labels returned by the inferred
classiﬁer and the target classiﬁer, we show that the deep learning
based exploratory attack requires a large number of labeled
training data samples. These labels can be collected by calling
the online API, but usually there is some strict rate limitation
on the number of allowed API calls. To mitigate the impact of
limited training data, we develop an active learning approach
that ﬁrst builds a classiﬁer based on a small number of API
calls and uses this classiﬁer to select samples to further collect
their labels. Then, a new classiﬁer is built using more training
data samples. This updating process can be repeated multiple
times. We show that this active learning approach can build an
adversarial classiﬁer with a small statistical difference from the
target classiﬁer using only a limited number of training data
samples. We further consider evasion and causative (poisoning)
attacks based on the inferred classiﬁer that is built by the
exploratory attack. Evasion attack determines samples that the
target classiﬁer is likely to misclassify, whereas causative attack
provides erroneous training data samples to reduce the reliability
of the re-trained classiﬁer. The success of these attacks show that
adversarial machine learning emerges as a feasible threat in the
realistic case with limited training data.

Index Terms—Adversarial machine learning, deep learning,
active learning, exploratory attack, evasion attack, causative
attack.

I. INTRODUCTION

As a subﬁeld of artiﬁcial intelligence (AI), machine learn-
ing has been used in many applications including cyber
security, intelligence analysis, Internet of Things (IoT), cyber-
physical systems, autonomous driving, and traditional or com-
puter games. Machine learning provides the automated means
to learn with data. Recently, signiﬁcant progress has been
made with deep neural networks (deep learning) supported by
advances in hardware and computational capabilities, making
machine learning much more effective than ever before. Some
of the machine learning systems are made available to public
or paid subscribers via an application programming interface
(API), e.g., Google’s Vision API provides image content
analysis via REST API [1]. A user can call such an API for

the speciﬁed data and obtain the returned results, which may
be further analyzed for different subsequent tasks.

The online service paradigm, although convenient, makes
machine learning vulnerable to various attacks. Machine learn-
ing applications may involve sensitive and/or proprietary infor-
mation that includes training data, machine learning algorithm,
hyperparameters, and functionality of underlying tasks. By
making machine learning applications online, in addition to
traditional attacks (e.g., DoS attack), these applications are
also becoming subject to various new exploits and attacks
(such as stealing the application functionality, trained model,
and training data).

The ﬁeld of adversarial machine learning has emerged to
understand the machine learning behavior in the presence
of an adversary. One example is from the image recogni-
tion application, where the adversary generates an image by
slightly perturbing real images to fool a state-of-the-art image
classiﬁer [2]. As a consequence, a perturbed panda image is
recognized as a gibbon by machine learning, while the original
image is recognized as a panda by a human. Adversarial
machine learning is still a new research ﬁeld and the security
limitations and vulnerabilities of machine learning are not
well understood yet. With the increasing use of machine
learning in critical commercial and government applications,
the security implications ranging from intellectual property
protection to national security raise the need to identify effects
of adversarial machine learning and provide the foundation for
attack mitigation techniques.

There have been efforts to develop various attacks based
on adversarial machine learning. Exploratory (or inference)
attacks [3]–[12] aim to understand how the underlying ma-
chine learning algorithm works for an application, and conse-
quently infer any sensitive and/or proprietary information. An
exploratory attack can be launched under different assumptions
made on the attacker’s knowledge. For instance, a black-
box attack can be launched without any prior knowledge on
the machine learning algorithm and the training data [4]–
[7]. As shown in [4]–[7], the adversary can call the target
classiﬁer T with a large number of samples, collect the labels
on these samples, and then use this data to train a deep
learning classiﬁer ˆT as an estimate of the target classiﬁer
T . This attack implicitly steals the underlying training data,
the inner-workings of the machine learning algorithm, and its
hyperparameter selection that constitute the core of intellectual

 
 
 
 
 
 
property (which has been often built with long-time research
and development investment). Moreover, once ˆT is inferred,
further attacks such as evasion attacks or causative (poisoning)
attacks can be launched [13].

training data,

One major assumption of exploratory attacks is that an
adversary can collect training data from the target machine
learning application. Supervised machine learning relies on
the quality and the quantity of training data samples. Without
sufﬁcient
it may not be possible to train a
successful machine learning algorithm [14]. However, machine
learning APIs, in general, have strict rate limitations on how
many API calls can be made in a certain period of time
(ranging from per second to per day). In addition, a user who
makes too frequent API calls may be identiﬁed as a potential
adversary and may be blocked later. Hence, we consider the
practical case that imposes strict rate limitations on adversarial
machine learning. In this paper, we select a real online machine
learning API for text analysis [15] as the target classiﬁer T .
The adversary treats T as a black box, since the underlying
machine learning algorithm, parameters, and training data are
all unknown.

First, we consider exploratory attacks. We show that when
the number of API calls is limited, the adversary cannot infer
the target classiﬁer T with acceptable accuracy, even when
it launches a deep learning based exploratory attack. Thus,
we design an approach based on active learning [16], [17]
to enhance the training process of the exploratory attack. The
adversary ﬁrst builds a classiﬁer ˆT based on a small number
of samples and collects their labels. Using active learning,
it further selects additional samples using ˆT and sends only
samples with the low classiﬁcation conﬁdence by ˆT to the
target classiﬁer T to be labeled. These samples are used
as additional training data to improve the inferred classiﬁer
ˆT . This updating process can be repeated multiple times.
The system model is shown in Figure 1. We show that the
proposed method signiﬁcantly improves the performance of
the inferred classiﬁer ˆT that is measured as the statistical
difference of classiﬁcation results,
labels returned by
the original classiﬁer T and the inferred classiﬁer ˆT . This
approach provides insights into situational awareness, attack
modeling, detection, and mitigation in different applications
of cyberspace [18]–[20].

i.e.,

Once the exploratory attack is successful, the adversary can
launch further attacks such as evasion and causative attacks
by using the inferred classiﬁer ˆT . The evasion attack [5],
[6], [21]–[23] aims to provide the target classiﬁer with some
test data that will likely result in incorrect labels (e.g., a
spam email ﬁlter is fooled into accepting spam emails as
legitimate). The adversary runs some samples through the
inferred classiﬁer ˆT based on a deep neural network and
further examines their classiﬁcation scores provided by the
classiﬁer. If these scores (likelihoods of labels returned by the
deep neural network) are within the decision region for label
j and are close to the decision region for another label i,
these sample are selected such that the target classiﬁer T is
very likely to misclassify data samples from label i as label

Fig. 1. Adversarial deep learning with limited training.

j. Such samples can be used to select the test data samples in
the form of evasion attacks.

On the other hand, the causative attack [16], [24]–[27] aims
to provide the target classiﬁer with incorrect information as the
additional training data samples to reduce the reliability of the
re-trained classiﬁer. For this purpose, the adversary sends some
data samples to its inferred classiﬁer ˆT . If the deep learning
scores are far away from the decision boundary, the adversary
changes their labels and sends these mislabeled samples as
additional training data to the target classiﬁer T . The target
classiﬁer that is re-trained this way is expected to perform
signiﬁcantly worse than before.

The problem of limited training data in adversarial machine
learning has been studied in [6], where the adversary ﬁrst
infers the target classiﬁer with limited real training data, and
then generates synthetic data samples from real data in an
evasion attack by adding adversarial perturbations that are
gradually improved by the derivative and checking labels
queried from the target classiﬁer. The difference is that we
do not use synthetic data in the evasion attack and our goal is
to improve the inferred classiﬁer (not the adversarial samples)
that can be used later to select an arbitrary number of samples
for evasion or causative attacks without further querying the
target classiﬁer.

The rest of the paper is organized as follows. Section II
studies the exploratory attack assuming that a large number of
API calls is allowed and then presents an active learning based
approach to launch the exploratory attack under limitations on
API calls. Section III investigates the subsequent evasion and
causative attacks. Section IV concludes the paper.

II. EXPLORATORY ATTACK AND TRAINING DATA
REFINEMENT WITH ACTIVE LEARNING

A. Exploratory Attack on an Online Classiﬁer

Many machine learning services are made available online
as APIs. Users can use these services by making API calls and
integrate their API outputs with their own applications. Such
applications include, but not limited to, topic classiﬁcation,
sentiment analysis, subjectivity analysis, keyword extraction,
language detection, and spam detection. APIs usually provide
free subscriptions that allow a user to call them only few times

Machine Learning Classifier under AttackLabelsBuild Training DatasetTest DataUntil producing labels similar to the classifier under attack Train a Deep Learning ClassifierActive LearningFunctionally Equivalent ClassifierAdding totest datain a certain time frame. For example, DatumBox provides a
number of machine learning APIs for text analysis [15]. A
user can specify the input text data, call a DatumBox API to
analyze this text, and obtain results such as classiﬁcation of
the input text as subjective or objective.

This paradigm raises potential security issues for online
services, since an adversary can also access the input and
output of these services. In particular, the adversary can per-
form a black-box exploratory attack, where the adversary does
not have any knowledge on the target classiﬁer T , including
the training data, training algorithm, and machine learning
parameters. Such an attack can be launched as follows:

1) The adversary calls the online API of the target classiﬁer
T multiple times using a set S of samples and for each
sample s ∈ S, the adversary collects the label T (s)
returned by the online API.

2) By using S and the collected labels, the adversary trains
a deep neural network to build a classiﬁer ˆT , as an
estimation of T .

In this paper, we consider a black-box exploratory attack,
i.e., the adversary only has a set S of samples to call the online
API and the returned labels. The target classiﬁer T has been
trained before. The adversary does not know the training data
or the algorithm (e.g., Naive Bayes, Support Vector Machine
(SVM), or a more sophisticated neural network algorithm)
used to build this classiﬁer. The data is divided into training
data and test data. Using the training data,
the adversary
applies deep learning (based on a deep neural network) to
infer T . Deep learning refers using training data to train a
deep neural network (namely, ﬁnding the weights in the neural
network) for computing labels. In particular, we consider a
feedforward neural network (FNN) shown in Figure. 2. We
use the backpropagation algorithm to train the deep neural
network. A neural network consists of simple elements called
neurons and weighted connections, a.k.a. synapses, among
these neurons. A neuron j performs a basic computation over
its input synapses wji from each neuron i that is connected
to j, and outputs a single scalar value yj, which can be inter-
preted as its activation or ﬁring rate. In a feedforward neural
network (FNN) architecture, neurons are arranged in layers
and synapses are connecting neurons in one layer to neurons
in the next layer. The activations of neurons in the input layer
are set externally while the activations of the hidden layer
neurons and output layer neurons are computed as speciﬁed
above. In particular, the activations of the output layer neurons
represent the result of the network’s computation.

We consider a real online classiﬁer API, namely the subjec-
tivity analysis API of DatumBox [15], as the target classiﬁer T .
The adversary calls this API with a number of text samples and
collects the returned labels, which may be label 1 (subjective)
or label 2 (objective). The free subscription allows 1000 calls
per day, i.e., labels for 1000 samples can be collected per day.
For the numerical results presented in this paper, the adver-
sary collects labels for 10000 data samples over time. Each
data sample is the text of a tweet that has been collected from
the public Twitter API. In this section, the adversary uses half

Input
layer

...

Hidden
layer 1

. . .

Hidden
layer N

Output
layer

wji

j

...

i

...

...

Fig. 2. Feedforward neural network.

of the data samples as training data to train a classiﬁer ˆT and
the other half as test data to evaluate its performance d( ˆT , T )
of ˆT .

Deep learning requires representing each sample as a set of
features. We use top word distribution to build such a set of
features as follows. The adversary ﬁrst obtains a list of words
W = (w1, w2, · · · ) sorted by their frequencies of occurrence
in text, i.e., p1 ≥ p2 ≥ · · · where pi is the frequency of
occurrence for word wi. Then the features for a sample of
text is the set of numbers of occurrence oi for each word wi.
Thus, 2000 features are obtained by considering distributions
of top 2000 words. The adversary uses these features to train
the deep learning classiﬁer ˆT .

The adversary builds a deep neural network (an FNN) as
the classiﬁer ˆT . To optimize the hyperparameters of the deep
neural network, the following two measures are computed.
The difference d1( ˆT , T ) between ˆT and T on label 1 is the
number of samples with ˆT (s) = 2 and T (s) = 1 divided by
the number of samples with label 1 in test data. We can deﬁne
d2( ˆT , T ) similarly. Then, we aim to minimize dmax( ˆT , T ) =
max{d1( ˆT , T ), d2( ˆT , T )} for test data to balance the effect
on both labels.

We implement this process by using the Microsoft Cognitive
Toolkit (CNTK) [28] in a Python code to train the FNN with
the optimal hyperparameters. The hyperparatemers of the deep
neural network are optimized as follows.

• The number of hidden layers is 2.
• The number of neurons per layer is 60.
• The loss function (that is used to measure the difference
of the labels returned by the deep neural network from
the ground truth labels) is cross entropy.

• The activation function in hidden layers is sigmoid.
• The activation function in output layer is softmax.
• Weights and biases are not initially scaled.
• Input values are unit normalized in the ﬁrst training pass.
• The minibatch size is 5.
• The momentum coefﬁcient to update the gradient is 0.9.
• The number of epochs per time slot is 8.

The difference between labels returned by T and ˆT for test

data is found as

d1( ˆT , T ) = 12.82%, d2( ˆT , T ) = 12.88%.

Thus, we obtain

dmax( ˆT , T ) = 12.88%.

However, the above attack requires to collect labels for
many samples, which needs to be done over multiple days
(due to the rate limit of 1000 samples per day). Next, we
will consider the case when the adversary can perform the
exploratory attack with a small number of calls.

B. Active Learning with Limited Training Data

Our approach to mitigate the effect of limited training on the
adversary is based on active learning, which can jointly and
iteratively build the inferred classiﬁer ˆT using limited training
data. Suppose that
the adversary starts with 200 samples
and their labels obtained from the target classiﬁer T (see
Figure 3(a)). Data is split in 100 training samples (to build
the classiﬁer ˆT ) and 100 test samples.

Again, the adversary builds the optimal ˆT by selecting deep
learning hyperparameters to minimize the difference between
ˆT and T . These hyperparameters are given by:

• The number of hidden layers is 2.
• The number of neurons per layer is 10.
• The loss function is squared error.
• The activation function in hidden layers is sigmoid.
• The activation function in output layer is softmax.
• All weights and biases are initially scaled by 0.5.
• Input values are unit normalized in the ﬁrst training pass.
• The minibatch size is 25.
• The momentum coefﬁcient to update the gradient is 0.9.
• The number of epochs per time slot is 10.
Note that

this deep neural network is smaller than the
previous one because fewer training data samples are used
to build this second neural network. The difference between
labels returned by T and ˆT is found as

d1( ˆT , T ) = 33.78%,

d2( ˆT , T ) = 30.77%.

Thus, we obtain

dmax( ˆT , T ) = 33.78%.

The adversary then collects more labels from T to improve
ˆT . In the benchmark approach, randomly selected samples are
sent to T and the returned labels are collected and added to
the training data in order to improve ˆT .

We ﬁrst take a closer look at the inferred classiﬁer ˆT . For
each sample s, ˆT ﬁrst determines a classiﬁcation likelihood
score ˆS(s) and then compares it with a threshold S ˆT , which
is optimized to minimize dmax( ˆT , T ). The optimal S ˆT is
found as 0.17. If ˆS(s) < S ˆT , sample s is classiﬁed as
label 1, otherwise sample s is classiﬁed as label 2. The
difference between score ˆS(s) and S ˆT captures the conﬁdence
of classiﬁcation. If ˆS(s) is close to S ˆT , then its is likely that

(a) Step 1.

(b) Step 2.

Fig. 3. Exploratory attack with active learning.

such a classiﬁcation is wrong. On the other hand, if ˆS(s) is
far away from S ˆT , then it is unlikely that such a classiﬁcation
is wrong.

Based on this understanding of ˆS(s), an active learning

approach is designed as follows (see Figure 3(b)).

1) The adversary sends randomly selected text samples
s to its inferred classiﬁer ˆT and obtains classiﬁcation
likelihood scores ˆS(s) from the inferred classiﬁer.
2) If classiﬁcation likelihood score ˆS(s) is close to thresh-
old S ˆT , the adversary sends this text sample to classiﬁer
T and obtain its label T (s). The adversary adds text
sample s and its label T (s) to the training data.

3) If additional training data is sufﬁcient, the adversary

updates ˆT . Else, the adversary goes to Step 1.

Using the above steps, the adversary generates 500 (1000, or
2000) additional text queries in Step 1 of the training process
and obtains the scores of the corresponding samples from the
inferred classiﬁer ˆT in Step 2. Based on these scores, 224
(579, or 1007) text samples close to S ˆT are selected by active
learning to call classiﬁer T . We refer to these samples as
the actively learned samples. Using the scores returned by
T , the inferred ˆT is updated. In the benchmark scheme, we
consider that only randomly selected text sample is sent to
the original classiﬁer. Active learning performs better than
the benchmark because it calls T with data samples that are
uncertain with respect to the original inferred classiﬁer ˆT ,
while the benchmark is not able to adapt its decision boundary
to the uncertainty of such samples. In both active learning and
benchmark scheme, the adversary uses the original 100 test
samples to evaluate the performance.

The results are presented in Table I. The total number
of training data samples is set the same for active learn-
ing and benchmark scheme. With 224 additional (actively
learned or randomly selected) training data samples, active
learning performs better than the benchmark scheme and

Inferred Classifier ෡𝑻Target  Classifier 𝑻SamplesLabels^^Inferred Classifier ෡𝑻New SamplesAdditional selected samplesUpdated Inferred ෡𝑻Target  Classifier 𝑻ScoresLabelsPrevious samples and labelsTABLE I
PERFORMANCE (DIFFERENCE BETWEEN LABELS RETURNED BY T AND ˆT ) WITH ACTIVE LEARNING.

Training data size

Initial Samples Additional Samples

100
100
100

224
579
1007

Total Samples
324
679
1107

Active learning

Benchmark

d1( ˆT , T )
28.38%
24.32%
18.92%

d2( ˆT , T )
26.92%
23.08%
19.23%

d1( ˆT , T )
31.08%
29.73%
31.08%

d2( ˆT , T )
30.77%
30.77%
30.77%

achieves dmax( ˆT , T ) = 28.38%, whereas the benchmark
scheme achieves dmax( ˆT , T ) = 31.08%. This performance
gain of active learning further increases with more (579 or
1007) additional training data samples. For example, with
1107 total samples, active learning reduces dmax( ˆT , T ) to
19.23% (which corresponds to 38.13% improvement over the
benchmark scheme). Thus, active learning always achieves
better performance than the benchmark scheme. Moreover,
with more additional training data samples, the advantage
of active learning grows, which shows the importance of
identifying and selecting uncertain samples in performance
improvement.

III. EVASION ATTACKS AND CAUSATIVE ATTACKS

When the adversary completes a successful exploratory
attack and infers a classiﬁer ˆT that is similar to the target
classiﬁer T , it can then try to learn the behavior of T by
using ˆT and launch subsequent attacks. In this section, we
demonstrate two such attacks, namely, the evasion attack and
causative attack.

A. Evasion Attack

The evasion attack aims to ﬁnd test samples for which labels
returned by T are likely wrong. Although T is unknown by
the adversary, the inferred classiﬁer ˆT can used to predict
which samples may be incorrectly classiﬁed. There may be
different attack objectives and approaches. In one example,
the objective is to maximize the average error. To achieve
this objective, the adversary selects samples with classiﬁcation
scores that are close to the threshold (i.e., samples with low
conﬁdence on classiﬁcation). In another example, the objective
is to maximize the error of misclassifying a label 1 sample
as label 2. To achieve this objective, the adversary selects
samples classiﬁed with label 2 and with score that is close
to the threshold. The objective of maximizing the error of
misclassifying a label 2 sample as label 1 can be achieved
similarly.

We need the ground truth (i.e., the correct label for samples
in test data) to evaluate the effectiveness of such attacks. Since
it is assumed that the adversary has no knowledge of ground
truth, we omit the performance results for evasion attack.

B. Causative Attack

To reduce classiﬁcation errors, some classiﬁers are updated
(re-trained) by user feedback. That
is, a user can review
the returned labels by T and manually ﬁx labels on some
samples or agree on the returned labels. If such ﬂags are

received, the sample with user deﬁned labels can be used
as additional training data to re-train the classiﬁer. However,
such a process also introduces a new type of attack, namely
causative attack, where the adversary provides wrong labels
such that the updated classiﬁer ˜T becomes less reliable in
terms of classiﬁcation accuracy.

There are different strategies to provide wrong labels. In the
extreme case, the adversary may provide wrong labels for all
data samples to maximize the impact of its attack. However,
it is easy to detect such an attack and block the adversary,
i.e., its feedback will not be considered in the re-training
process. Thus, the adversary aims to achieve a signiﬁcant
impact of its attack by using a small number of wrong labels.
Suppose that the adversary can change labels for only p% of
all samples, where p is a small number. Then, the problem is
to determine how to select the best set of data samples and
provide wrong labels on this set of data samples. Clearly, this
selection requires the knowledge of T , which can be obtained
by analyzing ˆT after a successful exploratory attack.

We again consider the classiﬁcation scores provided by the
deep learning based classiﬁer ˆT . If the adversary changes the
label for a sample with a score that is far away from the
threshold (i.e., with high classiﬁcation conﬁdence), this new
label will be very likely a wrong label and will make the
updated classiﬁer ˜T worse. On the other hand, if the adversary
changes the label for a sample with a score that is close to the
threshold (i.e., with low classiﬁcation conﬁdence), this new
label may not be a wrong label and will not make the updated
classiﬁer ˜T worse. Thus, the adversary performs a causative
attack as follows.

1) The adversary sends some samples to ˆT and receives
their scores and labels. Note that ˆT is the classiﬁer
inferred by the adversary and thus there is no rate
limitation, i.e., the adversary can collect a large number
of samples with scores and labels.

2) The adversary selects samples with top p

2 % scores and
2 % scores. These scores are far

samples with bottom p
away from the threshold.

3) The adversary switches labels of selected samples and

sends these labels as user feedback.

To measure the impact of a causative attack, we compare the
outputs of the original classiﬁer T and the updated classiﬁer ˜T .
For samples with label 1 (by class T ), suppose that the number
of these sample is n1, while T and ˜T provide different labels
on m1 samples. Then, we deﬁne the difference on samples
with label 1 as d1(T, ˜T ) = m1
. Similarly, deﬁne the difference
n1

on samples with label 2 as d2(T, ˜T ) = m2
and the average of
n2
the difference on all samples as d(T, ˜T ) = m1+m2
, where n2
n1+n2
is the number of samples with label 2 and m2 is the number
of different labels by T and ˜T on these n2 samples.

We use the classiﬁer ˆT built in Section II with 1107 training
data samples, we apply it on another set of 1000 samples and
perform a causative attack with p = 10. Then, we obtain

d1(T, ˜T ) = 39.92%, d2(T, ˜T ) = 55.64%.

Thus, the average difference on all samples is

d(T, ˜T ) = 48.00%.

As a result, the causative attack built upon the exploratory
attack reduces the reliability of the updated classiﬁer ˜T sig-
niﬁcantly.

IV. CONCLUSION

We studied the exploratory attack to infer a target online
classiﬁer, when the number of calls to the online API is
limited. Typically, adversarial deep learning requires a large
number of training samples. Under strict rate limitations on
API calls, we showed that there is a signiﬁcant difference
between the labels returned by the inferred classiﬁer and the
target classiﬁer. To mitigate this effect, we designed an active
learning process for adversarial deep learning, which ﬁrst
builds a classiﬁer based on limited training data and then uses
this classiﬁer to selectively make more API calls. We showed
that this attack can reliably infer the target classiﬁer even with
a limited number of API calls. By analyzing the classiﬁer
inferred by a successful exploratory attack, the adversary can
launch further attacks. In particular, we designed evasion and
causative attacks using the inferred classiﬁer to select test
and training data samples, respectively. The evasion attack
identiﬁes test data that the target classiﬁer cannot reliably
classify, while the causative attack effectively poisons the re-
training process such that the reliability of updated classiﬁer
degraded. We showed that
these attacks are feasible with
limited training data and adversarial machine learning emerges
as a practical threat to online classiﬁer APIs.

REFERENCES

[1] Cloud Vision API, available at https://cloud.google.com/vision.
[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572. 2014.

[3] M. Barreno, B. Nelson, R. Sears, A. Joseph, and J. Tygar, “Can Machine
Learning be Secure?” ACM Symposium on Information, Computer and
Communications Security, Taipei, Taiwan, Mar. 2006.

[4] F. Tramer, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart, “Stealing
Machine Learning Models via Prediction APIs,” USENIX Security
Symposium, Austin, TX, Aug. 2016.

[5] Y. Shi, Y. E. Sagduyu, K. Davaslioglu, and R. Levy, “Vulnerability
Detection and Analysis in Adversarial Deep Learning,” in Guide to
Vulnerability Analysis for Computer Networks and Systems: An Arti-
ﬁcial Intelligence Approach, S. Parkinson, A. Crampton, R. Hill, Eds.
Springer, 2018, pp. 235–258.

[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A.
Swami, “Practical Black-Box Attacks Against Deep Learning Systems
Using Adversarial Examples,” ACM Conference on Computer and
Communications Security (CCS), Dallas, TX, Oct.-Nov. 2017.

[7] Y. Shi, Y. E. Sagduyu, and A. Grushin, “How to Steal a Machine Learn-
ing Classiﬁer with Deep Learning,” IEEE Symposium on Technologies
for Homeland Security (HST), Boston, MA, Apr. 2017.

[8] G. Ateniese, L. Mancini, A. Spognardi, A. Villani, D. Vitali, and G.
Felici, “Hacking Smart Machines with Smarter Ones: How to Extract
Meaningful Data from Machine Learning Classiﬁers,” International
Journal of Security and Networks, 10(3):137-150, 2015.

[9] M. Fredrikson, S. Jha, and T. Ristenpart, “Model Inversion Attacks
that Exploit Conﬁdence Information and Basic Countermeasures,” ACM
SIGSAC Conference on Computer and Communications Security, Den-
ver, CO, Oct. 2015.

[10] F. Tramer, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
“The Space of Transferable Adversarial Examples,” ArXiv e-prints
arXiv:1704.03453, 2017.

[11] Y. Shi, Y. E Sagduyu, T. Erpek, K. Davaslioglu, Z. Lu, and J. Li, “Ad-
versarial deep learning for cognitive radio security: Jamming attack and
defense strategies,” IEEE International Conference on Communications
(ICC) Workshop on Promises and Challenges of Machine Learning in
Communication Networks, Kansas City, MO, May 2018.

[12] T. Erpek, Y. E. Sagduyu, and Y. Shi, “Deep learning for launching and
mitigating wireless jamming attacks,” arXiv preprint arXiv:1807.02567,
2018.

[13] Y. Shi and Y. E Sagduyu, “Evasion and Causative Attacks with Ad-
versarial Deep Learning,” IEEE Military Communications Conference
(MILCOM), Baltimore, MD, Oct. 2017.

[14] K. Davaslioglu and Y. E. Sagduyu, “Generative adversarial learning for
spectrum sensing,” IEEE International Conference on Communications
(ICC), Kansas City, MO, May 2018.

[15] DatumBox Machine Learning API, available at http://www.datumbox.

com/machine-learning-api/

[16] L. Pi, Z. Lu, Y. Sagduyu, and S. Chen, “Defending Active Learning
against Adversarial Inputs in Automated Document Classiﬁcation,”
IEEE Global Conference on Signal and Information Processing (Glob-
alSIP), Washington, D.C, Dec. 2016.

[17] B. Settles, “Active Learning,” in Synthesis Lectures on Artiﬁcial Intel-
ligence and Machine Learning, Morgan & Claypool Publishers, 2012,
vol. 6, pp. 1-114.

[18] O. Savas, Y. E. Sagduyu, J. Deng, and J. Li, “Tactical Big Data
Analytics: Challenges, Use Cases, and Solutions,” ACM SIGMETRICS
Performance Evaluation Review, vol. 41, no. 4, pp. 86–89, Apt. 2014.
[19] Z. El Jamous, S. Soltani, Y. E. Sagduyu, and J. H. Li, “RADAR:
An Automated System for Near Real-Time Detection and Diversion
of Malicious Network Trafﬁc,” IEEE Symposium on Technologies for
Homeland Security (HST), Bosto, MA, May 2016.

[20] Y. Cheng, Y. E. Sagduyu, J. Deng, J. Li, and P. Liu, “Integrated Situa-
tional Awareness for Cyber-attack Detection, Analysis, and Mitigation,”
SPIE Defense, Security and Sensing Conference, Baltimore, MD, Apr.
2012.

[21] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G.
Giacinto, and F. Roli, “Evasion Attacks Against Machine Learning at
Test Time,” European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML-PKDD),
Prague, Czech Republic, Sept. 2013.

[22] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial Examples in the

Physical World,” arXiv preprint arXiv:1607.02533, 2016.

[23] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and A.
Swami, “The Limitations of Deep Learning in Adversarial Settings,”
IEEE European Symposium on Security and Privacy, Saarbrucken,
Germany, Mar. 2016.

[24] M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan and N. K. Jha,
“Systematic Poisoning Attacks on and Defenses for Machine Learning
in Healthcare,” IEEE Journal of Biomedical and Health Informatics, vol.
19, no. 6, pp. 1893–1905, Nov. 2015.

[25] B. Biggio, B. Nelson and P. Laskov, ”Poisoning Attacks against Support
Vector Machines,” in International Conference on Machine Learning
(ICML), Edinburgh, Scotland, June-July 2012.

[26] S. Alfeld, X. Zhu and P. Barford, “Data Poisoning Attacks against
Autoregressive Models,” in AAAI Conference on Artiﬁcial Intelligence,
Phoenix, AZ, Feb. 2016.

[27] Y. Shi, T. Erpek, Y. E. Sagduyu, and J. Li, ”Spectrum Data Poison-
ing with Adversarial Deep Learning,” IEEE Military Communications
Conference (MILCOM), Los Angeles, CA, Oct. 2018.

[28] Microsoft Cognitive Toolkit (CNTK), available at https://docs.microsoft.

com/en-us/cognitive-toolkit.

