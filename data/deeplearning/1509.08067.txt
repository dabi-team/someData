6
1
0
2

p
e
S
3

]

V
C
.
s
c
[

6
v
7
6
0
8
0
.
9
0
5
1
:
v
i
X
r
a

ARXIV VERSION

1

Online Object Tracking, Learning and Parsing
with And-Or Graphs

Tianfu Wu, Yang Lu and Song-Chun Zhu

Abstract—This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown
objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated
in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes
on-the-ﬂy. During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.g., lighting and
partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar
objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative
examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn
the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame.
In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the
TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] — VOT 2013, 2014, 2015 and TIR2015 (thermal imagery
tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep
convolutional network [5], [6]. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the
state-of-the-art methods in VOT2014, 2015 and TIR2015.

Index Terms—Visual Tracking, And-Or Graphs, Latent SVM, Dynamic Programming, Intrackability

(cid:70)

1 INTRODUCTION

1.1 Motivation and Objective

O NLINE object tracking is an innate capability in human and

animal vision for learning visual concepts [7], and is an
important task in computer vision. Given the state of an unknown
object (e.g., its bounding box) in the ﬁrst frame of a video, the
task is to infer hidden states of the object in subsequent frames.
Online object tracking, especially long-term tracking, is a difﬁcult
problem. It needs to handle variations of a tracked object, includ-
ing appearance and structural variations, scale changes, occlusions
(partial or complete), etc. It also needs to tackle complexity of the
scene, including camera motion, background clutter, distractors,
illumination changes, frame cropping, etc. Fig. 1 illustrates some
typical issues in online object tracking. In recent literature, object
tracking has received much attention due to practical applica-
tions in video surveillance, activity and event prediction, human-
computer interactions and trafﬁc monitoring.

This paper presents an integrated framework for online track-
ing, learning and parsing (TLP) of unknown objects with a uniﬁed
representation. We focus on settings in which object state is
represented by bounding box, without using pre-trained models.
We address ﬁve issues associated with online object tracking in

•

•

•

T.F. Wu is with the Department of Electrical and Computer Engineering
and the Visual Narrative Cluster, North Carolina State University. This
work was mainly done when T.F. Wu was research assistant professor at
UCLA.
E-mail: tianfu wu@ncsu.edu
Y. Lu is with the Department of Statistics, University of California, Los
Angeles.
E-mail: yanglv@ucla.edu
S.-C. Zhu is with the Department of Statistics and Computer Science,
University of California, Los Angeles.
E-mail: sczhu@stat.ucla.edu

Manuscript received MM DD, YYYY; revised MM DD, YYYY.

Fig. 1: Illustration of some typical issues in online object tracking
using the “skating1” video in the benchmark [2]. Starting from
the object speciﬁed in the ﬁrst frame, a tracker needs to handle
many variations in subsequent frames which include illuminative
variation, scale variation, occlusion, deformation, fast motion, in-
plane and out-of-plane rotation, background clutter, etc.

the following.

Issue I: Expressive representation accounting for structural
and appearance variations of unknown objects in tracking. We are
interested in hierarchical and compositional object models. Such
models have shown promising performance in object detection [1],
[8], [9], [10], [11] and object recognition [12]. A popular modeling
scheme represents object categories by mixtures of deformable
part-based models (DPMs) [1]. The number of mixture compo-
nents is usually predeﬁned and the part conﬁguration of each
component is ﬁxed after initialization or directly based on strong
supervision. In online tracking, since a tracker can only access the
ground-truth object state in the ﬁrst frame, it is not suitable for it
to “make decisions” on the number of mixture components and
part conﬁgurations, and it does not have enough data to learn. It’s
desirable to have an object representation which has expressive
power to represent a large number of part conﬁgurations, and

Input 
 
 
 
 
 
ARXIV VERSION

2

Fig. 2: Overview of our AOGTracker.
(a) Illustration of the tracking, learn-
ing and parsing (TLP) framework.
It consists of four components. (b)
Examples of capturing structural and
appearance variations of a tracked
object by a series of object conﬁg-
urations inferred on-the-ﬂy over key
frames #1, #173, #282, etc. (c) Illus-
tration of an object AOG, a parse tree
and an object conﬁguration in frame
#282. A parse tree is an instantia-
tion of an AOG. A conﬁguration is
a layout of latent parts represented
by terminal-nodes in a parse tree. An
object AOG preserves ambiguities by
capturing multiple parse trees.

can facilitate computationally effective inference and learning.
We quantize the space of part conﬁgurations recursively in a
principled way with a hierarchical and compositional And-Or
graph (AOG) representation [8], [11]. We learn and update the
most discriminative part conﬁgurations online by pruning the
quantized space based on part discriminability.

Issue II: Computing joint optimal solutions. Online object
tracking is usually posed as a maximum a posterior (MAP)
problem using ﬁrst order hidden Markov models (HMMs) [2],
[13], [14]. The likelihood or observation density is temporally in-
homogeneous due to online updating of object models. Typically,
the objective is to infer the most likely hidden state of a tracked
object in a frame by maximizing a Bayesian marginal posterior
probability given all the data observed so far. The maximization
is based on either particle ﬁltering [15] or dense sampling such
as the tracking-by-detection methods
[16], [17], [18]. In most
prior approaches (e.g., the 29 trackers evaluated in the TB-100
benchmark [2]), no feedback inspection is applied to the history
of inferred trajectory. We utilize tracking-by-parsing with hierar-
chical models in inference. By computing joint optimal solutions,
we can not only improve prediction accuracy in a new frame by
integrating past estimated trajectory, but also potentially correct
errors in past estimated trajectory. Furthermore, we simultaneously
address another key issue in online learning (Issue III).

Issue III: Maintaining the purity of a training dataset. The
dataset consists of a set of positive examples computed based
on the current trajectory, and a set of negative examples mined
from outside the current trajectory. In the dataset, we can only
guarantee that the positives and the negatives in the ﬁrst frame are
true positives and true negatives respectively. A tracker needs to
carefully choose frames from which it can learn to avoid model
drifting (i.e., self-paced learning). Most prior approaches do not
address this issue since they focus on marginally optimal solutions
with which object models are updated, except for the P-N learning
in TLD [17] and the self-paced learning for tracking [18]. Since
we compute joint optimal solutions in online tracking, we can
maintain the purity of an online collected training dataset in a
better way.

Issue IV: Failure-aware online learning of object models. In
online learning, we mostly update model parameters incrementally
after inference in a frame. Theoretically speaking, after an initial

object model
is learned in the ﬁrst frame, model drifting is
inevitable in general setting. Thus, in addition to maintaining the
purity of a training dataset, it is also important that we can identify
critical moments (caused by different structural and appearance
variations) automatically. At those moments, a tracker needs to
re-learn both the structure and the parameters of object model
using the current whole training dataset. We address this issue by
computing uncertainty of an object model in a frame based on its
response maps.

Issue V: Computational efﬁciency by dynamic search strategy.
Most tracking-by-detection methods run detection in the whole
frame since they usually use relatively simple models such as
a single object template. With hierarchical models in tracking
and sophisticated online inference and updating strategies, the
computational complexity is high. To speed up tracking, we
need to utilize a dynamic search strategy. This strategy must
take into account the trade-off between generating a conservative
proposal state space for efﬁciency and allowing an exhaustive
search for accuracy (e.g., to handle the situation where the object
is completely occluded for a while or moves out the camera view
and then reappears). We address this issue by adopting a simple
search cascade with which we run detection in the whole frame
only when local search has failed.

Our TLP method obtains state-of-the-art performance on one
popular tracking benchmark [2]. We give a brief overview of our
method in the next subsection.

1.2 Method Overview

As illustrated in Fig.2 (a), the TLP method consists of four
components. We introduce them brieﬂy as follows.

(1) An AOG quantizing the space of part conﬁgurations. Given
the bounding box of an object in the ﬁrst frame, we assume object
parts are also of rectangular shapes. We ﬁrst divide it evenly
into a small cell-based grid (e.g., 3 × 3) and a cell deﬁnes the
smallest part. We then enumerate all possible parts with different
aspect ratios and different sizes which can be placed inside the
grid. All the enumerated parts are organized into a hierarchical
and compositional AOG. Each part is represented by a terminal-
node. Two types of nonterminal nodes as compositional rules:
an And-node represents the decomposition of a large part into
two smaller ones, and an Or-node represents alternative ways

AOG282(1) And-Or Graph(2) Learning(3) Parsing(4) TrackingAOG1tParsingAOG282(a)(b)Parse treeConﬁgurationAnd-nodeOr-nodeTerminal-nodeARXIV VERSION

3

of decompositions through different horizontal or vertical binary
splits. We call it the full structure AOG1. It is capable of
exploring a large number of latent part conﬁgurations (see some
examples in Fig. 2 (b)), meanwhile it makes the problem of online
model learning feasible.

(2) Learning object AOGs. An object AOG is a subgraph
learned from the full structure AOG (see Fig. 2 (c) 2). Learning an
object AOG consists of two steps: (i) The initial object AOG are
learned by pruning branches of Or-nodes in the full structure AOG
based on discriminative power, following breadth-ﬁrst search
(BFS) order. The discriminative power of a node is measured
based on its training error rate. We keep multiple branches for
each encountered Or-node to preserve ambiguities, whose training
error rates are not bigger than the minimum one by a small positive
value. (ii) We retrain the initial object AOG using latent SVM
(LSVM) as it was done in learning the DPMs [1]. LSVM utilizes
positive re-labeling (i.e., inferring the best conﬁguration for each
positive example) and hard negative mining. To further control
the model complexity, we prune the initial object AOG through
majority voting of latent assignments in positive re-labeling.

(3) A spatial dynamic programming (DP) algorithm for com-
puting all the proposals in a frame with the current object AOG.
Thanks to the DAG structure of the object AOG, a DP parsing
algorithm is utilized to compute the matching scores and the
optimal parse trees of all sliding windows inside the search region
in a frame. A parse tree is an instantiation of the object AOG which
selects the best child for each encountered Or-node according to
matching score. A conﬁguration is obtained by collapsing a parse
tree onto the image domain, capturing layout of latent parts of a
tracked object in a frame.

(4) A temporal DP algorithm for inferring the most likely
trajectory. We maintain a DP table memorizing the candidate
object states computed by the spatial DP in the past frames.
Then, based on the ﬁrst-order HMM assumption, a temporal
DP algorithm is used to ﬁnd the optimal solution for the past
frames jointly with pair-wise motion constraints (i.e., the Viterbi
path [14]). The joint solution can help correct potential tracking
errors (i.e., false negatives and false positives collected online)
by leveraging more spatial and temporal information. This is
similar in spirit to methods of keeping N-best maximal decoder
for part models [19] and maintaining diverse M-best solutions in
MRF [20].

2 RELATED WORK

In the literature of object tracking, either single object tracking or
multiple-object tracking, there are often two settings.

Ofﬂine visual tracking [21], [22], [23], [24]. These methods
assume the whole video sequence has been recorded, and consist
of two steps. i) It ﬁrst computes object proposals in all frames
using some pre-trained detectors (e.g., the DPMs [1]) and then
form “tracklets” in consecutive frames. ii) It seeks the optimal
object trajectory (or trajectories for multiple objects) by solving an
optimization problem (e.g., the K-shortest path or min-cost ﬂow
formulation) for the data association. Most work assumed ﬁrst-
order HMMs in the formulation. Recently, Hong and Han [25]

1. By full structure, it means all the possible compositions on top of the grid

with binary composition being used for And-nodes

2. We note that there are some Or-nodes in the object AOGs which have
only one child node since they are subgraphs of the full structure AOG and we
keep their original structures.

proposed an ofﬂine single object tracking method by sampling
tree-structured graphical models which exploit the underlying
intrinsic structure of input video in an orderless tracking [26].

Online visual tracking for streaming videos. It starts tracking
after the state of an object
is speciﬁed in certain frame. In
the literature, particle ﬁltering [15] has been widely adopted,
which approximately represents the posterior probability in a non-
parametric form by maintaining a set of particles (i.e., weighted
candidates). In practice, particle ﬁltering does not perform well
in high-dimensional state spaces. More recently,
tracking-by-
detection methods [16], [17] have become popular which learn and
update object models online and encode the posterior probability
using dense sampling through sliding-window based detection on-
the-ﬂy. Thus, object tracking is treated as instance-based object
detection. To leverage the recent advance in object detection,
object
tracking research has made progress by incorporating
discriminatively trained part-based models [1], [8], [27] (or more
generally grammar models [9], [10], [11]). Most popular methods
also assume ﬁrst-order HMMs except for the recently proposed
online graph-based tracker [28]. There are four streams in the
literature of online visual tracking:

i) Appearance modeling of the whole object, such as incremen-
tal learning [29], kernel-based [30], particle ﬁltering [15],
sparse coding [31] and 3D-DCT representation [32]; More
recently, Convolutional neural networks are utilized in im-
proving tracking performance [5], [6], [33], which are usually
pre-trained on some large scale image datasets such as the
ImageNet [34] or on video sequences in a benchmark with
the testing one excluded.

ii) Appearance modeling of objects with parts, such as patch-
based [35], coupled 2-layer models [36] and adaptive sparse
appearance [37]. The major limitation of appearance mod-
eling of a tracked object is the lack of background models,
especially in preventing model drift from distracotrs (e.g.,
players in sport games). Addressing this issue leads to dis-
criminant tracking.

iii) Tracking by discrimination using a single classiﬁer, such as
support vector tracking [38], multiple instance learning [39],
STRUCK [40], circulant structure-based kernel method [41],
and discriminant saliency based tracking [42];

iv) Tracking by part-based discriminative models, such as online
extensions of DPMs [43], and structure preserving tracking
method [27], [44].

Our method belongs to the fourth stream of online visual
tracking. Unlike predeﬁned or ﬁxed part conﬁgurations with star-
model structure used in previous work, our method learns both
structure and appearance of object AOGs online, which is, to our
knowledge, the ﬁrst method to address the problem of online ex-
plicit structure learning in tracking. The advantage of introducing
AOG representation are three-fold.

i) More representational power: Unlike TLD [17] and many
other methods (e.g., [18]) which model an object as a single
template or a mixture of templates and thus do not perform
well in tracking objects with large structural and appearance
variations, an AOG represents an object in a hierarchical and
compositional graph expressing a large number of latent part
conﬁgurations.

ii) More robust tracking and online learning strategies: While
the whole object has large variations or might be partially
occluded from time to time during tracking, some other parts

ARXIV VERSION

4

remain stable and are less likely to be occluded. Some of the
parts can be learned to robustly track the object, which can
also improve accuracy of appearance adaptation of terminal-
nodes. This idea is similar in spirit to ﬁnding good features
to track objects [45], and we ﬁnd good part conﬁgurations
online for both tracking and learning.

iii) Fine-grained tracking results:

In addition to predicting
bounding boxes of a tracked object, outputs of our AOG-
Tracker (i.e., the parse trees) have more information which
are potentially useful for other modules beyond tracking such
as activity or event prediction.
Our preliminary work has been published in [46] and the
method for constructing full structure AOG was published in
[8]. This paper extends them by: (i) adding more experimental
results with state-of-the-art performance obtained and full source
code released; (ii) elaborating details substantially in deriving the
formulation of inference and learning algorithms; and (iii) adding
more analyses on different aspects of our method. This paper
makes three contributions to the online object tracking problem:

i) It presents a tracking-learning-parsing (TLP) framework

which can learn and track objects AOGs.

ii) It presents a spatial and a temporal DP algorithms for
tracking-by-parsing with AOGs and outputs ﬁne-grained
tracking results using parse trees.

iii) It outperforms the state-of-the-art tracking methods in a re-
cent public benchmark, TB-100 [2], and obtains comparable
performance on a series of VOT benchmarks [4].

Paper Organization. The remainder of this paper is organized as
follows. Section 3 presents the formulation of our TLP framework
under the Bayesian framework. Section 4 gives the details of
spatial-temporal DP algorithm. Section 5 presents the online
learning algorithm using the latent SVM method. Section 6 shows
the experimental results and analyses. Section 7 concludes this
paper and discusses issues and future work.

3 PROBLEM FORMULATION

3.1 Formulation of Online Object Tracking

In this section, we ﬁrst derive a generic formulation from gener-
ative perspective in the Bayesian framework, and then derive the
discriminative counterpart.

3.1.1 Tracking with HMM
Let Λ denote the image lattice on which video frames are deﬁned.
Denote a sequence of video frames within time range [1, T ] by,

I1:T = {I1, · · · , IT }.

(1)

Denote by Bt the bounding box of a target object in It. In online
object tracking, B1 is given and Bt’s are inferred by a tracker
(t ∈ [2, T ]). With ﬁrst-order HMM, we have,

The prior model:

The motion model:

The likelihood:

B1 ∼ p(B1) ,
Bt|Bt−1 ∼ p(Bt|Bt−1) ,
It|Bt ∼ p(It|Bt).

(2)

(3)

(4)

Then, the prediction model is deﬁned by,

(8)

(9)

where ΩBt−1 is the candidate space of Bt−1, and the updating
model is deﬁned by,

p(Bt|I1:t) = p(It|Bt)p(Bt|I1:t−1)/p(It|I1:t−1),

(6)

which is a marginal posterior probability. The tracking result, the
best bounding box B∗

t , is computed by,

B∗

t = arg max
Bt∈ΩBt

p(Bt|I1:t),

(7)

which is usually solved using particle ﬁltering [15] in practice.

To allow feedback inspection of the history of a trajectory, we

seek to maximize a joint posterior probability,

p(B1:t|I1:t) = p(B1:t−1|I1:t−1)

p(Bt|Bt−1)p(It|Bt)
p(It|I1:t−1)
p(Bi|Bi−1)p(Ii|Bi)
p(Ii|I1:i−1)

.

= p(B1|I1)

t
(cid:89)

i=2

By taking the logarithm of both sides of Eqn.(8), we have,

B∗

1:t = arg max
B1:t
= arg max
B1:t
t
(cid:88)

log p(B1:t|I1:t)

{log p(B1) + log p(I1|B1)+

[log p(Bi|Bi−1) + log p(Ii|Bi)]}.

i=2
where the image data term p(I1) and (cid:80)t
i=2 p(Ii|I1:i−1) are not
included in the maximization as they are treated as constant terms.
Since we have ground-truth for B1, p(I1|B1) can also be
treated as known after the object model is learned based on B1.
Then, Eqn.(9) can be reproduced as,

B∗

2:t = arg max
B2:t

log p(B2:t|I1:t, B1)

(10)

= arg max
B2:t

t
(cid:88)
{

i=2

[log p(Bi|Bi−1) + log p(Ii|Bi)]}.

3.1.2 Tracking as Energy Minimization over Trajectories

To derive the discriminative formulation of Eqn.(10), we show that
only the log-likelihood ratio matters in computing log p(Ii|Bi) in
Eqn.(10) with very mild assumptions.

Let ΛBi be the image domain occupied by a tracked object,
and ΛBi
the remaining domain (i.e., ΛBi ∪ ΛBi = Λ and
ΛBi ∩ ΛBi = ∅) in a frame Ii. With the independence assumption
between IΛBi and IΛBi
given Bi, we have,

p(Ii|Bi) = p(IΛBi

, IΛBi

|Bi) = p(IΛBi

= p(IΛBi

|Bi)q(IΛBi

) = q(IΛ)

|Bi)

|Bi)p(IΛBi
p(IΛBi
|Bi)
)

q(IΛBi

,

(11)

) = p(IΛBi

where q(IΛ) is the probability model of background scene and
|Bi) w.r.t. context-free assumption.
we have q(IΛBi
So, q(IΛ) does not need to be speciﬁed explicitly and can be
omitted in the maximization. This derivation gives an alternative
explanation for discriminant tracking v.s. tracking by generative
appearance modeling of an object [47].

p(Bt|I1:t−1) =

(cid:90)

ΩBt−1

p(Bt|Bt−1)p(Bt−1|I1:t−1)dBt−1,

Based on Eqn.(10), we deﬁne an energy function by,

(5)

E(B2:t|I1:t, B0) ∝ − log p(B2:t|I1:t, B1).

(12)

ARXIV VERSION

5

Fig. 3: We assume parts are of rectangular shapes. (a) shows a
conﬁguration with 3 parts. Two different, yet equivalent, decom-
position rules in representing a conﬁguration are shown in (b) for
decomposition with branching factor equal to the number of parts
(i.e., a ﬂat structure), and in (c) for a hierarchical decomposition
with branching factor being set to 2 at all levels.

And, we do not compute log p(Ii|Bi) in the probabilistic way,
instead we compute matching score deﬁned by,

Score(Ii|Bi) = log

p(IΛBi

q(IΛBi

|Bi)
)

(13)

= log p(Ii|Bi) − log q(IΛ).

which we can apply discriminative learning methods.

Also, denote the motion cost by,

Cost(Bi|Bi−1) = − log p(Bi|Bi−1).

(14)

We use a thresholded motion model in experiments: the cost is 0
if the transition is accepted based on the median ﬂow [17] (which
is a forward-backward extension of the Lucas-Kanade optimal
ﬂow [48]) and +∞ otherwise. A similar method was explored
in [18].

So, we can re-write Eqn.(10) in the minimization form,

B∗

2:t = arg min
B2:t

E(B2:t|I1:t, B1)

(15)

= arg min
B2:t

{

t
(cid:88)

i=2

[Cost(Bi|Bi−1) − Score(Ii|Bi)]}.

In our TLP framework, we compute Score(Ii|Bi) in Eqn.( 15)
with an object AOG. So, we interpret a sliding window by the
optimal parse tree inferred from object AOG. We treat parts as
latent variables which are modeled to leverage more information
for inferring object bounding box. We note that we do not track
parts explicitly in this paper.

3.2 Quantizing the Space of Part Conﬁgurations

In this section, we ﬁrst present the construction of a full structure
AOG which quantizes the space of part conﬁgurations. We then
introduce notations in deﬁning an AOG.

Part conﬁgurations. For an input bounding box, a part con-
ﬁguration is deﬁned by a partition with different number of parts
of different shapes (see Fig. 3 (a)). Two natural questions arise:
(i) How many part conﬁgurations (i.e., the space) can be deﬁned
in a bounding box? (ii) How to organize them into a compact
representation? Without posing some structural constraints, it is a
combinatorial problem.

We assume rectangular shapes are used for parts. Then, a
conﬁguration can be treated as a tiling of input bounding box
using either horizontal or vertical cuts. We utilize binary splitting

Fig. 4: Illustration of (a) the dictionary of part types, and (b) part
instances generated by placing a part type in a grid. Given part
instances, (c) shows how a sub-grid is decomposed in different
ways. We allow overlap between child nodes (see (c.3)).

rule only in decomposition (see Fig. 3 (b) and (c)). With these
two constraints, we represent all possible part conﬁgurations by a
hierarchical and compositional AOG constructed in the following.
Given a bounding box, we ﬁrst divide it evenly into a cell-
based grid (e.g., 9 × 10 grid in the right of Fig. 4). Then, in
the grid, we deﬁne a dictionary of part types and enumerate all
instances for all part types.

A dictionary of part types. A part type is deﬁned by its width
and height. Starting from some minimal size (such as 2 × 2 cells),
we enumerate all possible part types with different aspect ratios
and sizes which ﬁt the grid (see A, B, C, D in Fig.4 (a)).

Part instances. An instance of a part type is obtained by
placing the part type at a position. Thus, a part instance is deﬁned
by a “sliding window” in the grid. Fig.4 (b) shows an example of
placing part type D (2 × 5 cells) in a 9 × 10 grid with 48 instances
in total.

To represent part conﬁgurations compactly, we exploit the

compositional relationships between enumerated part instances.

The full structure AOG. For any sub-grid indexed by the
left-top position, width and height (e.g., (2, 3, 5, 2) in the right-
middle of Fig.4 (c)), we can either terminate it directly to the
corresponding part instance (Fig.4 (c.1)), or decompose it into
two smaller sub-grids using either horizontal or vertical binary
splits. Depending on the side length, we may have multiple valid
splits along both directions (Fig.4 (c.2)). When splitting either
side we allow overlaps between the two sub-grids up to some
ratio (Fig.4 (c.3)). Then, we represent the sub-grid as an Or-node,
which has a set of child nodes including a terminal-node (i.e.
the part instance directly terminated from it), and a number of
And-nodes (each of which represents a valid decomposition). This
procedure is applied recursively for all child sub-grids. Starting

123(a)(b)(c)ABCD…......……….........(a) Dictionary of Part TypesD1(c) DecompositionDAAD48BBwidthheightB(c.2) w/o overlap(c.3) with overlap(b) Part Instances(c.1) terminateoverlapBA: 2×2B: 2×3...(2, 3, 5, 2)ARXIV VERSION

6

Fig. 5: Illustration of full structure And-Or Graph (AOG) representing the space of part conﬁgurations. It is of directed acyclic graph
(DAG) structure. For clarity, we show a toy example constructed for a 3 × 3 grid. The AOG can generate all possible part conﬁgurations
(the number is often huge for typical grid sizes, see Table.1), while allowing efﬁcient exploration with a DP algorithm due to the DAG
structure. See text for details. (Best viewed in color and with magniﬁcation)

from the whole grid and using BFS order, we construct a full
structure AOG, all summarized in Algorithm 1 (see Fig. 5 for an
example). Table. 1 lists the number of part conﬁgurations for three
cases from which we can see that full structure AOGs cover a
large number of part conﬁgurations using a relatively small set of
part instances.

Input: Image grid Λ with W × H cells; Minimal size of a
part type (w0, h0); Maximal overlap ratio r between
two sub-grids.

Output: The And-Or graph G =< V, E > (see Fig.5)
Initialization: Create an Or-node OΛ for the grid Λ,
V = {OΛ}, E = ∅, BFSqueue= {OΛ};
while BFSqueue is not empty do

Pop a node v from the BFSqueue;
if v is an Or-node then

i) Add a terminal-node t (i.e. the part instance)
V = V ∪ {t}, E = E ∪ {< v, t >};
ii) Create And-nodes Ai for all valid cuts;
E = E ∪ {< v, Ai >};
if Ai /∈ V then

V = V ∪ {Ai};
Push Ai to the back of BFSqueue;

end

else if v is an And-node then

Create two Or-nodes Oi for the two sub-grids;
E = E ∪ {< v, Oi >};
if Oi /∈ V then

V = V ∪ {Oi};
Push Oi to the back of BFSqueue;

end

end

end
Algorithm 1: Constructing the grid AOG using BFS

We denote an AOG by,

G = (VAnd, VOr, VT , E, Θ)

(16)

where VAnd, VOr and VT represent a set of And-nodes, Or-nodes
and terminal-nodes respectively, E a set of edges and Θ a set of

Grid
3 × 3
5 × 5
10 × 12

primitive part #Conﬁguration #T-node #And-node
1 × 1
1 × 1
2 × 2

319
76,879,359
3.8936e+009

48
600
5209

35
224
1409

TABLE 1: The number of part conﬁgurations generated from our
AOG without considering overlapped compositions.

parameters (to be deﬁned in Section 4.1). We have,
i) The object/root Or-node (plotted by green circles), which repre-

sents alternative object conﬁgurations;

ii) A set of And-nodes (solid blue circles), each of which represents
the rule of decomposing a complex structure (e.g., a walking
person or a running basketball player) into simpler ones;

iii) A set of part Or-nodes, which handle local variations and

conﬁgurations in a recursive way;

iv) A set of terminal-nodes (red rectangles), which link an object
and its parts to image data (i.e., grounding symbols) to account
for appearance variations and occlusions (e.g., head-shoulder of
a walking person before and after opening a sun umbrella).

An object AOG is a subgraph of a full structure AOG with the
same root Or-node. For notational simplicity, we also denote by
G an object AOG. So, we will write Score(Ii|Bi; G) in Eqn.( 15)
with G added.

A parse tree is an instantiation of an object AOG with the best
child node (w.r.t. matching scores) selected for each encountered
Or-node. All the terminal-nodes in a parse tree represents a part
conﬁguration when collapsed to image domain.

We note that an object AOG contains multiple parse trees to
preserve ambiguities in interpreting a tracked object (see examples
in Fig. 2 (c) and Fig. 7).

4 TRACKING-BY-PARSING WITH OBJECT AOGS

In this section, we present details of inference with object AOGs.
We ﬁrst deﬁne scoring functions of nodes in an AOG. Then, we
present a spatial DP algorithm for computing Score(Ii|Bi; G),
and a temporal DP algorithm for inferring the trajectory B∗
2:t in
Eqn.(15).

Or-nodeAnd-nodeTerminal-nodeARXIV VERSION

7

4.1 Scoring Functions of Nodes in an AOG
Let F be the feature pyramid computed for either the local ROI or
the whole image It, and Λ the position space of pyramid F. Let
p = (l, x, y) ∈ Λ specify a position (x, y) in the l-th level of
pyramid F.

Given an AOG G = (VT , VAnd, VOr, E, Θ) (e.g., the left in
Fig.6), we deﬁne four types of edges, i.e., E = ET ∪ EDef ∪
EDec ∪ ESwitch as shown in Fig.6. We elaborate the deﬁnitions
of parameters Θ = (Θapp, Θdef , Θbias):
i) Each terminal-node t ∈ VT has appearance parameters θapp
t ⊂
Θapp, which is used to ground a terminal-node to image data.
i) The parent And-node A of a part terminal-node with defor-
mation edge has deformation parameters θdef
A ⊂ Θdef . They
are used for penalizing local displacements when placing a
terminal-node around its anchor position. We note that the
object template is not allowed to perturb locally in inference
since we infer the optimal part conﬁguration for each given
object location in the pyramid with sliding window technique
used, as done in the DPM [1], so the parent And-node of the
object terminal-node does not have deformation parameters.
iii) A child And-node of the root Or-node has a bias term Θbias =
{b}. We do not deﬁne bias terms for child nodes of other Or-
nodes.

Appearance Features. We use three types of features: his-
togram of oriented gradient (HOG) [49], local binary pattern
features (LBP) [50], and RGB color histograms (for color videos).
Deformation Features. Denote by δ = [dx, dy] the displace-
ment of placing a terminal-node around its anchor location. The
deformation feature is deﬁned by Φdef (δ) = [dx2, dx, dy2, dy](cid:48)
as done in DPMs [1].

We use linear functions to evaluate both appearance scores and
deformation scores. The score functions of nodes in an AOG are
deﬁned as follows:

i) For a terminal-node t, its score at a position p is computed

Fig. 6: Illustration of the spatial DP algorithm for parsing with
AOGs (e.g., AOG172 in the left). Right-middle: The input image
(ROI in the 173-th frame in the “Skating1” sequence) and the
inferred object conﬁguration. Right-top: The score map pyramid
for root Or-node. Middle: For each node in AOG, we show one
level of score map pyramid at which the optimal parse tree is
retrieved.

the third case for computing the score maps of an And-node
which has two child nodes through composition.

by,

Score(t, p|F) =< θapp

t

, F(t, p) >

(17)

4.2 Tracking-by-Parsing

where < ·, · > represents inner product and F(t, p) extracts
features in feature pyramid.

ii) For an Or-node O, its score at position p takes the maximum

score over its child nodes,

Score(O, p|F) = max
c∈ch(O)

Score(c, p|F)

(18)

where ch(v) denotes the set of child nodes of a node v.
iii) For an And-node A, we have three different functions w.r.t.
the type of its out-edge (i.e., Terminal-, Deformation-, or
Decomposition-edge),

With scoring functions deﬁned above, we present a spatial DP and
a temporal DP algorithms in solving Eqn.(15).

Spatial DP: The DP algorithm (see Algorithm 2) consists of
two stages: (i) The bottom-up pass computes score map pyramids
(as illustrated in Fig. 6) for all nodes following the depth-ﬁrst-
search (DFS) order of nodes. It computes matching scores of all
possible parse trees at all possible positions in feature pyramid.
(ii) In the top-down pass, we ﬁrst ﬁnd all candidate positions for
the root Or-node O based on its score maps and current threshold
τG of the object AOG, denoted by

(19)

Ωcand = {p; Score(O, p|F) ≥ τG and p ∈ Λ}.

(20)

Score(A, p|F) =

Score(t, p|F),

maxδ[Score(t, p ⊕ δ|F)− < θdef
(cid:80)

eA,t ∈ ET

c∈ch(A) Score(c, p|F),



eA,c ∈ EDec

A , Φdef (δ) >], eA,t ∈ EDef

where the ﬁrst case is for sharing score maps between the
object terminal-node and its parent And-node since we do
not allow local deformation for the whole object, the second
case for computing transformed score maps of parent And-
node of a part terminal-node which is allowed to ﬁnd the best
placement through distance transformation [1], ⊕ represents
the displacement operator in the position space in Λ, and

Then, following BFS order of nodes, we retrieve the optimal parse
tree at each p ∈ P: starting from the root Or-node, we select the
optimal branch (with the largest score) of each encountered Or-
node, keep the two child nodes of each encountered And-node, and
retrieve the optimal position of each encountered part terminal-
node (by taking arg max for the second case in Eqn.(19)).

After spatial parsing, we apply non-maximum suppression
(NMS) in computing the optimal parse trees with a predeﬁned
intersection-over-union (IoU) overlap threshold, denoted by τNMS.
We keep top Nbest parse trees to infer the best B∗
t together with a
temporal DP algorithm, similar to the strategies used in [19], [20].

And-nodeOr-nodeTerminal-nodeSwtiching-edgeDecomposition-edgeDeformation-edgeTerminal-edgeARXIV VERSION

Input: An image Ii, a bounding box Bi, and an AOG G
Output: Score(Ii|Bi; G) in Eqn.(8) and the optimal

conﬁguration C∗
at frame i.

i from the parse tree for the object

Initialization: Build the depth-ﬁrst search (DFS) ordering
queue (QDF S) of all nodes in the AOG;
Step 0: Compute scores for all nodes in QDF S;
while QDF S is not empty do

Pop a node v from the QDF S;
if v is an Or-node then

Score(v) = maxu∈ch(v) Score(u); // ch(v) is the set
of child nodes of v
Score(v) = (cid:80)

u∈ch(v) LocalMax(Score(u))

else if v is an And-node then

else if v is a Terminal-node then

Compute the ﬁlter response map for IN (Λv). //
N (Λv) represents the image domain of the
LocalMax operation of Terminal-node v.

end

end
Score(Ii|Bi; G) = Score(RootOrNode).;
Step 1: Compute C∗
QBF S = {RootOrNode}, C∗
while QBF S is not empty do

i = (Bi), k = 1;

i using the breadth-ﬁrst search;

Pop a node v from the QBF S;
if v is an Or-node then

Push the child node u with maximum score into
QBF S(i.e., Score(u)=Score(v)).

else if v is an And-node then

Push all the child nodes v’s into QBF S.

else if v is a Terminal-node then

i = Deformed(Λv) to C∗

i = (C∗

i , B(k)

i

).

Add B(k)
Increase k = k + 1.

end

end

Algorithm 2: The spatial DP algorithm for parsing with the
AOG, Parse(Ii|Bi; G)

Temporal DP: Assuming that all the N-best candidates for
B2, · · · , Bt are memoized after running spatial DP algorithm
in I2 to It, Eqn.(15) corresponds to the classic DP formula-
tion of forward and backward inference for decoding HMMs
with −Score(Ii|Bi; G) being the singleton “data” term and
Cost(Bi|Bi−1) the pairwise cost term.

Let Bi[Bi] be energy of the best object states in the ﬁrst i

frames with the constraint that the i-th one is Bi. We have,

B1[B1] = −Score(I1|B1; G),
Bi[Bi] = −Score(Ii|Bi; G)

+ min
Bi−1

(Bi−1[Bi−1] + Cost(Bi|Bi−1)).

(21)

When B1 is the input bounding box. Then, the temporal DP
algorithm consists of two steps:

i) The forward step for computing all Bi[Bi]’s, and caching
the optimal solution for Bi−1 as a function of Bi for later
back-tracing starting at i = 2,

Ti[Bi] = arg min
Bi−1

{Bi−1[Bi−1] + Cost(Bi|Bi−1)}.

ii) The backward step for ﬁnding the optimal

(B1, B∗

2 , · · · , B∗

t ), where we ﬁrst take,

8

trajectory

B∗

t = arg min
Bt

Bt[Bt],

and then in the order of i = t − 1, · · · , 2 trace back,

B∗

i = Ti+1[B∗

i+1].

(22)

(23)

In practice, we often do not need to run temporal DP in the
whole time range [1, t], especially for long-term tracking, since
the target object might have changed signiﬁcantly or we might
have camera motion, instead we only focus on some short time
range, [t − ∆t, t] (see settings in experiments).

Remarks: In our TLP method, we apply the spatial and the
temporal DP algorithms in a stage-wise manner and without
tracking parts explicitly. Thus, we do not introduce loops in
inference. If we instead attempt to learn a joint spatial-temporal
AOG, it will be a much more difﬁcult problem due to loops in joint
spatial-temporal inference, and approximate inference is used.

Search Strategy: During tracking, at time t, Bt is initialized
by Bt−1, and then a rectangular region of interest (ROI) centered
at the center of Bt is used to compute feature pyramid and run
parsing with AOG. The ROI is ﬁrst computed as a square area
with the side length being sROI times longer than the maximum of
width and height of Bt and then is clipped with the image domain.
If no candidates are found (i.e., Ωcand is empty), we will run the
parsing in whole image domain. So, our AOGTracker is capable of
re-detecting a tracked object. If there are still no candidates (e.g.,
the target object was completely occluded or went out of camera
view), the tracking result of this frame is set to be invalid and we
do not need to run the temporal DP.

4.3 The Trackability of an Object AOG

To detect critical moments online, we need to measure the quality
of an object AOG, G at time t. We compute its trackability based
on the score maps in which the optimal parse tree is placed. For
each node v in the parse tree, we have its position in score map
pyramid (i.e., the level of pyramid and the location in that level),
(lv, xv, yv). We deﬁne the trackability of node v by,

Trackability(v|It, G) = S(lv, xv, yv) − µS

(24)

where S(lv, xv, yv) is the score of node v, µS the mean score
computed from the whole score map. Intuitively, we expect
the score map of a discriminative node v has peak and steep
landscape, as investigated in [51]. The trackabilities of part nodes
are used to infer partial occlusion and local structure variations,
and trackability of the inferred parse tree indicate the “goodness”
of current object AOG. We note that we treat trackability and
intrackability (i.e., the inverse of th trackability) exchangeably.
More sophisticated deﬁnitions of intrackability in tracking are
referred to [52].

We model trackability by a Gaussian model whose mean and
standard derivation are computed incrementally in [2, t]. At time t,
a tracked object is said to be “intrackable” if its trackability is less
than meantrackability(t) − 3 · stdtrackability(t). We note that
the tracking result could be still valid even if it is “intrackable”
(e.g., in the ﬁrst few frames in which the target object is occluded
partially, especially by similar distractors).

ARXIV VERSION

9

5 ONLINE LEARNING OF OBJECT AOGS

In this section, we present online learning of object AOGs, which
consists of three components: (i) Maintaining a training dataset
based on tracking results; (ii) Estimating parameters of a given
object AOG; and (iii) Learning structure of the object AOG by
pruning full structure AOG, which requires (ii) in the process.

In the ﬁrst frame, we have D+

5.1 Maintaining the Training Dataset Online
Denote by Dt = D+
t ∪ D−
t
t , a positive dataset, and D−
consisting of D+

the training dataset at time t,
t , a negative dataset.
1 = {(I1, B1)} and let
B1 = (x1, y1, w1, h1). We augment it with eight locally shifted
positives, i.e., {I1, B1,i; i = 1, · · · , 8} where x1,i ∈ {x1 ± d}
and y1,i ∈ {y±d} with width and height not changed. d is set
to the cell size in computing HOG features. The initial D−
1 uses
the whole remaining image IΛB1
for mining hard negatives in
training.

t = D+

At time t, if Bt is valid according to tracking-by-parsing,
we have D+
t all other
candidates in Ωcand (Eqn. 20) which are not suppressed by Bt
according to NMS (i.e., hard negatives). Otherwise, we have
Dt = Dt−1.

t−1 ∪ {(It, Bt)}, and add to D−

5.2 Estimating Parameters of a Given Object AOG

We use latent SVM method (LSVM) [1]. Based on the scoring
functions deﬁned in Section 4.1, we can re-write the scoring
function of applying a given object AOG, G on a training example
(denoted by IB for simplicity),

Score(IB; G) = max
pt∈ΩG

< Θ, Φ(F, pt) >

(25)

where pt represents a parse tree, ΩG the space of parse trees, Θ the
concatenated vector of all parameters, Φ(F, pg) the concatenated
vector of appearance and deformation features in feature pyramid
F w.r.t. parse tree pt, and the bias term.

The objective function in estimating parameters is deﬁned by

the l2-regularized empirical hinge loss function,

LDt(Θ) =

1
2

||Θ||2

2+

C
|Dt|

(cid:88)

[
IB ∈D+
t

max(0, 1 − Score(IB; G))

(cid:88)

IB ∈D−
t

max(0, 1 + Score(IB; G))]

(26)

where C is the trade-off parameter in learning. Eqn.( 26) is a semi-
convexity function of the parameters Θ due to the empirical loss
term on positives.

In optimization, we utilize an iterative procedure in a “coor-
dinate descent” way. We ﬁrst convert the objective function to a
convex function by assigning latent values for all positives using
the spatial DP algorithm. Then, we estimate parameters. While
we can use stochastic gradient descent as done in DPMs [1], we
adopt LBFGS method in practice 3 [53] since it is more robust and
efﬁcient with parallel implementation as investigated in [9], [54].
The detection threshold, τG is estimated as the minimum score of
positives.

3. We

reimplemented

the

matlab

code

available

at

http://www.cs.ubc.ca/∼schmidtm/Software/minConf.html in c++.

Fig. 7: Illustration of learning an object AOG in the ﬁrst frame
(top) and re-learning an object AOG in the 281-th frame when
a critical moment has triggered. It consists of two steps: (a)
learning initial object AOG by pruning branches of Or-nodes
in full structure AOG, and (b) learning reﬁned object AOG by
pruning part conﬁgurations w.r.t. majority voting in positive re-
labeling in LSVM.

5.3 Learning Object AOGs
With the training dataset Dt and the full structure AOG con-
structed based on B1, an object AOG is learned in three steps:

i) Evaluating the ﬁgure of merits of nodes in the full structure
AOG. We ﬁrst train the root classiﬁer (i.e., object appearance
parameters and bias term) by linear SVM using D+
t and data-
mining hard negatives in D−
t . Then, the appearance parameters
for each part terminal-node t is initialized by cropping out the
corresponding portion in the object template 4. Following DFS
order, we evaluate the ﬁgure of merit of each node in the full
structure AOG by its training error rate. The error rate is calculated
on Dt where the score of a node is computed w.r.t. scoring
functions deﬁned in Section 4.1. The smaller the error rate is,
the more discriminative a node is.

ii) Retrieving an initial object AOG and re-estimating param-
eters. We retrieve the most discriminative subgraph in the full
structure AOG as initial object AOG. Following BFS order, we
start from the root Or-node, select for each encountered Or-node
the best child node (with the smallest training error rate among
all children) and the child nodes whose training error rates are not

4. We also tried to train the linear SVM classiﬁers for all the terminal-nodes
individually using cropped examples, which increases the runtime, but does not
improve the tracking performance in experiments. So, we use the simpliﬁed
method above.

In 1st Frame(a) Initial Object AOG(b) Reﬁned In 281-th FrameFull Structure AOG(Re-learning)PruningPruningARXIV VERSION

Representation

l
a
c
o
L

e
t
a
l
p
m
e
T

(cid:88) (cid:88)

r
o
l
o
C

m
a
r
g
o
t
s
i
H

e
c
a
p
s
b
u
S

e
s
r
a
p
S

(cid:88) (cid:88)

(cid:88)

(cid:88) (cid:88)

(cid:88)

(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88) (cid:88)

(cid:88)

(cid:88)

(cid:88) (cid:88)

(cid:88)
(cid:88) (cid:88)

(cid:88) (cid:88)

(cid:88) (cid:88)
(cid:88) (cid:88)

(cid:88) (cid:88)

(cid:88)
(cid:88) (cid:88)

(cid:88) (cid:88)

(cid:88) (cid:88)

(cid:88)

(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)

(cid:88)
(cid:88)

ASLA [55]
BSBT [56]
CPF [57]
CSK [58]
CT [59]
CXT [60]
DFT [61]
FOT [62]
FRAG [63]
IVT [29]
KMS [30]
L1APG [64]
LOT [65]
LSHT [66]
LSK [67]
LSS [68]
MIL [39]
MTT [69]
OAB [70]
ORIA [71]
PCOM [72]
SCM [73]
SMS [74]
SBT [75]
STRUCK [40]
TLD [17]
VR [76]
VTD [77]
VTS [78]
AOG

r
a
a
H

r
o

y
r
a
n
i
B

H

H
B

H

H

H
H

H
H
B

Search
m
u
m

C
M
C
M

i
t
p
O

l
a
c
o
L

(cid:88)
(cid:88)

10

g
n
i
l
p
m
a
S

e
s
n
e
D

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88) (cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88) (cid:88)

e
v
i
t
a
n
i
m

i
r
c
s
i
D

(cid:88)

r
e
t
l
i
F

e
l
c
i
t
r
a
P

e
t
a
d
p
U

e
v
i
t
a
r
e
n
e
G

l
e
d
o
M
(cid:88) (cid:88) (cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)

HOG [+Color] (cid:88)

TABLE 2: Tracking algorithms evaluated in the TB-100 bench-
mark (reproduced from [2]).

only if the tracking result is valid in a frame based on tracking-by-
parsing.

6 EXPERIMENTS

In this section, we present comparison results on the TB-
50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks
[4]. We also analyze different aspects of our method. The source
code 5 is released with this paper for reproducing all results. We
denote the proposed method by AOG in tables and plots.

Parameter Setting. We use the same parameters for all experi-
ments since we emphasize online learning in this paper. In learning
object AOGs, the side length of the grid used for constructing the
full structure AOG is either 3 or 4 depending the slide length
of input bounding box (to reduce the time complexity of online
learning). The number of intervals in computing feature pyramid
is set to 6 with cell size being 4. The factor s in computing
search ROI is set to sROI = 3. The NMS IoU threshold is set
to τNMS = 0.7. The number of top parse trees kept after spatial
DP parsing is set NBest = 10. The time range in temporal DP
algorithm is set to ∆t = 5. In identifying critical moments, we set
NIntrackable = 5 and NNewSample = 10. The LSVM trade-off parameter
in Eqn.(26) is set to C = 0.001. When re-learning structure and
parameters, we could use all the frames with valid tracking results.
To reduce the time complexity, the number of frames used in re-
learning is at most 100 in our experiments. At time t, we ﬁrst
take the ﬁrst 10 frames with valid tracking results in [1, t] with
the underlying intuition that they have high probabilities of being
tracked correctly (note that we alway use the ﬁrst frame since the
ground-truth bounding box is given), and then take the remaining
frames in reversed time order.

Speed. In our current c++ implementation, we adopt FFT in
computing score pyramids as done in [54] which also utilizes
multi-threads with OpenMP. We also provide a distributed version

5. Available at https://github.com/tfwu/RGM-AOGTracker

Fig. 8: Illustration of the three types of evaluation methods in
TB-100/50/CVPR2013. In one-pass evaluation (OPE), a tracker
is initialized in the ﬁrst frame and let it track the target until the
end of the sequence. In temporal robustness evaluation (TRE),
a tracker starts at different starting frames initialized with the
corresponding ground-truth bounding boxes and then tracks the
object until the end. 20 starting frames (including the ﬁrst frame)
are used in TB-100. In spatial robustness evaluation (SRE), a
tracker runs multiple times with spatially scaled (4 types) and
shifted (8 types of perturbation) initializations in the ﬁrst frame.

bigger than that of the best child by some predeﬁned small positive
value (i.e., preserving ambiguities), keep the two child nodes
for each encountered And-node, and stop at each encountered
terminal-node. We show two examples in the left of Fig. 7.
We train the parameters of initial object AOG using LSVM [1]
with two rounds of positive re-labeling and hard negative mining
respectively.

iii) Controlling model complexity. To do that, a reﬁned object
AOG for tracking is obtained by further selecting the most dis-
criminative part conﬁguration(s) in the initial object AOG learned
in the step ii). The selection process is based on latent assignment
in relabeling positives in LSVM training. A part conﬁguration in
the initial object AOG is pruned if it relabeled less than 10%
positives (see the right of Fig. 7). We further train the reﬁned
object AOG with one round latent positive re-labeling and hard
negative mining. By reducing model complexity, we can speed up
the tracking-by-parsing procedure.

Veriﬁcation of a reﬁned object AOG. We run parsing with a
reﬁned object AOG in the ﬁrst frame. The reﬁned object AOG is
accepted if the score of the optimal parse tree is greater than the
threshold estimated in training and the IoU overlap between the
predicted bounding box and the input bounding box is greater than
or equals the IoU NMS threshold, τNMS in detection.

Identifying critical moments in tracking. A critical moment
means a tracker has become “uncertain” and at the same time
accumulated “enough” new samples, which is triggered in tracking
when two conditions were satisﬁed. The ﬁrst is that the number of
frames in which a tracked object is “intrackable” was larger than
some value, NIntrackable. The second is that the number of new valid
tracking results are greater than some value, NNewSample. Both are
accumulated from the last time an object AOG was re-learned.

The spatial resolution of placing parts. In learning object
AOGs, we ﬁrst place parts at the same spatial resolution as the
object. If the learned object AOG was not accepted in veriﬁcation,
the
we then place parts at
object and re-learn the object AOG. In our experiments, the two
speciﬁcations handled all testing sequences successfully.

twice the spatial resolution w.r.t.

Overall ﬂow of online learning. In the ﬁrst frame or when a
critical moment is identiﬁed in tracking, we learn both structure
and parameters of an object AOG, otherwise we update parameters

OPESRESpatially ScaledSpatially ShiftedTREARXIV VERSION

11

Metric
Evaluation
Subset

AOG Gain
Runner-up

100

50

CVPR2013

100

OPE

SRE
50

CVPR2013

100

TRE
50

CVPR2013

13.93 / 18.06

16.84 / 22.23

STRUCK [40]

2.74 / 19.37
SO-DLT [6] / STRUCK [40]

11.47 / 16.79

12.52 / 17.82

11.89 / 17.55

9.25 / 11.06

11.37 / 14.61

11.59 / 14.38

STRUCK [40]

Success Rate / Precision Rate

Subsets in TB-50

DEF(23)

FM(25) MB(19)

IPR(29)

BC(20)

OPR(32)

OCC(29)

IV(22)

LR(8)

SV(38)

OV(11)

AOG Gain (success rate)
Runner-up

15.89

15.56

17.29

12.29

17.81

14.04

STRUCK [40]

TLD [17]

14.7
SCM [73]

15.73

6.65

18.38

15.99
MIL [39]

TABLE 3: Performance gain (in %) of our AOGTracker in term of success rate and precision rate in the benchmark [2]. Success plots
of TB-100/50/CVPR2013 are shown in Fig. 9. The success plots of the 11 subsets in TB-50 are shown in Fig. 10. Precision plots are
provided in the supplementary material due to space limit here.

Fig. 9: Performance comparison in TB-100 (1st row), TB-50 (2nd row) and TB-CVPR2013 (3rd row) in term of success plots of
OPE (1st column), SRE (2nd column) and TRE (3rd colum). For clarity, only top 10 trackers are shown in color curves and listed
in the legend. Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their
performance plots manually added in the left-bottom ﬁgure). We note that the plots are reproduced with the raw results provided at
http://cvlab.hanyang.ac.kr/tracker benchmark/. (Best viewed in color and with magniﬁcation)

based on MPI 6 in evaluation. The FPS is about 2 to 3. We are
experimenting GPU implementations to speed up our TLP.

6. https://www.mpich.org/

6.1 Results on TB-50/100/CVPR2013

The TB-100 benchmark has 100 target objects (58, 897 frames in
total) with 29 publicly available trackers evaluated. It is extended
from a previous benchmark with 51 target objects released at
CVPR2013 (denoted by TB-CVPR2013). Further, since some tar-
get objects are similar or less challenging, a subset of 50 difﬁcult

00.20.40.60.81020406080100thresholdsOPE−100(sequence average)  AOG [60.10]STRUCK [46.17]SCM [44.58]TLD [42.54]CXT [41.29]ASLA [40.91]CSK [38.49]LSK [38.36]VTD [36.82]OAB [36.51]00.20.40.60.810102030405060708090thresholdsSRE−100(sequence average)  AOG [55.11]STRUCK [43.64]ASLA [40.73]TLD [40.39]SCM [40.11]CXT [39.13]LSK [36.97]OAB [36.60]CSK [35.78]RS [35.15]00.20.40.60.81020406080100thresholdsTRE−100(sequence average)  AOG [61.02]STRUCK [51.77]SCM [47.27]ASLA [46.07]CXT [45.01]CSK [44.70]TLD [44.67]OAB [43.23]LSK [42.84]MTT [41.76]00.20.40.60.810102030405060708090thresholdsOPE−50(sequence average)  AOG [55.24]STRUCK [38.40]SCM [36.77]TLD [36.39]ASLA [33.27]CXT [32.06]CSK [31.41]VTD [31.14]VTS [30.20]L1APG [29.90]00.20.40.60.810102030405060708090thresholdsSRE−50(sequence average)  AOG [49.85]STRUCK [37.33]TLD [34.60]ASLA [33.69]SCM [32.75]LSK [30.58]CXT [30.53]OAB [29.66]VTD [29.18]CSK [28.44]00.20.40.60.810102030405060708090thresholdsTRE−50(sequence average)  AOG [56.19]STRUCK [44.82]SCM [39.04]TLD [38.42]ASLA [38.11]CSK [37.32]LSK [36.75]CXT [35.90]L1APG [35.84]OAB [35.84]00.20.40.60.81020406080100thresholdsOPE−CVPR2013(51, sequence average)AOG [62.94]SCM [49.84]STRUCK [47.21]TLD [43.43]ASLA [43.25]CXT [42.36]VTS [41.53]VTD [41.52]LSHT [40.54]CSK [39.57]SO-DLT [60.2]CNT [54.5]00.20.40.60.810102030405060708090thresholdsSRE−CVPR2013(51, sequence average)  AOG [56.19]STRUCK [44.30]ASLA [43.53]SCM [43.31]TLD [40.71]CXT [39.83]VTD [38.61]VTS [38.25]LSK [37.82]LSHT [37.75]00.20.40.60.81020406080100thresholdsTRE−CVPR2013(51, sequence average)  AOG [62.96]SCM [51.37]STRUCK [50.86]ASLA [48.42]VTD [46.01]CXT [46.01]VTS [45.87]CSK [45.12]TLD [44.58]LSK [44.40]ARXIV VERSION

12

Fig. 10: Performance comparison in the 11 subsets (with different attributes and different number of sequences as shown by the titles
in the sub-ﬁgures) of TB-50 based on the success plots of OPE.

and representative ones (denoted by TB-50) is selected for an in-
depth analysis. Two types of performance metric are used, the
precision plot (i.e., the percentage of frames in which estimated
locations are within a given threshold distance of ground-truth
positions) and the success plot (i.e., based on IoU overlap scores
which are commonly used in object detection benchmarks, e.g.,
PASCAL VOC [79]). The higher a success rate or a precision rate
is, the better a tracker is. Usually, success plots are preferred to
rank trackers [2], [4] (thus we focus on success plots in compari-

son). Three types of evaluation methods are used as illustrated in
Fig.8.

To account for different factors of a test sequence affecting
performance, the testing sequences are further categorized w.r.t.
11 attributes for more ind-depth comparisons: (1) Illumination
Variation (IV, 38/22/21 sequences in TB-100/50/CVPR2013), (2)
Scale Variation (SV, 64/38/28 sequences), (3) Occlusion (OCC,
49/29/29 sequences), (4) Deformation (DEF, 44/23/19 sequences),
(5) Motion Blur (MB, 29/19/12 sequences), (6) Fast Motion

00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − BC(BackgroundClutters) (20)  AOG [55.87]SCM [38.06]TLD [37.05]STRUCK [36.29]CSK [36.00]VTD [35.51]VTS [35.37]ASLA [34.84]L1APG [31.48]DFT [31.29]00.20.40.60.8101020304050607080thresholdsSuccess plots of OPE − DEF(Deformation) (23)  AOG [48.21]STRUCK [32.32]SCM [31.78]DFT [29.52]VTD [29.27]ASLA [28.96]LSHT [28.72]CPF [28.05]VTS [27.44]KMS [27.28]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − FM(FastMotion) (25)  AOG [56.50]STRUCK [40.94]TLD [38.20]CXT [37.22]OAB [31.30]KMS [30.26]TM [29.83]LOT [29.81]PD [28.49]FRAG [28.44]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − MB(MotionBlur) (19)  AOG [58.26]TLD [40.97]STRUCK [40.65]CXT [38.35]OAB [32.72]TM [32.38]PD [30.92]CSK [29.96]KMS [29.18]L1APG [28.95]00.20.40.60.8101020304050607080thresholdsSuccess plots of OPE − IPR(In−PlaneRotation) (29)  AOG [51.71]TLD [39.42]STRUCK [38.13]CXT [37.32]VTD [33.75]SCM [33.74]VTS [31.95]L1APG [31.81]ASLA [31.46]LSK [31.19]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − OPR(Out−of−PlaneRotatio) (32)  AOG [50.37]SCM [36.33]VTD [35.47]STRUCK [33.53]VTS [33.01]ASLA [32.85]TLD [32.18]LSK [30.39]LOT [30.30]LSHT [30.14]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − OCC(Occlusion) (29)  AOG [51.84]SCM [37.14]VTD [35.60]VTS [33.90]ASLA [33.88]STRUCK [33.65]LSK [31.52]LOT [31.27]L1APG [30.76]OAB [29.06]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − IV(IlluminationVariation) (22)  AOG [57.05]SCM [41.32]ASLA [38.33]VTD [35.01]VTS [33.53]STRUCK [33.01]LSHT [32.85]TLD [32.61]DFT [32.04]CSK [31.66]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − LR(LowResolution) (8)  AOG [51.53]SCM [44.88]ASLA [43.70]TLD [33.45]STRUCK [31.88]LSK [31.87]CXT [31.78]L1APG [30.81]IVT [29.95]LSS [29.43]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − OV(Out−of−View) (11)  AOG [50.69]MIL [34.70]VTD [34.15]TLD [34.13]STRUCK [34.03]CT [33.63]VTS [32.64]TM [31.72]LSS [31.44]SCM [31.15]00.20.40.60.810102030405060708090thresholdsSuccess plots of OPE − SV(ScaleVariation) (38)  AOG [56.33]SCM [37.95]STRUCK [36.24]TLD [34.34]ASLA [34.31]CXT [30.83]VTD [29.99]LSK [29.98]VTS [28.86]OAB [28.71]ARXIV VERSION

13

Fig. 11: Qualitative results. For clarity, we show tracking results (bounding boxes) in 6 randomly sampled frames for the top 10 trackers
according to their OPE performance in TB-100. (Best viewed in color and with magniﬁcation.)

ARXIV VERSION

14

Fig. 12: Performance comparison of the six variants of our AOGTracker in TB-100/50/CVPR2013 in term of the success plots of OPE
(1st column), SRE (2nd column) and TRE (3rd colum).

(FM, 39/25/17 sequences), (7) In-Plane Rotation (IPR, 51/29/31
sequences), (8) Out-of-Plane Rotation (OPR, 63/32/39 sequences),
(9) Out-of-View (OV, 14/11/6 sequences), (10) Background Clut-
ters (BC, 31/20/21 sequences), and (11) Low Resolution (LR,
9/8/4 sequences). More details on the attributes and their distri-
butions in the benchmark are referred to [2], [3].

Table. 2 lists the 29 evaluated tracking algorithms which are
categorized based on representation and search scheme. See more
details about categorizing these trackers in [2]. In TB-CVPR2013,
two recent trackers trained by deep convolutional network (CNT
[5], SO-DLT [6]) were evaluated using OPE.

We summarize the performance gain of our AOGTracker in
Table.3. Our AOGTracker obtains signiﬁcant improvement (more
than 12%) in the 10 subsets in TB-50. Our AOGTracker handles
out-of-view situations much better than other trackers since it is
capable of re-detecting target objects in the whole image, and it
performs very well in the scale variation subset (see examples
in the second and fourth rows in Fig. 11) since it searches over
feature pyramid explicitly (with the expense of more computa-
tion). Our AOGTracker obtains the least improvement in the low-
resolution subset since it uses HOG features and the discrepancy

between HOG cell-based coordinate and pixel-based one can
cause some loss in overlap measurement, especially in the low
resolution subset. We will add automatic selection of feature types
(e.g., HOG v.s. pixel-based features such as intensity and gradient)
according to the resolution, as well as other factors in future work.
Fig.9 shows success plots of OPE, SRE and TRE in TB-
100/50/CVPR2013. Our AOGTracker consistently outperforms all
other trackers. We note that for OPE in TB-CVPR2013, although
the improvement of our AOGTracker over the SO-DLT [6] is not
very big, the SO-DLT utilized two deep convolutional networks
with different model update strategies in tracking, both of which
are pretrained on the ImageNet [34]. Fig. 11 shows some qualita-
tive results.

6.2 Analyses of AOG models and the TLP Algorithm

To analyze contributions of different components in our AOG-
Tracker, we compare performance of six different variants– three
different object representation schema: AOG with and without
structure re-learning (denoted by AOG and AOGFixed respec-
tively), and whole object template only (i.e., without part con-
ﬁgurations, denoted by ObjectOnly), and two different inference

00.20.40.60.81020406080100thresholdsOPE−100(sequence average)  AOG−st [60.10]AOGFixed−st [58.18]AOG−s [56.22]AOGFixed−s [55.48]ObjectOnly−s [38.68]ObjectOnly−st [36.50]00.20.40.60.810102030405060708090thresholdsSRE−100(sequence average)  AOG−st [55.11]AOGFixed−st [54.04]AOG−s [51.47]AOGFixed−s [49.32]ObjectOnly−s [35.24]ObjectOnly−st [34.76]00.20.40.60.81020406080100thresholdsTRE−100(sequence average)  AOG−st [61.02]AOGFixed−st [59.48]AOG−s [57.97]AOGFixed−s [54.39]ObjectOnly−s [39.89]ObjectOnly−st [39.49]00.20.40.60.810102030405060708090thresholdsOPE−50(sequence average)  AOG−st [55.24]AOGFixed−st [53.90]AOG−s [50.50]AOGFixed−s [47.84]ObjectOnly−s [33.87]ObjectOnly−st [31.19]00.20.40.60.810102030405060708090thresholdsSRE−50(sequence average)  AOG−st [49.85]AOGFixed−st [48.31]AOG−s [45.88]AOGFixed−s [42.41]ObjectOnly−s [31.00]ObjectOnly−st [30.17]00.20.40.60.810102030405060708090thresholdsTRE−50(sequence average)  AOG−st [56.19]AOGFixed−st [54.02]AOG−s [52.15]AOGFixed−s [48.56]ObjectOnly−s [34.58]ObjectOnly−st [34.45]00.20.40.60.81020406080100thresholdsOPE−CVPR2013(51, sequence average)  AOG−st [62.94]AOGFixed−st [59.76]AOG−s [56.36]AOGFixed−s [56.31]ObjectOnly−s [38.06]ObjectOnly−st [36.16]00.20.40.60.810102030405060708090thresholdsSRE−CVPR2013(51, sequence average)  AOG−st [56.19]AOGFixed−st [55.30]AOG−s [51.18]AOGFixed−s [49.83]ObjectOnly−s [34.64]ObjectOnly−st [34.33]00.20.40.60.81020406080100thresholdsTRE−CVPR2013(51, sequence average)  AOG−st [62.96]AOGFixed−st [60.88]AOG−s [58.40]AOGFixed−s [54.89]ObjectOnly−st [40.93]ObjectOnly−s [40.56]ARXIV VERSION

15

strategies for each representation scheme: inference with and
without temporal DP (denoted by -st and -s respectively). As stated
above, we use a very simple setting for temporal DP which takes
into account ∆t = 5 frames, [t − 5, t] in our experiments.

Fig. 12 shows performance comparison of the six variants.
AOG-st obtains the best overall performance consistently. Trackers
with AOG perform better than those with whole object template
only. AOG structure re-learning has consistent overall perfor-
mance improvement. But, we observed that AOGFixed-st works
slightly better than AOG-st on two subsets out of 11, Motion-Blur
and Out-of-View, on which the simple intrackability measurement
is not good enough. For trackers with AOG, temporal DP helps im-
prove performance, while for trackers with whole object templates
only, the one without temporal DP (ObjectOnly-s) slightly outper-
form the one with temporal DP (ObjectOnly-st), which shows that
we might need strong enough object models in integrating spatial
and temporal information for better performance.

6.3 Comparison with State-of-the-Art Methods

We explain why our AOGTracker outperforms other trackers on
the TB-100 benchmark in terms of representation, online learning
and inference.

Representation Scheme. Our AOGTracker utilizes three types
of complementary features (HOG+LBP+Color) jointly to capture
appearance variations, while most of other trackers use simpler
ones (e.g., TLD [17] uses intensity based Haar like features).
More importantly, we address the issue of learning the optimal
deformable part-based conﬁgurations in the quantized space of la-
tent object structures, while most of other trackers focus on either
whole objects [58] or implicit conﬁgurations (e.g., the random fern
forest used in TLD). These two components are integrated in a
latent structured-output discriminative learning framework, which
improves the overall tracking performance (e.g., see comparisons
in Fig. 12).

Online Learning. Our AOGTracker includes two components
which are not addressed in all other trackers evaluated on TB-100:
online structure re-learning based on intrackability, and a simple
temporal DP for computing optimal joint solution. Both of them
improve the performance based on our ablation experiments. The
former enables our AOGTracker to capture both large structural
and sudden appearance variations automatically, which is espe-
cially important for long-term tracking. In addition to improve
the prediction performance, the latter improves the capability of
maintaining the purity of online collected training dataset.

Inference. Unlike many other trackers which do not handle
scale changes explicitly (e.g., CSK [58] and STRUCK [40]), our
AOGTracker runs tracking-by-parsing in feature pyramid to detect
scale changes (e.g., the car example in the second row in Fig. 11).
Our AOGTracker also utilizes a dynamic search strategy which
re-detects an object in whole frame if local ROI search failed. For
example, our AOGTracker handles out-of-view situations much
better than other trackers due to the re-detection component (see
examples in the fourth row in Fig. 11).

Limitations. All the performance improvement stated above
are obtained at the expense of more computation in learning
and tracking. Our AOGTracker obtains the least improvement
in the low-resolution subset since it uses HOG features and the
discrepancy between HOG cell-based coordinate and pixel-based
one can cause some loss in overlap measurement, especially in the
low resolution subset. We will add automatic selection of feature

Fig. 13: Performance comparison in VOT2013. Left: Ranking plot
for the baseline experiment. The smaller the rank number is, the
better a tracker is w.r.t. accuracy and/or robust (i.e., the right-top
region indicates better performance) Right: Accuracy-Robustness
plot. The larger the rate is, the better a tracker is.

Fig. 14: Performance comparison in VOT2014.

types (e.g., HOG v.s. pixel-based features such as intensity and
gradient) according to the resolution, as well as other factors in
future work.

6.4 Results on VOT

In VOT, the evaluation focuses on short-term tracking (i.e., a
tracker is not expected to perform re-detection after losing a
target object), so the evaluation toolkit will re-initialize a tracker
after it loses the target (w.r.t. the condition the overlap between
the predicted bounding box and the ground-truth one drops to
zero) with the number of failures counted. In VOT protocol, a
tracker is tested on each sequence multiple times. The performance
is measured in terms of accuracy and robustness. Accuracy is
computed as the average of per-frame accuracies which them-
selves are computed by taking the average over the repetitions.
Robustness is computed as the average number of failure times
over repetitions.

We integrate our AOGTracker in the latest VOT toolkit7 to run

experiments with the baseline protocol and to generate plots 8.

7. Available at https://github.com/votchallenge/vot-toolkit, version 3.2
8. The plots for VOT2013 and 2014 might be different compared to those in

the original VOT reports [80], [81] due to the new version of vot-toolkit.

ARXIV VERSION

16

The VOT2013 dataset [80] has 16 sequences which was
selected from a large pool such that various visual phenomena
like occlusion and illumination changes, were still represented
well within the selection. 7 sequences are also used in TB-100.
There are 27 trackers evaluated. The readers are referred to the
VOT technical report [80] for details.

Fig.13 shows the ranking plot and AR plot in VOT2013. Our
AOGTracker obtains the best accuracy while its robustness is
slightly worse than three other trackers (i.e., PLT [80], LGT [82]
and LGTpp [83], and PLT was the winner in VOT2013 challenge).
Our AOGTracker obtains the best overall rank.

The VOT2014 dataset [81] has 25 sequences extended from
VOT2013. The annotation is based on rotated bounding box
instead of up-right rectangle. There are 33 trackers evaluated.
Details on the trackers are referred to [81]. Fig.14 shows the
ranking plot and AR plot. Our AOGTracker is comparable to other
trackers. One main limitation of AOGTracker is that it does not
handle rotated bounding boxes well.

The VOT2015 dataset [84] consists of 60 short sequences
(with rotated bounding box annotations) and VOT-TIR2015 com-
prises 20 sequences (with bounding box annotations). There are
62 and 28 trackers evaluated in VOT2015 and VOT-TIR2015 re-
spectively. Our AOGTracker obtains 51% and 65% (tied for third
place) in accuracy in VOT2015 and VOT-TIR2015 respectively.
The details are referred to the reports [84] due to space limit here.

7 DISCUSSION AND FUTURE WORK

We have presented a tracking, learning and parsing (TLP) frame-
work and derived a spatial dynamic programming (DP) and a
temporal DP algorithm for online object tracking with AOGs.
We also have presented a method of online learning object AOGs
including its structure and parameters. In experiments, we test our
method on two main public benchmark datasets and experimental
results show better or comparable performance.

In our on-going work, we are studying more ﬂexible computing
schemes in tracking with AOGs. The compositional property
embedded in an AOG naturally leads to different bottom-up/top-
down computing schemes such as the three computing processes
studied by Wu and Zhu [85]. We can track an object by matching
the object template directly (i.e. α-process), or computing some
discriminative parts ﬁrst and then combine them into object (β-
process), or doing both (α + β-process, as done in this paper).
In tracking, as time evolves, the object AOG might grow through
online learning, especially for objects with large variations in long-
term tracking. Thus, faster inference is entailed for the sake of real
time applications. We are trying to learn near optimal decision
policies for tracking using the framework proposed by Wu and
Zhu [86].

In our future work, we will extend the TLP framework by
incorporating generic category-level AOGs [8] to scale up the TLP
framework. The generic AOGs are pre-trained ofﬂine (e.g., using
the PASCAL VOC [79] or the imagenet [34]), and will help the
online learning of speciﬁc AOGs for a target object (e.g., help to
maintain the purity of the positive and negative datasets collected
online). The generic AOGs will also be updated online together
with the speciﬁc AOGs. By integrating generic and speciﬁc AOGs,
we aim at the life-long learning of objects in videos without
annotations. Furthermore, we are also interested in integrating
scene grammar [87] and event grammar [88] to leverage more
top-down information.

ACKNOWLEDGMENTS

This work is supported by the DARPA SIMPLEX Award N66001-
15-C-4035, the ONR MURI grant N00014-16-1-2007, and NSF
IIS-1423305. T. Wu was also supported by the ECE startup fund
201473-02119 at NCSU. We thank Steven Holtzen for proofread-
ing this paper. We also gratefully acknowledge the support of
NVIDIA Corporation with the donation of one GPU.

REFERENCES

[1] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, “Ob-
ject detection with discriminatively trained part-based models,” PAMI,
vol. 32, no. 9, pp. 1627–1645, 2010.

[2] Y. Wu, J. Lim, and M.-H. Yang, “Object tracking benchmark,” PAMI,

vol. 37, no. 9, pp. 1834–1848, 2015.

[3] ——, “Online object tracking: A benchmark,” in CVPR, 2013.
[4] M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. P. Pﬂugfelder,
G. Fern´andez, G. Nebehay, F. Porikli, and L. Cehovin, “A novel
performance evaluation methodology for single-target trackers,” CoRR,
vol. abs/1503.01313, 2015.
[Online]. Available: http://arxiv.org/abs/
1503.01313

[5] K. Zhang, Q. Liu, Y. Wu, and M.-H. Yang, “Robust visual tracking via
convolutional networks,” arXiv preprint arXiv:1501.04505v2, 2015.
[6] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung, “Transferring rich feature hi-
erarchies for robust visual tracking,” arXiv preprint arXiv:1501.04587v2,
2015.

[7] S. Carey, The Origin of Concepts. Oxford University Press, 2011.
[8] X. Song, T. Wu, Y. Jia, and S.-C. Zhu, “Discriminatively trained and-or

tree models for object detection,” in CVPR, 2013.

[9] R. Girshick, P. Felzenszwalb, and D. McAllester, “Object detection with

grammar models,” in NIPS, 2011.

[10] P. Felzenszwalb and D. McAllester, “Object detection grammars,” Uni-

versity of Chicago, Computer Science TR-2010-02, Tech. Rep., 2010.

[11] S. C. Zhu and D. Mumford, “A stochastic grammar of images,” Foun-
dations and Trends in Computer Graphics and Vision, vol. 2, no. 4, pp.
259–362, 2006.

[12] Y. Amit and A. Trouv´e, “POP: patchwork of parts models for object

recognition,” IJCV, vol. 75, no. 2, pp. 267–282, 2007.

[13] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,” ACM

Comput. Surv., vol. 38, no. 4, 2006.

[14] L. R. Rabiner, “Readings in speech recognition,” A. Waibel and K.-F.
Lee, Eds., 1990, ch. A Tutorial on Hidden Markov Models and Selected
Applications in Speech Recognition, pp. 267–296.

[15] M. Isard and A. Blake, “Condensation - conditional density propagation

for visual tracking,” IJCV, vol. 29, no. 1, pp. 5–28, 1998.

[16] M. Andriluka, S. Roth, and B. Schiele, “People-tracking-by-detection

and people-detection-by-tracking,” in CVPR, 2008.

[17] Z. Kalal, K. Mikolajczyk, and J. Matas, “Tracking-learning-detection,”

PAMI, vol. 34, no. 7, pp. 1409–1422, 2012.

[18] J. S. Supancic III and D. Ramanan, “Self-paced learning for long-term

tracking,” in CVPR, 2013.

[19] D. Park and D. Ramanan, “N-best maximal decoder for part models,” in

ICCV, 2011.

[20] D. Batra, P. Yadollahpour, A. Guzm´an-Rivera, and G. Shakhnarovich,
“Diverse m-best solutions in markov random ﬁelds,” in ECCV, 2012.
[21] L. Zhang, Y. Li, and R. Nevatia, “Global data association for multi-object

tracking using network ﬂows,” in CVPR, 2008.

[22] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal greedy

algorithms for tracking a variable number of objects,” in CVPR, 2011.

[23] J. Berclaz, F. Fleuret, E. T¨uretken, and P. Fua, “Multiple object tracking
using k-shortest paths optimization,” PAMI, vol. 33, no. 9, pp. 1806–
1819, 2011.

[24] A. V. Goldberg, “An efﬁcient implementation of a scaling minimum-cost

ﬂow algorithm,” J. Algorithms, vol. 22, no. 1, pp. 1–29, 1997.

[25] S. Hong and B. Han, “Visual

tracking by sampling tree-structured

graphical models,” in ECCV, 2014.

[26] S. Hong, S. Kwak, and B. Han, “Orderless tracking through model-

averaged posterior estimation,” in ICCV, 2013.

[27] R. Yao, Q. Shi, C. Shen, Y. Zhang, and A. van den Hengel, “Part-based

visual tracking with online latent structural learning,” in CVPR, 2013.

[28] H. Nam, S. Hong, and B. Han, “Online graph-based tracking,” in ECCV,

2014.

[29] D. A. Ross, J. Lim, R.-S. Lin, and M.-H. Yang, “Incremental learning for
robust visual tracking,” IJCV, vol. 77, no. 1-3, pp. 125–141, 2008.

ARXIV VERSION

17

[30] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based object tracking,”

[63] A. Adam, E. Rivlin, and I. Shimshoni, “Robust fragments-based tracking

PAMI, vol. 25, no. 5, pp. 564–575, 2003.

using the integral histogram,” in CVPR, 2006.

[64] C. Bao, Y. Wu, H. Ling, and H. Ji, “Real time robust L1 tracker using

accelerated proximal gradient approach,” in CVPR, 2012.

[65] S. Oron, A. Bar-Hillel, D. Levi, and S. Avidan, “Locally orderless

tracking,” in CVPR, 2012.

[66] S. He, Q. Yang, R. W. Lau, J. Wang, and M.-H. Yang, “Visual tracking

via locality sensitive histograms,” in CVPR, 2013.

[67] B. Liu, J. Huang, L. Yang, and C. A. Kulikowski, “Robust tracking using
local sparse appearance model and k-selection,” in CVPR, 2011.
[68] D. Wang, H. Lu, and M.-H. Yang, “Least soft-thresold squares tracking,”

in CVPR, 2013.

[69] T.Zhang, B. Ghanem, S. Liu, and N. Ahuja, “Robust visual tracking via

multi-task sparse learning,” in CVPR, 2012.

[70] H. Grabner, M. Grabner, and H. Bischof, “Real-time tracking via on-line

boosting,” in BMVC, 2006.

[71] Y. Wu, B. Shen, and H. Ling, “Online robust image alignment via

iterative convex optimization,” in CVPR, 2012.

[72] D. Wang and H. Lu, “Visual tracking via probability continuous outlier

model,” in CVPR, 2014.

[73] W. Zhong, H. Lu, and M. Yang, “Robust object tracking via sparsity-

based collaborative model,” in CVPR, 2012.

[74] R. T. Collins, “Mean-shift blob tracking through scale space,” in CVPR,

2003.

[75] H. Grabner, C. Leistner, and H. Bischof, “Semi-supervised on-line

boosting for robust tracking,” in ECCV, 2008.

[76] R. T. Collins, Y. Liu, and M. Leordeanu, “Online selection of discrimi-

native tracking features,” PAMI, vol. 27, no. 10, pp. 1631–1643, 2005.

[77] J. Kwon and K. M. Lee, “Visual tracking decomposition,” in CVPR,

2010.

[78] ——, “Tracking by sampling trackers,” in ICCV, 2011.
[79] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,
“The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Re-
sults.”

[80] M. Kristan and et al, “The visual object tracking vot2013 challenge
results,” 2013. [Online]. Available: http://www.votchallenge.net/vot2013/
program.html

[81] ——, “The visual object tracking vot2014 challenge results,” 2014.
[Online]. Available: http://www.votchallenge.net/vot2014/program.html
[82] L. Cehovin, M. Kristan, and A. Leonardis, “Robust visual tracking using
an adaptive coupled-layer visual model,” PAMI, vol. 35, no. 4, pp. 941–
953, 2013.

[83] J. Xiao, R. Stolkin, and A. Leonardis, “An enhanced adaptive coupled-
layer lgtracker++,” in Vis. Obj. Track. Challenge VOT2013, In conjunc-
tion with ICCV2013, 2013.
al,

[84] M. Kristan and et

“The visual object

tracking vot2015
[Online]. Available: http:

and tir2015 challenge results,” 2015.
//www.votchallenge.net/vot2015/program.html

[85] T. Wu and S. C. Zhu, “A numerical study of the bottom-up and top-down
inference processes in and-or graphs,” IJCV, vol. 93, no. 2, pp. 226–252,
2011.

[86] T. Wu and S. Zhu, “Learning near-optimal cost-sensitive decision policy
for object detection,” TPAMI, vol. 37, no. 5, pp. 1013–1027, 2015.
[87] Y. Zhao and S. C. Zhu, “Image parsing with stochastic scene grammar,”

in NIPS, 2011.

[88] M. Pei, Z. Si, B. Z. Yao, and S. Zhu, “Learning and parsing video events
with goal and intent prediction,” CVIU, vol. 117, no. 10, pp. 1369–1383,
2013.

[31] X. Mei and H. Ling, “Robust visual tracking and vehicle classiﬁcation
via sparse representation,” PAMI, vol. 33, no. 11, pp. 2259–2272, 2011.
[32] X. Li, A. R. Dick, C. Shen, A. van den Hengel, and H. Wang, “Incremen-
tal learning of 3d-dct compact representations for robust visual tracking,”
PAMI, vol. 35, no. 4, pp. 863–881, 2013.

[33] H. Nam and B. Han, “Learning multi-domain convolutional neural

networks for visual tracking,” in CVPR, 2016.

[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:

A Large-Scale Hierarchical Image Database,” in CVPR, 2009.

[35] J. Kwon and K. M. Lee, “Highly nonrigid object tracking via patch-based
dynamic appearance modeling,” PAMI, vol. 35, no. 10, pp. 2427–2441,
2013.

[36] L. Cehovin, M. Kristan, and A. Leonardis, “Robust visual tracking using
an adaptive coupled-layer visual model,” PAMI, vol. 35, no. 4, pp. 941–
953, 2013.

[37] X. Jia, H. Lu, and M.-H. Yang, “Visual tracking via adaptive structural

local sparse appearance model,” in CVPR, 2012.

[38] S. Avidan, “Support vector tracking,” PAMI, vol. 26, no. 8, pp. 1064–

1072, 2004.

[39] B. Babenko, M.-H. Yang, and S. Belongie, “Robust object tracking with
online multiple instance learning,” PAMI, vol. 33, no. 8, pp. 1619–1632,
2011.

[40] S. Hare, A. Saffari, and P. H. S. Torr, “Struck: Structured output tracking

with kernels,” in ICCV, 2011.

[41] J. Henriques, R. Caseiro, P. Martins, and J. Batista, “Exploiting the
circulant structure of tracking-by-detection with kernels,” in ECCV, 2012.
[42] V. Mahadevan and N. Vasconcelos, “Biologically inspired object tracking
using center-surround saliency mechanisms,” PAMI, vol. 35, no. 3, pp.
541–554, 2013.

[43] R. Yao, Q. Shi, C. Shen, Y. Zhang, and A. van den Hengel, “Part-based

visual tracking with online latent structural learning,” in CVPR, 2013.

[44] L. Zhang and L. van der Maaten, “Structure preserving object tracking,”

in CVPR, 2013.

[45] J. Shi and C. Tomasi, “Good feature to track,” in CVPR, 1994.
[46] Y. Lu, T. Wu, and S.-C. Zhu, “Online object tracking, learning and

parsing with and-or graphs,” in CVPR, 2014.

[47] X. Li, W. Hu, C. Shen, Z. Zhang, A. R. Dick, and A. van den Hengel,
“A survey of appearance models in visual object tracking,” CoRR, vol.
abs/1303.4803, 2013.

[48] S. Baker and I. Matthews, “Lucas-kanade 20 years on: A unifying

framework,” IJCV, vol. 56, no. 3, pp. 221–255, 2004.

[49] N. Dalal and B. Triggs, “Histograms of oriented gradients for human

detection,” in CVPR, 2005.

[50] T. Ojala, M. Pietikainen, and D. Harwood, “Performance evaluation of
texture measures with classiﬁcation based on kullback discrimination of
distributions,” in ICPR, 1994.

[51] J. Kwon and K. M. Lee, “Highly nonrigid object tracking via patch-based
dynamic appearance modeling,” TPAMI, vol. 35, no. 10, pp. 2427–2441,
2013.

[52] H. Gong and S. C. Zhu, “Intrackability: Characterizing video statistics
and pursuing video representations,” IJCV, vol. 97, no. 3, pp. 255–275,
2012.

[53] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, “A limited memory algorithm
for bound constrained optimization,” SIAM J. Sci. Comput., vol. 16, no. 5,
pp. 1190–1208, 1995.

[54] C. Dubout and F. Fleuret, “Exact acceleration of linear object detectors,”

in ECCV, 2012.

[55] X. Jia, H. Lu, and M.-H. Yang, “Visual tracking via adaptive structural

local sparse appearance model,” in CVPR, 2012.

[56] S. Stalder, H. Grabner, and L. van Gool, “Beyond semi-supervised
tracking: Tracking should be as simple as detection, but not simpler than
recognition,” in ICCV Workshop, 2009.

[57] P. P´erez, C. Hue, J. Vermaak, and M. Gangnet, “Color-based probabilistic

tracking,” in ECCV, 2002.

[58] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “Exploiting the
circulant structure of tracking-by-detection with kernels,” in ECCV, 2012.
[59] K. Zhang, L. Zhang, and M. Yang, “Fast compressive tracking,” PAMI,

vol. 36, no. 10, pp. 2002–2015, 2014.

[60] T. B. Dinh, N. Vo, and G. G. Medioni, “Context tracker: Exploring
supporters and distracters in unconstrained environments,” in CVPR,
2011.

[61] L. Sevilla-Lara and E. Learned-Miller, “Distribution ﬁelds for tracking,”

in CVPR, 2012.

[62] T. Vojir and J. Matas, “Robustifying the ﬂock of trackers,” in Computer

Vision Winter Workshop, 2011.

ARXIV VERSION

18

Tianfu Wu received Ph.D. degree in Statis-
tics from University of California, Los Angeles
(UCLA) in 2011. He joined NC State University
in August 2016 as a Chancellors Faculty Excel-
lence Program cluster hire in Visual Narrative.
He is currently assistant professor in the De-
partment of Electrical and Computer Engineer-
ing. His research focuses on explainable and
improvable visual Turing test and robot auton-
omy through life-long communicative learning by
pursuing a uniﬁed framework for machines to
ALTER (Ask, Learn, Test, Explain, and Reﬁne) recursively in a principled
way: (i) Statistical learning of large scale and highly expressive hierarchi-
cal and compositional models from visual big data (images and videos).
(ii) Statistical inference by learning near-optimal cost-sensitive decision
policies. (iii) Statistical theory of performance guaranteed learning algo-
rithm and optimally scheduled inference procedure.

Yang Lu is currently Ph. D. student in the Center
for Vision, Cognition, Learning and Autonomy
at the University of California, Los Angeles. He
received B.S. degree and M.S. degree in Com-
puter Science from Beijing Institute of Technol-
ogy, China, in 2009 and in 2012 respectively. He
received the University Fellowship from UCLA
and National Fellowships from Department of
Education at China. His current research inter-
ests include Computer Vision and Statistical Ma-
chine Learning. Speciﬁcally, his research inter-
ests focus on statistical modeling of natural images and videos, and
structure learning of hierarchical models.

Song-Chun Zhu received Ph.D. degree from
Harvard University in 1996. He is currently pro-
fessor of Statistics and Computer Science at
UCLA, and director of Center for Vision, Cog-
nition, Learning and Autonomy. He received a
number of honors, including the J.K. Aggarwal
prize from the Int’l Association of Pattern Recog-
nition in 2008 for ”contributions to a uniﬁed foun-
dation for visual pattern conceptualization, mod-
eling, learning, and inference”, the David Marr
Prize in 2003 with Z. Tu et al. for image parsing,
twice Marr Prize honorary nominations in 1999 for texture modeling and
in 2007 for object modeling with Z. Si and Y.N. Wu. He received the
Sloan Fellowship in 2001, a US NSF Career Award in 2001, and an
US ONR Young Investigator Award in 2001. He received the Helmholtz
Test-of-time award in ICCV 2013, and he is a Fellow of IEEE since 2011.

