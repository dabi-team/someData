2
2
0
2

r
a

M
6
1

]

R
C
.
s
c
[

3
v
2
0
6
5
0
.
9
0
0
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

1

Semantics-preserving Reinforcement Learning
Attack Against Graph Neural Networks
for Malware Detection

Lan Zhang;Peng Liu, IEEE member ; Yoon-Ho Choi;Ping Chen

Abstract—As an increasing number of deep-learning-based malware scanners have been proposed, the existing evasion techniques,
including code obfuscation and polymorphic malware, are found to be less effective. In this work, we propose a reinforcement learning
based semantics-preserving (i.e. functionality-preserving) attack against black-box GNNs (Graph Neural Networks) for malware
detection. The key factor of adversarial malware generation via semantic Nops insertion is to select the appropriate semantic Nops
and their corresponding basic blocks. The proposed attack uses reinforcement learning to automatically make these “how to select”
decisions. To evaluate the attack, we have trained two kinds of GNNs with three types (e.g., Backdoor, Trojan, and Virus) of Windows
malware samples and various benign Windows programs. The evaluation results have shown that the proposed attack can achieve a
signiﬁcantly higher evasion rate than four baseline attacks, namely the binary diversiﬁcation attack, the semantics-preserving random
instruction insertion attack, the semantics-preserving accumulative instruction insertion attack, and the semantics-preserving
gradient-based instruction insertion attack.

Index Terms—Adversarial samples generation, Malware detection, Graph Neural Networks, Reinforcement Learning.

(cid:70)

1 INTRODUCTION
The detect-evade “game” between malware scanners (e.g.,
antivirus software) and malware writers is a long-lasting
theme in cybersecurity. With the emergence of public online
malware scanning platforms such as VirusTotal, it has been
reported in recent years that some in-development malware
were found on VirusTotal before their major outbreaks. For
example, the very ﬁrst known LeakerLocker sample could
data back to November 2016 when it was submitted to
VirusTotal [1], but not until July 2017 did security analysts
ﬁnd it widespread. The study conducted in [2] shows that
many malware writers have been anonymously submitting
in-development malware samples to online malware scan-
ning platforms. Since such platforms are hosting a com-
prehensive collection of state-of-the-art malware scanners,
the scan reports would provide malware writers with a
best possible assessment. In addition, through a continu-
ous submit-and-revise process, an in-development malware
sample can evolve in such a way that when it was released
into the wild and known to public, no signature was avail-
able and all the malware scanners running on VirusTotal
mistakenly reported benign. Since it usually takes more
than six months to generate well-crafted signatures [3], the
above-mentioned evasion strategy renders a very serious

•

Lan Zhang and Peng Liu are with the Department of Information Science
and Technology, Penn State University, State College, PA, 16801.
Lan Zhang: lfz5092@psu.edu
Peng Liu: pxl20@psu.edu

• Yoon-Ho Choi

is with Pusan National University. Email:

yhchoi@pusan.ac.kr

• Ping Chen is with Fudan University. Email: pchen@fudan.edu.cn

Manuscript received Nov 13, 2020; accepted Feb 12, 2022.

security threat to the society.

In order to help solve this serious problem, new kinds
of malware scanners (e.g., [4], [5]) have been proposed in
recent years to complement the existing scanners, and a key
characteristic of these new malware scanners is that they
all leverage machine learning, especially deep-learning (DL)
models, to detect malware. The main merits of DL-based
malware scanners are reﬂected in the following comparison:
(a) While signature-based malware scanners often require
substantial manual effort to extract signatures, DL-based
malware detection models are automatically trained; (b)
While signature-based malware scanners usually require a
signiﬁcant amount of domain knowledge, DL-based mal-
ware detection models could be well trained with minimum
domain knowledge; (c) Due to the generalization ability
of DL-based malware detection models, a malware sample
revised (by its malware writer) to avoid having any known
signatures could still be detected.

Since the DL-based malware scanners could make the
above-mentioned evasion strategy ineffective, the malware
writers must answer three key questions. Question 1: Are
the current malware revising techniques (e.g. code obfusca-
tion) still effective? If not, why? Question 2: If the current
malware revising techniques are no longer very effective,
how to revise a malware sample in a new way? Question 3:
How to ensure that the new way of malware revising will
not introduce too much extra burden?

Although how to evade DL-based malware detection
models has been attracting an increasing interest in the
research community, the existing works are still very limited
in answering the three questions. Almost all the existing
works (e.g., [6], [7], [8]) focus on investigating how CNN
(convolutional neural network) or RNN (recurrent neural
network) detection models could be evaded by adversarial

 
 
 
 
 
 
IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

2

examples. Although these works have made substantial
progress on answering Question 2, most of them could
not answer Question 1 or Question 3 due to the following
reasons. First, to avoid changing the malware semantics
(e.g. functionality of the malicious code), most existing
works restrict the adversarial examples by only modifying
certain metadata, such as PE header metadata and Section
metadata[6], or injecting some bytes [9]. Second, although
code obfuscation techniques such as polymorphic malware
have been playing a very dominant role in real-world to
evade malware scanners, very few existing works study
the relationships between code obfuscation and adversarial
examples. Since byte-level data (e.g., a malware sample is
largely viewed as a stream of bytes) are used, the trained
CNN and RNN models are largely a black box and very
hard to be explained. Accordingly, the semantics of the
generated adversarial examples are very hard to be linked
with code obfuscation (unless the models are assumed to be
white-box). Hence, it is very difﬁcult to answer Question 3
and reduce the extra burden by piggybacking the generation
of adversarial examples on code obfuscation.

In this work, we seek to answer these questions in a
systematic manner. Our new insight is as follows: instead
of using CNN or RNN models, Graph Neural Network
(GNN) malware detection models (e.g., [10], [5]), which are
as effective as CNN and RNN models, can be leveraged to
discover and understand the inherent relationships among
code obfuscation, the dominant technology in malware
revising, and adversarial examples, the dominant concept
in evading a deep neural network. These inherent relation-
ships then enable us to answer the three key questions. To
the best of our knowledge, this is the ﬁrst work investigating
how GNN based detection models could be automatically
evaded by revising the basic blocks of a malware sample in
a semantics-preserving way.

Our insight is gained based on two observations. First,
since Control Flow Graphs (CFG) play an essential role in
both code obfuscation and training of GNN based detection
models [5], CFGs are a natural connection between code ob-
fuscation and adversarial examples. Second, since CFG and
basic block data are used to train a GNN based detection
model, some adversarial examples are inherently related to
basic-block-level code obfuscation. To systematically answer
the aforementioned key questions, we face the following
challenges: 1) Given a malware sample, the features in its
graph structure involve discrete data, so we cannot directly
leverage gradient-based attacks to generate inﬁnitesimal
small perturbation to its original CFG. 2) The generated
adversarial examples should not change the original mal-
ware semantics. In each adversarial example, which is also
a malware sample, the attacker must preserve exactly the
same functionality as the original malware sample. Thus,
the manipulation should not remove original features in-
cluding edges and nodes.

To address these challenges, we proposed a novel
method to automatically generate adversarial examples for
GNN malware detection models while preserving the orig-
inal malware functionality and semantics. We designed a
reinforcement learning approach, namely the Semantics-
preserving Reinforcement Learning (SRL) attack, to generate
adversarial examples. The key factor of adversarial mal-

ware generation via semantic Nops injection is to select the
appropriate semantic Nops and their corresponding basic
blocks. However, since the decision making process involves
discrete values, we cannot directly apply gradient based
attack to make such decisions. The proposed SRL attack uses
reinforcement learning to automatically make the above-
mentioned decisions. These decisions result in sequentially
injecting semantic Nops into the CFGs. Since semantic Nops
will never change malware functionality, the SRL attack
achieves semantics-preserving.

To evaluate the proposed SRL attack, we let it be com-
pared with four baseline attacks: the binary diversiﬁca-
tion attack[7], the Semantics-preserving Random Insertion
(SRI) attack inspired by classical code obfuscation meth-
ods [11], the Semantics-preserving Accumulated Insertion
(SAI) attack inspired by hill-climbing methods [7], and
the Semantics-preserving Gradient based Insertion (SGI)
attack inspired by FGSM attacks[12]. For this purpose,
we extracted CFGs from 8,000 benign and malicious real-
world Windows programs and constructed abstract directed
graphs to represent the CFGs. We then trained two kinds of
GNN models, the basic GCN model [13] and the DGCNN
model[10], respectively, to classify the graphs.

We found that the four baseline attacks are limited in
evading GNN malware detection models. The experimental
results have shown that the binary diversiﬁcation attack
achieved less than 10% on the basic GCN models and the
DGCNN models. The SRI attack achieved an evasion rate
of 45% on the basic GCN models and 72% on the DGCNN
models. The SGI attack fooled the basic GCN model with
an evasion rate near 41% and deceived the DGCNN models
with evasion rate 83%. The SAI attack achieved over 90%
evasion rate on both models. In contrast, the proposed SRL
attack achieved 100% evasion rate on both the basic GCN
models and the DGCNN models. We added the adversar-
ial samples and retrained the detection models to defend
against those attacks. The retrained models can achieve
similar detection accuracy, but the evasion rates of the SRI
attack, the SAI attack , and the SGI attack signiﬁcantly
dropped to 2%, 16%, and 0.7%, respectively. In contrast, the
evasion rate of the proposed SRL attack dropped to 85%,
which is still a fairly high evasion rate.

In summary, we have made the following contributions:
1) To the best of our knowledge, this is the ﬁrst work on
semantics-preserving black-box attacks against GNN mal-
ware detection models. The proposed SRL attack achieves
semantics-preserving using reinforcement learning to se-
lect semantic Nops and their corresponding basic blocks.
2) Using both the basic GCN malware detection models
and the DGCNN models, we evaluated and demonstrated
the effectiveness of the proposed SRL attack via extensive
experiments with different settings. We built a baseline with
four attacks, and thoroughly compared the proposed SRL
attack with the baseline attacks. The results show that the
proposed SRL attack is signiﬁcantly more effective than the
baseline attacks.

The remaining of the paper is organized as follows. In
Section 2, we review some background of malware detection
and graph neural networks. The threat model and problem
statement are presented in Section 3. The proposed SRL
attack is described in Section 4. The proposed attack is

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

compared with three baseline attacks in Section 5. In Section
6, we discuss the related works. Finally, we conclude the
paper in Section 7.

the following propagation rule:

Hl+1 = σ( (cid:101)D−1 (cid:101)AHlW )

3

(1)

2 BACKGROUND

In this section, we provide a background of the proposed
attack.

2.1 Malware Detection

Malware detection models primarily make use of analysis
techniques to understand the intention of malware. Features
leveraged in malware detection can be grouped into three
categories: static features, dynamic features, and hybrid
features. Static features are extracted without running the
executable ﬁles. Dynamic features are extracted by analyz-
ing the behaviors of a program while it is being executed in
a simulated and monitored environment. Hybrid features
combine both static and dynamic features.

Various approaches are deployed to extract static fea-
tures. Some of them make use of the binary ﬁle itself
as indicators to detect the malware[14]. The characteris-
tics of the binary ﬁles, such as PE import features, meta-
data, and strings, are also ubiquitously applied in malware
detection[4]. Others leverage reverse engineering, including
instruction ﬂow graph, control ﬂow graphs, call graph, and
opcode sequences[4], [15], is used to understand the pro-
grams’ architecture and extract related features. Dynamic
analysis executes the programs in a virtual environment
to monitor their behaviors and observe their functionality.
Features obtained by dynamic analysis are API calls, system
calls, registry changes, memory writes, network patterns,
etc[16], [17]. Dynamic analysis can address some obfus-
cated malware and hence provide more accurate programs’
behaviors. In line with the static analysis, attackers adopt
approaches to prevent malware from dynamic analysis[18].
The malware starts an early check and immediately exits if
it runs on virtual machines. Some malware even executes
benign behaviors so humans draw incorrect conclusions
about the intent of the malware.

Because the signature-based detection method is not re-
silient to slight variations, researchers have applied conven-
tional machine learning algorithms (e.g., Random Forest)
and deep learning method to detect malware. Convolutional
neural networks and fully connected dense layers are lever-
aged to learn the high level features out of the selected
features[14]. For sequential data such as API calls and
instruction sequence, recurrent neural networks (RNNs) are
applied to classify the malware [17], [16]. Recently some
researchers use graph neural networks to classify malware
programs represented as control ﬂow graphs[5]. The struc-
ture information contained by CFGs can be used to ﬁnd
unreachable code, ﬁnd syntactic structure (like loops), and
predict programs’ defect[19], [20].

2.2 Graph Neural networks

Here, (cid:101)A = A + I is the adjacency matrix A of the directed
graph G with added self-connections I. (cid:101)Dii = (cid:80)
j (cid:101)Aij is
its diagonal degree matrix. W is a layer-speciﬁc trainable
weight matrix. σ(·) is a nonlinear activation function. Hl is
the matrix of activations in the l-th layer; H0 = X, where X
denotes the node information matrix of graph G.

The graph convolutional layer propagates node features
to neighboring nodes as well as the node itself to extract
local substructure information. We stack multiple graph
convolution layers to get high-level substructure features.
For the basic GCN model, we add a classiﬁcation layer
after node embedding to extract graph features. The clas-
siﬁcation layer simply ﬂattens the high-level substructure
features and adds a fully-connected layer followed by a
nonlinear activation function. For another model, we follow
the same architectures as the DGCNN model [10]. First, we
concatenate the output of multiple GCN layers. Then, we
use the SortPooling layer to sort the features followed by 1-
D convolutional layers and dense layers to learn the graph-
level features.
2.3 Reinforcement Learning

As a decision-making algorithm, Reinforcement Learning
(RL) applies to many real-life sequential decision-making
tasks under uncertainty with the objective of optimizing
reward. The RL tasks can be formed as a ﬁnite horizon
markov decision process (MDP) that contains four compo-
nents: 1) discrete set of environment states S; 2) discrete set
of environment actions A; 3) transition function T that cal-
culates the transition probabilities between states; 4) reward
function R that provides short-term rewards of the state. At
each time step t, the agent takes an action at from the action
set A for current state st and the environment provides a
reward r(st, at). According to the transition probability and
the reward, the state transitions to the next state s(t+1). The
goal of the RL agent is to learn a policy π(s, a) ∈ Π to decide
which action to perform in a given state, where Π is a set of
policies. Under a given policy π, the Q-value of an action a
with a state s is

Qπ(s, a) = E[r1 + γr2 + ...|s, a, π],

(2)

where γ is the discount factor that trades off the importance
of immediate and future rewards. Q-learning ﬁts Bellman
optimality equation for the Q-value function as follows:

Y Q
t = r(st+1, at+1) + γ max
a(cid:48)∈A

Qπ(st+1, a(cid:48); θt),

(3)

where θt refers to the parameters that deﬁne the Q-values
at the t-th iteration. The deep Q-network (DQN) algorithm
trains a neural network Q(s, a; θ) to select actions for each
state. The DQN algorithm uses a target network, with
parameters θ− to update the target value:

In this work, we study attacks targeting malware detection
models built from CFG-represented data.

To embed structural information inherent in graph like
data, we use two graph neural network: the basic GCN
model[13] and the DGCNN model[10]. In alignment with
the DGCNN model[10], our graph convolution layer takes

Y Q
t = r(st+1, at+1) + γ max
a(cid:48)∈A

Qπ(st+1, a(cid:48); θ−

t ),

(4)

The parameters θ− are updated every C iterations with
θ− = θ. The experience replay in an online setting keeps
). Every T time step, a mini-batch
past experience (s, a, r, s
experience is selected within the replay memory and used

(cid:48)

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

4

to train the Q network. Here, the loss function to update
parameters θ is the square loss:

LDQN = (Q(s, a; θ) − Y Q

t )2.

(5)

2.4 Code Obfuscation

Code obfuscation tools serve two main purposes: (1) to pro-
tect intellectual properties; (b) to evade malware detection
systems. There are a variety of code obfuscation schemes.
A basic requirement is that the program semantics must be
preserved after the code is transformed by such tools. Tradi-
tionally, attackers use obfuscation methods, including dead-
code insertion, register reassignment, subroutine reordering,
instruction substitution and so on, to morph their malware
to evade malware detection[11]. Here we list the deﬁnition
of some obfuscation methods: 1) Semantic Nops Insertion:
inserting certain ineffective instructions, e.g., N OP , to the
original binary without changing its behavior; 2) Register
Reassignment: switching registers while keeping the pro-
gram code and its behavior same, such as registers EAX
in the binary are reassigned to EBX; 3) Instruction Sub-
stitution: replacing some instructions with other equivalent
ones, for example, xor can be replaced with sub; 4) Code
Transposition: reordering the sequence of the instructions.

Code obfuscation is effective to evade the signature-
based detection system because it could signiﬁcantly change
the syntactic of original malware. Although code obfus-
cation tools have enabled attackers to successfully evade
various real-world malware detection systems, the unique
capabilities of deep learning models, especially graph neural
networks, indicate that the evasion ability of code obfusca-
tion tools can no longer be taken for granted. The exper-
iments in our paper demonstrate that randomly inserting
dead instruction can not achieve a good result compared to
other attacks with the help of prediction conﬁdence.

3 PROBLEM STATEMENT
To answer the overarching research question, ”How effec-
tively can we exploit the potential shortcomings of the deep
learning malware detection models?”, we studied the resilience
of GNN malware detection models over CFG-represented
malware. After showing how to represent malware using
CFG, we formulate the evasion problem based on the code
obfuscation methods.
3.1 CFG-Represented Malware Model

A CFG is a directed graph representation that illustrates all
reachable paths of the program during execution. Nodes
of the CFG represents the basic blocks of the program.
Each basic block is a consecutive, single-entry code without
any branching except at end of the sequence. Edges in the
CFG represent possible control ﬂow in the program. Control
enters only at the beginning of the basic block and leaves
only at the end of the basic block. Each basic block can have
multiple incoming/outgoing edges. Each edge corresponds
to a potential program execution. By using only opcodes
such as M OV , ADD or JM P as features of the instruction
after ignoring all operands, we abstract a basic block. For
example, sub %eax, %ebx and sub [%ecx], %edx would be
both represented by sub in a basic block. This is because
some instructions, such as JM P and IN C, share the same
bytes, but they have different semantic meanings. Also,

operands or instruction values result in large syntactic dif-
ferences, e.g. different registers being used, but the semantic
meaning of the instructions can be similar.

From the executable ﬁles, we generate a CFG G =<
V, E >, where vi ∈ V and eij ∈ E. Each node vi represents
a basic block in a CFG, while a directed edge eij points
from the ﬁrst basic block vi to the second basic block vj. We
also adopted the concepts of Bag-of-words (BoW) in NLP
to vectorize the basic blocks. First, we map n opcodes of
the x86 instruction set to a list S = {s1, s2, ..., sn}, where
si is a particular opcode. After counting the occurrence of
opcodes in the basic block vi, a basic block vi in a CFG
G =< V, E > is transformed as an array of integer counts
with size same as S. Let vi = {x1...xn}, a vector of counts
over each opcode. xk is set as the occurrence if the k-th
opcode exists in the basic block, otherwise, it is set as 0.

In the context of the adversarial malware functionality,
we can not directly change edges in the CFG to modify
the adversarial malware. To preserve the functionality of
the malware, we limit manipulation on the malware into
semantic Nops insertion. That is, when deﬁning a list of
semantic Nops that will not affect the program functionally,
we do not change the structure of the original CFG.
3.2 Problem Formula

In this paper, we consider the black-box setting where the
attacker can only receive the ﬁnal estimation(probability)
results from the malware detection model C. Even though
a malicious CFG G =< V, E > can be correctly labeled as
a malware by the pretrained and ﬁxed malware detection
model, the attacker aims to safely manipulate the basic
blocks and generate an adversarial graph (cid:101)G =< (cid:101)V , E >
to deceive the malware detection model C.

Let G =< V, E > be a given graph, where V ∈ Rm×n
and E ∈ Rm×m, m is the number of nodes and n is the num-
ber of opcodes in the instruction set. The semantic Nops are
encoded using the aforementioned method in Section 3.1.
The manipulation on malware is small and imperceptible
perturbations that should not change the program’s original
functionality. We deﬁne a list of dead instructions ζ ∈ RI×D,
where I is the number of dead instructions. By selecting
and inserting the dead instructions into the original sample
G, we generate an adversarial sample (cid:101)G =< V + δ, E >.
The adversarial sample has the same graph structure as the
original sample. The proposed semantics-preserving attacks
aim to maximize the probability of the target label under
the constraints that at most ∆ instructions can be injected.
Thus, an adversarial example is generated by solving the
following constrained optimization problem:

argmin

C( (cid:101)G, (cid:101)y; θ)
(cid:101)G
s.t. d(G, (cid:101)G) ≤ ∆,

(6)

where (cid:101)y is the target label, C(·) is the malware detection
model with parameters θ, and d(·) is the distance function
to calculate the number of injected instructions.
4 ATTACKS ON GRAPH-BASED MALWARE DETEC-

TION MODEL

To solve the optimization problem in Eq. 6, we designed
four semantics-preserving attacks against GNN for malware
detection. While one is a semantics-preserving reinforce-

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

5

ment learning attack which is designed by sequentially
injecting semantic Nops into the CFG, the other three attacks
are semantics-preserving attacks designed using the ideas
such as random insertion, hill-climbing, gradient-based in-
sertion methods respectively.

4.1 SRL Attack

The key factor of adversarial malware generation via seman-
tic Nops injection is to select the appropriate semantic Nops
and their corresponding basic blocks. However, since the
decision making process involves discrete values, we can
not directly apply gradient based attack to inject semantic
Nops. Reinforcement learning can be a valid approach to
attack GNNs for malware detection when modifying the
graph structure and node features [21], [22]. Thus, we design
a semantics-preserving reinforcement learning attack which
results in sequentially injecting semantic Nops into the
CFGs.

As shown in Figure 1, the RL agent iteratively chooses
basic blocks and semantic Nops while modifying the gen-
erated malicious graph until it successfully misleads the
malware detection model. During each iteration, the RL
algorithm records the topk basic blocks represented by
embedding the opcodes into the CFGs as described in
Section 3.1, the semantic Nops chosen by the RL agent
and the rewards calculated by feeding the generated CFG
to the detection model. The injected features do not affect
the program functionally, and change the structure of the
original CFG. Using a small batch of training records, the
RL agent learns which semantic Nops should be chosen
and which basic blocks should be modiﬁed. As a result, the
learned RL agent selects the modiﬁed basic blocks and the
semantic Nops for one CFG to maximize rewards and evade
the detection model. To solve the optimization problem in
Eq. 6, we design reinforcement learning environment and
reward as follows:
State. The state st at time t represents a CFG Gt =< Vt, E >
with some of the manipulated basic blocks.
Action. Each action includes two folds: 1) the importance
of the basic blocks vt; 2) a dead instruction at. The action
space of picking up a dead instruction is O(I), where I is
the numbers of semantic Nops. Similar to [7], below is the
rule we followed when generating the semantic Nops: 1)
Some atomic instructions that do not change the memory or
register value, e.g. N OP . 2) An invertible instruction, such
as arithmetic operation and logical operation, followed by
the inverse instruction, e.g. P U SH, P OP , ADD and SU B.
Reward. The ultimate goal of the reinforcement learning
model is to generate new samples that can misclassify the
detection model. In practice, the decision process will take
long to ﬁnd the right action during training process. Thus,
we calculate the rewards of each state as an intermediate
feedback. If one CFG can successfully avoid detection, the
reward rt is associated with the action sequence length. We
design the guiding reward rt to be one if it increases the
probability pst,at,vt = C(st, at, vt, (cid:101)y) of successfully evading
the prediction model after being recognized as the target
label (cid:101)y, and to be zero otherwise.

rt(st, at, vt) =

(cid:26)1;
0;

if pst,at,vt > pst−1,at−1,vt−1
otherwise.

(7)

Initialize replay memory buffer M

Algorithm 1 SRL attack against malware detection
Input: C(·), (cid:101)y, N opsList, topk, niters, ∆, T , C
1:
Initialize Q(s, a, v) with random parameters θ
2: Set target function ˆQ with parameters θ− = θ
3:
4: for each G =< V, E > do
5:
6:
7:
8:
9:
10:

t ← 0
st ← G
while argmax(C(st)) (cid:54)= (cid:101)y and t < niters do

With probability (cid:15) select a random action at and vt
Otherwise select at and vt by the Q network
Insert the action at into topk basic block vt to get new graph G(cid:48) as
st+1
Compute rt by feeding G(cid:48) to C
if Dif f (st+1, st) > ∆ then

t ← niter
rt ← 0

end if
Calculate the absolute temporal difference(TD) error |δt|
Store {st, at, vt, rt, st+1, |δt|} in memory M
if Every T queries then

Sample minibatch transitions {sj , aj , vj , rj , sj+1}
Calculate the target value and the square loss based on the target
function ˆQ and the reward rj
Update parameters of the Q network

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

end if
if Every C queries then

Reset ˆQ = Q

21:
22:
23:
24:
25:
26:
27:
28:
29: end for

end if
st ← st+1
t ← t + 1

end while

Terminal. Once the injected instructions reach the budget
∆, or current state can be misclassifed, the process stops.

As other work on adversarial graph generation[21], the
policy network used to learn the MDP is Q-learning, speciﬁ-
cally the deep Q-network (DQN) algorithm. In Algorithm 1,
we show the operational procedure of the DQN algorithm
for generating the proposed SRL attack. The RL agent it-
eratively chooses basic blocks and dead instructions while
modifying the malicious input until it successfully misleads
the malware detection model. First, a CFG is represented as
a state s0. Second, for each turn t, the RL agent chooses an
action at ∈ A to decide which dead instruction should be
inserted into the observable environmental state vector st.
For each malicious CFG, the RL agent chooses topk basic
blocks based on the output of the SortPooling layer and one
semantic Nop in the list of semantic Nops. To determine topk
basic blocks vt that will be manipulated at each iteration,
the RL agent sorts the basic blocks in the graph according to
their importance. The RL agent uses the SortPooling layer to
calculate nodes’ importance. The SortPooling layer extracts
and sorts the vertex features based on the structural roles
within the graph[10]. Here, the extracted vertex features
are the continuous Weisfeiler Lehman (WL) colors. As with
the DGCNN model, after we sort all the vertices using
the last output layer’s output, the topk sorted vertices are
chosen to inject the semantic Nops. The transformation to
insert at into the topk basic blocks is applied to the original
graph G. We get a new state G(cid:48) which is the input of the
malware detection model C to get the rewards rt ∈ R for
the action. We obtain the reward for the generated graph
G(cid:48) and store the current state, actions, rewards and next
state to the memory buffer M. The buffer records past
experience denoted as (st, at, vt, rt, s(t+1), |δt|) with states,
actions taken at those states, the rewards and the next
state and its absolute temporal difference(TD) error. We use

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

6

the prioritized experience replay technique with memory
buffer M to train the Q network. For every T queries,
a mini-batch set of samples from the memory buffer are
selected with probability P (i) calculated by the normalized
TD error. The algorithm runs until niters iterations, the
inserted features is more than the deﬁned budget ∆, or the
generated samples are misclassiﬁed as the target label (cid:101)y.
Each attack iteratively modiﬁes a malicious CFG until the
malicious CFG is misclassiﬁed as a benign program or the
maximum number of iterations reaches.
4.2 Other Semantics-preserving Attacks

In this section, we introduce the proposed other three at-
tacks: (1) Semantics-preserving Random Insertion(SRI) at-
tack using the idea of random insertion; (2) Semantics-
preserving Accumulated Insertion(SAI) Attack using the
idea of hill-climbing; and (3) Semantics-preserving Gradient
based Insertion(SGI) Attack using the idea of gradient-based
insertion.
4.2.1 SRI Attack
Algorithm 2 in the appendix shows the pseudocode of the
SRI attack. In each iteration, after randomly choosing topk
basic blocks in the CFG G =< V, E >, the algorithm picks
up one dead instruction in the list of semantic Nops. Next,
the algorithm inserts the dead instruction into the chosen
basic blocks. Here, to limit the numbers of manipulation on
the original CFG, difference between the generated CFG and
the original CFG should be less than the predeﬁned value
of ∆. The algorithm repeats until niters iterations, because
the SRI attack may be identiﬁed due to the large number of
queries on the malware detection model.
4.2.2 SAI Attack
Instead of directly transforming the original binary with
the random decision as in Algorithm 2, the SAI algorithm
follows a hill-climbing approach[7]. The SAI algorithm de-
clines some decisions if the probability of the target class
identiﬁcation decreases. For each CFG, attacker probes the
detection model C(·) to retrieve the probability p of the tar-
get class. Next, in each iteration, attacker randomly chooses
topk basic blocks and one dead instruction in a list of
semantic Nops N opsList. Attackers query the detection
model using the transformed CFG to obtain the changed
probability of the target class, p(cid:48). The transformation is
accepted only if the probability increases. Similar to the
SRI attack, the SAI attack can also limit the number of
effected basic blocks in each iteration and the total number
of inquiries. Manipulation on the original CFG should also
be less than ∆. However, attackers require the predicted
probability of the targeted model to apply the SAI attack.
4.2.3 SGI Attack
The SGI attack solves the constrained optimization problem
in Eq. 6 with a gradient-descent algorithm. To ﬁt the black
box setting, we train a substitute model C(cid:48) which approxi-
mates decision boundaries of the malware detection model
C [23]. We assume that the attackers have some fundamental
knowledge of the malware detection model including the
input and the expected output. The substitute model C(cid:48) is
trained iteratively with the graphs and the predicted labels.
The substitute model C(cid:48) is used when generating adver-
sarial samples. In Algorithm 3 (in the appendix), we show
the pseudocode for generating the adversarial samples.

Here, N is the number of basic blocks in the CFG, and K
is the number of semantic Nops. As shown in Algorithm 3
when generating adversarial samples, the adversary com-
putes the perturbation, e.g., the signed gradient from the ith
iteration, as follows:

gi = sgn(

∂JC(G, y)
∂V

),

(8)

where V is the feature matrix, each row of which describes
instructions in a basic block vi of the graph G, and y is the
label of the CFG. Attacker heuristically inserts a semantic
Nops that is closest to the gradient gi into the corresponding
basic block of the CFG. In each iteration, attacker injects the
closet semantic Nops to the sign gradient descent. Attacker
repeats this procedure until a maximum number of itera-
tions T .
5 ASSESSING THE RESILIENCE OF GNN FOR MAL-

WARE DETECTION

We conducted experiments to answer the overarching re-
search question, ”What kind of manipulations can be applied
on original CFGs to avoid the malware detection models without
changing the programs’ behaviors?”. We evaluated the perfor-
mance of the proposed semantics-preserving attack under
various parameters such as the impacts of graph size and
the semantic Nops. From the experiments, we observed
that: 1) the proposed semantics-preserving attack achieves
the high evasion rate; 2) it generates small-scale manipu-
lations on original features to succeed attacks; 3) it does
not change malicious behaviors while avoiding malware
detection model.
5.1 Experimental Environment

To evaluate the performance of the proposed semantics-
preserving attacks, we conducted experiments using the
malware collected from the VXHeavens Dataset [24] and
the VirusShare website [25]. We used VirusTotal [26] to label
the malware samples and identify the malware families.
We also used three families of Windows malware samples
such as Trojan samples such as Downloader and FakeAV,
Virus samples such as HLLC and HLLO, and Backdoor
samples such as Simda and Haxdoor. For benign programs,
we installed standard packages on a x86 Windows 10 virtual
machine using Ninite and Chocolatey2 package managers
and collected the benign binaries generated by those pack-
ages. The categories of the installed packages vary including
popular applications,security applications,developer tools,
and so on. We use a Python framework for analyzing bina-
ries, called Angr[27], to extract CFG in those datasets. After
extracting CFGs, we transform the basic blocks in CFGs to a
directed graph. Because some graphs have millions of basic
blocks, it is impossible to train on the entire graph at once
due to GPU memory and training time constraints. In our
experiments, the CFGs with less than 3,000 basic blocks
are used for malware classiﬁcation. Next, we mix benign
programs and malware together, and randomly select 20%
samples as the test dataset. We trained the model on 70%
samples and validated the model on 10% samples. Also, to
reduce variability on a limited data sample, we trained the
evaluation model using 7-fold cross-validation.

All experiments are conducted on Ubuntu 16.04, using
Python 3.7 and Tensorﬂow 2.1 with NVIDIA GTX980 Ti

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

7

Fig. 1: Architecture of the SRL Attack

Graphics Processing Unit(GPU). We trained two GNNs as
malware detection models over CFG-based features: the
basic GCN model; and the DGCNN model. The basic GCN
model stacks four graph convolution layers with 128, 64,
32, 16 output channels and use the last graph convolutional
layer followed by a fully-connected layer. The DGCNN
model[10] concatenates four graph convolution layers with
32, 32, 32, 1 output channels, followed by a SortPooling
layer to keep top 1000 nodes. Two 1-D convolutional layers
with 16 and 32 output channels followed by one dense
layer with 128 hidden units are applied to obtain the graph
label. We trained two models for 200 epochs with the batch
size 100. In Table 1, we show the classiﬁcation performance
of the malware detection models used in the following
experiments. The result of other detection models are listed
in Table 8 (in the appendix).In Table 9 (in the appendix),
we show the composition of the dataset and the node size
distribution for each type of program. The mean value of
the Virus CFGs’ node sizes is smaller than the mean value of
node sizes of CFGs from other malware families and benign
programs, but generally all types contain large and small
graphs and are uniformly distributed in the train, validation
and test dataset. We generated 28 semantic Nops based on
rules mentioned in section 3.2. In Table 2, we show some
examples of semantic Nops.

TABLE 1: Classiﬁcation performance

Model

Basic GCN
DGCNN

ACC(%)
Val

93.61
93.61

Train

97.35
98.37

Test

93.92
95.77

FPR(%)

FNR(%)

5.52
4.70

6.62
3.73

5.2 Evaluation Results

To fool the malware detection model, we generated the
proposed semantics-preserving attacks by inserting seman-
tic Nops. By default, the parameter values for the attacks
are conﬁgured as follows. Each attack probes the detection
model less than 30 iterations. We set 5% as the maximum
injection budget. In the SRI, SAI, and SRL attacks, the
maximum effected basic blocks in each iteration is set as
1250. If the number of the nodes in a CFG is less than 1250,
one semantic Nop is inserted into all basic blocks each step.
The norm value of the distance function in the SGI attack is
set as 2 to calculate the semantic Nops that is close to the
direction of the gradient.

We compared our work with the black box algorithm
of the binary-diversiﬁcation attack [7]. The main concept of
the black-box attack is similar to the SAI attack: it queries

TABLE 2: Examples of semantic Nops

NOP
PUSH %rbx
NOT %rbx
XCHG %rax,%rax
ADD $5,%r10

POP %rbx
NOT %rbx
XCHG %rax,%rax
SUB $5,%r10

TABLE 3: Number of attack successes for different families
of malware samples

Attack

IPR
Disp-5
IPR+Disp-5
SRI
SAI
SGI
SRL

Trojan

Virus

Backdoor

ER(%)

Trojan

Virus

Backdoor

ER(%)

Basic GCN

DGCNN

0
6
7
140
293
120
298

0
1
2
77
160
44
162

2
2
2
156
300
153
311

0.25
1.16
1.42
45.58
97.27
41.22
100.0

10
30
29
216
288
239
297

2
3
9
112
158
127
163

6
15
13
233
303
255
315

2.32
6.19
6.58
72.25
96.74
83.08
100.0

∗ER=Evasion Rate

the detection model after transforming each function, and
accepts the modiﬁcation only if the probability of the benign
class increases. This attack implements two categories of
detection misleading binary transformations: the in-place
randomization (IPR) work of Pappas et al. [28] and the
code displacement (Disp), proposed by Koo and Polychron-
akis [29]. The implementation of the IRP transformations
is available at https://github.com/kevinkoo001/ropf. We
evaluated three variants of the binary-diversiﬁcation at-
tack [7]. The Disp-5 variant relies on the Disp transforma-
tions and increases binaries’ sizes up by 5%. The last variant,
IPR+Disp-5, combines the IPR and Disp transformations.
The target models of the binary-diversiﬁcation attack are
DNN models trained on raw bytes for malware binary
detection. To compare it with our method, after transform-
ing the binary, during each iteration, we ﬁrst extract the
CFG of generated binary and the graphed is represented as
explained in the Section III-A. Then we fed the new graph
to the GNN detection model to retrieve the probability of
the target class. We executed the black box attacks up to 200
iterations and stopped early if the binaries were successfully
misclassiﬁed, the size of the new CFG is more than 3000, or
the binaries’ sizes increase by 5%.

For the SGI attack, we trained two substitute models
using the validation dataset to approximate the malware
detection models[23]. The substitute models have the same
structure as the basic GCN model. Both of them are trained
for 10 epochs from scratch with CFGs and their correspond-
ing predicted labels. After 10 epochs, while the accuracy
of the substitute model for the basic GCN model reached
by 85.83% on the test dataset, the substitute model for the
DGCNN model showed an accuracy by 81.42%.

For the SRL attack, we used the RMSProp algorithm

RLAgent*!!+,-./012+0x44c160:ja0x44c162…0x44c162:loopne0x44c1610x44c164:jg0x44c0f2…0x44c0f2:pushes0x44c0f3:subdwordptr[eax], eax0x44c0f5:sbbbyte ptr[ecx+ 0x14], al0x44c0f8:addbyte ptr[eax], al0x44c0fa:addbyte ptr[eax], al0x44c0fc:addch, cl0x44c0fe:saldwordptr[esi], cl0x44c100:ja0x44c0ce…0x44c166:cmc0x44c167:adcal, byte ptr[eax]0x44c169:movesp, 0x43890x44c16e:addbyte ptr[eax], al0x44c170:addal, bh0x44c172:jmp0x44c171…0x44c0eb:adcal, byte ptr[eax]0x44c0ed:rclbyte ptr[edx-0x2c], 10x44c0f0:ja0x44c12e013451.6.17859:ℎjaLoopnejgpush…cmc…AdcRcljaGraphRepresentation0,1,…,0,0,00,1,…,0,1,00,1,…,0,0,01,0,…,0,0,00,1,…,0,0,00,1,…,3,1,02,1,…,0,1,00,0,…,1,1,01,1,…,0,1,00,0,…,2,1,00,1,…,3,1,02,1,…,0,1,00,0,…,1,1,01,1,…,0,1,00,0,…,2,1,00,1,…,0,0,00,1,…,0,1,00,1,…,0,0,01,0,…,0,0,00,1,…,0,0,02,1,…,0,1,02,1,…,0,1,00,1,…,0,1,00,0,…,1,1,00,0,…,1,1,00,1,…,0,0,00,0,…,2,1,00,0,…,2,1,00,1,…,0,0,0GeneratedGraph0,1,…,0,0,00,1,…,2,1,10,1,…,2,0,11,0,…,0,0,00,1,…,2,0,1DetectionModelBasicGCNModelDGCNNModelExtractParseFeedSortActionModifyRewardIEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

8

with minibatches of size 512. We trained the model with
malicious samples in the training dataset. Each graph can
query the detection model for at most 30 steps. We used a re-
play memory of 3000 most recent queries. The policy during
training continuously decayed with (cid:15) decaying linearly from
1 to 0.1 over the ﬁrst 3000 queries, and ﬁxed at 0.1 thereafter.
The agent drops actions that returned negative actions with
probability 50% instead of keeping every queries.

In Table 3, we show the number of attack successes and
the average performance for different families of malware
samples against two malware detection models using seven
attacks. The binary-diversiﬁcation attack [7] showed the
evasion rate less than 10%. We have two observations for the
binary-diversiﬁcation attack. 1) We observed the IPR based
transformation is not capable to attack the graph-based
models. First, the node features we used is the summation
of one-hot encoded opcodes within the basic blocks. Thus,
reordering instructions and reassigning registers will not
affect the representation of the node features. Second, it
only substitutes or reorders the instructions, which does
not modify the structure of the generated CFG. The graph-
based models, speciﬁcally the graph convolutional layers,
learn the high-level substructure features by propagating
node features to neighboring nodes and the node itself.
Thus, it is hard to evade the GNN based detection models
without modify the graph structure. 2) The Disp based
transformation create certain jump basic blocks and dead
code basic blocks. So the structure of the generated CFG
will be changed. Thus, it performs better compared with
IPR based transformation. However, the jump basic blocks
only contain one opcode and the dead code basic blocks will
never be reached, so they have small effect when calculating
the graph features.

The SRI attack showed the evasion rate by 45.58% on the
basic GCN model when inserting instruction by 0.96% on
average, and by 72.25% on the DGCNN model when 1.28%
of features changed on average. The SGI attack showed
the evasion rate by 41.22% on the basic GCN model and
by 83.08% on the DGCNN model. Overall, we observed
that SAI and SRL attacks have shown a good performance
in general. The SAI attack showed the evasion rate by
97.27% on the basic GCN model with 0.54% injected features
and 96.74% on the DGCNN model with 0.62% injected
features. In practice, SRI attacks took more time when
generating an sample compared with SAI attack on average.
This is because the SAI attack drops some transformation
and causes the misclassiﬁcation from the detection model
within fewer iterations. Also, the SGI attack costs longer
time because of the gradient calculation. The SRL attack
against both models showed the evasion rate by 100% with
0.17% injected features on the basic GCN model and 0.23%
injected features on the DGCNN model. SRL altered CFGs
by inserting semantic Nops into the corresponding basic
blocks.

To explain why the SRL attack achieves a high attack
rate, we analyzed the frequency of the inserted features.
The semantic Nops used in our experiments contains 28
opcodes. In Figure 5a (in the appendix), we show the oc-
currence frequency of those opcodes in our dataset. Even
though some opcodes such as CM OV A, CM OV L, and
CM OV N S are not commonly seen in the whole datasets,

there is a signiﬁcant discrepancy between benign and ma-
licious programs. For example, the frequency of appear-
ance of CM OV A in benign programs is 6.44e−5, which
is different from that in malicious programs 2.12e−5. In
Figure 5b (in the appendix) and Figure 5c (in the appendix),
we show the opcodes chosen by different attacks for the
DGCNN model and the basic GCN model. We run four
attacks on each detection model 10 times and calculate the
inserted opcodes of the adversarial samples. We observed
the SRL attack highly relies on certain opcodes that have
a substantial difference, such as CM OV G, CM OV A, and
CM OV S, to evade detection. The SRL attack seems to
strategically perform less frequently used opcodes, which
implies that the detection models learn those differences
as graph features to detect malware and the SRL attack
effectively ﬁnds the blind spot of the detection models.
5.2.1 Impact of Various Attack Parameters

In this section, we discuss the impact of various attack
parameters including the number of effected basic blocks,
maximum iterations, and insertion budget.

We suggest choosing the value of topk based on the
node size of the CFGs. If the value of topk is too large, it
causes large-scale manipulation on original features and can
affect the functionality of the program. On the other hand,
if it is too small, it requires more iterations to evade the
detection models. In practice, we can determine the value
of topk from the empirical experiments which compare
the performance under some reasonable values. In Figure
2a, we shows evasion rates of SRI, SAI and SRL attacks
under different numbers of effected basic blocks in each
iteration. Here, the number of iterations and the insertion
budget are set as 30 and 5% respectively. We immediately
observed that more effected basic blocks do not signiﬁcantly
improve the evasion rate after the number of effected basic
blocks reaches 1250. That is because when the number of the
effected basic blocks is more than the node size of the CFG,
one dead instruction is inserted to all basic blocks. With
more effected basic blocks, we observed that the evasion
rate tends to depend on other factors such as the number
of iterations and the insertion budget as shown in Figure 2b
and Figure 2c.

In Figure 2b and Figure 2c, we show how the evasion
rates on two detection models vary under different max-
imum iterations and insertion budget. Generally, we ob-
served that attacks with more iterations and higher budget
are more successful. Four attacks showed a good perfor-
mance at about 30 iterations. Also, most attacks showed
a good performance when insertion budgets reached 5%.
As one would expect, more iterations and insertion budget
change more features when generating an adversarial sam-
ple. For example, for SGI attack against the DGCNN model,
the number of changed features increased from 0.57% to
1.22% with the increase of the insertion budget on average.
Even though the insertion budget is small, e.g. 1%, we
observed that the SRL attack showed the high evasion
rate close to 100%. However, let us note that the evasion
rate is highly dependent on the initial value of the action-
value function. If the evasion rate before training the Q
function is less than 40%, most queries in the earlier train-
ing process will get negative rewards, and consequentially

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

9

a: Impact of the basic blocks

b: Impact of iteration

c: Impact of insertion budget

Fig. 2: Impact of various attack parameters

need more time when training the Q function for a good
result. Moreover, smaller iterations and insertion budget
restrict the length of action sequences, which may reduce the
number of queries with positive rewards. We also observed
that although a smaller number of iterations and insertion
budget obtained a good performance, it might take longer
time to train the Q function because most queries in the
beginning of the training process return negative rewards.
5.2.2 Impact of Graph Size

We measured the evasion rates of four attacks under various
size of graph using the test dataset, which is separated
into four groups according to the the number of the nodes
in a graph. As shown in Table 4, we consider the ﬁrst
quartile, the median, and the third quartile of the graph
size to group the test dataset. For the SGI attack, larger
graphs signiﬁcantly showed higher evasion rates. However,
from the other attacks, we did not observe any meaningful
relationship between the graph size and the evasion rate.
This observation indicates that the evasion rate is not highly
dependent on the graph size.
5.2.3 Impact of Dead Instructions

To evaluate the impact of dead instructions, we investigated
the performance of a single dead instruction with different
attacks on two detection models. In Table 5, we show impact
of some dead instructions with different semantic N ops.
First, we observe that some opcodes, such as CM OV A,
CM OV G, CM OV L and CM OV S, are useful for evading
the detection models. This is because the number of those
opcodes in the benign and malicious programs is different
and the detection models learned this discrepancy. Second,
with some dead instructions, such as N OT, N OT , three
attacks on different models also showed divergent evasion
rates. The key factor that leads to this different impact is two
models have different decision boundaries and the DGCNN
model holds more ﬁne-grained information collected from
the all graph convolution layers which lead it easier to be
evaded by inserting simple instructions. Third, we observed
that sometimes the SAI attacks can not show the same
performance as the SRI attacks. The SAI attacks reject some
insertions and consequentially reduce the action sequences.
This strategy achieves a high evasion rate when we do not
know which instructions are useful. However, it is hard for
reduced action lists to evade the malware detection models.
Also, some dead instructions may not cause the misclas-
siﬁcation. Even using those instructions as candidates, the
SAI and SRL attacks can reject those ‘useless’ instructions or

locate a better combination to deceive the detection model.

TABLE 4: Impact of graph size(%)

Attack

SRI
SAI
SGI
SRL

25%

44.44
98.06
10.14
100.0

Basic GCN
75%
50%

51.44
98.55
17.30
100.0

47.44
98.97
54.59
100.0

DGCNN

100%

25%

50%

75%

100%

53.64
94.79
86.97
100.0

70.81
94.73
54.02
100.0

71.29
95.83
82.87
100.0

77.94
97.43
84.10
100.0

79.47
95.78
88.94
100.0

TABLE 5: Impact of dead instructions(%)

ID

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Semantic Nops

NOP
SUB
ADD
LEA
MOV
XCHG
CMOVO
CMOVP
CMOVA
CMOVG
CMOVS
CMOVL
CMOVNS
CMOVNP
CMOVNO
ADD,SUB
SUB,ADD
NEG,NEG
NOT,NOT
PUSH,POP
PUSHF,POPF
XCHG,XCHG
BSWAP,BSWAP
PUSH,NOT,POP
XOR,XOR,XOR
MOV,ADD,MOV
INC, PUSH, DEC, DEC
MOV,CMP,SETG,MOVZX,MOV,MOV

SRI

28.51
0
54.91
14.57
7.97
0
0
0
98.14
100
100
99.25
0
0
0
0
0
87.17
0
2.24
0
0
0
0
0
3.73
4.23
98.38

Basic GCN
SAI

SGI

SRI

DGCNN
SAI

38.89
0
55.29
13.32
7.97
0
0
0
99.25
100
100
100
0
0
0
0
0
71.98
0
2.36
0
0
0
0
0
3.73
2.11
98.38

32.12
0
57.53
14.44
8.09
0
0
0
99.25
100
100
99.25
0
0
0
0
0
86.05
0
2.24
0
0
0
0
0
3.73
4.23
98.38

95.55
0.24
67.16
2.22
81.85
4.19
77.16
86.54
100
99.87
70.00
77.65
0
91.48
94.19
0.24
1.23
99.75
53.33
53.08
4.32
5.18
10.49
47.40
96.79
81.72
46.54
75.30

80.49
0.12
47.77
4.07
41.23
2.09
22.96
90.74
100
96.54
63.70
33.95
0
86.67
66.41
0.37
0.24
97.03
42.71
30.24
1.11
3.08
3.70
35.92
40.37
64.56
13.95
74.07

SGI

95.30
0.24
67.90
2.46
84.69
4.07
80.98
91.23
100
99.87
68.76
81.72
0
84.81
93.33
0.24
0.24
99.50
49.75
52.83
8.76
9.38
10.24
43.20
90.86
82.09
47.28
74.56

5.3 Potential Mitigation

In this section, we brieﬂy discuss a potential mitigation
method to make malware detection robust against the pro-
posed semantic-preserving attacks. To defend against the
proposed semantic-preserving attacks, we retrained the de-
tection model using the original training dataset together
with the adversarial samples generated by the attacks. For
each attack, we randomly selected 500 adversarial samples
and added them into the training dataset. As shown in
Table 6, detection models showed the same accuracy as
the original one after 200 epochs. For SRI, SAI and SGI
attacks, the evasion rate of the retrained model signiﬁcantly
decreased as shown in Table 7. Let us note that even though
such a defense method is effective, the cost of retraining
the detection model is large if the training set is large.
Also, to decrease the susceptibility against those attacks, we
recommend to design a malware detection system which
combines multiple detection algorithms.

5001000150020002500300035000.40.50.60.70.80.91.0Attack RateSRI + Basic GCNSRI + DGCNNSAI + Basic GCNSAI + DGCNNSRL + Basic GCNSRL + DGCNN5101520253035400.20.40.60.81.0Attack RateSRI + Basic GCNSRI + DGCNNSAI + Basic GCNSAI + DGCNNSGI + Basic GCNSGI + DGCNNSRL + Basic GCNSRL + DGCNN0.010.020.030.040.050.060.070.080.20.40.60.81.0Attack RateSRI + Basic GCNSRI + DGCNNSAI + Basic GCNSAI + DGCNNSGI + Basic GCNSGI + DGCNNSRL + Basic GCNSRL + DGCNNIEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

TABLE 6: Classiﬁcation performance after retraining the
model

Model

Basic GCN
DGCNN

Train

98.96%
98.71%

ACC
Val

Test

FPR

FNR

93.33%
93.61%

94.70%
95.23%

4.94%
3.41%

5.66%
6.14%

TABLE 7: Impact of a potential mitigation method based on
retraining

Attack

Basic GCN

DGCNN

ER(%)

FG(%)

ER(%)

FG(%)

SRI
SAI
SGI
SRL

1.05
13.24
0.24
27.22

0.91
0.94
2.94
1.20

2.02
16.96
0.73
85.74

1.01
0.94
1.03
0.83

5.4 Discussion

The proposed SRL attack generates a new CFG by inserting
semantic Nops into the CFG, so it does not generate actual
malware. In practice, the inserted feature will increase the
size of each basic block which also changes the position
of other code and data. Generally speaking, there are three
methods to modify the original binary based on the gen-
erated adversarial CFG [30]: (1) Source code level[31]: the
compiler is modiﬁed to insert enough space and generate
certain types of instructions for the basic blocks; (2) Interme-
diate code level [32]: the target, e.g. jump table, is modiﬁed
using intermediate language such as LLVM to automatically
adjust the addresses; (3) Machine code level [33], [34]: the
last instruction of each basic blocks can be replaced as a
jump instruction to jump to a trampoline function which
helps to calculate the real address. It should be noticed that
after those methods are applied, the functionality of the
original malware will not be changed.

In our current experiment, the SRL attack is adaptive for
different detection models. The generated perturbations are
different for the basic GCN model and the DGCNN model.
This indicates the SRL attack certainly could fool other
graph based detection models. To further test the hypothe-
sis, we trained another graph-based detection model, called
the GAT (graph attention networks) model, and applied
four attacks on it. The key difference between the GAT
model and the GCN model is how they generalize the in-
formation of the nodes and their neighborhoods. The GCN
model calculates the normalized sum of the node features
of neighbors. For the GAT model, instead of applying the
graph convolution operation, it performs the self-attention
mechanism on the nodes by computing the attention coefﬁ-
cients of each node over their neighborhoods’ features. The
GAT model stacks four graph attentional layers to extract
the graph features. After adding one classiﬁcation layer to
summarize the substructure features, the GAT model makes
predictions for the CFGs. The model achieved an accuracy of
88.62% on the test dataset. For the SGI attack, the structure
of the substitute model of the GAT model is the same as the
basic GCN model and it can obtain an accuracy of 82.32%
on the test dataset. Also, the SRI, SAI, SGI, and SRL attacks
achieved 18.79%, 81.76%, 84.43%, and 98.03% respectively.

6 RELATED WORK

In this section, we introduce research on attacking neural
networks and malware detection models.

10

6.1 Attacks on Neural networks

Adversarial samples are generated by adding imperceptible
perturbations to original samples to deceive deep learning
algorithms. Attacks can be grouped into three scenarios
based on their knowledge: white-box attack, black-box at-
tack, and semi-white (gray) box attack.

In the white-box scenario, attacks have access to the
architecture and parameters of the neural networks. The
FGSM attack[12] is a fast method to generate adversarial
samples.Carlini and Wagner [35] proposed gradient-based
attacks to generate adversarial samples by calculating one
back-propagation step. In a black-box attack setting, the
architecture parameters of the neural network is unavailable
to attackers. Attackers only have the query access to gener-
ate adversarial samples. Papernot et al. [23] design a substi-
tute neural network to ﬁt the black-box neural network and
then generated adversarial examples according to the sub-
stitute neural network. This method assumes the attackers
can only obtain the label information from the target neural
network. Zeroth order optimization based black box attack
has a different assumption that the attackers have access
to the prediction conﬁdence (score) from the target neural
networks[36]. In the grey-box setting, attackers have access
to the the structure of the target model. Generative Adver-
sarial Network(GAN) is introduced to generate adversarial
examples directly from the generative network[37].
6.2 Attacks on Malware Detection

Adversarial malware generation contrasts with previous
applications of adversarial sample generation in computer
vision because most features in malware detection involve
in discrete data, which means the gradients to train the
generator are zero almost everywhere. There are two con-
straints for adversarial malware generation: 1) large-scale
manipulations on original features may change the pro-
gram’s functionality; 2) generated samples should not re-
move original features[38]. Multiple attacks are introduced
to evade deep learning models. Some of them modify orig-
inal malware, for example, add benign code to malware,
to mimic benign program[39], [9], [7]. Anderson et al.[6]
modify PE header metadata, Section metadata, Import and
Export Table metadata, etc. by reinforcement learning to
evade static PE machine learning malware models. Park et
al.[8] generate the executable adversarial malware examples
by inserting semantic Nops to evade CNN based detection
models. Other attacks focus on the gradient of the detection
model or the substitute model to tweak some features of
the target model, e.g. adding new API calls [40]. The evo-
lutionary computation techniques are used to develop new
variants of mobile malware[41].

Although researchers have been developing graph based
neural network models for malware detection, no systematic
study on whether such models can be attacked. Unlike to
image data, it is harder to generate adversarial samples on
graph-based data for two reasons: 1) the graph structure is
discrete so we cannot use inﬁnitesimal small perturbation,
and 2) large graphs can not easily be veriﬁed visually. The
attacks on graph based model can be grouped into two
categories: gradient-based attack and non-gradient-based
attack[42]. Gradient-based attacks retrieve or estimate the
gradient information of the detection model to modify the

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

11

original samples. Chen et al.[43] introduced a network
embedding attack that uses the gradient information of
the adjacency matrix to iteratively add or delete some key
links. Z ¨ugner et al.[44] ﬁrst proposed a method N ettack
to perturb the graph data to perform poisoning attack on
GCN model. Non-gradient-based attacks solve the graph
based optimization problem without using the gradient of
the detection model. Dai et al.[21] proposed a reinforcement
learning based attack method RL-S2V that learns to modify
the graph structure by sequentially adding or dropping
edges from the graph. Wang et al.[45] developed two al-
gorithms, Greedy and Greedy-GAN, to attack GCN models
by adding fake nodes into the original graph. Wang et
al.[46] formulated a graph-based optimization problem to
manipulate edges and solved the problem using projected
gradient descent method. Sun et al.[22] extend the idea of
RL-S2V to sequentially add fake nodes, introduce fake links,
and modify the labels of fake nodes.

Differently from some of the prior works, e.g. [7], [40],
our target models are graph based deep learning models
from CFGs. The generated samples should not change the
program’s functionality. So we can not arbitrarily add or
remove the edges and nodes features of the CFGs. In our
paper, we focus on generate adversarial CFGs by iteratively
insert semantic Nops into original graphs. Perhaps most
closely related to our work on evading CFG based deep
learning model[47]. First, instead of using graph neural
networks, they extracted some indicators such as closeness
centrality, density, and betweenness centrality, to represent
the CFG and constructed a detection model. Because those
features are continuous value, they can directly apply meth-
ods of white box attack to generate adversarial samples.
However, those generated adversarial samples can not be
reverted to CFG and might change the structure, features,
and consequently, the programs’ behaviors. The graph em-
bedding and augmentation method they proposed reply on
expert experience on modify the CFG and doesn’t consider
the guidance of the malware detection.

7 CONCLUSION

In this work, we propose a reinforcement learning based
semantics-preserving attack against black-box GNNs. The
key factor of adversarial malware generation via semantic
Nops insertion is to select the appropriate semantic Nops
and their corresponding basic blocks. The proposed attack
uses reinforcement learning to automatically make these
“how to select” decisions. To evaluate the attack, we have
trained two kinds of GNNs with three types of Windows
malware samples and various benign Windows programs.
The evaluation results shown that the proposed attack
can achieve a signiﬁcantly higher evasion rate than three
baseline attacks, namely the semantics-preserving random
instruction insertion attack, the semantics-preserving accu-
mulative instruction insertion attack, and the semantics-
preserving gradient-based instruction insertion attack.

Our work focuses on attacks targeting GNNs for mal-
ware detection using CFGs as input. We believe the method
can be applied to other ML models based on sequential
data. For example, prior work studied the effectiveness of
instruction sequences and API Call sequences as features
for malware detection.By inserting semantic Nops into the

instruction sequences, one can potentially mislead RNN-
based detection models. Another potential extension for the
SRL attack is to leverage other code obfuscation techniques
such as instruction substitution and code transposition.
REFERENCES

[1] VirusTotal, A leakerlocker

2016-11-23),

seen:
virustotal.com/en/ﬁle/d82330e1d84c2f866a0ff21093cb9669aa\
ef2b07bf430541ab6182f98f6fdf82/analysis/1479960699

2016.

sample

(ﬁrst
[Online]. Available: https://www.

on virustotal

found

[2] L.-P. Yuan, W. Hu, T. Yu, P. Liu, and S. Zhu, “Towards large-scale
hunting for android negative-day malware,” in 22nd International
Symposium on Research in Attacks, Intrusions and Defenses ({RAID}
2019), 2019, pp. 533–545.
Security
malware,

detect
Available:
security-tools-taking-too-long-detect-new-malware-analysis-warns/

to
[Online].
https://www2.cso.com.au/article/566738/

tools
analysis

taking
warns,

Braue,
new

too
2015.

[3] D.

long

[5]

[4] L. De La Rosa, S. Kilgallon, T. Vanderbruggen, and J. Cavazos,
“Efﬁcient characterization and classiﬁcation of malware using
deep learning,” in 2018 Resilience Week (RWS).
IEEE, 2018, pp.
77–83.
J. Yan, G. Yan, and D. Jin, “Classifying malware represented
as control ﬂow graphs using deep graph convolutional neural
network,” in 2019 49th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN).
IEEE, 2019, pp. 52–63.
[6] H. S. Anderson, A. Kharkar, B. Filar, D. Evans, and P. Roth,
“Learning to evade static pe machine learning malware models
via reinforcement learning,” arXiv preprint arXiv:1801.08917, Jan.
2018.

[7] M. Sharif, K. Lucas, L. Bauer, M. K. Reiter, and S. Shintre,
“Optimization-guided binary diversiﬁcation to mislead neural
networks for malware detection,” arXiv preprint arXiv:1912.09064,
2019.

[8] D. Park, H. Khan, and B. Yener, “Generation & evaluation of
adversarial examples for malware obfuscation,” in 2019 18th
IEEE International Conference On Machine Learning And Applications
(ICMLA).

IEEE, 2019, pp. 1283–1290.

[9] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas, and
J. Keshet, “Deceiving end-to-end deep learning malware detectors
using adversarial examples,” arXiv preprint arXiv:1802.04528, 2018.
[10] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end
deep learning architecture for graph classiﬁcation,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.
[11] I. You and K. Yim, “Malware obfuscation techniques: A brief
survey,” in 2010 International conference on broadband, wireless com-
puting, communication and applications.

IEEE, 2010, pp. 297–300.

[12] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and
harnessing adversarial examples,” arXiv preprint arXiv:1412.6572,
2014.

[13] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with
graph convolutional networks,” arXiv preprint arXiv:1609.02907,
2016.

[14] Z. Cui, F. Xue, X. Cai, Y. Cao, G.-g. Wang, and J. Chen, “Detection
of malicious code variants based on deep learning,” IEEE Transac-
tions on Industrial Informatics, vol. 14, no. 7, pp. 3187–3196, 2018.

[15] L. Xu, D. Zhang, N. Jayasena, and J. Cavazos, “Hadm: Hybrid
analysis for detection of malware,” in Proceedings of SAI Intelligent
Systems Conference. Springer, 2016, pp. 702–724.

[16] S. Tobiyama, Y. Yamaguchi, H. Shimada, T. Ikuse, and T. Yagi,
“Malware detection with deep neural network using process be-
havior,” in 2016 IEEE 40th annual computer software and applications
conference (COMPSAC), vol. 2.

IEEE, 2016, pp. 577–582.

[17] R. Nix and J. Zhang, “Classiﬁcation of android apps and malware
using deep neural networks,” in 2017 International joint conference
on neural networks (IJCNN).

IEEE, 2017, pp. 1871–1878.

[18] K. K. Ispoglou and M. Payer, “malwash: Washing malware to
evade dynamic analysis,” in 10th {USENIX} Workshop on Offensive
Technologies ({WOOT} 16), 2016.

[19] A. Machiry, N. Redini, E. Gustafson, Y. Fratantonio, Y. R. Choe,
C. Kruegel, and G. Vigna, “Using loops for malware classiﬁcation
resilient to feature-unaware perturbations,” in Proceedings of the
34th Annual Computer Security Applications Conference, 2018, pp.
112–123.

[20] A. V. Phan, M. Le Nguyen, and L. T. Bui, “Convolutional neural
networks over control ﬂow graphs for software defect prediction,”

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

12

[41] S. Sen, E. Aydogan, and A. I. Aysan, “Coevolution of mobile mal-
ware and anti-malware,” IEEE Transactions on Information Forensics
and Security, vol. 13, no. 10, pp. 2563–2574, 2018.

[42] L. Sun, Y. Dou, C. Yang, J. Wang, P. S. Yu, L. He, and B. Li,
“Adversarial attack and defense on graph data: A survey,” arXiv
preprint arXiv:1812.10528, 2018.

[43] J. Chen, Y. Wu, X. Xu, Y. Chen, H. Zheng, and Q. Xuan,
“Fast gradient attack on network embedding,” arXiv preprint
arXiv:1809.02797, 2018.

[44] D. Z ¨ugner, A. Akbarnejad, and S. G ¨unnemann, “Adversarial at-
tacks on neural networks for graph data,” in Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, 2018, pp. 2847–2856.

[45] X. Wang, M. Cheng, J. Eaton, C.-J. Hsieh, and F. Wu, “Attack graph
convolutional networks by adding fake nodes,” arXiv preprint
arXiv:1810.10751, 2018.

[46] B. Wang and N. Z. Gong, “Attacking graph-based classiﬁcation via
manipulating the graph structure,” in Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security, 2019,
pp. 2023–2040.

[47] A. Abusnaina, A. Khormali, H. Alasmary, J. Park, A. Anwar,
and A. Mohaisen, “Adversarial learning attacks on graph-based
iot malware detection systems,” in 2019 IEEE 39th International
Conference on Distributed Computing Systems (ICDCS).
IEEE, 2019,
pp. 1296–1305.

in 2017 IEEE 29th International Conference on Tools with Artiﬁcial
Intelligence (ICTAI).
IEEE, 2017, pp. 45–52.

[21] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song,
“Adversarial attack on graph structured data,” in International
conference on machine learning. PMLR, 2018, pp. 1115–1124.
[22] Y. Sun, S. Wang, X. Tang, T.-Y. Hsieh, and V. Honavar, “Node
injection attacks on graphs via reinforcement learning,” arXiv
preprint arXiv:1909.06543, 2019.

[23] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,”
in Proceedings of the 2017 ACM on Asia conference on computer and
communications security, 2017, pp. 506–519.

[24] Y. Tan, Artiﬁcial immune system: applications in computer security.

John Wiley & Sons, 2016.

[25] “Virusshare,” accessed: 2019-09-30.

[Online]. Available: https:

//virusshare.com/

[26] “Virustotal,” accessed: 2019-09-30.

[Online]. Available: https:

//www.virustotal.com/

[27] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens, M. Polino,
A. Dutcher, J. Grosen, S. Feng, C. Hauser, C. Kruegel et al.,
“Sok:(state of) the art of war: Offensive techniques in binary
analysis,” in 2016 IEEE Symposium on Security and Privacy (SP).
IEEE, 2016, pp. 138–157.

[28] V. Pappas, M. Polychronakis, and A. D. Keromytis, “Smashing
the gadgets: Hindering return-oriented programming using in-
place code randomization,” in 2012 IEEE Symposium on Security
and Privacy.

IEEE, 2012, pp. 601–615.

[29] H. Koo and M. Polychronakis, “Juggling the gadgets: Binary-
level code randomization using instruction displacement,” in
Proceedings of the 11th ACM on Asia Conference on Computer and
Communications Security, 2016, pp. 23–34.

[30] N. M. Hai, M. Ogawa, and Q. T. Tho, “Obfuscation code lo-
calization based on cfg generation of malware,” in International
symposium on foundations and practice of security.
Springer, 2015,
pp. 229–247.

[31] Y. Chen, Z. Wang, D. Whalley, and L. Lu, “Remix: On-demand live
randomization,” in Proceedings of the sixth ACM conference on data
and application security and privacy, 2016, pp. 50–61.

[32] S. Crane, C. Liebchen, A. Homescu, L. Davi, P. Larsen, A.-R.
Sadeghi, S. Brunthaler, and M. Franz, “Readactor: Practical code
randomization resilient to memory disclosure,” in 2015 IEEE Sym-
posium on Security and Privacy.

IEEE, 2015, pp. 763–780.

[33] D. Williams-King, G. Gobieski, K. Williams-King, J. P. Blake,
X. Yuan, P. Colp, M. Zheng, V. P. Kemerlis,
J. Yang, and
W. Aiello, “Shufﬂer: Fast and deployable continuous code re-
randomization,” in 12th {USENIX} Symposium on Operating Sys-
tems Design and Implementation ({OSDI} 16), 2016, pp. 367–382.
[34] P. Chen, J. Xu, Z. Hu, X. Xing, M. Zhu, B. Mao, and P. Liu,
“What you see is not what you get! thwarting just-in-time rop with
chameleon,” in 2017 47th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN).
IEEE, 2017, pp. 451–
462.

[35] N. Carlini and D. Wagner, “Towards evaluating the robustness of
neural networks,” in 2017 ieee symposium on security and privacy
(sp).

IEEE, 2017, pp. 39–57.

[36] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo:
Zeroth order optimization based black-box attacks to deep neural
networks without training substitute models,” in Proceedings of the
10th ACM workshop on artiﬁcial intelligence and security, 2017, pp.
15–26.

[37] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, “Generating
adversarial examples with adversarial networks,” arXiv preprint
arXiv:1801.02610, 2018.

[38] Q. Wang, W. Guo, K. Zhang, A. G. Ororbia, X. Xing, X. Liu, and
C. L. Giles, “Adversary resistant deep neural networks with an
application to malware detection,” in Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2017, pp. 1145–1153.

[39] P. Laskov et al., “Practical evasion of a learning-based classiﬁer: A
case study,” in 2014 IEEE symposium on security and privacy.
IEEE,
2014, pp. 197–211.

[40] A. Al-Dujaili, A. Huang, E. Hemberg, and U.-M. O’Reilly, “Ad-
versarial deep learning for robust detection of binary encoded
malware,” in 2018 IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 76–82.

13

TABLE 9: Data distribution for each type of program

Dataset

Node size

Benign

Trojan

Malware
Virus

Backdoor

Train

Val

Test

count
mean
std
min
max
count
mean
std
min
max
count
mean
std
min
max

2822
1148
948
14
3237
392
1133
974
28
3240
786
1184
967
21
3238

1165
1111
821
13
3205
165
1063
792
35
3145
318
1132
817
28
3181

531
671
771
22
3175
80
666
698
24
2428
171
747
855
22
3200

1082
1318
938
22
3210
163
1249
916
58
3192
325
1280
949
42
3211

All

5600
1128
920
13
3237
800
1096
914
24
3240
1600
1147
934
21
3238

APPENDIX E
PERFORMANCE OF THE SRL ATTACK
In Figure 3a and Figure 3b, we show the performance of the
SRL attack against the basic GCN model and the DGCNN
model. We run the attacks against two models 10 times
and periodically compute the evasion rate on the validation
dataset every 10,000 queries during training. Here, while
the bold lines shows average values over 10 independent
learning trials, the shaded area show the maximum and
minimum values from 10 independent learning trials. We
observed that evasion rate against the basic GCN model
converged smoother and faster than evasion rate against the
DGCNN model.

IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

APPENDIX A
ALGORITHM OF THE SRI ATTACK
Algorithm 2 shows the pseudocode of the SRI attack.

Algorithm 2 SRI Attack against Malware Detection
Input: C(·), G =< V, E > , (cid:101)y, N opsList, topk, niters, ∆
Output: (cid:101)G =< (cid:101)V , E >

node mask ← RandomBasicBlock( (cid:101)V , topk)
X ← RandomInstruction(N opsList)
(cid:101)V ← V ∗ node mask + (V + X) ∗ node mask
G(cid:48) ←< (cid:101)V , E >
if Dif f (G(cid:48), G) <= ∆ then

1: i ← 0
2: (cid:101)G ← G
3: while argmax(C( (cid:101)G)) (cid:54)= (cid:101)y and i < niters do
4:
5:
6:
7:
8:
9:
10:
11:
12: end while

end if
i ← i + 1

(cid:101)G ← G(cid:48)

APPENDIX B
ALGORITHM OF THE SGI ATTACK
Algorithm 3 shows the pseudocode of the SGI attack.

Algorithm 3 SGI attack against malware detection
Input: C(·), G =< V, E >, y, (cid:101)y, N opsList, niters, ∆, N , K
Output: (cid:101)G =< (cid:101)V , E >

)

∂V

for j ← 1 to K do

dj ← (cid:107)gi − N opsListj (cid:107)p

gi ← sgn( ∂JC(G,y)
for i ← 1 to N do

1: i ← 0
2: (cid:101)G ← G
3:
4: while argmax(C( (cid:101)G)) (cid:54)= (cid:101)y and i < niters do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end while

end for
(cid:101)V ← V + X
G(cid:48) ←< (cid:101)V , E >
if Dif f (G(cid:48), G) <= ∆ then

end for
Xi ← N opsListargmin(dj )

end if
i ← i + 1

(cid:101)G ← G(cid:48)

APPENDIX C
MALWARE CLASSIFICATION RESULTS
Table 8 shows the performance of malware classiﬁcation.

TABLE 8: Classiﬁcation performance

a: On the basic GCN model

Model

Basic GCN
DGCNN
Basic GCN
DGCNN
Basic GCN
DGCNN
Basic GCN
DGCNN
Basic GCN
DGCNN
Basic GCN
DGCNN
Basic GCN
DGCNN

1

2

3

4

5

6

7

ACC(%)
Val

93.81
93.50
92.62
93.25
91.37
92.62
92.18
91.75
89.75
92.37
92.12
89.00
93.61
93.61

Train

98.58
94.57
96.12
95.01
96.44
95.17
96.44
93.64
94.66
93.55
99.00
91.03
97.35
98.37

Test

93.75
94.12
92.81
94.00
93.06
94.31
92.12
93.06
91.81
92.81
93.87
91.50
93.92
95.77

FPR(%)

FNR(%)

7.37
4.54
7.00
4.29
8.47
5.77
8.23
5.03
9.95
5.28
4.43
8.59
5.52
4.70

6.36
7.25
7.37
7.76
5.34
5.59
7.37
8.90
6.36
9.16
7.88
8.39
6.62
3.73

APPENDIX D
DATA DISTRIBUTION FOR EACH TYPE OF PROGRAM
Table 9 shows the data distribution of the whole dataset.

b: On the DGCNN model

Fig. 3: Performance of the SRL attack

05202510 150.30.40.50.60.70.80.91.0Attack RateEpisode(Every 10,000 queries)0525301015 20  0.40.50.60.70.80.91.0Attack Rate Episode(Every 10,000 queries)IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING

14

APPENDIX F
INJECTED FEATURES

Figure 4 illustrates the injected features of the proposed
attack. To illustrate the adversarial samples generated
by the SRL attack, we take one backdoor CFG (Back-
door.Win32.Assasin) as an example. Figure 4a and Figure 4b
are snippets of the sample and the injected features for two
models. These results show that the SRL attack can adapt
to the detection models. For different models, the agent
takes different actions on different basic blocks. The agent
only requires one semantic Nop and its corresponding basic
blocks to evade the detection model.

APPENDIX G
IMPACT OF THE INJECTED OPCODES
Figure 5 shows the impact of different opcodes.

a: Occurrence frequency of opcodes in the dataset

b: Opcodes chosen by different attacks for the DGCNN
model

c: Opcodes chosen by different attacks for the Basic GCN
model

Fig. 5: Impact of the inserted opcodes

a: For the basic GCN model

b: For the DGCNN model

Fig. 4: Injected Features by the SRL attack

0x41bedc:movebx, dwordptr[esi]0x41bede:subesi, -40x41bee1:adcebx, ebx0x41bee3:adcecx, ecx0x41bee5:addebx, ebx0x41bee7:jne0x41bef00x41bf4a:popesi0x41bf4b:movedi, esi0x41bf4d:movecx, 0x84f0x41bf52:moval, byte ptr[edi]0x41bf54:incedi0x41bf55:subal, 0xe80x41bf57:cmpal, 10x41bf59:ja0x41bf52cmova0x41bef0:adcecx, ecx0x41bef2:jne0x41bf14cmova0x41bee9:movebx, dwordptr[esi]0x41beeb:subesi, -40x41beee:adcebx, ebx0x41bf5b:cmpbyte ptr[edi], 90x41bf5e:jne0x41bf52cmova0x41bf14:cmpebp, 0xfffff3000x41bf1a:adcecx, 10x41bf1d:leaedx, [edi+ ebp]0x41bf20:cmpebp, -40x41bf23:jbe0x41bf340x41bf34:moveax, dwordptr[edx]0x41bf36:addedx, 40x41bf39:movdwordptr[edi], eax0x41bf3b:addedi, 40x41bf3e:subecx, 40x41bf41:ja0x41bf340x41bfdf:popal0x41bfe0:jmp0x41177c0x41bf60:moveax, dwordptr[edi]0x41bf62:movbl, byte ptr[edi+ 4]0x41bf65:shrax, 80x41bf69:roleax, 0x100x41bf6c:xchgah, al0x41bf6e:subeax, edi0x41bf70:subbl, 0xe80x41bf73:addeax, esi0x41bf75:movdwordptr[edi], eax0x41bf77:addedi, 50x41bf7a:moveax, ebx0x41bf7c:loop0x41bf570x41bf7e:leaedi, [esi+ 0x19000]0x41bf84:moveax, dwordptr[edi]0x41bf86:oreax, eax0x41bf88:je0x41bfdfgnn0x41bedc:movebx, dwordptr[esi]0x41bede:subesi, -40x41bee1:adcebx, ebx0x41bee3:adcecx, ecx0x41bee5:addebx, ebx0x41bee7:jne0x41bef0cmovg0x41bf4a:popesi0x41bf4b:movedi, esi0x41bf4d:movecx, 0x84fcmovg0x41bf52:moval, byte ptr[edi]0x41bf54:incedi0x41bf55:subal, 0xe80x41bf57:cmpal, 10x41bf59:ja0x41bf520x41bef0:adcecx, ecx0x41bef2:jne0x41bf140x41bee9:movebx, dwordptr[esi]0x41beeb:subesi, -40x41beee:adcebx, ebxcmovg0x41bf5b:cmpbyte ptr[edi], 90x41bf5e:jne0x41bf520x41bf14:cmpebp, 0xfffff3000x41bf1a:adcecx, 10x41bf1d:leaedx, [edi+ ebp]0x41bf20:cmpebp, -40x41bf23:jbe0x41bf340x41bf34:moveax, dwordptr[edx]0x41bf36:addedx, 40x41bf39:movdwordptr[edi], eax0x41bf3b:addedi, 40x41bf3e:subecx, 40x41bf41:ja0x41bf34cmovg0x41bfdf:popal0x41bfe0:jmp0x41177ccmovg0x41bf60:moveax, dwordptr[edi]0x41bf62:movbl, byte ptr[edi+ 4]0x41bf65:shrax, 80x41bf69:roleax, 0x100x41bf6c:xchgah, al0x41bf6e:subeax, edi0x41bf70:subbl, 0xe80x41bf73:addeax, esi0x41bf75:movdwordptr[edi], eax0x41bf77:addedi, 50x41bf7a:moveax, ebx0x41bf7c:loop0x41bf570x41bf7e:leaedi, [esi+ 0x19000]0x41bf84:moveax, dwordptr[edi]0x41bf86:oreax, eax0x41bf88:je0x41bfdf0.080.280.48BenignMalware0.00010.02010.04010.0601addbswapcmovacmovgcmovlcmovnocmovnpcmovnscmovocmovpcmovscmpdecincleamovmovzxnegnopnotpoppopfpushpushfsetgsubxchgxor0.000000.000050.00010addbswapcmovacmovgcmovlcmovnocmovnpcmovnscmovocmovpcmovscmpdecincleamovmovzxnegnopnotpoppopfpushpushfsetgsubxchgxor0.000.050.100.150.200.25SRI_GCNSAL_GCNSGI_GCNSRL_GCNaddbswapcmovacmovgcmovlcmovnocmovnpcmovnscmovocmovpcmovscmpdecincleamovmovzxnegnopnotpoppopfpushpushfsetgsubxchgxor0.000.050.100.150.200.25SRI_GNNSAL_GNNSGI_GNNSRL_GNN