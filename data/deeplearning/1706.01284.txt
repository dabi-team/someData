8
1
0
2

r
a

M
8

]

G
L
.
s
c
[

4
v
4
8
2
1
0
.
6
0
7
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2018TOWARDSSYNTHESIZINGCOMPLEXPROGRAMSFROMINPUT-OUTPUTEXAMPLESXinyunChenChangLiuDawnSongUniversityofCalifornia,BerkeleyABSTRACTInrecentyears,deeplearningtechniqueshavebeendevelopedtoimprovetheper-formanceofprogramsynthesisfrominput-outputexamples.AlbeititssigniÔ¨Åcantprogress,theprogramsthatcanbesynthesizedbystate-of-the-artapproachesarestillsimpleintermsoftheircomplexity.Inthiswork,wemoveasigniÔ¨Åcantstepforwardalongthisdirectionbyproposinganewclassofchallengingtasksinthedomainofprogramsynthesisfrominput-outputexamples:learningacontext-freeparserfrompairsofinputprogramsandtheirparsetrees.Weshowthatthisclassoftasksaremuchmorechallengingthanpreviouslystudiedtasks,andthetestaccuracyofexistingapproachesisalmost0%.Wetacklethechallengesbydevelopingthreenoveltechniquesinspiredbythreenovelobservations,whichrevealthekeyingredientsofusingdeeplearningtosynthesizeacomplexprogram.First,theuseofanon-differentiablemachineisthekeytoeffectivelyrestrictthesearchspace.Thusourproposedapproachlearnsaneuralprogramoperatingadomain-speciÔ¨Åcnon-differentiablemachine.Second,recursionisthekeytoachievegeneralizability.Thus,webake-inthenotionofrecursioninthedesignofournon-differentiablemachine.Third,reinforcementlearningisthekeytolearnhowtooperatethenon-differentiablemachine,butitisalsohardtotrainthemodeleffectivelywithexistingreinforcementlearningalgorithmsfromacoldboot.Wedevelopanoveltwo-phasereinforcementlearning-basedsearchalgorithmtoovercomethisissue.Inourevaluation,weshowthatusingournovelapproach,neuralparsingprogramscanbelearnedtoachieve100%testaccuracyontestinputsthatare500√ólongerthanthetrainingsamples.1INTRODUCTIONLearningadomain-speciÔ¨Åcprogramfrominput-outputexamplesisanimportantopenchallengewithmanyapplications(Balogetal.,2017;Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017;Devlinetal.,2018;Parisottoetal.,2017;Gulwanietal.,2012;Gulwani,2011).Approachesinthisdomainlargelyfallintotwocategories.Onelineofworklearnsaneuralnetwork(i.e.,afully-differentiableprogram)togenerateoutputsfrominputsdirectly(Vinyalsetal.,2015b;Aharoni&Goldberg,2017;Dong&Lapata,2016;Devlinetal.,2018).Despitetheirpromisingperformance,theseapproachestypicallycannotgeneralizewelltopreviouslyunseeninputs.Anotherlineofworksynthesizesanon-differentiable(discrete)programinadomain-speciÔ¨Åclanguage(DSL)usingeitheraneuralnetwork(Devlinetal.,2018;Parisottoetal.,2017)orSMTsolvers(Ellisetal.,2016).However,thecomplexityofprogramsthatcanbesynthesizedusingexistingapproachesislimited.AlthoughmanyeffortsaredevotedintotheÔ¨Åeldofneuralprogramsynthesis,allofthemarestillfocusingonsynthesizingsimpletextbook-levelprograms,suchasarraycopying,Quicksort,andacombinationofnomorethan10stringoperations.Webelievethatthenextimportantstepforthecommunityistoconsidermorecomplexprograms.Inthiswork,weendeavortopursuethisdirectionandmoveabigstepforwardtosynthesizemorecomplexprogramsthanbefore.Alongtheway,weidentifyseveralnovelchallengesdealingwithcomplexprogramsthathavenotbeenfullydiscussedbefore,andproposenovelprincipledapproachestotacklethem.First,anend-to-enddifferentiableneuralnetworkishardtogeneralize,andinsomecasesishardtoevenachieveatestaccuracythatisgreaterthan0%.WeobservethataneuralnetworkistooÔ¨Çexibletoapproximateanyfunctions,buttheprogramsthatwewanttosynthesizetypicallylie1 
 
 
 
 
 
PublishedasaconferencepaperatICLR2018inasearchspaceofinterest.Itisveryhardtorestrictthelearnedneuralnetworktoalwaysrepresentaninstanceinthesearchspace.Whennot,thenetworkissimplyoverÔ¨Åttingtothetrainingdata,andthuscannotgeneralize.Tomitigatethisissue,weemploytheapproachtotrainadifferentiableneuralprogramtooperateadomain-speciÔ¨Åcnon-differentiablemachine.ThiscombinationallowsustorestrictthesearchspacebydeÔ¨Åningthenon-differentiablemachine,sothatanyneuralprogramthatcanoperatethemachineisalwaysavalidprogramofinterest.Second,thedomain-speciÔ¨Åcmachineneedstobeexpressiveenough.Inparticular,state-of-the-artapproaches,suchasRobustFill(Devlinetal.,2018),mayfailattasksinvolvinglongoutputsbecausetheycanonlysynthesizeprogramsofupto10stringoperations,whichdonotsupportrecursion.AsalsonotedbyCaietal.(2017),recursionisakeyconcepttoenableperfectgeneralization.Therefore,itisdesirablethatthenon-differentiablemachinecanbake-intheconceptofrecursionintothedesign.Third,thenon-differentiablemachinemakesthemodelhardtobetrainedend-to-end,especiallywhenthetracestooperatethemachinearenotgivenduringthetraining.Thus,werelyonareinforcementlearningalgorithmtotraintheneuralprogramwhilerecoveringtheexecutiontraces.However,thisischallenging.Previousattempts(Zaremba&Sutskever,2015)alongthisdirectioncanonlysucceedtolearntocomputeadditionoftwonumbers,andfailevenforthetasksofthree-numberadditions.Inourevaluation,weobservethattrainingfromacoldstartisthemaindifÔ¨Åculty.Inparticular,themodeltrainedusingexistingreinforcementlearningalgorithmsfromacoldbootalwaysgetsstuckatalocalminimumtoÔ¨Åttoonlyasubsetofthetrainingsamples.Moreimportantly,therecoveredtracesaresometimes‚Äúwrong‚Äù,whichaggravatestheissue.Tothebestofourknowledge,thisissueisalong-standingchallengingproblemforneuralprogramsynthesis,andwearenotawareofanypromisingsolution.Totacklethisissue,weproposeatwo-phasereinforcementlearning-basedalgorithm.Intuitively,webreakupthewholeproblemintotwoseparatetasks:(1)searchingforpossibletraces;and(2)trainingthemodelwiththesupervisionoftraces.Eachofthesetwotasksareeasierforreinforcementlearningtohandle,andwedevelopanestedalgorithmtocombinethesolutionstothesetwotaskstoprovidetheÔ¨Ånalsolution.Todemonstrateourideas,weproposeanovelchallengingproblem:learningaprogramtoparseaninputsatisfyingan(unknown)context-freegrammarintoitsparsetree.Aswewillshow,thisclassofprogramsaremuchmorechallengingtolearnthanthoseconsideredpreviously:usingmoststate-of-the-artapproaches,thetestaccuracyalmostremains0%whenthetestinputsarelongerthanthetrainingones.Meanwhile,learningaparserisalsoanimportantproblemonitsownwithmanyapplications,suchaseasingthedevelopmentofdomain-speciÔ¨Åclanguagesandmigratinglegacycodeintonovelprogramminglanguages.Inthissense,thisproblemexhibitsbothmorecomplexityandmorepracticalitythansomepreviouslyconsideredproblems,suchassynthesizinganarray-copyingfunction.Therefore,ournewlyproposedproblemservesasagoodnextstepchallengetotackleinthedomainoflearningprogramsfrominput-outputexamples.Toimplementtheideaoflearninganeuralprogramoperatinganon-differentiablemachine,weÔ¨ÅrstdesigntheLLmachine,whichbakesintheconceptofrecursioninitsdesign.Wealsodesignaneuralparsingprogram,suchthateveryneuralparsingprogramoperatinganLLmachineisrestrictedtorepresentanLLparser.Toevaluateourapproach,wedeveloptwoprogramminglanguages,animperativeoneandafunctionalone,astwodiversetasks.Combinedwithournewlyproposedtwo-phasereinforcementlearning-basedalgorithm,wedemonstratethatforbothtasks,ourapproachcanachieve100%accuracyontestsamplesthatare500√ólongerthantrainingsamples,whileexistingapproaches‚Äôcorrespondingtestaccuraciesare0%.Tosummarize,ourworkmakesthefollowingcontributions:(1)weproposeanovelstrategythatcombinestrainingneuralprogramsoperatinganon-differentiablemachinewithreinforcementlearn-ing,andthisstrategyallowsustosynthesizemorecomplexprogramsfrominput-outputexamplesthanthosethatcanbehandledbyexistingtechniques;(2)werevealthreeimportantobservationswhysynthesizingcomplexprogramscanbechallenging;(3)weproposeanoveltwo-phasereinforcementlearning-basedalgorithmtosolvethecoldstarttrainingproblem,whichmaybeofindependentinterests;(4)weproposetheparserlearningproblemasanimportantandchallengingnextstepforprogramsynthesisfrominput-outputexamplesthatexistingapproachesfailwith0%accuracy;(5)wedemonstratethatourstrategycanbeappliedtosolvetheparserlearningproblemwith100%accuracyontestsamplesthatare500√ólonger.Weconsiderapplyingourstrategytomorecomplextasksotherthantheparserlearningproblemasfuturework.2PublishedasaconferencepaperatICLR2018a = 1 if x==yIdIdxyInput:Output:EqIfIdLita1AssignFigure1:Aninput-outputexample.Theinputisasequenceoftokens[‚Äúa",‚Äú=",‚Äú1",‚Äúif",‚Äúx",‚Äú==",‚Äúy"],andtheoutputisitsparsetree.Thenon-terminalsaredenotedbyboxes,andterminalsaredenotedbycircles.2THEPARSINGPROBLEMANDAPPROACHOVERVIEWToillustrateourstrategytowardssynthesizingcomplexprograms,wewanttoputourpresentationinacontext.Tothisend,inthissection,wedeÔ¨Ånetheparsingproblemandoutlineourapproach.Notethatourstrategycouldalsobeadaptedforotherproblems.Inthefollowing,westartwiththeformaldeÔ¨Ånitionoftheparsingproblem.DeÔ¨Ånition1(Theparsingproblem)Assumethereexistacontext-freelanguageLandaparsingoracleœÄthatcanparseeveryinstanceinLintoanabstractsyntaxtree.BothLandœÄareunknown.TheproblemistolearnaparsingprogramP,suchthat‚àÄI‚ààL,P(I)=œÄ(I).Figure1providesanexampleofaninputanditsoutputparsetree.Theinternalnodesofthetreearecallednon-terminals,andtheleafnodesarecalledterminals.Thesetsofnon-terminalsandterminalsaredisjoint.Eachterminalmustcomefromoneinputtoken,butthenon-terminalsdonotnecessarilyhavesuchacorrespondence.Tosimplifytheproblem,weassumetheinputisalreadytokenized.Thesetsofallnon-terminalsandterminalscanbeextractedfromthetrainingcorpus,i.e.,allnodesintheoutputparsetreesoftrainingsamples.Inthiswork,weassumethevocabularyset(i.e.,allterminalsandnon-terminals)isÔ¨Ånite,andourworkcanbeextendedtohandleunboundedvocabularysetwithtechniquessuchaspointernetworks(Vinyalsetal.,2015a).Remarks.Notethatourparsingproblemhasitscounterparttohandlenaturallanguages,whichhasbeenextensivelystudiedintheliterature(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016;Dyeretal.,2016).Wewanttoremarkonthedifferencebetweenthetwoproblems.Ontheonehand,aprogramminglanguagewithacontext-freegrammariseasiertolearnthananaturallanguageinthesensethatthegrammarhasarigorousspeciÔ¨Åcationtoavoidambiguity.Thatis,itisalwayspossibletoconstructaparsertoachieve100%accuracyforacontext-freeprogramminglanguage,whilethismaynotbethecaseforanaturallanguage.Ontheotherhand,learningaprogramminglanguageparsermaybemorechallengingthananaturallanguageparser,sinceaninstanceinaprogramminglanguagecanbearbitrarilylong,whileanaturallanguagesentencetypicallyhasonlyalimitednumberofwords.Inthissense,anapproachthatcanlearnanaturallanguageparserwellmaynotbeabletohandleaprogramminglanguage,whenthetestsamplesaremuchlongerthantrainingsamples.Aswewillobserveinourevaluation,thisisindeedanissueforexistingapproaches.Wealsowanttoremarkonpotentialapplicationsoftheparsingproblem.Nowadays,weneedtodevelopnewdomain-speciÔ¨Åclanguagesinmanyscenarios.Whilethecurrentpracticeistodevelopthegrammarandparsermanually,thisprocessiserror-prone.Inourexperience,whencreatingthedatasetsforevaluation,weÔ¨Åndthatdesigningatrainingsetof(program,parsetree)pairsistypicallyeasier,butdevelopingtheparsertakes2√óor3√ómoretimethandevelopingthetrainingset.Intuitively,buildingatrainingsetonlyneedsdevelopingatutorialincludingbasicexampleswhoseparsetreesareeasytoconstructmanually;ontheotherhand,implementingaparserrequiresmuchlongertimeindebugging,typicallywiththehelpofthedevelopedtrainingset.Therefore,ourproposedparsergenerationproblemisanovelpracticalusecaseofprogram-by-example.Challenges.LearningtheparsingprogramPischallengingforseveralreasons.First,thecor-respondencebetweennon-terminalsandinputtokensisunknown.Forexample,inFigure1,theparserneedstoÔ¨Åndoutthattoken‚Äú="correspondstothenon-terminalAssign.Second,theorderofnon-terminalsinthetreemaynotalignwellwiththeinputtokens.Forexample,inFigure1,thesub-expression‚Äúa=1",whichistotheleftofthesub-expression‚Äúx==y",correspondstothe3PublishedasaconferencepaperatICLR2018rightchildofthenon-terminalIf,whichistotherightofthesub-treecorrespondingto‚Äúx==y"(thesub-treewhoserootisthenon-terminalEq).Third,theassociationoftokensmaydependonothertokens.Forexample,inexpressions‚Äúx+y*z"and‚Äúx+y+z",whether‚Äúx+y"formsasub-treedependsontheoperator(i.e.,‚Äú+"or‚Äú*")afterit.Solutionoverviewandpaperorganization.Totacklethechallenges,wemakeseveralinnovations.First,weemployaparadigmtolearnadifferentiableneuralprogramoperatinganon-differentiablemachineinSection3.Inparticular,weintroduceLLmachines(Section3.1)asanexampleofnon-differentiablemachine.LLmachinescanrestrictthatalearnedprogramalwaysrepresentsaprogramofinterest,i.e.,anLLparser.Tobake-inthenotionofrecursion,wethinkthenon-differentiablemachineshouldhaveastackstructureandprovideCALLandRETURNinstructionstosimulaterecursivecalls.Inaddition,theneuralprogramshouldoperatethemachinebasedononlythestacktoptomakesurethelearnedprogramcangeneralize.WedesigntheLLmachineandneuralparsingprograms(Section3.2)tooperateanLLmachinefollowingtheseprinciples.Wewillshowthatindoingso,theneuralparsingprogramcanbelearnedtogeneralizetolongerprograms.NotethatherewemainlyusetheLLmachinesandtheneuralparsingprogramtodemonstratethatadesignsupportingrecursionisnecessarytoachievegeneralization,andourstrategyisnotlimitedtothiscombinationonly.Third,wedesignanoveltwo-phasereinforcementlearning-basedsearchalgorithm(Section4)totacklethechallengeoftraininganeuralparsingprogramwithoutthesupervisionofexecutiontraces.Inourevaluation(Section6),wedemonstratethatourapproachachieves100%trainingaccuracy,whiletheaccuraciesonalltestsetsarealso100%.WethendiscussrelatedworkinSection7,andconcludeinSection8.3NEURALPROGRAMSOPERATINGANON-DIFFERENTIABLEMACHINEInthissection,wedemonstratehowtoemploytheneuralprogramoperatinganon-differentiablemachineapproachtotackletheneuralparsingproblem.Tocarryoutthisagenda,weÔ¨ÅrstdesignLLmachinesinSection3.1.Thedesignbakesinthenotionofrecursion.Thatis,anLLmachinehasastackforrecursivecallstacks,andprovidesaCALLinstructionandaRETURNinstructionwhichcanbeusedtosimulaterecursivecalls.Then,wedesignaneuralprogramoperatingtheLLmachineinSection3.2.WeshowthattheneuralprogrammakesitsdecisionsbasedononlythestacktopintheLLmachine.Indoingso,theneuralprogramcantakeadvantageoftherecursionsupportofanLLmachine,andbelearnedtogeneralizetohandlelongerinputs.Wepresentthedetailsinthefollowing.3.1LLMACHINESWebrieÔ¨ÇypresentthedesignofLLmachines.ItisinspiredbytheLL(1)parsingalgorithm(Parr&Quong,1995),andanLLmachineisgenericenoughtoallowconstructanyLLparsers.Forexample,inourevaluation,wedemonstratethatbothanimperativelanguageandafunctionallanguagecanusetheLLmachinetoconstructtheirparsers.TheLLmachinemaintainsaninternalstateusingastackofframesandanexternalstateofthestreamofinputtokens.EachstackframeisanID-listpair,wheretheIDisafunctionIDandinthelistare(n,T)pairs,whereTisaparsetree,andnistherootnodeofT.AnLLmachinehasÔ¨Åvetypesofinstructions:SHIFT,RETURN,CALL,REDUCE,andFINAL.TheirsemanticsarepresentedinTable1.Intuitively,theseinstructionscanbeclassiÔ¨Åedintothreeclasses.TheÔ¨Årstclass,includingSHIFTandREDUCE,takeschargeofallparserrelatedprimitives.Infact,Knuth(1965)hasdemonstratedthatSHIFTandREDUCEprimitivesaresufÔ¨Åcienttoconstructanycontext-freegrammars.Thesecondclass,includingCALLandRETURN,isusedtooperatethestacktosimulaterecursivecalls.Aswewillshowinthenextsection,thecontroller,i.e.,aneuralparsingprogram,candecidewhatinstructiontoexecutebasedononlythestacktopframe,whosesizecanbebounded.Thisiscrucialforthewell-trainedneuralparsingprogramtogeneralizetotestsamplesthataremuchlongerthantrainingsamples.4PublishedasaconferencepaperatICLR2018SHIFTREDUCE	<Id>,	(1)Input+ùë¶IdxT1StackREDUCE	<Op+>,	(1,	3)Input+ùë¶StackInputùë•+ùë¶Stack0Inputùë¶StackIdxT1SHIFTCALL	1Inputùë¶StackIdxT1SHIFTInputEOFStack1(y,y)0(Id, T1), (+,+)IdxT1REDUCE	<Id>,	(1)InputEOFStackIdxT1IdyT2RETURNInputEOFStack0(Id, T1), (+,+),(Id, T2)IdxT1IdyT2IdIdxyInputEOFStack(Op+, T3)T3FINAL123456789IdIdxy00(x,x)0(Id,T1)0(Id,T1),(+,+)1(Id,T2)0(Id, T1), (+,+)10(Id, T1), (+,+)Op+Op+Figure2:Afullexampletoparsex+yintoaparsetree.9instructionsareexecutedtoconstructtheÔ¨Ånaltree.Weuseredlabelstoillustratethechangesafterperformingeachoperation.Foreaseofillustration,ifatreehasonlyonenode,whichisaterminal,thenwesimplyusethisterminaltorepresentboththerootnodeandthetreeinthestack;otherwise,wedrawthetreenexttothestack,andrefertoitwithauniquelabelinthestack.InsturctionArgumentDescriptionSHIFT(None)Pullonetokenfromtheinputstreamtoappendtotheendofthestacktop.REDUCEn,c1,...,cmReducethestack‚Äôstopframetoformanewnoderootedatn.cidenotesthatthei-thchildoftherootnistheci-thelementoriginallyinthestacktopframe.CALLÔ¨ÅdPushoneframewith(Ô¨Åd,[])atthestacktop.RETURN(None)Popthestacktopandappendthepoppeddatatothenewstacktop.FINAL(None)Terminatetheexecution,andreturnthestacktopastheoutput.Table1:LLmachineinstructionsemanticsThethirdclass,includingonlyFINAL,isaninstructiontoterminatethemachine‚ÄôsexecutionandproducetheÔ¨Ånalresult.Thisinstructionshouldbeincludedinanynon-differentiablemachinedesign.Throughthisdesign,wewanttohighlightthreekeypropertiesinthedesignofanon-differentiablemachine:instructionsforthecorefunctionalityrelatedtotheprogramsofinterest;instructionstoenablerecursion;andinstructionstoproducetheresultsandterminatethemachine.WepresentarunningexampleusingLLmachineinFigure2,andmoredetailsandexamplestoexplainhowanLLmachineworkscanbefoundinAppendixB.3.2ANEURALPARSINGPROGRAMAparsingprogramoperatesanLLmachineviaasequenceofLLmachineinstructionstoparseaninputtoaparsetree.SpeciÔ¨Åcally,aparsingprogramoperatinganLLmachinedecidesthenextinstructiontoexecuteaftereachtimestep.AkeypropertyofthecombinationoftheLLmachineandtheparsingprogramisthatitsdecisioncanbemadebasedonthreecomponentsonly:(1)thefunctionIDofthetopframe;(2)allrootnodesofthetrees(butnottheentiretrees)inthelistofthetopframe;and(3)thenextinputtoken.WecansafelyassumethatthelistinanystackframecanhaveatmostKelements(seeAppendixB).Here,Kisahyper-parameterofthemodel.Therefore,theparseronlyneedstolearna(small)Ô¨Ånitenumberofscenariosinordertogeneralizetoallvalidinputs.Tolearntheparsingprogram,werepresentitasaneuralnetwork,whichpredictsthenextinstructiontobeexecutedbytheLLmachine.SpeciÔ¨Åcally,weconsidertwoinferenceproblemsthatcomputetheprobabilitiesofthetypeandargumentsofthenextinstructionrespectively:p(inst|Ô¨Åd,l,tok)p(arginst|inst,Ô¨Åd,l,tok)5PublishedasaconferencepaperatICLR2018Stack top(Id, 1)(+,+)(Id, 2)Aùëí1ùëí2ùëí3AùëìùëñùëëAùë°ùëúùëòfidnext input tokenùëí0ùëíùë•LSTM1LSTM2ùêπùê∂ùêπùê∂ùëÜùëúùëìùë°ùëöùëéùë•ùëÜùëúùëìùë°ùëöùëéùë•Instruction TypeCall ArgumentsLSTM3ùêπùê∂ùëÜùëúùëìùë°ùëöùëéùë•REDUCE Arguments ùëõOp+Softmax‚Ä¶1, 22, 11, 33, 1‚Ä¶3,2,1‚Ä¶‚Ä¶‚Ä¶0.99‚Ä¶‚Ä¶‚Ä¶ùëéOp+REDUCE Arguments ùëê1,‚Ä¶,ùëêùëöFigure3:TheNeuralParsingProgramModel.where(Ô¨Åd,l)denotesthecurrentstacktop,tokistheÔ¨Årsttokenofcurrentinput,instisthetypeofthenextinstruction,arginstaretheargumentsofthenextinstructioninst.NotethatthesecondprobabilityisneededonlyifthepredictedinstructiontypeiseitherCALLorREDUCE.Atahigh-level,ateachstep,theneuralnetworkÔ¨Årstconvertseachrootnodeinthelistofthetopstackframeintoanembeddingvector,andthenrunsthreeseparateLSTMstopredictthetype(i.e.,Formula(1))andarguments(i.e.,Formula(2)and(3))ofthenextinstruction.ThisnetworkisillustratedinFigure3.Weexplaineachcomponentinthefollowing.Fornotation,weuseLtodenotethelengthofl,andDthedimensionalityforbothinputembeddingsandLSTMhiddenstates.softmax(...)idenotesthei-thdimensionofthesoftmaxoutput.MoresubtledetailscanbefoundinAppendixC.Embeddings.Foreachelement(ni,Ti)(1‚â§i‚â§L)inthestacktop‚Äôslistl,weusealookuptableAoverallterminalsandnon-terminalstoconvertniintotheembeddingspace.SpeciÔ¨Åcally,wecomputeaD-dimensionalvectorei=A(ni)for1‚â§i‚â§L.Thus,wecomputee1,...,eLfroml.Instructionprobability.WeuseanLSTMtocomputePinst(inst|Ô¨Åd,l,tok)asfollows:Pinst(inst|Ô¨Åd,l,tok)=softmax(W1¬∑LSTM1(A(Ô¨Åd),e1,...,eL)+W2¬∑A(tok))inst(1)SpeciÔ¨Åcally,eachfunctionIDÔ¨Ådistreatedasaspecialtoken,whichisconvertedintotheembeddingusingAaswell.WeuseLSTM1(A(Ô¨Åd),e1,...,eL)toindicatetheÔ¨ÅnalhiddenstateofLSTM1whentheinputsequencetotheLSTMisA(Ô¨Åd),e1,...,eL.Further,A(tok)encodesthecurrenttokenusingthesamelookuptableAasabove.W1andW2areM√óDtrainablematrices,whereM=5sincethereare5differenttypesofinstructions.TheoperationW1¬∑LSTM1(A(Ô¨Åd),e1,...,eL)+W2¬∑A(tok)isequivalenttoconcatenatingtheLSTMoutputwithA(tok)andpassingitthroughafully-connectedlayer(i.e.,FCinFigure3).PredictingCALLarguments.TopredictargumentÔ¨Åd0oftheCALLinstruction,wecomputePÔ¨Åd(Ô¨Åd0|Ô¨Åd,l,tok)=softmax(W01¬∑LSTM2(A(Ô¨Åd),e1,...,eL)+W02¬∑A(tok))Ô¨Åd0(2)Thispartissimilartotheonefornext-instructionpredictionasshowninFormula(1),thoughadifferentsetofparameters(i.e.,W01,W02)isused.ThelookuptableAistheonlyoverlap.PredictingREDUCEarguments.ForaREDUCEinstruction,weneedtopredictbothnand(c1,...,cm),whichdeÔ¨Ånehowtoconstructthenewsub-tree.Toachievethis,themodelpredictsnÔ¨Årst,andthenpredicts(c1,...,cm)basedonn.SpeciÔ¨Åcally,wehavePn(n|l)=softmax(W00¬∑LSTM3(e1,...,eL))n(3)Pc(c1,...,cm|n)=softmax(an)c1,...,cm(4)6PublishedasaconferencepaperatICLR2018whereLSTM3isthethirdLSTM,W00isanN√óDtrainablematrix.HereNisthenumberofdifferenttypesofnon-terminals.NotethatdifferentfrompredictingthenextinstructiontypeandtheCALLarguments,predictingndoesnotlookatÔ¨Ådandtok,onlye1,...,eL.Thechoiceofc1,...,cmisentirelydecidedbyn.Tothisend,weconvertthispredictionproblemasaone-hotpredictionproblem.Inparticular,weencodeeachpossiblecombinationofc1,...,cmintoauniqueID.Infact,givenm‚â§K,thereareatmostf(K)=bK!exp(1)‚àí1cpossibledifferentcombinationsofc1,...,cm,whereK!isthefactorialofK,andexp(1)isthebaseoftheNaturalLogarithm(seeAppendixC).Therefore,wemodelthepredictionproblemofc1,...,cmasaf(K)-wayclassiÔ¨Åcationproblem.InEquation4,anisaf(K)-dimensionaltrainablevectorforeachn.AssumetheIDfor(c1,...,cm)isŒæ,thensoftmax(...)c1,...,cmindicatestheŒæ-thdimensionofthesoftmaxoutput.NoticethatsettingKto4isenoughtohandletwonon-triviallanguagesusedinourevaluation.Inbothcases,f(K)‚â§65,whichistractableasthenumberofclassesinaclassiÔ¨Åcationproblem.WeconsidertohandlealargerKasfuturework.4LEARNINGANEURALPARSINGPROGRAMTraininganeuralparsingprogramischallengingduetothenon-differentiabilityoftheLLmachine.ThemainproblemisthattheexecutiontraceoftheLLmachineisunknown,andthusreinforcementlearningisnecessaryforrecoveringtheexecutiontrace.However,trainingwithreinforcementlearningisveryunstable,andisusuallystuckatalocalminimumthatcanÔ¨Åttoonlyafewexamples.Insuchacase,moreimportantly,therecoveredexecutiontracesmaybe‚Äúwrong".Tothebestofourknowledge,thisisalong-standingopenproblem,andthereisnoeffectivemechanismtoÔ¨ÅndtheglobaloptimumforamodelthatcanÔ¨Åttoallexamplesatonce.Inthiswork,wetacklethischallengebyproposingatwo-phasetrainingstrategy.Infact,themainchallengeoftrainingwithareinforcementlearningalgorithmliesinthedifÔ¨Åcultyofjointlyrecoveringtheexecutiontracesandlearningamodelwitheffectiveparameters.Themainideaofourtrainingstrategyistodecoupletheproblemintotwophases,wheretheÔ¨Årstphasetriestorecovertheexecutiontraces,whilethesecondphasetriestotrainasetofparameters.Inthefollowing,weÔ¨Årstexplainthehigh-levelideaofthistwo-phasetrainingapproach(Section4.1),andthenexplainhowreinforcementlearningisused(Section4.2).WewillillustratewhyandhowouralgorithmworkswitharunningexampleinSection5.4.1TWO-PHASETRAININGSTRATEGYWhenthetrainingsetcontainsonlyinput-outputpairswithoutanyinformationontheexecutiontraces,learningamodelthatcanparseallvalidinputs100%accuratelyischallenging.Themainissueisthatalearnedmodelmaycorrectlyparsesomeinputs,butfailonothers.Weobservethatforeachinput-outputpair,theremayexistmultiplevalidexecutiontraces(seeAppendixDforanexample),whereamodeltrainedtomimiconecertaintraceforoneinput-outputpairmaynotbeabletolearntomimiconecertainexecutiontraceforanotherpairatthesametime.Thus,ourgoalistoÔ¨Åndconsistentexecutiontracesforallinput-outputpairsinthetrainingset.Toachievethisgoal,welearntheneuralparsingprogramintwophases.First,foreachinput-outputpair,weÔ¨Åndasetofvalidcandidateinstructiontypetraceswithapreferencetowardshorterones.Werefertothissetoftracesasthecandidatetracesetforagiveninput-outputpair.Second,wetrytosearchforasatisÔ¨ÅablespeciÔ¨Åcation.AspeciÔ¨Åcationisasetofinput-output-tracetriplesthatassignaninstructiontypetracefromthecorrespondingcandidatetracesetforeachinput-outputpairinthetrainingset.WesaythataspeciÔ¨ÅcationissatisÔ¨Åable,ifthereexistsaneuralparsingprogramthatcanparseallinputsintotheiroutputsusingthecorrespondinginstructiontypetracesinthespeciÔ¨Åcation.AsketchofthealgorithmispresentedinAlgorithm1.Wenowpresentthedetailsinthefollowing.PhaseI:Searchingforcandidatetraceset.Duetothelargesearchspace,exhaustivesearchisnotpracticalevenforaveryshortinput.Instead,weadopttheideaoftraininganeuralparsingprogramtoexplorethesearchspacetoÔ¨Åndafeasibletracethroughpolicygradient.7PublishedasaconferencepaperatICLR2018Algorithm1Asketchofthetwo-phasereinforcementlearningalgorithm1:functionSEARCH(Net0,Lesson,TrainingData)2://Phase1:nestedlooptocomputethecandidatetracesetforeachinput-outputpair3:for(inputi,treei)‚ààLessondo4:Netout‚ÜêNet05:foroutItr‚Üê1toM2do//Outerloop6:SampleaninstructiontypetracetraceusingthepolicynetworkNetout7:Netin‚ÜêNetout8:forInItr‚Üê1toM1do//Innerloop9:Executetheprogramsfollowingtraceand10:usethepolicynetworkNetintopredictthearguments11:ÀÜT‚ÜêPredictedparsetree12:ifdiÔ¨Ä(ÀÜT,treei)=0then13:updatethecandidatetracesetfor(inputi,treei)14:endif15:FollowingtheREINFORCEalgorithmtoupdateNetin16:endfor17:FollowingtheREINFORCEalgorithmtoupdateNetout18:endfor19:endfor20:21://Phase2:Ô¨ÅndasatisÔ¨ÅablespeciÔ¨Åcation22:for(inputi,treei)‚ààTrainingDatado23:Assumetherearedcandidatetracesfor(inputi,treei)24:CreateŒ∏iasad-dimensionalvectorandrandomlyinitializeit25:endfor26:whileTruedo//Ittypicallyterminateswithin50iterations27:for(inputi,treei)‚ààTrainingDatado28:Sampleacandidatetracefollowingthedistributionsoftmax(Œ∏i)29:endfor30:TrainanetworkNetusingreinforcementlearningasdiscussedinSection4.231:ifTrainingaccuracyis100%then32:returnNet33:endif34:endwhile35:endfunctionSpeciÔ¨Åcally,wedevelopatwo-nested-loopprocesstosearchforthecandidatetracesetforeachinput-outputpair.Ineachiterationoftheouterloop,werunaforwardpassofthemodeltosampleanexecutiontraceincludingasequenceofinstructionsandtheirarguments.WesampletheexecutiontraceusingthemodeldescribedinSection3.2,exceptthatwhilesamplingthenextinstructiontypeamongvalidinstructiontypes,weusethefollowingthedistributioninstead:p(inst|Ô¨Åd,l,tok)‚àùsoftmax(...)inst+œÉ(5)Here,œÉ>0isaconstantallowingexplorationduringthesearch.Afteraforwardpass,weusethedifferencebetweenthepredictedparsetreeandthegroundtruthastherewardtoupdatethemodel‚Äôsparameterspredictingthenextinstructiontypeusingpolicygradient.Thisalgorithmisreferredtoaslearningwithoutsupervisionontraces,andwewillexplainthedetailsinSection4.2.Ifthepredictedtreeisidenticaltothegroundtruth,thenwehavesuccessfullyfoundavalidinstructiontypetrace,andweadditintothecandidatetraceset.Otherwise,wetestintheinnerloopswhetherthesampledinstructiontypetraceiswrong,oronlytheargumentsarepredictedwrongly.Todoso,intheinnerloops,weusethesampledinstructiontypetraceintheouterloopasthecandidategroundtruth,andtrainthemodelwithweaklysupervisedlearningmethodwhichwillbeexplainedinSection4.2.Ifanypredictiontreeduringtheinnerloopsmatchesthecandidate8PublishedasaconferencepaperatICLR2018groundtruth,weaddthesampledinstructiontypetracetothecandidateset.Otherwise,themodel‚Äôsparametersarerevertedbacktothoseatthebeginningoftheinnerloop,andthesampledinstructiontypetraceisdropped.Attheendoftheouterloop,thecandidatetracesetisformed,whichtypicallyincludes3to5instructiontypetraces,andthemodelusedduringtheloopisdropped.InthedescriptionofPhase1inAlgorithm1,M1andM2aretwohyper-parameters,whereM1isthenumberofiterationsfortheinnerloop,andM2isthenumberofiterationsfortheouterloop.Meanwhile,toescapefromasub-optimalmodel,were-initializethemodelwiththeonelearnedfromthepreviouslessonforeveryM3iterationsintheouterloop.ThevaluesofM1,M2andM3forourexperimentsaredescribedinAppendixE.PhaseII:SearchingforasatisÔ¨ÅablespeciÔ¨Åcation.ToÔ¨ÅndasatisÔ¨ÅablespeciÔ¨Åcation,again,thenaiveideatoperformanexhaustivesearchrequirestoexploreanexponentialnumberofspeciÔ¨Åcationsinthevolumeoftrainingsamples,whichisimpractical.Forexample,ifeachinput-outputexamplehas3candidatetraces,thesearchspaceofPhaseIIforatrainingsetof20input-outputexampleshas320=3,486,784,401instances.Alternatively,weemployasampling-basedapproach.Foreachinput-outputpair(ik,Tk)inthetrainingset,weassumeSk={trk,1,...,trk,d}isitscandidatetracesetincludingdtraces.Wesampleatracefollowingthedistributionp(trk,j)=softmax(Œ∏k)j(6)whereŒ∏kisad-dimensionalvector.Afteronetraceissampledforeachinput-outputpair,thesetracesformaspeciÔ¨Åcation,andwetrytotrainamodelusingtheweaklysupervisedlearningalgorithmdescribedinSection4.2withthisspeciÔ¨Åcation.Ifthemodelcancorrectlyparseallinputs,thenweÔ¨ÅndasatisÔ¨ÅablespeciÔ¨Åcation.Otherwise,foreachinput-outputpair(ik,Tk)thatiswronglyparsed,wedecreasetheprobabilityofsamplingcurrenttraceinthefuturebyupdatingŒ∏kusing:Œ∏k‚ÜêŒ∏k‚àíœÑ¬∑diÔ¨Ä(ÀÜTk,Tk)¬∑‚àáŒ∏klogp(trk,j)(7)whereÀÜTkisthepredictedparsetree,andœÑ=1.0.Weobservethatsuchasampling-basedapproachcanefÔ¨ÅcientlysampleasatisÔ¨ÅablespeciÔ¨Åcationwithin30attemptsinourexperiments,whichcouldbe8ordersofmagnitudesmallerthantheexhaustivesearch.Curriculumlearning.Searchingforavalidtraceforalongerinputfromarandomlyinitializedmodelcanbeveryhard.Tosolvethisproblem,weusecurriculumlearningtotrainthemodeltolearntoparseinputsfromshorterlengthtolongerlength.Inthecurriculum,theÔ¨Årstlessoncontainstheshortestinputs.Inthiscase,werandomlyinitializethemodel,andtrainittoparseallsamplesinLesson1.Afterwards,foreachnewlesson,weusetheparameterslearnedfromthepreviouslessontoinitializethemodel.Whenlearningeachlesson,alltrainingsamplesfrompreviouslessonsarealsoaddedintothetrainingsetforthecurrentlessontoavoidcatastrophicforgetting(Kirkpatricketal.,2017).Suchaprocesscontinuesuntilthemodelcancorrectlyparseallsamplesinthecurriculum.4.2TRAININGUSINGREINFORCEMENTLEARNINGNowweexplainhowreinforcementlearning,especiallythepolicygradientalgorithmREIN-FORCE(Williams,1992),canbeusedtoupdatethemodelduringthetwo-phasetrainingtoeffectivelyÔ¨ÅndamodelforbothtraceexplorationandspeciÔ¨ÅcationsatisÔ¨Åabilitychecking.InSection4.1,weexplainedthatweusetwoversionsofthealgorithms:learningwithnosupervisionontracesexplorespossibleexecutiontraceswhilethegroundtruthisnotgiven;andweaklysupervisedlearningtriestotrainthemodeltoÔ¨ÅtforagivensetoftracespeciÔ¨Åcations.Theonlydifferencebetweenthetwoalgorithmsiswhetherasetofgroundtruthexecutiontracesisgiven.Inourexperiments,weÔ¨ÅndthatthemainchallengetoapplytheREINFORCEalgorithmisthatthetrainingprocessisverysensitivetothedesignoftherewardfunctions.Inthefollowing,weÔ¨Årstpresentourdesignoftherewardfunctionsfortrainingtheargumentpredictionsub-networks.Intheend,wediscussthedifferentapproachestotraintheinstructiontypepredictionsub-networkwhentheexecutiontracesaregivenornot.MoredetailscanbefoundinAppendixD.9PublishedasaconferencepaperatICLR2018LearningtopredictREDUCEargumentsnand(c1,...,cm).FortheREDUCEinstruction,ourintuitionisthatifawrongsetofargumentsisused,thegeneratedsub-treewilllookverydifferentthanthegroundtruthtree.Therefore,wedesigntherewardfunctionbasedonthedifferencebetweenthepredictedsub-treeandthegroundtruth.First,wedeÔ¨ÅnethedifferencebetweentwotreesTandT0,denotedasdiÔ¨Ä(T,T0),tobetheeditdistancebetweenTandT0(Tai,1979).AssumeÀÜTistheÔ¨ÅnalgeneratedparsetreeandTgisthegroundtruthoutputtree.OurgoalistominimizediÔ¨Ä(ÀÜT,Tg),i.e.,to0.AssumetheparsetreeconstructedbytheREDUCEinstructionisÀÜTr.SincetheÔ¨Ånalgeneratedparsetreeiscomposedbythesesmallertrees,acorrectparsetreeÀÜTrshouldalsobeasub-treeofTg.Basedonthisintuition,wedeÔ¨ÅnemindiÔ¨Ä(ÀÜTr,Tg)=minT‚ààS(Tg){diÔ¨Ä(ÀÜTr,T)},whereS(Tg)indicatesthesetofallsub-treesofTg.IfalloftheREDUCEargumentsarepredictedcorrectly,mindiÔ¨Ä(ÀÜTr,Tg)shouldbe0.Wedesigntherewardfunctionfornand(c1,...,cm)asbelow:rreduce(ÀÜTr)=‚àílog(Œ±¬∑mindiÔ¨Ä(ÀÜTr,Tg)+Œ≤)whereŒ±>1,Œ≤‚àà(0,1)aretwohyperparameters.Inourexperiments,wechooseŒ±=3,Œ≤=0.01.Inaddition,wehaveamoreefÔ¨Åcientapproachtolearnthepredictionfornviasupervisedlearning.ThedetailscanbefoundinAppendixD.LearningtopredictCALLargumentÔ¨Åd.DesigningtherewardfunctiontolearnthepredictionofÔ¨Ådischallenging.AswecanseeinFigure5,thechoiceofeachÔ¨Ådaffectsonlythepredictionofsubsequentinstructiontypes.OurdesignoftherewardfunctionforÔ¨Ådtakesthisintoaccount.Intuitively,awrongguessofÔ¨Ådwillresultinincorrectsubsequentpredictedinstructiontypes.Basedonthisintuition,wedesigntherewardfunctionasfollows:rf(fid(t))=t0Xj=t+1logp(ÀÜinst(j)|Ô¨Åd(j),l(j),tok(j))inst(j)wheretindicatesthecurrentsteptoexecuteaCALLinstruction,t0thenextsteptoexecuteaCALLin-struction,ÀÜinst(j)andinst(j)thepredictedandgroundtruthinstructiontypes,and(Ô¨Åd(j),l(j)),tok(j)theframeatthestacktopandthenextinputtokenatstepj.Basically,therewardfunctionrfaccumulatesthenegationofthecross-entropylossofthepredictedinstructionsfromthecurrentCALLinstructiontillthenextone.MoreexplanationscanbefoundinAppendixD.Learningtopredictthenextinstructiontype.Whentheexecutiontracesaregiven,trainingthenextinstructionpredictionsub-networkisasupervisedlearningproblem,andcanbesolvedusingNPI-styletrainingapproaches(Reed&DeFreitas,2016;Caietal.,2017).Whentheexecutiontracesarenotgiven,weuseREINFORCEtoupdatethesub-networkfornextinstructiontypepredictionaswell.Inparticular,assumethepredictiontreegeneratedduringtheforwardpassisÀÜTandthegroundtruthoftheoutputisTg.Thentherewardfunctionisdesigntobe‚àílog(Œ±¬∑diÔ¨Ä(ÀÜT,Tg)+Œ≤).(8)5ARUNNINGEXAMPLEInthissection,wedetailthedesignofourreinforcementlearning-basedalgorithmusingarunningexample.Forillustrationpurposes,weuseatoylanguage,whosegrammarisstillchallengingtolearn.Thegrammarisverysimple:allexpressionscomposedbyadditionandmultiplication.WeallowxandyasidentiÔ¨Åersand0and1forliterals.WerefertothislanguageasAddition-Multiplication(AM),anditisalsoasmallsubsetoftheWHILElanguage,whichwewillevaluateinthenextsection.ThecurriculumofAMlanguageispresentedbelow.10PublishedasaconferencepaperatICLR2018x+yx‚àóyx+0x‚àó00+10‚àó1y+x+0y+0+x0+x+yy‚àóx‚àó0y‚àó0‚àóx0‚àóx‚àóyy‚àóx+0y+x‚àó00‚àó1+x0+1‚àóxy+1+x+0y+1+x‚àó0y+1‚àóx+0y+1‚àóx‚àó0y‚àó1+x+0y‚àó1+x‚àó0y‚àó1‚àóx+0y‚àó1‚àóx‚àó0ThedifÔ¨Åcultyoftheproblem:searchspace.TounderstandthedifÔ¨Åcultyoftheproblem,wewouldliketoemphasizethatlearningacorrectparserisequivalenttoÔ¨Åndingthecorrectexecutiontraceforeachinput-outputexample.Ontheonehand,ifthecorrectparserislearned,thenwecanrecoverthecorrecttraceforeachinput-outputexamplebysimplyexecutingtheparserovertheinput;ontheotherhand,ifthecorrecttraceofeachinput-outputexampleisrecovered,thenwecaneasilyusesupervisedlearningtotraintheparsingprogram.Therefore,thedifÔ¨Åcultyofthelearningproblemcanbemeasuredbythevolumeofthesearchspace.Wenowestimatethesizeoftheentiresearchspace.ForAMgrammar,weparameterizetheLLmachinewithK=3andF=3,andthereare4differentnon-terminals.Wepresentthenumberoftheshortestvalidexecutiontracesversustheinputlengthbelow.InputlengthLengthofcorrectexec.tracesNumberofshortestvalidexec.traces391,5725152,771,7127217,458,826,752Here,wecallanexecutiontracetobevalidifthetraceendswithavalidFINALinstruction,buttheoutputtreedoesnotnecessarilymatchthegroundtruth.Meanwhile,weonlycalculatethenumberoftracesthatareofthesamelengthsasthecorrectexecutiontraces,wherecorrecttracesmeantheshortesttracesthatcanleadtothegroundtruthoutputtrees.Wepresentthelengthsoftheshortestcorrecttracesabove,anddenotetracesofthesamelengthsasshortesttracesforbrevity.Givenourtrainingcurriculum,anaivewaytoÔ¨Åndacorrectsetofexecutiontracesforallinput-outputexamplesistoperformanexhaustivesearchoverthespaceof15726√ó277171210√ó74588267528=3.87√ó10162.Suchahugesearchspacemakestheexhaustivesearchapproachimpractical.Noticethatthenumberofvalidexecutiontracesincreasesexponentiallyastheinputlengthincreases.Meanwhile,thissearchspaceisjustforasimplegrammar,i.e.,AM.Formorecomplexlanguagesthatwewilluseinourevaluation,i.e.,WHILEandLAMBDA,thevalueofFandthenumberofdifferentnon-terminalsarelarger.Thus,thetotalnumberofvalidexecutiontracesforWHILEorLAMBDAismuchhigherthanforAM.Also,theaverageinputlengthsforWHILEandLAMBDAare9.3and5.6respectively,whichfurtherincreasesthesearchspacevolumesigniÔ¨Åcantly.Motivatingthetwo-phasealgorithm.Giventhehugesearchspace,wedesignourtechniquestoreducethesearchspace.Weobserveseveralaspectswhythesearchspaceishuge.TheÔ¨Årstissueisthatthelargesearchspaceismainlyduetotheruleofproductwhenconsideringthecombinationsoftracesforallinput-outputexamples.Whenconsideringonlyoneinput-outputexample,e.g.,aninputoflength7,thesizeofthesearchspaceisaround7.5billion.Thoughitisstilllarge,suchasizeismoretractablethan3.87√ó10162.Thisinspirestheideaoftwo-phaselearning:theÔ¨Årstphasesearchesforcorrecttracesforeachinput-outputexample,andthesecondphasesearchesacombinationoftracesfordifferentsamples.Indoingso,theÔ¨Årstphaseonlyneedstoperformasearchoveraspaceof6√ó1572+10√ó2771712+8√ó7458826752=5.97√ó1010instances,whichisstilllarge,butmuchmoreamenablethantheoriginalsearchspace.Havingdonethisseparation,wecanfocusontherestissues:(1)5.97√ó1010isstillalargesearchspace,andthusperformingthesearchintheÔ¨ÅrstphaseisnotefÔ¨Åcientenoughyet;and(2)howtomakethesecondphaseefÔ¨Åcientandeffectiveisunclear.Inthefollowing,wewillexplainhowourdesignresolvestheseissues.Reducingthesearchspace:usinginstructiontypetracesinsteadofexecutiontraces.Weobservethatthetotalnumberoftheshortestvalidexecutiontracesforaninput-outputexampleinthecurriculumcanbeaslargeas7.5√ó109,whichisstilllargeforanexhaustiveenumeration.Noticethatmostofthemareequivalenttoeachotheruptopermutationoftheinstructionarguments.Infact,11PublishedasaconferencepaperatICLR2018thetotalnumberoftheshortestvalidinstructiontypetracesforeachexampleismuchsmaller,asweshowbelow.InputlengthNumberofshortestvalidexec.tracesNumberofshortestvalidinst.typetraces31,572952,771,71238277,458,826,75223,816Therefore,enumeratingvalidinstructiontypetracescanbemoreefÔ¨Åcientthanenumeratingvalidexecutiontraces.Thisobservationinspiresthenested-loopalgorithmintheÔ¨Årstphase.Infact,theouterloopusesreinforcementlearningtoenumeratedifferentinstructiontypetraces,whiletheinnerloopveriÔ¨Åeswhetheraninstructiontypetracecanbeinstantiatedasanexecutiontracebysearchingforargumentsusingreinforcementlearning.Wewilldefermorediscussionabouttheinnerlooplater.Meanwhile,usinginstructiontypetracesalsohelpstoreducethesearchspaceofthesecondphase.Forexample,amongallvalidinstructiontypetraces,ourtrainingalgorithmtypicallyÔ¨Ånds3to5tracesthatcanleadtothecorrectoutputforeachinput-outputexample.Thus,theentiresearchspaceisoftheorderof3nto5n,wherenisthetotalnumberofinput-outputexamplesbeingsearchedtogether.Ontheotherhand,ifallargumentsarecountedaswell,thenforeachinput-outputexamples,theseinstructiontypetracescorrespondtotensofexecutiontracesleadingtothecorrectoutput.Inthiscase,thesearchspaceisordersofmagnitudelargerthanconsideringonlyinstructiontypetraces.Therefore,usinginstructiontypetracesisalsoakeytoreducethesizeofthesearchspaceinthesecondphase.Reinforcementlearning-basedsamplingversusenumeration.IntheÔ¨Årstphase,wechoosetousereinforcementlearningtoexploredifferentpossibilitiesofinstructiontypetracesinsteadofexhaustiveenumeration.Thisdesignisbasedonthefollowingobservation.Althoughweobservethatthetotalnumberofvalidinstructiontypetracesissmallforshortinputs,thisnumbercangrowexponentiallylargewhentheinputlengthincreases.Forexample,fortheWHILElanguage,amajorityofthetraininginputshavealengthlargerthan9.Insuchacase,exhaustivelysearchingoverallvalidinstructiontypetracesisnotefÔ¨Åcient,andusingasamplingbasedapproachistypicallymoreeffective.Thisobservationisconsistentwithpreviouswork(Bergstra&Bengio,2012;Gulwanietal.,2017).Notethatourdesignofthesearchprocessinthesecondphaseisalsoinspiredbythesameobservation.Thereisacaveatforbothstrategies:itmaynotbeeasytoknowthelengthoftheshortestcorrectinstructiontypetracesinadvance.Intheexhaustivesearchapproach,onecanenumerateeachlengthoftracesintheascendingorderandcheckifthereexistsacorrectinstructiontypetrace.Usingareinforcementlearningapproach,thisprocesscouldbereversed:sinceeachouterloopmaysampleanarbitrarilylonginstructiontypetrace,itmaysampleseverallongerinstructiontypetracesbeforereachingtheoneswiththeminimallength.Thismaycauseourreinforcementlearningalgorithmtorunlongerforashortinput;butforalonginput,thesamplingapproachtypicallycanÔ¨Åndthesetofcandidateinstructiontypetracesmuchsoonerthanusingtheexhaustiveenumerationapproach.Inourevaluation,weobservethattypicallysettingM2=10,000issufÔ¨ÅcientlylargetoÔ¨Åndagoodsetofcandidatetracesregardlessoftheinputlength.ThereisasidebeneÔ¨Åtofusingareinforcementlearning-basedsamplingintheouterloop:thesameRLframeworkcanbeusedintheinnerlooptosearchforavalidexecutiontraceinstantiationfromtheinstructiontypetrace.Thus,theRLalgorithmcanalsobeusedasanefÔ¨ÅcientveriÔ¨Åcationtooltocheckwhetheravalidinstructiontypetraceiscorrectornot.Theeffectivenessoftrainingcurriculum.Thetrainingcurriculumhelpswiththetraininginthreeaspects.First,theRLalgorithmhasacaveat:foralonginput,RLalgorithmcannotÔ¨Åndevenonecorrectinstructiontypetracefromacoldstart.Thus,curriculumlearningcanhelptomitigatethisissue.Inparticular,whensearchingforcorrectinstructiontypetracesforanexampleinonelesson,themodelisinitializedwithparametersthatcanÔ¨Åttoallexamplesinpreviouslessons.Indoingso,theRLalgorithmcaneffectivelyskipmanyobviouslybadtraces,andthusÔ¨ÅndthecorrectonesmoreefÔ¨Åciently.Second,thetrainingcurriculumcanalsohelpRLtoskipthoseinstructiontypetracesthatarecorrectfortheexaminedinput-outputexamples,butareinconsistentwithotherexamplesinpreviouslessons.ForourAMlanguage,weprovidethenumberofcorrectinstructiontracesaswellasthenumberof12PublishedasaconferencepaperatICLR2018Example#ofcorrectExample#ofcorrectExample#ofcorrecttracestracestracesx+y9(3)x*y9(3)x+09(3)x*09(3)0+19(3)0*19(3)y+x+099(11)y+0+x99(11)0+x+y99(11)y*x*099(11)y*0*x99(11)0*x*y99(11)y*x+099(11)y+x*081(9)0*1+x99(11)0+1*x81(9)y+1+x+01107(41)y+1+x*0891(33)y+1*x+01053(39)y+1*x*0891(33)y*1+x+01107(41)y*1+x*0891(33)y*1*x+01107(41)y*1*x*01107(41)Table2:ThenumbersofcorrectinstructiontracesandinstructiontypetracesforeachexampleintheAMtrainingset.Thetwonumbersareprovidedinthecolumnof‚Äú#ofcorrecttraces"intheformofn(m),wheren(outsidethebrackets)indicatesthenumberofinstructiontraces,andm(insidethebrackets)indicatesthenumberofinstructiontypetraces.correctinstructiontypetracesforeachexampleinTable2.Fromthetable,wecanobservethatthenumberofcorrecttracesincreasessigniÔ¨Åcantlywithrespecttotheincreaseoftheinputlength.Usingcurriculumtraining,wecanreducethenumberofcandidateinstructiontypetracestobe3to5foreachexampleregardlessoftheinputlength,whichthusfurtherreducesthesearchspaceforPhaseII.Third,itcanfurtherreducethesearchspaceofthesecondphase.AssumethealgorithmÔ¨Ånds3candidateinstructiontypetracesforeachinput-outputpair.Sincethereare24examplesinthetrainingset,thesearchspaceofthesecondphasecanbeaslargeas324=2.82√ó1011.Whenfollowingthetrainingcurriculum,eachlessonmayhaveonly6examples.Thus,thesearchspaceforeachlessoninthesecondphasecanbeasfewas36=729,i.e.,8ordersofmagnitudesmaller.Whentrainingonthenextlesson,sincetheinstructiontypetracesforexamplesinpreviouslessonshavebeendetermined,thesearchcanfocusonthecurrentlesson.Therefore,usingatrainingcurriculumcanfurtherreducethesearchspacewhileguaranteeingthatthemodelisnotoverÔ¨Åttingtoasubsetofexamples.6EVALUATIONToshowthatourapproachisgeneralandabletolearntoparsedifferenttypesofcontext-freelanguagesusingthesamearchitectureandapproach,weevaluateourapproachontwotaskstolearnaparserforanimperativelanguageWHILEandanML-style(Milner,1997)functionallanguageLAMBDArespectively.WHILEandLAMBDAcontain73and66productionrules,andtheirparsingprogramscanbeimplementedin89and46linesofPythoncoderespectively.Noticethattheseprogramsaremoresophisticatedthanpreviousstudiedexamples.Forexample,Quicksortstudiedin(Caietal.,2017)canbeimplementedin3linesofPythoncode,andFlashFilltasksstudiedin(Devlinetal.,2018;Parisottoetal.,2017)canbeimplementedin10linesofcodeintheirDSL.GrammarspeciÔ¨ÅcationsofthetwolanguagesarepresentedinAppendixGandHrespectively.Foreachtask,wepreparethreetrainingsets:(1)Curriculum:awell-designedtrainingcurriculumincluding100to150examplesthatenumeratesalllanguageconstructors;(2)Std-10:atrainingsetincludesallexamplesinthecurriculum,andalso10,000additionalrandomlygeneratedinputswithlength10onaverage;and(3)Std-50:atrainingsetincludesallexamplesinthecurriculum,andalso1,000,000additionalrandomlygeneratedinputswithlength50onaverage.Inalldatasets,allgroundtruthparsetreesareprovided.Notethatonceourmodellearnstoparseallinputsinthecurriculum,itcanparseallinputsintrainingsets(2)and(3)forfree.Weincludetwostandardtrainingsets,i.e.,Std-10andStd-50,toallowafaircomparisonagainstbaselineapproaches,whichtypicallyrequirealargeamountoftrainingdata.Wecompareourapproachwithtwosetsofbaselines.TheÔ¨Årstclassofapproacheslearnend-to-endneuralnetworkmodelsastheprogram.Thisclassincludesasequence-to-sequenceapproach(seq2seq)(Vinyalsetal.,2015b),asequence-to-tree(seq2tree)approach(Dong&Lapata,2016),andLSTMwithunboundedmemory(Grefenstetteetal.,2015).Inparticular,weevaluateallthreevariantsproposedin(Grefenstetteetal.,2015).Thesecondclassincludesthestate-of-the-artapproachforneuralprogramsynthesis,i.e.,RobustFill(Devlinetal.,2018),whichlearnsadiscreteprograminaDSL.13PublishedasaconferencepaperatICLR2018While-LangTrainTestOursSeq2seqSeq2treeStackQueueDeQueRobust-LSTMLSTMLSTMFill(Projected)CurriculumTraining100%81.29%100%100%100%100%13.67%Test-10100%0%0.8%0%0%0%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Std-10Training100%94.67%100%81.01%72.98%82.59%0.19%Test-10100%20.9%88.7%2.2%0.7%2.8%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Std-50Training100%87.03%100%0%0%0%0.0019%Test-50100%86.6%99.6%0%0%0%0%Test-500100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Lambda-LangTrainTestOursSeq2seqSeq2treeStackQueueDeQueRobust-LSTMLSTMLSTMFill(Projected)CurriculumTraining100%96.47%100%100%100%100%29.21%Test-10100%0%0%0%0%0%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Std-10Training100%93.53%100%0%95.93%2.23%0.26%Test-10100%86.7%99.6%0%6.5%0.1%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Std-50Training100%66.65%89.65%0%0%0%0.0026%Test-50100%66.6%88.1%0%0%0%0%Test-500100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Table3:ExperimentalresultsonWhile-LangandLambda-Langdataset.Weevaluateourapproach(ours),seq2seq(Vinyalsetal.,2015b),seq2tree(Dong&Lapata,2016),withStackLSTM,QueueLSTMandDeQueLSTMfrom(Grefenstetteetal.,2015).‚ÄúStd-10"indicatesthestandardtrainingsetwith10,000samplesoflength10,‚ÄúStd-50‚Äùindicatesthestandardtrainingsetwith1,000,000samplesoflength50,and‚ÄúCurriculum"indicatesthespeciallydesignedlearningcurriculum.‚ÄúTest-LEN"indicatesthetestsetincludinginputsoflengthLEN.Fortesting,wecreatesixlevelsoftestsets,i.e.,Test-10,Test-50,Test-100,Test-500,Test-1000andTest-5000,whereeachinputhas10,50,100,500,1000and5000tokensonaveragerespectively.Eachtestsetcontains1000randomlygeneratedexpressions.Weguaranteethattestdatadoesnotoverlapwithtrainingsamples.Table3showsexperimentalresultsonWHILEandLAMBDAlanguages.Wediscusstheresultsbelow.Observationsonourapproach.Weobservethatonceourneuralparsingprogramistrainedtoachieve100%accuracyonthetrainingdata,itcanalwaysachieve100%testaccuracyonarbitrarytestsamplesregardlessoftheirlengths.Weemphasizethatalltheresultsofourapproachareachievedbyusingthecurriculumlearningapproach.Withoutcurriculumtraining,ourapproachcannottrainanyeffectivemodels,andthetestaccuracyis0%whenthetestinputislongerthantraininginputs.Also,weobservethatourtwo-phasetrainingalgorithmcanalwaysmaketheneuralnetworkbetrainedtoachieve100%onthe14PublishedasaconferencepaperatICLR2018curriculumtrainingset.Further,inourevaluation,weobservethatthetrainingcurriculumisveryimportanttoregularizethereinforcementlearningprocess.Therefore,ourevaluationdemonstratesthatthecombinationofourthreeideasenablesustolearnaprogramtoachieve100%accuracyontestsamplesthatcanbeeven500√ólongerthanthetrainingones,whilebaselineapproachesarehardtoevenachieveatestaccuracythatisgreaterthan0%.Observationsonapproachestolearnend-to-endneuralnetworkmodels.WeÔ¨Årstobservethatwhenthelengthoftestsamplesislargerthantrainingones,thetestaccuracydropsto0%regardlessoftheend-to-enddifferentialapproachbeingevaluated.Thisillustratesthatnoneoftheseapproachescangeneralizetolongerinputs.Aswehaveexplained,itisveryhardtoenforcethelearnedmodeltoalwaysrepresentaparsertohandlearbitrarilylonginputs.Thuseventhewell-trainedmodelssimplyoverÔ¨Åttothetrainingsamples,andcannotgeneralizetolongerinputs.Also,whentrainingonthecurriculumdataset,noapproachescangeneralizetoanytestdata.Thisissimplybecausethetrainingsetcontainstoofewinstances,sothattheoverÔ¨Åttingphenomenonexplainedabovebecomesmoreprominent.Whentestsamplesareofthesamelengthastrainingonesandtrainingwiththestandardtrainingsets,weobservethatseq2treeperformsbetterthanseq2seq.Weattributethisphenomenontothereasonthattheseq2treemodelessentiallyemploystherecursionideainitsdecoderdesign.Infact,inthedecodingphase,differentfromseq2seq,whichgeneratestheentiresequenceatonce,seq2treetraversesdownalongthepathsfromtheroottoleavesandrecursivelydecodeseachlayeroftheparsetreealongapath.OnWHILEandLAMBDAtasks,itcanevenachieveanaccuracyofalmost100%onTest-50andTest-10respectively,showingthatthisrecursivedecodingapproachiseffective.However,theencodingphaseofseq2treeisnotrecursive.Thishindersitsgeneralizationtolongerinputs.Meanwhile,forbothseq2seqandseq2treemodels,whentheyaretrainedonStd-50trainingset,thegapbetweenthetrainingaccuracyandthetestaccuracyismuchsmallerthantheonestrainedonStd-10trainingset.Forexample,onWHILEdataset,thetestaccuracyofseq2seqisaround20%onTest-10;however,whentrainedonStd-50,itcanreachanaccuracyofabove85%onTest-50.ThisobservationsuggeststhatthemodelsoverÔ¨ÅttotheStd-10trainingset.WhenStd-50isusedfortraining,ontheotherhand,theoverÔ¨Åttingissueismitigated.Inaddition,wenoticethatwhilethetestaccuracyfortheWHILEtaskincreaseswhenStd-50isusedfortraining,itdropsfortheLAMBDAtask.WeattributeittothefactthatthesizeoftheparsetreeinLAMBDAlanguageismuchlargerthantheparsetreeinWHILElanguagegiventheinputwiththesamenumberoftokens.Thus,whentheinputsgetlonger,themodelishardertoÔ¨Åttotheparsetreesoflargersizes.Wealsoobservethatmodelsproposedin(Grefenstetteetal.,2015)performpoorly,andmuchworsethantheothertwoend-to-endneuralnetworkapproaches.Grefenstetteetal.(2015)proposesLSTMdecoderswithunboundedmemorytogeneralizetheideaofneuralpushdownautomaton,whichwasdesignedtohandletheparsingtasks.OurresultsshowthatwhenthesemodelsaretrainedonStd-10,theysufferseverelyfromoverÔ¨Åtting.Further,whentrainedonStd-50,thetrainingaccuracydropsto0%.Weattributethepoorperformancetothefactthattheproductionandtransductionrulesdevelopedinthetwoevaluatedlanguagesaretoocomplexforsucharchitecturestolearneffectively.Infact,Grefenstetteetal.(2015)reportsthatallthreeproposedapproachesperformpoorlyonthebi-gramÔ¨Çippingtask,whichisamuchsimplersub-taskofthetwolanguagesinconsideration.Thisfurtherillustratesthechallengesoftrainingsuchend-to-enddifferentiableneuralnetworkstosimulateevensimpledatastructuressuchasstacksorqueues.Observationsonapproachestosynthesizediscreteprograms.NotethatthetrainingofRobust-Fill,asdescribedin(Devlinetal.,2018),donotusetheinput-outputexamplesinthetrainingset,butconstructtheirowntrainingsetinstead.ThesourcecodeofRobustFillisnotavailable,andourre-implementationcannotproduceanymeaningfulprograms.Tomakeafaircomparison,wecomputeaprojectedaccuracywhichistheaccuracythatcanbeachievedbythemosteffectiveprograminthespaceoftheRobustFillDSL.NotethatgivenRobustFillcanproduceprogramsoflengthofupto10,theentireoutputprogramspaceofRobustFillisÔ¨Ånite,thoughexponentiallylarge.However,wenoticethatwecanefÔ¨Åcientlyenumeratethesmallsub-set15PublishedasaconferencepaperatICLR2018ofeffectiveprogramsusingasimpleheuristictocutineffectiveconstructorsinaprogram.WedetailthealgorithminAppendixK.TheresultsinTable3reportanupperboundoftheaccuracythatcanbeachievedbytheRobustFillapproach.WecanobservethatthebestprogramintheRobustFillDSLspacecanonlyÔ¨Åttoasmallsubsetofthetrainingdata,andcannotgeneralizetolongertestinputsduetothelengthoftheprogramsislimited.Essentially,theoutputsofaprogramintheRobustFillDSLspaceareboundedbythelengthoftheprogramitself(seeAppendixKforadiscussion),whiletheoutputsofourparsingproblemcangrowarbitrarilylong.Whenthetestsamplesbecomelonger,theRobustFillapproachwillsoonfailwith0%accuracy.Therefore,tomaketheRobustFillapproachabletohandleourparsingtask,itisnecessarytodevelopanovelDSLwithenoughexpressiveness(i.e.,supportingrecursiontoallowarbitrarilylongoutputs).However,thisisahighlynon-trivialtask,andisoutofthescopeofthiswork.7RELATEDWORKWenowpresentahigh-leveloverviewofrelatedwork.Amorein-depthdiscussionabouttherelationshipbetweenourworkandpreviousworkispresentedinAppendixA.Recentworksproposetousesequence-to-sequencemodels(Vinyalsetal.,2015b;Aharoni&Gold-berg,2017)andtheirvariants(Dong&Lapata,2016)todirectlygenerateparsetreesfrominputs.However,theyoftendonotgeneralizewell,andourexperimentsshowthattheirtestaccuracyisalmost0%oninputslongerthanthoseseenintraining.OtherworksstudylearninganeuralprogramtooperateaShift-Reducemachine(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016)oratop-downparser(Dyeretal.,2016)toperformparsingtasksfornaturallanguages.Intheseworks,theexecutiontracesareeasytorecoverfrominput-outputpairs,whileinourworkthetracesarehardtorecover.Recentworksstudylearningneuralprogramsanddifferentiablemachines(Gravesetal.,2014;Kurachetal.,2015;Joulin&Mikolov,2015;Kaiser&Sutskever,2015;Buneletal.,2016).Theirproposedapproacheseitherdonotgeneralizetolongerinputsthanthoseseenduringtraining,orareevaluatedonlyonsimpletasks.Inparticular,StackRNN(Joulin&Mikolov,2015)alsostudieslearningcontext-freelanguages,buttheirmainfocusistogeneratelanguageinstances,whileourgoalistolearntheparser.Employingasimilaridea,Grefenstetteetal.(2015)designanend-to-enddifferentiablepush-downautomatonfortransductiontasks,whicharesimilartoours.Aswewilldemonstrate,suchanapproachhasevenworsegeneralizationthanasequence-to-sequencemodel.Ontheotherhand,otherworksstudyneuralprogramsoperatingnon-differentiablemachines(Caietal.,2017;Lietal.,2017;Reed&DeFreitas,2016;Zarembaetal.,2016;Zaremba&Sutskever,2015),butintheseworks,eitherextrasupervisiononexecutiontracesisneededduringtraining(Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017),orthetrainedmodelcannotgeneralizewell(Zarembaetal.,2016;Zaremba&Sutskever,2015).Inparticular,Zarembaetal.(2016)studylearningsimplealgorithmsfrominput-outputexamples;however,theapproachfailstogeneralizeonverysimpletasks,suchas3-numberaddition.OurworkistheÔ¨Årstonedemonstratingthataneuralprogramachievingfullgeneralizationtolongerinputscanbetrainedfrominput-outputpairsonly.Anotherlineofresearchstudiesusingneuralnetworkstosynthesizeaprograminadomain-speciÔ¨Åclanguage(DSL).Recentworks(Devlinetal.,2018;Parisottoetal.,2017)studyusingneuralnetworkstogenerateaprograminaDSLfromafewinput-outputexamplesfortheFlashFillproblem(Gulwanietal.,2012;Gulwani,2011).However,theDSLcontainsonlysimplestringoperations,whichisnotexpressiveenoughtoimplementaparser.Meanwhile,intheseworks,theycanonlysuccessfullysynthesizeprogramswithlengthsnotlargerthan10.Theseconstraintsmaketheirapproachesunsuitableforourproblemcurrently.DeepCoder(Balogetal.,2017)presentsaneuralnetwork-basedsearchtechniquetoacceleratesearch-basedprogramsynthesis.Again,lengthsofthesynthesizedprogramsinthisworkareatmost5,whiletheparsingprogramthatwestudyinthisworkismuchmorecomplex.Thereareotherapproaches(Ellisetal.,2016)thatemploySMTsolverstosampleprograms.Again,itisonlydemonstratedtosolveasubsetoftheFlashFillproblemandseveralsimplearraymanipulationtasks.16PublishedasaconferencepaperatICLR20188CONCLUSIONANDFUTUREWORKInthiswork,wemoveasigniÔ¨Åcantstepforwardtolearncomplexprogramsfrominput-outputexamplesonly.Inparticular,weproposeanovelclassofgrammarinductionproblemstolearnaparserfromtheinput-treepairs.Wedemonstratethattheparsingproblemsaremorechallengingasmostexistingapproachesfailtogeneralize,i.e.,thetestaccuracyis0%.Tosolvethisproblem,werevealthreenovelchallengesandproposenovelprincipledapproachestotacklethem.First,wepromoteahybridapproachtolearnaneuralprogramoperatinganon-differentiablemachinetoeffectivelyrestrictthelearnedprogramswithinthespaceofinterest.Second,wedesignthemachinetobake-inthenotionofrecursiontomakethelearnedneuralprogramsgeneralizable.Third,weproposeanoveltwo-phasereinforcementlearning-basedalgorithmtoeffectivelytrainsuchaneuralprogram.Combiningthethreetechniques,wedemonstratethattheparsingproblemcanbefullysolvedontwodiverseinstancesofgrammars.Inthefuture,weareinterestedinboththedomainofparsingproblemsandevenmorecomplexprograms.Fortheparsingproblems,weareinterestedinrecoveringtheproductionrulesfrominput-outputexamples,ratherthanonlylearningtheparser,andrelaxseveraltechnicalassumptions,suchastheknowledgeoftheterminalsetandhyper-parameterK,whichisthemaximumnumberelementsinthelistofeachstackframe.Formorecomplexprograms,weareinterestedinextendingourapproachtolearnalgorithmsonmorecomplexdatastructuressuchastreesandgraphs.ACKNOWLEDGEMENTWethankRichardShin,DengyongZhou,YuandongTian,HeHe,YuZhang,andWarrenHefortheirhelpfuldiscussions.ThismaterialisinpartbaseduponworksupportedbytheNationalScienceFoundationunderGrantNo.TWC-1409915,BerkeleyDeepDrive,andDARPASTACunderGrantNo.FA8750-15-2-0104.Anyopinions,Ô¨Åndings,andconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthor(s)anddonotnecessarilyreÔ¨ÇecttheviewsoftheNationalScienceFoundation.REFERENCESRoeeAharoniandYoavGoldberg.Towardsstring-to-treeneuralmachinetranslation.InACL,2017.DanielAndor,ChrisAlberti,DavidWeiss,AliakseiSeveryn,AlessandroPresta,KuzmanGanchev,SlavPetrov,andMichaelCollins.Globallynormalizedtransition-basedneuralnetworks.arXivpreprintarXiv:1603.06042,2016.DanaAngluin.Learningregularsetsfromqueriesandcounterexamples.Informationandcomputation,75(2):87‚Äì106,1987.MatejBalog,AlexanderLGaunt,MarcBrockschmidt,SebastianNowozin,andDanielTarlow.Deepcoder:Learningtowriteprograms.InICLR,2017.JamesBergstraandYoshuaBengio.Randomsearchforhyper-parameteroptimization.JournalofMachineLearningResearch,13(Feb):281‚Äì305,2012.RudyRBunel,AlbanDesmaison,PawanKMudigonda,PushmeetKohli,andPhilipTorr.Adaptiveneuralcompilation.InAdvancesinNeuralInformationProcessingSystems,pp.1444‚Äì1452,2016.JonathonCai,RichardShin,andDawnSong.Makingneuralprogrammingarchitecturesgeneralizeviarecursion.InICLR,2017.DanqiChenandChristopherDManning.Afastandaccuratedependencyparserusingneuralnetworks.InEMNLP,2014.NoamChomsky.Threemodelsforthedescriptionoflanguage.IRETransactionsoninformationtheory,2(3):113‚Äì124,1956.ColinDelaHiguera.Grammaticalinference:learningautomataandgrammars.CambridgeUniversityPress,2010.17PublishedasaconferencepaperatICLR2018JacobDevlin,JonathanUesato,SuryaBhupatiraju,RishabhSingh,Abdel-rahmanMohamed,andPushmeetKohli.RobustÔ¨Åll:NeuralprogramlearningundernoisyI/O.InICML,2018.LiDongandMirellaLapata.Languagetologicalformwithneuralattention.InACL,2016.ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahASmith.Recurrentneuralnetworkgrammars.InNAACL,2016.KevinEllis,ArmandoSolar-Lezama,andJoshTenenbaum.Samplingforbayesianprogramlearning.InNIPS,2016.AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401,2014.EdwardGrefenstette,KarlMoritzHermann,MustafaSuleyman,andPhilBlunsom.Learningtotransducewithunboundedmemory.InAdvancesinNeuralInformationProcessingSystems,pp.1828‚Äì1836,2015.SumitGulwani.Automatingstringprocessinginspreadsheetsusinginput-outputexamples.InACMSIGPLANNotices,2011.SumitGulwani,WilliamRHarris,andRishabhSingh.Spreadsheetdatamanipulationusingexamples.CommunicationsoftheACM,55(8):97‚Äì105,2012.SumitGulwani,OleksandrPolozov,RishabhSingh,etal.Programsynthesis.FoundationsandTrendsR(cid:13)inProgrammingLanguages,4(1-2):1‚Äì119,2017.ArmandJoulinandTomasMikolov.Inferringalgorithmicpatternswithstack-augmentedrecurrentnets.InNIPS,2015.≈ÅukaszKaiserandIlyaSutskever.Neuralgpuslearnalgorithms.arXivpreprintarXiv:1511.08228,2015.JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal.Overcomingcatastrophicforgettinginneuralnetworks.ProceedingsoftheNationalAcademyofSciences,pp.201611835,2017.DonaldEKnuth.Onthetranslationoflanguagesfromlefttoright.Informationandcontrol,8(6):607‚Äì639,1965.KarolKurach,MarcinAndrychowicz,andIlyaSutskever.Neuralrandom-accessmachines.arXivpreprintarXiv:1511.06392,2015.ChengtaoLi,DanielTarlow,AlexanderGaunt,MarcBrockschmidt,andNateKushman.Neuralprogramlattices.InICLR,2017.RobinMilner.ThedeÔ¨ÅnitionofstandardML:revised.MITpress,1997.Jos√©OncinaandPedroGarc√≠a.Identifyingregularlanguagesinpolynomialtime.AdvancesinStructuralandSyntacticPatternRecognition,5(99-108):15‚Äì20,1992.EmilioParisotto,Abdel-rahmanMohamed,RishabhSingh,LihongLi,DengyongZhou,andPushmeetKohli.Neuro-symbolicprogramsynthesis.InICLR,2017.TJParrandRWQuong.Antlr:Apredicated.Software‚ÄîPracticeandExperience,25(7):789‚Äì810,1995.ScottReedandNandoDeFreitas.Neuralprogrammer-interpreters.InICLR,2016.Kuo-ChungTai.Thetree-to-treecorrectionproblem.JournaloftheACM(JACM),26(3):422‚Äì433,1979.OriolVinyals,MeireFortunato,andNavdeepJaitly.Pointernetworks.InAdvancesinNeuralInformationProcessingSystems,pp.2692‚Äì2700,2015a.18PublishedasaconferencepaperatICLR2018OriolVinyals,≈ÅukaszKaiser,TerryKoo,SlavPetrov,IlyaSutskever,andGeoffreyHinton.Grammarasaforeignlanguage.InNIPS,2015b.RonaldJWilliams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.Machinelearning,1992.DaniYogatama,PhilBlunsom,ChrisDyer,EdwardGrefenstette,andWangLing.Learningtocomposewordsintosentenceswithreinforcementlearning.InICLR,2016.WojciechZarembaandIlyaSutskever.Reinforcementlearningneuralturingmachines-revised.arXivpreprintarXiv:1505.00521,2015.WojciechZaremba,TomasMikolov,ArmandJoulin,andRobFergus.Learningsimplealgorithmsfromexamples.InProceedingsofThe33rdInternationalConferenceonMachineLearning,pp.421‚Äì429,2016.19PublishedasaconferencepaperatICLR2018AMOREDISCUSSIONABOUTRELATEDWORKGrammarinduction.Learningthegrammarfromacorpusofexampleshaslongbeenstudiedintheliteratureasthegrammarinductionproblem,andalgorithmssuchasL-Star(Angluin,1987)andRPNI(Oncina&Garc√≠a,1992)havebeenproposedtohandleregularexpressions.Incontrast,inthiswork,weareinterestedinlearningcontext-freelanguages(Chomsky,1956),whichismuchmorechallengingthanlearningregularlanguages(DelaHiguera,2010).Sequence-to-sequencestyleapproaches.Recentworksproposetousesequence-to-sequencemodels(Vinyalsetal.,2015b;Aharoni&Goldberg,2017)andtheirvariants(Dong&Lapata,2016)todirectlygenerateparsetreesfrominputs.However,theyoftendonotgeneralizewell,andourexperimentsshowthattheirtestaccuracyisalmost0%oninputslongerthanthoseseenintraining.ParsingapproachesusingmachinesinNLPliteratures.Arecentlineofresearch(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016)studyingdependencyparsingemploysneuralnetworkstooperateaShift-Reducemachine.However,eachnodeinthegenerateddependencytreecorrespondstoaninputtoken,whileinourproblem,thereisnotadirectcorrespondencebetweentheinternalnodesinparsetreesandtheinputtokens.Further,RNNG(Dyeretal.,2016)learnsaneuralprogramoperatingatop-downparsertogenerateparsetrees,whichincludenon-terminals.However,asexplicitlystatedinthepaper(Dyeretal.,2016),theinputtokensalignwellwiththepre-ordertraversaloftheparsetree.Inourwork,suchorderisoftennotpreservedandthecorrespondenceishardtoberecovered.Thus,theseapproachesdonotdirectlyapplytoourproblem.Neuralprograminduction.Recentworksstudylearningneuralprogramsanddifferentiablemachines(Gravesetal.,2014;Kurachetal.,2015;Joulin&Mikolov,2015;Kaiser&Sutskever,2015;Buneletal.,2016).Theirproposedapproacheseitherdonotgeneralizetolongerinputsthanthoseseenduringtraining,orareevaluatedonlyonsimpletasks.Inparticular,StackRNN(Joulin&Mikolov,2015)alsostudieslearningcontext-freelanguages,buttheirmainfocusistogeneratelanguageinstances,whileourgoalistolearntheparser.Grefenstetteetal.(2015)adoptsasimilarideaforlearningtotransduce.However,aswedemonstrate,suchanapproachperformspoorlyandevenworsethanasequence-to-sequencemodel.Ontheotherhand,otherworksstudyneuralprogramsoperatingnon-differentiablemachines(Caietal.,2017;Lietal.,2017;Reed&DeFreitas,2016;Zarembaetal.,2016;Zaremba&Sutskever,2015),butintheseworks,eitherextrasupervisiononexecutiontracesisneededduringtraining(Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017),orthetrainedmodelcannotgeneralizewell(Zarembaetal.,2016;Zaremba&Sutskever,2015).Inparticular,(Zarembaetal.,2016)studieslearningsimplealgorithmsfrominput-outputexamples;however,theapproachfailstogeneralizeonverysimpletasks,suchas3-numberaddition.OurworkistheÔ¨Årstonedemonstratingthataneuralprogramachievingfullgeneralizationtolongerinputscanbetrainedfrominput-outputpairsonly.Neuralprogramsynthesis.Anotherlineofresearchstudiesusingneuralnetworkstosynthesizeaprograminadomain-speciÔ¨Åclanguage(DSL).Recentworks(Devlinetal.,2018;Parisottoetal.,2017)studyusingneuralnetworkstogenerateaprograminaDSLfromafewinput-outputexamplesfortheFlashFillproblem(Gulwanietal.,2012;Gulwani,2011).However,theDSLcontainsonlysimplestringoperations,whichisnotexpressiveenoughtoimplementaparser.Meanwhile,intheseworks,theycanonlysuccessfullysynthesizeprogramswithlengthsnotlargerthan10.Theseconstraintsmaketheirapproachesunsuitableforourproblemcurrently.DeepCoder(Balogetal.,2017)presentsaneuralnetwork-basedsearchtechniquetoacceleratesearch-basedprogramsynthesis.Again,lengthsofthesynthesizedprogramsinthisworkareatmost5,whiletheparsingprogramthatwestudyinthisworkismuchmorecomplex.Thereareotherapproaches(Ellisetal.,2016)thatemploySMTsolverstosampleprograms.Again,itisonlydemonstratedtosolveasubsetoftheFlashFillproblemandseveralsimplearraymanipulationtasks.AsalltheseapproachesfollowthesameparadigmtosynthesizeaprogramintheDSLusedinRobustFill,inAppendixK,wegiveafundamentalreasonwhysuchapproachescannotgeneralizetohandletheparsingproblems.20PublishedasaconferencepaperatICLR2018SHIFTREDUCE	<Id>,	(1)Input+ùë¶IdxT1StackREDUCE	<Op+>,	(1,	3)Input+ùë¶StackInputùë•+ùë¶Stack0Inputùë¶StackIdxT1SHIFTCALL	1Inputùë¶StackIdxT1SHIFTInputEOFStack1(y,y)0(Id, T1), (+,+)IdxT1REDUCE	<Id>,	(1)InputEOFStackIdxT1IdyT2RETURNInputEOFStack0(Id, T1), (+,+),(Id, T2)IdxT1IdyT2IdIdxyInputEOFStack(Op+, T3)T3FINAL123456789IdIdxy00(x,x)0(Id,T1)0(Id,T1),(+,+)1(Id,T2)0(Id, T1), (+,+)10(Id, T1), (+,+)Op+Op+Figure4:AcopyofFigure2toprovideafullexampletoparsex+yintoaparsetree.9instructionsareexecutedtoconstructtheÔ¨Ånaltree.Weuseredlabelstoillustratethechangesafterperformingeachoperation.Foreaseofillustration,ifatreehasonlyonenode,whichisaterminal,thenwesimplyusethisterminaltorepresentboththerootnodeandthetreeinthestack;otherwise,wedrawthetreenexttothestack,andrefertoitwithauniquelabelinthestack.BLLMACHINESWestartwiththepresentationoftheLLmachines‚Äôdesign.ItisinspiredbytheLL(1)parsingalgorithm(Parr&Quong,1995),althoughwedonotrequirethereaderstobefamiliarwiththeLL(1)algorithm.Throughoutthedescription,weuseFigure2(weprovideacopyinFigure4whichisclosertothedescriptionbelow)asarunningexampletoillustratetheconcepts.States.AnLLmachinemaintainsasequenceof(partial)inputtokensandastackofframesasitsinternalstate.EachstackframeisanID-listpair,wheretheIDisafunctionID,whichwillbeexplainedlater,andinthelistare(n,T)pairs,whereTisaparsetree,andnistherootnodeofT.Forexample,inFigure4,afterstep6,thestackframeatthetopcontainsanID1andalistofoneelement(Id,T2).Instructions.AnLLmachinehasÔ¨Åvetypesofinstructions:SHIFT,CALL,RETURN,REDUCE,andFINAL.AparseroperatesanLLmachineusingtheseÔ¨Åvetypesofinstructionstoconstructtheparsetreerecursively.Inthefollowing,weexplaintheseinstructionsandhowtheyareusedforparsinganinput.Tobeginwith,thestackcontainsoneframe(0,[]),where[]denotesanemptylist.ASHIFTinstruction(e.g.,steps1,3,and5inFigure4)removesthenexttokentfromtheinputsequence,constructsaone-nodetreeTconsistingoft,andappends(t,T)totheendofthestacktop‚Äôslist.TheSHIFTinstructionhasnoargument.Whentheparsertriestoparseasub-expressionasasub-tree,itusesaCALLinstructiontocreateanewstackframe.Forexample,beforestep4,thesub-expression‚Äúy"needstobeparsedintoT2withrootId.Inthiscase,aCALLinstructionisexecutedtopushanewframewithanemptylistontothestack.CALLhasanargumentÔ¨Åd,whichisthefunctionIDofthenewframeatthestacktop.ThisfunctionIDcarriesinformationfromthepreviousframetothenewone,e.g.,tohelpdecidetheboundaryofthesub-expression.InFigure5,forexample,whenparsing‚Äúx+y*z"and‚Äúx*y*z",oncetheÔ¨Årsttwotokens(i.e.,‚Äúx+"and‚Äúx*")areconsumed,theparserexecutesaCALLinstructiontocreateanewframetoparsethesub-expressions‚Äúy*z"and‚Äúy"respectively.Sincetheremaininginputsequences(i.e.,‚Äúy*z")arethesameinbothcases,thefunctionIDsprovidetheonlycluetodetecttheboundariesofthesub-expressions.TheparserissuesaREDUCEinstructiontoconstructalargertree,onceallchildrenofitsrootareconstructedandlaidoutinthetopframe‚Äôslist.REDUCEn,(c1,...,cm)hastwoargumentsforspecifyinghowtoconstructthenewtree.Therootofthenewlyconstructedtreeisnandhasm21PublishedasaconferencepaperatICLR2018children.Thej-thchildofnisthecj-thtreeinthestacktop‚Äôslist.Forexample,inFigure4,afterstep8,T1andT2arecombinedtoconstructT3.Thelistinthetopframecontainsthreeelements,i.e.,(Id,T1),(+,+),and(Id,T2).Inthiscase,theREDUCEargumentnisOp+,indicatingthatT3‚ÄôsrootisOp+;forthesecondargument(c1,...,cm),m=2,c1=1andc2=3,indicatingthattheÔ¨Årstandthirdelementsinthelist(i.e.,T1andT2)constitutetheÔ¨ÅrstandsecondchildrenofT3.Notethatthechildrenoftherootareordered.Afterasub-expressionisconvertedintoatreeusingtheREDUCEinstruction,aRETURNinstructioncanbeexecutedtomovethetreeintothepreviousstackframe,sothatitcanbeusedtofurtherconstructlargertrees.Formally,whenthelistinthetopframecontainsonlyoneelement(n,T),RETURN(e.g.,step7inFigure4)popsthestack,andappends(n,T)totheendofnewstacktop‚Äôslist.Whenallinputtokensareconsumedandthestackcontainsonlyonetree,theparserexecutesFINAL(e.g.,step9inFigure4)toterminatethemachine.BothRETURNandFINALhavenoarguments.Validinstructionset.Ateachstep,anLLmachineprovidesasetofvalidinstructionsthatcanbeexecuted.Indoingso,themachinecanguaranteethatthestateremainsvalidiftheinstructionstobeexecutedarealwayschosenfromthisset.WenowdemonstratethathowourLLmachinesrestrictthespaceofthelearnedprograms.Toachievethis,weimposeseveralconstraintsontheinstructiontypesthatcanbeappliedateachtimestep.Wedenotethecurrentstacktopas(Ô¨Åd,l),thelengthoflasL,andtheÔ¨Årsttokenofthecurrentinputastok(tok=EOFifthecurrentinputisempty).Meanwhile,weassumethateachstackframe‚ÄôslisthasatmostKelements,andwewillexplainwhythisassumptionholdslater.TheconstraintsforwheneachoftheÔ¨Åveinstructionsisallowedareasbelow:1.SHIFT:itisallowediftok6=EOFandL<K.2.CALL:itisallowediftok6=EOF,0<L<K,andtheinstructiontypeatprevioustimestepisnotCALL.ForitsargumentÔ¨Åd0,0‚â§Ô¨Åd0<F,whereF>0isahyper-parameterindicatingthenumberofdifferentfunctionIDs.3.RETURN:itisallowedifthecurrentstackhasmorethanoneframe,andL=1.4.REDUCE:itisallowedifL>0,andtheinstructiontypeatprevioustimestepisnotREDUCE.ForREDUCEargumentsnand(c1,...,cm),nischosenfromthenon-terminalset,and1‚â§ci‚â§Lfor1‚â§i‚â§m.5.FINAL:itisallowediftok=EOF,L=1,andthecurrentstackhasonlyoneframe.Moredetails.ThenweexplainwhywecansafelyassumethatthereexistsKsuchthateachstackframe‚ÄôslisthasatmostKelements.Astheparsingprogramcontinues,eachstackframe‚ÄôslistcontainspartiallyÔ¨Ånishedsub-treesthatcorrespondtoapreÔ¨Åxofoneproductionruleinthegrammar.Sincethelengthofproductionrulesinacontext-freegrammarisÔ¨Ånite,wecanassumethattheupperboundofthelengthisK.AccordingtotheinstructionconstraintsimposedbyLLmachines,usingthesameKastheupperboundonthelengthofeachstackframe‚Äôslist,wecanensurethatforeachCALL	1Input‚àóùëßInputùë¶‚àóùëßStack(Id, T1), (+,+)IdxT1Inputùë¶‚àóùëßStack(Id, T1), (+,+)Stack(Id, T1), (+,+)(Id, T2)IdxT1IdxT1IdyT2SHIFT‚Ä¶CALL	2Input‚àóùëßInputùë¶‚àóùëßStack(Id, T1), (*,*)IdxT1Inputùë¶‚àóùëßStack(Id, T1), (*,*)IdxT1RETURN‚Ä¶01010002IdxT1IdyT2Stack(Id, T1), (*,*)(Id, T2)02Figure5:The(partial)executiontracesforparsing‚Äúx+y*z"(above)and‚Äúx*y*z"(bottom)respec-tively.For‚Äúx+y*z",‚Äúy*z"needstobeassociated;when‚Äúy"isreducedtoatreewiththerootId,thetoken"*"needstobeshiftedintothestacktop.Ontheotherhand,for‚Äúx*y*z",‚Äúx*y"needstobeassociated,thusafter‚Äúy"isreduced,theparsetreeof‚Äúx*y"shouldbepoppedbeforeshiftingthenexttoken"*"intheinput.Inthiscase,onlythefunctionIDinthestacktop,i.e.,1(above)or2(bottom),candistinguishwhetherSHIFTorRETURNshouldbeexecutednext.22PublishedasaconferencepaperatICLR2018inputinthegrammar,thereexistsatracesatisfyingsuchconstraintsthatcanparsetheinputtoitsparsetreecorrectly.CMODELARCHITECTUREWeexplainhowthemodelchoosestheinstructiontobeexecutedateachstep.Asforthepredictionofinstructiontypes,Letp(inst|Ô¨Åd,l,tok)bethepredictedprobabilitydistributionoveralldifferentinstructiontypesbytheparsingprogram,whichiscomputedinthewaydescribedinSection3.2.BasedoncurrentstateoftheLLmachine,theLLmachineprovidesasetofvalidinstructiontypes.Thenforeachinstructiontype,ifitisinthesetofvalidinstructiontypes,thenitsprobabilityforsamplingisp(inst|Ô¨Åd,l,tok),otherwiseitsprobabilityissettobe0.UnlessotherwisespeciÔ¨Åed,ateachstep,themodelchoosestheinstructiontypepredictedwiththehighestprobability.ThewaysofpredictingargumentsforCALLandREDUCEinstructionsaresimilar.Wenowgiveananalysisoff(K),whichisthetotalnumberofpossiblecombinationsofc1,...,cmgivenm‚â§K.Weconsiderg(i)asthetotalnumberofc1,...,cicombinationsforaÔ¨Åxedi,thenwehavef(K)=KXi=1g(i)=KXi=1K!(K‚àíi)!=K!(K‚àí1Xi=01i!)Wenowestimateit.Infact,wehavethatexp(1)‚àíK‚àí1Xi=01i!=+‚àûXi=01i!‚àíK‚àí1Xi=01i!=+‚àûXi=K1i!<1K!+1K!2K+1Also,wehaveexp(1)‚àíK‚àí1Xi=01i!>1K!Therefore,wehave0‚â§f(K)‚àíK!exp(1)+1<2K+1<1whereK‚â•2.Therefore,weconcludef(K)=bK!exp(1)‚àí1c.DTRAININGDETAILSBelowwepresentfulldetailsabouthowtotrainthemodel.FollowingSection4,weÔ¨Årstillustratethetrainingapproachwhenweaksupervisionisprovided,andthenexplainhowtotrainthemodelwithinput-outputpairsonly.D.1WEAKLYSUPERVISEDLEARNINGWeassumethatthesetofallparametersisŒò.WeapplyAdamoptimizertoupdateŒò(i+1)‚ÜêŒò(i)‚àíŒ∑‚àÜŒò(i)whereŒ∑isthelearningrate,and‚àÜŒò(i)isthegradientthatconsistsofthreecomponents:‚àÜŒò(i)=Œ≥1¬∑‚àÜŒò1+Œ≥2¬∑‚àÜŒò2+Œ≥3¬∑‚àÜŒò3Inthefollowing,wedescribethethreecomponents‚àÜŒò1,‚àÜŒò2,and‚àÜŒò3respectively.D.1.1REDUCEARGUMENT(c1,...,cm)First,wepresentthedetailsofdiÔ¨Ä(T,T0)inAlgorithm2.TheÔ¨Årstcomponentofthegradientiscomputedasthefollowing:‚àÜŒò1=Xt‚àÇlogp(c(t)1,...,c(t)m|n(t))‚àÇŒò¬∑rreduce(ÀÜT(t)r)wheretiteratesoverallREDUCEoperations,c(t)1,...,c(t)mandn(t)indicatethepredictedargumentsinthet-thoperation,andÀÜT(t)rindicatesthepredictedtreeinthet-thoperation.23PublishedasaconferencepaperatICLR2018Algorithm2ThealgorithmtocomputethedifferencebetweenTandT0.Inthealgorithm,weuseT=N(T1,...,Tj)toindicatethatT‚Äôsrootisnon-terminalN,whichhasjchildrenT1,...,Tj.functiondiÔ¨Ä(T,T0)T=N(T1,...,Tj)T0=N0(T01,...,T0j0)ifN=N0thensum‚Üê0elsesum‚Üê1endififj<j0thenfori‚Üê1tojdosum‚Üêsum+diÔ¨Ä(Ti,T0i)endforfori‚Üêjtoj0dosum‚Üêsum+|T0i|endforelsefori‚Üê1toj0dosum‚Üêsum+diÔ¨Ä(Ti,T0i)endforfori‚Üêj0tojdosum‚Üêsum+|Ti|endforendifreturnsumendfunctionD.1.2REDUCEARGUMENTnForlearningtopredicttheREDUCEargumentn,wecanusereinforcementlearningtechniquesimilartothemethodabove.Inthefollowing,wepresentanothertrainingmethodusingsupervisedlearning.Weobservethatsuchatrainingmethodismoretime-efÔ¨Åcientinourexperiments.WeÔ¨ÅrstmatcheachREDUCEoperationtoatentativegroundtruth.GiventhepredictedtreeÀÜTandthegroundtruthTg,wematcheachnodeinÀÜTtoanodeinTginthefollowingway.AssumingthatÀÜT=ÀÜN(ÀÜT1,...,ÀÜTk)andTg=Ng(Tg1,...,Tgk0),ÀÜNismatchedtoNgÔ¨Årst,thenÀÜTiismatchedtoTgirecursivelyfori=1,...,min(k,k0).Ifk>k0,thenÀÜTifori‚àà{k0,...,k}ismatchedtoanygroundtruth.Afterwards,thesecondcomponentiscomputedasfollows:‚àÜŒò2=Xt‚àÇlogp(n(t)g|Ô¨Åd(t),l(t),tok(t))‚àÇŒòwheretiteratesoverallREDUCEoperationssuchthatthegeneratednon-terminalhasamatchedtentativegroundtruthn(t)g,andlogp(n(t)g|Ô¨Åd(t),l(t),tok(t))isthecross-entropylossbetweenp(n(t)|Ô¨Åd(t),l(t),tok(t))andtheone-hotvectorofn(t)g.D.1.3CALLARGUMENTÔ¨ÅdWeÔ¨ÅrstgiveanexampletoillustrateourdesignofrewardfunctionrfinFigure6.Thethirdcomponentiscomputedasfollows:‚àÜŒò3=Xt‚àÇlogp(Ô¨Åd0(t)|Ô¨Åd(t),l(t),tok(t))‚àÇŒò¬∑rf(Ô¨Åd0(t))wheretiteratesoverallCALLoperations.24PublishedasaconferencepaperatICLR2018‚Ä¶CALLùëñùëõùë†ùë°1ùëñùëõùë†ùë°2ùëñùëõùë†ùë°3CALL‚Ä¶Prediction‚Ä¶CALL‡∑£ùëñùëõùë†ùë°1‡∑£ùëñùëõùë†ùë°2‡∑£ùëñùëõùë†ùë°3‡∑£ùëñùëõùë†ùë°4‚Ä¶Ground truthùëô(‡∑£ùëñùëõùë†ùë°1,ùëñùëõùë†ùë°1)ùëô(‡∑£ùëñùëõùë†ùë°2,ùëñùëõùë†ùë°2)ùëô(‡∑£ùëñùëõùë†ùë°3,ùëñùëõùë†ùë°3)ùëô(‡∑£ùëñùëõùë†ùë°4,CALL)‚àëùëüùëì1ùõº1ùõº2ùõº3ùõº4Figure6:Theillustrationoftherewardfunctionrf.Theinstructionscoloredorangeindicatethegroundtruth,wherenoneofinst1,inst2,andinst3isaCALLinstruction.TherewardfunctionrfforthepredictionoftheÔ¨ÅrstCALL‚Äôsargument1(inthegreencircle)isthesummationoffourlosses,wherethelossfunctionisthecrossentropyloss.SHIFTREDUCE<Id>	(1)Input+ùë¶StackIdxT1REDUCE<Op+>	(1,	2)Input+ùë¶StackInputùë•+ùë¶StackInput+ùë¶StackIdxT1CALL0SHIFTInputùë¶StackIdxT1SHIFTInputEOFStackIdxT1REDUCE<Id>	(2)InputEOFStackIdxT1IdyT2RETURNInputEOFStackIdxT1IdyT2IdIdxyInputEOFStackT3FINALIdIdxy123456789000(x,x)0(Id,T1)00(Id, T1)00(+,+)0(Id, T1)00(+,+),(y,y)0(Id, T1)00(Id,T2)0(Id, T1)0(Id,T1),(Id,T2)0(Op+,T3)Op+Op+Figure7:Awrongexecutiontracethatcancorrectlyparsex+y.Thewrongoperations(i.e.,steps3,4,6,and8)arecoloredpurple.D.2TRAININGWITHINPUT-OUTPUTPAIRSONLYInthissection,wefurtherdescribethealgorithmfortrainingwithinput-outputpairsonly,especiallyforhowtosearchforthecandidatetraceset.AsexplainedinSection4.1,thealgorithmneedstoÔ¨Åndthesetofvalidcandidatetracesforeachinput-outputexample.Noticethatforoneinput-outputexample,thepossiblevalidexecutiontracesarenotunique.Figure7providesonealternativeexecutiontracethatsuccessfullyparsesx+yintoitsparsetree.Onlywhencombiningmultipleexamples,themodeltrainedwiththistracecannotÔ¨Åtallexamplesatthesametime.Searchingforthecandidatetraceset.Herewefurtherexplainthetwo-nested-loopprocesstosearchforthecandidatetracesetfollowingSection4.1.First,intheouterloop,werandomlysampleaninstructiontypetracebasedonthedistributiondescribedinSection4.1.Thenintheinnerloop,wetrytousethesampledtraceintheexternalloopasthetentativegroundtruth,andthenemploytheweaklysupervisedlearningapproachtotraintheparameterspredictingtheargumentsforM1iterations.IfinanyoftheseM1iterations,thecorrectoutputisproduced,weaddthesampledinstructiontracetothecandidatetraceset.Otherwise,ifthecorrectoutputisneverproducedduringtheseM1iterations,werevertthemodel‚ÄôsparameterstopredicttheargumentsbacktothosebeforetheseM1weaksupervisedlearningiterations,andcontinuesamplinganotherinstructiontrace.This25PublishedasaconferencepaperatICLR2018processiscontinuedforM2iterations,i.e.,atotalofM2instructiontracesaresampled.Meanwhile,toescapefromasub-optimalmodel,were-initializethemodelwiththeonelearnedfromthepreviouslessoneveryM3iterations.EHYPERPARAMETERSOFOURPROPOSEDMETHODFortheLLmachines,F=10.AboutthecapacityofeachstackframeK,K=3forWHILElanguage,andK=4forLAMBDAlanguage.Inthearchitectureoftheneuralparsingprogram,eachLSTMhas1layer,withitshiddenstatesizeD=50,whichisthesameastheembeddingsize.Asforthetraining,learningrateisŒ∑=0.01withnodecay.Nodropoutisused.Gradientweightsforthethreecomponents‚àÜŒò1,‚àÜŒò2and‚àÜŒò3areŒ≥1=10.0,Œ≥2=1.0,andŒ≥3=0.01respectively.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingAdamoptimizer.Allweightsareinitializeduniformlyrandomlyin[‚àí0.1,0.1].Themini-batchsizeis1.Forcandidatetracesearch,œÉ=0.1,M1=20,M2=10,000,andM3=2,000.Typically,foreachinput,thecorrecttracecouldbefoundaftersamplingwithin1,000traces.FHYPERPARAMETERSOFBASELINEMODELSForthebaselinemodelsinourevaluation,i.e.,seq2seqVinyalsetal.(2015b),seq2treeDong&Lapata(2016),andLSTMwithunboundedmemoryGrefenstetteetal.(2015),weimplementthemourselves.Wechoosetheirhyperparametersbasedontheirpapersrespectively,andfurthertuneonourdatasetstogetbetterexperimentalresults.SpeciÔ¨Åcally,intheseq2seqmodelVinyalsetal.(2015b),eachoftheencoderandthedecoderisa3-layerLSTM,andthehiddenstatesizeofeachlayeris256,whichisthesameastheembeddingsize.WeapplytheattentionmechanismdescribedinVinyalsetal.(2015b).Asfortraining,learningrateis0.01.Thedropoutrateis0.5.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingAdamoptimizer.Allweightsareinitializeduniformlyrandomlyin[‚àí0.1,0.1].Themini-batchsizeis256.Intheseq2treemodelDong&Lapata(2016),eachoftheencoderandthedecoderisa1-layerLSTM,anditshiddenstatesizeis256,whichisthesameastheembeddingsize.WeapplytheattentionmechanismdescribedinDong&Lapata(2016).Asfortraining,learningrateis0.005.Thedropoutrateis0.5.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingRMSPropoptimizer.Allweightsareinitializeduniformlyrandomlyin[‚àí0.1,0.1].Themini-batchsizeis20.InStack-LSTM,Queue-LSTMandDeQue-LSTMmodelsdescribedinGrefenstetteetal.(2015),eachoftheencoderandthedecoderisa1-layerLSTM,anditshiddenstatesizeis256,whichisthesameastheembeddingsize.Asfortraining,learningrateis0.001.Wedonotusedropoutforthesemodels.GradientswithL2normlargerthan1.0arescaleddowntohavethenormof1.0.ThemodelistrainedusingRMSPropoptimizer.Allweightsareinitializeduniformlyrandomlyin[‚àí0.1,0.1].Themini-batchsizeis10.GWHILELANGUAGEBelowisthegrammarspeciÔ¨ÅcationoftheWHILElanguage.26PublishedasaconferencepaperatICLR2018<Identifier>::=x|y<Literal>::=0|1<Op*>::=<Identifier>√ó<Identifier>|<Identifier>√ó<Literal>|<Literal>√ó<Identifier>|<Literal>√ó<Literal>|<Op*>√ó<Identifier>|<Op*>√ó<Literal><Op+>::=<Identifier>+<Identifier>|<Identifier>+<Literal>|<Identifier>+<Op*>|<Literal>+<Identifier>|<Literal>+<Literal>|<Literal>+<Op*>|<Op+>+<Identifier>|<Op+>+<Literal>|<Op+>+<Op*>|<Op*>+<Identifier>|<Op*>+<Literal>|<Op*>+<Op*><Eq>::=<Identifier>==<Identifier>|<Identifier>==<Literal>|<Identifier>==<Op+>|<Identifier>==<Op*>|<Literal>==<Identifier>|<Literal>==<Literal>|<Literal>==<Op+>|<Literal>==<Op*>|<Op+>==<Identifier>|<Op+>==<Literal>|<Op+>==<Op+>|<Op+>==<Op*>|<Op*>==<Identifier>|<Op*>==<Literal>|<Op*>==<Op+>|<Op>==<Op*><Assign>::=<Identifier>=<Identifier>|<Identifier>=<Literal>|<Identifier>=<Op+>|<Identifier>=<Op*><If>::=<Assign>if<Identifier>|<Assign>if<Literal>|<Assign>if<Op+>|<Assign>if<Op*>|<Assign>if<Eq>|<If>if<Identifier>|<If>if<Literal>|<If>if<Op+>|<If>if<Op*>|<If>if<Eq>27PublishedasaconferencepaperatICLR2018<Seq>::=<Assign>;<Assign>|<Assign>;<If>|<Assign>;<While>|<If>;<Assign>|<If>;<If>|<If>;<While>|<While>;<Assign>|<While>;<If>|<While>;<While>|<Seq>;<Assign>|<Seq>;<If>|<Seq>;<While><Block>::={<Assign>}|{<If>}|{<While>}|{<Seq>}<While>::=while<Identifier><Block>|while<Literal><Block>|while<Op+><Block>|while<Op*><Block>|while<Eq><Block>HLAMBDALANGUAGEBelowisthegrammarspeciÔ¨ÅcationoftheLAMBDAlanguage.<Var>::=a|b|...|z<App>::=<Var><Var>|<App><Var><Bind>::=lama|...|lamz<Lam>::=<Bind>.<Var>|<Bind>.<App>|<Bind>.<Lam>|<Bind>.<Let><LetExpr>::=<Var>=<Var>|<Var>=<App>|<Var>=<Lam>|<Var>=<Let><Let>::=let<LetExpr>in<Var>|let<LetExpr>in<App>|let<LetExpr>in<Lam>|let<LetExpr>in<Let>IPYTHONIMPLEMENTATIONOFWHILELANGUAGEPARSER1defnextInstruction(self):2fid,top=self.fid[‚àí1],self.stack[‚àí1]3next=self.input[self.cur]ifself.cur<len(self.input)elseNone4iflen(top)==0:5returnself.shift,None6eliflen(top)==1:7iftop[0][1]==‚Äôwhile‚Äô:8returnself.call,09eliftop[0][1]==‚Äô{‚Äô:10returnself.call,011eliftop[0][1]==‚Äôx‚Äôortop[0][1]==‚Äôy‚Äô:12returnself.reduce,(IDENT,[0])13eliftop[0][1]==‚Äô0‚Äôortop[0][1]==‚Äô1‚Äô:28PublishedasaconferencepaperatICLR201814returnself.reduce,(LIT,[0])15elifnext==‚Äô;‚Äô:16iffid<1:17returnself.shift,None18else:19returnself.ret,None20elifnext==‚Äôif‚Äô:21iffid<2:22returnself.shift,None23else:24returnself.ret,None25elifnext==‚Äô=‚Äô:26iffid<3:27returnself.shift,None28else:29returnself.ret,None30elifnext==‚Äô==‚Äô:31iffid<4:32returnself.shift,None33else:34returnself.ret,None35elifnext==‚Äô+‚Äô:36iffid<5:37returnself.shift,None38else:39returnself.ret,None40elifnext==‚Äô‚àó‚Äô:41iffid<6:42returnself.shift,None43else:44returnself.ret,None45elifnext==None:46iflen(self.stack)==1:47returnself.final,None48else:49returnself.ret,None50else:51returnself.ret,None52eliflen(top)==2:53iftop[0][1]==‚Äô{‚Äôandnext==‚Äô}‚Äô:54returnself.shift,None55else:56next_fid=057iftop[0][1]==‚Äôwhile‚Äô:58next_fid=659eliftop[1][1]==‚Äô;‚Äô:60next_fid=161eliftop[1][1]==‚Äôif‚Äô:62next_fid=263eliftop[1][1]==‚Äô=‚Äô:64next_fid=365eliftop[1][1]==‚Äô==‚Äô:66next_fid=467eliftop[1][1]==‚Äô+‚Äô:68next_fid=569eliftop[1][1]==‚Äô‚àó‚Äô:70next_fid=671returnself.call,next_fid72else:#len(top)==373iftop[1][1]==‚Äô=‚Äô:74returnself.reduce,(ASSIGN,[0,2])75eliftop[1][1]==‚Äôif‚Äô:76returnself.reduce,(IF,[2,0])77eliftop[1][1]==‚Äô;‚Äô:78returnself.reduce,(SEQ,[0,2])29PublishedasaconferencepaperatICLR201879eliftop[0][1]==‚Äôwhile‚Äô:80returnself.reduce,(WHILE,[1,2])81eliftop[0][1]==‚Äô{‚Äôandtop[2][1]==‚Äô}‚Äô:82returnself.reduce,(BLOCK,[1])83else:84iftop[1][1]==‚Äô+‚Äô:85returnself.reduce,(OP_P,[0,2])86eliftop[1][1]==‚Äô‚àó‚Äô:87returnself.reduce,(OP_M,[0,2])88eliftop[1][1]==‚Äô==‚Äô:89returnself.reduce,(EQ,[0,2])JPYTHONIMPLEMENTATIONOFLAMBDALANGUAGEPARSER1defnextInstruction(self):2fid,top=self.fid[‚àí1],self.stack[‚àí1]3next=self.input[self.cur]ifself.cur<len(self.input)elseNone4iflen(top)==0:5returnself.shift,None6eliflen(top)==1:7iftop[0][1]==‚Äôlet‚Äô:8returnself.call,09eliftop[0][1]==‚Äôlam‚Äô:10returnself.shift,None11eliftop[0][0]<0andtop[0][1]inself.alpha:12returnself.reduce,(VAR,[0])13else:14ifnextinself.alpha:15iffid==0:16returnself.call,117else:18returnself.ret,None19elifnext==‚Äô=‚Äôornext==‚Äô.‚Äô:20returnself.shift,None21else:22iflen(self.stack)>1:23returnself.ret,None24else:25returnself.final,None26eliflen(top)==2:27iftop[0][1]==‚Äôlet‚Äô:28returnself.shift,None29eliftop[0][1]==‚Äôlam‚Äô:30returnself.reduce,(BIND,[1])31eliftop[1][1]==‚Äô=‚Äô:32returnself.call,033eliftop[0][1]==BIND:34returnself.call,035else:36returnself.reduce,(APP,[0,1])37eliflen(top)==3:38iftop[0][1]==‚Äôlet‚Äôortop[0][1]==‚Äôlam‚Äô:39returnself.call,040else:41nt=LETEXPR42iftop[0][1]==BIND:43nt=LAMBDA44returnself.reduce,(nt,[0,2])45else:#len(top)==4:46returnself.reduce,(LET,[1,3])30PublishedasaconferencepaperatICLR2018KMOREANALYSISOFTHEROBUSTFILLDSLEfÔ¨ÅcientlyenumeratetheRobustFillspace.RobustFillDSLallowsprogramstoemitaconcate-nationofupto10constructors,eachofwhicheitheroutputsaconstrantcharacter,ortheresultofextractingasubstringfromtheinputandperformingastringtransformationsuchasreplace,trim,etc.Foreachconstructor,therecanbeapproximately30millionuniqueexpressions,andthusthetotalnumberofprogramswithupto10constructorscanbehuge.ToefÔ¨Åcientlyenumeratethebestexpression,werelyonheuristicstocutmostprogramsthatdonotleadtothebestaccuracy.Infact,ifweenumeratetheexpressionforeachconstructorfromtheÔ¨Årsttothelast,weneedtorequirethatthepartialprogramgeneratedshouldmatchthe‚Äúmostnumberofinput-outputexamples".Tothisend,wemanuallyconstructagoodprogram,andassumeitsaccuracyisp.Foreachpartialprogram,ifitcannotyieldabetteraccuracybyp,thentheprogramwillbedropped.Thatis,thereareatleast(1‚àíp)NinputsthattheprogramwillresultinanoutputthatdoesnotmatchthepreÔ¨Åxofthegroundtruth.Further,therewillbeemptyconstructorswhichwilloutputanemptystringforanyinput.TheywillcausetheenumerationinefÔ¨Åcient,andwealsoeliminateallemptyconstructorsfrombeingenumerated.Indoingso,wecanestimateanupperboundontheaccuracythatcanbeachievedbytheRobustFillapproach.WhyRobustFillDSLcannothandletheparsingproblem?Inthissection,weprovethattheRobustFillDSLisnotexpressivefortheparsingproblem.Infact,thetop-levelconstructoroftheDSLallowsanarbitrarynumberoftokensconcatenatedtogether.SincetheRobustFillapproachhasthelimitationtosynthesizeonlyprogramswithupto10tokens,weputthisconstrainttotheDSL.AlthoughthisconstraintlooksartiÔ¨Åcial,thisisnotthefundamentalreasonforwhytheDSLisnotexpressiveenough.Eachofthese10tokenscanbeeitheraconstantchar,oratransformationofthesubstringoftheinputsequence.Therefore,theoutputofanyprogramoftheDSLcanhaveupto10tokens.However,inthetrainingset,eachparsetreehasmorethan20tokens,andthusnoprogramcangeneratetheparsetree.NotethattheproofworksforanyprogramswithÔ¨Ånitelength.Givenaprogramoflengthn,itcannotgenerateanoutputwithmorethanntokens,butthetestsetcancontainarbitrarilylongoutputs.Therefore,anyspeciÔ¨Åcprogramcannotgeneralizetolongeroutputs.LMISCELLANEOUSWepresentthethree-linePythonimplementationofQuicksortbelow:defqsort(a):iflen(a)<=1:returnareturnqsort([xforxinaifx<a[0]])+\[xforxinarrayifx==a[0]]+qsort([xforxinaifx>a[0]])31