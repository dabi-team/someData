8
1
0
2

r
a

M
8

]

G
L
.
s
c
[

4
v
4
8
2
1
0
.
6
0
7
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2018TOWARDSSYNTHESIZINGCOMPLEXPROGRAMSFROMINPUT-OUTPUTEXAMPLESXinyunChenChangLiuDawnSongUniversityofCalifornia,BerkeleyABSTRACTInrecentyears,deeplearningtechniqueshavebeendevelopedtoimprovetheper-formanceofprogramsynthesisfrominput-outputexamples.Albeititssigniﬁcantprogress,theprogramsthatcanbesynthesizedbystate-of-the-artapproachesarestillsimpleintermsoftheircomplexity.Inthiswork,wemoveasigniﬁcantstepforwardalongthisdirectionbyproposinganewclassofchallengingtasksinthedomainofprogramsynthesisfrominput-outputexamples:learningacontext-freeparserfrompairsofinputprogramsandtheirparsetrees.Weshowthatthisclassoftasksaremuchmorechallengingthanpreviouslystudiedtasks,andthetestaccuracyofexistingapproachesisalmost0%.Wetacklethechallengesbydevelopingthreenoveltechniquesinspiredbythreenovelobservations,whichrevealthekeyingredientsofusingdeeplearningtosynthesizeacomplexprogram.First,theuseofanon-differentiablemachineisthekeytoeffectivelyrestrictthesearchspace.Thusourproposedapproachlearnsaneuralprogramoperatingadomain-speciﬁcnon-differentiablemachine.Second,recursionisthekeytoachievegeneralizability.Thus,webake-inthenotionofrecursioninthedesignofournon-differentiablemachine.Third,reinforcementlearningisthekeytolearnhowtooperatethenon-differentiablemachine,butitisalsohardtotrainthemodeleffectivelywithexistingreinforcementlearningalgorithmsfromacoldboot.Wedevelopanoveltwo-phasereinforcementlearning-basedsearchalgorithmtoovercomethisissue.Inourevaluation,weshowthatusingournovelapproach,neuralparsingprogramscanbelearnedtoachieve100%testaccuracyontestinputsthatare500×longerthanthetrainingsamples.1INTRODUCTIONLearningadomain-speciﬁcprogramfrominput-outputexamplesisanimportantopenchallengewithmanyapplications(Balogetal.,2017;Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017;Devlinetal.,2018;Parisottoetal.,2017;Gulwanietal.,2012;Gulwani,2011).Approachesinthisdomainlargelyfallintotwocategories.Onelineofworklearnsaneuralnetwork(i.e.,afully-differentiableprogram)togenerateoutputsfrominputsdirectly(Vinyalsetal.,2015b;Aharoni&Goldberg,2017;Dong&Lapata,2016;Devlinetal.,2018).Despitetheirpromisingperformance,theseapproachestypicallycannotgeneralizewelltopreviouslyunseeninputs.Anotherlineofworksynthesizesanon-differentiable(discrete)programinadomain-speciﬁclanguage(DSL)usingeitheraneuralnetwork(Devlinetal.,2018;Parisottoetal.,2017)orSMTsolvers(Ellisetal.,2016).However,thecomplexityofprogramsthatcanbesynthesizedusingexistingapproachesislimited.Althoughmanyeffortsaredevotedintotheﬁeldofneuralprogramsynthesis,allofthemarestillfocusingonsynthesizingsimpletextbook-levelprograms,suchasarraycopying,Quicksort,andacombinationofnomorethan10stringoperations.Webelievethatthenextimportantstepforthecommunityistoconsidermorecomplexprograms.Inthiswork,weendeavortopursuethisdirectionandmoveabigstepforwardtosynthesizemorecomplexprogramsthanbefore.Alongtheway,weidentifyseveralnovelchallengesdealingwithcomplexprogramsthathavenotbeenfullydiscussedbefore,andproposenovelprincipledapproachestotacklethem.First,anend-to-enddifferentiableneuralnetworkishardtogeneralize,andinsomecasesishardtoevenachieveatestaccuracythatisgreaterthan0%.Weobservethataneuralnetworkistooﬂexibletoapproximateanyfunctions,buttheprogramsthatwewanttosynthesizetypicallylie1 
 
 
 
 
 
PublishedasaconferencepaperatICLR2018inasearchspaceofinterest.Itisveryhardtorestrictthelearnedneuralnetworktoalwaysrepresentaninstanceinthesearchspace.Whennot,thenetworkissimplyoverﬁttingtothetrainingdata,andthuscannotgeneralize.Tomitigatethisissue,weemploytheapproachtotrainadifferentiableneuralprogramtooperateadomain-speciﬁcnon-differentiablemachine.Thiscombinationallowsustorestrictthesearchspacebydeﬁningthenon-differentiablemachine,sothatanyneuralprogramthatcanoperatethemachineisalwaysavalidprogramofinterest.Second,thedomain-speciﬁcmachineneedstobeexpressiveenough.Inparticular,state-of-the-artapproaches,suchasRobustFill(Devlinetal.,2018),mayfailattasksinvolvinglongoutputsbecausetheycanonlysynthesizeprogramsofupto10stringoperations,whichdonotsupportrecursion.AsalsonotedbyCaietal.(2017),recursionisakeyconcepttoenableperfectgeneralization.Therefore,itisdesirablethatthenon-differentiablemachinecanbake-intheconceptofrecursionintothedesign.Third,thenon-differentiablemachinemakesthemodelhardtobetrainedend-to-end,especiallywhenthetracestooperatethemachinearenotgivenduringthetraining.Thus,werelyonareinforcementlearningalgorithmtotraintheneuralprogramwhilerecoveringtheexecutiontraces.However,thisischallenging.Previousattempts(Zaremba&Sutskever,2015)alongthisdirectioncanonlysucceedtolearntocomputeadditionoftwonumbers,andfailevenforthetasksofthree-numberadditions.Inourevaluation,weobservethattrainingfromacoldstartisthemaindifﬁculty.Inparticular,themodeltrainedusingexistingreinforcementlearningalgorithmsfromacoldbootalwaysgetsstuckatalocalminimumtoﬁttoonlyasubsetofthetrainingsamples.Moreimportantly,therecoveredtracesaresometimes“wrong”,whichaggravatestheissue.Tothebestofourknowledge,thisissueisalong-standingchallengingproblemforneuralprogramsynthesis,andwearenotawareofanypromisingsolution.Totacklethisissue,weproposeatwo-phasereinforcementlearning-basedalgorithm.Intuitively,webreakupthewholeproblemintotwoseparatetasks:(1)searchingforpossibletraces;and(2)trainingthemodelwiththesupervisionoftraces.Eachofthesetwotasksareeasierforreinforcementlearningtohandle,andwedevelopanestedalgorithmtocombinethesolutionstothesetwotaskstoprovidetheﬁnalsolution.Todemonstrateourideas,weproposeanovelchallengingproblem:learningaprogramtoparseaninputsatisfyingan(unknown)context-freegrammarintoitsparsetree.Aswewillshow,thisclassofprogramsaremuchmorechallengingtolearnthanthoseconsideredpreviously:usingmoststate-of-the-artapproaches,thetestaccuracyalmostremains0%whenthetestinputsarelongerthanthetrainingones.Meanwhile,learningaparserisalsoanimportantproblemonitsownwithmanyapplications,suchaseasingthedevelopmentofdomain-speciﬁclanguagesandmigratinglegacycodeintonovelprogramminglanguages.Inthissense,thisproblemexhibitsbothmorecomplexityandmorepracticalitythansomepreviouslyconsideredproblems,suchassynthesizinganarray-copyingfunction.Therefore,ournewlyproposedproblemservesasagoodnextstepchallengetotackleinthedomainoflearningprogramsfrominput-outputexamples.Toimplementtheideaoflearninganeuralprogramoperatinganon-differentiablemachine,weﬁrstdesigntheLLmachine,whichbakesintheconceptofrecursioninitsdesign.Wealsodesignaneuralparsingprogram,suchthateveryneuralparsingprogramoperatinganLLmachineisrestrictedtorepresentanLLparser.Toevaluateourapproach,wedeveloptwoprogramminglanguages,animperativeoneandafunctionalone,astwodiversetasks.Combinedwithournewlyproposedtwo-phasereinforcementlearning-basedalgorithm,wedemonstratethatforbothtasks,ourapproachcanachieve100%accuracyontestsamplesthatare500×longerthantrainingsamples,whileexistingapproaches’correspondingtestaccuraciesare0%.Tosummarize,ourworkmakesthefollowingcontributions:(1)weproposeanovelstrategythatcombinestrainingneuralprogramsoperatinganon-differentiablemachinewithreinforcementlearn-ing,andthisstrategyallowsustosynthesizemorecomplexprogramsfrominput-outputexamplesthanthosethatcanbehandledbyexistingtechniques;(2)werevealthreeimportantobservationswhysynthesizingcomplexprogramscanbechallenging;(3)weproposeanoveltwo-phasereinforcementlearning-basedalgorithmtosolvethecoldstarttrainingproblem,whichmaybeofindependentinterests;(4)weproposetheparserlearningproblemasanimportantandchallengingnextstepforprogramsynthesisfrominput-outputexamplesthatexistingapproachesfailwith0%accuracy;(5)wedemonstratethatourstrategycanbeappliedtosolvetheparserlearningproblemwith100%accuracyontestsamplesthatare500×longer.Weconsiderapplyingourstrategytomorecomplextasksotherthantheparserlearningproblemasfuturework.2PublishedasaconferencepaperatICLR2018a = 1 if x==yIdIdxyInput:Output:EqIfIdLita1AssignFigure1:Aninput-outputexample.Theinputisasequenceoftokens[“a",“=",“1",“if",“x",“==",“y"],andtheoutputisitsparsetree.Thenon-terminalsaredenotedbyboxes,andterminalsaredenotedbycircles.2THEPARSINGPROBLEMANDAPPROACHOVERVIEWToillustrateourstrategytowardssynthesizingcomplexprograms,wewanttoputourpresentationinacontext.Tothisend,inthissection,wedeﬁnetheparsingproblemandoutlineourapproach.Notethatourstrategycouldalsobeadaptedforotherproblems.Inthefollowing,westartwiththeformaldeﬁnitionoftheparsingproblem.Deﬁnition1(Theparsingproblem)Assumethereexistacontext-freelanguageLandaparsingoracleπthatcanparseeveryinstanceinLintoanabstractsyntaxtree.BothLandπareunknown.TheproblemistolearnaparsingprogramP,suchthat∀I∈L,P(I)=π(I).Figure1providesanexampleofaninputanditsoutputparsetree.Theinternalnodesofthetreearecallednon-terminals,andtheleafnodesarecalledterminals.Thesetsofnon-terminalsandterminalsaredisjoint.Eachterminalmustcomefromoneinputtoken,butthenon-terminalsdonotnecessarilyhavesuchacorrespondence.Tosimplifytheproblem,weassumetheinputisalreadytokenized.Thesetsofallnon-terminalsandterminalscanbeextractedfromthetrainingcorpus,i.e.,allnodesintheoutputparsetreesoftrainingsamples.Inthiswork,weassumethevocabularyset(i.e.,allterminalsandnon-terminals)isﬁnite,andourworkcanbeextendedtohandleunboundedvocabularysetwithtechniquessuchaspointernetworks(Vinyalsetal.,2015a).Remarks.Notethatourparsingproblemhasitscounterparttohandlenaturallanguages,whichhasbeenextensivelystudiedintheliterature(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016;Dyeretal.,2016).Wewanttoremarkonthedifferencebetweenthetwoproblems.Ontheonehand,aprogramminglanguagewithacontext-freegrammariseasiertolearnthananaturallanguageinthesensethatthegrammarhasarigorousspeciﬁcationtoavoidambiguity.Thatis,itisalwayspossibletoconstructaparsertoachieve100%accuracyforacontext-freeprogramminglanguage,whilethismaynotbethecaseforanaturallanguage.Ontheotherhand,learningaprogramminglanguageparsermaybemorechallengingthananaturallanguageparser,sinceaninstanceinaprogramminglanguagecanbearbitrarilylong,whileanaturallanguagesentencetypicallyhasonlyalimitednumberofwords.Inthissense,anapproachthatcanlearnanaturallanguageparserwellmaynotbeabletohandleaprogramminglanguage,whenthetestsamplesaremuchlongerthantrainingsamples.Aswewillobserveinourevaluation,thisisindeedanissueforexistingapproaches.Wealsowanttoremarkonpotentialapplicationsoftheparsingproblem.Nowadays,weneedtodevelopnewdomain-speciﬁclanguagesinmanyscenarios.Whilethecurrentpracticeistodevelopthegrammarandparsermanually,thisprocessiserror-prone.Inourexperience,whencreatingthedatasetsforevaluation,weﬁndthatdesigningatrainingsetof(program,parsetree)pairsistypicallyeasier,butdevelopingtheparsertakes2×or3×moretimethandevelopingthetrainingset.Intuitively,buildingatrainingsetonlyneedsdevelopingatutorialincludingbasicexampleswhoseparsetreesareeasytoconstructmanually;ontheotherhand,implementingaparserrequiresmuchlongertimeindebugging,typicallywiththehelpofthedevelopedtrainingset.Therefore,ourproposedparsergenerationproblemisanovelpracticalusecaseofprogram-by-example.Challenges.LearningtheparsingprogramPischallengingforseveralreasons.First,thecor-respondencebetweennon-terminalsandinputtokensisunknown.Forexample,inFigure1,theparserneedstoﬁndoutthattoken“="correspondstothenon-terminalAssign.Second,theorderofnon-terminalsinthetreemaynotalignwellwiththeinputtokens.Forexample,inFigure1,thesub-expression“a=1",whichistotheleftofthesub-expression“x==y",correspondstothe3PublishedasaconferencepaperatICLR2018rightchildofthenon-terminalIf,whichistotherightofthesub-treecorrespondingto“x==y"(thesub-treewhoserootisthenon-terminalEq).Third,theassociationoftokensmaydependonothertokens.Forexample,inexpressions“x+y*z"and“x+y+z",whether“x+y"formsasub-treedependsontheoperator(i.e.,“+"or“*")afterit.Solutionoverviewandpaperorganization.Totacklethechallenges,wemakeseveralinnovations.First,weemployaparadigmtolearnadifferentiableneuralprogramoperatinganon-differentiablemachineinSection3.Inparticular,weintroduceLLmachines(Section3.1)asanexampleofnon-differentiablemachine.LLmachinescanrestrictthatalearnedprogramalwaysrepresentsaprogramofinterest,i.e.,anLLparser.Tobake-inthenotionofrecursion,wethinkthenon-differentiablemachineshouldhaveastackstructureandprovideCALLandRETURNinstructionstosimulaterecursivecalls.Inaddition,theneuralprogramshouldoperatethemachinebasedononlythestacktoptomakesurethelearnedprogramcangeneralize.WedesigntheLLmachineandneuralparsingprograms(Section3.2)tooperateanLLmachinefollowingtheseprinciples.Wewillshowthatindoingso,theneuralparsingprogramcanbelearnedtogeneralizetolongerprograms.NotethatherewemainlyusetheLLmachinesandtheneuralparsingprogramtodemonstratethatadesignsupportingrecursionisnecessarytoachievegeneralization,andourstrategyisnotlimitedtothiscombinationonly.Third,wedesignanoveltwo-phasereinforcementlearning-basedsearchalgorithm(Section4)totacklethechallengeoftraininganeuralparsingprogramwithoutthesupervisionofexecutiontraces.Inourevaluation(Section6),wedemonstratethatourapproachachieves100%trainingaccuracy,whiletheaccuraciesonalltestsetsarealso100%.WethendiscussrelatedworkinSection7,andconcludeinSection8.3NEURALPROGRAMSOPERATINGANON-DIFFERENTIABLEMACHINEInthissection,wedemonstratehowtoemploytheneuralprogramoperatinganon-differentiablemachineapproachtotackletheneuralparsingproblem.Tocarryoutthisagenda,weﬁrstdesignLLmachinesinSection3.1.Thedesignbakesinthenotionofrecursion.Thatis,anLLmachinehasastackforrecursivecallstacks,andprovidesaCALLinstructionandaRETURNinstructionwhichcanbeusedtosimulaterecursivecalls.Then,wedesignaneuralprogramoperatingtheLLmachineinSection3.2.WeshowthattheneuralprogrammakesitsdecisionsbasedononlythestacktopintheLLmachine.Indoingso,theneuralprogramcantakeadvantageoftherecursionsupportofanLLmachine,andbelearnedtogeneralizetohandlelongerinputs.Wepresentthedetailsinthefollowing.3.1LLMACHINESWebrieﬂypresentthedesignofLLmachines.ItisinspiredbytheLL(1)parsingalgorithm(Parr&Quong,1995),andanLLmachineisgenericenoughtoallowconstructanyLLparsers.Forexample,inourevaluation,wedemonstratethatbothanimperativelanguageandafunctionallanguagecanusetheLLmachinetoconstructtheirparsers.TheLLmachinemaintainsaninternalstateusingastackofframesandanexternalstateofthestreamofinputtokens.EachstackframeisanID-listpair,wheretheIDisafunctionIDandinthelistare(n,T)pairs,whereTisaparsetree,andnistherootnodeofT.AnLLmachinehasﬁvetypesofinstructions:SHIFT,RETURN,CALL,REDUCE,andFINAL.TheirsemanticsarepresentedinTable1.Intuitively,theseinstructionscanbeclassiﬁedintothreeclasses.Theﬁrstclass,includingSHIFTandREDUCE,takeschargeofallparserrelatedprimitives.Infact,Knuth(1965)hasdemonstratedthatSHIFTandREDUCEprimitivesaresufﬁcienttoconstructanycontext-freegrammars.Thesecondclass,includingCALLandRETURN,isusedtooperatethestacktosimulaterecursivecalls.Aswewillshowinthenextsection,thecontroller,i.e.,aneuralparsingprogram,candecidewhatinstructiontoexecutebasedononlythestacktopframe,whosesizecanbebounded.Thisiscrucialforthewell-trainedneuralparsingprogramtogeneralizetotestsamplesthataremuchlongerthantrainingsamples.4PublishedasaconferencepaperatICLR2018SHIFTREDUCE	<Id>,	(1)Input+𝑦IdxT1StackREDUCE	<Op+>,	(1,	3)Input+𝑦StackInput𝑥+𝑦Stack0Input𝑦StackIdxT1SHIFTCALL	1Input𝑦StackIdxT1SHIFTInputEOFStack1(y,y)0(Id, T1), (+,+)IdxT1REDUCE	<Id>,	(1)InputEOFStackIdxT1IdyT2RETURNInputEOFStack0(Id, T1), (+,+),(Id, T2)IdxT1IdyT2IdIdxyInputEOFStack(Op+, T3)T3FINAL123456789IdIdxy00(x,x)0(Id,T1)0(Id,T1),(+,+)1(Id,T2)0(Id, T1), (+,+)10(Id, T1), (+,+)Op+Op+Figure2:Afullexampletoparsex+yintoaparsetree.9instructionsareexecutedtoconstructtheﬁnaltree.Weuseredlabelstoillustratethechangesafterperformingeachoperation.Foreaseofillustration,ifatreehasonlyonenode,whichisaterminal,thenwesimplyusethisterminaltorepresentboththerootnodeandthetreeinthestack;otherwise,wedrawthetreenexttothestack,andrefertoitwithauniquelabelinthestack.InsturctionArgumentDescriptionSHIFT(None)Pullonetokenfromtheinputstreamtoappendtotheendofthestacktop.REDUCEn,c1,...,cmReducethestack’stopframetoformanewnoderootedatn.cidenotesthatthei-thchildoftherootnistheci-thelementoriginallyinthestacktopframe.CALLﬁdPushoneframewith(ﬁd,[])atthestacktop.RETURN(None)Popthestacktopandappendthepoppeddatatothenewstacktop.FINAL(None)Terminatetheexecution,andreturnthestacktopastheoutput.Table1:LLmachineinstructionsemanticsThethirdclass,includingonlyFINAL,isaninstructiontoterminatethemachine’sexecutionandproducetheﬁnalresult.Thisinstructionshouldbeincludedinanynon-differentiablemachinedesign.Throughthisdesign,wewanttohighlightthreekeypropertiesinthedesignofanon-differentiablemachine:instructionsforthecorefunctionalityrelatedtotheprogramsofinterest;instructionstoenablerecursion;andinstructionstoproducetheresultsandterminatethemachine.WepresentarunningexampleusingLLmachineinFigure2,andmoredetailsandexamplestoexplainhowanLLmachineworkscanbefoundinAppendixB.3.2ANEURALPARSINGPROGRAMAparsingprogramoperatesanLLmachineviaasequenceofLLmachineinstructionstoparseaninputtoaparsetree.Speciﬁcally,aparsingprogramoperatinganLLmachinedecidesthenextinstructiontoexecuteaftereachtimestep.AkeypropertyofthecombinationoftheLLmachineandtheparsingprogramisthatitsdecisioncanbemadebasedonthreecomponentsonly:(1)thefunctionIDofthetopframe;(2)allrootnodesofthetrees(butnottheentiretrees)inthelistofthetopframe;and(3)thenextinputtoken.WecansafelyassumethatthelistinanystackframecanhaveatmostKelements(seeAppendixB).Here,Kisahyper-parameterofthemodel.Therefore,theparseronlyneedstolearna(small)ﬁnitenumberofscenariosinordertogeneralizetoallvalidinputs.Tolearntheparsingprogram,werepresentitasaneuralnetwork,whichpredictsthenextinstructiontobeexecutedbytheLLmachine.Speciﬁcally,weconsidertwoinferenceproblemsthatcomputetheprobabilitiesofthetypeandargumentsofthenextinstructionrespectively:p(inst|ﬁd,l,tok)p(arginst|inst,ﬁd,l,tok)5PublishedasaconferencepaperatICLR2018Stack top(Id, 1)(+,+)(Id, 2)A𝑒1𝑒2𝑒3A𝑓𝑖𝑑A𝑡𝑜𝑘fidnext input token𝑒0𝑒𝑥LSTM1LSTM2𝐹𝐶𝐹𝐶𝑆𝑜𝑓𝑡𝑚𝑎𝑥𝑆𝑜𝑓𝑡𝑚𝑎𝑥Instruction TypeCall ArgumentsLSTM3𝐹𝐶𝑆𝑜𝑓𝑡𝑚𝑎𝑥REDUCE Arguments 𝑛Op+Softmax…1, 22, 11, 33, 1…3,2,1………0.99………𝑎Op+REDUCE Arguments 𝑐1,…,𝑐𝑚Figure3:TheNeuralParsingProgramModel.where(ﬁd,l)denotesthecurrentstacktop,tokistheﬁrsttokenofcurrentinput,instisthetypeofthenextinstruction,arginstaretheargumentsofthenextinstructioninst.NotethatthesecondprobabilityisneededonlyifthepredictedinstructiontypeiseitherCALLorREDUCE.Atahigh-level,ateachstep,theneuralnetworkﬁrstconvertseachrootnodeinthelistofthetopstackframeintoanembeddingvector,andthenrunsthreeseparateLSTMstopredictthetype(i.e.,Formula(1))andarguments(i.e.,Formula(2)and(3))ofthenextinstruction.ThisnetworkisillustratedinFigure3.Weexplaineachcomponentinthefollowing.Fornotation,weuseLtodenotethelengthofl,andDthedimensionalityforbothinputembeddingsandLSTMhiddenstates.softmax(...)idenotesthei-thdimensionofthesoftmaxoutput.MoresubtledetailscanbefoundinAppendixC.Embeddings.Foreachelement(ni,Ti)(1≤i≤L)inthestacktop’slistl,weusealookuptableAoverallterminalsandnon-terminalstoconvertniintotheembeddingspace.Speciﬁcally,wecomputeaD-dimensionalvectorei=A(ni)for1≤i≤L.Thus,wecomputee1,...,eLfroml.Instructionprobability.WeuseanLSTMtocomputePinst(inst|ﬁd,l,tok)asfollows:Pinst(inst|ﬁd,l,tok)=softmax(W1·LSTM1(A(ﬁd),e1,...,eL)+W2·A(tok))inst(1)Speciﬁcally,eachfunctionIDﬁdistreatedasaspecialtoken,whichisconvertedintotheembeddingusingAaswell.WeuseLSTM1(A(ﬁd),e1,...,eL)toindicatetheﬁnalhiddenstateofLSTM1whentheinputsequencetotheLSTMisA(ﬁd),e1,...,eL.Further,A(tok)encodesthecurrenttokenusingthesamelookuptableAasabove.W1andW2areM×Dtrainablematrices,whereM=5sincethereare5differenttypesofinstructions.TheoperationW1·LSTM1(A(ﬁd),e1,...,eL)+W2·A(tok)isequivalenttoconcatenatingtheLSTMoutputwithA(tok)andpassingitthroughafully-connectedlayer(i.e.,FCinFigure3).PredictingCALLarguments.Topredictargumentﬁd0oftheCALLinstruction,wecomputePﬁd(ﬁd0|ﬁd,l,tok)=softmax(W01·LSTM2(A(ﬁd),e1,...,eL)+W02·A(tok))ﬁd0(2)Thispartissimilartotheonefornext-instructionpredictionasshowninFormula(1),thoughadifferentsetofparameters(i.e.,W01,W02)isused.ThelookuptableAistheonlyoverlap.PredictingREDUCEarguments.ForaREDUCEinstruction,weneedtopredictbothnand(c1,...,cm),whichdeﬁnehowtoconstructthenewsub-tree.Toachievethis,themodelpredictsnﬁrst,andthenpredicts(c1,...,cm)basedonn.Speciﬁcally,wehavePn(n|l)=softmax(W00·LSTM3(e1,...,eL))n(3)Pc(c1,...,cm|n)=softmax(an)c1,...,cm(4)6PublishedasaconferencepaperatICLR2018whereLSTM3isthethirdLSTM,W00isanN×Dtrainablematrix.HereNisthenumberofdifferenttypesofnon-terminals.NotethatdifferentfrompredictingthenextinstructiontypeandtheCALLarguments,predictingndoesnotlookatﬁdandtok,onlye1,...,eL.Thechoiceofc1,...,cmisentirelydecidedbyn.Tothisend,weconvertthispredictionproblemasaone-hotpredictionproblem.Inparticular,weencodeeachpossiblecombinationofc1,...,cmintoauniqueID.Infact,givenm≤K,thereareatmostf(K)=bK!exp(1)−1cpossibledifferentcombinationsofc1,...,cm,whereK!isthefactorialofK,andexp(1)isthebaseoftheNaturalLogarithm(seeAppendixC).Therefore,wemodelthepredictionproblemofc1,...,cmasaf(K)-wayclassiﬁcationproblem.InEquation4,anisaf(K)-dimensionaltrainablevectorforeachn.AssumetheIDfor(c1,...,cm)isξ,thensoftmax(...)c1,...,cmindicatestheξ-thdimensionofthesoftmaxoutput.NoticethatsettingKto4isenoughtohandletwonon-triviallanguagesusedinourevaluation.Inbothcases,f(K)≤65,whichistractableasthenumberofclassesinaclassiﬁcationproblem.WeconsidertohandlealargerKasfuturework.4LEARNINGANEURALPARSINGPROGRAMTraininganeuralparsingprogramischallengingduetothenon-differentiabilityoftheLLmachine.ThemainproblemisthattheexecutiontraceoftheLLmachineisunknown,andthusreinforcementlearningisnecessaryforrecoveringtheexecutiontrace.However,trainingwithreinforcementlearningisveryunstable,andisusuallystuckatalocalminimumthatcanﬁttoonlyafewexamples.Insuchacase,moreimportantly,therecoveredexecutiontracesmaybe“wrong".Tothebestofourknowledge,thisisalong-standingopenproblem,andthereisnoeffectivemechanismtoﬁndtheglobaloptimumforamodelthatcanﬁttoallexamplesatonce.Inthiswork,wetacklethischallengebyproposingatwo-phasetrainingstrategy.Infact,themainchallengeoftrainingwithareinforcementlearningalgorithmliesinthedifﬁcultyofjointlyrecoveringtheexecutiontracesandlearningamodelwitheffectiveparameters.Themainideaofourtrainingstrategyistodecoupletheproblemintotwophases,wheretheﬁrstphasetriestorecovertheexecutiontraces,whilethesecondphasetriestotrainasetofparameters.Inthefollowing,weﬁrstexplainthehigh-levelideaofthistwo-phasetrainingapproach(Section4.1),andthenexplainhowreinforcementlearningisused(Section4.2).WewillillustratewhyandhowouralgorithmworkswitharunningexampleinSection5.4.1TWO-PHASETRAININGSTRATEGYWhenthetrainingsetcontainsonlyinput-outputpairswithoutanyinformationontheexecutiontraces,learningamodelthatcanparseallvalidinputs100%accuratelyischallenging.Themainissueisthatalearnedmodelmaycorrectlyparsesomeinputs,butfailonothers.Weobservethatforeachinput-outputpair,theremayexistmultiplevalidexecutiontraces(seeAppendixDforanexample),whereamodeltrainedtomimiconecertaintraceforoneinput-outputpairmaynotbeabletolearntomimiconecertainexecutiontraceforanotherpairatthesametime.Thus,ourgoalistoﬁndconsistentexecutiontracesforallinput-outputpairsinthetrainingset.Toachievethisgoal,welearntheneuralparsingprogramintwophases.First,foreachinput-outputpair,weﬁndasetofvalidcandidateinstructiontypetraceswithapreferencetowardshorterones.Werefertothissetoftracesasthecandidatetracesetforagiveninput-outputpair.Second,wetrytosearchforasatisﬁablespeciﬁcation.Aspeciﬁcationisasetofinput-output-tracetriplesthatassignaninstructiontypetracefromthecorrespondingcandidatetracesetforeachinput-outputpairinthetrainingset.Wesaythataspeciﬁcationissatisﬁable,ifthereexistsaneuralparsingprogramthatcanparseallinputsintotheiroutputsusingthecorrespondinginstructiontypetracesinthespeciﬁcation.AsketchofthealgorithmispresentedinAlgorithm1.Wenowpresentthedetailsinthefollowing.PhaseI:Searchingforcandidatetraceset.Duetothelargesearchspace,exhaustivesearchisnotpracticalevenforaveryshortinput.Instead,weadopttheideaoftraininganeuralparsingprogramtoexplorethesearchspacetoﬁndafeasibletracethroughpolicygradient.7PublishedasaconferencepaperatICLR2018Algorithm1Asketchofthetwo-phasereinforcementlearningalgorithm1:functionSEARCH(Net0,Lesson,TrainingData)2://Phase1:nestedlooptocomputethecandidatetracesetforeachinput-outputpair3:for(inputi,treei)∈Lessondo4:Netout←Net05:foroutItr←1toM2do//Outerloop6:SampleaninstructiontypetracetraceusingthepolicynetworkNetout7:Netin←Netout8:forInItr←1toM1do//Innerloop9:Executetheprogramsfollowingtraceand10:usethepolicynetworkNetintopredictthearguments11:ˆT←Predictedparsetree12:ifdiﬀ(ˆT,treei)=0then13:updatethecandidatetracesetfor(inputi,treei)14:endif15:FollowingtheREINFORCEalgorithmtoupdateNetin16:endfor17:FollowingtheREINFORCEalgorithmtoupdateNetout18:endfor19:endfor20:21://Phase2:ﬁndasatisﬁablespeciﬁcation22:for(inputi,treei)∈TrainingDatado23:Assumetherearedcandidatetracesfor(inputi,treei)24:Createθiasad-dimensionalvectorandrandomlyinitializeit25:endfor26:whileTruedo//Ittypicallyterminateswithin50iterations27:for(inputi,treei)∈TrainingDatado28:Sampleacandidatetracefollowingthedistributionsoftmax(θi)29:endfor30:TrainanetworkNetusingreinforcementlearningasdiscussedinSection4.231:ifTrainingaccuracyis100%then32:returnNet33:endif34:endwhile35:endfunctionSpeciﬁcally,wedevelopatwo-nested-loopprocesstosearchforthecandidatetracesetforeachinput-outputpair.Ineachiterationoftheouterloop,werunaforwardpassofthemodeltosampleanexecutiontraceincludingasequenceofinstructionsandtheirarguments.WesampletheexecutiontraceusingthemodeldescribedinSection3.2,exceptthatwhilesamplingthenextinstructiontypeamongvalidinstructiontypes,weusethefollowingthedistributioninstead:p(inst|ﬁd,l,tok)∝softmax(...)inst+σ(5)Here,σ>0isaconstantallowingexplorationduringthesearch.Afteraforwardpass,weusethedifferencebetweenthepredictedparsetreeandthegroundtruthastherewardtoupdatethemodel’sparameterspredictingthenextinstructiontypeusingpolicygradient.Thisalgorithmisreferredtoaslearningwithoutsupervisionontraces,andwewillexplainthedetailsinSection4.2.Ifthepredictedtreeisidenticaltothegroundtruth,thenwehavesuccessfullyfoundavalidinstructiontypetrace,andweadditintothecandidatetraceset.Otherwise,wetestintheinnerloopswhetherthesampledinstructiontypetraceiswrong,oronlytheargumentsarepredictedwrongly.Todoso,intheinnerloops,weusethesampledinstructiontypetraceintheouterloopasthecandidategroundtruth,andtrainthemodelwithweaklysupervisedlearningmethodwhichwillbeexplainedinSection4.2.Ifanypredictiontreeduringtheinnerloopsmatchesthecandidate8PublishedasaconferencepaperatICLR2018groundtruth,weaddthesampledinstructiontypetracetothecandidateset.Otherwise,themodel’sparametersarerevertedbacktothoseatthebeginningoftheinnerloop,andthesampledinstructiontypetraceisdropped.Attheendoftheouterloop,thecandidatetracesetisformed,whichtypicallyincludes3to5instructiontypetraces,andthemodelusedduringtheloopisdropped.InthedescriptionofPhase1inAlgorithm1,M1andM2aretwohyper-parameters,whereM1isthenumberofiterationsfortheinnerloop,andM2isthenumberofiterationsfortheouterloop.Meanwhile,toescapefromasub-optimalmodel,were-initializethemodelwiththeonelearnedfromthepreviouslessonforeveryM3iterationsintheouterloop.ThevaluesofM1,M2andM3forourexperimentsaredescribedinAppendixE.PhaseII:Searchingforasatisﬁablespeciﬁcation.Toﬁndasatisﬁablespeciﬁcation,again,thenaiveideatoperformanexhaustivesearchrequirestoexploreanexponentialnumberofspeciﬁcationsinthevolumeoftrainingsamples,whichisimpractical.Forexample,ifeachinput-outputexamplehas3candidatetraces,thesearchspaceofPhaseIIforatrainingsetof20input-outputexampleshas320=3,486,784,401instances.Alternatively,weemployasampling-basedapproach.Foreachinput-outputpair(ik,Tk)inthetrainingset,weassumeSk={trk,1,...,trk,d}isitscandidatetracesetincludingdtraces.Wesampleatracefollowingthedistributionp(trk,j)=softmax(θk)j(6)whereθkisad-dimensionalvector.Afteronetraceissampledforeachinput-outputpair,thesetracesformaspeciﬁcation,andwetrytotrainamodelusingtheweaklysupervisedlearningalgorithmdescribedinSection4.2withthisspeciﬁcation.Ifthemodelcancorrectlyparseallinputs,thenweﬁndasatisﬁablespeciﬁcation.Otherwise,foreachinput-outputpair(ik,Tk)thatiswronglyparsed,wedecreasetheprobabilityofsamplingcurrenttraceinthefuturebyupdatingθkusing:θk←θk−τ·diﬀ(ˆTk,Tk)·∇θklogp(trk,j)(7)whereˆTkisthepredictedparsetree,andτ=1.0.Weobservethatsuchasampling-basedapproachcanefﬁcientlysampleasatisﬁablespeciﬁcationwithin30attemptsinourexperiments,whichcouldbe8ordersofmagnitudesmallerthantheexhaustivesearch.Curriculumlearning.Searchingforavalidtraceforalongerinputfromarandomlyinitializedmodelcanbeveryhard.Tosolvethisproblem,weusecurriculumlearningtotrainthemodeltolearntoparseinputsfromshorterlengthtolongerlength.Inthecurriculum,theﬁrstlessoncontainstheshortestinputs.Inthiscase,werandomlyinitializethemodel,andtrainittoparseallsamplesinLesson1.Afterwards,foreachnewlesson,weusetheparameterslearnedfromthepreviouslessontoinitializethemodel.Whenlearningeachlesson,alltrainingsamplesfrompreviouslessonsarealsoaddedintothetrainingsetforthecurrentlessontoavoidcatastrophicforgetting(Kirkpatricketal.,2017).Suchaprocesscontinuesuntilthemodelcancorrectlyparseallsamplesinthecurriculum.4.2TRAININGUSINGREINFORCEMENTLEARNINGNowweexplainhowreinforcementlearning,especiallythepolicygradientalgorithmREIN-FORCE(Williams,1992),canbeusedtoupdatethemodelduringthetwo-phasetrainingtoeffectivelyﬁndamodelforbothtraceexplorationandspeciﬁcationsatisﬁabilitychecking.InSection4.1,weexplainedthatweusetwoversionsofthealgorithms:learningwithnosupervisionontracesexplorespossibleexecutiontraceswhilethegroundtruthisnotgiven;andweaklysupervisedlearningtriestotrainthemodeltoﬁtforagivensetoftracespeciﬁcations.Theonlydifferencebetweenthetwoalgorithmsiswhetherasetofgroundtruthexecutiontracesisgiven.Inourexperiments,weﬁndthatthemainchallengetoapplytheREINFORCEalgorithmisthatthetrainingprocessisverysensitivetothedesignoftherewardfunctions.Inthefollowing,weﬁrstpresentourdesignoftherewardfunctionsfortrainingtheargumentpredictionsub-networks.Intheend,wediscussthedifferentapproachestotraintheinstructiontypepredictionsub-networkwhentheexecutiontracesaregivenornot.MoredetailscanbefoundinAppendixD.9PublishedasaconferencepaperatICLR2018LearningtopredictREDUCEargumentsnand(c1,...,cm).FortheREDUCEinstruction,ourintuitionisthatifawrongsetofargumentsisused,thegeneratedsub-treewilllookverydifferentthanthegroundtruthtree.Therefore,wedesigntherewardfunctionbasedonthedifferencebetweenthepredictedsub-treeandthegroundtruth.First,wedeﬁnethedifferencebetweentwotreesTandT0,denotedasdiﬀ(T,T0),tobetheeditdistancebetweenTandT0(Tai,1979).AssumeˆTistheﬁnalgeneratedparsetreeandTgisthegroundtruthoutputtree.Ourgoalistominimizediﬀ(ˆT,Tg),i.e.,to0.AssumetheparsetreeconstructedbytheREDUCEinstructionisˆTr.Sincetheﬁnalgeneratedparsetreeiscomposedbythesesmallertrees,acorrectparsetreeˆTrshouldalsobeasub-treeofTg.Basedonthisintuition,wedeﬁnemindiﬀ(ˆTr,Tg)=minT∈S(Tg){diﬀ(ˆTr,T)},whereS(Tg)indicatesthesetofallsub-treesofTg.IfalloftheREDUCEargumentsarepredictedcorrectly,mindiﬀ(ˆTr,Tg)shouldbe0.Wedesigntherewardfunctionfornand(c1,...,cm)asbelow:rreduce(ˆTr)=−log(α·mindiﬀ(ˆTr,Tg)+β)whereα>1,β∈(0,1)aretwohyperparameters.Inourexperiments,wechooseα=3,β=0.01.Inaddition,wehaveamoreefﬁcientapproachtolearnthepredictionfornviasupervisedlearning.ThedetailscanbefoundinAppendixD.LearningtopredictCALLargumentﬁd.Designingtherewardfunctiontolearnthepredictionofﬁdischallenging.AswecanseeinFigure5,thechoiceofeachﬁdaffectsonlythepredictionofsubsequentinstructiontypes.Ourdesignoftherewardfunctionforﬁdtakesthisintoaccount.Intuitively,awrongguessofﬁdwillresultinincorrectsubsequentpredictedinstructiontypes.Basedonthisintuition,wedesigntherewardfunctionasfollows:rf(fid(t))=t0Xj=t+1logp(ˆinst(j)|ﬁd(j),l(j),tok(j))inst(j)wheretindicatesthecurrentsteptoexecuteaCALLinstruction,t0thenextsteptoexecuteaCALLin-struction,ˆinst(j)andinst(j)thepredictedandgroundtruthinstructiontypes,and(ﬁd(j),l(j)),tok(j)theframeatthestacktopandthenextinputtokenatstepj.Basically,therewardfunctionrfaccumulatesthenegationofthecross-entropylossofthepredictedinstructionsfromthecurrentCALLinstructiontillthenextone.MoreexplanationscanbefoundinAppendixD.Learningtopredictthenextinstructiontype.Whentheexecutiontracesaregiven,trainingthenextinstructionpredictionsub-networkisasupervisedlearningproblem,andcanbesolvedusingNPI-styletrainingapproaches(Reed&DeFreitas,2016;Caietal.,2017).Whentheexecutiontracesarenotgiven,weuseREINFORCEtoupdatethesub-networkfornextinstructiontypepredictionaswell.Inparticular,assumethepredictiontreegeneratedduringtheforwardpassisˆTandthegroundtruthoftheoutputisTg.Thentherewardfunctionisdesigntobe−log(α·diﬀ(ˆT,Tg)+β).(8)5ARUNNINGEXAMPLEInthissection,wedetailthedesignofourreinforcementlearning-basedalgorithmusingarunningexample.Forillustrationpurposes,weuseatoylanguage,whosegrammarisstillchallengingtolearn.Thegrammarisverysimple:allexpressionscomposedbyadditionandmultiplication.Weallowxandyasidentiﬁersand0and1forliterals.WerefertothislanguageasAddition-Multiplication(AM),anditisalsoasmallsubsetoftheWHILElanguage,whichwewillevaluateinthenextsection.ThecurriculumofAMlanguageispresentedbelow.10PublishedasaconferencepaperatICLR2018x+yx∗yx+0x∗00+10∗1y+x+0y+0+x0+x+yy∗x∗0y∗0∗x0∗x∗yy∗x+0y+x∗00∗1+x0+1∗xy+1+x+0y+1+x∗0y+1∗x+0y+1∗x∗0y∗1+x+0y∗1+x∗0y∗1∗x+0y∗1∗x∗0Thedifﬁcultyoftheproblem:searchspace.Tounderstandthedifﬁcultyoftheproblem,wewouldliketoemphasizethatlearningacorrectparserisequivalenttoﬁndingthecorrectexecutiontraceforeachinput-outputexample.Ontheonehand,ifthecorrectparserislearned,thenwecanrecoverthecorrecttraceforeachinput-outputexamplebysimplyexecutingtheparserovertheinput;ontheotherhand,ifthecorrecttraceofeachinput-outputexampleisrecovered,thenwecaneasilyusesupervisedlearningtotraintheparsingprogram.Therefore,thedifﬁcultyofthelearningproblemcanbemeasuredbythevolumeofthesearchspace.Wenowestimatethesizeoftheentiresearchspace.ForAMgrammar,weparameterizetheLLmachinewithK=3andF=3,andthereare4differentnon-terminals.Wepresentthenumberoftheshortestvalidexecutiontracesversustheinputlengthbelow.InputlengthLengthofcorrectexec.tracesNumberofshortestvalidexec.traces391,5725152,771,7127217,458,826,752Here,wecallanexecutiontracetobevalidifthetraceendswithavalidFINALinstruction,buttheoutputtreedoesnotnecessarilymatchthegroundtruth.Meanwhile,weonlycalculatethenumberoftracesthatareofthesamelengthsasthecorrectexecutiontraces,wherecorrecttracesmeantheshortesttracesthatcanleadtothegroundtruthoutputtrees.Wepresentthelengthsoftheshortestcorrecttracesabove,anddenotetracesofthesamelengthsasshortesttracesforbrevity.Givenourtrainingcurriculum,anaivewaytoﬁndacorrectsetofexecutiontracesforallinput-outputexamplesistoperformanexhaustivesearchoverthespaceof15726×277171210×74588267528=3.87×10162.Suchahugesearchspacemakestheexhaustivesearchapproachimpractical.Noticethatthenumberofvalidexecutiontracesincreasesexponentiallyastheinputlengthincreases.Meanwhile,thissearchspaceisjustforasimplegrammar,i.e.,AM.Formorecomplexlanguagesthatwewilluseinourevaluation,i.e.,WHILEandLAMBDA,thevalueofFandthenumberofdifferentnon-terminalsarelarger.Thus,thetotalnumberofvalidexecutiontracesforWHILEorLAMBDAismuchhigherthanforAM.Also,theaverageinputlengthsforWHILEandLAMBDAare9.3and5.6respectively,whichfurtherincreasesthesearchspacevolumesigniﬁcantly.Motivatingthetwo-phasealgorithm.Giventhehugesearchspace,wedesignourtechniquestoreducethesearchspace.Weobserveseveralaspectswhythesearchspaceishuge.Theﬁrstissueisthatthelargesearchspaceismainlyduetotheruleofproductwhenconsideringthecombinationsoftracesforallinput-outputexamples.Whenconsideringonlyoneinput-outputexample,e.g.,aninputoflength7,thesizeofthesearchspaceisaround7.5billion.Thoughitisstilllarge,suchasizeismoretractablethan3.87×10162.Thisinspirestheideaoftwo-phaselearning:theﬁrstphasesearchesforcorrecttracesforeachinput-outputexample,andthesecondphasesearchesacombinationoftracesfordifferentsamples.Indoingso,theﬁrstphaseonlyneedstoperformasearchoveraspaceof6×1572+10×2771712+8×7458826752=5.97×1010instances,whichisstilllarge,butmuchmoreamenablethantheoriginalsearchspace.Havingdonethisseparation,wecanfocusontherestissues:(1)5.97×1010isstillalargesearchspace,andthusperformingthesearchintheﬁrstphaseisnotefﬁcientenoughyet;and(2)howtomakethesecondphaseefﬁcientandeffectiveisunclear.Inthefollowing,wewillexplainhowourdesignresolvestheseissues.Reducingthesearchspace:usinginstructiontypetracesinsteadofexecutiontraces.Weobservethatthetotalnumberoftheshortestvalidexecutiontracesforaninput-outputexampleinthecurriculumcanbeaslargeas7.5×109,whichisstilllargeforanexhaustiveenumeration.Noticethatmostofthemareequivalenttoeachotheruptopermutationoftheinstructionarguments.Infact,11PublishedasaconferencepaperatICLR2018thetotalnumberoftheshortestvalidinstructiontypetracesforeachexampleismuchsmaller,asweshowbelow.InputlengthNumberofshortestvalidexec.tracesNumberofshortestvalidinst.typetraces31,572952,771,71238277,458,826,75223,816Therefore,enumeratingvalidinstructiontypetracescanbemoreefﬁcientthanenumeratingvalidexecutiontraces.Thisobservationinspiresthenested-loopalgorithmintheﬁrstphase.Infact,theouterloopusesreinforcementlearningtoenumeratedifferentinstructiontypetraces,whiletheinnerloopveriﬁeswhetheraninstructiontypetracecanbeinstantiatedasanexecutiontracebysearchingforargumentsusingreinforcementlearning.Wewilldefermorediscussionabouttheinnerlooplater.Meanwhile,usinginstructiontypetracesalsohelpstoreducethesearchspaceofthesecondphase.Forexample,amongallvalidinstructiontypetraces,ourtrainingalgorithmtypicallyﬁnds3to5tracesthatcanleadtothecorrectoutputforeachinput-outputexample.Thus,theentiresearchspaceisoftheorderof3nto5n,wherenisthetotalnumberofinput-outputexamplesbeingsearchedtogether.Ontheotherhand,ifallargumentsarecountedaswell,thenforeachinput-outputexamples,theseinstructiontypetracescorrespondtotensofexecutiontracesleadingtothecorrectoutput.Inthiscase,thesearchspaceisordersofmagnitudelargerthanconsideringonlyinstructiontypetraces.Therefore,usinginstructiontypetracesisalsoakeytoreducethesizeofthesearchspaceinthesecondphase.Reinforcementlearning-basedsamplingversusenumeration.Intheﬁrstphase,wechoosetousereinforcementlearningtoexploredifferentpossibilitiesofinstructiontypetracesinsteadofexhaustiveenumeration.Thisdesignisbasedonthefollowingobservation.Althoughweobservethatthetotalnumberofvalidinstructiontypetracesissmallforshortinputs,thisnumbercangrowexponentiallylargewhentheinputlengthincreases.Forexample,fortheWHILElanguage,amajorityofthetraininginputshavealengthlargerthan9.Insuchacase,exhaustivelysearchingoverallvalidinstructiontypetracesisnotefﬁcient,andusingasamplingbasedapproachistypicallymoreeffective.Thisobservationisconsistentwithpreviouswork(Bergstra&Bengio,2012;Gulwanietal.,2017).Notethatourdesignofthesearchprocessinthesecondphaseisalsoinspiredbythesameobservation.Thereisacaveatforbothstrategies:itmaynotbeeasytoknowthelengthoftheshortestcorrectinstructiontypetracesinadvance.Intheexhaustivesearchapproach,onecanenumerateeachlengthoftracesintheascendingorderandcheckifthereexistsacorrectinstructiontypetrace.Usingareinforcementlearningapproach,thisprocesscouldbereversed:sinceeachouterloopmaysampleanarbitrarilylonginstructiontypetrace,itmaysampleseverallongerinstructiontypetracesbeforereachingtheoneswiththeminimallength.Thismaycauseourreinforcementlearningalgorithmtorunlongerforashortinput;butforalonginput,thesamplingapproachtypicallycanﬁndthesetofcandidateinstructiontypetracesmuchsoonerthanusingtheexhaustiveenumerationapproach.Inourevaluation,weobservethattypicallysettingM2=10,000issufﬁcientlylargetoﬁndagoodsetofcandidatetracesregardlessoftheinputlength.Thereisasidebeneﬁtofusingareinforcementlearning-basedsamplingintheouterloop:thesameRLframeworkcanbeusedintheinnerlooptosearchforavalidexecutiontraceinstantiationfromtheinstructiontypetrace.Thus,theRLalgorithmcanalsobeusedasanefﬁcientveriﬁcationtooltocheckwhetheravalidinstructiontypetraceiscorrectornot.Theeffectivenessoftrainingcurriculum.Thetrainingcurriculumhelpswiththetraininginthreeaspects.First,theRLalgorithmhasacaveat:foralonginput,RLalgorithmcannotﬁndevenonecorrectinstructiontypetracefromacoldstart.Thus,curriculumlearningcanhelptomitigatethisissue.Inparticular,whensearchingforcorrectinstructiontypetracesforanexampleinonelesson,themodelisinitializedwithparametersthatcanﬁttoallexamplesinpreviouslessons.Indoingso,theRLalgorithmcaneffectivelyskipmanyobviouslybadtraces,andthusﬁndthecorrectonesmoreefﬁciently.Second,thetrainingcurriculumcanalsohelpRLtoskipthoseinstructiontypetracesthatarecorrectfortheexaminedinput-outputexamples,butareinconsistentwithotherexamplesinpreviouslessons.ForourAMlanguage,weprovidethenumberofcorrectinstructiontracesaswellasthenumberof12PublishedasaconferencepaperatICLR2018Example#ofcorrectExample#ofcorrectExample#ofcorrecttracestracestracesx+y9(3)x*y9(3)x+09(3)x*09(3)0+19(3)0*19(3)y+x+099(11)y+0+x99(11)0+x+y99(11)y*x*099(11)y*0*x99(11)0*x*y99(11)y*x+099(11)y+x*081(9)0*1+x99(11)0+1*x81(9)y+1+x+01107(41)y+1+x*0891(33)y+1*x+01053(39)y+1*x*0891(33)y*1+x+01107(41)y*1+x*0891(33)y*1*x+01107(41)y*1*x*01107(41)Table2:ThenumbersofcorrectinstructiontracesandinstructiontypetracesforeachexampleintheAMtrainingset.Thetwonumbersareprovidedinthecolumnof“#ofcorrecttraces"intheformofn(m),wheren(outsidethebrackets)indicatesthenumberofinstructiontraces,andm(insidethebrackets)indicatesthenumberofinstructiontypetraces.correctinstructiontypetracesforeachexampleinTable2.Fromthetable,wecanobservethatthenumberofcorrecttracesincreasessigniﬁcantlywithrespecttotheincreaseoftheinputlength.Usingcurriculumtraining,wecanreducethenumberofcandidateinstructiontypetracestobe3to5foreachexampleregardlessoftheinputlength,whichthusfurtherreducesthesearchspaceforPhaseII.Third,itcanfurtherreducethesearchspaceofthesecondphase.Assumethealgorithmﬁnds3candidateinstructiontypetracesforeachinput-outputpair.Sincethereare24examplesinthetrainingset,thesearchspaceofthesecondphasecanbeaslargeas324=2.82×1011.Whenfollowingthetrainingcurriculum,eachlessonmayhaveonly6examples.Thus,thesearchspaceforeachlessoninthesecondphasecanbeasfewas36=729,i.e.,8ordersofmagnitudesmaller.Whentrainingonthenextlesson,sincetheinstructiontypetracesforexamplesinpreviouslessonshavebeendetermined,thesearchcanfocusonthecurrentlesson.Therefore,usingatrainingcurriculumcanfurtherreducethesearchspacewhileguaranteeingthatthemodelisnotoverﬁttingtoasubsetofexamples.6EVALUATIONToshowthatourapproachisgeneralandabletolearntoparsedifferenttypesofcontext-freelanguagesusingthesamearchitectureandapproach,weevaluateourapproachontwotaskstolearnaparserforanimperativelanguageWHILEandanML-style(Milner,1997)functionallanguageLAMBDArespectively.WHILEandLAMBDAcontain73and66productionrules,andtheirparsingprogramscanbeimplementedin89and46linesofPythoncoderespectively.Noticethattheseprogramsaremoresophisticatedthanpreviousstudiedexamples.Forexample,Quicksortstudiedin(Caietal.,2017)canbeimplementedin3linesofPythoncode,andFlashFilltasksstudiedin(Devlinetal.,2018;Parisottoetal.,2017)canbeimplementedin10linesofcodeintheirDSL.GrammarspeciﬁcationsofthetwolanguagesarepresentedinAppendixGandHrespectively.Foreachtask,wepreparethreetrainingsets:(1)Curriculum:awell-designedtrainingcurriculumincluding100to150examplesthatenumeratesalllanguageconstructors;(2)Std-10:atrainingsetincludesallexamplesinthecurriculum,andalso10,000additionalrandomlygeneratedinputswithlength10onaverage;and(3)Std-50:atrainingsetincludesallexamplesinthecurriculum,andalso1,000,000additionalrandomlygeneratedinputswithlength50onaverage.Inalldatasets,allgroundtruthparsetreesareprovided.Notethatonceourmodellearnstoparseallinputsinthecurriculum,itcanparseallinputsintrainingsets(2)and(3)forfree.Weincludetwostandardtrainingsets,i.e.,Std-10andStd-50,toallowafaircomparisonagainstbaselineapproaches,whichtypicallyrequirealargeamountoftrainingdata.Wecompareourapproachwithtwosetsofbaselines.Theﬁrstclassofapproacheslearnend-to-endneuralnetworkmodelsastheprogram.Thisclassincludesasequence-to-sequenceapproach(seq2seq)(Vinyalsetal.,2015b),asequence-to-tree(seq2tree)approach(Dong&Lapata,2016),andLSTMwithunboundedmemory(Grefenstetteetal.,2015).Inparticular,weevaluateallthreevariantsproposedin(Grefenstetteetal.,2015).Thesecondclassincludesthestate-of-the-artapproachforneuralprogramsynthesis,i.e.,RobustFill(Devlinetal.,2018),whichlearnsadiscreteprograminaDSL.13PublishedasaconferencepaperatICLR2018While-LangTrainTestOursSeq2seqSeq2treeStackQueueDeQueRobust-LSTMLSTMLSTMFill(Projected)CurriculumTraining100%81.29%100%100%100%100%13.67%Test-10100%0%0.8%0%0%0%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Std-10Training100%94.67%100%81.01%72.98%82.59%0.19%Test-10100%20.9%88.7%2.2%0.7%2.8%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Std-50Training100%87.03%100%0%0%0%0.0019%Test-50100%86.6%99.6%0%0%0%0%Test-500100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Lambda-LangTrainTestOursSeq2seqSeq2treeStackQueueDeQueRobust-LSTMLSTMLSTMFill(Projected)CurriculumTraining100%96.47%100%100%100%100%29.21%Test-10100%0%0%0%0%0%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Std-10Training100%93.53%100%0%95.93%2.23%0.26%Test-10100%86.7%99.6%0%6.5%0.1%0%Test-100100%0%0%0%0%0%0%Test-1000100%0%0%0%0%0%0%Std-50Training100%66.65%89.65%0%0%0%0.0026%Test-50100%66.6%88.1%0%0%0%0%Test-500100%0%0%0%0%0%0%Test-5000100%0%0%0%0%0%0%Table3:ExperimentalresultsonWhile-LangandLambda-Langdataset.Weevaluateourapproach(ours),seq2seq(Vinyalsetal.,2015b),seq2tree(Dong&Lapata,2016),withStackLSTM,QueueLSTMandDeQueLSTMfrom(Grefenstetteetal.,2015).“Std-10"indicatesthestandardtrainingsetwith10,000samplesoflength10,“Std-50”indicatesthestandardtrainingsetwith1,000,000samplesoflength50,and“Curriculum"indicatesthespeciallydesignedlearningcurriculum.“Test-LEN"indicatesthetestsetincludinginputsoflengthLEN.Fortesting,wecreatesixlevelsoftestsets,i.e.,Test-10,Test-50,Test-100,Test-500,Test-1000andTest-5000,whereeachinputhas10,50,100,500,1000and5000tokensonaveragerespectively.Eachtestsetcontains1000randomlygeneratedexpressions.Weguaranteethattestdatadoesnotoverlapwithtrainingsamples.Table3showsexperimentalresultsonWHILEandLAMBDAlanguages.Wediscusstheresultsbelow.Observationsonourapproach.Weobservethatonceourneuralparsingprogramistrainedtoachieve100%accuracyonthetrainingdata,itcanalwaysachieve100%testaccuracyonarbitrarytestsamplesregardlessoftheirlengths.Weemphasizethatalltheresultsofourapproachareachievedbyusingthecurriculumlearningapproach.Withoutcurriculumtraining,ourapproachcannottrainanyeffectivemodels,andthetestaccuracyis0%whenthetestinputislongerthantraininginputs.Also,weobservethatourtwo-phasetrainingalgorithmcanalwaysmaketheneuralnetworkbetrainedtoachieve100%onthe14PublishedasaconferencepaperatICLR2018curriculumtrainingset.Further,inourevaluation,weobservethatthetrainingcurriculumisveryimportanttoregularizethereinforcementlearningprocess.Therefore,ourevaluationdemonstratesthatthecombinationofourthreeideasenablesustolearnaprogramtoachieve100%accuracyontestsamplesthatcanbeeven500×longerthanthetrainingones,whilebaselineapproachesarehardtoevenachieveatestaccuracythatisgreaterthan0%.Observationsonapproachestolearnend-to-endneuralnetworkmodels.Weﬁrstobservethatwhenthelengthoftestsamplesislargerthantrainingones,thetestaccuracydropsto0%regardlessoftheend-to-enddifferentialapproachbeingevaluated.Thisillustratesthatnoneoftheseapproachescangeneralizetolongerinputs.Aswehaveexplained,itisveryhardtoenforcethelearnedmodeltoalwaysrepresentaparsertohandlearbitrarilylonginputs.Thuseventhewell-trainedmodelssimplyoverﬁttothetrainingsamples,andcannotgeneralizetolongerinputs.Also,whentrainingonthecurriculumdataset,noapproachescangeneralizetoanytestdata.Thisissimplybecausethetrainingsetcontainstoofewinstances,sothattheoverﬁttingphenomenonexplainedabovebecomesmoreprominent.Whentestsamplesareofthesamelengthastrainingonesandtrainingwiththestandardtrainingsets,weobservethatseq2treeperformsbetterthanseq2seq.Weattributethisphenomenontothereasonthattheseq2treemodelessentiallyemploystherecursionideainitsdecoderdesign.Infact,inthedecodingphase,differentfromseq2seq,whichgeneratestheentiresequenceatonce,seq2treetraversesdownalongthepathsfromtheroottoleavesandrecursivelydecodeseachlayeroftheparsetreealongapath.OnWHILEandLAMBDAtasks,itcanevenachieveanaccuracyofalmost100%onTest-50andTest-10respectively,showingthatthisrecursivedecodingapproachiseffective.However,theencodingphaseofseq2treeisnotrecursive.Thishindersitsgeneralizationtolongerinputs.Meanwhile,forbothseq2seqandseq2treemodels,whentheyaretrainedonStd-50trainingset,thegapbetweenthetrainingaccuracyandthetestaccuracyismuchsmallerthantheonestrainedonStd-10trainingset.Forexample,onWHILEdataset,thetestaccuracyofseq2seqisaround20%onTest-10;however,whentrainedonStd-50,itcanreachanaccuracyofabove85%onTest-50.ThisobservationsuggeststhatthemodelsoverﬁttotheStd-10trainingset.WhenStd-50isusedfortraining,ontheotherhand,theoverﬁttingissueismitigated.Inaddition,wenoticethatwhilethetestaccuracyfortheWHILEtaskincreaseswhenStd-50isusedfortraining,itdropsfortheLAMBDAtask.WeattributeittothefactthatthesizeoftheparsetreeinLAMBDAlanguageismuchlargerthantheparsetreeinWHILElanguagegiventheinputwiththesamenumberoftokens.Thus,whentheinputsgetlonger,themodelishardertoﬁttotheparsetreesoflargersizes.Wealsoobservethatmodelsproposedin(Grefenstetteetal.,2015)performpoorly,andmuchworsethantheothertwoend-to-endneuralnetworkapproaches.Grefenstetteetal.(2015)proposesLSTMdecoderswithunboundedmemorytogeneralizetheideaofneuralpushdownautomaton,whichwasdesignedtohandletheparsingtasks.OurresultsshowthatwhenthesemodelsaretrainedonStd-10,theysufferseverelyfromoverﬁtting.Further,whentrainedonStd-50,thetrainingaccuracydropsto0%.Weattributethepoorperformancetothefactthattheproductionandtransductionrulesdevelopedinthetwoevaluatedlanguagesaretoocomplexforsucharchitecturestolearneffectively.Infact,Grefenstetteetal.(2015)reportsthatallthreeproposedapproachesperformpoorlyonthebi-gramﬂippingtask,whichisamuchsimplersub-taskofthetwolanguagesinconsideration.Thisfurtherillustratesthechallengesoftrainingsuchend-to-enddifferentiableneuralnetworkstosimulateevensimpledatastructuressuchasstacksorqueues.Observationsonapproachestosynthesizediscreteprograms.NotethatthetrainingofRobust-Fill,asdescribedin(Devlinetal.,2018),donotusetheinput-outputexamplesinthetrainingset,butconstructtheirowntrainingsetinstead.ThesourcecodeofRobustFillisnotavailable,andourre-implementationcannotproduceanymeaningfulprograms.Tomakeafaircomparison,wecomputeaprojectedaccuracywhichistheaccuracythatcanbeachievedbythemosteffectiveprograminthespaceoftheRobustFillDSL.NotethatgivenRobustFillcanproduceprogramsoflengthofupto10,theentireoutputprogramspaceofRobustFillisﬁnite,thoughexponentiallylarge.However,wenoticethatwecanefﬁcientlyenumeratethesmallsub-set15PublishedasaconferencepaperatICLR2018ofeffectiveprogramsusingasimpleheuristictocutineffectiveconstructorsinaprogram.WedetailthealgorithminAppendixK.TheresultsinTable3reportanupperboundoftheaccuracythatcanbeachievedbytheRobustFillapproach.WecanobservethatthebestprogramintheRobustFillDSLspacecanonlyﬁttoasmallsubsetofthetrainingdata,andcannotgeneralizetolongertestinputsduetothelengthoftheprogramsislimited.Essentially,theoutputsofaprogramintheRobustFillDSLspaceareboundedbythelengthoftheprogramitself(seeAppendixKforadiscussion),whiletheoutputsofourparsingproblemcangrowarbitrarilylong.Whenthetestsamplesbecomelonger,theRobustFillapproachwillsoonfailwith0%accuracy.Therefore,tomaketheRobustFillapproachabletohandleourparsingtask,itisnecessarytodevelopanovelDSLwithenoughexpressiveness(i.e.,supportingrecursiontoallowarbitrarilylongoutputs).However,thisisahighlynon-trivialtask,andisoutofthescopeofthiswork.7RELATEDWORKWenowpresentahigh-leveloverviewofrelatedwork.Amorein-depthdiscussionabouttherelationshipbetweenourworkandpreviousworkispresentedinAppendixA.Recentworksproposetousesequence-to-sequencemodels(Vinyalsetal.,2015b;Aharoni&Gold-berg,2017)andtheirvariants(Dong&Lapata,2016)todirectlygenerateparsetreesfrominputs.However,theyoftendonotgeneralizewell,andourexperimentsshowthattheirtestaccuracyisalmost0%oninputslongerthanthoseseenintraining.OtherworksstudylearninganeuralprogramtooperateaShift-Reducemachine(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016)oratop-downparser(Dyeretal.,2016)toperformparsingtasksfornaturallanguages.Intheseworks,theexecutiontracesareeasytorecoverfrominput-outputpairs,whileinourworkthetracesarehardtorecover.Recentworksstudylearningneuralprogramsanddifferentiablemachines(Gravesetal.,2014;Kurachetal.,2015;Joulin&Mikolov,2015;Kaiser&Sutskever,2015;Buneletal.,2016).Theirproposedapproacheseitherdonotgeneralizetolongerinputsthanthoseseenduringtraining,orareevaluatedonlyonsimpletasks.Inparticular,StackRNN(Joulin&Mikolov,2015)alsostudieslearningcontext-freelanguages,buttheirmainfocusistogeneratelanguageinstances,whileourgoalistolearntheparser.Employingasimilaridea,Grefenstetteetal.(2015)designanend-to-enddifferentiablepush-downautomatonfortransductiontasks,whicharesimilartoours.Aswewilldemonstrate,suchanapproachhasevenworsegeneralizationthanasequence-to-sequencemodel.Ontheotherhand,otherworksstudyneuralprogramsoperatingnon-differentiablemachines(Caietal.,2017;Lietal.,2017;Reed&DeFreitas,2016;Zarembaetal.,2016;Zaremba&Sutskever,2015),butintheseworks,eitherextrasupervisiononexecutiontracesisneededduringtraining(Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017),orthetrainedmodelcannotgeneralizewell(Zarembaetal.,2016;Zaremba&Sutskever,2015).Inparticular,Zarembaetal.(2016)studylearningsimplealgorithmsfrominput-outputexamples;however,theapproachfailstogeneralizeonverysimpletasks,suchas3-numberaddition.Ourworkistheﬁrstonedemonstratingthataneuralprogramachievingfullgeneralizationtolongerinputscanbetrainedfrominput-outputpairsonly.Anotherlineofresearchstudiesusingneuralnetworkstosynthesizeaprograminadomain-speciﬁclanguage(DSL).Recentworks(Devlinetal.,2018;Parisottoetal.,2017)studyusingneuralnetworkstogenerateaprograminaDSLfromafewinput-outputexamplesfortheFlashFillproblem(Gulwanietal.,2012;Gulwani,2011).However,theDSLcontainsonlysimplestringoperations,whichisnotexpressiveenoughtoimplementaparser.Meanwhile,intheseworks,theycanonlysuccessfullysynthesizeprogramswithlengthsnotlargerthan10.Theseconstraintsmaketheirapproachesunsuitableforourproblemcurrently.DeepCoder(Balogetal.,2017)presentsaneuralnetwork-basedsearchtechniquetoacceleratesearch-basedprogramsynthesis.Again,lengthsofthesynthesizedprogramsinthisworkareatmost5,whiletheparsingprogramthatwestudyinthisworkismuchmorecomplex.Thereareotherapproaches(Ellisetal.,2016)thatemploySMTsolverstosampleprograms.Again,itisonlydemonstratedtosolveasubsetoftheFlashFillproblemandseveralsimplearraymanipulationtasks.16PublishedasaconferencepaperatICLR20188CONCLUSIONANDFUTUREWORKInthiswork,wemoveasigniﬁcantstepforwardtolearncomplexprogramsfrominput-outputexamplesonly.Inparticular,weproposeanovelclassofgrammarinductionproblemstolearnaparserfromtheinput-treepairs.Wedemonstratethattheparsingproblemsaremorechallengingasmostexistingapproachesfailtogeneralize,i.e.,thetestaccuracyis0%.Tosolvethisproblem,werevealthreenovelchallengesandproposenovelprincipledapproachestotacklethem.First,wepromoteahybridapproachtolearnaneuralprogramoperatinganon-differentiablemachinetoeffectivelyrestrictthelearnedprogramswithinthespaceofinterest.Second,wedesignthemachinetobake-inthenotionofrecursiontomakethelearnedneuralprogramsgeneralizable.Third,weproposeanoveltwo-phasereinforcementlearning-basedalgorithmtoeffectivelytrainsuchaneuralprogram.Combiningthethreetechniques,wedemonstratethattheparsingproblemcanbefullysolvedontwodiverseinstancesofgrammars.Inthefuture,weareinterestedinboththedomainofparsingproblemsandevenmorecomplexprograms.Fortheparsingproblems,weareinterestedinrecoveringtheproductionrulesfrominput-outputexamples,ratherthanonlylearningtheparser,andrelaxseveraltechnicalassumptions,suchastheknowledgeoftheterminalsetandhyper-parameterK,whichisthemaximumnumberelementsinthelistofeachstackframe.Formorecomplexprograms,weareinterestedinextendingourapproachtolearnalgorithmsonmorecomplexdatastructuressuchastreesandgraphs.ACKNOWLEDGEMENTWethankRichardShin,DengyongZhou,YuandongTian,HeHe,YuZhang,andWarrenHefortheirhelpfuldiscussions.ThismaterialisinpartbaseduponworksupportedbytheNationalScienceFoundationunderGrantNo.TWC-1409915,BerkeleyDeepDrive,andDARPASTACunderGrantNo.FA8750-15-2-0104.Anyopinions,ﬁndings,andconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthor(s)anddonotnecessarilyreﬂecttheviewsoftheNationalScienceFoundation.REFERENCESRoeeAharoniandYoavGoldberg.Towardsstring-to-treeneuralmachinetranslation.InACL,2017.DanielAndor,ChrisAlberti,DavidWeiss,AliakseiSeveryn,AlessandroPresta,KuzmanGanchev,SlavPetrov,andMichaelCollins.Globallynormalizedtransition-basedneuralnetworks.arXivpreprintarXiv:1603.06042,2016.DanaAngluin.Learningregularsetsfromqueriesandcounterexamples.Informationandcomputation,75(2):87–106,1987.MatejBalog,AlexanderLGaunt,MarcBrockschmidt,SebastianNowozin,andDanielTarlow.Deepcoder:Learningtowriteprograms.InICLR,2017.JamesBergstraandYoshuaBengio.Randomsearchforhyper-parameteroptimization.JournalofMachineLearningResearch,13(Feb):281–305,2012.RudyRBunel,AlbanDesmaison,PawanKMudigonda,PushmeetKohli,andPhilipTorr.Adaptiveneuralcompilation.InAdvancesinNeuralInformationProcessingSystems,pp.1444–1452,2016.JonathonCai,RichardShin,andDawnSong.Makingneuralprogrammingarchitecturesgeneralizeviarecursion.InICLR,2017.DanqiChenandChristopherDManning.Afastandaccuratedependencyparserusingneuralnetworks.InEMNLP,2014.NoamChomsky.Threemodelsforthedescriptionoflanguage.IRETransactionsoninformationtheory,2(3):113–124,1956.ColinDelaHiguera.Grammaticalinference:learningautomataandgrammars.CambridgeUniversityPress,2010.17PublishedasaconferencepaperatICLR2018JacobDevlin,JonathanUesato,SuryaBhupatiraju,RishabhSingh,Abdel-rahmanMohamed,andPushmeetKohli.Robustﬁll:NeuralprogramlearningundernoisyI/O.InICML,2018.LiDongandMirellaLapata.Languagetologicalformwithneuralattention.InACL,2016.ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahASmith.Recurrentneuralnetworkgrammars.InNAACL,2016.KevinEllis,ArmandoSolar-Lezama,andJoshTenenbaum.Samplingforbayesianprogramlearning.InNIPS,2016.AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401,2014.EdwardGrefenstette,KarlMoritzHermann,MustafaSuleyman,andPhilBlunsom.Learningtotransducewithunboundedmemory.InAdvancesinNeuralInformationProcessingSystems,pp.1828–1836,2015.SumitGulwani.Automatingstringprocessinginspreadsheetsusinginput-outputexamples.InACMSIGPLANNotices,2011.SumitGulwani,WilliamRHarris,andRishabhSingh.Spreadsheetdatamanipulationusingexamples.CommunicationsoftheACM,55(8):97–105,2012.SumitGulwani,OleksandrPolozov,RishabhSingh,etal.Programsynthesis.FoundationsandTrendsR(cid:13)inProgrammingLanguages,4(1-2):1–119,2017.ArmandJoulinandTomasMikolov.Inferringalgorithmicpatternswithstack-augmentedrecurrentnets.InNIPS,2015.ŁukaszKaiserandIlyaSutskever.Neuralgpuslearnalgorithms.arXivpreprintarXiv:1511.08228,2015.JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal.Overcomingcatastrophicforgettinginneuralnetworks.ProceedingsoftheNationalAcademyofSciences,pp.201611835,2017.DonaldEKnuth.Onthetranslationoflanguagesfromlefttoright.Informationandcontrol,8(6):607–639,1965.KarolKurach,MarcinAndrychowicz,andIlyaSutskever.Neuralrandom-accessmachines.arXivpreprintarXiv:1511.06392,2015.ChengtaoLi,DanielTarlow,AlexanderGaunt,MarcBrockschmidt,andNateKushman.Neuralprogramlattices.InICLR,2017.RobinMilner.ThedeﬁnitionofstandardML:revised.MITpress,1997.JoséOncinaandPedroGarcía.Identifyingregularlanguagesinpolynomialtime.AdvancesinStructuralandSyntacticPatternRecognition,5(99-108):15–20,1992.EmilioParisotto,Abdel-rahmanMohamed,RishabhSingh,LihongLi,DengyongZhou,andPushmeetKohli.Neuro-symbolicprogramsynthesis.InICLR,2017.TJParrandRWQuong.Antlr:Apredicated.Software—PracticeandExperience,25(7):789–810,1995.ScottReedandNandoDeFreitas.Neuralprogrammer-interpreters.InICLR,2016.Kuo-ChungTai.Thetree-to-treecorrectionproblem.JournaloftheACM(JACM),26(3):422–433,1979.OriolVinyals,MeireFortunato,andNavdeepJaitly.Pointernetworks.InAdvancesinNeuralInformationProcessingSystems,pp.2692–2700,2015a.18PublishedasaconferencepaperatICLR2018OriolVinyals,ŁukaszKaiser,TerryKoo,SlavPetrov,IlyaSutskever,andGeoffreyHinton.Grammarasaforeignlanguage.InNIPS,2015b.RonaldJWilliams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.Machinelearning,1992.DaniYogatama,PhilBlunsom,ChrisDyer,EdwardGrefenstette,andWangLing.Learningtocomposewordsintosentenceswithreinforcementlearning.InICLR,2016.WojciechZarembaandIlyaSutskever.Reinforcementlearningneuralturingmachines-revised.arXivpreprintarXiv:1505.00521,2015.WojciechZaremba,TomasMikolov,ArmandJoulin,andRobFergus.Learningsimplealgorithmsfromexamples.InProceedingsofThe33rdInternationalConferenceonMachineLearning,pp.421–429,2016.19PublishedasaconferencepaperatICLR2018AMOREDISCUSSIONABOUTRELATEDWORKGrammarinduction.Learningthegrammarfromacorpusofexampleshaslongbeenstudiedintheliteratureasthegrammarinductionproblem,andalgorithmssuchasL-Star(Angluin,1987)andRPNI(Oncina&García,1992)havebeenproposedtohandleregularexpressions.Incontrast,inthiswork,weareinterestedinlearningcontext-freelanguages(Chomsky,1956),whichismuchmorechallengingthanlearningregularlanguages(DelaHiguera,2010).Sequence-to-sequencestyleapproaches.Recentworksproposetousesequence-to-sequencemodels(Vinyalsetal.,2015b;Aharoni&Goldberg,2017)andtheirvariants(Dong&Lapata,2016)todirectlygenerateparsetreesfrominputs.However,theyoftendonotgeneralizewell,andourexperimentsshowthattheirtestaccuracyisalmost0%oninputslongerthanthoseseenintraining.ParsingapproachesusingmachinesinNLPliteratures.Arecentlineofresearch(Andoretal.,2016;Chen&Manning,2014;Yogatamaetal.,2016)studyingdependencyparsingemploysneuralnetworkstooperateaShift-Reducemachine.However,eachnodeinthegenerateddependencytreecorrespondstoaninputtoken,whileinourproblem,thereisnotadirectcorrespondencebetweentheinternalnodesinparsetreesandtheinputtokens.Further,RNNG(Dyeretal.,2016)learnsaneuralprogramoperatingatop-downparsertogenerateparsetrees,whichincludenon-terminals.However,asexplicitlystatedinthepaper(Dyeretal.,2016),theinputtokensalignwellwiththepre-ordertraversaloftheparsetree.Inourwork,suchorderisoftennotpreservedandthecorrespondenceishardtoberecovered.Thus,theseapproachesdonotdirectlyapplytoourproblem.Neuralprograminduction.Recentworksstudylearningneuralprogramsanddifferentiablemachines(Gravesetal.,2014;Kurachetal.,2015;Joulin&Mikolov,2015;Kaiser&Sutskever,2015;Buneletal.,2016).Theirproposedapproacheseitherdonotgeneralizetolongerinputsthanthoseseenduringtraining,orareevaluatedonlyonsimpletasks.Inparticular,StackRNN(Joulin&Mikolov,2015)alsostudieslearningcontext-freelanguages,buttheirmainfocusistogeneratelanguageinstances,whileourgoalistolearntheparser.Grefenstetteetal.(2015)adoptsasimilarideaforlearningtotransduce.However,aswedemonstrate,suchanapproachperformspoorlyandevenworsethanasequence-to-sequencemodel.Ontheotherhand,otherworksstudyneuralprogramsoperatingnon-differentiablemachines(Caietal.,2017;Lietal.,2017;Reed&DeFreitas,2016;Zarembaetal.,2016;Zaremba&Sutskever,2015),butintheseworks,eitherextrasupervisiononexecutiontracesisneededduringtraining(Reed&DeFreitas,2016;Caietal.,2017;Lietal.,2017),orthetrainedmodelcannotgeneralizewell(Zarembaetal.,2016;Zaremba&Sutskever,2015).Inparticular,(Zarembaetal.,2016)studieslearningsimplealgorithmsfrominput-outputexamples;however,theapproachfailstogeneralizeonverysimpletasks,suchas3-numberaddition.Ourworkistheﬁrstonedemonstratingthataneuralprogramachievingfullgeneralizationtolongerinputscanbetrainedfrominput-outputpairsonly.Neuralprogramsynthesis.Anotherlineofresearchstudiesusingneuralnetworkstosynthesizeaprograminadomain-speciﬁclanguage(DSL).Recentworks(Devlinetal.,2018;Parisottoetal.,2017)studyusingneuralnetworkstogenerateaprograminaDSLfromafewinput-outputexamplesfortheFlashFillproblem(Gulwanietal.,2012;Gulwani,2011).However,theDSLcontainsonlysimplestringoperations,whichisnotexpressiveenoughtoimplementaparser.Meanwhile,intheseworks,theycanonlysuccessfullysynthesizeprogramswithlengthsnotlargerthan10.Theseconstraintsmaketheirapproachesunsuitableforourproblemcurrently.DeepCoder(Balogetal.,2017)presentsaneuralnetwork-basedsearchtechniquetoacceleratesearch-basedprogramsynthesis.Again,lengthsofthesynthesizedprogramsinthisworkareatmost5,whiletheparsingprogramthatwestudyinthisworkismuchmorecomplex.Thereareotherapproaches(Ellisetal.,2016)thatemploySMTsolverstosampleprograms.Again,itisonlydemonstratedtosolveasubsetoftheFlashFillproblemandseveralsimplearraymanipulationtasks.AsalltheseapproachesfollowthesameparadigmtosynthesizeaprogramintheDSLusedinRobustFill,inAppendixK,wegiveafundamentalreasonwhysuchapproachescannotgeneralizetohandletheparsingproblems.20PublishedasaconferencepaperatICLR2018SHIFTREDUCE	<Id>,	(1)Input+𝑦IdxT1StackREDUCE	<Op+>,	(1,	3)Input+𝑦StackInput𝑥+𝑦Stack0Input𝑦StackIdxT1SHIFTCALL	1Input𝑦StackIdxT1SHIFTInputEOFStack1(y,y)0(Id, T1), (+,+)IdxT1REDUCE	<Id>,	(1)InputEOFStackIdxT1IdyT2RETURNInputEOFStack0(Id, T1), (+,+),(Id, T2)IdxT1IdyT2IdIdxyInputEOFStack(Op+, T3)T3FINAL123456789IdIdxy00(x,x)0(Id,T1)0(Id,T1),(+,+)1(Id,T2)0(Id, T1), (+,+)10(Id, T1), (+,+)Op+Op+Figure4:AcopyofFigure2toprovideafullexampletoparsex+yintoaparsetree.9instructionsareexecutedtoconstructtheﬁnaltree.Weuseredlabelstoillustratethechangesafterperformingeachoperation.Foreaseofillustration,ifatreehasonlyonenode,whichisaterminal,thenwesimplyusethisterminaltorepresentboththerootnodeandthetreeinthestack;otherwise,wedrawthetreenexttothestack,andrefertoitwithauniquelabelinthestack.BLLMACHINESWestartwiththepresentationoftheLLmachines’design.ItisinspiredbytheLL(1)parsingalgorithm(Parr&Quong,1995),althoughwedonotrequirethereaderstobefamiliarwiththeLL(1)algorithm.Throughoutthedescription,weuseFigure2(weprovideacopyinFigure4whichisclosertothedescriptionbelow)asarunningexampletoillustratetheconcepts.States.AnLLmachinemaintainsasequenceof(partial)inputtokensandastackofframesasitsinternalstate.EachstackframeisanID-listpair,wheretheIDisafunctionID,whichwillbeexplainedlater,andinthelistare(n,T)pairs,whereTisaparsetree,andnistherootnodeofT.Forexample,inFigure4,afterstep6,thestackframeatthetopcontainsanID1andalistofoneelement(Id,T2).Instructions.AnLLmachinehasﬁvetypesofinstructions:SHIFT,CALL,RETURN,REDUCE,andFINAL.AparseroperatesanLLmachineusingtheseﬁvetypesofinstructionstoconstructtheparsetreerecursively.Inthefollowing,weexplaintheseinstructionsandhowtheyareusedforparsinganinput.Tobeginwith,thestackcontainsoneframe(0,[]),where[]denotesanemptylist.ASHIFTinstruction(e.g.,steps1,3,and5inFigure4)removesthenexttokentfromtheinputsequence,constructsaone-nodetreeTconsistingoft,andappends(t,T)totheendofthestacktop’slist.TheSHIFTinstructionhasnoargument.Whentheparsertriestoparseasub-expressionasasub-tree,itusesaCALLinstructiontocreateanewstackframe.Forexample,beforestep4,thesub-expression“y"needstobeparsedintoT2withrootId.Inthiscase,aCALLinstructionisexecutedtopushanewframewithanemptylistontothestack.CALLhasanargumentﬁd,whichisthefunctionIDofthenewframeatthestacktop.ThisfunctionIDcarriesinformationfromthepreviousframetothenewone,e.g.,tohelpdecidetheboundaryofthesub-expression.InFigure5,forexample,whenparsing“x+y*z"and“x*y*z",oncetheﬁrsttwotokens(i.e.,“x+"and“x*")areconsumed,theparserexecutesaCALLinstructiontocreateanewframetoparsethesub-expressions“y*z"and“y"respectively.Sincetheremaininginputsequences(i.e.,“y*z")arethesameinbothcases,thefunctionIDsprovidetheonlycluetodetecttheboundariesofthesub-expressions.TheparserissuesaREDUCEinstructiontoconstructalargertree,onceallchildrenofitsrootareconstructedandlaidoutinthetopframe’slist.REDUCEn,(c1,...,cm)hastwoargumentsforspecifyinghowtoconstructthenewtree.Therootofthenewlyconstructedtreeisnandhasm21PublishedasaconferencepaperatICLR2018children.Thej-thchildofnisthecj-thtreeinthestacktop’slist.Forexample,inFigure4,afterstep8,T1andT2arecombinedtoconstructT3.Thelistinthetopframecontainsthreeelements,i.e.,(Id,T1),(+,+),and(Id,T2).Inthiscase,theREDUCEargumentnisOp+,indicatingthatT3’srootisOp+;forthesecondargument(c1,...,cm),m=2,c1=1andc2=3,indicatingthattheﬁrstandthirdelementsinthelist(i.e.,T1andT2)constitutetheﬁrstandsecondchildrenofT3.Notethatthechildrenoftherootareordered.Afterasub-expressionisconvertedintoatreeusingtheREDUCEinstruction,aRETURNinstructioncanbeexecutedtomovethetreeintothepreviousstackframe,sothatitcanbeusedtofurtherconstructlargertrees.Formally,whenthelistinthetopframecontainsonlyoneelement(n,T),RETURN(e.g.,step7inFigure4)popsthestack,andappends(n,T)totheendofnewstacktop’slist.Whenallinputtokensareconsumedandthestackcontainsonlyonetree,theparserexecutesFINAL(e.g.,step9inFigure4)toterminatethemachine.BothRETURNandFINALhavenoarguments.Validinstructionset.Ateachstep,anLLmachineprovidesasetofvalidinstructionsthatcanbeexecuted.Indoingso,themachinecanguaranteethatthestateremainsvalidiftheinstructionstobeexecutedarealwayschosenfromthisset.WenowdemonstratethathowourLLmachinesrestrictthespaceofthelearnedprograms.Toachievethis,weimposeseveralconstraintsontheinstructiontypesthatcanbeappliedateachtimestep.Wedenotethecurrentstacktopas(ﬁd,l),thelengthoflasL,andtheﬁrsttokenofthecurrentinputastok(tok=EOFifthecurrentinputisempty).Meanwhile,weassumethateachstackframe’slisthasatmostKelements,andwewillexplainwhythisassumptionholdslater.Theconstraintsforwheneachoftheﬁveinstructionsisallowedareasbelow:1.SHIFT:itisallowediftok6=EOFandL<K.2.CALL:itisallowediftok6=EOF,0<L<K,andtheinstructiontypeatprevioustimestepisnotCALL.Foritsargumentﬁd0,0≤ﬁd0<F,whereF>0isahyper-parameterindicatingthenumberofdifferentfunctionIDs.3.RETURN:itisallowedifthecurrentstackhasmorethanoneframe,andL=1.4.REDUCE:itisallowedifL>0,andtheinstructiontypeatprevioustimestepisnotREDUCE.ForREDUCEargumentsnand(c1,...,cm),nischosenfromthenon-terminalset,and1≤ci≤Lfor1≤i≤m.5.FINAL:itisallowediftok=EOF,L=1,andthecurrentstackhasonlyoneframe.Moredetails.ThenweexplainwhywecansafelyassumethatthereexistsKsuchthateachstackframe’slisthasatmostKelements.Astheparsingprogramcontinues,eachstackframe’slistcontainspartiallyﬁnishedsub-treesthatcorrespondtoapreﬁxofoneproductionruleinthegrammar.Sincethelengthofproductionrulesinacontext-freegrammarisﬁnite,wecanassumethattheupperboundofthelengthisK.AccordingtotheinstructionconstraintsimposedbyLLmachines,usingthesameKastheupperboundonthelengthofeachstackframe’slist,wecanensurethatforeachCALL	1Input∗𝑧Input𝑦∗𝑧Stack(Id, T1), (+,+)IdxT1Input𝑦∗𝑧Stack(Id, T1), (+,+)Stack(Id, T1), (+,+)(Id, T2)IdxT1IdxT1IdyT2SHIFT…CALL	2Input∗𝑧Input𝑦∗𝑧Stack(Id, T1), (*,*)IdxT1Input𝑦∗𝑧Stack(Id, T1), (*,*)IdxT1RETURN…01010002IdxT1IdyT2Stack(Id, T1), (*,*)(Id, T2)02Figure5:The(partial)executiontracesforparsing“x+y*z"(above)and“x*y*z"(bottom)respec-tively.For“x+y*z",“y*z"needstobeassociated;when“y"isreducedtoatreewiththerootId,thetoken"*"needstobeshiftedintothestacktop.Ontheotherhand,for“x*y*z",“x*y"needstobeassociated,thusafter“y"isreduced,theparsetreeof“x*y"shouldbepoppedbeforeshiftingthenexttoken"*"intheinput.Inthiscase,onlythefunctionIDinthestacktop,i.e.,1(above)or2(bottom),candistinguishwhetherSHIFTorRETURNshouldbeexecutednext.22PublishedasaconferencepaperatICLR2018inputinthegrammar,thereexistsatracesatisfyingsuchconstraintsthatcanparsetheinputtoitsparsetreecorrectly.CMODELARCHITECTUREWeexplainhowthemodelchoosestheinstructiontobeexecutedateachstep.Asforthepredictionofinstructiontypes,Letp(inst|ﬁd,l,tok)bethepredictedprobabilitydistributionoveralldifferentinstructiontypesbytheparsingprogram,whichiscomputedinthewaydescribedinSection3.2.BasedoncurrentstateoftheLLmachine,theLLmachineprovidesasetofvalidinstructiontypes.Thenforeachinstructiontype,ifitisinthesetofvalidinstructiontypes,thenitsprobabilityforsamplingisp(inst|ﬁd,l,tok),otherwiseitsprobabilityissettobe0.Unlessotherwisespeciﬁed,ateachstep,themodelchoosestheinstructiontypepredictedwiththehighestprobability.ThewaysofpredictingargumentsforCALLandREDUCEinstructionsaresimilar.Wenowgiveananalysisoff(K),whichisthetotalnumberofpossiblecombinationsofc1,...,cmgivenm≤K.Weconsiderg(i)asthetotalnumberofc1,...,cicombinationsforaﬁxedi,thenwehavef(K)=KXi=1g(i)=KXi=1K!(K−i)!=K!(K−1Xi=01i!)Wenowestimateit.Infact,wehavethatexp(1)−K−1Xi=01i!=+∞Xi=01i!−K−1Xi=01i!=+∞Xi=K1i!<1K!+1K!2K+1Also,wehaveexp(1)−K−1Xi=01i!>1K!Therefore,wehave0≤f(K)−K!exp(1)+1<2K+1<1whereK≥2.Therefore,weconcludef(K)=bK!exp(1)−1c.DTRAININGDETAILSBelowwepresentfulldetailsabouthowtotrainthemodel.FollowingSection4,weﬁrstillustratethetrainingapproachwhenweaksupervisionisprovided,andthenexplainhowtotrainthemodelwithinput-outputpairsonly.D.1WEAKLYSUPERVISEDLEARNINGWeassumethatthesetofallparametersisΘ.WeapplyAdamoptimizertoupdateΘ(i+1)←Θ(i)−η∆Θ(i)whereηisthelearningrate,and∆Θ(i)isthegradientthatconsistsofthreecomponents:∆Θ(i)=γ1·∆Θ1+γ2·∆Θ2+γ3·∆Θ3Inthefollowing,wedescribethethreecomponents∆Θ1,∆Θ2,and∆Θ3respectively.D.1.1REDUCEARGUMENT(c1,...,cm)First,wepresentthedetailsofdiﬀ(T,T0)inAlgorithm2.Theﬁrstcomponentofthegradientiscomputedasthefollowing:∆Θ1=Xt∂logp(c(t)1,...,c(t)m|n(t))∂Θ·rreduce(ˆT(t)r)wheretiteratesoverallREDUCEoperations,c(t)1,...,c(t)mandn(t)indicatethepredictedargumentsinthet-thoperation,andˆT(t)rindicatesthepredictedtreeinthet-thoperation.23PublishedasaconferencepaperatICLR2018Algorithm2ThealgorithmtocomputethedifferencebetweenTandT0.Inthealgorithm,weuseT=N(T1,...,Tj)toindicatethatT’srootisnon-terminalN,whichhasjchildrenT1,...,Tj.functiondiﬀ(T,T0)T=N(T1,...,Tj)T0=N0(T01,...,T0j0)ifN=N0thensum←0elsesum←1endififj<j0thenfori←1tojdosum←sum+diﬀ(Ti,T0i)endforfori←jtoj0dosum←sum+|T0i|endforelsefori←1toj0dosum←sum+diﬀ(Ti,T0i)endforfori←j0tojdosum←sum+|Ti|endforendifreturnsumendfunctionD.1.2REDUCEARGUMENTnForlearningtopredicttheREDUCEargumentn,wecanusereinforcementlearningtechniquesimilartothemethodabove.Inthefollowing,wepresentanothertrainingmethodusingsupervisedlearning.Weobservethatsuchatrainingmethodismoretime-efﬁcientinourexperiments.WeﬁrstmatcheachREDUCEoperationtoatentativegroundtruth.GiventhepredictedtreeˆTandthegroundtruthTg,wematcheachnodeinˆTtoanodeinTginthefollowingway.AssumingthatˆT=ˆN(ˆT1,...,ˆTk)andTg=Ng(Tg1,...,Tgk0),ˆNismatchedtoNgﬁrst,thenˆTiismatchedtoTgirecursivelyfori=1,...,min(k,k0).Ifk>k0,thenˆTifori∈{k0,...,k}ismatchedtoanygroundtruth.Afterwards,thesecondcomponentiscomputedasfollows:∆Θ2=Xt∂logp(n(t)g|ﬁd(t),l(t),tok(t))∂ΘwheretiteratesoverallREDUCEoperationssuchthatthegeneratednon-terminalhasamatchedtentativegroundtruthn(t)g,andlogp(n(t)g|ﬁd(t),l(t),tok(t))isthecross-entropylossbetweenp(n(t)|ﬁd(t),l(t),tok(t))andtheone-hotvectorofn(t)g.D.1.3CALLARGUMENTﬁdWeﬁrstgiveanexampletoillustrateourdesignofrewardfunctionrfinFigure6.Thethirdcomponentiscomputedasfollows:∆Θ3=Xt∂logp(ﬁd0(t)|ﬁd(t),l(t),tok(t))∂Θ·rf(ﬁd0(t))wheretiteratesoverallCALLoperations.24PublishedasaconferencepaperatICLR2018…CALL𝑖𝑛𝑠𝑡1𝑖𝑛𝑠𝑡2𝑖𝑛𝑠𝑡3CALL…Prediction…CALL෣𝑖𝑛𝑠𝑡1෣𝑖𝑛𝑠𝑡2෣𝑖𝑛𝑠𝑡3෣𝑖𝑛𝑠𝑡4…Ground truth𝑙(෣𝑖𝑛𝑠𝑡1,𝑖𝑛𝑠𝑡1)𝑙(෣𝑖𝑛𝑠𝑡2,𝑖𝑛𝑠𝑡2)𝑙(෣𝑖𝑛𝑠𝑡3,𝑖𝑛𝑠𝑡3)𝑙(෣𝑖𝑛𝑠𝑡4,CALL)∑𝑟𝑓1𝛼1𝛼2𝛼3𝛼4Figure6:Theillustrationoftherewardfunctionrf.Theinstructionscoloredorangeindicatethegroundtruth,wherenoneofinst1,inst2,andinst3isaCALLinstruction.TherewardfunctionrfforthepredictionoftheﬁrstCALL’sargument1(inthegreencircle)isthesummationoffourlosses,wherethelossfunctionisthecrossentropyloss.SHIFTREDUCE<Id>	(1)Input+𝑦StackIdxT1REDUCE<Op+>	(1,	2)Input+𝑦StackInput𝑥+𝑦StackInput+𝑦StackIdxT1CALL0SHIFTInput𝑦StackIdxT1SHIFTInputEOFStackIdxT1REDUCE<Id>	(2)InputEOFStackIdxT1IdyT2RETURNInputEOFStackIdxT1IdyT2IdIdxyInputEOFStackT3FINALIdIdxy123456789000(x,x)0(Id,T1)00(Id, T1)00(+,+)0(Id, T1)00(+,+),(y,y)0(Id, T1)00(Id,T2)0(Id, T1)0(Id,T1),(Id,T2)0(Op+,T3)Op+Op+Figure7:Awrongexecutiontracethatcancorrectlyparsex+y.Thewrongoperations(i.e.,steps3,4,6,and8)arecoloredpurple.D.2TRAININGWITHINPUT-OUTPUTPAIRSONLYInthissection,wefurtherdescribethealgorithmfortrainingwithinput-outputpairsonly,especiallyforhowtosearchforthecandidatetraceset.AsexplainedinSection4.1,thealgorithmneedstoﬁndthesetofvalidcandidatetracesforeachinput-outputexample.Noticethatforoneinput-outputexample,thepossiblevalidexecutiontracesarenotunique.Figure7providesonealternativeexecutiontracethatsuccessfullyparsesx+yintoitsparsetree.Onlywhencombiningmultipleexamples,themodeltrainedwiththistracecannotﬁtallexamplesatthesametime.Searchingforthecandidatetraceset.Herewefurtherexplainthetwo-nested-loopprocesstosearchforthecandidatetracesetfollowingSection4.1.First,intheouterloop,werandomlysampleaninstructiontypetracebasedonthedistributiondescribedinSection4.1.Thenintheinnerloop,wetrytousethesampledtraceintheexternalloopasthetentativegroundtruth,andthenemploytheweaklysupervisedlearningapproachtotraintheparameterspredictingtheargumentsforM1iterations.IfinanyoftheseM1iterations,thecorrectoutputisproduced,weaddthesampledinstructiontracetothecandidatetraceset.Otherwise,ifthecorrectoutputisneverproducedduringtheseM1iterations,werevertthemodel’sparameterstopredicttheargumentsbacktothosebeforetheseM1weaksupervisedlearningiterations,andcontinuesamplinganotherinstructiontrace.This25PublishedasaconferencepaperatICLR2018processiscontinuedforM2iterations,i.e.,atotalofM2instructiontracesaresampled.Meanwhile,toescapefromasub-optimalmodel,were-initializethemodelwiththeonelearnedfromthepreviouslessoneveryM3iterations.EHYPERPARAMETERSOFOURPROPOSEDMETHODFortheLLmachines,F=10.AboutthecapacityofeachstackframeK,K=3forWHILElanguage,andK=4forLAMBDAlanguage.Inthearchitectureoftheneuralparsingprogram,eachLSTMhas1layer,withitshiddenstatesizeD=50,whichisthesameastheembeddingsize.Asforthetraining,learningrateisη=0.01withnodecay.Nodropoutisused.Gradientweightsforthethreecomponents∆Θ1,∆Θ2and∆Θ3areγ1=10.0,γ2=1.0,andγ3=0.01respectively.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingAdamoptimizer.Allweightsareinitializeduniformlyrandomlyin[−0.1,0.1].Themini-batchsizeis1.Forcandidatetracesearch,σ=0.1,M1=20,M2=10,000,andM3=2,000.Typically,foreachinput,thecorrecttracecouldbefoundaftersamplingwithin1,000traces.FHYPERPARAMETERSOFBASELINEMODELSForthebaselinemodelsinourevaluation,i.e.,seq2seqVinyalsetal.(2015b),seq2treeDong&Lapata(2016),andLSTMwithunboundedmemoryGrefenstetteetal.(2015),weimplementthemourselves.Wechoosetheirhyperparametersbasedontheirpapersrespectively,andfurthertuneonourdatasetstogetbetterexperimentalresults.Speciﬁcally,intheseq2seqmodelVinyalsetal.(2015b),eachoftheencoderandthedecoderisa3-layerLSTM,andthehiddenstatesizeofeachlayeris256,whichisthesameastheembeddingsize.WeapplytheattentionmechanismdescribedinVinyalsetal.(2015b).Asfortraining,learningrateis0.01.Thedropoutrateis0.5.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingAdamoptimizer.Allweightsareinitializeduniformlyrandomlyin[−0.1,0.1].Themini-batchsizeis256.Intheseq2treemodelDong&Lapata(2016),eachoftheencoderandthedecoderisa1-layerLSTM,anditshiddenstatesizeis256,whichisthesameastheembeddingsize.WeapplytheattentionmechanismdescribedinDong&Lapata(2016).Asfortraining,learningrateis0.005.Thedropoutrateis0.5.GradientswithL2normlargerthan5.0arescaleddowntohavethenormof5.0.ThemodelistrainedusingRMSPropoptimizer.Allweightsareinitializeduniformlyrandomlyin[−0.1,0.1].Themini-batchsizeis20.InStack-LSTM,Queue-LSTMandDeQue-LSTMmodelsdescribedinGrefenstetteetal.(2015),eachoftheencoderandthedecoderisa1-layerLSTM,anditshiddenstatesizeis256,whichisthesameastheembeddingsize.Asfortraining,learningrateis0.001.Wedonotusedropoutforthesemodels.GradientswithL2normlargerthan1.0arescaleddowntohavethenormof1.0.ThemodelistrainedusingRMSPropoptimizer.Allweightsareinitializeduniformlyrandomlyin[−0.1,0.1].Themini-batchsizeis10.GWHILELANGUAGEBelowisthegrammarspeciﬁcationoftheWHILElanguage.26PublishedasaconferencepaperatICLR2018<Identifier>::=x|y<Literal>::=0|1<Op*>::=<Identifier>×<Identifier>|<Identifier>×<Literal>|<Literal>×<Identifier>|<Literal>×<Literal>|<Op*>×<Identifier>|<Op*>×<Literal><Op+>::=<Identifier>+<Identifier>|<Identifier>+<Literal>|<Identifier>+<Op*>|<Literal>+<Identifier>|<Literal>+<Literal>|<Literal>+<Op*>|<Op+>+<Identifier>|<Op+>+<Literal>|<Op+>+<Op*>|<Op*>+<Identifier>|<Op*>+<Literal>|<Op*>+<Op*><Eq>::=<Identifier>==<Identifier>|<Identifier>==<Literal>|<Identifier>==<Op+>|<Identifier>==<Op*>|<Literal>==<Identifier>|<Literal>==<Literal>|<Literal>==<Op+>|<Literal>==<Op*>|<Op+>==<Identifier>|<Op+>==<Literal>|<Op+>==<Op+>|<Op+>==<Op*>|<Op*>==<Identifier>|<Op*>==<Literal>|<Op*>==<Op+>|<Op>==<Op*><Assign>::=<Identifier>=<Identifier>|<Identifier>=<Literal>|<Identifier>=<Op+>|<Identifier>=<Op*><If>::=<Assign>if<Identifier>|<Assign>if<Literal>|<Assign>if<Op+>|<Assign>if<Op*>|<Assign>if<Eq>|<If>if<Identifier>|<If>if<Literal>|<If>if<Op+>|<If>if<Op*>|<If>if<Eq>27PublishedasaconferencepaperatICLR2018<Seq>::=<Assign>;<Assign>|<Assign>;<If>|<Assign>;<While>|<If>;<Assign>|<If>;<If>|<If>;<While>|<While>;<Assign>|<While>;<If>|<While>;<While>|<Seq>;<Assign>|<Seq>;<If>|<Seq>;<While><Block>::={<Assign>}|{<If>}|{<While>}|{<Seq>}<While>::=while<Identifier><Block>|while<Literal><Block>|while<Op+><Block>|while<Op*><Block>|while<Eq><Block>HLAMBDALANGUAGEBelowisthegrammarspeciﬁcationoftheLAMBDAlanguage.<Var>::=a|b|...|z<App>::=<Var><Var>|<App><Var><Bind>::=lama|...|lamz<Lam>::=<Bind>.<Var>|<Bind>.<App>|<Bind>.<Lam>|<Bind>.<Let><LetExpr>::=<Var>=<Var>|<Var>=<App>|<Var>=<Lam>|<Var>=<Let><Let>::=let<LetExpr>in<Var>|let<LetExpr>in<App>|let<LetExpr>in<Lam>|let<LetExpr>in<Let>IPYTHONIMPLEMENTATIONOFWHILELANGUAGEPARSER1defnextInstruction(self):2fid,top=self.fid[−1],self.stack[−1]3next=self.input[self.cur]ifself.cur<len(self.input)elseNone4iflen(top)==0:5returnself.shift,None6eliflen(top)==1:7iftop[0][1]==’while’:8returnself.call,09eliftop[0][1]==’{’:10returnself.call,011eliftop[0][1]==’x’ortop[0][1]==’y’:12returnself.reduce,(IDENT,[0])13eliftop[0][1]==’0’ortop[0][1]==’1’:28PublishedasaconferencepaperatICLR201814returnself.reduce,(LIT,[0])15elifnext==’;’:16iffid<1:17returnself.shift,None18else:19returnself.ret,None20elifnext==’if’:21iffid<2:22returnself.shift,None23else:24returnself.ret,None25elifnext==’=’:26iffid<3:27returnself.shift,None28else:29returnself.ret,None30elifnext==’==’:31iffid<4:32returnself.shift,None33else:34returnself.ret,None35elifnext==’+’:36iffid<5:37returnself.shift,None38else:39returnself.ret,None40elifnext==’∗’:41iffid<6:42returnself.shift,None43else:44returnself.ret,None45elifnext==None:46iflen(self.stack)==1:47returnself.final,None48else:49returnself.ret,None50else:51returnself.ret,None52eliflen(top)==2:53iftop[0][1]==’{’andnext==’}’:54returnself.shift,None55else:56next_fid=057iftop[0][1]==’while’:58next_fid=659eliftop[1][1]==’;’:60next_fid=161eliftop[1][1]==’if’:62next_fid=263eliftop[1][1]==’=’:64next_fid=365eliftop[1][1]==’==’:66next_fid=467eliftop[1][1]==’+’:68next_fid=569eliftop[1][1]==’∗’:70next_fid=671returnself.call,next_fid72else:#len(top)==373iftop[1][1]==’=’:74returnself.reduce,(ASSIGN,[0,2])75eliftop[1][1]==’if’:76returnself.reduce,(IF,[2,0])77eliftop[1][1]==’;’:78returnself.reduce,(SEQ,[0,2])29PublishedasaconferencepaperatICLR201879eliftop[0][1]==’while’:80returnself.reduce,(WHILE,[1,2])81eliftop[0][1]==’{’andtop[2][1]==’}’:82returnself.reduce,(BLOCK,[1])83else:84iftop[1][1]==’+’:85returnself.reduce,(OP_P,[0,2])86eliftop[1][1]==’∗’:87returnself.reduce,(OP_M,[0,2])88eliftop[1][1]==’==’:89returnself.reduce,(EQ,[0,2])JPYTHONIMPLEMENTATIONOFLAMBDALANGUAGEPARSER1defnextInstruction(self):2fid,top=self.fid[−1],self.stack[−1]3next=self.input[self.cur]ifself.cur<len(self.input)elseNone4iflen(top)==0:5returnself.shift,None6eliflen(top)==1:7iftop[0][1]==’let’:8returnself.call,09eliftop[0][1]==’lam’:10returnself.shift,None11eliftop[0][0]<0andtop[0][1]inself.alpha:12returnself.reduce,(VAR,[0])13else:14ifnextinself.alpha:15iffid==0:16returnself.call,117else:18returnself.ret,None19elifnext==’=’ornext==’.’:20returnself.shift,None21else:22iflen(self.stack)>1:23returnself.ret,None24else:25returnself.final,None26eliflen(top)==2:27iftop[0][1]==’let’:28returnself.shift,None29eliftop[0][1]==’lam’:30returnself.reduce,(BIND,[1])31eliftop[1][1]==’=’:32returnself.call,033eliftop[0][1]==BIND:34returnself.call,035else:36returnself.reduce,(APP,[0,1])37eliflen(top)==3:38iftop[0][1]==’let’ortop[0][1]==’lam’:39returnself.call,040else:41nt=LETEXPR42iftop[0][1]==BIND:43nt=LAMBDA44returnself.reduce,(nt,[0,2])45else:#len(top)==4:46returnself.reduce,(LET,[1,3])30PublishedasaconferencepaperatICLR2018KMOREANALYSISOFTHEROBUSTFILLDSLEfﬁcientlyenumeratetheRobustFillspace.RobustFillDSLallowsprogramstoemitaconcate-nationofupto10constructors,eachofwhicheitheroutputsaconstrantcharacter,ortheresultofextractingasubstringfromtheinputandperformingastringtransformationsuchasreplace,trim,etc.Foreachconstructor,therecanbeapproximately30millionuniqueexpressions,andthusthetotalnumberofprogramswithupto10constructorscanbehuge.Toefﬁcientlyenumeratethebestexpression,werelyonheuristicstocutmostprogramsthatdonotleadtothebestaccuracy.Infact,ifweenumeratetheexpressionforeachconstructorfromtheﬁrsttothelast,weneedtorequirethatthepartialprogramgeneratedshouldmatchthe“mostnumberofinput-outputexamples".Tothisend,wemanuallyconstructagoodprogram,andassumeitsaccuracyisp.Foreachpartialprogram,ifitcannotyieldabetteraccuracybyp,thentheprogramwillbedropped.Thatis,thereareatleast(1−p)Ninputsthattheprogramwillresultinanoutputthatdoesnotmatchthepreﬁxofthegroundtruth.Further,therewillbeemptyconstructorswhichwilloutputanemptystringforanyinput.Theywillcausetheenumerationinefﬁcient,andwealsoeliminateallemptyconstructorsfrombeingenumerated.Indoingso,wecanestimateanupperboundontheaccuracythatcanbeachievedbytheRobustFillapproach.WhyRobustFillDSLcannothandletheparsingproblem?Inthissection,weprovethattheRobustFillDSLisnotexpressivefortheparsingproblem.Infact,thetop-levelconstructoroftheDSLallowsanarbitrarynumberoftokensconcatenatedtogether.SincetheRobustFillapproachhasthelimitationtosynthesizeonlyprogramswithupto10tokens,weputthisconstrainttotheDSL.Althoughthisconstraintlooksartiﬁcial,thisisnotthefundamentalreasonforwhytheDSLisnotexpressiveenough.Eachofthese10tokenscanbeeitheraconstantchar,oratransformationofthesubstringoftheinputsequence.Therefore,theoutputofanyprogramoftheDSLcanhaveupto10tokens.However,inthetrainingset,eachparsetreehasmorethan20tokens,andthusnoprogramcangeneratetheparsetree.Notethattheproofworksforanyprogramswithﬁnitelength.Givenaprogramoflengthn,itcannotgenerateanoutputwithmorethanntokens,butthetestsetcancontainarbitrarilylongoutputs.Therefore,anyspeciﬁcprogramcannotgeneralizetolongeroutputs.LMISCELLANEOUSWepresentthethree-linePythonimplementationofQuicksortbelow:defqsort(a):iflen(a)<=1:returnareturnqsort([xforxinaifx<a[0]])+\[xforxinarrayifx==a[0]]+qsort([xforxinaifx>a[0]])31