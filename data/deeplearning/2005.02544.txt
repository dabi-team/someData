Under Review

AutoScale: Optimizing Energy Efﬁciency of End-to-End
Edge Inference under Stochastic Variance

Young Geun Kim∗ and Carole-Jean Wu∗†
Arizona State University∗ Facebook AI†
younggeun.kim@asu.edu carolejeanwu@fb.com

0
2
0
2

y
a
M
6

]

G
L
.
s
c
[

1
v
4
4
5
2
0
.
5
0
0
2
:
v
i
X
r
a

ABSTRACT
Deep learning inference is increasingly run at the edge. As
the programming and system stack support becomes mature,
it enables acceleration opportunities within a mobile system,
where the system performance envelope is scaled up with a
plethora of programmable co-processors. Thus, intelligent
services designed for mobile users can choose between run-
ning inference on the CPU or any of the co-processors on
the mobile system, or exploiting connected systems, such as
the cloud or a nearby, locally connected system. By doing
so, the services can scale out the performance and increase
the energy efﬁciency of edge mobile systems. This gives
rise to a new challenge–deciding when inference should run
where. Such execution scaling decision becomes more com-
plicated with the stochastic nature of mobile-cloud execution,
where signal strength variations of the wireless networks and
resource interference can signiﬁcantly affect real-time infer-
ence performance and system energy efﬁciency. To enable
accurate, energy-efﬁcient deep learning inference at the edge,
this paper proposes AutoScale. AutoScale is an adaptive and
light-weight execution scaling engine built upon the custom-
designed reinforcement learning algorithm. It continuously
learns and selects the most energy-efﬁcient inference execu-
tion target by taking into account characteristics of neural
networks and available systems in the collaborative cloud-
edge execution environment while adapting to the stochastic
runtime variance. Real system implementation and evalua-
tion, considering realistic execution scenarios, demonstrate an
average of 9.8 and 1.6 times energy efﬁciency improvement
for DNN edge inference over the baseline mobile CPU and
cloud ofﬂoading, while meeting the real-time performance
and accuracy requirement.

1.

INTRODUCTION

It is expected that there will be more than 7 billion mo-
bile device users and 900 million wearable device users in
2021 [84, 85], including smartphones, smart watch, wearable
virtual or mixed reality devices. To improve mobile user
experience, various intelligent services, such as virtual assis-
tance [1, 3], face/image recognition [31], and language trans-
lation [33], have been introduced in recent years. Many com-
panies, including Amazon, Facebook, Google, and Microsoft,
are using sophisticated machine learning models, especially
Deep Neural Networks (DNNs) as the key machine learning
component for these intelligent services [1, 33, 65, 92].

Traditionally, due to the compute- and memory-intensive
nature of the DNN workloads [5, 15, 38], both training and
inference were executed on the cloud [22, 44], while the
mobile devices only acted as user-end sensors and/or user

interfaces. More recently, with the advancements of power-
ful mobile System-on-Chips (SoCs) [35, 41, 90], there have
been increasing pushes to execute DNN inference on the
edge mobile devices [8, 22, 36, 44, 46, 55, 89, 90, 92, 98]. This
is because executing inference at the edge can improve the
response time of services, by removing data transmission
overhead. However, executing inference on the edge mo-
bile devices also results in increased energy consumption of
the mobile SoCs [44]. Since the edge mobile devices are
energy-constrained [51], it is necessary to optimize the en-
ergy efﬁciency of the DNN inference, while satisfying the
Quality-of-Service (QoS) requirements of these services.

To address these performance and energy efﬁciency chal-
lenges, modern mobile devices employ more and more ac-
celerators and/or co-processors, such as Graphic Processing
Units (GPU), Digital Signal Processors (DSPs), and Neu-
ral Processing Units (NPUs) [10, 42], scaling up the overall
system performance. Furthermore, the mobile system stack
support for DNNs has become more mature, allowing DNN
inference to leverage the computation and energy efﬁciency
advantages provided by the co-processors. For example, mod-
ern deep learning compiler and programming stacks, such as
TVM [10], SNPE [77], and Android NN API [2, 42], enable
inference execution on a diverse set of hardware back-ends.
These recent advancements give rise to a new challenge—
deciding when inference should run where. Intelligent ser-
vices aiming to run on the mobile devices can choose between
running inference on the CPU or any of the co-processors
on the device, or exploiting connected systems, such as the
cloud or a nearby, locally-connected system [4] that is more
powerful than the device itself. By doing so, the services
can scale out the performance and increase the energy efﬁ-
ciency of edge mobile devices. For example, many person-
alized health and entertainment use cases are powered by
a collaborative execution environment composed of smart
watches, smartphones, and the cloud [25, 39, 73, 91]. Simi-
larly, virtual and augmented reality systems consist of wear-
able electronics, smartphones as the staging device, and the
cloud [30, 32, 66, 72]. However, the decision process is chal-
lenging for any intelligent services, since energy efﬁciency of
each execution target signiﬁcantly vary depending on various
features, such as NN characteristics and/or edge-cloud system
proﬁles. The extremely fragmented mobile SoCs make this
decision process even more difﬁcult, as there are myriads of
hardware targets with different proﬁles [92] to choose from.
To determine the optimal execution scaling decision, state-
of-the-art approaches, such as [22,36,44,89,90,98], proposed
to build predictive models. However, these prior approaches
did not consider stochastic runtime variances, such as inter-

1

 
 
 
 
 
 
ference from co-running tasks or network signal strength vari-
ations, which have a large impact on energy efﬁciency [29].
In a realistic execution environment, there can be several
applications simultaneously running along with the DNN
inference [48, 57, 83], since recent mobile devices support
multitasking features [83] such as screen sharing between
multiple applications. In addition, signal strength variations
of the wireless networks can signiﬁcantly affect performance
and energy efﬁciency of cloud inference, since the data trans-
mission latency and energy exponentially increase when the
signal strength is weak [52], which accounts for 43% of
data transmission [16]. Therefore, without considering such
stochastic variances, one would not be able to choose the
optimal execution scaling decision for DNN inference.

This paper proposes an adaptive and light-weight execution
scaling engine, called AutoScale, to make accurate scaling
decisions for the optimal execution target of edge DNN in-
ference under the presence of stochastic variances. Since the
optimal execution target signiﬁcantly varies depending on the
NN characteristics, the underlying execution platforms, as
well as the stochastic runtime variances, it is infeasible to enu-
merate the massive design space exhaustively. Therefore, Au-
toScale leverages a lightweight reinforcement learning tech-
nique for continuous learning, that captures and adapts to the
environmental variances of stochastic nature [17, 67, 71, 82].
AutoScale observes NN characteristics, such as layer compo-
sition, and current system information, such as interference
intensity and network stability, and selects an execution target
which is expected to maximize the energy efﬁciency of DNN
inference, satisfying the performance and accuracy targets.
The result of the selection is then measured from the system
and fed back to AutoScale, allowing AutoScale to contin-
uously learn and predict the optimal execution target. We
demonstrate AutoScale with real system-based results that
show improved energy efﬁciency of DNN inference by 9.8X
and 1.6X on average, compared to the baseline settings of
mobile CPU and cloud ofﬂoading, satisfying both the QoS
and accuracy constraints with 97.9% of prediction accuracy.

This paper makes the following key contributions:
• This paper provides an in-depth characterization of
DNN inference execution on mobile and edge-cloud
systems. The characterization results show that the
optimal execution scaling decision signiﬁcantly varies
depending on the NN characteristics and the stochastic
nature of mobile execution (Section 3).

• This paper proposes an intelligent execution scaling
engine that accurately selects the optimal execution
target of mobile inference in the presence of stochastic
variances (Section 4).

• To demonstrate the feasibility and practicality of the
proposed execution scaling engine, we implement and
evaluate AutoScale with a variety of on-device inference
use cases under the edge-cloud execution environment
using real systems and devices, allowing AutoScale to
be adopted immediately1 (Section 6).

2. BACKGROUND

This section introduces the necessary background that
makes up the components for the AutoScale framework, i.e.,

1We plan to open source AutoScale upon paper acceptance.

DNN, inference at the edge, and QoE of real-time inference.
2.1 Deep Neural Network

DNNs are constructed by connecting a large number of
functional layers to extract features from inputs at multiple
levels of abstraction [45, 56]. Each layer is composed of
multiple processing elements (neurons), which are applied
with the same function to process different parts of an input.
Depending on what function is applied, the layers can be
classiﬁed into the various types [15]. These layers and their
execution characteristic differences are essential since they
can affect the decision made by AutoScale. We give brief
descriptions for each layer type below.

Convolutional layer (CONV) performs a two-dimensional
convolution to extract a set of feature maps from its input. To
selectively activate meaningful features, an activation func-
tion, such as sigmoid or rectiﬁed-linear, is applied to the ob-
tained feature maps. Typically, this layer is compute-intensive
due to the calculation of convolutions.

Fully-connected layer (FC) computes the weighted sum
of the inputs using a set of weights and then applies the
activation function to the weighted sum of the inputs. This
layer is one of the most compute- and memory-intensive
layers in DNNs [15,44,46], since its neurons are exhaustively
connected to all the neurons in the previous layer.

Recurrent layer (RC) is a layer where the output of current
step in a sequence is used as an additional input in the next
step of the sequence. In each step, this layer also computes
the weighted sum of the inputs using a set of weights. This
layer is even more compute- and memory-intensive than FC
layer, since its neurons can be connected to neurons in the
previous, current, and the next layer.

Other commonly-used layers include: Pooling layer ap-
plies a sub-sampling function, such as max or average, to re-
gions of the input feature maps; Normalization layer normal-
izes features across spatially grouped feature maps; Softmax
layer produces a probability distribution over the number of
possible classes for classiﬁcation; Argmax layer chooses the
class with the highest probability; Dropout layer randomly
ignores neurons during training and allows the neurons to
pass through during inference. These layers are typically less
compute- and memory-intensive than CONV, FC, and RC lay-
ers, such that they do not have a large impact on performance
and energy efﬁciency of DNN inference.

DNNs can be constructed with various compositions of
layers. For example, NNs used for computer vision appli-
cations (e.g., Inception, Mobilenet, Resnet, etc.) are mainly
composed of CONV, POOL and FC layers. On the other
hand, NNs used for language processing applications (e.g.,
BERT) mainly consist of RC layers, such as Long Short-
Term Memory (LSTM) and attention. Since each layer has
unique characteristics due to different compute and memory
intensities, to optimize inference execution for DNNs, it is
important to consider the layer composition.
2.2 DNN Inference Execution at the Edge

Figure 1 depicts the general structure of the system stack
for machine learning inference execution at the edge. At
the front-end, DNNs are implemented with various frame-
works, such as TensorFlow [88], PyTorch [76], Caffe [7],
MXNet [70], etc, whereas the middleware allows deploy-

2

Figure 1: System stack for DNN inference execution.

ment of DNN inference execution onto a diverse set of hard-
ware back-ends. They also enable efﬁcient inference at
the edge—various NN optimizations, such as quantization
[13, 26, 43, 53, 55, 92, 97], weight compression [37, 58] and
graph pruning [93, 96] can be employed before the DNNs
are deployed. Among the optimizations, the quantization
is one of the most widely used ones for the edge execution,
since it reduces both compute and memory intensities of the
inference; quantization shrinks the 32-bit ﬂoating-point val-
ues (FP32) of NNs to fewer bits such as 16-bit FP values
(FP16) or 8-bit integer values (INT8). Since the middleware
does not select a speciﬁc hardware target for DNN inference
execution, intelligent services should choose one among the
possible hardware targets. However, this decision process is
challenging, since energy efﬁciency of each execution target
can signiﬁcantly vary depending on various features.

2.3 Real-Time Inference Quality of Experience
Quality of user experience is a key metric for mobile opti-
mization. For real-time inference, the Quality-of-Experience
(QoE) is the product of inference latency, inference accuracy,
and system energy efﬁciency. To improve energy efﬁciency
of mobile devices, a number of energy management tech-
niques can be used [51]. Unfortunately, the techniques often
sacriﬁce performance (i.e., latency) for energy efﬁciency,
degrading QoE of real-time inference.

Inference latency is an important factor for QoE, since if
the latency of a service exceeds the human acceptable limit,
users would abandon the service [83, 99]. However, a single-
minded pursuit of performance is not desirable in mobile
devices due to their energy constrained nature. Hence, there
is a need to provide just enough performance to meet the QoS
expectations of users with minimal energy consumption. The
QoS expectation of users can be deﬁned as a certain latency
value (e.g., 33.3 ms for 30 FPS video frame rate [19,99] or 50
ms for interactive applications [20, 63]), below which most
users cannot perceive any notable difference.

Various NN optimizations can improve both the latency
and energy efﬁciency of inference. However, the optimiza-
tions often sacriﬁce inference accuracy. Since human-level
accuracy is one of the key requirements toward user satisfac-
tion [5, 15, 46], it is also important to maintain the inference
accuracy above the inference quality expectation of users.

In summary, to maximize the quality of user experience
for real-time inference, it is crucial to maximize the system-

3

Figure 2: Optimal execution target depends on NN char-
acteristics and edge-cloud system proﬁles. Note that
PPW is normalized to Edge(CPU) and latency is normal-
ized to the QoS target.

wide energy efﬁciency while satisfying the human acceptable
latency and accuracy expectations.
3. MOTIVATION

This section presents system characterization results for
realistic DNN inference scenarios deployed on real mobile
and edge-cloud systems. We examine the design space that
covers three important axes—latency, accuracy, and energy
efﬁciency (performance per watt).

For mobile inference, we select three smartphones—Xiaomi
Mi8Pro, Samsung Galaxy S10e, and Motorola Moto X Force—
to represent the categories of high-end mobile systems with
GPU and DSP co-processors, high-end mobile systems with
GPU but without DSP, and mid-end mobile systems2, respec-
tively. The edge-cloud inference execution is emulated with
the three smartphones and a server-class Intel Xeon proces-
sor, hosting an NVIDIA P100 GPU. For a locally connected
mobile device, we use a tablet, Samsung Galaxy Tab S6; note
that we connect the smartphones with the tablet via a Wi-Fi-
based peer-to-peer wireless network, Wi-Fi direct. Detailed
speciﬁcations of the mobile and edge-cloud execution setup
are presented in Section 5.
3.1 Varying Optimal DNN Execution Target

• Optimal edge-cloud execution depends on the NN charac-

teristics and edge-cloud system proﬁles.

Figure 2 shows the energy efﬁciency and latency of three
commonly-deployed mobile inference use cases over the
2High-end mobile systems with and without an NN-specialized
accelerator (i.e., DSP) are used to examine the performance scale-up
from off-the-shelf mobile systems. In addition, we select Moto X
Force to represent the mid-end mobile systems with a much wider
market coverage [92] (see detail in Section 5).

Figure 3: Each layer in NNs exhibits different latency on
different mobile processors. For this reason, the optimal
execution target for NNs vary depending on layer compo-
sitions. Note that latency is normalized to that of CPU.

three mobile and the edge-cloud setup. The x-axis represents
the running mobile system with three representative NNs.

For the high-end systems (i.e., Mi8Pro and Galaxy S10e),
the optimal edge-cloud execution shifts, depending on NN
characteristics. For example, in the case of light NNs, such as
InceptionV1 and MobilenetV3, edge inference execution is
more efﬁcient than cloud inference execution. This is because
the performance of off-the-shelf mobile SoCs is sufﬁcient to
satisfy the QoS target of the light NNs. On the other hand, in
the case of heavy NNs, such as MobileBERT, cloud execution
is more efﬁcient than edge execution, since the performance
of the mobile SoCs is insufﬁcient. In this case, the perfor-
mance gain of cloud execution (reduced computation time
and energy) outweighs its loss (increased data transmission
time and energy).

For the mid-end system (i.e., Moto X Force), however,
scaling out to the connected systems is always advantageous,
since performance of the SoC in this system is not enough
even for the light NNs. In the case of light NNs, scaling out
to a locally connected device could be an option, as opposed
to scaling out to the cloud, since 1) the higher-end device (i.e.,
tablet) can satisfy the QoS constraint of the light NNs, and
2) data transmission overhead between the locally connected
edge devices is usually smaller than that between edge-cloud.
On the other hand, in case of heavy NNs, there is no other
option than scaling out to the cloud.

• Optimal execution target depends on layer compositions.

Another important observation in edge inference execution
is that, the optimal execution target can vary depending on
the layer compositions of the NNs. Figure 3 shows the cu-
mulative latency of different layers in two NNs3 running on
different processors in Mi8Pro. The compute- and memory-
intensive FC layers exhibit much longer latency when running
on co-processors, while other layers exhibit longer latency
when running on CPUs. Due to this difference, NNs which
have a larger number of FC layers (e.g., MobilenetV3) run
more efﬁciently on CPUs, while others (e.g., InceptionV1)
run more efﬁciently on co-processors. This result also implies
that the co-processors do not always outweigh the CPUs, so
that carefully choosing one considering layer compositions is
crucial for energy efﬁciency.

3MobileBERT was not used for this experiment, since the inference
execution of MobileBERT on co-processors is not supported by any
middleware yet.

Figure 4: Depending on the inference accuracy target,
optimal edge-cloud execution also shifts. Note that PPW
is normalized to Edge(CPU FP32).

• The optimal edge-cloud execution varies with the inference

quality requirement.

Figure 4 shows the energy efﬁciency (PPW) and accuracy
of DNN inference on different execution targets, where the
inference quality (i.e., accuracy) of each NN highly depends
on the execution target. Note that the accuracy for each
processor is measured in our edge-cloud systems, by using
ImageNet validation set [14]. If the accuracy target is 50%,
the optimal target might be DSP INT8 and CPU INT8 for
InceptionV1 and MobilenetV3, respectively; it shows the
highest energy efﬁciency while satisfying the QoS constraint.
However, if the accuracy target is 65%, the optimal target
should shift to the cloud to satisfy the accuracy target.

3.2

Impact of Runtime Variance on Inference
Execution

In a realistic execution environment, there can be on-device
interference from co-running applications [48, 57, 83]. In
addition, the network signal strength can signiﬁcantly vary,
depending on the movement of edge device users. In fact,
users suffer signiﬁcant signal strength variations in daily life
(43% of data are transmitted under weak signal strength [16]).

• On-device interference and varying network stability shifts

the optimal edge-cloud execution.

Figure 5 shows the normalized energy efﬁciency (PPW)
and latency of DNN (i.e., MobilenetV3) inference when CPU-
intensive or memory-intensive synthetic applications are co-
running, changing the optimal execution target. When a CPU-
intensive application is co-running, the energy efﬁciency of
the inference execution on CPU is signiﬁcantly degraded, due
to 1) competitions for CPU resources, and 2) frequent ther-
mal throttling from high CPU utilization [50]. In this case,
the optimal execution target shifts from the CPU to the GPU.
On the other hand, when a memory-intensive application is
co-running, the energy efﬁciency of all the on-device proces-
sors (including CPU, GPU, and DSP) is degraded, since the
inference execution is competing with other applications for
the memory resources. In this case, the optimal execution
target shifts from the edge to the cloud.

Figure 6 shows the normalized energy efﬁciency (PPW)
and latency of DNN (i.e., Resnet50) inference when signal
strength of wireless networks vary. When the signal strength
gets weaker, the energy efﬁciency of inference execution
on the connected systems is signiﬁcantly degraded, since

4

Figure 5: In the presence of on-device interference, the
optimal edge-cloud execution shifts. Note that PPW is
normalized to Edge(CPU) with no co-running app and
latency is normalized to the QoS target.

Figure 6: Under the signal strength variation, the opti-
mal target for edge-cloud execution also shifts. Note that
PPW is normalized to Edge(Best Processor) and latency
is normalized to the QoS target.

1) the data transmission time exponentially increases with
decreased data rate [16, 52], and 2) the network interface
consumes more power to transmit data with stronger signals.
If only the Wi-Fi signal strength gets weaker, the locally-
connected edge device can still serve as an optimal execution
target. However, if the signal strength of Wi-Fi direct also
gets weak, the optimal target would shift to the edge.

3.3

Inefﬁciency of Prediction-based Approaches

The energy optimization of mobile DNN inference can be
formulated as the problem of choosing the optimal execution
target under the presence of stochastic runtime variances,
which optimizes energy efﬁciency while satisfying the QoS
and accuracy constraints. One of the possible solutions for
this kind of problems is to evaluate all the execution targets
based on a prediction model. Unfortunately, due to the mas-
sive design space, it is difﬁcult to simply build an accurate
prediction model. The inaccurate prediction can result in the
selection of a sub-optimal execution target.

• It is infeasible to enumerate the massive design space ex-
haustively. Simple prediction-based approaches are in-
sufﬁcient, leaving a signiﬁcant room for energy efﬁciency
improvement.

To shed light on the inefﬁciency of existing prediction-
based approaches, we compare two types of prediction-based
approaches with the baseline (Edge CPU) and oracular design
(Opt): (1) regression-based approaches and (2) classiﬁcation-
based approaches. For each type of approaches, we use
methods that are widely adopted by existing works in this do-
main [8, 22, 36, 44, 98]. For the regression-based approaches,
we use Linear Regression (LR) [81] and Support Vector Re-
gression (SVR) [18]. On the other hand, for the classiﬁcation-

5

Figure 7: There is a signiﬁcant gap between Opt and
existing prediction-based approaches, as they fail to ac-
curately predict the optimal execution target under the
presence of runtime variances.

based approaches, we use Support Vector Machine (SVM) [86]
and K-Nearest Neighbor (KNN) [94].

Figure 7 shows the energy efﬁciency (PPW) and the QoS
violation ratio of prediction-based approaches normalized
to those of Edge CPU. Although the prediction-based ap-
proaches improve energy efﬁciency compared to the baseline,
there is a signiﬁcant gap between the approaches and Opt, as
they fail to accurately select the optimal execution target.

When there is no runtime variance, the MAPEs (Mean
Absolute Percentage Errors) of LR and SVR are 13.6% and
10.8%, respectively. However, under the presence of stochas-
tic runtime variances, MAPEs of LR and SVR are 24.6%
and 21.1%, respectively. Due to the inaccurate prediction of
energy efﬁciency and latency, these approaches fail to run
DNN inference on the optimal execution target, degrading
energy efﬁciency and violating the QoS constraint.

On the other hand, the miss-classiﬁcation ratio of SVM and
KNN are 12.7% and 14.3%, respectively, under the presence
of runtime variances. Though the two values do not seem
to be large, these approaches degrade energy efﬁciency and
latency much more than regression model-based approaches.
This is because the wrong decision occurs regardless of the
absolute magnitude of energy efﬁciency and latency. For
example, even though the on-device inference is much more
efﬁcient than cloud inference in case of weak signal strength,
cloud inference can be selected as the execution target.

These results call for the need of a novel scheduler design
which can accurately select the optimal DNN inference execu-
tion target, while adapting to the stochastic runtime variances.
In the next section, we explain our proposed AutoScale which
self-learns the optimal execution target under the presence of
runtime variances based on reinforcement learning.

4. AUTOSCALE

Figure 8 provides the design overview of AutoScale in the
context of the mobile and edge-cloud DNN inference exe-
cution. For each inference execution, AutoScale observes
the current execution state ( 1(cid:13)), including NN characteristics
as well as runtime variances. For the observed state, Au-
toScale selects an action (i.e., execution target) ( 2(cid:13)), which
is expected to maximize energy efﬁciency satisfying QoS
and inference quality target, based on a lookup table (i.e.,
Q-table); the table contains accumulated rewards of the pre-
vious selections. AutoScale executes DNN inference on the

Figure 8: AutoScale design overview.

target deﬁned by the selected action ( 3(cid:13)), while observing its
result (i.e., energy, latency, and inference accuracy). Based
on the observed result, AutoScale calculates the reward ( 4(cid:13)),
which indicates how much the selected action improves en-
ergy efﬁciency and satisﬁes QoS and accuracy targets. Finally,
AutoScale updates Q-table with the calculated reward ( 5(cid:13)).
AutoScale leverages Reinforcement Learning (RL) as an
adaptive prediction mechanism. Generally, an RL agent
learns a policy to select the best action for a given state,
based on accumulated rewards [74]. In the context of mo-
bile and edge-cloud inference execution, AutoScale learns
a policy to select the optimal inference execution target for
the given NN under the presence of runtime variances, based
on the accumulated energy, latency, and accuracy results of
selections. To solve system optimization with RL, there are
three important design requirements for mobile deployments.
High Prediction Accuracy: The success of RL depends
on how much the predicted execution target is close to the
optimal one. For the accurate prediction, it is important to
correctly model the core components—State, Action, and
Reward—in a realistic environment. We deﬁne these compo-
nents based on our observations of a realistic edge inference
execution environment (Section 4.1).

In addition to the core components, it is also important to
avoid local optima. This is deeply related to a classical RL
problem, exploitation versus exploration dilemma [23, 54].
If an RL agent always exploits an action with the temporary
highest reward, it might get stuck in local optima. On the
other hand, if it keeps exploring all possible actions, the
convergence might get slower. To solve this problem, we
employ epsilon-greedy algorithm, which is one of the widely
adopted randomized greedy algorithms in this domain [64,
71, 74], due to its simplicity and effectiveness (Section 4.2).
Minimal Training Overhead: In case of RL, training
is continuously performed on-device. Due to this reason,
reducing training overhead is crucial, particularly for the
energy-constrained edge domain. As we observed in Section
3, although performance of execution targets vary across
heterogeneous devices, they share similar energy trend for
each NN. An RL model trained in a device implicitly has this
energy trend knowledge. Hence, we consider transferring a
model trained from one device for other devices to expedite
the convergence, which might reduce the training overhead.
(detailed results are presented in Section 6.3).

Low Latency Overhead: For the real-time inference exe-

cution on the energy constrained edge devices, latency over-
head is also one of the crucial factors. Among the various
forms of RL [74], such as Q-learning [12], TD-learning [60],
and deep RL [67], Q-learning has an advantage for low la-
tency overhead, as it ﬁnds the best action with a look-up table.
Hence, in this paper, we use Q-learning for AutoScale.

4.1 AutoScale RL Design

In RL, there are three core components: State, Action, and
Reward. In this section, we deﬁne the core components to
formulate the optimization space for AutoScale.

State - Based on the observations presented in Section 3,
we identify states that are critical to edge inference execution.
Table 1 summarizes the states.

As we explored in Section 3.1, the optimal execution target
depends on layer compositions of NNs. However, identifying
states with all the layer types is not desirable, since the la-
tency overhead (i.e., Q-table lookup time) increases. Hence,
we identify states with layer types that are deeply correlated
to the energy efﬁciency and performance of inference execu-
tion. We test the correlation strength between each layer type
and energy/latency by calculating the squared correlation co-
efﬁcient (ρ 2) [100]. We ﬁnd CONV, FC, and RC layers are
the most correlated to the energy efﬁciency and performance,
due to their compute- and/or memory-intensive natures. Thus,
we identify SCONV , SFC, and SRC which represent the number
of CONV, FC, and RC layers in NNs, respectively. We also
identify SMAC, the number of MAC operations to consider
heaviness of NNs.

As we explored in Section 3.2, the efﬁciency of edge in-
ference highly depends on the CPU-intensity and memory-
intensity of co-running applications. Hence, we use SCo_CPU
and SCo_MEM which represent the CPU utilization and mem-
ory usage of co-running applications, respectively. In addi-
tion, the efﬁciency of inference execution on the connected
systems highly depends on the signal strength of wireless
networks. For this reason, we use SRSSI_W and SRSSI_P which
stand for the RSSI of wireless local area network (e.g., Wi-Fi,
LTE, and 5G) and RSSI of peer-to-peer wireless network
(e.g., Bluetooth, Wi-Fi direct, etc.), respectively.

When a feature has a continuous value, it is difﬁcult to
deﬁne the state in a discrete manner for the lookup table of
Q-learning [12, 71]. To convert the continuous features into
discrete values, we applied DBSCAN clustering algorithm to
each feature [12]; DBSCAN determines the optimal number

6

State

NN-
related
features

Runtime
variances

Description
SCONV
# of CONV layers
SFC
# of FC layers
SRC
# of RC layers
SMAC
# of MAC operations
SCo_CPU
CPU utilization of co-running apps
SCo_MEM Memory usage of co-running apps
SRSSI_W
SRSSI_P

Discrete values
Small (<30), Medium (<50), Large (<90), Larger (>=90)
Small (<10), Large (>=10)
Small (<10), Large (>=10)
Small (<1000M), Medium (<2000M), Large (>=2000M)
None (0%), Small (<25%), Medium (<75%), Large (100%)
None (0%), Small (<25%), Medium (<75%), Large (100%)
RSSI of wireless local area network
Regular (>-80dBm), Weak (<=-80dBm)
RSSI of peer-to-peer wireless network Regular (>-80dBm), Weak (<=-80dBm)

Table 1: State-related features

of clusters for the given data. The last column of Table 1
summarizes discrete values for each state.

Action - Actions in reinforcement learning represent the
choosable control knobs of the system. In the context of the
edge-cloud inference execution, we deﬁne the actions as the
available execution targets. For the edge inference execution,
available processors in mobile SoCs, such as CPUs, GPUs,
DSPs, and NPUS, are deﬁned as the actions. On the other
hand, for the cloud execution, server-class processors, such
as CPUs, GPUs, and TPUs, are deﬁned as the actions.

The set of actions can be augmented to consider other con-
trol knobs, such as Dynamic Voltage and Frequency Scaling
(DVFS) and quantization. For example, as long as the QoS
constraint is satisﬁed, it is possible to reduce the frequency
of processors, saving energy. In addition, employing the
quantization for each processor can reduce both compute and
memory intensities of the inference execution, improving
energy efﬁciency and performance.

Reward - Reward in RL models the optimization objective
of the system. To represent the three important optimization
axes, we encode three different rewards, Rlatency, Renergy, and
Raccuracy. Rlatency is the measured inference latency for a
selected action (i.e., execution target for DNN inference). On
the other hand, Renergy is the estimated energy consumption
of the selected action. Raccuracy is pre-measured inference
accuracy of the given NN on each execution target.

We estimate Renergy of edge execution as follows. When
the CPU is selected as the action, Renergy is calculated using
the utilization-based CPU power model [46, 95] as in (1),
Core is the power consumed by the i-th core, t f
where Ei
busy and
tidle are the time spent in the busy state at frequency f and that
in the idle state, respectively, and P f
busy and Pidle are power
consumed during t f
busy at f and that during tidle, respectively.

table of AutoScale.

Renergy = ∑
f

(P f

busy × t f

busy) + Pidle × tidle

(2)

If scaling out the inference execution to DSPs is selected
as the action, Renergy is calculated as in (3), where PDSP is a
pre-measured power consumption of DSP; we use the con-
stant value for PDSP, since PDSP was consistent during 100
inference runs of 10 NNs.

EDSP = PDSP × Rlatency
(3)
On the other hand, if scaling out the inference execution to
connected systems is selected as the action, Renergy is calcu-
lated using the signal strength-based energy model [52] as
in (4), where tT X and tRX are data transmission latency mea-
sured while transmitting the input and receiving the output,
respectively and PS
RX are power consumed by a wire-
less network interface during tT X and tRX at signal strength
T X and PS
S, respectively. Note PS
RX values for each network
are obtained by measuring power consumption of wireless
network interfaces at each signal strength while transmitting
and receiving data, respectively.
Renergy =PS

T X and PS

T X × tT X + PS
+ Pidle × (Rlatency − tT X − tRX )

RX × tRX

(4)

Since the energy estimation is based on the measured la-
tency, the Mean Absolute Percentage Error (MAPE) of the
energy estimation is 7.3%, which is low enough to identify
the optimal action.

To make AutoScale learn and select an efﬁcient execution
decision which maximizes energy efﬁciency satisfying the
QoS and accuracy constraints, the reward R is calculated
as in (5), where α and β are the weights of latency and
accuracy, respectively; we use 0.1 for both weights, but we
can use higher weights if the inference workload requires
higher performance and accuracy.

i f Raccuracy < In f erence Quality Requirement,

Ei
Core,

Renergy = ∑
i
ECore = ∑
f

(P f

busy × t f

busy) + Pidle × tidle

(1)

R = −Raccuracy

else

Similarly, if scaling out the inference execution to GPUs
within the system is selected as the action, Renergy is is calcu-
lated using the utilization-based GPU power model [49] as
in (2). Note P f
busy and Pidle values for CPU/GPU are obtained
from procfs and sysfs in Linux kernel [48], while P f
busy and
Pidle values for CPU/GPU are obtained by measuring power
consumption of CPU/GPU at each frequency in the busy state
and that in the idle state, respectively, and stored in a look-up

i f Rlatency < QoS Constraint,

(5)

R = −Renergy + αRlatency + β Raccuracy

else

R = −Renergy + β Raccuracy

If the inference quality requirement of the selected action is
not satisﬁed, Raccuracy multiplied by -1 is used as the reward
value, to avoid choosing the target from the next inference
running. Otherwise, the reward value is calculated depending

7

Algorithm 1 Training Q-Learning Model
Variable: S, A

S is the variable for the state
A is the variable for the action

Constants: γ, µ, ε

γ is the learning rate
µ is the discount factor
ε is the exploration probability

Initialize Q(S,A) as random values
Repeat (whenever inference starts):

Observe state and store in S
if rand() < ε then

Choose action A randomly

else

Choose action A which maximizes Q(S,A)

Run inference on a target deﬁned by A
(when inference ends)
Measure Rlatency, estimate Renergy, and obtain Raccuracy
Calculate reward R
Observe new state S’
Choose action A’ which maximizes Q(S’,A’)
Q(S,A) ← Q(S,A) + γ[R + µQ(S’,A’) - Q(S,A)]
S ← S’

on whether the QoS constraint is satisﬁed or not. In (5),
Renergy is multiplied by -1, to produce higher rewards for
lower energy consumption.

4.2 AutoScale Implementation

As previously discussed, we use Q-learning for AutoScale’s
implementation due to its low runtime overhead. To deal with
the exploitation versus exploration dilemma in RL, we also
employ the epsilon-greedy algorithm for AutoScale, which
chooses the action with the highest reward or a uniformly
random action based on an exploration probability.

In Q-learning, the value function, denoted as Q(S,A), takes
State S and Action A as parameters. Q(S,A) is a form of
a look-up table, called Q-table. Algorithm 1 shows the de-
tailed algorithm for training the Q-table for on-device DNN
inference. At the beginning, the Q-table is initialized with
random values. At runtime, for each DNN inference, the
algorithm observes S by checking the NN characteristics and
runtime variances. For the given S, the algorithm evaluates a
random value compared to ε 4. If the random value is smaller
than ε, the algorithm randomly chooses A for exploration.
Otherwise, the algorithm chooses A with the largest Q(S,A).
After choosing A, the algorithm runs the inference on a target
deﬁned by A. During the inference, the algorithm measures
Rlatency and estimates Renergy, as explained in Section 4.1. In
addition, it obtains Raccuracy from the stored inference accu-
racy of the given NN on the selected execution target. Based
on these values, the algorithm calculates reward R as in (5)
of Section 4.1. After calculating the R value, the algorithm
observes new state S’ and chooses A’ for the given S’ with
the largest Q(S’,A’). The algorithm updates the Q(S,A) based
on the equation in Algorithm 1. In the equation for updating
the Q(S,A), γ and µ are hyperparameters, which represent

4Note we use 0.1 for the ε value by referring to previous RL-based
works in this domain [64, 71].

Device

Mi8Pro

Galaxy
S10e

Moto X
Force

GPU

0.7GHz w/
7 V/F steps
(2.8 W)

CPU
Cortex A75 - Adreno 630 -
2.8GHz w/
23 V/F steps
(5.5 W)
Mongoose - Mali-G76 -
2.7GHz w/
0.7GHz w/
9 V/F steps
21 V/F steps
(2.4 W)
(5.6 W)
Cortex A57 - Adreno 430 -
1.9GHz w/
15 V/F steps
(3.6 W)

0.6GHz w/
6 V/F steps
(2.0 W)

DSP

Hexagon
685 (1.8 W)

-

-

Table 2: Mobile device speciﬁcation with the peak power
consumption shown in the parenthesis.

the learning rate and the discount factor, respectively. The
learning rate indicates how much the newly acquired infor-
mation overrides the old information. On the other hand,
the discount factor gives more weight to the rewards in the
near future. We set γ and µ, based on a sensitivity test on
hyperparameters (details are explained in Section 5.3).

After the learning is completed (i.e., the largest Q(S,A)
value for each state S is converged), the Q-table is used to
select A which maximizes Q(S,A) for the observed S.

5. EXPERIMENTAL METHODOLOGY

5.1 Real System Measurement Infrastructure
We perform our experiments on three smartphones—Mi8Pro

[40], Galaxy S10e [79], and Moto X Force [69]. Table 2
summarizes their speciﬁcations5. Note we only use the smart-
phone with DSP rather than that with NPU, since 1) NPUs are
only programmable with vendor-provided Software Devel-
opment Kits (SDKs) which have not been publicly released
yet [42], and 2) DSPs in recent mobile SoCs are optimized
for DNN inference so that they can act as NPUs [42, 77].

For cloud inference execution, we connect the smartphones
to a server, equipped with an Intel Xeon CPU E5-2640 with
2.4GHz of 40 cores, NVIDIA Tesla P100 GPU, and 256 GB
of RAM, via Wi-Fi. To control the Wi-Fi signal strength, we
adjust the distance between the smartphones and Wi-Fi Ac-
cess Point (AP). For inference execution on locally connected
edge, we use a tablet, Galaxy Tab S6, equipped with 2.84GHz
of Cortex A76 CPU, Adreno 640 GPU, and Hexagon 690
DSP. We connect the smartphones to the tablet through Wi-Fi
direct, one of the Wi-Fi-based peer-to-peer wireless networks.
To control the signal strength of Wi-Fi Direct, we adjust the
distance between the locally connected devices. We measure
the system-wide power consumption of the smartphones us-
ing an external Monsoon Power Meter [68] – similar practice
is used in a number of prior works [6, 9, 75].

To execute DNN inference on diverse processors in edge-
cloud systems, we build on top of TVM [10] and SNPE [77].
TVM compiles NNs from TensorFlow/TFLite and generates
executables for edge/cloud CPUs and GPUs, whereas SNPE

5Though there exist lower-performance cores in mobile CPUs, we
only present the high-performance cores, since DNN inference usu-
ally run on the high-performance cores.

8

Workload

Image
classiﬁcation

Object
detection

DNNs
InceptionV1
InceptionV3
MobilenetV1
MobilenetV2
MobilenetV3
Resnet50
SSD MobilenetV1
SSD MobilenetV2
SSD MobilenetV3

Translation MobileBERT

SCONV
49
94
14
35
23
53
19
52
28
0

SFC
1
1
1
1
20
1
1
1
20
1

SRC
0
0
0
0
0
0
0
0
0
24

Table 3: DNN inference workloads. Layer compositions
are obtained from the TensorFlow NN implementations.

complies NNs and generates executables for mobile DSPs.
The executables are deployed onto each device with library
implementations and are used for edge inference at runtime.
To evaluate the effectiveness of AutoScale, we compare Au-
toScale to ﬁve baselines available in our edge-cloud systems—
(1) Edge(CPU FP32) which always runs DNN inference on
the CPU of the edge device, (2) Edge (Best) which runs the
inference on the most energy-efﬁcient processor of the edge
device, (3) Cloud which always runs inference on the cloud,
(4) Connected Edge which always runs inference on another
locally connected edge, and (5) Opt, an oracular design which
always runs inference on the optimal execution target.

5.2 Benchmarks and Execution Scenarios

For our evaluation, we use 10 neural networks that are
widely used in real use case scenarios [36, 78]. As summa-
rized in Table 3, each NN has different layer compositions.

To explore real use cases, we implement an Android appli-
cation. For computer vision workloads (i.e., image classiﬁca-
tion and object detection), we implement two use case sce-
narios: non-streaming and streaming. For the non-streaming
scenario, the Android application takes an image from the
camera and performs inference on the image. For this sce-
nario, short response time is important to users. Since users
cannot perceive any notable difference as long as the response
time is less than 50 ms [20, 63, 99], we use 50 ms as the QoS
target. On the other hand, for the streaming scenario, the
Android application takes a real-time video from the camera
and performs inference on the video. For this scenario, high
Frames Per Second (FPS) is important for user satisfaction.
Since users cannot perceive any difference on the QoS as long
as the FPS is greater than 30 [19, 99], we consider 30 FPS as
the QoS target. For MobileBERT in NLP, we implement one
use case scenario, where the Android application performs
the translation on a sentence typed by the keyboard. For this
scenario, we use 100 ms as the QoS target [78].

To validate the effectiveness of AutoScale in real execu-
tion environment with varying runtime variances, we run
our experiments in two execution environments—static and
dynamic. For the static environment, we perform experi-
ments in ﬁxed runtime variances (e.g., co-running apps with
constant CPU and memory usages and constant Wi-Fi and
Wi-Fi Direct signal strengths). On the other hand, for the
dynamic environment, we perform experiments with varying
runtime variances. To mimic real execution environment,
for the co-running app, we implement synthetic applications

Environment Description

Static

Dynamic

S1 No runtime variance
S2 CPU-intensive co-running app
S3 Memory-intensive co-running app
S4 Weak Wi-Fi signal strength
S5 Weak Wi-Fi direct signal strength
D1 Co-running app - music player
D2 Co-running app - web browser
D3 Random Wi-Fi signal strength

Table 4: DNN inference execution environment

based on the CPU and memory usage trace of two real-world
applications—a web browser and a music player. In addi-
tion, since the signal strength variance is usually modeled
with Gaussian distribution [16], we emulate random signal
strength with a Gaussian distribution by adjusting the band-
width limit of the Wi-Fi AP. Table 4 summarizes the DNN
inference execution environments.

5.3 AutoScale Design Speciﬁcation

Actions We determine actions of AutoScale with proces-
sors available in our edge-cloud system. Since the energy
efﬁciency of mobile CPU/GPU can be further optimized via
DVFS, we identify each Voltage/Frequency (V/F) step of
mobile CPU/GPU as the augmented action; the number of
available V/F steps is presented in Table 2. We do not con-
sider DVFS for DSP in our experiments, since DSP does not
support DVFS yet. We also identify the quantization avail-
able for each mobile processor (INT8 for CPU and DSP, and
FP16 for GPU) as the augmented action.

Hyperparameters To determine two hyperparameters—
the learning rate and the discount factor—we evaluate three
values of 0.1, 0.5, and 0.9 for each hyperparameter. We ob-
serve that higher learning rate is better, which means the
more reward is reﬂected to the Q values, the better AutoScale
works. We also observe that a lower discount factor is better.
This means that the consecutive states have a weak relation-
ship due to the stochastic nature, therefore giving less weight
to the rewards in the near future improves the efﬁciency of
AutoScale. Thus, in our evaluation, we use 0.9 and 0.1 for the
learning rate and discount factor, respectively.

Training - To cover the design space of AutoScale with
sufﬁcient training samples, we repeatedly execute inference
100 times for each NN in each runtime variance-related state
(i.e., SCO_CPU , SCO_MEM, SRSSI_W , and SRSSI_P in Table 1).
This results in a total of 64,000 training samples for AutoScale.
We analyze the training overhead in Section 6.3.

6. EVALUATION RESULTS AND ANALYSIS

6.1 Performance and Energy Efﬁciency

Figure 9 shows the average energy efﬁciency (PPW) nor-
malized to Edge (CPU FP32) and the QoS violation ratio
of DNN inference on three mobile devices at static environ-
ments. Overall, AutoScale improves the average energy efﬁ-
ciency of the DNN inference by 9.8X, 2.3X, 1.6X, and 2.7X,
compared to Edge(CPU FP32), Edge(Best), Cloud, and Con-
nected Edge, respectively. Across the diverse collection of
neural networks, AutoScale can predict the optimal execution
target to optimize the energy efﬁciency of DNN inference,

9

Figure 11: Since AutoScale accurately predicts optimal
target under the stochastic variances, it largely improves
energy efﬁciency of DNN inference in realistic environ-
ments satisfying the QoS target as much as possible.

Figure 9: AutoScale signiﬁcantly improves energy efﬁ-
ciency compared to baselines satisfying QoS constraints
as much as possible.

Figure 10: Even when the inference intensity increases
(i.e., from non-streaming to streaming) AutoScale still im-
proves energy efﬁciency substantially and shows much
lower QoS violation ratio, compared to baselines.

satisfying the QoS constraint as much as possible. AutoScale
achieves almost the same energy efﬁciency improvement as
Opt; the energy efﬁciency difference between AutoScale and
Opt is only 3.2%.

In addition, AutoScale shows signiﬁcantly lower QoS vi-
olation ratio, compared to the baselines. In fact, AutoScale
achieves almost the same QoS violation ratio with Opt; the
QoS violation ratio difference between AutoScale and Opt
is only 1.9%. For light NNs, AutoScale does not violate the
QoS constraint except for the case when CPU-intensive and
memory-intensive applications are co-running or the signal
strength of wireless networks is weak. For heavy NNs, Au-
toScale mostly rely on cloud execution so that QoS violation
occurs when the signal strength of Wi-Fi is weak.

When the inference intensity increases (i.e., streaming
scenario), the energy efﬁciency and QoS violation ratio of
AutoScale is degraded, as shown in Figure 10. Neverthe-
less, AutoScale still signiﬁcantly improves energy efﬁciency
and shows much lower QoS violation ratio, compared to the
baselines. In addition, since AutoScale accurately selects the
optimal execution target regardless of the inference inten-
sity, it achieves almost the same energy efﬁciency and QoS
violation ratio as Opt.

10

Figure 12: When AutoScale uses higher accuracy target,
its energy efﬁciency and QoS violation ratio are slightly
degraded. Nevertheless, it still signiﬁcantly improves en-
ergy efﬁciency compared to baselines.

6.2 Adaptability and Accuracy Analysis

Adaptability to Stochastic Variances: Figure 11 shows
the average energy efﬁciency normalized to Edge(CPU FP32)
and QoS violation ratio of DNN inference in the presence
of stochastic variance. The x-axis represents the inference
execution environments (Table 4). Since AutoScale accu-
rately predicts the optimal execution scaling decision even in
the presence of stochastic variance, it improves the average
energy efﬁciency of DNN inference by 10.4X, 2.2X, 1.4X,
and 3.2X, compared to Edge(CPU FP32), Edge(Best), Cloud,
and Connected Edge, respectively, showing a similar QoS
violation ratio as Opt.

Adaptability to Inference Quality Targets: Figure 12
shows the average energy efﬁciency and the QoS violation ra-
tio with different inference accuracy targets under AutoScale.
When AutoScale uses 50% as the inference accuracy target, it
chooses processors on-device with low precision where some
NN inference results in a low prediction accuracy. However,
when AutoScale uses higher inference accuracy target (i.e.,
65%), it does not choose the on-device processors with low
precision operations. Due to this reason, when AutoScale
uses higher inference accuracy target, its energy efﬁciency
and QoS violation ratio are slightly degraded. Nonetheless, it
still improves the energy efﬁciency compared to the baseline.
Prediction Accuracy: To analyze the prediction accuracy
of AutoScale, we compare the execution scaling decision se-
lected by AutoScale to the optimal one. Figure 13 shows how

Figure 13: AutoScale accurately selects the optimal exe-
cution target.

the execution scaling decision is selected by AutoScale and
Opt on three mobile devices. AutoScale accurately selects the
optimal execution scaling decision for all devices, achieving
97.9% of the prediction accuracy. AutoScale mis-predicts
the optimal execution target only when the energy difference
between optimal execution target and the (mis-predicted) sub-
optimal execution target is less than 1%. This is because of
the small error of Renergy. Although AutoScale chooses the
sub-optimal execution target for a few cases, it does not de-
grade the overall system energy efﬁciency and QoS violation
ratio by much, as compared to Opt. This is due to the small
energy difference between the optimal and sub-optimal ones.

6.3 Overhead Analysis

Training Overhead: Figure 14 shows that, when a model
is trained from scratch, the reward converges with around
40-50 inference runs. Before the reward converges, com-
pared to Opt., AutoScale shows 18.9% lower average energy
efﬁciency. Nevertheless, it still achieves 66.1% energy sav-
ing against Edge(CPU FP32). The training overhead can
be alleviated with learning transfer. As shown in Figure 14,
when the model trained on Mi8Pro is used for Galaxy S10e
and Moto X Force, the training converges more rapidly, re-
ducing the average training time overhead by 21.2%. This
result implies that AutoScale is able to capture and learn the
common characteristics across the variety of edge inference
workloads, performance and power proﬁles of edge systems,
and uncertainties from the mobile-cloud environment.

Runtime Overhead: To show viability for mobile infer-
ence deployment, we evaluate AutoScale performance over-
head. the average performance overhead of AutoScale is 10.6
µs for Q-table training, which is 0.5% of the lowest latency
of mobile DNN inference. In addition, when using the trained
Q-table, the overhead can be reduced to 7.3 µs, with only
0.3% overhead. The energy overhead is only 0.4% and 0.2%
of the total system energy consumption, respectively. The
overall memory requirement of AutoScale is 0.4MB, translat-
ing to only 0.01% of the 3GB DRAM capacity of a typical
mid-end mobile device [69].

7. RELATED WORK

With the emergence of DNN-based intelligent services, en-
ergy optimization of mobile DNN inference has been widely
studied. Due to the compute- and memory-intensive nature,
many of the early works executed DNN inference in the
cloud [11, 24, 47, 62]. As mobile systems become higher-
performing [27, 35, 41, 90], there have been increasing pushes

11

Figure 14: The reward is usually converged in 40-50 runs.
Learning transfer can improve the speed of convergence.

to execute DNN inference at the edge [8, 22, 36, 44, 46, 55,
89, 90, 92, 98]. As an intermediate stage, many techniques
tried to partition DNN inference execution between the cloud
and local mobile device [21, 22, 34, 44, 59], based on perfor-
mance/energy prediction models. However, these techniques
do not consider fully executing inference at the edge. Accord-
ing to our analysis, there exist various cases where the edge
inference execution outweighs the cloud inference execution
by removing data transmission overhead. More importantly,
the techniques also do not consider stochastic variances which
largely affect the efﬁciency of inference execution.

To fully execute DNN inference at the edge, many opti-
mizations, such as model architecture search [80, 87, 101],
quantization [13,26,43,53,55,92,97], weight compression [15,
37, 58, 61] and graph pruning [93, 96], have been proposed.
Along with these optimizations, deep learning compiler and
programming stacks have been improved to ease the adop-
tion of energy efﬁcient co-processors, such as GPUs, DSPs,
and NPUs. On top of these works, many researchers tried to
optimize the performance and/or energy efﬁciency of edge
inference execution by exploiting the co-processors along
with CPUs [8, 36, 46, 55, 89, 90, 98]. However, most of the
above techniques are based on existing prediction approaches
which are prone to being affected by stochastic variances. In
addition, the above techniques also do not consider executing
inference on connected systems, such as the cloud server or a
locally connected mobile device.

Considering uncertainties in the mobile execution environ-
ment, various energy management techniques have been pro-
posed [28, 29, 47, 52, 83]. In order to maximize the energy ef-
ﬁciency of smartphones subject to user satisfaction demands
under the memory interference, DORA takes a regression-
based predictive approach to control the settings of mobile
CPUs at runtime [83]. Gaudette et al. proposed to use ar-
bitrary polynomial chaos expansions to consider the impact
of various sources of uncertainties on mobile user experi-
ence [29]. Other works explored the use of reinforcement
learning to handle runtime variance for web browsers, for
latency-critical cloud services, and for CPUs [12, 64, 71].

To the best of our knowledge, this is the ﬁrst work that
demonstrates the potential of machine learning inference
at the edge by automatically leveraging programmable co-
processors as well as other computing resources nearby and
in the cloud. We examine a collection of machine learning-
based predictive approach and tailor-design an automatic
execution scaling engine with light-weight, customized rein-
forcement learning. AutoScale achieves near-optimal energy
efﬁciency for DNN edge inference while taking into account
stochastic variance, particularly important for user quality of
experience in the mobile domain.

8. CONCLUSION

Given the growing ubiquity of intelligent services, such
as virtual assistance, face/image recognition, and language
translation, deep learning inference is increasingly run at the
edge. To enable energy-efﬁcient inference at the edge, we
propose an adaptive and light-weight deep learning execution
scaling engine—AutoScale. The in-depth characterization of
DNN inference execution on mobile and edge-cloud systems
demonstrates that the optimal scaling decision shifts depend-
ing on various features, namely NN characteristics, desired
QoS and accuracy targets, underlying system proﬁles, and
stochastic runtime variance. AutoScale continuously learns
and selects the optimal execution scaling decision by tak-
ing into account the features and dynamically adapting to
the stochastic runtime variance. We design and construct
representative edge inference use cases and mobile-cloud ex-
ecution environment using off-the-shelf systems. AutoScale
improves the energy efﬁciency of DNN inference by an aver-
age of 9.8X and 1.6X, as compared to the baseline settings
of mobile CPU and cloud ofﬂoading, satisfying both the QoS
and accuracy constraints. We demonstrate that AutoScale is
a viable solution and will pave the path forward by enabling
future work on energy efﬁciency improvement for DNN edge
inference in a variety of realistic execution environment.

ACKNOWLEDGMENTS
This work is supported in part by the National Science Foun-
dation under #1652132 and #1525462.

REFERENCES

[1] Amazon, “Alexa.” [Online]. Available:

https://developer.amazon.com/en-US/alexa

[2] Android, “Android neural networks api.” [Online]. Available:
https://developer.android.com/ndk/guides/neuralnetworks

[3] Apple, “Siri.” [Online]. Available: https://www.apple.com/siri

[4] J. I. Benedetto, L. A. Gonzalez, P. Sanabria, A. Neyem, and J. Navon,
“Towards a practical framework for code ofﬂoading in the internet of
things,” Future Generation Computer Systems, vol. 92, pp. 424–437,
2019.

[5] S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Bechmark

analysis of representative deep nerual network architectures,” IEEE
Access, vol. 6, no. 1, pp. 64 270–64 277, 2018.

[6] W. L. Bircher and L. K. John, “Complete system power estimation:

A trickle-down approach based on performance events,” in
Proceedings of the International Symposium on Performance
Analysis of Systems and Software (ISPASS), 2007, pp. 171–180.

[7] Caffe2, “A new lightweight, modular, and scalable deep learning

framework.” [Online]. Available: https://caffe2.ai

[8] E. Cai, D.-C. Juan, D. Stamoulis, and D. Maculescu, “Neuralpower:
Predict and deploy energy-efﬁcient convolutional neural networks,”
in Proceedings of the Asian Conference on Machine Learning
(ACML), 2017.

[9] A. Caroll and G. Heiser, “An analysis of power consumption in a
smartphone,” in Proceedings of the USENIX Annual Technical
Conference, 2010.

[10] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, M. Cowan, H. Shen,
L. Wang, Y. Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy, “Tvm:
An automated end-to-end optimizing compiler for deep learning,” in
Proceedings of the USENIX Symposium on Operating Systems
Design and Implementation, 2018, pp. 579–594.

[11] Y. Chen, J. Hen, X. Zhang, C. Hao, and D. Chen, “Cloud-dnn: An
open framework for mapping dnn models to cloud fpgas,” in
Proceedings of the International Symposium on Field-Programmable
Gate Arrays (FPGA), 2019, pp. 73–82.

12

[12] Y. Choi, S. Park, and H. Cha, “Optimizing energy efﬁciency of

browsers in energy-aware scheduling-enabled mobile devices,” in
Proceedings of the International Conference on Mobile Computing
and Networking (MobiCom), 2019.

[13] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks: Training deep neural networks with
weights and activations constrained to +1 or -1,”
arXiv:1602.02830v3, 2016.

[14] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei,

“Imagenet: A large-scale hierarchical image database,” in
Proceedings of the IEEE Computer Vision and Pattern Recognition
(CVPR), 2009.

[15] C. Ding, Y. Wang, N. Liu, Y. Zhuo, C. Wang, X. Qian, Y. Bai,

G. Yuan, X. Ma, Y. Zhang, J. Tang, Q. Qiu, X. Lin, and B. Yuan,
“Circnn: Accelerating and compressing deep neural networks using
block-circulantweight matrices,” in Proceedings of the International
Symposium on Microarchitecture (MICRO), 2017, pp. 395–408.

[16] N. Ding, D. Wagner, X. Chen, A. Pathak, Y. C. Hu, and A. Rice,

“Characterizing and modeling the impact of wireless signal strength
on smartphone battery drain,” in Proceedings of the International
Conference on Measurement and Modeling of Computer Systems
(SIGMETRICS), 2013, pp. 29–40.

[17] B. Donyanavard, A. Sadighi, T. Muck, F. Maurer, A. M. Rahmani,
A. Herkersdorf, and N. Dutt, “Sosa: Self-optimizing learning with
self-adaptive control for hierarchical system-on-chip management,”
in Proceedings of the International Symposium on Microarchitecture
(MICRO), 2019, pp. 685–698.

[18] H. Drucker, C. Burges, L. Kaufman, A. Smola, and V. Vapnik,

“Support vector regression machines,” in Proceedings of Advances in
Neural Information Processing Systems (NIPS), 1997.

[19] B. Egilmez, M. Schuchhardt, G. Memik, R. Ayoub,

N. Soundararajan, and M. Kishinevsky, “User-aware frame rate
management in android smartphones,” ACM Transactions on
Embedded Computing Systems, vol. 16, no. 5s, pp. 1–17, 2017.

[20] Y. Endo, Z. Wang, J. B. Chen, and M. I. Seltzer, “Using latency to

evaluate interactive system performance,” in Proceedings of the
USENIX Symposium on Operating Systems Design and
Implementation (OSDI), 1996.

[21] A. E. Eshratifar, M. S. Abrishami, and M. Pedram, “Jointdnn: An
efﬁcient training and inference engine for intelligent mobile cloud
computing services,” IEEE Transactions on Mobile Computing,
2020.

[22] A. E. Eshratifar and M. Pedram, “Energy and performance efﬁcient
computation ofﬂoading for deep neural networks in a mobile cloud
computing environment,” in Proceedings of Great Lakes Symposium
on VLSI (GVLSI), 2018, pp. 111–116.

[23] E. Even-Dar, S. Mannor, and Y. Mansour, “Action elimination and
stopping conditions for the multi-armed bandit and reinforcement
learning problems,” Journal of Machine Learning Research, vol. 7,
pp. 1079–1105, 2006.

[24] Z. Fang, T. Yu, O. J. Mengshoel, and R. K. Gupta, “Qos-aware

scheduling of heterogeneous servers for inference in deep neural
networks,” in Proceedings of the ACM Conference on Information
and Knowledge Management (CIKM), 2017, pp. 2067–2070.

[25] Fitbit, “Fitbit ﬂex 2.” [Online]. Available:

https://www.ﬁtbit.com/in/ﬂex2

[26] J. Fromm, M. Cowan, M. Philipose, L. Ceze, and S. Patel, “Riptide:
Fast end-to-end binarized neural networks,” in Proceedings of
Machine Learning and Systems (MLSys), 2020.

[27] C. Gao, A. Gutierrez, M. Rajan, R. Dreslinski, T. Mudge, and C.-J.

Wu, “A study of mobile device utilization,” in Proceedings of the
International Symposium on Performance Analysis of Systems and
Software (ISPASS), 2015.

[28] B. Gaudette, C.-J. Wu, and S. Vrudhula, “Improving smartphone user
experience by balancing performance and energy with probabilistic
qos guarantee,” in Proceedings of the International Symposium on
High Performance Computer Architecture (HPCA), 2016.

[29] B. Gaudette, C.-J. Wu, and S. Vrudhula, “Optimizing user

satisfaction of mobile workloads subject to various sources of
uncertainties,” IEEE Transactions on Mobile Computing, vol. 18,
no. 12, pp. 2941–2953, 2019.

[30] Google, “Google cardboard.” [Online]. Available:

https://arvr.google.com/cardboard/

[31] Google, “Google cloud vision.” [Online]. Available:

https://cloud.google.com/vision

[32] Google, “Google daydream.” [Online]. Available:

https://arvr.google.com/daydream/

[33] Google, “Google translate.” [Online]. Available:

https://translate.google.com

[34] T. Guo, “Cloud-based or on-device: An empirical study of mobile

deep inference,” in Proceedings of International Conference on
Cloud Engineering (IC2E), 2018, pp. 184–190.

[35] M. Halpern, Y. Zhu, and V. J. Reddi, “Mobile cpu’s rise to power:

Quantifying the impact of generational mobile cpu design trends on
performance, energy, and user satisfaction,” in Proceedings of the
IEEE International Symposium on High Performance Computer
Architecture (HPCA), 2016, pp. 64–76.

[36] M. Han, J. Hyun, S. Park, J. Park, and W. Baek, “Mosaic:

Heterogeneity-, communication-, and constraint-aware model slicing
and execution for accurate and efﬁcient inference,” in Proceedings of
the International Conference on Parallel Architecture and
Compilation Techniques (PACT), 2019, pp. 165–177.

[37] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” in Proceedings of the International Conference on Learning
Representations (ICLR), 2016.

[38] J. Hauswald, Y. Kang, M. A. Laurenzano, Q. Chen, C. Li, T. Mudge,

R. G. Dreslinski, J. Mars, and L. Tang, “Djinn and tonic: Dnn as a
service and its implications for future warehouse scale computers,” in
Proceedings of the IEEE International Symposium on Computer
Architecture (ISCA), 2015.

[39] C. Healthcare, “The intelligent healthcare platform.” [Online].

Available:
https://www.changehealthcare.com/about/innovation/intelligent-
healthcare-platform

[40] Huawei, “Kirin 980, the world’s ﬁrst 7nm process mobile ai chipset.”

[Online]. Available:
https://consumer.huawei.com/en/campaign/kirin980/

[41] L. N. Huynh, Y. Lee, and R. K. Balan, “Deepmon: Mobile gpu-based
deep learning framework for continuous vision applications,” in
Proceedings of the International Conference on Mobile Systems,
Applications, and Services (MobiSys), 2017, pp. 82–95.

[42] A. Ignatov, R. Timofte, W. Chou, K. Wang, T. Hartley, and L. V.
Gool, “Ai benchmark: Running deep nerual networks on android
smartphones,” arXiv:1810.01109, 2018.

[43] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,

H. Adam, and D. Kalenichenko, “Quantization and training of neural
networks for efﬁcient integer-arithmetic-only inference,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.

[44] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and
L. Tang, “Neurosurgeon: Collaborative intelligence between the
cloud and mobile edge,” in Proceedings of the International
Conference on Architectural Support for Programming Languages
and Operating Systems (ASPLOS), 2017, pp. 615–629.

[45] A. Karpathy, G. Toderici, S. Shetty, T. Leung, A. Sukthankar, and

L. Fei-fei, “Large-scale video classiﬁcation with convolutional neural
networks,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014, pp. 1725–1732.

[46] Y. Kim, J. Kim, D. Chae, D. Kim, and J. Kim, “ulayer: Low latency
on-device inference using cooperative single-layer acceleration and
processor-friendly quantization,” in Proceedings of the European
Conference on Computer Systems (EuroSys), 2019.

[47] Y. G. Kim and S. W. Chung, “Signal strength-aware adaptive

ofﬂoading for energy efﬁcient mobile devices,” in Proceedings of the
International Symposium on Low Power Electronics and Design
(ISLPED), 2017, pp. 1–6.

[48] Y. G. Kim, M. Kim, and S. W. Chung, “Enhancing energy efﬁciency
of multimedia applications in heterogeneous mobile multi-core
processors,” IEEE Transactions on Computers, vol. 66, no. 11, pp.
1878–1889, 2017.

[49] Y. G. Kim, M. Kim, J. M. Kim, M. Sung, and S. W. Chung, “A novel

13

gpu power model for accurate smartphone power breakdown,” ETRI
Journal, vol. 37, no. 1, pp. 157–164, 2015.

[50] Y. G. Kim, M. Kim, J. Kong, and S. W. Chung, “An adaptive thermal

management framework for heterogeneous multi-core processors,”
IEEE Transactions on Computers, published online.

[51] Y. G. Kim, J. Kong, and S. W. Chung, “A survey on recent os-level
energy management techniques for mobile processing units,” IEEE
Transactions on Parallel and Distributed Systems, vol. 29, no. 10, pp.
2388–2401, 2018.

[52] Y. G. Kim, Y. S. Lee, and S. W. Chung, “Signal strength-aware
adaptive ofﬂoading with local image preprocessing for energy
efﬁcient mobile devices,” IEEE Transactions on Computers, vol. 69,
no. 1, pp. 99–101, 2020.

[53] J. H. Ko, D. Kim, T. Na, J. Kung, and S. Mukhopadhyay, “Adaptive
weight compression for memory-efﬁcient neural networks,” in
Proceedings of Design, Automation, and Test in Europe Conference
(DATE), 2017.

[54] D. E. Koulouriotis and A. Xanthopoluos, “Reinforcement learning
and evolutionary algorithms for non-stationary multi-armed bandit
problems,” Applied Mathematics and Computation, vol. 196, pp.
913–922, 2008.

[55] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, L. Jiao,
L. Qendro, and F. Kawsar, “Deepx: A software accelerator for
low-power deep learning inference on mobile devices,” in
Proceedings of the International Conference on Information
Processing in Sensor Networks (IPSN), 2016, pp. 98–107.

[56] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, “Convolutional deep

belief networks for scalable unsupervised learning of hierarchical
representations,” in Proceedings of the International Conference on
Machine Learning (ICML), 2009, pp. 609–616.

[57] S.-Y. Lee and C.-J. Wu, “Performance characterization, prediction,

and optimization for heterogeneous systems with multi-level memory
interference,” in Proceedings of the IEEE International Symposium
on Workload Characterization (IISWC), 2017, pp. 43–53.

[58] D. Li, X. Wang, and D. Kong, “Deeprebirth: Accelerating deep

neural network execution on mobile devices,” in Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018.

[59] G. Li, L. Liu, X. Wang, X. Dong, P. Zhao, and X. Feng, “Auto-tuning

neural network quantization framework for collaborative inference
between the cloud and edge,” in Proceedings of International
Conference on Artiﬁcial Neural Networks (ICANN), 2018, pp.
402–411.

[60] X. Lin, Y. Wang, and M. Pedram, “A reinforcement learning-based

power management framework for green computing data centers,” in
Proceedings of the International Conference on Cloud Engineering
(IC2E), 2016, pp. 135–138.

[61] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, and J. Du, “On-demand deep
model compression for mobile devices: A usage-driven model
selection framework,” in Proceedings of the International Conference
on Mobile Systems, Applications, and Services (MobiSys), 2018.

[62] Y. Liu, Y. Wang, R. Yu, M. Li, V. Sharma, and Y. Wang, “Optimizing
cnn model inference on cpus,” in Proceedings of the USENIX Annual
Technical Conference, 2019, pp. 1025–1039.

[63] D. Lo, T. Song, and G. E. Suh, “Prediction-gudided

performance-energy trade-off for interactive applications,” in
Proceedings of the International Symposium on Microarchitecture
(MICRO), 2015, pp. 508–520.

[64] S. K. Mandal, G. Bhat, J. R. Doppa, P. P. Pande, and U. Y. Ogras,

“An energy-aware online learning framework for resource
management in heterogeneous platforms,” ACM Transactions on
Design Automation and Electronic Systems, 2020.

[65] Microsoft, “Azure artiﬁcial intelligence.” [Online]. Available:

https://azure.microsoft.com/en-us/free/ai/

[66] Microsoft, “Hololens2.” [Online]. Available:

https://www.microsoft.com/en-us/hololens

[67] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.

Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
S. Petersen, C. Beattile, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wiersta, S. Legg, and D. Hassabis, “Human-level
control thourgh deep reinforcement learning,” Nature, vol. 518, no.
7540, pp. 529–533, 2015.

[68] Monsoon, “High voltage power monitor.” [Online]. Available:

https://www.msoon.com/high-voltage-power-monitor

https://www.statista.com/statistics/490231/wearable-devices-
worldwide-by-region/

[86] J. A. K. Suykens and J. Vandewalle, “Least squares support vector

machine classiﬁers,” Neural Processing Letters, vol. 9, pp. 293–300,
1999.

[87] M. Tan and Q. V. Le, “Efﬁcientnet: Rethinking model scaling for
convolutional neural networks,” arXiv:1905.11946, 2019.

[88] Tensorﬂow, “An end-to-end open source machine learning platform.”

[Online]. Available: https://www.tensorﬂow.org/

[89] S. Wang, G. Ananthanarayanan, Y. Zeng, N. Goel, A. Pathania, and
T. Mitra, “High-throughput cnn inference on embedded arm big.little
multi-core processors,” IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems, published online.

[90] S. Wang, A. Pathania, and T. Mitra, “Neural network inference on

mobile socs,” IEEE Design & Test, published online.

[91] Withings, “The world’s ﬁrst analog watch with a built-in

electrocardiogram to detect atrial ﬁbrillation.” [Online]. Available:
https://www.withings.com/us/en/move-ecg?utm_source=CJ&utm_
medium=Afﬁliate&utm_campaign=afﬁliation-
Skimlinks&utm_content=7099101-Home+Page+US-
13184200&CJEVENT=3669c89e7f8511ea83c700830a1c0e13

[92] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan,
K. Hazelwood, E. Isaac, Y. Jia, B. Jia, T. Leyvand, H. Lu, Y. Lu,
L. Qiao, B. Reagen, J. Spisak, F. Sun, A. Tulloch, P. Vajda, X. Wang,
Y. Wang, B. Wasti, Y. Wu, R. Xian, S. Yoo, and P. Zhang, “Machine
learning at facebook: Understanding inference at the edge,” in
Proceedings of the IEEE International Symposium on High
Performance Computer Architecture (HPCA), 2019, pp. 331–344.

[93] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke,
“Scalpel: Customizing dnn pruning to the underlying hardware
parallelism,” in Proceedings of the IEEE International Symposium on
Computer Architecture (ISCA), 2017, pp. 548–560.

[94] B. Zhang and S. N. Srihari, “Fast k-nearest neighbor classiﬁcation
using cluster-based trees,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 26, no. 4, pp. 525–528, 2004.

[95] L. Zhang, B. Tiwana, Z. Qian, Z. Wang, R. P. Dick, Z. M. Mao, and

L. Yang, “Accurate online power estimation and automatic battery
behavior based power model generation for smartphones,” in
Proceedings of the International Conference on Hardware/Software
Codesign and System Synthesis, 2010, pp. 105–114.

[96] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and Y. Wang,
“A systematic dnn weight pruning framework using alternating
direction method of multipliers,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 184–199.

[97] R. Zhao, Y. Hu, J. Dotzel, C. D. Sa, and Z. Zhang, “Improving neural
network quantization without retraining using outlier channel
splitting,” in Proceedings of the International Conference on
Machine Learning (ICML), 2019.

[98] G. Zhong, A. Dubey, C. Tan, and T. Mitra, “Synergy: An hw/sw
framework for high throughput cnns on embedded heterogeneous
soc,” ACM Transactions on Embedded Computing Systems, vol. 18,
no. 2, pp. 1–23, 2019.

[99] Y. Zhu, M. Halpern, and V. J. Reddi, “Event-based scheduling for
energy-efﬁcient qos (eqos) in mobile web applications,” in
Proceedings of the IEEE International Symposium on High
Performance Computer Architecture (HPCA), 2015, pp. 137–149.

[100] Y. Zhu and V. J. Reddi, “High-performance and energy-efﬁcient

mobile web browsing on big/little systems,” in Proceedings of the
IEEE International Symposium on High Performance Computer
Architecture (HPCA), 2013, pp. 13–24.

[101] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning

transferable architectures for scalable image recognition,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018, pp. 8697–8710.

[69] Motorola, “Moto x force - technical specs.” [Online]. Available:

https://support.motorola.com/uk/en/solution/MS112171

[70] MXNet, “A ﬂexible and efﬁcient library for deep learning.” [Online].

Available: https://mxnet.apache.org/

[71] R. Nishtala, P. Carpenter, V. Petrucci, and X. Martorell, “Hipster:
Hybrid task manager for latency-critical cloud workloads,” in
Proceedings of the International Symposium on High Performance
Computer Architecture (HPCA), 2017, pp. 409–420.

[72] Oculus, “Turn the world into your arcade.” [Online]. Available:

https://www.oculus.com/?locale=en_US

[73] Omron, “Healthguide - blood pressure monitoring anytime,

anywhere.” [Online]. Available:
https://omronhealthcare.com/products/heartguide-wearable-blood-
pressure-monitor-bp8000m/

[74] S. Pagani, S. Manoj, A. Jantsch, and J. Henkel, “Machine learning

for power, energy, and thermal management on multicore processors:
A survey,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 39, no. 1, pp. 101–116, 2020.

[75] D. Pandiyan and C.-J. Wu, “Quantifying the energy cost of data

movement for emerging smart phone workloads on mobile platforms,”
in Proceedings of the IEEE International Symposium on Workload
Characterization (IISWC), 2014, pp. 171–180.

[76] PyTorch, “From research to production: An open source machine
learning framework that accelerates the path from research
prototyping to production deployment.” [Online]. Available:
https://pytorch.org/

[77] Qualcomm, “Snapdragon neural processing engine sdk.” [Online].

Available:
https://developer.qualcomm.com/docs/snpe/overview.html

[78] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J.
Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka,
C. Coleman, S. Davis, G. Diamos, J. Duke, D. Fick, J. S. Gardner,
I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar,
D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Mcikevicius,
C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeria, A. Sirasao,
F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada,
B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou, “Mlperf inference
benchmark,” arXiv:1911.02549, 2019.

[79] Samsung, “Samsung galaxy s10e, s10, & s10+.” [Online]. Available:

https://www.samsung.com/global/galaxy/galaxy-s10

[80] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2018.

[81] G. A. F. Seber and A. J. Lee, Linear Regression Analysis, 2nd ed.

John Wiley & Sons, 2012.

[82] H. Shen, Y. Tan, J. Lu, Q. Wu, and Q. Qiu, “Achieving autonomous

power management using reinforcement learning,” ACM
Transactions on Design Automation of Electronic Systems, vol. 18,
no. 2, pp. 1–32, 2013.

[83] D. Shingari, A. Arunkumar, B. Gaudette, S. Vrudhula, and C.-J. Wu,

“Dora: Optimizing smartphone energy efﬁciency and web browser
performance under interference,” in Proceedings of the IEEE
International Symposium on Performance Analysis of Systems and
Software (ISPASS), 2018, pp. 64–75.

[84] Statista, “Forecast number of mobile users worldwide from 2019 to

2023.” [Online]. Available:
https://statista.com/statistics/218984/number-of-global-mobile-
users-since-2010

[85] Statista, “Number of connected wearable devices worldwide by

region from 2015 to 2022.” [Online]. Available:

14

