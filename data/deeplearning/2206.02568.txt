2
2
0
2

n
u
J

3

]

C
O
.
h
t
a
m

[

1
v
8
6
5
2
0
.
6
0
2
2
:
v
i
X
r
a

A DEEP REINFORCEMENT LEARNING FRAMEWORK FOR
COLUMN GENERATION

A PREPRINT

Cheng Chi
Department of Mechanical & Industrial Engineering
University of Toronto
cheng.chi@mail.utoronto.ca

Amine Mohamed Aboussalah
Department of Mechanical & Industrial Engineering
University of Toronto
amine.aboussalah@mail.utoronto.ca

Elias B. Khalil
Department of Mechanical & Industrial Engineering
University of Toronto
elias.khalil@utoronto.ca

Juyoung Wang
Department of Mechanical & Industrial Engineering
University of Toronto
juyoung.wang@mail.utoronto.ca

Zoha Sherkat-Masoumi
Department of Mechanical & Industrial Engineering
University of Toronto
zoha.sherkatmasoumi@mail.utoronto.ca

June 7, 2022

ABSTRACT

Column Generation (CG) is an iterative algorithm for solving linear programs (LPs) with an extremely
large number of variables (columns). CG is the workhorse for tackling large-scale integer linear
programs, which rely on CG to solve LP relaxations within a branch and bound algorithm. Two
canonical applications are the Cutting Stock Problem (CSP) and Vehicle Routing Problem with
Time Windows (VRPTW). In VRPTW, for example, each binary variable represents the decision
to include or exclude a route, of which there are exponentially many; CG incrementally grows the
subset of columns being used, ultimately converging to an optimal solution. We propose RLCG, the
ﬁrst Reinforcement Learning (RL) approach for CG. Unlike typical column selection rules which
myopically select a column based on local information at each iteration, we treat CG as a sequential
decision-making problem, as the column selected in an iteration affects subsequent iterations of the
algorithm. This perspective lends itself to a Deep Reinforcement Learning approach that uses Graph
Neural Networks (GNNs) to represent the variable-constraint structure in the LP of interest. We
perform an extensive set of experiments using the publicly available BPPLIB benchmark for CSP and
Solomon benchmark for VRPTW. RLCG converges faster and reduces the number of CG iterations by
22.4% for CSP and 40.9% for VRPTW on average compared to a commonly used greedy policy.

Keywords Column generation · Machine learning for combinatorial optimization · Reinforcement learning

1

Introduction

Machine Learning (ML) for Mathematical Optimization (MO) is a growing ﬁeld with a wide range of recent studies
that enhance optimization algorithms by embedding ML in them to replace human-designed heuristics Bengio et al.
[2021]. Previous work has overwhelmingly focused on combinatorial or integer programming problems/algorithms
for which all decision variables are explicitly given in advance. For example, in a knapsack problem, each binary

 
 
 
 
 
 
arXiv Template

A PREPRINT

variable represents the decision to include or exclude an item in the knapsack; even with hundreds of thousands of items,
modern integer programming solvers or heuristics can assign values to some or all variables simultaneously without any
memory issues. However, there are many optimization problems in which there are more decision variables than one
could ever explicitly deal with. In VRPTW, for example, each binary variable represents the decision to include or
exclude a route, of which there are exponentially many.

Column generation (CG) is an algorithm for solving linear programs (LPs) with a prohibitively large number of variables
(columns) Desaulniers et al. [2006]. It leverages the insight that in an optimal solution to LP, only a few variables will
be active. In each iteration of CG, a column with negative reduced cost is added to a Restricted Master Problem (RMP,
where “restricted" refers to the use of only a subset of columns) until no more columns with negative reduced cost
are found. When that occurs, CG will have provably converged to an optimal solution to the LP. To solve an integer
linear program, CG can be used as an LP relaxation solver within the so-called branch and price algorithm. In this
work, we will focus on CG as an algorithm for solving large-scale linear programs; our conclusion section discusses the
straightforward application of our method for the integer case.

A commonly used heuristic rule is to greedily select the column with the most negative reduced cost in each iteration.
However, is this always the “optimal" column to add if one is interested in converging in as few iterations as possible?
Could ML-guided column selection, based on the structure of the current instance being solved, make better selection
decisions that speed up the convergence of CG? These are the questions we tackle in this work. We adopt the standard
view that one is interested in solving problem instances that have the same mathematical formulation but whose data
(objective function coefﬁcients, constraint coefﬁcients, num. of vehicles, etc.) differ and are drawn from the same
distribution. In VRPTW, for example, the geographical locations of customers that must be served by the vehicles may
vary, as do the corresponding service time windows.

To tackle these questions, we propose a ML framework to accelerate convergence of the CG algorithm. In particular,
we utilize RL to select columns for LPs with many variables, where the state of the CG algorithm and the structure
of the LP instance are encoded using GNNs that operate on a bipartite graph representation of the variable-constraint
interactions Gasse et al. [2019]. Q-learning is used to derive a column selection policy that minimizes the total number of
iterations through an appropriately designed reward function. Our premise is that by directly optimizing for convergence
speed using RL on similar LP instances from the same problem (e.g., CSP, VRPTW), one can outperform traditional
heuristics such as the greedy rule. Our contributions can be summarized as follows:

1. CG as a sequential task: We bring forth the ﬁrst integration of RL and CG which appropriately captures the
sequential decision-making nature of CG algorithm. Our RL agent, RLCG learns a Q-function which takes the
future rewards (namely, total number of iterations) into consideration and therefore can make better column
selections at each step. In contrast with prior work on RL for mathematical optimization, ours is, to our
knowledge, the ﬁrst to tackle the very widely applicable class of problems with exponentially many variables.

2. Curricula for learning over a diverse set of instances: In practice, instances of the same optimization
problem may vary not only in their data, but also their size/complexity. For example, in the CSP, instances may
vary in the number of rolls that must be cut. To enable efﬁcient RL over a widely varying instance distribution,
we show how a curriculum can be designed for a given dataset of instances with a minimal amount of domain
knowledge.

3. Evidence of substantial improvements over existing myopic heuristics: We evaluate RLCG on the CSP,
which is known as the representative problem in this domain Ben Amor and Valerio de Carvalho [2004],
Desaulniers et al. [2006], and the VRPTW, another widely studied and applied problem. We compare RLCG
with the commonly used greedy column selection strategy and an expensive, integer programming based
one-step lookahead strategy described by Morabit et al. [2021]. Our algorithm converges faster than both
of these in terms of number of iterations and total time. Our results show the value of considering CG as a
sequential decision-problem and optimizing the entire solving trajectory through RL.

2 Related Work

Work on the use of RL to guide iterative algorithms can be traced back to Zhang and Dietterich [1995], who used RL
for a scheduling problem. More recently, Dai et al. [2017] and Bello et al. [2016] proposed deep RL for constructing
solutions to graph optimization problems; the survey by Mazyavkina et al. [2021] summarizes subsequent advances in
this space. Dai et al. [2017] were the ﬁrst to use GNNs in this setting, a line of work that has also grown substantially
(e.g., Cappart et al. [2021]) including in integer programming Gasse et al. [2019]. Relatedly, Tang et al. [2020] apply

2

arXiv Template

A PREPRINT

RL to the cutting plane algorithm for integer linear programming, where a policy that selects “good" Gomory cuts is
derived using evolutionary strategies. Within the framework of Constraint Programming, Cappart et al. [2020] present
a deep RL approach for branching variable selection within an exact algorithm. Nonetheless, to our knowledge, the
sequential decision-making perspective has not been leveraged in the context of column generation or integer/linear
programming with many variables, a setting which is relevant to many practical applications such as resource allocation
(e.g., CSP), routing problems (e.g., VRPTW), and airline crew scheduling Barnhart et al. [2003].

The closest work to ours is that of Morabit et al. [2021]. They formulate the column selection in each CG iteration as a
column classiﬁcation task where the label of each column (select or not) is given by an “expert". This “expert" performs
a one-step lookahead to identify the column which maximally improves the LP value of the Restricted Master Problem
(RMP) of the next iteration. This is done with an extremely time-consuming mixed-integer linear program (MILP). The
RMPs are encoded using bipartite graphs with columns nodes (v) and constraint nodes (c), where an edge between v
and c in the graph indicates the contribution of a column v to a constraint c. Each node of the graph is then annotated
with additional useful information for column selection stored as node features. Morabit et al. [2021] then use a GNN
to imitate the node selection of the expert using supervision, similarly to what was done for branching in Gasse et al.
[2019].

In contrast, our work treats the column generation as a sequential decision-making problem and utilizes RL to select a
column at each iteration of CG. Our GNN acts as a Q-function approximator that maximizes the total future expected
reward. As such, our work focuses on directly reducing the total number of CG iterations, whereas Morabit et al. [2021]
derive a classiﬁer that does not consider the interdependencies across iterations, treating them as independent. One
approach to accelerate CG further is to add multiple columns per CG iteration Desaulniers et al. [1999]; we discuss this
extension in the conclusion section.

3 Preliminaries: Column Generation

We will use the canonical Cutting Stock Problem (CSP) to describe the CG method as is typically done in textbooks on
the topic Desaulniers et al. [2006]. CSP is a general resource allocation problem where the objective is to subdivide a
given quantity of a resource into a number of predetermined allocations to meet certain demands so that the total usage
of the resources (e.g., total number of size-ﬁxed paper rolls) is minimized. Such minimization is achieved by ﬁnding a
set of optimal divisions of each resource, or in other words, using a set of optimal cutting patterns to divide resources.
Due to the CSP’s combinatorial nature and its exponentially large set of possible patterns (variables), CG is used to
solve the LP relaxation of CSP iteratively without explicitly enumerating all possible patterns.

The CSP formulation and column generation algorithm we use is a common modiﬁcation of Gilmore and Gomory
[1961]. The set of all feasible patterns P that can be cut from a roll is deﬁned as:

(cid:26)

P =

xk ∈ Nn :

n
(cid:88)

i=1

aixik ≤ L, xik ≥ 0 ∀i ∈ {1, 2, . . . , n}, ∀k ∈ {1, 2, . . . , |P|}

.

(cid:27)

where each pattern p ∈ P is represented using a vector xk ∈ Nn. With ai being a possible cut length from a roll with
length L, each element of xk speciﬁes how many such cuts with length ai are included in pattern p. For instance,
assume the length of the resource roll L is 4m, so all possible ais are 1m, 2m, 3m and 4m, and then one possible cutting
pattern p is represented by xk = (0, 2, 0, 0). Let λp be the number of times pattern p is used. The formulation with λp
being a decision variable is:

min
λ∈N|P|

(cid:26) (cid:88)

(cid:88)

λp :

p∈P

p∈P

xipλp = di ∀i ∈ {1, 2, . . . , n}

,

(cid:27)

where the objective function minimizes the total number of patterns used, which is equivalent to minimizing the number
of rolls used. The constraints ensure demand is met, while enforcing the integrality restriction on λp.

This problem has an extremely large number of decision variables as P is exponentially large. Therefore, the problem
is decomposed into the Restricted Master Problem (RMP) and the Sub-Problem (SP). The RMP is obtained by relaxing
the integrality restrictions on λp with an initial set ˜P where ˜P ⊂ P. The RMP formulation of the cutting stock problem
is deﬁned as follows:

min
λ∈N| ˜P|

(cid:26) (cid:88)

p∈ ˜P

λp :

(cid:88)

p∈ ˜P

xipλp = di ∀i ∈ {1, · · · , n}, λp ≥ 0 ∀p ∈ ˜P

(cid:27)

.

3

arXiv Template

A PREPRINT

The SP formulation of the provided cutting stock problem is deﬁned as follows:

(cid:40) n
(cid:88)

i=1

min
x∈Nn

πixi :

n
(cid:88)

i=1

(cid:41)

aixi ≤ L

,

where πi is the dual value associated with the demand constraints. The SP is used to generate a pattern p represented by
vector x ∈ Nn with the most negative reduced cost, then adding that p to ˜P in the next iteration. Below, we provide an
overview of the column generation algorithm, noting that our method will intervene in Step 4 to make a potentially
non-greedy column selection decision:

1. Solve the RMP to obtain λ(cid:63) and ¯π;
2. Update the SP objective function using ¯π;
3. Solve SP to obtain x(cid:63)
i ;
4. if 1 − (cid:80)n

i=1 ¯πix(cid:63)

i ≤ 0, add the column associated with x(cid:63)

i and return to step 1, else Stop.

4 The RLCG Framework

For the sake of brevity, we refer to our RL-aided column selection strategy as RLCG. At a high level, the method works
as follows. We assume that the sub-problem (SP) is solved at each iteration and a set of near-optimal column candidates
G is returned, which is a general feature of optimization solvers such as Gurobi. While the greedy CG algorithm, as
described in section 3, adds the single column with the most negative reduced cost from G to the RMP in the next
iteration, RLCG selects columns from G according to the Q-function learned by the RL agent. The RL agent is fused
within the CG loop and actively selects the column to be added to the next iteration using the information extracted
from the current RMP and SP. An illustration comparing CG and RLCG is provided in Figure 1 below.

Figure 1: CG vs RLCG framework

We can interpret the CG solving process as the environment for RL agent, and RL can extract information (state) from
that environment and act upon it by choosing columns to add during CG solving process.

4.1 Formulating CG as MDP

We formulate CG as a Markov decision process (MDP). As is customary, we use (S, A, T , r, γ) to denote our MDP,
where S is the state space, A the action space, T : S × S × A → [0, 1], (s(cid:48), s, a) (cid:55)→ P(s(cid:48)|s, a) the transition function,
r : S × A × A → R the reward function, and γ ∈ (0, 1) the discount factor. We train the RL agent with experience
replay [Mnih et al., 2015].

4

arXiv Template

A PREPRINT

4.1.1 State space S

The state represents the information that the agent has about the environment. In RLCG, the environment is the CG
solution process corresponding for a given problem instance. As shown in Figure 1, the information passed to the RL
agent is the bipartite graph of the current CG iteration from the RMP and the candidate columns from the SP. At each
iteration, the RMP is an LP as shown in section 3. As introduced in Gasse et al. [2019], such an LP is encoded using a
bipartite graph with two node types: column nodes V and constraint nodes C. An edge (v, c) exists between a node
v ∈ V and a node c ∈ C if column v contributes to constraint c. An example bipartite graph representation of the state
is shown in left of Figure 2 with column nodes shown on the left hand side, the constraint nodes on the right hand side,
and the action nodes, which are the candidate columns returned from the SP, are shown in green (e.g., v6, v7).

To incorporate richer information about the current CG iteration, we include node features for both column nodes and
constraint nodes (vf and cf next to the nodes). We designed 9 column node features and 2 constraint node features in
our environment based on our previous experience with CG and inspiration from Morabit et al. [2021]. These node
features are described in the appendix G.

Therefore, the state space S is the space of bipartite graphs representing all possible RMP from cutting stock instances
following distribution D with node features given above. The bipartite graph shown in the left of Figure 2 is a particular
state s in S. As states are bipartite graphs with node features, it is natural to use Graph Neural Network (GNN) as
the Q-function approximator in our DQN agent. This bipartite graph representation not only encodes variable and
constraint information in the RMP, but also interactions between variables and constraints through its edge connections.

4.1.2 Actions, transition, and reward

Action space A. As shown in Figure 2, the RL agent selects one column to add to the RMP for the next iteration from
the candidate set G returned from current iteration SP. Therefore, the action space A contains all possible candidate
columns that can be generated from SPs; for example, the green nodes v6 and v7 in Figure 2. As the action space is
discrete and the state space is continuous, the GNN (Q-network) performs the operation of returning action values in
the current state, i.e., ˆq(s, a1; w), . . . , ˆq(s, am; w).

Transition function T . Transitions are deterministic. After selecting an action from the current candidate set G, the
selected column enters the basis in the next RMP iteration. We then delete the action nodes that were not selected from
the bipartite graph (current state), turn the selected action node into a column node, solve the new CG’s RMP and SP,
update all the features, and augment all the action nodes returned from the next SP iteration into the left-hand side of
the graph, which results in a new bipartite graph state. Take the bipartite graph state shown in the left of Figure 2 as an
example, and assume action v6 is selected at this iteration. The transition occurs as follows: v6 (in grey) becomes a
column node, and candidate columns v8 and v9 returned by SP (in green) are added.

Figure 2: State transition: Two green action nodes are considered, one of them is selected, transitioning to a new state.

Reward function R. The reward function consists of two components: (1) the change in the RMP objective value,
where a bigger decrease in value is preferred; (2) a unit penalty for each additional iteration. Together, they incentivize
the RL agent to converge faster. The reward at time step t is deﬁned as:

rt = α ·

(cid:32)

objt−1 − objt
obj0

(cid:33)

− 1,

5

(1)

arXiv Template

A PREPRINT

where obj0 is the objective value of the RMP in the ﬁrst CG iteration, which is used to normalize (objt−1 − objt) across
instances of various sizes; α is a non-negative hyperparameter that weighs the normalized objective value change in the
reward.

4.2

RLCG training and execution

Algorithm 1 shows how a trained RLCG agent is applied to solve a problem instance. The MDP components
(S, A, T , r, γ) used in this section are deﬁned in section 4.1. Before starting the iterative optimization, the ini-
tialization steps 1–3 build the initial bipartite graph with computed node features and add an initial set of columns
into basis (for instance, in CSP, we ﬁrst add simple cutting patterns into the basis to initialize). Inside the while loop,
the optimal action is selected from the candidate set G based on the best Q-value computed by the RL agent. Then,
the RMP and the SP are updated in the same way as the traditional CG algorithm. The MDP model corresponds to
extracting the MDP components from the current updated RMP and SP, which are discussed in section 4.1.1 and section
4.1.2. Steps 7, 8 correspond to the deterministic state transition T described in section 4.1.2, and St+1 is the resulting
state due to action a∗
t .

Algorithm 1: RLCG (RL-aided column generation algorithm)
Input: Problem instance p from distribution D & trained Q-function.
Output: Optimal solution
1 t = 0; RMP0 = Initialize(p)
2 Solve RMP0 to get dual values; Use dual values to construct SP0.
3 (cid:104)S0, A0, T0, R0(cid:105) = MDP(RMP0,SP0)
4 while CG algorithm has not converged do
5

a∗
t = arg maxat∈At Q(St, at) ∀at ∈ At ;
Add variable a∗
t to RMPt and get RMPt+1
Solve RMPt+1 to get dual values; Use dual values to build SPt+1.
(cid:104)St+1, At+1, Tt+1, Rt+1(cid:105) = MDP(RMPt+1,SPt+1)
t = t + 1

6

7

8

9
10 end
11 Return optimal solutions from RMPt, SPt

Training. The DQN algorithm with experience replay is used Mnih et al. [2015] with the typical mean squared loss
between Q(s0, a) and Qtarget(s0, a) ∀ a (all green action nodes) in bipartite graph state s0. Qtarget(s0, a) is deﬁned as
r + γ · maxa Q(s1, a). A GNN is used as Q-function approximator. Training instances are sequenced based on a
domain-speciﬁc curriculum described in Section 5.

5 Experimental Results

Baseline strategies and evaluation metrics. To assess the performance of RLCG, we consider two baseline methods:
the greedy column selection strategy (most negative reduced cost) and the MILP expert column selection of Morabit et al.
[2021] as described in section 2. This expert provides an upper bound on the performance of the supervised learning
approach of Morabit et al. [2021], as their ML model is approximately imitating the expert, i.e., it will never select
better columns than the expert. We consider two standard evaluation metrics: (1) Number of iterations to convergence
of CG; (2) Time in seconds. For the latter, this includes GNN inference time and feature computations for our method.
Our computing environment is described in section C.

5.1 Cutting Stock Problem (CSP)

Dataset. We use BPPLIB Delorme et al. [2018], a widely used collection of benchmark instances for binary packing
and cutting stock problems, which includes a number of instances proven to be difﬁcult to solve Delorme and Iori
[2020], Wei et al. [2020], Martinovic et al. [2020]. BPPLIB contains instances of different sizes with the roll length
n varying from 50 to 1000 and the number of orders m varying from 20 to 500. Our training set has instances with
n = 50, 100, 200. The remaining instances with n = 50, 100, 200, 750 are split into a validation set and a testing set,
with hard instances n = 750 only appearing in the testing set, as it is very expensive to solve such large instances during
training. The detailed statistics of these three sets are listed in Table 4 in Appendix E. Note that our test set is more
challenging than that used in training, allowing us to assess the agent’s generalization ability.

6

arXiv Template

A PREPRINT

Curriculum Design. In the RL context, curriculum learning Narvekar et al. [2020] serves to sort the instances that
a RL agent observes during the training process from easy to hard. In our experiments, we noticed that adopting the
curriculum learning paradigm improves the learning of the RL agent and results in better column selection and faster
convergence. We train our RLCG agent by feeding the instances in order of increasing difﬁculty. For CSP, instances
are sorted based on their roll length n and number of orders m to build a training curriculum (Table 5 in Appendix C).
The detailed comparison of training with and without the curriculum is deferred to Appendix A. In short, the former is
crucial to convergence when the training set contains instances of varying difﬁculties.

Hyperparameter tuning. We brieﬂy describe the outcome of a large hyperparameter tuning effort which is described
at length in Appendix B. We tune the main hyperparameters, α in the reward function (1), (cid:15) the exploration probability
in DQN, γ the discount factor, and lr the learning rate in gradient descent. A grid with 81 possible conﬁgurations is
explored by sampling 31 conﬁgurations, training the agent, and evaluating them on a validation set. The majority of the
conﬁgurations resulted in agents that outperform the greedy strategy (see Appendix B Figure 8). The best conﬁguration
is: α = 300, (cid:15) = 0.05, γ = 0.9 and lr = 0.001. For all the experiments conducted below, we use this conﬁguration,
both for CSP and VRPTW.

Performance comparison. Figure 3 shows pair-wise comparisons between RLCG and the two baselines in terms of
the number of CG iterations (top row) and solving time (bottom row) on the testing instances. Each point in a plot
corresponds to a test instance. Across all n and for both baselines in Figure 3, the majority of the points are above
the diagonal line, which indicates that RLCG outperforms the competing method w.r.t. the evaluation metric. Such a
tendency becomes more pronounced as the CSP instances become larger (left to right). RLCG performs better than the
greedy column selection for all the roll lengths n and the performance improvement becomes larger as n grows. For
CSP instances with n = 50 and n = 200, the MILP expert is able to maintain similar performance compared to RL.
However, solving the MILP expert requires solving a MILP at each iteration with only one-step lookahead which is
time consuming. For CSP instances with n = 750, RLCG begins to outperform the MILP expert due to its ability to take
future rewards into consideration.

(a) CG iterations, n = 50

(b) CG iterations, n = 200

(c) CG iterations, n = 750

(d) Time in seconds, n = 50

(e) Time in seconds, n = 200

(f) Time in seconds, n = 750

Figure 3: CSP: Scatter plots of CG iterations (top row) and running time (bottom row) with RLCG on the horizontal axis
and Greedy (Expert) on the vertical axis, respectively, in each of three pairs of sub-ﬁgures. Each point represents a test
instance of a given size n. Points above the diagonal indicate that RLCG is faster than the competitor.

Notice that even though we did not train RLCG using CSP instances with n = 750, it was able to perform well on such
instances, indicating strong generalization to harder problems. We also generate box plots to compare statistically
the three methods shown in Figure 4. Detailed statistics can be found in Table 6. As shown by all the subplots in
Figure 4, especially for large instances in the subplots (c) and (f), the proposed RLCG achieves statistically meaningful
improvements on the CG convergence speed compared to both the greedy and the expert column selection methods.

Convergence plots. In Figure 5 (a), we visualize the CG solving trajectories for all test instances with n = 750; similar
plots for n = 50 and n = 200 are shown in Appendix H. We record the objective values of the RMP at each CG
iteration for given method, then take the average over all instances. Note that we normalized the objective values
to be in [0,1] before taking the average among instances. Since the MILP expert is by far the slowest, we restricted
our attention to the two less expensive methods: RLCG and Greedy. Looking at Figure 5 (a), it is clear that not only

7

020406080IterationsforRLCGforn=50020406080IterationsforGreedyforn=50020406080IterationsforRLCGforn=50020406080IterationsforExpertforn=50050100IterationsforRLCGforn=2000255075100125IterationsforGreedyforn=200050100IterationsforRLCGforn=200020406080100120IterationsforExpertforn=2000100200300400IterationsforRLCGforn=7500100200300400IterationsforGreedyforn=7500100200300IterationsforRLCGforn=7500100200300IterationsforExpertforn=7500510TimeforRLCGforn=500.02.55.07.510.012.5TimeforGreedyforn=50051015TimeforRLCGforn=50051015TimeforExpertforn=500204060TimeforRLCGforn=2000204060TimeforGreedyforn=200020406080TimeforRLCGforn=200020406080TimeforExpertforn=20005001000TimeforRLCGforn=750025050075010001250TimeforGreedyforn=750050010001500TimeforRLCGforn=750050010001500TimeforExpertforn=750arXiv Template

A PREPRINT

(a) CG iterations, n = 50

(b) CG iterations, n = 200

(c) CG iterations, n = 750

(d) Time in seconds, n = 50

(e) Time in seconds, n = 200

(f) Time in seconds, n = 750

Figure 4: CSP: Box-plots of CG iterations (top row) and running time (bottom row) for the proposed method, RL
(green), Greedy (red), and Expert (blue). Each box represents the distribution of the iterations or running time for a
given method and roll size n. Lower is better.

(a) CSP, n = 750 test instances

(b) VRPTW, Large test instances

Figure 5: CG convergence plots. The solid curves are the mean of the objective values for all instances and the shaded
area shows ±1 standard deviation.

does RLCG terminate in fewer iterations and less time, but it also dominates Greedy throughout the CG iterations. In
other words, if one had to terminate CG earlier, RLCG would result in a better (smaller) objective value as compared to
what Greedy would achieve.

5.2 Vehicle Routing Problem with Time Windows (VRPTW)

The VRPTW seeks a set of possible routes for a given number of vehicles to deliver goods to a group of geographically
dispersed customers while minimizing the total travel costs. In the language of CG, each route is one column and there
are exponentially many. Constraints of VRPTW include: vehicle capacity; a vehicle has to start from a depot and return

8

RLCGGreedyExpert01020304050607080Iterationsforn=50RLCGGreedyExpert020406080100120Iterationsforn=200RLCGGreedyExpert050100150200250300350Iterationsforn=750RLCGGreedyExpert02468101214Timeforn=50RLCGGreedyExpert01020304050607080Timeforn=200RLCGGreedyExpert0200400600800100012001400Timeforn=750050100150200250300350ColumnGenerationIteration0.00.20.40.60.81.0RelativeobjectivevalueConvergenceplotsfortestinstancesofsizen=750GreedyRLCG0100200300400ColumnGenerationIteration0.00.20.40.60.81.0RelativeobjectivevalueConvergenceplotsforlargetestinstancesGreedyRLCGarXiv Template

A PREPRINT

to it; all customers should be served exactly once during their speciﬁed time windows. The detailed formulation of
VRPTW is given by Cordeau et al. [2002].

Dataset. We use the well-known Solomon benchmark Solomon [1987]. This dataset contains six different problem
types (C1, C2, R1, R2, RC1, RC2), each of which has 8–12 instances with 50 customers. “C" refers to customers which
are geographically clustered, “R" to randomly placed customers, “RC" to a mixture. The “1" and “2" labels refer to
narrow time windows/small vehicle capacity and large time windows/large vehicle capacity, respectively. The difﬁculty
levels of these sets are in order of C, R, RC. There are 56 instances in total in Solomon’s dataset, and from each original
Solomon instance, we can generate smaller instances by considering only the ﬁrst n < 50 customers.

We use instances from types C1, R1, RC1 for training. For the training set, we generated 80 smaller instances per type
from the original Solomon’s instances by only considering the ﬁrst n customers where n is randomly sampled from
5–8, for a total of 240 training instances. For testing, two sets of instances are considered: (1) 60 small-size instances
(number of customers n within same range as training instances); (2) 37 large-size instances (number of customers n
from 15–30). All the test instances are either generated from held-out Solomon instances (e.g., sets C2, R2, RC2) or
generated from some training instance but with a much different n.

Curriculum Design. As in CSP, we designed a curriculum for VRPTW that sequences instances in order of difﬁculty:
C1, R1, then RC1. This order is based on the fact that clustered instances have more structure that enables “compact"
routes for neighboring customers, whereas random instances require more complex routes. The mixed RC1 instances
require reasoning about both types of customers simultaneously Desrochers [1992].

Performance comparison. The hyperparemeters of the RL agent are chosen to be the same as the best set of
hyperparameters found for CSP. Figure 6 shows similar trends to those seen for CSP: RLCG converges in fewer iterations
and less time than Greedy on most instances. This effect is more pronounced for the large VRPTW instances, a fact that
can also be observed in the convergence plot of Figure 5 (b): RLCG converges in roughly 100 iterations compared to 300
iterations for Greedy, on average. Additional box plots are deferred to Appendix J. Note that we do not compare to the
Expert here given that it is much too slow and practically intractable.

(a) VRPTW with small instances

(b) VRPTW with large instances

Figure 6: VRPTW: Scatter plots of CG iterations and running time with RLCG. Each point represents a test instance of
a given size n. Points above the diagonal indicate that RLCG is faster than greedy.

6 Conclusions & Discussion of Limitations

RLCG shows superior performance and better convergence both in terms of the number of iterations and time compared
to the greedy column selection and the MILP expert strategy. In addition, our curriculum learning enables the agent to
generalize well when facing harder test instances. Our experiments on two important large-scale LP families show
that there is value in modeling CG as a sequential decision-making problem: taking the future impact of adding a
column into account helps convergence.However, our current work is restricted to adding only one column per CG
iteration. Adding multiple columns per iteration can speed up convergence. We believe this is an exciting next step but
one that stretches beyond the limits of our paper, which is the ﬁrst ever on RL for CG. A trained RLCG agent can also
be embedded within the branch and price algorithm for solving the integer-constrained versions of CSP/VRPTW and
invoked to solve each LP relaxation. The speed-ups demonstrated herein would transfer to that setting.

9

02040IterationsforRLCG01020304050IterationsforGreedy02505007501000TimeforRLCG02004006008001000TimeforGreedy0200400IterationsforRLCG0100200300400IterationsforGreedy05000100001500020000TimeforRLCG05000100001500020000TimeforGreedyarXiv Template

A PREPRINT

References

Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological

tour d’horizon. European Journal of Operational Research, 290(2):405–421, 2021.

Guy Desaulniers, Jacques Desrosiers, and Solomon M. Marius. Column generation - a primer in column generation.

Springer, page 1–32, 2006.

Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization

with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.

Hatem Ben Amor and Jose Valerio de Carvalho. Cutting stock problems - in column generation. Springer, page 1–32,

2004.

Mouad Morabit, Guy Desaulniers, and Andrea Lodi. Machine-learning–based column selection for column generation.

Transportation Science, 55(4):815–831, 2021.

Wei Zhang and Thomas G Dietterich. A reinforcement learning approach to job-shop scheduling. In IJCAI, volume 95,

pages 1114–1120. Citeseer, 1995.

Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms

over graphs. Advances in neural information processing systems, 30, 2017.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with

reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial

optimization: A survey. Computers & Operations Research, 134:105400, 2021.

Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veliˇckovi´c. Combinatorial

optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021.

Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer programming: Learning to cut. In

International Conference on Machine Learning, pages 9367–9376. PMLR, 2020.

Quentin Cappart, Thierry Moisan, Louis-Martin Rousseau, Isabeau Prémont-Schwarz, and Andre Cire. Combining
reinforcement learning and constraint programming for combinatorial optimization. arXiv preprint arXiv:2006.01610,
2020.

Cynthia Barnhart, Amy M Cohn, Ellis L Johnson, Diego Klabjan, George L Nemhauser, and Pamela H Vance. Airline

crew scheduling. In Handbook of transportation science, pages 517–560. Springer, 2003.

Guy Desaulniers, Jacques Desrosiers, and Marius Solomon. Accelerating strategies in column generation methods for

vehicle routing and crew scheduling problems. 15, 01 1999. doi:10.1007/978-1-4615-1507-4_14.

P. C. Gilmore and R. E. Gomory. A linear programming approach to the cutting-stock problem. Operations research, 9
(6):849–859, 1961. ISSN 0030-364X. doi:10.1287/opre.9.6.849. URL https://doi.org/10.1287/opre.9.6.
849.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
learning. nature, 518(7540):529–533, 2015.

Maxence Delorme, Manuel Iori, and Silvano Martello. Bpplib: a library for bin packing and cutting stock problems.

Optimization Letters, 12(2):235–250, 2018.

Maxence Delorme and Manuel Iori. Enhanced pseudo-polynomial formulations for bin packing and cutting stock

problems. INFORMS Journal on Computing, 32(1):101–119, 2020.

Lijun Wei, Zhixing Luo, Roberto Baldacci, and Andrew Lim. A new branch-and-price-and-cut algorithm for one-

dimensional bin-packing problems. INFORMS Journal on Computing, 32(2):428–443, 2020.

John Martinovic, Maxence Delorme, Manuel Iori, Guntram Scheithauer, and Nico Strasdat. Improved ﬂow-based

formulations for the skiving stock problem. Computers & Operations Research, 113:104770, 2020.

Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning

for reinforcement learning domains: A framework and survey. arXiv preprint arXiv:2003.04960, 2020.

J.-F. Cordeau, G. Desaulniers, J. Desrosiers, and F. Soumis. The vrp with time windows. SIAM Monographs on Discrete

Mathematics and its Applications, pages 157–193, 2002.

10

arXiv Template

A PREPRINT

M. Solomon. Algorithms for the vehicle routing and scheduling problem with time window constraints. Journal of
Service Science and Management, 35:254–265, 1987. URL https://www.sintef.no/projectweb/top/vrptw/
solomon-benchmark/.

Jacques Desrosiers; Marius Solomon; Martin Desrochers. A new optimization algorithm for the vehicle routing problem

with time windows. Operations Research, 40(2):342–354, 1992.

11

arXiv Template

A PREPRINT

Appendix

A Curriculum learning

(a) curriculum training

(b) No curriculum training

Figure 7: (a) shows the training process with such curriculum and Figure (b) shows the training process without a
curriculum. In both plots, the x axis is the instance index (ordered for (a) and randomized for (b)) while the y axis
shows the total number of RL guided CG iterations until that particular index x instance is solved.

Figure 7 above compares the training trajectories between training with 400 CSP instances following the sequence
provided in Table 5 and training with the same 400 CSP instances randomly ordered. In (a), as every 40 instances we
increase the CSP difﬁculty. Although there is an upward trend in the training curve, however, within each instance
difﬁculty setting (ﬁxed n and m), there is a downward trend showing a sign of learning. In contrast, there is no clear sign
of learning in (b). Therefore, for all the experiments shown in this paper, the RLCG model is trained using a curriculum.

B Hyperparameter tuning

Table 1 shows RL related parameters.

Table 1: RL Parameters

Parameter
state
normalization
step penalty

reward

action

Value
features normalization is MinMaxScalar
from sklearn
For each iteration RLCG takes before the cur-
rent CSP instance is solved, penalize each
step by 1 in the reward design
there are two settings α = 5 and β = 1,
α = 5 and β = 0, step penalty is always 1
solution pool 10 from Gurobi solver for SP,
which means at each iteration our action
space contains 10 columns.

In Table 2, we provide GNN parameters that we used.

We conduct hyperparameters tuning and sensitivity analysis using a validation set over: the parameter α used in equation
1 to weight the change in the normalized objective value used in our reward function, the exploration parameter of
the RL agent (cid:15), the discount factor γ, and the learning rate lr. All other hyperparameters and their values are listed
in Table 1 and Table 2. The values we consider for each hyperparameter are the following: α ∈ {0, 100, 300}, (cid:15) ∈
{0.01, 0.05, 0.2}, γ ∈ {0.9, 0.95, 0.99} and lr ∈ {0.01, 1e-3, 3e-4}. We choose the value for α ∈ {0, 100, 300} because
when α = 0, we place no weight on decreasing the objective value, otherwise, α = 100, 300 will bring the normalized

12

4080120160200240280320360400Instanceindex(easiesttohardest)255075100125150175200NumberofiterationsLearningcurvewithcurriculumlearning4080120160200240280320360400Instanceindex(arbitraryordering))255075100125150175200NumberofiterationsLearningcurvewithoutcurriculumlearningarXiv Template

A PREPRINT

Table 2: GNN Parameters

Parameter
Optimizer
Network Structure
Batch Size

Value
Adam
refer to code for details
32

change in the objective values into similar scale as the step penalty. Therefore, the search space for hyperparameters is
deﬁned as the Cartesian product between all these sets of different hyperparameters possible values, which gives us 81
conﬁgurations, and we randomly select 31 conﬁgurations out of them. Then we train 31 RLCG models corresponding to
selected 31 conﬁgurations, and we evaluate these models using our validation set. The validation metric or reward is
deﬁned as the ratio of the total number of iterations RLCG takes to solve each CSP instance divided by the total number
of iterations greedy takes. For each model, we compute such ratio for all the instances in validation set, and we generate
the following box plot shown in Figure 8 showing the validation metric for all the models. Among the 31 conﬁgurations
we tested, the majority of the models were able to outperform greedy strategy on the instances in the validation set. The
best conﬁguration is model 3: α = 300, (cid:15) = 0.05, γ = 0.9 and lr = 0.001. For all the results reported in this paper,
we use this conﬁguration. This includes VRPTW, although no direct hyperparameter tuning has been done for this
problem class. To assess the sensitivity of the RL training with respect to randomness such as the GNN initialization
and the exploration in RL, we compare the average validation reward relative to greedy for the selected model across
ﬁve random seeds. The average varies between 1.23 and 1.25, indicating little to no sensitivity.

Figure 8: Hyperparameter sensitivity: the vertical axis list all the models we evaluated by its index. The detailed
hyperparameter conﬁgurations each index refers to are listed in Appendix B Table 8. The horizontal axis shows the
ratio of RLCG model solving iterations to greedy column selection solving iterations. The blue vertical line shows the
ratio threshold indicating same performance as greedy. The best to worst models are ordered from bottom to top.

In Table 3, we provide detailed conﬁgurations of each model index for our analysis of hyperparameters as well as their
detailed validation results.

13

0.81.01.21.41.61.8ValidationrewardrelativetoGreedy3111805262192417138122242523142291273010716192015286HyperparameterconﬁgurationarXiv Template

A PREPRINT

Model index Hyperparameters conﬁg

iterations mean

iterations median

iterations std

Model 0
Model 1
Model 2
Model 3
Model 4
Model 5
Model 6
Model 7
Model 8
Model 9
Model 10
Model 11
Model 12
Model 13
Model 14
Model 15
Model 16
Model 17
Model 18
Model 19
Model 20
Model 21
Model 22
Model 23
Model 24
Model 25
Model 26
Model 27
Model 28
Model 29
Model 30

(100, 0.2, 0.9, 0.0003)
(100, 0.05, 0.99, 0.01)
(100, 0.2, 0.9, 0.01)
(300, 0.05, 0.9, 0.001)
(300, 0.05, 0.99, 0.001)
(300, 0.05, 0.95, 0.001)
(0, 0.05, 0.9, 0.0003)
(0, 0.2, 0.95, 0.0003)
(100, 0.05, 0.9, 0.0003)
(100, 0.2, 0.9, 0.001)
(0, 0.05, 0.95, 0.0003)
(300, 0.01, 0.95, 0.001)
(300, 0.05, 0.9, 0.001)
(100, 0.05, 0.95, 0.0003)
(100, 0.2, 0.9, 0.001)
(100, 0.05, 0.9, 0.001)
(0, 0.2, 0.99, 0.001)
(300, 0.2, 0.95, 0.001)
(300, 0.2, 0.9, 0.0003)
(0, 0.05, 0.9, 0.0003)
(0, 0.2, 0.9, 0.001)
(300, 0.05, 0.99, 0.0003)
(300, 0.2, 0.99, 0.001)
(100, 0.2, 0.9, 0.01)
(300, 0.05, 0.9, 0.001)
(100, 0.2, 0.95, 0.001)
(100, 0.01, 0.9, 0.001)
(0, 0.2, 0.9, 0.001)
(0, 0.2, 0.9, 0.001)
(0, 0.05, 0.95, 0.001)
(100, 0.05, 0.95, 0.0003)

1.22
1.00
1.00
1.25
1.12
1.18
0.91
0.98
1.14
1.19
0.97
1.22
1.11
1.15
1.07
0.95
0.99
1.18
1.24
0.92
0.95
1.18
1.11
1.06
1.16
1.07
1.19
1.00
0.90
0.97
1.01

1.20
1.00
1.00
1.24
1.09
1.18
0.88
0.96
1.11
1.16
0.97
1.23
1.11
1.12
1.04
0.91
0.95
1.15
1.23
0.94
0.92
1.16
1.10
1.06
1.16
1.07
1.18
0.99
0.90
1.00
0.98

0.14
0.00
0.00
0.13
0.19
0.14
0.13
0.14
0.16
0.18
0.14
0.16
0.12
0.16
0.17
0.15
0.13
0.17
0.13
0.12
0.19
0.16
0.16
0.10
0.17
0.12
0.13
0.13
0.11
0.11
0.15

Table 3: Evaluated models’ conﬁgurations and validation performances

C Computing environment

To implement GNN, we use Tensorﬂow 2.7.0. To solve both RMP and SP optimization problems in CG, we use Gurobi
9.5.0. For the training using 400 instances schedule for CSP and 240 instances schedule for VRPTW, the training
takes around 8-10 hours CPU time using the following CPU settings: Intel 2.30 Ghz, 2 CPU Cores, Intel (R) Xeon(R),
Haswell CPU family.

D Graph neural network for bipartite graph

Graph Neural Network (GNN) has been successfully applied to many different machine learning tasks with graph
structured data. GNN includes a message passing method where the features of each node for each node pass to other
neighbouring nodes through learned transformations to generate aggregated information, and such information can be
used for node classiﬁcation, edge selection so on so forth. Due to the effectiveness of GNN to utilize graphical structure
of the data, catch node-level dependencies, and has permutation invariant properties, GNN is an appropriate method for
performing node(column) selection task in the present column generation problem.

For this study we are encoding the information for each iteration of CG using a bipartite graph G = (E, V ) as the state,
where each column and constraint is represented by node v ∈ V and there is an edge e ∈ E between columns and
constraints only if the column contributes to the constraint. Detailed encoding for state is discussed in section 4.1.1. As
we are using a bipartite graph, the GNN we use should be able to achieve convolution on a bipartite graph with two
types of nodes (variable nodes and constraint nodes), and we utilize the similar bipartite GNN as in the study conducted
by (Morabit et al. [2021]) with modiﬁcation of the task it performs (from binary classiﬁcation to Q value regression).
Here we give a brief overview of how convolution or the features update is achieved in this bipartite GNN.

14

arXiv Template

A PREPRINT

The features update is done in two phase: phase 1 updates the constraint features and phase 2 updates the column
features. In phase 1, the constraint features in next iteration is obtained by applying a non-linear transformation of
previous constraint node features and aggregate information of its neighbouring nodes, which are column nodes that are
connected to this constraint node. This can be treated as information passing from variable nodes to constraint nodes.
Similarly,the second phase can be seen as message passing from constraint nodes to variable nodes.

Once the column node features have been updated for several iterations for all the column nodes, these features are fed
into a fully connected layer, which results in Q values for each column node.

E Training, Validation, Testing set

Table 4 below shows the information of the instances contained in the training set, validation set and testing set for CSP.
Column lists total number of instances in each dataset, while other columns list the number of instances with speciﬁc
roll length n in that dataset. The division of VRPTW dataset can be found in Section 5.2.

Table 4: Dataset division for CSP

Dataset

Total

Training
Validation
Testing

400
30
156

n =
50
160
10
49

n =
100
160
10
0

n =
200
80
10
86

n =
750
0
0
21

F Curriculum Learning design

In Table 5, we provide data characteristic we considered, for the sake of curriculum learning of the RL agent. In
this paper the RL agent is trained on instances that are ordered according to their difﬁculty level. To accomplish this
instances are divided into three categories of easy, normal and hard according to the stock length for CSP.

Easy instances have stock length of 50, normal instances have stock length of 100 and hard instances have stock length
of 200. There are 40 instances for each instance type. Details of training curriculum of different instance types are
shown in table 5. Figure 7 displays the number of steps to convergence vs. instance number for training instances. It
is clear that for each instance type the steps taken for convergence decreases as the model is trained on more of the
same instance type. This shows that the RL agent successfully learns to select columns to enter basis. However, when
instances are ordered randomly (Figure 7b) there are no speciﬁc trend on the steps taken to converge. As demonstrated,
instances with random training overall take longer to converge than greedy. However, the RL algorithm that is trained on
the ordered instances converge faster than the others. This highlights the necessity of curriculum learning. Curriculum
for VRPTW can be found in Section 5.2.

Table 5: Curriculum Learning schedule for CSP

Training curriculum

Type of Instance

Easy
Easy
Easy
Easy
Normal
Normal
Normal
Normal
Hard
Hard

Number of
Instances
40
40
40
40
40
40
40
40
40
40

15

Stock
Length
50
50
50
50
100
100
100
100
200
200

Number
of Orders
50
75
100
120
75
100
120
150
125
150

arXiv Template

A PREPRINT

G Node Features

Node features used for CSP:

1. Column node features:

Feature (a) and (c) relate to solving the RMP problem as they are all information about decision variables
in RMP, and each column node corresponds to one decision variable. Feature (b) and (d) are determined by
the problem formulation of each cutting stock instance, while feature (e) - (i) corresponds to the dynamical
information of each column entering and leaving the basis.

(a) Reduced cost: Reduced cost is a quantity associated with each variable indicating how much the objective
function coefﬁcient on the corresponding variable must be improved before the solution value of the
decision variable will be positive in the optimal solution (the cutting pattern will be used in optimal set of
cutting patterns). The reduced cost value is only non-zero when the optimal value of a variable is zero.

(b) Connectivity of column node: Total number of constraint nodes each column node connects to. As each
constraint is a particular demand, this node feature indicates the ability of a column node (a pattern) to
satisfy demands. It also indicates the connectivity of each column node in the bipartite graph representing
the state.

(c) Solution value: The solution value of each decision variable corresponding to each column node after
solving the RMP in the current iteration. For each column node, this feature is continuous number greater
than or equal to 0. The candidate column nodes have this feature set to be 0.

(d) Waste: A feature recording the remaining length of a roll if we were to cut the current pattern from the
roll. Again, each column node corresponds to one decision variable in RMP, which also represents one
particular cut pattern.

(e) Number of iterations that each column node stays in the basis: If the column node stays in the basis

for a long time, it is most likely that the pattern corresponds to this column node is really good.

(f) Number of iterations that each column node stays out of the basis: if the column node stays out of
the basis for a long time, it is most likely never enters the basis and being used in optimal solution in
future iterations.

(g) If the column left basis on the last iteration or not: This is a binary feature recording the dynamics of

each column node.

(h) If the column enter basis on the last iteration or not: Similar binary feature as (f).

(i) Action node or not: A binary feature indicating whether a column node is a candidate (a newly added
action) or not. If the column node is a candidate node (column) that is generated at the current iteration
by SP, then this binary feature is 1 otherwise 0.

2. Constraint node features:

Each constraint node corresponds to one constraint in RMP, so the number of constraint nodes are ﬁxed for
each cutting stock problem instance.

(a) Dual value: Dual value or shadow price is the coefﬁcient of each dual variable in sub-problem objective
function, and as each constraint node corresponds to one dual variable, we record dual value as one
feature for constraint node.

(b) Connectivity of constraint node: Total number of column nodes each constraint node connects to,
which also indicates the connectivity of each constraint node in the bipartite graph representing the state.

VRPTW node features used are quite similar to CSP:
Column node features: Reduced cost, connectivity of column node, solution value, route cost, Number of iterations
that each column node stays in the basis, Number of iterations that each column node stays in the basis, If the column
left basis on the last iteration or no, If the column enter basis on the last iteration or no. Constraint node features:
Dual value, connectivity of constraint node

16

arXiv Template

A PREPRINT

H Other Convergence plots

Figure 9: CSP Column Generation convergence comparison average results for all testing instance with n = 50. The
solid curves are the mean of the objective values trajectory for all n = 50 instances, and the shaded area shows ± 1
standard deviation

Figure 10: CSP Column Generation convergence comparison average results for all testing instance with n = 50. The
solid curves are the mean of the objective values trajectory for all n = 200 instances, and the shaded area shows ± 1
standard deviation

Figure 11: VRPTW Column Generation convergence comparison average results for all small testing instance. The
solid curves are the mean of the objective values trajectory for small testing instances, and the shaded area shows ± 1
standard deviation

17

020406080ColumnGenerationIteration0.00.20.40.60.81.0RelativeobjectivevalueConvergenceplotsfortestinstancesofsizen=50GreedyRLCG020406080100120ColumnGenerationIteration0.00.20.40.60.81.0RelativeobjectivevalueConvergenceplotsfortestinstancesofsizen=200GreedyRLCG01020304050ColumnGenerationIteration0.00.20.40.60.81.0RelativeobjectivevalueConvergenceplotsforsmalltestinstancesGreedyRLCGarXiv Template

A PREPRINT

I Detailed statistics of testing results

In Table 6, we provide statistics obtained from our experimental results for CSP. We report both the average and standard
deviation of number of iterations, solution time measured in seconds, and the objective function values. Note that the
resulting objective function values between column selection policies might differ due to the early-stopping criteria
we adopted. Note that for the sake table spacing, we use Objval to denote objective function value, and µ, σ for mean
and standard deviation, respectively. We observe the clear dominance of RLCG over the potential of RLCG in solving
challenging CG problems in practice.

n = 50

n = 200

n = 750

Iteration Time(s) Objval Iteration Time(s) Objval

Iteration

Time(s)

µ

σ

µ σ

µ

σ

µ

σ

µ

σ

µ

σ

µ

σ

µ

σ

Objval

µ

σ

Greedy 51.2 11.4 6.0 2.5 23.4 2.8 78.0 23.0 22.3 14.4 91.1 10.1 292.8 47.7 640.8 240.2 327.6 18.2

RL 41.6 10.0 4.9 2.2 23.5 2.8 64.7 20.5 18.1 12.3 91.5 9.8 227.2 42.7 460.3 219.9 328.6 18.2

Expert 42.5 9.5 7.1 3.1 23.4 2.8 69.1 18.8 29.3 18.9 91.2 10.1 251.3 35.2 827.9 302.0 327.2 18.7

Table 6: Solution time, iteration and objective function value reports with µ mean, and σ standard deviation for CSP

In Table 7, we provide the same statistics obtained from our experimental results for VRPTW.

small test instances

large test instances

Iteration Time(s)

Objval

Iteration

Time(s)

µ

σ

µ

σ

µ

σ

µ

σ

µ

σ

Objval

µ

σ

Greedy 18.8 11.7 291.1 285.0 102.6 30 128.1 100.3 3268.2 4627.5 458.6 144.6

RL

9.5 6.3 179.9 247.1 102.6 30.0 75.6 39.3 1832.8 2015.3 458.6 144.6

Table 7: Solution time, iteration and objective function value reports with µ mean, and σ standard deviation for
VRPTW

J VRPTW box plots

Figure 12: Pair-wise comparisons between RLCG and greedy benchmark in terms of the number of CG iterations and
the solving time over small and large testing sets for VRPTW

Similar to CSP, we see the upward generalization of RLCG in VRPTW: we only train our model with small sized
instances, and tested on large instances with more customers, and we observe that RLCG was able to perform well using
VRPTW testing instances with large size. These results, again indicate that our proposed RLCG can be preferable in
solving challenging column generation problems because of its ability to generalize well. Besides, compare large testing
results with small testing results for VRPTW (also compare n=750 results with n=50,200 for CSP), we observe that the
more challenging the problem, the larger gap exist between RLCG and benchmark method. This indicates that for CG
problem we considered, the harder the problem, the more important considering the future effect of CG becomes, thus
taking future into consideration would drastically accelerate the solving process of CG.

18

RLCGGreedy01020304050IterationsRLCGGreedy05001000150020002500TimearXiv Template

A PREPRINT

K Per instance results

K.1 per instance results VRPTW

Names
Instance name

r107.txt, n = 20
r110.txt, n = 19
c101.txt, n = 25
r106.txt, n = 20
r111.txt, n = 20
r104.txt, n = 20
r103.txt, n = 19
rc102.txt, n = 15
rc104.txt, n = 15
rc103.txt, n = 19
rc107.txt, n = 15
rc105.txt, n = 15
rc101.txt, n = 16
rc102.txt, n = 14
rc106.txt, n = 19
rc108.txt, n = 15
r104.txt, n = 15
r101.txt, n = 16
r102.txt, n = 14
r103.txt, n = 19
c101.txt, n = 15
r105.txt, n = 15
c107.txt, n = 15
c105.txt, n = 20
rc201.txt, n = 16
rc105.txt, n = 15
rc106.txt, n = 15
r101.txt, n = 15
r205.txt, n = 19
rc107.txt, n = 15
r107.txt, n = 18
r108.txt, n = 15
r108.txt, n = 18
r109.txt, n = 19
r105.txt, n = 15
c101.txt, n = 20
rc103.txt, n = 15

Iterations
Greedy_iter RL_iter Greedy_time RL_time Greedy_obj RL_obj

Objective value

Time

328
189
140
56
203
261
440
131
196
163
77
107
40
115
104
345
319
22
110
43
36
173
93
87
57
172
128
63
11
77
28
37
49
153
86
83
292

84
101
90
30
126
119
142
157
79
84
77
88
27
73
58
173
131
23
59
44
32
93
83
81
51
66
98
55
16
77
32
30
39
71
67
65
125

15164.16
4093.69
4123.05
807.00
6965.91
18788.82
15789.96
1138.89
2992.44
2135.99
766.64
578.83
106.21
999.08
660.81
5449.42
16084.76
31.79
960.38
580.56
88.32
3877.94
5609.29
3874.88
1752.45
1531.70
991.33
100.00
1498.44
799.65
549.75
2992.61
6062.88
1294.90
243.73
1585.00
3429.17

3843.84
2170.76
2632.07
429.26
4350.76
8359.52
5132.63
1365.07
1211.46
1106.87
783.29
497.95
69.52
645.00
370.68
2649.93
6497.48
32.16
513.88
598.27
78.41
2100.79
4938.09
3609.85
1567.67
571.56
745.47
87.98
2213.35
819.63
615.12
2321.59
4529.36
603.83
191.69
1238.26
1470.35

517.00
522.00
637.00
637.00
534.00
504.00
437.00
564.00
491.00
667.00
341.00
415.00
571.00
391.00
631.00
383.00
343.00
583.00
382.00
424.00
491.00
583.00
249.00
485.00
641.00
276.00
276.00
409.00
749.00
341.00
515.00
148.75
156.00
578.00
433.00
628.00
337.00

517.00
522.00
637.00
637.00
534.00
504.00
437.00
564.00
491.00
667.00
341.00
415.00
571.00
391.00
631.00
383.00
343.00
583.00
382.00
424.00
491.00
583.00
249.00
485.00
641.00
276.00
276.00
409.00
749.00
341.00
515.00
148.75
156.00
578.00
433.00
628.00
337.00

K.2 per instance results CSP

19

arXiv Template

A PREPRINT

Name
Instance name

BPP_50_125_0.1_0.7_2
BPP_50_200_0.2_0.8_5
BPP_50_50_0.1_0.8_0
BPP_50_125_0.1_0.8_8
BPP_50_200_0.2_0.7_2
BPP_50_125_0.1_0.7_8
BPP_50_200_0.2_0.7_0
BPP_50_125_0.1_0.8_6
BPP_50_200_0.2_0.8_7
BPP_50_125_0.1_0.8_1
BPP_50_50_0.1_0.8_1
BPP_50_200_0.2_0.7_3
BPP_50_125_0.2_0.7_4
BPP_50_200_0.2_0.7_5
BPP_50_125_0.1_0.8_3
BPP_50_200_0.1_0.8_2
BPP_50_125_0.1_0.7_1
BPP_50_200_0.2_0.7_8
BPP_50_200_0.2_0.7_9
BPP_50_200_0.1_0.8_1
BPP_50_200_0.1_0.8_4
BPP_50_125_0.1_0.7_4
BPP_50_125_0.2_0.7_0
BPP_50_200_0.1_0.8_6
BPP_50_200_0.1_0.8_0
BPP_50_125_0.1_0.7_5
BPP_50_200_0.2_0.8_2
BPP_50_50_0.1_0.8_2
BPP_50_125_0.2_0.7_2
BPP_50_50_0.1_0.8_4
BPP_50_200_0.2_0.7_4
BPP_50_125_0.1_0.8_7
BPP_50_200_0.1_0.8_7
BPP_50_125_0.2_0.7_3
BPP_50_125_0.1_0.8_2
BPP_50_125_0.1_0.7_7
BPP_50_125_0.1_0.8_5
BPP_50_125_0.2_0.7_1
BPP_50_200_0.2_0.8_0
BPP_50_125_0.1_0.8_9
BPP_50_125_0.1_0.7_6
BPP_50_200_0.1_0.8_9
BPP_50_50_0.1_0.8_3
BPP_50_125_0.1_0.7_9
BPP_50_125_0.1_0.8_0
BPP_50_200_0.1_0.8_3
BPP_50_200_0.1_0.8_5
BPP_50_200_0.2_0.7_7
BPP_50_200_0.2_0.8_1

Iterations
Greedy_iter RL_iter Expert_iter Greedy_time RL_time Expert_time Greedy_obj RL_obj Expert_obj

Objective value

Time

2.80
3.46
2.22
4.31
5.10
5.65
4.85
5.71
4.83
6.36
1.56
5.44
2.80
7.33
2.90
9.25
5.79
3.94
4.00
8.05
7.25
5.01
3.91
6.96
11.63
2.87
6.21
1.66
3.29
2.53
3.17
5.20
12.93
3.19
4.73
5.04
3.46
4.97
5.75
5.35
4.64
5.28
1.76
5.10
4.70
5.84
3.57
5.61
3.32

3.36
5.12
2.81
7.84
6.47
6.82
5.50
11.61
7.93
7.83
1.92
9.92
3.02
9.08
4.97
12.15
5.79
6.69
6.39
12.62
9.00
6.78
6.26
10.77
14.99
5.43
8.93
2.06
4.66
3.21
5.24
8.07
15.27
3.95
7.02
6.99
4.66
7.88
10.72
8.84
6.51
9.66
1.75
7.23
7.57
7.68
5.60
8.30
6.06

18.18
29.00
23.50
20.74
23.06
20.04
23.46
25.25
24.00
22.67
28.00
23.50
25.00
24.60
28.50
20.29
18.92
21.65
23.21
23.21
25.00
21.30
21.38
22.60
23.17
19.71
26.00
25.00
21.74
23.25
21.83
26.00
21.36
23.00
20.80
19.63
29.50
23.15
27.50
23.50
20.77
21.88
20.06
20.46
26.00
21.98
30.00
23.50
27.50

18.32
29.00
23.50
21.31
23.00
20.31
23.58
25.50
23.96
22.79
28.00
23.50
25.00
24.70
28.50
20.51
18.95
21.83
23.56
23.25
25.00
21.33
21.62
22.86
23.57
20.04
26.00
25.00
22.07
23.29
21.92
26.00
21.21
23.00
20.82
19.81
29.50
23.15
27.50
23.50
20.98
21.99
20.12
20.66
26.00
22.14
30.00
23.50
27.50

18.31
29.00
23.50
20.64
23.00
20.18
23.45
25.25
23.94
22.68
28.00
23.50
25.00
24.70
28.50
20.33
18.98
21.62
23.15
23.07
25.00
21.42
21.38
22.58
23.15
19.88
26.00
25.00
21.89
23.25
21.83
26.00
21.19
23.00
20.62
19.70
29.50
23.15
27.50
23.50
20.90
21.83
20.07
20.59
26.00
22.21
30.00
23.50
27.50

45
46
40
53
57
57
43
61
57
67
25
63
44
68
35
59
46
56
50
62
58
51
44
62
84
45
53
31
46
39
55
50
59
46
39
47
40
61
70
50
52
64
30
51
49
66
34
57
41

30
31
31
38
42
52
42
43
44
54
23
45
37
52
29
55
48
31
38
56
52
42
41
50
69
28
48
29
38
35
33
43
75
39
40
42
36
44
42
43
40
42
30
42
41
45
33
48
28

28
31
31
48
38
47
35
57
50
49
23
54
31
46
34
52
37
34
42
59
46
41
46
53
65
36
47
30
39
34
39
46
66
38
42
41
35
48
51
48
40
52
26
43
46
42
35
50
33

4.45
5.15
2.64
6.04
6.70
6.13
4.48
8.35
6.08
7.64
1.50
7.91
3.08
9.80
3.42
9.66
5.11
7.96
5.37
8.62
8.05
6.13
3.90
8.99
14.12
4.90
6.58
1.62
3.98
2.56
5.51
5.70
8.85
3.52
4.14
5.65
3.67
7.03
10.33
5.94
6.01
8.31
1.65
6.26
5.32
8.79
3.48
6.51
4.82

20

arXiv Template

A PREPRINT

Name
Instance name

BPP_200_100_0.2_0.7_1
BPP_200_100_0.1_0.7_4
BPP_200_75_0.2_0.8_5
BPP_200_75_0.2_0.7_9
BPP_200_75_0.1_0.7_4
BPP_200_100_0.2_0.7_0
BPP_200_100_0.1_0.7_5
BPP_200_50_0.2_0.8_1
BPP_200_120_0.1_0.7_7
BPP_200_50_0.2_0.8_5
BPP_200_100_0.1_0.7_7
BPP_200_75_0.1_0.7_7
BPP_200_75_0.1_0.7_1
BPP_200_75_0.1_0.8_7
BPP_200_75_0.2_0.8_0
BPP_200_120_0.1_0.7_3
BPP_200_100_0.2_0.8_8
BPP_200_100_0.2_0.7_8
BPP_200_100_0.2_0.8_2
BPP_200_75_0.2_0.7_2
BPP_200_75_0.1_0.8_9
BPP_200_100_0.1_0.8_4
BPP_200_100_0.1_0.8_6
BPP_200_75_0.1_0.8_4
BPP_200_100_0.2_0.8_9
BPP_200_75_0.2_0.8_8
BPP_200_100_0.1_0.8_8
BPP_200_75_0.1_0.7_2
BPP_200_75_0.2_0.8_2
BPP_200_75_0.2_0.7_7
BPP_200_100_0.1_0.7_9
BPP_200_75_0.1_0.7_9
BPP_200_100_0.2_0.8_6
BPP_200_75_0.2_0.8_3
BPP_200_100_0.1_0.7_2
BPP_200_75_0.2_0.7_8
BPP_200_100_0.1_0.7_8
BPP_200_100_0.1_0.8_5
BPP_200_100_0.1_0.7_3
BPP_200_75_0.2_0.8_6
BPP_200_75_0.2_0.8_7
BPP_200_120_0.1_0.7_0
BPP_200_75_0.1_0.7_4
BPP_200_100_0.2_0.7_7
BPP_200_120_0.1_0.7_8
BPP_200_100_0.1_0.8_3
BPP_200_75_0.1_0.8_8
BPP_200_100_0.1_0.7_0
BPP_200_50_0.2_0.8_2
BPP_200_120_0.1_0.8_1
BPP_200_120_0.1_0.7_6
BPP_200_120_0.1_0.7_5
BPP_200_100_0.2_0.8_3
BPP_200_50_0.2_0.8_3
BPP_200_100_0.2_0.8_1
BPP_200_120_0.1_0.7_9

Iterations
Greedy_iter RL_iter Expert_iter Greedy_time RL_time Expert_time Greedy_obj RL_obj Expert_obj

Objective value

Time

18.19
31.03
13.37
10.35
19.91
19.05
31.68
3.44
52.32
5.46
28.03
18.52
21.38
31.89
15.58
63.39
31.85
25.20
39.09
9.31
33.23
51.75
47.84
28.12
40.78
15.57
73.34
15.03
16.48
9.84
37.97
18.38
48.23
15.06
33.39
9.78
41.74
61.02
26.34
10.44
13.34
50.55
19.99
27.69
38.58
49.13
32.42
33.65
4.57
81.43
44.49
46.90
38.00
4.16
31.66
51.57

90.50
79.53
103.00
89.24
80.44
93.45
78.27
115.50
78.47
103.00
77.10
82.80
82.69
91.71
102.33
82.81
111.00
91.21
99.56
92.30
92.00
94.50
88.96
92.03
101.94
101.50
93.24
78.03
96.65
91.00
80.15
82.59
98.00
102.50
80.63
89.12
80.91
90.08
79.88
115.00
108.00
80.17
80.44
93.57
79.73
89.03
87.02
81.23
103.50
84.03
82.98
80.48
100.33
101.83
106.00
79.99

90.81
79.93
103.00
89.67
81.72
93.40
79.52
115.50
79.01
103.00
77.60
83.70
83.60
92.08
102.33
83.30
111.00
92.33
99.59
92.96
92.28
94.50
89.42
92.06
102.00
101.50
93.29
78.43
96.58
91.00
80.62
83.25
98.00
102.50
81.30
89.70
81.01
91.64
80.18
115.00
108.00
80.86
81.72
93.69
80.62
89.98
87.65
81.34
103.50
84.11
83.25
80.68
100.33
101.83
106.00
80.27

90.50
79.70
103.00
89.25
80.37
93.40
78.02
115.50
78.47
103.00
77.03
82.92
82.64
91.72
102.33
83.24
111.00
91.19
99.56
92.44
92.00
94.50
89.18
92.01
101.87
101.50
93.58
78.03
96.58
91.00
80.34
82.41
98.00
102.50
81.02
89.12
80.70
89.99
79.66
115.00
108.00
80.89
80.37
93.50
79.57
89.15
87.03
80.98
103.50
83.68
83.06
80.82
100.33
101.83
106.00
79.99

57
80
63
55
66
65
69
30
87
44
71
78
75
97
59
111
85
88
107
56
84
106
105
84
121
75
124
60
71
54
77
58
110
61
95
56
85
102
80
54
57
104
66
87
69
106
87
80
35
99
88
95
97
40
87
94

45
61
52
47
48
53
49
30
72
32
56
57
63
82
48
89
67
51
89
46
87
87
84
82
92
55
112
48
55
50
70
54
93
50
69
39
74
86
67
41
52
84
48
72
53
93
81
72
29
102
75
78
77
33
59
80

56
68
52
52
67
59
71
29
87
42
65
64
69
82
59
92
73
75
88
49
87
85
87
75
89
59
106
54
61
50
77
65
99
58
72
50
85
95
65
43
54
81
67
78
73
88
81
74
36
102
80
78
84
34
73
76

9.18
18.23
9.10
6.43
8.93
11.01
13.06
2.70
26.98
2.93
15.24
11.12
12.84
21.65
8.15
39.32
18.42
10.12
26.74
6.12
23.16
34.82
30.47
21.97
28.35
9.63
52.58
8.97
9.86
6.85
22.05
9.92
29.91
8.45
20.94
4.96
23.43
34.58
18.24
6.63
8.60
34.28
8.78
16.82
16.40
35.01
22.07
21.35
3.35
52.01
26.87
30.99
22.22
3.04
15.68
30.36

11.86
24.36
10.91
7.55
12.67
13.45
19.29
2.42
33.18
3.92
20.23
16.39
15.37
26.72
9.94
51.56
24.35
20.32
33.04
7.75
20.47
43.36
39.42
21.36
40.25
13.82
56.95
11.35
12.99
7.37
24.27
10.46
36.26
10.39
31.75
7.73
27.10
42.48
22.67
8.81
9.08
44.38
12.58
21.06
22.31
40.46
23.46
23.89
3.00
48.69
31.71
37.97
29.38
3.54
25.16
36.10

21

arXiv Template

A PREPRINT

Name
Instance name

BPP_200_75_0.2_0.7_6
BPP_200_75_0.1_0.8_5
BPP_200_50_0.2_0.8_0
BPP_200_50_0.2_0.8_9
BPP_200_120_0.1_0.7_2
BPP_200_50_0.2_0.8_6
BPP_200_50_0.2_0.8_4
BPP_200_50_0.2_0.8_7
BPP_200_75_0.1_0.8_2
BPP_200_100_0.1_0.7_6
BPP_200_100_0.1_0.8_0
BPP_200_75_0.1_0.7_0
BPP_200_100_0.2_0.8_4
BPP_200_75_0.2_0.8_4
BPP_200_75_0.2_0.7_0
BPP_200_75_0.1_0.8_6
BPP_200_75_0.1_0.8_0
BPP_200_75_0.1_0.7_8
BPP_200_75_0.2_0.8_1
BPP_200_75_0.1_0.7_6
BPP_200_75_0.1_0.8_3
BPP_200_120_0.1_0.7_1
BPP_200_75_0.2_0.8_9
BPP_200_120_0.1_0.8_3
BPP_200_120_0.1_0.7_4
BPP_200_100_0.2_0.7_9
BPP_200_100_0.1_0.8_1
BPP_200_100_0.1_0.7_1
BPP_200_75_0.1_0.7_4
BPP_200_75_0.2_0.7_1
BPP_750_300_0.1_0.7_7
BPP_750_300_0.1_0.7_8
BPP_750_300_0.1_0.8_2
BPP_750_300_0.1_0.8_3
BPP_750_300_0.1_0.8_7
BPP_750_300_0.2_0.7_6
BPP_750_300_0.2_0.7_7
BPP_750_300_0.2_0.7_8
BPP_750_300_0.1_0.7_5
BPP_750_300_0.1_0.7_2
BPP_750_300_0.2_0.7_1
BPP_750_300_0.1_0.7_4
BPP_750_300_0.1_0.7_9
BPP_750_300_0.2_0.7_0
BPP_750_300_0.2_0.7_2
BPP_750_300_0.2_0.7_3
BPP_750_300_0.2_0.7_4
BPP_750_300_0.1_0.7_6
BPP_750_300_0.2_0.7_0
BPP_750_300_0.2_0.7_2
BPP_750_300_0.2_0.7_5

Iterations
Greedy_iter RL_iter Expert_iter Greedy_time RL_time Expert_time Greedy_obj RL_obj Expert_obj

Objective value

Time

5.82
22.59
2.90
2.44
32.88
2.64
2.88
3.23
20.13
21.39
48.24
11.65
18.17
12.03
8.86
17.78
21.46
9.84
14.93
12.27
13.84
34.34
12.43
50.27
22.68
21.21
46.64
19.44
8.78
5.70
431.99
438.36
984.78
992.71
900.67
348.74
374.72
343.14
553.77
424.71
324.76
424.10
576.85
291.99
289.76
347.74
299.66
452.20
294.21
298.91
271.92

7.85
28.18
4.70
4.16
58.13
4.54
4.24
3.90
30.15
37.84
70.80
20.31
30.14
20.98
12.56
27.62
32.58
19.50
20.73
16.06
19.95
51.44
17.49
81.97
45.75
27.41
69.57
27.35
19.97
9.00
761.27
868.59
1575.72
1487.61
1466.03
695.31
761.56
643.76
991.46
723.86
513.24
839.43
867.30
602.91
719.85
667.53
567.41
755.89
618.61
749.31
508.35

94.45
89.04
98.33
108.50
80.49
101.50
105.50
102.00
87.01
81.77
92.00
83.04
112.00
102.73
89.60
84.92
88.28
77.35
101.83
82.75
105.50
82.28
94.89
89.24
78.44
94.59
92.69
78.83
80.44
86.87
299.97
301.65
344.83
333.55
334.83
339.59
346.00
339.14
302.62
308.22
343.90
299.13
303.05
338.58
344.99
333.32
343.59
302.22
338.58
344.99
335.84

94.45
89.48
98.33
108.50
81.01
101.50
105.50
102.00
87.31
82.77
92.67
83.35
112.00
102.73
90.05
85.28
88.92
77.48
101.83
82.68
105.50
82.78
94.92
89.80
79.64
94.92
92.90
78.92
81.72
87.82
299.85
302.74
346.43
334.33
336.11
341.83
346.91
340.80
305.25
310.59
343.73
301.11
301.99
337.66
347.38
332.97
343.89
303.04
337.66
347.38
337.90

94.45
89.34
98.33
108.50
80.66
101.50
105.50
102.00
87.02
82.23
92.17
82.97
112.00
102.73
89.65
84.83
88.43
77.21
101.92
82.81
105.50
82.28
94.89
89.14
78.58
94.59
92.97
79.06
80.37
86.80
298.28
301.51
344.20
333.36
335.26
343.53
345.16
340.28
302.32
307.71
344.31
298.10
301.48
336.33
345.10
333.40
343.54
300.90
336.33
345.10
335.33

52
99
41
36
107
37
38
36
96
97
126
73
78
87
70
81
89
56
87
74
59
104
67
118
96
97
117
69
66
59
230
288
385
330
397
377
302
266
262
282
251
252
251
264
330
291
304
254
264
330
239

45
84
32
28
82
30
31
35
80
72
111
59
66
65
59
72
80
52
74
61
59
86
64
100
60
86
112
65
48
43
215
215
329
327
317
194
202
190
225
218
215
213
266
199
206
223
201
221
199
206
190

44
74
37
33
91
36
34
32
82
81
109
68
70
75
61
74
83
64
72
57
57
84
63
104
73
79
112
61
67
46
231
252
331
316
321
226
241
213
243
225
213
248
255
240
284
258
224
224
240
284
209

6.17
27.22
3.57
3.06
46.05
3.23
3.30
3.00
25.07
31.30
56.03
14.63
21.59
17.02
10.48
20.60
24.40
10.33
17.77
14.43
13.11
43.51
21.74
61.68
41.30
23.71
48.07
20.25
12.80
8.50
465.58
642.50
1222.12
987.16
1238.79
924.06
652.23
555.75
676.38
607.61
393.61
526.86
518.30
436.00
595.43
496.82
539.88
545.02
437.81
595.81
398.43

22

