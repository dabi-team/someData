0
2
0
2
c
e
D
0
3

]
E
S
.
s
c
[

2
v
0
6
0
1
1
.
2
1
0
2
:
v
i
X
r
a

Adversarial Patch Generation
for Automatic Program Repair

Abdulaziz Alhefdhi
University of Wollongong
aa043@uowmail.edu.au

Hoa Khanh Dam
University of Wollongong
hoa@uow.edu.au

Xuan-Bach D. Le
The University of Melbourne
bach.le@unimelb.edu.au

Aditya Ghose
University of Wollongong
aditya@uow.edu.au

Abstract—Automatic program repair (APR) has seen a growing interest in recent years with
numerous techniques proposed. One notable line of research work in APR is search-based
techniques which generate repair candidates via syntactic analyses and search for valid repairs
in the generated search space. In this work, we explore an alternative approach which is inspired
by the adversarial notion of bugs and repairs. Our approach leverages the deep learning
Generative Adversarial Networks (GANs) architecture to suggest repairs that are as close as
possible to human generated repairs. Preliminary evaluations demonstrate promising results of
our approach (generating repairs exactly the same as human ﬁxes for 21.2% of 500 bugs).

INTRODUCTION

Software bugs are costly to be detected and
rectiﬁed [1], [2], [3]. Due to short time to mar-
ket, software programs are often delivered with
known or unknown bugs. [4]. The number of
bugs may be far more than the amount of hu-
man resources available to address them. As a
result, it can take days or even years for software
defects to be repaired [3]. Automated approaches
to detecting and rectifying software bugs are thus
of tremendous value to reduce these costs.

Automatic program repair (APR), is now an
active and exciting research area, which en-
gages both academia and the software industry.
Real-world defects from large programs have

been shown to be efﬁciently and effectively re-
paired by state-of-the-art APR techniques [5].
Notably, in 2018, Facebook announced the ﬁrst-
ever
industrial-scale APR technique, namely
GetAFix [6], being developed and widely used
in house. GetAFix was directly inspired from a
recent research work in APR [7], demonstrating
that research in APR has a great potential to make
practical and immediate impact.

APR can generally be divided to two main
families, namely search-based and semantics-
based approaches. While semantics-based ap-
proaches use semantic analyses such as static
analyses or symbolic execution [8], [9], search-
based approaches often use syntactic analyses to
generate and traverse a syntactic search space

1

 
 
 
 
 
 
such as genetic programming [5], pattern recog-
nition [7], and machine learning [10]. Semantics-
based approaches are typically precise, but lim-
ited by the capability of
the underlying se-
mantic analysers, (e.g., symbolic execution [8],
[9]). Search-based approaches typically generate
a large syntactic search space, rendering it difﬁ-
cult to navigate through to ﬁnd correct solutions.
Both APR families however suffer from the
same issue, namely overﬁtting, in which gener-
ated repairs fail to generalise beyond the speci-
ﬁcations (e.g., test suite) used for generating the
repairs, hindering the efﬁciency and effectiveness
of APR [11], [12]. The overﬁtting problem occurs
mainly due to the fact that software speciﬁcations
such as test suites are known to be incomplete in
practice. They do not comprehensively coverall
unintended behaviours of a software program.
Hence, using those approaches in practice re-
quires additional resources beyond the incomplete
speciﬁcations to reliably generate and suggest a
few high-quality candidate patches for developers
to review in a timely manner.

The availability of millions of

software
projects (e.g. over 17 millions on GitHub) has
given us an unprecedented opportunity in ﬁxing
software defects and vulnerabilities. A massive
amount of repositories of data about defects,
vulnerabilities and code patches has enabled us to
automatically learn and discover patterns of vul-
nerabilities and their patches. These empower us
to develop Artiﬁcial Intelligence/Machine Learn-
ing (AI/ML) algorithms that are capable of au-
tomatically learning patterns of bug ﬁxes and
suggesting ﬁxes for newly discovered bugs.

Traditional machine learning techniques have
been utilised for patch ranking (e.g. [13]). Recent
work (e.g. [14], [15]) has started exploring to
apply breakthroughs in deep learning, particu-
larly neural machine translation (NMT), to APR.
They often formulate APR as an NMT problem:
translating buggy code into ﬁxed code. Most
of them use a sequence-to-sequence translation
model which typically consists of two main com-
ponents: an encoder which takes as input a buggy
code sequence, and a decoder which generates a
corresponding ﬁxed code sequence. Those models
often rely on the maximum likelihood estimation
principle for training (i.e. maximising the prob-
ability of the target ground-truth code sequence

conditioned on the source sequence). Hence, ﬁxes
generated by those models might not be natural
and correct with respect
to human generated
ﬁxes. In this article, we present a new approach,
which leverages the deep learning Generative
Adversarial Networks (GANs) architecture [16],
to automatically generate bug ﬁxes.

Adversarial Patch Generation

Many adversarial scenarios exist in life. For
example, a counterfeiter tries to deposit fake
money while a bank teller tries to detect it. The
counterfeiter tries to bring more realistic money
each time while the bank teller tries to improve
their detection skills. These adversarial settings,
whether initiated intentionally or unintentionally,
can be exploited for learning and development
purposes. The well-known deep learning architec-
ture, Generative Adversarial Networks (GANs),
was introduced in 2014 by Goodfellow et al.
[16] to explicitly address this. This architecture
consists of two neural networks1: a generator and
a discriminator. The former aims to generate new
data examples from existing data, while the latter
aims to detect the authenticity of the examples.
Both of them are connected and trained together
in a zero-sum game style. The generator tries to
fool the discriminator into thinking that the data
it generates are “real”, meaning that it is a data
point from the dataset under experimentation.
The discriminator tries to distinguish between
“fake” data produced by the generator and real
data from the dataset. The adversarial setting of
GANs allows for the two networks to learn from
each other, and consequently improve their own
performance over time.

Bugs and patches can be seen as adversaries in
which bugs are introduced into code and patches
are provided to remove the existence of those
bugs. Thus, we leverages the ideas of GANs
to develop a system which aims to generate
patches which are as close as possible to human-
generated ﬁxes. This system consists of two main
components: Patch Generator and Patch
Discriminator (see Figure 1). The Patch
Generator takes as input a buggy code frag-

1The mechanism in which neural networks operate is inspired
by how the human brain and its networks of neurons process
information. An artiﬁcial neural network consists of layers of
nodes connected to each other by weights.

2

Figure 1. The overall workﬂow of our approach. In the training phase, the Patch Discriminator is used to
help the Patch Generator learn to imitate human ﬁxes. The trained Patch Generator is then used in the
production phase to generate patches for buggy code.

t.setAutoFlush(false);) and generates a
ment (e.g.,
new code fragment in which the bug (hopefully)
has been ﬁxed (e.g. t.setAutoFlush(true);). We adapt
the Conditional GANs (cGANs) architecture [17]
(an extension of GANs) to develop the Patch
Generator since cGANs allows for the output
(i.e., patched code) to be conditioned on a certain
input (i.e., buggy code).

the

On

the

other

hand,

Patch
Discriminator takes as input a pair of
buggy code and patched code fragments and
determines if the patched code is provided by
human developers (assuming that it was correctly
conﬁrmed by them) or artiﬁcially generated.
The key novelty here is that both the Patch
Generator and Patch Discriminator
are trained together
they are
competing with one another (i.e., one’s loss is
another’s gain). This mechanism enables the
Patch Generator to learn to produce ﬁxes
that resemble the way that human developers ﬁx
bugs.

in a way that

the

the

We

have

using

implemented

Patch
Generator
encoder-decoder
architecture for sequence-to-sequence learning
[18]. The encoder and decoder are two Long
Short-Term Memory (LSTM) networks [19].
Patch Discriminator was
also
The
implemented using LSTMs, but with sigmoid

code

Input

fragments

are ﬁrst
activation.
tokenised (based on the syntax and grammar in
which the code was written) into code sequences
as input
to our model. We also implemented
an attention mechanism [20] for an enhanced
input-output unit-mapping. To aid our model in
capturing the ground truth distribution, random
noises were introduced by applying dropout [21]
to the Patch Generator’s layers [22].

To train our model, we used the BigFix dataset
[14], which consists of pairs of buggy code and
the corresponding human-provided ﬁxed code.
We removed duplicate pairs and collected only
single-line ﬁxes (as done in previous work [14],
[15]). Our ﬁnal dataset consists of 5,749 buggy-
ﬁxed pairs, 500 of which were used for evaluation
and the rest was used for training the model. Dur-
ing training, each buggy code fragment is input
to the Patch Generator, which generates a
candidate repair for the buggy code. The Patch
Discriminator then examines two pairs. The
ﬁrst pair consists of the buggy code and the code
ﬁxed by human developers (collected from the
dataset, e.g., t.setAutoFlush(false, true);). The second
pair consists of the same buggy code and the
code generated by the Patch Generator. The
Patch Discriminator is trained to deter-
mine which pair contains a human ﬁx and which
pair contains an artiﬁcially generated ﬁx.

3

Table 1. Sample ﬁxes generated by our model compared with the human ﬁxes

Buggy Code Fragment

Human-Suggested Fix

Model-Generated Fix

Notes

return new

return new

return new

- Identical ﬁx

HiveQueryResultSet.Builder()

HiveQueryResultSet.Builder(null)

HiveQueryResultSet.Builder(null)

detectDeadlock(e, "unlock");

detectDeadlock(dbConn, e, "unlock" );

detectDeadlock(dbConn, e, +++e);

partial

- Provides
adding the dbConn argument
- Syntax error

solution:

Utilities.clearWorkMap();

Utilities.clearWorkMap(jconf);

Utilities.clearWorkMap(jc);

Processor childProcessor =

Processor childProcessor =

Processor childProcessor =

routeContext.createProcessor(this);

this.createChildProcessor(routeContext,

this.createChildProcessor(routeContext,

true);

false);

- Suggests different ﬁx: different
variable name.

- Similar ﬁx: solution guide/hint
- Semantic difference: false
instead of true

the

During

training,

Patch
Discriminator passes the gradient of the
loss (the degree to which it correctly classiﬁes
to the Patch
an artiﬁcial or human ﬁx)
Generator. The Patch Generator uses
this loss (combining with the loss calculated
from the difference between the generated
to improve its
ﬁxes and the ground truths)
learning and continues trying to “fool” the
Patch Discriminator by generating ﬁxes
as close as possible to human ﬁxes. Once
the training phase is ﬁnished, we use the
trained Patch Generator for bug ﬁxing and
discard the Patch Discriminator. The
Patch Discriminator’s job in out setting
is to help improve the Patch Generator
In production
in the patch-generation game.
trained Patch
phase
(see Figure 1),
Generator is presented with new buggy
code fragments (e.g., Processor childProcessor =
routeContext.createProcessor(this);), and it will suggest
patches
(e.g., Processor
childProcessor = this.createChildProcessor(routeContext,
false);).

for ﬁxing the bugs

the

EARLY RESULTS AND FUTURE
DIRECTIONS

The current experimentation of our approach
demonstrates promising results. Overall, in the
500 bugs used for evaluation, our model achieves
a Bleu-4 score of 0.42, generating 106 ﬁxes
(21.2%) that are identical to the human-ﬁxes (i.e.,
the ground truths). We present here (see Table 1)
a few examples of how our model generates ﬁxes
compared with the ﬁxes provided by software
developers.

The ﬁrst example in Table 1 demonstrates
that our model was able to generate ﬁxes that
are exactly the same as human-generated ﬁxes.
The patch correctly adds a parameter (null) to

4

the constructor. The remaining three examples in
Table 1 represent cases where the generated ﬁxes
provide partial solutions, which are slightly dif-
ferent from the human ﬁxes. They offer valuable
insight for further improvements and deployment
of our approach. We discuss them in detail below.

In the second example, our model was able
the addition of the argu-
to correctly suggest
ment dbConn to ﬁx the bug, but
it changes
(from unlock to +++e),
the last argument
rendering a syntactical error. These errors can
be detected by employing a program analysis
(in a similar way like a compiler) ﬁlter which
checks for the syntactical correctness of the sug-
gested patches. This ﬁltering functionality can be
added as a post-processing step of the Patch
Generator to ensure that syntactically incor-
rect candidate patches are removed. Alternatively,
this functionality can be added to the Patch
Discriminator to augment its capability of
detecting syntactically incorrect patches (and the
Patch Generator will indirectly beneﬁt from
it due to the adversarial learning setting we setup).

The third example demonstrates a case where
the generated patch is similar to the human ﬁx in
suggesting the addition of an argument to func-
tion clearWorkMap(). However, they differ in
terms of the variables used: our model suggested
to use variable jc, while the human developer
used jconf. If jc does not exist, the ﬁltering
mechanism above can easily detect and remove
this candidate patch. However, if jc does exist,
this suggests another improvement direction for
our work. We need to widen our input scope
and capture the context of the surrounding code,
especially in this case, the deﬁnition-use depen-
dencies between variables. Encoding this context
into the Patch Generator and/or the Patch
Discriminator will help improve the correct-

ness of generated patches.

Finally,

the last example demonstrates that
our model was capable of generating complex
ﬁxes, which involve multiple correct changes
adding this, providing the
(e.g.,
correct
method createChildProcessor instead of
createProcessor), and providing the correct
number of arguments passed to the method with
a correct ﬁrst argument). Compared to state-of-
the-art repair techniques such as [5], [7], [9], this
result is impressive as the ﬁx involves multiple
actions/mutations altogether. The model however
incorrectly suggested the value of the last argu-
ment (false instead of true). Although this
looks simple, it requires the capability of un-
derstanding the semantics of the code and its
intended behaviour. Semantics-based approaches
[8], [9]) rely on some underlying se-
(e.g.,
mantic analysers (e.g., symbolic execution) to
provide this capability. We plan to explore how
to combine our approach with semantics-based
approaches to better combine both syntactic and
semantic reasoning.

In summary, we have presented in this article
an AI-powered approach to automatic program re-
pair. Our approach leverages the notions of Gen-
erative Adversarial Networks (which naturally
match with the adversarial settings of bugs and
repairs) to develop a new, adversarial patch gener-
ation framework. Our approach is automated as it
does not require hard-coding of bug-ﬁxing rules.
In addition, our solution does not require a set of
test cases, enabling early repairs and interventions
of software bugs (thus saving the signiﬁcant cost
increase later). Furthermore, our approach does
not perform enumerative search for repairs, but
learns to ﬁx a common class of bugs directly
from patch examples. Thus, we can offer a fast
and scalable solution that can generalize across
software applications. Promising results from our
preliminary evaluation not only suggests the fea-
sibility of deploying our approach in practice,
but also generate insights for future directions for
further enhancements.

2. E. Engstr ¨om and P. Runeson, “A qualitative survey of re-

gression testing practices,” in International Conference

on Product Focused Software Process Improvement.

Springer, 2010, pp. 3–16.

3. M. B ¨ohme and A. Roychoudhury, “Corebench: Study-

ing complexity of regression errors,” in Proceedings of

the International Symposium on Software Testing and

Analysis, 2014, pp. 105–115.

4. J. Anvik, L. Hiew, and G. C. Murphy, “Coping with an

open bug repository,” in Proceedings of the OOPSLA

workshop on Eclipse technology eXchange, 2005, pp.

35–39.

5. W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest,

“Automatically ﬁnding patches using genetic program-

ming,” in 31st International Conference on Software

Engineering.

IEEE, 2009, pp. 364–374.

6. J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getaﬁx:

Learning to ﬁx bugs automatically,” Proceedings of the

ACM on Programming Languages, vol. 3, no. OOPSLA,

pp. 1–27, 2019.

7. X. B. D. Le, D. Lo, and C. Le Goues, “History driven pro-

gram repair,” in 23rd International Conference on Soft-

ware Analysis, Evolution, and Reengineering, vol. 1.

IEEE, 2016, pp. 213–224.

8. S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix:

Scalable multiline program patch synthesis via symbolic

analysis,” in Proceedings of the 38th international con-

ference on software engineering, 2016, pp. 691–701.

9. X.-B. D. Le, D.-H. Chu, D. Lo, C. Le Goues, and

W. Visser, “S3: syntax-and semantic-guided repair syn-

thesis via programming by examples,” in Proceedings

of the 11th Joint Meeting on Foundations of Software

Engineering, 2017, pp. 593–604.

10. F. Long and M. Rinard, “Automatic patch generation by

learning correct code,” in Proceedings of the 43rd An-

nual ACM SIGPLAN-SIGACT Symposium on Principles

of Programming Languages, 2016, pp. 298–312.

11. X. B. D. Le, F. Thung, D. Lo, and C. Le Goues, “Over-

ﬁtting in semantics-based automated program repair,”

Empirical Software Engineering, vol. 23, no. 5, pp.

3007–3033, 2018.

12. E. K. Smith, E. T. Barr, C. Le Goues, and Y. Brun, “Is the

cure worse than the disease? overﬁtting in automated

program repair,” in Proceedings of the 10th Joint Meet-

ing on Foundations of Software Engineering, 2015, pp.

REFERENCES

532–543.

1. A. K. Onoma, W.-T. Tsai, M. Poonawala, and H. Sug-

13. F. Long, P. Amidon, and M. Rinard, “Automatic inference

anuma, “Regression testing in an industrial environ-

of code transforms for patch generation,” in Proceedings

ment,” Communications of the ACM, vol. 41, no. 5, pp.

of the 11th Joint Meeting on Foundations of Software

81–86, 1998.

Engineering, 2017, pp. 727–739.

5

University of Wollongong (UOW) in Australia. He is
Associate Director for the Decision System Lab at
UOW, heading its Software Analytics research pro-
gram. His research interests lie primarily in the inter-
section of Software Engineering and Artiﬁcial Intelli-
gence (AI). He develops AI solutions for project man-
agers, software engineers, QA and security teams
to improve software quality/cybersecurity and ac-
celerate productivity. His research also focuses on
methodologies and techniques for engineering au-
tonomous AI multi-agent systems.

Xuan-Bach D. Le is currently a Lecturer at
the
University of Melbourne, Australia. Previously, he
was a postdoc at Carnegie Mellon University, work-
ing with ACM Distinguished Scientist and Associate
Prof. Corina Pasareanu. His research interests span
software engineering and programming languages,
including: software mining, empirical software engi-
neering, program analysis, repair, synthesis, and ver-
iﬁcation. His work, History Driven Program Repair,
inspired/inﬂuenced GetaFix by Facebook,
the ﬁrst
ever automated repair tool deployed at large scale
codebase in the industry.

Aditya Ghose is Professor of Computer Science
at the University of Wollongong. He leads a team
conducting research into knowledge representation,
agent systems, services, business process manage-
ment, software engineering and optimisation and
draws inspiration from the cross-fertilisation of ideas
from this spread of research areas. He works closely
with some of the leading global IT ﬁrms. Ghose is
President of the Service Science Society of Australia
and served as Vice-President of CORE (2010-2014),
Australia’s apex body for computing academics. He
holds PhD and MSc degrees in Computing Science
from the University of Alberta, Canada (he also spent
parts of his PhD candidature at the Beckman Institute,
University of Illinois at Urbana Champaign and the
University of Tokyo) and a Bachelor of Engineering
degree in Computer Science and Engineering from
Jadavpur University, Kolkata, India.

14. Y. Li, S. Wang, and T. N. Nguyen, “Dlﬁx: Context-based

code transformation learning for automated program

repair,” in Proceedings of the ACM/IEEE 42nd Interna-

tional Conference on Software Engineering, 2020, pp.

602–614.

15. Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet,

D. Poshyvanyk, and M. Monperrus,

“Sequencer:

Sequence-to-sequence learning for end-to-end pro-

gram repair,” IEEE Transactions on Software Engineer-

ing, 2019.

16.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,

D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,

“Generative adversarial nets,” in Advances in neural

information processing systems, 2014, pp. 2672–2680.

17. M. Mirza and S. Osindero, “Conditional generative ad-

versarial nets,” arXiv preprint arXiv:1411.1784, 2014.

18.

I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to

sequence learning with neural networks,” in Advances

in neural

information processing systems, 2014, pp.

3104–3112.

19. S. Hochreiter and J. Schmidhuber, “Long short-term

memory,” Neural computation, vol. 9, no. 8, pp. 1735–

1780, 1997.

20. D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine

translation by jointly learning to align and translate,”

arXiv preprint arXiv:1409.0473, 2014.

21. Y. Gal and Z. Ghahramani, “A theoretically grounded

application of dropout in recurrent neural networks,”

Advances in neural

information processing systems,

vol. 29, pp. 1019–1027, 2016.

22. P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-

image translation with conditional adversarial networks,”

in Proceedings of the IEEE conference on computer

vision and pattern recognition, 2017, pp. 1125–1134.

Abdulaziz Alhefdhi is a PhD candidate in Software
Engineering and Computer Science at the School of
Computing and Information Technology, University of
Wollongong (UOW), Australia. He is also with the
department of Computer Science at Prince Sattam
bin Abdulaziz University (PSAU), Saudi Arabia. Al-
hefdhi received his Bachelor’s and Master’s degrees
in Computer Science form Imam Mohammad Ibn
Saud Islamic University (IMSIU), Saudi Arabia and
the University of Queensland (UQ), Australia, respec-
tively. He is a part of the Decision Systems Lab (DSL)
in UOW. His research interests include Software An-
alytics and AI-empowered Software Engineering.

Hoa Khanh Dam is Associate Professor in the
School of Computing and Information Technology,

6

