Disengagement Cause-and-Effect Relationships
Extraction Using an NLP Pipeline

Yangtao Zhang, X. Jessie Yang, Feng Zhou

1

1
2
0
2

v
o
N
5

]

C
H
.
s
c
[

1
v
1
1
5
3
0
.
1
1
1
2
:
v
i
X
r
a

Abstract—The advancement in machine learning and artiﬁ-
cial
intelligence is promoting the testing and deployment of
autonomous vehicles (AVs) on public roads. The California
Department of Motor Vehicles (CA DMV) has launched the
Autonomous Vehicle Tester Program, which collects and releases
reports related to Autonomous Vehicle Disengagement (AVD)
from autonomous driving. Understanding the causes of AVD is
critical to improving the safety and stability of the AV system and
provide guidance for AV testing and deployment. In this work,
a scalable end-to-end pipeline is constructed to collect, process,
model, and analyze the disengagement reports released from 2014
to 2020 using natural language processing deep transfer learning.
The analysis of disengagement data using taxonomy, visualization
and statistical tests revealed the trends of AV testing, categorized
cause frequency, and signiﬁcant relationships between causes and
effects of AVD. We found that (1) manufacturers tested AVs
intensively during the Spring and/or Winter, (2) test drivers
initiated more than 80% of the disengagement while more than
75% of the disengagement were led by errors in perception,
localization & mapping, planning and control of the AV system
itself, and (3) there was a signiﬁcant relationship between the
initiator of AVD and the cause category. This study serves as a
successful practice of deep transfer learning using pre-trained
models and generates a consolidated disengagement database
allowing further investigation for other researchers.

Index Terms—Autonomous vehicles, disengagement, cause-
and-effect extraction, natural language processing, deep transfer
learning.

I. INTRODUCTION

T He advancement of machine learning and artiﬁcial intel-

ligence is bringing Autonomous Vehicles (AVs) closer to
the roads with potential beneﬁts to save people’s lives, smooth
trafﬁc ﬂow, improve transportation efﬁciency, reduce energy
consumption, and so on [1]. According to the deﬁnition from
the Society of Automotive Engineers (SAE) [2], there are six
levels of diving automation from Level 0 (No Driving Automa-
tion) to Level 5 (Full Driving Automation). While manufactur-
ers are making the effort to deliver Level 5 AVs in the future,
most AVs driving or testing on public roads nowadays are SAE
Level 2 (partial automation), Level 3 (conditional automation),
and Level 4 (high automation in geofenced areas) vehicles.
Particularly, SAE Level 3 automation does not require the

Y. Zhang is with School of

Information, The University of Michi-
gan, Ann Arbor, 500 S State St, Ann Arbor, MI 48109 USA (e-mail:
maxzhang@umich.edu).

X. J. Yang is with Industrial and Operations Engineering, The University
of Michigan, Ann Arbor, 500 S State St, Ann Arbor, MI 48109 USA (e-mail:
xijyang@umich.edu).

F. Zhou is with the Department of Industrial and Manufacturing, Systems
Engineering, The University of Michigan, Dearborn, 4901 Evergreen Rd.
Dearborn, MI 48128 USA (e-mail: fezhou@umich.edu).

Manuscript received October 9, 2021; revised xxx 26, 2021.

human driver to monitor the driving environment [2], which
allows the driver to shift attentional resources to non-driving-
related tasks [3]. Even though the driver is required to be ready
to take over control whenever requested, s/he may stay out of
the control loop for a prolonged period, thus not monitoring
the driving situation [4], [5], [6], [7], [8]. The transition of
control from the AV to the human driver, also known as the
Autonomous Vehicle Disengagement (AVD), plays a vital role
in the Level 3 AV testing and deployment. Understanding the
cause-and-effect relationships of the AVD not only helps to
evaluate the safety and performance of the current AV systems,
but also provides guidelines on redesign of the AV systems for
improvement and the future regulations for AVs.

Generally speaking, investigations into AVD can be divided
into two categories, 1) experimental studies in takeover tran-
sitions in driving simulators and 2) analysis of naturalistic
studies. In the ﬁrst category, many researchers examined the
inﬂuences of critical factors on takeover performance, such
as takeover lead time (e.g., [9]), non-driving related tasks
(e.g., [10], [11]), trafﬁc densities (e.g., [9], [12]), emotions
[6], and drivers’ characteristics (e.g., [10], [13]). This type
of studies linked drivers’ behavior to well-controlled takeover
transitions and examined how critical human factors inﬂuenced
takeover performance. For example, Du et al. [6] found that
positive emotions (valence) led to better takeover quality, while
inﬂuence takeover time. However, due to
arousal did not
the gap between driving simulation and naturalistic driving,
their ﬁndings may only be transferred to real world in a
speciﬁc scope with limitations. For example, it was found
that participants’ perceived risk in driving simulator was lower
compared to that in naturalistic driving [14].

The second category of studies focused on analyzing
takeover transitions in naturalistic driving, particularly by ex-
amining the disengagement reports released by the California
Department of Motor Vehicles (CA DMV) using statistical
methods (e.g., [15], [16], [17]). The CA DMV disengagement
dataset consisted of speciﬁc disengagement reports in natu-
ralistic driving submitted by testing companies from 2014 to
2020, covering important data ﬁelds, such as location, initiator,
description of disengagement, which was perfect for analyzing
the cause-and-effect relationships of AVD. For example, Boggs
et al. [16] explored ﬁve W questions (i.e., who, what, when,
where, and why) of AVD using CA DMV disengagement
reports from September 2014 to November 2018. They found
that 1) the test operators were more likely to initiate the
disengagement on streets and roads than on freeways and inter-
states, 2) the major causes to system-initiated disengagement
were software, hardware, and planning issues, and 3) system-

 
 
 
 
 
 
initiated disengagement marginally increased compared to
operator-initiated disengagement despite the advancement in
AV technologies between 2014 and 2018. However, in order
to conduct such studies, laborious manual processing of the
reports is needed.

In consideration of the pros and cons of the two types of
studies, in this paper, we built a natural language processing
(NLP) pipeline to extract cause-and-effect relationships from
disengagement reports of the most updated CA DMV dataset.
This NLP pipeline promised 1) to process various formats
of disengagement data at scale, 2) to analyze two types of
disengagement on future incoming reports, and 3) to provide
implications on AV testing and deployment without human
manual work. To achieve those goals,
the NLP pipeline
utilized deep transfer learning, which improved the learning
of a new task, i.e., cause-and-effect relationship extraction of
ADV “through the transfer of knowledge from related natural
language understanding tasks that have already been learned”
[18]. By doing so, our deep learning model achieved human-
level performance with only a small portion of manual labeled
data.

As a summary, the contributions of this study are:
• We proposed an NLP pipeline to process and analyze
AVD reports at scale with satisfactory performance.
• We built an NLP deep learning model that was ﬁrst pre-
trained on a large-scale natural language corpus (e.g.,
Wikipedia, Google News), then ﬁne-tuned on a task-
speciﬁc dataset (SemEval-2010 Task 8 [19]) extended by
Li et al. [20] to include embedded causality [21], and
ﬁnally post-trained on the CA DMV dataset.

• We developed a taxonomy of the causes and effects of
AVD with a focus on how the AV system worked and
interacted with other factors.

• We identiﬁed the signiﬁcant relationships between causes
and initiators of AVD and analyzed contributions of
causes of two types of AVD, which provided valuable
insights into the current issues existing in the AV system
and potential improvements for manufacturers to improve
AV safety.

II. RELATED WORK

A. Experiment Studies in Driving Simulators

Many previous studies conducted driving simulation to
investigate the inﬂuence of individual factors on takeover
performance in takeover transitions and analyzed driver be-
haviors under various controlled scenarios related to AVD. For
example, Du et al. [9] examined the effects of three factors on
takeover performance, including drivers’ cognitive workload,
takeover time budget, and trafﬁc density. They found that
takeover performance was worse when the participants had a
high level of cognitive workload, the time budget was small,
and the oncoming trafﬁc density was heavy. Wandtner et
al. [11] investigated the effects of non-driving related tasks
on takeover performance and found that visual-manual tasks
signiﬁcantly worsened takeover performance compared to
auditory-vocal tasks. Clark and Feng [13] found that younger
(age 18-35, n = 17) and older drivers (age 62-81, n = 18)

2

showed different behaviors in simulated automated driving,
such as different preferences for non-driving related tasks,
different takeover reaction time, and signiﬁcant impacts on
driving performance. Gold et al. [4] designed takeover exper-
iments, identiﬁed related variables, and developed regression
models to evaluate takeover performance. Their study revealed
the effects of key factors, such as time budget, trafﬁc density,
and repetition, on takeover performances. Markkula et al. [22]
created simulation-ready human behavior models to reproduce
qualitative patterns of important scenarios like “an AV handing
over control to a human driver in a critical rear-end situation”.
With driving simulations, their models allowed optimization of
AV impacts on safety.

These experimental studies based on driving simulation
discovered various ﬁndings of takeover transitions, human
factors, and potential causes of AVD. And the ﬁndings may
be transferred to open roads to help improve the transition
process of control from the AV system to the human driver.
Even though Eriksson et al. [23] have found strong positive
correlation between the driving simulation and on-road con-
ditions for control transitions in terms of workload, perceived
usefulness and satisfaction, on-road validation is still necessary
for those results discovered through driving simulation. Not
only because the AV system used in driving simulation cannot
reproduce the driving experience provided by various AV
systems developed by different manufacturers, but also only
a small number of individual factors were considered in the
models developed on the experimental data from simulators
[4], [23]. Furthermore, the number of participants for driving
simulation tasks was mostly less than 100 and the number of
records of experimental data was mostly less than 1000 [4],
[23], [24], [25], [26], [27], [7]. The number of the participants
and the amount of the data may limit the generalizability and
scalability of those studies.

B. Naturalistic studies

With the availability of the CA DMV dataset, many re-
searchers focused on the exploration of the disengagement
reports using traditional statistical and machine learning mod-
els. For example, Favar`o et al. [15] analyzed the contributory
factors, disengagement frequencies of AVD with taxonomy
and statistical visualization of disengagement overview and
trends. Boggs et al. [16] used logistic regression to identify
and quantify who, what/why, where, and when (5 Ws) of
AVD, and they found that 1) it was more likely to have human
operator-initiated disengagement than system-initiated, 2) the
most frequent causes of disengagement of system-initiated
were planning and software/hardware discrepancies, 3) dis-
engagement was more likely to happen on local roads than
on expressways, and 4) regardless of system improvement,
the likelihood of system-initiated disengagement
increased
marginally compared to human operator-initiated. Wang et
al. [17] used multiple statistical modeling approaches and
classiﬁcation trees to quantitatively investigate the underlying
causes of AVD, and found that lacking a certain number of
sensors signiﬁcantly induced AVD. However, the researchers
in these studies had to manually process the reports in various

3

Fig. 1: Overview of the data pipeline from Stage 1 to Stage 4

formats, which was time-consuming and laborious. Thus, these
studies lacked the scalability to handle large-scale data and
cannot be applied on new incoming disengagement reports to
beneﬁt future analysis. On the other hand, other researchers
applied natural language processing techniques to process the
text data automatically. For example, Alambeigi et al. [28]
used probabilistic topic modeling to identify themes from
the CA DMV crash reports. Their ﬁndings emphasized the
safety concerns with transitions of AV system to human
control. However,
the number of crash reports was much
smaller (167 reports in [28]), compared to the disengagement
reports. Banerjee et al. [29] applied the data pipeline method
to process and analyze data from the system’s perspective
with over a million miles of cumulative autonomous miles,
5,328 disengagement reports, and 42 crash reports from 2014
and 2017. They found that 1) AVs were 15 - 4000 times
worse than human drivers in terms of accidents per cumulative
mile driven, 2) perception, decision, and control discrepancies
resulting from the AVs’ machine-learning-based system were
the primary causes of AVD, and 3) human operators of AVs
had to stay as alert as drivers of manual vehicles. Such a
method provide promises to automatically process the data
systematically. However, the NLP model was based on key-
words matching and voting, which could end up with many
unknown instances when assigning the causes to a speciﬁc

predeﬁned category.

III. NLP PIPELINE

A scalable end-to-end pipeline (Fig. 1) was constructed to
collect, process, model, and analyze the disengagement reports
with high efﬁciency and accuracy. The pipeline consisted of
four stages. At Stage 1, it collected multi-format disengage-
ment reports from the CA DMV disengagement database and
classiﬁed them into corresponding years. At Stage 2, it used
Optical Character Recognition (OCR) to extract information
from the PDF format reports and exported to CSV ﬁles. Then
those CSV ﬁles were ﬁltered, cleaned, parsed, and ﬁnally
labeled by human workers. At Stage 3, we proposed an NLP
deep learning model based on ELECTRA (Efﬁciently Learning
an Encoder that Classiﬁes Token Replacements Accurately)
[30] using transfer learning. In order to compare with other
popular NLP deep learning models, BERT [31], DistilBERT
[32], and XLNet [33] were also included. At Stage 4,
it
analyzed the disengagement database by creating a taxonomy,
summarizing results in visualization and statistical analysis.

A. Data Collection

In September 2014, the CV DMV initiated the Autonomous
Vehicle Tester Program which allowed permit-holding manu-
facturers to test AVs with a test operator in the driver seat on

public infrastructure [34]. The program also required permit
holders to track and submit disengagement reports when “their
vehicles need to disengage from the autonomous mode during
tests” and collision reports for “every collision involving one
of their vehicles” [34]. Since the establishment of the program,
the CA DMV has been releasing disengagement reports and
collision reports to the public on a yearly basis. These reports
consisting of thorough and speciﬁc raw data are ideal data
sources for investigating AVD.

The raw disengagement data were retrieved from the CA
DMV’s disengagement database, which included the disen-
gagement reports collected from manufacturers participating in
the Autonomous Vehicle Tester Program on a yearly basis. For
this study, the disengagement reports released from 2014 to
2020 were used as the initial data for processing and modeling.
However, the proposed method can be updated easily with new
data.

B. Data processing

Cleaning: The format requirement of the disengagement
reports has changed over time, including photoshopped non-
regulated formats across different manufactures between 2014
and 2017, standard PDF template in part of 2018, and consol-
idated CSV ﬁles between 2018 and 2020. For the latest CVS
format, it combined all the records together and released two
ﬁnal CSV ﬁles - Autonomous Vehicle Disengagement Reports
and Autonomous Mileage Reports, which were easy to process
and distribute to the public. For each record, the ﬁnal Au-
tonomous Vehicle Disengagement Reports contained 9 ﬁelds.
In order to resolve these differences across different formats,
we attempted to collect all the information in the previous
years (2014 - 2018) based on the latest format (2018 - 2020).
An OCR pipeline based on OpenCV [35], Tesseract [36] and
PyImageSearch [37] was built to extract texts from the PDF
ﬁles, compile them, and export them to CSV ﬁles. As the
PDF ﬁles submitted by various manufacturers were scanned,
they were subject to random scaling, rotation, and skew. To ﬁx
this problem, a standard template was used as the reference
to adjust other scanned disengagement reports to generate the
de-skewed versions of images. The bounding boxes were then
identiﬁed and the text within them were extracted. Next, raw
texts were cleaned, ﬁltered, and ﬁnally exported as CSV ﬁles.
Finally, we collected the disengagement reports in the last 7
years (2014 - 2020) and collected and inspected, manufacturer,
date, initiator, location and description were selected as the
ﬁelds for the consolidated disengagement database for further
analysis. Table I shows the sample data and format in the
database. N/A was introduced as the placeholder for missing
values to ensure the integrity of the database.

Filtering: The collected data with various formats required
further data preprocessing. The disengagement reports with
cause description less than ﬁve words were removed to reduce
the noise existed in the raw data. In addition, as suggested
by Boggs et al. [38], “Apple and Uber lacked a variation
among human-initiated disengagements” in terms of cause
description and those disengagement reports replicated the
same information hundreds of thousands of times. Therefore,
those records were excluded as well.

4

Fig. 2: Overview of entities in the database

Fig. 3: The number of ﬁltered disengagement reports of each year in the
consolidated database.

With all these preprocessing steps, a consolidated database
(Fig. 2) was generated. It contained information of four entities
- report, description, cause, and word. The four entities were
weaved together logically - reports had descriptions, causes
existed in descriptions, and causes were made of words. Fig.
3 shows the number of ﬁltered disengagement reports of each
year submitted by manufacturers in the database with a total
number of 14, 282 reports.

Human Insights: Two types of human insights were re-
quired for this study to better understand the dataset: cause-
and-effect relationship labeling and cause categorization. The
former was used to identify the causes and effects from the
disengagement reports for training the NLP deep learning
models and the latter contributed to classifying the causes into
proper categories. Three student workers, who had at least 3
months experience in this ﬁeld, provided the human insights.
For cause-and-effect relationships, both simple causality and
embedded causality were considered in this study. Following
the standard cause-and-effect tagging format [20], [21], the
Inside, Outside, Beginning (IOB) notations were used to label
the tokens. To be more speciﬁc, sufﬁx C - Cause, E - Effect and
CE - Embedded Cause were added to the IOB notations. For
example, B-C represents the beginning of a cause token while
I-E represents the interior of an effect token. Based on human
insights, each word in the description of disengagement was
tagged with labels and categories and an example is shown in
Table II.

For the cause category, three main categories (AV System,
Human Factors, Environmental Factors & Others) and nine
subcategories (Perception, Localization & Mapping, Planning,
Control, System General, AV Driver, Other Driver & Vehicle,
Environment, Other) were derived from the conceptual orga-

TABLE I: Sample data and format for collected reports.

5

Manufacturer Date
EasyMile

Initiator
11/30/2020 AV System Street

Location Description

Apple
Uber
Waymo
Tesla
Volkswagen

Test Driver

06/19/2019 AV System Street
Street
03/01/2018
Highway Disengage for a software discrepancy
09/01/2017 N/A
Follower Output Invalid
10/15/2016 AV System Freeway
Planner not ready
06/12/2015 N/A

N/A

A collision hazard in the environment ahead was detected by the software, which
triggered an emergency stop
Motion planning timed out
Precautionary Takeover or Operator Discretion

TABLE II: Sample sentence for IOB labelling and cause categorization. (B-E
represents the beginning of an effect, I-E represents the interior of an effect,
B-C represents the beginning of a cause, I-C represents the interior of a cause,
and O represents others. 2 means the cause belongs to the planning category).

C. Cause and Effect Modeling

Words
driver
disengagement
due
to
planning
discrepancy
in
the
determination
of
autonomous
vehicle
speed

2 - planning
2 - planning

Label Category
B-E
I-E
O
O
B-C
I-C
O
O
O
O
O
O
O

TABLE III: Annotation quality score for eight labels and worker quality score
for three workers.

Label

AQS

Worker ID WQS

0
1
2

0.9802
0.9751
0.9845

O
B-C
I-C
B-E
I-E
B-CE
I-CE

0.9942
0.9257
0.9352
0.9461
0.9184
0.9042
0.9331

nization highlighting trust development produced by Schaefer
et al. [39], the AV hierarchical control structure drawn by
Banerjee et al. [29], and the taxonomy developed by Boggs et
al. [16].

While different workers provided their insights, it was nec-
essary to aggregate those insights and to determine the ground
truth. The framework CrowdTruth [40] was used for ground
truth aggregation. It not only selected the most reliable labels
among workers but also offered useful metrics,
including
Worker Quality Score (WQS) and Annotation Quality Score
(AQS) to evaluate the performance of workers as well as the
quality of their annotations. As shown in Table III, the WQS
and AQS of the ground truth aggregation were higher than
0.9, which provided convincing support for the quality of the
human annotation of all the disengagement reports.

We proposed an NLP deep learning model based on
ELECTRA [30] using transfer learning. First, ELECTRA pre-
trained on Wikipedia and BooksCorpus consisting of 3.3
Billion tokens was imported. ELECTRA is a novel NLP deep
learning model that outperformed multiple existing techniques,
including BERT [31], DistilBERT [32], and XLNet [33], on
numerous natural language understanding benchmark with the
same computational resources. Second, in order to gain task-
speciﬁc knowledge for extracting cause-and-effect relation-
ships, ELECTRA was ﬁne-tuned on the SemEval-2010 Task 8
[19]).The SemEval-2010 Task 8 is a benchmark for multi-way
classiﬁcation of semantic relations between pairs of nominals.
In this dataset, we used the samples with annotated cause-and-
effect relationships to ﬁne-tune the ELECTRA model. Third,
ELECTRA was post-trained on our consolidated disengage-
ment dataset created in Stage 2 of the data pipeline to further
improve the performance of the models. In order to compare
with other popular NLP deep learning models, BERT [31],
DistilBERT [32], and XLNet [33] were also included. These
models were evaluated based on weighted F-1 scores and
the cost of computational resources to select the best model.
Among them, weighted F-1 scores calculated the average F-
1 scores among labels weighted by their support to handle
label imbalance existed in the training data and the cost of
computational resources was measured by the time needed to
train the models with the same conﬁgurations of the computer.

During the transfer learning process, the target task was
deﬁned with two different approaches to maximize the beneﬁts
of transfer learning. The ﬁrst approach utilized two same-type
pre-trained models with different heads for different purposes
- Model One was ﬁne-tuned for cause-and-effect relationship
extraction with a token classiﬁcation head and Model Two was
ﬁne-tuned for cause category classiﬁcation with a sequence
classiﬁcation head. The two models were chained together,
so that the extracted causes from Model One were fed into
Model Two directly to obtain their categories. This approach
allowed Model One to beneﬁt from post-training but dropped
the sentence context for Model Two. The second approach
only involved one end-to-end model. The tagged labels and
categories were combined (Table IV shows how the combi-
nation works) to satisfy the requirements of the end-to-end
model. Compared with the ﬁrst approach, the second approach
was more computationally efﬁcient and enabled the usage of
sentence context for category prediction.

TABLE IV: How labels and categories were combined.

TABLE V: Weighted F-1 score and training time for cause-and-effect rela-
tionships extraction.

6

Words
driver
disengagement
due
to
planning
discrepancy

Label
B-E
I-E
O
O
B-C
I-C

Category

Combined Label
B-E
I-E
O
O

2 - planning B-C-2
I-C-2
2 - planning

D. Statistical Summary and Visualization

We analyzed the disengagement database by creating a
taxonomy at Stage 4, summarizing the results with statistical
analysis and visualization. In terms of the taxonomy, the AVD
causes were classiﬁed into categories and subcategories. The
frequency and distribution of the causes in different categories
revealed insights, such as “in which stage, the AV system
failed to execute tasks most frequently” and “which environ-
mental factors caused AVD most often”. Various visualizations
offered more direct and intuitive insights. For example, the
word cloud gave an impression of those hot words used fre-
quently to describe AVD while the time-series graph showed
how the cases of AVD changed over the past seven years and
demonstrated patterns which deserved further investigation. In
addition, the statistical analysis provided more quantitative
ﬁndings of signiﬁcant relationships between variables with
statistical tests.

IV. RESULTS

A. Cause-Effect Extraction

For the ﬁrst approach described in Section Cause and Effect
Modeling, following the best practice of transfer learning [41],
the setting of the four pre-trained deep learning models were
shown as follows: Fine-tuned using AdamW as the optimizer
with an initial learning rate of 5e−5, and a learning rate
scheduler decreasing the learning rate linearly in 15 epochs. To
investigate the generalization of the models to new data, ﬁve-
fold cross-validation was applied. The whole labeled dataset
was partitioned evenly into 5 complementary subsets based on
the distribution of the labels. Each subset was used for testing
once while the rest four subsets were used for training. In this
manner, the whole dataset was fully utilized and the scores of
the models were averaged over ﬁve testing subsets to generate
the ﬁnal balanced score.

As shown in Table V, among the four pre-trained models,
ELECTRA achieved the highest weighted F-1 score with rel-
atively low computation resources. Furthermore, post-training
signiﬁcantly improved the performance of the best model at
the cost of longer training time. The pre-trained model with
ﬁne-tuning and post-training achieved the best performance.

B. End-to-End Token Classiﬁcation

For the second approach described in Section Cause and
Effect Modeling, the training setup for this approach was the
same as the ﬁrst approach. Since more speciﬁc labels were
used for the end-to-end model, the difﬁculty and complexity
for prediction increased signiﬁcantly. However, as shown in

Model
BERT + Fine-tuning
XLNET + Fine-tuning
DistillBERT + Fine-tuning
ELECTRA + Fine-tuning
ELECTRA + Post-training
ELECTRA + Fine-tuning + Post-training

Weighted F-1
0.76
0.78
0.75
0.82
0.69
0.90

Training Time
17min 39s
24min 52s
10min 30s
17min 46s
12min 36s
28min 14s

TABLE VI: Weighted F-1 score and training time for end-to-end token
classiﬁcation.

Model
BERT + Fine-tuning
XLNET + Fine-tuning
DistillBERT + Fine-tuning
ELECTRA + Fine-tuning

Weighted F-1
0.72
0.74
0.71
0.75

Training Time
17min 59s
24min 57s
10min 37s
18min 12s

Table VI, the pre-trained models still achieved relatively good
performance. If the results in Table V was compared to those
in Table VI, it is satisfactory to ﬁnd that the weighted F-1
score of ELECTRA model with ﬁne-tuning only dropped from
82% to 75% while the complexity of token classiﬁcation task
scaled up to almost 8 times from 7 tags (O, B-C, I-C, B-E,
I-E, B-CE, I-CE) to 55 tags (O, B-C-0, ... , I-C-0, ... , B-E-0,
... , I-E-8, ... , B-CE-8, ..., I-CE-8).

C. Taxonomy

Schaefer et al. [39] used a meta-analysis method to iden-
tify three major factors inﬂuencing the trust development
in automation, including human factors, system factors, and
environment factors. This potentially caused the operators to
initiate the disengagement due to a lack of trust [1]. Similarly,
we adopted this method to categorize the causes of the dis-
engagement. Furthermore, the failure of different components
- perception, planning, control [42] in the AV system also
led to disengagement. Such failures could be caused by the
environment and the system itself. Thus, a taxonomy (Fig. 4)
covering both human driver trust and AV system failures, and
environmental factors was generated. This taxonomy served
as the basis of the categories and subcategories for causes,
which made our analysis more intuitive and efﬁcient. Table
VII shows the number of unique causes in each category.

D. System-Initiated vs. Human-Initiated Disengagement

The pipeline can also conduct multiple statistical tests to
answer questions related to two types of disengagement, such

TABLE VII: The number of unique causes in different categories.

Cause Category
0 - perception
1 - localization & mapping
2 - planning
3 - control
4 - AV driver
5 - other driver & vehicle
6 - environment
7 - system general
8 - others
Total

Count
52
43
52
59
41
60
33
29
8
377

7

Fig. 4: Overview of the taxonomy, which has three main categories of causes, including human factors, AV system, environmental factors and others. Each
main category also has their own subcategories as shown in the ﬁgure.

TABLE VIII: The contingency table for initiator and cause category.

Cause Categories

AV System
Human Factors
Environmental Factors and Others

Initiator

AV Systems
1703
1
46

Test Operators
5493
1871
397

TABLE IX: The contingency table for initiator and sub cause category.

Cause Category

Cause Subcategories

AV System

Human Factors

Environmental
Factors and Others

0 - perception
1 - localization & mapping
2 - planning
3 - control
4 - AV driver
5 - other driver & vehicle
6 - environment
7 - system general
8 - others

Initiator

AV Systems
322
106
775
71
0
1
38
429
8

Test Operators
998
221
1423
2291
1534
337
185
560
212

as “Was there a signiﬁcant relationship between the initiator
and the cause category?” or “How did the different cause
category contribute to the two types of disengagement?”. Fig
5 shows how the different cause categories contributed to two
types of disengagement in the consolidated database. Table
VIII and Table IX were the contingency tables for initiators
between the categories and the subcategories. Table VIII shows
that for AVD initiated by AV systems and test drivers, most
causes came from the AV systems themselves. Table IX
provided a more detailed insight suggesting that for AVD
initiated by the AV systems, the planning stage was the most
unreliable stage in the AV systems, while for AVD initiated
by the test operators, the majority were caused by either the
control stage of the AV systems or the discomfort felt by the
test operators. In addition, Chi-Square tests for independence
were conducted on the two contingency tables. There was a

Fig. 5: Sankey chart for cause category contribution to two types of disen-
gagement.

signiﬁcant relationship between the initiators of AVD and the
main categories, χ2(2, 9511) = 571.53, p < 0.001. And the
frequency of the causes in the subcategories differed signiﬁ-
cantly by the initiators as well, χ2(8, 9511) = 1726.13, p <
0.001. According to the disengagement reports released by
CA DMV, the initiator of disengagement was either the AV
System when it failed to execute due to technical issues and
thus requested the test operator to take over control, or the test

8

V. DISCUSSION

A. Model Performance

Our NLP pipeline built on the ELECTRA model was able
to extract cause-and-effect relationships with a weighted F-1
score of 0.90, which outperformed other selected deep learning
models. Though the training time was longer compared to
other models, it was still acceptable within 30 minutes. When
we used it for end-to-end token classiﬁcation, the weighted
F-1 score was decreased due to the signiﬁcant increase in
difﬁculty and complexity of token classiﬁcation. Nevertheless,
the ELECTRA-based model also performed best among the
selected NLP deep learning models. Further research should
be conducted to further improve the performance of the model.
For example, the disengagement scenarios were described in
speciﬁc domain language and the ﬁne-tuning sample from
the SemEval-2010 Task 8 was relatively small. Thus, NLP
deep learning models can be further pre-trained in domain
language and ﬁne-tuned with a larger sample size of cause-
and-effect data. Only three workers provided insights of cause-
and-effect relationships and cause categories, which may cause
bias. More workers are needed to make the ground truth labels
more reliable to improve the performance of the NLP pipeline.
The methods used in this study also have implications
for future AVD analysis. As Bimbraw et al. [43] concluded
“Most cars are expected to be fully autonomous by 2035”,
before that actually happens, AV testing still requires the
presence of human operators which will generate a large
amount of disengagement reports. However, the majority of
previous studies heavily relied on manual work and their
ﬁndings cannot be applied to incoming disengagement reports
seamlessly. The weighted F-1 score of this study suggested
that with transfer learning, pre-trained models with millions
of parameters can be ﬁne-tuned and post-trained to learn the
domain knowledge of AVD and the task knowledge of speciﬁc
analytical tasks, thus achieving satisfying performance close
to human workers with much less cost and time. Furthermore,
because of the scale of the future disengagement reports,
the end-to-end pipeline approach used in this study is both
efﬁcient and necessary for the large-scale analysis of AVD
that other researchers may be interested in.

B. Two Types of Disengagement

The analysis of disengagement initiators suggests that more
than 80% of the disengagement were initiated by test drivers,
who either felt uncomfortable about the maneuver of the AVs
or made precautionary takeovers because of insufﬁcient trust.
Therefore, additional researches on human trust towards AV
and human comfort levels are important to further investigate
and explain the manual takeover, which will be a solid step to
solve unnecessary disengagement and lay the foundation for
the future full automation. The results of various causes related
to the AV system also suggest that discrepancies happened
in perception, localization & mapping, planning and control
were the primary causes that led to the failures of executing
certain tasks by the AV system. Our NLP pipeline not only can
summarize the cause-and-effect relationships in the taxonomy
as shown in Table VIII, Table IX, and Fig. 5, but also it can

Fig. 6: Sample word cloud for the category environment.

Fig. 7: Sample disengagement time series chart for Toyota in 2020.

operator when he/she felt uncomfortable with or did not trust
the AV system and thus took over control proactively.

E. Visualization

The pipeline can produce various visualizations to support
further analysis, including but not limited to the word cloud
of causes under different categories (e.g., Fig. 6), the time
series of disengagement reported by manufacturers in speciﬁc
time range (e.g., Fig. 7), the multi-series bar chart for causes
initiated by the AV system or the human operator, etc.

Fig. 8 demonstrated how cases of reported AVD for top 5
manufacturers changed over time. From the ﬁgure, it was easy
to ﬁnd that Cruise started AV testing very early but paused
at the end of 2018, while other manufacturers continued to
test AVs until early 2020 when COVID-19 happened. After
COVID-19 was gradually under control, they resumed the AV
testing but had not reached to their full capacity yet by the end
of 2020. Some patterns can also be identiﬁed from the ﬁgure.
For instance, those peaks in the ﬁgure always happened in
the Winter and/or Spring seasons: (1) Cruise - 2015 Winter,
2016 Spring, (2) Lyft - 2019 Spring, (3) Mercedes Benz -
2018 Spring, 2019 Spring, 2019 Winter, 2020 Spring, (4)
Nvidia - 2019 Spring, (5) Toyota - 2019 Winter, 2020 Spring.
Further researches are needed to investigate whether it was a
coincidence or there were reasons for manufacturers to test
AVs intensively during these two seasons.

9

Fig. 8: Time series chart for top ﬁve manufacturers having the most AVD counts.

identify the speciﬁc cause-and-effect pairs that give the spe-
ciﬁc causes of the disengagement. The majority of prior studies
focusing on the exploration of disengagement causes using
taxonomy had successfully concluded the categories [15], but
they did not identify the speciﬁc causes, such as “a wrong
speed control command” or “software module generated a
wrong path and froze” that were valuable to manufacturers
in terms of improving the AV system design and testing.

VI. CONCLUSIONS
This study created a scalable end-to-end pipeline based
on NLP deep transfer learning, which can not only collect,
process raw data to generate a consolidated database, but also
extract and analyze the causes of AVD from the CA DMV
dataset. The best model used in the pipeline was the product
of the best practice of transfer learning followed by post-
training. A taxonomy covering both human operator trust, AV
system safety, and environmental factors was introduced to
better understand the causes. Other examples of the results
produced from this pipeline were also presented, including
AVD counts over the years, word cloud for speciﬁc cause
categories, statistical analysis between two different types of
disengagement initiators. Our model is also able to analyze
future incoming AVD in a seamless way to update the current
results, which show the potential of our proposed pipeline.

REFERENCES

[1] J. Ayoub, X. J. Yang, and F. Zhou, “Modeling dispositional and initial
learned trust in automated vehicles with predictability and explainabil-
ity,” Transportation research part F: trafﬁc psychology and behaviour,
vol. 77, pp. 102–116, 2021.

[2] SAE, Taxonomy and Deﬁnitions for Terms Related to Driving Automa-
tion Systems for On-Road Motor Vehicles. SAE International in United
States, J3016–201806, Jun. 2018.

[3] N. Merat, B. Seppelt, T. Louw, J. Engstr¨om, J. D. Lee, E. Johansson,
C. A. Green, S. Katazaki, C. Monk, M. Itoh et al., “The “out-of-the-
loop” concept in automated driving: Proposed deﬁnition, measures and
implications,” Cognition, Technology & Work, vol. 21, no. 1, pp. 87–98,
2019.

[4] C. Gold, R. Happee, and K. Bengler, “Modeling take-over performance
in level 3 conditionally automated vehicles,” Accident Analysis &
Prevention, vol. 116, pp. 3–13, 2018.

[5] F. Zhou, X. J. Yang, and X. Zhang, “Takeover transition in autonomous
vehicles: A YouTube study,” International Journal of Human–Computer
Interaction, pp. 1–12, 2019.

[6] N. Du, F. Zhou, E. M. Pulver, D. M. Tilbury, L. P. Robert, A. K. Pradhan,
and X. J. Yang, “Examining the effects of emotional valence and
arousal on takeover performance in conditionally automated driving,”
Transportation Research Part C: Emerging Technologies, vol. 112, pp.
78–87, 2020.

[7] ——, “Predicting driver takeover performance in conditionally auto-
mated driving,” Accident Analysis & Prevention, vol. 148, p. 105748,
2020.

[8] F. Zhou, X. J. Yang, and J. de Winter, “Using eye-tracking data to
predict situation awareness in real
time during takeover transitions
in conditionally automated driving,” IEEE Transactions on Intelligent
Transportation Systems, 2021.

[9] N. Du, J. Kim, F. Zhou, E. Pulver, D. M. Tilbury, L. P. Robert, A. K.
Pradhan, and X. J. Yang, “Evaluating effects of cognitive load, takeover
request lead time, and trafﬁc density on drivers’ takeover performance
in conditionally automated driving,” in 12th International Conference
on Automotive User Interfaces and Interactive Vehicular Applications,
2020, pp. 66–73.

[10] K. Zeeb, M. H¨artel, A. Buchner, and M. Schrauf, “Why is steering
not
the same as braking? the impact of non-driving related tasks
on lateral and longitudinal driver interventions during conditionally
automated driving,” Transportation research part F: trafﬁc psychology
and behaviour, vol. 50, pp. 65–79, 2017.

[11] B. Wandtner, N. Sch¨omig, and G. Schmidt, “Effects of non-driving
related task modalities on takeover performance in highly automated
driving,” Human factors, vol. 60, no. 6, pp. 870–881, 2018.

[12] C. Gold, M. K¨orber, D. Lechner, and K. Bengler, “Taking over control
from highly automated vehicles in complex trafﬁc situations: the role of
trafﬁc density,” Human factors, vol. 58, no. 4, pp. 642–652, 2016.
[13] H. Clark and J. Feng, “Age differences in the takeover of vehicle control
and engagement in non-driving-related activities in simulated driving
with conditional automation,” Accident Analysis & Prevention, vol. 106,
pp. 468–479, 2017.

[14] O. Carsten and A. H. Jamson, “Driving simulators as research tools in
trafﬁc psychology,” in Handbook of trafﬁc psychology. Elsevier, 2011,
pp. 87–96.

[15] F. Favar`o, S. Eurich,

“Autonomous vehicles’
and N. Nader,
disengagements: Trends, triggers, and regulatory limitations,” Accident
Analysis & Prevention, vol. 110, pp. 136–148, 2018. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0001457517303822

[16] A. M. Boggs, R. Arvin, and A. J. Khattak, “Exploring the who, what,
when, where, and why of automated vehicle disengagements,” Accident

Analysis & Prevention, vol. 136, p. 105406, 2020. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S000145751931019X
[17] S. Wang and Z. Li, “Exploring causes and effects of automated
vehicle disengagement using statistical modeling and classiﬁcation tree
based on ﬁeld test data,” Accident Analysis & Prevention, vol. 129,
pp. 44–54, 2019. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0001457519300016

[18] L. Torrey and J. Shavlik, “Transfer learning,” in Handbook of research
on machine learning applications and trends: algorithms, methods, and
techniques.

IGI global, 2010, pp. 242–264.

[19] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. O. S´eaghdha,
S. Pad´o, M. Pennacchiotti, L. Romano, and S. Szpakowicz, “Semeval-
2010 task 8: Multi-way classiﬁcation of semantic relations between pairs
of nominals,” arXiv preprint arXiv:1911.10422, 2019.

[20] Z. Li, Q. Li, X. Zou, and J. Ren, “Causality extraction based on
self-attentive bilstm-crf with transferred embeddings,” Neurocomputing,
vol. 423, p. 207–219, Jan 2021. [Online]. Available: http://dx.doi.org/
10.1016/j.neucom.2020.08.078

J. Allen,
and
[21] N. Mostafazadeh, A. Grealish, N. Chambers,
relation scheme
L. Vanderwende, “CaTeRS: Causal and temporal
for semantic annotation of event structures,” in Proceedings of
the
Fourth Workshop on Events. San Diego, California: Association for
Computational Linguistics, Jun. 2016, pp. 51–61. [Online]. Available:
https://www.aclweb.org/anthology/W16-1007

[22] G. Markkula, R. Romano, R. Madigan, C. W. Fox, O. T. Giles, and
N. Merat, “Models of human decision-making as tools for estimating
and optimizing impacts of vehicle automation,” Transportation research
record, vol. 2672, no. 37, pp. 153–163, 2018.

[23] A. Eriksson, V. Banks, and N. Stanton, “Transition to manual: Com-
paring simulator with on-road control transitions,” Accident Analysis &
Prevention, vol. 102, pp. 227–234, 2017.

[24] C. Gold, D. Damb¨ock, L. Lorenz, and K. Bengler, ““take over!” how
long does it take to get the driver back into the loop?” in Proceedings
of the human factors and ergonomics society annual meeting, vol. 57,
no. 1. Sage Publications Sage CA: Los Angeles, CA, 2013, pp. 1938–
1942.

[25] C. Gold, L. Lorenz, and K. Bengler, “Inﬂuence of automated brake ap-
plication on take-over situations in highly automated driving scenarios,”
in Proceedings of the FISITA 2014 World Automotive Congress, 2014.
[26] J. Radlmayr, C. Gold, L. Lorenz, M. Farid, and K. Bengler, “How trafﬁc
situations and non-driving related tasks affect the take-over quality in
highly automated driving,” in Proceedings of the human factors and
ergonomics society annual meeting, vol. 58, no. 1. Sage Publications
Sage CA: Los Angeles, CA, 2014, pp. 2063–2067.

[27] M. K¨orber, C. Gold, D. Lechner, and K. Bengler, “The inﬂuence of age
on the take-over of vehicle control in highly automated driving,” Trans-
portation research part F: trafﬁc psychology and behaviour, vol. 39, pp.
19–32, 2016.

[28] H. Alambeigi, A. D. McDonald, and S. R. Tankasala, “Crash themes
in automated vehicles: A topic modeling analysis of the california
department of motor vehicles automated vehicle crash database,” 2020.
[29] S. S. Banerjee, S. Jha, J. Cyriac, Z. T. Kalbarczyk, and R. K. Iyer,
“Hands off the wheel in autonomous vehicles?: A systems perspective
on over a million miles of ﬁeld data,” in 2018 48th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN),
2018, pp. 586–597.

[30] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-

training text encoders as discriminators rather than generators,” 2020.

[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training

of deep bidirectional transformers for language understanding,” 2019.

[32] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled

version of bert: smaller, faster, cheaper and lighter,” 2020.

[33] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le,
“Xlnet: Generalized autoregressive pretraining for language understand-
ing,” 2020.

[34] S. of California Department of Motor Vehicles,

“Autonomous
https://www.dmv.ca.gov/portal/vehicle-industry-services/

vehicles,”
autonomous-vehicles/, accessed: 2021-04-12.

[35] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software

Tools, 2000.

[36] R. Smith, “An overview of the tesseract ocr engine,” in Proc. Ninth Int.
Conference on Document Analysis and Recognition (ICDAR), 2007, pp.
629–633.

[37] A. Rosebrock, “Ocr a document, form, or invoice with tesseract,
https://www.pyimagesearch.com/2020/09/07/

opencv,
ocr-a-document-form-or-invoice-with-tesseract-opencv-and-python/,
accessed: 2021-04-12.

python,”

and

10

[38] A. M. Boggs, B. Wali, and A. J. Khattak, “Exploratory analysis
of automated vehicle crashes in california: A text analytics &
hierarchical bayesian heterogeneity-based approach,” Accident Analysis
& Prevention, vol. 135, p. 105354, 2020.
[Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0001457519308735

[39] K. Schaefer, J. Chen, J. Szalma, and P. Hancock, “A meta-analysis of
factors inﬂuencing the development of trust in automation: Implications
for understanding autonomy in future systems,” Human Factors: The
Journal of the Human Factors and Ergonomics Society, vol. 58, 03
2016.

[40] A. Dumitrache, O. Inel, L. Aroyo, B. Timmermans, and C. Welty,
“Crowdtruth 2.0: Quality metrics for crowdsourcing with disagreement,”
2018. [Online]. Available: https://arxiv.org/abs/1808.06080

[41] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”

2019.

[42] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H.
Eng, D. Rus, and M. H. Ang, “Perception, planning, control, and
coordination for autonomous vehicles,” Machines, vol. 5, no. 1, 2017.
[Online]. Available: https://www.mdpi.com/2075-1702/5/1/6

[43] K. Bimbraw, “Autonomous cars: Past, present and future a review
of the developments in the last century,
the present scenario and
the expected future of autonomous vehicle technology,” in 2015 12th
international conference on informatics in control, automation and
robotics (ICINCO), vol. 1.

IEEE, 2015, pp. 191–198.

Yangtao Zhang received his B.S. degree in Elec-
trical and Computer Engineering from UM-SJTU
Joint Institute, Shanghai Jiao Tong University
in 2019 and his master degree in Information
Science from University of Michigan, Ann Arbor,
in 2021. He is currently a software engineer
at Amazon. His main research interests include
natural language processing and human-machine
interaction.

X. Jessie Yang is an Assistant Professor in the
Department of Industrial and Operations En-
gineering, University of Michigan, Ann Arbor.
She earned a PhD in Mechanical and Aerospace
Engineering (Human Factors)
from Nanyang
Technological University, Singapore. Dr. Yang’s
research include human-autonomy interaction,
human factors in high-risk industries and user
experience design.

Dr Feng Zhou received the Ph.D. degree in Hu-
man Factors Engineering from Nanyang Techno-
logical University, Singapore, in 2011 and Ph.D.
degree in Mechanical Engineering from Gatech
Tech in 2014. He was a Research Scientist at
MediaScience, Austin TX, from 2015 to 2017.
He is currently an Assistant Professor with the
Department of Industrial and Manufacturing
Systems Engineering, University of Michigan,
Dearborn. His main research interests include
human factors, human-computer interaction, en-

gineering design, and human-centered design.

