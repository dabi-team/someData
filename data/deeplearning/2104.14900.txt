Discrete-Time Mean Field Control with Environment States

Kai Cui, Anam Tahir, Mark Sinzger and Heinz Koeppl

1
2
0
2

c
e
D
7
1

]

G
L
.
s
c
[

2
v
0
0
9
4
1
.
4
0
1
2
:
v
i
X
r
a

Abstract— Multi-agent reinforcement learning methods have
shown remarkable potential
in solving complex multi-agent
problems but mostly lack theoretical guarantees. Recently,
mean ﬁeld control and mean ﬁeld games have been established
as a tractable solution for large-scale multi-agent problems
with many agents. In this work, driven by a motivating
scheduling problem, we consider a discrete-time mean ﬁeld
control model with common environment states. We rigorously
establish approximate optimality as the number of agents
grows in the ﬁnite agent case and ﬁnd that a dynamic
programming principle holds, resulting in the existence of
an optimal stationary policy. As exact solutions are difﬁcult
in general due to the resulting continuous action space of
the limiting mean ﬁeld Markov decision process, we apply
established deep reinforcement learning methods to solve the
associated mean ﬁeld control problem. The performance of
the learned mean ﬁeld control policy is compared to typical
multi-agent reinforcement learning approaches and is found to
converge to the mean ﬁeld performance for sufﬁciently many
agents, verifying the obtained theoretical results and reaching
competitive solutions.

I. INTRODUCTION

Reinforcement Learning (RL) has proven to be a very
successful approach for solving sequential decision-making
problems [1]. Today it has numerous applications e.g. in
robotics [2], strategic games [3] or communication networks
[4]. Many such applications are modelled as special cases
of Markov games, which has led to empirical success in the
multi-agent RL (MARL) domain.

However, MARL problems quickly become intractable for
large numbers of agents and proposed solutions offer few
rigorous guarantees [5]. An increasingly popular approach
in resolving this curse of dimensionality are mean ﬁeld
approximation. The main idea is to convert a many-agent
system with N indistinguishable and interchangeable agents
into a problem where one representative agent interacts with
e.g. the empirical state distribution – the mean ﬁeld – of the
other agents. Since the N -agent model is reduced to a single
agent and a mean ﬁeld, this lends the problem tractability
with theoretical guarantees for sufﬁciently large N .

The framework of mean ﬁeld games (MFG) was ﬁrst
introduced in [6] and [7] for stochastic differential games and
has since been extended to discrete-time [8], [9]. It provides
a framework for analyzing many-agent competitive problems,
for which learning-based solutions have become increasingly
popular [10], [11], [12]. Mean ﬁeld theory applied to the
cooperative setting is known as mean ﬁeld control (MFC),

The authors are with the Department of Electrical Engineering,
64287 Darmstadt, Germany.
{kai.cui, anam.tahir, mark.sinzger,

Technische Universit¨at Darmstadt,
Contact:
heinz.koeppl}@bcs.tu-darmstadt.de

Fig. 1. Overview of the multi-agent system as a probabilistic graphical
model using plate notation [23], where circles and diamonds indicate
stochastic and deterministic nodes respectively. Each agent i chooses an
action ui,N
and local agent state
xi,N
t+1 only via their empirical
t
distribution GN
t . Agent states are assumed i.i.d. for simplicity of analysis.

t
, inﬂuencing the next environment state x0,N

conditional on the environment state x0,N

t

where one assumes that many agents cooperate to achieve
Pareto optima [13], [14]. MFC has various applications e.g.
in smart heating [15] or portfolio management [16].

The dimensions of the MFC problem are independent
of the speciﬁc number of agents, making it more tractable.
However, solving the MFC problem has the challenge of time-
inconsistency due to the non-Markovian nature of the problem
[13], [17], [18]. A recent way of handling this inherent time-
inconsistency problem is to use an enlarged state-action space
[19], [20], [21], [22]. We similarly apply this technique by
lifting up the state-action space into its probability measure
space, since it will enable usage of dynamic programming
and established reinforcement learning methods.

In this work we extend the theory of discrete-time MFC
by considering additional environment states. An advantage
of discrete-time models is applicability of a plethora of rein-
forcement learning solutions. Our model can be considered
a special case of the MFC equivalent of major-minor mean
ﬁeld games [24], [25] with trivial major agent policy, which
to the best of our knowledge has not been formulated yet.
We expect that our results can be generalized, similar to
approaches e.g. in [9] for the competitive mean ﬁeld game,
although for deterministic mean ﬁelds.

The main contributions of this paper are: (i) We propose
a new discrete-time MFC formulation that transforms large-
scale multi-agent control problems with common environment
states into a simple Markov decision process (MDP) with
lifted state-action space; (ii) we rigorously show approximate
optimality for sufﬁciently large systems as well as existence of
an optimal stationary policy through a dynamic programming
principle, and (iii) associated with this standard discrete-time
MDP with continuous action space, we verify our theoretical
ﬁndings empirically using modern reinforcement learning
techniques. As a result, we outperform existing baselines for
the many-agent case and obtain a methodology to solve large

 
 
 
 
 
 
to its destination queue, it is clearly sufﬁcient to consider
the empirical distribution: Sampling from the empirical
distribution, using the sampled action and, if inaccessible,
resampling an accessible queue provides the desired behavior.

III. MEAN FIELD CONTROL

In this section, we formulate a N -agent model that in the
limit of N → ∞ results in a more tractable MFC problem.
Importantly, we will then show approximate optimality and
a dynamic programming principle for the MFC problem,
allowing for application of reinforcement learning.

Notation. Let A be a ﬁnite set. We equip A with the
discrete metric and denote the set of real-valued functions on
A by RA, For f ∈ RA let (cid:107)f (cid:107)∞ = maxa∈A f (a). Denote
by |A| the cardinality of A. Denote by P(A) = {p ∈
RA : p(a) ≥ 0, (cid:80)
a∈A p(a) = 1} the space of probability
simplices, equivalent to the probability measures on A. Equip
P(A) with the l1-norm (cid:107)µ−ν(cid:107)1 = (cid:80)
a∈A |µ(a) − ν(a)|. For
readability, we uncurry occurrences of multiple parentheses,
e.g. πt(x0
t )(x) ≡ πt(x0
a∈A f (a)µ(a)
for any µ ∈ P(A), f : A → R.

t , x). Deﬁne µ(f ) := (cid:80)

A. Finite Agent Model

Let X , U be a ﬁnite state and action space respectively.
Let X 0 be a ﬁnite environment state space. For any N ∈ N,
at each time t = 0, 1, . . ., the states and actions of agent
i = 1, . . . , N are random variables denoted by xi,N
t ∈ X and
ui,N
t ∈ U. Analogously, the environment state is a random
variable denoted by x0,N
∈ X 0. Deﬁne the empirical state-
t
action distribution GN
t = 1
) ∈ P(X × U).
N
For each agent i, we consider locally Markovian policies
πi = {πi
t}t≥0 ∈ ΠN from the space of admissible Markov
t : X 0 × X → P(U). Further, we deﬁne
policies ΠN where πi
the policy proﬁle π = (π1, . . . , πN ) ∈ ΠN
N .

i=1 δ(xi,N

,ui,N
t

(cid:80)N

t

Acting only on local and environment information may
seem like a strong restriction. However, other agent states
are uninformative under continuity assumptions as N → ∞
as the interaction between agents will be restricted to the
increasingly deterministic empirical state-action distribution.
Let µ0 ∈ P(X ) be the initial agent state distribution,
µ0
0 ∈ P(X 0) the initial environment state distribution and
P 0 : X 0 × P(X × U) → P(X 0) a transition kernel. The
random variables shall follow x0,N
0 and subsequently

0 ∼ µ0

xi,N
t ∼ µ0,
ui,N
t(x0,N
t ∼ πi
t+1 ∼ P 0(x0,N
x0,N

t

t

),

, xi,N
t
, GN

t ),

(1)

(2)

(3)

where for simplicity of further analysis the agent states are
always sampled according to µ0.

Remark. While this is a strong dynamics assumption,
our formulation is nonetheless sufﬁcient for the scheduling
problem. In principle, any results should similarly hold under
appropriate assumptions for nontrivial agent state dynamics
by considering mean ﬁeld and environment state together.
As this will signiﬁcantly complicate analysis, an according
extension of theoretical results is left to future works.

Fig. 2. Overview of the queuing system. Many schedulers (middle) obtain
packets at a ﬁxed rate (left) that must be assigned to one of the accessible
queues (right) such that total packet drops are minimized.

multi-agent control problems such as the following.

II. SCHEDULING SCENARIO

While the concept of mean ﬁeld limits has been used in
queuing systems before, it has mostly been used for the state
of the buffer ﬁllings of queues or the number of servers/queues
[26], [27]. In this work we use mean ﬁelds to represent the
state of a large amount of schedulers while modeling the
queues exactly. See also Figure 2 for a visualization of the
problem. Note that in principle, our model could be used for
any similar resource allocation problem such as allocation of
many ﬁreﬁghters to houses on ﬁre.

t

Consider a queuing system with N agents called schedulers,
[s1, . . . , sN ], and M parallel servers, each with its own ﬁnite
FIFO queue. Denote the queue ﬁlling by bi ∈ {0, . . . , Bi},
i = 1, . . . , M where Bi is the maximum buffer space for
the i-th queue. At any time step t, the state xi,N
∈ X of
a scheduler is the set of queues it has access to. The agent
state space X therefore consists of all combinations of queue
access where every agent has access to at least one of the
queues. The environment state is the current buffer ﬁlling
x0 = [b1, . . . , bM ], where bj is the buffer ﬁlling of queue j.
In discrete-time, the number of job arrivals to be assigned
at each time step t is Poisson distributed with rate λ∆T
and the number of serviced jobs for each server is Poisson
distributed with rate β∆T , where ∆T > 0 can be considered
the time span between each synchronization of schedulers.
As an approximation, we assume that all queue departures in
a time slot happen before the new arrivals, and newly arrived
jobs thus cannot be serviced in the same time slot.

We split the total number of job packets which arrive
in some time step ∆T uniformly at random amongst the
schedulers. The jobs assigned to each scheduler need to be
sent out immediately. Each scheduler decides which of the
accessible queues it sends its arrived jobs to during each time
step. If a job is mapped to a full buffer, it is lost and a penalty
cd is incurred. The goal of the system is therefore to minimize
the number of job drops. At each step of the decision making,
we assume that the state of the environment x0 and their own
accessible queues are known to the schedulers.

We can model the dynamics of the environment state
dependent on the empirical state-action distribution of all
schedulers: Consider agents choosing some choice of queues
as their action, where inaccessible queues are treated as
randomly picking a destination. In that case, to assign a packet

t (x0), . . . , πN

Let us introduce another notation. First, deﬁne the space
of decision rules H := {h : X → P(U)}. Then a one-step
policy proﬁle h = (h1, . . . , hN ) ∈ HN is an N -fold decision
rule. Our major example of a one-step policy proﬁle is
(π1
t (x0)) for ﬁxed t ≥ 0, ﬁxed x0 ∈ X 0 and
potentially different policies for the N agents. For given agent
state distribution µ0 and a one-step policy proﬁle h ∈ HN
let xi,N ∼ µ0, ui,N ∼ hi(xi,N ), s.t. (xi,N , ui,N )i=1,...,N
are independent. Then, consider a random measure GN
h ∈
P(X × U) or equivalently its random probability mass
function GN

h : X × U → [0, 1],
N
(cid:88)

GN

h (x, u) :=

1
N

i=1

1x,u(xi,N , ui,N ) .

(4)

Deﬁne GN (µ0, h) as the distribution of GN
a distribution over the set P(X × U) and GN
Consider the primary example h = (π1
t (x0), . . . , πN
In contrast to the empirical distribution GN
t
a random x0,N
has x0,N
t = x0 ﬁxed. By E[GN
the entry-wise expectation {E[GN

h , so GN (µ0, h) is
h ∼ GN (µ0, h).
t (x0)).
that depends on
, the random probability mass function GN
h
h ] we denote in the following

h (x, u)]}(x,u)∈X ×U .

Let γ ∈ (0, 1) be the discount factor and r : X 0 × P(X ×
U) → R a reward function. The goal is to maximise the
discounted accumulated reward

t

J N (π) = E

(cid:34) ∞
(cid:88)

γtr(x0,N

t

(cid:35)
, GN
t )

(5)

t=0
which generalizes optimizing an average per-agent reward

J N (π) =

N
(cid:88)

E

(cid:34) ∞
(cid:88)

i=1

t=0

γt˜r(xi,N

t

, x0,N
t

(cid:35)
, GN
t )

for any x ∈ X , u ∈ U. The random environment state
0 ∼ µ0
variables therefore follow x0
t+1 ∼ P 0(x0
x0
Analogously, the objective becomes

0 and subsequently
t , Gt) .

(10)

J(π) = E

(cid:35)
t , Gt)

γtr(x0

.

(cid:34) ∞
(cid:88)

t=0

(11)

We require the following simple continuity assumption to

obtain meaningful results in the limit as N → ∞.

Assumption 1 (Continuity of r and P 0). The functions r
and P 0 are continuous, i.e. for all x0 ∈ X 0 and Gn → G ∈
P(X × U) we have

r(x0, Gn) → r(x0, G), P 0(x0, Gn) → P 0(x0, G) . (12)

By compactness of P(X × U), we have boundedness.

Proposition 1. Under Assumption 1, r is bounded by some
R, i.e. for any x0 ∈ X 0, G ∈ P(X × U) we have

|r(x0, G)| ≤ R .

(13)

Our ﬁrst goal will be to show that as N → ∞, the optimal
solution to the MFC is approximately Pareto optimal in the
ﬁnite N case. This will motivate solving the MFC problem.

IV. APPROXIMATE OPTIMALITY

We ﬁrst show the following lemma on uniform convergence
in probability of empirical state-action distributions to their
state-action-wise average for ﬁxed one-step policy proﬁles.

(6)

Lemma 1. Let x0 ∈ X 0 and h ∈ HN be an arbitrary
one-step policy proﬁle. Let GN ∼ G(µ0, h). Then

for some shared ˜r : X × X 0 × P(X × U) → R through
r(x0,N
t
As the optimality concept in this work, we therefore deﬁne

x∈X ˜r(x, x0,N

t ) ≡ (cid:80)

t (x, u).

t ) (cid:80)

, GN

, GN

GN

u∈U

t

approximate Pareto optimality.

Deﬁnition 1 (Pareto optimality). For (cid:15) > 0, π(cid:15) ∈ ΠN
(cid:15)-Pareto optimal if and only if

N is

J N (π(cid:15)) ≥ sup
π
A visualization of this model can be found in Figure 1.

J N (π) − (cid:15) .

(7)

B. Mean Field Model

As N → ∞, we formally obtain the following mean ﬁeld
MDP, which will be rigorously justiﬁed in the sequel. At
each time t = 0, 1, . . ., the environment state is a random
variable denoted by x0
t ∈ X 0. We consider Markovian upper-
level policies π = {πt}t≥0 ∈ Π from the space of such
policies Π where πt : X 0 → H. We equip both H and Π
with the supremum metric. As mentioned, the population
state distribution is ﬁxed to µ0 ∈ P(X ) at all times. The
random state-action distribution is therefore given by

Gt := G(µ0, πt(x0

t ))

where G : P(X ) × H → P(X × U) is deﬁned by

G(µ, h)(x, u) := h(x, u)µ(x)

(8)

(9)

1

4N

(cid:3) ≤ |X |2|U |2

(i) E (cid:2)(cid:107)GN − E[GN ](cid:107)2
(ii) P (cid:0)(cid:107)GN − E[GN ](cid:107)1 ≥ (cid:15)(cid:1) ≤ |X |2|U |2
Proof. By Chebyshev’s inequality, (i) implies (ii). It remains
to prove (i). Let xi,N ∼ µ0 be i.i.d. and ui,N ∼ πi(x0, xi,N ),
s.t. (xi,N , ui,N )i=1,...,N are independent. Then by the sub-
additivity of E[(·)2] 1

2 , we have

4(cid:15)2N

E (cid:2)(cid:107)GN − E[GN ](cid:107)2



1

= E





(cid:88)

x∈X ,u∈U

(cid:3) 1

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

1x,u(xi,N , ui,N )

−E

(cid:34)

1
N

N
(cid:88)

i=1

1x,u(xi,N , ui,N )

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:32)

(cid:34)

V

1
N

N
(cid:88)

i=1

1x,u(xi,N , ui,N )

1
2

(cid:33)2


(cid:35)(cid:33) 1

2

(cid:32)

(cid:32)

1
N 2

1
N 2

N
(cid:88)

i=1

N
(cid:88)

i=1

V (cid:2)1x,u(xi,N , ui,N )(cid:3)

(cid:33) 1

2

(cid:33) 1

2

1
4

=

|X ||U|
√
N
2

≤

=

≤

x∈X ,u∈U

(cid:88)

x∈X ,u∈U

(cid:88)

x∈X ,u∈U

using the trivial variance bound 1

4 for indicator functions.

To achieve approximate optimality of mean ﬁeld solutions
in the N -agent case, we ﬁrst deﬁne how to obtain an N -agent
policy πN ∈ ΠN

N from a mean ﬁeld policy ˆπ ∈ Π by

πN (ˆπ) = (π1, . . . , πN ) with πi

t(x0, x) = ˆπt(x0)(x)

for all i = 1, . . . , N , i.e. all agents with state x ∈ X will
follow the action distribution ˆπt(x0

t )(x) at times t ≥ 0.

Theorem 1. Under Assumption 1, we have uniform conver-
gence of the N -agent objective to the mean ﬁeld objective
as N → ∞, i.e.

that depends on a random x0,N

, the random

t

contrast to GN
t
π has x0,N
probability mass function GN
(cid:104)
f (x0,N
t
π )(cid:3)

f (x0, G(µ0, πt(x0))) − E
= f (x0, Gπ) − E (cid:2)f (x0, GN

t = x0 ﬁxed. Then
t = x0(cid:105)

t ) | x0,N

, GN

We observe that for any (x, u) ∈ X × U

E[GN

π (x, u)] = Gπ(x, u).

(18)

let xi,N ∼ µ0 be i.i.d. and ui,N ∼
For this purpose,
πt(x0, xi,N ), s.t. (xi,N , ui,N )i=1,...,N are independent. Then
for any (x, u) ∈ X × U we have

lim
N→∞

sup
π∈Π

(cid:12)J N (πN (π)) − J(π)(cid:12)
(cid:12)

(cid:12) = 0 .

(14)

E[GN

π (x, u)] =

1
N

Proof. We have by deﬁnition
(cid:12)J N (πN (π)) − J(π)(cid:12)
(cid:12)
(cid:12)

sup
π∈Π

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

t=0

(cid:104)

γtE

r(x0,N
t

= sup
π∈Π

≤

∞
(cid:88)

t=0

γt sup
π∈Π

(cid:12)
(cid:12)
(cid:12)

E

(cid:104)
r(x0,N
t

, GN

t ) − r(x0

(cid:105)
t , Gt)

, GN

t ) − r(x0

t , Gt)

(15)

(16)

(17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:105)(cid:12)
(cid:12)
(cid:12) .

To obtain the desired result, we ﬁrst show for any t ≥ 0 that
t ) →
t , Gt) weakly uniformly over all π ∈ Π. Note that

t )(cid:107)1 → 0 implies L(x0,N

) − L(x0

, GN

t

t

supπ∈Π(cid:107)L(x0,N
L(x0
supπ∈Π(cid:107)L(x0,N
) − L(x0
(cid:12)
(cid:12)L(x0,N
(cid:12)

t

t

sup
π∈Π

t )(cid:107)1 → 0 by deﬁnition implies

)(x0) − L(x0

(cid:12)
t )(x0)
(cid:12)
(cid:12) → 0

for any x0 ∈ X 0. For the joint law, consider any f : X 0 ×
P(X × U) → R, continuous and bounded by |f | ≤ F . Then

L(x0,N
t

)(x)

sup
π∈Π

t

(cid:12)
(cid:12)L(x0,N
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)

= sup
π∈Π

, GN

t )(f ) − L(x0

(cid:104)
f (x0,N
t

, GN
t )

(cid:105)

≤ sup
π∈Π

(cid:88)

x0∈X 0

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

f (x0,N
t

(cid:12)
t , Gt)(f )
(cid:12)
(cid:12)
t , Gt)(cid:3)(cid:12)
− E (cid:2)f (x0
(cid:12)
(cid:12)
t = x0(cid:105)

t ) | x0,N

, GN

− f (x0, G(µ0, πt(x0)) L(x0
(cid:12)
, GN
(cid:12)
(cid:12)

t ) | x0,N

f (x0,N
t

E

(cid:104)

(cid:88)

≤

x0∈X 0

sup
π∈Π

· sup
π∈Π
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
π∈Π

(cid:88)

+

x0∈X 0

(cid:12)
(cid:12)L(x0,N
(cid:12)

t

)(x0) − L(x0

f (x0, G(µ0, πt(x0))

t )(x0)(cid:12)
(cid:12)
t = x0(cid:105)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t )(x0)

(cid:104)

− E

f (x0,N
t

, GN

t ) | x0,N

t = x0(cid:105) (cid:12)

(cid:12)
(cid:12)
(cid:12)

L(x0

t )(x0),

· sup
π∈Π

where the ﬁrst sum goes to zero by assumption and bound-
edness of f . For the second term, consider arbitrary ﬁxed
x0 ∈ X 0. Write Gπ short for G(µ0, πt(x0)) and introduce
GN
π ∼ G(µ0, (πt(x0), . . . , πt(x0))) for all N, π ∈ Π. So in

N
(cid:88)

i=1
N
(cid:88)

E (cid:2)1x,u(xi,N , ui,N )(cid:3)

µ0(x)πt(x0, x, u)

=

1
N

i=1
= Gπ(x, u) .

Let (cid:15) > 0 arbitrary. By compactness of P(X × U), the
function f (x0, ·) : P (X × U) → R is uniformly continuous.
Consequently, there exists δ > 0 such that for all π ∈ Π

(cid:107)Gπ − GN

π (cid:107)1 < δ
=⇒ (cid:12)

(cid:12)f (x0, Gπ) − f (x0, GN

π )(cid:12)

(cid:12) <

(cid:15)
2

.

By Lemma 1 (ii) and (18) there exists N (cid:48) ∈ N such that for
N > N (cid:48) and for all π ∈ Π we have

P (cid:0)(cid:107)Gπ − GN

π (cid:107)1 ≥ δ(cid:1) ≤

(cid:15)
4F

.

As a result, we have

E (cid:2)(cid:12)

(cid:3)

π )(cid:12)
(cid:12)

(cid:12)f (x0, Gπ) − f (x0, GN
π

(cid:12)f (x0, Gπ) − f (x0, GN
≤ P (cid:0)(cid:12)
(cid:1)(cid:12)
(cid:12) ≥
(cid:15)
≤ P (cid:0)(cid:107)Gπ − GN
π (cid:107)1 ≥ δ(cid:1) · 2F +
2
(cid:15)
2

· 2F +

(cid:15)
4F

= (cid:15) .

≤

(cid:15)
2

) · 2F + 1 ·

(cid:15)
2

Since (cid:15) was arbitrary, and no choices depended on π ∈ Π,
we have the desired convergence of the second term

lim
N→∞

sup
π∈Π

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (x, G(µ0, πt(x))

(cid:104)

− E

f (x0,N
t

, GN

t ) | x0,N

t = x

= 0 .

(cid:105) (cid:12)
(cid:12)
(cid:12)
(cid:12)

We can now show L(x0,N

t , Gt) weakly
uniformly over all π ∈ Π by induction over all t, which by
Assumption 1 will imply

t ) → L(x0

, GN

t

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

r(x0,N
t

sup
π∈Π

, GN

t ) − r(x0

t , Gt)

(cid:105)(cid:12)
(cid:12)
(cid:12) → 0

(19)

for all t ≥ 0 and hence the desired statement by the dominated
convergence theorem applied to (17).

At t = 0, we trivially have L(x0,N

and therefore L(x0,N

0

, GN

0 ) → L(x0

t

) = µ0

0 = L(x0
t )
0, G0) uniformly by the

prequel. Assume that the induction assumption holds at time
t, then at time t + 1 we have
(cid:107)L(x0,N

t+1)(cid:107)1
t+1)(x0) − L(x0

t+1)(x0)

(cid:12)
(cid:12)
(cid:12)

P 0(x0 | x0,N

t

(cid:105)

, GN
t )

− E (cid:2)P 0(x0 | x0

t , Gt)(cid:3)(cid:12)
(cid:12)
(cid:12)

t+1) − L(x0
(cid:12)
(cid:88)
(cid:12)L(x0,N
(cid:12)
(cid:12)
(cid:104)
(cid:12)
(cid:12)

x0∈X 0
(cid:88)

E

=

=

x0∈X 0

→ 0

uniformly by Assumption 1 and induction assumption.

To extend to optimality over arbitrary asymmetric policy
tuples, we show that the performance of policy tuples is close
to the averaged policy as N → ∞.

Theorem 2. Under Assumption 1, as N → ∞ we have
similar performance of any policy tuple π = (π1, . . . , πN ) ∈
N and its average policy ˆπ(π) ∈ Π deﬁned by ˆπt(x0)(a |
ΠN
x) = 1
t(a | x0, x) in the N -agent case, i.e. with
i=1 πi
N
shorthand ˆπ = ˆπ(π) we have
(cid:12)J N (π1, . . . , πN ) − J N (πN (ˆπ))(cid:12)
(cid:12)

(cid:12) = 0 .

(cid:80)N

(20)

lim
N→∞

sup
π∈ΠN
N

Proof. Let π ∈ ΠN
(cid:12)J N (π1, . . . , πN ) − J N (πN (ˆπ))(cid:12)
(cid:12)
(cid:12)

N arbitrary. Again, we have by deﬁnition

sup
π∈ΠN
N

≤

∞
(cid:88)

t=0

γt sup
π∈ΠN
N

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

r(x0,N
t

, GN

t ) − r(ˆx0,N

t

(21)

, ˆGN
t )

(cid:105)(cid:12)
(cid:12)
(cid:12) (22)

by introducing random variables ˆx0,N
, i =
1, . . . , N induced by instead applying the averaged policy
tuple πN (ˆπ) in (2). By dominated convergence, it is sufﬁcient
to show term-wise convergence to zero in (22).

t , ˆxi,N

, ˆui,N
t

, ˆGN

t

t

Fix t ≥ 0. As in the proof of Theorem 1, we
)(cid:107)1 → 0 implies
(cid:12)
(cid:12)
(cid:12) → 0 for any
t )(f )

show that supπ∈Π(cid:107)L(x0,N
(cid:12)
t )(f ) − L(ˆx0,N
(cid:12)L(x0,N
(cid:12)
supπ∈ΠN
f : X 0 × P(X × U) → R continuous and bounded, since

) − L(ˆx0,N

t
, ˆGN

, GN

N

t

t

t

t (x0))) and GN

ﬁxed x0 ∈ X 0, π ∈ ΠN
N . Then introduce random vari-
ables GN
t (x0), . . . , πN
π ∼ GN (µ0, (π1
ˆπ ∼
GN (µ0, (ˆπt(x0), . . . , ˆπt(x0))) for every N ∈ N and π ∈ ΠN
N .
Then we have
f (x0, ˆGN
= E (cid:2)f (x0, GN

f (x0, GN
π )(cid:3) .
We observe that for any (x, u) ∈ X × U:

t = x0(cid:105)
(cid:3) − E (cid:2)f (x0, GN

t = x0(cid:105)

t ) | ˆx0,N

t ) | x0,N

− E

E

(cid:104)

(cid:104)

ˆπ

E[GN

ˆπ (x, u)] = E[GN

π (x, u)] .

(23)

For this purpose, let xi,N ∼ µ0 and ui,N ∼ πi
t(x0, xi,N )
as well as ˆxi,N ∼ µ0 and ˆui,N ∼ ˆπt(x0, xi,N ), s.t.
(xi,N , ui,N )i=1,...,N and (ˆxi,N , ˆui,N )i=1,...,N are indepen-
dent, respectively. Then for any (x, u) ∈ X × U:

E[GN

ˆπ (x, u)] =

1
N

N
(cid:88)

i=1

P(ˆxi,N = x, ˆui,N = u)

N
(cid:88)

i=1

N
(cid:88)

=

=

1
N

1
N

µ0(x)ˆπ(x, u) =

1
N

N
(cid:88)

i=1

µ0(x)

1
N

N
(cid:88)

j=1

πj(x, u)

µ0(x)πj(x, u) =

1
N

N
(cid:88)

j=1

P(xi,N = x, ui,N = u)

j=1
= E[GN
π (x, u)] .

Then by (23), sub-additivity of E[(·)2] 1

2 and Lemma 1 (i),

π (cid:107)2
1]

1
2

E[(cid:107)GN

ˆπ − GN
(cid:104)(cid:0)(cid:107)GN
≤ E[(cid:107)GN

≤ E

ˆπ − E[GN

ˆπ − E[GN
ˆπ ](cid:107)2
1]
|X | · |U|
√
N

=

1

√

|X | · |U|
4N

.

≤ 2

ˆπ ](cid:107)1 + (cid:107)GN

(cid:1)2(cid:105) 1

2

π − E[GN

π ](cid:107)1
π − E[GN

π ](cid:107)2
1]

1
2

2 + E[(cid:107)GN

Chebyshev’s inequality implies

P (cid:0)(cid:107)GN

ˆπ − GN

π (cid:107)1 ≥ (cid:15)(cid:1) ≤

|X |2|U|2
(cid:15)2N

(24)

, GN

t )(f ) − L(ˆx0,N

t

, ˆGN

t )(f )

(cid:12)
(cid:12)
(cid:12)

independent of π ∈ ΠN
N .

Then analogously to the proof of Theorem 1,

sup
π∈ΠN
N

t

(cid:12)
(cid:12)L(x0,N
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

= sup
π∈ΠN
N
(cid:88)

≤

x0∈X 0

(cid:104)
f (x0,N
t

, GN
t )

(cid:105)

(cid:104)

− E

(cid:105)(cid:12)
(cid:12)
(cid:12)

f (ˆx0,N
t

, ˆGN
t )
t = x0(cid:105)(cid:12)
(cid:12)
(cid:12)

, GN

t ) | x0,N

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

f (x0,N
t

sup
π∈ΠN
N

(cid:12)
(cid:12)L(x0,N
(cid:12)

t

)(x0) − L(ˆx0,N

t

(cid:12)
)(x0)
(cid:12)
(cid:12)

· sup
π∈ΠN
N
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
π∈ΠN
N
(cid:104)

− E

(cid:88)

+

x0∈X 0

(cid:104)
f (x0, ˆGN

E

t = x0(cid:105)
t ) | ˆx0,N
t = x0(cid:105) (cid:12)

(cid:12)
(cid:12)
(cid:12)

f (x0, GN

t ) | x0,N

L(ˆx0,N
t

)(x0)

· sup
π∈ΠN
N

where the ﬁrst sum goes to zero by assumption and
boundedness of f . For the second term, consider arbitrary

(cid:104)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
π∈ΠN
N

f (x0, ˆGN

t ) | ˆx0,N

t = x0(cid:105)

(cid:104)

− E

f (x0, GN

t ) | x0,N

t = x0(cid:105) (cid:12)

(cid:12)
(cid:12)
(cid:12)

→ 0

can be concluded, showing the desired implication.

t

N

We now show by induction over all t that for any t ≥ 0,
and any f : X 0 × P(X × U) → R continuous and bounded,
(cid:12)
t )(f ) − L(ˆx0,N
(cid:12)
supπ∈ΠN
t )(f )
(cid:12) → 0 which
by Assumption 1 will again imply that (22) goes to zero.

(cid:12)
(cid:12)L(x0,N
(cid:12)

, ˆGN

, GN

At t = 0, we have L(x0,N

0 = L(ˆx0,N
) = µ0
), implying
0
(cid:12)
, ˆGN
0 )(f ) − L(ˆx0,N
(cid:12)
(cid:12) → 0 for any
0 )(f )
supπ∈ΠN
f : X 0 × P(X × U) → R by the prequel. Assuming the
induction assumption holds at time t, then at time t + 1

(cid:12)
(cid:12)L(x0,N
(cid:12)

, GN

0

0

0

N

t

(cid:107)L(x0,N

t+1) − L(ˆx0,N

t+1)(cid:107)1

P 0(x0 | x0,N

t

(cid:88)

=

x0∈X 0

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

→ 0

, GN

t ) − P 0(x0 | ˆx0,N

t

, ˆGN
t )

(cid:105)(cid:12)
(cid:12)
(cid:12)

uniformly by induction assumption and continuity and bound-
edness of P 0, which implies the desired statement.

Corollary 1. Under Assumption 1, for any (cid:15) > 0 there exists
N ((cid:15)) such that for all N > N ((cid:15)) a policy π∗ optimal in the
MFC MDP – that is, J(π∗) = supπ∈Π J(π) – is (cid:15)-Pareto
optimal in the N -agent case, i.e.

J N (πN (π∗)) ≥ sup
π

J N (π) − (cid:15) .

(25)

Proof. By Theorem 1 and Theorem 2, there exists N (cid:48) ∈ N
such that for average policy ˆπ of π and all N > N (cid:48) we have
(cid:0)J N (π) − J N (πN (π∗))(cid:1)

sup
π

(cid:0)J N (π) − J N (πN (ˆπ))(cid:1)
(cid:0)J N (πN (ˆπ)) − J(ˆπ)(cid:1)

(J(ˆπ) − J(π∗))
(cid:0)J(π∗) − J N (πN (π∗))(cid:1)

≤ sup
π
+ sup
π
+ sup
π
+ sup
π

(cid:15)
3
Reordering terms gives the desired inequality.

+ 0 +

= (cid:15) .

(cid:15)
3

(cid:15)
3

<

+

V. DYNAMIC PROGRAMMING PRINCIPLE

The following dynamic programming principle for the
MFC MDP is a standard result, for which the MDP state will
be only the environment state, see e.g. [20], [22].
Deﬁne action-value function Q : X 0 × H → R,

Q(x0, h) := sup
π∈Π

E

(cid:34) ∞
(cid:88)

t=0

γtr(x0

t , G(µ0, πt(x0

t )))

| x0

0 = x0, π0(x0) = h(cid:3) .

(26)

arg maxh∈H Q(x0, h) for any x0 ∈ X 0, then the policy π∗
t (x0) = hx0 is an optimal stationary policy.
with π∗

Proof. For uniqueness, deﬁne the space of R
functions Q := {f : X 0 × H → [− R
1−γ , R
Bellman operator B : Q → Q deﬁned by

1−γ -bounded
1−γ ]} and the

(BQ)(x0, h) = r(x0, G(µ0, h))

+ γE˜x0∼P 0(x0,G(µ0,h))

(cid:35)

Q(˜x0, ˜h)

.

(28)

(cid:34)

sup
˜h∈H

We show that Q is a complete metric space under the
supremum norm. Let (Qn)n∈N be a Cauchy sequence of
functions Qn ∈ Q. Then by deﬁnition, for any (cid:15) > 0 there
exists n(cid:48) ∈ N such that for all n, m > n(cid:48) we have

=⇒ ∀x0 ∈ X 0, h ∈ H : (cid:12)

(cid:12)Qn(x0, h) − Qm(x0, h)(cid:12)

(cid:107)Qn − Qm(cid:107)∞ < (cid:15)
(cid:12) < (cid:15)

such that for all x0 ∈ X 0, h ∈ H there exists a value
cx0,h ∈ [− R
1−γ ] for which Qn(x0, h) → cx0,h. Deﬁne
the function Q(cid:48) ∈ Q by Q(cid:48)(x0, h) = cx0,h, then we have

1−γ , R

(cid:12)Qn(x0, h) − Q(cid:48)(x0, h)(cid:12)
(cid:12)
(cid:12)
(cid:12)Qn(x0, h) − Qm(x0, h)(cid:12)
(cid:12)

= lim
m→∞

(cid:12) < (cid:15)

for all x0 ∈ X 0, h ∈ H, n > n(cid:48), and hence Qn → Q(cid:48) ∈ Q
as n → ∞. This implies completeness of (Q, (cid:107)·(cid:107)∞).

We now show that B is a contraction under the supremum

norm, i.e.

(cid:107)BQ1 − BQ2(cid:107)∞ ≤ C(cid:107)Q1 − Q2(cid:107)∞

some C < 1. Deﬁne the shorthand ˜x0 ∼

for
P 0(x0, G(µ0, h)). We have

(cid:107)BQ1 − BQ2(cid:107)∞
(cid:12)BQ1(x0, h) − BQ2(x0, h)(cid:12)
(cid:12)
(cid:12)

=

sup
x0∈X 0,h∈H

γE˜x0

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γ(cid:107)Q1 − Q2(cid:107)∞

sup
˜h∈H

Q1(˜x0, ˜h) − sup
˜h∈H

(cid:35)

(cid:12)
(cid:12)
Q2(˜x0, ˜h)
(cid:12)
(cid:12)
(cid:12)

Note that by boundedness of r, we trivially have

|Q| ≤

R
1 − γ

.

≤

≤

sup
x0∈X 0,h∈H

sup
x0∈X 0,h∈H

As we have an MDP with ﬁnite state space X 0, the

following Bellman equation will hold, see [28].

Theorem 3. The Bellman equation

Q(x0, h) = r(x0, G(µ0, h))

+ γE˜x0∼P 0(x0,G(µ0,h))

(cid:35)
Q(˜x0, ˜h)

(cid:34)

sup
˜h∈H

(27)

holds for all x0 ∈ X 0, h ∈ H.

In the following, we obtain existence of an optimal
stationary policy by compactness of H and continuity of
Q, which shall be inherited from the continuity of r and P 0.

Lemma 2. The unique function that satisﬁes the Bellman
equation is given by Q. Further, if there exists hx0 ∈

with γ < 1. Therefore, by Banach ﬁxed point theorem, B
has the unique ﬁxed point Q.

For optimality, deﬁne the policy action-value function Qπ

for π ∈ Π as the ﬁxed point of Bπ : Q → Q deﬁned by
(BπQ)(x0, h) = r(x0, G(µ0, h)) + γE˜x0

(cid:2)Q(˜x0, π(˜x0))(cid:3) .

From this, we immediately have

Qπ∗

(x0, h) = r(x0, G(µ0, h)) + γE˜x0

= r(x0, G(µ0, h)) + γE˜x0

= Q(x0, h)

(cid:2)Q(˜x0, π∗(˜x0))(cid:3)
(cid:35)
(cid:34)

Q(˜x0, ˜h)

sup
˜h∈H

which implies that π∗ is optimal, see also [28].

Lemma 3. The action-value function Q is continuous.

Proof. We will show as x0

n → x0 ∈ X 0 and hn → h ∈ H,

Q(x0

n, hn) → Q(x0, h) .

TABLE I
PARAMETER AND HYPERPARAMETER SETTINGS USED IN THE

EXPERIMENTS OF THIS WORK.

By the Bellman equation, we immediately have

Symbol

Function

|Q(x0
≤ (cid:12)

n, hn) − Q(x0, h)|
(cid:12)r(x0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜x0∈X 0

+

γ

n, G(µ0, hn)) − r(x0, G(µ0, h))(cid:12)
(cid:12)
(cid:88)

(cid:0)P 0(˜x0 | x0

n, G(µ0, hn))

−P 0(˜x0 | x0, G(µ0, h))(cid:1) sup
˜h∈H

≤ (cid:12)

n, G(µ0, hn)) − r(x0, G(µ0, h))(cid:12)
(cid:12)r(x0
(cid:12)
γR
(cid:12)
(cid:12)P 0(˜x0 | x0
1 − γ

n, G(µ0, hn))

(cid:88)

+

˜x0∈X 0

−P 0(˜x0 | x0, G(µ0, h))(cid:12)

(cid:12) → 0

(cid:12)
(cid:12)
Q(˜x0, ˜h)
(cid:12)
(cid:12)
(cid:12)

cd
M
Bi
∆T
λ
β
γ

lr
λPPO
βPPO
dtarg
(cid:15)
B
Bm
k

Packet drop penalty
Number of queues
Queue buffer sizes
Time step size
Packet arrival rate
Queue servicing rate
Discount factor

PPO

Learning rate
GAE coefﬁcient
Initial KL coefﬁcient
KL target
Clip parameter
Training batch size
SGD mini-batch size
SGD iterations per batch

Value

1
2
5
0.5 s
(3M − 1)
3 s−1
0.99

s−1

5e−5
0.2
0.2
0.01
0.3
4000
128
30

since r, P 0, G are continuous and Q is bounded.

Corollary 2. There exists an optimal stationary policy
π∗ : X 0 → H such that Qπ∗

= Q.

Proof. By Lemma 3, Q is continuous. Furthermore, H is
compact. By the extreme value theorem, there exists hx0 ∈
arg maxh∈H Q(x0, h) for any x0 ∈ X 0. By Lemma 2, there
exists an optimal stationary policy π∗.

Since H is continuous, general exact solutions are difﬁcult.
Instead, we apply reinforcement learning with stochastic
policies to ﬁnd an optimal stationary policy.

VI. EXPERIMENTS

We compare the empirical performance of the mean ﬁeld
solution in the aforementioned scheduling problem. Since
there exist few theoretical guarantees for tractable multi-
agent reinforcement learning methods [5], we compare our
approach (MF) to empirically effective independent learning
(IL) [29], i.e. applying single-agent RL to each separate
agent (NA), as well as the well-known Join-Shortest-Queue
(JSQ) algorithm [26], where agents choose the shortest queue
accessible and otherwise randomly. To make independent
learning more tractable, we also share policy parameters
between all agents using parameter sharing (PS) [30] and
train each policy via the PPO algorithm [31] using the RLlib
1.2.0 Pytorch implementation [32] for 400, 000 time steps
in the N -agent cases and 2 million time steps in the MF
case, which is sufﬁcient for convergence of MF and N -agent
policies up to N = 4, after which N -agent training becomes
unstable under the shared hyperparameters in Table I and
continues to fail even with more time steps.

For policies and critics, we use separate feedforward
networks with two hidden layers of 256 nodes and tanh acti-
vations. In the mean ﬁeld case the policy outputs parameters
µ, σ of a diagonal Gaussian distribution over actions, which
are sampled and clipped between 0 and 1. We normalize each
of these output values such that they give the probability of
assigning to an accessible queue given some agent state, i.e.

Fig. 3. Overview of mean ﬁeld control application in N -agent systems:
Conditional on the environment state x0,N
, the upper-level mean ﬁeld policy
t
ˆπ outputs a sampled, shared lower-level policy for all agents i, from which
random actions ui,N
.

are sampled conditional on local agent states xi,N

t

t

a shared lower-level decision rule h ∈ H for all agents. A
visualization of this process can be found in Figure 3.

Note that we use stochastic policies as required by
stochastic policy gradient methods, though we can easily
obtain a deterministic policy if necessary by simply using the
mean parameter of the Gaussian distribution. In the N -agent
case, we output queue assignment probabilities for each of the
agents via a standard softmax ﬁnal layer. Invalid assignments
to queues that are not accessible by an agent are treated as
randomly sampling one from all accessible queues.

As can be seen in Figure 4 for µ0 given such that the
probability of access to both queues is 0.6 and otherwise
uniformly random, the mean ﬁeld solution reaches its mean
ﬁeld performance in the N -agent case as N grows large. This
validates our theoretical ﬁndings empirically. Our solution
further appears to outperform NA and PS for sufﬁciently
many agents, as IL approaches increasingly fail to learn due
to the credit assignment problem.

Moreover, our best learned policy is close to JSQ and
competitive with slight irregularities at b0 = 0. Observe in
Figure 4 that the MF policy gives an interpretable solution.
As a queue becomes more ﬁlled, the optimal solution will
be more likely to avoid assignment of packets to that queue.

[9] N. Saldi, T. Basar, and M. Raginsky, “Markov–nash equilibria in
mean-ﬁeld games with discounted cost,” SIAM Journal on Control and
Optimization, vol. 56, no. 6, pp. 4256–4287, 2018.

[10] D. Mguni, J. Jennings, and E. M. de Cote, “Decentralised learning
in systems with many, many strategic agents,” Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, 2018.

[11] X. Guo, A. Hu, R. Xu, and J. Zhang, “Learning mean-ﬁeld games,”
in Advances in Neural Information Processing Systems, 2019, pp.
4966–4976.

[12] K. Cui and H. Koeppl, “Approximately solving mean ﬁeld games
via entropy-regularized deep reinforcement learning,” in International
Conference on Artiﬁcial Intelligence and Statistics. PMLR, 2021, pp.
1909–1917.

[13] D. Andersson and B. Djehiche, “A maximum principle for sdes of
mean-ﬁeld type,” Applied Mathematics & Optimization, vol. 63, no. 3,
pp. 341–356, 2011.

[14] A. Bensoussan, J. Frehse, P. Yam et al., Mean ﬁeld games and mean

ﬁeld type control theory. Springer, 2013, vol. 101.
[15] A. C. Kizilkale and R. P. Malhame, “Collective target

tracking
mean ﬁeld control for electric space heaters,” in 22nd Mediterranean
Conference on Control and Automation.
IEEE, 2014, pp. 829–834.
[16] B. Djehiche and H. Tembine, “Risk-sensitive mean-ﬁeld type control
under partial observation,” in Stochastics of Environmental and
Financial Economics. Springer, Cham, 2016, pp. 243–263.

[17] B. Djehiche, H. Tembine, and R. Tempone, “A stochastic maximum
principle for risk-sensitive mean-ﬁeld type control,” IEEE Transactions
on Automatic Control, vol. 60, no. 10, pp. 2640–2649, 2015.

[18] M. F. Djete, D. Possama¨ı, and X. Tan, “Mckean-vlasov optimal control:
the dynamic programming principle,” arXiv preprint arXiv:1907.08860,
2019.

[19] H. Pham and X. Wei, “Bellman equation and viscosity solutions for
mean-ﬁeld stochastic control problem,” ESAIM: Control, Optimisation
and Calculus of Variations, vol. 24, no. 1, pp. 437–461, 2018.
[20] M. Motte and H. Pham, “Mean-ﬁeld markov decision processes
with common noise and open-loop controls,” arXiv preprint
arXiv:1912.07883, 2019.

[21] H. Gu, X. Guo, X. Wei, and R. Xu, “Dynamic programming principles

for learning mfcs,” arXiv preprint arXiv:1911.07314, 2019.

[22] ——, “Mean-ﬁeld controls with q-learning for cooperative marl: Con-
vergence and complexity analysis,” arXiv preprint arXiv:2002.04131,
2020.

[23] K. P. Murphy, Machine learning: a probabilistic perspective. MIT

press, 2012.

[24] M. Nourian and P. E. Caines, “(cid:15)-nash mean ﬁeld game theory for
nonlinear stochastic dynamical systems with major and minor agents,”
SIAM Journal on Control and Optimization, vol. 51, no. 4, pp. 3302–
3331, 2013.

[25] P. E. Caines and A. C. Kizilkale, “(cid:15)-nash equilibria for partially
observed lqg mean ﬁeld games with a major player,” IEEE Transactions
on Automatic Control, vol. 62, no. 7, pp. 3225–3234, 2016.

[26] A. Mukhopadhyay and R. R. Mazumdar, “Analysis of randomized
join-the-shortest-queue (jsq) schemes in large heterogeneous processor-
sharing systems,” IEEE Transactions on Control of Network Systems,
vol. 3, no. 2, pp. 116–126, 2016.

[27] W. R. KhudaBukhsh, S. Kar, B. Alt, A. Rizk, and H. Koeppl,
“Generalized cost-based job scheduling in very large heterogeneous
cluster systems,” IEEE Transactions on Parallel and Distributed
Systems, vol. 31, no. 11, pp. 2594–2604, 2020.

[28] M. L. Puterman, Markov decision processes: discrete stochastic

dynamic programming.

John Wiley & Sons, 2014.

[29] M. Tan, “Multi-agent reinforcement learning: Independent vs. coopera-
tive agents,” in Proceedings of the tenth international conference on
machine learning, 1993, pp. 330–337.

[30] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multi-agent
control using deep reinforcement learning,” in International Conference
on Autonomous Agents and Multiagent Systems. Springer, 2017, pp.
66–83.

[31] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[32] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg,
J. Gonzalez, M. Jordan, and I. Stoica, “Rllib: Abstractions for
distributed reinforcement learning,” in International Conference on
Machine Learning. PMLR, 2018, pp. 3053–3062.

Fig. 4.
(a): Cumulative reward average over 500 runs with 95% conﬁdence
interval achieved against number of agents N . The dotted line indicates
cumulative reward of MF in the MFC MDP. NA and PS are trained separately
for each N , while the MF policy is trained only once and used for all N .
As N grows, the MF policy performance becomes increasingly close to
the MFC MDP and competitive with JSQ, while NA and PS begin to fail
learning due to the credit assignment problem. (b): MF policy probabilities
of assigning to queue 1 against buffer ﬁllings b0, b1 for agents with access
to both queues, averaged over 500 samples.

VII. CONCLUSION

In this work, we have formulated a discrete-time mean ﬁeld
control model with common environment states motivated by
a scheduling problem. We have rigorously shown approximate
optimality as N → ∞ and applied reinforcement learning
to solve the MFC MDP. Empirically, we obtain competitive
results for sufﬁciently many agents and validate our theoretical
results. For future work, it could be interesting to consider
partial observability of the system for schedulers, or methods
to scale to large numbers of queues. Potential extensions
are manifold and include dynamic agent states, major-minor
systems, partial observability and general non-ﬁnite spaces.

ACKNOWLEDGMENT

This work has been co-funded by the LOEWE initiative
(Hesse, Germany) within the emergenCITY center, the Euro-
pean Research Council (ERC) within the Consolidator Grant
CONSYN (grant agreement no. 773196) and the German
Research Foundation (DFG) as part of sub-project C3 within
the Collaborative Research Center (CRC) 1053 – MAKI.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[2] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238–1274, 2013.

[3] N. Brown and T. Sandholm, “Superhuman ai for multiplayer poker,”

Science, vol. 365, no. 6456, pp. 885–890, 2019.

[4] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C.
Liang, and D. I. Kim, “Applications of deep reinforcement learning in
communications and networking: A survey,” IEEE Communications
Surveys & Tutorials, vol. 21, no. 4, pp. 3133–3174, 2019.

[5] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learning:
A selective overview of theories and algorithms,” Handbook of
Reinforcement Learning and Control, pp. 321–384, 2021.

[6] M. Huang, R. P. Malham´e, P. E. Caines et al., “Large population
stochastic dynamic games: closed-loop mckean-vlasov systems and the
nash certainty equivalence principle,” Communications in Information
& Systems, vol. 6, no. 3, pp. 221–252, 2006.

[7] J.-M. Lasry and P.-L. Lions, “Mean ﬁeld games,” Japanese journal of

mathematics, vol. 2, no. 1, pp. 229–260, 2007.

[8] D. A. Gomes, J. Mohr, and R. R. Souza, “Discrete time, ﬁnite state
space mean ﬁeld games,” Journal de math´ematiques pures et appliqu´ees,
vol. 93, no. 3, pp. 308–328, 2010.

(a)(b)