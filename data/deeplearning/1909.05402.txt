Generalized Policy Iteration for Optimal Control in
Continuous Time

Jingliang Duan, Shengbo Eben Li*, Zhengyu Liu, Monimoy Bujarbaruah, and Bo Cheng

1

9
1
0
2

p
e
S
1
1

]

Y
S
.
s
s
e
e
[

1
v
2
0
4
5
0
.
9
0
9
1
:
v
i
X
r
a

Abstract—This paper proposes the Deep Generalized Policy
Iteration (DGPI) algorithm to ﬁnd the inﬁnite horizon optimal
control policy for general nonlinear continuous-time systems with
known dynamics. Unlike existing adaptive dynamic programming
algorithms for continuous time systems, DGPI does not require
the “admissibility” of initialized policy, and input-afﬁne nature
of controlled systems for convergence. Our algorithm employs
the actor-critic architecture to approximate both policy and
value functions with the purpose of
iteratively solving the
Hamilton-Jacobi-Bellman equation. Both the policy and value
functions are approximated by deep neural networks. Given
any arbitrary initial policy, the proposed DGPI algorithm can
eventually converge to an admissible, and subsequently an
optimal policy for an arbitrary nonlinear system. We also relax
the update termination conditions of both the policy evaluation
and improvement processes, which leads to a faster convergence
speed than conventional Policy Iteration (PI) methods, for the
same architecture of function approximators. We further prove
the convergence and optimality of the algorithm with thorough
Lyapunov analysis, and demonstrate its generality and efﬁcacy
using two detailed numerical examples.

Index Terms—Dynamic programming, Reinforcement learn-

ing, Optimal control.

I. INTRODUCTION

D YNAMIC programming offers a theoretical and sys-

tematic way to solve Continuous Time (CT) inﬁnite
horizon optimal control problems with known dynamics for
unconstrained linear systems, by employing the principle
of Bellman optimality via the solution of the underlying
Hamilton-Jacobi-Bellman (HJB) equation [1]. This yields the
celebrated Linear Quadratic Regulator, where the optimal
control policy is an afﬁne state feedback [2]. However, if
the system is subject to operating constraints, or is modeled
by nonlinear dynamics, solving an inﬁnite horizon optimal
control problem analytically is a challenging task. This is
due to the fact that it is difﬁcult to get an analytical solution
of the HJB equation (typically nonlinear partial differential
equation [3]) by applying traditional DP, as the computation
grows exponentially with increase in the dimensionality of the
system, summarized by the phrase curse of dimensionality [4].

All correspondences should be sent to S. Li with email: lisb04@gmail.com.
J. Duan, S. Li, Z. Liu and B., Cheng are with State Key Lab of Automotive
Safety and Energy, School of Vehicle and Mobility, Tsinghua University,
Beijing, 100084, China. They are also with Center for Intelligent Connected
Vehicles and Transportation, Tsinghua University, Beijing, China. Email:
(djl15, liuzheng17)@mails.tsinghua.edu.cn;
(lishbo, chengbo)@tsinghua.edu.cn.

To ﬁnd a suboptimal approximation to the optimal control
policy for nonlinear dynamics, Werbos deﬁned a family of
actor-critic algorithms, which he termed Adaptive Dynamic
Programming (ADP) algorithms [5], [6]. Another well-known
name for this kind of algorithms, especially in the ﬁeld of
machine learning, is Reinforcement Learning (RL) [7], [8].
A distinct feature of the ADP method is that it employs a
critic parameterized function, such as a Neural Network (NN)
for value function approximation and an actor parameterized
function for policy approximation. For the sake of ﬁnding
suitable approximation of both value function and policy,
most ADP methods adopt an iterative technique, called Policy
Iteration (PI) [9]. PI refers to a class of algorithms built
as a two-step iteration: 1) policy evaluation, in which the
value function associated with an admissible control policy
is evaluated, and 2) policy improvement, in which the policy
is updated to optimize the corresponding value function, using
Bellman’s principle of optimality.

Over the last few decades, numerous ADP (or RL) methods
and the inherent analyses have appeared in literature for
controlling autonomous systems [10]–[17], including for CT
systems. Some of these algorithms for CT systems are also
called approximate DP or neuro-DP [18]–[21]. Abu-Khalaf
and Lewis (2005) proposed an ADP algorithm to ﬁnd nearly
optimal constrained control state feedback laws for general
nonlinear systems by introducing a non-quadratic cost function
[22]. The value function represented by a linear combination
of artiﬁcially designed basis functions is trained by least-
squares method at the policy evaluation step, while the policy
is directly derived from the value function. Utilizing the same
single approximator scheme, Dierks and Jagannathan (2010)
derived a novel online parameter tuning law that not only
ensures the optimal value function and policy are achieved,
but also ensures the system states remain bounded during
the online learning process [23]. Vamvoudakis and Lewis
(2012) proposed a synchronous PI algorithm implemented
as actor-critic architecture for nonlinear CT systems without
control constraints. Both the value function and policy are
approximated by linear methods and tuned simultaneously
online [24]. Furthermore, Vamvoudakis (2014) presented an
event-triggered ADP algorithm that reduces the computation
cost by updating the policy only when an event-triggered
condition was violated [25]. Dong et al. (2017) extended this
idea to nonlinear systems with saturated actuators [26]. In
addition, the ADP method has also been widely applied in
the optimal control of incompletely known dynamic systems
[27]–[31] and multi-agent systems [32], [33].

J. Duan and M. Bujarbaruah are with Department of Mechanical Engineer-
ing, University of California Berkeley, Berkeley, CA 94720, USA. Email:
It should be pointed out that most existing ADP techniques
monimoyb@berkeley.edu.
©2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
for CT systems are valid on the basis of one or both of the
following two assumptions:

• A1: Admissibility of Initial Policy: The inﬁnite horizon
value function can be evaluated only in the case of
stabilizing control policies. Hence, the initial policy must
be “admissible”, that is, it has to stabilize the system
(detailed in Deﬁnition 1). However, in practical situations,
especially for complex systems, it is often difﬁcult to
obtain an admissible policy.

• A2: Input-Afﬁne Nature of System: Most ADP methods
are subject to input-afﬁne systems. This is due to the fact
that these methods require that the optimal policy needs
to be directly represented by the value function. Which
means that the minimum point of the Hamilton function
could be solved analytically, when the value function is
given. For non input-afﬁne systems, directly solving the
optimal policy in this way is often intractable.

In this paper, we propose a Deep Generalized Policy
Iteration (DGPI) algorithm with proof of convergence and
optimality, for solving optimal control problems of general
nonlinear CT systems with known dynamics to overcome the
limitation of the above two central assumptions. Both the
actor and critic are approximated by deep NNs which build
a map from the system states to action and value function
respectively. Our main contributions can be summarized as
follows:

1) Given any arbitrary initial policy, the proposed DGPI al-
gorithm is proven to converge to an admissible policy by
continuously minimizing the square of the Hamiltonian.
This relaxes the requirement of A1.

2) We prove faster convergence speeds of DGPI than
corresponding PI methods, due to novel update ter-
mination conditions of both the policy evaluation and
improvement processes. The policy network is updated
by directly minimizing the associated Hamiltonian, and
the tuning rules are generalized to arbitrary nonlinear,
non input-afﬁne dynamics. This relaxes the requirement
of A2.

The paper is organized as follows. In Section II, we provide
the formulation of the optimal control problem, followed by
the general description of PI and DPI algorithm. In Section III,
we describe the DGPI algorithm and analyze its convergence
and optimality. In Section IV, we present simulation exam-
ples that show the generality and effectiveness of the DGPI
algorithm for CT system. Section V concludes this paper.

II. MATHEMATICAL PRELIMINARIES

A. HJB Equation of
Problem

the Continuous-time Optimal Control

Consider the general time-invariant dynamical system given

by

˙x(t) = f (x(t), u(t)),

(1)

with state x(t) ∈ Rn, control
input u(t) ∈ Rm, and
f : Rn × Rm → Rn. We assume that f (x(t), u(t)) is
Lipschitz continuous on a compact set Ω that contains the
origin, and that the system is stabilizable on Ω, i.e., there exists

2

a continuous policy π(x), where u(t) = π(x(t)), such that the
system is asymptotically stable on Ω. The system dynamics
f (x(t), u(t)) is assumed to be known, it can be nonlinear and
input non-afﬁne analytic functions, Neural Networks (NNs),
or even a MATLAB/Simulink model (only if ∂f
∂u is available).
Moreover, the system input u(t) can be either constrained or
unconstrained. Given the policy π(x), deﬁne its associated
inﬁnite horizon value function
(cid:90) ∞

V π(x(t)) =

l(x(τ ), π(x(τ ))dτ,

(2)

t

where l(x, u) : Rn × Rm → R is positive deﬁnite, i.e. x (cid:54)=
0 ∨ u (cid:54)= 0 → l(x, u) > 0, and x = 0 ∧ u = 0 → l(x, u) = 0.
For dynamics in (1) with the value function in (2), introduce
the associated Hamiltonian

H(x, u,

∂V π(x)
∂x

) = l(x, u) +

∂V π(x)
∂x(cid:62) f (x, u).

(3)

Deﬁnition 1. (Admissible Policy). A control policy π(x) is
deﬁned as admissible with respect to (2) on Ω, denoted by
π(x) ∈ Ψ(Ω), if π(x) is continuous on Ω, π(0) = 0, u(x) =
π(x) stabilizes (1) on Ω, and V (x) is ﬁnite x ∈ Ω [22].

For any control policy π(x) ∈ Ψ(Ω),

the differential
equivalent to (2) on Ω is a sort of a Lyapunov equation for
systems in (1)

H(x, π(x),

∂V π(x)
∂x

) = 0,

V π(0) = 0.

(4)

Therefore, given a policy π(x) ∈ Ψ(Ω), the value function in
(2) associated with the system (1) can be found by solving
the Lyapunov equation. Then the optimal control problem for
continuous-time (CT) system can now be formulated as ﬁnding
a policy π(x) ∈ Ψ(Ω) such that the value function associated
with systems in (1) is minimized. The minimized or optimal
value function V ∗(x(t)) deﬁned by
(cid:90) ∞

V ∗(x(t)) = min

l(x(τ ), π(x(τ ))dτ,

π(x)∈Ψ(Ω)

t

satisﬁes the Hamilton-Jacobi-Bellman (HJB) equation [3]
∂V ∗(x)
∂x

min
π(x)∈Ψ(Ω)

V ∗(0) = 0.

H(x, u,

) = 0,

(5)

Meanwhile, the optimal control π∗(x) for every state x ∈ Ω

can be derived as

π∗(x) = arg min

u

H(x, u,

∂V ∗(x)
∂x

).

(6)

Inserting this optimal control policy π∗(x) and optimal value
function V ∗(x) in the Lyapunov equation, we obtain the
formulation of the HJB equation in terms of ∂V ∗(x)
and u∗
[3]

∂x

l(x, u∗) +

V ∗(0) = 0.

∂V ∗(x)
∂x(cid:62) f (x, u∗) = 0,
Existence and uniqueness of the value function has been shown
in [34]. In order to ﬁnd the optimal policy for CT systems
one only needs to solve the HJB equation (5) for the value
function and then substitute the solution into (6) to obtain
the optimal control. However, due to the nonlinear nature of
the HJB equation, ﬁnding its solution is generally difﬁcult or
impossible.

B. Policy Iteration

The proposed algorithm for CT system used in this paper is
motivated by Policy Iteration (PI) [7] . Therefore in this section
we describe PI. PI is an iterative method of reinforcement
learning (RL) for solving optimal policy of CT or discrete-
time systems, and involves computation cycles between policy
evaluation based on (4) and policy improvement based on (6).
The pseudo-code of PI is shown in Algorithm 1.

Algorithm 1 PI algorithm

Initial with policy π0(x) ∈ Ψ(Ω)
Given an arbitrarily small positive (cid:15) and set i = 0
while (cid:107)V i+1(x) − V i(x)(cid:107) ≤ (cid:15) do

1. Solve value function V i(x) for ∀x ∈ Ω using

l(x, πi(x)) +

∂V i
∂x(cid:62) f (x, πi(x)) = 0,

V i(0) = 0

(7)

2. Solve new policy πi+1(x) for ∀x ∈ Ω using

πi+1(x) = arg min

[l(x, u) +

u

∂V i
∂x(cid:62) f (x, u)]

(8)

end while

As Algorithm 1 shows, the ﬁrst step of PI is to ﬁnd an initial
policy π0(x) ∈ Ψ(Ω) because the associated value function
V 0(x) is ﬁnite only when the system is asymptotically stable.
Algorithm 1 will iteratively converge to the optimal control
policy π∗(x) ∈ Ψ(Ω) and value function V ∗(x). Proofs of
convergence and optimality have been given in [22].

C. Value Function and Policy Approximation

In previous adaptive dynamic programming (ADP) re-
searches for CT systems, the value function V i(x) and policy
πi(x) are usually approximated by linear methods, which
requires a large number of artiﬁcially designed basis func-
tions [31]. In recent years, deep NNs are favored in many
ﬁelds such as RL and machine learning due to their better
generality and higher ﬁtting ability [35], [36]. In our work,
both the value function and policy are approximated by deep
NNs, called respectively the value network (or critic network)
V (x; ω) (Vω(x) for short) and the policy network (or actor
network) π(x; θ) (πθ(x) for short), where w and θ are network
parameters. These two networks directly build a map from
the raw system states to the approximated value function and
control inputs respectively; in this case, no hand-crafted basis
function is needed.

Inserting the value and policy network in (3), we obtain the
formulation of approximate Hamiltonian in terms of w and θ

H(x, ω, θ) = l(x, π(x; θ)) +

∂V (x; ω)

∂x(cid:62) f (x, π(x; θ)).

We refer to the algorithm combining PI and deep NN approx-
imators as Deep PI (DPI), which involves alternatively tuning
each of the two networks to ﬁnd optimal parameters ω∗ and
θ∗ such that V ∗(x) = V (x; ω∗), π∗(x) = π(x; θ∗).

The policy evaluation process of DPI proceeds by tuning
the value network by solving (7). Given any policy π(x; θ) ∈

3

Ψ(Ω), it is desired to ﬁnd parameters w to minimize the critic
loss function

Lc(ω, θ) = Ex∈Ω

(cid:2)H(x, ω, θ)2(cid:3).

(9)

Noted that V (x; ω) ≡ 0 can be easily guaranteed by selecting
proper activation function σV (·) for the value network. Based
on (8), the policy improvement process is carried out by tuning
the policy network to minimize expectation of Hamiltonian in
each state, which is also called actor loss function here

La(ω, θ) = Ex∈Ω

(cid:2)H(x, ω, θ)(cid:3).

(10)

Many off-the-shelf NN optimization methods can be used to
tune these two NNs, such as Stochastic Gradient Descent
(SGD), RMSProp, Levenberg Marquardt or Adam [37]. In
fact, the value network and policy network usually require
multiple updating iterations to make (7) and (8) hold respec-
tively. Therefore, compared with the PI algorithm mentioned
above,
two inner updating loops would be introduced to
update value network and policy network respectively until
convergence. Taking the SGD optimization method as an
example, the pseudo-code of DPI is shown in Algorithm 2.

Algorithm 2 DPI algorithm

Initial with θ0 such that πθ0(x) ∈ Ψ(Ω) and arbitrary ω0
Choose the appropriate learning rates αω and αθ
Given an arbitrarily small positive (cid:15) and set i = 0
while (cid:107)Vωi+1(x) − Vωi(x)(cid:107) ≤ (cid:15) do

1. Estimate Vωi+1(x) using πθi(x)

ωi+1 = ωi
repeat

ωi+1 = ωi+1 − αω

dLc(ωi+1, θi)
dωi+1

(11)

until Lc(ωi+1, θi) ≤ (cid:15)

2. Find improved policy πθi+1(x) using Vωi+1(x)

θi+1 = θi
repeat

La,old = La(ωi+1, θi+1)

θi+1 = θi+1 − αθ

dLa(ωi+1, θi+1)
dθi+1
until |La(ωi+1, θi+1) − La,old| ≤ (cid:15)

(12)

end while

III. DEEP GENERALIZED POLICY ITERATION ALGORITHM

Algorithm 2 proceeds by alternately updating the value
and policy network by minimizing (9) and (10) respectively.
Note that while one NN is being tuned, the other is held
constant. Besides, each NN usually requires multiple updating
iterations to satisfy the terminal conditions, which is the
so-called protracted iterative computation problem [7]. This
problem usually leads to the admissibility requirement because
the initial policy network needs to satisfy πθ0 (x) ∈ Ψ(Ω)
to have a ﬁnite and converged value function Vω1(x). Many

4

previous studies used trials and errors process to obtain the
range of the initial weights for the policy network to keep the
stability of the system [8], [25]. However, this method usually
takes a lot of time, especially for complex systems. On the
other hand, the protracted problem also often results in slower
learning [7].

A. Description of the DGPI Algorithm

Inspired by the idea of generalized PI framework, which is
typically utilized in discrete-time dynamic RL problems [7],
we present the Deep Generalized PI (DGPI) algorithm for CT
systems to relax the requirement A1 (from Introduction) and
improve the learning speed by truncating the inner loops (re-
laxing the requirement A2) of Algorithm 2 without losing the
convergence guarantees. The pseudo-code of DGPI algorithm
shown in Algorithm 3.

Algorithm 3 DGPI algorithm

Initial with arbitrary θ0 and ω0
Choose the appropriate learning rates α, αω and αθ
Given an arbitrarily small positive (cid:15) and set i = 0
Phase 1: Warm-up

while maxx∈Ω H(x, ωi, θi) ≤ 0 do

Update ω and θ using:

Assumption 2. If both the value network Vω and policy
network πθ are over-parameterized, the global minimum of
the critic loss function in (9) and actor loss function in (10)
can be found respectively using an appropriate optimization
algorithm such as SGD.

Next, the convergence property of Algorithm 3 will be
established. As the iteration index i tends to inﬁnity, we will
show that the optimal value function and optimal policy can be
achieved using Algorithm 3. Before the main theorem, some
lemmas are necessary at this point.

Lemma 1. (Universal Approximation Theorem). For any con-
tinuous function F (x) on a compact set Ω, there exists a
feed-forward NN, having only a single hidden layer, which
uniformly approximates F (x) and its gradient to within arbi-
trarily small error (cid:15) ∈ R+ on Ω [40].

Lemma 1 allows us to ignore the NN approximation errors

when proving convergence of Algorithm 3.

Lemma 2. Consider the CT dynamic optimal control problem
for (1) and (2). Suppose V π(x) ∈ C 1 : Rn → R is a smooth
positive deﬁnite solution to the HJB in (5). The control policy
π(x) is given by (6). Then we have that V π(x) = V ∗(x) and
π(x) = π∗(x) [3].

{ωi+1, θi+1} = {ωi, θi} − α

dLc(ωi, θi)
d{ωi, θi}

(13)

The following lemma shows how Algorithm 3 can be used
to obtain a policy π(x; θ) ∈ Ψ(Ω) given any initial policy
π(x; θ0).

end while

Phase 2: PI with relaxed termination conditions

while (cid:107)Vωi+1(x) − Vωi(x)(cid:107) ≤ (cid:15) do

1. Estimate Vωi+1(x) using πθi(x)

ωi+1 = ωi
repeat

Update ω using (11)

until H(x, ωi, θi) ≤ H(x, ωi+1, θi) ≤ 0, ∀x ∈ Ω

2. Find improved policy πθi+1(x) using Vωi(x)

θi+1 = θi
repeat

Update θ using (12)

until maxx∈Ω H(x, ωi+1, θi+1) ≤ 0

end while

B. Convergence and Optimality Analysis

The solution to (7) may not be smooth for general nonlinear
non input-afﬁne systems. However, in keeping with other work
in the literature [24] we make the following assumption.

Assumption 1. The solution to (7) is smooth if π(x) ∈ Ψ(Ω),
i.e. V π(x) ∈ C 1(Ω) [22], [24].

In recent years, many experimental results and theoretical
proofs have shown simple optimization algorithms such as
SGD can ﬁnd global minima on the training objective of deep
NNs in polynomial time if the network is over-parameterized
(i.e., the number of hidden neurons is sufﬁciently large) [38],
[39]. Based on this fact, our second assumption is:

i.e.,

Lemma 3. Consider the CT dynamic optimal control problem
for (1) and (2). The value function V (x; ω) and policy π(x; θ)
are represented by over-parameterized NNs. The parameters
the initial policy
w and θ are initialized randomly,
π(x; θ0) can be inadmissible. These two NNs are updated with
Algorithm 3. Let Assumption 1 and 2 hold, and suppose all the
hyper-parameters (such as α, αw and αθ) and NN optimization
method are properly selected. The NN approximation errors
are ignored according to Lemma 1. Suppose all the activation
functions σV (·) and biases bV of the value network V (x; ω)
are set to σV (0) = 0 and bV ≡ 0, and the output layer
activation function σVout satisﬁes σVout (·) ≥ 0. We have that:
∃Na ∈ Z+, if i ≥ Na, then π(x; θi) ∈ Ψ(x) for systems (1)
on Ω.

Proof. According to (4) and Lemma 1, if πθ ∈ Ψ(Ω), there
exists parameters (ω, θ), such that H(x, ω, θ) = 0 for all x ∈
Ω. It follows that

min
ω,θ

H(x, ω, θ) ≤ 0,

∀x ∈ Ω,

which implies that the global minima of loss function Lc is
equal to 0, corresponding to the Hamiltonian vanishing for all
states x ∈ Ω. From Lemma 1, utilizing the fact that global
H(x, ω, θ)2(cid:105)
minima of Lc = Ex∈Ω
can be obtained, one has

(cid:104)

min
ω,θ

Lc(ω, θ) = min
ω,θ

Ex∈Ω

(cid:104)

H(x, ω, θ)2(cid:105)

= 0.

Since Algorithm 3 updates ω and θ using (13) to continuously
minimize Lc(ω, θ) in Phase 1 if maxx∈Ω H(x, ωi, θi) > 0,

according to Assumption 2, there exists Na ∈ Z+, such that

H(x, ωNa , θNa ) ≤ 0,

∀x ∈ Ω.

(14)

Take the time derivative of V (x; ω) to obtain

dV (x; ω)
dt

∂V (x; ω)

=

∂x(cid:62) f (x, π(x; θ)),
= H(x, ω, θ) − l(x, π(x; θ)).

(15)

Using (14) and (15), one has

dV (x; ωNa )
dt

≤ −l(x, π(x; θNa )),

∀x ∈ Ω.

As the utility function l(x, π(x; θ)) is positive deﬁnite,
follows

it

dV (x; ωNa )
dt

< 0,

∀x ∈ Ω\{0}.

(16)

Since σV (0) = 0, bV ≡ 0 and σVout(·) ≥ 0, we have

(cid:40)

V (x; ω) ≥ 0, ∀x ∈ Ω\{0} ∧ ∀ω,

V (x; ω) ≡ 0, x = 0 ∧ ∀ω.

From (16) and (17), we have

V (x; ωNa ) > min
z∈Ω

V (z; ωNa ) = 0,

∀x ∈ Ω\{0}.

(18)

From (17) and (18), we infer that the V (x; ωNa ) is positive
deﬁnite. Then, according to (16), V (x; ωNa ) is a Lyapunov
function for closed loop dynamics obtained from (1) when
policy π(x; θNa ) is used. Therefore, the policy π(x; θNa ) ∈
Ψ(Ω) for the system in (1) on Ω [41], that is, it is a stabilizing
admissible policy.

At this point, Algorithm 3 enters Phase 2. According to (4),

one has

Lc(ω, θNa ) = 0.

min
ω

So, from Assumption 2 and Lemma 1, we can always ﬁnd
ωNa+1 by continuously applying (11), such that

H(x, ωNa , θNa ) ≤ H(x, ωNa+1, θNa ) ≤ 0,

∀x ∈ Ω.

Again, from Lemma 1, utilizing the fact that global minima
of La = Ex∈Ω

can be obtained, we get

(cid:104)
H(x, ω, θ)

(cid:105)

La(ωNa+1, θ) = Ex∈Ω

min
θ

(cid:104)

min
θ

(cid:105)
H(x, ωNa+1, θ)

.

This implies that Hamiltonian H(x, ωNa+1, θ) can be taken to
global minimum, for any value of x, by minimizing over θ.
Then, we can also ﬁnd θNa+1 through (12), such that

H(x, ωNa+1, θNa+1) ≤ H(x, ωNa+1, θNa ) ≤ 0,

∀x ∈ Ω.

This implies that like the case with V (x; ωNa ), V (x; ωNa+1) is
also a Lyapunov function. So, π(x; θNa+1) ∈ Ψ(Ω). Extending
this for all subsequent time steps, V (x; ωi) is a Lyapunov
function for all i ≥ Na, and it is obvious that

H(x, ωi, θi) ≤ H(x, ωi+1, θi) ≤ 0,

∀i ≥ Na ∧ ∀x ∈ Ω,

and

π(x; θi) ∈ Ψ(Ω),

∀i ≥ Na.

(19)

(20)

5

This proves Lemma 3. We have thus proven that starting from
any arbitrary initial policy, the DGPI algorithm in Algorithm 3
converges to an admissible policy. As claimed previously, this
relaxes the requirement A1, which is typical to most other
ADP algorithms.

We now present our main result. It is shown in the following
theorem that the value function Vω(x) and policy πθ(x) con-
verge to optimum uniformly by applying DGPI Algorithm 3.

Deﬁnition 2. (Uniform Convergence). A sequence of functions
{fn} converges uniformly to f on a set K if ∀(cid:15) > 0, ∃N ((cid:15)) ∈
Z+ : n > N → supx∈K |fn(x) − f (x)| < (cid:15).
Theorem 1. For arbitrary V (x; ω0) and π(x; θ0), if these
two NNs are updated with Algorithm 3, V (x; ωi) → V ∗(x),
π(x; θi) → π∗(x) uniformly on Ω as i goes to ∞.

Proof. From Lemma 3, it can be shown by induction that the
policy π(x; θi) ∈ Ψ(Ω) for system in (1) on Ω when i ≥ Na.
Furthermore, according to (15) and (19),

(17)

dV (x; ωi)
dt

≤

dV (x; ωi+1)
dt

≤ 0,

∀x ∈ Ω ∧ i ≥ Na.

(21)

From Newton-Leibniz formula,

V (x(t); ω) = V (x(∞); ω) −

According to (17) and (20),

(cid:90) ∞

t

dV (x(τ ); ω)
dτ

dτ.

(22)

V (x(∞); ω) = V (0; ω) = 0,

i ≥ Na ∧ ∀ω.

(23)

So, from (17), (21), (22) and (23), it follows that

0 ≤ V (x; ωi+1) ≤ V (x; ωi),

∀x ∈ Ω ∧ i ≥ Na.

As such, V (x; ωi) is pointwise convergent as i → ∞. We can
write limi→∞ V (x; ωi) = V (x; ω∞). Because Ω is compact,
then uniform convergence follows immediately from Dinis
theorem [42].

From Deﬁnition 2, given arbitrarily small (cid:15)V > 0, ∃Nc ≥

Na, such that

sup
x∈Ω

|V (x; ωi)−V (x; ωi+1)|,

≤ sup
x∈Ω
< (cid:15)V ,

(cid:12)
(cid:12)V (x; ωi) − V (x; ω∞)(cid:12)
(cid:12) ,

∀i ≥ Nc.

According to (15) and (22), one has

V (x; ωi) − V (x; ωi+1)

=

=

(cid:90) ∞

t
(cid:90) ∞

t

d(V (x(τ ); ωi+1) − V (x(τ ); ωi))
dτ

dτ,

(cid:104)

(cid:105)
H(x(τ ), ωi+1, θi) − H(x(τ ), ωi, θi)

dτ.

Since limi→∞ supx∈Ω |V (x; ωi) − V (x; ωi+1)| = 0, we have

lim
i→∞

sup
x∈Ω

|H(x, ωi+1, θi) − H(x, ωi, θi)| = 0.

From Lemma 1, (4) and (20),

Lc(ω, θi) = 0,

∀i ≥ Nc.

min
ω

So, it is true that

6

lim
i→∞

H(x, ωi, θi) = lim
i→∞

H(x, ωi+1, θi) = 0,

∀x ∈ Ω.

(24)
Therefore, V (x; ω∞) and π(x; θ∞) are the solution of the
Lyapunov equation (4), and it follows that

V (x; ω∞) = V πθ∞ (x).

Policy πθi ∈ Ψ(Ω) for i ≥ Na, therefore the state trajec-
tories generated by it is unique due to the locally Lipschitz
continuity assumption on the dynamics [22]. Since V (x; ωi)
converges uniformly to V (x; ω∞), this implies that the system
trajectories converge for all x ∈ Ω. Therefore, π(x; θi) also
converges uniformly to π(x; θ∞) on Ω. From (24), it is also
obvious that

lim
i→∞

min
θ

H(x, ωi, θ) = 0,

∀x ∈ Ω.

(25)

it

(25) and Lemma 2,

According to (24),
follows that
limi→∞ Vωi(x) = V ∗(x) and limi→∞ πθi(x) = π∗(x).
Therefore, we can conclude that Vωi (x) → Vω∗ (x) and
πθi(x) → πθ∗ (x) uniformly on Ω as i goes to ∞. Thus we
have proven that the DGPI Algorithm 3 converges uniformly,
to V ∗(x), to the optimal policy π∗(x). As claimed previously,
this also relaxes the requirement A2.

Remark 1. Since the state x is continuous, it is usually
intractable to check the H(x, ω, θ) value of every x ∈ Ω.
Therefore, in practical applications, we usually use the ex-
pected value of H(x, ω, θ) to judge whether each termination
condition in Algorithm 3 is satisﬁed. So, the DGPI Algorithm 3
can also be formulated as Algorithm 4. Fig. 1 shows the
frameworks of DPI Algorithm 2 and DGPI Algorithm 4.

Algorithm 4 DGPI algorithm: Tractable Relaxation

Initial with arbitrary θ0 and ω0
Choose the appropriate learning rates α, αω and αθ
Given an arbitrarily small positive (cid:15) and set i = 0
Phase 1: Warm-up

while La(ωi, θi) ≤ 0 do

Update ω and θ using (13)

end while

Phase 2: PI with relaxed termination conditions

while (cid:107)Vωi+1(x) − Vωi(x)(cid:107) ≤ (cid:15) do

Update w using (11)
Update θ using (12)

end while

Remark 2. In previous analysis, the l(x, u) is limited to a
positive deﬁnite function, i.e., the equilibrium state (denoted
by xe) of the system must be xe = 0. If we take x − xe as the
input of value network V (x; ω), the DGPI Algorithm 4 can
be extended to problems with non-zero xe, where l(x, u) = 0
only when x = xe ∧ u = 0. The corresponding convergence
and optimality analysis is similar to the problems of xe = 0.

Fig. 1: DPI and DGPI algorithm framework diagram.

networks, we propose another effective method that drives the
V (xe; ω) to gradually approach 0 by adding an equilibrium
term to the critic loss function (9)

Lc

(cid:48)(ω, θ) = Ex∈Ω

(cid:2)H(x, ω, θ)2(cid:3) + ηV (xe; ω),

where η is the hyper-parameter that trades off the importance
of the Hamiltonian term and equilibrium term.

IV. RESULTS

To support the proposed DGPI Algorithm 4, we offer two
simulation examples, one with linear, and the other one with a
nonlinear non input-afﬁne system. We apply Algorithm 4 and
Algorithm 2 to solve the optimal policy and value function
for these two systems. The simulation results show that our
algorithm performs better than Algorithm 2 in both cases.

A. Example 1: Linear Time Invariant System

1) Problem Description: Consider the CT aircraft plant
control problem used in [24], [25], [43], which can be for-
mulated as
(cid:90) ∞

(x(cid:62)Qx + u(cid:62)Ru)





0.90506 −0.00215
−1.01887
0.82225 −1.07741 −0.17555
0

−1

0



 x +



 u,


0
0

1

min
u

0

s.t.

˙x =

Remark 3. According to Lemma 3, all activation functions σV
and biases bV of V (x; ω) are set to σV (0) = 0 and bV ≡ 0 to
ensure V (xe; ω) ≡ 0. To remove these restrictions for value

where Q and R are identity matrices of appropriate di-
mensions. In this linear case, the optimal analytic strategy
π∗(x) = 0.1352x1 + 0.1501x2 − 0.4329x3 and optimal value

DGPI Warm-up(,𝜔,𝜃)2 Arbitrary 0 Arbitrary 𝜔0 ≤0 =+1  Policy Evaluation  (;𝜔) 𝜋(;𝜃) Policy Improvement =+1 𝜔+1=𝜔 𝜃+1=𝜃  ∆ ≤ε PI with relaxed termination conditionValue Networkstate state Policy NetworkPolicy Improvement DPI PI (;𝜔) 𝜋(;𝜃) =+1 Admissible 0 Arbitrary 𝜔0 ≤ε  ∆ ≤ε 𝜔+1=𝜔  ∆ ≤ε 𝜃+1=𝜃 Policy Evaluation function V ∗(x) = x(cid:62)P x can be easily found by solving the
algebraic Riccati equation, where




P =



1.4245
1.1682
−0.1352 −0.1501

1.1682 −0.1352
1.4349 −0.1501
0.4329

 .

2) Algorithm Details: This system is very special, in par-
ticular, if the parameters θ of the policy network is randomly
initialized around 0, which is a very common initialization
method, then the initialized policy πθ0 (x) ∈ Ψ(Ω). Therefore,
to compare the learning speed of Algorithm 2 and Algorithm 4,
both algorithms are implemented to ﬁnd the optimal policy
and value function. The value function and policy are repre-
sented by 3-layer fully-connected NNs, which have the same
architecture except for the output layers. For each network, the
input layer is composed of the states, followed by 2 hidden
layers using exponential
linear units (ELUs) as activation
functions with 28 units per layer. The outputs of the value
and policy network are V (s; ω) and π(s; θ), using softplus
unit and linear unit as activation functions respectively. The
training set consists of 28 states which are randomly selected
from the compact set Ω at each iteration. The learning rate αω
and αθ are both set to 0.01 and the Adam update rule is used
to minimize the loss functions.

3) Result Analysis: Each algorithm was run 20 times and
the mean and 95% conﬁdence interval of the training per-
formance are shown in Fig. 2. We plot the policy error eπ
and value error eV of Algorithm 2 and Algorithm 4 at each
iteration, which are solved by

eπ = Ex∈X





π(x; θ) − π∗(x)
π∗(x) − min
x∈X

max
x∈X

π∗(x)



 ,

7

Fig. 2: DGPI vs DPI performance comparison: Example 1.

B. Example 2: Nonlinear and Non Input-Afﬁne System

1) Problem Description: Consider the vehicle trajectory
tracking control problem with non input-afﬁne nonlinear ve-
hicle system derived as in [44], [45]. The desired velocity is
12 m/s and the desired vehicle trajectory is shown in Fig.
4. The system states and control inputs of this problem are
listed in Table I, and the vehicle parameters are listed in Table
II. The vehicle is controlled by a saturating actuator, where
δ ∈ [−0.35, 0.35] rad and ax ∈ [−3, 3] m/s2. The dynamics
of the vehicle along with detailed state and input descriptions
is given as









, u =

(cid:21)

(cid:20) δ
ax

, f (x, u) =









vy
r
vx
φ
y



Fyf cos δ+Fyr
m

− vxr

aFyf cos δ−bFyr


Iz

ax + vyr − Fyf sin δ


r

vx sin φ + vy cos φ

m










,

eV = Ex∈X





V (x; ω) − V ∗(x)
V ∗(x) − min
x∈X

max
x∈X

V ∗(x)



 ,

x =

where X is the test set which contains 500 states randomly
selected from the compact set Ω at the beginning of each
simulation. We also draw violin plots in different iterations to
show the precision distribution and 4-quartiles. Noted that one
iteration of Fig. 2 corresponds to one NN update.

It is clear from Fig. 2 that both two algorithms can make
the value and policy network approximation errors (eπ and
eV ) fall with iteration. And after 105 iterations, both er-
rors of Algorithm 4 are less than 1%. This indicates that
Algorithm 4 has the ability to converge value function and
policy to optimality. In addition, the t-test results in Fig. 2
show that both eπ and eV of Algorithm 4 are signiﬁcantly
smaller than that of Algorithm 2 (p < 0.001) under the same
number of iterations. From the perspective of convergence
speed, Algorithm 4 requires only about 104 iterations to make
both approximation errors less than 0.03, while Algorithm 2
requires about 105 steps. Based on this, Algorithm 4 is about
10 times faster than Algorithm 2. To summarize, Algorithm 4
can converge to the optimal value function and policy, and the
convergence speed of Algorithm 4 is signiﬁcantly higher than
that of Algorithm 2.

where Fyf and Fyr are the lateral tire forces of the front
and rear tires respectively. The lateral tire forces are usually
approximated according to the Fiala tire model:

Fy† = −sgn(α†) ∗ min

(cid:110)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C† tan α†

(cid:16) C 2

† (tan α†)2
27(µ†Fz†)2 −

C† |tan α†|
3µ†Fz†

+ 1

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

, |µ†Fz†|

(cid:111)
,

where α† is the tire slip angle, Fz† is the tire load, µ† is the
lateral friction coefﬁcient, and the subscript † ∈ {f,r} repre-
sents the front or rear tires. The slip angles can be calculated
from the geometric relationship between the front/rear axle
and the center of gravity (CG):

αf = arctan(

vy + ar
vx

) − δ, αr = arctan(

vy − br
vx

).

Assuming that the rolling resistance is negligible, the lateral
friction coefﬁcient of the front/rear wheel is:
(cid:112)(µFz†)2 − (Fx†)2
Fz†

µ† =

,

0246810 (×104)Iteration3.02.52.01.51.00.50.0Logarithmic error[p=3×10−27[p=10−31[p=2×10−27[p=7×10−27[p=9×10−27[p=3×10−19[p=5×10−15[p=4×10−12[p=9×10−11NetworkPolicy πValue VMethodDGPIDPIwhere Fxf and Fxr are the longitudinal tire forces of the front
and rear tires respectively, calculated as

8

Fxf =

(cid:40) 0,

max
2

ax ≥ 0

, ax < 0

, Fxr =

(cid:40) max,
max
2

ax ≥ 0

.

, ax < 0

The loads on the front and rear tires can be approximated by

Fzf =

b
a + b

mg, Fzr =

a
a + b

mg.

The control objective is to minimize the output tracking errors.
Hence, the optimal control problem is given by
(cid:20)280
0

0.4(vx − 12)2 + 80y2 + u(cid:62)

0
0.3

min
u

(cid:90) ∞

dt

(cid:16)

(cid:17)

u

(cid:21)

0

s.t.

˙x = f (x, u).

TABLE I
State and control input

state

input

Lateral velocity
Yaw rate at center of gravity (CG)
Longitudinal velocity
Yaw angle between vehicle & trajectory
Distance between CG & trajectory
Front wheel angle
Longitudinal acceleration

vy
r
vx
φ
y
δ
ay

[m/s]
[rad/s]
[m/s]
[rad]
[m]
[rad]
[m/s2]

TABLE II
Vehicle parameters

Front wheel cornering stiffness
Rear wheel cornering stiffness
Distance from CG to front axle
Distance from CG to rear axle
Mass
Polar moment of inertia at CG
Tire-road friction coefﬁcient

Cf
Cr
a
b
m
Iz
µ

88000 [N/rad]
94000 [N/rad]
1.14 [m]
1.40 [m]
1500 [kg]
2420 [kg·m2]
1.0

2) Algorithm Details: We use the 6-layer fully-connected
NNs to approximate V (s; ω) and π(s; θ), and the state input
layer of each NN is followed by 5 fully-connected hidden
layers, 25 units per layer. The selection of activation function
is similar to that of Example 1, except that the output layer
of the policy network is set as a tanh(·) layer with two
units, multiplied by the vector [0.35, 3] to confront bounded
controls. Inspired by the ideas used in multi-threaded variants
of Deep RL, the training set consists of the current states of
28 parallel independent vehicles with different initial states,
thereby obtaining a more realistic state distribution [46]. We
use Adam method to update two NNs, while the learning
to 8−4
rate of value network and policy network are set
and 2−4 respectively. Besides, we use η = 0.1 to trade off
the Hamiltonian term and equilibrium term of the critic loss
function (Remark 3).

3) Result Analysis: Fig. 3 shows the evolution of the
average absolute Hamiltonian |H| of 28 random states and the
training performance of 20 different runs. The shaded area rep-
resents the 95% conﬁdence interval. The policy performance
at each iteration is measured by the accumulated cost function
in 20s time domain

C =

(cid:90) 20

0

l(x(τ ), u(τ ))dτ,

Fig. 3: DGPI vs DPI performance comparison: Example 2.

where initial state x(0) is randomly selected for each run.
Since the initial policy is not admissible, that is, πθ0 (x) /∈
Ψ(Ω), Algorithm 2 can never make |H| close to 0, hence the
terminal condition of policy evaluation can never be satisﬁed.
Therefore, the ﬁnite horizon cost C has no change during the
entire learning process, i.e., Algorithm 2 can never converge
to an admissible policy if πθ0(x) /∈ Ψ(Ω).

On the other hand,

|H| of Algorithm 4 can gradually
converge to 0, while the ﬁnite horizon cast C is also reduced
to a small value during the learning process. Fig. 4 shows the
state trajectory controlled by one of the trained DGPI policies.
The learned policy can make the vehicle reach the equilibrium
state very quickly, which takes less than 0.5s for the case in
Fig. 4. The results of Example 2 show that Algorithm 4 can
solve the CT dynamic optimal control problem for general non
input-afﬁne nonlinear CT systems with saturated actuators and
handle inadmissible initial policies.

Fig. 4: State trajectory.

In conclusion, these two examples demonstrate that the
proposed DGPI algorithm can converge to the optimal policy
and value function for general nonlinear and non input-afﬁne

02468(×104)Iteration20406080100Average absolute HamiltonianValue|H|lgCMethodDGPIDPI01234567Finite horizon cost02468101214161820time (s)20020Statevx [m/s]r [° / s]vy [m/s]φ [°]y[dm]0100200300400Longitudinal position (m)-3.0-2.01.000.01.0Lateral position (m)Reference TrajectoryActual TrajectoryCT systems without reliance on initial admissible policy. In
addition, if the initial policy πθ0(x) ∈ Ψ(Ω), the learning
speed of Algorithm 4 is also faster than that of Algorithm 2.

V. CONCLUSION

The paper presented the Deep Generalized Policy Iteration
(DGPI) Algorithm 4, along with proof of convergence and
optimality, for solving optimal control problems of general
nonlinear CT systems with known dynamics. The proposed
algorithm can circumvent the requirements of “admissibility”
and input-afﬁne system dynamics (described in A1 and A2 of
Introduction), quintessential to previously proposed counter-
part algorithms. As a result, given any arbitrary initial policy,
the DGPI algorithm is shown to eventually converge to an
admissible and optimal policy, even for general nonlinear non
input-afﬁne system dynamics. The convergence and optimal-
ity were mathematically proven by using detailed Lyapunov
analysis. We further demonstrated the efﬁcacy and theoretical
accuracy of our algorithm via two numerical examples, which
yielded faster learning speed of the optimal policy starting
from an admissible initialization, as compared to conventional
Deep Policy Iteration (DPI) algorithm (Algorithm 2).

VI. ACKNOWLEDGMENT

We would like to acknowledge Prof. Francesco Borrelli, Ms.
Ziyu Lin, Dr. Yiwen Liao, Dr. Xiaojing Zhang and Ms. Jiatong
Xu for their valuable suggestions throughout this research.

REFERENCES

[1] D. P. Bertsekas, Dynamic Programming and Optimal Control. Athena

Scientiﬁc Belmont, MA, 2005.

[2] T. Pappas, A. Laub, and N. Sandell, “On the numerical solution of
the discrete-time algebraic riccati equation,” IEEE Transactions on
Automatic Control, vol. 25, no. 4, pp. 631–641, 1980.

[3] F. L. Lewis, D. Vrabie, and V. L. Syrmos, Optimal control. John Wiley

& Sons, 2012.

[4] F. Wang, H. Zhang, and D. Liu, “Adaptive dynamic programming:
An introduction,” IEEE Computational Intelligence Magazine, vol. 4,
pp. 39–47, May 2009.

[5] P. Werbos, “Beyond regression: New tools for prediction and analysis in
the behavioral sciences,” Ph. D. dissertation, Harvard University, 1974.
[6] P. Werbos, “Approximate dynamic programming for realtime control and
neural modelling,” Handbook of Intelligent Control: Neural, Fuzzy and
Adaptive Approaches, pp. 493–525, 1992.

[7] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

MIT press, 2018.

[8] D. Liu, Q. Wei, D. Wang, X. Yang, and H. Li, Adaptive Dynamic
Programming with Applications in Optimal Control. Springer, 2017.

[9] R. A. Howard, “Dynamic programming and markov processes,” 1964.
[10] K. Doya, “Reinforcement learning in continuous time and space,” Neural

Computation, vol. 12, no. 1, pp. 219–245, 2000.

[11] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforce-
ment learning,” in Proceedings of the 21st International Conference on
Machine Learning, (Banff, Alberta, Canada), pp. 1–, ACM, 2004.
[12] J. Peters and S. Schaal, “Natural actor-critic,” Neurocomputing, vol. 71,

no. 7-9, pp. 1180–1190, 2008.

[13] S. Levine and V. Koltun, “Guided policy search,” in Proceedings of the
30th International Conference on Machine Learning, vol. 28, (Atlanta,
Georgia, USA), pp. 1–9, PMLR, 17–19 Jun 2013.

[14] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
“Deterministic policy gradient algorithms,” in Proceedings of the 31st
International Conference on Machine Learning, vol. 32, (Bejing, China),
pp. 387–395, PMLR, 22–24 Jun 2014.

9

[15] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-
marking deep reinforcement learning for continuous control,” in Pro-
ceedings of the 33rd International Conference on International Confer-
ence on Machine Learning, vol. 48, (New York, NY, USA), pp. 1329–
1338, PMLR, 2016.

[16] B. Recht, “A tour of reinforcement learning: The view from continuous
control,” Annual Review of Control, Robotics, and Autonomous Systems,
vol. 2, pp. 253–279, 2019.

[17] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
region policy optimization,” in Proceedings of the 32nd International
Conference on Machine Learning, vol. 37, (Lille, France), pp. 1889–
1897, PMLR, 07–09 Jul 2015.

[18] W. B. Powell, Approximate Dynamic Programming: Solving the Curses

of Dimensionality. John Wiley & Sons, 2007.

[19] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, “Discrete-time nonlin-
ear hjb solution using approximate dynamic programming: convergence
proof,” IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), vol. 38, no. 4, pp. 943–949, 2008.

[20] D. P. Bertsekas and J. N. Tsitsiklis, “Neuro-dynamic programming: an
overview,” in Proceedings of the 34th IEEE Conference on Decision and
Control, vol. 1, (New Orleans, LA, USA), pp. 560–564, IEEE, 1995.

[21] R. Kamalapurkar, P. Walters, and W. E. Dixon, “Model-based rein-
forcement learning for approximate optimal regulation,” in Control of
Complex Systems, pp. 247–273, Elsevier, 2016.

[22] M. Abu-Khalaf and F. L. Lewis, “Nearly optimal control

laws for
nonlinear systems with saturating actuators using a neural network hjb
approach,” Automatica, vol. 41, no. 5, pp. 779–791, 2005.

[23] T. Dierks and S. Jagannathan, “Optimal control of afﬁne nonlinear
continuous-time systems,” in American Control Conference (ACC),
2010, (Baltimore, MD, USA), pp. 1568–1573, IEEE, 2010.

[24] K. G. Vamvoudakis and F. L. Lewis, “Online actor critic algorithm
to solve the continuous-time inﬁnite horizon optimal control problem,”
Automatica, vol. 46, no. 5, pp. 878–888, 2010.

[25] K. G. Vamvoudakis, “Event-triggered optimal adaptive control algorithm
for continuous-time nonlinear systems,” IEEE/CAA Journal of Automat-
ica Sinica, vol. 1, no. 3, pp. 282–293, 2014.

[26] L. Dong, X. Zhong, C. Sun, and H. He, “Event-triggered adaptive
dynamic programming for continuous-time systems with control con-
straints,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 28, no. 8, pp. 1941–1952, 2017.

[27] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, “Adaptive optimal
control of unknown constrained-input systems using policy iteration and
neural networks,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 24, no. 10, pp. 1513–1525, 2013.

[28] X. Yang, D. Liu, and Q. Wei, “Online approximate optimal control
for afﬁne non-linear systems with unknown internal dynamics using
adaptive dynamic programming,” IET Control Theory & Applications,
vol. 8, no. 16, pp. 1676–1688, 2014.

[29] D. Vrabie, K. Vamvoudakis, and F. Lewis, “Adaptive optimal controllers
based on generalized policy iteration in a continuous-time framework,”
in 17th Mediterranean Conference on Control and Automation, (Thes-
saloniki, Greece), pp. 1402–1409, IEEE, 2009.

[30] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. L. Lewis, “Adaptive
optimal control for continuous-time linear systems based on policy
iteration,” Automatica, vol. 45, no. 2, pp. 477–484, 2009.

[31] Y. Jiang and Z.-P. Jiang, “Global adaptive dynamic programming for
continuous-time nonlinear systems,” IEEE Transactions on Automatic
Control, vol. 60, no. 11, pp. 2917–2929, 2015.

[32] K. G. Vamvoudakis, F. L. Lewis, and G. R. Hudas, “Multi-agent
differential graphical games: Online adaptive learning solution for syn-
chronization with optimality ,” Automatica, vol. 48, no. 8, pp. 1598–
1611, 2012.

[33] J. Li, H. Modares, T. Chai, F. L. Lewis, and L. Xie, “Off-policy rein-
forcement learning for synchronization in multiagent graphical games,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 28,
no. 10, pp. 2434–2445, 2017.

[34] S. Lyashevskiy, “Constrained optimization and control of nonlinear
systems: new results in optimal control,” in Proceedings of 35th IEEE
Conference on Decision and Control, vol. 1, (Kobe, Japan), pp. 541–546,
IEEE, 1996.

[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through
deep reinforcement learning,” Nature, vol. 518, pp. 529–533, Feb. 2015.
[36] Y. LeCun, Y. Bengio, and G. E. Hinton, “Deep learning,” Nature,

vol. 521, pp. 436–444, 2015.

10

[37] S. Ruder, “An overview of gradient descent optimization algorithms,”

arXiv preprint arXiv:1609.04747, 2016.

[38] Z. Allen-Zhu, Y. Li, and Z. Song, “A convergence theory for deep learn-
ing via over-parameterization,” in Proceedings of the 36th International
Conference on Machine Learning, vol. 97, (Long Beach, California,
USA), pp. 242–252, PMLR, 09–15 Jun 2019.

[39] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai, “Gradient descent
ﬁnds global minima of deep neural networks,” in Proceedings of the
36th International Conference on Machine Learning (K. Chaudhuri
and R. Salakhutdinov, eds.), vol. 97, (Long Beach, California, USA),
pp. 1675–1685, PMLR, 09–15 Jun 2019.

[40] K. Hornik, M. Stinchcombe, and H. White, “Universal approximation of
an unknown mapping and its derivatives using multilayer feedforward
networks,” Neural Networks, vol. 3, no. 5, pp. 551–560, 1990.

[41] A. M. Lyapunov, “The general problem of the stability of motion,”

International Journal of Control, vol. 55, no. 3, pp. 531–534, 1993.

[42] R. G. Bartle and D. R. Sherbert, Introduction to Real Analysis. Hoboken,

NJ: Wiley, 2011.

[43] B. L. Stevens and F. L. Lewis, Aircraft control and simulation. New

Jersey: John Willey, 2003.

[44] J. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli, “Kinematic and
dynamic vehicle models for autonomous driving control design,” in
2015 IEEE Intelligent Vehicles Symposium (IV), (Seoul, South Korea),
pp. 1094–1099, IEEE, 2015.

[45] R. Li, Y. Li, S. Li, E. Burdet, and B. Cheng, “Driver-automation
indirect shared control of highly automated vehicles with intention-aware
authority transition,” in 2017 IEEE Intelligent Vehicles Symposium (IV),
(Los Angeles, CA, USA), pp. 26–32, IEEE, 2017.

[46] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lil-
licrap, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for
deep reinforcement learning,” in Proceedings of the 33rd International
Conference on International Conference on Machine Learning - Volume
48, (New York, NY, USA), pp. 1928–1937, PMLR, 2016.

