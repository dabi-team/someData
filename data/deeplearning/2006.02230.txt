0
2
0
2

v
o
N
7
1

]

C
D
.
s
c
[

2
v
0
3
2
2
0
.
6
0
0
2
:
v
i
X
r
a

PolyDL: Polyhedral Optimizations for Creation of High
Performance DL primitives

SANKET TAVARAGERI, ALEXANDER HEINECKE, SASIKANTH AVANCHA, BHARAT
KAUL, Intel Labs
GAGANDEEP GOYAL, RAMAKRISHNA UPADRASTA, IIT Hyderabad

Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming
ubiquitous including in software for image recognition, speech recognition, speech synthesis, language
translation, to name a few. The training of DNN architectures however is computationally expensive. Once
the model is created, its use in the intended application â€“ the inference task, is computationally heavy too
and the inference needs to be fast for real time use. For obtaining high performance today, the code of Deep
Learning (DL) primitives optimized for specific architectures by expert programmers exposed via libraries is
the norm. However, given the constant emergence of new DNN architectures, creating hand optimized code is
expensive, slow and is not scalable.

To address this performance-productivity challenge, in this paper we present compiler algorithms to
automatically generate high performance implementations of DL primitives that closely match the performance
of hand optimized libraries. We develop novel data reuse analysis algorithms using the polyhedral model to
derive efficient execution schedules automatically. In addition, because most DL primitives use some variant
of matrix multiplication at their core, we develop a flexible framework where it is possible to plug in library
implementations of the same in lieu of a subset of the loops. We show that such a hybrid compiler plus a
minimal library-use approach results in state-of-the-art performance. We develop compiler algorithms to also
perform operator fusions that reduce data movement through the memory hierarchy of the computer system.
Using Convolution Neural Network (CNN) models and matrix multiplication operations, we demonstrate
that our approach automatically creates high performing DNN building blocks whose performance matches
the performance of hand-crafted kernels of Intelâ€™s oneDNN library on high end CPUs. At the same time, our
techniques take only a fraction of time ( 1
20 or less) compared to AutoTVM, a deep learning auto-tuner to
create optimized implementations.

1 INTRODUCTION
Deep learning has revolutionized many spheres of human activity, examples of which include,
speech recognition [33], image recognition [31, 37], web search [1], language translation [53],
conversational artificial intelligence [21] etc. Training and inference using deep neural networks
(DNNs) that lie at the heart of Deep Learning (DL) are computationally intensive tasks. In todayâ€™s
datacenters, predominantly CPUs are used for inference tasks partly due to latency considerations.
According to a recent McKinsey study [11], CPUs account for 75% of the inference market. Software
frameworks such as TensorFlow, and PyTorch have been created to allow data scientists to write
high performance deep learning code in an efficient manner. However, all these frameworks use
manually optimized primitives to deliver high performance.

Given the ubiquity of CPUs and their widespread use for inference in deep learning applications,
in this work we focus on automatically creating high performance implementations of DL primitives
on CPU platforms. Creating a high performance implementation of a DL primitive requires that
the code is parallelized in a load balanced fashion to take advantage of the multiple cores. Within a
single core, the code should be cache friendly: this often means the loops of the code are tiled so
that effective data reuse out of different levels of cache (L1, L2, and L3) is possible. Tiling/blocking
program transformation [13] facilitates data reuse from caches and therefore masks the long
latency of fetching data from the main memory. Additionally, CPUs feature wide SIMD/vector units.

Authorsâ€™ addresses: Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul, Intel Labs, sanket.tavarageri@
intel.com; Gagandeep Goyal, Ramakrishna Upadrasta, IIT Hyderabad, ramakrishna@iith.ac.in.

, Vol. 1, No. 1, Article . Publication date: November 2020.

 
 
 
 
 
 
2Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

Therefore, the loops should be adequately vectorized. Oftentimes, the different transformations
mentioned are intertwined and that presents challenges for the compiler to produce fully optimized
code automatically. Existing automatic compilation, and auto-tuning techniques [7, 10, 13, 14, 16,
17, 19, 29, 35, 42, 44â€“46] are either 1) inadequate to generate code that matches the performance
of hand-tuned library implementations â€“ later on in the paper we show that the state-of-the-
art compiler generated code can lag library implementations by as much as ~10X or more, or 2)
expensive â€“ it would require running of 1000s of code versions to discover the best performing
version and yet, fall short of reaching the peak performance of the machine. The main reason for
the failure of automatic compilation techniques in achieving very high performance levels needed
is that the sophistication in the CPU microarchitecture has increased over successive generations
of CPUs (data prefetching, speculative execution, vector units, deep memory hierarchies, complex
cache replacement algorithms etc). Consequently, the cost functions used to optimize code are
unable to capture the nitty-gritties of the underlying architectures, and therefore are unable to
derive the most effective execution schedules. Auto-tuning is an alternative approach where one
explores a large number of program variants and selects the best performing version, sidestepping
the complexity of defining a cost function that adequately models the intricacies of the underlying
hardware architecture. However, auto-tuning is expensive and furthermore, it may fall short of
manually created library in performance as our study shows later on in the paper with respect to
AutoTVM [16], a deep learning auto-tuning system. We characterize the performance, productivity
trade-off qualitatively in Figure 1.

At the one end of the spectrum, expert coded
primitives such as that of Intel oneDNN library
attain high performance at the cost of produc-
tivity â€“ expert programmers have to hand craft
the logic of the primitives for the target archi-
tectures. Autotuning systems ease the burden
on programming to an extent but do not attain
highest levels of performance. Using function-
ally correct code with vendor supplied compilers
is most productive but comes at the expense of
performance. Our work â€“ PolyDL is close to at-
taining the highest levels of performance and at
the same time being most productive.

Fig. 1. The performance, productivity trade-off of dif-
ferent approaches

It has been shown that 95% of all deep learning applications running in the data centers today
have a recurring pattern in their inner most loops, namely blocked matrix multiplication [24, 34]. We
decompose the overall DL primitive optimization problem into two parts: 1) efficient parallelization
of the code and discovering a structure of the loops that uses the multi-level caches well, and 2)
effective vectorization of the code for a high degree utilization of the vector units. In this paper,
we develop a novel polyhedral model based data reuse algorithm to derive load balanced parallel
loops and cache friendly execution schedules. For the latter i.e., to use the vector units optimally,
we develop a flexible framework where the inner most loops of kernels can be replaced with
microkernels, i.e., manually optimized library implementations. As the number of recurring patterns
in the inner most loops of DL primitives is small, our hybrid approach is more scalable because

, Vol. 1, No. 1, Article . Publication date: November 2020.

ProductivityPerformanceIdeal systemHand crafted libraries e.g., oneDNNAuto-tuning systems e.g., AutoTVMNaive code, vendor compilersPolyDL (our work)PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

3

the number of microkernels that the expert programmers have to create is small as well. Thus, the
problem of DL library development will now be reduced to hand coding a few microkernels as
opposed to hand coding each of the large number of DL primitives.

To account for the sophisticated memory hierarchy, we use a code-generator to create a number
of program variants - ğ‘› in number - for a given program. The generated code variants are then
analyzed by our novel data reuse algorithm â€“ PolyDL to characterize their cache behavior. We
have developed a relative ranking algorithm which ranks the ğ‘› variants based on their potential
performance. The top ğ‘˜ variants are selected and are run on the target hardware and the best
performing program version is discovered. Thus, PolyDL narrows down the number of variants
to actually run on the target architecture from ğ‘› to a small, manageable ğ‘˜ (ğ‘˜ << ğ‘›). Through
our experimental evaluation on convolutions of a range of popular and the state-of-the-art image
recognition models, we show that the top variant (a single variant) picked by our compilation
machinery is one of the best performing variants, and the realized performance is close to and in
many cases, higher than that of Intelâ€™s oneDNN library [4] (formerly known as MKL-DNN), a hand-
tuned library for deep learning kernels. Additionally, we develop a fusion algorithm that â€œfusesâ€
element-wise operators with their preceding or succeeding compute-intensive operators. Such
patterns where a computationally heavy operator such as convolution is succeeded by element-wise
operators such as ReLU, occur frequently in DL workloads. Therefore, the DL domain specific
fusion algorithm that we develop will increase the overall performance by reducing the extra
memory traffic that the element-wise operators would otherwise incur.

The contributions of the paper are the following:

â€¢ We present a novel cache data reuse analysis to characterize a loop nestâ€™s behavior with

respect to a multi-level cache hierarchy.

â€¢ We describe a methodology to rank program variants in terms of performance using the
compiler generated statistics and the system parameters, i.e., cache sizes. To this purpose, we
develop two ranking techniques: one, a heuristic for ranking and two, a DNN based approach.

â€¢ We develop a deep learning domain specific operator fusion algorithm.
â€¢ We conduct extensive experiments comparing our technology with the Intel oneDNN library
and with AutoTVM. The experiments show that we are able to match the performance of
expert coded DL primitives in the oneDNN library and exceed the performance of the ones
discovered by AutoTVM via extensive auto-tuning.

To the best of our knowledge, this is the first work that examines automatic compilation tech-
niques and the use of microkernels in an integrated fashion. The rest of the paper is organized as
follows. We motivate the need for derivation of automatic execution schedules for loops in Section
2. Section 3 describes preliminary concepts that will be used in developing the compiler algorithms.
In Section 4, we develop algorithms for compile-time selection of top performing code version(s).
We present a cache data reuse analysis and a poly-ranking system to rank the candidate program
variants in terms of performance. The operator fusion algorithm is expounded in Section 5. Section
6 details the experimental evaluation conducted. The related work is discussed in Section 7 while
Section 8 presents the conclusions from this work.

2 MOTIVATION
The deep learning primitives are computationally intensive and most of the neural network training
and inferencing time is spent in them. However, for different layers of the deep neural networks,
the optimizations (e.g., loop order, tile sizes in tiled code etc) that need to be applied are different.
Using a version of the code optimized for one layer of a neural network for all others can yield poor
performance for the overall neural network. It is this need for custom optimization for different

, Vol. 1, No. 1, Article . Publication date: November 2020.

4Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

layers of neural networks (the number of layers in a deep neural network can be large) that makes
generating efficient code for deep learning primitives a challenging problem. To illustrate the need
for such tailored loop optimizations we consider the convolution layers of the Fast R-CNN model
[26], one of the leading image recognition CNN models. We generate four variants of convolution
code which differ only in the loop order and the rest of the loop structure remains the same for all
of them (more details are provided in Â§6) and measure performance on a 28-core Intel(R) Xeon(R)
Platinum 8280 (a.k.a Cascade Lake) CPU server. Figure 2 shows the normalized performance of the
code variants on 25 convolution layers of Fast R-CNN: the performance is normalized with respect
to the highest performing code among the four variants.

Fig. 2. Performance of four code variants

Fig. 3. Performance of the PolyDL picked variant

From Figure 2, we observe that the performances of different versions of the code vary widely
from layer to layer. A convolution layer differs from another convolution layer in problem sizes
â€“ image sizes, channel widths, filter sizes, strides, and padding. The code version v2 has the best
performance from layer 1 through 19, and is subpar from layer 20 through 25. The efficiencies of the
other versions viz., v1, v3, and v4 are much more widely varying. Using the compiler technology we
have developed â€“ PolyDL that we detail in the rest of the paper, we are able to effectively analyze the
four code variants and pick the best performing variant for each layer. The performance achieved
by PolyDL picked code shown in Figure 3 is close to the highest performance among the four
variants for all 25 layers of Fast R-CNN. Thus using the PolyDL system, using compile-time static
analysis alone, we are able to automatically identify and apply the loop optimizations required for
each layer of a deep neural network in order to achieve high performance.

3 PRELIMINARIES

3.1 Notation
We use the polyhedral model [23], which is an advanced mathematical framework to reason about
dependences and loop transformations, to develop our data reuse algorithm. We use the Integer Set
Library [50] for performing polyhedral operations in this work and we use the same notation as
used in ISL to elucidate the concepts and the algorithm. The matrix multiplication code shown in
Figure 4 will be used to illustrate the workings of the data reuse analysis.

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

5

f o r

( i = 0 ;

i < M;

f o r

( j = 0 ;

i ++ )

{
j ++ )
j < N ;
( k = 0 ; k < K ; k ++ )

{

f o r
C[ i ] [ j ] += A[ i ] [ k ]

{
âˆ— B [ k ] [ j ] ;

}

}

}

Sets. A set is a tuple of variables ğ‘¥ğ‘– s along with a
collection of constraints ğ‘ğ‘˜ s defined on the tuple
variables. ğ‘  = {[ğ‘¥1, . . . , ğ‘¥ğ‘›] : ğ‘1 âˆ§ . . . ğ‘ğ‘š }
The iteration spaces of loop nests are represented
by sets. The iteration space of the loop in Figure
4 is defined as the following set. ğ¼ = {ğ‘† [ğ‘–, ğ‘—, ğ‘˜] :
0 <= ğ‘– < ğ‘€ âˆ§ 0 <= ğ‘— < ğ‘ âˆ§ 0 <= ğ‘˜ < ğ¾ }

Fig. 4. Matrix multiplication code

Relations. A relation is a mapping from input tuple variables ğ‘¥ğ‘– s to output tuple variables ğ‘¦ ğ‘— s.
In addition, a set of constraints ğ‘ğ‘˜ s can be defined for a relation that will place constraints on the
input/output tuple variables. ğ‘Ÿ = {[ğ‘¥1, . . . , ğ‘¥ğ‘›] â†¦â†’ [ğ‘¦1, . . . , ğ‘¦ğ‘š] : ğ‘1, . . . , ğ‘ğ‘ }

The read and write access functions of a loop nest can be modeled with relations. The read
relations in the Figure 4 code are shown below: ğ‘Ÿ1 = {ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ¶ [ğ‘–, ğ‘—]}, ğ‘Ÿ2 = {ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’
ğ´[ğ‘–, ğ‘˜]}, ğ‘Ÿ3 = {ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğµ [ğ‘˜, ğ‘—]}. The sole write relation in the loop is: ğ‘¤1 = ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ¶ [ğ‘–, ğ‘—].
The domain of a relation ğ‘Ÿ is denoted by dom ğ‘Ÿ .

Apply operation. When a relation ğ‘Ÿ is applied on a set ğ‘ , the domain of ğ‘Ÿ will be intersected with ğ‘ 
and the resulting range will be a new set ğ‘  â€². The set ğ‘  â€² is said to be the result of the apply operation.
The operation is mathematically defined as: ( (cid:174)ğ‘¦ âˆˆ ğ‘  â€²) â‡â‡’ (âˆƒ(cid:174)ğ‘¥ s.t ( (cid:174)ğ‘¥ âˆˆ ğ‘  âˆ§ (cid:174)ğ‘¥ â†¦â†’ (cid:174)ğ‘¦) âˆˆ ğ‘Ÿ )

The data footprint of the loop can be computed by applying read and write relations on the

iteration space set: ğ‘Ÿ1(ğ¼ ) âˆª ğ‘Ÿ2(ğ¼ ) âˆª ğ‘Ÿ3 (ğ¼ ) âˆª ğ‘¤1(ğ¼ )

Lexicographic operations. The lexicographical operations can be applied on sets. ğ‘ 1 << ğ‘ 2 outputs
all the elements of ğ‘ 1 that are lexicographically strictly smaller than all the elements of ğ‘ 2, while
ğ‘ 1 <<= ğ‘ 2 gets us the elements of ğ‘ 1 that are lexicographically smaller than or equal to the elements
of ğ‘ 2. The lexicographically smallest element of a set ğ‘  is queried using lexmin ğ‘ . Similarly, the
lexicographically largest element is obtained using lexmax ğ‘ .

Set difference. The set difference between set ğ‘ 1 and ğ‘ 2 is denoted by ğ‘ 1 âˆ’ ğ‘ 2, i.e., the resulting set

will have elements of ğ‘ 1 that do not appear in ğ‘ 2.

3.2 Polyhedral dependences
The exact data dependences in loop nests can be computed in the polyhedral model and are
expressed as maps from source iterations to target iterations involved in the dependence. For cache
data reuse analysis developed in Â§4, we consider four kinds of dependences â€“ Read-After-Read
(RAR), Read-After-Write (RAW, a.k.a flow), Write-After-Read (WAR, a.k.a anti), and Write-After-
Write (WAW). The data dependencies of the matrix multiplication code in Figure 4 are shown
below.

ğ‘‘1 ={ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ‘† [ğ‘– â€², ğ‘— â€², ğ‘˜ â€²] : ğ‘– â€² = ğ‘– âˆ§ ğ‘— â€² = ğ‘— âˆ§ ğ‘˜ < ğ‘˜ â€² < ğ¾ }
ğ‘‘2 ={ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ‘† [ğ‘– â€², ğ‘— â€², ğ‘˜ â€²] : ğ‘– â€² = ğ‘– âˆ§ ğ‘˜ â€² = ğ‘˜ âˆ§ ğ‘— < ğ‘— â€² < ğ‘ }
ğ‘‘3 ={ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ‘† [ğ‘– â€², ğ‘— â€², ğ‘˜ â€²] : ğ‘— â€² = ğ‘— âˆ§ ğ‘˜ â€² = ğ‘˜ âˆ§ ğ‘– < ğ‘– â€² < ğ‘€ }

The dependence ğ‘‘2 is induced by array reference A[i][k]. An element of array A, say A[0][0]
which is accessed in source iteration [ğ‘– = 0, ğ‘— = 0, ğ‘˜ = 0] gets reused in target iterations [ğ‘– â€² = 0, ğ‘— â€² >

, Vol. 1, No. 1, Article . Publication date: November 2020.

6Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

Fig. 5. The PolyDL system

0, ğ‘˜ â€² = 0]. The source to target iteration relationships such as this are expressed in a parametric
fashion as the relation ğ‘‘2.

4 COMPILE TIME SELECTION OF TOP PERFORMING CODE VERSION
The input to our compiler tool is, the loop nest to be optimized â€“ L along with the microkernel that
forms the inner-most loops. Figure 5 shows the overall system design. The loop based specification
of the microkernel â€“ M is substituted in the code for further analysis. The resulting loop structure
â€“ L â€² is regular. The code generator takes the loop nest L â€² and generates a large number of program
variants while keeping the inner most loops that correspond to M intact. For each generated code
variant, the working set sizes are computed as described in Â§4.1. The statistics calculated for all
the variants are then input to the poly-ranking algorithm described in Â§4.2 and it picks the top ğ‘˜
best performing versions. The original microkernels are inserted back into the code of the ğ‘˜ picks.
The top performing variants selected analytically are now run on the target architecture and best
performing code among the ğ‘˜ loop nests is determined.

Microkernel Specification The microkernel function call is annotated with a pragma compiler
directive which contains the loop-based functionally equivalent code. The microkernel function call
is substituted with the loop based code for the compiler analysis in a pre-processing pass. When
the cache data reuse analysis and ranking of the code variants are done, in a post-processing pass,
the loop-based inner most loops are replaced with the call to the microkernel.

We assume that the data accessed by the microkernelâ€™s equivalent loop based code and by the
implementation of the microkernel are the same. The order of the loops within the microkernel
implementation could be potentially different. There is no assumption on how the microkernel is
implemented. The only assumption is that the data set accessed should be the same.

4.1 Working set size computation
We develop a polyhedral model based cache data reuse analysis to characterize a loop-nestâ€™s
behavior with respect to a given cache hierarchy. The analysis computes the various existing data
reuses of a program and then for the input cache hierarchy determines which data reuses are
exploitable at various levels of cache.

Each data dependence in a loop is also a case of data reuse â€“ the source and target iterations
involved in the dependence touch the same data element and therefore, the data is reused. For a
data dependence and hence data reuse to be realizable in a given level of cache, all the data elements
accessed between the source and target iterations of the dependence â€“ the working set â€“ have to be
retained in the cache so that when the execution reaches the target iteration, the data element(s)
used in the source iteration will still be present in the cache.

Algorithm 1 computes all the working sets of the input loop nest. First, the input C source file is
parsed using the Polyhedral Extraction Tool (PET) [51] to obtain the polyhedral representation of

, Vol. 1, No. 1, Article . Publication date: November 2020.

Loop nest + MicrokernelE.g., convolution + gemmLoop variant generatorPolyDL TOP CODE VERSION SELECTORTarget architectureHigh Performance primitiven versionstop k versionsSystem Configuration: Cache SizesPolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

7

Algorithm 1: Compute working set sizes
Input: Loop nest: L
Output: The working set sizes: ğ‘Š ğ‘†all

1 {Iteration space: I, Read relations: ğ‘Ÿread, Write relations: ğ‘Ÿwrite, Schedule: ğ›¿} â† Parse the loop nest L
2 {DRAR, DRAW, DWAR, DWAW} â† Compute read-after-read, read-after-write, write-after-read, write-after-write dependences of

L

3 Dall â† DRAR âˆª DRAW âˆª DWAR âˆª DWAW
4 ğ‘Š ğ‘†all â† âˆ…

/* Iterate through all dependences to compute the working set sizes

5 for ğ‘‘ âˆˆ Dall do
6

if ğ‘‘ spans parallel iterations then

else

/* The dependence spans iterations of the parallel loop(s)
ğ‘–ğ‘ â† The outermost parallel iterator in dom ğ‘‘
Ipar â† Parameterize iterators outer to ğ‘–ğ‘ in I
ğ‘Š ğ‘†par â† |ğ‘Ÿread ( Ipar) âˆª ğ‘Ÿwrite ( Ipar) |
Add ğ‘Š ğ‘†par to ğ‘Š ğ‘†all

/* The dependence spans iterations of the sequential loops
Isource â† lexmin dom ğ‘‘
Imin_tar â† lexmin ğ‘‘ ( Isource)
Imax_tar â† lexmax ğ‘‘ ( Isource)
Imin_WS â† ( I <<= Imin_tar) âˆ’ ( I << Isource)
Imax_WS â† ( I <<= Imax_tar) âˆ’ ( I << Isource)
ğ‘Š ğ‘†min â† |ğ‘Ÿread ( Imin_WS) âˆª ğ‘Ÿwrite ( Imin_WS) |
ğ‘Š ğ‘†max â† |ğ‘Ÿread ( Imax_WS) âˆª ğ‘Ÿwrite ( Imax_WS) |
Add ğ‘Š ğ‘†min and ğ‘Š ğ‘†max to ğ‘Š ğ‘†all

7

8

9

10

11

12

13

14

15

16

17

18

19

*/

*/

*/

the program, namely iteration space of the loop nest, read and write relations and the schedule
(line 1). The exact (and not transitive) RAR, RAW, WAR, WAW dependences are then computed and
a union of all the four kinds of dependences is formed (line 2 and 3). The task now is to compute
the working set size for each dependence which is carried out from line 6 through 19.

We distinguish between the data dependences that span parallel iterations and those that do not.
The working set sizes for the two kinds of dependences are computed differently. If a dependence
spans a parallel loop and the data set used in the entire set of parallel iterations is held in the
given level of cache, then the data reuse is guaranteed to happen out of that cache. This is because,
irrespective of the order of execution of the iterations of the parallel loop, the data accessed by
the source and target iterations will be present in the cache as the cache is big enough to hold the
entire set of data elements accessed by all parallel iterations collectively. The working set for such
a dependence is calculated by parameterizing the iterations outer to the parallel loop variable and
evaluating the size of the read and write sets within the parameterized iteration set (line 7 to 9).

For the data dependences that span sequential loops, we compute the working set size as follows.
We consider a representative source â€“ the first iteration (lexicographically) of all the source iterations
of a dependence (line 12). We can now compute the target iterations for the lexicographically
first/minimum iteration. If the data element that is used in the source iteration is used in multiple
subsequent iterations then there may be multiple target iterations for the same source iteration.
Therefore, the working sets to exploit the data reuse may vary. For this reason, we compute the
first (i.e., lexicographically minimum) and the last (i.e., lexicographically maximum) iterations of
the target iteration set (line 13 and 14). The intervening iterations between the source and the first
target iteration are determined (line 15). Similarly, iterations between the source and the last target
iteration are derived (line 16). The working sets will be the union of all the read and written data
elements between the source and the first/last iterations of the target iteration set (line 17 and 18).
Correspondingly, for each dependence we compute two working set sizes â€“ ğ‘Š ğ‘†ğ‘šğ‘–ğ‘› and ğ‘Š ğ‘†ğ‘šğ‘ğ‘¥ , if

, Vol. 1, No. 1, Article . Publication date: November 2020.

8Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

there are multiple target iterations for a source iteration in a given dependence. What this means
is, in order to be able to exploit at least one data reuse arising from the dependence ğ‘‘, the cache
memory should be capacious enough to hold at least ğ‘Š ğ‘†ğ‘šğ‘–ğ‘› data elements. If all the data reuses are
to be realized â€“ till the last target iteration, then the cache should of size equal to or greater than
ğ‘Š ğ‘†ğ‘šğ‘ğ‘¥ times the datatype size.

We illustrate the operation of the algorithm using the running example in Figure 4. Let us
examine the following dependence carried by the ğ‘— loop arising because of the array reference
ğ´[ğ‘–] [ğ‘˜]: ğ‘‘2 = {ğ‘† [ğ‘–, ğ‘—, ğ‘˜] â†¦â†’ ğ‘† [ğ‘– â€², ğ‘— â€², ğ‘˜ â€²] : ğ‘– â€² = ğ‘– âˆ§ ğ‘˜ â€² = ğ‘˜ âˆ§ ğ‘— < ğ‘— â€² < ğ‘ }. Of all the source iterations,
the first/lexicographically minimum iteration is: Iğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ = {ğ‘† [ğ‘– = 0, ğ‘— = 0, ğ‘˜ = 0]} Its target
iterations are: {ğ‘† [ğ‘– = 0, ğ‘—, ğ‘˜ = 0] : 0 < ğ‘— < ğ‘ }. Among the target iterations, the first one is:
ğ¼ğ‘šğ‘–ğ‘›_ğ‘¡ğ‘ğ‘Ÿ = {ğ‘† [ğ‘– = 0, ğ‘— = 1, ğ‘˜ = 0]} and the last one is: ğ¼ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘ğ‘Ÿ = {ğ‘†3 [ğ‘– = 0, ğ‘— = ğ‘ âˆ’ 1, ğ‘˜ = 0]}

The number of data elements of the three arrays â€“ A, B, C accessed between Iğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ and ğ¼ğ‘šğ‘–ğ‘›_ğ‘¡ğ‘ğ‘Ÿ

is derived by applying the read and write relations on the intervening iteration set and it is:

ğ‘Š ğ‘†ğ‘šğ‘–ğ‘› = 2ğ¾ + 3
The ğ¾ elements of array A â€“ ğ´[0] [0, 1, . . . , ğ¾ âˆ’ 1], the ğ¾ + 1 elements of array B â€“ ğµ [0, 1, . . . , ğ¾ âˆ’
1] [0] and ğµ [0] [1], and finally 2 elements of array C â€“ ğ¶ [0] [0], ğ¶ [0] [1] accessed between the source
iteration ğ‘† [ğ‘– = 0, ğ‘— = 0, ğ‘˜ = 0] and the target iteration ğ¼ğ‘šğ‘–ğ‘›_ğ‘¡ğ‘ğ‘Ÿ = ğ‘† [ğ‘– = 0, ğ‘— = 1, ğ‘˜ = 0] lead to the
ğ‘Š ğ‘†ğ‘šğ‘–ğ‘› size of 2ğ¾ + 3.

The maximum working set size â€“ the size of the data touched between Iğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ and ğ¼ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘ğ‘Ÿ is:

ğ‘Š ğ‘†ğ‘šğ‘ğ‘¥ = ğ‘ Ã— ğ¾ + ğ‘ + 1
The ğ‘Š ğ‘†ğ‘šğ‘ğ‘¥ size is arrived at by counting the number of array elements accessed between the source
iteration - ğ‘† [ğ‘– = 0, ğ‘— = 0, ğ‘˜ = 0] and the target iteration - ğ¼ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘ğ‘Ÿ = {ğ‘†3 [ğ‘– = 0, ğ‘— = ğ‘ âˆ’ 1, ğ‘˜ = 0]}. As
far as array A is concerned, ğ¾ elements of it â€“ ğ´[0] [0, 1, . . . , ğ¾ âˆ’ 1] are read. Array Bâ€™s elements â€“
ğµ [0, 1, . . . , ğ¾ âˆ’ 1] [0, 1, . . . , ğ‘ âˆ’ 2] plus ğµ [0] [ğ‘ âˆ’ 1] are read which total ğ¾ Ã— (ğ‘ âˆ’ 1) + 1. ğ‘ elements
of array C are read and written â€“ ğ¶ [0] [0, 1, . . . , ğ‘ âˆ’ 1]. Therefore, a total of ğ‘ Ã— ğ¾ + ğ‘ + 1 are read
and written.

4.2 Poly-ranking algorithm

Algorithm 2: Compute working set sizes w.r.t cache sizes
Input: The working set sizes: ğ‘Š ğ‘†ğ‘ğ‘™ğ‘™ ,
Cache sizes: ğ‘†ğ‘–ğ‘§ğ‘’ğ¿1 , . . . ğ‘†ğ‘–ğ‘§ğ‘’ğ¿ğ‘›
Output: Working set sizes per cache: ğ‘Š ğ‘†ğ¿ğ‘– for ğ‘– = 1, . . . , ğ‘›,
Memory working set size: ğ‘Š ğ‘†ğ‘šğ‘’ğ‘š
1 Initialize ğ‘Š ğ‘†ğ¿ğ‘– to 0 for ğ‘– = 1, . . . , ğ‘›,
2 Sort working set sizes in ğ‘Š ğ‘†ğ‘ğ‘™ğ‘™ from smallest to largest
3 for ğ‘Š ğ‘† ğ‘— âˆˆ ğ‘Š ğ‘†ğ‘ğ‘™ğ‘™ do
4

for ğ‘†ğ‘–ğ‘§ğ‘’ğ¿ğ‘– âˆˆ ğ‘†ğ‘–ğ‘§ğ‘’ğ¿1 , . . . ğ‘†ğ‘–ğ‘§ğ‘’ğ¿ğ‘› do

5

6

7

if (ğ‘Š ğ‘† ğ‘— + ğ‘Š ğ‘†ğ¿ğ‘– ) â‰¤ ğ‘†ğ‘–ğ‘§ğ‘’ğ¿ğ‘– then
ğ‘Š ğ‘†ğ¿ğ‘– = ğ‘Š ğ‘†ğ¿ğ‘– + ğ‘Š ğ‘† ğ‘—
break

8 Add the working sets ğ‘Š ğ‘† ğ‘— âˆˆ ğ‘Š ğ‘†ğ‘ğ‘™ğ‘™ that do not fit any cache to ğ‘Š ğ‘†ğ‘šğ‘’ğ‘š

We have built a code generator to emit a number of program variants. The code generator creates
the loop variants by applying tiling and loop interchange program transformations. The tile sizes
are varied as well. The working set size computation analysis â€“Â§4.1 is performed on each program
version generated. Among the many variants generated, the poly-ranking algorithm described
below picks the top ğ‘˜ best performing versions, where ğ‘˜ is a parameter.

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

9

We assume fully associative, and exclusive caches. If the working set size corresponding to a data
reuse in the program is smaller than the cache size then the data reuse is exploitable in the cache.
The poly-ranking system considers caches at different levels (typically L1, L2, and L3) and for each
data reuse, determines at what level of cache hierarchy is the data reuse realizable. Algorithm 2
shows the steps to determine the cumulative working set sizes at each level of cache. The inputs
to the algorithm are the working set sizes computed for a loop nest, and the cache sizes of the
target system. The algorithm determines the fastest level of cache where the working set size
corresponding to each data reuse fits and adds it to that cacheâ€™s working set size. The working
set sizes that fit in a particular level of cache ğ¿ğ‘– are denoted by ğ‘Š ğ‘†ğ¿ğ‘– . If a working set does not fit
in any cache, then the data reuse happens out of the main memory. Consequently, the memoryâ€™s
working set size is updated.

4.2.1 Performance cost model based ranking. The running time of the loop is directly related to the
latency of the cache where the data reuse occurs as well as the working set size. Furthermore, the
running time is inversely related to the bandwidth of the cache. Based on these observations, we
define the following cost function:

âˆ‘ï¸

ğ‘Š ğ‘†ğ¿ğ‘– Ã—

C =

ğ¿ğ‘–

latğ¿ğ‘–
bwğ¿ğ‘–

+ ğ‘Š ğ‘† mem Ã—

latmem
bwmem

(1)

The latency of cache ğ¿ğ‘– is latğ¿ğ‘– while its bandwidth is denoted by bwğ¿ğ‘– . For each code variant
generated, we run the cache data reuse analysis and calculate the above cost function. Then, the
variants are ranked in the decreasing order of the value of the cost function. The working set size at
cache level is multiplied with that cacheâ€™s latency and divided by its bandwidth. The multiplicands
are then added together. The lower the value of the cost function, the higher is its presumed
performance, and higher is its rank.

4.2.2 DNN-based ranking algorithm. We explore the use of deep neural networks (DNNs) for
ranking of code variants. For the purposes of training the DNN model, we collect the performance
data of code variants generated and the statistics as outputted by Algorithm 2 â€“ working set sizes
at different levels of the memory hierarchy.

We train the DNN model to perform relative ordering of two code variants. We then use a
tournament based ranking system to assign ranks to the different code versions created â€“ we play
each code variant against every other code variant. For each variant, we record the number of
wins it has accumulated. We then rank the variants based on the number of wins â€“ the higher the
number of wins, the higher the rank.

We use four intermediate layers of 64,
32, 16, 8 neurons respectively. We use
relu, relu, softsign, and relu activation
functions for the four intermediate lay-
ers.

Fig. 6. The DNN architecture for ranking of code variants

, Vol. 1, No. 1, Article . Publication date: November 2020.

10Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

ğ‘£2

ğ‘£1

+ ğ‘Š ğ‘†ğ¿1

ğ‘£2 + ğ‘Š ğ‘†ğ¿2

ğ‘£2 + ğ‘Š ğ‘†ğ¿3

ğ‘£1 + ğ‘Š ğ‘†ğ¿2

ğ‘£1 + ğ‘Š ğ‘†ğ‘šğ‘’ğ‘š

We use a four layer feed forward neural network architecture shown in Figure 6. We normalize
the compiler generated statistics of two code variants in the following fashion and input them
to the DNN. We sum the working set sizes of the two variants together: ğ‘ ğ‘¢ğ‘š = ğ‘Š ğ‘†ğ¿1
ğ‘£1 +
ğ‘Š ğ‘†ğ¿3
ğ‘£2 + ğ‘Š ğ‘†ğ‘šğ‘’ğ‘š
and divide the individual statistic by this sum.
The rationale for considering the sum of the two statistics together is that if one of the variants is
creating higher volume working set sizes then its statistics should appear bigger to the DNN. This
is because the smaller the working set sizes, we can expect higher performance. Therefore, for the
DNN to learn the relative performances of the two variants, it is crucial that it sees the relative sizes
of the working set sizes. Normalizing each variant individually (by considering the sum of statistics
of one variant alone) would not bring out the differences in the absolute values of the working set
sizes of the two variants at different cache levels. The output layer consists of two neurons and we
use the softmax function for the output layer. The values of the two output neurons, because of the
use of the softmax function, sum to 1. If the output value is above a threshold - ğœƒ , we consider it a
1, otherwise a 0. If the first neuron fires a 1, then the first variant is considered the winner. If the
second neuron fires a 1, then the second variant is considered the winner. If both of them are zero
because none of them are above the threshold, then it is a draw between the two variants. In this
work, we set the threshold ğœƒ to 0.6. We experimented with deeper models as well. However, the
depth beyond four layers did not have any discernible effect on accuracy.

5 OPERATOR FUSION
Often, in the DNN architectures, a heavy operator (where most of the compute cycles are spent)
is followed by activation functions which are element-wise operators. The output of the heavy
operator is processed by the activation functions such as ReLU, Sigmoid etc. in an element-wise
fashion, i.e., without involving any reduction. The element-wise operator is mainly a memory
bound operator. fused with the heavy operator in order that the extra data movement through the
memory hierarchy that would otherwise be necessitated by the element-wise operator is eliminated.

Algorithm 3: Perform operator fusion
Input: Loops of computationally heavy operator: ğ‘œğ‘â„ğ‘¦ ,
loops of element-wise operator ğ‘œğ‘ğ‘’ğ‘¤
Output: Fused operator: ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘

1 Wâ„ğ‘¦ â† the write set of ğ‘œğ‘â„ğ‘¦
2 Wğ‘’ğ‘¤ â† the write set of ğ‘œğ‘ğ‘’ğ‘¤
3 ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘ â† âˆ…
4 if Wâ„ğ‘¦ = Wğ‘’ğ‘¤ then
5

if |ğ¼ ğ‘œğ‘ğ‘’ğ‘¤ | = |Wğ‘’ğ‘¤ | then

if No writes or reads to any element of Wâ„ğ‘¦ between ğ‘œğ‘â„ğ‘¦ and

ğ‘œğ‘ğ‘’ğ‘¤ then

// We will now fuse the two ops
Iğ‘’ğ‘¤ â† instructions in the inner most loops of ğ‘œğ‘ğ‘’ğ‘¤
ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘ â† Insert Iğ‘’ğ‘¤ in the last iteration of ğ‘œğ‘â„ğ‘¦ â€™s

reduction loops

ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘ â† Apply index set splitting on ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘

11 if ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘ = âˆ… then
12

// We will return the original loop nests
ğ‘œğ‘ ğ‘“ ğ‘¢ğ‘ ğ‘’ğ‘‘ â† {ğ‘œğ‘â„ğ‘¦, ğ‘œğ‘ğ‘’ğ‘¤ }

6

7

8

9

10

13

Algorithm 3 presents a generic algo-
rithm that fuses a heavy operator with
the subsequent element-wise operator.

The following conditions have to be met for an element-wise operator ğ‘œğ‘ğ‘’ğ‘¤ to be fused with the
arithmetically intensive, heavy operator ğ‘œğ‘â„ğ‘¦:

â€¢ The two operators â€” ğ‘œğ‘ğ‘’ğ‘¤ and ğ‘œğ‘â„ğ‘¦ should be writing to the same set of elements (line 4 in

the algorithm)

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

11

â€¢ The element wise operator ğ‘œğ‘ğ‘’ğ‘¤ should be writing to each array element only once. We
check if the cardinality of the iteration space of ğ‘œğ‘ğ‘’ğ‘¤ is equal to the cardinality of the write
set of ğ‘œğ‘ğ‘’ğ‘¤ (line 5). This check will confirm that ğ‘œğ‘ğ‘’ğ‘¤ is indeed an element-wise operator
and does not involve a reduction.

â€¢ The operator ğ‘œğ‘â„ğ‘¦ should be immediately followed by ğ‘œğ‘ğ‘’ğ‘¤ without any intervening code.
Or, if there is any code between the two operators, it should not be writing to or reading
from the write set of the two operators (line 6).

Once the aforementioned conditions are met, the instructions of ğ‘œğ‘ğ‘’ğ‘¤ are inserted in the last
iteration of ğ‘œğ‘â„ğ‘¦ operatorâ€™s reduction loops subsequent to the instructions of ğ‘œğ‘â„ğ‘¦ (line 9). To
reduce the overheads stemming from the conditional checking if an iteration of the ğ‘œğ‘â„ğ‘¦ operatorâ€™s
loops is the last iteration of the reduction loops, we apply index set splitting transformation to peel
the last iteration from the rest of the iterations.

A symmetric analysis can be applied to fuse an element-wise operator with a subsequent heavy
operator as well. In that case the operation of the element-wise operator will be fused with the first
iteration of the heavy operatorâ€™s reduction loops.

6 EXPERIMENTAL EVALUATION
We conduct experiments to evaluate the efficacy of the PolyDL system in its ability 1) to derive
high performance primitive implementations, and 2) to create efficient fused operators. PolyDLâ€™s
goals are two-fold: one, to achieve performance competitive with manually optimized code, and
two, to do so with compile-time analyses alone without requiring auto-tuning which can be
expensive. Accordingly, we gauge the performance of PolyDL against a state-of-the-art library
created specifically for deep learning networks â€“ the latest version of Intel oneDNN [4] viz., v1.4.
We also compare PolyDLâ€™s performance with AutoTVM systemâ€™s. AutoTVM is an auto-tuning
system â€“ it generates a large number of program variants, runs them on the target architecture,
and observing the performance of different variants, identifies the best performing variant.

6.1 Set up
In this work, we evaluate the performance benefits of the PolyDL system on CPUs for inference
tasks. The forward pass convolution operation, batch-normalization, and ReLU and its variants
form the bulk of the compute of inference CNN models for image recognition. The GEMM operation
is at the heart of Fully Connected (FC) layers and multi-layer perceptron (MLP) implementations.
Therefore, we focus on them in our experimental evaluation.

The experiments are run on the latest Intel servers â€“ Intel(R) Xeon(R) Platinum 8280 (Cascade
Lake) CPU servers running at the frequency of 2.70 GHz. A single socket processor has 28 cores,
32KB private L1 cache, 1MB private L2 cache, and 39MB shared L3 cache. The programs are compiled
with Intel icc compiler 19.0.3.199 with the highest optimization flag -O3.

6.2 The GEMM microkernel
We use the LIBXSMM [3, 24, 32] implementation of GEMM (GEneral Matrix Multiplication) as
the microkernel. The data used by a microkernel fits in the registers or at most is L1 cache
resident. The microkernel performs the computation: ğ¶ = ğ›½.ğ¶ + ğ›¼ .ğ´.ğµ, where A, B, and C are
matrices and ğ›¼ and ğ›½ are scalars.

, Vol. 1, No. 1, Article . Publication date: November 2020.

12Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

Algorithm 4: The GEMM Microkernel
Input: ğ´ âˆˆ Rğ‘šÃ—ğ‘˜, ğµ âˆˆ Rğ‘˜Ã—ğ‘›, ğ¶ âˆˆ
Rğ‘šÃ—ğ‘›, ğ›¼, ğ›½ âˆˆ R

Output: ğ¶ = ğ›½.ğ¶ + ğ›¼ .ğ´.ğµ

1 acc_regs â† load C
2 for ğ‘–ğ‘˜ = 0 . . . k-1 with step 1 do
// Perform outer product
3
acc_regs += A columnğ‘–ğ‘˜ Ã— B rowğ‘–ğ‘˜

.

4
5 C â† acc_regs

Fig. 7. Outer product small GEMM microkernel.
Source: Georganas et al. [24]

Algorithm 4 shows how the GEMM microkernel is implemented. The C matrix is brought into
registers. The entire matrix multiplication is realized as a series of outer products: columns of A
are multiplied with rows of B. While the outer products are being computed, either columns of A
are reused or rows of B are reused depending on the matrix sizes. During the entire computation,
the C matrix is held in registers and the result of outer products are accumulated in the C matrix.
Figure 7 depicts how an example outer product is performed. In this illustrative example, ğ‘š = 64
and ğ‘› = 6. Let us consider that there are 32 vector registers in the underlying architecture and each
vector register holds up to 16 tensor elements. Vector registers 6 through 30 hold the output C
matrix. (ğ‘š Ã— ğ‘› = 64 Ã— 6 = 384 elements of the C matrix can fit in 24 vector registers, namely, 7 - 30.)
A row of B is broadcast into 6 vector registers. The first 16 elements of the first column of matrix
A are loaded into a register and multiplied with Bâ€™s vector registers 1 - 6 via fused-multiply-add
(FMA) operations and the accumulator registers 7 - 12 are updated. The rest of the 48 elements of
the first column of matrix A are brought into registers in a similar fashion and accumulators 13 - 30
are updated. At this point a column of A would have been completely multiplied with a row of B
and the results would have been accumulated for matrix C in registers 7 - 30. Later, all ğ‘˜ columns of
A are streamed in and multiplied with the same row of B. After the results are accumulated in this
fashion, the second row of B would be broadcast to vector registers 1 - 6 and matrix A is streamed
in again. In this set up, arrays B, and C are in registers throughout the whole computation while
matrix A is streamed in ğ‘˜ times. Finally, the C values in accumulators are stored back to memory.
We note that this is one of the strategies adopted in LIBXSMM. Depending on the ğ‘š and ğ‘› values,
and the architecture at hand (vector length, the number of vector registers) different schemes (such as
streaming in the B matrix and completely reusing the A matrix) could be used. Additionally, software
prefetches for matrices A and B are also issued to mitigate the cache miss latency overheads. For a
given set of problem sizes, the LIBXSMM framework JIT-compiles the microkernel (JIT compilation
- Just In Time compilation). It emits assembly instructions and provides a function pointer to the
JITed microkernel.

6.3 Evaluation of Compile Time Selection of Top Performing Code Version
We evaluate the efficacy of the developed techniques on two prominent DL operators, viz., convo-
lutions and GEMMs.

6.3.1 Convolutions. We use the PolyDL system to optimize the convolutions of Resnet-50 [31], Fast
R-CNN (fastrcnn) [26], Mask R-CNN (maskrcnn) [30], the popular and the state-of-the-art image
recognition neural network models. We also measure the performance of the same convolutions

, Vol. 1, No. 1, Article . Publication date: November 2020.

bcast B row in 6 vec registersC accumulators in 24 vector registersA columnPolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

13

using the implementations from the Intel oneDNN library and those obtained via auto-tuning with
the AutoTVM system. We pick the top 1 variant the code generator produces, i.e., ğ‘˜ = 1. That is, a
single version is selected.

f o r p r i v a t e ( o f m _ t i l e ,

i f m _ t i l e ,

i j , o j , k j , k i ,

i i )

#pragma omp p a r a l l e l
f o r

( img = 0 ;

{
( o f m _ t i l e = 0 ; o f m _ t i l e < nOfm / GEMM_BLOCK ; ++ o f m _ t i l e )

img < nImg ; ++img )

{

f o r

f o r

f o r

( i f m _ t i l e = 0 ;

i f m _ t i l e < nIfm / GEMM_BLOCK ; ++ i f m _ t i l e )

{

( o j = 0 ; o j < o f h ; ++ o j )

{

i j = o j

âˆ— STRIDE_H ;

f o r

f o r

( k j = 0 ; k j < kh ; ++ k j )

{

( k i = 0 ; k i < kw ; ++ k i )

{

/ âˆ— GEMM o p e r a t i o n b e g i n s

âˆ— /

f o r

( o i = 0 ; o i < ofw ; ++ o i )

{

i i = o i

âˆ— STRIDE_W ;

f o r

( ofm = 0 ; ofm < GEMM_BLOCK ; ++ofm )

{

f o r

( i f m = 0 ;

i f m < GEMM_BLOCK ; ++ i f m )
o u t p u t [ img ] [ o f m _ t i l e ] [ o j ] [ o i ] [ ofm ] +=

{

f i l t e r [ o f m _ t i l e ] [ i f m _ t i l e ] [ k j ] [ k i ] [ i f m ] [ ofm ]
âˆ—

i n p u t [ img ] [ i f m _ t i l e ] [ i j + k j ] [ i i + k i ] [ i f m ] ;

}

}

}

/ âˆ— GEMM o p e r a t i o n e n d s

âˆ— /

} } } } } }

Figure 8 shows the convolu-
tion code. The shown code is
data tiled in the input and out-
put channel dimensions. The
convolution code has a matrix
multiplication operation (de-
noted GEMM in the code) em-
bedded in it. We use the per-
formance obtained using the
code shown in 8 as the base-
line. The GEMM (matrix mul-
tiplication) operation in the
Figure will be replaced with
a call to the LIBXSMM imple-
mentation of matrix multipli-
cation.

Fig. 8. The 2-D Convolution code

PolyDL performs outer loop optimization around the call to the matrix multiplication microkernel
by loop reordering and tiling using various tile sizes. We show the performance obtained by inserting
the LIBXSMM microkernel in the code listed in Figure 8 under the banner of Microkernel in the
subsequent performance graphs. Comparing the performance of Microkernel with PolyDL will
show the need to perform outer loop tuning as done by PolyDL to obtain high performance for all
layers and for all models. Depending on the tensor sizes, we generate different number of code
variants for each layer. The number of variants generated varies from 5 to 21. This is because, the
number of tile sizes we can explore is a function of the tensor sizes. We generate a larger number of
variants for convolutions on larger tensors and fewer variants for convolutions on smaller tensors.
On average, for each layer, 11 versions are generated. Consequently, the task of the PolyDL system
is to rank the generated variants based on performance. Each program is run a 1000 times and the
average performance across those runs is reported in the paper.

The machine has a 512-bit SIMD vector unit and supports AVX-512 vector instructions. Conse-
quently, 16 floating point arithmetic operations can be performed at a time (each floating point
number is 32 bits long, and therefore, 16 floating point numbers make up 512 bits: 32 Ã— 16 = 512).
Since the microkernel vectorizes along the input and output channel loops (ğ‘– ğ‘“ ğ‘š and ğ‘œ ğ‘“ ğ‘š loops
in the code), to fully utilize the vector unit, the input and output channel widths have to be 16 or
multiples of 16. In the CNN models considered, 86% of the convolutions meet this criterion and
those convolutions are selected for experimental evaluation. The peak single precision floating
point performance of a 28-core Cascade Lake processor is ~3,300 GFLOPS/s. We set the mini-batch
size to 28 and use data parallelism: the convolution operator is applied on 28 images simultaneously.
To train a DNN model for performing ranking of code variants as described in Â§4.2.2, we use
70% of the experimental data collected (to avoid overfitting) â€“ that is, performance data of 70% of
the code versions generated are used to form the training data set. We create a single DNN model
using data from all CNN models and use it to rank variants across the CNN models.

, Vol. 1, No. 1, Article . Publication date: November 2020.

14Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

Figure 9 shows the perfor-
mance in terms of GFLOPS/s
(Giga Floating point Opera-
tions per second) of the base-
line code, PolyDL, AutoTVM
and oneDNN on convolutions
of fastrcnn. The PolyDL per-
formance shown is the perfor-
mance of the top code variant
selected using the cost model-
ing based poly-ranking algo-
rithm described in Â§4.2.1.

Fig. 9. Performance of fastrcnn layers

The performance of PolyDL vis-a-vis the baseline code is anywhere between 4X and 11X across
layers. The higher performance of PolyDL is due to 1) the optimization of outer loops 2) the use
of optimized GEMM microkernel for the inner loops. PolyDL performance is close to oneDNNâ€™s.
For some layers such as layer 11, PolyDL is 9% faster than oneDNN while for a few layers notably
layer 1, and 3, oneDNN performs better. AutoTVMâ€™s performance often lags that of PolyDLâ€™s. The
geometric average of GFLOPS/s numbers are also shown in the graph. They are 1965, 2322, and
2408 for AutoTVM, PolyDL, and oneDNN respectively.

Figure 10 shows the perfor-
mance distribution for all lay-
ers of fastrcnn. The perfor-
mance is normalized with re-
spect to that of the best per-
forming variant found empir-
ically.

Fig. 10. Performance distribution of code variants

The crux of the PolyDL technology presented in the paper is to rank a given set of code variants
using compile-time static analysis. Therefore, the closer the performance of the PolyDL picked
version is to the maximum performance seen by any code variant explored, the more efficacious the
PolyDL algorithms are. In the graph we show the minimum performance observed, the maximum
performance seen, the performance of the variant with default loop order shown in Figure 8 with
microkernel inserted â€“ Microkernel, the performance of the code picked per the poly-ranking
algorithm (Â§4.2.1) â€“ PolyDL and the performance of the code picked per the DNN based ranking
algorithm (Â§4.2.2) â€“ PolyDL-DNN. Here, we see that the performance distribution is great: the
difference between the performance of the best and the worst code variant seen is vast for all layers
except layers 1, and 18. We observe that PolyDL is able to pick a variant whose performance is
close to the performance of the best performing version. In the case of fastrcnn, we see that PolyDL
outperforms Microkernel significantly clearly showing the need for outer loop tuning in addition to

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

15

having a high performance implementation of matrix multiplication in the inner most loops. PolyDL
picked code achieves ~2X performance gains over the code with the default loop order for layers 4,
7, 8, 10, and 11 while for layer 25, PolyDL is 56% higher performing. Across all layers of fastrcnn,
PolyDL improves the performance over the default loop order by 28% on average.

The performances achieved
by different methods for the
convolutions of resnet are
shown in Figure 11. The per-
formance of PolyDL over the
baseline is 5X to 10X for all
layers. In most cases, PolyDL
closely matches the perfor-
mance of oneDNN library.

Fig. 11. Performance of Resnet-50 layers

In several instances, PolyDL outperforms oneDNN, notably for layers with IDs 7, 11, 15, and 16
where the performance gain is over 10%. On some layers such as layer 1, oneDNN fares bet-
ter. This is explained by customizations for specific problem sizes including insertion of careful
data prefetching instructions in the oneDNN library code. In contrast, PolyDLâ€™s approach is
automatic and in the case of Resnet-50, we observe that we are able to attain the same perfor-
mance levels as oneDNN overall. PolyDL is 14% higher performing than AutoTVM on average.

Figure 12 shows the perfor-
mance distribution of code
variants generated for each
layer of Resnet-50. We note
that the performance of the
PolyDL version is close to
the maximum performance in
most layers save layer 9.

Fig. 12. Performance distribution of code variants

Even though in terms of cache behavior (PolyDL primarily models the cache behavior), the variant
selected by PolyDL may be the best, other factors such as prefetching, TLB behavior etc may cause
its performance to be lower than those of other variants. The minimum performance seen i.e.,
the performance of the worst code variant, varies across layers â€“ for layer 12 through 19, the
minimum performance is much farther from the maximum performance. For the initial layers
however, the different code variants generated perform similarly. For Resnet-50, performance levels
of Microkernel and PolyDL are similar indicating that the original loop order shown in Figure 8
gets good performance. Even so, for layer 1, PolyDL is 7% higher performing than Microkernel. We
observe that there is not a considerable difference in the performance achieved by the cost model
based ranking method â€“ PolyDL, and the DNN based ranking method â€“ PolyDL-DNN.

, Vol. 1, No. 1, Article . Publication date: November 2020.

16Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

In Figure 13 and Figure 14, we show the performance achieved by various systems and the

performance distribution of code variants seen for the CNN model â€“ maskrcnn.

In Figure 13 we observe that
the performances of two lay-
ers of maskrcnn â€“ layer 31,
and 32 are very low compared
to the machine peak. The rea-
son is, the image sizes for the
two layers are 7X7 and 1X1
respectively.

Fig. 13. Performance of maskrcnn layers

Consequently, the amount of work that each core has to perform is less and therefore, all three
systems â€“ AutoTVM, PolyDL, and oneDNN are not able to attain performance close to the machine
peak.

For maskrcnn too, we dis-
cover that the default loop or-
der â€“ Microkernel, leaves a lot
of performance on the table:
for layers 4, 5, 6, 9, 10, 14, 15,
PolyDL gets more than 2X ex-
tra performance compared to
only the use of the microker-
nel. For layer 7, PolyDL is 3X
higher performing than Mi-
crokernel.

Fig. 14. Performance distribution of code variants

Across all layers of maskrcnn, on average PolyDL is 1.29X faster compared to Microkernel.

Across different models, the performance of PolyDL-DNN is consistently slightly better than that
of PolyDL. Further, PolyDL achieves magnitudes of higher performance compared to the baseline
code and is very competitive with respect to the hand crafted oneDNN library code. AutoTVM
generates and runs typically over 1000 code variants for each layer and takes âˆ¼15 - 20 minutes
to discover the best performing variant for a single layer. The PolyDL method zeroes in on the
best version in under one minute. We note that AutoTVMâ€™s methodology is not fully automatic.
For example, we have obtained the performance results using CPU X86 specific implementations
of Convolution code within AutoTVM [52], [20]. AutoTVM developers have created CPU specific
and additionally architecture specific (i.e., Cascade Lake) code within the framework. Although
AutoTVM discovers good tile sizes through auto-tuning, a lot of customized code has also been
written to obtain high performance on Cascade Lake CPUs. Therefore, for the purposes of this
experimental evaluation, AutoTVM represents a combination of library development and auto-
tuning approach.

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

17

Comparison with prior compiler works: In this paper, we presented an approach to char-
acterizing the working set sizes of the loops and relating them to the cache sizes of a computer
system. Such an analysis formed the basis for ranking different code variants and selecting the
best performing program version in our work. Alternately, one could compute the number of
cache misses for a given program variant at different levels of the cache hierarchy and using the
number of cache misses approximate its execution time. Subsequently, the code variant with the
smallest estimated execution time can be considered to be the code variant that achieves the highest
performance. Prior works have developed analytical cache miss computation methods [8, 25, 28].
However, none of them can analyze parallel programs. Our PolyDL compiler algorithms presented
in this paper can analyze parallel code in addition to sequential code. Even so, we compare PolyDL
with the latest analytical cache miss calculation work, viz., Gysi et al.â€™s cache modeling work [28]
in the following manner. In our convolution related experiments, the image â€“ img loop is parallel
(Figure 8). For the sake of this experimental evaluation, we convert the problem to a sequential
one by assuming that the loop length of the img loop is 1 and each core of the processor gets
an equal share of the shared L3 cache. This is a reasonable assumption because in our experi-
ments, each core processes an image each. The execution time of a program is estimated to be:
ğ¿1 misses Ã— latğ¿2 + ğ¿2 misses Ã— latğ¿3 + ğ¿3 misses Ã— latmem.

The running time is equal to the sum of latencies at different cache levels. One of the summands,
for example is, the number of misses at L1 cache is multiplied by the latency of the L2 cache (because
L1 misses are serviced from L2 cache). For this comparison study, we invoke sequential analysis in
PolyDL too with identical assumptions (img loop length being set to 1, and each processor getting
an equal share of the L3 cache).

Figure 15 shows the speed-ups obtained by
PolyDL and PolyDL-DNN with respect to Gysi
et al.â€™s cache modeling work [28]. Their tool is
termed HayStack and the same name is used in
Figure 15. For most of the total 201 convolution
layers (corresponding to the layers of fastrcnn,
resnet, maskrcnn, xception, yolov2, mobilenet,
alexnet, overfeat, googlenetv1, googlenetv3), the
performances of code versions picked by PolyDL,
and HayStack are identical. In some instances
HayStack is better and in others PolyDL/PolyDL-
DNN are higher performing.

Fig. 15. Comparison of performance achieved by the
code version selected by PolyDL techniques with that
of HayStack picked version
On average, PolyDL-DNN achieves marginally better performance â€“ it has a 1.002X speed-up
over HayStack. Between PolyDL and HayStack, HayStack has a slight edge: PolyDL is at 0.99X
performance levels of HayStack. We attribute the observed performance levels of the different
methods to the nature of statistics we obtain using the PolyDL algorithms (working set sizes) and
those that we obtain using the HayStack algorithm (the number of cache misses). It is straight-
forward to relate the number of cache misses to the running time of the program using the cache
latencies as done above. However, relating working set sizes to the running time of the program
could be more complicated and the use of DNN techniques (i.e., PolyDL-DNN) is therefore superior
to approximating the running time using cache latencies directly (i.e., PolyDL).

We observed that the running time of the HayStack tool is highly variable. In our experiments,
HayStack took anywhere from a couple of seconds to 37 minutes to process a single code variant.

, Vol. 1, No. 1, Article . Publication date: November 2020.

18Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

Table 1. GEMM sizes

Workload
GNMT (Machine translation)
GNMT (Machine translation)
GNMT (Machine translation)
DeepBench (General workload)

M
128
320
2048
1024

N
2048
3072
4096
16

K Workload
Synthetic
Synthetic
Synthetic
Synthetic

4096
4096
32
500000

M
4096
1024
1024
32768

N
4096
1024
32768
1024

K
4096
32768
1024
1024

On average, it takes âˆ¼ 35 seconds. PolyDLâ€™s running time was more uniform â€“ it takes 2 - 3 seconds
for any variant. Thus, PolyDLâ€™s analysis is orders of magnitude faster than HayStackâ€™s.

6.3.2 GEMMs. The GEMM operation is at the heart of deep learning [5, 40]. The fully connected
(FC) layers and multi-layer perceptrons map to GEMM operations: During the forward pass (for
inference and for training too), the inputs and the weights are the two matrices that are multiplied.
During the backward pass (for training), the error gradient with respect to the inputs and the
weight matrices are multiplied.

We evaluate PolyDLâ€™s efficacy on several matrix sizes. The GEMM operation is: ğ¶ = ğ›½.ğ¶ + ğ›¼ .ğ´.ğµ
where C matrixâ€™s dimensions are ğ‘€ Ã— ğ‘ while A and B matrices are of size ğ‘€ Ã— ğ¾ and ğ¾ Ã— ğ‘
respectively. Table 1 lists the GEMM sizes evaluated. We use several problem sizes from DL
workloads. We use four synthetic ones also to measure the performance obtained when all M, N,
and K sizes are similar and when one of them is much higher than others (tall-skinny matrices,
short-stout ones). We measure the performance of GEMMs when utilizing 32 processor cores.

Figure 16 shows the baseline GEMM code. We report the performance obtained by this code
when compiled with the icc compiler and â€œ-O3â€ flag in Figure 18 â€“ designated as Baseline in the
Figure. PolyDL framework generates several code variants for GEMM and ranks them. In Figure 17,
we are showing one of the code variants generated. It is a two level tiled code. Tile sizes â€“ M2_Tile,
N2_Tile, K2_Tile, M1_Tile, N1_Tile, and K1_Tile are tunable parameters: PolyDL will vary these
values and create distinct code variants. In the inner-most loops the GEMM microkernel is invoked
for performing GEMM operation on matrix sizes M1_Tile, N1_Tile, and K1_Tile. Additionally,
either loop it2 or jt2 could be made parallel. The PolyDL ranking algorithm will chose which
loop is to be made parallel. The number of variants generated varies between 4 (for GEMM sizes
M=1024,N=16,K=500000) and 1228 (for GEMM sizes M=4096,N=4096,K=4096). On average 491 code
variants are explored for each set of GEMM sizes. We select the top 5% variants after PolyDL ranks
them, and report the maximum performance obtained among those top 5% variants.

// First level of tiling
// Potential parallel loop1: it2

#pragma omp p a r a l l e l
f o r

( i = 0 ;

i < M;

i ++ )

f o r p r i v a t e ( j , k )

f o r

( j = 0 ;

j < N ;
( k = 0 ; k < K ; k ++ )
f o r
C[ i ] [ j ] = b e t a âˆ— C[ i ] [ j ] +

j ++ )

a l p h a âˆ— A[ i ] [ k ]

âˆ— B [ k ] [ j ] ;

Fig. 16. Baseline GEMM code

, Vol. 1, No. 1, Article . Publication date: November 2020.

( i t 2 = 0 ;

f o r
// Potential parallel loop2: jt2
( j t 2 = 0 ;

i t 2 < M;

j t 2 < N ;

f o r

i t 2 += M 2 _ T i l e )

{

j t 2 += N 2 _ T i l e )

{

( k t 2 = 0 ; k t 2 < K ; k t 2 += K 2 _ T i l e )

f o r
// Second level of tiling

{

f o r ( i t 1 = i t 2 ; i t 1 < i t 2 + M 2 _ T i l e ; i t 1 += M 1 _ T i l e ) {
f o r ( j t 1 = j t 2 ; j t 1 < j t 2 + N 2 _ T i l e ; j t 1 += N 1 _ T i l e ) {
f o r ( k t 1 = k t 2 ; k t 1 < k t 2 + K 2 _ T i l e ; k t 1 += K 1 _ T i l e ) {
//Call to GEMM microkernel of size:
//M1_Tile,N1_Tile,K1_Tile
m i c r o k e r n e l ( . . )
} } } } } }

Fig. 17. A code version generated for PolyDL

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

19

We compare the performance obtained by PolyDL with three other systemsâ€™: 1) Pluto [13], a
polyhedral source-to-source compiler, 2) AutoTVM, and 3) oneDNN. We input the code shown
in Figure 16 to the Pluto tool and obtain tiled and parallelized code. We auto-tune for tile sizes:
Pluto expects tile sizes to be inputted and we input several tile sizes. We report the maximum
performance obtained by any of the tile size combinations in Pluto. To obtain the best performance
possible from AutoTVM, we follow the GEMM optimization recipe provided on the TVM web pages
[2] and additionally explore many tile sizes and report the maximum performance obtained by any
set of tile sizes.

Figure 18 depicts the per-
formance obtained for var-
ious matrix sizes by differ-
ent systems. In the graph,
on the X-axis, the GEMM
sizes (M, N, and K values as
M_N_K) are shown. We ob-
serve that PolyDL-DNN and
oneDNN performances are
much higher than the other
two, viz., Pluto and AutoTVM.

Fig. 18. Performance of GEMMs for different problem sizes

In 5 out of 8 GEMM sizes, PolyDL-DNN obtains substantially higher performance than oneDNN:
For M=320 N=3072 K=4096 sizes, PolyDL-DNN is 2.6X higher performing than oneDNN. On aver-
age, PolyDL-DNN implementations are 1.24X faster compared to oneDNNâ€™s (geometric average).
PolyDL-DNN GEMMâ€™s are 13.4X higher performing than baseline implementations. Pluto per-
formance hovers around the same levels as that of the baseline code. On average, PolyDL-DNN
achieves 11.0X speed-up over Pluto. We note that AutoTVMâ€™s performance is orders of magnitude
higher than baselineâ€™s. This is partly explained by the fact that explicit vectorization encoded in
the execution schedule (the most beneficial vectorization loop was already set manually) which
resulted in good vectorization. PolyDL-DNNâ€™s GEMMs were 2.7X faster compared to AutoTVMâ€™s on
average. The average (geo-mean) performance obtained in terms of GFLOPS/s across eight GEMM
sizes by different systems, namely, Baseline, Pluto, AutoTVM, PolyDL-DNN, and oneDNN are 162,
197, 802, 2171, and 1745 respectively. We note that PolyDL achieves the highest performance among
the cohort evaluated in this set of experiments.

In Figure 19, we show the performance spread among the different code variants that PolyDL
system considered. The performances of all the code variants are normalized with respect to
the maximum performance recorded for any variant. MIN denotes the lowest performance of
any variant witnessed. Performances achieved by PolyDL-DNN (ranking of variants using the
DNN model described in Â§4.2.2) and PolyDL (ranking of variants using the heuristics based on
cache latencies as presented in Â§4.2.1) are shown. The closer the PolyDLâ€™s and PolyDL-DNNâ€™s
performances are to 1.0 (the MAX line), the better it is â€“ it shows that they are able to rank the
variants correctly. We observe that performances of PolyDL-DNN picked code variants are closer
to the maximum performance â€“ within 0.95X of maximum performance. This shows that through
compile-time modeling of data reuses in the loops as described in the paper, our techniques are
able to identify high performance configurations for the loops.

, Vol. 1, No. 1, Article . Publication date: November 2020.

20Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

We also show the performance obtained
when the GEMM microkernels are used
without any outer loop tuning in the
graph â€“ denoted as Microkernel. To mea-
sure the performance by using micro-
kernels alone, we tile the baseline code
shown in Figure 16 by factors of 16 in
the three loop dimensions, and invoke
the GEMM microkernel to perform ma-
trix multiplications on matrices of sizes
16X16.

Fig. 19. Performance distribution of code variants

The microkernel-performance alone is quite low. Nevertheless, the use of microkernels alone
speeds up baseline GEMM implementations by 2.2X. PolyDL-DNN achieves 6.0X speed-up over
Microkernel implementations. This underscores the importance of performing outer loop optimiza-
tions to obtain high performance.

Between PolyDL and PolyDL-DNN, PolyDL-DNN picked code variants have higher performance,
on average by a factor of 1.09X. For cost model based ranking of code variants, we use memory
hierarchyâ€™s latency and bandwidth values. While it fetches good rankings, DNN model is able
to learn the latency and bandwidth values more accurately and can learn underlying hardware
characteristics such as data prefetching automatically. Therefore, PolyDL-DNNâ€™s rankings tend to
be slightly better.

6.4 Evaluation of Operator Fusion
We evaluate the benefits of operator fusion algorithm presented in the paper ( Â§5) on two sequences:
1) batch normalization followed by the activation function ReLU, 2) convolution followed by the
activation function ReLU6, which is a variant of ReLU. The activation function ReLU is defined
as: ğ‘¦ = max(ğ‘¥, 0), while ReLU6 is defined as ğ‘¦ = min(max(ğ‘¥, 0), 6) [36], that is, the output value is
capped at 6. The batch normalization (bnorm for short) and ReLU sequence occurs frequently in
CNN models including that of Resnet-50. We compare the performances of 1) bnorm, ReLU unfused
code which forms the baseline 2) fused bnorm + ReLU operator using our fusion algorithm 3) fused
bnorm + ReLU operator in the oneDNN library. There is no facility in AutoTVM to perform operator
fusion automatically and therefore, we do not compare its performance on the sequence. Figure
20 shows the speed-ups obtained by our fused operator vis-a-vis the baseline code and oneDNNâ€™s
corresponding fused operator for the tensor sizes of all layers of CNN models we have considered
in our experiments. The speed-ups achieved by the fused operator are 1.59X and 1.20X (geometric
average across tensor sizes) compared to the unfused baseline code and the oneDNNâ€™s operator
respectively. Batch normalization is a memory bandwidth bound operation and therefore, fusing the
subsequent ReLU operation reduces round trips to the main memory which substantially improves
the performance. We perform fusion experiments with the convolution and ReLU6 sequence too. We
compare the performances of unfused and fused versions of the sequence. We note that ReLU6 is not
supported in the oneDNN library. Researchers have found that ReLU6 improves the performance
of image recognition models [36]. However, ReLU6 is not widely adopted and it is not supported
in the oneDNN library presumably because the high investment in supporting it is not justified.
This underscores that 1) hand coding of a plethora of DNN primitives researchers experiment
with is not scalable, 2) when an operator is not available in a library like oneDNN, it hampers the

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

21

Fig. 20. Speed up achieved by bnorm and ReLU fused
operator over that of unfused baseline code and
oneDNN implementation (higher the better).

Fig. 21. Speed up achieved by conv and ReLU6 fused
operator over that of unfused baseline code (higher
the better).

data scientistsâ€™ ability to quickly experiment and refine the DNN models. Therefore, automatic
compilation techniques like the one developed in this paper are needed to address these bottlenecks.
Figure 21 shows the speed-ups of the fused operator when compared to the corresponding
unfused operator. The ReLU6 activation function is applied on the output of the convolution. When
the size of the output tensor is large, we observe a higher speed-up. When the output size is smaller,
fusion has a marginal benefit. On average, the fused operator is 1.10X faster (geometric average).
Furthermore, convolution is a compute-bound operation and therefore, the time spent in it is large
relative to the time spent in the ReLU6 operation. This explains the contrasting speed-ups seen in
the two sequences we have evaluated. For the bnorm + ReLU sequence, the speed-ups are larger
because one, bnorm is a memory bound operation and two, the time spent in it is less compared to
convolution. Consequently, the time spent in ReLU as a percentage of the time spent in bnorm is
larger. Therefore, fusing the two operators shows larger performance gains for the bnorm + ReLU
sequence.

7 RELATED WORK
We discuss related works from polyhedral compilation, auto-tuning systems, deep learning DSL
(Domain Specific Language) frameworks, and GEMM microkernel research.

Polyhedral compilation techniques have been developed for source-to-source code transfor-
mation for better cache locality and parallelization. The Pluto compiler [13] derives an execution
schedule for the code that attempts to minimize data reuse distances. The effect of the Pluto
transformation will be that the iterations that use the same data will be executed close to each
other in time and therefore, it will be cache friendly. The Pluto algorithm can accomplish fusion
of loops too. However, Plutoâ€™s performance can be far from what we can achieve with the use of
microkernels that exploit the vector hardware effectively and by doing outer loop tuning in the way
we have developed this work: In Section 6.3.2, we show that our techniques can produce on average
11.0X higher performing GEMM implementations. Furthermore, the Pluto algorithm experiences
scalability issues â€“ as the number of loops in a loop nest increases, the Pluto algorithm/tool can take
exceedingly long time to derive a schedule. When we inputted the convolution followed by ReLU
code sequence, the tool took several hours and did not produce an output. Kong et al. [35] develop
a framework to decompose a program into sub-parts and use customized criteria (as opposed to
using a single objective function) â€“ such as stride optimization, outer loop optimization, inner loop
optimization to transform code. They show that their work without tiling transformation is able to
achieve comparable results to that of Pluto.

, Vol. 1, No. 1, Article . Publication date: November 2020.

22Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

The existing polyhedral compilation techniques fail to achieve performance competitive with
hand tuned libraries. The main reason is, the loop transformations polyhedral techniques encode
operate at a high level (source-to-source) and therefore, are unable to perform low-level orches-
tration of vector register allocation, detailed instruction scheduling (e.g., like that of the GEMM
microkernel implementation described in Â§6.2). The latter aspects are crucial to achieving high
performance on CPUs. An effective approach we believe is to use the polyhedral model based loop
scheduling for outer loops for efficient use of cache hierarchy and to use microkernels for effective
vectorization in the inner loops such as the one we have proposed in this paper.

Bondhugula et al. [12] propose a fusion model that considers loss of parallelism with aggressive
fusion. The number of available hardware prefetch streams is also used as a constraint to determine
the beneficial fusion structures. In this paper, we have proposed a domain specific fusion algorithm
that fuses heavy operators with element-wise operators. We have simplified the criteria for fusion
while preserving parallelism. The patterns that our algorithm tackles occur frequently in DL
workloads and our algorithm presents a first generic approach towards automatic fusions of
operators in deep learning. Cornwall et al. [18] discuss using software engineering concepts like
component based programming in the context of the development of a visual effects library. They
use algorithmic skeletons to extract the iteration space for loops, and high level metadata for
dependence analysis such that various optimizations can be applied. They propose using shifting
as an enabler for fusion. Along with support for array-contraction which enables vectorization, the
presented techniques result in good performance.

Compile-time modeling of cache behavior and in particular calculating the number of cache
misses has been an active area of research [8, 25, 28]. Researchers have demonstrated good accuracy
in predicting the number of cache misses on simulators. The modern computer architectures employ
a hash based scheme to map memory addresses to cache sets [54] which breaks the assumptions
behind the cache miss analyses. In the present work, we model the behavior of caches as well.
However, we do not model cache misses rather we consider data reuses and determine the size
of cache needed to exploit the data reuses under conservative conditions. We ignore streaming
accesses as their misses in cache will not be crucial in the resulting performance. Our analysis
works on parallel code, whereas prior works are not equipped to handle parallel loops. Because
of the these improvements, we show that we are able to accurately rank code variants in terms
of performance. Our experimental evaluation comparing against the latest cache miss analysis
work [28] using sequential loop setting (Â§6.3.1) shows that the working set size approach we have
adopted in our present work is as good as using cache misses for modeling of performance or is
slightly better.

DSLs and Autotuning systems: Iterative compilation [38] and/or combined model-driven and
iterative compilation techniques have been explored for program optimization [39]. TVM [15], a
compiler for deep learning, introduces the concept of tensorization, where a unit of computation
can be replaced with a microkernel written using hardware intrinsics. AutoTVM [16] which is
based on TVM, is targeted at accelerating deep learning workloads and uses machine learning
to guide auto-tuning of deep learning primitives. We compared our techniques with AutoTVM
in this paper and showed that the DL primitives created by PolyDL (our work) enjoy superior
performance vis-a-vis AutoTVM. Further, the time it takes for our techniques to create high
performance code is a fraction of the time AutoTVM takes. Tiramisu [7] is a polyhedral model
based compiler framework that introduces a scheduling language to allow the programmer to
explore various program transformations. Halide [41] is a Domain Specific Language (DSL) and
framework that introduces the distinction between an algorithm and its associated schedule. The
DSL provides high level abstractions for the programmer to encode different schedules and thereby,
explore different schedules in a productive way and discover high performance schedules. Adams

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

23

et al [6] automate the task of finding optimal schedules in Halide using machine learning. In
our present work, we take a compiler-centric approach: Using compiler-generated features we
identify promising schedules (either through a heuristic based cost model or with the use of a
neural network model). Because of these contrasting methodologies, Adamsâ€™ system would require
a lot more training data (and therefore, resources to optimize a given code) than our PolyDL system.
SWIRL [48] is a system similar to Halide in the sense SWIRL also allows the programmer to specify
the algorithm separately from its schedule. It allows one to encode numerous transformation recipes.
Our PolyDL system can be complementary to SWIRL in that, the techniques developed in our work
could be used to automate the task of deriving the beneficial transformation recipes automatically.
TensorComprehensions [47] and Diesel [22] are two DSL systems developed for generating efficient
code for GPUs. Both systems use the polyhedral model for code generation and provide auto-tuning
capabilities. Unlike our system which is geared towards CPUs, both TensorComprehensions and
Diesel target only GPUs. Since the considerations for optimizing code for GPUs are quite different
from that for CPUs, their techniques are not directly applicable for CPUs.

GEMM microkernel optimizations: Goto and van de Geijn [27] focus on creating high-
performance GEMMs by using a layered decomposition over the three micro-kernels, from which
the others could be derived. The results show how these three lowest level decompositions can
achieve high performance, thus resulting in an overall high performance for GEMM implementa-
tions. Springer et al. [43] use Tensor contraction for reducing the rank of matrices, followed by
packing the tensor operands of the macro-kernel in the available caches, thus attaining the high
performance. To remove the need of customization of microkernel for each hardware Veras et al.
[49] present an automated approach by decomposing the microkernel (GEMM) into unit updates
and building different algorithms over it. All these works focusing on microkernels are complemen-
tary to our approach: we could leverage microkernels created from any of these frameworks for
our inner loops, thereby increasing the programmer productivity in our PolyDL approach.

Two level optimization of code: Barthou et al [9] present a hierarchical compilation model,
wherein the outer loops are optimized for data locality, while the inner loops are abstracted into
kernels and are optimized for ILP (Instruction Level Parallelism) using the backend compiler. While
at a high level, their approach and ours are similar, there are some crucial differences: 1) The outer
loop optimization as developed in our work is more sophisticated (working set size enumeration
and subsequent use of a DNN model for ranking of code variants) compared to the selection of tile
sizes such that the data accessed in a tile fit in a certain level of cache as adopted in Barthou et alâ€™s
work. 2) The reliance on the backend compiler for kernel optimization in their work can lead to
substantially lower performance compared to what is achievable through the use of microkernels.
E.g., In our experimental evaluation (Â§6.3.2), we find that AutoTVMâ€™s (their approach is nearly
similar to AutoTVMâ€™s) performance is substantially lower than PolyDLâ€™s. 3) Their techniques are
not applicable for parallel code and consequently, for multi-core architectures while handling of
parallel code is baked into our techniques.

The BLIS framework [55] shows that BLAS routines can be instantiated using a single DGEMM
microkernel. The microkernel needs to be customized for a given hardware and the programmer
has to tune outer loops for cache use by choosing appropriate block (tile) sizes for the target
architecture. Our PolyDL approach is similar to BLISâ€™s in spirit: we decompose the problem of
optimizing an operator into two subproblems â€“ one, performing outer loop optimization, and use
of a microkernel for the inner loops. In our framework, we use novel polyhedral model based cache
data reuse algorithms to perform outer loop optimizations automatically. Additionally, the use of
a neural network based approach, and operator fusion algorithms are new and are beneficial for
deep learning workloads.

, Vol. 1, No. 1, Article . Publication date: November 2020.

24Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul and Gagandeep Goyal, Ramakrishna Upadrasta

8 CONCLUSION
In this paper, we presented novel compiler algorithms to derive high performing DL primitive
implementations automatically and to perform operator fusions. We proposed a methodology to
optionally use microkernels for the inner most loops of DL primitives to optimally use the vector
pipelines of modern CPUs. With a combination of the above techniques, we demonstrated through
experimental evaluation that we are able to match the performance of expert coded implementations
of the Intel oneDNN library for CNNs and GEMMs. Additionally, because our method works at
compile-time, we require much less time and compute resources to derive efficient implementations
compared to auto-tuning systems such as AutoTVM. Our system â€“ PolyDL will ease the development
of computer architecture specific libraries by at most requiring the development of only a small
number of microkernels. Additionally, it will allow data scientists to enjoy high performance on
the new DNN architectures they develop immediately and automatically, without waiting for their
DNN constructs to be implemented in a library by expert programmers.

REFERENCES
[1] [n.d.]. Google is AI first: 12 AI projects powering Google products. https://blog.aimultiple.com/ai-is-already-at-the-heart-of-google/
[2] [n.d.]. How to optimize GEMM on CPU. https://tvm.apache.org/docs/tutorials/optimize/opt_gemm.html
[3] [n.d.]. Library targeting Intel Architecture for specialized dense and sparse matrix operations, and deep learning primitives.

https:

//github.com/hfp/libxsmm

[4] [n.d.]. oneAPI Deep Neural Network Library (oneDNN). https://github.com/oneapi-src/oneDNN
[5] [n.d.]. Why GEMM is at the heart of deep learning. https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/
[6] Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li, MichaÃ«l Gharbi, Benoit Steiner, Steven Johnson, Kayvon
Fatahalian, FrÃ©do Durand, et al. 2019. Learning to optimize halide with tree search and random programs. ACM Transactions on
Graphics (TOG) 38, 4 (2019), 1â€“12.

[7] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Abdurrahman Akkas, Yunming Zhang, Patricia Suriana,
Shoaib Kamil, and Saman Amarasinghe. 2019. Tiramisu: A polyhedral compiler for expressing fast and portable code. In Proceedings
of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization. IEEE Press, 193â€“205.

[8] Wenlei Bao, Sriram Krishnamoorthy, Louis-Noel Pouchet, and P Sadayappan. 2017. Analytical modeling of cache behavior for affine

programs. Proceedings of the ACM on Programming Languages 2, POPL (2017), 32.

[9] Denis Barthou, Sebastien Donadio, Patrick Carribault, Alexandre Duchateau, and William Jalby. 2007. Loop optimization using hi-
erarchical compilation and kernel decomposition. In International Symposium on Code Generation and Optimization (CGOâ€™07). IEEE,
170â€“184.

[10] Muthu Manikandan Baskaran, Albert Hartono, Sanket Tavarageri, Thomas Henretty, Jagannathan Ramanujam, and Ponnuswamy
Sadayappan. 2010. Parameterized tiling revisited. In Proceedings of the 8th annual IEEE/ACM international symposium on Code generation
and optimization. ACM, 200â€“209.

[11] Gaurav Batra, Zach Jacobson, Siddarth Madhav, Andrea Queirolo, and Nick Santhanam. 2018. Artificial-intelligence hardware: New

opportunities for semiconductor companies. https://www.mckinsey.com/industries/semiconductors/our-insights

[12] Uday Bondhugula, Sanjeeb Dash, Oktay Gunluk, and Lakshminarayanan Renganarayanan. 2010. A model for fusion and code motion
in an automatic parallelizing compiler. In 2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT).
IEEE, 343â€“352.

[13] Uday Bondhugula, Albert Hartono, J. Ramanujam, and P. Sadayappan. 2008. A Practical Automatic Polyhedral Program Optimization

System. In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI).

[14] Chun Chen. 2007. Model-guided empirical optimization for memory hierarchy. University of Southern California.
[15] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis
Ceze, et al. 2018. {TVM}: An automated end-to-end optimizing compiler for deep learning. In 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18). 578â€“594.

[16] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018.

Learning to optimize tensor programs. In Advances in Neural Information Processing Systems. 3389â€“3400.

[17] I-Hsin Chung, Jeffrey K Hollingsworth, et al. 2004. Using information from prior runs to improve automated tuning systems. In

Proceedings of the 2004 ACM/IEEE conference on Supercomputing. IEEE Computer Society, 30.

[18] Jay LT Cornwall, Paul HJ Kelly, Phil Parsonage, and Bruno Nicoletti. 2007. Explicit dependence metadata in an active visual effects

library. In International Workshop on Languages and Compilers for Parallel Computing. Springer, 172â€“186.

[19] Alain Darte, Alexandre Isoard, et al. 2014. Parametric tiling with inter-tile data reuse. IMPACT 2014 (2014).
[20] AutoTVM developers. 2020. AutoTVMâ€™s X86 specific code. https://github.com/apache/incubator-tvm/blob/master/topi/python/topi/x86
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for

language understanding. arXiv preprint arXiv:1810.04805 (2018).

[22] Venmugil Elango, Norm Rubin, Mahesh Ravishankar, Hariharan Sandanagobalane, and Vinod Grover. 2018. Diesel: DSL for linear
algebra and neural net computations on GPUs. In Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning
and Programming Languages. 42â€“51.

[23] Paul Feautrier. 1996. Automatic parallelization in the polytope model. In The Data Parallel Programming Model. Springer, 79â€“103.
[24] Evangelos Georganas, Kunal Banerjee, Dhiraj Kalamkar, Sasikanth Avancha, Anand Venkat, Michael Anderson, Greg Henry, Hans
Pabst, and Alexander Heinecke. 2020. Harnessing Deep Learning via a Single Building Block. In 2020 IEEE International Parallel and
Distributed Processing Symposium (IPDPS). IEEE, 222â€“233.

[25] Somnath Ghosh, Margaret Martonosi, and Sharad Malik. 1997. Cache miss equations: An analytical representation of cache misses. In

International Conference on Supercomputing. Citeseer, 317â€“324.

, Vol. 1, No. 1, Article . Publication date: November 2020.

PolyDL: Polyhedral Optimizations for Creation of High Performance DL primitives

25

[26] Ross Girshick. 2015. Fast R-CNN. arXiv:1504.08083 [cs.CV]
[27] Kazushige Goto and Robert A van de Geijn. 2008. Anatomy of high-performance matrix multiplication. ACM Transactions on Mathe-

matical Software (TOMS) 34, 3 (2008), 1â€“25.

[28] Tobias Gysi, Tobias Grosser, Laurin Brandner, and Torsten Hoefler. 2019. A fast analytical model of fully associative caches. In Proceed-

ings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. ACM, 816â€“829.

[29] Albert Hartono, Muthu Manikandan Baskaran, CÃ©dric Bastoul, Albert Cohen, Sriram Krishnamoorthy, Boyana Norris, Jagannathan
Ramanujam, and Ponnuswamy Sadayappan. 2009. Parametric multi-level tiling of imperfectly nested loops. In Proceedings of the 23rd
international conference on Supercomputing. ACM, 147â€“157.

[30] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. 2017. Mask R-CNN. CoRR abs/1703.06870 (2017). arXiv:1703.06870

http://arxiv.org/abs/1703.06870

[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the

IEEE conference on computer vision and pattern recognition. 770â€“778.

[32] Alexander Heinecke, Greg Henry, Maxwell Hutchinson, and Hans Pabst. 2016. LIBXSMM: accelerating small matrix multiplications
by runtime code generation. In SCâ€™16: Proceedings of the International Conference for High Performance Computing, Networking, Storage
and Analysis. IEEE, 981â€“991.

[33] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
IEEE Signal processing

Nguyen, Brian Kingsbury, et al. 2012. Deep neural networks for acoustic modeling in speech recognition.
magazine 29 (2012).

[34] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden,
Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International
Symposium on Computer Architecture (ISCA). IEEE, 1â€“12.

[35] Martin Kong and Louis-NoÃ«l Pouchet. 2019. Model-driven Transformations for Multi- and Many-core CPUs. In Proceedings of the 40th
ACM SIGPLAN Conference on Programming Language Design and Implementation (Phoenix, AZ, USA) (PLDI 2019). https://doi.org/10.
1145/3314221.3314653

[36] Alex Krizhevsky and Geoff Hinton. 2010. Convolutional deep belief networks on cifar-10. Unpublished manuscript 40, 7 (2010), 1â€“9.
[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In

Advances in neural information processing systems. 1097â€“1105.

[38] Louis-NoÃ«l Pouchet, CÃ©dric Bastoul, Albert Cohen, and John Cavazos. 2008. Iterative optimization in the polyhedral model: Part II,

multidimensional time. ACM SIGPLAN Notices 43, 6 (2008), 90â€“100.

[39] Louis-NoÃ«l Pouchet, Uday Bondhugula, CÃ©dric Bastoul, Albert Cohen, Jagannathan Ramanujam, and Ponnuswamy Sadayappan. 2010.
Combined iterative and model-driven optimization in an automatic parallelization framework. In SCâ€™10: Proceedings of the 2010
ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1â€“11.

[40] Eric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, Bharat Kaul, and Tushar Krishna.
2020. Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training. In 2020 IEEE International Symposium
on High Performance Computer Architecture (HPCA). IEEE, 58â€“70.

[41] Jonathan Ragan-Kelley, Andrew Adams, Sylvain Paris, Marc Levoy, Saman Amarasinghe, and FrÃ©do Durand. 2012. Decoupling algo-

rithms from schedules for easy optimization of image processing pipelines. ACM Transactions on Graphics (TOG) 31, 4 (2012), 1â€“12.

[42] Lakshminarayanan Renganarayanan, DaeGon Kim, Sanjay Rajopadhye, and Michelle Mills Strout. 2007. Parameterized tiled loops for

free. In ACM SIGPLAN Notices, Vol. 42. ACM, 405â€“414.

[43] Paul Springer and Paolo Bientinesi. 2018. Design of a high-performance gemm-like tensorâ€“tensor multiplication. ACM Transactions

on Mathematical Software (TOMS) 44, 3 (2018), 1â€“29.

[44] Sanket Tavarageri, Albert Hartono, Muthu Baskaran, Louis-NoÃ«l Pouchet, J Ramanujam, and P Sadayappan. 2010. Parametric tiling of

affine loop nests. In Proc. 15th Workshop on Compilers for Parallel Computers. Vienna, Austria.

[45] Sanket Tavarageri, J Ramanujam, and P Sadayappan. 2013. Adaptive parallel tiled code generation and accelerated auto-tuning. The

International Journal of High Performance Computing Applications 27, 4 (2013), 412â€“425.

[46] Ananta Tiwari, Chun Chen, Jacqueline Chame, Mary Hall, and Jeffrey K Hollingsworth. 2009. A scalable auto-tuning framework for

compiler optimization. In 2009 IEEE International Symposium on Parallel & Distributed Processing. IEEE, 1â€“12.

[47] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, An-
drew Adams, and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions.
arXiv preprint arXiv:1802.04730 (2018).

[48] Anand Venkat, Tharindu Rusira, Raj Barik, Mary Hall, and Leonard Truong. 2019. SWIRL: High-performance many-core CPU code

generation for deep neural networks. The International Journal of High Performance Computing Applications 33, 6 (2019), 1275â€“1289.

[49] Richard Michael Veras, Tze Meng Low, Tyler Michael Smith, Robert van de Geijn, and Franz Franchetti. 2016. Automating the last-mile

for high performance dense linear algebra. arXiv preprint arXiv:1611.08035 (2016).

[50] Sven Verdoolaege. 2010.
Springer, 299â€“302.

isl: An integer set library for the polyhedral model. In International Congress on Mathematical Software.

[51] Sven Verdoolaege and Tobias Grosser. 2012. Polyhedral Extraction Tool. In Second Int. Workshop on Polyhedral Compilation Techniques

(IMPACTâ€™12). Paris, France.

[52] Yao Wang and Eddie Yan. 2020. Auto-tuning a convolutional network for x86 CPU. https://docs.tvm.ai/tutorials/autotvm/tune_relay_

x86.html

[53] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, et al. 2016. Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144 (2016).

[54] Yuval Yarom, Qian Ge, Fangfei Liu, Ruby B Lee, and Gernot Heiser. 2015. Mapping the Intel Last-Level Cache. IACR Cryptology ePrint

Archive 2015 (2015), 905.

[55] Field G Van Zee, Tyler M Smith, Bryan Marker, Tze Meng Low, Robert A Van De Geijn, Francisco D Igual, Mikhail Smelyanskiy, Xianyi
Zhang, Michael Kistler, Vernon Austel, et al. 2016. The BLIS framework: Experiments in portability. ACM Transactions on Mathematical
Software (TOMS) 42, 2 (2016), 1â€“19.

, Vol. 1, No. 1, Article . Publication date: November 2020.

