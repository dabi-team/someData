1 

Deep Recurrent Neural Networks for Time-
Series Prediction 

Sharat C Prasad, Member, IEEE, and Piyush Prasad 

Abstract—  Ability  of  deep  networks  to  extract  high  level 
features  and  of  recurrent  networks  to  perform  time-series 
inference have been studied. In view of universality of one hidden 
layer  network  at  approximating 
functions  under  weak 
constraints,  the  benefit  of  multiple  layers  is  to  enlarge  the  space 
of  dynamical  systems  approximated  or,  given  the  space,  reduce 
the  number  of  units  required  for  a  certain  error.  Traditionally 
shallow  networks  with  manually  engineered  features  are  used, 
back-propagation extent is limited to one and attempt to choose a 
large  number  of  hidden  units  to  satisfy  the  Markov  condition  is 
made.  In  case  of  Markov  models,  it  has  been  shown  that  many 
systems need to be modeled as higher order. In the present work, 
we  present  deep  recurrent  networks  with 
longer  back-
propagation  through  time  extent  as  a  solution  to  modeling 
systems  that  are  high  order  and  to  predicting  ahead.  We  study 
epileptic  seizure  suppression  electro-stimulator.  Extraction  of 
manually engineered complex features and prediction employing 
them  has  not  allowed  small  low-power  implementations  as,  to 
avoid  possibility  of  surgery,  extraction  of  any  features  that  may 
be  required  has  to  be  included.  In  this  solution,  a  recurrent 
neural network performs both feature extraction and prediction. 
We  prove  analytically  that  adding  hidden  layers  or  increasing 
backpropagation  extent 
increases  the  rate  of  decrease  of 
approximation  error.  A  Dynamic  Programming  (DP)  training 
procedure employing matrix operations is derived. DP and use of 
matrix  operations  makes  the  procedure  efficient  particularly 
when using data-parallel computing. The simulation studies show 
the geometry of the parameter space, that the network learns the 
temporal structure, that parameters converge while model output 
displays  same  dynamic  behavior  as  the  system  and  greater  than 
.99 Average Detection Rate on all real seizure data tried.  

Index  Terms—  Recurrent  Neural  Networks,  Multi-layer 

Neural Networks, Artificial Neural Network, Neural Prosthesis 

I. 

INTRODUCTION 

T 

found 

in  many  areas 

including 
IME-SERIES  are 
communication, health and finance. Large-scale computer 
and  communication  networks  generate  time-varying  metric 
that  characterize  the  state  of  the  networks.  Audio  signal 
arriving at a microphone or radio-frequency signal arriving at 
a  receive  antenna  are  series  of  values.  In  the  area  of  health 
Electrocardiograph,  electroencephalogram,  etc  all  are  or 
generate  time-series  data.  End-of-day  face-values  of  different 

Date first submitted 8/15/2014.  
Sharat  C  Prasad 

is  Best3  Corp,  Saratoga,  CA  95070 

(e-mail: 

scprasad@ieee.org).  

Piyush  Prasad  is  with  the  Dept  of  Biomedical  Engineering,  University  of 

Washington, ST Louis, MO 65030 (e-mail:piyushsprasad@gmail.com). 

have 

Many 

understandably 

financial instruments are time-series. In all these cases, it is of 
interest  to  discern  patterns,  detect  behavior  that  differs  from 
ordinary and predict future behavior. 
applications 

stringent 
seizure 
requirements.  We  pick 
suppression  electro-stimulator  driven  by  automatic  seizure 
prediction  as  such  an  application  and  to  provide  the  context 
for  a  study  of  time-series  prediction.  Epileptic  seizure  is  an 
abnormality  characterized  by  brief  and  episodic  neuronal 
synchronous 
increased 
amplitude. 

discharges  with 

dramatically 

implantable 

epileptic 

Electroencephalography (EEG) provides measure of cortical 
resolution.  Signal 
activity  with  millisecond 
representations  and  processing  of  EEG  recordings  has  been 
considered since early days of EEG. Recordings are band-pass 
filtered (e.g. with a pass band of 1-70Hz) and digitized (e.g.  at 
200 samples/second and 12-bit resolution).  

temporal 

The current gold standard for determining onset times is the 
trained eye of an encephalographer. Expert encephalographers 
look  for  rhythmic  activity.  Brain  activity  during  a  seizure 
displays  a  great  deal  of  variation  from  patient  to  patient  and 
even from one incident to the next for the same patient.  

Implantable  neural  prosthetic 

that  deliver  electrical 
stimulation on-demand have emerged as a viable and preferred 
treatment  for  a  portion  of  the  population  of  seizure  patients. 
Up until recently for reasons of feasibility of implementation, 
systems  normally  take  the  form  of  bed-side  equipment 
communicating with implanted electrodes.  

Size 

is  a  consideration  for  any 

implantable  device. 
Furthermore  both  on  account  of  power  availability  and  of 
damage  to  surrounding  tissue  by  the  heat  produced  by  the 
implanted device, power consumption is a consideration. 

Detection  algorithms  employ  characteristics  of  or  features 

extracted from EEG signal for detection.  

the  overall  size  and  power 

Both feature computation and prediction modules contribute 
to 
requirement.  Feature 
computation  and  prediction  employ  algorithms  required  to 
their  objective.  Each  feature  computation  may 
achieve 
compute a distinct feature and only this feature.  

 Savings  result  if  same  computation  element  is  reused  for 
sub-phases  of  the  same  computation  phase  (e.g.  different 
if  same 
feature  computations).  Greater  savings 
computation  element  is  reused  across  phases  (e.g.  feature 
computation and prediction). 

result 

Given the variability in signature in EEG of seizure even for 
the same patient, a rich variety of features has been designed 
to  assist  with  the  task  of  seizure  detection  [17][9].  Given  the 

 
  
 
 
 
consideration  of  surgery,  it  is  desirable  that  the  implant 
include  support  for  any  feature  computation  that  is  or  may 
become necessary1. Support for comprehensive set of features 
is made complicated by the possibility that the optimal set of 
features  may  differ  for  the  same  patient  from  one  stage  of 
treatment to a following stage. 
Given  the  vector  of  inputs 

,  vector  of  features 

  is 

computed. System is modeled as having internal state 

 and 

transition 

function 

  such 

that 

  and 

measurement function 

 such that 

, are defined. 

In defining the internal state 

, attempt is made to satisfy the 

Markov condition 

. 

As is usual to the type of problem at hand, the parameters of 
the  prediction  algorithm  may  need  to  be  set  by  a  training 
phase.  Also,  as  elsewhere,  there  are  two  options  to  training  - 
offline  and  online.  A  consideration  with  offline  trained 
algorithms  is  that  as  parameters  change,  the  offline  training 
needs to be repeated (or also continuously run in parallel) and 
the detection algorithm needs to be configured afresh with the 
new parameters.  

Systems  employing  online  training  can  be  considered  to 
continuously incrementally train themselves and therefore able 
to adapt.  

We  will  refer  to  frameworks  where  given  the  sequence  of 

[
U k+1

inputs 
Z k[
known  –  the  measurement  -  function  but 

] = u1, u2,…, uk+1
{
}   where  each 

] = z1, z2,…, zk

and 

  is 

}  

{

observations 

, 

  is  a 

s  are  not 

2 

. 

(1) 

  is  a  trade-off  constant  that  trades-off  training  error 

Here 
minimization for regularization. 

As  expressed  above  the  problem  is  called  ridge-regression 

and has a simple closed form solution 

. 

(2) 

The above solution represents an off-line training method and, 
as introduced above has following assumptions: 
 is linearly dependent on 

, 

• 
• 
• 
• 

s are i.i.d., 

components of  

 are not linearly correlated, 

given training pairs (

,

) are noise free and 

objective and regularization are as in Ridge 
Regression. 

We  know  each  of  these  to  not  hold  for  Epileptic  Seizure 
prediction.  Adaptability  and  therefore  online  training  is 
desirable  in  seizure  prediction  methods.  Only  a  highly 
complex  feature  can  hope  to  achieve  linear  relation  to 
prediction  output.  Individual  feature  vectors  in  a  sequence  of 
feature  vectors  are  not  independent.  Training  data  is  usually 
significantly noisy. An objective such as maximizing average 
detection  rate  is  not  maximizing  linear  combination  of 
distances. 

When  components  of 

  are  not  completely  orthogonal, 

€ 

observable,  notation 

  stands  for  estimate  of 

  given 

methods exist to orthogonalize them [13]. 

€ 

 and 

 and then, when  

When 

s  are  not  i.i.d.,    'Time  Series'  methods  consider 

, we first predict 
{

is  available,  compute 

}  as  predict-
correct frameworks. The ability of predict-correct frameworks 
to  “go  back”  and  compute  an  improved  estimate 

,…, ˆ x k−q+2 k+1

ˆ x k+1 k+1
|

, ˆ x k k+1
|

|

than 

€ 

  computed  earlier  offers  the  possibility  that 

occurrence of an easier to detect event at instant 
used  to  learn  correct  mapping  when 

 can be 
  were  either  not 

available or were not informative. 

The problem is one of computing parameters 

of a parameterized function 

the  training  input  data 

from  values 

ˆ Z = ˆ z j = f w x j

{

and supplied output values 

€ 

j=N

w = wi{

i=M  
} i=1
 such that when it is applied to 
X = x j{
} j=1
} j=1
)
(
Z = z j{ } j=1

  returned  by  the  function 

  some  metric  computed 

 is optimized. 

€ 

j=N

j=N

For example, the problem can be expressed as 
€ 

€ 

1  Another  application  area  with  this  characteristic  is  Deep  Space 
Exploration.  Once  a  explorer  is  launched  with  certain,  preferably  learning, 
computation  engines,  these  computation  engines  have  to  learn  to  solve  the 
problem even if the characteristics of the problems evolve as explorer travels 
deeper and deeper into outer space. 

that 

  is  value  at  'time  instant' 

, 

  is  value  at  'time 

instant' 

  and  so  on.  These  methods  take  covariance 

explicitly into account. 

Assumption  of  linear  relation  of 

  to 

  can  be 

eliminated by employing a Neural Network.  

Feedback 

interconnections  within  Recurrent  Neural 
Networks  impart  them  the  ability  to  retain  information  and 
enable  them  to  perform  inference  tasks  such  as  Epileptic 
Seizure  Prediction 
as 
Electroencephalographic data of brain activity. 

involving 

series 

such 

time 

Deep  Neural  Networks  can  automatically  extract  relevant 
high-level  characteristics  making  it  unnecessary  to  have  to 
manually  engineer  characteristic  extraction  and  avoiding  loss 
of performance from characteristics being not matched to task 
importantly,  automatic  feature 
at  hand.  However,  most 
extraction,  addresses  the  need  for  adaptability  whereby 
features suited to a patient are used.  

However  both,  the  network  being  deep  and  network 
incorporating  feedback,  add  to  the  difficulty  of  training  the 
network.  Approaches  considered  for  training  deep  networks 
include  combining  and  following  unsupervised  pre-training 
with supervised training [4]. At core of both of these training 
algorithms is usually the backpropagation gradient descent.  

 
 
 
 
Among 

training 
the  earliest  methods  considered  for 
Recurrent  Networks  is  the  Back-Propagation  Through  Time 
(BPTT)  method  [24]  that  can  be  considered  the  analogue  of 
the  Back-Propagation  method  for  Feed-Forward  Networks. 
However the BPTT method had been believed to be prone to 
getting  caught  in  many  shallow  local  minima  on  the  error 
hyper-surface in the parameter hyper-space. Traditionally, for 
local  minima  avoidance,  unit  activation  and/or  weight  update 
are  made  stochastic  and  the  activation  or  the  weight  are 
assigned, rather than the exact computed value, a value drawn 
from  the  applicable  distribution  with  the  computed  values  as 
moments of the distribution. 

Furthermore  limitations  stem  from  approximating  the  non-
linear dynamics of systems as (locally) linear for purposes of 
deriving  the  moments  of  the  posterior  state  distribution  and 
from approximating the state distributions as Gaussian. 

It is to be noted that greater the non-linearity greater are the 
errors  introduced  by  the  linear  approximation.  In  case  when 
process and measurement functions are deep neural networks, 
if the transformation from external inputs thru the collection of 
layers  to  final  outputs  is  considered,  the  transformation 
represents a large degree of non-linearity.  

Computing  the  derivatives  is  tedious  and  is  considered 

another shortcoming of methods requiring derivatives.  

It  is  needed  to  address  both  the  eventual  and  the  speed  of 
convergence. The model of a system, that when supplied with 
some  input  with  stationary  statistics,  is  producing  the  same 
time  varying  output  as  the  system,  cannot  be  considered 
converged if one-step updates are non-zero. 

The  main  contribution  of  the  present  work  are  (i)  Deep 
Recurrent Neural Network as a unified solution for automatic 
extraction  of  relevant  features,  automatic  construction  of 
internal  state  and  prediction  for 
time-series  data,  (ii) 
theoretical  derivation  of  the  broader  space  of  dynamical 
systems  enabled  by  both  more  than  1  hidden  layer  and  back-
propagation  through  time  by  greater  than  1  instant  (iii)  the 
Back-Propagation Thru Time-and-Space training method as a 
solution to hard problem of training deep recurrent networks, 
(iv)  the  dynamic  programming  formulation  of  the  weight 
computation  and  (v)  small,  low-power  and  adaptive  epileptic 
seizure prediction solution. 

In outline, this report presents in Section 2, Deep Recurrent 
Neural Networks and their  Back-propagation Thru Time and 
Space Training. Section 3 discusses universality of Recurrent 
Neural Networks and the bound on error in approximation by 
RNN.  Section  5  presents  simulation  study  of  training  of  and 
prediction  using  a  Deep  Recurrent  Neural  Network  for 
automatically extracted features. 

II.  TIME-SERIES AND PREDICTION  

A.  Epileptic Seizure 

episodic  neuronal 

Epileptic  seizure  is  an  abnormality  characterized  by  brief 
and 
synchronous  discharges  with 
dramatically  increased  amplitude.  Seizures  are  generated  by 
abnormal  synchronization  of  neurons  which  is  unforeseeable 
for the patients. 

3 

Electroencephalography (EEG) provides measure of cortical 
activity with millisecond temporal resolution. It is a record of 
electrical potential generated by the cerebral cortex nerve cells 
(Figure  1).  When  epileptic  brain  activity  occurs  locally  in  a 
region  of  the  brain  and  it  is  seen  only  in  a  few  channels  of 
EEG  and,  when  it  happens  globally  in  the  brain,  it  is  seen  in 
all channels of EEG. 

An  example  setup  for  EEG  recording  [21]  may  include 
Ag/AgCl disc electrodes placed in accordance with one of the 
electrode  placement  systems  (e.g.  the  10-20  international 
electrode  placement  system  [16]).  Electrodes  may  be  placed 
on  the  scalp,  or  may  be  leads  that  are  implanted  in  the 
cranium.  These  may  be  implanted  in  or  on  the  cortex  at  the 
seizure  onset  zones.  An  example 
the  sensor-neuro-
stimulator  device  from  NeuroPace  [23]  that  includes  an 
implantable battery-powered neuro-stimulator and quadripolar 
depth or strip leads that are implanted in the cranium and are 
used for both sensing and stimulation. 

is 

Signal  representations  and  processing  of  EEG  recordings 
has been considered since early days of EEG. Recordings are 
band-pass  filtered  (e.g.  with  a  pass  band  of  1-70Hz)  and 
digitized  (e.g.    at  200  samples/second  and  12-bit  resolution). 
Representations  based  on  Fourier  Transform  have  been 
popular.  Four  characteristic  bands  -  delta  (<4  Hz),  theta  (4-8 
Hz), alpha (8-13 Hz) and beta (13-10 Hz) are identified in the 
Fourier Transform. 

The current gold standard for determining onset times is the 
trained eye of an encephalographer. Expert encephalographers 
look for rhythmic activity. Epileptiform discharge may be seen 
in  the  form  of  spikes  repeating  at  a  frequency  of  3-Hz  and 
wave  complex 
(Figure  2).  Phase 
synchronization and changes in it, are believed to accompany 
seizures.  Reported  measures  representative  of  seizure  also 
include spectral power, synchronization or amplitude deviates 
from normal  bounds. 

in  absence  seizure 

Figure 1: Normal brain activity. 

Brain  activity  during  a  seizure  displays  a  great  deal  of 
variation from patient to patient and even from one incident to 
the  next  for  the  same  patient.  A  patient  may  exhibit  low 
voltage fast gamma activity, may have a beta frequency burst 

 
 
 
just prior to high frequency activity, may have onset activity in 
alpha  bands  characterized  by  rhythmic  round  sinusoidal 
waves,  may  have  sharp  waves  in  the  delta-theta  range 
followed by high amplitude spiking activity, may have one of 
these  onset  activities  or  voltage  gamma  activity  preceded  by 
spike-and-wave  activity  or  may  have  activity  from  the  above 
entire range preceded by semi rhythmic slow waves [17].  

 There are two benefits to the Time Series analysis of EEG 
data advocated here - (1) offline to determine whether pre-ictal 
states  exist  enabling  long-term  advance  prediction  of  seizure 

4 

While wireless communication between implanted electrode 
and external control unit is possible to simplify the implanted 
electronics, this may limit fine-grained analysis as the possible 
data  rates  may  remain  of  the  order  of  100s  of  Kbps  and 
therefor  too  low  for  some  time  to  come  [25].  While  new 
wireless powering technology is coming into being, the rate of 
energy  transferred  may  remain  low  for  some  time  to  come 
[18].  Hence  size  and  power  consumption  need 
to  be 
considered along side detection efficiency. 

Detection  algorithms  employ  characteristics  of  or  features 
extracted  from  EEG  signal  for  detection.  Feature  may  be 
computed  on  the  raw  time-domain  or  frequency  or  another 
transform  domain  representation.  For  example,  sequence  of 
samples  can  be  divided  into  time  windows,  frequency 
transform  can  be  computed  for  each  time  window  to  obtain 
spectral  power  in  a  few  different  frequency  bands,  the  vector 

Figure 2: Brain activity during seizure. 

and  (2)  online  employing  information  gained  from  (1)  for 
preventative  therapy  in  the  form  of  electro-stimulation  that 
have  been  seen  to  be  able  to  suppress  or  limit  the  seizure  if 
applied in a timely manner. Electro-stimulation is believed to 
reset brain dynamic from pre-ictal to inter-ictal state. 

Implantable  neural  prosthetic 

that  deliver  electrical 
stimulation on-demand have emerged as a viable and preferred 
treatment  for  a  portion  of  the  population  of  seizure  patients. 
While  timely  application  of  electro-stimulation  is  seen  to 
suppress  seizures,  extended  or  more  than  necessary  electro-
stimulation  is  seen  to  damage  neuronal  tissue  and  have  such 
adverse  consequences  as  requiring  increasing  amounts  of 
stimulation to have effect and stimulation induced suppression 
of  neuronal  excitability.  Up  until  recently  for  reasons  of 
feasibility of implementation, systems normally take the form 
of  bed-side  equipment  communicating  with 
implanted 
electrodes. In comparison self-contained neural prosthetic will 
not  require  the  patient  to  be  bed-bound.  Responsive  neuro-
stimulation, by virtue of being timely and limiting the amount, 
may  also  significantly  increase  the  effectiveness  of  therapy. 
The advent of very low-power battery-operated electronics has 
made  possible 
implantable  electronics  with  closed-loop 
working of detection and stimulation. 

Traditionally  detection  algorithms  have  emphasized 
detection  efficacy  and  have  disregarded  power  consumption 
and  size.  Size  is  a  consideration  for  any  implantable  device. 
Furthermore  both  on  account  of  power  availability  and  of 
damage  to  surrounding  tissue  by  the  heat  produced  by  the 
implanted device, power consumption is a consideration. 

Figure 3: Organization of detection sub-system. 

of  spectral  powers  can  be  a  feature  vector  and  a  sequence  of 
such vectors may be analyzed by a detection algorithm. Figure 
3  shows  the  overall  organization  of  the  detection  sub-system 
comprising of feature computation and prediction modules. 

the  overall  size  and  power 

Both feature computation and prediction modules contribute 
to 
requirement.  Feature 
computation  and  prediction  employ  algorithms  required  to 
achieve 
their  objective.  Each  feature  computation  may 
compute a distinct feature and only this feature.  

 In  above,  we  gave  examples  of  characteristics  expert 
encephalographers  look  for  in  the  EEG.  Some  of  these 
characteristics  were  described  in  terms  of  frequency  domain 
representation.  To  make  use  of  such  characteristic,  frequency 
to  be  computed.  Efficient 
domain  representation  needs 
frequency 
specialized 
computation 
algorithms  and  space,  time  and  power.  If  realized  as  circuits, 
to  compute  a  different 
the  circuits  may  not  serve 
representation. 

transform 

requires 

Time-domain 

features  avoid 

the  cost  of 

transform 

computation but require specialized algorithms. 

One  noteworthy  aspect  of  the  transform  computation 

 
 
 
algorithms, that we will use to argue in favor of our thesis, is 
that their regular structure enables easy to visualize trading-off 
of space (number of computation engines, amount of memory) 
for time and remain acceptable as long overall latency remains 
within  limits.  To  save  space  and  power,  computation  may  be 
performed  iteratively,  reusing  the  same  computation  element. 
Savings result if same computation element is reused for sub-
phases  of  the  same  computation  phase  (e.g.  different  feature 
computations).  Greater  savings  result  if  same  computation 
element is reused across phases (e.g. feature computation and 
prediction). 

Given the variability in signature in EEG of seizure even for 
the same patient, a rich variety of features has been designed 
to assist with the task of seizure detection [17][9]. There have 
been studies of efficacy of different detection features. Given 
the  consideration  of  surgery,  it  is  desirable  that  the  implant 
include  support  for  any  feature  computation  that  is  or  may 
become necessary2. Support for comprehensive set of features 
is made complicated by the possibility that the optimal set of 
features  may  differ  for  the  same  patient  from  one  stage  of 
treatment to a following stage. 

Advent  of  small  low-power  computer-on-chip  [20]  makes 
them  one  option 
feature  computation 
requirement. However, as numerous studies have shown, size 
and  power  requirement  of  general  purpose  programmable 
solutions always exceed custom optimized solutions. 

to  address 

the 

There  have  also  been  studies  of  attempts  [9]  to  improve 
performance  by  using  detection  features  in  combination. 
Latter  has  shown  that  when  all  of  a  set  of  detection  features 
are  used  best  performance  results.  However  the  benefit  from 
using  more  than  a  certain  number  of  features  may  not  justify 
their  incremental  cost  when  size  and  power  constraints  exist 
[17]. 

€ 

B.  Time-series prediction 

Given  the  vector  of  inputs 

,  vector  of  features 

  is 

computed. System is modeled as having internal state 

 and 

transition 

function 

  such 

that 

  and 

measurement function 

 such that 

, are defined. 

In defining the internal state 

, attempt is made to satisfy the 

Markov condition 

. 

As is usual to the type of problem at hand, the parameters of 
the  prediction  algorithm  may  need  to  be  set  by  a  training 
phase.  Also,  as  elsewhere,  there  are  two  options  to  training  - 
offline  and  online.  An  offline  (batch)  training  algorithm  is 

given  feature  vector 

  and  target 

  pairs 

{
(

xi, zi

i= N

}i=1
)

. 

The  training  goes  over  all  the  data  to  determine  detection 
parameters. A consideration with offline trained algorithms is 
that  as  parameters  change,  the  offline  training  needs  to  be 

€ 

2  Another  application  area  with  this  characteristic  is  Deep  Space 
Exploration.  Once  a  explorer  is  launched  with  certain,  preferably  learning, 
computation  engines,  these  computation  engines  have  to  learn  to  solve  the 
problem even if the characteristics of the problems evolve as explorer travels 
deeper and deeper into outer space. 

5 

repeated  (or  also  continuously  run  in  parallel)  and  the 
detection  algorithm  needs  to  be  configured  afresh  with  the 
new parameters.  

On  the  other  hand,  an  online  (incremental)  algorithm 
initializes  its  parameters  to  certain  initial  values,  is  given 
  or  one  feature-vector-target 
either  just  one  feature  vector 

pair 

.  An  online  algorithm  only  produces  a  prediction 

 if only given a feature vector or also updates its parameters 

if  given  a  feature-vector-target  pair 

.  To  start  with,  an 

on-line  trained  algorithm  may  produce  poor  predictions.  As 
the  data  characteristics  are 
improve. 
Systems  employing  online  training  can  be  considered  to 
continuously incrementally train themselves and therefore able 
to adapt. One issue with online training is that for supervised 
 is 
training, target 

 corresponding to each feature-vector 

learnt,  predictions 

required.  A  second  issue  is  that,  on  one  hand,  recent  feature-
vector-target  pairs  may  cause  invariant  characteristics  to  be 
over-written by transient characteristics and on the other hand, 
recent  pairs  may  not  cause  transient  characteristics  to  be 
overwritten by new transient characteristics. 

We  will  refer  to  frameworks  where  given  the  sequence  of 

[
U k+1

{
inputs 
Z k[ ] = z1, z2,…, zk
is  a  known  –  the  measurement  -  function  but 

} 
] = u1,u2,…,uk+1
}   where  each 

and 

  is 

{

observations 

, 

s  are  not 

observable,  notation 

€ 

  stands  for  estimate  of 

  given 

 and 

, we first predict 

 and then, when  

{

, ˆ x k k+1
|

,…, ˆ x k −q+2 k+1

is  available,  compute 

ˆ x k+1 k+1
|
predict-correct  frameworks.  The  ability  of  predict-correct 
frameworks  to  “go  back”  and  compute  an  improved  estimate 
the 

than 
€ 
possibility  that  occurrence  of  an  easier  to  detect  event  at 
  can  be  used  to  learn  correct  mapping  when 
instant 

  computed  earlier  offers 

|

}  as 

 were either not available or were not informative. 

Metrics  called  specificity  (

Average  Detection  Rate  (

),  sensitivity  (

)  and 
)  are  used.  Sensitivity  is 

defined  as 

(
SEN ≡ Y + / Y + + N −

)   where 

  is  the  count  of  

true  positives  and 

  is  the  count  of  false  negatives. 

Specificity  is  defined  as 

€ 

the  count  of  true  negatives  and 
positives. Finally 

  where 

  is 

  is  the  count  of  false 

. 

The  problem  is  one  of  computing  parameters 

of a parameterized function 

the  training  input  data 

from  values 

ˆ Z = ˆ z j = f w x j

{

and supplied output values 

€ 

j=N

  some  metric  computed 

 such that when it is applied to 
} j=1
U = u j{
} j=1
(
)
Z = z j{ } j=1

  returned  by  the  function 

 is optimized. 

j=N

j=N

€ 

€ 

 
 
 
 
 
 
For example, it may be desired that sum of squared errors is 

minimized. Then the problem is, find 

 where 

(3)  

prediction  output.  Individual  feature  vectors  in  a  sequence  of 
feature  vectors  are  not  independent.  Training  data  is  usually 
significantly  noisy.  Objectives  such  as  maximizing  average 
detection  rate  are  not  maximizing  linear  combination  of 
distances. 

6 

Having  computed 

,  prediction 

is  made  as 

. 

For  the  problem  under  consideration, 

,  and  a 
problem  encountered  is  that  of  over-fitting  wherein  a 
  is 
determined that well minimizes the sum of squared errors for 
the  training  data  (by  learning  too  well  the  characteristics 
present)  but  then  performs  poorly  on  other  valid  data  (that 
may exhibit a broader set of characteristics). To counter this, a 
regularization term may be added to the minimization and the 
problem may be written as 

argmin

ˆ w =

w

∑

⎛ 
(
⎜ 
⎝ 

(
f w u j

) − z j

)2

⎞ 
)
+λg w(
⎟ 
⎠ 

(4) 

If it is assumed that the relation of output 
input 

 is linear, i.e. 
m

 to corresponding 

€ 

z j =

∑ wi = u j

uij

T w j , 

(5) 

€ 

€ 

€ 

i=1

s  are  drawn  from  independent  identical  distributions  (

does  not  depend  on  any 

, 

  and 

  for  all 

,  are 

drawn from the same distribution), components of 

 are not 

linearly  correlated  and  lastly  the  regularization  function 
simply  takes  the  sum  of  square  of  individual  elements,  i.e. 

, the problem can be expressed as 

Figure 4: Prediction with manually engineered features 

∑

⎛ 
⎜ 
⎝ 

(

argmin

ˆ w =

w
argmin
(
(

w

=

Uw − z

(
f w u j

) − z j

)2
)T Uw − z

⎞ 
)
+λg w(
⎟ 
⎠ 
) . 
) +λwT w

(

(6) 

  is  a  trade-off  constant  that  trades-off  training  error 

Here 
minimization for regularization. 

As  expressed  above  the  problem  is  called  ridge-regression 

and has a simple closed form solution 

. 

(7) 

The above solution represents an off-line training method and, 
as introduced above has following assumptions: 
 is linearly dependent on 

, 

• 
• 
• 
• 
• 

s are i.i.d., 

components of 

 are not linearly correlated, 

given training pairs (

,

) are noise free and 

objective and regularization are as in Ridge 
Regression. 

We  know  each  of  these  to  not  hold  for  Epileptic  Seizure 
prediction.  Adaptability  and  therefore  online  training  is 
desirable  in  seizure  prediction  methods.  Only  a  highly 
complex  feature  can  hope  to  achieve  linear  relation  to 

Figure  5:  Metrics  SPC,  SEN  and  ADR  versus  illustrative  manually 
engineered features on a sample of real data shows SPC, SEN and ADR with 
respect to four manually engineered features [17] – Energy,  Hjorth Variance, 
Coastline and RMS Amplitude. 

Complex  objectives  rarely  have  solution  in  closed  form.  As 
long  as  the  objective  is  differentiable,  we  can  employ  a 
member  of  the  Gradient  Descent  family  of  algorithms  for 
by 
functions, 
solution. 
  are  able  to  express  a  wider  set 

characterized 

Convex 

of  objectives.  Carefully  designed  members  of  the  Gradient 
Descent 
family  can  optimize  convex  objectives,  can 
accommodate  samples  that  are  not  independent  [7]  and  even 
overcome noise and certain forms of non-convexity by taking 
advantage of the structure in the objective [15]. 

 
 
 
 
 
 
 
 
When  components  of 

  are  not  completely  orthogonal, 

methods  exist  to  orthogonalize  them  [13].  Most  regression 
packages detect when some components are linearly correlated 
and  either  suggest  components  to  eliminate  or  set  respective 
coefficients to zero. 

Ignoring  for  the  moment  that 

s  are  not  i.i.d.  and 

employing  manually  engineered 
features,  we  can  use 
organization  shown  in  Figure  3  to  perform  prediction.  The 
organization  uses  a  single  neuron.  The  weights  of  the  neuron 
can be trained using a member of the Gradient Descent family 
of algorithms. This organization does not compute any further 
higher  layer  features  beyond  those  manually  engineered  and 
makes predictions that are non-linear but not highly non-linear 
functions of the features. 

Figure  5  shows  SPC,  SEN  and  ADR  with  respect  to  four 
manually  engineered  features  0  –  Energy,    Hjorth  Variance, 
Coastline and RMS Amplitude. The non-smooth and gradient-
discontinuous  nature  of  the  plot  is  from,  among  others, 
presence of noise and use of discrete 'counts' of 'events' – a 0 if 
did not occur and a 1 if it did - in the definition of the SPC and 
SEN. 

When 

s  are  not  i.i.d.,    'Time  Series'  methods  consider 

that 

  is  value  at  'time  instant' 

, 

  is  value  at  'time 

instant' 

  and  so  on.  These  methods  take  covariance 

explicitly into account. 

Assumption  of  linear  relation  of 

  to 

  can  be 

eliminated  by  employing  a  Neural  Network.  Often  inference 
requires  computing  an  unknown  and,  possibly  highly,  non-
linear  function  of  some  characteristics  of  the  data.  Neural 
Networks can efficiently find nonlinear mappings. 

Feedback 

interconnections  within  Recurrent  Neural 
Networks  impart  them  the  ability  to  retain  information  and 
enable  them  to  perform  inference  tasks  such  as  Epileptic 
as 
Seizure  Prediction 
Electroencephalographic data of brain activity. 

involving 

series 

such 

time 

Deep  Neural  Networks  can  automatically  extract  relevant 
high-level  characteristics  making  it  unnecessary  to  have  to 
manually  engineer  characteristic  extraction  and  avoiding  loss 
of performance from characteristics being not matched to task 
at  hand.  However,  most 
importantly,  automatic  feature 
extraction,  addresses  the  need  for  adaptability  whereby 
features  suited  to  a  patient  are  used.  Furthermore,  as  the 
treatment progresses, the features used can also evolve to suit.  
However  both,  the  network  being  deep  and  network 
incorporating  feedback,  add  to  the  difficulty  of  training  the 
network.  Approaches  considered  for  training  deep  networks 
include  combining  and  following  unsupervised  pre-training 
with  supervised  training  [4].  The  unsupervised  pre-training 
tries  to  minimize  energy  as  low-energy  states  correspond  to 
parameters  attaining  values  that  capture  structures  present  in 
data. The following training then is intended to further evolve 
the parameters to capture the desired input-output mapping. At 
core  of  both  of  these  training  algorithms  is  usually  the 
backpropagation gradient descent. 

7 

Figure 6: Automatic High-Level Feature Extraction Using Deep Recurrent 

Neural Network 

Among 

training 
the  earliest  methods  considered  for 
Recurrent  Networks  is  the  Back-Propagation  Through  Time 
method [24] that can be considered the analogue of the Back 
Propagation method for Feed-Forward Networks. However the 
BPTT method had been believed to be prone to getting caught 
in  many  shallow  local  minima  on  the  error  hyper-surface  in 
the  parameter  hyper-space.  Traditionally,  for  local  minima 
avoidance,  unit  activation  and/or  weight  update  are  made 
stochastic and the activation or the weight are assigned, rather 
than  the  exact  computed  value,  a  value  drawn  from  the 
applicable  distribution  with  the  computed  values  as  the 
moments of the distribution. 

linear 

systems 

It  was  first  noted  in  late  80s  that  the  Neural  Networks  are 
not  unlike  non-linear  physical  systems  and  that  there  exists  a 
theory  of  estimating  hidden  state  and 
well-developed 
from  noisy 
estimating  parameters  of 
observations and somewhat less developed similar theories for 
nonlinear systems. A number of methods adhering to Predict-
Correct  framework  (e.g.  the  celebrated  Kalman  Filter)  have 
been  developed  over  time  that  are  optimal  in  the  sense  of 
Minimum Mean Squared Error for state only, parameter only 
and  joint  state  and  parameter  estimation  problems  for  linear 
systems.  Optimal  estimation  is  no  longer  computationally 
tractable when the system is nonlinear. 

system 

for  non-linear 

The  realization  that  Recurrent  Neural  Networks  are  non-
linear systems led to use of Extended Kalman Filter, popularly 
used 
identification,  parameter 
estimation, etc for training Recurrent Networks. The Extended 
Kalman  Filter  has  a  set  of  well  known  limitations.  The 
limitations  stem  from  approximating  the  non-linear  dynamics 
of  systems  as  (locally)  linear  for  purposes  of  deriving  the 
moments  of 
the  posterior  state  distribution  and  from 
approximating the state distributions as Gaussian. 

 
 
  
 
Time-series  prediction  has  been  addressed  by  Statistics, 
Dynamical  Systems,  Machine  Learning,    Signal  Processing 
and  Automatic  Control  communities.  The  need  to  address 
larger  and  more  complex  systems  has  led  beyond  Back-
Propagation  Through  Time  and  first  order  term    (in  Taylor 
expansion)  only  Extended  Kalman  Filter  [11]  to  including 
second order terms [14], avoiding explicit use of Jacobian and 
Hessian  [14],  Quadrature  [1]  and  Cubature  [1]  forms,  the 
Unscented  [12]  form,  the  marginalized  Sigma  Points  [19] 
form,  the  Particle  Bayesian  [22]  form,  the  Variational 
Bayesian [22] form and the Gaussian Process [3] form. While 
an  improvement  results,  a  conceptual  and  sometimes  also  a 
computational complexity cost has to be paid. 

It is to be noted that greater the non-linearity greater are the 
errors  introduced  by  the  linear  approximation.  In  case  when 
process and measurement functions are deep neural networks, 
if the transformation from external inputs thru the collection of 
layers  to  final  outputs  is  considered,  the  transformation 
represents a large degree of non-linearity.  

Computing  the  derivatives  is  tedious  and  is  considered 

another shortcoming of  methods requiring derivatives.  

It  is  needed  to  address  both  the  eventual  and  the  speed  of 
convergence. The model of a system, that when supplied with 
some  input  with  stationary  statistics,  is  producing  the  same 
time  varying  output  as  the  system,  cannot  be  considered 
converged if one-step updates are non-zero.  

C.  One-step and long-term time-series prediction 

8 

instead 
be 
approximations

obtained 

as 

a 

sequence 

of 

,

, 

, etc. 

networks,  where  weight 

In  the  context  of  training  multi-layer  recurrent  neural 
as 
,  this  fact  can  be  taken  advantage  of  by 

computed 

update 

is 

including  in  the  state,  in  addition  to  prior  outputs  of  hidden 
layers, the current output of visible and hidden layers. 

E.  High order Markov Processes 

Given  a  process  that  receives  input 

  at  instant 

  and 

produces  output 

,  the  process  may  employ  internal  state 

 to remember as much of past history as it needs to so that 

it  can  compute  output 

  considering  only 

  and 

. 

Expressed  another  way, 

and  system  output  is  considered  independent  of  past  input 
given the current internal state. Such processes are called first-
this  may  require,  not  unlike  carefully 
order.  However 
manually engineering features, carefully manually engineering 
the internal state that system must remember. A solution is to 
  instants. 
require  system  to  remember  history  going  back 
  and 
Then 

The problem comprises of being given an input time-series 

the process is called of order 

. 

  and  noisy  observations  of    corresponding  output 

time-series 

  where  observations  may  not  be 

available  at  each  instant  of  time 
result  being 

  and  the  required 
.  As  an  example,  the  input  time-series 

may  be  intra-cranial  electrode  sensed  electric  potential,  the 
, 
output time-series values may only be available for certain 
may  be  whether  the  patient  was  experiencing  an  epileptic 
seizure3 and the required result is whether the patient is about 
to experience an epileptic seizure 

 instants from now.  

D.  Extrapolating highly non-linear functions 

y = f x( ) = ea1e a2x+b2+b1   specified  as 
and 

) = ea1y1 +b1  

y = f1 y1(

functions: 

Consider  a  function 

a 
sequence  of 
y1 = f 2 x( ) = ea2x+b2 . 
€ 
We are given value 

 of 

 at 

 and it is desired to 

€ 

extrapolate value at 

. 
A  linear  approximation  can  be  done  and  we  can  write 

€ 

It  is  easy  to  show  that  a  more  accurate  approximation  can 

. 

3  Note  that  in  an  offline  training  situation,  the  EEG  data  may  be 
examined  by  an  expert  encephalographer  and  intervals  corresponding  to 
seizure  determined.  In  an  online  training  context,  the  parameters  may  be 
initialized  to  values  determined  offline  and  predictions  used  only  after 
stability is reached. 

III.  DEEP RECURRENT NEURAL NETWORKS 

 Neural  Networks  comprise  of  one  or  more  inputs,  one  or 
more neurons (referred to in this work as units), one or more 
outputs, directed or undirected interconnections among units, a 
weight associated with each undirected connection of or each 
directed connection into a unit (from an input or another unit) 
and  each  unit  computing  its  activation  by  applying  its 
activation function to weighted sum of inputs. 

Feed-Forward Neural Networks are arranged in layers with 
units  in  a  layer  receiving  inputs  from  only  units  in  previous 
layer  (or  outside  in  case  of  the  input  layer).  Feed-Forward 
networks  process  data  presented  at  an  instant  independent  of 
data  presented  at  previous  instants  and  so  are  unable  to 
perform inference on time series data. In a Recurrent Network, 
some of the interconnections are from units in a layer through 
a delay to units in the same or an earlier layer. 

As  both  stacking  of  recurrent  layers  and  back-propagating 
have  the  effect  of  extracting  information  from  input  or  from 
information  extracted  from  input  to  consider  alongside  input, 
it  is  a  question  as  to  whether  (i)  either  is  more  effective  than 
the  other  and  (ii)  having  both 
the 
approximation ability of the network.  

together  adds 

to 

Each unit in a layer associates a weight to and has as inputs 
a fixed bias, output of all units in the previous layer (external 
inputs in case of the visible layer) and, in case of the recurrent 
layer,  its  own  outputs  delayed  by  a  time  instant.  Output  of  a 

 
 
 
unit 

 is the result of applying a nonlinear function 

 to the 

weighted sum of inputs 

: 

. 

The  network  has  a  visible  layer, 

  hidden  layers  and  an 

output  layer  of  units  (neurons)  with,  respectively, 

, 

 and 

 units in the layer. The network receives 

inputs 

, 

 and produces outputs 

, 

. 

Furthermore, 

 is the  

-th unit (or alternatively unit 

) in 

the visible layer, 

 is the unit 

 in the hidden layer 

, 

 is 

the  unit 

  in the output layer, 

 is the input 

. 

As a convention, 

 is the vector with elements 

the output of unit 

, 

 is the weight from unit 

 is 

, 
€ 
 to  unit 

y

, 

− k(
hl, j
hidden  layer 

)  is the output at (past) instant 

 of the node 
€ 
.  Notionally,  associated  with  the  unit 

 in 

  in 

hidden  layer 
outputs 

y

− B(
hl, j

  are 
) through

  memories  that  retain  the  previous 
y
  is  referred  to  as 

) of  the  unit. 

−1(
hl, j

the extent of backpropagation.  

€ 

Inputs to each unit in any layer include a fixed bias 1. We 
use the same symbol as used to denote the output signal vector 
of  a  layer,  but  with  a  tilde  accent,  to  denote  a  signal  vector 
with  1  (corresponding  to  the  fixed  bias)  as  an  additional  and 
the first element and remaining elements same as the original 
signal vector. Then 
y v = Wv,u ˜ y u  
y hNL
y

= WhNL , v ˜ y v + W
−1( ) y
hNL
, hNL
) ˜ y u −1(
) + W

( −1) = W
( −1),hNL
hNL
hNL
Reader  may  notice  that  we  have  chosen  to  qualify,  rather 
than  the  symbol  denoting  the  output  signal  of  a  node,  the 
 denoting 
symbol denoting the node with the superscripts 

( −1),v −1(
hNL

−1( )  
hNL

( −2)  
hNL

( −2) y

(10) 

(8) 

(9) 

” rather than “value at past 
value of  “the node at past instant 
instant 
” of the node. This is deliberate and is to suggest that 
we consider the network to have been unfolded back and there 
to  exist 
  instants 
  copies,  one  corresponding  to  each  of 
spanning  the  extent  of  the  backpropagation,  of  the  complete 
“sub-network”  comprising  of  the  visible  and  each  of  the 
hidden layers. As a result, the notation allows not only vectors 
,  but 

  to  be  distinguished  from  the  vector 

also  weight  matrices 

distinguished from 

Wv −1(
. If this distinction was not made (10)  

) ,, Wv − B(

to  be 

) , u − B(

) , u −1(

)  

€ 

€ 

€ 

€ 

€ 

) ˜ y 

above will be 
( −1) = W
y
( −1) , hNL
hNL
hNL
Outputs  of  all  the  units  as  well  as  the  weights  associated 
with  the  inputs  to  the  units  are  considered  collected  into  a 
  is 
joint-state-and-parameter-vector.  This  vector  at  instant 

( −1) , v −1(
hNL
€ 

( −2)  
hNL

) + W

( −2) y

(11) 

v −1(

€ 

denoted as 

, estimate of the vector is denoted as 

and 

element 

 of the same is denoted as 

ˆ x k( )

(

)i

. Final outputs of 

9 

the  network  are  also  considered  collected  into  a  vector.  This 
vector at instant 

is denoted as 

. 

A  significant  component  of  any  method  making  use  of  the 

derivatives are the derivative computations 

€ 

(
F k( )

)ij

=

(
H k( )

)ij

=

)i
(
∂ ˆ x k( )
) j
(
∂ ˆ x k( )
)i
(
∂ ˆ z k( )
) j
(
∂ ˆ x k( )

(12) 

(13) 

Figure  7:  Back-Propagation  Through  Time  and  Space  in  Deep  Recurrent 
Neural Networks 

Visualization  of  dependencies  considered  in  the  derivative 
computation is eased by considering the nodes making up the 
network to be arranged as columns of stacks of layers of linear 
array of nodes (Figure 7). The input layer is at the bottom, the 
hidden layers in the middle and the output layer at the top. The 
rightmost  column  corresponds  to  current  instant  and  the 
columns to left to successively previous instants. A notion of 
distance can  be associated with any pair of units and of a unit 
and a weight. A unit in the (present-instant, output-layer) is at  
a  distance  of  1  from  any  unit  in  the  (present-instant,  hidden 
layer), at a distance 2 from any unit in (present instant, visible 
layer),  also  at  a  distance  2  from  (previous  instant,  hidden 

layer) and so on. Similarly the instant 
at  a  distance  1  from  (instant 

,  hidden  layer 

 weight 

 is 
, 

)  unit 

 
  
  
 
 
 
 
distance  2  from  any  unit  in  the  (present  instant,  hidden  layer 
,  layer 

),  also  distance  2  from  any  unit  in  the  (instant 

), distance 3 from any unit in the (present instant, layer 

),  also  distance  3  from  any  unit  in  (instant

,  layer 

), etc. 

Output  at  instant 

state at 
and state at 

. The state at instant 

  depends  on  parameters  (weights)  and 
 in turn depends on weights 
. Once weights have been updated at instant 
  can  be  computed.  To  compute  this 
  is  required.  However  the  state  at 
 itself can be recomputed using the updated weights and 
€ 

,  a  new  output  at 
output,  the  state  at 

€ 

€ 

the state at 

 and so on. 

Given  the  error  in  output  at 

,  part  of  the  error  can  be 
attributed  to  errors  in  weights  and  part  to  error  in  the 
(estimated) state at 
 in turn depends 
in part on errors in weights and part on error in state at 

. The error in state at 

€ 
€ 

. 

If the mapping from input to output is invariant, the weights 
at  instants 
  are  the  same.  However  if  the 
mapping itself is evolving then the two sets of weights are not 
the same. 

    and 

Once the weights corresponding to present and to each past 
instant  are  made  different  and  the  present  error  is  computed, 
weights  at  instant 
  can  be  overwritten  with  weights  at 
instant 
  and  present  instant  weights  can  be  updated  to  next 
instant weights. Alternatively, the back-propagation itself can 
be used to compute weights at present as well as each retained 
past instant. 

€ 

€ 

€ 

The  mapping 

  may  be  time-invariant, 

may  need  to  be  learnt  and,  once  learnt,  does  not  change. 
Alternatively the mapping may continuously evolve. 

€ 

Computing  only  next  instant  weights  by  updating  present 
instant  weights  and  overwriting  instant 
  weights  with 
  weights  is  consistent  with  models  that  allow  the 
instant 
mapping  to  be  learnt  and  then  assume  them  to  remain 
constant. Computing all the weights using back-propagation is 
consistent  with  models  where  mapping  may  continuously 
evolve. 

€ 

€ 

€ 

A.  Universality of single hidden layer feed-forward network, 
bounds on approximation error and multilayer networks 

Single  hidden  layer  feed-forward  networks  using  the 
monotone  cosine  squashers  are  capable  of  embedding  as  a 
special  case  a  Fourier  network  that  yields  a  Fourier  series 
approximation  to  a  given  function  at  its  outputs  [8]  and  thus 
possesses  all  the  approximation  properties  of  Fourier  series 
representations. 

€ 

€ 

A  feed-forward  network  with  a  single  hidden  layer  of 
continuous sigmoidal nonlinear squashers (in addition to linear 
visible  and  output  layers)  are  complete  in  the  space  of 

functions  on 
derivatives up through order 

 [10]. 

  that  have  continuous  and  uniformly  formed 

€ 

For  a  network  with  a  hidden  layer  with 

  units,  the 

approximation  error  is  bounded  by 

  [2]  where 

10 

depends on the norm of the Fourier Transform of the function 
being approximated 
(
|
∫
= 2r ω|
|

) 2
) 2
˜ F  dω(
(
|
)
c f ' = 2rC f
˜ F  w(
)  Fourier  distribution  of  function 
where 
) = eiθ w(
f x( ) = eiwx
∫
) , 

˜ F  dw(

, 

 is 

such that 

˜ F  dw(
the  magnitude  distribution, 

)F dw(
  the  phase  at  frequency 

(14) 

) , 

, 

and 
€ 

w the vector of frequencies 

(
ω1,,ωd

€ 

  be  the  class  of  functions 
Let 
˜ F  dw(
| < ∞  and 
|
∫
)
C f = w|
|
€ 
˜ F  dw(
R d  for which 
|
∫
C f = w|
|
Then, for every choice of fixed basis functions 

| < C . 
)

  the  class  of  functions 

) . 
  on 

  for  which 

[2] 

€ 
(
(
s f,span h1, h2,, hn
sup
f ∈ ΓC
where 
s f,G(

) = inf

s f, g(

)  

g ∈ G

) ≥κ
)

C
d

⎛ 
1
⎜ 
n
⎝ 

1

d( )
⎞ 
⎟ 
⎠ 

) =

∫

) 2
f x( ) − g x( )

(

dx  

s f, g(
Let, 

  on 

(15) 

(16) 

(17) 

 be a bounded set in 
the  set  of 
(
∫
€ 
)F dw(

functions 
˜ F  dw(
)
f x( ) = f 0( ) + eiw.x − 1
˜ F  dw(

) , 
the  set  of  functions 

) = eiθ w(

R d which contains 

  on 

R d  

)   holds  for 

, 
for  which 

  and  some 

€ 

€ 

  on 

R d   for  which 

(18) 

(19) 

F dω(

) ≤ C > 0 

∫
ω|
|B
where 
w|

{
|B = sup

w.x

} , 

x∈B

then  for  every  function 

,  for  every  probability 

measure 

, for every sigmoidal function 

,  for every 

, 

there exists a linear combination of functions 

 of the form 

n

f n x( ) = c k

∑ φ a kx +bk
(

) +c 0  

k=1

such that [2] 
(
∫
where  

) 2
µ dx(
Br = x ∈ R d x|

f n x( ) − f x( )
{

| ≤ r

} . 

) < c f ' / n = 2rC f

(

) 2

/ n  

(20) 

Hence  adding  a  sigmoidal  layer  improves  the  rate  from 

€ 

  to 

)1/ d
1/ n(
By how much? 

.  Will  adding  another  layer  improve  further? 

In  a  one  linear  layer  network  we  take  a  weighted  sum  of 

inputs  and  add  in  a  bias 

  where  for 

ease  of  explanation  we  number  the  elements  of 
starting at 

. 

  and 

Equation  (7)  bounds  below  the  approximation  error  of  one 

 
 
 
 
  
 
  
 
 
 
 
linear layer network. 

In  a  1  hidden  layer  network,  we  take 

  distinct  copies  of 
one  linear  layer  networks,  feed  the  output  of  each  into  a 
nonlinearity,  take  a  weighted  sum  of  the  outputs  of  the 
nonlinearities and add in a bias 
(
(
),,φ a n
n1 x( ) = bT . φ a1
T
⎞ 
)
),,
⎟ 
⎠ 

T x + a10
(

(
x( )

T x + an0

+b0 . 

= bT .

(21) 

+b0

x( )

)T

⎛ 
(
⎜ 
⎝ 

n0 ,n

n0 ,1

)

f

f

f

Equation (8) bounds below the rate of error. Note that here 
we  are  concerned  with  inherent  representability  using  multi-
layer  neural  networks,  are  not  concerned  with  error  in 
determination  of  weights  and  assume  that  values  of  weights 
minimizing error, if they exist, can be found.  
n2 x( ) − f x( )

) 2
µ dx(
n2 x( )   is  the  function 
computed  by  the  two  hidden  layers  network.  The  two  hidden 
  instances  of  the  one  hidden 
layers  network  has  (logically) 

Consider 

) . 

€ 

€ 

€ 

∫

(

f

f

layer  network  as  sub-networks.  Let 

€ 

€ 

  be  the  function 

€ 

minimum  error  and 

computed by the two hidden layers network when it achieves 
x( ),, ˜ f 
needed  to  be  computed  by  its  one  hidden  layer  sub-networks 
when the two hidden layers network achieves minimum error. 

)   be  the  functions 

˜ f 
n1 ,1

x( )

n1 , n

(

Let 

(

the 
˜ f 
n0 ,1

(

f

x( ),, f
€ 
n1 , n
hidden 
x( )

n1 ,1
one 
x( ),, ˜ f 

n0 , n

x( )

)   be  the  actual  functions  computed  by 

€ 
sub-networks.  Similarly 

layer 

let 

)  be the functions needed to be computed 

€ 

by  their  one  linear  layer  sub-networks  when  the  one  hidden 
layer  network 
to 
x( ),, ˜ f 
˜ f 
n1 ,1

achieves  minimum 
x( ),, f

n0 ,1
functions computed by the one linear layer sub-networks.  

)   be  the  actual 

error 
x( )

)   and 

relative 

x( )

n0 , n

n1 , n

(

(

f

n i ,1

Let us denote  
⎛ 
⎜ 
⎝ 
⎛ 
⎜ 
⎝ 
⎛ 
⎜ 
⎝ 
⎛ 
⎜ 
⎝ 

(
ψn i x( ) = φ f
(
˜ ψ ni x( ) = φ ˜ f 
(
+ x( ) = φ f
(
− x( ) = φ f

ψni

+
ni ,1

−
ni ,1

ni ,1

x( )
€ 

x( )

x( )

),,φ f
(
(
),,φ ˜ f 
),,φ f
(
(
),,φ f

n i , n

ni , n

+
ni , n

−
ni , n

)
x( )
)
x( )
)
x( )
)
x( )

⎞ 
⎟  
⎠ 
⎞ 
⎟  
⎠ 
⎞ 
⎟  
⎠ 
⎞ 
⎟  
⎠ 

f

x( )

ψni
Then  
∗ x( ) = ˜ ψ n1 x( ).c +c0 , 
n 2
n1 x( ) =ψn0 x( ).b+b0 , 
) 2
(
x( ) − ˜ f 
µ dx(
x( )

n1 ,k

n1 ,k

f

f

∫
and 

s ˜ f 
(

n 0, f

n 0

) ≥κ

sup
f ∈ΓC

C
d

⎛ 
1
⎜ 
n
⎝ 

1

d( )
⎞ 
⎟ 
⎠ 

. 

) < c f ' / n d = 2rC f

(

) 2

/ n d  

€ 

€ 

€ 

€ 

€ 

(22) 

(23) 

€ 

(24) 

€ 
€ 

(25) 

Hence  in  spite  of  the  one  linear  layer  subnetworks  having 

error  bounded  below  by 

,  1  hidden  layer 

∫
∫

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

11 

∫

(

f

n2 x( ) − f x( )

) 2
φ dx(

)  

subnetworks manage to achieve error  

bounded above by 

(

2rC f

) 2

/ n . 

What 

if 

the  errors  of  functions  computed  by 

the 

subnetworks 

are 

€ 
smaller 

e.g. 

(

2rC f

) 2

/ n  

versus 

 terms 

(26) 

€ 
) 1/ n(

) 1/ d(

) ? 

f

(
κ C / d
Note function computed is weighted sum of 
n1 x( ) =ψn 0 x( ).b+b0  
For squared minimum error 
2
⎞ 
∗ x( ) − f x( )
⎟ 
n1
⎠ 

⎛ 
⎜ 
f
⎝ 

€ 

f

) 2

n0,n

=

,b ,b0

(
ψn0 x( ).b+b0 − f x( )

inf
n0,1,, f
) 2
(
= ˜ ψ n 0 x( ).b+b0 − f x( )
While  there  can  be  cases  of  small2  input  errors  and  errors 
adding,  such  that  a  large  output  error  results.  However  in 
expectation and averaged over large number of points, smaller 
in  small  output 
input  (sub-network)  errors  will  result 
(network) errors. 

(27) 

If 
) 2
(
˜ ψ n0 x( ).b+b0 − f x( )

dµ x( )  

inf
n0,1,, f
n0,n

f

,b' ,b0'

) 2
(
ψn 0 x( ).b' +b' 0 − f x( )

∫

dµ x( )  

∫

=

and 

dµ x( ) 

then 
) 2
(
− x( ) .b+b0 − f x( )
∫
dµ x( )  
ψn0
) 2
(
+ x( ).b+b0 − f x( )
∫
< ψn0
Consider 
) 2
) 2
(
(
− x( ).b+b0 − f x( )
+ x( ).b+b0 − f x( )
ψn0
− ψn0
) 2
) 2
(
(
− ˜ ψ n0 x( ).b+b0 − f x( )
+ x( ).b+b0 − f x( )
= ψn0
) 2
) 2
(
(
+ ˜ ψ n0 x( ).b+b0 − f x( )
− x( ).b+b0 − f x( )
− ψn0
(
)
(
).b+ 2b0 − 2 f x( )
(
(
+ x( ) − ˜ ψ n 0 x( )
+ x( ) + ˜ ψ n 0 x( )
= ψn 0
× ψn 0
(
)
(
(
).b+ 2b0 − 2 f x( )
(
− x( ) − ˜ ψ n 0 x( )
− x( ) + ˜ ψ n 0 x( )
− ψn 0
× ψn 0
> 0 

)
).b
)  
).b

€ 

(32) 
The last step follows from each (inner) term of both factors 
of the first (outer) term of the above expression being greater 
than or equal to the corresponding (inner) term of the second 
(outer) term. 
Hence (11) follows. 

€ 

span

Intuitively 
⎛ 
(
⎜ 
φ f
⎝ 
(

span

nL −1 ,1

f

x( )

(
),,φ f

nL −1 ,1

x( ),, f

nL −1 , n

x( )

nL −1 , n

difference 
)
x( )
) dµ x( )  

⎞ 
⎟  dµ x( )  
⎠ 

between 

and 

over 

the 

entire 

(28) 

(29) 

(30) 

(31) 

 
  
  
  
  
  
  
  
  
  
  
  
  
 
  
 
 
 
  
  
function 

hypersphere 

  can  be  understood  by  considering  a 

 which remains equal to some value with 

) d
0,1(
f x( )  of 
 nearly everywhere but dips steeply down 
large magnitude 
)  points.  A  linear 
to 
function  can  only  descend  and  then  ascend  back  up  at  rate 
€ 
determined  by  the  linear  coefficients  and  will  produce  large 
 is not convex. 
error. Note nonlinearity such as 

  at  some  number  of  (related  to 

€ 

However 

 is. Furthermore, nonlinearity may still be 

convex/concave  between 
)π < x < −nπ . 
(
− n −1

(
− n − 2

)π < x < − n − 1

(

)π 

€ 

and 

(vector of weights for the hidden layer unit 
also 

  and  we  compute 

,  ..  , 

 at instant 

12 

) but 
,  ..  ,  

,  outputs  of  hidden  layer  unit 

  at  instants 

through 

. When the weights are updated, not only 

but  also 

,  ..  , 

  are  updated  and 

,  ..  , 

yH,i,k−1   are  recomputed. 
instant 
history. 

  and 

,  to  be  computed  at 

  now  make  use  of  recomputed  weights  and 

€ 

IV.  BACK-PROPAGATION THRU TIME AND SPACE 

€ 

For  Back-Propagation  Thru  Time  and  Space  Gradient 
  is  updated  according  to 

Descent  Training,  weight 

  where 

  is  the  error, 

  is  the 

output  of  the  single  output  network  at  the  instant  when  input 
 is a training input-

 has been applied at its input and 

output pair. 

Visualization  of  dependencies  considered  in  the  derivative 
computation is eased by considering the nodes making up the 
neural  network  to  be  arranged  as  columns  of  stacks  of  layers 
of linear array of nodes (Figure 7). The output layer is at the 
top, the hidden layers in the middle and the input layer at the 
bottom. The rightmost column corresponds to current time and 
the columns to left to successively previous times. 

A notion of distance can be associated with any pair of units 
and  any  pair  of  a  unit  and  a  weight.  A  unit  in  the  (present-
instant, output-layer) is at a distance of 1 from any unit in the 
),  at  a  distance  2  from  any 
(present-instant,  hidden  layer 

unit in (present instant, visible layer), also at a distance 2 from 
)  and  so  on.  Similarly  the 
(previous  instant,  hidden  layer 

present  instant  weight 

is  at  a  distance  1  from  (present 

instant, visible layer) unit 

, distance 2 from any unit in the 

(present instant, hidden layer 

), distance 3 from any unit in 

the (present instant, hidden layer 

), distance 3 from any 

unit  in  (previous  instant,  hidden  layer 

),  etc.  Hence  the 

distances here are partially over space and partially over time. 

Details  of  computation  of  the  derivatives  are  in  the 
Appendix. These suggest a dynamic programming method for 
computing  the  derivatives  comprising  of  traversing  the  graph 
from  inputs  to  outputs,  computing  derivative  of  unit  outputs 
wrt distance 1 away unit outputs, distance 1 away weights and 
distance  2  away  weights.  Derivatives  wrt  unit  outputs  at 
distance  greater  than  1  away  and  weights  at  distance  greater 
than  2  away  are  computed  by  taking  weighted  sums  over 
derivatives of distance 1 away unit outputs.  

A.  Adaptation and Autonomous Self Improvement 

BPTT  implies  that  at  instant 

,  we  not  only  have 

Figure 8: Minimum error (over all output layer weights) versus hidden layer 
weights for input weights (-1.5, -1.5) 

The ability of predict-correct frameworks to “go back” and 

compute  an  improved  estimate 

  than  the 

offers  the  possibility  that  occurrence  of  an  easier  to  detect 
  can  be  used  to  learn  a  mapping  that 
event  at  instant 
  alone  was 
produces  a  better 

  than  when 

available.  This 
to  seizure  onset 
is  especially  relevant 
prediction  as  determining  that  a  seizure  is  about  to  occur  is 
harder for both humans and computers than determining that a 
seizure is in progress. 

Figure 9: Minimum error (over all output layer weights) versus hidden layer 
weights for input weights (-1.5, 1.5) 

 
 
 
 
 
 
 
 
 
 
 
V.  EXPERIMENTAL RESULTS 

The  first  set  of  experiments  explored  the  geometry  of  the 
parameter space first with (hidden layers 1, back-propagation 
extent  1),  (hidden  layers  1,  back-propagation  extent  2)  and 
finally with (hidden layers 2, back-propagation extent 1). The  
visible,  hidden  and  output  layers  each  had  one  unit.  Units  in 
visible  and  output  layers  had  one  input  from  outside  or  the 
previous layer and one bias input. The unit in the hidden layer 
had  one  input  from  the  visible  layer,  one  bias  input  and  one 
input  that  was  its  own  output  delayed  by  one  instant.  There 
was  one  input  to  and  one  output  from  the  network.  Hence 
there are in total 7 weights in the first, 12 in the second and 10 
in the last case. Each weight corresponded to one dimension. 
A  point  in  the  first  parameter  space  was  specified  by  7 
coordinates. Each coordinate was restricted to one of a set of 
discrete  values.  Range  spanned  by  all  of  the  coordinates  was 
restricted    to  a  subrange  of  [-2,  2).  For  different  coordinates, 
different  number  of  discrete  values  evenly  distributed  across 
the range were visited during the systematic exploration. 

13 

Figure  12:  Error  versus  output  layer  weights  with  hidden  and  input  layer 
weights corresponding to minimum error 

Figure 11: Predicted versus noise corrupted actual 

Figure 10: Minimum error (over all output layer weights) versus hidden layer 
weights for input weights (1.5, -1.5) 

For  the  first  two  coordinates,  corresponding  to  the  weights 
associated with the visible layer unit, typically a small number 
(e.g. 2), for the next 3 coordinates corresponding to the hidden 
layer,  a  medium  number  (e.g.  8)  and  finally  for  the  last  two 
coordinates corresponding to the output layer a larger (e.g 32) 
discrete  values  were  visited.  Example  number  of  values 
different  points 
mentioned  above  resulted  in
in the parameter space to be visited. 

For  each  of  the  64  vectors  of  values  of  two  of  the  three 
hidden  layers  weights  –  corresponding  to  the  recurrent  input 
and input from visible layer - minimum error was determined 
separately  for  each  of  4  values  of 
the  weight  pair 
corresponding to weights associated with the visible layer unit. 
These are plotted in Figures 8 through 11. 

Next with the vector of visible and the hidden layer weights 
set  to  the  vector  corresponding  to  minimum  error  determined 
above, the error at the 1024 output layer weight pairs is plotted 
in Figure 13. 

Figure 13: Predicted versus noise corrupted actual 

Figures  8  through  11  have  a  bearing  on  the  question  as  to 
whether  with  visible  and  hidden  layer  weights  initialized 
randomly  and  output  layer  weights  learnt  using  gradient-
descent-backpropagation-through-time  suffices  to  minimize 
error  to  arbitrarily  small  values.  These  present  empirical 
evidence  that  it  is  not  so.  Arbitrary  initialization  of  'lower' 
layers and only learning weights of higher layers lower bounds 
the error, such that smaller error cannot be achieved. 

Figure 12 has a bearing on the question whether with 'lower' 
layer  weights  set  correctly,  if  the  values  of  the  error 
corresponding  to  different  values  of  output  layer  weights  are 
considered to lie on a manifold, what does this manifold look 
like. As the output layer comprises of sigmoidal units as well, 
the manifold is non-convex. 

Simulation  study  was  conducted  with  synthetic  and  real 
data.  The  synthetic  data  was  the  Sine  and  Mackey  Glass 
function  generated  series.  In  case  of  the  sine  function 
generated  series,  the  run  was  conducted  upto 
. 
However during 
, prediction was performed but 
measurement  update  was  not  performed.  We  see  that  (Figure 
11)  predictions  match  the  true  data  well.  Even  in  case  of  the 

 
 
 
 
 
 
 
 
Mackey  Glass  function  generated  series,  which  is  not  simply 
periodic,  predictions match the true data (Figure 14). 

At  each  instant,  input  data  is  received,  output  is  estimated 
and,  if  actual  output  is  made  available,  the  parameters  are 
updated.  It  can  be  argued  that  with  input  with  stationary 
statistics,  if  parameters  do  not  attain  stable  values,  correct 
output  being  estimated  does  not  mean  that  input  to  output 
mapping  has  been  learnt.  As  the  plot  of  evolution  of  weights 
(Figure  14)  shows,  stable  values  are  attained  and  the  input-
output mapping is truly being learnt. 

The  second  set  of  simulation  experiments  are  with  real 
Electroencephalogram data collected at the Freiburg university 
[26]. There are 6 channels in all the recordings because of use 
of  6  electrodes.  Each  recording  has  been  examined  by  an 
expert encephalographer and occurrences of seizure identified. 
Each recording covers a nearly continuous length of time that 
included  several  Ictal  as  well  as  inter-Ictal  intervals.    To 
shorten  the  run-times,  middle  of  the  inter-seizure  intervals 
were  shortened  (e.g.  to  a  unit  variance  Gaussian  random 
number  with  a  mean  262144  samples  rather  than  original 
intervals  exceeding  921600  samples)  but  intervals  leading 
upto  and  the  seizure  intervals  were  included  unmodified.  For 
purposes  of  the  simulation  experiment,  for  each  recording, 
measurement  updates  were  performed  through  first  several 
seizures  (“training”  seizures),  measurement  updates  were 
stopped  sometime  before  the  occurrence  of  last  few  seizures 
(“test”  seizures)  and  seizure  prediction  was  compared  to 
determination  made  by  the  expert  encephalographer.  Table 
presents summary of these experiments. 

These set of experiments were repeated for prediction ahead 
into  future  by  different  amounts  of  time.  The  three  sets 
presented  in  the  table  corresponded  to  prediction  ahead  by  1, 
1025 and 2049 instants. High ADRs were achieved for each of 
these cases. To study whether, in spite of quantitatively similar 
performance  for  prediction  ahead  into  future  by  different 
amounts  of  time,  whether  performance  differed  qualitatively. 
Figure 16, Figure 17 and Figure 18 show close-up of when in 
network determination recordings transition from no seizure to 
seizure when predicting ahead by 1, 1025 and 2049 instants. 

Figure 14: Predicted versus noise corrupted actual data for the Mackey Glass 
series 

14 

Figure 15: Evolution of weights 

TABLE 1: SIMULATION EXPERIMENTS WITH EPILEPTIC SEIZURE DATA 

EXPERIMENT  RECORD 
NAME 
Pat003 
Pat003 
Pat003 

1 
2 
3 

RECORD 
LENGTH 
2361858 
2361858 
2302384 

PREDICT 
AHEAD 
1 
1025 
2049 

TRAIN 
SAMPLES 
2128306 
2128306 
2128304 

TEST 
SAMPLES 
233552 
233552 
174080 

TABLE 2 
REAL EPILEPTIC SEIZURE DATA TRAINING SEGMENTS 

EXPERIMENT 

TRAIN 
SEIZURES 

1 
2 
3 

3 
3 
3 

TRAIN 
SAMPLES 
MARKED 
SEIZURE 
111560 
111560 
111560 

EXPERIMENT 

1 

2 

3 

TABLE 3 
REAL EPILEPTIC SEIZURE DATA TEST RESULTS 

TRUE 
POSITIVE, 
FALSE 
NEGATIVE 
6224, 175  

SPC 

0.97 

6308, 91  

0.97 

6224175 

0.97 

TRUE 
NEGATIVE, 
FALSE 
POSITIVE 
227010, 
143 
227106, 
47 
167538, 
143 

SEN 

ADR 

1 

1 

1 

0.986011 

0.992786 

0.99 

The  blue  curve  corresponds  to  expert  determination  and  the 
green  curve  to  network  determination.  Similarly  Figure  19, 
Figure  20  and  Figure  21  show  close-up  of  when  in  network 
determination recordings transition from seizure to no seizure 
when predicting ahead by 1, 1025 and 2049 instants. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
15 

Figure  16:  1  instant  ahead  prediction  of  seizure  onset  –  predicted  (green) 
versus expert determination (blue) 

Figure  18:  2048  instants  ahead  prediction  of  seizure  onset  –  predicted 

(dots) versus expert determination (solid line) 

Figure  17:  1024  instants  ahead  prediction  of  seizure  onset  –  predicted 

(green) versus expert determination (blue) 

Figure 19: 1 instant ahead prediction of seizure subsiding 

Figure 20: 1024 instants ahead prediction of seizure subsiding 

 
 
 
 
 
 
 
 
 
 
(34) 

(35) 

(36) 

(37) 

(38) 

APPENDIX 

Output of visible node 

yvp

= f

⎛ 
∑
⎜ ⎜ 
⎝ 

q

wvpuq

˜ y uq

⎞ 
⎟ ⎟  
⎠ 

16 

(33) 

where 

 is set to 1, 

 is the 

th input and 

 is 

wvpuq

. Hence 

 to the visible node 

= ʹ′ f ' a( ) a=avp

the weight from input 
∂yvp
∂yuq
and 
∂yv p
∂wv p uq
Alternatively, above can be written in matrix notation as  

= ʹ′ f ' a( ) a= avp

yuq

. 

, 

= ʹ′ f  a( ) a=av

Wvu  and 

= ʹ′ f  a( ) a=av

. 

∂y v
∂y u
∂y v
∂Wvu
Now 

  is  the  vector  of  inputs  to  the  network, 

  is  the 

vector 

  with  a  1  as  the  additional  and  the  first  element 

(corresponding to the bias input), 

to  visible  layer  units  from  the  inputs, 

 is the matrix of weights 
ʹ′ f  a( ) a=av

  is  the 

(diagonal) matrix of derivatives of vector 

 wrt vector 

evaluated  at 

  and 

  is  the  matrix  of  partial 

derivatives  of  outputs  of  visible  layer  units  wrt  inputs. 
 is also a 

€ 
 wrt matrix 

, derivative of vector 

matrix  (and  not  a  3-dimensional  array)  as  derivative  of  the 
output  of  a  unit  in  a  layer  wrt  weights  to  other  units  in  the 
same layer are 0. 

Note  that  representing  the  set  of  derivatives  as  matrices  is 
more  than  just  a  matter  of  compactness.  Representing  as 
matrices  eases  implementation  on  scalar  processors  using 
mathematical  library  functions  by  reducing  the  overheads 
associated with library function calls and on vector processors 
such  as  GPUs  enables  gains  from  the  data  parallelism. 
However  in  the  rest  of  the  following  derivation,  for  ease  of 
understanding, we continue to use scalar notation and employ 
sum over elements of vector as necessary. 

Output of hidden layer 

 node 

yhNL , j

= f

⎛ 
N H
∑
⎜ 
⎜ 
⎝ 

p=1

whNL , j hNL , p

−

yhNL , p

−

NV
∑

p=1

+ whNL , j v p

yv p

⎞ 
⎟ 
⎟ 
⎠ 

(39) 

where 

  is  the  output  of  hidden  layer 

  node 

, 

  the  weight  from  hidden    layer 

  node 

  at  the 

previous  instant  to  hidden  layer 

  node 

  at  present 

instant, 

is the output of the hidden layer 

 node 

 at 

the  previous  instant,

the  weight  from  the  hidden  layer 

€ 

€ 

€ 

Figure 21: 2048 instants ahead prediction of seizure subsiding 

VI.  CONCLUSION 

€ 

€ 

We  presented  the  time-series  prediction  problem  and 
critically  discussed  several  aspects.  Some  of  the  aspects 
pertain  to  applications  with  stringent  requirements  such  as 
Epileptic Seizure onset prediction. We discussed suitability of 
Recurrent  Neural  Networks  for  time  series  prediction.  We 
discuss  an  architecture  for  Recurrent  Neural  Networks  that 
grows  along  three  dimensions  –  number  of  units  in  layers, 
number  of  hidden  layers  and  extent  in  number  of  instants  of 
back-propagation. We highlighted the similarity between RNN 
with greater than 1 instant back-propagation extent and higher 
order  Markov  processes.  We  prove  analytically  the  fact  that 
additional hidden layers improve the approximation error rate. 
Application  of  theory  of  back-propagation  gradient  descent 
leads  to  a  training  method  called  Gradient  Descent  Back-
propagation  Through  Time-and-Space.  We  derive  a  Dynamic 
Programming (DP) procedure employing matrix operations for 
the  training.  DP  and  use  of  matrix  operations  makes  the 
procedure  efficient  particularly  when  using  data-parallel 
libraries  and  on  data-parallel  or  scalar  architectures.  The 
simulations  studies  present  the  geometry  of  the  parameter 
space  and  verify  using  synthetic  data  that  the  network  learns 
the temporal structure in the data so that parameters converge 
while  model  output  displays  same  dynamic  behavior  as  the 
system. Simulation further showed the method to attain near-
perfect Average Detection Ratio on real epileptic seizure data. 
that  are 
enhancements  to  reduce  cost  of  software  and  Integrated 
Circuit  implementations  while  preserving  the  very  good 
performance  on  the  discussed  as  well  as  other  applications. 
The  enhancements  fit  number  of  hidden  layers,  number  of 
units  in  hidden  layers  and  extent  of  back-propagation  to 
problem  and  consider  local  minima  avoidance  in  gradient 
descent  training.  Applications  include  Natural  Language 
Processing,  Automatic  Speech  Recognition  and  music 
analysis.  

related  ongoing  work 

We  have 

several 

€ 

 
 
 
 
 
 
 
 
 
j   to  visible  node 

p  and 

yvp

  the  output  of  visible 

propagating 

T  instants, the derivative of final output 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

€ 

p . 

N L   node 
node 
Hence 
∂yhNL , j
€ 
∂yhNL , p
∂yhNL , j

−

∂w

−

hNL , j hNL , p

€ 

= ʹ′ f ' a( ) a= ahNL , j

€ 
whNL , j hNL , p

−

, 

= ʹ′ f ' a( ) a= ahNL , p

yhNL , p

−

 and 

= ʹ′ f ' a( ) a=ahNL , j

yvp

. 

∂yhNL , j
∂whNL , jv p
From (33) and (39), 
⎛ 
N H
∑
⎜ 
⎜ 
⎝ 

yhNL , j

= f

w

p=1

( −1) y

hNL , j hNL , p

( −1)
hNL , p

+ whNL , jv p

∑

NV

p=1

f

⎛ 
NI
∑
⎜ ⎜ 
⎝ 

q=1

wv p uq yuq

⎞ 
⎞ 
⎟ 
  (43) 
⎟ ⎟ 
⎟ 
⎠ 
⎠ 

Hence 
∂yhNL , j
∂wvpuq

= ʹ′ f  a( ) a=ahNL , j

whNL , jvp

ʹ′ f  a( ) a=avp

y uq

whNL , jv p

∂y vp
∂wvpuq
NV

= ʹ′ f ' a( ) a=ahNL , j
∂yhNL , j
∂y uq

= ʹ′ f  a( ) a=ahNL , j

∑

p=1

whNL , jvp

ʹ′ f  a( ) a=ahNL , p

wvpuq

NV
∑

whNL , jv p

p=1

= ʹ′ f  a( ) a=ahNL , j
Output of hidden layer 
⎛ 
NH
∑
⎜ ⎜ 
⎝ 

yhNL −1, j

= f

w

hNL −1, j hNL −1, p

−

∂y vp
∂y uq
N L −1, 

y

−
hNL −1, p

⎞ 
⎟ ⎟ 
+χ
⎠ 

hNL −1, jhNL , p

f ξ( )

y

−
hNL ,q

NV
∑

q=1

+ whNL , pvq

f

⎛ 
NI
∑
⎜ ⎜ 
⎝ 

r=1

wvqur

y ur

⎞ 
⎟ ⎟ 
⎠ 

p=1
€ 

where 
NH
∑

χ =

w

p=1
NH
∑

w

ξ =

−

hNL , phNL ,q

q=1
Hence 
∂yhNL −1, j
∂wvpur
= ʹ′ f  a( ) a=ahNL −1, j
NH
∑

.

∂y vq
∂wvpur
∂y hNL , p
∂wvpur

whNL −1, j hNL , p

ʹ′ f  a( ) a=ahNL , p

whNL , pvq

p=1

N H

∑

= ʹ′ f  a( ) a=ahNL −1, j

∂yhNL −1,j
∂wvpur
(23)  and  (26)  can  in  fact  be  generalized  so  we  may  compute 
the derivative of a unit output or a weight wrt to a unit output 
)  at  an  arbitrary  distance  away  in  terms  of  the 
(say 

whNL −1, j hNL , p

(48) 

p=1

yx1

derivative  of  the  same  weight  or  output  wrt  to  (an)  output(s) 
  and 
yx2
derivative  of  the  original  unit  output  or  weight  wrt 

1  away,  the  weight  connecting 

  distance 

  to 

yx2

yx1
yx2

.  For 
N L  hidden layers, we are back 

example in a network that has 

€ 

€ 

€ 

€ 

€ 

€ 

(40) 

€ 

(41) 

€ 

(42) 

(44) 

(45) 

(46) 

(47) 

17 

 wrt 
−T  to input layer 

yzi

€ 

(49) 

q  at instant 

the weight associated with input 
unit 

p is given by 
∂y zi

NH
∑
= f' a( ) a=azi
€ 
p=1

wzihNL , p

∂y hNL , p
∂w
€ 
− T(
vp

€ 
∂w

)

)

)

)

− T(
uq

− T(
uq

− T(
vp
Above  suggests  a  dynamic  programming  method  for 
computing  the  derivatives  comprising  of  traversing  the  graph 
from  inputs  to  outputs,  computing  derivative  of  unit  outputs 
wrt distance 1 away unit outputs, distance 1 away weights and 
distance  2  away  weights.  Derivatives  wrt  unit  outputs  at 
distance greater than 1 and weights at distance greater than 2 
are  computed  by  taking  weighted  sums  over  corresponding 
derivatives of distance 1 away unit outputs. The distances here 
are partially over space and partially over time. 
In  the  joint-state-and-parameter-vector 

,  the  individual 

layer unit outputs and unit input weights are assigned indices 
so  as  to  facilitate  computation  of  the  derivatives. 

are  the  current  final  outputs, 

current 

hidden 

layer 

  are  the 

unit 

outputs, 
  are  the  current 

,  instant 

visible  layer  unit  outputs.  Current  outputs  are  followed  by 
instant 
  is 
,  …,  instant 
the  extent  of  back-propagation.  Weights  follow  the  unit 
outputs.  Current  weights  associated  with  output  layer  units 
occupy indices 

  outputs  where 

 those with hidden layers 

, 

(50) 

and finally those with the input layer 

. 

 As with outputs, current weights are followed by instant 
through instant 

 weights. 

(51) 

(52) 

instant 

Before computing new current outputs, the current through 
 through instant 
  outputs.  Similarly  before  updating,  the  current  through 
  through 

  weights  are  copied  to  instant 

 outputs are copied to instant 

instant 
instant 

 weights. 

However  significant  time-efficiency  is  achieved  by  noting 
that no actual copying needs to be performed. The unit outputs 
and  weights  sections  are  treated  as 
  (instants)  long 
circular  buffers  and  the  current  instant  pointer  is  simply 
advanced one instant! 

The  row  and  column  indices  of  matrices  of  derivatives  of 
unit  outputs  and  weights  wrt  upstream  unit  outputs  and 
weights  are  also  similarly 
incremented  circularly  when 
advancing  to  next  instant.  This  makes  it  unnecessary  to 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
physically  copy  weights.  As  operations  of  matrix-vector 
product  and  vector-vector  inner  product  involves  summing 
over  products  of  pairs  of  elements  and  the  sum  operation  is 
order-independent, the matrix operations are not affected. This 
makes  the  time-complexity  when  using  back-propagation 
extent greater than 1 same as the time complexity when using 
back-propagation extent equal to 1. 

With 

NO   long  network  outputs  vector 

y z   and  the 
derivatives  of  weights  wrt  network  outputs  arranged  in  a 
the  updated  weights  are  given 
NO × NW   matrix 
by
€ 

J , 
). 
w ← w −ηJ ˆ y z − y z
Thus,  the  space  complexity  is 

.  With 

€ 

(

€ 

€ 

€ 

  a  constant  independent  of  network  dimensions,  this  is 
linear in network dimension. With access to previous outputs 
  time,  the  time-complexity  of 
and  weights  requiring 

computations  at  each  instant  is 

  under  normally 

valid assumptions of 

 and 

. 

ACKNOWLEDGMENT 

Piyush was a summer intern at the Nano-Electronics Research 
Laboratory  in  the  Electrical  Engineering  Dept  of  Purdue 
University  for  a  part  of  this  work  and  is  grateful  for  support 
and  encouragement  received  from  Prof  K.  Roy  and  Dr  S. 
Raghunathan.  

REFERENCES 

[1] 

I.  Arasaratnam  et.  al.,  “Discrete  Time  Nonlinear  Filering  Using  Gauss-
Hermite  Quadrature,” Proc. of IEEE,    vol. x, no. x, pp. 953-977, May, 
2007. 

[2]  Andrew  S.  Barron, 

for 
Superpositions  of  a  Sigmoidal  Function,”  IEEE  Transactions  on 
Information Theory, vol. 39(3), pp. 930-945, 1993. 

“Universal  Approximation  Bounds 

[3]  Belhouari S. B. et. al., “Gaussian Process for Nonstationary Time Series 
Prediction,” in Computational Statistics and Data Analysis, xth ed. City 
of Publisher, Country if not USA: Elsevier, 2004, ch. x, sec. x, pp. xxx–
xxx. 

[4]  Bengio  Y.  et.  al.,  “Deep  Learning  of  Representations  for  Unsupervised 
and  Transfer  Learning,”  Journal  of  Machine  Learning  Research  (Proc. 
Unsupervised  and  Transfer  Learning  Challenge  and  Workshop),  Vol. 
27, 2012. 

[5]  R.  van  der  Merwe  and  E.  Wan,  “Sigma-point  Kalman  filters  for 
probabilistic  inference  in  dynamic  state-space  models,”  in  Proceedings 
of  the  Workshop  on  Advances  in  Machine  Learning,  City  of  Conf., 
Abbrev. State (if given), 2003, pp. xxxxxx. 

[6]  R.  van  der  Merwe,  “Sigma-point  Kalman  filters  for  probabilistic 
inference  in  dynamic  state-space  models,”  Ph.D.  thesis,  OGI  School  of 
Science  &  Engineering,  Oregon  Health  &  Science  University,  City  of 
Univ., Portland, OR, USA, April 2004. 

[7]  Duchi  et.  al.  ,  “Ergodic  Subgradient  Descent,”  Allerton  Conference  on 

Communication, Control and Computing, 2011. 

[8]  Gallant A. R. et. al., “There exists a neural network that does not make 
the  Second  Annual  IEEE 

avoidable  mistakes,”  Proceedings  of 
Conference on Neural Networks, Vol 2, I.657 – I.664, 1988. 

[9]  Greene B R et al, “A comparison of quantitative EEG features for neo-

natal seizure detection,” Clinical Neurophysiology, 2008. 

[10]  Hornik  K.  et.  al.,  “Universal  Approximation  of  an  Unknown  Function 
and  its  Derivatives  using  a  Multilayer  Feedforward  Networks,”  Neural 
Networks, Vol 3, 550-560, 1990. 

[11]  A.  H.  Jazwinski,  “Stochastic  Processes  and  Filtering  Theory,” 

Academic Press, 1970. 

[12]  S.  J.  Julier,  J.  K.  Uhlmann,  and  H.  F.  Durrant-Whyte,  “A  new  method 
for the nonlinear transformation of means and covariances in filters and 

18 

estimators,”  IEEE  Transactions  on  Automatic  Control,  vol.  45(3),  pp. 
477–482, March 2000. 

[13]  Kimball  B.  F.,  “Note  on  Computation  of  Orthogonal  Predictors,”  The 

Annals of Mathematical Statistics, Vol. 24, No. 2, 1953. 

[14]  F.  Le  Dimet  et.  al.  “Second  Order  Information  in  Data  Assimilation,” 
Journal of the American Metrological Society, Vol. 130, No. 3, pp 629-
648, March 2002. 

[15]  Loh  P.-L.  et.  al.  “High-dimensional  regression  with  noisy  and  missing 
at 

data:  Provable  guarantees  with  non-convexity,” 
http://arxive.org/abs/1104.4824, 2012. 

available 

[16]  Niedermeyer  E  et.  al.,  “Electroencephalography:  Basic  Principles, 
Clinical  Applications,  and  Related  Fields,”  Liipicott  Williams  & 
Wilkins, 2004. 

[17]  Raghunathan  S  et  al,  “  A  hardware-algorithm  co-design  approach  to 
optimize  seizure  detection  algorithms  for  implantable  applications,” 
Elsevier Journal of Neuroscience Methods, 2010. 

[18]  Rhew H-G et al, “A Wirelessly Powered Log-based Closed-Loop Deep 
Brain  Stimulation  SOC  with  Two-Way  Wireless  Telemetry  for 
Treatment  of  Neurological  Disorders,”  Symposium  on  VLSI  Circuits, 
2012. 

[19]  F. Sandblom et. al., “Marginalized Sigma Point Form,” , 2011. 
[20]  Sha  H  et  al,  “A  micro-controller-based  implantable  nerve  stimulator 
used  for  rats,”  IEEE  Engineering  in  Medicine  and  Biology  Society 
Conference, 2006. 

[21]  Subasi  A.,  “Classification  of  EEG  signal  using  neural  network  and 
logistic regression,” Computer Methods and Programs in Bio-Medicine, 
Vol. 78, No. 2, 2005. 

[22]  Sudderth  E.  B.  et.  al.  “Nonparametric  Belief  Propagation,” 
Communications  of  the  ACM,  Vol.  53,  No.  10,  pp  95-103,  October 
2010. 

[23]  Sun  F.  T.  et.  al.,  “Responsive  Cortical  stimulation  for  the  treatment  of 

epilepsy,” Neurotherapeutics, Vol. 5, 2008. 

[24]  Werbos P. J., “Backpropagation Thru Time: what it does and how to do 

it,” Proceedings of the IEEE, Vol.78, No. 10, 1990. 

[25]  Yeager  D  et  al.  “A  Fully-Integrated  10.5  uW  Miniaturized  (0.125mm2) 

Wireless Neural Sensor,” Symposium on VLSI Circuits, 2012. 

[26]  Seizure  Prediction  Project,  http://epilepsy.uni-freiburg.de/friburg-

seizure-prediction-project. 

Sharat  C  Prasad  (M’87)  received  the  B.Tech.  degree  in 
Electronics  and  Communication  Engineering  from  I.I.T. 
Kaharagpur,  India  in  1983,  M.E.  in  Computer  Science  and 
Automation from I.I.Sc. Bangalore India in 1987 and Ph.D. in 
Electrical  Engineering  from  University  of  Texas  at  Dallas  in 
2005. 

He  was  an  Intern  at  the  Tata  Institute  of  Fundamental 
Research  during  the  summer  of  1982,  worked  at  the  Indian 
Subsidiary  of  International  Computers  Limited,  U.K.  from 
1983 until 1985, at Texas Instruments from 1987 until 1997, at 
Cisco Systems from 1998 until 2006, at Google Inc from 2007 
until 2012 and is with Best3 Corp since 2012. 
  Dr Prasad is a member of IEEE and ACM. He has authored 
or  co-authored  one  book,  over  5  journal,  over  10  conference 
and over 30 patents. 

to 

Piyush  Prasad  was  born  in  Plano,  Texas,  in  1995.  He  is 
expecting 
in  Biomedical 
receive  his  B.S.  degree 
Engineering  from  Washington  University,  St  Louis,  MO  in 
2017. 
    He was a summer intern at the Nano-electronics laboratory 
of Dept of Electrical Engineering at Purdue University. 

Mr  Prasad  was  a  recipient  of  several  awards  at  a  national 

Robotics league and is the co-author of a patent. 

 
 
 
 
 
 
 
 
 
19 

 
 
 
    
    
 
 
