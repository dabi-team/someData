CCTEST: Testing and Repairing Code Completion
Systems

Zongjie Lia, Chaozheng Wangb, Zhibo Liua, Haoxuan Wangc, Shuai Wanga, Cuiyun Gaob
a The Hong Kong University of Science and Technology, Hong Kong SAR
b Harbin Institute of Technology, Shenzhen, China
c Swiss Federal Institute of Technology Lausanne, Switzerland
{zligo,zliudc,shuaiw}@cse.ust.hk, {wangchaozheng}@stu.hit.edu.cn
{gaocuiyun}@hit.edu.cn, {haoxuan.wang}@epﬂ.ch

2
2
0
2

g
u
A
7
1

]
E
S
.
s
c
[

1
v
9
8
2
8
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Code completion, a highly valuable topic in the
software development domain, has been increasingly promoted
for use by recent advances in large language models (LLMs).
To date, visible LLM-based code completion frameworks like
GitHub Copilot and GPT are trained using deep learning over
vast quantities of unstructured text and open source codes. As
the paramount component and the cornerstone in daily program-
ming tasks, code completion has largely boosted professionals’
efﬁciency in building real-world software systems.

In contrast to this ﬂourishing market, we ﬁnd that code
completion models often output suspicious results, and to date,
an automated testing and enhancement framework for code
completion models is not available. This research proposes
CCTEST, a framework to test and repair code completion
systems in blackbox settings. CCTEST features a novel mutation
strategy, namely program structure-consistency (PSC) mutations,
to generate mutated code completion inputs. Then, it detects
inconsistent outputs, representing likely erroneous cases, from all
the completed code cases. Moreover, CCTEST repairs the code
completion outputs by selecting the output that mostly reﬂects the
“average” appearance of all output cases, as the ﬁnal output of
the code completion systems. We detected a total of 33,540 inputs
that can trigger likely erroneous cases from eight popular LLM-
based code completion systems. With repairing, we show that
the performance of code completion models notably increased
by 53.51% on average.

I. INTRODUCTION

Large language models (LLMs) such as GitHub Copi-
lot [23], OpenAI’s Codex [1] and Tabnine [5] are increasingly
being promoted for use within the software development
domain. Such models are built using machine learning (ML)
over vast quantities of unstructured text, including websites,
books, and open source codes. This enables them to produce
“completions” given some input prompt made up of code
and comments (documentation). To date, de facto LLM-based
code completion frameworks are advocated with the aim to
provide an “AI pair programmer” capable of automatically
generating programs from natural language speciﬁcations or
code snippets.

Despite being the highly powerful and promising component
in augmenting modern software development, our observation
is that code completion systems are not perfect, and they
frequently generate confusing and likely erroneous results.
We argue that the “sub-optimal” and even buggy behavior
of code completion systems are undesirable, undermining the

Fig. 1. Motivating example over Codegen [42].

performance and usability of code completion. Nevertheless, it
is yet neglected by today’s research community, whereas pre-
liminary studies on LLM-based code completion frameworks
are primarily on its security implication, cost reduction, or
potential extension in different domains [68], [46], [44], [29],
[50].

Considering Fig. 1(a), where Codegen generates a code
snippet as the completion of the prompt input. Moreover, by
slightly tweaking the prompt, we ﬁnd that Codegen outputs a
dramatically different code snippet, as illustrated in Fig. 1(b).
While it is generally obscure to directly decide the “correct-
ness” of these two completed code snippets, given that the
two inputs are (semantically) similar to a human observer, the
high distinction of two completed code snippets indicates that
Codegen’s outputs are of low consistency, which is a sign of
erroneous outputs. Overall, this obvious inconsistency in the
generated code completions motivates us to test and repair
code completion systems. Thus, our intuition is to form a
testing oracle, with the aim of checking the structure-level
consistency in completed code snippets.

This paper introduces CCTEST, an automated testing and

repairing framework for code completion systems.

Our research thrusts are two-fold. First, we design a set
of program mutation strategies, namely program structure-
consistency (PSC) mutations, to generate mutated code snip-
pets with similar or identical program structures. Given the
corresponding set O of code completion outputs for each
(mutated) code snippet, we identify erroneous cases (i.e.,
outliers) in O by deﬁning and comparing program distances
of generated code completions. The mutated program whose

deffind_top_k(data_list,TOP_K):length = len(data_list)begin = 0end = length-1index= divide(data_list,begin,end)ifindex == -1:return-1else:returnindexdeffind_top_k(data_list,K):length = len(data_list)begin = 0end = length-1index = divide(data_list,begin,end)ifindex == K:returndata_list[index]elifindex < K:returnfind_top_k(data_list,K)else:returnfind_top_k(data_list,K)(a)completion result with original prompt.(b) completion result when aparameter issubstitutedtogenerate the new prompt. 
 
 
 
 
 
derived code completion output has long distances compared
to other mutated completion outputs will be deemed as spu-
rious. We further design a code repairing scheme, forming a
post-processing process to repair the code completion outputs
by selecting the code completion output ˆo that mostly reﬂects
the “average” appearance of O. We show that ˆo generally
manifests high consistency and quality with the reference
inputs, extensively improving the accuracy of code completion
systems. Furthermore, our testing and repairing scheme treats
the code completion models as a “black box”, and we do
not assume any speciﬁc implementation details of the code
completion systems or their underlying LLMs.

CCTEST offers an up-to-date assessment of de facto LLM-
based code completion systems and the correctness of their
outputs, which may impede reaching the full potential of
modern “AI pair programmer” in software development. From
a total of 182,464 test inputs used for this study, we found
33,540 programs exposing code completion errors from eight
widely-used LLM-based code completion systems, one of
which (Copilot) is a popular commercial tool, and the other
seven are either actively developed and maintained by the non-
proﬁt organization (CodeParrot [18]) or hosted by the indus-
trial companies (EleutherAI’s GPT-Neo [8] and Salesforce’s
the average
Codegen [42]). With repairing, we show that
performance of code completion models notably increased by
40.1% and 67 % on two metrics. In sum, this research makes
the following contributions:

• To study LLM-based code completion systems in a real-
istic setting and to delineate their up-to-date capabilities,
we introduce and advocate a new focus, conducting
comprehensive and large-scale testing and repairing on
their outputs. The ﬁndings obtained in this study will
guide future research that aims to use and improve code
completion tools.

• We propose CCTEST, an automated testing and repair-
ing framework for code completion systems. CCTEST
features PSC, a novel testing scheme that is particularly
designed to test code completion systems. We further
propose a black-box repairing scheme to augment the ac-
curacy of code completion outputs. CCTEST incorporates
a set of design principles and optimizations to deliver an
efﬁcient testing and repairing workﬂow.

• Empirically, we evaluate one commercial with seven pop-
ular free code completion systems, and we successfully
found 33,540 programs causing code completion errors.
Our repairing scheme further enhances the accuracy of
the code completion models with much higher accuracy.
We made various observations and obtained inspiring
ﬁndings regarding modern LLM-based code completion
systems.

II. PRELIMINARY

This section introduces the background of code completion
systems to deliver a self-contained paper. Note that the auto-
mated testing and repairing pipeline shipped by CCTEST treats
each code completion system as a black box, and we do not

Fig. 2. A holistic view of LLM-based code completion system and how
CCTEST facilitates testing and repairing of them.

assume a speciﬁc internal. Nevertheless, given the dominating
usage of LLMs in forming today’s code completion systems,
we primarily introduce LLM-based code completion models.
Our evaluations are accordingly designed to test LLM-based
code completion models.

Fig. 2 presents a holistic view of the LLM-based code com-
pletion system, and explains how CCTEST ﬁts the workﬂow
to test and repair a code completion system. Beneﬁt from
the prosperous development and major success of transformer-
based natural language models such as OpenAI’s GPT2 and
GPT3 [31], [13], [9], the code completion task, as a typical
conditioned open-ended language generation task, are exten-
sively improved with much higher accuracy.1

Though details of training datasets are often obscure, mod-
ern LLMs-based code completion systems are advertised as
being trained with over millions or even billions of lines of
code [23]. Typically, the input training data are dissected into
sentences and further into tokens, where each token comprises
a sequence of characters before being fed to LLMs. For
instance, the tokenizer of Codex and GPT-3 is conﬁgured to
produce tokens with an average of four characters. Tokens are
encoded with unique numeric identiﬁers, up to user-deﬁned
vocabulary size. This process is often referred to as byte pair
encoding (BPE) [20], allowing the models to ingest more text
into their (ﬁxed-size) input window. Various techniques have
been proposed to conﬁgure and improve the performance of
LLMs, such as learning from rare tokens and deciding a proper
stop word.

During the query phase, the code completion system’s input
is often referred to as a prompt, denoting an incomplete code
snippet. Similarly, the prompt code snippet is ﬁrst abstracted
into sentences and further into tokens, for which the code
completion system can predict a list of code completion
suggestions (ranked by the conﬁdence scores) that mostly
likely continue/complete the input prompt. For instance, the
code completion system is frequently assessed by completing
a function body, given the function prototypes and some
statements in the function prologue. Note that modern code
completion systems can process prompts of different types,

1A full introduction of transformer models and how it is trained to boost
open-ended language generation is beyond the scope of this paper. Interested
audiences can refer to [55].

LLM-based Code Completion SystemsTraining DatasetSentences/TokensabstractiontrainingTraining PhaseQueryPhasePromptSentences/TokensabstractionLLM LookupCCTestmutateCandidate completionsselect (to repair)(to test)including code snippets and natural
language descriptions
(comments) of the expected program. For natural language
sentences, they are usually divided into background, input and
output to describe a competitive programming problem [33].
CCTEST is designed to test the code completion system by
mutating the prompts into a variant set O and identifying bug-
triggering prompt variants using an implicitly-formed oracle.
Moreover, CCTEST can repair the code completion output,
by analyzing outputs O of prompt variants to select cases that
are most close to the “average” appearance of O. We now
introduce the workﬂow of CCTEST in Sec. III.

III. APPROACH OVERVIEW

Fig. 3 presents an overview of CCTEST in terms of testing

and repairing code completion systems. In particular,
(cid:172) Prompt Variants Generation. CCTEST launches PSC
and generates a set of structural-consistent variants P of an
input prompt p0. Here, we propose a novel testing scheme,
PSC,
that mutates a prompt with code structural-invariant
transformations. The mutated prompts manifest closely related
structural representations from a human observer’s perspective.
(cid:173) Testing Oracle Generation. The target code completion
model outputs a set of code completion outputs in accordance
with the prompt variants. Here, we form a testing oracle by
comparing the structural-level consistency (similarity) among
completion outputs. The key observation is that, for prompt
variants with closely similar structures, the code completion
outputs should consistently manifest high (visual) consis-
tency. Thus, “outlier” code completion outputs are deemed
erroneous, whose corresponding prompt variants are defect-
inducing inputs.
(cid:174) Inconsistent Completion Repairing. We note that the
aforementioned testing pipeline can be extended to automati-
cally repair the inconsistent code completion outputs. To this
end, CCTEST identiﬁes output ˆo that is mostly close to the
“average” appearance of O (after excluding outliers in (cid:173)).
Note that this step is also applicable to black-box models.
Study Scope. We primarily target the erroneous code comple-
tion outputs, denoting “stealthy logic bugs” of code completion
systems. As detailed in Sec. III-B, our testing approach can
automatically expose inconsistency defects in code comple-
tion systems, meaning that when given a set of structurally-
consistent prompts,
the completion outputs are deemed to
share closely similar appearances as well.

During preliminary study, we also ﬁnd that when feeding
a set of structurally-consistent prompts to code completion
systems, certain prompt variants may somehow impede the
code completion systems from generating any completion
this is comparable to identifying
outputs. In some sense,
a “crash” of the code completion system. Considering that
fuzzing or random testing may likely trigger such salience
states during in-house development, they are not the primary
focus of CCTEST. Nevertheless, we still record and report all
such defects we encountered during the evaluation.

Note that we are not using extreme (broken) prompts to
stress code completion systems. Modern code completion

models are on the basis of LLMs, and our preliminary study
shows providing some trivial, broken prompts may make
the code completion system to generate meaningless outputs.
The current implementation of CCTEST supports to generate
syntactic valid Python code snippets as prompts for testing,
given the popularity and representativeness of the Python
language. Nevertheless, it is easy to see that our method is
not limited to Python code; we leave extending CCTEST to
support other programming languages as one future work.

A. Program Structure-Consistency (PSC) Mutations

We ﬁrst introduce the PSC mutation scheme, which per-
forms structural-consistent mutations over an input prompt.
Overall, our key observation is that programs with similar/i-
dentical control structures will retain such consistency in the
corresponding completion code. Hence, by observing cer-
tain code completion outputs manifesting inconsistent control
structures, potential code completion errors can be ﬂagged.

if not

Fig. 3 depicts the workﬂow of PSC testing. Starting from
an input prompt p0, we conduct a two-step approach to 1)
performing structural-consistent mutation to generate a set
of mutated prompts with similar,
identical, control
structures and collect the code completion output ok from each
mutated prompt pk, and 2) employing a testing oracle to cross
compare the similarity of the code completion outputs among
ok and identify outliers. We identify outlier oi such that its
control structure has an abnormally long distance compared to
the others; these outliers are deemed as wrong code completion
errors. In this section, we detail the design of PSC code
mutations.

To generate structural-consistent mutation, one can replace
one code fragment s0 (e.g., a statement) in the seed prompt
p0 with similar fragments using one PSC transformation and
generate one mutated prompt pk. The mutated prompt
is
expected to retain program structures compared to p0.

TABLE I
TRANSFORMATION PASSES IMPLEMENTED IN CCTEST.

Class

Identiﬁer
Level

Instruction
Level

Block
Level

Methods
rename parameter regulate
rename parameter context
rename local variable regulate
rename local variable context
instruction replacement
replace bool expression
garbage code insertion regulate
garbage code insertion context
print statement insertion

Abbreviations
REP R
REP C
REL R
REL C
IRR
RTF
GRA R
GRA C
INI

1) Mutation Schemes: At this step, we implement a set of
PSC mutations, which perform transformations on different
levels of the program hierarchical structure, ranging from
identiﬁers, instructions, to basic blocks. Table I lists all the
PSC mutations adopted in this work. We now introduce each
mutation below. Note that due to the limited space, here we
only discuss the design and implementation essential.
Design Goal. Overall, the mutation scheme is designed as
incremental and retains the structural-level representation of

Fig. 3. Workﬂow of CCTEST. CCTEST launches PSC testing by mutating a prompt into a collection of prompt variants, cross compare code completion
results, and enable automated code completion repairing.

the mutated prompts. However, we make an encouraging ob-
servation that such straightforward mutation imposes notable
challenges for code completion systems to comprehend the
inputs and accordingly generate consistent completion outputs
over prompts and the mutated variants; see sample ﬂaws in
Sec. V-B.
REP R & REP C. Our ﬁrst mutation scheme performs
renaming toward function parameters. Overall, this pass will
replace one parameter in the function’s parameters list with
a new identiﬁer, and every usage of this parameter will
be replaced accordingly. The way of naming can either be
“regulate” or “context”. Considering the following case:

operations into its semantics equivalent expressions, but with
different syntactic forms. Considering the following case,

where the += statement is replaced with a standard addition
expression. Overall, we implement 4 replacement rules over
different common arithmetic expressions at this step.
RTF. In addition to mutating arithmetic expressions, we also
implement RTF schemes to mutate boolean expressions, par-
ticularly expressions used in forming branch conditions, with
its semantics equivalent versions. Considering the following
case:

where for the “regular” scheme (REP R), we replace the
function parameter b with Param1. As REP C which takes
the speciﬁc context
into account, we extend the function
parameter b with a new identiﬁer that subsumes both function
name add and b.
REL R & REL C. In addition to mutating the function
parameters, we also propose another mutation scheme at the
identiﬁer level to rename local variables. Similar to replacing
parameters, this pass will randomly select a local variable
whose scope is in the function. Then, we replace all of its
references with a new identiﬁer. Considering the following
case:

where the boolean expression is extended with an always-true
condition. This would not alter the functionality, nor does it
largely change the program structures. However, we ﬁnd that
such schemes can effectively impede code completion systems
from generating structural-consistent outputs, as shown in
Sec. V.
GRA R & GRA C. This mutation inserts a small chunk
of “garbage code” into the program, which does not alter
the program semantics, but slightly increases the program’s
control ﬂow complexity. In particular, we use always-false
conditions to form an if condition, where we insert a small set
of statements into the newly formed branch, which will never
be executed. Similar to mutations mentioned above, creating
garbage code may take context
information into account.
Considering the following case:

where for REL R, we replace the local variable res with
LocalVar1. As for REL C which takes the context infor-
mation into account, we rewrite the local variable res with
a new identiﬁer that subsumes both function name compare
and res.
IRR. This scheme implements a set of mapping rules, such
that it will search and replace certain common arithmetic

where GRA R inserts a branch whose if condition expression
is “False” with a new variable named TempVar. As for
GRA C that takes context into consideration, we create an

Promptp0mutatedpromptp2mutatedpromptp1mutatedpromptpkcompletion outputo1completion outputo2completion outputok…codecompletionsystemsPSCmutationsoutlierdetectionselect best completionrepaired completion outputo"…completion outputo0error-triggeringinput// seed promptdefadd(a,b):res = a+bdefadd(a,Param1):// REP_Rres = a+Param1defadd(a, Add_Param_b):// REP_Cres = a+Add_Param_b// seed promptdefcompare(a,b):res = a > bdefcompare(a,b):// REL_RLocalVar1= a>bdefcompare(a,b):// REL_Ccompare_res = a>bdefaddassign(a,b)://seeda += breturnadefaddassign(a,b):// IRRa = a+breturnadefadd (a,b,ignore):// seedifignore:returnadefadd (a,b,ignore):// RTFifignore == (b==b):returna// seed promptdefadd(a,b):res = a + b defadd(a,b):// GRA_Rif(False):TempVar= ares = a+bdefadd(a, b):// GRA_Cif(b!=b):Add_TempVar= ares = a+balways-false condition on the basis of the syntactic form of
the function parameters, and use it to form the if condition
expression. Similarly, variable names in the enclosed branch
also subsume both function name and TempVar.
INI. The last mutation pass inserts a “print” statement into the
prompt. Overall, this scheme is designed based on the obser-
vation that programmers often insert such “print” statements
during the debugging process, such that they print the status of
some variables to help debugging. See the following example:

where we insert print into the prompt. Though this scheme
does not change the program structures, we ﬁnd that it effec-
tively improves the performance of popular code completion
systems; see details in Sec.V-C.
Alternative: Mutating Natural Language Comments. It is
worth noting that besides processing prompts in the source
code form, modern code completion systems can also generate
code completions given prompts in natural language sentences.
In such cases, the input sentences are usually code comments
or descriptions of the intended code functionality. Careful
audiences may wonder about the feasibility of mutating natural
language comments to test code completion models, whose
expected workﬂow may be similar to recent advances in testing
machine translation systems [52], [54]. In fact, we tenta-
tively explored this direction. We clarify that unlike machine
translation system testing, whose inputs are arbitrary natural
language sentences, code comments are often in limited forms,
and a vast majority of code comments have no adjectives [47],
[45]. This imposes a major difﬁculty in comprehensively
mutating “synonyms” in code comments for testing purposes.

B. PSC Testing: Forming Testing Oracles

Given a list of code completion outputs O in accordance
with the mutated prompts P, we compare each output oi ∈ O
with the remaining cases in O and decide if it is an “outlier.”
The full algorithm is shown in Algorithm 1. We ﬁrst iterate
each case in O, decide its similarity with the remaining
cases (lines 2–3), and normalize the scores (line 4). Here, we
compute the similarity score using the Levenshtein distance
implemented by fuzzywuzzy, a common algorithm to decide
the edit distance among two programs. Then, we employ a
threshold T to decide whether a case exhibits anomalously
low similarity with other cases for more than T times (lines
5–12). and if so, the case will be deemed as an outlier. The
corresponding pi deems as an error-inducing input. Overall,
given that we have implemented N (N = 9 in the current
implementation of CCTEST) passes to mutate a prompt, T is
a conﬁgurable hyper-parameter (T ≤ N ), such that a code
completion output deems an outlier, if its distance with T
code completion outputs are longer than the average distance
of code completion output pairs.

T will be decided with empirical evidences, as will be
discussed in Sec. V-B. After performing Algorithm 1, we keep

the remaining code completion outputs O∗ = O \ L for usage
in the repairing phase, as will be explained in Sec. III-C.

Algorithm 1 Outlier Selection Algorithm

for j in i to k do ScoreM atrix[i][j] = Sim(oi, oj)

Input: O: Code completion output set of size k
Input: T : threshold
Output: L: Outliers
1: ScoreM atrix = []
2: for i in 1 to k do
3:
4: N ormalize(ScoreM atrix)
5: for i in 1 to k do
count = 0
6:
for j in 1 to k do
7:
8:
9:
10:
11:
12:
13: return L

count++
if count >= T then
L.append(oi)
break

if ScoreM atrix[i][j] < M edian(ScoreM atrix) then

Fig. 4. Simply matching code completion reference appears to be overly strict
and lead to false positives.

Alterantive Testing Oracles. This section primarily focuses
on the establishment of an implicit testing oracle, by com-
paring the consistency among code completion outputs O;
outliers will be deemed as errors, given their corresponding
input prompts are visually similar under our carefully designed
PSC schemes. Given that said, careful readers may wonder
about the feasibility of forming alternative testing oracles,
which may be more obvious. Below, we discuss two alternative
testing oracles and analyze their (in-)capability in our research
context.
Comparing Syntactic Form According to Reference.
One
straightforward way of forming a test oracle is to cut a
reference program p into two chunks p1 and p2, whereas
p1 serves the prompt, and p2 serves the code completion
reference; we decide the error of code completion model by
comparing its completion output o1 with p2.

We clarify that this oracle is often too strict. Fig. 4 compares
the code completion output of Copilot and the reference code
snippet (this code snippet is from LeetCode). The reference
code snippet leverages a helper function inorder to deliver
a classic inorder traversal of the input binary tree. Copilot’s
completion output appears to present a code completion output
on the basis of recursive calls. Overall, though the code com-
pletion output appears (visually) distinct with the reference
code snippet, the code completion output is functional valid.

// seed promptdefadd(a,b):res = a + bdefadd(a,b):// INIprint(b)res = a + b(a)A(simplified)LeetCodeprogramsnippet.(b)Copilot’scompletion output w.r.t.the promptingrey.definorderSuccessor(root, p):ifnotroot:returnNonedefinorder(node):ifnotnode:return[]returninorder(node.left) \+ [node] + inorder(node.right)l = inorder(root)i= l.index(p)returnl[i+1] ifi<len(l)-1elseNonedefinorderSuccessor(root, p):ifnotroot:returnNoneifp == root:returnrootelifp.left:returnself.inorderSuccessor(p.left,p)else:returnself.inorderSuccessor(p.right,p)We believe that strictly comparing the syntactic equality
of code completion outputs with the reference code, though
straightforward, may not necessarily and faithfully reﬂect the
true defects of code completion systems. In other words, such
a strict comparison may induce an extensive amount of false
positive cases.
Checking Functionality According to Reference. To form the
testing oracle for code completion systems, recent works have
also explored strictly checking the functionality correctness
of code completion outputs in accordance with the reference
code snippet. In particular, [41] forms the prompt using
function prototypes and some code snippets from leetcode
to Copilot, and counts the number of passed test cases over
the code completion outputs (when being used together with
the prompts). More passed test cases indicate that the code
completion outputs are of higher quality. We admit that this
approach is more ﬂexible than the aforementioned alternative,
given that it allows some (syntactic) changes in the output,
as long as it does not alter the functionality. Nevertheless,
we respectively argue that this may be less desirable than our
formed oracle. In principle, modern code completion systems
are not championed to replace human programmers; rather,
it aids human programmers with suggested code completion
outputs that are useful. In other words, we believe strict
functionality accuracy may not be the ﬁrst design choice for
a code completion system (e.g., some sloppy arithmetic errors
in completion outputs may be easily ﬁxed by the users in a
post-process phase). Thus, we believe it is more desirable to
cross compare the structural-level consistency (as in CCTEST),
rather than strictly comparing the functionality correctness.

C. Repairing

In this section, we present the technical pipeline of repairing
the code completion outputs. With the testing launched in
Sec. III-B, we have collected a set of code completion outputs
O∗ with outliers being excluded. At this step, we aim to
identify a code completion output that appears mostly close to
the “average appearance” of O∗. To this end, we measure the
average pair-wise edit distance ˆd of every oi, oj ∈ O∗. Then,
we search for an ˆo ∈ O∗, whose average pair-wise distance
with all other elements in O∗ is the closest to ˆd. This ˆo will
be the repaired code completion output returned to the users.

IV. IMPLEMENTATION AND EVALUATION SETUP

CCTEST is implemented in Python, with about 5k LOC.
CCTEST is currently implemented to mutate Python code,
given the popularity of this language in software development
and the matureness of corresponding code completion systems.
We now discuss the implementation and evaluation setup.
Parsing and Mutating Programs. We parse Python code with
tree-sitter [10], a mature and dependency-free parser generator
tool with an open-source license. It is widely used in code-
related projects such as Codebert [19], and it does not require
the input code to be executable, meaning that incomplete code
fragments without a building script can also be parsed. We
ﬁrst parse the prompt code into a concrete syntax tree, and

conduct several sanity checks (see in Sec. V-A) on the target
code to see which transformation pass could be performed.
Then, after applying feasible transformations, CCTEST will
output the corresponding transformed code with IDs of the
applied transformation passes.

TABLE II
STATISTICS OF THE PYTHON PROMPTS USED IN THE STUDY.

Split
Total # of seed programs
Total LOC in seed programs
Total # of generated variants
Total LOC in generated variants

Leetcode
613
15,155
4296
106,659

CodeSearchNet
2,297
42,822
15,602
299,863

Seed Programs. Consistent with most research in this ﬁeld,
we form our evaluation dataset from a hosted Leetcode an-
swers repo [3] and CodesearchNet [28]. Leetcode is an online
platform for practicing algorithmic coding challenges designed
to prepare software engineers for technical interviews. As
shown in III, since the LLM-based code completion systems
have the limitation of the max token size, we only select the
programs whose token length is between 64 and 2048. Overall,
each seed program contains a medium size function with
presumably complex control structures. As shown in Table II,
we picked up 613 code snippets from the Github repo as the
seed programs. The total number of generated variants for
Leetcode programs is 4,296.

We also take another commonly-used dataset, CodeSearch-
Net [28], in the evaluation. This dataset is particularly de-
signed and widely-used in the research of code representation
learning. The Python component of this dataset contains train,
validation, and test splits; we use the test split for the evalua-
tion. Besides the similar data ﬁltering process for Leetcode, we
only keep one code snippet for the codes in the same “path”
in the test split to keep the result balanced. Each program in
this dataset contains a Python function with medium size and
non-trivial control structures. We use a total of 2,297 code
samples from this dataset, as shown in Table II.

We clarify that both LeetCode and CodeSearchNet are
deemed proper for neural code learning tasks like code com-
pletion [28]. Thus, errors and repairing revealed in our eval-
uation should mostly reﬂect common obstacles and enhance-
ments users can expect during daily usage of code completion
systems. In contrast, too complex prompts (e.g., real-world
complex software) may inherently impede the understanding
and assessment of LLM-based code completion systems.
Statistics of Test Cases. Table II reports statistics of the
Python programs used in the study. We use Leetcode and
CodeSearchNet to generate 2,910 programs. These programs
are the seed inputs of our mutation scheme for each LLM-
based code completion system. The total number of generated
variants is 19,898 (see Table V for the breakdown). Each
seed program contains a medium size funtion with presumably
complex control structures and many global variables. For each
seed program with its variants, we equally split the function
into two parts, the ﬁrst part is used as “prompt” and the

TABLE III
CODE COMPLETION SYSTEMS EVALUATED IN THE STUDY.

System
Name
Copilot
codeparrot-small
codeparrot
GPT-Neo-125M
GPT-Neo-13B
GPT-J
Codegen-2B
Codegen-6B

#
Params
?
110M
1.5B
125M
1.3B
6B
2B
6B

# Vocab
(tokens)
?
32,768
32,768
50,257
50,257
50,400
50,400
50,400

# Max.
tokens
?
1024
1024
2048
2048
2048
2048
2048

remaining is used as “oracle” to measure the quality of the
result.
Code Completion Systems. Table III reports the code comple-
tion systems used in this study. First, we use Github Copilot,
one highly visible commercial code completion system that
generates quite a buzz in the community [63]. We purchase the
standard commercial license (for single user) to unleash its full
potential. As “black-box” commercial products, it is unclear
about their implementation details (marked ? in Table III).

We also evaluate several well-known LLM models for code
completion, including CodeParrot [18], GPT-Neo [8], GPT-
J [17], and CodeGen [42]. All of these models are deemed em-
ploying large-scale language models, given that up to billions
of parameters are involved in their underlying models, and
tens of thousands of vocabularies are considered. CodeParrot
is a GPT-2 model that is trained speciﬁcally for Python code
generation. We use two variants, CodeParrot-small and Code-
Parrot. The smaller variant contains less amount of parameters
and is trained over less data. Nevertheless, we ﬁnd that both
variants manifest a high level of code generation capability. As
for GPT-Neo, we use two variants, GPT-Neo-125M and GPT-
Neo-13B, which are both GPT3-like models. Our observation
shows that comparing to its larger variant, GPT-Neo-125M is
prone to generate code snippets with less diversity but more
straightforward. Given that said, we believe that both models
are well-suited for code generation, and they indeed manifest a
reasonably high robustness under our testing campaign. GPT-
J is also a transformer model; the pre-trained model used in
our research contains 6B parameters, and is often referred to
as GPT-J-6B. Two versions of CodeGen are evaluated, and
it is specially designed for program synthesis. We use two
variants, CodeGen-2B-mono and CodeGen-6B-mono, which
are trained on a large corpus with Python code, to generate
the code snippets.

All these systems are stated to be trained on a large corpus
of open-source source code. For instance, Copilot is noted to
include the vast majority of GitHub’s open-source code. GPT-
J and GPT-Neo are trained on the Pile dataset [22], a diverse
language modeling dataset. For all LLMs except Copilot, we
download their pre-trained models from huggingface [2] and
run the code completion locally.

In all, LLM-based code completion systems presented in
to the best of our knowledge and experience,

Table III,

represent the best systems available to the public. We indeed
tentatively explored other code completion solutions based on
conventional machine learning or rule-based approaches. We
clarify that these conventional methods are seen to produce
much worse and shallow code completion outputs compared
with these LLM-based systems.

V. FINDINGS

In evaluation, we mainly explore the following research
questions. RQ1: Can CCTEST generate high-quality and
structural-consistent prompt variants? RQ2: How effective is
CCTEST to detect code completion defects? RQ3: To what
extent can CCTEST enhance the quality of code completion
outputs? We answer each research question in one subsection
below.

A. RQ1: Effectiveness on Input Generation

Answering RQ1 requires assessing the quality of mutated
prompts. At this step, for each seed prompt, we generate
mutants and ﬁrst check whether they pass the compilation.
In particular, for each sentence, we generate up to 9 mutants,
leading to a total of 19,898 mutant prompts generated on top of
2,910 seed prompts. As a quick sanity check on the validity of
transformed code, we use the standard ast module in Python
to compile all transformed Python prompts. We report that
all these generated mutant prompts can pass the compilation,
indicating that they are valid prompts.

TABLE IV
DISTRIBUTION OF STRUCTURAL CONSISTENCY SCORES.

Distance (%)
Frequency (%)
Cumulative Frequency (%)

[0, 5]
90.16
90.16

[5, 10]
8.30
98.46

[10, 15]
0.92
99.38

[15, 20]
0.30
99.68

[20, 90]
0.32
100.0

[90, 100]
0
100.0

To illustrate the structural-level consistency of mutated
prompts, we compute and cross compare the distances of
the mutated prompts from the same seed prompt. Ideally, the
smaller distance indicates that the mutants manifest closely
related structures, thus justifying the consistency of derived
code completion outputs. We use pycode-similar [4], a
well-performing tool to decide Python code similarity based
on AST tree matching. Let the prompt AST have n nodes,
where m nodes are matched toward nodes on the mutated
prompt’s AST. pycode-similar returns ratio m
n , denoting
how similar two programs are. We report the distribution of the
“distance score” (distance is computed as 1 − m
n ) in Table IV.
Overall,
that for the vast majority of mutated
prompts (over 98%), structural-level distance is less than
10%. Recall that several transformation passes in CCTEST
(Sec. III-A1) introduce only identiﬁer-level changes. As ex-
pected, the distance scores between their mutated prompts and
the seed prompts are zero, implying that program structures
are retained. A few cases manifest a relatively high distance.
With manual inspection, we ﬁnd that it is primarily due to the
fact that the size of the original prompt is short, leading to an
increased distance 1 − m

it evident

n where n is small.

Answer to RQ1: CCTEST can generate highly qual-
ity prompt mutants that are grammatically valid and
structurally consistent.

B. RQ2: Bug Detection

To answer RQ2, we launch testing to detect code com-
pletion defects. Table V provides an overview of our ﬁnd-
ings. Note that we use two datasets for the testing. For
instance, 4909+17899 in the Copilot “#Prompts” cell denotes
that we generate 4,909 mutated prompts over the LeetCode
dataset, and 17,899 mutated prompts are generated from the
CodeSearchNet dataset. Similarly, “3003 + 12101” in the
ﬁrst “#Defect T = 1” cell means that 3,003 outliers are
found using the LeetCode mutated inputs, whereas 12,101
outliers are detected by using the other test input set. For
the outlier detection evaluation, we assess the performance
under different values of the hyper-parameter T (see details
below). Recall CCTEST offers two metrics, bleu4 score and
edit distance score, to assess the effectiveness of repairing. For
each model (under different outlier thresholds T ), we quantify
the repairing effectiveness under both metrics, and also with
respect to the two leveraged datasets.

First, As shown in the second column of Table V, while
nearly all mutated prompts can be processed, we still ﬁnd
58 (0.03%) mutated prompts that trigger “no response” for
the tested code completion systems. As expected, such cases
are rare, given the high capability and comprehensiveness of
LLM-based production code completion systems.

As clariﬁed in Sec. III-B, CCTEST leverages a threshold T
to form the testing oracle: a code completion output is deemed
as an outlier, if it has a long distance with T code completion
outputs mutated from the same seed prompt. Therefore, T is
an integer ranging from 1 to the total number of passes (9 in
our implementation). Table V reports the number of uncovered
outliers in accordance with different models and thresholds.
Overall, out of in total 182,464 test cases, a different number of
defects are found under ﬁve thresholds T . In particular, when
setting the threshold T as 9, we ﬁnd 5,912 and 27,628 defects
for 8 LLMs on the LeetCode and CodeSearchNet datasets,
respectively.

As expected, we observe that with the increment number
of T , the number of detected defects (outliers) decreases.
As we illustrated in Algorithm 1, the stringency of selection
for outliers depends on the threshold T , where a larger T
represents a more strict standard. For example, compared to
T = 5, the number of the detected outlier for codeparrot on
LeetCode is only 904, less than a quarter of the former (3,631).
And if we set T = 1, it is evident that nearly all the prompts
are deemed as “outliers”, which implies a high false positive
rate.

Copilot outperforms the other seven LLMs, given less
number of inconsistency defects (293+2184 for T = 9).
However, Copilot has the most “no response” failures (41 out
of 58). We believe the reason behind such observation is that

its output is related to the network quality and it would be
possible that Copilot refuse to return anything even query for
ten times. Despite the small number of “no response” failures,
all the other cases can be analyzed and completed with some
non-trivial outputs.

Overall, Table V illustrates that the inconsistency bugs in
code completion systems are pervasive, and a large number of
defects can be found even if bugs remain even for highly per-
missive consistency thresholds. We present two representative
cases in Fig. 5 and Fig. 6, respectively. In particular, Fig. 5
presents a case, such that the code completion output of the
seed prompt is largely deviated from the ground truth. Note
that when only given the code completion output in Fig. 5(b),
it is inherently challenging to decide if the deviation between
code completion outputs in Fig. 5(a) and Fig. 5(b) are due
to model capacity or bugs. Nevertheless, when referring to
Fig. 5(c), it becomes evident that the tested code completion
system, Copilot, is capable of generating high-quality code
completion outputs that manifest closer structural represen-
tation to the ground truth. Note that comparing to the seed
prompt, we only tweak a little on the parameter name for the
prompt used in Fig. 5(c). Thus, it becomes accurate to assume
that Fig. 5(b) denotes a bug of code completion, which may
be likely ﬁxable.

Moreover, Fig. 6(a) presents another case, such that the code
completion output of the seed prompt appears to be highly
similar to the ground truth (Fig. 6(b)). In contrast, when ap-
plying the REL R scheme to mutate a local variable’s name,
the code completion output of a mutated prompt (Fig. 6(c))
becomes largely distinct from the ground truth. This clearly
denotes a bug of the code completion system.
Processing Time & Cost. We employ a GPU server for run-
ning the involved models locally. The server has an Intel Xeon
Patinum 8276 CPU, 256 GB memory and 4 NVIDIA A100
GPU. Although processing time is in general not a concern
for this study, we record and report that it takes on average
0.3 CPU seconds to generate one mutated prompt and about
35 CPU seconds to infer and obtain the results. Nevertheless,
for commercial models, Table V has revealed that roughly
10.86% percent of its code completion outputs are inconsistent
and spurious when we set the highest threshold T. In other
words, we interpret that about 10.86% percent of the code
completion outputs are highly confusing to the users and are
thus “wasted.” This may indicate an undesirable situation and
ﬁnancial loss, given that modern cloud-based code completion
systems may feature a “pay-as-you-go” mode, where users are
charged based on how many queries they send to the services.

Answer to RQ2: CCTEST identiﬁes a large number
of defects when being used to test different (com-
mercial) code completion systems, despite the varying
thresholds used in deciding the distances. In produc-
tion usage, we recommend conﬁguring T = 9 as a
presumably proper threshold to detect outliers.

TABLE V
OVERVIEW OF OUTLIER DETECTION AND ENHANCEMENT RESULTS.

System

#Prompts

# No Results

Copilot

codeparrot

codeparrot-small

GPT-J

GPT-NEO-13B

GPT-NEO-125M

Codegen-2B-mono

Codegen-6B-mono

4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)
4909+17899
bleu4(%)
edit dist.(%)

4+37

1+0

2+0

0+0

1+0

0+2

2+9

0

T= 1
3003 + 12101
13.16 + 1.95
1.6 + 6.18
4798 + 17605
6.0 + 4.52
5.31 + 1.75
4812 + 17611
7.07 + 3.71
2.52 + 3.54
4606 + 17280
2.84 + 7.59
5.59 + 6.8
4729 + 17427
3.85 + 6.6
6.51 + 6.97
4734 + 17509
2.81 + 2.89
5.03 + 9.07
4661 + 17221
1.77 + 2.82
3.95 + 5.28
4578 + 17016
2.3 + 0.87
3.65 + 4.41

T= 3
1347 + 7928
10.12 + 8.28
2.74 + 15.5
4379 + 16118
10.7 + 12.43
26.59 + 14.07
4469 + 16069
9.25 + 12.0
26.56 + 21.09
3776 + 15073
20.79 + 12.77
20.98 + 29.35
4088 + 15495
19.82 + 16.06
54.98 + 31.41
4281 + 15556
8.77 + 14.74
33.6 + 41.78
3794 + 15112
9.66 + 14.59
14.82 + 27.35
3575 + 14595
9.19 + 6.57
20.44 + 22.08

#Defects
T= 5
803 + 5570
29.88 + 16.54
4.62 + 19.73
3631 + 13368
27.63 + 27.94
48.67 + 30.32
3776 + 13454
16.49 + 21.46
36.39 + 33.47
2832 + 12005
31.06 + 25.47
60.58 + 54.7
3311 + 12536
27.09 + 31.38
81.01 + 47.56
3654 + 12907
25.89 + 26.71
91.77 + 68.07
2731 + 12241
25.56 + 19.77
44.53 + 49.83
2493 + 11546
22.15 + 14.55
35.22 + 37.46

Total Outliers
Average Enhancement
Average Enhancement

39272+143192
bleu4(%)
edit dist.(%)

10+48

35921 + 133770
4.97 + 3.86
4.27 + 5.50

29709 + 115946
12.28 + 12.18
25.08 + 25.32

23231 + 93627
25.71 + 22.97
50.34 + 42.64

T= 7
559 + 3899
10.47 + 30.31
3.55 + 32.4
2359 + 9023
47.47 + 43.06
52.44 + 52.41
2470 + 9344
32.75 + 32.72
54.74 + 50.29
1786 + 7875
40.33 + 38.66
71.35 + 67.44
2120 + 8462
35.86 + 40.47
84.53 + 67.46
2523 + 8966
46.88 + 42.17
100.27 + 89.53
1638 + 8460
23.85 + 29.29
46.37 + 58.16
1452 + 7866
38.05 + 22.31
39.06 + 55.35

14907 + 63895
34.45 + 34.87
56.53 + 59.13

T= 9
293 + 2184
21.22 + 33.98
4.77 + 38.77
904 + 3778
59.96 + 50.21
56.74 + 64.45
1033 + 4009
38.31 + 42.85
53.41 + 65.74
586 + 3174
36.14 + 46.9
87.93 + 80.16
794 + 3463
40.58 + 50.04
102.3 + 82.8
1079 + 3700
52.74 + 54.71
100.69 + 110.84
639 + 3761
39.81 + 22.96
52.81 + 72.93
584 + 3559
32.84 + 21.02
37.48 + 67.17

5912 + 27628
40.2 + 40.33
62.01 + 72.85

Fig. 5. Outlier case study. The code completion output of the seed prompt is largely deviated, whereas a mutated prompt results in much better code completion
output.

Fig. 6. Outlier case study. The code completion output of a mutated prompt is largely deviated, whereas a the seed prompt results in much better code
completion output.

defcanConvert(str1, str2):ifstr1 == str2:returnTruemapping = dict()mapping = dict()mapping["a"] = "a"mapping["b"] = "b"mapping["c"] = "c"mapping["d"] = "d"mapping["e"] = "e"mapping["f"] = "f"mapping_list= [mapping[s] fors instr1](a)Leetcode1153 codecompletion result.defcanConvert(str1, str2):ifstr1 == str2:returnTruemapping = dict()fori, char inenumerate(str1):ifstr2[i] != char:ifchar inmapping andmapping[char] != str2[i]: returnFalsemapping[char] = str2[i]iflen(set(str1)) == 26andlen(set(str2)) == 26:returnFalsereturnTruedefcanConvert(str1, param_temp):  //mutatedifstr1 == param_temp:         //mutatedreturnTruemapping = dict()temp = str1forchar intemp:ifchar inmapping:returnFalseforchar instr2:ifchar inmapping:returnFalsereturnTrue(b)GPT-J completion result with original prompt, detected as an outlier.(c)GPT-J completion result with prompt transformed by REP_R.defcountAndSay(n):record = ["1"]foriinrange(1, n):pre = record[i-1]idx= 0tmp= ""whileidx< len(pre):cnt= 1while(idx+1< len(pre) andpre[idx]==pre[idx+ 1]):idx+= 1cnt+= 1tmp+= str(cnt) + pre[idx]idx+= 1record.append(tmp)returnrecord[-1](a)Leetcode38 completion result.(b)Copilot completion result overtheseed prompt.(c)Copilot completion result with prompt transformed by REL_R, detected as an outlier.defcountAndSay(n):record = ["1"]foriinrange(1, n):pre = record[i-1]idx= 0res = ""whileidx< len(pre):count = 1whileidx<len(pre)-1andpre[idx]==pre[idx+ 1]:count += 1idx+= 1res += str(count) + pre[idx]idx+= 1record.append(res)defcountAndSay(n):record = ["1"]foriinrange(1, n):pre = record[i-1]localvar1= 0       //mutatedlocalvar2 = 0localvar3 = 0localvar4 = 0localvar5 = 0localvar6 = 0localvar7 = 0localvar8 = 0localvar9 = 0localvar10 = 0C. RQ3: Repairing Effectiveness

To answer RQ3, we ﬁrst present the accuracy improvement
in terms of different evaluated code completion systems. Then,
we assess the potency of each PSC scheme implemented in
CCTEST across different models in terms of their contribution
to repairing.

Accuracy Enhancement After Repairing. As noted in RQ2,
we report
the accuracy improvement of each model with
respect to both edit distance and bleu4 score in Table V. Note
that both edit distance and bleu4 metrics are commonly used in
relevant research to access the performance of LLMs; a higher
edit distance score or bleu4 score indicates better performance.
To clarify, Table V reports the enhancement ratio. For instance,
when assessing Copilot against the LeetCode dataset, let the
edit distance or bleu4 score be s. We compute the enhancement
ratio as r = s(cid:48)−s
, where s(cid:48) is the edit distance/bleu4 score
s
after repairing. Moreover, for all evaluated models under both
metrics, the improvement ratios are generally improved with
T growing. We mark the best improvement of each evaluation
setting in blue . Not all the cases are the same, for Copilot on
the LeetCode dataset, the bleu4 score reaches the peak when
T = 5. Similar observations are made on Codegen-6B-mono
where corresponding T = 7. We attribute this observation
to the fact that Copilot and Codegen have relatively better
performance than the remaining code completion systems.
Therefore, deciding “outliers” under a strict threshold (T = 9)
may likely overlook outliers and undermine the opportunities
of enhancing accuracy.

Potency of Transformation Passes. At this step, we measure
the potency of all nine transformation passes implemented
in CCTEST. Let code completion output oi be the repaired
output, such that oi is generated by using the prompt mutated
from transformation pass ti. We deem ti under such circum-
stance as the “optimal” transformation that successfully con-
tributes to the code completion repairing. For both LeetCode
and CodeSearchNet, we report the distribution of different
transformations selected as “optimal” under different models
in Fig.7 and Fig. 8, respectively. Overall, all transformations
introduced in Sec.III are extensively served as the “optimal”
in both datasets. In particular, nearly all passes, except “IRR”
and “RTF,” are chosen for roughly equal amount of times. IRR
has a relatively smaller application scope, given that IRR looks
for speciﬁc arithmetic operations like “+=” that may not be
pervasively used. Similarly, RTF requires the existence of an
if condition with a relatively simple condition, which may
not be available in our test cases. Overall, we interpret the
evaluation results are highly encouraging, showing that all the
designed passes manifest high applicability and effectiveness
in enhancing different models. When using the LeetCode
dataset, “IRR” is obviously more useful in contributing “op-
timal” cases. With manual inspection, we ﬁnd that the codes
in LeetCode, an online judge (OJ) platform, are more likely
to contain arithmetic operations such as “+=” in comparison
to the code in the CodeSearchNet dataset.

Fig. 7. Distribution of PSC transformations contributing the repaired outputs
when evaluating the LeetCode dataset.

Fig. 8. Distribution of PSC transformations contributing the repaired outputs
when evaluating the CodeSearchNet dataset.

In addition to reporting the distribution of the “optimal”
transformations, we also report to what extent each transfor-
mation, when contributing to the code completion repairing,
can enhance the bleu4 score of code completion systems.

Fig. 9 and Fig. 10 report the enhancement ratios across all
code completion systems under two datasets. Aligned with the
observation in Fig. 7 and Fig. 8, we ﬁnd that all transformation
passes manifest a promising enhancement ratio under different
settings and datasets. That is, instead of identifying one or few
dominating passes that signiﬁcantly contribute to enhancing
the output of code completion, each pass offers a reasonable
degree of contribution. “INI” delivers particularly outstanding
improvement for the Copilot/LeetCode evaluation.

Again, we interpret the evaluation results are highly encour-
aging, justifying the necessity of every transformation pass
designed in CCTEST.

IRRGRA_RGRA_CREP_RREP_CINIRTFREL_RREL_CPass010203040# Num of casescodeparrotsmallcodeparrotgptneo125mcopilotgptneo13bgptjcodegen2bcodegen6bIRRGRA_RGRA_CREP_RREP_CINIRTFREL_RREL_CPass0255075100125150175# Num of casescodeparrotsmallcodeparrotgptneo125mcopilotgptneo13bgptjcodegen2bcodegen6bthat aim to detect ﬂaws of deep learning-based applications
with dynamic testing rather than veriﬁcation.

We check code completion outputs by comparing a set
of outputs that are supposed to exhibit consistent structures.
The evaluation shows that the focus of structural-consistency
effectively unveils a large number of defects. However, a
possible threat is that defects can be neglected in the com-
pletion output, in case all outputs share aligned yet erroneous
code patterns. We deem this as a general and well-known
hurdle for invariant property-based testing techniques. We
leave exploring solutions to this challenge for future work.

Besides, there exists the potential threat that the proposed
testing and repairing framework, CCTEST, may not adapt
to other types of code completion systems. Nevertheless,
we mitigate this threat to external validity by designing an
approach that is system and algorithm independent. As a result,
our approach is anticipated to be applicable to other settings
outside the current scope. We believe the proposed technique
is general, and we give further discussions regarding other
settings in this section.
Cross Comparison of Code Completion Outputs. Overall,
CCTEST individually tests each code completion system.
Holistically speaking, the proposed approach constitutes pro-
gram property-based testing (or metamorphic testing). With
this regard, careful readers may wonder about the feasibility
of conducting a differential testing, by processing the same
prompt with different code completion systems and differen-
tiating their outputs. However, we note that the code com-
pletion outputs can have drastically different representations
since different code completion systems have different model
training data and LLM model capacity. For instance, Copilot
is seen to produce a large chunk of code snippets (with
multiple statements), whereas some other well-known systems
are prone to giving more succinct outputs for the same input
prompts. Our preliminary exploration also shows that they
manifest different tactics and translation templates in code
generation. Thus, the similarity among the code completion
outputs are deemed as low across different code completion
systems. Overall, we leave it as one future work to explore
practical methods to perform cross comparison, for instance,
by extracting certain “semantics-level” signatures or regulating
their output code patterns ﬁrst.
Other Settings. The main focus of this study is Python code
completion, one challenging and popular task commonly en-
countered in real-world software engineering missions. Over-
all, we envision the feasibility of smoothly migrating the
prototype of CCTEST to test code completion models of other
languages, such as C and Java. While extending CCTEST to
handle other programming languages demands new parsers
and re-implementation of current PSC schemes, we see that the
key technical pipeline, including mutation, outlier detection,
and repairing, are mostly model and language independent.
Note that these are engineering endeavors rather than open-
ended research problems. We leave it as one future work to
support other languages.

Fig. 9. Distribution of enhancement ratio contributed by different PSC
transformations on the LeetCode dataset.

Fig. 10. Distribution of enhancement ratio contributed by different PSC
transformations on the CodeSearchNet dataset.

Answer to RQ3: CCTEST can successfully improve
the quality of different code completion systems. We
also ﬁnd that instead of one or few prompt mutation
schemes that signiﬁcantly contribute to enhancing code
completion, all schemes are shown as effective in im-
proving the performance of code completion systems.

VI. DISCUSSION

Limitations and Threats to Validity. We now give a discus-
sion of the validity and shortcomings of this paper’s approach.
In this research, construct validity denotes the degree to which
our metrics actually reﬂect the correctness of code completion
systems. Overall, we conduct dynamic testing and manual
inspection to study the outputs of de facto code completion
systems. Hence, while this practical approach detects their
defects and reveals chances of repairing their outputs, the most
possible threat is that our testing approach cannot guarantee
the correctness of code completion systems. We clarify that
our work roots the same assumption as previous testing works

IRRGRA_RGRA_CREP_RREP_CINIRTFREL_RREL_CPass0100200300400500600700800Average performance improvement (%)codeparrotsmallcodeparrotgptneo125mcopilotgptneo13bgptjcodegen2bcodegen6bIRRGRA_RGRA_CREP_RREP_CINIRTFREL_RREL_CPass050100150200250300350Average performance improvement (%)codeparrotsmallcodeparrotgptneo125mcopilotgptneo13bgptjcodegen2bcodegen6bVII. RELATED WORK
Testing & Repairing Neural Models. In recent several years,
many works have been applying software testing methods to
neural models [67], [57], [48], [69], [43], [60], [16], [40]. To
date, the tested neural models include typical computer vision
tasks like image classiﬁcation, object detection [61], [51], [56],
auto-driving [67], [69], as well as natural language processing
tasks like sentiment analysis [49], [38], [21], [58], question
answering [14], and machine translation [26], [27], [25], [53],
[12], and speciﬁc properties like fairness [15]. CCTEST is
inspired by recent advances in machine translation testing.
It addresses domain speciﬁc challenges in mutating program
inputs, and for the ﬁrst time, provides a systematic framework
for testing and repairing code completion systems in blackbox
settings.
Neural Code Comprehension. In addition to using LLMs
for code completion, deep learning-based methods have been
extensively used in related important code comprehension
tasks and have achieved promising results. For instance, func-
tion naming decides the function name by summarizing the
function body [6], [7], [71]. Often, code paths on the func-
tion ASTs are extracted for embedding and name prediction,
and given the large volume of available paths, optimization
schemes like attention are often used to speed up the pro-
cessing. Code classiﬁcation and code search are two popular
tasks in this ﬁeld. De facto methods explore learning from
ASTs and CFGs to leverage structural information for code
classiﬁcation [39], [37], [62] and for code search [11], [24].
Tree-based convolutional neural networks and graphics neural
networks are leveraged in these tasks [59], [36]. Furthermore,
many security downstream applications have been built on
the basis of neural code comprehension models, including
code cloning and plagiarism detection [32], [35], malware
clustering [30], software component analysis [66], [64], [65],
vulnerability detection [34], [70].

VIII. CONCLUSION

We conducted a systematic investigation on the output
consistency of modern code completion systems. We of-
fer CCTEST, a testing framework to perform structural-
consistency mutations on input prompts, and we further design
a black-box repairing scheme to enhance the accuracy of
code completion systems. Our evaluation of eight prominent
(commercial) code completion systems identiﬁed thousands
of inconsistency defects, whereas our repairing scheme ef-
fectively enhances their accuracy. This work may serve as a
roadmap for researchers and users interested in utilizing and
improving code completion systems.

REFERENCES

[1] Codex. https://openai.com/blog/openai-codex/.
[2] Hugging Face. https://huggingface.co/.
[3] Leetcode Python. https://github.com/JiayangWu/LeetCode-Python.
[4] pycode-similar. https://pypi.org/project/pycode-similar/.
[5] Tabnine. https://www.tabnine.com/.
[6] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav.

code2seq:
Generating sequences from structured representations of code. arXiv
preprint arXiv:1808.01400, 2018.

[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec:
Learning distributed representations of code. Proceedings of the ACM
on Programming Languages, 3(POPL):1–29, 2019.

[8] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.
GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-
Tensorﬂow, March 2021. If you use this software, please cite it using
these metadata.

[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural
information processing systems, 33:1877–1901,
2020.

[10] Max Brunsfeld. Tree-sitter. https://github.com/tree-sitter/tree-sitter.
[11] Jos´e Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish
In Marlon Dumas,
Chandra. When deep learning met code search.
Dietmar Pfahl, Sven Apel, and Alessandra Russo, editors, Proceedings
of the ACM Joint Meeting on European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering,
ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, pages
964–974. ACM, 2019.

[12] Jialun Cao, Meiziniu Li, Yeting Li, Ming Wen, and Shing-Chi Cheung.
SemMT: A semantic-based testing approach for machine translation
systems. arXiv preprint arXiv:2012.01815, 2020.

[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, et al. Evaluating large language models trained
on code. arXiv preprint arXiv:2107.03374, 2021.

[14] Songqiang Chen, Shuo Jin, and Xiaoyuan Xie. Testing your question
answering software via asking recursively. In Proceedings of the 36th
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE ’21, page 104–116. IEEE Press, 2021.

[15] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark
Harman. Fairness testing: A comprehensive survey and analysis of
trends. arXiv e-prints, pages arXiv–2207, 2022.

[16] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M.
Rao, R. P. Jagadeesh Chandra Bose, Neville Dubash, and Sanjay
Identifying implementation bugs in machine learning based
Podder.
image classiﬁers using metamorphic testing. In ISSTA, 2018.

[17] EleutherAI. Gpt-j. https://huggingface.co/EleutherAI/gpt-j-6B.
[18] Hugging Face.
codeparrot.

Codeparrot.

https://huggingface.co/codeparrot/

[19] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,
Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming
Zhou. Codebert: A pre-trained model for programming and natural
languages. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings
of the Association for Computational Linguistics: EMNLP 2020, Online
Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL,
pages 1536–1547. Association for Computational Linguistics, 2020.
[20] Philip Gage. A new algorithm for data compression. C Users Journal,

12(2):23–38, 1994.

[21] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. Fairness testing:
testing software for discrimination. In ACM ESEC/FSE, pages 498–510.
ACM, 2017.

[22] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe,
Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima,
Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse
text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[23] GitHub. Copilot.
[24] Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu
Zhang, Zenglin Xu, and Michael R. Lyu. Cradle: Deep code retrieval
based on semantic dependency learning. Neural Networks, 141:385–394,
2021.

[25] Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. Machine
translation testing via pathological invariance. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pages 863–875,
2020.

[26] Pinjia He, Clara Meister, and Zhendong Su. Structure-invariant testing

for machine translation. In ICSE, 2020.

[27] Pinjia He, Clara Meister, and Zhendong Su. Testing machine translation
In 2021 IEEE/ACM 43rd International
via referential transparency.
Conference on Software Engineering (ICSE), pages 410–422. IEEE,
2021.

[28] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and
Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of
semantic code search. arXiv preprint arXiv:1909.09436, 2019.

[29] Saki Imai. Is github copilot a substitute for human pair-programming?
an empirical study. In 2022 IEEE/ACM 44th International Conference
on Software Engineering: Companion Proceedings (ICSE-Companion),
pages 319–321. IEEE, 2022.

[30] Mahmoud Kalash, Mrigank Rochan, Noman Mohammed, Neil DB
Bruce, Yang Wang, and Farkhund Iqbal. Malware classiﬁcation with
In 2018 9th IFIP international
deep convolutional neural networks.
conference on new technologies, mobility and security (NTMS), pages
1–5. IEEE, 2018.

[31] Klemens Lagler, Michael Schindelegger, Johannes B¨ohm, Hana Kr´asn´a,
and Tobias Nilsson. GPT2: Empirical slant delay model for radio space
geodetic techniques. Geophysical research letters, 40(6):1069–1073,
2013.

[32] Maggie Lei, Hao Li, Ji Li, Namrata Aundhkar, and Dae-Kyoo Kim.
Deep learning application on code clone detection: A review of current
knowledge. Journal of Systems and Software, 184:111141, 2022.
[33] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian
Schrittwieser, R´emi Leblond, Tom Eccles, James Keeling, Felix Gimeno,
Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-
son d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes
Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J.
Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Fre-
itas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code
generation with alphacode. CoRR, abs/2203.07814, 2022.

[34] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang,
Zhijun Deng, and Yuyi Zhong. Vuldeepecker: A deep learning-based
system for vulnerability detection. arXiv preprint arXiv:1801.01681,
2018.

[35] Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen
Nie, and Shi Wu. Unleashing the power of compiler intermediate rep-
resentation to enhance neural program embeddings. In 44th IEEE/ACM
44th International Conference on Software Engineering, ICSE 2022,
Pittsburgh, PA, USA, May 25-27, 2022, pages 2253–2265. ACM, 2022.
[36] Shangqing Liu, Xiaofei Xie, Lei Ma, Jing Kai Siow, and Yang Liu.
Graphsearchnet: Enhancing gnns via capturing global dependency for
semantic code search. CoRR, abs/2111.02671, 2021.

[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang,
et al. Codexglue: A machine learning benchmark dataset for code
understanding and generation. arXiv preprint arXiv:2102.04664, 2021.
[38] Pingchuan Ma, Shuai Wang, and Jin Liu. Metamorphic testing and
certiﬁed mitigation of fairness violations in nlp models. In IJCAI, pages
458–465, 2020.

[39] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural
networks over tree structures for programming language processing. In
Thirtieth AAAI conference on artiﬁcial intelligence, 2016.

[40] Shin Nakajima and Tsong Yueh Chen. Generating biased dataset for
In IFIP-ICTSS,

metamorphic testing of machine learning programs.
2019.

[41] Nhan Nguyen and Sarah Nadi. An empirical evaluation of github
copilot’s code suggestions. In IEEE/ACM 19th International Conference
on Mining Software Repositories, MSR 2022, Pittsburgh, PA, USA, May
23-24, 2022, pages 1–5. IEEE, 2022.

[42] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo
Zhou, Silvio Savarese, and Caiming Xiong. A conversational paradigm
for program synthesis. arXiv preprint arXiv:2203.13474, 2022.
[43] Augustus Odena and Ian Goodfellow. Tensorfuzz: Debugging neural net-
works with coverage-guided fuzzing. arXiv preprint arXiv:1807.10875,
2018.

[44] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and
Brendan Dolan-Gavitt. Can openai codex and other large language
models help us ﬁx security bugs? arXiv preprint arXiv:2112.02125,
2021.

[45] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and
Brendan Dolan-Gavitt. Can openai codex and other large language
models help us ﬁx security bugs? CoRR, abs/2112.02125, 2021.
[46] Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy, Farshad
Khorrami, Ramesh Karri, and Brendan Dolan-Gavitt. Pop quiz! can
a large language model help with reverse engineering? arXiv preprint
arXiv:2202.01142, 2022.

[47] Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy, Farshad
Khorrami, Ramesh Karri, and Brendan Dolan-Gavitt.
Pop quiz!
can a large language model help with reverse engineering? CoRR,
abs/2202.01142, 2022.

[48] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore:
Automated whitebox testing of deep learning systems. In Proceedings of
the 26th Symposium on Operating Systems Principles, SOSP ’17, pages
1–18, New York, NY, USA, 2017. ACM.

[49] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer
Singh. Beyond accuracy: Behavioral testing of nlp models with check-
list. arXiv preprint arXiv:2005.04118, 2020.

[50] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic
generation of programming exercises and code explanations with large
language models. arXiv preprint arXiv:2206.11861, 2022.

[51] Jinyang Shao. Testing object detection for autonomous driving systems
via 3d reconstruction. In 2021 IEEE/ACM 43rd International Conference
on Software Engineering: Companion Proceedings (ICSE-Companion),
pages 117–119. IEEE, 2021.

[52] Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and Lu Zhang.
Automatic testing and improvement of machine translation. In Gregg
Rothermel and Doo-Hwan Bae, editors, ICSE ’20: 42nd International
Conference on Software Engineering, Seoul, South Korea, 27 June - 19
July, 2020, pages 974–985. ACM.

[53] Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang.
In Pro-
Automatic testing and improvement of machine translation.
ceedings of the ACM/IEEE 42nd International Conference on Software
Engineering, pages 974–985, 2020.

[54] Zeyu Sun, Jie M. Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis,
Improving machine translation systems via isotopic
and Lu Zhang.
In 44th IEEE/ACM 44th International Conference on
replacement.
Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27,
2022, pages 1181–1192. ACM, 2022.

[55] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient
transformers: A survey. ACM Computing Surveys (CSUR), 2020.
[56] Yongqiang Tian, Shiqing Ma, Ming Wen, Yepang Liu, Shing-Chi
Cheung, and Xiangyu Zhang. To what extent do dnn-based image
classiﬁcation models make unreliable inferences? Empirical Software
Engineering, 26(5):1–40, 2021.

[57] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. DeepTest: Au-
tomated testing of deep-neural-network-driven autonomous cars. ICSE
’18, 2018.

[58] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. Automated

directed fairness testing. ASE, 2018.

[59] Huanting Wang, Guixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang
Huang, Dingyi Fang, Yansong Feng, Lizhong Bian, and Zheng Wang.
Combining graph-based learning with automated data collection for code
vulnerability detection. IEEE Transactions on Information Forensics and
Security, 16:1943–1958, 2020.

[60] Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang.
Adversarial sample detection for deep neural network through model
mutation testing. ICSE, 2019.

[61] Shuai Wang and Zhendong Su. Metamorphic object insertion for testing

object detection systems. In ASE, 2020.

[62] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5:
Identiﬁer-aware uniﬁed pre-trained encoder-decoder models for code
understanding and generation. arXiv preprint arXiv:2109.00859, 2021.
[63] Dror Weiss. Comparison between copilot and tabnine. https://twitter.

com/drorwe/status/1539329682851733505.

[64] Seunghoon Woo, Sunghan Park, Seulbae Kim, Heejo Lee, and Hakjoo
Oh. Centris: A precise and scalable approach for identifying modiﬁed
In 2021 IEEE/ACM 43rd International
open-source software reuse.
Conference on Software Engineering (ICSE), pages 860–872. IEEE,
2021.

[65] Zeping Yu, Wenxin Zheng, Jiaqi Wang, Qiyi Tang, Sen Nie, and Shi
Wu. Codecmr: Cross-modal retrieval for function-level binary source
code matching. Advances in Neural Information Processing Systems,
33:3872–3883, 2020.

[66] Xian Zhan, Lingling Fan, Sen Chen, Feng We, Tianming Liu, Xiapu
Luo, and Yang Liu. Atvhunter: Reliable version detection of third-party
libraries for vulnerability identiﬁcation in android applications. In 2021
IEEE/ACM 43rd International Conference on Software Engineering
(ICSE), pages 1695–1707. IEEE, 2021.

[67] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz
Khurshid. DeepRoad: GAN-based Metamorphic Testing and Input
Validation Framework for Autonomous Driving Systems. In ASE, 2018.
[68] Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. Diet
code is healthy: Simplifying programs for pre-trained models of code.
arXiv preprint arXiv:2206.14390, 2022.

[69] Husheng Zhou, Wei Li, Yuankun Zhu, Yuqun Zhang, Bei Yu, Lingming
Zhang, and Cong Liu. Deepbillboard: Systematic physical-world testing
of autonomous driving systems. ICSE, 2020.

[70] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu.
Devign: Effective vulnerability identiﬁcation by learning comprehensive
program semantics via graph neural networks. Advances in neural
information processing systems, 32, 2019.

[71] Daniel Z¨ugner, Tobias Kirschstein, Michele Catasta, Jure Leskovec,
Language-agnostic representation learn-
arXiv preprint

and Stephan G¨unnemann.
ing of source code from structure and context.
arXiv:2103.11318, 2021.

