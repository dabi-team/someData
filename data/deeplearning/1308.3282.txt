5
1
0
2

l
u
J

8
2

]
E
N
.
s
c
[

2
v
2
8
2
3
.
8
0
3
1
:
v
i
X
r
a

COMPLETE STABILITY ANALYSIS OF A HEURISTIC
APPROXIMATE DYNAMIC PROGRAMMING CONTROL
DESIGN

YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

Abstract. This paper provides new stability results for Action-Dependent
Heuristic Dynamic Programming (ADHDP), using a control algorithm that
iteratively improves an internal model of the external world in the autonomous
system based on its continuous interaction with the environment. We extend
previous results for ADHDP control to the case of general multi-layer neural
networks with deep learning across all layers. In particular, we show that the
introduced control approach is uniformly ultimately bounded (UUB) under
speciﬁc conditions on the learning rates, without explicit constraints on the
temporal discount factor. We demonstrate the beneﬁt of our results to the con-
trol of linear and nonlinear systems, including the cart-pole balancing problem.
Our results show signiﬁcantly improved learning and control performance as
compared to the state-of-art.

1. Introduction

Adaptive Dynamic Programming (ADP) addresses the general challenge of opti-
mal decision and control for sequential decision making problems in real-life scenar-
ios with complex and often uncertain, stochastic conditions without the presump-
tion of linearity. ADP is a relatively young branch of mathematics; the pioneering
work (Werbos, 1974) provided powerful motivation for extensive investigations of
ADP designs in recent decades (Barto, Sutton & Anderson, 1983; Werbos, 1992;
Bertsekas & Tsitsiklis, 1996; Si, Barto & Powell & Wunsch, 2004; Vrabie & Lewis,
2009; Lendaris, 2009; Wang, Liu, Wei & Zhao & Jin, 2012; Zhang, Liu, Luo &
Wang, 2013). ADP has not only shown solid theoretical results to optimal con-
trol but also successful applications (Venayagamoorthy & Harley & Wunsch, 2003).
Various ADP designs demonstrated powerful results in solving complicated real-life
problems, involving multi-agent systems and games (Valenti, 2007; Al-Tamini &
Lewis & Abu-Khalaf, 2007; Zhang & Wei & Liu, 2011).

The basic ADP approaches include heuristic dynamic programming (HDP), dual
heuristic dynamic programming (DHP) and globalized DHP (GDHP) (Werbos,
1974, 1990; White & Sofge, 1992; Prokhorov & Wunsch, 1997). For each of these
approaches there exists an action-dependent (AD) variation (White & Sofge, 1992).
For several important cases, the existence of stable solution for ADP control has
been shown under certain conditions (Abu-Khalaf & Lewis, 2005; Vrabie & Lewis,
2009; Lewis & Liu, 2012; Zhang, Zhang, Luo & Liang, 2013).

Key words and phrases. Adaptive Dynamic Programming; Action-Dependent Heuristic Dy-
namic Programming; Adaptive control; Adaptive critic; Neural network; Gradient Descent; Lya-
punov function.

1

 
 
 
 
 
 
2

YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

The stability of ADP in the general case is an open and yet unsolved problem.
There are signiﬁcant eﬀorts to develop conditions for stability in various ADP de-
signs. We solved the stability problem for the speciﬁc ADHDP control case using
the Lyapunov approach, which is a classical method of investigating stability of
dynamical processes. Here we are addressing a discrete time dynamical system,
where the dynamics is described by a diﬀerence equation. The discrete time Lya-
punov function is used to prove the stability of the controlled process under certain
conditions.
In this paper we generalize the results of (Liu, Sun, Si, & Guo &
Mei, 2012) for deriving stability conditions for ADHDP with traditional three layer
Multi-Layer Perceptron (MLP). The work (Liu et al., 2012) derives a stability con-
dition for the system with weights adapted between the hidden and output layers
only, under the assumption that networks have large enough number of neurons in
the hidden layers.

The approach presented in (Liu et al., 2012), in eﬀect, is equivalent to a linear ba-
sis function approach: it is easy but it leads to scalability problems. The complexity
of the system is growing exponentially for the required degree of approximation of a
function of given smoothness (Barron, 1994). Additional problems arise regarding
the accuracy of parameter estimation, which tends to grow with the number of pa-
rameters, all other factors are kept the same. If we have too many parameters for
a limited set of data, it leads to overtraining. We need more parsimonious model,
capable of generalization, hence our intention is to use fewer parameters in truly
nonlinear networks, which is made possible by implementing more advanced learn-
ing algorithm. In the present work we focus on studying the stability properties of
the ADP system with MLP-based critic, when the weights are adapted between all
layers. By using Lyapunov approach, we study the uniformly ultimately bounded
property of the ADHDP design. Preliminary results of our generalized stability
studies have been reported in (Kozma & Sokolov, 2013), where we showed that our
general approach produced improved learning and convergence results, especially
in the case of diﬃcult control problems.

The rest of the paper is organized as follows. First we brieﬂy outline theoretical
foundations of ADHDP. Next we describe the learning algorithm based on gradient
descent in the critic and action networks. This is followed by the statements and
the proofs of our main results on the generalized stability criteria of the ADP
approach. Finally, we illustrate the results using examples of two systems. The
ﬁrst one is a simple linear system used in (Liu et al., 2012), and the second example
is the inverted pendulum system, similar to (He, 2011). We conclude the paper by
outlining potential beneﬁts of our general results for future applications in eﬃcient
real-time training and control.

2. Theoretical foundations of ADHDP control

2.1. Basic deﬁnitions. Let us consider a dynamical system (plant) with discrete
dynamics, which is described by the following nonlinear diﬀerence equation:

x(t + 1) = f (x(t), u(t)) ,

(2.1)

where x is the m-dimensional plant state vector and u is the n-dimensional control
(or action) vector.

Previously we reported some stability results for ADP in the general stochastic
case (Werbos, 2012). In this paper we focus on the deterministic case, as described

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

3

in equation (2.1) and introduce action-dependent heuristic dynamic programming
(ADHDP) to control this system. The original ADHDP method has been used
in the 1990’s for various important applications, including the manufacturing of
carbon-carbon composite parts (White & Sofge, 1992). ADHDP is a learning algo-
rithm for adapting a system made up of two components, the critic and the action,
as shown in Fig. 1. These two major components can be implemented using any
kind of diﬀerentiable function approximator. Probably the most widely used value
function approximators in practical applications (as surveyed in Lewis and Liu,
2012) are neural networks, linear basis function approximators, and piecewise lin-
ear value functions such as those used by (Powell, 2011). In this work we use MLP
as the universal function approximator.

The optimal value function, J ∗ is the solution of the Bellman equation (White
& Sofge, 1992), which is a function of the state variables but not of the action
variables. Here we use function J, which is closely related to J ∗, but J is a function
of both the state and the action variables. Function J is often denoted by J (cid:48) in the
literature, following the deﬁnition in (White & Sofge, 1992, Chapter 3). The critic
provides the estimate of function J, which is denoted as ˆJ. Function Q, used in
traditional Q-learning (Si et al., 2004) is the discrete-variable equivalent of J.

The action network represents a control policy. Each combination of weights
deﬁnes a diﬀerent controller, hence by exploring the space of possible weights we
approximate the dynamic programming solution for the optimal controller. AD-
HDP is a method for improving the controller from one iteration to the next, from
time instant t to t + 1. We also have internal iterations, which are not explicit
(Lewis & Liu, 2012; He 2011). Namely, at a given t, we update the weights of the
neural networks using supervised learning for a speciﬁc number of internal iteration
steps.

In ADHDP, the cost function is expressed as follows; see, e.g., (Lewis & Liu,

2012):

J(x(t), u(t)) =

∞
(cid:88)

i=t

αi−tr(x(i + 1), u(i + 1)),

(2.2)

where 0 < α ≤ 1 is a discount factor for the inﬁnite horizon problem, and
r(x(t), u(t)) is the reward or reinforcement or utility function (He, 2011; Zhang, Liu
& Luo & Wang, 2013). We require r(t) = r(x(t), u(t)) to be a bounded semideﬁnite
function of the state x(t) and control u(t), so the cost function is well-deﬁned. Using
standard algebra one can derive from (2.2) that 0 = αJ(t) + r(t) − J(t − 1), where
J(t) = J(x(t), u(t)).

2.2. Action network. Next we introduce each component, starting with the ac-
tion component. The action component will be represented by a neural network
(NN), and its main goal is to generate control policy. For our purpose, MLP with
one hidden layer is used. At each time step this component needs to provide an
action based on the state vector x(t) = (x1(t), . . . , xm(t))T , so x(t) is used as an
input for the action network. If the hidden layer of the action MLP consists of Nha
nodes; the weight of the link between the input node j and the hidden node i is
denoted by ˆw(1)
aij (t), where i = 1, . . . , n,
j = 1, . . . , Nha is the weight from j(cid:48)s hidden node to i(cid:48)s output. The weighted sum of

aij (t), for i = 1, . . . , Nha and j = 1, . . . , m. ˆw(2)

4

YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

Figure 1. Schematics of the implemented ADHDP design

Figure 2. Illustration of the action network as a MLP with one
hidden layer.

all inputs, i.e., the input to a hidden node k is given as σak (t) = (cid:80)m
j=1 ˆw(1)
The output of hidden node k of the action network is denoted by φak (t).

akj (t)xj(t).

For neural networks a variety of transfer functions are in use, see, e.g. (Zhang,
Liu & Luo & Wang, 2013). Hyperbolic tangent is a common transfer function,
which is used here: φak (t) = 1−e−σak
(t) . A major advantage of the standard MLP
1+e−σak
neural network described here is the ability to approximate smooth nonlinear func-
tions more accurately than linear basis function approximators, as the number of
inputs grows (Barron, 1993; 1994). Finally, the output of the action MLP is a
n-dimensional vector of control variables ui(t) = (cid:80)Nha
aij (t)φaj (t). The diagram
of the action network is shown in Fig. 2.

j=1 ˆw(2)

(t)

2.3. Critic network. The critic neural network, with output ˆJ, learns to ap-
proximate J function and it uses the output of the action network as one of
its inputs. This is shown in Fig. 3. The input to the critic network is y(t) =
(x1(t), . . . , xm(t), u1(t), . . . , un(t))T , where u(t) = (u1(t), . . . , un(t))T is output of
the action network. Just as for the action NN, here we use an MLP with one
hidden layer, which contains Nhc nodes.
cij (t), for i = 1, . . . , Nhc and j =
1, . . . , m + n is the weight from j(cid:48)s input to i(cid:48)s hidden node of the critic net-
work. Here hyperbolic tangent transfer function is used. For convenience, the

ˆw(1)

ActionNetworkSystemCriticNetwork+--+x(t)u(t)x(t)^J(t)r(t)Ucx(t-1)a^J(t-1)^J(t-1)u(t-1)CriticNetwork+sfakak ^  (2)           wa11 ^  (1)           wa11x1x2xmu1u2unCOMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

5

Figure 3. Illustration of the critic network as a MLP with one
hidden layer.

j=1 ˆw(1)

j=1 ˆw(1)

ckj (t)xj(t) + (cid:80)n

input to a hidden node k is split in two parts with respect to inputs σck (t) =
(cid:80)m
ci(m+j)(t)uj(t). The output of hidden node k of the
critic network is given as φck (t) = 1−e−σck
(t) . Since the critic network has only
1+e−σck
one output, we have Nhc weights between hidden and output layers of the form
ˆw(2)
ci (t). Finally, the output of the critic neural network can be described in the
form ˆJ(t) = ˆw(2)
ci (t)φci(t), where ∗ denotes the inner product.

c (t)∗φc(t) = (cid:80)Nhc

i=1 ˆw(2)

(t)

3. Gradient-descent Learning Algorithm
3.1. Adaptation of the critic network. Let ec(t) = α ˆJ(t) + r(t) − ˆJ(t − 1)
be the prediction error of the critic network and Ec(t) = 1
c(t) be the objective
function, which must be minimized. Let us consider gradient descent algorithm as
the weight update rule, that is, ˆwc(t + 1) = ˆwc(t) + ∆ ˆwc(t). Here the last term is
∆ ˆwc(t) = lc

and lc > 0 is the learning rate.

2 e2

(cid:104)

(cid:105)

− ∂Ec(t)
∂ ˆwc(t)

By applying the chain rule, the adaptation of the critic network’s weights between

input layer and hidden layer is given as follow ∆ ˆw(1)

cij (t) = lc

yields

(cid:21)

(cid:20)
− ∂Ec(t)
∂ ˆw(1)
cij (t)

, which

∂Ec(t)
∂ ˆw(1)
cij (t)

=

∂Ec(t)
∂ ˆJ(t)

∂φci(t)
∂σci(t)

=

∂σci(t)
∂ ˆw(1)
cij (t)
(cid:21)

∂ ˆJ(t)
∂φci(t)
(cid:20) 1
2

(t)

αec(t) ˆw(2)
ci

(1 − φ2
ci

(t))

yj(t).

(3.1)

The last calculation is obtained with respect to the main HDP (and ADHDP)
paradigm, which treats ˆJ(·) at diﬀerent time steps as diﬀerent functions; see e.g.,
(Lewis & Liu, 2012; Werbos, 2012). Application of the chain rule for the adapta-
tion of the critic network’s weights between hidden layer and output layer yields
∆ ˆw(2)

, which leads to

(cid:21)

ci (t) = lc

(cid:20)
− ∂Ec(t)
∂ ˆw(2)
ci (t)

∂Ec(t)
∂ ˆw(2)
ci (t)

=

∂Ec(t)
∂ ˆJ(t)

∂ ˆJ(t)
∂ ˆw(2)
ci (t)

= αec(t)φci(t).

(3.2)

 ^  (1)           wsfckckc11 ^  (2)           wc1x1u1unJ^6

YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

3.2. Adaptation of the action network. The training of the action network
can be done by using the backpropagated adaptive critic method (White & Sofge,
1992), which entails adapting the weights so as to minimize ˆJ(t). In this paper
we used an importance-weighted training approach. We denote by Uc the desired
ultimate objective function. Then the minimized error measure is given in the form
a(t), where ea(t) = ˆJ(t) − Uc is the prediction error of the action NN.
Ea(t) = 1
In the framework of the reinforcement learning paradigm, the success corresponds
to an objective function, which is zero at each time step (Barto, Sutton & Anderson,
1983). Based on this consideration and for the sake of simplicity of the further
derivations, we assume Uc = 0, that is, the objective function is zero at each time
step, i.e. there is success.

2 e2

Let us consider gradient descent algorithm as the weight update rule similarly
as we did for the critic network above. That is, ˆwa(t + 1) = ˆwa(t) + ∆ ˆwa(t), where
∆ ˆwa(t) = la

and la > 0 is the learning rate.

(cid:105)

(cid:104)

− ∂Ea(t)
∂ ˆwa(t)

By applying the chain rule, the adaptation of the action network’s weights be-

tween input layer and hidden layer is given as ∆ ˆw(1)

aij (t) = la

(cid:20)
− ∂Ea(t)
∂ ˆw(1)
aij (t)

(cid:21)
,

∂Ea(t)
∂ ˆw(1)
aij (t)

=

∂Ea(t)
ˆJ(t)

(cid:34)

∂ ˆJ(t)
∂u(t)

(cid:35)T

∂u(t)
∂φai(t)

∂φai(t)
∂σai (t)

∂Ea(t)
ˆJ(t)

n
(cid:88)

k=1

∂ ˆJ(t)
∂uk(t)

∂uk(t)
∂φai(t)

∂φai(t)
∂σai (t)

∂σai(t)
∂ ˆw(1)
aij (t)
∂σai(t)
∂ ˆw(1)
aij (t)

=

=

ˆJ(t)

n
(cid:88)

Nhc(cid:88)

k=1

r=1

(cid:20)
ˆw(2)
cr

(t)

1
2

(1 − φ2
cr

(t)) ˆw(1)

cr,m+k

(cid:21)

(t)

× ˆw(2)
aki

(t)

1
2

(1 − φ2
ai

(t))xj(t),

(3.3)

where

∂ ˆJ(t)
∂uk(t)

=

Nhc(cid:88)

i=1

∂ ˆJ(t)
∂φci(t)

∂φci(t)
∂σci (t)

∂σci (t)
∂uk(t)

.

(3.4)

Using similar approach for the action network’s weights between hidden layer and
output layer, ﬁnally we get the following ∆ ˆw(2)

(cid:21)
,

aij (t) = la

(cid:20)
− ∂Ea(t)
∂ ˆw(2)
aij (t)

∂Ea(t)
∂ ˆw(2)
akj (t)
Nhc(cid:88)

ea(t)

r=1

=

∂Ea(t)
ˆJ(t)

∂ ˆJ(t)
∂uk(t)

∂uk(t)
∂ ˆw(2)
akj (t)

=

(cid:20)
ˆw(2)
cr

(t)

1
2

(1 − φ2
cr

(t)) ˆw(1)

cr,m+k

(cid:21)

(t)

φaj (t).

(3.5)

4. Lyapunov stability analysis of ADHDP

In this section we employ Lyapunov function approach to evaluate the stability
of dynamical systems. The applied Lyapunov analysis allows to establish the UUB
property without deriving the explicit solution of the state equations.

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

7

4.1. Basics of the Lyapunov approach. Let w∗
that is, the following holds: w∗
that the desired ultimate objective Uc = 0 corresponds to success then w∗
arg min ˆwa

a denote the optimal weights,
(cid:13)
(cid:13)
(cid:13); we assume
a =

(cid:13)
(cid:13)α ˆJ(t) + r(t) − ˆJ(t − 1)
(cid:13)
c = arg min ˆwc

c , w∗

(cid:13)
(cid:13)
ˆJ(t)
(cid:13)
(cid:13)
(cid:13).
(cid:13)

Consider the weight estimation error over full design, that is, over both critic
and action networks of the following form: ˜w(t) := ˆw(t) − w∗. Then equations
(3.1), (3.2), (3.3) and (3.5) deﬁne a dynamical system of estimation errors for some
nonlinear function F in the following form

˜w(t + 1) = ˜w(t) − F ( ˆw(t − 1), ˆw(t), φ(t − 1), φ(t)) .

(4.1)

Deﬁnition 1. A dynamical system is said to be uniformly ultimately bounded with
ultimate bound b > 0, if for any a > 0 and t0 > 0, there exists a positive number
N = N (a, b) independent of t0, such that (cid:107) ˜w(t)(cid:107) ≤ b for all t ≥ N + t0 whenever
(cid:107) ˜w(t0)(cid:107) ≤ a.

In the present study, we make use of a theorem concerning the UUB property
(Sarangapani, 2006). Detailed proof of this theorem appears in (Michel & Hou & Liu,
2008). We adapt the notation for our situation and address the special case of a
discrete dynamical systems as given in (4.1).

Theorem 1. (UUB property of a discrete dynamical system) If, for system
(4.1), there exists a function L( ˜w(t), t) such that for all ˜w(t0) in a compact set K,
L( ˜w(t), t) is positive deﬁnite and the ﬁrst diﬀerence, ∆L( ˜w(t), t) < 0 for (cid:107) ˜w(t0)(cid:107) >
b, for some b > 0, such that b-neighborhood of ˜w(t) is contained in K, then the
system is UUB and the norm of the state is bounded to within a neighborhood of b.

Based on this theorem, which gives a suﬃcient condition, we can determine the
UUB property of the dynamical system selecting an appropriate function L. For
this reason, we ﬁrst consider all components of our function candidate separately
and investigate their properties, and thereafter we study the behavior of L function
to match the condition from Theorem 1.

4.2. Preliminaries. In this subsection we introduce four lemmas which will be
used in the proof of our main theorem.

Assumption 1. Let w∗
works. Assume they are bounded, i.e., (cid:107)w∗

c be the optimal weights for action and critic net-
and (cid:107)w∗

a and w∗

.

c (cid:107) ≤ wmax

a(cid:107) ≤ wmax

a

Lemma 1. Under Assumption 1, the ﬁrst diﬀerence of L1(t) = 1
lc

tr

is expressed by

∆L1(t) = −α2 (cid:107)ζc(t)(cid:107)2 −
(cid:13)
2
(cid:13)
(cid:13)

c (t − 1)φc(t − 1)

ˆw(2)

(cid:16)

1 − α2lc (cid:107)φc(t)(cid:107)2(cid:17) (cid:13)
(cid:13)α ˆw(2)
(cid:13)
(cid:13)
(cid:13)αw∗(2)
(cid:13)

c φc(t) + r(t) − ˆw(2)

+

c (t)φc(t) + r(t)−

c (t − 1)φc(t − 1)

(cid:13)
2
(cid:13)
(cid:13)

,

(4.2)

where ζc(t) = ˜w(2)
network.

c (t)φc(t) is the approximation error of the output of the critic

c
(cid:20)(cid:16)

(cid:17)T

˜w(2)

c (t)

(cid:21)
c (t)

˜w(2)

8

YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

Proof. (Lemma 1 ). Using (3.2) and taking into account that w∗(2)
pend on t, i.e., it is optimal for each time moment t, we get the following

c

does not de-

˜w(2)

c (t + 1) = ˆw(2)

∗

=

c (t + 1) − w(2)
(cid:104)
α ˆw(2)

c

˜w(2)

c (t) − αlcφc

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:105)T

.

(4.3)

Based on the last expression, we can ﬁnd the trace of multiplication of ˜w(2)
by itself in the following way:
(cid:20)(cid:16)

(cid:17)T

(cid:17)T

(cid:16)

(cid:21)

˜w(2)

c (t + 1)

˜w(2)

c (t + 1)

=

˜w(2)

c (t)

˜w(2)

c (t) −

tr

c (t + 1)

(cid:104)

2αlc ˜w(2)

c (t)φc(t)
c (cid:107)φc(t)(cid:107)2 (cid:13)
(cid:13)α ˆw(2)
(cid:13)

α2l2

α ˆw(2)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:105)T

+

c φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:13)
2
(cid:13)
(cid:13)

.

(4.4)

Since ˜w(2)
as follows:

c (t)φc(t) is a scalar, we can rewrite the middle term in the above formula

−2αlc ˜w(2)

c (t)φc(t)

c (t)φc(t) + r(t) − ˆw(2)

(cid:104)
α ˆw(2)

(cid:105)
c (t − 1)φc(t − 1)

=

(cid:18)(cid:13)
(cid:13)α ˆw(2)
(cid:13)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1) − α ˜w(2)

c (t)φc(t)

(cid:13)
2
(cid:13)
(cid:13)

−

lc

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

(cid:13)
(cid:13)α ˜w(2)
(cid:13)
(cid:18)(cid:13)
(cid:13)αw∗(2)
(cid:13)

lc

−

(cid:13)
(cid:13)α ˆw(2)
(cid:13)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

2(cid:19)

(cid:13)
(cid:13)
(cid:13)

=

c φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:13)
(cid:13)α ˆw(2)
(cid:13)

c (t)φc(t) + r(t) − ˆw(2)

(cid:13)
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2(cid:19)

− α2 (cid:107)ζc(t)(cid:107)2 −

.

(4.5)

Here the deﬁnition of ˜w(2)
expression.

c (t) = ˆw(2)

c (t) − w∗(2)

c

is applied to obtain the above

Now let us consider the ﬁrst diﬀerence of L1(t) in the form

∆L1(t) =

(cid:20)(cid:16)

1
lc

˜w(2)

c (t + 1)

(cid:17)T

˜w(2)

c (t + 1) −

(cid:16)

˜w(2)

c (t)

(cid:17)T

(cid:21)

.

˜w(2)

c (t)

(4.6)

Substituting the results for
ment of the lemma, as required.

(cid:16)

˜w(2)

c (t + 1)

(cid:17)T

˜w(2)

c (t + 1), ﬁnally we get the state-
(cid:3)

Lemma 2. Under Assumption 1, the ﬁrst diﬀerence of L2(t) = 1
laγ1

tr

(cid:20)(cid:16)

˜w(2)

a (t)

(cid:17)T

(cid:21)
a (t)

˜w(2)

is bounded by

∆L2(t) ≤

(cid:18)

(cid:18)

−

1
γ1

1 − la (cid:107)φa(t)(cid:107)2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

2(cid:19) (cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
c (t)C(t)
(cid:13)
2(cid:19)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

4 (cid:107)ζc(t)(cid:107)2 + 4

(cid:13)
(cid:13)w∗(2)
(cid:13)

c φc(t)

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

,

+

(4.7)

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

9

where ζa(t) = ˜w(2)
a (t)φa(t) is the approximation error of the action network output
and γ1 > 0 is a weighting factor; C(t) is the Nhc × n matrix with coeﬃcients
Cij(t) = 1
2

ci,m+j (t), where i = 1 . . . Nhc , and j = 1 . . . n.

(cid:0)1 − φ2
ci

(t)(cid:1) ˆw(1)

Proof. (Lemma 2 ). Let us consider the weights from the hidden layer to output
layer of the action network which are updated according to (3.5)

˜w(2)

a (t + 1) = ˆw(2)

a (t + 1) − w∗(2)

a = ˆw(2)

a (t) − laφa(t) ˆw(2)

− w∗(2)

a = ˜w(2)

a (t) − laφa(t) ˆw(2)

c (t)C(t)

ˆw(2)

c (t)φc(t)

.

(4.8)

c (t)C(t)
(cid:104)

(cid:105)T

(cid:104)
ˆw(2)

c (t)φc(t)
(cid:105)T

Based on this expression, it is easy to see that

a (t + 1))T ˜w(2)

= ( ˜w(2)

a (t))T ˜w(2)

a (t)+

(cid:104)

( ˜w(2)
tr
a (cid:107)φa(t)(cid:107)2 (cid:13)
(cid:13) ˆw(2)
l2
(cid:13)

(cid:105)

a (t + 1)
2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

c (t)C(t)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

− 2la ˆw(2)

c (t)C(t)

(cid:104)

ˆw(2)

c (t)φc(t)

(cid:105)T

ζa(t).

(4.9)

Here the last formula is based on the assumption that all vector multiplications

are under trace function.

Now let us consider the ﬁrst diﬀerence of function L2(t), that is, the following

expression

∆L2(t) =

1
laγ1

tr

(cid:104)
( ˜w(2)

a (t + 1))T ˜w(2)

a (t + 1) − ( ˜w(2)

a (t))T ˜w(2)

a (t)

(cid:105)

.

(4.10)

After substituting the appropriate terms in the last formula, we get

∆L2(t) =

(cid:18)

1
γ1

la (cid:107)φa(t)(cid:107)2(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
c (t)C(t)
(cid:13)

− 2 ˆw(2)

c (t)C(t)

Consider the last term of (4.11)

(cid:104)
ˆw(2)

c (t)φc(t)

(cid:105)T

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

2(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:19)

ζa(t)

.

(4.11)

− 2 ˆw(2)

c (t)C(t)

(cid:104)

ˆw(2)

c (t)φc(t)

(cid:105)T

ζa(t) =

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)φc(t) − ˆw(2)

(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)
(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

c (t)φc(t)

(cid:13)
(cid:13) ˆw(2)
(cid:13)

−

(cid:13)
2
(cid:13)
(cid:13)

−

After substituting this formula into ∆L2, we get

∆L2(t) =

(cid:18)

1
γ1

la (cid:107)φa(t)(cid:107)2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
c (t)C(t)
(cid:13)

2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)φc(t)

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)φc(t) − ˆw(2)

c (t)C(t)ζa(t)

(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

−

(cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)φc(t)

2(cid:19)

(cid:13)
(cid:13)
(cid:13)

.

Notice that

.(4.12)

(4.13)

10 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

≤

c (t)C(t)ζa(t)

(cid:13)
2
(cid:13)
(cid:13)

−

(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

≤

2

+

c (t)φc(t) − ˆw(2)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:17)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:16)
˜w(2)
(cid:13)
(cid:13)
(cid:16)(cid:13)
(cid:13) ˜w(2)
(cid:13)
4 (cid:107)ζc(t)(cid:107)2 + 4

c (t) + w∗(2)

c
(cid:13)
(cid:13)
(cid:13) +
c (t)φc(t)
(cid:13)
(cid:13)w∗(2)
(cid:13)

c φc(t)

2

2

c (t)C(t)ζa(t)

+

φc(t)

(cid:13)
2
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
2
(cid:13) ˆw(2)
(cid:13)
(cid:13)
c (t)C(t)ζa(t)
(cid:13)
(cid:13)
(cid:13)
(cid:17)2
(cid:13)w∗(2)
(cid:13)
(cid:13)
c φc(t)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)
(cid:13)
2
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

(cid:13)
(cid:13) ˆw(2)
(cid:13)

+

+

.

≤

(4.14)

Finally we get the following bound for ∆L2(t), as required:

∆L2(t) ≤

(cid:18)

(cid:18)

−

1
γ1

1 − la (cid:107)φa(t)(cid:107)2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

2(cid:19) (cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
c (t)C(t)
(cid:13)
2(cid:19)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

4 (cid:107)ζc(t)(cid:107)2 + 4

(cid:13)
(cid:13)w∗(2)
(cid:13)

c φc(t)

(cid:13)
2
(cid:13)
(cid:13)

+

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
c (t)C(t)ζa(t)
(cid:13)

.

+

(4.15)

(cid:3)

Remark 1. If we introduce the following normalization for the network’s weights
(cid:13)
(cid:13)( ˆw(2)
(cid:13)
1 and 2, we can readily obtain the results given by (Liu et al., 2012).

= 1 and ﬁx the weights of the input layer, then applying Lemmas

c (t))T C(t)

(cid:13)
2
(cid:13)
(cid:13)

Lemma 3. Under Assumption 1, the ﬁrst diﬀerence of L3(t) = 1
lcγ2

tr

(cid:20)(cid:16)

is bounded by

(cid:17)T

˜w(1)

c (t)

(cid:21)
c (t)

˜w(1)

∆L3(t) ≤

(cid:18)

α2lc

1
γ2

(cid:13)
(cid:13)α ˆw(2)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)α ˆw(2)
c (t)y(t)aT (t)
(cid:13)
(cid:13)
(cid:13)

+ α

(cid:13)
(cid:13) ˜w(1)
(cid:13)

α

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:13)
2
(cid:13)
(cid:13)

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2 +

c (t)φc(t) + r(t) − ˆw(2)

(cid:13)
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)

2(cid:19)

,

(4.16)

where γ2 > 0 is a weighting factor and a(t) is a vector, with ai(t) = 1
2
for i = 1 . . . Nhc.

(cid:0)1 − φ2
ci

(t)(cid:1) ˆw(2)

ci (t)

Proof. (Lemma 3 ). Let us consider the weight update rule of the critic network
between input layer and hidden layer in the form

ˆw(1)

c (t + 1) = ˆw(1)

c (t) − αlc

(cid:16)

α ˆw(2)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:17)T

B(t),

(4.17)

(t)) ˆw(2)
where Bij(t) = 1
Following the same approach as earlier, we can express ˜w(1)

2 (1 − φ2
ci

ci (t)yj(t), for i = 1, . . . , Nhc , j = 1, . . . , m + n.

c (t + 1) by

˜w(1)

c (t + 1) = ˆw(1)
c (t)φc(t) + r(t) − ˆw(2)

(cid:16)

αlc

α ˆw(2)

c (t + 1) − w∗(1)

c = ˜w(1)
c (t) −
(cid:17)T

c (t − 1)φc(t − 1)

B(t).

(4.18)

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

11

For convenience, we introduce the following notation BT (t)B(t) = yT (t)aT (t)a(t)y(t) =

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2. Then the trace of multiplication can be written as

(cid:20)(cid:16)

tr

˜w(1)

c (t + 1)

(cid:17)T

˜w(1)

c (t + 1)

(cid:21)

(cid:16)

=

(cid:17)T

˜w(1)

c (t)

˜w(1)

c (t) +

α2l2
c

2αlc

(cid:13)
(cid:13)α ˆw(2)
(cid:13)
(cid:16)
α ˆw(2)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

BT (t)B(t) −

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

BT (t) ˜w(1)

c (t).

(4.19)

(cid:13)
2
(cid:13)
(cid:13)
(cid:17)

Using the property of trace function, that is, the following tr
(cid:16)
˜w(1)

, we can express the last term of (4.19) as follows:

c (t)y(t)aT (t)

(cid:17)

tr

(cid:16)

y(t)aT (t) ˜w(1)

c (t)

=

(cid:17)

(cid:16)

− 2αlc
(cid:18)(cid:13)
(cid:13)α ˆw(2)
(cid:13)

αlc

α ˆw(2)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1) − ˜w(1)

(cid:17)

y(t)aT (t) ˜w(1)

c (t) =
(cid:13)
2
c (t)y(t)aT (t)
(cid:13)
(cid:13)
2(cid:19)

−

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13) ˜w(1)
(cid:13)

(cid:13)
2
c (t)y(t)aT (t)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)α ˆw(2)
(cid:13)

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

. (4.20)

Therefore, using (4.19), (4.20), the ﬁrst diﬀerence of L3(t) can be bounded by

∆L3(t) ≤

(cid:18)

α2lc

1
γ2

(cid:13)
(cid:13)α ˆw(2)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)α ˆw(2)
c (t)y(t)aT (t)
(cid:13)
(cid:13)
(cid:13)

+ α

(cid:13)
(cid:13) ˜w(1)
(cid:13)

α

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:13)
2
(cid:13)
(cid:13)

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2 +

c (t)φc(t) + r(t) − ˆw(2)

(cid:13)
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)

2(cid:19)

.

(4.21)

(cid:3)

Lemma 4. Under Assumption 1, the ﬁrst diﬀerence of L4(t) = 1
laγ3

tr

(cid:20)(cid:16)

˜w(1)

a (t)

(cid:17)T

(cid:21)
a (t)

˜w(1)

is bounded by

∆L4(t) ≤

(cid:18)

la

1
γ3

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
(cid:13)
c (t)φc(t)
(cid:13)

2(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
c (t)C(t)DT (t)
(cid:13)
(cid:13)

+

(cid:13)
2
c (t)C(t)DT (t)
(cid:13)
(cid:13)

(cid:107)x(t)(cid:107)2 +

2 (cid:13)
(cid:13) ˜w(1)
(cid:13)

(cid:13)
(cid:13)
a (t)x(t)
(cid:13)

2(cid:19)

,

(4.22)

where γ3 > 0 is a weighting factor; and Dij(t) = 1
2
1 . . . Nha and j = 1 . . . n.

(cid:0)1 − φ2
ai

(t)(cid:1) ˆw(2)

aji(t) for i =

Proof. (Lemma 4 ). Let us consider the weights from the input layer to the hidden
layer of the action network

˜w(1)

a (t+1) = ˆw(1)

a (t+1)−w∗(1)

a = ˜w(1)

a (t)−la ˆw(2)

c (t)φc(t)D(t)C T (t)

(cid:16)

(cid:17)T

ˆw(2)

c (t)

xT (t).
(4.23)

12 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

Let us consider

(cid:104)

tr

( ˜w(1)

a (t + 1))T ˜w(1)

a (t + 1)

(cid:105)

= ( ˜w(1)

a (t))T ˜w(1)

a (t) +

l2
a

c (t)φc(t)

(cid:13)
(cid:13) ˆw(2)
(cid:13)
2la ˆw(2)

2 (cid:13)
(cid:13) ˆw(2)
(cid:13)
c (t)C(t)DT (t)φT

(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
c (t)C(t)DT (t)
(cid:13)
(cid:13)
(cid:17)T

(cid:16)

c (t)

ˆw(2)

c (t)

(cid:107)x(t)(cid:107)2 −

˜w(1)

a (t)x(t).

(4.24)

We obtained the last term since tr(AT B + BT A) = tr(AT B) + tr([AT B]T ) =
2 tr(AT B) and tr(AB) = tr(BA).

The last term in (4.23) can be transformed into the form:

−2la ˆw(2)
(cid:18)(cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)C(t)DT (t)φT
(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

c (t)

la

+

(cid:16)

(cid:17)T

ˆw(2)

c (t)

c (t)C(t)DT (t)

˜w(1)

a (t)x(t) ≤
2 (cid:13)
(cid:13)
(cid:13) ˜w(1)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
a (t)x(t)
(cid:13)

2(cid:19)

.

(4.25)

Based on the last result, we can obtain the upper bound for ∆L4(t), which is given
in the statement of the lemma:

∆L4(t) ≤

(cid:18)

la

1
γ3

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)

+

c (t)C(t)DT (t)

(cid:13)
(cid:13)
c (t)φc(t)
(cid:13)

2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
c (t)C(t)DT (t)
(cid:13)
(cid:13)

(cid:107)x(t)(cid:107)2 +

(cid:13)
(cid:13)
(cid:13)

2 (cid:13)
(cid:13) ˜w(1)
(cid:13)

(cid:13)
(cid:13)
a (t)x(t)
(cid:13)

2(cid:19)

.

(4.26)

(cid:3)

4.3. Stability analysis of the dynamical system. In this section we introduce
a candidate of Lyapunov function for analyzing the error estimation of the system.
To this aim, we utilize the following auxilary function L = L1 + L2 + L3 + L4.

Theorem 2. (Main Theorem) Let the weights of the critic network and the action
network are updated according to the gradient descent algorithm, and assume that
the reinforcement signal is a bounded semideﬁnite function. Then under Assump-
tion 1, the errors between the optimal networks weights w∗
c and their estimates
ˆwa(t), ˆwc(t) are uniformly ultimately bounded (UUB), if the following conditions
are fulﬁlled:

a, w∗

lc < min

t

(cid:16)

α2γ2

(cid:107)φc(t)(cid:107)2 + 1
γ2

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2(cid:17) ,

γ2 − α

(4.27)

la < min

t

γ3 − γ1
(cid:13)
c (t))T C(t)(cid:107)2 (cid:107)φa(t)(cid:107)2 + γ1
(cid:13) ˆw(2)
(cid:13)

γ3 (cid:107)( ˆw2

c (t)C(t)DT (t)

(4.28)

(cid:13)
2
(cid:13)
(cid:13)

(cid:107)x(t)(cid:107)2

Proof. (Theorem 2 ) At ﬁrst, let us collect all terms of ∆L(t) based on the results
of lemmas 1 - 4. Hence ∆L(t) is bounded by

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

13

∆L(t) ≤

(cid:110)
−α2 (cid:107)ζc(t)(cid:107)2 −

− ˆw(2)

(cid:13)
2
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)
1 − la (cid:107)φa(t)(cid:107)2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

+

−

(cid:18)

(cid:26)

1
γ1

1 − α2lc (cid:107)φc(t)(cid:107)2(cid:17) (cid:13)
(cid:16)
(cid:13)α ˆw(2)
(cid:13)
(cid:13)
(cid:13)αw∗(2)
(cid:13)

c φc(t) + r(t) − ˆw(2)

c (t)φc(t) + r(t)−

c (t − 1)φc(t − 1)

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

+

c (t)C(t)

c (t)φc(t)

(cid:13)
2
(cid:13)
(cid:13)

+ 4 (cid:107)ζc(t)(cid:107)2

+4

(cid:13)
(cid:13)w∗(2)
(cid:13)

(cid:13)
2
(cid:13)
c φc(t)
(cid:13)

+

(cid:13)
(cid:13) ˆw(2)
(cid:13)

c (t)C(t)ζa(t)

r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:13)
2
(cid:13)
(cid:13)

(cid:110)

α2lc

(cid:13)
(cid:13)α ˆw(2)
(cid:13)

c (t)φc(t)+

(cid:13)
(cid:13) ˜w(1)
(cid:13)

(cid:13)
2
c (t)y(t)aT (t)
(cid:13)
(cid:13)

+

2(cid:19) (cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13)
1
γ2

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

+

α

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

+

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2 + α
2(cid:27)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
c (t)φc(t)
(cid:13)

2 (cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
c (t)C(t)DT (t)
(cid:13)
(cid:13)

(cid:13)
c (t)C(t)DT (t)
(cid:13)
(cid:13)

2 (cid:13)
(cid:13) ˜w(1)
(cid:13)

a (t)x(t)

2(cid:27)

(cid:13)
(cid:13)
(cid:13)

.

(cid:107)x(t)(cid:107)2 +

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)φc(t)
(cid:13)

+

(4.29)

(cid:13)
(cid:13)α ˆw(2)
(cid:13)
(cid:26)
(cid:13)
(cid:13) ˆw(2)
(cid:13)

la

1
γ3
(cid:13)
(cid:13) ˆw(2)
(cid:13)

The ﬁrst diﬀerence of L(t) can be rewritten as

∆L(t) ≤ −(α2 −

(cid:18)

) (cid:107)ζc(t)(cid:107)2 −

4
γ1

1 − α2lc (cid:107)φc(t)(cid:107)2 −

c (t)φc(t) + r(t) − ˆw(2)

c (t − 1)φc(t − 1)

(cid:107)φa(t)(cid:107)2 −

(cid:19) (cid:13)
(cid:13)α ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)C(t)
(cid:13)

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
(cid:13)w∗(2)
(cid:13)

α
γ2
la
γ1
4
γ1
(cid:13)
(cid:13)αw∗(2)
c φc(t) + r(t) − ˆw(2)
(cid:13)
(cid:13)
(cid:13)
α
2
(cid:13) ˜w(1)
(cid:13)
(cid:13)
(cid:13)
γ2

(cid:13)
2
(cid:13)
c φc(t)
(cid:13)

c (t)y(t)

1
γ1

+

(cid:107)a(t)(cid:107)2 +

(cid:13)
(cid:13) ˆw(2)
(cid:13)

la
γ3

(cid:13)
(cid:13) ˆw(2)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

c (t)C(t)

(cid:107)ζa(t)(cid:107)2 +

(cid:13)
2
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)

1
γ3

c (t)C(t)DT (t)

+

α2lc
γ2
(cid:13)
(cid:13) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

−

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2 −

2 (cid:18) 1
(cid:13)
(cid:13)
c (t)φc(t)
(cid:13)
γ1
(cid:19)

−

(cid:13)
2
c (t)C(t)DT (t)
(cid:13)
(cid:13)

(cid:107)x(t)(cid:107)2 −

1
γ3

+

.

(4.30)

(cid:13)
(cid:13)
(cid:13)

2 (cid:13)
(cid:13) ˜w(1)
(cid:13)

(cid:13)
2
(cid:13)
a (t)x(t)
(cid:13)

To guarantee that the second and the third terms in the last expression are

negative, we need to choose learning rates in the following manner

1 − α2lc (cid:107)φc(t)(cid:107)2 −

α2lc
γ2

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2 −

α
γ2

> 0.

Therefore,

lc < min

t

(cid:16)

α2γ2

γ2 − α

(cid:107)φc(t)(cid:107)2 + 1
γ2

(cid:107)a(t)(cid:107)2 (cid:107)y(t)(cid:107)2(cid:17) .

In particular, γ2 > α. Similarly, for the action network we obtain:

1
γ1

−

1
γ1

la

(cid:13)
(cid:13)( ˆw(2)
(cid:13)

c (t))T C(t)

(cid:13)
2
(cid:13)
(cid:13)

(cid:107)φa(t)(cid:107)2 −

la
γ3

(cid:13)
(cid:13)D(t)C T (t) ˆw(2)
(cid:13)

(cid:13)
2
(cid:13)
c (t)
(cid:13)

(cid:107)x(t)(cid:107)2 −

(4.31)

(4.32)

1
γ3

> 0,

(4.33)

14 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

la < min

t

(cid:13)
(cid:13)( ˆw(2)
(cid:13)

c (t))T C(t)

(cid:13)
2
(cid:13)
(cid:13)

γ3

γ3 − γ1
(cid:13)
(cid:13) ˆw(2)
(cid:107)φa(t)(cid:107)2 + γ1
(cid:13)

c (t)C(t)DT (t)

(4.34)

(cid:107)x(t)(cid:107)2

(cid:13)
2
(cid:13)
(cid:13)

In particular, γ3 > γ1. Notice that the norm of sum can be bounded by sum of
norms, thus we have the following
(cid:13)
(cid:13)αw∗(2)
(cid:13)
4α2 (cid:13)
(cid:13)w∗(2)
(cid:13)

c φc(t) + r(t) − ˆw(2)
(cid:13)
2
(cid:13)
c φc(t)
(cid:13)

c (t − 1)φc(t − 1)

c (t − 1)φc(t − 1)

+ 4r2(t) + 2

(cid:13)
(cid:13) ˆw(2)
(cid:13)

(4.35)

(cid:13)
2
(cid:13)
(cid:13)

≤

.

Let C, wa1, wa2, wc1, φa, y, x, a, D be upper bounds of C(t), ˜w(1)
c (t), φa(t), y(t), x(t), a(t), D(t), correspondingly; while wc2=max {w∗(2)

a (t),
c2 },
c (t). Finally, we obtain the following bound:

is the upper bound of ˆw(2)

˜w(1)
where w(M )

c

c2

(cid:13)
2
(cid:13)
(cid:13)
a (t), ˜w(2)
, w(M )

(cid:13)
2
(cid:13)
(cid:13)

(cid:107)ζa(t)(cid:107)2 +

c (t)C(t)

(cid:13)
(cid:13) ˆw(2)
(cid:13)

+

1
γ1

(cid:13)
(cid:13)w∗(2)
(cid:13)

(cid:13)
2
(cid:13)
c φc(t)
(cid:13)

4
γ1
(cid:13)
c φc(t) + r(t) − ˆw(2)
(cid:13)αw∗(2)
(cid:13)
(cid:13)
(cid:13)
α
2
(cid:13) ˜w(1)
(cid:13)
(cid:13)
(cid:13)
γ2
(cid:18) 4
(cid:19)
γ1

+ 4α2 + 2

c (t)y(t)

(cid:107)a(t)(cid:107)2 +

(cid:13)
2
(cid:13)
c (t − 1)φc(t − 1)
(cid:13)
(cid:13)
(cid:13) ˆw(2)
(cid:13)
1
γ1

1
γ3

c (t)C(t)DT (t)

+

(wc2φc)2 + 4r2 +

(wc2C wa2φa)2 +

(cid:13)
(cid:13)
(cid:13)

2 (cid:13)
(cid:13) ˜w(1)
(cid:13)
α
γ2

1
γ3

(wc2C Dwa1x)2 = M.

(cid:13)
2
(cid:13)
a (t)x(t)
(cid:13)

≤

(wc1y a)2 +

(4.36)

> 0, that is, γ1 > 4

Therefore, if α2 − 4
γ1

α2 and α ∈ (0, 1), then for la and lc with
, we get ∆L(t) < 0. Based on

constraints from (4.32), (4.34) and (cid:107)ζc(t)(cid:107)2 > M
α2− 4
γ1
Theorem 1, this means that the system of estimation errors is ultimately uniformly
bounded.

(cid:3)

4.4. Interpretation of the results. It is to be emphasized that present results
do not pose any restrictions on the discount factor α, as opposed to with (Liu et
al., 2012). The choice of the discount factor depends on the given problem and the
absence of any constraints on this factor is a clear advantage of our approach. A
constraint on the discount factor can reduce the performance of the design. Also it
should be mentioned that parameters γ1, γ2, and γ3 allow ﬁne-tuning of the learning
in diﬀerent layers of the networks, thus leading to further improved performance.
Further consequences of this advantage will be the subject of our future research.

5. Simulation study

In this section, we consider two examples and compare our results with previous
studies.
In our case, we allow adaptation in the whole MLP, and denote this
approach AdpF ull. Previous studies by (Liu et al., 2012) employ partial adaptation
in the output layers only, so we call it AdpP art. We use a relatively easy example
for a linear system, similar to (Liu et al., 2012), to demonstrate the similarity

COMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

15

(a)

(b)

Figure 4. Behavior of the linear system, Eq. 5.1, during training
using: (a) AdpF ull; (b) AdpP art .

between AdpF ull and AdpP art. Then we introduce a more complicated example,
to demonstrate the advantages of the more general results by AdpF ull.

5.1. Linear problem. Following (Liu et al., 2012), we consider a system described
by the linear discrete time state-space equation of the form:

xk+1 = 1.25xk + uk.

(5.1)

We apply ADHDP to stabilize this system. For this purpose we utilize two
neural networks, the parameters of which match the condition of Theorem 2. In
the implementations we use MATLAB environment. We choose the discount factor
as follows α = 0.9. The number of nodes in the hidden layer of both networks are
set to Nhc = Nhc = 6. In the training process, the learning rates are lc = la = 0.1.
Like in (Liu et al., 2012), the initial state is chosen as x(0) = 1, and the weights
of both critic and action networks are set randomly. The reinforcement learning
signal is of the form rk = 0.04x2
k. The convergence of the state, control
and cost-to-go function for approaches from this paper and (Liu et al., 2012) are
shown in Fig. 4(a) and Fig. 4(b), correspondingly. At each time step, we perform
a ﬁxed number of iterations to adapt the critic and action networks. The number
of internal iterations are selected according to the given problem. In the case of
the linear control we chose smaller number of iterations (up to 50), while for more
diﬃcult problems we have 100 iterations.

k + 0.01u2

After learning is completed, we ﬁx weights of both networks and test the con-
troller. Additionally, we compare performance of the controller with that in (Liu
et al., 2012). The corresponding graphs are shown in Fig. 6(a) and in Fig. 6(b).
Our results show that AdpF ull and AdpP art control system perform similarly and
they reach the equilibrium state fast, within 5 time steps. Detailed analysis shows,
that AdpF ull reaches the target state in average one step earlier.

In the linear problem, the linear quadratic regulator (LQR) control provides the
exact solution (Bryson & Ho, 1975). Therefore, it is of interest to compare the re-
sults obtained by our ADHDP controller and the LQR controller. We implemented
and compared these control approaches and here summarize the results. Our anal-
ysis shows that the ADHDP control is very close to the exact optimal solution given

0510152025303540455000.511.52time stepsx05101520253035404550−2−101time stepsu05101520253035404550−0.4−0.3−0.2−0.10time stepspredicted J05101520253035404550−2−101time stepsu05101520253035404550−0.4−0.3−0.2−0.10time stepspredicted J0510152025303540455000.511.52time stepsx16 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

Figure 5. Illustration of the cart-pole balancing system.

(a)

(b)

Figure 6. The state trajectory (x) and control action (u) for the
linear system Eq. 5.1 using: (a) AdpF ull; (b) AdpP art .

by LQR. This conclusion is in agreement with the results described by (Liu, Sun,
Si, Guo & Mei, 2012) for the linear case.

5.2. The cart-pole balancing problem. We present the case of a nonlinear
control problem to illustrate the diﬀerence between our current study and previous
approaches (Liu et al., 2012). We consider the cart-pole balancing problem, which is
a very popular benchmark for applying methods of ADP and reinforcement learning
(He, 2011). We consider a system almost the same as in (He, 2011); the only
diﬀerence is that for simplicity we neglect friction. The model shown in Fig. 5 can
be describe as follows

d2θ
dt2 =

d2x
dt2 =

g sin θ + cos θ

(cid:16) −F −mpl ˙θ2 sin θ
mc+mp
(cid:17)

l

mc+mp

(cid:16) 4
3 − mp cos2 θ
F + mpl( ˙θ2 sin θ − ¨θ cos θ)
mc + mp

(cid:17)

,

,

(5.2)

(5.3)

where g = 9.8 m/s2, the acceleration due to gravity; mc = 1.0 kg, the mass of the
cart; mp = 0.1 kg, the mass of the pole; l = 0.5 m, the half-pole length; F = ±10
N, force applied to cart center of mass.

This model has four state variables (θ(t), x(t), ˙x(t), ˙θ(t)), where θ(t) is the angle
of the pole with respect to the vertical position, x(t) is the position of the cart on
the track, ˙x(t) is the cart velocity and ˙θ(t) is the angular velocity.

In our current simulation, a run includes 100 consecutive trials. A run is con-
sidered successful if the last trial lasted 600 time steps where one time step is 0.02
s. A trial is a complete process from start to fall. System is considered fallen if

qFxx0=005101520253035404550−1.5−1−0.500.5timeaction0510152025303540455000.51timeposition05101520253035404550−1.5−1−0.500.5timeaction0510152025303540455000.51timepositionCOMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

17

the pole is outside the range of [−12◦, 12◦] and/or the cart is moving beyond the
range [−2.4, 2.4] m in reference to the central position on the track. The controller
can apply force to the center of mass of the system with ﬁxed magnitude in two
directions. In this example, a binary reinforcement signal r(t) is considered. We
utilized similar structure of critic and action networks as in the previous example,
therefore it is possible to set the same network parameters.

Figs. 7(a) - 7(d) show examples of the time dependence of the action force,
the position, and the angle trajectories, respectively. These ﬁgures correspond to
simulations which are produced after training is completed and weights are ﬁxed.
In the case of successful control by ADHDP, the angle oscillates within limits ±0.4
degrees. This control outcome is quite reasonable, as the observed angle deviation
is more than an order of magnitude below than the required ±12 degrees threshold
speciﬁed in the description of the task.

Next, we demonstrate the diﬀerence between the control approaches in our
current study (AdpF ull) and the one described in (Liu et al., 2012) (AdpP art).
We select two initial position (0.85, 0, 0, 0) and (2, 0, 0, 0), as described next.
In
Figs. 7(a) - 7(b), controllers AdpF ull and AdpP art show similar performance; the
initial angle has small disturbance θ = 0.85 with respect to equilibrium position.
However, even in this case, one can see a small drift on the cart position from 0
to 0.15. This indicates that AdpF ull is able to properly stabilize the cart, but
AdpP art has some problem with this task.

By selecting initial condition θ = 2, we observe essential diﬀerences between
the two approaches, see in Figs. 7(c) - 7(d). Our AdpP art approach stabilizes the
cart after about 3000 steps. At the same time, the AdpP art approach produces
divergent behavior; after 6000 iterations the cart moves out of the allowed spatial
region [−2.4, 2.4]. This behavior is discussed in the concluding section.

6. Discussion and Conclusions

In this work, we introduce several generalized stability criteria for the ADHDP
system trained by gradient descent over the critic and action networks modeled by
MLPs. It is shown here that the proposed ADHDP design is uniformly ultimately
bounded under some constraints on the learning rates, but we do not discuss bounds
on the accuracy of estimation of the approximation of the J function. Our approach
is more general than the one available in the literature, as our system allows adapta-
tion across all layers of the networks. This generalization has important theoretical
and practical consequences.

• From theoretical point of view, it is known that an MLP with at least one
hidden layer is a universal approximator in a broad sense. However, by
assuming that the weights between the input and the hidden layer are not
adaptable, the generalization property of the network will be limited.

• As for practical aspects, the diﬀerence between our approach and previous
studies is demonstrated using two problems. An easy one with a linear
system to be controlled, and a more diﬃcult system with the cart-pole
balancing task.

• Our results show that the two approaches give very similar results for the
easier linear problem. However, we demonstrate signiﬁcant diﬀerences in
the performance of the two systems for more complicated tasks (pole bal-
ancing). In particular, with larger initial deviation in the pole angle, our

18 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

(a)

(c)

(b)

(d)

Figure 7. Simulated results of balancing the inverted pendulum
using: (a) AdpF ull stability criteria; initial angle is θ = 0.85; (b)
AdpP art stability criteria; initial angle is θ = 0.85; (c) AdpF ull
stability criteria; initial angle is θ = 2; (d) AdpP art stability cri-
teria; initial angle is θ = 2.

approach is able to balance the system. At the same time, the approach
using a simpliﬁed control system with non-adaptable weights between the
input and hidden layer is unable to solve this diﬃcult task.

These results show the power of the applied ADP approach when using the deep
It is expected that our results will be very
learning algorithm introduced here.
useful for training of the intelligent control and decision support systems, including
multi-agent platforms, leading to more eﬃcient real-time training and control.

References

[1] Abu-Khalaf, M. & Lewis, F. L. (2005). Nearly optimal control laws for nonlinear systems
with saturating actuators using a neural network HJB approach. Automatica, 41, 779-791.
[2] Al-Tamimi, A. & Lewis, F. L. & Abu-Khalaf, M. (2007) Model-free Q-learning designs for
linear discrete-time zero-sum games with application to H-inﬁnity control. Automatica, 43,
473-481.

[3] Barron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal function.

IEEE Transactions on Information Theory, 39, 930-944.

[4] Barron, A. (1994). Approximation and estimation bounds for artiﬁcial neural networks. Ma-

chine Learning, 14, 113-143.

[5] Barto, A. G. & Sutton, R. S. & Anderson, C. W. (1983). Neuronlike elements that can solve
diﬃcult learning control problems. IEEE Transactions on Systems Man and Cybernetics,
SMC, 13, 835846.

[6] Bertsekas, D. P. & Tsitsiklis, J. N. (1996). Neuro-dynamic programming, MA, Athena Scien-

tiﬁc.

[7] Bryson, A. E. & Ho, Y. C. (1975). Applied optimal control, Washington, DC: Hemisphere.
[8] He, H. (2011). Self-Adaptive Systems for Machine Intelligence, Wiley.

01002003004005006007008009001000−0.500.5timeaction01002003004005006007008009001000−101timetheta, degrees0100200300400500600700800900100000.0050.010.0150.02timeposition, meters01002003004005006007008009001000−0.0500.050.1timeaction01002003004005006007008009001000−0.500.51timetheta, degrees0100200300400500600700800900100000.10.2timeposition, meters0100020003000400050006000−0.2−0.100.10.2timeaction0100020003000400050006000−202timetheta, degrees010002000300040005000600000.20.40.60.8timeposition, meters0100020003000400050006000−0.1−0.0500.050.1timeaction0100020003000400050006000−2−1012timetheta, degrees0100020003000400050006000012timeposition, metersCOMPLETE STABILITY ANALYSIS OF A HEURISTIC ADP CONTROL DESIGN

19

[9] Kozma, R. & Sokolov, Y. (2013). Improved Stability Criteria of ADP Control for Eﬃcient
Context-Aware Decision Support Systems. Int. Conf. on Awareness Science and Technology
(iCAST2013), pp. 41-46, Aizu-Wakamatsu, Japan, November 2-4, 2013, IEEE Press.

[10] Lendaris, G. G. (2009). Adaptive dynamic programming approach to experience-based sys-

tems identiﬁcation and control. Neural Networks, 22(5), 822.

[11] Lewis, F. & Liu, D. (Eds) (2012). Reinforcement Learning and Approximate Dynamic Pro-

gramming for Feedback Control, Wiley-IEEE Press.

[12] Liu, F., Sun, J., Si, J. & Guo, W. & Mei, S. (2012). A boundness result for the direct heuristic

dynamic programming,Neural Networks 32, 229-235.

[13] Michel, A. & Hou, L. & Liu, D. (2008). Stability of dynamical system, Birkhauser.
[14] Powell, W. (2011). Approximate dynamic programming, Wiley.
[15] Prokhorov, D. & Wunsch, D. (1997). Adaptive critic Designs. IEEE Trans. on Neural Netw.,

8(5), 997-1007.

[16] Sarangapani, J. (2006). Neural network control of nonlinear discrete-time systems, Taylor

and Francis.

[17] Si, J., Barto, A. G. & Powell, W. B. & Wunsch, D. C. (2004). Handbook of learning and

approximate dynamic programming. Piscataway, NJ: IEEE Press.

[18] Valenti, M. J. (2007). Approximate dynamic programming with applications in multi-agent

systems, PhD Dissertation, MIT.

[19] Venayagamoorthy, G. K. & Harley, R. G. & Wunsch, D. C. (2003). Dual heuristic program-
ming excitation neurocontrol for generatos in a multimachine power system. IEEE Trans. on
Industry Applications, 39(2), 382-394.

[20] Vrabie, D. & Lewis, F. L. (2009). Generalized policy iteration for continuous-time systems.

In Proceeding of international joint conference on neural networks, 3224-3231.

[21] Wang, D., Liu, D., Wei, Q. & Zhao, D. & Jin, N. (2012). Optimal control of unknown nonaﬃne
nonlinear discrete-time systems based on adaptive dynamic programming. Automatica, 48,
1825-1832.

[22] Werbos, P. J. (1974). Beyond regression: New Tools for Prediction and Analysis in the Behav-
ioral Science Ph.D. thesis, Committee on Applied Mathematics, Harvard Univ., Cambridge,
MA.

[23] Werbos, P. J. (1990). Consistency of HDP applied to a simple reinforcement learning problem.

Neural Networks, 3, 179-189.

[24] Werbos, P. J. (1992). Approximate dynamic programming for real-time control and neural
modeling. In: D. A. White, & Sofge, D. A. (Eds.), Handbook of intelligent control, New York,
Van Nostrand Reinhold.

[25] Werbos, P. J. (2012). Stable Adaptive Control Using New critic Designs. http://arxiv.org/

abs/adap-org/9810001.

[26] White, D. A. & Sofge, D. A. (Eds.) (1992). Handbook of Intelligent Control: Neural, Fuzzy,

and Adaptive Approaches, Chapter 3, 10 and 13.

[27] Zhang, H. & Wei, Q. & Liu, D. (2011). An iterative adaptive dynamic programming method
for solving a class of nonlinear zero-sum diﬀerential games. Automatica, 47(1), 207-214.
[28] Zhang, J., Zhang, H. & Luo, Y. & Liang, H. (2013). Nearly optimal control scheme using
adaptive dynamic programming based on generalized fuzzy hyperbolic model. ACTA Auto-
matica Sinica, 39(2), 142-148.

[29] Zhang, H., Liu, D. & Luo, Y. & Wang, D. (2013). Adaptive Dynamic Programming for

Control: Algorithms and Stability. London:Springer-Verlag.

20 YURY SOKOLOV, ROBERT KOZMA, LUDMILLA D. WERBOS, AND PAUL J. WERBOS

(Yury Sokolov) Department of Mathematical Sciences, The University of Memphis,

United States

E-mail address, Yury Sokolov: ysokolov@memphis.edu

(Robert Kozma) Department of Mathematical Sciences, The University of Memphis,

United States

E-mail address, Robert Kozma: rkozma@memphis.edu

(Ludmilla D. Werbos) IntControl LLC, Arlington, and CLION, The University of Mem-

phis, USA

E-mail address, Ludmilla D. Werbos: l.dalmat@gmail.edu

(Paul J. Werbos) CLION and the National Science Foundation (NSF), United States
E-mail address, Paul J. Werbos: werbos@ieee.org

