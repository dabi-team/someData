Out of Sight But Not Out of Mind:
An Answer Set Programming Based Online Abduction Framework for
Visual Sensemaking in Autonomous Driving

Jakob Suchan1,3 , Mehul Bhatt2,3 and Srikrishna Varadarajan3
1University of Bremen, Germany, 2 ¨Orebro University, Sweden
3CoDesign Lab / Cognitive Vision – www.codesign-lab.org

9
1
0
2

y
a
M
1
3

]
I

A
.
s
c
[

1
v
7
0
1
0
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

We demonstrate the need and potential of system-
atically integrated vision and semantics solutions
for visual sensemaking (in the backdrop of au-
tonomous driving). A general method for online vi-
sual sensemaking using answer set programming is
systematically formalised and fully implemented.
The method integrates state of the art in (deep
learning based) visual computing, and is developed
as a modular framework usable within hybrid ar-
chitectures for perception & control. We evaluate
and demo with community established benchmarks
KITTIMOD and MOT. As use-case, we focus on the
signiﬁcance of human-centred visual sensemaking
—e.g., semantic representation and explainability,
question-answering, commonsense interpolation—
in safety-critical autonomous driving situations.

1 MOTIVATION
Autonomous driving research has received enormous aca-
demic & industrial interest in recent years. This surge has
coincided with (and been driven by) advances in deep learn-
ing based computer vision research. Although deep learn-
ing based vision & control has (arguably) been successful
for self-driving vehicles, we posit that there is a clear need
and tremendous potential for hybrid visual sensemaking so-
lutions (integrating vision and semantics) towards fulﬁlling
essential legal and ethical responsibilities involving explain-
ability, human-centred AI, and industrial standardisation (e.g,
pertaining to representation, realisation of rules and norms).

Autonomous Driving: “Standardisation & Regulation”
As the self-driving vehicle industry develops, it will be nec-
essary —e.g., similar to sectors such as medical comput-
ing, computer aided design— to have an articulation and
community consensus on aspects such as representation, in-
teroperability, human-centred performance benchmarks, and
data archival & retrieval mechanisms.1 In spite of major in-

1

Within autonomous driving, the need for standardisation and ethical reg-
ulation has most recently garnered interest internationally, e.g., with the Fed-
eral Ministry of Transport and Digital Infrastructure in Germany taking a lead in
eliciting 20 key propositions (with legal implications) for the fulﬁlment of ethical
commitments for automated and connected driving systems [BMVI, 2018].

Figure 1: Out of sight but not out of mind; the case of hidden
entities: an occluded cyclist.

vestments in self-driving vehicle research, issues related to
human-centred’ness, human collaboration, and standardisa-
tion have been barely addressed, with the current focus in
driving research primarily being on two basic considerations:
how fast to drive, and which way and how much to steer.
This is necessary, but inadequate if autonomous vehicles are
to become commonplace and function with humans. Ethi-
cally driven standardisation and regulation will require ad-
dressing challenges in semantic visual interpretation, natural
/ multimodal human-machine interaction, high-level data an-
alytics (e.g., for post hoc diagnostics, dispute settlement) etc.
This will necessitate –amongst other things– human-centred
qualitative benchmarks and multifaceted hybrid AI solutions.

Visual Sensemaking Needs Both “Vision & Semantics”
We demonstrate the signiﬁcance of semantically-driven
methods rooted in knowledge representation and reasoning
(KR) in addressing research questions pertaining to explain-
ability and human-centred AI particularly from the viewpoint
of sensemaking of dynamic visual imagery. Consider the oc-
clusion scenario in Fig. 1:
Car (c) is in-front, and indicating to turn-right; during this time, per-
son (p) is on a bicycle (b) and positioned front-right of c and moving-
forward. Car c turns-right, during which the bicyclist < p, b > is
not visible. Subsequently, bicyclist < p, b > reappears.
The occlusion scenario indicates several challenges concern-
ing aspects such as: identity maintenance, making default as-
sumptions, computing counterfactuals, projection, and inter-
polation of missing information (e.g., what could be hypoth-

gets occludedreappears 
 
 
 
 
 
Figure 2: A General Online Abduction Framework / Conceptual Overview

realtime,

esised about bicyclist < p, b > when it is occluded; how can
this hypothesis enable in planning an immediate next step).
Addressing such challenges —be it realtime or post-hoc—
in view of human-centred AI concerns pertaining to ethics,
explainability and regulation requires a systematic integra-
tion of Semantics and Vision, i.e., robust commonsense rep-
resentation & inference about spacetime dynamics on the one
hand, and powerful low-level visual computing capabilities,
e.g., pertaining to object detection and tracking on the other.
Key Contributions. We develop a general and systematic
declarative visual sensemaking method capable of online
incremental, commonsense semantic
abduction:
question-answering and belief maintenance over dynamic vi-
suospatial imagery. Supported are (1–3):
(1). human-centric
representations semantically rooted in spatio-linguistic prim-
itives as they occur in natural language [Bhatt et al., 2013;
Mani and Pustejovsky, 2012];
(2). driven by Answer Set
Programming (ASP) [Brewka et al., 2011], the ability to ab-
ductively compute commonsense interpretations and expla-
nations in a range of (a)typical everyday driving situations,
e.g., concerning safety-critical decision-making;
(3). online
performance of the overall framework modularly integrating
high-level commonsense reasoning and state of the art low-
level visual computing for practical application in real world
settings. We present the formal framework & its implemen-
tation, and demo & empirically evaluate with community es-
tablished real-world datasets and benchmarks, namely: KIT-
TIMOD [Geiger et al., 2012] and MOT [Milan et al., 2016].

2 VISUAL SENSEMAKING:
A GENERAL METHOD DRIVEN BY ASP
Our proposed framework, in essence, jointly solves the prob-
lem of assignment of detections to tracks and explaining
overall scene dynamics (e.g. appearance, disappearance) in
terms of high-level events within an online integrated low-
level visual computing and high level abductive reasoning

framework (Fig. 2). Rooted in answer set programming, the
framework is general, modular, and designed for integration
as a reasoning engine within (hybrid) architectures designed
for real-time decision-making and control where visual per-
ception is needed as one of the several components. In such
large scale AI systems the declarative model of the scene dy-
namics resulting from the presented framework can be used
for semantic Q/A, inference etc. to support decision-making.

2.1 SPACE, MOTION, OBJECTS, EVENTS
Reasoning about dynamics is based on high-level representa-
tions of objects and their respective motion & mutual interac-
tions in spacetime. Ontological primitives for commonsense
reasoning about spacetime (Σst) and dynamics (Σdyn) are:
• Σst: domain-objects O = {o1, ..., on} represent the vi-
sual elements in the scene, e.g., people, cars, cyclists; el-
ements in O are geometrically interpreted as spatial en-
tities E= {ε1, ..., εn}; spatial entities E may be regarded
as points, line-segments or (axis-aligned) rectangles based
on their spatial properties (and a particular reasoning task
at hand). The temporal dimension is represented by time
points T = {t1, ..., tn}. MT oi = (εts , ..., εte ) represents
the motion track of a single object oi, where ts and te de-
note the start and end time of the track and εts to εte de-
notes the spatial entity (E) —e.g., the axis-aligned bound-
ing box—corresponding to the object oi at time points ts
to te. The spatial conﬁguration of the scene and changes
within it are characterised based on the qualitative spatio-
temporal relationships (R) between the domain objects. For
the running and demo examples of this paper, positional re-
lations on axis-aligned rectangles based on the rectangle al-
gebra (RA) [Balbiani et al., 1999] sufﬁce; RA uses the rela-
tions of Interval Algebra (IA) [Allen, 1983] RIA ≡ {before,
after, during, contains, starts, started by, ﬁnishes, ﬁnished by,
overlaps, overlapped by, meets, met by, equal} to relate two ob-
jects by the interval relations projected along each dimension
separately (e.g., horizontal and vertical dimensions).

EventsCamerasRadarsLIDARGPS...High-Level AbductionOntologySemantic Query ProcessingHypothesized SituationLow-LevelControlLow-Level Motion TrackingDeclarative Model of Scene DynamicsSpaceBlocked LanePredictionAssociationMotionObjectsMotion TracksMatchingBlocked VisibilityLane ChangesHidden EntitySudden Stop...OcclusionIdentityVisuo-spatial ConceptsAttachmentControl DecisionsVisual ProcessingObject DetactionSemantic SegmentationLane DetectionEgo-MotionassignstartendhaltresumestandbyHypotheses on Object InteractionsJoint Optimizationof Scene DynamicsObject TracksObservationsfor each t in T:slow_downchange_lane...emergency_breakTrk1Trk2t1t2Trk1Trk2t1t2t3passing behindgetting occluded...tstets+1εts+1MToi = {εts,..., εte}Trk1Trk2t1t2moving togetherPerceiveDecideInterpretOnlineVision andControlAlgorithm 1: Online Abduction(V, Σ)

Data: Visual imagery (
V
background knowledge Σ

Result: Visual Explanations (
∅

events

MT ←
for t

∅,
T do

H

←

), and
≡def Σdyn ∪
)

EX P

Σst

(also: Refer Fig 3)

Vt)
∅

observe(

∈
VOt ←
∅,
Pt ←
MLt ←
for trk
∈ MT t−1 do
ptrk
∈ VOt do
mltrk,obs ←
MLt ← MLt ∪

ptrk ←
Pt ← Pt ∪
for obs

kalman predict(trk)

calc IoU (ptrk, obs)

mltrk,obs

Abduce(<

assign
t
H
events

events
,
t
H
assign
[
t
∧
H
events

Σ

∧H
events

>), such that:
events
]
t

=
|

(Step 2)

VOt ∧Pt ∧MLt

H
MT t ←

← H
update(

return

EX P ←

<

H

∧H
events
t

∪ H
MT t−1,
events,

VOt,
>

MT

Hassign)

1

2

3

4

5
6
7

8
9
10

11

12

13

14

• Σdyn: The set of ﬂuents Φ = {φ1, ..., φn} and events Θ =
{θ1, ..., θn} respectively characterise the dynamic properties
of the objects in the scene and high-level abducibles (Table
1). For reasoning about dynamics (with <Φ, Θ>), we use a
variant of event calculus as per [Ma et al., 2014; Miller et al.,
2013]; in particular, for examples of this paper, the functional
event calculus fragment (Σdyn) of Ma et al. [2014] sufﬁces:
main axioms relevant here pertain to occurs-at(θ, t) denoting
that an event occurred at time t and holds-at(φ, v, t) denoting
that v holds for a ﬂuent φ at time t.2
• Σ: Let Σ ≡def Σdyn <Φ, Θ> ∪ Σst <O, E, T , MT , R>

2.2 TRACKING AS ABDUCTION
Scene dynamics are tracked using a detect and track ap-
proach: we tightly integrate low-level visual computing (for
detecting scene elements) with high-level ASP-based abduc-
tion to solve the assignment of observations to object tracks
in an incremental manner. For each time point t we generate
a problem speciﬁcation consisting of the object tracks and vi-
sual observations and use (ASP) to abductively solve the cor-
responding assignment problem incorporating the ontological
structure of the domain / data (abstracted with Σ). Steps 1–3
(Alg. 1 & Fig. 3) are as follows:

Step 1. FORMULATING THE PROBLEM SPECIFICATION
The ASP problem speciﬁcation for each time point t is given
by the tuple < VOt, Pt, MLt > and the sequence of events
(Hevents) before time point t.
• Visual Observations Scene elements derived directly from
the visual input data are represented as spatial entities E, i.e.,
VOt = {εobs1, ..., εobsn } is the set of observations at time t
(Fig. 3). For the examples and empirical evaluation in this
paper (Sec. 3) we focus on Obstacle / Object Detections
– detecting cars, pedestrians, cyclists, trafﬁc lights etc using
YOLOv3 [Redmon and Farhadi, 2018]. Further we generate
scene context using Semantic Segementation – segmenting

2

ASP encoding of the domain independent axioms of the Functional Event

Calculus (FEC) used as per: https://www.ucl.ac.uk/infostudies/efec/fec.lp

Figure 3: Computational Steps for Online Visual Abduction

the road, sidewalk, buildings, cars, people, trees, etc. us-
ing DeepLabv3+ [Chen et al., 2018], and Lane Detection –
estimating lane markings, to detect lanes on the road, using
SCNN [Pan et al., 2018]. Type and conﬁdence score for each
observation is given by typeobsi and confobsi .
• Movement Prediction For each track trki changes in po-
sition and size are predicted using kalman ﬁlters; this results
in an estimate of the spatial entity ε for the next time-point t
of each motion track Pt = {εtrk1, ..., εtrkn }.
• Matching Likelihood For each pair of tracks and ob-
servations εtrki and εobsj , where εtrki ∈ Pt and
εobsj ∈ VOt, we compute the likelihood MLt =
{mltrk1,obs1 , ..., mltrki,obsj } that εobsj belongs to εtrki . The
intersection over union (IoU) provides a measure for the
amount of overlap between the spatial entities εobsj and εtrki.
Following
Step 2. ABDUCTION BASED ASSOCIATION
perception as logical abduction most directly in the sense of
Shanahan [2005], we deﬁne the task of abducing visual expla-
nations as ﬁnding an association (Hassign
) of observed scene
elements (VOt) to the motion tracks of objects (MT ) given
by the predictions Pt, together with a high-level explanation
(Hevents
] is consistent with
t

), such that [Hassign

∧ Hevents
t

t

t

IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).Foreacht2TStep1.FormulatingtheProblemSpeciﬁcation<VOt,Pt,MLt>(1)detectVisualObservations(VOt)e.g.,People,Cars,Objects,Roads,Lanes,(2)Predictions(Pt)ofnextpositionandsizeofobjecttracksusingkalmanﬁlters,and(3)calculateMatchingLikelihood(MLt)basedonIntersectionoverUnion(IoU)betweenpredictionsanddetections.obs(obs_0,car,99).obs(...)....box2d(obs_16,1078,86,30,44)....trk(trk_0,car).trk(...)....box2d(trk_0,798,146,113,203)....iou(trk_0,obs_0,83921).iou(...)....iou(trk_23,obs_16,0)....Step2.AbductionbasedAssociationgeneratehypothesisfor(1)matchingoftracksandobservations(Hassignt),and(2)andhigh-levelevents(Heventst)explaining(1).trk1trk2obs2obs1topology: poIOU: 0.89IOU: 0.23IOU: 0.00...obsnstart...endassignassignhaltTracksObservationsPREDICTtrk1trk2...TracksUPDATEtntn+1obs1...obsnObservationshaltassignstandbytopology: poIOU: 0.91conf: 0.43holds-at(visibility(trk2),  partially_occluded, tn).occurs-at(hides_behind(trk2, trk1), tn+1).holds-at(visibility(trk2), fully_visible, tn-1).occurs-at(gets_behind(trk2, trk1), tn).holds-at(visibility(trk2),             fully_occluded, tn+1).Step3.FindingtheOptimalHypothesisJointlyoptimizeHassigntandHeventstbymaximizingmatchinglikelihoodMLtandminimizingeventcosts.RESULT.Visuo-SpatialSceneSemanticsResultingmotiontracksandthecorre-spondingeventsequence,explainingthelow-levelmotion....occurs_at(missing_detections(trk_10),35)...occurs_at(...)...occurs_at(recover(trk_10),36)...occurs_at(lost(trk_18),41)...occurs_at(hides_behind(trk_9,trk_10),41)...occurs_at(...)...occurs_at(unhides_from_behind(trk_9,trk_10),42)...Table1:ComputationalStepsforOnlineVisualAbductiont

t

t

∧ Hevents
t

] |= VOt ∧ Pt ∧ MLt
consists of the assignment of detections to ob-
consists of the high-level events Θ

the background knowledge and the previously abduced event
sequence Hevents, and entails the perceived scene given by
< VOt, Pt, MLt >:
(cid:73) Σ ∧ Hevents ∧ [Hassign
where Hassign
ject tracks, and Hevents
explaining the assignments.
• Associating Objects and Observations Finding the best
match between observations (VOt) and object tracks (Pt) is
done by generating all possible assignments and then max-
imising a matching likelihood mltrki,obsj between pairs of
spatial entities for matched observations εobsj and predicted
track region εtrki (See Step 3). Towards this we use choice
rules [Gebser et al., 2014] (i.e., one of the heads of the rule
has to be in the stable model) for εobsj and εtrki, generat-
ing all possible assignments in terms of assignment actions:
assign, start, end, halt, resume, ignore det, ignore trk.

(cid:73) MATCHING TRACKS AND DETECTIONS

For each assignment action we deﬁne integrity constraints3
that restrict the set of answers generated by the choice rules,
e.g., the following constraints are applied to assigning an ob-
servation εobsj to a track trki, applying thresholds on the
IoUtrki,obsj and the conﬁdence of the observation confobsj ,
further we deﬁne that the type of the observation has to match
the type of the track it is assigned to:

(cid:73) INTEGRITY CONSTRAINTS ON MATCHING

• Abducible High-Level Events For the length of this paper,
we restrict to high-level visuo-spatial abducibles pertaining
to object persistence and visibility (Table 1): (1). Occlusion:
Objects can disappear or reappear as result of occlusion with
other objects; (2). Entering / Leaving the Scene: Objects can
enter or leave the scene at the borders of the ﬁeld of view;
(3). Noise and Missing Observation: (Missing-)observations
can be the result of faulty detections.

Lets take the case of occlusion: functional ﬂuent visibil-
ity could be denoted f ully visible, partially occluded or
f ully occluded:

(cid:73) VISIBILITY - FLUENT AND POSSIBLE VALUES

We deﬁne the event hides behind/2, stating that an object
hides behind another object by deﬁning the conditions that

EVENTS
enters fov(Trk)
leaves fov(Trk)
hides behind(Trk1, Trk2)
unhides from behind(Trk1, Trk2)
missing detections(Trk)

Description
Track Trk enters the ﬁeld of view.
Track Trk leaves the ﬁeld of view.
Track Trk1 hides behind track Trk2.
Track Trk1 unhides from behind track Trk2.
Missing detections for track Trk.

FLUENTS
in fov(Trk)
hidden by(Trk1, Trk2)
visibility(Trk)

Values

}
}

true;false
true;false
fully visible;

{
{
{
partially occluded;
fully occluded

}

Description
Track Trk is in the ﬁeld of view.
Track Trk1 is hidden by Trk2.
Visibility state of track Trk.

1:
Table
(Dis)Appearance

Abducibles;

Events

and Fluents Explaining

have to hold for the event to possibly occur, and the ef-
fects the occurrence of the event has on the properties of
the objects, i.e., the value of the visibility ﬂuent changes to
f ully occluded.

(cid:73) OCCLUSION - EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS

For abducing the occurrence of an event we use choice rules
that connect the event with assignment actions, e.g., a track
getting halted may be explained by the event that the track
hides behind another track.

(cid:73) GENERATING HYPOTHESES ON EVENTS

Step 3. FINDING THE OPTIMAL HYPOTHESIS To en-
sure an optimal assignment, we use ASP based optimization
to maximize the matching likelihood between matched pairs
of tracks and detections. Towards this, we ﬁrst deﬁne the
matching likelihood based on the Intersection over Union
(IoU) between the observations and the predicted boxes for
each track as described in [Bewley et al., 2016]:

(cid:73) ASSIGNMENT LIKELIHOOD

We then maximize the matching likelihood for all assign-

ments, using the build in maximize statement:

(cid:73) MAXIMIZING ASSIGNMENT LIKELIHOOD

To ﬁnd the best set of hypotheses with respect to the obser-
vations, we minimize the occurrence of certain events and as-
sociation actions, e.g., the following optimization statements
minimize starting and ending tracks; the resulting assignment
is then used to update the motion tracks accordingly.

(cid:73) OPTIMIZE EVENT AND ASSOCIATION COSTS

3

Integrity constraints restrict the set of answers by eliminating stable models

where the body is satisﬁed.

It is important here to note that: (1). by jointly abducing the
object dynamics and high-level events we can impose con-

IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).Situation
OVERTAKING
HIDDEN ENTITY
REDUCED VISIBILITY

SUDDEN STOP
BLOCKED LANE
EXITING VEHICLE

Objects
vehicle, vehicle
entity, object
object

vehical
lane, object
person, vehicle

Description
vehicle is overtaking another vehicle
trafﬁc participant hidden by obstacle
visibility reduced by object in front.

vehicle in front stopping suddenly
lane of the road is blocked by some object.
person is exiting a parked vehicle.

Table 2: Safety-Critical Situations

Figure 4: Abducing Occlusion to Anticipate Reappearance

straints on the assignment of detections to tracks, i.e., an as-
signment is only possible if we can ﬁnd an explanation sup-
porting the assignment; and (2). the likelihood that an event
occurs guides the assignments of observations to tracks. In-
stead of independently tracking objects and interpreting the
interactions, this yields to event sequences that are consistent
with the abduced object tracks, and noise in the observations
is reduced (See evaluation in Sec. 3).

3 APPLICATION & EVALUATION
We demonstrate applicability towards identifying and inter-
preting safety-critical situations (e.g., Table 2); these encom-
pass those scenarios where interpretation of spacetime dy-
namics, driving behaviour, environmental characteristics is
necessary to anticipate and avoid potential dangers.
Reasoning about Hidden Entities Consider the situation
of Fig. 4: a car gets occluded by another car turning left and
reappears in front of the autonomous vehicle. Using online
abduction for abducing high-level interactions of scene ob-
jects we can hypothesize that the car got occluded and antici-
pate its reappearance based on the perceived scene dynamics.
The following shows data and abduced events.

We deﬁne a rule stating that a hidden object may unhide from
behind the object it is hidden by and anticipate the time point
t based on the object movement as follows:

We then interpolate the objects position at time point t to pre-
dict where the object may reappear.

For the occluded car in our example we get the following
prediction for time t and position x, y:

hicle, which could be used by the control mechanism, e.g., to
adapt driving and slow down in order to keep safe distance:

Empirical Evaluation For online sensemaking, evaluation
focusses on accuracy of abduced motion tracks, real-time per-
formance, and the tradeoff between performance and accu-
racy. Our evaluation uses the KITTI object tracking dataset
[Geiger et al., 2012], which is a community established
it consists of 21
benchmark dataset for autonomous cars:
training and 29 test scenes, and provides accurate track an-
notations for 8 object classes (e.g., car, pedestrian, van, cy-
clist). We also evaluate tracking results using the more gen-
eral cross-domain Multi-Object Tracking (MOT) dataset [Mi-
lan et al., 2016] established as part of the MOT Challenge; it
consists of 7 training and 7 test scenes which are highly un-
constrained videos ﬁlmed with both static and moving cam-
eras. We evaluate on the available groundtruth for training
scenes of both KITTI using YOLOv3 detections, and MOT17
using the provided faster RCNN detections.
• Evaluating Object Tracking For evaluating accuracy
(MOTA) and precision (MOTP) of abduced object tracks we
follow the ClearMOT [Bernardin and Stiefelhagen, 2008]
evaluation schema. Results (Table 3) show that jointly ab-
ducing high-level object interactions together with low-level
scene dynamics increases the accuracy of the object tracks,
i.e, we consistently observe an improvement of about 5%,
from 45.72% to 50.5% for cars and 28.71% to 32.57% for
pedestrians on KITTI, and from 41.4% to 46.2% on MOT.
• Online Performance and Scalability Performance of on-
line abduction is evaluated with respect to its real-time ca-
pabilities.4 (1). We compare the time & accuracy of online
abduction for state of the art (real-time) detection methods:
YOLOv3, SSD [Liu et al., 2016], and Faster RCNN [Ren et
al., 2015] (Fig. 5). (2). We evaluate scalability of the ASP
based abduction on a synthetic dataset with controlled num-
ber of tracks and % of overlapping tracks per frame. Results
(Fig. 5) show that online abduction can perform with above
30 frames per second for scenes with up to 10 highly over-
lapping object tracks, and more than 50 tracks with 1fps (for
the sake of testing, it is worth noting that even for 100 objects
per frame it only takes about an average of 4 secs per frame).
Importantly, for realistic scenes such as in the KITTI dataset,
abduction runs realtime at 33.9fps using YOLOv3, and 46.7
using SSD with a lower accuracy but providing good preci-
sion.

Based on this prediction we can then deﬁne a rule that gives
a warning if a hidden entity may reappear in front of the ve-

4

Evaluation using a dedicated Intel Core i7-6850K 3.6GHz 6-Core Proces-

sor, 64GB RAM, and a NVIDIA Titan V GPU 12GB.

IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).IMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).hides behindunhides from behindt160t180t200t220fully_occludedanticipated reappearanceIMATCHINGTRACKSANDDETECTIONS1{assign(Trk,Det):det(Det,_,_);end(Trk);ignore_trk(Trk);halt(Trk);resume(Trk,Det):det(Det,_,_)}1:-trk(Trk,_).1{assign(Trk,Det):trk(Trk,_);start(Det);ignore_det(Det);resume(Trk,Det):trk(Trk,_).}1:-det(Det,_,_).IINTEGRITYCONSTRAINTSONMATCHING:-assign(Trk,Det),notassignment_constraints(Trk,Det).assignment_constraints(Trk,Det):-trk(Trk,Trk_Type),trk_state(Trk,active),det(Det,Det_Type,Conf),Conf>conf_thresh_assign,match_type(Trk_Type,Det_Type),iou(Trk,Det,IOU),IOU>iou_thresh.IVISIBILITY-FLUENTANDPOSSIBLEVALUESfluent(visibility(Trk)):-trk(Trk,_).possVal(visibility(Trk),fully_visible):-trk(Trk,_).possVal(visibility(Trk),partially_visible):-trk(Trk,_).possVal(visibility(Trk),not_visible):-trk(Trk,_).IOCCLUSION-EVENT,EFFECTSAND(SPATIAL)CONSTRAINTSevent(hides_behind(Trk1,Trk2)):-trk(Trk1,_),trk(Trk2,_).causesValue(hides_behind(Trk1,Trk2),visibility(Trk1),not_visible,T):-trk(Trk1,_),trk(Trk2,_),time(T).:-occurs_at(hides_behind(Trk1,Trk2),curr_time),trk(Trk1,_),trk(Trk2,_),notposition(overlapping_top,Trk1,Trk2).IGENERATINGHYPOTHESESONEVENTS1{occurs_at(hides_behind(Trk,Trk2),curr_time):trk(Trk2,_);...}1:-halt(Trk).IASSIGNMENTLIKELIHOODassignement_prob(Trk,Det,IOU):-det(Det,_,_),trk(Trk,_),iou(Trk,Det,IOU).IMAXIMIZINGASSIGNMENTLIKELIHOOD#maximize{(Prob)@1,Trk,Det:assign(Trk,Det),assignment_prob(Trk,Det,Prob)}.IOPTIMIZEEVENTANDASSOCIATIONCOSTS#minimize{5@2,Trk:end(Trk)}.#minimize{5@2,Det:start(Det)}.trk(trk_3,car).trk_state(trk_3,active).......trk(trk_41,car).trk_state(trk_41,active).......det(det_1,car,98)....box2d(trk_3,660,460,134,102).......box2d(trk_41,631,471,40,47).......occurs_at(hides_behind(trk_41,trk_3),179))...anticipate(unhides_from_behind(Trk1,Trk2),T):-time(T),curr_time<T,holds_at(hidden_by(Trk1,Trk2),curr_time),topology(proper_part,Trk1,Trk2),movement(moves_out_of,Trk1,Trk2,T).point2d(interpolated_position(Trk,T),PosX,PosY):-time(T),curr_time<T,T1=T-curr_time,box2d(Trk1,X,Y,_,_),trk_mov(Trk1,MovX,MovY),PosX=X+MovX*T1,PosY=Y+MovX*T1.anticipate(unhides_from_behind(trk_41,trk_2),202)point2d(interpolated_position(trk_41,202),738,495)warning(hidden_entity_in_front(Trk1,T)):-time(T),T-curr_time<anticipation_threshold,anticipate(unhides_from_behind(Trk1,_),T),position(in_front,interpolated_pos(Trk1,T)).SEQUENCE
KITTI tracking – Cars
(8008 frames, 636 targets)
KITTI tracking – Pedestrians
(8008 frames, 167 targets)
MOT 2017
(5316 frames, 546 targets)

Tracking
without Abduction
with Abduction
without Abduction
with Abduction
without Abduction
with Abduction

MOTP
MOTA
45.72 %
76.89 %
50.5 % 74.76 %
71.43 %
28.71 %
32.57 % 70.68 %
88.0 %
41.4 %
87.9 %
46.2 %

ML
19.14 %
20.21 %
26.94 %
22.15 %
35.53 %
31,32 %

MT
23.04 %
23.23 %
9.58 %
14.37 %
16.48 %
20.7 %

FP
785
1311
1261
1899
4877
5195

FN
11182
10439
6119
5477
60164
54421

ID sw.
1097
165
539
115
779
800

Frag.
1440
490
833
444
741
904

Table 3: Evaluation of Tracking Performance; accuracy (MOTA), precision (MOTP), mostly tracked (MT) and mostly lost (ML) tracks,
false positives (FP), false negatives (FN), identity switches (ID Sw.), and fragmentation (Frag.).

DETECTOR
YOLOv3
SSD
FRCNN

Recall
0.690
0.599
0.624

MOTA
50.5 %
30.63 %
37.96 %

MOTP
74.76 %
77.4 %
72.9 %

f psdet
45
8
5

f psabd
33.9
46.7
32.0

No. tracks ms/f rame
23.33
31.36
62.08
511.83
3996.38

5
10
20
50
100

f ps

42.86
31.89
16.11
1.95
0.25

Figure 5: Online Performance and Scalability; performance for
pretrained detectors on the ’cars’ class of KITTI dataset, and pro-
cessing time relative to the no. of tracks on synthetic dataset.

Discussion of Empirical Results Results show that inte-
grating high-level abduction and object tracking improves the
resulting object tracks and reduce the noise in the visual ob-
servations. For the case of online visual sense-making, ASP
based abduction provides the required performance: even
though the complexity of ASP based abduction increases
quickly, with large numbers of tracked objects the frame-
work can track up to 20 objects simultaneously with 30f ps
and achieve real-time performance on the KITTI benchmark
dataset. It is also important to note that the tracking approach
in this paper is based on tracking by detection using a naive
measure, i.e, the IoU (Sec. 2.2; Step 1), to associate obser-
vations and tracks, and it is not using any visual information
in the prediction or association step. Naturally, this results in
a lower accuracy, in particular when used with noisy detec-
tions and when tracking fast moving objects in a benchmark
dataset such as KITTI. That said, due to the modularity of
the implemented framework, extensions with different meth-
ods for predicting motion (e.g., using particle ﬁlters or opti-
cal ﬂow based prediction) are straightforward: i.e., improving
tracking is not the aim of our research.

4 RELATED WORK
ASP is now widely used as an underlying knowledge
representation language and robust methodology for non-
monotonic reasoning [Brewka et al., 2011; Gebser et al.,
2012]. With ASP as a foundation, and driven by semantics,
commonsense and explainability [Davis and Marcus, 2015],
this research aims to bridge the gap between high-level for-
malisms for logical abduction and low-level visual processing
by tightly integrating semantic abstractions of space-change
with their underlying numerical representations. Within KR,
the signiﬁcance of high-level (abductive) explanations in a

range of contexts is well-established: planning & process
recognition [Kautz, 1991], vision & abduction [Shanahan,
2005], probabilistic abduction [Blythe et al., 2011], reason-
ing about spatio-temporal dynamics [Bhatt and Loke, 2008],
reasoning about continuous spacetime change [Muller, 1998;
Hazarika and Cohn, 2002] etc. Dubba et al. [2015] uses ab-
ductive reasoning in an inductive-abductive loop within in-
ductive logic programming (ILP). Aditya et al. [2015] for-
malise general rules for image interpretation with ASP. Simi-
larly motivated to this research is [Suchan et al., 2018], which
uses a two-step approach (with one huge problem speciﬁca-
tion), ﬁrst tracking and then explaining (and ﬁxing) tracking
errors; such an approach is not runtime / realtime capable. In
computer vision research there has recently been an interest to
synergise with cognitively motivated methods; in particular,
e.g., for perceptual grounding & inference [Yu et al., 2015]
and combining video analysis with textual information for
understanding events & answering queries about video data
[Tu et al., 2014].

5 CONCLUSION & OUTLOOK
We develop a novel abduction-driven online (i.e., realtime, in-
cremental) visual sensemaking framework: general, system-
atically formalised, modular and fully implemented. Integrat-
ing robust state-of-the-art methods in knowledge representa-
tion and computer vision, the framework has been evaluated
and demonstrated with established benchmarks. We highlight
application prospects of semantic vision for autonomous driv-
ing, a domain of emerging & long-term signiﬁcance. Spe-
cialised commonsense theories (e.g., about multi-sensory in-
tegration & multi-agent belief merging, contextual knowl-
edge) may be incorporated based on requirements. Our on-
going focus is to develop a novel dataset emphasising se-
mantics and (commonsense) explainability; this is driven by
mixed-methods research –AI, Psychology, HCI– for the study
of driving behaviour in low-speed, complex urban environ-
ments with unstructured trafﬁc. Here, emphasis is on natural
interactions (e.g., gestures, joint attention) amongst drivers,
pedestrians, cyclists etc. Such interdisciplinary studies are
needed to better appreciate the complexity and spectrum of
varied human-centred challenges in autonomous driving, and
demonstrate the signiﬁcance of integrated vision & semantics
solutions in those contexts.
Acknowledgements
Partial funding by the German Research Foundation (DFG)
via the CRC 1320 EASE – Everyday Activity Science and
Engineering” (www.ease-crc.org), Project P3: Spatial Rea-
soning in Everyday Activity is acknowledged.

ms/frame1101001Knum. tracks510205010030 fps1 fps15 fps 2References
[Aditya et al., 2015] Somak Aditya, Yezhou Yang, Chitta Baral,
Cornelia Fermuller, and Yiannis Aloimonos. Visual common-
sense for scene understanding using perception, semantic parsing
and reasoning. In 2015 AAAI Spring Symposium Series, 2015.
[Allen, 1983] James F. Allen. Maintaining knowledge about tem-

poral intervals. Commun. ACM, 26(11):832–843, 1983.

[Balbiani et al., 1999] Philippe Balbiani, Jean-Franc¸ois Condotta,
and Luis Fari˜nas del Cerro. A new tractable subclass of the rect-
In Thomas Dean, editor, IJCAI 1999, Sweden,
angle algebra.
pages 442–447. Morgan Kaufmann, 1999.

[Bernardin and Stiefelhagen, 2008] Keni Bernardin and Rainer
Stiefelhagen. Evaluating multiple object tracking performance:
The clear mot metrics. EURASIP Journal on Image and Video
Processing, 2008(1):246309, May 2008.

[Bewley et al., 2016] Alex Bewley, Zongyuan Ge, Lionel Ott,
Fabio Ramos, and Ben Upcroft. Simple online and realtime track-
ing. In 2016 IEEE International Conference on Image Processing
(ICIP), pages 3464–3468, 2016.

[Bhatt and Loke, 2008] Mehul Bhatt and Seng W. Loke. Modelling
dynamic spatial systems in the situation calculus. Spatial Cogni-
tion & Computation, 8(1-2):86–130, 2008.

[Bhatt et al., 2013] Mehul Bhatt, Carl Schultz, and Christian
Freksa. The ‘Space’ in Spatial Assistance Systems: Conception,
Formalisation and Computation. In: Representing space in cog-
nition: Interrelations of behavior, language, and formal models.
Series: Explorations in Language and Space. 978-0-19-967991-
1, Oxford University Press, 2013.

[Blythe et al., 2011] James Blythe, Jerry R. Hobbs, Pedro Domin-
Implementing
gos, Rohit J. Kate, and Raymond J. Mooney.
weighted abduction in markov logic. In Proc. of 9th Intl. Confer-
ence on Computational Semantics, IWCS ’11, USA, 2011. ACL.
[BMVI, 2018] BMVI. Report by the ethics commission on auto-
mated and connected driving., bmvi: Federal ministry of trans-
port and digital infrastructure, germany, 2018.

[Brewka et al., 2011] Gerhard Brewka,

and
Miroslaw Truszczy´nski. Answer set programming at a glance.
Commun. ACM, 54(12):92–103, December 2011.

Thomas Eiter,

[Chen et al., 2018] Liang-Chieh Chen, Yukun Zhu, George Papan-
dreou, Florian Schroff, and Hartwig Adam. Encoder-decoder
with atrous separable convolution for semantic image segmen-
tation. arXiv:1802.02611, 2018.

[Davis and Marcus, 2015] Ernest Davis and Gary Marcus. Com-
monsense reasoning and commonsense knowledge in artiﬁcial
intelligence. Commun. ACM, 58(9):92–103, 2015.

[Dubba et al., 2015] Krishna Sandeep Reddy Dubba, Anthony G.
Cohn, David C. Hogg, Mehul Bhatt, and Frank Dylla. Learning
relational event models from video. J. Artif. Intell. Res. (JAIR),
53:41–90, 2015.

[Gebser et al., 2012] Martin Gebser, Roland Kaminski, Benjamin
Kaufmann, and Torsten Schaub. Answer Set Solving in Practice.
Morgan & Claypool Publishers, 2012.

[Gebser et al., 2014] Martin Gebser, Roland Kaminski, Benjamin
Kaufmann, and Torsten Schaub. Clingo = ASP + control: Pre-
liminary report. CoRR, abs/1405.3694, 2014.

[Geiger et al., 2012] Andreas Geiger, Philip Lenz, and Raquel Ur-
tasun. Are we ready for autonomous driving? the kitti vision
benchmark suite. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2012.

[Hazarika and Cohn, 2002] Shyamanta M Hazarika and Anthony G
Cohn. Abducing qualitative spatio-temporal histories from par-
tial observations. In KR, pages 14–25, 2002.

[Kautz, 1991] Henry A. Kautz. Reasoning about plans. chapter
A Formal Theory of Plan Recognition and Its Implementation,
pages 69–124. Morgan Kaufmann Publishers Inc., USA, 1991.
[Liu et al., 2016] Wei Liu, Dragomir Anguelov, Dumitru Erhan,
Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexan-
der C. Berg. SSD: single shot multibox detector. In ECCV (1),
volume 9905 of LNCS, pages 21–37. Springer, 2016.

[Ma et al., 2014] Jiefei Ma, Rob Miller, Leora Morgenstern, and
Theodore Patkos. An epistemic event calculus for asp-based rea-
soning about knowledge of the past, present and future. In LPAR:
19th Intl. Conf. on Logic for Programming, Artiﬁcial Intelligence
and Reasoning, volume 26 of EPiC Series in Computing, pages
75–87. EasyChair, 2014.

[Mani and Pustejovsky, 2012] Inderjeet Mani and James Puste-
jovsky. Interpreting Motion - Grounded Representations for Spa-
tial Language, volume 5 of Explorations in language and space.
Oxford University Press, 2012.

[Milan et al., 2016] Anton Milan, Laura Leal-Taix´e, Ian D. Reid,
Stefan Roth, and Konrad Schindler. MOT16: A benchmark for
multi-object tracking. CoRR, abs/1603.00831, 2016.

[Miller et al., 2013] Rob Miller, Leora Morgenstern, and Theodore
Patkos. Reasoning about knowledge and action in an epistemic
event calculus. In COMMONSENSE 2013, 2013.

[Muller, 1998] Philippe Muller. A qualitative theory of motion
based on spatio-temporal primitives. In Anthony G. Cohn et. al.,
editor, KR 1998, Italy. Morgan Kaufmann, 1998.

[Pan et al., 2018] Xingang Pan, Jianping Shi, Ping Luo, Xiaogang
Wang, and Xiaoou Tang. Spatial as deep: Spatial CNN for traf-
ﬁc scene understanding.
In Sheila A. McIlraith and Kilian Q.
Weinberger, editors, AAAI 2018. AAAI Press, 2018.

[Redmon and Farhadi, 2018] Joseph Redmon and Ali Farhadi.
Yolov3: An incremental improvement. CoRR, abs/1804.02767,
2018.

[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross B. Girshick,
and Jian Sun. Faster R-CNN: towards real-time object detection
with region proposal networks. In Annual Conference on Neural
Information Processing Systems 2015, Canada, 2015.

[Shanahan, 2005] Murray Shanahan.

Perception as abduction:
Turning sensor data into meaningful representation. Cognitive
Science, 29(1):103–134, 2005.

[Suchan and Bhatt, 2016] Jakob Suchan and Mehul Bhatt. Seman-
tic question-answering with video and eye-tracking data: AI
foundations for human visual perception driven cognitive ﬁlm
studies. In S. Kambhampati, editor, IJCAI 2016, New York, USA,
pages 2633–2639. IJCAI/AAAI Press, 2016.

[Suchan et al., 2018] Jakob Suchan, Mehul Bhatt, Przemyslaw An-
drzej Walega, and Carl P. L. Schultz. Visual explanation by high-
level abduction: On answer-set programming driven reasoning
about moving objects. In: AAAI 2018. AAAI Press, 2018.

[Tu et al., 2014] Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun
Choe, and Song Chun Zhu. Joint video and text parsing for under-
standing events and answering queries. IEEE MultiMedia, 2014.
[Yu et al., 2015] Haonan Yu, N. Siddharth, Andrei Barbu, and Jef-
frey Mark Siskind. A Compositional Framework for Grounding
Language Inference, Generation, and Acquisition in Video. J.
Artif. Intell. Res. (JAIR), 52:601–713, 2015.

