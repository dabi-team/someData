Data Poisoning Attacks to Deep Learning Based
Recommender Systems

1
2
0
2

n
a
J

8

]

R
C
.
s
c
[

2
v
4
4
6
2
0
.
1
0
1
2
:
v
i
X
r
a

Hai Huang1, Jiaming Mu1, Neil Zhenqiang Gong2, Qi Li1, Bin Liu3, Mingwei Xu1
1Institute for Network Sciences and Cyberspace & Department of Computer Science and Technology, Tsinghua University
1Beijing National Research Center for Information Science and Technology (BNRist)
2Duke University
3Department of Management Information Systems, West Virginia University

Abstract‚ÄîRecommender systems play a crucial role in helping
users to Ô¨Ånd their interested information in various web services
such as Amazon, YouTube, and Google News. Various recom-
mender systems, ranging from neighborhood-based, association-
rule-based, matrix-factorization-based, to deep learning based,
have been developed and deployed in industry. Among them,
deep learning based recommender systems become increasingly
popular due to their superior performance.

In this work, we conduct the Ô¨Årst systematic study on data
poisoning attacks to deep learning based recommender systems.
An attacker‚Äôs goal is to manipulate a recommender system such
that the attacker-chosen target items are recommended to many
users. To achieve this goal, our attack injects fake users with
carefully crafted ratings to a recommender system. SpeciÔ¨Åcally,
we formulate our attack as an optimization problem, such that
the injected ratings would maximize the number of normal
users to whom the target items are recommended. However,
it is challenging to solve the optimization problem because it
is a non-convex integer programming problem. To address the
challenge, we develop multiple techniques to approximately solve
the optimization problem. Our experimental results on three real-
world datasets, including small and large datasets, show that our
attack is effective and outperforms existing attacks. Moreover, we
attempt to detect fake users via statistical analysis of the rating
patterns of normal and fake users. Our results show that our
attack is still effective and outperforms existing attacks even if
such a detector is deployed.

I.

INTRODUCTION

In the era of data explosion, people often encounter infor-
mation overload problems in their daily lives. For example,
when they are shopping online, reading news, listening to
music or watching videos,
they often face challenges of
choosing their interested items from a huge number of candi-
dates. Recommender systems help people Ô¨Ånd their interested
items easily by mining historical user-item interaction data.
Therefore, recommender systems have been widely used in
the real world, which brings huge economic beneÔ¨Åts.

Unlike the non-personalized recommender system that rec-
ommends the same items to all users, personalized recom-
mender system that we focus in this work uses users‚Äô historical

Network and Distributed Systems Security (NDSS) Symposium 2021
21-24 February 2021
ISBN 1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24525
www.ndss-symposium.org

behavior (e.g., ratings or clicks) to model their preferences and
make personalized recommendations for each user [37]. In a
typical personalized recommender system setting, we are given
a set of users, a set of items, and a log of the users‚Äô historical
interactions (e.g., ratings) with the items, and the goal is to
recommend each user a list of top ranked items based on
user preferences learned from the historical interactions. Tradi-
tional recommender systems include neighborhood-based [38],
association-rule-based [10], matrix-factorization-based (a.k.a
latent factor model) [26], and graph-based [14]. Recently,
with the rapid development of deep learning techniques, deep
neural networks have been applied to empower recommender
systems [6], [8], [20], [34]. Moreover, due to various ad-
vantages, such as nonlinear transformation and representation
learning that cannot be realized by traditional techniques, deep
learning is gradually becoming a technology trend in the Ô¨Åeld
of recommender systems [50].

Meanwhile, various studies have shown that recommender
systems are vulnerable to data poisoning attacks [12], [13],
[27], [28], [32], [45], [46] (a.k.a shilling attacks [18]). Par-
ticularly, in a data poisoning attack, an attacker injects fake
users with carefully crafted ratings to a recommender system
such that the recommender system makes recommendations
as the attacker desires, e.g., an attacker-chosen target item is
recommended to many normal users. Data poisoning attacks
pose severe threats to the trustworthiness of recommender
systems and could manipulate Internet opinions. For instance,
if an attacker manipulates a news recommender system such
that a particular type of news are always recommended to
users, then the attacker may be able to manipulate the users‚Äô
opinions. However, existing data poisoning attacks are either
agnostic to recommender system algorithms [27], [32] or
optimized to traditional recommender system algorithms such
as association-rule-based [46], graph-based [13], and matrix-
factorization-based [12], [28]. Although deep learning based
recommender systems gain increasing attention and are de-
ployed in industry, their security against data poisoning attacks
is largely unknown.

In this work, we aim to bridge this gap. SpeciÔ¨Åcally,
we propose data poisoning attacks that are optimized for
deep learning based recommender systems. We consider an
attacker‚Äôs goal is to promote a target item in a deep learning
based recommender system, i.e., an attacker-chosen target item
is recommended to a large number of users. To achieve this
goal, an attacker injects fake users with carefully crafted
ratings to the recommender system. As resources are limited

 
 
 
 
 
 
in an attack, we assume that the attacker can only inject a
limited number of fake users and each fake user rates a limited
number of items (including the target item and other non-
target items) to evade trivial detection. The key challenge
of constructing the attack is to choose the rated items for
each fake user. To address the challenge, we formulate the
attack as an optimization problem with an objective function
of maximizing the hit ratio of the target item, where the hit
ratio of an item is the fraction of normal users to whom the
item is recommended.

However, the optimization problem is difÔ¨Åcult to solve
because of the following reasons: i) the inputs of the problem,
i.e., data of users and items in deep learning based recom-
mender systems, are discrete variables, and ii) the training
process for a deep neural network is time-consuming, which
makes it impossible for any method to require a large number
of training iterations for solving the problem. Thus, we develop
heuristics to approximately solve the optimization problem.
Instead of directly generating the desired rated items for fake
users, we train a surrogate model called poison model and
carefully modify it to simulate the target deep learning based
recommender system. Then, we utilize this poison model to
predict the rating score vector of each fake user, and then we
process the vector to assist in selecting the rated items for each
fake user, so as to achieve our goal effectively.

We evaluate our attack and compare it with existing data
poisoning attacks using three real-world datasets with different
sizes, i.e., MovieLens-100K [19], Last.fm [2], and MovieLens-
1M [19]. Our results show that our attack can effectively
promote target items and signiÔ¨Åcantly surpasses the baseline
attacks under the white-box setting. For example, via inserting
only 5% of fake users, our attack can make unpopular target
items recommended to about 52.6 times more normal users
in the Last.fm dataset. Moreover, on the larger MovieLens-
1M [19] dataset, our attack achieves a hit ratio of 0.0099
items when injecting only 5% of fake
for random target
users, which is about 1.2 times of the best hit ratio achieved
by the baseline attacks. We further explore the impact of
partial knowledge on our poisoning attack under two different
partial knowledge settings. We observe that our attack remains
effective and signiÔ¨Åcantly outperforms the baseline attacks in
these settings. For example, when the attacker only knows 30%
of ratings in the original user-item rating matrix, our attack
obtains a hit ratio of 0.0092 for random target items when in-
jecting 5% of fake users on the MovieLens-1M dataset, which
is at least 1.3 times of the hit ratio of the baseline attacks. In
addition, our attack is transferable to structure-unknown deep
learning based recommender systems. In particular, even if we
do not know the exact neural network architecture used by
the target recommender system, our attack still makes random
target items recommended to about 5.5 times more normal
users when injecting 5% of fake users in the MovieLens-100K
dataset. Our results demonstrate that our attack poses a severe
security threat to deep learning based recommender systems.

Moreover, we explore detecting fake users via statistical
analysis of their rating patterns and measure the attack ef-
fectiveness under such detection. The intuition behind the
detection is that fake users may have rating patterns that are
statistically different from those of normal users as they are
generated according to speciÔ¨Åc rules. Particularly, for each

2

user, we extract multiple features from its ratings. Then,
we train a binary classiÔ¨Åer to distinguish between fake and
normal users based on the feature values and utilize the SVM-
TIA [51] method to detect potential fake users. The service
provider removes the detected fake users before training the
recommender system. Our experimental results show that such
a method can effectively detect the fake users generated by
existing attacks. However, the method falsely identiÔ¨Åes a large
fraction (e.g., 30%) of the fake users constructed by our
attack as normal users. As a result, our attack is still effective
and signiÔ¨Åcantly outperforms existing attacks even if such a
detection method is deployed.

The contributions of our paper are summarized as follows:

‚Ä¢ We perform the Ô¨Årst systematic study on data poi-
soning attacks to deep learning based recommender
systems.

‚Ä¢ We formulate our attack as an optimization problem
and develop multiple techniques to approximately
solve it.

‚Ä¢ We evaluate our attack and compare it with existing

ones on three real-world datasets.

‚Ä¢ We study detecting fake users via statistical analysis
of their ratings and its impact on the effectiveness of
data poisoning attacks.

II. BACKGROUND AND RELATED WORK

In this section, we brieÔ¨Çy introduce recommender systems

and existing data poisoning attacks to them.

A. Recommender Systems

We consider a typical collaborative Ô¨Åltering based rec-
ommender system setting where we have M users and N
items, and we are given a record of the users‚Äô past user-item
interactions {(cid:104)u, i, yui(cid:105)}, where yui denotes the preference
of user u to item i. The observed user-item interactions
{(cid:104)u, i, yui(cid:105)} can be represented as a user-item interaction
matrix Y ‚àà RM √óN . Typically, Y is extremely sparse, i.e.,
on average each user would have interactions with only a
small portion of all the N items. We use a row vector of
Y, indicated as y(u) (i.e., y(u) = {yu1, yu2, . . . , yuN }), to
represent each user u, and a column vector of Y, indicated as
y(i) (i.e., y(i) = {y1i, y2i, . . . , yM i}) , to represent each item
i. Then, the task of a recommender system can be transformed
Y based
into inferring a complete predicted interaction-matrix
on Y, where
Y denotes the predicted score of yui. The
Y is then used to recommend to
inferred interaction-matrix
users a list of items that the users have not experienced yet.
SpeciÔ¨Åcally, if we want to recommend K items for user u, we
select the top K items that (1) they have not been rated by the
user, and that (2) they have the highest predicted sores in the
row vector

yui in

yuN }) of

y(u) (i.e.,

y(u) = {

yu2, . . . ,

yu1,

Y.

(cid:98)

(cid:98)

(cid:98)

(cid:98)

Depending on how to analyze the user-item interaction
(cid:98)
matrix, traditional collaborative Ô¨Åltering based recommender
i.e.,
systems can be roughly divided into four categories,

(cid:98)

(cid:98)

(cid:98)

(cid:98)

(cid:98)

neighborhood-based [38], association-rule-based [10], matrix-
factorization-based (a.k.a latent factor model) [26], and graph-
based [14]. Due to good performance and Ô¨Çexibility in com-
positing more sophisticated models, matrix factorization (MF)
has become the most widely used approach among them.

More recently, with the rapid development of the deep
learning techniques, deep neural networks have been applied
to recommender systems and have been found to outperform
traditional methods in various aspects. Deep learning based
recommender systems use different neural networks struc-
tures to model user-item interactions to boost recommenda-
tion performance [50]. For example, Multilayer Perceptron
(MLP) [20], [21], Autoencoder (AE) [5], Adversarial Networks
(AN) [17], and Deep Reinforcement Learning (DRL) [30], [50]
have been applied to recommender systems to improve the
recommendation accuracy.

In this paper, without loss of generality, we focus on a
general deep learning based recommender system framework,
Neural Collaborative Filtering (NCF) [20]. NCF explores deep
neural networks to model sophisticated nonlinear user-item
interactions. Note that MF-based recommendation methods
assume a latent factor vector to represent each user and
each item, and apply a simple linear model on the user and
item vectors to capture the user-item interactions. In contrast,
NCF uses deep neural networks to capture nonlinear user-item
interactions by passing the user and item latent factor vectors
through multilayer perceptron (MLP). The output layer of NCF
is the prediction of the user-item interaction yui.

In particular, we consider neural matrix factorization
(NeuMF) [20], an instantiation of NCF, to model user-item
interactions. As shown in Figure 1, NeuMF is a fusion of MF
and MLP, which allows them to learn separate embeddings
and then combines the two models by concatenating their
last hidden layers. The input layer consists of two binarized
sparse vectors with one-hot encoding for the user u and item i,
respectively. These sparse vectors are then separately projected
into four dense latent vectors, i.e., MF user vector, MF item
vector, MLP user vector, and MLP item vector, two of which
are the embeddings for the user and the item in the MF model,
and the others are those in the MLP model. There are then two
parts separately processing latent vectors. One is a linear MF
part, which uses a MF layer to compute the inner product
of MF user vector and MF item vector, and the other is a
nonlinear MLP part, which adds a standard MLP with X
layers on the concatenated latent vector to learn the nonlinear
interaction between user u and item i, where X is the number
of MLP layers and the activation function in the MLP layers
is ReLU [15]. Finally, the last hidden layers of MF part and
MLP part are concatenated and fully connected to the output
layer to predict
yui. After training using the observed user-item
interactions, this model can predict the missing entries in the
original sparse interaction matrix Y to constitute a predicted
(cid:98)
Y which can be further used for constructing
interaction matrix
recommendation list for each user.

(cid:98)
B. Attacks to Recommender Systems

Existing studies showed that recommender systems are
vulnerable to various security attacks [1], [27], [28], [46],
which deceive a recommender system, e.g., to promote a target

Fig. 1: Neural matrix factorization model (NeuMF), an instan-
tiation of Neural Collaborative Filtering (NCF) [20].

item and recommend it to as many users as possible. Roughly
speaking, there are two categories of such attacks, i.e., data
poisoning attacks (a.k.a shilling attacks) [11]‚Äì[13], [18], [27],
[28], [46] and proÔ¨Åle pollution attacks [45], which compromise
a recommender system at training and testing, respectively.
SpeciÔ¨Åcally, data poisoning attacks aim to spoof a recom-
mender system to make attacker-desired recommendations by
injecting fake users to the recommender system, while proÔ¨Åle
pollution attacks intend to pollute the historical behavior of
normal users to manipulate the recommendations for them.

Data Poisoning Attacks. Data poisoning attacks inject fake
users to a recommender system and thereby modify the recom-
mendation lists. SpeciÔ¨Åcally, to construct a poisoning attack,
the attacker Ô¨Årst needs to register a number of fake users in a
web service associated with the recommender system. Each
fake user generates well-crafted rating scores for a chosen
subset of items. These fake data will be included in the training
dataset of the target recommender system and then poisons the
training process. According to whether data poisoning attacks
are focused on a speciÔ¨Åc type of recommender system, we
can divide them into two categories: algorithm-agnostic and
algorithm-speciÔ¨Åc. The former (e.g., types of shilling attacks
like random attacks [25], [31] and bandwagon attacks [25],
[35]) does not consider the algorithm used by the recommender
system and therefore often has limited effectiveness. For
instance, random attacks just choose rated items at random
from the whole item set for fake users, and bandwagon attacks
tend to select certain items with high popularity in the dataset
for fake users. The algorithm-speciÔ¨Åc data poisoning attacks
are optimized to a speciÔ¨Åc type of recommender systems
and have been developed for graph-based recommender sys-
tems [13], association-rule-based recommender systems [46],
matrix-factorization-based recommender systems [12], [28],
and neighborhood-based recommender systems [4]. As these
attacks are optimized, they often are more effective. However,
there is no study on algorithm-speciÔ¨Åc data poisoning attacks
to deep learning based recommender systems. We bridge this
gap in this paper.

ProÔ¨Åle Pollution Attacks. The key idea of proÔ¨Åle pollution
attacks is to pollute a user‚Äôs proÔ¨Åle (e.g., historical behavior)

3

0100¬∑¬∑¬∑0MF User VectorMLP Layer 1MLP Layer 2MLP Layer Xbyui<latexit sha1_base64="94/g/wkCBKQVOT3anTyvE65/62c=">AAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==</latexit><latexit sha1_base64="94/g/wkCBKQVOT3anTyvE65/62c=">AAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==</latexit><latexit sha1_base64="94/g/wkCBKQVOT3anTyvE65/62c=">AAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==</latexit>yui<latexit sha1_base64="oe3zeA26Yu2aBQ+qSu84+nYsZRg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==</latexit><latexit sha1_base64="oe3zeA26Yu2aBQ+qSu84+nYsZRg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==</latexit><latexit sha1_base64="oe3zeA26Yu2aBQ+qSu84+nYsZRg=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==</latexit>MLP User Vector0010¬∑¬∑¬∑0MF Item VectorMLP Item VectorMF LayerNeuMF Layer‚Ä¶‚Ä¶ReLUReLUElement-wise ProductConcatenationConcatenationùùàTrainingScoreUseru<latexit sha1_base64="HdrHs+9WrEY+c6wp70bq3BGtMmw=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==</latexit><latexit sha1_base64="HdrHs+9WrEY+c6wp70bq3BGtMmw=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==</latexit><latexit sha1_base64="HdrHs+9WrEY+c6wp70bq3BGtMmw=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==</latexit>i<latexit sha1_base64="9jtkmSnVkzQh6f031uh1XGFh3yI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYzt</latexit><latexit sha1_base64="9jtkmSnVkzQh6f031uh1XGFh3yI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYzt</latexit><latexit sha1_base64="9jtkmSnVkzQh6f031uh1XGFh3yI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYzt</latexit>Itemvia cross-site request forgery (CSRF) [49]. For instance, Xing
et.al. [45] proposed proÔ¨Åle pollution attacks to recommender
systems in web services, e.g., YouTube, Amazon, and Google.
Their study shows that all these services are vulnerable to
their attacks. However, proÔ¨Åle pollution attacks have two key
limitations: i) proÔ¨Åle pollution attacks rely on CSRF, which
makes it hard to perform the attacks at a large scale, and ii)
proÔ¨Åle pollution attacks can not be applied to item-to-item
recommender systems because the attackers are not able to
pollute the proÔ¨Åle of an item [46].

III. PROBLEM FORMULATION

In this section, we Ô¨Årst present our threat model and then
we formulate our poisoning attack as an optimization problem.

A. Threat Model

Attacker‚Äôs Goal. We consider an attacker‚Äôs goal is to promote
a target item. SpeciÔ¨Åcally, suppose a recommender system
recommends top-K items for each user. An attacker‚Äôs goal is to
make its target item appear in the top-K recommendation lists
of as many normal users as possible. We note that an attacker
could also aim to demote a target item, making it appear in
the top-K recommendation lists of as few normal users as
possible. For instance, an attacker may demote its competitor‚Äôs
items. Since demoting a target item can be implemented by
promoting other items [46], we focus on promotion in this
work.

Attacker‚Äôs Background Knowledge. We assume an attacker
has access to the user-item interaction matrix Y. In many
recommender systems such as Amazon and Yelp, users‚Äô ratings
are public. Therefore, an attacker can write a crawler to collect
users‚Äô ratings. However, in our experiments, we will also show
that our attack is still effective when the attacker has access to
a partial user-item interaction matrix. The attacker may or may
not have access to the internal neural network architecture of
the target deep learning based recommender system. When
the attacker does not have access to the neural network
architecture of the target recommender system, the attacker
performs attacks by assuming a neural network architecture. As
we will show in experiments, our attack can transfer between
different neural networks, i.e., our attack constructed based on
one neural network architecture based recommender system is
also effective for other recommender systems that use different
neural network architectures.

Attacker‚Äôs Capabilities. We assume that an attacker has
limited resources, so the attacker can only inject a limited
number of fake users. We use m to denote the upper bound of
the number of fake users. In addition to the target item, each
fake user can rate up to n other items to evade trivial detection.
We call these items Ô¨Åller items. SpeciÔ¨Åcally, normal users often
rate a small number of items, and thus fake users who rate
a large number of items are suspicious and can be detected
easily. We assume the attacker can inject the fake users‚Äô ratings
into the training dataset of the target recommender system to
manipulate the training process of the deep learning model.

B. Formulating Attacks as an Optimization Problem

We deÔ¨Åne the hit ratio of an item t, denoted as HRt, as
the fraction of normal users who would receive the item t

in their top-K recommendation lists. In other words, the hit
ratio of t indicates the probability that t is recommended to
a normal user. An attacker‚Äôs goal is to maximize the hit ratio
of a target item t. Let y(v) denote the rating score vector of
the fake user v, and yvi denote the rating score that the fake
user v gives to item i. A rating score is an element in a set
of integers {0, 1, . . . , rmax}, where yvi = 0 means that the
fake user v has not rated item i and yvi > 0 represents the
preference score fake user v gives to item i. For instance,
rmax = 5 in many recommender systems. Our goal is to craft
the ratings for the fake users such that the hit ratio of the target
item is maximized. Formally, following previous work [13], we
formulate crafting the ratings for the fake users as solving the
following optimization problem:

max HRt

subject to ||y(v)(cid:107)0 ‚â§ n + 1, ‚àÄv ‚àà {v1, v2, . . . , vm},

(1)

yvi ‚àà {0, 1, . . . , rmax},

where (cid:107)y(v)(cid:107)0 is the number of non-zero entries in fake user
v‚Äôs rating score vector y(v), n is the maximum number of
Ô¨Åller items, m is the maximum number of fake users, and
{v1, v2, . . . , vm} is the set of m fake users.

IV. ATTACK CONSTRUCTION: SOLVING THE
OPTIMIZATION PROBLEM

A. Overview of Our Proposed Attacks

A data poisoning attack is essentially to solve the optimiza-
tion problem in Eq. (1). However, the optimization problem
is computationally intractable as it is a non-convex integer
programming problem. To address the challenge, we develop
multiple heuristics to approximately solve the optimization
problem. Our heuristics are inspired by previous work [13]
on attacking graph-based recommender systems. Figure 2
shows the overview of our data poisoning attack. First, we
approximate the hit ratio using a loss function, where a smaller
loss roughly corresponds to a higher hit ratio. Given the
loss function, we transform the optimization problem into a
tractable one. Second, based on our designed loss function,
we construct a poison model to simulate a compromised deep
learning based recommender system. In particular, we Ô¨Årst pre-
train the poison model to ensure that it can correctly predict
the preferences of users by using the validation dataset, and
then we update the poison model using a loss function, which
is derived by extracting the attack related parts in the loss
function obtained in the Ô¨Årst step, to approach the expected
state of the compromised target recommender system. Third,
we select Ô¨Åller items for a fake user according to its rating
score vector predicted by the poison model and a selection
probability vector, where the selection probability vector of
items is periodically updated to choose Ô¨Åller items for the next
fake user. We repeat the second and third steps until m fake
users are generated for the poisoning attack.

B. Approximating the Hit Ratio

The optimization problem we formulated in Eq. (1) is
computationally intractable because the rating scores are in-
teger variables in the domain {0, 1, . . . , rmax} and the hit
ratio is a highly non-linear non-differentiable function of
the rating scores due to the complexity of the recommender

4

Fig. 2: An overview of our data poisoning attack. We Ô¨Årst use approximation methods to transform the optimization problem
into a tractable one and obtain a loss function. Second, according to the obtained loss function, the algorithm used in the target
recommender system, and the training dataset, we train a poison model that simulates the compromised target recommender
system. Third, we select Ô¨Åller items according to the predicted rankings generated by the poison model and the selection
probability. Note that, we will repeat the second and third steps until enough fake users are generated to construct the attack,
and the selection probability will be updated in each iteration.

system. To address the computational challenge, we design
multiple techniques to convert the optimization problem into
a computationally tractable one.

Relaxing Rating Scores to Obtain Continuous Variables.
As for the rating scores in Y and
Y, we can treat them as
continuous variables in our attacking process. SpeciÔ¨Åcally, the
predicted rating scores, which range from 0.0 to 1.0 in the
recommender systems built upon implicit datasets, can be seen
as correlations between users and items. After acquiring Ô¨Ånal
rating scores from the target recommender system, we can
project them into discrete integer numbers if necessary.

(cid:98)

Approximating the Hit Ratio. The hit ratio HRt
is the
proportion of normal users whose top-K recommendation lists
include the target item t. Since HRt is a highly non-linear non-
differentiable function of the users‚Äô rating scores, we propose
to use a loss function to approximate it. In particular, a smaller
loss roughly corresponds to a higher hit ratio. Normally, a
recommender system uses the predicted user-item interaction
Y to make recommendations for users. Therefore, we
matrix
propose to use the following steps to convert the optimization
problem as shown in Eq. (1).

(cid:98)

1)

Loss Function for Each User. We leverage a loss
function lu over the predicted score vector for each
user to increase the hit ratio for target item t. Intu-
itively, if the target item t is already included in the
recommendation list Lu of user u, it is not necessary
to further improve this recommendation. Otherwise,
we should reÔ¨Çect the requirement of the user u in lu
and promote the target item t to get a better ranking
among all items. We apply the following loss function
for user u:

lu = max{min
i‚ààLu

log[

yui] ‚àí log[

yut], ‚àíŒ∫},

(2)

(cid:98)

where Œ∫ ‚â• 0 is a tunable parameter that can be
(cid:98)
used to enhance the robustness and transferability
of our attack. The use of the log operator lessens
the dominance effect, while preserving the order of
conÔ¨Ådence scores due to the monotonicity. As we can
see, if a target item t is in Lu, lu will be 0 when
Œ∫ = 0. Otherwise, larger
yut that is smaller than the

(cid:98)

5

(cid:98)

minimum value of
yui in Lu, larger the positive value
of lu will be. Œ∫ can ensure the target item t keep a
distance from the item with the lowest rating in Lu.
Thus, we can have a higher probability to include the
target item t in the recommendation list of user u by
minimizing the loss function lu.
Loss Function for All Users. Now we build a loss
function for all users. Since our attack goal is to
promote the target item to as many users as possible,
we design a loss function over all users according to
Eq. (2) as follows:

l(cid:48) =

lu,

(3)

(cid:88)u‚ààS
where S is the set of all normal users who have not
rated target item t yet.
Converting the Optimization Problem. After re-
laxing discrete variables to continuous variables and
approximating the hit ratio, we can approximate the
optimization problem as follows:
min G[y(v)] = (cid:107)y(v)(cid:107)2

2 + Œ∑ ¬∑ l(cid:48)

(4)

2)

3)

subject to yvi ‚àà [0, rmax],

where Œ∑ > 0 is a coefÔ¨Åcient to achieve the objective
of promoting the target item t with a limited number
of ratings from fake users. Here, we use the (cid:96)2
norm to replace the (cid:96)0 norm in Eq. (1), in order to
facilitate the calculation of gradients and the stepwise
approximation of global optimal values because the
(cid:96)0 norm can only compare a limited number of Ô¨Åller
item combinations and cannot continuously change,
while (cid:96)1 regularization generates sparse rating score
vectors, which will reduce the diversity of the se-
lected Ô¨Åller items for fake users. As for the constraint
on the number of Ô¨Åller items, we can achieve it by
choosing only a limited number of items for fake user
v based on his Ô¨Ånal rating score vector y(v). Thus, we
can generate fake users by solving the optimization
problem above.

As the users and items in deep learning based recommender
systems are completely with discrete labels, gradients will

Predictedratings(2) Constructing The Poison Model(3)SelectingFillerItemsLossfunctionSelectionProbabilityAlgorithmRatingdata(1)ApproximatingTheHitRatioTraining DatasetSelecting Filler Items for UsersUpdatedratingdataFakeUserPoison TrainingPre-trainingTarget Systemdisappear when they back-propagate to the input layer. Thus,
it is infeasible to directly adopt the back-gradient optimization
method that has been applied to attack image classiÔ¨Åers [33].
A natural idea is to treat the rating score vector y(v) of fake
user v as the independent variable and formulate the poisoning
attack as follows:

can be inserted fake users one by one to simulate attack results.
In order to make the poison model change towards our desired
objective, we need to deÔ¨Åne an effective loss function to update
the model iteratively. According to the optimization problem in
Eq. (5), we propose the following loss function for the poison
model in the attack:

min
y(v)

G[y(v), w‚àó]

subject to w‚àó = arg min

w

L[w, Y ‚à™ y(v)],

(5)

where w‚àó represents model parameters, and L is the original
loss function for training the target recommender system. This
is a bilevel optimization problem as the lower-level constraint
for w‚àó also depends on y(v). It is quite challenging to solve
this optimization problem for deep learning models because the
model parameters w‚àó need to be updated through re-training
the model once y(v) changes. The process would be time-
consuming because it needs to generate enough fake users
if we directly compute high order gradients w.r.t. y(v) and
repeat the training process with the whole dataset in each
iteration when we gradually update the rating score vector
y(v). In particular, we require a large number of of iterations,
even thousands of iterations, to accumulate enough changes
on the randomly initialized rating score vector for each fake
user, which is not practical for large recommender systems
in the real world. Moreover, the rating score matrix used by
the recommender systems is usually sparse, and the neural
network trained on it might generate predicted rating scores
that vary within a certain range, which will be misleading
for gradient-based optimization algorithms since they are with
small
learning rate and can be easily interfered with the
randomness of model training.

C. Constructing the Poison Model

SpeciÔ¨Åcally in this step, we construct the poison model to
guide the selection of Ô¨Åller items for each fake user according
to the obtained loss functions so that we can efÔ¨Åciently
the attack. Here, we investigate and utilize the
construct
characteristics of a recommender system itself from a new
perspective. For a deep learning based recommender system,
as a special type of neural network, it tries to reduce the
entropy between users‚Äô predicted score vectors and real rating
score vectors during the training process. Intuitively, items
with higher scores in user u‚Äôs predicted rating score vector
are more likely to have been rated by user u in reality with
high scores than other items. If we can successfully construct
a poison model to simulate the expected state of the original
recommender system after a successful poisoning attack, we
can infer what kind of fake users in the training dataset
can contribute most to the current recommender system. The
poison model, derived from the initial target recommender
system, periodically updates during the attack to approach our
attack goal gradually. We can then use the poison model to give
predictions on fake users‚Äô preferences and choose the items
with the highest predicted rating scores as Ô¨Åller items for fake
users.

Note that, the internal structure and hyperparameter settings
of the poison model are consistent with the target recommender
system. Moreover, its training datatset should be identical to
the original training dataset of the target system initially and

l = L + Œª ¬∑ G[

y(v)],

(6)

(cid:98)

where L is the loss function chosen in the process of training
(cid:98)
the original recommender system, e.g., the binary cross entropy
over the whole training dataset, G[
y(v)] correlates strongly
with our attack goal, and Œª > 0 is a coefÔ¨Åcient that trades off
between the model validity and the attack objective, which al-
lows us to generate the poison model close to the recommender
system trained under normal circumstances, while achieving
the validity correlates with L and
our attack goal. Here,
measures the degree to which the model accurately predicts
user preferences on the validation dataset. We use the predicted
y(v) of fake user v to replace v‚Äôs real
rating score vector
rating score vector y(v) according to the correlation between
them such that we can avoid high order gradient calculation,
which is really time-consuming. Note that, if the validity of
the poison model is much lower than that of a model trained
normally with the same dataset and the original loss function
(i.e., L), it is less likely that the poison model approximates the
Ô¨Ånal state of a compromised target recommender system since
the target recommender system will always use a validation
dataset to guarantee its best performance in the normal model
training process. Thus, it is necessary to ensure the validity of
the poison model during the attack. In order to make the poison
model better simulate the results of the poisoning attack, we
design two stages of training: i.e., pre-training and poison
training.

(cid:98)

Pre-training. The poison model will be randomly initialized
at Ô¨Årst and trained on its training dataset with the same
loss function (i.e., L) as the target recommender system.
After enough iterations, the poison model will be similar to
the recommender system obtained from the normal training,
ensuring the validity of the model. We can utilize this model
to start poison training subsequently.

Poison Training. The poison model in this stage will use Eq.
(6) as loss function and be trained repeatedly w.r.t all model
parameters inside it with the back-propagation method. We
select the initial Œª such that the loss of the poison model
on the validation dataset and the loss that models the attack
effectiveness are roughly in the same order of magnitude. In
the training process, the poison model will get closer to our
attack goal and eventually become an ideal state of the target
recommender system. We can then use the poison model to
help the item selection process for fake users.

D. Selecting Filler Items

Now we can select Ô¨Åller items for each fake user based on
predicted ratings generated by the acquired Ô¨Ånal poison model
in the last poison training process. Note that, items with higher
scores in the user‚Äôs predicted rating score vector given by the
recommender system tend to have greater relevance to the user
since the system reduces the entropy between users‚Äô predicted
score vectors and real rating score vectors during the training
process. Thus, as long as we get a reasonable poison model,

6

y(v) for fake
we can obtain the predicted rating score vector
user v according to the model, and the top-n items other than
target item t will be selected as Ô¨Åller items for fake user v.

(cid:98)

However, as the datasets used in recommender systems
are usually very sparse and the models trained from the data
have high randomness, the recommendation results of deep
learning based recommender systems for speciÔ¨Åc users and
items tend to be unstable, which means fake users obtained
from the poison model may not be good choices. Thus, if
we always directly use the predictions of the poison model
to select Ô¨Åller items for fake users, we are more likely to
gradually deviate from the right direction. In order to avoid
this, we develop a concept of selection probability, i.e., the
probability of an item being selected as Ô¨Åller items. We deÔ¨Åne
a selection probability vector as p = {p1, p2, . . . , pN }, each
element of which represents the selection probability of the
corresponding item. If item i is selected as Ô¨Åller item, pi will
change as follows:

pi = pi ¬∑ Œ¥,

(7)

where 0 ‚â§ Œ¥ ‚â§ 1 is an attenuation coefÔ¨Åcient that reduces the
selection probabilities of selected items. The more times an
item is selected as Ô¨Åller item, the lower its selection probability.
Note that, p is initialized to a vector with all element values
of 1.0 at Ô¨Årst. If all elements in p are below 1.0 after the
poisoning attack, p will be initialized again. After the poison
y(v) for fake user v,
model gives predicted rating score vector
we combine it with p to guide Ô¨Åller item selection as follows:

rv =

y(v)pT.

(cid:98)

(8)

According to Eq. (8), we select these items with the highest
(cid:98)
n scores in rv for fake user v as Ô¨Åller items and update the
corresponding selection probabilities using Eq. (7). The use
of selection probability refrains from repeated selection of
speciÔ¨Åc items, and provides greater chance of being selected
for more candidate items, which allows the target item to
build potential correlations with more other items and makes
our attack more likely to be effective globally. As for the
recommender systems with sparser datasets, which means
greater uncertainty of recommendation results, we recommend
to choose a smaller Œ¥ to strengthen the internal system con-
nectivity, i.e., the target item can correlate more other items,
which avoids local optimal results and improves the attack
performance. Combining above insights, we can effectively
solve the optimization problem.

The heuristics that solve the optimization problem are
shown in Algorithm 1. Note that, since our attack is not des-
ignated to speciÔ¨Åc deep learning recommendation systems and
can be generalized to any deep learning based recommender
system, the algorithm can opt to solve the problems in various
systems. Our item selection follows three steps. First, we use
y(v) for a
the poison model to predict a rating score vector
fake user v. Second, we compute the element-wise product of
y(v) and a selection probability vector p as an adjusted rating
score vector rv. Third, we select the n non-target items with
the largest adjusted rating scores as the Ô¨Åller items for v. For
(cid:98)
each Ô¨Åller item i for v, we decrease its selection probability pi
by multiplying it with a constant (e.g., 0.9) such that it is less
likely to be selected as a Ô¨Åller item for other fake users. We use
the selection probability vector to increase the diversity of the
fake users‚Äô Ô¨Åller items so that the target item can be potentially

(cid:98)

Algorithm 1 Our Attack Method
Input: User-item interaction matrix Y, target item t, param-

eters m, n, K, Œª, Œ∑, Œ∫.

Output: m fake users v1, v2, . . . , vm.

1: // Add fake users one by one
2: for v = v1, v2, . . . , vm do
3:

size.

Initialize poison model Mp with expanding input user

4:
5:
6:
7:

8:
9:

Add the rating tuple (v, t, rmax) to Y.
Pre-train Mp on Y with L.
Start poison training to get the Ô¨Ånal poison model Mp.
y(v) for
Use Mp to give predicted rating score vector

user v.

Get rv using Eq. (8).
Choose these items other than t with the highest n

(cid:98)

scores in rv as Ô¨Åller items.
Update p using Eq. (7).
Generate rating scores for chosen Ô¨Åller items, consti-

10:
11:

tuting rating score vector y(v) for user v.

Y ‚Üê Y ‚à™ y(v).

12:
13: end for
14: return y(v1), y(v2), . . . , y(vm).

correlated with more items. Note that we assume the same
n is used for each fake user. However, the attacker can also
use different number of Ô¨Åller items for different fake users. In
particular, an attacker can use our attack to add Ô¨Åller items for
a fake user one by one and stop adding Ô¨Åller items once the hit
ratio of the target item begins to decrease. Finally, we generate
rating scores for each Ô¨Åller item according to their previous
Ô¨Åtted normal distributions to ensure their scores much similar
to other normal ratings, which also be used to effectively evade
detection. We will elaborate on the detection performance in
Section VI. Note that, to speed up the process of generating
all fake users, we can also choose to generate s (s > 1) fake
users each time at the cost of reducing the Ô¨Åne-grained control
on the attack effectiveness.

V. EXPERIMENTS

In this section, we Ô¨Årst present the experimental setup.
Second, we evaluate the effectiveness and the transferability
of our poisoning attacks.

A. Experimental Setup

Datasets. We use three real-world datasets to perform our poi-
soning attacks. They are MovieLens-100K [19], MovieLens-
1M [19] and Last.fm [2], two different types of typical recom-
mender system datasets. MovieLens-100K is a classic movie
dataset which consists of 943 users, 1,682 movies and 100,000
ratings ranging from 1 to 5. Each user has at least 20 ratings in
this dataset. Similarly, MovieLens-1M is a larger movie dataset
including 6,040 users, 3,706 movies and 1,000,209 ratings
ranging from 1 to 5. Last.fm is a music dataset which contains
1,892 users, 17,632 music artists and 186,479 tag assignments.
A user can assign a tag to a music artist, which can be seen as
a positive interaction between them. Last.fm is a pure implicit
dataset as the tag assignments cannnot be quantiÔ¨Åed with
numerical values. We apply several data processing strategies
to make it suitable for our experiments. First, we binarize

7

their interactions as 1.0 for positive ones and 0.0 for others,
which can be seen as implicit ratings. Second, we drop the
duplicates in the datasets as one user might assign multiple
tags to one artist. Third, as the obtained dataset is still sparse,
we iteratively Ô¨Ålter the dataset to make sure the remaining in
the dataset has at least 10 ratings for each user and item (i.e.,
artist) to avoid the ‚Äúcold start‚Äù problem. We end up with a
dataset of 701 users, 1,594 items and 36,626 ratings.

Note that we use implicit training dataset for our target
recommender system (i.e., NeuMF), so we also project the
ratings in MovieLens-100K and MovieLens-1M to 1.0 when
they are larger than 0 and 0.0 otherwise. For a binary implicit
rating score, 1.0 indicates that the user has rated the item, but
it does not necessarily represent that the user likes the item.
Likewise, an implicit rating score of 0 does not necessarily
represent that the user dislikes the item. Our attack can be
also applied to recommender systems based on explicit ratings
since the ratings can be normalized between 0 and 1 before
training for such datasets.

Target Recommender System. In our experiments, we evalu-
ate the effectiveness of the attacks by using Neural Matrix
Factorization (NeuMF) as the target recommender system.
Note that, collaborative Ô¨Åltering systems are one of the most
popular and effective recommender systems in the real world.
Most websites (e.g., Amazon, YouTube, and NetÔ¨Çix) utilize
collaborative Ô¨Åltering as a part of their recommender systems.
Moreover, Neural Collaborative Filtering (NCF) is the typical
representative of deep learning based recommender systems.
Thus, we choose NeuMF, an instantiation of NCF, as the target
recommender system since matrix factorization is the most
popular technique of collaborative Ô¨Åltering.

Baseline Attacks. We compare our poisoning attack to several
existing poisoning attacks. In all these attacks, an attacker in-
jects m fake users to the target recommender system. Different
attacks use different strategies to select Ô¨Åller items for fake
users.

1)

2)

Random Attack: In a random attack, the attacker
randomly chooses n Ô¨Åller items for each fake user.
If the training dataset is explicit, the attacker will
Ô¨Åt normal distributions on the initial user-item in-
teraction matrix to generate new continuous rating
scores for Ô¨Åller items. These rating scores will then
be projected to discrete integer numbers if necessary.
Even if the training data is implicit, if the initial form
of the dataset collected by the recommender system
is explicit, the attacker still needs to use the same
method to generate rating scores for Ô¨Åller items to
evade detection.
Bandwagon Attack: In a bandwagon attack,
the
popularity of items plays a role in the selection of
Ô¨Åller items. We use the average score of the item to
represent its popularity on the explicit dataset, and the
frequency of the item to represent its popularity on
the implicit dataset. We randomly choose n √ó 10%
items from the set of 10% items with the highest
popularity and n √ó 90% items among the left unse-
lected items to constitute all Ô¨Åller items. Then we can
generate rating scores for Ô¨Åller items with the same
method in random attacks.

3)

Poisoning Attack to MF-based Recommender Sys-
tems (MF Attack) [12], [28]: MF attack is one of
effective poisoning attacks on the most widely used
recommender systems, namely matrix-factorization
(MF) based recommender systems. Note that MF
is a traditional and non-deep learning approach for
recommender systems, and our work is the Ô¨Årst to
implement well-designed poisoning attacks on deep
learning based recommender systems, and the target
recommender system we conduct experiments on is
generally a special kind of matrix-factorization-based
recommender systems. SpeciÔ¨Åcally, we use the PGA
attack in [28] as the baseline MF attack in our
experiments. In an MF attack, the attacker will initial-
ize a generalized user-item interaction matrix based
on the training dataset at Ô¨Årst and then implement
optimized poisoning attack on it. We will inject the
fake users generated by MF attack into our target
deep learning based recommender system to evaluate
the effectiveness of this poisoning attack and compare
with other poisoning attacks. Note that, the existing
poisoning attack [12] to matrix-factorization-based
recommender systems cannot be directly applied be-
cause it requires deriving the inÔ¨Çuence function for
NCF, which can be an interesting topic for future
work. Thus, we Ô¨Ånally choose the PGA attack in [28]
as the MF attack to conduct our experiments.

Target Items. We evaluate two types of target items in our
work, i.e., random and unpopular target items. Random target
items are sampled uniformly at random from the whole item
set, while unpopular items are collected randomly from those
items with less than 6 ratings, 10 ratings and 12 ratings for the
ML-100K (i.e., MovieLens-100K), ML-1M (i.e., MovieLens-
1M), and Music (i.e., Last.fm) datasets, respectively. To make
our results more convincing, we sample 10 instances for
each kind of target items by default and will average their
experimental results respectively.

Evaluation Metrics. We use the hit ratio of target item t (i.e.,
HRt@K) as the metric to evaluate the effectiveness of poison-
ing attacks for promoting target item t. Suppose there are K
items in the recommendation list for each user. HRt@K is the
proportion of normal users whose top-K recommendation lists
include target item t. We compare HRt@K before and after
attacks to show the attack effectiveness. As the deep learning
based recommender system itself is usually unstable, we train
and evaluate the target model on the dataset for 30 times and
average the evaluation results. Note that, for the same kind of
target items, we will further use the average value of their hit
ratios, recorded as HR@K, to comprehensively evaluate attack
performance.

Parameter Setting. Unless otherwise mentioned, the param-
eter setting for our poisoning attacks is as follows: Œ∫ = 1,
Œª = 0.01, Œ∑ = 100, and Œ¥ = 0.9 for the ML-100K dataset;
Œ∫ = 1, Œª = 0.01, Œ∑ = 100, and Œ¥ = 0.8 for the ML-1M
dataset; Œ∫ = 1, Œª = 0.01, Œ∑ = 100, and Œ¥ = 0.3 for the
Music dataset; and m equals to 1% of the number of normal
users, n = 30, and K = 10 for all datasets. We conduct our
experiments on a CentOS server with 4 NVIDIA Tesla V100
GPUs, 64-bit 14-core Intel(R) Xeon(R) CPU E5-2690 v4 @
2.60GHz and 378 GBs of RAM.

8

TABLE I: HR@10 for different attacks with different attack sizes.

Dataset

Attack

Random target items

Unpopular target items

Attack size

ML-100K

Music

None
Random
Bandwagon
MF
Our attack
None
Random
Bandwagon
MF
Our attack

0.5%

0.0025
0.0028
0.0030
0.0032
0.0034
0.0024
0.0037
0.0036
0.0034
0.0047

1%

3%

5%

0.0025
0.0034
0.0034
0.0035
0.0046
0.0024
0.0048
0.0046
0.0050
0.0068

0.0025
0.0053
0.0055
0.0069
0.0100
0.0024
0.0115
0.0104
0.0120
0.0144

0.0025
0.0078
0.0081
0.0090
0.0151
0.0024
0.0216
0.0176
0.0210
0.0243

0.5%

0
0.0002
0.0002
0.0001
0.0007
0.0003
0.0006
0.0005
0.0005
0.0012

1%

3%

5%

0
0.0003
0.0004
0.0002
0.0019
0.0003
0.0014
0.0011
0.0017
0.0026

0
0.0013
0.0013
0.0014
0.0111
0.0003
0.0053
0.0044
0.0058
0.0086

0
0.0025
0.0024
0.0033
0.0206
0.0003
0.0118
0.0094
0.0118
0.0161

B. Effectiveness of Poisoning Attacks

Now we conduct our experiments under the white-box
setting. Under this setting, we assume that the attacker is
aware of the internal structure,
the training data and the
hyperparameters of the target recommender system so that we
can train an initial poison model that has similar functions with
the target recommender system.

Impact of the Number of Inserted Fake Users. Table I shows
the results of poisoning attacks with different number of fake
users. We measure the effectiveness of attacks with different
attack sizes, i.e., the fraction of the number of fake users
to that of original normal users. In the table, ‚ÄúNone‚Äù means
no poisoning attacks performed on the target recommender
system and MF represents the poisoning attack method on
matrix-factorization-based recommender systems. We Ô¨Ånd that
our attack is very effective in promoting target items on both
datasets. For example, after inserting only 5% fake users
into the Music dataset, the hit ratio for random target items
increases by about 9.1 times.

Also, we observe that the attack performance of all at-
tack methods increases as the number of fake users inserted
increases. For instance, after injecting 0.5% fake users for
random target items to the ML-100K dataset, our attack can
achieve a hit ratio of 0.0034, while the hit ratio increases
to 0.0151 when injecting 5% fake users. The results are
reasonable because, when more fake users are inserted, the
target items occur more in the poisoned training dataset and
thus can inÔ¨Çuence the recommender system more signiÔ¨Åcantly.

Our attack signiÔ¨Åcantly outperforms the baseline attacks
in all cases. As for the ML-100K dataset, our attack is
quite outstanding and comprehensively surpasses all compared
methods. In particular, when inserting 5% fake users for un-
popular target items, our attack achieves the hit ratio of 0.0206,
about 6.2 times of the best hit ratio obtained by other attacks.
With the Music dataset, our attack is still the most effective
for all situations. For instance, our attacks can improve the
hit ratio of unpopular target items from 0.0003 to 0.0086
with an attack size of 3%. The MF attack achieves the best
performance among the baseline attacks. It can increase the hit
ratio to 0.0058, which is only 67.4% of that of our attack. The
possible reason is that the random attack and the bandwagon
attack do not leverage the information of deep learning models,

e.g., the model structure and parameters, so that they cannot
perform well on deep learning based systems. The MF attack
is designed for factorization-based recommender systems that
use linear inner product of latent vectors to recover Y, while
the target deep learning based recommender system in our
experiments uses extra nonlinear structure. Thus, the MF attack
cannot achieve good attack effectiveness as our attack.

To further evaluate the effectiveness of poisoning attacks
on large datasets, we conduct the experiments on the ML-
1M dataset with an attack size of 5% and sample 5 items for
each type of target items. Note that, to speed up our poisoning
attack, we generate 5 fake users each time. The results are
shown in Table II. First, we observe that, similar to the small
datasets,
is also vulnerable to poisoning
attacks. The hit ratio of unpopular target items increases from
0 to 0.0034 and 0.0060 with the bandwagon attack and our
attack, respectively. Second, our attack still performs the best
among all poisoning attacks on both random target items and
unpopular ones. For example, the hit ratio of random target
items under our attack is 0.0099, about 1.2 times of the highest
result among the baseline attacks.

the large dataset

Moreover, the increase of the hit ratio of unpopular target
items is much more signiÔ¨Åcant than that of random target
items. For instance, when injecting 5% fake users into the
Music dataset, the hit ratio of our attacks for random target
items increases by around 9.1 times compared with initial
hit ratio while that of unpopular items increases by about
52.7 times. We suppose that it is caused by the existence
of competing items. When the hit ratio of the target item
increases, the hit ratios of other items correlated with it also
tend to increase. As the sum of all hit ratios is Ô¨Åxed (i.e., 1),
there will be a competitive relationship between them when
the hit ratios on both sides rise to a large value. Unpopular
items have few correlated items since they have few ratings
in the original dataset. Therefore, after a successful attack,
there will be fewer items competing with them than random
target items. This result is encouraging because the items that
attackers want to promote are usually unpopular ones.

Furthermore, all poisoning attacks on the Music dataset are
more effective than the ML-100K and ML-1M datasets. For
example, when promoting random target items with our attack
method, an attacker can increase the hit ratio by about 5.0
times, 4.8 times, and 9.1 times for the ML-100K, ML-1M and

9

TABLE II: HR@10 on a large dataset.

Dataset

Attack

Target items
Random Unpopular

ML-1M

None
Random
Bandwagon
MF
Our attack

0.0017
0.0069
0.0080
0.0060
0.0099

0
0.0024
0.0034
0.0029
0.0060

TABLE III: HR@K for different K.

Dataset

Attack

ML-100K

Music

None
Random
Bandwagon
MF
Our attack
None
Random
Bandwagon
MF
Our attack

K

5

10

15

20

0
0.0002
0.0002
0.0002
0.0012
0.0001
0.0005
0.0003
0.0006
0.0007

0
0.0003
0.0004
0.0002
0.0019
0.0003
0.0014
0.0011
0.0017
0.0026

0
0.0005
0.0006
0.0004
0.0033
0.0005
0.0025
0.0018
0.0029
0.0042

0
0.0006
0.0007
0.0006
0.0042
0.0007
0.0037
0.0027
0.0040
0.0061

Music dataset, respectively, after injecting 5% fake users. The
possible reason is that the Music dataset is more sparse, mak-
ing the recommender system less stable and more vulnerable
to the poisoning attacks. The standard deviations of the hit
ratios in Table I can be found in Appendix A, and the change
of the hit ratio for each target item is presented in Appendix
B. These results further demonstrate the effectiveness of our
attack.

Impact of the Number of Recommended Items. Table III
shows the results of poisoning attacks with different numbers
of recommended items (i.e., K) in a recommendation list.
Attack size for all poisoning attacks is set to 1% and the
number of Ô¨Åller items (i.e., n) is set to by default 30 for all
methods. We choose unpopular target items to conduct our
experiments. First, we observe that our attack is still the most
effective method among all the poisoning attacks in all cases,
e.g., when K = 20, the hit ratio of our attack on the ML-100K
dataset is about 6.0 times of the best hit ratio achieved by the
baseline attacks. On the Music dataset, we can observe similar
results. For example, the MF attack can increase the hit ratio
to 0.0040 when K = 20, which is the best among the existing
methods, while our attack achieves the performance of 0.0061,
about 1.5 times of the former.

The hit ratios of all methods tend to increase with K. As
we can see, the initial hit ratio with no injected fake users
increase when K increases on the Music dataset. Similarly,
hit ratios for all poisoning attacks gradually become larger
when K increases on both datasets. For instance, the hit ratio
of our attack on the ML-100K dataset when K = 20 is about
3.5 times of that when K = 5. A larger K means a greater
chance for target items to be included in the recommendation
list. This phenomenon is particularly obvious and signiÔ¨Åcant
in our attack.

10

Impact of the Number of Filler Items. Figure 3 illustrates
the results of poisoning attacks with different numbers of
Ô¨Åller items (i.e., n) that changes from 10 to 50. We choose
unpopular target items as our target items. We have some
interesting observations. First, our attack always outperforms
other existing poisoning attacks in all test cases, which further
demonstrates the effectiveness and robustness of our attack.
In particular, when n is relatively small, the performance of
our attack is still the best. Therefore, when an attacker tries
to insert as few Ô¨Åller items as possible to evade detection by
the target recommender system, our attack method is the best
choice to implement effective attacks. Second, the hit ratio
may not always increase when n increases. On the ML-100K
dataset, the performance of our attack increases Ô¨Årst and then
dicreases with the increase of n. It achieves the best result
when n = 20. The attack effectiveness of the MF attack
tends to decrease when n increases, while other attacks achieve
relatively stable performance. However, on the Music dataset,
the hit ratio of our attack descends Ô¨Årst and then ascends with
the increase of n, while the hit ratios of other attacks Ô¨Çuctuate.
These results show that there is no linear correlation between
the attack effectiveness and n. As with different datasets, the
most suitable n can be different for the existing poisoning
attacks. We suppose that, when n is small, each fake user can
only exert limited inÔ¨Çuence on the target recommender system,
while, when n is large, there might be some items that are
ineffective in promoting the target item, and even competing
items included in Ô¨Åller items. Thus, the best number of Ô¨Åller
items is closely related to the attack methods and the used
datasets.

Impact of Œ¥. As an important parameter used in our attack,
Œ¥ can affect the diversity of Ô¨Åller items and further impact
the attack effectiveness. We select two random target items
from the ML-100K dataset and the Music dataset respectively
and analyze the diversity of the Ô¨Åller items selected by our
attack. For simplicity, we inject 5% fake users. The results are
shown in Figure 4. First, we observe that Ô¨Åller items on both
datasets have good diversities. The highest frequency of Ô¨Åller
items on the ML-100K dataset is 13, around 1.4% of the total
number of normal users, and all other items have relatively
low frequency. On the Music dataset, the frequency of all Ô¨Åller
items is not larger than 2, indicating a strong diversity. Second,
the Ô¨Åller items on the Music dataset have a stronger diversity
than that on the ML-100K dataset. The Ô¨Åller items on the
Music dataset are more evenly distributed than those on the
ML-100K dataset and the average of their frequency is lower
than that of the ML-100K. The reason is that we use a smaller
Œ¥ for the Music dataset, which ensures a better diversity.

To further investigate the impact of Œ¥ on the attack effec-
tiveness, we change the value of Œ¥ and inject 5% fake users on
the ML-100K dataset for random target items. The results are
illustrated in Figure 5. First, we observe that Œ¥ has a signiÔ¨Åcant
inÔ¨Çuence on the attack effectiveness of our method on the ML-
100K dataset. The hit ratio for target items does not always
ascend when Œ¥ increases, and the best Œ¥ for the ML-100K
dataset is around 0.9. Second, compared to the hit ratio of
target items when Œ¥ = 1, i.e., no change is required for the
selection probability vector after generating a fake user, Œ¥ helps
to promote the attack effectiveness when Œ¥ < 1. Third, our
attack still outperforms other attack methods in most cases,
which demonstrates the robustness of our attack.

(a) ML-100K

(b) Music

Fig. 3: The impact of the number of Ô¨Åller items on the attack effectiveness.

(a) ML-100K

(b) Music

Fig. 4: The impact of Œ¥ on the diversity of Ô¨Åller items.

real ratings and predicted ratings. To accurately evaluate the
impact of the target item rated by fake users, we now consider
what if the target item is not rated by fake users by default
in various poisoning attacks. Note that, we set Œ¥ = 1.0 here
and select the (n+1) items with the highest adjusted predicted
rating scores as those items rated by the fake user in our attack,
and the baseline attacks follow their own rules to select the
(n+1) rated items for fake users. We choose the random target
items in the ML-100K dataset to conduct the experiments.
The experimental results are shown in Table IV. Compared
to the results presented in Table I, we can observe that the
effectiveness of all attack methods is reduced signiÔ¨Åcantly
when the target items are not selected by default. However,
our method remains effective as the hit ratio of target items
still increases by 1.6 times when injecting 5% fake users, while
other baseline attacks are ineffective in this scenario.

C. Attacks with Partial Knowledge

Fig. 5: The impact of Œ¥ on the attack effectiveness for the
ML-100K dataset.

Impact of the Target Item Rated by Fake Users. We assume
that each fake user will certainly rate the target item in the
attacks including our attack as well as the baseline attacks. It
is inspired by the observation that the most effective method to
promote an item is to assign it with high rating scores in the
training dataset due to the strong correlation between users‚Äô

In the experiments above, we assume that an attacker has
full access to the whole dataset of the target recommender
system, which does not always hold in practice. The attacker
may only have partial knowledge of the dataset. To evaluate the
effectiveness of different poisoning attacks under this setting,
we conduct further experiments with two different types of

11

1020304050Number of filler items0.00000.00050.00100.00150.0020HR@10NoneRandomBandwagonMFOur attack1020304050Number of filler items0.00050.00100.00150.00200.0025HR@10NoneRandomBandwagonMFOur attack050010001500Item ID024681012Frequency050010001500Item ID012Frequency0.30.40.50.60.70.80.91.00.0070.0080.0090.0100.0110.0120.0130.0140.015HR@10TABLE IV: HR@10 for different attacks without target items
selected by default.

Dataset

Attack

ML-100K

None
Random
Bandwagon
MF
Our attack

0.5%

0.0025
0.0025
0.0026
0.0026
0.0028

Attack size

1%

3%

5%

0.0025
0.0025
0.0026
0.0026
0.0034

0.0025
0.0024
0.0028
0.0027
0.0043

0.0025
0.0025
0.0024
0.0025
0.0064

partial knowledge. One partial knowledge is that the attacker
knows partial rating scores of all normal users, and the other is
that the attacker knows all rating scores of only partial normal
users. Note that, all these experiments are evaluated on the
original dataset that contains all users and all rating scores.
We use the random target items in our experiments. The results
are shown in Table VI and Table VII, respectively. According
to Table VI, we observe that, even with only 30% ratings of
the original rating matrix, the hit ratio of the random target
items in our attack is 0.0092, which is only slightly smaller
than that with full knowledge, i.e., 0.0099, (see Table II) and
much larger than the best result of baseline attacks, i.e., 0.0069
achieved by the random attack. However, the bandwagon attack
and the MF attack are much less effective with only partial
knowledge. Similarly, in Table VII, our attack still outperforms
the baseline attacks, and the hit ratio of our attack is only
slightly smaller than that with full knowledge. The results
demonstrate that our attack is still effective even when the
attacker only has partial knowledge of the training data, while
the bandwagon attack and the MF attack heavily relies on the
information informed from the observed dataset.

D. Transferability

In the previous experiments, we assume a white-box setting
under which an attacker knows the internal structure, the train-
ing data and the hyperparameters of the target recommender
system. As long as we use the known data and the model
structure to train a surrogate model locally, we can obtain a
model having a similar function to the target recommender
system. To further evaluate the transferability of our attack,
we consider the gray-box setting under which the attacker only
knows the algorithm and the training data used by the target
recommender system.

We assume that an attacker generates fake users based on
a surrogate model that is different from the internal structure
of the target recommender system. SpeciÔ¨Åcally, we change the
number of MLP layers to constitute a different target recom-
mender system. Note that, these target items and Ô¨Åller items
generated for all fake users under this setting are consistent
with that under the white-box setting. Table V shows the hit
ratios of our attacks and the existing attacks for both random
and unpopular target items on two datasets. First, both our
attack and the existing attacks can increase the hit ratio of
target items notably. For instance, our method increases the
hit ratio of random target items by about 22.8 times compared
to the initial hit ratio when the attack size is 5% on the Music
dataset.

Second, our method shows the best transferring effective-
ness in most situations, which means that our method has
better transferability than the existing attacks. For example,
our attack achieves the highest hit ratio of random target
items, i.e., 0.0150, with an attack size of 5% on the ML-100K
dataset, which is about 1.6 times of the best performance of
the baseline attacks. Similarly, on the Music dataset, our attack
increases the hit ratio of unpopular target items from 0.0001 to
0.0184 by injecting 5% fake users, while the existing attacks
obtain the highest hit ratio of 0.0101, which is 54.9% of ours.

Third, similar to the results under the white-box setting,
we can observe that the increase of the hit ratio on the Music
dataset is more notable than that on the ML-100K dataset. For
instance, with an attack size of 5% on random target items, our
attacks can increase the hit ratio by around 22.8 times and 5.5
times on the Music and the ML-100K datasets, respectively,
compared with the corresponding initial hit ratios. The reason
is that the Music dataset is more sparse, which makes the
recommender systems trained on it less stable and easier to be
compromised.

In summary, our attack achieves a better transferability
than the baseline attacks, which means that our attack poses a
greater threat to unknown target recommender systems.

VI. DETECTING FAKE USERS

In this section, we evaluate the effectiveness of the attack
under a detector built upon rating scores. Detecting fake users
is also known as Sybil detection. Many methods have been
proposed for Sybil detection. These methods leverage user
registration information [48], user-generated content [3], [44],
and/or social graphs between users [9], [16], [24], [41]‚Äì[43],
[47]. Since we have no access to users‚Äô registration information
and social graphs, similar to [13], we utilize a detection method
based on user-generated content, i.e., the ratings of users on
items. We extract useful features from the datasets and generate
certain feature values for each user. We train a fake user
classiÔ¨Åer for each poisoning attack to detect fake users. We
will study the effectiveness of the poisoning attacks when the
recommender system has deployed such a detector.

Rating Score Based Detection. Similar to the existing de-
fenses [7], [13], [32] that leverage several statistical features
from rating scores to distinguish normal users from fake users,
we adopt these features to train our detection classiÔ¨Åers. The
details of these features are described as follows.

‚Ä¢

Rating Deviation from Mean Agreement (RDMA) [7].
The feature indicates the average deviation of rating
scores of a user to the mean rating scores of the
corresponding items, which is computed as follows
for a user u:

RDMAu =

i‚ààIu
(cid:80)

|yui‚àíy(i)|
ci

| Iu |

,

(9)

where Iu is the set of items that user u has rated, | Iu |
is the number of items in Iu , yui is user u‚Äôs ratings
score for item i, y(i) is the average rating score of

12

TABLE V: HR@10 under the transferability setting.

Dataset

Attack

Random target items

Unpopular target items

Attack size

ML-100K

Music

None
Random
Bandwagon
MF
Our attack
None
Random
Bandwagon
MF
Our attack

0.5%

0.0023
0.0027
0.0027
0.0027
0.0038
0.0009
0.0020
0.0011
0.0015
0.0015

1%

3%

5%

0.0023
0.0035
0.0030
0.0036
0.0042
0.0009
0.0024
0.0025
0.0028
0.0022

0.0023
0.0070
0.0070
0.0064
0.0099
0.0009
0.0088
0.0074
0.0087
0.0128

0.0023
0.0083
0.0092
0.0096
0.0150
0.0009
0.0189
0.0160
0.0152
0.0214

0.5%

0
0.0002
0.0003
0.0003
0.0010
0.0001
0.0003
0.0001
0.0004
0.0007

1%

3%

5%

0
0.0005
0.0005
0.0005
0.0023
0.0001
0.0010
0.0004
0.0009
0.0014

0
0.0016
0.0018
0.0019
0.0082
0.0001
0.0042
0.0027
0.0049
0.0101

0
0.0030
0.0034
0.0035
0.0141
0.0001
0.0101
0.0091
0.0096
0.0184

TABLE VI: HR@10 on ML-1M dataset with a partial rating
matrix.

Knowledge level

Attack

Random target items

30%

None
Random
Bandwagon
MF
Our attack

0.0017
0.0069
0.0060
0.0040
0.0092

TABLE VII: HR@10 on ML-1M dataset with a subset of users.

Knowledge level

Attack

Random target items

30%

None
Random
Bandwagon
MF
Our attack

0.0017
0.0069
0.0057
0.0035
0.0091

item i, and ci is the total number of ratings for item
i in the whole dataset.

‚Ä¢ Weighted Degree of Agreement (WDA)

[32]. The
feature is the numerator of the RDMA feature, which
is computed as follows:

WDAu =

(cid:88)i‚ààIu

| yui ‚àí y(i) |
ci

.

(10)

Deviation
[32]. This

Agreement
‚Ä¢ Weighted
(WDMA)
feature considers more the
items that have less ratings, which is similar in form
to RDMA. It is calculated as follows:

from Mean

WDMAu =

i‚ààIu
(cid:80)

|yui‚àíy(i)|
c2
i

| Iu |

.

(11)

‚Ä¢ Mean Variance (MeanVar) [32]. This feature denotes
the average variance of rating scores of a uesr to the
mean rating scores of the corresponding items. The

13

MeanVar feature for a user u is computed as follows:

[yui ‚àí y(i)]2

MeanVaru =

i‚ààIu
(cid:80)

| Iu |

.

(12)

‚Ä¢

Filler Mean Target Difference (FMTD) [32]. This
feature measures the divergence between rating scores
of a user, which is obtained by:

FMTDu = (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

yui

i‚ààIuM
(cid:80)
| IuM |

yuj

j‚ààIuO
‚àí (cid:80)
| IuO |

,

(13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where IuM is the set of items in Iu that u gave
the maximum rating score and IuO includes all other
items in Iu.

For each kind of poisoning attack, we generate certain
amount of fake users and extract the same number of normal
users from the original dataset to form a user set. The corre-
sponding features for each user in the user set is calculated
to constitute a training dataset for fake user classiÔ¨Åer. In our
experiments, 300 normal users and 300 fake users are included
in the training dataset. We follow the workÔ¨Çow of SVM-
TIA [51] method and use the grid search with 5-fold cross
validation to select the best parameters for the classiÔ¨Åer. SVM-
TIA [51] is one of the state-of-the-art detection methods for
shilling attacks. The detection method contains two phases. In
the Ô¨Årst phase, i.e., the support vector machine (SVM) phase,
an SVM classiÔ¨Åer is used to Ô¨Ålter out a suspicious user set that
may contain both fake users and normal users. To keep the
normal users in the suspicious user set, the second target item
analysis (TIA) phase tries to Ô¨Ånd out target items by counting
the number of maximum rating (or minimum rating under
demotion attacks) of each item in the suspicious user set. Then
the items whose number of maximum rating exceeds threshold
œÑ will be regarded as target items under the assumption that
attackers will always give the maximum rating to target items.
The users who set target items the maximum rating will be
judged as fake users while others are viewed as normal users.
Here, œÑ is a hyperparameter that balances between Ô¨Åltering
out fake users and retaining normal users. That is, a higher œÑ
will cause fake users with a small attack size easier to escape
detection, while a lower œÑ makes the detector more likely to

TABLE VIII: Detection results for different attacks.

Dataset

Phase

Attack

ML-100K

SVM

TIA

Random
Bandwagon
MF
Our attack
Random
Bandwagon
MF
Our attack

0.5%

0.0106
0.0127
0.0191
0.1410
0.0001
0
0.0001
0.1267

FPR

1%

3%

5%

0.0106
0.0127
0.0191
0.1410
0.0001
0
0.0001
0.1273

0.0106
0.0127
0.0191
0.1410
0.0001
0.0003
0.0013
0.1283

0.0106
0.0127
0.0191
0.1410
0.0001
0.0008
0.0050
0.1290

0.5%

0.0200
0.0400
0.0400
0.3400
0.0200
0.0400
0.0400
0.3800

FNR

1%

3%

5%

0.0111
0.0222
0.0556
0.3444
0.0111
0.0222
0.0556
0.3444

0.0179
0.0214
0.0500
0.2357
0.0179
0.0214
0.0500
0.2357

0.0021
0.0127
0.0298
0.2340
0.0021
0.0127
0.0298
0.2340

incorrectly Ô¨Ålter out normal users in the suspicious user set. As
the attack size of our attack can be quite small (e.g., 0.5%), in
order to retain as many as normal users while maintaining the
ability to Ô¨Ålter out fake users, we set œÑ to 0.4% (i.e., œÑ = 4 for
ML-100K) of the total number of normal users, slightly small
to the smallest attack size (i.e., 0.5%) in our experiments.

Note that, before training and testing the classiÔ¨Åer, we
perform data scaling on the input data, which signiÔ¨Åcantly
improves the model performance in this scenario. After the
classiÔ¨Åer is trained well, we can simply deploy the classiÔ¨Åer
on the recommender system to Ô¨Ålter input training datasets
and include these users who are predicted to be fake users
into the suspicious user set. This rating scores based detection
method is designed for explicit dataset. Note that, the Music
dataset is purely implicit, which means that the above features
will always be 0. The detection process for implicit dataset re-
quires other complicated techniques like semantic analysis that
closely relates to the platform corpus. However, the workÔ¨Çow
for fake user detection can be similar to that on explicit dataset,
the main difference between them is features and the way
of obtaining feature values for subsequent classiÔ¨Åer training.
Here, we mainly conduct detection experiments with random
target items on the ML-100K dataset.

Effectiveness of Fake User Detectors. In the detection pro-
cess, we focus on whether the detector can effectively detect
false users and whether the detector affects the original dataset.
Here we use False Positive Rate (FPR) and False Negative
Rate (FNR) to evaluate the performance of the detector, where
FPR stands for the fraction of normal users who are falsely
predicted as fake users while FNR means the proportion of
fake users who are falsely predicted as normal users. The
detection results including both phases of SVM-TIA on the
ML-100K dataset are shown in Table VIII. First, the TIA
phase can decrease FPR after the SVM phase, while it does not
inÔ¨Çuence FNR in most cases. However, we can observe that
there is an abnormal increase in FNR when attack size is 0.5%
(which is slightly larger than œÑ ). This is because some fake
users have escaped detection in the SVM phase and the number
of the maximum ratings of the target items is lower than œÑ .
Thus, the detector cannot identify the target items and all fake
users will escape detection, which further increases the FNR.
Second, the fake user detectors are quite efÔ¨Åcient in detecting
fake users that are generated by the baseline attacks. As we can
see, FPRs and FNRs for these attacks under different attack
sizes are lower than 5% in most cases, which means most
fake users and normal users are correctly classiÔ¨Åed by the

TABLE IX: HR@10 for different attacks under detection.

Dataset

Attack

Attack size

ML-100K

None
Random
Bandwagon
MF
Our attack

0.5%

0.0025
0.0031
0.0032
0.0030
0.0030

1%

3%

5%

0.0025
0.0029
0.0029
0.0029
0.0029

0.0025
0.0023
0.0037
0.0036
0.0045

0.0025
0.0020
0.0019
0.0031
0.0067

detectors, showing the effectiveness of this detection method.
Third, the detector for our attack is not effective enough. FPR
is around 12% and FNR is around 30%, which means the
detector still makes a large amount of false judgements for our
attack and around 30% of fake users are successfully inserted
to the training dataset. According to the above observations,
it is obvious that it is much harder to detect our attack than
other baseline attacks.

Effectiveness of Poisoning Attacks under Detection. Now
we test the hit ratio of target items for different poisoning
attacks after deploying fake user detectors on the target recom-
mender systems. The experimental results are shown in Table
IX, where ‚ÄúNone‚Äù means there is neither poisoning attack
nor fake user detector deployed on the target recommender
system. The hit ratio of target items with baseline attacks
does not signiÔ¨Åcantly change with different attack sizes. As
shown in Table IX, overall our attack still outperforms the
baseline attacks, and the baseline attacks achieve only small
improvements on the initial hit ratio. In particular, our attack
is still effective under detection, e.g., when inserting 5% fake
users into the target recommender system, the hit ratio for
target items rise to 0.0067, about 2.7 times of the initial hit
ratio. The reason is that almost 30% of fake users are not
Ô¨Åltered out and they can still have a large impact on the target
recommender system. Note that, when the attack size is small
(e.g., 0.5%), many normal users that have rated the target items
are falsely Ô¨Åltered out by the detector, while only few fake
users are successfully injected into the dataset in our attack,
which leads to relatively low hit ratios. Even though, our attack
achieves similar performance to the baseline attacks when the
attack size is small.

Discussion. Attackers can use various strategies to evade
detection. For example, as the SVM-TIA detection method
heavily relies on the frequency distribution of items, an

14

attacker could evade detection by adjusting the process of
constructing fake users, e.g., avoiding selecting the same items
frequently by controlling the selection probability. Moreover,
an attacker can construct fake users without
items
selected by default, thus decreasing the frequency of the target
items. As our experimental results in Section V showed, when
the target items are not selected by fake users by default, our
attack remains effective and still signiÔ¨Åcantly outperforms the
baseline attacks.

target

Besides the above statistical analysis of the rating patterns
of normal and fake users, there are also some other detection
and defense mechanisms against data poisoning attacks. For
instance, Steinhardt et al. [39] bound the training loss when the
poisoned training examples are in a particular set, i.e., poisoned
training examples are constrained. It is an interesting topic for
future work to generalize such analysis to bound the training
loss of recommender systems when an attacker can inject a
bounded number of fake users. Paudice et al. [36] aim to
statistically analyze the features of training examples and use
anomaly detection to detect poisoned training examples. We
explore supervised learning based defenses in our experiments,
where the features are extracted from users‚Äô rating scores. As
future work, we can extend the anomaly detection method to
detect fake users based on statistical patterns of their rating
scores.

There are also certiÔ¨Åably robust defenses [22], [23], [29],
[40] against data poisoning attacks to machine learning al-
gorithms. However, recommender systems are different from
the machine learning algorithms considered in these work.
For instance, top-K items are recommended to each user in
a recommender system, while a machine learning classiÔ¨Åer
predicts a single label
is an
interesting future work to generalize these certiÔ¨Åed robustness
guarantee to recommender systems.

in these work. However,

it

VII. CONCLUSION AND FUTURE WORK

In this work, we show that data poisoning attack to deep
learning based recommender systems can be formulated as an
optimization problem, which can be approximately solved via
combining multiple heuristics. Our empirical evaluation results
on three real-world datasets with different sizes show that 1)
our attack can effectively promote attacker-chosen target items
to be recommended to substantially more normal users, 2)
our attack outperforms existing attacks, 3) our attack is still
effective even if the attacker does not have access to the neural
network architecture of the target recommender system and
only has access to a partial user-item interaction matrix, and
4) our attack is still effective and outperforms existing attacks
even if a rating score based detector is deployed. Interesting
future work includes developing new methods to detect the
fake users and designing new recommender systems that are
more robust against data poisoning attacks.

ACKNOWLEDGEMENT

We thank our shepherd Jason Xue and the anonymous
reviewers for their constructive comments. This work is sup-
ported in part by NSFC under Grant 61572278 and BNRist
under Grant BNR2020RC01013. Qi Li is the corresponding
author of this paper.

15

REFERENCES

[1]

[2]

J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, and
V. Shmatikov, ‚Äú‚Äù you might also like:‚Äù privacy risks of collaborative
Ô¨Åltering,‚Äù in 2011 IEEE Symposium on Security and Privacy.
IEEE,
2011, pp. 231‚Äì246.
I. Cantador, P. Brusilovsky, and T. KuÔ¨Çik, ‚Äú2nd workshop on informa-
tion heterogeneity and fusion in recommender systems (hetrec 2011),‚Äù
in Proceedings of the 5th ACM conference on Recommender systems,
ser. RecSys 2011. New York, NY, USA: ACM, 2011.

[3] Q. Cao, X. Yang, J. Yu, and C. Palow, ‚ÄúUncovering large groups of

active malicious accounts in online social networks,‚Äù in CCS, 2014.

[4] L. Chen, Y. Xu, F. Xie, M. Huang, and Z. Zheng, ‚ÄúData poisoning
attacks on neighborhood-based recommender systems,‚Äù Transactions on
Emerging Telecommunications Technologies, 2020.

[5] M. Chen, Z. Xu, K. Weinberger, and F. Sha, ‚ÄúMarginalized denoising
autoencoders for domain adaptation,‚Äù arXiv preprint arXiv:1206.4683,
2012.

[6] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,
G. Anderson, G. Corrado, W. Chai, M. Ispir et al., ‚ÄúWide & deep
learning for recommender systems,‚Äù in Proceedings of the 1st workshop
on deep learning for recommender systems. ACM, 2016, pp. 7‚Äì10.

[7] P.-A. Chirita, W. Nejdl, and C. ZamÔ¨År, ‚ÄúPreventing shilling attacks in
online recommender systems,‚Äù in Proceedings of the 7th annual ACM
international workshop on Web information and data management.
ACM, 2005, pp. 67‚Äì74.

[8] P. Covington, J. Adams, and E. Sargin, ‚ÄúDeep neural networks for
youtube recommendations,‚Äù in Proceedings of the 10th ACM conference
on recommender systems. ACM, 2016, pp. 191‚Äì198.

[9] G. Danezis and P. Mittal, ‚ÄúSybilinfer: Detecting sybil nodes using social

[10]

networks.‚Äù in NDSS, 2009.
J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, U. Gargi,
S. Gupta, Y. He, M. Lambert, B. Livingston et al., ‚ÄúThe youtube video
recommendation system,‚Äù in Proceedings of the fourth ACM conference
on Recommender systems. ACM, 2010, pp. 293‚Äì296.

[11] M. Dong, F. Yuan, L. Yao, X. Wang, X. Xu, and L. Zhu, ‚ÄúTrust in
recommender systems: A deep learning perspective,‚Äù arXiv preprint
arXiv:2004.03774, 2020.

[12] M. Fang, N. Z. Gong, and J. Liu, ‚ÄúInÔ¨Çuence function based data
poisoning attacks to top-n recommender systems,‚Äù in Proceedings of
The Web Conference 2020, 2020, pp. 3019‚Äì3025.

[13] M. Fang, G. Yang, N. Z. Gong, and J. Liu, ‚ÄúPoisoning attacks to
graph-based recommender systems,‚Äù in Proceedings of the 34th Annual
Computer Security Applications Conference. ACM, 2018, pp. 381‚Äì392.
[14] F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens, ‚ÄúRandom-walk
computation of similarities between nodes of a graph with application to
collaborative recommendation,‚Äù IEEE Transactions on knowledge and
data engineering, vol. 19, no. 3, pp. 355‚Äì369, 2007.

[15] X. Glorot, A. Bordes, and Y. Bengio, ‚ÄúDeep sparse rectiÔ¨Åer neural
networks,‚Äù in Proceedings of the fourteenth international conference
on artiÔ¨Åcial intelligence and statistics, 2011, pp. 315‚Äì323.

[16] N. Z. Gong, M. Frank, and P. Mittal, ‚ÄúSybilbelief: A semi-supervised

[17]

[18]

learning approach for structure-based sybil detection,‚Äù TIFS, 2014.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù in
Advances in neural information processing systems, 2014, pp. 2672‚Äì
2680.
I. Gunes, C. Kaleli, A. Bilge, and H. Polat, ‚ÄúShilling attacks against
recommender systems: a comprehensive survey,‚Äù ArtiÔ¨Åcial Intelligence
Review, vol. 42, no. 4, pp. 767‚Äì799, 2014.

[19] F. M. Harper and J. A. Konstan, ‚ÄúThe movielens datasets: History
and context,‚Äù Acm transactions on interactive intelligent systems (tiis),
vol. 5, no. 4, pp. 1‚Äì19, 2015.

[20] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, ‚ÄúNeural
collaborative Ô¨Åltering,‚Äù in Proceedings of the 26th international confer-
ence on world wide web.
International World Wide Web Conferences
Steering Committee, 2017, pp. 173‚Äì182.

[21] K. Hornik, M. Stinchcombe, and H. White, ‚ÄúMultilayer feedforward
networks are universal approximators,‚Äù Neural networks, vol. 2, no. 5,
pp. 359‚Äì366, 1989.

[22]

J. Jia, X. Cao, and N. Z. Gong, ‚ÄúCertiÔ¨Åed robustness of nearest
neighbors against data poisoning attacks,‚Äù Arxiv, 2020.

[23] ‚Äî‚Äî, ‚ÄúIntrinsic certiÔ¨Åed robustness of bagging against data poisoning

[24]

attacks,‚Äù in AAAI, 2021.
J. Jia, B. Wang, and N. Z. Gong, ‚ÄúRandom walk based fake account
detection in online social networks,‚Äù in DSN, 2017.

[25] S. Kapoor, V. Kapoor, and R. Kumar, ‚ÄúA review of attacks and its de-
tection attributes on collaborative recommender systems.‚Äù International
Journal of Advanced Research in Computer Science, vol. 8, no. 7, 2017.
[26] Y. Koren, R. Bell, and C. Volinsky, ‚ÄúMatrix factorization techniques for

recommender systems,‚Äù Computer, no. 8, pp. 30‚Äì37, 2009.

[27] S. K. Lam and J. Riedl, ‚ÄúShilling recommender systems for fun and
proÔ¨Åt,‚Äù in Proceedings of the 13th international conference on World
Wide Web. ACM, 2004, pp. 393‚Äì402.

[28] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, ‚ÄúData poisoning attacks
on factorization-based collaborative Ô¨Åltering,‚Äù in Advances in neural
information processing systems, 2016, pp. 1885‚Äì1893.

[29] Y. Ma, X. Zhu, and J. Hsu, ‚ÄúData poisoning against differentially-
private learners: Attacks and defenses,‚Äù in Proceedings of the Twenty-
Eighth International Joint Conference on ArtiÔ¨Åcial Intelligence, 2019,
pp. 4732‚Äì4738.

[30] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., ‚ÄúHuman-level control
learning,‚Äù
Nature, vol. 518, no. 7540, p. 529, 2015.

through deep reinforcement

[31] B. Mobasher, R. Burke, R. Bhaumik, and J. J. Sandvig, ‚ÄúAttacks and
remedies in collaborative recommendation,‚Äù IEEE Intelligent Systems,
vol. 22, no. 3, pp. 56‚Äì63, 2007.

[32] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams, ‚ÄúToward
trustworthy recommender systems: An analysis of attack models and al-
gorithm robustness,‚Äù ACM Transactions on Internet Technology (TOIT),
vol. 7, no. 4, p. 23, 2007.

[33] L. MuÀúnoz-Gonz¬¥alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, ‚ÄúTowards poisoning of deep learning
algorithms with back-gradient optimization,‚Äù in Proceedings of the 10th
ACM Workshop on ArtiÔ¨Åcial Intelligence and Security. ACM, 2017,
pp. 27‚Äì38.

[34] S. Okura, Y. Tagami, S. Ono, and A. Tajima, ‚ÄúEmbedding-based news
recommendation for millions of users,‚Äù in Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 2017, pp. 1933‚Äì1942.

[35] M. P. O‚ÄôMahony, N. J. Hurley, and G. C. Silvestre, ‚ÄúRecommender

systems: Attack types and strategies,‚Äù in AAAI, 2005, pp. 334‚Äì339.

[36] A. Paudice, L. MuÀúnoz-Gonz¬¥alez, A. Gy¬®orgy, and E. C. Lupu, ‚ÄúDetection
of adversarial training examples in poisoning attacks through anomaly
detection.‚Äù arXiv preprint arXiv:1802.03041, 2018.

[37] H. Pi, Z. Ji, and C. Yang, ‚ÄúA survey of recommender system from
data sources perspective,‚Äù in 2018 8th International Conference on
Management, Education and Information (MEICI 2018).
Atlantis
Press, 2018.

[39]

[38] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ‚ÄúItem-based collabo-
rative Ô¨Åltering recommendation algorithms,‚Äù in Proceedings of the 10th
international conference on World Wide Web, 2001, pp. 285‚Äì295.
J. Steinhardt, P. W. W. Koh, and P. S. Liang, ‚ÄúCertiÔ¨Åed defenses for data
poisoning attacks,‚Äù in Proceedings of the 31st International Conference
on Neural Information Processing Systems, 2017, pp. 3517‚Äì3529.
[40] B. Wang, X. Cao, J. Jia, and N. Z. Gong, ‚ÄúOn certifying robustness
against backdoor attacks via randomized smoothing,‚Äù in CVPR 2020
Workshop on Adversarial Machine Learning in Computer Vision, 2020.
[41] B. Wang, N. Z. Gong, and H. Fu, ‚ÄúGang: Detecting fraudulent users in
online social networks via guilt-by-association on directed graphs,‚Äù in
ICDM, 2017.

[42] B. Wang, J. Jia, and N. Z. Gong, ‚ÄúGraph-based security and privacy
analytics via collective classiÔ¨Åcation with joint weight learning and
propagation,‚Äù in NDSS, 2019.

[43] B. Wang, L. Zhang, and N. Z. Gong, ‚ÄúSybilscar: Sybil detection in
online social networks via local rule based propagation,‚Äù in INFOCOM,
2017.

[44] G. Wang, T. Konolige, C. Wilson, X. Wang, H. Zheng, and B. Y. Zhao,
‚ÄúYou are how you click: Clickstream analysis for sybil detection,‚Äù in
USENIX Security, 2013.

[45] X. Xing, W. Meng, D. Doozan, A. C. Snoeren, N. Feamster, and W. Lee,
‚ÄúTake this personally: Pollution attacks on personalized services,‚Äù
in Presented as part of
the 22nd {USENIX} Security Symposium
({USENIX} Security 13), 2013, pp. 671‚Äì686.

[46] G. Yang, N. Z. Gong, and Y. Cai, ‚ÄúFake co-visitation injection attacks

to recommender systems.‚Äù in NDSS, 2017.

[47] H. Yu, H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman,
‚ÄúSybilguard: defending against sybil attacks via social networks,‚Äù in
SIGCOMM, 2006.

[48] D. Yuan, Y. Miao, N. Z. Gong, Z. Yang, Q. Li, D. Song, Q. Wang,
and X. Liang, ‚ÄúDetecting fake accounts in online social networks at the
time of registrations,‚Äù in CCS, 2019.

[49] W. Zeller and E. W. Felten, ‚ÄúCross-site request forgeries: Exploitation

and prevention,‚Äù The New York Times, pp. 1‚Äì13, 2008.

[50] S. Zhang, L. Yao, A. Sun, and Y. Tay, ‚ÄúDeep learning based rec-
ommender system: A survey and new perspectives,‚Äù ACM Computing
Surveys (CSUR), vol. 52, no. 1, p. 5, 2019.

[51] W. Zhou, J. Wen, Q. Xiong, M. Gao, and J. Zeng, ‚ÄúSvm-tia a shilling
attack detection method based on svm and target item analysis in
recommender systems,‚Äù Neurocomputing, vol. 210, pp. 197‚Äì205, 2016.

APPENDIX

A. Standard Deviations of Experimental Results

In this section, we provide the standard deviations of
experimental results (see Table I in Section V-B), which is
corresponding to that
in Table X. We can observe some
interesting Ô¨Åndings in Table I and Table X. First, the increase
of standard deviations is slower than that of the average hit
ratios. For example, on the ML-100K dataset, the average
hit ratio for random target items is 0.0025 for the ‚ÄúNone‚Äù
setting, while the standard deviation of the hit ratios for these
target items is 0.0033, even larger than the former. As for all
attack methods, after injecting 5% fake users into the target
recommender system, all average hit ratios for random target
items are larger than the standard deviations for hit ratios
of these target items, which indicates that all attack methods
can promote target items. Second, our attack have the highest
standard deviations in most cases. The reason is that our attack
promotes target items most signiÔ¨Åcantly among all attacks and
the hit ratios for some target items tend to increase faster than
others.

B. Hit Ratio per Target Item

In this section, we show the change of the hit ratio for
each target item in different attacks with different attack sizes.
We count the number of target items whose hit ratio has
been promoted compared to the original value. The results
are shown in Table XI. We Ô¨Ånd that not all target items can
get promoted when injecting limited number of fake users,
which often happens when the attack size is very small. As
the attack size increases, more target
items get promoted
and Ô¨Ånally all target items obtain an increased hit ratio in
all attack methods when 5% fake users are injected. More
importantly, we observe that our attack increases the hit ratios
of most selected target items, especially when the attack size
is small, e.g., 0.5%. All these results demonstrate our attack is
effective to promote target items in poison deep learning based
recommender systems.

16

TABLE X: Standard deviations for different attacks with different attack sizes.

Dataset

Attack

Random target items

Unpopular target items

Attack size

ML-100K

Music

None
Random
Bandwagon
MF
Our attack
None
Random
Bandwagon
MF
Our attack

0.5%

0.0033
0.0036
0.0038
0.0038
0.0038
0.0038
0.0044
0.0044
0.0043
0.0055

1%

3%

5%

0.0033
0.0041
0.0039
0.0045
0.0043
0.0038
0.0049
0.0049
0.0052
0.0066

0.0033
0.0054
0.0056
0.0070
0.0090
0.0038
0.0063
0.0068
0.0073
0.0079

0.0033
0.0074
0.0075
0.0089
0.0122
0.0038
0.0098
0.0076
0.0084
0.0109

0.5%

0
0.0002
0.0002
0.0002
0.0008
0.0005
0.0008
0.0006
0.0005
0.0015

1%

3%

5%

0
0.0003
0.0003
0.0003
0.0021
0.0005
0.0015
0.0014
0.0018
0.0025

0
0.0009
0.0009
0.0020
0.0060
0.0005
0.0041
0.0029
0.0045
0.0063

0
0.0016
0.0015
0.0032
0.0101
0.0005
0.0064
0.0061
0.0062
0.0100

TABLE XI: The number of promoted target items for different attacks with different attack sizes.

Dataset

Attack

Random target items

Unpopular target items

0.5% 1% 3% 5% 0.5% 1% 3% 5%

Attack size

ML-100K

Music

None
Random
Bandwagon
MF
Our attack
None
Random
Bandwagon
MF
Our attack

0
4
6
7
9
0
9
8
10
9

0
8
8
8
9
0
10
10
10
10

0
9
10
10
10
0
10
10
10
10

0
10
10
10
10
0
10
10
10
10

0
6
6
5
9
0
7
8
8
9

0
9
10
8
10
0
9
9
9
10

0
10
10
10
10
0
10
10
10
10

0
10
10
10
10
0
10
10
10
10

17

