0
2
0
2

p
e
S
1
1

]

G
L
.
s
c
[

1
v
0
6
6
5
0
.
9
0
0
2
:
v
i
X
r
a

Abstract Neural Networks

Matthew Sotoudeh and Aditya V. Thakur

University of California, Davis, USA
{masotoudeh,avthakur}@ucdavis.edu

Abstract. Deep Neural Networks (DNNs) are rapidly being applied to
safety-critical domains such as drone and airplane control, motivating
techniques for verifying the safety of their behavior. Unfortunately, DNN
veriﬁcation is NP-hard, with current algorithms slowing exponentially
with the number of nodes in the DNN. This paper introduces the no-
tion of Abstract Neural Networks (ANNs), which can be used to soundly
overapproximate DNNs while using fewer nodes. An ANN is like a DNN
except weight matrices are replaced by values in a given abstract do-
main. We present a framework parameterized by the abstract domain
and activation functions used in the DNN that can be used to construct
a corresponding ANN. We present necessary and suﬃcient conditions
on the DNN activation functions for the constructed ANN to soundly
over-approximate the given DNN. Prior work on DNN abstraction was
restricted to the interval domain and ReLU activation function. Our
framework can be instantiated with other abstract domains such as oc-
tagons and polyhedra, as well as other activation functions such as Leaky
ReLU, Sigmoid, and Hyperbolic Tangent.
Code: https://github.com/95616ARG/abstract neural networks

Keywords: Deep Neural Networks · Abstraction · Soundness.

1

Introduction

Deep Neural Networks (DNNs), deﬁned formally in Section 3, are loop-free com-
puter programs organized into layers, each of which computes a linear combi-
nation of the layer’s inputs, then applies some non-linear activation function to
the resulting values. The activation function used varies between networks, with
popular activation functions including ReLU, Hyperbolic Tangent, and Leaky
ReLU [13]. DNNs have rapidly become important in a variety of applications,
including image recognition and safety-critical control systems, motivating re-
search into the problem of verifying properties about their behavior [18,9].

Although they lack loops, the use of non-linear activation functions intro-
duces exponential branching behavior into the DNN semantics. It has been shown
that DNN veriﬁcation is NP-hard [18]. In particular, this exponential behavior
scales with the number of nodes in a network. DNNs in practice have very large
numbers of nodes, e.g., the aircraft collision-avoidance DNN ACAS Xu [17] has
300 and a modern image recognition network has tens of thousands [20]. The

 
 
 
 
 
 
2

M. Sotoudeh and A. V. Thakur

number of nodes in modern networks has also been growing with time as more
eﬀective training methods have been found [3].

One increasingly common way of addressing this problem is to compress the
DNN into a smaller proxy network which can be analyzed in its place. How-
ever, most such approaches usually do not guarantee that properties of the
proxy network hold in the original network (they are unsound). Recently, Prab-
hakar et al. [30] introduced the notion of Interval Neural Networks (INNs), which
can produce a smaller proxy network that is guaranteed to over-approximate the
behavior of the original DNN. While promising, soundness is only guaranteed
with a particular activation function (ReLU) and abstract domain (intervals).

In this work, we introduce Abstract Neural Networks (ANNs), which are like
DNNs except weight matrices are replaced with values in an abstract domain.
Given a DNN and an abstract domain, we present an algorithm for constructing a
corresponding ANN with fewer nodes. The algorithm works by merging groups
of nodes in the DNN to form corresponding abstract nodes in the ANN. We
prove necessary and suﬃcient conditions on the activation functions used for
the constructed ANN to over-approximate the input DNN. If these conditions
are met, the smaller ANN can be soundly analyzed in place of the DNN. Our
formalization and theoretical results generalize those of Prabhakar et al. [30],
which are an instantiation of our framework for ReLU activation functions and
the interval domain. Our results also show how to instantiate the algorithm
such that sound abstraction can be achieved with a variety of diﬀerent abstract
domains (including polytopes and octagons) as well as many popular activation
functions (including Hyperbolic Tangent, Leaky ReLU, and Sigmoid).

Outline In this paper, we aim to lay strong theoretical foundations for research
into abstracting neural networks for veriﬁcation. Section 2 gives an overview of
our technique. Section 3 deﬁnes preliminaries. Section 4 deﬁnes Abstract Neural
Networks (ANNs). Section 5 presents an algorithm for constructing an ANN
from a given DNN. Section 6 motivates our theoretical results with a number of
examples. Section 7 proves our soundness theorem. Section 8 discusses related
work, while Section 9 concludes with a discussion of future work.

2 Motivation

DNNs are often denoted by a graph of the form shown in Figure 1a. The input
node x1 is assigned the input value, then the values of h1 and h2 are computed
by ﬁrst a linear combination of the values of the previous layer (in this case x1)
followed by some non-linear activation function. The behavior of the network
is dependent on the non-linear activation function used. We will assume that
the output layer with nodes y1, y2, and y3 uses the identity activation function
I(x) = x. For the hidden layer with nodes h1 and h2 we will consider two
scenarios, each using one of the following two activation functions:

σ(x) =

(

x if x
0

≥
otherwise.

0

φ(x) =

(

x
if x
0.5x otherwise.

≥

0

.

Abstract Neural Networks

3

x1

h1

1

−1

h2

1

1

1

0

0

1

y1

y2

y3

[−1, 1]

x1

h

[2, 2]

[0, 2]

y1

y2

[0, 2]

y3

0.5

x1

h

2

2

0

y1

y2

y3

(a) DNN N1

(b) Corresponding INN

(c) One instantiation of the
INN

Fig. 1: Example DNN to INN and one of many instantiations of the INN.

Using σ as the activation function for the hidden layer, when x1 = 1 we
1x1) = 0. That in turn gives us y1 =

have h1 = σ(1x1) = 1 and h2 = σ(
I(1h1 + 1h2) = 1, y2 = I(1h1 + 0h2) = 1, and y3 = I(0h1 + 1h2) = 0.

−

Using σ as the activation function for the hidden layer, when x1 = 1, we have

h1 = σ(1x1) = 1
y1 = I(1h1 + 1h2) = 1

1x1) = 0

h2 = σ(
y2 = I(1h1 + 0h2) = 1

−

y3 = I(0h1 + 1h2) = 0.

Using φ as the activation function for the hidden layer, when x1 = 1, we have

h1 = φ(1) = 1
y1 = 0.5

h2 = φ(
y2 = 1

−

1) =

0.5

−

y3 =

0.5.

−

2.1 Merging Nodes

Our goal is to merge nodes and their corresponding weights in this DNN to
produce a smaller network that over-approximates the behavior of the original
one. One way of doing this was proposed by Prabhakar et al. [30], where nodes
within a layer can be merged and the weighted interval hull of their edge weights
is taken. For example, if we merge all of the hi nodes together into a single h node,
this process results in an Interval Neural Network (INN) shown in Figure 1b.

Intuitively, given this new INN we can form a DNN instantiation by picking
any weight within the interval for each edge. We can then ﬁnd the output of
this DNN instantiation on, say, x1 = 1. We take the output of the INN on an
input x1 to be the set of all such (y1, y2, y3) triples outputted by some such
instantiated DNN on x1.

For example, we can take the instantiation in Figure 1c. Using the σ acti-
vation function, this implies (y1 = 1, y2 = 1, y3 = 0) is in the output set of the
INN on input x1 = 1. In fact, the results of Prabhakar et al. [30] show that,
if the σ activation function is used, then for any input x1 we will have some
assignment to the weights which produces the same output as the original DNN
(although many assignments will produce diﬀerent outputs — the output set is
an over-approximation of the behavior of the original network).

4

M. Sotoudeh and A. V. Thakur

However, something diﬀerent happens if the network were using the φ acti-
vation function, a case that was not considered by Prabhakar et al. [30]. In that
scenario, the original DNN had an output of (0.5, 1,
0.5), so if the INN were
to soundly over-approximate it there would need to be some instantiation of the
weights where y1 and y3 could have opposite signs. But this cannot happen —
both will have the same (or zero) sign as h!

−

These examples highlight the fact that the soundness of the algorithm from Prab-

hakar et al. [30] is speciﬁc to the ReLU activation function (σ above) and In-
terval abstract domain. Their results make no statement about whether INNs
over-approximate DNNs using diﬀerent activation functions (such as φ above),
or if abstractions using diﬀerent domains (such as the Octagon Neural Networks
deﬁned in Deﬁnition 11) also permit sound DNN over-approximation.

This paper develops a general framework for such DNN abstractions, parame-
terized by the abstract domain and activation functions used. In this framework,
we prove necessary and suﬃcient conditions on the activation functions for a
Layer-Wise Abstraction Algorithm generalizing that of Prabhakar et al. [30] to
produce an ANN soundly over-approximating the given DNN. Finally, we discuss
ways to modify the abstraction algorithm in order to soundly over-approximate
common DNN architectures that fail the necessary conditions, extending the
applicability of model abstraction to almost all currently-used DNNs.

These results lay a solid theoretical foundation for research into Abstract
Neural Networks. Because our algorithm and proofs are parameterized by the
abstract domain and activation functions used, our proofs allow practitioners to
experiment with diﬀerent abstractions, activation functions, and optimizations
without having to re-prove soundness for their particular instantiation (which,
as we will see in Section 7, is a surprisingly subtle process).

3 Preliminaries

In this section we deﬁne Deep Neural Networks and a number of commonly-used
activation functions.

3.1 Deep Neural Networks

In Section 2, we represented neural networks by graphs. While this is useful for
intuition, in Section 4 we will talk about, e.g., octagons of layer weight matrices,
for which the graph representation makes signiﬁcantly less intuitive sense. Hence,
for the rest of the paper we will use an entirely equivalent matrix representation
for DNNs, which will simplify the deﬁnitions, intuition, and proofs considerably.
With this notation, we think of nodes as dimensions and layers of nodes as
intermediate spaces. We then deﬁne a layer to be a transformation from one
intermediate space to another.

Deﬁnition 1. A DNN layer from n to m dimensions is a tuple (W , σ) where W
R is an arbitrarily-chosen activation function.
is an m

n matrix and σ : R

×

→

Abstract Neural Networks

5

We will often abuse notation such that, for a vector v, σ(v) is the vector

formed by applying σ to each component of v.

Deﬁnition 2. A Deep Neural Network (DNN) with layer sizes s0, s1, . . . , sn is a
collection of n DNN layers (W (1), σ(1)), . . . , (W (n), σ(n)), where the (W (i), σ(i))
layer is from si−1 to si dimensions.

Every DNN has a corresponding function, deﬁned below.

Deﬁnition 3. Given a DNN from s0 to sn dimensions with layers (W (i), σ(i)),
Rsn given
the function corresponding to the DNN is the function f : Rs0
by f (v) = v(n), where v(i) is deﬁned inductively by v(0) = v and v(i) =
σ(i)(W (i)(v(i−1))).

→

Where convenient, we will often refer to the corresponding function as the

DNN or vice-versa.

Example 1. The DNN N1 from Figure 1a, when using the σ hidden-layer ac-

tivation function, is represented by the layers

, σ

and

, I

. The

(cid:17)
function corresponding to the DNN is given by N1(x1) =

(cid:16)h

1
−1
i

1 1
1 0
0 1

1 1
1 0
0 1

(cid:20)

(cid:21)

(cid:18)(cid:20)
σ

(cid:16)h

(cid:21)

1
−1
i

(cid:19)
[x1]

.

(cid:17)

3.2 Common Activation Functions

There are a number of commonly-used activation functions, listed below.

Deﬁnition 4. The Leaky Rectiﬁed Linear Unit (LReLU) [22], Rectiﬁed Linear
Unit (ReLU), Hyperbolic Tangent (tanh), and Threshold (thresh) activation
functions are deﬁned:

LReLU(x; c) :=

0
x
x
cx x < 0

≥

,

(

ReLU(x) := LReLU(x; 0),

tanh :=

e2x
1
−
e2x + 1

,

thresh(x; t, v) :=

(

t

x if x
v

≥
otherwise

.

Here LReLU and thresh actually represent families of activation functions pa-
rameterized by the constants c, t, v. The constants used varies between networks.
c = 0 is a common choice for the LReLU parameter, hence the explicit deﬁnition
of ReLU.

All of these activation functions are present in standard deep-learning toolk-
its, such as Pytorch [27]. Libraries such as Pytorch also enable users to implement
new activation functions. This variety of activation functions used in practice
will motivate our study of necessary and suﬃcient conditions on the activation
function to permit sound over-approximation.

6

M. Sotoudeh and A. V. Thakur

4 Abstract Neural Networks

In this section, we formalize the syntax and semantics of Abstract Neural Net-
works (ANNs). We also present two types of ANNs: Interval Neural Networks
(INNs) and Octagon Neural Networks (ONNs).

An ANN is like a DNN except the weights in each layer are represented by

an abstract value in some abstract domain. This is formalized below.

Deﬁnition 5. An n
×
connection (αA, γA) with the powerset lattice

m weight set abstract domain is a lattice

with Galois

(Rn×m) of n

P

×

A

m matrices.

Deﬁnition 6. An ANN layer from n to m dimensions is a triple (
A is a member of the weight set abstraction
chosen activation function.

and σ : R

→

A

, A, σ) where
R is an arbitrarily-

A

Thus, we see that each ANN layer (
γA(A). Finally, we can deﬁne the notion of an ANN:

A

, A, σ) is associated with a set of weights

Deﬁnition 7. An Abstract Neural Network (ANN) with layer sizes s0, s1, . . . , sn
(i), A(i), σ(i)), where the ith layer is from si−1
is a collection of n ANN layers (
to si dimensions.

A

We consider the output of the ANN to be the set of outputs of all instantia-

tions of the ANN into a DNN, as illustrated in Figure 2.

Deﬁnition 8. We say a DNN with layers (W (i), σ(i)) is an instantiation of an
γA(i) (A(i)). The set of all
ANN T with layers (
DNNs that are instantiations of an ANN T is given by γ(T ).

(i), A(i), σ(i)) if each W (i)

A

∈

The semantics of an ANN naturally lift those of the DNN instantiations.

Deﬁnition 9. For an ANN T from s0 to sn dimensions, the function corre-
sponding to T is the set-valued function T : Rs0
T (v) :=

(Rsn ) deﬁned by

g(v)

→ P

g

.

{

γ(T )
}

∈

|

Space constraints prevent us from deﬁning a full Galois connection here, how-
ever one can be established between the lattice of ANNs of a certain architecture
and the powerset of DNNs of the same architecture.

The deﬁnition of an ANN above is agnostic to the actual abstract domain(s)
used. For expository purposes, we now deﬁne two particular types of ANNs:
Interval Neural Networks (INNs) and Octagon Neural Networks (ONNs).

Deﬁnition 10. An Interval Neural Network (INN) is an ANN with layers
(
A
domain represents sets of matrices by their component-wise interval hull.

(i) is an interval hull domain [5]. The interval hull

(i), A(i), σ(i)), where each

A

Notably, the deﬁnition of INN in Prabhakar et al. [30] is equivalent to the
above, except that they further assume every activation function σ(i) is the ReLU
function.

Abstract Neural Networks

7

v

=
v

v

v

σ(1)
=

σ(1)

σ(1)

A(2)
γ

H (1,2)

H (2,2)

v(1,1)

v(2,1)

A(1)
γ

H (1,1)

H (2,1)

w(1,1)

w(2,1)

...

w(1,2)

w(2,2)

...

σ(2)
=

σ(2)

σ(2)

A(3)
γ

H (1,3)

H (2,3)

v(1,2)

v(2,2)

w(1,3)

w(2,3)

...

T (v)
∋
v(1,3)

∈ T (v)

v(2,3)

∈ T (v)

σ(3)
=

σ(3)

σ(3)

H (j,1)

w(j,1)

v(j,1)

H (j,2)

w(j,2)

v(j,2)

H (j,3)

σ(2)

w(j,3)

σ(3)

σ(1)

v(j,3)

∈ T (v)

...

...

...

Fig. 2: Visualization of ANN semantics for a 3-layer ANN T (ﬁrst row). Diﬀerent
DNN instantiations (other rows) of T are formed by replacing each abstract
γ(A(i)). v(j,3) is
weight matrix A(i) by some concrete weight matrix H (j,i)
the output of each instantiation on the input v. The set of all such outputs
producable by some valid instantiation is taken to be the output T (v) of the
ANN on vector v.

∈

Example 2. We ﬁrst demonstrate the interval hull domain: γInt

[−1, 1]
[0, 2]
[−3, −2] [1, 2]

=

a b
c d
i

a

[

1, 1], b

[0, 2], c

[

3,

2], d

[1, 2]

. We can thus deﬁne a two-

|

∈

∈

−

−

[0, 1] [0, 1]

∈
−
ReLU

nh
layer INN f (v) :=

∈
[−1, 1]
[0, 2]
[−3, −2] [1, 2]
2
network in a variety of ways, for example g(v) :=
−2.5 1.5
γ(f ). Taking arbitrarily (1, 1)T as an example input, we have g((1, 1)T ) =
i
∈
f ((1, 1)T ). In fact, f ((1, 1)T ) is the set of all values that can be achieved by such
instantiations, which in this case is the set given by f ((1, 1)T ) =

. We can instantiate this

(cid:17)
0.5 1

ReLU

[0, 3]

o
v

(cid:16)h

(cid:16)h

1
(cid:17)

∈

v

i

(cid:2)

(cid:2)

(cid:3)

(cid:3)

(cid:3)

(cid:2)

0

.

i(cid:17)

(cid:16)h

(i), A(i), σ(i)), where each

Deﬁnition 11. An Octagon Neural Network (ONN) is an ANN with layers
(i) is an octagon hull domain [23]. The octagon
(
A
hull domain represents sets of matrices by octagons in the space of their compo-
nents.

A

(cid:2)

(cid:3)

Example 3. Octagons representing a set of n
m matrices can be thought of
exactly like an octagon in the vector space Rn·m. Unfortunately, this is partic-
ularly diﬃcult to visualize in higher dimensions, hence in this example we will
stick to the case where nm = 2.

×

Let O1, O2 be octagons such that

γOct(O1) =

γOct(O2) =

a
b

(cid:26)(cid:20)

(cid:21)
a b

|

(cid:8)(cid:2)

(cid:3)

a

b

−
a

≤
b

|

−

≤

a + b

1,

−

≤

1, a + b

2,

−

≤

a

a + b

2,

−

≤

3, a + b

4,

−

≤

b

2

−
a

≤
b

−

≤

,

.

(cid:27)
5

(cid:9)

8

M. Sotoudeh and A. V. Thakur

We can thus deﬁne a two-layer ONN f (v) := O2ReLU (O1v). One instantiation
of this ONN f is the DNN g(v) :=
γ(f ). We can conﬁrm
that g(1) =

0.5
1.5
i

ReLU

f (1).

3 1

(cid:16)h

∈

(cid:17)

v

3

(cid:2)

(cid:3)

∈

We can similarly deﬁne Polyhedra Neural Networks (PNNs) using the poly-

(cid:3)

(cid:2)

hedra domain [6].

5 Layer-Wise Abstraction Algorithm

Given a large DNN, how might we construct a smaller ANN which soundly
over-approximates that DNN? We deﬁne over-approximation formally below.
Deﬁnition 12. An ANN T over-approximates a DNN N if, for every v
N (v)

T (v).

Rn,

∈

Remark 1. By Deﬁnition 9, then, T over-approximates N if, for every v we can
ﬁnd some instantiation Tv

γ(T ) such that Tv(v) = N (v).

∈

∈

Algorithm 3 constructs a small ANN that, under certain assumptions dis-
cussed in Section 2, soundly over-approximates the large DNN given. The basic
idea is to merge groups of dimensions together, forming an ANN where each di-
mension in the ANN represents a collection of dimensions in the original DNN.
We formalize the notion of “groups of dimensions” as a layer-wise partitioning.

Deﬁnition 13. Given a DNN with layer sizes s0, s1, . . . , sn, a layer-wise parti-
tioning P of the network is a set of partitionings P(0), P(1), . . . , P(n) where each
P(i) partitions
. For ease of notation, we will write partitionings with
set notation but assume they have some intrinsic ordering for indexing.

1, 2, . . . , si}

{

Remark 2. To maintain the same number of input and output dimensions in our
ANN and DNN, we assume P(0) =
, . . . ,
,

and P(n) =

, . . . ,

2

1

1

2

,

{{

}

{

}

s0}}

{

{{

}

{

}

.

sn}}

{

Example 4. Consider the DNN corresponding to the function

f (x1) =

1 1
1 0
0 1

(cid:20)

Hence, one valid layer-wise partitioning is to merge the two inner dimensions:
P(0) =
. Here we have, e.g.,
P(0)
1 =

P(2) =
.
3

1
}}
, P(1)

1 =

{{

}}

}

{

{

}

3

2

1

,

,

{{
1
}

{

ReLU

(cid:21)
P(1) =
1, 2

{

}

(cid:16)h

[x1]

1
−1
i
1, 2
}}
{{
, and P(2)
3 =

(cid:17)

{

}

. The layer sizes are s0 = 1, s1 = 2, s2 = 3.

Our layer-wise abstraction algorithm is shown in Algorithm 3. For each layer
in the DNN, we will call Algorithm 1 to abstract the set of mergings of the
layer’s weight matrix. This abstract element becomes the abstract weight A(i)
for the corresponding layer in the constructed ANN.

The functions PCMs and ScaleCols are deﬁned more precisely below.

Deﬁnition 14. Let P be some partition, i.e., non-empty subset, of
.
}
Then a vector c
Rn is a partition combination vector (PCV) if (i) each
component ci is non-negative, (ii) the components of ci sum to one, and (iii)
ci = 0 whenever i

1, 2, . . . , n

P .

∈

{

6∈

in,

out,

P

Algorithm 1:

)
α(M,
A
Input: Matrix M . Partitionings
P in, P out with |P in| = k.
Abstract domain A.
Output: Abstract element

P

b

Abstract Neural Networks

9

in,

Algorithm 2:

αbin(M,

out,
Input: Matrix M . Partitionings
P in, P out with |P in| = k.
Abstract domain A.
Output: Abstract element

P

P

b

)

A

representing all merges of
M .

1 S ← {}
2 w ← (|P in
3 for C ∈ PCMs(P in) do
4

1 |, |P in

2 |, . . . , |P in

for D ∈ PCMs(P out) do

k |)

representing all binary
merges of M

1 S ← {}
2 w ← (|P in
2 |, . . . , |P in
1 |, |P in
k |)
3 for C ∈ BinPCMs(P in) do
4

for D ∈ BinPCMs(P out) do

5

S ← S ∪
{ScaleCols(DT M C, w))}

5

S ← S ∪
{ScaleCols(DT M C, w))}

6 return αA(S)

6 return αA(S)

A, Σ
Algorithm 3: AbstractLayerWise
h
Input: DNN N consisting of n layers (W (i), σ(i)) with each σ(i) ∈ Σ.

(N , P,
i

)
A

Layer-wise partitioning P of N . List of n abstract weight domains
A(i) ∈ A.

Output: An ANN with layers (A(i), A(i), σ(i)) where A(i) ∈ A(i) ∈ A.

]

1 A ← [
2 for i ∈ {1, 2, . . . , n} do
3

A(i) ←
A.append(cid:0)(A(i), A(i), σ(i))(cid:1)

α(W (i), P(i−1), P(i), A(i))
b

4

5 return A

Deﬁnition 15. Given a partitioning

tioning combination matrix (PCM) is a matrix C =

P

of

1, 2, . . . , n

= k, a parti-
, where each ci
Pi. We refer to the set of all such PCMs for a partitioning

c1 c2 · · · ck

with

|P|

(cid:20)

(cid:21)

}

{

is a PCV of partition

by PCMs(

).

P

P
Deﬁnition 16. A PCM is binary if each entry is either 0 or 1. We refer to the
set of all binary PCMs for a partitioning

as BinPCMs(

).

P

P

Deﬁnition 17. For an n
and PCM D for partitioning
of M .

×

m matrix M , PCM C of partitioning
1, 2, . . . , n

out of

P
, we call DT M C a merging

1, 2, . . . , m

{

,

}

in of

P

{

}

The jth column in M C is a convex combination of the columns of M that
in
j , weighted by the jth column of C. Similarly, the ith row
belong to partition
in DT M is a convex combination of the rows in M that belong to partition
out
.
i
In total, the i, jth entry of merged matrix DT M C is a convex combination of the
in
entries of M with indices in
j . This observation will lead to Theorem 1
in Section 5.1.

out
i × P

P

P

P

10

M. Sotoudeh and A. V. Thakur

Deﬁnition 18. Given a matrix M , the column-scaled matrix formed by weights
w1, w2, . . . , wk is the matrix with entries given component-wise by
ScaleCols(M, (w1, . . . , wk))i,j := Mi,jwj .

Intuitively, column-scaling is needed because what were originally n dimen-
sions contributing to an input have been collapsed into a single representative
dimension. This is demonstrated nicely for the speciﬁc case of Interval Neural
Network and ReLU activations by Figures 3 and 4 in Prabhakar et al. [30].

Example 5. Given the matrix M =

"

of the input dimensions and P(1) =

can deﬁne a PCM for P(0) as C :=

m1,1 m1,2 m1,3
m2,1 m2,2 m2,3
m3,1 m3,2 m3,3
m4,1 m4,2 m4,3#
,

1, 3

2, 4

, partitioning P(0) =

1, 3

2

,

}

{

}}

{{

{{
0.25 0
1
0
0.75 0
(cid:21)

(cid:20)

of the output dimensions, we

{

}}

}
and a PCM for P(1) as: D :=

.

0 0.99
0.4 0
0 0.01
0.6 0 #
0.25m1,1 + 0.75m1,3 m1,2
0.25m2,1 + 0.75m2,3 m2,2
0.25m3,1 + 0.75m3,3 m3,2
0.25m4,1 + 0.75m4,3 m4,2#

"

,

"

We can then compute the column–merged matrix M C =

and furthermore the column-row–merged matrix
DT M C =

0.4(0.25m2,1 + 0.75m2,3) + 0.6(0.25m4,1 + 0.75m4,3)

Finally, we can column-scale this matrix like so:

0.99(0.25m1,1 + 0.75m1,3) + 0.01(0.25m3,1 + 0.75m3,3) 0.99m1,2 + 0.01m3,2
h

0.4m2,2 + 0.6m4,2

.

i

ScaleCols(DT M C, (2, 2))

=

0.8(0.25m2,1 + 0.75m2,3) + 1.2(0.25m4,1 + 0.75m4,3)
1.98(0.25m1,1 + 0.75m1,3) + 0.02(0.25m3,1 + 0.75m3,3) 1.98m1,2 + 0.02m3,2(cid:21)

0.8m2,2 + 1.2m4,2

.

(cid:20)

5.1 Computability

In general, there are an inﬁnite number of mergings. Hence, to actually compute
α (Algorithm 1) we need some non-trivial way to compute the abstraction of the
(i) is convex, it can be shown
inﬁnite set of mergings. If the abstract domain
that one only needs to iterate over the binary PCMs, of which there are ﬁnitely
b
many, producing a computationally feasible algorithm.

A

Deﬁnition 19. A weight set abstract domain
concrete values, γA(αA(S)) is convex.

A

is convex if, for any set S of

Many commonly-used abstractions — including intervals [5], octagons [23],

and polyhedra [6] — are convex.

Theorem 1. If

A

is convex, then

α(M,

in,

P

P

out,

) =

A

αbin(M,

in,

P

P

out,

).

A

Proof. Please see Appendix A for the proof of this theorem.

b

b

Remark 3. Consider PCMs C and D corresponding to merged matrix DT W (i)C.
We may think of C and D as vectors in the vector space of matrices. Then their
C forms a convex coeﬃcient matrix of the binary mergings R
outer product D

⊗

Abstract Neural Networks

11

⊗

of W (i), such that (D
C)R = DT W (i)C. From this intuition, it follows that the
converse to Theorem 1 does not hold, as every matrix E cannot be decomposed
into vectors D
C as described (i.e., not every matrix has rank 1). Hence, the
convexity condition may be slightly weakened. However, we are not presently
aware of any abstract domains that satisfy such a condition but not convexity.

⊗

Example 6. Let W (i) =

1 −2 3
4 −5 6
7 −8 9

(cid:20)

(cid:21)

and consider P(i−1) =

1, 2

3

,

}

{

}}

{{

and P(i) =

1, 3

2

,

}

{

}}

{{

. Then we have the binary PCMs BinPCMs(P(i−1)) =

1 0
0 0
0 1

,

0 0
1 0
0 1

(cid:21)(cid:27)
. These correspond to the column-scaled

(cid:26)(cid:20)

(cid:21)

(cid:20)

and BinPCMs(P(i)) =

binary mergings

(cid:21)(cid:27)
14 9
2 3
8 6
8 6
i
i
We can take any PCMs such as C =

(cid:26)(cid:20)
,

(cid:20)
,

h

,

0 0
0 1
1 0

,

1 0
0 1
0 0
(cid:21)
−4 3
−10 6
i
h

.

−16 9
−10 6
0.75 0
h
0.25 0
1
0
(cid:21)

nh

0.5 0
0 1
0.5 0
(cid:20)
(cid:21)
3.5 6
for P(i), resulting in the scaled merging ScaleCols(DT W (i)C, (2, 1)) =
.
3.5 6
According to Theorem 1, we can write this as a convex combination of the four
i
h
column-scaled binary merged matrices. In particular, we ﬁnd the combination

for P(i−1) as well as D =

io

(cid:20)

3.5 6
3.5 6(cid:21)

(cid:20)

=(1.5/2)(1)(0.5)(1)

2 3
8 6(cid:21)

(cid:20)

+ (0.5/2)(1)(0.5)(1)

−4 3
−10 6(cid:21)

(cid:20)

+ (1.5/2)(1)(0.5)(1)

14 9
8 6(cid:21)

(cid:20)

+ (0.5/2)(1)(0.5)(1)

−16 9
−10 6(cid:21)

(cid:20)

.

We can conﬁrm that this is a convex combination, as

(1.5/2)(1)(0.5)(1)+(0.5/2)(1)(0.5)(1)+(1.5/2)(1)(0.5)(1)+(0.5/2)(1)(0.5)(1) = 1.

Because we can ﬁnd such a convex combination for any such non-binary
merging in terms of the binary ones, and because the abstract domain is assumed
to be convex, including only the binary mergings will ensure that all mergings
are represented by the abstract element A(i).

5.2 Walkthrough Example

Example 7. Consider again the DNN from Example 4 corresponding to

1 1
1 0
0 1
,

σ

f (x1) =

1
−1
}}
(cid:20)
i
P(2) =
, which collapses the two hidden dimensions, and assume
1
}}
}
the abstract domains

, the partitioning P(0) =

(i) are all convex.

, P(1) =

(cid:16)h
3
,
{

(cid:21)
2
{

1, 2

[x1]

{{

{{

{{

}}

(cid:17)

}

1

,

For the input layer, we have w = (1), because the only partition in P(0) has
. However, there are

size 1. Similarly, the only binary PCM for P(0) is C =
two binary PCMs for P(1), namely D =

. These correspond to the

1

A

1
binary merged matrices
completing the ﬁrst layer.
(cid:2)

(cid:3)

and

1

−

(cid:2)

(cid:3)

or D =

1
0
h
i

0
1
. Hence, we get A(1) = αA(1) (
h
i
{

(cid:3)

(cid:2)

1

,

(cid:2)

(cid:3)

1

),

}

(cid:3)

−
(cid:2)

12

M. Sotoudeh and A. V. Thakur

For the output layer, we have w = (2), because the only partition in P(1)
contains two nodes. Hence, the column scaling will need to play a role: because
we have merged two dimensions in the domain, we should interpret any value
from that dimension as being from both of the dimensions that were merged. We

1
1
0
(cid:21)

(cid:20)

and

1
0
1
(cid:21)

(cid:20)

, which after rescaling gives us

have two binary mergings, namely

A(2) = αA(2)

2
2
0
(cid:18)(cid:26)(cid:20)

2
.
0
2
(cid:21)(cid:27)(cid:19)

,

(cid:21)

(cid:20)

In total then, the returned ANN can be written (A(1), σ), (A(2), x

x) or
in a more functional notation as g(x) = A(2)σ(A(1)x), where in either case

7→

A(1) = αA(1) (
{

1

,

), and A(2) = αA(2)

1

}

−
(cid:2)

(cid:3)

(cid:2)

(cid:3)

2
2
0
(cid:21)

,

2
.
0
2
(cid:21)(cid:27)(cid:19)

(cid:20)

(cid:18)(cid:26)(cid:20)

Note in particular that, while the operation of the algorithm was agnostic to
the exact abstract domains A and activation functions Σ used, the semantics of
the resulting ANN depend entirely on these. Hence, correctness of the algorithm
will depend on the abstract domain and activation functions satisfying certain
conditions. We will discuss this further in Section 6.

6 Layer-Wise Abstraction: Instantiations and Examples

This section examines a number of examples. For some DNNs, Algorithm 3 will
produce a soundly over-approximating ANN. For others, the ANN will provably
not over-approximate the given DNN. We will generalize these examples to nec-
essary and suﬃcient conditions on the activation functions Σ used in order for
A, Σ
AbstractLayerWise
h

) to soundly over-approximate N .

(N , P,
i

A

6.1 Interval Hull Domain with ReLU Activation Functions

Consider again the DNN from Example 7 given by f (x1) =

ReLU

(cid:17)
and partitioning which merges the two intermediate dimensions. Using the inter-

val hull domain in Example 7 gives the corresponding INN: g(x1) =

ReLU ([[−1, 1]] [x1]).

1 1
1 0
0 1

(cid:20)

(cid:21)

[x1]

1
−1
i

(cid:16)h
[2, 2]
[0, 2]
[0, 2]

(cid:20)

(cid:21)

In fact, because the ReLU activation function and interval domain was used,
it follows from the results of Prabhakar et al. [30] that g in fact over-approximates
f . To see this, consider two cases. If x1 > 0, then the second component in the
hidden dimension of f will always become 0 under the activation function. Hence,

f (x1) =

1
1
0
(cid:20)

ReLU ([1] [x1]) =

(cid:21)

2
2
0
(cid:21)

(cid:20)

ReLU ([0.5] [x1]), which is a valid instantiation

of the weights in g. Otherwise, if x1 ≤
which is again a valid instantiation. Hence in all cases, the true output f (x1) can
be made by some valid instantiation of the weights in g. Therefore, f (x1)
g(x1)
for all x1 and so g over-approximates f .

0, we ﬁnd f (x1) =

ReLU ([−0.5] [x1]),

∈

(cid:21)

2
0
2
(cid:20)

Abstract Neural Networks

13

Suﬃciency Condition The soundness of this particular instantiation can be
generalized to a suﬃciency theorem, Theorem 2, for soundness of the layer-wise
abstraction algorithm. Its statement relies on the activation function satisfying
the weakened intermediate value property, which is deﬁned below:

Deﬁnition 20. A function f : R
Value Property (WIVP) if, for every a1 ≤
b
∈

[a1, an] such that f (b) = P

i f (ai)
n

→

.

R satisﬁes the Weakened Intermediate
R, there exists some

a2 ≤ · · · ≤

an ∈

Every continuous function satisﬁes the IVP and hence the WIVP. Almost
all commonly-used activation functions, except for thresh, are continuous and,
therefore, satisfy the WIVP. However, the WIVP is not equivalent to the IVP,
as the below proof shows by constructing a function f such that f ((a, b)) = Q
for any non-empty open interval (a, b).

Proof. Please see Appendix B for the proof of this theorem.

We now state the soundness theorem below, which is proved in Section 7.

Theorem 2. Let A be a set of weight set abstract domains and Σ a set of ac-
Σ has entirely non-negative outputs,
tivation functions. Suppose (i) each σ
Σ satisﬁes the Weakened Intermediate Value Property (Deﬁ-
and (ii) each σ
A, Σ
nition 20). Then T = AbstractLayerWise
) (Algorithm 3) soundly
h
over-approximates the DNN N .

(N , P,
i

A

∈

∈

6.2 Interval Hull Domain with Leaky ReLUs

Something diﬀerent happens if we slightly modify f in Example 7 to use an
activation function producing negative values in the intermediate dimensions.
This is quite common of activation functions like Leaky ReLU and tanh, and
was not mentioned by Prabhakar et al. [30]. For example, we will take the Leaky
ReLU function (Deﬁnition 4) with c = 0.5 and consider the DNN f (x1) =
1 1
1 0
0 1
(cid:21)
[2, 2]
[0, 2]
[0, 2]

1
−1
i
LReLU ([[−1, 1]] [x1]; 0.5).

. Using the same partitioning gives us the INN g(x1) =

LReLU

[x1]; 0.5

(cid:16)h

(cid:17)

(cid:20)

Surprisingly, this small change to the activation function in fact makes the
constructed ANN no longer over-approximate the original DNN. For example,
note that f (1) =
and consider g(1). In g, the output of the LReLU
is one-dimensional, hence, it will have either positive, negative, or zero sign. But
no matter how the weights in the ﬁnal matrix are instantiated, every component
of g(1) will have the same (or zero) sign, and so f (1)
g(1), because f (1) has
mixed signs.

0.5 1

0.5

−

6∈

(cid:3)

(cid:2)

T

(cid:20)

(cid:21)

14

M. Sotoudeh and A. V. Thakur

Necessary Condition: Non-Negative Values We can generalize this coun-
terexample to the following necessary condition on soundness:

Theorem 3. Suppose some σ
Σ is an activation function with neither entirely
A is at least as
non-negative nor entirely non-positive outputs, and every
precise as the interval hull abstraction. Then there exists a neural network N that
A, Σ
uses σ and a partitioning P such that T = AbstractLayerWise
)
A
h
does not over-approximate N .

(N , P,
i

A ∈

∈

Proof. Please see Appendix C for the proof of this theorem.

Handling Negative Values Thankfully, there is a workaround to support
sometimes-negative activation functions. The constructive theorem below implies
that a given DNN can be modiﬁed into a shifted version of itself such that the
input-output behavior on any arbitrary bounded region is retained, but the
intermediate activations are all non-negative.

Theorem 4. Let N be a DNN and suppose that, on some input region R, the
output of the activation functions are lower-bounded by a constant C. Then,
there exists another DNN N ′, with at most one extra dimension per layer, which
satisﬁes (i) N ′(x) = N (x) for any x
R, (ii) N ′ has all non-negative activation
functions, and (iii) the new activation functions σ′ are of the form σ′(x) =
max(σ(x) +

, 0).

C

∈

|

|

Notably, the proof of this theorem is constructive with a straightforward
construction. The one requirement is that a lower-bound C be provided for the
output of the nodes in the network. This lower-bound need not be tight, and
can be computed quickly using the same procedure discussed for upper bounds
immediately following Equation 1 in Prabhakar et al. [30]. For tanh in particular,
its output is always lower-bounded by
1
−
for a network using only tanh activations.

1 so we can immediately take C =

−

Proof. Please see Appendix D for the proof of this theorem.

6.3 Interval Hull Abstraction with Non-Continuous Functions

Another way that the constructed ANN may not over-approximate the DNN
is if the activation function does not satisfy the Weakened Intermediate Value
Property (WIVP) (Deﬁnition 20). For example, consider the threshold activation
function (Deﬁnition 4) with parameters t = 1, v = 0 and the same overall

network, i.e. f (x1) =

get the INN g(x1) =

(cid:20)

1 1
1 0
0 1

(cid:21)
[2, 2]
[0, 2]
[0, 2]

thresh

[x1]; 1, 0

and the same partitioning. We

1
−1
i

(cid:16)h

(cid:17)

thresh ([[−1, 1]] [x1]; 1, 0). We have f (1) =

1 1 0

T

,

(cid:20)

(cid:21)
however, in g(1), no matter how we instantiate the [
1, 1] weight, the output
of the thresh unit will either be 0 or 1. But then the output of the ﬁrst output
component must be either 0 or 2, neither of which is 1, and so g does not over-
approximate f .

−

(cid:2)

(cid:3)

Necessary Condition: WIVP We can generalize this example to the following
necessary condition:

Abstract Neural Networks

15

Theorem 5. Suppose some σ
Σ is an activation function which does not sat-
∈
A is at least as precise as the interval hull abstrac-
isfy the WIVP, and every
tion. Then there exists a neural network N using only the identity and σ activa-
A, Σ
(N , P,
tion functions and partitioning P such that T = AbstractLayerWise
i
h
does not over-approximate N .

A ∈

)
A

Proof. Please see Appendix E for the proof of this theorem.

While this is of some theoretical curiosity, in practice almost all commonly-
used activation functions do satisfy the WIVP. Nevertheless, if one does wish to
use such a function, one way to soundly over-approximate it with an ANN is to
replace the scalar activation function with a set-valued one. The ANN semantics
can be extended to allow picking any output value from the activation function
in addition to any weight from the weight set.

For example, consider again the thresh(x; 1, 0) activation function. It can be
completed to a set-valued activation function which satisﬁes the WIVP such as

thresh′(x; 1, 0) :=

{x}
if x > 1
{a | a ∈ [0, 1]} if x = 1
{0}

otherwise

(

. The idea is that we “ﬁll the gap” in

the graph. Whereas in the original threshold function we had an issue because
there was no x
2 , on the
set-valued function we can take x = 1

[0, 1] which satisﬁed thresh(x; 1, 0) = f (0)+f (1)

= 1
thresh′(1; 1, 0).

[0, 1] to ﬁnd 1

∈

2

∈

2 ∈

6.4 Powerset Abstraction, ReLU, and

αbin

α (Algorithm 1) requires abstracting the, usually-inﬁnite, set of all
Recall that
b
merged matrices DT W (i)C. However, in Section 5.1 we showed that for convex
abstract domains it suﬃces to only consider the ﬁnitely-many binary mergings.
The reader may wonder if there are abstract domains for which it is not suﬃcient
to consider only the binary PCMs. This section presents such an example.

b

ANN g(x1) =

B. If we (incorrectly) used
2
2
0
(cid:21)

2
0
2
(cid:21)(cid:27)

ReLU

(cid:26)(cid:20)

,

Suppose we use the same ReLU DNN f as in Section 6.1, for which we noted
before the corresponding INN over-approximates it. However, suppose instead
of intervals we used the powerset abstract domain, i.e., α(S) = S and A
B =
α, we would get the powerset
A
T

αbin instead of

∪

⊔

1

,

1

x1

. Recall that f (1) =

1 1 0

.

(cid:20)

−
However, with g(1), the ﬁrst output will always be either 0 or 2, so g does not
(cid:2)
over-approximate f . The basic issue is that to get the correct output, we need
to instantiate the inner weight to 0.5, which is in the convex hull of the original
weights, but is not either one of the original weights itself.

{
b
(cid:0)

}
(cid:3)

b
(cid:3)(cid:1)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

Note that, in this particular example, it is possible to ﬁnd an ANN that over-
approximates the DNN using only ﬁnite sets for the abstract weights. However,
this is only because ReLU is piecewise-linear, and the size of the sets needed

16

M. Sotoudeh and A. V. Thakur

will grow exponentially with the number of dimensions. For other activation
functions, e.g., tanh inﬁnite sets are required in general.

In general, non-convex abstract domains will need to use some other method
of computing an over-approximation of
α. One general-purpose option is to use
techniques such as those developed for symbolic abstraction [36] to iteratively
compute an over-approximation of the true A(i) and use that instead.

b

7 Proof of Suﬃcient Conditions

We now prove Theorem 2, which provides suﬃcient conditions on the acti-
vation functions for which Algorithm 3 produces an ANN that soundly over-
approximates the given DNN.

∈

∈

The structure of the proof is illustrated in Figure 3. To show that ANN T
T (v) for every v. This
over-approximates DNN N , we must show that N (v)
occurs, by deﬁnition, only if there exists some instantiation Tv
γ(T ) of T for
which N (v) = Tv(v). Recall that an instantiation of an ANN is a DNN formed by
replacing each abstract weight A(i) with a concrete weight matrix H (i)
γ(A(i)).
In particular, our proof will proceed layer-by-layer. On an input v = v(0), the
ith layer of DNN N maps v(i−1) to v(i) until the output v(n) is computed.
We will prove that, for each abstract layer (A(i), σ(i),
(i)), there is a matrix
H (i) = G(A(i), v(i−1))
γ(A(i)) for which the instantiated layer (H (i), σ(i)),
roughly speaking, also maps v(i−1) to v(i). However, by design the abstract layer
will have fewer dimensions, hence the higher-dimensional v(i−1) and v(i) may not
belong to its domain and range (respectively). We resolve this by associating
with each vector v(i) in the intermediate spaces of N a mean representative
vector v(i)
/P(i) in the intermediate spaces of Tv. Then we can rigorously prove
that the instantiated layer (H (i), σ(i)) maps v(i−1)
/P(i−1) to v(i)
/P(i) . Applying
this fact inductively gives us Tv(v
/P(0) ) = (N (v))/P(n). Because P(0) and P(n)
are the singleton partitionings, this gives us exactly the desired relationship
Tv(v) = N (v).

A

∈

∈

7.1 Vector Representatives

Our proof relies heavily on the concept of representatives.
Deﬁnition 21. Given a vector v = (v1, v2, . . . , vn) and a partitioning
with
R(v,

= k, we deﬁne the convex representative set of v under
.
j. minh∈Pj vh ≤

(z1, z2, . . . , zk)

maxh∈Pj vh

zj ≤

) =

| ∀

P

) is referred to as AV (v) in Prabhakar et al. [30], and is always a box
(cid:8)

(cid:9)

of
P
to be

1, 2, . . . , n

}

{

|P|
P
R(v,

in Rk.

P

One representative will be particularly useful, so we give it a speciﬁc notation:

Deﬁnition 22. Given a vector (v1, v2, . . . , vn) and a partitioning
with

= k, we deﬁne the mean representative of v under

P
to be

of

1, 2, . . . , n

}

{

P

|P|

v/P =

vj

j∈P1
|P1|

P

, . . . , P

j∈Pk
|Pk|

vj

(cid:16)

(cid:17)

Abstract Neural Networks

17

v(0)

=

·/P(0)

v′

(0)

W (1)

α
b
A(1)
G

H (1)

σ(1)

w(1)

R(·, P(1) )

w′

(1)

σ(1)

v(1)

·/P(1)

v′

(1)

W (2)

α
b
A(2)
G

H (2)

σ(2)

w(2)

R(·, P(2) )

w′

(2)

σ(2)

v(2)

·/P(2)

v′

(2)

W (3)

α
b
A(3)
G

H (3)

w(3)

σ(3)

v(3)

R(·, P(3) )

·/P(3)

=

w′

(3)

v′

(3)

σ(3)

Fig. 3: Visualization of the relationships between concrete, abstract, and instan-
tiated elements in the soundness proof. The original DNN’s action on an input
vector v(0) is shown on the top row. This DNN is abstracted to an ANN, rep-
resented by the A(i)s on the middle row. We will show that we can instantiate
the ANN such that the instantiation has the same output as the original DNN
on v(0).

Example 8. Consider the vector v := (5, 6, 11, 2, 1) and the partitioning

=
. Then we have v/P = ((5 + 11)/2, (6 + 2 + 1)/3) = (8, 3) and

2, 4, 5

P

1, 3
{{
R(v,

}
P

,
{
) =

{

}}
(z1, z2)

z1 ∈

[5, 11], z2 ∈

|

[1, 6]

.

}

7.2 Proof of Soundness Theorem

The operation G presented in Algorithm 4 shows how to instantiate an abstract
weight matrix such that it has input/output behavior corresponding to that of
the original DNN layer. We now prove the correctness of Algorithm 4.

Lemma 1. Given any w′
and H = G(M,
w′.

in,

P

P

R(M v,

out, v, w′), then H

∈

in), a vector v with non-negative entries,
out)) and H(v/P in ) =

α(M,

in,

γ(

P

∈

P

P

b

Proof. To prove correctness of Algorithm 4, it suﬃces to show that (i) C and D
are PCMs, and (ii) the returned matrix H satisﬁes the equality H(v
/P in ) = w′.
C is a PCM by construction: The ith column only has non-zero entries
for rows that are in the ith partition. The sum of all entries in a column is
vk) = 1. All entries are non-negative by assumption on v.
j∈P in
D is also a PCM: The ith column only has two entries. It suﬃces to show
i is in

P
that Da,i is in [0, 1], which follows because w′
between the minimum b and maximum a.

out) implies w′

R(M v,

vj/(

k∈P in

P

P

∈

i

i

By associativity, line 11 is equivalent to returning H = DT M E where E =
/P in) = w′, it suﬃces to show (i) that

ScaleCols(C, s). Thus, to show that H(v
E(v/P in) = v, and (ii) that DT M v = w′.

Note that here Ej,i = Cj,i|P
))

Then we ﬁnd that the jth output component of E(v
(vj /(
is v.

k∈P in

k∈P in

i vk

i vk

in
i

in
i

|P

|P

)/

((

|

|

|

in
i

/P in ) is

. Then to show (i), consider any index j

) = vj. Hence, the entire output vector

P

P

To show (ii), note that each column of D is exactly the convex combination

that produces the output w′

i from the maximum/minimum indices of M v.

in
i

.

∈ P

18

M. Sotoudeh and A. V. Thakur

Algorithm 4: G(M,

in,

out, v, w′)

P

P

Input: An n × m matrix M . Partitionings P in, P out. A vector v with

non-negative entries. A vector w′ ∈ R(M v, P out).

α(M, P in, P out)) such that H(v
b

/P in ) = w′.

Output: A matrix H ∈ γ(
1 C, D ← 0|P in|×n, 0|P out|×m
2 for i = 1, 2, . . . , |P in| do
3

for j ∈ P in

i do
Cj,i ← vj /(Pk∈P in

4

vk)

i

8

5 w ← M v
6 for i = 1, 2, . . . , |P out| do
7

a, b ← argmaxp∈P out
Da,i ← (w′
Db,i ← 1 − Da,i
10 s ← (|P in
1 |, . . . , |P in
11 return ScaleCols (cid:0)DT M C, s(cid:1)

|P in||)

9

i

i − wb)/(wa − wb)

wp, argminp∈P out

wp

i

In total then, the returned matrix is in γ(
/P in ) = w′.

H(v

α(M,

in,

P

P

out)) and satisﬁes

The next lemma implies that we can always ﬁnd such a w′

b

satisfying the relations in Figure 3.

R(M v,

in)

P

∈

Lemma 2. Let σ be an activation function satisfying the WIVP, w any vector,
and
w′

a partitioning the dimensions of w. Then there exists a vector

) such that σ(w′) = (σ(w))/P .

P
R(w,

∈

P

Proof. Because σ(i) is deﬁned to be a component-wise activation function, we can
assume WLOG that P(i) has only a single partition, i.e., P(i) =
.
}}
w(i)
In that case, label the components of w(i) such that w(i)
n .
Then the statement of the lemma is equivalent to the assertion that there exists
some b
j )/n. But this is exactly the
deﬁnition of the WIVP. Hence, by assumption that σ(i) satisﬁes the WIVP, we
complete the proof.

n ] such that σ(i)(b) = (

1, 2, . . . , s(i)
. . .

{{
w(i)
2 ≤

1 , w(i)

j w(i)

1 ≤

[w(i)

P

≤

∈

We are ﬁnally prepared to prove the soundness theorem. It is restated here

for clarity.

Theorem 2. Let A be a set of weight set abstract domains and Σ a set of ac-
tivation functions. Suppose (i) each σ
Σ has entirely non-negative outputs,
Σ satisﬁes the Weakened Intermediate Value Property (Deﬁ-
and (ii) each σ
A, Σ
nition 20). Then T = AbstractLayerWise
) (Algorithm 3) soundly
h
over-approximates the DNN N .

(N , P,
i

A

∈

∈

Abstract Neural Networks

19

Proof. A diagram of the proof is provided in Figure 3.

Consider the ith layer. By Lemma 2, there exists some vector w′(i)

R(w(i), P(i))

such that σ(i)(w′(i)) = v
H (i)
can instantiate the ith abstract layer to (H (i), σ(i)), which maps v(i−1)
to v(i)

/P(i) . Furthermore, by Lemma 1 there exists some
/P(i−1) ) = w′(i). Therefore, in total we
/P(i−1)

γ(A(i)) such that H (i)(v(i−1)

∈

/P(i) .

∈

By applying this construction to each layer, we ﬁnd an instantiation of the
ANN that maps v(0)
/P(n) . Assuming P(0) and P(n) are the singleton
partitionings, then, we have that the instantiation maps v(0) = v to v(n) = N (v),
as hoped for. Hence, N (v)
T (v) for any such vector v, and so the ANN
overapproximates the original DNN.

/P(0) to v(n)

∈

8 Related Work

The recent results by Prabhakar et al. [30] are the closest to this paper. Prab-
hakar et al. introduce the notion of Interval Neural Networks and a sound quo-
tienting (abstraction) procedure when the ReLU activation function is used.
Prabhakar et al. also proposed a technique for veriﬁcation of DNNs using ReLU
activation functions by analyzing the corresponding INN using a MILP encod-
ing. Prabhakar et al. leaves open the question of determining the appropriate
partitioning of the nodes, and their results assume the use of the ReLU activa-
tion function and interval domain. We have generalized their results to address
the subtleties of other abstract domains and activation functions as highlighted
in Section 6.

There exists prior work [2,28,8] on models using interval-weighted neural net-
works. The goal of such approaches is generally to represent uncertainty, instead
of improve analysis time of a corresponding DNN. Furthermore, their semantics
are deﬁned using interval arithmetic instead of the more-precise semantics we
give in Section 4. Nevertheless, we believe that future work may consider appli-
cations of our more general ANN formulation and novel abstraction algorithm
to the problem of representing uncertainty.

There have been many recent approaches exploring formal veriﬁcation of
DNNs using abstractions. ReluVal [38] computes interval bounds on the outputs
of a DNN for a given input range. Neurify [37] extends ReluVal by using sym-
bolic interval analysis. Approaches such as DeepPoly [34] and AI2 [9] perform
abstract interpretation of DNNs using more expressive numerical domains such
as polyhedra and zonotopes. In contrast, Abstract Neural Networks introduced
in this paper use abstract values to represent the weight matrices of a DNN, and
are a diﬀerent way of applying abstraction to DNN analysis.

This paper builds upon extensive literature on numerical abstract domains
[5,24,6,23], including libraries such as APRON [16] and PPL [1]. Of particular
relevance are techniques for veriﬁcation of ﬂoating-point computation [4,29,29].

Techniques for compression of DNNs reduce their size using heuristics [15,7,14].

They can degrade accuracy of the network, and do not provide theoretical guar-
antees. Gokulanathan et al. [12] use the Marabou Veriﬁcation Engine [19] to

20

M. Sotoudeh and A. V. Thakur

simplify neural networks so that the simpliﬁed network is equivalent to the given
network. Shriver et al. [33] refactor the given DNN to aid veriﬁcation, though
the refactored DNN is not guaranteed to be an overapproximation.

9 Conclusion and Future Directions

We introduced the notion of an Abstract Neural Network (ANN). The weight
matrices in an ANN are represented using numerical abstract domains, such
as intervals, octagons, and polyhedra. We presented a framework, parameter-
ized by abstract domain and DNN activation function, that performs layer-wise
abstraction to compute an ANN given a DNN. We identiﬁed necessary and
suﬃcient conditions on the abstract domain and the activation function that
ensure that the computed ANN is a sound over-approximation of the given
DNN. Furthermore, we showed how the input DNN can be modiﬁed in or-
der to soundly abstract DNNs using rare activation functions that do not sat-
isfy the suﬃciency conditions are used. Our framework is applicable to DNNs
that use activation functions such as ReLU, Leaky ReLU, and Hyperbolic Tan-
gent. Our framework can use convex abstract domains such as intervals, oc-
tagons, and polyhedra. Code implementing our framework can be found at
https://github.com/95616ARG/abstract neural networks.

The results in this paper provide a strong theoretical foundation for further
research on abstraction of DNNs. One interesting direction worth exploring is
the notion of completeness of abstract domains [11] in the context of Abstract
Neural Networks. Our framework is restricted to convex abstract domains; the
use of non-convex abstract domains, such as modulo intervals [25] or donut
domains [10], would require a diﬀerent abstraction algorithm. Algorithms for
computing symbolic abstraction might show promise [31,21,35,36,32].

This paper focused on feed-forward neural networks. Because convolutional
neural networks (CNNs) are special cases of feed-forward neural networks, fu-
ture work can directly extend the theory in this paper to CNN models as well.
Such future work would need to consider problems posed by non-componentwise
activation functions such as MaxPool, which do not ﬁt nicely into the framework
presented here. Furthermore, extensions for recursive neural networks (RNNs)
and other more general neural-network architectures seems feasible.

On the practical side of things, it would be worth investigating the impact of
abstracting DNNs on the veriﬁcation times. Prabhakar et al. [30] demonstrated
that their abstraction technique improved veriﬁcation of DNNs. The results in
this paper are a signiﬁcant generalization of the results of Prabhakar et al.,
which were restricted to interval abstractions and ReLU activation functions.
We believe that our approach would similarly help scale up veriﬁcation of DNNs.
Acknowledgments We thank the anonymous reviewers and Cindy Rubio Gonz´alez
for their feedback on this work.

References

Abstract Neural Networks

21

a

1. Bagnara, R., Hill, P.M., Zaﬀanella, E.: The parma polyhedra library:
analysis
numerical
systems. Sci. Comput. Pro-
https://doi.org/10.1016/j.scico.2007.08.001,

Toward
and veriﬁcation of hardware and software
gram.
3–21
https://doi.org/10.1016/j.scico.2007.08.001

abstractions

complete

72(1-2),

(2008).

the

set

for

of

2. Beheshti, M., Berrached, A., de Korvin, A., Hu, C., Sirisaengtaksin, O.: On in-
terval weighted three-layer neural networks. In: Proceedings 31st Annual Sim-
ulation Symposium (SS ’98), 5-9 April 1998, Boston, MA, USA. pp. 188–194.
IEEE Computer Society (1998). https://doi.org/10.1109/SIMSYM.1998.668487,
https://doi.org/10.1109/SIMSYM.1998.668487

3. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei,
D.: Language models are few-shot learners. CoRR abs/2005.14165 (2020),
https://arxiv.org/abs/2005.14165

In: Ramalingam, G.

4. Chen, L., Min´e, A., Cousot, P.: A sound ﬂoating-point polyhedra ab-
(ed.) Programming Languages and
stract domain.
India, De-
Systems,
cember
in Computer Science,
vol. 5356, pp. 3–18. Springer (2008). https://doi.org/10.1007/978-3-540-89330-1 2,
https://doi.org/10.1007/978-3-540-89330-1 2

6th Asian Symposium, APLAS 2008, Bangalore,

2008. Proceedings. Lecture Notes

9-11,

5. Cousot, P., Cousot, R.: Abstract interpretation: A uniﬁed lattice model for static
analysis of programs by construction or approximation of ﬁxpoints. In: Graham,
R.M., Harrison, M.A., Sethi, R. (eds.) Conference Record of the Fourth ACM Sym-
posium on Principles of Programming Languages, Los Angeles, California, USA,
January 1977. pp. 238–252. ACM (1977). https://doi.org/10.1145/512950.512973,
https://doi.org/10.1145/512950.512973

6. Cousot, P., Halbwachs, N.: Automatic discovery of linear restraints among vari-
ables of a program. In: Aho, A.V., Zilles, S.N., Szymanski, T.G. (eds.) Confer-
ence Record of the Fifth Annual ACM Symposium on Principles of Programming
Languages, Tucson, Arizona, USA, January 1978. pp. 84–96. ACM Press (1978).
https://doi.org/10.1145/512760.512770, https://doi.org/10.1145/512760.512770
7. Deng, L., Li, G., Han, S., Shi, L., Xie, Y.: Model compression and hard-
ware acceleration for neural networks: A comprehensive survey. Proceedings of
the IEEE 108(4), 485–532 (2020). https://doi.org/10.1109/JPROC.2020.2976475,
https://doi.org/10.1109/JPROC.2020.2976475

8. Garczarczyk, Z.A.:

Interval neural networks.

IEEE International Sym-
ISCAS 2000, Emerging Technologies
the 21st Century, Geneva, Switzerland, 28-31 May 2000, Proceed-
IEEE (2000). https://doi.org/10.1109/ISCAS.2000.856123,

posium on Circuits and Systems,
for
ings. pp. 567–570.
https://doi.org/10.1109/ISCAS.2000.856123

In:

9. Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev,
M.T.: AI2: safety and robustness certiﬁcation of neural networks with ab-
In: 2018 IEEE Symposium on Security and Privacy,
stract interpretation.
SP 2018, Proceedings, 21-23 May 2018, San Francisco, California, USA. pp.
3–18. IEEE Computer Society (2018). https://doi.org/10.1109/SP.2018.00058,
https://doi.org/10.1109/SP.2018.00058

22

M. Sotoudeh and A. V. Thakur

10. Ghorbal, K., Ivancic, F., Balakrishnan, G., Maeda, N., Gupta, A.: Donut do-
mains: Eﬃcient non-convex domains for abstract interpretation. In: Kuncak, V.,
Rybalchenko, A. (eds.) Veriﬁcation, Model Checking, and Abstract Interpreta-
tion - 13th International Conference, VMCAI 2012, Philadelphia, PA, USA, Jan-
uary 22-24, 2012. Proceedings. Lecture Notes in Computer Science, vol. 7148,
(2012). https://doi.org/10.1007/978-3-642-27940-9 16,
pp. 235–250. Springer
https://doi.org/10.1007/978-3-642-27940-9 16

11. Giacobazzi, R., Ranzato, F., Scozzari, F.: Making abstract interpretations com-
plete. J. ACM 47(2), 361–416 (2000). https://doi.org/10.1145/333979.333989,
https://doi.org/10.1145/333979.333989

12. Gokulanathan, S., Feldsher, A., Malca, A., Barrett, C.W., Katz, G.: Simplifying
neural networks with the marabou veriﬁcation engine. CoRR abs/1910.12396
(2019), http://arxiv.org/abs/1910.12396

13. Goodfellow, I.J., Bengio, Y., Courville, A.C.: Deep Learning. Adaptive computa-
tion and machine learning, MIT Press (2016), http://www.deeplearningbook.org/
14. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
work with pruning, trained quantization and huﬀman coding. In: Bengio, Y., Le-
Cun, Y. (eds.) 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings (2016),
http://arxiv.org/abs/1510.00149

15. Iandola, F.N., Moskewicz, M.W., Ashraf, K., Han, S., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model
size. CoRR abs/1602.07360 (2016), http://arxiv.org/abs/1602.07360

16. Jeannet, B., Min´e, A.: Apron: A library of numerical abstract domains for
static analysis. In: Bouajjani, A., Maler, O. (eds.) Computer Aided Veriﬁ-
cation, 21st International Conference, CAV 2009, Grenoble, France, June 26
- July 2, 2009. Proceedings. Lecture Notes in Computer Science, vol. 5643,
pp. 661–667. Springer
(2009). https://doi.org/10.1007/978-3-642-02658-4 52,
https://doi.org/10.1007/978-3-642-02658-4 52

17. Julian, K.D., Kochenderfer, M.J., Owen, M.P.: Deep neural network compres-
sion for aircraft collision avoidance systems. CoRR abs/1810.04240 (2018),
http://arxiv.org/abs/1810.04240

18. Katz, G., Barrett, C.W., Dill, D.L., Julian, K., Kochenderfer, M.J.:
for verifying deep neural networks.
Reluplex: An eﬃcient SMT solver
In: Majumdar, R., Kuncak, V.
(eds.) Computer Aided Veriﬁcation -
29th International Conference, CAV 2017, Heidelberg, Germany, July 24-
28, 2017, Proceedings, Part
in Computer Science, vol.
I. Lecture Notes
10426, pp. 97–117. Springer (2017). https://doi.org/10.1007/978-3-319-63387-9 5,
https://doi.org/10.1007/978-3-319-63387-9 5

19. Katz, G., Huang, D.A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah,
P., Thakoor, S., Wu, H., Zeljic, A., Dill, D.L., Kochenderfer, M.J., Barrett,
C.W.: The marabou framework for veriﬁcation and analysis of deep neural
networks. In: Dillig, I., Tasiran, S. (eds.) Computer Aided Veriﬁcation - 31st
International Conference, CAV 2019, New York City, NY, USA, July 15-18,
2019, Proceedings, Part I. Lecture Notes in Computer Science, vol. 11561,
(2019). https://doi.org/10.1007/978-3-030-25540-4 26,
pp. 443–452. Springer
https://doi.org/10.1007/978-3-030-25540-4 26

20. Krizhevsky, A., Sutskever,

I., Hinton, G.E.:

deep convolutional neural networks.
Burges, C.J.C., Bottou, L., Weinberger, K.Q.

Imagenet classiﬁcation with
In: Bartlett, P.L., Pereira, F.C.N.,
(eds.) Advances in Neural

Abstract Neural Networks

23

Information Processing Systems 25: 26th Annual Conference on Neural
Information Processing Systems 2012. Proceedings of a meeting held De-
cember 3-6, 2012, Lake Tahoe, Nevada, United States. pp. 1106–1114 (2012),
http://papers.nips.cc/paper/4824-imagenet-classiﬁcation-with-deep-convolutional-neural-networks

21. Li, Y., Albarghouthi, A., Kincaid, Z., Gurﬁnkel, A., Chechik, M.: Sym-
bolic optimization with SMT solvers. In: Jagannathan, S., Sewell, P. (eds.)
The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL ’14, San Diego, CA, USA, January 20-
21, 2014. pp. 607–618. ACM (2014). https://doi.org/10.1145/2535838.2535857,
https://doi.org/10.1145/2535838.2535857

22. Maas, A., Hannun, A., Ng, A.: Rectiﬁer nonlinearities improve neural network
acoustic models. In: Proceedings of the International Conference on Machine Learn-
ing (2013)

23. Min´e, A.: The

octagon abstract domain. High. Order Symb. Com-
https://doi.org/10.1007/s10990-006-8609-1,

(2006).

19(1),

31–100
put.
https://doi.org/10.1007/s10990-006-8609-1

24. Min´e, A.: Tutorial

invariants by ab-
inference
stract interpretation. Found. Trends Program. Lang. 4(3-4), 120–372 (2017).
https://doi.org/10.1561/2500000034, https://doi.org/10.1561/2500000034

of numeric

on static

25. Nakanishi, T., Joe, K., Polychronopoulos, C.D., Fukuda, A.: The modulo inter-
val: A simple and practical representation for program analysis. In: Proceed-
ings of the 1999 International Conference on Parallel Architectures and Compi-
lation Techniques, Newport Beach, California, USA, October 12-16, 1999. pp. 91–
96. IEEE Computer Society (1999). https://doi.org/10.1109/PACT.1999.807422,
https://doi.org/10.1109/PACT.1999.807422

26. Oman, G., et al.: The converse of the intermediate value theorem: from conway
to cantor to cosets and beyond. Missouri Journal of Mathematical Sciences 26(2),
134–150 (2014)

27. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨opf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,
J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning
library. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox,
E.B., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019, 8-14 December 2019, Vancouver, BC, Canada. pp. 8024–8035 (2019),
http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library

(eds.)

28. Pati˜no-Escarcina, R.E., Bedregal, B.R.C., Lyra, A.:

Interval computing in
In: Das, G., Gulati,
neural networks: One layer interval neural networks.
Information Technology, 7th International Confer-
V.P.
ence on Information Technology, CIT 2004, Hyderabad,
India, December
20-23, 2004, Proceedings. Lecture Notes in Computer Science, vol. 3356,
https://doi.org/10.1007/978-3-540-30561-3 8,
pp.
https://doi.org/10.1007/978-3-540-30561-3 8

Intelligent

Springer

(2004).

68–75.

29. Ponsini, O., Michel, C., Rueher, M.: Verifying ﬂoating-point programs
with constraint programming and abstract interpretation techniques. Autom.
Softw. Eng. 23(2), 191–217 (2016). https://doi.org/10.1007/s10515-014-0154-2,
https://doi.org/10.1007/s10515-014-0154-2

30. Prabhakar, P., Afzal, Z.R.: Abstraction based output range analysis for neural
networks. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F.,

24

M. Sotoudeh and A. V. Thakur

Fox, E.B., Garnett, R. (eds.) Advances in Neural Information Processing Systems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
2019, 8-14 December 2019, Vancouver, BC, Canada. pp. 15762–15772 (2019),
http://papers.nips.cc/paper/9708-abstraction-based-output-range-analysis-for-neural-networks

31. Reps, T.W., Sagiv, S., Yorsh, G.: Symbolic implementation of the best trans-
former. In: Steﬀen, B., Levi, G. (eds.) Veriﬁcation, Model Checking, and Ab-
stract Interpretation, 5th International Conference, VMCAI 2004, Venice, Italy,
January 11-13, 2004, Proceedings. Lecture Notes in Computer Science, vol. 2937,
pp. 252–266. Springer
(2004). https://doi.org/10.1007/978-3-540-24622-0 21,
https://doi.org/10.1007/978-3-540-24622-0 21

32. Reps, T.W., Thakur, A.V.: Automating abstract interpretation. In: Jobstmann,
B., Leino, K.R.M. (eds.) Veriﬁcation, Model Checking, and Abstract Inter-
pretation - 17th International Conference, VMCAI 2016, St. Petersburg, FL,
USA, January 17-19, 2016. Proceedings. Lecture Notes in Computer Science,
vol. 9583, pp. 3–40. Springer (2016). https://doi.org/10.1007/978-3-662-49122-5 1,
https://doi.org/10.1007/978-3-662-49122-5 1

33. Shriver, D., Xu, D., Elbaum, S.G., Dwyer, M.B.: Refactoring neural networks for
veriﬁcation. CoRR abs/1908.08026 (2019), http://arxiv.org/abs/1908.08026
34. Singh, G., Gehr, T., P¨uschel, M., Vechev, M.T.: An abstract domain for certi-
fying neural networks. Proc. ACM Program. Lang. 3(POPL), 41:1–41:30 (2019).
https://doi.org/10.1145/3290354, https://doi.org/10.1145/3290354

35. Thakur, A.V., Elder, M., Reps, T.W.: Bilateral

In: Min´e, A., Schmidt, D.

sym-
bolic abstraction.
-
19th International Symposium, SAS 2012, Deauville, France, September 11-
in Computer Science, vol. 7460,
13, 2012. Proceedings. Lecture Notes
pp. 111–128. Springer
(2012). https://doi.org/10.1007/978-3-642-33125-1 10,
https://doi.org/10.1007/978-3-642-33125-1 10

(eds.) Static Analysis

algorithms

for

36. Thakur, A.V., Reps, T.W.: A method for symbolic computation of abstract
In: Madhusudan, P., Seshia, S.A. (eds.) Computer Aided Ver-
operations.
iﬁcation - 24th International Conference, CAV 2012, Berkeley, CA, USA,
July 7-13, 2012 Proceedings. Lecture Notes in Computer Science, vol. 7358,
pp. 174–192. Springer
(2012). https://doi.org/10.1007/978-3-642-31424-7 17,
https://doi.org/10.1007/978-3-642-31424-7 17

37. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Eﬃcient formal safety anal-
ysis of neural networks. In: Bengio, S., Wallach, H.M., Larochelle, H., Grauman,
K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Process-
ing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, 3-8 December 2018, Montr´eal, Canada. pp. 6369–6379 (2018),
http://papers.nips.cc/paper/7873-eﬃcient-formal-safety-analysis-of-neural-networks

38. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Formal security anal-
ysis of neural networks using symbolic intervals. In: Enck, W., Felt, A.P.
(eds.) 27th USENIX Security Symposium, USENIX Security 2018, Baltimore,
MD, USA, August 15-17, 2018. pp. 1599–1614. USENIX Association (2018),
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi

A Proof of Theorem 1

In Section 5.1 we argued that, when the abstract domain used is convex,
α and
αbin produce the same result. Hence the latter (which is computable) can be

b

b

Abstract Neural Networks

25

used in place of the former when executing Algorithm 3. We now prove this fact.
To do so, we use the following Lemma, which implies a similar claim for the
PCMs:

Lemma 3. Let
PCMs(

P

P

be a partitioning of

1, 2, . . . , n

. Then every PCM C

) is a convex combination of the binary PCMs BinPCMs(

).

{

}

∈

P

∈

P
BinPCMs(

). It suﬃces to construct C as a convex combination of

Proof. Let C
PCMs(
the binary PCMs Bi ∈
We ﬁrst note that the columns in the binary PCMs are independent — given
two binary PCMs B1 and B2 the PCM B3 formed by setting the ith column
of B3 to be either the ith column of B1 or that of B2 arbitrarily is always a
valid binary PCM. Because of this, we can consider each column separately, i.e.,
assume that C and the Bis have only a single column.

P

).

In that case, the binary PCMs can be thought of as length-n vectors each
with all zero entries except for a single 1 entry. Similarly, C must have zeros in
the entries where the binary PCMs all have zeros, and the non-zero entries must
be positive and sum to one. But this is equivalent to stating that C lies in the
convex span of the binary PCMs Bi, as claimed.

Then we can prove

Theorem 1. If

A

is convex, then

α(M,

in,

P

P

out,

) =

A

αbin(M,

in,

P

P

out,

).

A

Proof. By convexity, it suﬃces to show that every DT M C with C and D being
PCMs can be written as a convex combination of the F T M E matrices, with E
out respectively. Note that the scaling
and F being binary PCMs of
is uniform, so we may ignore the ScaleCols.

in and

P

P

b

b

By Lemma 3 we can write C as a convex combination of the binary PCMs
j αjEj. We can similarly write D as a convex combination of the

k βkFk. By distributivity, then, we have

Ej, i.e., C =
binary PCMs Fk, i.e., D =
P

P
DT M C =

αjβkF T

k M Ej.

j
X

Xk

k M Ej matrices are exactly the binary mergings used by

The F T
αbin, hence it
suﬃces now to show that the αj βk coeﬃcients are non-negative and sum to one.
They are non-negative by construction, and sum to one because

b

αjβk =

αj

j  
X

Xk

=

βk

!

j
X

1 =

αj ·

j
X

αj = 1.

j
X

Xk

Therefore, any such DT M C is a convex combination of the binary mergings, as
claimed, and hence
) =

α(M,

out,

out,

αbin(M,

in,

in,

).

P

P

A

P

P

A

b

b

26

M. Sotoudeh and A. V. Thakur

B The WIVP is Strictly Weaker Than the IVP

In Section 6.1 we stated that the WIVP was strictly weaker than the IVP, i.e.,
there exists a function f which satisﬁes the WIVP but not the IVP. The below
proof constructs such a function.

Proof. Let g be any strongly Darboux function, e.g., Conway’s Base 13 func-
tion [26]. The strongly Darboux property implies that the postimage of any
non-empty open interval under g is all of R.
Then, let h be any surjective map from R

Q. For example, we can take

→

Finally, deﬁne

h(x) =

x x
0 x

(

Q
Q.

∈
6∈

f (x) := h(g(x))

and note that (i) f (x)
open interval under f is exactly Q.

∈

Q for every x while (ii) the postimage of any non-empty

First, f does not satisfy the IVP because, for instance, 1 and 2 are in the

image of f but not √2.

On the other hand, f does satisfy the WIVP. To see this, consider any a1 ≤
an. If they are all the same, then f (a1) is equal to the average.

a2 ≤ · · · ≤
Otherwise, note that each f (ai)

Q hence

∈
i f (ai)
n

P
(a1, an)

∈

⊆

Q = f ((a1, an))

∈

[a1, an] with f (b) =

and so there exists a b

i f (ai)/n as desired.

Therefore, the WIVP is strictly weaker than the IVP.
P

C Proof of Theorem 3

In Section 6.2 we stated a necessary condition for sound abstraction via Algo-
rithm 3, namely that the activation functions have non-negative outputs. We
now proceed to prove this theorem, restated below.

Theorem 3. Suppose some σ
Σ is an activation function with neither entirely
A is at least as
non-negative nor entirely non-positive outputs, and every
precise as the interval hull abstraction. Then there exists a neural network N that
A, Σ
uses σ and a partitioning P such that T = AbstractLayerWise
)
A
h
does not over-approximate N .

(N , P,
i

A ∈

∈

Proof. Label x, y
DNN deﬁned by the function

∈

R such that σ(x) < 0 but σ(y) > 0. We will then take the

N (v) =

1 0
0 1

(cid:20)

σ

(cid:21)

(cid:18)(cid:20)

v

x
y

(cid:21)

(cid:19)

Abstract Neural Networks

27

and the partitioning

P = (

{{

1

,

}}

{{

1, 2

,

,

1
}

2
{

}}

)

{{

}}

which collapses all of the hidden dimensions. Then the corresponding interval
ANN is given by

TInt(v) =

[0, 2]
[0, 2]

(cid:21)

(cid:20)

σ

[x, y]

v

.

(cid:0)(cid:2)

(cid:3)

(cid:1)

But then the components of N (1) have opposite signs, which can never happen
TInt(1). If we used any more-precise
for an instantiation of TInt(1), hence N (1)
abstraction than intervals to get an ANN T we would have T (1)
TInt(1), hence
still N (1)
T (1). Therefore, T does not over-approximate N , completing the
proof.

⊆

6∈

6∈

D Proof of Theorem 4

Theorem 4. Let N be a DNN and suppose that, on some input region R, the
output of the activation functions are lower-bounded by a constant C. Then,
there exists another DNN N ′, with at most one extra dimension per layer, which
satisﬁes (i) N ′(x) = N (x) for any x
R, (ii) N ′ has all non-negative activation
functions, and (iii) the new activation functions σ′ are of the form σ′(x) =
max(σ(x) +

, 0).

C

∈

|

|

Proof. We will construct N ′ with layers (W (i)′

, σ(i)′
We will ﬁrst deﬁne the activation functions σ(i)′

).
used by N ′. For every i < n
set σ(i)′
:= σ(n). Note that, by
assumption for any input in R we will have σ(i)(x) +
> 0, so the max is just
needed to ensure that the activation function is formally non-negative on the
entirety of R.

, 0). For i = n set σ(n)′
C

(x) := max(σ(i)(x) +

C

|

|

|

|

We now deﬁne the weight matrices W (i)′
such that

W (i). For every i > 1, deﬁne W (i)′

used by N . For i = 1, set W (i)′

=

W (i)′ v := W (i)(v

) = W (i)v

C

|

− |

W (i)

C

,

|

|

−

where we have abused notation to let C here refer to the vector with every
component ﬁxed to the constant C. Note that adding constant terms such as
this can be done by adding a single additional dimension to each layer according
to a standard transformation.

We now argue that N and N ′ have the same output on any vector v. Let v(1),
v(2), . . . , v(n) be the post-activation vector after each layer in the original DNN
as deﬁned in Deﬁnition 3, and v(1)′
, . . . , v(n)′
be the same for the constructed
DNN on an input v.

28

M. Sotoudeh and A. V. Thakur

Then we have, by construction, v(1)′

= v(1), v(2)′

= v(2) +

2 < i < n we have inductively

, and for all

C

|

|

v(i)′

)

= σ(i)′
(W (i)′ v(i−1)′
= max(σ(i)(W (i)(v(i−1)′
− |
= max(σ(i)(W (i)((v(i−1) +
= max(σ(i)(W (i)v(i−1)) +
= max(v(i) +

, 0).

C

|

|

|

)) +

C

, 0)

|
)) +

|
C

− |

|

C

|
C

|
C

|

)

|
, 0)

, 0)

C

|

|

If v
v(i) and hence this gives simply

∈

R, then we have by assumption that C is a lower-bound for the value of

v(i)′

= max(v(i) +

, 0) = v(i) +

C

|

|

C

.

|

|

Finally, for the last layer we have

v(n)′

)

(W (n)′ v(n−1)′
= σ(n)′
= σ(n)(W (n)(v(n−1)′
− |
= σ(n)(W (n)((v(n−1) +
= σ(n)(W (n)v(n−1))
= v(n)

|

))

C

|
C

)

C

))

|

− |

|

as desired.

We now provide an example of this construction.

Example 9. In the LeakyReLU example from Section 6.2, suppose we are only
1, 1]. On that domain, the output of the
[
interested in the behavior of f for x1 ∈
0.5. Applying the construction
LReLU is at least
−
x1
from the theorem, we have f ′(x1) =
1
LReLU′(x; 0.5) := max(LReLU(x; 0.5) + 0.5, 0).

0.5, hence we can take C =

−
LReLU′

1 1 −1
1 0 −0.5
0 1 −0.5

0
1
−1 0
0 0.5

where

; 0.5

(cid:21) h

(cid:18)(cid:20)

−

(cid:19)

(cid:21)

(cid:20)

i

Now consider, for example, x1 =

−

1. In the original network we had

1) =

f (

−

1 1
1 0
0 1

(cid:21)

(cid:20)

LReLU

1
−1
i

(cid:16)h

[−1]; 0.5

=

(cid:17)

(cid:20)

1 1
1 0
0 1

−0.5
1

=

(cid:21) h

(cid:20)

i

0.5
−0.5
1

(cid:21)

,

noticing that the output of the LReLU had a negative component. In the new
network, on the other hand, we have

f ′(x1) =

LReLU′

1 1 −1
1 0 −0.5
0 1 −0.5

(cid:20)

(cid:21)

1
0
−1 0
0 0.5

(cid:18)(cid:20)

(cid:21) h

−1
1

; 0.5

=

(cid:19)

(cid:20)

1 1 −1
1 0 −0.5
0 1 −0.5

0
1.5
1

=

0.5
−0.5
1

(cid:21) (cid:20)

(cid:21)

(cid:20)

(cid:21)

i
where we can see that indeed the output of the new activation function LReLU′
is non-negative.

= f (1)

Abstract Neural Networks

29

E Proof of Theorem 5

In Section 6.3 we claimed that activation functions satisfying the WIVP is a
necessary condition for soundness of Algorithm 3. This is formalized by the
theorem below, which we now prove.

Theorem 5. Suppose some σ
Σ is an activation function which does not sat-
∈
A is at least as precise as the interval hull abstrac-
isfy the WIVP, and every
tion. Then there exists a neural network N using only the identity and σ activa-
A, Σ
tion functions and partitioning P such that T = AbstractLayerWise
(N , P,
i
h
does not over-approximate N .

A ∈

)
A

Proof. Let a1 ≤
does not exist any b

a2 ≤ · · · ≤
∈

[a1, an] such that

an be the points violating the WIVP for σ, i.e. there

σ(b) =

We can then take the DNN given by

i ai
n
P

.

N (v) =

1 1

· · ·

(cid:2)

1

(cid:3)

σ 





and the partitioning

a1
a2
...
an















v





P = (

{{

1

,

}}

{{

1, . . . , n

,

}}

1

)

}}

{{

which collapses all of the hidden dimensions. Then in the interval abstraction
we get the ANN

T (v) =

[n, n]

σ

[a1, an]

v

.

Now, consider N (1) and T (1). We have by deﬁnition that

(cid:2)

(cid:3)

(cid:0)(cid:2)

(cid:3)

(cid:1)

N (1) = σ(a1) +

+ σ(an).

· · ·

Suppose for sake of contradiction that T over-approximates N . Then we must
T (1). Then there must be an assignment to the weights in T which
have N (1)
matches N , i.e., then there must be a b

[n, n] such that

[a1, an] and m

∈

∈
mσ(b) = f (1) =

∈

σ(ai).

i
X

But m

∈

[n, n] implies m = n, hence in that case we have

σ(b) =

i σ(ai)
n

.

P

But b
a1, . . . , an and so completing the proof.

∈

[a1, an], contradicting the assumption that σ violates the WIVP at

