1
2
0
2

l
u
J

6
2

]

Y
S
.
s
s
e
e
[

2
v
9
9
6
3
0
.
1
1
0
2
:
v
i
X
r
a

Diﬀerentiable Predictive Control:
Deep Learning Alternative to Explicit Model Predictive Control
for Unknown Nonlinear Systems

J´an Drgoˇna1, Karol Kiˇs2, Aaron Tuor1, Draguna Vrabie1, Martin Klauˇco2
1Paciﬁc Northwest National Laboratory, Richland, Washington, USA,
{jan.drgona, aaron.tuor, draguna.vrabie}@pnnl.gov
2Slovak University of Technology, Bratislava, Slovakia,
{karol.kis, martin.klauco}@stuba.sk

Abstract

We present diﬀerentiable predictive control (DPC) as a deep learning-based alternative
to the explicit model predictive control (MPC) for unknown nonlinear systems. In the
DPC framework, a neural state-space model is learned from time-series measurements of
the system dynamics. The neural control policy is then optimized via stochastic gradient
descent approach by diﬀerentiating the MPC loss function through the closed-loop system
dynamics model. The proposed DPC method learns model-based control policies with
state and input constraints, while supporting time-varying references and constraints. In
embedded implementation using a Raspberry-Pi platform, we experimentally demonstrate
that it is possible to train constrained control policies purely based on the measurements of
the unknown nonlinear system. We compare the control performance of the DPC method
against explicit MPC and report eﬃciency gains in online computational demands, memory
requirements, policy complexity, and construction time. In particular, we show that our
method scales linearly compared to exponential scalability of the explicit MPC solved via
multiparametric programming.

1. Introduction

Incorporation of machine learning methods in control applications is becoming one of
the leading research avenues in the ﬁeld of control theory. The design of many novel control
methods based on machine learning (ML) approaches is heavily inspired by the beneﬁts
of model predictive control (MPC), such as constraints handling and robustness. The
substitution of MPC with ML-based controllers was studied by several well-established
researchers in the control domain [1, 2, 3, 4, 5]. Furthermore, the application of neural
networks as substitutes of MPC behavior has been considered in practical applications as
well [6, 7, 8, 9]. All aforementioned works fall into the category of so-called approximate
MPC based on imitation learning of original MPC. As such, these works have one signiﬁcant
disadvantage, which is the reliance on data sets collected from closed-loop experiments, i.e.,
the ML-based controllers are trained based on experiments involving fully implemented
MPC.

Addressing these challenges, we present a novel control method called diﬀerentiable
predictive control (DPC) for learning both nonlinear system dynamics and constrained

Preprint submitted to Elsevier

July 27, 2021

 
 
 
 
 
 
control policies without supervision from the expert controller in a sample eﬃcient way.
The only requirement is a dataset of suﬃciently excited time-series of the system dynamics.
This in contrast with the recent implementation in [1], where a trained ML-based controller
runs alongside an online MPC strategy, making the algorithm computationally expensive
in many applications running in low computational resource settings. The presented DPC
method is based on the neural parametrization of the closed-loop dynamical system with
two building blocks: i) neural system model, and ii) a deep learning formulation of model
predictive control policy. In particular, we combine system identiﬁcation based on structured
neural state-space model and policy optimization with embedded inequality constraints via
backpropagation of the control loss through the closed-loop system dynamics model. The
conceptual methodology of DPC is illustrated in Fig. 1.

Figure 1: Conceptual methodology of the proposed constrained nonlinear diﬀerentiable predictive control
(DPC).

The presented DPC method supports nonlinear systems, with both input and state
constraints, arbitrary reference signals and represents a methodological extension of prior
work on DPC for linear systems [10]. A method similar in spirit has been recently proposed
by [11], where the authors use Log-Sum-Exp neural networks to approximate the MPC-
related cost function given the measurements of the system dynamics. However, in contrast
to the proposed DPC method, the implementation of the method in [11] still relies on the
online nonlinear optimization, does not handle state constraints, and training on arbitrary
reference signals poses computational challenges.

We demonstrate the capabilities of the proposed DPC method on experimental results
using a laboratory device called FlexyAir with nonlinear dynamics and noisy measurement
signals. We demonstrate several key features of DPC resulting in data-eﬃciency, scalability,
and constraint handling:

1. Data-driven explicit solution of constrained optimal control problems for nonlinear

systems by means of diﬀerentiable parametric optimization.

2. Diﬀerentiable closed-loop system model based on constrained neural control policies

and neural state-space models.

3. In contrast with explicit MPC, our method supports dynamical constraints and trajec-

tory preview capabilities.

4. Linear scalability in terms of the number of decision variables and length of the

2

featuresinitial conditionsreferencesconstraintscontrol policysystem modeldifferentiablecontrol objectivereference trackingconstraints penaltiesbackward propagationfeedbackforward propagationsimulationdifferentiable closed-loop systemprediction horizon compared to exponential scalability of explicit MPC solutions based
on multi-parametric programming.

5. Our approach requires less online computation time and memory than explicit MPC

solutions.

1.1. Related Work

Explicit model predictive control. Explicit MPC is a powerful method for the systematic design
of optimal control policies with a fast online evaluation procedure. In explicit MPC, instead
of solving the corresponding optimal control problem online, the solution is pre-computed
oﬄine through the parametric optimization [12, 13, 14, 15]. The resulting controller then
boils down to obtaining a set of piecewise aﬃne (PWA) functions that can be evaluated on
an embedded platform. Unfortunately, the explicit MPC comes with a signiﬁcant drawback:
scalability of the solution and the memory footprint of the resulting PWA control policy [16].
Even for small-scale dynamical systems, the explicit MPC’s memory footprint can reach
several megabytes. In the last years, researchers developed many methods for decreasing the
complexity and memory footprint of the explicit solution [17, 18, 19, 20, 21, 22, 23, 24, 25].
Nevertheless, all of the aforementioned methods are often costly in terms of processing
time and do not scale to large dimensions. Moreover, most of the explicit MPC methods
handle only linear system dynamics and constraints. For dealing with nonlinear systems,
there are few methods based on approximations of the underlying nonlinear multiparametric
programs (mpNLP) [26, 27, 28]. While authors in [29] incorporated ReLU neural networks
as constraints in multiparametric programming problems (mpP). Despite these numerous
advances, all of the contemporary explicit MPC methods suﬀer the curse of dimensionality
associated with the solution of the underlying mpP [30].
In this paper, we present an
alternative method based on diﬀerentiable programming for the approximate solution of the
underlying mpP with linear scalability with respect to a number of decision variables.

Approximate MPC. Recently, several works have been devoted to the use of machine
learning to approximate the solution of the underlying multiparametric programming problem
to obtain the explicit MPC control laws. One set of these works includes supervised
learning based on mimicking the MPC behavior and replacing the control law with a neural
network [31, 32, 7]. These approaches are known in the machine learning community as
imitation learning [33], or MPC-guided policy search [34]. Several procedures were designed
to approximate MPC’s behavior with neural networks, which signiﬁcantly reduces the
implementation requirements of the MPC with minimal impact on the control performance [32,
7]. Constraint satisfaction using neural network-based approximations of MPC control laws
is a new and active research area. To tackle this issue, some authors proposed including
additional layers in the policy network projecting the control inputs onto the constrained
region of the state and action spaces [35, 36]. Authors in [1] use an additional dual policy
neural network to estimate the sub-optimality of the learned control law with probabilistic
guarantees. Others employ learning bounds for empirically validating constraints handling
capabilities after the network is trained [5]. For more comprehensive overview of the
approximate MPC, and learning-based MPC (LBMPC) methods we refer the reader to [37].
However, all previous works rely upon several factors, which are prerequisites for their
successful implementation. First, the supervisory MPC strategy needs to be designed to
generate the training data. In this work, we present an alternative approach that does not
require a supervisory controller, to begin with. Instead, the proposed strategy learns the

3

system dynamics model and uses this model as part of the diﬀerentiable closed-loop system
model for policy optimization via stochastic gradient descent.

Neural Models in MPC. To tackle the modeling problem, some researchers focus on training
neural networks as prediction models for MPC [38]. Authors in [39] use low-rank features
of the high-dimensional system to train a recurrent neural network (RNN) to predict the
control relevant quantities for MPC. Other authors have used structured neural network
models inspired by classical linear time-varying state-space models [40], whereas some have
proposed using convex neural architectures [41], graph neural networks [42], or stable neural
networks based on Lyapunov functions [43]. In this work, we build on these trends and
employ neural state space models as generic abstractions for learning nonlinear dynamics
models of the controlled system.

Automatic Diﬀerentiation in Control. The use of automatic diﬀerentiation (AD) for control
and optimization is a well-established method. For instance, CasADi toolbox [44] uses known
system dynamics and constraints for constructing a computational graph and computes the
gradients for nonlinear optimization solvers. From the perspective of machine learning, the
authors in [41] investigated the idea of solving the optimal control problem by backpropaga-
tion through the learned system model parametrized via convex neural networks [45]. Others
developed, domain-speciﬁc diﬀerentiable physical models for the robotics domain [46, 47].
Learning linear Model Predictive Control (MPC) policies by diﬀerentiating the KKT con-
ditions of the convex approximation at a ﬁxed point was introduced in [48]. However, a
generic method for explicit solution of constrained nonlinear optimal control problems with
unknown dynamics using AD is still lacking. The presented work aims to bridge this gap.

Constrained deep learning. Incorporating constraints into deep learning represents multiple
challenges such as non-convexity, convergence, and stability of the learning process [49].
Penalty methods and loss function regularizations are the most straightforward way of
imposing constraints on the deep neural network outputs and parameters [50, 51, 52]. In those
methods, the loss function is augmented with additional terms penalizing the violations of
soft constraints via slack variables, which typically works well in practice, often outperforming
hard constraint methods [53, 54]. Authors in [55] presented a method for imposing hard
equality and inequality constraints in the context of learning solutions of optimization
problems. For control applications, authors in [56, 57] use barrier methods combined
with Lyapunov functions to enforce output constraints, stability, and boundedness of the
neural network controller. While authors in [58] use penalty methods for soft constrained
reinforcement learning applied to process control application. An alternative to penalty
and barrier methods are neural network architectures imposing hard constraints, such as
linear operator constraints [59], or architectures with Hamiltonian [60] and Lagrangian [61]
structural priors for enforcing energy conservation laws. In this paper, we leverage both,
penalty functions for imposing inequality constraints on state and action variables, and neural
architecture design imposing hard equality constraints representing temporal dependencies
of the system states, also called single shooting formulation.

4

2. Preliminaries

2.1. System Dynamics

We assume an unknown partially observable nonlinear dynamical system in discrete time:

xk+1 = f (xk, uk)
yk = g(xk)

(1a)

(1b)

where xk ∈ Rnx is the unknown system state, yk ∈ Rny is the observed output, and uk ∈ Rnu
is the control input at time k. We assume we have access to the data generated by the
system in the form of input-output tuples:

Ξ = {(yi

k, ui

k), (yi

k+1, ui

k+1), · · · , (yi

k+N , ui

k+N )}, i ∈ Nn
1

(2)

where n is the number of sampled trajectories with N time steps.

2.2. Model Predictive Control

In this work, we consider a well-known model predictive controller (MPC) [62]. We follow
standard formulation of the linear MPC as the quadratic optimization problem, speciﬁcally
formed as

min
u0,...,uN −1

s.t.

N −1
(cid:88)

(cid:0)||yk − r||2
Qr

+ ||uk − uk−1||2

Qdu

(cid:1)

k=0
xk+1 = Axk + Buk,
yk = Cxk + Duk,
uk ≤ uk ≤ uk,
xk ≤ xk ≤ xk,
x0 = x(t),
u−1 = u(t − Ts).

(3a)

(3b)

(3c)

(3d)

(3e)

(3f)

(3g)

Here, the objective function is deﬁned as the ﬁnite sum of two terms over a prediction horizon
Q = a(cid:124)Qa. Note, that to
N . Both terms are considered as a weighted second norm, i.e. ||a||2
enforce problem (3) feasibility, the weighting factors Qdu and Qr must be chosen as positive
deﬁnite and positive semi-deﬁnite matrices, respectively. The constraints (3b)-(3e) are
enforced for k ∈ 0, 1, . . . , N − 1. Moreover, to enforce reference tracking, the minimization
objective, Equation 3a, includes a term for reference tracking error, ||yk − rk||2
, as well as
Qr
a control rate penalisation term, ||uk − uk−1||2
, which is a standard way to enforce oﬀset
free tracking [63]. The optimization problem is initialised with current state measurement
x(t), as in (3f), by the value of the control action from previous sampling instant u(t − Ts),
as described in (3g), and by the reference value r in (3a). Note, that the notation u−1 is
valid for k = 0, which is necessary to compute the value of the objective function in the
initial prediction step.

Qdu

The control strategy is implemented in the receding horizon fashion, where we solve (3) to-
wards global optimality yielding an optimal sequence of control actions U(cid:63) = [u(cid:63)(cid:124)
N −1](cid:124),
while only the ﬁrst action is applied to the system. Representation of such an implementation
is visualised on the Fig. 2.

0 , . . . , u(cid:63)(cid:124)

5

Figure 2: Realisation of the closed-loop control system with an unknown dynamical system.

We formulate the quadratic optimization problem (3) in Matlab with the YALMIP
toolbox [64]. The problem is then solved numerically with the GUROBI solver. Due to the
fast dynamics of the controlled system, and that the numerical solution to the MPC problem
takes more time compared to the sampling rate Ts, we consider a parametric solution to the
optimization problem (3). The parametric solution allows us to evaluate the control law to
obtain the optimal control action within allotted time.

2.3. Explicit Model Predictive Control

Parametric optimization theory allows us to create an analytical map between initial
conditions of the optimal control problem (3) and the optimal solution U(cid:63). Speciﬁcally, the
vector of parameters, thus the initial conditions, is deﬁned as

ˆξ =





x(t)
r
u(t − Ts)



 .

(4)

After applying elementary matrix operations, presented in [65], the OCP from (3) can be
reformulated to

U(cid:124)HU + ˆξ(cid:124)FU

min
U
s.t. GU ≤ w + Sˆξ,

(5a)

(5b)

which constitutes as a parametric quadratic optimization problem (PQP). The solution to
this problem can be obtain via standard procedures of parametric programming. Namely,
the results is represented by a piece-wise aﬃne (PWA) function, given as

U(cid:63)(θ) =






α1

ˆξ + β1

αnR

ˆξ + βnR

if ˆξ ∈ R1
...
if ˆξ ∈ RnR

.

(6)

Here, the variable ˆξ stands for the vector of parameters, nR denotes the total number of
regions, while the vectors αi and βi deﬁne the speciﬁc control law with respect to a region
Ri. The regions are deﬁned as polyhedral sets, namely

Ri = { ˆξ | Γi

ˆξ ≤ γi}

i = 1, . . . , nR,

(7)

Here, the matrices Γi and γi denote the half-space representation of regions. Since, the
optimization problem (3) is a quadratic problem with linear constraints, all regions are
deﬁned by linear inequalities.

6

UnknownDynamicalSystemMPCu(t)y(t)r(t)Note that the procedure of obtaining a numerical representation of matrices Γi and γi
is done by means of Multi-Parametric Toolbox [66] in Matlab. To use the explicit MPC
in connection with the laboratory device, we export the control law into Python source
code. The export to Python source code contains two parts. The ﬁrst part is the coeﬃcients
from (3.2) and (7), and the second part is the algorithm that evaluates the control law. The
algorithm is based on a well-known sequential search method from [67].

3. Diﬀerentiable Predictive Control

This section presents Diﬀerentiable Predictive Control (DPC), a constrained neural
network-based method for learning nonlinear state-space models and optimal control policies
for unknown dynamical systems. Our system identiﬁcation method is based on neural state-
space model architecture [68] including constraints to enforce physically realistic predictions.
In DPC, we combine the neural state-space model with a neural control policy, constructing a
fully parametrized diﬀerentiable closed-loop system dynamics model. This generic closed-loop
architecture allows us to learn a wide range of constrained control policies using end-to-
end auto-diﬀerentiation of the MPC-like loss functions and stochastic gradient descent
optimization.

In the context of explicit MPC, the proposed approach can be seen as an data-driven
approximate solution of the underlying parametric programming problem (5). Similar to
explicit MPC, DPC optimizes control policies oﬄine using N -step ahead predictions of the
closed-loop system dynamics model generated as a response to the distribution of synthetically
generated control features ξ. After the training, analogous to MPC, DPC is deployed in the
receding horizon control (RHC) fashion. The high-level summary of the DPC method is
provided in Algorithm 1.

Algorithm 1 Diﬀerentiable Predictive Control.

1: input system identiﬁcation dataset ΞID
2: input sampled control parameters dataset Ξctrl
3: input optimizer O
4: input Diﬀerentiable closed-loop system model composed of neural state space model fθ

and neural control policy πΘ

5: input System identiﬁcation loss (cid:96)ID with penalty constraints
6: input MPC-inspired loss (cid:96)DPC with penalty constraints
7: train neural state space model fθ using dataset ΞID to optimize loss (cid:96)ID with optimizer

O

8: train neural policy πΘ in the closed-loop with the system model fθ using dataset Ξctrl

to optimize loss (cid:96)DPC with optimizer O

9: return learned system dynamics model fθ and optimized policy πΘ

3.1. Constrained Neural State Space Models

We aim to learn a constrained neural representation of the unknown system dynamics,

given the input-output time-series dataset (2) obtained from system observation.

7

Model architecture. We present a generic block neural state-space model (BN-SSM) to
represent and learn partially observable unknown nonlinear system dynamics (1), given
the labeled dataset (2). The BN-SSM architecture is shown in Fig. 3 with corresponding
equations given as follows:

xk+1 = fx(xk) + fu(uk)
yk = fy(xk)
x0 = fo(y1−N , . . . , y0)
k ∈ NN
0

(8a)

(8b)

(8c)

(8d)

Where k deﬁnes the discrete time step, and N deﬁnes the prediction horizon, i.e. number
of rollout steps of the recurrent model. Individual block components fx, and fu, fy, and
fo are represented by neural networks. Where fx, and fu deﬁne the hidden state and
input dynamics, replacing the A and B matrices in the classical linear state-space model,
respectively. The block fy deﬁnes the output mapping from hidden states xk to observables
yk, replacing the C matrix in the linear model. The observer block fo maps the past output
trajectories Yp = {y1−N , . . . , y0} onto initial states x0, which is necessary for handling
partially observable systems. Now let us compactly represent the N -step ahead rollout of
the model (8) as {Yf , Xf } = f N

θ (Yp, Uf ) with lumped parameters θ.

Figure 3: System identiﬁcation with block-structured neural state-space model (BN-SSM). Here y in red and
blue color represent observed and predicted system outputs, respectively, x are hidden states, and u are
observed control action trajectories.

Remark. The proposed BN-SSM architecture (8) represents a generalization of a family
of neural state-space models [69, 70, 71, 72, 73, 74, 75, 76]. Depending on the choice of
neural blocks fx, fu, fy, and fy one can represent fully nonlinear, Hammerstein-Weiner,
Weiner, Hammerstein, or simply linear dynamics models with or without internal feedback.
Additionally, a block architecture allows us to impose local regularizations on the block
structure or constraints on internal block-generated variables.

System identiﬁcation loss. We train the neural state-space dynamics (8) on sampled input-
output trajectories (2) of the observed system dynamics. The multi-term system identiﬁcation

8

SSMfxfufyNfox0u1, u2, ... , uNy1-Ny2-Ny0...y1y2yN...Time-series datasetof system dynamicsSystem ID loss function -uyyyloss is given as follows:

LID(Ytrue, Y, Y, Y|θ) =

1
nN

n
(cid:88)

N
(cid:88)

(cid:16)

i=1

k=1

||ytrue,i
k

− yi

k||2

2 + Qdx||xi

k − xi

k−1||2

2+

Qy||p(yi

k, yi
k

)||2

2 + Qy||p(yi

k, yi

k)||2

2+

Qu||p(fu(ui

k), fu)||2

2 + Qu||p(fu(ui

k), fu)||2
2

(cid:17)

(9)

1, . . . , yi

Here k represents time step of the prediction horizon N , and i is the batch index of n
sampled trajectories. The ﬁrst term represents the trajectory tracking loss deﬁned as the two
norm over a vector of residuals between the true Ytrue = {ytrue,i
} and predicted
Y = {yi
N } output trajectories over N steps. The second term is a regularization for
smoothing the trajectories by penalizing the one-time step diﬀerence between the successive
hidden states x. The third and fourth terms impose constraints on the outputs via penalties.
In this work, we employ penalty functions for time-varying lower and upper bounds y
, yk,
respectively, given as follows:

, . . . , ytrue,i

N

k

1

p(yk, y
) = max(0, −yk + y
k
p(yk, yk) = max(0, yk − yk)

k

)

(10a)

(10b)

Furthermore we can leverage the structure of the proposed block neural state-space model (8)
and impose similar constraints on the inﬂuence of the control input dynamics components
fu(uk) as given in the last two terms of the loss function (9).

Remark. Penalty functions in the form (10) can be straightforwardly implemented in
modern deep-learning libraries such as Pytorch or Tensorﬂow using standard ReLU activation
functions.

3.2. Constrained Diﬀerentiable Predictive Control

The objective is to learn the constrained diﬀerentiable predictive control (DPC) policy to
govern the unknown dynamical system (1), given the learned neural state-space model (8).
We formulate DPC problem as following generic parametric optimal control problem:

LDPC = min
Θ

N −1
(cid:88)

k=0

(cid:96)DPC(yk, uk, ξk)

s.t. xk+1 = fx(xk) + fu(uk), k ∈ NN −1

0

yk = fy(xk)
uk = πΘ(ξk)
xk ∈ X
uk ∈ U
ξk ∈ Ξctrl ⊂ Rn

(11a)

(11b)

(11c)

(11d)

(11e)

(11f)

(11g)

Where Θ are decision variables representing weights and biases of the neural control policy
πΘ(ξk), and (cid:96)DPC(yk, uk) represents MPC-inspired loss function, e.g. reference tracking
with constraints violation penalties. In the following paragraphs we elaborate on the design
choices of DPC problem (11) implemented and solved as constrained deep learning problem.

9

Neural control policy. The control parameters ξ sampled from the set Ξctrl represent a
design choice that captures all decision relevant signals of the control problem. For instance,
we can deﬁne ξk = xk as a full state feedback policy. However, in general the neural
control policy can take a vector of any relevant control parameters. In this paper we use
ξk = (cid:2)Y(cid:124)

(cid:124)(cid:3) with a neural control policy map is given as:

p R(cid:124) Y(cid:124) Y

U = πΘ(ξk)

(12)

Where U = {u1, . . . , uN } is an optimal control trajectory, Yp = {y1−N , . . . , y0} represents
observed output trajectories N -steps into the past, R = {r1, . . . , rN }, is a tensor of reference
}, and Y = {y1, . . . , yN } are tensors of imposed lower
trajectories, while Y = {y
and upper bounds for future output trajectories, respectively. In this paper, we assume
πΘ(ξk) : Rm → Rn to be a fully connected neural network architecture with l ∈ NL
1 layers
given as:

, . . . , y

N

1

πΘ(ξk) = WLhL + bL

hl = σ(Wl−1hl−1 + bl−1)
h0 = ξ

(13a)

(13b)

(13c)

parametrized by Θ = {Wl, bl, |∀l ∈ NL
1 } with weights Wl and biases bl, and nonlinear
activation function σ : Rnh → Rnh . In DPC, the neural control policy (12) replaces the
PWA control law of explicit MPC described in Section 2.3, thus ξ in DPC the neural policy
represents an expanded parametric space compared to the lower-dimensional parametric
space of explicit MPC given as eq. (4).

Diﬀerentiable closed-loop system architecture. To train the constrained control policy (12),
we design the neural representation of the closed-loop dynamics using the learned neural
state-space model f N
θ

(8):

U = πΘ(ξk)
Yf = f N

θ (Yp, U)

(14a)

(14b)

The system dynamics model f N
(8) is used to predict N -steps ahead future output
θ
trajectories Yf , given the control actions trajectories U generated by the neural control
policy (12).

The corresponding neural network architecture is shown in Fig. 4. Here the closed-
loop model is constructed by connecting the learned system dynamics model (8) with the
control policy (12), through the control actions trajectories U. Hence the proposed control
policy represents a predictive control strategy with a preview of future constraints and
reference signals. The policy is optimized by diﬀerentiating the closed-loop neural dynamics
model on sampled past output trajectories Yp, and given the forecast of lower and upper
constraint trajectories Y, and Y. The parameters of pre-trained neural state-space model (8)
representing the open-loop system dynamics are ﬁxed during the policy optimization. The
distribution of past output trajectories Yp represents a sampling of initial conditions, while
the distribution of time-varying constraints and reference signals represent a sampling of
diﬀerent operational scenarios and tasks.

MPC-inspired loss function. The closed-loop system parametrization (14) now allows us to
simulate the eﬀect of varying control features ξ on the system’s output dynamics Yf . This

10

Figure 4: Diﬀerentiable predictive control (DPC) architecture. Here y represents controlled outputs of the
system, y and y represent lower and upper output constraints, r are sampled reference trajectories, and u
are control actions generated by the neural policy optimized with MPC-inspired loss function.

11

ui1, ui2, ... , uiNyi1-Nyi2-Nyi0...SSMfxfufyNfox0Policyyi1yi2yiN...yi1yi2yiN...yi1yi2yiN...ri1, ri2, ... , riNSynthetic dataset MPC-inspired loss-yryyryyysimulation capability, together with diﬀerentiability of the closed-loop model (14) is a key
feature of the proposed control method which allows us to perform data-driven optimization
of neural control policy (12). We train the policy parameters by sampling the distribution of
the control features ξ, and backpropagating the gradients of the loss function through the
closed-loop system model. In particular, we leverage the following MPC-inspired multi-term
loss function. The primary reference tracking term is augmented with control smoothing
and penalty terms (10) imposed on control actions and output trajectories:

LDPC(R, Y, Y, Y, U, U|Θ) =

1
nN

n
(cid:88)

N
(cid:88)

(cid:16)

i=1

k=1

Qr||ri

k − yi

k||2

2 + Qdu||ui

k − ui

k−1||2

2+

(15)

Qy||p(yi

Qu||p(ui

)||2

k, yi
k
k)||2
k, ui

2 + Qy||p(yi
2 + Qu||p(ui

k, yi
k, ui

k)||2
2+
(cid:17)
k)||2
2

Where k represents time index, N is the prediction horizon, i is the batch index and n is
the number of batches of sampled trajectories, respectively. R represent sampled reference
trajectories to be tracked by the output trajectories of the closed-loop system Y, where
Qr is reference tracking weight. The second term weighted by Qdu represents the control
action smoothing. Similar to Y, and Y, U, and U are tensors of lower and upper bounds for
the N -step ahead control action trajectories. We optimize the policy parameters Θ, while
keeping the parameters of the dynamics model (8) ﬁxed. The penalty terms are weighted by
terms Qy, and Qu for output and input constraints, respectively.

Remark. The proposed control policy optimization procedure does not require an interaction
with the real system or its emulator model. Instead, the policy is trained by sampling the
closed-loop system using the trained dynamics model. Moreover, the training is extremely
data-eﬃcient as all sampled trajectories can be generated synthetically.

4. Experimental Case Study

4.1. System Description

The presented control approaches are implemented on a laboratory device called FlexyAir1.
The device is a single-input single-output system, where the actuator is a fan that drives air
into a vertical tube with a ﬂoater inside. An infrared proximity sensor is placed on the top
of the tube, which measures the ﬂoater’s level. The manipulated variable in this laboratory
process is a fan speed command, given to an internal fan speed controller, which sets the
corresponding current for the fan itself. Next, the process variable is the position of the
ﬂoater in the vertical tube. The control objective is to stabilize the ﬂoater’s vertical distance
at the desired reference level, while satisfying given constraints. The sketch of the laboratory
device is shown on the Fig. 5.

4.2. Dataset
System identiﬁcation. Our experimental dataset is obtained by observing the real system
dynamics with sampling time Ts = 0.25 seconds. The measured input-output time series in

1www.ocl.sk/flexyair

12

Figure 5: Sketch of the laboratory device with the Raspberry-Pi platform.

the form (2) has m = 9 · 103 datapoints which are used to create training, validation, and
test sets with equal lengths of 1000 samples. To take into account time horizons during the
training, we apply N -step time shift to generate the past Ytrue
and future Ytrue tensors
for output variables, respectively. The time series in each set are subsequently separated to
N -step batches, generating tensors with dimensions (N, n, ny), where n represents number
of batches, and ny is the dimension of the variable y (the same applies for u). The number
of batches n = m
N depends on the total number of datapoints m and length of the prediction
horizon N .

p

Closed-loop control. As mentioned in Section 3.2, the control policy training is based on a
sampling of the input sequences of the closed-loop dynamics model and does not require
extra measurements of the real system. To demonstrate the data-eﬃciency, we generate each
continuous time series with only 3 · 103 samples for train, validation, and test set, respectively.
We apply the same N -step horizon batching as in the case of system identiﬁcation task. The
dataset is also normalized using min-max normalization.

The past observations of the output trajectories Yp are randomly sampled continuous
trajectories, while predicted future trajectories Y are internally generated by the trained
system dynamics model. To improve generalization across dynamic modes, we assume
that sampled trajectories Yp are dynamically generated sine waves with varying frequency,
amplitude, and noise at each optimization epoch. The time-varying references and constraints
bounds can be arbitrarily sampled from user-deﬁned distribution to generalize the control
across a set of tasks.

In our case, we sample sine waves for the output reference R with amplitude in range

13

3.5mmoutCompositeVideo+audioRaspberry Pi Model B+ V1.2(C)Raspberry Pi 2014EthernetRJ452x USB 2.0HDMIMicroUSBPower inCPU/GPUBroadcomBCM2835512MB SDRAMCameraCSIDisplayDSIEthernetcontrollerLAN95144x USB +StatusLED'sACTPWR2x USB 2.0EthernetmicroSDslot40pins: 28x GPIO, I2C, SPI, UARTRegulatorpolarity protectioncurrent3.3V&1.8V14 polesHDMI outjackRUNonbottomsidelimiterpowergoodinternalcontrollercontrol actionby eMPC/DPCRPMmeasureddistanceinfraredsensorfloaterRaspberryPiplatform[0.2, 0.8], the lower bound Y in range [0.1, 0.4], and the upper bound Y in range [0.6, 0.9].
The control action bounds can be in principle time-varying as well, however, in our case due
to the nature of the experimental setup, we assume static constraints U = 0.0 and U = 1.0

Remark. Training of the control policy on dynamically sampled system output trajectories
Yp with varying frequencies and amplitudes is inspired by the fact that system response
can be decomposed to a set of dynamic modes with ﬁxed frequencies [77]. Alternatively, the
trajectories Yp could be generated by perturbing the learned system dynamics or simply
represented by observations of the real system.

4.3. Metrics

We assess trained DPC performance on two sets of metrics, the ﬁrst one for training and
hyperparameter selection, the second for task-speciﬁc performance evaluation. For training,
we evaluate the mean squared error (MSE) of system identiﬁcation loss (9) and policy learning
loss (15), respectively. The loss function MSE evaluated on development sets are used for
hyperparameter selection, while MSE on test sets are used for performance assessment of the
training process. Second, instead of training-oriented processed datasets, we deﬁne the task-
speciﬁc metrics using the real system data with T time steps. For the system identiﬁcation,
we evaluate MSE of the open-loop response of the trained model compared to the response
of the real system given as: 1
||2
2. For evaluation of the closed-loop control
T
performance, we compute the reference tracking MSE as 1
2, and integral of
T
the absolute error (IAE) as (cid:80)T
k=1 |yk − rk|. For constraints satisfaction, we evaluate mean
)| + |p(yk, yk)|(cid:1).
(cid:80)T
absolute (MA) value of the output constraints violations: 1
T

k=1 ||yk − ytrue

k=1 ||yk − rk||2

(cid:0)|p(yk, y

(cid:80)T

(cid:80)T

k=1

k

k

4.4. Optimization, and Hyperparameters Selection

The presented method with structured neural network models was implemented using
Pytorch [78]. We train our models with randomly initialized weights using Adam optimizer
[79] with a learning rate of 0.001. All neural network blocks in our models are designed
with GELU activation functions [80]. We use a grid search for ﬁnding the best performing
hyperparameters by assessing the performance of the trained models on the development set
and task performance metrics speciﬁed in section 4.3. For both the dynamics model and
control policy, we select the prediction horizon N = 32 steps, which with sampling time
Ts = 0.25 seconds, corresponds to the 8 seconds time window.

System identiﬁcation. The system dynamics model (8) is trained using the system identi-
ﬁcation dataset on 1000 epochs. The state transition block fx : R30 → R30 and the input
dynamics block fu : R1 → R30 are represented by residual neural networks, while output
decoder fy : R30 → R1 is a simple linear map. The state encoder map fo : R1 → R30
is represented by a standard, fully connected neural network. For simplicity, we assume
only one step time lag for the state encoder. All individual neural block components are
designed with 4 hidden layers and 30 hidden neurons. The resulting neural state-space model
has nθ = 24661 trainable parameters. The weight factors of the system identiﬁcation loss
function (9) are given as follows: Qdx = 0.2, Qy = 1.0, Qu = 1.0. The trained block neural
state-space model (8) is subsequently used for the design of the constrained diﬀerentiable
control policy, as described in section 3.2.

14

Closed-loop control. The constrained diﬀerentiable control policy (12) is trained using
synthetically sampled closed-loop system dataset on 5 · 103 epochs with early stopping based
on development set MSE2. The policy map πΘ : R128 → R32 is represented by fully connected
neural network with 3 layers each with 20 hidden neurons, resulting in nΘ = 7272 trainable
parameters. The weight factors of the constrained control loss function (15) are Qr = 1.0,
Qdu = 0.1, Qy = 2.0, Qu = 10.0.

In case of the explicit model predictive control (eMPC) strategy (3a), we chose to set
the length of the prediction horizon to N = 5, while the tuning factors were set to Qr = 3,
and Qdu = 4. Because, classical MPC can not handle neural state-space models, the
corresponding quadratic optimization problem (3) is constructed using a simpliﬁed linear
model obtained from the System Identiﬁcation toolbox in Matlab [81].

4.5. Real-time Closed-loop Control Performance

This section shows the results of the real-time implementation of two control strategies.
First, we present a step-change scenario to show the performance concerning transient
behavior and reference tracking. Secondly, we introduce an experimental case study, where
a harmonic reference alongside a set of harmonic constraints is considered. Here, we show
how the proposed diﬀerentiable predictive control (DPC) handles constraint satisfaction.
All control strategies for these real-time experiments are implemented using an embedded
platform running on Raspberry-Pi 3 using Python 3.7.

Step-change scenario. This scenario presents the tracking performance when a step change
occurs in the process variable. Speciﬁcally, we consider a step changes in the reference from
20 → 15 → 28cm for the ﬂoater position. Moreover, we set the constraints to y = 30cm and
bottom y = 13cm. Here, we compare the performance of 2 controllers. First, we set the
baseline with the explicit MPC strategy. Then we include the proposed DPC policy. The
tuning factors of individual controllers are mentioned in the section 4.4.

The control performance of respective policies is visualized on Fig. 6. Furthermore,
rigorous validation of measured performances is reported in the Table 1. We can see that
both eMPC and DPC follow the trajectory and respect bottom and top constraints from
reported results. We can observe that eMPC and DPC react to the reference change in the
same fashion (slope of the transient behavior). The diﬀerence is just the time instant when
the change occurs. This is determined by the length of the prediction horizon of individual
policies. While for the DPC the length of the horizon is not a bottleneck (32 samples in
this case), the eMPC can not be constructed for longer horizons than 10 in this case we use
N = 5 due to the memory footprint..

Table 1: Qualitative evaluation of control performance, based on metrics given in 4.3.

reference tracking
MSE

IAE

constraints violation
MSE

MAE

DPC
1.76
eMPC 8.97

265
526

0
0

0
0

2Using early stopping the DPC policy training typically converged using less than 1000 epochs.

15

Finally, we direct the reader to the table 1, where a quantitative numerical evaluation is
reported. Here we followed the metrics established in section 4.3. In terms of MSE criterion,
the DPC performs almost 8 times better than the explicit MPC. We also include the IAE
criterion, which is more common in the control community, and in this case, the DPC
outperforms the eMPC by more than 50%. In terms of constraints violations, these criteria
read to 0 since neither DPC and eMPC cross the limits.

(a) Position measurements.

(b) Proﬁle of the manipulated variable.

Figure 6: Real-time measurement proﬁles of DPC and eMPC control strategies with static constraints. Top
ﬁgure shows the reference (red-dashed), explicit model predictive control (purple), constrained diﬀerentiable
predictive control (blue) and constraints (black-dashed). Bottom ﬁgure shows the fan speeds of respective
controllers.

Constraints Satisfaction. The second scenario involves the demonstration of systematic
constraint handling by the DPC strategy. Here we utilize a harmonic reference and harmonic
constraints. Concrete results are presented in the Fig. 7. Note, that the constraints are not
violated even if the reference crosses out of allowed space, thus empirically demonstrating
remarkable robustness capabilities even in this challenging scenario with high measurement
noise.

16

0306090time [s]15202530position [cm]0306090time [s]33343536fan speed [%](a) Position measurements.

(b) Proﬁle of the manipulated variable.

Figure 7: Real-time measurement proﬁle of proposed DPC strategy under the inﬂuence of harmonic reference
and constraints. Top ﬁgure shows the reference signal (red-dashed) and measured ﬂoater position (blue).
Bottom ﬁgure shows the proﬁle of the fan speed.

4.6. Idealized Simulation Case Studies

The purpose of this section is to demonstrate control capabilities of the proposed con-
strained diﬀerentiable predictive control policies (DPC) in idealized simulations. We use the
trained system dynamics model for representing the controlled system, omitting the inﬂuence
of the plant-model mismatch and additive real-time disturbances aﬀecting the dynamics
of the real system. We show that, in case of perfect system dynamics model, DPC can
achieve oﬀset-free reference tracking capabilities, and robust constraints satisfaction, hence
empirically demonstrating convergence to near optimal control performance.

Reference Tracking. Fig. 8 shows the oﬀset-free reference tracking capabilities of the trained
DPC method assessed using four diﬀerent dynamic signals. Besides tracking an arbitrary
dynamic reference, DPC policy demonstrates predictive control capabilities using a reference
preview.
In particular, notice the change in the trajectories several time steps before
previewed step changes in the reference signals. To be able to react safely and in advance
of forecasted parameters such as references and constraints is a desired feature for many

17

050100150200time [s]0102030position [cm]050100150200time [s]343638fan speed [%]Figure 8: Simulated closed-loop control trajectories demonstrating reference tracking capabilities of DPC.

Figure 9: Simulated closed-loop control trajectories demonstrating balancing conﬂicting objectives with
dynamic reference and dynamic constraints.

industrial control applications. The preview capability also represents additional value
compared to explicit MPC, which can not handle preview of its parameters.

Constraints Satisfaction. We demonstrate the capability of DPC to balance conﬂicting
objectives in terms of reference tracking and constraints handling. Fig. 9 plots DPC
performance with dynamic reference crossing dynamic constraints. Even in this challenging
scenario, the trained DPC policy satisﬁes the constraints while compromising on the tracking
performance of the unattainable reference.

4.7. Scalability Analysis

In Table 2 we compare the scalability of the proposed diﬀerentiable predictive control
(DPC) against explicit model predictive control (eMPC) in terms of on-line computational
requirements, memory footprint, policy complexity, and oﬀ-line construction time with the
increasing length of the prediction horizon N .

We compare the mean and maximum online computational (CPU) time and memory
footprint required to evaluate and store the DPC and eMPC control policies. As shown
in Table 2, the online evaluation of both control policies is extremely fast in terms of

18

0125250375500625time [s]0.20.30.40.50.60.70.8position [cm]0125250375500625time [s]0.20.30.40.50.60.70.8position [cm]0125250375500625time [s]0.20.30.40.50.60.70.8position [cm]0125250375500625time [s]0.20.30.40.50.60.70.8position [cm]0125250375500625time [s]0.20.40.60.8position [cm]Table 2: Scalability analysis of the proposed diﬀerentiable predictive control (DPC) policy against explicit
MPC (eMPC).

N

5

7

10

12

15

DPC
eMPC

DPC
eMPC

DPC
eMPC

DPC

eMPC

mean CPU time [ms]
0.371
0.355
0.369
0.429
0.472
0.455

max CPU time [ms]
7.945
7.978
6.978
4.684
1.927
1.325

memory footprint [kB]
13
611

17
65200

15
9300

0.380
-

0.502
-

8.066
-

5.026
-

19
-

21
-

number of policy parameters
3252
1845

2247

2850

number of policy regions
108

1420

347

2631

3855

5333

DPCab
eMPCc

construction time [h]
0.1
0.1
4.0
0.1

0.1
66.5

0.2
-

0.2
-

aTrained on 1000 epochs without early stopping.
bComputed on Core i7 2.6GHz CPU with 16GB RAM.
cComputed on Core i7 4.0GHz CPU with 32GB RAM.

CPU time. This property is crucial for controlling fast and highly dynamical systems with
high-frequency sampling rates, such as UAVs or agile robotic systems. However, memory
requirements are signiﬁcantly diﬀerent for linearly scalable DPC compared to exponentially
growing requirements of eMPC. In the case of eMPC, its enormous memory demands
limit the applicability of this control strategy only to small scale systems with very low
prediction horizons. Contrary to eMPC, DPC policies have an extremely low memory
footprint regardless of the prediction horizon’s length, opening the doors to large-scale
practical control applications.

To deeper understand the reported memory requirements, we evaluate the complexity of
the control policies in terms of the number of parameters for DPC and the number of critical
regions for eMPC. Table 2 shows that the number of parameters of the DPC scale linearly
with the increasing prediction horizon N , while a number of critical regions of eMPC scales
exponentially. The reason behind this is that complexity of eMPC policy (3.2) is primarily
given by the number of constraints that scale exponentially with the length of the prediction
horizon and the number of optimized variables. On the other hand, the complexity of DPC
policy depends mainly on the number of hidden nodes and the number of layers, which
allows it to scale to large state-action spaces with long prediction horizons.

Table 2 also reports the construction time for the eMPC policy using multiparametric
programming [66, 82], to give the reader notions about the limitations of this solution
method. Speciﬁcally, we direct the attention to the construction time of the eMPC with

19

N = 10 for which the generation of the associated controller took almost 3−days. Contrarily,
the construction time of DPC scales linearly to a large prediction horizon, as reported in
Table 2. Here we demonstrate a tremendous potential of the proposed DPC policies to scale
up to large-scale control problems way beyond the reach of classical eMPC.

5. Limitations and Future Work

Despite experimentally demonstrated robust performance, the proposed DPC method as
described in this paper does not provide theoretical guarantees on the constraints satisfaction
or closed-loop system stability. From a theoretical perspective, structural similarity of the
underlying optimization problem to well studied MPC problem presents opportunities for
further extensions of the proposed DPC methodology. Some examples include adaptive
control via online updates of the nonlinear system model, robust and stochastic control via
constraints tightening, or closed-loop stability guarantees based on stability of MPC [83,
84, 85]. Additional extensions include oﬀset-free formulation with augmented state space
model and control error integrator dynamics recently proposed for neural network-based
controllers [86]. From the deep learning perspective, methods such as [55] can be employed to
obtain hard constraints satisfaction guarantees on states and control actions. From the system
identiﬁcation perspective, various neural architectures might be more or less suitable to
parametrize the proposed neural state space models, depending on the underlying dynamical
system. Straightforward model architecture extensions of the proposed methodology may
include a whole range of diﬀerentiable models, for instance, sparse identiﬁcation of nonlinear
dynamics (SINDy) [87], multistep neural networks [88], or graph neural networks [89].

6. Conclusions

We have experimentally demonstrated that it is possible to train constrained optimal
control policies purely based on the observations of the dynamics of the unknown nonlinear
system. The principle is based on optimizing control policies with constraints penalty
functions by diﬀerentiating trained neural state-space models representing an internal model
of the observed nonlinear system. We denote this control approach constrained diﬀerentiable
predictive control (DPC).

We compare the performance of trained DPC policy against classical explicit model
predictive control (eMPC). The control algorithms are implemented on a laboratory device
with the Raspberry-Pi platform. In comparison with eMPC using a linear model, DPC
achieves better control performance due to the nonlinear system dynamics model and
the reference and constraints preview capability. However, most importantly, DPC has
unprecedented scalability beyond the limitations of eMPC. DPC scales well with increased
problem complexity deﬁned by the length of the prediction horizon resulting in an increased
number of decision variables and constraints of the underlying optimization problem. DPC
demonstrates linear scalability in terms of memory footprint, number of policy parameters,
and required construction time. Therefore, we believe that the proposed DPC method has
the potential for wide adoption in large-scale control systems with limited computational
resources and fast sampling rates.

20

7. Acknowledgements

This research was supported by the Mathematics for Artiﬁcial Reasoning in Science
(MARS) initiative via the Laboratory Directed Research and Development (LDRD) invest-
ments at Paciﬁc Northwest National Laboratory (PNNL). PNNL is a multi-program national
laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute
under Contract No. DE-AC05-76RL0-1830.

K. Kiˇs and M. Klauˇco gratefully acknowledge the contribution of the Scientiﬁc Grant

Agency of the Slovak Republic under the grants 1/0585/19 and 1/0545/20.

References

[1] X. Zhang, M. Bujarbaruah, and F. Borrelli, “Near-Optimal Rapid MPC Using Neural
Networks: A Primal-Dual Policy Learning Framework,” IEEE Transactions on Control
Systems Technology, pp. 1–13, 2020.

[2] S. Lucia and B. Karg, “A deep learning-based approach to robust nonlinear model
predictive control,” IFAC-PapersOnLine, vol. 51, no. 20, pp. 511 – 516, 2018, 6th IFAC
Conference on Nonlinear Model Predictive Control NMPC 2018. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S2405896318326958

[3] S. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J. Pappas, and M. Morari,
“Approximating explicit model predictive control using constrained neural networks,” in
2018 Annual American Control Conference (ACC), 2018, pp. 1520–1527.

[4] E. T. Maddalena, C. G. da S. Moraes, G. Waltrich, and C. N. Jones, “A neural network

architecture to learn explicit mpc controllers from data,” 2019.

[5] M. Hertneck, J. K¨ohler, S. Trimpe, and F. Allg¨ower, “Learning an approximate model
predictive controller with guarantees,” IEEE Control Systems Letters, vol. 2, no. 3, pp.
543–548, 2018.

[6] S. Lucia, D. Navarro, B. Karg, H. Sarnago, and O. Luc´ıa, “Deep learning-based model
predictive control for resonant power converters,” IEEE Transactions on Industrial
Informatics, vol. 17, no. 1, pp. 409–420, 2021.

[7] Y. Lohr, M. Klauˇco, M. Fikar, and M. M¨onnigmann, “Machine learning assisted solutions
of mixed integer mpc on embedded platforms,” in Preprints of the 21st IFAC World
Congress (Virtual), Berlin, Germany, July 12-17, 2020, vol. 21, July 12-17, 2020 2020.
[Online]. Available: https://www.uiam.sk/assets/publication info.php?id pub=2192

[8] J. Drgoˇna, D. Picard, M. Kvasnica, and L. Helsen, “Approximate model predictive
building control via machine learning,” Applied Energy, vol. 218, pp. 199 –
216, 2018.
[Online]. Available: http://www.sciencedirect.com/science/article/pii/
S0306261918302903

[9] U. Rosolia, X. Zhang, and F. Borrelli, “Data-driven predictive control

for
autonomous systems,” Annual Review of Control, Robotics, and Autonomous
Systems,
https:
//doi.org/10.1146/annurev-control-060117-105215

vol. 1, no. 1, pp. 259–286,

[Online]. Available:

2018.

21

[10] J. Drgona, A. Tuor, and D. Vrabie, “Constrained physics-informed deep learning for
stable system identiﬁcation and control of unknown linear systems,” vol. abs/2004.11184,
2020.

[11] S. Br¨uggemann and C. Possieri, “On the use of diﬀerence of log-sum-exp neural networks
to solve data-driven model predictive control tracking problems,” IEEE Control Systems
Letters, vol. 5, no. 4, pp. 1267–1272, 2021.

[12] E. N. Pistikopoulos, V. Dua, N. A. Bozinis, A. Bemporad, and M. Morari,
“On-line optimization via oﬀ-line parametric optimization tools,” Computers &
[Online]. Available:
Chemical Engineering, vol. 26, no. 2, pp. 175 – 185, 2002.
http://www.sciencedirect.com/science/article/pii/S0098135401007396

[13] A. Bemporad, M. Morari, V. Dua, and E. N. Pistikopoulos, “The explicit linear quadratic
regulator for constrained systems,” Automatica, vol. 38, no. 1, pp. 3–20, 2002. [Online].
Available: https://www.sciencedirect.com/science/article/pii/S0005109801001741

[14] D. Tavernini, M. Metzler, P. Gruber, and A. Sorniotti, “Explicit nonlinear model
predictive control for electric vehicle traction control,” IEEE Transactions on Control
Systems Technology, vol. 27, no. 4, pp. 1438–1451, 2019.

[15] R. Oberdieck, N. A. Diangelakis, and E. N. Pistikopoulos, “Explicit model predictive
control: A connected-graph approach,” Automatica, vol. 76, pp. 103–112, 2017. [Online].
Available: https://www.sciencedirect.com/science/article/pii/S0005109816303971

[16] M. Kvasnica, J. Hled´ık, I. Rauov´a, and M. Fikar, “Complexity reduction of explicit
model predictive control via separation,” Automatica, vol. 49, no. 6, pp. 1776–1781, 2013.
[Online]. Available: https://www.uiam.sk/assets/publication info.php?id pub=1378

[17] F. Scibilia, S. Olaru, and M. Hovd, “Approximate explicit linear mpc via delaunay
tessellation,” in 2009 European Control Conference (ECC), 2009, pp. 2833–2838.

[18] M. Kvasnica, J. L¨ofberg, and M. Fikar, “Stabilizing polynomial approximation of
explicit mpc,” Automatica, vol. 47, no. 10, pp. 2292–2297, 2011. [Online]. Available:
https://www.uiam.sk/assets/publication info.php?id pub=1171

[19] M. Kvasnica and M. Fikar, “Clipping-based complexity reduction in explicit mpc,”
IEEE Transactions On Automatic Control, vol. 57, no. 7, pp. 1878–1883, July 2012.

[20] M. Kvasnica, B. Tak´acs, J. Holaza, and S. Di Cairano, “On region-free explicit model
predictive control,” in 54rd IEEE Conference on Decision and Control, vol. 54, Osaka,
Japan, December 15-18, 2015 2015, pp. 3669–3674.

[21] J. Drgoˇna, M. Klauˇco, F. Janeˇcek, and M. Kvasnica, “Optimal control of a labora-
tory binary distillation column via regionless explicit mpc,” Computers & Chemical
Engineering, pp. 139–148, 2017.

[22] M. Kvasnica, P. Bakar´aˇc, and M. Klauˇco, “Complexity reduction in explicit mpc: A
reachability approach,” Systems & Control Letters, vol. 124, pp. 19–26, 2019. [Online].
Available: https://www.uiam.sk/assets/publication info.php?id pub=1980

22

[23] S. Hovland and J. T. Gravdahl,

through model
7711–7716,
//www.sciencedirect.com/science/article/pii/S1474667016401874

“Complexity reduction in explicit mpc
reduction,” IFAC Proceedings Volumes, vol. 41, no. 2, pp.
https:

17th IFAC World Congress.

[Online]. Available:

2008,

[24] N. A. Nguyen, M. Gulan, S. Olaru, and P. Rodriguez-Ayerbe, “Convex lifting: Theory
and control applications,” IEEE Transactions on Automatic Control, vol. 63, no. 5, pp.
1243–1258, 2018.

[25] C. N. Jones and M. Morari, “Polytopic approximation of explicit model predictive
controllers,” IEEE Transactions on Automatic Control, vol. 55, no. 11, pp. 2542–2553,
2010.

[26] I. Pappas, N. A. Diangelakis, and E. N. Pistikopoulos, “Multiparametric/explicit
for quadratically constrained problems,”
[Online]. Available:

nonlinear model predictive
Journal of Process Control, vol. 103, pp. 55–66, 2021.
https://www.sciencedirect.com/science/article/pii/S0959152421000706

control

[27] T. Johansen, “On multi-parametric nonlinear programming and explicit nonlinear model
predictive control,” in Proceedings of the 41st IEEE Conference on Decision and Control,
2002., vol. 3, 2002, pp. 2768–2773 vol.3.

[28] P. Petsagkourakis and C. Theodoropoulos, “Data driven reduced order nonlinear
multiparametric mpc for large scale systems,” in 28th European Symposium
ser. Computer Aided Chemical
on Computer Aided Process Engineering,
Engineering, A. Friedl, J. J. Klemeˇs, S. Radl, P. S. Varbanov, and T. Wallek,
Eds.
https:
//www.sciencedirect.com/science/article/pii/B9780444642356502175

2018, vol. 43, pp. 1249–1254.

[Online]. Available:

Elsevier,

[29] J. Katz, I. Pappas, S. Avraamidou, and E. N. Pistikopoulos, “Integrating deep learning
models and multiparametric programming,” Computers & Chemical Engineering, vol.
136, p. 106801, 2020. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0098135419311378

[30] I. Pappas, D. Kenefake, B. Burnak, S. Avraamidou, H. Ganesh, J. Katz, N. A. Di-
angelakis, and E. Pistikopoulos, “Multiparametric programming in process systems
engineering: Recent developments and path forward,” in Frontiers in Chemical Engi-
neering, 2020.

[31] B. Karg and S. Lucia, “Approximate moving horizon estimation and robust nonlinear
model predictive control via deep learning,” Computers & Chemical Engineering, vol.
148, p. 107266, 2021. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0098135421000442

[32] K. Kiˇs and M. Klauˇco, “Neural network based explicit mpc for chemical reactor
control,” Acta Chimica Slovaca, vol. 12, no. 2, pp. 218–223, 2019. [Online]. Available:
https://www.uiam.sk/assets/publication info.php?id pub=2115

[33] I. Mordatch and E. Todorov, “Combining the beneﬁts of function approximation and

trajectory optimization,” in In Robotics: Science and Systems (RSS, 2014.

23

[34] T. Zhang, G. Kahn, S. Levine, and P. Abbeel, “Learning deep control policies for
autonomous aerial vehicles with MPC-guided policy search,” CoRR, vol. abs/1509.06791,
2015. [Online]. Available: http://arxiv.org/abs/1509.06791

[35] S. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J. Pappas, and M. Morari,
“Approximating explicit model predictive control using constrained neural networks,” in
2018 Annual American Control Conference (ACC), June 2018, pp. 1520–1527.

[36] P. L. Donti, M. Roderick, M. Fazlyab, and J. Z. Kolter, “Enforcing robust control
guarantees within neural network policies,” in The Ninth International Conference on
Learning Representations (ICLR), 2021.

[37] L. Hewing, J. Kabzan, and M. N. Zeilinger, “Cautious model predictive control using
gaussian process regression,” IEEE Transactions on Control Systems Technology, vol. 28,
no. 6, pp. 2736–2743, 2020.

[38] I. Lenz, R. A. Knepper, and A. Saxena, “Deepmpc: Learning deep latent features for

model predictive control,” in Robotics: Science and Systems, 2015.

[39] K. Bieker, S. Peitz, S. L. Brunton, J. N. Kutz, and M. Dellnitz, “Deep model predictive
control with online learning for complex physical systems,” CoRR, vol. abs/1905.10094,
2019. [Online]. Available: http://arxiv.org/abs/1905.10094

[40] A. Broad, I. Abraham, T. D. Murphey, and B. D. Argall, “Structured neural
network dynamics for model-based control,” CoRR, vol. abs/1808.01184, 2018. [Online].
Available: http://arxiv.org/abs/1808.01184

[41] Y. Chen, Y. Shi, and B. Zhang, “Optimal control via neural networks: A convex

approach,” 2018.

[42] Y. Li, J. Wu, J.-Y. Zhu, J. B. Tenenbaum, A. Torralba, and R. Tedrake, “Propagation

networks for model-based control under partial observation,” in ICRA, 2019.

[43] Y.-C. Chang, N. Roohi, and S. Gao, “Neural lyapunov control,” in Advances in
Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc., 2019, pp. 3245–
3254. [Online]. Available: http://papers.nips.cc/paper/8587-neural-lyapunov-control.
pdf

[44] J. A. E. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and M. Diehl, “Casadi: a
software framework for nonlinear optimization and optimal control,” Mathematical
Programming Computation, vol. 11, no. 1, pp. 1–36, Mar 2019. [Online]. Available:
https://doi.org/10.1007/s12532-018-0139-4

[45] B. Amos, L. Xu, and J. Z. Kolter, “Input convex neural networks,” CoRR, vol.

abs/1609.07152, 2016. [Online]. Available: http://arxiv.org/abs/1609.07152

[46] F. de Avila Belbute-Peres, K. Smith, K. Allen, J. Tenenbaum, and J. Z. Kolter, “End-to-
end diﬀerentiable physics for learning and control,” in Advances in Neural Information
Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, pp. 7178–7189.

24

[47] J. Degrave, M. Hermans, J. Dambre, and F. Wyﬀels, “A diﬀerentiable physics engine
for deep learning in robotics,” CoRR, vol. abs/1611.01652, 2016. [Online]. Available:
http://arxiv.org/abs/1611.01652

[48] B. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter, “Diﬀerentiable
MPC for end-to-end planning and control,” CoRR, vol. abs/1810.13400, 2018. [Online].
Available: http://arxiv.org/abs/1810.13400

[49] T. Yang, “Advancing non-convex and constrained learning: Challenges and
opportunities,” AI Matters, vol. 5, no. 3, p. 29–39, Dec. 2019. [Online]. Available:
https://doi.org/10.1145/3362077.3362085

[50] D. Pathak, P. Kr¨ahenb¨uhl, and T. Darrell, “Constrained convolutional neural networks
for weakly supervised segmentation,” CoRR, vol. abs/1506.03648, 2015. [Online].
Available: http://arxiv.org/abs/1506.03648

[51] Z. Jia, X. Huang, E. I. Chang, and Y. Xu, “Constrained deep weak supervision for
histopathology image segmentation,” IEEE Transactions on Medical Imaging, vol. 36,
no. 11, pp. 2376–2388, 2017.

[52] C. K. Goh, Y. Liu, and A. W. K. Kong, “A constrained deep neural network for
ordinal regression,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2018, pp. 831–839.

[53] P. M´arquez-Neila, M. Salzmann, and P. Fua, “Imposing hard constraints on deep
networks: Promises and limitations,” CoRR, vol. abs/1706.02025, 2017. [Online].
Available: http://arxiv.org/abs/1706.02025

[54] H. Kervadec, J. Dolz, J. Yuan, C. Desrosiers, E. Granger, and I. B. Ayed,
“Log-barrier constrained cnns,” CoRR, vol. abs/1904.04205, 2019. [Online]. Available:
http://arxiv.org/abs/1904.04205

[55] P. Donti, D. Rolnick, and J. Z. Kolter, “Dc3: A learning method for optimization with
hard constraints,” in International Conference on Learning Representations, 2021.

[56] Y. Liu, C. Su, H. Li, and R. Lu, “Barrier function-based adaptive control for uncertain
strict-feedback systems within predeﬁned neural network approximation sets,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 31, no. 8, pp. 2942–2954,
2020.

[57] K. Zhao and J. Chen, “Adaptive neural quantized control of mimo nonlinear systems
under actuation faults and time-varying output constraints,” IEEE Transactions on
Neural Networks and Learning Systems, vol. 31, no. 9, pp. 3471–3481, 2020.

[58] O. Dogru, N. Wieczorek, K. Velswamy, F. Ibrahim, and B. Huang, “Online
reinforcement learning for a continuous space system with experimental validation,”
Journal of Process Control, vol. 104, pp. 86–100, 2021.
[Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0959152421000950

[59] J. Hendriks, C. Jidling, A. Wills, and T. Sch¨on, “Linearly constrained neural networks,”
Submitted to IEEE Transactions on Neural Networks and Learning Systems, 2020.

25

[60] S. Greydanus, M. Dzamba, and J. Yosinski, “Hamiltonian neural networks,” CoRR, vol.

abs/1906.01563, 2019. [Online]. Available: http://arxiv.org/abs/1906.01563

[61] M. Lutter, C. Ritter, and J. Peters, “Deep lagrangian networks: Using physics as
model prior for deep learning,” CoRR, vol. abs/1907.04490, 2019. [Online]. Available:
http://arxiv.org/abs/1907.04490

[62] D. Q. Mayne, J. B. Rawlings, C. V. Rao, and P. O. M. Scokaert, “Constrained
model predictive control: Stability and optimality,” Automatica, vol. 36, no. 6, pp.
789 – 814, 2000. [Online]. Available: http://www.sciencedirect.com/science/article/pii/
S0005109899002149

[63] G. Pannocchia, “Robust disturbance modeling for model predictive control
with application to multivariable ill-conditioned processes,” Journal of Process
Control, vol. 13, no. 8, pp. 693 – 701, 2003.
http:
//www.sciencedirect.com/science/article/pii/S0959152402001348

[Online]. Available:

[64] J. L¨ofberg, “YALMIP : A Toolbox for Modeling and Optimization in MATLAB,” in
Proc. of the CACSD Conference, Taipei, Taiwan, 2004, available from http://users.isy.
liu.se/johanl/yalmip/.

[65] F. Borrelli, A. Bemporad, and M. Morari, Predictive Control

and Hybrid Systems. Cambridge University Press, 2017.
https://books.google.de/books?id=cdQoDwAAQBAJ

for Linear
[Online]. Available:

[66] M. Herceg, M. Kvasnica, C. Jones, and M. Morari, “Multi-parametric toolbox 3.0,” in

2013 European Control Conference, Zurich, Switzerland, 2013, pp. 502–510.

[67] B. Tak´acs, J. ˇStevek, R. Valo, and M. Kvasnica, “Python code generation for explicit mpc
in mpt,” in European Control Conference 2016, Aalborg, Denmark, 2016, pp. 1328–1333.
[Online]. Available: https://www.uiam.sk/assets/publication info.php?id pub=1737

[68] E. Skomski, S. Vasisht, C. Wight, A. Tuor, J. Drgona, and D. Vrabie, “Constrained
block nonlinear neural dynamical models,” arXiv preprint arXiv:2101.01864, 2021.

[69] R. G. Krishnan, U. Shalit, and D. Sontag, “Structured inference networks for nonlinear
state space models,” in AAAI’17: Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence, 2017.

[70] D. Hafner, T. P. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson,
“Learning latent dynamics for planning from pixels,” CoRR, vol. abs/1811.04551, 2018.
[Online]. Available: http://arxiv.org/abs/1811.04551

[71] O. P. Ogunmolu, X. Gu, S. B. Jiang, and N. R. Gans, “Nonlinear systems identiﬁcation
using deep dynamic neural networks,” CoRR, vol. abs/1610.01439, 2016. [Online].
Available: http://arxiv.org/abs/1610.01439

[72] S. S. Rangapuram, M. W. Seeger, J. Gasthaus, L. Stella, Y. Wang, and T. Januschowski,
“Deep state space models for time series forecasting,” in Advances in Neural Information
Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, Eds. Curran Associates, Inc., 2018, pp. 7785–7794.

26

[73] Jeen-Shing Wang and Yi-Chung Chen, “A hammerstein-wiener recurrent neural network
with universal approximation capability,” in 2008 IEEE International Conference on
Systems, Man and Cybernetics, Oct 2008, pp. 1832–1837.

[74] D. Masti and A. Bemporad, “Learning nonlinear state-space models using deep au-
toencoders,” in 2018 IEEE Conference on Decision and Control (CDC), 2018, pp.
3862–3867.

[75] A. Tuor, J. Drgona, and D. Vrabie, “Constrained neural ordinary diﬀerential equations

with stability guarantees,” 2020.

[76] J. Schoukens and L. Ljung, “Nonlinear system identiﬁcation: A user-oriented roadmap,”
CoRR, vol. abs/1902.00683, 2019. [Online]. Available: http://arxiv.org/abs/1902.00683

[77] P. Schmid, “Dynamic mode decomposition of numerical and experimental data,” Journal

of Fluid Mechanics, vol. 656, pp. 5–28, 2008.

[78] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga et al., “Pytorch: An imperative style, high-performance deep
learning library,” in Advances in Neural Information Processing Systems, 2019, pp.
8024–8035.

[79] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint

arXiv:1412.6980, 2014.

[80] D. Hendrycks and K. Gimpel, “Bridging nonlinearities and stochastic regularizers with
gaussian error linear units,” CoRR, vol. abs/1606.08415, 2016. [Online]. Available:
http://arxiv.org/abs/1606.08415

[81] L. Ljung, System Identiﬁcation: Theory for the User, 2nd edition. Prentice-Hall, Upper

Saddle River, NJ, 1999, p. 607.

[82] R. Oberdieck, N. Diangelakis, I. Nascu, M. Papathanasiou, M. Sun, S. Avraamidou, and
E. Pistikopoulos, “On multi-parametric programming and its applications in process
systems engineering,” Chemical Engineering Research and Design, 2016.

[83] D. Mayne, J. Rawlings, C. Rao, and P. Scokaert, “Constrained model predictive control:
Stability and optimality,” Automatica, vol. 36, no. 6, pp. 789–814, 2000. [Online].
Available: https://www.sciencedirect.com/science/article/pii/S0005109899002149

[84] A. Zheng and M. Morari, “Stability of model predictive control with mixed constraints,”

IEEE Transactions on Automatic Control, vol. 40, no. 10, pp. 1818–1823, 1995.

[85] A. Bemporad and M. Morari, “Robust model predictive control: A survey,” in Robustness
in identiﬁcation and control, A. Garulli and A. Tesi, Eds. London: Springer London,
1999, pp. 207–226.

[86] P. Pauli, J. K¨ohler, J. Berberich, A. Koch, and F. Allg¨ower, “Oﬀset-free setpoint
tracking using neural network controllers,” in Proceedings of the 3rd Conference
on Learning for Dynamics and Control, ser. Proceedings of Machine Learning
Research, vol. 144. PMLR, 07 – 08 June 2021, pp. 992–1003. [Online]. Available:
http://proceedings.mlr.press/v144/pauli21a.html

27

[87] S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from
data by sparse identiﬁcation of nonlinear dynamical systems,” Proceedings of the
National Academy of Sciences, vol. 113, no. 15, pp. 3932–3937, 2016. [Online]. Available:
https://www.pnas.org/content/113/15/3932

[88] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Multistep neural networks for data-
driven discovery of nonlinear dynamical systems,” arXiv preprint arXiv:1801.01236,
2018.

[89] A. Sanchez-Gonzalez, J. Godwin, T. Pfaﬀ, R. Ying, J. Leskovec, and P. W. Battaglia,

“Learning to simulate complex physics with graph networks,” in ICML, 2020.

28

