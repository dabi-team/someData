2
2
0
2

n
u
J

1

]

V
C
.
s
c
[

2
v
6
9
5
1
1
.
4
0
2
2
:
v
i
X
r
a

A Simple Structure For Building A Robust
Model(cid:63)

Xiao Tan1,2[+8617610072986], JingBo Gao1[+8618667090860], and Ruolin
Li1[+8615399311502]

1 Xidian Hangzhou Institute of Technology, Zhejiang Province, China
https://hz.xidian.edu.cn
2 1203550038@qq.com

Abstract. As deep learning applications, especially programs of com-
puter vision, are increasingly deployed in our lives, we have to think
more urgently about the security of these applications.One eﬀective way
to improve the security of deep learning models is to perform adver-
sarial training, which allows the model to be compatible with samples
that are deliberately created for use in attacking the model.Based on
this, we propose a simple architecture to build a model with a certain
degree of robustness, which improves the robustness of the trained net-
work by adding an adversarial sample detection network for cooperative
training.At the same time, we design a new data sampling strategy that
incorporates multiple existing attacks, allowing the model to adapt to
many diﬀerent adversarial attacks with a single training.We conducted
some experiments to test the eﬀectiveness of this design based on Cifar10
dataset, and the results indicate that it has some degree of positive ef-
fect on the robustness of the model.Our code could be found at https:
//github.com/dowdyboy/simple_structure_for_robust_model.

Keywords: Robustness · Adversarial Training · Deep Learning

1

Introduction

Currently, applications using deep learning methods are gradually and widely
used in our lives[1,2,3].In many scenarios, deep learning algorithms and models
must be secure.However, models trained by general deep learning methods are
often very sensitive to the input data.Small changes in the input data can lead to
large deviations in the model output, which makes it possible to spoof the model
by falsifying data that is indistinguishable to the human eye.Such artiﬁcially
created samples used to deceive the model are called adversarial samples[4].

To reduce the negative impact of such problems, there are two mainstream
approaches, one is adversarial sample detection[5] and the other is training ro-
bust models[6].Both schemes have their advantages and disadvantages, and both
can mitigate the impact of attacks on the model to some extent. In practical ap-
plication scenarios, the two approaches often need to be used in conjunction[7],

(cid:63) Supported by AutoDL.

 
 
 
 
 
 
2

as many adversarial samples are diﬃcult to detect or have a high error detection
rate.

We carefully analyzed the existing methods[8,9] and found that the two so-
lutions can be used not only in combination at the application level, but also in
conjunction with each other at the training stage.Therefore, we propose a simple
structure(Fig. 1), which improves the eﬃciency of robustness training through
the intermediate results of adversarial sample detection.This architecture has
excellent scalability, as it is suitable for almost any network structure.With the
control backbone network unchanged, we also designed diﬀerent adversarial sam-
ple detection networks and conducted experiments, and found some interesting
results(Table. 3).

Fig. 1. Pretrained Net and Target Net have the same network structure. The input
samples are entered into these two networks separately and the intermediate feature
maps of the network outputs are obtained. The two feature maps compute the diﬀer-
ences in Detector Net and provide feedback and ﬁne-tuning to the upstream network.

A Simple Structure For Building A Robust Model

3

In a real deployment environment, models need to face a wide variety of
attacks[10].After some investigation, we found that the data sampling strategy
has an impact on the training of the model[11,12].Based on this, we propose
a training data sampling strategy which can obtain a richer sample type with
the same training data size.This approach allows the model to be adapted to
more complex test scenarios in a single training session.In addition, we adopt
an oﬄine data processing scheme[13], which requires more storage space but can
eﬀectively improve the training speed.

We tested our designed structure using the Cifar10 dataset[14] and found that
our scheme can improve the robustness of the model to some extent compared
to the common adversarial training method[6,15].Also, because of the new data
sampling strategy, the model is able to be more adaptable to more complex
testing environments.

2 Related Work

In recent years, the problem of robustness of deep learning models has attracted
increasing attention, and numerous attack methods and defense methods have
been proposed.Here, we will discuss those methods that are relevant to our
schema and what they bring to our design.

2.1 Adversarial Attacks

Deep learning has developed rapidly and has been widely used in many areas,
such as speech recognition[16], image classiﬁcation[17], and object detection[18].
However, it also comes with the problem of its security. In 2013, Szegedy et al.
found that adding small perturbations to images can trick deep neural networks
(DNN) with high probability[19] into producing incorrect classiﬁcation results,
and these misclassiﬁed samples are called adversarial samples.

Gaussian Noise The simplest adversarial attack is to add Gaussian noise to
the image[20].These Gaussian noises, because they possess local randomness, can
interfere with the pixel value distribution of the image.Although these images
with simple Gaussian noise added do not produce large changes compared to
the original (and are not misleading to the human eye), they can still lead to
discrimination errors in simple models.

FGSM FGSM was proposed by Goodfellow et al. and is a classical algorithm
in the ﬁeld of adversarial samples[21].The main reason for the vulnerability of
neural networks to adversarial perturbations is their linear nature and, the linear
behavior in high-dimensional spaces is suﬃcient to cause misclassiﬁcation of
samples.FGSM is based on this principle and generates adversarial samples with
the help of gradients and the goal of maximizing the loss function[22].

4

BIM The BIM algorithm[23] uses an iterative approach to search for perturba-
tions at individual pixel points, rather than modifying all pixel points at once as
a whole.During the iterative update process, some pixel values of the sample may
overﬂow as the number of iterations increases.These values need to be replaced
by 0 or 1 in order to ﬁnally generate a valid image, which is used to ensure that
each pixel of the new sample is within a certain ﬁeld of each pixel of the original
sample.

DeepFool The DeepFool algorithm[24] adds less noise and takes less time to
generate samples compared to the FGSM algorithm.It is based on the classiﬁca-
tion idea of hyperplane and can accurately calculate the perturbation value.Adversarial
training using DeepFool samples can eﬀectively improve the robustness of the
model.

C&W The C&W algorithm[25] is an optimization-based attack algorithm.It
sets a special loss function to measure the diﬀerence between the input and the
output.This loss function contains adjustable hyperparameters, as well as param-
eters that control the conﬁdence level of the generated adversarial samples.By
choosing appropriate values for these two parameters, excellent adversarial sam-
ples are generated.

NST Neural Style Transfer is a way to perform style changes on images[26].Since
the texture of the image is modiﬁed during the transformation of the image, it can
obviously be applied to generate adversarial samples as well.These adversarial
samples deceive the model in terms of the underlying texture and direct the
model output to the wrong class.

2.2 Adversarial Defensive

Currently popular adversarial defense methods include model distillation[27] and
adversarial training[6].Model distillation uses a teacher model to guide the train-
ing of the student model, and the teacher model is a pre-trained model.When
training is performed, the input data are ﬁrst entered into the teacher model
and the probability distribution of the output is obtained.Training the student
model with this probability distribution and the original input data allows the
student model to quickly learn the capabilities of the teacher model.Also, Since
the probability distribution of the teacher model output is used as the training
label, it can hide the gradient information of the model to some extent[28], thus
making the trained student model robust to some adversarial attacks, but it is
not eﬀective for attack methods like C&W[25].

Adversarial training is a commonly used method to improve the robustness of
models.It is to exploit the powerful expressive power of deep neural networks[29]
to improve the robustness of the model by learning adversarial samples.On top of
classical adversarial training, it can also incorporate self-supervised tasks to ob-
tain pre-trained models[30] and can be ﬁne-tuned using expanded datasets.These

A Simple Structure For Building A Robust Model

5

techniques can further strengthen the model.Adversarial training also has certain
limitations, such as the possibility of overﬁtting the employed attack method and
the adversarial sample[31].To address these issues, our approach uses a detection
head for assisted training as a way to further strengthen the eﬀectiveness of the
model, and on the other hand, it uses a data sampling strategy that can combine
more attack methods and adversarial samples to prevent overﬁtting.

3 Methods

3.1 Structure Design

Adversarial training is an eﬀective way to resist adversarial samples and im-
prove model robustness.However, due to the large number of adversarial sam-
ples added to the training data, this leads to a longer convergence process of the
training.Inspired by the design of the auxiliary head[32], our method also adopts
a similar design as a way to improve the model convergence speed.Also, in the
selection of the auxiliary head, instead of using the auxiliary head of the same
type of task, we use the adversarial sample detection task as the auxiliary head
training method because we believe that intuitively, the adversarial sample de-
tection task and the adversarial training have some correlation and can extract
diﬀerent characteristics from the adversarial sample.

Fig. 2. The adversarial detection network receives the image input and goes through
three stages of processing: in the ﬁrst stage, the data is fed into the backbone network
to obtain feature maps of diﬀerent layers; in the second stage, data embedding is per-
formed, and the feature maps are convolved or deconvoluted and outputted uniformly
as feature maps of the same size and number of channels; in the third stage, all the
feature maps are stitched and spread into the output that Transformer can receive.

We designed a simple shallow Transformer[33] structure as an auxiliary net-
work to perform the adversarial sample detection task.However, unlike a normal

6

network model, its input is not the original data, but a feature map of the out-
put of an intermediate layer of a network that has been pre-trained on a clean
dataset.And, inspired by Vision in Transformer[34], we abandon the use of fully
connected layers as embedding layers and use convolutional structures as em-
bedding layers instead(Fig. 2).Since our input needs to fuse feature maps from
diﬀerent levels of the pre-trained network as input, we introduce deconvolution
in our embedding layer[35].After experiments, we found that fusing other levels
of feature maps with a shallow feature map as the center can have better results.
Because the secondary and primary networks perform diﬀerent classes of
tasks, they cannot simply perform gradient returns and updates separately.For
this case, our approach is to capture the output of a speciﬁc layer in the auxiliary
network and then ﬁnd the diﬀerential performance of the output features of the
pre-trained network and the target network on the auxiliary network by using
the Smooth L1 function[22] as part of the loss function.The ﬁnal loss function
consists of the categorical cross-entropy error and the L1 error [equation 1].

loss(X, Y, L) =

N
(cid:88)

i=1

α × CrossEntropyLoss(Xi, Li) + β × SmoothL1Loss(Xi, Yi)

(1)

3.2 Sampling Strategy

Classical sampling strategies for adversarial training tend to use a ﬁxed ratio
of adversarial samples as input[6], however, this approach may in some cases
make the network overﬁt to the selected adversarial sample data[31], because
the type of adversarial attack to which a particular picture belongs under that
sampling strategy is speciﬁc to a particular one.Faced with this situation, we
designed a more ﬂexible sampling strategy.It uses a dynamic probability with a
bounded range to determine the type of adversarial attack to which the sample
belongs.Due to the random nature of dynamic probabilities, each image generates
more types of adversarial samples, thus enriching the entire training dataset.

In addition, unlike the scheme of generating adversarial samples in the pro-
cess of training, we take the approach of generating adversarial samples oﬄine
(Fig. 3).Adversarial samples of all attack types are generated for each sample
in the training dataset and stored in the database before starting training.To
perform training, adversarial samples of the speciﬁed type are selected from the
database as input to the model based on dynamic probabilities.Oﬄine generation
of adversarial samples consumes more storage space, but can improve the eﬃ-
ciency of the training process.It also does not reduce the diversity of the sample
space due to the use of a sampling strategy with greater randomness.

4 Experiments

For our proposed model architecture, we conducted a series of experiments.All
of our experiments were run on a dual-card RTX A4000 mainframe.The exper-
imental results show that our method is able to make the network model more

A Simple Structure For Building A Robust Model

7

Fig. 3. The Clean dataset is programmed in an oﬄine manner to generate adversarial
samples of all categories and stored in a database. The sampler samples the database
according to the conﬁgured dynamic probability range and feeds the selected samples
to the model.

robust compared to ordinary adversarial training(Table. 2).In addition, we also
tried some detailed modiﬁcations, including the proportion of adversarial sam-
ples, the structure of the detection head, and the weight of the loss function,
and found some laws that aﬀect the performance of the model(Table. 3).

Table 1. Intensity of sample noise under diﬀerent types of adversarial attacks.

Name: Gaussian FGSM BIM DeepFool C&W NST
0.005 0.001 0.1
Value: 0.1

0.005 0.1

We used the Cifar10 dataset[14] for our experiments.We randomly sampled
45,000 images from the original training set as our training set and the remaining
5,000 images as the test set.The reason for this is that we want to keep the data
distribution of the training and test sets as consistent as possible.We generated
datasets with adversarial sample ratios of 0.5, 0.75, and 0.25, and the types of
adversarial samples contain FGSM, DeepFool, etc., and they use the appropriate
noise intensity(Table. 1), respectively.

We chose ResNet34[36] as our experimental network, and we modiﬁed the
network model so that it can output all intermediate feature maps.We ﬁrst per-
formed training on a clean dataset and general adversarial training, after which

8

Table 2. Comparison of the prediction results of the ordinary model, the adversarial
training model and our method, in terms of accuracy.

Adv. Data Ratio Accuracy
Structure
0.5
Baseline
Adv Train
0.5
Adv Train + Detector (ours) 0.5

0.6778
0.8464
0.8688

we conducted experiments using our method on the same dataset(Table. 1), and
ﬁnally, we conducted some exploratory experiments for diﬀerent parameter con-
ﬁgurations of our method(Table. 3).In addition, experiments were conducted on
a single adversarial sample test set in order to more fully validate the expres-
siveness of the model(Table. 4).The number of iterations in the training phase
for all models is 150 epochs.

Table 3. Comparison of results on adversarial sample detection and classiﬁcation for
diﬀerent input types, adversarial sample ratios, and number of self-attentive layers.

Input Type Adv. Data Ratio SA Layers Detector Accuracy Classiﬁcation Accuracy
Low
Low
Low
Low
Low
Full
Full
Full
Full
Full

0.8658
0.842
0.845
0.8694
0.8416
0.8674
0.8406
0.8446
0.8688
0.8462

0.734
0.8332
0.8302
0.735
0.8364
0.7538
0.8458
0.8492
0.7506
0.8536

0.5
0.75
0.75
0.5
0.75
0.5
0.75
0.75
0.5
0.75

4
4
8
2
2
4
4
8
2
2

With the same adversarial sample ratio, our method has some improvement
in accuracy over the general adversarial training(Table. 2).Also, our method is
more robust to each diﬀerent type of adversarial attack(Table. 4), which indicates
that it does not have a signiﬁcant bias on the performance improvement and is
a more general method to improve the robustness of the model.

We also tried to conﬁgure diﬀerent input types, adversarial sample ratios, and
the number of self-attentive modules for the detection network(Table. 3).We ﬁnd
that embedding all level feature maps can improve the accuracy of the detection
network compared to embedding only low level feature maps, but has no signif-
icant eﬀect on the classiﬁcation accuracy of the images.In addition, excessively
increasing the ratio of adversarial samples may not improve the performance
of the model, as too many adversarial samples may make it more diﬃcult for
the model to capture the essential features needed to perform the classiﬁca-
tion.And, adding more self-attentive modules does not improve the performance
of the model, but leads to a small decrease, which may be due to the fact that

A Simple Structure For Building A Robust Model

9

for capturing detectability features with neural networks, too many layers are
not needed, otherwise some useless information may be captured, thus causing
negative feedback to the target network.

Table 4. Prediction results of diﬀerent structures on diﬀerent types of adversarial
sample datasets.

Structure
Baseline
Adv Train
Adv Train + Detector (ours) 0.8916 0.8542 0.7834 0.8296 0.8884

Clean Gaussian FGSM BIM DeepFool C&W NST
0.9012 0.6616
0.8737 0.8282

0.1074 0.6236
0.7736 0.836
0.8066 0.8592

0.1748 0.1984
0.7906 0.867

0.307
0.751

5 Conclusion

As the security and robustness of deep learning models are increasingly em-
phasized, adversarial attacks may gradually become a necessary factor to be
considered for training models.An eﬀective means to deal with adversarial at-
tacks is to train the model adversarially, which enhances the performance and
robustness of the model in a hostile environment by strengthening its adaptation
to adversarial samples.Our proposed method adds a small adversarial sample de-
tection network to the adversarial training by which some additional features of
the data are extracted and used to improve the eﬃciency and performance of the
adversarial training.In addition, we design a new oﬄine data sampling strategy
that incorporates a variety of adversarial attack types and has stronger random-
ness, allowing the trained model to be more adaptable to complex environments
while improving the training eﬃciency to a certain extent.After experiments, we
found that the above method has certain eﬀectiveness in practice.Also, since this
design structure is very expandable, optimizing some design and conﬁguration
details on the basis of this structure may further increase the model’s capability.

References

1. Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber and Saber Fallah. A Sur-
vey of Deep Learning Applications to Autonomous Vehicle Control. IEEE Transac-
tions on Intelligent Transportation Systems, 2020.

2. Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang and Robert X.
Gao. Deep Learning and Its Applications to Machine Health Monitoring: A Survey.
IEEE Transactions on Neural Networks and Learning Systems, 2017.

3. Li Wang, Dennis Sng. Deep Learning Algorithms with Applications to Video Ana-

lytics for A Smart City: A Survey. arXiv:1512.03131, 2015.

4. Julia Lust, Alexandru Paul Condurache. A Survey on Assessing the Generalization
Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and
Adversarial Samples. arXiv:2008.09381, 2021.

10

5. Reuben Feinman, Ryan R. Curtin, Saurabh Shintre and Andrew B. Gardner. De-

tecting Adversarial Samples from Artifacts. ICML, 2017.

6. Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang and Xu-Yao Zhang. A Survey of
Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and
Methodologies. arXiv:2203.14046, 2022.

7. Xuwang Yin, Soheil Kolouri and Gustavo K. Rohde. Adversarial Example Detection

and Classiﬁcation With Asymmetrical Adversarial Training. ICLR, 2020.

8. Sebastian Scher, Andreas Trügler. Robustness of Machine Learning Models Beyond

Adversarial Attacks. arXiv:2204.10046, 2022.

9. Maor Ivgi, Jonathan Berant. Achieving Model Robustness through Discrete Adver-

sarial Training. EMNLP, 2021.

10. Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu and In So

Kweon. A Survey On Universal Adversarial Attack. IJCAI, 2021.

11. Alexander Dallmann, Daniel Zoller and Andreas Hotho. A Case Study on Sam-
pling Strategies for Evaluating Neural Sequential Item Recommendation Models.
arXiv:2107.13045, 2021.

12. Ehsan Kamalloo, Mehdi Rezagholizadeh and Ali Ghodsi. When Chosen Wisely,
More Data Is What You Need: A Universal Sample-Eﬃcient Strategy For Data
Augmentation. ACL, 2022.

13. Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine and
Chelsea Finn. Conservative Data Sharing for Multi-Task Oﬄine Reinforcement
Learning. arXiv:2109.08128, 2021.

14. Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.
15. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand and Victor Lempitsky. Domain-
Adversarial Training of Neural Networks. JMLR 2016.

16. Priyabrata Karmakar, Shyh Wei Teng and Guojun Lu. Thank you for Attention: A
survey on Attention-based Artiﬁcial Neural Networks for Automatic Speech Recog-
nition. IEEE/ACM, 2021.

17. Lars Schmarje, Monty Santarossa, Simon-Martin Schröder and Reinhard Koch. A
survey on Semi-, Self- and Unsupervised Learning for Image Classiﬁcation. IEEE,
2021.

18. Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng and
Rong Qu. A Survey of Deep Learning-based Object Detection. arXiv:1907.09408,
2019.

19. Gabriel Resende Machado, Eugênio Silva and Ronaldo Ribeiro Goldschmidt. Ad-
versarial Machine Learning in Image Classiﬁcation: A Survey Towards the De-
fender’s Perspective. ACM Computing Surveys, 2021.

20. Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka and Volkan Cevher. Adversar-

ially Robust Optimization with Gaussian Processes. arXiv:1810.10775, 2018.

21. Ian J. Goodfellow, Jonathon Shlens and Christian Szegedy. Explaining and Har-

nessing Adversarial Examples. arXiv:1412.6572, 2014.

22. Hong Xuan and Robert Pless. Dissecting the impact of diﬀerent loss functions with

gradient surgery. arXiv:2201.11307, 2022.

23. Alexey Kurakin, Ian Goodfellow and Samy Bengio. Adversarial examples in the

physical world. arXiv:1607.02533, 2016.

24. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi and Pascal Frossard. DeepFool:
A Simple and Accurate Method to Fool Deep Neural Networks. CVPR, 2016.
25. N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks.

IEEE, 2017.

A Simple Structure For Building A Robust Model

11

26. Yanghao Li, Naiyan Wang, Jiaying Liu and Xiaodi Hou. Demystifying Neural Style

Transfer. IJCAI, 2017.

27. Bharat Bhusan Sau and Vineeth N. Balasubramanian. Deep Model Compression:

Distilling Knowledge from Noisy Teachers. arXiv:1610.09650, 2016.

28. Lichao Sun and Lingjuan Lyu. Federated Model Distillation with Noise-Free Dif-

ferential Privacy. IJCAI, 2021.

29. Or Sharir, Amnon Shashua and Giuseppe Carleo. Neural tensor contractions and

the expressive power of deep neural quantum states. arXiv:2103.10293, 2021.

30. Haoyu Dong, Zhoujun Cheng, Xinyi He, Mengyu Zhou, Anda Zhou, Fan Zhou, Ao
Liu, Shi Han and Dongmei Zhang. Table Pre-training: A Survey on Model Archi-
tectures, Pre-training Objectives, and Downstream Tasks. IJCAI, 2022.

31. Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S. Dhillon and
Cho-Jui Hsieh. The Limitations of Adversarial Training and the Blind-Spot Attack.
ICLR, 2019.

32. Joel Ye, Dhruv Batra, Erik Wijmans and Abhishek Das. Auxiliary Tasks Speed

Up Learning PointGoal Navigation. CoRL, 2020.

33. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. Attention Is All You Need.
arXiv:1706.03762, 2017.

34. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit and Neil Houlsby. An Image is Worth 16x16
Words: Transformers for Image Recognition at Scale. ICLR, 2021.

35. Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep

learning. arXiv:1603.07285, 2016.

36. Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Deep Residual Learning

for Image Recognition. arXiv:1512.03385, 2015.

