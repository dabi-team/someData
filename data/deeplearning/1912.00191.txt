1
2
0
2

y
a
M
6

]
I

A
.
s
c
[

3
v
1
9
1
0
0
.
2
1
9
1
:
v
i
X
r
a

Learning a Decision Module
by Imitating Driver’s Control Behaviors

Junning Huang1,∗ Sirui Xie2,∗ Jiankai Sun5,∗Qiurui Ma4 Chunxiao Liu3 Dahua Lin5 Bolei Zhou5

1Technische Universit¨at Darmstadt 2University of California, Los Angeles 3SenseTime Research
4Hong Kong University of Science and Technology 5The Chinese University of Hong Kong

Abstract: Autonomous driving systems have a pipeline of perception, decision,
planning, and control. The decision module processes information from the per-
ception module and directs the execution of downstream planning and control
modules. On the other hand, the recent success of deep learning suggests that
this pipeline could be replaced by end-to-end neural control policies, however,
safety cannot be well guaranteed for the data-driven neural networks.
In this
work, we propose a hybrid framework to learn neural decisions in the classical
modular pipeline through end-to-end imitation learning. This hybrid framework
can preserve the merits of the classical pipeline such as the strict enforcement of
physical and logical constraints while learning complex driving decisions from
data. To circumvent the ambiguous annotation of human driving decisions, our
method learns high-level driving decisions by imitating low-level control behav-
iors. We show in the simulation experiments that our modular driving agent can
generalize its driving decision and control to various complex scenarios where
the rule-based programs fail.
It can also generate smoother and safer driving
trajectories than end-to-end neural policies. Demo and code are available at
https://decisionforce.github.io/modulardecision/.

1

Introduction

An autonomous driving system is essentially a decision-making system that takes a stream of on-
board sensory data as input, processes it with prior knowledge about driving scenarios, and then
make a reasonable decision, and outputs control signals to steer the vehicle [1]. Such a system is
often modularized into perception, decision, planning, and control. While the perception and its aux-
iliary, map reconstruction, tend to be more open to machine learning methods [2], the downstream
modules such as decision, planning and control remain as program-based to ensure the safe interac-
tion with the physical world. In other words, this safety guarantee is built upon a human-inspectable
and interruptible basis. However, in complex driving environments, it is extremely difﬁcult to build
a complete rule-based decision-making system. So many corner cases are out there such that a sig-
niﬁcant program refactorization is required when one corner case is inconsistent with the existing
rules. Towards a more ﬂexible alternative, we explore a learning-based decision-making module
whose downstream modules, i.e. planning and control, remain encapsulated. Essentially, we draw
inspiration from a prior work [3], which, in stark contrast to end-to-end control policies, learns a
decision module to switch between controllers designed with Lyapunov domain knowledge.

The primary challenge we have for a learning-based decision module is the source of supervision.
Driving decision is deﬁned as the high-level abstraction about what lane the driver wants the vehicle
to be with at which velocity in T seconds. For an autonomous driving system, this decision deter-
mines the execution of downstream planning and control modules. For a human driver, however,
it is both difﬁcult and ambiguous to describe such a driving decision. Imagine two drivers try to
merge into trafﬁc at one roundabout, they may have different habitual behaviors to execute. Even
the same individual may make different decisions in almost identical scenarios. To this end, there
is hardly a set of golden criteria to evaluate the quality of human-annotated driving decisions. In

∗indicates equal contribution.

4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA.

 
 
 
 
 
 
Figure 1: Modular decision framework. The green line shows the modular pipeline of the au-
tonomous driving system. Solid orange lines indicate the ofﬂine training of the neural discriminator,
while the dashed blue line indicates that the planning and control modules are not differentiable. But
the decision policy can be trained with reward assigned on control action by the discriminator after
reparameterization, as indicated by the dashed red line.

contrast, drivers’ physical behaviors, such as stepping on throttle/brake and steering the wheel, im-
plicitly reﬂect their high-level decisions. Even though it is only a weak supervision in the sense that
the decision is not directly supervised, it is easily accessible and technically reliable.

The setup of weak and indirect supervision requires a mechanism to infer human intentions from
their behaviors and imitate these cognitive decisions. Intention understanding and behavior imitation
have been extensively studied in the ﬁeld of artiﬁcial intelligence and cognitive science, under the
banner of Inverse Planning [4] and Inverse Reinforcement Learning [5, 6, 7]. The former one mainly
focuses on inferring the unobservable intention from the observation of human behaviors in a third-
person view [8]; The latter one adopts end-to-end neural policies and does not distinguish intentions
from actions [9, 10]. Henceforth, neither of them discussed the formulation of indirect supervision
as in this work. More speciﬁcally, our explicit representation of intentions and modular pipeline
is appealing to physical systems like autonomous driving since the end-to-end neural counterparts
do not guarantee the fulﬁllment of low-level constraints in states or actions [11]. Nor do they have
robustness or stability guarantees [12], which are of great importance to the real-world deployment
of classical controllers. Given human driving behaviors, the expected framework should learn a
high-level decision module in the hierarchical pipeline where downstream modules inherit the merits
of the classical paradigm. Due to the non-differentiability of these downstream programs, a tailored
learning scheme is needed.

In this work, we propose an imitation learning framework for a modular driving pipeline that learns
neural decisions from human behavioral demonstrations. The learning is conducted in a generative-
adversarial manner [13]. The generator (green line in Fig.1) simulates the modularized driving
pipeline. In the upstream, a neural decision module generates decisions according to the informa-
tion from a local map. Decisions are then passed into the programmed downstream modules for
planning and control. The generation process ends at the control module, where the interaction with
the environment is triggered. For adversarial training (orange lines in Fig.1), the neural discriminator
takes in the generated trajectories and compares them with drivers’ behavior data. Due to the block-
age of gradients at the planning and control modules (blue dashed line in Fig.1), we derive a novel
learning objective to propagate credits of control actions back to the corresponding decisions (red
line in Fig.1). This learning framework is hence agnostic to the mechanisms of perception, planning,
and control modules. We evaluate this framework on various simulated urban driving scenarios in
CARLA, including (i) following, (ii) merging at the crossing, (iii) merging at the roundabout, and
(iv) overtaking. The learning-based modular system demonstrates superior performance over rule-
based modular systems and end-to-end control policies.

We summarize our contributions as:

• The proposed framework combines a program-based system with a generative learning
method, where the high-level decision policy is data-driven while the low-level planning
and control modules remain conﬁgurable and physically constrained. These low-level con-
straints also improve sample efﬁciency in interactive learning.

• The proposed generative adversarial learning method is weakly supervised. It learns high-
level driving decisions from low-level control data in an end-to-end manner, circumventing
the ambiguous annotation of human driving decisions.

2

s0

a0

s1

r1

s2

s0

a1

r2

d0

u0

s1

r1

d1

u1

s2

r2

(a) Markov Decision Process

(b) Markov Modular Decision Process

Figure 2: Comparison between Markov Decision Process and Markov Modular Decision Process

• This framework successfully distills behaviors in different scenarios into one decision mod-
ule. Empirical results demonstrate that the learned decision module forms synergy with the
programmed downstream modules, endowing the vehicle with smoother driving trajecto-
ries. It also exhibits substantial generalization capability in novel and complex scenarios.

2 Related Work

Classical autonomous driving system. The design of the autonomous driving pipeline could be
traced back to DARPA Urban Challenge 2007 [14]. The survey from [1] provides an overview of
the hierarchical structure of perception [15, 16], decision [17], planning, and control. Speciﬁcally,
ﬁnite state machine (FSM) was used in the decision level, or equivalently upper planning level, in
[18]. Classical autonomous driving systems are organized in this way such that it is accessible to
testing and diagnosis. Based on this modular pipeline, our work aims at developing a data-driven
decision module to replace the FSM.

[10] and Ross et al.

Imitation learning-based autonomous driving system. With the popularity of deep generative
learning, methods have been proposed to learn end-to-end neural control policies from the driver’s
data. Previously, Ziebart et al.
[9] proposed general methods in Inverse
Reinforcement Learning and Interactive Learning from Demonstration, with an empirical study on
a driving game. More recently, Kueﬂer et al.
[20] learn an end-to-
end policy in a GAIL[21]-like manner. Codevilla et al. [22] and Liang et al. [23] share similar
hierarchical perspective as us, but their control policies are completely neural. One thing worth
mentioning is Codevilla et al. [22] proposed to learn a downstream policy through the imitation of
human’s high-level command, whose motivation is similar to ours. Rhinehart et al. [24] propose to
learn a generative predictor for vehicles’ coordinate sequence. Li et al. [25] imitate the mapping
from predicted waypoints to control commands via neural network policy. Our work differentiates
from all of them as we imitate the driver’s low-level behaviors to learn high-level decisions, whose
execution is conducted by a transparent and conﬁgurable program.

[19] and Behbahani et al.

Baker et al.

Learning human intention.
[4] formalized humans’ mental model of planning
with Markov Decision Process (MDP) and proposed to learn human’s mental states with Bayesian
inverse planning. A similar probabilistic model of decisions is adopted by us but the learning is
done with a novel generative adversarial method. Wang et al. [8] proposed a Hidden Markov Model
(HMM) for human behaviors and materialized it with Gaussian Process (GP), where the intent is the
hidden variable. Jain et al. [26] model the interaction between human intentional maneuvers and
their behaviors in a driving scenario, both in-car and outside, as an Auto-Regressive Input-Output
HMM and learn it with EM algorithm. However, they didn’t explicitly distinguish humans from
the environment. Besides, rather than inferring the human’s mental state, we focus more on how a
robot, i.e. can make good decisions.

3 Modular Driving System

3.1 System Overview

Fig.1 illustrates the proposed learning framework. We focus on the decision-making module and
its connection to the downstream planning and control modules. The state estimator (usually a
perception module) constructs a local map by processing the sensory data and combining them
with prior knowledge from the map and rules. The decision module then receives observation and
decides a legal local driving task that steers the car towards the destination. To complete this task,

3

the planning module searches for an optimal trajectory under enforced physical constraints. The
control module then reactively corrects any errors in the execution of the planned motion.

In our proposed framework, the decision module is a conditional probability distribution parameter-
ized by neural networks. The downstream planning and control module are encapsulated. Because
they are not the main contribution of this work, we adopt minimal settings to stay focused on our
learning frameworks. These capsulated modules could be replaced by arbitrary alternatives.

3.2 Decision Making Module

We assume that a local map centered at the ego vehicle could be constructed from either the state
estimator or an ofﬂine global HD map. This local map contains critical driving information such as
the routing guidance to the destination, legal lanes, lane lines, other vehicle coordinates, and velocity
in the ego vehicle’s surroundings, as well as ego vehicle’s speed and acceleration.

The interaction between the decision module and the environment through its downstream is
modeled as Markov Decision Process (MDP). MDP is normally represented as a tuple M =
(S, D, P, r, ρ0, γ), with a state set S, a decision set D, a transitional probability distribution
P : S × D × S → R, a bounded reward function r : S × D → R, an initial state distribution
ρ0 : S → R, a discount factor γ ∈ [0, 1] for inﬁnite horizon. Decision policy πθ : S × D → R
takes current state st ∈ S from local map and generates a high-level decision dt. Note that the
notation of D and d are unconventional, we explicitly denote decision with dt to differentiate it
from control action ut. This decision directs the execution of planning and control module, and
makes the autonomous driving system proceed in the environment to acquire next state st+1. There-
fore there is a modiﬁcation of the decision process, termed as Markov Modular Decision Process
shown in Fig.2. The optimization objective of this policy is to maximize the expected discounted
return Eτ [(cid:80)T
t=0 γtR(st, ut)], where τ = (s0, d0, u0, ...) denotes the whole trajectory, s0 ∼ ρ0(s0),
dt ∼ πθ(dt|st), ut ∼ P (ut|st, dt) and st+1 ∼ P(st+1|st, ut).

Below we introduce the observation space and decision space, which are the interfaces in our mod-
ular system. Note that we assume full access to necessary information on the local map.

To make the decision-making policy more gen-
the
eralizable to different driving scenarios,
output of the state estimator, also as the input
to the neural decision module, is the local map
and trafﬁc state. As shown in Fig.3, a part of
the observation is the 3D coordinates of sam-
ple points of lane lines. We select two sets of
lanes for the decision policy. One for the ego
vehicle’s current lane and the other for the edge
of legal regions. Note that here legal means
driving in that region abides by both the trafﬁc
rules and a global routing, which would have a
crucial effect at the crossing. To reduce the di-
mension of input, these lane points are sampled
with exponentially increasing slots towards the
further end. In some sense, it mimics the effect
of Lidar, with the observation more accurate in
the near end. The observation also includes the
coordinates and velocities of the 6 nearest vehi-
cles in trafﬁc within 70m range of the ego ve-
hicle.

Figure 3: Illustration of input and output of the
decision module when the driving system con-
ducts overtake. The trained policy decides to pick
the left lane, in terms of the target point and the
target speed. The planner generates a trajectory,
which is tracked by the controller. Observations
are shown on the right. Yellow lines are the ego
vehicle’s current lane lines and the red lines are
the edge of legal regions.

We deﬁne the decision as three independent cat-
egorical variables, one for lateral, one for lon-
gitudinal and one for velocity. Combining with the local map, each of them is assigned a speciﬁc
semantic meaning. The lateral decision variable has three classes as changing to the left lane, keep-
ing current lane and changing to the right lane. Note that at a crossing, global routing information
has been implanted into the local map as introduced in the last paragraph. The lateral decision
set is complete since these three are the only possible lateral decisions for a vehicle. The longi-

4

tudinal decision variable has four different classes, with each indicating the distance along with
waypoints in time interval T. The exact coordinate of the endpoint could be extracted from the local
map. Combining with the predicted target speed at the endpoint from the velocity decision variable,
which discretizes the allowed speed range equally into four baskets, this decision module provides
a quantitative conﬁguration of goal states for the downstream trajectory planner.

3.3 Planning and Control Modules

We introduce a minimal design of the planning module and the control module for completeness,
which is not the main contribution of this work. In the classical autonomous driving system, the
planning module calculates the trajectories under constraints such as collision avoidance. Its opti-
mization objective is to reach the goal state speciﬁed by the decision module with minimal cost. In
our minimal setting, the planning module processes path and velocity separately. Paths are planned
with cubic Bezier curves, while velocity is planned with Quadratic Programming (QP) to minimize
the acceleration, jerk, and discontinuity in curvature. The control module drives the vehicle to trace
the planned trajectories. And we implement it minimally with PID. Details are provided in Appx.
A.

It is ﬂexible to further extend each module of the proposed framework, as long as the alternatives are
deterministic or stationary. Note that while the interface between decision and planning is the loca-
tion of a goal point and the speed ego vehicle is expected to maintain when reaching it, the planning
module could be replaced by a more sophisticated search-based or optimization-based trajectory
planner, to fulﬁll more practical requirements in execution time and constraint enforcement. Sim-
ilarly, the model-free controller could be replaced with alternatives like Model Predictive Control
(MPC). Other practical constraints and concerns over the controller such as robustness and stability
could also be taken into consideration if necessary.

4

Imitation Learning

4.1 From IRL to GAIL

Under the MDP modeling, we choose imitation learning over reinforcement learning. In principle,
imitation learning could save practitioners from ad hoc reward engineering and focus their effort on
reusable infrastructure development. Among all the imitation learning methods, behavior cloning
(BC) [27] is the most straightforward one. The policy is trained as a regressor in a purely supervised
manner, ignoring the temporal effect of each action in an MDP trajectory. Thus it suffers from
covariate shift when presented with states which are not covered by the training data. By contrast,
imitation learning such as Inverse Reinforcement Learning (IRL) explicitly models the interaction
between policy and environment and approximates for exact value iteration [10] or Monte Carlo
approximation [21]. Thus they can generalize better in states not covered in the demonstration.

Fig.2 illustrates the difference in the graphical model of a trajectory from an MDP when the
In this modiﬁed probabilistic model, a trajectory is
modular system is adopted for the agent.
a sequence τ = (s0, d0, u0, s1, d1, u1...). However, what is observable from demonstration is
ˆτ = (s0, u0, s1, u1...). According to the formulation of MaxEnt IRL [10], when maximum en-
tropy principle is applied for the tie-breaking between cost functions that can generate identical
optimal trajectories, the probability of a trajectory is

1
Z
where Z is the partition function, R is the reward function, which is normally Markovian R(ˆτ ) =
(cid:80) r(si, ui). Given a set of demonstration DE, we can infer the reward function by Maximum
Likelihood (MLE):

exp(R(ˆτ )),

p(ˆτ |R) =

(1)

LIRL = EDE [log p(ˆτ |R)] = EDE [

(cid:88)

r(si, ui) − log Z].

(2)

ˆτ

In the standard IRL, with the learned reward function, we can solve for the policy with any rein-
forcement learning methods. However, as the partition function is always intractable if the state
space is continuous, methods have been proposed to approximate it [10, 28]. Basically, there is a
background distribution q(ˆτ ) for the estimation of Z. In [29], it is further illustrated how a delicately

5

designed importance sampling scheme can connect this IRL loss to a speciﬁc type of discrimina-
tor over trajectories. Ho et al. [21] propose Generative Adversarial Imitation Learning (GAIL) to
further approximate this discriminator with a step-wise one, remedy the trajectory-wise information
with advantage accumulation and learn a policy with off-the-shelf reinforcement learning methods.
Intuitively, it learns a policy whose actions at given states are indistinguishable from demonstration
data. More formally, with expert demonstration DE, a neural discriminator Dφ : S × U → (0, 1) is
introduced and the reward function is deﬁned as r(s, u) = − log(Dφ(s, u)). The training objective
is:

LGAIL = min

θ

max
φ

{Epθ(u|s)[log(Dφ(s, u))] + EDE [log(1 − Dφ(sE, uE))]}.

(3)

Dφ is optimized with cross entropy loss, while πθ is optimized with policy gradient [30].

4.2 Handling Non-differentiable Downstream Modules

Different from GAIL, in our framework the generation process is modularized, following the struc-
ture of classical autonomous driving systems. Because the downstream planning and controls mod-
ules are not necessarily differentiable, a separation occurs between decisions from neural policy
and the control data from drivers, as illustrated by the blue dotted line in Fig 1: policy πθ gen-
erates decisions dt while discriminator only distinguishes actions ut. And as shown in Fig 2,
pθ(ut|st) = (cid:80)
p(ut|dt)πθ(dt|st). Since planning and control modules are both deterministic, at
dt
every state, once the decision is chosen, the transformation from decision to control is a deterministic
and correspondence mapping. Hence, one important insight of our work is that the transformation
at each state s

gs : {dt} onto−−−→

1-to-1

{ut},

(4)

which is the abstraction of the planning and control module, is a deterministic and correspon-
dence mapping. Therefore, it is a candidate for reparametrization [31] or push-forward [32].
Reparametrize u in the ﬁrst expectation in Eq.3 with d:

Epθ(u|s)[log(Dφ(s, u))] =
(cid:88)

(cid:88)

pθ(u|s) log(Dφ(s, u))

=

πθ(d|s) log(Dφ(s, gs(d))) = Eπθ(d|s)[log(Dφ(s, gs(d)))].

(5)

Intuitively, we only use this black-box function gs for Monte Carlo sampling, thus there is no need
In the training stage, we do not actually need to calculate ut with
to know its analytical form.
the push-forward function as in Eq.4. Instead, we calculate the true gs(dt) according to Sec. 3
during sampling, and save both dt and ut for training. During training, dt is fed to πθ and the
corresponding ut is fed to Dφ. Since D is discrete, it is differentiated with policy gradient. We then
have this modiﬁed learning objective:
L = min

{Eπθ(d|s)[log(Dφ(s, u))] + EDE [log(1 − Dφ(sE, uE))]}.

(6)

max
φ
The whole algorithm is described in Algorithm 1.

θ

Algorithm 1 Learning executable decisions from hu-
man behavioral data
Require: Expert demonstration DE, batch size B

Initialize policy πθ, discriminator Dφ, (optionally)
value function Vw
while policy iteration not converged do
Initialize data buffer B with length B
while ¬B.f ull() do

Sample expert trajectory: τE ∼ DE
Initialize sim with sE
0
traj = GENERATE-TRAJ
Append traj to B

end while
Compute reward for samples in B with Dφ
Update Dφ with data in B and DE
Policy iteration on πθ with tuples in B, (option-
ally with value iteration on Vw)

end while

Algorithm 2 GENERATE-TRAJ

Require: Simulator (or ROS ofﬂine re-
player) with local map sim, plan-
ning module planner, control module
controller, maximum trajectory hori-
zon H
Initialize placeholder traj, t = 0
while t < H or ¬sim.done do

Get st from sim
Sample dt from πθ(ot)
Get plnt from planner(sim, dt)
Get ut from controller(sim, plnt)
Append (st, dt, ut) to traj
Send ut to sim
t + +
end while
Yield traj

6

Figure 4: Trajectories from our proposed framework (green lines) are smoother than the ones from
the end-to-end neural policy (orange lines), because of the explicit enforcement of geometrical con-
straints in the downstream planning module. (× indicates the car violates routing rules.)

5 Experiments

5.1 Rudimentary Driving in Empty Town
Experiments in Empty Town show the difference between a modular driving system and an end-to-
end neural control policy. The training time on an 8-GPU computation server for the end-to-end
control policy is 31 hours versus 15 hours for our modular driving system. Different training time
in the same platform implies different numbers of interactions these two methods need to converge,
showing the advantage of using the modular pipeline.

As shown in Fig.4, trajectories from our proposed framework are smoother than the ones from the
end-to-end policy. This demonstrates the effect of explicit enforcement of geometrical constraints in
the downstream planning module. Interestingly, there are some road structures where the learning-
based modular system can pass while the end-to-end control policy cannot. For example, Fig.4(d)
shows a narrow sharp turn where end-to-end control policy drives across the legal lane line. We
believe this is a difﬁcult temporally-consistent exploration problem. An agent needs some success-
ful trials of consecutive right-turning action samples to learn, which is challenging for generative
interactive learning. The modular system, in contrast, only explores behaviors that are temporally
plausible like human drivers. With the planning module that searches for a geometrically constrained
path, the agent can effortlessly make this sharp turn.

Table 1 shows the statistics of behaviors from the learning-based modular system, end-to-end control
policy, and rule-based method. The comparison shows that our method can drive more safely (0%
collision rate), much faster (less time to ﬁnish), and with higher comfort (less acceleration). This
exhibits the advantage of having both the learned decision policy and the classic planning and control
module. Besides, the decision module trained in Town 3 works similarly well in Town 2, showing
its generalization ability.

5.2 Socially Interactive Driving in Trafﬁc Scenarios

We further test our model’s performance in each trafﬁc scenario. The statistics of Trafﬁc Scenarios
in Table 1 are the average of 100 evaluations. Trafﬁc scenarios include basic scenarios such as Car
Following and Crossroad Merge where vehicle’s interactions with zombies are relatively simple and
monotonic; and complex ones such as Crossroad Turn Left, Roundabout Merge and Overtake. In
complex trafﬁc scenarios, the dynamics around the ego vehicle is of higher variation, due to either
more trafﬁc users or more complex relations between them.

As shown in Table 1, in basic trafﬁc scenarios, rule-based and learning-based modular systems are
somehow on par in terms of time taken to ﬁnish, while rule-based agent seems to drive a little
bit more comfortably. End-to-end agent drives more rudely in most of the scenarios, expect in
Crossroad Merge, where it takes much longer time to ﬁnish. Since the modular pipeline enforces
smoother planning and control, the learning-based modular system offers more driving comfort than
the end-to-end system. In basic trafﬁc scenarios, none of them drives as well as experts.

In complex trafﬁc scenarios, rule-based agent conﬁgured in some scenarios fails to drive safely in
others while learning-based modular agent is safer and smarter. This is because rule-based decision
module is based on FSM, which is composed of a huge number of rules. In order to drive safely
in all environments, the rules of safety have priority over other factors such as driving comfort. In
complex environments, the rules in FSM would be more conservative to handle corner cases and un-
certainty(e.g., more hard brakes or accelerations). Unlike the rule-based system, the learning-based
system can minimize the acceleration and jerk by imitating human experiences. Furthermore, since
the rule-based system is program-based, its conﬁgurations might vary in different scenarios, which

7

Table 1: Performance Comparision (LBM: Learning-based Module, E2E: End-to-end neural policy,
RB: Rule-based method, ED: Expert Data)

Scenario Time Taken (s) Acceleration (m/s2)

Jerk (m/s3) Collision Rate (%)

n
w
o
T
y
t
p
m
E

r
a
C
s
e
n
a
L

g
n
i
w
o
l
l
o
F

o
w
T

r
a
C
e
n
a
L

g
n
i
w
o
l
l
o
F

e
l
g
n
i
S

d
a
o
r
s
s
o
r
C

t
u
o
b
a
d
n
u
o
R

e
g
r
e
M

e
g
r
e
M

d
a
o
r
s
s
o
r
C

t
f
e
L
n
r
u
T

e
k
a
t
r
e
v
O

makes it difﬁcult to generalize across scenarios. When compared with end-to-end control policy, a
similar conclusion could be drawn as in the previous subsection that learning-based modular agent
offers more comfort because the modular pipeline enforces smoother planning and control. Inter-
estingly, in complex social scenarios, the learning-based modular system achieves higher comfort
(lower acceleration) than experts. This may be attributed to human errors, and a learning-based mod-
ular agent somehow ﬁxes it. Experiments of generalization capability can be found in Appendix.

6 Conclusion

This work introduces a ﬂexible framework for learning modular decision-making for autonomous
driving. To learn the driver’s high-level driving decision, the proposed framework imitates driver’s
control behavior with a modular generation pipeline. Being agnostic to the design of the non-
differentiable downstream planning and control modules, this framework can train the neural de-
cision module through reparametrized generative adversarial learning. We evaluate its effectiveness
in simulation environments with human driving demonstrations. This work can be extended to more
complex environments where other vehicles are also learning agents, i.e. multi-agent learning sys-
tem. Alternatively, if we can collect data in the real-world and replay them in the simulator, this
framework could be tested in real driving environments.

8

01002003000123024601234LBME2ERBED020406001234024680123401020300120123402460204060012301230102002040608001230246051015200102030400120246051015200204060024024605101520References
[1] B. Paden, M. ˇC´ap, S. Z. Yong, D. Yershov, and E. Frazzoli. A survey of motion planning and
control techniques for self-driving urban vehicles. IEEE Transactions on intelligent vehicles,
1(1):33–55, 2016.

[2] S. Thrun, W. Burgard, and D. Fox. Probabilistic robotics. MIT press, 2005.

[3] T. J. Perkins and A. G. Barto. Lyapunov design for safe reinforcement learning. Journal of

Machine Learning Research, 3(Dec):803–832, 2002.

[4] C. L. Baker, R. Saxe, and J. B. Tenenbaum. Action understanding as inverse planning. Cogni-

tion, 113(3):329–349, 2009.

[5] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Pro-
ceedings of the twenty-ﬁrst international conference on Machine learning, page 1, 2004.

[6] J. Sun, L. Yu, P. Dong, B. Lu, and B. Zhou. Adversarial inverse reinforcement learning with
self-attention dynamics model. IEEE Robotics and Automation Letters, 6(2):1880–1886, 2021.
doi:10.1109/LRA.2021.3061397.

[7] X. Chen, Z. Ye, J. Sun, Y. Fan, F. Hu, C. Wang, and C. Lu. Transferable active grasping and
real embodied dataset. In 2020 IEEE International Conference on Robotics and Automation
(ICRA), pages 3611–3618, 2020. doi:10.1109/ICRA40945.2020.9197185.

[8] Z. Wang, K. M¨ulling, M. P. Deisenroth, H. Ben Amor, D. Vogt, B. Sch¨olkopf, and J. Peters.
Probabilistic movement modeling for intention inference in human–robot interaction. The
International Journal of Robotics Research, 32(7):841–858, 2013.

[9] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction
In Proceedings of the fourteenth international conference on

to no-regret online learning.
artiﬁcial intelligence and statistics, pages 627–635, 2011.

[10] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement

learning. 2008.

[11] J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula, and C. J. Tomlin. A
general safety framework for learning-based control in uncertain robotic systems. IEEE Trans-
actions on Automatic Control, 2018.

[12] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel. Adversarial attacks on neural

network policies. arXiv preprint arXiv:1702.02284, 2017.

[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
In Advances in neural information processing

and Y. Bengio. Generative adversarial nets.
systems, pages 2672–2680, 2014.

[14] M. Buehler, K. Iagnemma, and S. Singh. The DARPA urban challenge: autonomous vehicles

in city trafﬁc, volume 56. springer, 2009.

[15] H. Yi, S. Shi, M. Ding, J. Sun, K. Xu, H. Zhou, Z. Wang, S. Li, and G. Wang. Segvoxelnet:
Exploring semantic context and depth-aware features for 3d vehicle detection from point cloud.
In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 2274–
2280, 2020. doi:10.1109/ICRA40945.2020.9196556.

[16] B. Pan, J. Sun, H. Y. T. Leung, A. Andonian, and B. Zhou. Cross-view semantic segmentation
for sensing surroundings. IEEE Robotics and Automation Letters, 5(3):4867–4873, 2020. doi:
10.1109/LRA.2020.3004325.

[17] J. Sun, H. Sun, T. Han, and B. Zhou. Neuro-symbolic program search for autonomous driving

decision module design. In Proceedings of the Conference on Robot Learning (CoRL) 2020.

[18] M. Montemerlo, J. Becker, S. Bhat, H. Dahlkamp, D. Dolgov, S. Ettinger, D. Haehnel,
T. Hilden, G. Hoffmann, B. Huhnke, et al. Junior: The stanford entry in the urban challenge.
Journal of ﬁeld Robotics, 25(9):569–597, 2008.

9

[19] A. Kueﬂer, J. Morton, T. Wheeler, and M. Kochenderfer. Imitating driver behavior with gener-
ative adversarial networks. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 204–211.
IEEE, 2017.

[20] F. Behbahani, K. Shiarlis, X. Chen, V. Kurin, S. Kasewa, C. Stirbu, J. Gomes, S. Paul,
F. A. Oliehoek, J. Messias, et al. Learning from demonstration in the wild. arXiv preprint
arXiv:1811.03516, 2018.

[21] J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Infor-

mation Processing Systems, pages 4565–4573, 2016.

[22] F. Codevilla, M. Miiller, A. L´opez, V. Koltun, and A. Dosovitskiy. End-to-end driving via
conditional imitation learning. In 2018 IEEE International Conference on Robotics and Au-
tomation (ICRA), pages 1–9. IEEE, 2018.

[23] X. Liang, T. Wang, L. Yang, and E. Xing. Cirl: Controllable imitative reinforcement learning
for vision-based self-driving. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 584–599, 2018.

[24] N. Rhinehart, R. McAllister, and S. Levine. Deep imitative models for ﬂexible inference,

planning, and control. arXiv preprint arXiv:1810.06544, 2018.

[25] G. Li, M. Mueller, V. Casser, N. Smith, D. L. Michels, and B. Ghanem. Oil: Observational

imitation learning. arXiv preprint arXiv:1803.01129, 2018.

[26] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena. Car that knows before you do:
In Proceedings of the IEEE

Anticipating maneuvers via learning temporal driving models.
International Conference on Computer Vision, pages 3182–3190, 2015.

[27] D. A. Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation.

Neural Computation, 3(1):88–97, 1991.

[28] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via
policy optimization. In International conference on machine learning, pages 49–58, 2016.

[29] C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adver-
sarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016.

[30] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.

In International conference on machine learning, pages 1889–1897, 2015.

[31] D. P. Kingma and M. Welling.

Auto-encoding variational bayes.

arXiv preprint

arXiv:1312.6114, 2013.

[32] N. Rhinehart, K. M. Kitani, and P. Vernaza. R2p2: A reparameterized pushforward policy for
diverse, precise generative path forecasting. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 772–788, 2018.

[33] J. Choi, R. Curry, and G. Elkaim. Path planning based on b´ezier curve for autonomous ground
vehicles. In Advances in Electrical and Electronics Engineering - IAENG Special Edition of
the World Congress on Engineering and Computer Science 2008, pages 158–166, 2008.

[34] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. CARLA: An open urban driving
simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 1–16, 2017.

[35] S. Shalev-Shwartz, S. Shammah, and A. Shashua. On a formal model of safe and scalable

self-driving cars. arXiv preprint arXiv:1708.06374, 2017.

[36] S. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin. Combining optimal control and
learning for visual navigation in novel environments. arXiv preprint arXiv:1903.02531, 2019.

10

Appendices

A Planning and Control Modules

The planning module processes path and velocity separately. Paths are planned with cubic Bezier
curve [33]:

B(t) = (1 − t)3Ps + 3(1 − t)2tP1 + 3(1 − t)t2P2 + t3Pg,

t ∈ [0, 1]

(7)

where Ps, Pg are the starting and goal point, P1 and P2 are intermediate control points, imposing
the geometrical constraints such as end pose aligning with lane direction.

The velocity planning module reparameterizes Eq.(7) with arc-length, introducing the temporal unit.
This reparameterization is not done directly, rather, n spline segments with identical time unit ∆T
are used to ﬁt sample points from (7). Each of the segments is deﬁned as:

si = ai + bit + cit2 + dit3 + eit4, t ∈ [0, ∆T ].

(8)

Here we consider the constraint on velocity planning, by minimizing the acceleration and jerk while
ﬁtting to the sample points from (7) and maintaining the continuity of curvature. Formally, the ﬁtting
is done by Quadratic Programming (QP):

min

n
(cid:88)

(cid:90) ∆T
[

i=1

0

¨s2
i dt +

(cid:90) ∆T

0

...
s 2

i dt],

s.t. ˙s0(0) = vnow, ¨s0(0) = anow, ˙sn(∆T ) = vgoal

˙sk(∆T ) = ˙sk+1(0),¨sk(∆T ) = ¨sk+1(0), f or k = 0, 1...n − 1

(9)

The control module is a basic PID controller. Here to reconcile with the MDP assumption in deci-
sion, we adopt the discrete-time form:

u(t) = Kpe(tk) + Ki

k
(cid:88)

i=0

e(tk)∆t + Kd

e(tk) − e(tk−1)
∆t

,

(10)

where Kp, Ki and Kd denote the coefﬁcients for the proportional, integral, and derivation terms
respectively. e(t) is the error function. In this work we have two independent error functions elat(t)
and elon(t) for lateral controller steering wheel usteer(t) and longitudinal controller throttle/brake
uv(t), which are both capped by mechanical constraints.

B Training Details

B.1 Experimental Setup

Training System We evaluate the proposed framework in CARLA simulation platform [34]. As
the proposed framework focuses on the decision and its downstream, the state estimator is neglected
and the observations are extracted directly from the simulator. Since the most recent version of
CARLA is not efﬁcient enough for large-scale simulation and learning, we build a parallel training
system. The parallel training system shown in Fig.5 consists of two parts. The ﬁrst part(denoted as
a blue dotted line box named Rollout) collects the transition data and the second part updates the
model’s parameters with the transition data. Every iteration, in Rollout, an actor synchronizes with
the Policy node and obtains the model’s parameters. Then the actor retrieves the transition data via
the CARLA frontend. After that, the transition data would be stored in the Trajectory node. Here
the transition data is generated by the interaction between the CARLA frontend and the CARLA
backend in a Docker container. After the collection of the transition data ﬁnished, we start to up-
date our model’s parameters. The Discriminator and Policy nodes would update their parameters
by following Algorithm 1 and using the data in the Trajectory and Demo nodes. To improve the
robustness of the system, a Daemon reboot node is injected to reboot the Docker container when the
system hangs.

11

Figure 5: Parallel Training System.
The ﬁrst
part(denoted as a blue dotted line box named Rollout)
collects the transition data and the second part updates
the model’s parameters with the transition data.

Figure 6: Autonomous Driv-
ing Simulation Platform, which
is used to collect
the low-level
drivers’ control behavior. We learn
high-level driving decisions by im-
itating the low-level drivers’ con-
trol commands.

Scenarios Setup Multiple common driving scenarios of varying complexity are constructed for
evaluation. All driving scenarios are deployed in the Map Town 3 except Overtake. Since Town 3
provides only two-lane roads and Overtake should be demonstrated on a three-lane road, we choose
Town 4 to design Overtake. To increase the complexity of the environment, the speed of the zombie
car is set with randomness, and we give its average speed and design standard deviation.

• Empty Town is a special trafﬁc scenario where there is no trafﬁc.
structures like a straight road, curve road, crossing, and roundabout.

It covers urban road

• Car Following (Fig.7a). There are two sub-scenarios in this scenario: Two Lanes Car
Following (Fig.7a) and Single Lane Car Following. Two Lanes Car Following is designed
based on a four-lane dual carriageway. Two zombie cars are running side by side in two
lanes, and the ego-car follows them in one of the lanes. The speed of the zombie cars is
18 ± 1 km/h. In Single Lane Car Following, only one zombie car is running ahead in a
single lane, and the ego-car follows the zombie car. The speed of the zombie car is 15 ± 1
km/h.

• Crossroad Merge (Fig.7b). In the lateral lane, no less than three zombie cars as a trafﬁc
stream drive through the intersection in series, and ego-car merges into their trafﬁc ﬂow
from a lane perpendicular to the lateral lane. The speed of zombie cars is randomly chosen
from 21 ± 1 km/h, 25 ± 1 km/h. There are two sub-scenarios.

• Crossroad Turn Left (Fig.7c). Multiple zombie cars drive in the lateral lane and the opposite
lane. The ego-car needs to cross the intersection and then head to the left lane, during
which it needs to avoid collision with the zombie cars and obey the trafﬁc rules. The speed
of zombie cars is randomly chosen from 26 ± 1 km/h, 42 ± 1 km/h. There are two sub-
scenarios.

• Roundabout Merge (Fig.7d). Four zombie cars are entering the roundabout one by one,
bypassing 270 degrees counterclockwise and then driving out of the roundabout. The ego-
car enters the roundabout and joins the trafﬁc ﬂow at about 90 degrees. After following the
zombie cars around the roundabout for about 180 degrees, the ego-car drives out with them
together. The speed of zombie cars is randomly chosen from 21 ± 1 km/h, 25 ± 1 km/h,
35 ± 1 km/h. There are three sub-scenarios.

• Overtake (Fig.3). Ego Car drives in the middle lane (medium speed lane) of the three lanes
with Zombie Car I in front of it. Zombie Car II (different from Zombie Car I), which is on
the left front of Zombie Car I, drives in the high-speed lane. Under the premise of obeying
trafﬁc rules, Ego Car has to go beyond Zombie Car I in front of it by changing to the high-
speed lane. Due to the blocking of Zombie Car II, Ego Car will change back to the medium
speed lane after exceeding Zombie Car I, then completing the entire overtaking process.

12

(a) Two-lane Car Following

(b) Crossroad Merge

(c) Crossroad Turn Left

(d) Roundabout Merge

Figure 7: Screenshots of four scenarios.

The maneuvers of zombies in all scenarios are deﬁned by two items: conﬁgurations of zombies,
and a CARLA built-in waypoint-following controller. Conﬁgurations of zombies for experiments in
Table 1 is the footnote part in Table 2.

Besides the car velocity variance mentioned above, the evaluation environments for each scenario
also vary in seeds and vehicle models. For example, Audi TT, Chevrolet, Dodge (the police car),
Etron, Lincoln, Mustang, Tesla 3S provided by CARLA.

Scenarios

Number
of Cars

Table 2: Conﬁgurations of Scenarios

Starting
Velocity
of Ego
Car
(km/h)

Starting
Velocity
of
Zombie
Cars
(km/h)

Target Velocity
of Zombie Cars
(km/h)

Starting
Direc-
tion
(◦)

Expected
Direc-
tion
(◦)

Single Lane Car
Following
Two Lanes Car
Following

2

3

25.20

10.00

15.00 ± 1.00‡

32.40

11.00

18.00 ± 1.00‡

Crossroad Merge

> 4

25.20

10.00

Roundabout
Merge

Crossroad Turn
Left

> 4

> 7

25.20

10.00

25.20

10.00

Overtake

3

33.48

11.00

21.00 ± 1.00‡
25.00 ± 1.00
21.00 ± 1.00,
25.00 ± 1.00,
35.00 ± 1.00‡
26.00 ± 1.00,
42.00 ± 1.00‡
25.00 ± 1.00‡
(Zombie Car I),
40.00 ± 1.00‡
(Zombie Car II)

0.00

0.00

0.00

180.00

0.00

90.00

0.00

0.00

0.00

−42.00

0.00

0.00

‡The footnote part is the speed conﬁguration for experimental results in Table 1 and Table 4.

Data Collection The demonstration data is collected by human driving ego car in an autonomous
driving simulation platform. The simulation platform shown in Fig.6 includes CARLA simulator
with Logitech G29 Driving Force Steering Wheels & Pedals. The PC conﬁguration is Ubuntu 16.04
x86, Intel Core i7-8700 CPU, GeForce GTX 1060 and 16 GB memory. The following criteria are
used to set to ﬁlter out low-quality demonstrations:

• There is no collision;
• No dangerous moves and obeying trafﬁc rules.

For each scenario, experts repeatedly drive in the simulator to ﬁnish episodes with 200 steps. Tra-
jectories violating the aforementioned criteria are rejected. After collecting 1000 trajectories in
each scenario, we each choose 100 trajectories that meet the above criteria as a demonstration. To

13

improve the generalization ability of the learned policy, stochasticity was added into the expert’s
steering and throttle during the collecting procedure. Namely, a random conﬁguration is sampled
at the beginning of each episode, the expert would choose different strategies to ﬁnish the current
scenario. Aggressive strategies may include rash steering and throttle.

The statistic results of expert data are listed as Table 3. Note that the ‘speed’ column in the Table 3 is
not the expert data statistics but only used to distinguish sub-environments. For example 26.00±1.00
indicates the expert data collected under the Crossroad Turn Left (26.00 ± 1.00) sub-environment.
Statistical indicators include Collision Rate, Time Taken, Accel. and Jerk.

Table 3: Statistics of Expert Data

Scenarios

Speed (km/h)

Empty Town
Single Lane
Car Following
Two Lanes
Car Following
Crossroad
Merge

Roundabout
Merge

Crossroad
Turn Left

Overtake

/

15.00 ± 1.00

18.00 ± 1.00

21.00 ± 1.00

25.00 ± 1.00
21.00 ± 1.00

25.00 ± 1.00

35.00 ± 1.00
26.00 ± 1.00

42.00 ± 1.00
25.00 ± 1.00,
40.00 ± 1.00

Collision
Rate (%)
0.00

Time Taken
(s)
280.21±10.48

Accel.
(m/s2)
1.59 ± 0.23

Jerk (m/s3)

4.25 ± 0.21

0.00

0.00

0.00

0.00
0.00

0.00

0.00
0.00

0.00

0.00

23.21 ± 2.98

2.01 ± 0.59

2.15 ± 0.42

48.56 ± 1.79

2.62 ± 0.51

7.12 ± 0.49

47.95 ± 4.21

2.42 ± 0.45

1.54 ± 0.48

48.87 ± 1.21
36.23 ± 3.69

2.63 ± 0.43
2.11 ± 0.19

1.69 ± 0.19
1.69 ± 0.60

34.91 ± 0.34

2.01 ± 0.32

1.65 ± 0.19

42.17 ± 0.59
34.91 ± 3.33

2.59 ± 0.92
1.93 ± 0.21

1.69 ± 0.21
2.45 ± 0.21

35.01 ± 3.01

2.19 ± 0.11

2.72 ± 0.31

33.52 ± 1.65

4.99 ± 0.51

4.95 ± 0.55

Evaluation Metrics We compare the learned decision module with both the rule-based module
and the end-to-end neural control policy. Rule-based module is composed following [35]. The
end-to-end policy is trained with vanilla GAIL. Following [36], we conduct quantitative comparison
including metrics like collision rate, time to accomplish tasks, average acceleration, and jerk after
100 trials in evaluation environments. These metrics reﬂect how safe and smooth the agent drives.
The trajectory from different approaches is also visualized and compared qualitatively. We provide
demo videos in the supplementary.

B.2 Baseline Methods

Rule-based Method The rule-based system in our paper is based on the formal model provided
in the paper of MobileEye [35]. This paper proposed the deﬁnitions of the safe longitudinal and
lateral distance and designed a car-following model. For example, the vehicle should keep the safe
longitudinal distance to the front car and avoid collision with the rear car. Since the paper didn’t
mention the overtake scenario, we divided the overtake scenario into three stages and design rules
with the instructions of the formal model. In the ﬁrst stage, the car-following stage, the vehicle
would make decisions instructed by the car-following model. In the second stage, the turning-left
stage, the agent would decide when to turn left according to the safe longitudinal and lateral distance,
then turning-left with high speed. In the last stage, the turning-right stage, similar to the turning-left
decision, the agent would turn right back at a reasonable speed.

End-to-end Neural Policy The end-to-end method used in our paper is GAIL [21]. Nothing has
changed except that the state space and action space have been adapted to the CARLA environment.

B.3 Other Training Details

Reparameterization The planning and control module are both deterministic. Hence at every
state, once the decision is chosen, the transformation from decision to control is a deterministic and

14

correspondence mapping. The reparameterization trick can be seen in Equation 5. We reparametrize
u in the ﬁrst expectation with d. In other words, the conditional probability of d is the same as u.

Decision Space Human decisions can be divided into three parts: which lane to go, how far to
go, at which speed to go. We model the three parts as three categorical variables to simplify its
representations. To make a tractable inversion of the collected data, a 10 meters interval is used
in the longitudinal goal while a 10km/h interval in the target speed. The interval is based on the
feedback from our human drivers.

Policy Architecture and Hyperparameters The policy architecture of the decision module in our
method is exactly the same as the architecture in the paper of GAIL [21]. For a fair comparison, all
hyperparameters are also the same. Learning-based Module method and End-to-end Neural Policy
baseline are trained for 500 iterations before evaluation while Rule-based method baseline does not
require training and can be evaluated directly. The maximum time step for each iteration is 1000.
We use a two-layer ReLU network with 32 units for the discriminator of GAIL and our method. For
the policy, we use a two-layer (32 units) ReLU neural network. Entropy regularizer weight is set to
be 0.1 across our method and GAIL. We use a batch size of 512 steps per update. The learning rate
for generator and discriminator is 0.0003.

C More Experiment Results

C.1 Policy Distillation (Generalization Capability)

The disadvantage of end-to-end learning is not just that there are no guarantees, but also that gener-
alization is tricky. Generalization capability is crucial for our method. There are three experiments
that provide evidence for the generalization capability of our method.

• Different towns: The Empty town model trained in Town3 can be applied in Town2 and

Town1 (cf. attached video).

• Different scenarios: The model trained in Crossroad Merge also performs well in Round-

about Merge and Single Lane Car Following (cf. One Policy for Various Scenarios).

• Different velocities: The Crossroad Merge model trained with the velocity of 25km/h also

performs well at 15km/h (cf. One Policy for Various Speeds).

The aforementioned experiments train one speciﬁc model for each scenario separately. To test the
practical potential of the proposed framework, we train one policy with demonstration data for
multiple scenarios and multiple speeds. This policy distillation problem is divided into: I) One
Policy for Various Scenarios and II) One Policy for Various Speeds.

For One Policy for Various Scenarios, the policy distillation process runs as follows: We leverage the
same demonstration data as the previous experiments in four scenarios (Single Lane Car Following,
Crossroad Merge, Roundabout Merge and Crossroad Turn Left) together to train one policy for these
scenarios jointly. Four CARLA simulators run in parallel, each corresponding to one scenario, while
only one learner is deployed to learn a policy from all data sampled simultaneously. Then we can
obtain one policy model for all scenarios.

For One Policy for Various Speeds, we leverage the same demonstration data for each scenario.
However, the demonstration data for each scenario contains various speeds (as Table 2 shows). We
train a policy for each scenario using corresponding demonstration data and environments at various
speeds, while only one learner is deployed to learn one policy from all data sampled from different
speeds simultaneously. Finally, one policy is learned for various speeds.

Table 4 shows the statistics of behaviors of the learning-based modular system for various scenarios.
After the training is ﬁnished, the learned policy is evaluated on the evaluation environment of dif-
ferent scenarios respectively (100 trials each). We also evaluated the learned policy on the other two
unseen environments and reported the average results of 100 trials in Table 4. Table 5 6 7 shows the
statistics of behaviors of the learning-based modular system for various speeds. The learned policy
is evaluated on the evaluation environment of various speeds respectively (100 trials each). The
distilled policy shows a slight performance loss compared to the policies previously trained sepa-
rately in the case of one speciﬁc scenario. But the performance degradation is within the acceptable

15

range. The above policy distillation results (One Policy for Various Scenarios and One Policy for
Various Speeds) exhibits the potential that our framework can handle scenarios in different levels of
complexity concurrently without compromising performance.

Table 4: Generalized to Various Scenarios

Scenarios

Single Lane Car Following
Crossroad Merge
Roundabout Merge
Crossroad Turn Left
Two Lane Car Following
(Unseen)
Overtake (Unseen)

Collision
Rate (%)
2.00
8.00
14.00
14.00

6.00

18.00

Learning-based Module

Time Taken
(s)
24.35 ± 1.01
52.83 ± 0.61
40.99 ± 1.26
35.21 ± 2.29

Accel.
(m/s2)
2.41 ± 0.56
2.92 ± 0.21
2.21 ± 0.67
4.81 ± 0.95

Jerk
(m/s3)
1.92 ± 0.22
5.31 ± 0.33
1.82 ± 0.32
5.22 ± 0.68

50.35 ± 4.81

2.81 ± 0.61

7.92 ± 0.53

35.62 ± 1.16

5.63 ± 0.55

5.43 ± 0.72

Table 5: Generalized to Various Speeds (Crossroad Merge)

Learning-based Module

Scenarios

Speed
(km/h)

Crossroad
Merge

21.00±1.00

Collision
Rate (%)
10.00

Time Taken
(s)
51.82 ± 4.32

Accel.
(m/s2)
2.99 ± 0.44

Jerk
(m/s3)
5.52 ± 0.79

25.00±1.00

12.00

47.83 ± 5.21

2.83 ± 0.35

5.69 ± 0.58

Table 6: Generalized to Various Speeds (Roundabout Merge)

Learning-based Module

Scenarios

Speed
(km/h)

21.00±1.00

Collision
Rate (%)
14.00

Time Taken
(s)
41.52 ± 5.22

Accel.
(m/s2)
2.63 ± 0.31

Jerk
(m/s3)
1.72 ± 0.35

Roundabout
Merge

25.00±1.00

12.00

39.56 ± 6.21

2.09 ± 0.39

1.70 ± 0.41

35.00±1.00
(Unseen)

14.00

37.15 ± 5.59

2.01 ± 0.49

1.72 ± 0.42

Table 7: Generalized to Various Speeds (Crossroad Turn Left)

Learning-based Module

Scenarios

Crossroad
Turn Left

Speed
(km/h)

26.00±1.00

Collision
Rate (%)
14.00

Time Taken
(s)
36.49 ± 3.21

Accel.
(m/s2)
2.15 ± 0.29

Jerk
(m/s3)
2.99 ± 0.32

42.00±1.00

16.00

38.53 ± 3.18

2.09 ± 0.81

2.72 ± 0.69

16

