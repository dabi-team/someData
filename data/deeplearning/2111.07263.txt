Code Representation Learning Using Pr ¨ufer Sequences

Tenzin Jinpa and Yong Gao
Department of Computer Science
University of British Columbia, Okanagan
Kelowna, BC, Canada
tenzin.jinpa@ubc.ca , yong.gao@ubc.ca

1
2
0
2

v
o
N
4
1

]
I

A
.
s
c
[

1
v
3
6
2
7
0
.
1
1
1
2
:
v
i
X
r
a

Abstract

An effective and efﬁcient encoding of the source code
of a computer program is critical to the success of
sequence-to-sequence deep neural network models for
tasks in computer program comprehension, such as au-
tomated code summarization and documentation. A sig-
niﬁcant challenge is to ﬁnd a sequential representation
that captures the structural/syntactic information in a
computer program and facilitates the training of the
learning models.
In this paper, we propose to use the Pr¨ufer sequence of
the Abstract Syntax Tree (AST) of a computer program
to design a sequential representation scheme that pre-
serves the structural information in an AST. Our rep-
resentation makes it possible to develop deep-learning
models in which signals carried by lexical tokens in
the training examples can be exploited automatically
and selectively based on their syntactic role and im-
portance. Unlike other recently-proposed approaches,
our representation is concise and lossless in terms of
the structural information of the AST. Empirical stud-
ies on real-world benchmark datasets, using a sequence-
to-sequence learning model we designed for code sum-
marization, show that our Pr¨ufer-sequence-based repre-
sentation is indeed highly effective and efﬁcient, out-
performing signiﬁcantly all the recently-proposed deep-
learning models we used as the baseline models.

Introduction

The use of deep learning techniques and related neural ma-
chine translation (NMT) models in code understanding has
drawn much recent attention. It has been shown that methods
using NMT models can achieve a much better performance
than traditional Informational Retrieval (IR) techniques in
tasks such as automated code summarization (Haije et al.
2016; Zhang et al. 2020) and program property prediction
(Alon et al. 2018), thus improving productivity (LaToza
et al. 2006) and reducing software development costs (Srid-
hara et al. 2010). Unlike natural languages, which are un-
structured and noisy, computer programs are highly struc-
tured. It is thus critical to encode as much as possible the
structural information in a sequence-to-sequence learning
model and to take advantage of the encoded information in
the training.

1

Recently, several methods have been proposed to incorpo-
rate the structural information in the Abstract Syntax Tree
(AST) of a computer program in a sequence-to-sequence
learning model. In the Structure-Based Traversal (SBT)
method proposed by Hu et al. (2018a), an AST is repre-
sented by a sequence of syntactic tokens and is generated by
a depth-ﬁrst traversal of the AST with parentheses pairs to
retain the sub-tree information. The model Code2Seq (Alon
et al. 2018) uses the concatenation of the token sequences
along the paths between pairs of terminal nodes in an AST
to represent a computer program. While these methods and
models have been shown to be effective in comparison to
the most straightforward representation by “ﬂat” sequence
of tokens arranged in the same order as they appear in the
source code, the choices of the traversal method and the or-
dering of the tokens appear to be still arbitrary. For example,
for a standard tree traversal method such as the depth-ﬁrst
search, there are at least three ways to order the tokens in
the sequence: in-order, pre-order, and post-order. We also
note that the length of these sequence representations (and
thus the input dimension of the learning model) is signiﬁ-
cantly increased. Due to the use of parentheses for sub-tree
information, the length of the representation proposed by Hu
et al. (2018a) is, in the worst case, three times as long as that
of the source code. The length of the representation used in
the model Code2Seq (Alon et al. 2018) is, in the worst case,
cubic in the length of the source code.

In this paper, we propose to use the Pr¨ufer sequence of
the Abstract Syntax Tree (AST) of a computer program to
design a sequential representation scheme that preserves the
structural information in an AST. Our representation makes
it possible to develop deep-learning models in which sig-
nals carried by lexical tokens in the training examples can be
exploited automatically and selectively based on their syn-
tactic role and importance. Unlike other recently-proposed
approaches, our representation is concise and lossless due to
the fact that an AST can be uniquely reconstructed from its
Pr¨ufer sequence. Empirical studies on real-world benchmark
datasets, using a sequence-to-sequence learning model we
designed for code summarization, showed that our Prufer-
sequence-based representation is indeed highly effective and
efﬁcient, and our model outperforms signiﬁcantly all the
recently-proposed deep-learning models used as the base-
lines in the experiments.

 
 
 
 
 
 
Figure 1: Pr¨ufer Sequences and Context of an AST

Previous Work
The application of deep learning models in representation
learning for computer programs has attracted much recent
attention and is widely used for many tasks in computer pro-
gram comprehension such as automated code summariza-
tion (Wan et al. 2018; Haque et al. 2020; Ahmad et al. 2020),
code generation (Balog et al. 2016), and code retrieval (Al-
lamanis et al. 2015).

Several approaches have been investigated to making use
of syntactic and structural information, explicitly or implic-
itly, in representation learning from the source code of com-
puter programs. Raychev et al. (2016b) used the relations
in the abstract syntax tree as a feature for training a learn-
ing model. Bielik et al. (2016) and Raychev et al. (2016a)
used the paths in an AST to identify the context node. Hu
et al. (2018a) proposed a Structure-Based Traversal (SBT)
method to represent ASTs as a linear sequence contain-
ing syntactic information in a sequence-to-sequence learn-
ing model for code summarization. Hu et al. (2020) further
extended their SBT-based model (Hu et al. 2018a) by adding
to their model another encoder that learns from lexical in-
formation in the source code. Another representation (Alon
et al. 2019) uses the concatenation of the token sequences
along the paths between pairs of terminal nodes in an AST.
In addition to these efforts of directly using the structural
information in the ASTs in a sequence-to-sequence model,
Ahmad et al. (2020) explored the possibility of exploiting
structural information implicitly with a transformer model
enhanced by pariwise semantic relationships of tokens in

the model’s attention mechanism. A signiﬁcant downside of
a transformer-based approach is the increase of the model
complexity (quadratic in the code length), and thus the sam-
ple complexity and the computational complexity.

Abstract Syntax Trees and Pr ¨ufer Sequences
Pr¨ufer sequences have been used in the past as a sequential
representation of tree structures in stochastic search methods
(Molla-Alizadeh-Zavardehi et al. 2011; Hashemi and Tari
2018), problems in fuzzy systems (Nayeem and Pal 2013;
Xu et al. 2008), and hierarchical or graph data management
(Yang and Wang 2020; Pradhan and Bhattacharya 2019). To
the best of our knowledge, our work is the ﬁrst effort in using
Pr¨ufer sequences and exploiting their unique properties in
(deep) representation learning for structured data.

In this section, we discuss the details of our Pr¨ufer-
sequence-based code representation, including its main idea
and its construction. We also discuss the advantages of our
representation over other sequential representations in terms
of effectiveness, efﬁciency, and ﬂexibility.

ASTs and Sequence to Sequence Models

An abstract syntax tree (AST) is a tree structure that mod-
els the abstract syntactic structure of the source code of
a computer program. In an AST, the leaves (or terminal
nodes) are labeled by tokens that contain user-deﬁned val-
ues or reference types such as variable names, whereas in-
ternal nodes (or non-terminal nodes) are labeled by tokens

that summarize the purpose of code blocks such as condi-
tions, loops, and other ﬂow-control statements. We note that
a token labeling an internal node does not have to be from
the source code or speciﬁcation of the particular program-
ming language. By slightly abusing the notion, we call a to-
ken labeling a leave node a “lexical token” as it contains
program-speciﬁc information in the source code and call a
token labeling a non-terminal node a “syntactic token” as
it contains generic information about the structure and the
purpose of a code block. Shown in Fig.1 is a function in
Java and its AST where the token set {Override, Int, String,
mergeErrorIntoOutput, Boolean, Commands} are used to la-
bel the terminal nodes and the token set {BasicType, Formal-
Parameter, MethodInvocation, ReturnStatement, ClassCre-
ator, ReferenceType} are used in label the internal nodes.

In a sequence-to-sequence model for code representation
learning, tokens (or their word embedding) from the source
code have to be represented as a linear sequence and are used
as the input of the model. As has been shown by Hu et al.
(2018a) and Alon et al. (2019), a linear sequence represen-
tation that makes use of structural information encoded in
the AST has a great advantage over a simple ﬂat sequence
representation where tokens appear in the same order as they
appear in the source code (Iyer et al. 2016).

Pr ¨ufer Sequence of an AST

The Pr¨ufer sequence of a node-labeled tree is a sequence
of node labels from which the tree can be uniquely recon-
structed. The famous proof of Caley’s formula for the num-
ber of labeled trees by Heinz Pr¨ufer (Pr¨ufer 1918; West
2000) is based on the one-to-one correspondence between
the set of labeled trees and the set of such sequences. Given
a tree T with n nodes labeled by the integers {1, · · · , n}, its
Pr¨ufer sequence is a sequence of (n − 2) node labels (i.e.,
integers) and can be formed by successively removing the
leave with the smallest label and including the label of its
parent as the next node label in the Pr¨ufer sequence. The
process stopped when only two nodes were left in the tree.

Since ASTs are labeled by syntactic and lexical tokens,
we use a ﬁxed mapping to map each token in the given to-
ken set to a unique integer and use it as the integer label of
the AST-node that is labeled by the token. The Pr¨ufer se-
quence constructed from this integer-labeled AST is then
mapped back to a sequence of syntactic tokens, which we
call the “syntactic Prufer sequence” and is used as part of
the input sequence to our learning model. Fig. 1 illustrates
the process of Pr¨ufer sequence generation from the AST of
a Java method. The syntactic Pr¨ufer sequence for the Java
method in Fig. 1 is

{MethodDeclaration, MethodDeclaration, FormalPa-
rameter, MethodDeclaration, FormalParameter, Method-
Declaration, ReturnStatement, MethodInvocation, Method-
Invocation, ClassCreator, TypeArgument, ReferenceType,
ClassCreator, MethodInvocation}

Note that in the above sequence, a syntactic token may
appear multiple times and at different positions. Also, note
that the terminal nodes never appear in the sequence. The
signiﬁcance and relevance of those properties of the syntac-

tic Pr¨ufer sequence will be discussed below and further ex-
plored in the next two sections on the design of our learning
model and our empirical studies.

Advantages of Learning with a Pr ¨ufer-Sequence
Representation
The syntactic Pr¨ufer sequence can be regarded as a “trans-
formed” and “quantiﬁed” version of an AST and the corre-
sponding source code where

1. the frequency with which a syntactic token appears is de-
cided by the degree of the corresponding AST node and
quantiﬁes the “importance” of the token (measured the
size of the code block it controls);

2. the positions of the appearances of syntactic tokens in the
Pr¨ufer sequence are decided by the position of the corre-
sponding node in the tree; and

3. a lexical token labeling a leave node of an AST never ap-
pears in the Pr¨ufer sequence, but its “signiﬁcance” can be
measured by the syntactic importance of the parent of the
leave node.

This is in sharp contrast with all other recently proposed se-
quential representations of a source code and its AST, where
all tokens are treated equally, and their positions only par-
tially capture their roles in the AST.

Supported by observations from our empirical studies
(discussed in the second last section), we believe that these
are the properties that make it possible (or much easier) for
our learning models to exploit information in the training
examples that are hard, if not impossible, for other recently-
proposed learning models to detect.

As we observed in our experiments, other properties that
distinguish our Prufer-sequence representation from exiting
representations and play important roles in the performance
of our learning model are as follows.

1. A Uniqueness and Lossless Representation

To our best knowledge, all existing sequential represen-
tations of source code and its AST (Alon et al. 2018; Hu
et al. 2018a) are lossy encoding in the sense that the orig-
inal AST cannot be uniquely reconstructed. Our Pr¨ufer-
sequence representation is a lossless encoding because,
given a ﬁxed syntactic-token-to-integer mapping, there is
a one-to-one correspondence between the set of ASTs and
their syntactic Pr¨ufer sequences. This property may help
improve the ability of a learning method to distinguish or
detect subtle differences in training examples.

2. More Concise Input (or Lower Input Dimension)

For an AST with n nodes, the length of our representa-
tion is n − 2. In comparison, the length of the represen-
tation proposed by Hu et al. (2018a) is 3n in the worst
case, while the length of the representation proposed by
Alon et al. (2019) is in Ω (n3) in the worst case. In our
experiments, we observed that the Pr¨ufer sequence rep-
resentation has an average length of 100.81 tokens while
the representation based on SBT (Hu et al. 2018a) has the
193.71 tokens to represent the same AST corpus (Table
5), resulting in faster training of our model.

Figure 2: Pr¨ufer Based Learning Model for Code Summarization

Pr ¨ufer-Sequence-Based Learning Model For
Code Summarization
To study the effectiveness of the Pr¨ufer-sequence-based rep-
resentation, we developed a deep-learning model for code
summarization. The model maps a Java method to a sum-
mary of the method’s purpose in English. The training data
are pairs of source code of the Java method and develop-
ers’ comments. The high-level structure of our model is de-
picted in Fig.2. It is a sequence-to-sequence (seq2seq) model
(Sutskever et al. 2014) in the encoder-decoder paradigm,
where two separate encoders are used to learn from the struc-
tural information of an AST and from a structure-aware rep-
resentation of the lexical tokens from the source code. An at-
tention module, similar to the one used by Hu et al. (2020),
is used to combine the output of the two encoders into a
context vector which is then used as the input to a standard
decoder described in (Sutskever et al. 2014) to output a code
summary/comment in English.

We describe the details of the two encoders and their ra-

tionale below.

Pr ¨ufer Sequence Encoder
The Pr¨ufer Sequence Encoder is designed to learn from
the structural information of the ASTs that are losslessly
encoded in their syntactic Pr¨ufer sequences. Gated Recur-
rent Units (GRUs), as discussed by (Cho and van Merrien-
boer 2014), are used to map the syntactic Pr¨ufer sequence
(X = x1, ...., xn) of a computer program to a sequence of
hidden states as follows:

st = GRU (xt, st−1)
In our implementation, lexical tokens labeling the termi-
nal nodes are appended to the syntactic Pr¨ufer sequence as
part of the input to the encoder, resulting in an input length
of at most 2n−3. We observed in our initial experiments that
the model Hybrid-DeepCom (Hu et al. 2020) with its SBT-
based encoder replaced by our Pr¨ufer Sequence Encoder al-

ready outperformed notably the original Hybrid-DeepCom
Model and other baseline models. It turned out that the bot-
tleneck to further improvement of such models is the design
of the second encoder, the Source Encoder (Hu et al. 2020),
that learns from source code tokens directly. This observa-
tion in our early investigation motivated us to design our own
second encoder, which we call the Context Encoder, that ex-
ploits lexical information in a structure-aware way. We dis-
cuss our deﬁnition of the context of an AST and the design
of the Context Encoder in the next subsection.

Context Encoder
The context encoder, also consisting of GRUs, is designed to
learn from the collection of lexical tokens (i.e., user-deﬁned
and program-speciﬁc values in the source code) organized
in a way that reﬂects the structural information of the AST.
For each node in an AST, we deﬁne its context to be the
set of lexical tokens that label the node’s leaf child/children.
A node with no leaf child has an empty context. The context
of an AST is deﬁned to be the union of the contexts of the
AST nodes ordered in the same way as they appear in the
syntactic Pr¨ufer sequence. The context of an AST can be
calculated from the AST’s Pr¨ufer sequence (See Fig. 1). The
context encoder maps the context deﬁned in the above to a
sequence of hidden states.

The context of an AST deﬁned in this way is a structure-
aware sequence of lexical tokens because (1) the frequency
of a lexical token is decided by the degree of the parent of
the leaf node that the token labels; and (2) the order in which
these tokens appear in the context is the same as the order
of the parent nodes in the Pr¨ufer sequence. As observed in
our experiments, the use of the context encoder helps boost
the performance of our learning model signiﬁcantly. This is
because, we believe, that the context we have designed helps
amplify the learning-relevant lexical signals in the source
code in a way that other models (such as those in Hu et al.
(2020)) cannot detect that simply use the collection of entire

tokens as they appear in the source code.

Experimental Studies

In this section, we ﬁrst introduce the experimental setup, in-
cluding the datasets, the baseline models, and the evaluation
metrics. We then discuss observations from and analyses of
our experimental results on the power, effectiveness, and ef-
ﬁciency of the proposed Pr¨ufer sequence representation and
the role it plays for our learning model to signiﬁcantly out-
perform recently-proposed learning models.

Dataset and Experiment Setup

We perform our experiment on two Java datasets. Dataset-
1(68469 pairs of java code and comments) is a popular
dataset used in many research and is collected by Hu et al.
(2018b) from popular repositories in Github. Dataset-2 (
163316 pairs of java code and comments) is part of the
CodeXGlue dataset developed by Microsoft (Lu et al. 2021).
It is known for its high quality and complexity and is be-
lieved to be one of the most challenging datasets for deep
learning approaches to program understanding and genera-
tion. We split the data into 8:1:1 for training, testing, and
validation. Some statistics of the datasets are shown in Ta-
bles 1 and 2.

We tokenized the java source code and comments by the
programs Javalang and NLTK1 respectively. The size of the
vocabulary for comments, code, code’s context, and Pr¨ufer
sequence is set to 30000 (Hu et al. 2018a; Sutskever et al.
2014). The maximum length of the Pr¨ufer sequence was
kept at 200, and the size of the code and code’s context was
kept at 500 (Hu et al. 2018a; Sutskever et al. 2014). Spe-
cial tokens <START> representing the start of the sequence
and <EOS> representing the end of the sequence are added
to the decoder sequence during the training. The maximum
comment length is set to 30 and out-of-vocabulary repre-
sented by especial token <UNK>.

Our model uses one layered GRU with 256 dimensions
of hidden state and 256-dimensional word embedding. The
maximum iterations are 60 epochs. The learning rate is set to
0.5, and we clip the gradients norm by 5. The learning rate is
decayed using the rate 0.99. The model uses the TensorFlow
version 1.15, and we train our model on a single GPU of
Tesla P100-PCIE-16GB with 25 GB RAM and 110GB disk.

Table 1: Statistics of the Java Methods
Dataset Type Avg. Mode Median <200 Tokens
89.48%
16
Dataset-1
89.29%
42
Dataset-2

97.05
98.35

64
69

Table 2: Statistics of the Java comments
Dataset Type Avg. Mode Median <30 Tokens
88.42%
Dataset-1
97.33%
Dataset-2

13
11.02

11
9

7
7

1https://www.nltk.org/

Baseline
In our experiments, we compared the performance of our
learning model with the following baseline models to em-
pirically analyze the power, effectiveness, and efﬁciency of
the proposed Pr¨ufer-sequence-based representation.

1. TL-CodeSum Model (Hu et al. 2018b). This is an NMT
based code summarization method that uses API knowl-
edge and source code tokens as the input in a sequence-
to-sequence model.

2. Hybrid-DeepCom Model

(Hu et al. 2020). This
sequence-to-sequence model for code summarization is
one of the recent models that is designed to exploit struc-
tural information in the AST of a computer program. It
uses two encoders: a source-token encoder and an SBT
encoder. The SBT encoder uses a depth-ﬁrst-traversal se-
quence representation of an AST as its input, and the
code-token encoder is used to learn from the lexical in-
formation of the source code.

3. Code2Seq Model (Alon et al. 2019). This is a deep-
learning model for general code representation learning.
Code2Seq uses the concatenation of the token sequences
along the paths between pairs of terminal nodes in an AST
as its input representation.

4. BFS-Hybrid-DeepCom. This model

is based on the
Hybrid-DeepCom Model (Hu et al. 2020) with the SBT
representation of an AST replaced by a sequence repre-
sentation constructed from a breadth-ﬁrst-search (BFS)
traversal of the AST. We included this model to help ver-
ify our claim that all the recently-proposed sequence rep-
resentations are more or less arbitrary. As observed from
our experiments (see next Section), this BFS-based model
performs equally well (or even slightly better) than those
recently proposed models.

5. Lexical-Token-Only Model. This basic attention-based
seq2seq model has only one encoder that learns from the
lexical information in the source code. We used this model
to understand the importance of incorporating syntactic
information (in the AST) in deep-learning approaches for
code summarization. The parameter setting of the model
is similar to that of the Hybrid-DeepCom model.

Metrics
To evaluate the effectiveness of different approaches, we use
four widely-used machine translation metrics: two BLEU
scores, the METEOR score, and ROUGE-L. The BLEU
Score (Papineni et al. 2002) is a family of metrics to check
the quality of machine-translated texts against that of the
human-written texts. In this paper, use the Sentence-Level
BLEU score (S-BLUE) with smoothing-4 method (Chen
and Cherry 2014) and Corpus-level BLEU (C-BLUE) and
computed them by a program “multi-bleu.perl”. METEOR
is a is recall-oriented evaluation method (Denkowski and
Lavie 2014), which evaluates the translation hypotheses
by aligning them to reference translations and calculat-
ing sentence-level similarity scores (Denkowski and Lavie
2014). ROUGE-L (Lin 2004) is one of the four measures
of the ROUGE family, where L stands for Longest Common

Table 3: Effectiveness of Models based on Machine Translation metrics for Dataset-1

Model
Meteor ROUGE-L
Lexical-Token-Only Model
40.78
Code2Seq
20.63
TL-CodeSum
41.29
BFS-Hybrid-DeepCom
41.03
Hybrid-DeepCom
41.15
Our Model (Pr ¨ufer Encoder + Hu’s Source Encoder) 38.38 (0.5%) 29.43 (0.5%) 20.13 (1.3%) 41.82 (1.3%)
39.67 (3.3%) 31.01 (5.7%) 21.01 (5.6%) 43.45 (5.1%)
Our Model (Pr ¨ufer Encoder + Context Encoder)

C-BLEU
27.30
4.56
28.43
29.08
29.28

S-BLEU
36.21
20.72
37.20
37.98
38.19

19.01
10.21
19.64
19.72
19.87

Table 4: Effectiveness of Models based on Machine Translation metrics For Dataset-2
Model
Meteor
Lexical-Token-Only Model
7.96
Code2Seq
3.5
BFS-Hybrid-DeepCom
7.29
8.27
Hybrid-DeepCom
Our Model (Pr ¨ufer Encoder + Hu’s Source Encoder) 15.50 (3.15%) 3.85 (3.97%) 8.9 (6.925%)
Our Model (Pr ¨ufer Encoder + Context Encoder)

ROUGE-L
19.84
12.23
20.42
18.01
20.79 (1.8%)
16.15(7.02%) 4.49 (19.29%) 9.72 (15.05%) 24.73 (19.09%)

C-BLEU
3.07
0.30
3.47
3.7

S-BLEU
9.21
2.27
13.41
15.02

Subsequence. It computes the F-score (deﬁned as the har-
monic mean of the recall and precision value from ﬁnding
the longest common sequence of the texts.)

Analysis of Experimental Results and Observations

In Tables 3 and 4, we summarize the experiment results on
the effectiveness of our Pr¨ufer-based learning model and the
baseline models for code summarization on two public Java
datasets: a) Dataset-1 (Hu et al. 2018b) and b) Dataset-2
(Lu et al. 2021). We note that the performance scores for
Dataset-2 is much lower, but this is not a surprise—as we
mentioned in the previous section, this dataset is believed
to be one of the most challenging dataset for deep learning
approaches to program understanding and generation. A de-
tailed discussion can be found on CodeXGlue’s project web-
page (https://microsoft.github.io/CodeXGLUE/). Also, note
that in Table 4 (for Dataset-2), we do not have a row for the
model TL-CodeSum because Dataset-2 does not provide the
API knowledge required to train the model. As we can see,
the model that uses our Pr¨ufer Sequence Encoder and Hu’s
Source Encoder (second last rows in both Tables 3 and 4) has
already had notable improvement over the baseline models.
The improvement by our complete Pr¨ufer-sequence-based
model (using our Pr¨ufer Sequence Encoder and our Context
Encoder) is even more signiﬁcant, especially on Dataset-2
(last rows in both Tables 3 and 4.)

In the rest of this section, we discuss our observations
from the experiments and analyze the results to under-
stand the power, effectiveness, efﬁciency, and the robustness
(against the code length) of our Pr¨ufer-sequence-based rep-
resentation. These observations and their analyses are solid
evidence, supporting our belief that our Pr¨ufer-sequence-
based representation makes it possible (or much easier) for
learning models to exploit information in the training ex-
amples that are hard, if not impossible, for other recently-
proposed learning models to detect.

Power and Effectiveness of Pr ¨ufer-Sequence Represen-
tation of AST. As shown in the second last rows in Tables
3 and 4, our Pr¨ufer Sequence Encoder and Hu’s Source En-
coder (second last rows in both Tables 3 and 4) has already
had notable improvement over the baseline models, with the
average performance improvement being 0.9% for Dataset-1
and 3.96% for Dataset-2. We attribute the performance im-
provement to the properties of the Pr¨ufer sequence repre-
sentation discussed in the previous Section (Abstract Syn-
tax Trees and Pr¨ufer Sequences): a concise and lossless en-
coding that quantiﬁes the “importance” of syntactic tokens
and preserves their structural roles in describing the source
code. This is further supported by the relatively poor perfor-
mance of the recently-proposed general model, Code2Seq,
that uses a lossy encoding with the input length cubic to the
size of the AST in the worst case. Note that the performance
of the Code2Seq Model is worse than the model that does
not make use of any structural information of the AST (ﬁrst
rows in both tables.)

We can also see that the performance of the two versions
of the Hybrid-DeepCom Model based on respectively the
BFS sequence and the SBT sequence are comparable on
both datasets, justifying our claim in the introduction that in
such traversal-based sequence representation recently pro-
posed, the order of the tokens in a sequence is largely arbi-
trary and only partially captures the structure of an AST.

Importance of Structure-Aware Context Sequence of
Lexical Tokens. As shown in the last row in both Tables
3 and 4, the use of the Context Encoder boosted the perfor-
mance of our model dramatically. The average performance
improvement over baseline models is increased from 0.9%
to 5% for Dataset-1 and 3.96% to 15.11% for Dataset-2.

Considering that both of the two recently-proposed deep-
learning models (Code2Seq and Hybrid-DeepCom, the sec-
ond rows and the third last rows in both tables) are also de-
signed to make use of structural information of an AST as
well as lexical tokens from the source code, the signiﬁcant

performance gain of our model is best interpreted by the fact
that the “context” sequence deﬁned in our model as the in-
put to the Context Encoder is structure-aware. The differ-
ence between our context sequence and the those used in the
Hybrid-DeepCom model and the Code2Seq model is that in
our context sequence, the frequency of a lexical token from
the source code is decided by the degree of its parent node
in the AST, whereas Hybrid-DeepCom and Code2Seq treat
tokens from the source code equally regardless of their role
and signiﬁcance in the program.

Efﬁciency of the Pr ¨ufer Sequence Encoding. The input
dimension to a seq-to-seq deep learning model depends on
the encoding scheme. The lower the dimension is, the faster
it is to complete one epoch of training. There is, of course, a
tradeoff among the effectiveness/ability of a model, its input
dimension, and the training time. An ideal encoding is the
one that preserves as much as possible the structural infor-
mation and has a short length.

Figure 3: Time required to train different the models with
different AST representation.

Our experiments conﬁrmed that our Pr¨ufer sequence rep-
resentation, while encoding the structure of an AST loss-
lessly, is more concise than other representations and in-
deed requires less training time. Fig. 3 summarizes our ob-
servation on the time required to complete different train-
ing epochs for three learning models: Our Model, Hybrid-
DeepCom, BFS-Hybrid-DeepCom. Among the models,
BFS-Hybrid-DeepCom uses the shortest sequence represen-
tation (70.21 on average over the training data), and Hybrid-
DeepCom has the longest representation (193.91 on av-
erage). The average length of our Pr¨ufer sequence repre-
sentation is 100.81. It is a surprise to observe that BFS-
Hybrid-DeepCom (a model we customized from Hybrid-
DeepCom using a straightforward and much shorter breadth-
ﬁrst-search-based representation), requires less training time
but has a comparable performance with Hybrid-DeepCom
that is based on a carefully designed and more sophisticated
representation. While our model requires more time to train
than BFS-Hybrid-DeepCom (as expected but not by much),
the performance gain of our model is signiﬁcant.

Performance over Source Code of Different Lengths.
We further analyzed the accuracy of the three models when
they are trained and tested on source codes of different
lengths. We observed (Fig.4) that for all the three models,
the performance decreases as the code length increases. We
note that our Pr¨ufer-sequence-based model performs better

Figure 4: BLEU score for different method lengths

than the other two baseline models regardless of the code
lengths. For java methods with 150 or more tokens, the
Pr¨ufer sequence-based model had a clear edge over the other
two methods, suggesting that it is more robust against the in-
crease of code length.

The correlation coefﬁcient between the BLEU score and
code length is the smallest for the Pr¨ufer-sequence-based
model (-0.037), while for BFS and SBT, it is -0.16 and -
0.19, respectively. This indicates that the (negative) correla-
tion between the performance of and the code length is much
weaker for our model than the other two models.

Table 5: Statistics of the AST Representation Methods for
Dataset-1

Method
Prufer Sequence 100.81
193.71
SBT
70.21
BFS

Avg. Mode Median <200 Tokens
88.06 %
67.74
68.77%
124
94.16%
44

8
24
8

Conclusion
In this work, we proposed a concise and effective representa-
tion scheme that can be used in sequence-to-sequence mod-
els for code representation learning. By encoding structural
information of abstract syntax trees of computer programs,
our Pr¨ufer-sequence-based representation makes it possible
(or much easier) to develop sequence-to-sequence learning
models to exploit automatically and selectively lexical and
syntactic signals that are hard, if not impossible, for other
recently-proposed sequence-to-sequence learning models to
detect.

Ahmad et al. (2020) investigated the possibility of incor-
porating AST information in their transformer-based model
and concluded that AST information does not provide any
help. Our studies in this paper suggest that it depends on how
the (hierarchical) AST information is encoded and used in a
learning model. It is a very interesting future work to study
how our Pr¨ufer-sequence-based encoding of ASTs can be
used in a transformer-based model (such as the one in (Ah-
mad et al., 2020)) to decrease the model’s complexity and
improve its effectiveness.

While the model we developed and the experiments con-
ducted are on the task of code summarization for a particular
program language, no assumptions were made about the pro-
gramming language, its speciﬁcation, and the format of the

abstract syntax trees, making our approach language inde-
pendent and potentially applicable to other tasks in program
comprehension and in any application domains of sequence-
to-sequence learning models where sequential and hierarchi-
cal signals both exist in the training data.

References
Ahmad, W. U., Chakraborty, S., Ray, B., and Chang, K.-
W. (2020). A transformer-based approach for source
code summarization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics
(ACL).

Allamanis, M., Tarlow, D., Gordon, A., and Wei, Y. (2015).
Bimodal modelling of source code and natural language.
In Bach, F. and Blei, D., editors, Proceedings of the
32nd International Conference on Machine Learning,
volume 37 of Proceedings of Machine Learning Re-
search, pages 2123–2132, Lille, France. PMLR.

Alon, U., Brody, S., Levy, O., and Yahav, E. (2019).
code2seq: Generating sequences from structured repre-
sentations of code. In International Conference on Learn-
ing Representations.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. (2018). A
general path-based representation for predicting program
In Proceedings of the 39th ACM SIGPLAN
properties.
Conference on Programming Language Design and Im-
plementation, PLDI 2018, page 404–419. Association for
Computing Machinery.

Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S.,
and Tarlow, D. (2016). Deepcoder: Learning to write pro-
grams. CoRR, abs/1611.01989.

Bielik, P., Raychev, V., and Vechev, M. (2016). Phog: Prob-
In Balcan, M. F. and Wein-
abilistic model for code.
berger, K. Q., editors, Proceedings of The 33rd Interna-
tional Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pages 2933–
2942. PMLR.

Chen, B. and Cherry, C. (2014). A systematic comparison of
smoothing techniques for sentence-level BLEU. In Pro-
ceedings of the Ninth Workshop on Statistical Machine
Translation, pages 362–367, Baltimore, Maryland, USA.
Association for Computational Linguistics.

Cho, K. and van Merrienboer, B. (2014). Learning phrase
representations using rnn encoder-decoder for statistical
machine translation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1724–1734.

Denkowski, M. and Lavie, A. (2014). Meteor universal:
Language speciﬁc translation evaluation for any target
language. In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, pages 376–380, Baltimore,
Maryland, USA. Association for Computational Linguis-
tics.

Haque, S., LeClair, A., Wu, L., and McMillan, C. (2020).
Improved automatic summarization of subroutines via at-
tention to ﬁle context. Proceedings of the 17th Interna-
tional Conference on Mining Software Repositories.

Hashemi, Z. and Tari, F. G. (2018). A prufer-based genetic
algorithm for allocation of the vehicles in a discounted
transportation cost system. International Journal of Sys-
tems Science: Operations & Logistics, 5(1):1–15.

Hu, X., Li, G., Xia, X., Lo, D., and Jin, Z. (2018a). Deep
In Proceedings of the 26th
code comment generation.
Conference on Program Comprehension, ICPC ’18, page
200–210. Association for Computing Machinery.

Hu, X., Li, G., Xia, X., Lo, D., and Jin, Z. (2020). Deep
code comment generation with hybrid lexical and syn-
tactical information. Empirical Software Engineering,
25(3):2179–2217.

Hu, X., Li, G., Xia, X., Lo, D., Lu, S., and Jin, Z. (2018b).
Summarizing source code with transferred api knowl-
In Proceedings of the 27th International Joint
edge.
Conference on Artiﬁcial Intelligence, IJCAI’18, page
2269–2275. AAAI Press.

Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. (2016).
Summarizing source code using a neural attention model.
In Proceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2073–2083. Association for Computational
Linguistics.

LaToza, T. D., Venolia, G., and DeLine, R. (2006). Main-
taining mental models: a study of developer work habits.
In Proceedings of the 28th international conference on
Software engineering, pages 492–501.

Lin, C.-Y. (2004). ROUGE: A package for automatic evalu-
ation of summaries. In Text Summarization Branches Out,
pages 74–81, Barcelona, Spain. Association for Compu-
tational Linguistics.

Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A.,
Blanco, A., Clement, C. B., Drain, D., Jiang, D., Tang, D.,
Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong,
M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K.,
Fu, S., and Liu, S. (2021). Codexglue: A machine learn-
ing benchmark dataset for code understanding and gener-
ation. CoRR, abs/2102.04664.

Molla-Alizadeh-Zavardehi, S., Hajiaghaei-Keshteli, M., and
Tavakkoli-Moghaddam, R. (2011). Solving a capacitated
ﬁxed-charge transportation problem by artiﬁcial immune
and genetic algorithms with a pr¨ufer number representa-
tion. 38(8):10462–10474.

Nayeem, S. M. A. and Pal, M. (2013). Diameter con-
Inter-
strained fuzzy minimum spanning tree problem.
national Journal of Computational Intelligence Systems,
6(6):1040–1051.

Haije, T., Intelligentie, B. O. K., Gavves, E., and Heuer, H.
(2016). Automatic comment generation using a neural
translation model. Inf. Softw. Technol, 55(3):258–268.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
Bleu: a method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting of the

Association for Computational Linguistics, pages 311–
318, Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics.

Pradhan, M. and Bhattacharya, B. B. (2019). A prufer-
sequence based representation of large graphs for struc-
tural encoding of logic networks. In Proceedings of the
ACM India Joint International Conference on Data Sci-
ence and Management of Data, CoDS-COMAD ’19, page
293–296, New York, NY, USA. Association for Comput-
ing Machinery.

Pr¨ufer, H. (1918). Neuer beweis eines satzes ¨uber permuta-
tionen (a new prof. of a theorem on permutations). Archiv
der mathematik und Physik, 3:27.

Raychev, V., Bielik, P., and Vechev, M. (2016a). Probabilis-
In Proceedings
tic model for code with decision trees.
of the 2016 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages, and
Applications, OOPSLA 2016, page 731–747. Association
for Computing Machinery.

Raychev, V., Bielik, P., Vechev, M., and Krause, A. (2016b).
In Proceedings of
Learning programs from noisy data.
the 43rd Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL ’16, page
761–774, New York, NY, USA. Association for Comput-
ing Machinery.

Sridhara, G., Hill, E., Muppaneni, D., Pollock, L., and Vijay-
Shanker, K. (2010). Towards automatically generating
In Proceedings
summary comments for java methods.
of the IEEE/ACM international conference on Automated
software engineering, pages 43–52.

Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence
to sequence learning with neural networks. In Proceed-
ings of the 27th International Conference on Neural In-
formation Processing Systems - Volume 2, NIPS’14, page
3104–3112. MIT Press.

Wan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J.,
and Yu, P. S. (2018). Improving automatic source code
summarization via deep reinforcement learning. CoRR,
abs/1811.07234.

West, D. B. (2000). Introduction to Graph Theory. Prentice

Hall.

Xu, J., Liu, Q., and Wang, R. (2008). A class of multi-
objective supply chain networks optimal model under ran-
dom fuzzy environment and its application to the industry
of chinese liquor. Inf. Sci., 178(8):2022–2043.

Yang, L. and Wang, Y. (2020). Prufer coding: A vectoriza-
tion method for undirected labeled graph. IEEE Access,
8:175360–175369.

Zhang, J., Wang, X., Zhang, H., Sun, H., and Liu, X.
(2020). Retrieval-based neural source code summariza-
In Proceedings of the ACM/IEEE 42nd Interna-
tion.
tional Conference on Software Engineering, ICSE ’20,
page 1385–1397. Association for Computing Machinery.

