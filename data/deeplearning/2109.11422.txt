Programming and Training Rate-Independent
Chemical Reaction Networks

Marko Vasica,1,2, Cameron Chalka,1,2, Austin Luchsingera, Sarfraz Khurshida, and David Soloveichika,2

aThe University of Texas at Austin, USA

This manuscript was compiled on September 24, 2021

1
2
0
2

p
e
S
0
2

]

G
L
.
s
c
[

1
v
2
2
4
1
1
.
9
0
1
2
:
v
i
X
r
a

Embedding computation in biochemical environments incompatible
with traditional electronics is expected to have wide-ranging impact
in synthetic biology, medicine, nanofabrication and other ﬁelds. Nat-
ural biochemical systems are typically modeled by chemical reaction
networks (CRNs), and CRNs can be used as a speciﬁcation language
for synthetic chemical computation. In this paper, we identify a class
of CRNs called non-competitive (NC) whose equilibria are absolutely
robust to reaction rates and kinetic rate law, because their behav-
ior is captured solely by their stoichiometric structure. Unlike prior
work on rate-independent CRNs, checking non-competition and us-
ing it as a design criterion is easy and promises robust output. We
also present a technique to program NC-CRNs using well-founded
deep learning methods, showing a translation procedure from rec-
tiﬁed linear unit (ReLU) neural networks to NC-CRNs.
In the case
of binary weight ReLU networks, our translation procedure is sur-
prisingly tight in the sense that a single bimolecular reaction corre-
sponds to a single ReLU node and vice versa. This compactness ar-
gues that neural networks may be a ﬁtting paradigm for programming
rate-independent chemical computation. As proof of principle, we
demonstrate our scheme with numerical simulations of CRNs trans-
lated from neural networks trained on traditional machine learning
datasets (IRIS and MNIST), as well as tasks better aligned with po-
tential biological applications including virus detection and spatial
pattern formation.

chemical computation | ReLU neural networks | molecular programming

C ompared to our remarkable capacity to build complex

electronic circuits, we lack in our ability to engineer so-
phisticated reaction networks like the regulatory networks
prevalent in biology. Molecular programming aims to engi-
neer synthetic chemical information processors of increasing
complexity from ﬁrst principles. This approach yields control
modules compatible with the chemical environments within
natural or synthetic cells, bioreactors, and in-the-ﬁeld diagnos-
tics. Such computation could, for example, recognize disease
state based on chemical inputs and actuate drug delivery to
the aﬀected cell.

A key object of molecular programming are chemical re-
action networks (CRNs). CRNs formally model chemical
concentrations changing due to coupled chemical reactions
in a well-mixed solution. Biological CRNs are often hard to
analyze because, in general, they require working with systems
of coupled non-linear diﬀerential equations capable of highly
complex dynamical systems behavior such as multi-stability,
oscillation and chaos (1). However, in engineering we may aim
at speciﬁc classes of CRNs that are easier to reason about.
One such class has recently emerged in which information pro-
cessing occurs solely due to the stoichiometric exchange of the
reactants for products rather than the reaction rate (2). An
example of such computation is the single irreversible reaction
A + B → C which computes the minimum function in the

sense that the concentration of C converges to the minimum
of the initial concentrations of A and B. By coupling multiple
reactions, more complex functions can be computed. Although
stoichiometric computation is limited to continuous piecewise
linear functions (with possible discontinuities at the axes), this
class of functions is computationally powerful as evidenced
by the ability to approximate arbitrary functions, as well as
the widespread use of continuous piecewise linear functions
in machine learning (e.g., neural networks with the ReLU
activation function, see below).

Besides ease of analysis, such stoichiometrically computing
CRNs are absolutely robust to variations in kinetics (rate-
independence). Computation carried out by stoichiometry
alone is correct whether the system obeys standard mass-
action kinetics, Hill-function or Michaelis-Menten kinetics, or
any other kinetic laws, and does not err if the system is not
well-mixed. Engineering may also be aided by the fact that,
unlike factors contributing to reaction rates, the stoichiometry
of reactants and products is inherently digital and can be set
exactly by the nature of the reaction. For example, if realized
with DNA strand displacement cascades, the identity and
stoichiometry of reactants and products can be programmed
by synthesizing DNA strands with speciﬁc parts that are
identical or complementary (4–6). Note that such reactions
can be made eﬀectively irreversible as they are strongly driven
by the formation of new base pairs.∗

D R A FT

Although we are motivated mostly by engineering concerns, some biological CRNs may exhibit
similar stoichiometric, rate-independent behaviour as identiﬁed in searches of the Biomodels repos-
itory (7).

Signiﬁcance Statement

∗

To program complex behavior in environments incompatible
with electronic controllers such as within bioreactors or engi-
neered cells, we need to turn to chemical information proces-
sors. While chemical reactions can perform computation in the
stoichiometric exchange of reactants for products (how many
molecules of which reactants result in how many molecules of
which products), the control of reaction rates is usually thought
to allow more complex computation. Motivated by the fact that
correct stoichiometry is easier to ensure than reaction rates,
we provide a method for programming and training chemical
computation by stoichiometry. We show such computation can
be straightforwardly programmed in a manner analogous to
imperative programming, and demonstrate the execution of
neural networks capable of complex machine learning tasks.

A preliminary conference version of this work appeared as ref. (3).

1M.V. contributed equally to this work with C.C.

2To whom correspondence should be addressed. E-mail: vasic@utexas.edu, ctchalk@utexas.edu,
david.soloveichik@utexas.edu

www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

PNAS | September 24, 2021 |

vol. XXX | no. XX | 1–16

 
 
 
 
 
 
In the ﬁrst part of the paper we develop a new technique for
proving that a class of CRNs stoichiometrically computes the
desired function. We identify the non-competitive property,
which means that a species is consumed in at most one reaction
(see later for a formal deﬁnition). We show that for non-
competitive CRNs, rate-independence can be veriﬁed and the
function computed can be determined by simple reasoning
analogous to sequential programming: Although all reactions
occur simultaneously with continuously varying rates, we can
imagine, counter-factually, that reactions happen sequentially
in a series of straight line segments. Non-competition is easy to
check, and further fully captures the computational power of
stoichiometric computation. Thus, non-competitive CRNs are
a powerful class of CRNs for rationally programming chemical
behavior. All subsequent constructions in this paper are non-
competitive, and their correctness is proven via the above
technique.

Fig. 1. Representations of chemical reaction networks. The law of mass-action
induces the differential equations describing the CRN’s change in concentrations over
time, where, e.g., a represents the concentration of species A. The stoichiometry ma-
trix captures the net change in species by each reaction, where entry i, j corresponds
to the change in species i by applying reaction j.

species at most once and with unit stoichiometry. We show
that such CheLU CRNs and binary-weight ReLU networks
can be considered to be equivalent models of computing as
they can simulate each other with the number of ReLU nodes
equalling the number of bimolecular reactions.

In the second part of this paper, motivated by the
widespread use of neural networks to generate behavior that is
not easily speciﬁed programmatically, we show a natural way
to specify rate-independent chemical input-output behavior
through training. Speciﬁcally, we show how (feed-forward)
ReLU (Rectiﬁed Linear Unit) neural networks can be directly
implemented by non-competitive CRNs. ReLU neural net-
works are one of the most successful types of neural networks
for deep learning, prevalent in all areas of machine learning.
Thus we provide a powerful paradigm for creating chemical
systems with complex computational functionality not easily
obtained by other means.

The key elements of our general (rational-weight) ReLU
neural network implementation are the ReLU and the weight
multiplication modules. Our ReLU module consists of a single
unimolecular and a single bimolecular reaction. Our weight
multiplication module uses a number of uni- and bimolecu-
lar reactions that is proportional to the number of bits of
precision in the weight. (Although weight multiplication can
be performed with two high-order reactions, such reactions
cannot easily be implemented and are slow.)

To simplify the construction even further we consider re-
stricting the class of ReLU neural networks to have {−1, 0, 1}
weights. Despite the restriction on the values of the weights,
such binary-weight ReLU neural networks are known to be pow-
erful in solving machine learning tasks and are well-researched
in deep learning community (8). Applying an optimized ver-
sion of our construction to binary weight ReLU networks yields
a surprisingly compact CRN with only a single bimolecular re-
action per ReLU node (plus additional unimolecular reactions
at the input layer).

D R A FT

In the last part of the paper, we demonstrate through ex-
amples our procedure of using binary-weight ReLU neural
networks to embed functionality in CRNs. For each example,
we train the neural network classiﬁer, generate the resulting
CRN, and numerically simulate the CRN under the usual mass-
action kinetics. The kinetic simulation conﬁrms convergence
to the expected output and provides additional information
about convergence time. First, we train classiﬁers on the
widely used machine learning datasets IRIS and MNIST. Next,
motivated by the envisioned application of molecular computa-
tion in medical diagnostics, we diﬀerentiate between four viral
infections using chemical information as input (gene expression
levels). Finally, an important direction of chemical compu-
tation in synthetic biology lies in spatial pattern formation
with applications in tissue and organ engineering (9). As an
example of spatial pattern formation, we use a neural network
to generate a 2D pattern (heart shape).

1. Chemical Reaction Networks

Chemical reaction networks (CRNs) formally model the time
evolution of molecules in a solution undergoing chemical inter-
actions. Besides the use of CRNs to capture the behavior of
naturally existing chemical systems, synthetic biologists and
molecular programmers often use CRNs as a programming lan-
guage for rationally designed synthetic chemical networks such
as DNA strand displacement cascades (5, 6) and DNA-enzyme
networks (10). Related models of distributed computation
include population protocols (11), Petri nets (12), and vector
addition systems (13).

Next we provide some formal notation for CRNs aimed
towards understanding the results of this work. A CRN con-
sists of a set of species Λ and a set of reactions. Reactions are
written generally in this form:

r1R1 + · · · + rnRn

k−→ p1P1 + · · · + pmPm,

where Ri, Pj ∈ Λ are the reactant and product species, respec-
tively, the ri, pj ∈ N are stoichiometric coeﬃcients quantifying
how much of each species is produced and how much is con-
sumed, and k is the rate constant used to describe the rate of
the reaction in kinetic models like mass-action kinetics. We
note that although reactions written this way are irreversible,
i.e., the products cannot react to form the reactants, in nature
reactions always have some degree of reversibility. However,

Showing how two models of computing can simulate each
other elucidates the computational power of one model in
terms of the other. In the case of stoichiometrically comput-
ing CRNs and ReLU neural networks, they are both capable
of computing arbitrary continuous piecewise linear functions.
However, since the size of the CRN depends on the digits
of precision of the weights, making a quantitative connection
between the computational power of the two models (e.g., com-
paring the number of reactions versus number of ReLU nodes
to achieve the same functionality) is diﬃcult. Nonetheless, in
the case of binary weight ReLU networks, we can make a tight
connection between binary weight ReLU and the subclass
of non-competitive CRNs in which a reaction involves any

2 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

Mass-ac�on kine�cs:Chemical reac�on network:(3)(2)(1)Stoichiometry matrix:(3)(2)(1)synthetic chemical reactions can be made highly irreversible †
and if desired this model can include the reverse of each re-
action, e.g. R1 + R2 −→ P and P −→ R1 + R2. While the
results of Section 2 apply to reactions with arbitrarily many
reactants, the constructions in Sections 3 and 4 consist of
reactions with at most two reactants. Reactions with more
than two reactants are slow in practice, as they require the
co-localization of more than two molecules before reactions
can occur. Further, while simulation of high-order reactions
by bimolecular ones is possible, the typical method disturbs
kinetics and does not ﬁt in the non-competitive class (deﬁned
later) we are focusing on.‡

A state of a CRN is an assignment of nonnegative real-
valued concentrations (amount per volume) to each species.
It helps to pick an arbitrary ordering on the species so that
we can view states as vectors from RΛ
≥0 for compatibility with
linear algebra techniques used later. We use a(S) to denote
the concentration of species S in state a.

Fig. 2. Two rate-dependent CRNs and one stoichiometrically-deﬁned CRN com-
puting y = x
2 . The concentration of species Y as time goes to inﬁnity is half of the
initial concentration of species X. The rate-dependent CRNs require that the rate
constants of the two reactions are equal, while the third single-reaction CRN has no
rate constraints.

ﬂux vector u applicable§ at a such that b = Mu + a; this
is straight-line reachability. Given a →1
u b, we say reaction
R is being applied if u(R) > 0. We say a → b if there is a
ﬁnite length sequence a →1 · · · →1 b, i.e., → is the transitive
reﬂexive closure of →1; this is called line-segment reachability.
If no ﬂux vectors u besides the zero vector are applicable at
state b, then we call b a static state.

CRNs are typically modeled either by diﬀerential equations
or as stochastic processes. Much of the discussion in this pa-
per centers on the ubiquitous continuous mass-action kinetics
model (example in Figure 1) which prescribes diﬀerential equa-
tions from reaction rates proportional to the product of the
reactants’ concentrations. However, we focus on CRNs whose
convergence state is independent of rate law, so assuming
mass-action kinetics is not required for our theory to hold and
constructed CRNs to compute correctly. Further, an analogy
of our Theorem 1 holds for discrete stochastic models and is
presented in SI Appendix F.

Next we present a nondeterministic kinetic model, ﬁrst
proposed by (2), designed to isolate the eﬀect of stoichiometry
from the eﬀect of rates. This model does not intend to capture
real-world chemical kinetics directly. Instead, it is a simpliﬁed
model that aids analysis of CRNs: as we will show, for the
class of CRNs of interest, convergence in this simpliﬁed model
implies convergence under mass-action kinetics and a wide
variety of rate laws, even if the state of the CRN is initially
perturbed. Intuitively, the model explores the set of states
reachable by the CRN assuming nothing about the kinetics
besides that stoichiometry is obeyed.

The stoichiometry matrix M captures the stoichiometric
constraints of the CRN (example in Figure 1). Assuming an
ordering on species and reactions, each column corresponds to
a reaction, and each row to a species: Mij corresponds to the
net increase/decrease of species i by applying reaction j.

2. Programming CRN Computation by Stoichiometry

D R A FT

The computational power of CRNs typically arises from both
kinetics and stoichiometry. However, the equilibrium of cer-
tain CRNs can be understood entirely by the stoichiometric
exchange of reactants for products (Figure 2). Such systems
have been used as an alternate paradigm for programming
complex chemical behavior (2, 14), inspired by similar notions
in distributed computing (11). We call such CRNs stoichio-
metrically deﬁned.¶

To view CRNs as a method of computation (or, a program-
ming language), we assign some species to be the inputs and
others to be the outputs. Then, given initial concentrations of
the input species, the output of the computation is the equilib-
rium state of the system, i.e., the concentrations of the output
species in the limit as time goes to inﬁnity. ‖ Generally, given
a function f : Rn
≥0, some input species X1, . . . , Xn
and an initial concentration assignment to each will represent
an input vector x, and output species Y1, . . . , Ym and their
respective concentrations at equilibrium will represent the
output vector y such that f (x) = y.

≥0 → Rm

A small example is the reaction X1 + X2 −→ Y which com-
putes f (x1, x2) = min(x1, x2), since the reaction converges to
a state where either X1 or X2, whichever has initially lower
concentration, is depleted. A more complex example computes
f (x1, x2) = max(x1, x2) (Figure 3).

Recall that by arbitrarily ordering the set of species Λ,
we can view states of the CRN as vectors of concentrations
a ∈ RΛ
≥0. Then we can also describe ﬂux vectors which are
column vectors u ∈ RΛ
≥0 which describe arbitrary, simultane-
ous applications of reactions, which when multiplied by the
stoichiometry matrix M yield the change in concentrations
caused by applying those reactions. Since u describes a set
of reactions to happen, we say u is applicable at a state a
if all species which are reactants in the set of reactions in u
have positive concentration in a; formally, u is applicable at
a if u(S) > 0 implies that all reactants R of reaction S have
a(R) > 0. For states a and b, we say a →1
u b if there is a

†

‡

For example, implementing CRNs via DNA strand displacement yields reactions which are driven
by the formation of additional base pairs, and can be designed to be highly thermodynamically
favorable (4–6).
The typical method for simulating, e.g., the reaction 3X −→ Y is to use the reactions X +
X −*)−X1 and X + X1 −→ Y .

A. Non-competitive CRNs. Here we identify a class of CRNs
which we will show are easy to analyze and yet do not lose
any computational power if we restrict to stoichiometrically
deﬁned, rate-independent computation. To identify the class,
note that an intuition for why the max-computing CRN does
not depend on rates is that each species is a reactant in at most
one reaction, i.e., there is no competition between reactions
for species. For this reason, we ﬁnd that reaction (1) of the

§

¶

‖

Removing the applicability constraint would trivialize ﬁnding the set of reachable states of the CRN
but would lead to erroneous analysis. For example, given the CRN X1 + X2 −→ Y + Z,
Z −→ X2, given the ordering on species X1, X2, Y, Z and an initial state a = [10, 0, 0, 0],
state b = [0, 0, 10, 0], and ﬂux vector u = [10, 10], we would have that b = Mu + a,
although from a no reactions should be applicable because there is initially zero concentration of
X2 and Z.
Previous work calls this notion stable computation. We use the term stoichiometrically deﬁned to
avoid confusion with other notions of stability in chemistry.

There are alternative notions of computation by CRN; for example, a CRN may compute f (t) in
the sense that the concentration of a species is equal to f (t) for all times t.

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 3

Rate-dependent:Stoichometric:(Left) A stoichiometrically-deﬁned CRN computing the max function.
Fig. 3.
Let xi(0) be the initial concentration of the input species Xi; all other initial con-
centrations are assumed to be 0. Reactions (1) and (2) converge to an amount
of Y equal to x1(0) + x2(0), and amounts of A1, A2 equal to x1(0), x2(0),
respectively. Reaction (3) converges to an amount of M equal to the min be-
tween the amounts of A1 and A2 produced by reactions (1) and (2), i.e., the
min between x1(0) and x2(0).
In reaction (4), the M species annihilate the
Y species, so that the concentration of Y at convergence is decreased by the
concentration of M , effectively computing subtraction. In all, the amount of Y con-
verges to x1(0) + x2(0) − min(x1(0), x2(0)) = max(x1(0), x2(0)). Using
Theorem 1, a formal argument of convergence is given by applying the reactions
maximally, one-by-one, and in numerical order in the nondeterministic kinetic model.
(Right) Composing the max computing CRN with a min computing CRN does
not yield a stoichiometrically-deﬁned CRN computing min ◦ max. Reaction (5)
attempts to use the output Y of the max computing reactions shown in the left
panel to compute min(y, x3). This fails since reaction (5) might consume more
Y than max(x1, x2), and thus generate more than the correct amount of Z, by
outcompeting reaction (4). The extent of the error depends on the relative rates of
reactions (4) and (5).

max-computing CRN must produce an amount of Y and A1
equal to the initial amount of X1 as time goes to inﬁnity,
since X1 cannot be decreased (nor increased) by any other
reaction. Reasoning about the other reactions similarly yields
the correct output. Carefully formalizing this intuition yields
the following class of CRNs:

Deﬁnition 1. Non-competitive CRNs. A CRN is non-
competitive if every species which is decreased in a reaction is
a reactant in only that reaction.

Note that by the deﬁnition above, a reactant may appear in
any number of reactions if it is not decreased (e.g., if it acts
as a catalyst).

In SI Appendix C, we prove the following about non-

competitive CRNs:

Theorem 1. For non-competitive CRNs, if a → b and b
is a static state, then for any state a0 such that a → a0,
a0 converges to b for any rate constants under mass-action
kinetics.

Figure 4 illustrates a small application of this theorem. The
precondition of this theorem, that a → b with b static, is
the same as providing a line-segment path from the input
state to a static state with the correct output.
(For the
max example, the line-segment path is simply to apply the
reactions maximally in order.) Thus, this theorem greatly
simpliﬁes the analysis of equilibrium for non-competitive CRNs.
Further, the theorem states that any state stoichiometrically
compatible with the initial state still converges correctly under
mass-action kinetics. The path a → a0 captures a wide class
of perturbations, allowing any adversarial conditions to be
applied to the system initially, such as non-well-mixedness or
withholding of certain reactions, as long as stoichiometry is
still obeyed. Then, as long as mass-action kinetics are allowed
to take over, the system converges to the output state b. (Note
that a0 can be equal to a, since a → a, meaning that this
theorem also implies convergence from the initial state.)

D R A FT

Deﬁnition 2. A fair rate law is any kinetic rate law which
satisﬁes: (1) at any time, the rate of a reaction is nonzero if
all of its reactants have nonzero concentration, and (2) if b
can be reached from a according to the rate law, then a → b.

In fact, we can apply Theorem 1 to rate laws more general

than mass action:

Fig. 4. Example application of Theorem 1 on the non-competitive CRN X +
X −→ R + Y , R + R −→ X with initial state a = [10, 0, 0]. The shaded region
shows all stoichiometrically reachable states from a, i.e., all states d such that a → d.
Solid lines are straight-line reachable paths (speciﬁcally, a →1 c and c →1 b)
and dashed lines are mass-action trajectories (assuming both reactions have rate
constant 1, although the theorem applies to any rate constants). Since there is a
path a → b, Theorem 1 implies that a also converges to b under mass action or
any other fair rate law. Further, as shown by the state a0, any state stoichiometrically
reachable from a will also converge to b under mass action or any other fair rate law,
showing that the convergence is robust to any initial perturbations that do not leave
the stoichiometrically reachable space of states.

Theorem 1 holds for any fair rate law. In (2), it is proven
that mass-action kinetics is fair. (Note that only item (2)
of Deﬁnition 2 is nontrivial.) One only needs to prove their
relevant kinetic model has a fair rate law in order to apply
Theorem 1.

By the end of this section, we will see that restricting
stoichiometrically deﬁned computation to the non-competitive
subclass does not restrict computational power.

B. Composition of CRNs. To construct large programs out of
smaller ones requires composability: CRNs computing func-
tions f1 and f2 should be straightforwardly concatenated so
that f2 ◦ f1 is computed. However, some of the constructions
described do not satisfy composability. For example, consider
composing the min and max computing CRNs to compute
z = min(max(x1, x2), x3) (Figure 3). Based on this failure to
compose, we can intuit that a CRN’s output species must not
be a reactant for a CRN to be composable:

4 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

(3)(2)(1)(4)(3)(2)(1)(4)(5)1 (0) + x−

Fig. 5. Non-competitive, dual-rail, composable CRNs for computing min (Left) and max (Middle). To analyze using Theorem 1, we can apply reactions maximally,
one-by-one, and in order. For the min CRN, applying reaction (1) maximally yields x+
1 (0); then applying reaction (2) yields
x+
1 = x+
2 (0) and y− = x−
2 , and y− from
applying the ﬁrst two reactions, we get y+ − y− = min(x+
2 (0)) as desired.
The max CRN’s correctness follows from a similar analysis. (Right) Continuous piecewise linear functions are compositions of max, min, and linear functions. An
example application of Theorem 2 is shown. Using the CRNs on the left and middle, along with composable, non-competitive, dual-rail CRNs to compute y = p
q x and
y = x1 + x2, any continuous piecewise linear function can be computed by ﬁrst applying the transformation of Theorem 2.

2 (0) + x−
2 (0). Then applying reaction (3) maximally yields y+ = min(x+

1 (0) and y− = x−
1 , x+

2 ), and substituting the values of x+

1 , x+
2 (0) − x−

2 (0)) = min(x+

2 (0)) − (x−

1 (0) + x−

1 (0) − x−

1 (0) + x−

2 (0) + x−

1 (0) + x−

2 = x+

1 (0), x+

2 (0), x+

Deﬁnition 3. Composability. A CRN is composable if its
output species Y1, . . . , Yn do not appear as reactants.

Previous work (15) proves that this composability deﬁnition
is necessary∗∗ and suﬃcient to compose stoichiometrically-
deﬁned CRN computations. Further, they prove that the
functions computable while obeying this constraint must be
superadditive:

Deﬁnition 4. Superadditive. A function f : Rn
superadditive if and only if for all x, y ∈ Rn
f (x) + f (y).

Superadditivity is a very strong restriction; for example,
the max function is not superadditive, and so cannot be com-
puted by a composable CRN. However, an alternative method
for representation of logical values in a CRN avoids the su-
peradditivity restriction for composability and simultaneously
allows representation of negative numbers, as we will describe
next.

C. Dual-rail CRN computation. If we wish to represent a vari-
able x that can take on negative values, we use a dual-rail
representation, which expresses a value x as a diﬀerence in
concentration between two species X + and X −. There are
composable CRNs with dual-rail input/output convention
which compute the min and max functions (Figure 5).

These min and max modules are important artifacts related
to the computational power of stoichiometrically-deﬁned com-
putation, due to the following theorem. Continuous piecewise
rational linear functions were proven equivalent to expres-
sions which are a max over mins over rational linear functions
(Figure 5). Formally:

Theorem 2. Proven in (16): For every continuous piecewise
linear function f with pieces f1, . . . , fp, there exists a family
S1, . . . , Sq ⊆ {1, . . . , p} with Si 6⊆ Sj if i 6= j, such that for all
x, f (x) = maxi∈1,...,q minj∈Si fj(x).

Rational linear functions are computable, e.g., qX −→ pY com-
putes y = p
q x. (We will revisit the computation of rational
multiplication later in this work, in the context of neural net-
work weight multiplication, and address the issue of using
reactions with many reactants which is undesirable.) Rational
aﬃne functions are also computable when the CRN has initial
context (initial concentrations of non-input species). Then,
the min and max modules allow a method for piecewise com-
position of the rational aﬃne pieces according to Theorem 2.

≥0 7→ Rm
≥0 is
≥0, f (x + y) ≥

D R A FT

Ultimately, the exact characterization of dual-rail, composable,
stoichiometrically-deﬁned CRN computable functions is the set
of continuous piecewise rational aﬃne functions (2). Further,
as we have shown how to compute min, max, and rational
aﬃne functions by composable, non-competitive CRNs, we
have shown that restricting CRNs to be non-competitive does
not restrict computational power.

While at ﬁrst glance the functions computed seem rather
limited since they are composed of rational aﬃne pieces, they
indeed can approximate arbitrary curves to any desired accu-
racy. Further, their power is underwritten by the empirical
power of ReLU neural networks, since such neural networks
indeed compute only piecewise rational aﬃne functions. Thus
we motivate the connection between CRNs and ReLU neu-
ral networks, and explore this connection in more detail in
Sections 3 and 4.

3. RReLU: Rational-Weight ReLU Neural Networks

In this and the subsequent section we develop constructions for
implementing ReLU neural networks with stoichiometrically-
deﬁned CRNs. We start with broadly allowing arbitrary ra-
tional weights in this section, and focus on binary weights in
Section 4.

Rational-Weight ReLU neural networks (RReLU) are neu-
ral networks with rational weights and ReLU activation
function. Figure 6A shows an example RReLU neural
network. This network consists of an input layer, a sin-
gle hidden layer and an output layer with ReLU activa-
tion functions. The output of the network is deﬁned by:
y = ReLU (W2 · ReLU (W1 · x + b1) + b2), where x ∈ R2 is
an input vector, W1 ∈ Q2×2 is a weight matrix into the hid-
den layer, b1 ∈ R2 is a vector of bias terms, W2 ∈ Q1×2 is a
weight vector into the output layer with b2 the corresponding
bias term, and y ∈ R is the output††:

W1 =

(cid:20) 1
−1/2 −1/2

1

(cid:21)

, x = (cid:2)x1

(cid:3)>

, b1 = (cid:2)−3/2

x2

1/2(cid:3)>

,

W2 = (cid:2)4

4(cid:3) , b2 = −1

(Although the inputs and outputs are interpreted as real-value
quantities, this particular network happens to compute the
XNOR function: y = x1 ⊕ x2 if 0 and 1 values represent logical
False and True.)

Figure 6C shows an implementation of such RReLU net-
works with composable, non-competitive CRNs. Note that the
diﬀerent CRN modules (fan-out, weighted sum, and ReLU)

We assume all vectors to be column vectors, unless otherwise noted.

∗∗

Although CRNs exist which can be composed and do have their output species as reactants in
some reactions, (15) proves that these CRNs can easily be simpliﬁed to CRNs which do not have
their outputs as reactants.

††

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 5

(3)(2)(1)(3)(2)(1)Fig. 6. CRN implementation of RReLU neural networks. (A) An example RReLU network. (B) Decomposition of a neuron into weighted summation and nonlinearity. (C)
CRN implementation for each RReLU network component.

D R A FT

are composed in a feedforward manner, where the outputs of
the upstream modules are inputs for the downstream modules.
The feedforward structure of the modules allows us to analyze
the system module by module, obtaining a path from the
initial state to a static state. We can then apply Theorem 1.
Fan-out—passing a value to multiple downstream neurons—
is implemented by consuming the input species and producing
n output species (n is equal to the fan-out degree), for both
positive and negative inputs, as shown in Figure 6C. First apply
the ﬁrst reaction (X + −→ Y +
n ) until completion.
This results in y+
i = x+(0). Then apply the second reaction
(X − −→ Y −
1 + · · · + Y −
n ) until completion. This results in
i = x−(0), and thus yi = y+
y−
i = x+(0) − x−(0) = x(0).
Since this is a static state of the fan-out module, by Theorem 1
this CRN computes fan-out.

1 + · · · + Y +

i − y−

Weighted sum—combining outputs of multiple predecessor
neurons by multiplying them with weight (rational number)
and summing up the values—is implemented by controlling
the stoichiometry of input and output species as shown in
Figure 6C. Consider the contribution to the weighted sum
by the reaction qjX +
j −→ pjY +. Running this reaction till
completion, x+
j (0) amount of input is consumed to produce
(pj/qj)x+
j (0) amount of the output. The negative input and
output species in reaction qjX −
j −→ pjY − work similarly. The
total contribution to the output species is (pj/qj)(x+
j (0) −
x−
j (0)) = (pj/qj)xj(0). Similar reactions are included for the
other input species of the weighted sum (note that positive
and negative species are ﬂipped in the case of a negative-signed
weight), which results in reaching a static equilibrium where
the total contribution to the output species is equal to the
weighted sum of the inputs.

While rational weight multiplication is easily computable
through stoichiometry as above (e.g., qX −→ pY computes
y = p
q x), the use of many reactants is undesirable as dis-
cussed in Section 1.We can use the scheme shown in Figure 7
for rational weight multiplication using only non-competitive
uni- and bimolecular reactions. Using reactions of the form
L0 −→ L1 + L1 and R0 + R0 −→ R1 we can double and halve
the concentration of a species, respectively. In this way, a
set of reactions may mimic the binary expansion of a given
rational p
q , generating an output species Y for each 1 bit in the

Fig. 7. Non-competitive bimolecular rational multiplication. (A) Binary represen-
tation of rational p
q where a, b, and c are binary strings. Strings a and b are length i
and j, respectively, while c is an inﬁnitely repeated string of length k. (B) A scheme
for constructing a non-competitive bimolecular CRN for rational multiplication. The
reaction chain uses i + j + k + 1 reactions to “implement” the binary expansion.
The last reaction creates a “loop” in the reaction chain which corresponds to string c.
The number of times each reaction will be applied (before looping) is some multiple of
the initial count of X, as indicated by the multiplier column. An output species Y will
appear as a product for each reaction where a 1 appears in the binary expansion. (C)
An example CRN which computes y = 19
6 x. Note: To achieve a dual-rail representa-
tion we can repeat this construction twice, for both positive (X +, Y +) and negative
(X −, Y −) input and output species.

binary representation. If the rational number has an inﬁnitely
repeating portion in its binary expansion, our CRN uses a ﬁnal
reaction which “loops” back to a previous reaction. Figure 7c
shows a concrete example of this. A detailed proof of correct-
ness for this construction may be found in SI Appendix E.
The proof shows a path from a state with concentration x of
the input species to a state at static equilibrium with con-
centration p
q x of the output species. By Theorem 1 (and
the fact that this CRN is non-competitive), this is suﬃcient
to show that the construction computes p
q x. To satisfy the
dual-rail representation, the construction is repeated for both
the positive X + and negative X − species. Since this CRN is
composable, it may be used for the weighted sum by creating
similar reaction chains for all input species.

ReLU is implemented with two reactions shown in Fig-

6 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

symbolequa�onCRNReLUCbias	termAB............weightedsumfan-out...::::::::ABCmul�plier,ure 6 ‡‡. We will show a particular line-segment path that
leads to a static equilibrium computing ReLU, which by The-
orem 1 implies that the CRN computes the ReLU. Con-
sider at ﬁrst applying the ﬁrst reaction (X + −→ M + Y +)
as long as X + is present. This results in: y+ = m = x+(0)
and x+ = 0. Then, consider applying the second reaction
(M + X − −→ Y −) until completion. The second reaction will
execute for min(m, x−) = min(x+(0), x−(0)). This results in:
y− = min(x+(0), x−(0)) and m = 0 ∨ x− = 0. The output of
the CRN is then: y = y+ − y− = x+(0) − min(x+(0), x−(0)) =
max(x+(0) − x−(0), 0) = ReLU(x(0)). Also, it holds that
x+ = 0 and m = 0 ∨ x− = 0; from which it follows that at
least one reactant of both reactions is zero, thus the static
equilibrium is reached. From Theorem 1 it follows that the
CRN computes ReLU.

Finally, bias terms are implemented by setting the initial
concentrations of the corresponding species to the dual-rail
value of the bias terms.

ReLU modules, can be eliminated from the CRN by altering
the bimolecular reactions and the initial concentrations of the
CRN species, a process which we describe next. Unimolecular
reactions are those with exactly one reactant like A −→ B + C.
Whenever A is produced in another reaction, we can replace
it with B + C. For example, if there is another reaction
X −→ A + B, we replace the reaction with X −→ 2B + C.
Further, we adjust the initial concentrations of the product
species (B and C) by increasing them by the initial concen-
trations of the reactant (A). Importantly, this transformation
works only if A is not a reactant in any other reaction; for
example, if there were another reaction like X + A −→ Y , it is
not clear what to replace instances of A with, and indeed it is
not possible to remove the unimolecular reaction in that case.
Luckily, our constructions are non-competitive and we are
able to show that for non-competitive CRNs the optimization
does not aﬀect the state of convergence (SI Appendix D). The
optimization procedure is illustrated in Figure 8.

To see that the composed modules converge, note that we
have shown that each module is composable as in Deﬁnition 3,
and further that since each module is non-competitive, the
entire network is non-competititve. Therefore, applying re-
actions maximally module-by-module, layer-by-layer gives a
straightforward path in the nondeterministic kinetic model
from the initial state to a static state with the output equal to
the output of the neural network. Theorem 1 then argues that
the CRN converges correctly under mass-action kinetics or
any fair rate law. We show an example RReLU neural network
and its complete CRN implementation in SI Appendix A.

4. BReLU: Binary-Weight ReLU Neural Networks

Binary-Weight ReLU neural networks (BReLU) are neural net-
works with binary weights (±1) and ReLU activation function.
Since they are a subclass of RReLU networks, the same trans-
lation procedure as illustrated for RReLU applies. BReLU
networks were popularized in the machine learning community
due to the computational speed-ups they bring (they elim-
inate the need for a large portion of multipliers which are
the most space and power hungry components of specialized
deep learning hardware), while at the same time preserving
the performance (8). From the angle of CRNs, computing
rational weights p
q x in dual-rail requires either two reactions
with many reactants or many reactions with at most two reac-
tants, neither of which is desirable. Thus, BReLU networks
are a better suited class of neural networks for CRNs than
RReLU, producing CRNs that are easier to implement in a
wet lab. In other words, restriction to binary weights simpliﬁes
both silicon- and chemical-hardware implementations of deep
learning while maintaining performance.

Note that the fan-out and weighted sum can be merged
into a single step since BReLU networks have ±1 weights.
Thus, by default, the fan-out and weighted sum of BReLU
networks is implemented using a reaction set similar to the
fan-out module in Figure 6, with the diﬀerence that the ±
signs of the output species are ﬂipped in the case of negative
weight.

A. Translation optimization. We ﬁnd that unimolecular reac-
tions of non-competitive CRNs, such as the ﬁrst reactions of

‡‡

Enumeration of small CRNs shows that this is the simplest stoichiometrically-deﬁned, composable
CRN computing ReLU in the sense that ReLU cannot be computed in this manner with fewer than
2 reactions or 5 species (17).

RReLU networks allow for the optimization of fan-out
modules, partial optimization of ReLU modules (only the
unimolecular reaction) and weighted sum modules only in the
cases where the weight denominator is equal to 1 (integer
weights). BReLU networks in addition allow optimization of
weighted sum modules in all cases. Note that the unimolecular
reactions corresponding to the input species are not optimized
in order not to alter the input to the system. The CRN
resulting from the optimization of a BReLU network thus
has the property that there are no unimolecular reactions
besides the input layer, for which there are two reactions per
input. In other words, the CRN of a BReLU network consists
of (a) a bimolecular reaction per RReLU node, and (b) two
unimolecular reactions per input of the neural network.

Optimization of some adversarial ReLU networks results in
reactions with a number of products exponential in the depth
of the network. Understanding the scaling of the number of
products is an important avenue for future work to ensure
feasible CRNs.

D R A FT

B. BReLU networks simulate CRNs. We have seen that non-
competitive CRNs can compute any function computed by
a BReLU network where each reaction (except for the input
layer reactions) corresponds to one BReLU node. One in-
terpretation of this is that CRNs eﬃciently simulate BReLU
networks. A natural question is the converse: can any CRN be
eﬃciently simulated by a BReLU network? In this subsection
we answer this question at least for a subclass of CRNs which
we call CheLU networks, showing that they can be simulated
by BReLU networks with one ReLU node per reaction.

First we deﬁne a subclass of CRNs as the target to be
simulated. The ﬁrst restriction is that reactions have at most
two reactants (reactions with more than two reactants are
anomalous as discussed in Section 1). The second restriction
is that the CRN is feed-forward. This can be formalized by
saying that there is a total ordering on reactions such that
products of a reaction cannot be reactants of a reaction earlier
in the ordering. The third restriction is that every species
appears at most once per reaction. Intuitively, this restriction
is placed because a reaction like X +X → . . . essentially halves
the signal of X, which has no analog in binary-weight neural
networks. Lastly, we restrict the CRNs to be non-competitive.
For their connection to BReLU networks, we call this class of
CRNs CheLU networks.

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 7

Fig. 8. CRN optimization procedure. (Left) Neural network and its corresponding CRN before the optimization. (Right) Neural network and its corresponding CRN after the
optimization.

under mass-action kinetics using an ODE simulator (22). Our
main goal is to show the equivalence of a trained neural net-
work and compiled CRN, and not to improve accuracy of ML
models, which is orthogonal to our work.

A. IRIS.

D R A FT

Dataset. The IRIS dataset consists of 150 examples of
3 classes of ﬂowers (Setosa, Versicolor or Virginica), and 4
features per example (sepal length and width, and petal length
and width). Considering a small dataset size (150 examples),
and that our primary goal is to show the equivalence of a
neural network and the compiled CRN, we train and evaluate
on the whole IRIS dataset.

Results. We train a neural network with a single hidden
layer consisting of 3 units, 4 input units (capturing the features
of IRIS ﬂowers), and 3 output units where the unit with
the highest value determines the output class. We achieve
accuracy of 98% (147 out of 150 examples correctly classiﬁed)
with a trained BinaryConnect neural network. In the resulting
network, 5 weights out of 21 total weights are zero-valued. We
translate the network to the equivalent CRN consisting of 18
chemical reactions (unoptimized compilation), or 9 chemical
reactions (optimized compilation). We simulate both versions
of CRNs and conﬁrm that their outputs (labels) match the
outputs of the neural network in all of the 150 examples.

B. MNIST.

Dataset. The MNIST dataset consists of labeled hand-
written digits, where features are image pixels, and labels
are digits (0 to 9). We split the original MNIST training set
consisting of 60, 000 images into 50, 000 for the training set,
and 10, 000 for the validation set. We use the original test set
consisting of 10, 000 images. In a preprocessing stage we center
the images (as done in the BinaryConnect work). Additionally,
aiming at a smaller neural network and CRN, we scale the
images down from 28 × 28 to 14 × 14.

Results. We train a neural network with one hidden
layer of 64 units. The neural network has 142 input units
(one per pixel), and we use 10 output units (for digits 0 to
9). We train the neural network to maximize the output
unit corresponding to the correct digit. Our model achieves
accuracy of 93.92% on the test set, In the resulting model
23% of weights are zero. Note that we did not focus on
achieving high accuracy; BinaryConnect in original paper
achieves accuracy of over 98%, but uses more hidden layers

Fig. 9. A composable binary-weight ReLU network simulating a chemical re-
action A + B −→ C. Given any vector x of initial concentrations of species
A, B, and C, the equilibrium state b of the reaction A + B −→ C has b(A) =
a(A) − min(a(A), a(B)), b(B) = a(B) − min(a(A), a(B)), and b(C) =
min(a(A), a(B)).

We next deﬁne what is meant by simulation of CheLU
networks by BReLU networks. Of course, BReLU networks
have no sense of kinetics or dynamics. For this reason we
disregard kinetics and instead focus on initial and equilibrium
states of the CheLU network, and mapping those states to
inputs and outputs of a BReLU network. Formally, if a CRN
has one equilibrium state, we say a ReLU neural network
simulates that CRN if, for all initial states a, the equilibrium
state b given a is equal to the output vector of the ReLU
neural network given a as input.

We give a small, composable BReLU network (Figure 9)
which simulates a single CheLU reaction. Composing this small
network to simulate larger CheLU networks is straightforward
since we restrict CheLU networks to be feed-forward. The
BReLU network uses one ReLU node and two summation
nodes per reaction, although the summation nodes can be
removed with the clever addition of more edges to achieve one
ReLU node per reaction.

Thus, BReLU networks and CheLU networks simulate each
other, one node per reaction and vice versa, and so eﬃcient
networks in one model transfer to the other. Although CheLU
networks at ﬁrst seem restricted, the empirical power shown
of BReLU networks implies that CheLU networks are a rich
and powerful class of CRNs, whose restrictions make them
easy targets for implementation by synthetic means.

5. Simulations

In this section we describe numerical experiments showcas-
ing compilation from BReLU neural networks to CRNs. We
train BReLU networks on IRIS (18, 19), MNIST (20), virus
infection (21), and pattern formation datasets. We translate
trained neural networks to CRNs following our compilation
technique (Figure 6), and simulate the reactions’ behavior

8 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

ReLUReLUfan‐outfan‐outReLUReLUweighted	sumweighted	sumin the coordinate system, and 2 output units classifying the
input location (pixel) as a black or white. We achieve test
set accuracy of 98.44% (4 out of 255 pixels are misclassiﬁed).
Both original and learned image are shown in Figure 10. Note
that test and training set are same, as the goal in this task is
to overﬁt to the training set (image). In the resulting model
15.62% of weights are set to zero (5 out of 32 weights). We
translate the network to the equivalent CRN consisting of 36
reactions (unoptimized compilation), and 13 reactions (opti-
mized compilation). We simulate the CRN on all inputs, and
conﬁrm that output matches that one of the neural network
in all of the cases.

F. Training Speciﬁcs.

We use the implementation of BinaryConnect networks
published by the authors of the original work (8), and follow
the same training procedure except for the following: (1) We
focus solely on the ReLU activation function since other ac-
tivation functions such as sigmoid, hyperbolic tangent, and
softmax are not continuous piecewise linear and thus cannot
be implemented with rate-independent CRNs (2). (2) We add
support for 0 weights by discretizing the real valued weight
to zero if it is in the range [−τ, τ ]; where for τ we used 0.15.
(3) We do not use batch normalization (24). Batch normaliza-
tion would incur multiplication and division operations at the
inference stage (training stage is not a problem) that would
be hard to eﬃciently implement in CRNs. Instead, we rely on
Dropout (25) (stochastically dropping out units in a neural
network during training) as a regularization technique. In
all our experiments we use the square hinge loss (as used in
BinaryConnect) with ADAM optimizer.

We train on IRIS dataset for 10, 000 epochs, batch size 16
and return the best performing epoch. We train on MNIST
dataset for 250 epochs, batch size 100, measuring the validation
accuracy at each epoch, and returning the model that achieved
the best validation accuracy during training. For the MNIST
subset dataset we use same number of epocs and batch size.
We train on the virus infection dataset for 200 epochs, batch
size 16, and return the model that achieved the best validation
set accuracy. We train on the pattern formation dataset for
50, 000 epochs, and batch size of 255. We use an exponentially
decaying learning rate. The rate constants of all reactions are
set to 1, and all chemical simulations are performed for 50
arbitrary time units in the CRNSimulator package (22).

D R A FT

and units (3 layers with 1024 units each). Instead we used fewer
units in order to produce a smaller neural network and CRN.
We translate the network to an equivalent CRN consisting of
648 chemical reactions (unoptimized compilation), and 456
chemical reactions (optimized compilation). The CRN consists
of 2 · 142 input species (two species per input unit encoding
positive and negative parts), and similarly 2 · 10 output species.
We simulate the CRN on 100 randomly chosen examples from
the test set, and conﬁrm that output matches that of the
neural network in all of the cases.

C. MNIST Subset.

Dataset. With a goal of creating a smaller network we
trained a model on a subset of the MNIST dataset (only digits
0 and 1).

Results. We train a network with 1 hidden layer with 4
units. We now scaled images to 8 × 8, using a neural network
with 82 input units and 2 output units. Our model achieves
accuracy of 98.82% on the test set. In the resulting model
23% of weights are set to zero. The resulting CRN consists
of 140 reactions (unoptimized compilation), and 128 reactions
(optimized compilation).

D. Virus Infection.

Dataset. For the virus infection classiﬁer, we used data
from NCBI GSE73072 (21). The dataset contains microarray
data capturing human gene expression proﬁles, with the goal
of studying four viral infections: H1N1, H3N2, RSV, and HRV
(labels). There are 148 patients in the dataset, each with
about 20 separate proﬁles taken at diﬀerent times during their
infection period, for a total of 2, 886 samples. The dataset
contains information about which patient was infected and
during which point of time. We ﬁlter the samples leaving only
those that correspond to an active infection, and thus make
the data suitable for classiﬁcation of the four viruses. Finally,
we have in total 698 examples, split in 558 for training, 34
for validation, and 104 for testing. Each sample measures
expression of 12, 023 diﬀerent genes (features); we use the 10
most relevant genes as features which are selected using the
GEO2R tool (23) from the NCBI GEO.

Results. We train a neural network with one hidden layer
with 8 units, 10 input units capturing the expression of diﬀerent
genes, and 4 output units classifying between virus infections.
In the resulting
We achieve test set accuracy of 98.08%.
model 32% of weights are zero. We translate the network
to the equivalent CRN consisting of 52 chemical reactions
(unoptimized compilation), or 28 chemical reactions (optimized
compilation). We simulate the CRN on 100 randomly chosen
examples from the test set, and conﬁrm that output matches
that one of the neural network in all of the cases.

E. Pattern Formation.

Dataset. We construct the dataset from the image shown
in Figure 10. For each pixel, we create a training example with
(x1, x2) coordinates as input and a label representing value of
the pixel. Input x1 represents the horizontal distance from the
center of the image, and x2 represents the vertical distance
from the top left corner of the image. The value of the label
is 0 if the pixel is black and 1 if white. The dimensions of the
ﬁgure are 17 × 15; thus there are 255 examples in the dataset.
Results. We train a neural network with one hidden layer
containing 8 units, 2 input units for specifying the location

6. Related Work

A brief conference version of this work focused on the binary-
weight ReLU network implementation (3). In this full version,
we introduce the machinery of non-competitive CRNs allowing
for proofs of correctness, the general construction for rational
weight ReLU networks, and the inverse construction showing
simulation of CRNs by ReLU networks.

Prior work has studied a number of properties of CRNs that
arise from stoichiometry alone and are independent of rates (26,
27). In the context of using CRNs to perform computation,
computation by stoichiometry (2) was directly motivated by
the notion of stable computation in population protocols (11).
Other notions of nearly rate-independent computation involved
a coarse separation into fast and slow reactions (28).

Recent work took a diﬀerent but related approach to formal-
izing and verifying rate independence (7). They considered a

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 9

D R A FT

Fig. 10. Neural network architecture, input/output encoding, and CRN simulations for different datasets. (1) IRIS. (1A) Neural network architecture. (1B) CRN
simulation results. (2) MNIST. (2A) Input image and its input/output encoding. Each image from the MNIST dataset is unrolled into a vector, and the output label is represented
as a 10D vector. (2B) Neural network architecture. (2C) CRN simulation results for the input shown in 2A. (3) Virus Infection. (3A) Neural network architecture. (3B) CRN
simulation results. (4) Pattern formation. (4A) Image used to construct a dataset. (4B) Input and output encoding for a position (pixel) in the input image. An input is encoded
using 2D coordinates: (x1) symmetric horizontal coordinates (starting in the image center) and (x2) vertical coordinates starting from the top left edge of the image. An output,
which can be either black or white pixel, is encoded as a 2D vector as shown in the ﬁgure. (4C) Neural network architecture. (4D) Image learned by the neural network, and
CRN simulation results for 2 input values (positions).

broad class of rate functions and identiﬁed three easy-to-check
conditions that force convergence to the same point under any
rate function in this class. Speciﬁcally, they showed that it
is suﬃcient for the CRN to be synthesis-free, loop-free, and
fork-free. The ﬁrst condition means that every reaction de-
creases some species, the second condition is equivalent to our
feedforward condition, and the last is a more restricted version
of non-competition. Although most of the constructions in this
paper satisfy the above conditions, our construction for im-
plementing rational multiplication with bimolecular reactions
(Fig. 7) does not satisfy the loop-free (feedforward) condition
and is thus not amenable to this analysis.

The connection between CRNs and neural networks has
a long history. It has been observed that biological regula-
tory networks may behave in manner analogous to neural
networks. For example, both phosphorylation protein-protein

interactions (29, 30) and transcriptional networks (31) can be
viewed as performing neural network computation. Hjelmfelt
et al (32) proposed a binary-valued chemical neuron, whose
switch-like behavior relies on competition between excitation
and inhibition. More recently, Moorman et al (33) proposed
an implementation of ReLU units based on a fast bimolecu-
lar sequestration reaction which competes with unimolecular
production and degradation reactions. Recently, Anderson et
al (34) developed a diﬀerent mass-action CRN for computing
the ReLU and smoothed ReLU function.

In contrast to the prior work, our implementation relies
solely on the stoichiometric exchange of reactants for products,
and is thus completely independent of the reaction rates. Our
CRN is also signiﬁcantly more compact, using only a single
bimolecular reaction per neuron, with two species per every
connection (without any additional species for the neuron

10 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

4A4B4C4Dunrolltime-1.830.34valuey0-52timevalue-10y8y9y3y4y5y7y6y2y12A2B2C1A3AH1N1H3N2HRVRSVtimevalue3B1Bsetosaversicolorvirginicatimevalue0.5-0.5-1.00.5-1.5-0.5-2.5itself).

to reaction rates.

We use neural networks as a way to program chemistry.
The programming is done oﬄine in the sense that neural
networks are trained in silico. However, there is a body of
work on creating chemical systems that are capable of learning
in chemistry (35, 36). Although these constructions are much
more complex than ours, and arguably diﬃcult to realize, they
demonstrate the proof-of-principle that chemical interactions
such as those within a single cell are capable of brain-like
behavior.

Besides the above mentioned theoretical work on chemical
neural networks, wet-lab demonstration of synthetic chemi-
cal neural computation argues that the theory is not vapid
and that neural networks could be realized in chemistry. A
chemical linear classiﬁer reading gene expression levels could
perform basic disease diagnostics (37). Larger systems based
on strand displacement cascades were used to implement Hop-
ﬁeld associative memory (38), and winner-take-all units to
classify MNIST digits (39). Interestingly, the direct strand
displacement implementation of a neuron by our construction
is signiﬁcantly simpler (in terms of the number of compo-
nents needed) than the previous laboratory implementations,
arguing for its feasibility.

Only three kinds of computing hardware are currently
widespread: electronic computers, living brains, and chemical
regulatory networks, the last occurring within every cell in
every living organism. Given the society-changing success of
electronic computers and the recent neural networks revolu-
tion inspired by computation in the brain, it may be argued
that chemical computation is the least understood of the three.
Upon the reﬁnement of theoretical principles and experimental
methods, the impact of chemical computation could be felt
in far-reaching ways in synthetic biology, medicine, and other
ﬁelds. Chemical computation by stoichiometry, and methods
of programming and training such computation developed
here, provide a distinct approach to bottom-up engineering of
molecular information processing.

ACKNOWLEDGMENTS. This work was supported by NSF grant
CCF-1901025 to DS, and CCF-1718903 to SK. We thank David
Doty and Erik Winfree for essential discussions.

7. Conclusion

IR Epstein, JA Pojman, An introduction to nonlinear chemical dynamics: oscillations, waves,
patterns, and chaos. (Oxford University Press), (1998).

2. HL Chen, D Doty, D Soloveichik, Rate-independent computation in continuous chemical reac-
tion networks in Proc. of the 5th Conference on Innovations in Theoretical Computer Science.
(2014).

3. M Vasic, C Chalk, S Khurshid, D Soloveichik, Deep Molecular Programming: A natural im-
plementation of binary-weight ReLU neural networks in Proceedings of the 37th International
Conference on Machine Learning, Proceedings of Machine Learning Research, eds. HD III,
A Singh. (PMLR), Vol. 119, pp. 9701–9711 (2020).

4. D Soloveichik, G Seelig, E Winfree, DNA as a universal substrate for chemical kinetics. Proc.

Natl. Acad. Sci. 107, 5393–5398 (2010).

5. YJ Chen, et al., Programmable chemical controllers made from DNA. Nat. nanotechnology

6. N Srinivas, J Parkin, G Seelig, E Winfree, D Soloveichik, Enzyme-free nucleic acid dynamical

systems. Science 358, eaal2052 (2017).

7. E Degrand, F Fages, S Soliman, Graphical conditions for rate independence in chemical
reaction networks in International Conference on Computational Methods in Systems Biology.
(Springer), pp. 61–78 (2020).

8. M Courbariaux, Y Bengio, JP David, BinaryConnect: Training deep neural networks with
binary weights during propagations in Advances in Neural Information Processing Systems.
(2015).

9. J Santos-Moreno, Y Schaerli, Using synthetic biology to engineer spatial patterns. Adv.

1.

8, 755 (2013).

D R A FT

Biosyst. 3, 1800280 (2019).

10. T Fujii, Y Rondelez, Predator–prey molecular ecosystems. ACS Nano 7, 27–34 (2013).
11. D Angluin, J Aspnes, Z Diamadi, MJ Fischer, R Peralta, Computation in networks of passively

mobile ﬁnite-state sensors. Distributed computing 18, 235–253 (2006).

12. CA Petri, Communication with automata. (1966).
13. RM Karp, RE Miller, Parallel program schemata. J. Comput. system Sci. 3, 147–195 (1969).
14. HL Chen, D Doty, D Soloveichik, Deterministic function computation with chemical reaction

networks. Nat. computing 13, 517–534 (2014).

15. C Chalk, N Kornerup, W Reeves, D Soloveichik, Composable rate-independent computation
IEEE/ACM Transactions on Comput. Biol. Bioin-

in continuous chemical reaction networks.
forma. 18, 250–260 (2021).

16. S Ovchinnikov, Max-min representation of piecewise linear functions. Contributions to Algebr.

Geom. 43, 297–302 (2002).

17. M Vasic, D Soloveichik, S Khurshid, CRNs Exposed: Systematic exploration of chemical re-
action networks in International Conference on DNA Computing and Molecular Programming.
(2020).

18. RA FISHER, The use of multiple measurements in taxonomic problems. Annals Eugen. 7,

179–188 (1936).

19. E Anderson, The species problem in iris. Annals Mo. Bot. Gard. 23, 457–509 (1936).
20. Y Lecun, L Bottou, Y Bengio, P Haffner, Gradient-based learning applied to document recog-

nition. Proc. IEEE 86, 2278–2324 (1998).

21. Host gene expression signatures of H1N1, H3N2, HRV, RSV virus infection in adults https:

//www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE73072.

22. Mathematica package for working with networks of coupled chemical reactions. http://users.

ece.utexas.edu/~soloveichik/crnsimulator.html.
Identifying Differentially Expressed Genes https://www.ncbi.nlm.nih.gov/geo/geo2r/.

23.
24. S Ioffe, C Szegedy, Batch normalization: Accelerating deep network training by reducing inter-
nal covariate shift in Proceedings of the 32nd International Conference on Machine Learning,
(PMLR, Lille, France),
Proceedings of Machine Learning Research, eds. F Bach, D Blei.
Vol. 37, pp. 448–456 (2015).

25. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Dropout: a simple way
to prevent neural networks from overﬁtting. The journal machine learning research 15, 1929–
1958 (2014).

26. BL Clarke, Stoichiometric network analysis. Cell biophysics 12, 237–253 (1988).
27. M Feinberg, Foundations of chemical reaction network theory. (Springer), (2019).

While computation in CRNs typically depends on reaction
rates, rate-independent information processing occurs in the
stoichiometric transformation of reactions for products. In
order to better program such computation, we advance non-
competition as a useful property, allowing us to analyze an
inﬁnite continuum of possible, highly parallel trajectories via
a simple sequential analysis. We further demonstrate embed-
ding complex information processing in such rate-independent
CRNs by mimicking neural network computation. For binary
weight neural networks, our construction is surprisingly com-
pact in the sense that we use exactly one reaction per ReLU
node. This compactness argues that neural networks may be a
ﬁtting paradigm for programming rate-independent chemical
computation.

As proof of principle, we demonstrate our scheme with
numerical simulations of traditional machine learning tasks
(IRIS and MNIST), as well as tasks better aligned with po-
tential biological applications (virus identiﬁcation and pattern
formation). The last two examples rely on chemically available
information for input, and thus argue for the potential biologi-
cal and medical utility of programming chemical computation
via a translation from neural networks.

While numerical simulations conﬁrm convergence to the
correct output, further work is needed to study the speed of
convergence. How does the speed vary with the complexity
and structure of the CRN and the corresponding neural net-
work? As an example of how such convergence speed might be
analyzed, prior work showed that, e.g., 90%-completion time
scales quadratically with the number of layers in the network
if it logically represents a tree of bimolecular reactions (40).
Although in principle arbitrary CRNs can be implemented
using DNA strand displacement reactions, current laboratory
demonstrations have been limited to small systems (6), and
many challenges remain in constructing large CRNs in the
laboratory. Rate independent CRNs possibly oﬀer an attrac-
tive implementation target due to their absolute robustness

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 11

28. P Senum, M Riedel, Rate-independent constructs for chemical computation in Biocomputing

2011. (World Scientiﬁc), pp. 326–337 (2011).

29. KJ Hellingwerf, PW Postma, J Tommassen, HV Westerhoff, Signal transduction in bacteria:
phospho-neural network(s) in Escherichia coli? FEMS microbiology reviews 16, 309–321
(1995).

30. D Bray, Protein molecules as computational elements in living cells. Nature 376, 307–312

(1995).

31. NE Buchler, U Gerland, T Hwa, On schemes of combinatorial transcription logic. Proc. Natl.

Acad. Sci. 100, 5136–5141 (2003).

32. A Hjelmfelt, ED Weinberger, J Ross, Chemical implementation of neural networks and Turing

machines. Proc. Natl. Acad. Sci. 88, 10983–10987 (1991).

33. A Moorman, CC Samaniego, C Maley, R Weiss, A dynamical biomolecular neural network in

58th IEEE Conference on Decision and Control. (IEEE), (2019).

34. DF Anderson, A Deshpande, B Joshi, On reaction network implementations of neural net-

works. (arXiv preprint arXiv:2010.13290), (2020).

35. HJK Chiang, JHR Jiang, F Fages, Reconﬁgurable neuromorphic computation in biochemical
systems in 2015 37th Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC). (IEEE), pp. 937–940 (2015).

36. D Blount, P Banda, C Teuscher, D Stefanovic, Feedforward chemical neural network: An in

silico chemical system that learns xor. Artif. life 23, 295–317 (2017).

37. R Lopez, R Wang, G Seelig, A molecular multi-gene classiﬁer for disease diagnostics. Nat.

chemistry 10, 746–754 (2018).

38. L Qian, E Winfree, J Bruck, Neural network computation with DNA strand displacement cas-

cades. Nature 475, 368–372 (2011).

39. KM Cherry, L Qian, Scaling up molecular pattern recognition with DNA-based winner-take-all

neural networks. Nature 559, 370–376 (2018).

40. G Seelig, D Soloveichik, Time-complexity of multilayered DNA strand displacement circuits in

International Workshop on DNA-Based Computers. (Springer), pp. 144–153 (2009).

D R A FT

12 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

1,1,2

1,2,2

1,2

1,1,2

1,2,2

X +

X +

X −

1,1 + F −

1,1 + F −

1 −→ M1,1 + 4M2,1 + 4Y + + F +
1 −→ I −
2 −→ M1,1 + 4M2,1 + 4Y + + F +
X −
2 −→ I −
1,1,2 −→ I −
1,1,2 −→ M1,2 + 4M2,1 + 4Y +
1,1,2 −→ I −
1,1,2 −→ M1,2 + 4M2,1 + 4Y +
1,1 −→ 4I −
1,2 −→ 4I −
2,1 −→ Y −

1,2

2,1

2,1

1,1,2 + F +
F +
F −
1,1,2 + F −
F +
1,2,2 + F +
1,2,2 + F −
F −
M1,1 + I −
M1,2 + I −
M2,1 + I −

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

(a) Optimized CRN implementing the RReLU neural network from Figure 11a.

[1]

[2]

[3]

[4]

[5]

[6]

[7]

i−
1,1(0) = 3/2
i−
2,1(0) = 1
m1,2(0) = 1/2
m2,1(0) = 2
y+(0) = 2

[41]

[42]

[43]
[44]

[45]

(b) Initial concentrations after the optimization.

Fig. 12. Optimized CRN implementing RReLU neural network.

[8]

[9]

[13]

[14]

[15]

[11]

[12]

[10]

D R A FT

[22]

[24]

[19]

[23]

[17]

[18]

[20]

[21]

[16]

B. BReLU example. Figure 13 shows a full implementation of an
BReLU network.

C. Proof of Theorem 1. Here we prove that if a non-competitive CRN
can reach a static state in the nondeterministic kinetic model, then
the CRN converges to that state under any fair rate law. This idea
simpliﬁes the proof of a non-competitive CRNs’ convergence to the
simple task of identifying one path to a static state. Notably, the
results here simplify proofs of convergence for constructions given
in (2, 15).

To prove Theorem 1, several lemmas are provided along the way.
This ﬁrst lemma does most of the work, showing that line-segment
reachability of a → b with b a static state places severe restriction
on the possible paths leaving a. Note that a path from a to b refers
to the sequence of straight-line reachability relations →1
which
ui
show that a → b.

[25]

[26]

Lemma 1. Assume a CRN is non-competitive. Consider two paths
p1 and p2 leaving state a. If p1 has ﬁnite length and ends in state
b and p2 applies some reaction R more than p1, then b is not
static.

1,1,2

1,1,2

1,2,2

1,2,2

(a) RReLU neural network.

1,2

1,2

1,1

1,1

1,2

1,2

1,1

1,1

X +

X +

X −

1,1,1 + F +
1,1,1 + F −
1,2,1 + F +
1,2,1 + F −

1 −→ F +
1 −→ F −
2 −→ F +
2 −→ F −
X −
1,1,1 −→ I +
F +
F −
1,1,1 −→ I −
F +
1,1,2 + F +
1,1,2 −→ I −
1,1,2 −→ I +
1,1,2 + F −
F −
F +
1,2,1 −→ I +
F −
1,2,1 −→ I −
1,1,2 −→ I −
1,2,2 + F +
F +
1,1,2 −→ I +
1,2,2 + F −
F −
1,1 −→ M1,1 + H +
I +
1,1 −→ H −
M1,1 + I −
1,2 −→ M1,2 + H +
I +
1,2 −→ H −
M1,2 + I −
1,1 −→ F +
H +
1,1 −→ F −
1,2 −→ F +
1,2 −→ F −
H −
2,1,1 −→ 4I +
F +
2,1,1 −→ 4I −
F −
F +
2,2,1 −→ 4I +
2,2,1 −→ 4I −
F −
I +
2,1 −→ M2,1 + Y +
M2,1 + I −
2,1 −→ Y −

H −

H +

2,1,1

2,2,1

2,2,1

2,1,1

2,1

2,1

2,1

2,1

1,2

1,1

1,1

1,2

(b) CRN implementation of the BReLU neural network.

i−
1,1(0) = 3/2
i+
1,2(0) = 1/2
i−
2,1(0) = 1

[27]

[28]

[29]

(c) Initial concentrations implementing bias terms of the RReLU neural network.

Fig. 11. Example RReLU network and its CRN counterpart.

Supplementary Information Appendix

A. RReLU example. Figure 11 shows a full implementation of an
RReLU network, and Figure 12 shows the CRN after optimization
procedure is performed.

w1

· · · →1

Proof. First, some notation: if x →1
wn y, then we use this
shorthand notation for the sum of the ﬂux of a reaction R along
the path: Fx→y(R) = Pn
wi(R).
i=1
Write path p2 as a →1
a1 →1
. . . . Choose the minimal
u1
u2
i such that ai satisﬁes that there exists a reaction R such that
Fa→ai (R) > Fa→b(R). Note that such a state ai exists by the
lemma’s assumption. Note that since ai−1 →1
ui
λui
a0 for any λ ∈ [0, 1]. In other words, every state along the line
segment from ai−1 to ai is reachable from a. Find the minimal λ
such that there exists a reaction R0 such that R0 is being applied
on this line segment (formally, ui(R0) > 0) and Fa→a0 (R0) =
Fa→b(R0). These minimal choices of i and λ ensure that for all
R00 6= R0, Fa→a0 (R00) ≤ Fa→b(R00).

ai, then ai−1 →1

Let S be an arbitrary reactant of R0, and let r be the entry in the
stoichiometry matrix corresponding to species S and reaction R0.
Let P1, . . . , Pn be the reactions which produce species S, and let pi
be the entries of the stoichiometry matrix corresponding to species

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 13

(a) BReLU neural network.

1,2

1,2

1,2

1,2

X +

X +

X −

1,1 + I −
1,1 + I +
1,1 + I +
1,1 + I −

1 −→ I +
1 −→ I −
2 −→ I −
2 −→ I +
X −
I +
1,1 −→ M1,1 + H +
1,1 −→ H −
M1,1 + I −
I +
1,2 −→ M1,2 + H +
1,2 −→ H −
M1,2 + I −
H +
1,1 −→ Y +
1,1 −→ Y −
1,2 −→ Y +
1,2 −→ Y −

H −

H −

H +

1,1

1,2

1,1

1,2

X +

X +

X −

1 −→ M1,1 + Y + + I −
1 −→ I −
1,1 + M1,2 + Y +
2 −→ I −
1,1 + M1,2 + Y +
2 −→ M1,1 + Y + + I −
X −
M1,1 + I −
1,1 −→ Y −
M1,2 + I −
1,2 −→ Y −

Towards proving the theorem, ﬁrst, we must eliminate the possi-
bility that although a → b in the nondeterministic kinetic model
and b is a static state, that somehow the CRN may converge under
the rate law to a dynamic equilibrium or to some oscillatory cycle
of states, or that it does not converge at all. These kinetic behav-
iors are associated with the following kinds of inﬁnite paths in the
nondeterministic kinetic model as described in Lemma 3 below.

Deﬁnition 5. Given a CRN and a state a, a has unbounded
a1 →1
a2 →1
potential if there exists a path a →1
. . . such
u2
u3
that there exists a reaction R such that P∞
ui(R) = ∞.

u1

i=1

Lemma 3. Assume a CRN is non-competitive.
If a does not
converge to a static equilibrium under fair rate law kinetics, then a
has unbounded potential.

Proof. There are two cases; either a converges to a dynamic equi-
librium, or a does not converge.
If a converges to a dynamic
equilibrium c, then by the fair rate law assumption, a → c. Since c
is a dynamic equilibrium, there exists a nonzero ﬂux vector u such
that c →1
u . . . . This path
shows that a has unbounded potential.

u c. Consider the path a → c →1

u c →1

[46]

[47]

[48]

[49]

[50]

[51]

[52]

1,2

[53]

[56]

[54]

[55]

[57]

Otherwise, a does not converge as t → ∞. In this case, intu-
itively, we use the assumption of non-convergence to construct a
path with unbounded potential. Formally, letting st be the state of
the CRN starting at a under mass-action kinetics after time t, we
will show how to ﬁnd an inﬁnite sequence of time points t0, t1, . . .
such that a → st0 → st1 → . . . and this path has inﬁnite ﬂux on
some reaction R, thus showing that a has unbounded potential.

D R A FT

Let s(t) be the state reached at time t starting from a under
mass-action kinetics. By negating the deﬁnition of convergence,
non-convergence means that for any state x ∈ Rn
≥0, we can ﬁnd
an ε ∈ R such that for any time t, we can ﬁnd a t0 > t such
that there is a species S such that |s(t0)(S) − x(S)| ≥ ε, i.e., s(t0)
is outside of the open ball of ε radius centered at x. Let the
initial state a be the x in the non-convergence deﬁnition, then let
ε0 = ε, take an arbitrary time t, and any t0 > t. Some species S
has |s(t0)(S) − x(S)| ≥ ε0, and by the fair rate law assumption,
a → s(t0). Then, similarly, letting s(t0) be the x in the non-
convergence deﬁnition, let ε1 = ε, an arbitrary t > t0, and take
any t1 > t. Now, some species S has |s(t1)(S) − s(t0)(S)| ≥ ε1,
and by the fair rate law assumption, s(t0) → s(t1). Repeating this
process yields an inﬁnite path a → s(t0) → s(t1) . . . and an inﬁnite
sequence ε0, ε1, . . . with the property that, given i ∈ N, there is a
species S such that |s(ti)(S) − s(ti−1)(S)| ≥ εi. Note that we can
choose each ti such that εi ≥ εi−1.§§ Since each s(ti) is at least
ε1 away from s(ti+1), we have a path a → s(t0) → s(t1) → . . .
showing that a has unbounded ﬂux.

We prove that states which have a path to a static state have
bounded potential, and so by the contrapositive of Lemma 3 must
converge to a static equilibrium under fair rate laws.

[58]

[59]

[63]

[60]

[61]

[62]

[64]

1,2

(c) Optimized CRN implementation of the BReLU neural network.

Fig. 13. Example BReLU network and its CRN counterpart.

(b) CRN implementation of the BReLU neural network. This CRN is partially optimized –
only fan-out module is optimized.

S and reactions Pi. Note that by non-competition, p1, . . . , pn are
nonnegative. We can write the concentrations of S in a0 and b
as the initial concentration plus the amount changed by reaction
application as follows:
a0(S) = a(S) + p1Fa→a0 (P1) + · · · + pnFa→a0 (Pn) + rFa→a0 (R0),
b(S) = a(S) + p1Fa→b(P1) + · · · + pnFa→b(Pn) + rFa→b(R0).
Recall that a0 was chosen such that Fa→a0 (R0) = Fa→b(R0) and
for all reactions R00 6= R0 (notably, the Pi reactions), Fa→a0 (R00) ≤
Fa→b(R00). So we have a0(S) ≤ b(S). Further, recall that R0
is applicable in a0, so a0(S) > 0, and so b(S) > 0. Since S was
arbitrary, all reactants needed to apply reaction R0 are available in
b, so b is not static.

While Theorem 1 is stated in Section 2 in terms of mass-action
kinetics, we reiterate that the theorem holds for any fair rate law
(Deﬁnition 2). Previous work shows that mass-action is indeed a
fair rate law:

§§

Lemma 2. Proven in (2): For any CRN, if a can reach b under
mass action, then a → b. (This holds even if b takes inﬁnite time
to reach under mass action, i.e., it is the limit state.)

Lemma 4. Assume a CRN is non-competitive. If a → b and b is
a static state, then a does not have unbounded potential.

Proof. Towards contradiction, assume a has unbounded potential.
Let p1 be any path from a → b. Since a has unbounded potential,
there is a path p2 from a with some reaction R which is applied
with inﬁnite ﬂux. Then that reaction R is applied more in p2 than
in p1 (since it must be applied with ﬁnite ﬂux in the ﬁnite path p1),
so by Lemma 1, b is not static.

All that remains is to prove that the static equilibrium reached
by the fair rate law is in fact the same state b as assumed in the
nondeterministic kinetic model. First we prove that we cannot have
two diﬀerent static states b and c both reachable from a.

Lemma 5. For non-competitive CRNs, if a → b and a → c and
b and c are static states, then b = c.

To show this, towards contradiction assume the following proposition P:
for all choices of the
inﬁnite sequence of ti, there is an inﬁnite subsequence t0
i−1.
Choose an arbitrary inﬁnite sequence of ti; it must be that after some tj , each εk < εk−1
for all k > j. Otherwise, there would be an inﬁnite subsequence of t0
1 . . . of the ti with
ε0
i ≥ ε0i − 1, contradicting proposition P. The sequence tk . . . show that the CRN converges,
contradicting that the CRN does not converge.

1 . . . of the ti such that ε0

i < ε0

14 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

Proof. Towards contradiction, assume b 6= c. Then, without loss
of generality, a → c applies some reaction R more than a → b. So
by Lemma 1, b cannot be static.

Using this lemma, there is only one static state b reachable from
a. The next lemma is a restricted version of Theorem 1, assuming
that the starting state is a. After, we will show how the same
lemma holds for any a0 such that a → a0.

Lemma 6. Assume a CRN is non-competitive. If a → b and b is
a static state, then a converges to b under any fair rate law.

Proof. . By Lemma 4, a does not have unbounded potential. So by
the contrapositive of Lemma 3, a converges to a static equilibrium
under mass action. We will show that this static equilibrium must
be b. Towards contradiction, assume a converges to some c 6= b
under mass action. Then by the fair rate law assumption, a → c.
Also note that c is a static state since it is a static equilibrium. So
Lemma 5 implies c = b.

Next we will show that the above holds for any state a0 such
that a → a0. This is done by showing that any reachable state a0
can still reach the static state b, and thus intuitively any reachable
a0 may replace a for all of the lemmas above.

Lemma 7. For non-competitive CRNs, if a → b and b is a static
state, then for all a0 such that a → a0, it must be that a0 → b.

Proof. There are two cases: given a fair rate law, a0 either converges
or does not converge to a static equilibrium. If a0 reaches a static
equilibrium c, then by Lemma 2, a0 → c, so a → c. Then Lemma 5
implies b = c. Otherwise, if a0 does not reach a static equilibrium,
then Lemma 3 implies a has unbounded potential. However, since
a → b and b is static, this contradicts Lemma 4.

Together, Lemmas 6 and 7 prove Theorem 1 for any fair rate

law.

D. Proof of Optimization Procedure. Here we prove that the optimiza-
tion procedure of Section A does not change the state of convergence
if the CRN is non-competitive. For simplicity, we prove the theorem
in the case that the optimization removes one reaction. Removing
many reactions is done by removing one reaction at a time. If a is
a vector of length Λ, then let a\i be the same vector without an
entry for element i, i.e., the projection of a from the space RΛ
≥0 to
the subspace RΛ\i
≥0 . Intuitively, this maps states and ﬂux vectors
of a CRN to its optimized CRN (when just one species/reaction is
removed).

Theorem 3. Assume a CRN is non-competitive, and consider its
optimized CRN generated by removing a reaction R with reactant
S. If a → b and b is a static state and a(S), b(S) = 0, then the
optimized CRN has a\S → b\S .

\R
2

\R
1

\R
k

u1 a1 →1

Proof. We write a →1
b. For the optimized
CRN, we will show that the same sequence of ﬂux vectors is a valid
path for the optimized CRN which reaches the same state. Formally,
we will show a\S →1
u

u2 · · · →1
uk

· · · →1
u

1 →1
a0
u

b\S .

i=1

i=1

(u\R
i

First note since b = M Pk

ui + a, that also b\S =
M0 Pk
) + a\S where M0 is the stoichiometry matrix for
the optimized CRN. This holds reactions producing R’s reactant
now produce R’s products in M0; and because a(S), b(S) = 0, any
reactant of R that is produced in the path from a → b must be
consumed by reaction R to produce the products in b (they must
be consumed by R due to non-competition).
Then it remains to show that u\R

i+1 is applicable at state a0
i,
noting that a0
. It helps to decompose the
reactions into three sets: the removed reaction {R}, the set T of
reactions which produced species S in the original CRN, and the
set of reactions K which did not produce S in the original CRN so
are unmodiﬁed by the optimization. Consider an arbitrary species
A 6= S; we will show that a0
i+1 is

i(A) ≥ ai(A), implying that u\R

i is not necessarily a\S

i

applicable in a0
concentrations:

i since it is applicable in ai. We can determine the

i(A) = a\S (A) +
a0

X

K∈K

i
X

M 0

A,K

!

u\R
j

(K)

X

+

M 0

A,T

j=1

i
X

!

u\R
j

(T )

ai(A) = a(A) +

T ∈T

X

K∈K

j=1

i
X

MA,K

!

uj (K)

[67]

[65]

[66]

[68]

[69]

X

+

T ∈T

MA,T

j=1

i
X

j=1

!

uj (T )

+ MA,R

i
X

j=1

uj (R)

!

(K)

=

X

K∈K

MA,K

!

uj (K)

,

i
X

j=1

u\R
j

X

T ∈T

i
X

j=1

M 0

A,T

!

uj (T )

+ MA,R

i
X

j=1

!

u\R
j

(T )

≥

[70]

i
X

j=1

uj (R).

[71]

If A is not produced in R, then MA,R = 0 and M0

A,T = MA,T
so the terms are equal. Otherwise, A is produced in R. Since
reaction R has only one reactant S and the initial concentration of
S is zero, we know that the total ﬂux through R depends on the
total ﬂux through reactions in T (the reactions which produce S),

MS,T

!

uj (T )

≥

i
X

j=1

i
X

j=1

uj (R),

[72]

Due to the optimization procedure, the amount of A produced
by T is equal to the original amount produced plus the amount
produced by R times the number of appearances of S as a reactant,
i.e., M0

A,T = MA,T + MS,T MA,R, so:

X

T ∈T

X

T ∈T

=

X

≥

T ∈T

M 0

A,T

MA,T

MA,T

i
X

j=1

i
X

j=1

i
X

j=1

!

u\R
j

(T )

!

u\R
j

(T )

+ MA,R

!

u\R
j

(T )

+ MA,R

X

T ∈T

i
X

j=1

[73]

!

u\R
j

(T )

[74]

[75]

MS,T

i
X

j=1

uj (R).

Therefore a0

i(A) ≥ ai(A) so the ﬂux vector u\R

i+1 is applicable at
i. Since i was arbitrary, we have constructed a path showing

state a0
that a\S → b\S .

j=1

A,K

K∈K

M 0

X

i
X

Note that

so it remains to show:

D R A FT

MA,T

X

X

T ∈T

T ∈T

Vasic, Chalk et al.

PNAS | September 24, 2021 |

vol. XXX | no. XX | 15

 
 
 
 
 
 
 
 
 
 
 
 
 
E. Non-competitive Bimolecular Rational Multiplication. Here we
show correctness for the construction from Figure 7. We argue
for any two numbers p, q ∈ Z, our construction computes y = p
q x.
First, we describe how to construct the CRN from Figure 7. Let
a.bc∞ be the binary expansion of p
q where a ∈ {0, 1}i, b ∈ {0, 1}j ,
and c ∈ {0, 1}k. Construct a CRN of the form given in Fig 7b with
n reactions (n = i + j + k + 3) where each of the reactions (other
than the ﬁrst and last) is either of the form Lr −→ Lr+1 + Lr+1
or Rr + Rr −→ Rr+1. Let us enumerate the bits in a.bc∞ (from
left to right) as bibi−1 . . . b2b1.bi+1bi+2 . . . For each bit bm, where
0 < m ≤ n, in a.bc∞, if bm = 1 add the output species Y as a
product to reaction m.

To prove correctness, it is suﬃcient to reason about the sto-
ichiometry of one particular path to a static state (due to the
non-competitive nature of this CRN). Given an ordering on species
(X, L0, L1, . . . , Li, R0, R1, . . . , Rj , Y ) and an ordering on reactions
as listed in Fig 7b, consider a + Mv = b with initial state
a = [x, 0, . . . , 0], ﬁnal state b = [0, 0, . . . , p
q x], and a stoichiom-
etry matrix as deﬁned by the CRN:

of CRNs diﬀers from the concentration-, ODE-based kinetic models
of CRNs mainly in that concentrations are replaced by discrete
amounts of species and reaction applications are discrete events
which change species’ amounts by integer values.

We provide some basic deﬁnitions of reachability in the stochastic
model. It will be suﬃcient to reason only about reachability.¶¶
Note ﬁrst that the stoichiometry matrix M is the same as the
continuous model. States of a CRN are an assignment of counts to
each species, and so we can view them as vectors of nonnegative
integers. To deﬁne reachability by applying single reactions as
discrete events, we say state a →1
R b if there is a reaction R such
that R is applicable in a and b = MuR + a, where uR(R0) = 0
for all R0 6= R and uR(R) = 1. Then we let → be the transitive
reﬂexive closure of →1, i.e., reachability by applying zero or more
reactions. If a → b, we can think of the existing sequence of →1
relations to get from a to b as a path.

Note that the following lemma is analogous to Lemma 1, but

has a simpler proof due to the discrete model.

Lemma 8. Assume a CRN is non-competitive. Consider two paths
p1 and p2 leaving state a. If p1 has ﬁnite length and ends in state
b and p2 applies some reaction R more than p1, then b is not
static.

We

=
can
solve
[x, x, 2x, 4x, . . . , 2i−1x, x
Our prob-
lem, however, is that v is not applicable at a. To remedy this, we
decompose v into u1, u2, . . . , un such that each u is applicable.

for
4 , . . . , x

to
x
2j+k ]T .

v
2j , . . . ,

2 , x

v

Proof. The proof is mostly the same as Lemma 1. Note that
since reaction events are discrete, we can set a0 := ai and set
R0 := R, while still ensuring that Fa→a0 (R0) = Fa→b(R0) and
for all R00 6= R0, Fa→a0 (R00) ≤ Fa→b(R00). The rest of the proof
remains the same.

ﬁnd

D R A FT

Using the above lemma we can state a useful theorem which
captures non-competitive CRN behavior in stochastic kinetic models.
Note that reactions as discrete events simplify the notion of a length
of a path as the number of reaction applications, or the number of
→1

u(R) relations (excluding the “empty” reaction →1

[0,...,0]T ).

Theorem 4. Assume a stochastic CRN is non-competitive. If
a → b via path p with length ‘p and b is static, then there is no
path from a with length longer than ‘p, any path with length ‘p
also ends in b, and any path with length shorter than ‘p ends in a
state which is not static.

Proof. Let p0 be a path from a of length ‘p0 .

If ‘p0 > ‘p, towards contradiction, p0 applies some reaction R
more than p, so by Lemma 8 b is not static which contradicts the
lemma’s assumption, so no such p0 exists.

If ‘p0 ≤ ‘p and p 6= p0, then some reaction R applies more in p
than in p0, so the state at the end of the path p0 cannot be static.
If p = p0, then both paths must end in b.

n
X

v =

ui where

u1 = [

v[0], 0, 0, . . . , 0]T

i=1
1
2

u2 = [0,

1
4

u3 = [0, 0,

v[1], 0, . . . , 0]T

v[2], . . . , 0]T

1
8

...

ui+1 = [0, . . .

v[i], 0 . . . , 0]T

1
4

ui+2 = [0, . . . , 0,

v[i + 1] . . . , 0]T

1
8

...

un−1 = [0, . . . ,

1
2j+k

v[n − 1]]T

un = v −

n−1
X

i=1

ui

Then,

n
X

a + M

ui = b.

i=1
Thus, by Theorem 1, our construction stoichiometrically computes
y = p

q x.

F. Analogous Theorems for Stochastic Kinetic Models. Here we show
that a theorem analogous to Theorem 1 is also true for non-
competitive CRNs in the stochastic model. The stochastic model

¶¶

Typically stochastic CRNs are modeled as continuous time Markov processes, but our results hold
as long as transition probabilities corresponding to applying a reaction are positive if all reactants
for the reaction are positive. In other words, the kinetics must obey a stochastic equivalent of the
fair rate law assumption in the continuous case.

16 | www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX

Vasic, Chalk et al.

