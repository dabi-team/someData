Adversary Resistant Deep Neural Networks with an Application
to Malware Detection
Qinglong Wang12, Wenbo Guo1, Kaixuan Zhang1, Alexander G. Ororbia II1,
Xinyu Xing1, C. Lee Giles1, Xue Liu2
1Pennsylvania State University, 2McGill University

7
1
0
2

r
p
A
7
2

]

G
L
.
s
c
[

4
v
9
3
2
1
0
.
0
1
6
1
:
v
i
X
r
a

ABSTRACT
Beyond its highly publicized victories in Go, there have been nu-
merous successful applications of deep learning in information
retrieval, computer vision and speech recognition. In cybersecurity,
an increasing number of companies have become excited about
the potential of deep learning, and have started to use it for vari-
ous security incidents, the most popular being malware detection.
These companies assert that deep learning (DL) could help turn
the tide in the battle against malware infections. However, deep
neural networks (DNNs) are vulnerable to adversarial samples, a
flaw that plagues most if not all statistical learning models. Recent
research has demonstrated that those with malicious intent can
easily circumvent deep learning-powered malware detection by
exploiting this flaw.

In order to address this problem, previous work has developed
various defense mechanisms that either augmenting training data
or enhance model’s complexity. However, after a thorough analysis
of the fundamental flaw in DNNs, we discover that the effectiveness
of current defenses is limited and, more importantly, cannot provide
theoretical guarantees as to their robustness against adversarial
sampled-based attacks. As such, we propose a new adversary resis-
tant technique that obstructs attackers from constructing impactful
adversarial samples by randomly nullifying features within sam-
ples. In this work, we evaluate our proposed technique against a
real world dataset with 14,679 malware variants and 17,399 benign
programs. We theoretically validate the robustness of our tech-
nique, and empirically show that our technique significantly boosts
DNN robustness to adversarial samples while maintaining high
accuracy in classification. To demonstrate the general applicability
of our proposed method, we also conduct experiments using the
MNIST and CIFAR-10 datasets, generally used in image recognition
research.

ACM Reference format:
Qinglong Wang12, Wenbo Guo1, Kaixuan Zhang1, Alexander G. Ororbia
II1,
Xinyu Xing1, C. Lee Giles1, Xue Liu2. 2017. Adversary Resistant Deep
Neural Networks with an Application to Malware Detection. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Halifax, CA, Aug. 2017 (KDD’17), 9 pages.
DOI: 10.475/123 4

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD’17, Halifax, CA
© 2017 ACM. 123-4567-24-567/08/06. . . $15.00
DOI: 10.475/123 4

1 INTRODUCTION
The past decades have witnessed the evolution of various malware
detection technologies, ranging from signature-based solutions that
compare an unidentified piece of code to known malware, to sand-
boxing solutions that execute a file within a virtual environment
in order to determine whether the file is malicious or not. Un-
fortunately, none of these technologies seem to help much in the
never-ending battle against malware infection. According to a re-
cent report from Symantec Corporation [27], one million malware
variants hit the Internet every day that go completely undetected
by many of the most common cybersecurity technologies in use
today.

Substantial progress in neural network research, or deep learn-
ing (DL), has yielded a revolutionary approach to the cybersecurity
community in the form of automatic feature learning. Recent re-
search has demonstrated that malware detection approaches based
on deep neural networks (DNNs) can recognize abstract complex
patterns from a large amount of malware samples. This might offer
a far better way to detect all types of malware, even in instances of
heavy mutation [2, 7, 8, 15, 18–20, 24, 29].

Despite its potential, deep neural architectures, like all other
machine learning approaches, are vulnerable to what is known as
adversarial samples [3, 6, 26]. This means that these systems can
be easily deceived by non-obvious and potentially dangerous ma-
nipulation [5, 9, 10, 13, 28]. To be more specific, an adversary can
infer the learning model underlying an application, examine fea-
ture/class importance, and identify the features that have greatest
significant impact on correct classification. With this knowledge
of feature importance, an adversary can, with minimal effort, craft
an adversarial sample – a synthetic example generated by slightly
modifying a real example in order to make the deep learning system
“believe” this modified sample belongs to an incorrect class with
high confidence.

This flaw has been widely exploited to fool DNNs trained for
image recognition (e.g., [10, 22, 28]). With the broad adoption of
DNNs in malware detection, we speculate malware authors will also
increasingly seek to exploit this vulnerability to circumvent mal-
ware detection. To note, recent research has already demonstrated
that a malware author can leverage feature amplitude inequilibrium
to bypass malware detectors powered by DNNs [1, 11].

Past research [10, 23] in developing defense mechanisms relies on
strong assumptions, which typically do not hold in many scenarios.
Also, the techniques proposed can only be empirically validated
and cannot provide any theoretical guarantees. This is particularly
disconcerting when they are applied in a security critical application
such as malware detection.

In this work, we propose a new technical approach that can be
empirically and theoretically guaranteed to be effective for malware

 
 
 
 
 
 
KDD’17, Aug. 2017, Halifax, CA

Q. Wang et al.

detection and, more importantly, resistant to adversarial manipu-
lation. To be specific, we introduce a random feature nullification
in both the training and testing phase, which makes a DNN model
non-deterministic. This non-deterministic nature manifests itself
when attackers attempt to examine feature/class importance or
when a DNN model takes input for classification. As such, there
is a much lower probability that attackers could correctly identify
critical features for target DNN models. Even if attackers could infer
critical features and construct a reasonable adversarial sample, the
non-deterministic nature introduced in the model’s processing of
input will significantly reduce the effectiveness of these adversarial
samples.

Technically speaking, our random feature nullification approach
can also be viewed as stochastically “dropping” or omitting neuronal
inputs. It can be viewed as a special case of dropout regulariza-
tion [25], which involves randomly dropping unit activities (along
with their connections), especially in the hidden layers, of a stan-
dard DNN. However, in normal drop-out, since a DNN is treated as
deterministic at test-time 1, critical features of the DNN model can
still be correctly identified and manipulated to create synthesized
adversarial samples. Our approach is fundamentally different in
that we nullify features at both train and test time. In Section 5, we
compare our random feature nullification with standard drop-out.
The simple approach proposed in this work is beneficial for a
variety of reasons. First, it increases the difficulty for attackers
to exploit the vulnerabilities of DNNs. Second, our adversary-
resistant DNN maintains desirable classification performance while
requiring only minimal modification to existing architectures. Third,
the technique we propose can theoretically guarantee the resistance
of DL to adversarial samples. Lastly, while this work is primarily
motivated by the need to safeguard DNN models used in malware
detection, it should be noted that the proposed technique is general
and can be adopted to other applications where deep learning is
popularly applied, such as image recognition. We demonstrate this
applicability using two additional, publicly-available datasets in
Section 5.

The rest of the paper is organized as follows. In Section 2, we
provide background on adversarial samples and, in Section 3, we
survey relevant work. Section 4 presents our technique and de-
scribes its properties. Experimental results appear in Section 5,
where we compare our technique to other approaches. Finally,
conclusions are drawn in Section 6.

2 BACKGROUND
Even though a well-trained model is capable of recognizing out-
of-sample patterns, a deep neural architecture can be easily fooled
by introducing perturbations to the input samples that are often
indistinguishable to the human eye [28]. These so-called “blind
spots”, or adversarial samples, exist because the input space of
DNN is too broad to be fully explored [10]. Given this, an adversary
can uncover specific data samples in the input space and bypass
DNN models. More specifically, it has been shown in [10] that
attackers can find the most powerful blind spots through effective

optimization procedures. In multi-class classification tasks, the
adversarial samples uncovered through this optimization can cause
a DNN model to classify a data point into a class other than the
correct one (and, oftentimes, not even a reasonable alternative).

Furthermore, according to [28], DNN models that share the same
design goal, for example recognizing the same image set, all approx-
imate a common highly complex, nonlinear function. Therefore, a
relatively large fraction of adversarial examples generated from one
trained DNN will be misclassified by other DNN models trained
from the same data set but with different hyper-parameters. Given
a target DNN, we refer to adversarial samples that are generated
from other different DNN models but still maintain their attack
efficacy against the target as cross-model adversarial samples.

Adversarial samples are generated by computing the derivative
of the cost function with respect to the network’s input variables.
The gradient of any input sample represents a direction vector
in this high-dimensional input space. Along this direction, any
small change of this input sample will cause a DNN to generate
a completely different prediction result. This particular direction
is important since it represents the most effective way to degrade
the performance of a DNN. Discovering this particular direction is
done by passing the layer gradients from the output layer all the
way back to the input layer via back-propagation. The gradient
at the input may then be applied to the input samples to craft a
adversarial examples.

To be more specific, let us define a cost function L(θ, X , Y ),
where θ , X and Y denotes the parameters of the DNN, the input
dataset, and the corresponding labels respectively. In general, ad-
versarial samples are created by adding an adversarial perturbation
δX to real samples. In [10], the straightforward fast gradient sign
method was proposed for calculating adversarial perturbations as
shown in (1):

δX = ϕ · siдn(JL(X )),

(1)

here δX is calculated by multiplying the sign of the gradients of real
sample X with some coefficient ϕ. JL(X ) denotes the derivative of
the cost function L(·) with respect to X . ϕ controls the scale of the
gradients to be added.

An adversarial perturbation indicates the actual direction vec-
tor to be added to real samples. This vector drives a data point X
towards a direction that the cost function L(·) is significantly sen-
sitive to. However, it should be noted that δX must be maintained
within a small scale. Otherwise adding δX will cause significant
distortions to real samples, leaving the manipulation to be easily
detected.

3 RELATED WORK
In order to counteract adversarial samples, recent research has been
mainly focused on two different approaches – data augmentation
and model complexity enhancement. In this section, we summarize
these techniques and discuss their limitations as follows.

1In fact, “inverted” drop-out is applied in practice, which requires an extra division
of the drop-out probability at training time in order to avoid the need for re-scaling
at test-time. This specific implementation is used so that feed-forward inference is
directly comparable to that under standard DNNs.

3.1 Data Augmentation
To resolve the issue of “blind spots” (a more informal name given
to adversarial samples), many methods that could be considered

Adversary Resistant Deep Neural Networks with an Application to Malware Detection

KDD’17, Aug. 2017, Halifax, CA

as sophisticated forms of data augmentation2 have been proposed
(e.g. [10, 12, 21]). In principle, these methods expand their training
set by combining known samples with potential blind spots, the
process of which has been called adversarial training [10]. Here, we
analyze the limitations of data augmentation mechanisms and argue
that these limitations also apply to adversarial training methods.
Given the high dimensionality of data distributions that a DNN
typically learns from, the input space is generally considered infi-
nite [10]. This implies that, for each DNN model, there could also
be an adversarial space carrying an infinite amount of blind spots.
Therefore, data augmentation based approaches face the challenge
of covering these very large spaces. Since adversarial training is a
form of data augmentation, such a tactic cannot possibly hope to
cover an infinite space.

Adversarial training can be formally described as adding a regu-
larization term known as DataGrad to a DNN’s training loss func-
tion [21]. The regularization penalizes the directions uncovered
by adversarial perturbations (introduced in Section 2). Therefore,
adversarial training works to improve the worst case performance
of a standard DNN. Treating the standard DNN much like a genera-
tive model, adversarial samples are produced via back-propagation
and mixed into the training set and directly integrated into the
model’s learning phase. Despite the fact that there exists an infi-
nite amount of adversarial samples, adversarial training has been
shown to be effective in defending against those which are power-
ful and easily crafted. This is largely due to the fact that, in most
adversarial training approaches [10, 21], adversarial samples can be
generated efficiently for a particular type of DNN. The fast gradient
sign method [10] can generate a large pool of adversarial samples
quickly while DataGrad [21] focuses on dynamically generating
them per every parameter update. However, the simplicity and
efficiency of generating adversarial samples also makes adversarial
training vulnerable when these two properties are exploited to at-
tack the adversarial training method itself. Given that there exist
infinite adversarial samples, we would need to repeat an adversarial
training procedure each time a new adversarial example is discov-
ered. Let us briefly consider DataGrad [21], which could be viewed
as taking advantage of adversarial perturbations to better explore
the underlying data manifold. While this leads to improved gener-
alization, it does not offer any guarantees in covering all possible
blind-spots. In this work, we do not address this issue by training
a DNN model that covers the entire adversarial space. Rather, our
design principle is to increase the difficulty for adversaries to find
adversarial space in an efficient manner.

3.2 Enhancing Model Complexity
DNN models are already complex, with respect to both the nonlin-
ear function that they try to approximate as well as their layered
composition of many parameters. However, the underlying archi-
tecture is straightforward when it comes to facilitating the flow of
information forwards and backwards, greatly alleviating the effort
in generating adversarial samples. Therefore, several ideas [12, 23]
have been proposed to enhance the complexity of DNN models,

2Data augmentation refers to artificially expanding the data-set. In the case of images,
this can involve deformations and transformations, such as rotation and scaling, of
original samples to create new variants.

Figure 1: A DNN equipped with a random feature nullifica-
tion layer.

aiming to improve the tolerance of complex DNN models with
respect to adversarial samples generated from simple DNN models.
[23] developed a defensive distillation mechanism, which trains a
DNN from data samples that are “distilled” from another DNN. By
using the knowledge transferred from the other DNN, the learned
DNN classifiers become less sensitive to adversarial samples. Al-
though shown to be effective, this method is still vulnerable. This
is because both DNN models used in this defense can be approx-
imated by an adversary via training two other DNN models that
share the same functionality and have similar performance. Once
the two approximating DNN models are learned, the attacker can
generate adversarial samples specific to this distillation-enhanced
DNN model. Similar to [23], [12] proposed to stack an auto-encoder
together with a standard DNN. It shows that this auto-encoding
enhancement increases a DNN’s resistance to adversarial samples.
However, the authors also admit that this stacked model can be
easily approximated and exploited.

Given the observation and analysis above, going beyond conceal-
ing adversarial space, we argue that an adversary-resistant DNN
model also needs to be robust against adversarial samples generated
from its best approximation. In light of these, this paper presents
a new adversary-resistant DNN that not only increases the diffi-
culty in finding its blind spots but also immunizes itself against
adversarial samples generated from its best approximation.

4 RANDOM FEATURE NULLIFICATION
Figure 1 illustrates a DNN equipped with our random feature nul-
lification method. Different from a standard DNN, it introduces
an additional layer between the input and the first hidden layer.
This intermediate layer is stochastic, serving as the source of ran-
domness during both training and testing phases. In particular, it
randomly nullifies or masks the features within the input. Let us
consider image recognition as an example. When a DNN passes
an image sample through the layer, it randomly cancels out some
pixels within the image and then feeds the partially corrupted im-
age to the first hidden layer. The proportion of pixels nullified is
2
determined from hyper parameters µp and σ
p

In this section, in addition to describing feature nullification and
how to train a model using it, we will explain why our method

.

Input Nullification HiddenOutputHidden....KDD’17, Aug. 2017, Halifax, CA

Q. Wang et al.

offers theoretical guarantees of resistance and how it is different
from other adversary-resistant techniques.

4.1 Model Description
Given input samples denoted by X ∈ RN ×M , where N and M
denote the number of samples and features, respectively, random
feature nullification amounts to simply performing element-wise
multiplication of X with ˆIp . Here, ˆIp ∈ RN ×M is a mask matrix
with the same dimensions as X . Note that in performing random
nullification, it is inevitable that some feature information, which
might be useful for classification, will be lost. To compensate for
this, we choose to set a different nullification rate for each data
sample. We hypothesize that this process could potentially lead
to a better exploration of the input data’s underlying manifold
during training, ultimately obtaining a slightly better classification
performance. We will verify this hypothesis with an experiment in
malware classification in Section 5.

When training a DNN, for each input sample xi a corresponding
Ipi is generated, where Ipi is a binary vector, where each element
is either 0 or 1. In Ipi , the total number of zeros, determined by
pi , are randomly distributed, following the uniform distribution.
Formally, we denote the number of zeros in Ipi as (cid:100)M · pi (cid:101), which
will be randomly located, where (cid:100)·(cid:101) is the ceiling function. Note
that pi is sampled from a Gaussian distribution N(µp , σ

From Figure 1, random feature nullification can be viewed as a
process in which a specialized layer simply passes nullified input
to a standard DNN. As such, the objective function of a DNN with
random feature nullification can be defined as follows.

2
p ).

min
θ

N
(cid:213)

i=1

L (cid:0)f (xi , Ipi ; θ ), yi

(cid:1).

(2)

Here, yi is the label of the input xi and θ represents the set of model
parameters. The random feature nullification process is represented
by function q(xi , Ipi ) = xi (cid:12) Ipi , where (cid:12) denotes the Hadamard-
Product and f (xi , Ipi ; θ ) = f (q(xi , Ipi ); θ ).

During training, Equation (2) can be solved using stochastic
gradient descent in a manner similar to that of a standard DNN.
The only difference is that for each training sample, the randomly
picked Ipi is fixed during forward and backward propagation until
the next training sample arrives. This makes it feasible to compute
(cid:1) with respect to θ and update θ
the derivative of L (cid:0) f (xi , Ipi ; θ ), yi
accordingly. During the testing process, when model parameters
are fixed, in order to get stable test results, we use the expectation of
2
the Gaussian distribution N(µp , σ
p ) as a substitute for the random
variable pi . More specifically, we generate a vector Ip following the
same procedure described earlier, but with p equal to µp .

4.2 Analysis: Model Resistance to Adversaries
We will now present a theoretical analysis of our model’s ability
to resist adversarial samples. First, recall (Section 2) that an adver-
sary needs to generate adversarial perturbations in order to craft
adversarial samples. According to Equation 1, the adversarial per-
turbation is generated by computing the derivative of the DNN’s
cost function with respect to the input samples.

Now let us assume that an adversary uses the same procedure to
attack our proposed model. To be specific, the adversary computes

Figure 2: An example of generating an adversarial sample
and testing it on a DNN with random feature nullification.

the partial derivative of L (cid:0) f ( ˜x, Ip ; θ ), ˜y(cid:1) with respect to ˜x, where ˜x
denotes an arbitrary testing sample and ˜y denotes the corresponding
label. More formally, the adversary needs to solve the following
derivative:

(3)

JL( ˜x) =

∂L (cid:0) f ( ˜x, Ip ; θ ), ˜y(cid:1)
∂ ˜x
∂q( ˜x, Ip )
∂ ˜x

.

= JL(q) ·
where JL(q) = ∂L (cid:0) f ( ˜x, Ip ; θ ), ˜y(cid:1)/∂q( ˜x, Ip ). Here, as mentioned
earlier, Ip is a mask matrix used during testing. Once the derivative
above (Equation (3)) is calculated, an adversarial sample can be
crafted by adding ϕ · siдn(JL( ˜x)) to ˜x, following [10].

To resolve Equation (3), both JL(q) and ∂q( ˜x, Ip )/∂ ˜x need to
be computed. Note that JL(q) can be easily solved using back
propagation of errors. However, term ∂q( ˜x, Ip )/∂ ˜x carries random
variable Ip , making derivative computation impossible. In other
words, it is the random variable Ip itself that prohibits attackers
from computing a derivative needed to produce an adversarial
perturbation.

Recall that for each sample, the locations of zeroes within Ip are
randomly distributed. It is almost impossible for an adversary to
pick up a value for Ip that will match that which was randomly
generated. Therefore, for this adversary, the best practice would be
to approximate the value of Ip . To allow this adversary to make the
best possible approximation, we further assume that the value of p
is known. With this assumption, one can randomly sample Ip and
treat it as a best approximation I ∗
. Using this approximation, the ad-
p
versary then computes the most powerful adversarial perturbation.
As shown in the top shaded region of Figure 2, for the black-boxed
DNN, we assume the most powerful adversarial perturbation is δ ˜x.
Then, the adversarial perturbation for real sample ˜x is δ ˜x (cid:12) I ∗
p

Assume the adversary uses a synthesized adversarial sample
˜x + δ ˜x (cid:12) I ∗
to attack the system shown in the bottom shaded region
p
of Figure 2. As we can see, the synthesized sample must pass
through the the feature nullification layer before passing through
the actual DNN. We describe this nullification in the following form.

.

(cid:0) ˜x + δ ˜x (cid:12) I ∗
p

(cid:1) (cid:12) Ip = (cid:0) ˜x (cid:12) Ip

(cid:1) + δ ˜x (cid:12) I ∗
p (cid:12) Ip .
Here, ˜x (cid:12) Ip is a nullified real sample, and δ (cid:12) I ∗
p (cid:12) Ip represents
the adversarial perturbation added to it. With I ∗
p (cid:12) Ip , even though
δ ˜x is the adversarial perturbation that impacts the DNN the most,

(4)

BackpropagationFeed-forwardDNNFeed-forwardDNNAdversary Resistant Deep Neural Networks with an Application to Malware Detection

KDD’17, Aug. 2017, Halifax, CA

this high-impact adversarial perturbation is still distorted and no
longer represents the most effective perturbation for fooling the
DNN. In Section 5, we will provide empirical evidence to further
validate this result.

In short, stochasticity, which naturally comes from Ip , is poten-
tially our best defense against adversarial perturbation. It is also
important to interpret our particular form of drop-out as a form
of “security through randomness”. Our parametrized feature nul-
lification input layer, does not serve as a form of implicit model
ensembling (or Bayesian averaging, which drop-out has been shown
to be equivalent to in the case of single hidden-layer networks),
especially given that randomness is still introduced at test-time.

4.3 Comparison with Existing Defense

Methods

In the following, we thoroughly analyze the limited resistance
provided by existing defense techniques introduced in Section 3.
According to [14, 21, 25], existing defense techniques can be gener-
alized as training a standard DNN with various regularization terms
(or even more generally as the DataGrad=regularized objective).
More formally, the general objective is as follows:

min
θ

G(θ, ˜x, ˜y) = L(θ, ˜x, ˜y) + γ · R(θ, ˜x, ˜y),

(5)

where L(θ, ˜x, ˜y) is the training objective for a standard DNN, and
R(θ, ˜x, ˜y) is a regularization term. Here, γ controls the strength
of the regularization. By adding regularization, (5) penalizes the
direction represented by the adversarial perturbation that is optimal
for crafting adversarial samples.

However, existing defense methods that fall under this unifying
framework are still vulnerable to adversarial samples problems, as
shown below. To craft an adversarial sample from a model trained
by solving (5), an adversary can easily produce an adversarial per-
turbation by computing the derivative with respect to a test sample
˜x as follows:

JG( ˜x) = ∂G(θ, ˜x, ˜y)

∂ ˜x

=

∂ (cid:0)L(θ, ˜x, ˜y) + γ R(θ, ˜x, ˜y)(cid:1)
∂ ˜x

(6)

.

This indicates that prior studies only construct DNN models that
are resistant to adversarial samples that target a standard DNN but
do not build resistance to adversarial samples that would be gener-
ated to trick these newly “hardened” models. In addition, as we will
show in Section 5, the added regularization only imposes a limited
penalty to the most effective adversarial perturbation. Hence these
methods might still be ineffective against adversarial samples that
target standard DNNs, especially if an adversary simply increases
the scale factor ϕ when generating adversarial samples.

In other words, according to [10], the space containing both real
samples and adversarial samples is too broad to be exhaustively
explored. In the end, since adversarial training is a form of data
augmentation, it cannot possibly hope to fully solve this problem.
While all machine learning methods are susceptible to a broad space
of adversarial samples, our proposed method, however, is a model-
complexity-based approach that hardly adds any extra parameters,
thus leaving the per-iteration run-time relatively untouched.

5 EVALUATION
In this section, we first evaluate our proposed technique and com-
pare it with adversarial training and dropout for a malware clas-
sification task using the dataset from [4]. Then we will show that
our proposed method can be integrated with existing adversarial
training methods and compare the combined approach’s perfor-
mance with both standalone methods – random feature nullification
(RFN) and adversarial training, respectively. Finally, we will demon-
strate the generality of our proposed method by conducting some
experiments in image recognition. In particular, we contrast our
method with adversarial training and dropout on the MNIST [17]
and CIFAR-10 [16] datasets.

5.1 Datasets & Experimental Design
To comprehensively evaluate our method, we measure classifica-
tion accuracy as well as model resistance to adversarial samples.
In particular, to evaluate and compare the resistance of all three
defense techniques, we test the DNN models against adversarial
samples generated from the exact models trained either with RFN,
adversarial training, and dropout. This means that we created three
adversarial sample pools, one for each dataset (i.e., malware dataset,
MNIST and CIFAR-10). The evaluation of resistance assumes that
adversaries had acquired the full knowledge of each DNN model
(i.e. hyper-parameters) and could construct the most effective ad-
versarial samples to the best of their abilities. In this experimental
setting, the observed resistance will then reflect a lower bound on
model resistance against adversarial samples. For each dataset, we
specify how to craft adversarial samples, especially with respect to
the malware dataset.
Malware. The malware dataset we experimented with is a collec-
tion of window audit logs3. The dimensionality of the feature-space
for each audit log sample is reduced to 10,000 according to the fea-
ture select metric used in [4]. Each feature indicates the occurrence
of either a single event or a sequence of events4, thus taking on the
value of 0 or 1. Here, 0 indicates that the sequence of events did
not occur while 1 indicates the opposite. Classification labels are
either 1, indicating a malware variant, or 0, indicating a benign pro-
gram. The dataset is split into 26,078 training examples, with 14,399
benign software samples and 11,679 malicious software samples,
and 6,000 testing samples, with 3,000 benign software samples and
3,000 malicious software samples. The task is to classify whether a
given sample is benign or malicious.

Adversarial perturbation for malware samples can be computed
according to Equation (1). However, a bit of care must be taken
when generating adversarial samples for the malware dataset. Mal-
ware samples are usually represented by features that take on dis-
crete and finite values, e.g. records of file system accesses, types of
system calls incurred, etc. Therefore, it is more appropriate to use
the l0 distance:

|| ˆx − x ||0 < ε,
where ˆx = x + δx represents adversarial samples generated from
legitimate sample x.

(7)

3Window audit logs are collected using standard, built-in facilities, composed of two
sources–users of a enterprise network as well as sandboxed virtual machine simulation
runs using a set of malicious and benign binaries
4The number of events in one sequence can be as high as 3.

KDD’17, Aug. 2017, Halifax, CA

Q. Wang et al.

Examples of Changed Features

WINDOWS FILE:Execute:[system]\slc.dll,
WINDOWS FILE:Execute:[system]\cryptsp.dll
WINDOWS FILE:Execute:[system]\wersvc.dll,
WINDOWS FILE:Execute:[system]\faultrep.dll
WINDOWS FILE:Execute:[system]\imm32.dll,
WINDOWS FILE:Execute:[system]\wer.dll
WINDOWS FILE:Execute:[system]\ntmarta.dll,
WINDOWS FILE:Execute:[system]\apphelp.dll
WINDOWS FILE:Execute:[system]\faultrep.dll,
WINDOWS FILE:Execute:[system]\imm32.dll
Table 1: Illustration of manipulated features in malware
dataset: each feature in this table contains a sequence of two
events. The two events in each row happened in the same or-
der as displayed.

A similar approach was also adopted in [11]. Furthermore, as
discussed in [11], malware data contains stricter semantics in com-
parison to image data. In our case, each feature of a malware sample
indicates whether or not a potential bit of malware has initiated
a certain file system access. Therefore, large-scale manipulations
across all features, as is typically done with image data, may break
down a malicious program’s functionality. To avoid this, we restrict
the total number of manipulations that can occur per malware sam-
ple to be as small as possible. In this paper’s setting, we set this to
be 10. Moreover, since removing certain file system calls may also
jeopardize a malware’s internal logic, we further restrict the manip-
ulation by only allowing the addition of new file system accesses.
This equivocates to only positive manipulations, i.e. changing a
feature from 0 to 1. Finally, since malware manipulation is done
with the intent of fooling a DNN malware classifier, there is no need
to modify a benign application such that it is classified as malicious.
Therefore, in our experiments we only generate adversarial samples
from the malware data points. In Table 1, we show a few examples
of features added to a malware sample. These added features only
cause the malware to call several dynamically linked library files
without damaging the program’s malicious intent.
MNIST & CIFAR-10. The MNIST dataset is composed of 70,000
greyscale images (of 28×28, or 784, pixels) of handwritten digits,
ranging from 0 to 9. The dataset is split into a training set of 60,000
samples and a test set of 10,000 samples.

The CIFAR-10 dataset consists of 60,000 images, divided into
10 classes. The training split contains 50,000 samples while the
test split contains 10,000 samples. Since the samples of CIFAR-10
dataset are color images, each image is made up of 32×32 pixels
where each pixel is represented by three color channels (i.e., RGB).
For the MNIST and CIFAR-10 datasets, we generate adversarial
samples by simply adding the adversarial perturbation δx, intro-
duced in Section 2), directly to the original image (since feature
values are continuous/real-valued). The degree of manipulation is
controlled by selecting different ϕ, as in Equation (1).

5.2 Malware Classification Results
Sensitivity to Nullification Rate We first implement a group of
experiments to quantify the effect that nullification rates have on
model classification accuracy as well as model resistance. More
specifically, we allow the nullification rate to range from 10% to

Expectation of
nullification rates (%)
10
20
30
40
50
60
70
80
90

Malware

Accuracy (%)
95.22
94.67
93.92
95.20
93.18
93.77
93.10
93.08
90.88

Resistance (%)
36.46
36.76
38.56
45.19
51.43
49.03
53.96
62.30
64.86

Table 2: Classification accuracy vs. model resistance with
various feature nullification rates on the malware dataset.
Note that the nullification rate hyper-parameter p is simply
an expectation, as detailed in Section 4, while the other hy-
per parameter σ is set to be 0.05 for these experiments.

90% with 10% increments, both at training and testing time. By
comparing each experiment result, we may then select the optimal
nullification rate. We then integrate our defense mechanism with
adversarial training and compare it against all aforementioned
methods.

Measures of classification accuracy and model resistance, cor-
responding to different nullification rates, are shown in Table 2.
As observed in Table 2, the classification accuracy of trained mod-
els decreases when the nullification rate is increased except when
nullification rate is at 40% or 60%. These two rates may roughly
imply the proportion of noise contained within the original dataset.
The average classification accuracy is 93.66% while the highest
achieved is 95.22, when nullification rate is 10%. This shows us that
classification performance is more negatively impacted as more
important features are discarded. Note that the accuracy remains at
a surprisingly high value even when the nullification rate reaches
90%. This aligns with the fact that the malware data is quite sparse.
On the contrary, as shown in Table 2, model resistance shows the
opposite trend. Maximum resistance against adversarial samples is
reached at a 90% nullification rate. Clearly, with such a high nul-
lification rate, more carefully manipulated features are discarded.
The different trends for both classification accuracy and resistance
demonstrate well the trade-off between achieving one of the two
key goals (i.e., accuracy and robustness). By examining Table 2, we
adopt 80% as our feature nullification rate expectation for experi-
ments that follow, as the trained model with this nullification rate
maintains the best balance between resistance and accuracy.
Comparative Results Next, we implement five distinct DNN mod-
els by training them with different learning techniques as specified
in Table 3. We present the architecture of these DNN models as
well as the corresponding hyper-parameters in the Appendix. With
certain perturbations added to the data samples, Table 3 first shows
that the standard DNN model exhibits poor resistance when classi-
fying adversarial samples. Surprisingly, as shown for dropout and
adversarial training, these two methods yield even worse resistance
compared to the standard DNN. This strengthens our previous anal-
ysis in Section 4. Although these mechanisms have been shown to
provide certain resistance to already seen adversarial samples and

Adversary Resistant Deep Neural Networks with an Application to Malware Detection

KDD’17, Aug. 2017, Halifax, CA

Defense Methods

Malware
Accuracy (%) Resistance (%)

Expectation of
nullification rates

Standard
Dropout
Adv Training
RFN
Adv Training
& RFN

93.99
93.16
92.68
93.08

94.81

30.00
13.96
26.07
62.30

68.77

Table 3: Classification accuracy vs. model resistance of dif-
ferent learning technologies on the malware dataset. In this
table, dropout rates are 50% and feature nullification rates
are 80%.
‘Adv Training’ simply means adversarial training.
Note that ’Standard’ means standard deep neural architec-
ture without any regularization.

so-called ‘cross-model’ adversarial samples 5, they are even more
vulnerable to more specifically crafted adversarial samples. These
results are also consistent with those reported in [12]. This implies
that the regularization involved in adversarial training and dropout
offer poor general resistance to adversarial examples.

In comparison, RFN provides a significantly better resistance
against adversarial samples, as is shown in Table 3. The model
resistance afforded by our method improves more than 100% (rela-
tive error) when comparing with standard DNN. Recall that RFN
can also be viewed as a preprocessing approach for the successive
DNN. As such, it can be combined with other existing defense
mechanisms. It is expected that such a combination would further
improve model robustness. In order to verify this, we next combine
RNF with adversarial training and compare the hybrid approach to
both standalone RFN and adversarial training.

Table 3 specifies the classification accuracy and model resistance
of the hybrid technique. We observe that the combined technique
does indeed provide better resistance when compared to standalone
RFN. This may due to the fact that RFN and adversarial training
penalize adversarial samples in two different manners, and an en-
semble of the two favorably amplifies the model resistance that
each technique induces. From Table 3, we also notice that both
standalone RFN and aforementioned combined approach do slightly
but noticeably reduce classification accuracy. However, the combi-
nation of RFN and adversarial training results in near-negligible
degradation. This indicates that RFN, either standalone or when
combined with adversarial training, provides much better resistance
either adversarial training and dropout on the malware dataset.

5.3 Image Recognition Results
In the following experiments, we examine the generality of our
proposed method by applying it to the MNIST and CIFAR-10 image
recognition tasks. For MNIST, we build a standard feed-forward
fully connected DNN, while for CIFAR-10, we build a convolutional
neural network (CNN). Similar to the experiments implemented on
malware dataset, we also implement two groups of experiments,
one for determining the optimal p on each dataset, and another for
comparing between different defense technologies. Other hyper-
parameters and neural architectural details appear in the Appendix.

5Adversarial samples that are crafted from a different DNN that is built to approximate
some standard targeted DNN.

MNIST

CIFAR-10

Accuracy

98.17%
98.09%
97.89%
97.53%
96.78%

Resistance
ϕ = 0.15
70.39%
73.55%
78.31%
81.49%
83.68%

Accuracy

80.01%
77.62%
75.95%
74.49%
74.02%

Resistance
ϕ = 0.15
55.87%
59.55%
61.63%
65.59%
67.85%

10%
20%
30%
40%
50%

Table 4: Classification accuracy vs. model resistance with
various feature nullification rates on MNIST and CIFAR-10.
Hyper parameter σ is also set to be 0.05 in this evaluation.

As is shown in Table 4, the trend of accuracy and resistance
are consistent with that found in the malware experiments. Max-
imum resistance against adversarial image samples is reached at
50% nullification rate. With respect to classification accuracy, our
proposed method demonstrates roughly similar performance at
various nullification rates. Based on this result, we adopted 50% as
our feature nullification rate in the experiments to follow.

In Table 5, we show measures of classification accuracy and
model resistance of all aforementioned approaches on the MNIST
and CIFAR-10 datasets. Much as in the malware experiments, we
further evaluate our RFN method combined with adversarial train-
ing on both datasets. In Table 5, we also measure the resistance of
these DNN models against various coefficients ϕ.

As is shown in Table 5, adversarial samples generated from a
standard DNN are capable of lowering the accuracy of the standard
DNN to as low as 0.01% on MNIST and 10.68% on CIFAR-10. In
contrast, all of the investigated defense mechanisms yield improved
resistance, with, again, models trained with RFN reaching the best
level of resistance. In addition, the combination of RFN with adver-
sarial training achieves the best resistance of 91.28 on MNIST and
74.12% on CIFAR-10. Though different than in the case of malware
classification, both dropout and adversarial training alone do pro-
vide somewhat improved resistance on both datasets. This indicates
that the resistance provided by these methods might be highly de-
pendent on the data type (images, in this case). In particular, since
adversarial training is designed to handle adversarial samples, it
demonstrates much better resistance when compared directly to
dropout, though both methods offer model regularization.

As for classification accuracy, dropout achieves the highest accu-
racy on both datasets. For MNIST dataset, both RFN and adversarial
training, as well as their combination, do trade some classification
accuracy for better resistance. However, for the CIFAR-10 dataset,
these methods demonstrate slightly improvement for accuracy. This
is due to the fact that the CIFAR-10 task is much more complex
than that of MNIST, hence the regularization provided by all of
these methods leads to improved generalization. In general, despite
the minor accuracy degradation caused by using RFN or the hy-
brid method, the significant improvement over resistance in both
datasets demonstrates that our proposed method is quite promising
for classification tasks when resistance to adversarial samples is
important. Finally, our method is agnostic to the choice of the DNN
architecture, given that we evaluate RFN with both feed-forward
fully connected DNNs and CNNs (as evidenced in Table 5).

KDD’17, Aug. 2017, Halifax, CA

Q. Wang et al.

Learning
Technology

Accuracy

Standard
Dropout
Adv Training
RFN
Adv Training & RFN

98.43
98.61
97.46
96.78
96.11

MNIST

ϕ = 0.15
8.19
19.51
67.68
83.69
91.28

Resistance
ϕ = 0.25
0.56
3.86
28.37
71.44
84.92

ϕ = 0.35
0.01
0.96
7.62
60.69
78.18

Accuracy

73.59
81.07
80.62
74.02
74.12

CIFAR-10

ϕ = 0.15
19.48
17.43
33.97
67.85
71.03

Resistance
ϕ = 0.25
13.51
16.59
19.76
51.89
55.49

ϕ = 0.35
10.68
16.40
13.73
41.29
49.84

Table 5: Classification accuracy vs. model resistance with different learning methods, under different ϕ, for both MNIST and
CIFAR-10. In this table, dropout rates and feature nullification rates are set 50% for both datasets.

6 CONCLUSION
In this paper, we proposed a simple method for constructing deep
neural network models that are robust to adversarial samples. Our
design is based on a thorough analysis of neural model’s vulnera-
bility to adversarial perturbation as well as the limitations of pre-
viously proposed defenses. Using our proposed Random Feature
Nullification, we have shown that it is impossible for an attacker to
craft specifically designed adversarial sample that can force a DNN
to misclassify its inputs. This implies that our proposed technology
does not suffer, as previous methods do, from attacks that rely on
generating model-specific adversarial samples.

We apply our method to malware dataset and empirically demon-
strated that we significantly improve model resistance with only
negligible sacrifice of accuracy, compared to other defense mecha-
nisms. Cross-data generality was also demonstrated through exper-
iments in image recognition. Future work will entail investigating
the performance of our method to an even wider variety of applica-
tions.

REFERENCES
[1] Hyrum Anderson, Jonathan Woodbridge, and Bobby Filar. 2016. DeepDGA:
Adversarially-Tuned Domain Generation and Detection. arXiv:1610.01969 [cs.CR]
(2016).

[2] Matt Wolff Andrew Davis.

2015.

assembly.
us-15-Davis-Deep-Learning-On-Disassembly.pdf.

on Dis-
https://www.blackhat.com/docs/us-15/materials/

Learning

Deep

[3] Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. 2010. The

Security of Machine Learning. Mach. Learn. 81, 2 (Nov. 2010), 121–148.

[4] Konstantin Berlin, David Slater, and Joshua Saxe. 2015. Malicious behavior
detection using windows audit logs. In Proceedings of the 8th ACM Workshop on
Artificial Intelligence and Security. ACM, 35–44.

[5] Ran Bi. 2015. Deep Learning can be easily fooled. http://www.kdnuggets.com/

2015/01/deep-learning-can-be-easily-fooled.html.

[6] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks against
Machine Learning at Test Time.. In ECML/PKDD (3).

[7] BIZETY 2016.
AI Malware.
deep-learning-neural-nets-are-effective-against-ai-malware/.

Deep Learning Neural Nets Are Effective Against
https://www.bizety.com/2016/02/05/
BIZETY.

[9]

2015.

[8] George Dahl, Jack W. Stokes, Li Deng, and Dong Yu. 2013. Large-Scale Malware
Classification Using Random Projections and Neural Networks. In Proceedings
IEEE Conference on Acoustics, Speech, and Signal Processing.
Ian Goodfellow.
Clarifying Misconceptions.
deep-learning-adversarial-examples-misconceptions.html.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[11] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2016. Adversarial Perturbations Against Deep Neural Net-
works for Malware Classification. arXiv preprint arXiv:1606.04435 (2016).
[12] Shixiang Gu and Luca Rigazio. 2014. Towards deep neural network architectures

Deep Learning Adversarial Examples –
http://www.kdnuggets.com/2015/07/

[10]

robust to adversarial examples. arXiv:1412.5068 [cs] (2014).
The

James.

2014.

Flaw Lurking In Every Deep Neural
http://www.i-programmer.info/news/105-artificial-intelligence/

[13] Mike
Net.

7352-the-flaw-lurking-in-every-deep-neural-net.html.

[14] D.K. Kang, J. Zhang, A. Silvescu, and V. Honavar. 2005. Multinomial event model
based abstraction for sequence and text classification. Abstraction, Reformulation
and Approximation (2005), 901–901.

[15] Will Knight. 2015.
More Malware.
antivirus-that-mimics-the-brain-could-catch-more-malware/.

the Brain Could Catch
https://www.technologyreview.com/s/542971/

Antivirus That Mimics

[16] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features

from tiny images. (2009).

[17] Yann LeCun, Corinna Cortes, and Christopher JC Burges. 1998. The MNIST

database of handwritten digits. (1998).
2015.
Baidu,
Spot Malware.

[18] Cade Metz.
ing AI
to
baidu-the-chinese-google-is-teaching-ai-to-spot-malware/.

the Chinese Google,

Teach-
https://www.wired.com/2015/11/

Is

[19] MIT Technology Review 2016.

the Darknet
nology Review.
machine-learning-algorithm-combs-the-darknet-for-zero-day-exploits-and-finds-them/.

Machine-Learning Algorithm Combs
MIT Tech-
https://www.technologyreview.com/s/602115/

for Zero Day Exploits, and Finds Them.

[20] Linda Musthaler. 2016. How to use deep learning AI to detect and prevent malware
and APTs in real-time. http://www.networkworld.com/article/3043202/security/
how-to-use-deep-learning-ai-to-detect-and-prevent-malware-and-apts-in-real-time.
html.

[21] Alexander G. Ororbia II, C. Lee Giles, and Daniel Kifer. 2016. Unifying Adver-
sarial Training Algorithms with Flexible Deep Data Gradient Regularization.
arXiv:1601.07213 [cs] (2016).

[22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 372–387.

[24]

[23] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2015. Distillation as a defense to adversarial perturbations against deep neural
networks. arXiv preprint arXiv:1511.04508 (2015).
Joshua Saxe and Konstantin Berlin. 2015. Deep Neural Network Based Malware
Detection Using Two Dimensional Binary Program Features. CoRR (2015).
[25] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. Journal of Machine Learning Research 15, 1 (2014), 1929–1958.
[26] Nedim Srndic and Pavel Laskov. 2014. Practical Evasion of a Learning-Based
Classifier: A Case Study. In Proceedings of the 2014 IEEE Symposium on Security
and Privacy.

[27] Symantec 2016. Internet Security Threat Report. Symantec. https://www.symantec.

com/content/dam/symantec/docs/reports/istr-21-2016-en.pdf.

[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural
networks. In International Conference on Learning Representations.

[29] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. 2014. Droid-Sec:
Deep Learning in Android Malware Detection. In Proceedings of the 2014 ACM
Conference on SIGCOMM (SIGCOMM ’14).

7 APPENDIX
To augment the experimental setup presented in Section 5, the
tables in this part contain the hyper parameters used for model
training.

Adversary Resistant Deep Neural Networks with an Application to Malware Detection

KDD’17, Aug. 2017, Halifax, CA

Learning Technologies

Standard DNN
Dropout
Adv. Training
RFN
RFN & Adv. Training

DNN Structure
784-784-784-784-10
784-784-784-784-10
784-784-784-784-10
784-784-784-784-10
784-784-784-784-10

Hyper Parameters

Activation Optimizer

Learning Rate Dropout Rate Batch Size Epoch

Relu
Relu
Relu
Relu
Relu

SGD
SGD
SGD
SGD
SGD

0.1
0.1
0.01
0.1
0.01

×
0.5
0.5
0.25
0.25

100
100
100
100
100

25
25
70
25
70

Table 6: The hyper parameters of MNIST models: the network structure specifies the number of hidden layers and hidden
units in each layer. The last layer of each model is followed by Softmax non-linearity. Note that standard DNN stands for DNN
trained without any regularization.

Learning Technology

Standard DNN
Dropout
Adv. Training
RFN
RFN & Adv. Training

Hyper Parameters

DNN Structure Activation Optimizer
5000-1000-100-2
5000-1000-100-2
5000-1000-100-2
5000-1000-100-2
5000-1000-100-2

Adam
Adam
SGD
Adam
SGD

Relu
Relu
Relu
Relu
Relu

Learning Rate Dropout Rate Batch Size Epoch

0.001
0.001
0.01
0.001
0.01

×
0.5
0.5
0.5
0.5

500
500
500
500
500

20
20
40
15
40

Table 7: The hyper parameters of Malware models.

Learning Technology

Standard DNN
Dropout
Adv. Training
RFN
RFN & Adv. Training

Hyper parameters

Activation Optimizer

Learning rate Dropout rate Batch Size Epoch

Relu
Relu
Relu
Relu
Relu

Adam
Adam
SGD
Adam
SGD

0.001
0.001
0.01
0.001
0.01

×
0.5
0.5
0.5
0.5

128
128
128
128
128

50
50
50
50
50

Table 8: The hyper parameters of CIFAR-10 models, in this evaluation we use CNN instead of standard DNN

Layer type

Convolutional
Convolutional
Max pooling
Convolutional
Convolutional
Max pooling
Fully Connect
Fully Connect
Softmax

Standard DNN
64 filter(3 × 3)
64 filter(3 × 3)
2 × 2
72 filter(3 × 3)
72 filter(3 × 3)
2 × 2
512 units
256 units
10 units

Dropout
64 filter(3 × 3)
64 filter(3 × 3)
2 × 2
72 filter(3 × 3)
72 filter(3 × 3)
2 × 2
512 units
256 units
10 units

Learning Technology
Adv. Training
64 filter(3 × 3)
64 filter(3 × 3)
2 × 2
128 filter(3 × 3)
128 filter(3 × 3)
2 × 2
256 units
256 units
10 units

RFN
64 filter(3 × 3)
64 filter(3 × 3)
2 × 2
128 filter(3 × 3)
128 filter(3 × 3)
2 × 2
256 units
256 units
10 units

RFN & Adv. Training
64 filter(3 × 3)
64 filter(3 × 3)
2 × 2
128 filter(3 × 3)
128 filter(3 × 3)
2 × 2
256 units
256 units
10 units

Table 9: The network structure of CIFAR-10 models: the activation function of convolutional layers and fully connected layers
are shown in Table 8.

