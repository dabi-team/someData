MicroAnalyzer: A Python Tool for 
Automated Bacterial Analysis with 
Fluorescence Microscopy

Jonathan Reiner1Â¶, Guy Azran1Â¶, Gal Hyams1 

1The Department of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel 
Â¶These authors contributed equally to this work. 

Abstract  â€”  Fluorescence  microscopy  is  a 

existing  tools  do  not  consider  experiment 

widely  used  method  among  cell  biologists  for 

assumptions,  nor  do  they  provide  fluorescence 

studying  the  localization  and  co-localization  of 

cluster  detection  without  the  need  for  any 

fluorescent  protein.  For  microbial 

cell 

specialized equipment. 

biologists,  these  studies  often  include  tedious 

and  time-consuming  manual  segmentation  of 

bacteria  and  of  the  fluorescence  clusters  or 

working  with  multiple  programs.  Here,  we 

present MicroAnalyzer - a tool that automates 

these tasks by providing an end-to-end platform 

for microscope image analysis. While such tools 

do exist, they are costly, black-boxed programs. 

Microanalyzer 

offers 

an 

open-source 

alternative to these tools, allowing flexibility and 

The key goal of MicroAnalyzer is to automate 

the entire process of cell and fluorescence image 

analysis  â€œfrom  microscope 

to  databaseâ€, 

meaning  it  does  not  require  any  further  input 

from the researcher except for the initial deep-

learning  model  training.  In  this  fashion,  it 

allows  the  researchers  to  concentrate  on  the 

bigger picture instead of granular, eye-straining 

labor.  

expandability by advanced users. 

I. 

INTORDUCTION 

MicroAnalyzer  provides  accurate  cell  and 

1.1. Background 

fluorescence  cluster  segmentation  based  on 

Recent 

advancements 

in 

fluorescence 

state-of-the-art  deep-learning 

segmentation 

microscopy  allow  researchers  to  detect  proteins  

models,  combined  with  ad-hoc  post-processing 

within  single-celled  microorganisms  and 

to 

and  Colicoords  -  an  open  source  cell  image 

determine  their  specific  subcellular  localization 

analysis  tool  for  calculating  general  cell  and 

(different patterns of localizations can be seen in [1]  

fluorescence  measurements.  Using 

these 

Figure  1),  providing  new  insights  into  numerous 

methods, 

it  performs  better  than  generic 

molecular  processes  [2]â€“[6].  The  endcaps  of  rod-

approaches since the dynamic nature of neural 

shaped bacterial cells, termed poles, emerge as hubs 

networks  allows  for  a  quick  adaptation  to 

for protein clusters [7], [8]. 

experiment restrictions and assumptions. Other 

 
 
 
Protein  localization  studies  can  be  performed 

that  can  obtain  spatial  data  on  single  fluorescent 

using  microscopic  image  analysis.  In  order  to 

molecules. While this technique simplifies the issue 

produce the images, the researchers must: 

of  cluster  segmentation  immensely,  SR  capable 

-  Grow  bacteria  with  fluorescence  proteins 

microscopes  can  be  very  expensive  and  are  not 

bound to the proteins being studied. 

available in every lab. 

-  Carefully and evenly place the bacteria on a 

petri  dish,  perhaps  using  an  adhesive 

material in order to keep the cells in place. 

-  Taking multiple images of different regions 

on the petri dish. 

Open-source  developers  and  some  private 

companies have been attempting to fully automate 

the  process  of  cell  and  cell  nuclei  segmentation 

using  deep-learning 

techniques 

[17], 

[18]. 

However,  these  algorithms  attempt  to  generalize 

this task to many different bacterial species and try 

Advancements 

in 

fluorescence  microscopy 

to find as many cells as possible, meaning they do 

automation  over  the  past  decade  have  given 

not take into consideration experiment constraints, 

researchers  the  ability  to  produce  thousands  of 

such as the  researchersâ€™ preferred cell  size and or 

images  overnight  [9]â€“[11],  creating  a  demand  for 

desired  spacing.  Furthermore,  none  of  them  [17], 

fast and reliable microscopic image analysis on the 

[18]  handle  segmentation  of  fluorescence  clusters 

bacterial  images  and  their  fluorescence  channels. 

which  is  required  for  calculating  localization 

Current methods for performing these tasks involve 

metrics of the observed material. 

using  specialized  programs  that  allow  manual 

segmentation of the cells and fluorescence clusters 

for  every  image,  then  performing  automated 

analysis using the  segmentations  as a baseline for 

their  location  and general  shape  [12],  [13].  While 

part of the segmentation process may be automated 

as well, the used algorithms are not reliable enough 

to allow their outputs to go unchecked. This results 

in  a  time-consuming  process  that  requires  much 

human interaction and decision making. 

After  the  information  has  been  extracted  from 

the  images  (cells  and  clusters  segmentation),  the 

researchers must perform calculations on that data. 

This  information  must  be  accurate  in  order  to 

ensure  the  reliability  of  collected  statistics.  While 

there  are  existing  tools  that  perform  this  analysis, 

the  open-source  (free)  options  are  standalone,  i.e. 

do  not  contain  the  segmentation  feature.  This 

requires the researchers to move data from one tool 

to another, and they must do so carefully to avoid 

A common solutions  for studying fluorescence 

data corruption. 

localization 

include  advanced  super-resolution 

microscopy 

(SR) 

[14] 

techniques  based  on 

fluorescence 

photoactivated 

localization 

microscopy  (FPALM)  [15],  such  as  Stochastic 

optical reconstruction microscopy (STORM) [16], 

2 

MicroAnalyzer attempts to be the solution to the 

following  problem:  how  can  the  process  of 

 
analyzing 1

 rod-shaped 

cells  and  polar 

d)  Fluorescence  clusters  that  do  not  intersect 

fluorescence  clusters  in  a  raw  image  from  a 

with the boundaries of a cell should not be 

microscope be automated? This automation task 

analyzed and are to be viewed as noise. 

can be divided into three subtasks: 

MicroAnalyzer,  is  a  tool  that  accepts  the  labâ€™s 

i.  Cell segmentation â€“ finding a good enough2 

microscopeâ€™s  raw  image  files  and  outputs  a  full 

algorithm  to  get  a  rough  estimation  of  the 

analysis  database  including  fluorescence  channel 

location of the cells under the restrictions of 

data,  under  the  above  assumptions  with  useful 

the experiment. 

visualizations. 

ii. 

Fluorescence  clusters 

segmentation  â€“ 

In  order  to  evaluate  MicroAnalyzerâ€™s  results, 

finding a good enough algorithm to find the 

this  paper 

introduces  a  new  criterion 

for 

location of clusters within cells. 

segmentation  model  validity.  This  criterion  takes 

iii.  Cell  and  fluorescence  analysis  â€“  finding 

accurate  locations  and  measurements  of 

cells  and  fluorescence  clusters  using  the 

into  consideration  the  experimentâ€™s  assumptions 

and  the  possibility  of  multiple  ground  truths  as  a 

result of disagreement between researchers. 

segmentation 

results  acquired 

in 

the 

1.3. Prior Solutions: 

previous tasks. 

There are several available tools for solving cell 

1.2. Experiment Assumptions. 

segmentation,  fluorescence  cluster  segmentation 

The article follows the requirements for a set of 

experiments  conducted  by  Orna  Amster-Choderâ€™s 

lab and thus takes their main assumptions: 

a)  Cells that are too close together are invalid 

and should not be analyzed. 

b)  Cells  that  are  out  of  focus  are  invalid  and 

should not be analyzed. 

and  cell  and  fluorescence  analysis. Some of them 

provide  an  end-to-end  platform  for  all  sub-tasks, 

and some solve only a single sub-task. 

Existing tools that offer the entire pipeline, from 

the  raw  microscope  image  to  the  final  output 

database, such as  ImageJ [12], require the user to 

perform the segmentation for cells and fluorescence 

clusters  manually.  This  is  a  time-consuming  and 

c)  Minimize  false  positive  cell  detections  (a 

error  prone  task.  Private  companies,  Nikon  for 

false positive cell detection is worse than a 

example,  offer  proprietary,  paid  programs  (e.g. 

false negative). 

NIS-Elements  [13])  that  have  similar  features  to 

ImageJ, but offer deep learning algorithms that can 

perform  the  segmentation  task  without  human 

interaction (after training) as a paid plugin.  These 

1 â€œAnalysisâ€ refers to performing calculations for the output 
database (see appendix F) 

2 A â€œgood enoughâ€ algorithm is one that provides â€œvalid 
predictionsâ€ as defined in section 3.1. 

3 

 
 
are  closed  source  tools  and  are  therefore  not 

i)  Cluster  segmentation  â€“  Feature  Pyramid 

expandable. 

Network (FPN) [27], a segmentation neural 

Other open source alternatives offer solutions to 

network.. 

each of the sub-tasks separately, and do not perform 

ii)  Cell  segmentation  -  Mask-RCNN  [28],  an 

the entire task (from microscope to database) in an 

object detection neural-network. 

automated fashion [17]â€“[19]. 

iii) Cell  and  fluorescence  analysis  â€“  The  cell 

Cell  and  cluster  segmentation  can  be  achieved 

analyzing  component  of  MicroAnalyzer 

by using different neural network architectures for 

(CellAnalyzer)  is  a  modified  version  of 

segmentation and object detection [20]â€“[26]. They 

Colicoords  (see  1.3),  that  supports  cluster 

offer  a  general  solution  for  these  segmentations 

segmentation data and calculations. 

which  do  not  require  any  manual  configuration 

(after  training)  and  have  been  found  reliable  for 

similar tasks. 

II.  MICROANALYZER METHODS 

2.1. Components. 

MicroAnalyzer 

consolidates  open 

source 

solutions  for  each  subtask  into  a  single  tool  that 

performs  the  entire  pipeline  of  operations:  cell 

segmentation,  fluorescence  cluster  segmentation 

and  data  analysis  (Figures  1  describe  the  flow  of 

operations performed in the program). The chosen 

methods for each task are: 

FPN  is  an  object  segmentation  convolutional 

neural  network  that  uses  a  special  architecture  in 

order to observe the data at different resolutions, i.e. 

different detail levels, similar to the idea behind the 

U-Net segmentation network [23]. This architecture 

keeps  the  image  at  multiple  resolution  levels  and 

maintains  strong  semantic  features  throughout 

these levels, giving it an edge in segmenting smaller 

objects over its predecessors  

Mask-RCNN 

is  a 

state-of-the-art  object 

detection  neural  network  that  performs  instance 

segmentation  at  the  object-level.  This  is  done  by 

initially  finding  regions  where  the  location  of  an 

object  is  suspected,  then  classifying  the  object  in 

that  region,  and  finally  finding  a  pixel-wise 

Figure 1: The chart describes the full flow of MicroAnalyzer form microscope to database. The microscope takes images and outputs a 
computer readable ND2 file which is input for MicroAnalyzer. It performs segmentation on cells and fluorescence clusters using Mask-
RCNN and FPN networks respectively, performs analysis to create using the CellAnalyzer module and finally output the database. 

4 

 
Figure 2: CellAnalyzer module flow chart for cell and fluorescence cluster analysis. It performs filtering on both clusters and bacteria 
based on minimum cell dimensions and proximity and cluster intersection with cells. Colicoords is used to extract the individual cells from 
the image fits a coordinate system for them, creating the final and accurate binary mask. The extracted cell information is used to calculate 
and populate the database fields. 

segmentation of each object. More accurately, this 

according  to  the  original  microscope  image  using 

model  attempts 

to  minimize 

three 

losses 

the  provided  segmentation  as  a  baseline.  This 

simultaneously: 

â€¢  ğ¿ğ‘ğ‘œğ‘¥ â€“ bounding-box regression (defined 

in [29] appendix C). 

â€¢  ğ¿ğ‘ğ‘™ğ‘ 

 â€“  classification 

loss 

(average 

categorical cross-entropy loss). 

â€¢  ğ¿ğ‘šğ‘ğ‘ ğ‘˜ â€“ mask loss (average binary cross-

entropy loss). 

coordinate system can also be used to map the exact 

location  of  the  cell  in  any  and  all  fluorescence 

channels,  allowing  for  calculations  on 

those 

channels within the cell boundary. 

Using  the  above  tools,  along  deterministic 

calculations  on  the  fluorescence  clusters  (not 

included in Colicoords), allows the user to perform 

high  accuracy  end-to-end  cell  and  fluorescence 

This is in fact an evolution of â€œFaster RCNNâ€ [25] 

cluster  analysis  in  a  fully  automated  environment 

for  finding  the  regions  of  interest  using  an  FPN 

while  being  free  and  open  source.  Moreover,  the 

backbone for pixel-wise segmentation. 

code  is  highly    tunable  and  adaptable  to  other 

Colicoords  is  an  open  source  bacterial  image 

experiments and conditions. 

analyzer  that  structures  microscopy  data  at  the 

2.2. Pipeline 

single-cell  level  and  implements  a  coordinate 

system  describing  each  cell  [19].  Given  a 

microscope  image  of  rod-shaped  cells  and  their 

fluorescence channels and a segmentation of some 

or all of the cells in the image, Colicoords fits the 

aforementioned  coordinate  system  for  each  cell 

Figure 1 shows the entire flow of operations for 

a study using MicroAnalyzer. After the microscope 

has finished a photo session, an output ND2 file is 

created.  This  is  a  Nikon  proprietary  binary  file 

containing all the cameraâ€™s channels, including the 

grayscale  bacteria  image  and  the  fluorescence 

5 

 
intensity 

channels. 

given 

such 

a 

file, 

can be found in the input image and zeros represent 

MicroAnalyzer  initially  separates  the  bacteria 

the background.  

image channel from the fluorescence channels. 

In 

experiments  with 

non-deterministic 

The images are pre-processed (see appendix C) 

assumptions,  one  must  consider  the  possibility  of 

and 

fed 

into 

their 

respective  models 

for 

having  more  than  one  ground  truth  segmentation 

segmentation  /  detection  (Mask-RCNN  for  cells, 

(see Figure 3. For example, assumptions (a) and (b) 

FPN for clusters). The modelsâ€™ outputs are binary 

are  subject  to  researcher  variability  since  the 

masks indicating the location of cells / clusters in 

definitions â€œtoo close togetherâ€ and â€œout of focusâ€ 

the image. 

The  output  masks  are sent to  the  CellAnalyzer 

module  (see  Figure  2)  which  filters  invalid  cells 

and  fluorescence  clusters  using  deterministic 

are open to interpretation. One researcher may find 

a cell acceptably sharp in  an image while another 

may decide it to be blurry and omit it from the final 

segmentation. 

algorithms  based  on  minimal  object  size  and 

There  are  many  possible  ways  to  evaluate  the 

proximity.  All  images  and  their  corresponding 

quality  of  a  segmentation,  such  as  the  classic 

masks  are  analyzed  by  Colicoords,  which 

metrics, including accuracy, precision, recall, etc., 

accurately  extracts  cell 

information  with 

the 

metrics  that  combine  the  classic  metrics,  e.g.  f-

underlying  fluorescence  data.  Using  this  output, 

score,  and  specialized  metrics,  e.g.  Jaccard  loss  / 

CellAnalyzer is able to map fluorescence clusters to 

IoU score. All of the above metrics are measured on 

their  enclosing  cells  and  then  calculate  desired 

a  pixel  level,  meaning  that  they  penalize  a 

database  fields  and  construct  the  output  database 

segmented object if it is slightly smaller or larger 

and visualizations. 

than the ground truth segmentation. Since the task 

III.  EVALUATION METHODS 

only requires finding the general location and shape 

of the objects3, these metrics have less meaning in 

3.1. Segmentation 

this scenario. 

The responsibility of the segmentation phase is 

to find the general location and shape of the cells 

and  fluorescence  clusters  (since  the  accurate 

location and shape are found in the analysis phase). 

This is represented by a binary image (of ones and 

zeros) where the ones represent pixels where a cell 

Alternatively, one can view this problem as an 

Object  Detection  task.  In  that  case,  common 

evaluation  metrics  are  average  precision  (AP), 

average  recall  (AR),  average ğ‘“ğ›½ -score  (AF),  etc., 

based  on  intersection  over  union  score  (IoU) 

thresholding. These metrics fall short as well â€“ most 

of them cannot not take into account the experiment 

3 A crude binary mask of cell / fluorescence clusters that 
tells Colicoords where to search for cells. 

6 

 
 
Figure 3: An example of two different ground truths for the same image. The top row presents the cell segmentation of each researcher (cells 
are randomly colored to divide nearby cells) where the left and right images show segmentations of different researchers. The bottom row 
shows the differences between the segmentations. 

assumptions, e.g. penalize more on false positives, 

â€¢ 

for all (ğ‘–, ğ‘—) âˆˆ ğ‘, ğ‘ ğ‘–[ğ‘–, ğ‘—] = 1 

and  none  of  them  consider  the  possibility  that 

researchers  may 

provide 

very 

different 

segmentations for the same image (see Figure 3).  

The  evaluation  metric  defined 

in 

the  next 

paragraphs  takes  a  similar  approach  to  AF  in  the 

sense that it uses an IoU threshold to identify false 

â€¢  all  neighboring  coordinates (ğ‘˜, ğ‘™) âˆˆ ğ‘› Ã— ğ‘š 

(= (ğ‘– Â± 1, ğ‘—), (ğ‘–, ğ‘— Â± 1)) have pixel value 1 

if and only if (ğ‘˜, ğ‘™) is part of the connected 

component, i.e.: 

ğ‘ ğ‘–[ğ‘˜, ğ‘™] = 1 âŸº (ğ‘˜, ğ‘™) âˆˆ ğ‘ 

positive  and  negative  detections,  but  takes  into 

Define ğ¶ğ‘  to be the set of connected components in 

account  both  this  experiment  assumptions  and 

ğ‘ . It is possible, and even intuitive, to only consider 

multiple ground truth possibilities. 

connected  components 

since  all  cells  and 

3.1.1. Intersecting Connected Components 

fluorescence  clusters  appear  connected  in  the 

Let  ğ‘ 1, ğ‘ 2 âˆˆ ğ‘€ğ‘›Ã—ğ‘š({0,1})  be 

two  binary 

segmentations  of  the  same  image.  A  connected 

images.  

Given  connected  components  ğ‘1, ğ‘2  ,  the  IoU 

component  in  binary  segmentation ğ‘  is  a  set ğ‘ of 

score is defined to be: 

pixel coordinates, i.e. ğ‘ âŠ† ğ‘› Ã— ğ‘š, such that 

7 

 
ğ¼ğ‘œğ‘ˆ(ğ‘1, ğ‘2) =

|ğ‘1 âˆ© ğ‘2|
|ğ‘1 âˆª ğ‘2|

ğ‘ 1  that  appear  in ğ‘ 2. These  are  the  false  negative 

predictions in ğ‘ 1 given that ğ‘ 2 is the ground truth. 

Say  that  they  intersect  if  ğ¼ğ‘œğ‘ˆ(ğ‘1, ğ‘2) â‰¥ ğ‘‡  where 

ğ‘‡ is  a  predefined  threshold.  For  this  experiment, 

choose ğ‘‡ğ¶ğ‘’ğ‘™ğ‘™ = 0.84, and ğ‘‡ğ‘“ğ‘™ğ‘¢ğ‘œ = 0.6. 

3.1.2. Experimental ğ’ğ’†ğ’™-Error 

The 

vagueness 

of 

this 

experimentâ€™s 

assumptions, e.g. assumption (a) uses the term too 

Let ğ‘‡ > 0.5 be the chosen threshold and ğ‘, ğ‘ âˆˆ

close,  can  lead  severe  differences  between  two 

ğ¶ğ‘ 1 ,  ğ‘ âˆˆ ğ¶ğ‘ 2  and  assume  that  ğ¼ğ‘œğ‘ˆ(ğ‘, ğ‘) > ğ‘‡  and 
ğ¼ğ‘œğ‘ˆ(ğ‘, ğ‘) > ğ‘‡.  Then  by  the  pigeonhole  principal, 

researchersâ€™  segmentations  on  the  same  image  as 

one  researcher  might  be  more  conservative  while 

ğ‘ âˆ© ğ‘ â‰  âˆ…  and 

since 

these  are  connected 

the other might be fairly permissive. To account for 

components in the same segmentation then ğ‘ = ğ‘. 

this,  there  must  be  more  than  one  researcher 

This  defines  an  equivalence  relation  given  two 

segmentation.  Let  ğº1  and  ğº2  be  two  possible 

binary segmentations ğ‘, ğ‘ âˆˆ ğ¶ğ‘ 1 âˆª ğ¶ğ‘ 2: 

ground truth segmentations for the same image. 

{ğ‘ 1,ğ‘ 2}

ğ‘  ~

 ğ‘ âŸº ğ¼ğ‘œğ‘ˆ(ğ‘, ğ‘) > ğ‘‡ 

Finally, define 

ğ¶[ğ‘ 1,ğ‘ 2] = {[ğ‘]

{ğ‘ 1,ğ‘ 2}|ğ‘ âˆˆ ğ‘ 1} 

~

In  other  words,  ğ‘1 âˆˆ ğ¶[ğ‘ 1,ğ‘ 2]  and  ğ‘2 âˆˆ ğ¶[ğ‘ 2,ğ‘ 1]  are 
equal if and only if ğ¼ğ‘œğ‘ˆ(ğ‘1, ğ‘2) > ğ‘‡. 

Let  ğ¹ğ‘ƒ  be  a  function  such  that  ğ¹ğ‘ƒ(ğ‘ 1, ğ‘ 2) =

|ğ¶[ğ‘ 1,ğ‘ 2] âˆ– ğ¶[ğ‘ 2,ğ‘ 1]|  is  the  number  of  connected 

components  in  ğ‘ 1  that  do  not  intersect  with  any 

connected components in ğ‘ 2, i.e. extra objects in ğ‘ 1 

that do not appear in ğ‘ 2. These are the false positive 

predictions in ğ‘ 1 given that ğ‘ 2 is the ground truth. 

Let  ğ¹ğ‘  be  a  function  such  that  ğ¹ğ‘(ğ‘ 1, ğ‘ 2) =

Given  one  of 

the  ground 

truth  binary 

segmentations ğºğ‘–, a prediction binary segmentation 
ğ‘ğ‘‘,  and  ğ›½ âˆˆ [0,1] ,  define  the  experimental  ğ‘™ğ‘’ğ‘¥ -

error of ğ‘ğ‘‘ according to ğºğ‘– to be: 

    ğ‘™ğ‘’ğ‘¥(ğ‘ğ‘‘, ğºğ‘–) 

=

ğ›½ â‹… ğ¹ğ‘ƒ(ğ‘ğ‘‘, ğºğ‘–) + (1 âˆ’ ğ›½)ğ¹ğ‘(ğ‘ğ‘‘, ğºğ‘–)
|ğ¶[ğº1,ğº2] âˆª ğ¶[ğº2,ğº1]|

This score attempts to avoid the classic metric issue 

of  pixel-wise 

loss/scoring  while  also 

taking 

assumption  (c)  into  consideration  by  penalizing 

more on extra objects (false positives) than missing 

objects using large ğ›½ value. 

One might want to consider using AF instead of 

ğ‘™ğ‘’ğ‘¥ -error.  However,  using  AF  or  other  similar 

|ğ¶[ğ‘ 2,ğ‘ 1] âˆ– ğ¶[ğ‘ 1,ğ‘ 2]|  is  the  number  of  connected 

metrics, e.g. AP and AR, leads to unintuitive results 

components  in  ğ‘ 2  that  do  not  intersect  with  any 

in  terms  of  the  experiment  assumptions.  For 

connected components in ğ‘ 1, i.e. missing objects in 

example, according to assumption (c) it is better to 

detect no cells in the image than to detect many false 

4 Lower thresholds accepted cell masks that did not provide 
enough context to Colicoords, causing runtime errors and in 
worse cases invalid output data. 

8 

 
 
 
 
positive cells, but the AP, AR and AF for a blank 

the  similarity  of  two  segmentations  in  the  taskâ€™s 

mask will always be 0 (since it  does not find any 

context. 

true positive detections) and detecting all the cells 

in an image (such that many of them are not valid 

for  this  experiment)  will  receive  a  positive  score, 

indicating that it is a better prediction than the blank 

mask (see Table 1). 

3.1.3. Experimental Distance 

The  experimental  distance  between  them  is 

defined as: 

ğ‘‘ğ‘’ğ‘¥(ğº1, ğº2) =

ğ‘™ğ‘’ğ‘¥(ğº1, ğº2) + ğ‘™ğ‘’ğ‘¥(ğº2, ğº1)
2

3.1.4 Valid Predictions 

Given  a  prediction  segmentation  ğ‘ğ‘‘  and  two 

ground  truth  segmentations ğº1, ğº2, ğ‘ğ‘‘ is  called  a 

valid prediction if: 

ğ‘™ğ‘’ğ‘¥(ğ‘ğ‘‘, ğº1) + ğ‘™ğ‘’ğ‘¥(ğ‘ğ‘‘, ğº2)
2

â‰¤ ğ‘‘ğ‘’ğ‘¥(ğº1, ğº2) 

Ultimately, a valid prediction is one that is less â€œfar 

awayâ€  from  the  ground  truth  segmentations  than 

they  are  from  each  other,  meaning  the  prediction 

might  as  well  be  the  segmentation  of  another 

i.e. 

the  average  experimental  ğ‘™ğ‘’ğ‘¥ -error  of 

researcher, i.e. another ground truth segmentation. 

considering each segmentation as the ground truth. 

Simplifying reveals that: 

ğ‘™ğ‘’ğ‘¥(ğº1, ğº2) + ğ‘™ğ‘’ğ‘¥(ğº2, ğº1)
2

For 

this  experiment, 

in  order 

to  satisfy 

assumptions  (a-c),  define  ğ›½ğ‘ğ‘’ğ‘™ğ‘™ = 0.7  in  order  to 

penalize false positive predictions more and attempt 

to minimize the segmentation of invalid cells that 

=

1
2

=

1
2

=

=

1
2
1
2

(ğ›½ğ¹ğ‘ƒ(ğº1, ğº2) + (1 âˆ’ ğ›½)ğ¹ğ‘(ğº1, ğº2)

negate  these  assumptions.  Also  define  ğ›½ğ‘“ğ‘™ğ‘¢ğ‘œ =

+ ğ›½ğ¹ğ‘ƒ(ğº2, ğº1)

+ (1 âˆ’ ğ›½)ğ¹ğ‘(ğº2, ğº1)) 

(ğ›½|ğ¶[ğº1,ğº2] âˆ– ğ¶[ğº2,ğº1]|

0.15 which will encourage to choose a model that 

finds  as  many  fluorescence  clusters  as  possible 

while  missing  as  little  as  possible  ground  truth 

clusters.  Using  assumption  (d)  and  the  already 

+ (1 âˆ’ ğ›½)|ğ¶[ğº2,ğº1] âˆ– ğ¶[ğº1,ğº2]|

generated  cell  segmentation,  segmented  clusters 

+ ğ›½|ğ¶[ğº2,ğº1] âˆ– ğ¶[ğº1,ğº2]|

+ (1 âˆ’ ğ›½)|ğ¶[ğº1,ğº2] âˆ– ğ¶[ğº2,ğº1]|) 

that  are  not  within  a  cell  boundary  can  be  later 

removed.  That  is  why  in  the  fluorescence  cluster 

segmentation task the goal should be to find many 

(|ğ¶[ğº1,ğº2] âˆ– ğ¶[ğº2,ğº1]| + |ğ¶[ğº2,ğº1] âˆ– ğ¶[ğº1,ğº2]|) 

valid clusters, even if they are not within cells. 

|ğ¶[ğº1,ğº2]Î”ğ¶[ğº2,ğº1]|  

3.2. Analysis. 

The analysis phase calculates the measurements 

of  cells  and 

fluorescence 

intensity  using 

Basically, the experimental distance is proportional 

to  the  disagreed  objects  in  the  segmentations,  i.e. 

the number of objects that appear in exactly one of 

the  segmentations.  This  gives  a  representation  of 

9 

 
 
  
 
Colicoords.  This  includes  fields 5  C  â€“  J  in  the 

researcher   ğ‘— .  All  models  are  evaluated  on  the 

database.  Fields  K  â€“  T  were  calculated  in  the 

validation set. 

methods  defined  by  the  researchers  and  are 

described in appendix F (API). 

IV.  RESULTS 

Images were all taken with the same microscope. 

All cells that appear in the images are rod-shaped. 

The fluorescence channels contained several types 

MicroAnalyzer  expects  to  receive  images  of 

of proteins and RNA. 

cells  with  their  fluorescence  channels.  In  this 

4.1. Cell Segmentation. 

experiment, the data was received as â€˜.nd2â€™ files, a 

In this section, two main approaches were taken 

Nikon proprietary binary file type (handled using an 

in order to perform the segmentation. The first is a 

open-source solution nd2reader [30]). 

The  given  data 6  for  evaluation  was  45  cell 

images  and  31  fluorescence 

images  of  size 

1022 Ã— 1024 .  With  each  cell  or  fluorescence 

image  was  provided  a  ground  truth  binary  mask 

deterministic  approach  which  uses  thresholding-

based  techniques  to  find  the  ideal  algorithm  for 

valid  cell  segmentation.  The  second  is  the  use  of 

known segmentation and detection neural networks 

in order to attempt to mimic a researcher. 

from one of the experimentâ€™s researchers generated 

4.1.1. Thresholding-based algorithm. 

manually using NIS-Elements. According to these 

Thresholding  was  performed  using  several 

ground truth segmentations, the cell images have an 

existing methods, including minimum [31] and Yen 

average of 38.8 segmented cells per image and the 

[32]  thresholding.  Using  such  algorithms,  it  is 

fluorescence  images  have  an  average  of  57.5 

possible  to  find  cell-shaped  objects  in  the  input 

segmented  clusters  per  image.  The  dataset  of  cell 

images.  Some  outputs  do 

require 

further 

images was divided into a training set consisting of 

processing, but all methods basically find the same 

40 images and a test set consisting of 5 images. The 

set  of  cells  in  the  images.  Nonetheless,  these 

dataset  of  fluorescence  images  was  split  into  a 

methods with their default settings find all (or most) 

training set  consisting of  27 images  and a test set 

of the cell shapes in the image (see Figure 4) and 

consisting of 4 images. 

Additionally, two other images were segmented 

by  two  different  researchers  independently  from 

one  another.  These  two  images  will  be  called  the 

validation  set 7  and  denoted  {ğºğ‘–,ğ‘—}

 where 

(ğ‘–,ğ‘—)âˆˆ2Ã—2

ğºğ‘–,ğ‘— is the ground truth segmentation of image ğ‘– by 

do  not  take  into  account  any  of  the  experiment 

assumptions. This includes fuzzy, out of focus cells 

and extremely crowded cells. Correspondingly, the 

experimental ğ‘™ğ‘’ğ‘¥ -error  is  very  high.  Furthermore, 

inputting the segmentation (that contains all cells in 

the  image)  into  MicroAnalyzerâ€™s  CellAnalyzer 

5 See appendix F (API) 
6 Can be downloaded via the link in appendix A (Dataset) 

7 Validation images and the evaluation process can be found 
in appendix A under the link â€œValidationâ€ 

10 

 
 
module  does  not  filter  the  prediction  enough  to 

make it valid i.e. 

ğ‘™ğ‘’ğ‘¥(ğ‘ğ‘‘, ğºğ‘–,1) + ğ‘™ğ‘’ğ‘¥(ğ‘ğ‘‘, ğºğ‘–,2)
2

> ğ‘‘ğ‘’ğ‘¥(ğºğ‘–1, ğºğ‘–2) 

Figure  5  shows  the  resulting  cell  mask  of  using 

minimum  thresholding  and  CellAnalyzer  post-

processing. 

It  is  possible  to  achieve  better  results  using 

custom 

configurations,  but 

the 

acceptable 

configurations change from image to image. This is 

the opposite of automation, and thus this method is 

not used in MicroAnalyzer, which aims to request 

as little information as possible from the user. 

Note:  Maybe  correct  thresholding  configurations 

can  be  learned  using  known  machine  learning 

methods. 

4.1.2. Deep-learning algorithm 

Three known neural network models were tested 

for  this  task:  U-Net,  Mask  RCNN,  and  Cellpose. 

The first two were trained in a similar manner, and 

the  Cellpose  model  was  trained  in  the  restricted 

settings of its package. 

U-Net  [23]  is  a  convolutional  neural  network 

originally  created  for  biomedical  segmentation.  It 

outperformed 

its 

predecessors 

greatly 

by 

Figure 4: An example for minimum thresholding finding all cells 
in the image in its segmentation 
×© 

introducing a new approach different from the then 

Figure 5:  An example of minimum thresholding segmentation with 
CellAnalyzer post-processing.  

popular â€œsliding windowâ€ method. The idea behind 

this network is to convolve over and down-sample 

the  image  several  times,  using  more  filters  as  the 

image gets smaller, training it to slowly reduce the 

image  to  context  information.  This  information is 

then  up-sampled  and  combined  with  the  different 

image 

resolutions 

and 

then 

reduced  via 

convolutions  into  the  final  segmentation.  For  its 

purpose  and  ground-breaking  performance  of  its 

time, it is used as a baseline model. 

4.1.2.1 U-Net & Mask-RCNN 

Training for these models was conducted with 30 

epochs with an initial learning rate of 1 Ã— 10âˆ’3 for 

the  first  10  epochs,  1 Ã— 10âˆ’4  for  the  next  10 

epochs,  and 1 Ã— 10âˆ’5 for  the  last  10  epochs.  The 

experimental ğ‘™ğ‘’ğ‘¥-error  is  not  a  known  method  for 

11 

 
segmentation evaluation, and thus is not offered as 

an  option  for  training  models  in  major  deep-

learning frameworks.  In order to account for  this 

logically using tried and tested evaluation metrics, 

the  loss  minimized  for  U-Net  was  binary  cross-

entropy loss summed with Jaccard loss to maximize 

overlap.  These  losses  are  defined  as  follows:  let 

ğ‘ğ‘Ÿ âˆˆ ğ‘€ğ‘›Ã—ğ‘š([0,1]) be a prediction probability map 

for  an 

image  whose  ground 

truth 

is  ğº âˆˆ

ğ‘€ğ‘›Ã—ğ‘š({0,1}) .  Given  a  matrix  of  size  ğ´ âˆˆ
ğ‘€ğ‘›Ã—ğ‘š(ğ‘†) (where ğ‘† is some value set), let ğ´âˆ— be the 
flattening of matrix ğ´, i.e. ğ´âˆ— âˆˆ ğ‘†ğ‘›âˆ—ğ‘š and ğ´[ğ‘–, ğ‘—] =

ğ´âˆ—[ğ‘– âˆ— ğ‘› + ğ‘—]. Then: 

Binary Cross-entropy Loss: 

ğ»(ğ‘ğ‘Ÿ, ğº) = âˆ’

1
ğ‘› âˆ— ğ‘š

ğ‘›âˆ—ğ‘š
âˆ‘ ğºâˆ—[ğ‘–] â‹… log(ğ‘ğ‘Ÿâˆ—[ğ‘–])
ğ‘–=1

+ (1 âˆ’ ğºâˆ—[ğ‘–]) â‹… log(1 âˆ’ ğ‘ğ‘Ÿâˆ—[ğ‘–]) 

Jaccard Loss: 

ğ‘‘ğ½(ğ‘ğ‘Ÿ, ğº)

Figure 6: An example of a U-Net segmentation with incomplete 
cell masks. 

A  quick  sanity  check  reveals  that  the  U-Net 

model  outputs  a  large  number  of  incomplete 

segmentations (see Figure 6), while Mask-RCNN 

provides a clean output. This is justified by the fact 

that  Mask-RCNN  minimizes  several  losses  aside 

from 

the  mask,  helping 

it  concentrate  on 

= 1 âˆ’

âˆ‘ ğ‘ğ‘Ÿ[ğ‘–, ğ‘—] â‹… ğº[ğ‘–, ğ‘—]

ğ‘–ğ‘—

segmenting  areas  where cells  have  been  detected. 

âˆ‘ ğ‘ğ‘Ÿ[ğ‘–, ğ‘—] + ğº[ğ‘–, ğ‘—]

ğ‘–ğ‘—

âˆ’ âˆ‘ ğ‘ğ‘Ÿ[ğ‘–, ğ‘—] â‹… ğº[ğ‘–, ğ‘—]

ğ‘–ğ‘—

However,  this  U-Net  issue  is  easily  defeated  by 

Mask-RCNN  is  a  multi-task  network  which 

removing  objects  of  a  certain  size  from  the 

minimizes a specific set of losses, including loss for 

segmentation, as done in CellAnalyzer. 

detection  boxes  which  this  experiment  does  not 

Finally, looking at the experimental ğ‘™ğ‘’ğ‘¥-errors of 

require. All networks losses were optimized using 

these  models,  it  is  clear  that  only  Mask-RCNN 

an  Adam  optimizer  [33].  Images  were  fed  to  the 

meets the evaluation criteria for the purposes of this 

models with a batch size of 1 (a single image every 

experiment (see Table 1). 

time)  and  every  time  an  image  is  loaded  it  is 

4.1.2.2. Cellpose. 

randomly transformed using rigid transformations, 

Cellpose  attempts 

to  generalize 

the  cell 

e.g. flip (horizontal/vertical), rotate, transpose, etc. 

segmentation task to many different kinds of cells 

and brightness and gamma transformations. 

and image formats. It can be seen clearly that even 

Note:  The  exact  model  architectures  used  can  be 

without  extra  training,  the  pretrained  weights 

seen in appendix A.2. 

provided by the tool offer a visually seeming high-

quality segmentation for this experimentâ€™s images. 

12 

 
 
Table 1: Cell detection validation results performed on validation images segmented by two different researchers. Only Mask-RCNN 
model upholds the experiment criterion giving valid predictions for all validation data. Note that the â€œblank maskâ€ model should receive a 
better precision, recall and F2-Score than â€œthresholding (no CellAnalyzer)â€ model. The experiment assumptions and the researcher 
feedback say this is not the case. Only ğ‘™ğ‘’ğ‘¥-error reflects this correctly. 

However,  this  does  not  match  the  experiment 

training, meaning that recording other metrics is not 

assumptions  since  many  cells  that  were  not 

possible as of the writing of this paper. 

segmented  by  the  researchers  are  segmented  by 

Training with and without the provided weights 

Cellpose, including crowded cells and blurry ones. 

seem to generalize well during training. However, 

Furthermore,  a  â€œclumpâ€  of  crowded  cells  that  is 

without  using  the  pretrained  weights  the  model 

useless for this experiment may be segmented as a 

overfits  on  the  last  100  epochs  when  the  weight 

single  cell,  which  is  the  worst  possible  outcome 

decay kicks in, and with the pretrained weights the 

under the assumptions, since it seems like one false 

model  still  segments  â€œclumpsâ€  of  cells.  This  is 

positive detection, when actually it is several. 

similar  to  the  issues  observed  in  using  the 

Training the model is not possible in the manner 

Thresholding model. 

described  for  the  previous  models.  The  library 

4.2 Fluorescence cluster segmentation. 

containing  this  model  was  released  with  full 

Delving into the fluorescence images data, one 

documentation  two  months  before  the  writing  of 

can  see  a  recurring  shape  of  a  three-dimensional 

this article. The user is given a choice of an initial 

Gaussian  distribution  at  its  location,  i.e.  the 

learning rate and a number of epochs to run. For the 

algorithms  should  search  for  a  three-dimensional 

rest of the training, the learning rate stays the same 

Gaussian shape in the image (see Figure 7). 

until the last 100 epochs where it is halved and is 

For this task, the evaluated models were U-Net 

slowly  deteriorated  by  a  linear  weight  decay  of 

and  FPN.  The  choice  to  use  only  segmentation 

1 Ã— 10âˆ’5. The  minimized  loss  is a  sum of  binary 

models  instead  of  mask-RCNN  detection  model 

cross-entropy and L2 loss optimized by a standard 

arises  from  mask-RCNNâ€™s  poor  performance  on 

SGD optimizer with momentum. Furthermore, the 

small objects. 

API  does  not  allow  access  to  the  model  during 

Once  again,  the  desired  network  is  one  that  is 

produces  valid  segmentations  relative 

to 

the 

13 

 
Table 2: Fluorescence cluster segmentation validation results. performed on validation images segmented by two different researchers. Both 
tested models are capable of giving valid predictions for this experiment and can both be considered new researchers. 

Figure 7: An example of a single cluster image and 3D bar plot 
with matching axes (top left corners are (0,0)). Notice the general 
shape of a 3D Gaussian distribution plot. 

Figure 8: A sample fluorescence cluster segmentation neural 
network input RGB image. The R channel contains the cells 
grayscale image and the G and B channels contain the 
fluorescence channel. 

validation  set,  but  this  time  the  experimental ğ‘™ğ‘’ğ‘¥-

4.3.1 Neural Network Training 

error  is  defined  mostly  by  the  number  of  missing 

Model training was performed on a CUDA GPU. 

clusters. 

As mentioned earlier, all models were trained over 

U-Net and FPN were trained in the exact same 

30  epochs.  Mask-RCNN  took  approximately  2 

way as the U-Net model for the cell segmentation 

minutes per epoch, and had a total runtime of 59.3 

task. In order to give the models a sense of the cell 

minutes. FPN took approximately 0.9 minutes per 

positions  and  to  encourage  finding  clusters  at  the 

epoch, and had a total runtime of 27.8 minutes. 

location  of  cells,  the  input  for  these  models  is  an 

4.3.2 Analysis pipeline 

RGB image where the R channel is the cells image 

This  flow  was  tested  using  two  different 

and  the  G  and  B  channels  are  the  fluorescence 

hardware setups. The results can be seen in Table 3. 

channel being segmented (see Figure 8). 

U-Net and FPN  both give  valid  predictions  for 

V.  DISCUSSION 

both  validation  images  (see  Table  2).  This  means 

5.1. Conclusion 

that  these  models  are  interchangeable  for  this 

The objective of this study was to automate the 

experiment. 

4.3 Runtime 

process  of  cell  and  fluorescence  channel  analysis 

starting  from 

the  raw 

image  output  of 

the 

14 

 
Table 3: Runtime results table for two different hardware specifications and operating systems. 

microscope  and  to  bypass  the  cell  segmentation 

images.  Without  fluorescence  data,  the  output 

bottleneck of todayâ€™s tools. The tool was evaluated 

database  and  visualizations  still  contain  cell 

using  the  defined  experimental  ğ‘™ğ‘’ğ‘¥ -error  and 

segmentation  and  analysis  data,  which  may  be 

distance (ğ‘‘ğ‘’ğ‘¥) in section II. As seen in the Results 

useful for experiments that do not rely on additional 

section  (IV),  MicroAnalyzer  truly  does  provide  a 

channels.  Something  similar  could  be  achieved 

good  analysis  for  a  valid  number  of  cells  and 

with three-dimensional images as well. 

fluorescence  clusters  in  each  image,  according  to 

the  defined  evaluation  method.  This  was  done  by 

using known object segmentation neural networks 

to find cells and fluorescence clusters in the image 

and  Colicoords  as  an  open-source  alternative  for 

analysis.  This  is  a  testament  to  the  incredible 

flexibility  and  reliability  of  modern  computer 

vision  techniques,  and  how  there  might  just  be  a 

model out there that can fit any experimentâ€™s data. 

Time-lapse  image  analysis  is  another  form  of 

microscopic  output  used 

for 

studying 

the 

organismsâ€™  behavior  and  subcellular  organization 

over  time.  Specifically,  for  this  experiment,  the 

time-lapse imagesâ€™ time data exists within the ND2 

files.  Object  tracking  networks  are  available  in 

open-source  repositories  and  are  proving  reliable, 

making 

tasks  such  as 

tracking  cell  mitosis 

frequency  seem  undaunting  as  a  logical  next  step 

Nevertheless, 

the 

segmentation  evaluation 

for the development of MicroAnalyzer. 

method  used  is  very  specific  to  the  experiment 

discussed in this paper. The models used here may 

not be as efficient for other experiment assumptions 

and images. Cellpose and others like it may be key 

in any generic version of MicroAnalyzer. 

5.2. Future Work 

One  option  for the  expansion  of  this project  is 

analyzing three-dimensional cell images. Cellpose 

offers a feature that finds cells in 3D microscopic 

images,  and  thus  may  be  a  viable  option  for  this 

task.  The  experiment  has  a  certain  emphasis  on 

Another  possible  direction  is  the  analysis  of 

different  localization  patterns  other  than  polar. 

Patterns like the helix can be far more difficult to 

detect  as they do not  have the signature  Gaussian 

shape and should not be properly segmented using 

MicroAnalyzerâ€™s models. Perhaps the correct way 

to do this is to look at each cell individually and to 

classify  the  localization  of  the  protein  in  the  cell. 

The experiment in this paper concentrates on polar 

localization, but there very may well be a demand 

for other localizations in the future. 

polar  localization,  but  MicroAnalyzer  doesnâ€™t 

5.3. Further Discussion 

actually  require  any  other  channels  but  the  cell 

15 

 
The  ability  to  perform  cell  and  fluorescence 

VI.  REFERENCES 

cluster  analysis  quickly  and  with  minimal  human 

[1] 

O. Amster-Choder, â€œThe compartmentalized 

interaction  will  allow  labs  to  produce  enormous 

vessel,â€ Cell. Logist., vol. 1, no. 2, pp. 77â€“81, Mar. 

amounts  of  analysis  data  in  a  much  shorter  time, 

2011, doi: 10.4161/cl.1.2.16152. 

and even shorter as the lab upgrades their hardware. 

[2] 

S. Kannaiah, J. Livny, and O. Amster-Choder, 

Statistical  questions  that  could  not  be  answered 

â€œSpatiotemporal Organization of the E. coli 

previously due to lack of data can now be studied 

more deeply, e.g. perhaps a certain set of properties 

of  a  cell  and  its  fluorescence  data  point  to  some 

phenomenon  with  a  high  probability.  Neural 

networks for regression and classification are often 

Transcriptome: Translation Independence and 

Engagement in Regulation,â€ Mol. Cell, vol. 76, no. 

4, Nov. 2019, doi: 10.1016/j.molcel.2019.08.013. 

[3] 

K. Peters et al., â€œStreptococcus pneumoniae PBP2x 

mid-cell localization requires the C-terminal 

PASTA domains and is essential for cell shape 

used for these tasks, and in this case create a chain 

maintenance,â€ Mol. Microbiol., vol. 92, no. 4, 2014, 

of neural networks working together to achieve one 

doi: 10.1111/mmi.12588. 

larger  goal.  Now  imagine  automating  the  entire 

[4] 

M. Badieirostami, M. D. Lew, M. A. Thompson, 

pipeline: the microscope takes thousands of images 

overnight  which  are  input  into  MicroAnalyzer  to 

generate data for hundreds of thousands of cells and 

the studied material, and run the statistical analysis 

algorithm on this giant database. Even if this takes 

a month to run, it is much faster than performing a 

and W. E. Moerner, â€œThree-dimensional localization 

precision of the double-helix point spread function 

versus astigmatism and biplane,â€ Appl. Phys. Lett., 

vol. 97, no. 16, Oct. 2010, doi: 10.1063/1.3499652. 

[5] 

P. M. Slovak, G. H. Wadhams, and J. P. Armitage, 

â€œLocalization of MreB in Rhodobacter sphaeroides 

under conditions causing changes in cell shape and 

manual experiment filled with pesky, unpredictable 

membrane structure,â€ in Journal of Bacteriology, 

human  errors.  It  also  frees  the  researchers  to 

perform other tasks that cannot (yet) be performed 

reliably by a machine. Herein lies the true power of 

the  dynamic  duo  that  is  machine  learning  and 

automation. 

Acknowledgements 

Jan. 2005, vol. 187, no. 1, doi: 10.1128/JB.187.1.54-

64.2005. 

[6] 

S. Govindarajan, N. Albocher, T. Szoke, A. 

Nussbaum-Shochat, and O. Amster-Choder, 

â€œPhenotypic Heterogeneity in Sugar Utilization by 

E. coli Is Generated by Stochastic Dispersal of the 

General PTS Protein EI from Polar Clusters,â€ Front. 

Microbiol., vol. 8, no. JAN, Jan. 2018, doi: 

The idea for this project came from members of 

10.3389/fmicb.2017.02695. 

Orna  Amster-Choder  lab,  Tamar  Szoke,  Nitsan 

[7] 

G. Laloux and C. Jacobs-Wagner, â€œHow do bacteria 

Albocher  and  Omer  Goldberger,  who  raised  the 

localize proteins to the cell pole?,â€ Journal of Cell 

need  for  a  tool  to  analyze  their  fluorescence 

Science, vol. 127, no. 1. Company of Biologists, 

microscopy  data.  We  thank  them  for  putting  the 

Jan. 01, 2014, doi: 10.1242/jcs.138628. 

time  to  define  their  needs,  provide  fluorescence 

[8] 

M. R. K. Alley, J. R. Maddock, and L. Shapiro, 

images and analyze them manually. 

â€œPolar localization of a bacterial chemoreceptor,â€ 

16 

 
 
 
 
Genes Dev., vol. 6, no. 5, 1992, doi: 

[17]  C. Stringer, T. Wang, M. Michaelos, and M. 

10.1101/gad.6.5.825. 

[9] 

C. Conrad and D. W. Gerlich, â€œAutomated 

microscopy for high-content RNAi screening,â€ 

Pachitariu, â€œCellpose: a generalist algorithm for 

cellular segmentation,â€ bioRxiv, Feb. 2020, doi: 

10.1101/2020.02.02.931238. 

Journal of Cell Biology, vol. 188, no. 4. The 

[18]  G. Bokota et al., â€œPartSeg, a Tool for Quantitative 

Rockefeller University Press, Feb. 22, 2010, doi: 

Feature Extraction From 3D Microscopy Images for 

10.1083/jcb.200910105. 

[10]  R. Pepperkok and J. Ellenberg, â€œHigh-throughput 

Dummies,â€ bioRxiv, Jul. 2020, doi: 

10.1101/2020.07.16.206789. 

fluorescence microscopy for systems biology,â€ 

[19] 

J. H. Smit, Y. Li, E. M. Warszawik, A. Herrmann, 

Nature Reviews Molecular Cell Biology, vol. 7, no. 

and T. Cordes, â€œColiCoords: A Python package for 

9. Nat Rev Mol Cell Biol, Sep. 19, 2006, doi: 

the analysis of bacterial fluorescence microscopy 

10.1038/nrm1979. 

data,â€ PLoS One, vol. 14, no. 6, Jun. 2019, doi: 

[11]  M. Zeder, E. Kohler, and J. Pernthaler, â€œAutomated 

10.1371/journal.pone.0217524. 

quality assessment of autonomously acquired 

[20] 

L. C. Chen, G. Papandreou, I. Kokkinos, K. 

microscopic images of fluorescently stained 

Murphy, and A. L. Yuille, â€œDeepLab: Semantic 

bacteria,â€ Cytom. Part A, vol. 77, no. 1, Jan. 2010, 

Image Segmentation with Deep Convolutional Nets, 

doi: 10.1002/cyto.a.20810. 

Atrous Convolution, and Fully Connected CRFs,â€ 

[12]  C. A. Schneider, W. S. Rasband, and K. W. Eliceiri, 

â€œNIH Image to ImageJ: 25 years of image analysis,â€ 

IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 

4, Apr. 2018, doi: 10.1109/TPAMI.2017.2699184. 

Nature Methods, vol. 9, no. 7. NIH Public Access, 

[21]  Y. Al-Kofahi, A. Zaltsman, R. Graves, W. Marshall, 

Jul. 2012, doi: 10.1038/nmeth.2089. 

and M. Rusu, â€œA deep learning-based algorithm for 

[13] 

â€œNIS-Elements | Software | Products | Nikon 

Instruments Inc.â€ 

https://www.microscope.healthcare.nikon.com/prod

2-D cell segmentation in microscopy images,â€ BMC 

Bioinformatics, vol. 19, no. 1, Oct. 2018, doi: 

10.1186/s12859-018-2375-z. 

ucts/software/nis-elements (accessed Sep. 06, 2020). 

[22]  A. Chaurasia and E. Culurciello, â€œLinkNet: 

[14] 

L. Schermelleh et al., â€œSuper-resolution microscopy 

demystified,â€ Nature Cell Biology, vol. 21, no. 1. 

Nature Publishing Group, Jan. 01, 2019, doi: 

10.1038/s41556-018-0251-8. 

Exploiting Encoder Representations for Efficient 

Semantic Segmentation,â€ 2017 IEEE Vis. Commun. 

Image Process. VCIP 2017, vol. 2018-Janua, Jun. 

2017, doi: 10.1109/VCIP.2017.8305148. 

[15] 

S. T. Hess, T. P. K. Girirajan, and M. D. Mason, 

[23]  O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: 

â€œUltra-high resolution imaging by fluorescence 

photoactivation localization microscopy,â€ Biophys. 

J., vol. 91, no. 11, pp. 4258â€“4272, Dec. 2006, doi: 

10.1529/biophysj.106.091116. 

[16] 

J. Xu, H. Ma, and Y. Liu, â€œStochastic optical 

reconstruction microscopy (STORM),â€ Curr. 

Protoc. Cytom., vol. 2017, Jul. 2017, doi: 

10.1002/cpcy.23. 

Convolutional networks for biomedical image 

segmentation,â€ in Lecture Notes in Computer 

Science (including subseries Lecture Notes in 

Artificial Intelligence and Lecture Notes in 

Bioinformatics), May 2015, vol. 9351, doi: 

10.1007/978-3-319-24574-4_28. 

[24] 

Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. 

Liang, â€œUNet++: A Nested U-Net Architecture for 

Medical Image Segmentation,â€ Jul. 2018, Accessed: 

17 

 
Aug. 26, 2020. [Online]. Available: 

â€œRich feature hierarchies for accurate object 

http://arxiv.org/abs/1807.10165. 

detection and semantic segmentation,â€ in 

[25] 

S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster R-

CNN: Towards Real-Time Object Detection with 

Region Proposal Networks,â€ IEEE Trans. Pattern 

Anal. Mach. Intell., vol. 39, no. 6, Jun. 2017, doi: 

Proceedings of the IEEE Computer Society 

Conference on Computer Vision and Pattern 

Recognition, Sep. 2014, pp. 580â€“587, doi: 

10.1109/CVPR.2014.81. 

10.1109/TPAMI.2016.2577031. 

[30]  R. Verweij, â€œrbnvrw/nd2reader.â€ Jun. 2020, 

[26]  H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, 

â€œPSPNet,â€ Proc. - 30th IEEE Conf. Comput. Vis. 

Accessed: Aug. 22, 2020. [Online]. Available: 

https://github.com/rbnvrw/nd2reader. 

Pattern Recognition, CVPR 2017, vol. 2017-Janua, 

[31] 

J. Kittler and J. Illingworth, â€œMinimum error 

Dec. 2017, doi: 10.1109/CVPR.2017.660. 

thresholding,â€ Pattern Recognit., vol. 19, no. 1, Jan. 

[27] 

T.-Y. Lin, P. DollÃ¡r, R. Girshick, K. He, B. 

1986, doi: 10.1016/0031-3203(86)90030-0. 

Hariharan, and S. Belongie, â€œFeature Pyramid 

[32] 

J. C. Yen, F. J. Chang, and S. Chang, â€œA New 

Networks for Object Detection,â€ Dec. 2016, 

Criterion for Automatic Multilevel Thresholding,â€ 

Accessed: Aug. 26, 2020. [Online]. Available: 

IEEE Trans. Image Process., vol. 4, no. 3, 1995, 

http://arxiv.org/abs/1612.03144. 

doi: 10.1109/83.366472. 

[28]  K. He, G. Gkioxari, P. DollÃ¡r, and R. Girshick, 

[33]  D. P. Kingma and J. L. Ba, â€œAdam: A method for 

â€œMask R-CNN,â€ IEEE Trans. Pattern Anal. Mach. 

stochastic optimization,â€ Dec. 2015, Accessed: Aug. 

Intell., vol. 42, no. 2, Feb. 2020, doi: 

27, 2020. [Online]. Available: 

10.1109/TPAMI.2018.2844175. 

https://arxiv.org/abs/1412.6980v9. 

[29]  R. Girshick, J. Donahue, T. Darrell, and J. Malik, 

18 

 
 
  
 
 
APPENDICES 

Appendix A: Relevant Links 

1.  MicroAnalyzer Repository 

- 

https://github.com/JG-codies/MicroAnalyzer 

2.  Neural Network Model Architectures 

- 

https://github.com/JG-
codies/MicroAnalyzer/blob/master/Notebooks/m
odel_summaries.ipynb 

3.  Dataset 

- 

https://drive.google.com/drive/folders/1byIX3Dt
aSTsBLF8a91012ljtrNkqVGa8?usp=sharing 

4.  Neural Network Models Training8 
-  Cells â€“ https://github.com/JG-

codies/MicroAnalyzer/blob/master/Notebooks/ce
ll_training.ipynb 

-  Clusters â€“ https://github.com/JG-

codies/MicroAnalyzer/blob/master/Notebooks/fl
uo_training.ipynb 

5.  Validation 

- 

- 

Image 1 â€“ https://github.com/JG-
codies/MicroAnalyzer/blob/master/Notebooks/V
alidation%20Image%201.ipynb 
Image 2 â€“ https://github.com/JG-
codies/MicroAnalyzer/blob/master/Notebooks/V
alidation%20Image%202.ipynb 

6.  Usage Demo 

- 

https://github.com/JG-
codies/MicroAnalyzer/blob/master/Notebooks/us
age_demo.ipynb 

7.  Sample DB 

-  https://github.com/JG-

codies/MicroAnalyzer/blob/master/samp
le_output/validation_images/1/database.c
sv 

Figure 9: Cells detected by Maks-RCNN that appeared in only 
one of the two ground truth segmentations due to disagreement 
on size, spacing, and focus. 

considered as a â€œworseâ€ false positive? Could it 

possibly be a true positive that a third researcher 

might have segmented? 

The  reason  such  a  prediction  should  not  be 

additionally  penalized  for  this  is  because  it  is 

already  penalized  twice  for  each  image  in  the 

evaluation  criterion  (once  for  each  researcher 

segmentation), and more importantly it may be a 

valid  cell  that  both  researchers  missed  due  to 

human  faults.  Once  the  rule  of  multiple  ground 

truths has been accepted, the experiment relies on 

human  competence  which  can  never  be 

Appendix B: â€œSoftâ€ Segmentation Evaluation 

guaranteed,  and  thus  all  result  outcomes  are 

Remember 

that  since 

the  experiment 

is 

evaluated 

using  multiple 

ground 

truth 

segmentations, 

the  definition 

the  evaluation 

method in section III considers a valid prediction 

as  if  it  were  a  segmentation  from  another 

researcher. However, what if there are extra cells 

that appear in the prediction that do not appear in 

either  ground  truth  segmentation?  Should  it  be 

probabilistic  and  not  deterministic,  forcing  this 

last evaluation to be performed manually by the 

researchers. This helps give a general idea of how 

well  the  model  fits  to  the  desires  of  the 

researchers. 

In 

the  manual  check  of 

the 

predictions  of  the  Mask-RCNN  model  on  the 

validation  set,  the  researchers  debated  amongst 

themselves about the validity of the â€œrogueâ€ cell 

detections  (as  seen  in  Figure  9),  showing  that 

8 Performed in â€œGoogle Colabâ€ with GPU. Training 
details are described int the article (Results section). 

19 

 
 
 
 
 
keeping those cells for analysis may or may not 

1 

be  valid.  This  phenomenon  may  hint  as  to  why 

Length 

radius 

{ 

deep-learning  models  greatly  outperformed  the 

deterministic image processing approach. 

center 

W
i
d
t
h

0 
Figure 10: A partial database columns description diagram, 

1 

It is irrelevant to talk about cells that appear in 

Appendix F: API 

both  ground  truths  but  are  missing  in  the 

prediction  since  the  already  defined  evaluation 

guarantees that not â€œtoo manyâ€ cells are missing 

(depending on the chosen ğ›½). 

Appendix C: Preprocessing 

The input file is an ND2 file which is a Nikon 

proprietary  file  type  containing  a  set  of  16bit 

microscope 

images  with  all 

the  cameraâ€™s 

channels, including the grayscale bacteria image 

and the fluorescence intensity channels. 

Given  an  ND2  file,  initially  separate  the 

bacteria  image  channel  from  the  fluorescence 

channels. The bacteria image pre-processing is as 

follows: 

1.  Convert the image to 8bit RGB image (all 

channels are the same). 

2.  Pad 

the 

images  such 

that 

theyâ€™re 

dimensions  are  divisible  by  25 .  This 

allows  us  to  down-sample  the  image 

evenly at least five times (required by the 

neural networks). 

Each fluorescence image is pre-processed in the 

same way as the bacteria image, except that the R 

channel of the image is substituted with the 8bit 

version of the bacteria image. 

MicroAnalyzer was built to support ND2 files 

and the name  of the  channel  containing the  cell 

images as inputs and outputs a database with the 

results  of the  analysis  and several  visualizations 

of the segmentations. 

A.  Id â€“ the identifier in the mask of the ROI 

presented in the row. 

B.  frame id â€“ the index of the image in the 
nd2 file that the ROI was found in. 
C.  length â€“ the ROI length in ğœ‡ğ‘š (see 

Figure 10). 

D.  width â€“ the ROI width in ğœ‡ğ‘š (see 

Figure 10). 

E.  area â€“ the 2D area of the ROI according 

to the image in ğœ‡ğ‘š2. 

F.  radius â€“ the distance between the edge 
of the ROI and its mid-line in ğœ‡ğ‘š (see 
Figure 10). 

G.  circumference â€“ the length of the 
perimeter of the ROI in ğœ‡ğ‘š. 

H.  surface area â€“ an estimation of the 

surface area of the ROI modeled as a 3D 
object. 

I.  Volume â€“ an estimation of the volume of 

the ROI modeled as a 3D object. 

J.  <Fluorescence-name> cell mean/std 

intensity â€“ the mean/std pixel intensity 
of the fluorescence in the entire 
boundary of the cell. 

K.  <Fluorescence-name> cell intensity 

CVI â€“ â€œ<Fluorescence-name> cell mean 
intensityâ€ divided by <Fluorescence-
name> cell std intensity (both calculated 
by Colicoords). 

L.  <Fluorescence-name> 

vertical/horizontal mean/max/sum 
intensity profile â€“ sample 20 points 
evenly along the vertical/horizontal axis 
and aggregate the fluorescence intensity 

20 

 
 
 
 
 
on the perpendicular axis according to 
the function mean/max/sum. 

M. <Fluorescence-name> number of 

clusters â€“ the number of clusters that 
intersect the cell boundary (pixel-wise). 
N.  <Fluorescence-name> has clusters - a 
Boolean that is true if and only if the 
matching â€œnumber of clustersâ€ field is 
not 0.  

O.  <Fluorescence-name> cluster <index> 
id - the identifier in the mask of cluster 
<index> presented in the row. 

P.  <Fluorescence-name> cluster <index> 
size â€“ the size of the cluster in ğœ‡ğ‘š2 
according to cluster mask in image. 
Q.  <Fluorescence-name> cluster <index> 

center â€“ a tuple (x, y) of numbers 
between 0 and 1 representing the 
position of the cluster (the point of max 
intensity) in proportion to the boundaries 
of the cell (see Figure 10). 

R.  <Fluorescence-name> cluster <index> 
is polar â€“ A boolean that is true if and 
only if the cluster center x coordinate is 
less than 0.25 or greater than 0.75. 
S.  <Fluorescence-name> cluster <index> 
mean/std/max/sum intensity â€“ the 
mean/std/max/sum of the fluorescence 
image pixel intensity within the 
boundaries of the cluster. 

T.  <Fluorescence-name> leading cluster 
index â€“ the index of the cluster (that 
appears in the column headers) with the 
highest â€œmax intensityâ€ field. 

21 

 
 
 
