7
1
0
2

v
o
N
6

]

R
C
.
s
c
[

1
v
8
8
0
2
0
.
1
1
7
1
:
v
i
X
r
a

Computer activity learning from
system call time series

Curt Hastings and Ronnie Mainieri

{cah,rxm} at permissionbit.com

PermissionBit, 1750 Tysons Blvd. Ste. 1500, McLean VA 22102

26 October 2017

Abstract

Using a previously introduced similarity function for the stream of system
calls generated by a computer, we engineer a program-in-execution classiﬁer
using deep learning methods. Tested on malware classiﬁcation, it signiﬁcantly
outperforms current state of the art. We provide a series of performance mea-
sures and tests to demonstrate the capabilities, including measurements from
production use. We show how the system scales linearly with the number of
endpoints. With the system we estimate the total number of malware families
created over the last 10 years as 3450, in line with reasonable economic con-
straints. The more limited rate for new malware families than previously ac-
knowledged implies that machine learning malware classiﬁers risk being tested
on their training set; we achieve F1 = 0.995 in a test carefully designed to miti-
gate this risk.

1 Introduction

Current practices in cybersecurity do not scale, reﬂecting an asymmetry in the ability
to automate that favors the attacker. There is an emerging consensus in the industry
that artiﬁcial intelligence (AI) could recover the advantage, but even with advances in
other domains performant AI systems for cybersecurity remain elusive despite con-
certed efforts to develop them.

We previously described a method for embedding system call traces in a metric
space using techniques that respect constraints to deployment [1]. Since Forrest’s
1994 report [2] there has been a consensus among academics that sequences of sys-
tem calls are the preferred data source for characterizing the behavior of a computer

 
 
 
 
 
 
[3, 4, 5, 6, 7, 8], yet we are not aware of any viable systems that are based on this tech-
nique. We believe this is because until recently the data were difﬁcult to obtain and
the necessary statistical techniques did not exist. Here we engineer an in-execution
malware detection system that signiﬁcantly outperforms current state of the art, and
we demonstrate that it will scale linearly to networks with a few million endpoints.

We ﬁrst demonstrate the properties of our similarity function and show that we
can identify relationships between versions of common programs that differ by sev-
eral years of development. The relationship between resource expenditures and se-
curity was explored in Van Dijk et al. [9]; an in-execution system for detecting pro-
gram similarity would eliminate much current reverse engineering cost (and would
be competitive with existing systems). It would also force attackers to write their
tools anew for each campaign. The observed reliance of attackers on a small number
of malware code bases suggests that current tools used by defenders are unable to
impose this cost.

We next characterize the distribution of expected activities that will be observed
using our system under real usage conditions. We show that no more than a few
thousand non-malicious activities are expected over long time periods (or across
wide networks). The rate at which new malware programs appear is also much lower
than generally asserted. Using the system, we conducted over 2.6 million experi-
ments with a well-sampled corpus of live malware. We estimate the total number of
malware families created for the Windows platform over the last 10 years to be 3450,
in line with reasonable economic constraints to software development; of these we
have identiﬁed 1111.

The broad coverage of the set of both malicious and non-malicious programs
permits us to use features derived from our embedding as input to bootstrap a sim-
ple deep belief network that classiﬁes programs that are dissimilar to any in our mal-
ware and non-malware corpora. The small number of malicious program families
implies that care must be taken when testing the performance of AI subsystems; with
millions of samples even rare programs have with high probability many homologs
in the training set. We carefully test our system for its ability to generalize by using
malware sampled from the wild several months after we generated the data that was
used to train the machine learning subsystems and by isolating the performance of
the deep belief net by considering only the performance on samples dissimilar to any
in our list of known malware.

Finally, we extrapolate the performance of our system to very large networks. We
show that the similarity function and deep belief network together create an AI sys-
tem that can analyze terabytes of system call sequence data per week. As we are inter-

2

ested in creating a system for live indentiﬁcation of malicious activity on endpoints
(often described as threat hunting in the trade), our work differs from others that
boost performance by relying on data only available in an instrumented laboratory
environment or that cannot be collected when the majority of devices are mobile.
While these assumptions simplify development, the importance of endpoint con-
text in determining the behavior of a program implies that designing for the more
stringent requirements associated with in-execution monitoring is worthwhile. Our
results demonstrate that a performant and scalable AI system can be constructed
without emulation, hooking, VM introspection, or sandboxing at the network edge.

2 Detection system

In our system, each endpoint runs a sensor, a data collection program, that transmits
data to the compute server, an analysis daemon that identiﬁes traces of any executing
malware. The compute server has access to several databases and machine learning
subsystems that it uses to organize the data it receives into detections. The detec-
tions are shipped to a security information and event management (SIEM) system
for visualization and decision-making. Each detection is assigned a probability that
it corresponds to malicious execution on the originating endpoint.

System calls are collected in batches by a sensor on the endpoint using facili-
ties provided by the operating system (the ETW framework in the case of Windows).
The sensor collects all calls issued in a 5 second interval, corresponding to about
400 000 calls. Within a batch, the streams of calls from each thread are grouped into
sequences of 5-grams (referred to as words). The sensor then waits for a short period
of random duration (which could be zero) before collecting the next batch of system
calls.

Observing a set of computers under use for many hours under diverse usage con-
ditions shows that the distribution of words follows an apparently long-tailed distri-
bution. The observation frequency of a word is proportional to a function h that goes
to zero with ever larger rank r but at a rate slower than the tail of a normal distribu-
−α
tion. The expectation arising from the behavior of complex systems is that h(r ) ∼ r
for some α ≥ 1, but we have instead observed that in professionally maintained net-
−αr for α positive (although the sum of two ex-
works the rank distribution h(r ) ∼ e
ponentials provides a better ﬁt to the more frequent words). It is this fast decay that
makes it possible to use a small collection of words as the basis in which to collect
information from a computer. This was an unexpected result.

The words in each batch are processed on the endpoint into more information-

3

rich and compact structures that are better suited for additional processing by ma-
chine learning algorithms. All words belonging to the same process in a batch are
summarized into a pair of vectors: one, labeled zeta, is a 141-dimensional vector of
reals ζ that reﬂects the information from the most frequent words; the other, labeled
mu, is a bag of word-counts µ that reﬂects the information from the less frequent
words. Together the two form the feature vector (cid:161)ζ, µ(cid:162) that is used in further process-
ing. The feature vector is typically stored with the timestamp and some metadata
(process names and their numerical identiﬁers, for example) that may be useful in
forensic work and in a SIEM user interface. The construction of the feature vectors
has been described in detail by Hastings and Mainieri [1], where they are called pro-
cess dots.

2.1 Similarities

The similarity sim(· , ·) between two feature vectors pa = (ζa, µa) and pb = (ζb, µb)
can be deﬁned as a linear combination of the similarities between the zeta and the
mu parts. Because of the probabilistic construction of both zeta and mu, small cor-
rections are made for edge cases of extreme values as previously described [1], but
essentially each similarity is cosine-like, computed from the scalar product of the
components suitably normalized:

sim(pa, pb) = θζ〈ζa, ζb〉 + θµ〈µa, µb〉 .

(1)

The real numbers θζ and θµ are chosen so that each dot product contributes approx-
imately equal parts to the total similarity. As zeta and mu belong to different vector
spaces, their scalar products are different operations.

The compute server uses the similarity function sim(·, ·) to aggregate some of the
feature vectors from a few batches into tight clusters. These are clusters in which
most of the pairs of vectors have similarity above a threshold θS (and a few addi-
tional graph-theoretical properties are present [1]). Tight clusters can be compared
by comparing the feature vectors within them. Given two clusters ca and cb, their
similarity simT C can be deﬁned as the average similarity among their constituent
feature vectors, with pi ∈ ca and pk ∈ cb:

simT C (ca, cb) =

1
|ca||cb|

(cid:88)

pi ,p j

sim(pi , p j )

(2)

where the sum runs over all possible pairs of feature vectors in the tight clusters and
|cα| is the number of feature vectors in cluster α. The actual implementation of the

4

sum needs to correct for extreme values and can be constructed as an estimate com-
puted in constant time.

The similarity function sim(· , ·) when used with tight clusters already provides
a performant malware detection system. Figure 1 shows the probability distribution
for the distance among feature vectors drawn from different clusters. The majority of
the feature vectors are dissimilar (have zero similarity), with the distribution rapidly
falling off. In Figure 1, left, only 0.85% of the similarities are larger than 90 (the thresh-
old used to label two feature vectors as similar). Assume we could establish two lists
of tight clusters: G, formed by feature vectors produced by non-malicious programs
G = {g1, g2, . . .} and B , formed by feature vectors only ever produced by malicious
programs B = {b1, b2, . . .}. If any of the programs in B ∪ G execute and produce a
series of feature vectors yielding a tight cluster c, we can check from which set it
originates by using the largest similarity to select a set. If

max
b∈B

simT C (c, b) > max
g ∈G

simT C (c, g )

(3)

then c originates from malware, and the endpoint that produced it is likely compro-
mised.

If the distribution in Figure 1, left, were a good estimate for the distribution of
similarities between feature vectors in different tight clusters (taken over tight clus-
ters from all processes), then the fraction of tight clusters that would be incorrectly
considered similar by the decision procedure in Equation 3 would be 0.85% (at θS =
90). Such a malware detection procedure would be immune to obfuscations of exe-
cutable ﬁles, and it would be competitive with existing detection systems. The dis-
tribution in Figure 1, right, also shows the need for tight clusters. If we applied the
decision procedure in Equation 3 to the feature vectors without ﬁrst grouping them
into clusters, around 23% of the decisions would be misclassiﬁcations.

2.2 Program families

We next provide an example that illustrates the use of feature vectors to distinguish
between evolving versions of two open source code bases in the same program class.
The programs are Chrome [10] and Firefox [11], each of which has about 17 million
lines of code (loc, used to estimate the size of a code base; 17 Mloc is about 5000
developer-years of effort) as of this report. The different releases of each browser are
each treated as a separate program identiﬁed by their version number, as in Chrome
v12 or Firefox v3, for a total of 11 programs. We proﬁled Chrome versions 12, 17, 21,
and 35 and Firefox versions 3, 17, 20, 28, 32, 39, and 44 using executables that did not

5

Figure 1: Probability distribution of similarities among feature vectors from different
tight clusters. Left, 250 feature vectors were sampled from a small collection of 45
tight clusters, and similarities were calculated over all pairs consisting of two vectors
from different clusters. Right, the same set of tight clusters are used to show the
distribution of similarities for pairs of feature vectors within a tight cluster. In the
distribution on the right, 23% of the similarities are smaller than the θS threshold.

require installation [12]. We refer to any of these 11 programs as a browser program.
Each browser program was exercised by visiting a variety of websites. The process
was repeated six times for each version. During the experiment, stock Windows pro-
cesses may have also been executing, as no effort was made to limit them. Batches of
system calls were collected (5 second batches with Poisson distributed wait between
them with mean 3 seconds) and uploaded, but only data from the last few dozen were
used to generate the matrix. When Chrome is launched, multiple processes are cre-
ated as part of its sandboxing functionality. The feature vectors were collected for all
processes that were labeled chrome. To avoid the confusing visual banding this may
create, the feature vectors from Chrome were sorted ﬁrst by their version number
and then by process id.

Figure 3 shows the similarities among the feature vectors created by the different
versions of both browsers. The feature vectors generated by the browser programs
were indexed from 1 to 3073, gathered by contiguous ranges of integers for each
browser program: cv12 = [1, . . . , 659], cv17 = [660, . . . , 1335], . . . , fv44 = [2954, . . . , 3073].
Entry i , j of any matrix in the ﬁgure is a similarity between the feature vectors pi and
p j . The rightmost matrix is the similarity sim(pi , p j ), which can be decomposed as
the sum of the similarity between its zeta components, simζ(pi , p j ), and its mu com-
ponents, simµ(pi , p j ). Aside from small corrections due to extreme values, these are
essentially the cosine-like distances between the components of the feature vector
simζ(pi , p j ) = θζ〈ζi , ζ j 〉 and simµ(pi , p j ) = θµ〈µi , µ j 〉 .

6

00.0000.0020.0040.0060.0080.010501001502000.081sim(pa,pb)θS02004006008000.0000.0020.0040.0060.0080.010sim(pa,pb)0.014θSThe results in Figure 3 show that the browsers can be compared based just on the
average similarity of their feature vectors. Versions 12, 17, and 21 of Chrome average
64.2 similarity with each other, whereas their similarity to Chrome version 35 is only
18.9. We attribute the large similarity difference to the replacement of the rendering
engine from WebKit to Blink, which happened in version 23 [13]. The similarities
between any Chrome version and any Firefox version are all below 30.
The complete set of similarities among the versions of Firefox,

v3
126.6
47.4
57.0
42.9
43.8
28.0
25.4

v17
47.4
272.6
311.7
249.4
176.5
115.2
74.2

v20
57.0
311.7
405.2
322.0
244.7
139.2
90.8

v28
42.9
249.4
322.0
330.8
270.8
145.6
87.6

v32
43.8
176.5
244.7
270.8
361.2
165.9
97.7

v39
28.0
115.2
139.2
145.6
165.9
323.7
272.2

v44
25.4
74.2
90.8
87.6
97.7
272.2
294.1

v3
v17
v20
v28
v32
v39
v44

shows the consistency of the similarity and the smooth behavior as the code bases
increasingly differ. While Firefox version 3 differs the most from the other versions, it
still has an average similarity of 47.4 to version 17. The similarities slowly decay with
increasing version difference (and increasing changes in the code base).

If we had built a whitelisting system that compared feature vectors with a thresh-
old of θS = 30, we would have obtained only one false alert from the browser pro-
grams (the one originating from the introduction of Chrome 23). All other browser
programs would have been detected as updates. Given the estimated distribution
of similarities among different programs, illustrated in Figure 1, the whitelisting sys-
tem would be expected to work with an arbitrary program, as 93% of the mass of the

Figure 2: Browser timeline. Release dates for the versions of the browser programs
used to test the sensitivity of the similarity function to changes to the code base.

7

20082010201220142016ChromeFirefox121721351732028323944Figure 3: Similarities among the feature vectors collected from executing browsers.
Each row and column of the 3073×3073 matrix is shaded by similarity, darker hues in-
dicating higher similarity. Feature vectors are gathered by version. The diagonal has
been erased. Left, zeta components are compared; center, mu components; right,
the full feature vector. Chrome and Firefox have similarities in zeta component but
little in the mu, while different versions of each have mu similarity without being
identical.

probability distribution in in Figure 1, left, occurs at sim(· , ·) < 30. That is, a simple
comparison of feature vectors with a cutoff of 30 for the average feature vector sim-
ilarity can generalize for code development, and only radically new code bases or
completely new efforts would produce alerts.

2.3 Activities

The feature vectors produced by the system are grouped into clusters of high similar-
ity. These are the tight clusters endowed with the similarity simT C (· , ·) of Equation 2.
The feature vectors that comprise a tight cluster belong to a sequence of batches col-
lected over a few minutes and need not originate from the same process. This allows
us to detect longer range correlations among the system calls and helps us to identify
malware that executes in more than one process.

Each tight cluster corresponds roughly to an activity on the computer. To gain in-
sight into how tight clusters relate to usage, we compared the data generated by two
users over the course of 12 weeks. Different programs generated different numbers
of tight clusters, reﬂecting the variety of actions they performed:

8

ChromeChromeFirefoxFirefoxζμ(ζ,μ)executable
chrome
WINWORD
virtscrl
spoolsv
SetPoint
igfxTray

clusters description

66
46
32
26
20
7

Chrome browser
Microsoft Word
Lenovo Trackpad support
Windows print spooler
Logitech mouse support
Intel Graphics Accelerator System Tray Helper

The number of tight clusters contributed by a program decays rapidly with rank.
There were 66 tight clusters that contained feature vectors from chrome; the most
common clusters were seen 373, 146, and 126 times, but 43 tight clusters were seen 4
times or less. Most of the tight clusters had several contributing processes (the tight
clusters are also not unique to a process). Some of the tight clusters produced by
Chrome are also produced by Internet Explorer. A preliminary analysis shows that
different users tend to generate different distributions of tight clusters when using
the same programs.

The compositional nature of programs is reﬂected in the number of new activ-
ities that are created over time. New activities arise from novel execution paths or
user inputs that signiﬁcantly change the control ﬂow. Limits to programmer produc-
tivity and user variability manifest themselves in a human-scale creation rate for new
activities.

We collected 2174 hours of computer activity from a commercial version of our
system. The computers generally were shut down (or hibernated) when users left for
the day and on weekends. Each endpoint contributed a set of tight clusters approx-
imately every 6 minutes (for an average of a few thousand per week). The set has
345 463 tight clusters, but only 780 distinct ones. The distribution is shown in Fig-
−0.916. While distributions with
ure 4 and is well described by 93557e
power laws are difﬁcult to ﬁt (see the discussion in Clauset, Shalizi, and Newman
[14]), it is simple to verify that a power law function will not reproduce the empirical
distribution. (That is, a cutoff, as provided by the exponential, is necessary.)

−x/154(x + 5.75)

Figure 5 shows the number of new activities produced by one computer used by
one person over several weeks. The novelty count rises rapidly when the sensor is
ﬁrst installed, but then slows. While the rate differs for different classes of users, over
a similar time period we have not seen more than 160 new activities from a single
endpoint.

The new activity creation rate for users (such as the one shown in Figure 5) all
α

appear to have a fractional power law behavior where the novelty count grows as t

9

Figure 4: Activity counts sorted by the rank. The data represent 2174 hours of activity.
Among the 780 activities seen, the most common one was detected 16 551 times. The
data are well described by a truncated power-law (see the text), suggesting that the
number of activities is bounded in practice.

Figure 5: Example rate of new activities for a single endpoint. The rate of new activi-
ties continues to slow, with 109 new activities observed in 54 calendar days.

10

0200400600110104103100rankcount102030405020406080100newactivitiescalendar daysFigure 6: Rate of new activities for all endpoints on a network. As the number of
machines active at any one time varied, the number of distinct activities is expressed
in terms of the total number of activities observed.

for some α < 1. If we interleave several of these processes with different values of α
αc with a value of αc
on time scales of minutes, the resulting process will still grow as t
that is bounded from above by the most productive of the interleaved processes (the
one with the largest α). This suggests that the creation rate of new activities can be
estimated from the larger dataset obtained by joining streams of data from different
endpoints. (We were inspired by the metabook concept of Bernhardsson, Correa da
Rocha, and Minnhagen [15].)

Figure 6 shows the number of new tight clusters as a function of the total number
of tight clusters over the 2174 hours of data collected. The data are well described
by a power law with the number of new clusters m given as a function of the total
number N of tight clusters collected

m = 8.36 N 0.358 .

(4)

Larger exponents bound the data from above, and smaller exponents miss earlier
data but ﬁt later data better, making (as will be shown) Equation 4 a conservative
estimate.

The power law growth of new tight clusters exhibited in Equation 4 is reminiscent
of Heaps’ description of the number of unique words in a corpus as a function of its
size [16]. The relation (known as Heaps’ law) can be derived from Zipf’s distribu-
tion of word frequencies by assuming that the corpus is randomly drawn. Motivated
by the existence of a cutoff in the distribution of tight cluster counts, we consider a
modiﬁcation of Heaps’ law when the data are ﬁnite.

The overlap of the possibility of an inﬁnite number of different tight clusters with

11

50000100000150000200000250000300000tight clusters200400600800uniquetightclustersthe restriction of an always ﬁnite data set makes the problem subtle. The sequence of
tight clusters can be approximated as a generalized Pólya urn scheme [17] if we make
the assumption that each tight cluster is independent of previous ones (which is in
our case an approximation). In this case, the number of tight clusters is expected to
be ﬁnite and well-characterized by a Dirichlet distribution. Baek, Bernhardsson, and
Minnhagen [18] describe how the Dirichlet is expected to have a parameter vector
where the magnitude of its components is rank distributed as an exponentially trun-
cated power law as the number of components grows. Baek et al. also explain that
as the size of the collected dataset grows, the number of distinct items seen will grow
following an approximate Heaps’ law with a parameter α that approaches zero at a
rate of 1/ ln N .

We have checked that our dataset is well described by the results in Baek, Bern-
hardsson, and Minnhagen [18]. The probability distribution for groups of tight clus-
with k0 = 896
ters of size k is described by a distribution proportional to e
and γ = 1.04. By a resampling procedure in which only half the data are utilized, we
observed that k0 is in the range [453, 2953] and γ is in the range [0.9, 1.1]. Parameters
outside those intervals yielded visibly poor ﬁts. We also used a resampling procedure
to estimate the exponent α in Equation 4. Using half the data the exponent was 0.41,
suggesting that it decreases with increasing numbers of observations, as expected
from ﬁnite size considerations.

−k/k0 x

−γ

2.4 Phylogeny

To collect the tight clusters of potentially malicious programs, we needed a large and
representative sample of executables. We sampled a number of collections, some
containing advanced or specialized malware, and others containing a broad collec-
tion. The broad collections were from VX Heavens (covering roughly the years 2005–
2010) [19], Open Malware (covering roughly 2011–2012) [20], and Virus Share (cov-
ering 2012–present) [21]. The specialized collections, which contain samples that
were found interesting by one or more cybersecurity researchers, were from Con-
tagio Dump / DeepEnd Research [22], malwarechannel [23], theZoo [24], and Ker-
nelMode [25]. We ran experiments on all PE32 and PE32+ executables sourced from
the advanced collections. For the broad collections, we grouped the PE32 and PE32+
samples statically using their import lists, then resampled each group to try to nor-
malize the populations of different malware families. Small clusters were included
whole.

We deﬁne a program family as the subset of executables that have an orderable
set of activities when placed in similar environments. A program family roughly cor-

12

responds to any executable created from the same code base, even if this code base
has gone through a small amount of evolution. (Rarely, it could indicate convergent
evolution to the level of implementation details.)

All maintained software evolves: bugs get ﬁxed, features get added, and code
adapts to changing features of its ecosystem. Given the nature of a program and
its evolution, developers distinguish between releases with a variety of numbering
schemes. In a major.minor release scheme, a program family corresponds to the
variations due to patches and minor releases. The results in subsection 2.2 show
how several releases of Google Chrome group into one family.

The relationship between a code base and the set of tight clusters that may occur
permits a facile construction of a malware phylogeny. We ran each PE32 or PE32+
sample that we had selected repeatedly, to capture as many of the tight clusters as
possible. Experiments were run using Windows 7 x64 virtual machines on Xen hosts.
Countering all possible anti-virtualization approaches was impossible on the scale of
the experiments conducted (for example methods for detecting sandboxes, see Or-
tega [26]). We avoided easily-detected methods that require an agent on the guests
to generate the user events.
Instead, we recorded actions and played them back
through the vm console (sometimes in a randomized order).

We used our system to extract tight clusters from each experiment. We also con-
structed a list of tight clusters extracted from identical experiments conducted in the
absence of malware. For each sample we pooled all tight clusters not matching any of
the tight clusters extracted in experiments conducted in the absence of malware; we
then reprocessed the feature vectors from them as though they had originated from
a single endpoint. This allowed us to generate a list of tight clusters for each sample.
The use of tight clusters allowed us to summarize over 50 000 hours of activity in the
absence of malware in a list of about 200 tight clusters. When we used a less exten-
sive list of tight clusters generated from a few hundred hours of usage in the absence
of malware, contamination was found in the lists of tight clusters associated with 2%
of the samples. (In production deployments the AI layer handles previously unseen
activity; here, it was necessary to mask it so that we could bootstrap the AI layer.)

We then compared all pairs of samples for which we obtained tight clusters, im-
puting a directed edge a → b if each of the tight clusters in b had at least one similar
tight cluster in a. To avoid both spurious similarities and similarities between unre-
lated malware that ran inside common Windows processes (such as explorer), we
deﬁned tight clusters a and b as similar when θµ〈µa, µb〉 > 100. As there were over 25
billion pairs, it was only possible to conduct the comparison using a reverse index on
the µ-words. Even so, it was necessary to conduct the calculation in parts.

13

The resulting directed graph contains over 99 million edges in 99 235 ﬁles. The
ﬁles were combined by repeatedly: (1) merging two ﬁles; (2) merging vertices in
loops; and (3) computing the transitive reduction. It was also necessary to conduct
this calculation in parts.

The merged and reduced graph was converted to an undirected graph, which
contained 2.1 million edges and 9280 vertices, referencing 51 648 malicious executa-
bles. The graph consists of a few large connected components and a long tail of
smaller ones. For each connected component other than the largest we created one
family. For the largest connected component we iteratively removed vertices with
(These vertices are suprema
no in-degree (in the directed graph) until invariant.
of fully-ordered subsets of the vertices in the connected component, and where no
other apparently identical sample was present. We believe that these samples mainly
are downloaders that manifest varied behavior across experiments.) We then cre-
ated one family for each cluster found in the invariant part using Mathematica’s
FindGraphCommunities with the default algorithm (which is similar to infomap
[27]).

From these steps we obtained 1111 families (all of which contain at least two
members). The association between observations (here, malicious executables) and
families permits us to estimate the population size using statistical procedures sim-
ilar to those adopted in the ecological literature (see Efron and Thisted [28] for a
widely adopted method and Orlitsky, Suresh, and Wu [29] for a recent one). In the
ecological case, ﬁeld observations provide a value for the number of specimens of
different spices as a function of some sampling effort (typically the observation pe-
riod). In the malware case we are interested in estimating the total number of de-
ployed malware families that could execute on a Windows endpoint. Given the many
estimation procedures (see [30] for a review) for the total population, reﬂecting dif-
ferent assumptions about the underlying statistical processes, we applied a variety
of these, including Monte Carlo methods. The estimates for the number of current
malware families ranged from 1700 to 5200, from which we quote the median of 3450
as a reasonable estimate. Relevant to both the malware detection problem and the
functioning and scalability of our AI system, this number is in the thousands and not
hundreds of thousands or millions. With the estimated range for the total number of
malware families, one should expect 1 to 2 new malware families appearing per day.
We then repeated the comparison, graph construction, and clustering processes
using those samples that were dissimilar to any other in the test above. In this step
we deﬁned tight clusters a and b as similar when θµ〈µa, µb〉 > 60. We refer to families
identiﬁed in this step as weak program families; we found an additional 12 (out of 42

14

estimated) weak families. Identifying these families is not essential to the operation
of the system, but doing so allowed us to either include in a family (or a weak family)
or identify as similar to a family (or a weak family) over 99% of the samples that pro-
duced tight clusters in our 2.6 million experiments. We refer to the sets of previously
detected program families as a cover. (In sketching this process we referred to the
malware cover as B and the non-malware cover as G.)

The estimate of 3450 families was obtained using a corpus derived by resampling
using static features. The estimator therefore reﬂects the distribution of malware
sampled by the probability of breaching the ﬁrewall and antivirus layers (which also
rely extensively on static features). This is the relevant population for an endpoint
detection and response system; our collection represents an estimated 32% of the
whole. While not strictly required, the existence of a reasonably-sized population
with good coverage in the training set provides some assurance that an AI layer will
generalize to new and previously-unseen samples.

2.5 Deep learning

For the features we use the zeta component of the original feature vector, a random
projection of a subset of the 2-grams present in the subsequences used to create the
mu component, and global statistics of both components derived from a partition
function over their probabilities (by taking constant size boxes in the method de-
scribed by Halsey et al. [31]). We trained a 6-layer deep belief network (hidden layer
sizes 512, 256, 128, 64, 32) with ReLU activation functions and softmax output on the
two-class classiﬁcation problem (cl n, mal ). We used this network to bootstrap class
labels for the three-class classiﬁcation problem (cl n, l i b, mal ) (where the target la-
bel is cl n for all non-malware input, mal for malware input classiﬁed correctly by the
two-class classiﬁer, and l i b for malware input classiﬁed incorrectly by the two-class
classiﬁer). We train an identical (other than the width of the visible layer) network on
the bootstrapped class labels and map the softmax output σ(z) (where z is the output
of the visible layer) to an odds ratio with

pmal
pcl n

=

σ(z)mal + (1 − θcl n←l i b)σ(z)l i b
σ(z)cl n + θcl n←l i bσ(z)l i b

.

(5)

Intuitively, mixtures of the lib and mal state correspond to hypothetical malicious
programs that can be constructed as main.obj (malicious) and one or more libraries
such as msvcrt.dll and Crypt32.dll (not inherently malicious). The bootstrap-
ping step was necessary. With the two-class classiﬁer we obtained F1 ≈ 0.9 (without
extensive training). With the three-class classiﬁer and θcl n←l i b = 0.9 performance

15

increased to F1 = 0.995 (as will be described). We did not further optimize θcl n←l i b.
For testing we isolated malware that was well-separated by collection time from
the training data. The training data was collected prior to March, 2017, the validation
data was collected between March and June, 2017, and the test data was collected in
July, 2017. We then removed from the test sample all tight clusters similar to any in
the training sample. This includes tight clusters that were not present in the cover of
1111 families but were obtainable from the training data. We also excluded from the
test any samples for which we failed to obtain at least 10 feature vectors (about 80
seconds of execution; this step is necessary to avoid testing on samples that identify
the sandbox environment and exit rapidly).

The low rate of new malware families means that estimating the ability of the
deep network to generalize requires ﬁltering the test set using a dissimilarity. Other-
wise, with many samples, even rare samples in the test set will have with high prob-
ability many homologs in the training set that differ only in obfuscation of the static
features. As shown by Zhang et al., deep neural networks are capable of memorizing
their training set [32].

For the non-malware portion of the test tight clusters were randomly sampled
from endpoints that were excluded from the training set. Human interface device
driver control programs were excluded. (The drivers capture and modify user events,
a dangerous behavior; programs in this class should be considered malicious by an AI
system. The kernel driver means that signature-based controls are required for these
programs anyway.) Among the most recently observed 7746 tight clusters there were
11 that were considered more likely than not to be potentially malicious: 2 of the
search indexer; 1 of a management utility; and 8 of a browser add-on that rewrites
web pages before rendering to add reputation information to links. We conserva-
tively considered all of these to be erroneous in calculating the overall performance;
excluding the rewriter (which shares a behavior with a broad class of malware) would
reduce the false positive rate to 0.38 per thousand.

For the malware portion of the test we considered whether a sample is identi-
ﬁed as malicious using all of its tight clusters. (The use of all tight clusters is similar
to the use of all system calls with more complex network architectures, which may
learn to identify thread context switches.) We excluded tight clusters that were not
well-characterized (fewer than 10 feature vectors) or not novel (similar to any used
to train). We excluded entirely any samples that produced no non-excluded tight
clusters.

Results from the deep network should be considered to be a lower bound on per-
formance, as the test samples were not hand-labeled. Few groups have made public

16

confusion

performance

predicted

mal
cl n

actual

mal
7384
70

cl n
11
7735

FPR

FNR

0.0094
0.0014
precision 0.9985
0.9906

recall

F1
Youden’s index
Error rate

0.9945
0.9892
0.0053

Table 1: Performance of the neural net layer used to score previously-unseen activ-
ities. Results are shown using Equation 5 with θcl n←l i b = 0.9 and a threshold odds
ratio of 1 to return the mal class label.

the performance of deployable AI systems; our results compare favorably with them
(F1 0.65 to 0.85, error rate 0.13 to 0.25 [6, 33]). We also report Youden’s index, which
better describes the residual work after applying a test (here, the AI). We estimate the
residual work with our system to be 1.1%, with others falling between 26% and 51%.
The greater than 20-fold reduction in work provides a basis for AI-driven cybersecu-
rity.

3 Performance

We next describe the performance of the system from two perspectives: using tradi-
tional machine learning tests to describe the ability of the system to perform a task
well and in a generalizable manner; and by a cost analysis to demonstrate that the
algorithms and parameters together can be used in a million-endpoint deployment.

3.1 AI

In Table 2 we quote the performance of our system. We use the same corpus and
methods used in subsection 2.5 to test the neural net performance (7556 detections
from recent malware samples and 7746 detections from non-malware; the num-
bers differ slightly from those in Table 1 due to differing exclusion criteria). For the
contamination reasons discussed in subsection 4.2, we do not provide performance
numbers for the traditional test in machine learning (in which 10% of the training
data is held out). Testing the cover in isolation is difﬁcult because it would require
us to produce independently-labeled similar activities. We can report that in several
months of using the system we did not see any false positives for which it was difﬁcult
to understand why the system labeled a program as malicious.

One of the reasons to test a machine learning system is to demonstrate that it
has not been overﬁtted and that it generalizes well. This is difﬁcult to establish if

17

system
Cover + net
No homologs
Net only

F1
0.992
0.987
0.995

error
0.008
0.010
0.005

FPR
0.0014
0.0014
0.0014

Table 2: Performance of the system under different conditions. The line “Cover +
net” best reﬂects the expected performance under deployed conditions. For details
see the text.

the test data were contaminated with the training data. In a cybersecurity context
this may occur if the training and test samples are distinguished only by checksums.
As most malware is obfuscated, it is common to generate many different executa-
bles (with different checksums) by randomizing aspects of an executable that do not
modify the effects of executing it. To avoid contamination, many cybersecurity ma-
chine learning systems are tested with malware samples that are collected after the
training in an effort to include new malware code bases. This is the approach that we
used to report the performance data shown in Table 2.

The performance shown for the “Cover + net” system was measured by compar-
ing the test detections to all tight clusters in the cover, then feeding those that did not
match well to the deep neural net described in subsection 2.5. This is the common
“by-date” test used to test machine learning systems for cybersecurity.

Even with the by-date approach the test data may still be contaminated. Samples
in the test set may arise from existing malicious codebases either from code reuse or
because the malware was only recently detected and added to the corpus. To mitigate
that possibility, we adopted an approach that uses the phylogeny we established in
subsection 2.4 to eliminate any activities that match an existing activity in the cover.
The cover allows us to perform a close approximation of an attack with never before
used executables. By eliminating any tight clusters generated from malware sam-
ples that match the cover, we eliminate from the test set most of the malware that
is a slight modiﬁcation of previously-seen malware (the homologs). The results are
shown on the “No homologs” line; we see very little loss in performance.

We reproduce the results of using only the AI layer (reported in subsection 2.5)
on the line “Net only”. The slightly increased performance reﬂects the fact that the
exclusion process described in subsection 2.5 preferentially eliminates failure cases
for the network. These cases (which match activity from malware experiments but
were not included in the cover) may reﬂect non-malicious behavior early in the mal-
ware execution pathway (frequently for sandbox evasion). The network prefers to
label them cl n despite many training examples with the mal and l i b class labels.

18

3.2 Scaling

In our system data streams from different endpoints only interact with each other
through the shared database of observed activities. Activities are timestamped with
their initial observation time and can be shared among a ﬂeet of servers; if an activity
is detected nearly simultaneously on two or more servers, then agreeing to keep only
the activity with the earliest detection time among similar activities creates an even-
tually consistent collection. To model how the system scales to very large networks,
we need only consider the server-side compute cost, the number of false positives
(which creates human costs, as best practices in computer security suggest they be
investigated), and the implications of a larger working set of activities that have been
recently observed.

The number of activities will grow slowly. Given the theoretical robustness of
the distributions we have observed, we expect that a fractional power law function
(Equation 4) will bound the number of unique tight clusters as a function of the num-
ber of observed clusters, even when the number of observed clusters is much larger
than in our current datasets. Larger datasets will have even smaller exponents, im-
proving the performance of the system. Even with the exponent of 0.358, one million
endpoints should produce about 21 thousand new activities in one week, which can
be handled by an in-memory database. We can ignore the cost of eventual consis-
tency among databases; even with one billion endpoints and neglecting improve-
ments to scale, Equation 4 estimates the new activity rate to average fewer than 1 per
second.

The interaction between the sensor and the compute server occurs through a
REST API [34], which allows the system to leverage existing hyper-scalable web tech-
nologies. Using CPUs to compute and discarding data after processing (which allows
us to dispense with some database costs), we observe server side compute costs un-
der 1 watt per continuously protected endpoint. (Using a sampling strategy and cor-
recting for the duty cycle, this would allow a single 10 MW data center to protect tens
to hundreds of millions of endpoints. Bandwidth usage would not saturate modern
interconnects.)

The false positive rate is determined by the performance of the AI layer and the
rate of new activities (as false positives, once investigated, can be made not to recur).
The AI layer described in subsection 2.5 has a false positive rate of 1.3 per 1000. With
one million endpoints and assuming an exponent of 0.358, the system would average
26.2 false positives per week. (Further scaling improvements would be observed on
larger networks.)

The size of the working set has implications for detecting malware, as the like-

19

lihood of a spurious match between a new activity and one in the database grows
with the database size. (To characterize users by the distribution of the observed ac-
tivities, it sufﬁces to substitute a probability distribution for the one-hot vector that
encodes the activity.) Estimating the number of false negatives requires an estimate
of the infection rate, but if 1% of the endpoints were executing malware each in a sin-
gle process, with one million endpoints the system would average 62 false negatives
per week from working set size effects.

4 Discussion

We have described an AI system that produces a stream of scored descriptors that re-
ﬂect the activities of an endpoint. The system has an F1 score of 99.1% at a false posi-
tive rate of 0.14%. This is achieved by using a similarity function for the activities that
generalizes well and that groups the bulk of the malware seen over the last 10 years
into 1111 program families. A version of the system has been deployed. We have ar-
gued that it could scale with the observed (or better) characteristics to a network of
one million endpoints. We now place these claims into the context of previous work
and accepted practices in the cybersecurity and machine learning communities.

4.1 Similarity function

The performance of the system is a reﬂection of the properties of the similarity func-
tion sim(pa, pb) (Equation 1). The similarity changes smoothly with modiﬁcations
to a code base, as the examples of Chrome and Firefox show. Between 2011 and 2016
the loc for each program more than tripled, yet the similarity function identiﬁes sim-
ilarities between the most distantly related versions shown in Figure 2. The similarity
function is at the appropriate scale for following a code base that only changes as fast
as programmers can modify it.

We can use the similarity function to deﬁne a program family as the set of exe-
cutables that have strong µ-similarity, as we did in creating the malware phylogeny
(subsection 2.4). Because the system characterizes in-execution malware, it is not
hampered by static ﬁle obfuscations and steganographic techniques; because it does
not rely on emulation techniques, it is not hampered by ﬂow-control hiding. These
properties allow a better characterization of existing malware. We estimated in sub-
section 2.4 that the total number of malware families was 3450, much smaller than
generally believed.

The malware phylogeny is constructed using the similarity function simT C (· , ·).
Several algorithms in machine learning do not have an explicit similarity function,

20

but if they are consistent (converge with large training samples), then they are also
local in the feature space (values of the function depend only on nearby points), as
discovered by Zakai and Ritov [35]. Those algorithms could, in principle, reproduce
our construction of the malware phylogeny, estimate the number of families, and
remove homologs for testing AI subsystems, as described in subsection 4.2.

The cover-matching part of our AI is a k-nearest neighbor algorithm, which is of-
ten eschewed in machine learning because of the difﬁculty in ﬁnding nearest neigh-
bors [36, see sections 13.3–5]. In our case the feature vectors can be indexed using
information retrieval techniques because the mu part is a very sparse vector of token
counts.

4.2 Malware in the wild

Reported numbers on malware vary dramatically depending on how they are classi-
ﬁed. Based on ﬁle checksums, VirusTotal receives over 50 000 new executables every
day that are not labeled by any of the scanners that it hosts [37]. Detection rates
increase with a lag, but some of those executables are tagged as generic malware
rather than as a member of any of the families that antivirus vendors track [38, 39].
Such numbers cannot reﬂect different program families in the sense of different code
bases. A Fermi estimate [40, 41] for the number of groups developing malware sug-
gests not more than a few hundred groups engaged in malware development: a num-
ber of states sponsoring multiple groups and a handful of additional black market
enterprises. If each of these groups releases two substantially new malicious pro-
grams a year, we should expect between one and two new (not detected by existing
engines) pieces of malware daily in VirusTotal (and maybe 4000 over 10 years). We
attribute the difference to the poor detection capabilities of current systems. (Us-
ing ssdeep [42] to cluster samples from Virus Share, Oi [43] achieved about a 17-fold
reduction, which is still over 1000-fold too high.)

If we accept that there is much less distinct malware than what is estimated from
the number of distinct checksums, then it is clear that testing of malware detection
AI systems needs to be modiﬁed. One of the reasons that machine learning systems
are tested is to ascertain their ability to generalize. If a randomly-chosen tenth of
the data is used for testing, it is very likely that copies of the same malware (but
with different checksums) will be in both the training and the test sets. For the com-
mon case in which the labeled corpus consists of n malware samples and n benign
programs with a tenth used for testing, a malware that occurs r times in the set is
−r /10/40) likely to be in the training and test sets. To a ﬁrst ap-
1 − e
proximation the probability is independent of the corpus size, 2n. In a corpus with

−r /10 + O(n

−1r 2e

21

F1
system
Cover + net
0.992
Android LSTM 0.915
0.857
ESN + LR
0.686
SOC

error
0.008
0.084
0.125
0.455

FPR
0.0014
0.0710
0.0014
0.9000

reference
[this paper]
[Ref. 44]
[Ref. 6]
[see text]

Table 3: Comparison of various AI approaches to security operations. A composite
estimate of the expected performance of current SOC practices is included for refer-
ence.

250 000 pieces of malware, 46 copies of the same malware (code, not hash) would be
sufﬁcient to have a 99% probability of having one of the 46 in both the training and
test sets. With just two copies the probability is 18%.

4.3 Performance comparisons

The system was designed to be deployable (the technology has been incorporated
into a commercial product), which limits the set of algorithms that can be used,
bounds the needed performance, and caps operational costs per endpoint. The
stream of scored activities can be combined with other data to create a system that
comes to a decision and performs an action. We have concentrated on the case of
an alert that is used to trigger an investigation into possible exploitation of an end-
point (how we quoted the performance). The data have also proven useful for the
insider threat aspect of cybersecurity; that use case requires a time series of actions
that were performed on the endpoint.

For comparison we have included a few other results from the literature, shown
in Table 3. Production workﬂows use analysts in security operations centers (SOCs)
to interpret the results of ad hoc collections of tools that process log data, network
data, and (in some cases) sparse subsets of system calls. We are not aware of any
published tests of the performance of such workﬂows, but public comments from
practitioners suggest high false positive rates (FPR). As a baseline guide we provide
an estimate using 0.9 as the FPR, 0.01 as the false negative rate (FNR), and a test in
which an equal number of malware and non-malware programs had to be identiﬁed.
Its performance is indicated by the line marked “SOC” in Table 3, with an F1 score of
0.686 and an error rate of 0.455 (deﬁned as the number of erroneous decisions as a
fraction of the total). The only other result we identiﬁed that quotes a performance
with a FPR of under 1% was the work of Pascanu et al. [6]; in their work ROC curves
−4. From those curves we computed the num-
present data starting at a FPR of 1 × 10

22

bers for the line “ESN + LR”. At an FPR of 1%, the same system achieves an F1 score of
0.952. Another approach that appears well suited to identifying malware with system
calls employs LSTM networks. We quote the results of Xiao et al. [44] (which do not
go below a FPR of 7.1%, unsuitable for production work) on the “Android LSTM” line
in the table.

4.4 Sensor

The sensor collects the complete stream of system calls using facilities provided by
the operating system. We use the ETW (Event Tracing for Windows) framework. ETW
has a great advantage for professional (as opposed to research) software; it is devel-
oped and maintained by the authors of the operating system. Other systems that
attempt to characterize the functioning of a computer by examining the system calls
use some form of function hooking (in which the system calls are made to execute
code that provides new functionality, such as recording that the function was called).
Hooking breaks one or more software invariants (locking strategies, calling conven-
tions, timing constraints, and many other hard-to-enumerate properties) that pro-
grammers rely upon and performant and compliant software demands. Hooking
also slows down the computer; factors of 2 are common [45]. The slowdown arises
from the loss of cached context when the operating system switches between kernel
and user modes.

To keep the load placed by the sensor on the CPU below 1%, the arguments to
the system calls are not collected. Intuition and previous work [4] suggest that better
performance can be achieved by using the arguments of the system calls. By concen-
trating on only the calls, we were forced to develop detection techniques that reﬂect
the actions of processes rather than the target of those actions (a verbs versus nouns
distinction). Battery-powered devices connected over cellular networks are limited
in the amount of power that they can allocate to data transmission; this gives prefer-
ence to systems with low bandwidth requirements. The sensor averaging 5 seconds
executing and 10 seconds idling will upload an average of 650 bytes/s.

4.5 Strategies

Cyber attackers use automated means that render defenses that incorporate manual
steps infeasible at scale. Van Dijk et al. [9] captured the essence of the problem by
framing it as a resource utilization contest; alerting and triage are among the least
efﬁcient steps in the defender’s kill chain [46]. AI could reverse this asymmetry, but
only if it is sufﬁciently performant to operate autonomously. Our system has several

23

characteristics that make this possible. The cover (subsection 2.4) will often detect
previously seen malware with just one batch of data. The false negative rate of 0.0094
(subsection 2.5) permits fully automated procedures for all but the largest or most
secure organizations. The ability to detect executables that are variations of a code
base (subsection 2.2) will force attackers to use increasingly expensive strategies in
an effort to maintain access.

References

[1] Curt Hastings and Ronnie Mainieri. “Methods and systems for encoding com-
puter processes for malware detection”. U.S. pat. req. 62088324. Dec. 5, 2014.

[2]

Stephanie Forrest et al. “Self-nonself discrimination in a computer”. In: Re-
search in Security and Privacy, 1994. Proceedings., 1994 IEEE Computer Society
Symposium on. IEEE. 1994, pp. 202–212.

[3]

Stephanie Forrest et al. “A sense of self for Unix processes”. In: Security and
Privacy, 1996. Proceedings., 1996 IEEE Symposium on. IEEE. 1996, pp. 120–128.

[4] Davide Canali et al. “A quantitative study of accuracy in system call-based mal-
ware detection”. In: Proceedings of the 2012 International Symposium on Soft-
ware Testing and Analysis. ACM. 2012, pp. 122–132.

[5] Kristina Blokhin, Josh Saxe, and David Mentis. “Malware similarity identiﬁca-
tion using call graph based system call subsequence features”. In: Distributed
Computing Systems Workshops (ICDCSW), 2013 IEEE 33rd International Con-
ference on. IEEE. 2013, pp. 6–10.

[6] Razvan Pascanu et al. “Malware classiﬁcation with recurrent networks”. In:
2015 IEEE International Conference on Acoustics, Speech and Signal Process-
ing (ICASSP). We compare our results with the data in Fig. 2 (right). IEEE, 2015,
pp. 1916–1920.

[7] Bojan Kolosnjaji et al. “Deep learning for classiﬁcation of malware system
call sequences”. In: Australasian Joint Conference on Artiﬁcial Intelligence.
Springer. 2016, pp. 137–149.

[8] Ben Athiwaratkun and Jack W Stokes. “Malware classiﬁcation with LSTM and
GRU language models and a character-level CNN”. In: Acoustics, Speech and
Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE. 2017,
pp. 2482–2486.

24

[9] Marten van Dijk et al. “FlipIt: The Game of ‘Stealthy Takeover’”. In: Journal of

Cryptology 26 (2013), pp. 655–713.

[10] Chromium (Google Chrome). URL: https://www.openhub.net/p/chrome.

[11] Mozilla Firefox. URL: https://www.openhub.net/p/firefox.

[12] PortableApps.com. URL: https://portableapps.com/.

[13] Klint Finley. “Google Chrome Breaks Up With Apple’s WebKit”. In: Wired (Mar.

2013). URL: https://goo.gl/YmfUbj.

[14] Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. Newman. “Power-Law Dis-
tributions in Empirical Data”. In: SIAM Review 51.4 (2009), pp. 661–703.

[15]

Sebastian Bernhardsson, Luis Enrique Correa da Rocha, and Petter
Minnhagen. “The meta book and size-dependent properties of written
language”. In: New Journal of Physics 11 (Dec. 2009), p. 123015.

[16] H S Heaps. Information retrieval, computational and theoretical aspects. Li-

brary and information science. Academic Press, 1978.

[17] David Blackwell and James B MacQueen. “Ferguson Distributions via Pólya

Urn Schemes”. In: The Annals of Statistics 1 (1973), pp. 353–355.

[18]

Seung Ki Baek, Sebastian Bernhardsson, and Petter Minnhagen. “Zipf’s law un-
zipped”. In: New Journal of Physics 13.4 (Apr. 2011), p. 043004.

[19] VX Heavens Snapshot. URL: https://archive.org/details/vxheavens-2010-05-

18.

[20] Open Malware. URL: http://openmalware.org/.

[21]

J.-Michaels Roberts. VirusShare, Online malware repository. Oct. 2017. URL: ht
tps://virusshare.com.

[22] Mila Parkour. contagio malware dump. URL: https://contagiodump.blogspot

.com/.

[23] MalwareChannel. Feb. 2015. URL: https://twitter.com/malwarechannel.

[24] Yuval tisf Nativ and Shahak Shalev. theZoo aka Malware DB. July 2017. URL: ht

tp://thezoo.morirt.com/.

[25] KernelMode.info. URL: http://www.kernelmode.info/.

[26] Alberto Ortega. paﬁsh. URL: https://github.com/a0rtega/pafish.

[27] Martin Rosvall and Carl T Bergstrom. “Maps of random walks on complex net-
works reveal community structure”. In: Proceedings of the National Academy
of Sciences 105.4 (2008), pp. 1118–1123.

25

[28] Bradley Efron and Ronald Thisted. “Estimating the number of unseen species:
How many words did Shakespeare know?” In: Biometrika 63.3 (1976), pp. 435–
447.

[29] Alon Orlitsky, Ananda Theertha Suresh, and Yihong Wu. “Optimal prediction
of the number of unseen species.” In: Proceedings of the National Academy of
Sciences 113.47 (Nov. 2016), pp. 13283–13288.

[30]

John Bunge and M Fitzpatrick. “Estimating the number of species: a review”.
In: Journal of the American Statistical Association 88.421 (1993), pp. 364–373.

[31] Thomas C. Halsey et al. “Fractal measures and their singularities: The charac-
terization of strange sets”. In: Physical Review A 33 (1986), pp. 1141–1151. DOI:
10.1103/PhysRevA.33.1141.

[32] Chiyuan Zhang et al. “Understanding deep learning requires rethinking gener-

alization”. In: arXiv preprint arXiv:1611.03530 (2016).

[33]

Iain Thomson. Google reveals Android Robocop AI to spot and destroy malware.
The Register. Sept. 2017. URL: https://goo.gl/YwTaav.

[34] Roy T. Fielding and Richard N. Taylor. “Principled Design of the Modern Web
Architecture”. In: ACM Transactions on Internet Technology 2 (May 2002),
pp. 115–150.

[35] Alon Zakai and Ya’acov Ritov. “Consistency and Localizability”. In: Journal of

Machine Learning Research 10 (Apr. 2009), pp. 827–856.

[36] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Sta-
tistical Learning: Data Mining, Inference, and Prediction, (Springer Series in
Statistics). 2nd ed. 2009. Springer, 2009. ISBN: 978-0-387-84857-0.

[37] VirusTotal. See the statistics section. Oct. 2017. URL: https://virustotal.com.

[38] Giovanni Vigna. Antivirus Isn’t Dead, It Just Can’t Keep Up. Lastline Blog. May

2014. URL: https://goo.gl/jq7ivb.

[39] Marcos Sebastián et al. “Avclass: A tool for massive malware labeling”. In:
Research in Attacks, Intrusions, and Defenses: 19th International Symposium,
RAID 2016, Proceedings. Springer, 2016, pp. 230–253.

[40] Philip Morrison. “Fermi Questions”. In: American Journal of Physics 31.8

(1963), pp. 626–627. DOI: 10.1119/1.1969701.

[41] Edward M. Purcell. “The Back of the Envelope”. In: American Journal of Physics

53.7 (1985), pp. 615–615. DOI: 10.1119/1.14264.

26

[42]

Jesse Kornblum. “Identifying almost identical ﬁles using context triggered
piecewise hashing”. In: Digital investigation 3 (2006), pp. 91–97.

[43] Tsukasa Oi. VirusShare-related archives. URL: http://a4lg.com/downloads/vxs

hare/index.en.html.

[44] Xi Xiao et al. “Android malware detection based on system call sequences and
LSTM”. In: Multimedia Tools and Applications 2 (Sept. 2017), pp. 1–21.

[45] Zhui Deng et al. “IntroLib: Efﬁcient and transparent library call introspection
for malware forensics”. In: Digital Investigation 9 (Aug. 2012), S13–S23.

[46] Ram Shankar Siva Kumar, Andrew Wicker, and Matt Swann. “Practical Ma-
chine Learning for Cloud Intrusion Detection: Challenges and the Way For-
ward”. In: arXiv preprint arXiv:1709.07095 (2017).

27

