0
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

1
v
6
3
5
0
1
.
3
0
0
2
:
v
i
X
r
a

PROGRAML: GRAPH-BASED DEEP LEARNING FOR
PROGRAM OPTIMIZATION AND ANALYSIS

Chris Cummins∗
School of Informatics
University of Edinburgh
c.cummins@ed.ac.uk

Zacharias V. Fisches*
Department of Computer Science
ETH Zurich
zfisches@student.ethz.ch

Tal Ben-Nun
Department of Computer Science
ETH Zurich
talbn@inf.ethz.ch

Torsten Hoeﬂer
Department of Computer Science
ETH Zurich
htor@inf.ethz.ch

Hugh Leather
School of Informatics
University of Edinburgh
hleather@inf.ed.ac.uk

March 25, 2020

ABSTRACT

The increasing complexity of computing systems places a tremendous burden on optimizing compil-
ers, requiring ever more accurate and aggressive optimizations. Machine learning offers signiﬁcant
beneﬁts for constructing optimization heuristics but there remains a gap between what state-of-the-
art methods achieve and the performance of an optimal heuristic. Closing this gap requires improve-
ments in two key areas: a representation that accurately captures the semantics of programs, and a
model architecture with sufﬁcient expressiveness to reason about this representation.
We introduce PROGRAML — Program Graphs for Machine Learning — a novel graph-based pro-
gram representation using a low level, language agnostic, and portable format; and machine learning
models capable of performing complex downstream tasks over these graphs. The PROGRAML rep-
resentation is a directed attributed multigraph that captures control, data, and call relations, and
summarizes instruction and operand types and ordering. Message Passing Neural Networks prop-
agate information through this structured representation, enabling whole-program, per-instruction,
and per-variable classiﬁcation tasks. PROGRAML is a compiler-independent representation with
support currently for LLVM and XLA IRs.
PROGRAML provides a general-purpose program representation that equips learnable models to
perform the types of program analysis that are fundamental to optimization. To this end, we eval-
uate the performance of our approach ﬁrst on a suite of traditional compiler analysis tasks: control
ﬂow reachability, dominator trees, data dependencies, variable liveness, and common subexpres-
sion detection. On a benchmark dataset of 250k LLVM-IR ﬁles covering six source programming
languages, PROGRAML achieves an average 94.0 F1 score, signiﬁcantly outperforming the state-
of-the-art approaches. We then apply our approach to two high-level tasks — heterogeneous device
mapping and program classiﬁcation — setting new state-of-the-art performance in both.

1

Introduction

The landscape of computing ecosystems is becoming increasingly complex: multi-core and many-core processors,
heterogeneous systems, distributed and cloud platforms. Manually extracting performance and energy beneﬁts from
systems like these is beyond the capabilities of most programmers. In such an environment, high quality optimization
heuristics are not just desirable, they are required. Despite this, good optimization heuristics are hard to come by.

∗Both authors contributed equally

 
 
 
 
 
 
PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Figure 1: Our proposed approach for compiler analyses driven by graph-based deep learning. The PROGRAML
representation is derived from a compiler’s IR and serves as input to Message Passing Neural Networks, which provide
optimization decisions in place of traditional handwritten heuristics.

Designing and tuning optimization heuristics takes time, effort, and resources. To make things worse, this is a
Sisyphean task: even minor changes in a development toolchain might require retuning the heuristic; major changes
in the software or the hardware usually require freshly designed heuristics. Machine learning offers to liberate us from
this cycle by replacing fragile hand-tuned optimization heuristics with models that are inferred automatically from real
performance data [1, 2]. Typically, programs are represented using a sequence of numerical features which are derived
from programs using ad-hoc analysis, but such approaches fail to capture the rich semantic structure of programs,
limiting the ability for models to reason about program behavior. This is easy to see in traditional machine learned
approaches, where the designer explicitly chooses a program representation based only on a few properties deemed
important. Such representations prevent models from reproducing the reasoning of even basic compiler analyses and
that, in turn, limits their ability to make good optimization decisions.

Recent deep learning approaches that work directly on code [3] are limited, both in the way they represent the inputs
and in the way they process them. Representations based on source code and its direct artifacts (e.g. AST) put
unnecessary emphasis on naming and stylistic choices that might or might not correlate with the functionality of the
code [4, 5, 6]. Current IR-based approaches [7, 8, 9] use compilation to remove such noise but in the process they omit
important information about the program.

In both cases, the model is expected to reason about the ﬂow of information in the program using representations that
do not encode this information clearly. If this was not difﬁcult enough already, processing the code representations se-
quentially, as most existing approaches do, makes it practically impossible. Related statements can easily be separated
by hundreds of lines of irrelevant code in sequential representations. Due to vanishing gradients [10] and catastrophic
forgetting [11], neural networks are unlikely to even notice such very long range dependencies.

In this paper, we propose overcoming this limitation by making the program’s control, data, and call dependencies
a central part of the program’s representation and a primary consideration when processing it. We achieve this by
seeing the program as a graph, in which individual statements are connected to other statements through relational
dependencies. The latent representation of each statement is then a function of not just the statement itself but the
latent representations of its graph neighborhood. Each statement and data element in the program is understood only
in the context of the statements interacting with it. In contrast to prior sequential learning systems for code, this rep-
resentation closely resembles the intermediate representations used by compilers, and the propagation of information
through these graphs mimics the behavior of typical iterative data-ﬂow analyses.

Techniques for learning over graphs have recently been proposed and have shown promise in a number of do-
mains [12, 13, 14]. Our intent with this work is to extend these approaches to the domain of program analysis and
provide a systematic evaluation of their suitability and limits for compiler tasks. With a graph-based approach, we are
able to automatically learn established compiler analyses that rely on control and data ﬂow and do it far better than
existing code modeling approaches. Downstream tasks built on top of such graph models can then natively incorporate
approximate compiler analyses into their decision making, leading to superior and more powerful models.

2

CompilerInput ProgramIntermediate RepresentationProGraML RepresentationMessage Passing Neural NetworksOptimization PassesOutput ExecutablePROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

1.1 Contributions

We make the following contributions:

• A portable, language-agnostic graph representation of programs derived from compiler intermediate represen-
tations (IR), and machine learning models for relational reasoning about the control, data, and call relations
in programs1. The PROGRAML representation is compiler-angostic, with support currently for LLVM and
XLA IRs, and is the ﬁrst to summarize instructions, operand types, and operand order.

• As a benchmark for our approach, we pose a suite of established compiler analysis tasks as supervised ma-
chine learning problems, comparing the performance of models using PROGRAML against state-of-the-art
code representations. Succeeding at these benchmark tasks requires the ability to model control- and data-
ﬂow, function boundaries, instruction types, and the type and order of operands. On a large dataset of over
250k LLVM-IRs taken from real-world software projects, our approach achieves an average 94.0 F1 score
(98.9 precision, 92.0 recall), a 3.44× improvement over the state-of-the-art.

• We demonstrate the efﬁcacy of our approach on two challenging downstream compiler tasks: heterogeneous
device mapping, and program classiﬁcation. Our results in heterogeneous device mapping show the supe-
riority of graph based approaches.
In an ablation study in program classiﬁcation, we show a signiﬁcant
performance improvement due both to the rich structure of the PROGRAML representation and the message
passing neural network approach. Our approach reaches an accuracy of 96.22% in program classiﬁcation, a
1.27× improvement over the state-of-the-art.

2 Motivation

Machine learning promises signiﬁcant beneﬁts for simplifying and improving the construction of optimization heuris-
tics by replacing fragile and expensive hand-tuned heuristics with data-driven statistical modeling. For this goal to
be realized, we require machine learning systems capable of reasoning about program semantics. Despite tremen-
dous gains in recent years, state-of-the-art approaches for learning on code are not sufﬁciently powerful to replicate
the analysis tasks that are fundamental to compilers. The crux of the problem lies in two central aspects of machine
learning: input representation and model algorithmic complexity.

(I) Input Representation To be processed by a Neural Network (NN), code inputs may be encoded either directly
from a source language, by way of AST analysis, or using an IR. Examples of each abound in the literature [3, 15, 2].
One state-of-the-art encoder, code2vec [4], uses AST paths to embed programs. code2vec proves highly effective
at software engineering tasks such as algorithm classiﬁcation, where the code was written by humans. However, as
shown in Figure 2a, the trained representation can put more weight on names rather than code structure, where minute
modiﬁcations completely change classiﬁcation outcome. There are many beneﬁts to such a representation, including
smart pasting and automated refactoring. However, when analyzing code for optimization, identiﬁer names are rarely
of use, whereas structure and semantics should be the primary consideration. For example, in choosing to represent
code at the AST level, the code2vec representation does not capture the data or control relations between statements.

An alternate approach which emphasizes semantics is Neural Code Comprehension [7], where an encoder uses Con-
textual Flow Graphs (XFG) built from LLVM-IR statements to create inputs for neural networks. The XFG combines
partial data- and control-ﬂow to represent the context of an individual statement. The statements are then mapped to
latent-space representations using their neighbors in that graph. However, in partially combining DFGs and CFGs, the
XFG representation omits important information such as order of instruction operands (as shown in Figure 2b), and
the representation fails to capture execution order, critical for many optimization tasks.

A recent LLVM-IR representation uses Control and Data Flow Graphs (CDFG) [9]. This representation makes the
control and data relations between instructions explicit, but uses only instruction opcodes to compute latent represen-
tations. This omits information about programs that may be critical for optimization, such as data types, the presence
of variables and constants, and the ordering of operands, shown in Figure 2c.

Each of the three methods of encoding programs as inputs to neural networks omit information that is vital for compiler
analysis. This hinders the ability of machine learning models to reason about optimizations and their impact on
program behavior. To address these shortcomings we require an input representation that captures all parts of a
program’s semantics that are required for performing such analyses.

1Code and datasets available at https://chriscummins.cc/ProGraML

3

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

(a) code2vec [4].

(b) XFG [7].

(c) CDFG [9].

Figure 2: Limitations in state-of-the-art learnable code representations. In (a) the model over-emphasizes identiﬁer
names such that the same algorithm produces three different classiﬁcations by changing the name of a function. The
data-ﬂow representation of (b) does not capture operand order, such that non-commutative statements such as division
are indistinguishable. In (c) control and data relations are captured, but both type information and operand order are
omitted. Our approach is insensitive to identiﬁer names and preserves operand order and type information.

(II) Model Complexity The range of core operators in deep learning on code is largely conﬁned to recurrent units
(e.g. RNN, LSTM, GRU) on sequences. This poses limitations on the representation space of any such network’s
outputs. Take, for instance, dominator tree construction. An LSTM iterating forward over input code will not be
able to solve the task by deﬁnition, as statements needs to be analyzed backwards with respect to dependencies. Yet,
the neural network only maintains O(1) memory capacity w.r.t. program length. Iterating in the opposite direction,
in addition to being problem-specialized, would still not sufﬁce, as dependencies may “vanish” in the corresponding
gradients if dependent statements are far away from each other, such as in the presence of diverging control ﬂow.

One way to increase the algorithmic complexity of the model is by allowing it to increase the number of code tokens
that are processed simultaneously during inference. This approach, commonly used in literature by Transformer
Networks [16], use preceding tokens (unidirectional encoding) or preceding and subsequent tokens (bidirectional
encoding) to learn attention matrices. Such matrices “focus” the network on certain subsets of tokens, skipping
others. However, this approach scales quadratically in memory and computation with the number of tokens.

Unlike in natural language text, the dependency structure of code is made explicit during compilation. We can thus
employ domain-speciﬁc knowledge to construct the attention matrices in a scalable manner, using a graph representa-
tion of the tokens with dependencies as edges. A graph representation not only enables meaningful attention learning,
but also facilitates propagating information across the graph in a manner similar to typical compiler analyses. Within
the same step, a recurrent unit generates O(1) activations, whereas a graph NN generates O(|V |).

To demonstrate this expressive power, let us consider control-ﬂow reachability analysis as a learning task. The goal
of the model is to tag statements that are reachable from one or more given tagged statements. With a sequential
LSTM, the model would have to be trained to memorize nodes along some linear order of the given program. With
an unbounded number of nodes to track and variably-sized regions to skip, the task becomes infeasible. A message-
passing graph neural network, in contrast, needs only to learn to pass a message forward in the case of an existing
control-ﬂow edge between two nodes, essentially learning an identity operation over control-ﬂow edges and zero on
others.

In this work, we overcome the above limitations of prior model and representation approaches, leveraging the graph
structure of IR code, taking path analysis and the semantics of individual statements into account.

3 A Graphical Program Representation for Analysis and Optimization

For machine learning to successfully reason over programs, a suitable input representation must be used. This section
presents PROGRAML, a novel program representation that closely matches the representations used traditionally
within compilers and can be processed natively by machine learning models. Unlike prior approaches that rely on
hand-engineered feature extractors [1, 2] or which are closely tied to the syntax of the target program language [17],
our approach captures whole-program control, data, and call relations and is both task- and language-agnostic.

4

intf(intn) {if(n == 0) return1;elsereturnn * f(n-1);}factorial50.93%fact19.15%pad8.92%sinc77.78%times3.89%isPowerOfTwo3.36%intf(intx) {if(x== 0) return1;elsereturnx* f(x-1);}intfibonacci(intn) {if(n == 0) return1;elsereturnn * fibonacci(n-1);}fibonacci99.09%testRun0.75%Iter0.07%%1%2%30.0fdiv floatfdiv floatsextloadloadgetelementptrPROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

(a) Compiler intermediate representation.

(b) Construct control ﬂow.

(c) Add data ﬂow for variables and constants.

(d) Add call ﬂow for call sites.

Figure 3: Construction of a PROGRAML representation for a simple C implementation of recursive Fibonacci using
LLVM-IR. The input program is passed through the compiler front end to produce an intermediate representation (a).
A full ﬂow graph is constructed from the IR statements and control ﬂow edges inserted (b). Vertices for variables and
constant values are added and data-ﬂow edges inserted (c). Finally, call sites are extracted and call edges inserted from
call sites to function entry statements, and from function exit vertices to call sites (d). All edges are positional, for
clarity we have omitted position labels where not required.

5

define i32 @Fib(i32) #0 { switch i32 %0, label %3 [   i32 0, label %9   i32 1, label %2 ]; <label>:2: br label %9; <label>:3: %4 = add nsw i32 %0, -1 %5 = tail call i32 @Fib(i32 %4) %6 = add nsw i32 %0, -2 %7 = tail call i32 @Fib(i32 %6) %8 = add nsw i32 %7, %5 ret i32 %8; <label>:9: %10 = phi i32 [ 1, %2 ], [ %0, %1 ] ret i32 %10}int Fib(int x) { switch (x) {  case 0:    return 0;  case 1:   return 1;  default:   return Fib(x - 1)         + Fib(x - 2); }}Input programCompiler IRswitchadd nswbrtail calladd nswtail calladd nswretret012switchadd nswbrtail calladd nswtail calladd nswretreti32i32i32i32i32i32i32 0i32 -1i32 -2012010110i32 101i32 1switchadd nswbrtail calladd nswtail calladd nswret[external]i32i32i32i32i32i32i32 0i32 -1i32 -2retPROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

3.1 Overview

The PROGRAML representation of a compiler IR serves as the union between a call graph, control-ﬂow graph, and
data-ﬂow graph. We represent programs as directed multigraphs where statements, identiﬁers, and immediate values
are vertices, and relations between vertices are edges. Edges are typed to differentiate control-, data-, and call-ﬂow.
Additionally, we augment edges with a positional label to encode the order of operands for statements, and to dif-
ferentiate between divergent branches in control-ﬂow. The PROGRAML representation is processed natively by our
machine learning models, described in Section 4.

3.2 Graph Construction

We construct a PROGRAML graph G = (V, E) by traversing a compiler IR. Graph construction is divided into
three stages: control-ﬂow, data-ﬂow, and call-ﬂow, though in practice the three stages can be combined in a single
O(|V | + |E|) pass. The representation is compiler-agnostic, adding support for a new compiler requires only a parser
for the IR. Currently we support LLVM [18] and XLA HLO [19] IRs. Figure 3 shows the graph construction approach.

(I) Control Flow We construct a full-ﬂow graph from an IR by inserting a graph vertex for each instruction and
control ﬂow edges between them, as shown in Figure 3b. All control edges are augmented with a numeric position
label using an ascending sequence based on their order in the list of a vertex’s outgoing control edges. For instructions
with a single control successor, the position of the control edge is 0. For a branching statement with n successor
statements, the control edge positions are in the range 0 ≤ epos ≤ n. We do not need to encode the source function
or basic block [18] of instructions as this information is captured implicitly in structure of the graph; basic blocks are
regions of instructions with a single entry and exit control edge, and functions are disconnected subgraphs.

(II) Data Flow We introduce additional graph vertices for constant values and variables, shown in Figure 3c. Data-
ﬂow edges are added to capture the relation from constants and variables to the instructions that use them as operands,
and instructions to produced variables. Each unique variable and constant is a vertex, which implicitly models the
scope of variables, and unlike the tokenized representations of prior machine learning works, variables in different
scopes always map to distinct vertices and can thus be discerned. Similarly, constant values with the same textual
representation in source code (such as the number 3.14 with float and double precision types) are distinguishable
in our representation. As with control edges, data edges have a position label which encodes the order of operands for
instructions. The latent representation of an IR statement is thus a function of the vertex representing the instruction
and the vertices of any operand variables or constants, modulated by their order in the list of operands.

(III) Call Flow Control edges do not span functions, such that an IR with functions F produces |F | disconnected
subgraphs (the same is not true for data edges which may cross function boundaries, for example in the case of an
global constant which is used within multiple functions of a program). Instead, the relation between a statement which
calls a function and the called function is captured through call edges, shown in Figure 3d. An outgoing call edge is
inserted from the calling statement to the entry statement of a function. Return call edges are added from all terminal
statements of a function to the calling statement. Call edges do not use position labels as there is no ordering to be
imposed between the call sites of a function. For IRs which support external linkage, an additional vertex is created
representing an external callsite and connected to all externally visible functions. Similarly, if a call site references
a function not deﬁned in the current IR, a dummy function deﬁnition is created consisting of a pair of entry and exit
instruction vertices, and connected normally through call edges. A single dummy function deﬁnition is created for
each externally deﬁned function and shared across all call sites in the current IR.

3.3 Comparison to Other Representations

As an input for machine learning, what distinguishes PROGRAML from prior works is its close proximity to the
structured representations traditionally used within compilers for program analysis and optimization. Speciﬁcally, it
is distinct from prior machine learning representations in three key areas:

1. as an IR-level representation, it is independent of the source language and accompanying variances such as

code style and identiﬁer naming that affect source-level representations [4, 20];

2. by representing programs as graphs with explicit control, data, and call edge types PROGRAML captures a

greater range of intra-program relations than prior graph representations [7, 17, 21];

3. and in trading sequential for graph representations, we do not sacriﬁce local sequence order, such as the
ordering of diverging control ﬂow or the ordering of operands that is lost in prior representations [7, 9].

6

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Source Languages

Representation

AST Paths [22] C#, Java, JavaScript, Python AST

CDFG [9] OpenCL

code2vec [4]

Java

DeepTune [23] OpenCL
DeepTune-IR[24] OpenCL

DeepTyper [25]

JavaScript

inst2vec [7] C++, OpenCL

IR Graph
AST
Token Sequence
IR Token Sequence
Token Sequence
IR Graph

PROGRAML

C, C++, Fortran, Haskell,
OpenCL, Swift

IR Graph

Flow-
sensitive?

Position-
sensitive?

Value-
sensitive?
(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

Table 1: Taxonomy of recent code representations for machine learning. We classify approaches based on the type
of representation used and the sensitivity to three categories: {control/data/call}-ﬂow, operand positions, and operand
values. Prior approaches require a trade-off in representational power, e.g. substituting a position-sensitive token
sequence for a ﬂow-sensitive graph. PROGRAML is the ﬁrst representation to span all categories.

Table 1 provides a comparison of PROGRAML to several recent learned representations of code.

4 Graph-based Machine Learning

We formulate our system in a Message Passing Neural Network (MPNN) framework [14, 12] and implement a single
uniﬁed model for all our experiments. Our design mimics the transfer functions and meet operators of classical
iterative dataﬂow analysis [26, 27], replacing the rule-based implementations with deep learning analogues (message
and update functions). Those can be specialized through training to solve a diverse set of problems without human
intervention or algorithm design.

4.1 Overview

We learn over PROGRAML representations of compiler IRs by mapping graph vertices to an initial state vector using
an embedding. The vertex states are updated iteratively in a sequence of message passing steps, where at each step
a new vertex state is computed as a function of its previous state and the state of its neighboring vertices. Separate
functions are learned to update vertex neighbors based on their relation type, be it control, data or call, and reverse
edges enable backwards propagation of information. After repeating this process of updating vertex states for a ﬁxed
number of iterations a readout function is used to aggregate the vertex representations to a single graph-level vector or
set of vertex-level vectors.

4.2 Model Design

The PROGRAML model takes as input a directed graph with additional information as presented in Section 3 and
consists of three logical phases: input encoding, message propagation, and result readout.

(I) Input Encoding Starting from the augmented graph representation G = (V, E) introduced in Section 3, we
capture the semantics of the program graph vertices by mapping every instruction, constant, and variable vertex v ∈ V
v ∈ Rd by lookup in a ﬁxed size embedding table CIR ∈ Rn×d. The mapping from vertex
to a vector representation h0
to embedding vector f : v (cid:44)→ h0
v ∈ CIR must be deﬁned for each IR, though the embeddings themselves can be
learned during training.

For LLVM-IR we extend the inst2vec [7] vocabulary, which represents 8,566 statements derived from a large corpus
of LLVM-IR using 200-dimensional embeddings. Since the space of statements is unbounded, inst2vec uses a normal-
ization process to inline type deﬁnitions and strip identiﬁers and immediate values, depicted in Figure 4. An inst2vec
representation combines an instruction and its operands into a single token, so we augment the vocabulary with ID and
VAL tokens to represent variable and constant value vertices, respectively.

In expressing a statement as a combination of an instruction and its operands, our data-driven approach trades a certain
amount of semantic resolution against good coverage of the vocabulary by the available datasets. The long tail of the
distribution of statements jointly maps onto a special UNKNOWN token vector in the vocabulary. In future work will
simplify the LLVM-IR statement encoding by constructing separate vocabularies for instructions and operand types,

7

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Figure 4: Normalizing an LLVM-IR statement (a) by inlining type deﬁnitions and stripping identiﬁers using
inst2vec [7] (b). A normalized statement is used as the key for an embedding table lookups to produce the ini-
tial vector representation of a vertex. (c) shows the statement as contextualized in a PROGRAML graph, where the
operand variables and constants are data elements, differentiated by their position. Vertex labels represent embedding
table keys. In this example, the statement is represented using ﬁve vertices and encoded with three unique embeddings.

increasing the expressiveness of the encoding by allowing a larger combination of instruction and operands to be
represented. Input vertex-selectors, encoded as binary one-hot vectors, are used to mark the starting point for certain
analyses and are concatenated to the vertex embeddings. Other global input features are used as auxiliary input features
at readout time in step (III), where required.

(II) Message Propagation Each iteration step is divided into a message propagation step followed by a vertex state
v ∈ Rd from its undirected
update. During message propagation, each vertex in the graph collects learned messages mt
neighbors:

mt

v =

(cid:88)

w∈N (v)

Atype(ewv) (ht−1

w (cid:12) POS(ewv))

(1)

where (cid:12) denotes the Hadamard product and ewv ∈ E is the typed edge between vertex w and v. In order to allow
for reverse-propagation of messages, which is necessary for backward compiler analyses, we add backward edges for
each edge in the graph. For backward edges we introduce separate parameters following Li et al. [12] to enable the
network to distinguish between an edge and its backward sibling. During message propagation we scale the source
v with POS(·) ∈ Rd, which is a constant sinusoidal position embedding [16, 28] that encodes the argument
states ht−1
order of incoming (and outgoing) edges. This information is necessary for the network to distinguish non-commutative
operations such as division.
The collected messages are subsequently used to update the vertex states hv ∈ Rd in parallel according to an update
function. In all our experiments, we employ a Gated Recurrent Unit (GRU) [29] as our update function:

Step (II) is iterated T times to extract vertex representations that are contextualized with respect to the given graph
structure.

v = GRU(ht−1
ht

v

, mt
v)

(2)

(III) Result Readout We support whole program classiﬁcation, per-statement classiﬁcation, or per-identiﬁer classi-
ﬁcation by employing different readout heads on top of the iterated feature extraction: for graph-level classiﬁcation
we deﬁne a set function RG({hT
v}v∈V ) that maps to class-scores, while for vertex-level inference, we separately
map the extracted node features hT

v to probabilities Rv(hT

v) in parallel:

v , h0

v , h0
v) = σ (cid:0)i(hT
(cid:88)

Rv(hT
v , h0

v , h0
v}v∈V ) =

RG({hT

v , h0
Rv(hT

v)(cid:1) · j(hT
v )
v , h0
v)

(3)

(4)

where i(·) and j(·) are feed forward Neural Networks and σ(·) is the sigmoid activation function. In the case where
auxiliary graph-level features are available, those are concatenated to the readout values and fed through another feed
forward Neural Network that employs Batch Normalization [30] to allow for vastly different feature scales.

v∈V

8

val%id = getelementptr inbounds {i32},    {i32}* %id, i64 %int, i32 %int%id%idval0120%id = getelementptr inbounds {i32},    {i32}* %id, i64 %int, i32 %int%struct.X = type { i32 }%3 = getelementptr inbounds %struct.X,     %struct.X* %0, i64 0, i32 0(a) IR Statement(b) inst2vec-normalized(c) ProGraML representationPROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

5 Experimental Methodology

We evaluate the effectiveness of our approach in three case studies. In the ﬁrst, we apply our methodology to a suite of
established compiler analysis tasks. These serve as demonstrations of the representational power of our approach and
highlight the limitations both in prior machine learning approaches and in current MPNNs. The second case study then
applies the approach to the challenging real-world optimization task of heterogeneous device mapping, comparing the
performance of our model against state-of-the-art machine learning-based approaches. Finally, we apply our approach
to the task of classifying algorithms from unlabelled implementations. This section describes the methodology used
to construct these experiments: the datasets used, the model parameters, and training regimes.

5.1 Case Study A: Compiler Analyses

We construct a benchmark suite of traditional compiler analysis tasks to evaluate the representational power of our
approach. We chose a diverse set of tasks to capture a mixture of both forward and backward analyses, and control-,
data-, and procedure-sensitive analyses. Our goal is not to suggest that machine learning should replace the existing
implementations of these standard algorithms which can be found in any compiler, but rather, if a machine learning
system is not capable of producing these analyses, it stands to reason that performance on downstream tasks which
depend on these analyses will suffer.

5.1.1 Benchmark Analyses

We selected ﬁve traditional compiler analyses to use as benchmarks for evaluating the representational power of our
approach.

(I) Reachability Control reachability is a fundamental compiler analysis which determines the set of statements that
can be reached from a particular starting statement. Given succ(n), which returns the control successors of statement
n, the set of reachable statements starting at root n can be found using forward analysis:

Reachable(n) = {n} ∪





(cid:91)

s∈succ(n)



Reachable(s)



(5)

(II) Dominator trees Statement n dominates statement m if every control ﬂow path to m passes through n. A
dominator tree is the set of all nodes that dominate the statment at a particular program point. Like reachability, this
analysis only requires propagation of control ﬂow, but unlike reachability, dominator trees are typically constructed
using backward analysis [31, 32]:

Dominators(n) = {n} ∪





(cid:92)

p∈pred(n)

Dominators(p)





(6)

Where pred(n) which returns the control predecessors of statement n.

(III) Live-out variables A variable a is live-out of statement n if there exists some control successor of n that uses
a. Given uses(n), which returns the operand variables of n, and defs(n), which returns deﬁned variables, the live-out
variables can be computed forwards using:

LiveOut(n) =

(cid:91)

uses(s) ∪ (cid:0)LiveOut(s) − defs(s)(cid:1)

(7)

s∈succ(n)

(IV) Data dependencies The data dependencies of statement n is the set of predecessor statements that must be
evaluated to produce the operands of n. Computing data dependencies requires data sensitivity and is computed
backwards:

DataDep(n) = defs(n) ∪





(cid:91)

p∈defs(n)



DataDep(p)



(8)

9

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Source
language

Domain

IR ﬁles

IR lines

Avg.
vertices / IR

Avg. edges /
IR

BLAS 3.8.0

Linux 4.19

Fortran

Scientiﬁc Computing

300

345,613

C

Operating Systems

13,851

41,332,089

OpenCL Benchmarks [23] OpenCL

Benchmarks

OpenCV 3.4.0

POJ-104 [33]

Tensorﬂow [34]

GitHub

Total

C++

C++

C++

C

Haskell

OpenCL

Swift

—

256

400

149,779

1,168,758

Computer Vision

Standard Algorithms

182,815

64,518,837

Machine learning

1,584

8,444,443

Various

42,880

89,356,570

1,371

6,745,312

5,188

10,472,388

1,783

205,140

1,664

1,857

1,027

3,761

312

5,786

927

4,705

2,299

134

3,276

3,760

1,970

7,442

569

11,482

1,794

7,518

5,132

371

—

250,428

222,738,929

153,426,059

294,685,614

Table 2: The ten sources of LLVM-IR used to produce datasets for evaluating data ﬂow analyses. Our corpus comprises
six programming languages from functional to imperative, high-level to accelerators. The software covers a broad
range of disciplines from compilers and operating systems to traditional benchmarks, machine learning systems, and
unclassiﬁed code downloaded from popular open source repositories.

Where defs(n) returns the statements that produce operands of n.

(V) Global Common Subexpressions The identiﬁcation of common subexpressions is an important analysis for
optimization. For compiler IRs we deﬁne a subexpression as a statement and its operands, ordered by either their
position (for non-commutative operations), or lexicographically (for commutative operations). We thus formulate the
common subexpression problem as, given a statement (which forms part of a subexpression), label any other statements
in the program which compute the same subexpression. This is an inter-procedural analysis, though operands must
obey their scope. Common subexpressions are typically identiﬁed using available expression analysis:

Avail(n) = uses(n) ∪





(cid:92)

p∈pred(n)



Avail(p)

 − defs(n)

Where uses(n) return the expressions used by statement n, and defs(n) returns the expressions deﬁned by n.

5.1.2 Datasets

We assembled a large corpus of real-world LLVM-IRs from a variety of sources, summarized in Table 2. We selected
popular open source software projects that cover a diverse range of domains and disciplines, augmented by uncatego-
rized code mined from popular GitHub projects using the methodology described by Cummins et al. [20]. Our corpus
comprises a range of source languages (C, C++, Fortran, Haskell, OpenCL, and Swift) and exceeds 250k ﬁles. We
de-duplicated the corpus ﬁrst at the source level, then again after lowering to LLVM-IR. Lowering from source to IR
was performed using the inst2vec methodology [7], in which an optimization level is chosen randomly per-ﬁle.

We generated a single PROGRAML representation for each of the LLVM-IRs, taking an average of 31 ms per IR. Our
corpus of unlabeled graphs totals 153M vertices and 295M edges, with an average of 616 vertices per graph with 472
unique vertex representations, and 1183 edges with a maximum edge position of 355 (a large switch statement found
in a Tensorﬂow compute kernel).

We produced labeled graphs from the unlabeled corpus by computing ground truth labels for each of the analysis tasks
using a traditional analysis implementation. For each of the ﬁve tasks, and for every unlabeled graph in the corpus, we
produce n labeled graphs by selecting unique source vertices v0 ∈ V , where n is proportional to the size of the graph:
(cid:18)(cid:24) |V |
10

n = min

, 10

(9)

(cid:19)

(cid:25)

10

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

(a) REACHABILITY.

(b) DOMTREE.

(c) DATADEP.

(d) LIVENESS.

(e) SUBEXPRESSIONS.

Figure 5: Example input-output graphs for each of the ﬁve benchmark compiler analyses. A single vertex is randomly
selected from the input graph to represent the starting program for computing the analysis results, indicated using the
vertex selector. The output graphs contain binary labels for each of the graph vertices after the analysis has completed.
As a supervised classiﬁcation task, the goal of the model is to predict the output vertex labels given the input graph.
These small graphs are for illustrative purposes, the LLVM-IR graphs in our real-world corpus contain an average 616
vertices and 1,183 edges.

Each instance in the datasets consists of an input graph in which the source vertex is indicated using the vertex selector,
and an output graph with the ground truth labels used for training or for evaluating the accuracy of model predictions.
Figure 5 illustrates an example input-output instance for each of the ﬁve tasks.

We divided the datasets randomly using a 3:1:1 ratio for training, validation, and test instances. The same random
allocation of instances was used for each of the ﬁve tasks. Where multiple labeled graphs were derived from a single
IR, instances derived from the same IR were allocated to the same split.

5.1.3 Models

LSTM Baseline As no prior work offers the expressiveness required to perform the per-statement and per-variable
classiﬁcation required for these analysis tasks, we extended DeepTune [23], a state-of-the-art deep learning framework
for whole-program classiﬁcation, to enable per-statement classiﬁcation. In DeepTune, an OpenCL program is ﬁrst
tokenized and mapped to a sequence of embedding vectors which are then processed through a sequential LSTM
model. The ﬁnal state of the LSTM is optionally concatenated with program-level features, then fed through a fully
connected neural network to produce a program-level classiﬁcation.

Figure 6 shows how we extended this approach for statement-level classiﬁcation of LLVM-IR. We ﬁrst replaced the
OpenCL tokenizer using one derived from LLVM IR, resulting in a 179-element vocabulary. To adapt the approach
for performing statement-level classiﬁcation, we group embedding vectors by their source statement before using
element-wise summation to merge embedding vectors.

We use the same models parameters as in the original work [23] — 64-dimensional embedding vectors trained jointly,
with 64 sequences per batch, padded and truncated to the same length. As LLVM IR is more verbose than OpenCL, the
sequences are signiﬁcantly longer requiring greater device memory during training and inference. This is a common
issue with recurrent neural networks, as sufﬁcient memory is required to store both the intermediate results of the
forward pass and the gradients during training [35]. We found that a sequence length of 5,000 was the maximum that
our experimental platform allowed. Where sequences exceed this length, they are truncated, and the model outputs
padded with zeros to match the expected shape.

ProGraML We use the model design outlined in Section 4 for each of the compiler analysis tasks. While we use
the vocabulary of inst2vec, we do not use the pre-trained embeddings, instead initializing the embeddings randomly
and training jointly.

Message Passing Neural Networks typically use a small number of propagation steps out of practical consideration
for time and space efﬁciency [14, 12], and address problems on smaller graphs than used in this work [17]. For a
large class of monotone data ﬂow analysis problems, however, it is known that up to d(G) + 3 passes over the graph
are required, where d(G) is the loop connectedness of G [27, 26]. The loop connectedness captures the notion of
loop-nesting depth in a program and is therefore a program-dependent, but generally unbounded quantity1.

We address this challenge with PROGRAML by iterating for a ﬁxed number T of message passing steps for training
and inference and excluding from the test set graphs for which a traditional implementation of the analysis task requires
greater than T iterations to solve. For the experiments in this work we set T = 30, leading to 12.56% of the graphs
in the corpus to be excluded across the ﬁve tasks. For fairness, we also excluded these graphs from evaluation of the
LSTM baseline.

1Given any depth-ﬁrst spanning tree (DFST) of G, backward edges are deﬁned as edges in G that connect a node to one of its

ancestors in the DFST and d(G) is the maximum number of backwards edges in any acyclic path in G.

11

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Figure 6: Extending DeepTune [23] to perform per-statement classiﬁcation of an LLVM-IR. In the original work,
the latent representation of the entire program token sequence was used for program-level classiﬁcation, we enable
classiﬁcation of arbitrary token groupings so that we can perform statement-level classiﬁcation of a program. In the
above diagram, + denotes element-wise summation, and (cid:95) denotes vector concatenation.

5.1.4 Training Details and Parameters

All models were trained in an end-to-end fashion with the Adam optimizer [36] using the default conﬁguration and
learning rate 0.001. We trained the models on a NVIDIA GTX 1080 GPU-equipped machine in increments of 10k
graphs, testing on a 20k validation set at each checkpoint. Training terminated after six hours, or if accuracy on the
validation set reached 99.99%. After training completed we selected the checkpoint with the highest accuracy on the
validation set to use for testing.

5.2 Case Study B: Heterogeneous Device Mapping

We apply our methodology to the challenging domain of heterogeneous device mapping (DEVMAP). Given an
OpenCL kernel and a choice of two devices to run it on (CPU or GPU), the DEVMAP task is to predict the de-
vice which will provide the best performance. We chose this problem as it has received signiﬁcant prior attention, with
previous approaches using both hand-engineered features [37] and sequential models [23, 7].

12

%15 = load i32, i32* %nian, align 4, !tbaa !1%rem38 = and i32 %15, 3%rem11 = srem i32 %15, 100  LLVM-IRTokenizedEmbeddings……Grouped Embeddings5000vocabulary indices,padded & truncated5000 x 64embedding vectorsnum_statements x 64Grouped embedding vectors++++num_segments1-hot “selector” vectorAuxiliary Inputsnum_segments x (64+2)concatenated embedding and selector vectorsModel inputsLSTMnum_segments x 21-hot outputsModel outputsFully-connected⌢t1t2t3tnum_segments ⌢⌢⌢…PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

5.2.1 Datasets

We used the dataset of [23], which provides labeled CPU/GPU instances for 256 OpenCL kernels sourced from seven
benchmark suites on two combinations of CPU/GPU pairs. The AMD set uses an Intel Core i7-3820 CPU and AMD
Tahiti 7970 GPU; the NVIDIA set uses an Intel Core i7-3820 CPU and an NVIDIA GTX 970 GPU. Each dataset
consists of 680 labeled instances derived from the 256 unique kernels by varying dynamic inputs.

5.2.2 Models

We compare ProGraML with four approaches: First, with a static baseline that predicts the mode device of the dataset
distribution. Second, with DeepTune [23], which is a sequential LSTM model at the OpenCL source level. Third, to
isolate the impact of transitioning from OpenCL source to LLVM-IR, we evaluate against a new DeepTuneIR model,
which adapts DeepTune to using tokenized sequences of LLVM-IR as input instead of OpenCL tokens, using the
vocabulary described in Section 5.1.3. Finally, we compare against the state-of-the-art approach NCC [7], which
replaces the OpenCL tokenizer with a sequence of 200-dimensional embeddings, pre-trained on a large corpus of
LLVM-IR using a skip-gram model.

5.2.3 Training Details and Parameters

All neural networks are regularized with Dropout [38] for generalization and Batch Normalization [30] in order to
be uniformly applicable to vastly different scales of auxiliary input features. We used 10-fold cross-validation with
rotating 80/10/10 splits by training on 80% of the data and selecting the model with the highest validation accuracy,
setting aside 1/10th of the training data to use for validation. We trained each model for 100 epochs and selected the
epoch with the greatest validation accuracy for testing.

5.3 Case Study C: Algorithm Classiﬁcation

In a third case study, we apply our approach to task of classifying algorithms. We use the POJ-104 [33] dataset.
It contains implementations of 104 different algorithms that were submitted to a judge system. All programs were
written by students in higher education. The dataset has around 500 samples per algorithm. We compile them with
different combinations of optimization ﬂags to generate a dataset of overall 240k samples. Approximately 10,000 ﬁles
are held out each as a development and test set.

5.3.1 Models

We compare with recently published tree-based convolutional neural networks (TBCNN) [33] and NCC [7], which
uses the same approach approach as described in Section 5.2.2 on this dataset. To further test the expressive power
of the graph-based representation against the tree-based (TBCNN) and sequential (NCC) prior work, we present
additional experiments: Graph-based baselines based on XFG [7] and a PROGRAML structure-only baseline.

To better understand the qualitative aspects of replacing a graph-based representation that captures program semantics
like Contextual Flow Graphs [7] (XFG) with the more complete PROGRAML representation, we adapted a GGNN [12]
to directly predict algorithm classes from XFG representations of the programs.
In contrast to this, Ben-Nun et
al. [7] used XFG only to generate statement contexts for use in skip-gram pre-training. Here, we lift this graphical
representation and make it accessible to a deep neural network directly, as opposed to the structureless sequential
approach in the original work (NCC).

Additionally, we include a structure-only baseline of our PROGRAML approach, where only the type of each node
(instruction, variable, or constant) is encoded, refraining completely from tokenizing statements. We think that algo-
rithm classiﬁcation is a problem that lends itself especially well to judging the power of the representation structure,
since most algorithms are well-deﬁned independent of implementation details such as datatypes.

To test the limits of the expressivity of PROGRAML, combine our representation with a powerful 10-layer Trans-
former [16] encoder model, adapted as a graph neural network for attributed graphs. We induce graph structure by
masking the attention scores in the self-attention layer with the adjacency matrix of the PROGRAML graphs. A new
space-efﬁcient sparse implementation allows processing of graphs with on the order of 105 vertices. Different edge
types are encoded by introducing separate key and value projection matrices into the self-attention layer [16, 39].

5.3.2 Training Details and Parameters

The GGNN models were trained with the AdamW [40] optimizer with learning rate 2.5 · 10−4, β1 = 0.9, β2 =
0.999, ε = 10−8 for 80 epochs. Dropout regularization is employed on the graph states with a rate of 0.2. The Trans-

13

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

Problem

Analysis type

Example optimization

Model

Precision

Recall

REACHABILITY

Forwards, control ﬂow
only

Dead code elimination

DOMTREE

DATADEP

LIVENESS

Backwards, control ﬂow
only

Global Code Motion

Backwards, control and
data ﬂow

Instruction scheduling

Backwards, control and
data ﬂow

Register allocation

SUBEXPRESSIONS

Forwards, statement and
operand values and
positions

Global Common
Subexpression
Elimination

Average

—

—

DeepTuneIR

ProGraML

DeepTuneIR

ProGraML

DeepTuneIR

ProGraML

DeepTuneIR

ProGraML

DeepTuneIR

ProGraML

DeepTuneIR

ProGraML

0.520

0.997

0.721

0.985

0.999

1.000

—

1.000

1.000

0.965

0.810

0.989

0.497

0.995

0.081

0.693

0.136

0.988

—

0.999

0.123

0.925

0.209

0.920

F1

0.504

0.996

0.138

0.781

0.236

0.993

—

0.999

0.214

0.930

0.273

0.940

Table 3: Benchmark compiler analyses results using two approaches: (a) DeepTuneIR, a sequential model adapted
to work at the LLVM-IR level for statement-level classiﬁcation, and (b) PROGRAML, our approach. The relational
representation signiﬁcantly outperforms a sequential approach across all problems. For the Global Common Subex-
pressions analysis, DeepTuneIR achieved a higher precision than PROGRAML by predicting only the root statement
as a component in a subexpression, avoiding false-positives.

former model uses the same hyperparameters as the GGNN. Additionally a batch size of 64, Dropout regularization
of 0.2 on the graph states and weight-sharing between adjacent pairs of layers is employed. The model dimension is
equal to the embedding size (200) and the hidden size of the feed-forward layers is 512. The self-attention layers use
5 heads [16]. Overall, the Transformer model has 5.6 million trainable parameters, around 1.7 million of which are in
the embedding layer.

6 Experimental Results

This section evaluates the performance and limitations of our approach for the three case studies described in Section 5,
and provides a comparison to state-of-the-art approaches. First we show that PROGRAML, unlike prior state-of-the-
art approaches to machine learning over code, is capable of replicating core compiler analysis tasks that are key to
optimization. Second, we improve upon prior approaches to the task of heterogeneous device mapping. Finally, we set
a new state of the art in the difﬁcult task of classifying program algorithms, and ablate the contribution of the structure
and content of the PROGRAML representation.

6.1 Case Study A: Compiler Analysis

Table 3 summarizes the performance of the PROGRAML approach when tasked with learning a suite of benchmark
compiler analysis, along with the performance of a state-of-the-art sequential approach, DeepTuneIR. As a binary
classiﬁcation task, compiler analyses display an extreme class imbalance as only a small fraction of a program graph
is typically relevant to computing the result set of an analysis. On our datasets, an accuracy of 96.6% can be achieved
by always predicting the negative class. For this reason we report only binary precision, recall, and F1 scores with
respect to the positive class.

As can be seen in Table 3, the relational representation of our approach, coupled with learning through iterative
message passing, yields models that far outperform the state-of-the-art sequential approaches to modeling code. The
grouping of program tokens by statement performed by DeepTuneIR offers a more restrictive classiﬁcation interface
than with PROGRAML (where data elements are also represented as graph vertices). As such, it is not able to perform

14

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

(a) DeepTuneIR

Figure 7: The F1 score of compiler analysis models on a 20k-graph validation set as a function of the number of
training graphs from 10k to 1M. Each model was given six hours to train on a machine with a GTX 1080 GPU, with
early termination if accuracy on the validation set reached 99.99%. We have applied a Gaussian ﬁlter (σ = 1) to aid
in visualizing the trends in each set of scores.

(b) PROGRAML

the per-variable classiﬁcation required for LIVENESS analysis1. For fairness, we exclude LIVENESS from LSTM
aggregate values in Table 3. Despite this, PROGRAML achieves an average 94.0 F1, versus 27.3 F1 for DeepTuneIR.

In many cases, the sequential model regresses to a classiﬁcation mode in which only the source vertex is labeled as
positive, yielding poor recall. The PROGRAML models achieve both high precision and high recall in all tasks except
DOMTREE, where recall is comparatively poor. When model performance is considered as a function of the number of
training graphs, as shown in Figure 7, we see that the performance of the PROGRAML models quickly convergences
towards near-perfect F1 score on a holdout validation set, except in the case of DOMTREE, where the model is still
improving at the end of training. This suggests estimating the transfer and meet operators of this backwards analysis
poses a greater challenge for the network, which may require further training.

Table 4 shows confusion matrices for the per-vertex classiﬁcation of PROGRAML models on the test set. While the
distribution of errors is balanced for REACHABILITY, in the case of DOMTREE and SUBEXPRESSIONS the ratio of
false negatives (T+P−) outweighs the false positives (T−P+), indicating that models favor under-approximating the
value sets of these analyses. This may be an artifact of training with such a large imbalance towards negatives (T−)
over positive (T+) class labels. In future work we will explore addressing this class imbalance by selecting multiple
root points on a graph for analysis simultaneously, thereby increasing the size of the value set to include multiple
(possibly overlapping) regions.

6.2 Case Study B: Heterogeneous Device Mapping

The performance of ProGraML and baseline models is shown in Table 5. We reused the pre-trained inst2vec-
embeddings for the NCC baseline that were published with the original work, however all models themselves have
been reimplemented to ensure fair comparison across different models under a uniﬁed evaluation regime and absolute
performance numbers can thus deviate from the original publications.

1Theoretically the same approach for grouping embeddings by statement could also be extended to group all statements by
variables, but this would require duplicating many statements in the input representation, increasing the length of the sequences far
beyond what we can practically process using a sequential model.

15

104105106#. training graphs (log)0.00.20.40.60.81.0F1 scoreReachabilityDomtreeLivenessDatadepSubexpressions104105106#. training graphs (log)0.00.20.40.60.81.0F1 scoreReachabilityDomtreeLivenessDatadepSubexpressionsPROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

P−

P+

P−

P+

P−

P+

P−

P+

P−

P+

T−

T+

96.64%

0.01%

0.01%

3.34%

T−

T+

95.94%

0.13%

1.70%

2.23%

T−

T+

98.80%

0.00%

0.00%

1.20 %

T−

T+

90.00%

0.00%

0.00%

9.99%

T−

T+

99.12%

0.02%

0.13%

0.73%

(a) REACHABILITY

(b) DOMTREE

(c) DATADEP

(d) LIVENESS

(e) SUBEXPRESSIONS

Table 4: Confusion matrices for compiler analyses using PROGRAML. Rows denote true negative (T−) and true
positive (T+), columns denote predicted negative (P−) and predicted positive (P+). The value of a cell is the ratio of
per-vertex model outputs of this type, e.g. T−P+ is the ratio of false positives.

Accuracy

Precision

Recall

F1

Accuracy

Precision Recall

F1

Static Mapping

58.8%

DeepTune [23]

71.9%

DeepTuneIR

NCC [7]

ProGraML

73.8%

80.3%

86.6%

0.35

0.72

0.76

0.81

0.89

0.59

0.72

0.74

0.80

0.87

0.44

0.72

0.75

0.80

0.88

Static Mapping

56.9%

DeepTune [23]

61.0%

DeepTuneIR

NCC [7]

ProGraML

68.4%

78.5%

80.0%

0.32

0.69

0.70

0.79

0.81

0.57

0.61

0.68

0.79

0.80

0.41

0.65

0.69

0.79

0.80

(a) AMD

(b) NVIDIA

Table 5: Five approaches to predicting heterogeneous device mapping: (a) Static Mapping (b) DeepTune [23], a
sequential model using tokenized OpenCL, (c) DeepTuneIR, the same model adapted for tokenized LLVM-IR, (d)
NCC, which uses pre-trained statement embeddings, and (e) PROGRAML, our approach.

Baseline models were trained with hyperparameters from the original works. For the PROGRAML results we used 6
layers in the GGNN corresponding to 6 timesteps of message propagation, while sharing parameters between even and
odd layers to introduce additional regularization of the weights. We ran a sweep of basic hyperparameters which led
us to use the pre-trained inst2vec statement embeddings [7] and to exclude the use of position representations. Both
of these hyperparameter choices help generalization by reducing the complexity of the model. This is not surprising
in light of the fact that the dataset only contains 680 samples derived from 256 unique programs. PROGRAML was
trained with the Adam optimizer with default parameters, a learning rate of 10−3 and a batch size of 18,000 nodes
for 300 epochs (resulting in ca. 12000 iteration steps of the optimizer). Additionally we found dropout [41] with a
rate of 0.1 on the weights of the message propagation function to be beneﬁcial on the validation set as well. For the
PROGRAML result, we repeat the automated sweep for all hyperparameter conﬁgurations and picked the conﬁguration
with the best average validation performance. Performance on the unseen tenth of the data is reported.

As can be seen, PROGRAML outperforms prior approaches to this problem by all metrics (accuracy, precision, recall,
and F1), across both device datasets.

6.3 Case Study C: Algorithm Classiﬁcation

Table 6 summarizes the algorithm classiﬁcation accuracy results of our method and baselines.

Baseline Experiments Lifting the XFG graph representation from pretraining embeddings [7] to the high-level
task of algorithm classiﬁcation showed a strong improvement of performance. We found that jointly learning the
embeddings with the model from scratch (cf. XFG-rnd in Table 6) outperformed both ﬁxed inst2vec embeddings
(XFG-i2v) as well as ﬁnetuning of inst2vec embeddings (not shown). Both baselines are an improvement over inst2vec
and show that graph-based models are more suited to the task than models with less structure.

Next, we want to understand whether our particular graph-representation reached its design goals and can provide
additional improvement on POJ104.

PROGRAML Experiments To ablate the contribution of the tokenization from the performance boost provided
by the PROGRAML representation itself, we include a GGNN-based structure-only baseline (denoted as GGNN-s)
of our approach, where the only information on each node is whether it represents a statement or an identiﬁer in

16

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

TBCNN [33] NCC [7]

XFG

ProGraML

Metric

Test Error [%]
6.0
Improvement [%] —

5.17
0.0

i2v

4.56
11.8

rnd

4.29
17.0

GGNN-s GGNN Transformer

3.87
25.1

3.78
26.9

3.33
35.6

Table 6: Algorithm Classiﬁcation Error on POJ-104 [33]. The two XFG models are distinct only in their embedding
layers: XFG-i2v uses inst2vec embeddings, while XFG-rnd jointly learns the embeddings. The results denoted by
Surface Features and TBCNN are reproduced from Mou et al. [33].

the graph, but we refrain from tokenizing statements. We think that algorithm classiﬁcation is a problem that lends
itself especially well to judging the power of the representation structure, since most algorithms are well-deﬁned
independent of implementation details, such as datatypes.

The results in Table 6 show that the structure alone of our PROGRAML representation is sufﬁcient to outperform the
XFG baselines and prior work on the task of algorithms classiﬁcation. Adding inst2vec statement tokenization further
improves performance. This suggests that there is room for improvement in performance by extending the graph
encoding method to achieve better vocabulary coverage and stronger generalization. We leave this to future work.

7 Related Work

In order to perform machine learning on programs, prior work has employed methods from Natural Language Process-
ing and represented programs as a sequence of lexical tokens [42, 43, 23]. However, it has been observed [44, 17, 22]
that it is critical to capture the structured nature of programs and syntactic (tree-based) as well as semantic (graph-
based) representations have been proposed [3, 9]. There is a line of research that considers program representations
based on Abstract Syntax Trees (ASTs): Dam et al. [45] annotate nodes in the AST with type information and employ
Tree-Based LSTMs [46] for program defect prediction. Both Raychev et al. [44] and Alon et al. [4, 22] use path-based
abstractions of the AST as program representations, while Allamanis et al. [17] augment ASTs with a hand-crafted
set of additional typed edges and use GGNNs [12] to learn downstream tasks related to variable naming. Another line
of research considers modelling binary similarity via control-ﬂow graphs (CFGs) with an adaptation of GNNs called
Graph Matching Networks [47].

The history of representing programs as graphs for optimization goes back to Program Dependence Graphs
(PDGs) [48], which remove superﬂuous control ﬂow edges to ease optimization with a compact graph representa-
tion. A more contemporary precursor of our PROGRAML representation ConteXtual Flow Graphs (XFGs) [7], which
combine control ﬂow with dataﬂow in order to learn unsupervised embeddings of LLVM-IR statements. While PRO-
GRAML is designed to extend the concepts of XFGs, PROGRAML preserve the notion of argument order and includes
nodes for both variables and constant values and all control ﬂow edges. These changes reﬂects the design goals of
the representations — XFGs are designed to easily express program semantics by omitting superﬂuous control rela-
tions and other execution-speciﬁc properties. PROGRAML, in combining CG, CFG, and DFG, offers a compiler-level
program representation that is designed to be useful for a variety of purposes from speciﬁc program analyses to down-
stream optimization tasks.

Another approach is taken by IR2Vec [49], which deﬁnes an LLVM-IR-speciﬁc statement representation that elegantly
models part-of-statements as relations. However, in order to compute the values of the embeddings, IR2Vec requires
access to the type of data ﬂow analyses that our approach is learning from data alone.

Brauckmann et al. independently proposed a graph-based representation based on Control and Data Flow Graphs
(CDFG) [9]. As in this work, CDFGs represent instructions as graph vertices and have bi-directional edges for control
and data relations. What differentiates PROGRAML from CDFGs is the richness of the program representation:
CDFGs ignore the data elements of programs, and only instruction opcodes are used for vertex embeddings. The latent
representation of statements is thus invariant to instruction operands, their order, data types, and instruction modiﬁers.
This limits the representational power of the approach, e.g. by omitting code properties required for reasoning about
variables and expressions, as we explore through the LIVENESS and SUBEXPRESSIONS experiments in this work.
Finally, our approach provides 76.6× improved inference throughput, though we suspect this may be an artifact of our
implementation as both approaches scale linearly w.r.t. the size of the graph and number of message passing steps.

Graph Neural Networks comprise a diverse class of neural networks that learn by propagating information along
edges [50, 51, 52]. Approaches based on Recurrent Neural Networks (RNNs) [12, 14] as well as convolutional [53, 54]

17

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

and attention-based methods [55, 56] exist. GNNs as a family have been shown to have enough expressive power to
address difﬁcult combinatorial problems like the graph isomorphism test [57] at least as well as the Weisfeiler-Lehman
algorithm [58]. Please refer to Battaglia et al. [52] for a comprehensive review of GNNs.

8 Conclusions

With the demand for aggressively optimizing compilers increasing, there is an increasing burden on compiler heuristics
to make good optimization decisions. While tuning heuristics by hand is expensive and slow to keep up with the pace of
compiler and architecture advancements, machine learning offers tremendous beneﬁts for automatically constructing
heuristics that are both cheaper to develop and better performing than hand-crafted equivalents. The success of these
machine learning approaches is bound by the quality of the input used to represent programs, and the ability of models
to process these representations.

In this work, we present a graph-based representation for programs, derived from compiler IRs, that accurately captures
the semantics of a program’s statements and the relations between them. Our approach is more expressive than prior
sequence- or graph-based representations, while closely approximating the representations that are traditionally used
within compilers.

We have shown through a constructivist approach that machine learning is capable of approximating the types of
compiler analyses that are key for optimizations. In testing our approach on a suite of established compiler tasks that
even state-of-the-art machine learning methods struggle with, our goal is to inspire conﬁdence in machine learning as a
viable tool for reasoning about program semantics, as opposed to a black box which discourages, rather than inspires, a
more systematic approach to reasoning about optimizations. When tasked with real-world problems spanning multiple
domains and source languages, our approach outperforms prior state-of-the-art approaches.

Our hope in developing PROGRAML is to provide a re-usable toolbox for representing and reasoning about programs
that can be used for a wide variety of downstream tasks. Promising research avenues for downstream tasks enabled by
our enriched program representation and the ability to perform statement-level inference include automatic paralleliza-
tion, static performance estimation, and IR-to-IR transpilation. Additionally, while the applications of deep learning
to compilers is rapidly evolving [3, 59], we hope to focus attention on the challenges that machine learning methods
face in the domain of programming languages: scalability when faced with large inputs, modeling very-long-range
dependencies, and learning over unbounded vocabularies.

References

[1] A. H. Ashouri, W. Killian, J. Cavazos, G. Palermo, and C. Silvano. A Survey on Compiler Autotuning using

Machine Learning. CSUR, 51(5), 2018.

[2] Z. Wang and M. O’Boyle. Machine learning in Compiler Optimization. Proceedings of the IEEE, 106(23), 2018.
[3] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton. A Survey of Machine Learning for Big Code and Natural-

ness. CSUR, 51(4), 2018.

[4] U. Alon, M. Zilberstein, O. Levy, and E. Yahav. code2vec: Learning Distributed Representations of Code. In

POPL, 2018.

[5] P. Yin, G. Neubig, M. Allamanis, M. Brockschmidt, and A. L. Gaunt.

Learning to Represent Edits.

arXiv:1810.13337, 2018.

[6] Ameer Haj-Ali, N. K. Ahmed, T. Willke, S. Shao, K. Asanovic, and I. Stoica. NeuroVectorizer: End-to-End

Vectorization with Deep Reinforcement Learning. In CGO, 2020.

[7] T. Ben-Nun, A. S. Jakobovits, and T. Hoeﬂer. Neural Code Comprehension: A Learnable Representation of

Code Semantics. In NeurIPS, 2018.

[8] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou, N. Kumar, M. Norouzi, S. Bengio, and J. Dean.

Device Placement Optimization with Reinforcement Learning. In ICML, 2017.

[9] A. Brauckmann, S. Ertel, A. Goens, and J. Castrillon. Compiler-Based Graph Representations for Deep Learning

Models of Code. In CC, 2020.

[10] Y. Bengio, P. Simard, and P. Frasconi. Learning Long-Term Dependencies with Gradient Descent is Difﬁcult.

IEEE Transactions on Neural Networks, 5(2), 1994.

[11] M. McCloskey and N. J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential Learning

Problem. Psychology of Learning and Motivation, 24, 1989.

18

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

[12] Y. Li, R. Zemel, M. Brockscmidt, and D. Tarlow. Gated Graph Sequence Neural Networks. arXiv:1511.05493,

2015.

[13] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling. Modeling Relational Data with

Graph Convolutional Networks. In ESWC, 2018.

[14] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural Message Passing for Quantum

Chemistry. In ICML. PMLR, 2017.

[15] Z. Chen and M. Monperrus. A Literature Study of Embeddings on Source Code. arXiv:1904.03061, 2019.
[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention

Is All You Need. In NIPS, 2017.

[17] M. Allamanis, M. Brockschmidt, and M. Khademi. Learning to Represent Programs with Graphs. In ICLR,

2017.

[18] C. Lattner and V. Adve. LLVM: A compilation framework for lifelong program analysis & transformation. In

CGO. IEEE, 2004.

[19] C. Leary and T. Wang. XLA: TensorFlow, compiled. In TensorFlow Dev Summit, 2017.
[20] C. Cummins, P. Petoumenos, W. Zang, and H. Leather. Synthesizing Benchmarks for Predictive Modeling. In

CGO. IEEE, 2017.

[21] E. Park, J. Cavazos, and M. A. Alvarez. Using Graph-Based Program Characterization for Predictive Modeling.

In CGO. IEEE, 2012.

[22] U. Alon, M. Zilberstein, O. Levy, and E. Yahav. A General Path-Based Representation for Predicting Program

Properties. In PLDI. ACM, 2018.

[23] C. Cummins, P. Petoumenos, Z. Wang, and H. Leather. End-to-end Deep Learning of Optimization Heuristics.

In PACT. IEEE, 2017.

[24] F. Barchi, G. Urgese, E. Macii, and A. Acquaviva. Code Mapping in Heterogeneous Platforms Using Deep

Learning and LLVM-IR. In DAC. ACM, 2019.

[25] V. J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis. Deep Learning Type Inference. In ESEC/FSE, 2018.
[26] J. B. Kam and J. D. Ullman. Monotone Data Flow Analysis Frameworks. Acta Informatica, 7(3), 1977.
[27] K. D. Cooper, T. J. Harvey, and K. Kennedy. Iterative Data-ﬂow Analysis, Revisited. Technical report, Depart-

ment of Computer Science, Rice University, 2004.

[28] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional Sequence to Sequence Learning.

In ICML. PMLR, 2017.

[29] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning
Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In EMNLP, 2014.
[30] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal

Covariate Shift. In ICML. PMLR, 2015.

[31] T. Lengauer and R. E. Tarjan. A Fast Algorithm for Finding Dominators in a Flow Graph. TOPLAS, 1(1), 1979.
[32] S. Blazy, D. Demange, and D. Pichardie. Validating Dominator Trees for a Fast, Veriﬁed Dominance Test. In

ITP, 2015.

[33] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. Convolutional Neural Networks over Tree Structures for Program-

ming Language Processing. In AAAI, 2016.

[34] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng. TensorFlow: A System for Large-scale Machine Learning. In OSDI, 2016.
[35] T. Ben-Nun and T. Hoeﬂer. Demystifying parallel and distributed deep learning: An in-depth concurrency

analysis. ACM Computing Surveys, 52(4), 2019.

[36] D. P. Kingma and J. L. Ba. Adam: a Method for Stochastic Optimization. In ICLR, 2015.
[37] D. Grewe, Z. Wang, and M. O’Boyle. Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous

Systems. In CGO. IEEE, 2013.

[38] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving Neural Networks

by Preventing Co-adaptation of Feature Detectors. arXiv:1207.0580, 2012.

[39] Z. Fisches. Neural Self-Supervised Models of Code. Masters thesis, ETH Zurich, 2020.

19

PROGRAML: Graph-based Deep Learning for Program Optimization and Analysis

[40] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2019.
[41] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A Simple Way to Prevent

Neural Networks from Overﬁtting. JMLR, 15, 2014.

[42] M. Allamanis and C. Sutton. Mining Source Code Repositories at Massive Scale using Language Modeling. In

MSR, 2013.

[43] M. Allamanis. Learning Natural Coding Conventions. PhD thesis, University of Edinburgh, 2016.
[44] V. Raychev, M. Vechev, and A. Krause. Predicting Program Properties from "Big Code". In POPL, 2015.
[45] H. K. Dam, J. Grundy, T. Kim, and C. Kim. A Deep Tree-Based Model for Software Defect Prediction.

arXiv:1802.00921, 2018.

[46] K. S. Tai, R. Socher, and C. D. Manning.

Improved Semantic Representations From Tree-Structured Long

Short-Term Memory Networks. arXiv:1503.00075, 2015.

[47] L. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli. Graph Matching Networks for Learning the Similarity of

Graph Structured Objects. In ICML. PMLR, 2019.

[48] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The Program Dependence Graph and Its Use in Optimization.

TOPLAS, 9(3), 1987.

[49] V. Keerthy S, R. Aggarwal, S. Jain, M. S. Desarkar, R. Upadrasta, and Y. N. Spkant. IR2Vec: A Flow Analysis

based Scalable Infrastructure for Program Encodings. arXiv:1909.06228, 2019.

[50] M. Gori, G. Monfardini, and F. Scarselli. A New Model for Learning in Graph Domains. In IJCNN. IEEE, 2005.
[51] S. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The Graph Neural Network Model.

IEEE Transactions on Neural Networks, 20(1), 2009.

[52] P. Battaglia, J. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetii, D. Raposo,
A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash,
V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational
Inductive Biases, Deep Learning, and Graph Networks. arXiv:1806.01261, 2018.

[53] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized

Spectral Filtering. In NIPS, 2016.
[54] T. N. Kipf and M. Welling.
arXiv:1609.02907, 2017.

Semi-supervised Classiﬁcation with Graph Convolutional Networks.

[55] P. Velickovic, G. Cucurull, A. Casanova, A. Romera, P. Lio, and Y. Bengio. Graph Attention Networks. In ICLR,

2018.

[56] G. Wang, R. Ying, J. Huang, and J. Leskovec. Improving Graph Attention Networks with Large Margin-based

Constraints. arXiv:1910.11945, 2019.

[57] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How Powerful are Graph neural Networks? In ICLR, 2019.
[58] B. Weisfeiler and A. A. Lehman. A Reduction of a Graph to a Canonical Form and an Algebra Arising During

this Reduction. Nauchno-Technicheskaya Informatsia, 2(9), 1968.

[59] C. Cummins. Deep Learning for Compilers. PhD thesis, University of Edinburgh, 2020.

20

