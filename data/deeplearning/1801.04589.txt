Deep Reinforcement Fuzzing

Konstantin B¨ottinger1, Patrice Godefroid2, and Rishabh Singh2
1Fraunhofer AISEC, 85748 Garching, Germany
konstantin.boettinger@aisec.fraunhofer.de
2Microsoft Research, 98052 Redmond, USA

pg,risin
}

{

@microsoft.com

8
1
0
2

n
a
J

4
1

]
I

A
.
s
c
[

1
v
9
8
5
4
0
.
1
0
8
1
:
v
i
X
r
a

Abstract—Fuzzing is the process of ﬁnding security vulnera-
bilities in input-processing code by repeatedly testing the code
with modiﬁed inputs. In this paper, we formalize fuzzing as a
reinforcement learning problem using the concept of Markov
decision processes. This in turn allows us to apply state-of-the-
art deep Q-learning algorithms that optimize rewards, which
we deﬁne from runtime properties of the program under test.
By observing the rewards caused by mutating with a speciﬁc
set of actions performed on an initial program input,
the
fuzzing agent learns a policy that can next generate new higher-
reward inputs. We have implemented this new approach, and
preliminary empirical evidence shows that reinforcement fuzzing
can outperform baseline random fuzzing.

I. INTRODUCTION

Fuzzing is the process of ﬁnding security vulnerabilities
in input-processing code by repeatedly testing the code with
modiﬁed, or fuzzed, inputs. Fuzzing is an effective way to
ﬁnd security vulnerabilities in software [1], and is becoming
standard in the commercial software development process [2].
Existing fuzzing tools differ by how they fuzz program
inputs, but none can explore exhaustively the entire input space
for realistic programs in practice. Therefore, they typically
use fuzzing heuristics to prioritize what (parts of) inputs to
fuzz next. Such heuristics may be purely random, or they may
attempt to optimize for a speciﬁc goal, such as maximizing
code coverage.

In this paper, we investigate how to formalize fuzzing as a
reinforcement learning problem. Intuitively, choosing the next
fuzzing action given an input to mutate can be viewed as
choosing a next move in a game like Chess or Go: while an
optimal strategy might exist, it is unknown to us and we are
bound to play the game (many times) in the search for it. By
reducing fuzzing to reinforcement learning, we can then try to
apply the same neural-network-based learning techniques that
have beaten world-champion human experts in Backgammon
[3], [4], Atari games [5], and the game of Go [6].

Speciﬁcally, fuzzing can be modeled as learning process
with a feedback loop. Initially, the fuzzer generates new inputs,
and then runs the target program with each of them. For each
program execution, the fuzzer extracts runtime information
(gathered for example by binary instrumentation) for evaluat-
ing the quality (with respect to the deﬁned search heuristic) of
the current input. For instance, this quality can be measured
as the number of (unique or not) instructions executed, or
the overall runtime of the execution. By taking this quality
feedback into account, a feedback-driven fuzzer can learn from

Fig. 1. Modeling Fuzzing as a Markov decision process.

past experiments, and then generate other new inputs hopefully
of better quality. This process repeats until a speciﬁc goal
is reached, or bugs are found in the program. Similarly, the
reinforcement learning setting deﬁnes an agent that interacts
with a system. Each performed action causes a state transition
of the system. Upon each performed action the agent observes
the next state and receives a reward. The goal of the agent is
to maximize the total reward over time.

Our mathematical model of fuzzing is captured in Figure 1.
An input mutator engine M generates a new input I by
performing a fuzzing action a, and subsequently observes
a new state x directly derived from I as well as a reward
r(x, a) that is measured by executing the target program P
with input I. We reduce input fuzzing to a reinforcement
learning problem by formalizing it using Markov decision
processes [7]. Our formalization allows us to apply state-of-
the-art machine learning methods. In particular, we experiment
with deep Q-learning.

In summary, we make the following contributions:

•

•

•

•

We formalize fuzzing as a reinforcement learning prob-
lem using the concept of Markov decision processes.
We introduce a fuzzing algorithm based on deep Q-
learning that learns to choose highly-rewarded fuzzing
actions for any given propgram input.
We implement and evaluate a prototype of our approach.
We present empirical evidence that reinforcement fuzzing
can outperform baseline random fuzzing.

II. RELATED WORK

Our work is inﬂuenced by three main streams of research:

fuzzing, grammar reconstruction, and deep Q-learning.

MIPr(x,a)xa 
 
 
 
 
 
A. Fuzzing

[8],

(1) blackbox random fuzzing [1],

There are three main types of fuzzing techniques in use
(2) white-
today:
box constraint-based fuzzing [9], and (3) grammar-based
fuzzing [10], [1], which can be viewed as a variant of model-
based testing [11]. Blackbox and whitebox fuzzing are fully
automatic, and have historically proved to be very effective at
ﬁnding security vulnerabilities in binary-format ﬁle parsers.
In contrast, grammar-based fuzzing is not fully automatic:
it requires an input grammar specifying the input format of
the application under test. This grammar is typically written
by hand, and this process is laborious, time consuming, and
error-prone. Nevertheless, grammar-based fuzzing is the most
effective fuzzing technique known today for fuzzing applica-
tions with complex structured input formats, like web-browsers
which must take as (untrusted) inputs web-pages including
complex HTML documents and JavaScript code.

State-of-art fuzzing tools like SAGE [9] or AFL [12]
use coverage-based heuristics to guide their search for bugs
towards less-covered code parts. But they do not use machine
learning techniques as done in this paper.

Combining statistical neural-network-based machine learn-
ing with fuzzing is a novel approach and, to the best of
our knowledge, there is just one prior paper on this topic:
Godefroid et al. [13] use character-based language models to
learn a generative model of fuzzing inputs, but they do not
use reinforcement learning.

B. Grammar Reconstruction

Research on reconstructing grammars from sample inputs
for testing purposes started in the early 1970’s [10], [14].
More recently, Bastani et al. [15] proposed an algorithm for
automatic synthesis of a context-free grammar given a set of
seed inputs and a black-box target. Cui et al. [16] automatically
detect record sequences and types in the input by identiﬁcation
of chunks based on taint tracking input data in respective
subroutine calls. Similarly, the authors of [17] apply dynamic
tainting to identify failure-relevant inputs. Another recently
proposed approach [18] mines input grammars from valid
inputs based on feedback from dynamic instrumentation of
the target by tracking input characters.

C. Deep Q-Learning

Reinforcement learning [19] emerged from trial and error
learning and optimal control for dynamic programming [7].
Especially the Q-learning approach introduced by Watkins
[20], [21] was recently combined with deep neural networks
[3], [4], [5], [6] to efﬁciently learn policies over large state
spaces and has achieved impressive results in complex tasks.

III. REINFORCEMENT LEARNING

In this section we give the necessary background on rein-
forcement learning. We ﬁrst introduce the concept of Markov
decision processes [7], which provides the basis to formalize
fuzzing as a reinforcement learning problem. Then we discuss

the Q-learning approach to such problems and motivate the
application of deep Q-networks.

Reinforcement learning is the process of adapting an agent’s
behavior during interaction with a system such that it learns
to maximize the received rewards based on performed actions
and system state transitions. The agent performs actions on
the system
tries to control. For each action,
a system it
undergoes a state transition. In turn, the agent observes the
new state and receives a reward. The aim of the agent is to
maximize its cumulative reward received during the overall
time of system interaction. The following formal notation
relates to the presentation given in [19].

M

is deﬁned as

The interaction of the agent with the system can be seen as
a stochastic process. In particular, a Markov decision process
= (X, A, P0), where X denotes a set
M
of states, A a set of actions, and P0 the transition probability
A and each
kernel. For each state-action pair (x, a)
U
x, a)
such that performing action a in state x causes the system to
transition into some state of X that yields some real-valued
reward U . P0 directly provides the state transition probability
kernel P for single transitions (x, a, y)

R the kernel P0 gives the probability P0(U

X

X

X

X

×

×

⊂

A

∈

|

∈

×

×

y
P (x, a, y) = P0(
{

} ×

x, a).

R

|

(1)

This naturally gives rise to a stochastic process: An agent
observing a certain state chooses an action to cause a state
transition with the corresponding reward. By subsequently
observing state transitions with corresponding rewards the
agent aims to learn an optimal behavior that earns the maximal
possible cumulative reward over time. Formally, with the
stochastic variables (y(x, a), r(x, a)) distributed according to
x, a) the expected immediate reward for each choice
P0(
of action is given by E[r(x, a)]. In the following, for a
stochastic variable v the notation v
D indicates that v
is distributed according to D. During the stochastic process
xt, at) the aim of an agent is to maximize
(xt+1, rt+1)
P (
·|
the total discounted sum of rewards

∼

∼

·|

=

R

∞(cid:88)

t=0

γtrt+1,

(2)

∈

where γ
(0, 1) indicates a discount factor that prioritizes
rewards in the near future. The choice of action at an agent
makes in reaction to observing state xt is determined by its
policy at
xt). The policy π maps observed states to
actions and therefore determines the behavior of the agent.
Let

π(

∼

·|

Qπ(x, a) = E

(cid:34)

∞(cid:88)

t=0

(cid:35)

γtrt+1

x0 = x, a0 = a
|

(3)

denote the expected cumulative reward for an agent
that
behaves according to policy π. Then we can reduce our
problem of approximating the best policy to approximating

the optimal Q function. One practical way to achieve this is
adjusting Q after each received reward according to

Q(xt, at)

←

Q(xt, at)
(cid:16)

+ α

rt + γ max

a

Q(xt+1, a)

Q(xt, at)

−

(4)

(cid:17)

,

(5)

∈

where α
(0, 1] indicates the learning rate. The process
in this setting works as follows: The agent observes a state
xt, performs the action at = arg maxa Q(xt, a) (where
arg maxa f (a) denotes the argument value a that maximizes
f (a)) that maximizes the total expected future reward and
thereby causes a state transition from xt to xt+1. Receiving
reward rt and observing xt+1 the agent then considers the
best possible action at+1 = arg maxa Q(xt+1, a). Based on
this consideration, the agent updates the value Q(xt, at). If
for example the decision of taking action at in state xt led to
a state xt+1 that allows to choose a high reward action and
additionally invoked a high reward rt, the Q value for this
decision is adapted accordingly. Here, the factor α determines
the rate of this Q function update.

For small state and action spaces, Q can be represented as a
table. However, for large state spaces we have to approximate
Q with an appropriate function. An approximation using deep
neural networks was recently introduced by Mnih et al. [5]. For
such a representation, the update rule in Equation (4) directly
translates to minimizing the loss function

(cid:16)

L =

r + γ max

a

Q(xt+1, a)

−

(cid:17)2

Q(xt, at)

.

(6)

The learning rate α in Equation (4) then corresponds to the
rate of stochastic gradient descent during backpropagation.

Deep Q-networks have been shown to handle large state
spaces efﬁciently. This allows us to deﬁne an end-to-end
algorithm directly on raw program inputs, as we will see in
the next section.

IV. MODELING FUZZING AS A MARKOV DECISION
PROCESS

In this section we formalize fuzzing as a reinforcement
learning problem using a Markov decision process by deﬁning
states, actions, and rewards in the fuzzing context.

A. States

We consider the system that the agent learns to interact with
to be a given “seed” program input. Further, we deﬁne the
states that the agent observes to be substrings of consecutive
symbols within such an input. Formally, let Σ denote a ﬁnite
written
set of symbols. The set of possible program inputs
:= Σ∗.
in this alphabet is then deﬁned by the Kleene closure
For an input string x = (x1, ..., xn)

I
I

let

∈ I

S(x) :=

{

(x1+i, ..., xm+i)

i

|

≥

0, m + i

n)

(7)

denote the set of all substrings of x. Clearly,
I
holds. We deﬁne the states of our Markov decision process to
be
denotes an input for the target
program and x(cid:48)

a substring of this input.

. In the following, x

S(x)

∈ I

∈I

I

≤
x
∪

}
S(x) =

∈

⊂ I

B. Actions

We deﬁne the set of possible actions

of our Markov
decision process to be random variables mapping substrings
of an input to probabilistic rewriting rules

A

A

:=

a :
{
= σ(

I →

(

I × I

, P )

,

F

a

π(x(cid:48))
}

,

∼

|

(8)

I × I

F
I × I

) denotes the σ-algebra of the sample
where
space (
) and P gives the probability for a given rewrite
rule. In our implementation (see Section VI) we deﬁne a small
subset A
of probabilistic string rewrite rules that operate
⊂ A
on a given seed input.

C. Rewards

We deﬁne rewards independently for both characteristics of:
1) the next performed action a and 2) the program execution
with the next generated input x, i.e., r(x, a) = E(x) + G(a).
In our implementation in Section VI we experiment with E
providing number of newly discovered basic blocks, execution
path length, and execution time of the target that processes
the input x. For example, we can deﬁne the number of newly
discovered blocks as

E1(x, I (cid:48)) :=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(cx)





\

(cid:91)

(cid:48)

χ

∈I

B(cχ)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

.

(9)

where cx denotes the execution path the target program takes
when processing input x, B(cx) is the set of unique basic
I is the set of previously
blocks of this path, and I (cid:48)
⊂
processed inputs. Here, we deﬁne a basic block as a sequence
of program instructions without branch instructions.

V. REINFORCEMENT FUZZING ALGORITHM

In this section we present the overall reinforcement fuzzing

algorithm.

A. Initialization

∈ I

We start with an initial seed input x

. The choice of x
is not constrained in any way, it may not even be valid with
regard to the input format of the target program. Next, we
initialize the Q function. For this, we apply a deep neural net
that maps states to the estimated Q values of each action, i.e.,
we simultaneously approximate the Q values for all actions A
given a state x(cid:48)
(cid:55)→
Q(x(cid:48), a) representation provides the advantage that we only
need one forward pass to simultaneously receive the Q values
for all actions a
forward passes. During
Q function initialization we distribute the network weights
randomly.

S(x) as deﬁned in Equation (7). The x(cid:48)

A instead of

A

∈

∈

|

|

B. State Extraction

The state extraction step State() takes as input a seed x

∈ I
S(x). In Section IV we deﬁned
and outputs a substring of x(cid:48)
= Σ∗. For
the states of our Markov decision process to be
the given seed x
S(x)
at offset o
. In other words,
|}
the seed x corresponds to the system as depicted in Figure 1

we extract a strict substring x(cid:48)

∈ I
x
|

of width

0, ...,

x(cid:48)
|

| − |

∈ {

x(cid:48)

∈

∈

I

|

and the reinforcement agent observes a fragment of the whole
system via the substring x(cid:48). We experimented with controllable
(via action) and predeﬁned choices of offsets and substring
widths, as discussed in Section VI.

C. Action Selection

The action selection step takes as input

the current Q
function and an observed state x(cid:48) and outputs an action a
A
as deﬁned in Equation (8). Actions are selected according to
the policy π following an (cid:15)-greedy behavior: With probability
(cid:15) (for a small (cid:15) > 0) the agent selects an action
1
a = arg maxa(cid:48) Q(x(cid:48), a(cid:48)) that is currently estimated optimal
by the Q-function, i.e., it exploits the best possible choice
based on experience. With a probability (cid:15) it explores any other
action, where the probability of choice is uniformly distributed
within

−

∈

.

A
|

|

D. Mutation

The mutation step takes as input a seed x and an action
a. It outputs the string that is generated by applying action
a on x. As indicated in Equation (8) we deﬁne actions to
be mappings to probabilistic rewriting rules and not rewriting
rules on their own. So applying action a on x means that we
mutate x according to the rewrite rule mapped by a within the
probability space (
, P ). We make this separation to
distinguish between the random nature of choice for the action
a

x(cid:48)) and the randomness within the rewrite rule.

I × I

π(

F

,

∼

·|

E. Reward Evaluation

∈

A, and an input x

The reward evaluation step takes as input the target program
that was generated
P , an action a
by the application of a on a seed. It outputs a positive number
R+. The stochastic reward variable r(x, a) = E(x)+G(a)
r
sums up the rewards for both generated input and selected
action. Function E rewards characteristics recorded during the
program execution as deﬁned in Section IV-C.

∈ I

∈

F. Q-Update

∈

S(x),

The Q-update step takes as input the extracted substring
the action a that generated x,
the evaluated
x(cid:48)
∈
R+, and the Q function approximation, which
reward r
in our case is a deep neural network. It outputs the updated
Q approximation. As indicated above, the choice of applying
a deep neural network Q is motivated by the requirement to
learn on raw substrings x(cid:48)
S(x). The Q function predicts
for a given state the expected rewards for all deﬁned actions
it maps substrings according to
of A simultaneously,
Q(x(cid:48), a). We update Q in the sense that we adapt
x(cid:48)
the predicted reward value Q(xt, at) according to the target
r + γ maxa Q(xt+1, a). This yields the loss function L given
are
by Equation (6) for action at. All other actions A
updated with zero loss. The convergence rate of Q is primarily
determined by the learning rate of stochastic gradient descent
during backpropagation as well as the choice of γ.

i.e.,

\ {

(cid:55)→

at

∈

}

Fig. 2. Reinforcement fuzzing algorithm.

G. Joining the Pieces

Now that we have presented all individual steps we can
the overall fuzzing

proceed with combining them to get
algorithm as depicted in Figure 2.

∈

∈ I

We start with an initialization phase that outputs a seed x
as well as the initial version of Q. Then, the fuzzer enters
the loop of state extraction, action selection, input mutation,
reward evaluation, Q update, and test case reset. Starting with
S(x) and
, the algorithm extracts a substring x(cid:48)
a seed x
based on the observed state x(cid:48) then chooses the next action
according to its policy. The choice is made looking at the best
possible reward predicted via x(cid:48)
Q(x(cid:48), a) and applying an
(cid:15)-greedy exploitation-exploration strategy. To guarantee initial
exploration we initially deﬁne a relatively high value for (cid:15)
and monotonically decrease (cid:15) over time until it reaches a ﬁnal
small threshold, from then on it remains constant. The selected
action provides a string substitution as indicated in Equation
(8) which is applied to x for mutation. The generated mutant
input is fed into the target program P to evaluate the reward
r. Together with Q, x, and a, this reward is taken into account
for Q update. Finally, the Reset() function periodically resets
input x to a valid seed. In our implementation we reset the
seed after each mutation as described in Section VI. After
reset, the algorithm continues the loop.

(cid:55)→

We formulated the algorithm with just one single input seed.
However, we could generalize this to a set of seed inputs by
choosing another seed within this set for each iteration of the
main loop.

The algorithm above performs reinforcement fuzzing with
activated policy learning. We show in our evaluation in Sec-
tion VI that the Q-network generalizes on states. This allows
us to switch to high-throughput mutant generation with a ﬁxed
policy after a sufﬁciently long training phase.

VI. IMPLEMENTATION AND EVALUATION

In this section we present details regarding our implemen-

tation together with an evaluation of the prototype.

Input:ProgramPx Seed()Q Qnet()do:x0 State(x)a Action(x0,Q)x Mutate(x,a)r Reward(P,x)Q Update(Q,x0,a,r)x Reset()while(true)A. Target Programs

As fuzzing targets we chose programs processing ﬁles in
the Portable Document Format PDF. This format is complex
enough to provide a realistic testbed for evaluation. From the
1,300 pages long PDF speciﬁcation [22], we just need the fol-
lowing basic understanding: each PDF document is a sequence
of PDF bodies each of which includes three sections named
objects, cross-reference table, and trailer. While our algorithm
is deﬁned to be independent of the targeted input format, we
used this structure to deﬁne fuzzing actions speciﬁcally crafted
for PDF objects.

Initially we tested different PDF processing programs in-
cluding the PDF parser in the Microsoft Edge browser on
Windows and several command line converters on Linux.
All results in the following presentation refer to fuzzing the
pdftotext program mutating a 168 kByte seed ﬁle with 101
PDF objects including binary ﬁelds.

B. Implementation

In the following we present details regarding our imple-
mentation of the proposed reinforcement fuzzing algorithm.
We apply existing frameworks for binary instrumentation and
neural network training and implement the core framework
including the Q-learning module in Python 3.5.

|

•

{

|}

x(cid:48)

∈ {

| − |

0, ...,

x
|
to be the width of the current state.

a) State Implementation: Our fuzzer observes and mu-
tates input ﬁles represented as binary strings. With Σ =
0, 1
}
we can choose between state representations of different
granularity, for example bit or byte representations. We encode
the state of a substring x(cid:48) as the sequence of bytes of this
string. Each byte is converted to its corresponding ﬂoat value
when processed by the Q network. As introduced in Section
to be the offset of x(cid:48) and
V we denote o
x(cid:48)
w =
|
b) Action Implementation: We implement each action as
a function in a Python dictionary. As string rewriting rules we
take both probabilistic and deterministic actions into account.
In the following we list the action classes we experiment with.
Random Bit Flips. This type of action mutates the
substring x(cid:48) with predeﬁned and dynamically adjustable
mutation ratios.
Insert Dictionary Tokens. This action inserts tokens from
a predeﬁned dictionary. The tokens in the dictionary
consist of ASCII strings extracted from a set of selected
seed ﬁles.
Shift Offset and Width. This type of action shifts the
offset and width of the observed substring. Left and right
shift take place at the PDF object level. Increasing and
decreasing the width take place with byte granularity.
Shufﬂe. We deﬁne two actions for shufﬂing substrings.
The ﬁrst action shufﬂes bytes within x(cid:48), the second action
shufﬂes three segments of the PDF object that is located
around offset o.
Copy Window. We deﬁne two actions that copy x(cid:48) to a
random offset within x. The ﬁrst action inserts the bytes
of x(cid:48), the second overwrites bytes.

•

•

•

•

•

Delete Window. This action deletes the observed substring
x(cid:48).
c) Reward Implementation: For evaluation of the reward
R(x, a) we experimented with both coverage and execution
time information.

To measure E(x) = E1(x,

(cid:48)) as deﬁned in Equation (9),
I
we used existing instrumentation frameworks. We initially
used the Microsoft Nirvana toolset for measuring code cover-
age for the PDF parser included in Edge. However, to speed up
training of the Q net we switched to smaller parser targets. On
Linux we implemented a custom Intel PIN-tool plug-in that
counts the number of unique basic blocks within the pdftotext
program.

d) Q Network Implementation: We implemented the Q
learning module in Tensorﬂow [23] by constructing a feed for-
ward neural network with four layers connected with nonlinear
activation functions. The two hidden layers included between
64 and 180 hidden units (depending on the state size) and we
applied tanh as activation function. We initialize the weights
[0, 0.1]. The
randomly and uniformly distributed within wi
initial learning rate of the gradient descent optimizer is set to
0.02.

∈

C. Evaluation

In this section we evaluate our implemented prototype. We
present improvements to a predeﬁned baseline and also discuss
current limitations. All measurements were performed on a
Xeon E5-2690 2.6 Ghz with 112 GB of RAM. The summary
of the improvements obtained in accumulated rewards based
on different reward functions, modifying state size, and gen-
eralization to new inputs is shown in Table I. We now explain
the results in more detail.

1) Baseline: To show that our new reinforcement learning
algorithm actually learns to perform high-reward actions given
an input observation, we deﬁne a comparison baseline policy
that randomly selects actions, where the choice is uniformly
distributed among the action space A. Formally, actions in
the baseline policy πB are distributed uniformly according to
1. After ng =
a
x) =
A : πB(a
|
1000 generations, we calculated the quotient of the most recent
500 accumulated rewards by our algorithm and the baseline
to measure the relative improvement.

x) and

πB(

−
|

∼

A

∈

∀

·|

a

|

2) Replay Memory: We experimented with two types of
agent memory: The recorded state-action-reward-state se-
quences as well as the history of previously discovered basic
blocks. The ﬁrst type of memory is established during the
fuzzing process by storing sequences et := (xt, at, rt, xt+1)
in order to regularly replay samples of them in the Q-update
step. For each replay step at time t a random experience out
of
is sampled to train the Q network. We could not
measure any improvement compared to the baseline with this
method. Second, comparing against the history of previously
discovered basic blocks also did not result in any improvement.
Only a memoryless choice of I (cid:48) =
yielded good results.
Regarding our algorithm as depicted in Figure 2 we reset the
basic block history after each step via the Reset() function.

e1, ..., et

}

{

∅

Improvement

Reward functions
Code coverage r1
Execution time r2
Combined r3
State width w = |x(cid:48)|
r2 with w = 32 Bytes
r2 with w = 80 Bytes
Generalization to new inputs
r2 for new input x

7.75%
7%
11.3%

7%
3.1%

4.7%

TABLE I
THE IMPROVEMENTS COMPARED TO THE BASELINE (AS DEFINED
INVI-C1) IN THE MOST RECENT 500 ACCUMULATED REWARDS AFTER
TRAINING THE MODELS FOR 1000 GENERATIONS.

|

|

x

|}

x(cid:48)

| − |

x(cid:48)
|

Since both types of agent memory did not yield any
improvement, we switched them off for the following mea-
surements. Further, we deactivated all actions that do not
mutate the seed input, e.g. random bit ﬂip actions of adjusting
the global mutation ratio or shifting offsets and state widths.
Instead of active offset o and state width w =
selection
the offset for each iteration
via an agent action, we set
randomly, where the choice is uniformly distributed within
and ﬁxed w = 32 Bytes.
0, ...,
{
3) Choices of Rewards: We experimented with three dif-
types of rewards: Maximization of code coverage
ferent
), execution time r2(x, a) = E2(x) =
r1(x, a) = E1(x,
T (x), and a combined reward r3(x, a) = E1(x,
) + T (x)
with rescaled time for multi-goal fuzzing. While r1(x, a) is
deterministic, r2(x, a) comes with minor noise in the time
measurement. Measuring the execution time for different seeds
and mutations revealed a variance that
is two orders of
magnitude smaller than the respective mean so that r2 is stable
enough to serve as a reliable reward function. All three choices
provided improvements with respect to the baseline.

{}

{}

When rewarding execution time according to r2 our pro-
posed fuzzing algorithm cumulates in average 7% higher
execution time reward in comparison to the baseline.

Since both time and coverage rewards yielded comparable
improvements with regard to the baseline, we tested to what
extend those two types of rewards correlate: We measured
an average Pearson correlation coefﬁcient of 0.48 between
coverage r1 and execution time r2. This correlation motivates
the combined reward r3(x, a) = E1(x,
) + T (x), where
T (x) is a simple rescaling of execution time by a multiplicative
106 so that the execution time contributes to the
factor 1
reward equitable to E1. Training the Q net with r3 yielded
an improvement of 11.3% in execution time. This result is
better that taking exclusively r1 or r2 into account. There are
two likely explanations for this result. First, the noise of time
measurement could introduce rewarding explorative behavior
of the Q net. Second, deterministic coverage information could
add stability to r2.

{}

∗

4) Q-net Activation Functions: From all activation func-
tions provided by the Tensorﬂow framework, we found the
tanh function to yield the best results for our setting. The
following list compares the different activation functions with
respect to improvement in reward r1.

tanh
elu
sigmoid
7.75% 6.56% 5.3%

softplus
2%

softsign
6.4%

relu
1.3%

x(cid:48)
|

5) State Width: Increasing the state width w =

from
32 Bytes to 80 Bytes decreased the improvement (measured in
average reward r2(x, a) compared to the baseline) from 7% to
3.1%. In other words, smaller substrings are better recognized
than large ones. This indicates that our proposed algorithm
actually takes the structure of the state into account and learns
to perform best rewarded actions according to this speciﬁc
structure.

|

In order

6) State Generalization:

to achieve high-
throughput fuzzing we tested if the already trained Q net
generalizes to previously unseen inputs. This would allow
us to switch off Q net training after a while and therefore
avoid the high processing costs of evaluating the coverage
reward. To measure generalization we restricted the offset
in the training phase to values in the
o
ﬁrst half of the seed ﬁle. For testing, we omitted reward
measurement in the Q update step as depicted in Figure 2
to stop the training phase and only considered offsets in the
second half of the seed ﬁle. This way, the Q net is confronted
with previously unseen states. This resulted in an improvement
in execution time of 4.7% compared to the baseline.

0, ...,

| − |

∈ {

x(cid:48)

|}

x

|

VII. CONCLUSION

Inspired by the similar nature of feedback-driven random
testing and reinforcement
learning, we introduce the ﬁrst
fuzzer that uses reinforcement learning in order to learn high-
reward mutations with respect to predeﬁned reward metrics.
By automatically rewarding runtime characteristics of the
target program to be tested, we obtain new inputs that likely
drive program execution towards a predeﬁned goal, such as
maximized code coverage or processing time. To achieve this,
we formalize fuzzing as a reinforcement learning problem
using Markov decision processes. This allows us to construct
an reinforcement-learning fuzzing algorithm based on deep Q-
learning that chooses high-reward actions given an input seed.
The policy π as deﬁned in Section III can be viewed
as a form of generalized grammar for the input structure.
Given a speciﬁc state, it suggests a string replacement (i.e., a
fuzzing action) based on experience. Especially if we reward
execution path depth, we indirectly reward validity of inputs
with regard to the input structure, as non-valid inputs are likely
to be rejected early during parsing and result in small path
depths. We presented preliminary empirical evidence that our
reinforcement fuzzing algorithm can learn how to improve
its effectiveness at generating new inputs based on successive
feedback. Future research should investigate this further, with
more setup variants, benchmarks, and experiments.

REFERENCES

[1] M. Sutton, A. Greene, and P. Amini, Fuzzing: Brute Force Vulnerability
Discovery, 1st ed. Boston, MA, USA: Addison-Wesley Professional,
2007.

[2] M. Howard and S. Lipner, The Security Development Lifecycle. Mi-

crosoft Press, 2006.

[3] G. Tesauro, “Practical issues in temporal difference learning,” in Ad-

vances in neural information processing systems, 1992, pp. 259–266.

[4] ——, “Td-gammon: A self-teaching backgammon program,” in Appli-

cations of Neural Networks. Springer, 1995, pp. 267–285.

[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.

through deep reinforcement

[6] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” Nature, vol. 529, no. 7587, pp. 484–489, 2016.
[7] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press Cambridge, 1998.

[8] A. Takanen, J. DeMott, and C. Miller, Fuzzing for Software Security
Testing and Quality Assurance, 1st ed. Norwood, MA, USA: Artech
House, Inc., 2008.

[9] P. Godefroid, M. Y. Levin, and D. A. Molnar, “Automated whitebox
fuzz testing.” in NDSS, vol. 8, 2008, pp. 151–166. [Online]. Avail-
able:
http://46.43.36.213/sites/default/ﬁles/Automated%20Whitebox%
20Fuzz%20Testing%20(paper)%20(Patrice%20Godefroid).pdf

[10] P. Purdom, “A sentence generator for testing parsers,” BIT Numerical

Mathematics, vol. 12, no. 3, pp. 366–375, 1972.

[11] M. Utting, A. Pretschner, and B. Legeard, “A Taxonomy of Model-Based
Testing,” Department of Computer Science, The University of Waikato,
New Zealand, Tech. Rep, vol. 4, 2006.

[12] M. Zalewski, “American fuzzy lop,” http://lcamtuf.coredump.cx/aﬂ/.
[13] P. Godefroid, H. Peleg, and R. Singh, “Learn&fuzz: Machine
in 32nd IEEE/ACM International
learning for
Conference on Automated Software Engineering (ASE 2017), January
2017. [Online]. Available: https://www.microsoft.com/en-us/research/
publication/learnfuzz-machine-learning-input-fuzzing/

fuzzing,”

input

[14] K. V. Hanford, “Automatic generation of test cases,” IBM Systems

Journal, vol. 9, no. 4, pp. 242–257, 1970.

[15] O. Bastani, R. Sharma, A. Aiken, and P. Liang, “Synthesizing
program input grammars,” in Proceedings of the 38th ACM SIGPLAN
Conference on Programming Language Design and Implementation,
ser. PLDI 2017. New York, NY, USA: ACM, 2017, pp. 95–110.
[Online]. Available: http://doi.acm.org/10.1145/3062341.3062349
[16] W. Cui, M. Peinado, K. Chen, H. J. Wang, and L. Irun-Briz, “Tupni:
Automatic reverse engineering of input formats,” in Proceedings of the
15th ACM Conference on Computer and Communications Security, ser.
CCS ’08. New York, NY, USA: ACM, 2008, pp. 391–402. [Online].
Available: http://doi.acm.org/10.1145/1455770.1455820

inputs using dynamic tainting,” in Proceedings of

[17] J. Clause and A. Orso, “Penumbra: Automatically identifying failure-
the
relevant
Eighteenth International Symposium on Software Testing and Analysis,
ser. ISSTA ’09. New York, NY, USA: ACM, 2009, pp. 249–260.
[Online]. Available: http://doi.acm.org/10.1145/1572272.1572301
[18] M. H¨oschele and A. Zeller, “Mining input grammars with autogram,”
the 39th International Conference on Software
in Proceedings of
Engineering Companion, ser. ICSE-C ’17.
Piscataway, NJ, USA:
IEEE Press, 2017, pp. 31–34. [Online]. Available: https://doi.org/10.
1109/ICSE-C.2017.14

[19] C. Szepesv´ari, “Algorithms for reinforcement learning,” Synthesis lec-
tures on artiﬁcial intelligence and machine learning, vol. 4, no. 1, pp.
1–103, 2010.

[20] C. Wattkins, “Learning from delayed rewards,” Ph.D. dissertation, Cam-

bridge University, 1989.

[21] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.

3-4, pp. 279–292, 1992.

[22] PDF Reference, 6th ed., Adobe Systems Incorporated, Nov. 2006, avail-
at http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/

able
pdfs/pdf reference 1-7.pdf.

[23] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan,
P. Warden, M. Wicke, Y. Yu, and X. Zheng, “Tensorﬂow: A system for
large-scale machine learning,” in 12th USENIX Symposium on Operating
Systems Design and Implementation, OSDI 2016, Savannah, GA, USA,
November 2-4, 2016., 2016, pp. 265–283.

