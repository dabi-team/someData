Learning to Branch: Accelerating Resource
Allocation in Wireless Networks

Mengyuan Lee, Guanding Yu, Senior Member, IEEE, and Geoffrey Ye Li Fellow, IEEE

1

9
1
0
2

v
o
N
6
1

]
T
I
.
s
c
[

3
v
9
1
8
1
0
.
3
0
9
1
:
v
i
X
r
a

Abstract—Resource allocation in wireless networks, such as
device-to-device (D2D) communications, is usually formulated as
mixed integer nonlinear programming (MINLP) problems, which
are generally NP-hard and difﬁcult to get the optimal solutions.
Traditional methods to solve these MINLP problems are all based
on mathematical optimization techniques, such as the branch-
and-bound (B&B) algorithm that converges slowly and has
forbidding complexity for real-time implementation. Therefore,
machine leaning (ML) has been used recently to address the
MINLP problems in wireless communications. In this paper, we
use imitation learning method to accelerate the B&B algorithm.
With invariant problem-independent features and appropriate
problem-dependent feature selection for D2D communications,
a good auxiliary prune policy can be learned in a supervised
manner to speed up the most time-consuming branch process
of the B&B algorithm. Moreover, we develop a mixed training
strategy to further reinforce the generalization ability and a deep
neural network (DNN) with a novel loss function to achieve better
dynamic control over optimality and computational complexity.
Extensive simulation demonstrates that the proposed method can
achieve good optimality and reduce computational complexity
simultaneously.

Index Terms—Machine learning, device-to-device communica-
tions, resource allocation, mixed integer nonlinear programming,
imitation learning, branch-and-bound algorithm

I. INTRODUCTION

To meet with the increasing demand for higher data rate
and better QoS requirements, device-to-device (D2D) com-
munications have been proposed as a promising technique
for the LTE-Advanced networks and have recently attracted
signiﬁcant attention from both academic and industry com-
munities [1]–[5]. The key idea of D2D communications is
enabling proximity users to communicate directly by reusing
the wireless spectrum of conventional cellular users (CUs).
Compared with traditional cellular communications, this new
technique can signiﬁcantly reduce the trafﬁc load in the core
network and increase the network throughput as well. How-
ever, D2D communications may generate severe interference
to the existing CUs due to spectrum reusing. Proper resource

Copyright (c) 2015 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to pubs-permissions@ieee.org.
This work was supported by the National Key Research and Development
Program of China under Grant 2018YFB1802302 and by the Fundamental
Research Funds for the Central Universities. This paper was presented in part
at IEEE VTC 2019-Fall, Honolulu, HI, USA, Sep. 2019.

M. Lee and G. Yu are with the Zhejiang Provincial Key Laboratory of In-
formation Processing, Communication and Networking, Zhejiang University,
Hangzhou 310027, China. e-mail: {mengyuan lee, yuguanding}@zju.edu.cn.
G. Y. Li is with the School of ECE, Georgia Institute of Technology,

Atlanta, GA, USA. e-mail: liye@ece.gatech.edu.

allocation can improve the system performance while reducing
the co-channel interference between D2D and CUs.

Resource allocation for D2D communications and other
wireless networks is usually formulated as mixed integer non-
linear programming (MINLP) problems, which are in general
NP-hard and no efﬁcient global optimal algorithm is available
yet. Existing solutions to those MINLP problems are usually
based on various mathematical optimization techniques. In
general, the optimal solutions to the MINLP problems can
only be achieved by the branch-and-bound (B&B) algorithm
[6]. However, the worst-case computational complexity of
the B&B algorithm is exponential, causing it
impractical
for real-time implementation. Therefore, some works aim to
develop near-optimal or sub-optimal algorithms and reduce
computational complexity, such as methods based on game
theory [7], [8] or graph theory [9], [10]. Heuristic algorithms,
which alternatively or iteratively update the combinatorial and
continuous variables, have also been widely investigated [11],
[12]. The sub-optimal methods usually suffer from two major
shortcomings. On the one hand, the performance gaps between
the sub-optimal solutions and the optimal ones are hard to
control. On the other hand, many iterative based heuristic
algorithms still have high computational complexity for real-
time implementation although they are faster than the B&B
algorithm.

The aforementioned two disadvantages can be overcome
by ML techniques. Its inference stage is generally very fast
and we can adopt different techniques to improve and control
the output performance. Therefore, this paper incorporates
the machine learning (ML) technique and mathematical opti-
mization technique to address the resource allocation in D2D
communications, which can be also applied to resource allo-
cation in other wireless networks. We are inspired by recently
emerged studies on applying the ML techniques to address
mathematical optimization problems in wireless communica-
tions [13]–[19]. In [13], [14], deep neural networks (DNNs)
have been utilized to deal with the power control problems
in wireless networks. In [15]–[17], reinforcement learning has
been adopted to solve resource allocation problems for various
wireless networks. In [18], spatial learning has been developed
to schedule interfering links in D2D networks without the
need of channel state information (CSI). In [19], linear sum
assignment problems (LSAPs), which are often encountered in
wireless communications, have been solved based on DNNs.
All the above studies follow the end-to-end learning paradigm
[13] and treat a given resource optimization problem as a
“black box” to learn its input/output relation by various ML
techniques. This end-to-end paradigm is very suitable for

 
 
 
 
 
 
optimization problems with only one kind of output variables.
For examples, problems in [13]–[15] only include continuous
output variables, whereas [18], [19] only involve combinatorial
ones. However, such end-to-end paradigm is hard to effectively
solve the MINLP problems due to the more complicated
algorithm structures.

Therefore, this paper exploits the speciﬁc algorithm struc-
tures to solve the MINLP problems for resource allocation in
D2D communications using ML technique. Speciﬁcally, we
formulate a mathematical problem to maximize the minimum
data rate among D2D pairs by jointly optimizing the chan-
nel allocation and power control. Instead of directly using
mathematical
techniques to solve it, we turn to a hybrid
ML and mathematical method. We propose to leverage the
imitation learning to reduce the computational complexity of
the B&B algorithm by accelerating the most time-consuming
branch process. The imitation learning has been ﬁrst proposed
in [20] for the mixed integer linear programming (MILP)
problems. Although this very approach has been introduced
in wireless communications in [21] to solve the resource
allocation problems in cloud radio access networks (Cloud-
RANs), there is no work applying the imitation learning to
solve D2D resource allocation problems.

Imitation learning can be further converted into a binary
classiﬁcation problem with proper feature design, which can
be solved by the classical support vector machine (SVM) in
a supervised manner. To improve the learning performance,
we carefully select the problem-dependent features for the
resource allocation problems in D2D communications in addi-
tion to the problem-independent features in [20]. The problem-
dependent features include the power constraint of each D2D
link as well as the CSI of both D2D and interference links.
We further investigate the inﬂuence of the problem-dependent
features in terms of optimality and computational complexity.
The ML based algorithm is expected to have few needed
training samples, fast training speed, and good generaliza-
tion ability. Therefore, we make the following three major
improvements on the speciﬁc learning algorithm as compared
with the work in [20], [21]. First, we simplify the learning
task without performance loss by learning an auxiliary prune
policy to prune the non-optimal nodes that are not fathomed.
This improvement leads to fewer needed training samples and
faster training speed than the existing one that discards the
original prune policy in the B&B algorithm and learns a new
one to prune all the non-optimal nodes. Second, we propose to
only collect the nodes that are visited under current policy and
are not fathomed by the original B&B prune policy to further
accelerate the training process. This modiﬁcation can also
mitigate the dataset imbalance issue and improve the accuracy
of the binary classiﬁer. Third, we develop a mixed training
strategy to reinforce the generalization ability of the imitation
learning method. A novel loss function is also developed when
utilizing the DNN as the binary classiﬁer to dynamically con-
trol the optimality and computational complexity. By extensive
simulation results, we verify the effectiveness of the proposed
method for D2D communications and highlight some insights
about practical implementation as well.

The rest of this paper is organized as follows. In Section

2

II, we formulate resource allocation into an MINLP problem.
In Section III, we will transform the problem into a more
tractable one and develop the optimal B&B algorithm to solve
it. Section IV introduces the imitation learning method to
accelerate the B&B algorithm. In Section V, we present test
results of the proposed method, which motivate us to make two
further improvements in Section VI. Finally, we will conclude
this paper in Section VII.

II. PROBLEM FORMULATION

In this section, we formulate the resource allocation in
wireless networks into an MINLP problem using D2D com-
munications as an example.

A. Resource Allocation in D2D Communications

As depicted in Fig. 1, we consider an uplink single-cell
system with K CUs in a set K = {1, ..., K} and L D2D
pairs in a set L = {1, ..., L}. We assume that each uplink
CU connects to the Evolved NodeB (eNB) with an orthogonal
channel. Moreover, we assume that D2D pairs transmit data by
reusing the uplink channels of CUs. In the D2D networks, the
number of D2D pairs is usually smaller than that of cellular
users [22], [23]. Therefore, we assume K ≥ L in this paper.
Nevertheless, our proposed method can also be utilized while
K ≤ L.

(cid:12)(cid:2)

$
!"

(cid:11)(cid:2)

(cid:8)(cid:2)
(cid:11)(cid:16)(cid:28)(cid:26)(cid:16)(cid:1)(cid:27)(cid:18)(cid:16)(cid:1)
(cid:15)(cid:18)(cid:14)(cid:23)(cid:23)(cid:16)(cid:21)(cid:1)(cid:24)(cid:17)(cid:1)(cid:7)(cid:13)(cid:2)

$’
!"

#’
!&

(cid:7)(cid:13)(cid:3)

(cid:7)(cid:24)(cid:22)(cid:22)(cid:28)(cid:23)(cid:19)(cid:15)(cid:14)(cid:27)(cid:19)(cid:24)(cid:23)(cid:1)(cid:21)(cid:19)(cid:23)(cid:20)

(cid:9)(cid:23)(cid:27)(cid:16)(cid:25)(cid:17)(cid:16)(cid:25)(cid:16)(cid:23)(cid:15)(cid:16)(cid:1)(cid:21)(cid:19)(cid:23)(cid:20)

(cid:7)(cid:13)(cid:2)

#$
!""

#’
!"

$’
!&

(cid:12)(cid:3)

$
!&

(cid:16)(cid:10)(cid:6)

#’
!(

(cid:7)(cid:13)(cid:4)

(cid:11)(cid:3)

(cid:8)(cid:3)
(cid:11)(cid:16)(cid:28)(cid:26)(cid:16)(cid:1)(cid:27)(cid:18)(cid:16)(cid:1)
(cid:15)(cid:18)(cid:14)(cid:23)(cid:23)(cid:16)(cid:21)(cid:1)(cid:24)(cid:17)(cid:1)(cid:7)(cid:13)(cid:5)

#$
!%&

#’
!%

(cid:7)(cid:13)(cid:5)

Fig. 1. System model.

As in Fig. 1, we denote hCD

kl as the instantaneous channel
power gain of the interference link between CU k and the
receiver of D2D pair l, hCB
k as the channel power gain between
CU k and the eNB, hD
l as the channel power gain between
D2D pair l, and hDB
as the channel power gain of the
interference link between the transmitter of D2D pair l and
the eNB.

l

We further introduce ρ = [ρkl] as the indicator vector of
the channel allocation. Speciﬁcally, ρkl = 1 if the channel of
CU k is reused by D2D pair l, and ρkl = 0 otherwise. Denote
pC = [pC
kl] as the transmit power vectors for
K CUs and L D2D pairs, respectively, where pC
k denotes the
allocated transmit power for CU k and pD
kl denotes the power
of D2D pair l on the channel of CU k.

k] and pD = [pD

Since each CU channel is assumed to be reused by at
most one D2D pair, the signal-to-interference-plus-noise ratio

(SINR) of D2D pair l on the channel of CU k can be written
as

SIN RD

kl(pC, pD, ρ) =

ρklpD
N + pC
σ2

klhD
l
khCD
kl

,

where σ2
N denotes the power of the additive white Gaussian
noise (AWGN). Similarly, the SINR achieved by CU k can be
expressed as

SIN RC

k(pC, pD, ρ) =

pC
khCB
k
l∈L ρklpD

klhDB
l

.

σ2
N +

Accordingly, the data rates in bits per second per hertz (i.e.,
normalized by the channel bandwidth) of CU k and D2D pair
l on all channels can be written as

P

RC

k (pC, pD, ρ) = log(1 + SIN RC

k(pC, pD, ρ)),

(1)

RD

l (pC, pD, ρ) =

ρklRD

kl(pC, pD, ρ)

Xk∈K

=

ρkl log(1 + SIN RD

kl(pC, pD, ρ)),

Xk∈K

(2)
kl is the data rate of D2D pair l on the

respectively, where RD
channel of CU k.

Resource allocation in D2D communications is to determine
pC, pD, and ρ to optimize the overall network performance.

B. Problem Formulation

To achieve fairness among different users, we speciﬁcally
consider a resource allocation problem to maximize the min-
imum data rate of D2D pairs in this paper with the following
the minimum data rate of each CU is
constraints. First,
required to be no less than given thresholds. Second, the power
of individual links is also constrained. Finally, as mentioned
above, each channel can be reused by at most one D2D pair
to limit the interference between different D2D pairs. There-
fore, the resource allocation problem can be mathematically
formulated as

max
,pD

{pC

,ρ}

RD

l (pC, pD, ρ),

min
l∈L

subject to

ρkl ∈ {0, 1},

∀k ∈ K , l ∈ L ,

ρkl ≤ 1,

∀k ∈ K ,

Xl∈L

ρklpD

kl ≤ P D

max,

Xk∈K

RC

k(pC, pD, ρ) ≥ RC

min,

∀l ∈ L ,

∀k ∈ K ,

(3)

(3a)

(3b)

(3c)

(3d)

(3e)

∀k ∈ K ,

k ≤ P C
pC

max,
min is the minimum rate of CUs, P C

where RC
max
are the maximum transmit power level of CUs and D2D
pairs, respectively. Note that, the proposed algorithm can be
extended to other resource allocation problems with different

max and P D

3

objective functions since the B&B algorithm is a general
approach to solve MINLP problems.

Problem (3) is a typical resource allocation problem in D2D
communications. Similar resource allocation problems can be
also found in other wireless networks. It is an MINLP problem,
which is NP-hard in general. The B&B algorithm can ﬁnd its
optimal solution but with high computational complexity [6].
In the next section, we will focus on developing the B&B al-
gorithm for Problem (3) as the preparation for aforementioned
ML-based acceleration process.

III. OPTIMAL BRANCH-AND-BOUND ALGORITHM

The B&B algorithm is widely used for MINLP problems.
However, most existing studies on resource allocation have
focused on heuristic algorithms and only used the B&B
algorithm as the baseline without detailed development due
to its high computational complexity. In this paper, we will
accelerate the B&B algorithm with the help of an ML tech-
nique, thus developing the optimal B&B algorithm in detail
is necessary. Therefore, in this section, we will ﬁrst transform
Problem (3) into a more tractable one and then develop the
optimal B&B algorithm to solve it.

A. Problem Transformation

To transform Problem (3) into a more tractable one, we
ﬁrst derive a proposition about the optimal power allocation
of D2D pair l on the channel of CU k in the following, which
is proved in Appendix A.

Proposition 1: If D2D pair l is allowed to reuse the channel
of CU k, then the optimal power of CU k can be written as

pC
k =

(2RC

min − 1)(σ2
hCB
k

N + pD

klhDB
l )

,

where pD

kl ∈ [0, pmax
kl = min{(1/hDB
pmax

kl

] and

l )(pC

maxhCB

k /(2RC

min − 1) − σ2

N ), P D

max}.

Based on Proposition 1 and the derivation in Appendix B,
we can rewrite the data rate of D2D pair l on all channels as
a function of {pD, ρ} as stated in the following.

Proposition 2: The data rate of D2D pair l on all channels
deﬁned in (2) can be rewritten into a function of pD and ρ as

RD

l (pC, pD, ρ) =

log(1 +

ρklpD
kl
akl + bklpD
kl

) , ˆRD

l (pD, ρ),

where akl , σ2
(2RC
min − 1)hCD
From the above discussion, Problem (3) is equivalent to the

N /hD
kl hDB

min − 1)hCD

and bkl ,

l /hD

N /hD

l hCB
k

kl σ2

following optimization problem

max
{pD
,ρ}

min
l∈L

ˆRD

l (pD, ρ),

subject to (3a), (3b), (3c), and

pD
kl ≤ pmax

kl

,

∀k ∈ K , l ∈ L .

(4)

(4a)

Xk∈K
l + (2RC
l hCB
k .

B. Optimal Branch-and-Bound Algorithm

We introduce a new variable skl = pD

kl ˆρkl. Then

We propose to use the B&B algorithm to solve Problem
(4). Since the integer variable in Problem (4) is binary, the
B&B algorithm can attain the globally optimal solution by
iteratively searching a binary tree. Each node n in the tree is
associated with a nonlinear sub-problem of Problem (4), where
the constraints of the integer variables, i.e., constraint (3a), are
modiﬁed. Speciﬁcally, some integer variables are determined
while others are undetermined and relaxed into continuous
variables within [0, 1]. By solving the corresponding nonlinear
problem at node n, its local upper bound, bn
U , can be obtained,
because the feasible region of the corresponding nonlinear
problem is larger than that of the original MINLP problem.
The searching process of the B&B algorithm contains three
iterative steps.

• Node selection: selecting a node from the unvisited node

list of the tree.

• Fathom decision: using the local upper bound, bn

• Evaluation: solving the corresponding nonlinear sub-
problem of the node to obtain its local upper bound, bn
U .
U , and
the global lower bound, bL, which is the optimal value of
the objective function by far, to decide whether the node
should be fathomed.

The searching process comes to an end by iteratively repeating
the above three steps until the node list is empty.

In the following, we will ﬁrst solve the corresponding
nonlinear problem at each node, and then develop the complete
B&B algorithm for Problem (4).

1) Local Upper Bound Algorithm: As mentioned before, at
each node of the binary tree generated by the B&B algorithm,
some channel allocation indicators are determined while others
are relaxed into a continuous variable within [0, 1]. We use ρd
to denote the matrix that stores the values of the determined
channel allocation indicators. Then the nonlinear sub-problem
of Problem (4) corresponding to node n(ρd) can be written as

subject to

max
, ˆρ}
{pD

min
l∈L

ˆRD

l (pD, ˆρ, ρd),

ˆρkl ∈ [0, 1],

∀k ∈ K , l ∈ L ,

ˆρkl ≤ 1,

∀k ∈ K ,

Xl∈L

ˆρkl = ρd
kl,

∀{k, l} ∈ Dn,

ˆρklpD

kl ≤ P D

max,

∀l ∈ L ,

Xk∈K

pD
kl ≤ pmax

kl

,

∀k ∈ K , l ∈ L ,

(5)

(5a)

(5b)

(5c)

(5d)

(5e)

where Dn is the index set of determined channel allocation
indicators. It is obvious that the solution to Problem (5) serves
as an upper bound of Problem (4).

4

(6)

(7)

(7a)

(7b)

(7c)

(8)

pD
kl =

0,
skl/ ˆρkl,

(cid:26)

ˆρkl = 0,
otherwise.

Then Problem (5) can be modiﬁed into

η(ρd),

max
{η,s, ˆρ}

subject to (5a), (5b), (5c), and

skl ≤ P D

max,

∀l ∈ L ,

Xk∈K

skl ≤ ˆρklpmax

kl

,

∀k ∈ K , l ∈ L ,

ˆRD

l (s, ˆρ) ≥ η,

∀l ∈ L ,

where

ˆRD

l (s, ˆρ) =

log(1 +

Xk∈K

ˆρklskl
akl ˆρkl + bklskl

).

Problem (7) is obviously equivalent to Problem (5). Fur-
thermore, we can prove that (8) is a concave function [24]
and Problem (7) is a convex optimization problem. Therefore,
the interior-point method can be used to solve it and thus the
local upper bound of each node can be obtained, which will
be used to decide whether a node should be fathomed during
the B&B searching process.

2) B&B Algorithm: After obtaining the local upper bound
of each node, we can develop the B&B algorithm for Problem
(4), which is summarized in Table I. For simplicity, we adopt
the depth-ﬁrst-search (DFS) [27] as the node selection rule
and always choose the ﬁrst undetermined channel indicator for
variable selection process. However, the algorithm in Table I
is impractical due to its exponential computational complexity,
i.e., O(2KL), which motivates us to accelerate it by ML
techniques.

IV. ACCELERATING BY IMITATION LEARNING

In this section, we will use the ML technique, speciﬁcally
the imitation learning method, to accelerate the algorithm in
Table I. We will ﬁrst discuss the optimal auxiliary prune policy
and then give a brief introduction to the concept of imitation
learning. Finally, we will introduce how to use it in our speciﬁc
problem and illustrate the detailed training process.

A. Optimal Auxiliary Prune Policy

During the B&B searching process in Table I, there are two
main goals: ﬁnding an optimal solution and guaranteeing its
optimality by searching all feasible solutions and comparing
with it. Most of the time is consumed in the latter one and a
good prune policy can signiﬁcantly reduce the computational
complexity. The more nodes are pruned, the less time is
consumed.

The original prune policy of the algorithm in Table I only

includes three cases:

• The sub-problem is infeasible as shown in line 6 of Table
I. If the relaxed nonlinear sub-problem corresponding to

TABLE I
OPTIMAL B&B ALGORITHM

Algorithm 1 Optimal B&B Algorithm
1: initialization

• Set node list: N ← {n([])}.
• Set global lower bound: bL = −∞.
• Set optimal solution: ρ∗ ← N one, pD ∗

← N one.

2: while N 6= ∅ do
3:
4:

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

Node Selection: pop the ﬁrst node n(ρd) in N .
Evaluation: use interior-point method solve Problem (7)
corresponding to n(ρd).
Fathom Decision:
if Problem (7) corresponding to n(ρd) is infeasible then

Go to step 2.

else

Get η, ˆρ, pD. Set bn
if ˆρ is integral then

U = η.

An integer solution is found.
if bn

U > bL then
Update global lower bound and optimal solution.
bL = bn

U , ρ∗ = ˆρ, pD ∗

= pD.

end if
Go to step 2.

else

if bn

U < bL then
Prune node n(ρd).
Go to step 2.

else

Branch on node n(ρd).
Variable Selection: branch on the ﬁrst undeter

-mined variable in ˆρ.
Add the new nodes into N .
Go to step 2.

24:
25:
26:
27:
end if
28:
29: end while

end if

end if

n(ρd) is infeasible, the related MINLP problem is also
infeasible and then the node n(ρd) is fathomed.

• A feasible solution is found as shown in line 10 of Table
I. If ˆρ is an integral vector, the result is also a feasible
solution of the related MINLP problem and then the node
n(ρd) is fathomed.

• The local upper bound is smaller than the current global
lower bound as shown in line 18 of Table I. If ˆρ is not an
integral vector and bn
U < bL, then the node n(ρd) would
not lead to a better solution and is fathomed.
Besides the three cases mentioned above, all

the non-
optimal nodes should be pruned to accelerate the B&B al-
gorithm. We can achieve this goal by using the method in
[20], [21], where the aforementioned original prune policy
in the B&B algorithm has been replaced with a learned
policy that would prune all the non-optimal nodes. However,
it is redundant to learn to prune the fathomed nodes since
the original prune policy is compact and explicit. Therefore,
we propose to simplify the learning task by keeping the
original prune policy and learning an auxiliary prune policy
to reinforce it. The auxiliary prune policy is supposed to
prune the non-optimal nodes that are not fathomed instead of
all the non-optimal ones. Our proposed simpliﬁcation about
learning task can reduce the complexity of target model.
Generally, simpler model has smaller Vapnik-Chervonenkis

5

(VC) dimension, which leads to fewer needed training samples
and speeds up the training process consequently. In this way,
the best auxiliary prune policy, π∗
p, should discard all non-
optimal nodes that have not been fathomed. In the following,
we introduce how to use imitation learning to learn the best
auxiliary prune policy, π∗
p.

B. Basic Imitation Learning

Imitation learning has been widely used in sequential deci-
sion problems, where the learner tries to mimic an expert’s
action, i.e., an oracle, to achieve the best performance in
a supervised manner [28]. Generally, a sequential decision
problem is deﬁned by a state space, S , an action space, A ,
and a policy space, Π. A policy π ∈ Π reﬂects the mapping
relations between the states and actions, i.e., π(s) = a. The
oracle, π∗, is a special policy that provides the optimal action,
a∗, for any possible state s ∈ S . The goal of imitation
learning is to ﬁnd a policy that mimics the oracle’s actions.

We shall note that

imitation learning is different from
reinforcement learning. Although the goal of reinforcement
learning is also to learn a policy, the best policy for reinforce-
ment learning is unknown and should be learned by interacting
with the environment in an unsupervised manner. Whereas, the
best policy in imitation learning is known and can be learned in
a supervised manner. Imitation learning better ﬁts accelerating
the B&B algorithm than reinforcement learning.

The B&B searching process in Table I can be formulated
as a sequential decision-making process. At each node en-
countered during the searching process, we need to decide
whether it is pruned using the prune policy and the decision
can inﬂuence the whole B&B searching process. Here, the
state space, S , is the set of all the visited nodes with the
corresponding global lower bound and the current optimal
solution. The action space, A , is {prune, branch} and the
policy space is deﬁned as Πp.

Let φ : S → Rq be the feature mapping that ﬁnds a q-
dimension feature-vector description for any state s ∈ S .
Given that our goal is to learn the best auxiliary prune policy,
π∗
p, i.e., learn the optimal action, a∗, for any possible state
s ∈ S , the imitation learning problem can be converted into a
supervised learning problem: the policy takes a feature-vector
description of the state s as input and tries to predict the oracle
action, a∗, as output. Moreover, because the action space, A ,
is only two-dimensional, the imitation learning problem can
be further converted into a binary classiﬁcation problem.

C. Imitation Learning for Auxiliary Prune Policy

Now, we utilize the imitation learning method to learn
the optimal auxiliary prune policy. As mentioned above, the
imitation learning problem can be converted into a binary
classiﬁcation task with appropriate feature mapping, φ. In this
part, we will ﬁrst discuss how to design appropriate feature
mapping, and then introduce how to use SVM to train the
binary classiﬁer.

1) Feature Design: Feature design is very important for
training classiﬁers. We need to dig out the features that are
closely related to state s ∈ S . In [20], problem-independent
features have been used for general MILP problems, which
can be extended to the MINLP problems. On the other hand,
problem-dependent features have been exploited in [21] in
addition to the problem-independent ones to further improve
the performance of the classiﬁer. Here, we adopt both kinds
of features.

i. Problem-Independent Features:
Problem-independent features focus on the structure of the
binary tree generated by the B&B algorithm, which can be
used in all MINLP problems regardless of applications. It
consists of the following three categories [20].

• Node features: State s is highly relevant to current visiting
node n, thus features computed from current node n are
essential. They include the depth of node n, the plunge
depth of node n, and the local upper bound, bn
U , of node
n.

• Branching feature: State s also depends on the branching
variable leading to current node n, which is chosen at
the variable selection step of the parent of node n. It is
less important than node features in the auxiliary prune
policy learning problem [20]. Here we use the value of
the branching variable as the only branching feature.
• Tree features: Features computed from the binary tree
are also important for describing state s, which includes
current global
lower bound, bL, and the number of
solutions obtained so far.

Most of the features mentioned above vary largely for
different problems. For example, the objective values of prob-
lems with different CSI vary a lot. On the other hand, the
plunge depths of nodes vary with problem sizes. Therefore, we
normalize the features involving bound with the local upper
bound at the root node as well as those involving depth with
the maximum depth of the binary tree. This normalization
process makes the above features size-independent, i.e., in-
dependent on the numbers of CUs and D2D pairs, which can
help improve the generalization ability of the proposed learned
prune policy.

ii. Problem-Dependent Features:
Problem-dependent features, as the name implies, are
closely related to the speciﬁc problem we solve. To choose
appropriate problem-dependent features, we need to check the
original Problem (3) and ﬁnd the key parameters related to
the channel and power allocation for D2D communications.
It is obvious that CSI and power constraints are two key fac-
tors. Therefore, we try to design problem-dependent features
according to these two factors in the following.

k , hD

l , and hDB

• CSI feature: At node n, there are four different kinds
of CSI involved, i.e., hCD
kl , hCB
, when the
ﬁrst undetermined channel allocation indicator is ˆρkl. In
Proposition 2, we have introduced two new variables
akl and bkl that combine these four kinds of CSI with
the data rate of D2D pair l on the channel of CU
kl. According to (9), we can formulate the
k,
CSI feature as log(1 + 1/(akl + bkl)). On the other

i.e., RD

l

6

hand, both akl and bkl are related to RC
min that varies
across different problems. Therefore, we normalize the
CSI feature with RC
min and get the size-independent CSI
feature as f (CSI) = (log(1 + 1/(akl + bkl)))/RC

min.

• Power feature: At node n, when the ﬁrst undetermined
channel allocation indicator is ˆρkl, we formulate a func-
tion of pkl as the power feature. According to (4a),
. Because pmax
we can denote the power feature as pmax
kl
kl
also varies across different problems,
the normalized
power feature can be similarly formulated as g(pkl) =
KLpmax
kl /

pmax
kl

K

L

.

Pk=1

Pl=1

These two aforementioned problem-dependent features are
also size-independent. Combined with the size-independent
problem-independent features, we can expect that the auxiliary
prune policy we learn from the oracle of speciﬁc scenarios
can be applied to scenarios with different sizes, i.e., different
numbers of CUs and D2D pairs, which will be conﬁrmed by
the test results in Section V.

2) Binary Classiﬁer Learning: After choosing an appropri-
ate feature mapping, φ, we can handle the binary classiﬁcation
problem. We use SVM to train the classiﬁer, which is widely
used for classiﬁcation. The SVM aims to ﬁnd a mapping of
training examples so that they can be divided by a clear gap
that is as wide as possible in the new space [29]. We stack the
eight features mentioned above in an 8-dimension vector as
the input and use the corresponding action, a∗, as the output.
Moreover, we denote branch, i.e., optimal node, as 1, and
prune, i.e., non-optimal node, as 0.

During the B&B searching process, mistakes made at early
stages are more serious. For example, the root node is always
supposed to be classiﬁed as branching. If it is misclassiﬁed as
pruning, the searching process ends and no feasible solution
can be found. On the other hand, it is more serious to prune the
optimal nodes by mistake than to keep non-optimal nodes. If
an optimal node is misclassiﬁed as non-optimal and is pruned,
the optimal solution cannot be found either. However, if a
non-optimal node is misclassiﬁed as optimal, just more time
is consumed but the optimality of result is still preserved.

−Bd

To deal with the ﬁrst problem mentioned above, we place
higher weights on the training examples from the nodes with
small depth. Because the subtree size decreases exponentially
with the node’s depth, the weight related to node’s depth, ω1, is
also supposed to decrease exponentially for nodes at different
depths. Therefore, we set ω1 for a node with depth d as ω1 =
D , where D is the maximum depth of the B&B tree,
Ae
A and B are the parameters to be tuned during the training
process. Speciﬁcally, we set A = 5 and B = 2.68 according
to [20], which enables the weight of the root node to be 5 and
the weights for nodes at different depths decrease at the rate
2.68/D. Nevertheless, different values of A and B will impose
different impacts on the results. Increasing A will reinforce the
optimality by placing higher weights while increasing B will
increase the decay rate and differentiate more clearly for nodes
at different depths. In practice, we can choose appropriate A
and B according to different goals.

As for the second problem mentioned above, we place
higher weights on the training examples from optimal nodes

and denote the weight related to node’s optimality as ω2. We
set ω2 as 1 for non-optimal nodes and tune it for optimal
ones from {1, 2, 4, 8} during the training process. There are
two main beneﬁts of placing ω2: avoiding the serious optimal
node misclassiﬁcation issue mentioned above and dealing with
the dataset imbalance issue. Dataset imbalance issue means
the number of the optimal nodes is much smaller than that
of the non-optimal ones, which can be mitigated by placing
higher weights at the optimal nodes. Combined these two parts
mentioned above, the total weight, ω, for each training sample
is the product of ω1 and ω2, i.e., ω = ω1 × ω2.

D. Iterative Training Process

The binary classiﬁer is trained in the supervised manner.
the nodes searched by the
The general method puts all
algorithm in Table I into the training set. However, this method
is memory-consuming because the number of nodes increases
exponentially. Therefore, we adopt an iterative training al-
gorithm, dataset aggregation (DAgger) [20], [30], to achieve
more efﬁcient training.

DAgger is widely used in the imitation learning to provide a
learning reduction with strong performance guarantee [30]. It
proceeds by collecting a dataset at each iteration using current
policy and then trains a new policy with the aggregation of
all collected datasets. The iterative dataset collection process
can enhance the generalization ability of the model to some
extent.

The detailed training procedures for imitation learning using
DAgger are shown in Tables II and III. At the ﬁrst iteration,
p, as the initial policy and denoted as π(1)
we use the oracle, π∗
p .
We search each problem in the problem set, Q, with π(1)
and
collect data into the training dataset, T . Then we use T to
train SVM and get a new learned policy denoted as π(2)
p . We
repeat this process for M rounds and choose the policy, π(m)
,
that performs the best on the validation set.

p

p

Note that, some steps in Algorithm 3 are similar with those
in Algorithm 1 because the B&B tree needs to be searched in
both algorithms. However, Algorithm 3 is mainly developed
to collect the training data for the iterative training process
in Table II. The new learned policy has been adopted in the
Algorithm 3 for pruning decision, which makes it different
from that in Algorithm 1.

Here, we make some improvements on the training process
imbalance
to accelerate it and also deal with the dataset
problem. Speciﬁcally, we only collect the nodes that are visited
under current policy and are not fathomed by the original B&B
prune policy during the data collection process shown in Table
III. It is different from the data collection process in [20] and
[21], which includes all the visiting nodes into the dataset.
There exist two main reasons to make this improvement.

7

TABLE II
TRAINING PROCESS FOR IMITATION LEARNING WITH DAGGER

Algorithm 2 Training Process for Imitation Learning with
DAgger
1: initialization

• Set policy: π(1)
• Set training dataset: T = ∅.

p = π∗
p.

for problem Q in Q do

T (Q) ← DataCollection(Q, π(m)
T ← T ∪ T (Q)

p

)

2: for m = 1 to M do
3:
4:
5:
6:
7:
8: end for
9: return Best π(m)

end for
π(m+1)
p

p

on the validation set.

← train SVM (classiﬁer) using T .

cannot be neglected during the data collection process.
Therefore, the improvement on the training process is
directly led by keeping the original prune policy.

• The nodes that are fathomed by the original B&B prune
policy are non-optimal. Therefore, discarding these nodes
from the dataset can decrease the number of non-optimal
nodes and mitigate the dataset imbalance issue mentioned
above to some extent.

TABLE III
DATA COLLECTION ALGORITHM: DATACOLLECTION(Q,πp)

Algorithm 3 DataCollection(Q, πp)
1: initialization

• Set node list: NQ ← {n([])}.
• Set dataset: D = ∅.

2: while NQ 6= ∅ do
3:
4:

Node Selection: pop the ﬁrst node nQ(ρd) in NQ.
Evaluation: use interior-point method solve Problem (7)
corresponding to nQ(ρd).
Fathom Decision:
if Node nQ(ρd) is fathomed by original B&B then

Go to step 2.

else

p(nQ(ρd))}

D ← {φ(nQ(ρd)), π∗
if πp(nQ(ρd)) = branch then
Branch on node nQ(ρd).
Variable Selection: branch on the ﬁrst undetermined

variable in ˆρ.

Add the new nodes into NQ.

5:
6:
7:
8:
9:
10:
11:
12:

end if
Go to step 2.

13:
14:
15:
end if
16:
17: end while
18: return D

V. PERFORMANCE TEST RESULTS

• When visiting nodes that are fathomed by the original
B&B prune policy, the learned auxiliary prune policy is
not needed. Therefore, ignoring those fathomed nodes
would not induce loss in performance but can speed up
the training process of imitation learning. Note that if we
discard the original prune policy and learn a new one to
replace it as proposed in [20], [21], the fathomed nodes

In this section, we will test the performance of the proposed
imitation learning method on accelerating the algorithm in
Table I. All the codes are implemented in python 3.6 except the
interior-point algorithm that is implemented in Matlab. They
are implemented on a computer station with one 4-core Intel
processor and 128 GB of memory. To avoid over-ﬁtting, we
set M = 4 for all the imitation learning training process and

use LIBSVM [31] for the step of training SVM (classiﬁer)
in the following. Given that the main goal of this paper is
using a ML technique to accelerate the B&B algorithm, we
mainly test the performance of the ML technique with speciﬁc
focus on scalability and generalizability. Moreover, to honor
the tradition in the ML community, all the testing results are
presented in tables with speciﬁc numerical values.

A. System Setup

We consider a single-cell network with a radius of 500 m as
in Fig. 1. The eNB is located in the center of the cell and the
CUs are distributed uniformly in the cell. According to [18],
the transmitter of each D2D pair is also distributed uniformly
in the cell and the corresponding receiver is distributed in
a disk centered by the transmitter with ﬁxed uniform link
distance distribution between rmin and rmax. Our simulation
parameters are summarized in Table IV.

TABLE IV
SIMULATION PARAMETERS

Parameter
Cell radius
D2D distance, rmin, rmax
Noise spectral density
Path loss model
for cellular links
Path loss model
for cellular D2D links
Shadowing standard deviation
Maximum transmitter
power of CU, P C
max
Maximum transmitter
power of D2D pair, P D
max
Minimum data rate of CU, RC

min

Value
500 m
15 m, 50 m
-174 dBm/Hz

128.1+37.6log(d[km])

148+40log(d[km])

10 dB

20 dBm

20 dBm

2 bit/s/Hz

B. Inﬂuence of the Number of Training Samples

An important test of the proposed method is how many
training samples are enough to learn a good auxiliary prune
policy since training samples are always difﬁcult and expen-
sive to get, especially in wireless networks where a large
number of real data are hard to obtain.

We test the performance of the proposed method with differ-
ent numbers of training samples for scenarios with different
numbers of CUs and D2D pairs. Because we usually have
a small labeled dataset for wireless problems, we set
the
ratio between the number of training samples and that of
testing samples as 10:1 for the subsequent test and use 20
testing samples for each scenario. During the testing stage, the
learned model will be applied to every encountered node while
searching the B&B tree for each testing sample. Given that the
number of nodes of the B&B tree is exponentially increasing,
20 testing samples include hundreds even thousands of testing
nodes, which are sufﬁcient for performance evaluation.

The testing results are shown in Table V. We use four
metrics to measure the performance of the proposed method.
Ogap is the optimality gap, which means the gap between the

8

TABLE V
PERFORMANCE OF IMITATION LEARNING WITH DIFFERENT NUMBERS OF
TRAINING SAMPLES
(a) K = 5, L = 2

Number of
training samples
Ogap
Speed
Optimal recognition
rate
Extra prune rate

Number of
training samples
Ogap
Speed
Optimal recognition
rate
Extra prune rate

Number of
training samples
Ogap
Speed
Optimal recognition
rate
Extra prune rate

50

3.88%
2.50x

100

3.23%
2.21x

150

2.27%
2.17x

200

2.01%
2.06x

89.22% 93.27% 93.33% 93.38%

30.88% 30.32% 29.00% 28.20%
(b) K = 7, L = 2

50

7.81%
9.51x

100

7.62%
4.91x

150

7.06%
4.89x

200

6.96%
4.82x

80.46% 86.92% 88.65% 90.21%

18.07% 38.24% 41.11% 40.96%
(c) K = 5, L = 3

50

100

150

200

14.54% 13.08% 12.54% 11.92%
11.66x
15.79x

13.91x

15.45x

83.33% 85.46% 86.21% 86.89%

42.22% 48.47% 50.00% 50.91%

optimal objective function and the one achieved by the pro-
posed imitation learning method. Speed refers to the speedup
with respect to the original B&B searching process, which is
computed as the ratio between the number of nodes explored
without and with the learned auxiliary policy. Note that all the
running time is tested with the same hardware set. Optimal
recognition rate is the accuracy rate of the learned policy on
recognizing optimal nodes, which is positively related to ogap.
Extra prune rate is the percentage of the non-optimal nodes
that are not fathomed by the original B&B prune policy but
can be recognized by the learned auxiliary policy. It is obvious
that speed and extra prune rate are also positively correlated.
Note that the values of ogap and speed are the average of the
20 testing samples while those of optimal recognition policy
and extra prune policy are the average of the hundreds and
even thousands of testing nodes of the B&B trees for the 20
testing samples.

Our algorithm is expected to attain a small objective func-
tion gap while reducing the computational complexity as much
as possible. As shown in Table V, the proposed method can
speed up the B&B searching process by 2.06 times with
only 2.01% loss of accuracy for the scenario with K = 5,
L = 2, and can speed up the B&B searching process by
4.82 times with 6.96% loss of accuracy for the scenario with
K = 7, L = 2, both with only 200 training samples. For
the more complicated scenario with K = 5, L = 3, the
proposed method can speed up the B&B searching process
by 11.66 times with 11.92% loss of accuracy while using
200 training samples. If reducing to 50 training samples, the
proposed method can still achieve low performance gaps of
3.88%, 7.81% and 14.54% for the three scenarios, respectively.

9

TABLE VI
PERFORMANCE OF IMITATION LEARNING WITH DIFFERENT SETS OF
FEATURES FOR THE SCENARIO WITH K = 5, L = 2

TABLE VII
CROSS GENERALIZATION TEST ON THE PERFORMANCE OF IMITATION
LEARNING FOR SCENARIOS WITH K = 5, L = 2 AND K = 7, L = 2

Feature set

Ogap
Speed
Optimal recognition
rate
Extra prune rate

Problem-independent
features only
3.11%
1.93x

Two kinds of
features combined
2.01%
2.06x

91.45%

26.48%

93.38%

28.20%

Cross
generalization
test combination
Ogap
Speed
Optimal
recognition
rate
Extra
prune rate

Policy(5,2)
on
Problem(5,2)
2.01%
2.06x

Policy(5,2)
on
Problem(7,2)
3.70%
3.46x

Policy(7,2)
on
Problem(5,2)
9.44%
3.25x

Policy(7,2)
on
Problem(7,2)
6.96%
4.82x

93.38%

90.65%

85.29%

90.21%

28.20%

24.77%

47.35%

40.96%

Comparing to the one million training samples used in [13],
the proposed imitation learning method can achieve a good
performance with a small set of training samples, and thus
is more feasible for wireless networks. On the other hand,
ogap and speed both decrease with the increase of the number
of training samples. As we can imagine, using more training
samples will
increase the optimality but will also induce
more computational complexity, which indicates the optimality
and computational complexity trade-off issue. Small training
sets lead to a policy with lower computational complexity
but also lower optimality, while larger training sets lead to
the opposite results. This can help us choose appropriate
numbers of training samples in practice according to our
speciﬁc goals instead of using as much training samples as
possible. Furthermore, with the increase on the number of
training samples, the performance for the scenario with K = 5,
L = 2 improves very slowly while those for the scenarios with
K = 7, L = 2 and K = 5, L = 3 improve at a stable speed.
And the achieved optimality for the scenario with K = 5,
L = 2 is always higher than those for the scenarios with
K = 7, L = 2 and K = 5, L = 3 when using the same
number of training samples. These results indicate that the
policy for the scenario with K = 5, L = 2 is fully trained
but those for the scenarios with K = 7, L = 2 and K = 5,
L = 3 are not yet. Larger training sets are needed for larger
scale problems than for smaller scale problems to achieve the
same optimality.

C. Inﬂuence of Feature Selection

Feature selection is very important for training classiﬁer
and we have used eight problem-independent and problem-
dependent features. As mentioned in Section IV-C, problem-
independent features are general for all MINLP problems
while problem-dependent ones need to be carefully selected
according to speciﬁc applications. We will test the importance
of both kinds of features in the following to obtain some
insights about feature selection in practice.

We test on the scenario with K = 5, L = 2 using
200 training samples and 20 testing samples. The results
of training with problem-independent features only and with
both kinds of features are summarized in Table VI. From the
table, the proposed method can speed up the B&B searching
process by 1.93 times with only 3.11% performance loss with
only problem-independent features while adding the problem-
dependent features can improve the optimality and reduce
computational complexity simultaneously. This result suggests

the necessity of the problem-dependent features in reinforcing
the performance.

Furthermore, we rank all the eight features using F-test,
i.e., joint hypotheses test, which is widely applied in feature
selection for ML. We ﬁnd that the most important two features
are the local upper bound, bn
U , and the global lower bound,
bL. The learned policy is inclined to branch on the nodes
whose difference between bn
U and bL are very small, which
means these nodes have very large probabilities to be the
optimal ones. The less two important features are two problem-
dependent features, which coincides with the fact mentioned
above that the proposed method can also achieve a satisfactory
performance without problem-dependent features.

Inspired from the above observation, we can implement the
imitation learning method in a two-step paradigm in practice.
First, we only use problem-independent features for training.
If the optimality and the computational complexity of the
learned policy is acceptable, then we can skip the intricate
design process of problem-dependent features. Otherwise, we
then add problem-dependent features and train a new learned
policy by using both kinds of features.

D. Generalization to Scenarios with Different Sizes

Generalization ability is another important property of ML
techniques. As mentioned in Section IV-C, we expect our
method can be generalized to scenarios with different problem
sizes. In the following, we ﬁrst test whether the proposed
imitation learning method has generalization ability, which
includes generalization abilities to both the larger and the
smaller scenarios. Then we test how strong its generalization
ability is.

First, we do cross generalization tests on scenarios with
K = 5, L = 2 and K = 7, L = 2. We use 200 training
samples and 20 testing samples for both scenarios. The results
are summarized in Table VII, where Policy(k, l) means the
policy learned from the scenario with K = k, L = l,
and Problem(k, l) means the problem on the scenario with
K = k, L = l. It is obvious that the proposed method has
the generalization ability to the larger and the smaller scale
problems. Speciﬁcally, the optimality by using Policy(5, 2)
on Problem(7, 2) is even higher than using Policy(7, 2) on
Problem(7, 2). This result seems not intuitive but is reasonable.
As mentioned above, Policy(7, 2) is not fully trained and
more training samples are needed to improve its performance.
Meanwhile, Policy(5, 2) is fully trained and has a strong gen-
eralization ability. Therefore, Policy(5, 2) can achieve better

10

TABLE VIII
PERFORMANCE OF IMITATION LEARNING FOR SCENARIOS
WITH DIFFERENT SIZES BY USING POLICY(5,2)

TABLE IX
PERFORMANCE OF IMITATION LEARNING FOR SCENARIOS
WITH DIFFERENT SIZES BY USING MIXED TRAINING STRATEGY

Problem size
Ogap
Speed
Optimal recognition
rate
Extra prune rate

(7,2)
(8,2)
(5,3)
3.70% 11.53% 5.46%
7.25x
4.87x
3.46x

(10,2)
7.69%
22.13x

90.65% 90.47% 89.78% 84.91%

24.77% 31.36% 26.87% 28.42%

Problem size
Ogap
Speed
Optimal recognition
rate
Extra prune rate

(7,2)
(8,2)
(5,3)
2.91% 10.59% 2.64%
7.90x
6.74x
3.39x

(10,2)
2.09%
2.46x

91.55% 91.55% 92.50% 95.35%

24.09% 42.45% 31.58%

3.9%

performance than Policy(7, 2) on Problem(7, 2). This result
suggests that if there are not enough training samples for large
scenario, using a fully trained policy from a small scenario
can achieve better performance than training a policy with
insufﬁcient samples from the large scenario.

After checking the existence of the generalization ability
of the proposed method, we now turn to test how strong its
generalization ability is, especially the generalization ability
to the larger scenarios. Given that Policy(5, 2) is fully trained
and will improve very little with more training samples, we
regard Policy(5, 2) learned with 200 training samples as the
best policy. We test the performance of this policy on scenarios
with different problem sizes. For each scenario, we use 20
testing samples. The result is shown in Table VIII, where
problem size (k, l) denotes the scenario with K = k, L = l.
Note that,
the largest scenario we consider here includes
10 CUs and 2 D2D pairs, because the training samples for
larger-scale scenarios are extremely hard to obtain due to the
exponentially increasing complexity of the B&B algorithm.

From Table VIII, the performance of Policy(5, 2) still re-
mains acceptable even for Problem(10, 2) whose complexity
is 210 larger than that of Problem(5, 2). The above result shows
that we can use the policy learned from the smaller scale
problems to the larger scale problems with good performance.
On the other hand, the optimal recognition rate decreases with
the increase of the problem scale. This result implies that the
generalization ability is constrained by the problem size of
training samples and deteriorates with the size of objective
problems. This phenomenon inspires us to use mixed training
strategy and soft-decision algorithm to further improve the
training performance in the next section.

VI. DISCUSSION AND FURTHER IMPROVEMENT

Inspired by the test results in Section V, we come up with
two important questions: how to strengthen the generalization
ability of the proposed imitation learning method, and how
to dynamically control the optimality and computational com-
plexity trade-off. In this section, we will focus on these two
questions and come up with some methods to solve them.

A. Mixed Training Strategy

As shown in Section V-D, the generalization ability is good
but also limited. Policy learned from small-scale problems can
be generalized to large-scale problems, but the performance
deteriorates with the increase of the problem scale. It
is
obvious that the generalization ability is constrained by the
problem scale of training samples.

In [21],

the transfer learning method has been used to
solve this problem. Its basic idea is adding few unlabeled
training samples from large-scale problems and making use
of an exploration policy to enhance the generalization ability.
However, the exploration policy, which is crucial for transfer
learning, is always hard to design for the MINLP problems in
wireless networks. Furthermore, the transfer learning method
would induce higher computational complexity for training
process. Therefore, we propose to use the mixed training
strategy to overcome the aforementioned shortcomings.

Speciﬁcally, if we want to solve Problem(k, l), we use a
training set consists of many samples from the problems of
smaller size than Problem(k, l) and very few samples from
Problem(k, l). Different from [21], samples from Problem(k, l)
are also labeled. This will not cost much overhead because
it is not difﬁcult to get very few labeled training samples
from the large-scale problems in practice. For instance, we
can make use of system history record to get some optimal
allocation results and use them as training samples. Since
the proposed algorithm needs very few training samples,
the cost of gathering historical data would be fairly low.
In this way, we reinforce the generalization ability without
designing complicated exploration policy and consuming more
training time. The only overhead is getting few labeled training
samples from the large-scale problems, which is acceptable.
We implement the mixed training strategy for each scenario
with the sizes in Table VIII to test the effectiveness of the
proposed strategy. For each scenario, we use a mixed training
set consisting of 190 samples from the scenario with K = 5,
L = 2 and 10 samples from itself. Moreover, we use 20
testing samples for each scenario. The results are summarized
in Table IX. Comparing Tables VIII and IX, ogap achieved
by the mixed training strategy is always lower than that
achieved by using the original training strategy. For some
speciﬁc scenarios, it can even achieve lower computational
complexity at the same time. The results suggest that the
mixed training strategy achieves better optimality than using
small-scale problems’ training samples only. This method can
achieve a trade-off between the learned policy’s performance
and the cost of getting training samples from the large-scale
problems.

B. Soft-Decision Algorithm

We have thus far used SVM to train the classiﬁer to learn the
optimal prune policy. However, it cannot achieve dynamic con-
trol over optimality and computational complexity. Because
the result of SVM classiﬁer is either 0 or 1. Once the classiﬁer

is decided, the category of each node is determined and we
cannot modify any more. If we want to get a less optimal
solution with higher speed, we have to turn to a new policy,
which is not convenient in practical systems.

To deal with this problem, we adopt the forward neural
network (FNN) to replace SVM as the classiﬁer inspired by
[21], [32]. For the binary classiﬁcation problem, FNN can
output the probability of each class rather than the deﬁnite
result. It provides us with a soft-decision result and we can
use it to dynamically control the optimality and computational
complexity.

Speciﬁcally, we construct a J-layer FNN. We use Relu, i.e.,
Relu(x) = max(0, x), as the activation function for J −1 hidden
layers and use softmax for the output layer to indicate the
probability of each class. The label y of the optimal and non-
optimal nodes are denoted as (1, 0) and (0, 1), respectively.
And the loss function is the weighted cross-entropy, as

Loss = −ω[1]y[1] log(o[1]) − ω[2]y[2] log(o[2]),

where o is the output vector of the FNN and ω is the weight
vector for each class. ω is closely related to ω1 and ω2
mentioned in Section IV-C. Speciﬁcally, ω[1] = ω1 × ω2 and
ω[2] = ω1. This loss function is different from the one in
[21] that does not take the depth-dependent weights, ω1, into
consideration.

With the help of DAgger, FNN can be trained in a similar
way to SVM classiﬁer suggested in Tables II and III. After
FNN-based policy is learned, we can get the output probability
vector, on, for node n, during the B&B searching process.
Then we compare the optimal probability on[1] with a given
threshold, τ . If on[1] > τ , node n is classiﬁed as the optimal
node and is to be branched on; otherwise, node n is classiﬁed
as the non-optimal node and is to be pruned. The threshold,
τ , can be used to control the optimality and computational
complexity trade-off. For example, if we want to get a less
optimal solution with higher speed, we do not need to train a
new policy. We only need to reduce τ iteratively to satisfy our
goal. This dynamic control process is summarized in Table X.
We use the scenario with K = 5, L = 2 as an example
to compare the performance of the SVM-based and FNN-
based imitation learning methods. We use 200 training samples
and 20 testing samples for each method. In order to make a
fair comparison with the loss function used in [21], we adopt
the FNN structure in [21] and construct an FNN with three
hidden layers with 16, 32, and 16 nodes, respectively. We
set the number of training epochs to be 30 and the batch
size to be 128. We set the ogap limit of the FNN-based
method as the ogap that can be attained by the SVM-based
method, i.e., 2.01%. We change τ by 0.01 for each iteration.
The comparison results are summarized in Table XI. Because
the optimal recognition rate and extra prune rate change
iteratively with τ in the FNN-based method, we only focus
on speed here to compare the SVM-based and FNN-based
methods in a fair way. From Table XI, the computational
complexity of our proposed method is slightly lower than that
in [21]. It also suggests that the speed achieved by the FNN-
based method is lower than that of the SVM-based method
while achieving the same optimality. Because the FNN-based

11

TABLE X
DYNAMIC SOFT-DECISION ALGORITHM

Algorithm 4 Dynamic Soft-Decision Algorithm

1: initialization

• Set node list: NQ ← {n([])}.
• Set threshold: τ = 0.5.

2: while Speed or Ogap is not achieved do
3:
4:

while NQ 6= ∅ do

Node Selection: pop the ﬁrst node nQ(ρd) in NQ.
Evaluation: use interior-point method solve Prob-

lem (7) corresponding to nQ(ρd).
Fathom Decision:
if Node nQ(ρd) is fathomed by original B&B then

Go to step 2.

else

if πp(nQ(ρd))[1] >= τ then
Branch on node nQ(ρd).
Variable Selection: branch on the ﬁrst

undetermined variable in ˆρ.

Add the new nodes into NQ.

end if
Go to step 2.

5:

6:

7:

8:
9:

10:
11:

12:

13:

14:

15:
16:

end if
end while
Increase or Decrease τ .

17:
18:
19: end while

TABLE XI
PERFORMANCE OF SVM-BASED AND FNN-BASED IMITATION LEARNING
FOR THE SCENARIO WITH K = 5, L = 2

Method
SVM-based imitation learning
FNN-based imitation learning with new loss function
FNN-based imitation learning with loss function in [21]

Speed
2.06x
1.74x
1.69x

method needs iteration process to tune appropriate threshold τ
for each problem, it will consume more time than the SVM-
based method to achieve dynamic control.

VII. CONCLUSIONS AND FUTURE RESEARCH

This paper introduces a hybrid ML and mathematical
method to solve resource allocation problems in D2D com-
munication networks, which are usually formulated as MINLP
problems. The main goal of leveraging the ML technique is to
overcome the disadvantages of traditional mathematical tech-
niques and get an efﬁcient algorithm for resource allocation
in D2D communications. The key idea is to accelerate the
B&B algorithm, the widely-used globally optimal algorithm
for MINLP problems, by learning a good auxiliary prune
policy. This is achieved by the imitation learning method. Ex-
tensive experiment results have demonstrated that the proposed
method can achieve good optimality and reduce computational
complexity at the same time with only hundreds of training
samples. It also has the generalization ability to large-scale
problems. These two aforementioned properties are preferred

in wireless networks. To further improve the generalization
ability of the proposed method, we have also developed the
mixed training strategy, which can balance the learned policy’s
performance and the cost of getting training samples from
large-scale problems. Moreover, we have utilized a DNN as
the binary classiﬁer and proposed a novel loss function to
dynamically control the optimality and computational com-
plexity. Our studies in this work can be also applied to the
MINLP problems in other wireless communication systems,
such as communication mode selection and multi-cell resource
allocation problems in D2D networks [23], [25], [26].

There still exist some problems about accelerating the B&B
algorithm for resource allocation problems by imitation learn-
ing method. First, the optimality of the proposed method is
guaranteed by the optimality of the B&B algorithm. However,
optimal solutions can even not be achieved by the B&B
algorithm for more difﬁcult problems whose corresponding
nonlinear problems are non-convex. Our proposed method
can still be used for the non-convex problems to achieve a
faster B&B algorithm, but the optimality cannot be guaranteed
since the B&B algorithm is not optimal in that case. Second,
our proposed method can speed up the B&B algorithm, but
sometimes the speed up rates are not high enough as the
test results suggest. Attempting to further speed up the B&B
algorithm while remaining the optimality at the same time
is an important future direction. Also, ﬁnding other effective
method to avoid dataset imbalance problem, further reducing
the number of training samples, and even adopting new ML
techniques to accelerate resource allocation are very interesting
issues for ﬁne-tuning our proposal.

APPENDIX A
PROOF OF PROPOSITION 1
From [24], if D2D pair l is allowed to reuse the channel of
CU k, the following equation should be satisﬁed to maximize
the minimum data rate of all D2D pairs and guarantee the data
rate of CU k

log(1 +

khCB
pC
k
klhDB
N + pD
σ2
l

) = RC

min.

Then, we have

k = (2RC
pC

min − 1)

klhDB
l

N + pD
σ2
hCB
k

.

, where

From (3c) and (3e), we can get the new bound of pD
pmax
kl
kl = min{P D
pmax

max, (1/hDB

k /(2RC

maxhCB

l )(pC

min − 1) − σ2

N )}.

kl as pD

kl ≤

APPENDIX B
PROOF OF PROPOSITION 2
From Proposition 1, the data rate of D2D pair l on the

channel of CU k can be rewritten as
klhD
l
khCD
kl

kl(pC, pD, ρ) = log(1 +

ρklpD
N + pC
σ2

RD

)

= log(1 +

σ2

N +

, ˆRD

kl(pD, ρ).

klhD
l

ρklpD
min −1)hCD
kl

RC

(2

hCB
k

(σ2

N + pD

klhDB
l )

)

12

,

For convenience, we introduce
(2RC

kl σ2
N

+

akl , σ2
N
hD
l
bkl , (2RC

min − 1)hCD
l hCB
hD
k
min − 1)hCD
kl hDB
l
l hCB
hD
k
kl(pD, ρ) can be rewritten as

.

Then, ˆRD

ˆRD

kl(pD, ρ) = log(1 +

ρklpD
kl
akl + bklpD
kl

).

(9)

Furthermore, we can rewrite (2) as

RD

l (pC, pD, ρ) =

ρklRD

kl(pC, pD, ρ)

Xk∈K

Xk∈K

Xk∈K

=

=

=

ρkl log(1 +

log(1 +

log(1 +

klhD
l
khCD
kl

)

ρklpD
N + pC
σ2
klhD
l
khCD
kl

ρklpD
N + pC
σ2
ρklpD
kl
akl + bklpD
kl

)

)

Xk∈K
, ˆRD

l (pD, ρ).

This equation always holds because ρkl is a binary variable.

REFERENCES

[1] K. Doppler et al., “Device-to-device communication as an underlay to
LTE-Advanced networks,” IEEE Commun. Mag., vol. 47, no. 12, pp. 42–
49, Dec. 2009.

[2] G. Fodor et al., “Design aspects of network assisted device-to-device
communications,” IEEE Commun. Mag., vol. 50, no. 3, pp. 170–177,
Mar. 2012.

[3] J. Liu, N. Kato, H. Ujikawa, and K. Suzuki, “Device-to-device commu-
nication for mobile multimedia in emerging 5G networks,” ACM Trans.
Multimed. Comput. Comm. Appl., vol. 12, no. 5s, Article 75, Dec. 2016.
[4] A. Asadi, Q. Wang, and V. Mancuso, “A survey on device-to-device
communication in cellular networks,” IEEE Commun. Surveys Tuts., vol.
16, no. 4, pp. 1801–1819, 4th Quart., 2014.

[5] J. Liu, N. Kato, J. Ma, and N. Kadowaki, “Device-to-device communi-
cation in LTE-Advanced networks: A survey,” IEEE Commun. Surveys
Tuts., vol. 17, no. 4, pp. 1923–1940, 4th Quart., 2015.

[6] A. H. Land and A. G. Doig, “An automatic method of solving discrete
programming problems,” Econometrica, J. Econometric Soc., vol. 28, no.
3, pp. 497–520, Jul. 1960.

[7] R. Yin et al., “Pricing-based interference coordination for D2D commu-
nications in cellular networks,” IEEE Trans. Wireless Commun., vol. 14,
no. 3, pp. 1519–1532, Mar. 2015.

[8] H.-H. Nguyen et al., “Distributed resource allocation for D2D commu-
nications underlay cellular networks,” IEEE Commun. Lett., vol. 20, no.
5, pp. 942–945, May 2016.

[9] H. Zhang, L. Song, and Z. Han, “Radio resource allocation for device-to-
device underlay communication using hypergraph theory,” IEEE Trans.
Wireless Commun., vol. 15, no. 7, pp. 4852–4861, Jul. 2016.

[10] R. Zhang et al., “Interference-aware graph based resource sharing for
device-to-device communications underlaying cellular networks,” in Proc.
IEEE Wireless Commun. Netw. Conf. (WCNC), Shanghai, China, Apr.
2013, pp. 140–145.

[11] D. Feng, L. Lu, Y. Yuan-Wu, G. Y. Li, G. Feng, and S. Li, “Device-
to-device communications underlaying cellular networks,” IEEE Trans.
Commun., vol. 61, no. 8, pp. 3541–3551, Aug. 2013.

[12] Y. Jiang et al., “Energy efﬁcient joint resource allocation and power
control for D2D communications,” IEEE Trans. Veh. Technol., vol. 65,
no. 8, pp. 6119–6127, Aug. 2015.

[13] H. Sun et al., “Learning to optimize: Training deep neural networks for
interference management,” IEEE Trans. Signal Process., vol. 66, no.20,
pp. 5438–5453, Oct. 2018.

13

[14] W. Lee, M. Kim, and D. H. Cho, “Deep power control: Transmit power
control scheme based on convolutional neural network,” IEEE Commun.
Lett., vol. 22, no. 6, pp. 1276–1279, Jun. 2018.

[15] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for
dynamic power allocation in wireless networks,” IEEE J. Sel. Areas
Commun., vol. 37, no. 10, pp. 2239–2250, Oct. 2019.

[16] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning based
resource allocation for V2V communications,” IEEE Trans. Veh. Technol.,
vol. 68, no. 4, pp. 3163–3173, Apr. 2019.

[17] X. Chen et al., “Multi-tenant cross-slice resource orchestration: A deep
reinforcement learning approach,” IEEE J. Sel. Areas Commun., vol. 37,
no. 10, pp. 2377–2392, Oct. 2019.

[18] W. Cui, K. Shen, and W. Yu, “Spatial deep learning for wireless
scheduling,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1248–1261,
Jun. 2019.

[19] M. Lee, Y. Xiong, G. Yu, and G. Y. Li, “Deep neural networks for linear
sum assignment problems,” IEEE Wireless Commun. Lett., vol. 7, no. 6,
pp. 962–965, Dec. 2018.

[20] H. He, H. Daume III, and J. M. Eisner, “Learning to search in branch
and bound algorithms,” in Proc. Adv. Neural Inform. Process. Syst., pp.
3293–3301, Dec. 2014.

[21] Y. Shen, Y. Shi, J. Zhang, and K. B. Letaief, “LORM: Learning to
optimize for resource management in wireless networks with few training
samples,” to appear in IEEE Trans. Wireless Commun., Oct. 2019.
[22] H. Min, J. Lee, S. Park, and D. Hong, “Capacity enhancement using an
interference limited area for device-to-device uplink underlaying cellular
networks,” IEEE Trans. Wireless Commun., vol. 10, no. 12, pp. 3995–
4000, Dec. 2011.

[23] G. Yu, L. Xu, D. Feng, R. Yin, G. Y. Li, and Y. Jiang, “Joint mode
selection and resource allocation for device-to-device communications,”
IEEE Trans. Commun., vol. 62, no. 11, pp. 3814–3824, Nov. 2014.
[24] T. D. Hoang, L. B. Le, and T. Le-Ngoc, “Energy-efﬁcient resource
allocation for D2D communications in cellular networks,” IEEE Trans.
Veh. Technol., vol. 65,no. 9, pp. 6972–6986, Sep. 2016.

[25] J. Liu et al., “On the outage probability of device-to-device communi-
cation enabled multi-channel cellular networks: A RSS threshold-based
perspective,” IEEE J. Sel. Areas Commun., vol. 34, no. 1, pp. 163–175,
Jan. 2016.

[26] J. Liu et al., “Device-to-device communications for enhancing quality of
experience in software deﬁned multi-tier LTE-A networks,” IEEE Netw.,
vol. 29, no. 4, pp. 46–52, Jul. 2015.

[27] J. Lee and S. Leyffer, Mixed Integer Nonlinear Programming. The IMA
Volumes in Mathematics and its Applications, Springer-US, Dec. 2011.
[28] A. Attia and S. Dayan, “Global overview of imitation learning,”

arXiv:1801.06503v1, Jan. 2018.

[29] C. Corinna and V. Vapnik, “Support-vector networks.” Machine learning

, vol. 20, no. 3, pp. 273–297, Sept. 1995.

[30] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
and structured prediction to no-regret online learning,” in Proc. Int. Conf.
Artiﬁcial Intell. Stat., pp. 627–635, Apr. 2011.

[31] C. Chang and C. Lin, “Libsvm: A library for support vector machines,”
ACM Transactions on Intelligent Systems and Technology (TIST), vol. 2,
no. 3, article 27, Apr. 2011.

[32] J. Song et al., “Learning to search via self-imitation with application to
risk-aware planning,” in Proc. Adv. Neural Inform. Process. Syst., Dec.
2017.

