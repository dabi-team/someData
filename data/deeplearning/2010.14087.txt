0
2
0
2

t
c
O
7
2

]

G
L
.
s
c
[

1
v
7
8
0
4
1
.
0
1
0
2
:
v
i
X
r
a

Hamilton–Jacobi Deep Q-Learning for Deterministic
Continuous-Time Systems with Lipschitz Continuous Controls∗

Jeongho Kim†

Jaeuk Shin‡

Insoon Yang‡

Abstract

In this paper, we propose Q-learning algorithms for continuous-time deterministic optimal
control problems with Lipschitz continuous controls. Our method is based on a new class of
Hamilton–Jacobi–Bellman (HJB) equations derived from applying the dynamic programming
principle to continuous-time Q-functions. A novel semi-discrete version of the HJB equation
is proposed to design a Q-learning algorithm that uses data collected in discrete time without
discretizing or approximating the system dynamics. We identify the condition under which the
Q-function estimated by this algorithm converges to the optimal Q-function. For practical im-
plementation, we propose the Hamilton–Jacobi DQN, which extends the idea of deep Q-networks
(DQN) to our continuous control setting. This approach does not require actor networks or nu-
merical solutions to optimization problems for greedy actions since the HJB equation provides
a simple characterization of optimal controls via ordinary diﬀerential equations. We empirically
demonstrate the performance of our method through benchmark tasks and high-dimensional
linear-quadratic problems.

1

Introduction

Model-free reinforcement learning (RL) algorithms provide an eﬀective data-driven solution to se-
quential decision-making problems, in particular, in the discrete-time setting [1–3]. Recently, there
has been a growing interest in and demand for applying these techniques to complex physical con-
trol tasks, motivated by robotic and autonomous systems. However, many physical processes evolve
in continuous time, requiring the need for RL methods that can systematically handle continuous-
time dynamical systems. These systems are often described by deterministic ordinary diﬀerential
equations (ODEs). Classical approaches ﬁrst estimate the model parameters by using system iden-
tiﬁcation techniques and then design a suitable model-based controller (e.g., [4]). However, we
do not often have such a luxury of having a separate training period for parameter identiﬁcation,
which often requires large-scale high-resolution data. Furthermore, when the model parameters
change over time, the classical techniques have fundamental limitations in terms of adaptivity. The
focus of this work is to study a control-theoretic model-free RL method that extends the popular
Q-learning [5] and deep Q-networks (DQN) [6] to the continuous-time deterministic optimal control
setting.

∗This work was supported in part by the Creative-Pioneering Researchers Program through SNU, the National
Research Foundation of Korea funded by the MSIT(2020R1C1C1009766), the Information and Communications
Technology Planning and Evaluation (IITP) grant funded by MSIT(2020-0-00857), and Samsung Electronics.

†Institute of New Media and Communications, Seoul National University, Seoul 08826, South Korea,

(jhkim206@snu.ac.kr).

‡Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National

University, Seoul 08826, South Korea, ({sju5379, insoonyang}@snu.ac.kr).

1

 
 
 
 
 
 
2

One of the most straightforward ways to tackle such continuous-time control problems is to
discretize time, state, and action, and then employ an RL algorithm for discrete Markov decision
processes (MDPs). However, this approach could easily be rendered ineﬀective when a ﬁne dis-
cretization is used [7]. To avoid the explicit discretization of state and action, several methods have
been proposed using function approximators [8]. Among those, algorithms that use deep neural net-
works as function approximators provide strong empirical evidence for learning high-performance
policies, on a range of benchmark tasks [9–12]. To deal with continuous action spaces, such discrete-
time model-free deep RL methods numerically solve optimization problems for greedy actions [13]
or use parameterized policies and learn the network parameters via policy gradient [14, 15], actor-
critic methods [16–20], or normalized advantage functions [21]. However, in these methods it is
unclear how to choose the size of discretized time steps or how the algorithms should be systemat-
ically modiﬁed to take into account the eﬃciency and the stability of learning processes according
to the characteristics of the continuous-time systems.

The literature regarding continuous-time RL is relatively limited; most of them have tried to
avoid explicit discretization using the structural properties of limited classes of system dynamics
(for example, see [22–29] for linear or control-aﬃne systems, and see [30] for semi-MDPs with ﬁnite
state and action spaces). We also refer to [31], where the policy gradient method in continuous-time
setting is introduced. However, the reward function does not depend on the control signal in their
framework.

In general continuous-time cases, the dynamic programming equation is expressed as a Hamilton–
Jacobi–Bellman (HJB) equation that provides a sound theoretical framework. Previous methods use
HJB equations for learning the optimal state-value function or its gradient via convergent discretiza-
tion [32], barycentric interpolation [33], advantage functions [34], temporal diﬀerence algorithms [7],
kernel-based approximations [35], adaptive dynamic programming [36], path integrals [37, 38] and
neural network approximation [39, 40].

However, to our knowledge, HJB equations have not been studied for admitting Q-functions as a
solution (i.e., state-action value functions) in the previous methods although there have been a few
attempts to construct variants of Q-functions for continuous-time dynamical systems. In [41], the Q-
function for linear time-invariant systems is deﬁned as the sum of the optimal state-value function
and the Hamiltonian. Another variant of Q-functions is introduced as the sum of the running
cost and the directional derivative of the state-value function [42], which is then approximated by a
parameterized family of functions. However, in our opinion, the deﬁnitions of the Q-function in these
works are diﬀerent from the standard state-action value function that is deﬁned as the maximum
expected cumulative reward incurred after starting from a particular state with a speciﬁc action.
Moreover, they have only used HJB equations for the state-value function without introducing
or using HJB equations for the constructed Q-functions. The practical performances of these
methods have only been demonstrated through low-dimensional tasks. More recently, [43] devises
a new method combining advantage updating [44] and existing oﬀ-policy RL algorithms to propose
continuous-time RL algorithms that are robust to time discretization. However, to tackle problems
with continuous action spaces, this method uses oﬀ-policy actor-critic methods rather than relying
only on the state-value functions.

In this work, we consider continuous-time deterministic optimal control problems with Lipschitz
continuous controls in the inﬁnite-horizon discounted setting. We show that the standard Q-
function is well deﬁned in continuous-time under Lipschitz constraints on controls. Applying the
dynamic programming principle to the Q-function, we derive a novel class of HJB equations. The
HJB equation is shown to admit a unique viscosity solution, which corresponds to the optimal
Q-function. To the best of our knowledge, this is the ﬁrst attempt to rigorously characterize the
HJB equations for Q-functions in continuous-time control. The HJB equations provide a simple

3

model-free characterization of optimal controls via ODEs and a theoretical basis for our Q-learning
method. We propose a new semi-discrete version of the HJB equation to obtain a Q-learning
algorithm that uses sample data collected in discrete time without discretizing or approximating
the continuous-time dynamics. By design, it attains the ﬂexibility to choose the sampling interval to
take into account the features of continuous-time systems, but without the need for sophisticated
ODE discretization methods. We provide a convergence analysis that suggests a limit for the
sampling interval for the convergence guarantee. This study may open a new exciting avenue of
research that connects HJB equations and Q-learning domain.

For a practical implementation of our HJB-based Q-learning, we combine it with the idea
of DQN. This new model-free oﬀ-policy deep RL algorithm, which we call the Hamilton-Jacobi
DQN (HJ DQN), is as simple as DQN but capable of solving continuous-time problems without
discretizing the system dynamics or the action space. Instead of using any parameterized policy or
numerically optimizing the estimated Q-functions to compute greedy actions, HJ DQN beneﬁts from
the simple ODE characterization of optimal controls, which are obtained in our theoretical analysis
of the HJB equations. Thus, our algorithm is computationally light and easy to implement, thereby
requiring less hyperparameter tuning compared to actor-critic methods for continuous control. We
evaluate our algorithm on OpenAI benchmark tasks and high-dimensional linear-quadratic (LQ)
control problems. The result of our experiments suggests that actor networks in actor-critic methods
may be replaced by the optimal control obtained via our HJB equation.

This paper is signiﬁcantly expanded from a preliminary conference version [45]. A Q-learning
algorithm and its DQN variant are newly designed in a principled manner to use transition data
collected in discrete time. Furthermore, convergence properties of our Q-learning method are
carefully studied in this paper. It contains the results of more thorough numerical experiments on
several benchmark tasks and LQ problems, as well as ablation studies.

The remainder of this paper is organized as follows. In Section 2, we deﬁne the Q-functions
for continuous-time optimal control problems with Lipschitz continuous controls and derive the
associated HJB equations. We also characterize optimal control dynamics via an ODE. In Section
3, we propose a Q-learning algorithm based on the semi-discrete HJB equation and identify its
convergence properties. In Section 4, we introduce the HJ DQN algorithm and discuss its features.
Section 5 provides the results of our experiments on benchmark problems as well as LQ control
problems. All the mathematical proofs are contained in Appendix B.

2 Hamilton–Jacobi–Bellman Equations for Q-Functions

Consider a continuous-time dynamical system of the form1

˙x(t) = f (x(t), a(t)),

t > 0,

(2.1)

where x(t) ∈ Rn and a(t) ∈ Rm are the system state and the control action, respectively. Here, the
vector ﬁeld f : Rn × Rm → Rn is an unknown function. The standard inﬁnite-horizon discounted
optimal control problem can be formulated as2

Jx(a) :=

sup
a∈A

(cid:90) ∞

0

e−γtr(x(t), a(t)) dt,

(2.2)

1Here, ˙x denotes dx/dt.
2Although the focus of this work is deterministic control, one may also consider its stochastic counterpart. We

brieﬂy discuss the extension of our method to the stochastic control setting in Appendix C.

4

with x(0) = x, where r : Rn × Rm → R is an unknown reward function of interest and γ > 0 is
a discount factor. We follow the convention in continuous-time deterministic optimal control that
considers control trajectory, instead of control policy, as the optimization variable [46].

The (continuous-time) Q-function of (2.2) is deﬁned as

Q(x, a) := sup
a∈A

(cid:26) (cid:90) ∞

0

e−γtr(x(t), a(t)) dt | x(0) = x, a(0) = a

(cid:27)

,

(2.3)

(cid:8) (cid:82) ∞

which represents the maximal reward incurred from time 0 when starting from x(0) = x with
a(0) = a. Suppose for a moment that the set of admissible controls A has no particular constraints,
i.e., A := (cid:8)a : R≥0 → Rm | a measurable(cid:9). Then, Q(x, a) reduces to the standard optimal value
0 e−γtr(x(t), a(t)) dt | x(0) = x(cid:9) for all a ∈ Rm since the action can
function v(x) := supa∈A
be switched immediately from a to an optimal control and in this case a does not aﬀect the total
cost or the system trajectory in the continuous-time setting.
Proposition 1. Suppose that A := (cid:8)a : R≥0 → Rm | a measurable(cid:9). Then, the optimal Q-function
(2.3) corresponds to the optimal value function v for each a ∈ Rm, i.e., Q(x, a, t) = v(x, t) for all
(x, a, t) ∈ Rn × Rm × [0, T ].

Thus, if A is chosen as above, the Q-function has no additional interesting property under
the standard choice of A.3 Motivated by the observation, we restrict the control a(t) to be a
Lipschitz continuous function in t. Since any Lipschitz continuous function is diﬀerentiable almost
everywhere, we choose the set of admissible controls as

A := (cid:8)a : R≥0 → Rm | a measurable, | ˙a(t)| ≤ L a.e.(cid:9),

where | · | denotes the standard Euclidean norm, and L is a ﬁxed constant. From now on, we will
focus on the optimal control problem (2.2) with Lipschitz continuous controls, i.e., | ˙a(t)| ≤ L a.e.,
and the corresponding Q-function (2.3).

Our ﬁrst step is to study the structural properties of the optimality equation and the opti-
mal control via dynamic programming. Using the discovered structural properties, a DQN-like
algorithm is then designed to solve the optimal control problem (2.2) in a model-free manner.

2.1 Dynamic Programming and HJB Equations

By the dynamic programming principle, we have

Q(x, a) = sup
a∈A

(cid:26) (cid:90) t+h

t

e−γ(s−t)r(x(s), a(s)) ds + e−γhQ(x(t + h), a(t + h)) | x(t) = x, a(t) = a

(cid:27)

for any h > 0. Rearranging this equality, we obtain

0 = sup
a∈A

(cid:26) 1
h

(cid:90) t+h

t

e−γ(s−t)r(x(s), a(s)) ds +

1
h

[Q(x(t + h), a(t + h)) − Q(x, a)]

+

e−γh − 1
h

Q(x(t + h), a(t + h)) | x(t) = x, a(t) = a

.

(cid:27)

Letting h tend to zero and assuming for a moment that the Q-function is continuously diﬀerentiable,
its Taylor expansion yields

γQ(x, a) − ∇xQ · f (x, a) −

sup
b∈Rm,|b|≤L

∇aQ · b − r(x, a) = 0,

3This observation is consistent with the previously reported result on the continuous limit of Q-functions [43, 44].

5

where the optimization variable b represents ˙a(t). Note that the supremum is attained at b(cid:63) =
L ∇aQ

|∇aQ| . Thus, we obtain

γQ(x, a) − ∇xQ · f (x, a) − L|∇aQ| − r(x, a) = 0,

(2.4)

which is the HJB equation for the Q-function. However, the Q-function is not continuously diﬀer-
entiable in general. This motivates us to consider a weak solution of the HJB equation. Among
several types of weak solutions, it is shown in Appendix A that the Q-function corresponds to the
unique viscosity solution [47] of the HJB equation under the following assumption:

Assumption 1. The functions f and r are bounded and Lipschitz continuous, i.e., there exists
a constant C such that (cid:107)f (cid:107)L∞ + (cid:107)r(cid:107)L∞ < C and (cid:107)f (cid:107)Lip + (cid:107)r(cid:107)Lip < C, where (cid:107) · (cid:107)Lip denotes a
Lipschitz constant of argument.

2.2 Optimal Controls

In the derivation of the HJB equation above, we deduce that an optimal control a must satisﬁes
˙a = L ∇aQ
|∇aQ| when Q is diﬀerentiable. The viscosity solution framework [46] can be used to ob-
tain the following more rigorous characterization of optimal controls when the Q-function is not
diﬀerentiable.

Theorem 1. Suppose that Assumption 1 holds. Consider a control trajectory a(cid:63)(s), s ≥ t, deﬁned
by

˙a(cid:63)(s) = L

∀p = (p1, p2) ∈ D±Q(x(cid:63)(s), a(cid:63)(s))

(2.5)

p2
|p2|

for a.e. s ≥ t, and a(cid:63)(t) = a, where ˙x(cid:63) = f (x(cid:63), a(cid:63)) for s ≥ t and x(cid:63)(t) = x. Assume that the
function Q is locally Lipschitz in a neighborhood of (x(cid:63)(s), a(cid:63)(s)) and that D+Q(x(cid:63)(s), a(cid:63)(s)) =
∂Q(x(cid:63)(s), a(cid:63)(s)) for a.e. s ≥ t.4 Then, a(cid:63) is optimal among those in A such that a(t) = a, i.e., it
satisﬁes

a(cid:63) ∈ arg max

(cid:26) (cid:90) ∞

a∈A

t

e−γ(s−t)r(x(s), a(s)) ds,

(cid:12)
(cid:12)
(cid:12) x(t) = x, a(t) = a

(cid:27)

.

(2.6)

If, in addition,

then a(cid:63) is an optimal control, i.e., it satisﬁes

a ∈ arg max

a(cid:48)∈Rm

Q(x, a(cid:48)),

a(cid:63) ∈ arg max

(cid:26)(cid:90) ∞

a∈A

t

e−γ(s−t)r(x(s), a(s)) ds

(cid:12)
(cid:12)
(cid:12) x(t) = x

(cid:27)

.

Note that at a point (x, a) where Q is diﬀerentiable, the ODE (2.5) is simpliﬁed to ˙a(cid:63) =
L ∇aQ(x(cid:63),a(cid:63))
|∇aQ(x(cid:63),a(cid:63))| . A useful implication of this theorem is that for any a ∈ Rm, an optimal control in A
such that a(t) = a can be obtained using the ODE (2.5) with the initial condition a(cid:63)(t) = a. Thus,
when the control is initialized as an arbitrary value a at arbitrary time t in Q-learning, we can still
use the ODE (2.5) to obtain an optimal control. Another important implication of Theorem 1 is
that an optimal control can be constructed without numerically solving any optimization problem.
This salient feature assists in the design of a computationally eﬃcient DQN algorithm for continuous
control without involving any explicit optimization nor any actor network.

4Here, D+Q and D−Q denote the super- and sub-diﬀerentials of Q, respectively, and D±Q := D+Q ∪ D−Q. At
a point (x, a) where Q is diﬀerentiable, the super- and sub-diﬀerentials are identical to the singleton of the classical
derivative of Q. Moreover, ∂Q denotes the Clarke’s generalized gradient of Q (see, e.g., p. 63 of [46]). Note that the
right-hand side of ODE (2.5) can be arbitrarily chosen when p2 = 0.

3 Hamilton–Jacobi Q-Learning

3.1 Semi-Discrete HJB Equations and Asymptotic Consistency

6

In practice, even though the underlying physical process evolves in continuous time, the observed
data, such as sensor measurements, are collected in discrete (sample) time. To design a concrete
algorithm for learning the Q-function using such discrete-time data, we propose a novel semi-discrete
version of the HJB equation (2.4) without discretizing or approximating the continuous-time system.
k=0 | bk ∈ Rm, |bk| ≤ L}, where bk is
Let h > 0 be a ﬁxed sampling interval, and let B := {b := {bk}∞
analogous to ˙a(t) in the continuous-time case. Given (x, a) ∈ Rn × Rm and a sequence b ∈ B, we
let

Qh,b(x, a) := h

r(xk, ak)(1 − γh)k,

∞
(cid:88)

k=0

where {(xk, ak)}∞
k=0 is deﬁned by xk+1 = ξ(xk, ak; h) and ak+1 = ak + hbk with (x0, a0) = (x, a).
Here, ξ(xk, ak; h) denotes the state of (2.1) at time t = h with initial state x(0) = xk and constant
action a(t) ≡ ak, t ∈ [0, h). It is worth emphasizing that our semi-discrete approximation does
not approximate the system dynamics and thus is more accurate than the standard semi-discrete
method. The optimal semi-discrete Q-function Qh,(cid:63) : Rn × Rm → R is then deﬁned by

Qh,(cid:63)(x, a) := sup
b∈B

Qh,b(x, a).

(3.1)

Then, Qh,(cid:63) satisﬁes a semi-discrete version of the HJB equation (2.4).
Proposition 2. Suppose that 0 < h < 1
semi-discrete HJB equation:

γ . Then, the function Qh,(cid:63) is a solution to the following

Qh,(cid:63)(x, a) = hr(x, a) + (1 − γh) sup
|b|≤L

Qh,(cid:63)(ξ(x, a; h), a + hb).

(3.2)

Under Assumption 1, Qh,(cid:63) coincides with the unique solution of the semi-discrete HJB equa-
tion (3.2). Moreover, the optimal semi-discrete Q-function converges uniformly to its original
counterpart in every compact subset of Rn × Rm.
Proposition 3. Suppose that 0 < h < 1
the unique solution to the semi-discrete HJB equation (3.2). Furthermore, we have

γ and that Assumption 1 holds. Then, the function Qh,(cid:63) is

lim
h→0

sup
(x,a)∈K,Kcompact

|Qh,(cid:63)(x, a) − Q(x, a)| = 0.

This proposition justiﬁes the use of the semi-discrete HJB equation for small h. We aim to
estimate the optimal Q-function using sample data collected in discrete time, enjoying the beneﬁts
of both the semi-discrete HJB equation (3.2) and the original HJB equation (2.4). Namely, the
semi-discrete version yields to naturally make use of Q-learning and DQN, and the original version
provides an optimal control via (2.5) without requiring a numerical solution for any optimization
problems or actor networks as we will see in Section 4.

3.2 Convergence Properties

Consider the following model-free update of Q-function using the semi-discrete HJB equation (3.2):
In the kth iteration, for each (x, a) we collect data (xk := x, ak := a, rk, xk+1) and update the
Q-function, with learning rate αk, by

Qh

k+1(x, a) := (1 − αk)Qh

k(x, a) = αk

(cid:104)

hrk + (1 − γh) sup
|b|≤L

(cid:105)
k(xk+1, a + hb)

Qh

,

(3.3)

7

where xk+1 is obtained by running (or simulating) the continuous-time system from xk with action
ak ﬁxed for h period without any approximation, i.e., xk+1 = ξ(xk, ak; h), and rk = r(xk, ak). We
refer to this synchronous Q-learning as Hamilton–Jacobi Q-learning. Note that this method is not
practically useful because the update must be performed for all state-action pairs in the continuous
space. In the following section, we propose a DQN-like algorithm to approximately perform HJ Q-
learning employing deep neural networks as function approximators. Before doing so, we identify
conditions under which the Q-function updated by (3.3) converges to the optimal semi-discrete
Q-function (3.1) in L∞.

Theorem 2. Suppose that 0 < h < 1
k=0 of learning rates satisﬁes (cid:80)∞
{αk}∞

γ , 0 ≤ αk ≤ 1 and that Assumption 1 holds. If the sequence
k=0 αk = ∞, then

lim
k→∞

(cid:107)Qh

k − Qh,(cid:63)(cid:107)L∞ = 0.

Finally, by Propositions 3 and Theorem 2, we establish the following convergence result associ-

ating HJ Q-learning (3.3) and the optimal Q-function in the original continuous-time setting.

Corollary 1. Suppose that 0 ≤ αk ≤ 1 and that Assumption 1 holds. If the sequence {αk}∞
of learning rates satisﬁes (cid:80)∞
h (cid:80)kh−1

k=0 αk = ∞ then, for each 0 < h < 1
τ =0 ατ → ∞ as h → 0. Moreover, for such a choice of kh, we have

k=0
γ , there exists kh such that

lim
h→0

sup
k≥kh

sup
(x,a)∈K,Kcompact

|Qh

k(x, a) − Q(x, a)| = 0.

4 Hamilton–Jacobi DQN

The convergence result in the previous section suggests that the optimal Q-function can be esti-
mated in a model-free manner through the use of the semi-discrete HJB equation. However, as
mentioned, it is intractable to directly implement HJ Q-learning (3.3) over a continuous state-action
space. As a practical function approximator, we employ deep neural networks. We then propose
the Hamilton–Jacobi DQN that approximately performs the update (3.3) without discretizing or
approximating the continuous-time system. Since our algorithm has no actor, we only consider a
parameterized Q-function Qθ(x, a), where θ is the parameter vector of the network.

As with DQN, we use a separate target function Qθ−, where the network parameter vector θ−
is updated more slowly than θ. This allows us to update θ by solving a regression problem with an
almost ﬁxed target, resulting in consistent and stable learning [6]. We also use experience replay by
storing transition data (xk, ak, rk, xk+1) in a buﬀer with ﬁxed capacity and by randomly sampling
a mini-batch of transition data {(xj, aj, rj, xj+1)} to update the target value. This reduces bias by
breaking the correlation between sample data that are sequential states [6].

When setting the target value in DQN, the target Q-function needs to be maximized over all
admissible actions, i.e., y−
:= hrj + γ(cid:48) maxa Qθ−(xj+1, a). Evaluating the maximum is tractable
j
in the case of discrete action spaces. However, in our case of continuous action spaces, it is
computationally challenging to maximize the target Q-function with respect to the action variable.
To resolve this issue, we go back to the original HJB equation and use the corresponding optimal
action in Theorem 1. Speciﬁcally, we consider the action dynamics (2.5) with bj := L ∇aQθ− (xj ,aj )
|∇aQθ− (xj ,aj )|
ﬁxed over sampling interval h to obtain

aj+1 = aj + hbj := aj + hL

∇aQθ−(xj, aj)
|∇aQθ−(xj, aj)|

.

(4.1)

8

Algorithm 1: Hamilton–Jacobi DQN

Initialize Q-function Qθ with random weights θ, and target Q-function Qθ− with weights
θ− = θ;
Initialize replay buﬀer with ﬁxed capacity;
for episode = 1 to M do

Randomly sample initial state-action pair (x0, a0);
for k = 0 to K do

Execute action ak and observe reward rk and the next state xk+1;
Store (xk, ak, rk, xk+1) in buﬀer;
Sample the random mini-batch {(xj, aj, rj, xj+1)} from buﬀer;
Set y−
:= hrj + (1 − γh)Qθ−
j
Update θ by minimizing (cid:80)
Update θ− ← (1 − α)θ− + αθ for α (cid:28) 1;
Set the next action as ak+1 := ak + hL ∇aQθ(xk,ak)

(cid:0)xj+1, a(cid:48)
j(y−
j − Qθ(xj, aj))2;

(cid:1) ∀j where a(cid:48)

j

j := aj + hL ∇aQθ(xj ,aj )
|∇aQθ(xj ,aj )| ;

|∇aQθ(xk,ak)| + ε, where ε ∼ N (0, σ2Im);

end for

end for

Using this optimal control action, we can approximate the maximal target Q-function value as
max|a−aj |≤hL Qθ−(xj+1, a) ≈ Qθ−(xj+1, aj + hbj). This approximation becomes more accurate as
h decreases.

Proposition 4. Suppose that Qθ− is twice continuously diﬀerentiable with bounded ﬁrst and second
derivatives. If ∇aQθ−(xj, aj) (cid:54)= 0, we have

(cid:12)
(cid:12)
(cid:12) max
|a−aj |≤hL

lim
h→0

Qθ−(xj+1, a) − Qθ−(xj+1, aj + hbj)

(cid:12)
(cid:12)
(cid:12) = 0.

Moreover, the diﬀerence above is O(h2) as h → 0.

The major advantage of using the optimal action obtained in the continuous-time case is to
avoid explicitly solving the nonlinear optimization problem max|a−aj |≤hL Qθ−(xj+1, a), which is
computationally demanding. With this choice of target Q-function value and the semi-discrete
HJB equation (3.2), we set the target value as y−
:= hrj + (1 − γh)Qθ−(xj+1, aj + hbj). To mitigate
j
the overestimation of Q-functions, we can employ double Q-learning [48] by simply modifying
In this
bj as bj
double Q-learning version, Proposition 4 remains valid except for the O(h2) convergence rate. The
network parameter θ can then be trained to minimize the loss function (cid:80)
j − Qθ(xj, aj))2. For
exploration, we add the additional Gaussian noise ε ∼ N (0, σ2Im) to generate the next action as
ak+1 := ak + hL ∇aQθ(xk,ak)
|∇aQθ(xk,ak)| + ε. The overall algorithm is presented in Algorithm 1.5

|∇aQθ(xj ,aj )| to use a greedy action with respect to Qθ instead of Qθ−.

:= L ∇aQθ(xj ,aj )

j(y−

4.1 Discussion

We now discuss a few notable features of HJ DQN with regard to existing works:

No use of parameterized policies. Most of model-free deep RL algorithms for continuous
control use actor-critic methods [16, 18–20] or policy gradient methods [14, 21] to deal with contin-
uous action spaces. In these methods, by parametrizing policies, the policy improvement step is

5When ∇aQθ(xj, aj) = 0, ∇aQθ (xj ,aj )

|∇aQθ (xj ,aj )| is replaced by an arbitrary vector with norm 1 of the same size.

9

performed in the space of network weights. By doing so, they avoid solving possibly complicated
optimization problems over the policy or action spaces. However, these methods are subject to the
issue of being stuck at local optima in the policy (parameter) space due to the use of gradient-based
algorithms, as pointed out in the literature regarding policy gradient/search [49–51] and actor-critic
methods [52]. Moreover, it is reported that the policy-based methods are sensitive to hyperparame-
ters [53]. Departing from these algorithms, HJ DQN is a value-based method for continuous control
without requiring the use of an actor or a parameterized policy. Previous value-based methods for
continuous control (e.g., [13]) have a computational challenge in ﬁnding a greedy action, which
requires a solution to a nonlinear program. Our method avoids numerically optimizing Q-functions
over the continuous action space through the use of the optimal control (2.5). This is a notable
beneﬁt of the proposed HJB framework.

Continuous-time control. Many existing RL methods for continuous-time dynamical systems
have been designed for linear systems [22–24] or control-aﬃne systems [25, 27–29], in which value
functions and optimal policies can be represented in a simple form. For general nonlinear systems,
Hamilton–Jacobi–Bellman equations have been considered as the optimality equations for state-
value functions v(x) [7,32,34,35]. Unlike these methods, our method uses variant of Q-function and
thus beneﬁts from modern deep RL techniques developed in the literature on DQN. Moreover, as
opposed to discrete-time RL methods, it does not discretize or approximate the system dynamics
and has the ﬂexibility of choosing the sampling interval h in its algorithm design, without needing
a sophisticated ODE discretization method.

4.2 Smoothing

A potential defect of our Lipschitz constrained control setting is that the rate of change in action
has a constant norm L ∇aQ(x(cid:63),a(cid:63))
|∇aQ(x(cid:63),a(cid:63))| . This is also observed in Algorithm 1, where the action is updated
by hL ∇aQθ(xj ,aj )
|∇aQθ(xj ,aj )| . Therefore, the magnitude of ﬂuctuations in action is always ﬁxed as hL, which
may lead to the oscillatory behavior of action. Such oscillatory behaviors are not uncommon in
optimal control (e.g., bang-bang solutions). To alleviate this potential issue, one may introduce
Inspired by [54], we modify the term
an additional smoothing process when updating action.
|∇aQθ(xj ,aj )| by multiplying a smoothing function. Instead of using hL ∇aQθ(xj ,aj )
∇aQθ(xj ,aj )
|∇aQθ(xj ,aj )| in the update of
action, we suggest to use

φ(|∇aQθ(xj, aj)|)∇aQθ(xj, aj)
|∇aQθ(xj, aj)|
where φ : [0, +∞) → [0, 1] is an increasing function with φ(0) = 0 and limr→∞ φ(r) = 1. A typical
example of such a function φ is φ(r) = tanh (cid:0) r
L+r . This action update rule is expected
L
to remove the undesirable oscillatory behavior of action, as conﬁrmed in Section 5.

(cid:1) or φ(r) = r

hL

,

5 Experiments

In this section, we present the empirical performance of our method on benchmark tasks as well as
LQ problems. The source code of our HJ DQN implementation is available online 6.

5.1 Actor Networks vs. Optimal Control ODE

We choose deep deterministic policy gradient (DDPG) [16] as a baseline to compare since it is
another variant of DQN for continuous control. DDPG is an actor-critic method using separate

6https://github.com/HJDQN/HJQ

10

(a) Hopper-v2

(b) Swimmer-v2

(c) HalfCheetah-v2

(d) Walker2d-v2

Figure 1: Learning curves for the OpenAI gym continuous control tasks.

(a) HJ DQN

(b) DDPG

Figure 2: Action trajectories obtained by HJ DQN and DDPG for HalfCheetah-v2.

actor networks while ours is a valued-based method that does not use a parameterized policy.
Although there are state-of-the-art methods built upon DDPG, such as TD3 [19] and SAC [18], we
focus on the comparison between ours and DDPG to examine whether the role of actor networks can
be replaced by the optimal control characterized through our HJB equation. The hyperparameters
used in the experiments are reported in Appendix D.

We consider continuous control benchmark tasks in OpenAI gym [10] simulated by MuJoCo
engine [9]. Figure 1 shows the learning curves for both methods, each of which is tested with
ﬁve diﬀerent random seeds for 1 million steps. The solid curve represents the average of returns
over 20 consecutive evaluations, while the shaded regions represent half a standard deviation of
the average evaluation over ﬁve trials. As shown in Figure 1, the performance of our method is
comparable to that of DDPG when the default sampling interval is used. Ours outperforms DDPG
on Walker2d-v2 while the opposite result is observed in the case of HalfCheetah-v2. As sampling
interval h is a hyperparameter of Algorithm 1, we also identify an optimal h for each task, other
than the default sampling interval. When we test the diﬀerent sampling interval, we also tune
the learning rate α, as suggested in [43]. Precisely, when the sampling interval is multiplied by a
constant from the default interval, the learning rate is also multiplied by the same constant. Except
the HalfCheetah-v2, the ﬁnal performances or learning rate are improved, compared to the default
sampling interval. Overall, the results indicate that actor networks may be replaced by the ODE

0.00.20.40.60.81.0steps (×106)050010001500200025003000HJ DQNHJ DQN (h tuned)DDPG0.00.20.40.60.81.0steps (×106)05001000150020002500average return0.00.20.40.60.81.0steps (×106)200204060801001201400.00.20.40.60.81.0steps (×106)02000400060008000100000.00.20.40.60.81.0steps (×106)0500100015002000250030003500101a0101a1101a2101a3101a402004006008001000steps101a5101a0101a1101a2101a3101a402004006008001000steps101a511

(a)

(b)

Figure 3: Learning curves for the LQ problem: (a) the comparison between HJ DQN and DDPG,
and (b) the eﬀect of problem sizes.

characterization (2.5) of optimal control obtained using our HJB framework. Without using actor
networks, our method has clear advantages over DDPG in terms of hyperparameter tuning and
computational burden.

In Figure 2, we report the action trajectories obtained by HJ DQN and DDPG for HalfCheetah-
v2. The action trajectories obtained by HJ DQN is less oscillating compared to DDPG. This
conﬁrms the fact that oscillations in action are not uncommon in optimal control. In this particular
case of HalfCheetah-v2, where DDPG outperforms HJ DQN, we suspect that fast changes in action
may be needed for good performance. Oscillatory actions may be beneﬁcial to some control tasks.

5.2 Linear-Quadratic Problems

We now consider a classical LQ problem with system dynamics

˙x(t) = Ax(t) + Ba(t),

t > 0,

x(t), a(t) ∈ Rd,

and reward function

r(x, a) = −(x(cid:62)Qx + a(cid:62)Ra),

where Q = Q(cid:62) (cid:23) 0 and R = R(cid:62) (cid:31) 0. Note that our method solves a slight diﬀerent problem due to
the Lipschitz constraint on controls. Thus, the control learned by our method must be suboptimal.
Each component of the system matrices A ∈ Rd×d and B ∈ Rd×d was generated uniformly from
[−0.1, 0.1] and [−0.5, 0.5], respectively. The produced matrix A has an eigenvalue with positive real
part, and therefore the system is unstable. The discount factor γ and Lipschitz constant L are set
to be e−γh = 0.99999 and L = 10. We ﬁrst compare the performance of HJ DQN with DDPG for
the case of d = 20 and report the results in Figure 3a. The learning curves are plotted in the same
manner as the ones in Section 5.1. The y-axis of each ﬁgure is the log of the ratio between the
actual cost and the optimal cost. Therefore, the curve approaches the x-axis as the performance
improves. The result implies that the DDPG cannot reduce the cost at all, whereas HJ DQN
successfully learns an eﬀective (suboptimal) policy. In Figure 3b, we present the learning curves for
HJ DQN with diﬀerent system sizes. Although learning speed is aﬀected by the problem size, HJ
DQN can successfully solve the LQ problem with high-dimensional systems (d = 50). Moreover,

0.000.250.500.751.001.251.501.752.00steps (×104)0.02.55.07.510.012.515.017.520.0log(cost / cost*)HJ DQNDDPG0.000.250.500.751.001.251.501.752.00steps (×104)010203040log(cost / cost*)d=20d=30d=5012

(a) Double Q-learning

(b) Sampling interval

(c) Control constraint

Figure 4: Results of the ablation study using the Swimmer-v2 with respect to (a) double Q-learning,
(b) sampling interval h, and (c) control constraint.

it is observed that the standard deviations over trials are relatively small, and the learning curves
have almost no variation over trials after approximately 104 steps.

5.3 Ablation Study

We make ablations and modiﬁcations to HJ DQN to understand the contribution of each compo-
nent. Figure 4 presents the results for the following design evaluation experiments.

Double Q-learning. We ﬁrst modify our algorithm to test whether double Q-learning con-
tributes to the performance of our algorithm, as in DQN. Speciﬁcally, when selecting actions to
update the target value, we instead use bj := L ∇aQθ− (xj ,aj )
|∇aQθ− (xj ,aj )| to remove the eﬀects of double Q-
learning. Figure 4 (a) shows that double Q-learning improves the ﬁnal performance. This observa-
tion is consistent with the eﬀect of double Q-learning in DQN. Moreover, double Q-learning reduces
the variance of the average return, indicating its contribution to the stability of our algorithm.

Sampling interval. To understand the eﬀect of sampling interval h, we run our algorithm
with multiple values of h. As we mentioned before, we also adjust the learning rate α according to
the sampling interval. As shown in Figure 4 (b), the ﬁnal performance and learning speed increase
as h varies from 0.01 to 0.08 and the ﬁnal performance decreases as h varies from 0.08 to 0.16.
When h is too small, each episode has too many sampling steps; thus, the network is trained in
a small number of episodes given ﬁxed total steps. This limits exploration, thereby decreasing
the performance of our algorithm. On the other hand, as Proposition 4 implies, the target error
increases with sampling interval h. This error is dominant in the case of large h. Therefore, there
exists an optimal sampling interval (h = 0.08 in this task) that presents the best performance.

Control constraint. Recall that admissible controls satisfy the constraint | ˙a(t)| ≤ L. The
parameter L can be derived from speciﬁc control problems or considered as a design choice. We
consider the latter case and display the eﬀect of L on the learning curves in Figure 4 (c). The
ﬁnal reward is the lowest, compared to others, in the case of L = 1 because the set of admissible
controls is too small to allow rapid changes in control signals. HJ DQN, with large enough L
(≥ 10), presents a similar learning speed and performance. The ﬁnal performance and learning
speed slightly decrease as L varies from 20 to 40. This is due to too large variation and frequent
switching in action values, prohibiting a consistent improvement of Q-functions.

Smoothing. Finally, we present the eﬀect of the smoothing process introduced in Section 4.2.

0.20.40.60.81.0steps (×106)020406080100120average returnHJ DQNDouble HJ DQN0.20.40.60.81.0steps (×106)20020406080100120h=0.01h=0.08h=0.02h=0.16h=0.040.20.40.60.81.0steps (×106)20020406080100120L=1L=20L=10L=4013

Figure 5: Eﬀect of smoothing on the 20-dimensional LQ problem.

Figure 5 shows |x(t)| and |a(t)| generated by the control learned with and without smoothing on
(cid:1) is chosen as the smoothing function. As
the 20-dimensional LQ problem. Here, φ(r) = tanh (cid:0) r
L
expected, with no smoothing process, the action trajectory shows wobbling oscillations (solid blue
line). However, when the smoothing process is applied, the action trajectory has no such undesirable
oscillations and presents a smooth behavior (solid red line). Regarding |x(t)|, the smoothing process
has only a small eﬀect. Therefore, the smoothing process can eliminate oscillations in action without
signiﬁcantly aﬀecting the state trajectory.

6 Conclusions

We have presented a new theoretical and algorithmic framework that extends DQN to continuous-
time deterministic optimal control for continuous action space. A novel class of HJB equations
for Q-functions has been derived and used to construct a Q-learning method for continuous-time
control. We have shown the theoretical convergence properties of this method. For practical imple-
mentation, we have combined the HJB-based method with DQN, resulting in a simple algorithm
that solves continuous-time control problems without an actor network. Beneﬁting from our the-
oretical analysis of the HJB equations, this model-free oﬀ-policy algorithm does not require any
numerical optimization for selecting greedy actions. The result of our experiments indicates that
actor networks in DDPG may be replaced by our optimal control simply characterized via an ODE,
while reducing computational eﬀort. Our HJB framework may provide an exciting avenue for future
research in continuous-time RL in terms of improving the exploration capability with maximum
entropy methods, and exploiting the beneﬁts of models with theoretical guarantees.

A Viscosity Solution of the Hamilton–Jacobi Equations

The Hamilton–Jacobi equation is a partial diﬀerential equation of the form

F (z, u(z), ∇zu(z)) = 0,

z ∈ Rk,

(A.1)

where F : Rk × R × Rk → R. A function u : Rk → R that solves the HJ equation is called a (strong)
solution. However, such a strong solution exists only in limited cases. To consider a broad class of HJ

0246810t1234norm|x(t)||a(t)||x(t)| (smoothing)|a(t)| (smoothing)14

equations, it is typical to adopt the concept of weak solutions. Among these, the viscosity solution
is the most relevant to dynamic programming and optimal control problems [46, 47]. Speciﬁcally,
under a technical condition, the viscosity solution is unique and corresponds to the value function of
a continuous-time optimal control problem. In the following deﬁnition, C(Rk) and C1(Rk) denote
the set of continuous functions and the set of continuously diﬀerentiable functions respectively.

Deﬁnition 1. A function u ∈ C(Rk) is called the viscosity solution of (A.1) if it satisﬁes the
following conditions:

1. For any φ ∈ C1(Rk) such that u − φ attains a local maximum at z0,

F (z0, u(z0), ∇zφ(z0)) ≤ 0;

2. For any φ ∈ C1(Rk) such that u − φ attains a local minimum at z0,

F (z0, u(z0), ∇zφ(z0)) ≥ 0.

Note that the viscosity solution does not need to be diﬀerentiable. In our case, the HJB equation

(2.4)

γQ(x, a) − ∇xQ(x, a) · f (x, a) − L|∇aQ(x, a)| − r(x, a) = 0

can be expressed as (A.1) with

F (z, q, p) = γq − p1 · f (z) − L|p2| − r(z),

where z = (x, a) ∈ Rn × Rm and p = (p1, p2) ∈ Rn × Rm. We can show that the HJB equation
admits a unique viscosity solution, which coincides with the optimal Q-function.

Theorem 3. Suppose that Assumption 1 holds.7 Then, the optimal continuous-time Q-function is
the unique viscosity solution to the HJB equation (2.4).

Proof. First, recall that our control trajectory satisﬁes the constraint | ˙a| ≤ L. Therefore, our
dynamical system can be written in the following extended form:

˙x(t) = f (x(t), a(t)),

˙a(t) = b(t),

t > 0,

|b(t)| ≤ L,

by viewing x(t) and a(t) as state variables. More precisely, the dynamics of the extended state
variable z(t) = (x(t), a(t)) can be written as

˙z(t) = G(z(t), b(t)),

t > 0,

|b(t)| ≤ L,

(A.2)

(cid:26) (cid:90) t+h

where G(z, b) = (f (z), b). Applying the dynamic programming principle to the Q-function, we
have

Q(z) = sup

e−γ(s−t)r(z(s)) ds + e−γhQ(z(t + h)) | z(t) = z

.

|b(t)|≤L

t

The remaining proof is almost the same as the proof of Proposition 2.8, Chapter 3 in [46]. However,
for the self-completeness of the paper, we provide a detailed proof. In the following, we show that
the Q-function satisﬁes the two conditions in Deﬁnition 1.

First, let φ ∈ C1(Rn+m) such that Q − φ attains a local maximum at z. Then, there exists
δ > 0 such that Q(z) − Q(z(cid:48)) ≥ φ(z) − φ(z(cid:48)) for |z(cid:48) − z| < δ. Since f and r are bounded

7Assumption 1 can be relaxed by using a modulus associated with each function as in Chapter III.1–3 in [46].

(cid:27)

15

Lipschitz continuous, there exists h0 > 0, which is independent of b(s), such that |z(s) − z| ≤ δ,
|r(z(s)) − r(z)| ≤ C(s − t) and |f (z(s)) − f (z)| ≤ C(s − t) for t ≤ s ≤ t + h0, where z(s) is
a solution to (A.2) for s ≥ t with z(t) = z. Now, the dynamic programming principle for the
Q-function implies that, for any 0 < h < h0 and ε > 0, there exists b(s) with |b(s)| ≤ L such that

Q(z) ≤

(cid:90) t+h

t

e−γ(s−t)r(z(s)) ds + e−γhQ(z(t + h)) + hε,

where z(s) is now a solution to (A.2) with z(t) = z under the particular choice of b. On the other
hand, it follows from our choice of h that

(cid:90) t+h

t

e−γ(s−t)r(z(s)) ds =

(cid:90) t+h

t

e−γ(s−t)r(z) ds + o(h),

Q(z) ≤

(cid:90) t+h

t

e−γ(s−t)r(z) ds + e−γhQ(z(t + h)) + hε + o(h).

which implies that

Therefore, we have

φ(z) − φ(z(t + h)) ≤ Q(z) − Q(z(t + h))

(cid:90) t+h

≤

t

e−γ(s−t)r(z) ds + (e−γh − 1)Q(z(t + h)) + hε + o(h).

Since the left-hand side of the inequality above is equal to − (cid:82) t+h
G(z(s), b(s)) ds, we obtain that

t

d

ds φ(z(s)) ds = − (cid:82) t+h

t ∇zφ(z(s))·

(cid:90) t+h

0 ≤

∇zφ(z(s)) · G(z(s), b(s)) ds

t

+

(cid:90) t+h

e−γ(s−t)r(z) ds +

(cid:16)

e−γh − 1

(cid:17)

Q(z(t + h)) + hε + o(h)

t
(cid:90) t+h

≤

(∇xφ(z(s)) · f (z) + L|∇aφ(z(s))|) ds

t

+

(cid:90) t+h

t

e−γ(s−t)r(z) ds +

(cid:16)

e−γh − 1

(cid:17)

Q(z(t + h)) + hε + o(h).

By dividing both sides by h and letting h → 0, we conclude that

∇xφ(z) · f (z) + L|∇aφ(z)| + r(z) − γQ(z) + ε ≥ 0.

Since ε was arbitrarily chosen, we conﬁrm that the Q-function satisﬁes the ﬁrst condition in Deﬁ-
nition 1, i.e.,

γQ(z) − ∇xφ(z) · f (z) − L|∇aφ(z)| − r(z) ≤ 0.

We now consider the second condition. Let φ ∈ C1(Rn+m) such that Q − φ attains a local
minimum at z, i.e., there exists δ such that Q(z) − Q(z(cid:48)) ≤ φ(z) − φ(z(cid:48)) for |z(cid:48) − z| < δ. Fix an
arbitrary b ∈ Rm such that |b| ≤ L and let b(s) ≡ b be a constant function. Let z(s) be a solution

16

to (A.2) for s ≥ t with z(t) = z under the particular choice of b(s) ≡ b. Then, for suﬃciently small
h, |z(t + h) − z| ≤ δ, and therefore we have

Q(z) − Q(z(t + h)) ≤ φ(z) − φ(z(t + h)) = −

(cid:90) t+h

t

d
ds

φ(z(s)) ds

= −

(cid:90) t+h

t

∇zφ(z(s)) · G(z(s), b) ds.

(A.3)

On the other hand, the dynamic programming principle yields

Q(z) − Q(z(t + h)) ≥

(cid:90) t+h

t

By (A.3) and (A.4), we have

e−γ(s−t)r(z(s)) ds + (e−γh − 1)Q(z(t + h)).

(A.4)

(e−γh − 1)Q(z(t + h)) +

(cid:90) t+h

t

e−γ(s−t)r(z(s)) ds ≤ −

(cid:90) t+h

t

∇zφ(z(s)) · G(z(s), b) ds.

Dividing both sides by h and letting h → 0, we obtain that

−γQ(z) + r(z) ≤ −∇zφ(z) · (f (z), b),

or equivalently

Since b was arbitrarily chosen from {b ∈ Rm : |b| ≤ L}, we have

γQ(z) − ∇xφ(z) · f (z) − ∇aφ(z) · b − r(z) ≥ 0.

γQ(z) − ∇xφ(z) · f (z) − L|∇aφ(z)| − r(z) ≥ 0,

which conﬁrms that the Q-function satisﬁes the second condition in Deﬁnition 1. Therefore, we
conclude that the Q-function is a viscosity solution of the HJB equation (2.4).

Lastly, the uniqueness of the viscosity solution can be proved by using Theorem 2.12, Chapter

3 in [46].

B Proofs

B.1 Proposition 1

Proof. Fix (x, a, t) ∈ Rn × Rm × [0, T ]. Let ε be an arbitrary positive constant. Then, there exists
a ∈ A such that (cid:82) T
t r(x(s), a(s)) ds + q(x(T )) < v(x, t) + ε, where x(s) satisﬁes (2.1) with x(t) = x
in the Carath´eodory sense: x(s) = x + (cid:82) s
t f (x(τ ), a(τ )) dτ . We now construct a new control ˜a ∈ A
as ˜a(s) := a if s = t; ˜a(s) := a(s) if s > t. Such a modiﬁcation of controls at a single point does
not aﬀect the trajectory or the total cost. Therefore, we have

v(x, t) ≤ Q(x, a, t) ≤

(cid:90) T

t

r(x(s), ˜a(s)) ds + q(x(T )) < v(x, t) + ε.

(B.1)

Since ε was arbitrarily chosen, we conclude that v(x, t) = Q(x, a, t) for any u ∈ Rm.

17

B.2 Theorem 1

Proof. The classical theorem for the necessary and suﬃcient condition of optimality (e.g. Theorem
2.54, Chapter III in [46]) implies that a(cid:63) is optimal among those in A such that a(t) = a if and
only if

p1 · f (x(cid:63)(s), a(cid:63)(s)) + p2 · ˙a(cid:63)(s) + r(x(cid:63)(s), a(cid:63)(s))
= max
|b|≤L

{p1 · f (x(cid:63)(s), a(cid:63)(s)) + p2 · b + r(x(cid:63)(s), a(cid:63)(s))}

for all p = (p1, p2) ∈ D±Q(x(cid:63)(s), a(cid:63)(s)). This optimality condition can be expressed as the desired
ODE (2.5). Thus, its solution a(cid:63) with a(cid:63)(t) = a satisﬁes (2.6).

Suppose now that a ∈ arg maxa(cid:48)∈Rm Q(x, a(cid:48)). It follows from the deﬁnition of Q that

e−γ(s−t)r(x(s), a(s)) ds | x(t) = x

(cid:27)

(cid:26)(cid:90) ∞

t

max
a∈A

= max
a(cid:48)∈Rm

max
a∈A

(cid:26) (cid:90) ∞

t

e−γ(s−t)r(x(s), a(s)) ds | x(t) = x, a(t) = a(cid:48)

(cid:27)

Q(x, a(cid:48)) = Q(x, a) = max
a∈A

(cid:26)(cid:90) ∞

t

e−γ(s−t)r(x(s), a(s))ds | x(t) = x, a(t) = a

(cid:27)

= max
a(cid:48)∈Rm
(cid:90) ∞

=

e−γ(s−t)r(x(cid:63)(s), a(cid:63)(s)) ds.

t

Therefore, a(cid:63) is an optimal control.

B.3 Proposition 2

Proof. We ﬁrst show that Qh,(cid:63) satisﬁes (3.2). Fix an arbitrary sequence b := {bn}∞
follows from the deﬁnition of Qh,b that

n=0 ∈ B.

It

Qh,b(x, a) = hr(x, a) + (1 − γh)Qh,˜b(ξ(x, a; h), a + hb0).

where ˜b := {b1, b2, . . .} ∈ B. Since Qh,˜b(ξ(x, a; h), a + hb0) ≤ Qh,(cid:63)(ξ(x, a; h), a + hb0), we have

Qh,b(x, a) ≤ hr(x, a) + (1 − γh)Qh,(cid:63)(ξ(x, a; h), a + hb0)

≤ hr(x, a) + (1 − γh) sup
|b|≤L

(cid:110)

Qh,(cid:63)(ξ(x, a; h), a + hb)

(cid:111)

.

Taking supremum of both sides with respect to b ∈ B yields

Qh,(cid:63)(x, a) ≤ hr(x, a) + (1 − γh) sup
|b|≤L

(cid:110)

Qh,(cid:63)(ξ(x, a; h), a + hb)

(cid:111)

.

(B.2)

To obtain the other direction of inequality, we ﬁx an arbitrary b ∈ Rm such that |b| ≤ L. Let
n=0 ∈ B

x(cid:48) := ξ(x, a; h) and a(cid:48) := a + hb. Fix an arbitrary ε > 0 and choose a sequence c := {cn}∞
such that

Qh,(cid:63)(x(cid:48), a(cid:48)) ≤ Qh,c(x(cid:48), a(cid:48)) + ε.

We now construct a new sequence ˜c := {b, c0, c1, . . .} ∈ B. Then,

Qh,˜c(x, a) = hr(x, a) + (1 − γh)Qh,c(x(cid:48), a(cid:48)) ≥ hr(x, a) + (1 − γh)(Qh,(cid:63)(x(cid:48), a(cid:48)) − ε),

which implies that

Qh,(cid:63)(x, a) ≥ Qh,˜c(x, a) ≥ hr(x, a) + (1 − γh)(Qh,(cid:63)(x(cid:48), a(cid:48)) − ε).

Taking the supremum of both sides with respect to b ∈ Rm such that |b| ≤ L yields

Qh,(cid:63)(x, a) ≥ hr(x, a) − (1 − γh)ε + (1 − γh) sup
|b|≤L

Since ε was arbitrarily chosen, we ﬁnally obtain that

(cid:110)

Qh,(cid:63)(ξ(x, a; h), a + hb)

(cid:111)

18

.

Qh,(cid:63)(x, a) ≥ hr(x, a) + (1 − γh) sup
|b|≤L

(cid:110)

Qh,(cid:63)(ξ(x, a; h), a + hb)

.

(B.3)

(cid:111)

Combining two estimates (B.2) and (B.3), we conclude that Qh,(cid:63) satisﬁes the semi-discrete HJB
equation (3.2). Since the proof for the uniqueness of the solution is almost the same as the proof
of Theorem 4.2, Chapter VI in [46], we have omitted the detailed proof.

B.4 Proposition 3

Proof. For the completeness of the paper, we provide a sketch of the proof although it is similar to
the proof of Theorem 1.1, Chapter VI in [46]. We begin by deﬁning two functions Q(cid:63) and Q

as

(cid:63)

Q(cid:63)(x, a) :=

lim inf
(x(cid:48),a(cid:48),h)→(x,a,0+)

Qh,(cid:63)(x(cid:48), a(cid:48)),

(cid:63)
Q

(x, a) :=

lim sup
(x(cid:48),a(cid:48),h)→(x,a,0+)

Qh,(cid:63)(x(cid:48), a(cid:48)).

(cid:63)
According to the proof of Theorem 1.1, Chapter VI in [46], it suﬃces to show that Q
satisﬁes the
ﬁrst condition of Deﬁnition 1 and Q(cid:63) satisﬁes the second condition of Deﬁnition 1. To this end,
(cid:63)
for any φ ∈ C1, let (x0, a0) be a strict local maximum point of Q
− φ and choose a small enough
(cid:63)
neighborhood N of (x0, a0) such that (Q
− φ). Then, there exists a
sequence {(xn, an, hn)} with (xn, an) → (x0, a0) and hn → 0+ such that

(cid:63)
− φ)(x0, a0) = maxN (Q

(Qhn,(cid:63) − φ)(xn, an) = max

(Qhn,(cid:63) − φ)

N

and

(cid:63)
Qhn,(cid:63)(xn, an) → Q

(x0, a0).

Recall that Qh,(cid:63) satisﬁes (3.2). Thus, there exists bn with |bn| ≤ L such that

Qhn,(cid:63)(xn, an) − hnr(xn, an) − (1 − γhn)Qhn,(cid:63)(ξ(xn, an; hn), an + hbn) = 0.

Since Qhn,(cid:63) − φ attains a local maximum at (xn, an), we have

(1 − γhn)(φ(xn, an) − φ(ξ(xn, an; hn), an + hbn) + γhnQhn,(cid:63)(xn, an) − hnr(xn, an) ≤ 0

(B.4)

for small enough hn > 0. Since |bn| ≤ L for all n ≥ 0, there exists a subsequence nk and b with
|b| ≤ L such that bnk → b as k → ∞. Then, we substitute n in (B.4) by nk, divide both sides by
hnk and let k → ∞ to obtain that at (x0, a0)

where we use the fact that

(cid:63)
−∇xφ · f − ∇aφ · b + γQ

− r ≤ 0,

lim
h→0

ξ(x, a; h) − x
h

= f (x, a).

This implies that the ﬁrst condition of Deﬁnition 1 is satisﬁed. Similarly, it can be shown that Q(cid:63)
satisﬁes the second condition of Deﬁnition 1.

19

B.5 Theorem 2

We begin by deﬁning an optimal Bellman operator in the semi-discrete setting, T h : L∞ → L∞, by

(T hQ)(x, a) := hr(x, a) + (1 − γh) sup
|b|≤L

Q(ξ(x, a; h), a + hb),

(B.5)

where ξ(x, a; h) denotes the solution of the ODE (2.1) at time t = h with initial state x(0) = x
and constant action a(t) ≡ a for t ∈ [0, h). Our ﬁrst observation is that the Bellman operator is a
monotone (1 − γh)-contraction mapping for a suﬃciently small h.

Lemma 1. Suppose that 0 < h < 1
mapping. More precisely, it satisﬁes the following properties:

γ . Then, the Bellman operator T h is a monotone contraction

(i) T hQ ≤ T hQ(cid:48) for all Q, Q(cid:48) ∈ L∞ such that Q ≤ Q(cid:48);

(ii) (cid:107)T hQ − T hQ(cid:48)(cid:107)L∞ ≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞ for all Q, Q(cid:48) ∈ L∞.

Proof. (i) Since Q(x, a) ≤ Q(cid:48)(x, a) for all (x, a) ∈ Rn × Rm, we have

sup
|b|≤L

Q(ξ(x, a; h), a + hb) ≤ sup
|b|≤L

Q(cid:48)(ξ(x, a; h), a + hb).

Multiplying (1 − γh) and then adding hr(x, a) to both sides, we conﬁrm the monotonicity of T h
as desired.

(ii) We ﬁrst note that for any b ∈ Rm with |b| ≤ L,

(cid:2)hr(x, a) + (1 − γh)Q(ξ(x, a; h), a + hb)(cid:3) − (cid:2)hr(x, a) + (1 − γh)Q(cid:48)(ξ(x, a; h), a + hb)(cid:3)
= (1 − γh)(cid:2)Q(ξ(x, a; h), a + hb) − Q(cid:48)(ξ(x, a; h), a + hb)(cid:3)
≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞.

By the deﬁnition of T hQ(cid:48), we have

hr(x, a) + (1 − γh)Q(ξ(x, a; h), a + hb)
≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞ + hr(x, a) + (1 − γh)Q(cid:48)(ξ(x, a; h), a + hb)
≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞ + T hQ(cid:48)(x, a).

Taking the supremum of both sides with respect to b ∈ Rm such that |b| ≤ L, yields

T hQ(x, a) ≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞ + T hQ(cid:48)(x, a),

or equivalently

T hQ(x, a) − T hQ(cid:48)(x, a) ≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞.

We now change the role of Q and Q(cid:48) to obtain

|T hQ(x, a) − T hQ(cid:48)(x, a)| ≤ (1 − γh)(cid:107)Q − Q(cid:48)(cid:107)L∞.

Therefore, the operator T h is a (1 − γh)-contraction with respect to (cid:107) · (cid:107)L∞.

Using the Bellman operator T h, HJ Q-learning (3.3) can be expressed as

20

Qh

k+1 := (1 − αk)Qh

k + αkT hQh
k.

Consider the diﬀerence ∆h
kth iteration. It satisﬁes

k := Qh

k − Qh,(cid:63). Note that (cid:107)∆h

k(cid:107)L∞ represents the optimality gap at the

∆h

k+1 = (1 − αk)∆h

k + αk[T h(∆h

k + Qh,(cid:63)) − T hQh,(cid:63)],

(B.6)

where we used the semi-discrete HJB equation Qh,(cid:63) = T hQh,(cid:63). The contraction property of the
Bellman operator T h can be used to show that the optimality gap (cid:107)∆h
k(cid:107)L∞ decreases geometrically.
More precisely, we have the following lemma:

Lemma 2. Suppose that 0 < h < 1
inequality holds:

γ , 0 ≤ αk ≤ 1 and that Assumption 1 holds. Then, the following

(cid:107)∆h

k(cid:107)L∞ ≤

(cid:18) k−1
(cid:89)

(1 − ατ γh)

τ =0

(cid:19)

(cid:107)∆h

0 (cid:107)L∞.

Proof. We use mathematical induction to prove the assertion. When k = 1, it follows from the
Q-function update (3.3) and the contraction property of T h that

(cid:107)∆h

1 (cid:107)L∞ ≤ (1 − α0)(cid:107)∆h
≤ (1 − α0)(cid:107)∆h
= (1 − α0γh)(cid:107)∆h

0 (cid:107)L∞.

0 (cid:107)L∞ + α0(cid:107)T h(∆h
0 (cid:107)L∞ + α0(1 − γh)(cid:107)∆h

0 (cid:107)L∞

0 + Qh,(cid:63)) − T hQh,(cid:63)(cid:107)L∞

Therefore, the assertion holds for k = 1. We now assume that the assertion holds for k = n:

(cid:32)n−1
(cid:89)

(cid:33)

(cid:107)∆h

n(cid:107)L∞ ≤

(1 − ατ γh)

(cid:107)∆h

0 (cid:107)L∞.

We need to show that the inequality holds for k = n + 1. By using the same estimate as in the case
of k = 1 and the induction hypothesis for k = n, we obtain

τ =0

n(cid:107)L∞ + αn(cid:107)T h(∆h
n(cid:107)L∞ + αn(1 − γh)(cid:107)∆h

n(cid:107)L∞

n + Qh,(cid:63)) − T hQh,(cid:63)(cid:107)L∞

(cid:107)∆h

n+1(cid:107)L∞ ≤ (1 − αn)(cid:107)∆h
≤ (1 − αn)(cid:107)∆h
= (1 − αnγh)(cid:107)∆h
(cid:32)n−1
(cid:89)

≤ (1 − αnγh)

n(cid:107)L∞

(cid:33)

(1 − ατ γh)

(cid:107)∆h

0 (cid:107)L∞

(cid:32) n
(cid:89)

=

τ =0

(cid:33)

(1 − ατ γh)

(cid:107)∆h

0 (cid:107)L∞.

τ =0

This completes our mathematical induction, and thus the result follows.

This lemma yields a condition on the sequence of learning rates under which the Q-function

updated by (3.3) converges to the optimal semi-discrete Q-function (3.1) in L∞.

21

Proof. It suﬃces to show that

lim
k→∞

(cid:107)∆h

k(cid:107)L∞ = 0.

By Lemma 2 and the elementary inequality 1 − x ≤ e−x, we have

(cid:32)k−1
(cid:89)

(cid:107)∆h

k(cid:107)L∞ ≤

(cid:33)

(cid:32)

(1 − ατ γh)

(cid:107)∆h

0 (cid:107)L∞ ≤ exp

−γh

τ =0

(cid:33)(cid:33)

(cid:32)k−1
(cid:88)

τ =0

ατ

(cid:107)∆h

0 (cid:107)L∞.

Therefore, if (cid:80)∞

τ =0 ατ = ∞, the result follows.

B.6 Corollary 1
Proof. We ﬁrst observe that there exists an index kh, depending on h, such that (cid:80)kh−1
since (cid:80)∞

τ =0 ατ = ∞. Then, we have

τ =0 ατ > 1
h2

(cid:32)kh−1
(cid:88)

h

τ =0

(cid:33)

ατ

>

1
h

→ ∞ as h → 0.

Moreover, by the triangle inequality, we have

|Qh

k(x, a) − Q(x, a)| ≤ |Qh

k(x, a) − Qh,(cid:63)(x, a)| + |Qh,(cid:63)(x, a) − Q(x, a)|

for all (x, a) ∈ Rn × Rm. By Proposition 2, the second term on the right-hand side uniformly
vanishes over any compact subset K of Rn × Rm as h → 0. The ﬁrst term is nothing but |∆h
k(x, a)|,
which is bounded as follows (by Lemma 2):

|∆h

k(x, a)| ≤

(cid:32)k−1
(cid:89)

(cid:33)

(cid:32)

(1 − ατ γh)

(cid:107)∆h

0 (cid:107)L∞ ≤ exp

−γh

τ =0

(cid:33)(cid:33)

(cid:32)k−1
(cid:88)

τ =0

ατ

(cid:107)∆h

0 (cid:107)L∞,

k ≥ 1,

where the second inequality holds because 1 − x ≤ e−x. Our choice of kh then yields

(cid:32)

(cid:107)∆h

k(cid:107)L∞ ≤ exp

−γh

sup
k≥kh

(cid:33)(cid:33)

ατ

(cid:107)∆h

0 (cid:107)L∞ → 0

(cid:32)kh−1
(cid:88)

τ =0

as h → 0. Therefore, we conclude that

sup
k≥kh

sup
(x,a)∈K
Kcompact

|Qh

k(x, a) − Q(x, a)|

≤ sup
k≥kh

≤ sup
k≥kh

as h → 0.

sup
(x,a)∈K
Kcompact
(cid:107)∆h

|Qh

k(x, a) − Qh,(cid:63)(x, a)| + sup
k≥kh

sup
(x,a)∈K
Kcompact
|Qh,(cid:63)(x, a) − Q(x, a)| → 0

k(cid:107)L∞ + sup

(x,a)∈K
Kcompact

|Qh,(cid:63)(x, a) − Q(x, a)|

B.7 Proposition 4

Proof. We ﬁrst notice that by the triangle inequality,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
|a−aj |≤hL

(cid:12)
(cid:12)
Qθ−(xj+1, a) − Qθ−(xj+1, aj + hbj)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
|a−aj |≤hL
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Qθ−

(cid:18)

+

Qθ−(xj+1, a) − Qθ−

xj+1, aj + hL

(cid:18)

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

xj+1, aj + hL

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

(cid:19)

(cid:12)
(cid:12)
− Qθ−(xj+1, aj + hbj)
(cid:12)
(cid:12)

22

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

=: ∆1 + ∆2.

We ﬁrst consider ∆1. Let a(cid:63) := arg max|a−aj |≤hL Qθ−(xj+1, a). By the Taylor expansion, we

have

max
|a−aj |≤hL

Qθ−(xj+1, a) = Qθ−(xj+1, a(cid:63))

= Qθ−(xj+1, aj) + ∇aQθ−(xj+1, aj) · (a(cid:63) − aj) + O(h2).

Similarly, we again use the Taylor expansion to obtain that

(cid:18)

Qθ−

xj+1, aj + hL

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

(cid:19)

= Qθ−(xj+1, aj) + hL|∇aQθ−(xj+1, aj)| + O(h2).

Subtracting one equality from another yields

Qθ−(xj+1, a(cid:63)) − Qθ−

(cid:18)

xj+1, aj + hL

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

(cid:19)

= ∇aQθ−(xj+1, aj) · (a(cid:63) − aj) − hL|∇aQθ−(xj+1, aj)| + O(h2) ≤ O(h2),

where the last inequality holds because |a(cid:63) − aj| ≤ hL. Since our choice of a(cid:63) implies that the
left-hand side of the inequality above is always non-negative, we conclude that ∆1 = O(h2).

Regarding ∆2, we have

∆2 =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)

Qθ−

xj+1, aj + hL

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

(cid:19)

(cid:18)

− Qθ−

xj+1, aj + hL

∇aQθ−(xj, aj)
|∇aQθ−(xj, aj)|

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Lh(cid:107)∇aQθ−(cid:107)L∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇aQθ−(xj+1, aj)
|∇aQθ−(xj+1, aj)|

−

∇aQθ−(xj, aj)
|∇aQθ−(xj, aj)|

(cid:12)
(cid:12)
.
(cid:12)
(cid:12)

Note that for any two non-zero vectors v, w,
(cid:18) 1
|v|

|v − w|
|v|

w
|w|

v
|v|

+ |w|

≤

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

−

1
|w|

=

|v − w|
|v|

+

|w| − |v|
|v|

≤

2|v − w|
|v|

.

On the other hand, we have

|∇aQθ−(xj+1, aj) − ∇aQθ−(xj, aj)| ≤ (cid:107)∇2

xaQθ−(cid:107)L∞|xj+1 − xj| = O(h).

Since we assume that Qθ− is twice diﬀerentiable and |∇aQθ−(xj, aj)| =: C > 0, we have |∇aQθ−(xj+1, aj)| >
C/2 for suﬃciently small h. Therefore, we obtain that

∆2 ≤ 2Lh(cid:107)∇aQθ−(cid:107)L∞

|∇aQθ−(xj+1, aj) − ∇aQθ−(xj, aj)|
|∇aQθ−(xj+1, aj)|

= O(h2).

Combining the estimates of ∆1 and ∆2 yields
(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
|a−aj |≤hL

(cid:12)
(cid:12)
Qθ−(xj+1, a) − Qθ−(xj+1, aj + hbj)
(cid:12)
(cid:12)

23

= O(h2)

as desired.

C Brief Discussion on Extension to Stochastic Systems

The Hamilton-Jacobi Q-learning can be extended to the continuous-time stochastic control setting
with controlled diﬀusion processes. Consider the following stochastic counterpart of the system
(2.1):

(C.1)
where σ : Rn × Rm → Rn×k is the diﬀusion coeﬃcient and Wt is the k-dimensional standard
Bronwian motion. We now deﬁne the Q-function as

dxt = f (xt, at)dt + σ(xt, at)dWt,

t > 0,

Q(x, a) := sup
a∈A

E

(cid:20)(cid:90) ∞

(cid:21)
e−γtr(xt, at)dt | x0 = x, a0 = a

.

0

Again, the dynamic programming principle implies

0 = sup
a∈A

E

(cid:20) 1
h

(cid:90) t+h

e−γ(s−t)r(x(s), a(s)) ds +

1
h

[Q(x(t + h), a(t + h)) − Q(x, a)]

t
e−γh − 1
h

+

Q(x(t + h), a(t + h)) | x(t) = x, a(t) = a

(cid:21)
.

(C.2)

Then, we use the Itˆo formula

dQ(xt, at) = ∇xQ · dxt + ∇aQ · ˙adt +

1
2

dx(cid:62)

t ∇2

xQdxt

= ∇xQ · (f (xt, at)dt + σ(xt, at)dWt) + ∇aQ · ˙adt +

(dWt)(cid:62)σ(cid:62)∇2

xQσdWt

2

to derive the following Hamilton-Jacobi-Bellman equation for the stochastic system (C.1):

γQ − ∇xQ · f (x, a) − L|∇aQ| − r(x, a) −

tr(σ(cid:62)∇2

xQσ)

2

= 0.

(C.3)

Note that, in this case also, the optimal control satisﬁes ˙a = L ∇aQ

|∇aQ| when Q is diﬀerentiable.

Since in most practical systems transition samples are collected in discrete time, we also in-
troduce the semi-discrete version of (C.3). We deﬁne a stochastic semi-discrete Q-function Qh,(cid:63)
as

Qh,(cid:63)(x, a) := sup
b∈B

E

h

r(xk, ak)(1 − γh)k

,

(cid:35)

(cid:34)

∞
(cid:88)

k=0

k=0 | bk ∈ Rm, |bk| ≤ L}, xk+1 = ξ(xk, ak; h) and ak+1 = ak + hbk. Here,
where B := {b := {bk}∞
ξ(xk, ak; h) is now a solution to the stochastic diﬀerential equation (C.1) at time t = h with initial
state x and constant control a(t) ≡ a, t ∈ [0, h). Then, similar to the deterministic semi-discrete
HJB equation (3.2), its stochastic counterpart can be written as follows:

Qh,(cid:63)(x, a) = hr(x, a) + (1 − γh) sup
|b|≤L

(cid:104)

(cid:105)
Qh,(cid:63)(ξ(x, a; h), a + hb)

.

E

Table 1: Hyperparameters for HJ DQN.

24

Hyperparameter
optimizer
learning rate
Lipschitz constant (L)
default sampling interval (h)
tuned sampling interval (h)
(Continuous) discount (γ)
replay buﬀer size
target smoothing coeﬃcient (α)
Noise coeﬃcient (σ)
number of hidden layers
number of hidden units per layer
number of samples per minibatch
nonlinearity

10−4
30
0.008
0.032

5 × 10−4
30
0.05
0.01

HalfCheetah-v2 Hopper-v2 Walker2d-v2
Adam [57]
10−4
30
0.008
0.016
− log(0.99)/h, where h is the sampling interval
106
0.001
0.1
2 (fully connected)
256
128
ReLU

Swimmer-v2

Adam [57]

5 × 10−4
15
0.04
0.08

LQ

10−3
10
0.05
-

− log(0.99)/h − log(0.99999)/h

106

2 × 104

0.001
0.1
2 (fully connected)
256

128

512

ReLU

Using Robbins-Monro stochastic approximation [55, 56], we obtain the following model-free update
rule: in the kth iteration, we collect data (xk, ak, rk, xk+1) and update the Q-function by

Qh

k+1(xk, ak) := (1 − αk)Qh

k(xk, ak) + αk

(cid:104)

hrk + (1 − γh) sup
|b|≤L

Qh

k(xk+1, ak + hb)

(cid:105)
,

(C.4)

where xk+1 is obtained by simulating the stochastic system from xk with action ak ﬁxed for h
period, i.e., xk+1 = ξ(xk, ak; h). The corresponding HJ DQN algorithm for stochastic systems is
essentially the same as Algorithm 1 although the transition samples are now collected through the
stochastic system.

D Implementation Details

All the simulations in Section 5 were conducted using Python 3.7.4 on a PC with Intel Core i9-9900X
@ 3.50GHz, NVIDIA GeForce RTX 2080 Ti and 64GB RAM.

25

Table 2: Hyperparameters for DDPG.

Hyperparameter
optimizer
actor learning rate
critic learning rate
(Discrete) discount (γ(cid:48))
replay buﬀer size
target smoothing coeﬃcient (α)
number of hidden layers
number of hidden units per layer
number of samples per minibatch
nonlinearity

MuJoCo tasks

LQ

Adam [57]
10−4
10−3

0.99
106

0.99999
2 × 104

0.001
2 (fully connected)
256

128

512

ReLU

Table 1 shows the list of hyperparameters that are used in our implementation of HJ DQN
for each MuJoCo task and the LQ problem. For DDPG, we list our choice of hyperparameters
in Table 2, which are taken from [16] for MuJoCo tasks, except the network architecture which is
used in OpenAI’s implementation of DDPG8. The discount factor in the discrete-time algorithms
is chosen as γ(cid:48) = 0.99 for MuJoCo tasks and 0.99999 for the LQ problem so that it is equivalent to
e−γh ≈ (1 − γh) in our algorithm for continuous-time systems.

References

[1] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA: Athena

Scientiﬁc, 1996.

[2] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA:

MIT Press, 1998.

[3] C. Szepesvari, Algorithms for Reinforcement Learning. San Rafael, CA: Morgan and Claypool

Publishers, 2010.

[4] L. Ljung, System Identiﬁcation: Theory for the User, 2nd ed. Pearson, 1998.

[5] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, pp. 279–292, 1992.

[6] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, and S. Petersen, “Human-level control through
deep reinforcement learning,” Nature, vol. 518, pp. 529–533, 2015.

[7] K. Doya, “Reinforcement learning in continuous time and space,” Neural Computation, vol. 12,

pp. 219–245, 2000.

[8] G. J. Gordon, “Stable function approximation in dynamic programming,” in International

Conference on Machine Learning, 1995, pp. 261–268.

[9] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for model-based control,” in
IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012, pp. 5026–5033.

8https://github.com/openai/spinningup

26

[10] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba,

“OpenAI gym,” arXiv preprint arXiv:1606.01540, 2016.

[11] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Benchmarking deep rein-
forcement learning for continuous control,” in International Conference on Machine Learning,
2016, pp. 1329–1338.

[12] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. L. Casas, D. Budden, A. Abdol-
maleki, J. Merel, A. Lefrancq, and T. Lillicrap, “DeepMind control suite,” arXiv preprint
arXiv:1801.00690, 2018.

[13] M. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja, and C. Boutilier, “CAQL: Continuous

action Q-learning,” arXiv preprint arXiv:1909.12397, 2020.

[14] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, “Trust region policy optimiza-

tion,” in International Conference on Machine Learning, 2015, pp. 1889–1897.

[15] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimiza-

tion algorithms,” arXiv preprint arXiv:1707.06347, 2017.

[16] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wier-
stra, “Continuous control with deep reinforcement learning,” arXiv preprint arXiv:1509.02971,
2015.

[17] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and
K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in International
Conference on Machine Learning, 2016, pp. 1928–1937.

[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Oﬀ-policy maximum
entropy deep reinforcement learning with a stochastic actor,” in International Conference on
Machine Learning, 2018, pp. 1861–1870.

[19] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation error in actor-critic

methods,” in International Conference on Machine Learning, 2018, pp. 1587–1596.

[20] C. Tessler, G. Tennenholtz, and S. Mannor, “Distributional policy optimization: An alternative
approach for continuous control,” in Advances in Neural Information Processing Systems, 2019,
pp. 1350–1360.

[21] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep Q-learning with model-based

acceleration,” in International Conference on Machine Learning, 2016, pp. 2829–2838.

[22] M. Palanisamy, H. Modares, F. L. Lewis, and M. Aurangzeb, “Continuous-time Q-learning for
inﬁnite-horizon discounted cost linear quadratic regulator problems,” IEEE Transactions on
Cybernetics, vol. 45, pp. 165–176, 2015.

[23] T. Bian and Z.-P. Jiang, “Value iteration and adaptive dynamic programming for data-driven

adaptive optimal control design,” Automatica, vol. 71, pp. 348–360, 2016.

[24] K. G. Vamvoudakis, “Q-learning for continuous-time linear systems: A model-free inﬁnite
horizon optimal control approach,” Systems & Control Letters, vol. 100, pp. 14–20, 2017.

[25] Y. Jiang and Z.-P. Jiang, “Global adaptive dynamic programming for continuous-time nonlin-

ear systems,” IEEE Transactions on Automatic Control, vol. 60, pp. 2917–2929, 2015.

27

[26] J. Kim and I. Yang, “Hamilton–Jacobi–Bellman equations for maximum entropy optimal con-

trol,” arXiv preprint arXiv:2009.13097, 2020.

[27] S. Bhasin, R. Kamalapurkar, M. Johnson, K. G. Vamvoudakis, F. L. Lewis, and W. E. Dixon,
“A novel actor-critic-identiﬁer architecture for approximate optimal control of uncertain non-
linear systems,” Automatica, vol. 49, pp. 82–92, 2013.

[28] H. Modares and F. L. Lewis, “Optimal tracking control of nonlinear partially-unknown
constrained-input systems using integral reinforcement learning,” Automatica, vol. 50, pp.
1780–1792, 2014.

[29] K. G. Vamvoudakis and F. Lewis, “Online actor-critic algorithm to solve the continuous-time

inﬁnite horizon optimal control problem,” Automatica, vol. 46, pp. 878–888, 2010.

[30] S. J. Bradtke and M. O. Duﬀ, “Reinforcement learning methods for continuous-time Markov
decision problems,” in Advances in Neural Information Processing Systems, 1995, pp. 393–400.

[31] R. Munos, “Policy gradient in continuous time,” Journal of Machine Learning Research, vol. 7,

pp. 771–791, 2006.

[32] ——, “A study of reinforcement learning in the continuous case by the means of viscosity

solutions,” Machine Learning, vol. 40, pp. 265–299, 2000.

[33] R. Munos and A. W. Moore, “Barycentric interpolators for continuous space and time reinforce-
ment learning,” in Advances in Neural Information Processing Systems, 1999, pp. 1024–1030.

[34] P. Dayan and S. P. Singh, “Improving policies without measuring merits,” in Advances in

Neural Information Processing Systems, 1996, pp. 1059–1065.

[35] M. Ohnishi, M. Yukawa, M. Johansson, and M. Sugiyama, “Continuous-time value function
approximation in reproducing kernel Hilbert spaces,” in Advances in Neural Information Pro-
cessing Systems, 2018, pp. 2813–2824.

[36] Y. Yang, D. Wunsch, and Y. Yin, “Hamiltonian-driven adaptive dynamic programming for
continuous nonlinear dynamical systems,” IEEE Transactions on Neural Networks and Learn-
ing Systems, vol. 28, pp. 1929–1940, 2017.

[37] E. Theodorou, J. Buchli, and S. Schaal, “A generalized path integral control approach to
reinforcement learning,” Journal of Machine Learning Research, vol. 11, pp. 3137–3181, 2010.

[38] K. Rajagopal, S. N. Balakrishnan, and J. R. Busemeyer, “Neural network-based solutions for
stochastic optimal control using path integrals,” IEEE Transactions on Neural Networks and
Learning Systems, vol. 28, pp. 534–545, 2017.

[39] Y. Tassa and T. Erez, “Least squares solutions of the HJB equation with neural network
value-function approximators,” IEEE Transactions on Neural Networks, vol. 18, pp. 1031–
1041, 2007.

[40] M. Lutter, B. Belousov, K. Listmann, D. Clever, and J. Peters, “HJB optimal feedback control
with deep diﬀerential value functions and action constraints,” in Conference on Robot Learning,
2020, pp. 640–650.

28

[41] G. P. Kontoudis and K. G. Vamvoudakis, “Kinodynamic motion planning with continuous-
time Q-learning: An online, model-free, and safe navigation framework,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 30, pp. 3803–3817, 2019.

[42] P. Mehta and S. Meyn, “Q-learning and pontryagin’s minimum principle,” in IEEE Conference

on Decision and Control, 2009, pp. 3598–3605.

[43] C. Tallec, L. Blier, and Y. Ollivier, “Making deep Q-learning methods robust to time dis-

cretization,” in International Conference on Machine Learning, 2019, pp. 6096–6104.

[44] L. C. Baird, “Reinforcement learning in continuous time: advantage updating,” in IEEE In-

ternational Conference on Neural Networks, 1994, pp. 2448–2453.

[45] J. Kim and I. Yang, “Hamilton–Jacobi–Bellman for Q-learning in continuous time,” in Learning

for Dynamics and Control (L4DC), 2020, pp. 739–748.

[46] M. Bardi and I. Capuzzo-Dolcetta, Optimal Control and Viscosity Solutions of Hamilton–

Jacobi–Bellman Equations. Boston, MA: Birkh¨auser, 1997.

[47] M. Crandall and P.-L. Lions, “Viscosity solutions of Hamilton–Jacobi equations,” Transactions

of the American Mathematical Society, vol. 277, pp. 1–42, 1983.

[48] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double Q-learning,”

in Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016, pp. 2094–2100.

[49] N. Kohl and P. Stone, “Policy gradient reinforcement learning for fast quadrupedal locomo-

tion,” in IEEE International Conference on Robotics and Automation, 2004, pp. 2619–2624.

[50] S. Levine and V. Koltun, “Guided policy search,” in International Conference on Machine

Learning, 2013, pp. 1–9.

[51] M. Fazel, R. Ge, S. M. Kakade, and M. Mesbahi, “Global convergence of policy gradient

methods for the linear quadratic regulator,” arXiv preprint arXiv:1801.05039, 2018.

[52] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy

gradient algorithms,” in International Conference on Machine Learning, 2014, pp. 387–395.

[53] D. Quillen, E. Jang, O. Nachum, C. Finn, J. Ibarz, and S. Levine, “Deep reinforcement learning
for vision-based robotic grasping: A simulated comparative evaluation of oﬀ-policy methods,”
in IEEE International Conference on Robotics and Automation, 2018, pp. 6284–6291.

[54] M. Abu-Khalaf and F. L. Lewis, “Nearly optimal control laws for nonlinear systems with
saturating actuators using a neural network HJB approach,” Automatica, vol. 41, pp. 779–
791, 2005.

[55] H. Robbins and S. Monro, “A stochastic approximation method,” Annals of Mathematical

Statistics, vol. 22, pp. 400–407, 1951.

[56] H. Kushner and G. G. Yin, Stochastic Approximation and Recursive Algorithms and Applica-

tions. New York: Springer Science & Business Media, 2003.

[57] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International

Conference on Learning Representation, 2015.

