Robust Collective Classiﬁcation against Structural Attacks

0
2
0
2

l
u
J

6
2

]

G
L
.
s
c
[

1
v
3
7
0
3
1
.
7
0
0
2
:
v
i
X
r
a

Kai Zhou
Computer Science and Engineering
Washington University in St. Louis
Saint Louis, MO 63105
zhoukai@wustl

Yevgeniy Vorobeychik
Computer Science and Engineering
Washington University in St. Louis
Saint Louis, MO 63105
yvorobeychik@wustl.edu

Abstract

Collective learning methods exploit relations
among data points to enhance classiﬁcation
performance. However, such relations, rep-
resented as edges in the underlying graphical
model, expose an extra attack surface to the
adversaries. We study adversarial robustness
of an important class of such graphical mod-
els, Associative Markov Networks (AMN), to
structural attacks, where an attacker can mod-
ify the graph structure at test time. We for-
mulate the task of learning a robust AMN
classiﬁer as a bi-level program, where the in-
ner problem is a challenging non-linear inte-
ger program that computes optimal structural
changes to the AMN. To address this techni-
cal challenge, we ﬁrst relax the attacker prob-
lem, and then use duality to obtain a convex
quadratic upper bound for the robust AMN
problem. We then prove a bound on the qual-
ity of the resulting approximately optimal so-
lutions, and experimentally demonstrate the
efﬁcacy of our approach. Finally, we apply
our approach in a transductive learning setting,
and show that robust AMN is much more ro-
bust than state-of-the-art deep learning meth-
ods, while sacriﬁcing little in accuracy on non-
adversarial data.

1

INTRODUCTION

Data from various domains can be compactly represented
as graphs, such as social networks, citation networks,
and protein interaction networks. In such graphical mod-
els, nodes, associated with attributes, represent the enti-
ties and edges indicate their relations. A common task
is to classify nodes into different categories, e.g., clas-
sifying an account in a social network as malicious or

Proceedings of the 36th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), PMLR volume 124, 2020.

benign. Collective learning methods (Sen et al., 2008;
Taskar et al., 2002) exploit such relational structure in the
data for classiﬁcation tasks. For instance, in hypertext
classiﬁcation, the linked Web-pages tend to possess the
same label, as they often share similar contents. Graph-
ical models (Koller et al., 2009), such as Markov net-
works (Richardson and Domingos, 2006; Taskar et al.,
2004b,a; Kok and Domingos, 2005), take into account
such linking information in classifying relational data,
exhibiting considerable performance improvement on a
wide range of classiﬁcation tasks (Li, 1994; Taskar et al.,
2004c; Munoz et al., 2009).

However, making use of relational information in clas-
siﬁcation also exposes a new vulnerability in adversar-
ial settings. Consider the task of classifying nodes in a
social network as malicious or benign. If connectivity
is used for this task, malicious parties have an incen-
tive to manipulate information obtained about the net-
work structure to reduce classiﬁcation accuracy. For ex-
ample, malicious nodes may ostensibly cut ties among
one another, or add links to benign nodes, with the pur-
pose of remaining undetected. While such structural at-
tacks on collective learning have recently emerged (Dai
et al., 2018; Z¨ugner et al., 2018; Z¨ugner and G¨unnemann,
2019), they have focused primarily on defeating neural
network embedding-based approaches and transductive
learning scenarios.

Our goal is to learn robust Associative Markov Networks
(AMN) (Taskar et al., 2004a) that jointly classify the
nodes in a given graph, where edges indicate that the
connected nodes are likely to have the same label. We
formalize the problem of learning robust AMN as a bi-
level program in which the inner optimization problem
involves optimal structural attacks (adding or deleting
edges). The key technical challenge is that even the in-
ner problem is a non-linear integer program, rendering
the full optimization problem intractable. We address
this challenge by ﬁrst relaxing the inner adversarial op-
timization, which allows us to approximate the full bi-

 
 
 
 
 
 
level program by a convex quadratic upper bound that
we can now efﬁciently minimize. Our subsequent anal-
ysis ﬁrst exhibits an approximation bound for the adver-
sarial problem, and then an approximation bound on the
solutions of our approximate robust AMN approach.

We test our approach on real datasets in several different
settings.

First, we show that structural attacks can effectively de-
grade the accuracy of AMN, with relational information
now becoming an Achilles heel instead of an advantage.
In contrast, robust AMN degrades gracefully, preserv-
ing the advantages of the use of relational information
in classiﬁcation, even with a relatively large adversarial
modiﬁcation of graph structure.
In addition, we com-
pared robust AMN to a Graph Convolutional Network
(GCN) classiﬁer in a transductive learning setting under
a structural attack, and show that robust AMN is signif-
icantly more robust than GCN, and is nearly as accu-
rate as GCN on non-adversarial data. This observation is
particularly noteworthy given the fact that robust AMN
was not speciﬁcally designed to be robust to transduc-
tive learning attacks, which effectively pollute both the
training and test data.

Related Work Our work falls into the realm of learn-
ing robust classiﬁers against decision-time reliability at-
tacks (Vorobeychik and Kantarcioglu, 2018). There is
a rich collection of prior work on decision-time attacks
targeting a variety of classiﬁcation approaches, rang-
ing from classical models (Globerson and Roweis, 2006;
Torkamani and Lowd, 2013) to deep-learning methods
(Eykholt et al., 2018; Grosse et al., 2017). As their coun-
termeasures, several efforts (Li and Vorobeychik, 2014,
2018; Goodfellow et al., 2015; Madry et al., 2018) have
been devoted to enhancing the robustness of classiﬁers
in adversarial settings. A fundamental difference in our
work is that we are defending against structural attacks
that exploit the relations among data points for the pur-
pose of attacking, while most prior work on robust learn-
ing considers settings that treat data independently.

More closely related to our focus are several prior studies
of the vulnerability and robustness of collective learn-
ing models. Torkamani and Lowd (2013) also consid-
ered the robustness of AMN to prediction-time attacks,
but consider attacks that modify node features, leav-
ing robustness to structural attacks as an open prob-
lem. Recently there have been a number of demon-
strations of vulnerabilities of graph neural networks to
attacks in semi-supervised (transductive learning) set-
tings (Dai et al., 2018; Z¨ugner et al., 2018; Z¨ugner and
G¨unnemann, 2019). Speciﬁcally, Dai et al. (2018) and
Z¨ugner et al. (2018) proposed targeted attacks aiming
at misclassifying a subset of target nodes while Z¨ugner

and G¨unnemann (2019) focused on reliability attacks at
training time. While our focus is not on such trans-
ductive learning problems (in which attacks also poison
the training data), we explore the robustness of our ap-
proach in such settings in the experiments below. Other
than collective learning, there are a host of works study-
ing structural attacks as well as defense approaches in a
more general setting of network analysis tasks, such as
link prediction (Zhou et al., 2019b,a), community detec-
tion (Waniek et al., 2018) and so on.

2 BACKGROUND

A Markov network is deﬁned over an undirected graph
G = (V, E), where a node vi ∈ V , i = 1, 2, · · · , N ,
is associated with a node variable Yi, representing an
object to be classiﬁed. We assume there are K dis-
crete labels, i.e., Yi ∈ {1, 2, · · · , K}. At a high level,
a Markov network deﬁnes a joint distribution of Y =
{Y1, Y2, · · · , YN }, which is a normalized product of po-
tentials: Pφ(y) = 1
c∈C φc(yc), where φc(yc) is a
Z
potential function associated with each clique c ∈ C in
G, and Z is a normalization factor. The potential func-
tion φc(yc) maps a label assignment yc of the nodes in
the clique c to a non-negative value.

(cid:81)

Our focus is the pairwise Associative Markov Networks
(AMN) (Taskar et al., 2004a), where each clique is either
a node or a pair of nodes. Thus, the joint distribution of
Y can be written as

Pφ(y|G) =

1
Z

N
(cid:89)

i=1

(cid:89)

φi(yi)

(i,j)∈E

φij(yi, yj),

(1)

where we make explicit the dependency of the proba-
bility on the network structure G. In detail, let yk
i be a
binary indicator where yk
i = 1 means that node i is as-
signed label k. The node and edge potentials then join
such label assignments with the node and edge features,
respectively. Speciﬁcally, let xi ∈ Rdn and xij ∈ Rde
be the feature vectors of node i and edge (i, j). In the
log-linear model, the potential functions are deﬁned as
log φi(yk
· xij,
j ) = we
∈ Rde are node and edge pa-
where wk
rameters. Note that such parameters are label-speciﬁc in
that they are different with respect to labels k, k(cid:48), and the
same for the nodes and edges, respectively.

i ) = wk
n ∈ Rdn and wk,k(cid:48)

n · xi and log φij(yk

i , yk(cid:48)

k,k(cid:48)

e

In associative MN, the link (i, j) indicates an associative
relation between nodes i and j, meaning that i and j tend
to be classiﬁed as the same label. Reﬂected on the edge
potentials, it is assumed that φij(yk
j ) = 1 for any
different labels k, k(cid:48) ∈ {1, 2, · · · , K} while φij(yk
i , yk
j )
equals some value greater than 1. Consequently, the edge

i , yk(cid:48)

potentials associated with those edges connecting differ-
ently labeled nodes are 0 in the log space. For simplicity,
we write the edge parameters for label k as wk
e . Putting
everything together, the AMN deﬁnes the log of condi-
tional probability as

log Pw(y|x, G)
N
(cid:88)

K
(cid:88)

(wk

n · xi)yk

i +

=

(2)

(cid:88)

K
(cid:88)

(wk

e · xij)yk

i yk
j

i=1

k=1

(i,j)∈E

k=1

− log Zw(x),

where w and x represent all the node and edge param-
eters and features, and y represents a label assignment.
Importantly, the last term Zw(x) does not depend on the
assignment y.

Two essential tasks of AMN are inference and learn-
ing.
In inference, one seeks the optimal assign-
ment y that maximizes the log conditional probability
log Pw(y|x, G) (excluding the term Zw(x)), given ob-
served features x and learned parameters w. Taskar
et al. (2004a) showed that in AMN, such an inference
problem can be (approximately) solved in polynomial
time on arbitrary graph structures (when K = 2, the
solution is optimal). To learn the weights w, Taskar
et al. (2004b) proposed a maximum margin approach by
maximizing the gap between the conﬁdence in the true
labeling ˆy and any alternative labeling y, denoted by
∆Pw(ˆy, y|x, G) = log Pw(ˆy|x, G) − log Pw(y|x, G).
Speciﬁcally, they formulate the learning problem as fol-
lows:

min

s.t.

||w||2 + Cξ,

1
2
ξ ≥ max
y∈Y

(∆(ˆy, y) − ∆Pw(ˆy, y|x, G)).

(3)

By relaxing the integrality constraints and using strong
duality of linear programming, the inner maximization
problem is replaced with its dual minimization problem.
As a result, the weights w can be efﬁciently learned
through solving the quadratic program with linear con-
straints.

3 LEARNING ROBUST AMN

3.1 MODEL

In max-margin AMN learning, the exponential set of
constraints is replaced by a single most-violated con-
straint.
In the adversarial setting, the attacker is also
modifying the structure of G, potentially strengthening
the constraint in Eqn. (3). Our robust formulation ex-
tends the max-margin learning formulation, taking into
account the change caused by modiﬁcations of G.

We begin by considering an attacker who can delete ex-
isting edges from G, and subsequently show that our
model can be extended to the case where an attacker can
both add and delete edges. To formalize, we assign a
binary decision variable eij for each edge (i, j) ∈ E,
where eij = 0 means that the attacker decides to delete
that edge. Then e = (eij)(i,j)∈E is the decision vector of
the attacker. Let E = {e : eij ∈ {0, 1}; (cid:80)
(i,j)∈E eij ≥
|E| − D−} be the space of all the possible decision vec-
tors, where D− is a budget on the number of edges that
the attacker can delete. Then robust learning can be for-
mulated as

min

s.t.

||w||2 + Cξ,

1
2
ξ ≥ max

y∈Y,e∈E

(∆(ˆy, y) − ∆Pw(ˆy, y|x, G(e))),

(4)

where G(e) denotes the modiﬁed graph obtained by
deleting edges as indicated in e from G. Henceforth, we
omit the edge features xij and write wk
e to sim-
plify notation. From Eqn. (2), we can rewrite Eqn. (4) as

e xij as wk

min

1
2

||w||2 + Cξ,

s.t.

ξ ≥ max

y∈Y,e∈E

N
(cid:88)

K
(cid:88)

(wk

nxi)(yk

i − ˆyk
i )

i=1

k=1

(5a)

(5b)

(cid:88)

K
(cid:88)

+

(i,j)∈E

k=1

wk

e (yk

i yk

j − ˆyk

i ˆyk

j ) · eij

+ N −

N
(cid:88)

K
(cid:88)

i=1

k=1

ˆyk
i yk
i .

In this formulation, the attacker’s choice of which edges
to remove is captured by the inner maximization problem
over e inside Constraint (5b). We call this attack, where
edges can only be deleted, Struct-D.

A natural extension is to consider a strong attacker who
can simultaneously delete and add edges. We term such
an attack Struct-AD (AD for adding and deleting edges).
A critical difference between Struct-D and Struct-AD is
that the search space of which “non-edges” should be
added is signiﬁcantly larger than that of edges to be re-
moved, since graphs tend to be sparse. This dramatically
increases the complexity of solving Eqn. (5) (in particu-
lar, of enforcing the constraint associated with comput-
ing the optimal attack). To address this, we restrict the
attacker to only add edges between two data points that
have different labels, resulting in a reduced set of non-
edges denoted by ¯E. The intuition behind this restriction
is that adding edges between nodes with the same label
provides useful information for classiﬁcation, and is un-
likely to be a part of an optimal attack; rather, the attacker

would focus on adding edges between pairs of nodes
with different labels to increase classiﬁcation noise.

e ˆyk

i ˆyk

(i,j)∈ ¯E

For the Struct-AD attack, we use a binary decision vari-
able ¯eij for each non-edge (i, j) ∈ ¯E, where ¯eij = 1
means that the attacker chooses to add an edge between
i and j. As a by-product, each term wk
j ¯eij becomes
0. Then in the formulation of Struct-AD, we only need to
add terms (cid:80)
(cid:80)K
k=1 wk
j ¯eij in the attacker’s
objective and an extra linear constraint (cid:80)
(i,j)∈ ¯E ¯eij ≤
D+, where D+ is the constraint on the number of edges
that the attacker can add. We can further extend the for-
mulation to allow additional restrictions on the attacker,
such as limiting the change to node degrees; indeed, we
can accommodate the addition of any linear constraints
on the attack.

i yk

e yk

constant terms in y and e:

max
y,e

N
(cid:88)

K
(cid:88)

(wk

nxi − ˆyk

i )yk

i +

i=1

k=1

(cid:88)

K
(cid:88)

−

(i,j)∈E

k=1

wk

e ˆyk

i ˆyk

j eij

(cid:88)

K
(cid:88)

wk

e zk
ij

(i,j)∈E

k=1

(6)

s.t. ∀i,

K
(cid:88)

k=1

i = 1, yk
yk

i ≥ 0; ∀(i, j) ∈ E, eij ∈ [0, 1];

(cid:88)

|E| −

eij ≤ D−;

(i,j)∈E
∀(i, j) ∈ E, ∀k, zk

ij ≤ yk

i , zk

ij ≤ yk

j , zk

ij ≤ eij

The weights of the robust AMN are learned through solv-
ing the bi-level optimization problem above. However,
this is a challenging task: ﬁrst, even the inner maximiza-
tion problem (optimal structural attack) is a combinato-
rial optimization problem, and the bi-level nature of the
underlying formulation makes it all the more intractable.
Next, we present an efﬁcient approximate solution to ro-
bust AMN learning with provable guarantees both for
the attack subproblem, and to the overall robust learning
problem. We focus on formulation (5) to simplify expo-
sition, but all our results generalize to the setting with
Struct-AD attacks.

3.2 APPROXIMATE SOLUTION

Our solution is based on approximating the inner-layer
non-linear integer program by a linear program (LP). To
this end, we ﬁrst linearize the non-linear terms (prod-
uct of three binary variables) using standard techniques,
and then relax the integrality constraints. Finally, we use
LP duality to obtain a single convex quadratic program
for robust AMN learning, thereby minimizing an upper
bound on the original bi-level program in Eqn. (5). Sub-
sequently, we provide approximation guarantees for the
resulting solutions.

ij ≤ yk

i , zk
ij ≥ yk

To begin, we replace each non-linear term yk
i yk
j eij in
Eqn. (5) with a non-negative continuous variable zk
ij and
ij ≤ yk
add three linear constraints zk
j , and
i +yk
ij ≤ eij. We omit the constraint zk
zk
j +eij −2,
since we are maximizing the objective (in the inner opti-
mization problem) and the weights wk
e are non-negative;
consequently, the optimal solution takes the value zk
ij =
j , eij}, which is equivalent to yk
min{yk
j eij in the
binary case. We further relax the integrality constraints
on the binary variables y and e, resulting in a linear pro-
gram to approximate the attacker’s problem, omitting the

i , yk

i yk

By LP duality, we can replace the attacker’s maximiza-
tion problem using its dual minimization problem, which
is further integrated into Eqn. (5). Consequently, we
can approximate Eqn. (5) by a convex quadratic program
(QP), which is presented in the appendix. We can use the
same techniques to formulate a corresponding quadratic
program for Struct-AD.

When the LP relaxation deﬁned in Eqn. (6) produces in-
tegral solutions of y and e, the QP will produce opti-
mal weights.
In the case where the LP’s solutions are
fractional, we obtain an upper bound on the true objec-
tive function. Thus, the approximation quality of LP
determines the gap between the true and approximate
objective values for the defender and, consequently, the
gap between approximate and optimal solutions to robust
AMN. In Section 4 we bound this gap.

4 BOUND ANALYSIS

The key to analyzing the bound of the defender’s objec-
tive is to devise a well-approximated integral solution to
the attacker’s problem as deﬁned in Eqn. (5). On the one
hand, such an integral solution bridges the gap between
the optimal integral solution of the attacker’s problem
and the optimal fractional solution of its LP relaxation.
On the other hand, it generates effective structural attacks
to test our robust AMN model. We ﬁrst focus on approx-
imating structural attacks and then analyze how to trans-
fer the bound on the attack performance to the bound on
the defender’s objective.

4.1 APPROXIMATING STRUCTURAL

ATTACKS

Given ﬁxed weights of the AMN model, the attacker
solves the LP in (6) and determine which edges to delete.
Unfortunately, Eqn. (6) will produce fractional solutions,

meaning that the attacker needs to round the results to ob-
tain a feasible (but not necessarily optimal) attack. Klein-
berg and Tardos (2002) proposed a randomized rounding
scheme to assign labels to nodes in a class of classiﬁca-
tion problems with pairwise relationships. We follow the
idea and apply it to our case where we are simultaneously
assigning K labels to the nodes and two labels (delete or
not) to the edges.

Given the optimal solution (y∗, e∗, z∗) of LP (6), the ran-
domized rounding procedure (termed RRound) produces
a corresponding integral solution (yI , eI ). Speciﬁcally,
RRound rounds y∗ and e∗ in phases. In a single phase,
we independently draw a label k ∈ {1, 2, · · · , K} and a
label b ∈ {0, 1} (where b = 0 means the corresponding
edge is chosen to be deleted) at the beginning. We assign
this speciﬁc label k to each node and b to each edge in a
probabilistic way. Speciﬁcally, we generate a continuous
random number β uniformly from [0, 1]. For each node
i, if β ≤ yk∗
i = 1).
For each edge (i, j) ∈ E, if β ≤ eb∗
ij = e∗
ij
and e0∗
ij, we assign the label b to (i, j), i.e.,
eij = b. RRound stops when all nodes and edges are as-
signed labels. For the auxiliary variable zk
ij, it takes the
minimum of the rounded yk
j , and eij, which is equiv-
j · eij in the binary space. We thus omit zI
alent to yk
and specify the output of RRound as (yI , eI ).

, we assign the label k to i (i.e., set yk
ij , where e1∗

ij = 1 − e∗

i · yk

i , yk

i

(cid:80)

(cid:80)K

(cid:80)K

(i,j)∈E

(i,j)∈E

i )yk

simplify

the
(6)

objective

in Eqn.

we write

approximate

presentation,

k=1 wk
k=1 wk

k=1(wk
nxi − ˆyk
and A2(z; w)

at-
To
tacker’s
as
A(y, e, z; w) = A1(y, e; w) + A2(z; w), where
A1(y, e; w) = (cid:80)N
i −
i=1
(cid:80)K
(cid:80)
e ˆyk
i ˆyk
=
j eij
e zk
ij.

the attacker’s
true objective speciﬁed in (5) (inner problem) is de-
noted as AI (y, e) = AI
2(y, e; w),
where AI
k=1(wk
nxi −
1(y, e; w)
i − (cid:80)
ˆyk
i )yk
j eij and A2(z; w) =
(cid:80)K
(cid:80)
ij, with the constraint that y

k=1 wk
j ek
i yk
and e take binary values.

= (cid:80)N
i=1
i ˆyk
e ˆyk

1(y, e; w) + AI

(i,j)∈E
k=1 wk

Similarity,

e yk

(i,j)∈E

(cid:80)K

(cid:80)K

Our primary interest is then to analyze the gap be-
tween AI (yI , eI ; w) and A(y∗, e∗, z∗; w). The follow-
ing lemma is a direct extension of Lemma B.1 of (Taskar
et al., 2004a).
Lemma 4.1 ((Taskar et al., 2004a)). RRound assigns la-
bel k to node i with probability yk∗
i and assigns label 1
to edge (i, j) with probability e∗
ij.

Next, Lemma 4.2 gives a bound on the probability that
edge (i, j) is not deleted and node i and j are assigned
the same label.
Lemma 4.2. RRound assigns label k to node i and node
j and assigns label 1 to edge (i, j) simultaneously with

probability at least

1

K+4 zk∗
ij .

1

1

, yk∗

j , e1∗

j , e1∗

2K min{yk∗

2K zk∗
ij = min{yk∗
i

Proof. We consider the assignment in a single phase
when nodes i, j and edge (i, j) are not assigned any la-
bels at the beginning of this phase. Note that in every
phase, there are a total 2K combinations of the random
draws (k, b) with equal probabilities. Then the proba-
bility that i and j are assigned k and (i, j) is assigned
ij } = 1
1 is
ij , since the opti-
i
, yk∗
mal solution of LP satisﬁes zk∗
ij }.
Consider a speciﬁc draw (k, b), the probability that at
least one of i and j is assigned label k or edge (i, j)
2K max{yk∗
ij }. By sum-
is assigned a label is
ming over all combination of (k, b), we have the prob-
ability that none of i, j, and (i, j) is assigned any
2K max{yk∗
ij } = 1 −
label
1
j , e1∗
, yk∗
j , e0∗
ij }).
2K
Now consider all the phases. Nodes i and j and edge
(i, j) can be assigned label k and 1 simultaneously in a
single phase or in separate phases. For the purpose of de-
riving the lower-bound, we only consider the probability
that i and j are assigned k and (i, j) is assigned 1 in a
single phase. Summing over all phases, the probability
that i and j are assigned k and (i, j) is assigned 1 is at
least

is 1 − (cid:80)
(k,b)
k=1(max{yk∗
i

j , eb∗
ij } + max{yk∗
i

j , eb∗

, yk∗

, yk∗

, yk∗

(cid:80)K

1

i

i

K
(cid:88)

(max{yk∗

i

, yk∗

j , e0∗
ij }

j , e1∗

k=1
ij })]p−1
zk∗
ij
j , e0∗
ij } + max{yk∗
i
zk∗
ij
ij + yk∗
j + e0∗

i + yk∗

, yk∗

j + e1∗
ij )

, yk∗

j , e1∗
ij }

∞
(cid:88)

1
2K

zk∗
ij [1 −

1
2K

p=1
+ max{yk∗

i

, yk∗

=

≥

=

(cid:80)K

k=1(max{yk∗
i

i + yk∗

(cid:80)K

k=1(yk∗
zk∗
ij
K + 4

.

The ﬁrst equality comes from (cid:80)∞
1.

i=0 di = 1

1−d for d <

Finally, we can use both of these lemmas to prove a lower
bound on the expected value of AI (yI , eI ; w):
Theorem 4.3. Let E[AI (yI , eI ; w)] be the expected
Then E[AI (yI , eI ; w)] ≥
value of AI (yI , eI ; w).
A1(y∗, e∗; w) + 1

K+4 A2(z∗; w)

Proof. From Lemma 4.1 and Lemma 4.2, we
have E[AI
A1(y∗, e∗; w)
and
E[AI
K+4 A2(z∗; w), giving the
claimed result.

2(yI , eI ; w)] ≥

1(yI , eI ; w)]

=
1

Derandomization We propose a semi-derandomized
algorithm (termed Semi-RRound) to select the edges.
The key observation is that once y are determined, the
objective is a linear function with respect to e. Thus,
by selecting those edges in ascending order of their co-
efﬁcients, the objective is maximized. Let eI (cid:48)
be the
decision vector for the edges output by Semi-RRound.
Speciﬁcally, Semi-RRound takes an output (yI , eI ) of
RRound and compute the objective as a linear function
It then deletes edges (i, j) (sets eij = 0) in as-
of e.
cending order of the coefﬁcients of eij until D− edges
are deleted. Let eI (cid:48)
be the decision vector for the edges
output by Semi-RRound and AI (yI , eI (cid:48)
; w) be the corre-
sponding objective. The following Corollary shows that
the expected value of AI (yI , eI (cid:48)
; w) retains the lower
bound in Theorem 4.3.
Corollary 4.4. E[AI (yI , eI (cid:48)
K+4 A2(z∗; w).

; w)] ≥ A1(y∗, e∗; w) +

1

that maximizes AI (yI , e; w).

Proof. For each (yI , eI ), Semi-RRound ﬁnds
optimal eI (cid:48)
AI (yI , eI (cid:48)
(yI , eI ).
output
tion, we have E[AI (yI , eI (cid:48)
K+4 A2(z∗; w).

the
Thus,
; w) ≥ AI (yI , eI ; w) for every rounded
By the deﬁnition of expecta-
; w)] ≥ A1(y∗, e∗; w) +

1

ˆFD(w(cid:5)) ≤ ˆFD(w∗). Then

ˆFD(w(cid:5)) − FD(w∗)
≤ ˆFD(w∗) − FD(w∗)
= C · ˆFA(w∗) − C · FA(w∗)
= C(A(y∗, e∗, z∗; w∗) − AI (yI∗, eI∗; w∗))

≤ C

K + 3
K + 4

A2(z∗; w∗).

(8)

(9)

The second inequality is from the result of Corollary 4.4
and the fact that (yI∗, eI∗) is the integral optimal so-
lution. Then A2(z∗; w∗) = (cid:80)
e z∗k
k=1 w∗k
ij .
ij ≤ y∗k
As z∗k
k=1 y∗k
= 1 in the op-
i
i
fractional solution, we have A2(z∗; w∗) ≤
timal
(cid:80)
Thus ˆFD(w(cid:5)) −

and (cid:80)K

= |E|(cid:15).

(i,j)∈E

(cid:80)K

(i,j)∈E (cid:15) (cid:80)K

k=1 y∗k
i
FD(w∗) ≤ (K+3)C|E|

K+4

(cid:15).

We note that the bound analysis can be extended to the
case where the attacker can delete and add links simul-
taneously. One difference is that instead of using the in-
equality z∗
, we can use ¯z∗k
ij is
the auxiliary variable to represent the product yk
j ¯eij.
This leads to a bound (K+3)C(|E|+K·D+)
(cid:15), where D+ is
the budget on the number of added edges.

ij, where ¯zk
i yk

ij ≤ y∗k
i

ij ≤ ¯e∗

K+4

4.2 BOUNDING THE DEFENDER’S

OBJECTIVE

5 EXPERIMENTS

We rewrite the defender’s objective in Eqn. (5) as

min FD(w) = D(w) + C · max

y∈Y,e∈E

AI (y, e; w), (7)

i=1

(cid:80)K

nxi ˆyk

k=1 wk

2 ||w||2+C(N −(cid:80)N
where D(w) = 1
i ).
Note that the inner maximization problem implicitly de-
ﬁnes a function of w, which we denote by FA(w) =
AI (yI∗, eI∗; w). In solving Eqn. (7), we are approxi-
mating AI (y, e; w) using its LP relaxation; that is, the
approximated inner-layer maximization deﬁnes a new
function ˆFA(w) = A(y∗, e∗, z∗; w). Thus, instead
of directly minimizing the actual objective FD(w) =
D(w) + CFA(w), the defender is minimizing an ap-
proximated (upper-bound) objective ˆFD(w) = D(w) +
C ˆFA(w).
Let w∗ = arg min FD(w) and w(cid:5) = arg min ˆFD(w).
The following theorem bounds the difference between
ˆFD(w(cid:5)) and FD(w∗).
Theorem 4.5. Let (cid:15) = max{w∗1
Then, ˆFD(w(cid:5)) − FD(w∗) ≤ (K+3)C|E|

e , · · · , w∗K
e , w∗2
(cid:15).

e }.

K+4

Proof. Since w(cid:5) minimizes

ˆFD(w), we

have

We test the performance of the robust AMN classiﬁer
(henceforth R-AMN) under structural attacks as well as
a recently proposed attack on Graph Convolutional Net-
works (GCN). We focus on the binary classiﬁcation task
as AMN will in this case learn the optimal weights.

5.1 DATASETS

We consider four real-world datasets: Reuters, WebKB,
Cora and CiteSeer. For the Reuters dataset (Taskar et al.,
2004a; Torkamani and Lowd, 2013), we follow the pro-
cedure in (Taskar et al., 2004a) to extract four categories
(“trade”, “crude”, “grain”, and “money-fx”) of docu-
ments where each document belongs to a unique cate-
gory. We create a binary dataset by ﬁxing “trade” as the
positive class and randomly drawing an equal number of
documents from the other three categories. We extract
the bag-of-words representation for each document. To
construct the links, we connect each document to its three
closest neighbors in terms of the cosine distance of TF-
IDF representations, resulting in a graph of 862 nodes
and 1860 edges. The WebKB dataset (Craven et al.,
1998) contains webpages (represented as binary vectors
of dimension 1703) from four universities, which are

(a) Reuters-H

(b) Reuters-L

(a) Reuters-H

(b) Reuters-L

(c) Cora-H

(d) Cora-L

(c) Cora-H

(d) Cora-L

(e) WebKB-H

(f) WebKB-L

(e) WebKB-H

(f) WebKB-L

(g) CiteSeer-H

(h) CiteSeer-L

(g) CiteSeer-H

(h) CiteSeer-L

Figure 1: Accuracies of AMN (dotted lines) and R-AMN
(solid lines) under edge deletion attacks.

Figure 2: Accuracies of AMN (dotted lines) and R-AMN
(solid lines) under edge addition and deletion attacks

classiﬁed into ﬁve classes. We ﬁx “student” as the posi-
tive class and the rest as the negative class. We connect
each webpage to its 3 closest neighbors based on cosine
distance, resulting in a network of 877 nodes and 2282
edges. Cora (McCallum et al., 2000) and CiteSeer (Giles
et al., 1998) are two citation networks, where the nodes
in the graph are the bag-of-words representations of pa-
pers and edges are the citation relations among them. For
Cora, we use the “Prob-methods” as the positive class
and randomly draw an equal number of papers from the
other categories to form the negative class, resulting in a
subgraph of 852 nodes and 838 edges. Similarly for Cite-
Seer, we ﬁx “agents” as the positive class, resulting in a
subgraph of 1192 nodes and 953 edges. Cora and Cite-
Seer have more sparse graph structures than the Reuters
and WebKB. Moreover, Reuters and WebKB tend to be
regular graphs while a few nodes in Cora and CiteSeer
have large degrees. We split each dataset evenly into
training and test data for our experiments.

5.2 ROBUSTNESS AGAINST STRUCTURAL

ATTACKS

The AMN model jointly uses node features and structural
information to do classiﬁcation. The impact of structural
attacks depends on the importance of structure in classi-
ﬁcation, which in turn depends on the quality of infor-
mation captured by node features.

To study the impact of such node-speciﬁc information,
we consider two settings: high-discriminative (H-Dis),
which uses more features, and low-discriminative (L-
Dis), which uses a smaller subset of features. Specif-
ically, for the Reuters dataset, we select 200 top (in
terms of frequencies) features in the H-Dis case (termed
Reuters-H henceforth) and randomly selected 200 out of
the top 600 features in the L-Dis case (Reuters-L). For
WebKB, we randomly select 200 in L-Dis case (WebKB-
L) and 300 in H-Dis case (WebKB-H) out of the 1703
features. For Cora, we use all the 1433 features in H-
Dis case (Cora-H) and randomly select 600 features in

0.000.050.100.150.200.250.30Attack strength0.910.920.930.940.950.96AccuracyStruct-DRobust-DStruct-RSRobust-RSSVM0.000.050.100.150.200.250.30Attack strength0.860.880.900.920.940.96AccuracyStruct-DRobust-DStruct-RSRobust-RSSVM0.000.050.100.150.200.250.30Attack strength0.830.840.850.860.870.88AccuracyStruct-DRobust-DStruct-RSRobust-RSSVM0.000.050.100.150.200.250.30Attack strength0.780.790.800.810.820.830.840.850.86AccuracyStruct-DRobust-DStruct-RSRobust-RSSVM0.000.050.100.150.200.250.30Attack strength0.740.750.760.770.780.790.80AccuracyRobust-DStruct-DRobust-RSStruct-RSSVM0.000.050.100.150.200.250.30Attack strength0.700.720.740.760.78AccuracyRobust-DStruct-DRobust-RSStruct-RSSVM0.000.050.100.150.200.250.30Attack strength0.730.740.750.760.770.780.79AccuracyRobust-DStruct-DRobust-RSStruct-RSSVM0.000.050.100.150.200.250.30Attack strength0.680.700.720.740.76AccuracyRobust-DStruct-DRobust-RSStruct-RSSVM0.000.050.100.150.200.250.30Attack strength0.880.890.900.910.920.930.940.950.96AccuracyStruct-ADRobust-ADStruct-RSADRobust-RSADSVM0.000.050.100.150.200.250.30Attack strength0.840.860.880.900.920.940.96AccuracyStruct-ADRobust-ADStruct-RSADRobust-RSADSVM0.000.050.100.150.200.250.30Attack strength0.800.820.840.860.88AccuracyStruct-ADRobust-ADStruct-RSADRobust-RSADSVM0.000.050.100.150.200.250.30Attack strength0.760.780.800.820.840.86AccuracyStruct-ADRobust-ADStruct-RSADRobust-RSADSVM0.000.050.100.150.200.250.30Attack strength0.720.740.760.780.80AccuracyRobust-ADStruct-ADRobust-RSADStruct-RSADSVM0.000.050.100.150.200.250.30Attack strength0.680.700.720.740.760.78AccuracyRobust-ADStruct-ADRobust-RSADStruct-RSADSVM0.000.050.100.150.200.250.30Attack strength0.730.740.750.760.770.780.79AccuracyRobust-ADStruct-ADRobust-RSADStruct-RSADSVM0.000.050.100.150.200.250.30Attack strength0.700.720.740.76AccuracyRobust-ADStruct-ADRobust-RSADStruct-RSADSVMthe L-Dis case (Cora-L). For CiteSeer, we randomly se-
lect 500 in L-Dis case (CiteSeer-L) and 1000 in H-Dis
case (CiteSeer-H) out of the 3703 features. In our exper-
iments, we use cross-validation on the training set to tune
the two parameters of the R-AMN: the trade-off param-
eter C and the adversarial budget b. We note that when
tuning R-AMN, the defender has no knowledge of the
strength of the attacker. We use a simulated attacker that
can modify the validation set (in cross-validation) with
b = 0.1, meaning that the attacker can change 10% of
the edges.

We consider a baseline attacker that can randomly add
links between nodes belonging to different classes and
delete links between nodes with the same labels. We
term such an attack Struct-RSAD (Remove Same and
Add Different). In the case where the attacker is only
allowed to delete links, we term the attack as Struct-RS.
We test R-AMN under four structural attacks: Struct-D
and Struct-AD (our attacks, deleting links in the former,
and adding or deleting in the latter), and Struct-RS and
Struct-RSAD (heuristic baseline attacks above). We de-
note the performance of R-AMN exposed to these attacks
as Robust-D, Robust-AD, Robust-RS, Robust-RSAD, re-
spectively. We overload the notations by denoting the
performance of AMN under the four attacks as Struct-D,
Struct-AD, Struct-RS, Struct-RSAD, respectively.

Fig. 1 and Fig. 2 show the average accuracy (over 20
independent data splits) of AMN (dotted lines) and R-
AMN (solid lines) under structural attacks as well as the
accuracy of a linear SVM classiﬁer. First, by modifying
a small portion of the links in the graph, the accuracy
of AMN drops below that of SVM (which does not ex-
ploit relations), meaning that relations among data points
indeed introduce extra vulnerabilities. Moreover, struc-
tural attacks tend to be more severe when the node fea-
tures are less discriminative (the L-Dis case), where link-
ing information plays a relatively more important role in
classiﬁcation. Notably, the accuracy of R-AMN drops
signiﬁcantly slower than that of AMN under structural
attacks in all settings and stays above that of the SVM,
even when a relatively large fraction of the node connec-
tions are modiﬁed. These show that robust AMN pre-
serves the beneﬁts of using structural information even if
network structure is maliciously modiﬁed.

5.3 ROBUSTNESS AGAINST DEEP LEARNING

BASED ATTACKS

Having observed that our approach for robust AMN is
indeed robust against our newly designed attacks on
AMN, it is natural to wonder whether robust AMN re-
mains robust against recent attacks on graph convolu-
tional networks (GCNs). The answer is non-obvious:

(a) Reuters-H

(b) Reuters-L

(c) WebKB-H

(d) WebKB-L

Figure 3: R-AMN and R-GCN under deep-attack; GCN
under deep-attack and Struct-AD attack.

(a) Reuters-H

(b) Reuters-L

(c) WebKB-H

(d) WebKB-L

Figure 4: R-AMN and GCN on non-adversary data as
graphs are puriﬁed, e.g. R-AMN(0.5) stands for R-AMN
when noisy edges are deleted with probability 0.5.

on the one hand, attacks on GCN may not transfer to
AMN—although there is ample prior evidence that at-
tacks do often transfer from one learning approach to
another (Vorobeychik and Kantarcioglu, 2018); on the
other hand, attacks on GCN target transductive learn-
ing, and as such also poison the data on which the AMN
would be trained. To this end, we use a recent structural
attack on GCN (termed deep-attack) proposed in (Z¨ugner
and G¨unnemann, 2019), which has demonstrated impres-
sive attack performance and transferability to neural net-
work embedding-based approaches. We thus test our R-
AMN against this deep-attack, compared to GCN. In ad-
dition, we compare the performance of R-AMN with a
GCN based classiﬁer (Kipf and Welling, 2016) on non-
adversarial data.

0.000.050.100.150.200.250.30Attack strength0.40.50.60.70.80.9AccuracyGCN/deep-attackGCN/Struct-ADR-AMN/deep-attackR-GCN/deep-attack0.000.050.100.150.200.250.30Attack strength0.30.40.50.60.70.80.9AccuracyGCN/deep-attackGCN/Struct-ADR-AMN/deep-attackR-GCN/deep-attack0.000.050.100.150.200.250.30Attack strength0.500.550.600.650.700.750.80AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attack0.000.050.100.150.200.250.30Attack strength0.500.550.600.650.700.75AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attackGCN/0R-AMN/0GCN/.5R-AMN/.5GCN/.8R-AMN/.80.900.920.940.960.98AccuracyGCN/0R-AMN/0GCN/.5R-AMN/.5GCN/.8R-AMN/.80.880.900.920.940.960.98AccuracyGCN(0)R-AMN(0)GCN(0.5)R-AMN(0.5)GCN(0.8)R-AMN(0.8)0.50.60.70.80.91.0AccuracyGCN(0)R-AMN(0)GCN(0.5)R-AMN(0.5)GCN(0.8)R-AMN(0.8)0.50.60.70.80.91.0AccuracyGCN classiﬁes nodes in a transductive setting, where
labeled and unlabeled nodes reside in the same graph;
while R-AMN is trained over a training graph, and
makes predictions over an unseen test graph.
To
adapt R-AMN to the transductive setting, we use deep-
attack (with the same conﬁgurations as in (Z¨ugner and
G¨unnemann, 2019), e.g., strongest “Meta-Self” mode
and 0.1/0.1/0.8 train/validation/test split) to modify the
training graph and test graph separately (these can be
subsets of the same larger graph, as is the case in trans-
ductive learning). Then we train R-AMN on the attacked
training graph and test it on the attacked test graph.
We also test the performance of GCN under deep-attack
and our proposed structural attack Struct-AD on the test
graph. In addition, we test a robust GCN model (termed
R-GCN) under deep-attack, which is based on adversar-
ial training approach proposed in (Xu et al., 2019) on
the test graph. The accuracies of R-AMN, GCN, and
R-GCN under these attacks are presented in Fig. 5 for
the Reuters and WebKB datasets (the appendix presents
similar results for the Cora and CiteSeer datasets). The
main observation is that where R-GCN cannot defend
against such deep-attack in a transductive setting, our
proposed R-AMN is essentially invariant under deep-
attack, in contrast to GCN, which is highly vulnerable
to this attack, and also quite vulnerable to our proposed
structural attacks aimed at AMN.

Finally, we compare R-AMN and GCN on non-
adversarial data, which are evenly split into training and
test graphs. We note that R-AMN ignores the links be-
tween the training and test (sub-)graphs. The results
are shown in Fig. 4. To interpret these, consider ﬁrst
GCN/0 and R-AMN/0 bars, which correspond to the
direct performance comparison on the given data. We
can observe that the difference in accuracy between R-
AMN and GCN in these cases is either small (on WebKB
data) or, in fact, R-AMN actually outperforms GCN (on
Reuters data). It’s this latter observation that is surpris-
ing. The reason is that Reuters data contains ∼ 10%
of noisy links, that is, links connecting pairs of nodes
with different labels. This can be viewed as another
symptom of ragility of GCN, but in any case, we next
consider what happens when we remove each noisy link
with some probability (p = 0.5 or p = 0.8). The re-
sults are presented as bars with R-AMN/p and GCN/p,
where p corresponds to this probability of removed noisy
links, and, as expected, GCN performance improves as
we improve data quality. This improvement is signiﬁ-
cant when the graph is not particularly noisy (WebKB),
but the gap between R-AMN and GCN remains relatively
small when enough noisy links remain (Reuters, as well
as Cora and CiteSeer; see the appendix, Fig. 6). This fur-
ther attests to greater robustness of R-AMN, but does ex-

hibit some cost in terms of accuracy on non-adversarial
data, if this data is sufﬁciently high quality.

6 CONCLUSION

We study robustness of the associative Markov network
classiﬁer under test-time attacks on network structure,
where an attacker can delete and/or add links in the un-
derlying graph. We formulate the task of robust learning
as a bi-level program and propose an approximation al-
gorithm to efﬁciently solve it. Our experiments on real-
world datasets demonstrate that the performance of ro-
bust AMN degrades gracefully even under large adver-
sarial modiﬁcations of the graph structure, preserving the
advantages of using structural information in classifying
relational data. We additionally compare robust AMN
with the state-of-the-art deep learning based approaches
in the transductive setting and demonstrate that robust
AMN is signiﬁcantly more robust to structural perturba-
tions compared to deep graph embedding methods while
sacriﬁcing little performance on non-adversarial data,
except when network data is of extremely high quality
(a rarity in practice).

Acknowledgements

This work was partially supported by the National
Science Foundation (grants IIS-1905558 (CAREER)
and IIS-1903207) and Army Research Ofﬁce (grants
W911NF1810208 (MURI) and W911NF1910241).

References

M. Craven, A. McCallum, D. PiPasquo, T. Mitchell, and
D. Freitag. Learning to extract symbolic knowledge
from the world wide web. Technical report, Carnegie-
mellon univ pittsburgh pa school of computer Science,
1998.

H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu,
and L. Song. Adversarial attack on graph structured
data. In International Conference on Machine Learn-
ing, pages 1115–1124, 2018.

K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rah-
mati, C. Xiao, A. Prakash, T. Kohno, and D. Song.
Robust physical-world attacks on deep learning visual
classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
1625–1634, 2018.

C. L. Giles, K. D. Bollacker, and S. Lawrence. Citeseer:
An automatic citation indexing system. In ACM DL,
pages 89–98, 1998.

A. Globerson and S. Roweis. Nightmare at test time: ro-
In Proceedings of

bust learning by feature deletion.

the 23rd international conference on Machine learn-
ing, pages 353–360. ACM, 2006.

I. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations, 2015.

K. Grosse, N. Papernot, P. Manoharan, M. Backes, and
P. McDaniel. Adversarial examples for malware de-
tection. In European Symposium on Research in Com-
puter Security, pages 62–79. Springer, 2017.

T. N. Kipf and M. Welling. Semi-supervised classiﬁca-
In Interna-
tion with graph convolutional networks.
tional Conference on Learning Representations, 2016.

J. Kleinberg and E. Tardos. Approximation algorithms
for classiﬁcation problems with pairwise relation-
ships: Metric labeling and markov random ﬁelds.
Journal of the ACM (JACM), 49(5):616–639, 2002.

S. Kok and P. Domingos. Learning the structure of
In Proceedings of the 22nd
markov logic networks.
international conference on Machine learning, pages
441–448. ACM, 2005.

D. Koller, N. Friedman, and F. Bach. Probabilistic
graphical models: principles and techniques. MIT
press, 2009.

B. Li and Y. Vorobeychik. Feature cross-substitution in
adversarial classiﬁcation. In Advances in neural infor-
mation processing systems, pages 2087–2095, 2014.

B. Li and Y. Vorobeychik. Evasion-robust classiﬁcation
on binary domains. ACM Transactions on Knowledge
Discovery from Data (TKDD), 12(4):50, 2018.

S. Z. Li. Markov ﬁeld models in computer vision.

In
European conference on computer vision, pages 361–
370. Springer, 1994.

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to
In International Conference on
adversarial attacks.
Learning Representations, 2018.

A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore.
Automating the construction of internet portals with
Information Retrieval, 3(2):127–
machine learning.
163, 2000.

D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert.
Contextual classiﬁcation with functional max-margin
markov networks. In 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 975–982.
IEEE, 2009.

M. Richardson and P. Domingos. Markov logic net-
works. Machine learning, 62(1-2):107–136, 2006.

P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher,
and T. Eliassi-Rad. Collective classiﬁcation in net-
work data. AI magazine, 29(3):93–93, 2008.

B. Taskar, P. Abbeel, and D. Koller. Discriminative prob-
abilistic models for relational data. In Proceedings of
the Eighteenth conference on Uncertainty in artiﬁcial
intelligence, pages 485–492. Morgan Kaufmann Pub-
lishers Inc., 2002.

B. Taskar, V. Chatalbashev, and D. Koller.

Learn-
In Proceedings of
ing associative markov networks.
the twenty-ﬁrst international conference on Machine
learning, page 102. ACM, 2004a.

B. Taskar, C. Guestrin, and D. Koller. Max-margin
markov networks. In Advances in neural information
processing systems, pages 25–32, 2004b.

B. Taskar, M.-F. Wong, P. Abbeel, and D. Koller. Link
prediction in relational data. In Advances in neural in-
formation processing systems, pages 659–666, 2004c.

M. Torkamani and D. Lowd. Convex adversarial col-
lective classiﬁcation. In International Conference on
Machine Learning, pages 642–650, 2013.

Y. Vorobeychik and M. Kantarcioglu. Adversarial ma-
chine learning. Synthesis Lectures on Artiﬁcial Intelli-
gence and Machine Learning, 12(3):1–169, 2018.

M. Waniek, T. P. Michalak, M. J. Wooldridge, and
T. Rahwan. Hiding individuals and communities in a
social network. Nature Human Behaviour, 2(2):139–
147, 2018.

K. Xu, H. Chen, S. Liu, P.-Y. Chen, T.-W. Weng,
M. Hong, and X. Lin. Topology attack and defense
for graph neural networks: An optimization perspec-
In International Joint Conference on Artiﬁcial
tive.
Intelligence, 2019.

K. Zhou, T. Michalak, and Y. Vorobeychik. Adversarial
robustness of similarity-based link prediction. In IEEE
International Conference on Data Mining, 2019a.

K. Zhou, T. P. Michalak, M. Waniek, T. Rahwan, and
Y. Vorobeychik. Attacking similarity-based link pre-
In Proceedings of the
diction in social networks.
18th International Conference on Autonomous Agents
and MultiAgent Systems, pages 305–313. International
Foundation for Autonomous Agents and Multiagent
Systems, 2019b.

D. Z¨ugner and S. G¨unnemann. Adversarial attacks
In In-
on graph neural networks via meta learning.
ternational Conference on Learning Representations
(ICLR), 2019.

D. Z¨ugner, A. Akbarnejad, and S. G¨unnemann. Adver-
sarial attacks on neural networks for graph data.
In
Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining,
pages 2847–2856. ACM, 2018.

Appendix

A Formulation of Convex Quadratic Program

We explicitly write out the convex quadratic program for learning robsut AMN, which is omitted in the main paper. By
LP duality, we can replace the attacker’s maximization problem using its dual minimization problem, which is further
integrated into Eqn. (5). Consequently, we can approximate Eqn. (5) by the following convex quadratic program:

min

1
2

||w||2 + C(N −

N
(cid:88)

K
(cid:88)

i=1

k=1

s.t. ∀i, k,

ti −

(cid:88)

wk

nxi ˆyk

i +

N
(cid:88)

ti +

(cid:88)

pij − D− · tD)

i=1
nxi + ˆyk

i ≥ 0,

ij − wk
tk

(i,j)∈E

∀(i, j) ∈ E, k,

∀(i, j) ∈ E,

(i,j),(j,i)∈E
ij + tk
sk
K
(cid:88)

ij + tk

ji − wk

ij, tk

ji ≥ 0,

ij, tk
e ≥ 0, sk
K
(cid:88)

(cid:88)

pij −

sk
ij − tD +

wk

e ˆyk

i ˆyk

j ≥ 0, pij, tD ≥ 0.

(10)

The minimization is over the weights w and the dual variables ti, pij, sk

ij, tk

ij, tk

ji, tD.

k=1

(i,j)∈E

k=1

B Additional Experiment Results

We compare R-AMN and GCN under the deep-attack as well as on non-adversarial data on the Cora and CiteSeer
datasets in the same experiment settings as in the main paper. Speciﬁcally, in Fig. 5, ”R-AMN/deep-attack” shows the
accuracies of R-AMN under deep-attack with various degrees of graph perturbations, where the train graph and test
graph are attacked by deep-attack separately. It demonstrates that R-AMN is robust to deep-attack even with relatively
large structural perturbations. ”GCN/deep-attack” and ”GCN/Struct-AD” show the accuracies of GCN under deep-
attack and our proposed Struct-AD attack, respectively. Generally, deep-attack is a much more effective method to
attack GCN models. Fig. 6 demonstrated that on non-adversarial data, the performances of R-AMN and GCN are
comparable.

(a) Cora-H

(b) Cora-L

(c) CiteSeer-H

(d) CiteSeer-L

Figure 5: R-AMN and R-GCN under deep attack; GCN under deep attack and Struct-AD attack.

(a) Cora-H

(b) Cora-L

(c) CiteSeer-H

(d) CiteSeer-L

Figure 6: R-AMN and GCN on non-adversary data as graphs are puriﬁed, e.g. R-AMN(0.5) stands for R-AMN when
noisy edges are deleted with probability 0.5.

0.050.100.150.200.250.30Attack strength0.650.700.750.800.85AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attack0.050.100.150.200.250.30Attack strength0.650.700.750.800.85AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attack0.050.100.150.200.250.30Attack strength0.6000.6250.6500.6750.7000.7250.7500.775AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attack0.050.100.150.200.250.30Attack strength0.550.600.650.700.75AccuracyGCN/deep-attackR-AMN/deep-attackGCN/Struct-ADR-GCN/deep-attackGCN/0R-AMN/0GCN/.5R-AMN/.5GCN/.8R-AMN/.80.7750.8000.8250.8500.8750.9000.9250.950AccuracyGCN/0R-AMN/0GCN/.5R-AMN/.5GCN/.8R-AMN/.80.700.750.800.850.900.95AccuracyGCN(0)R-AMN(0)GCN(0.5)R-AMN(0.5)GCN(0.8)R-AMN(0.8)0.50.60.70.80.91.0AccuracyGCN(0)R-AMN(0)GCN(0.5)R-AMN(0.5)GCN(0.8)R-AMN(0.8)0.50.60.70.80.91.0Accuracy