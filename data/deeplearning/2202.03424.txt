2
2
0
2

b
e
F
5

]

G
L
.
s
c
[

1
v
4
2
4
3
0
.
2
0
2
2
:
v
i
X
r
a

Reinforcement learning for multi-item retrieval in the puzzle-based storage system

†Jing Hea,b, †Xinglu Liuc, Qiyao Duanc, Wai Kin Victor Chanc, Mingyao Qib,∗

aDepartment of Industrial Engineering, Tsinghua University, Beijing, 100084, China
bResearch Center for Modern Logistics, Shenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China
cIntelligent Transportation and Logistics Systems Laboratory, Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen
518055, China

Abstract

Nowadays, fast delivery services have created the need for high-density warehouses. The puzzle-based

storage system is a practical way to enhance the storage density, however, facing diﬃculties in the retrieval

process.

In this work, a deep reinforcement learning algorithm, speciﬁcally the Double&Dueling Deep Q

Network, is developed to solve the multi-item retrieval problem in the system with general settings, where

multiple desired items, escorts, and I/O points are placed randomly. Additionally, we propose a general compact

integer programming model to evaluate the solution quality. Extensive numerical experiments demonstrate that

the reinforcement learning approach can yield high-quality solutions and outperforms three related state-of-the-

art heuristic algorithms. Furthermore, a conversion algorithm and a decomposition framework are proposed to

handle simultaneous movement and large-scale instances respectively, thus improving the applicability of the

PBS system.

Keywords: Machine learning; Puzzle-based storage system; Deep reinforcement learning; Multi-item

retrieval; Integer Programming

1. Introduction

Fast delivery services (e.g., same-day service, next-day service provided by FedEx, JD.com, Alibaba.com,

and SF Express, etc.) contribute to enhancing the competitiveness of e-commerce and logistics ﬁrms. These ser-

vices are achieved by ﬁrst forecasting demand using historical data and then transporting inventory in advance

to front warehouses near demand areas (especially large cities, where demand is typically more concentrated).

Based on such a strategy, it requires more storage space or a more compact storage policy to store enough

inventory for near-large-city front warehouses. However, it may not be economical to expand warehouse space

as land is extraordinarily scarce and costly in large cities. Therefore, developing more compact storage systems

seems to be a practical way to relieve the space scarcity challenge.

In recent years, two main streams of compact storage systems are aisle-based shuttle storage systems and

grid-based shuttle storage systems (Azadeh et al. (2019)). Gue (2006) showed that the storage density (the

†: These authors contributed equally to this work.
∗Corresponding author
Email address: qimy@sz.tsinghua.edu.cn (Mingyao Qi)

Preprint submitted to European Journal of Operational Research

February 9, 2022

 
 
 
 
 
 
storage area occupied by pallets or shelves over the total area of the warehouse) in the aisle-based shuttle storage

systems cannot exceed 2k

(2k+1) (achieved with only one aisle), where k is the maximum shelves depth. Grid-based
shuttle storage systems, in which all aisles are eliminated, seem to be a better proposal to alleviate the pressure

of space scarcity. The dynamic version of grid-based shuttle storage systems (i.e., each item/SKU is stored

on a shuttle and is individually movable) is called puzzle-based storage (PBS) system. The PBS system was

originally proposed by Gue & Kim (2007) and has been implemented in warehouses and distribution centers,

automated car parking systems (Siddique et al. (2021)) and container terminals (Zaerpour et al. (2015)). This

system can be viewed as a grid with cells, each of which can be empty or occupied by an item, and thus its
storage density limit is (mn−1)

, where m and n denote the size of the grid. As shown in Figure 1, the density
3 (note there are three aisles) and 35

36 , respectively.

limits of these two systems are 2

mn

I/O

I/O#1

I/O#2

1
1

2

desired item

escort

non-desired item

(a) Aisle-based shuttle storage

(b) Puzzle-based storage

Figure 1: Example of aisle-based shuttle storage and puzzle-based storage

Although the PBS system achieves the highest space utilization, the retrieval process is signiﬁcantly com-

plicated. For aisle-based shuttle storage systems, retrieving items is easy as the desired items are moved directly

along the empty aisles to reach the I/O points. However, the aisle-less conﬁguration in the PBS system forces

the items to move only to adjacent empty cells (escorts), requiring both desired and non-desired items to move

corporately, thereby complicating the process. Retrieval paths with fewer movements can shorten response

time and save energy consumption, thus improving the applicability of the PBS system. Therefore, how to take

fewer movements to ﬁnish the tasks remains the key issue in the PBS system.

The existing literature mainly focuses on single-item retrieval (only one desired item is considered in each

retrieval task). Nonetheless, according to Mirzaei et al. (2017), several picking stations work simultaneously

in warehouses and it is common to execute multiple retrieval requests parallelly (so-called multi-item retrieval)

in real application environments. As demonstrated in Section 3.2, joint optimization of multi-item retrieval is

signiﬁcantly more eﬃcient than breaking it into several independent single-item retrieval tasks. Consequently,

to ﬁll this research gap, we concentrate on the multi-item retrieval problem in the general setting of the PBS

system (i.e., multiple desired items, multiple randomly placed escorts, multiple I/O points are considered).

Besides, unlike previous studies that use programming approaches (dynamic programming, integer program-

ming), or search-based heuristic approaches, we aim at developing a learning approach (speciﬁcally, reinforce-

ment learning) to solve the multi-item retrieval problem more eﬃciently. In the past few years, reinforcement

2

learning with a deep neural network has shown its eﬀectiveness in numerous complex tasks. The main idea of

reinforcement learning is that through a sequence of actions and rewards, the agent learns an action strategy

from their interactions with the environment. Compared to handcrafted heuristics, reinforcement learning can

generate high-quality solutions with less CPU time based on the trained models.

This paper makes the following contributions.

1) We propose a general compact integer programming (IP) formulation for the multi-item retrieval prob-

lems in PBS that dramatically reduces the number of decision variables and constraints when compared to the

literature.

2) We develop a reinforcement learning algorithm for multi-item retrieval problems in general settings

under the single-load movement (only one move is allowed at a timestamp) assumption. speciﬁcally, a semi-

random and semi-guided action selection mechanism is designed and integrated into the approach to address

the convergence issue, which remains a typical challenge for reinforcement learning. Extensive numerical

experiments demonstrate that the reinforcement learning solutions are quite close to optimum and outperform

those of the existing heuristics.

3) A conversion algorithm is proposed to consider simultaneous movement (multiple moves are allowed at

a timestamp), thereby signiﬁcantly reducing the retrieval time.

4) We propose a decomposition framework to handle large-scale instances in seconds, which performs

comparably to state-of-the-art algorithms while is capable of solving multi-item retrieval problems.

The rest of this paper is structured as follows. In Section 2, we summarize the related literature about the

PBS system and reinforcement learning. Section 3 describes the multi-item retrieval problem in detail and

then provides an illustrative example to explain that the multi-item retrieval is indeed necessary. In Sections

4 and 5, an IP formulation and a reinforcement learning method are presented, respectively. Sections 6 and 7

introduce the decomposition framework and the conversion algorithm. Section 8 discusses the generation of

benchmark instances and the experimental results. Section 9 summarizes this work and discusses its limitations

and potential future directions.

2. Related Work

2.1. Puzzle-based storage

Existing research on the PBS and retrieval system involves three major categories: system analysis, design

optimization, and operations planning and control. Interested readers can refer to Azadeh et al. (2019) for more

information about the PBS system and other types of compact storage systems. The system analysis-oriented

literature mainly concerns about the retrieval performance of the PBS system, including (i) evaluating the ex-

pected retrieval time and comparing it with that of traditional systems (Gue & Kim (2007), Zaerpour et al.

(2017a)), (ii) analyzing the eﬀect of escort location on retrieval time (Kota et al. (2015)). Design optimization-

related studies prefer to pay more attention to warehouse shape optimization under various storage policies

3

(Zaerpour et al. (2017a), Zaerpour et al. (2017b), Zaerpour et al. (2017c)) and emerging technologies on ad-

vanced systems as well as smarter equipments (Gue et al. (2013)). Operations planning and control problems

are dedicated to optimizing the retrieval path (Gue & Kim (2007), Rohit et al. (2010), Yu et al. (2016), Yalcin

et al. (2019), Alﬁeri et al. (2012), Gue et al. (2013), Mirzaei et al. (2017)), or AGV dispatching plan (Alﬁeri

et al. (2012)). Since that the third type of research is very close to this work and the other two are not, we

will not go into the detail of those categories. In this work, we assume that the resources (AGV, etc.) are

well-equipped and thus we focus on the optimization of the retrieval path.

Table 1: Item retrieval optimization literature in the PBS system

Reference

Topic

Methodology

Problem type

Desired items × Escorts × I/O Points × Move type × Objective

Gue & Kim (2007)

Single-item retrieval path

Closed-form formular,

S × S-F × S × lm × Tmax

optimization with ﬁxed escort

dynamic Programming,

locations (consider block move)

heuristic

S × S-F × S × bm × Tmax

S × M-F × S × bm × Tmax

Taylor & Gue (2008)

Evaluate eﬀect of escort locations

Discrete time simulation

S × M-R × S × lm × Tmax

Alﬁeri et al. (2012)

Optimize the shelf movements and

AGV dispatching

Heuristic

S × M-R × S × AGV × Tmax

Gue et al. (2013)

Kota et al. (2015)

Ma et al. (2021)

Deadlock free decentralized control
scheme, eﬀect of WIP and escorts

Heuristic

on the throughput rate

M × M-R × conveyor × GS& sm × S T

Multiple (arbitrary number) Desired items

Retrieval time estimation with

Closed-form formular

S × S-R × M(2)-R × S × lm × Tmax

randomly located escorts

Single-item retrieval with multiple

randomly located escorts

Heuristics

Heuristics

S × M-R × S × lm × Tmax

S × M-R × S × lm × Nm

Mirzaei et al. (2017)

Multi-item retrieval

Closed-form formular

M(2) × S-R × S × lm × Tmax

Yalcin et al. (2019)

Single-item retrieval

Single-item retrieval, eﬀect of si-

Heuristics

Optimal

Heuristics

M(3) × S-R × S × lm × Tmax

S × M-R × M × lm × Nm

Yu et al. (2016)

multaneous and block movement of

Integer programming

S × M-R × S × sm, bm × Tmax

items and escorts

Bukchin & Raviv (2020)

Single-item retrieval

Heuristic

S × M-R × M × lm × Tmax

S × M-R × M × bm × Tmax
S × M-R × M × sm, lm × Tmax + αNm
S × M-R × M × sm, bm × Tmax + αNm

Rohit et al. (2010)

Single-item retrieval

Integer programming

S × M-R × S × lm × Nm

Zou & Qi (2021)

Muti-item retrieval

Heuristic

M × M-R × M × lm × Nm

This work

Multi-item retrieval

Integer programming

Reinforcement Lerning

M × M-R × M × lm, sm × Nm

S: single; M: multiple, M(k) denotes that the number equals to k; S-R: single randomly placed escort; M-R: multiple randomly placed escorts; S-F:

single ﬁxed-position escort; M-F: multiple ﬁxed-position escorts; lm: single-load movement; bm: block movement; sm: simultaneous movement,
diﬀerent loads are allowed to move at a timestamp; Tmax: the completion time of the retrieval process; Nm: the total number of moves; S T : the

system throughput; AGV: refers to a system where automated guided vehicles (AGVs) are used to move the loads; GS: refers to a system based on

GridStore technology.

We identify the relevant work for item retrieval by the desired item number (single, multiple), escort number

(single or multiple, ﬁxed location or random location), I/O point number (single or multiple), load movement

4

type (single-load movement, simultaneous movement, and block movement) and objective function (minimize

the total completion time of the retrieval process, the total number of moves and the system throughput). The

relevant literature is summarized in Table 1.

The majority of the existing literature studies single-item retrieval problem. Optimizing the retrieval process

and evaluating the performance of the PBS system are two major concerns in this ﬁeld. Gue & Kim (2007)

aim to estimate the expected retrieval time under some special cases (e.g., considering a single, ﬁxed-position

escort, etc.). Besides, suﬃcient comparisons of retrieval performance between the PBS system and traditional

systems are provided. To explore the more general cases, several follow-up literature has considered multiple

randomly located escorts under the single-load movement assumption (Taylor & Gue (2008), Kota et al. (2015),

Yalcin et al. (2019), Rohit et al. (2010)), and multiple randomly located escorts under the simultaneous or

block movement assumptions (Yu et al. (2016), Bukchin & Raviv (2020)). Speciﬁcally, Kota et al. (2015)

extend the analysis results to estimate retrieval performance, observing that retrieval time depends heavily

on escorts’ locations. Similar investigation see Taylor & Gue (2008). Rohit et al. (2010) present a general

IP formulation for single-item retrieval with multiple arbitrarily located escorts.

It fails to solve medium-

or large-scale instances because of the dramatic increase in the number of variables and constraints. Yalcin

et al. (2019) focus more on algorithm design and proposes a search-based optimal approach and a heuristic

algorithm for small and large instances, respectively. Ma et al. (2021) develop an eﬃcient hybrid heuristic

algorithm that incorporates the state evaluation, neighborhood search, as well as beam search techniques to

further improve the solution quality and reduce the computational complexity. All the above studies make

the single-load motion assumption, which simpliﬁes the problem considerably but seems to be less practical,

leading to a waste of retrieve time. Therefore, it is essential to consider simultaneous and block movement.

Alﬁeri et al. (2012) assume that the retrieval actions are performed by a limited number of automated vehicles,

and develops a heuristic approach to optimize the AGVs’ scheduling and shelves’ movement. Bukchin & Raviv

(2020) develop an exact dynamic programming algorithm to solve the single-item retrieval problem that allows

simultaneous or block movement.

However, the above studies only consider single-item retrieval, which is still far from the real warehouse

operation. In practice, multiple picking stations work parallelly to pursue a shorter system-wide picking time,

typically in manufacturing environments and e-commerce warehouses. Therefore, multi-item retrieval opti-

mization tends to be essential and urgent. However, few research eﬀorts have attempted to cope with this

challenge so far. The only three closely relevant papers are Gue et al. (2013), Mirzaei et al. (2017) and Zou

& Qi (2021). Gue et al. (2013) discuss the retrieval based on GridStore technology. Since the focus of their

work is not on algorithm design, it is beyond our scope. Mirzaei et al. (2017) propose a closed-form formula

for the two desired items retrieval case where two loads are ﬁrst moved to a joining location and then brought

together to the I/O point. Moreover, a heuristic is extended to retrieve three and more loads. But the method

is valid only for systems with very high space utilization, i.e. only one escort, which is quite unrealistic. Zou

& Qi (2021) propose a heuristic algorithm to handle the multi-item retrieval problem with multiple randomly

5

located escorts and I/O points. The method, however, has some incompleteness, resulting in a relatively high

failure rate.

2.2. Reinforcement learning and its applications

Our work is closely related to reinforcement learning, speciﬁcally the Q-learning and Deep Q-Network

(DQN) algorithms. Reinforcement learning (RL) is a learning paradigm tailored to solve sequential decision

problems (e.g., consider sequencing the moves of the desired item in our problem). According to RL, an agent

(an escort in the PBS system) learns to action based on the feedback it received from the environment (the

PBS system). The value of taking an action on a state is evaluated by the state-action function, also called

the Q function. Therefore, getting such a Q function is critical for solving the problem. Details on RL can be

referred to Sutton & Barto (2018). Q-learning is a model-free algorithm for RL to learn the Q function without

knowing a model of the environment (e.g., the state transition probabilities). Watkins & Dayan (1992) present

in detail that Q-learning converges to optimal action values based on his previous work (Watkins (1989)), but it

is limited to domains with fully observed low-dimensional state spaces. DQN is a Deep Q-learning algorithm

that overcomes this shortcoming by using a deep neural network to approximate the Q function. Mnih et al.

(2013) apply DQN to learn control policies in Atari 2600 games, which was the ﬁrst to introduce the deep

learning model into reinforcement learning, making it possible for the agent to derive eﬃcient representations

of the environment from high-dimensional sensory inputs. Later enhanced DQN version in Mnih et al. (2015)

outperforms all previous approaches, showing the great potential of deep learning models with reinforcement

learning. Van Hasselt et al. (2016) present Double DQN to separate action selection from value estimation

to avoid overestimation. Dueling DQN, proposed by Wang et al. (2016), designs two separate estimators for

the state value function and the action advantage function respectively, which leads to better policy evaluation,

particularly when the Q values are close. Inspired by the human thought process, Schaul et al. (2015) emphasize

the critical role of prioritized experience replay in DQN training, and how DQN with prioritized experience

replay can improve sample utilization eﬃciency. More details of reinforcement learning can be found in Sutton

& Barto (2018).

In recent years, a large number of applications of reinforcement learning exist in logistics and transporta-

tion related ﬁelds, e.g., manufacturing planning for material handling systems (Govindaiah & Petty (2019),

Li et al. (2018)), routing in baggage handling systems (Mukhutdinov et al. (2019)), order dispatching in ride-

hailing/ride-sharing systems (Xu et al. (2018), Tang et al. (2019)), rebalancing in bike-sharing system (Pan

et al. (2019)). Reinforcement learning also shows promising potential for solving classical combinatorial op-

timization problems, e.g., travelling salesman problem (Vinyals et al. (2015)), vehicle routing problems (Kool

et al. (2019), Nazari et al. (2018)) etc.

To our knowledge, few studies have focused on the general multi-item retrieval problem in the PBS system.

Additionally, no existing work has tried to address this problem in a deep reinforcement learning manner and

further explores how to solve potential diﬃculties (e.g., convergence problems). This work attempts to ﬁll this

6

gap and extend existing research.

3. Problem Description

3.1. Multi-item retrieval

Consider a PBS system of size m × n with d (d (cid:62) 1) desired items, e (e (cid:62) 1) escorts and d I/O points, where

m, n are the number of rows and columns respectively. The locations of I/O points can be chosen randomly

but must be determined before training; this is reasonable given that picking stations in the warehouse do not

generally move. Typically, the number of desired items can be more or less than the I/O points; however, we

can always execute a pre-assignment step to assign a corresponding I/O point for each desired item. Therefore,

we set the number of the desired items and the I/O points to be the same. We leave the optimization of the

pre-assignment step for future work due to the complexity of the entire problem.

The objective of this work is to minimize the total number of moves. To simplify the modeling, we ﬁrst

assume that each step allows for only one move, i.e., the single-load movement assumption. Then, we oﬀer a

conversion algorithm to transfer the results to the simultaneous movement version.

3.2. Retrieve jointly and retrieve separately

An obvious approximate solution to a multi-item retrieval problem is to break it down into several single-

item retrieval problems and solve them sequentially. Note that this approach provides an upper bound for the

original problem. Here we give an example (Figure 2 and 3) to illustrate the diﬀerence between joint retrieval

and separate retrieval. Suppose there are two desired items ([2,2], [1,1]), two escorts ([0,1], [1,2]) and two I/O

points ([0,0], [0,3]) in a 4 × 4 grid ([0,0] is at the left-up corner). Intuitively, it is easy to decompose the problem

into two sequential single-item retrieval tasks (e.g., retrieve d1 ﬁrst, then d2, or retrieve d2 ﬁrst, then d1). At

least 17 moves are needed when retrieving d1 ﬁrst, whereas only 15 moves are required if we retrieve d2 ﬁrst.

Figure 2 shows the optimal path under the separate retrieval policy. While according to our proposed multi-item

retrieval algorithm (described in section 5), the minimum number of moves is 13 under the joint retrieval policy,

resulting in a saving of 15.4% (as shown in Figure 3). Thus, it is not desirable to simply decompose multi-item

retrieval problems into single-item retrieval ones and multi-item retrieval is worthy of dedicated optimization.

1
2

2
1

1
2

2
1

2

1
2
1

2
12
1

2

2

1

1

2 1

21

2

2

1

2

1

12
1

Initailiza-

(b) move 1

(c) move 2

(d) move 3

(e) move 4

(f) move 5

(g) move 6

(h) move 7

(a)
tion

2

2

2

1
1

1

2
1

2

1

2

1

2

2
1

1

1
2

1

2

1

2

2

1

2

1

2

2

2

1

1

1

(i) move 8

(j) move 9

(k) move 10

(l) move 11

(m) move 12

(n) move 13

(o) move 14

(p) move 15

Figure 2: Optimal path when retrieving separately

7

1
2

2
1

1
12
2

2
1

1
2

2
1

1
2

2 1
1

2

2 1
1

2

2 1
1

2

Initailiza-

(b) move 1

(c) move 2

(d) move 3

(e) move 4

(f) move 5

(g) move 6

(a)
tion

2 1 2
1

2

21
1

2

1 2
1

1
1

2

2

1

2

2

1

1

2

2

1

1 2

2

1

(h) move 7

(i) move 8

(j) move 9

(k) move 10

(l) move 11

(m) move 12

(n) move 13

Figure 3: Optimal path when retrieving jointly

4. Integer Programming Formulation

In this section, we provide a generic IP formulation to cover a variety of problem settings, including (i)

multiple desired items, multiple escorts, and multiple I/O points are all placed randomly, (ii) the number of

desired items/escorts can be set to any feasible values. In what follows, we ﬁrst present brief descriptions

about state transition that motivate us to design decision variables properly and then present the complete

mathematical formulation.

Suppose a grid of size m×n with e escorts and the position ID of each cell is denoted by p, p = 0, 1, · · · , mn−

1. There are d desired items in the grid, which are indexed by r. Intuitively, the state of the grid at each

timestamp, denoted as occupancy state, is determined by whether each cell is occupied and the positions of

the desired items. To better illustrate occupancy state and its changes, we introduce following four groups of

binary variables:

• xk

• yk

• zk

= 1; otherwise, xk
p: if position p is occupied after move k, then xk
p
p
p,r: if position p is occupied by desired item r after move k, then yk
p,r
p,q: if an item moves from position p to q at move k, then zk
p,q
p,q,r: if desired item r occupies position p and moves from position p to q at move k, then wk

= 1; otherwise, yk
p,r

= 1; otherwise, zk

= 0.

= 0.

= 0.

p,q

• wk

p,q,r

= 1;

otherwise, wk

p,q,r

= 0.

As shown in Figure 4, we move desired item 1 from position 8 to 7 during move k, and the occupancy

state changed accordingly. The whole retrieval process can be interpreted as a sequence of changing occupancy

states, i.e., the process starts with an initial occupancy state and terminates when and only when all desired

items reach their pre-assigned I/O points.

8

position

8

1

desired item ID

I/O#1

I/O#2

0

6

12

18

24

30

1

7

13

19

25

31

1

2

8

14

20

26

32

3

9

15

21

27

33

2

4

10

16

22

28

34

5

11

17

23

29

35

7

8

1

8

7

1

Occupancy state: xk−1

7

= 0, xk−1

8

= 1, yk−1
8,1

= 1

Move: zk

8,7

= 1, wk

8,7,1

= 1

Occupancy state: xk
7

= 1, xk
8

= 0, yk
7,1

= 1

(a) Occupancy state after move k − 1 (1 (cid:54) k (cid:54) K)

(b) Move k: move a desired item

I/O#1

I/O#2

0

6

12

18

24

30

1

1

7

13

19

25

31

2

8

14

20

26

32

3

9

15

21

27

33

2

4

10

16

22

28

34

5

11

17

23

29

35

(c) Occupancy state after move k

Figure 4: Move-occupancy state sequence

Based on the above observations, we attempt to formulate the problem in the form of IP. In order to make

the formulation tractable, we introduce an input parameter, the maximum number of moves K, which implies

that the retrieval task must be completed in at most K moves. K can be set to a suﬃciently large number or can

be estimated by a regression model (see Section 8.4 for details). And we use a parameter Ap,q to represent if

the position p and q are adjacent, i.e., the legality of the move. If it is possible to move from position p to q,
then Ap,q = 1, otherwise Ap,q = 0. Let P be the set of all the positions and D the set of all desired items, where
P = {0, 1, 2, · · · , mn − 1}, D = {1, · · · , d}. The multi-item retrieval problem can be formulated as follows:

s.t. x0
p

=

if position p is occupied initially

otherwise

∀p ∈ P,

if desired item r is at position p initially

otherwise

min

K(cid:88)

(cid:88)

(cid:88)

zk
p,q

k=0

q∈P

1,

p∈P



0,




=

1,

0,

y0
p,r

z0
p,q
(cid:88)

p∈P

= 0

xk
p

= |P| − e

xk−1
p −

(cid:88)

q∈P

zk
p,q

+

(cid:88)

q∈P

zk
q,p

= xk
p

9

∀p ∈ P, ∀r ∈ D,

∀p, q ∈ P,

∀k = 0, 1, · · · , K,

∀k = 1, 2, · · · , K, ∀p ∈ P,

(1)

(2)

(3)

(4)

(5)

(6)

yk
p,r
(cid:88)

p∈P
(cid:88)

(cid:54) xk
p

yk
p,r

= 1

yk
p,r

(cid:54) 1

r∈D
yk−1
p,r −

(cid:88)

q∈P

wk

p,q,r

+

(cid:88)

q∈P

wk

q,p,r

= yk
p,r

∀k = 0, 1, · · · , K, ∀p ∈ P, ∀r ∈ D,

∀k = 0, 1, · · · , K, ∀r ∈ D,

∀k ∈ k = 0, 1, · · · , K, ∀p ∈ P,

(7)

(8)

(9)

∀k = 1, 2, · · · , K, ∀p ∈ P, ∀r ∈ D,

(10)

= 1

yK
IOr,r
K(cid:88)

zk
p,q

(cid:54) Ap,qK

k=1

zk
p,q
(cid:88)

p∈P
(cid:88)

(cid:54) 1 − xk−1
(cid:88)

q

zk
p,q

(cid:54) 1

q∈P

wk

p,q,r

(cid:54) zk

p,q

∀r ∈ D,

∀p, q ∈ P,

∀k = 1, 2, · · · , K, ∀p, q ∈ P,

∀k = 0, 1, · · · , K,

∀k = 0, 1, · · · , K, ∀p, q ∈ P,

r∈D
if yk−1
p,r

= 1 and zk

p,q

= 1, then wk

p,q,r

= 1

∀k = 0, 1, · · · , K, ∀p, q ∈ P, ∀r ∈ D,

p, yk
xk

p,r, wk

q,p,r ∈ {0, 1}

∀k = 0, 1, · · · , K, ∀p, q ∈ P, ∀r ∈ D.

(11)

(12)

(13)

(14)

(15)

(16)

(17)

The objective function equation (1) is to minimize the total number of moves. Constraints (2)-(4) initialize

the value of the decision variables at step 0. Constraints (5) restrict that the number of occupied positions is

equal to the number of items (includes desired items and normal items) at every step. Constraints (6) denote the

occupancy state transition between move k − 1 and k. Constraints (7) imply that desired item r can not locate at

position p if p is an escort. Constraints (8) and (9) guarantee that each desired item must locate at one position

at each step, and each position can be occupied by at most one desired item as well. Similar to constraints

(6), constraints (10) denote the occupancy state transition of each desired item between move k − 1 and k.

Constraints (11) ensure that all desired items reach the corresponding I/O points at the ﬁnal move. Constraints

(12)-(16) are move relevant constraints. Constraints (12) restrict that an item can only move to its neighbor

positions over K timestamps. Constraints (13) imply that a move is allowed if the targeted position is an escort.

Constraints (14) ensure that at most one move is allowed at each step. Constraints (15) impose that the desired

items can move from position p to q only if an action from position p to q is executed. Constraints (16) clarify

the relationship among yk−1

p,r , zk

p,q and wk

p,q,r to specify the move condition of the desired items, and they can be

easily implemented by existing commerical optimization solvers (e.g., Gurobi). Finally, the variables’ type is

deﬁned by constraints (17).

We note that this IP model is quite compact compared to the IP model in Rohit et al. (2010) which is

dedicated to single-item retrieval problems. Speciﬁcally, we remove subscript i from decision variables x and y

of the previous IP model and add variables z and w. As a result, the number of decision variables and constraints

is signiﬁcantly reduced.

10

Even though the proposed IP formulation is based on the single-movement assumption, it can be easily

extended to simultaneous movement schemes by making the following three revisions: (i) remove constraints

(14), (ii) introduce binary decision variables fk to indicate whether any move is executed at timestamp k, and
add one more group of constraints to ensure the relationship between fk and zk
equations are fk (cid:62) zk

p,q, ∀p, q ∈ P, ∀k = 0, 1, · · · , K), and (iii) change the objective function to min (cid:80)K

p,q (more speciﬁcally, the detailed

k=1 fk.

5. Reinforcement Learning Formulation

We characterize this research issue as a sequential decision-making problem and structure it as a Markov

Decision Process based on its characteristics. The proposed model is then solved via reinforcement learning,

using a well-designed environment, dueling DQN, and double DQN for the PBS system. Finally, rather than

using traditional random exploration, we develop a semi-random and semi-guided action selection mechanism

to accelerate convergence. We will oﬀer a detailed description of the proposed techniques in this section.

5.1. Environment

An environment serves as the foundation for training a reinforcement learning model, in which an agent

interacts with the environment and learns from its experiences. The agent receives speciﬁc feedback (rewards)

based on the current state of the environment and the actions performed. If the agent receives a positive reward,

the appearance of the action will be enhanced; otherwise, the appearance of the action will be weakened.

Through the learning procedure, the agent tends to pick the best action (not 100%, given the eﬀects of possible

noise, exploration, possibility to converge to a suboptimal policy, etc.), i.e. the one with the highest cumulative

reward.

The iterative process of reinforcement learning for the PBS system is described as follows. To begin,

prepare the environment by placing d desired items and e escorts in random order. Then perform a move,

update the environment, and receive the corresponding reward. As shown in Figure 5, the environment changes

when a normal move is performed, and it remains unchanged with an illegal move (when the escort moves out

of the grid). Continue until all desired items have arrived at their I/O points, then terminate the round and reset

the environment.

(a) normal move

(b) illegal move

Figure 5: Types of moves

This section will focus on creating an environment suitable for the PBS system, including state, action, and

reward.

11

State To continue the learning procedure, a reinforcement learning agent needs environment-relevant in-

formation, which implies the environment should feedback state after each step. Rather than using local data

to generate a distributed decision (Mukhutdinov et al. (2019)), the state in this paper should reﬂect information

about the entire system. As a result, the state is represented by the coordinates of the desired items and escorts.

If there are d desired items, e escorts, and d I/O points in a m × n grid, the size of the state space is

Ad+e
m×n − Ae

m×n−d

(18)

=

(m×n)!

where Ad+e
m×n

(m×n−d−e)! , standing for the number of all permutations of the corre-
sponding setting. The second subtracted term represents the cases where the desired items are already at the

(m×n−d−e)! and Ae

m×n−d

= (m×n−d)!

corresponding I/O points and should be excluded.

Action Each escort has four possible actions correspongding to moving up, down, left, and right, and hence

there are 4e optional actions in total.

Reward The environment provides a positive reward (e.g., 1) if the desired items reach their I/O points;

otherwise, the reward is zero. This encourages the agent to discover the termination as soon as possible. In

fact, before implementing the semi-random and semi-guided action selection mechanism, we tried to design a

reward rule to tackle the sparse reward problem, but it was too sophisticated to guarantee the global optimum.

Therefore, sophisticated incentive design is no longer an option.

5.2. Semi-random and semi-guided action selection mechanism

In reinforcement learning, it is essential to strike a balance between exploration and exploitation when

selecting actions. Traditionally, pick actions randomly to explore the environment; relatively, choose actions

based on the predicted action values from the neural network to exploit the stored information. With exploitation

remaining unchanged, we ﬁnd the random action selection for exploration is not suitable for this problem as

it is diﬃcult to reach the ﬁnal state due to the large state space and the non-directional escort motions. The

experiments indicate that when the grid size is 6 × 6 and there are 2 desired items and 2 escorts, it generally

takes tens of thousands of moves to reach the ﬁnal state; that is, there are only a few meaningful pieces among

the tens of thousands of experiences. Sparse reward means that the agent will not receive enough eﬀective

feedbacks, which is detrimental to learning. To increase the rate of receiving non-zero rewards, we introduce

a guiding method to help the agent choose reasonable actions and reach the ﬁnal state quickly. Therefore, a

semi-random and semi-guided action selection mechanism is designed to replace the random one.

Semi-random and semi-guided action selection mechanism implies that the chance of adopting a random

action selection mechanism is 1 − η and the likelihood of using a guided action selection mechanism is η, where

η is a parameter and can take any value ranging from 0 to 1. During the early stages of training, the neural

network has very limited knowledge. Thus, a large η should be used to aid the agents in locating the ﬁnal state

more quickly. In the later stages, as the neural network matures and stores more data, the value of η should be

decreased to encourage the agent to explore new information.

12

5.2.1. Random action selection mechanism

It is self-evident that the random action selection mechanism chooses one action randomly among the

available actions. This is not a subject that we address extensively here.

5.2.2. Guided action selection mechanism

As the name implies, the guided action selection mechanism is a heuristic algorithm that guides escorts

to move eﬀectively to enable the agent to complete the task sooner. Note that exploration is inherently part

of reinforcement learning, thus this approach does not need to be a complete heuristic algorithm as long as

it is instructive. There are two essential evaluation indices: the distance between the desired item and the
corresponding I/O point ditem manhattan and the distance between the escort and its useful point (which may be
considered as the I/O point for the escort) descort manhattan. Let
(xio, yio) the location of its corresponding I/O point.

be the location of the desired item,

xp, yp

(cid:17)

(cid:16)

Thus, the distance between the desired item and the corresponding I/O point is

ditem manhattan = (cid:12)(cid:12)(cid:12)xio − xp

(cid:12)(cid:12)(cid:12) + (cid:12)(cid:12)(cid:12)yio − yp

(cid:12)(cid:12)(cid:12)

(19)

Likewise, let (xescort, yescort) be the location of the escort,

(cid:16)

xuse f ul, yuse f ul

(cid:17)

the location of its useful point.

Points that satisfy the following two conditions are useful. (i) The point is adjacent to the desired item, at which
|xuse f ul − xp| + |yuse f ul − yp| = 1 holds. (ii) ditem manhattan decreases if the desired item reaches this point, i.e.,
(cid:12)(cid:12)(cid:12)xio − xp
(cid:12)(cid:12)(cid:12). Thus, the distance between the escort and its useful point
is

(cid:12)(cid:12)(cid:12) + (cid:12)(cid:12)(cid:12)yio − yuse f ul

(cid:12)(cid:12)(cid:12)xio − xuse f ul

(cid:12)(cid:12)(cid:12) + (cid:12)(cid:12)(cid:12)yio − yp

(cid:12)(cid:12)(cid:12) >

descort manhattan = |xuse f ul − xescort| + |yuse f ul − yescort| + 2 × θ

(20)

where θ is a binary indicator of whether the desired item is between the useful point and the escort.

Take Figure 6(a) as an example, the desired item [1,1] is not between the useful point [0,1] and the escort
[2,2], thus θ = 0 and descort manhattan = |0 − 2| + |1 − 2| + 2 × 0 = 3. In Figure 6(b), the desired item [1,1] is located
between the useful point [0,1] and the escort [2,1], thus θ = 1 and descort manhattan = |0 − 2| + |1 − 1| + 2 × 1 = 4.

desired item

escort

useful point

1

1

1
1

(a) θ = 0

(b) θ = 1

Figure 6: Example of diﬀerent θ

As shown in Figure 6, one desired item can be searched both horizontally and vertically to obtain two useful

points. For a particular escort and d desired items, a matrix of shape d × 2 (denoted as descort manhattan matrix) can

be used to store descort manhattan. While the desired item may occasionally lack a vertical or horizontal useful

13

point, we set descort manhattan = ∞ accordingly.

The guided action selection mechanism is described in detail below.

Algorithm 1 Guided action selection mechanism
Input: State
Output: The selected action

1: Randomly choose an escort
2: (descort manhat t an mat rix)
3: for item in desired items do
4:

5:

6:

7:

Get the vertical useful point
Compute descort manhattan between the escort and the vertical useful point
Get the horizontal useful point
Compute descort manhattan between the escort and the horizontal useful point
Compute ditem manhattan

8:
9: end for
10: Find the min descort manhattan, corresponding ditem manhattan, desired item and useful point
11: (Find a useful move)
12: for action in shuﬄed(up,down,left,right) do
13:

Take action
Compute ditem manhattan new
if ditem manhattan new < ditem manhattan then

Return action

end if
if ditem manhattan new = ditem manhattan then

Compute descort manhattan new between the escort and the useful point
if descort manhattan new < descort manhattan then

Return action

end if

14:

15:

16:

17:

18:

19:

20:

21:

22:

end if

23:
24: end for

5.3. Dueling deep neural network

The neural network, which acts as an approximation function Q(s, a), determines the value of a state-action

pair (s, a). It appears natural to represent the PBS system as a picture, with each pixel representing a grid cell,

and feed the image data into a convolutional neural network for learning. Experiments, however, demonstrate

that this technique is unfavorable. The convolutional neural network tends to obliterate details in favor of

capturing the primary features. While in this case, the action option is inﬂuenced by the “image details”. Once

the desired items or escorts are relocated, the action will alter completely.

Considering the characteristics of the problem, we use an input layer, several hidden layers and an output

layer, which are all fully connected layers, to build the deep neural network (DNN) and choose Relu as the

activation function. This structure can eﬃciently compute the Q-values for all actions in a given state S (Mnih

et al. (2015)). The input of the neural network is represented by an array of the desired items’ coordinates

followed by the escorts’ coordinates, in the form of (x1, y1, x2, y2, · · · , xd, yd, x(cid:48)
e) where
(x, y) are items’ coordinates and (x(cid:48), y(cid:48)) are escorts’ coordinates. Naturally, the numbers of hidden layers and

2, · · · , x(cid:48)

1, x(cid:48)

2, y(cid:48)

1, y(cid:48)

e, y(cid:48)

neurons are up to the complexity of the speciﬁc problem. The more complex the problem, the higher the

14

numbers; conversely, the lower the numbers. Moreover, we observe that states with a small ditem manhattan

generally have a higher state value V(s) than those with a larger ditem manhattan in the PBS system. Scilicet,

the state has an anticipated V(s) independent of the actions available. As a result, we adopt the dueling DNN

as our neural network architecture, as shown in Figure 7. According to Wang et al. (2016), the dueling DNN

can signiﬁcantly improve the learning performance by separating the state-action value function Q(s, a) into

two parts: state value function V(s) and advantage value function A(s, a). Then, two deep neural networks, as

shown in the top and bottom of Figure 7, are used to estimate V(s) and A(s, a), respectively. After that, the Q
(cid:80)
a(cid:48)∈A

value is calculated by Q(s, a) = V(s) + (A(s, a) − 1
|A|

A(s, a(cid:48))), where A is the set of all actions. For more

details of Dueling DNN, see Wang et al. (2016).

Figure 7: Structure of dueling DNN

5.4. Double & Dueling DQN algorithm for the PBS system

We utilize DQN (Mnih et al. (2015)) as the framework for reinforcement learning, that is, to deduce the

parameters θ (the weights of the connections) of the state-action value function Q(s, a; θ). Furthermore, since

overestimations of the value functions commonly occur in DQN, we adopt the Double DQN (Van Hasselt et al.

(2016)) to solve our problem. Due to the existence of sumTree, the data structure for the implementation of

prioritized experience replay, the priority DQN (Schaul et al. (2015)) (an alternative improvement to DQN) is

somewhat time-consuming and hence not adopted.

When choosing an action, the agent has a probability of ε to explore using a semi-random and semi-guided

mechanism, and a probability of 1 − ε to exploit knowledge already revealed by the neural network. The

Dueling DNN introduced in Section 5.3 serves as the action-value function Q (with weights θ) and the target

action-value function ˆQ (with weights θ−). For a mini-batch of samples, at iteration j, the expected Q value y j

15

·····················𝑉(𝑠)···𝑄(𝑠,𝑎)𝐴(𝑠,𝑎)············Coordinatesofdesired itemsCoordinatesofescorts···Fully connect layerswithReluactivation···𝑄𝑠,𝑎=𝑉𝑠+(𝐴𝑠,𝑎−1𝒜-!!"𝒜𝐴(𝑠,𝑎$))can be calculated by

y j =





r j

(cid:16)

r j + γ ˆQ

s j+1, arg maxa Q

(cid:16)

s j+1, a; θ

(cid:17)

, θ−(cid:17)

if done j+1

otherwise

(21)

where done j+1 indicates whether all desired items reached their I/O point at the next iteration.The loss function
L j (θ) measures the diﬀerence between the expected Q value and the predicted Q value and is written as

L j (θ) = Es,a

(cid:20)(cid:16)

(cid:16)

y j − Q

s j, a j; θ

(cid:17)(cid:17)2(cid:21)

(22)

where Q

(cid:16)

(cid:17)

s j, a j; θ

is the predicted Q value by function Q. The network parameter θ of function Q is modiﬁed

at each training step and θ− of function ˆQ is updated every C episodes according to a gradient descent method.

The Double & Dueling DQN algorithm dedicated to the PBS system is presented in detail in Algorithm 2.

Algorithm 2 Double & Dueling DQN algorithm for the PBS system
Input: Initialize replay memory D to capacity N

Initialize action-value function Q with random weights θ
Initialize target action-value function ˆQ with weights θ− = θ
Initialize action choose parameters ε and η

Output: Trained DNN
1: for episode = 1, M do
2:

t = 0
Initialize state st with donet = False
Update ε and η (ε and η gradually decrease)
while not donet do

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

With probability ε select action at by using semi-random and semi-guided mechanism(η)
With probability 1 − ε select action at = arg maxa Q (st, a; θ)
Execute action at in emulator and observe reward rt, next state st+1 and if episode terminates donet+1
Store transition(st, at, rt, st+1, donet+1) in D
Set t = t + 1
Sample random mini-batch of transition (s j, a j, r j, s j+1, done j+1) from D

Set y j =

(cid:16)

r j + γ ˆQ





r j

(cid:16)

s j+1, arg maxa Q
(cid:16)

s j+1, a; θ
(cid:16)

(cid:17)

, θ−(cid:17)
(cid:17)(cid:17)2
s j, a j; θ

if done j+1
otherwise

with respect to the network parameters θ

Perform a gradient decent step on

y j − Q

end while
Reset ˆQ = Q every C episodes

15:
16: end for

6. Simultaneous movement consideration: a conversion algorithm

The assumption of single-load movement wastes time and is therefore uneconomical for real warehouses.

Multiple independent movements (for example, moves 1 and 2 in Figure 3) can be combined into a collection

and then executed at the same timestamp. Several existing literature also makes the simultaneous movement

assumption, e.g., Gue et al. (2013), Yu et al. (2016), and Bukchin & Raviv (2020). As seen in Figure 8, the

example in Figure 3 can be completed in 8 timestamps instead of 13, reducing the processing time by about

16

40%. Thus, allowing simultaneous movement is a reasonable and necessary requirement.

1
2

2
1

2
1

1
2

2
1

1

2

2 1
1

2

2

21
1

2

2

1
1

1

1

2

2

1

1

2

2

1

1 2

2

(a) Initailiza-
tion

(b) time 1

(c) time 2

(d) time 3

(e) time 4

(f) time 5

(g) time 6

(h) time 7

(i) time 8

Figure 8: Optimal path with simultaneous movement

Since the reinforcement learning method is based on the single-load movement assumption, we develop an

additional algorithm to convert the results into a simultaneous movement version. The main function of this

algorithm is to detect whether the current escort’s action contradicts the prior ones. When no conﬂict appears,

escorts can move at the same timestamp. If not, the current escort must wait. The ﬁrst type of conﬂict occurs

when the current escort is moving to the locations the items just arrived. In other words, an item can only be

moved once during the same time period. Take Figure 9(a) as an example, both escorts 1 and 2 serve the desired

item 1. As a result, these two processes should be done independently. Sometimes, it is possible for two escorts

to exchange positions and the passively moving escort may cause the second form of conﬂict. As shown in

Figure 9(b), the single-load movement solution is (1-right,2-up), and the movement of escort 1 (1-right) leads to

the passive movement of escort 2 (2-left). Therefore, the active movement of escort 2 (2-up) should be deferred

until the passive action is completed. Based on these two forms of conﬂict, we summarize two principles of

identiﬁcation. Considering the locations of the previous escorts as forbidden places, principle 1 stipulates that

the escort cannot enter forbidden areas and principle 2 states that the escort currently in forbidden areas cannot

leave.

1 2
1

11
2

1 1
2

1 2
1

1 1
2

1 2
1

12
1

2

1
1

1 2
1

2

1
1

(a) conﬂict 1

(b) conﬂict 2

Figure 9: Two conﬂict situations of simultaneous movement

Now, we are ready to present the conversion algorithm in Algorithm 3. Based on the single-load movement

solution, check whether it violates principles 1 and 2 for each escort. If it does not, it can be combined with

the previous escorts, otherwise, it must wait. Iterate this process until the conversion is complete. The list

move timestamps stores all move timestamps for each escort, the list move actions stores the action at each

timestamp and the list escort locations stores the locations of all escorts for each timestamp.

7. Solving large-scale instances: a decomposition framework

Reinforcement learning is capable of providing solutions for small-sized instances (e.g., grid size 8 × 8)

in milliseconds. As the problem size increases, the training time required for reinforcement learning becomes

longer. Consider a 20 × 20 PBS system with two desired items and three escorts, the state space of which is

17

Algorithm 3 Conversion algorithm from single-load to simultaneous movement
Input: Lists move timestamps, move actions and escort locations
Output: List simultaneous moves

1: while True do
2:

Save the next move timestamp for each escort as available according to move timestamps
if available is empty then

break

end if
Sort available according to the timestamps
(The ﬁrst move in available is absolutely allowed)
Add the ﬁrst escort in available, its current location, and action into moved, prohibited, and

simultaneous moves temp respectively

Delete timestamp 1 in move timestamps
Delete the ﬁrst escort in available
Let from denotes timestamp 1
for move info in available do

Get the escort and let to denotes the new timestamp
for time in range(from+1, to) do

Get the temp escort which is moved at timestamp time
Add the current location of temp escort into prohibited

end for
(Conﬂicts 1 and 2)
if the next or current location of the escort is in prohibited then

Continue

else

Add the action of the escort into simultaneous moves temp
Delete to in move timestamps

end if
Add the escort and its current loaction into moved and prohibited respectively
from = to

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

end for
Add simultaneous moves temp into simultaneous moves

28:
29: end while

A2+3
20×20

− A3

20×20−2

= 9.98617 × 1012, implying an unaﬀordable training time. Thus, reinforcement learning is

too time-consuming to be applied directly in real-world industrial scenarios. To extend it to large-scale cases,

we construct a framework (as seen in Figure 10) to decompose large-scale instances into smaller ones, called

sub-PBS, and solve them one by one. With such an approach, the advantages of reinforcement learning are

fully exploited. Note that this framework can also integrate existing exact and heuristic algorithms.

18

Figure 10: Decomposition algorithm framework

The decomposition framework works as follows.

• First, determine the grid size of the sub-PBS. Similar to the ﬁlter in a convolutional neural network

(CNN), the sub-PBS region will slide through the PBS system, thus generating the small-sized problems

to be solved. And there are several points to note: (i) Generally speaking, since the decomposition

process usually has a negative impact on the solution, the sub-PBS should be as large as possible. At the

extreme, if the sub-PBS is the same size as the PBS, i.e., if the original problem is solved directly, the

error caused by the decomposition process will not occur. In our experiments, we used a dimension of

5 × 5. (ii) The sliding of the sub-PBS regions should be overlapping, i.e., the stride should be smaller than

the corresponding edge length, thus ensuring that the desired items have a chance to reach any location

in the PBS system. In addition, a small stride allows choosing from more sub-problems, but it may

lead to increased computation burden at the same time. With a long stride, the computational eﬀort will

decrease, but it is likely to result in a limited choice of sub-problems. In short, there is no clear set of

criteria for choosing the stride. In our experiments, we randomly use a stride of 1 or 2 to balance the

speed and accuracy.

• Then, slide the sub-PBS region from bottom to top and from left to right in the PBS system until there is

at least one desired item in the sub-PBS region.

• Next, generate a solvable sub-PBS problem according to the sub-PBS region. We need to determine

where to relocate the desired items within the sub-PBS region. Speciﬁcally, the (virtual) I/O point of an

item is assigned to an ambient location of the region which is mostly toward the ﬁnal I/O point. Moreover,

we assume that there should be at least e (e ≥ 1) escorts in the sub-PBS region. If there are less than e

escorts, we must transfer the available escorts from outside to the current sub-PBS region; to distinguish

them, we refer to these escorts as transferred escorts.

• Following, execute the reinforcement learning algorithm (can be other exact and heuristic algorithms) to

19

Sub-problemSingle-loadmovementsolutionMulti-itemretrieval problemSimultaneousmovementsolutionDecompositionReinforcement learningConversionFinishedYesNosolve the sub-PBS problem. Note that some transferred escorts may not be utilized in the solution of the

current sub-PBS problem, resulting in a waste of the total number of movements. These invalid escorts

would return to their previous locations, and the resulted additional movements would not be recorded.

• Finally, convert the single-load movement solution into a simultaneous movement version with Algo-

rithm 3 after completing the iterative process.

1
1

2

1
1

2

1

1

2

1

1

2

2

2

2

2

(a) Puzzle-based storage

(b) Sub-PBS

(c) Call
learning

reinforcement

(d) Updated puzzle-based storage

Figure 11: Iterative example of the decomposition algorithm

We present an example to illustrate the process of a single iteration. As shown in Figure 11(a), there is a

sub-PBS region marked by black dotted lines. In Figure 11(b), we extract the sub-PBS and mark the virtual I/O

points of the desired items 1 and 2 with red dotted lines. Let e = 2, unfortunately, there are not enough escorts.

Therefore we transfer escorts 1 and 2 into the region. In Figure 11(c), we apply the reinforcement learning

algorithm to solve the sub-PBS issue, indicating that transferred escort 2 is not used. Finally, in Figure 11(d),

escort 2 returns to its previous position, while the sub-PBS region advances to the next iteration.

8. Numerical Experiments

We conduct extensive numerical experiments to investigate the performance of the reinforcement learning

approach and the decomposition framework (in which the conversion algorithm is embedded). We ﬁrst compare

the solution of the reinforcement learning approach to the optimal solution obtained by the IP model. After

which, we compare our approach with existing algorithms in their corresponding problem settings. Then, we

explore the boundary of the proposed method via increasing the grid size and construct a regression model

to estimate the retrieval eﬀort. Finally, we demonstrate the eﬀectiveness of the decomposition framework by

examining medium- and large-scale instances. Our purpose is to demonstrate that (i) the reinforcement learning

solutions are extremely close to optimal for the majority of small-scale instances, (ii) reinforcement learning can

adapt to a variety of cases, and outperforms state-of-the-art algorithms in the corresponding problem settings,

(iii) the proposed approach can handle small-scale instances eﬃciently, and (iv) the decomposition framework

is capable of solving large-scale problems and the conversion algorithm is beneﬁcial.

In reinforcement learning-related training, the neural networks contain an input layer, 6 to 10 hidden layers

with 64 or 128 neurons (determined by the complexity), and an output layer. Our model is trained with the

20

Adam optimizer using a learning rate of 10−4. The capacity N of the replay memory D is 106 and we use mini-

batches of 103 sequences. As for the two coeﬃcients related to exploration and exploitation, ε decays uniformly

from 1 to 0.1 and η from 0.7 to 0. To prevent vanishing gradient, the decay coeﬃcient γ should be large enough,

while it may cause exploding gradient. To ensure successful and stable training, we let γ gradually increase

from 0.8 to 0.98 during the training process. All reinforcement learning-related neural network training is

performed on a single GPU (2080Ti), and other numerical experiments are carried out on a Windows Operating

System with a 2.90-GHz CPU and 16 GB of RAM (IP models are solved by Gurobi 9.0.1). Detailed numerical

experimental results can be viewed via the GitHub link 1.

8.1. Experiment setting

We randomly generate a series of benchmark instances for various problem settings, including single-item

and multi-item retrieval problems. The detailed information of the benchmark instance set is summarized in

Table 2.

Instance Series ID

R422
R622
F611

F621

R611

R612
R613

R621
R622

R623
R422
R522
R622
R722
R822
R-6×37-1-22
R-6×37-13-22
R-10×61-1-61
R-10×61-21-61
Total

Grid
size
4 × 4
6 × 6
6 × 6

6 × 6

6 × 6

6 × 6
6 × 6

6 × 6
6 × 6

6 × 6
4 × 4
5 × 5
6 × 6
7 × 7
8 × 8
6 × 37
6 × 37
10 × 61
10 × 61

Table 2: Comparision between RL and IP

Desired
items
2
2
1

Escorts

2
2
1 ([0, 0])

I/O points

2 ([[0, 0], [0, 3]])
2 ([[0, 0], [0, 5]])
1 ([0, 0])

Instance
Number
1000
1000
35

2

1

1
1

2
2

2
2
2
2
2
2
1
13
1
21

1 ([0, 0])

2 ([[0, 0], [0, 1]])

200

1

2
3

1
2

3
2
2
2
2
2
22
22
61
61

1 ([0, 0])

1 ([0, 0])
1 ([0, 0])

2 ([[0, 0], [0, 5]])
2 ([[0, 0], [0, 5]])

2 ([[0, 0], [0, 5]])
2 ([[0, 0], [0, 3]])
2 ([[0, 0], [0, 4]])
2 ([[0, 0], [0, 5]])
2 ([[0, 0], [0, 6]])
2 ([[0, 0], [0, 7]])
1([0,18])
13
1([0,30])
21

1000

1000
1000

1000
1000

1000
1000
1000
1000
1000
1000
200
100
200
100
10835

Purpose

Compare IP and RL

Compare with Gue &
Kim (2007)
Compare with Mirzaei
et al. (2017)

Compare with Yalcin
et al. (2019)

Compare with Zou &
Qi (2021)

Sensitivity analysis on
the RL approach

Test the performance of
the decomposition
framework

1Escorts: [0, 0] denotes that the escort location is ﬁxed at location [0, 0]. If not particularly marked, the escorts are placed randomly.

For readability, we generate instance series IDs for each dataset according to its characteristics. We assume

that small instances (where the grid size is no more than 8 × 8) have a square grid shape, i.e., m = n. Therefore,

we refer to the grid size as n in our instance series ID. Speciﬁcally, we label these instances as Pnde, where P

has two optional values: R (escorts are randomly distributed) and F (only one escort is considered and placed

1https://github.com/hsinglukLiu/Puzzle-based_storage_appendix

21

at [0,0]), and n, d, e are the grid size, the number of desired items and escorts respectively. For medium and

large instances, we intuitively denote the instance ID as R-m × n-d-e. For instances with d > 1, we simply

assign the ithdesired item to the ith I/O point where i = 1, 2, ...d. All 10835 instances are classiﬁed into four

series for diﬀerent purposes. More speciﬁcally, the R422 and R622 series aim to verify the solution quality

of the reinforcement learning approach by comparing its results with those of the IP formulation. Then, 8

groups of instances are used to compare the proposed method to existing algorithms. After that, experiments

on Rn22 (n ∈ {4, 5, 6, 7, 8}) instance sets are conducted to demonstrate the eﬀectiveness of the reinforcement

learning approach on handling small-scale instances and conduct relevant sensitivity analysis. Finally, we test

the performance of the decomposition framework employing warehouses of size 6 × 37 and 10 × 61 as medium

and large instances, respectively.

Based on our experiments, the reinforcement learning method performs well on Rn11 (100% optimal ac-

tually). However, it is diﬃcult to ensure optimality when training complex scenarios. Occasionally, the model

does not learn how to solve a complicated instance, i.e., the solution obtained is invalid and the number of

actions may be unreasonably large. Therefore, we need to eliminate such outliers. The elimination rule is:

since the minimum number of moves for Rn11 is 8n − 11 (according to our experiments and proved by Gue &

Kim (2007)), the upper bound for Rnde instances is (8n − 11)d. If the reinforcement learning solutions exceed

(8n − 11)d, we think the method fails to solve the instances, otherwise, the instances are considered successful.

The elimination rule is also applicable to heuristic algorithms.

8.2. Solution quality for small instances: comparison between reinforcement learning and integer program-

ming

To validate the solution quality of the reinforcement learning approach, we compare its results with the IP

solutions. Since the IP model is diﬃcult to solve as the problem size increases, we only test R422 and R622

instances. To avoid wasting too much time on hard instances, we set the CPU time limit to 14400 seconds (4h).

The results are summarized in Table 3.

The indices in Table 3 are deﬁned as follows:
• Average IP best LB = (cid:80)N

i=1 (IP best LBi) /N , where i is the instance ID, N is the number of tested
instances. IP best LB is the minimum lower bound reported by the solver within the CPU time limit

(all tested instances are included).

• Average IP incumbent Gap (%) = 100×(cid:80)N
i=1

(cid:16) IP incumbent Obji−IP best LBi
IP incumbent Obji

(cid:17)

/N %, representing the optimality

gap of the best solution obtained by the solver.

• Average RL Gap to IP incumbent (%) = 100 × (cid:80)N
i=1

(cid:16) RL Obji−IP incumbent Obji
RL Obji

(cid:17)

/N %, indicating the gap

between the reinforcement learning solutions and the incumbents obtained by the solver.

For the set of R422 instances, all 1000 instances are solved to optimality by the IP formulation. It is gratify-

ing that the RL approach obtains optimal solution on 933 instances, achieving a quite tight average optimality

gap (only 0.271% actually). With respect to R622 instances, the overall results become more complicated

22

Instance series
Total number of instances
RL Obj range
Number of tested instances

Average RL Obj

Average RL CPU time (s)

Average IP incumbent Obj

Average IP best LB

Average IP incumbent Gap (%)
Average IP CPU time (s)

Average RL Gap to IP incumbent (%)
Total Success (RL)
Total Optimal (RL)

Total Success (IP)
Total Optimal (IP)

Table 3: Comparision between RL and IP

R422
1000
[1, 30]
1000

15.503

0.014

15.461

15.461

0 %
33.621

0.271%
992/1000
933/1000
1000/1000
1000/1000

R622
1000

[0, 20]
110

[21, 29]
275

[30, 32]
124

[33, 74]
491

15.591

25.807

31.008

39.648

31.615

0.031

0.015

0.025

0.031

0.038

15.582

25.712

30.860

34.946

27.011*

25.601

15.582
0 %
17.346

0.061%
110/110
109/110
110/110
110/110

25.567
0.486%
2116.403

0.320%
275/275
243/275
274/275
262/275

28.323
7.781%
10964.640

0.427%
124/124
43/124
114/124
57/124

27.177
27.177%
14232.863

1.072%
479/491
8/491
148/491
13/491

*: Note that only the instances with at least one found integer feasible solution are included when computing this
metric. This is exactly why the average IP incumbent objective on R622 instance set (646 included instances) is
signiﬁcantly smaller than that of RL approach (i.e., 31.615, and 988 included instances). However, it does not mean
that RL solutions are far from the optimal solution.

because of the increased problem size. To illustrate the comparison results more clearly, we categorize the in-

stances into four groups by their RL objective values, which reﬂects the computational complexity of instances,

as those with higher RL objective values are often more diﬃcult to solve using the IP formulation, more specif-

ically, [0, 20], [21, 29], [30, 32], [33, 74]. Basically, the ﬁrst two sets of instances are relatively easy to solve

and the performance of the RL approach is well veriﬁed according to the reported results. Speciﬁcally, for the

ﬁrst group of instances (110 in total), the IP model obtains optimal solutions on all of them. Additionally, the

RL approach achieves an impressive performance on these instances, where 109 out of 110 instances’ solutions

are proved to be optimal as well as the corresponding overall average optimality gap hits 0.061%. And then,

for the second set of instances, the RL approach solves 243 out of 275 instances to optimality, with an average

gap of 0.320%. However, as the RL objective value rises, the computational burden of the IP model increases

dramatically, resulting in more instances where no feasible solution can be found within the time limit. To be

more speciﬁc, 10 and 343 instances fail to yield available feasible solutions by solving the IP model for the third

and fourth groups of instances respectively. As a result, the optimality gap of RL solutions cannot be eﬀectively

evaluated on the aforementioned instances. Fortunately, the results indicate that these two sets of instances’ RL

solutions are very near to their IP solutions, with corresponding overall average gaps of 0.427% and 1.072%.

It is also worth noting that the needed computational time of the trained RL models for all tested instances is

extremely short (in milliseconds). In contrast, the IP model is usually highly time-consuming, especially for

those hard-to-solve instances. Based on the above comparisons, the reinforcement learning approach seems to

be powerful on small-scale instances, both in solution quality and computational time.

We can also look into the failed instances by the RL method and ﬁgure out how to tackle them. It is easy

to ﬁnd that instances with larger RL objectives are more likely to fail. These instances require more moves

23

to ﬁnish the retrieval tasks, i.e., more rounds of action-taking decisions. In each round, the RL method has

the possibility to give an inappropriate action due to the noise of the training process. Consequently, the more

rounds there are, the more likely the task will fail. In practice, we further suggest two options to solve the failed

instances. First, notice that RL fails because the route length surpasses the given limit; for example, the item

is tracked and moving in a local area. Therefore, we can try to ﬁnd out such one or a few “weak” points and

randomly or heuristically select a move other than RL’s suggestion. We leave this for future work. Second,

if, unfortunately, the ﬁrst one fails again, we can adopt any algorithm which supports single-item multi-escort

retrieval in PBS, such as Yalcin et al. (2019), to route the items separately, and then use our conversion method

to transfer the solution to a simultaneous movement plan.

Table 4: Comparision of RL and other algorithms

Instance series ID

Methodology

Ave. Obj

Ave. CPU time (s)

Total Success

F611

F621

R611

R612

R613

R621

R622

R623

RL

Gue & Kim (2007)

RL

Mirzaei et al. (2017)

RL

Yalcin et al. (2019)

RL

Yalcin et al. (2019)

RL

Yalcin et al. (2019)

RL

Zou & Qi (2021)

RL

Zou & Qi (2021)

RL

Zou & Qi (2021)

19.857

19.857

32.462

38.720

19.369

19.463

17.505

18.198

16.931

17.534

34.72

42.179

31.615

33.785

29.545

30.967

0.014

0.000004

0.028

0.0000052

0.024

0.00032

0.015

0.00099

0.015

0.00123

0.030

0.346

0.028

0.346

0.028

0.556

35/35
35/35

199/200
200/200

1000/1000
1000/1000
999/1000
1000/1000
995/1000
1000/1000

1000/1000
900/1000
988/1000
948/1000
974/1000
976/1000

Diﬀ (%)

0

−19.278

−0.005

−3.959

−3.562

−21.483

−6.864

−4.813

1Diﬀ (%): 100(RL Obj−Other Obj)

RL Obj

8.3. Extension to several reduction versions

Since existing researches rarely consider multiple desired items and multiple escorts at the same time, it is

diﬃcult for us to verify the eﬀectiveness of our method. However, by slightly changing the parameters, rein-

forcement learning can solve the reduction versions, i.e., single-item retrieval with a single escort, single-item

retrieval with multiple escorts, multi-item retrieval with a single escort. We conducted several numerical exper-

iments to show that reinforcement learning outperforms state-of-the-art algorithms. The results are summarized

in Table 4 and the details of the experimental design and analysis of the results are described in the following

sections.

24

8.3.1. Comparison between Gue & Kim (2007) and reinforcement learning

For Fn11, Gue & Kim (2007) presented a closed-form formula (optimal) to indicate the minimum number

of moves. To retrieve an item located at (i, j), the needed number of moves is

6i + 2 j − 13,

6 j + 2i − 13,

8i − 11,

if i > j,

if j > i,

if i = j.

(23)

(24)

(25)

This work starts from 0 instead of 1 (Gue & Kim (2007)) to index the grid cells. To be uniform, we

must execute a transfer step before running. More speciﬁcally, assuming that the coordinates are (i(cid:48), j(cid:48)) in our

generated instances, let i = i(cid:48) + 1,

j = j(cid:48) + 1 to use the formula. As the escort must initially locate at the

I/O point in Gue & Kim (2007), the number of available instances is limited (only 35 instances in total). We

compare RL with Gue & Kim (2007) by testing F611 instances.

For F611, all instances are solved to optimality by the RL approach. As for the CPU time, the RL ap-

proach terminates within 0.1 seconds. Since the closed-form formula can oﬀer the number of moves without

considering state transitions, its CPU time is extremely short.

8.3.2. Comparison between Mirzaei et al. (2017) and reinforcement learning

Another variant is Fn21, where the initial position of the single escort is ﬁxed at [0,0]. Mirzaei et al. (2017)

proposed a heuristic for Fn21, which we found does not fully consider the boundary conditions. We reﬁned

Theorem 1 of Mirzaei et al. (2017) by ﬁlling in the missing cases: the minimum number of moves to retrieve

two horizontally aligned adjacent desired items in a puzzle system is

3i + 7 j − 11,

10i − 4,

3i + 7 j − 13,

10i − 7,

10i − 9,

7i + 3 j − 7,

7i + 3 j − 9,

i < j and i even,

j − i = 1 and i odd,

j − i (cid:62) 2 and i odd,

i = j and i even,

i = j and i odd,

i > j and j even,

i > j and j odd.

(26)

(27)

(28)

(29)

(30)

(31)

(32)

where (i, j) is the position of the desired item closer to the I/O point and the escort is positioned to the right of

the two desired items. Note that the two desired items must be gathered together before applying the formula.

Similar to the procedure in Section 8.3.1, the index transformation is required before running the algorithm,

which we will not describe here. To make the setup of this paper consistent with that of Mirzaei et al. (2017),

we specify that the retrieval task is completed when the two desired items are moved to the positions [0,0] and

25

[0,1], respectively. We ignore the ﬁnal move (i.e., move the second desired item from [0,1] to [0,0]) in Mirzaei

et al. (2017). Hence, the result oﬀered by the revised heuristic method is one less than that calculated by the

original version. In addition, the initial position of the escort is ﬁxed at [0,0]. These two settings result in fewer

instances (200 instances are selected).

The results indicate that RL outperforms the heuristic. RL obtained valid solutions in 99.5 percent of

all instances. Moreover, RL provides a 19.28% reduction in the average number of moves compared to the

heuristic algorithm. The main reason is that gathering the two desired items requires too many moves and,

therefore, impairs the heuristic method’s performance. The CPU time is not comparable because the heuristic

results are calculated by directly employing formulas.

8.3.3. Comparison between Yalcin et al. (2019) and reinforcement learning

A more complex variant of Fn11 is to consider multiple randomly placed escorts, which we call Rn1e. The

reinforcement learning method is capable of dealing with Rn1e naturally. Based on the A∗ algorithm, Yalcin

et al. (2019) proposed a heuristic for Rn1e. For each move of the desired item, an escort is selected according

to a certain rule. The chosen escort helps the desired item to achieve the movement. The process repeats until

the retrieval operation is completed. There are three speciﬁc search guiding estimates: manhattan distance,

occupation estimate, and escorts estimate, and we implement their heuristic by using the ﬁrst one. We compare

RL with Yalcin et al. (2019) by conducting R611, R612, and R613 instances.

For R611, RL can successfully solve all 1000 instances, and the diﬀerence with the heuristic of Yalcin et al.

(2019) is neglectable (0.005%) in terms of the objective value. For R612 and R613, 99.9% and 99.5% instances

are successfully solved by RL respectively. The average number of moves provided by RL is approximately

4.0% and 3.5% less than the results of the heuristic. The CPU time of the heuristic is very short for both RL

and the heuristic.

8.3.4. Comparison between Zou & Qi (2021) and reinforcement learning

By designing a series of estimation rules to evaluate the score of each action under the state, Zou & Qi

(2021) proposed a heuristic algorithm for Rnde, which has the identical problem setting of reinforcement

learning. However, for a new instance, the heuristic algorithm must be re-executed, whereas reinforcement

learning can solve it eﬃciently using the experience stored in the neural network. Not to mention that the

rules in Zou & Qi (2021) have some incompleteness, leading to a lower success rate. R621, R622, and R623

instances are used to compare RL with Zou & Qi (2021).

For the results of all the tested instances, it is obvious that RL outperformed the heuristic method in all

three aspects: average number of moves, CPU time, and success rate. We can also observe that the performance

advantage of RL over Zou & Qi (2021) diminishes as escort number increases; for example, a large diﬀerence

(21.483%) can be observed when escort number is equal to one, but the diﬀerence decreases to 6.864% and

4.813% when escort number is 2 and 3, respectively. Future eﬀorts can be devoted to further increasing RL’s

ability to handle instances with more escorts, e.g., increasing the training time when there are more escorts.

26

(a) R422 instance set

(b) R522 instance set

(c) R622 instance set

(d) R722 instance set

(e) R822 instance set

Figure 12: Results of RL on instances with diﬀerent grid size

8.4. Sensitivity analysis on the reinforcement learning approach

Occasionally, managers need to estimate the workload of diﬀerent locations to schedule goods. That is,

given the locations of the desired items and the number of available escorts, predict the number of moves

required to complete the task in a given grid. Therefore, we attempt to analyze the relationship between
(cid:80) ditem manhattan (deﬁned in Section 5.2.2), e (the number of escorts), and the average number of needed moves

under various gird sizes.

We ﬁrst take experiments on Rn22 (n ∈ {4, 5, 6, 7, 8}) instances to observe the eﬀects of grid size. As shown

in Figure 12, a moderate amount of movement is required in most cases for a given grid, with a low incidence

of too little or too much movement. Moreover, the average number of moves seems to show a linear growth as

the grid size increases. The CPU time depends on the average number of moves, which represents the number

of state transitions and neural network calls. Despite this, the CPU time is still short enough (< 0.1 sec).

27

          2 E M H F W L Y H  W R W D O  P R Y H V              1 X P E H U  R I  L Q V W D Q F H V 7 R W D O  6 X F F H V V          $ Y H U D J H  & 3 8  W L P H  V         $ Y H U D J H  2 E M      5             2 E M H F W L Y H  W R W D O  P R Y H V              1 X P E H U  R I  L Q V W D Q F H V 7 R W D O  6 X F F H V V          $ Y H U D J H  & 3 8  W L P H  V         $ Y H U D J H  2 E M       5             2 E M H F W L Y H  W R W D O  P R Y H V              1 X P E H U  R I  L Q V W D Q F H V 7 R W D O  6 X F F H V V          $ Y H U D J H  & 3 8  W L P H  V         $ Y H U D J H  2 E M       5             2 E M H F W L Y H  W R W D O  P R Y H V              1 X P E H U  R I  L Q V W D Q F H V 7 R W D O  6 X F F H V V          $ Y H U D J H  & 3 8  W L P H  V        $ Y H U D J H  2 E M       5             2 E M H F W L Y H  W R W D O  P R Y H V              1 X P E H U  R I  L Q V W D Q F H V 7 R W D O  6 X F F H V V          $ Y H U D J H  & 3 8  W L P H  V         $ Y H U D J H  2 E M       5   (a) The inﬂuence of grid size

(b) The inﬂuence of escort number

Figure 13: The average number of moves with given desired items and grid size

Figure 13(a) presents the statistical results over various instance sets, which concerns only the successful

instances. Notably, for any tested instance set, the average minimum number of moves shows a strong linear
relationship with the (cid:80) ditem manhattan, which makes intuitive sense. Additionally, larger-scale instances often
require more moves on average with the same (cid:80) ditem manhattan due to the increase in the average distance
between the desired items and the escorts. Based on this observation, we introduce the grid size n as a variable

as well. To take sensitivity analysis on the number of escorts, we use the experimental results on R621, R622,

and R623 as summarized in Table 4. As Figure 13(b) shows, the number of moves can be saved when given

more escorts; however, the marginal saving decreases.

With these observations, we propose the following empirical prediction model for the average minimum

number of moves dpred

dpred = α0 + α1

(cid:88)

ditem manhattan + α2 ln(e) + α3n.

(33)

Note that we use a nonlinear (ln) term for escort number e due to its dimishing marginal saving nature. Here
α0, α1, α2, α3 are the coeﬃcients obtained by regression. We take data from R422 ∼ R822 and R621 ∼ R623
for regression, and the results are α0 = −9.517, α1 = 2.351, α2 = −4.142, α3 = 3.445. The R-square R2 =
(cid:80) ditem manhattan,

0.916 conﬁrms a good ﬁtting. The ﬁrst term α0 is a constant term. The second term, α1

provides an approximated number of needed moves to deliver the desired items from their dwell positions to
the corresponding I/O points. Then, α2 ln(e) (α2 < 0) can be interpreted as the number of saved moves due to
the introduction of escorts. The last term, α3n, reﬂects the eﬀect of grid size. Equation (33) can also be used

to obtain a tight K for the IP model: ﬁrst estimate an average number of moves, that is, dpred, then we can

gradually increase the value if the solver returns an infeasible message.

8.5. Performance of decomposition framework (conversion algorithm) on medium and large instances

To demonstrate the eﬀectiveness of the decomposition framework, we execute the approach on medium-

and large-scale cases. We mainly use the RL method to solve the sub-PBS problems; however, for the IP-

28

          ∑ditem_manhattan                  $ Y H U D J H  P R Y H V  U H T X L U H G 5    5    5    5    5                                ∑ditem_manhattan             $ Y H U D J H  P R Y H V  U H T X L U H G 5    5    5   based decomposition, the computational time grows signiﬁcantly as the number of iterations increases, so we

only tried IP on R-6×37-1-22 instances to further evaluate the performance of our method. Here, the escort

number is set to 10% of the total number of cells, as the PBS system is most applicable when the required

density exceeds about 90% (Gue & Kim (2007)). The only comparable approach is Yalcin et al. (2019), whose

method solves single-item retrieval problems with multiple escorts in a heuristic manner that can be applied

directly to large-scale cases. The other three highly relevant pieces of literature are not quite suitable. Gue &

Kim (2007) aims to provide exact solutions for single-item instances with a single escort, and Mirzaei et al.

(2017) focuses on two-load and three-load with a single escort. These two settings may be less practical in

medium- and large-scale warehouse environments. For Zou & Qi (2021), their algorithm can not provide a

feasible solution within 0.5h, which is unacceptable for a heuristic method, so we do not conduct comparative

experiments with them. The results are summarized in Table 5. Note that due to the randomness in the iterative

process of the decomposition algorithm, we take the best value among the results of three runs. And multiple

runs are acceptable because reinforcement learning takes a very short time.

Table 5: Results of decomposition framework on medium- and large-scale instances

Instance series ID

Methodology

Total success

Ave. Obj (lm)

Ave. Obj (sm)

Average CPU time (s)

R-6×37-1-22

R-10×61-1-61

R-6×37-13-22

R-10×61-21-61

Decompositio(IP)

Decompositio(RL)

Yalcin et al. (2019)

Decomposition(RL)

Yalcin et al. (2019)

Decomposition(RL)

Decomposition(RL)

200/200
199/200
200/200
200/200
200/200

100/100
100/100

40.39

40.55

40.60

58.48

58.18

255.54

631.71

-

30.45

-

45.45

-

64.81

91.79

59.95

0.084

0.004

0.105

0.056

0.437

1.250

Ave. Obj (lm): the average number of moves over tested instances under single-load movement assumption

Ave. Obj (sm): the average retrieval time over tested instances under simultaneous movement assumption

According to the results, our approach is comparable with that of Yalcin et al. (2019) for the single-item re-

trieval scheme. For example, the IP-based decomposition, the RL-based decomposition, and Yalcin et al. (2019)

require 40.39, 40.55, and 40.60 moves on average for R-6×37-1-22, respectively, with very subtle diﬀerences.

Meanwhile, the IP-based decomposition algorithm performs slightly better than the RL-based decomposition

one, which is intuitive because reinforcement learning is not guaranteed to yield the optimal solutions. In the

large-scale setting, 58.48 and 58.18 moves are needed on average according to the RL-based decomposition

and Yalcin et al. (2019). Moreover, retrieval eﬃciency improves signiﬁcantly when simultaneous movements

are taken into account. For medium-scale instances, the average retrieval time under the single-load movement

assumption is 3.94 (= 255.54/64.81) times longer than that under the simultaneous movement setting. And

the ﬁgure for large-scale cases is 6.88 (= 631.71/91.79). Additionally, we present the results for medium-

and large-scale multi-item retrieval cases. In conclusion, the decomposition framework performs comparably

to state-of-the-art algorithms in single-item retrieval cases and achieves high eﬃciency in multi-item retrieval

problems.

29

9. Conclusion and future research

In this work, we address the multi-item retrieval problem in the PBS system with general settings. Under

the single-load movement assumption, a RL algorithm (double & dueling DQN) is developed to provide a near-

optimal retrieval path. A semi-random and semi-guided action selection mechanism is proposed to balance

exploration and exploitation and accelerate convergence. Furthermore, we develop a general compact IP model

for the studied problem that achieves a considerable reduction in the number of decision variables and con-

straints when compared to the literature. It is then utilized to evaluate the solution quality of the RL approach.

Following that, a conversion algorithm is proposed to enable the obtained solutions to handle simultaneous

movement considerations. Finally, to solve large-scale instances, we design a decomposition framework, in

which each iteration extracts a sub-area and then solves the subproblem using the RL method.

We generate 10835 instances in the absence of a comparable benchmark, serving as a benchmark dataset for

this and future relevant research. According to the experimental results presented in Section 8.2, the solutions

yielded by the RL method are very close to optimal. For simple instances (R422), RL obtains the optimal

solution over 93.3% of tested instances, and achieves a quite tight average optimality gap of 0.271% over full

dataset. For more diﬃcult instances (R622), the results reveal that the solutions of the RL approach are very

close to those of the IP model as well, and the average gap over four groups of instances are 0.061%, 0.320%,

0.427% and 1.072%, respectively. Moreover, the results reported in Section 8.3 indicate that the RL method

always outperforms the three highly relevant heuristic algorithms after extending the proposed approach to the

corresponding problem variants, not to mention that the RL method can oﬀer speciﬁc retrieval paths within

in a very short computational time. Then, we conduct the sensitivity analysis based on the results of small-

scale benchmark instances and propose a regression model that can be adopted to approximate the minimum

number of moves required for a given condition. In addition, the decomposition framework performs well in

terms of solution quality and processing time for both medium- and large-scale instances. This implies that

our approaches have a great potential to be applied in real manufacturing environments and (semi-)automated

warehouses, providing high-quality and executable retrieval solutions in real-time for companies.

The main limitation of our method is that the optimality gap for medium- and large-scale instances are not

provided, owing to the high complexity of the IP model. In addition, due to the way the state is designed,

the neural networks need to be trained separately for problems with diﬀerent settings. And because of the

value-based property of DQN, the dimension of the action depends on the number of escorts. Therefore, there

are several possible directions for future research. First, eﬀective algorithms can be developed to accelerate

the exact model and to oﬀer tight bounds. Second, apply transfer learning policies to help the input represen-

tation be independent of the numbers of desired items and escorts, or even the grid size. Then, policy-based

RL algorithms can be considered to weaken the relationship between the action and the number of escorts.

Moreover, retrieval problems in three-dimensional warehouses would be a potential challenge that remains to

be investigated.

30

Acknowledgments

This work is supported by the National Natural Science Foundation of China (Grant No. 71772100 and

71971127), and the Guangdong Pearl River Plan (2019QN01X890).

References

Alﬁeri, A., Cantamessa, M., Monchiero, A., & Montagna, F. (2012). Heuristics for puzzle-based storage systems driven by a limited

set of automated guided vehicles. Journal of Intelligent Manufacturing, 23, 1695–1705.

Azadeh, K., De Koster, R., & Roy, D. (2019). Robotized and automated warehouse systems: Review and recent developments.

Transportation Science, 53, 917–945.

Bukchin, Y., & Raviv, T. (2020). Optimal retrieval in puzzle-based storage systems with simultaneous load and block movement.

working paper, .

Govindaiah, S., & Petty, M. D. (2019). Applying reinforcement learning to plan manufacturing material handling part 1: Background

and formal problem speciﬁcation. In Proceedings of the 2019 ACM Southeast Conference ACM SE ’19 (p. 168–171). New York,

NY, USA: Association for Computing Machinery.

Gue, K. R. (2006). Very high density storage systems. IIE Transactions, 38, 79–90.

Gue, K. R., Furmans, K., Seibold, Z., & Uluda˘g, O. (2013). Gridstore: a puzzle-based storage system with decentralized control. IEEE

Transactions on Automation Science and Engineering, 11, 429–438.

Gue, K. R., & Kim, B. S. (2007). Puzzle-based storage systems. Naval Research Logistics, 54, 556–567.

Kool, W., van Hoof, H., & Welling, M. (2019). Attention, learn to solve routing problems! In International Conference on Learning

Representations.

Kota, V. R., Taylor, D., & Gue, K. R. (2015). Retrieval time performance in puzzle-based storage systems. Journal of Manufacturing

Technology Management, .

Li, M. P., Sankaran, P., Kuhl, M. E., Ganguly, A., Kwasinski, A., & Ptucha, R. (2018). Simulation analysis of a deep reinforcement

learning approach for task selection by autonomous material handling vehicles. In 2018 Winter Simulation Conference (WSC) (pp.

1073–1083).

Ma, Y., Chen, H., & Yu, Y. (2021).

An eﬃcient heuristic for minimizing the number of moves for the retrieval of

a single item in a puzzle-based storage system with multiple escorts.

European Journal of Operational Research,

.

URL: https://www.sciencedirect.com/science/article/pii/S0377221721008079. doi:https://doi.org/10.1016/

j.ejor.2021.09.032.

Mirzaei, M., De Koster, R., & Zaerpour, N. (2017). Modelling load retrievals in puzzle-based storage systems. International Journal

of Production Research, 55, 6423–6435.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep

reinforcement learning. arXiv preprint arXiv:1312.5602, .

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K.,

Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D.

(2015). Human-level control through deep reinforcement learning. Nature, 518, 529–533.

Mukhutdinov, D., Filchenkov, A., Shalyto, A., & Vyatkin, V. (2019). Multi-agent deep learning for simultaneous optimization for time

and energy in distributed routing system. Future Generation Computer Systems, 94, 587–600.

Nazari, M., Oroojlooy, A., Tak´aˇc, M., & Snyder, L. V. (2018). Reinforcement learning for solving the vehicle routing problem. In

Proceedings of the 32nd International Conference on Neural Information Processing Systems NIPS’18 (p. 9861–9871). Red Hook,

NY, USA: Curran Associates Inc.

Pan, L., Cai, Q., Fang, Z., Tang, P., & Huang, L. (2019). A deep reinforcement learning framework for rebalancing dockless bike

sharing systems. In Proceedings of the AAAI conference on artiﬁcial intelligence (pp. 1393–1400). volume 33.

31

Rohit, K. V., Taylor, G. D., & Gue, K. R. (2010). Retrieval time performance in puzzle-based storage systems.

In IIE Annual

Conference. Proceedings (p. 1). Institute of Industrial and Systems Engineers (IISE).

Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952, .

Siddique, P., Gue, K., & Usher, J. (2021). Puzzle-based parking. Transportation Research Part C Emerging Technologies, 127.

doi:10.1016/j.trc.2021.103112.

Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.

Tang, X., Qin, Z. T., Zhang, F., Wang, Z., Xu, Z., Ma, Y., Zhu, H., & Ye, J. (2019). A deep value-network based approach for multi-

driver order dispatching. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data

Mining (pp. 1780–1790). ACM.

Taylor, G. D., & Gue, K. R. (2008). The eﬀects of empty storage locations in puzzle-based storage systems. In IIE Annual Conference.

Proceedings (p. 519). Institute of Industrial and Systems Engineers (IISE).

Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double q-learning. In Proceedings of the AAAI

Conference on Artiﬁcial Intelligence. volume 30.

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. In Proceedings of the 28th International Conference on Neural

Information Processing Systems - Volume 2 NIPS’15 (p. 2692–2700). Cambridge, MA, USA: MIT Press.

Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement

learning. In International conference on machine learning (pp. 1995–2003). PMLR.

Watkins, C. J. C. H. (1989). Learning from delayed rewards. Ph.D. thesis King’s College.

Watkins, C. J. C. H., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8, 279–292.

Xu, Z., Li, Z., Guan, Q., Zhang, D., Li, Q., Nan, J., Liu, C., Bian, W., & Ye, J. (2018). Large-scale order dispatch in on-demand

ride-hailing platforms: A learning and planning approach. In Proceedings of the 24th ACM SIGKDD International Conference on

Knowledge Discovery & Data Mining (pp. 905–913). ACM.

Yalcin, A., Koberstein, A., & Schocke, K. O. (2019). An optimal and a heuristic algorithm for the single-item retrieval problem in

puzzle-based storage systems with multiple escorts. International Journal of Production Research, 57, 143–165.

Yu, H., Yu, Y., & De Koster, R. (2016). Optimal algorithms for scheduling multiple simultaneously movable empty cells to retrieve a

load in puzzle-based storage systems. Available at SSRN 3506480, .

Zaerpour, N., Yu, Y., & De Koster, R. (2015). Storing fresh produce for fast retrieval in an automated compact cross-dock system.

Production and Operations Management, 24, 1266–1284.

Zaerpour, N., Yu, Y., & De Koster, R. (2017a). Small is beautiful: A framework for evaluating and optimizing live-cube compact

storage systems. Transportation Science, 51, 34–51.

Zaerpour, N., Yu, Y., & De Koster, R. (2017b). Optimal two-class-based storage in a live-cube compact storage system. IISE Transac-

tions, 49, 653–668.

Zaerpour, N., Yu, Y., & De Koster, R. (2017c). Response time analysis of a live-cube compact storage system with two storage classes.

IISE Transactions, 49, 461–480.

Zou, Y., & Qi, M. (2021).

A heuristic method for load retrievals route programming in puzzle-based storage systems.

arXiv:2102.09274.

32

