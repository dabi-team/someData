DEEP VULMAN: A DEEP REINFORCEMENT
LEARNING-ENABLED CYBER VULNERABILITY MANAGEMENT
FRAMEWORK

2
2
0
2

g
u
A
3

]
I

A
.
s
c
[

1
v
9
6
3
2
0
.
8
0
2
2
:
v
i
X
r
a

Soumyadeep Hore

Industrial and Management Systems Engineering
University of South Florida
Tampa, FL 33620
soumyadeep@usf.edu

Ankit Shah
Industrial and Management Systems Engineering
University of South Florida
Tampa, FL 33620
ankitshah@usf.edu

Nathaniel D. Bastian
Army Cyber Institute
United States Military Academy, West Point
NY 10996
nathaniel.bastian@westpoint.edu

ABSTRACT

Cyber vulnerability management is a critical function of a cybersecurity operations center (CSOC) that
helps protect organizations against cyber-attacks on their computer and network systems. Adversaries
hold an asymmetric advantage over the CSOC, as the number of deﬁciencies in these systems is
increasing at a signiﬁcantly higher rate compared to the expansion rate of the security teams to mitigate
them in a resource-constrained environment. The current approaches are deterministic and one-time
decision-making methods, which do not consider future uncertainties when prioritizing and selecting
vulnerabilities for mitigation. These approaches are also constrained by the sub-optimal distribution
of resources, providing no ﬂexibility to adjust their response to ﬂuctuations in vulnerability arrivals.
We propose a novel framework, Deep VULMAN, consisting of a deep reinforcement learning agent
and an integer programming method to ﬁll this gap in the cyber vulnerability management process.
Our sequential decision-making framework, ﬁrst, determines the near-optimal amount of resources to
be allocated for mitigation under uncertainty for a given system state and then determines the optimal
set of prioritized vulnerability instances for mitigation. Our proposed framework outperforms the
current methods in prioritizing the selection of important organization-speciﬁc vulnerabilities, on
both simulated and real-world vulnerability data, observed over a one-year period.

Keywords Cyber Vulnerability Management · Vulnerability Prioritization · Security Resources Optimization · Deep
Reinforcement Learning · Integer Programming · DRL Cyber Framework

1

1

Introduction

Adversaries are actively looking to exploit unpatched vulnerabilities in the computer and network systems to cause
signiﬁcant damage to public and private organizations. Recently, the United States White House issued a memo urging
organizations to promptly identify and remediate vulnerabilities in their systems, among other recommendations to
bolster cybersecurity against the adversaries WH [2021]. Major challenges faced by the organizations to implement this
recommendation result from a signiﬁcant recent increase in the number of new vulnerabilities that are reported in the

1This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which

this version may no longer be accessible.

 
 
 
 
 
 
Deep VULMAN

National Vulnerability Database NVD [2022], as well as the lack of security personnel (resources) available to mitigate
them. This has resulted in vulnerabilities persisting in the computer and network systems of the organizations for a
long time, thereby creating a signiﬁcant advantage for the adversaries. There exists a critical gap in research needed
to develop resource-constrained approaches for effectively identifying and mitigating important organization-speciﬁc
security vulnerabilities to protect against adversarial exploitation and minimize damage from cyber-attacks.

A typical cyber vulnerability management process starts with the scanning of the software and hardware components of
an organization’s network with a vulnerability scanner (such as Tenable, Qualys, or IBM) to ﬁnd vulnerabilities reported
in the NVD. The generated vulnerability report contains all vulnerability instances found in the network along with their
attributes, which include the common vulnerability exposure (CVE) code, host name, description, and the common
vulnerability scoring system (CVSS) severity rating, among others. The security teams at the cybersecurity operations
centers (CSOCs) then assign resources to mitigate the vulnerability instances based on certain schemes. Examples of
actions taken by security personnel are applying patches (vendor-supplied or CSOC-designed), upgrading software,
disabling services, and adding IP ﬁlters, among others. The current approaches for vulnerability management, which
include methods employed at the CSOCs and proposed in recently published literature, use rule-based mechanisms or
static (one-time) optimization models Farris et al. [2018], Shah et al. [2019], Hore et al. [2022] to prioritize the selection
of vulnerabilities for mitigation, given the number of resources available at a particular time-step (for instance, a week
or a month).

There are many shortcomings in the current approaches. First, the vulnerability selection process does not include
a comprehensive list of factors associated with the host machine and the respective organizational environment to
determine the true priority of a vulnerability instance found in a scan report. For instance, a CSOC security team performs
many functions, which include intrusion detection system (IDS) alert management along with vulnerability management.
An IDS alert log can identify host machines with possible intrusion attempts and integrating this information, along
with other factors such as the CVSS severity score, into prioritizing vulnerability instances found on such machines can
help better protect against potential attacks. Second, recently proposed optimization models have focused on selecting
vulnerability instances from dense reports to maximize their cumulative vulnerability utility or exposure score, given a
limited number of available resources. Such an approach does not result in the selection of all important vulnerabilities
as these mathematical formulations focus on the value of selecting a vulnerability instance based on the time it takes to
patch or mitigate it. These methods will select a larger number of less important vulnerabilities if their mitigation time
is considerably low when compared to an important vulnerability with a signiﬁcantly higher mitigation time. Third,
the current approaches assume a deterministic environment for solving this problem, in which the number and type of
vulnerability arrivals are considered to be known and are uniformly distributed across the time horizon. They do not
take into account the uncertainty in vulnerability arrivals and consider a pre-determined (often, an equal) number of
resources distributed across all the individual decision-making time-steps to prioritize the selection of vulnerabilities for
mitigation.

Cyber vulnerability management is a continuous process aimed at strengthening the security posture of an organization
within an inﬁnite time horizon. This requires sequential decision-making, and to make it robust against the uncertainties
in the process, it is imperative that (i) the number of resources to be allocated at each time-step is optimized and (ii)
the important vulnerabilities are identiﬁed and prioritized for mitigation, given the optimized allocation of resources.
Our research objective is to ﬁll the current gap in the cyber vulnerability management process by proposing a novel
artiﬁcial intelligence (AI) enabled framework, powered by a deep reinforcement learning (DRL) agent and an integer
programming method for effective vulnerability triage and mitigation.

The main contributions of the paper are as follows. First, we developed a novel dynamic cyber vulnerability triage
framework, Deep VULMAN, which is designed to combat the uncertainty in the vulnerability management process
and select the most important vulnerability instances for mitigation from a dense list of vulnerabilities identiﬁed in
the network. Unlike other methods in recent literature, we pose the problem as a sequential decision-making problem
and segregate the vulnerability management process in our proposed framework into two parts: (i) determining the
near-optimal amount of resources required for mitigation, given the observed state of the system and (ii) determining the
optimal set of prioritized vulnerability instances for mitigation which has the maximizing average cumulative attribute
score among all the vulnerability instances. Second, we developed a DRL agent based on a policy gradient approach
that learns to make near-optimal resource allocation decisions under uncertainty in vulnerability arrivals. The agent
continuously interacts with a simulated CSOC operations environment built using real-world vulnerability data and
gets feedback from a novel reward signal engineered from (i) the mitigation of important vulnerabilities and (ii) the
number of resources utilized at each time-step. Third, we formulated and solved a combinatorial mathematical model
with an integer programming method for vulnerability prioritization and selection for mitigation with the allocated
resource decision from the DRL agent. Unlike the recent methods in the literature, we present a unique formulation
that generates an optimal set of prioritized vulnerability instances for mitigation, which has the maximum average
cumulative attribute score among all the vulnerability instances. Fourth, to the best of our knowledge, this study is the

2

Deep VULMAN

ﬁrst to propose a framework that integrates alert information from IDS to vulnerability data to improve the vulnerability
management process at a CSOC. This is a major step toward building a robust defense system against adversaries. Our
experiment results demonstrated that with this added information from the alert logs, through prioritized vulnerability
instances, we were able to ﬁnd machines that had very old or expired versions of software making them easier targets
for the adversaries. Finally, we provided valuable insights obtained using our proposed framework by comparing our
results with recent vulnerability prioritization and selection methods from the literature. Our experimental results using
real-world vulnerability data show that our approach is more efﬁcient and effective in terms of selecting important
organization-speciﬁc vulnerabilities in comparison with the other methods.

The paper is organized as follows. Section 2 presents the related literature. Section 3 presents the proposed Deep
VULMAN framework, which consists of the CSOC operations simulation environment and the AI-enabled decision-
support component that recommends near-optimal decisions for vulnerability management. Section 4 presents the
numerical experiments performed using real-world vulnerability scan data. Section 5 presents the experimental results
and comparisons with recent methods from the literature. Lastly, in Section 6, we provide conclusions.

2 Related Literature

We organized the literature review by dividing the related literature into two topics: (i) vulnerability scoring systems
and triage methods, and (ii) DRL approaches in solving sequential decision-making problems under uncertainty.

2.1 Vulnerability Scoring Systems and Triage Methods

To gauge the severity or threat of a vulnerability, it is important to have a mechanism for scoring the attributes or
impacts of the vulnerability. In 2006, Mell et al. [2006] proposed the common vulnerability scoring system (CVSS) to
provide a base score to quantify the vulnerability severity. Later, in 2007, the same authors proposed CVSS version 2 to
cover the shortcomings of CVSS version 1 by reducing inconsistencies, providing additional granularity, and increasing
the capability to reﬂect a wide variety of vulnerabilities Mell et al. [2007]. The CVSS framework is managed by the
Forum of Incident Response and Security Teams (FIRST), and the latest version of CVSS in use today is version 3.1.
The CVSS metric consists of eight base metrics, three temporal metrics, and four environmental metrics FIR [2020].
However, the computation of environmental metrics is complicated and not well proven Gallon [2010]. The NVD omits
the temporal and environmental metrics and considers only the base metrics when calculating the CVSS severity of
reported vulnerabilities Fruhwirth and Mannisto [2009]. CVSS base metric group is a common choice of application
among most organizations to gauge the severity of the vulnerabilities present in their network. However, anecdotal
and literary evidences suggest that the CVSS base score alone is not sufﬁcient to measure the impact of a vulnerability
in a particular organization due to the absence of organizational context Fruhwirth and Mannisto [2009], Farris et al.
[2018], Holm et al. [2011, 2012]. There have been many contributions from researchers to bridge this gap. Some of
the important contributions are by McQueen et al. [2009], in which the authors proposed two metrics Median Active
Vulnerabilities (MAV) and Vulnerability-Free Days (VFD) based on the report time of the vulnerability and time when
the patch is issued by the vendor Allodi and Massacci [2014]; they considered black-market exploit data to boost the
statistical signiﬁcance of the indication pertaining to the true severity of a vulnerability; Farris et al. [2018] proposed
two performance metrics: Total Vulnerability Exposure (TVE) that scores the density of unmitigated vulnerabilities
per month and Time-to-Vulnerability Remediation (TVR) based on the maximum amount of time (in months) an
organization is willing to tolerate the presence of a certain vulnerability in their system; and Hore et al. [2022] presented
a novel Vulnerability Priority Scoring System (VPSS) that takes into account the context of the vulnerability along with
the CVSS score by considering relevant host machine information (positional signiﬁcance of the host machine, level of
importance of the host machine, and protection level of the host machine).

2.2 Deep Reinforcement Learning (DRL) Approaches

DRL is one of the most promising solution methods for obtaining near-optimal policies under uncertain (stochastic)
conditions. DRL was ﬁrst applied by Mnih et al. in 2013 to successfully learn a control policy from sensory inputs with
high dimensions Mnih et al. [2013]. Today, DRL has been used in various application domains such as autonomous
vehicles, stock trading, robotics, cyber-security, and marketing, among others Bogyrbayeva et al. [2021], Kirtas et al.
[2020], Liang [2020]. The model-free DRL methods in published literature can be broadly classiﬁed in two parts:
value-based and policy-based. In value-based DRL approaches, we try to estimate the Q-value or a state-action pair by
employing a deep neural network estimator. Policy based methods aim to directly learn the stochastic or deterministic
policies, where the action is generated by sampling from the policy. Mnih et. al proposed a novel method, Deep Q
Learning (DQN), which is a value-based method with superior performance demonstrated on Atari 2600 games. Some
of the notable advancements made in the area of value-based DRL methods include the works by: Van hasselt et. al,

3

Deep VULMAN

who proposed DRL with double q-learning (DDQN) to overcome the overestimation suffered by DQN Van Hasselt
et al. [2016]; and Wang et. al, who proposed the dueling network architectures for DRL with two identical but separate
neural network estimators for estimating the state value function and action advantage function Wang et al. [2016],
among others. One of the popular advancements in policy-based methods includes the work by Mnih et. al, who
presented asynchronous methods for DRL with parallel actor learners, asynchronous advantage actor critic (A3C),
and outperformed others on Atari 2600 games Mnih et al. [2016]. Vanilla policy gradient algorithms generally suffer
from high variance, poor sample efﬁciency, and slow convergence. Schulman et al. [2015] presented Trust Region
Policy Optimization (TRPO) that limits the policy update with a certain KL-divergence constraint and also guarantees
monotonic improvement. In 2017, Schulman et al. [2017] proposed Proximal policy Optimization (PPO) that has all the
advantages of TRPO, and in addition, it is simpler, faster, and more sample efﬁcient. PPO uses a clipped surrogate
objective function that prevents large changes in the policy. The clipped surrogate objective is also a lightweight
replacement of the KL-divergence constraint in TRPO. Due to its simplicity, sample efﬁciency, and robustness to
hyper-parameter tuning, PPO is a promising approach to solving dynamic sequential decision-making problems.

There is a clear gap in the literature for cyber vulnerability prioritization and selection, as most of the work has been
focused on formulating one-time (static) strategies for selecting vulnerabilities from dense vulnerability reports by
considering a ﬁxed amount of resource availability and without taking future vulnerability arrivals into account. To
the best of our knowledge, no research has addressed the vulnerability management problem as a sequential decision-
making problem under the uncertainty of vulnerability arrivals and with resource ﬂuctuations. This paper focuses on
strengthening the security posture of the CSOCs by generating robust vulnerability management policies for real-world
uncertain environments. Next, we present the proposed framework for dynamic vulnerability management under
uncertainty.

3 Deep Reinforcement Learning-enabled Cyber Vulnerability Management (Deep

VULMAN) Framework

Figure 1: Deep VULMAN Framework for Cyber Vulnerability Management.

4

Deep VULMAN

We propose the development of a sequential decision-making framework that provides a dynamic resource allocation
strategy along with an optimal selection of vulnerabilities that are prioritized for mitigation. Figure 1 shows a schematic
representation of the proposed Deep VULMAN framework. The framework consists of two key components: (i) a
CSOC operations environment, where relevant computer and network data are collected and aggregated using various
software applications, and (ii) a decision-support component, in which (a) a DRL agent is trained using a policy gradient
algorithm to make near-optimal resource allocation decisions under uncertainty and (b) an integer programming model
is developed to generate the set of vulnerabilities, which are prioritized for mitigation with the amount of resources
allocated by the DRL agent. We ﬁrst describe the CSOC operations environment, in which we propose a simulator to
overcome the data insufﬁciency issues for training the DRL agent, followed by the decision-support component.

3.1 CSOC Operations Simulation

Obtaining a real and large data set for a research study is a major challenge for cybersecurity researchers. Very few
studies in published literature, such as Farris et al. [2016], Xu et al. [2018], and Shah et al. [2019], have investigated
the process of cyber-incident or vulnerability emergence using historical data. However, these have been small and/or
private data sets. The unavailability of data sets is due to a lack of complete information in a cyber environment
or conﬁdentiality reasons. Cyber-incident data have been studied in Haldar and Mishra [2017] and Kuypers and
Paté-Cornell [2016] for large cyber breaches and it has been found that a Poisson distribution provides the best ﬁt
for describing the arrivals in these data sets. It is imperative that the DRL agent interacts with an environment that
closely resembles the real CSOC operations to learn the best policies that can be implemented in real-world conditions.
Hence, to overcome the challenges, such as having insufﬁcient data to properly train a DRL agent or learning in a
slow-moving real-world environment Dulac-Arnold et al. [2019], we built a simulator from the large amount of real data
that we collected by working with a CSOC. We developed an agent-based discrete event simulation (DES) algorithm
with ﬁxed-increment time progression to model the vulnerability management process at a CSOC. The agent-based
approach is added to the traditional DES to accommodate the interaction between the DRL-agent (explained in the next
section) and the simulation environment. The inputs to the algorithm are the various vulnerability scan reports and other
relevant network related information obtained from applications such as Nessus, Lansweeper, and IDS. The uncertainty
in the vulnerability arrival process is captured in the simulator by randomly generating vulnerability arrival patterns
at each time-step. For example, there could be a high, medium, or a low number of vulnerability arrivals in a given
week. Vulnerability instances with varying characteristics and related host machine data are randomly sampled from
the historical data sets at each time-step. The arrivals are generated using a Poisson distribution with varying mean,
which can be obtained from the historical data.

The cyber vulnerability instances, the respective host machine information, and the resources available are then passed
on to the decision-support component (see Figure 1) as the state of the system at the given time-step. The action
pertaining to this system state is then taken as input by the simulated environment from the decision-support component.
The simulation algorithm executes this action, which contains the set of vulnerabilities selected for mitigation. A
scalar reward is computed in the simulator, which consists of two terms, one related to the mitigation of important
vulnerabilities and another for the number of resources utilized. The details of the action selection and the reward
function are presented in the next section. The cumulative time taken to mitigate the selected vulnerabilities is then
deducted from the total available time at the beginning of the time-step. The environment is then stepped forward
to the next time-step with the remaining resources. A new set of vulnerability instances is then generated, and this
process continues for the entire episode (e.g., a month). The simulator adds the new set of vulnerabilities (arrivals) to
the unmitigated set of vulnerabilities from the previous time-step.

3.2 Decision Support for Vulnerability Management

The objective of this research is to identify and prioritize important cyber vulnerabilities for mitigation under uncertainty
of future vulnerability arrivals in a resource-constrained system. It is to be noted that the decision-making problem
can be broken down into obtaining two decisions: (i) determining the near-optimal resources to be allocated and (ii)
determining the set of vulnerability instances for mitigation given these resources, which reduces the vulnerability
exposure of the organization in the long run. The former decision of allocating the appropriate amount of resources
is affected by the uncertainty in the environment and the CSOC can enhance their security with a dynamic resource
allocation strategy. Once the decision on the amount of resources allocated is made, the mathematical model can be
invoked to optimally select the set of important vulnerabilities for mitigation. We ﬁrst describe the DRL problem
formulation for optimizing the resource allocation strategy, followed by the formulation of the mathematical model,
which outputs the vulnerability selection decision.

5

Deep VULMAN

3.2.1 DRL Formulation

The problem of making sequential decisions for resource allocation to mitigate important vulnerabilities and thereby
reducing the vulnerability exposure and strengthening the security posture of an organization in the long run can be
formulated as a Markov decision process (MDP). The key elements of the MDP formulation are as follows:

• State, st, represents the information that is visible to the agent at time t, which consists of the vulnerability
instances, their respective attributes, and the total amount of resources available. The state space is N ∗ (M + 1)
dimensional, where M is the number of attributes and N is the maximum number of vulnerabilities historically
found in the vulnerability scan reports. We use the concept of zero padding to ﬁll empty rows (N - J number
of vulnerabilities found at each scan) with zeros Lin et al. [2020]. The state space provides the DRL agent
with the information needed to make the resource allocation decision for vulnerability selection.

• Action, at, represents the control. The action is the amount of resources to be allocated at time t, given a state,

st. The action space is continuous for this problem.

• State transition function determines the probability with which a system will transition from state st to st+1
under action at. The state transition probabilities for this problem are unknown and the possible number of
state transitions are very high (state space explosion). Hence, it is infeasible to determine the state transition
probabilities.

• Reward, rt, is a measure of the goodness of an action, at, taken in a given state, st, at time t. The agent’s
goal is to maximize the long-term cumulative reward. Hence, setting up the reward signal is critical to train the
agent to achieve the research objective. In this research, we engineer a novel reward function, which consists
of two weighted terms. The reward is obtained from: (i) the mitigation of important vulnerabilities (r1) and
(ii) the number of resources utilized (r2). The reward function, at time t, is given by Equation 1 where w1 and
w2 are weights associated with the reward terms and whose sum must be equal to 1.

rt = w1 ∗ r1

t + w2 ∗ r2
t

(1)

The importance of a vulnerability instance is determined by taking into consideration the following attributes:
asset criticality, level of protection, and organizational relevance of the host machine, the CVSS severity of
the vulnerability instance, and if the host machine has been identiﬁed in any IDS alerts. These attributes
are obtained using various applications from the organization’s computer and network systems. Categorical
attributes are transformed into numerical values based on certain rules from literature. We use the same
scheme, as in Hore et al. [2022], Shah et al. [2018], Farris et al. [2018], to identify various categories for
each attribute and assign normalized numerical values. For completeness, we describe this scheme here. The
following attributes: asset criticality, level of protection, and organizational relevance of the host machine
are assigned either of the three categories, high, medium, or low. It is to be noted that more categories could
be added to this list such as a critical priority category. The categorical attribute with the highest priority is
assigned a numerical value of 1 and the lowest is assigned a value of 0.1. The ordered categories in between
the highest and lowest priorities are then assigned numerical values based on a linear scale. For instance, if
the asset criticality associated with a certain machine is of the highest priority (critical), then it takes a value
of 1 and if it is the lowest priority (low), then it is assigned a value of 0.1. The CVSS severity score for a
vulnerability is obtained from the NVD through the application, which is then normalized between 0 and 1. For
instance, NESSUS provides this score as a part of the scan report, which ranges from 1 to 10. If the machine is
identiﬁed in the IDS alert logs for a possible intrusion, then the attribute is assigned a value of 1, else 0. All
these factors are considered equally important. There is a positive reward for selecting vulnerabilities, which is
calculated by taking the average of all the attribute values of the selected vulnerabilities. If there are J number
of selected vulnerabilities and vij represents the value of attribute i of the vulnerability instance j, then the
positive reward can be calculated as r1
. Since the CSOC operations environment is resource
constrained, there exists a trade-off between the number of vulnerabilities selected for mitigation and the
number of resources that remain available for vulnerability selection in the next time-step. Hence, we assign a
small cost to the utilization of the resources in this formulation. For the J number of vulnerability instances
selected for mitigation with Sj representing the time required to mitigate vulnerability j and C representing
the cost/unit resource utilized, then the resource utilization penalty (r2

t ) is calculated as − (cid:80)J

j=1 C ∗ Sj.

i=1 vij

t =

I∗J

(cid:80)J

(cid:80)I

j=1

The large state space and continuous action space make this problem infeasible to solve using conventional reinforcement
learning approaches. To overcome the issue of not being able to calculate and store the action-value (or Q value) for all
possible state-action pairs due to state space explosion, we propose a deep neural network-based learning model with a
policy gradient algorithm for efﬁciently solving this problem Silver et al. [2014]. Vanilla policy gradient algorithms

6

Deep VULMAN

have disadvantages such as poor data efﬁciency, lack of robustness, and are often subjected to large changes in policies
resulting in unstable learning. Hence, we propose the proximal policy optimization (PPO) approach Schulman et al.
[2017] for solving this problem, which is an on-policy algorithm that overcomes the aforementioned challenges. PPO
ensures smoother learning of the policies with the objective clipping feature. Additionally, PPO is easy to implement
and tune, and provides better sample efﬁciency.

3.2.2 Vulnerability Prioritization and Selection Model

The prioritization and selection of cyber vulnerability instances is achieved by solving a mathematical model, whose
solution provides us with the set of prioritized vulnerabilities selected for mitigation by the available resources (decision
made by the DRL agent). The vulnerability selection problem is posed as a combinatorial optimization problem and
solved using integer programming. The static vulnerability prioritization and selection models in Farris et al. [2018],
Shah et al. [2019], Hore et al. [2022] directly maximize the cumulative utility or exposure scores of their respective
factors to obtain the sets of prioritized vulnerability instances. Such a set of vulnerabilities may not contain all the
important vulnerabilities, as their formulations do not maximize the average value of the selected vulnerabilities. In our
proposed formulation, we counter this issue by maximizing the average of the cumulative value of all the attributes
across all selected vulnerability instances subject to the total time available for mitigation in any given time-period.
In addition, we take into consideration the largest set of attributes associated with any vulnerability and its respective
host machine in published literature. Below, we present the input parameters, decision variables, objective function,
constraints, and the output of the vulnerability selection model.

Input parameters:

• The attribute scores for all vulnerability instances, vij ∀i, j.

• Expected time taken to mitigate a vulnerability instance j, Sj.

• Total number of vulnerability instances in the scan report, J.

• Total resources available at time t (action from the DRL agent), at.

Decision variables:

• zj = 1 if vulnerability instance j is selected, and 0 otherwise.

Objective function: The objective of the model is to select the set of vulnerability instances prioritized for mitigation
that maximizes the average of the cumulative value of the attribute scores across all selected vulnerability instances.
The objective function is given by:

y = M ax

(cid:80)J

j=1

(cid:80)I

i=1 vij ∗ zj
j=1 zj

(cid:80)J

(2)

Constraint: The constraint for the model is the availability of resource time, at, at any given time t, which is obtained
from the DRL agent. The constraint for the total time taken to mitigate the selected vulnerability instances not being
higher than the total resource time available at time t is expressed as:

J
(cid:88)

j=1

Sj ∗ zj ≤ at

(3)

Output: The output of the vulnerability prioritization and selection model is the set of prioritized vulnerability instances
selected for mitigation.

4 Numerical Experiments

We worked closely with a CSOC to collect the vulnerability data and other relevant computer network information. Our
conversations with the security analysts helped us determine various parameter values that were used in setting up the
environment, and training and testing the proposed Deep VULMAN framework.

7

Deep VULMAN

4.1 Data Collection and Simulation Environment for CSOC Operations

We developed a simulator from the real-world data set that we collected by working with a CSOC. We used two
applications: Tenable’s Nessus vulnerability scanner and Lansweeper to collect the vulnerability data. We collected
a total of 98,842 vulnerability instances over a span of two years. We also collected relevant host machine data and
alert data generated by the IDS. The Lansweeper report contained information about the host machines in the network,
which included the software versions of the operating system and SQL server, among others. In this research study,
we also integrated information from the IDS alert logs to obtain the intrusion status of the host machine. If a host
machine with the reported vulnerability was identiﬁed in the IDS alert log for the respective time-period (say, between
time t − 1 and t), then this information was recorded and the intrusion status attribute of a vulnerability instance was
set accordingly. All the machine-speciﬁc information for the host machines on which the vulnerabilities were found
was added to the consolidated data set. The aggregated data set contained information about the host machine and
vulnerability instances such as the host IP, CVE code, the description, the CVSS severity score, the importance of
the machine in the network, the versions of software running on the host machine, and the estimated personnel-hours
required to mitigate the vulnerability instance Farris et al. [2018], among other known information. We then applied
vulnerability data preprocessing techniques, which included quantiﬁcation of the attributes: asset criticality, level of
protection, and organizational relevance of the host machine, along with the CVSS severity of the vulnerability. We
used the same categories and the quantiﬁcation process as used in Farris et al. [2018] and Hore et al. [2022].

We created an agent-based DES that mimics the arrival and mitigation process of vulnerability instances in a CSOC.
With the help of a simulation model, we generated diverse patterns of new vulnerability arrivals to expose the DRL agent
to uncertainty it may ﬁnd in a real-world environment. From our discussions with the CSOC security personnel and
historical evidence, along with the information published in literature Farris et al. [2018], we modeled the vulnerability
instance arrival process using a Poisson distribution and varied the average number of arrivals from 40 to 600 per week
(indicating a very large network). We segregated our arrivals into three different categories, namely, high, medium, and
low. Different patterns of arrivals, based on the aforementioned average numbers per week, were simulated for training
the DRL agent. Some examples of arrival patterns for four consecutive weeks in a month include [high, high, low, low],
[medium, medium, medium, low], and [low, high, medium, high], among others. Vulnerability instances were randomly
sampled from the data set based on the arrival pattern (Poisson distribution with the respective average number of
arrivals) at each time-step (i.e., t = 1 week) emulating the arrival process in the CSOC. All the information about the
vulnerability instances is then passed to the decision-support component (explained in the next sub-section) to obtain an
action indicating the set of vulnerability instances that are selected for mitigation. Upon receiving this information,
the simulation algorithm is stepped forward and the selected vulnerability instances are mitigated utilizing the time
assigned to each of the vulnerability instances in the consolidated data set. The total mitigation time of the selected
vulnerability instances is then deducted from the available resource time from the previous time-step and the remaining
resource time is carried forward to the next time-step. Each week is represented as a time-step in the simulator. Next,
we describe the training and testing phases of the proposed Deep VULMAN framework.

4.2 Training Phase

We conducted our experiments with some of the hyper-parameter values from published literature Schulman et al.
[2017] and Carvalho Melo and Omena Albuquerque Máximo [2019], and tuned the remaining by trial-and-error, which
involved running experiments with different sets of values. The PPO approach is known to be more forgiving to
sub-optimal initialization of hyper-parameter values. We conducted the experiments on a machine with 11th Gen Intel
Core i7-12700H processor with NVIDIA GeForce RTX 2080 graphics card (16GB RAM).

We used a multi-layer perceptron (MLP) model with two hidden layers, each containing 68 perceptrons and Tanh
activation functions for the actor and critic networks. It is to be noted that we implemented various architectures with a
larger number of hidden layers and perceptrons but did not ﬁnd any signiﬁcant improvements in the performance of the
DRL agent and selected the two hidden layer model that was computationally efﬁcient among the others. The DRL
agent took actions using the policy network. There was some standard deviation added to these actions, which started
with a value of 0.65 and decayed to 0.01 with a rate of 0.025. The decay rate and decay frequency are problem-speciﬁc,
and hence we had to tune it with a trial-and-error approach. To avoid getting stuck in a local optimum and encourage
exploration, we used 0.01 as the entropy co-efﬁcient value, which was multiplied by the entropy and subtracted from
the loss function. The value of the entropy factor was adopted from the literature Schulman et al. [2017]. We set the
maximum number of time-steps for training to 200M. At each time-step, the output of the DRL agent is provided as an
input to the vulnerability prioritization and selection mathematical model and the vulnerability instances are selected
for mitigation, which are then passed on to the CSOC operations environment. Based on these actions, a scalar reward
value is calculated, which is derived from the two terms in the reward function (as shown in Equation 1). We considered

8

Deep VULMAN

equal values of the weights used for the two reward terms in the reward function (in Equation 1) and assigned a value of
10−5 to the cost per unit resource utilized (C).

4.3 Testing Phase

We evaluated the DRL-enabled Deep VULMAN framework with the real-world vulnerability data from the collaborating
CSOC. We set the standard deviation to zero during the testing phase, to avoid any further exploration by the DRL
agent when taking actions using the policy network. We compared our method with two recent vulnerability selection
methods from published literature, namely, VPSS Hore et al. [2022] and VULCON Farris et al. [2018]. We did not
consider the CVSS-value based selection method in our comparison due to its limitation in taking the context of an
organization into consideration. To compare the three approaches, we recorded the vulnerabilities that were selected
for mitigation from (a) high value assets, (b) machines with lower level of protection, (c) organizationally relevant
machines (i.e., web and database servers), and (d) machines with intrusion detection alert signals. Next, we describe
and analyze the results obtained from the aforementioned experiments.

5 Analysis of Results

Figure 2: Comparison of the total number of vulnerabilities selected from real-world data (one year) from (a) high value
assets, (b) machines with low level of protection, (c) organization-speciﬁc relevant machines, and (d) machines with
intrusion alert signals.

In this section, we present the evaluation results obtained using the real-world CSOC data. We evaluated the performance
of our approach on the real-world vulnerability data set, which was collected from a collaborating CSOC for a period of
one year. As shown in, Figure 2(a) and Figure 2(c), with our proposed approach more vulnerabilities are prioritized
for mitigation from the important machines, i.e., web and database servers. We had observed similar results on
the previously unseen simulated data. These results matched the requirements we had gathered from the security
personnel at the CSOC. Figure 2(d) shows another interesting result obtained using our method is the prioritization of
vulnerabilities that were found on machines identiﬁed in potential attacks using the IDS alert data. Further investigation
of these machines also revealed that the majority of these machines were identiﬁed to have a lower level of protection
(old software versions with no or limited support from the vendor) Figure 2(b), which indicates that they were an easier

9

Deep VULMAN

target for adversaries and vulnerabilities found in them must be prioritized. The results point towards a high degree of
robustness an organization can achieve by employing the proposed DRL-enabled Deep VULMAN framework in the
vulnerability triage process.

(cid:80)I

i=1 vi,j
I

Figure 3 shows a particular episode (month), in which the vulnerability arrival pattern ﬂuctuates between high, medium
and low among the four time-steps (weeks). The orange bar shows the total expected mitigation time required (in
minutes) to mitigate all the vulnerabilities identiﬁed in the network and the blue bar shows the amount of resources
allocated by the DRL agent. We have highlighted the expected mitigation time of vulnerabilities, whose average for the
cumulative normalized attribute values is high, in red. In particular, we have considered the vulnerability instances with
≥ 0.75 to show the effectiveness of our proposed approach in allocating resources to mitigate
the value of
these critical vulnerabilities. The dotted line in the ﬁgure represents the even distribution of resources, which is a
commonly employed practice at the CSOCs and is utilized by the other two methods (VPSS and VULCON). It can
be seen that the DRL agent allocates a lower than average number of resources in the ﬁrst week to match the arrival
pattern of vulnerabilities, followed by a lower than average number of resources in the second week, thereby saving
more resources in anticipation of a larger number of new vulnerability arrivals in the last two weeks of the month. In
this case, the DRL agent demonstrates that it is able to react appropriately in the ﬁrst two time-steps by allocating less
number of resources and has learned non-trivial decisions of utilizing conserved resources to counter an anticipated
future event (of high arrivals). Accordingly, the prioritization and selection model is able to prioritize the selection of
vulnerabilities across all the factors. This episode example amongst many others shows that the DRL agent has learned
to make better decisions in the wake of uncertain vulnerability arrivals.

Figure 3: Comparison between expected mitigation time of critical vulnerabilities and mitigation time allocated by the
DRL agent.

6 Conclusions

The paper presented a novel cyber vulnerability management framework, Deep VULMAN, to identify and prioritize
important vulnerabilities for mitigation in the wake of uncertain vulnerability arrivals in a resource-constrained
environment. We ﬁrst trained a state-of-the-art DRL agent using a simulated CSOC operations environment, which was
built using real-world CSOC data, to learn the near-optimal policy of allocating resources for selecting vulnerabilities
for mitigation. Next, a mathematical model for vulnerability prioritization and selection was formulated and solved
using the integer programming method, which generated the set of important vulnerabilities prioritized for mitigation
based on the resources allocated. We conducted our experiments on both simulated and real-world vulnerability data for
a one-year period. The results showed that our proposed framework outperformed the current methods by prioritizing
the selection of maximum number of vulnerability instances from high-value assets, organizationally relevant machines
(web and database servers), machines identiﬁed in intrusion detection alert signals, and machines with lower level
of protection. The DRL agent learned non-trivial decisions in the wake of uncertain vulnerability arrival patterns.
For instance, the agent was able to anticipate future events of high vulnerability arrivals, and accordingly adjusted
(conserved) the allocation of resources in earlier time-steps to counter the important vulnerabilities during those events.

10

Deep VULMAN

We ﬁrst trained a state-of-the-art DRL agent using a simulated CSOC operations environment, which was built using
real-world CSOC data, to learn the near-optimal policy of allocating resources for selecting vulnerabilities for mitigation.
Next, a mathematical model for vulnerability prioritization and selection was formulated and solved using the integer
programming method to obtain the prioritized set of important vulnerabilities selected for mitigation. We conducted
our experiments on both simulated and real-world vulnerability data for a one-year period. The results showed that
our proposed framework outperformed the current methods by prioritizing the selection of the maximum number of
vulnerability instances from high-value assets, organizationally relevant machines (web and database servers), machines
identiﬁed in intrusion detection alert signals, and machines with lower level of protection. The DRL agent learned
non-trivial decisions in the wake of uncertain vulnerability arrival patterns. For instance, the agent was able to anticipate
future events of high vulnerability arrivals, and accordingly adjusted (conserved) the allocation of resources in earlier
time-steps to counter the important vulnerabilities during those events.

The proposed DRL-enabled cyber vulnerability management framework, Deep VULMAN, can strengthen the security
posture of an organization by generating robust policies in uncertain and resource-constrained real-world environments.
In this study, we also determined the optimal allocation of limited number of resources that are available in a CSOC,
across different time-steps under uncertainty. An interesting follow-up work or a future research direction can include
the development of data-driven models to determine an optimal number of security personnel needed to achieve the
performance goal of a vulnerability management team. Furthermore, a trade-off study can be conducted comparing the
impact of budget on stafﬁng and performance of the vulnerability management teams.

Acknowledgments

This work was supported in part by the U.S. Military Academy (USMA) under Cooperative Agreement No. W911NF-
22-2-0045, the U.S. Army Combat Capabilities Development Command (DEVCOM) Army Research Laboratory under
Support Agreement No. USMA21050, and the U.S. Army DEVCOM C5ISR Center under Support Agreement No.
USMA21056. The views and conclusions expressed in this paper are those of the authors and do not reﬂect the ofﬁcial
policy or position of the U.S. Military Academy, U.S. Army, U.S. Department of Defense, or U.S. Government.

References

Executive Order on Improving the Nation’s Cybersecurity (Presidential Actions, May 12, 2021). https://www.

whitehouse.gov/briefing-room/presidential-actions, 2021. [Online; accessed 1-May-2022].
National Vulnerability Database. https://nvd.nist.gov/vuln, 2022. [Online; accessed 1-May-2022].
Katheryn A Farris, Ankit Shah, George Cybenko, Rajesh Ganesan, and Sushil Jajodia. Vulcon: A system for
vulnerability prioritization, mitigation, and management. ACM Transactions on Privacy and Security (TOPS), 21(4):
1–28, 2018.

Ankit Shah, Katheryn A Farris, Rajesh Ganesan, and Sushil Jajodia. Vulnerability selection for remediation: An

empirical analysis. The Journal of Defense Modeling and Simulation, page 1548512919874129, 2019.

Soumyadeep Hore, Fariha Moomtaheen, Ankit Shah, and Xinming Ou. Towards optimal triage and mitigation of

context-sensitive cyber vulnerabilities. IEEE Transactions on Dependable and Secure Computing, 2022.

Peter Mell, Karen Scarfone, and Sasha Romanosky. Common vulnerability scoring system. IEEE Security & Privacy, 4

(6):85–89, 2006.

Peter Mell, Karen Scarfone, Sasha Romanosky, et al. A complete guide to the common vulnerability scoring system

version 2.0. In Published by FIRST-forum of incident response and security teams, volume 1, page 23, 2007.

Common Vulnerability Scoring System version 3.1: Speciﬁcation Document. https://www.first.org/cvss/

specification-document, 2020. [Online; accessed 18-May-2022].

Laurent Gallon. On the impact of environmental metrics on cvss scores. In 2010 IEEE Second International Conference

on Social Computing, pages 987–992. IEEE, 2010.

Christian Fruhwirth and Tomi Mannisto. Improving cvss-based vulnerability prioritization and response with context
information. In 2009 3rd International symposium on empirical software engineering and measurement, pages
535–544. IEEE, 2009.

Hannes Holm, Teodor Sommestad, Jonas Almroth, and Mats Persson. A quantitative evaluation of vulnerability

scanning. Information Management & Computer Security, 2011.

Hannes Holm, Mathias Ekstedt, and Dennis Andersson. Empirical analysis of system-level vulnerability metrics

through actual attacks. IEEE Transactions on dependable and secure computing, 9(6):825–837, 2012.

11

Deep VULMAN

Miles A McQueen, Trevor A McQueen, Wayne F Boyer, and May R Chafﬁn. Empirical estimates and observations of
0day vulnerabilities. In 2009 42nd Hawaii international conference on system sciences, pages 1–12. IEEE, 2009.
Luca Allodi and Fabio Massacci. Comparing vulnerability severity and exploits using case-control studies. ACM

Transactions on Information and System Security (TISSEC), 17(1):1–20, 2014.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin

Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

Aigerim Bogyrbayeva, Sungwook Jang, Ankit Shah, Young Jae Jang, and Changhyun Kwon. A reinforcement learning
approach for rebalancing electric vehicle sharing systems. IEEE Transactions on Intelligent Transportation Systems,
2021.

M Kirtas, Konstantinos Tsampazis, Nikolaos Passalis, and Anastasios Tefas. Deepbots: A webots-based deep reinforce-
ment learning framework for robotics. In IFIP International Conference on Artiﬁcial Intelligence Applications and
Innovations, pages 64–75. Springer, 2020.

Haiqing Liang. A precision advertising strategy based on deep reinforcement learning. Ingénierie des Systèmes

d’Information, 25(3), 2020.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings

of the AAAI conference on artiﬁcial intelligence, volume 30, 2016.

Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures
for deep reinforcement learning. In International conference on machine learning, pages 1995–2003. PMLR, 2016.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,
and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on
machine learning, pages 1928–1937. PMLR, 2016.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization.

In International conference on machine learning, pages 1889–1897. PMLR, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347, 2017.

Katheryn A Farris, Sean R McNamara, Adam Goldstein, and George Cybenko. A preliminary analysis of quantifying
computer security vulnerability data in" the wild". In Sensors, and Command, Control, Communications, and
Intelligence (C3I) Technologies for Homeland Security, Defense, and Law Enforcement Applications XV, volume
9825, page 98250T. International Society for Optics and Photonics, 2016.

Maochao Xu, Kristin M Schweitzer, Raymond M Bateman, and Shouhuai Xu. Modeling and predicting cyber hacking

breaches. IEEE Transactions on Information Forensics and Security, 13(11):2856–2871, 2018.

Kaushik Haldar and Bimal Kumar Mishra. Mathematical model on vulnerability characterization and its impact on
network epidemics. International Journal of System Assurance Engineering and Management, 8(2):378–392, 2017.
security incidents.
stanford.

and Elisabeth Paté-Cornell.
Security

Marshall Kuypers

Department of

Stanford, CA.

energy cyber

International

https://cisac.

fsi.

for

Center
and Cooperation,
edu/sites/default/ﬁles/doe_cyber_security_incidents. pdf, 2016.

Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv

preprint arXiv:1904.12901, 2019.

Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement learning for

deformable object manipulation. arXiv preprint arXiv:2011.07215, 2020.

Ankit Shah, Rajesh Ganesan, Sushil Jajodia, and Hasan Cam. A two-step approach to optimal selection of alerts for

investigation in a csoc. IEEE Transactions on Information Forensics and Security, 14(7):1857–1870, 2018.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy

gradient algorithms. In International conference on machine learning, pages 387–395. PMLR, 2014.

Luckeciano Carvalho Melo and Marcos Ricardo Omena Albuquerque Máximo. Learning humanoid robot running
skills through proximal policy optimization. In 2019 Latin American Robotics Symposium (LARS), 2019 Brazil-
ian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE), pages 37–42, 2019.
doi:10.1109/LARS-SBR-WRE48964.2019.00015.

12

