Deep Neural Programs for
Adaptive Control in Cyber-Physical Systems

K. Selyunin, D. Ratasich, E. Bartocci, and R. Grosu
Vienna University of Technlogy
konstantin.selyunin,denise.ratasich,ezio.bartocci,radu.grosu@tuwien.ac.at

5
1
0
2

b
e
F
3
1

]
I

A
.
s
c
[

1
v
3
1
0
4
0
.
2
0
5
1
:
v
i
X
r
a

ABSTRACT
We introduce Deep Neural Programs (DNP), a novel pro-
gramming paradigm for writing adaptive controllers for cy-
ber-physical systems (CPS). DNP replace if and while state-
ments, whose discontinuity is responsible for undecidability
in CPS analysis, intractability in CPS design, and frailness
in CPS implementation, with their smooth, neural nif and
nwhile counterparts. This not only makes CPS analysis de-
cidable and CPS design tractable, but also allows to write
robust and adaptive CPS code. In DNP the connection be-
tween the sigmoidal guards of the nif and nwhile statements
has to be given as a Gaussian Bayesian network, which re-
ﬂects the partial knowledge, the CPS program has about its
environment. To the best of our knowledge, DNP are the
ﬁrst approach linking neural networks to programs, in a way
that makes explicit the meaning of the network. In order to
prove and validate the usefulness of DNP, we use them to
write and learn an adaptive CPS controller for the parallel
parking of the Pioneer rovers available in our CPS lab.

Keywords
Cyber-Physical Systems, Adaptive Control, Car Parking,
Deep Neural Programs, Gaussian Bayesian Networks.

INTRODUCTION

1.
Recent advances in sensing, actuation, communication, and
computation technologies, as well as innovations in their
integration within increasingly smaller, interconnected de-
vices, has lead to the emergence of a new and fascinating
class of systems, the so-called cyber-physical systems (CPS).
Examples of CPS include smart grids, smart factories, smart
transportation, and smart health-care [4].

Similarly to living organisms, CPS operate in an uncertain,
continuously evolving ecosystem, where they compete for
a limited supply of resources. For survival, CPS need to
continuously adapt, such that, they react in real time and
optimal fashion, with regard to an overall survival metric,

their partial knowledge, and their bounded sensing, actua-
tion, communication and computation capabilities.

In order to equip CPS with such exceptional features, vari-
ous researchers have started to wonder weather our current
CPS analysis, design and implementation techniques are still
adequate. Going back to Parnas, Chaudhuri and Lezama
identiﬁed in a series of intriguing papers [6,7,25], the if-then-
else construct as the main culprit for program frailness. In
a simple decision of the form if (x > a), the predicate x > a
acts like a step function (see Figure 1), with inﬁnite plateaus
to the left and to the right of the discontinuity point x = a.
In a typical mid-size program, the nesting of thousands of
if-then-else conditions leads to a highly nonlinear program,
consisting of a large number of plateaus separated by dis-
continuous jumps. This has important implications.

From a CPS-analysis point of view, predicates of the form
f (x) > a, where f (x) is a nonlinear analytic function, are
a disaster. They render CPS analysis undecidable.
Intu-
itively, in order to separate all points on one side of the curve
f (x) = a, from all on the other side, one needs to forever de-
crease the size of a grid, in all the rectangles that are crossed
by the curve. Such a process does never terminate, except
for linear functions where computation is still prohibitive.
For this reason, a series of papers, of Fraenzle, Ratschan,
Wang, Gao and Clarke [11, 12, 27, 32], proposed the use of
an indiﬀerence region δ (see Figure 1), and rewrite the predi-
cates in the form f (x)−a > δ. This approach not only makes
program analysis (wrt. reals) decidable, and computable in
polynomial time, but it also aligns it with the ﬁnite compu-
tational precision available in today’s computers.

From a CPS-design point of view, where one is interested to
ﬁnd the values of a for which an optimization criterion is sat-
isﬁed, predicates of the form f (x) > a are a nightmare. They
render CPS optimization intractable. Intuitively, a gradient-
descent method searching for a local minimum, gets stuck
in plateaus, where a small perturbation to the left or to the
right, still keeps the search on the same plateau. In order
to alleviate this problem, Chaudhuri and Lezama [6] pro-
posed to smoothen the steps, by passing a Gaussian input
distribution through the CPS. This can be thought of as cor-
responding to the sensing and actuation noise. The parame-
ters of this distribution control the position of the resulting
sigmoidal curve (see Figure 1), and its steepness, that is,
the width of the above indiﬀerence region δ. The authors
however, stopped short of proposing a new programming

 
 
 
 
 
 
paradigm, and the step-like functions in the programs to be
optimized, posed considerable challenges in the analysis, as
they cut the Gaussians in very undesirable ways.

Figure 1: Sigmoid (blue) and step (black) functions.

From a CPS-implementation point of view, conditional state-
ments of the form if (f(x) > a) are also a disaster. They
render CPS frail and nonadaptive. In other words, a small
change in the environment or the program itself, may lead to
catastrophic consequences, as the CPS is not able to adapt.
In the AI community, where steps are called hard neurons
and sigmoid curves are called soft neurons, adaptation and
robustness is achieved by learning a particular form of Ba-
yesian networks with soft-neuron distributions, called neu-
ral networks. Such networks, and in particular deep neural
networks, have recently achieved amazing performance, for
example in the recognition of sophisticated patterns [8, 10].
This technology looks so promising that major companies
such as Google and Amazon are actively recruiting experts
in this area. However, the neural-networks learned are still
met with considerable opposition, as it is very diﬃcult, if
not impossible, to humanly understand them.

Having identiﬁed the if-then-else programming construct as
the major source of trouble in the analysis, design and im-
plementation of CPS, the following important question still
remains: Is there a simple, humanly understandable way to
develop robust and adaptive CPS? It is our belief, that such
a way not only exists, but it is also amazingly simple!

First, as program skeletons express domain knowledge and
developer intuition, they are here to stay. However, one
needs to replace hard neurons with their soft counterparts.
We call such program statements neural if-then-else state-
ments, or nif-then-else for short. They represent probabilis-
tic, probit distributions, and the decision to choose the left
or the right branch is sampled from their associated Gaus-
sian distributions. As a consequence, a program with nif
statements represents not only one, but a very large (up to
the computational precision) set of correct executions.

Second, the partial knowledge of such a program is encoded
as a Bayesian network, expressing the conditional dependen-
cies among the Gaussian distributions occurring within the
nif statements. These dependencies may be given, learned
through a preliminary phase and continuously improved dur-
ing deployment, or inferred through optimization techniques.
In this case, learning and optimization are considerably sim-
pliﬁed, as the program is by deﬁnition smooth. The depth
and the branching structure of these Bayesian networks re-
ﬂect the sequential and parallel nesting within the program,
which is an essential asset in program understanding.

may possess a quad-tree Bayesian structure, hierarchically
reﬂecting the neighbourhood relation among subimages. The
depth of the network is determined by the height of the tree.
Similarly, a purely sequential program, representing succes-
sive decisions, will have a very linear Bayesian structure,
whose depth is determined by the number of decisions.

In order to validate our new paradigm, we use the parking
example from [5]. The goal of this example was to automati-
cally learn the parameters of a program skeleton, intuitively
expressing the control as follows: Go backwards up to a
point a1, turn up to an angle b1, go backwards up to a2, turn
again up to b2 and ﬁnally go backwards up to a3. Since this
program uses classical if statements, it is not adaptive, and
a small perturbation such as a slippery environment, may
lead to an accident. We therefore rewrite the program with
nif statements, and learn the conditional Gaussian network
associated with the predicates within these statements. Us-
ing its sensors, the control program is now able to detect the
actual stopping or turning points, and to adequately sample
its next targets. Although this program is written once and
for all, it is able to adapt to a varying environment.

The main contributions of the work presented in this paper
can be therefore brieﬂy summarized as follows:

1. We propose a new programming paradigm for the de-
velopment of smooth and adaptive CPS in which:

• Troublesome ifs are replaced by neural ifs, thus
improving analysis, design and implementation,
• Partial knowledge is encoded within a learned Ba-
yesian network, with Gaussian distributions.

2. We demonstrate the versatility of this programming
paradigm on a parking example using Pioneer rovers.
The associated youtube videos are available at [2].

Given obvious space limitations, we do not address CPS-
analysis and CPS-design (optimization) in this paper. They
will be the subject of a series of follow up papers.

The rest of the paper is organized as follows. In Section 2 we
introduce Bayesian inference, Bayesian networks, and Gaus-
sian and Probit distributions. In Section 3 we introduce our
programming paradigm. In Section 4 we discuss how to learn
the Gaussian Bayesian network. In Section 5 we discuss our
implementation platform and the associated results. In Sec-
tion 6 we discuss related work. Finally in Section 7 we give
our concluding remarks and directions for future work.

2. BACKGROUND
The main tool for logical inference is the Modus-Ponens rule:
Assuming that proposition A is true, and that from the truth
of A one can infer the truth of proposition B, one can con-
clude that propositions A and B are both true. Formally:

A ∧ (A → B) = A ∧ B = B ∧ (B → A)

In probability theory, the uncertainty in the truth of a propo-
sition (also called an event) is expressed as a probability,
and implication between propositions is expressed as a con-
ditional probability. This leads to a probabilistic extension
of Modus-Ponens, known as the Bayes’ rule. Formally:

For example, a parallel algorithm for pattern recognition,

P (A) P (B | A) = P (A ∧ B) = P (B) P (A | B)

This rule, consistent with logic, is the main mechanism for
probabilistic inference [28]. It allows to reason in both for-
ward, or causal way, and backwards, or diagnostic way. For
example if B is causally implied by A, then the left term in
the above equation denotes a causal relation, and the right
term, a diagnostic relation. Equating the two, allows one to
use causal information (or observed events), for diagnostic
inference. In real-world systems, causal relations are usually
chained and can form sophisticated structures.

Bayesian Networks. A probabilistic system is completely
characterized by the joint probability distribution of all of its
(possibly noisy) components. However, the size of this dis-
tribution typically explodes, and its use becomes intractable.
In such cases, the Bayes’ rule, allows to successively decom-
pose the joint distribution according to the conditional de-
pendences among its random variables (RV). These are both
discrete or continuous variables, which associate to each
value (or inﬁnitesimal interval) in their range, the rate of its
occurrence. Networks of conditional dependencies among
random variables are known as Bayesian networks (BN),
and they have a very succinct representation.

Syntactically, a BN is a direct acyclic graph G = (V, E),
where each vertex vi ∈ V represents a random variable Xi
and each edge eij ∈ E represents a conditional dependence
of the variable Xj on the variable Xi. To avoid the com-
plications induced by the use of the joint probability dis-
tribution (or density), each variable Xi is associated with
a conditional probability distribution (CPD) that takes into
account dependencies only between the variable and its di-
rect parents [20, 28]. Such a compact representation keeps
information about the system in a distributed manner and
makes reasoning tractable even for large number of variables.

Although in many interesting applications the variables of a
BN have discrete distributions (e.g. in fault detection, a de-
vice might have only a ﬁnite number of diagnosable errors,
caused by a ﬁnite set of faults), in many other applications,
continuous random variables naturally describe the entities
of interest. For instance, in our parallel parking running
example, a Pioneer rover starting from an initial location,
needs to execute a sequence of motion primitives (e.g. driv-
ing or turning forward or backward with ﬁxed speed for a
particular distance Xi or angle Xj), which will result in
parking the rover in a dedicated parking spot.

Gaussian Distributions. Any real measurement of a phys-
ical quantity is aﬀected by noise. Hence, the distances and
the angles occurring in our parking example are naturally
expressed as continuous RVs. In this paper we assume that
variables have Normal, also called, Gaussian distributions
(GD). These distributions naturally occur from the mixing
of several (possibly unobservable) RVs, and they have math-
ematical properties making them very attractive.

An univariate Gaussian distribution (UGD) is denoted by
N (µ, σ2) and it is characterized by two parameters: The
mean µ and the variance σ2. In our example, the desired
distance in the ﬁrst motion is associated with µ, which is
perturbed by noise with variance σ2. The Gaussian proba-

bility density of a RV X with values x is deﬁned as follows:

pdfµ,σ2 (x) =

√

1
2πσ

(cid:18)

exp

−

(x − µ)2
2σ2

(cid:19)

.

(1)

Parallel parking includes a sequence of motion primitives
that are mutually dependent. To express these dependencies
we use a multivariate Gaussian Distribution (MGD) [15],
which generalizes the Gaussian distribution to multiple di-
mensions. For a n-dimensional vector of random variables
X the probability density function is characterized by a n-
dimensional mean vector µ and a symmetric positive deﬁnite
covariance matrix Σ. To express the probability density of
a multivariate Gaussian distribution we use the inverse of
covariance matrix, called precision matrix T = Σ−1, which
will be helpful later during the learning phase. The proba-
bility density then can be written as follows [24]:

pdfµ,σ2 (x) =

1

(2π)n/2(det(T−1))1/2 exp

(cid:18)

−

1
2

(cid:19)

∆2(x)

, (2)

where ∆2(x) = (x − µ)T T(x − µ).

A Gaussian BN (GBN) is a BN where random variables X
associated to each node in the network have associated a
Gaussian distribution, conditional on their parents Xi.

Probit Distributions. In order to smoothen the decisions
in a program, we need to choose a function without plateaus
and discontinuities. Since we operate with Gaussian random
variables, the natural candidate is their cumulative distri-
bution function (CDF). This is an S -shaped function or a
sigmoid (see Figure 2), whose steepness is deﬁned by σ2,
where erf denotes the error function:

cdfµ,σ2 (x) =

(cid:18)

1 + erf

1
2

(cid:18) x − µ
√
2
σ

(cid:19)(cid:19)

,

(3)

For a particular value x of X, the function cdfµ,σ2 (x) returns
the probability that a random sample from the distribution
N (µ, σ2) will belong to the interval (−∞, x].

Since the sensors and actuators of the Pioneer rover are
noisy, the trajectories it follows are each time diﬀerent from
the optimal one (assuming that such diﬀerence is tolerated
by the parking conﬁguration), even if the optimal trajec-
tory of the parking example is known. To be adaptive we
use a combined approach: we incorporate probabilistic con-
trol structures in the program (introduced in the Section 3)
and sample commands from a GBN, whose parameters were
learned experimentally. To detect changes in the environ-
ment and get more accurate position estimates, data from
various sensors are combined with a sensor fusion algorithm.

3. NEURAL CODE
Traditional inequality relations (e.g. >, ≥, ≤, <) deﬁne a
sharp (or ﬁrm) boundary on the satisfaction of a condition,
which represents a step function (see Fig 1). Using ﬁrm
decisions in a program operating on Normal RVs, cuts dis-
tributions in halves, leaving unnormalized and invalid PDFs
(see Figure 3: The upper right plot shows the approximation
of the PDF after passing a Normal RV through a traditional
conditional statement). Hence, to keep a proper PDF after
passing a Normal RV through an if or a loop statement one
needs to perform a re-normalization of the PDF.

In order to avoid re-shaping of probability density each time
after a variable is passed through a condition, we introduce
a special type of control structure called neural if, or nif
for short. The name is coined to express the key novelty
of our approach: We propose to use a smooth conditionals
cdfµ,σ2 (x) instead of ﬁrm ones (see Figure 1). A nif state-
ment operates on an inequality relation and a variance σ2,
and decides which branch should be taken: nif(x # y,σ2),
where # can be replaced with (>, ≥, ≤ or <) and σ2 rep-
resents the uncertainty of making a decision. For the case
when σ2 → 0 (no uncertainty) we require the nif statement
to behave as a traditional if statement.

The evaluation of the nif() statement is explained on hand
of the following example, where x, a ∈ R and σ2 ∈ R+.

interval is (−∞; +∞) and includes every possible sample;
for the second case the probability of taking S1 is 0, hence
the interval is empty and cannot contain any sample. An if
statement is an nif statement without uncertainty.

Let us illustrate the concept on a concrete example. Suppose
that in the current execution x = 0 and a = 1. Figure 2
illustrates how decisions are made if σ2 is: 0.42, π, 42. Since
diff(x,a) = 1, the probability of executing S1 is deﬁned by
cdf0,σ2 (1) and for the above cases is equal to 0.994, 0.714
and 0.599 respectively. The intervals for the above cases are
as follows:
[-1.095;1.095], [-1.890; 1.890] and [-3.357; 3.357].
In the second step we sample from the distributions with
the corresponding σ2 (N (0, 0.42), N (0, π) and N (0, 42)) and
check whether the value lies within the intervals.

nif ( x >= a , σ2 ) S1 else S2

The evaluation is done in two steps: (i) Find an R inter-
val I representing the conﬁdence of making the decision;
(ii) Check if a sample from the GD N (0, σ2) belongs to I.

Since the input RV has a GD, and a GD is used to evaluate
the condition, the result is a product of two GDs, which is
also a GD scaled by some constant factor k. To ﬁnd I in
(i), we estimate the diﬀerence diff(x,a) between x and a.
For the general case nif(x # a,σ2), with arbitrary #, the
diﬀerence diff(x,a) is deﬁned as below, where (cid:15) represents
the smallest real number on a computer.

diff(x,a) =





x - a − (cid:15)
x - a
a - x − (cid:15)
a - x

if # is >,
if # is ≥,
if # is <,
if # is ≤ .

Informally, our conﬁdence is characterized by the diﬀerence:
The larger diff(x,a) is, the larger is the probability of
executing S1. The probability to execute S1 is given by
cdf0,σ2 (diff(x,a)) and is used to obtain the interval [q1; q2]
by calculating two symmetric quantiles q1 and q2 such that:

(cid:90) q2

q1

pdf0,σ2 (x)dx = cdf0,σ2 (diff(x,a)).

(4)

In the second step a random sample from the distribution
N (0, σ2) is checked to belong to the interval [q1; q2]. If it
is within the interval, S1 is executed, otherwise S2. At this
point the probability value to execute S1 is inﬂuenced by
the variance σ2 (see Figure 2). Hence, the dependence is
twofold: diff(x,a) shows how conﬁdent we are in making
a decision, and σ2 characterizes the uncertainty.

For the case σ2 → 0 the nif statement is equivalent to the
if statement. For inﬁnitesimal σ2 the PDF is expressed as
a Dirac function δ(x), which has the following properties:

(i)
(ii)

δ(x) = +∞ if x = 0 else 0
(cid:82) ∞
−∞ δ(x) dx = 1.

The Dirac function essentially concentrates all the PD in a
single point x = 0. In this case the cdf0,σ2→0(x) becomes a
step function (see Figure 1). We consider two cases, as fol-
lows: (i) diff(x,a) ≥ 0 and (ii) diff(x,a) < 0. In the ﬁrst
case the probability of executing S1 is equal to 1, hence the

Figure 2: CDFs, PDFs and the quantiles for x = 1

So far we were concerned with single samples x ∼ N (µ, σ2).
Figure 3 illustrates what happens to the distributions: The
diﬀerences of passing a GD RV x ∼ N (0, 0.1)
through the
statements if(x >= 0.15) and nif(x >= 0.15, 0.1). Us-
ing our approach the GD is not cut in undesirable ways, and
it maintains its GD form after passing the nif statement.

Figure 3: Passing RVs through conditions

We can introduce now the conﬁdence-uncertainty trade-oﬀ
to loops. The neural while statement, or nwhile for short, is
an extension of a traditional while statement which incor-
porates uncertainty. The statement nwhile( x # a, σ2){P1}
takes an inequality relation and variance σ2 and executes
the program P1 according to the following rule: (1) Com-
pute diff(x # a) and obtain quantiles q1 and q2 according to
the Equation 4; (2) Check if a random sample x ∼ N (0, σ2)
is within the interval [q1; q2]; (3) If the sample belongs to
the interval, execute P1 and go to step (1), else exit.

Since the nif and nwhile statements subsume the behavior

q1(0,4)q1(0,0.4)q2(0,0.4)q1(0,       )q2(0,       )q2(0,4)if( x > 0.15) { x;}nif( x > 0.15,  0.1) { x;}x ~ N(0 , 0.1)of traditional if and while statements (the case σ2 → 0), we
use them to deﬁne an imperative language with probabilistic
control structures. Binary operators bop (such as addition
multiplication), unary operators uop (negation), and con-
stants c are used to form expressions E. A program P is a
statement S or combination of statements.

E ::= xi | c | bop(E1, E2) | uop(E1)
S ::= skip | xi := E | S1; S2 | nif( xi # c, σ2 ) S1 else S2 |

nwhile( xi # c, σ2){ S1 }

In order to deﬁne the denotational semantics for the nif and
the nwhile statements, we use check(xi, c, σ2, #), a function
which: (1) Computes the diﬀerence diff(xi, c), (2) Finds
quantiles q1 and q2 (Equation 4), and (3) Checks if a sample
x ∼ N (0, σ2) belongs to the interval [q1; q2]. If it does, it re-
turns value 1, otherwise it returns value 0. The denotational
semantics of neural programs is then deﬁned as follows:

(x) = x
skip
(cid:74)
(cid:75)
xi := E
(cid:74)

S1; S2

(x) = x[
(cid:75)
(x) =
(cid:75)

E
(cid:74)
S2
(cid:74)

(
(cid:75)

(x) (cid:55)→ xi]
(cid:75)
S1
(cid:74)

(x))
(cid:75)

(cid:74)
nif( xi # c, σ2) S1 else S2
(cid:74)

(x) =
(cid:75)
check(xi, a, σ2, #)
(x)
S1
(cid:74)
(cid:75)
(cid:74)
¬check(xi, a, σ2, #)
(x)
(cid:74)
(cid:75)
nwhile( xi # c, σ2){ S1 }
(cid:74)
¬check(xi, a, σ2, # )
x
(cid:74)
check(xi, a, σ2, # )
(cid:74)

(x) +
(cid:75)
(x)
S2
(cid:74)
(cid:75)
(x) =
(cid:75)
(x) +
(cid:75)
nwhile( xi # c, σ2){ S1 }
(
(x)
(cid:75)
(cid:74)
(cid:75)

S1
(cid:74)

x)
(cid:75)

We are now ready to write the control-program skeleton for
the parallel parking task of our Pioneer rover, as a sequence
of nwhile statements, as shown in Listing 1. Each nwhile
corresponds to executing one motion primitive of the infor-
mal description in Section 1. The functions moving() and
getPose() are output and input statements, which for sim-
plicity, were omitted from the denotational semantics.

Listing 1: Parallel parking program skeleton
nwhile ( currentDistance < targetLocation1 , sigma1 ) {

moving () ;
cu rren tDis tan ce = getPose () ;
}

u p d a t e T a r g e t L o c a t i o n s () ;
nwhile ( currentAngle < targetLocation2 ,

sigma2 ) {

turning () ;
currentAngle = getAngle () ;
}

u p d a t e T a r g e t L o c a t i o n s () ;
nwhile ( cur ren tDistance < targetLocation3 , sigma3 ) {

moving () ;
cu rren tDis tan ce = getPose () ;
}

u p d a t e T a r g e t L o c a t i o n s () ;
nwhile ( currentAngle < targetLocation4 ,

sigma4 ) {

turning () ;
currentAngle = getAngle () ;
}

u p d a t e T a r g e t L o c a t i o n s () ;
nwhile ( cur ren tDistance < targetLocation5 , sigma5 ) {

moving () ;
cu rren tDis tan ce = getPose () ;
}

The versatility of this approach is that the program skeleton
is written only once and comprises inﬁnite number of con-
trollers. The question we need to answer next is:

What are the distances and turning angles for each action
and how uncertain are we about each of them?

To ﬁnd the unknown parameters from Listing 1, namely the
target locations targetLocations and variances sigmas, we
use the learning procedure described in Section 4.

4. BAYESIAN-NETWORK LEARNING
Parking can be seen as a sequence of moves and turns, where
each action depends on the previous one. For example, the
turning angle typically depends on the previously driven dis-
tance. Due to sensor noise and imprecision, inertia and
friction forces, and also many possible ways to perform a
parking task starting from one initial location, we assume
that the dependence between actions is probabilistic, and in
particular, the RVs are distributed according to Gaussian
distributions (GD). We represent the dependencies between
actions as the GBN in Figure 4, where li or αj denotes a
distance or a turning angle of the corresponding action and
bij is a conditional dependence between consecutive actions.

b21

α1

b32

b43

α2

b54

b65

α3

b76

l4

l3

l2

l1

Figure 4: Gaussian Bayesian Network for parking

In order to learn the conditional probability distributions of
the GBN in Figure 4, and to ﬁll in the targetLocations
and the sigmas in Listing 1, we record trajectories of the
successful parkings done by a human expert. Figure 5 shows
example trajectories used during the learning phase.

Figure 5: Example trajectories for the parking task

We than use the fact that any GBN can be converted to an
MGD [24] in our learning routine. Learning the parameters
of the GBN can be divided into three steps:

1. Convert the GBN to the corresponding MGD,
2. Update the precision matrix T of the MGD,
3. Extract σ2s and conditional dependences from T.

1. Conversion step. To construct MGD we need to obtain
the mean vector µ and the precision matrix T. The mean
vector µ comprises the means of all the variables from the
GBN. To ﬁnd the symbolic form of the precision matrix, we

0.200.20.40.60.811.21.41.610.500.5use the recursive notation in [16], where the value of the
coeﬃcients bi, will be learned in the update step below.

The size of the training set v∗ is updated to its new value:

Ti+1 =

Ti +






bi+1bT
σ2

i+1

i+1

bT
σ2

i+1

i+1






v∗ = v + M

(9)

(5)

The updated covariance matrix β∗ combines the prior ma-
trix β with the covariance matrix of the training set s:

− bi+1
σ2

i+1
1
σ2

i+1

−

s =

M
(cid:88)

(cid:16)

i=1

(cid:17) (cid:16)

x(i) − x

(cid:17)T

x(i) − x

β∗ =β + s +

(cid:16)

rm
v + M

x(i) − x

(cid:17) (cid:16)

(cid:17)T

x(i) − x

(10)

Finally, the new value of the matrix β is used to calculate
the covariance matrix (T∗)−1, where α∗ = α + M .

(T∗)−1 =

v∗ + 1
v∗(α∗ − n + 1)

β∗

(11)

3. Extraction step. The new parameters of the GBN can
now be retrieved from the updated mean vector µ∗ and from
(T∗)−1.
If new traces are available at hand, one can up-
date the distributions by recomputing µ∗ and (T∗)−1 using
Equations 8-11. We depict the whole process in Figure 6:
Unknown parameters from the program skeleton are learned
from successful traces and these dependencies are used dur-
ing the execution phase to sample the commands.

In order to apply Equation 5 we deﬁne an ordering starting
with the initial node l1. Its precision matrix is equal to:

T1 =

1
σ2
1

.

The vector bi comprises dependence coeﬃcients for node i
on all its immediate parents it in the ordering. For example,
the dependence vector for node α2 in the Figure 4 equals to:

b4 =









0
0
b43

After applying the Equation 5 to each node in the GBN, we
obtain the precision matrix T7, shown in Figure 12. Since
each action in the parking task depends only on the previous
one (for example, in Figure 4 the turning angle depends on
the previously driven distance only), we can generalize the
precision matrix for the arbitrary number of moves. For a
GBN with k moves, all non-zero elements of the precision
matrix T ∈ Rk;k can be found according to the Equation 6,
where T(r, c) is a c-th element in a r-th row of the precision
matrix with indices started from one.

T(i, i − 1) = −

T(i, i) =

1
σ2
i

+

T(i, i + 1) = −

bi(i−1)
σ2
i

b2
(i+1)i
σ2
i+1
b(i+1)i
σ2

i+1

,

,

,

(6)

2. Update step. Once we derived the symbolic form of the
precision matrix (T7 in our example), we use the training
set, in order to learn the actual values of its parameters, as
described in the algorithm from [24]. Each training example
x(i) corresponds to a vector of lengths and turning angles
for a successful parking task. The total number of examples
in the training set is M . The procedure allows us to learn
iteratively and adjust the prior belief by updating the values
of the mean µ and covariance matrix β of the prior, where v
is a size of a training set for the prior belief, and α = v − 1.

β =

v(α − n + 1)
v + 1

T−1,

(7)

The updated mean value µ∗ incorporates prior value of the
mean µ and the mean value of the new training examples x.

x =

µ∗ =

(cid:80)M

i=1 x(i)
M
vµ + M x
v + M

(8)

Figure 6: Learning Parameters in a Neural Program

5. EXPERIMENTAL RESULTS
We performed our experiments on a Pioneer P3AT-SH mo-
bile rover from Adept MobileRobots [1] (see Figure 7). The
rover uses the Carma Devkit from SECO [21] as a main com-
putational unit. The comprised Tegra 3 ARM CPU runs the
Robot Operating System (ROS) on top of Ubuntu 12.04.

5.1 Structure of the Parking System
The parking system can be separated into several building
blocks (see Figure 8). The block Rover Interface senses and
controls the rover, that is, it establishes an interface to the
hardware. The block Sensor Fusion takes the sensor values
from the Rover Interface block, and provides the estimated
pose of the rover to the high-level controller Engine. The
Engine uses the GBN block to update the motion commands

Neural Program with Uncertain ParametersSet of tracesSamplingfrom GBN with learnedparametersLearningprocedureLearning phaseExecution phaseLearnedParametersT7 =




















1
σ2
1

+ b2
21
σ2
2
− b21
σ2
2
0

0

0

0

0

1
σ2
2

− b21
σ2
2
+ b2
32
σ2
3
− b32
σ2
3
0

0

0

0

0

1
σ2
3

− b32
σ2
3
+ b2
43
σ2
4
− b43
σ2
4
0

0

0

0

0

1
σ2
4

− b43
σ2
4
+ b2
54
σ2
5
− b54
σ2
5
0

0

0

0

0

1
σ2
5

− b54
σ2
5
+ b2
65
σ2
6
− b65
σ2
6
0

0

0

0

0

− b65
σ2
6
+ b2
76
σ2
7
− b76
σ2
7

1
σ2
6




















0

0

0

0

0

− b76
σ2
7
1
σ2
7

(12)

for sampling the motion commands. Before starting the run
we obtain the initial command vector from the distributions
learned. The distribution of the ﬁrst move l1 is independent
from any other move and has the form N (µ1, σ2
1). Starting
from the second move α1, each motion depends on the previ-
ous one: For motion number n, the distribution has the form
N (µn − bn,n−1 ∗ xsampled
n). The initial command vector
is obtained by sampling from l1, and each subsequent com-
mand vector is obtained by taking into account the previous
sample, when sampling from its own distribution.

, σ2

n−1

Figure 7: Experimental platform: Pioneer Rover

based on the estimated pose. Furthermore, the Engine maps
the (higher level) motion commands to velocity commands
needed by the Rover Interface to control the rover.

Figure 8: Parking system architecture

5.1.1 The Gaussian Bayesian Network Block
The goal of the GBN block in Figure 8, is to generate motion
commands for the Engine to execute. A motion command
corresponds to a driving distance or a turning angle.

During the learning phase, the distributions of the random
variables (RVs) in the Gaussian Bayesian network (GBN) in
Figure 4 are collected in a CSV ﬁle of following format:

motionType , m o t i o n D i r e c t i o n , mean , v a r i a n c e ,
d e p e n d e n c e C o e f f i c i e n t

Parsing the CSV ﬁle initializes the GBN that will be used

As the rover and its environment are uncertain (e.g., sensors
are disturbed by noise) we use the pose provided by the
sensor fusion unit to update the motion commands. Hence
the motion commands are constantly adapted to take into
account the actual driven distance (which could be diﬀerent
from the planned one due to the aforementioned uncertainty
of the CPS). This allows us to incorporate the results of the
sensor fusion algorithm in the updated commands.

5.1.2 The Engine Block
During the run we execute a motion command according to
the semantics of the nwhile loop.
In particular, the esti-
mated pose is passed from the Sensor Fusion block to the
Engine and compared with the target location, speciﬁed as
a point on a 2-D plane. Since the rover is aﬀected by noise
its path can deviate and never come to the target location.
To be able to detect and overcome this problem we estimate
the scalar product of two vectors: The ﬁrst one is the initial
target location, and the second one is the current target lo-
cation. This product is monotonely decreasing and becomes
negative after passing the goal even on a deviating path. In
an nwhile statement we monitor the distance (or angle) and
detect if we should process the next command.

To obtain the current state of the rover (its pose), and send
velocity commands, we start two separate threads: (i) Re-
ceive the pose and (ii) Send the velocity command. The
motion command (containing desired driving distance or
turning angle) is converted to a suitable velocity or steering
command respectively, for the Rover Interface. After each
executed command, we resample the pose in order to take
into account actual driving distance in subsequent moves.

5.1.3 The Rover Interface Block
The block Rover Interface implements the drivers for sen-
sors and actuators. The wheel velocities are measured by en-
coders, already supplied within the Pioneer rover. A built-in

Enginenwhile ( . )  moving();nwhile ( . ) turning();...initial motioncommandsvelocitycommandsvl, vr, a, ωx, y,  θactualpositionresampledcommandsGBN(distributions)Rover InterfaceSensorFusionmicrocontroller reads the encoders and sends their value to
the Carma Devkit. Additionally the rover is equipped with
an inertial measurement unit (IMU) including an accelerom-
eter, measuring the linear acceleration, and a gyroscope,
measuring the angular velocity of the rover. The Raspberry
Pi mounted on top of the rover samples the accelerometer
and gyroscope, and forwards the raw IMU measurements
to the Carma Devkit. The rover is controlled according to
the incoming velocity commands containing desired linear
velocity into forward direction (x-translation) and the de-
sired angular velocity (z-rotation). The desired translation
and rotation is converted to the individual wheel velocities,
which are sent to and applied by the built-in microcontroller.

5.1.4 The Sensor Fusion Block
Parking is often performed by applying predeﬁned rules [22]
or following a speciﬁc trajectory [22]. So typically an au-
tonomously parking car stops at a speciﬁc position beside
a parking space and then turns and moves for a ﬁxed time,
angle or distance. The car has to stop, move and turn ex-
actly as designated to park successfully. The car has to be
aware of its current pose, that is, position and heading, oth-
erwise parking will most likely fail (whatsoever controller is
used). However, the current pose is observed by sensors,
which suﬀer from uncertainty. Measurements are distorted
by noise, e.g., caused by ﬂuctuations of the elements of the
electrical circuit of the sensors. The environment may be
unpredictable, e.g., the car may slip over water or ice when
parking. To overcome such problems sensor fusion tech-
niques are applied, i.e., several sensors are combined to es-
timate a more accurate state. A common method is state
estimation (also called ﬁltering) [23, 30].

In this application, an unscented Kalman ﬁlter (UKF) [31] is
used. This ﬁlter combines the measurements listed in Table
1 with a suitable model describing the relations from the
measured variables to the pose of the car.

Sensor

vl
vr
a
ω

left wheel’s velocity
right wheel’s velocity
linear acceleration
angular velocity

Variance

0.002 m/s
0.002 m/s
0.25 m/s2
0.00017 rad/s

Table 1: Used sensors and its variances.

The belief state maintained by the UKF, e.g., the current lin-
ear velocity, will be continuously updated: (i) By predicting
the state, and (ii) By updating the prediction with measure-
ments. For example, the linear velocity will be predicted by
the current belief of acceleration and the time elapsed since
the previous estimation. Next, the measurements from ac-
celerometer and wheel encoders are used to update the pre-
dicted velocity. Because the wheel encoders are much more
accurate than the acceleration sensor (see variance in Ta-
ble 1), the measurements from the wheel encoders will be
trusted more (for simplicity one can think of weighting and
averaging the measurements, where the particular weight
corresponds to the reciprocal variance of the sensor). How-
ever, by using more than one sensor, the unscented Kalman
ﬁlter reduces the error of estimated and actual velocity.

Integration into ROS

5.2
ROS [26] is a meta-operating system that provides common
functionality for robotic tasks including process communi-
cation, package management, and hardware abstraction. A
basic building block of a system in ROS is a so-called node,
that performs computation and exchanges information with
other entities. Nodes communicate with each other sub-
scribing for or publishing messages to speciﬁc topics. So all
the nodes subscribed to a particular topic A, will receive
messages from nodes publishing to this topic A.

Since the application is implemented in ROS we use the
utility roslaunch to start the required ROS nodes (as shown
in Figure 9) corresponding to the blocks given in Section 5.1.

Rover Interface: The node RosAria is used to control the
velocity of the rover and provide the values of the wheel
encoders for the sensor fusion node. The ROS nodes
imu3000 and kxtf9 running on the Raspberry Pi pro-
vide data from acceleromenter and gyroscope.

Sensor Fusion: sf_filter node reads sensor values, im-
plements the sensor fusion algorithm and provides the
estimated pose of the rover.

GBN and Engine: pioneer_driver is a node implement-
ing resampling of commands based on the actual driven
distance and constantly providing the required velocity
commands to the RosAria node (see Figure 9).

Figure 9: Parking system in ROS.

5.3 Results
After the learning phase, we obtain the parameters of the
GBN that we use in the program skeleton (Table. 2). Since
we track the position using the data from the sensor fusion
and each movement has the experimentally learned uncer-
tainty, we are resistive to the perturbation of the actual
driving distances and angles. If in the current distance of
the robot deviates from the planned one, the commands, re-
sampled from the GBN will try compensate the deviation
with the dependencies obtained from the learning phase.

−
σ2
1
b54
σ2
5

0.0062

-0.0045
0.0008

b21
σ2
2
b65
σ2
6

0.7968
0.0032

1.1920
0.0178

b32
σ2
3
b76
σ2
7

-0.2086
0.0019

-0.0968
0.0013

b43
σ2
4

0.5475
0.022

Table 2: BN variances and coeﬃcient dependences.

6. RELATED WORK
Although probabilistic programs (PP), Gaussian Bayesian
networks (GBN) and neural networks were considered be-
fore, to the best of our knowledge, the development of smooth
control statements, related within a GBN ontology is new.
The ontology represents the knowledge of the PP about both
its environment and its own control logic.

Probabilistic programs are represented by diﬀerent languages
and frameworks [9, 13, 14]. The authors in [14] diﬀerentiate
probabilistic programs from “traditional” ones, by the abil-
ity to sample at random from the distribution and condition
the values of variables via observation. Although they con-
sider both discrete and continuous probability distributions,
and transformation of Bayesian Networks and Discrete Time
Markov Chains to probabilistic programs, they do not men-
tion probabilistic control structures linked in GBN.

In [6], the authors adapted the signal and image process-
ing technique called Gaussian smoothing (GS), for program
optimization. Using GS, a program could be approximated
by a smooth mathematical function, which is in fact a con-
volution of a denotational semantics of a program with a
Gaussian function. This approximation is used to facilitate
optimization for solving the parameter synthesis problem.
In [7] this idea was developed further and soundness and ro-
bustness for smooth interpretation of programs was deﬁned.
In both papers the authors do not consider any means for
eliminating the re-normalization step of the probability den-
sity function when a variable is passed through a conditional
branch in the current execution trace. Moreover, they stop
short of proposing new, smooth control statements.

Learning Bayesian Networks comprises diﬀerent tasks and
problem formulations: i) Learning the structure of the net-
work, ii) Learning the conditional probabilities for the given
structure and iii) Performing querying-inference for a given
Bayesian Network [24]. In [16] the authors introduce a uni-
ﬁed method for both discrete and continuous domains to
learn the parameters of Bayesian Network, using a combi-
nation of prior knowledge and statistical data.

Various formulations of a mobile parking problem were ex-
tensively studied for robots with diﬀerent architectures [17–
19, 22, 29]. For instance, in [22] the authors use a custom
spatial conﬁguration of the ultrasonic sensors and binaural
method to perceive the environment and park the robot us-
ing predeﬁned rules.
In [19] the authors approximate the
trajectory for the parking task with a polynomial curve,
that the robot could follow with the constraints satisﬁed,
and used fuzzy controller to minimize the diﬀerence between
speciﬁed trajectory and actual path.

In order to govern a physical process (e.g., parking a car),
the controller must be aware of the internal state of the
process (e.g., the position of the car). Sensors measure the
outputs of a process, whereof the state can be estimated.
However, the measurements are distorted by noise and the
environment may be unpredictable. State estimators [3, 23,
30] and in particular Kalman ﬁlters [30, 31] are commonly
used methods to increase the conﬁdence of the state estimate
evaluated out of raw sensor measurements.

7. CONCLUSION
In this paper we introduced deep neural programs (DNP),
a new formalism for writing robust and adaptive cyber-
physical-system (CPS) controllers. Key to this formalism is:
(i) The use of smooth Probit distributions in conditional and
loop statements, instead of their classic stepwise counter-
parts, and (2) The use of a Gaussian Bayesian network, for
capturing the dependencies among the Probit distributions
within the conditional and loop statements in the DPN.

We validated the usefulness of DPNs by developing, once
and for all, a parallel parking CPS-controller, which is able
to adapt to unforeseen environmental situations, such as a
slippery ground, or noisy actuators. No classic program has
such ability: One would have to encode all this unforeseen
situations, which would lead to an unintelligible code.

In future work, we plan to explore the advantages of DPNs
in the analysis, as well as, in the design (optimization) of
CPS controllers. The nice mathematical properties of DPNs
make them an ideal formalism for these tasks.

8. REFERENCES
[1] Adept MobileRobots(2013). Pioneer 3-AT.

http://www.mobilerobots.com/ResearchRobots/
P3AT.aspx (Accessed 24.08.2014).

[2] Parking Videos. http://youtu.be/xNOj AR-

SEYs?list=PLP5Gx6r7gK2cxjKv0K2V5fBedovfo8 3y
(Accessed 12.10.2014).

[3] M. Arulampalam, S. Maskell, N. Gordon, and

T. Clapp. A Tutorial on Particle Filters for Online
Nonlinear/Non-Gaussian Bayesian Tracking. IEEE
Transactions on Signal Processing, 50(2):174–188,
2002.

[4] M. Broy and E. Geisberger. Cyber-physical systems,

driving force for innovation in mobility, health, energy
and production. Acatech: The National Academy Of
Science and Engineering, 2012.

[5] S. Chaudhuri and A. Solar-Lezama. Smooth

Interpretation: Presentation Slides.
http://people.csail.mit.edu/asolar/Talks/
PLDI2010Final.pptx (Accessed 14.06.2014).
[6] S. Chaudhuri and A. Solar-Lezama. Smooth

interpretation. In PLDI, pages 279–291, 2010.
[7] S. Chaudhuri and A. Solar-Lezama. Smoothing a
program soundly and robustly. In CAV, pages
277–292, 2011.

[8] D. Ciresan, U. Meier, and J. Schmidhuber.

Multi-column deep neural networks for image
classiﬁcation. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on,
pages 3642–3649, June 2012.

[9] A. Dekhtyar and V. Subrahmanian. Hybrid

Probabilistic Programs . The Journal of Logic
Programming, 43(3):187 – 250, 2000.

[10] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,
P. Vincent, and S. Bengio. Why does unsupervised
pre-training help deep learning? J. Mach. Learn. Res.,
11:625–660, Mar. 2010.

[11] M. Fraenzle. Analysis of hybrid systems: An ounce of
realism can save an inﬁnity of states. In Computer

Science Logic, volume 1683 of Lecture Notes in
Computer Science, pages 126–139. Springer Berlin
Heidelberg, 1999.

[12] S. Gao, S. Kong, W. Chen, and E. M. Clarke.

Delta-complete analysis for bounded reachability of
hybrid systems. CoRR, abs/1404.7171, 2014.

[13] W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. A
Language and Program for Complex Bayesian
Modelling. Journal of the Royal Statistical Society.
Series D (The Statistician), 43(1):pp. 169–177, 1994.

open-source robot operating system. In ICRA
Workshop on Open Source Software, 2009.

[27] S. Ratschan and Z. She. Constraints for continuous
reachability in the veriﬁcation of hybrid systems. In
J. Calmet, T. Ida, and D. Wang, editors, Artiﬁcial
Intelligence and Symbolic Computation, volume 4120
of Lecture Notes in Computer Science, pages 196–210.
Springer Berlin Heidelberg, 2006.

[28] S. Russell and P. Norvig. Artiﬁcial Intelligence: A
Modern Approach. Prentice-Hall, 3rd edition, 2010.

[14] A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K.

[29] N. Scicluna, E. Gatt, O. Casha, I. Grech, and

J. Micallef. Fpga-based autonomous parking of a
car-like robot using fuzzy logic control. In Electronics,
Circuits and Systems (ICECS), 2012 19th IEEE
International Conference on, pages 229–232, Dec 2012.

[30] S. Thrun, W. Burgard, and D. Fox. Probabilistic

Robotics. MIT Press, Cambridge, 2006.

[31] E. Wan and R. Van der Merwe. The Unscented

Kalman Filter for Nonlinear Estimation. In Adaptive
Systems for Signal Processing, Communications, and
Control Symposium 2000. AS-SPCC. The IEEE 2000,
pages 153–158, 2000.

[32] Q. Wang, P. Zuliani, S. Kong, S. Gao, and E. M.

Clarke. Sreach: Combining statistical tests and
bounded model checking for nonlinear hybrid systems
with parametric uncertainty. CoRR, abs/1404.7206,
2014.

Rajamani. Probabilistic Programming. In
International Conference on Software Engineering
(ICSE Future of Software Engineering). IEEE, May
2014.

[15] G. Grimmett and D. Stirzaker. Probability and
random processes. Oxford science publications.
Clarendon Press, 1985.

[16] D. Heckerman and D. Geiger. Learning bayesian
networks: A uniﬁcation for discrete and gaussian
domains. In UAI, pages 274–284, 1995.

[17] M.-A. Ibarra-Manzano, J.-H. De-Anda-Cuellar, C.-A.

Perez-Ramirez, O.-I. Vera-Almanza, F.-J.
Mendoza-Galindo, M.-A. Carbajal-Guillen, and D.-L.
Almanza-Ojeda. Intelligent algorithm for parallel
self-parking assist of a mobile robot. In Electronics,
Robotics and Automotive Mechanics Conference
(CERMA), 2012 IEEE Ninth, pages 37–41, Nov 2012.

[18] K. Jiang and L. Seneviratne. A sensor guided

autonomous parking system for nonholonomic mobile
robots. In Robotics and Automation, 1999.
Proceedings. 1999 IEEE International Conference on,
volume 1, pages 311–316 vol.1, 1999.

[19] M. Khoshnejad and K. Demirli. Autonomous parallel

parking of a car-like mobile robot by a neuro-fuzzy
behavior-based controller. In Fuzzy Information
Processing Society, 2005. NAFIPS 2005. Annual
Meeting of the North American, pages 814–819, June
2005.

[20] D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques - Adaptive
Computation and Machine Learning. The MIT Press,
2009.

[21] M. A. Lee. CUDA on ARM: Tegra3 Based Low-Power
GPU Compute Node, 2013. Poster presented at GPU
Technical Conference, 2013.

[22] T. Li, Y.-C. Yeh, J.-D. Wu, M.-Y. Hsiao, and C.-Y.
Chen. Multifunctional Intelligent Autonomous
Parking Controllers for Carlike Mobile Robots.
Industrial Electronics, IEEE Transactions on,
57(5):1687–1700, May 2010.

[23] H. Mitchell. Multi-Sensor Data Fusion - An

Introduction. Springer, Berlin, Heidelberg, New York,
2007.

[24] R. E. Neapolitan. Learning Bayesian Networks.

Prentice-Hall, Inc., Upper Saddle River, NJ, USA,
2003.

[25] D. L. Parnas. Software aspects of strategic defense

systems. Commun. ACM, 28(12):1326–1335, Dec.
1985.

[26] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. B.

Foote, J. Leibs, R. Wheeler, and A. Y. Ng. ROS: an

