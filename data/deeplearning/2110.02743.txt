1
2
0
2

v
o
N
4

]
S
A
.
s
s
e
e
[

2
v
3
4
7
2
0
.
0
1
1
2
:
v
i
X
r
a

Towards efﬁcient end-to-end speech recognition
with biologically-inspired neural networks

Thomas Bohnstingl∗
IBM Research, Zurich
Graz University of Technology

Ayush Garg
IBM Research, Zurich
ETH Zurich

Stanisław Wo´zniak
IBM Research, Zurich

George Saon
IBM Research AI, Yorktown Heights

Evangelos Eleftheriou†
IBM Research, Zurich

Angeliki Pantazi
IBM Research, Zurich

Abstract

Automatic speech recognition (ASR) is a capability which enables a program
to process human speech into a written form. Recent developments in artiﬁcial
intelligence (AI) have led to high-accuracy ASR systems based on deep neural
networks, such as the recurrent neural network transducer (RNN-T). However,
the core components and the performed operations of these approaches depart
from the powerful biological counterpart, i.e., the human brain. On the other hand,
the current developments in biologically-inspired ASR models, based on spiking
neural networks (SNNs), lag behind in terms of accuracy and focus primarily on
small scale applications. In this work, we revisit the incorporation of biologically-
plausible models into deep learning and we substantially enhance their capabilities,
by taking inspiration from the diverse neural and synaptic dynamics found in
the brain. In particular, we introduce neural connectivity concepts emulating the
axo-somatic and the axo-axonic synapses. Based on this, we propose novel deep
learning units with enriched neuro-synaptic dynamics and integrate them into the
RNN-T architecture. We demonstrate for the ﬁrst time, that a biologically realistic
implementation of a large-scale ASR model can yield competitive performance
levels compared to the existing deep learning models. Speciﬁcally, we show that
such an implementation bears several advantages, such as a reduced computational
cost and a lower latency, which are critical for speech recognition applications.

1

Introduction

Speech is one of the most natural ways for human beings to communicate and to interact, which led
several research groups into developing speech recognition applications using computer systems.
However, even if the origin of the sound signal is the same for the humans and the computers, the
transformation, the feature extraction and the further processing differ signiﬁcantly. Particularly,
the later often utilizes mathematical abstractions, such as the Hidden Markov Model (HMM) or
domain-speciﬁc models [3, 14, 6, 39]. More recently, AI researchers have increasingly resorted to
deep learning approaches [16, 24, 38, 27]. Especially three kinds of architectures have been widely
adopted in the literature [34, 28]: architectures based solely on recurrent neural networks (RNN),
such as the RNN transducer [15], architectures based on RNNs with the attention mechanism, such as
the listen attend and spell model (LAS) [7], and recently also transformer-based models, such as the
conformer [17]. While the transformer-based and attention-based models potentially provide higher
accuracy, they require the entire audio signal to compute the transcript and hence have a high latency.

∗Correspondence to boh@zurich.ibm.com
† Currently with Axelera AI, Zurich

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
In contrast, the RNN-T architecture can produce a transcript while receiving the audio signal and
thus exhibits a lower latency, allows for real-time transcription and is even commercially deployed,
for example in mobile devices [19]. Recently, researchers have also tried to merge the RNN-T and
the LAS model in order to leverage the beneﬁts of both [29, 21, 22].

Despite their great successes, all the aforementioned models take inspiration from biology only
remotely and depart considerably from the human speech recognition system. Researchers working
on biologically plausible spiking neural networks (SNNs), have used the leaky integrate-and-ﬁre (LIF)
neurons for speech recognition [9, 37, 10]. However, there are several limitations of these works. For
example, many of them use simpler network architectures compared to the ones from the ML domain.
Moreover, often only parts of the network architecture employ biologically-inspired units and the
learning algorithms used are inferior to error-backpropagation [18, 31]. Thus, the performance of
those approaches lags behind their ML-based counterparts in terms of accuracy.

In this paper, we address these limitations by proposing an RNN-T architecture that incorporates
biologically-inspired neural units. Speciﬁcally, we leverage the diverse neuron and synapse types
observed in the brain and propose novel extensions of the LIF model with much richer dynamics.
We build upon the spiking neural units (SNUs) [36], which allows us to leverage the advanced
training capabilities from the ML domain [25, 26] that are essential for speech recognition [30].
We demonstrate that a state-of-the-art network incorporating biologically-inspired units can yield
competitive performance levels in a large-scale speech recognition application, while signiﬁcantly
reducing the computational cost as well as the latency. In particular, our key contributions are:

• a novel neural connectivity concept emulating the axo-somatic synapses that enhances the

threshold adaptation in biologically-inspired neurons,

• a novel neural connectivity concept emulating the axo-axonic synapses that modulates the

output of biologically-inspired neurons,

• a biologically-inspired RNN-T architecture with signiﬁcantly reduced computational cost
and increased throughput, which still provides a competitive performance to the LSTM-
based architecture.

2 Biologically-inspired models for deep learning

Typically, architectures of biologically-inspired neurons consider only axo-dendritic synapses, in
which the output from the pre-synaptic neuron travelling down the axon is modulated by the synaptic
weight and arrives at the post-synaptic dendrite. However, neural networks in the brain exhibit a much
more complex connectivity structure [20, 4]. In this work, we leverage this diversity and investigate
two additional types of synapses, namely the axo-somatic and the axo-axonic synapses, and propose
novel biologically-inspired models with richer dynamics. Figure 1a shows an illustration of such
neuron models and their connectivity via the various types of synapses.

The simplest biologically-inspired model is based on the LIF dynamics which employs axo-dendritic
synapses only, see dark blue color in Figure 1a. It can be abstracted into the form of a recurrent unit
using the SNU framework [35]. Figure 1b shows the basic conﬁguration of the SNU. A layer of n
SNUs receiving m inputs is governed by the following equations:

st = g(Wxt + Hyt−1 + d · st−1 (cid:12) (1 − yt−1)),
yt = h(st + b),

(1)

(2)

where xt ∈ IRm represents the inputs at time step t, st ∈ IRn represents the membrane potentials,
, yt ∈ IRn represents the outputs , d ∈ IR represents the constant decay of the
the hidden states,
membrane potential, b ∈ IRn represents the trainable ﬁring threshold, W ∈ IRn × m and H ∈ IRn × n
denote the trainable input and the recurrent weight matrices. Note that although the SNU shares
similarities with an RNN, e.g., the term Wxt + Hyt−1 and the activation function h, it has a richer
dynamics, including a state decay and an internal state reset mechanism, i.e., d · st−1 (cid:12) (1 − yt−1).
As described in [36], the SNU can operate in principle in two different modes, one in which it yields
continuous outputs (called sSNU), i.e. h = σ, where σ denotes the sigmoid function, and one in
which it emits discrete spikes, i.e. h = Θ, where Θ indicates the Heaviside function. In this work,
we focus on the dynamics of the neuronal models, in particular on the different synapse types, and
thus mainly consider the sSNU variants.

2

Figure 1: Illustration of different biological synapse and neuron types and their realization in
simulations. a A neural network example composed of three neuron and three synapse types along
with recordings of membrane potentials from the Human Brain Atlas [13]. b Visualization of SNU
modelling the LIF dynamics using solely axo-dendritic synapses. c Visualization of the SNU-a
modelling the adaptive threshold behavior using axo-somatic synapses, illustrated with yellow color
in a. d Visualization of the SNU-o using axo-axonic synapses, illustrated in orange in a.

One well-known property of neurons in the human brain is that they can adapt their ﬁring threshold
across a wide variety of timescales [13, 5, 35]. In biology, there are complex mechanisms inﬂuencing
the ﬁring threshold. In particular, it can be increased or decreased following the neuron’s own
dynamics, but also based on the activity of the other neurons [12, 23]. We propose a novel adaptive
SNU (SNU-a) that enhances the threshold adaptivity dynamics by emulating the axo-somatic synapses,
indicated with yellow color in Figure 1a. A layer of n SNU-a units is governed by

st = g(Wxt + Hyt−1 + d · st−1 (cid:12) (1 − yt−1)),
bt = ρ (cid:12) bt−1 + (1 − ρ) (cid:12) (cid:0)Waxt + Hayt−1(cid:1) ,
yt = h(st + βbt + b0),

(3)

(4)

(5)
where b0 ∈ IRn represents the trainable baseline threshold, β ∈ IR represents a constant scaling
factor, bt ∈ IRn represents the state of the threshold at time t, ρ ∈ IRn represents the constant decay
of the threshold, Wa ∈ IRn × m and Ha ∈ IRn × n denote the trainable input and recurrent weight
matrices inﬂuencing the threshold via axo-somatic synapses. Figure 1c shows an illustration of the
SNU-a and the lower right part of Figure 1a shows an example of the membrane potential evolution.

Another type of neural connectivity in the human brain is via axo-axonic synapses, indicated with
orange color in Figure 1a. These synapses mediate the release of neurotransmitters from the pre-
synaptic to the post-synaptic neuron. Interneurons (INs), e.g. Chandelier INs, that connect via axo-
axonic synapses are found in the brain and often act as inhibitors for the post-synaptic neuron [33, 11].
However, it has also been found that such neurons may be excitatory as well [11]. We incorporate the
axo-axonic synapses into the SNU framework and propose SNU-o units, with the following equations
st = g(Wxt + Hyt−1 + d · st−1 (cid:12) (1 − ˜yt−1)),
˜yt = h(st + bt),
yt = ˜yt (cid:12) o (cid:0)Woxt + Hoyt−1 + bt

(7)

(8)

(6)

(cid:1)

o

3

recurrentinputrecurrentoutputSNUaxo-dendriticsynapsesresetmembrane potential(optional)(optional)drecurrentoutputSNU-aresetmembrane potential(optional)thresholdadaptationSNU-oresetmembrane potentialbdcbaSNUAxo-axonic synapseSNU-ainputoutput modulationorecurrentSNU-oAxo-dendritic synapseDendriteSomaAxonAxo-somatic synapseaxo-somaticsynapsesaxo-axonicsynapsesrecurrentoutput(optional)ddrecurrentinputaxo-dendriticsynapses(optional)recurrentinputaxo-dendriticsynapses(optional)---time0.00.10.20.30.40.50.60.70.80.91.01.1mV-80-60-40-2040200time0.00.10.20.30.40.50.60.70.80.91.01.1mV-80-60-40-2040200Membrane potential evolution (SNU-o)Membrane potential evolution with threshold adaptation (SNU-a)where ˜yt represents the unmodulated output of the neuron driving the neural reset, yt represents the
modulated output of the neuron propagating to other connected neurons. The trainable parameters
Wo ∈ IRn × m and Ho ∈ IRn × n denote the input and recurrent weight matrices and bt
o ∈ IRn
denotes the trainable bias term that all three modulate the neuronal output via axo-axonic connections.
Note that in our simulations we use the sigmoid function as the activation for the output modulation
in Equation 8, i.e., o = σ, to mimic the inhibitory character of these synapses. However, by using a
different activation function o, the outputs can also be modulated in an excitatory manner. Figure 1d
shows an illustration of the SNU-o with the output modulation and the upper right part of Figure 1a
shows the connectivity motif along with an example of the membrane potential evolution.

As these novel units provide different dynamics than those of the predominantly used LSTM units,
we include a more detailed comparison of their dynamics in the supplementary material 1. Note that
although the subsequently presented models use only a single type of synapses per neuron, multiple
synapse types can be combined, resulting in further enriched neural dynamics.

3 RNN-T with biologically-inspired dynamics

The network architecture of RNN-T, illustrated in Figure 2, consists of two main subnetworks, the
encoding network and the prediction network, whose outputs are combined to ultimately form the
speech transcript. More details of the network architecture are presented in supplementary material 2.

In this work, we redesign the RNN-T architecture, using the novel units introduced in Section 2. In
order to do so, we follow a three-step approach. First, the units are introduced into the prediction
network, replacing the LSTMs, while the encoding network remains composed of LSTM units. In a
second step, we incorporate the sSNU variants into the encoding network only, i.e., the prediction
network remains composed of LSTMs. Finally, the sSNU variants are integrated into both network
parts and thus the full RNN-T architecture is composed of sSNU units. The encoding network carries
out a different task than the prediction network and hence different units might be better suited. Note
that a composition of diverse units and synapse types is also observed in the brain.

Speech recognition requires a large amount of compute resources. Thus, reducing the computational
cost is of paramount importance. Our proposed units not only reﬂect the biological inspirations, but

Figure 2: Illustration of the RNN transducer architecture. An input vector xt, containing MFCC
features of a speech signal, is processed by the encoding network, here represented with sSNU-o
units. The prediction network, corresponding to a language model, here represented with sSNU-a
units, enhances the predictions based on the past output sequence of the RNN-T, i.e. yu−1. The joint
network forms the ﬁnal predictions based on the outputs from both subnetworks.

4

Layer 1Layer k...............SoftmaxEncodingnetworkHow are you doingH o w       ar e   y ou   do in gJoin with    and apply tanhLayer 1.........SNU-aPredictionnetworkAxo-axonic synapseAxo-dendritic synapseAxo-somatic synapseEmbeddingSNU-aSNU-aSNU-a.........SNU-oSNU-oSNU-oSNU-o.........SNU-oSNU-oSNU-oSNU-o.........SNU-oSNU-oSNU-oSNU-o.........SNU-oSNU-oSNU-oSNU-oTable 1: RNN acronyms used in the result tables and details on the trainable parameters.

Axo-
dendritic

Feedforward
Recurrent
Adaptive thr. feedforward
Adaptive thr. recurrent

RNN Sufﬁx Comment
Thr.
b W
sSNU
sSNU R
b W, H
b0 W
sSNU-a
sSNU-a R
b0 W, H
sSNU-a Ra Adaptive thr. axo-somatic recurrent b0 W, H
sSNU-o
sSNU-o R

Output modulating feedforward
Output modulating recurrent

b W
b W, H

Axo-
somatic

Axo-
axonic

Ha

Wo, bo
Wo, Ho, bo

additionally are simpler in terms of the number of gates and parameters than LSTMs. Hence, they
provide the potential to drastically reduce the computational cost and the transcription time.

4 Experiments and results

The simulations were performed using the Switchboard speech corpus comprising roughly 300 hours
of English telephone conversations. In order to be comparable to the state-of-the-art literature, we
closely followed the preprocessing, the evaluation, as well as the network architecture setup presented
in [30]. An LSTM-based version of this RNN-T architecture achieves a word error rate (WER) of
12.7 %, which we consider as our baseline. We investigated various conﬁgurations of the sSNU
variants, listed with their abbreviations and trainable parameters in Table 1. In addition, supplementary
material 3 provides details about the dataset and its preprocessing, as well as hyperparameter settings.

Comparison of the word error rate. The ﬁrst part of Table 2 summarizes the results of the
RNN-T architecture, where the sSNU variants were integrated into the prediction network. Several
conﬁgurations, e.g., sSNU R (12.4% WER), sSNU-a R (12.0% WER) and sSNU-o R (12.4% WER),
outperform the LSTM-based variant. In a second step, we evaluated the performance of the encoding
network composed of sSNU-a and sSNU-o units. The best performance is achieved with sSNU-o
R with 14.7% WER compared to 12.7% WER of the LSTM baseline. Finally, the sSNU units
are integrated into both subnetworks so that the RNN-T is solely based on sSNUs, see the last
part of Table 2. Consistent with the prior two cases, the RNN-T architecture achieved competitive
performance with 16.0% WER and 14.9% WER.

Evaluation of the inference time. The inference time is a critical metric for speech recognition that
depends on the model architecture and the efﬁciency of the neural units. In our case, this efﬁciency is
largely dominated by the number of multiplications, including vector-matrix and scalar multiplica-
tions, and the number of activations that the units require to compute. Thus, we accumulated the total

Table 2: Performance comparison of the RNN-T network for different conﬁgurations of the prediction
and the encoding network

Enc. RNN

LSTM

Pred. RNN WER (%)
LSTM
sSNU
sSNU R
sSNU-a
sSNU-a R
sSNU-o
sSNU-o R

LSTM
sSNU-a Ra
sSNU-o
sSNU-o R
LSTM
sSNU-o R
sSNU-o R

LSTM

LSTM
sSNU-a R
sSNU-o R

12.7
15.1
12.4
12.1
12.0
12.6
12.4
12.7
25.2
23.2
14.7
12.7
16.0
14.9

# Multipl.

9.2k

(%)
2.39M 100
< 1
0.60M 25
11.52k
< 1
0.60M 25
17.66k
<1
1.20M 50
54.20M 100
18.50M 34
17.28M 32
27.11M 50
56.58M 100
27.71M 49
28.30M 50

tinf (s)
2.78
2.71
2.76
2.73
2.78
2.75
2.76
2.78
1.23
1.22
1.76
2.78
1.64
1.66

(%)
100
97
99
98
100
98
99
100
44
44
63
100
59
60

# Param.

(%)
2.39M 100
< 1
8.45k
0.60M 25
8.45k
< 1
0.60M 25
16.90k
< 1
1.20M 50
54.20M 100
18.47M 34
17.27M 32
27.10M 50
56.59M 100
27.70M 49
28.30M 50

5

Figure 3: Comparison of the transcription time of various architectures. The data points repre-
sent the mean time taken to decode utterances of different lengths. The markers indicate the various
architectures used and the error bars depict the standard deviation over 10 executions.

time, running on CPUs, for all computations involved during the inference of a single utterance. The
last three columns in Table 2 show the number of parameters, the number of multiplications as well
as the average time taken for the greedy decoding of an utterance with T = 388 input frames (~7.8s
of audio input).

When incorporating the sSNUs into the prediction network, the number of multiplications of this
subnetwork is reduced substantially, see the ﬁrst part of Table 2. However, this is not fully reﬂected
in the transcription time, because this subnetwork contributes only a small fraction to the total
computational cost of the RNN-T. As one can see in the second part of Table 2, when the LSTM
units of the encoding network are replaced with sSNUs, the total number of parameters is reduced by
more than 50% and this results in a more than ~40% reduction of the inference time. A similar trend
can be observed in the ﬁnal part of Table 2, where the RNN-T is solely composed of sSNUs. In this
case, the RNN-T achieves a competitive WER, albeit using 50% fewer parameters and experiencing
a ~40% reduce inference time.

Figure 3 depicts a comparison of the transcription time to decode utterances of various lengths for
selected models. To ensure reproducibility and consistency, the examples were chosen such that the
transcripts of all models were identical, and the timing results were averaged over 10 repetitions.
A general observation is that the time taken to transcribe utterances increases proportionally to the
utterance length. The RNN-T architecture composed of LSTM units is indicated with red dots and is
the slowest among the evaluated models. Depending on the sSNU variant used, the inference time
can be reduced. For example, the RNN-T using sSNU-o units for the encoding and sSNU-o units
for the prediction network (6x640 sSNU-o R, 1x768 sSNU-o R), has an approximately 40% reduced
inference time.

5 Conclusions

In this work we combine insights from neuroscience with a state-of-the-art network architecture from
machine learning and apply it to the task of speech recognition. We resort to the diverse types of
synapses present in the brain and enhance the dynamics of the commonly used LIF neuron model
with a threshold adaptation mechanism based on axo-somatic synapses as well as with an output
modulating mechanism based on the axo-axonic synapses. We successively integrate those novel
units into the RNN-T architecture and not only demonstrate that end-to-end speech recognition with
biologically-inspired units is possible, but also that they achieve a competitive performance level of
14.9% WER, compared to 12.7% WER of the LSTM-based RNN-T. Moreover, the introduced units
are simpler than LSTMs, so that the computational costs, as well as the inference time can be reduced
by 50% and 40% respectively. Finally, it is worth mentioning that biology provides an abundance of
mechanisms, which could further enrich the dynamics of neurons, and that are not yet covered in
practical neural networks. Our simulation results indicate that such effects can potentially bolster the
performance of biologically-inspired neural units and are therefore an essential step going forward.

6

9Supplementary material

1 Comparison of biologically-inspired units to LSTMs

As discussed in Section 1 of the main paper, LSTM units are commonly used in state-of-the-art
machine learning networks for speech recognition. They have three distinct gates which mediate the
input to the units, the decay of the internal state as well as the output of the units. In particular, a
layer of n LSTM units with m inputs is governed by the following equations
it = σ(Wixt + Hiyt−1 + bi),
ct = σ(Wcxt + Hcyt−1 + bc)
f t = σ(Wf yt + Hf yt−1 + bf ),
st = f t (cid:12) st−1 + it (cid:12) tanh(Wsyt + Hsyt−1 + bs),
yt = ct (cid:12) tanh(st).

(2)

(1)

(4)

(3)

(5)

We found empirically that the standard LIF dynamics lacks such functionalities and thus has intrinsic
limitations in tackling ASR in comparison with LSTMs. For example, the dynamics of the output gate
in LSTMs is governed by Equation 2, where Wc ∈ IRn × m, Hc ∈ IRn × n, bc ∈ IRn are trainable
parameters. The novel variants of SNUs introduced in Section 2 of the main paper, exploit additional
dynamics beyond the common LIF model, achieved via the axo-somatic and axo-axonic synapses. In
particular, the threshold adaptation of the SNU-a, as represented in Equation 4 of the main paper,
controls the output of the neuron by increasing or decreasing the ﬁring threshold. By comparing the
output mechanisms of the LSTM to the SNU-a

Output gate LSTM: σ(Wcxt + Hcyt−1 + bc),

Adaptive threshold SNU-a: βbt,

with

bt = ρ (cid:12) bt−1 + (1 − ρ) (cid:12) (cid:0)Waxt + Hayt−1(cid:1) ,
one can see that the adaptive threshold mechanism is different from the LSTM gate. The threshold
adaptation presents a low-pass ﬁlter, with a constant decay of ρ, of the input activity and the recurrent
activity multiplied with a trainable matrix. Moreover, its relation to the neuronal output is additive,
whereas for the output gates in LSTM units it is multiplicative, see Equations 5 of the main paper and
Equation 5 of these notes.

In contrast, the output modulating mechanism of the SNU-o, mimicking the axo-axonic synapses, is
more closely related to the LSTM output gate. This becomes apparent when comparing the relevant
equations for the LSTM units and SNU-o units:

Output gate LSTM: σ(Wcxt + Hcyt−1 + bc),
Ouput modulation SNU-o: σ (cid:0)Woxt + Hoyt−1 + bt
(cid:1) .
It is important to mention that although the output modulating mechanism of the SNU-o resembles
closer the behavior of the LSTM output gate, the former mechanism has a different background and
is inspired from the existence of different synapse types in the human brain.

o

2 Details of the RNN-T architecture

As described in the main paper, we utilize a state-of-the-art RNN-T network and redesign it incor-
porating biologically-inspired units. Broadly speaking, the RNN-T consists of two main network
components leveraging RNNs, the encoding network and the prediction network.

7

Table 1: RNN acronyms used in the result tables and details on the included parameters.

Axo-
dendritic

Feedforward
Recurrent
Adaptive thr. feedforward
Adaptive thr. recurrent

RNN Sufﬁx Comment
Thr.
b W
sSNU
sSNU R
b W, H
b0 W
sSNU-a
sSNU-a R
b0 W, H
sSNU-a Ra Adaptive thr. axo-somatic recurrent b0 W, H
sSNU-o
sSNU-o R

Output modulating feedforward
Output modulating recurrent

b W
b W, H

Axo-
somatic

Axo-
axonic

Ha

Wo, bo
Wo, Ho, bo

The encoding network is responsible for the feature encoding and processes the MFCCs, denoted
with xt = x0, x1, . . . , xT −1 ∈ IRnM F CC , where nM F CC is the number of mel frequency cepstral
coefﬁcients and T is the length of the input sequence. The encoding network uses k bidirectional
layers, we use k = 6 in our simulations. The second part, the prediction network, acts as a language
model and processes the output produced thus far by the RNN-T without the blank symbols, i.e.,
yu = y0, y1, . . . , yU ∈ IR(nvoc+1). Here nvoc is the size of the vocabulary which in our case
consists of 45 individual characters, U is the length of the ﬁnal output sequence, which might be
different from the input sequence length T and the initial input to the prediction network y0 is always
the blank symbol. Note that the prediction network is composed of an embedding layer followed
by a layer of recurrent neural units. The outputs of the encoding network, the acoustic embedding,
ht
enc ∈ IRnenc , where nenc is the number of units in the last layer of the encoder, and the output
of the prediction network, the prediction vector hu
pred ∈ IRnpred where npred is the number of
units in the last layer of the prediction network, are expanded, combined together via a Hadamard
product and then further processed by a tanh activation and softmax operation. The ﬁnal result,
ht,u
joint ∈ IRT × U × (nvoc+1), is then used to compute the output distribution and in turn the most
probable input-output alignment using the Forward-Backward algorithm as proposed in [15]. Note
that the output of the joint network may contain a special blank symbol that allows for alignment
of the speech signal with the transcript. This symbol gets removed from the ﬁnal prediction of the
RNN-T network.

3 Data preprocessing and simulation details

In our work we investigate the Switchboard speech corpus, which is a widely adopted dataset of
roughly 300 hours of English two-sided telephone conversations on predeﬁned topics. In particular,
the dataset contains speech from a total of 543 speakers from different areas of the United States and
is licensed under the Linguistic Data Consortium [2].

Initially, four data augmentations are applied to the original dataset in which the speed as well as
the tempo of the conversation are increased and decrease by a value of 1.1 and 0.9, respectively.
Then, a 40-dimensional MFCC vector is extracted every 10 ms and extended with the ﬁrst and second
order derivatives, yielding a 120-dimensional vector. Next, a time reduction technique is applied that
involves stacking consecutive pairs of frames, resulting in a 240-dimensional vector. Additionally,
the extracted features are combined with speaker-dependent vectors, called i-vectors [8], to form a
340-dimensional input used for neural network training.

As highlighted in the main paper, the two network components of the RNN-T are responsible to carry
out different tasks, hence different sSNU variants might be better suited for application in one or the
other. Therefore, we investigated various conﬁgurations of our novel biologically-plausible variants.

The training is accomplished using the AdamW [26] optimizer with a one-cycle learning rate
schedule [32], where the maximum learning rate ηm has been determined for each run individually.
We trained for 20 epochs wherein the learning rate was linearly ramped up from an initial value to
a maximum value within the ﬁrst six epochs and linearly ramped down to a minimum value in the
subsequent 14 epochs, similar to [30]. Table 1 lists explicitly the trainable parameters for different
sSNU variants along with their abbreviations. We trained with a batch size of 64 on two V100-GPUs
for approximately 10 days. To avoid overﬁtting we employed gradient clipping, in which the gradients

8

are combined to a single vector w and the individual components are then computed as

˜w = w (cid:12)

c
||w||2

,

with c = 1 or c = 10. In addition, we use dropout with a dropout probability of pW = 0.25 for all
the input weights and pE = 0.05 for the embedding layer.

The ﬁnal speech transcript was produced using beam search with a beam width of 16. For the
evaluation of our models, we followed the common procedure to report the word error rates (WER)
on the Hub5 2000 Switchboard and the CallHome test set jointly [1, 30]. As the baseline for
our benchmark, we re-implemented the very recent state-of-the-art results from [30]. We focused
primarily on the dynamics of the neurons and thus used a basic model implementation without any
external language model. Such an LSTM-based RNN-T architecture achieves a WER of 12.7 %,
which we consider as our baseline. However, it is worth mentioning that the research ﬁeld of speech
recognition is very active and although our selected baseline is representative of the state-of-the-art,
it can potentially be improved with the features mentioned in [30].

As mentioned in the main paper, we followed a three-step approach to integrate our biologically-
inspired units into the RNN-T network architecture. In addition to Section 4 of the main paper, the
Tables 2, 3 and 4 contain more detailed hyperparameter settings of our simulations.

Table 2: Hyperparameters of the prediction network

RNN
Cfg.
LSTM
1x768
sSNU
1x768
sSNU R
1x768
sSNU-a
1x768
sSNU-a R
1x768
1x768
sSNU-o
sSNU-o R 1x768
1x768
SNU

ηm
5 · 10−4
5 · 10−4
5 · 10−4
5 · 10−4
5 · 10−4
5 · 10−4
5 · 10−4
5 · 10−4

c
10
10
10
10
10
10
10
10

Additional

d = 0.9
d = 0.9
d = 0.9, β = 0.1, ρ = 0.9
d = 0.9, β = 0.1, ρ = 0.9
d = 0.9
d = 0.9
d = 0.9

Table 3: Hyperparameters of the encoding network

RNN
LSTM
sSNU-a Ra
sSNU-o
sSNU-o R

Cfg.
6x640
6x640
6x640
6x640

ηm
5 · 10−4
5 · 10−4
5 · 10−4
9 · 10−4

c
10
1
10
1

Additional

d = 0.9, β = 0.1, ρ = 0.9
d = 0.9
d = 0.9

Table 4: Hyperparameters of the full RNN-T network

Encoding network
Cfg.
RNN
LSTM
1x768
sSNU-o R 6x640
sSNU-o R 1x768

d = 0.9
d = 0.9

Prediction network
Cfg.
LSTM
6x640
sSNU-a R
1x768
sSNU-o R 6x640

Additional RNN

Additional

d = 0.9, β = 0.1, ρ = 0.9
d = 0.9

ηm
5 · 10−4
9 · 10−4
5 · 10−4

c

10
10
1

References

[1] 2000 HUB5 English Evaluation Transcripts - Linguistic Data Consortium, Jan 2002.
[Online; accessed via https:

URL https://catalog.ldc.upenn.edu/LDC2002T43.
//catalog.ldc.upenn.edu/LDC2002T43 on 2. May 2021].

[2] Switchboard-1 Release 2 - Linguistic Data Consortium, May 2021. URL https://catalog.
[Online; accessed via https://catalog.ldc.upenn.edu/

ldc.upenn.edu/LDC97S62.
LDC97S62 on 2. May 2021].

9

[3] J. Baker. The dragon system–an overview. IEEE Transactions on Acoustics, Speech, and Signal

Processing, 23(1):24–29, 1975. doi: 10.1109/TASSP.1975.1162650.

[4] Mark Bear, Barry Connors, and Michael A Paradiso. Neuroscience: Exploring the brain. Jones

& Bartlett Learning, LLC, 2020.

[5] Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass.
Long short-term memory and Learning-to-learn in networks of spiking neurons. In NeurIPS,
pages 787–797, 2018.

[6] M. Benzeghiba, R. De Mori, O. Deroo, S. Dupont, T. Erbes, D. Jouvet, L. Fissore, P. Laface,
A. Mertins, C. Ris, R. Rose, V. Tyagi, and C. Wellekens. Automatic speech recognition and
speech variability: A review. Speech Communication, 49(10):763–786, Oct 2007.
ISSN
0167-6393. doi: 10.1016/j.specom.2007.02.006.

[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In 2016 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4960–4964, 2016.
doi: 10.1109/ICASSP.2016.7472621.

[8] Najim Dehak, Pedro A Torres-Carrasquillo, Douglas Reynolds, and Reda Dehak. Language
recognition via i-vectors and dimensionality reduction. In Twelfth annual conference of the
International Speech Communication Association, 2011.

[9] Jonathan Dennis, Qiang Yu, Huajin Tang, Huy Dat Tran, and Haizhou Li. Temporal coding of
local spectrogram features for robust sound recognition. In 2013 IEEE International Conference
on Acoustics, Speech and Signal Processing, pages 803–807, 2013. doi: 10.1109/ICASSP.2013.
6637759.

[10] Linhao Dong and Bo Xu. CIF: Continuous Integrate-and-Fire for End-to-End Speech Recogni-

tion. arXiv, May 2019. URL https://arxiv.org/abs/1905.11235v4.

[11] Gord Fishell and Bernardo Rudy. Mechanisms of Inhibition within the Telencephalon: “Where
the Wild Things Are”. Annu. Rev. Neurosci., 34(1):535–567, Jun 2011. ISSN 0147-006X. doi:
10.1146/annurev-neuro-061010-113717.

[12] Bertrand Fontaine, José Luis Peña, and Romain Brette. Spike-Threshold Adaptation Predicted
by Membrane Potential Dynamics In Vivo. PLoS Comput. Biol., 10(4):e1003560, Apr 2014.
ISSN 1553-7358. doi: 10.1371/journal.pcbi.1003560.

[13] Allen Institute for Brain Science. Allen Human Brain Atlas, 2010. URL https://human.

brain-map.org.

[14] Mark Gales and Steve Young. The application of hidden markov models in speech recognition.
Found. Trends Signal Process., 1(3):195–304, January 2007. ISSN 1932-8346. doi: 10.1561/
2000000004. URL https://doi.org/10.1561/2000000004.

[15] Alex Graves. Sequence Transduction with Recurrent Neural Networks. arXiv, Nov 2012. URL

https://arxiv.org/abs/1211.3711v1.

[16] Alex Graves, Douglas Eck, Nicole Beringer, and Juergen Schmidhuber. Biologically plausible
speech recognition with LSTM neural nets. In Auke Jan Ijspeert, Masayuki Murata, and Naoki
Wakamiya, editors, Biologically Inspired Approaches to Advanced Information Technology,
pages 127–136, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg. ISBN 978-3-540-27835-
1.

[17] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,
Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-
augmented Transformer for Speech Recognition. In Proc. Interspeech 2020, pages 5036–5040,
2020. doi: 10.21437/Interspeech.2020-3015.

[18] Robert Gütig and Haim Sompolinsky. The tempotron: a neuron that learns spike timing–based
decisions - Nature Neuroscience. Nat. Neurosci., 9(3):420–428, Mar 2006. ISSN 1546-1726.
doi: 10.1038/nn1643.

10

[19] Yanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao,
David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang, Deepti Bhatia, Yuan
Shangguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo-yiin Chang, Kanishka Rao,
and Alexander Gruenstein. Streaming End-to-end Speech Recognition For Mobile Devices.
arXiv, Nov 2018. URL https://arxiv.org/abs/1811.06621v1.

[20] Allyson Howard, Gabor Tamas, and Ivan Soltesz. Lighting the chandelier: new vistas for
axo-axonic cells. Trends Neurosci., 28(6):310–316, Jun 2005. ISSN 0166-2236. doi: 10.1016/j.
tins.2005.04.004.

[21] Ke Hu, Tara N. Sainath, Ruoming Pang, and Rohit Prabhavalkar. Deliberation Model Based
Two-Pass End-to-End Speech Recognition. arXiv, Mar 2020. URL https://arxiv.org/
abs/2003.07962v1.

[22] Ke Hu, Ruoming Pang, Tara N. Sainath, and Trevor Strohman. Transformer based deliberation
for two-pass speech recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT),
pages 68–74, 2021. doi: 10.1109/SLT48900.2021.9383497.

[23] Chao Huang, Andrey Resnik, Tansu Celikel, and Bernhard Englitz. Adaptive Spike Threshold
Enables Robust and Temporally Precise Neuronal Encoding. PLoS Comput. Biol., 12(6):
e1004984, Jun 2016. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1004984.

[24] Biing-Hwang Juang and Lawrence R Rabiner. Automatic speech recognition–a brief history of
the technology development. Georgia Institute of Technology. Atlanta Rutgers University and
the University of California. Santa Barbara, 1:67, 2005.

[25] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv, Dec

2014. URL https://arxiv.org/abs/1412.6980v9.

[26] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv, Nov 2017.

URL https://arxiv.org/abs/1711.05101v3.

[27] Thai-Son Nguyen, Sebastian Stueker, and Alex Waibel. Super-Human Performance in Online
Low-latency Recognition of Conversational Speech. arXiv, Oct 2020. URL https://arxiv.
org/abs/2010.03449v3.

2021,

A Comparison

[28] G. Rao.
tures
in
a-survey-on-end-to-end-speech-recognition-architectures-in-2021.
via
[Online;
a-survey-on-end-to-end-speech-recognition-architectures-in-2021
May 2021].

Speech Recognition Architec-
URL https://www.assemblyai.com/blog/

https://www.assemblyai.com/blog/
6.

of End-to-End

accessed

January

2021.

on

[29] Tara Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirkó
Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu, Ian McGraw, and Chung-Cheng Chiu.
Two-pass end-to-end speech recognition. pages 2773–2777, 09 2019. doi: 10.21437/Interspeech.
2019-1341.

[30] George Saon, Zoltan Tueske, Daniel Bolanos, and Brian Kingsbury. Advancing RNN Transducer
Technology for Speech Recognition. arXiv, Mar 2021. URL https://arxiv.org/abs/2103.
09935v1.

[31] Cong Shi, Tengxiao Wang, Junxian He, Jianghao Zhang, Liyuan Liu, and Nanjian Wu.
DeepTempo: A Hardware-Friendly Direct Feedback Alignment Multi-Layer Tempotron Learn-
ing Rule for Deep Spiking Neural Networks. IEEE Trans. Circuits Syst. II, 68(5):1581–1585,
Mar 2021. ISSN 1558-3791. doi: 10.1109/TCSII.2021.3063784.

[32] Leslie N. Smith and Nicholay Topin. Super-Convergence: Very Fast Training of Neural
Networks Using Large Learning Rates. arXiv, Aug 2017. URL https://arxiv.org/abs/
1708.07120v3.

[33] Robin Tremblay, Soohyun Lee, and Bernardo Rudy. Gabaergic interneurons in the neocortex:

from cellular properties to circuits. Neuron, 91(2):260–292, 2016.

11

[34] Dong Wang, Xiaodong Wang, and Shaohe Lv. An Overview of End-to-End Automatic Speech
Recognition. Symmetry, 11(8):1018, Aug 2019. ISSN 2073-8994. doi: 10.3390/sym11081018.

[35] Stanisław Wo´zniak, Angeliki Pantazi, Thomas Bohnstingl, and Evangelos Eleftheriou. Deep
learning incorporating biologically-inspired neural dynamics. arXiv, Dec 2018. URL https:
//arxiv.org/abs/1812.07040v2.

[36] Stanisław Wo´zniak, Angeliki Pantazi, Thomas Bohnstingl, and Evangelos Eleftheriou. Deep
learning incorporating biologically inspired neural dynamics and in-memory computing. Nat.
Mach. Intell., 2:325–336, Jun 2020. ISSN 2522-5839. doi: 10.1038/s42256-020-0187-0.

[37] Jibin Wu, Yansong Chua, and Haizhou Li. A biologically plausible speech recognition frame-
work based on spiking neural networks. In 2018 International Joint Conference on Neural
Networks (IJCNN), pages 1–8, 2018. doi: 10.1109/IJCNN.2018.8489535.

[38] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig. The
Microsoft 2016 conversational speech recognition system. 2017 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 5255–5259, Mar 2017. ISSN
2379-190X. doi: 10.1109/ICASSP.2017.7953159.

[39] Dong Yu and Li Deng. Automatic Speech Recognition. Springer, London, England, UK, 2015.

doi: 10.1007/978-1-4471-5779-3.

Acknowledgment

We thank the Neuromorphic Computing and I/O Links group at IBM Research – Zurich for fruitful
discussions and comments.

12

