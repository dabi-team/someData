9
1
0
2

c
e
D
7
2

]

G
L
.
s
c
[

1
v
5
4
3
2
1
.
2
1
9
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2019

SYNTHETIC DATASETS FOR NEURAL PROGRAM SYNTHESIS

Richard Shin∗
UC Berkeley

Neel Kant
UC Berkeley and ML@B†

Kavi Gupta
UC Berkeley

Christopher Bender
UC Berkeley and ML@B

Brandon Trabucco
UC Berkeley and ML@B

Rishabh Singh
Google Brain

Dawn Song
UC Berkeley

ABSTRACT

The goal of program synthesis is to automatically generate programs in a particular
language from corresponding speciﬁcations, e.g. input-output behavior. Many
current approaches achieve impressive results after training on randomly generated
I/O examples in limited domain-speciﬁc languages (DSLs), as with string transfor-
mations in RobustFill. However, we empirically discover that applying test input
generation techniques for languages with control ﬂow and rich input space causes
deep networks to generalize poorly to certain data distributions; to correct this, we
propose a new methodology for controlling and evaluating the bias of synthetic
data distributions over both programs and speciﬁcations. We demonstrate, using
the Karel DSL and a small Calculator DSL, that training deep networks on these
distributions leads to improved cross-distribution generalization performance.

1

INTRODUCTION

Program synthesis is one of the central problems in Artiﬁcial Intelligence studied from the early
days (Manna & Waldinger, 1971; Waldinger & Lee, 1969) and has seen a lot of recent interest in the
machine learning and programming languages community (Alur et al., 2013; Gulwani et al., 2012;
2015; Muggleton, 1991; Lin et al., 2014; Solar-Lezama, 2013). The recent neural approaches can be
broadly classiﬁed into two categories: program induction and program synthesis. Both approaches
share the objective of learning program semantics but do so in different ways. Program induction
aims to embed the semantics of a particular algorithm into a differentiable model trained end-to-end,
whereas the goal of program synthesis is for a model to learn the semantics of a domain-speciﬁc
language (DSL) and produce programs deﬁned by corresponding speciﬁcations. Both problems
necessitate large datasets, either of I/O pairs in the case of program induction, or programs with
corresponding I/O pairs in the case of program synthesis.

However, since large datasets for program induction and synthesis tasks do not exist, these approaches
train models on large synthetically generated datasets. Presumably, if a model can accurately predict
arbitrary program outputs (for induction) or programs in the DSL (for synthesis) then it has likely
learnt the correct algorithm or DSL semantics.

Although this approach has led to some impressive synthesis results in many domains, synthetically
generating datasets that cover all DSL programs and the corresponding input space can be problematic,
especially for more complex DSLs like Karel the Robot (Pattis, 1981) which includes complex
control-ﬂow primitives (while loops and if conditionals) and operators. Likewise, for induction tasks,
the sampling procedure for program speciﬁcations may lead to undesirable biases in the training
distribution that inhibit strong generalization.

In this paper, we consider two problem settings. The ﬁrst is the Karel domain and the recently
proposed Karel synthesis model (Bunel et al., 2018). We identify many distributions of input
examples and DSL programs for which the Karel synthesis model performs poorly. The second
problem setting is a program induction problem in which a model is trained to execute and predict
the output of simple arithmetic expressions, which we denote the Calculator domain; for this domain,

∗ricshin@cs.berkeley.edu
†Machine Learning at Berkeley

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2019

we considered common synthetic data generation strategies including one from tensor2tensor
(Vaswani et al., 2018), an open-source deep learning library. Upon analysis, we ﬁnd evidence of
undesirable artifacts resulting from certain biases in the generation algorithm.

Our results indicate that models trained with common methodologies for synthesizing datasets fail
to learn the full semantics of the DSL, even when they perform well on a test set, and suggest
the need of a more principled way to generate synthetic datasets. For some program and input
distributions, the state-of-the-art neural synthesis models perform quite poorly, often achieving
less than 5% generalization accuracy. In our paper, we develop a new methodology for creating
training distributions over programs in the DSL to mitigate some of these issues. Moreover, unlike
previous works that have ignored considering the distributions over input space, we show that input
distributions also play a signiﬁcant role in determining the synthesizer performance. Our methodology
involves deﬁning the distribution over DSL programs and input space using a set of random variables
to encode much of the valuable features which describe the data, e.g. in the Karel domain, the amount
of control ﬂow nesting in programs or the number of markers present in the inputs.

Our methodology allows us to identify several specialized distributions over the input space and Karel
programs on which the current state of the art synthesis models (Bunel et al., 2018) perform poorly
when trained on traditional program and input distributions, and tested on our new distributions. From
this, we design new training distributions by ensuring greater uniformity over the random variables
in our methodology. By retraining the same architecture on these new training data distributions,
we observe a greater ability to generalize, with signiﬁcant improvements when evaluated on the
aforementioned test sets. We also observe similar improvements in the Calculator domain as well.

This paper makes the following key contributions:

• We propose a new methodology to generate different desirable distributions over the space

of datasets for program induction and synthesis tasks.

• We instantiate the methodology for the Karel and Calculator domains and show that model

generalization is worse on datasets generated by our technique.

• We then retrain models in both domains and demonstrate that models achieve greater overall
generalization performance when trained on datasets generated with our methodology.

2 RELATED WORK

2.1 TRAINING MODELS WITH SYNTHETIC DATA

In certain domains like computer vision and robotics, collecting high-quality real-world training
data incurs signiﬁcant cost, and so many researchers have investigated the use of large amounts of
synthetic data. For example, Christiano et al. (2016); Peng et al. (2017); Pinto et al. (2017); Bousmalis
et al. (2017) aim to learn robotics policies that compensate for differences between the real world and
the simulation. Within computer vision, Shrivastava et al. (2016) demonstrate learning from entirely
synthetic images for gaze and pose estimation.

2.2 NEURAL PROGRAM INDUCTION AND SYNTHESIS.

Program induction methods learn differentiable modules such as stack (Joulin & Mikolov, 2015),
RAM (Kurach et al., 2016), GPU (Kaiser & Sutskever, 2015), and read-write external memory (Graves
et al., 2014) to represent algorithms. Other methods attempt to learn differentiable control ﬂow
operations (Gaunt et al., 2016; Neelakantan et al., 2015). These approaches reconstruct outputs
given inputs, inferring the underlying algorithms. Other methods instead learn neural modules from
program traces rather than from I/O examples (Reed & de Freitas, 2015; Xiao et al., 2018; Cai et al.,
2017).

Devlin et al. (2017); Parisotto et al. (2017) use neural program synthesis techniques for learning string
editing programs in RobustFill. Similarly, Balog et al. (2016) learn array programs in DeepCoder,
and Bhupatiraju et al. (2017) learn to compose API calls. Bunel et al. (2018) apply neural program
synthesis to the Karel domain we consider in our paper; we use their architecture and dataset. Many
of these approaches report high test performance, but good performance on synthetically-speciﬁced
programs may not indicate the model’s ability to generalize to arbitrary user-desired programs. In

2

Published as a conference paper at ICLR 2019

fact, our results show that under certain distributions, these models perform quite poorly. To verify
these models appropriately generalize to reasonably complex arbitrary programs, the test set should
sufﬁciently represent the universe of these programs and their speciﬁcations. Likewise, to avoid
biasing the result, the test set should also draw uniformly from these programs and speciﬁcations. For
example, for RobustFill, the data generation methodology only sampled programs uniformly from
the string DSL, but did not take into account the distribution over input strings such as their length,
frequencies of occurrence of regular expressions and their nesting, common words and constants etc.

3 DATA GENERATION METHODOLOGY

Currently, automated data generation focuses in large part on a constructive process, whose parameters
can be tuned. We propose a complementary approach in which we perform a subsequent ﬁltering
step on this process to ensure that the resulting distribution has certain properties.

We deﬁne a salient random variable as one whose distribution in the ﬁnal dataset is of interest. In the
case of program synthesis, we consider two kinds of salient variables: variables denoting important
features of a program in the given DSL, such as its length and degree of nesting; and variables
denoting features for the input space.

In many cases, we can modify our sampling procedure to ensure a desirable distribution of a particular
salient variable. However, for some salient variables, it is infeasible to tune the parameters of a given
sampling procedure in order to obtain a desired distribution for that salient variable. For example, if
we sample programs directly from a context-free grammar, it is difﬁcult to control the distribution of
various salient variables such as program length, degree of nesting, etc. This is a notable problem in
both the Karel and Calculator domains.

Furthermore, within the context of program synthesis speciﬁcally, there is often an additional
challenge: not all inputs are valid for all programs. For example, in the Karel domain, an input for
a given program would be invalid if the program attempts to perform illegal actions for that input
(such as move into walls or pickMarker in a cell containing no markers). The requirement that
the program/input pairs suit each other itself acts as an unpredictable ﬁlter that makes it difﬁcult to
ensure uniformity of salient variables by tuning generation parameters.
As such, we propose a methodology for randomly sampling a dataset D (consisting of elements of S)
while ensuring that a given salient variable ν : S → X (where X is ﬁnite and discrete), denoted as a
random variable X = ν(s), has a uniform distribution throughout D. To do this, we ﬁrst sample an
example s ∼ q(·) from an original distribution q. We then add s to D with probability g(s), where

g(s) = (Pq[X = ν(s)] + ε)−1

(cid:18)

min
x∈X

(cid:19)

Pq[X = x] + ε

,

with Pq[X], the probabilities induced over X via q, calculated empirically based on counts computed
with past samples drawn from q. We repeat the above until D is of a desired size. For full pseudocode
see Section B.1 in the appendix.
We use ε ∈ R+ as a hyperparameter to trade off the runtime of the above procedure with the level
of X’s uniformity in D. In Section B.2 (in the Appendix), we provide a probabilistic bound on the
uniformity of X in the resulting distribution, for the case where ε = 0. Also, for when ε > 0, we can
show that drawing a single sample is possible in O( 1
ε ) calls to the original sampler (proof in Section
B.3, with empirical experiments in Section B.4 and B.5). Increasing ε increases the algorithm’s speed,
at the cost of allowing the distribution of X in D to further diverge from uniform.

4 APPLICATION TO KAREL: EXPERIMENTS WITH NEW TEST DISTRIBUTIONS

Karel is an educational programming language (Pattis, 1981) where the programmer writes imperative
programs with conditionals and loops to produce a sequence of actions for an agent (a robot named
Karel) which lives in a rectangular m × n grid world. For a detailed description of the particular
instantiation of the Karel language and input grid speciﬁcation that we consider, see Section A. The
program synthesis task that we consider is as follows: given a set of pairs of input and output grids
{(i1, o1), · · · , (in, on)}, ﬁnd a Karel program π such that executing π on i1 results in o1, i2 results

3

Published as a conference paper at ICLR 2019

Figure 1: An example Karel synthesis task, where the goal is to synthesize the program in (b) given
the two I/O examples shown in (a). See the Appendix for more details about Karel.

in o2, and so on. In the paper, n = 5 unless otherwise speciﬁed. An example Karel synthesis task
with the I/O examples and the corresponding Karel program to be synthesized is shown in Figure 1.

In this section, we employ the Karel instantiation of our abstract data generation methodology in
Section 4.1 to generate different test datasets. By imposing a more uniform distribution over the
salient random variables when generating the I/O speciﬁcations and target programs which make up
the test set, we observe much lower accuracies of the previous Karel synthesis models (Bunel et al.,
2018) compared to the original test set.

4.1 SALIENT VARIABLES IN KAREL

We devised the following salient random variables to describe the input space in Karel:

• Grid size: Dimensions of the grid in which Karel can act.
• Marker ratio: Fraction of cells with at least one marker.
• Wall ratio: Fraction of cells which contain a wall.
• Marker count: Number of markers that are present in a cell containing markers.
• Number of grids: Number of I/O grids shown to the model to specify the desired program.

For the program space in the Karel DSL, we consider the following random variables:

• Program size: Size of the program in terms of number of tokens.
• Control ﬂow ratio: Number of control ﬂow structures appearing in the program.
• Nested control ﬂow: The amount of control ﬂow nesting in programs (e.g. while inside if).

4.2 CHANGING THE I/O DISTRIBUTION

We reproduced the encoder-decoder model of Bunel et al. (2018) and trained it using the provided
synthetic training set with the teacher-forcing maximum likelihood objective. On the existing test
set, our model achieves 73.52% generalization accuracy, slightly higher than the 71.91% accuracy
reported in Bunel et al. (2018). Generalization accuracy denotes how often the model’s output is
correct on both the 5 I/O examples shown to the model and the remaining held-out 6th I/O example.

To test how the model may be sensitive to changes in the I/O examples used to specify the program,
we created new test sets by sampling new input grids and running them on each of the programs
in the existing test set to obtain new I/O pairs. By keeping the programs themselves the same, we
avoid inadvertent changes in the inherent difﬁculty of the task (the complexity of the programs to be
synthesized).

Salient random variables with uniform distribution. We ﬁrst generated grids such that they
would follow a distribution that is as uniform as possible in the salient features in Section 4.1. We
used the following procedure to sample each grid: 1) sample the grid size (height and width) from

4

55Input i1Output o1Input i2Output o2def main():    while(not(rightIsClear())):       move()    turnRight()    move()    repeat(5):       putMarker()    turnLeft()    turnLeft()    move()    turnRight()    while(frontIsClear()):       move()(a) Input-output examples(b) Karel ProgramPublished as a conference paper at ICLR 2019

Table 1: Generalization accuracies of the baseline model and a model trained on a uniform I/O
distribution, on selected datasets. G, U , and A stand for Geom(0.5), U{1, . . . 9} and 10−Geom(0.5)
respectively. See Section 4.2 for dataset generation details, and Section 5.1 for details about the
Uniform model.

rwall
rmarker
Dmarker count
Baseline (%)
Uniform (%)

G

24.30
69.37

0.05
0.85
U

1.32
70.21

A

0.04
68.99

G

21.08
63.25

0.25
0.65
U

2.98
63.74

A

0.08
62.78

G

16.63
65.83

0.65
0.25
U

13.31
67.39

A

6.63
68.09

G

15.99
77.32

0.85
0.05
U

12.88
78.63

A

12.98
80.19

∆

+45.07

+68.89

+68.95

+42.17

+60.76

+62.70

+49.20

+54.08

+61.46

+61.33

+65.75

+67.21

x, y ∼ U{2, . . . 16}; 2) sample the marker ratio rmarker ∼ U(0, 1) and wall ratio rwall ∼ U(0, 1);
3) for each cell (i, j), 0 ≤ i < x, 0 ≤ j < y in the grid, sample mi,j ∼ Bernoulli(rmarker) and
wi,j ∼ Bernoulli(rwall); 4) if mi,j = 1 and wi,j = 0, sample marker count mci,j ∼ U{1, . . . 9},
otherwise set mci,j = 0; 5) place walls and markers in grid according to wi,j and mci,j; 6) place
Karel at a random location (not containing a wall) and with a random orientation. After generating 5
input grids for a given program, we ensure that the program does not crash on any of them and also
check whether the 5 input grids exhibit complete branch coverage (i.e., each branch is taken by at
least one of the 5 inputs). If either of these conditions are not satisﬁed, we discard all 5 grids and
start over.

On this dataset, the model trained on existing data achieved generalization accuracy of only 27.9%,
which was a drop of 44.6pp from the existing test set’s generalization accuracy of 73.52%.

Salient random variables with narrow distributions. We further investigated the performance
drop noted above by synthesizing “narrower" datasets that captured different parts of the joint
probability space over the salient input random variables. For each narrow dataset, we selected rwall
and rmarker (both between 0 and 1) as well as a distribution Dmarker count which would be the same for
all I/O grids. Then, we follow the procedure below for each grid: 1) sample the grid size (height and
width) x, y ∼ U{10, . . . 16}; 2) randomly choose xy · rwall cells to contain walls, and xy · rmarker cells
for markers; 3) sample mci,j ∼ Dmarker count for all cells (i, j) chosen to contain markers; 4) place
Karel at a random location (not containing a wall) and with a random orientation. In our experiments,
we primarily used 3 different distributions for Dmarker count: Geom(0.5) truncated at 9, U{1, . . . 9},
and 10 − Geom(0.5) which, when sampled, has a value equal to 10 minus a sample from Geom(0.5),
truncated at 1.

The results are shown in Table 1 (row 1, “Baseline (%)”). We discovered that the most correlated
factor with model performance was the distribution Dmarker count. A more negative skew consistently
lowered model performance, and this effect was more pronounced at higher values of rmarker.

4.3 CHANGING THE PROGRAM DISTRIBUTION

We will now examine how the existing model can surprisingly fail to perform well at synthesizing
certain programs that are different from those in the existing validation and test sets.

Performance on complex DSL constructs. We examined whether or not the model could succeed
in synthesizing programs which require nesting of conditional constructs. This was of interest since
these programs were relatively rare in the training dataset. We generated an evaluation dataset
comprised solely of programs that contained while inside while statements, and another dataset
in which all programs had while inside if statements.1 We found that the model fared very poorly
on these datasets, achieving only 0.64% and 2.23% accuracy respectively.

Programs only containing actions.
Intuitively, much of the difﬁculty in the Karel program syn-
thesis task should come from inferring the control ﬂow statements, i.e. if, ifElse, and while.
Synthesizing a Karel program that only contains actions is intrinsically a much more straightforward
task, which a relatively simple search algorithm (such as A*) can perform well.

1To avoid any negative effects from changes in the I/O distribution, we attempted to ensure that rwall, rmarker

and Dmarker count matches that of the provided training and test sets.

5

Published as a conference paper at ICLR 2019

Table 2: Results on programs only containing actions. The generalization accuracy on action-only
programs in the existing test set is 99.24%. See Section 5 for details on Action-Only Augmented.

Program length

Model type

1

2

3

4

5

6

7

8

Baseline
Action-Only Augmented

16.00% 30.00% 44.24% 52.88% 56.56% 66.94% 67.16% 73.06%
20.00% 41.60% 52.24% 61.72% 63.04% 72.20% 72.74% 78.12%

We performed an experiment using test datasets generated by enumerating action-only programs
of various lengths. As there are ﬁve actions (move, turnLeft, turnRight, putMarker,
pickMarker), there exist 5L textually unique action-only programs for length L. We sampled up
to 500 unique programs of lengths 1, 2, . . . , 8. For each program, we generated 10 speciﬁcations,
each containing 5 I/O pairs. We sampled each I/O pair from the set of all input grids in the existing
training data (of which there are 6.7 million), as to match its distribution as closely as possible.

Table 2 shows the results. Remarkably, even though the underlying programs have relatively low
complexity, the model’s accuracy is lower on every one of these action-only test sets than the existing
provided test set. The generalization accuracy grows as the program length becomes longer, even
though those programs should be harder to synthesize.

Among the existing action-only programs in the test set, the model’s generalization accuracy on that
subset is 99.24%. Given the surprising nature of this result, we investigated the difference between
the action-only programs we generated, and those in the existing test set. We found that in the existing
training and test sets, all programs contain at least two actions, and also contain at least one move
action somewhere in the program. These and any undiscovered differences in the distribution of
programs seem to have caused the gap in performance.

5 APPLICATION TO KAREL: CHANGING TRAINING DISTRIBUTIONS

In Section 4, we saw that the existing model performs much more poorly on certain test datasets
that we constructed, compared to its performance on the existing test set as reported in Section 4.2.
In light of the framework in Section 3, various imbalances of the salient random variables in the
existing training data could have caused these gaps in performance. Then, a natural solution is to
train using datasets constructed to avoid undesirable skews in the salient random variables, which
should hopefully perform better across a variety of distributions.

5.1 TRAINING DATASETS WITH UNIFORM I/O

We generated a training dataset by taking the programs of the existing training set and synthesizing
I/O pairs using the procedure described in Section 4.2. Furthermore, to make the “number of grids”
salient variable uniform, we modify the training procedure by uniformly sampling a number between
1 and 5 for each mini-batch, and using that many I/O grids to specify the program to the model. We
trained a model on this data and then evaluated it on the same set of narrow distribution evaluation
datasets as mentioned in Section 4.2. Table 1 and Figure 2 compares how this new model performs
to the baseline model. The model trained on uniform I/O distributions maintains much higher
generalization accuracy on the test sets of Section 4.2 than the baseline model. Note that the uniform
I/O distribution is not simply a union of the tested distributions and is intended to cover all possible
input speciﬁcations.

5.2 REAL-WORLD BENCHMARKS

We evaluated both the baseline model, and the uniform model described in the previous paragraph,
on a set of 36 real-world Karel programming problems. This dataset was compiled from the Hour
of Code Initiative and Stanford University’s introductory computer science course, CS106A, with
the problems being hand-designed as educational exercises for students. We found that the baseline
model got 4 correct (11.1%) while the uniform model got 7 correct (19.4%) when both models were
trained with 5 shown I/O pairs, i.e., without making the “number of grids” salient variable uniform.

6

Published as a conference paper at ICLR 2019

Figure 2: Comparison of generalization accuracies across the datasets given in Table 1 plus perfor-
mance on the original test set. The training datasets used for both models (denoted Baseline and
Uniform) contained the same programs; however, for Uniform, we sampled new I/O grids used to
specify the programs, with homogenized salient random variables, as described in Section 4.2.

This further demonstrates the uniform model’s increased ability to generalize to out-of-distribution
datasets, including those which are of interest to humans. Both models’ accuracies are still low
compared to the performance on the synthetically generated test set.

After further analysis, we believe the models face two challenges on the real-world test set: (1) many
of the real-world problems require long programs to solve compared to the synthetic test set, (2) the
speciﬁcations in the real-world examples always contains fewer than 5 I/O pairs, and often only a
single one. However, the original training methodology for the model assumes that it is provided with
a diverse set of 5 I/O pairs. When we modiﬁed the training procedure to vary the number of shown
I/O examples as in Section 5.1, the baseline model got 12 correct (33.3%) and the uniform model got
11 correct (30.6%). This shows that the homogenization on the number of I/O pairs was effective.
Overall, the real-world dataset’s I/O distribution was similar to the MSR training datset in terms of
the salient random variables we homogenized, so it is unsurprising that the baseline model was able
to outperform the uniform model, consistent with the results on the MSR test set in Figure 2.

5.3 AUGMENTING DATASET WITH ACTION-ONLY PROGRAMS

We observed in Section 4.3 that the model fails to do well on either action-only programs or programs
with many control-ﬂow statements. In the case of action-only programs, we found that the training
data had been pruned to only include programs with at least two actions and at least one move, and
in the case of programs with complex control ﬂow, we found a similar sparsity in the train set.

As discussed in Section 3, the principled way to counteract this sparsity is to introduce uniformity
into a set of salient variables. This methodology allows us to counteract both naturally sparse data
(such as complex control-ﬂow) and spurious data pre-processing (such as enforcing programs to have
at least two actions).

In our case, we introduce uniformity into the length of action-only programs by synthesizing 20,000
programs of each length 1 to 20, by uniformly selecting tokens from the ﬁve action choices and
generating I/O with other salient random variables as close to the original training set as possible;
we append these new programs to the original dataset to train a new model. Table 2 shows a clear
improvement when homogenizing this salient random variable. The new model achieved 71.8%
accuracy on the original test set, which is very close to the 73.52% accuracy of the baseline model.

5.4 TRAINING DATASETS WITH NARROWLY DISTRIBUTED I/O

As done in for the uniform training dataset, we generated “narrow” training datasets by keeping the
same programs as in the existing training data and replacing the I/O pairs with the process from
Section 4.2.

We trained a variety of models and evaluated them on 12 datasets of different I/O feature distributions.
Figure 3 summarizes the results of evaluating each model on each dataset by noting the performance
of the model on the narrow dataset of the same type and the outcome on every other narrow dataset.
For the models trained on the low variance datasets, we observed that they all consistently achieved
between 60 and 70 percent accuracy on their own training distribution; however, the uniform model
was able to achieve similar performance (between 57 and 80 percent accuracy) as shown in Table 1
and Figure 2. As such, we hypothesize that models trained on wide, uniform distributions can still

7

Published as a conference paper at ICLR 2019

Figure 3: Evaluation of models that were trained on narrow distributions. When evaluated on their
own training distribution, they consistently achieved similar results. When evaluated on a different
narrow distribution, the performance was rarely similar and usually very low.

perform comparable to models trained on narrow sub-distributions, even when tested on the same
sub-distributions.

6 APPLICATION TO CALCULATOR

The Calculator task is given as follows: given an expression such as "5+4*(2+3)", compute the
result modulo 10; in this case, 5. Calculator is a program induction task rather than a program
synthesis task like Karel; nevertheless, creating data for the Calculator problem involves sampling
from a context-free grammar. Additionally, Calculator is not as intricate a domain as Karel and
thus we can more completely control the environment of data generation with less fear of lurking
variables.

6.1 CALCULATOR ENVIRONMENT

Calculator model. Similar to the work by Zaremba & Sutskever (2014), we implement an LSTM
that parses calculator expressions on a character level. We perform a 10-class classiﬁcation problem
using a dense network on the ﬁnal hidden state of the LSTM. The prediction is correct if it exactly
matches the evaluation result of the expression, modulo 10.

Distributions of Calculator tasks. We propose 4 distributions for calculator tasks: direct CFG
sampling (DCFG), tensor2tensor sampling (T2T), “runs” CFG sampling (RCFG), and balanced
sampling (BAL).

Two of our distributions represent reasonable ways in which a researcher might choose to sample
data. The ﬁrst is DCFG, which involves returning a digit with some probability (1 − p), or else
recursively sampling two productions and combining them with a −, ∗, or +, each with probability
p
3 . This corresponds to a direct, weighted sampling of the CFG for the calculator grammar. The
second is T2T, which is used by the tensor2tensor library to sample arithmetic expressions in one of
its examples.2 It involves sampling a depth d, then ensuring that the resulting AST has depth d by
forcing a random side of the operation production to be sampled to d − 1 and the other side to be
sampled to a depth d(cid:48) ∼ U{0, 1, . . . , d − 1}.

The other two distributions represent potentially difﬁcult or nonstandard problems that might appear
in practical environments. RCFG is similar to DCFG but involves increasing the frequency of “runs”
of the associative operations + and ∗ by picking 2, 3, or 4 subexpressions and then combining them
with the given symbol. BAL (balanced sampling) involves selecting a depth and then creating an AST
that is a balanced binary tree at that depth. Importantly, regardless of sampling technique, redundant
parentheses are removed. This is to increase the difﬁculty somewhat as order of operations needs to
be established.

6.2 SALIENT VARIABLES AND METHODOLOGY

We use the following salient variables: length (rounded to the nearest even number), number of
operations, number of pairs of parentheses, mean parenthesized depth, and maximum parenthesized

2https://github.com/tensorflow/tensor2tensor/blob/8bd81e8fe9dafd4eb1dfa519255bcbe3e33c7ffa/

tensor2tensor/data_generators/algorithmic_math.py

8

020406080100Accuracy (%)On Training Dist.Off Training Dist.Accuracy of Models Trained on Narrow Distributions When Evaluated on Other Narrow DistributionsPublished as a conference paper at ICLR 2019

Original

83.83%
78.25%

T2T
DCFG

Length

Max Depth Mean Depth

#Operations

#Parens

+4.35pp
+3.84pp

+4.24pp
+5.92pp

+2.14pp
+4.02pp

+1.19pp
+6.72pp

+2.32pp
+4.51pp

Table 3: Improvements in Calculator performance over unhomogenized distributions when various
homogenizations were applied. See Section 6 for details on performance metrics.

depth. Parenthesized depth is deﬁned for each digit and refers to the number of nested parentheses it
is in. For example in (1+2)*(3-4)+5, the 1, 2, 3, and 4 are at depth 1 while the 5 is at depth 0.
We constructed 2 × (1 + 5) distributions in total, corresponding to a total of 2 task distributions, T2T
and DCFG, which represent the “natural” sampling techniques a researcher might employ, and 1 + 5
homogenization strategies, one unhomogenized and ﬁve homogenized corresponding to each salient
variable with ε = 0.025. We then evaluated each model on a fresh evaluation set sampled from a
mixture of the four unhomogenized distributions (T2T, DCFG, RCFG, BAL).

6.3 RESULTS

The original performances and improvements created by homogenizing different random variables
can be found in Table 3. On average, homogenizing the DCFG and T2T distributions caused the
accuracy to increase by 5.00pp and 2.84pp, respectively.

We note that the Calculator domain is much simpler than Karel when considering both input com-
plexity (grid worlds versus arithmetic expressions) and output complexity (a DSL program versus a
single digit). Furthermore, the difference in distributions between the naive sampling approaches
and the versions with one homogenized random variable are not as different in Calculator as what
we observed in Karel (see Table 1 for the dramatic effect of Dmarker). We hypothesize that it is this
difference in complexity that explains the smaller (but still consistent) effect of homogenizing salient
random variables in Calculator as compared to in Karel.

7 CONCLUSION

We demonstrate that existing sampling methods for randomly generating input-output examples have
unintended and overlooked distribution ﬂaws in both the Calculator and the Karel domain. These
ﬂaws prevent models trained on these distributions from generalizing to other test distributions, even
if the are very simple. To resolve these problems, we propose a robust strategy for controlling and
evaluating the bias of synthetic data distributions over programs and speciﬁcations by deﬁning certain
random variables that capture desired features of the program and input spaces, such as the number
of parentheses in a calculator expression, and speciﬁcally manipulating their distributions. Equipped
with our method, deep networks exhibit an increase in cross-distribution test accuracy, at the expense
of a minor decrease in on-distribution test accuracy. We believe this methodology would lead to more
rigorous evaluation of the synthesis techniques and moreover, aid them in learning better models that
generalize well.

Equipped with a set of hand-designed salient random variables, we demonstrate the effectiveness of
homogenizing synthetic datasets over this set. One of the core limitations of our approach is that the
salient random variables are engineered by hand. This requires the scientist to have insights about the
structure of the training examples they are randomly generating. Therefore, a promising extension
of this algorithm is to automatically select which salient random variables to use, and automatically
compute these variables; potentially via the use of a general unsupervised learning algorithm.

We evaluate our method on two domains: the Karel DSL, and a calculator expression parser. There is
a natural question of whether the methods developed in this paper will improve out-of-distribution
generalization on applications other than program synthesis which use synthetic data—for example,
a convolutional neural network that receives renderings of a virtual environment, for the robotics
and vision domains mentioned in Section 2.1. Providing a thorough evaluation of our proposed
homogenization algorithm on alternative domains is a promising area for future work.

9

Published as a conference paper at ICLR 2019

ACKNOWLEDGEMENTS

This material is in part based upon work supported by the National Science Foundation under Grant
No. TWC-1409915, Berkeley Deep Drive, and DARPA D3M under Grant No. FA8750-17-2-0091.
Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the views of the National Science Foundation and DARPA.

REFERENCES

Rajeev Alur, Rastislav Bodík, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A.
Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-
guided synthesis. In Formal Methods in Computer-Aided Design, FMCAD 2013, Portland, OR,
USA, October 20-23, 2013, pp. 1–8, 2013.

Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.

Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.

Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Deep API
programmer: Learning to program with apis. CoRR, abs/1704.04327, 2017. URL http://
arxiv.org/abs/1704.04327.

Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakr-
ishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, and Vincent
Vanhoucke. Using simulation and domain adaptation to improve efﬁciency of deep robotic
grasping. CoRR, abs/1709.07857, 2017. URL http://arxiv.org/abs/1709.07857.

Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging

grammar and reinforcement learning for neural program synthesis. In ICLR, 2018.

Jonathon Cai, Richard Shin, and Dawn Song. Making neural programming architectures generalize
via recursion. CoRR, abs/1704.06611, 2017. URL http://arxiv.org/abs/1704.06611.

Paul F. Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin,
Pieter Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning
deep inverse dynamics model. CoRR, abs/1610.03518, 2016. URL http://arxiv.org/abs/
1610.03518.

Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and
Pushmeet Kohli. Robustﬁll: Neural program learning under noisy I/O. In ICML, pp. 990–998,
2017.

Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan
Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction.
CoRR, abs/1608.04428, 2016. URL http://arxiv.org/abs/1608.04428.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines.

arXiv preprint

arXiv:1410.5401, 2014.

Sumit Gulwani, William R. Harris, and Rishabh Singh. Spreadsheet data manipulation using
examples. Commun. ACM, 55(8):97–105, 2012. doi: 10.1145/2240236.2240260. URL http:
//doi.acm.org/10.1145/2240236.2240260.

Sumit Gulwani, José Hernández-Orallo, Emanuel Kitzelmann, Stephen H. Muggleton, Ute Schmid,
and Benjamin G. Zorn. Inductive programming meets the real world. Commun. ACM, 58(11):
90–99, 2015. doi: 10.1145/2736282. URL http://doi.acm.org/10.1145/2736282.

Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent

nets. In NIPS, pp. 190–198, 2015.

Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. CoRR, abs/1511.08228, 2015.

Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. ICLR,

2016.

10

Published as a conference paper at ICLR 2019

Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B. Tenenbaum, and Stephen Muggleton. Bias refor-
mulation for one-shot function induction. In ECAI 2014 - 21st European Conference on Artiﬁcial
Intelligence, 18-22 August 2014, Prague, Czech Republic - Including Prestigious Applications of
Intelligent Systems (PAIS 2014), pp. 525–530, 2014.

Zohar Manna and Richard J. Waldinger. Toward automatic program synthesis. Commun. ACM, 14

(3):151–165, 1971.

Stephen Muggleton. Inductive logic programming. New generation computing, 8(4):295–318, 1991.

Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer: Inducing latent programs
with gradient descent. CoRR, abs/1511.04834, 2015. URL http://arxiv.org/abs/1511.
04834.

Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet

Kohli. Neuro-symbolic program synthesis. ICLR, 2017.

Richard E Pattis. Karel the robot: a gentle introduction to the art of programming. John Wiley &

Sons, Inc., 1981.

Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer
of robotic control with dynamics randomization. CoRR, abs/1710.06537, 2017. URL http:
//arxiv.org/abs/1710.06537.

Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.
Asymmetric actor critic for image-based robot learning. CoRR, abs/1710.06542, 2017. URL
http://arxiv.org/abs/1710.06542.

Scott E. Reed and Nando de Freitas. Neural programmer-interpreters. CoRR, abs/1511.06279, 2015.

URL http://arxiv.org/abs/1511.06279.

Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russell
Webb. Learning from simulated and unsupervised images through adversarial training. CoRR,
abs/1612.07828, 2016. URL http://arxiv.org/abs/1612.07828.

Armando Solar-Lezama. Program sketching. STTT, 15(5-6):475–495, 2013. doi: 10.1007/

s10009-012-0249-7. URL https://doi.org/10.1007/s10009-012-0249-7.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and
Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.
URL http://arxiv.org/abs/1803.07416.

Richard J. Waldinger and Richard C. T. Lee. PROW: A step toward automatic program writing. In
Proceedings of the 1st International Joint Conference on Artiﬁcial Intelligence, Washington, DC,
USA, May 7-9, 1969, pp. 241–252, 1969.

Da Xiao, Jo-Yu Liao, and Xingyuan Yuan. Improving the universality and learnability of neural
programmer-interpreters with combinator abstraction. CoRR, abs/1802.02696, 2018. URL http:
//arxiv.org/abs/1802.02696.

Wojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2014. URL

http://arxiv.org/abs/1410.4615.

11

Published as a conference paper at ICLR 2019

A THE KAREL DOMAIN

Prog p := def main():s
Stmt s

Cond b

:= while(b) : s | repeat(r) : s | s1; s2
a | if(b) : s | if(b) : s1else : s2
|
:= markersPresent() | leftIsClear()
|
|

rightIsClear() | frontIsClear()
not(b)

Action a := move() | turnLeft() | turnRight()

pickMarker() | putMarker()

|
:= 0 | 1 | · · · | 19

Cste r

GridWidth : m
GridHeight: n
Markers: {(i, j, k)l}l
Walls: {(i, j)t}t
KarelLoc: (i, j)
Orientation: d ∈ {N,S,E,W}
2 ≤ m, n ≤ 16; i ≤ m;
j ≤ n; 1 ≤ k ≤ 9

(a)

(b)

Figure 4: (a) The DSL of Karel programs taken from Bunel et al. (2018), and (b) a declarative
speciﬁcation of the space of valid input worlds for Karel programs.

A declarative speciﬁcation of the space of valid input worlds to Karel programs is shown in Figure 4(b).
As with Bunel et al. (2018), we assume a bound on the input grid size to be 2 ≤ m, n ≤ 16. Each
cell (i, j) in a grid can either be empty, contain an obstacle (i.e. a wall, speciﬁed by the list Walls),
or contain k ≤ 9 markers (deﬁned using the list Markers). The agent starts at some cell denoted by
KarelLoc in the grid (which may contain markers but no obstacle) with a particular orientation
direction denoted by Orientation.

The grammar for the Karel DSL we consider in this work is shown in Figure 4(a). The DSL allows the
Karel agent to perform a move action to move one step in the grid in the direction of the orientation,
actions turnLeft and turnRight to change its orientation direction, and actions pickMarker
and putMarker to manipulate markers. The language contains if, ifElse, while constructs
with conditionals {front,left,right}IsClear, markersPresent, and their negations.
The repeat construct allows for a ﬁxed number of repetitions. Note that the language does not
contain any variables or auxiliary functions.

B SALIENT VARIABLE HOMOGENIZATION ALGORITHM

The following is a more formal description of the Algorithm described in Section 3, as well as proofs
of correctness, and an investigation of the (cid:15) parameter’s pratical effect.

B.1 FULL PSEUDOCODE

Let S be some space that is sampled by some original distribution q : S → [0, 1]. Let X be the space
of a salient variable which is calculated by ν : S → X; it is a ﬁnite set. Let ε be our tolerance.

procedure SAMPLEDHOMOGENIZE(q : S → [0, 1], ν : S → X, ε ∈ R, n ∈ N)

C ← {x → 0 : x ∈ X}
t ← 0
D ← {}
while |D| < n do

(cid:46) Set up a dictionary of counts of each value of X
(cid:46) The total number of samples seen
(cid:46) D is originally an empty multiset

s ← a sample from q
C[ν(s)] ← C[ν(s)] + 1
t ← t + 1
pmin ← minx∈X C[x]
t
pcurr ← C[ν(s)]
g ← pmin+ε
pcurr+ε
(cid:26)1 with probability g

t

h ←

0 with probability 1 − g

12

Published as a conference paper at ICLR 2019

if h = 1 then

D ← D ∪ {s}

end if
end while
return D
end procedure

B.2 PROOF OF CORRECTNESS

Let the initial sampling distribution be q and the resulting distribution be r. We use the notation
Pr[X = x] to refer to the probability that X = x given that S is sampled from the distribution r.
C[x] refers to the count for the salient variable value x ∈ X among past samples from q, as deﬁned in
SAMPLEDHOMOGENIZE.
Theorem. The Salient Variable Homogenization algorithm produces samples from a distribution
close to uniform. Formally, after 48 log 2|X|
pm|X|2ξ2 samples are drawn from distribution q, we have that the
resulting homogenized distribution satisﬁes

δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Pr[X = x] −

1
|X|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ ξ

with probability at least 1 − δ, where pm = minx∈X Pq[X = x] > 0 and ε has been set to 0.

Proof. We can simplify the probability as

Pr[X = x] =

Pq[X = x]/C[x]
z∈X Pq[X = z]/C[z]

(cid:80)

(for ε = 0; see Probability Simpliﬁcation Lemma below).
4 . We have that n > 3 log 2|X|

Let α = ξ|X|

Bounding Lemma we have that

following computations assume
1 − δ.
We have nPq[X=x]

C[x]

∈ [1 − α, 1 + α]. We also have

δ
α2pm
nPq[X=x]
C[x] − 1
C[x] − 1

by assumption and substituting in α. By the Count
(cid:12)
(cid:12)
(cid:12) ≤ α for all x with probability at least 1 − δ. The
(cid:12)
(cid:12)
(cid:12) ≤ α for all x and thus are valid with probability

nPq[X=x]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
|X|

(cid:88)

z∈X

(cid:12)
(cid:12)
(cid:12)
nP [X = z]/C[z] − 1
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
|X|

(cid:88)

(nP [X = z]/C[z] − 1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|nP [X = z]/C[z] − 1|

z∈X
(cid:88)

z∈X
(cid:88)

z∈X

α

1
|X|

1
|X|

=≤

=≤

= α

and thus 1
|X|

(cid:80)

z∈X

nP [X=z]

C[z] ∈ [1 − α, 1 + α].

Combining the previous two ranges via a division, we have

|X|Pr[X = x] =

nPq[X=x]
C[x]

1
|X|

(cid:80)

z∈X

nP [X=z]
C[z]

(cid:20) 1 − α
1 + α

,

1 + α
1 − α

(cid:21)

∈

We have that 1−α

1+α = 1 − 2α
2 − 2α > 1 for small α. Thus, we have that

1+α ≥ 1 − 2α ≥ 1 − 4α and 1+α

1−α = 1 + 2α
(cid:105)

2−2α ≤ 1 + 4α since
⊆ [1 − 4α, 1 + 4α] and therefore we have

1−α = 1 + 4α

(cid:104) 1−α
1+α , 1+α

1−α

13

Published as a conference paper at ICLR 2019

|X|Pr[X = x] ∈ [1 − 4α, 1 + 4α]

we thus have that

(cid:12)
(cid:12)Pr[X = x] − 1
(cid:12)
|X|

(cid:12)
(cid:12) ≤ 4α
(cid:12)

|X| = ξ, completing our proof.

Lemma (Count Bounding). We have that if n > 3 log 2|X|

δ
α2pm

samples have been drawn from q that

(cid:20)
∃x ∈ X,

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

nPq[X = x]
C[x]

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

≥ α

≤ δ

where pm = minx∈X Pq[X = x]

Proof. We can model C[x] as a sum of n independent Bernoulli trials with probability Pq[X = x].
Using Chernoff bound, we have that

P [C[x] ≥ nPq[X = x](1 + α)] ≤ e−α2nPq[X=x]/3

and

P [C[x] ≥ nPq[X = x](1 − (cid:112)2/3α)] ≤ e−α2nPq[X=x]/3

Thus, we have by union bound that

(cid:34)

P

nPq[X = x]
C[x]

≥

1
1 − (cid:112)2/3α

∨

nPq[X = x]
C[x]

≤

1
1 + α

(cid:35)

≤ 2e−α2nPq[X=x]/3

For α ≤ (cid:112)3/2 − 1 we have that
1+α = 1 − α

1
√
2/3α
1+α ≥ 1 − α. Thus, we can restate the previous inequality as

= 1 +

2/3α
√

= 1 +

2/3α

1−

1−

1

√

α√

3/2−α

≤ 1 + α and

(cid:20) nPq[X = x]
C[x]

P

≥ 1 + α ∨

nPq[X = x]
C[x]

(cid:21)

≤ 1 − α

≤ 2e−α2nPq[X=x]/3

or in other words

P

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

nPq[X = x]
C[x]

− 1

(cid:21)

≥ α

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2e−α2nPq[X=x]/3

Thus we can bound the RHS as

2e−α2nPq[X=x]/3 ≤ 2e−α2npm/3 < 2e−α2 3 log

2|X|
δ
α2pm

pm/3 = 2e− log 2|X|

δ =

δ
|X|

where the ﬁrst inequality is since Pq[X = x] ≥ pm and the second is since n > 3 log 2|X|
We can again apply union bound to get that

δ
α2pm

.

(cid:20)
∃x ∈ X,

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

nPq[X = x]
C[x]

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

≥ α

≤ |X|

δ
X = δ

Lemma (Probability Simpliﬁcation).

Pr[X = x] =

Pq[X = x]/C[x]
z∈X Pq[X = z]/C[z]

(cid:80)

14

Published as a conference paper at ICLR 2019

Proof.

Pr[X = x] =

(cid:88)

Pr[S = s]

s∈S:ν(s)=x

(cid:88)

s∈S:ν(s)=x
(cid:80)

g(s)q(s)
s(cid:48)∈S g(s(cid:48))q(s(cid:48))

(cid:80)

s∈S:ν(s)=x g(s)q(s)
(cid:80)
s(cid:48)∈S g(s(cid:48))q(s(cid:48))
(cid:80)

s∈S:ν(s)=x g(s)q(s)
(cid:80)

s(cid:48)∈S:ν(s(cid:48))=z g(s(cid:48))q(s(cid:48))

(cid:80)

z∈X

=

=

=

Since we can simplify

(cid:88)

g(s)q(s) =

(cid:88)

s∈S:ν(s)=x

=

=

we have that

miny∈X C[y]
C[ν(s)]

q(s)

s∈S:ν(s)=x
miny∈X C[y]
C[x]

miny∈X C[y]
C[x]

(cid:88)

q(s)

s∈S:ν(s)=x

Pq[X = x]

Pr[X = x] =

(cid:80)

=

(cid:80)

=

(cid:80)

(cid:80)

s∈S:ν(s)=x g(s)q(s)
(cid:80)

s(cid:48)∈S:ν(s(cid:48))=y g(s(cid:48))q(s(cid:48))

Pq[X = x]

y∈X
miny∈X C[y]
C[x]
miny∈X C[y]
z∈X
C[z]
Pq[X = x]/C[x]
z∈X Pq[X = z]/C[z]

Pq[X = z]

B.3 EFFICIENCY ANALYSIS

We show that the number of samples from the original distribution required to produce a sample from
the homogenized distribution is O( 1

ε ) in expectation.

In the Salient Variable Homogenization algorithm, we have the probability of not rejecting a given
sample as g(s) = minx P [X=x]+ε

P [X=X(s)]+ε . We know that

g(s) =

minx P [X = x] + ε
P [X = v(s)] + ε

≥

ε
P [X = v(s)] + ε

≥

ε
1 + ε

Since each sample is independent, we can model this as a geometric distribution, and thus we have
that the expected number of tries t = 1
ε . We thus have that in expectation, we need to
sample O( 1

ε ) samples from the original distribution to produce one homogenized sample.

g(s) ≤ 1 + 1

B.4 EMPIRICAL EFFECT OF VARYING ε

The solid line is an upper bound ε
1+ε derived in Section B.3. Seen in the measured samples for
different values of ε, the upper bound appropriately reﬂects the maximum height of the samples, with

15

Published as a conference paper at ICLR 2019

Figure 5: Number of samples required by salient variable homogenization parameterized by an ε
before a new sample is returned.

Original

83.83%
78.25%

T2T
DCFG

ε = 0.025

ε = 0.050

ε = 0.100

ε = 0.200

+2.84pp
+5.00pp

+1.31pp
+4.50pp

+1.33pp
+3.54pp

+2.45pp
+3.42pp

Table 4: Improvements in Calculator performance with homogenized datasets of various sampling
parameters ε. See Section 6 for details on performance metrics.

very little remaining space on the DCFG dataset and some but not much on the T2T dataset, and is
thus a close bound. As the values of ε → ∞ the bound approaches the limit 1, indicating no samples
are rejected by the algorithm.

Increasing the parameter ε has the theoretical affect of causing the homogenized distribution to
deviate more from uniform in its salient random variables, as shown in B.3. In practice, we discover
that performance boosts tend to decrease with increasing ε, although the effect was not as pronounced
on the T2T dataset, potentially because the unhomogenized T2T distribution is closer to uniform and
thus homogenization has a limited effect for any larger ε values.

B.5 EMPIRICAL EVIDENCE FOR INCREASE IN UNIFORMITY

Empirically, the Salient Variable Homogenization algorithm led to increases in uniformity in the
variable being homogenized. We measure uniformity by KL divergence between the distribution
being measured and the uniform distribution. A table of relative improvements is given in Table 5.

Length

42.98%
46.68%

DCFG
T2T

Max Depth Mean Depth

#Operations

#Parens

30.77%
30.45%

27.05%
13.99%

43.95%
38.82%

23.63%
36.91%

Table 5: Percentage reductions in KL-divergence from uniform of the given salient variable when
homogenized at ε = 0.025.

16

