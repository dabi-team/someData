2
2
0
2

n
u
J

2

]

G
L
.
s
c
[

2
v
0
6
9
3
0
.
5
0
2
2
:
v
i
X
r
a

ğ›¼NAS: Neural Architecture Search using Property Guided
Synthesis

CHARLES JINâˆ—, MIT, US
PHITCHAYA MANGPO PHOTHILIMTHANA, Google, US
SUDIP ROYâˆ—, Cohere, US

In the past few years, neural architecture search (NAS) has become an increasingly important tool within the
deep learning community. Despite the many recent successes of NAS, however, most existing approaches
operate within highly structured design spaces, and hence explore only a small fraction of the full search
space of neural architectures while also requiring significant manual effort from domain experts. In this work,
we develop techniques that enable efficient NAS in a significantly larger design space. To accomplish this, we
propose to perform NAS in an abstract search space of program properties. Our key insights are as follows: (1)
the abstract search space is significantly smaller than the original search space, and (2) architectures with
similar program properties also have similar performance; thus, we can search more efficiently in the abstract
search space. To enable this approach, we also propose a novel efficient synthesis procedure, which accepts a
set of promising program properties, and returns a satisfying neural architecture. We implement our approach,
ğ›¼NAS, within an evolutionary framework, where the mutations are guided by the program properties. Starting
with a ResNet-34 model, ğ›¼NAS produces a model with slightly improved accuracy on CIFAR-10 but 96%
fewer parameters. On ImageNet, ğ›¼NAS is able to improve over Vision Transformer (30% fewer FLOPS and
parameters), ResNet-50 (23% fewer FLOPS, 14% fewer parameters), and EfficientNet (7% fewer FLOPS and
parameters) without any degradation in accuracy. 1

Additional Key Words and Phrases: Neural Architecture Search, Program Synthesis, Abstract Interpretation

1 INTRODUCTION
Neural architecture search (NAS) has been proven to discover many state-of-the-art neural network
architectures. Specifically, rather than relying on hand-designed neural architectures, which is
time-consuming and subject to human biases and failures, various methods have been proposed to
automatically search for neural architectures within a search space. These approaches have yielded
significant improvements in both raw accuracy and navigating the trade-off with other metrics of
practical interest such as parameter count and latency.

Despite the recent successes of NAS, the general NAS problem presents many key technical
challenges that have yet to be fully addressed. First, the search space of deep neural networks is
prohibitively large. Even if the high-level structure of a network is fixed, one still needs to select
between various operation variants (e.g., various types of convolutions) and parameter settings
(e.g., filter sizes, layer widths, layer depths). Second, the reward function is extremely sparse, i.e., a
randomly constructed network is unlikely to perform well. Finally, evaluating the reward function
is also computationally expensive, since this generally requires training the proposed network to
convergence on a target dataset from scratch.

To contend with these challenges, most existing approaches are applied to highly structured
search spaces. For instance, a common strategy is to first manually specify a fixed architecture,
then use NAS to tune selected parameters of the operators (e.g., depths and filter sizes) [Liu et al.
2018b,a; Tan and Le 2019, 2021; Zoph et al. 2018]. It has been observed that such restricted search

âˆ—Work performed while at Google.
1Our code is available at https://github.com/google-research/google-research/tree/master/abstract_nas.

Authorsâ€™ addresses: Charles Jin, CSAIL, MIT, Cambridge, MA, US, ccj@csail.mit.edu; Phitchaya Mangpo Phothilimthana,
Google, Mountain View, CA, US, mangpo@google.com; Sudip Roy, Cohere, Palo Alto, CA, US.

 
 
 
 
 
 
2

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

spaces also tend to bypass the second challenge, i.e., even randomly selected networks perform
quite well [Bender et al. 2020; Li and Talwalkar 2020; Yu et al. 2019].

However, imposing such structure comes at a direct cost of the expressiveness of the search
space, and it is not clear how prior approaches could be generalized to a fully unstructured search
space. Furthermore, while existing approaches have delivered impressive results when used to
tune existing architectures, to date, major architectural changes (AlexNet [Krizhevsky et al. 2012],
ResNet [He et al. 2015], Transformers [Vaswani et al. 2017], etc.) have still been driven by the
insight of human researchers.

More recently, several new approaches to NAS have been proposed that avoid the need to define
the highly structured search space by iteratively mutating a randomly selected subgraph inside a
neural network architecture, using an evolutionary search mechanism [Real et al. 2019, 2020; So et al.
2021]. Given the number of possible operations, each with different functional and performance
characteristics, directly generating a random subgraph is unlikely to perform well in place of
the original subgraph. To counteract this, prior approaches limit the scope of their mutations to
only small changes. For instance, Primer [So et al. 2021] uses mutations such as changing a single
parameter in a random operation, or swapping two nearby operations. However, restricting the
mutations to small changes requires many iterations to discover high-quality, novel architectures,
and in many cases, the search may struggle to escape local minima.

1.1 Our Approach
In this work, we present a new evolutionary approach to NAS. Prior evolutionary approaches to
NAS evolve new architectures by applying stochastic mutations directly on the computation graphs
of deep neural networks. Each such mutation produces a new architecture, which is evaluated for
accuracy, then inserted into the population as a potential starting point for future mutations.

In contrast to the prior approaches, our approach can search for architectures over significantly
larger search spaces and does so efficiently by making high-quality, substantial changes to the
architecture in each mutation step. To achieve this, we propose to perform the mutations in an
abstract space of program properties. In particular, we introduce shape, depth, and mixing properties
for neural networks, which we describe in Section 3. Our key insights are as follows:

(1) Since multiple concrete architectures share the same set of program properties, the abstract

search space is significantly smaller in size than the original search space.

(2) Architectures which have similar program properties also have similar performance. This
enables search algorithms which rely on an assumption of locality in the reward function
(e.g, hill-climbing or evolutionary algorithms) to be applied to the full search space.

Our approach, named ğ›¼NAS, begins by selecting a random subgraph of the computation graph
for mutation. Then, we infer a set of program properties for the selected subgraph, and apply
stochastic mutations on the properties of the subgraph. Finally, we synthesize a new subgraph
that satisfies the mutated properties, and insert the new subgraph in place of the old subgraph to
produce a new architecture.

Fig. 1 illustrates an example of a mutation synthesized by ğ›¼NAS during the evolution of the
Vision Transformer architecture [Dosovitskiy et al. 2021] for image classification. To perform this
mutation, we first select a random subgraph from the computation graph of the parent architecture
(step 1, Fig. 1; the selected subgraph is enclosed in a gray box).

Next, we abstract the subgraph into a set of program properties (step 2, Fig. 1). Since deep neural
network graphs can be computationally expensive to execute, one crucial requirement is that our
properties can be inferred statically, that is, without executing the computations on real data. Of
course, it is also important that the abstract properties capture semantically relevant information

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

3

Fig. 1. Our approach proposes a new neural network architecture by (1) randomly selecting a subgraph of an
existing architecture, (2) inferring program properties of the subgraph, (3) mutating the program properties,
and (4) synthesizing a new subgraph that satisfies the mutated program properties. We apply these four steps
iteratively. This particular mutation is found during our ImageNet experiment when starting from a Vision
Transformer architecture. A descendant of this model (consisting of 4 total mutations) ultimately decreases
FLOPS and parameter count by 28% and 30% respectively, while slightly increasing accuracy.

about the underlying computation. For instance, one of our properties is the depth of the subgraph
in terms of the number of alternating linear and nonlinear layers; here, the selected subgraph has a
depth of 3. We then apply stochastic mutations at the level of abstract program properties to obtain
new program properties (step 3, Fig. 1; the mutated depth property is marked by a red box).

The mutated properties are then concretized back into a subgraph (step 4, Fig. 1). The new
subgraph is inserted in place of the original subgraph (marked by the green box), yielding a new,
mutated architecture (called a child), which is ready to be evaluated and added to the population
for the next round of evolution. Note that our search space contains over 1402 possible operations
for each node. To satisfy the new depth property of 4, the synthesized subgraph must contain at
least 4 operations, yielding over 3.8 billion legal subgraphs. In order to perform the concretization
efficiently, in Section 4 we propose a novel program synthesis algorithm. Using our techniques,
synthesizing the subgraph in Fig. 1 (with 5 nodes) took only 88 seconds, which is a fraction of the
time needed to evaluate the resulting architecture (which involves training the model on a dataset).
The mutation displayed in Fig. 1 is ultimately a part of the 278th individual (evolved using 4 total
mutations) that improves the baseline architecture by decreasing FLOPS by 28% and parameter
count by 30%, while slightly increasing accuracy. Note that this single mutation is equivalent to 8
steps in the concrete space using previous techniques like Primer.

Hence, the primary benefit of our approach is that a single mutation in the abstract space can
result in a new architecture that would otherwise require many individual concrete mutations to
achieve. Intuitively, by requiring the synthesized subgraph to satisfy similar properties to the original
subgraph, our search is biased toward high quality replacements. Without restricting to small
changes, our method is able to explore new architectures far more quickly, and requires substantially

4

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

fewer iterations of evolutionary search compare to prior approaches; in our experiments, we only
require evolving 800 individuals to produce a population of novel, high quality architectures for
image classification on the ImageNet dataset, whereas Primer evolves nearly 25,000 individuals on
the task of language modeling.

1.2 Our Contributions
Our first contribution is a set of program properties defined over arbitrary neural architectures,
which constitute the abstract search space in which the search is performed. We also introduce
efficient algorithms for inferring these program properties; crucially, we infer these properties
statically, which enables the inference to occur exclusively on a CPU host, decoupling the costly
evaluation process from the rest of the search procedure.

Our second contribution is an efficient synthesis procedure, which accepts a set of program
properties, and returns a satisfying neural architecture. In general, inverting an abstraction function
is non-trivial; in the worst case, the only technique which is guaranteed to succeed is a brute-force
guess-and-check strategy that simply enumerates all possible architectures, and returns the first
satisfying architecture. However, by carefully crafting our synthesis procedure to leverage a notion
of distance to the program properties, our procedure is able to synthesize a satisfying architecture
exponentially faster than the naÃ¯ve enumerative strategy.

To empirically validate our approach, we implement our techniques within a basic evolutionary
search and evaluate performance on image classification tasks. Starting with a ResNet-34 model
on the CIFAR-10 dataset, ğ›¼NAS produces a model with slightly improved accuracy but 96% fewer
parameters. On the ImageNet dataset, ğ›¼NAS is able to improve over Vision Transformer (30% fewer
FLOPS and parameters), ResNet-50 (23% fewer FLOPS, 14% fewer parameters), and EfficientNet (7%
fewer FLOPS and parameters) without any degradation in accuracy. When compared against the
current state-of-the-art search mechanisms from Primer [So et al. 2021] and AutoML-Zero [Real
et al. 2020], ğ›¼NAS discovers significantly more Pareto optimal models.

2 PROBLEM FORMULATION
This section introduces some basic concepts used by our synthesis algorithm. We denote the
arbitrary, fixed space of programs as P.

2.1 Program Properties
Program properties form the basis of our synthesis problem.

Definition 2.1. A program property Î  = (V, â‰¤, ğ›¼) over the space of programs P consists of a
set V of program property values, with a partial order relation â‰¤ over V, and an abstraction function
ğ›¼ : P â†¦â†’ V that maps programs to property values.

Definition 2.2. A program ğ‘ âˆˆ P satisfies a program property value ğ‘£ âˆˆ V, denoted ğ‘ |= ğ‘£, if

ğ›¼ (ğ‘) â‰¥ ğ‘£.

For instance, the previous section introduces a depth property for deep neural networks. This
property takes values V in the nonnegative integers, where â‰¤ is the usual relation on integers, and
ğ›¼ maps a subgraph to its depth. For a feedforward neural network, the depth is simply the number
of alternating linear and non-linear operators in the network, and a subgraph satisfies the depth ğ‘‘
if its depth is at least ğ‘‘.

We can immediately generalize this definition to a setting where we are given a set of program

properties P = {Î ğ‘– = (Vğ‘–, â‰¤ğ‘–, ğ›¼ğ‘– )}ğ‘

ğ‘–=1:

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

5

Definition 2.3. A program ğ‘ âˆˆ P satisfies a set of program property values ğ‘† = {ğ‘£ğ‘– | ğ‘£ğ‘– âˆˆ Vğ‘– }ğ‘

ğ‘–=1,

denoted as ğ‘ |= ğ‘†, if for every ğ‘£ğ‘– âˆˆ ğ‘†, we have that ğ‘ |= ğ‘£ğ‘– .

We usually drop the subscript ğ‘– of the partial ordering â‰¤ğ‘– when it is clear from context.

2.2 Program Synthesis
Program transformations are the basic unit of our synthesis algorithm. We formalize the task of
program synthesis as the problem of producing a sequence of transformations that transform some
initial program ğ‘0 into ğ‘âˆ— satisfying the desired program properties ğ‘†.

Definition 2.4. A program transformation ğ‘¡ âˆˆ T : P â†¦â†’ P is a map from programs to

programs.

Definition 2.5. The synthesis task (ğ‘0, ğ‘£) is feasible if there exists some sequence ğ‘¡1, . . . , ğ‘¡ğ‘› âˆˆ T

such that (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1)(ğ‘0) |= ğ‘†.

In this work, we will always take the initial program ğ‘0 to be the identity program: id(ğ‘¥) = ğ‘¥ for
all inputs ğ‘¥. We will also assume that we are given a finite set ğ¸ of primitive transformations;
the space of program transformations T is then the set of all finite strings with ğ¸ as the alphabet:
T = ğ¸âˆ—. For synthesizing subgraphs of deep neural networks, the primitive transformations ğ¸
consist of appending one of the basic operations to the partial subgraph. Given any set of program
properties values ğ‘†, our synthesizer incrementally appends operations without backtracking, until
the resulting program satisfies the given properties. The key to our algorithmâ€™s success is defining
an appropriate notion of distance from programs to the desired property, such that we can always
decrease the distance by selecting a transformation from a small fixed subset ğ‘‡ âŠ‚ T . Hence, each
step of the synthesis algorithm simply enumerates over the transformations in ğ‘‡ , and selects
a transformation that is guaranteed to decrease the distance. This process is repeated until the
subgraph achieves a distance of 0, satisfying the target property.

For instance, to satisfy the depth property in Fig. 1, we can simply alternate between appending
linear and non-linear operations. Clearly, this strategy can always increase the depth of the subgraph
until the property is satisfied. Our key insight is that this seemingly simple procedure also applies
to program properties with more complex semantics.

As each synthesis step progressively brings the program closer to satisfying the property, we call
our technique progressive program synthesis. Notice that this entire process takes time linear
in the length of the synthesized subgraph, whereas the number of possible subgraphs is exponential
in the length.

2.3 Property Inference
ğ›¼NAS performs property inference (ğ›¼) during the (step 2) abstraction and (step 4) synthesis shown in
Fig. 1. During abstraction, we need to infer the properties of the selected subgraph only once. During
synthesis, however, our algorithm relies on inferring the properties of each partially synthesized
program after applying each transformation ğ‘¡.

To infer program properties efficiently, we extend the semantics of ğ›¼ from programs ğ‘ âˆˆ P to
transformations ğ‘¡ âˆˆ T . The idea is that if we already know ğ›¼ (ğ‘) = ğ‘£, instead of computing ğ›¼ (ğ‘¡ (ğ‘))
from scratch, we can leverage the partial result ğ‘£ and compute the effects of ğ‘¡ on ğ‘£ abstractly. We
formalize this intuition using the theory of abstract interpretation for transformations. In particular,
when computing the effects of a program transformation ğ‘¡ âˆˆ T on a program property value ğ‘£, we
need to preserve the soundness of the |= relationship in the following sense:

6

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 2. The key soundness property of abstract interpretation: ğ‘¡ğ›¼ (ğ‘¢) = ğ‘£ implies that âˆ€ğ‘ |= ğ‘¢, ğ‘¡ (ğ‘) |= ğ‘£.
Superscript ğ›¼ denotes the abstract interpretation of transformation ğ‘¡ with respect to a program property Î .

Definition 2.6. Given a program property Î  = (V, â‰¤, ğ›¼), an abstract interpretation of a pro-
gram transformation ğ‘¡ âˆˆ T is a function ğ‘¡ ğ›¼ : V â†¦â†’ V such that ğ‘¡ ğ›¼ (ğ‘¢) = ğ‘£ implies that âˆ€ğ‘ |= ğ‘¢,
ğ‘¡ (ğ‘) |= ğ‘£.

Henceforth, we use the superscript ğ›¼ to identify transformations ğ‘¡ over concrete subgraphs with
an abstract interpretation ğ‘¡ ğ›¼ over program properties. Fig. 2 illustrates the key soundness property
of abstract interpretation as a commutative diagram.

3 PROGRAM PROPERTIES FOR NEURAL ARCHITECTURE SEARCH
In this section, we present the set of program properties used by ğ›¼NAS to guide the neural archi-
tecture search. Before identifying the specific properties, we first describe the key considerations
in defining such properties for NAS.

As outlined in Section 1.1, a key step in evolutionary NAS is how architectures are mutated over
the course of the search. Thus, it is essential to find a good subgraph substitution that, on expectation,
yields a better child architecture. Since our approach for generating substitutions involves inferring
the properties of the original subgraph, mutating these properties, and synthesizing new subgraphs
from the mutated properties, choosing the right set of properties is critical.

Specifically, our objectives in designing the program properties are threefold. First, we want
subgraphs satisfying the same program properties to have similar performance characteristics, so
that we can avoid spending time on proposing and evaluating many similar subgraphs, enabling
faster exploration. Second, we want subgraphs that satisfy similar (but not necessarily identical)
program properties to perform similarly on the learning task to ensure that subgraphs synthesized
from mutated properties are high-quality candidates. Finally, since the substitution step involves
both inferring these program properties for a candidate subgraph, and synthesizing an alternate
subgraph based on mutated properties, both of these steps must be computationally efficient.

In the remainder of this section, we describe formally the subgraph properties we propose in
this work, as well as the associated efficient inference algorithms based on abstract interpretation.
For clarity of presentation, we assume that the subgraphs have only a single input and output;
to generalize our definitions to subgraphs with multiple inputs or multiple outputs, we simply
compute the property for every input-output pair.

3.1 Mixing Property
The mixing property captures key information about the expressivity of a subgraph as a linear
operator. Intuitively, if a subgraph consists purely of linear operators (and hence is linear as a
whole), then each element of the output is a linear combination of elements in the input; the mixing

(cid:83)(cid:87)(cid:87)(cid:11)(cid:83)(cid:12)(cid:88)(cid:89)(cid:87)(cid:3)(cid:302)(cid:17340)(cid:17340)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:69)(cid:86)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:83)(cid:85)(cid:72)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:83)(cid:85)(cid:82)(cid:83)(cid:72)(cid:85)(cid:87)(cid:76)(cid:72)(cid:86)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:86)ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

7

property summarizes information about which elements of the input have nonzero coefficients in
the linear combination.

An example. Consider a 3x3 convolutional layer:
ğ¶ğ‘–âˆ‘ï¸

1
âˆ‘ï¸

1
âˆ‘ï¸

ğ‘‚ [ğ‘, ğ‘ğ‘œ, ğ‘¤ğ‘œ, â„ğ‘œ ] =

ğ¼ [ğ‘, ğ‘ğ‘–, ğ‘¤ğ‘œ + ğ‘˜ğ‘¤, â„ğ‘œ + ğ‘˜â„] âˆ— ğ¾ [ğ‘ğ‘œ, ğ‘ğ‘–, ğ‘˜ğ‘¤ + 1, ğ‘˜â„ + 1]

and a dense layer:

ğ‘ğ‘– =0

ğ‘˜â„=âˆ’1

ğ‘˜ğ‘¤ =âˆ’1

ğ‘‚ [ğ‘, ğ‘ğ‘œ, ğ‘¤ğ‘œ, â„ğ‘œ ] =

ğ¶ğ‘–âˆ‘ï¸

ğ‘ğ‘– =0

ğ¼ [ğ‘, ğ‘ğ‘–, ğ‘¤ğ‘œ, â„ğ‘œ ] âˆ— ğ‘Š [ğ‘ğ‘œ, ğ‘ğ‘– ]

(1)

(2)

where ğ‘‚ is the output tensor, ğ¼ is the input tensor, ğ¾ is the kernel of the convolution, and ğ‘Š is the
projection weight matrix of the dense layer.

Intuitively, we want to capture the fact that the 3x3 convolutional layer is strictly more expressive
than a dense layer, since the dense layer is just a special case of the 3x3 convolution where only
the middle element of the convolutional filter is nonzero, i.e., any dense layer can be written as a
convolutional layer (but not vice-versa). This observation is reflected in the property that every
output element of a convolutional layer depends on a full slice of the input channel dimension, as
well as many elements of the input spatial dimensions; however every output element of a dense
layer depends on a full slice of the input channel dimension, but only a single element of the input
spatial dimension. By a similar token, a fully connected layer is strictly more expressive than both
the convolutional and dense layers. We formalize these intuitions in the mixing property, which is
decomposed into two subproperties.

3.1.1 Pairing dimensions. The pairing subproperty captures coarse-grained information about
which dimensions of the input tensor contribute to which dimensions of the output tensor. For
instance, consider a convolutional layer applied to an input with two spatial dimensions and
one channel dimension (e.g., a standard 2D image with three color channels). Then the channel
dimension of the output depends on the channel dimension of the input; however, the channel
dimension of the output does not depend on the spatial dimensions of the input, because the
convolutional kernel has a limited receptive field in the spatial dimensions.

More explicitly, given an input tensor and an output tensor, we say the operation pairs a dimension
of the input and a dimension of the output if, given a fixed slice of the output along the output
dimension, there is at least one element for each position in the input dimension which contributes
to the computation of the output slice.

The formal definition is as follows. Let ğ‘“ be a tensor-valued function which maps an input tensor
ğ´ of size (ğ‘1, . . . , ğ‘ğ‘›) to an output tensor ğµ of size (ğ‘1, . . . , ğ‘ğ‘š). In other words, for any element
ğµ [ğ‘œ1, . . . , ğ‘œğ‘š], where 1 â‰¤ ğ‘œğ‘– â‰¤ ğ‘ğ‘– for all 1 â‰¤ ğ‘– â‰¤ ğ‘š, we have a function ğ‘“ğ‘œ1,...,ğ‘œğ‘š defined as

ğµ [ğ‘œ1, . . . , ğ‘œğ‘š] = ğ‘“ğ‘œ1,...,ğ‘œğ‘š (ğ´)

(3)

where ğ‘“ğ‘œ1,...,ğ‘œğ‘š is a real-valued function over ğ‘›-dimensional tensor inputs ğ´.

Definition 3.1. An element ğ´[ğ‘–1, . . . , ğ‘–ğ‘›] of the input tensor contributes to the computation of

the element ğµ [ğ‘œ1, . . . , ğ‘œğ‘š] of the output tensor if
ğœ•ğ‘“ğ‘œ1,...,ğ‘œğ‘š
ğœ•ğ´[ğ‘–1, . . . , ğ‘–ğ‘›]

i.e., the partial derivative as a function of ğ´ is not identically zero.

(ğ´) â‰  0

(4)

8

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

(b) Locality. The inputâ€™s C dimension has all-to-
one locality with the paired output dimensions.
The inputâ€™s H and W dimensions have many-to-
one localities with the paired output dimensions.

(a) Pairing dimensions. Top: the inputâ€™s C dimension is
paired with the outputâ€™s C. Bottom: the inputâ€™s C and H
dimensions get paired with the outputâ€™s H dimension.

Fig. 3. Determining the mixing property of a 3x3 convolutional layer when applied to a 2D image (H x W)
with a channel (C) dimension.

Note that if ğ‘“ is a linear function, then the formula for ğµ [ğ‘œ1, . . . , ğ‘œğ‘š] can be expressed as a
linear combination over the elements of ğ´, and the partial derivative of ğ‘“ğ‘œ1,...,ğ‘œğ‘š with respect to
ğ´[ğ‘–1, . . . , ğ‘–ğ‘›] is just the coefficient of ğ´[ğ‘–1, . . . , ğ‘–ğ‘›] in the linear combination.

Definition 3.2. The preimage of the function ğ‘“ , denoted as ğ‘“ âˆ’1, is a map from sets of output
elements to sets of input elements, and is defined as follows. Let ğ‘† = {ğµ [ğ‘œ1,ğ‘—, . . . , ğ‘œğ‘š,ğ‘— ]} ğ‘— âˆˆğ½ be a set
of output elements, indexed by the set ğ½ . If ğ‘† consists of a single element ğµ [ğ‘œ1, . . . , ğ‘œğ‘š], then ğ‘“ âˆ’1(ğ‘†)
is the set of all input elements that contribute to the computation of ğµ [ğ‘œ1, . . . , ğ‘œğ‘š]. Otherwise,

ğ‘“ âˆ’1(ğ‘†) =

(cid:216)

ğ‘— âˆˆğ½

ğ‘“ âˆ’1({ğµ [ğ‘œ1,ğ‘—, . . . , ğ‘œğ‘š,ğ‘— ]})

(5)

Definition 3.3. Given a tensor ğµ of shape (ğ‘1, . . . , ğ‘ğ‘š), a slice along the ğ‘˜ğ‘¡â„ dimension, specified

by ğ‘š âˆ’ 1 indices {ğ‘œ1, . . . , ğ‘œğ‘˜âˆ’1, ğ‘œğ‘˜+1, . . . , ğ‘œğ‘š }, is the set of elements

ğµ [ğ‘œ1, . . . , ğ‘œğ‘˜âˆ’1, â€¢, ğ‘œğ‘˜+1, . . . , ğ‘œğ‘š] =

ğ‘ğ‘˜(cid:216)

ğµ [ğ‘œ1, . . . , ğ‘œğ‘˜âˆ’1, ğ‘œ, ğ‘œğ‘˜+1, . . . , ğ‘œğ‘š]

(6)

ğ‘œ=1
Definition 3.4. The function ğ‘“ pairs the ğ‘™ğ‘¡â„ dimension of the input tensor ğ´ with the ğ‘˜ğ‘¡â„ di-
mension of the output tensor ğµ if there exists a slice ğ‘†ğµ of the output tensor along the ğ‘˜ğ‘¡â„ di-
mension such that the preimage ğ‘“ âˆ’1 of ğ‘†ğµ contains one element in every position along the ğ‘™ğ‘¡â„
dimension of the input, i.e., for every 1 â‰¤ ğ‘– â‰¤ ğ‘ğ‘™ , there exists ğ‘–1, . . . , ğ‘–ğ‘™âˆ’1, ğ‘–ğ‘™+1, . . . , ğ‘–ğ‘› such that
ğ´[ğ‘–1, . . . , ğ‘–ğ‘™âˆ’1, ğ‘–, ğ‘–ğ‘™+1, . . . , ğ‘–ğ‘›] âˆˆ ğ‘“ âˆ’1(ğ‘†ğµ).

Fig. 3a illustrates how the pairing subproperty of a convolutional layer is determined for a
standard 2D image with an additional channel dimension. We can collect the pairing property over
all pairs of input and output dimensions into a matrix in the manner of Fig. 4a, which summarizes
the complete pairing subproperty of a convolutional layer for a batch of 2D images.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

9

In

B

H W C

1
B
H
0
W 0
0
C

0
1
0
0

0
0
1
0

1
1
1
1

Out

In

B

H W C

o
B
x
H
W x
x
C

x
m
x
x

x
x
m
x

a
a
a
a

Out

(a) Pairing dimensions. â€œ1â€ indicates a pairing and â€œ0â€
indicates no pairing. For instance, the output batch
dimension is paired with the input channel dimen-
sion.
Fig. 4. Matrices representing the mixing property of a 3x3 convolutional layer with a batch dimension (B).

(b) Locality. The locality types are all-to-one (a),
many-to-one (m), and one-to-one (o) pairings, or no
pairing (x). For instance, the output H dimension
depends on the input H dimension with m locality.

Locality. Consider again a dense layer versus a convolutional layer. The convolutional layer
3.1.2
has the same pairing subproperty as the dense layer. The locality subproperty captures a more
fine-grained notion of the contributing elements which distinguishes the two operation types. In
particular, pairing subproperty considers the preimage of slices to capture the relationship between
input and output dimensions, whereas the locality subproperty uses the preimage of a single element.
We distinguish between all-to-one, many-to-one, and one-to-one localities. For instance, a dense
layer has a one-to-one pairing between the spatial dimensions and an all-to-one pairing between the
channel dimensions, while a convolution has a many-to-one pairing between the spatial dimensions
and an all-to-one pairing between the channel dimensions. We have the following formal definition:

Definition 3.5. Let the function ğ‘“ pair the ğ‘™ğ‘¡â„ dimension of the input with the ğ‘˜ğ‘¡â„ dimension of

the output. The pairing has:

(1) all-to-one locality, if there exists an element indexed by {ğ‘œ1, ..., ğ‘œğ‘š } of the output tensor ğµ
such that the preimage ğ‘†ğ´ = ğ‘“ âˆ’1({ğµ [ğ‘œ1, ..., ğ‘œğ‘š]}) contains one element in every position
along the ğ‘™ğ‘¡â„ dimension of the input tensor, i.e., for every 1 â‰¤ ğ‘– â‰¤ ğ‘ğ‘™ , there exists indices
ğ‘–1, . . . , ğ‘–ğ‘™âˆ’1, ğ‘–ğ‘™+1, . . . , ğ‘–ğ‘› such that ğ´[ğ‘–1, . . . , ğ‘–ğ‘™âˆ’1, ğ‘–, ğ‘–ğ‘™+1, . . . , ğ‘–ğ‘›] âˆˆ ğ‘†ğ´.

(2) many-to-one locality, if there exists an element of the output tensor such that the preimage
contains an element in more than one (but less than all) positions along the ğ‘™ğ‘¡â„ dimension
of the input tensor.

(3) one-to-one locality, if there exists an element of the output tensor such that the preimage
contains an element in exactly one position along the ğ‘™ğ‘¡â„ dimension of the input tensor.

Clearly, a pairing between an input dimension and an output dimension is exactly one of all-
to-one (a), many-to-one (m), or one-to-one (o). Fig. 3b illustrates how the locality subproperty
of a convolutional layer is determined. Similar to the pairing property, we represent the locality
subproperty over all pairs of input and output dimensions as a matrix as in Fig. 4b, which displays
the full locality subproperty of a convolutional layer (with an additional batch dimension); a dense
layer would have the same matrix, except the many-to-one entries in the spatial dimensions would
be replaced by one-to-one entries. The additional symbol x denotes dimensions which are not
paired (and hence have no defined locality).

Finally, we introduce a partial order â‰¤ on the mixing properties as follows: ğ‘ƒ â‰¤ ğ‘„ if and only if
properties ğ‘ƒ and ğ‘„ have the same number of input and output dimensions, and ğ‘ƒ â‰¤ ğ‘„ element-
wise on all input-output pairs, where x < o < m < a. The partial order succinctly captures the
aforementioned relationship between dense, convolutional, and fully connected layers, where <
corresponds to â€œless expressiveâ€.

10

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

z

o

x
o
m
a

m

x
m
m
a

a

x
a
a
a

x

x
x
x
x

y

x
o
m
a

Fig. 5. Definition of ğ‘¦ âˆ— ğ‘§ over the elements
{x, o, m, a}, representing the locality prop-
erty, used in Lemma 3.6

Fig. 6. Depth property of a subgraph with inputs ğ¼1, ğ¼2 and
outputs ğ‘‚1, ğ‘‚2. Depth to ğ‘‚1 from ğ¼2 is defined to be 0 since
there is no path from ğ¼2 to ğ‘‚1. Since there are two paths
from ğ¼2 to ğ‘‚2, depth(ğ‘‚2, ğ¼2) is the max of the two paths.

Note that the locality subproperty cannot differentiate between an identity operation and a
transpose operation, and more generally, any â€œreshapingâ€ operation; for that we must use the
pairing subproperty. Hence, the two subproperties are complementary, and together give a more
complete view of the subgraph as a linear operator.

Concrete Inference. We first describe how to infer the mixing property of an operation using
concrete values. The key is that the â€œcontributes toâ€ relationship between input and output elements
is defined with respect to gradients; since our domain comes with a built-in back-propagation, we
can simply leverage the existing machinery to perform the inference efficiently. More explicitly, for
the pairing subproperty, we compute the gradient of a slice of the output tensor with respect to
the input tensor by summing along the slice of the output tensor. Any entries in the input tensor
with non-zero gradients are said to contribute to the slice. Evaluating the locality subproperty is
similar, except that we compute the gradient of the input tensor with respect to a single element of
the output tensor. Furthermore, due to the regularity of the individual operators used to construct
deep neural network graphs are highly regular, it suffices to evaluate the pairing and locality
subproperties at the center slice and element, respectively.

Abstract Interpretation. We next describe an abstract interpretation of the mixing property. Let ğ›¼ğ‘€
denote the property inference function which maps concrete subgraphs to their mixing properties.
Consider two programs ğ‘ and ğ‘. We will treat ğ‘ as the program transformation (appending ğ‘ to ğ‘)
for which we want to derive an abstract interpretation, i.e., we would like to infer a lower bound
for ğ›¼ğ‘€ (ğ‘ â—¦ ğ‘) given ğ›¼ğ‘€ (ğ‘) = ğ‘¢ without direct access to ğ‘.

Recall that a mixing property is represented by a matrix with elements x, o, m, a, (cf. Fig. 4b). The

abstract interpretation ğ‘ğ›¼

ğ‘€ can be defined as a matrix multiplication on ğ‘¢ and ğ›¼ğ‘€ (ğ‘).

Lemma 3.6. ğ‘ğ›¼

ğ‘€ (ğ‘¢) := ğ›¼ğ‘€ (ğ‘) Ã— ğ‘¢ is an abstract interpretation of ğ‘, where Ã— is a matrix multiplication
with + and âˆ— defined over the elements {x, o, m, a} as: ğ‘¦ + ğ‘§ = max(ğ‘¦, ğ‘§); and ğ‘¦ âˆ— ğ‘§ is given by the
look-up table in Fig. 5.

We defer the proof to Appendix D.1. Finally, to actually infer the mixing property of a subgraph,
we first infer the properties of each concrete operation within the subgraph, and then compose
them using the abstract interpretation. To reduce overhead, we also cache the properties for each
unique operation by hashing its specification (e.g., operation type and parameter values).

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

11

3.2 Depth Property
To complement the mixing property, which captures the expressivity of a subgraph as a linear
operator, the depth property is meant to capture the properties of the subgraph as a non-linear
function. More specifically, given an input and an output of a subgraph, the depth property specifies
the maximum depth over all paths from the input to the output, where the depth of a path is
defined as the number of alternating linear and non-linear operations along that path. Fig. 6 gives
an example of computing the depth property for a subgraph.

The depth property takes property values in the nonnegative integers. The partial order â‰¤ is
just the usual ordering on integers. In other words, given a depth property value ğ‘‘, any subgraph
whose depth property value matches or exceeds ğ‘‘ is said to satisfy the depth property.

Concrete Inference. An operation ğ‘“ is linear if ğ‘ğ‘“ (ğ‘¥) + ğ‘ ğ‘“ (ğ‘¦) = ğ‘“ (ğ‘ğ‘¥ + ğ‘ğ‘¦) for all scalars ğ‘, ğ‘ and
inputs ğ‘¥, ğ‘¦. Hence, one way to infer whether a single operation is linear is to simply take ğ‘¥, ğ‘¦ to
be the inputs to the operation ğ‘“ in the original computation graph as produced by two random
batches of inputs to the full graph, and check whether the above equation holds for scalar ğ‘, ğ‘.
However, as specifying whether an operation is linear or non-linear is relatively simple and does
not impose undue burden upon the programmer, we chose to manually specify, for each operation,
whether it is linear to avoid additional inference overheads at synthesis time.

Abstract Interpretation. Our programs are represented as a directed acyclic computation graph,
flattened via topological sort. Hence, given a mapping of primitive operations to their linear
property, we simply scan all the nodes in topological order to compute the maximum number
of alternating linear and non-linear operations among all paths between each pair of inputs and
outputs in linear time.

3.3 Shape Property
The shape property specifies the output shape of every output of the subgraph, given a shape for
every input of the subgraph. More explicitly, for any program ğ‘, the abstract interpretation ğ‘ğ›¼
ğ‘†
maps an input shape ğ‘¢ = (ğ‘–1, . . . , ğ‘–ğ‘›) to an output shape ğ‘£ = (ğ‘œ1, . . . , ğ‘œğ‘š), where given any tensor
ğ¼ with shape ğ‘¢, the output tensor ğ‘‚ = ğ‘ (ğ‘¢) has shape ğ‘£. To abstract a subgraph into the shape
property, we take advantage of JAXâ€™s built-in shape inference. This allows us to easily infer the
shape property without executing the graph on any realized input tensors.

3.4 Example
Fig. 7 illustrates a step-by-step how we infer program properties of the randomly selected subgraph
from the running example in Fig. 1. The initial properties are properties of the identity program,
so the mixing property is an identity matrix, the depth property is zero, and the shape property
preserves the input shape. We interpret each operation on its input program properties abstractly,
as explained in this section, to produce its output properties. The properties of the entire subgraph
is the final output from the abstract interpretation.

4 THE SYNTHESIS ALGORITHM
This section presents our synthesis algorithm formally and shows how we leverage a notion of
distance between programs and properties to achieve progress, soundness, and completeness. To
generate a program of length ğ‘› given a search space ğ‘‡ of transformations, the complexity of our
algorithm scales linearly with ğ‘› and |ğ‘‡ |. In contrast, the standard enumerative synthesis algorithm
that tries all sequences of transformations until satisfying the goal has the complexity of ğ‘‚ (|ğ‘‡ |ğ‘›).

12

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 7. The abstract interpretation to infer program properties of the randomly selected subgraph in Fig. 1

4.1 Distance Functions and Covering Sets

Definition 4.1. Given a program property Î  = (V, â‰¤, ğ›¼), a distance function is a function

ğ‘‘ : P Ã— V â†¦â†’ R+ âˆª {âˆ} such that:

(1) ğ‘‘ (ğ‘, ğ‘£) â‰¥ 0 for all ğ‘ âˆˆ P, ğ‘£ âˆˆ V.
(2) ğ‘‘ (ğ‘, ğ‘£) = 0 â‡â‡’ ğ‘ |= ğ‘£.
(3) ğ‘‘ (ğ‘, ğ‘£) = âˆ â‡â‡’ there does not exist any finite sequence of transformations ğ‘¡ = ğ‘¡1 â—¦ğ‘¡2 â—¦...â—¦ğ‘¡ğ‘›

such that ğ‘¡ (ğ‘) |= ğ‘£.

For the distance function to be useful for synthesis, it must be accompanied by a set of transfor-

mations which can be used to minimize it:

Definition 4.2. Given a space of programs P, a program property Î  = (V, â‰¤, ğ›¼), and a distance
function ğ‘‘, a set of transformations ğ‘‡ âŠ† T is a covering if for all ğ‘ âˆˆ P and ğ‘£ âˆˆ V with ğ‘‘ (ğ‘, ğ‘£) > 0,
either (1) there exists ğ‘¡ âˆˆ ğ‘‡ such that ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) < ğ‘‘ (ğ‘, ğ‘£) or (2) ğ‘‘ (ğ‘, ğ‘£) = âˆ.

For all feasible synthesis tasks, the covering ğ‘‡ is guaranteed to be able to make progress as
measured by the distance function ğ‘‘. Note that a necessary condition for the viability of a distance
is that the full space of transformations T should be a covering. However, as our algorithm scales
linearly with |ğ‘‡ |, we would like ğ‘‡ to be as small as possible, whereas T is often infinitely large. For
instance, in our case, program transformations are just compositions of programs, so the space
of program transformations is exactly the space of programs: |T | = |P |. To guarantee efficient
synthesis, we introduce a stronger notion of coverings:

Definition 4.3. A covering ğ‘‡ is a uniform covering if there exists a constant ğœ– > 0 such that for

all programs ğ‘ âˆˆ P and properties ğ‘£ âˆˆ V with ğ‘‘ (ğ‘, ğ‘£) > 0, there exists ğ‘¡ âˆˆ ğ‘‡ such that

(7)
In other words, the amount of progress can be bounded uniformly from below (i.e., the same
bound applies to all programs and properties). A uniform lower bound guarantees that the distance
will be zero after a finite number of steps, rather than approach zero in the limit.

ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) + ğœ– â‰¤ ğ‘‘ (ğ‘, ğ‘£)

4.2 Progressive Synthesis
In this section, we will assume black-box access to a distance function ğ‘‘ for a program property
Î  = (V, â‰¤, ğ›¼), as well as a uniform covering ğ‘‡ âŠ† T . Given an initial program ğ‘0 and a program

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

13

Algorithm 1 Greedy progressive synthesis
Input: initial program ğ‘, program property ğ‘£, distance function ğ‘‘, uniform covering ğ‘‡
Output: Ì¸|= if (ğ‘, ğ‘£) is infeasible; otherwise, a sequence of transformations ğœ such that ğœ (ğ‘) |= ğ‘£
1: if ğ‘‘ (ğ‘, ğ‘£) = âˆ then
return Ì¸|=
2:
3: end if
4: ğœ = {}, ğ‘0 = ğ‘
5: for ğ‘– = 1, 2, . . . do
6:
7:
8:
9:
10:
11:
12: end for

end if
ğ‘¡ğ‘– = arg minğ‘¡ âˆˆğ‘‡ ğ‘‘ (ğ‘¡ (ğ‘ğ‘–âˆ’1), ğ‘£)
ğ‘ğ‘– = ğ‘¡ğ‘– (ğ‘ğ‘–âˆ’1)
ğœ = (ğ‘¡1, . . . , ğ‘¡ğ‘– )

if ğ‘‘ (ğ‘ğ‘–âˆ’1, ğ‘£) = 0 then

return ğœ

property value ğ‘£, our synthesis procedure will iteratively produce programs ğ‘1, ğ‘2, ... until it finds
a program ğ‘âˆ— |= ğ‘£. The key insight is that we can exploit the distance to guide our synthesis
procedure.

Algorithm 1 describes the greedy progressive synthesis algorithm that synthesizes a program
ğ‘âˆ— satisfying any (feasible) program property ğ‘£. Each iteration ğ‘– starts with a partial program ğ‘ğ‘–âˆ’1,
where ğ‘0 is initialized to the input program. Given the distance function ğ‘‘, for each transformation
ğ‘¡ in the uniform covering ğ‘‡ , the algorithm computes the distance from the transformed program
ğ‘¡ (ğ‘ğ‘–âˆ’1) to the desired property ğ‘£, and selects the transformation ğ‘¡ğ‘– which achieves the minimum
distance. This process is repeated until the distance is 0, at which point the algorithm terminates
and returns the sequence of transformations.

The following theorem characterizes the correctness and efficiency of progressive synthesis.

Theorem 4.4. Given oracle access to a distance function ğ‘‘ and a uniform covering ğ‘‡ , the progressive

synthesis algorithm satisfies the following three properties:

(1) Soundness: if the algorithm returns ğ‘âˆ—, then ğ›¼ (ğ‘âˆ—) |= ğ‘£.
(2) Completeness: if the algorithm returns Ì¸|=, then the synthesis task is infeasible.
(3) Progress: the algorithm terminates after a finite amount of time, which is linear in the length of

the final sequence of transformations and linear in |ğ‘‡ |.

Here, oracle access means that we treat evaluating the distance function ğ‘‘ as a constant-time
operation for the purposes of computational complexity. We defer the proof to Appendix D.2. We
will also present a method analysis that accounts for the complexity of ğ‘‘ in Section 4.2.2.

Stochastic Progressive Synthesis. Selecting a random feasible operation (with non-zero prob-
4.2.1
ability ğ‘) and an operation that makes progress (with probability 1 âˆ’ ğ‘) increases the length of
the synthesized subgraph by a constant multiplicative factor. This strategy ensures that any finite
length subgraph that satisfies the desired properties has a positive probability of being generated.
We use this variant rather than a pure greedy approach in our implementation to encourage greater
diversity.

4.2.2 Progressive Synthesis as a Universal Synthesis System. A natural question is the extent to
which it is necessary to approach the task of program synthesis by defining a distance function
with a covering. Alternatively, we would like to know whether there are synthesis tasks which

14

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

have efficient solutions, but cannot be solved efficiently via progressive synthesis, i.e., how much
are we limited by our choice of synthesis technique?

The main result of this section identifies a natural class of synthesis algorithms that are exactly as
powerful as progressive synthesis. In particular, we define a class of recursively consistent synthesis
algorithms, which are those algorithms ğ´ that can be efficiently â€œrestartedâ€ from a partial solution,
e.g., if ğ´(ğ‘0, ğ‘£) = (ğ‘¡1, ğ‘¡2, ..., ğ‘¡ğ‘›), then ğ´(ğ‘¡1 (ğ‘0), ğ‘£) = (ğ‘¡2, ..., ğ‘¡ğ‘›).

The following (informal) theorem states that any task that can be efficiently solved by a recursively
consistent algorithm can also be solved efficiently by progressive synthesis, and vice-versa. As
recursive consistency is fairly natural assumption, this theorem establishes the universality of
progressive synthesis as a synthesis technique.

Theorem 4.5 (Informal Version.). Given a synthesis task specified by a space of programs P; a
program property Î  = (V, â‰¤, ğ›¼); and a set of primitive transformations ğ¸; the following are equivalent:
(1) There exists a sound, complete, and recursively consistent synthesis algorithm ğ´ that runs in
time polynomial in the length of the synthesized transformations |ğ´(ğ‘, ğ‘£)|, for all ğ‘ âˆˆ P, ğ‘£ âˆˆ V.

(2) ğ¸ is a uniform covering with respect to an efficiently computable distance ğ‘‘.

The second statement guarantees that progressive synthesis is sound, complete, and runs in
polynomial time. In particular, our analysis takes into account the complexity of the distance
function ğ‘‘ (cf. Theorem 4.4, which treats ğ‘‘ as an oracle). We state and prove the rigorous version
of this result in Appendix D.2.

4.3 Multiple Properties from Monotonic Transformations
This section discusses how to adapt our progressive synthesis algorithm when there are multiple
properties of interest. For instance, assume that we have two program properties Î 1 and Î 2. Each
property Î ğ‘– has a distance function ğ‘‘ğ‘– with a uniform covering ğ‘‡ğ‘– . The objective is to synthesize a
sequence of transformations satisfying a set of program property values ğ‘† = {ğ‘£1 âˆˆ V1, ğ‘£2 âˆˆ V2}.
The main problem is that applying a transformation ğ‘¡ âˆˆ ğ‘‡1 that improves the distance ğ‘‘1 to the
property ğ‘£1 may increase the distance ğ‘‘2 to the second property ğ‘£2. In other words, we cannot
guarantee there exists a transformation ğ‘¡ âˆˆ ğ‘‡1 âˆª ğ‘‡2 that simultaneously improves both ğ‘‘1 and ğ‘‘2. In
the worst case, progress is impossible: we may trade-off between ğ‘‘1 and ğ‘‘2 indefinitely. To resolve
this issue, we introduce a notion of monotonic transformations:

Definition 4.6. A transformation ğ‘¡ âˆˆ T is monotonic with respect to a property Î  and distance

ğ‘‘ if for all programs ğ‘ âˆˆ P and properties ğ‘£ âˆˆ V, ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) â‰¤ ğ‘‘ (ğ‘, ğ‘£).

A monotonic transformation, by definition, cannot cause the property Î  to regress. We will
also refer to a set ğ‘‡ âŠ† T as monotonic if all ğ‘¡ âˆˆ ğ‘‡ are monotonic. For instance, the set of all
transformations T is monotonic with respect to the depth property.

The following theorem gives a sufficient condition for progressive synthesis to succeed over

multiple properties. We defer the proof to Appendix D.3.

Theorem 4.7. Let P = {Î ğ‘– = (Vğ‘–, â‰¤ğ‘–, ğ›¼ğ‘– )}ğ‘
ğ‘–=1

be a set of program properties, each with a distance
function ğ‘‘ğ‘– and covering ğ‘‡ğ‘– , respectively, and let ğ‘† = {ğ‘£ğ‘– âˆˆ Vğ‘– }ğ‘
be a set of program property values.
ğ‘–=1
If for all ğ‘–, âˆªğ‘—â‰ ğ‘–ğ‘‡ğ‘— is monotonic with respect to ğ‘‘ğ‘– , then ğ‘‘ (ğ‘, ğ‘†) = (cid:205)ğ‘– ğ‘‘ğ‘– (ğ‘, ğ‘£ğ‘– ) is a distance function for
P with covering ğ‘‡ = âˆªğ‘
ğ‘–=1

ğ‘‡ğ‘– . Furthermore if each covering ğ‘‡ğ‘– is uniform, then so is ğ‘‡ .

4.4 Progressive Synthesis for NAS
This section shows how to apply the progressive synthesis algorithm to our setting of synthesizing
subgraphs satisfying the program properties described in Section 3. The first step is to interpret the

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

15

definition of distance functions and covering sets from Section 4.1 in the context of an abstract
interpretation ğ›¼. Briefly, given an abstract distance function ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) over properties ğ‘¢, ğ‘£ âˆˆ V such
that ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) = 0 implies ğ‘¢ â‰¥ ğ‘£, we say ğ‘‡ âŠ† T is an abstract uniform covering if there exists ğœ– > 0
such for all ğ‘¢, ğ‘£ âˆˆ V with ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) > 0, there exists ğ‘¡ âˆˆ ğ‘‡ such that

ğ‘‘ğ›¼ (ğ‘¡ ğ›¼ (ğ‘¢), ğ‘£) + ğœ– â‰¤ ğ‘‘ğ›¼ (ğ‘¢, ğ‘£)

(8)

The key is that if ğ›¼ (ğ‘0) = ğ‘¢ and ğ‘¡1, ..., ğ‘¡ğ‘› is a sequence of transformations such that the transformed
abstract distance is zero, i.e.,

ğ‘‘ğ›¼ ((ğ‘¡ ğ›¼

ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼

1 )(ğ‘¢), ğ‘£) = 0

(9)

then ğ‘ = (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1)(ğ‘0) |= ğ‘£. A complete treatment is provided in Appendix C.

In order to apply the abstract interpretation version Theorem 4.4 to our setting, in the following

sections we will prove:

(1) For each of the mixing, depth, and shape properties, there exists an abstract distance ğ‘‘ğ›¼
ğ‘–
computable in polynomial time with a uniform abstract covering of transformations ğ‘‡ğ‘– âŠ‚ T .

(2) ğ‘‡ = âˆªğ‘–ğ‘‡ğ‘– is monotonic with respect to both the depth and mixing properties.
(3) ğ‘‡ âˆ© ğ‘€ğ‘† , where ğ‘€ğ‘† is the set of transformations that preserve the shape of the output tensor,

is a uniform covering for both the depth and mixing properties.

For every property, we take ğ‘‡ğ‘– = ğ¸, the set of transformations ğ¸ consisting of adding a single

primitive operation. Then ğ‘‡ = ğ¸, and the following result is immediate:

Theorem 4.8. The progressive synthesis algorithm for ğ›¼NAS is sound, complete, and runs in time
linear in |ğ‘âˆ—| and |ğ¸|, where ğ‘âˆ— is the synthesized subgraph, and ğ¸ is the set of primitive operations.

Sequential Subgraphs Without Reshaping. We begin with the simple setting of subgraphs
4.4.1
that are sequential (i.e., each operation consumes exactly one input, and each output is consumed
exactly once) while preserving the input dimensions. In other words, we restrict our attention to
the set of simple primitives ğ¸ğ‘  âŠ† ğ¸ consisting of operations that consume exactly one input and also
do not reshape or transpose their inputs, and the corresponding space of simple transformations
Tğ‘  = ğ¸âˆ—
ğ‘  . We also limit the set of program properties values V to those satisfiable by a sequential
subgraph with operations from ğ¸ğ‘  . We present the key results below; all proofs are deferred to
Appendix D.4.

4.4.2 Mixing Property. We denote the mixing property as Î ğ‘€ = (Vğ‘€, â‰¤, ğ›¼ğ‘€ ). Without any reshape
operations, the number of input and output dimensions are equal; hence, the space of property
values contains only square matrices with entries in {a, m, o, x} satisfiable by a sequential subgraph
(cf. Fig. 3a). We define the following abstract distance function for the mixing property.

Definition 4.9. The function ğ‘‘ğ›¼

ğ‘€ (ğ‘ˆ , ğ‘‰ ) = the number of entries ğ‘–, ğ‘— such that ğ‘‰ğ‘– ğ‘— > ğ‘ˆğ‘– ğ‘— is a distance

on the mixing property.

The following lemma establishes that the set of primitive ops ğ¸ğ‘  is monotonic with respect to

the mixing property.

Lemma 4.10. âˆ€ğ‘¡1, ğ‘¡2 âˆˆ Tğ‘  and ğ‘ˆ âˆˆ Vğ‘€ , (ğ‘¡ ğ›¼

2 â—¦ ğ‘¡ ğ›¼

1 )(ğ‘ˆ ) â‰¥ max(ğ‘¡ ğ›¼

2 (ğ‘ˆ ), ğ‘¡ ğ›¼

1 (ğ‘ˆ )).

Theorem 4.11. The set of simple primitives ğ¸ğ‘  is a monotone uniform abstract covering for the

mixing property with respect to ğ‘‘ğ›¼
ğ‘€ .

16

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

4.4.3 Depth Property. For the depth property Î ğ· = (Vğ·, â‰¤, ğ›¼ğ· ), the space Vğ· of depth properties
for sequential subgraphs consists of all the non-negative integers. The function ğ‘‘ğ›¼
ğ· (ğ‘¢, ğ‘£) = max(0, ğ‘£âˆ’
ğ‘¢) is an abstract distance for this property. It is easy to see that the set of simple primitive operations
forms a uniform abstract covering with respect to this distance, since we can always append
alternating linear and non-linear operations to decrease the distance to ğ‘£. The depth property is
also trivially monotonic for all transformations T .

Theorem 4.12. The set of simple primitives ğ¸ğ‘  is a monotone uniform abstract covering for the

depth property with respect to ğ‘‘ğ›¼
ğ· .

Shape Property. In general, there are two ways a (non-reshape) operation can change the
4.4.4
shape of a tensor. The first is by changing the channel dimension to an arbitrary integer (e.g., a
dense layer). The second is by downsampling the spatial dimensions by an integer factor (e.g., a
2x2 pooling operation would decrease the spatial dimensions by a factor of 1/2). Convolutional
layers have the potential to perform both at the same time. Note that we do not support operations
that change the spatial dimensions by other amounts (for instance, strided convolutions without
padding), which does not materially limit our search space; in fact, all of the seed architectures
considered in Section 6.1 respect this condition.

The space of all shape properties Vğ‘š

ğ‘† of dimension ğ‘š â‰¥ 1 consists of all ğ‘š-tuples of positive
integers, which specify the shape of an ğ‘š-dimensional tensor: the first entry is the batch dimension,
which never changes; the last entry is the channel dimension; and all intermediate entries are the
spatial dimensions.

Definition 4.13. The function ğ‘‘ğ›¼

ğµ = (ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘1, ..., ğ‘ğ‘šâˆ’2, ğ‘ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ ) from Vğ‘š
ğ‘†
(cid:40)
1,
0
(cid:40)(cid:205)ğ‘šâˆ’2
ğ‘–=1

ğ‘‘ğ›¼
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ´, ğµ) :=

ğ‘‘ğ›¼
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ´, ğµ) :=

ğ‘† (ğ´, ğµ), defined below, on inputs ğ´ = (ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘1, ..., ğ‘ğ‘šâˆ’2, ğ‘ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ ),
is an abstract distance on the shape property:

if ğ‘ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ ! = ğ‘ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™
otherwise

ğ‘ğ‘– /ğ‘ğ‘–,

if ğ‘ğ‘– mod ğ‘ğ‘– = 0 âˆ€ğ‘–
otherwise

(10)

(11)

âˆ
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ´, ğµ) + ğ‘‘ğ›¼
ğ‘† (ğ´, ğµ) := ğ‘‘ğ›¼
ğ‘‘ğ›¼
Next we show that ğ¸ğ‘  is a uniform covering. Indeed, it is always possible to decrease ğ‘‘ğ›¼
ğ‘† by
appending one of two simple primitives: if there are any spatial dimensions that are unequal, a
pooling operator ğ‘ with an appropriately chosen window yields:
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ‘’ğ›¼
ğ‘‘ğ›¼
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ‘’ğ›¼
ğ‘‘ğ›¼

ğ‘ğ‘œğ‘œğ‘™,ğ‘† (ğ´), ğµ) < ğ‘‘ğ›¼
ğ‘ğ‘œğ‘œğ‘™,ğ‘† (ğ´), ğµ) = ğ‘‘ğ›¼

ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ´, ğµ)
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ´, ğµ)

ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ´, ğµ)

(14)

(12)

(13)

If ğ‘‘ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ > 0 then, a dense layer ğ‘’ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ achieves:

ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’,ğ‘† (ğ´), ğµ) = ğ‘‘ğ›¼
ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’,ğ‘† (ğ´), ğµ) < ğ‘‘ğ›¼
(16)
Theorem 4.14. The set of simple primitives ğ¸ğ‘  is a uniform abstract covering for the shape property

ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ‘’ğ›¼
ğ‘‘ğ›¼
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ‘’ğ›¼
0 = ğ‘‘ğ›¼

ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (ğ´, ğµ)
ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ (ğ´, ğµ)

(15)

with respect to ğ‘‘ğ›¼
ğ‘† .

4.4.5 Example. Consider the running example in Fig. 1. The goal of synthesis is to produce a
subgraph which satisfies the mutated properties. Fig. 8 illustrates how our synthesis algorithm makes
progress toward the goalâ€”reducing the abstract distance to the target properties (in red)â€”in each
step by adding an operation. The first operation BatchNorm makes progress on the depth properties.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

17

Fig. 8. Illustration of the synthesis of the first three operations in the subgraph from Fig. 1. The progressive
synthesizer adds an operation that reduces the distance to the target properties in each step. The annotation
â€œd=â€ on the right of each property indicates the distance to the target property.

As a non-linear activation function, SiLU changes neither the mixing nor shape properties, but
increases depth. This process repeats until the synthesized subgraph reaches a total distance of 0.

4.5 Compressing the Search Space
Many transformations are indistinguishable with respect to the abstract properties. For instance,
in our case, a 2x2 average pool and a 3x3 max pool have the same mixing, depth, and shape
properties. Hence, during synthesis it is sufficient to measure the progress of a single pooling
operator, which is representative of the progress for any other pooling operator. More explicitly,
given two transformations ğ‘  and ğ‘¡, if their abstract interpretations are equivalent, i.e., ğ‘ ğ›¼ = ğ‘¡ ğ›¼ ,
then we can safely remove one of them from the covering set ğ‘‡ while preserving the safety and
performance of the progressive synthesis algorithm. Our final synthesis algorithm considers only
87 out of the 1,402 total primitive transformations during synthesis. The remaining 1,315 operations
consist largely of different settings for spatial kernels (e.g., pooling or convolutions) and feature
groups (e.g., GroupNorm or grouped convolutions).

After a satisfying subgraph has been synthesized, we perform a second pass to randomly replace
each operation ğ‘’ in the synthesized subgraph with an operation from the set originally represented
by ğ‘’; by construction, the replacements maintain the desired properties of the subgraph. For example,
we could adjust the momentum of the first BatchNorm synthesized in Fig. 8. To ensure the final
subgraph is balanced, we only use operations of the same type as representatives (e.g., both SiLU
and ReLU are in the 87 operations, even though they have equivalent abstract interpretations).

4.6 Extensions Beyond Sequential Subgraphs
This section briefly describes how to extend our synthesis algorithm beyond sequential subgraphs
with simple operations. Supporting mutations of subgraphs with multiple inputs and outputs
proceeds in three steps. First, we identify the reshape and binary operations in the subgraph,
and decompose the subgraph into a set of sequential subgraphs composed of simple operations,
connected by the reshape and binary operations. Second, we can mutate the structure of the reshape
and binary operators. Finally, we synthesize a replacement for each of the sequential subgraphs
using our aforementioned techniques.

The details of the structure mutation are deferred to Appendix B.4. The key is ensuring that
the mutated structure remains feasible, e.g., the resulting structure should still be a connected

18

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

subgraph. In practice, most common architectures consist of a sequential backbone, augmented by
basic residual connections. Furthermore, reshaping occurs relatively infrequently and is usually
only applied at the end of the network to flatten the features for extracting logits. Hence, our
implementation does not perform the second step (the structure mutation).

5 IMPLEMENTATION

Evolutionary search. We initialize the evolutionary search with a given seed architecture (e.g.,
ResNet-50). At each iteration, we randomly select a single individual to mutate; individuals with
higher fitness are more likely to be chosen. This individual is trained from scratch and its fitness is
evaluated. This procedure is repeated for a fixed number of iterations, at which point we terminate
the search and return the population of evolved individuals. For more details on our evolutionary
search procedure with multiple objectives, see Appendix B.3.

Mutating Architectures. We perform our architecture search at the level of blocks. Each seed
architecture is split into predefined blocks, where each block consists of mutable components,
possibly wrapped within a non-mutable residual connection. For instance, we have the following
block types in the ResNet-50 architecture: (1) bottleneck block with a 2x2 spatial down-sampling
and (2) bottleneck block without any spatial down-sampling. We decompose the Vision Transformer
as follows: (1) self attention block, and (2) multi-layer perceptron (MLP) block. We also use a small
2-block CNN without residual connections; each block is a conv-ReLU-pool, where the convolution
doubles the feature depth and the pool reduces the spatial resolution by two. Hence, all our models
consist of a sequential stack of blocks with some residual connections.

Mutating Blocks. To mutate an architecture, we select a block at random, then select a subgraph
within that block at random. We then synthesize a replacement subgraph with mutated properties
and replace the subgraph into the block. Each mutation is also applied at random to other blocks of
the same type within the model. In addition to mutating individual blocks, we also include block
deletion and block duplication in our search space. When duplicating a block, we select a block at
random and insert a copy into another random location in the model.

Mutating properties. In general, we mutate properties in the direction of relaxing constraints:
since a synthesized program ğ‘ satisfies a property value ğ‘£ if ğ›¼ (ğ‘) â‰¥ ğ‘£, without any mutations the
synthesis is bound to only return subgraphs (and hence architectures) of the same complexity
or greater. Relaxing properties also ensures that the synthesis task is feasible, since the original
subgraph also satisfies the mutated properties. The only exception is the depth property, which is
preserved with 50% probability, otherwise we mutate it up or down by up to 2 uniformly at random.
We mutate the shape property by removing the output shape requirement with 50% probability.
We mutate the linear property by removing pairings with 50% probability.

Synthesis. We implement the basic progressive synthesis as described in Section 4.4.1. We syn-
thesize an operation randomly with probability proportional to 1/(1 + ğ‘‘), where ğ‘‘ is the sum of the
distances to the desired properties after appending the operation. Once the synthesized subgraph
exceeds the length of the original subgraph, we switch to the full greedy mode, which selects the
operation which reduces the distance by the largest amount. If the synthesis does not terminate
once the synthesized subgraph is 2 greater than the original subgraph, we consider this a failure
and report no individual for that iteration of evolution.

Environment. Our implementation is 10,000 lines in Python. We use JAX to train and evaluate

architectures on TPU v2 (for CIFAR-10) and TPU v3 (for ImageNet) accelerators.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

19

6 EXPERIMENTAL RESULTS
This section presents experimental results to evaluate the efficacy of our approach to generating
interesting novel architectures (Section 6.1), assess the importance of key components in our
approach (Section 6.2), and compare against prior approaches (Section 6.3). In each experiment,
we initialize the evolutionary search with a base model (displayed as yellow stars2 in the plots).
We run the search for a fixed number of trials; each trial involves training a proposed candidate
for a fixed number of epochs to evaluate its accuracy on a given dataset. The search trades off the
primary objective of accuracy (higher is better) against some secondary objectives: FLOPS (lower is
better), number of parameters (lower is better), and training throughput (images per second, higher
is better). Table 2 in Appendix B describes the setup for each experiment in more detail.

6.1 Case Studies
6.1.1 CIFAR-10. Our first set of experiments is evaluating on the CIFAR-10 dataset with 50,000
training images and 10 classes. We are specifically interested in how rapidly our evolutionary search
progresses when seeded with models that have room for improvement. Our first experiment seeds
a small 2-layer CNN with residual connections, which is suboptimal in terms of accuracy (80%)
but otherwise has good performance characteristics (low FLOPS, parameter count). Our method
discovers a model with substantially increased accuracy, from 81.3% to 89.6%, while also decreasing
the number of parameters slightly.

We also run a second search seeded with the ResNet-34 architecture. This base model has much
higher accuracy on CIFAR-10 (91.5%) but is less efficient. Figure 9 displays the results. Keeping
accuracy constant, our search discovers one model that decreases the FLOPS by 68.7%, and a second
model that reduces parameters by more than 96% (from 21.3 to 0.8 million parameters). A third
model increases the accuracy to 93.0% using only 5.8 million parameters, a 73.1% reduction. These
results support the hypothesis that our approach is able to make rapid progress and discover
substantially improved models for simpler tasks.

ImageNet. We next present results using the ILSVRC 2012 version of the ImageNet dataset
6.1.2
with 1.2 million training images and 1,000 classes. Our objective is to test whether our approach
can generate improvements in a more difficult setting.

2We evaluate the base model twice due to noise from the stochasticity of the training procedure.

Fig. 9. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS. Yellow stars represent the seed architecture.

20

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 10. Evolving a ResNet-50 model on ImageNet. We optimize for increased accuracy and decreased FLOPS
and increased training speed. For the same accuracy, we have decreased GFLOPS by 23% and increased the
training speed by 12%.

Fig. 11. Evolving a ViT-S/16 model on ImageNet. We optimize for increased accuracy, decreased FLOPS, and
increased training speed. Our search returns a model with the same accuracy using 30% fewer FLOPS and
parameters.

ResNet-50. The first experiment is seeded with the standard ResNet-50 architecture [He et al.
2015]. The ResNet architecture has seen many variants since being introduced in 2015, and its
main innovation (the residual connection) remains a central feature of many architectures today. It
remains a common benchmark for many vision tasks. Figure 10 displays the results. After evaluating
400 individuals, ğ›¼NAS discovers one model which has the same accuracy but 23% fewer FLOPS
(from 10.1 to 7.8 GFLOPS) and 13.7% fewer parameters (from 25.6 to 22.0 million); and a second
model which increases the training speed 12% (from 480 to 538 images per second per core) with
slightly better accuracy. We also discover several models with nearly 80% accuracy (4.5% increase,
not shown in the plot), though at a nearly 15-fold increase in both FLOPS and parameter count.

ViT-S/16. The second experiment is seeded with the S/16 variant of the Vision Transformer
architecture (ViT) [Dosovitskiy et al. 2021]. ViT is a relatively new architecture, and there have
been many variants proposed recently which improve any number of dimensions, such as accuracy,
training speed, inference speed, and sample efficiency. Many of these involve introducing some
convolutions to the architecture, so we are interested in seeing if ğ›¼NAS can automatically discover
many of these improvements.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

21

Fig. 12. Two mutations of the self-attention block in the ViT architecture. The final model is a strict improve-
ment over the base architecture in all objectives.

Figure 11 plots our results. Evaluating a total of 400 individuals, ğ›¼NAS discovers a model as the
278th individual that decreases FLOPS by 28% and the parameter count by 30% with a negligible
increase in accuracy. The mutation of the MLP block in Fig. 1 is present in this model. The resulting
block has almost no trainable parameters and very few FLOPS. Similar mutations of the MLP block
are common throughout the population. Such a mutation is nearly impossible to perform using
single-operation mutations, since it requires eight mutations and simultaneously increasing depth
while replacing linear operations with cheaper ones. If a linear operator is replaced before the depth
is increased, the accuracy could suffer. If depth is increased before replacing the linear operator,
the parameter count and FLOPS will increase. In both cases, the intermediate model is worse than
the parent, making it unlikely to be selected for further mutation.

We present another series of mutations in Fig. 12 that lead to another model which increases
the accuracy by nearly 1% and improves FLOPS, parameter count, and training speed by 6â€“7%.
Mutation 34 appears in nearly 20% of the evolved population, and it is a rare mutation that strictly
improves every metric. Mutation 161 replaces the multiplicative scaling with a self-gating SiLU
unit, and also replaces the softmax with a cheaper but related sigmoid. These analyses suggest
that ğ›¼NAS is able to discover a wide range of novel architectural improvements.

EfficientNet-B0. Our final experiment is seeded with EfficientNetV1-B0 [Tan and Le 2019]. Ef-
ficientNet is a family of extremely parameter- and FLOP- efficient models, and presents a very
strong baseline produced by existing NAS techniques. We are interested in evaluating whether our
methods can deliver an improvement over the state-of-the-art model. The results after training each
candidate for 90 epochs are displayed in Fig. 13. Although EfficientNet is indeed a more difficult
architecture to improve on, ğ›¼NAS is still able to discover a model (named Eâ€™) which improves both
FLOPS and parameter count by 7â€“8% with similar accuracy. Another model (named Eâ€) increases
accuracy by nearly 2% with fewer parameters, but at a 3Ã— increase in FLOPS.

However, the base EfficientNet model achieves higher accuracy when trained for longer. As
fully training each model during the evolutionary search would be prohibitively expensive, we
select a handful of promising models after 90 epochs and train them for 350 epochs to match the
evaluation of the original paper. Under this setting, the base EfficientNet-B0 architecture achieves

22

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 13. Evolving an EfficientNet-B0 model on ImageNet. We optimize for increased accuracy and decreased
FLOPS and increased training speed. We have decreased GFLOPS by 9% and parameter count by 20% for
comparable accuracy and speed.

75.2% accuracy. Model Eâ€™ has a final accuracy of 74.3%, but the 7â€“8% improvement on FLOPS and
parameters still holds. Model Eâ€ maintains a higher final accuracy with fewer parameters at 76.9%.

6.2 Ablation Studies
This section evaluates the importance of the key components in the design of ğ›¼NAS. In this
experiment, we seed the search with the ResNet-34 model and use the CIFAR-10 dataset.
6.2.1 Program Properties. To evaluate the importance of program properties, we compare ğ›¼NAS
with a similar strategy that replaces a randomly selected subgraph with a random subgraph
(ignoring program properties), called the random subgraph strategy. On average, both the original
and replacement subgraphs are of size 3. Unlike ğ›¼NAS, this strategy searches directly in the concrete
space of architectures, without performing property inference and guided synthesis. This study
tests the hypothesis that using abstract properties to guide the search enables ğ›¼NAS to perform
larger mutations while still evolving high-quality candidates. As shown in Fig. 14, ğ›¼NASâ€™s accuracy-
parameters Pareto curve completely dominates the random subgraph strategyâ€™s by a large margin,
and the random subgraph strategy fails to find models with high accuracy. These results support
our hypothesis that the program properties are critical to enabling larger high-quality mutations.
We also evaluate the importance of each individual program property by modifying ğ›¼NAS to use
only a single property throughout the search. Our result shows that all three program properties
are important as using all of them together outperforms using any individual property alone. More
details can be found in Appendix A.1.

6.2.2 Progressive Synthesis Algorithm. Next, we evaluate the benefit of having an efficient syn-
thesis algorithm. In this experiment, we compare with a naÃ¯ve enumerative algorithm that simply
enumerates all possible subgraphs of a given size in a random order, starting from a size of 1 and
increasing the size until it finds a subgraph that satisfies the target program properties. As expected,
the enumerative algorithm is highly inefficient, taking an average of 1,932 seconds to synthesize a
satisfying subgraph, compared to only 67 seconds when using our progressive synthesis algorithm;
for context, training a ResNet-34 model to evaluate its accuracy in our setting takes roughly 600
seconds, so enumerative synthesis introduces over 300% overhead. Efficiency notwithstanding, the
quality of candidates explored by the two variants are similar when evaluating the same number
of candidates (see Fig. 22 in Appendix A.3), which suggests that the constraints imposed by the
abstract properties are largely responsible for the quality of our search.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

23

Fig. 14. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. a random subgraph mechanism.

6.3 Comparison with Existing NAS Techniques
This section compares ğ›¼NAS against prior approaches that do not require defining a carefully-
crafted structured search space. In particular, we adopt the strategies used in Primer [So et al.
2021] and AutoML-Zero [Real et al. 2020], two state-of-the-art NAS techniques for unstructured
search spaces. These approaches search directly in the concrete space of architectures, similar to
the random subgraph strategy in Section 6.2.1, but compensate by using much smaller mutations.
Primer mutates on the basis of individual operations one at a time, by either deleting, inserting,
or swapping operations; or randomly changing one parameter value. AutoML-Zero considers an
additional mutation that randomizes an entire component, essentially combining mutations used
in Primer and the random subgraph strategy.

For clarity, we emphasize that our evaluation is only meant to imitate their search mechanisms
(i.e., mutation strategies) within our evolutionary framework. In particular, both Primer and AutoML-
Zero focus on slightly different tasks and hence use different program representations compared to
ours. They also implement other advanced techniques, which can be adopted by ğ›¼NAS, to reduce
candidate evaluation time (e.g., early stopping). AutoML-Zero was originally designed to evolve
both the neural network architecture and optimization algorithm jointly from scratch. Since this
strategy does not yield results comparable to even our baseline (i.e., ResNet-34 on CIFAR-10), in
order to establish a more fair comparison, we instead apply the respective search mechanisms to
evolve architectures starting from the same seed architecture.

According to the results in Fig. 15 and Fig. 23 in Appendix A.4, ğ›¼NAS discovers significantly
more Pareto optimal models than Primer and AutoML-Zero. Due to using only small mutations,
Primer requires many more evaluations to make large changes to the seed architecture, leading to
slower exploration compared to AutoML-Zero and ğ›¼NAS; the slow exploration is visible in Fig. 15
as Primer does not discover architectures with GFLOPS lower than 0.3. The distribution of the
candidates explored by AutoML-Zero is between Primerâ€™s and the random subgraph strategyâ€™s,
which is to be expected as AutoML-Zeroâ€™s mutations are the union of those twoâ€™s. In summary,
these results further support the benefit of using properties to guide the search over an abstract
design space, as ğ›¼NAS delivers a consistent and significant advantage over existing strategies that
search directly over the concrete space of actual architectures.

Additionally, it is worth comparing our result against Turner et al. [2021], which is another
approach that applies techniques in program languages (i.e. compiler optimizations) to NAS. As
we cannot reimplement Turner et al. [2021]â€™s compiler optimizations easily in our framework, we

24

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 15. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. the Primer search mechanism.

compare our results with those reported in their paper directly. On the CIFAR-10 dataset, starting
from ResNet-34, Turner et al. [2021] is able to compress the model by 2â€“3Ã— without accuracy
loss after evaluating 1000 candidates. In contrast, ğ›¼NAS is able to compress the same model by
over 26Ã— (Section 6.1.1) within 800 candidates. Finally, we note that the architectures produced
by Turner et al. [2021] are less interpretable than ours as their optimization occur at the level of
loop transformations, and hence the resulting architectures cannot be expressed in terms of the
standard human-readable neural network primitives.

7 RELATED WORK

7.1 Neural Architecture Search
This section describes other existing NAS approaches related to our work that have not been
discussed previously. Somewhat similar to our property-guided NAS, some NAS approaches restrict
their search spaces to only mathematically structure-preserving transformations, e.g., only mutating
the operationâ€™s parameters or changing network connections [Cai et al. 2018; Wei et al. 2016]. Their
main motivation is to be able to transfer already-trained weights from an existing architecture to
new architectures, so as to accelerate the candidate evaluation process; in contrast, our properties
are motivated by the semantics of deep neural networks. Their properties are also more strict than
ours, thus limiting their search spaces.

Our work is one of the first attempts to apply techniques in programming languages to NAS. A
prior work by Turner et al. [2021] combines non-semantics preserving transformations from NAS
with semantics preserving transformations from compiler optimizations, to discover candidates
that neither NAS nor compiler optimizations can discover on their own. Unlike their approach, our
work does not leverage low-level transformations from compiler optimizations but exploits other
program languages techniques including program synthesis, program abstraction, and abstract
interpretation.

7.2 Optimizing Neural Networks for Efficiency
Several techniques such as quantization [Banner et al. 2018; Gholami et al. 2021; Wang et al. 2018]
and pruning [Frankle and Carbin 2019; Hassibi and Stork 1992; LeCun et al. 1989; Luo et al. 2017] aim
to improve execution time efficiency. Note that these approaches are orthogonal to our work in the
sense that one can use our technique to generate new architectures, and then prune or quantize the
weights to compress the model once trained. Similarly, the primary benefit of auto-parallelization
[Huang et al. 2018; Jia et al. 2018; Narayanan et al. 2019], graph substitutions [Jia et al. 2019a,b;

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

25

Yang et al. 2021], and other tensor compiler optimizations [Adams et al. 2019; Chen et al. 2018;
Kjolstad et al. 2017; Phothilimthana et al. 2021; Vasilache et al. 2018; Zhou et al. 2020] is that
they generate semantic-preserving optimizations and can be (theoretically) applied to any neural
network architecture. In contrast, our method aims to discover semantically novel architectures.

7.3 Goal-Directed Program Synthesis
Our key concept of using program properties to guide the synthesis of neural network subgraphs
can be classified as goal-directed program synthesis. Similar to ours, a number of prior works use
abstract properties of partial programs to direct the search [Phothilimthana et al. 2016; Polozov
and Gulwani 2015; Wang et al. 2017a,b]. The main difference is that we develop a precise notion of
distance between partial programs and properties; thus, we can devise a linear-time algorithm to
synthesize satisfying programs. Our setting also departs from most program synthesis tasks in that
the design of neural networks does not have a strict correctness requirement.

8 CONCLUSION AND FUTURE WORK
With the increasing complexity of hardware accelerators with diverse performance characteristics,
a large variety of ML compilers that target them, and a wide range of well-known neural network
architectures, finding the right architecture to solve a task is challenging and often requires a deep
knowledge of the full stack. Techniques like neural architecture search (NAS) that automate the
exploration of this complex design space are therefore likely to be increasingly more important. In
this work, we identified some of the fundamental limitations of current approaches for NAS, and
proposed a principled approach for overcoming these limitations using a novel property guided
synthesis strategy. By effectively navigating this large design space, we demonstrate that our
strategy can identify significantly better model architectures for widely used benchmarks. As our
approach is broadly applicable to most if not all deep neural net architectures and domains, we
leave to future work the evaluation of other interesting domains such as language modeling or
audio speech recognition as they require different datasets and training set ups.

ACKNOWLEDGMENTS
We would like to thank Chen Liang, David So, Esteban Real, Hanxiao Liu, Hyeontaek Lim, Martin
Abadi, Mike Burrows, Ras Bodik, Rishabh Singh, and Yanqi Zhou for their help and insightful
feedback during the development of this project.

REFERENCES
Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li, MichaÃ«l Gharbi, Benoit Steiner, Steven Johnson,
Kayvon Fatahalian, FrÃ©do Durand, and Jonathan Ragan-Kelley. 2019. Learning to Optimize Halide with Tree Search and
Random Programs. ACM Trans. Graph. 38, 4, Article 121 (July 2019), 12 pages. https://doi.org/10.1145/3306346.3322967
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. 2018. Scalable Methods for 8-Bit Training of Neural Networks. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems (MontrÃ©al, Canada) (NIPSâ€™18).
Curran Associates Inc., Red Hook, NY, USA, 5151â€“5159.

Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, and Quoc V Le. 2020. Can
weight sharing outperform random architecture search? an investigation with tunas. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 14323â€“14332.

Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Efficient Architecture Search by Network Transfor-

mation. In AAAI.

Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei
Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing Compiler
for Deep Learning. In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation
(Carlsbad, CA, USA) (OSDIâ€™18). USENIX Association, USA, 579â€“594.

26

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa
Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth
16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 [cs.CV]

Jonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In

International Conference on Learning Representations. https://openreview.net/forum?id=rJl-b3RcF7

Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021. A Survey of Quantization
Methods for Efficient Neural Network Inference. CoRR abs/2103.13630 (2021). arXiv:2103.13630 https://arxiv.org/abs/
2103.13630

Babak Hassibi and David Stork. 1992. Second order derivatives for network pruning: Optimal Brain Surgeon. In Advances
in Neural Information Processing Systems, S. Hanson, J. Cowan, and C. Giles (Eds.), Vol. 5. Morgan-Kaufmann. https:
//proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition.

arXiv:1512.03385 [cs.CV]

Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. 2018. GPipe:
Efficient Training of Giant Neural Networks using Pipeline Parallelism. CoRR abs/1811.06965 (2018). arXiv:1811.06965
http://arxiv.org/abs/1811.06965

Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. 2019a. TASO: Optimizing Deep
Learning Computation with Automatic Generation of Graph Substitutions. In Proceedings of the 27th ACM Symposium on
Operating Systems Principles (Huntsville, Ontario, Canada) (SOSP â€™19). Association for Computing Machinery, New York,
NY, USA, 47â€“62. https://doi.org/10.1145/3341301.3359630

Zhihao Jia, James Thomas, Todd Warszawski, Mingyu Gao, Matei Zaharia, and Alex Aiken. 2019b. Optimizing DNN

Computation with Relaxed Graph Substitutions. In Proceedings of the 2nd SysML Conference (SysML â€™19).

Zhihao Jia, Matei Zaharia, and Alex Aiken. 2018. Beyond Data and Model Parallelism for Deep Neural Networks. In SysML.

arXiv:1807.05358 http://arxiv.org/abs/1807.05358

Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The Tensor Algebra Compiler.

Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (Oct. 2017), 29 pages. https://doi.org/10.1145/3133901

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Net-
works. In Advances in Neural Information Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (Eds.),
Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-
Paper.pdf

Yann LeCun, John Denker, and Sara Solla. 1989. Optimal Brain Damage. In Advances in Neural Information Pro-
https://proceedings.neurips.cc/paper/1989/file/

cessing Systems, D. Touretzky (Ed.), Vol. 2. Morgan-Kaufmann.
6c9882bbac1c7093bd25041881277658-Paper.pdf

Leonid Anatolevich Levin. 1973. Universal sequential search problems. Problemy peredachi informatsii 9, 3 (1973), 115â€“116.
Liam Li and Ameet Talwalkar. 2020. Random search and reproducibility for neural architecture search. In Uncertainty in

artificial intelligence. PMLR, 367â€“377.

Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang,
and Kevin Murphy. 2018b. Progressive Neural Architecture Search. In Computer Vision â€“ ECCV, Vittorio Ferrari, Martial
Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). Springer International Publishing.

Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2018a. Hierarchical
Representations for Efficient Architecture Search. In International Conference on Learning Representations (ICLRâ€™18).
https://openreview.net/forum?id=BJQRKzbA-

Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. ThiNet: A Filter Level Pruning Method for Deep Neural Network

Compression. 5068â€“5076. https://doi.org/10.1109/ICCV.2017.541

Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B.
Gibbons, and Matei Zaharia. 2019. PipeDream: Generalized Pipeline Parallelism for DNN Training. In Proceedings of
the 27th ACM Symposium on Operating Systems Principles (Huntsville, Ontario, Canada) (SOSP â€™19). Association for
Computing Machinery, New York, NY, USA, 1â€“15. https://doi.org/10.1145/3341301.3359646

Phitchaya Mangpo Phothilimthana, Amit Sabne, Nikhil Sarda, Karthik Srinivasa Murthy, Yanqi Zhou, Christof Angermueller,
Mike Burrows, Sudip Roy, Ketan Mandke, Rezsa Farahani, Yu Emma Wang, Berkin Ilbeyi, Blake Hechtman, Bjarke Roune,
Shen Wang, Yuanzhong Xu, and Samuel J. Kaufman. 2021. A Flexible Approach to Autotuning Multi-Pass Machine
Learning Compilers. In 2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT).
1â€“16. https://doi.org/10.1109/PACT52795.2021.00008

Phitchaya Mangpo Phothilimthana, Aditya Thakur, Rastislav Bodik, and Dinakar Dhurjati. 2016. Scaling up Superoptimiza-
tion. In Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and
Operating Systems (Atlanta, Georgia, USA) (ASPLOSâ€™16). Association for Computing Machinery, New York, NY, USA,
297â€“310. https://doi.org/10.1145/2872362.2872387

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

27

Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: A Framework for Inductive Program Synthesis. In Proceedings of
the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications
(Pittsburgh, PA, USA) (OOPSLAâ€™15). Association for Computing Machinery, New York, NY, USA, 107â€“126. https:
//doi.org/10.1145/2814270.2814310

Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized Evolution for Image Classifier Architecture

Search. arXiv:1802.01548 [cs.NE]

Esteban Real, Chen Liang, David R. So, and Quoc V. Le. 2020. AutoML-Zero: Evolving Machine Learning Algorithms From
Scratch. In International Conference on Machine Learning (ICMLâ€™20). arXiv:2003.03384 https://arxiv.org/abs/2003.03384
David R. So, Wojciech MaÅ„ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. 2021. Primer: Searching for Efficient

Transformers for Language Modeling. arXiv:2109.08668 [cs.LG]

Mingxing Tan and Quoc V. Le. 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In
Proceedings of International Conference on International Conference on Machine Learning (ICMLâ€™19). arXiv:2104.00298
Mingxing Tan and Quoc V. Le. 2021. EfficientNetV2: Smaller Models and Faster Training. In Proceedings of International

Conference on International Conference on Machine Learning (ICMLâ€™21). arXiv:2104.00298

Jack Turner, Elliot J. Crowley, and Michael F. P. Oâ€™Boyle. 2021. Neural Architecture Search as Program Transformation
Exploration. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages
and Operating Systems (Virtual, USA) (ASPLOSâ€™21). Association for Computing Machinery, New York, NY, USA, 915â€“927.
https://doi.org/10.1145/3445814.3446753

Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary DeVito, William S. Moses, Sven
Verdoolaege, Andrew Adams, and Albert Cohen. 2018. Tensor Comprehensions: Framework-Agnostic High-Performance
Machine Learning Abstractions. arXiv preprint arXiv:1802.04730 (2018). arXiv:1802.04730 [cs.PL]

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia
Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc.
https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

Chenglong Wang, Alvin Cheung, and Rastislav Bodik. 2017a. Synthesizing Highly Expressive SQL Queries from Input-Output
Examples. In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation
(Barcelona, Spain) (PLDIâ€™17). Association for Computing Machinery, New York, NY, USA, 452â€“466. https://doi.org/10.
1145/3062341.3062365

Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. 2018. Training Deep Neural
Networks with 8-Bit Floating Point Numbers. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems (MontrÃ©al, Canada) (NIPSâ€™18). Curran Associates Inc., Red Hook, NY, USA, 7686â€“7695.

Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017b. Program Synthesis Using Abstraction Refinement. In Symposium on
Principles of Programming Languages (POPLâ€™17). Association for Computing Machinery, New York, NY, USA. https:
//doi.org/10.1145/3158151

Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. 2016. Network Morphism. In Proceedings of the 33rd International
Conference on International Conference on Machine Learning - Volume 48 (New York, NY, USA) (ICMLâ€™16). 564â€“572.
Yichen Yang, Mangpo Phitchaya Phothilimtha, Yisu Remy Wang, Max Willsey, Sudip Roy, and Jacques Pienaar. 2021. Equality

Saturation for Tensor Graph Superoptimization. In MLSys. https://arxiv.org/abs/2101.01332

Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. 2019. Evaluating The Search Phase of

Neural Architecture Search. In International Conference on Learning Representations.

Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, Phitchaya Phothilimtha,
Shen Wang, Anna Goldie, Azalia Mirhoseini, and James Laudon. 2020. Transferable Graph Optimizers for ML Compilers.
In NeurIPS 2020.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. 2018. Learning Transferable Architectures for Scalable Image
Recognition. In Conference on Computer Vision and Pattern Recognition (CVPRâ€™18). 8697â€“8710. https://doi.org/10.1109/
CVPR.2018.00907

28

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

A ADDITIONAL EXPERIMENTAL RESULTS

A.1 Program Property Ablations

Variant

Description

No Properties Use no program properties to guide synthesis. Same as the random

Mixing Only
Depth Only
Shape Only
No Mutate
Baseline

subgraph strategy.
Use only mixing property to guide synthesis.
Use only depth property to guide synthesis.
Use only shape property to guide synthesis.
Use all three properties to guide synthesis but do not mutate properties.
Use all three properties to guide synthesis and mutate properties.

Table 1. Different variants of ğ›¼NAS to evaluate the benefit of each program property.

To evaluate the contributions of the proposed program properties, we compare ğ›¼NAS using the
standard properties and mutations with the modified variants in Table 1. For this experiment, we
train a ResNet-34 on CIFAR-10. For all the variants, the synthesized subgraph must be at least the
size of the original subgraph minus 2. The comparison results are shown in Figs. 16 to 20. Using one
single program property allows the search to discover more architectures closer to Pareto optimal
compared to using no property, but using a single property is still visibly inferior to the baseline.
When using all properties but no mutations on the properties, the outcome is quite similar to the
baseline in the medium to high FLOPS region. We attribute this behavior to the search being biased
toward proposing more expensive architectures without property mutations. This is because a
subgraph satisfies the target properties if its properties exceed the target, so without mutations to
decrease property values, the target property values can only increase over time.

Fig. 16. Evolving a ResNet-34 model on CIFAR-10 using the baseline approach vs. no properties.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

29

Fig. 17. Evolving a ResNet-34 model on CIFAR-10 using the baseline approach vs. mixing property only.

Fig. 18. Evolving a ResNet-34 model on CIFAR-10 using the baseline approach vs. depth property only.

Fig. 19. Evolving a ResNet-34 model on CIFAR-10 using the baseline approach vs. shape property only.

30

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Fig. 20. Evolving a ResNet-34 model on CIFAR-10 using the baseline approach vs. no mutations.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

31

A.2 Random Subgraph Ablations
We additionally varied the expected size of the subgraph synthesized by the random subgraph
strategy in Figs. 21a to 21c. In particular, we select a random subgraph of expected size 3 for mutation.
Then, we synthesize a single (random) operation. At this point, we stop and return the current
subgraph with probability 1 âˆ’ ğ‘. Otherwise, we synthesize another (random) operation and repeat.
Hence, the expected subgraph size is 1 + 1/(1 âˆ’ ğ‘). In particular, the plot for ğ‘ = 0.2 synthesizes
subgraphs of size roughly 2 on average, which represents fairly small mutations; correspondingly,
we see that the performance is the best of the three random subgraph strategies, and also similar to
the performance of the AutoML-Zero and Primer strategies. At ğ‘ = 0.5, the expected subgraph size
is 3 and there is a rapid decline in quality. The worst setting is at ğ‘ = 0.75, with an expected random
subgraph of size 5. These results further support our claims that generating larger mutations cannot
rely on purely random mutations. Note that Fig. 21b is the same as Fig. 14 in the main text.

32

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

(a) Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. a random subgraph mechanism with p=.20.

(b) Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. a random subgraph mechanism with p=.50.

(c) Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. a random subgraph mechanism with p=.75.

Fig. 21. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. a random subgraph mechanism with
varying expected sizes

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

33

A.3 Progressive vs. Enumerative Synthesis
In this section, we compare the performance of ğ›¼NAS when using progressive vs. enumerative
synthesis to perform the concretization step as described in Section 6.2.2. In theory, both synthesis
algorithms should produce similar outcomes in terms of the quality of the candidates explored, as
they use the same set of abstract properties to guide the search. The main difference is that the
enumerative synthesis is much slower than the progressive synthesis.

Fig. 22 presents the distributions of the candidates explored using the two synthesis algorithms.
Since the enumerative synthesis always returns a smallest subgraph that satisfies the target proper-
ties, while the progressive synthesis may not, the enumerative synthesis is biased toward smaller
subgraphs. Nevertheless, the performance of the enumerative synthesis is substantially similar
to the progressive synthesis (and is by far the most similar of the other synthesis approaches we
consider).

Fig. 22. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. the enumerative search mechanism. Due to
enumerative synthesis being slow, we compare it with ğ›¼NAS over 600 trials.

A.4 Comparsion with AutoML-Zero
Fig. 23 shows the result when comparing ğ›¼NAS and AutoML-Zero as described in Section 6.3.

Fig. 23. Evolving a ResNet-34 model on CIFAR-10 using ğ›¼NAS vs. the AutoML-Zero search mechanism.

34

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

B EXPERIMENTAL SETUP AND IMPLEMENTATION DETAILS

B.1 Experimental Setup
Table 2 summarizes the setup of our experiments.

For the ResNet architectures (ResNet-34, ResNet-50), we train for 90 epochs using SGD with
base learning rate 0.1, momentum 0.9, and weight decay 0.0004. The learning rate warms up over 5
epochs then follows a cosine decay schedule. We also add a non-standard dense layer between the
head and the first block, to project the feature dimension from 3 to 64, so that all the blocks have
consistent shape properties (otherwise the first block would be a singleton type).

For the ViT-S/16 architecture, we train for 90 epochs using the Adam optimizer, with base
learning rate .003, beta1 = 0.9, and beta2 = 0.999. The learning rate warms up over 10,000 batches,
then follows a cosine decay schedule down to .00001.

For the EfficientNet-B0 architecture, we follow the same setting as the original paper [Tan and
Le 2019], except that we train for 90 epochs. We use the RMSProp optimizer, with a base learning
rate of 0.016. The learning rate warms up over 5 epochs, then decays exponentially down to 0.012.
We also use exponential moving average with weight 0.9999.

We use standard Inception-style data augmentation for training on ImageNet, and 224x224

resolution.

Experiment

Dataset

Base Model

Accuracy Trials

Epochs

Secondary Objectives

Case Studies
(Section 6.1)

CIFAR-10

2-layer CNN
ResNet-34

ImageNet

ResNet-50
ViT-S/16
EfficientNet-B0

CIFAR-10

ResNet-34

81.3%
91.5%

75.7%
70.4%
72.8%

91.5%

400
400

400
400
400

800

90
90

90
90
90

90

flops, parameter count
flops, parameter count

flops, training throughput
flops, training throughput
flops, training throughput

flops, parameter count

Property Ablation
(Section 6.2.1,
Appendix A.1,
Appendix A.2)

Synthesis Ablation
(Section 6.2.2,
Appendix A.3)

Comparison with
Existing Approaches
(Section 6.3,
Appendix A.4)

CIFAR-10

ResNet-34

91.5%

800â€¡

90

flops, parameter count

CIFAR-10

ResNet-34

91.5%

800

90

flops, parameter count

Table 2. Training setup for the experiments. We initialize the search with the base model and run the search
for a given number of trials; each trial involves training a proposed candidate for a given number of epochs.
the accuracy column indicates the accuracy of the base model on the dataset. â€¡Except for the enumerative
search comparison, which uses only 600 trials due to synthesis being very slow.

B.2 Primitive Operations
Table 3 presents the set of primitive operations we currently support in our framework. Note that
our framework still supports evolving models with other operations outside the table, but we will
only mutate and synthesize subgraphs containing operations in the table.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

35

Name

Dense

Convolution

Description

A dense layer applied to
the output features
Any convolution operator
with same padding

with

Dilated Convolution

Grouped Convolution Any grouped convolu-
tion operator with same
padding
Any dilated convolution
operator
same
padding
Adds two input tensors
Multiply input by a scalar
ReLU activation
GeLU activation
SiLU activation
Sigmoid activation
Softmax layer
Batch normalization
Layer normalization
Group normalization
Dropout layer
Average pooling layer
Max pooling layer

Add
Scalar Multiply
ReLU
GeLU
SiLU
Sigmoid
Softmax
Batch Norm
Layer Norm
Group Norm
Dropout
Average Pool
Max Pool

Parameters

output features

output features, strides (either 1 or equal to
kernel), kernel shape (always square)
output features, strides (either 1 or equal to
kernel), kernel shape (always square), feature
groups > 1
output features, strides (either 1 or equal to
kernel), kernel shape (always square), and
input dilation > 1

Scalar

Number of groups

Window dimension
Window dimension

Table 3. Primitive operations that we use to synthesize subgraphs

B.3 Evolutionary Neural Architecture Search
In this section, we describe our evolutionary framework for NAS. Our design is based on the
evolutionary algorithm of Real et al. [2019], except that we do not perform the age regularization
due to only running the evolutionary search for 800 individuals.

In order to support multi-objective optimization, we develop a notion of weighting individuals
based on their Pareto optimality. A point is Pareto optimal with respect to multiple objectives if
no other point is a strict improvement on all objectives.

We assume that all objectives trade-off against a single primary objective; in our case, the goal
is always to maximize accuracy versus the various secondary objectives. We define the Pareto
weight of an individual as the shortest distance (in the â„“2 sense) from the individual to the Pareto
curve, which is built by linearly interpolating the Pareto-optimal points. Lower Pareto weights
correspond to better Pareto optimality; Pareto optimal points having a Pareto weight of 0.

However, as this distance is not invariant to units, we also perform a normalization in each
dimension. To compute the normalization factor, we simply compute the average slope of the full
Pareto curve, which is equivalent to the slope of the two endpoints. The idea is that this gives a
notion of how difficult, on average, it is to trade off between the two objectives.

Algorithm 2 provides a summary of our selection mechanism. This process is repeated inde-
pendently each time a new individual is requested for mutation, with the current population of
individuals as the input.

B.4 Structure Mutations for Subgraphs with Multiple Inputs and Outputs
This section provides a high level description of how to support structure mutations for subgraphs
with multiple inputs and outputs. Recall that our synthesis algorithm first decomposes a subgraph

36

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Algorithm 2 Evolutionary selection mechanism
Input: set of candidates ğ‘€, primary objective ğ‘œğ‘ , secondary objectives ğ‘‚ğ‘  , percentage ğ‘˜
Output: candidate to be mutated
1: Randomly select a secondary objective ğ‘œğ‘  âˆˆ ğ‘‚ğ‘ 
2: Build the Pareto curve ğ¶ of the primary objective ğ‘œğ‘ versus the secondary objective ğ‘œğ‘ 
3: Compute ğ‘Š = {ğ‘¤ğ‘– | ğ‘¤ğ‘– = Pareto weight of ğ‘šğ‘– âˆˆ ğ‘€ with respect to ğ¶}
4: ğ‘€top = top ğ‘˜% of ğ‘€ according to ğ‘Š
5: return Random ğ‘š âˆˆ ğ‘€top

with reshapes and binary operations into sequential subgraphs connected by reshape and binary
operations. When then mutate each sequential subgraph individually.

B.4.1 Reshape Operators. In this section, we provide a high level description of how to synthesize
sequential subgraphs with reshape operators. The main challenge is that for the mixing property,
we can no longer rely on Corollary D.16, which is defined only for square matrices. For instance, if
the input has 3 dimensions, but the output has 4 dimensions, it is not clear a priori how to associate
the input dimensions with the output dimensions without fixing in advance the sequence of reshape
operations.

The basic approach is to always synthesize reshape operators in pairs. Every reshape operator
ğ‘¡ (e.g., transpose, flatten) has a corresponding inverse ğ‘¡ âˆ’1 such that ğ‘¡ â—¦ ğ‘¡ âˆ’1 = id. Hence, we can
always â€œundoâ€ a reshape operator if it makes the synthesis problem infeasible. After applying the
reshape operator ğ‘¡, we can simultaneously transform the target properties ğ‘¢ and continue with
synthesis. At a later point, we then need to reverse the transformation by applying ğ‘¡ âˆ’1 to both the
program and the properties. This guarantees that the original properties are satisfied.

In some cases, we may have that ğ‘‘ (ğ‘¡ (ğ‘), ğ‘¢) < âˆ. In this scenario, it is safe to continue synthesis
without reversing the transformation; however, we should limit the number of such reshape
operators as there is no guarantee that ğ‘‘ (ğ‘¡ (ğ‘), ğ‘¢) â‰¤ ğ‘‘ (ğ‘, ğ‘¢), which violates the assumption of
progress.

Support for Binary Operations. We next describe how to support the insertion and deletion
B.4.2
of binary operations. Namely, whenever we choose to synthesize a binary operation as the next
transformation, we must also synthesize something to fill the other input slot before proceeding. To
do this, we first select a random starting point, which is either a previously generated operation, or
an input to the subgraph, with the requirement that the shape property from the selected starting
point to the binary input must be feasible. From here, a recursive call to the sequential synthesizer
can generate a subgraph that connects the selected starting point to the missing input of the binary
operation.

To delete a binary operator, we first check whether it is necessary by replacing one of its inputs
with a constant, placeholder input, and inferring the resulting properties with respect to its output.
If the target properties are satisfied, then we can safely replace the binary operation with an
identity operation, deleting the unneeded input. Note that one way a binary operation can become
unnecessary is a previous synthesis step already synthesized a new binary op.

Combining the ability to insert and delete binary operations yields the desired subgraph structure

mutations.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

37

C ABSTRACT PROGRESSIVE SYNTHESIS
We first redefine the relevant notions using the lens of abstract interpretation:

Definition C.1. Given a program property Î  = (V, â‰¤, ğ›¼) and an abstract interpretation ğ›¼, an

abstract distance function is a function ğ‘‘ğ›¼ : V Ã— V â†¦â†’ R+ âˆª {âˆ} such that:

(1) ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) â‰¥ 0 for all ğ‘¢, ğ‘£ âˆˆ V.
(2) ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) = 0 â‡â‡’ ğ‘¢ â‰¥ ğ‘£.
(3) ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) = âˆ â‡â‡’ there exists ğ‘ âˆˆ P such that ğ‘ |= ğ‘¢ for which there does not exist any

finite sequence of transformations ğ‘¡ = ğ‘¡1 â—¦ ğ‘¡2 â—¦ ... â—¦ ğ‘¡ğ‘› such that ğ‘¡ (ğ‘) |= ğ‘£.

Definition C.2. Given a space of programs P, a program property Î  = (V, â‰¤, ğ›¼), an abstract
interpretation ğ›¼, and an abstract distance function ğ‘‘ğ›¼ , a set of transformations ğ‘‡ âŠ† T is an
abstract covering if for all ğ‘¢, ğ‘£ âˆˆ V with ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) > 0, either (1) there exists ğ‘¡ âˆˆ ğ‘‡ such that
ğ‘‘ğ›¼ (ğ‘¡ ğ›¼ (ğ‘¢), ğ‘£) < ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) or (2) ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) = âˆ.

Definition C.3. An abstract covering ğ‘‡ is a uniform abstract covering if there exists a constant

ğœ– > 0 such that for all properties ğ‘¢, ğ‘£ âˆˆ V with ğ‘‘ğ›¼ (ğ‘¢, ğ‘£) > 0, there exists ğ‘¡ âˆˆ ğ‘‡ such that

ğ‘‘ğ›¼ (ğ‘¡ ğ›¼ (ğ‘¢), ğ‘£) + ğœ– â‰¤ ğ‘‘ğ›¼ (ğ‘¢, ğ‘£)

(17)

We have the following guarantees for the abstract version of the greedy progressive synthesis
algorithm, where we instead provide as input an initial program property value ğ›¼ (ğ‘0) = ğ‘¢, a target
property value ğ‘£, an abstract distance function ğ‘‘ğ›¼ , and an abstract uniform covering ğ‘‡ :

Theorem C.4. Given an abstract distance function ğ‘‘ğ›¼ and an abstract uniform covering ğ‘‡ , the

progressive synthesis algorithm satisfies the following three properties:

(1) Soundness: if the algorithm returns ğ‘¢âˆ— = (ğ‘¡ ğ›¼
(2) Completeness: if the algorithm returns Ì¸|=, then the synthesis task is infeasible.
(3) Progress: the algorithm terminates after a finite amount of time, which is linear in the length of

1 )(ğ‘¢), then ğ‘¢âˆ— â‰¥ ğ‘£.

ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼

the final sequence of transformations and linear in |ğ‘‡ |.

We omit the proof as it is identical to the proof of Theorem 4.4 except with the abstract semantics
ğ‘¡ ğ›¼ substituted for the concrete semantics ğ‘¡. Note that the only difference with the statement of
Theorem 4.4 is the first soundness property, which is stated in terms of the abstract semantics
ğ‘¢âˆ— â‰¥ ğ‘£ instead of the corresponding concrete semantics ğ‘âˆ— = (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1)(ğ‘0). The key is that the
soundness property of the abstract interpretation allows us to translate this into the required form:

Lemma C.5. If ğ›¼ (ğ‘0) = ğ‘¢ and ğ‘‘ğ›¼ ((ğ‘¡ ğ›¼
ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼
Proof. By definition, ğ‘‘ğ›¼ ((ğ‘¡ ğ›¼

1 )(ğ‘¢) = ğ‘¢âˆ— â‰¥ ğ‘£. By the
soundness property of abstract interpretations, since ğ‘0 |= ğ‘¢, it follows that ğ‘âˆ— = (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ğ‘¡1)(ğ‘0) |=
â–¡
ğ‘¢âˆ—. Putting these two facts together yields ğ‘âˆ— |= ğ‘£.

ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼
1 )(ğ‘¢), ğ‘£) = 0 implies that (ğ‘¡ ğ›¼

1 )(ğ‘¢), ğ‘£) = 0, then ğ‘âˆ— = (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1)(ğ‘0) |= ğ‘£.
ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼

Corollary C.6. Running the progressive synthesis algorithm with an abstract distance function
and an abstract uniform covering also achieves soundness with respect to the concrete semantics, i.e.,
if the algorithm returns ğ‘¢âˆ— = (ğ‘¡ ğ›¼

1 )(ğ‘¢), then ğ‘âˆ— = (ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1)(ğ‘0) |= ğ‘£.

ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡ ğ›¼

D DEFERRED PROOFS
D.1 Abstract Semantics of the Mixing Property

Proof of Lemma 3.6. Given two subgraphs ğ‘ and ğ‘, where ğ‘¢ = ğ›¼ğ‘€ (ğ‘), we need to show that
ğ‘ğ›¼
ğ‘€ (ğ‘¢) := ğ›¼ğ‘€ (ğ‘) Ã— ğ‘¢ is an abstract interpretation, i.e., ğ›¼ğ‘€ (ğ‘) Ã— ğ‘¢ â‰¤ ğ›¼ğ‘€ (ğ‘ â—¦ ğ‘). Denote the input
tensor as ğ¼ , the intermediate output ğ‘ (ğ¼ ) as ğ‘ƒ, and the output of the subgraph ğ‘(ğ‘ƒ) = (ğ‘ â—¦ ğ‘)(ğ¼ ) as
ğ‘‚.

38

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

We begin by showing that ğ‘ğ›¼

ğ‘€ is an abstract interpretation for 1-dimensional inputs and outputs,
i.e., when ğ¼ , ğ‘ƒ, and ğ‘‚ are all 1-dimensional tensors. First we establish the safety of the pairing
property, which corresponds to sufficient conditions for when ğ›¼ğ‘€ (ğ‘ â—¦ ğ‘) > x. Note that when both
ğ‘ and ğ‘ pair their respective inputs and outputs, then so does their composition (since the preimage
of ğ‘‚ in ğ‘ƒ is the entire slice, and the preimage of ğ‘ƒ in ğ¼ is also the entire slice, hence the preimage of
ğ‘‚ in ğ¼ is also the entire slice). Thus, we can safely define ğ›¼ğ‘€ (ğ‘) Ã— ğ‘£ = x if ğ‘£ = x or ğ‘¢ = ğ›¼ğ‘€ (ğ‘) = x.
This yields the first row and first column of Fig. 5.

Otherwise, both ğ‘¢ and ğ‘£ are greater than x (i.e., one of o, m, or a). When either ğ‘¢ or ğ‘£ have
all-to-one locality, so does ğ‘(ğ‘): if ğ‘ is all-to-one, then the preimage of a single element in ğ‘‚ is the
entire slice of ğ‘ƒ (and since ğ‘ has at least one-to-one locality, the preimage in ğ¼ is also the full slice); if
ğ‘ is all-to-one, then the preimage of a single element in ğ‘‚ is at least an element of ğ‘ƒ, and so tracing
back through ğ‘ yields the preimage is again the full slice of ğ¼ . So we can define ğ›¼ğ‘€ (ğ‘) Ã— ğ‘£ = a if
either ğ‘¢ = ğ›¼ğ‘€ (ğ‘) = a or ğ‘£ = a (and neither ğ‘¢ nor ğ‘£ is x). This yields the remaining entries of Fig. 5
in the last row and last column.

Finally, the only situation in which ğ‘ â—¦ ğ‘ has one-to-one locality is when both ğ‘ and ğ‘ are one-to-
one; the remaining entries must be many-to-one. This completes the definition of ğ›¼ğ‘€ (ğ‘) Ã— ğ‘£ = ğ‘¢ âˆ— ğ‘£
as given in Fig. 5.

When the tensors are higher dimensional, to determine whether a dimension ğ‘¥ in ğ¼ and ğ‘§ in ğ‘‚ are
paired, we need to check whether there exists some dimension ğ‘¦ in ğ‘ƒ which is paired with both ğ‘¥
and ğ‘§. The locality is the maximum locality over all paired dimensions ğ‘¦ in ğ‘ƒ, which corresponds to
the definition of + in Lemma 3.6. Hence we see the computation is exactly a matrix multiplication,
and ğ‘ğ›¼
â–¡

ğ‘€ (ğ‘¢) := ğ›¼ğ‘€ (ğ‘) Ã— ğ‘¢ is an abstract interpretation as claimed.

D.2 Performance and Universality of Progressive Synthesis
This section culminates in the statement and proof of the formal version of Theorem 4.5. We
first clarify how â€œlengthâ€ is measured for a sequence of transformations (ğ‘¡1, . . . , ğ‘¡ğ‘›) âˆˆ T ğ‘›. Note
that taking the length to be ğ‘› is not well-defined since T is the set of all finite length strings ğ¸âˆ—;
in particular, ğœ = ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1 âˆˆ T , i.e., any sequence of transformations is always functionally
equivalent to a singleton transformation.

Instead, we define a string representation of any sequence of transformations (ğ‘¡1, . . . , ğ‘¡ğ‘›) to be
the string representation of the composed transformation ğœ = ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1 (recall that this is defined
since ğœ âˆˆ T , and elements of T are also strings over ğ¸). This yields the following equivalence
relation over sequences of transformations:

Definition D.1. Let ğ‘¡ = (ğ‘¡1, . . . , ğ‘¡ğ‘›) and ğ‘  = (ğ‘ 1, . . . , ğ‘ ğ‘š) be two sequences of transformations. Then
ğ‘¡ and ğ‘  are representationally equivalent, denoted as ğ‘¡ âˆ¼ ğ‘ , if their string representations are
lexicographically equal.

Definition D.2. Given an equivalence class ğ‘… under âˆ¼, the primitive representative is the

unique maximal element with the longest sequence length.

It is not hard to see that this maximal element consists entirely of primitive operations, hence

the name.

Definition D.3. For any ğ‘¡ âˆˆ T , we define the length of ğ‘¡, denoted |ğ‘¡ |, as the number of transfor-

mations in the primitive representative of its equivalence class.

Under this definition of length, (ğ‘¡1, . . . , ğ‘¡ğ‘›) âˆˆ T ğ‘› and ğœ = ğ‘¡ğ‘› â—¦ Â· Â· Â· â—¦ ğ‘¡1 âˆˆ T have the same length.
Proof of Theorem 4.4. Soundness and completeness follows from the fact that ğ‘‘ (ğ‘£ğ‘–, ğ‘£) = 0 if
and only if ğ›¼ (ğ‘ğ‘– ) |= ğ‘£. Progress follows also from the fact that ğ‘‡ is a uniform covering and we

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

39

decrease ğ‘‘ by at least ğœ– each step. At each step we consider each transformation ğ‘¡ âˆˆ ğ‘‡ , and we do
this once for each transformation in the returned sequence. Hence, the total runtime is linear in |ğ‘‡ |
and the number of transformations in the returned sequence (which is bounded by the length of
â–¡
the returned sequence).

Next, we provide a formal definition of a recursively consistent synthesis algorithm, which was

first introduced informally in Section 4.2.2:

Definition D.4. Let ğ· be a positive integer. Then ğ´ is recursively consistent at depth ğ· if
there is a constant ğ¶ â‰¥ 0 such that for all feasible ğ‘, ğ‘£, there exists a pair of transformations
(ğ‘¡1, ğ‘¡2) âˆ¼ ğ´(ğ‘, ğ‘£) with |ğ‘¡1| â‰¤ ğ· satisfying

(1) ğ´(ğ‘¡1(ğ‘), ğ‘£) âˆ¼ ğ‘¡2; and
(2) ğ´(ğ‘¡1(ğ‘), ğ‘£) runs in at most ğ¶ more steps than ğ´(ğ‘, ğ‘£).

We claim that the existence of a recursively consistent algorithm is a relatively mild condition.
Roughly speaking, the synthesis problem has a recursively consistent algorithm ğ´ if there are
some natural â€œstopping pointsâ€ along the way for which ğ´ can output partial solutions (of bounded
length). For instance, large programs are often organized into several files, which themselves are
broken down into functions; when tackling a theorem, lemmas can be used to streamline the proof.
Hence, focusing on synthesis problems that are solvable via recursively consistent means does not
materially limit the scope of our results from a practical standpoint. Note that recursive consistency
is less a condition on the inherent difficulty of the synthesis task (as in standard computational
complexity) but rather the difficulty of computing a solution under the semantics of the language
of transformations ğ¸âˆ—.

We also need a more nuanced way to quantify the efficiency of synthesis. So far (e.g., in Theo-

rem 4.4), we have been using the following measure of efficiency:

Definition D.5. A synthesis algorithm ğ´ is output-efficient if its computational complexity is

polynomial in the length of its output, |ğ´(ğ‘, ğ‘£)|.

The main issue with this definition is that achieving a run time that is polynomial in output length
is, in some sense, trivial. For instance, suppose that algorithm ğ´ runs in time that is exponential
in its output, but there exists a transformation ğ‘¡id âˆˆ T that is the identity transformation, i.e.,
ğ‘¡id(ğ‘) = ğ‘ for all ğ‘ âˆˆ P. Then we can make ğ´ run in â€œpolynomial timeâ€ simply by appending an
exponentially long sequence of ğ‘¡id to the end of the original output. More generally, using output
efficiency leads to a host of problems that can be summarized as rewarding â€œverboseâ€ algorithms
(i.e., those that return longer sequences of transformations).

To resolve these issues, we will define a different notion of efficiency that uses an independent

baseline:

Definition D.6. A complexity function is a function ğ¾ : P Ã— V â†’ R+.

Definition D.7. A synthesis algorithm is K-efficient if its computational complexity is polyno-

mially bounded by the complexity function ğ¾.

One way to understand this is that the complexity function ğ¾ measures how â€œdifficultâ€ a particular
synthesis task (ğ‘, ğ‘£) is. Then a ğ¾-efficient algorithm scales gracefully with the difficult of the input
instance. Alternatively, the complexity function tells us roughly how much time we are willing to
spend to solve an instance of a synthesis task. In this case, a synthesis algorithm is ğ¾-efficient if
the amount of time it takes to solve an input instance grows polynomially with the computational
budget. Both of these interpretations give an objective way to compare the efficiency of different

40

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

return Ì¸|=

Algorithm 3 Parallel progressive synthesis
Input: initial program ğ‘, program property ğ‘£, a distance function ğ‘‘, uniform covering ğ‘‡ with lower bound ğœ–
Output: Ì¸|= if (ğ‘, ğ‘£) is infeasible; otherwise, a sequence of transformations ğœ such that ğœ (ğ‘) |= ğ‘£
1: ğ‘‘0 = ğ‘‘ (ğ‘, ğ‘£)
2: if ğ‘‘0 = âˆ then
3:
4: end if
5: ğœ = {}, ğ‘0 = ğ‘
6: for ğ‘– = 1, . . . do
7:
8:
9:
10:
11:
12:
13:
14: end for

end if
ğ‘¡ğ‘– = ParallelSearch(cid:0)ğ‘‡ ; ğœ†ğ‘¡ .(cid:0)ğ‘‘ (ğ‘¡ (ğ‘ğ‘–âˆ’1), ğ‘£) + ğœ– â‰¤ ğ‘‘ğ‘–âˆ’1
ğ‘ğ‘– = ğ‘¡ğ‘– (ğ‘ğ‘–âˆ’1)
ğ‘‘ğ‘– = ğ‘‘ (ğ‘ğ‘–, ğ‘£)
ğœ = (ğ‘¡1, . . . , ğ‘¡ğ‘– )

if ğ‘‘ğ‘–âˆ’1 = 0 then
return ğœ

(cid:1)(cid:1)

synthesis algorithms. Note that there is no requirement that the complexity function itself be
efficiently computable.

We now introduce a new progressive synthesis algorithm that is designed for ğ¾-efficiency
(rather than output efficiency). Algorithm 3 presents the pseudocode for this variant, which we call
parallel progressive synthesis. The main difference between the greedy and parallel variants
of progressive synthesis is the use of the subroutine ParallelSearch on line 10 in Algorithm 3.
ParallelSearch(ğ‘‡ ; Cond) takes two arguments: the first argument ğ‘‡ is a set, and the second
argument Cond is a conditional that takes an element of ğ‘‡ and returns either True or False.
ParallelSearch then initializes one instance of Cond for each element ğ‘¡ âˆˆ ğ‘‡ , and begins executing
Cond in parallel, one step at a time. Every time an instance of Cond finishes computing, we check
its output value and return if the condition is satisfied. Then the run time of ParallelSearch is
bounded by the size of the set ğ‘‡ multiplied by the minimum amount of time it takes to verify a
satisfying ğ‘¡ âˆˆ ğ‘‡ .

More explicitly, define ğ‘† (ğ‘“ ; ğ‘¥) to be the total number of steps needed to compute the function
ğ‘“ on the input ğ‘¥. Define the set of transformations that decrease the distance by at least ğœ– as
ğ‘‡ â†“ = {ğ‘¡ âˆˆ ğ‘‡ | ğ‘‘ (ğ‘¡ (ğ‘ğ‘– ), ğ‘£) + ğœ– â‰¤ ğ‘‘ğ‘– }. Then each call to ParallelSearch on line 10 in Algorithm 3
terminates after exactly |ğ‘‡ | minğ‘¡ âˆˆğ‘‡ â†“ ğ‘† (ğ‘‘; (ğ‘¡ (ğ‘ğ‘– ), ğ‘£)) steps. We say that ParallelSearch returns the
fastest continuation:

ğ‘¡ğ‘– = arg min

ğ‘† (ğ‘‘; (ğ‘¡ (ğ‘), ğ‘£))

ğ‘¡ âˆˆğ‘‡ â†“

(18)

We can compare this with the analogous greedy search subroutine from Algorithm 1 (greedy

progressive synthesis), which always takes the most progressive continuation:

ğ‘¡ğ‘– = arg min

ğ‘¡ âˆˆğ‘‡

ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£)

(19)

In contrast to ParallelSearch, the run time of this line is bounded from below by the maximum
amount of time it takes to compute ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) for any ğ‘¡ âˆˆ ğ‘‡ . This explains why we need to use the
ParallelSearch function, since we have no control over the worst-case performance of ğ‘‘.

We can now state the first version of our main result.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

41

Theorem D.8. Given a synthesis task specified by a space of programs P; a program property
Î  = (V, â‰¤, ğ›¼); and a set of primitive transformations ğ¸; for all complexity functions ğ¾ (ğ‘, ğ‘£), the
following are equivalent:

(1) There exists a sound, complete, ğ¾-efficient, and recursively consistent synthesis algorithm ğ´.
(2) There exists a distance function ğ‘‘ with a uniform covering, such that the parallel progressive

synthesis algorithm is sound, complete, and ğ¾-efficient.

Proof. (2) =â‡’ (1) follows immediately from the fact that progressive synthesis is recursively
consistent (with depth 1). The other direction requires us to demonstrate that the existence of
a recursively consistent synthesis algorithm also implies that the existence of a viable distance
function ğ‘‘ for progressive synthesis.

Then let us assume that ğ´ is a sound, complete, ğ¾-efficient, and recursively consistent synthesis
algorithm with depth ğ·. The first step is to construct the distance function ğ‘‘. We define the
following distance function:

ğ‘‘ (ğ‘, ğ‘£) :=

(cid:40)

âˆ,
|ğ´(ğ‘, ğ‘£)|

if ğ´(ğ‘, ğ‘£) returns Ì¸|=
otherwise

(20)

(21)

By the soundness and completeness of ğ´, ğ‘‘ satisfies the definition of a distance function.

Next, define ğ‘‡ to to be the set of all strings of length at most ğ·, formed from the alphabet ğ¸. Then
ğ‘‡ is a uniform covering with ğœ– = 1 since ğ´ is recursively consistent: given ğ´(ğ‘, ğ‘£) = (ğ‘¡1, . . . , ğ‘¡ğ‘›),
there is some (ğ‘ 1, ğ‘ 2) âˆ¼ (ğ‘¡1, . . . , ğ‘¡ğ‘›) with ğ‘ 1 âˆˆ ğ‘‡ â†“ âŠ† ğ‘‡ such that ğ‘‘ (ğ‘, ğ‘£) = |ğ´(ğ‘, ğ‘£)| and ğ‘‘ (ğ‘ 1(ğ‘), ğ‘£) =
|ğ´(ğ‘ 1(ğ‘), ğ‘£)| = |ğ‘ 2| â‰¤ |ğ´(ğ‘, ğ‘£)| âˆ’ 1. So by Theorem 4.4, the parallel progressive synthesis algorithm
is sound and complete.

It remains to show that parallel progressive synthesis is ğ¾-efficient given the distance function ğ‘‘
constructed above. Denote the output ğ´(ğ‘, ğ‘£) = (ğ‘¡1, . . . , ğ‘¡ğ‘›). We will prove the following two loop
invariants:

(1) The total number of iterations is at most ğ‘›.
(2) Within each iteration, the call to ParallelSearch is polynomially bounded by

|ğ‘‡ |(ğ‘† (ğ´; (ğ‘, ğ‘£) + ğ¶ âˆ— ğ‘–)

(22)

where ğ¶ is a constant.

Combining the two estimates yields an upper bound for the computational complexity of
ğ‘›|ğ‘‡ |(ğ‘† (ğ´; (ğ‘, ğ‘£)) + ğ¶ âˆ— ğ‘›)
(23)
for the parallel progressive synthesis algorithm. Since ğ´ is ğ¾-efficient by assumption, ğ‘† (ğ´; (ğ‘, ğ‘£))
is polynomially bounded by ğ¾ (ğ‘, ğ‘£). Similarly, the original output length ğ‘› = |ğ´(ğ‘, ğ‘£)| is upper
bounded by the run time of ğ´, and hence is also polynomially bounded by ğ¾ (ğ‘, ğ‘£). Thus, the total
computational complexity of the parallel progressive synthesis algorithm is polynomially bounded
by ğ¾ (ğ‘, ğ‘£), as claimed.

It remains to prove the two loop invariants. The first one follows from the fact that Paral-
lelSearch is guaranteed to return a transformation that reduces the distance by at least 1, and
since the distance is initialized to ğ‘‘0 = ğ‘‘ (ğ‘, ğ‘£) = |ğ´(ğ‘, ğ‘£)| = ğ‘›, the loop terminates after at most ğ‘›
iterations. To prove the second invariant, consider the ğ‘–ğ‘¡â„ iteration of the loop. By the definition
of recursive consistency, there exists some ğ‘ ğ‘– âˆˆ ğ‘‡ â†“ such that ğ´(ğ‘ ğ‘– (ğ‘ğ‘–âˆ’1), ğ‘£) runs in at most ğ¶ more
steps than ğ´(ğ‘ğ‘–âˆ’1, ğ‘£). Inducting backward on ğ‘– we see that ğ´(ğ‘ ğ‘– (ğ‘ğ‘–âˆ’1), ğ‘£) runs in at most ğ¶ âˆ— ğ‘–
more steps than ğ´(ğ‘0, ğ‘£). Furthermore, ParallelSearch is guaranteed to return ğ‘¡ğ‘– âˆˆ ğ‘‡ â†“ such

42

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

return Ì¸|=

Algorithm 4 Universal progressive synthesis
Input: initial program ğ‘, program property ğ‘£, a distance function ğ‘‘, primitives ğ¸
Output: Ì¸|= if (ğ‘, ğ‘£) is infeasible; otherwise, a sequence of transformations ğœ such that ğœ (ğ‘) |= ğ‘£
1: ğ‘‘0 = ğ‘‘ (ğ‘, ğ‘£)
2: if ğ‘‘0 = âˆ then
3:
4: end if
5: ğœ = {}, ğ‘0 = ğ‘
6: for ğ‘– = 1, . . . do
7:
8:
9:
10:
11:
12:
13:
14: end for

end if
ğ‘¡ğ‘– = UniversalSearch(cid:0)ğ¸; ğœ†ğ‘¡ .(cid:0)ğ‘‘ (ğ‘¡ (ğ‘ğ‘–âˆ’1), ğ‘£) < ğ‘‘ğ‘–âˆ’1
ğ‘ğ‘– = ğ‘¡ğ‘– (ğ‘ğ‘–âˆ’1)
ğ‘‘ğ‘– = ğ‘‘ (ğ‘ğ‘–, ğ‘£)
ğœ = (ğ‘¡1, . . . , ğ‘¡ğ‘– )

if ğ‘‘ğ‘–âˆ’1 = 0 then
return ğœ

(cid:1)(cid:1)

that ğ‘‘ (ğ‘¡ğ‘– (ğ‘), ğ‘£) takes at most as many steps as ğ‘‘ (ğ‘ ğ‘– (ğ‘), ğ‘£). Hence, ParallelSearch takes at most
|ğ‘‡ |ğ‘† (ğ´; (ğ‘, ğ‘£) + ğ¶ âˆ— ğ‘–) steps in the ğ‘–ğ‘¡â„ iteration.
â–¡

Note that there is a hidden constant in the total run time of the parallel progressive synthesis
algorithm in Eq. (23) that depends the size of the covering |ğ‘‡ | = ğ‘‚ (|ğ¸|ğ· + ğ·) (the addition of ğ·
makes explicit the dependence on depth when |ğ¸| = 1). Our proof relies crucially on the depth ğ· of
the recursive synthesis algorithm in order to construct the covering set ğ‘‡ of all transformations of
length at most ğ·. We also see that Theorem 4.5 is the informal statement of the special case when
the depth ğ· = 1.

The final result of this section presents a different approach based on universal search [Levin
1973]. Our objective is to define a version of progressive synthesis that does not need to be provided
with a covering set ğ‘‡ . The entire problem is then reduced to defining a suitable distance function ğ‘‘.
The main challenge without a priori access to a covering set ğ‘‡ is that we can no longer rely
on ParallelSearch, which essentially searches over a known finite set guaranteed to contain a
fast solution. Instead, we will define a new subroutine UniversalSearch that only requires the
existence of a fast solution of some (unknown) finite length.

UniversalSearch(ğ¸, Cond) takes two arguments: an alphabet ğ¸ and a conditional Cond that
takes as input any string ğ‘¡ âˆˆ ğ¸âˆ— and returns either True or False. The execution of UniversalSearch
then proceeds in phases, starting from phase ğ‘– = 0. During the ğ‘–ğ‘¡â„ phase, UniversalSearch
evaluates Cond on all strings ğ‘¡ âˆˆ ğ¸âˆ— of length at most ğ‘–, up to a total of max(ğ‘–, |ğ¸|ğ‘– ) steps per
instance in parallel (i.e., first by running all ğ‘¡ âˆˆ T of length equal to ğ‘– for up to max(ğ‘– âˆ’ 1, |ğ¸|ğ‘–âˆ’1)
steps in parallel, then by running all ğ‘¡ âˆˆ T of length at most ğ‘– for up to a total of max(ğ‘–, |ğ¸|ğ‘– ) steps
in parallel). The rest of the strategy is the same as ParallelSearch: whenever an instance of Cond
completes, we return if the condition is satisfied. Algorithm 4 presents the resulting synthesis
algorithm, which we call universal progressive synthesis.

Theorem D.9. Given a synthesis task specified by a space of programs P; a program property
Î  = (V, â‰¤, ğ›¼); and a set of primitive transformations ğ¸; for all complexity functions ğ¾ (ğ‘, ğ‘£), the
following are equivalent:

(1) There exists a sound, complete, ğ¾-efficient, and recursively consistent synthesis algorithm ğ´.

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

43

(2) There exists a distance function ğ‘‘ such that the universal progressive synthesis algorithm is

sound, complete, and ğ¾-efficient.

Proof. This proof is nearly the same as the proof of Theorem D.8, except that we need to analyze
the performance of the subroutine UniversalSearch instead of ParallelSearch. In particular,
we will use the same distance as in the proof of Theorem D.8, namely,

ğ‘‘ (ğ‘, ğ‘£) :=

(cid:40)

âˆ,
|ğ´(ğ‘, ğ‘£)|

if ğ´(ğ‘, ğ‘£) returns Ì¸|=
otherwise

(24)

(25)
Now assume that UniversalSearch returns a solution ğ‘¡ of length ğ· that takes ğ‘† steps to verify.

We can upper bound the run time of UniversalSearch as follows.

First, if |ğ¸| = 1, then Cond(ğ‘¡) begins execution during phase ğ· and completes execution during
phase ğ· + ğ‘†. At the end of phase ğ· + ğ‘†, there are exactly ğ· + ğ‘† instances of Cond which have
begun execution, each of which has been run for at most ğ· + ğ‘† steps. This yields an upper bound
for the total run time of at most (ğ· + ğ‘†)2.

Otherwise, if |ğ¸| > 1, then Cond(ğ‘¡) begins execution in phase ğ· and completes execution by

the end of phase ğ· + âŒˆlogğ¸ (ğ‘†)âŒ‰. At this point there are exactly

instances of Cond that have begun execution, which can be bounded from above by

ğ·+ âŒˆlogğ¸ (ğ‘†) âŒ‰
âˆ‘ï¸

|ğ¸|ğ‘–

ğ‘–=0

ğ·+ âŒˆlogğ¸ (ğ‘†) âŒ‰
âˆ‘ï¸

ğ‘–=0

|ğ¸|ğ‘– â‰¤ 2|ğ¸|ğ·+ âŒˆlogğ¸ (ğ‘†) âŒ‰

â‰¤ 2ğ‘† |ğ¸|ğ·+1

The total number of sequential steps taken by any instance of Cond is upper bounded by

|ğ¸|ğ·+logğ¸ (ğ‘†) = ğ‘† |ğ¸|ğ·

(26)

(27)

(28)

(29)

Putting all this together, the total number of steps taken by UniversalSearch is at most

2ğ‘† 2|ğ¸|2ğ·+1 + (ğ· + ğ‘†)2 = ğ‘‚ (ğ‘† 2 (|ğ¸|2ğ·+1 + ğ· 2))
In particular, we see that UniversalSearch pays at most a quadratic overhead3 over Paral-
lelSearch, except that here ğ· refers to the unknown finite depth of the recursively consistent
algorithm ğ´. We omit the remainder of this proof as it is otherwise identical to the proof of
â–¡
Theorem D.8.

(30)

D.3 Product Properties and Monotonic Transformations
The main result of this section is a proof of Theorem 4.7. We begin with a method of combining
two properties by taking their product.

Definition D.10. Let Î 1 = (V1, â‰¤1, ğ›¼1) and Î 2 = (V2, â‰¤2, ğ›¼2) be two properties. The product
property Î  = (V, â‰¤, ğ›¼), denoted Î  = Î 1 Ã— Î 2, takes values V = V1 Ã— V2, with the partial
order (ğ‘¢1, ğ‘¢2) â‰¤ (ğ‘£1, ğ‘£2) if and only if ğ‘¢1 â‰¤1 ğ‘£2 and ğ‘¢2 â‰¤2 ğ‘£2, and abstraction function ğ›¼ (ğ‘) :=
(ğ›¼1(ğ‘), ğ›¼2(ğ‘)).

3Using the original Levin search and some tighter analysis, it is possible to reduce this overhead; however, the asymptotic
behavior is still polynomial, so we present a version whose run time is easier to analyze.

44

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Here are some useful facts about the product property:
Lemma D.11. Let Î 1 = (V1, â‰¤1, ğ›¼1) and Î 2 = (V2, â‰¤2, ğ›¼2) be two properties, and let ğ‘‘1 and ğ‘‘2 be
distance functions on Î 1 and Î 2, respectively. Denote their product property as Î  = Î 1Ã—Î 2 = (V, â‰¤, ğ›¼),
and define a function ğ‘‘ : P â†’ V as ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£)) = ğ‘‘1(ğ‘, ğ‘¢) + ğ‘‘2(ğ‘, ğ‘£). Let ğ‘‡ âŠ† T be a set of
transformations. We have the following facts:

(0) Î  is a program property (cf. Definition 2.1).
(1) ğ‘ |= ğ‘¢ âˆˆ V1 and ğ‘ |= ğ‘£ âˆˆ V2 if and only if ğ‘ |= (ğ‘¢, ğ‘£) âˆˆ V.
(2) ğ‘‘ is a distance function for Î  (cf. Definition 4.1).
(3) If ğ‘‡ is monotonic with respect to ğ‘‘1 and ğ‘‘2, then ğ‘‡ is also monotonic with respect to ğ‘‘.
(4) Let ğ‘‡1 and ğ‘‡2 be monotonic with respect to ğ‘‘2 and ğ‘‘1, respectively, and define ğ‘‡ = ğ‘‡1 âˆª ğ‘‡2.

(a) If ğ‘‡1 is a covering for Î 1 (with respect to ğ‘‘1), and ğ‘‡2 is a covering for Î 2 (with respect to ğ‘‘2),

then ğ‘‡ is also a covering for Î  (with respect to ğ‘‘).

(b) Furthermore, if ğ‘‡1 and ğ‘‡2 are both uniform coverings, then so is ğ‘‡ .

From the first two facts, we see that for a synthesis task with multiple properties, it suffices
to solve the synthesis task on a single product property. The next statement gives an easy way
to construct a new distance function for the product property, and the following statement says
any set which was monotonic for the individual distance functions is also monotonic for this new
distance function. We omit these proofs as they follow immediately from the definitions.

Lemma D.11 (4a) and (4b) give sufficient conditions for constructing coverings for the product
property. Note that these statements are not true in general without monotonicityâ€”in fact, even if
ğ‘‡ is a uniform covering for both Î 1 and Î 2, the following statements about the product property
Î  = Î 1 Ã— Î 2 are false in general:
(1) ğ‘‡ is a covering for Î .
(2) if ğ‘‡ is a covering for Î , then ğ‘‡ is a uniform covering for Î .

So, monotonicity is the key to progressive synthesis over multiple properties when taking the
product.

Proof of Lemma D.11. Given ğ‘ âˆˆ P, ğ‘¢ âˆˆ V1, ğ‘£ âˆˆ V2, we first show that if 0 < ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£)) < âˆ,
there exists ğ‘¡ âˆˆ ğ‘‡ such that ğ‘‘ (ğ‘¡ (ğ‘), (ğ‘¢, ğ‘£)) < ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£)). Since ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£)) = ğ‘‘1(ğ‘, ğ‘¢) + ğ‘‘2 (ğ‘, ğ‘£) > 0,
and by the definition of a distance function ğ‘‘1 and ğ‘‘2 are nonnegative, it follows that at least one of
ğ‘‘1 or ğ‘‘2 is also strictly positive.

Assume without loss of generality that ğ‘‘1(ğ‘, ğ‘¢) > 0. If ğ‘‡1 is a covering, it follows that there
exists ğ‘¡ âˆˆ ğ‘‡1 such that ğ‘‘1(ğ‘¡ (ğ‘), ğ‘¢) < ğ‘‘1(ğ‘, ğ‘¢). Since ğ‘‘2 is monotonic with respect to ğ‘‡1, ğ‘‘2(ğ‘¡ (ğ‘), ğ‘£) â‰¤
ğ‘‘2 (ğ‘, ğ‘£). Adding these together yields that

i.e.,

ğ‘‘1 (ğ‘¡ (ğ‘), ğ‘¢) + ğ‘‘2(ğ‘¡ (ğ‘), ğ‘£) < ğ‘‘1(ğ‘, ğ‘¢) + ğ‘‘2(ğ‘, ğ‘£)

ğ‘‘ (ğ‘¡ (ğ‘), (ğ‘¢, ğ‘£)) < ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£))

(31)

(32)

Hence, ğ‘‡ is a covering for ğ‘‘, which proves Lemma D.11 (4a).

Furthermore, if ğ‘‡1 is actually a uniform covering for ğ‘‘1 with lower bound ğœ–1, then there exists

ğ‘¡ âˆˆ ğ‘‡1 such that ğ‘‘1(ğ‘¡ (ğ‘), ğ‘¢) + ğœ–1 < ğ‘‘1(ğ‘, ğ‘¢). Hence,

i.e.,

ğ‘‘1(ğ‘¡ (ğ‘), ğ‘¢) + ğ‘‘2(ğ‘¡ (ğ‘), ğ‘£) + ğœ–1 < ğ‘‘1(ğ‘, ğ‘¢) + ğ‘‘2(ğ‘, ğ‘£)

ğ‘‘ (ğ‘¡ (ğ‘), (ğ‘¢, ğ‘£)) + ğœ–1 < ğ‘‘ (ğ‘, (ğ‘¢, ğ‘£))

(33)

(34)

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

45

So ğ‘‡ is also a uniform covering for ğ‘‘ (with lower bound equal to the minimum of the lower bounds
â–¡
for ğ‘‘1 and ğ‘‘2), which proves Lemma D.11 (4b).

Proof of Theorem 4.7. We will prove this by inducting on the number of properties ğ‘ . If ğ‘ = 1,
then the statement is trivially true. Otherwise, assume the result holds for ğ‘ = ğ‘€; we will show it
holds for ğ‘€ + 1.

ğ‘‡ğ‘– is a covering for the distance function ğ‘‘ğ‘€ (ğ‘, ğ‘† ğ‘€ ) = (cid:205)ğ‘€
ğ‘–=1

We first prove the case when each ğ‘‡ğ‘– is a covering for ğ‘‘ğ‘– , respectively. By the inductive hypothesis,
ğ‘‡ ğ‘€ = âˆªğ‘€
ğ‘‘ğ‘– (ğ‘, ğ‘£ğ‘– ) with respect to the
ğ‘–=1
product property Î ğ‘€ = Î 1 Ã— Â· Â· Â· Ã— Î ğ‘€ . By assumption, ğ‘‡ ğ‘€ is monotonic with respect to ğ‘‘ğ‘€+1, and
by Lemma D.11 (4a), ğ‘‡ğ‘€+1 is monotonic with respect to ğ‘‘ğ‘€ . Hence, we can apply Lemma D.11 (4a)
to conclude that

ğ‘‡ ğ‘€+1 = ğ‘‡ ğ‘€ âˆª ğ‘‡ğ‘€+1 =

is a covering for the distance function

ğ‘‘ğ‘€+1 = ğ‘‘ğ‘€ + ğ‘‘ğ‘€+1 =

ğ‘€+1
(cid:216)

ğ‘–=1

ğ‘‡ğ‘–

ğ‘€+1
âˆ‘ï¸

ğ‘–=1

ğ‘‘ğ‘–

with respect to the product property

Î ğ‘€+1 = Î ğ‘€ Ã— Î ğ‘€+1 = Î 1 Ã— Â· Â· Â· Ã— Î ğ‘€+1

(35)

(36)

(37)

as desired.

If eachğ‘‡ğ‘– is a uniform covering for ğ‘‘ğ‘– , then by the same argument, we can conclude via Lemma D.11
â–¡

(4b) that ğ‘‡ ğ‘€+1 is a uniform covering for ğ‘‘ğ‘€+1 with respect to Î ğ‘€+1.

D.3.1 Weak uniform coverings. We conclude this section with a short technical note. Recall that
for a set ğ‘‡ to be a uniform covering of a distance function ğ‘‘, there must be some constant ğœ– > 0
such that if ğ‘‘ (ğ‘, ğ‘£) > 0, then we can find a ğ‘¡ âˆˆ ğ‘‡ such that ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) + ğœ– â‰¤ ğ‘‘ (ğ‘, ğ‘£). A consequence
of this definition that may not be immediately obvious is that ğ‘‘ (ğ‘, ğ‘£) < ğœ– implies that ğ‘‘ (ğ‘, ğ‘£) = 0.
It follows that if the distance function ğ‘‘ can get arbitrarily small, it cannot be uniformly covered.
We address this by introducing a weaker notion of uniform covering which can be applied to

distance functions that vanish:

Definition D.12. A covering ğ‘‡ is a weak uniform covering if there exists a constant ğœ– > 0 such

that for all programs ğ‘ âˆˆ P and properties ğ‘£ âˆˆ V with ğ‘‘ (ğ‘, ğ‘£) > 0, there exists ğ‘¡ âˆˆ ğ‘‡ such that
ğ‘‘ (ğ‘¡ (ğ‘), ğ‘£) + min(ğœ–, ğ‘‘ (ğ‘, ğ‘£)) â‰¤ ğ‘‘ (ğ‘, ğ‘£)
(38)
For a single program property, a weak uniform covering ğ‘‡ is almost as good as a uniform
covering in that if ğ‘‘ (ğ‘, ğ‘£) â‰¥ ğœ–, then searching over ğ‘‡ achieves the same uniform lower bound ğœ–; but
if ğ‘‘ (ğ‘, ğ‘£) < ğœ–, although the improvement achieved by searching over ğ‘‡ is not uniformly bounded
from below, it is still guaranteed to reach a distance of 0.

For multiple program properties, a subtle issue remains in that Lemma D.11 (4a) and (4b) do not
hold for weak uniform coverings, i.e., if ğ‘‡1 and ğ‘‡2 are weak uniform coverings, then ğ‘‡ = ğ‘‡1 âˆª ğ‘‡2 is
only guaranteed to be a covering (rather than a weak uniform covering). One way to address this
that ğ‘‡ 2, the set of all pairs of transformations from ğ‘‡ , is a weak uniform covering for ğ‘‘ = ğ‘‘1 + ğ‘‘2;
however, this introduces a quadratic factor to the dependence on the number of transformations in
the respective covering sets (and in general, for the product of of ğ‘› program properties, we would
need to search over transformations in ğ‘‡ ğ‘›, which is undesirable).

Fortunately, there is a simple way to convert a weak uniform covering into a uniform covering

that resolves this issue:

46

Charles Jin, Phitchaya Mangpo Phothilimthana, and Sudip Roy

Lemma D.13. If ğ‘‡ is a weak uniform covering for a distance function ğ‘‘, then there exists another

function ğ‘‘ â€² such that ğ‘‡ is a uniform covering for ğ‘‘ â€².

Proof. Assume that ğ‘‡ is a weak uniform covering with lower bound ğœ–. Define the following

function:

ğ‘‘ â€²(ğ‘, ğ‘£) :=

(cid:40)

0,
ğ‘‘ (ğ‘, ğ‘£) + ğœ–

if ğ‘‘ (ğ‘, ğ‘£) = 0
otherwise

Clearly, ğ‘‘ â€² is a distance function, and ğ‘‡ is a uniform covering for ğ‘‘ â€².

(39)

â–¡

D.4 Progressive Synthesis for NAS
Before presenting the next few results characterizing the depth, mixing, and shape properties, we
begin with a simple lemma that provides sufficient conditions for a covering ğ‘‡ to be a uniform
covering.

Lemma D.14. If the image of the distance function ğ‘‘ is a discrete set (with the usually topology on

R), then any covering ğ‘‡ is also a uniform covering.

We omit the proof as it follows immediately from the definitions. Since all our distance functions
take integer values, for the remainder of this section, it suffices to show that the set of simple
primitives ğ¸ğ‘  is a covering.

The next three results culminate in the proof of Lemma 4.10, which says that the set of primitive

operations are monotonic for the mixing property.

Lemma D.15. The set {x, o, m, a} along with the operations + and âˆ— as defined in Lemma 3.6 forms a

semi-ring.

Proof. The additive identity 0 is the element x, and the max operator is commutative and
reflexive. The multiplicative identity element is o, and the annihilating element is x. Finally, âˆ—
â–¡
distributes over + from both the right and the left.

Corollary D.16. The set of ğ‘š-by-ğ‘š mixing properties form a semi-ring with the usual matrix

multiplication and element-wise addition.

In other words, the abstract interpretation in Lemma 3.6 is just multiplication in the semi-ring of
square matrices over {x, o, m, a}. Note that the multiplicative identity element is the usual identity
matrix ğ¼ğ‘š, where the diagonal is ğ‘‚ and all other entries are ğ‘¥. As a mixing property, the identity
corresponds to an element-wise operation (e.g., a ReLU).

Proof of Lemma 4.10. First, all (non-reshape) operations preserve at least the diagonal, i.e., for
all ğ‘’ âˆˆ ğ¸ğ‘  , we have that ğ›¼ğ‘€ (ğ‘’) + ğ¼ = ğ›¼ğ‘€ (ğ‘’). Let ğ‘’ğ‘›1 â—¦ Â· Â· Â· â—¦ ğ‘’11 = ğ‘¡1 âˆˆ Tğ‘  and ğ‘’ğ‘š2 â—¦ Â· Â· Â· â—¦ ğ‘’12 = ğ‘¡2 âˆˆ Tğ‘  .
Then following the usual rules of matrix multiplication and addition, for any ğ‘ˆ âˆˆ Vğ‘€ we have

(ğ‘¡2)ğ›¼

ğ‘€ ((ğ‘¡1)ğ›¼

ğ‘€ (ğ‘ˆ )) = ğ›¼ğ‘€ (ğ‘’ğ‘š2) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’12) Ã— ğ›¼ğ‘€ (ğ‘’ğ‘›1) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’11) Ã— ğ‘ˆ

(40)
= (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘š2)) Â· Â· Â· (ğ¼ + ğ›¼ğ‘€ (ğ‘’12)) Ã— (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘›1)) Â· Â· Â· (ğ¼ + ğ›¼ğ‘€ (ğ‘’11)) Ã— ğ‘ˆ (41)
â‰¥ (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘š2) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’12)) Ã— (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘›1) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’11)) Ã— ğ‘ˆ
(42)

At this point, we either drop the second term on the right hand side to get:
ğ‘€ (ğ‘ˆ )) â‰¥ (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘š2) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’12)) Ã— ğ‘ˆ
â‰¥ ğ›¼ğ‘€ (ğ‘’ğ‘š2) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’12) Ã— ğ‘ˆ
= (ğ‘¡2)ğ›¼

ğ‘€ ((ğ‘¡1)ğ›¼

ğ‘€ (ğ‘ˆ )

(ğ‘¡2)ğ›¼

(43)

(44)

(45)

47

(46)

(47)

(48)
â–¡

ğ›¼NAS: Neural Architecture Search using Property Guided Synthesis

or drop the first term on the right hand side to get:
(ğ‘¡2)ğ›¼

ğ‘€ ((ğ‘¡1)ğ›¼

ğ‘€ (ğ‘ˆ )) â‰¥ (ğ¼ + ğ›¼ğ‘€ (ğ‘’ğ‘›1) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’11)) Ã— ğ‘ˆ
â‰¥ ğ›¼ğ‘€ (ğ‘’ğ‘›1) Â· Â· Â· ğ›¼ğ‘€ (ğ‘’11) Ã— ğ‘ˆ
= (ğ‘¡1)ğ›¼
ğ‘€ (ğ‘ˆ )
1 )(ğ‘ˆ ) â‰¥ max(ğ‘¡ ğ›¼

1 (ğ‘ˆ )) as desired.

2 (ğ‘ˆ ), ğ‘¡ ğ›¼

Putting these together yields (ğ‘¡ ğ›¼

2 â—¦ ğ‘¡ ğ›¼

Proof of Theorem 4.8. We will apply Theorem 4.7 to the depth, mixing, and shape properties.
From Theorems 4.11 and 4.12, we have that the set of simple primitives ğ¸ğ‘  is a uniform monotone
covering of both the depth and mixing properties, respectively. From Theorem 4.14, we have that
ğ¸ğ‘  is a uniform covering of the shape property. Hence, all we need to show is that ğ¸ğ‘  âˆ© ğ‘€ğ‘† , where
ğ‘€ğ‘† is the set of monotonic transformations for the shape property, is also a uniform covering of
the depth and mixing properties.

Note that ğ‘€ğ‘† consists exactly of the set of operations that preserve the shape of their input tensor.
Only 3 operations cause a change in the shape of their input tensor: (1) pooling operations, (2)
dense layers with a different number of output features, and (3) convolution layers with a different
number of output features. For the depth property, removing these operations clearly still leaves a
uniform covering. For the mixing property, in the case of (2) and (3), the remaining variants that do
not change the number of output features are equivalent in the abstract semantics but still preserve
their output shapes. Hence, removing (2) and (3) still leaves a uniform covering. For the pooling
operations, these introduce many-to-one locality between the spatial dimensions, but this can be
achieved by a convolution layer (that also preserves the shape). Hence, ğ¸ğ‘  âˆ© ğ‘€ğ‘† is also a uniform
â–¡
covering of the mixing property.

