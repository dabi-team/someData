Detecting and Tracking of Multiple Mice Using
Part Proposal Networks

Zheheng Jiang, Zhihua Liu, Long Chen, Lei Tong, Xiangrong Zhang, Xiangyuan Lan, Danny Crookes

Senior Member, IEEE, Ming-Hsuan Yang Fellow, IEEE and Huiyu Zhou

1

(cid:70)

2
2
0
2

r
a

M
5
2

]

V
C
.
s
c
[

3
v
1
3
8
2
0
.
6
0
9
1
:
v
i
X
r
a

Abstract—The study of mouse social behaviours has been increasingly
undertaken in neuroscience research. However, automated quantiﬁca-
tion of mouse behaviours from the videos of interacting mice is still a
challenging problem, where object tracking plays a key role in locating
mice in their living spaces. Artiﬁcial markers are often applied for multiple
mice tracking, which are intrusive and consequently interfere with the
movements of mice in a dynamic environment. In this paper, we propose
a novel method to continuously track several mice and individual parts
without requiring any speciﬁc tagging. Firstly, we propose an efﬁcient
and robust deep learning based mouse part detection scheme to gen-
erate part candidates. Subsequently, we propose a novel Bayesian-
inference Integer Linear Programming Model that jointly assigns the part
candidates to individual targets with necessary geometric constraints
whilst establishing pair-wise association between the detected parts.
There is no publicly available dataset in the research community that
provides a quantitative test-bed for the part detection and tracking of
multiple mice, and we here introduce a new challenging Multi-Mice
PartsTrack dataset that is made of complex behaviours. Finally, we
evaluate our proposed approach against several baselines on our new
datasets, where the results show that our method outperforms the other
state-of-the-art approaches in terms of accuracy. We also demonstrate
the generalization ability of the proposed approach on tracking zebra
and locust.

Index Terms—mouse part detection, geometric constraint, Bayesian-
inference Integer Linear Programming Model, Multi-Mice PartsTrack
dataset.

1 INTRODUCTION

I N neuroscience research, animal models are valuable tools

to understand the pathology and development of neurological
conditions such as Alzheimer’s and Parkinson’s diseases [1]–[3].

Z. Jiang is with School of Computing and Communication, Lancaster Univer-
sity, United Kingdom. E-mail: z.jiang01@lancaster.ac.uk.
Z. Liu, L. Chen, L. Tong and H. Zhou are with School of Computing and
Mathematical Sciences, University of Leicester, United Kingdom. E-mail:
{zl208;lc408;lt228;hz143}@leicester.ac.uk. H. Zhou is the corresponding au-
thor.
X. Zhang is The Key Laboratory of Intelligent Perception and Image Un-
derstanding of Ministry of Education, Xidian University, China. E-mail:
xrzhang@ieee.org.
X. Lan is with the Shenzhen NS-Tech Co.Ltd., China. E-mail: yuan-
lan@life.hkbu.edu.hk.
D. Crookes is with School of Electronics, Electrical Engineering and
Computer Science, Queen’s University Belfast, United Kingdom. E-mail:
d.crookes@qub.ac.uk.
M.-H. Yang s with the School of Engineering, University of California at
Merced, Merced, CA. E-mail: mhyang@ucmerced.edu.
Manuscript submitted in Apr 2020; revised xxxx.

Visual tracking of animals [4]–[6] is an essential task for many
applications and has been successfully used in mice behavior
analysis [7]. Scientiﬁc experiments in laboratories with mice
need long-term observations by researchers and other parties.
However, manually annotating long video recordings is a time-
consuming task. Furthermore, manual documentation suffers from
a number of limitations such as being highly subjective and having
scarce replicability. Hence, there is an increasing interest in the
development of systems for automated analysis of mice social
behaviour from videos [8]–[10].

Deﬁnitions of social behaviours can vary to some extent,
from all the behaviours that occur when two or more animals
are present in the scene [11], to only those behaviours in which
one inﬂuences another [12]. Despite its deﬁnition, to automati-
cally analyse social behaviours, discriminative features are often
required: to record behaviours, to track the positions (or parts)
of the participants, to identify individuals across time and space,
and to quantify animal interactions. For automated behaviour
analysis, a reliable and smart tracking method is required to
associate the detected behaviours with the correct individuals.
Moreover, accurate locations of mouse parts obtained from a
stable tracking method enable the representation of interactions
and allow for behavioural classiﬁcation. Many applications, such
as [13]–[15], analyse mouse behaviours using the tracking results
of mice. Simultaneous tracking of two or more individuals poses a
challenge in the computer vision community. The fact is that mice
are mostly identical and highly deformable objects. In addition,
social interaction between individuals makes the tracking mission
even more complicated due to frequent occlusions. A popular
method to track individuals during interactions is to label each
subject with a unique marker, e.g., by bleaching [16], color [17].
Also, Galsworthy et. al [15] monitor multiple mice in a single cage
by using radio transmitters buried under the skin and then record
their activities by the detection coils. However, these systems are
invasive for the tested subjects, and the labelling method in these
systems very likely inﬂuences an individual’s social behaviours
as it frequently provides an olfactory and/or visual stimulus [18].
Therefore, people generally prefer markerless identiﬁcation.

Several markerless approaches have been proposed to tackle
this challenging problem. A common approach is to count on
suitable foreground segmentation, which separates the extracted
foreground pixels into several spatially connected groups using
clustering algorithms such as Expectation-Maximization for Gaus-
sian Mixture Models (e.g. [16, 19, 20]) or watershed segmentation
(e.g. [14]). However, if two mice are spatially close, in contact

 
 
 
 
 
 
2

Fig. 1. Architecture of the proposed tracking system. Firstly, we propose an efﬁcient and robust deep learning based mouse part detection scheme,
which consists of a multi-stage Part Proposal Network for generating high-quality part and body proposals and two network branches for generating
part and body candidates. Subsequently, we propose a novel Bayesian-Inference Integer Linear Programming Model that jointly assigns the part
candidates to individual targets with necessary geometric constraints whilst establishing pair-wise association between the detected parts.

or occluding each other, it can be difﬁcult to separate them only
based on foreground detection. Moreover, the location estimation
of mouse parts is unreliable when occlusion occurs. De Chaumont
et al. [7] make use of prior knowledge when tracking the mouse
parts, and developed a model that connects a set of geometrical
primitives under physics-based principles and adapts the model’s
parts to the moving mouse based on the deﬁned physical engines.
However, this type of methods requires sophisticated skeleton
models which are hard-coded in the system, and thus limit the
ﬂexibility of the methods.

The above methods belong to the family of detection-free
tracking methods, which requires manual
initialization of the
mouse location in the ﬁrst frame, and then tracks mice and their
parts in subsequent frames. Although these methods have no need
of pre-trained object detectors, they are prone to drifts and identity
switches. Recently, tracking-by-detection methods have become
popular as they are efﬁcient at handling deformation and occlu-
sion. The essential idea is to ﬁrst detect objects and then handle
the data association problem over frames. This approach has been
widely used for human tracking and pose estimation [21]–[25].
Most recently, Wang et al. [4] propose a motion detection based
system to monitor small target such as insects.

To address the part tracking problem of multiple mice in the
context of tracking-by-detection, we break the original problem
into three interdependent but also associated sub-problems. First,
the positions of all the parts in each frame must be identiﬁed.
Second, the detected parts must be assembled to form a physical

mouse. Finally, the positions of the parts must be connected across
image frames in order to generate mouse motion trajectories.

Detecting mouse parts in the ﬁrst sub-problem is challenging
due to the small size and subtle local inter-class differences be-
tween image frames. To address this problem, we propose a mouse
parts and body detection framework based on the features from
deep neural networks. In fact, the use of deep neural networks
has already obtained promising outcomes for part detection and
pose estimation of humans in some challenging benchmarks [21,
22, 26]. This suggests that deep learning architectures can be
applied to part detection of lab animals. For the second and third
problems, we propose a Bayesian Inter Linear Program (BILP)
model that resolves these problems by minimizing a joint objective
function through Bayesian Inference. We wish to handle targets-
candidates assignment and pair-wise part association within a
single cost function. In order to solve the challenging problems
in mouse part detection and tracking, we here design and evaluate
a novel framework based on part association. Figure 1 shows the
ﬂowchart of the proposed tracking system. In summary, we have
the following novel contributions:

1. We propose a reliable detection framework that is efﬁcient
for identifying mouse body and parts and obtains competitive
performance on detection benchmarks.

2. With the detection candidates generated by the proposed
detector, a Bayesian-Inference Integer Linear Program Model is
proposed to estimate the locations of all the mouse parts present
in an image. The formulation is based on the targets’ assignment

and pair-wise part association, subject to mutual consistency and
exclusion constraints.

3. We formulate the parts’ localization as a Bayesian inference
process that combines the output of the proposed detector with
prior geometric models of mice. Unlike the previous work such as
[7], our prior on the conﬁguration of the mouse parts is not hard-
coded but derived from our rich collection of the labeled training
samples. In addition to geometric models, we also introduce mo-
tion cues to compensate for the missing appearance information.
4. Since there is no publicly available dataset that provides a
foundation to quantitatively evaluate the Multi-Mice part track-
ing, we also introduce a new challenging Multi-Mice PartsTrack
dataset
that comprises a set of video recordings of two or
three mice in a home cage with the top view. Several common
behaviours are included in this dataset such as ‘approaching’,
‘following’,‘moving away’,‘nose contact’,‘solitary’ and ‘Pinning’.
The true part and body locations of mice were manually labeled
for each image frame.

2 RELATED WORK
In this section, we review the established approaches related to
our proposed system. Section 2.1 reviews the existing methods for
tracking multiple mice. Section 2.2 discusses the methods used for
multi-people tracking.

2.1 Mouse based Tracking

Typically, the separation of foreground and background can be
used as the ﬁrst step of a multi-subject tracking algorithm, e.g. [7,
14, 16, 19, 20, 27]–[29]. In mouse tracking, the knowledge of the
foreground can be used to improve the accuracy of the tracking
scheme.

Some existing approaches for tracking multiple mice focus on
modelling the appearance of a mouse. For example, Hong et al.
[29] ﬁrst apply background subtraction and image segmentation
from the top-view camera. They then ﬁt an ellipse to each mouse
in the foreground. Thus, the position and body orientation of each
mouse are described by the ﬁtted ellipse. Twining et al. [27] ﬁnd
mice in a new image by ﬁtting active shape models to different
locations in the image and selecting the instance that best ﬁts the
models. Similarly, de Chaumont et al. [7] manually deﬁne shape
models based on geometrical primitives and ﬁt these models to
images using a physical engine. Although these methods are fast
and work well, the ﬂexibility of such methods is limited as they
require sophisticated skeleton models. Moreover, if two mice are
touching, or one mouse is occluding the other, it can be difﬁcult
to separate them based only on the shapes of the blobs of the
foreground pixels. Alternatively, P´erez-Escudero et al. [20] use
ﬁngerprints extracted from image frames in which the mice are not
interacting. The ﬁngerprints are then used to deal with occlusions
and identity shifts. The idtracker developed by Romero-Ferrero
et al. [30] identiﬁes animals that cross certain paths using deep-
learning-based image classiﬁers when tracking multiple animals.
In general, motion is a useful cue for dealing with occlusion.
Model-based tracking approaches can beneﬁt the identiﬁcation
by incorporating motion cues into the used model. A frequently
adopted tracking method that combines both appearance and
motion is based on particle ﬁltering. For example, Pistori et al.
[28] extend a particle ﬁltering approach with certain variations on
the observation model to track multiple mice from the top view.
Branson et al. [19] present a particle ﬁltering algorithm by tracking

3

the contours of multiple mice and acquire the images of the targets
from a side view. The ability of a particle ﬁlter to correctly track
mice depends on how well the observation model works with mice.
Due to the highly deformable shapes of mice, it is very difﬁcult to
explicitly model the entire mouse.

Another solution towards the problem of occlusions is to
mark mice. Shemesh et al. [31], for example, apply ﬂuorescent
paints that light up in different colors under UVA light, and
Ohayon et al. [16] dye the mouse fur with different patterns
of strokes and dots. An alternative to visible markers are radio-
frequency identiﬁers (RFID), which are used by Weissbrod et
al. [32] to identify individuals in combination with video data.
However, these markers must be placed before recording and
may be a potential distraction to mice. To deal with these issues,
Giancardo et al. [14] use a high-resolution thermal camera to
detect minor differences in body temperature. 3D tracking can also
disambiguate occlusions by using depth cameras (e.g. Hong et al.
[29]) or multiple video cameras at different viewpoints (e.g. Sheets
et al. [33]), which is a challenging solution requiring additional
equipment and calibration work. The summary of mouse based
tracking is shown in Supplementary A.

2.2 People based Tracking

Human tracking is also a well studied topic in computer vision.
Some of the techniques developed for human tracking may be
applied for mouse tracking. Most ﬁlter-based tracking methods
such as Kalman ﬁlter, Particle ﬁlter and correlation ﬁlter are well
suited for online applications due to their recursive nature. Some
approaches of this category focus on tracking a single target by
model evolution [34, 35]. Some of the others aim at training better
object classiﬁers [36] or learning better target representations [23].
However, these methods cannot guarantee a global optimum as
they conduct tracking on each target individually.

To alleviate the problems of ﬁlter-based tracking methods,
tracking-by-detection methods have been used in many appli-
cations. The fundamental idea is to ﬁrst detect objects in each
frame and then address the data association problem. Recent
approaches in this category have been focused on improving the
performance of the developed object detectors or designing better
data association techniques to improve tracking performance. For
example, Shu et al. in [37] propose an extension to deformable
part-based human detector [38] and utilize the visible part to
infer the state of the whole object. A number of approaches rely
on data association methods such as the Hungarian algorithm
[39, 40], global objective function optimization [41]–[45] and
Recurrent Neural Network [46]. Xiang et al. [39] propose to
formulate tracking as a Markov decision process with a policy
learned on the labelled training data. They aim to learn similarity
scores between targets and detections, which are used in the
Hungarian algorithm to obtain the targets’ assignment. In [41],
the authors propose a continuous formulation that analytically
models mutual occlusions, dynamics and trajectory continuity,
but utilizes a simple SVM detector in their appearance model.
Rezatoﬁghi et al. [45] propose an efﬁcient approximation of Joint
Probabilistic Data Association (JPDA) to reduce computational
complexity in data association. Recent approaches have formu-
lated data association as a network ﬂow problem and then seek
the solution by optimizing a global objective function. Zhang et
al. [42] show that a promising solution towards the network ﬂow
problem can be found in polynomial time and is highly efﬁcient

in practice. Pirsiavash et al. in [43] also adopt network ﬂows, and
use dynamic programming to search for a high quality sub-optimal
solution. Authors in [44] propose a multi-commodity network ﬂow
to incorporate the appearance consistency between the groups
of people. Milan et al. [46] propose data-driven approximations
to deal with the data association problem using long short-term
memory (LSTM) networks whilst utilizing RNNs for targets’
prediction and updating. Compared against these methods, we
consider targets’ assignment and pair-wise part association in
our problem formulation. Also, we utilize Bayesian Inference to
construct correct correspondences between the proposals and the
measurements. Moreover, we introduce a geometric constraint in
Bayesian Inference to help reduce the ambiguity caused by the
nearby targets with similar appearance.

2.3 Pose Estimation

Recent methods for multi-person pose estimation can be broadly
divided into two categories, i.e., top-down and bottom-up. Top-
down methods [47]–[49] aim to ﬁrst detect people in the image and
then estimate body pose independently. For example, Pishchulin et
al. [47] follow this paradigm by incorporating a trained deformable
part model into a pictorial structure framework. Gkioxari et al [48]
use a person detector modelled after poselets, which is more robust
to occlusions. The very recent Mask-RCNN method [49] extends
the standard Faster R-CNN by adding a branch for predicting
object masks, which shows satisfactory results in person key-
point estimation. However, such approaches are applicable only
if people appear well separable. Moreover, these pose estimation
methods always output a ﬁxed number of body joints without
accounting for occlusion. Bottom-up methods ﬁrstly detect body
parts instead of full persons, and then associate these parts to
individual instances. Pishchulin et al. [21], and later Insafutdinov
et al. [22] generate body parts hypotheses using CNN-based
part detectors, and then jointly solve the problems of people
detection and pose estimation by formulating these problems as
part grouping and labelling via an integer linear program. Cao et
al. [50] combine the unary joint detector modiﬁed from [51] with a
part afﬁnity ﬁeld and greedily generate person instance proposals.
In view of the ability of bottom-up approaches, they are more
robust to occlusion as they assemble parts to the corresponding
individuals. However, these methods do not incorporate any part
tracking and therefore only work on single images.

In the case of mice, previous studies commonly use the top-
down scheme. For example, Cootes et al. [52] and Twining et al.
[27] ﬁnd mice in a testing image by ﬁtting active shape models to
different locations in the image and selecting the instance that best
ﬁts the models. Similarly, de Chaumont et al. [7] manually deﬁne
shape models based on geometrical primitives and ﬁt these models
to the image using a physical engine. Although these methods
work effectively, the ﬂexibility of such methods is limited due to
the requirement of sophisticated skeleton models. Most recently,
based on previous designs of neural networks for human pose
estimation, Mathis et al. [53] develop their pose estimation model
by deploying transfer learning that can reduce the number of
the required training examples. Pereira et al. [54] incorporate the
previous work with a graphical user interface for labeling of body
parts whilst training the network. However, these animal pose
estimation systems are based on the single-image pose estimation
methods.

4

Fig. 2. Architecture of the proposed Part Proposal Network. A multi-
stage architecture is adopted to perform effective hard negative mining
for generating high-quality object proposals. The ﬁnal conﬁdence map
and bounding boxes are attained by aggregating the weighted output of
each stage.

3 PROPOSED MOUSE PARTS AND BODY DETEC-
TION

As stated before, a powerful part and body detector supplies the
solid foundation for successful body tracking. Driven by the recent
development in deep neural networks with ‘attention’ mecha-
nisms, we here propose a novel mouse part and body detection
framework which consists of two components: a multi-stage Part
Proposal Network that generates suitable proposals of mouse parts
and body, followed by a fully connected network that classiﬁes
these proposals using higher-resolution convolutional and Local
Binary Pattern (LBP) features extracted from the original images.

3.1 Part Proposal Network (PPN) for Mouse parts and
body Detection

Detecting mouse parts is non-trivial due to their small size and
subtle local inter-class differences across images. To develop a
strong detector, we ﬁrstly create a proposal generation network
which directs the downstream classiﬁer where to look. The region
proposal network (RPN) in the Faster R-CNN [55] and selec-
tive search [56, 57] are two state-of-the-art methods for object
proposal generation. These methods were developed as a class-
agnostic proposal generator aiming to create the bounding-boxes
that may contain objects. Without speciﬁc knowledge, these region
proposals may not be accurate. Deformable Part Model (DPM)
[38] is another frequently used algorithm to generate small object
proposals. DPM and its variants are a class-speciﬁc method,
which is based on the Histogram of Oriented Gradient (HOG)
features. In addition, most negative examples generated by these
methods belong to negative examples, because they are randomly
selected from the background. Unlike these methods, we adopt a
multi-stage architecture to perform effective hard negative mining.
Suppose we can have ﬁner region proposals, and the accuracy of
the proposed classiﬁer can be further improved. In this section, we
describe the proposed multi-stage PPN as shown in Fig. 2, which
can generate high-quality object proposals from the convolutional
feature maps.

We ﬁrstly adopt the VGG-19 net [58] as the backbone network,
which is pre-trained on the ImageNet dataset [59]. The input image
is ﬁrst analysed by a convolutional network initialized by the ﬁrst

5

being a positive object of the mouse body p0 or a mouse part
pj ∈ {p}J
j=1 at stage t. The negative predicted probabilities are
represented as S. For the regression loss Lreg, we use the robust
loss function (smooth L1) deﬁned in [55]. Bz
j (i) is a vector
which represents the 4 parameterized coordinates of the predicted
bounding box associated with the root box i of pj at stage z, and
ˆBz
j is that of the ground-truthed box. All the terms are ﬁnally
normalized by the mini-batch size (i.e., N = 256).

In order to utilize our region proposal networks trained at each
stage, we assign the weights to the output of the classiﬁcation-
regression networks (represented by Ez) based on their pseudo-
loss:

Ez =

1
num(Dz) ∗ j






(cid:88)

j

(cid:88)

i∈(Dz−Dz

pos)

Sz

j (i) +

(cid:88)

i∈Dz

pos

S

z
j (i)






αz =

1 − Ez
z=1 (1 − Ez)

(cid:80)Z

(2)

(3)

where num is a function to count the number of the root boxes.
After Z stages of the training, the conﬁdence scores Sj and the
bounding boxes Bj of the root boxes with respect to object pj
have the following form: Sj = (cid:80)

j and Bj = (cid:80)

z αzBz
j .

z αzSz

Fig. 3 shows the reﬁnement of the conﬁdence maps across dif-
ferent stages. To reduce the number of the proposals for efﬁciency,
we remove all the bounding boxes that have an intersection-over-
union (IoU) [55] ratio over 0.7 with another bounding box that has
a higher detection conﬁdence. Supplementary B summarises the
training steps of our PPN.

3.2 Part candidates’ generation

With the proposals generated by PPN, inspired by the Faster R-
CNN layer [55], we also crop the proposals and extract ﬁxed-
length features from the feature maps. However, the feature maps
of the faster R-CNN are of low resolution for detecting small
objects (e.g. mouse heads and tails). Generally, a small proposal
box is mapped onto only a small region (sometimes 1*1*n) at
the last pooling layer. Such small feature map lacks discriminative
information, and thus degrades the following classiﬁer. We address
this problem using the pooling features from the shallower layers,
and by additionally extracting texture features (e.g. LBP features)
from the original image. For classiﬁer training, we construct the
training set by selecting the top-ranked 200 proposals (and ground
truths) of each image for each class. At the testing stage, we use a
trained classiﬁer to classify all the proposals in an image, and then
perform non-maximum suppression independently for each class
to obtain part candidates.

4 PARTS TRACKING OF MULTI-MOUSE

In order to achieve successful body and part tracking for all
the mice freely interacting in a home cage, we here propose a
Bayesian-inference Integer Linear Program (BILP) formulation.
Solving this BILP will yield an optimal solution to provide the
locations of mouse parts (i.e., head and tail) in continuous videos,
whilst fulﬁlling mouse part association over time. Then the mouse
body is obtained by connecting tracked mouse parts.

Stage 1

Stage 2

Stage 3

Fig. 3. Conﬁdence maps of the root boxes of the mouse head (ﬁrst
row) and tail base (second row). More intense shades of red indicate
a higher conﬁdence score and lighter shades of blue indicate a lower
conﬁdence score. In stage 1, there is a confusion between the mouse
head and the tail, as several parts of mouse have high conﬁdence
scores. Moreover, some of the high conﬁdence scores come from the
background. However, the estimates are increasingly reﬁned in the later
stages.

15 layers of the VGG-19 net [58] and ﬁne-tuned for the region
proposal task to generate a set of feature maps as the input to
our proposed PPN. We use a partial VGG-19 network and add
another two trainable convolutional networks after the VGG-19
network, aiming to extract discriminative features and generate
feature maps F for the subsequent proposal generation tasks. To
produce the region proposals, we construct a multi-stage class-
speciﬁc proposal generation network. In each stage, we create a
new network including an intermediate 3*3 convolutional layer
sliding on feature maps F , followed by two sibling 1*1 convo-
lutional layers for classiﬁcation and the bounding box regression
respectively. For each sliding window in the 3*3 convolutional
layer, we simultaneously predict multiple region proposals (called
root boxes) in the original image. In our method, the multiple
scales and aspect ratios of the root boxes are different for indi-
vidual objects, dependent on the object size in the image. The
classiﬁcation layer provides the conﬁdence scores of the root
boxes, as used to mine and add negative root boxes to a mini-
batch of the next stage. Each mini-batch contains several positive
and negative example root boxes from the image. Initially, all the
positive root boxes in the image are selected and the negative
root boxes are randomly sampled. In each stage, hard negative
root boxes are used to replace the previous negative root boxes in
neg ⊂ Dz,
the mini-batch. Here, we deﬁne Dz
consisting of all the positive and negative root boxes respectively
in a mini-batch at stage z. Dz is a universal set containing all the
positive, negative and unlabelled root boxes in the image.

pos ⊂ Dz and Dz

With the above deﬁnitions, we infer a multi-task loss function

for the image at stage z as follows:

Lz = −

1
N

(cid:88)





(cid:88)

log Sz

j (i) +

j

i∈Dz

pos

(cid:88)

log S

z
j (i) +

(cid:88)

Lreg

i∈Dz

neg

i∈Dz

pos

(cid:16)
Bz

j (i) − ˆBz

j (i)

(cid:17)





Here, Sz

j (i) denote the predicted probability of the root box i

(1)

4.1 Problem Formulation
Given a video sequence F containing N ∗ = {1, ..., N } parts,
we generate a set of detection candidates M ∗
t = {1, ..., Mt}
using the proposed detector at time t. For the task of multi-
mouse part tracking, our goal is to jointly address two problems:
(1) Successful assembling of the detected parts to represent each
individual mouse, and (2) correct alignment of the mouse parts to
form motion trajectories.

For the beneﬁt of representation, similar to [21, 22] we deﬁne
two binary vector spaces BMt∗Mt and BN ∗(Mt+1), and encode
these two problems through the two binary vectors of γt ∈ Γt ⊆
BMt∗Mt and θt ∈ Θt ⊆ BN ∗(Mt+1):
(cid:110)

(cid:111)

(cid:48)

(cid:48)

(cid:48)

γt ∈ Γt =

m |sm
sm

m ∈ {0, 1} m, m

∈ M ∗
t

(4)

θt ∈ Θt = (cid:8)am

n |am

n ∈ {0, 1} m ∈ M ∗

0,t, n ∈ N ∗(cid:9) (5)

(cid:48)

(cid:48)

(cid:48)

can be
(cid:48)
∈

m = 1 with m, m

m suggests that, if the detected parts m and m

where sm
used to form an individual mouse, i.e., sm
M ∗
t .
Our approach to the second problem is to assign correct iden-
tities to the detection candidates. In an easy case, the relationship
between the targets and the detection candidates is bijective, which
means that all the tracked targets are also observed and each
measurement was generated by the tracked targets. This is an
unrealistic scenario as it does not consider the false detection
and the targets’ occlusions. In order to solve this assignment
problem, we introduce a placeholder for a ‘fake’ (or missed)
detection and also deﬁne m ∈ M ∗
0 = {0, 1, ..., M } to include the
‘fake’ detection and all the detection candidates. Similar to sm
m ,
a binary variable am
n represents that the detection candidate index
0,t which is generated by target n ∈ N ∗ = {1, ..., N }
m ∈ M ∗
at time t. Here, we have N ∗ (M + 1) paired with am
n = 1 if
candidate m is assigned to target n and am
n = 0 otherwise. By
deﬁnition, Γt and Θt consist of all possible solutions at time t.

(cid:48)

4.2.1 Bayesian Inference

6

With regards to the target tracking problem, the assumption of
Markov property in the target state sequence is frequently em-
ployed in Extended Kalman ﬁlter [60], particle ﬁltering [61, 62],
MCFPHD [63] or MDP [39]. These approaches are appropriate
for the task of online tracking due to their concern on the region
neighbourhood. We also utilize this assumption to resolve the
(cid:9) denote
target assignment problem. Let Vt = (cid:8)v1
the states of all the N targets. State vector vn contains dynamic
information of the position and velocity of the jth target at time
(cid:9) represent the positions of all the M
t. Dt = (cid:8)d1
part detection candidates at time t. The assumption of Markov
property is twofold. First, the kinematic state dynamics follow
(cid:1),
a ﬁrst-order Markov chain: p (cid:0)vn
t |vn
meaning that the state vn
t−1. Second,
each observation ˆdn
t is only related to its state corresponding to
(cid:16) ˆdn
(cid:16) ˆdn
t | ˆdn
this observation: p
. Then, we have
the following formulations:

t only depends on state vn

(cid:1) = p (cid:0)vn

t−1, ..., vn
1

t , ..., dM
t

t , ..., vN
t

1:tV1:t

t |vn

t , v2

t |vn
t

t , d2

= p

t−1

(cid:17)

(cid:17)

(cid:1) ∼ N (cid:0)ˆvn

p (cid:0)vn
p (cid:0)vn
(cid:16) ˆdn

p

t−1
t |vn
t−1
(cid:17)
t |vn
t

t−1, Σn
t−1
(cid:1) ∼ N (cid:0)A ∗ vn
∼ N (C ∗ vn

(cid:1)
t−1, Ω(cid:1)
t , Υ)

(7)

t−1 and Σn

where, A and C are our state transition (or motion) and obser-
vation models respectively. ˆvn
t−1 are the mean value
and the error covariance matrix at state vn
t−1. N is the normal
distribution. Ω and Υ represent the covariances of the process
(cid:16) ˆdn
and observation noise with the mean value of zero. p
is
(cid:1):

calculated by marginalizing p

and p (cid:0)vn

(cid:16) ˆdn

(cid:17)

(cid:17)

t

t |vn

t−1

t |vn
t

(cid:17)

(cid:16) ˆdn

t

p

=

(cid:90)

(cid:16) ˆdn

t |vn
t

p

(cid:17) (cid:90)

p (cid:0)vn

t |vn

t−1

(cid:1) ∗ p (cid:0)vn

t−1

(cid:1) dvn

t−1dvn
t
(8)

t , (cid:80)N

To ensure that every solution is physically available, we add
several constraints that (1) each candidate (except for the fake
hypothesis m = 0) is assigned to at most one target, i.e., (a) for
∀m ∈ M ∗
n=1 am
n ≤ 1. (2) each target is uniquely assigned
t , (cid:80)Mt
to a candidate, i.e., (b) for ∀n ∈ N ∗
n = 1, and (3)
only the candidates and targets of the same type (e.g. head to head
or tail to tail) can be corresponded, except for a fake hypothesis,
i.e., (c) for ∀m ∈ M ∗
n = 0 if ˆm (cid:54)= ˆn. In order to
ensure that feasible solutions θt ∈ Θt and γt ∈ Γt result in a valid
target assignment and pair-wise part association, we need to apply
an additional constraint that two detected parts are connected if
and only if both detections are assigned to different targets: (d) for
(cid:48)
∀m, m

t , ∀n ∈ N ∗, am

m=0 am

∈ M ∗

m ≤ (cid:80)N

m ≤ (cid:80)N

n=1 am
n .

n ∧ sm

n=1 am

t , sm

(cid:48)

(cid:48)

(cid:48)

However, in a realistic case, Markovian assumption based methods
carry the associated danger of drifting away from the correct
target, as they treat the targets as conditionally independent of
one another. This risk can be mitigated by optimizing data as-
signment and using prior knowledge, as shown in [64]–[66]. In
our approach, when dealing with the data association problem, we
use a Geometric Model as the prior knowledge. Given K training
samples centered at a single mouse, let Xk = (cid:8)x1
(cid:9)
k, ..., xn
k
(where k = 1, ..., K) denote the locations of the mouse parts
k is the location of the ith part in the
in kth training sample. xi
kth training sample. Let Xk refer to a geometric template and
suppose that the locations of the mouse parts in a feasible solution
θ is generated by one of our geometric templates Xk. We then
expand p (θt) as follows:

k, x2

4.2 Bayesian-inference Integer Linear Program

We jointly resolve the targets assignment and part association
problems by minimizing the following cost function:

p (θt) =

K
(cid:88)

k=1

p (θt|Xk) p (Xk)

(9)

Lt = min

θt∈Θ,γt∈Γ

− (log (p (θt)) + log (p (γt))))

(6)

where our collections of K training samples have been introduced
into the calculation of p (θt) before it can be marginalized out.

where the values of θt and γt which attain the minimum value of
Eq. (6) are the maximum likelihood of target assignment and part
association at time t.

By conditioning on the geometric template Xk, the locations
of the mouse parts can be treated as conditionally independent
from one another. As the association probability of the fake

detection is not constrained by our geometric template, we rewrite
the ﬁrst term of Eq. (9) as follows:

where

(cid:89)

pt (Xk|am

n ) pt (am
n )

σm (by

t ) =

p (θt|Xk) =

(cid:89)

pt

(cid:0)a0
n

(cid:1) ∗

n∈N ∗

m∈M ∗

t ,n∈N ∗

where

pt (am

n ) ∝






((1 − ˆp (dm
(cid:16)

ˆp (dm

t ) ∗ p

n

t )) β)am
(cid:16) ˆdn

t = dm
t

(cid:17)(cid:17)am

n

pt (Xk)

(10)

if

m = 0

7






ˆp (by
t )

if

0

otherwise

region of candidate m falls
outside region of by
t by at
most υ pixels

(14)

where, ˆp (by
t ) is the detection probability and null is an empty
value. In our experiments, we set υ = 10. With these in hand, we
approximate the ﬁrst term in Eq. (12) as:

otherwise

K
(cid:88)

(cid:89)

(11)

pt (Xk|am

n ) ≈

(cid:88)

(cid:89)

pt (Xk|am
n )

k=1

m∈M ∗,n∈N ∗

k∈O

m∈M ∗,n∈N ∗

t

(cid:17)

(cid:16) ˆdn

t ) is the detection probability of the mth part detection
where ˆp (dm
time t and p
is a probability distribution
candidate at
obtained from our motion model, pt (am
n ) is an assignment prob-
ability representing that the detection candidate index m ∈ M ∗
t
is generated by target n ∈ N ∗ at time t. If m = 0, which means
the target n is missing, the pt (am
n ) can be estimated using an
empirical parameter β of the false detection density.
Combining Eqs. (7),(8), (9) and (10) yields:



p (θt) =



K
(cid:88)

k=1

∗

(cid:89)

pt (Xk|am
n )


 ∗

(cid:32)

(cid:89)

(β)a0

n

n∈N ∗

(cid:0)ˆp (dm

t ) ∗ N (cid:0)dm

t ; C ∗ A ∗ ˆvn

t−1,

m∈M ∗,n∈N ∗
(cid:89)

m∈M ∗,n∈N ∗

(cid:16)

C ∗

A ∗ Σn

t−1 ∗ AT + Ω

(cid:17)

∗ C T + Υ

(cid:17)(cid:17)am

n (cid:19)

(12)

t , b2

where, p (Xk|am
n ) represents that, if candidate m is assigned to
target n, i.e., the location of target n is known, how well the
part location in the kth geometric template ﬁts the location of this
target. Computing the sum as shown in Eq. (12) is challenging due
to a large amount of training samples. However, we notice that,
if p (Xk|am
n = 1) is very small, it will be unlikely to contribute
much to the ﬁnal outcome. Thus, we mainly consider geometric
templates with large p (Xk|am
n = 1). To achieve this, we ﬁrstly
ﬁnd the O nearest neighbours of the body in the training samples
to match (cid:101)bm ∈ B = (cid:8)b1
(cid:9) in the convolutional feature
t , ..., bY
t
space, where B is a set of body candidates with Y elements
which are generated by the proposed detector at time t and (cid:101)bm
is the top-scoring body candidates which contain the candidate
n ) of the mth part
m. Then the probability distribution p (Xk|am
can be estimated by ﬁtting a Gaussian model to the location of
this part in the O body neighbours. Different from most top-down
methods [7, 27, 52] that ﬁrst track the mouse and then estimate
the locations of mouse parts, our approach establishes geometric
relationships between the mouse parts and the mouse body within
our probabilistic model and tracks the mouse parts directly based
on our probabilistic model, therefore our approach tolerates partial
occlusion of the mouse body. The selection of the body targets can
be formulated as follows:

(cid:101)bm =






argmaxby

t ∈B σm (by
t )

if

null

otherwise

t ∈B σm (by
t )

maxby
(cid:54)= 0

(13)

(15)

where the sum is now only taken over those k ∈ O. The term
pt (Xk|am
n ) is a 2D Gaussian distribution centred at the location
xk
m,n with a user-deﬁned variance. According to the proof of
Supplementary C, we obtain:

(cid:88)

(cid:89)

pt (Xk|am

n ) =

(cid:89)

(cid:88)

pt (Xk|am
n )

k∈O

m∈M ∗,n∈N ∗

m∈M ∗,n∈N ∗

k∈O

(16)

If O (cid:54)= ∅, we alternatively ﬁt a Gaussian model to the location
of each part in the O body neighbours and use this equation to
estimate the sum as shown in Eq. (16), given the type (e.g. head or
tail base) of the candidate m. Otherwise, we treat the sum in Eq.
(16) as a very small variable value of ε = 0.0001. We reformulate
Eq. (12) as follows:
(β)a0

(∆(am

p (θt) ≈

n ) ∗ ˆp (dm

t ) ∗

n ∗

(cid:89)

(cid:89)

n∈N ∗
(cid:16)

m∈M ∗,n∈N ∗

dm
t ; C ∗ A ∗ ˆvn
N
+Υ))am

n

t−1, C ∗

(cid:16)

A ∗ Σn

t−1 ∗ AT + Ω

(cid:17)

∗ C T

(17)

where ∆(am
n ) deﬁnes a scoring function, estimated by our pro-
posed geometric model given the location of the candidate m in
the body target.

4.2.2 Parts association model
To improve the multi-target tracking accuracy, several methods (
[67]–[69]) have also presented group models, in which each object
is considered as having the relationship with other objects and
surroundings. These models can alleviate performance deteriora-
tion in crowded scenes. Similarly, we here propose a new part
association model to establish the pair relationship between the
detection candidates. In our approach, the pair-wise term p (γt)
shown in Eq. (6) is deﬁned as follows:

p (γt) =

(cid:89)

(cid:17)sm(cid:48)

m

(cid:16)

sm(cid:48)
m

η

(18)

m,m(cid:48)∈M ∗
(cid:16)

(cid:17)

(cid:16)

(cid:16)

(cid:17)

sm(cid:48)
m
(cid:17)

t , dm(cid:48)
dm

t

/

1 − pt

= pt

where,
(cid:16)

∈ (0, 1) corresponds to the probability that de-

η
t , dm(cid:48)
dm
pt
t
tections dm
in a frame t belong to the same mouse.
t
(cid:17)(cid:17)
> 0.5.
Note that log
Since we aim to minimize the cost function shown as Eq. (6), a

and dm(cid:48)
t
(cid:16)
(cid:16)
sm(cid:48)
ηt
m

is negative if pt

t , dm(cid:48)
dm

(cid:16)

(cid:17)

.

t

t

(cid:16)

t , dm(cid:48)
dm

(cid:17)(cid:17)

(cid:16)

(cid:17)(cid:17)

(cid:16)
sm(cid:48)
m

t

t

t

t

(cid:17)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

=

ηt

t , dm(cid:48)
dm

t , dm(cid:48)
dm
t and dm(cid:48)

smaller log
i.e., a pair detection with a higher parts
association probability are preferred. The parts association prob-
depends on the types ˆm and ˆm(cid:48) of the part
ability pt
. If ˆm = ˆm(cid:48), we deﬁne pt
detections dm
(cid:16)
t , dm(cid:48)
dm

IoU
. This means that two close detections denoting
the same part belong to the same mouse. If the connection between
the two detections of the same type exists, the detections are
merged with the weighted mean of the detections, where the
t ). If ˆm (cid:54)= ˆm(cid:48), we estimate the posterior
weights are equal to ˆp (dm
(cid:17)
probability pt
based
on the Euclidean distance between the two detections. Assuming
the prior probability p
= 0.5, then
we have
(cid:16)
sm(cid:48)
m = 1|dis

(cid:16)
sm(cid:48)
m = 1|dis

(cid:17)
(cid:16)
sm(cid:48)
m = 1

(cid:17)
sm(cid:48)
m = 0

t , dm(cid:48)
dm

t , dm(cid:48)
dm

= p

= p

(cid:17)(cid:17)

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

p

t

t

t

t , dm(cid:48)
dm
(cid:16)

=

p (cid:0)dis (cid:0)dm

(cid:16)

(cid:17)
m = 1
dis
p
m = 1(cid:1) + p (cid:0)dis (cid:0)dm
(cid:1) |sm(cid:48)
t , dm(cid:48)
t

t , dm(cid:48)
dm

|sm(cid:48)

(cid:17)

t

t , dm(cid:48)
t

(cid:1) |sm(cid:48)

m = 0(cid:1)
(19)

t

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

dis

|sm(cid:48)

t , dm(cid:48)
dm

(cid:17)
where p
m = 1
denotes the positive likeli-
hood where two detections come from the same mouse. We esti-
(cid:17)
t , dm(cid:48)
dm
mate it by creating a 1-D Gaussian distribution of dis
from the positive training examples. The negative likelihood
p
is estimated in a similar way by
using the negative training examples. With the above deﬁnition,
ˆm = ˆm(cid:48) pairwise terms facilitate merging the mouse part
candidates with the same type candidates. ˆm (cid:54)= ˆm(cid:48) pairwise terms
allow us to connect the mouse part candidates within a valid mouse
pose.

(cid:17)
m = 0

t , dm(cid:48)
dm

|sm(cid:48)

dis

(cid:16)

(cid:17)

t

t

4.2.3 Optimization
By introducing Eqs. (9) and (18), we rewrite the cost function of
Eq. (6) as follows:

L = min −





(cid:88)

a0
n ∗ log β +

(cid:88)

n ∗ log (∆(am
am

n )∗

ˆp (dm

t ) ∗ N

n∈N ∗
(cid:16)
t ; C ∗ A ∗ ˆvn
dm

m∈M ∗

t ,n∈N ∗
(cid:16)

t−1, C ∗

A ∗ Σn
t−1 ∗ AT + Ω


(cid:17)

∗C T + Υ

(cid:17)(cid:17)

+

(cid:88)

sm(cid:48)
m log pt

m,m(cid:48)∈M ∗
t

(cid:17)

(cid:16)
sm(cid:48)
m



s.t.

(a),(b),(c),(d)

in section 4.1

(20)

Finally, the problems of targets assignment and part association
are jointly reformulated as:

min
Λ∈{0,1}J

ΦT Λ

s.t. Ay ≤ h

(21)

where Λ = [θ, γ]T is a binary vector of length J = N (M + 1)+
M ∗ M , and Φ = [Φ1, ..., ΦJ ]T is the cost vector with
n )pt (dm
Φj = − log (β) if 1 ≤ j ≤ M , Φj = − log (∆(am
t ) N )
(cid:16)
(cid:16)
(cid:17)(cid:17)
sm(cid:48)
pt
if M < j ≤ N (M + 1) and Φj = − log
if
m
N (M + 1) < j ≤ N (M + 1) + M ∗ M . A and h stand for
a constraint matrix and linear equality constraints respectively,
resulting in constraints (a), (b), (c) and (d).

To solve Eq. (21), we ﬁrst relax the constraint sm

{0, 1} , am

n |am

n ∈ {0, 1} to 0 ≤ sm

m ≤ 1, 0 ≤ am

(cid:48)

(cid:48)

m ∈
n ≤ 1. Then,

8

we utilize a branch-and-bound [70] based global optimization
method which is summarized and shown in Supplementary E.
With the convergence analysis of the proposed algorithm shown
in Supplementary E, it is proved that the algorithm converges to
the global optimum.

The time complexity to solve Eq. (21) is O (cid:0)A1.5B2(cid:1) [71],
where B = N ∗ (M + 1) + M ∗ M denotes the number of the
variables and A = N + M + 1 + M ∗ M denotes the number of
the constraints.

4.3 Targets state update

After having obtained an optimal solution of Eq. (21), we associate
each target with its corresponding detection candidate. If a target is
assigned to a fake detection candidate, we assume that the target
is occluded or lost. In order to guarantee tracking identiﬁcation
of each mouse part, we do not discard these targets like JPDAm
[45] and MDP [39] (see their tracking results in Fig.S8 and S9).
Instead we predict the target state using the motion model ˆvn
t =
A ∗ ˆvn
t−1 till the target is detected again. If a target is assigned to
a true detection candidate, our goal is to maximize the posterior
probability of each target state ˆvn
t . The
updated formula can be represented as follows:

t given the detection dm

ˆvn
t = argmax

vn
t

p (vn

t |dm
t )

(22)

To compute the posterior probability p (vn

t |dm

t ), we ﬁrstly deﬁne

P = A ∗ Σn

t−1 ∗ AT + Ω

(23)

Following the Bayes’ rule, we derive the posterior probability as
follows:

p (vn

t |dm

t ) ∼N

(cid:18)(cid:16)

P −1 + C T Υ−1C

(cid:17)−1 (cid:16)

C T Υ−1dm

t +

P −1Aˆvn

t−1

(cid:16)

(cid:1) ,

P −1 + C T Υ−1C

(cid:17)−1(cid:19)

(24)

where parameters N , C and Υ have been deﬁned as those of Eq.
(7).

Afterwards, the estimated state ˆvn

t is the mean of the Gaussian

distribution:

(cid:16)

ˆvn
t =

P −1 + C T Υ−1C

(cid:17)−1 (cid:16)

C T Υ−1dm

t + P −1Aˆvn

t−1

(cid:17)

(25)

Note that we solve the part tracking problem of multi-mice in
an online manner, which is illustrated in Supplementary F and is
summarized in Alg. 1.

Algorithm 1 Algorithm for online mouse part tracking.
Input: a video sequence F , the proposed mouse part and body

detector, C training samples of a single mouse.

Output: bounding boxes of all the targets.

1: for t = 1 to T do
2:

Generate a set of part and body detection candidates using
the proposed mouse part and body detector on the frame Ft

3:

4:

5:

6:
7:

8:
9:

(cid:16)

(cid:17)

sm(cid:48)
m

in Eq. (17);

n ), N (dm

Fit a geometric model with appearance features to O neigh-
bours of the top-scoring body candidate, which contain
candidate m;
Compute ∆ (am
t ) and pt
Construct constraint matrix A in Eq. (21) using the con-
straint formula (a), (b), (c) and (d);
Construct cost vector Ψ in Eq. (21);
Obtain best solution of θ and γ by solving Eq. (21) in Alg.
S2;
Update the state of each target using Eq. (25);
Obtain bounding boxes of all the targets dependent on the
best solution of θ and γ;

10: end for
11: return bounding boxes of all the targets.

5 EXPERIMENTAL SET-UP

5.1 Our Multi-Mice Parts Track Dataset

In this paper, we introduce our new dataset for multi-mice part
tracking in videos. The dataset was collected in collaboration with
biologists of Queen’s University Belfast, United Kingdom, for a
study of neurophysiological mechanisms involved in Parkinson’s
disease. In our dataset, two or three mice are interacting freely
in a 50*110*30cm home cage and are recorded from the top
view using a Sony Action camera (HDR-AS15) with a frame
rate of 30 fps and 640 by 480 pixels VGA video resolution. All
the experiments are conducted in an environment-controlled room
with constant temperature (27◦C) and light condition (long ﬂuo-
rescent lamp 40W). The dataset provides the detailed annotations
for multiple mice in each video, as shown in Supplementary G.
The mice used throughout this study were housed under constant
climatic conditions with free access to food and water. All the
experimental procedures were performed in accordance with the
Guidance on the Operation of the Animals (Scientiﬁc Procedures)
Act, 1986 (UK) and approved by the Queen’s University Belfast
Animal Welfare and Ethical Review Body. Our database covers a
wide range of activities like contacting, following and crossing.
Moreover, our database contains a large amount of mouse pose
and mouse part occlusion. After proper training, six professionals
were invited to annotate mouse heads, tail bases and localise each
mouse body in the videos. We assign a unique identity to every
mouse part appearing in the images. If a mouse part was in the
ﬁeld-of-view but became invisible due to occlusion, it is marked
‘occluded’. Those mouse parts outside the image border limits are
not annotated. In total, our dataset yields 5 videos of two mice and
5 videos of three mice, each video lasts 3 minutes and 400 frames
randomly sampled from each video are annotated (4000 frames
are annotated in total). Since the problem of multi-mice parts has
not been quantitatively evaluated in the literature, we follow the
evaluation metrics deﬁned in multi-target tracking [72, 73] for this
problem, and compare the results with several baseline methods.
We split the dataset into training and testing sets with an equal

duration of time and train our network based on transfer learning
with pre-trained models. The code and the dataset are published
on github: https://github.com/ZhehengJiang/BILP.

9

5.2 Evaluation metrics

In order to evaluate the proposed mouse part detector, we use
the widely adopted precision, recall, average precision (AP) and
mean average precision (mAP) [72]. For a speciﬁc class, the
precision value corresponds to the ratio of the positive object
detections against the total number of the objects that the classiﬁer
predicts, while the recall value is deﬁned as the percentage of the
positive object detections against the total number of the objects
labelled as ground-truth. The precision-recall curves are obtained
by varying the model score threshold in the range of 0 and 1,
which determines what is counted as a positive detection of the
class. Note that, with our metrics, only the object detections that
have an IoU ratio over 0.5 with the ground-truth are counted as
positive detections while the rest are negatives. The AP score is
deﬁned as the average of precision at the set of 11 equally spaced
recall values. mAP is just the average over all the classes. To
provide a fair comparison for both types of occlusion handling,
we consider an occluded mouse part correctly estimated either if
(a) it is predicted at the correct location despite being occluded, or
(b) it is not predicted at all. Otherwise, the prediction is considered
as a false positive.

To evaluate the part tracking performance, we consider each
mouse part trajectory as one individual target, and compute the
multiple object tracking precision (MOTP) and the multiple object
tracking accuracy (MOTA) [74]. The former is derived from
three types of error ratios: false positives (FP), missed targets
(MT), and identity switches (IDs). These errors are normalized
by the number of the objects appearing in the image frames and
can be summed up to produce the resulting tracking accuracy,
where 100% corresponds to zero errors. MOTP measures how
precise each mouse part has been localized. We also report the
trajectory-based measures of the number of mostly tracked (MT)
and mostly lost (ML) targets. If a track hypothesis has covered at
least 80% of its life span based on the ground truth trajectories,
it is considered as MT. If less than 20% are not tracked, the
track hypothesis is considered as ML. IDF1 [73] measures the
ratio of the correctly identiﬁed detections. OSPA computes the
optimal subpattern assignment metric between a set of tracks and
the ground truth.

6 EXPERIMENTAL RESULTS
In this section, we evaluate the proposed method for part tracking
on the newly introduced Multi-Mice PartsTrack dataset.

6.1 Evaluation of Part and Body Detector

6.1.1 Implementation
The multiple scales and aspect ratios of the root boxes in our
method are different for mouse parts and body. Root boxes of
inappropriate scales and aspect ratios are ineffective for mouse
detection. For mouse ‘head’ and ‘tail base’, we choose a single
aspect ratio of 1.13 (width to height) and four scales with the root
box widths of 24, 29, 35 and 42 pixels based on the statistics of the
object shape in the training set. For mouse ‘body’ cases, we use
multiple aspect ratios of 0.5 (landscape),1 (square) and 2 (portrait),
and three scales with the root box widths of 50, 80 and 128 pixels.

TABLE 1
Performance (precision) of the proposed part detector using different
methods.

TABLE 2
Comparisons of different classiﬁers and features.

LBP

HOG

SIFT + FV

Conv

Conv + LBP

Conv + HOG

Classiﬁer
linear SVM
fc layers
linear SVM
fc layers
linear SVM
fc layers
linear SVM
fc layers
linear SVM
fc layers
linear SVM
fc layers

head
93.1%
95.8%
93.2%
94.1%
77.6%
78.1%
92.9%
93.2%
96.5%
98.8%
93.3%
94.5%

tail base
92.2%
94.3%
85.3%
87.2%
72.4%
73.8%
91.1%
92.3%
95.4%
97.3%
89.5%
90.6%

Layers

head

tail
base

body mAP

ROI Feature

method

faster
rcnn [55]
ssd300 [75]

ssd512 [75]

YOLO [76]

Motr [16]
Blob detec-
tion [14]

ours

Pre-
trained
Model
ResNet50
ResNet101
VGGNet16
[58]
VGGNet16
[58]
Darknet
[76]
/
/

AlexNet [77]

VGGNet16
[58]

VGGNet19
[58]

/
/
/

/

/

/
/

93.5% 76.0% 97.1% 88.9%
96.3% 80.2% 95.6% 90.7%
86.9% 63.7% 98.1% 82.9%

90.6% 75.4% 98.2% 88.1%

93.6% 77.3% 98.6% 89.8%

/
/

/
/

82.4% /
89.8% /

5 layers
9 layers
15 layers
6 layers
11 layers
18 layers
6 layers
11 layers
15 layers
20 layers

88.8% 44.8% 86.9% 73.5%
49.5% 25.7% 89.5% 54.9%
35.2% 12.4% 86.8% 44.8%
89.9% 67.5% 77.3% 78.2%
91.5% 74.2% 90.3% 85.3%
85.8% 51.0% 89.8% 75.5%
91.5% 64.0% 87.3% 80.9%
95.6% 78.4% 95.4% 89.8%
98.4% 89.4% 98.3% 95.4%
76.8% 42.1% 91.5% 70.1%

10

background
94.7%
95.1%
83.4%
83.7%
60.8%
61.7%
94.7%
96.3%
95.3%
96.0%
90.3%
92.3%

We label a root box as a positive example if its IoU ratio is greater
than 0.7 with one ground-truthed box, and a negative example if
its IoU ratio is lower than 0.3 with all the ground-truthed boxes.
Root boxes that are neither positive nor negative do not contribute
to the training RPNs. This experiment is conducted on the 2-mice
dataset.

6.1.2 Results

We ﬁrstly investigate the performance of the proposed part and
body detector using different pre-trained models. All the ex-
periments adopt a 3-stage Part Proposal Network based on the
trade-off between speeds and system performance, as shown in
Supplementary H Fig.S3. From Table 1, we observe that deeper
network (VGG19) has achieved superior performance over other
shallow networks (i.e., AlexNet and VGG16). It is because the
deeper network can learn richer image representations. But sur-
prisingly, the accuracy of the mouse parts is degraded after using
more than 9 layers of AlexNet, 11 layers of VGGNet16 or 15
layers of VGGNet19. This limitation is partially because of the
low-resolution features in the feature map of the higher layer.
These features are not discriminative on the small regions, and
thus degrade the performance of the downstream classiﬁer. In
comparison, these features are cooperative enough to distinguish
the mouse body from the background as the region area of the
mouse body is 3 times larger than that of the mouse head and
the tail base. This result also suggests that, if reliable features
can be extracted, the downstream classiﬁer is able to improve the
detection accuracy. In addition, using the network with layers less
than 6 layers of VGGNet 16 or 6 layers of VGGNet 19 starts
to demonstrate accuracy degradation, which can be explained
by the weaker representation ability of the shallower layers. In
Table 1, We have also compared DNN based detectors with
two conventional methods [14, 16], which only provide methods
to detect mouse body. The DNN based detectors show better
performance than the conventional method in body detection.

(a) head

(b) tail

Fig. 4. ROC curve shows the true positive rate (TPR) against the false
positive rate (FPR) for different mouse parts.

In Table 2, we report the capability of different features to
distinguish mouse parts and the background. Since CNN features
perform better than the other hand-crafted features for body
detection, we do not exploit any hand-crafted feature to maintain
the system accuracy. For fair comparisons, all the proposals are
generated from the same PPN which is initialized by the ﬁrst
15 layers of VGG-19. We produce part proposals from images
and extract
low-level features such as LBP, HOG and SIFT.
Interestingly, training our fc layers with LBP features on the
same set of the PPN proposals actually has led to better average
accuracy 95.1%(vs. Conv’s 93.9% and HoG’s 88.3%), shown in
Table 2. Combining LBP and Conv features, we can achieve the
best result 97.37%. Similar results are also obtained using a linear
SVM classiﬁer. But the fc layers are more ﬂexible and can be
trained with other layers in an end-to-end style. Since too few
interest points are detected from the mouse head and the tail base,
SIFT features encoded by Fisher Vector [80] present a much worse
result on this task. Figure 4 clearly shows that our combined LBP
and conv features lead to the best performance for classifying
mouse ’head’ and ’tail base’ proposals.

We also compare our detection network against Faster R-CNN
[55], SDD [75] and YOLO [76] on our dataset. These methods are
also trained based on transfer learning with pre-trained models.
We train Faster R-CNN based on the two pre-trained models of
Residual Network 50 and 101 [81]. SSD [75] has two input sizes
(300 × 300 vs. 512 × 512). The experiments show that the model
with a larger input size leads to better results. YOLO uses its
own pre-trained model named Darknet. As shown in Figure 5,
most methods have high APs on the detection of head and body.
However, the performance of Faster RCNN, SSD and YOLO falls
considerably when they are applied to detecting the tail base. Our

(a) head

(b) tail

(c) body

Fig. 5. Precision/Recall curves of different methods on the 2-mice dataset.

TABLE 3
Quantitative evaluation of multi-mice parts tracking on the 3-Mice dataset.

Method

(speed)

(30.8 fps)

All
All(b,c,d)
All(a,c,d)
All(a,b,d)
All(a,b,c)

All without geometric model

(34.7 fps)

All without motion model

(31.6 fps)

All without parts association model

(31.2 fps)

(20.6 fps)

(2.5 fps)
(260 fps)

(32.6 fps)

MOTDT [78]
MDP [39]
SORT [40]
JPDAm [45]
RNN-LSTM [46]
CEM [41]
DeepSORT [79]

(7.7 fps)

(20 fps)

(50 fps)

MOTA
% ↑

MOTP
% ↑

MT
↑

ML
↓

21

61.2

Impact of the constraints
25
81.1
65.5
24
65.6
80.2
25
65.6
79.5
22
66.0
75.4
24
65.8
74.8
Impact of the geometric model
66.6
70.3
Impact of the motion model
65.5
Impact of the parts association model
65.4
Comparison with the state-of-the-art
31.4
70.2
70.5
65.1
67.3
69.5
32.4

20.5
64.6
59.9
67.0
50.6
34.1
42.7

21
21
19
26
20
21
19

76.3

24

22

0
0
0
0
0

0

2

1

0
0
1
0
0
1
0

FP
↓

566
581
615
738
764

892

FN
↓

546
571
595
718
735

872

1101

1061

688

1592
1114
971
1264
1688
2637
1796

693

1152
994
1331
548
1176
1093
1419

IDs
↓

12
26
12
12
12

12

152

30

3103
18
106
155
102
224
212

IDF1
% ↑

91.2
89.7
90.75
90.2
89.2

88.8

76.14

88.5

20.2
/
15.8
44.6
54.6
/
44.4

11

OSPA
↓

6.55
/
/
/
/

/

/

/

26.47
21.26
15.42
20.14
22.16
24.75
18.63

detection method has better performance to detect the tail base
and its AP degrades less than the other methods. We also observe
that if the threshold of detection scores was previously set high,
lowering the threshold in Faster RCNN, SSD and YOLO will
introduce more false positives than our methods, resulting in a
sharp decrease of precision as shown in Figure 5.

6.2 Evaluation of Multi-mice parts tracking

target’s

tracking algorithm, each part

6.2.1 Implementation
In our
state con-
tains position, velocity and shape of bounding box xt =
(xp, xv, yp, yv, wp, hp). (xp, yp), (xv, yv) and (wp, hp) respec-
tively denote the position, the velocity and the shape of each part
target. For the shape of each target, we use the width and height
of the detected bounding box. We model the motion of each part
target based on the linear dynamical system discretized in the time
domain and predict the state of each part target in the next image
frame as xt = Axt−1+µ, where A = diag(Ax, Ay) is a constant
transition model, and Ax = Ay = [1, τ ; 0, 1]. µ is a system
noise and subject to a Gausssian distribution with covariance
Ω = diag(Ωx, Ωy), Ωx = Ωy = qd[τ 3/3, τ 2/2; τ 2/2, τ ].

Here τ = 1 and qd = 0.5 refer to the sampling period and the
process noise parameter respectively. The uniform clutter density
β is estimated as β = ff alse/ (wim ∗ him),wim = 480 and
him = 640 are the width and height of the image, ff alse = 0.1 is
the average number of the false detections per image frame. This
experiment is conducted on both our 2-Mice and 3-Mice datasets.
We compare the proposed approach against six baselines following
their default settings.

6.2.2 Results
The results of multi-mice part tracking are reported in Table 3 and
Supplementary I Table S2. We quantitatively evaluate the proposed
system using the commonly seen multi-object tracking metrics.
The Up and Down arrows in the table indicate whether higher or
lower values are obtained. To evaluate the proposed optimization
objective function (Eq. (6)) for multi-mice part tracking, we have
quantiﬁed the impact of different constraints (a)-(d) during the
optimization. To this end, we optimize the problem by removing
one constraint at a time. As shown in Table 3, all the types of the
constraints make evident contributions to the system performance,
as they ensure that every solution is physically feasible. We also
examine the impact of the geometric, motion and part association

models. Removing one of the three components signiﬁcantly de-
creases the performance. In particular, the motion model plays the
most crucial role, which is obvious on the 3-mice Dataset. The part
association and geometric models can effectively reduce ID switch
and FP respectively. This is expected because these two models
help to obtain the best target assignment and part association.
We observe the change of our approach’s speeds by removing
different components and the geometric model has the biggest
impact on speeds. We also implement several standard multi-target
tracking methods on our multi-mice PartsTrack dataset. For fair
comparison, we use the proposed PPN detector to generate part
bounding boxes, and perform part tracking using several state-
of-the-art part trackers [39]–[41, 45, 46, 78]. As we can see, at
the bottom of Table 3 and Supplementary I Table S2, our tracker
achieves the highest MOTA scores. Our method results in the
lowest number of ID switches and highest IDF1 scores. This is
primarily due to our powerful geometric and pair based part asso-
ciation models, which can handle part identities more robustly. We
conduct ablation studies on the multi-stage training design shown
in Fig. S3, showing the effectiveness of our training strategy. The
established SORT [40] has achieved the best efﬁciency due to the
use of a simple target assignment model, where the cost function
is fully dependent on IOU distance between each detection and the
predicted bounding box. Evidence shows that this method cannot
maintain consistent tracking performance (see Table 3 and S3).
Note that the speed of all approaches reported in Table 3 and S3
excludes the time of the part candidate detection. Fig. S5 and Tab.
S2 show the tracking results of conventional methods (Motr and
MiceProﬁler), which track mice by ﬁtting ellipses and hard-coded
geometric models respectively. From Fig. S5, we observe that if
the mice are very close, the models used in these methods cannot
be properly ﬁtted, leading to tracking targets’ swaps or incorrect
part localization. Moreover, these models are hard-coded in the
system, and thus limit the ﬂexibility of their methods. Figs. S6, S8
and S9 in Supplementary F provide the qualitative comparison of
the proposed tracking method against MDP [39] and JPDAm [45]
using the same detection results. As shown in Fig. S6, MDP swap
identities between targets 2 and 5 after the occlusion is caused
by ‘Pinning’ in the image of the middle column, while JPDAm
assigns a new identity to target 6 after it is occluded by target
3. Similar problems can also be witnessed in Figs. S8 and S9.
Compared to these two tracking methods, the proposed approach
correctly integrates the detection results with the tacking practice
and predicts the occlusion.

Overall, our approach is not the fastest in multi-mice tracking,
but sustains the best accuracy. Fig. S4 shows exemplar tracking re-
sults of the test sequences in the proposed Multi-Mice PartsTrack
dataset.

6.3 Generalization to Other Datasets

To demonstrate the generalization of our proposed approach, we
also implement the approach on two public datasets related to
other insects or animals: locusts were recorded in a laboratory
setting and zebras recorded in the wild [82]. The publicly available
datasets of multiple animals recorded from the top view are scarce
and the two datasets appearing here are examples. They contain
700 and 900 annotated frames for training and validation, 1300
and 1600 unlabeled frames for testing, respectively. Following
[82], we split the annotated datasets into 90% training examples
and 10% validation examples. Our detection approach achieves

12

100% accuracy of detecting animal’s heads,
tails and bodies
on both datasets. Since their testing datasets are unlabelled, we
illustrate the performance of the proposed approach against these
datasets in the supplementary videos. In comparison, our dataset
seems more challenging, where the mouse parts are deformable
with more occlusions during moving. Moreover, our annotated
dataset is larger with a quantitative test-bed for part detection and
tracking of multiple mice.

7 CONCLUSION

In this paper, we have presented a novel method for markerless
multi-mice part detection and tracking. We have demonstrated
that the proposed multi-stage part and body detector performed
effective hard negative mining and achieved satisfactory detection
results. We also proposed a new formulation based on target
assignment with the learned geometric constraints and a pair-wise
association scheme with motion consistency and restriction. More-
over, we presented a challenging annotated dataset to evaluate the
proposed algorithms for multi-mice part tracking. Experimental
results on the proposed datasets demonstrate that the proposed
algorithm outperformed other state-of-the-art and conventional
methods. We also demonstrate the generalization ability of our
approach on other species. Our future work will address social
behaviour analysis using the proposed tracking method.

REFERENCES

[1] C. Liu, J. Wang, H. Li, C. Fietkiewicz, and K. A. Loparo, “Modeling
and analysis of beta oscillations in the basal ganglia,” IEEE transactions
on neural networks and learning systems, vol. 29, no. 5, pp. 1864–1875,
2017. 1

[2] A. Vogel-Ciernia, D. P. Matheos, R. M. Barrett, E. A. Kram´ar, S. Azzawi,
Y. Chen, C. N. Magnan, M. Zeller, A. Sylvain, J. Haettig et al., “The
neuron-speciﬁc chromatin regulatory subunit baf53b is necessary for
synaptic plasticity and memory,” Nature neuroscience, vol. 16, no. 5,
pp. 552–561, 2013. 1

[3] A. V. Kalueff, A. M. Stewart, C. Song, K. C. Berridge, A. M. Graybiel,
and J. C. Fentress, “Neurobiology of rodent self-grooming and its value
for translational neuroscience,” Nature Reviews Neuroscience, vol. 17,
no. 1, pp. 45–59, 2016. 1

[4] H. Wang, J. Peng, X. Zheng, and S. Yue, “A robust visual system for
small target motion detection against cluttered moving backgrounds,”
IEEE transactions on neural networks and learning systems, 2019. 1, 2
J. A. Villacorta-Atienza and V. A. Makarov, “Neural network architecture
for cognitive navigation in dynamic environments,” IEEE transactions on
neural networks and learning systems, vol. 24, no. 12, pp. 2075–2087,
2013. 1

[5]

[6] A. F. Araujo and O. V. Santana, “Self-organizing map with time-varying
structure to plan and control artiﬁcial locomotion,” IEEE transactions on
neural networks and learning systems, vol. 26, no. 8, pp. 1594–1607,
2014. 1

[7] F. De Chaumont, R. D.-S. Coura, P. Serreau, A. Cressant, J. Chabout,
S. Granon, and J.-C. Olivo-Marin, “Computerized video analysis of
social interactions in mice,” Nature methods, vol. 9, no. 4, p. 410, 2012.
1, 2, 3, 4, 7, 5

[8] H. Jhuang, E. Garrote, X. Yu, V. Khilnani, T. Poggio, A. D. Steele,
and T. Serre, “Automated home-cage behavioural phenotyping of mice,”
Nature communications, vol. 1, no. 5, pp. 1–9, 2010. 1

[9] Z. Jiang, D. Crookes, B. D. Green, Y. Zhao, H. Ma, L. Li, S. Zhang,
D. Tao, and H. Zhou, “Context-aware mouse behavior recognition us-
ing hidden markov models,” IEEE Transactions on Image Processing,
vol. 28, no. 3, pp. 1133–1148, 2018. 1

[10] X. P. Burgos-Artizzu, P. Doll´ar, D. Lin, D. J. Anderson, and P. Perona,
“Social behavior recognition in continuous video,” in Proc. CVPR, 2012.
1

[11] J. Altmann, “Observational study of behavior: sampling methods,” Be-

haviour, vol. 49, no. 3, pp. 227–266, 1974. 1

[12] M. B. Sokolowski, “Social interactions in “simple” model systems,”

Neuron, vol. 65, no. 6, pp. 780–794, 2010. 1

[13] A. Spink, R. Tegelenbosch, M. Buma, and L. Noldus, “The ethovision
video tracking system—a tool for behavioral phenotyping of transgenic
mice,” Physiology & behavior, vol. 73, no. 5, pp. 731–744, 2001. 1
[14] L. Giancardo, D. Sona, H. Huang, S. Sannino, F. Manag`o, D. Scheggia,
F. Papaleo, and V. Murino, “Automatic visual tracking and social be-
haviour analysis with multiple mice,” PloS one, vol. 8, no. 9, p. e74557,
2013. 1, 3, 10

[15] M. J. Galsworthy, I. Amrein, P. A. Kuptsov, I. I. Poletaeva, P. Zinn,
A. Rau, A. Vyssotski, and H.-P. Lipp, “A comparison of wild-caught
wood mice and bank voles in the intellicage: assessing exploration,
daily activity patterns and place learning paradigms,” Behavioural brain
research, vol. 157, no. 2, pp. 211–217, 2005. 1

[16] S. Ohayon, O. Avni, A. L. Taylor, P. Perona, and S. R. Egnor, “Automated
multi-day tracking of marked mice for the analysis of social behaviour,”
Journal of neuroscience methods, vol. 219, no. 1, pp. 10–19, 2013. 1, 3,
10, 5

[17] S. Ballesta, G. Reymond, M. Pozzobon, and J.-R. Duhamel, “A real-
time 3d video tracking system for monitoring primate groups,” Journal
of neuroscience methods, vol. 234, pp. 147–152, 2014. 1

[18] R. Dennis, R. Newberry, H.-W. Cheng, and I. Estevez, “Appearance
matters: artiﬁcial marking alters aggression and stress,” Poultry science,
vol. 87, no. 10, pp. 1939–1946, 2008. 1

[19] K. Branson and S. Belongie, “Tracking multiple mouse contours (without
too many samples),” in Computer Vision and Pattern Recognition, 2005.
CVPR 2005. IEEE Computer Society Conference on, vol. 1.
IEEE, 2005,
pp. 1039–1046. 1, 3

[20] A. P´erez-Escudero, J. Vicente-Page, R. C. Hinz, S. Arganda, and G. G.
De Polavieja, “idtracker: tracking individuals in a group by automatic
identiﬁcation of unmarked animals,” Nature methods, vol. 11, no. 7, p.
743, 2014. 1, 3

[21] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V.
Gehler, and B. Schiele, “Deepcut: Joint subset partition and labeling for
multi person pose estimation,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016, pp. 4929–4937. 2,
4, 6

[22] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele,
“Deepercut: A deeper, stronger, and faster multi-person pose estimation
model,” in European Conference on Computer Vision. Springer, 2016,
pp. 34–50. 2, 4, 6

[23] Y. Zheng, L. Sun, S. Wang, J. Zhang, and J. Ning, “Spatially regularized
tracking,” IEEE
structural support vector machine for robust visual
transactions on neural networks and learning systems, vol. 30, no. 10,
pp. 3024–3034, 2018. 2, 3

[24] S. Tang, B. Andres, M. Andriluka, and B. Schiele, “Subgraph decompo-
sition for multi-target tracking,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015, pp. 5033–5041. 2

[25] X. Liu, D. Tao, M. Song, L. Zhang, J. Bu, and C. Chen, “Learning
to track multiple targets,” IEEE transactions on neural networks and
learning systems, vol. 26, no. 5, pp. 1060–1073, 2014. 2

[26] E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang, E. Levinkov,
B. Andres, and B. Schiele, “Arttrack: Articulated multi-person tracking
in the wild.” 2

[27] C. Twining, C. Taylor, and P. Courtney, “Robust tracking and posture
description for laboratory rodents using active shape models,” Behavior
Research Methods, Instruments, & Computers, vol. 33, no. 3, pp. 381–
391, 2001. 3, 4, 7, 1

[28] H. Pistori, V. V. V. A. Odakura, J. B. O. Monteiro, W. N. Gonc¸alves, A. R.
Roel, J. de Andrade Silva, and B. B. Machado, “Mice and larvae tracking
using a particle ﬁlter with an auto-adjustable observation model,” Pattern
Recognition Letters, vol. 31, no. 4, pp. 337–346, 2010. 3, 1

[29] W. Hong, A. Kennedy, X. P. Burgos-Artizzu, M. Zelikowsky, S. G.
Navonne, P. Perona, and D. J. Anderson, “Automated measurement of
mouse social behaviors using depth sensing, video tracking, and machine
learning,” Proceedings of the National Academy of Sciences, vol. 112,
no. 38, pp. E5351–E5360, 2015. 3, 1

[30] F. Romero-Ferrero, M. G. Bergomi, R. C. Hinz, F. J. Heras, and G. G.
de Polavieja, “idtracker. ai: tracking all individuals in small or large
collectives of unmarked animals,” Nature methods, vol. 16, no. 2, p.
179, 2019. 3, 1

[31] Y. Shemesh, Y. Sztainberg, O. Forkosh, T. Shlapobersky, A. Chen, and
E. Schneidman, “High-order social interactions in groups of mice,” Elife,
vol. 2, p. e00759, 2013. 3, 1

[32] A. Weissbrod, A. Shapiro, G. Vasserman, L. Edry, M. Dayan,
A. Yitzhaky, L. Hertzberg, O. Feinerman, and T. Kimchi, “Auto-
mated long-term tracking and social behavioural phenotyping of animal
colonies within a semi-natural environment,” Nature communications,
vol. 4, p. 2018, 2013. 3, 1

13

[33] A. L. Sheets, P.-L. Lai, L. C. Fisher, and D. M. Basso, “Quantitative
evaluation of 3d mouse behaviors and motor function in the open-ﬁeld
after spinal cord injury using markerless motion tracking,” PloS one,
vol. 8, no. 9, p. e74536, 2013. 3, 1

[34] T. Zhang, C. Xu, and M.-H. Yang, “Multi-task correlation particle ﬁlter
for robust object tracking,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, vol. 1, no. 2, 2017, p. 3. 3
[35] Y. Zheng, H. Song, K. Zhang, J. Fan, and X. Liu, “Dynamically
spatiotemporal regularized correlation tracking,” IEEE transactions on
neural networks and learning systems, 2019. 3

[36] B. Ma, H. Hu, J. Shen, Y. Zhang, L. Shao, and F. Porikli, “Robust object
tracking by nonlinear learning,” IEEE transactions on neural networks
and learning systems, vol. 29, no. 10, pp. 4769–4781, 2017. 3

[37] G. Shu, A. Dehghan, O. Oreifej, E. Hand, and M. Shah, “Part-based
multiple-person tracking with partial occlusion handling,” in Computer
Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on.
IEEE, 2012, pp. 1815–1821. 3

[38] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,
“Object detection with discriminatively trained part-based models,” IEEE
transactions on pattern analysis and machine intelligence, vol. 32, no. 9,
pp. 1627–1645, 2010. 3, 4

[39] Y. Xiang, A. Alahi, and S. Savarese, “Learning to track: Online multi-
object tracking by decision making,” in Proceedings of the IEEE inter-
national conference on computer vision, 2015, pp. 4705–4713. 3, 6, 8,
11, 12, 9

[40] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and
realtime tracking,” in Image Processing (ICIP), 2016 IEEE International
Conference on.
IEEE, 2016, pp. 3464–3468. 3, 11, 12

[41] A. Milan, S. Roth, and K. Schindler, “Continuous energy minimization
for multitarget tracking,” IEEE transactions on pattern analysis and
machine intelligence, vol. 36, no. 1, pp. 58–72, 2013. 3, 11, 12, 4
[42] L. Zhang, Y. Li, and R. Nevatia, “Global data association for multi-
object tracking using network ﬂows,” in Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on.
IEEE, 2008, pp.
1–8. 3

[43] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal greedy
algorithms for tracking a variable number of objects,” in Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Conference on.
IEEE,
2011, pp. 1201–1208. 3, 4

[44] H. B. Shitrit, J. Berclaz, F. Fleuret, and P. Fua, “Multi-commodity
network ﬂow for tracking multiple people,” IEEE transactions on pattern
analysis and machine intelligence, vol. 36, no. 8, pp. 1614–1627, 2014.
3, 4

[45] S. Hamid Rezatoﬁghi, A. Milan, Z. Zhang, Q. Shi, A. Dick, and I. Reid,
“Joint probabilistic data association revisited,” in Proceedings of the
IEEE international conference on computer vision, 2015, pp. 3047–3055.
3, 8, 11, 12, 6, 9

[46] A. Milan, S. H. Rezatoﬁghi, A. Dick, I. Reid, and K. Schindler, “Online
multi-target tracking using recurrent neural networks,” in Thirty-First
AAAI Conference on Artiﬁcial Intelligence, 2017. 3, 4, 11, 12

[47] L. Pishchulin, A. Jain, M. Andriluka, T. Thorm¨ahlen, and B. Schiele,
“Articulated people detection and pose estimation: Reshaping the fu-
ture,” in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on.

IEEE, 2012, pp. 3178–3185. 4
[48] G. Gkioxari, B. Hariharan, R. Girshick, and J. Malik, “Using k-poselets
for detecting people and localizing their keypoints,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp.
3582–3589. 4

[49] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in Com-
IEEE,

puter Vision (ICCV), 2017 IEEE International Conference on.
2017, pp. 2980–2988. 4

[50] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds,” arXiv preprint
arXiv:1611.08050, 2016. 4

[51] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional
pose machines,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016, pp. 4724–4732. 4

[52] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active
shape models-their training and application,” Computer vision and image
understanding, vol. 61, no. 1, pp. 38–59, 1995. 4, 7

[53] A. Mathis, P. Mamidanna, K. M. Cury, T. Abe, V. N. Murthy, M. W.
Mathis, and M. Bethge, “Deeplabcut: markerless pose estimation of user-
deﬁned body parts with deep learning,” Nature Publishing Group, Tech.
Rep., 2018. 4

[54] T. D. Pereira, D. E. Aldarondo, L. Willmore, M. Kislin, S. S.-H. Wang,
M. Murthy, and J. W. Shaevitz, “Fast animal pose estimation using deep
neural networks,” Nature methods, vol. 16, no. 1, p. 117, 2019. 4

[55] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in Advances in neural
information processing systems, 2015, pp. 91–99. 4, 5, 10

[56] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Se-
lective search for object recognition,” International journal of computer
vision, vol. 104, no. 2, pp. 154–171, 2013. 4

[57] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international

conference on computer vision, 2015, pp. 1440–1448. 4

[58] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014. 4,
5, 10, 2

[59] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large scale visual
recognition challenge,” International Journal of Computer Vision, vol.
115, no. 3, pp. 211–252, 2015. 4

[60] D. Mitzel and B. Leibe, “Real-time multi-person tracking with detector
assisted structure propagation,” in 2011 IEEE International Conference
on Computer Vision Workshops (ICCV Workshops).
IEEE, 2011, pp.
974–981. 6

[61] Z. Khan, T. Balch, and F. Dellaert, “An mcmc-based particle ﬁlter
for tracking multiple interacting targets,” in European Conference on
Computer Vision. Springer, 2004, pp. 279–290. 6

[62] M. D. Breitenstein, F. Reichlin, B. Leibe, E. Koller-Meier, and
L. Van Gool, “Robust tracking-by-detection using a detector conﬁdence
particle ﬁlter,” in Computer Vision, 2009 IEEE 12th International Con-
ference on.

IEEE, 2009, pp. 1515–1522. 6

[63] N. Wojke and D. Paulus, “Global data association for the probability
hypothesis density ﬁlter using network ﬂows,” in 2016 IEEE Interna-
tional Conference on Robotics and Automation (ICRA).
IEEE, 2016,
pp. 567–572. 6

[64] N. Chenouard, I. Bloch, and J.-C. Olivo-Marin, “Multiple hypothesis
tracking for cluttered biological image sequences,” IEEE transactions on
pattern analysis and machine intelligence, vol. 35, no. 11, pp. 2736–
3750, 2013. 6

[65] A. A. Perera, C. Srinivas, A. Hoogs, G. Brooksby, and W. Hu, “Multi-
object tracking through simultaneous long occlusions and split-merge
conditions,” in Computer Vision and Pattern Recognition, 2006 IEEE
Computer Society Conference on, vol. 1.
IEEE, 2006, pp. 666–673. 6
[66] J. Giebel, D. M. Gavrila, and C. Schn¨orr, “A bayesian framework for
multi-cue 3d object tracking,” in European Conference on Computer
Vision. Springer, 2004, pp. 241–252. 6

[67] Z. Qin and C. R. Shelton, “Improving multi-target tracking via social
grouping,” in Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on.

IEEE, 2012, pp. 1972–1978. 7

[68] K. Yamaguchi, A. C. Berg, L. E. Ortiz, and T. L. Berg, “Who are you with
and where are you going?” in Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on.

IEEE, 2011, pp. 1345–1352. 7

[69] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool, “You’ll never walk
alone: Modeling social behavior for multi-target tracking,” in Computer
Vision, 2009 IEEE 12th International Conference on.
IEEE, 2009, pp.
261–268. 7

[70] E. L. Lawler and D. E. Wood, “Branch-and-bound methods: A survey,”

Operations research, vol. 14, no. 4, pp. 699–719, 1966. 8

[71] A. Nemirovski, “Interior point polynomial

time methods in convex

programming,” Lecture notes, vol. 42, no. 16, pp. 3215–3224, 2004. 8

[72] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
“The pascal visual object classes (voc) challenge,” International journal
of computer vision, vol. 88, no. 2, pp. 303–338, 2010. 9

[73] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, “Performance
measures and a data set for multi-target, multi-camera tracking,” in
European Conference on Computer Vision. Springer, 2016, pp. 17–35.
9

[74] K. Bernardin and R. Stiefelhagen, “Evaluating multiple object tracking
the clear mot metrics,” Journal on Image and Video

performance:
Processing, vol. 2008, p. 1, 2008. 9

[75] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg, “Ssd: Single shot multibox detector,” in European conference on
computer vision. Springer, 2016, pp. 21–37. 10

[76] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 779–
788. 10

[77] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105. 10

14

[78] L. Chen, H. Ai, Z. Zhuang, and C. Shang, “Real-time multiple peo-
ple tracking with deeply learned candidate selection and person re-
identiﬁcation,” 07 2018. 11, 12

[79] N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime
tracking with a deep association metric,” in 2017 IEEE International
Conference on Image Processing (ICIP).
IEEE, 2017, pp. 3645–3649.
11

[80] J. S´anchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classiﬁca-
tion with the ﬁsher vector: Theory and practice,” International journal of
computer vision, vol. 105, no. 3, pp. 222–245, 2013. 10

[81] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778. 10

[82] J. M. Graving, D. Chae, H. Naik, L. Li, B. Koger, B. R. Costelloe, and
I. D. Couzin, “Deepposekit, a software toolkit for fast and robust animal
pose estimation using deep learning,” Elife, vol. 8, p. e47994, 2019. 12

Zheheng Jiang received the B.Sc. degree in
Electrical Engineering and Automation (Grid
Monitoring) from Nanjing Institute of Technology
and the M.Sc. degree in Software Development
from Queen’s University of Belfast, Belfast, U.K.
He has been awarded his Ph.D. degree in Com-
puter Science from University of Leicester, Le-
icester, U.K. He is currently the Senior Research
Associate at the Computing and Communica-
tions, Lancaster University, Lancaster, U.K.

His current research interests include machine
learning for vision, object detection and recognition, video analysis and
event recognition.

Zhihua Liu is currently pursuing the Ph.D. de-
gree with the School of Informatics, University of
Leicester, Leicester, U.K. His research interests
include machine learning, deep learning and
computer vision.

Long Chen is currently pursuing the PhD de-
gree with the School of Informatics, University of
Leicester, U.K. His research interests are in the
areas of Computer Vision and Machine Learn-
ing.

Lei Tong is currently pursuing the Ph.D. de-
gree with the School of Informatics, University of
Leicester, Leicester, U.K. His research interests
include computer vision, social network analysis
and data mining.

Xiangrong Zhang received the B.S. and M.S.
degrees from the School of Computer Science,
Xidian University, Xi’an, China,
in 1999 and
2003, respectively, and the Ph.D. degree from
the School of Electronic Engineering, Xidian Uni-
versity, in 2006. Currently, she is a professor in
the Key Laboratory of Intelligent Perception and
Image Understanding of the Ministry of Educa-
tion, Xidian University, China. She has been a
visiting scientist in Computer Science and Arti-
ﬁcial Intelligence Laboratory, MIT between Jan.
2015 and March 2016. Her research interests include pattern recogni-
tion, machine learning, and remote sensing image analysis and under-
standing.

Huiyu Zhou received a Bachelor of Engineer-
ing degree in Radio Technology from Huazhong
University of Science and Technology of China,
and a Master of Science degree in Biomedi-
cal Engineering from University of Dundee of
United Kingdom, respectively. He was awarded a
Doctor of Philosophy degree in Computer Vision
from Heriot-Watt University, Edinburgh, United
Kingdom. Dr. Zhou currently is a full Professor
at School of Computing and Mathematical Sci-
ences, University of Leicester, United Kingdom.
He has published over 400 peer reviewed papers in the ﬁeld. His
research work has been or is being supported by UK EPSRC, ESRC,
AHRC, MRC, EU, Royal Society, Leverhulme Trust, Pufﬁn Trust, Invest
NI and industry.

15

Xiangyuan Lan received the B.Eng. degree
in computer science and technology from the
South China University of Technology, China, in
2012, and the Ph.D. degree from the Depart-
ment of Computer Science, Hong Kong Bap-
tist University, Hong Kong, in 2016. He was a
Visiting Scholar with the Center for Automation
Research, UMIACS, University of Maryland at
College Park, from January 2015 to July 2015.
His current research interests include intelligent
video surveillance, biometric security, and health
informatics. He is currently an Associate Editor of Signal, Image and
Video Processing (Springer).

Danny Crookes (SM’12) received the B.Sc. and
Ph.D. degrees from Queen’s University Belfast,
Belfast, U.K., in 1997 and 1980, respectively.
He is currently involved in projects in medical
imaging for cancer diagnosis, Speech separa-
tion and enhancement, and Capital Markets soft-
ware using graphics processing units (GPUs).
He was an appointed Professor of Computer
Engineeringwith Queen’s University Belfast, in
1993, and was the Head of Computer Science
from 1993–2002. His research interests include
the use of acceleration technologies (including FPGAs and GPUs) for
high performance image, video, and speech processing. He has pub-
lished some 230 scientiﬁc papers in journals and international confer-
ences.

Ming-Hsuan Yang is a Professor of Electrical
Engineering and Computer Science with the Uni-
versity of California at Merced and an Adjunct
Professor at Yonsei University. He served as an
Associate Editor of the IEEE Transactions on
Pattern Analysis and Machine Intelligence from
2007 to 2011, and is an Associate Editor of the
International Journal of Computer Vision, Image
and Vision Computing, and Journal of Artiﬁcial
Intelligence Research. He received the NSF CA-
the Senate Award for
REER Award in 2012,
Distinguished Early Career Research at UC Merced in 2011, and the
Google Faculty Award in 2009. He is a Fellow of the IEEE.

SUPPLEMENTARY A
Table S1 summarizes the mouse based tracking methods described in Section 2.1.

TABLE S1
Summary of mouse based tracking methods.

1

Authors
Twining et al. [27]

Requirements
Standard camera (top
view)

Brief Introduction
Use active shape models to detect tar-
gets

de Chaumont et al. [7]

Standard camera (top
view)

Fit the deﬁned geometrical models to
images

P´erez-Escudero et al.
[20]

Standard
view)

camera(top

Pistori et al. [28]

Standard camera (top
view)

Use ﬁngerprints to resolve occlusions
and identity combined with motion
models
Extension of the standard particle ﬁlter-
ing approach

Branson et al. [19]

Standard camera (side
view)

A particle ﬁltering algorithm for track-
ing the contours of multiple mice

Weissbrod et al. [32]

RFID and
camera (top view)

standard

Use RFID to identify individuals in
combination with video data

Giancardo et al. [14]

Thermal camera (top
view)

Use thermal camera to detect minor
changes in body temperature

Lighting change

Shemesh et al. [31]

Ohayon et al. [16]

Hong et al. [29]

Sheets et al. [33]

and

above

Fluorescent
colors,
UVA light and sensitive
color camera (top view)
camera
Dye
mounted
the
enclosure
Standard camera (top
view) and depth camera
(top view)
Ten synchronised video
cameras

Background subtraction and image seg-
mentation using all cameras

Shape-from-silhouette
3D shape of the mouse

to reconstruct

Romero-Ferrero et al.
[30]

Standard camera (top
view)

Use deep-learning-based image classi-
ﬁers to identify animals that cross paths

Ours

Standard
view)

camera(top

Novel BILP Model
candidates with individual targets

to associate part

Advantage: our approach is pow-
erful to track multi-mice parts us-
ing a standard camera without color
marking or additional equipment.

Limitation
Its ﬂexibility is limited be-
cause of the required sophis-
ticated skeleton models
Its ﬂexibility is limited as it
requires sophisticated skele-
ton models
Easily inﬂuenced by illumi-
nation variation

Difﬁcult to explicitly model
the entire mouse due to
highly deformable shapes
Difﬁcult to explicitly model
the entire mouse due to
highly deformable shapes of
mice
RFID does not provide suf-
ﬁcient spatial accuracy and
temporal resolution
Thermal images do not pro-
vide appearance informa-
tion such as illumination,
contrast and texture
Marking is invasive and can
change mice behaviours

Part Localization
Yes (inﬂuenced by active
shape models)

Yes (inﬂuenced by geo-
metrical models)

No

No

No

No

Yes (inﬂuenced by frame
difference and the esti-
mated mouse shape)

No

It requires additional equip-
ment
and calibration of
cameras
It requires additional equip-
and calibration of
ment
cameras
This method is to handle oc-
clusions of nearly rigid ani-
mals
Computing time is increas-
ing as the number of mice
arises

Yes (inﬂuenced by ﬁtted
ellipses)

(inﬂuenced by 3D

Yes
shape)

No

Yes
(by the proposed
Inte-
Bayesian-inference
ger Linear Programming
Model)

Dye the fur with different patterns of
strokes and dots

Marking is invasive and can
modify mice behaviour

Yes (inﬂuenced by dye
patterns)

SUPPLEMENTARY B

and
(cid:89)

(cid:88)

pt (Xk|am

n ) =

(cid:89)

(cid:88)

(cid:16)

N

xk
n,m, σ

2

(cid:17)

m∈M ∗,n∈N ∗

k∈O

m∈M ∗,n∈N ∗

(cid:89)

=

k∈O
(cid:32)

N

(cid:88)

xk
n,m,

(cid:33)

(cid:88)

σ

k∈O

(cid:35) (cid:0)(cid:80)

k∈O σ(cid:1)2
M ∗ N

,

Algorithm S1 Algorithm for training the proposed PPN.
Input: a video sequence F with ground-truth boxes and labels.
Output: {αz}Z
1: Initialize the shared convolutional layers by the ﬁrst 15 layers
of VGG-19 net [58] and all new layers by estimating weights
from a zero-mean Gaussian;

z=1, trained PPN.

= N

m∈M ∗,n∈N ∗
(cid:80)

(cid:32)(cid:34)

(cid:88)

n,m

n,m

k∈O
k∈O xk
k∈O σ(cid:1)2


(cid:0)(cid:80)

(cid:115)

(cid:0)(cid:80)

k∈O σ(cid:1)2
M ∗ N



2: Create positive and negative root boxes associated with the

object pj ∈ {p}J

j=0 in the ﬁrst stage;

3: Initially ﬁll the mini-batch of each image with all positive root

boxes and randomly sampled negative root boxes;

4: for z = 1 to Z do
5:
6:
7: Mine hard negative examples by sorting the positive proba-

Train our PPN Rz using Eq. (1) for all objects;
Calculate the pseudo-loss of Rz using Eq. (2);

8:

bility of root boxes in St
Replace negative examples in previous stage with new hard
negative examples;

j of object pj;

9: end for
10: Compute {αz}Z
11: return {αz}Z

z=1 using Eq. (3);

z=1, trained PPN.

SUPPLEMENTARY C

The proof of Eq. (16) is shown below: As the term pt (Xk|am
of a 2D Gaussian distribution, then we have

n ) is

(cid:88)

(cid:89)

k∈O

m∈M ∗,n∈N ∗

pt (Xk|am

n ) =

=

(cid:88)

k∈O

(cid:88)

k∈O

(cid:89)

(cid:16)

N

xk
n,m, σ

(cid:17)

(cid:32)(cid:34)

m∈M ∗,n∈N ∗
xk
n,m
σ2

(cid:88)

N

n,m

(cid:35)

σ2
M ∗ N

,

(cid:33)

(cid:32)

(cid:88)

n,m

(cid:114) σ2

M ∗ N

(cid:88)

N

=

k∈O
(cid:32)

= N

(cid:88)

(cid:88)

k∈O

n,m

(cid:114) σ2

M ∗ N

(cid:88)

k∈O

(cid:114) σ2

M ∗ N

xk
n,m
M ∗ N

xk
n,m
M ∗ N

,

,

(cid:33)

(1)

(cid:32)

(cid:88)

(cid:88)

= N

k∈O

n,m

(cid:114) σ2

M ∗ N

(cid:88)

k∈O

xk
n,m
M ∗ N

,

(cid:33)

(2)

hence:
(cid:88)

(cid:89)

pt (Xk|am

n ) =

(cid:89)

(cid:88)

pt (Xk|am
n )

k∈O

m∈M ∗,n∈N ∗

m∈M ∗,n∈N ∗

k∈O

(3)

SUPPLEMENTARY D

Algorithm S2 Algorithm for optimization of cost function Eq.
(17).
Input: the cost vector Φ, the constraint matrix A and Linear

equality constraints h.
Output: the optimal solution

1: Find the optimal solution to Eq. (17) with the 0-1 restrictions

relaxed.

5:

2: At the root node, let the relaxed solution be the lower bound
U0 and randomly select a 0-1 solution with the upper bound
L0, and set w = 0.
3: while Uw (cid:54)= Lw do
4:

Create two new constraints of ‘= 0’ constraint and ‘= 1’
constraint for the minimum fractional variable of the opti-
mal solution.
Create two new nodes, one for the ‘= 0’ constraint and one
for the ‘= 1’ constraint.
Solve the relaxed linear programming model with the new
constraint at each of these nodes.
Let the relaxed solution be the lower bound Uw+1 and the
existing maximum 0-1 solution be the upper bound Lw+1,
and set w = w + 1.
Select the node with the minimum lower bound for branch-
ing.
9: end while
10: return the optimal solution.

8:

6:

7:

(cid:33)

SUPPLEMENTARY E
Alg. S2 is used to seek the global minimum of
the cost
function Eq. (18) over a N ∗ (Mt + 1) dimensional solution
space RN ∗(Mt+1). For a subspace R ⊆ RN ∗(Mt+1), we deﬁne
Γmin (R) = minΛ∈R ΦT Λ. Thus, Γlb (R) ≤ Γmin (R) ≤
Γub (R), Γlb and Γub are functions to compute the lower and

TABLE S2
Comparison with conventional methods on the 2-Mice dataset.

Method
Motr [16]
MiceProﬁler [7]
our

MOTA% ↑ MOTP% ↑
62
49.6
81.1

53.9
56.0
65.5

IDF1% ↑
69
28.6
91.2

FPS↑
30
26
30.8

upper bounds respectively. We ﬁrst show that after a large number
of iterations k, the list of partition Pk must contain a subspace
of the original volume. The volume of the subspace is deﬁned as
vol (R) = (cid:81)
i (ri − li), [li, ri] is the interval of R along the ith
dimension and therefore

min
R∈Pk

vol (R) ≤

(cid:16)

RN ∗(Mt+1)(cid:17)

vol

k

We also have

vol (R) ≥ max

i

(ri − li)

(cid:18)

min
i

(cid:19)N ∗(Mt+1)−1

(ri − li)

We deﬁne the condition number of subspace R as follows:

cond (R) =

maxi (ri − li)
mini (ri − li)

(4)

(5)

(6)

Since we deﬁne R as the minimum lower bound as described in
Alg.S2, we have

cond (R) ≤ max

(cid:110)

cond

(cid:16)
RN ∗(Mt+1), 2

(cid:17)(cid:111)

(7)

Combing Eqs. (4), (5), (6) and (7), we have:

length (R) ≤ max

(cid:110)

cond

(cid:16)

RN ∗(Mt+1), 2

(cid:17)(cid:111)

min
R∈Pk

vol





RN ∗(Mt+1)(cid:17)
(cid:16)



1
N ∗(Mt+1)

k



(8)

where length(R) = maxi (ri − li). Thus, with the increasing k,
the maximum dimension of Rmin, which is the smallest subspace
in Pk, is decreasing. As length(R) goes to zero, the difference
between the upper and lower bounds (i.e., Γub (R) − Γlb (R))
uniformly converges to zero.

SUPPLEMENTARY F
Fig. S1 shows the part tracking of multi-mice described in Section
4.3.

Fig. S2 shows some exemplar frames and annotations from the
proposed Multi-Mice PartsTrack dataset described in Section 5.1.
In Fig. S3, we conduct ablation study on the multi-stage
training design and illustrate the Precision/Recall curves of the
proposed Part and Body Proposal Network across various stages
on the 2-mice dataset described in Section 6.1.2.

Tab. S3 shows quantitative evaluation of multi-mice part track-
ing on the 2-Mice dataset described in Section 6.2.2. Fig. S4 shows
exemplar tracking results of the test sequences in the proposed
Multi-Mice PartsTrack dataset.

Fig. S5 show tracking results of conventional methods (Motr
and MiceProﬁler). Figs. S6, S8 and S9 provide the qualitative
comparison of the proposed tracking method against MDP [49]
and JPDAm [63] described in Section 6.2.2.

3

Fig. S1. Top: Part (mouse head and tail) detection candidates shown
over four frames. Middle: Estimated locations of the parts for all the
mice. Each colourful line corresponds to a unique mouse identity and
each colourful bounding box corresponds to a unique part identity.
Bottom: Estimated trajectories of all the parts.

Fig. S2. Exemplar frames and annotations from the proposed Multi-Mice
PartsTrack dataset.

4

(a) head

(b) tail

(c) body

Fig. S3. Precision/Recall curves of the proposed Part and Body Proposal Network across various stages on the 2-mice dataset.

TABLE S3
Quantitative evaluation of multi-mice parts tracking on the 2-Mice dataset.

Method

All
All(b,c,d)
All(a,c,d)
All(a,b,d)
All(a,b,c)

All without geometric model

All without motion model

All without parts association model

74.4

MOTDT [77]
MDP [39]
SORT [40]
JPDAm [45]
RNN-LSTM [46]
CEM [41]

22.2
67.9
56.7
55.0
41.2
37.4

MOTA
% ↑

85.0
84.6
81.0
82.7
80.1

71.4

70.1

ML
↓

MOTP
% ↑

MT
↑
Impact of the constraints
0
19
65.5
0
19
65.4
2
17
66.1
0
18
66.0
0
17
69.6
Impact of the geometric model
66.1
0
15
Impact of the motion model
17
65.4
Impact of the parts association model
16
Comparison with the other state-of-the-arts
7
14
7
11
9
9

30.4
69.9
69.5
67.6
62.6
68.5

0
0
0
0
0
0

66.3

0

2

FP
↓

272
280
374
342
399

557

576

487

658
630
571
858
983
1138

FN
↓

288
288
331
299
356

514

533

462

504
583
509
815
916
904

IDs
↓

8
18
18
14
18

15

27

41

830
10
10
34
22
47

IDF1
% ↑

86.6
83.0
80.9
82.4
79.8

65.5

67.4

71.7

40.4
/
32.2
60.0
63.8
/

5

Motr [16]

MiceProﬁler [7]

(a)

(b)

Fig. S4. Tracking ground-truth (top) and results (bottom) of the test se-
quences in the proposed Multi-Mice PartsTrack dataset. The trajectory
and rectangle of each mouse part are shown in different colours: (a) The
leftmost mouse: head (labelled as 4) is green and tail (labelled as 3) is
pink, the rightmost mouse: head (labelled as 1) is grey and tail (labelled
as 2) is cyan; (b) the leftmost mouse: head (labelled as 3) is pink and tail
(labelled as 4) is green, the upper right mouse: head (labelled as 5) is
GreenYellow and tail(labelled as 6) is blue, the lower right mouse: head
(labelled as 1) is grey and tail (labelled as 2) is cyan.

Ours

ground truth

Fig. S5. Comparison with conventional methods i.e., Motr and MicePro-
ﬁler. Motr and MiceProﬁler track mice by ﬁtting ellipses and hard-coded
geometric models. When the mice are very close, the models used in
these methods cannot be properly ﬁtted, leading to tracking targets’
swaps or incorrect parts localization.

6

Fig. S6. Qualitative comparison of the proposed tracking method (row 4) against MDP [39] (row 2) and JPDAm [45] (row 3) using the same detection
results (row 1). MDP swap identities between targets 2 and 5 after the occlusion occurs due to ‘Pinning’ at the frame 106, i.e., the target identity is
swapped between the tail of the upper left mouse and the head of the lower left mouse. JPDAm assigns a new identity to target 6 after it is occluded
by target 3, i.e., the occluded tail of the upper left mouse is assigned to a new identity number.

7

Fig. S7. Conﬁdence maps of the mouse head and tail bases in Fig. S6.

8

Fig. S8. Qualitative comparison of the proposed tracking method (row 4) against MDP [39] (row 2) and JPDAm [45] (row 3) using the same detection
results (row 1). As shown at image frame 876 (for the MDP algorithm), target 3 (the tail of the leftmost mouse) sees drifts and switches to target 6
(the tail of the middle mouse). After the drifting of target 3, the original object is assigned to a new identity number as shown at image frame 911. In
the JPDAm algorithm, target 3 (the tail of the leftmost mouse) drifts towards target 5 (the head of the leftmost mouse) and ﬁnally switches to target
5 at image frame 911.

9

Fig. S9. Qualitative comparison of the proposed tracking method (row 4) against MDP [39](row 2) and JPDAm [45] (row 3) using the same detection
results (row 1). The problems of identity swap and target drift are more serious in this situation. In the MDP algorithm, target 3 (the tail of the
leftmost mouse at image frame 1819) has drifts and switches to target 1 (the tail of the upper right mouse in the ﬁrst column) at image frame 1861.
Moreover, in the same frame, target 7 (the head of the leftmost mouse at image frame 1819) is replaced by target 1, and its tail is assigned to a
new identity number 9. Similar problems occur for JPDAm, where target 6 (the tail of the upper right mouse at image frame 1819) replaces target 1
(the head of the upper right mouse at image frame 1819) and target 6 is replaced by a new identity number 7 at image frame 1861. Although target
6 (the tail of the upper right mouse at image frame 1819) in our algorithm also causes a drift due to occlusion at image frame 1836, target 6 ﬁnally
ﬁnds the correct object when the occluded part appear again at image frame 1861.

