2
2
0
2

y
a
M
5
2

]
L
P
.
s
c
[

2
v
8
7
1
4
0
.
2
0
2
2
:
v
i
X
r
a

VAEL: Bridging Variational Autoencoders and Probabilistic
Logic Programming.

Eleonora Misino∗1, Giuseppe Marra†2, and Emanuele Sansone‡2

1DISI, University of Bologna
2Department of Computer Science, KU Leuven

Abstract

We present VAEL, a neuro-symbolic generative model integrating variational autoencoders
(VAE) with the reasoning capabilities of probabilistic logic (L) programming. Besides standard
latent subsymbolic variables, our model exploits a probabilistic logic program to deﬁne a further
structured representation, which is used for logical reasoning. The entire process is end-to-end
diﬀerentiable. Once trained, VAEL can solve new unseen generation tasks by (i) leveraging the
previously acquired knowledge encoded in the neural component and (ii) exploiting new logical
programs on the structured latent space. Our experiments provide support on the beneﬁts of
this neuro-symbolic integration both in terms of task generalization and data eﬃciency. To the
best of our knowledge, this work is the ﬁrst to propose a general-purpose end-to-end framework
integrating probabilistic logic programming into a deep generative model.

1

Introduction

Neuro-symbolic learning has gained tremendous attention in the last few years [4, 10, 34, 3] as such
integration has the potential of leading to a new era of intelligent solutions, enabling the integration of
deep learning and reasoning strategies (e.g. logic-based or expert systems). Indeed, these two worlds
have diﬀerent strengths that complement each other [32]. For example, deep learning systems, i.e.
System 1, excel at dealing with noisy and ambiguous high dimensional raw data, whereas reasoning
systems, i.e. System 2, leverage relations between symbols to reason and to generalize from a small
amount of training data. While a lot of eﬀort has been devoted to devising neuro-symbolic methods
in the discriminative setting [49, 69, 51], less attention has been paid to the generative counterpart.
A good neuro-symbolic framework should be able to leverage a small amount of training data, acquire
the knowledge by learning a symbolic representation and generate data based on new forms of
high-level reasoning. For example, let us consider a task where a single image of multiple handwritten
numbers is labeled with their sum. Common generative approaches, like VAE-based models, have
a strong connection between the latent representation and the label of the training task [37, 31].
Consequently, when considering new generation tasks that go beyond the simple addition, they have
to be retrained on new data.
∗eleonora.misino2@unibo.it
†giuseppe.marra@kuleuven.be
‡emanuele.sansone@kuleuven.be

1

 
 
 
 
 
 
In this paper, we tackle the problem by providing a true neuro-symbolic solution, named VAEL. In
VAEL the latent representation is not directly linked to the label of the task, but to a set of newly
introduced symbols, i.e. logical expressions. Starting from these expressions, we use a probabilistic
logic program to deduce the label. Importantly, the neural component only needs to learn a mapping
from the raw data to this new symbolic representation. In this way, the model only weakly depends
on the training data and can generalize to new generation tasks involving the same set of symbols.
Moreover, the reasoning component oﬀers a strong inductive bias, which enables a more data eﬃcient
learning.

The paper is structured as follows. In Section 2, we provide a brief introduction to probabilistic logic
programming and to generative models conditioned on labels. In Section 3, we present the VAEL
model together with its inference and learning strategies. Section 4 shows our experiments, while
Section 5 places our model in the wider scenario of multiple related works. Finally, in Section 6, we
draw some conclusions and discuss future directions.

2 Preliminaries

2.1 Probabilistic Logic Programming

A logic program is a set of deﬁnite clauses, i.e. expressions of the form h ← b1 ∧ ... ∧ bn, where h is
the head literal or conclusion, while the bi are body literals or conditions. Deﬁnite clauses can be
seen as computational rules: IF all the body literals are true THEN the head literal is true. Deﬁnite
clauses with no conditions (n = 0) are facts. In ﬁrst-order logic programs, literals take the form
a(t1, ..., tm), with a a predicate of arity m and ti are the terms, that is constants, variables or functors
(i.e. functions of other terms). Grounding is the process of substituting all the variable in an atom or
a clause with constants.

ProbLog [9] lifts logic programs to probabilistic logic programs through the introduction of probabilistic
facts. Whereas a fact in a logic program is deterministically true, a probabilistic fact is of the form
pi :: fi where fi is a logical fact and pi is a probability.
In ProbLog, each ground instance of
a probabilistic fact fi corresponds to an independent Boolean random variable that is true with
probability pi and false with probability 1 − pi. Mutually exclusive facts can be deﬁned through
annotated disjunctions p0 :: f0; ... ; pn :: fn. with (cid:80)
i pi = 1. Let us denote with F the set of all
ground instances of probabilistic facts and with p their corresponding probabilities. Every subset
F ⊆ F deﬁnes a possible world wF obtained by adding to F all the atoms that can be derived from
F using the logic program. The probability P (wF ; p) of such a possible world wF is given by the
product of the probabilities of the truth values of the probabilistic facts; i.e:

P (wF ; p) =

(cid:89)

pi

(cid:89)

(1 − pi)

fi∈F

fi∈F \F

(1)

Two inference tasks on these probabilities are of interest for this paper.

Success: The probability of a query atom y, or formula, also called success probability of y, is the
sum of the probabilities of all worlds where y is True, i.e.,

P (y; p) =

(cid:88)

P (wF ; p)

F ⊆F :wF |=y

(2)

2

Sample with evidence: Given a set of atoms or formulas E, the evidence, the probability of a
world given evidence is:

(cid:40)

P (wF |E; p) =

1
Z

P (wF ; p)
0

if wF |= E
otherwise

(3)

where Z is a normalization constant. Sampling from this distribution provides only worlds that are
coherent with the given evidence.

Example 1 (Addition of two digits). Let us consider a setting where images contains two digits that
can only be 0 or 1. Consider the following two logical predicates: digit(img, I, Y) states that a given
image img has a certain digit Y in position I, while add(img, z) states that the digits in img sum to a
certain value z.

We can encode the digit addition task in the following program T :

p1 :: digit ( img ,1 ,0); p2 :: digit ( img ,1 ,1).
p3 :: digit ( img ,2 ,0); p4 :: digit ( img ,2 ,1).

add ( img , Z ) : - digit ( img ,1 , Y1 ) ,
digit ( img ,2 , Y2 ) ,
Z is Y1 + Y2 .

In this program T , the set of ground facts F is

{digit(img, 1, 0), digit(img, 1, 1), digit(img, 2, 0), digit(img, 2, 1)}.

The set of probabilities p is p = [p1, p2, p3, p4]. The ProbLog program T deﬁnes a probability dis-
tribution over the possible worlds and it is parameterized by p, i.e. P (ωF ; p). Then, we can ask
ProbLog to compute the success probability of a query using Equation 2, e.g. P (add(img, 1)); or
sample a possible world coherent with some evidence add(img, 2) using Equation 3, e.g. wF =
{digit(img, 1, 1), digit(img, 2, 1)}.

2.2 Generation Conditioned on Labels

(a) M1+M2

(b) CCVAE

(c) VAEL (ours)

Figure 1: Visual comparison for the probabilistic graphical models of [37] (M1+M2), of [31] (CCVAE)
and ours (VAEL). Black arrows refer to the generative model, whereas blue dashed arrows correspond
to the inference counterpart.

In this paper, we are interested in generative tasks where we consider both an image x and a label y.
The integration of supervision into a generative latent variable model has been largely investigated

3

in the past. For example, the work of [37] proposes an integrated framework between two generative
models, called M1 and M2 (cf. Figure 1). Model M1 learns a latent representation for input x, i.e.
zALL, which is further decomposed by model M2 into a symbolic and a subsymbolic vector y and z,
respectively. In this formulation, the generative process of the image is tightly dependent on the
label, and therefore on the training task. More recently, another approach, called

CCVAE [31], proposes to learn a representation consisting of two independent latent vectors, i.e. z
and zsym, and forces the elements of zsym to have a one-to-one correspondence with the L elements
of y, thus capturing the rich information of the label vector y (cf. Figure 1).

However, both the approaches are limited in terms of generation ability as their latent representation
encodes information about the training task. This could be problematic when the label y is only
weakly linked to the true symbolic structure of the image. For example, let us consider the addition
task in Example 1, where a single image of multiple handwritten numbers is labeled with their sum,
and y = 1. In a generative task where we are interested in creating new images,
e.g. x =
using only the information of the label y is not as expressive as directly using the values of the
single digits. Moreover, suppose that we want to generate images where the two digits are related by
other operations (e.g. subtraction, multiplication, etc). While we still want to generate an image
representing a pair of digits, none of the models mentioned before would be able to do it without
being retrained on a relabelled dataset. How can we overcome such limitations?

3 The VAEL Model

Figure 2: The VAEL model is composed of three components. First, the encoder (left) computes
an approximated posterior of the latent variables z from the image x. The latent variables are split
into two components: a subsymbolic z and a symbolic zsym. Second, zsym is used to parameterize a
ProbLog program (center). A MLP is used to map the real variables zsym into the probabilities of
the facts in the program. Then, the program is used to compute the label y and a possible world.
Finally, a decoder (right) takes both the latent vector z and the possible world from ProbLog to
reconstruct the image ˜x.

Here, we propose a probabilistic graphical model which enables to unify VAEs with Probabilistic
Logic Programming. The graphical model of VAEL (Figure 1) consists of four core variables.
x ∈ RH×W ×C represents the image we want to generate, while y ∈ {0, 1}K represents a label, i.e. a
symbolic information characterizing the image. The latent variable is split into a symbolic component
zsym ∈ RN and a subsymbolic component z ∈ RM . Conversely to other VAE frameworks, VAEL
does not rely on a one-to-one mapping between y and zsym, rather it exploits a probabilistic logic

4

program to link them. Indeed, the probabilistic facts F are used by the ProbLog program T to
compute the actual labels y and they can encode a more meaningful symbolic representation of the
image than y.

Generative model.

The generative distribution of VAEL (Figure 1) is factorized in the following way:

pθ(x, y, z) = p(x|z)p(y|zsym)p(z)

(4)

where z = [zsym, z] and θ are the parameters of the generative model. p(z) is a standard Gaussian
distribution, while p(y|zsym) is the success distribution of the label of the ProbLog program T (Eq. 2).
p(x|z) is a Laplace distribution with mean value µ and identity covariance, i.e. Laplace(x; µ, I). Here,
µ is a neural network decoder whose inputs are z and ωF . ωF is sampled from P (ωF ; M LP (zsym))
(Eq. 1).

Inference model. We amortise inference by using an approximate posterior distribution qφ(z|x, y)
with parameters φ. Furthermore, we assume that z and y are conditionally independent given x,
thus obtaining qφ(z|x, y) = qφ(z|x)1. This allows us to decouple the latent representation from the
training task. Conversely, the other VAE frameworks do not exploit this assumption and have a
latent representation that is dependent on the training task.

The overall VAEL model (including the inference and the generative components) is shown in Figure 2.

Objective Function. The objective function of VAEL computes an evidence lower bound (ELBO)
on the log likelihood of pair (x, y), namely:

L(θ, φ) = LREC(θ, φ) + LQ(θ, φ) − DKL[qφ(z|x)||p(z)]]

(5)

where

LREC(θ, φ) = Ez∼qφ(z|x)[log(p(x|z)], LQ(θ, φ) = Ezsym∼qφ(zsym|x))[log(p(y|zsym))]].

Note that we omit the dependence on ωF in the objective, thanks to an equivalence described in the
extended derivation (see Appendix A).

The objective is used to train VAEL in an end-to-end diﬀerentiable manner, thanks to the Reparametriza-
tion Trick [36] at the level of the encoder qφ(z|x) and the diﬀerentiability of the ProbLog inference,
which is used to compute the success probability of a query and sample a world.

In Appendix B we report VAEL training algorithm (Algorithm 1) along with further details on the
training procedure.

3.1 Downstream Applications

Label Classiﬁcation. Given x we use the encoder to compute zsym and by using the MLP we
compute the probabilities p = M LP (zsym). Then, we can predict labels by computing the probability
distribution over the labels P (y; p), as deﬁned in Eq. 2, and sampling y ∼ P (y; p). This process
subsumes the DeepProbLog framework [49].

1We use a Gaussian distribution with a mean parameterized by the encoder network and identity covariance

5

Image Generation. We generate images by sampling z = [zsym, z] from the prior distribution
N (0, 1) and a possible world ωF from P (ωF ; p). The distribution over the possible worlds P (ωF ; p)
is computed by relying on ProbLog inference starting from the facts probabilities p = M LP (zsym).

Conditional Image Generation. As described in Section 2.1, ProbLog inference allows us also to
sample with evidence. Thus, once sampled z from the prior, we can (i) compute p = M LP (ˆzsym),
then (ii) compute the conditional probability P (ωF | E; p), (iii) sampling ωF ∼ P (ωF | E; p) and (iv)
generate an image consistent with the evidence E.

Task Generalization As we have seen, VAEL factorizes the generation task into two steps: (i)
generation of the world ωF (e.g. the digits labels); (ii) generation of the image given the world.
Whereas the second step requires to be parameterized by a black-box model (e.g. a convolutional
neural network), the generation of a possible world ωF is handled by a symbolic generative process
encoded in the ProbLog program T . Thus, once trained VAEL on a speciﬁc symbolic task (e.g. the
addition of two digits), we can generalize to any novel task that involves reasoning with the same set
of probabilistic facts by simply changing the ProbLog program accordingly (e.g. we can generalize to
the multiplication of two integers). To the best of our knowledge, such a level of task generalization
cannot be achieved by any other VAE frameworks.

4 Experiments

In this Section, we validate our approach on the four downstream applications by creating two
diﬀerent datasets.

2digit MNIST dataset. We create a dataset of 64, 400 images of two digits taken from the MNIST
dataset [40]. We use 65%, 20%, 15% splits for the train, validation and test sets, respectively. Each
image in the dataset has dimension 28 × 56 and is labelled with the sum of the two digits. The dataset
contains a number of images similar to the standard MNIST dataset. However, it is combinatorial in
nature, making any task deﬁned on it harder than its single-digit counterpart.

Mario dataset. We create a dataset containing 6, 720 images of
two consequent states of a 3 × 3 grid world where an agent can move
by one single step (diagonals excluded). Each image has dimension
100 × 200 and is labelled with the move performed by the agent. For
example, the image in Figure 3 has label down. We use 70%, 20%,
10% splits for the train, validation and test sets, respectively.

In order to evaluate our approach, we rely on a reconstruction loss
(mREC) in terms of data log-likelihood and two accuracies, predic-
tive (mCLASS) and generative (mGEN ). Regarding the predictive
accuracy, we measure the predictive ability of the model as the clas-
siﬁcation accuracy on the true labels (the addition of the two digits
for 2digit MNIST dataset, and the move for Mario dataset). It is
worth mentioning that, for 2digit MNIST dataset, such accuracy
cannot be directly compared with standard values for the single-digit
MNIST, as the input space is diﬀerent: the correct classiﬁcation of an image requires both the digits
to be correctly classiﬁed. The generative accuracy is assessed by using an independent classiﬁer for
each dataset. For 2digit MNIST dataset, the classiﬁer is trained to classify single digit value; while

Figure 3: Example of Mario
dataset image. The 3 × 3
grid world (green area) is sur-
rounded by a frame (bricks).

6

(a)

(b)

Figure 4: Examples of generation (a) and conditional generation (b) for VAEL and CCVAE on 2digit
MNIST dataset. In (b) in each column the generation is conditioned on a diﬀerent label y.

for the Mario dataset, the classiﬁer learns to identify the agent’s position in a single state. The
evaluation process for the generative ability can be summarized as: (i) jointly generate the image
and the label ˜y; (ii) split the image into two sub-images and (iii) classify them independently; (iv)
ﬁnally, for 2digit MNIST dataset, we sum together the outputs of the classiﬁer and we compare
the resulting addition with the generated label ˜y; while for Mario Dataset, we verify whether the
classiﬁed agent’s positions are consistent with the generated label ˜y.

In the following tasks, we compare VAEL against CCVAE [31] when possible. The source code
and the datasets are available at https://github.com/EleMisi/VAEL under MIT license. Further
implementation details can be found in Appendix D.

Label Classiﬁcation. In this task, we want to predict the correct label given the input image, as
measured by the predictive accuracy mCLASS. Both VAEL and CCVAE use an encoder to map
the input image to a latent vector zsym. VAEL uses ProbLog inference to predict the label y. In
contrast, CCVAE relies on the distribution p(y|zsym), which is parameterized by a neural network.
As shown in Table 1, CCVAE and VAEL achieve comparable predictive accuracy in Mario dataset.
However, VAEL generalizes better than CCVAE in 2digit MNIST dataset. The reason behind this
performance gap is due to the fact that the addition task is combinatorial in nature and CCVAE
would require a larger number of training samples in order to solve it. We further investigate this
aspect in the Data eﬃciency experiment.

Image Generation. We want to test the performance when generating both the image and the
label. VAEL generates both the image and the label ˜y starting from the sampled latent vector
z ∼ N (0, 1). Conversely, CCVAE starts by sampling the label ˜y from its prior, then proceeds by
sampling the latent vector from p(z|y = ˜y), and ﬁnally generates the new image. Figure 4a shows
some random samples for both models for 2digit MNIST dataset. The pairs drawn by VAEL are
well deﬁned, while CCVAE generates more ambiguous digits (e.g., the 1 resembles a 0, the 4 may
be interpreted as a 9, and so on). This ambiguity makes it harder for the classiﬁer network to
distinguish among the digits during the evaluation process, as conﬁrmed by the quantitative results
in Table 1, where VAEL outperforms CCVAE in terms of generative ability. Regarding Mario dataset
(Figure 5a), VAEL is able to generate data-like images, where the background is preserved from one
state to the subsequent one (additional results can be found in Appendix E). Conversely, CCVAE
fails the generation task: although it correctly generates the background, it is not able to draw the

7

(a)

(b)

Figure 5: Examples of generation (a) and conditional generation (b) for VAEL and CCVAE on Mario
dataset. In (b) in each column the generation is conditioned on a diﬀerent label y.

agent. This is also supported by the disparity in the reconstructive ability, as reported in Table 1. In
Mario dataset, this is due to a systematic error in which CCVAE focuses only on reconstructing the
background, thus discarding the small portion of the image containing the agent, as shown in Figures
5a, 5b and in Appendix E. The diﬀerence in performance between CCVAE and VAEL lies in the fact
that for each label there are many possible correct images. For example, in the Mario dataset, there
are 6 possible pairs of agent’s positions that correspond to the label left. Our probabilistic logic
program explicitly encodes the digits value or the single agent’s positions in its probabilistic facts,
and uses the variable zsym to compute their probabilities. On the contrary, CCVAE is not able to
learn the proper mapping from the digits value or the agent’s positions to the label, but it can learn
to encode only the label in the latent space zsym.

Conditional Image Generation. In this task, we want to evaluate also the conditional generation
ability of our approach. In Figures 4b and 5b we report some qualitative results for both VAEL
and CCVAE (additional results can be found in Appendix E). As it can be seen in 4b, VAEL
always generates pairs of digits coherent with the evidence, showing also a variety of combinations.
Conversely, some of the pairs generated by CCVAE do not sum to the desired value. Regarding
Mario dataset (Figure 5b), VAEL generates pairs of states coherent with the evidence, and with
diﬀerent backgrounds that are preserved from one state to the subsequent one. On the contrary,
CCVAE is not able to draw the agent in the generated images, thus failing the task. The reason lies,
again, in the task complexity, that VAEL reduces by relying on its probabilistic logic program.

Task Generalization. We deﬁne several novel tasks to evaluate the task generative ability of
VAEL. For 2digit MNIST dataset, we introduce the multiplication, subtraction and power between
two digits, while for Mario dataset we deﬁne two shortest paths (up priority, i.e. up always ﬁrst,
and one with right priority, i.e. right always ﬁrst). To the best of our knowledge, such a level of
task generalization cannot be achieved by any existing VAE framework. On the contrary, in VAEL,
we can generalize by simply substituting the ProbLog program used for the training task with the

8

Table 1: Reconstructive, predictive and generative ability of VAEL and CCVAE. We use repeated
trials to evaluate both the models on a test set of 10K images for 2digit MNIST dataset and 1344
images for Mario dataset.

Dataset

2digit MNIST

Mario

Model mREC(↓)
CCVAE 1549 ± 2
VAEL
1542 ± 3
CCVAE 43461 ± 209
VAEL

42734 ± 246

mCLASS(↑)
0.5284 ± 0.0051
0.8477 ± 0.0178
1.0 ± 0.0
0.977 ± 0.0585

mGEN (↑)
0.5143 ± 0.0157
0.7922 ± 0.0350
0.0 ± 0.0
0.8135 ± 0.2979

(a)

(b)

Figure 6: Examples of the generation ability of VAEL in previously unseen tasks for 2digit MNIST
dataset (a) and Mario dataset (b).

program for the desired target task, without re-training the model. In Figure 6, we report qualitative
results: in 6a, the generation is conditioned on a diﬀerent label y referring to the corresponding
mathematical operation between the ﬁrst and second digit; in 6b, the model is asked to generate a
trajectory starting from the initial image (t = 0) and following the shortest path using an up priority
or a right priority.

In all the novel tasks of 2digit MNIST dataset (Figure 6a), VAEL generates pairs of numbers
consistent with the evidence, and it also shows a variety of digits combinations by relying on the
probabilistic engine of ProbLog. This should not surprise. In fact, in all these tasks, the decoder
takes as input a possible world, i.e., a speciﬁc conﬁguration of the two digits. Therefore, the decoder
is agnostic to the speciﬁc operation, which is entirely handled by the symbolic program. For this
reason, VAEL can be seamlessly applied to all those tasks that require the manipulation of two
digits. The same reasoning can be extended to Mario novel tasks (Figure 6b), where VAEL generates
subsequent states consistent with the shortest path, while preserving the background of the initial
state (t = 0) thanks to the clear separation between the subsymbolic and symbolic latent components.
Additional results can be found in Appendix E.

Data Eﬃciency. In this task, we want to verify whether the use of a logic-based prior helps the

9

learning in contexts characterized by data scarcity. To this goal, we deﬁne diﬀerent training splits of
increasing size for the addition task of 2digit MNIST dataset. In particular, the diﬀerent splits range
from 10 up to 100 images per pair of digits. The results (Figure 13 in Appendix F) show that VAEL
outperforms the baseline for all the tested sizes. In fact, with only 10 images per pair, VAEL already
performs better than CCVAE trained with 100 images per pair. When considering 10 images per
pair, the discriminative and generative accuracies of VAEL are 0.445 ± 0.057 and 0.415 ± 0.0418,
whereas CCVAE trained on 100 images per pair has a discriminative and generative accuracy of
0.121 ± 0.006 and 0.284 ± 0.006 respectively. The reason behind this disparity is that the logic-based
prior helps the neural model in properly structuring the latent representation, so that one part can
easily focus on recognizing individual digits and the other on capturing the remaining information in
the scene. Conversely, CCVAE needs to learn how to correctly model very diﬀerent pairs that sum
up to the same value. We further investigate the performance gap between CCVAE and VAEL by
running an identical experiment in a simpliﬁed dataset with only three possible digits values: 0, 1
and 2. The goal is to train CCVAE on a much larger number of images per pair, which is impractical
in the 10-digits setting, due to the combinatorial nature of the task. Additional details can be found
in Appendix F.

5 Related Work

Controlled image generation. We distinguish between generative models based on text descrip-
tions and generative models based on scene graphs. Regarding the ﬁrst category, substantial eﬀort
has been devoted to devising strategies able to generate images with control (i) on object proper-
ties/attributes (e.g. shape, color, texture of objects) [57, 58, 70, 71, 13], (ii) on spatial relations
between multiple objects (e.g. object A is below object B) [50, 54, 24, 46], (iii) or both [55]. Our
framework is related to these works as considering the problem of generation in a relational setting.
Diﬀerently from them, we use probabilistic logic programming to encode ﬁrst-order logical knowledge
and to perform reasoning over this knowledge. This comes with the advantage that we can generalize
to out-of-distribution relations, which consists of both the composition of previously seen relations
(e.g. the multiplication can be composed by using the sum in the domain of natural numbers) and
new relations (e.g. the subtraction cannot be composed by using the sum in the domain of natural
numbers). Regarding the second category, scene graphs are used as an alternative to text descriptions
to explicitly encode relations, such as spatial relations between objects [30, 1, 22, 42, 52, 23, 25, 6].
While related, our approach diﬀers from these last as logical programs are more expressive and allow
a more general reasoning than scene graphs alone.

Unsupervised scene decomposition We distinguish between object-oriented, part-oriented and
hierarchical approaches. The ﬁrst category attempts to learn individual object representations in
an unsupervised manner and to reconstruct the original image or the subsequent frame (in the
case of sequential data) from these representations. Several approaches have been proposed, based
[21, 15, 8] and their
on scene-mixtures [19, 63, 5, 20, 14, 47, 38, 62], spatial attention models
corresponding combination [45, 29]. In the second category, a scene with an object is decomposed
into its constituent parts. Speciﬁcally, an encoder and a decoder are used to decompose an object
into its primitives and to recombine them to reconstruct the original object, respectively. Several
approaches have been proposed for generating 3D shapes [65, 41, 72, 26, 33, 11] and for inferring
the compositional structure of the objects together with their physical interactions in videos [68,
43, 17]. These approaches focus on learning the part-whole relationships of object either by using
pre-segmented parts or by using motion cues. Last but not least, there has been recent eﬀort focusing
on integrating the previous two categories, thus learning to decompose a scene into both its objects

10

and their respective parts, the so called hierarchical decomposition [59, 12]. Our work diﬀers in several
aspects and can be considered as an orthogonal direction. First of all, we consider static images
and therefore we do not exploit temporal information. Secondly, we do not provide any information
about the location of the objects or their parts and use a plain autoencoder architecture to discover
the objects. Therefore, we could exploit architectural advances in unsupervised scene decomposition
to further enhance our framework. However, this integration is left to future investigation. Finally,
our model discovers objects in a scene, by leveraging the high-level logical relations among them.

Neuro-symbolic generation. This is an emerging area of machine learning as demonstrated by
works appeared in the last few years. For example, [28] proposes a generative model based on a
two-layered latent representation. In particular, the model introduces a global sub-symbolic latent
variable, capturing all the information about a scene and a symbolic latent representation, encoding
the presence of an object, its position, depth and appearance. However, the model is limited in the
form of reasoning, as able to generate images with objects fulﬁlling only speciﬁc spatial relations.
In contrast, our model can leverage a logical reasoning framework and solve tasks requiring to
manipulate knowledge to answer new generative queries.

There are two recent attempts focusing on integrating generative models with probabilistic program-
ming [16, 18], where reasoning is limited to spatial relationships of (parts of) the image. Moreover,
[18] is a clear example of the diﬃculty of integration the symbolic and the perceptual module. In
contrast, our work provides a uniﬁed model which can learn to generate images while perform logical
reasoning at the same time.

To the best of our knowledge, the work in [60] represents the ﬁrst attempt to integrate a generative
approach with a logical framework. However, the work diﬀers from ours in several aspects. Firstly,
the authors propose a model for an image completion problem on MNIST and it is unclear how the
model can be used in our learning setting and for generating images in the presence of unseen queries.
Secondly, the authors propose to use sum-product networks as an interface between the logical and
the neural network modules. In contrast, we provide a probabilistic graphical model which compactly
integrates the two modules without requiring any additional network. Thirdly, we are the ﬁrst to
provide experiments supporting the beneﬁts of such integration both in terms of task generalization
and data eﬃciency.

Structured priors for latent variable models. Several structured priors have been proposed
in the context of latent variable models. For example, The work in [64] focuses on learning priors
based on mixture distributions. [2] uses rejection sampling with a learnable acceptance function to
construct a complex prior. The works of [61, 48, 66, 39] consider learning hierarchical priors, [7,
53, 56] introduce autoregressive priors [7]. While structured priors oﬀer the possibility of learning
ﬂexible generative models and avoid the local minima phenomenon observed in traditional VAEs,
they are quite diﬀerent from ours. Indeed, our prior disentangles the latent variables to support
logical reasoning. Furthermore, the structure of the logic program is interpretable.

6 Conclusions and Future Works

In this paper, we presented VAEL, a neuro-symbolic generative model that integrates VAE with
Probabilistic Logic Programming. The symbolic component allows to decouple the internal latent
representation from the task at hand, thus allowing an unprecedented generalization power. We
showcased the potential of VAEL in two image generation benchmarks, where VAEL shows state-

11

of-the-art generation performance, also in regimes of data scarcity and in generalization to several
prediction tasks.

In the future, we plan to improve VAEL by investigating alternative and more scalable semantics
for probabilistic programs (e.g. stochastic logic program [67]). Moreover, we plan to apply VAEL
to other settings, like structured object generation [44], to showcase the ﬂexibility and expressivity
provided by the integration with a probabilistic logic program.

Acknowledgements

Giuseppe Marra is funded by the Research Foundation-Flanders (FWO-Vlaanderen, GA No 1239422N).
Emanuele Sansone is funded by TAILOR, a project funded by EU Horizon 2020 research and innovation
programme under GA No 952215. The authors would like to thank Luc De Raedt for supporting
this project as an Erasmus Master Thesis, and Federico Ruggeri for his support in the experimental
phase.

References

[1] Oron Ashual and Lior Wolf. “Specifying Object Attributes and Relations in Interactive Scene

Generation”. In: ICCV. 2019, pp. 4560–4568.

[2] Matthias Bauer and Andriy Mnih. “Resampled Priors for Variational Autoencoders”. In:

AISTATS. 2019, pp. 66–75.

[3] Yoshua Bengio and Gary Marcus. AI Debate. https://montrealartificialintelligence.

com/aidebate/. 2020.

[4] Tarek R Besold et al. “Neural-symbolic learning and reasoning: A survey and interpretation”.

In: arXiv preprint arXiv:1711.03902 (2017).

[5] Christopher P. Burgess et al. “MONet: Unsupervised Scene Decomposition and Representation”.

In: CoRR (2019).

[6] Xiaojun Chang et al. “A Comprehensive Survey of Scene Graphs: Generation and Application”.

In: IEEE Trans. Pattern Anal. and Mach. Intell. (2021), pp. 1–1.

[7] Xi Chen et al. “Variational Lossy Autoencoder”. In: ICLR. 2017.

[8] Eric Crawford and Joelle Pineau. “Spatially Invariant Unsupervised Object Detection with

Convolutional Neural Networks”. In: AAAI. 2019, pp. 3412–3420.

[9] Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. “ProbLog: A Probabilistic Prolog and

Its Application in Link Discovery.” In: IJCAI. Vol. 7. 2007, pp. 2462–2467.

[10] Luc De Raedt et al. “From statistical relational to neuro-symbolic artiﬁcial intelligence”. In:

IJCAI. 2020.

[11] Boyang Deng et al. “CvxNet: Learnable Convex Decomposition”. In: CVPR. 2020, pp. 31–41.

[12] Fei Deng et al. “Generative Scene Graph Networks”. In: ICLR. 2021.

[13] Yilun Du, Shuang Li, and Igor Mordatch. “Compositional Visual Generation and Inference

with Energy Based Models”. In: NeurIPS. 2020.

[14] Martin Engelcke et al. “GENESIS: Generative Scene Inference and Sampling with Object-

Centric Latent Representations”. In: ICLR. 2020.

12

[15] S. M. Ali Eslami et al. “Attend, Infer, Repeat: Fast Scene Understanding with Generative

Models”. In: NeurIPS. 2016, pp. 3225–3233.

[16] Reuben Feinman and Brenden M. Lake. “Generating New Concepts with Hybrid Neuro-Symbolic

Models”. In: CogSci. 2020.

[17] Anand Gopalakrishnan, Sjoerd van Steenkiste, and J¨urgen Schmidhuber. “Unsupervised Object

Keypoint Learning using Local Spatial Predictability”. In: ICLR. 2021.

[18] Nishad Gothoskar et al. “3DP3: 3D Scene Perception via Probabilistic Programming”. In:

NeurIPS. 2021.

[19] Klaus Greﬀ, Sjoerd van Steenkiste, and J¨urgen Schmidhuber. “Neural Expectation Maximiza-

tion”. In: NeurIPS. 2017.

[20] Klaus Greﬀ et al. “Multi-Object Representation Learning with Iterative Variational Inference”.

In: ICML. 2019, pp. 2424–2433.

[21] Karol Gregor et al. “DRAW: A Recurrent Neural Network For Image Generation”. In: ICML.

Vol. 37. 2015, pp. 1462–1471.

[22] Jiuxiang Gu et al. “Scene Graph Generation With External Knowledge and Image Reconstruc-

tion”. In: CVPR. 2019, pp. 1969–1978.

[23] Roei Herzig et al. “Learning Canonical Representations for Scene Graph to Image Generation”.

In: ECCV. Vol. 12371. 2020, pp. 210–227.

[24] Seunghoon Hong et al. “Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis”.

In: CVPR. 2018, pp. 7986–7994.

[25] Tianyu Hua et al. “Exploiting Relationship for Complex-scene Image Generation”. In: AAAI.

2021, pp. 1584–1592.

[26] Jialei Huang et al. “Generative 3D Part Assembly via Dynamic Graph Learning”. In: NeurIPS.

2020.

[27] Eric Jang, Shixiang Gu, and Ben Poole. “Categorical Reparameterization with Gumbel-

Softmax”. In: ICLR. 2017.

[28] Jindong Jiang and Sungjin Ahn. “Generative Neurosymbolic Machines”. In: NeurIPS. 2020.

[29] Jindong Jiang et al. “SCALOR: Generative World Models with Scalable Object Representa-

tions”. In: ICLR. 2020.

[30] Justin Johnson, Agrim Gupta, and Li Fei-Fei. “Image Generation From Scene Graphs”. In:

CVPR. 2018, pp. 1219–1228.

[31] Tom Joy et al. “Capturing Label Characteristics in VAEs”. In: ICLR. 2021.

[32] Daniel Kahneman. Thinking, fast and slow. Macmillan, 2011.

[33] Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz. “UCSG-NET - Unsupervised Discover-

ing of Constructive Solid Geometry Tree”. In: NeurIPS. 2020.

[34] Henry Kautz. The Third AI Summer. https://roc-hci.com/announcements/the-third-

ai-summer/. 2020.

[35] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: ICLR.

2015.

[36] Diederik P. Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: ICLR. 2014.

13

[37] Diederik P. Kingma et al. “Semi-supervised Learning with Deep Generative Models”. In:

NeurIPS. 2014, pp. 3581–3589.

[38] Thomas Kipf et al. “Conditional Object-Centric Learning from Video”. In: CoRR (2021).

[39] Alexej Klushyn et al. “Learning Hierarchical Priors in VAEs”. In: NeurIPS. 2019, pp. 2866–2875.

[40] Yann LeCun et al. “Gradient-based learning applied to document recognition”. In: IEEE 86.11

(1998), pp. 2278–2324.

[41] Jun Li et al. “GRASS: generative recursive autoencoders for shape structures”. In: ACM Trans.

Graph. 36.4 (2017), 52:1–52:14.

[42] Yikang Li et al. “PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph”.

In: NeurIPS. 2019, pp. 3950–3960.

[43] Yunzhu Li et al. “Causal Discovery in Physical Systems from Videos”. In: NeurIPS. 2020.

[44] Luca Di Liello et al. “Eﬃcient Generation of Structured Objects with Constrained Adversarial

Networks”. In: NeurIPS. 2020.

[45] Zhixuan Lin et al. “SPACE: Unsupervised Object-Oriented Scene Representation via Spatial

Attention and Decomposition”. In: ICLR. 2020.

[46] Nan Liu et al. “Learning to Compose Visual Relations”. In: NeurIPS Workshop (CtrlGen).

2021.

[47] Francesco Locatello et al. “Object-Centric Learning with Slot Attention”. In: NeurIPS. 2020.

[48] Lars Maaløe et al. “BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling”.

In: NeurIPS. 2019, pp. 6548–6558.

[49] Robin Manhaeve et al. “DeepProbLog: Neural Probabilistic Logic Programming”. In: NeurIPS.

2018, pp. 3753–3763.

[50] Elman Mansimov et al. “Generating Images from Captions with Attention”. In: ICLR. 2016.

[51] Pasquale Minervini et al. “Learning reasoning strategies in end-to-end diﬀerentiable proving”.

In: ICML. 2020.

[52] Gaurav Mittal et al. “Interactive Image Generation Using Scene Graphs”. In: ICLR Workshop

(DeepGenStruct). 2019.

[53] A¨aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete Representation

Learning”. In: NeurIPS. 2017, pp. 6306–6315.

[54] A¨aron van den Oord et al. “Conditional Image Generation with PixelCNN Decoders”. In:

NeurIPS. 2016, pp. 4790–4798.

[55] Aditya Ramesh et al. “Zero-Shot Text-to-Image Generation”. In: ICML. Vol. 139. 2021, pp. 8821–

8831.

[56] Ali Razavi, A¨aron van den Oord, and Oriol Vinyals. “Generating Diverse High-Fidelity Images

with VQ-VAE-2”. In: NeurIPS. 2019, pp. 14837–14847.

[57] Scott E. Reed et al. “Generative Adversarial Text to Image Synthesis”. In: ICML. 2016,

pp. 1060–1069.

[58] Scott E. Reed et al. “Learning What and Where to Draw”. In: NeurIPS. 2016.

[59] Sara Sabour et al. “Unsupervised Part Representation by Flow Capsules”. In: ICML. Vol. 139.

2021, pp. 9213–9223.

14

[60] A. Skryagin et al. “Sum-Product Logic: Integrating Probabilistic Circuits into DeepProbLog”.
In: ICML 2020 Workshop on Bridge Between Perception and Reasoning: Graph Neural Networks
and Beyond. 2020.

[61] Casper Kaae Sønderby et al. “Ladder Variational Autoencoders”. In: NeurIPS. 2016, pp. 3738–

3746.

[62] Aleksandar Stanic, Sjoerd van Steenkiste, and J¨urgen Schmidhuber. “Hierarchical Relational

Inference”. In: AAAI. 2021, pp. 9730–9738.

[63] Sjoerd van Steenkiste et al. “Relational Neural Expectation Maximization: Unsupervised

Discovery of Objects and their Interactions”. In: ICLR. 2018.

[64] Jakub M. Tomczak and Max Welling. “VAE with a VampPrior”. In: AISTATS. 2018, pp. 1214–

1223.

[65] Shubham Tulsiani et al. “Learning Shape Abstractions by Assembling Volumetric Primitives”.

In: CVPR. 2017, pp. 1466–1474.

[66] Arash Vahdat and Jan Kautz. “NVAE: A Deep Hierarchical Variational Autoencoder”. In:

NeurIPS. 2020.

[67] Thomas Winters et al. “DeepStochLog: Neural Stochastic Logic Programming”. In: UAI, to

appear. 2021.

[68] Zhenjia Xu et al. “Unsupervised Discovery of Parts, Structure, and Dynamics”. In: ICLR. 2019.

[69] Kexin Yi et al. “Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language

Understanding”. In: NeurIPS. 2018.

[70] Han Zhang, Tao Xu, and Hongsheng Li. “StackGAN: Text to Photo-Realistic Image Synthesis

with Stacked Generative Adversarial Networks”. In: ICCV. 2017, pp. 5908–5916.

[71] Han Zhang et al. “StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial

Networks”. In: IEEE Trans. Pattern Anal. Mach. Intell. 41.8 (2019), pp. 1947–1962.

[72] Chenyang Zhu et al. “SCORES: shape composition with recursive substructure priors”. In:

ACM Trans. Graph. 37.6 (2018), 211:1–211:14.

15

A ELBO derivation

To derive the ELBO deﬁned in (5) we start from the maximization of the log-likelihood of the input
image x and the class y, namely

log(p(x, y)) = log

(cid:18)(cid:90)

(cid:19)

p(x, y|z)dz

.

Recalling the generative network factorization (4), we can write

log(p(x, y)) = log

(cid:18)(cid:90)

pθ(x|z, zsym)pθ(y|zsym)p(z)p(zsym)dzdzsym

(cid:19)

(6)

(7)

Then, by introducing the variational approximation qφ(z|x) to the intractable posterior pθ(z|x) and
applying the factorization, we get

log(p(x, y)) = log

(cid:18)(cid:90) qφ(z|x)qφ(zsym|x)
qφ(z|x)qφ(zsym|x)

pθ(x|z, zsym)pθ(y|zsym)p(z)p(zsym)dzdzsym

.

(8)

(cid:19)

We now apply the Jensen’s inequality to equation (8) and we obtain the lower bound for the
log-likelihood of x and y given by

(cid:90)

qφ(z|x)qφ(zsym|x) log

pθ(x|z, zsym)pθ(y|zsym)

(cid:18)

p(z)p(zsym)
qφ(z|x)qφ(zsym|x)

(cid:19)

dzdzsym

.

(9)

Finally, by relying on the linearity of expectation and on logarithm properties, we can rewrite
equation (9) as

Ez∼qφ(z|x) [log(pθ(x|z))] + Ezsym∼qφ(zsym|x) [log(pθ(y|zsym))] + Ez∼qφ(z|x)

(cid:20)

log

(cid:18) p(z)
qφ(z|x)

(cid:19)(cid:21)

.

The last term is the negative Kullback-Leibler divergence between the variational approximation
qφ(z|x) and the prior p(z). This leads us to the ELBO of equation (5), that is

log(p(x, y)) ≥ Ez∼qφ(z|x) [log(pθ(x|z))] + Ezsym∼qφ(zsym|x) [log(pθ(y|zsym))] − DKL[qφ(z|x)||p(z)]

:= L(θ, φ).

In VAEL graphical model (Figure 1c), we omit ωF since we exploit an equivalence relation between
the probabilistic graphical models (PGMs) shown in Figure 7. Indeed, the objective for the PGM
where ωF is explicit is equivalent to the one reported in the paper. This is supported by the derivation
of log p(x, y) (Eq. 10), which is equivalent to Eq. (5) in our paper, where the expectation over ωF is
estimated through Gumbel-Softmax.

log p(x, y) = log

(cid:90)

z,zsym,ωF

q(z, zsym|x)p(x|z, ωF )p(y|zsym)p(ωF |zsym, y)

(cid:90)

≥

z,zsym,ωF

q(z, zsym|x)p(ωF |zsym, y) log p(x|z, ωF )p(y|zsym)

p(z, zsym)
q(z, zsym|x)
p(z, zsym)
q(z, zsym|x)

= Ez,zsym,ωF [log p(x|z, ωF )] + Ezsym [log p(y|zsym)] − KL[q(z, zsym|x)(cid:107)p(z, zsym)]

(10)

16

Figure 7: PGM with (left) and without (right) ProbLog box.

B ELBO estimation and Learning

We estimate the ELBO and its gradients w.r.t. the model parameters using standard Monte Carlo
estimates of expectations [36]. Since both qφ(z|x) and p(z) are chosen to be Gaussian distributions,
the Kullback-Leibler divergence in (5) can be integrated analytically by relying on its closed form.
Thus, only the expected reconstruction and query errors LREC(θ, φ) and LQ(θ, φ) require estimation
by sampling.
We can therefore deﬁne the ELBO estimator as

L(θ, φ) ≈ ˜L(θ, φ; (cid:15)) = ˜LREC(θ, φ; (cid:15)) + ˜LQ(θ, φ; (cid:15)) − DKL[qφ(z|x)||p(z)].

(11)

The estimators of LREC and LQ can be written as

where

˜LREC(θ, φ; (cid:15)) =

˜LQ(θ, φ; (cid:15)) =

1
N

1
N

N
(cid:88)

(log(pθ(x|ˆz(n))))

n=1

N
(cid:88)

(log(pθ(y|ˆz(n)

sym)))

n=1

ˆz(n) = {ˆz(n), ˆz(n)
(cid:15)(n) ∼ N (0, 1).

sym} := µ(x) + σ(x)(cid:15)(n),

(12)

(13)

During the training, we aim at maximizing L(θ, φ) with respect to both the encoder and the decoder
parameters, we therefore need to compute the gradient w.r.t. θ and φ. Since any sampling operation
prevents back-propagation, we need to reparametrize the two sampled variables z and ω. Due to their
nature, we use the well-known Reparametrization Trick [36] for the Gaussian z, while we exploit the
Categorical Reparametrization with Gumbel-Softmax [27] for the discrete variable ω corresponding
to the sampled possible world.
In particular, by deﬁning ω as the one-hot encoding of the possible worlds, we have

ˆωi =

exp((log πi + ˆgi)/λ
j=1 exp((log πj + ˆgj)/λ)

(cid:80)J

, with ˆgi ∼ Gumbel(0, 1)

(14)

17

where J is the number of possible worlds (e.g. all the possible pairs of digits), and πi depends on
zi
sym, which is reparametrized with the Gaussian Reparametrization Trick. In Algorithm 1 we report
VAEL training algorithm .

Algorithm 1: VAEL Training.

Data: Set of images X
θ, φ ← Initialization of paramters
repeat

Forward Phase
x ← Training sample
z = [z, zsym] ∼ q(z | x)
p = M LP (zsym)
ωF ∼ P (ωF ; p)
y ∼ P (y; p)
˜x ∼ p(x|z, ωF )
Backward Phase
g ← ∇θ,φL(θ, φ)
θ, φ ← Update parameters using gradients g

until convergence of parameters (θ, φ);

C Additional supervision for MNIST Task Generalization

During the training on 2digit MNIST dataset, the model may learn a mapping between symbol
and meaning that is logically correct, but diﬀerent from the desired one. Indeed, the two symbols 1
and 2 used for the left and right positions, respectively, of a handwritten digit in an image are just
an assumption. However, VAEL may switch the pairs (3, 2) and (2, 3), since they both sum up to
5. This would prevent VAEL from generalizing to tasks involving non-commutative operations (i.e.
subtraction and power ).

To solve this issue, we simply introduce additional supervision on the digits of very few images (1
image per pair of digits, i.e. 100 images in total) to guide the model toward the desired symbols
interpretation. This has to be intended just as an agreement between the model and the human. To
include this supervision in the training procedure, we add a regularizer term to the ELBO deﬁned in
(5), namely

LSU P (θ, φ) := L(θ, φ) + Ldigits(θ, φ)

where

Ldigits(θ, φ) = Ezsym∼qφ(zsym|x)[log(pθ(ydigits|zsym)]].

(15)

(16)

we have ydigits = [0, 1]).
In equation (16), ydigits refers to the labels over the digits (e.g. for image
Such a digit-level supervision can be easily done by virtue of ProbLog inference, that allows us to
retrieve the predicted label of each digit in an image by relying on the query over the digits values.

18

D Implementation details

D.1 VAEL

In Tables 2 and 3 we report the architectures of VAEL for 2digit MNIST and Mario dataset. For
both the datasets we performed a model selection by minimizing the objective function computed
on a validation set of 12, 000 samples for 2digit MNIST and 2, 016 samples for Mario. In all the
experiments we trained the model with Adam [35]. The explored hyper-parameters values are
reported in Section D.4.

latent space z ∈ RM , zsym ∈ RN with
For 2digit MNIST, the resulting best conﬁguration is:
dimension M = 8 and N = 15; weights 0.1, 1 × 10−5 and 1.0 for the reconstruction, Kullback-Leibler
and classiﬁcation term of the ELBO respectively; learning rate 1 × 10−3.

latent space z ∈ RM , zsym ∈ RN with dimension M = 30 and N = 18;
For Mario, we obtain:
weights 1 × 101, 1 × 101 and 1 × 104 for the reconstruction, Kullback-Leibler and classiﬁcation term
of the ELBO respectively; learning rate 1 × 10−4.

Table 2: VAEL architectures for 2digit MNIST dataset.

Encoder
Input 28 × 56 × 1 channel image
64 × 1 × 4 × 4 Conv2d stride 2 & ReLU
128 × 64 × 4 × 4 Conv2d stride 2 & ReLU
256 × 128 × 4 × 4 Conv2d stride 2 &ReLU
256 × 2(M + N ) Linear layer

Decoder
Input ∈ RM +20
(M + 20) × 256 Linear layer
256 × 128 × 5 × 4 ConvTranspose2d stride 2 & ReLU
128 × 64 × 4 × 4 ConvTranspose2d stride 2 & ReLU
1 × 64 × 4 × 4 ConvTranspose2d stride 2 & Sigmoid

MLP & ProbLog
Input ∈ RN
N × 20 Linear layer & ReLU
20 × 20 Linear layer
ProbLog (IN dim: 20, OUT dim: 100)

Table 3: VAEL architectures for Mario dataset.

Encoder
Input 200 × 100 × 3 channel image
64 × 3 × 5 × 5 Conv2d stride 2 & SELU
128 × 64 × 5 × 5 Conv2d stride 2 & SELU
256 × 128 × 5 × 5 Conv2d stride 2 & SELU
512 × 256 × 5 × 5 Conv2d stride 2 & SELU
512 × 2(M + 9) Linear layer

Decoder
Input ∈ RM +9
(M + 9) × 512 Linear layer
512 × 256 × 5 × 5 ConvTranspose2d stride 2 & SELU
256 × 128 × 5 × 5 ConvTranspose2d stride 2& SELU
128 × 64 × 5 × 5 ConvTranspose2d stride 2& SELU
3 × 64 × 5 × 5 ConvTranspose2d stride 2 & Sigmoid

MLP & ProbLog
Input ∈ RN
N × 20 Linear layer & ReLU
20 × 9 Linear layer
ProbLog (IN dim: 18, OUT dim: 24)

19

D.2 CCVAE

In the original paper [31], there was a direct supervision on each single element of the latent space.
To preserve the same type of supervision in our two digits addition task, where the supervision is on
the sum and not directly on the single digits, we slightly modify the encoder and decoder mapping
functions of CCVAE. By doing so, we ensure the correctness of the approach without changing the
graphical model. The original encoder function learns from the input both the mean µ and the
variance σ of the latent space distribution, while the decoder gets in input the latent representation
z = {zsym, z} (please refer to the original paper for more details [31]). In our modiﬁed version, the
encoder only learns the variance, while the mean is set to be equal to the image label µ = y, and the
decoder gets in input the label directly z∗ := {y, z}.

In Tables 4 and 5 we report the architectures of CCVAE for 2digit MNIST and Mario dataset. For
both the datasets we performed a model selection by minimizing the objective function computed
on a validation set of 12, 000 samples for 2digit MNIST and 2, 016 samples for Mario. In all the
experiments we trained the model with Adam [35]. The explored hyper-parameters values are
reported in Section D.4.

For 2digit MNIST, the resulting best conﬁguration is: latent space zsym ∈ RN with dimension equal
to the number of classes N = 19 (due to the one-to-one mapping between zsym and the label y);
latent space z ∈ RM with dimension M = 8, model objective reconstruction term with weight 0.05,
while the other ELBO terms with unitary weights; learning rate 1 × 10−4.

latent space zsym ∈ RN with dimension equal to the number of classes
For Mario, we obtain:
N = 4; latent space z ∈ RM with dimension M = 300, model objective Kullback-Leibler term and
classiﬁcation term with weight 1 × 104 and 1 × 103 respectively, while the other ELBO terms with
unitary weights; learning rate 1 × 10−4.

Table 4: CCVAE architectures for 2digit MNIST dataset.

Encoder
Input 28 × 56 × 1 channel image
64 × 1 × 4 × 4 Conv2d stride 2 & ReLU
128 × 64 × 4 × 4 Conv2d stride 2 & ReLU
256 × 128 × 4 × 4 Conv2d stride 2 &ReLU
256 × 2(M + N ) Linear layer

Decoder
Input ∈ RM +N
( M + N ) × 256 Linear layer
256 × 128 × 5 × 4 ConvTranspose2d stride 2 & ReLU
128 × 64 × 4 × 4 ConvTranspose2d stride 2& ReLU
1 × 64 × 4 × 4 ConvTranspose2d stride 2 & Sigmoid

Table 5: CCVAE architectures for Mario dataset.

Encoder
Input 200 × 100 × 3 channel image
64 × 3 × 5 × 5 Conv2d stride 2 & SELU
128 × 64 × 5 × 5 Conv2d stride 2 & SELU
256 × 128 × 5 × 5 Conv2d stride 2 &SELU
512 × 256 × 5 × 5 Conv2d stride 2 &SELU
512 × 2(M + N ) Linear layer

Decoder
Input ∈ RM +N
(M + N ) × 512 Linear layer
512 × 256 × 5 × 5 ConvTranspose2d stride 2 & SELU
256 × 128 × 5 × 5 ConvTranspose2d stride 2& SELU
128 × 64 × 5 × 5 ConvTranspose2d stride 2& SELU
3 × 64 × 5 × 5 ConvTranspose2d stride 2 & Sigmoid

20

D.3 Classiﬁers

In Table 6 we report the architecture of the classiﬁer used to measure the generative ability of VAEL
and CCVAE for 2digit MNIST dataset. We trained the classiﬁer on 60, 000 MNIST images [40] for 15
epochs with SGD with a learning rate of 1 × 10−2 and a momentum of 0.5, achieving 0.97 accuracy
on the test set.

Table 6

MNIST classiﬁer (2digit MNIST )
Input 28 × 28 × 1 channel image
Linear layer 784 × 128 & ReLU
Linear layer 128 × 64 & ReLU
Linear layer 64 × 10 & LogSoftmax

In Table 7 we report the architecture of the classiﬁer used to measure the generative ability of VAEL
and CCVAE for Mario dataset. We trained the classiﬁer on 9, 140 single state images of Mario
dataset for 10 epochs with Adam [35] optimizer with a learning rate of 1 × 10−4, achieving 1.0
accuracy on the test set.

Table 7

MNIST classiﬁer (Mario)
Input 100 × 100 × 3 channels image
Conv layer 5 × 5 × 32 & SELU
Conv layer 5 × 5 × 64 & SELU
Conv layer 5 × 5 × 128 & SELU
Linear layer 2048 × 9

D.4 Optimization

Experiments are conducted on a single Nvidia GeForce 2080ti 11 GB. Training consumed ∼ 2GB
for 2digit MNIST dataset and ∼ 2.8GB for Mario dataset, taking around 1 hour and 15 minutes to
complete 100 epochs for 2digit MNIST and 1 hour and 30 minutes to complete 100 epochs for Mario
dataset. As introduced in the previous sections, we performed a model selection based on ELBO
minimization for both the model.

In the following bullet lists, lr refers to the learning rate, z, zsym refer to the latent vectors dimensions,
WREC, WKL, WQ refer to the weights of LREC, DKL, LQ terms of VAEL objective function, and
WREC, WKL, Wq(y|zsym), Wq(y|x) refer to the corresponding terms of CCVAE objective function
(please refer to the original paper for more details [31]).

For 2digit MNIST we explore the following values; we repeat the model training 5 times for each
conﬁguration.

• VAEL

– z ∈ {8, 9, 10}

21

– zsym ∈ {15, 19}

– lr ∈ {0.0001, 0.001}

– WREC ∈ {0.0001, 0.001, 0.01, 0.1, 1, 10, 100}

– WKL ∈ {0.00001, 0.0001, 0.001}

– WQ ∈ {1, 5}

• CCVAE

– zsym ∈ {8, 10, 15, 20, 30}

– lr ∈ {0.00001, 0.0001}

– WKL ∈ {0.0001, 0.001, 0.01, 0.1, 1, 10, 100}

– WREC ∈ {0.01, 0.1, 1, 10, 100}

– Wq(y|zsym) ∈ {0.01, 0.1, 1, 10, 100}

– Wq(y|x) ∈ {0.01, 0.1, 1, 10, 100}

For Mario we explore the following values; we repeat the model training 5 times for each conﬁguration.

• VAEL

– z ∈ {20, 25, 30, 35, 40}

– zsym ∈ {18, 20}

– lr ∈ {0.0001, 0.0005}

– WREC ∈ {1, 10}

– WKL ∈ {0.1, 1, 10}

– WQ ∈ {1, 100, 10000}

• CCVAE

– zsym ∈ {3, 4, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400}

– lr ∈ {0.0001, 0.0005}

– WKL ∈ {0.0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000}

– WREC ∈ {1, 10}

– Wq(y|zsym) ∈ {1, 10, 100}

22

– Wq(y|x) ∈ {1, 10, 100, 1000}

E Additional Results

Here we report some additional results for the tasks described in Section 4.

Figures 8 and 9 show additional qualitative results for the Conditional Image Generation and Task
Generalization experiments relative to 2digit MNIST dataset.

In Figures 10 and 11, we report some additional examples of Image Generation and Task Generalization
for Mario dataset. As it can be seen in Figure 11, VAEL is able to generate subsequent states
consistent with the shortest path, whatever the agent’s position in the initial state (t = 0). Moreover,
the model generates states that are consistent with the initial one in terms of background.

Figure 12 shows some examples of image reconstruction for CCVAE. As it can be seen, CCVAE
focuses only on reconstructing the background and discards the small portion of the image containing
the agent, thus causing the disparity in the reconstructive and generative ability between VAEL and
CCVAE (Table 1).

Figure 8: Conditional generation for CCVAE and VAEL for 2digit MNIST dataset. In each column
the generation is conditioned on a diﬀerent sum y between the two digits.

Figure 9: Examples of the generation ability of VAEL in 3 previously unseen tasks for 2digit
MNIST dataset. In each column the generation is conditioned on a diﬀerent label y referring to the
corresponding mathematical operation between the ﬁrst and second digit.

23

Figure 10: Examples of the generation ability of CCVAE and VAEL for Mario dataset.

24

Figure 11: Examples of the generation ability of VAEL in previously unseen tasks for Mario dataset.
In each row, VAEL generates a trajectory starting from the initial image (t = 0) and following the
shortest path using an up priority or a right priority.

25

Figure 12: Examples of reconstructive ability of CCVAE and VAEL trained on Mario dataset.

26

F Data Eﬃciency: simpliﬁed setting

We compare VAEL and CCVAE discriminative, generative and reconstructive ability when varying
the training size of 2digit MNIST dataset. As it can be seen in Figure 13, VAEL outperforms the
baseline for all the tested sizes. In fact, with only 10 images per pair VAEL already performs better
than CCVAE trained with 100 images per pair.

Figure 13: Discriminative, generative and reconstructive ability of VAEL (red) and CCVAE (blue)
trained in contexts characterized by data scarcity. Both the models are evaluated on the same test
set. The training size refers to the number of samples per pair of digits see during the training.

To further investigate the performance gap between CCVAE and VAEL in the Data Eﬃciency task
4, we run an identical experiment in a simpliﬁed dataset with only three possible digits values: 0, 1
and 2. The goal is to train CCVAE on a much larger number of images per pair, which is impractical
in the 10-digits setting, due to the combinatorial nature of the task. The dataset consists of 30, 000
images of two digits taken from the MNIST dataset [40]. We use 80%, 10%, 10% splits for the train,
validation and test sets, respectively. As for the 10-digits dataset, each image in the dataset has
dimension 28 × 56 and is labelled with the sum of the two digits. In Figure 14 we compare VAEL
and CCVAE discriminative, generative and reconstructive ability when varying the training size.
In this simpliﬁed setting, CCVAE requires around 2500 images per pair to reach the accuracy that
VAEL achieves trained with only 10 images per pair.

Figure 14: Discriminative, generative and reconstructive ability of VAEL (red) and CCVAE (blue)
trained in contexts characterized by data scarcity. Both the models are evaluated on the same test
set. The training size refers to the number of samples per pair of digits see during the training.

27

