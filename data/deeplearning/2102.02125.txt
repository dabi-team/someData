Zuse Institute Berlin

Takustr. 7
14195 Berlin
Germany

LOVIS ANDERSON1, MARK TURNER2, THORSTEN KOCH3

Generative deep learning for decision
making in gas networks

1
2
0
2

b
e
F
3

]

C
O
.
h
t
a
m

[

1
v
5
2
1
2
0
.
2
0
1
2
:
v
i
X
r
a

1

2

3

0000-0002-4316-1862
0000-0001-7270-1496
0000-0002-1967-0077

ZIB Report 20-38 (December 2020)

 
 
 
 
 
 
Zuse Institute Berlin
Takustr. 7
14195 Berlin
Germany

Telephone: +49 30-84185-0
Telefax: +49 30-84185-125

E-mail: bibliothek@zib.de
URL: http://www.zib.de

ZIB-Report (Print) ISSN 1438-0064
ZIB-Report (Internet) ISSN 2192-7782

Generative deep learning for decision making in
gas networks

Lovis Anderson, Mark Turner, Thorsten Koch

February 4, 2021

Abstract

A decision support system relies on frequent re-solving of similar prob-
lem instances. While the general structure remains the same in corre-
sponding applications, the input parameters are updated on a regular
basis. We propose a generative neural network design for learning integer
decision variables of mixed-integer linear programming (MILP) formula-
tions of these problems. We utilise a deep neural network discriminator
and a MILP solver as our oracle to train our generative neural network.
In this article, we present the results of our design applied to the transient
gas optimisation problem. With the trained network we produce a feasi-
ble solution in 2.5s, use it as a warm-start solution, and thereby decrease
global optimal solution solve time by 60.5%.

1

Introduction

Mixed-Integer Linear Programming (MILP) is concerned with the modelling and
solving of problems from discrete optimisation. These problems can represent
real-world scenarios, where discrete decisions can be appropriately captured and
modelled by the integer variables. In real-world scenarios a MILP model is rarely
solved only once. More frequently, the same model is used with varying data to
describe diﬀerent instances of the same problem which are solved on a regular
basis. This holds true in particular for decision support systems, which can
utilise MILP to provide real-time optimal decisions on a continual basis, see [4]
and [40] for examples in nurse scheduling and vehicle routing. The MILPs that
these decision support systems solve have identical structure due to both their
underlying application and cyclical nature, and thus often have similar optimal
solutions. Our aim is to exploit this repetitive structure, and create generative
neural networks that generate binary decision encodings for subsets of important
variables. These encodings can then be used in a primal heuristic by solving the
induced sub-problem following variable ﬁxations. Additionally, the then result
of the primal heuristic can be used in a warm-start context to help improve solver
performance in a globally optimal context. We demonstrate the performance of
our neural network (NN) design on the transient gas optimisation problem [38],

1

speciﬁcally on real-world instances embedded in day-ahead decision support
systems.

The design of our framework is inspired by the recent development of Generative
Adversarial Networks (GANs) [17]. Our design consists of two NNs, a Generator
and a Discriminator. The Generator is responsible for generating the binary
decision values, while the Discriminator is tasked with predicting the optimal
objective function value of the MILP induced by ﬁxing these binary variables
to their generated values.

Our NN design and its application to transient gas-network MILP formula-
tions is an attempt to integrate Machine Learning (ML) into the MILP solving
process. This integration has recently received an increased focus [7, 16, 43],
which has been encouraged by the success of ML integration into other facets of
combinatorial optimisation, see [5] for a thorough overview. Our contribution
to this intersection of two ﬁelds is as follows: We introduce a new generative
NN design for learning integer variables of parametric MILPs, which interacts
with the MILP directly during training. We also apply our design to a much
more diﬃcult and convoluted problem than traditionally seen in similar papers,
namely the transient gas transportation problem. This paper is to the best our
knowledge the ﬁrst successful implementation of ML applied to discrete control
in gas-transport.

2 Background and Related Work

As mentioned in the introduction, the intersection of MILP and ML is cur-
rently an area of active and growing research. For a thorough overview of Deep
Learning (DL), the relevant subset of ML used throughout this article, we refer
readers to [18], and for MILP to [1]. We will highlight previous research from
this intersection that we believe is either tangential, or may have shared appli-
cations to that presented in this paper. Additionally, we will brieﬂy detail the
state-of-the-art in transient gas transport, and highlight why our design is of
practical importance. It should be noted as-well, that there are recent research
activities aiming at the reverse direction, with MILP applied to ML instead of
the orientation we consider, see [45] for an interesting example.

Firstly, we summarise applications of ML to adjacent areas of the MILP solving
process. [16] creates a method for encoding MILP structure in a bipartite graph
representing variable-constraint relationships. This structure is the input to a
Graph Convolutional Neural Network (GCNN), which imitates strong branching
decisions. The strength of their results stem from intelligent network design
and the generalisation of their GCNN to problems of a larger size, albeit with
some generalisation loss. [47] take a diﬀerent approach, and use a NN design
that incorporates the branch-and-bound tree state directly. In doing so, they
show that information contained in the global branch-and-bound tree state is
an important factor in variable selection. Furthermore, they are one of the

2

few publications to present techniques on heterogeneous instances. [12] show a
successful implementation of reinforcement learning for variable selection. [43]
show preliminary results of how reinforcement learning can be used in cutting-
plane selection. By restricting themselves exclusively to Gomory cuts, they are
able to produce an agent capable of selecting better cuts than default solver
settings for speciﬁc classes of problems.

There exists a continuous trade-oﬀ between model exactness and complexity in
the ﬁeld of transient gas optimisation, and as such, there is no standard model
for transient gas transportation problems. [31] presents a piece-wise linear MILP
approach to the transient gas transportation problem, [8] a non-linear approach
with a novel discretisation scheme, and [24] and [26] a linearised approach. For
the purpose of our experiments, we use the model of [24], which uses linearised
equations and focuses on active element heavy subnetworks. The current re-
search of ML in gas transport is still preliminary. [37] use a dual NN design to
perform online calculations of a compressors operating point to avoid re-solving
the underlying model. The approach constraints itself to continuous variables
and experimental results are presented for a gunbarrel type network. [30] present
a NN combined with a genetic algorithm for learning the relationship between
compressor speeds and the fuel consumption rate in the absence of complete
data. More often ML has been used in ﬁelds closely related to gas transport, as
in [20], with ML used to track the degradation of compressor performance, and
in [35] to forecast demand values at the boundaries of the network. For a more
complete overview of the transient gas literature, we refer readers to [38].

Our Discriminator design, which predicts the optimal objective value of an
induced sub-MILP, can be considered similar to [3] in what it predicts and
similar to [14] in how it works.
In the ﬁrst paper [3], a neural network is
used to predict the associated objective value improvements on cuts. This is
a smaller scope than our prediction, but is still heavily concerned with the
In the second paper [14], a technique is developed that
MILP formulation.
performs backward passes directly through a MILP. It does this by solving
MILPs exclusively with cutting planes, and then receiving gradient information
from the KKT conditions of the ﬁnal linear program. This application of a
neural network, which produces input to the MILP, is very similar to our design.
The diﬀerences arise in that we rely on a NN Discriminator to appropriately
distribute the loss instead of solving a MILP directly, and that we generate
variable values instead of parameter values with our Generator.

While our discriminator design is heavily inspired from GANs [17], it is also
similar to actor-critic algorithms, see [36]. These algorithms have shown success
for variable generation in MILP, and are notably diﬀerent in that they sam-
ple from a generated distribution for down-stream decisions instead of always
taking the decision with highest probability. Recently, [9] generated a series of
coordinates for a set of UAVs using an actor-critic based algorithm, where these
coordinates were continuous variables in a MINLP formulation. The indepen-
dence of separable sub-problems and the easily realisable value function within

3

their formulation resulted in a natural Markov Decision Process interpretation.
For a better comparison on the similarities between actor-critic algorithms and
GANs, we refer readers to [36].

Finally, we summarise existing research that also deals with the generation
of decision variable values for MIPs. [6, 7] attempt to learn optimal solutions
of parametric MILPs and MIQPs, which involves both outputting all integer
decision variable values and the active set of constraints. They mainly use
Optimal Classiﬁcation Trees in [6] and NNs in [7]. Their aim is tailored towards
smaller problems classes, where speed is an absolute priority and parameter
value changes are limited. [29] learn binary warm start decisions for MIQPs.
They use NNs with a loss function that combines binary cross entropy and a
penalty for infeasibility. Their goal of a primal heuristic is similar to ours, and
while their design is much simpler, it has been shown to work eﬀectively on
very small problems. Our improvement over this design is our non-reliance on
labelled optimal solutions which are needed for binary cross entropy. [11] present
a GCNN design which is an extension of [16], and use it to generate binary
decision variable values. Their contributions are a tripartite graph encoding
of MILP instances, and the inclusion of their aggregated generated values as
branching decisions in the branch-and-bound tree, both in an exact approach
and in an approximate approach with local branching [15]. Very recently, [32]
combined the branching approach of [16] with a novel neural diving approach,
in which integer variable values are generated. They use a GCNN both for
generating branching decisions and integer variables values. Diﬀerent to our
generator-discriminator based approach, they generate values directly from a
learned distribution, which is based on an energy function that incorporates
resulting objective values.

3 The Solution Framework

We begin by formally deﬁning both a MILP and a NN. Our deﬁnition of a MILP
is an extension of more traditional formulations, see [1], but still encapsulates
general instances.

Deﬁnition 1. Let π ∈ Rp be a vector of problem deﬁning parameters. We call
the following a MILP parameterised by π.

Pπ := min cT

3 z1 + cT

4 z2

1 x1 + cT
2 x2 + cT


x1
x2
z1
z2

≤ bπ









s.t Aπ

(1)

ck ∈ Rnk , k ∈ {1, 2, 3, 4}, Aπ ∈ Rm×n, bπ ∈ Rm
x1 ∈ Rn1 , x2 ∈ Rn2 , z1 ∈ Zn3, z2 ∈ Zn4

Furthermore let Σ ⊂ Rp be a set of valid problem deﬁning parameters. We then

4

Figure 1: The general design of N{θ1,θ2}

call {Pπ|π ∈ Σ} a problem class for Σ.

Note that the explicit parameter space Σ is usually unknown, but we assume
in the following to have access to a random variable Π that samples from Σ. In
addition, note that c, n1, n2, n2, and n4 are not parameterised by π, and as such
the objective function and variable dimensions do not change between scenarios.

Deﬁnition 2. A k layer NN Nθ is given by the following:

Nθ : R|a1| −→ R|ak+1|
hi : R|ai| −→ R|ai|,
ai+1 = hi+1(Wiai + bi),

∀i ∈ {2, ..., k + 1}

∀i ∈ {1, ..., k}

(2)

Here θ fully describes all weights (W ) and biases (b) of the network. hi’s are
called activation functions and are non-linear element-wise functions.
An outline of our framework is depicted in Figure 1. The Generator Gθ1 is a NN
that takes as input π. Gθ1 outputs values for the variables z1, which we denote
by ˆz1. These variable values ˆz1 alongside π are then input into another NN,
namely the Discriminator Dθ2 . Dθ2 ﬁnally outputs a prediction of the optimal
objective function value of Pπ with values of z1 ﬁxed to ˆz1, namely ˆf (P ˆz1
π ). More
formally this is:
Deﬁnition 3. The generator Gθ1 and discriminator Dθ2 are both NNs deﬁned
by the following:

Gθ1 : Rp −→ Zn3
Dθ2 : Rp × Zn3 −→ R

Furthermore, a forward pass of both Gθ1 and Dθ2 is deﬁned as follows:

ˆz1 = Gθ1(π)
π ) = Dθ2( ˆz1, π)

ˆf (P ˆz1

5

(3)

(4)

(5)

The hat notation is used to denote quantities that were approximated by a NN,
and f (Pπ) refers to the optimal objective function value of Pπ. We use super-
script notation to create the following instances:

π = Pπ
P ˆz1

s.t

z1 = ˆz1

(6)

Note that the values of ˆz1 must be appropriately rounded when explicitly solving
P ˆz1
π s.t they are feasible w.r.t. their integer constraints. As such, it is a slight
abuse notation to claim that Gθ1(π) lies in Zn3
The goal of this framework is to produce good initial solution values for z1,
which lead to an induced sub-MILP, Pz1
π , whose optimal solution is a good
feasible solution to the original problem. Further, the idea is to use this feasible
solution as a ﬁrst incumbent for warm-starting Pπ. To ensure feasibility for all
choices of z1, we divide the continuous variables into two sets, x1 and x2, as seen
in Deﬁnition 1. The variables x2 are potential slack variables to ensure that all
generated decisions result in feasible P ˆz1
π instances. Penalising these slacks in
the objective then feeds in naturally to our design, where Gθ1 aims to minimise
the induced optimal objectives. For the purpose of our application it should be
noted that z1 and z2 are binary variables instead of integer. Next we describe
the design of Gθ1 and Dθ2.

3.1 Generator and Discriminator Design
Gθ1 and Dθ2 are NNs whose structure is inspired by [17], as well as both incep-
tion blocks and residual NNs, which have greatly increased large scale model
performance [42]. We use the block design from Resnet-v2 [42], see Figure 3, al-
beit with slight modiﬁcations for the case of transient gas-network optimisation.
Namely, we primarily use 1-D convolutions with that dimension being time. Ad-
ditionally, we separate initial input streams by their characteristics, and when
joining two streams, use 2-D convolutions, where the second dimension is of size
2 and quickly becomes one dimensional again. See Figure 2 for an example of
this process. The ﬁnal layer of Gθ1 contains a softmax activation function with
temperature. As the softmax temperature increases, this activation function’s
output approaches a one-hot vector encoding. The ﬁnal layer of Dθ2 contains
a softplus activation function. All other intermediate layers of N{θ1,θ2} use the
ReLU activation function. We refer readers to [18] for a thorough overview of
deep learning, and to Figure 14 in Appendix A for our complete design.
For a vector x = (x1, · · · , xn), the Softmax function with temperature T ∈ R
(7), ReLu function (8), and Softplus function with parameter β ∈ R (9) are:

σ1(x, T ) :=

exp(T xi)
j=1 exp(T xj)

(cid:80)n

σ2(xi) := max(0, xi)
1
β

σ3(xi, β) :=

log(1 + exp(βxi))

6

(7)

(8)

(9)

Figure 2: Method of merging two 1-D input streams

Figure 3: 1-D Resnet-v2 Block Design

We can compose Gθ1 with Dθ2, as in Figure 1, so that the combined resulting
NN is deﬁned as:

N{θ1,θ2}(π) := Dθ2 (Gθ1(π), π)

(10)

3.2

Interpretations

In a similar manner to GANs and actor-critic algorithms, see [36], the design
of N{θ1,θ2} has a bi-level optimisation interpretation, see [10] for an overview of
bi-level optimisation. Here we list the explicit objectives of both Gθ1 and Dθ2,
and how their loss functions represent these objectives.
The objective of Dθ2 is to predict f (P ˆz1
of P ˆz1

π ), the optimal induced objective values

π . Its loss function is thus:

L(θ2, π) := (cid:12)

(cid:12)Dθ2(Gθ1(π), π) − f (PGθ1 (π)

π

)(cid:12)
(cid:12)

(11)

The objective of Gθ1 is to minimise the induced prediction of Dθ2.

Its loss

7

TimePadding1101122100442101011*Padded Input Stream 1Padded Input Stream 2Convolution Kernel Size 1 Padding 0ReLuConvolution Kernel Size 1 Padding 0Convolution Kernel Size 1 Padding 0Convolution Kernel Size 3 Padding 1Convolution Kernel Size 1 Padding 0InputConcatenateOutputConvolution Kernel Size 3 Padding 1Convolution Kernel Size 3 Padding 1Convolution Kernel Size 1 Padding 0function is thus:

L(cid:48)(θ1, π) := Dθ2 (Gθ1(π), π)

The corresponding bi-level optimisation problem can then be viewed as:

min
θ1

Eπ∼Π[Dθ2 (Gθ1(π), π)]

s.t min
θ2

Eπ∼Π[Dθ2(Gθ1(π), π) − f (PGθ1 (π)

π

)]

(12)

(13)

3.3 Training Method
For eﬀective training of Gθ1 , a capable Dθ2 is needed. We therefore pre-train
Dθ2 . The following loss function, which replaces Gθ1(π) with prior generated z1
values in (11), is used for this pre-training:

L(cid:48)(cid:48)(θ2, π) := (cid:12)

(cid:12)Dθ2(z1, π) − f (Pz1

π )(cid:12)
(cid:12)

(14)

However, performing this initial training requires generating instances of Pz1
π .
Here we do supervised training in an oﬄine manner on prior generated data.
After the initial training of Dθ2, we train Gθ1 as a part of N{θ1,θ2}, using samples
π ∈ Π, the loss function (12), and ﬁxed θ2. The issue of Gθ1 outputting con-
tinuous values for ˆz1 is overcome by the ﬁnal layer’s activation function of Gθ1.
The softmax with temperature (7) ensures that adequate gradient information
still exists to update θ1, and that the results are near binary. When using these
results to explicitly solve P ˆz1
π , we round our result to a one-hot vector encoding
along the appropriate dimension.

After the completion of both initial training, we alternately train both NN’s
using updated loss functions in the following way:

• Dθ2 training:

– As in the initial training, using loss function (14).
– In an online fashion, using predictions from Gθ1 and loss function

(11).

• Gθ1 training:

– As explained above with loss function (12).

Our design allows the loss to be back-propagated through Dθ2 and distributed
to the individual nodes of the ﬁnal layer of Gθ1 , i.e., that representing z1. This
is largely diﬀerent to other methods, many of which rely on using binary cross
entropy loss against optimal solutions of Pπ. Our advantage over these is that

8

the contribution to the objective function we are trying to minimise of each
variable decision in z1 can be calculated. This has an added beneﬁt of gener-
ated suboptimal solutions being much more likely to be near-optimal, as they
are trained in a manner to minimise the objective rather than copy previously
observed optimal solutions.

For our application, transient gas network optimisation, methods for sampling
instances currently do not exist.
In fact, even gathering data is notoriously
diﬃcult, see [28] and [46]. For this reason, we introduce a new method for
generating training data in section 5.

4 The Gas Transport Model

To evaluate the performance of our approach, we test our framework on the
transient gas optimisation problem, see [38] for an overview of the problem and
associated literature. This problem is diﬃcult to solve as it combines a transient
ﬂow problem with complex combinatorics representing switching decisions. The
natural modelling of transient gas networks as time-expanded networks lends
itself well to our framework however, due to the static underlying network and
repeated constraints at each time-step.

We use the description of transient gas networks by [24]. The advantages of
this description for our framework is a natural separation of z1 variables, which
induce feasible Pz1
π for all choices due to the existence of slack variables in the
description. These slack variables are then represented by x2 in Deﬁnition 1.
The gas network is modelled as a directed graph G = (V, A) where A is the
set of arcs representing network elements, e.g. pipes, and the nodes V represent
junctions between adjacent elements. Every arc a ∈ A models a speciﬁc element
with A = Api ∪ Ava ∪ Ars ∪ Arg ∪ Acs, i.e., pipes, valves, resistors, regulators,
and compressors. Additionally, the node set V contains multiple element types,
with V = V b ∪ V 0 partitioned into boundary and inner nodes respectively. The
boundary nodes represent the sources and sinks of the ﬂow network. Thus, ﬂow
and pressure forecasts are given for each v ∈ V b.

It should be noted that this description focuses on network stations, the beating
hearts of gas networks. Network stations are commonly located at the intersec-
tions of major pipelines and contain nearly all elements, which can be used to
control the gas ﬂow. Next, we brieﬂy explain the most important constraints
from the model of [24], particularly those which we exploit with our approach.
For a full deﬁnition of the MILP, please see [24].

As we optimise a transient problem, we deal with a time horizon, namely T0 :=
{0, . . . , k}. We aim to calculate a network state for each t ∈ T := T0 \ {0}, i.e.
control decisions for all future time steps. As such, the initial gas network state
at time 0 contains a complete description of that time step and is immutable. On
the other hand all future time steps contain, before optimising, only forecasted
pressure and ﬂow values at V b. We denote τ (t) as the time diﬀerence in seconds

9

from time step 0.

4.1 Pipe Equations

Pipes constitute the majority of elements in any gas transmission network. The
dynamics of ﬂow through pipes are governed by the Euler Equations, a set of
nonlinear hyperbolic partial diﬀerential equations, see [33]. We consider the
isothermal case and discretise as in [23]. Consider the pipe a = (u, v), a ∈ Api,
where u, v ∈ V are the two incident nodes. We attach a ﬂow-in qu,a,t and ﬂow-
out qv,a,t variable to each pipe. Additionally, each incident node has an attached
pressure variable, namely (pu,t) and (pv,t). Moreover, these ﬂow-in, ﬂow-out,
and pressure values also appear for each time step. Rs, za, and T are assumed to
be constant, and Da, La, sa, Aa, g, and λa are themselves constant. The above
constant assumptions are quite common in practice [38]. It is only after setting
the velocity of gas within each individual pipe, |vw,a| to be constant that all
non-linearities are removed however. We do this via a method developed in [23]
and seen in [13]. The resulting pipe equations are:

pu,t2 + pv,t2 − pu,t1 − pv,t1 +

2RsT za(τ (t2) − τ (t1))
LaAa

(qv,a,t2 − qu,a,t2) = 0 (15)

pv,t2 − pu,t2 +

λaLa
4DaAa

(|vu,a|qu,a,t2 + |vv,a|qv,a,t2)

+

gsaLa
2RsT za

(pu,t2 + pv,t2) = 0 (16)

As nodes represent junctions between network elements and thus have no vol-
ume in which to store any gas, the ﬂow conservation constraints (17) (18) are
required.
In the below equations, dv,t represents the inﬂow resp. outﬂow of
entry and exit nodes in the network at time t ∈ T0. Note that network elements
that aren’t pipes have only one associated ﬂow variable, instead of the in-out
ﬂow exhibited by pipes. This is due to them having no volume, and as such no
ability to store gas over time, i.e. line-pack.

(cid:88)

qw,a,t −

(cid:88)

qw,a,t

(u,w)=a∈Api

(w,v)=a∈Api

(cid:88)

qa,t −

(cid:88)

qa,t + dw,t = 0

∀w ∈ V b

(17)

(u,w)=a∈A\Api

(w,v)=a∈A\Api

(cid:88)

qw,a,t −

(cid:88)

qw,a,t

(u,w)=a∈Api

(w,v)=a∈Api

(cid:88)

qa,t −

(cid:88)

qa,t = 0

∀w ∈ V 0

(18)

(u,w)=a∈A\Api

(w,v)=a∈A\Api

+

+

10

4.2 Operation Modes

Operation modes represent binary decisions in our gas network. We identify the
corresponding binary variables with the z1 variables from our MILP formula-
tion (1). Let O represent the set of operation modes, and mom
o,t the associated
variables. Operation Modes are very important in our modelling context as
they describe every allowable combination of discrete decisions associated with
valves and compressors.

4.2.1 Compressors

Compressors are typically set up as a compressor station consisting of multiple
compressor units, which represent the union of one single compressor machine
and its associated drive. These compressor units are dynamically switched on
or oﬀ and used in diﬀerent sequences to meet the current needs in terms of com-
pression ratios and ﬂow rates. Out of the theoretically possible arrangements
of compressor units, the set of technically feasible arrangements are known as
the conﬁgurations of a compressor station.

Selecting an operation mode results in ﬁxed conﬁgurations for all compressor
stations. The binary variables associated with a compressor station a = (u, v) ∈
Acs at time t ∈ T0 are mby
c,a,t ∀c ∈ Ca (active).
Ca denotes the set of conﬁgurations associated to compressor station a avail-
able in active mode, where the conﬁguration’s operating range is a polytope in
space (pu,t, pv,t, qu,a,t). The polytope of conﬁguration c is represented by the
intersection of half-spaces, Hc = {(α0, α1, α2, α3) ∈ R4}.

a,t (closed), and mcf

a,t (bypass), mcl

1 =

(cid:88)

c∈Ca

mcf

c,a,t + mby

a,t + mcl
a,t

α0pu-cf

c,a,t + α1pv-cf

c,a,t + α2qcf

c,a,t + α3mcf

c,a,t ≤ 0

∀(α0, α1, α2, α3) ∈ Hc ∀c ∈ Ca

(19)

(20)

Note that the variables in (20) have an extra subscript and superscript compared
to those in (15) and (16). This is due to our use of the convex-hull reformulation,
see [2]. The additional subscript refers to the conﬁguration in question, and the
superscript the mode, with the pressure variables having an additional node
identiﬁer. It should also be noted that the continuous variables attached to a
compressor station are not ﬁxed by a choice in operation mode or conﬁguration,
but rather the operation mode restricts the variables to some polytope.

4.2.2 Valves

Valves decide the allowable paths through a network, and can separate areas,
decoupling their pressure levels. They are modelled as an arc a = (u, v), whose
discrete decisions can be decided by an operation mode choice. Valves have two

11

modes, namely open and closed. When a valve is open, similar to a compressor
station in bypass, ﬂow is unrestricted and there exists no pressure diﬀerence
between the valves start and endpoints. Alternatively in the closed mode, a valve
allows no ﬂow to pass, and decouples the pressure of the start- and endpoints of
the arc. The variable mop
a,t represents a valve being open with value 1 and closed
with value 0. The general notation
x and ¯x refer to lower and upper bounds of
¯
a variable x. The constraints describing valves are then as follows:

a,t)(¯pu,t −
pv,t)
¯
pu,t − ¯pv,t)
a,t)(
¯

pu,t − pv,t ≤ (1 − mop
pu,t − pv,t ≥ (1 − mop
qa,t ≤ (mop
a,t)¯qa,t
qa,t ≥ (mop
qa,t.
a,t)
¯

(21)

(22)

(23)

(24)

4.2.3 Valid Operation Modes

As mentioned earlier, not all combinations of compressor station conﬁgurations
and valve states are possible. We thus deﬁne a mapping M (o, a) from operation
mode o ∈ O to the discrete states of all a ∈ Ava ∪ Acs

M (o, a) := m where m is the mode or conﬁguration of arc a
in operation mode o ∀o ∈ O ∀a ∈ Ava ∪ Acs
m ∈ {op, cl} if a ∈ Ava
m ∈ {by, cl} ∪ Ca if a ∈ Acs

with

Using this mapping we can then deﬁne a set of constraints for all valid com-
binations of compressor station and valve discrete states for each t ∈ T . The
variable mom
o,t , o ∈ O t ∈ T , is a binary variable, where the value 1 represents
the selection of o at time step t.

12

(cid:88)

o∈O

mom

o,t = 1

mop

a,t =

mby

a,t =

mcl

a,t =

(cid:88)

mom
o,t

∀a ∈ Ava

o∈O:M (o,a)=op
(cid:88)

mom
o,t

∀a ∈ Acs

o∈O:M (o,a)=by
(cid:88)

mom
o,t

∀a ∈ Acs

o∈O:M (o,a)=cl
(cid:88)

mom
o,t

mcf

c,a,t =

o∈O:M (o,a)=c
o,t ∈ {0, 1} ∀o ∈ O.

mom

∀c ∈ Ca ∀a ∈ Acs

(25)

(26)

(27)

(28)

(29)

4.3 Flow Directions

Flow Directions deﬁne the sign of ﬂow values over the boundary nodes of a
network station. With regards to our MILP they are a further set of decision
variables. We avoid generating these decisions with our deep learning framework
as not all combinations of operation modes and ﬂow directions are feasible.
These variables thus exist as integer variables in Pz1
π , namely as a subset of z2,
see (1). They are few in number however due to the limited combinations after
the operation modes are ﬁxed.

4.4 Boundary Nodes and Slack

Boundary nodes, unlike inner nodes, have a prescribed ﬂow and pressure values
for all future time steps. For each boundary node v ∈ V b and t ∈ T , we have
v,t and σp−
σp+
v,t , which capture the positive and negative diﬀerence between the
prescribed and realised pressure. In addition to these pressure slack variables,
we have the inﬂow slack variables σd+
v,t which act in a similar manner
but for inﬂow. The relationships between the slack values, prescribed values,
and realised values can be modelled for each v ∈ V b and t ∈ T as:

v,t and σd−

ˆpv,t = pv,t − σp+
ˆdv,t = dv,t − σd+

v,t + σp−
v,t
v,t + σd−
v,t

∀v ∈ V b
∀v ∈ V b

(30)

(31)

Note that unlike the model from [24], we do not allow the inﬂow over a set of
boundary nodes to be freely distributed according to which group they belong
to. This is an important distinction, as each single node has a complete forecast.

4.5

Initial State

In addition to the forecast mentioned in subsection 4.4, we also start our op-
timisation problem with an initial state. This initial state contains complete

13

information of all discrete states and continuous values for all network elements
at t = 0.

4.6 Objective function

The objective of our formulation is to both minimise slack usage, and changes
in network operation. Speciﬁcally, it is a weighted sum of changes in the ac-
tive element modes, changes in the continuous active points of operation, and
the deviations from given pressure and ﬂow demands. For the exact objective
function we refer readers to [24].

5 Computational Experiments

In this section we propose an experimental design to determine the eﬀectiveness
of our neural network design approach. We outline how we generate synthetic
training data, and show the exact architecture and training method we use for
our neural network. Our ﬁnal test set consists of 15 weeks of real-world data
provided by our project partner OGE.

5.1 Data Generation

As mentioned previously, acquiring gas network data is notoriously diﬃcult
[28, 46]. Perhaps because of this diﬃculty, there exists no standard method for
generating valid states for a ﬁxed gas network. Below we outline our meth-
ods for generating synthetic transient gas instances for training purposes, i.e.
generating π ∈ Π and artiﬁcial z1 values. For our application of transient gas
instances, π is a tuple of a boundary forecast and an initial state.

5.1.1 Boundary Forecast Generation

We consider network stations as our gas network topology. They contain all
heavy machinery and at most only short segments of large scale transport
pipelines. As such, our gas networks cannot be used to store large amounts
of gas. We thus aim to generate balanced demand scenarios, with the require-
ment described as follows:

ˆdv,t = 0 ∀t ∈ T

(cid:88)

v∈V b

(32)

The distribution of gas demand scenarios is not well known. Hence we naively
assume a uniform distribution, and using the largest absolute ﬂow value found
over any node and time step in our real-world data, create an interval as follows:

14

Mq = max

v∈V b,t∈T

| ˆdv,t|

ˆdv,t ∈ [−1.05Mq, 1.05Mq]

(33)

In addition to the above, we require three MILP formulation speciﬁc require-
ments. The ﬁrst is that the absolute diﬀerence between the ﬂow values of a node
is not too large for any adjacent time steps. Secondly, the sign of the generated
ﬂow values must match the attribute of the boundary node, i.e., entry (+), exit
(-). Thirdly, the ﬂow values do not diﬀer too largely between boundary nodes
of the same fence group within the same time step. A fence group is denoted
by g ∈ G, and enforces the sign of all nodes in the group to be identical. These
constraints are described below:

| ˆdv,t − ˆdv,t−1| ≤ 200 ∀t ∈ T ,

v ∈ V b

sign( ˆdv,t) =

(cid:40)
1
−1

if

v ∈ V +

if

v ∈ V −

∀t ∈ T ,

v ∈ V b

(34)

| ˆdv1,t − ˆdv2,t| ≤ 200 ∀t ∈ T ,

v1, v2 ∈ g, g ∈ G, v1, v2 ∈ V b

To generate demand scenarios that satisfy constraints (32) and (33), we use the
method proposed in [39].
Its original purpose was to generate samples from
the Dirichlet distribution, but it can be used for a special case of the Dirichlet
distribution that is equivalent to a uniform distribution over a simplex in 3-
dimensions. Such a simplex is exactly described by (32) and (33) for each time
step. Hence we can apply it for all time-steps and reject all samples that do
not satisfy constraints (34). Note that this method is insuﬃcient for network
stations with more than three boundary nodes.

In addition to ﬂow demands, we require a pressure forecast for all boundary
nodes. Our only requirements here is that the pressures between adjacent time
steps for a single node not ﬂuctuate heavily and that the bounds are respected.
We create a bound on the range of pressure values by ﬁnding maximum and
minimum values over all nodes and time steps in our test set. We once again
assume our samples to be uniformly distributed and sample appropriately over
(35) with rejection of samples that do not respect constraint (36). Note that
many scenarios generated by this approach are unlikely to happen in practice,
as the pressure and ﬂow proﬁles may not match.

M +

p = max

v∈V b,t∈T

ˆpv,t

M −

p = min

v∈V b,t∈T

ˆpv,t

ˆpv,t ∈ [M −

p − 0.05(M +

p − M −

p ), M +

p + 0.05(M +

p − M −

p )]

15

(35)

|ˆpv,t − ˆpv,t−1| ≤ 5 ∀t ∈ T ,

v ∈ V b

(36)

Combining the two procedures from above yields the artiﬁcial forecast data
generation method described in Algorithm 1.

Algorithm 1: Boundary Value Forecast Generator

Result: A forecast of pressure and ﬂow values over the time horizon
ﬂow forecast = Sample simplex (32)(33) uniformly, rejecting via (34) ;
pressure forecast = Sample (35) uniformly, rejecting via (36) ;
return (ﬂow forecast, pressure forecast)

5.1.2 Operation Mode Sequence Generation
During oﬄine training, Dθ2 requires optimal solutions for a ﬁxed z1. In Algo-
rithm 2 we outline a naive yet eﬀective approach of generating reasonable z1
values, i.e., operation mode sequences:

Algorithm 2: Operation Mode Sequence Generator

Result: An Operation Mode per time step
operation modes = [ ] ;
for t = 1; t < |T |; t = t + 1 do

if t == 1 then

new operation mode = rand(O) ;

else if rand(0,1) ≥ 0.9 then

new operation mode = rand(O\ new operation mode) ;

end
operation modes.append(new operation mode) ;

end
return operation modes

5.1.3

Initial State Generation

Many coeﬃcients of Aπ are invariant due to static network topology. Many
others however are found by substituting multiple parameters into an equation
describing gas properties. This information is contained in the initial state, and
we generate them similar to boundary forecasts:

cstate ∈ {Temperature, Inﬂow Norm Density, Molar Mass,

Pseudo Critical Temperature, Pseudo Critical Pressure}

M +

c =

max
state∈initial states
c − 0.05(M +

cstate ∈ [M −

cstate

M −

c =

c − M −

c ), M +

c + 0.05(M +

min
state∈initial states
c − M −

c )]

cstate

(37)

(38)

We now have the tools to generate synthetic initial states, see Algorithm 3.

16

Algorithm 3 is designed to output varied and valid initial states w.r.t our MILP
formulation. However, it comes with some drawbacks. Firstly, the underlying
distribution of demand scenarios for both ﬂow and pressure are probably not
uniform nor conditionally independent. Moreover, the sampling range we use
is signiﬁcantly larger than that of our test set as we take single maximum and
minimum values over all nodes. Secondly, the choice of operation modes that
occur in reality is also not uniform.
In reality, some operation modes occur
with a much greater frequency than others. Our data is thus more dynamic
than reality, and likely to contain operation mode choices that do match the
demand scenarios. Finally, we rely on a MILP solver to generate new initial
states in our ﬁnal step. Hence we cannot rule out the possibility of a slight
bias. One example would be the case of a repeated scenario, which has multiple

17

optimal solutions, but the MILP solver always returns an identical solution.

Algorithm 3: Initial State Generator

Input: Desired time-step distance j ∈ [1, · · · , k]
Result: An initial state to the transient gas optimisation problem
ﬂow forecast, pressure forecast = Boundary Prognosis Generator() a ;
gas constants = Sample (38) uniformly ;
initial state = Select random state from real-world data ;
π = (ﬂow forecast, pressure forecast, gas constants, initial state) b ;
z1 = Operation Mode Sequence Generator() c ;
Pz1
( state 1, · · · , state k ) = Optimal solution states from solving Pz1
π ;
return state j

π = generate from π and z1;

aSee Algorithm 1
bNote that in general our π does not include gas constants. This is because the information
is generally encoded in initial state. Our gas constants in this context are randomly generated
however, and may not match the initial state. This does not aﬀect solving as these values are
simply taken as truths.
cSee Algorithm 2

Algorithm 4: Synthetic Gas Data Generator

Input: num states, num scenarios, time step diﬀerence
Result: num scenarios many gas instances and their optimal solutions
initial states = [] ;
for i = 0; i < num states; i = i + 1 do

initial states.append(Initial State Generator(time step diﬀerence))a;

end
forecasts = [] ;
for i = 0; i < num scenarios; i = i + 1 do

ﬂow forecast, pressure forecast = Boundary Prognosis Generator()b;
forecasts.append((ﬂow forecast, pressure forecast)) ;

end
solve data = [] ;
for i = 0; i < num scenarios; i = i + 1 do

z1 = Operation Mode Sequence Generator() c ;
initial state = Uniformly select from initial states ;
π = (forecasts[i], initial state) ;
Pz1
solution = Solve Pz1
π ;
solve data.append((z1, π, solution)) ;

π = Create MILP from π and z1;

end
return solve data

aSee Algorithm 3
bSee Algorithm 1
cSee Algorithm 2

18

In the case of initial state generation, we believe that further research needs to
be performed. Our method is eﬀective in the context of machine learning where
we aim for a diverse set of data, but it is naive and incapable of ensuring that
generated boundary scenarios are realistic.

5.1.4 Complete Transient Gas Instance Generation
To train Dθ2 and Gθ1 , we need both the transient gas transportation scenario,
and an optimal solution for it. Combining the generation methods for synthetic
data in subsections 5.1.1, 5.1.2, 5.1.3, and the solving process of the created
instances, we derive Algorithm 4.

5.2 Experimental Design

We generated our initial training and validation sets oﬄine. To do so we use
Algorithm 4 with inputs: num states = 104, num scenarios = 4 × 106, and
time step diﬀerence = 8. This initial training data is exclusively used for train-
ing Dθ2, and is split into a training set of size 3.2 × 106, a test set of 4 × 105,
and a validation set of 4 × 105.

The test set is checked against at every epoch, while the validation set is only
referred to at the end of the initial training. Following this initial training,
we begin to train N{θ1,θ2} as a whole, alternating between Gθ1 and Dθ2. The
exact algorithm is given in 5, which references functions provided in Appendix
A. For training, we used the Adam algorithm [27] as our descent method. The
associated parameters to this algorithm and a complete set of other training
parameters are listed in Table 4. In the case of a parameter being non-listed, the
default value was used. The intention behind our training method is to ensure
that N{θ1,θ2} receives no real-world data prior to its ﬁnal evaluation. With this
method we hope to show that synthetic data is suﬃcient for training purposes
and that N{θ1,θ2} successfully generalises to additional data sets. However, we
should note that Algorithm 3 does use real-world data as a starting point from
which to generate artiﬁcial data.

19

Algorithm 5: Neural Network Training

Input: Neural network N{θ1,θ2}, prelabelled data
Result: Trained neural network N{θ1,θ2}
set trainable(Dθ2);
set untrainable(Gθ1);
Discriminator Pretraining(Dθ2 , prelabelled data) a ;
softmax temperature = 0;
data = [];
for i = 0; i < num epochs do

set trainable(Gθ1);
set untrainable(Dθ2);
for i = 0; i < num generator epochs do

softmax temperature += 1;
set(Gθ1, softmax temperature);
loss = Generator Training(N{θ1,θ2}) b ;
if loss ≤ stopping loss generator then

break;

end

end
set trainable(Dθ2);
set untrainable(Gθ1 );
data = Prepare Discriminator Training Data(N{θ1,θ2}, data) c ;
mixed data = MixData(data, prelabelled data, num prelabelled);
training data, test data = split data(mixed data, ratio test);
optimizer = Adam(learning rate, weight decay) d ;
lr scheduler=ReduceLROnPlateau e(patience, factor);
dataloader = DataLoader(training data, batch size, shuﬄe=True);
for i = 0; i < num discriminator epochs do

Discriminator Training Loop(Dθ2, dataloader, optimizer) f ;
lr scheduler.step();
test loss = compute L1Loss(Dθ2 , test data);
if test loss ≤ stopping loss discriminator then

break;

end

end

end
return N{θ1,θ2}

aSee Algorithm 9
bSee Algorithm 11
cSee Algorithm 8
dSee [27] pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam.
eSee pytorch.org/docs/stable/optim.html#torch.optim.lr scheduler.ReduceLROnPlateau.
fSee Algorithm 10

We consider the solution of P ˆz1

π as a primal heuristic for the original problem

20

Pπ. Due to our usage of slack, i.e. the application of variables x2, any valid
π is a valid solution of Pπ. We aim to incorporate N{θ1,θ2} in a
solution for Pz1
global MIP context and do this by using a partial solution of P ˆz1
π as a warm-
start suggestion for Pπ. The partial solution consists of ˆz1, an additional set
of binary variables called the ﬂow directions, which are a subset of z2 in (1),
and pv,t ∀v ∈ V b, t ∈ T , which are a subset of x1 in (1). Note that partial
solutions are used as instances are numerically diﬃcult. In doing so, we hope to
generate valid solutions quickly, and speed up the global solution process. The
primal heuristic and warm-start algorithm can be seen in Algorithms 6 and 7
respectively.

Algorithm 6: Primal Heuristic

Input: Pπ
ˆz1 = Gθ1 (π) ;
P ˆz1
π = Create MILP from π and ˆz1;
solution = Solve P ˆz1
π ;
return solution ;
Result: Optimal solution of P ˆz1

π , primal solution of Pπ.

Algorithm 7: Warm Start Algorithm

Input: Pπ
primal solution = Primal Heuristic(Pπ) a ;
optimum = Solve Pπ with primal solution as a warm-start suggestion ;
Result: Optimal solution of Pπ

aSee Algorithm 6

For our experiments we used PyTorch 1.4.0 [34] as our ML modelling framework,
Pyomo v5.5.1 [21, 22] as our MILP modelling framework, and Gurobi v9.02 [19]
as our MILP solver. The MILP solver settings are available in Table 5 in
Appendix A. N{θ1,θ2} was trained on a machine running Ubuntu 18, with 384 GB
of RAM, composed of 2x Intel(R) Xeon(R) Gold 6132 running @ 2.60GHz, and
4x NVIDIA Tesla V100 GPU-NVTV100-16. The ﬁnal evaluation times were
performed on a cluster using 4 cores and 16 GB of RAM of a machine composed
of 2x Intel Xeon CPU E5-2680 running @ 2.70 GHz.
Our validation set for the ﬁnal evaluation of N{θ1,θ2} consists of 15 weeks of
live real-world data from our project partner OGE. Instances are on average 15
minutes apart for this period and total 9291.

All instances, both in training and test, contain 12 time steps (excluding the
initial state) with 30 minutes between each step. Additionally, we focus on
Station D from [24], and present only results for this station. The statistics for
Station D can be seen in Table 1, and its topology in Figure 4. Station D can
be thought of as a T intersection, and is of average complexity compared to the

21

stations presented in [24]. The station contains 6 boundary nodes, but they are
paired, such that for each pair only one can be active, i.e., have non-zero ﬂow.
Due to this, our sampling method in subsection 5.1.1 exists in 3-dimensions and
is uniform ∀t ∈ T .

Name

|V|

|A|

(cid:80)
a∈Api

La

|Api|

|Ca| ∀a ∈ Acs

|O|

|V b|

|Ava|

D

31

37

0.404 km

2, 6

56

3x2

11

Table 1: Overview of diﬀerent properties of station D.

Figure 4: Topology of Station D.

5.3 Exact Network Designs
As a large portion portion of our input data into both Gθ1 and Dθ2 is time-
expanded data, we originally believed that the ideal design would be a series
of LSTMs [25]. Preliminary results however showed that convolutional neural
networks (CNNs) were more eﬀective for our problem, in particular when using
Inception Blocks, see [42].
The exact block design used in N{θ1,θ2} can be seen in Figure 3, and the general
layout in Figure 1. For the complete network design we refer readers to Figure
14 and Table 6 in the Appendix.

22

6 Computational Results

We partition our results into three subsections. The ﬁrst focuses on the training
results of N{θ1,θ2}, the second on our data generation methods, while the third is
concerned with our results on the 15 weeks of real-world transient gas data. Note
that when training we scaled f (Pz1
π ) values by 500 to reduce the magnitude of
the losses. For visualisation purposes of comparing the performance of N{θ1,θ2}
and our data generation methods, we re-scaled all results.

6.1 Training Results

Figure 5: The loss per epoch of Dθ2 during the initial training of Algorithm 9.
The dashed lines show the performance of Dθ2 after N{θ1,θ2} has been completely
trained.

Figure 5 shows the training loss throughout the initial oﬄine training. We see
that Dθ2 learns how to accurately predict f (Pz1
π ) as the loss decreases. This is
a required result, as without a trained discriminator we cannot expect to train
a generator. Both the training and test loss converge to approximately 1000,
which is excellent considering the generated f (Pz1
π ) range well into the millions.
As visible by both the test loss and ﬁnal validation loss, we see Dθ2 generalises
to Pz1
π instances of our validation set that it has not seen. This generalisation
ability doesn’t translate perfectly to real-world data however. This is due to
the underlying distribution of real-world data and our generated data being
substantially diﬀerent. Despite this we believe that an L1 loss, in this case
simply the average distance between ˆf (Pz1
π ), of 10000 is still very
good. We discuss the issues of diﬀerent distributions in subsection 6.2.

π ) and f (Pz1

23

Figure 6: The loss per epoch of Dθ2 as it is trained using Algorithm 5

The loss during training using Algorithm 5 for Dθ2 is shown in Figure 6, and for
Gθ1 in Figure 7. The cyclical nature of the Dθ2 loss is caused by the re-training
of Gθ1 , which learns how to induce sub-optimal predictions from the then static
Dθ2 . These sub-optimal predictions are quickly re-learned, but highlight that
learning how to perfectly predict f (P ˆz1
π ) over all possibilities, potentially due to
the rounded nature of ˆz1, is unlikely without some error. Figure 7 (left) shows
the loss over time of Gθ1 as it is trained, with Figure 7 (right) displaying mag-
niﬁed losses for the ﬁnal epochs. We observe that Gθ1 quickly learns important
z1 decision values. We hypothesise that this quick descent is helped by ˆz1 that
are unlikely given our generation method in Algorithm 2. The loss increases
following this initial decrease in the case of Gθ1 , showing the ability of Dθ2 to
further improve. It should also be noted that signiﬁcant step-like decreases in
loss are absent in both (left) and (right) of Figure 7. Such steps would indicate
Gθ1 discovering new important z1 values (operation modes). The diversity of
produced operation modes however, see Figure 12, implies that early in training
a complete spanning set of operation modes is derived, and the usage of their
ratios is then learned and improved.

6.2 Data Generation Results
As an interlude between results from N{θ1,θ2}, we outline the performance of our
synthetic gas network data generation methods. Figure 8 (left) shows how our
generated ﬂow prognosis compares to that of historic real-world data. We see
that Nodes A, B, and C are not technically entry or exits, but over historical
data are dominated by a single orientation for each node. Speciﬁcally, Node
In addition to the
C is the general entry, and Nodes A / B are the exits.
general orientation, we see that each node has signiﬁcantly diﬀerent ranges and
distributions. These observations highlight the simplicity of our data generation
methods, as we see near identical distributions for all nodes over the artiﬁcial

24

Figure 7: (Left) The loss per epoch of Gθ1 as it is trained using Algorithm 5.
On the left the loss over all epochs is shown. (Right) A magniﬁed view of the
loss starting from epoch 20.

data. We believe this calls for further research in prognosis generation methods.
Figure 8 (right) shows our pressure prognosis compared to that of historic values.
Unlike historic ﬂow values, we observe little diﬀerence between historic pressure
values of diﬀerent nodes. This is supported by the optimal choices z∗
1 over the
historic data, see Figure 12, as in a large amount of cases compression is not
needed and the network station is in bypass. Note that each corresponding
entry (+) and exit (-) have identical pressure distributions due to the way they
are constructed.

1

A further comparison of how our generated data compares to historic data can
be seen in Figure 9. Here one can see the distribution of ˆf (P ˆz1
π ) and f (P ˆz1
π )
for the generated validation set, and ˆf (Pz∗
π ) and f (Pπ) for the real-world data.
As expected, the distributions are diﬀerent depending on whether the data
is artiﬁcial or not. Our data generation was intended to be simplistic, and as
independent as possible from the historic data. As such, the average scenario has
optimal solution larger than that of any real-world data point. The performance
of Dθ2 is again clearly visible here, with ˆf (P ˆz1
π ) being near identical
over the artiﬁcial data, keeping in mind that these data points were never used
in training. We see that this ability to generalise is relatively much worse on
real-world data, mainly due to the the lower values of f (Pπ) over this data.
Figure 9 (right) shows the results with log-scale axes to better highlight this
disparity. It should be noted that the real-world instances with larger f (Pπ) are
predicted quite well, and all real-world instances have an L1 distance between
ˆf (Pz∗

π ) and f (Pπ) that is small in terms of absolute diﬀerences.

π ) and f (P ˆz1

1

25

Figure 8: Comparison of generated ﬂow (Left) / pressure (Right) value distri-
butions per node vs. the distribution seen in real-world data.

6.3 Real-World Results
We now present results of our fully trained N{θ1,θ2} applied to the 15 weeks
of real-world data. Note that we had to remove 651 instances from our 9291
instances, as the warm-start resulted in an optimal solution value further away
than the optimality tolerances we set. These instances have been kept in the
graphics, but are marked and conclusions will not be drawn from them. We
believe the problems with reproducibility are caused by the numeric diﬃculties
in managing the pipe equality constraints.

π ) and f (Pπ). In a similar manner to Dθ2,
Figure 10 shows the comparison of f (P ˆz1
we see that Gθ1 struggles with instances where f (Pπ) is small. This is visible in
π ) values much larger than f (Pπ) for like π.
the bottom left, where we see f (P ˆz1
This comes as little surprise given the struggle of Dθ2 with small f (Pπ) values.
Drawing conclusions becomes more complicated for instances with larger f (Pπ)
values, because the majority hit the time limit. We can clearly see however, the
value of our primal heuristic. There are many cases, those below the line f (P ˆz1
π )
= f (Pπ), where our primal heuristic retrieves a better solution than the MILP
solver does in one hour. Additionally, we see that no unsolved point above
the line is very far from the line, showing that our primal heuristic produced a
comparable, sometimes equivalent solution in a much shorter time frame. For
a comparison of solve-times, see Table 2.

26

Figure 9: ˆf (P ˆz1
to f (P ˆz1

π ) for the validation set, and ˆf (Pz∗

1

π ) for real-world data, compared

π ) and f (Pπ) respectively. Linear scale (Left) and log-scale (Right).

Mean Median

STD Min

Max

N{θ1,θ2} Inference Time (s)
Warmstarted Pπ Time (s)
Pπ Time (s)
π + Warmstarted Pπ Time (s)
P ˆz1
P ˆz1
π Time (s)

0.009
100.830
147.893
103.329
2.499

0.008
9.380
24.380
12.130
1.380

0.001
421.084
463.279
424.543
12.714

0.008
0.130
3.600
0.190
0.060

0.017
3600.770
3601.280
3726.110
889.380

Table 2: Solve time statistics for diﬀerent solving strategies.

π ) compared to f (P ˆz1
π ) values slightly larger than f (P ˆz1

Figure 11 shows the performance of the predictions ˆf (P ˆz1
π ).
Interestingly, Dθ2 generally predicts ˆf (P ˆz1
π ).
We expect this for the smaller valued instances, as we know that Dθ2 struggles
with f (P ˆz1
π ) instances near 0, but the trend is evident for larger valued instance
too. The closeness of the data points to the line ˆf (P ˆz1
π ) show that Dθ2
can adequately predict ˆz1 solutions from Gθ1 despite the change in data sets.
Figure 10 showed that Gθ1 successfully generalised to a new data set, albeit with
diﬃculties around instances with f (Pπ) valued near 0. From Figures 10 and 11,
we can see that the entire N{θ1,θ2} generalises to unseen real-world instances,
despite some generalisation loss.
We now compare the operation modes ˆz1, which are generated by Gθ1, and the
z∗
1 , which are produced by our MILP solver. To do so we use the following
naming convention: We name the three pairs of boundary nodes N (north), S
(south), and W (west). Using W NS C 2 as an example, we know that ﬂow
comes from W, and goes to N and S. The C in the name stands for active

π ) = f (P ˆz1

27

Figure 10: A comparison of f (P ˆz1

π ) and f (Pπ) for all real-world data instances.

1
S
N

W
N

NW NS 1

884

NS SW 2

N SW C 1

NS NSW 1

W NS C 1

NS SW 1

NW S 2

NS SW 3

W NS C 2

NW S 1

Other

48

0

41

0

0

4

6

28

30

0

2
W
S
S
N

22

102

27

29

0

0

0

7

0

11

1

1
C

W
S
N

1
W
S
N
S
N

0

1

9529

40298

65

11008

26509

0

76

0

5220

0

0

0

0

0

0

0

0

1
C
S
N

W

31

0

0

0

0

0

0

0

1
W
S
S
N

37

114

4

28

0

1

0

7

0

12

0

2
S
W
N

2436

630

0

557

0

0

2

108

0

315

0

3
W
S
S
N

4

24

2

9

0

0

0

1

0

2

0

2
C
S
N

W

1
S
W
N

24

397

0

0

0

0

0

0

0

93

0

0

51

0

49

0

0

1

4

0

30

0

r
e
h
t
O

82

13

55

15

0

0

1

5

0

6

1

0

136

2880

78

0

0

Table 3: Operation Mode Correlation Matrix between ˆz1 and z∗
1 .

28

Figure 11: A comparison of ˆf (P ˆz1

π ) and f (P ˆz1

π ) for all real-world data instances.

compression, and the ﬁnal index is to diﬀerentiate between duplicate names.
As seen in Figure 12, which plots the frequency of speciﬁc z1 if they occurred
more than 50 times, a single choice dominates z∗
1 . This is interesting, because we
expected there to be a-lot of symmetry between z1, with the MILP solver select-
ing symmetric solutions with equal probability. For instance, take W NS C 1
and take W NS C 2. N{θ1,θ2} only ever predicts W NS C 2, however with half
the frequency the MILP solver selects each of them. This indicates that from
the MILP’s point of view they are symmetric, and either can be chosen, while
N{θ1,θ2} has recognised this and converged to a single choice. We can support
this by analysing the data, where the diﬀerence in W NS C 1 and W NS C 2
is which compressor machine is used, with both machines being identical. This
duplicate choice apparently does not exist in bypass modes however, where the
uniqueness of z1, determined by valve states, results in diﬀerent f (Pz1
π ) values.
It is observable then that for the majority of instances NS NSW 1 is the opti-
mal choice, and that N{θ1,θ2} has failed to identify its central importance. We
believe this is due to the training method, where over generalisation to a sin-
gle choice is strongly punished. For a comprehensive overview of the selection
of operation modes and the correlation between ˆz1 and z∗
1 , we refer interested
readers to Table 3.
As discussed above, N{θ1,θ2} cannot reliably produce z∗
1 . Nevertheless, it pro-
duces near-optimal ˆz1 suggestions, which are still useful in a warm-start context,
see Algorithm 7. The results of our warm-start algorithm are displayed in Fig-
ure 13. Our warm-start suggestion was successful 72% of the time, and the
algorithm resulted in an average speed up of 60.5%. We use the shifted geomet-

29

Figure 12: Frequency of operation mode choice by Gθ1 compared to MILP solver
for all real-world instances. (Left) Linear scale, and (Right) log scale.

ric mean with a shift of 1 for this measurement to avoid distortion by relative
variations of the smaller valued instances. Especially surprising is that some
instances that were previously unsolvable within the time-limit were easily solv-
able given the warm-start suggestion.
In addition, many of the solvable but
complicated instances are also solved near instantly with the warm-start sug-
gestion. As such, we have created an eﬀective primal heuristic that is both quick
to run and beneﬁcial in the context of locating a globally optimal solution.

7 Conclusion

In this paper, we presented a dual neural network design for generating decisions
in a MILP. This design is trained without ever solving the MILP with unﬁxed
decision variables. The neural network is both used as a primal heuristic and
used to warm-start the MILP solver for the original problem. We proved the
usefulness of our design on the transient gas transportation problem. While
doing so we created methods for generating synthetic transient gas data for
training purposes, reserving an unseen 9291 real-world instances for validation
purposes. Despite some generalisation loss, our trained neural network results
in a primal heuristic that takes on average 2.5s to run, and results in a 60.5%
decrease in global optimal solution time when used in a warm-start context.

While our approach is an important step forward in neural network design and
ML’s application to gas transport, we believe that there exists four primary
directions for future research. The ﬁrst of which is to convert our approach
into more traditional reinforcement learning, and then utilise policy gradient
approaches, see [44]. The major hurdle to this approach is that much of the
computation would be shifted online, requiring many more calls to solve the
induced MILPs. This could be oﬀset however, by using our technique to ini-

30

Figure 13: The combined running time of solving P ˆz1
started Pπ, compared to solving Pπ directly.

π , and solving a warm-

tialise the weights for such an approach, thereby avoiding early stage training
diﬃculties with policy gradient approaches. The second is focused on the recent
improvements in Graph Neural Networks, see [16]. Their ability to generalise
to diﬀerent input sizes would permit the creation of a single NN over multiple
network stations or gas network topologies. Thirdly, there exists a large gap in
the literature w.r.t data generation for transient gas networks. Improved meth-
ods are needed, which are scalable and result in real-world like data. Finally,
although we focused on the transient gas transportation problem, our approach
can be generalised to arbitrary problem classes.

Acknowledgements

The work for this article has been conducted in the Research Campus MODAL
funded by the German Federal Ministry of Education and Research (BMBF)
(fund numbers 05M14ZAM, 05M20ZBM), and was supported by the German
Federal Ministry of Economic Aﬀairs and Energy (BMWi) through the project
UNSEEN (fund no 03EI1004D).

31

References

[1] T. Achterberg. Constraint integer programming. PhD thesis, Technische

Universit¨at Berlin, 2007.

[2] E. Balas. The Convex Hull of a Disjunctive Set. In Disjunctive Program-

ming, pages 17–39. Springer International Publishing, Cham, 2018.

[3] R. Baltean-Lugojan, P. Bonami, R. Misener, and A. Tramontani. Scoring
positive semideﬁnite cutting planes for quadratic optimization via trained
neural networks. optimization-online preprint 2018/11/6943, 2019.

[4] J. Beli¨en, E. Demeulemeester, and B. Cardoen. A decision support system
for cyclic master surgery scheduling with multiple objectives. Journal of
scheduling, 12(2):147, 2009.

[5] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combi-
natorial optimization: a methodological tour d’horizon. arXiv preprint
arXiv:1811.06128, 2018.

[6] D. Bertsimas and B. Stellato. The voice of optimization. arXiv preprint

arXiv:1812.09991, 2018.

[7] D. Bertsimas and B. Stellato. Online mixed-integer optimization in mil-

liseconds. arXiv preprint arXiv:1907.02206, 2019.

[8] R. Burlacu, H. Egger, M. Groß, A. Martin, M. E. Pfetsch, L. Schewe, M. Sir-
vent, and M. Skutella. Maximizing the storage capacity of gas networks:
a global minlp approach. Optimization and Engineering, 20(2):543–573,
2019.

[9] Z. Chen, Y. Zhong, X. Ge, and Y. Ma. An actor-critic-based uav-bss deploy-
ment method for dynamic environments. arXiv preprint arXiv:2002.00831,
2020.

[10] S. Dempe. Foundations of bilevel programming. Springer Science & Business

Media, 2002.

[11] J.-Y. Ding, C. Zhang, L. Shen, S. Li, B. Wang, Y. Xu, and L. Song.
Optimal solution predictions for mixed integer programs. arXiv preprint
arXiv:1906.09575, 2019.

[12] M. Etheve, Z. Al`es, C. Bissuel, O. Juan, and S. Kedad-Sidhoum. Rein-
forcement learning for variable selection in a branch and bound algorithm.
arXiv preprint arXiv:2005.10026, 2020.

[13] J. Fang, Q. Zeng, X. Ai, Z. Chen, and J. Wen. Dynamic optimal energy
IEEE

ﬂow in the integrated natural gas and electrical power systems.
Transactions on Sustainable Energy, 9(1):188–198, 2017.

[14] A. Ferber, B. Wilder, B. Dilina, and M. Tambe. Mipaal: Mixed integer

program as a layer. arXiv preprint arXiv:1907.05912, 2019.

32

[15] M. Fischetti and A. Lodi. Local branching. Mathematical programming,

98(1-3):23–47, 2003.

[16] M. Gasse, D. Ch´etelat, N. Ferroni, L. Charlin, and A. Lodi. Exact combina-
torial optimization with graph convolutional neural networks. In Advances
in Neural Information Processing Systems, pages 15554–15566, 2019.

[17] I. Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv

preprint arXiv:1701.00160, 2016.

[18] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press,

2016.

[19] L. Gurobi Optimization. Gurobi optimizer reference manual, 2020.

[20] H. Hanachi, C. Mechefske, J. Liu, A. Banerjee, and Y. Chen. Performance-
based gas turbine health monitoring, diagnostics, and prognostics: A sur-
vey. IEEE Transactions on Reliability, 67(3):1340–1363, 2018.

[21] W. E. Hart, C. D. Laird, J.-P. Watson, D. L. Woodruﬀ, G. A. Hackebeil,
B. L. Nicholson, and J. D. Siirola. Pyomo–optimization modeling in python,
volume 67. Springer Science & Business Media, second edition, 2017.

[22] W. E. Hart, J.-P. Watson, and D. L. Woodruﬀ. Pyomo: modeling and solv-
ing mathematical programs in python. Mathematical Programming Com-
putation, 3(3):219–260, 2011.

[23] F. Hennings. Beneﬁts and limitations of simpliﬁed transient gas ﬂow formu-
lations. In Operations Research Proceedings 2017, pages 231–237. Springer,
2018.

[24] F. Hennings, L. Anderson, K. Hoppmann-Baum, M. Turner, and T. Koch.
Controlling transient gas ﬂow in real-world pipeline intersection areas. Op-
timization and Engineering, pages 1–48, 2020.

[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural com-

putation, 9(8):1735–1780, 1997.

[26] K. Hoppmann, F. Hennings, R. Lenz, U. Gotzes, N. Heinecke, K. Spreck-
elsen, and T. Koch. Optimal operation of transient gas transport networks.
Technical report, Technical Report 19-23, ZIB, Takustr. 7, 14195 Berlin,
2019.

[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[28] F. Kunz, M. Kendziorski, W.-P. Schill, J. Weibezahn, J. Zepter, C. R. von
Hirschhausen, P. Hauser, M. Zech, D. M¨ost, S. Heidari, et al. Electricity,
heat, and gas sector data for modeling the german system. Technical report,
DIW Data Documentation, 2017.

33

[29] D. Masti and A. Bemporad. Learning binary warm starts for multiparamet-
ric mixed-integer quadratic programming. In 2019 18th European Control
Conference (ECC), pages 1494–1499. IEEE, 2019.

[30] M. MohamadiBaghmolaei, M. Mahmoudy, D. Jafari, R. MohamadiBagh-
molaei, and F. Tabkhi. Assessing and optimization of pipeline system per-
formance using intelligent systems. Journal of Natural Gas Science and
Engineering, 18:64–76, 2014.

[31] S. Moritz. A mixed integer approach for the transient case of gas network

optimization. PhD thesis, Technische Universit¨at, 2007.

[32] V. Nair, S. Bartunov, F. Gimeno, I. von Glehn, P. Lichocki, I. Lobov,
B. O’Donoghue, N. Sonnerat, C. Tjandraatmadja, P. Wang, R. Addanki,
T. Hapuarachchi, T. Keck, J. Keeling, P. Kohli, I. Ktena, Y. Li, O. Vinyals,
and Y. Zwols. Solving mixed integer programs using neural networks, 2020.

[33] A. J. Osiadacz. Diﬀerent Transient Flow Models - Limitations, Advantages,
In PSIG-9606. Pipeline Simulation Interest Group,

And Disadvantages.
1996.

[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An impera-
tive style, high-performance deep learning library. In Advances in neural
information processing systems, pages 8026–8037, 2019.

[35] M. Petkovic, Y. Chen, I. Gamrath, U. Gotzes, N. S. Hadjidimitriou, J. Zit-
tel, and T. Koch. A hybrid approach for high precision prediction of gas
ﬂows. Technical Report 19-26, ZIB, Takustr. 7, 14195 Berlin, 2019.

[36] D. Pfau and O. Vinyals. Connecting generative adversarial networks and

actor-critic methods. arXiv preprint arXiv:1610.01945, 2016.

[37] A. Pourfard, H. Moetamedzadeh, R. Madoliat, and E. Khanmirza. Design
of a neural network based predictive controller for natural gas pipelines in
transient state. Journal of Natural Gas Science and Engineering, 62:275–
293, 2019.

[38] R. Z. R´ıos-Mercado and C. Borraz-S´anchez. Optimization problems in nat-
ural gas transportation systems: A state-of-the-art review. Applied Energy,
147:536–555, 2015.

[39] D. B. Rubin. The bayesian bootstrap. The annals of statistics, pages

130–134, 1981.

[40] R. Ruiz, C. Maroto, and J. Alcaraz. A decision support system for a
real vehicle routing problem. European Journal of Operational Research,
153(3):593–606, 2004.

[41] L. N. Smith. Cyclical learning rates for training neural networks, 2017.

34

[42] C. Szegedy, S. Ioﬀe, V. Vanhoucke, and A. A. Alemi.

inception-resnet and the impact of residual connections on learning.
Thirty-ﬁrst AAAI conference on artiﬁcial intelligence, 2017.

Inception-v4,
In

[43] Y. Tang, S. Agrawal, and Y. Faenza. Reinforcement learning for integer
programming: Learning to cut. arXiv preprint arXiv:1906.04859, 2019.

[44] P. S. Thomas and E. Brunskill. Policy gradient methods for reinforce-
ment learning with function approximation and action-dependent baselines.
arXiv preprint arXiv:1706.06643, 2017.

[45] E. Wong and J. Z. Kolter. Provable defenses against adversarial ex-
arXiv preprint

amples via the convex outer adversarial polytope.
arXiv:1711.00851, 2017.

[46] I. Yueksel Erguen, J. Zittel, Y. Wang, F. Hennings, and T. Koch. Lessons
learned from gas network data preprocessing. Technical report, Technical
Report 20-13, ZIB, Takustr. 7, 14195 Berlin, 2020.

[47] G. Zarpellon, J. Jo, A. Lodi, and Y. Bengio. Parameterizing branch-
arXiv preprint

and-bound search trees to learn branching policies.
arXiv:2002.05120, 2020.

A Appendix

Algorithm 8: Prepare Discriminator Training Data

Input: Neural network N{θ1,θ2}, labelled data
Result: Data for training Dθ2
new labelled data = [] ;
for i = 0; i < num data new do

initial state = Uniformly select from generated oﬄine data ;
ﬂow forecast, pressure forecast = Boundary Prognosis Generator()a;
π = (ﬂow forecast, pressure forecast, initial state);
ˆz1, ˆf (P ˆz1
π ) = solve P ˆz1
f (P ˆz1
π ;
new labelled data.append(π, ˆz1, f (P ˆz1

π ) = N{θ1,θ2} (π);

π )) ;

end
return concatenate(labelled data[-num data old:], new labelled data)

aSee Algorithm 1

35

Algorithm 9: Discriminator Pretraining

Input: Discriminator Dθ2, data
optimizer = Adam(learning rate, weight decay);
dataloader = DataLoader(data, batch size, shuﬄe=True);
lr scheduler=ReduceLROnPlateau();
for i = 0; i < num epochs do

Discriminator Training Loop(Dθ2 , dataloader, optimizer) 1;
lr scheduler.step();

end

Algorithm 10: Discriminator Training Loop

Input: Discriminator Dθ2, dataloader, optimizer
for batch in dataloader do
optimizer.zero grad();
ˆf (Pz1
π ) = Dθ2 (batch);
loss = L1Loss( ˆf (Pz1
loss.backward();
optimizer.step();

π ), f (Pz1

π ));

end

36

Algorithm 11: Generator Training
Input: Neural network N{θ1,θ2}
Result: Average loss in training
optimizer = Adam();
lr scheduler=cyclicLRa(max lr, base lr, step size up);
data = [] ;
for i = 0; i < num scenarios do

initial state = Uniformly select from generated oﬄine data ;
ﬂow forecast, pressure forecast = Boundary Prognosis Generator()b;
π = (ﬂow forecast, pressure forecast, initial state);
data.append(π) ;

end
dataloader = DataLoader(data, batch size, shuﬄe=True);
for batch in dataloader do
optimizer.zero grad();
losses = [];
ˆz1, ˆf (P ˆz1
loss = L1Loss( ˆf (P ˆz1
loss.backward();
optimizer.step();
lr scheduler.step();
losses.append(loss);

π ) = N{θ1,θ2} (batch);

π ), 0);

end
Result: mean(losses)

aIntroduced by Smith in [41], see also

pytorch.org/docs/stable/optim.html#torch.optim.lr scheduler.CyclicLR.

bSee Algorithm 1

37

Figure 14: Neural Network Architecture

38

Linear & ReLu x 2Linear & ReLu x 2Unfold over timeConstantsBoundary Flows & PressuresInceptionInceptionResInception x 2ResInceptionResInception x 6Convolution Kernel 1Normalize and Multiply by TemperatureSoftmax Operation ModesLinear & ReLu x 3InceptionInceptionResInceptionResInceptionCombine to 2DFlattenResInception x 8Linear & ReLu x 3Linear            output size 1SoftplusObjective ValueInitial Operation ModeLinear & ReLu x 2ResInception     2D -> 1D GeneratorDiscriminatorParameter

Method

Algorithm 9
batch size
Algorithm 9
num epochs
Algorithm 9 / Adam
learning rate
Algorithm 9 / Adam
weight decay
Algorithm 11
batch size
Algorithm 11 / CyclicLR
max lr
Algorithm 11 / CyclicLR
base lr
Algorithm 11 / CyclicLR
step size up
Algorithm 11
num scenarios
Algorithm 8
num data new
Algorithm 8
num data old
Algorithm 5
num epochs
Algorithm 5
num generator epochs
Algorithm 5
num discriminator epochs
stopping loss discriminator Algorithm 5
Algorithm 5
stopping loss generator
Algorithm 5 / mix in prelabelled data
num prelabelled
ratio test
Algorithm 5 / split data
Algorithm 5 / Adam
learning rate
weight decay
Algorithm 5 / Adam
Algorithm 5 / ReduceLROnPlateau
patience
Algorithm 5 / ReduceLROnPlateau
factor

Value

2048
500
0.005
5e-06
2048
0.0005
5e-06
10000
3200000
2048
8192
10
25
25
3 * 1022.51
0.9 * 121848.27 2
8192
0.1
0.001
5e-06
2
0.5

1 1022.5 was the test loss after initial discriminator training.
2 121848.27 represents the average ˆf (P ˆz1

π ) value over our artiﬁcial data.

Table 4: Parameters for training.

Parameter

Value

TimeLimit
FeasibilityTol
MIPGap
MIPGapAbs
NumericFocus

3600 (s)
1e-6
1e-4
1e-2
3

Table 5: Parameters for MIP solving.

39

Parameters

Inception Blocks

Small Inception Blocks

Neural Network
Generator
Discriminator
Inception Block
Small Inception Block

1,701,505
1,165,576
535,929
87,296
27,936

13
13
0
-
-

12
0
12
-
-

Table 6: Number of parameters in the neural network and submodules.

40

