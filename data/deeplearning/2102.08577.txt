2
2
0
2

r
p
A
6
2

]

G
L
.
s
c
[

2
v
7
7
5
8
0
.
2
0
1
2
:
v
i
X
r
a

DO-GAN: A Double Oracle Framework for Generative Adversarial Networks

Aye Phyu Phyu Aung1,2*, Xinrun Wang1*†, Runsheng Yu1*, Bo An1†
Senthilnath Jayavelu2, Xiaoli Li1,2,3†
1School of Computer Science and Engineering, Nanyang Technological University, Singapore
2Institute for Infocomm Research, A*STAR, Singapore
3A*STAR Centre for Frontier AI Research, Singapore
{ayep0001,xinrun.wang, boan}@ntu.edu.sg, runshengyu@gmail.com,

{j_senthilnath, xlli}@i2r.a-star.edu.sg

Abstract

In this paper, we propose a new approach to train Gen-
erative Adversarial Networks (GANs) where we deploy a
double-oracle framework using the generator and discrim-
inator oracles. GAN is essentially a two-player zero-sum
game between the generator and the discriminator. Train-
ing GANs is challenging as a pure Nash equilibrium may
not exist and even ﬁnding the mixed Nash equilibrium is
difﬁcult as GANs have a large-scale strategy space. In DO-
GAN, we extend the double oracle framework to GANs. We
ﬁrst generalize the players’ strategies as the trained models
of generator and discriminator from the best response or-
acles. We then compute the meta-strategies using a linear
program. For scalability of the framework where multi-
ple generators and discriminator best responses are stored
in the memory, we propose two solutions: 1) pruning the
weakly-dominated players’ strategies to keep the oracles
from becoming intractable; 2) applying continual learning
to retain the previous knowledge of the networks. We apply
our framework to established GAN architectures such as
vanilla GAN, Deep Convolutional GAN, Spectral Normaliza-
tion GAN and Stacked GAN. Finally, we conduct experiments
on MNIST, CIFAR-10 and CelebA datasets and show that
DO-GAN variants have signiﬁcant improvements in both
subjective qualitative evaluation and quantitative metrics,
compared with their respective GAN architectures.

1. Introduction

Generative Adversarial Networks (GANs) [8] have been
applied in various domains such as image and video gen-
eration, text-to-image synthesis and equipment condition
monitoring [18, 29–31]. Various architectures are proposed

*Equal contribution
†Corresponding author

to generate more realistic samples [23, 27, 28] as well as
regularization techniques [1, 25]. From the game-theoretic
perspective, GANs can be viewed as a two-player game
where the generator samples the data and the discriminator
classiﬁes the data as real or generated. They are alternately
trained to maximize their respective utilities till convergence
corresponding to a pure Nash Equilibrium (NE).

Figure 1. Training images with ﬁxed noise for SGAN and DO-
SGAN/P (pruning) until termination. Both during training and after
convergence, DO-SGAN/P can generate better quality images with
FID score of 6.32 against SGAN’s FID score of 6.98.

However, pure NE cannot be reliably reached by exist-
ing algorithms as pure NE may not exist [7, 21]. This also
leads to unstable training in GANs depending on the data
and the hyperparameters. Therefore, mixed NE is a more
suitable solution concept [11]. Several recent works propose
mixture architectures with multiple generators and discrimi-
nators that consider mixed NE such as MIX+GAN [2] and
MGAN [10] but they cannot guarantee to converge to mixed
NE. Mirror-GAN [11] computes the mixed NE by sampling
over the inﬁnite-dimensional strategy space and proposes
provably convergent proximal methods. However, the sam-
pling approach may not be efﬁcient as mixed NE may only
have a few strategies in the support set.

 
 
 
 
 
 
Double Oracle (DO) algorithm [20] is a powerful frame-
work to compute mixed NE in large-scale games. The algo-
rithm starts with a restricted game that is initialized with a
small set of actions and solves it to get the NE strategies of
the restricted game. The algorithm then computes players’
best-responses using oracles to the NE strategies and add
them into the restricted game for the next iteration. DO
framework has been applied in various disciplines [4, 13], as
well as Multi-agent Reinforcement Learning (MARL) [15].
Inspired by successful applications of DO framework, we,
for the ﬁrst time, propose a Double Oracle Framework for
Generative Adversarial Networks (DO-GAN). This paper
presents four key contributions. First, we treat the generator
and the discriminator as players and obtain the best responses
from their oracles and add the utilities to a meta-matrix. Sec-
ond, we propose a linear program to obtain the probability
distributions of the players’ pure strategies (meta-strategies)
for the respective oracles. The linear program computes an
exact mixed NE of the meta-matrix game in polynomial time.
Third, since multiple generators and discriminator from the
best responses oracles are stored in the memory, the algo-
rithm may be memory-inefﬁcient for problems to train GAN
with large-scaled real-world datasets. Thus, we propose two
solutions for the scalable double oracle framework: 1) a
pruning method for reducing the support set of best response
strategies to prevent the oracles from becoming intractable
as there is a risk of the meta-matrix growing very large with
each iteration of oracle training; 2) applying continual learn-
ing to retain the previous knowledge of the networks for the
best responses from the generator and discriminator oracles
in the multi-task learning setup. We also address the prob-
lems in continual learning such as catastrophic forgetting.
Finally, we provide comprehensive evaluation on the perfor-
mance of DO-GAN with different GAN architectures using
both synthetic and real-world datasets. Experiment results
show that DO-GAN variants have signiﬁcant improvements
in terms of both subjective qualitative evaluation and quanti-
tative metrics such as inception score and FID score.

2. Related Works

In this section, we brieﬂy introduce existing GAN archi-
tectures, double oracle algorithm and its applications such
as policy-state response oracles that are related to our work.

GAN Architectures. Various GAN architectures have
been proposed to improve the performance of GANs.
Deep Convolutional GAN (DCGAN) [28] replaces fully-
connected layers in the generator and the discriminator
with deconvolution layer of Convolutional Neural Networks
(CNN). Weight normalization techniques such as Spectral
Normalization GAN (SNGAN) [24] stabilize the training of
the discriminator and reduce the intensive hyperparameters
tuning. There are also multi-model architectures such as

Stacked Generative Adversarial Networks (SGAN) [12] that
consist of a top-down stack of generators and a bottom-up
discriminator network. Each generator is trained to generate
lower-level representations conditioned on higher-level rep-
resentations that can fool the corresponding representation
discriminator. Training GANs is very hard and unstable as
pure NE for GANs might not exist and cannot be reliably
reached by the existing approaches [21]. Considering mixed
NE, MIX+GAN [2] maintains a mixture of generators and
discriminators with the same network architecture but have
their own trainable parameters. However, training a mixture
of networks without parameter sharing makes the algorithm
computationally expensive. Mixture Generative Adversarial
Nets (MGAN) [10] propose to capture diverse data modes
by formulating GAN as a game between a classiﬁer, a dis-
criminator and multiple generators with parameter sharing.
However, MIX+GAN and MGAN cannot converge to mixed
NE. Mirror-GAN [11] ﬁnds the mixed NE by sampling over
the inﬁnite-dimensional strategy space and proposes prov-
ably convergent proximal methods. The sampling approach
may be inefﬁcient to compute mixed NE as the mixed NE
may only have a few strategies with positive probabilities in
the inﬁnite strategy space.

Double Oracle Algorithm. Double Oracle (DO) algo-
rithm starts with a small restricted game between two players
and solves it to get the players’ strategies at Nash Equi-
librium (NE) of the restricted game. The algorithm then
exploits the respective best response oracles for additional
strategies of the players. The DO algorithm terminates when
the best response utilities are not higher than the equilib-
rium utility of the current restricted game, hence, ﬁnding
the NE of the game without enumerating the entire strategy
space. Moreover, in two-player zero-sum games, DO con-
verges to a min-max equilibrium [20]. DO framework is
used to solve large-scale normal-form and extensive-form
games such as security games [13, 38], poker games [40]
and search games [5]. DO framework is also used in MARL
settings [15, 26]. Policy-Space Response Oracles (PSRO)
generalize the double oracle algorithm in a multi-agent re-
inforcement learning setting [15]. PSRO treats the players’
policies as the best responses from the agents’ oracles, builds
the meta-matrix game and computes the mixed NE but it
uses Projected Replicator Dynamics (PRD) that updates the
changes in the probability of each player’s policy at each
iteration. Since PRD needs to simulate the update for several
iterations, the use of PRD takes a longer time to compute the
meta-strategies and does not guarantee to compute an exact
NE of the meta-matrix game. However, in DO-GAN, we can
use a linear program to compute the players’ meta-strategies
in polynomial time since GAN is a two-player zero-sum
game [35]. We present the corresponding terminologies
between GAN and game theory in Appendix A.

Continual Learning and Catastrophic Forgetting. Con-
tinual learning in GANs has been ongoing research to com-
bine a network’s knowledge through time or knowledge of
multiple networks to a single network. Continual Learning in
GANs [36] employed Elastic Weight Consolidation (EWC)
to remedy the catastrophic forgetting in GANs continual
training. MGAN [10] and GMAN [6] have employed con-
tinual learning to multiple generators and multiple discrimi-
nators respectively. Our work is closely related to Bayesian
GAN [33] which assigns a posterior over the multiple net-
works of generator and discriminator. However, we cannot
directly adapt the work as it only assigns a distribution to
multiple generators and discriminators with a Bayesian for-
mula without a single continual network while we assign
the distributions to the generator/discriminator tasks of a
continual learning architecture by solving a meta-game.

3. Preliminaries

In this section, we mathematically explain the preliminary

works to effectively our DO-GAN approach.

3.1. Generative Adversarial Networks

Generative Adversarial Networks (GANs) [8] have be-
come one of the dominant methods for ﬁtting generative
models to complicated real-life data. GANs are deep neural
net architectures comprised of two neural networks trained
in an adversarial manner to generate data that resembles
a distribution. The ﬁrst neural network, a generator G, is
given some random distribution pz(z) on the input noise
z and a real data distribution pdata(x) on training data x.
The generator is supposed to generate as close as possible to
pdata(x). The second neural network, a discriminator D, is
to discriminate between two different classes of data (real or
fake) from the generator.

Let the generator’s differentiable function be denoted as
G(z, πg) and similarly D(x, πd) for the discriminator, where
G and D are two neural networks with parameters πg and πd.
Thus, D(x) represents the probability that x comes from the
real data. The generator loss LG and the discriminator loss
LD are deﬁned as:

LD = Ex∼pdata(x)[− log D(x)]

+ Ez∼pz(z)[− log(1 − D(G(z))],

LG = Ez∼pz(z)[log(1 − D(G(z))].

(1)

(2)

GAN is then set up as a two-player zero-sum game be-

tween G and D as follows:

minG maxD Ex∼pdata(x)[log D(x)]

+ Ez∼pz(z)[log(1 − D(G(z))].

(3)

During training, the parameters of G and D are up-
dated alternately until we reach the global optimal solution

D(G(z)) = 0.5. Next, we let Πg and Πd be the set of param-
eters for G and D, considering the probability distributions
σg and σd, the mixed strategy formulation [11] is:

min
σg

max
σd

+ Eπd∼

σd

Eπd∼
Eπg ∼

Ex

σd

∼
Ez
∼

σg

pdata(x)[log D(x, πd)]

pz(z)[log(1 − D(G(z, πg), πd)].

(4)

Similarly to GANs, DCGAN, SNGAN and SGAN can also
be viewed as two-player zero-sum games with mixed strate-
gies of the players. DCGAN modiﬁes the vanilla GAN by
replacing fully-connected layers with the convolutional lay-
ers. SGAN trains multiple generators and discriminators
using the loss as a linear combination of 3 loss terms: adver-
sarial loss, conditional loss and entropy loss.

3.2. Double Oracle Algorithm

A normal-form game is a tuple (Π, U, n) where n is the
number of players, Π = (Π1, . . . , Πn) is the set of strategies
for each player i ∈ N, where N = {1, . . . , n} and U :
Π → Rn is a payoff table of utilities R for each joint policy
played by all players. Each player chooses the strategy to
maximize own expected utility from Πi, or by sampling
from a distribution over the set of strategies σi ∈ ∆(Πi).
We can use linear programming, ﬁctitious play [3] or regret
minimization [32] to compute the probability distribution
over players’ strategies.

r ⊂ Πr and Πt

In the Double Oracle (DO) algorithm [20], there are two
best response oracles for the row and column player respec-
tively. The algorithm creates restricted games from a subset
of strategies at the point of each iteration t for row and
column players, i.e., Πt
c ⊂ Πc as well as
a meta-matrix U t at the tth iteration. We then solve the
meta-matrix to get the probability distributions on Πt
r and
Πt
c. Given a probability distribution σc of the column player
strategies, BRr(σc) gives the row player’s best response to
σc. Similarly, given probability distribution σr of the row
player’s strategies, BRc(σr) is the column player’s best re-
sponse to σr. The best responses are added to the restricted
game for the next iteration. The algorithm terminates when
the best response utilities are not higher than the equilibrium
utility of current restricted game. Although in the worst-case,
the entire strategy space may be added to the restricted game,
DO is guaranteed to converge to mixed NE in two-player
zero-sum games. DO is also extended to the multi-agent
reinforcement learning in PSRO [15] to approximate the best
responses to the mixtures of agents’ policies, and compute
the meta-strategies for the policy selection.

4. DO-GAN

As discussed in previous sections, computing mixed NE
for GANs is challenging as there is an extremely large num-
ber of pure strategies, i.e., possible parameter settings of the
generator and discriminator networks. Thus, we propose

a double oracle framework for GANs (DO-GAN) to com-
pute the mixed NE efﬁciently. DO-GAN builds a restricted
meta-matrix game between the two players and computes
the mixed NE of the meta-matrix game, then DO-GAN it-
eratively adds more generators and discriminators into the
meta-matrix game until termination.

4.1. General Framework of DO-GAN

(cid:80)

g, σt

g, σt

g and σt

πd∈D σt

g(πg) · σt

d) = (cid:80)
g , σt∗

GAN can be translated as a two-player zero-sum game
between the generator player g and the discriminator player d.
To compute the mixed NE of GANs, at iteration t, DO-GAN
creates a restricted meta-matrix game U t with the trained
generators and discriminators as strategies of the two players,
where the generators and discriminators are parameterized
by πg ∈ G and πd ∈ D. We use U t(πg, πd) to denote the
generator player’s payoff when playing πg against πd, which
is deﬁned as LD. Since GAN is zero-sum, the discriminator
player’s payoff is −U t(πg, πd). We deﬁne σt
d as the
mixed strategies of generator player and discriminator player,
respectively. With a slight abuse of notation, we deﬁne
the generator player’s expected utility of mixed strategies
(cid:104)σt
d(cid:105) as U t(σt
d(πd) ·
πg∈G
U t(πg, πd). We use (cid:104)σt∗
d (cid:105) to denote mixed NE of the
restricted meta-matrix game U t. We solve U t to obtain the
mixed NE, compute best responses and add them into U t for
next iteration. Figure 2 presents an illustration of DO-GAN
and Algorithm 1 describes the overview of the framework.
Our algorithm starts by initializing two arrays G and D
to store multiple generators and discriminators (line 1). We
train the ﬁrst πg and πd with the canonical training proce-
dure of GANs (line 2). We store the parameters of trained
models in G and D (line 3), compute the adversarial loss
LD and add it to the meta-matrix U 0 (line 4). We initialize
the meta-strategies σ0∗
g = [1] and σ0∗
d = [1] since there
is only one pair of generator and discriminator available
(line 5). For each epoch, we use generatorOracle() and
discriminatorOracle() to obtain best responses π(cid:48)
g and
π(cid:48)
d to σt∗
g via Adam Optimizer, respectively, then
add them into G and D (lines 7-10). We then augment U t−1
by adding π(cid:48)
d) to obtain
U t and compute the missing entries (line 11). We com-
pute the missing payoff entries U t(π(cid:48)
g, πd), ∀πd ∈ D and
U t(πg, π(cid:48)
d), ∀πg ∈ G by sampling a few batches of training
data. After that, we compute the mixed NE (cid:104)σt∗
d (cid:105) of U t
with linear programming (line 12). The algorithm terminates
if the criteria described in Algorithm 2 is satisﬁed (line 13).
g to obtain the best re-
d ), ∀πg ∈
d to
d) ≥
g , πd), ∀πd ∈ Πd. Full details of generator oracle and

sponse against σt∗
Πg. Similarly, in disciminatorOracle(), we train π(cid:48)
g , π(cid:48)
obtain the best response against σt∗
U t(σt∗
discriminator oracle can be found in Appendix B.

In generatorOracle(), we train π(cid:48)

d and calculating U t(π(cid:48)

d ) ≥ U t(πg, σt∗

g , i.e., U t(σt∗

d , i.e., U t(π(cid:48)

d and σt∗

g and π(cid:48)

g , σt∗

g, σt∗

g, π(cid:48)

Algorithm 1: DO − GAN()

1 Initialize generator and discriminator arrays G = ∅ and

D = ∅;

2 Train generator & discriminator to get the ﬁrst πg and πd;
3 G ← G ∪ {πg}; D ← D ∪ {πd};
4 Compute the adversarial loss LD and add it to meta-matrix

U 0;

5 Initialize σ0
∗g = [1] and σ0
6 for epoch t ∈ {1, 2, ...} do
7

∗d = [1];

8

9

10

11

12

13

∗g , G);

∗d , D);

π(cid:48)g ← generatorOracle(σt
G ← G ∪ {π(cid:48)g};
π(cid:48)d ← discriminatorOracle(σt
D ← D ∪ {π(cid:48)d};
1 with π(cid:48)g and π(cid:48)d to obtain U t and
Augment U t
−
compute missing entries;
Compute mixed NE (cid:104)σt
program;
if TerminationCheck(U t, σt
break;

∗d (cid:105) for U t with linear

∗d ) then

∗g , σt

∗g , σt

// Section 4.2

// Section 4.3

4.2. Linear Program for Meta-matrix Game

Since the current restricted meta-matrix game U t is a
zero-sum game, we can use a linear program to compute
the mixed NE in polynomial time [35]. Given the generator
player g’s mixed strategy σt
g, the discriminator player d will
play strategies that minimize the expected utility of g. Thus,
the mixed NE strategy for the generator player σt∗
g is to
maximize the worst-case expected utility, which is obtained
by solving the following linear program:

σt∗
g = arg maxσt

g

{v : σt

(cid:88)

i∈G

σt
g(i) = 1,

g ≥ 0,
U t(σt

g, πd) ≥ v, ∀πd ∈ D}.

(5)

Similarly, we can obtain the mixed NE strategy for the dis-
criminator σt∗
d by solving a linear program that maximizes
the worst-case expected utility of the discriminator player.
Therefore, we obtain the mixed NE (cid:104)σt∗
d (cid:105) of the re-
stricted meta-matrix game U t.

g , σt∗

4.3. Termination Check

g (or π(cid:48)

DO terminates the training by checking whether the best
response π(cid:48)
d) is in the support set G (or D) [13], but we
cannot apply this approach to DO-GAN as GAN has inﬁnite-
dimensional strategy space [11]. Hence, we terminate the
training if the best responses cannot bring a higher utility
to the two players than the entries of the current support
sets, as discussed in [15, 26]. Speciﬁcally, we ﬁrst compute
U t(σt∗
d ) and the expected utilities for new generator
and discriminator U t(G[m], σt∗
d ), U t(σt∗
g , D[n]) (line 1-3).
Then, we calculate the utility increment (lines 4-5) and re-
turns True if both U t(G[m], σt∗
g , D[n]) cannot
bring a higher utility than U t(σt∗

d ) and U t(σt∗
g , σt∗

d ) by (cid:15) (lines 6-8).

g , σt∗

Figure 2. An illustration of DO-GAN. Figure adapted from [15].

Algorithm 2: TerminationCheck(U t, σt

∗g , σt

∗d )

// U t is of size m × n
// |G| = m, |D| = n

∗d );

1 Compute U t(σt
∗g , σt
2 Compute U t(σt
∗g , D[n]);
3 Compute U t(G[m], σt
∗d );
4 genInc = U t(G[m], σt
∗d ) − U t(σt
∗g , σt
∗g , D[n]) − (−U t(σt
5 disInc = −U t(σt
6 if genInc < (cid:15) && − disInc < (cid:15) then
7

return True

∗d );
∗g , σt

∗d ));

8 else return False ;

Algorithm 3: DO-GAN/P

1

// I stores indices to be pruned from G and D
// G stores models to be pruned from G and D

2 DO − GAN();
3 Ig = ∅; Id = ∅
4 Kg = ∅; Kd = ∅ if |G| > s then
5

for i ∈ {0, . . . , |G| − 1} do
∗g (G[i]) == min σt
if σt
Kg ← Kg ∪ {G[i]} ;

6

∗g then Ig ← Ig ∪ {i};

7 if |D| > s then
8

9

for j ∈ {0, . . . , |D| − 1} do
∗d (D[j]) == min σt
if σt
Kd ← Kd ∪ {D[j]} ;

∗d then Id ← Id ∪ {j};

10 G ← G \ Kg; D ← D \ Kd;
11 U ← JIg ,m · U t · J T

Id,n

5. Practical Implementations

As the number of epochs grows during the training of
DO, the number of networks and the size of the meta-matrix
also grows. Hence, there is a risk that the support strategy
set becomes very large and G and D become intractable. To
make the algorithm practical and scalable, we propose two
methods: DO-GAN with meta-matrix pruning (DO-GAN/P)
and DO-GAN with continual learning (DO-GAN/C).

5.1. Meta-matrix Pruning (DO-GAN/P)

The ﬁrst method is to prune the meta-matrix. Here, we
adapt the greedy pruning algorithm, as depicted in Algo-

Algorithm 4: DO-GAN/C

1 Initialize generator and discriminator task arrays G = ∅

and D = ∅;

2 Train generator & discriminator to get with the ﬁrst task to

get π0

g and π0
d;
3 G ← G ∪ {πg}; D ← D ∪ {πd};
4 Compute the adversarial loss LD and add it to meta-matrix

U 0;

5 Initialize σ0
∗g = [1] and σ0
6 for epoch t ∈ {1, 2, ...} do
7

∗d = [1];

8

9

10

11

12

13

14

15

16

∗g , G);

∗d , D);

d};

Create new tasks πt
g and πt
d;
g ← generatorOracle(σt
πt
G ← G ∪ {πt
g};
πt
d ← discriminatorOracle(σt
D ← D ∪ {πt
if t ≥ 2 then
} and D \ {πt
G \ {πt
−
g
d
g and πt
Create U t with πt
, πt
g
d
∗g , σt
Compute mixed NE (cid:104)σt
program;
if TerminationCheck(U t, σt
break;

−

−

−

2

1

2

1

} ;

, πt
d;
∗d (cid:105) for U t with linear

// Section 4.2

∗g , σt

∗d ) then

// Section 4.3

rithm 3. When either |G| or |D| is greater than the limit of
the support set size s, we prune at least one strategy with
the least probability, which is the strategy that contributes
the least to the player’s winning. Speciﬁcally, we deﬁne JI,b
where I is the set of row numbers to be removed, b is the total
rows of a matrix. To remove the 2nd row of a matrix having
(cid:19)

3 rows, we deﬁne I = {1}, b = 3 and J{1},3 =

(cid:18)1
0

0 0
0 1

.

If |G| > s, at least one strategy with minimum probability
is pruned from G, similarly for D (lines 4-10). Finally, we
prune the meta-matrix using matrix multiplication (line 11).

5.2. Continual Learning (DO-GAN/C)

Our ablation studies show that we still need a support
set of at least s = 10 for DO-GAN/P to converge and the
time complexity grows as s increases. Thus, we further re-
duce both time and space complexity by making the network
retain the knowledge of previous networks so that the algo-
rithm will converge with even smaller support set. Hence,
we propose to adapt continual learning to consolidate the

StartMeta-matrixGameSolveMeta-matrixGameNE=h(3/4,1/4),(1/2,1/2)iComputeBestResponsesExpandMeta-matrixGameNewgenerator/discriminatorhπ0g,π0diareaddedTerminateBestresponsesdonotimproveresultsEndπd(1)πd(2)πg(1)-2-1πg(2)0-3knowledge of multiple networks to a single network while
setting s = 2 to reduce the space complexity as much as
possible. We treat each network as a task to train the adaptive
continual learning network while having a distribution over
the tasks to represent the player’s strategies.

To remedy the catastrophic forgetting i.e., all the genera-
tor tasks focus only towards fooling the newest discriminator,
we adapt Elastic Weight Consolidation (EWC) method [17].
We change the generator loss function from non-saturating
to saturating and add a penalty function accordingly. Let G
be trained on task t − 1 to have optimal parameters πt−1
,
the Fisher information F is:

g

F = Ez∼p(z)

(cid:34)

(cid:16) δ

δπt−1
g

log D(G(z)|πt−1

g

(cid:17)2(cid:35)
)

(6)

After obtaining the Fisher information, we directly use F as
a regularization loss to penalize the weight change during
the training. Hence, G’s loss function is augmented as:

LG = Ez∼pz(z)[− log D(G(z)

+ λ · σt∗
g ·

(cid:88)
i

Fi(πi − πt−1,i

g

)2

(7)

g

where πt−1
represents the parameters learned for task t − 1,
i is the index of each parameter of the generator model, and
λ is the regularization weight.

Algorithm 4 describes the changes to DO − GAN algo-
rithm with continual learning. Instead of arrays G and D in
DO − GAN, we initialize tasks arrays for generator and dis-
criminator (line 1). At every epoch, we create new tasks and
train the generator and discriminator networks to outperform
the previous optimal parameters with the distribution at NE
σt∗
d and σt∗
g respectively (lines 7-11). Then, we keep the
previously and currently trained optimal parameters (line 13)
to create meta-matrix, solve it to compute mixed-NE (lines
14-15). Finally, we perform the termination check (line 16).

Complexity. Given the same architectures, the space com-
plexity of DO-GAN is O(t) where t is the number of epochs
until convergence. In contrast, the space complexity of DO-
GAN/P is O(s) where s is the size of the support set. DO-
GAN/C has the minimum storage complexity which is O(1).

In DO-GAN, we add a pair of generator and discriminator
for every epoch of training. Thus, the space complexity of
DO-GAN is O(t), making the algorithm memory-inefﬁcient
to train with real-world datasets where it needs a large num-
ber of epochs to converge. The space complexity of DO-
GAN/P is O(s) since we prune the meta-matrix and the
players’ strategies if the |G| > s or |D| > s where s is the
limit of the support set size. In DO-GAN/C, we train a single
adaptive network storing the optimal strategies only for the
tasks created at (t − 1)th and tth epochs. Thus, the space
complexity of DO-GAN/C is kept at O(1).

6. Experiments

We conduct our experiments on a machine with Xeon(R)
CPU E5-2683 v3@2.00GHz and 4× Tesla v100-PCIE-
16GB running Ubuntu operating system. We evaluate DO-
framework for established GAN architectures such as vanilla
GAN [8], DCGAN [28], SNGAN [24] and SGAN [12]. We
adopt the parameter settings and criterion of the GAN ar-
chitectures as published. We set s = 10 unless mentioned
otherwise. We compute the mixed NE of the meta game with
Nashpy. According to the ablation studies by [17,36], we set
λ as 1000 for MNIST, 5000 for CIFAR-10 and 5 × 108 for
CelebA. The evaluation details are shown in Appendix C.

Figure 3. Comparison of GAN and DO-GAN/P on 2D synthetic
Gaussian Mixture Dataset

6.1. Evaluation on 2D Gaussian Mixture Dataset

To illustrate the effectiveness of the architecture, we train
a double oracle framework with the simple vanilla GAN
architecture on a 2D mixture of 8 Gaussian mixture com-
ponents with cluster standard deviation 0.1 which follows
the experiment by [22]. Figure 3 shows the evolution of
512 samples generated by GAN and DO-GAN/P through
20000 epochs. The goal of GAN and DO-GAN/P is to cor-
rectly generate samples at 8 modes as shown in the target.
The results show that GAN can only identify 6 out of 8
modes of the synthetic Gaussian data distribution, while the
DO-GAN/P can obtain all the 8 modes of the distribution.
Furthermore, DO-GAN/P takes shorter time (less than 5000
epochs) to identify all 8 modes of the data distribution. We
present a more detailed evolution of data samples through the
training process on 2D Gaussian Mixtures in Appendix D.

Ablations. We also varied the limit of support set size for
DO-GAN/P with s = 5, 10, 15 and recorded the computation
time as discussed in Appendix E. We found that the training
cannot converge when s = 5 and takes a long time when
s = 15. Thus, we chose s = 10 for the training.

6.2. Evaluation on Real-world Datasets

We evaluate the performance of the double oracle frame-
work which takes several established GAN architectures as
the backbone as discussed in Appendix H, i.e., GAN, DC-
GAN and SGAN with convolutional layers for deep neural
networks of GAN as well as SNGAN which uses normal-
ization techniques. We run experiments on MNIST [16],

CIFAR-10 [14] and CelebA [19]. MNIST contains 60,000
samples of handwritten digits with images of 28 × 28.
CIFAR-10 contains 50, 000 training images of 32 × 32 of
10 classes. CelebA is a large-scaled face dataset with more
than 200K images of size 128 × 128.

6.2.1 Qualitative Evaluation

We choose the CelebA dataset for the qualitative evaluation
since the training images contain noticeable artifacts (alias-
ing, compression, blur) that make the generator difﬁcult to
produce perfect and faithful images. We compare perfor-
mances of DO-DCGAN/P, DO-SNGAN/P and DO-SGAN/P
with their counterparts. SNGAN which is trained for 40
epochs with termination (cid:15) of 5 × 10−5 for DO-SNGAN/P
where other architectures are trained for 25 epochs with
termination (cid:15) of 5 × 10−5 for DO variants. The generated
CelebA images of DCGAN and DO-DCGAN/P are shown in
Figure 4, where we ﬁnd that DCGAN suffers mode-collapse,
while DO-DCGAN/P does not. We also present the gen-
erated images of SNGAN vs DO-SNGAN/P using ﬁxed
noise at different training epochs in Figure 5. From the
results, we can see that SNGAN, SGAN, DO-SNGAN/P
and DO-SGAN/P are able to generate various faces, i.e., no
mode-collapse. Judging from subjective visual quality, we
ﬁnd that DO-SNGAN/P and DO-SGAN/P are able to gener-
ate plausible images faster than SNGAN and SGAN during
training, i.e., 17 epochs for DO-SGAN/P and 20 epochs for
SGAN. More experimental results on CIFAR-10 and DO-
GAN/C variants. which can produce competitive results, can
be found in Appendix F.

Figure 4. Training images with ﬁxed noise for DCGAN and DO-
DCGAN/P until termination.

6.2.2 Quantitative Evaluation

In this section, we evaluate the performance of various archi-
tectures by quantitative metrics.

Inception Score. We ﬁrst leverage the Inception Score
(IS) [34] by using Inception_v3 [37] as the inception model.

To compute the inception score, we ﬁrst compute the
Kullback-Leibler (KL) divergence for all generated images
and use the equation IS = exp(Ex[KL(D(p(y|x) (cid:107) p(y)))])
where p(y) is the conditional label distributions for the im-
ages in the split and p(y|x) is that of the image x estimated
by the reference inception model. Inception score evaluates
the quality and diversity of all generated images rather than
the similarity to the real data from the test set.

FID Score. Fréchet Inception Distance (FID) measures the
distance between the feature vectors of real and generated
images using Inception_v3 model [9]. Here, we let p and
q be the distributions of the representations obtained by
projecting real and generated samples to the last hidden
layer of Inception model. Assuming that p and q are the
multivariate Gaussian distributions, FID measures the 2-
Wasserstein distance between the two distributions. Hence,
FID Score can capture the similarity of generated images to
real ones better than inception score.

Figure 5. Training images with ﬁxed noise for SNGAN and DO-
SNGAN/P until termination.

Results. The results are shown in Table 1. In CIFAR-10
dataset, the pruning method i.e., DO-GAN/P, DO-DCGAN/P
and DO-SNGAN/P obtain much better results (7.2 ± 0.16,
7.86 ± 0.14 and 8.55 ± 0.08) than GAN, DCGAN and
SNGAN (3.84±0.09, 6.32±0.05 and 7.58±0.12). However,
we do not see a signiﬁcant improvement in DO-SGAN/P
compared to SGAN 8.62±0.12 and 8.69±0.10 since SGAN
already can generate diverse images. We did not include IS
for CelebA dataset as IS cannot reﬂect the real image quality
for CelebA, as observed in [9]. In CIFAR-10 dataset, DO-
GAN/P, DO-DCGAN/P, DO-SNGAN/P and DO-SGAN/P
obtain much lower FID scores (31.44, 22.25, 16.56, 18.20)
respectively. The trend follows in CelebA obtaining 7.11 for
DO-DCGAN/P while 10.92 for DCGAN, 7.62 for SNGAN
while 6.92 for DO-SNGAN/P, 6.98 for SGAN and 6.32 for
DO-SGAN/P respectively. Although we see a signiﬁcant
improvement in the quality of DO-SGAN/P images, FID
score for DO-SGAN/P is affected by distortions.

Table 1. Inception scores (higher is better) and FID scores (lower is better) of DO-GAN with pruning (DO-GAN/P) and continual learning
(DO-GAN/C). The mean and standard deviation are drawn from running 10 splits on 10000 generated images.

Inception Score

FID Score

GAN
DCGAN
SNGAN
SGAN

MNIST

1.04 ± 0.05
1.26 ± 0.05
1.35 ± 0.11
1.39 ± 0.09

-
MIX+DCGAN
MGAN
-
DCGAN+ EWC -

CIFAR-10

3.84 ± 0.09
6.32 ± 0.05
7.58 ± 0.12
8.62 ± 0.12

7.72 ± 0.09
8.33 ± 0.10
7.58 ± 0.07

CIFAR-10

CelebA

71.44
37.66
25.50
24.83

-
26.70
25.51

-
10.92
7.62
6.98

-
-
-

DO-GAN/P
DO-DCGAN/P
DO-SNGAN/P
DO-SGAN/P

DO-GAN/C
DO-DCGAN/C
DO-SNGAN/C
DO-SGAN/C

1.39 ± 0.09 (+0.35)
1.42 ± 0.11 (+0.16)
1.42 ± 0.07 (+0.07)
1.42 ± 0.09 (+0.03)

7.20 ± 0.16 (+3.36)
7.86 ± 0.14 (+1.54)
8.55 ± 0.08 (+0.97)
8.69 ± 0.10 (+0.07)

1.42 ± 0.10 (+0.38)
1.43 ± 0.13 (+0.17)
1.43 ± 0.08 (+0.08)
1.45 ± 0.15 (+0.06)

7.32 ± 0.30 (+3.48)
8.04 ± 0.22 (+1.72)
8.54 ± 0.16 (+0.96)
9.78 ± 0.11 (+1.16)

31.44 (−40.00)
22.25 (−15.41)
18.20 (−7.30)
16.56 (−8.27)

26.93 (−44.51)
21.50 (−16.16)
19.57 (−5.93)
16.07 (−8.76)

-
7.11 (−3.81)
6.92 (−0.70)
6.32 (−0.66)

-
7.16 (−3.76)
6.74 (−0.88)
6.30 (−0.68)

Note: MIX+DCGAN and MGAN results are directly copied from [2, 10]. The magenta values are the improvements from non-DO counterparts.

We observe the competitive results for continual learn-
ing method with the pruning method: DO-GAN/C, DO-
DCGAN/C, DO-SNGAN/C and DO-SGAN/C obtain incep-
tion scores of 7.32 ± 0.30, 8.04 ± 0.22, 8.54 ± 0.16 and
9.78 ± 0.11 respectively as well as FID scores of 26.93,
21.50, 19.57 and 16.07 respectively for CIFAR-10 dataset.
Moreover, 7.16, 6.74 and 6.30 respectively for CelebA
dataset. We also compared with DCGAN+EWC which uses
continual learning without the double oracle framework for
CIFAR-10 dataset and obtained better results where DC-
GAN+EWC obtained inception score of 7.58 ± 0.07 and
FID score of 25.51.

Table 2. Wall-clock time comparison of SGAN variants

SGAN DO-SGAN/P DO-SGAN/C

Time (GPU-Hrs)

143.28

119.04

97.54

Note: SGAN is trained for 500 epochs on CIFAR-10. DO-SGAN/P
converged at 288 epochs and DO-SGAN/C at 236 epochs.

From the results, we can see that DO framework performs
better than each of their original counterpart architectures
with both methods: pruning and continual learning. More
details can be found in Appendix G. We can also see that
continual learning method that uses least storage space can
obtain competitive results with DO-GAN/P without memory
storage limitations or pruning the players’ strategies.

We also compare the wall-clock running times of DO-

SGAN/P and DO-SGAN/C with SGAN as shown in Table 2.
We let SGAN train for 500 epochs on CIFAR-10 dataset.
Meanwhile, DO-SGAN/P converged at 288 epochs and DO-
SGAN/C converged at 236 epochs. The recorded GPU hours
of 143.28 for SGAN, 119.04 for DO-SGAN/P and 97.54 for
DO-SGAN/C show that DO variants are more efﬁcient.

7. Conclusion

We propose a novel double oracle framework to GANs,
which starts with a restricted game and incrementally adds
the best responses of the generator and the discriminator
oracles as the players’ strategies. We then compute the
mixed NE to get the players’ meta-strategies by using a
linear program. We also propose two approaches to make
the solution scalable including pruning the support strategy
set and continual learning with an adaptive architecture to
store the multiple networks of generators and discriminators.
We apply DO-GAN approach to established GAN architec-
tures such as vanilla GAN, DCGAN, SNGAN and SGAN.
Extensive experiments with the synthetic 2D Gaussian mix-
ture dataset as well as real-world datasets such as MNIST,
CIFAR-10 and CelebA show that DO-GAN variants have
signiﬁcant improvements in comparison to their respective
GAN architectures in terms of both subjective image quality
and quantitative metrics.

Acknowledgement

This research was supported by the National Research
Foundation, Singapore under its AI Singapore Programme
(AISG Award No: AISG-RP-2019-0013), National Satellite
of Excellence in Trustworthy Software Systems (Award No:
NSOE-TSS2019-01), and NTU.

References

[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasser-
stein generative adversarial networks. In ICML, pages 214–
223, 2017. 1

[2] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi
Zhang. Generalization and equilibrium in generative adver-
sarial nets (GANs). In ICML, pages 224–232, 2017. 1, 2,
8

[3] Ulrich Berger. Brown’s original ﬁctitious play. Journal of

Economic Theory, 135(1):572–578, 2007. 3

[4] Branislav Bošanský, Christopher Kiekintveld, Viliam Lisy,
Jiri Cermak, and Michal Pechoucek. Double-oracle algo-
rithm for computing an exact Nash equilibrium in zero-sum
extensive-form games. In AAMAS, pages 335–342, 2013. 2
[5] B Bosansky, Christopher Kiekintveld, Viliam Lisy, and
Michal Pechoucek. Iterative algorithm for solving two-player
zero-sum extensive-form games with imperfect information.
In ECAI, pages 193–198, 2012. 2

[6] Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Genera-

tive multi-adversarial networks. In ICLR, 2017. 3

[7] Farzan Farnia and Asuman Ozdaglar. GANs may have no
Nash equilibria. arXiv preprint arXiv:2002.09124, 2020. 1
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NeurIPS,
pages 2672–2680, 2014. 1, 3, 6

[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium.
In NeurIPS, pages 6626–6637, 2017. 7, vi

[10] Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung.
MGAN: Training generative adversarial nets with multiple
generators. In ICLR, 2018. 1, 2, 3, 8

[11] Ya-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed
nash equilibria of generative adversarial networks. In ICML,
pages 2810–2819, 2019. 1, 2, 3, 4

[12] Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and
Serge Belongie. Stacked generative adversarial networks. In
CVPR, pages 5077–5086, 2017. 2, 6

[13] Manish Jain, Dmytro Korzhyk, Ondˇrej Vanˇek, Vincent
Conitzer, Michal Pˇechouˇcek, and Milind Tambe. A dou-
ble oracle algorithm for zero-sum security games on graphs.
In AAMAS, pages 327–334, 2011. 2, 4

[14] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
CIFAR-10 (Canadian Institute for Advanced Research).
https://www.cs.toronto.edu/ kriz/cifar.html, 2009. 7

[15] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Ange-
liki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver, and

Thore Graepel. A uniﬁed game-theoretic approach to multia-
gent reinforcement learning. In NeurIPS, pages 4190–4203,
2017. 2, 3, 4, 5

[16] Yann LeCun and Corinna Cortes. MNIST handwritten digit
database. http://yann.lecun.com/exdb/mnist/, 2010. 6
[17] Yijun Li, Richard Zhang, Jingwan Lu, and Eli Shechtman.
Few-shot image generation with elastic weight consolidation.
In NeurIPS, 2020. 6, iii

[18] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In NeurIPS, pages 700–
708, 2017. 1

[19] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In ICCV, pages
3730–3738, 2015. 7

[20] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum.
Planning in the presence of cost functions controlled by an
adversary. In ICML, pages 536–543, 2003. 2, 3

[21] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
The numerics of GANs. In NeurIPS, pages 1825–1835, 2017.
1, 2

[22] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks.
In
ICLR, 2017. 6

[23] Mehdi Mirza and Simon Osindero. Conditional generative

adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 1

[24] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative adver-
sarial networks. ICLR, 2018. 2, 6

[25] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin
Ishii. Virtual adversarial training: a regularization method for
supervised and semi-supervised learning. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 41(8):1979–
1993, 2018. 1

[26] Paul Muller, Shayegan Omidshaﬁei, Mark Rowland, Karl
Tuyls, Julien Perolat, Siqi Liu, Daniel Hennes, Luke Marris,
Marc Lanctot, Edward Hughes, et al. A generalized training
approach for multiagent learning. In ICLR, 2020. 2, 4
[27] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan
Li, Andrew Stevens, and Lawrence Carin. Variational autoen-
coder for deep learning of images, labels and captions. In
NeurIPS, pages 2352–2360, 2016. 1

[28] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional genera-
tive adversarial networks. arXiv preprint arXiv:1511.06434,
2015. 1, 2, 6, vi

[29] Mohamed Ragab, Zhenghua Chen, Min Wu, Chuan Sheng
Foo, Chee Keong Kwoh, Ruqiang Yan, and Xiaoli Li. Con-
trastive adversarial domain adaptation for machine remaining
useful life prediction. IEEE Transactions on Industrial Infor-
matics, 17(8):5239–5249, 2020. 1

[30] Mohamed Ragab, Zhenghua Chen, Min Wu, Haoliang Li,
Chee-Keong Kwoh, Ruqiang Yan, and Xiaoli Li. Adversar-
ial multiple-target domain adaptation for fault classiﬁcation.
IEEE Transactions on Instrumentation and Measurement,
70:1–11, 2020. 1

[31] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative adver-
sarial text to image synthesis. In ICML, 2016. 1

[32] Tim Roughgarden. Algorithmic game theory. Communica-

tions of the ACM, 53(7):78–86, 2010. 3

[33] Yunus Saatchi and Andrew Gordon Wilson. Bayesian gan.

arXiv preprint arXiv:1705.09558, 2017. 3

[34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training GANs. In NeurIPS, pages 2234–2242, 2016. 7

[35] Alexander Schrijver. Theory of Linear and Integer Program-

ming. John Wiley & Sons, 1998. 2, 4

[36] Ari Seff, Alex Beatson, Daniel Suo, and Han Liu. Contin-
ual learning in generative adversarial nets. arXiv preprint
arXiv:1705.08395, 2017. 3, 6, iii

[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In CVPR, pages 2818–2826,
2016. 7

[38] Jason Tsai, Thanh H Nguyen, and Milind Tambe. Security
games for controlling contagion. In AAAI, pages 1464–1470,
2012. 2

[39] Zhengwei Wang, Qi She, and Tomas E Ward. Generative
adversarial networks in computer vision: A survey and taxon-
omy. arXiv preprint arXiv:1906.01529, 2019. vii, xi
[40] Kevin Waugh, Nolan Bard, and Michael Bowling. Strategy
grafting in extensive games. In NeurIPS, pages 2026–2034,
2009. 2

A. Comparison of Terminologies between Game Theory and GAN

Table 1. Comparison of Terminologies between Game Theory and GAN

Game Theory terminology GAN terminology

Player

Strategy

Policy

Game

Generator/ discriminator

The parameter setting of generator/ discriminator, e.g., πg and πd
The sequence of parameters (strategies) till epoch t, e.g., (π1
g, π2
Note: Not used in DO-GAN.

g, ..., πt
g)

The minmax game between generator and discriminator

Meta-game/ meta-matrix

The minmax game between generator & discriminator with
their respective set of strategies at epoch t of DO framework

Meta-strategy

The mixed NE strategy of generator/discriminator at epoch t

B. Full Algorithm of DO-GAN

∗d , D)

Algorithm 1: GeneratorOracle(σt
1 Initialize a generator G with random parameter setting π(cid:48)
g;
2 for iteration k0 . . . kn do
Sample noise z;
3
πd = Sample a discriminator from D with σt∗
d ;
Initialize a discriminator D with parameter setting πd;
Update the generator G’s parameters π(cid:48)

6

5

4

g via Adam optimizer:

(cid:53)π(cid:48)g

log (1 − D(G(z)))

Algorithm 2: DiscriminatorOracle(σt
1 Initialize a discriminator D with random parameter setting π(cid:48)
d;
2 for iteration k0 . . . kn do
3

∗g , G)

Sample a minibatch of data x;
for a minibatch do
Sample noise z;
πg = Sample a generator from G with σt∗
g ;
Initialize a generator G with a parameter setting πg;
Generate and add to mixture G(z);
Update the discriminator D’s parameters π(cid:48)

4

5

6

7

8

9

10

d via Adam optimizer:

(cid:53)π(cid:48)d

log D(x) + log (1 − D(G(z)))

We train the oracles for some iterations which we denote as k0,1,2,.... For experiments, we train each oracle for an epoch for
the real-world datasets and 50 iterations for the 2D Synthetic Gaussian Dataset. At each iteration t, we sample the generators
from the support set G with the meta-strategy σt∗
g to generate the images for evaluation. Similarly, we conduct the performance

evaluation with the generators sampled from G with the ﬁnal σ∗
g at termination. SGAN consists of a top-down stack of GANs,
e.g, for a stack of 2, Generator 1 is the ﬁrst layer stacked on Generator 0 with each of them connected to Discriminator 1 and 0
respectively. Hence, in DO-SGAN, we store the meta-strategies for the Generator 0 and 1 in σt∗
g and the Discriminator 1 and 0
for σt∗
d and train Generator
1 ﬁrst then followed by calculating loss with Discriminator 1 and train Generator 0 subsequently, and ﬁnally calculate ﬁnal
loss with Discriminator 0 and train the whole model end to end. We perform the same process for DisciminatorOracle().

d . In GeneratorOracle(), we ﬁrst sample Discriminator 1 and 0 from discriminator distribution σt∗

C. Implementation Details

Table 2. Training Hyperparameters

GAN

DCGAN SNGAN SGAN

Generator Learning Rate
Discriminator Learning Rate
batch size
Adam: beta 1
Adam: beta 2

0.0002
0.0002
64
0.5
0.999

0.0002
0.0002
64
0.5
0.999

0.0002
0.0002
64
0.5
0.999

0.0001
0.0001
100
0.5
0.999

We implement our proposed method with Python 3.7, Pytorch=1.4.0 and Torchvision=0.5.0. We set the hyperparameters as
the original implementations. We present the hyperparameters set in Table 2. We use Nashpy to compute the equilibria of the
meta-matrix game.

C.1. Value of λ

The experiment in [17] has done ablation studies for FFHQ dataset which are emoji faces and hence we used the λ value for
CelebA. Meanwhile, we adopted the results from [36] and set 1000 for MNIST and the maximum value of ablation study for
SVHN dataset to train CIFAR-10 as we want to use λ values from the most similar datasets. The experiments in [36] reported
that they observed little difference in visual quality regarding with ablation study but high values of λ cause no loss in visual
ﬁdelity when beginning training on a new task rather than lower value of λ. Hence, we use the maximum value.

D. Full Training Process of 2D Gaussian Dataset

(a) GAN

(b) DO-GAN/P

Figure 1. Full comparison of GAN and DO-GAN/P on 2D Synthetic Gaussian Dataset

Figure 2. GAN and DO-GAN/P comparison with Gaussian Mixture 7 modes

Figure 1 shows the full training process of DO-GAN/P and GAN on 2D Synthetic Gaussian Dataset. From the results,
we ﬁnd that GAN struggles to generate the samples into 8 modes while DO-GAN/P can generate all the 8 modes of the
distribution. Furthermore, DO-GAN/P takes shorter time (less than 5000 iterations) to identify all 8 modes of the data
distribution. Moreover, we present the experiment results on 7 mode and 9-mode Gaussian Mixtures in Figure 2 and 3.

Figure 3. GAN and DO-GAN/P comparison with Gaussian Mixture 9 modes

E. Investigation of Support Set Size for DO-GAN/P

We vary the support set size s to 5, 10, 15 and record the training evolution and the running time as presented in Table 3
and Figure 4. We ﬁnd that if the support size is too small, e.g., s = 5, the best responses which are not optimal yet have better
utilities than the models in the support set are added and pruned from the meta-matrix repeatedly making the training not
able to converge. However, s = 15 takes a signiﬁcantly longer time as the time for the augmenting of meta-matrix becomes
exponentially long with the support set size. Hence, we chose s = 10 as our experiment support set size since we observed
that there is no signiﬁcant trade-off and shorter runtime.

Table 3. Runtime of DO-GAN/P on 2D Gaussian Dataset with s = 5, 10, 15

Support Set Size Runtime (GPU hours)

s = 5
s = 10
s = 15

> 1
0.5627
0.9989

Figure 4. Training evolution on 2D Gaussian Dataset with s = 5, 10, 15

F. Generated images of CelebA and CIFAR-10

In this section, we present the training images of CelebA and CIFAR-10 datasets. We do not evaluate the performance of
vanilla GAN and its DO variant on CelebA dataset since DCGAN and SGAN outperform vanilla GAN in image generation
tasks [28].

Figure 5. Training images with ﬁxed noise for DCGAN and DO-DCGAN until termination.

Figure 5 shows the training samples of DCGAN, DO-DCGAN/P and DO-DCGAN/C through the training process. Figure 6
also shows those of SNGAN which is trained for 40 epochs with termination (cid:15) of 5 × 10−5 for DO-SNGAN/P and DO-
SNGAN/C. Figure 7 shows those of SGAN which is trained until 20 epochs as well as DO-SGAN/P and DO-SGAN/C with
the same termination settings. The results show that DCGAN suffers from mode-collapse, generating similar face while DO-
DCGAN/P can generate more plausible and varying faces. We also present the generated images of DCGAN, DO-DCGAN/P,
DO-DCGAN/C, SNGAN, DO-SNGAN/P, DO-SNGAN/C, SGAN, DO-SGAN/P and DO-SGAN/C of CIFAR-10 dataset
showing that the variants of DO-DCGAN, DO-SNGAN and DO-SGAN can generate better and more identiﬁable images
than DCGAN, SNGAN and SGAN respectively. We present more of the generated samples from SNGAN, DO-SNGAN/P,
DO-SNGAN/C, SGAN, DO-SGAN/P, DO-SGAN/C on CelebA dataset in Figure 9.

G. FID score against iterations

To compute FID score, we use Inception_v3 model with max pool of 192 dimensions and the last layer as coding layer as
mentioned in [9]. We resized MNIST, CIFAR-10 generated and test images to 32 × 32 and CelebA images to 64 × 64. The
FID score against training epochs for CIFAR-10 dataset is as follows:

Figure 10 presents the FID score against each epoch of training for SGAN and DO-SGAN/P on CIFAR-10. While both
perform relatively well in generating plausible images, we can see that DO-SGAN/P terminates early at epoch 288 and has a
better FID score of 16.56 compared to 24.83 at 300 epoch until 21.284 at 500 epoch for the training of SGAN.

Figure 6. Training images with ﬁxed noise for SNGAN and DO-SNGAN until termination.

H. Choice of GAN Architectures for Experiments

We carried out experiments with the variants of GANs to evaluate the performance of our DO-GAN framework. We refer to
the taxonomy of GANs [39] and choose each architecture from the groups of GANs focused on Network Architecture, Latent
Space and Loss: DCGAN, SNGAN and SGAN as shown in Figure 11. We have also included comparisons with mixture
architectures such as MIXGAN and MGAN.

I. Example of Meta-matrix of DO-SGAN/P on CIFAR-10

Figure 7. Training images with ﬁxed noise for SGAN and DO-SGAN until termination.

(a) DCGAN

(b) SNGAN

(c) SGAN

(d) DO-DCGAN/P

(e) DO-SNGAN/P

(f) DO-SGAN/P

(g) DO-DCGAN/C

(h) DO-SNGAN/C

(i) DO-SGAN/C

Figure 8. Generated images of CIFAR-10 dataset

(a) SNGAN

(b) DO-SNGAN/P

(c) DO-SNGAN/C

Figure 9. Generated images of CelebA dataset for DO-SNGAN/P, DO-SNGAN/C and SNGAN

(d) SGAN

(e) DO-SGAN/P

(f) DO-SGAN/P

Figure 9. Generated images of CelebA dataset for DO-SGAN and SGAN

e
r
o
c
S
D
I
F

102

101.5

101

SGAN
DO-SGAN

50

100

150

200

250
Epochs

300

350

400

450

500

Figure 10. FID score vs. Epochs for SGAN and DO-SGAN trained on CIFAR-10

Figure 11. Taxonomy of GAN Architectures from [39]

(a) Meta-matrix at epoch t = 5
Meta-Strategies: σ5
Expected Payoff: U 5(σ5

∗g = [0, 0, 0, 0, 1], σ5
∗d ) = 0.017

∗g , σ5

∗d = [0, 0, 0, 0, 1]

(b) Meta-matrix at convergence
Meta-Strategies:
σ288
∗g
σ288
∗d
Expected Payoff: U 288(σ288
∗g

, σ288
∗d

) = −0.000133

= [0.015196, 0.025942, 0.141215, 0.000000, 0.0989, 0.163399, 0.000000, 0.535331, 0.000000, 0.020017]
= [0.320082, 0.004844, 0.000000, 0.111335, 0.141493, 0.071708, 0.046, 0.000000, 0.000000, 0.304538]

