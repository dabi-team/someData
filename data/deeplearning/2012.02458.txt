A data-set of piercing needle through deformable objects
for Deep Learning from Demonstrations

Hamidreza Hashempour†,2, Kiyanoush Nazari†,1, Fangxun Zhong3 and Amir Ghalamzan E. †,1

0
2
0
2
c
e
D
4

]

O
R
.
s
c
[

1
v
8
5
4
2
0
.
2
1
0
2
:
v
i
X
r
a

Abstract— Many robotic tasks are still teleoperated since
automating them is very time consuming and expensive. Robot
Learning from Demonstrations (RLfD) can reduce program-
ming time and cost. However, conventional RLfD approaches
are not directly applicable to many robotic tasks, e.g. robotic
suturing with minimally invasive robots, as they require a
time-consuming process of designing features from visual in-
formation. Deep Neural Networks (DNNs) have emerged as
useful tools for creating complex models capturing the rela-
tionship between high-dimensional observation space and low-
level action/state space. Nonetheless, such approaches require
a dataset suitable for training appropriate DNN models. This
paper presents a dataset of inserting/piercing a needle with
two arms of da Vinci Research Kit (DVRK) in/through soft
tissues. The dataset consists of (i) 60 successful needle insertion
trials with randomised desired exit points recorded by 6 high-
resolution calibrated cameras, (ii) the corresponding robot’s
data, calibration parameters and (iii) the commanded robot’s
control
input where all the collected data are synchronised.
The dataset is designed for Deep-RLfD approaches. We also
implemented several deep RLfD architectures, including simple
feed-forward CNNs and different Recurrent Convolutional
Networks (RCNs). Our study indicates RCNs improve the
prediction accuracy of the model despite that the baseline feed-
forward CNNs successfully learns the relationship between the
visual information and the next step control actions of the robot.
The dataset, as well as our baseline implementations of RLfD,
are publicly available for bench-marking1.

I. INTRODUCTION

Robotic technology is increasingly considered as the mean
of performing many tasks, especially in medical applications
such as Minimally Invasive Surgery (MIS) [1], [2] and
orthopaedic surgery [3]. One of the common robotic tasks in
MIS is suturing which usually takes a considerable amount of
operation time. Suturing contributes to the increased fatigue
and cognitive loading on the surgeon. Autonomous robotic
suturing has been of noticeable interest [4] to shorten the
operation time and decrease the cognitive load. Suturing
requires bi-manual robotic operations, precise position control
and a small margin of error which yields a minimal invasive
effect on the patient. Suturing patients’ tissues impose
very interesting and challenging research questions in terms
of robot perception, planning and control because of the
interaction of the robot and a deformable tissue. During
inserting the circular needle (see Fig. 1) into the deformable

†These authors have equal contribution.
1University
of Lincoln,

(IML)
aghalamzanesfahani@lincoln.ac.uk;
of
Tehran; 3 T Stone Robotics Institute and Dept. of Mechanical and
Automation Engineering, The Chinese University of Hong Kong, HKSAR,
China.

Intelligent Manipulation Lab

2 University

1https://github.com/imanlab/d-lfd

(a)

(b)

(c)

Fig. 1. Needle insertion in soft tissue: (a) the needle pushes the tissue
making the actual (red sphere) and desired (black-white sphere) needle exit
points as well as the actual (green dashed line) and desired (red dashed
line) needle paths different; (b) A 2D schematic of tissue deformation due
to needle insertion (original shape shown as grey dash line) alter the target
(red dot) from its original position (grey dot); and (c) Arm 2 is actively
manipulating the tissue to improve insertion accuracy, as adopted by manual
suturing skills.

tissue, the needle tip pushes the tissue, hence, the desired
and actual path differ and the actual exit-point, shown by the
black-white sphere in Fig. 1), drifts away from the desired
exit-point (red sphere) resulting in a less effective grip for
the stitch or (in some cases) failure of the stitch- as the stitch
cuts the tissue.

In practice, surgeons utilise Arm 2 to manipulate the tissue
and ensure the desired and actual exit-points are the same.
Surgeons only use visual feedbacks to predict the needle exit
point (performed by Arm 1 in Fig. 2 and 1) and close the
control loop by commanding Arm 2 to push/pool the tissue.
Formulating such tasks and implementing the corresponding
automatic controller is very complex and time-consuming.
Classically, Robot Learning from Demonstrations (RLfD) has
been proposed to reduce the programming time and cost by
learning such a control policy from sample successful task ex-
ecutions which may be teleoperated, e.g. teleoperated needle
insertion. In similar applications in medical robotics, various
control strategies have been proposed for robotic hands
position control [5] and autonomous robotic suturing [6],
[7]. However, hand designed features extracting semantically
meaningful information from high-dimensional images [8],
[9] limits the scalability/generalisability of such approaches.
An end-to-end data driven approach which efﬁciently ﬁnds the
mapping between the optimal control actions demonstrated
in the dataset of successful completion of the task and
high-dimensional image space as well as robot states can
signiﬁcantly helps automation of several complex tasks. To
have precise control in such unstructured environments, we
propose a framework of Deep-RLfD which allows to learn
the controller of complex task such as the needle insertion.
The contribution of this paper is twofold: (i) we present
a dataset of needle insertion into deformable objects. The

 
 
 
 
 
 
movements. In fact, such approaches may not be capable
of tackling high dimensional visual information or video
feed of the robot’s workspace. Recent approaches in video
prediction [13] illustrated a step towards learning an end-to-
end RLfD model.

Deep neural networks (DNN) have been utilised for video
prediction and state estimation based on visual sensory
information [14], [15], [16]. In the context of RLfD, however,
we need to build a model learning the robot controller
based on available sample demonstrations. In conventional
RLfD [10], it is highly desired to build a model which learns
a task only from a few demonstrations facilitating Human-
Robot Collaboration, i.e. a non-expert person can provide
the robot with a few samples of task completion and the
robot learns how to do it. In contrast, we aim at solving
the problem of RLfD where there are big datasets readily
available, e.g. in robotic surgery all the operations data can
be logged and may be readily available to train Deep RLfD.
In this work, we present a dataset that can be used for
RLfD. Moreover, we present several deep neural networks
architectures including convolutional and recurrent neural
networks, as well as, the combination of them (RCN) to
learn the robot control for the task of inserting a needle
in a deformable tissue which is an important/challenging
task in robotic surgery and suturing. This task is highly non-
linear as it involves the interaction between the robot and
the deformable object. Data-driven approaches are capable
of modelling highly nonlinear dynamic systems if they are
provided with enough training data. For sensitive applications
such as robotic surgery, a system with highly nonlinear
observation data e.g. image sequence, the maximum value
of tracking error plays a signiﬁcant role in evaluating
the reliability of the system. We present a discriminative
model which minimizes the tracking error relative to similar
approaches. In order to choose the most efﬁcient model,
various SOTA architectures are tested and by tuning the
hyper-parameters of the network the achieved accuracy and
error values are optimized.
Control theory for robotic surgery. Classic and modern
control theories have been employed to precisely control
medical robots, e.g., [17] developed an autonomous laparo-
scopic robotic suturing system in which point cloud data
is used to construct the tissue surface and plan the suturing
points. In [18], a vision-guided knot-tying method is proposed
for autonomous robotic suturing. Needle path planning in
autonomous robotic suturing is investigated in [19]. [20]
proposed a supervised autonomous robotic suturing system
in which three dimensional infrared imaging is used for point
cloud construction and then detecting tissue deformation.
However, tissue was ﬁrst marked by syringe with markers
to track motions which may have invasive effects. Visual
servoing has been used for suturing in micro-surgical task [21]
and steering ﬂexible needle [22]. However, these approaches
usually need time consuming hand-designed low dimensional
features extracted from high dimensional visual information
of the robot workspace.
Robot Learning from Demonstration (RLfD). [23] is one

(a)

(b)

(a) Dataset collection setup; (b) dataset includes a : joint space
Fig. 2.
kinematic data of Arm 1 (1×6); b: joint space kinematic data of Arm 2
(1×6); c : pose of Arm 1 w.r.t its base frame (4×4); d : pose of Arm 2
w.r.t its base frame (4×4); e: pose of Arm 1 w.r.t its base frame at t + 1;
f : pose of Arm 2 w.r.t its base frame at t + 1; g: 2D tracking target point
on the image captured by 6 cameras (6×2); h: computed 3D position of
the target point w.r.t Camera 3 (1×3)
needle insertion task is complex and involves highly nonlinear
interactions between the robot and the tissue. As such, the
framework allowing learning the controller to perform the
task can be used for other complex task too. The dataset
includes 60 successful needle insertion experiments. Dataset
has several interesting features opening avenues to future
studies in the context of Deep-RLfD: (1) the experiments
are recorded by 3 pairs of stereo cameras calibrated w.r.t.
the base frame of Arm 1 (in total 6 camera views as well
as their calibration parameters w.r.t. to Arm 1 and Arm 2
are available.). This allows future studies on generalising the
controller to different camera view. (2) Moreover, the camera
calibration provide useful information about the camera views,
hence, can be used by the model to improve the generalisation
capability of the model; (3) the dataset includes all the robot
data synched with the corresponding videos including the
joint position and end-effector pose at each time step;
(ii) We propose a novel Deep Robot Learning from Demonstra-
tions (D-RLfD) framework and employ state-of-the-art Deep
Neural Networks architectures in this framework. We illustrate
the effectiveness of our D-RLfD framework in learning control
actions (i.e. positions and orientations) for the robot hands to
insert a needle in a deformable tissue. Using the state of the
art DNN architectures, the developed D-RLfD yields high
accuracy in predicting control actions based on the visual
sensory information provided by a camera.

II. RELATED WORKS

Conventional RLfD [10],

imitation learning [11], be-
havioural cloning [12] and other such approaches usually
require low dimension feature describing robot workspace.
For instance, the obstacle position and velocity and initial
and goal point for planning and control the manipulative

of the earliest works on Robot Learning from Demonstration
(RLfD) that enabled further research in robotic surgery. For
instance, M. Power et al. [24] used continuous HMMs to
recognise the sub-tasks and learn needle passing and peg-
transfer with Haptic feedback to enhance teleoperation. GP
is used in Bi-manual knotting task with da Vinci robot [25]
to adapt the movement of the Arm 1 to the movements of
the Arm 2 and the rope shape. However, these are only
applied to low dimensional observations. Schulman et al.
[26] have used suturing trajectory transfer from human to a
Raven robot based on 3D point cloud data. Nonetheless, the
algorithm is not fully autonomous and requires a human’s
input for detecting the landmarks. Yang et al. [27] used DMPs
in a superﬁcial suturing task by decomposing the task into
simpler sub-tasks. However, it is limited to a single class of
superﬁcial suturing and the error values are not reported in
their work. In [28] RLfD is used in a vision-based swing
system in the textile industry. While the motion of the hand
which grips the needle is generated by an RLfD model, the
other robotic hand’s motion depends on the object on which
sewing is performed. These approaches are limited to low
dimensional observation and cannot be applied where only
high-dimensional visual information (video or images) is
available. Deep Neural Networks emerged as handy tools for
dealing with high dimensional visual observations.
Time series forecasting/vision in robotic surgery by DNNs
Wang et al. [29] used deep CNNs for skill assessments
in surgical robots by learning an expert deep model from
successful task completions. Zhou et al. [30] proposed a DNN
structure for sub-retinal injection and eye surgery with a high
level of accuracy for needle detection. Extending this concept
to learning a controller from a dataset is an interesting research
question. [31] uses CNN and LSTM in a network to estimate
surgical tool’s tip’s position and velocity with video sequence
input on JIGSAW dataset [32] which is a surgical activity
dataset for human motion modeling for 3 elementary tasks
including suturing, knot-tying and needle-passing teleoperated
by eight expert subjects. The gestures deﬁned in suturing in
JIGSAW dataset does not include tissue manipulation and
both robotic hands manipulate only the needle and the suture.
[15], [16] used Kalman ﬁlter in a DNN for state estimation in
base-line tasks only in simulation. [33] uses DNN with auto-
encoders and MPC for robot control from raw images and is
applied only on a toy dataset. However, neither of these works
learns a robot controller from sample demonstrations. We
present a dataset of inserting needle into a deformable tissue
suitable for deep-RLfD approaches. The dataset includes
videos captured by 6 cameras calibrated w.r.t. the robot base
frame, along with the corresponding calibration parameters,
robot states–position and orientation (pose) of the Arm 1 and
Arm 2–at time t, and the robot control inputs (which are the
desired pose of Arm 1 and Arm 2 at time t + 1). We also
present some baseline deep NN models which use a complex
feature extractor on a real-world robotic image dataset to
generate latent vector concatenated with robot state data and
calibration parameters and fed into a recurrent neural network
for deep time series prediction.

Fig. 3. Sample image sequence from Camera 1 (top-row) and Camera 4
(bottom-row) viewpoints. The task is to track the red cross sign which exist
in the videos. The green points show the initial position of the grippers in
pixel space and are just added here to illustrate the motion of the hands.

III. NEEDLE INSERTION DATA-SET

We have created a dataset of inserting a needle into a
deformable object using a da Vinci Research Kit (DVRK), see
Fig. 2 for the experimental setup used for data collection. The
phantoms (deformable objects used in our experiments) are
made of homogeneous polyethylene whose Young’s modulus
is circa 0.02 0.04 [GPa]. Inserting the circular needle in
(and piercing it thorough) a soft tissue (Fig. 1(b)) deforms
the tissue such that not enough tissue supports the stitch
which may result in failure as the stitch may cut the tissue.
In robotic surgery, Arm 2 is used to manipulate the soft
tissue (Fig. 1(c)) to ensure the needle pass/exits through the
desired point yielding enough gripped tissue by the stitch.
This procedure is delicate and requires signiﬁcant training
by novice surgeons and automating this by Deep-RLfD is of
notable interest.

The experimental setup we used for data collection is shown
in Fig. 2. We use two Patient-Side Manipulators (PSMs) from
the da Vinci Research Kit (namely Arm 1 and Arm 2 in Fig. 2)
to perform needle insertion in a soft and deformable object.
The circular needle is ﬁxed by a Large Needle Driver (LND)
on Arm 1 and ProGrasp Forcep (PGF) on Arm 2 is utilised to
grip the tissue and manipulate it. Three pairs of ﬁxed stereo
cameras are used to record the video of the experiments
in which the desired exit point of the needle on the tissue
surface and the needle tip is marked and tracked. All the
cameras are calibrated and the calibration parameters of the
left cameras of each stereo camera w.r.t base frame of Arm
1 (a1 Tli where i denotes the camera number) and Arm 2
(a2Tli) are included in the dataset. Moreover, the calibration
parameters of the right camera in every stereo camera is also
included in the dataset (riTli). One can readily obtain the
eye-to-hand transformations between the right cameras and
Arm 1 or Arm 2 by multiplying two transformation matrix
(a1Tri = a1 Tli

li Tri).

The 3D pose of the surgical needle is online localised [34]
during the dual-arm robot movements using the extended
forward kinematic. There is no information regarding me-
chanical properties of the tissue in the dataset. Each camera
captures videos with 310x244 resolution at 25 fps. The task

averagely takes ∼170-200 control loops (costing ∼7-10s) for
completion. We performed 60 needle insertion and piercing
through soft tissue with different tissue conﬁgurations and
collected all the corresponding information, including (i)
videos with 6 different camera views, (ii) the corresponding
hand-to-eye calibration parameters, (iii) poses of Arm 1 and
Arms 2 at time t (iv) control commands–desired pose for Arm
2, (v) the tracking of the needle tip during piercing through
the tissue, and (vi) the desired needle exit point on each
video at each time t. The exit point at each trial is randomly
selected on the tissue surface (tracked and visualised in the
videos by a red cross mark on the image). The initial grasping
conﬁguration of Arm 2 on tissue also differs across different
trials.

The dataset we collected consists of an array of cells at
each sampling time step t (each trial may have 140-200 cells
depends on the task duration.) where each cell includes a :
kinematic data of Arm 1 in joint space (1×6); b: kinematic
data of Arm 2 in joint space (1×6); c: pose (SE(3) where
SE(3) denotes the group of 3D poses2–3D position p and
3D orientation r) of Arm 1 w.r.t its base frame (4×4); d :
pose of Arm 2 w.r.t its base frame (4×4); e: pose of Arm
1 w.r.t its base frame at t + 1; f : pose of Arm 2 w.r.t its
base frame at t + 1; g: 2D tracking target point on the image
captured by 6 cameras (6×2); h: computed 3D position of
the target point w.r.t Camera 3 (1×3).

Thus, the robot state is denoted as a transformation matrix:

St =

(cid:20)Rt
0

(cid:21)

pt
1

=







r11
r21
r31
0

r12
r22
r32
0

r13
r23
r33
0







px
py
pz
1

(1)

Furthermore, the camera calibration data of the system is
saved which contain: (i) eye-to-hand calibration results (the
pose of left camera of the pair in stereo camera Px w.r.t. arm
Rx), (ii) calibration parameters of three stereo camera pairs
(P1: left pair, P2: middle pair, P3: right pair in Fig. 2(a)). We
decided to use unit quaternions to represent poses which is
robust to the singularity issue, a common issue with other
choices of orientation representations.

We preprocessed the dataset and created images from the
video frames corresponding to the sampling time of the robot
data collected during the experiments; the images are cropped
and resized to 224 × 224. As an illustration, ﬁgure 3 shows
a sample sequence of four images captured with two seconds
intervals from Camera 1 and Camera 4. The red cross sign in
the videos shows the desired needle tip exit point which needs
to be tracked. We also converted all the rotation matrices
to quaternions. The state of the robot is represented by a 7
dimensional vector St = [q0, q1, q2, q3, px, py, pz] ∈
R1×7 including quaternion and position.

IV. MODEL DESCRIPTION
Although we would ideally need low dimensional features
representing the state of the system, our observation consists

2SE(3) = R3 × SO(3) and SO(3) ⊂ R3×3 denotes the group of

rotations in three dimensions

of high dimensional sensory visual information. This is a
complex and highly non-linear function of the system’s state.
In classical computer vision methods, hand-designed features
can be extracted from these high-dimensional observations.
We recorded videos of task execution by the robot via six
high resolution calibrated cameras (with 310x244 pixels and
FPS=25). We exploit DNNs to extract the low dimensional
features (described in Section model). Figure5 shows that
the image sequence goes into the CNN (as the feature
extractor) and the output ﬂattened layer of the CNN is then
concatenated with robot’s current state vector; this vector
represents system’s state and builds the latent vector which
we use as a time series for next state prediction. This part of
the network is trained independently and has the advantage
that the entire model is conditioned on observation at time step
t. We have tried several CNN architectures including SOTA
and well-know CNNs such as VGG19 and ResNet152V2 as
well as customised structures; and tried to choose the best
model for feature extraction.
We performed 60 needle insertion tasks with different tissue
conﬁguration where the insertion and exit points are randomly
selected and recorded in each experiment with 6 cameras
having different poses. As such, we collected 360 videos
which form our dataset. Having 6 different camera views
allows studying how the learned D-RLfD model generalises to
a new camera view. The camera calibration data represent the
pose of the camera frame w.r.t the robot’s base frame which
is also included in the dataset. The future vector extractor
DNNs, hence, consists of a CNN whose output is concatenated
with robot states, and calibration data (Fig. 4). In our
experimental setup Arm 1 has a predeﬁned trajectory and Arm
2 is controlled. In our baseline model implementation, our
feedforward model (Fig. 4) uses a few dense layers, namely
{Dense_1, Dense_2, Dense_3}, taking the concatenated
feature vector and generates the predicted control input for
the robot. The output of the network is the predicted robot’s
state at time t + 1, which is a 1 × 7 vector where the ﬁrst
four components are robot’s orientation in quaternion and the
last three components are robot’s Cartesian position.
We also implemented SOTA RNN networks suitable for time
series (and dynamical behaviour) forecasting as RNN learns
the time dependencies of the system’s states. Although the
acquired accuracy with the CNN could be sufﬁcient for other
applications, to increase the precision necessary in safety
critical and sensitivity tasks, such as surgical tasks, we have
also tested multiple RNN models, such as LSTM and GRU
cells.

V. EXPERIMENTAL EVALUATION
In this section, we compare the performance of different
D-RLfD architectures including simple feedforward convolu-
tional networks, fully general RNN, GRU, and LSTM models.
We have tuned and tested several network architectures.
We only include the networks architectures with the best
performance in the manuscript where the information about
other networks can be found in supplementary materials.
To measure the performance of the proposed approach, we

Fig. 4. Feed-forward CNN model

use a series of metrics, including (i) mean square error
(MSE) to illustrate that our approach achieved sufﬁcient
amount of accuracy in position and orientation regression;
(ii) the maximum position error– MaxPE = max(ep) where
ep(t + 1) = (cid:107)Sp(t + 1) − ˆSp(t + 1)(cid:107), Sp(t + 1) and ˆSp
are the commanded position and the corresponding value
predicted by our D-RLfD at time t + 1; (iii) average position
(cid:80) ep(t + 1) where n is the number of
error, AvePE = 1
n
samples in each trial; (iv) the maximum orientation error–
MaxOE = max (eo) where eo(t + 1) is the quaternion
distance (see Eq. (2)) between actual commanded orientation
and the value predicted by our netweorks; (v) average
(cid:80) eo(t + 1). We have used
orientation error, AveOE = 1
n
mean squared error (MSE) which has the Euclidian distance
of the actual position commands in the dataset and the one
generated by our D-RLfD networks. As for the orientation,
we compute the quaternion distance, deﬁned in Eq. 2 as the
error:

eo =

1
T

T
(cid:88)

t=1

(1− < q2(t + 1), ˆq2(t + 1) >2)

(2)

where < q1, q2 > is the inner product of two quaternions. n
varies across different trials between 170 to 200 samples. In
general, we have 65,000 images/samples for the 60 tests.
The dataset is split into 80% and 20% for training and
testing, respectively, where 20% of training data are for cross
validation. The observation space can be denoted by Oi
t where
i = 1, 2, ..., 6 is the camera number and t is discrete time.
For each time step we have six images and the corresponding
robot state S(t). The Loss parameter in the tables show the
mean absolute error loss function value on the test set for
each network
A. Base-line Test with feed-forward model

Our baseline implementation contains a CNN, concatena-
tion layer combining latent features vector with robot’s state
data, and a few dense layers making the prediction of the
control commands, as shown in Fig. 4. The desired exit point
is marked with a red cross in the videos (as shown in ﬁgure
3) where the networks needs to output the the desired pose
(position Sp ∈ R3 and orientation ) of Arm 2 at t + 1 based
on the noisy, high-dimensional pixel data. We have tested
various CNN architectures, e.g. AlexNet, VGG19, ResNet

TABLE I
EVALUATION OF D-RLFD WITH CNN ARCHITECTURES

MaxPE† AvgPE‡ MaxOE
Model
2.518
1.983
0.212
AlexNet
1.641
1.65
0.166
VGG19
1.077
0.825
0.173
MobileNet
1.631
0.964
0.280
ResNet
Our CNN
1.528
0.755
0.154
† PE in [mm] and OE in degree
‡ e−3

AvgOE
1.027
0.267
0.389
0.477
0.475

Loss
0.053
0.054
0.049
0.050
0.005

TABLE II
EVALUATION OF THE D-RLFD WITH CNN ARCHITECTURE: TEST-SET
INCLUDES ONLY ONE CAMERA DATA UNSEEN IN TRAINING.

Test Camera MaxPE† AvgPE‡ MaxOE† AvgOE†

0.30
0.26
0.19
0.27
1.41
1.62

Camera1
Camera2
Camera3
Camera4
Camera5
Camera6

0.61
0.79
0.44
0.89
4.24
2.89
† PE in [mm] and OE in degree
† e−2

14.90
6.66
8.42
11.68
14.96
5.90

2.40
2.33
2.17
2.25
2.51
2.56

Loss
0.021
0.014
0.012
0.019
0.018
0.023

and our customised CNN (see Fig. 4), and the corresponding
results are presented in Table I. According to the results,
the customised model outperforms other tested approaches
in terms of MaxPE, AvePE, MaxOE and AveOE (position
values are in mm for xyz translation and orientation values
are in degree for euler angles). In this speciﬁc application the
maximum error value plays a vital role; since larger bound on
the maximum error indicates the risk of damaging the tissue
under operation and our CNN yields very small MaxPE and
MaxOE values indicating an acceptable margin of needle’s
tracking error during a suturing task.

B. One-Camera as Test set with feedforward model

In Section V-A, we used all the 6 camera views to form
the training and test set. However, this does not illustrates
whether the learned model performs well if the camera is
located at a new pose. We are also interested in understanding
how a model trained on some camera views generalises to a
new camera view unseen during training. So, we considered
data corresponding to 5 camera views as the training set and
the data corresponding to the 6th camera view as the test set.
Table II presents the results where one Camera i is excluded
in the training and used only for testing. We only use the

Fig. 5. System demonstration: Robot agent, Camera, and the network including CNN, concatenation and RNN blocks.

customised CNN shown in Fig. 4 in this section. Hence, the
error values in Table II must be compared to those in the
last rows of Table I in which all six cameras are included
in the training set. As table II shows only the position error
values for camera5 and camera6 are relatively higher but
considering the 10−2 order still in an acceptable range; and
it is due to the relative distance of the third stereo pair to the
other two.
C. Base-line Test with RCNs

To further improve the performance D-RLfD model to
generate a time series of control commands, we used the CNN
with the best performance in the Section V-A (which generates
the latent vector) and build several recurrent networks. RNNs
enable the model to use both the observation and dynamic of
the system to generate the control commands. Table 3 presents
the results for RNN, GRU, LSTM and simple feed-forward
model. All the recurrent networks yield improved precision
and accuracy of the position and orientation compared to the
feed-forward model where the results indicate LSTM has the
lowest error values.

D. Calibration data Augmentation

Thus far, we have used the image data and robot’s current
state, i.e. kinematic data, as the input to our D-RLfD models.
However, calibration data carries some relevant information

TABLE III
EVALUATION OF RCNS INCLUDING FULLY GENERAL RNN, GRU
AND LSTM.

MaxPE† AvgPE† MaxOE† AvgOE†
0.154
0.068
0.029
0.035

0.475
0.173
0.119
0.106

1.528
0.704
0.370
0.338

Model
Feedforward
RNN
GRU
LSTM

0.755
0.153
0.188
0.134
† PE in [mm] and OE in degree
† e−4

Loss
0.005
0.0097
0.0051
0.0048

TABLE IV
RESULTS OF THE FEED-FORWARD D-RLFD WITH AUGMENTED

CALIBRATION DATA CORRESPONDING WITH THE RESULTS PRESENTED IN

Test Camera MaxPE† AvgPE† MaxOE† AvgOE†

TABLE II.

Camera1
Camera3
Camera5

† e−3

0.19
0.31
1.07

0.70
0.58
0.78

7.28
4.93
8.53

2.05
4.60
1.50

Loss
0.019
0.010
0.015

of cameras position/orientation w.r.t robot’s base frame which
may improve the performance of our models. We, thus,
reﬁned our baseline feed-forward model so that the calibration
data is also concatenated with the latent feature vector
(ﬁgure 4). Here, for the proof of concept we only present
the results obtained by reﬁned feed-forward model (whose
basic implementation results are presented in Table II) with
augmented calibration data of Camera 1, 3 and 5. Table
IV presents the results of feed-forward D-RLfD model with
augmented calibration data which indicates improved accuracy
of control command prediction in terms of Loss and some
of the errors (presented with bold face). However, some
of the error values became worse (probably) because we
concatenated the calibration data with the feature vector and
network does not have enough layer after that to learn the
correlation between the calibration data and feature vector.

VI. CONCLUSION

This paper presents a dataset of inserting a circular needle
with a da Vinci Research Kit (DVRK) into a soft tissue
to be passed through the desired given exit point. The
dataset is suitable for data-driven approaches and consists
of 60 successful needle insertion trials. The videos of each
trial along with the corresponding robot’s data, calibration
parameters and commanded robot’s inputs are included
in the dataset. We also presented some baseline Deep-
Robot Learning from Demonstration (D-RLfD) networks. Our
results show RCNs outperform simple feed-forward NNs (as
expected), where augmenting the camera calibration data with
the feature vector (outputs from the CNNs) improves some
of the error metrics whereas other error metrics get slightly
worse. Moreover, the baseline feed-forward networks trained
on 5 camera view as train sets can successfully generalise to
the unseen camera view captured by camera 6.
In future works, we will study more sophisticated D-RLfD
architecture to improve the performance of control command
inference. We will also consider different architectures with
enough layers to capture the correlation between the feature
vector and calibration data and study a normalised error metric
as Euclidean and quaternion distances may be expressed in
different scales. Moreover, we will consider an extensive
study on generalising to an unseen camera view.

[22] P. Chatelain, A. Krupa, and N. Navab, “3d ultrasound-guided robotic
steering of a ﬂexible needle via visual servoing,” in 2015 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2015, pp. 2250–2255.

[23] C. G. Atkeson and S. Schaal, “Learning tasks from a single demon-
stration,” in Proceedings of International Conference on Robotics and
Automation, vol. 2.
IEEE, 1997, pp. 1706–1712.

[24] M. Power, H. Raﬁi-Tari, C. Bergeles, V. Vitiello, and G.-Z. Yang,
“A cooperative control framework for haptic guidance of bimanual
surgical tasks based on learning from demonstration,” in 2015 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2015, pp. 5330–5337.

[25] T. Osa, K. Harada, N. Sugita, and M. Mitsuishi, “Trajectory planning
under different initial conditions for surgical task automation by
learning from demonstration,” in 2014 IEEE International Conference
on Robotics and Automation (ICRA).

IEEE, 2014, pp. 6507–6513.

[26] J. Schulman, A. Gupta, S. Venkatesan, M. Tayson-Frederick, and
P. Abbeel, “A case study of trajectory transfer through non-rigid
registration for a simpliﬁed suturing scenario,” in 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems.
IEEE,
2013, pp. 4111–4117.

[27] D. Yang, Q. Lv, G. Liao, K. Zheng, J. Luo, and B. Wei, “Learning from
demonstration: dynamical movement primitives based reusable suturing
skill modelling method,” in 2018 Chinese Automation Congress (CAC).
IEEE, 2018, pp. 4252–4257.

[28] B. Huang, A. Vandini, Y. Hu, S.-L. Lee, and G.-Z. Yang, “A vision-
guided dual arm sewing system for stent graft manufacturing,” in 2016
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2016, pp. 751–758.

[29] Z. Wang and A. M. Fey, “Deep learning with convolutional neural
network for objective skill evaluation in robot-assisted surgery,” Inter-
national journal of computer assisted radiology and surgery, vol. 13,
no. 12, pp. 1959–1970, 2018.

[30] M. Zhou, X. Wang, J. Weiss, A. Eslami, K. Huang, M. Maier, C. P.
Lohmann, N. Navab, A. Knoll, and M. A. Nasseri, “Needle localization
for robot-assisted subretinal injection based on deep learning,” in 2019
International Conference on Robotics and Automation (ICRA).
IEEE,
2019, pp. 8727–8732.

[31] A. Marban, V. Srinivasan, W. Samek, J. Fernández, and A. Casals,
“Estimating position & velocity in 3d space from monocular video
sequences using a deep neural network,” in Proceedings of the IEEE
International Conference on Computer Vision Workshops, 2017, pp.
1460–1469.

[32] Y. Gao, S. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C.
Lin, L. Tao, L. Zappella, B. Béjar, D. D. Yuh, et al., “Jhu-isi gesture
and skill assessment working set (jigsaws): A surgical activity dataset
for human motion modeling,” in Miccai workshop: M2cai, vol. 3, 2014,
p. 3.

[33] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, “Embed
to control: A locally linear latent dynamics model for control from
raw images,” in Advances in neural information processing systems,
2015, pp. 2746–2754.

[34] F. Zhong, Y. Wang, Z. Wang, and Y.-H. Liu, “Dual-arm robotic needle
insertion with active tissue deformation for autonomous suturing,” IEEE
Robotics and Automation Letters, vol. 4, no. 3, pp. 2669–2676, 2019.

REFERENCES

[1] G. S. Guthart and J. K. Salisbury, “The intuitive/sup tm/telesurgery
system: overview and application,” in Proceedings 2000 ICRA. Mil-
lennium Conference. IEEE International Conference on Robotics and
Automation. Symposia Proceedings (Cat. No. 00CH37065), vol. 1.
IEEE, 2000, pp. 618–621.

[2] A. Mirbagheri, F. Farahmand, S. Sarkar, A. Alamdar, M. Moradi, and
E. Afshari, “The sina robotic telesurgery system,” in Handbook of
Robotic and Image-Guided Surgery. Elsevier, 2020, pp. 107–121.
[3] D. J. Jacofsky and M. Allen, “Robotics in arthroplasty: a comprehensive
review,” The Journal of arthroplasty, vol. 31, no. 10, pp. 2353–2363,
2016.

[4] S. G. Møller, N. Dohrn, S. K. Brisling, J. C. Larsen, and M. Klein,
“Laparoscopic versus robotic-assisted suturing performance among
novice surgeons: A blinded, cross-over study,” Surgical Laparoscopy,
Endoscopy & Percutaneous Techniques, vol. 30, no. 2, pp. 117–122,
2020.

[5] C. Ng, W. Liang, C. W. Gan, H. Y. Lim, and K. K. Tan, “Precision
motion control using nonlinear contact force model in a surgical device,”
in 2019 41st Annual International Conference of the IEEE Engineering
in Medicine and Biology Society (EMBC).
IEEE, 2019, pp. 5378–5381.
[6] K. Hauser, R. Alterovitz, N. Chentanez, A. Okamura, and K. Goldberg,
“Feedback control for steering needles through 3d deformable tissue
using helical paths,” Robotics science and systems: online proceedings,
p. 37, 2009.

[7] P. Sundaresan, B. Thananjeyan, J. Chiu, D. Fer, and K. Goldberg,
“Automated extraction of surgical needles from tissue phantoms,” in
2019 IEEE 15th International Conference on Automation Science and
Engineering (CASE).

IEEE, 2019, pp. 170–177.

[8] R. Alterovitz, K. Goldberg, and A. Okamura, “Planning for steerable
bevel-tip needle insertion through 2d soft tissue with obstacles,” in
Proceedings of the 2005 IEEE international conference on robotics
and automation.

IEEE, 2005, pp. 1640–1645.
[9] K. B. Reed, A. Majewicz, V. Kallem, R. Alterovitz, K. Goldberg, N. J.
Cowan, and A. M. Okamura, “Robot-assisted needle steering,” IEEE
robotics & automation magazine, vol. 18, no. 4, pp. 35–46, 2011.
[10] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of
robot learning from demonstration,” Robotics and autonomous systems,
vol. 57, no. 5, pp. 469–483, 2009.

[11] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, “Imitation learning:
A survey of learning methods,” ACM Computing Surveys (CSUR),
vol. 50, no. 2, pp. 1–35, 2017.

[12] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from

observation,” arXiv preprint arXiv:1805.01954, 2018.

[13] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video
prediction beyond mean square error,” arXiv preprint arXiv:1511.05440,
2015.

[14] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for
physical interaction through video prediction,” in Advances in neural
information processing systems, 2016, pp. 64–72.

[15] P. Becker, H. Pandya, G. Gebhardt, C. Zhao, J. Taylor, and G. Neumann,
“Recurrent kalman networks: Factorized inference in high-dimensional
deep feature spaces,” arXiv preprint arXiv:1905.07357, 2019.
[16] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop kf: Learning
discriminative deterministic state estimators,” in Advances in Neural
Information Processing Systems, 2016, pp. 4376–4384.

[17] H. Saeidi, H. N. Le, J. D. Opfermann, S. Léonard, A. Kim, M. Hsieh,
J. U. Kang, and A. Krieger, “Autonomous laparoscopic robotic suturing
with a novel actuated suturing tool and 3d endoscope,” in 2019
International Conference on Robotics and Automation (ICRA).
IEEE,
2019, pp. 1541–1547.

[18] D.-L. Chow, R. C. Jackson, M. C. Çavu¸so˘glu, and W. Newman, “A
novel vision guided knot-tying method for autonomous robotic surgery,”
in 2014 IEEE International Conference on Automation Science and
Engineering (CASE).

IEEE, 2014, pp. 504–508.

[19] R. C. Jackson and M. C. Çavu¸so˘glu, “Needle path planning for
autonomous robotic surgical suturing,” in 2013 IEEE International
Conference on Robotics and Automation.
IEEE, 2013, pp. 1669–1675.
[20] A. Shademan, R. S. Decker, J. D. Opfermann, S. Leonard, A. Krieger,
and P. C. Kim, “Supervised autonomous robotic soft tissue surgery,”
Science translational medicine, vol. 8, no. 337, pp. 64–75, 2016.
[21] G. Zong, Y. Hu, D. Li, and X. Sun, “Visually servoed suturing for
robotic microsurgical keratoplasty,” in 2006 IEEE/RSJ International
Conference on Intelligent Robots and Systems.
IEEE, 2006, pp. 2358–
2363.

