On the Generalizability of Neural Program Models with respect
to Semantic-Preserving Program Transformations

Md Rafiqul Islam Rabin
University of Houston

Nghi D. Q. Bui
Singapore Management University

Ke Wang
Visa Research

Yijun Yu
The Open University

Lingxiao Jiang
Singapore Management University

Mohammad Amin Alipour
University of Houston

1
2
0
2

r
a

M
8
1

]
E
S
.
s
c
[

3
v
6
6
5
1
0
.
8
0
0
2
:
v
i
X
r
a

ABSTRACT
Context: With the prevalence of publicly available source code
repositories to train deep neural network models, neural program
models can do well in source code analysis tasks such as predict-
ing method names in given programs that cannot be easily done
by traditional program analysis techniques. Although such neural
program models have been tested on various existing datasets, the
extent to which they generalize to unforeseen source code is largely
unknown. Objective: Since it is very challenging to test neural pro-
gram models on all unforeseen programs, in this paper, we propose
to evaluate the generalizability of neural program models with
respect to semantic-preserving transformations: a generalizable
neural program model should perform equally well on programs
that are of the same semantics but of different lexical appearances
and syntactical structures. Method: We compare the results of var-
ious neural program models for the method name prediction task
on programs before and after automated semantic-preserving trans-
formations. We use three Java datasets of different sizes and three
state-of-the-art neural network models for code, namely code2vec,
code2seq, and GGNN, to build nine such neural program models
for evaluation. Results: Our results show that even with small
semantically preserving changes to the programs, these neural pro-
gram models often fail to generalize their performance. Our results
also suggest that neural program models based on data and control
dependencies in programs generalize better than neural program
models based only on abstract syntax trees (ASTs). On the positive
side, we observe that as the size of the training dataset grows and
diversifies the generalizability of correct predictions produced by
the neural program models can be improved too. Conclusion: Our
results on the generalizability of neural program models provide
insights to measure their limitations and provide a stepping stone
for their improvement.

KEYWORDS
neural models, code representation, model evaluation, program
transformation, generalizability

1 INTRODUCTION
Abundance of publicly available source code repositories has en-
abled a surge in data-driven approaches to programs analysis tasks.
Those approaches aim to discover common programming patterns
for various downstream applications [3] that are not easily achiev-
able via traditional program analysis techniques, e.g., prediction of
data types in dynamically typed languages [23], detection of the

variable naming issues [4], or repair of software defects [17]. The ad-
vent of deep neural networks has accelerated the innovation in this
area and has greatly enhanced the performance of these approaches.
The performance of deep neural networks in cognitive tasks such
as method name prediction or variable naming has reached or
exceeded the performance of other data-driven approaches. The
performance of neural networks has encouraged researchers to in-
creasingly adopt neural networks in program analysis tasks, giving
rise to increasing uses of neural program models.

While the performance of neural program models continues to
improve, the extent to which they can generalize to new, unseen
programs is still unknown, even if the programs are in the same
programming language. This problem is of more importance if
we want to use them in downstream safety-critical tasks, such
as malware detection and automated defect repair. This problem
is particularly hard, as the interpretation of neural models that
constitute the core reasoning engine of neural program models
remains challenging—especially for the complex neural networks
(e.g., RNN) that are commonly used in the proposed neural program
models.

A comprehensive understanding of the extent of generalizability
of neural program models would help developers to know when to
use data-driven approaches and when to resort to traditional deduc-
tive methods of program analysis. It would also help researchers to
focus their efforts on devising new techniques to alleviate the short-
comings of existing neural program models. Lack of knowledge
about the limits of neural program models may exaggerate their
capability and cause careless applications of the neural program
models on the domains that they are not suited for; or, spending
time and efforts on developing neural program models while a tra-
ditional, more understandable technique can perform equally well
or better.

Recently, we have seen a growing interest in the rigorous eval-
uation of neural program models. Wang and Christodorescu [52]
compared the robustness of different program representations un-
der compiler optimization transformations. They found that the
program representations based on static code features are more
sensitive to such changes than dynamic code features. Allamanis
[1] evaluated the impact of code duplication in various neural pro-
gram models and found that code duplication in the training and
test datasets inflated the performance of almost all current neural
program models. More recently, preliminary studies in this field
started to emerge; e.g., Rabin and Alipour [40], Rabin et al. [42]
proposed the idea of testing neural program models using semantic-
preserving transformations; Bui et al. [13] measured the impact of a
specific code fragment by deleting it from the original source code;

 
 
 
 
 
 
Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

Zhang et al. [60] proposed a sampling approach to generate adver-
sarial examples for code classification models; and Compton et al.
[15] showed that the obfuscation of variable names makes a model
on source code more robust with less bias towards variable names.
Further, Yefet et al. [58] followed and proposed adversarial example
generation for neural program models using prediction attribution
[47]; Ramakrishnan et al. [43] increased robustness of neural rep-
resentations of code by adding semantically equivalent programs
to the training data; and Bielik and Vechev [11] proposed an ap-
proach for increasing the robustness of neural program models for
type prediction based on finding prediction attribution, adversarial
training, and refining source code representations. Although these
studies share the similar ultimate goal of evaluating and improving
the performance of neural program models with respect to unseen
programs, there is still a lack of systematic quantifiable metrics to
measure the extent to which the neural program models can gener-
alize to unseen programs, and it would not be fair either to evaluate
a neural program model against all possible unseen programs that
it was not designed for.
Goal. In this paper, we attempt to understand the limits of gener-
alizability of neural program models by comparing their behavior
before and after semantic-preserving program transformations.
That is, how the results of a neural program model generalize
to a semantically-equivalent program. By limiting unseen pro-
grams to semantically equivalent ones and controlling the semantic-
preserving program transformations, we are able to provide a fair,
systematic, quantifiable metric for evaluating the generalizability
of a neural program model.

In this paper, we report the results of a study on the generaliz-
ability of three highly-cited neural program models: code2vec [7],
code2seq [6], and GGNN [19]. To evaluate their generalizability, we
transform programs in the original datasets for testing to generate
semantically-equivalent counterparts. We employ six semantic-
preserving transformations that impact the structure of programs
(i.e. abstract syntax trees) with varying degrees, ranging from com-
mon refactoring, e.g., variable renaming, to more intrusive changes
such as changing for-loops to while-loops.

Our results suggest that all neural program models evaluated in
this study are sensitive to the semantic-preserving transformations;
that is, the output of the neural program model would be different
on transformed programs compared to its output on the original
programs. This sensitivity remains an issue even in the cases of
small changes to the programs, such as renaming variables or re-
ordering independent statements in a basic block. Moreover, our
results suggest that neural program models (e.g., GGNN ) that en-
code data and control dependencies in programs generalize better
than the neural program models that are solely based on abstract
syntax trees, and in most cases the generalizability of a neural pro-
gram model can be improved with the growth in the size of training
datasets.

The results of this study reveal that the generalizability of neural
program models is still far from ideal and require more attention
from the research community to devise more generalizable models
of source code, or designing pre-processing techniques, e.g. canoni-
calizing program representations, to increase immunity of neural
program models to such program transformations.

Compared to closely related work by Yefet et al. [58] and Ramakr-
ishnan et al. [43] where their goals are adversarial code generation
and increasing robustness of neural program models, this paper
provides a complementary view to the evaluation of neural program
models by focusing on the evaluation of generalizability of neu-
ral program models with a large number of transformations, and
in-depth analysis of changes in their behavior on transformed pro-
grams. This paper also evaluates the impact of the size of datasets
and programs on the generalizability of neural program models.
Contributions. This paper makes the following contributions.
• We introduce the notion of generalizability with respect to
semantic-preserving transformations for neural program models.
• We perform a large-scale study to evaluate the generalizability of
state-of-the-art neural program models. We also provide insights
into the generalizability of existing neural program models and
discuss their practical implications.

• We provide an in-depth analysis of changes in the prediction and
evaluate the impact of the size of datasets and programs on the
generalizability of neural program models.

2 MOTIVATING EXAMPLE & DEFINITION
We use code2vec [7] for exposition in this section. The code2vec [7]
is a recent, highly-cited (200+ citations as of Nov. 2020) neural
program model that predicts the name of a Java method given
the body of the method. Such a neural program model can assist
developers in classification of methods, code similarity detection,
and code search.

Figure 1 shows two semantically-identical methods that imple-
ment compareTo functionality. The only difference between them
is in the name of one of the variables. The left snippet in Figure 1
uses other, while the code on the right uses var01. However, the
code2vec outputs, i.e., predictions, on these semantically equivalent
programs are drastically different. code2vec predicts the snippet on
the left to be compareTo function, and the function on the right to
be getCount. It seems that the predictions of code2vec rely much
on the identifier names (e.g., other). This reliance would make
code2vec susceptible to a common refactoring such as variable re-
naming, and would make it not generalize to the code snippets
that are semantically the same, but are different syntactically, even
under common transformations.

Lack of generalizability would lead to distrust in the neural pro-
gram models and hamper their wider adoption and application. If
such neural program models were to be deployed in the problem
settings wherein higher levels of generalizability are required, e.g.,
malware detection and bug repair, it would be much better for the
neural program models to demonstrate a high level of generaliz-
ability with respect to certain metrics.
Generalizability. We define generalizability as the capability of a
neural program model to return the same results under semantic-
preserving transformations.

In this paper, we differentiate generalizability from the term ro-
bustness that is commonly used in the neural network literature [48]
for two main reasons. First, robustness is usually defined in the face

1var0 is not an uncommon identifier name in Java as it appears in the training vocab-
ulary of the datasets. At the time of writing, a search on the GitHub returns more than
75K Java classes that use this identifier.

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

public int compareTo ( ApplicationAttemptId other) {

public int compareTo ( ApplicationAttemptId var0) {

int compareAppIds = this . getApplicationId ()
. compareTo (other. getApplicationId () );

int compareAppIds = this . getApplicationId ()
. compareTo (var0. getApplicationId () );

if ( compareAppIds == 0) {

if ( compareAppIds == 0) {

return this . getAttemptId () - other.

return this . getAttemptId () - var0.

getAttemptId () ;

} else {

return compareAppIds ;

}

}

getAttemptId () ;

} else {

return compareAppIds ;

}

}

Prediction before transformation: compareTo

Prediction after transformation: getCount

Figure 1: Variable Renaming on java-small/test/hadoop/ApplicationAttemptId.java file.

of adversarial examples that have security implications, while we
do not generate adversarial examples. Second, robustness implies
imperceptible differences in the two focal inputs (e.g., minor pixel
changes in two images) that are hard to attain in a sparse domain
such as source code; the program transformations used in this paper
often lead to perceptible changes of textual appearances and syn-
tactic structures in program code. We also note that our definition
of generalizability differs from what is used in [28] that evaluates
the usefulness of a neural program model in various downstream
tasks.

Together with clearly defined semantic-preserving program
transformations and their change impact on the prediction results
of neural program models (cf. Section 4), we aim to provide a
systematic quantifiable way to measure the generalizability of
neural program models, and thus shed lights on their capabilities
and limits for future improvements. With the extensibility of
the program transformations and the measurements of their
change impact, our evaluation approach may also be extended
to measure the generalizability of neural program models more
comprehensively in the near future.

3 BACKGROUND
Most neural program models use neural network classifiers in their
core components that take a code snippet or a whole program as an
input, and make predictions about some of its characteristics; e.g., a
bug prediction classifier that predicts the buggy-ness of statements
in the input program.

Performance of a neural program model depends on three main
factors: quality of data (i.e., source code for this study), the repre-
sentation of data for the neural network, and the neural network
characteristics and its training parameters.

Quality of the data is concerned with the representativeness of
data, and proper cleaning and preprocessing of the data. Currently,
most studies use open-source projects usually in mainstream pro-
gramming languages, e.g., C#, Java, C, or JavaScript. The available
datasets for these tasks are still very immature and not standardized,
and their quality is somewhat unknown. For example, a recent study
by Allamanis [1] showed that virtually all available datasets suffer
from code duplication that can greatly impact the performance of
neural program models.

The second factor affecting the performance of neural program
models is source code representations. Since neural networks need

to take vectors of numbers as direct inputs, source code embeddings
are used to produce a vector representation of source code. The
representation determines which program features to include and
how they should be represented in the vector embeddings. The
representations can be broadly categorized into two categories:
static and dynamic. Static program representations consider only
the features that can be extracted from parsing texts of the programs,
while dynamic representations include some features pertaining to
the real executions of the programs.

The third factor impacting the performance of a neural pro-
gram model is the characteristics—e.g., type, topology, and hyper-
parameters—of the neural networks it uses. There are numerous
choices of network architectures each with different properties.
Currently, the class of recurrent neural networks (e.g., LSTM) and
graph neural networks are among the most popular architectures
in neural program models [6, 7, 19].

4 EVALUATION APPROACH
Our approach for evaluating neural program models relies on a
metamorphic relation that states: the outputs of a neural program
model should not differ on semantically-equivalent programs. To this
end, the evaluation approach is divided into two main steps: (1)
generating new programs using semantic-preserving transforma-
tions, and (2) comparing the outputs of a neural program model
before and after the transformations to compute generalizability
metrics. We describe these steps in the rest of this section.

4.1 Target Downstream Task
We use the method name prediction task [2, 5] in this work to
evaluate the generalizability of neural program models. The goal
of the task is to predict the name of a method given the body of
the method. This task has several applications such as code search
[35], code summarization [5], and code analogies [7]. Figure 1
depicts an example of this task wherein neural program models
are given a method body and return candidate names for the
method body, i.e., compareTo and getCount. This task has been
used as the downstream task to evaluate several state-of-the-art
neural program models [4, 6, 7].

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

4.2 Transformations
In this work, we only evaluate neural program models that take a
method body as their input, therefore, we use the following set of
transformations that are applicable to method-level code to gener-
ate semantically-equivalent methods. This set includes transforma-
tions ranging from common refactoring like variable renaming to
more intrusive ones like loop exchange. The goal is to evaluate the
generalizability of neural program models under a wide range of
semantic-preserving changes to the structure of a method.
• Variable Renaming (VN) is a refactoring that renames the
name of a variable in a method. The new name of the variable
will be in the form of varN for a value of N such that N has not
been defined in the scope. VN is a widely-used refactoring for
methods.

• Permute Statement (PS) swaps two independent statements
(i.e., with no data or control dependence) in a basic block of a
method.

• Unused Statement (UN) inserts an unused string declaration
to a randomly selected basic block in a method. Unused variables
in methods are a common malpractice by developers.

• Loop Exchange (LX) replaces for loops with while loops or

vice versa.

• Switch to If (SF) replaces a switch statement in a method with

an equivalent if statement.

• Boolean Exchange (BX) switches the value of a boolean variable
in a method from true to false or vice versa, and propagates
this change in the method to ensure a semantic equivalence of
the transformed method with the original method.
Note that each transformation has different impact on the struc-

ture of methods as follows.
• The Variable Renaming transformation only changes the terminal

values and does not affect the structure of an AST.

• The Permute Statement transformation does not change the nodes,

rather it only reorders two subtrees in an AST.

• The Unused Statement transformation adds a few nodes into an

AST, which increases the number of paths in the AST.

• The Loop Exchange transformation extensively impacts an AST

by removing and inserting nodes.

• The Switch to If transformation also impacts the AST of a method

substantially by removing and inserting nodes.

• The Boolean Exchange transformation alters the value of true
or false and modifies the structure of an AST by removing or
inserting unary-not nodes.

4.3 Generalizability Metrics
In this study, we define a few metrics to measure different results
of a neural program model for transformed programs and thus to
quantify the generalizability of the neural program model.

Specifically, suppose 𝑀 denotes a set of methods, given a
semantic-preserving program transformation 𝑇 that takes a
method and creates a set 𝑀 ′ = (cid:208)𝑚 ∈𝑀 𝑇 (𝑚) of transformed
methods, and a neural program model 𝑁 𝑃𝑀 : 𝑀 → 𝐿, where 𝐿
denotes a set of labels, maps methods to labels. We evaluate the
generalizability of 𝑁 𝑃𝑀 with respect to the transformation 𝑇 , by
comparing 𝑁 𝑃𝑀 (𝑚) and 𝑁 𝑃𝑀 (𝑚′) for 𝑚′ ∈ 𝑇 (𝑚) for 𝑚 ∈ 𝑀.
Ideally, the neural program model should produce the same results

on both 𝑚 and 𝑚′, that is 𝑁 𝑃𝑀 (𝑚) = 𝑁 𝑃𝑀 (𝑚′). We define the
following metrics.

Prediction Change Percentage. We compute the prediction
change percentage as follows:

𝑃𝐶𝑃 =

|{𝑚′ ∈ 𝑀 ′|𝑁 𝑃𝑀 (𝑚) ≠ 𝑁 𝑃𝑀 (𝑚′)}|
|{𝑚′ ∈ 𝑀 ′}|

∗ 100.

(1)

The lower values of PCP for 𝑁 𝑃𝑀 would suggest higher a degree
of its generalizability with respect to the transformation.
Types of Changes. Considering that the correctness of predicted
labels of the 𝑁 𝑃𝑀, five types of changes can happen:
(1) a correct prediction remains correct after the transformation,
(2) a correct prediction changes to a wrong prediction after the

transformation,

(3) a wrong predicted label remains the same wrong label after

the transformation,

(4) a wrong prediction changes to a correct prediction after the

transformation,

(5) a wrong predicted label changes into a different, yet still wrong

label after the transformation.

We use the following five metrics to denote the proportion of each
of these cases in the experiments. CCP, CWP, WWSP, WCP, and
WWDP respectively denote the percentage of correct predictions
that stay correct, the percentage of correct predictions that be-
come wrong, the percentage of wrong predictions that stay to the
same wrong prediction after the transformation, the percentage
of wrong predictions that become correct, and the percentage of
wrong predictions that change to a different wrong prediction after
the transformation.
Precision, Recall, and F1-Score. We also use the traditional sub-
token metrics (precision, recall and 𝐹1-score) as commonly used
in the literature for the method name prediction task [6, 7] in this
generalizability study. Suppose, 𝑡𝑝 denotes the number of true
positive sub-tokens, 𝑓 𝑝 denotes the number of false positive sub-
tokens, and 𝑓 𝑛 denotes the number of false negative sub-tokens in
the predicted method names.
• Precision indicates the percentage of predicted sub-tokens that
are true positives. It is the ratio of the correctly predicted positive
sub-tokens to the total number of predicted positive sub-tokens:
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =

𝑡𝑝
𝑡𝑝 + 𝑓 𝑝

• Recall indicates the percentage of true positive sub-tokens that
are correctly predicted. It is the ratio of the correctly predicted
positive sub-tokens to the total number of sub-tokens in actual
method names: 𝑅𝑒𝑐𝑎𝑙𝑙 =

𝑡𝑝
𝑡𝑝 + 𝑓 𝑛
• F1-Score is the harmonic mean of precision (P) and recall (R):

𝐹1–𝑆𝑐𝑜𝑟𝑒 =

2

𝑃 −1+𝑅−1 = 2 . 𝑃 . 𝑅
𝑃 + 𝑅

For example, a predicted name result_compute has two sub-
tokens result and compute, and is considered as an exact match
of the ground-truth name computeResult which also has the
same two sub-tokens (ignoring the case and the ordering of the
tokens). Similarly, a predicted name compute has 100% precision
but only 50% recall with respect to the same ground truth, and
compute_model_result has 100% recall but only 67% precision.

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

5 EXPERIMENTAL SETTING
5.1 Subject Neural Program Models
The task of method name prediction [5] has attracted some attention
recently. We use three neural program models that use different
code representations and neural network characteristics for the
task: code2vec [7], code2seq [6], and GGNN [19].

code2vec [7] uses a bag of AST paths to model source code. Each
path consists of a pair of terminal nodes and the corresponding
path between them in the AST. Each path, along with source and
destination terminals, is mapped into its vector embeddings which
are learned jointly with other network parameters during training.
The separate vectors of each path-context are then concatenated
to a single context vector using a fully connected layer which is
learned during training with the network. An attention vector
is also learned with the network; it is used to score each path-
context and aggregate multiple path-contexts to a single code vector
representing a method body. After that, the model predicts the
probability of each target method name given the code vector of
the method body via a softmax-normalization between the code
vector and each of the embeddings of all possible target method
names.

While code2vec uses monolithic path embeddings and only gener-
ates a single label at a time, the code2seq [6] model uses an encoder-
decoder architecture to encode paths node-by-node and generate
labels as sequences at each step. In code2seq, the encoder represents
a method body as a set of AST paths where each path is compressed
to a fixed-length vector using a bi-directional LSTM which encodes
paths node-by-node. The decoder uses attentions to select relevant
paths while decoding, and predicts sub-tokens of a target sequence
at each step when generating the method name.

In GGNN [19], a variety of semantic edges are added into the
AST of a method body to construct a graph, and the Gated Graph
Neural Network (GGNN) is applied to encode such graphs [4]. The
initial embedding for a node of the graph is the concatenation
between the node type embedding and node token embedding.
Then a fixed number of message passing steps are applied for a
node to aggregate the embeddings of its neighbors. The output of
the GGNN encoder is then fed into a bi-directional LSTM decoder to
generate the method name as a language model of sub-tokens [19].

5.2 Datasets
We have used the code2seq dataset for training neural program
models for the study. There are three Java datasets based on the
GitHub projects: Java-Small, Java-Med, and Java-Large.

• Java-Small: This dataset contains 9 Java projects for training,
1 for validation and 1 for testing. Overall, it contains about
700K methods. The compressed size is about 366MB and the
extracted size is about 1.9GB.

• Java-Med: This dataset contains 800 Java projects for train-
ing, 100 for validation and 100 for testing. Overall, it contains
about 4M methods. The compressed size is about 1.8GB and
the extracted size is about 9.3GB.

• Java-Large: This dataset contains 9000 Java projects for
training, 200 for validation and 300 for testing. Overall, it

Table 1: Performance of trained models for method name predic-
tion in the testing dataset.

Model

Dataset

code2vec

code2seq

GGNN

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

# Original methods in the
testing dataset
44426
351628
370930

44426
351628
370930

44426
351628
370930

Precision Recall

𝐹1-Score

28.36
42.55
45.17

46.30
59.94
64.03

49.12
58.30
60.76

22.37
30.85
32.28

38.81
48.03
55.02

47.18
47.49
50.32

25.01
35.76
37.65

42.23
53.33
59.19

48.59
52.34
55.23

contains about 16M methods. The compressed size is about
7.2GB and the extracted size is about 37GB.

5.3 Training Models per Datasets
The authors of code2vec and code2seq have made the source code
public for training and evaluating their models. For GGNN , the
implementation of the network is available but the code graph
generation is not; so we re-implement the step to generate graphs.
We use the parser SrcSlice2, an extension of SrcML3, to produce
data dependency edges among AST nodes for training GGNN .

We train each model for the method name prediction task with
the configurations described in their original papers on each of the
three aforementioned datasets, and thus construct three code2vec,
three code2seq, and three GGNN neural program models. Similar
to the state-of-the-art approaches, i.e. [6, 7], we train models on the
training set, tune on the validation set for maximizing 𝐹1-score, and
finally report results on the unseen testing set. Table 1 summarizes
the performance of trained models for method name prediction on
the testing set. While the performance of our trained models for
code2seq is on par with to the ones reported in the corresponding pa-
per [6], the performance of code2vec did not reach the performance
reported in [7], due to the differences in the dataset. However, the
performance of our trained code2vec models is similar to the one
reported in [6]. For GGNN , the performance is reasonably different
from what were reported in [19] for mainly three reasons: (1) the
ASTs produced by our parser are different, (2) the extraction of
some types of semantic edges proposed in [19] requires expensive
analysis of the methods; therefore, we implemented and included
only a subset (seven out of ten) of semantic edges into the ASTs
when constructing the graphs, and (3) the datasets are different.

5.4 Population of Transformed Programs
We have used our own tool based on the JavaParser4 library to
transform Java methods. Henceforth we use terms program and
method interchangeably. Two authors were involved in the imple-
mentation, testing and code review. We have performed manual
inspection of sample transformed programs to ensure correctness
of the transformations.

2https://github.com/srcML/srcSlice
3https://www.srcml.org/, +400 node types for supporting multiple programming
languages
4https://github.com/javaparser/javaparser

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

We have applied the applicable transformations to the methods
available in the testing data of the three datasets mentioned in Sec-
tion 5.2. The number of original methods in our study is 1, 415, 116,5.
Overall, the number of original methods with incorrect predictions
is, on average, 2.8 times higher than the number of methods with
correct predictions.

We create a set of single-place transformed programs (Table 2) by
applying transformations to each eligible location in methods sepa-
rately resulting in 2, 822, 810 transformed methods; e.g., if a method
has three eligible locations for a transformation, we would generate
three distinct methods by transforming each individual location
separately. The types and number of applicable transformations
vary from a method to another. Therefore, in our approach, different
methods, based on the language features that they use, produce a
different number of transformed programs. In total, the number of
transformed programs generated from the programs with incorrect
initial predictions is much higher (4.2x and higher) than the number
of transformed programs generated from the programs with correct
initial predictions, which may suggest that programs with correct
predictions may be smaller and simpler.

Artifacts. The source code of the program transformation tool
and the datasets of the transformed programs used in this paper
are publicly available at https://github.com/mdrafiqulrabin/tnpa-
generalizability/.

5.5 Research Questions
In this paper, we seek to answer the following research questions.

RQ1 How do the transformations impact the predictions of neural

program models in the single-place transformed dataset?

RQ2 When do the transformations affect neural program models

the most?

RQ3 How does the method length impact the generalizability of

neural program models?

RQ4 What are the trends in types of changes?
RQ5 How do the transformations affect the precision, recall and

𝐹1-score of the neural program models?

6 RESULTS
6.1 RQ1: Impact of Transformations on the

Predictions of Neural Program Models
Table 2 shows the prediction change percentage (PCP) of the neu-
ral program models for each transformation and dataset. In this
table, “# Original methods” denotes the number of methods eligible
for the corresponding transformation, “# Transformed methods”
denotes the number of methods generated as the result of apply-
ing the corresponding transformations on the original methods,
and “Prediction change (%)” denotes PCP as defined in Section 4.
“Weighted Average” provides the weighted average of PCP for each
transformation and neural models. The bold values in the Table 2
highlight the minimum value of PCP for the transformations. Since
a transformation can be applied in more than one place separately

5This total number is different from the numbers in Table 1 because a method in the
testing dataset may contain code elements eligible for multiple types of transformations
and be counted multiple times.

in a method body, the number of transformed methods can be larger
than the number of original methods.

As Table 2 depicts, all neural program models are likely to sus-
ceptible to semantic-equivalent transformations; however, the im-
pact of transformations on PCP differs among different neural net-
works and datasets. Overall, GGNN seems less prone to prediction
changes; in 14 out of 18 cases, PCP in GGNN is significantly less
than code2vec and code2seq. Moreover, in four out of six transfor-
mations, the weighted average of PCP for GGNN is lower than the
rest.
(cid:15)

(cid:12)

Observation 1: In most cases, GGNN seems less susceptible
to prediction changes under semantic-preserving transforma-
tions, compared to code2vec and code2seq.

(cid:14)

(cid:13)

Within code2vec, code2seq, and GGNN , the PCP trend varies for
different transformations and datasets. code2vec is comparatively
most sensitive to Permute Statement on all datasets. On the other
hand, code2seq is most vulnerable to Switch to If in Java-Small,
Variable Renaming in Java-Med, and Boolean Exchange in Java-
Large. In GGNN , Switch to If is the most powerful transformation
on all datasets. In most cases, for code2vec and code2seq, the PCP
for Unused Statement is comparatively less than the other transfor-
mations, except for code2seq in Java-Large where Switch to If is
less sensitive. In GGNN , Permute Statement is a comparatively less
powerful transformation than others on all datasets. Overall, based
on the weighted average, it is likely that, code2vec is most sensi-
tive to Permute Statement and least sensitive to Unused Statement,
code2seq is most sensitive to Boolean Exchange and least sensitive
to Switch to If , and GGNN is most sensitive to Switch to If and least
sensitive to Permute Statement.

Based on the weighted average, GGNN performs worst for Switch
to If and Unused Statement transformations. These two transforma-
tions add some additional nodes and paths in the AST. For code2vec
and code2seq, if models give less attention to those new paths, then
the change can less effective. However, GGNN works by using a
message passing mechanism among the nodes with a limited num-
ber of passing steps. In Unused Statement, because there is some
irrelevant information added into the code, the passing steps in
GGNN can capture this information and ignore other useful in-
formation, thus having a strong impact on the prediction results.
In Switch to If , because the structure of the AST is modified by
adding and removing nodes, and GGNN is a node-based method,
i.e., combining node information with message passing, thus the
GGNN can sensitive to node modification in the AST for Switch to
If .

Table 2 also supports that, in most cases, Permute Statement is
more powerful than Variable Renaming in code2vec model whereas
Variable Renaming is more powerful than Permute Statement in
code2seq model. This is probably caused by the real-value em-
beddings of AST paths are different for code2vec and code2seq. In
code2vec, an embedding matrix is initialized randomly for paths
and learned during training, that contains rows that are mapped
to each of the AST paths. On the other hand, in code2seq, each
node of a path comes from a learned embedding matrix, and then a
bi-directional LSTM is used to encode each of the AST paths sepa-
rately. The bi-directional LSTM reads the path once from beginning
to the end (as original order) and once from end to beginning (in

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

Table 2: Prediction Change Percentage (PCP) across all models, datasets, and transformations.

Transformation

Dataset

# Original methods

# Transformed methods

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

31113
235961
252725

1158
6407
8868

3699
17107
35565

246
3312
10478

3397
16150
21956

44426
351621
370927

123123
771208
916565
Weighted Average =

1519
8840
12107
Weighted Average =

5160
23533
49665
Weighted Average =

259
3839
11165
Weighted Average =

9169
44711
74973
Weighted Average =

44426
351621
370927
Weighted Average =

Prediction change (%) (PCP)
code2seq GGNN
code2vec
54.92
46.55
42.06
44.85

57.16
48.75
47.04
48.46

28.17
35.96
31.92
33.39

53.85
50.35
47.80
49.21

59.38
62.77
46.52
52.25

68.73
59.91
30.33
38.42

72.80
65.44
64.38
65.35

39.97

35.80
31.21
33.82

54.31
44.71
51.43
48.98

52.54
45.29
42.51
44.01

61.78

41.60
29.08
32.78
57.32
42.64
41.93
43.27

45.60
40.25
37.44
39.20

29.37
33.74
31.98
32.50

31.66
36.67
31.75
33.22

31.45
43.73
45.50
44.82

26.36
34.09
26.32
29.02

28.34
42.79
35.67
38.51

reverse order). Therefore, the order changes by Permute Statement
may become less sensitive to code2seq than code2vec.

Another observation is that, in most cases of code2vec and
code2seq, the PCP of the transformations in Java-Small is high,
and it is significantly lower on larger datasets, i.e., Java-Med, and
Java-Large. In GGNN , the PCP of the transformations shows a
different trend: lowest in Java-Small, in most cases, and highest in
Java-Med.
(cid:15)

(cid:12)

Observation 2: In most cases, the effect of prediction change
for code2vec and code2seq is reduced as the dataset size in-
creases, compared to GGNN .

(cid:14)

(cid:13)

6.2 RQ2: When Transformations Affect Neural

Program Models the Most?
Single-place transformation vs. All-place transformation. In
6.2.1
our analysis, thus far, if a program has multiple candidates for a
transformation, say 𝑛 candidates, for transformation, we only apply
them one at the time and end up with 𝑛 distinct transformed pro-
grams. We call this single-place transformation. Alternatively, we
can apply the transformations to all candidate locations in the pro-
gram simultaneously to create only one transformed program. We
call this all-place transformation. We evaluate the generalizability

of neural program models under all-place transformation for the
following transformations: Variable Renaming, Boolean Exchange,
Loop Exchange, and Switch to If . Note that the all-place transforma-
tion is not applicable to Permute Statement and Unused Statement
transformations, as we apply the Permute Statement on a pair of
statements and the Unused Statement on a random block.

Figure 2 compares the impact of single-place transformation and
all-place transformation on the prediction changes in all neural
program models. For the code2vec model, the percentage of pre-
diction change for the all-place transformation is higher than the
single-place transformation by a good margin for all the cases. Sim-
ilarly, for the code2seq model, the percentage of prediction change
for the all-place transformation is higher than the single-place
transformation by a good margin except for the case (Switch to
If , Java-Small). After a closer examination of Java-Small dataset
and Switch to If transformation, we observe that the number of
transformed methods for all-place is only 13, which is too low to
provide comparative insight. For the GGNN model, the difference
between all-place transformation and single-place transformation
is relatively very small compared to the code2vec and code2seq mod-
els. Even for (Boolean Exchange, Java-Small + Java-Med), (Loop
Exchange, Java-Med + Java-Large), and (Switch to If ,Java-Large),

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

(a) code2vec

(b) code2seq

(c) GGNN

Figure 2: Prediction Change Percentage (PCP) in single-place vs. all-place transformations.

(a) code2vec

(b) code2seq

(c) GGNN

Figure 3: Prediction Change Percentage (PCP) on correctly vs. incorrectly predicted methods.

the percentage of prediction changes for the single-place trans-
formation is higher than the all-place transformation. The results
may suggest that the performance of GGNN under single-place
transformations and all-place transformations is almost consistent.
(cid:23)

(cid:20)

Observation 3: While all-place transformations are more
likely to induce prediction changes in code2vec and code2seq
than single-place transformations, the performance of GGNN
remains relatively similar under both types of transformations.

(cid:22)

(cid:21)

6.2.2 Correctly predicted methods vs. Incorrectly predicted methods.
We also evaluate the generalizability of neural program models un-
der correctly and incorrectly predicted methods. Figure 3 compares
the impact of correctly predicted methods and incorrectly predicted
methods on the prediction changes in all neural program models.
In the code2vec model, the percentage of changes in predictions af-
ter transformation in the correctly predicted methods ranges from
10.45% to 42.86%, while, in the incorrectly predicted methods, a
larger portion of transformations, 38.18% to 76.00%, change the
prediction of code2vec. Similarly, in the code2seq model, the percent-
age of changes in predictions after transformation in the correctly
predicted methods and in the incorrectly predicted methods ranges
from 9.19% to 36.36% and 46.66% to 62.90%, respectively. However,
in the GGNN model, while the percentage of changes in predictions

after transformation on the correctly predicted methods ranges
from 1.90% to 8.58%, the percentages range from 31.05% to 62.01%
in the incorrectly predicted methods.
(cid:23)

(cid:20)

Observation 4: It is likely that GGNN is more stable than
code2vec and code2seq in the originally correct methods, and the
changes in prediction happen more frequently in the originally
incorrect methods for all models.

(cid:22)

(cid:21)

Loop Exchange,

6.2.3 The Effect of 𝑋 %-Transformation. In this section, we
evaluate the generalizability of neural program models un-
der 𝑋 %-transformation for
the following transformations:
and
Boolean Exchange,
Variable Renaming,
Switch to If . If a transformation 𝑡 is applicable to 𝑛 locations in a
method body, 𝑋 %-transformation randomly picks ⌊ 𝑛∗𝑋
100 ⌋ of those
locations and applies 𝑡 to create a new transformed program. The
number of all 𝑋 %-transformed programs grows exponentially with
the number of locations; therefore, to manage the complexity, in
𝑋 %-transformation we randomly pick the locations in a method
body, instead of considering all possible combinations, to create
transformed programs. We study the 𝑋 %-transformation with
𝑋 = {25, 50, 75}. For each transformation 𝑡, we first create a dataset
𝑑4
𝑡 that contains methods with four or more possible locations,
so that the transformation 𝑡 is applicable to each method for

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

Table 3: The PCP for 𝑋 %-transformations across different datasets and models.

Dataset

Transformation

# Transformed
methods

25% Transformation

50% Transformation

75% Transformation

code2vec

code2seq GGNN code2vec

code2seq GGNN code2vec

code2seq GGNN

Java-Small

Java-Med

Java-Large

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

15937

75

302

0

101003

428

1292

98

114748

642

2899

125

63.29

80.00

81.95

-

54.07

69.66

86.01

82.91

45.62

71.81

79.77

69.00

54.36

63.00

65.90

-

46.51

48.20

55.11

43.62

43.23

59.03

56.00

56.00

29.56

37.70

32.44
-

37.18

31.37

28.72

55.42

33.37

28.18

25.96

34.78

71.88

79.67

81.87

-

62.65

70.50

86.11

84.44

53.32

71.09

79.02

73.60

65.89

64.67

65.98

-

57.77

48.60

57.37

44.90

53.99

62.76

56.79

56.40

29.87

36.07

32.44
-

38.91

30.97

26.29

50.00

35.75

31.76

27.41

32.36

75.18

79.66

81.38

-

66.51

70.91

85.37

89.03

56.98

71.93

78.57

73.60

70.57

64.66

65.56

-

63.20

47.49

57.62

45.16

58.83

62.03

56.92

56.60

30.36

32.79

34.73
-

37.88

31.17

29.01

57.08

34.59

30.40

27.05

34.95

each 𝑋 = {25, 50, 75}, e.g., methods with at least four variables
for Variable Renaming. To account for the randomness, we run
each setting five times and report the average results, except for
the GGNN that we run only three times due to the longer graph
construction and processing time.

Table 3 shows the results of the 𝑋 %-transformations. In each
𝑋 %-transformation, GGNN has a much lower PCP value than the
code2vec and code2seq models for all the transformations across the
three Java datasets. Moreover, in GGNN models, the differences of
PCP under different 𝑋 are relatively small (mostly a few percent-
age points) and do not yield a clear trend. On the other hand, in
code2vec and code2seq models, with the Variable Renaming trans-
formation, the PCP tends to increase as 𝑋 grows, but with other
transformations expect Variable Renaming, the PCP shows mod-
est changes only. Note that in 𝑋 %-transformation, compared to
Variable Renaming, the numbers of transformed programs for other
transformations are much lower, which might be too low to provide
statistical significance or comparative insights.
(cid:23)

(cid:20)

Observation 5: The performance of GGNN in terms of PCP
remains similar in all cases under 𝑋 %-transformation, but the
PCP of code2vec and code2seq for Variable Renaming increases
as 𝑋 grows.

(cid:22)

(cid:21)

6.3 RQ3: Impact of Method Length on

Generalizability

An important metric of interest might be the generalizability in
terms of the number of statements in the methods. Figure 4 de-
picts the relation between the length of methods and the prediction
changes percentage (i.e., PCP) in the single-place transformed data.
In the figure, the “Number of statements in method” denotes the
number of executable lines in the body of methods before the trans-
formation.

As shown in Figure 4(a-f), in most cases, the code2vec and
code2seq models exhibit notable increases in PCP for all the
transformations and datasets as the number of lines in methods
increases. However, looking at Figure 4(g-i), it seems that GGNN

is less sensitive to the number of lines in methods compared to
code2vec and code2seq with respect to the transformations.

(cid:15)

Observation 6: The code2vec and code2seq show notable in-
creases in PCP as the length of methods grows, but PCP in
GGNN seems to be less sensitive to the length of methods.

(cid:14)

(cid:12)

(cid:13)

6.4 RQ4: Trends in the Types of Changes
Table 4 shows the full breakdown of the proportion of different types
of changes after the transformation of methods. In this experiment,
we use the same single-place transformed data that have been used
for the PCP in Table 2. In code2vec and code2seq, the value of CCP
increases with increase in the size of datasets. It may suggest that
with a larger dataset the neural program model can generalize the
correct predictions better.

𝐶𝑊 𝑃

In addition, we calculate

𝐶𝐶𝑃 +𝐶𝑊 𝑃 to approximate the ratio of
cases that the neural program model’s prediction switches from
correct to wrong after transformations with respect to all the cases
whose initial predictions are correct. The ratio helps us to simplify
the comparison of (in)generalizability across different models. On
average, 23% and 20% of cases, the neural program model switches
from a correct prediction to a wrong one in code2vec and code2seq,
respectively. In GGNN , on the other hand, this switch happens in
less than 5% of transformations.

𝑊 𝐶𝑃

Similarly,

𝑊 𝑊 𝑆𝑃 +𝑊 𝑊 𝐷𝑃 +𝑊 𝐶𝑃 approximates the ratio of cases
switching from a wrong prediction to a correct prediction after
transformations with respect to all the cases whose initial prediction
are wrong. In code2vec and code2seq, a transformation switches from
a wrong prediction to correct prediction in less than 3% of cases,
however, this switch happens in around 1% of transformations for
𝑊 𝑊 𝑆𝑃 +𝑊 𝑊 𝐷𝑃 +𝑊 𝐶𝑃 implies that
GGNN . Higher
transformations are likely to reduce the overall performance of the
neural program models.

𝐶𝐶𝑃 +𝐶𝑊 𝑃 than

𝑊 𝐶𝑃

𝐶𝑊 𝑃

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

(a) code2vec (Java-Small)

(b) code2vec (Java-Med)

(c) code2vec (Java-Large)

(d) code2seq (Java-Small)

(e) code2seq (Java-Med)

(f) code2seq (Java-Large)

(g) GGNN (Java-Small)

(h) GGNN (Java-Med)

(i) GGNN (Java-Large)

Figure 4: Prediction Change Percentage (PCP) across the number of statements in methods.

(cid:31)

(cid:28)

Observation 7: Transformations are likely to decrease the
overall performance of neural program models, and they are
more likely to change the correct prediction in code2vec and
code2seq than GGNN , while the generalizability of code2vec
and code2seq can be compensated by larger datasets more than
GGNN .
(cid:30)

(cid:29)

6.5 RQ5: Impact of the Transformations on

Precision, Recall, and 𝐹1-Score

The performance of neural program models in the literature are
often measured in classic metrics, such as precision, recall, and 𝐹1-
score. In particular for the method name prediction task trained for

code2vec, code2seq and GGNN , subtoken-level comparison is used
to calculate the metrics; i.e., the method names in both predicted
results and ground-truth names are split into individual tokens for
the measurements (cf. the definitions in Section 4.3).

We also study the impact of the program transformations on the
performance of neural program models in terms of these classic
metrics. Table 5 shows the changed precision, recall, and 𝐹1-scores
for the programs transformed by different transformations. In this
experiment, we use the same single-place transformed data that
have been used for the PCP in Table 2.

In comparison with Table 1, we can see the average precision,
recall, and 𝐹1-score in Table 5 have obvious decreases for all the

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

Table 4: The detailed PCP across all models, datasets, and transformations.

Dataset

Transformation

CCP

CWP

WWSP

WCP

WWDP

code2vec

code2seq GGNN code2vec

code2seq GGNN code2vec

code2seq GGNN code2vec

code2seq GGNN code2vec

code2seq GGNN

Java-Small

Java-Med

Java-Large

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

2.32

3.88

1.86

1.54

4.64

8.08

7.56

12.76

6.61

11.15

11.53

16.90

15.40

11.33

19.81

48.30

11.13

24.31

3.75

4.54

4.24

2.70

3.96

10.40

9.41

13.90

7.62

17.90

14.55

21.58

14.54

10.14

18.87

52.23

13.35

26.57

15.76

22.25

16.23

26.61

16.86

20.97

20.39

27.83

20.93

31.28

25.47

25.07

14.57

13.97

13.29

9.30

21.89

22.28

1.18

0.72

0.74

1.16

1.35

1.73

2.81

1.79

2.21

5.99

4.31

3.24

3.25

2.09

3.00

5.63

6.66

3.75

1.65

2.57

0.74

1.54

1.28

3.20

3.90

1.91

1.71

2.94

1.57

5.22

4.69

2.92

2.49

5.28

2.17

5.95

0.59

0.43

0.89

1.61

0.80

0.82

1.18

1.00

1.09

2.93

1.09

1.83

0.77

0.79

0.28

0.27

1.27

1.19

42.76

42.26

38.76

29.73

22.57

51.96

45.89

36.89

30.62

28.94

23.03

47.30

42.55

40.87

33.67

21.37

24.49

44.47

39.09

41.15

43.22

35.52

38.72

44.00

41.85

41.39

47.09

40.51

42.81

38.18

38.42

38.42

38.62

18.68

44.72

35.99

56.08

48.38

52.11

41.94

56.78

50.69

43.65

38.42

42.40

24.99

40.44

32.13

53.51

54.06

54.96

45.20

51.79

42.05

0.57

1.38

0.72

0.00

1.93

0.60

0.80

1.45

1.22

2.45

2.05

0.79

1.40

2.00

2.13

4.09

2.89

0.85

1.21

1.65

1.10

1.93

1.27

1.40

1.54

1.97

1.33

2.27

1.57

1.41

2.32

2.92

2.30

2.80

2.82

1.81

0.54

0.58

0.62

0.40

0.27

0.79

1.22

1.03

1.23

1.85

1.29

1.67

1.13

1.13

1.13

0.68

0.96

1.24

53.17

51.76

57.92

67.57

69.51

37.63

42.94

47.11

59.34

51.47

59.08

31.77

37.40

43.71

41.39

20.61

54.83

26.62

54.30

50.09

50.70

58.31

54.77

41.00

43.30

40.83

42.25

36.38

39.50

33.61

40.03

45.60

37.72

21.01

36.94

29.68

27.03

28.36

30.15

29.44

25.29

26.73

33.56

31.72

34.35

38.95

31.71

39.30

30.02

30.05

30.34

44.55

24.09

33.24

Table 5: The precision, recall and 𝐹1-score for subtokens across all models, datasets, and transformations.

Dataset

Transformation

# Transformed
methods

Precision

Recall

code2vec

code2seq GGNN code2vec

code2seq GGNN code2vec

𝐹1-Score
code2seq GGNN

Java-Small

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

123123

1519

5160

259

9169

44426

Weighted Average =

Java-Med

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

771208

8840

23533

3839

44711

351621

Java-Large

Weighted Average =

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Unused Statement

916565

12107

49665

11165

74973

370927

Weighted Average =

9.79

8.97

9.08

7.01

11.21

21.26

12.60

20.90

22.29

18.29

30.24

24.29

32.87

24.51

35.17

27.30

37.60

69.34

25.56

44.96

37.48

38.01

33.58

34.52

30.78

33.11

50.99
40.76

43.57
40.72

39.25

51.49
38.75

55.79

46.88

48.79

42.90

46.79

72.75

43.88

61.57

51.90

40.64

41.19

39.50

38.32

45.73
44.37

41.77

39.71

42.26

42.00
47.67

39.79
44.11

41.09

32.80

23.82

25.28

22.68

32.56

45.43

35.64

5.05

5.36

5.22

4.99

5.64

13.63

7.16

10.89

13.95

10.06

20.89

12.99

21.56

14.12

20.83

15.83

23.91

57.06

14.62

30.40

23.32

28.99

26.94

26.08
26.41

25.52

41.12

31.65

28.98

29.02

26.55

39.56

28.26

43.15

33.08

38.14

33.19

37.91

66.18

33.28

52.44

41.75

23.78

25.93

23.40

30.16
22.95

31.36

25.59

22.24

24.72

23.10

34.65

23.35

30.03

24.63

20.46

16.54

18.75

21.81

21.37

29.93

22.87

6.66

6.71

6.63

5.83

7.50

16.61

9.11

14.32

17.16

12.98

24.71

16.93

26.04

17.87

26.16

20.04

29.23

62.60

18.60

36.27

28.72

32.89
29.90

29.71
28.43

28.82

45.53

35.62

34.81

33.89

31.67

44.74

32.68

48.66

38.74

42.81

37.43

41.88

69.31

37.85

56.64

46.25

30.00

31.83
29.39

33.75

30.56
36.75

31.66

28.51

31.19

29.81

40.13

29.43

35.73

30.74

25.20

19.52

21.53

22.24

25.80

36.09

27.85

(cid:39)

(cid:36)

three neural program models across the three Java datasets, that
may indicate the (negative) impact of the transformations on the
neural program models.

We also find no obvious correspondence between the PCP shown
in Table 2 and the changes in precision, recall, and 𝐹1-score; high
PCP does not necessarily lead to high changes in precision, recall
and 𝐹1-score and vice versa.

Observation 8: Neural program models seem susceptible to
semantic-preserving transformations with respect to the clas-
sic metrics of precision, recall, and 𝐹1-score as well. While
our new metric of Prediction Change Percentage (PCP) shows
the impact of the transformations from a different and more
fine-grained perspective, the changes in the classic metrics are
not correlated with PCP.

(cid:38)

(cid:37)

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

7 DISCUSSION
In this paper, we study the current state of generalizability in neural
program models built on code2vec, code2seq, and GGNN . Although
limited, it provides interesting insights. In this section, we discuss
why neural networks have become a popular, or perhaps the de-
facto, tool for processing programs, and what are the implications
of using neural networks in processing source code.

Neural networks constitute a powerful class of machine learning
models with a large hypothesis class. For instance, a multi-layer
feed-forward network is called a universal approximator, meaning
that it can essentially represent any function [25]. Unlike traditional
learning techniques that require extensive feature engineering and
tuning, deep neural networks facilitate representation learning.
That is, they are capable of performing feature extraction out of
raw data on their own [32]. Given a sufficiently large dataset, neural
networks with adequate capabilities can substantially reduce the
burden of feature engineering. Availability of a large number of code
repositories makes data-driven program analysis a good application
of neural networks. However, it is still unknown if neural networks
are the best way to process programs [24] vs. [29].

Although the large hypothesis class of neural networks and
feature learning make them very appealing to use, the complex
models built by neural networks are still too difficult to understand
and interpret. Therefore, as we apply neural networks in program
analysis, we should develop specialized tools and techniques to
enhance their interpretability, generalizability and robustness.

7.1 Generalizability vs. Interpretability

vs. Robustness and Others

Interpretability studied in the literature may help to build more
understandable neural networks, revealing the limits and strengths
of the networks, and thus to some extent, it helps to evaluate and
understand the generalizability of the networks. However, our study
of generalizability with respect to program transformation provides
a different perspective complement to interpretability; the approach
may have the potential in the future to help identify interpretable
code elements by measuring the impact of certain types of code
transformations.

As mentioned in Section 2, there is a substantial line of work
on evaluating the robustness of neural networks especially in the
domain of vision and pattern recognition [48]. The key insight in
such domains is that small, imperceptible changes in input should
not impact the result of output. While this observation can be true
for domains such as vision, it might not be directly applicable to
the discrete domain of neural program models, since some minor
changes to a program can drastically change the semantic and
behavior of the program. Quantifying the imperceptibility and
many other aspects of source code is our future research goal.

7.2 Are we there yet?
Are neural program models ready for widespread use in program
analysis? The neural program models in our experiments are brittle
to even very small changes in the methods. The semantic-preserving
transformations can change the outputs of the neural program
models in 26% to 73% of cases. Although our findings are limited to
only one task, they suggest caution. The literature lacks techniques

for rigorous evaluation of neural program models. The recent line
of work by Nghi et al. [13] in interpretability of neural program
models, Rabin et al. [40–42] in testing them, and Yefet et al. [58]
are much needed steps in a right direction.

7.3 Code Representation
The performance of models used in neural program models, such
as ones used in this study, is relatively low compared to the per-
formance of neural models in domains such as natural language
understanding [44], text classification [31]. To improve their per-
formance, we would need novel code representations that better
capture interesting characteristics of programs.

8 RELATED WORK
Robustness of Neural Networks. There is a substantial line of
work on the robustness of artificial intelligence (AI) systems in
general and deep neural networks in particular. Szegedy et al. [48]
is the first to discover that deep neural networks are vulnerable
to small perturbations that are imperceptible to human eyes. They
developed the L-BFGS method for the systematic generation of
such adversarial examples. Goodfellow et al. [21] proposes a more
efficient method, called the Fast Gradient Sign Method that exploits
the linearity of deep neural networks. Many following up works
[14, 18, 30, 37, 58, 59, 63] further demonstrated the severity of the
robustness issues with a variety of attacking methods and defenses.
While aforementioned approaches only apply to models for image
classification, new attacks have been proposed that target models
in other domains, such as natural language processing [27, 33, 64]
and graphs [16, 65].

The automated verification research community has proposed
techniques to offer guarantees for the robustness of neural net-
works by adapting bounded model checking [46], abstract interpre-
tation [20], and Satisfiability Modulo Theory [26]. Amershi et al.
[8] study the challenges in developing AI solutions and Zhang et al.
[61] survey testing of machine-learning systems.
Models of Code. Early works directly adopted NLP models to dis-
cover textual patterns existed in the source code [22, 39]. Those
methods, unfortunately, do not account for the structural infor-
mation programs exhibit. Following approaches address this issue
by generalizing from the abstract syntax trees [6, 7, 36, 38]. As
Graph Neural Networks (GNN) [45] have been gaining increasing
popularity due to its remarkable representation capacity, many
works have leveraged GNN to tackle challenging tasks like pro-
gram repair and bug finding, and obtained quite promising re-
sults [4, 12, 17, 17, 34, 49, 55]. Besides, the attention mechanism [9]
has been applied into GNNs to improve the performance further
[10, 56, 62]. It is very interesting to see how the attention can help
to explain the output of the neural models [13, 50, 57]. In parallel,
Wang et al. developed a number of models [51, 53, 54] that feed off
the run time information for enhancing the precision of semantic
representation for model inputs.

9 THREATS TO VALIDITY
There are various threats to the validity of our approach.
Limited Data and Evaluation Scope. We only evaluated the gen-
eralizability of neural program models built on code2vec, code2seq,

On the Generalizability of Neural Program Models w.r.t. Semantic-Preserving Program Transformations

Information and Software Technology, IST Journal 2021, Elsevier

and GGNN , for one task in Java programs. Therefore, our results
may not generalize to other neural program models or other tasks
or other programming languages. We leave the evaluation of the
general applicability of our approach as future work.
Transformations. The proposed transformations in this paper
impact program ASTs in varying degrees. Some of the transforma-
tions, e.g. variable renaming, are common refactoring techniques.
However, these transformations may not represent many possible
transformations in other domains. We will instantiate and extend
our approach with other transformations from other domains.
Internal Validity. Some bugs may exist in the toolchain and neural
program models implemented in this paper. To reduce the probabil-
ity of bugs, two authors reviewed the code and manually inspected
a sample of transformed programs to ensure the reliability of trans-
formations.

10 CONCLUSION & FUTURE WORK
In this paper, we perform a large-scale, systematic evaluation of the
generalizability of state-of-the-art neural program models built on
code2vec, code2seq, and GGNN . In particular, we apply six semantic-
preserving program transformations to produce new programs on
which we expect the neural program models to keep their original
predictions. We find that such program transformations frequently
sway the predictions of these neural program models, indicating
serious generalization issues that could negatively impact the wider
applications of deep neural networks in program analysis tasks.
Although neural program models that encode more program de-
pendency information and are trained with larger datasets may
exhibit more generalizable behavior, their generalizability is still
limited. We believe this work provides a systematic approach and
metrics for evaluating neural program models, and can motivate
future research on training not only accurate but also generalizable
deep models of code. Future work that includes more semantic-
preserving and even some semi-semantic-preserving transforma-
tions in our approach and adapts more fine-grained prediction
change metrics may further extend the applicability of our approach
to various neural program models designed for different tasks. We
also plan to explore using transformed programs to improve the
generalizability of the neural program models.

ACKNOWLEDGMENTS
This research is supported by the Singapore Ministry of Education
(MOE) Academic Research Fund (AcRF) Tier 1 Grant No. 19-C220-
SMU-002 and the Research Lab for Intelligent Software Engineering
(RISE) Operational Fund from the School of Computing and Infor-
mation Systems (SCIS) at Singapore Management University (SMU).
We also thank the anonymous reviewers for their insightful com-
ments and suggestions, and thank the authors of previous related
works for sharing data and models.

REFERENCES
[1] Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine
Learning Models of Code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward! 2019). Association for Computing Machinery, New York, NY,
USA, 143–153. https://doi.org/10.1145/3359591.3359735

[2] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting Accurate Method and Class Names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering (ESEC/FSE 2015). Association

for Computing Machinery, New York, NY, USA, 38–49. https://doi.org/10.1145/
2786805.2786849

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv.
51, 4, Article Article 81 (July 2018), 37 pages. https://doi.org/10.1145/3212695

[4] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations.

[5] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. 2016. A Convolutional
Attention Network for Extreme Summarization of Source Code. In Proceedings of
the 33nd International Conference on Machine Learning, ICML.

[6] Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences
from Structured Representations of Code. In International Conference on Learning
Representations.

[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2Vec:
Learning Distributed Representations of Code. Proc. ACM Program. Lang. 3,
POPL, Article 40 (Jan. 2019), 29 pages. https://doi.org/10.1145/3290353

[8] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,
Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.
2019. Software Engineering for Machine Learning: A Case Study. In Proceedings
of the 41st International Conference on Software Engineering: Software Engineering
in Practice (ICSE-SEIP ’19). IEEE Press, 291–300. https://doi.org/10.1109/ICSE-
SEIP.2019.00042

[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
[10] Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-Sequence
Learning using Gated Graph Neural Networks. In Proceedings of the Association
for Computational Linguistics (ACL). Association for Computational Linguistics,
273–283.

[11] Pavol Bielik and Martin Vechev. 2020. Adversarial Robustness for Code. Proceed-

ings of the International Conference on Machine Learning (ICML) (2020).
[12] Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr
Polozov. 2019. Generative Code Modeling with Graphs. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019.

[13] N. D. Q. Bui, Y. Yu, and L. Jiang. 2019. AutoFocus: Interpreting Attention-Based
Neural Networks by Code Perturbation. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). 38–41. https://doi.org/10.
1109/ASE.2019.00014

[14] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp). IEEE,
39–57.

[15] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020. Embedding
Java Classes with code2vec: Improvements from Variable Obfuscation. In 17th In-
ternational Conference on Mining Software Repositories (MSR) 2020, Seoul, Republic
of Korea.

[16] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song.
2018. Adversarial Attack on Graph Structured Data. Proceedings of the 35th
International Conference on Machine Learning, PMLR 80 (Jul 2018).

[17] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2019. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in
Programs. In International Conference on Learning Representations.

[18] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 9185–9193.
[19] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured
Neural Summarization. In International Conference on Learning Representations.
[20] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of
neural networks with abstract interpretation. In 2018 IEEE Symposium on Security
and Privacy (SP). IEEE, 3–18.

[21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.).

[22] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence (AAAI’17). AAAI Press,
1345–1351.

[23] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.
Deep learning type inference. In Proceedings of the 2018 26th acm joint meeting
on european software engineering conference and symposium on the foundations of
software engineering. 152–162.

[24] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural
Networks the Best Choice for Modeling Source Code?. In Proceedings of the
2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE

Information and Software Technology, IST Journal 2021, Elsevier

M.R.I. Rabin, N.D.Q. Bui, K. Wang, Y. Yu, L. Jiang, M.A. Alipour

2017). Association for Computing Machinery, New York, NY, USA, 763–773.
https://doi.org/10.1145/3106237.3106290

[25] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-
forward networks are universal approximators. Neural networks 2, 5 (1989),
359–366.

[26] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
Verification of Deep Neural Networks. In Computer Aided Verification, Rupak
Majumdar and Viktor Kunčak (Eds.). Springer International Publishing, Cham,
3–29.

[27] Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading
Comprehension Systems. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/
d17-1215

[28] H. J. Kang, T. F. Bissyandé, and D. Lo. 2019. Assessing the Generalizability of
Code2vec Token Embeddings. In 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 1–12. https://doi.org/10.1109/ASE.2019.
00011

[29] Rafael-Michael Karampatsis and Charles Sutton. 2019. Maybe Deep Neural
Networks are the Best Choice for Modeling Source Code. arXiv:cs.SE/1903.05734
[30] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Machine
Learning at Scale. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
[31] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent Convolutional
Neural Networks for Text Classification. In Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI’15). AAAI Press, 2267–2273.

[32] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature

521, 7553 (2015), 436.

[33] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks
through representation erasure. arXiv preprint arXiv:1612.08220 (2016).
[34] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated
Graph Sequence Neural Networks. In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.).

[35] Kui Liu, Dongsun Kim, Tegawendé F. Bissyandé, Taeyoung Kim, Kisub Kim, Anil
Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to Spot and Refactor
Inconsistent Method Names. In Proceedings of the 41st International Conference
on Software Engineering (ICSE ’19). IEEE Press, 1–12. https://doi.org/10.1109/
ICSE.2019.00019

[36] Chris Maddison and Daniel Tarlow. 2014. Structured generative models of natural
source code. In International Conference on Machine Learning. 649–657.

[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a simple and accurate method to fool deep neural networks. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 2574–2582.
[38] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neu-
ral Networks over Tree Structures for Programming Language Processing. In
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI’16).
AAAI Press, 1287–1293.

[39] Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay.
2016. sk_p: a neural program corrector for MOOCs. In Companion Proceedings
of the 2016 ACM SIGPLAN International Conference on Systems, Programming,
Languages and Applications: Software for Humanity. ACM.

[40] Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2020. Evaluation of
Generalizability of Neural Program Analyzers under Semantic-Preserving Trans-
formations. (2020). arXiv:cs.SE/2004.07313

[41] Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, and Moham-
mad Amin Alipour. 2020. Towards Demystifying Dimensions of Source Code
Embeddings. In Proceedings of the 1st ACM SIGSOFT International Workshop
on Representation Learning for Software Engineering and Program Languages
(RL+SE&PL 2020). Association for Computing Machinery, New York, NY, USA,
29–38. https://doi.org/10.1145/3416506.3423580

[42] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour. 2019. Testing
Neural Program Analyzers. 34th IEEE/ACM International Conference on Automated
Software Engineering (Late Breaking Results-Track) (2019). https://arxiv.org/abs/
1908.10711

[43] Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh
Jha, and Thomas Reps. 2020. Semantic Robustness of Models of Source Code.

arXiv preprint arXiv:2002.03043 (2020).

[44] Ruhi Sarikaya, Geoffrey E. Hinton, and Anoop Deoras. 2014. Application of Deep
Belief Networks for Natural Language Understanding. IEEE/ACM Trans. Audio,
Speech and Lang. Proc. 22, 4 (April 2014), 778–784. https://doi.org/10.1109/TASLP.
2014.2303296

[45] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The
Graph Neural Network Model. IEEE Transactions on Neural Networks 20, 1 (2009),
61–80. https://doi.org/10.1109/TNN.2008.2005605

[46] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. 2015.

Towards Verification of Artificial Neural Networks.. In MBMV. 30–40.

[47] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In Proceedings of the 34th International Conference on Machine
Learning. 3319–3328.

[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In ICLR.

[49] Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine
Manzagol, Charles Sutton, and Edward Aftandilian. 2020. Learning to fix build
errors with graph2diff neural networks. In Proceedings of the IEEE/ACM 42nd
International Conference on Software Engineering Workshops. 19–20.

[50] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings.

[51] Ke Wang. 2019. Learning Scalable and Precise Representation of Program Se-

mantics. arXiv preprint arXiv:1905.05251 (2019).

[52] Ke Wang and Mihai Christodorescu. 2019. COSET: A Benchmark for Evaluating

Neural Program Embeddings. arXiv:cs.LG/1905.11445

[53] Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Dynamic neural program
embedding for program repair. In International Conference on Learning Represen-
tations.

[54] Ke Wang and Zhendong Su. 2020. Blended, Precise Semantic Program Em-
beddings. In Proceedings of the 41st ACM SIGPLAN International Conference on
Programming Language Design and Implementation (PLDI ’20).

[55] Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. 2019. Learning a Static

Bug Finder from Data. arXiv preprint arXiv:1907.05579 (2019).

[56] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim
Sheinin. 2018. Graph2seq: Graph to sequence learning with attention-based
neural networks. arXiv preprint arXiv:1804.00823 (2018).

[57] Xiaoran Xu, Songpeng Zu, Chengliang Gao, Yuan Zhang, and Wei Feng. 2018.
Modeling Attention Flow on Graphs. Relational Representation Learning Workshop,
NeurIPS 2018 (2018).

[58] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial Examples for Models
of Code. Proc. ACM Program. Lang. 4, OOPSLA, Article 162 (Nov. 2020), 30 pages.
https://doi.org/10.1145/3428230

[59] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples:
Attacks and defenses for deep learning. IEEE transactions on neural networks and
learning systems 30, 9 (2019), 2805–2824.

[60] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating
Adversarial Examples for Holding Robustness of Source Code Processing Models.
In The Thirty-Fourth AAAI Conference on Artificial Intelligence. AAAI Press, 1169–
1176.

[61] J. M. Zhang, M. Harman, L. Ma, and Y. Liu. 2020. Machine Learning Testing:
Survey, Landscapes and Horizons. IEEE Transactions on Software Engineering
(2020), 1–1. https://doi.org/10.1109/TSE.2019.2962027

[62] Shuo Zhang and Lei Xie. 2020. Improving Attention Mechanism in Graph Neural
Networks via Cardinality Preservation. In Proceedings of the Twenty-Ninth Inter-
national Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere
(Ed.). ijcai.org, 1395–1402. https://doi.org/10.24963/ijcai.2020/194

[63] Xiang Zhang and Marinka Zitnik. 2020. Gnnguard: Defending graph neural
networks against adversarial attacks. Advances in Neural Information Processing
Systems 33 (2020).

[64] Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating Natural Adver-
sarial Examples. In International Conference on Learning Representations.
[65] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2847–2856.

