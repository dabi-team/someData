1

MetaSensing: Intelligent Metasurface Assisted RF

3D Sensing by Deep Reinforcement Learning

Jingzhi Hu, Graduate Student Member, IEEE, Hongliang Zhang, Member, IEEE,

Kaigui Bian, Senior Member, IEEE, Marco Di Renzo, Fellow, IEEE, Zhu Han, Fellow, IEEE,

and Lingyang Song, Fellow, IEEE

Abstract

Using RF signals for wireless sensing has gained increasing attention. However, due to the unwanted

multi-path fading in uncontrollable radio environments, the accuracy of RF sensing is limited. Instead

of passively adapting to the environment, in this paper, we consider the scenario where an intelligent

metasurface is deployed for sensing the existence and locations of 3D objects. By programming its

beamformer patterns, the metasurface can provide desirable propagation properties. However, achieving

a high sensing accuracy is challenging, since it requires the joint optimization of the beamformer patterns

and mapping of the received signals to the sensed outcome. To tackle this challenge, we formulate an

optimization problem for minimizing the cross-entropy loss of the sensing outcome, and propose a deep

reinforcement learning algorithm to jointly compute the optimal beamformer patterns and the mapping

of the received signals. Simulation results verify the effectiveness of the proposed algorithm and show

how the sizes of the metasurface and the target space inﬂuence the sensing accuracy.

Index Terms

RF 3D sensing, metasurface, deep reinforcement learning, policy gradient algorithm, beamformer

pattern design.

J. Hu and L. Song are with Department of Electronics, Peking University. (email: {jingzhi.hu, lingyang.song}@pku.edu.cn)

H. Zhang is with Department of Electrical Engineering, Princeton University. (email: hongliang.zhang92@gmail.com)

K. Bian is with Department of Computer Science, Peking University. (email: bkg@pku.edu.cn)

M. Di Renzo is with Universit´e Paris-Saclay, CNRS and CentraleSup´elec, Laboratoire des Signaux et Syst`emes, 91192 Gif-

sur-Yvette, France. (email: marco.direnzo@centralesupelec.fr)

Z. Han is with Electrical and Computer Engineering Department, University of Houston, and also with Department of Computer

Science and Engineering, Kyung Hee University. (email: hanzhu22@gmail.com)

0
2
0
2

v
o
N
5
2

]
P
S
.
s
s
e
e
[

1
v
5
1
5
2
1
.
1
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

I. INTRODUCTION

Recently, leveraging widespread radio-frequency (RF) signals for wireless sensing applica-

tions has attracted growing research interest. Different from methods based on wearable devices

or surveillance cameras, RF sensing techniques need no direct contact with the sensing targets [1].

The basic principle behind RF sensing is that the inﬂuence of the target objects on the propagation

of wireless signals can be potentially recognized by the receivers [2]. RF sensing techniques can

be widely applied to many scenarios of daily life, such as surveillance [3], crowd sensing [4],

ambient assisted living [5], and remote health monitoring [6]. In these applications, it is crucial

to have high sensing accuracies.

Many RF-based sensing methods based on WiFi signals or millimeter wave signals have been

proposed for sensing and recognizing human being and objects. In [7], the authors designed an

RF sensing system that can detect the location and type of moving objects by using Wi-Fi

signals. In [8], the authors proposed a deep learning based RF sensing framework that can

remove environmental and subject-speciﬁc information and can extract environmental/subject-

independent features contained in the sensing data. In [9], the authors designed a low-power RF

sensing system that automatically collects behavior patterns of people.

In addition, using RF sensing to capture human beings and indoor scenes has being ex-

plored. In [10–11], the authors used wide-band RF transceivers with multiple-input-multiple-

output (MIMO) antennas to capture images of human skeletons and showed that it is possible

to reconstruct the human skeleton even when the RF signals are blocked by walls. In [12], the

authors proposed to use mutually orthogonally coded millimeter wave signals to image the scenes

including human beings and objects. However, using RF signals for sensing usually encompasses

a signal collection and analysis process which passively accept the radio channel environment.

The radio environment is unpredictable and usually unfavorable, and thus the sensing accuracy

of conventional RF sensing methods is usually affected by unwanted multi-path fading [13–14],

and/or unfavorable propagation channelsd from the RF transmitters to the receivers.

Intelligent metasurfaces have been proposed as a promising solution for turning unwanted

propagation channels into favorable ones [15–16]. A metasurface is composed of a large number

of electrically reconﬁgurable elements, which applies different phase-shifts on the RF signals that

impinge upon it [17–18]. By programming the reconﬁgurable elements, a metasurface deployed

in the environment can change the RF propagation channel and create favorable signal beams

3

for sensing[19]. We refer to the codings of the reconﬁgurable elements as the beamformer

patterns. Through dynamically designing the beamformer patterns, a metasurface can actively

control the RF signal beams in the sensing process, which potentially improves the sensing

accuracy. Instead of employing complex and sophisticated RF transmitters and receivers [20],

metasurface assisted RF sensing paves a new way of developing RF sensing methods, which

have the capabilities of controlling, programming, and hence customizing the wireless channe.

In literature, the authors of [21] explored the use of metasurfaces to assist RF sensing and obtain

2D images for human beings. Besides, in [22], the authors proposed a metasurface assisted RF

system to obtain localization of mobile users. Nevertheless, no research works have tackled the

analysis and design of metasurface assisted 3D RF sensing, which is more challenging to analyze

and optimize than 2D RF sensing.

In this paper, we consider a metasurface assisted RF 3D sensing scenario, which can sense

the existence and locations of 3D objects in a target space. Speciﬁcally, by programming

the beamformer patterns, the metasurface performs beamforming and provides desirable RF

propagation properties for sensing. However, there are two major challenges in obtaining high

sensing accuracy in metasurface assisted RF sensing scenarios.

• First, the beamformer patterns of the metasurface need to be carefully designed to create

favorable propagation channels for sensing.

• Second, the mapping of the received signals, i.e., the mapping from the signals received at the

RF receiver to the sensing results of the existence and locations of the objects, needs to be

optimized as well.

Nevertheless, the complexity of ﬁnding the optimal beamformer patterns is extremely high

because the associate optimization problem is a discrete nonlinear programming with a large

number optimization variables. Besides, the optimization of the beamformer patterns and the

mapping of the received signals are closely coupled together, which makes optimizing the sensing

accuracy in metasurface assisted RF sensing scenarios even harder.

To tackle these challenges, we formulate an optimization problem for sensing accuracy

maximization by minimizing the cross-entropy loss of the sensing results with respect to the

beamformer patterns and the mapping of the received signals. In order to solve the problem

efﬁciently, we formulate a Markov decision process (MDP) for the optimization problem and

propose a deep reinforcement learning algorithm. The proposed deep reinforcement learning

algorithm is based on the policy gradient algorithm [23] and is referred to as the progressing

4

reward policy gradient (PRPG) algorithm, since the reward function of the MDP is consistently

being improved during the learning process. The computational complexity and the convergence

of the proposed algorithm are analyzed. Moreover, we derive a non-trivial lower-bound for the

sensing accuracy for a given set of beamformer patterns of the metasurface. Simulation results

verify the effectiveness of the proposed algorithm and showcase interesting performance trends

about the sensing accuracy with respect to the sizes of the metasurface and the target space. In

particular, the contributions of this paper can be summarized as follows.

• We consider a metasurface assisted RF sensing scenario which can sense the existence and

locations of objects in a 3D space. Then, we formulate an optimization problem to minimize

the cross-entropy loss of the sensing results through optimizing the beamformer patterns and

the mapping of the received signals. To this end, we adopt a MDP-based framework.

• We propose a deep reinforcement learning algorithm named PRPG to solve the formulated

MDP. The complexity and the convergence of the proposed algorithm are analyzed, and a

non-trivial lower-bound for the sensing accuracy is derived.

• We use simulation results to verify that the proposed algorithm outperforms other benchmark

algorithms in terms of training speed and sensing accuracy. The simulation results unveil

trends about the sensing accuracy as a function of the sizes of the metasurface and the target

space, which gives insights on the implementation of practical metasurface assisted RF sensing

systems.

The rest of this paper is organized as follows. In Section II, we introduce the model of the

metasurface assisted RF sensing scenario. In Section III, we formulate the optimization problem

to optimize the sensing accuracy by minimizing the cross-entropy loss of the sensing results. In

Section IV, we formulate an MDP for the optimization problem and then proposed the PRPG

algorithm to solve it. In Section V, the complexity and convergence of the PRPG algorithm are

analyzed, and a lower-bound for the sensing accuracy is derived. Simulation results are provided

in Section VI and conclusions are drawn in Section VII.

II. SYSTEM MODEL

In this section, we introduce the metasurface assisted 3D RF sensing scenario, which is

illustrated in Fig. 1. In this scenario, there exist a pair of single-antenna RF transceivers, a

metasurface, and a target space where the objects are located. The metasurface reﬂects and

modiﬁes the incident narrow-band signals at a certain frequency fc. The Tx unit and Rx unit of

5

Fig. 1.

Illustration of the metasurface assisted RF sensing scenario.

the transceiver keep transmitting and receiving at fc. The target space is a cubical region that is

discretized into M equally-sized space grids. Each space grid is of size ∆lx

∆ly

∆lz.

×

×

The sensing process adopted in the considered scenario can be brieﬂy described as follow.

The signals transmitted by the Tx unit are reﬂected and modiﬁed by the metasurface before

entering into the target space. The modiﬁed signals are further reﬂected by the objects in the

target space and received by the Rx unit. Then, the Rx unit maps the received signals to the

sensing result, which indicates whether an object exists in each space grid.

In the following, we introduce the metasurface model in Subsection A, the channel model

accounting for the metasurface in Subsection B, and the sensing protocol in Subsection C.

A. Metasurface Model

A metasurface is an artiﬁcial thin ﬁlm of electromagnetic reconﬁgurable materials, which

is composed of uniformly distributed reconﬁgurable elements [24]. As shown in Fig. 1, the

reconﬁgurable element of the metasurface are arranged in a two-dimensional array. By con-

trolling the positive-intrinsic-negative (PIN) diodes coupled with each reconﬁgurable element,

the reconﬁgurable element can adjust its electromagnetic response to the incident RF signals.

For each reconﬁgurable element, we refer to the different responses to incident RF signals as

the reconﬁgurable element’s conﬁguration as in [25]. By changing the conﬁguration of each

reconﬁgurable element, the metasurface is able to modify the reﬂected signals and perform

beamforming [26].

We assume that each reconﬁgurable element has NS conﬁgurations, and each conﬁguration

of an element has a unique reﬂection coefﬁcient for the incident RF signals. To be speciﬁc, we

assume that each row and column of the metasurface contain the same number of reconﬁgurable

MetasurfaceTxRxTarget SpaceLoS PathMulti-pathObjectReflection Pathelements, and the total number of reconﬁgurable elements is denoted by N . Based on [27], we

denote the reﬂection coefﬁcient of the n-th reconﬁgurable element corresponding to the incident

6

signal from the TX unit and the reﬂected signal towards the m-th space grid by rn,m(cn). Here,
Z, where Z

[1, NS] denotes the conﬁguration of the n-th reconﬁgurable element and cn

cn

∈

∈

denotes the set of integers.

B. Channel Model

In the metasurface assisted RF sensing scenario, the Tx unit and Rx unit adopt single antennas

to transmit and receive RF signals. The Tx antenna is a directional antenna, which points towards

the metasurface so that most of the transmitted signals are reﬂected by the metasurface and

propagate into the target space. The signals reﬂected by the metasurface are reﬂected by the

objects in the target space and then reach the Rx antenna. The Rx antenna is assumed to be

omni-directional and located right below the metasurface, as shown in Fig. 1. This setting ensures

that the signals reﬂected by the metasurface are not directly received by the Rx antenna, and

thus most of the received signals contain the information of the objects in the target space.

As shown in Fig. 1, the transmission channel from the Tx antenna to the Rx antenna is

composed of three types of paths, i.e., the line-of-sight (LoS) path, the reﬂection paths, and the

environmental scattering paths. The LoS path indicates the direct signal path from the Tx antenna

to the Rx antenna. The reﬂection paths are the paths from the Tx antenna to the Rx antenna

via the reﬂections from the metasurface and the objects in the target space. The environmental

scattering paths account for the signals paths between the Tx antenna and the Rx antenna which

involve complex reﬂection and scattering in the surrounding environment. Then, the equivalent

baseband representation of the received signal containing the signals from all these three types

of paths is denoted by y and can be expressed as

y = hlos

√P

·

x +

·

M
(cid:88)

N
(cid:88)

m=1

n=1

hn,m(cn, νm)

√P

·

·

x + hrl

√P

·

·

x + σ,

(1)

where P is the transmit power, and x denotes the transmitted symbol.

The component terms of (1) can be explained in detail as follows. The ﬁrst term, i.e., hlos

P

x,

·

·

corresponds to the signal received in the LoS path, where hlos denotes the gain. Based on [28],

hlos can be expressed as

hlos =

λ
4π ·

√gT gR

e−j2πdlos/λ
·
dlos

,

(2)

M

·

C

∈

where λ is the wavelength of the signal, gT and gR denote the gains of the Tx and Rx antennas,

7

respectively, and dlos is the distance from the Tx antenna to the Rx antenna.

The second term in (1) corresponds to the signals that reach the Rx antenna via N

reﬂection paths. In the second term, hn,m(cn, νm) denotes the gain of the reﬂection path via the

n-th reconﬁgurable element in conﬁguration cn and the m-th space grid with reﬂection coefﬁcient

νm. Based on [26, 29], hn,m(cn, νm) can be formulated as follows

hn,m(cn, νm) =

rn,m(cn)

λ2

·

νm
·
·
(4π)2

√gT gR
dn

e−j2π(dn+dn,m)/λ
·
dn,m

,

(3)

·

·

where dn denotes the distance from the Tx antenna to the n-th reconﬁgurable element and dn,m

denotes the distance from the n-th reconﬁgurable element to the Rx antenna via the center of

the m-th space grid.

Finally, the third and forth terms in (1) correspond to the signals from the environmental

scattering paths and the additive noise at the Rx antenna, respectively. The symbol hrl

denotes the equivalent gain of all the environmental scattering paths, and σ is a random signal

that follows the complex normal distribution, σ

(0, (cid:15)) with (cid:15) being the power of the noise.

∼ CN
Moreover, we refer to the vector of conﬁgurations selected for the N reconﬁgurable elements

as a beamformer pattern of the metasurface, which can be represented by a N

NS-dimensional

binary row vector c = ( ˆo(c1), ..., ˆo(cN )). Speciﬁcally, ˆo(i) (

[1, NS]) denotes the NS-

×

i
∀

∈

dimensional row vector whose i-th element is 1 and the other elements are 0. Based on the

deﬁnition of the beamformer pattern, the received signal in (1) can be reformulated as

y = hlos

√P

x + cAν

√P

x + hrl

√P

x + σ,

·
where ν = (ν1, . . . , νM ) denotes the vector of reﬂection coefﬁcients of the M space grids,
A = (α1, . . . , αM ) is referred to as the projection matrix, and αm = ( ˆαm,1, . . . , ˆαm,N )T with

·

·

·

·

·

(4)

ˆαm,n = (ˆαm,n,1, . . . , ˆαm,n,NS ). Here, for all m
denotes the channel gain of the reﬂection path via the n-th reconﬁgurable element in conﬁguration

[1, NS], ˆαm,n,i

[1, N ], and i

[1, M ], n

∈

∈

∈

i and the m-the space grid with a unit reﬂection coefﬁcient, which can be expressed as follows

based on (3).

ˆαm,n,i =

λ2

·

C. RF Sensing Protocol

√gT gR

rn,m(i)
(4π)2dndn,m

·

e−j2π(dn+dn,m)/λ.

·

(5)

To describe the RF sensing process in the metasurface assisted scenario clearly, we formulate

the following RF sensing protocol. In the protocol, the timeline is slotted and divided into cycles,

8

Fig. 2. A cycle of the RF sensing protocol.

and the Tx unit, the Rx unit, and the metasurface operate in a synchronized and periodic manner.

As shown in Fig. 2, each cycle consists of four phases: a synchronization phase, a calibration

phase, a data collection phase, and a data processing phase. During the synchronization phase,

the Tx unit transmits a synchronization signal to the metasurface and to the Rx unit, which

identiﬁes the start time of a cycle.

Then, in the calibration phase, the Tx unit transmits a narrow band constant signal, i.e., sym-

bol x, at frequency fc. The metasurface sets the beamformer pattern to be c0 = ( ˆo(1), . . . , ˆo(1)),

i.e., the N reconﬁgurable elements are in their ﬁrst/default conﬁguration. Besides, the received

signal of the Rx unit is recorded as y0.

The data collection phase is divided into K frames that are evenly spaced in time. During

this phase, the Tx unit continuously transmits the narrow band RF signal, while the metasurface

changes its beamformer pattern at the end of each frame. As shown in Fig. 2, we denote the

beamformer patterns of the metasurface corresponding to the K frames by binary row vectors

c1, . . . cK. Speciﬁcally, the K beamformer patterns of the metasurface during the data collection
K)T . Besides, as ck is

phase constitutes the control matrix, which is denoted by C = (cT

1 , ..., cT

a binary row vector, control matrix is a binary matrix.

To remove the signal form the LoS path which contains no information of the target space,

the received signals in the K frames are subtracted by y0. The K differences constitute the

measurement vector, which is a noisy linear transformation of ν by the matrix Γ , i.e.,

˜y = y

y0 = Γ ν + ˜σ,

(6)

−
C0)A with C0 = (cT

where Γ = √P

0 )T , y is a K-dimensional vector consisting
0 , . . . , cT
of the sampled received signals during the K frames that can be calculated by (4), y0 is a K-

(C

−

x

·

·

dimensional vector with all the elements being y0, and ˜σ is the difference between the noise

!"#$%&’#()*+(’#,-%*./01,!(2#*3,4!!!(#/,5*6/,!(2#*37!"8’#9(2:&*+(’#.(cid:1)(cid:1)!!!"!#!$#$%&’()*&+$,"(cid:1)(cid:1);*+*,8’33/$+(’#,-%*./;*+*,-&’$/..(#2-%*./!"##$%&’()’*+’,$&%"-.!%8*3(<&*+(’#,-%*./signals and environmental scattering signals of y and y0. In this article, we assume that the

environment in the considered scenario is static or changing slowly. In this case, the signals

9

from the environmental scattering paths, i.e., hrl
the difference between the Gaussian noise signals of y and y0.1 Speciﬁcally, the k-th element

x is subtracted in (6), and ˜σ contains

·

·

√P

of ˜σ is ˜σk

(0, 2(cid:15)). We refer to ˜y as the measurement vector. Since Γ determines how the

∼ CN

reﬂection characteristics of the objects are mapped to the measurement vector, we refer to Γ as

the measurement matrix.

Finally, during the data processing phase, the receiver maps the measurement vector obtained

in the data collection phase to the sensing results, which is a vector indicating the probabilities

that objects exist in the M space grids. Given control matrix C, the mapping is modeled through

a parameterized function, i.e., ˆp = f w( ˜y) with w being the parameter vector that is referred

to as the mapping of the received signals. Moreover, the result of the mapping, i.e., ˆp, is an

M -dimensional real-valued vector. Speciﬁcally, its m-th element, i.e., ˆpm

[0, 1], indicates the

∈

ˆpm) indicates the probability

probability that an object exists at the m-th space grid; therefore (1

that the m-th space grid is empty.

−

III. PROBLEM FORMULATION

In this section, we formulate the optimization problem for maximizing the sensing accuracy

for the considered scenario. We adopt the cross-entropy loss as the objective function to measure

the sensing accuracy, as minimizing the cross-entropy loss function can signiﬁcantly improve the

accuracy of classiﬁcation and prediction [31]. In other words, the sensing accuracy is inversely

proportional to the cross-entropy loss.

We deﬁne the cross-entropy loss in the considered scenario as

LCE =

Eν∈V

−

(cid:104) M
(cid:88)

m=1

pm(ν)

·

ln(ˆpm) + (1

pm(ν))

ln(1

·

−

−

(cid:105)
,

ˆpm)

(7)

denotes the set of all possible reﬂection coefﬁcient vectors corresponding to the existence

where

V

of objects in the target space, and pm(ν) is a binary variable indicating the object existence in

1If the environment is changing rapidly, hrl ·

√

P · x can be considered as an additional complex Gaussian noise [30], and ˜σ

in (6) is composed of the difference of the noise signals at the Rx and that of the environmental scattering signals, and thus its

variance is 2(cid:15) + 2(cid:15)hl.

the m-th space grid. Speciﬁcally, pm(ν) can be expressed as

pm(ν) =




0,



1,

= 0,

if

νm
|

|
otherwise.

10

(8)

In (7), ˆp is determined by f w( ˜y). Generally, parameterized function f w( ˜y) can take any

form. For example, it can be a linear function, i.e., f w( ˜y) = W ˜y + w(cid:48), where W and w(cid:48) are

determined by w and obtained by minimizing the mean squared error of the sensing results [32].

Besides, f w( ˜y) can also be a nonlinear decision function, which determines the sensing results of

˜y by using conditional probabilities [33]. In this paper, we consider that f w( ˜y) is nonlinear and

modeled as a neural network, where the elements of w stand for the weights of the connections

and the biases of the nodes. We refer to the neural network for f w( ˜y) as the sensing network.

The optimization problem for the metasurface assisted scenario that maximizes the sensing

accuracy can be formulated as the following cross-entropy minimization problem, where the

control matrix and the mapping of the received signals parameter are the optimization variables,

i.e.,

(P1) : min
C,w

LCE(C, w),

s.t. (ˆp1, ..., ˆpM ) = f w( ˜y),

˜y = √P

C = (cT

x

(C

·

·
1 , ..., cT

−
K)T ,

C0)A + ˜σ,

ck = ( ˆo(ck,1), ..., ˆo(ck,N )),

k

ck,n

∈

[1, NS],

k

∀

∈

[1, K],

∈
[1, N ].

∀
[1, K], n

∈

(9)

(10)

(11)

(12)

(13)

(14)

In (P1), (9) indicates that the objective is to minimize the cross-entropy loss by optimizing C and

w. As ˆp is determined by f w( ˜y) and ˜y is determined by control matrix C, LCE deﬁned in (7)

can be expressed as a function of C and w. Constraint (10) indicates that the probabilities for

the M space grids to contain objects are calculated by the mapping of the received signals, i.e.,

f w( ˜y). Constraint (11) indicates that the measurement vector is determined by control matrix

C as in (6). Besides, constraints (12)

(14) are due to the deﬁnition of the control matrix in

Section II-C. Since the control matrix is a binary matrix and w is a real-valued vector, (P1) is

∼

a mixed-integer optimization problem and is NP-hard.

11

To tackle it efﬁciently, we decompose (P1) into two sub-problems, i.e., (P2), and (P3), as

follows:

(P2) : min

w

(P3) : min

C

LCE(C, w),

s.t. (10).

LCE(C, w),

s.t. (11) to (14).

(15)

(16)

In (P2), we minimize the cross-entropy loss by optimizing w given C, and in (P3), we minimize

the cross-entropy loss by optimizing C given w. Based on the alternating optimization tech-

nique [34], a locally optimal solution of (P1) can be solved by iteratively solving (P2) and (P3).

Nevertheless, given w, (P3) is still hard to solve due to the large number of integer variables

in the control matrix. Moreover, the number of iterations for solving (P2) and (P3) can be

large before converging to the local optimum of (P1). If traditional methods, such as exhaustive

search and branch-and-bound algorithms, are applied, they will result in a high computational

complexity. To solve (P2) and (P3) efﬁciently, we develop an MDP framework and solve it

by proposing an PRPG algorithm, which are discussed in the next section. Furthermore, the

convergence of the proposed algorithm to solve (P1) is analyzed in Section V.

IV. ALGORITHM DESIGN

In this section, we formulate an MDP framework for (P2) and (P3) in Subsection A and

propose a deep reinforcement learning algorithm named PRPG to solve it in Subsection B.

A. MDP Formulation

In (P3), the optimization variable C is composed of a large number of binary variables

satisfying constraints (12)

(14), which makes (P3) an integer optimization problem which is

∼

NP-hard and difﬁcult to solve. Nevertheless, the metasurface can be considered as an intelligent

agent who determines the conﬁguration of each reconﬁgurable element for each beamformer

pattern sequentially, and is rewarded by the negative cross-entropy loss. In this regard, the

integer optimization problem (P3) can be considered as a decision optimization problem for

the metasurface, which can be solved efﬁciently by the deep reinforcement learning technique,

since it is efﬁcient to solve highly-complexed decision optimization problems for intelligent

agents [35–36]. As the deep reinforcement learning algorithm requires the target problem to be

formulated as an MDP, we formulate (P2) and (P3) as an MDP, so that we can solve them by

proposing an efﬁcient deep learning algorithm.

An MDP encompasses an environment and an agent, and consists of four components: the set

12

of states

, the set of available actions

, the state transition function

, and the reward function

S

A

T

[23]. The states in

R
state and the adopted action. Suppose the agent takes action a in state s, and the consequent

S

obey the Markov property, i.e., each state only depends on the previous

state s(cid:48) is given by the transition function

, i.e., s(cid:48) = T (s, a). After the state transition, the

agent receives a reward that is determined by reward function

, i.e.,

(s(cid:48), s, a).

R

R

To formulate the MDP framework for (P2) and (P3), we view the metasurface as the agent,

T

and the RF sensing scenario including the surroundings, the RF transceiver, and the objects

in the target space are regarded, altogether, as the environment. We consider the state of the

metasurface the current control matrix, i.e., C and the action of the metasurface as selecting

the conﬁguration of a reconﬁgurable element for a beamformer pattern. Thus, actions of the

metasurface determine the elements in control matrix C. Therefore, the next state of the MDP

is determined by the current state and the action, and the Markov property is satisﬁed. In the

following, we describe the components of the MDP framework in detail.

State: In the MDP of the metasurface assisted RF sensing scenarios, the state of the envi-

ronment is deﬁned as

where k

[1, K] and n

∈

∈

s = (k, n, C),

(17)

[1, N ] are the row and column indexes for control matrix indicating the

conﬁguration that the metasurface aims to select. Besides, C in (17) denotes the current control

matrix of the metasurface in state s. The initial state of the MDP framework is denoted by

s0 = (1, 1, C0), where C0 is the control matrix of the metasurface whose reconﬁgurable elements

are in the ﬁrst/default conﬁguration. We refer to the states with indices (k, n) = (K + 1, 1) as

the terminal states. When the terminal states are reached, all the conﬁgurations in the control

matrix have been selected.

Action: In each state s = (k, n, C), the metasurface selects the state of the n-th reconﬁgurable

element in the k-th frame. The action set of the metasurface in each state can be expressed as

=

1, ..., NS

, where the j-th action (i

[1, NS]) indicates that the metasurface selects the

{

}

A
target conﬁguration to be the j-th conﬁguration. In other words, the metasurface sets (C)k,n =
ˆo(a), a

∈

.
∈ A

State Transition Function: After the metasurface selects the action, the MDP framework

transits into the next state, s(cid:48) = (k(cid:48), n(cid:48), C (cid:48)), if (k, n)

= (K, N ). Or, if (k, n) = (K, N ), the agent

(cid:54)
13

Fig. 3. Example of the state transition in the formulated MDP, with K = 2, N = 1, and NS = 2.

enters the terminal state of the MDP. For the non-terminal states, the elements of state s(cid:48) given

s and a can be expressed as follows

k(cid:48) = k + 1, n(cid:48) = mod(n + 1, N ) + 1,

(C (cid:48))k(cid:48)(cid:48),n(cid:48)(cid:48) =






(C)k(cid:48)(cid:48),n(cid:48)(cid:48) , if (k(cid:48)(cid:48), n(cid:48)(cid:48))

= (k, n),

ˆo(a) if (k(cid:48)(cid:48), n(cid:48)(cid:48)) = (k, n),

(18)

(19)

k(cid:48)(cid:48)
∀

∈

[1, K], n(cid:48)(cid:48)

[1, N ].

∈

An example of the state transition is illustrated in Fig. 3, where NS = 2, K = 2, and N = 1.

In Fig. 3, the red dotted box indicates the element of C that is determined by the action in

the current state. If (k, n) = (3, 1), it can be observed that all the conﬁgurations of the control

matrix have been determined, and the MDP transits into the terminal states, where control matrix

is denoted by Ct.

Reward Function: In general MDP frameworks, the reward is a value obtained by the

agent from the environment and quantiﬁes the degree to which the agent’s objective has been

achieved [23]. The reward for the agent is deﬁned as the negative cross-entropy loss of the

mapping of the received signals given the control matrix determined in the terminal states. If

the terminal state has not been reached, the reward for the state transition is set to be zero.

Speciﬁcally, given parameter w, the reward in state s is deﬁned as

(s

R

w) =
|






−
0,

LCE(Ct, w),

if s is a terminal state,

(20)

otherwise.

!"#$%"#$!"#$#%&’($%$)’&!*+$#,"’’"*+$#,"’’#(")*+#,)*+#,!"-$%"#$(")*+#,)*+#,!"-$%"#$(")*+-,)*+#,!".$%"#$($")*+#,)*+#,!".$%"#$($")*+#,)*+-,!".$%"#$($")*+-,)*+#,!".$%"#$($")*+-,)*+-,*+$#,"’’"*+$#,"’’"*+$#,"’’#*+$#,"’’#-)./#"%&’($%$)0(cid:54)
In the formulated MDP, the metasurface aims for obtaining an optimal policy to obtain the

maximum reward in the terminal states. To be speciﬁc, the policy of the agent is a mapping

14

from the state set to the available action set, i.e., π :

. To deﬁne the optimal policy π∗,

S → A

we ﬁrst deﬁne the state-value function given policy π and parameter vector w, which indicates

the accumulated reward of the agent via a certain state. Based on (20), the state-value function

can be expressed as

V (s






−
V (s(cid:48)

π, w) =
|

LCE(C, w), if s is a terminal state,

(21)

s(cid:48)=T (s,π(s)), otherwise,
π, w)
|
|
The state-value function for π in state s indicates the accumulated rewards of the agent after

state s. Based on (21), the state-value function for the initial state can be expressed as

V (s0

π, w) =
|

LCE(C π

t , w),

−

(22)

where C π

t denotes the terminal state of the metasurface adopting policy π.

Therefore, given parameter vector w, the optimal policy of the agent in the MDP framework

is given by

π∗(w) = arg max

π

V (s0

π, w)
|

⇐⇒

arg min

C

LCE(C, w).

(23)

In (23), it can be observed that ﬁnding the optimal policy of the agent in the formulated MDP

framework is equivalent to solving the optimal control matrix for (P3). Besides, solving (P2) is

equivalent to solving the optimal w given the policy π.

B. Progressing Reward Policy Gradient Algorithm

To jointly solve (P2) and (P3) under the formulated MDP framework, we propose a novel

PRPG algorithm. The proposed algorithm can be divided into two phase, i.e., the action selection

phase and the training phase, which proceed iteratively.

1) Action Selection Process: In the proposed algorithm, the agent, i.e., the metasurface, starts

from the initial state s0 and adopts the policy for selecting action in each state until reaching the

terminal state. To select the current action in each state, the metasurface use policy π that maps

the current state to a probability vector. To be speciﬁc, for a given state s, the policy results

in an NS-dimensional probability vector denoted by π(s

function. The i-th element of π(s

w) (i
|

∈

[1, NS]), i.e., πi(s

w), which we refer to as the policy
|

w), is in range [0, 1] and denotes
|

15

[1, NS]) satisﬁes

N

NS binary

·

·

Fig. 4. Network structure of the policy network used in the proposed algorithm.

the probability of selecting the action ai in state s. Besides, π(s
(cid:80)NS

i=1 πi(s
However, since the state contains the current control matrix that contains K

w) = 1.
|

w) (i
|

∈

variables, the agent faces a large state space, and the policy function is hard to be modeled by

using simple functions. To handle this issue, we adopt a neural network to model the policy

function as neural networks are a powerful tool to handle large state space [37]. The adopted

neural network is referred to as the policy network, and we train the policy network by using the

policy gradient algorithm [35]. Speciﬁcally, the policy network is denoted by πθ(s

w), where
|

θ denotes the parameters of the policy network and comprises the connection weights and the

biases of the activation functions in the neural network.

The structure of the policy network is shown in Fig. 4. In state s, k and n are embedded as

a K-dimensional and an N -dimensional vectors, respectively, where the k-th and n-th elements

in the vectors are ones and the other elements are zeros. Speciﬁcally, we refer to the resulted

vectors as the one-hot vectors. As for C, since the RF sensing for the target space is determined

by CA as shown by (4), we ﬁrst divide C to its real and imaginary parts and right-multiply

them by the real and imaginary parts of A, respectively. Then, driven by the concept of model-

based learning [38], we process the result, i.e., CA, by multi-layer perceptrons (MLPs). Besides,

since the K beamformer patterns are symmetric in their physical meaning and changing their

order does not impact the sensing performance, the MLPs that extract feature vectors from c1 to

cK need to be symmetric. This can be achieved by utilizing two symmetric MLP groups, each

containing K MLPs with shared parameters. This signiﬁcantly reduces the number of parameters

and thus facilitates the training of the policy network. The sizes of the MLPs are labeled in Fig. 4.

!!"#!"#$%&’()*+,-&../%0$%&’()*+,-&../%0!1*2*&"#$%!%"!!13,,&*435!"#64)789&’()&*+,-&+&,./!:)%%&;*)4!!)0&’1’12+.3+2#/!"#1)<*,2=!!#4)-2-/>/*/&95<)452;*/)%9?4)!56/$789:);/$<=9>);/$789:);/$<=9>);/!!"#!"#16

For example, (2M, 512, 256) indicates that each MLP in a symmetric group that has three layers

whose sizes are 2M , 512, and 256, respectively. Then, the one-hot vectors and the 2K extracted

feature vectors are connected and input to the ﬁnal MLP. The result of the ﬁnal MLP is fed

into the softmax layer which produces an NS-dimensional vector indicating the probability of

selecting the NS actions.

2) Training Process: The purpose of the training process is two-fold: (a) To make the policy

network improves the current policy in action selection based on (23). (b) To make the mapping of

the received signals incur lower cross-entropy loss. Accordingly, the training process consists of

two parts, i.e., training of the policy network and training of the sensing network. In the training

of the policy network, we adopt the policy gradient method [23]. Besides, the training of the

sensing network results in that the rewards for the terminal states progress during the training of

the policy. Due to these characteristics, the proposed algorithm is named as progressing reward

policy gradient algorithm.

Training of the Policy Network: To collect the training data for the policy network, a replay

buffer is adopted in order to store the experiences of the agent during state transitions. The replay

buffer of the agent is denoted by

=

e
{

. The stored experience in the replay buffer is given
}

B

by e = (s, a). It is worth noting that, differently from the replay buffer in traditional deep

reinforcement learning algorithms [35], the experience in the replay buffer does not record the

reward obtained during the state transitions. This is because the rewards are determined by the

current mapping of the received signals, which changes as w being updated. Thus, we propose

that the rewards are calculated when the training process is invoked, instead of being recorded

in the replay buffer.

We deﬁne a training epoch (or epoch in short) as the state transition process from the initial

state to a terminal state. The experience of the agent within an epoch is stored into the replay

buffer and used for training, which is discarded after being used. Based on the policy gradient

theorem[23], in the training process, the gradient of V (s0

θV (s0

π, w)
|

∝

∇

(cid:20)

EB,πθ

V (

T

θ, w)∇
(St, At)
|

π, w) with respect to θ satisﬁes
|
(cid:21)

,

(24)

θπθ
πθ
At

At(St
(St

w)
|
w)
|

are the samples of the state and action in the replay buffer of an agent

θ, w) denotes the reward for the agent after selecting the
|

where (St, At)
∈ B
following policy πθ, and Q(St, At
action At in St and then following πθ.

17

Fig. 5. Sensing network of the metasurface.

To calculate the gradient in (24), the rewards for the agent in (20) need to be calculated. If

s is a terminal state, the reward R(s

i.e.,

w) is calculated by using the Monte Carlo methods [39],
|

R(s

w) =
|

−

(cid:88)

Nmc(cid:88)

(cid:16) M
(cid:88)

ν∈V

i=1

m=1

pm(ν) ln(ˆpm) + (1

pm(ν)) ln(1

−

ˆpm)

−

(cid:17)(cid:12)
(cid:12)
(cid:12) ˆp=f w(Γ ν+ ˜σi)

.

(25)

Otherwise, R(s

w) = 0. In (25), Nmc indicates the number of sampled noise vectors, and ˜σi is the
|

i-th sampled noise vector. As the rewards in the non-terminal states are zero, V (

(St, At)

θ, w)
|

T

is equal to the reward at the ﬁnal state for St, At, and policy πθ.

Speciﬁcally, in (25), ˆp is generated by the sensing network, which is shown in Fig. 5. The

sensing network consists of two parts, i.e., the model-aided decoder and an MLP. Firstly, the
received vector is left-multiplied by the pseudo inverse of Γ , which is denoted by Γ + and

can be calculated based on [40]. According to the least-square method [32], the model-aided
decoder, i.e., ˆν = Γ +y, is the optimal linear decoder that results in the minimum mean square

error (MSE) for the actual reﬂection vector ν, and thus can potentially increase the sensing

accuracy of the sensing network. Then, ˆν is fed into a fully-connected MLP, which reconstructs

the probability vector ˆp.

In each process, θ is updated as follows

(cid:20)

θ = θ + α

θπθ
πθ
At
w) is calculated by using the back-propagation algorithm [41],
|
and α denotes the training rate.

where the gradient

w)
|
w)
|

θ, w)∇
|

At(St
(St

(St, At)

At(St

Ee∈B

θπθ

(26)

V (

∇

T

·

,

(cid:21)

Training of the Sensing Network: After updating θ, the training of the sensing network is

executed. The calculated rewards from (25) are used to train the sensing network which reduces

the cross-entropy loss. To be speciﬁc, the loss function used to train the sensing network can

be expressed as follows, which is in accordance with the objective function in the optimization

problem (P2), i.e.,

I(w) = E(s,a)∈B[R(s

w)].
|

L

(27)

!"Received vector×(2&,128,64,2&)MLPModel-aided decoder-.Sensing resultΓ+real partimaginary partAlgorithm 1: Proposed PRPG Algorithm

Input: Random initial network parameter vectors θ and w;

18

Empty replay buffer

B
Maximum number of training epochs Nep;

=

;
∅

Set of reﬂection coefﬁcient vectors

;

V

Number of Monte Carlo samples for noise Nmc;

Initial learning rate α0;

Maximum number of training epochs Nep

Output: Optimized sensing network parameter vector w∗ and the optimized policy network parameter θ∗.

for nep = 1 to Nep do

Set the current state to be the initial state, i.e., s = s0;

# Action selection phase

while s is not a terminal state do

Select the conﬁguration of the n-th reconﬁgurable element in the k-th frame following the

probability distribution given by πθ(s

w).
|

Set action a as the selected conﬁguration, and enter into the transited state s(cid:48) =

(s, a);

T

Store experience e = (s, a) into replay buffer

;

B

# Training phase

Collect all the experiences from

B

using (25);

, and calculate the reward for each sampled experience by

Update parameter θ and w by (26) and (28), respectively, where the learning rate

α =

α0
1+nep

10−3 ;
·

In each training process, w is updated by

where the gradient

w

∇

L

w = w + α

w

∇

I(w),

L

(28)

I(w) is calculated by using the back-propagation algorithm.

In summary, the proposed PRPG algorithm is summarized in Algorithm 1.

Remark: Using the proposed deep reinforcement learning technique enables our proposed

algorithm to handle the complicated cases where multiple metasurfaces exist. Speciﬁcally, when

the multiple metasurfaces are on the same plane, they can be considered as a whole, and thus the

channel model in (1) needs no changes. When the multiple metasurfaces are on different planes,

the channel model needs to be modiﬁed to adapt to the correlation between different metasurfaces,

which is left for future work. Nevertheless, since the problem formulation and the proposed

19

algorithm are independent of the speciﬁc channel model, the proposed problem formulation and

algorithm can also be adopted for the scenarios to optimize the sensing performance of the

general RF sensing scenarios with multiple metasurfaces.

V. ALGORITHM ANALYSIS

In this section, we analyze the computational complexity and the convergence of the proposed

algorithm in Subsections A and B, respectively. In addition, in Subsection C, we derive a non-

trivial lower-bound for the sensing accuracy based on an upper-bound for the cross-entropy loss

given a control matrix.

A. Computational Complexity

Since the PRPG algorithm consists of two main phases, i.e., the action selection phase and

the training phase, we analyze their respective computational complexities. The computational

complexities are analyzed with regard to the number of beamformer patterns, K, the number of

reconﬁgurable elements, N , the number of available conﬁguration, NS, and the number of space

grids, M .

1) Complexity of the Action Selection Phase: In the proposed algorithm, the computationally

most expensive part is the estimation of the action probabilities of the policy network. For each

action selection phase, the computational complexity is given in Theorem 1.

Theorem 1: (Computational Complexity of the Action Selection Phase) In the PRPG al-

gorithm, for the agent in each state, the complexity to calculate the action probabilities and

determine the action is

(KN NSM ).

Proof 1: See Appendix A.

O

2) Complexity of the Training Process: The computational complexity of (25) is provided

in Lemma 1.

Lemma 1: The computational complexity of the reward calculation in (25) is

(cid:0)KN NSM + K 2M + M 2(cid:1) .

O

Proof 2: See Appendix B

The computational complexities of training the policy network and the sensing network are

given in Lemma 2.

Lemma 2: After calculating the rewards, the complexity of the training the sensing network

20

and the policy network are

(M 2) and

(NS(K + N + M )), respectively. If a single MLP

O

O

is used to substitute the symmetric MLP group, the computational complexity of training the

policy network is

(KM NS + N NS).

Proof 3: See Appendix C.

O

It can be observed from Lemma 2 that using a symmetric MLP group instead of a single

large MLP in the policy network can reduce the complexity of the training process.

Based on Lemmas 1 and 2, the total computational complexity of each training process is

provided in Theorem 2.

Theorem 2: (Computational Complexity of the Training Process) The computational com-

plexity of each training phase of the PRPG algorithm is

Proof 4: See Appendix D.

(KN NSM + K 2M + M 2) .

O

B. Convergence Analysis

The detailed convergence analysis of the PRPG algorithm is based on the convergence

analysis of the block stochastic gradient (BSG) algorithm. We denote w by x1 and denote
θ by x2, and thus the objective function in (P1) can be denoted by F (x1, x2) = LCE(C πθ
where C πθ

indicates the control matrix in the terminal state for the metasurface with policy

, w),

t

t

πθ. Based on [42], a BSG algorithm for solving (P1) is formulated as Algorithm 2, whose

convergence analysis can be given by Lemma 3.

Lemma 3: Algorithm 2 converges to a locally optimal x∗

1 and x∗

2 as the number of iterations

Nitr

, given that the following conditions are satisﬁed:

→ ∞

1) There exist a constant c and a constant ε such that, for each iteration indexed by j, the

inequalities

E[˜gj
(cid:107)

i − ∇

xiF (x1, x2)]
2
(cid:107)

c

·

≤

maxi(αj

i ) and E[

˜gj
i − ∇
(cid:107)

xiF (x1, x2)
(cid:107)

2]

≤

ε2,

i = 1, 2 are fulﬁlled.

2) There exists a uniform Lipschitz constant (cid:37) > 0 such that

(cid:88)
i=1,2(cid:107)∇

xiF (x1, x2)

xiF (x(cid:48)

1, x(cid:48)
2)
(cid:107)

−∇

2
2 ≤

(cid:37)2 (cid:88)

i=1,2 (cid:107)

xi

2
x(cid:48)
2.
i(cid:107)

−

3) There exists a constant ψ such that E[

xj
1(cid:107)
(cid:107)

2
2 +

xj
2
2]
2(cid:107)

(cid:107)

≤

ψ2,

j.

∀

Proof 5: Please refer to Corollary 2.12 in [42], where the assumptions required in Corol-

lary 2.12 in [42] are equivalent to the three conditions in Lemma 3.

21

Comparing Algorithms 1 and 2, we can observe that the only difference between the two

algorithms is in the functions for updating parameters. Nevertheless, solving the minimization

problem (30), we can derive that (30) is equivalent to that

i = xj−1
xj

i −

i ˜gj
αj
i .

(29)

As the learning rate sequence

j in Algorithm 2 can be arbitrarily selected, the parameter

αj
i }

{

update of Algorithms 1 and 2 are essentially equivalent. In this regard, the proposed PRPG algo-

rithm can be categorized as an BSG algorithm, whose convergence analysis follows Lemma 3.

However, since neural networks are encompassed in the mapping of the received signals and

the policy function, the conditions in Lemma 3 are hard to be proven theoretically. Therefore, in

additional to the theoretical analyses provided above, we also analyze the convergence through

practical simulations in Section VI.

Moreover, the obtained solution by the proposed deep learning algorithm is a locally optimal

solution of (P1). As shown in Algorithm 1, we iteratively solve (P2) and (P3) by updating θ using

(26) and updating w using (28), respectively. Based on the Q-learning algorithm [23], updating

θ with the aim to maximize the total reward is equivalent to ﬁnding C minimizing LCE given

w. Besides, it can be observed that updating w directly minimizes LCE given C. When the

iteration terminates, updating the variables of C or w will not lead to a lower objective function

value, i.e., the cross-entropy loss. Therefore, the solution obtained by the proposed Algorithm 1

is a locally optimal solution of the original problem (P1).

C. Lower Bound for Sensing Accuracy

In this section, we compute a lower-bound for the sensing accuracy in (P2) given control

matrix C. To derive a lower bound, we assume that the mapping of the received signals maps

the received RF signals to the sensing results by using an optimal linear decoder and a threshold

judging process. In the following, we ﬁrst provide the detection criterion for sensing, and then

derive a lower-bound for sensing accuracy by leveraging an upper-bound for the cross-entropy

loss.

1) Detection Criterion for Sensing: The reconstructed reﬂection coefﬁcient vector from the

linear decoder can be expressed as

ˆν = Γ + ˜y = Γ +Γ ν + Γ + ˜σ.

(30)

Algorithm 2: BSG algorithm for solving (P1)

Input: Starting point x0

i , i = 1, 2;
αj

Learning rate sequence

i ; i = 1, 2
Maximum number of iterations Nitr;

{

}j=1,2,...;

Monte Carlo sampling size of the random noise Nmc.

Output: Optimized x∗1 and x∗2 for (P1).

for j = 1, 2, ..., Nitr do

for i = 1, 2 do

Compute sample gradient for the w in the j-th iteration by ˜gj

i =

∇

Update parameter xi by

xiF (xj

<i, x(j

−
i

≥

xj

i = arg min
xi

(˜gj
i )T (xi −

xj
i

1

−

) +

Output (xNitr

1

, xNitr
2

) as (x∗1, x∗2);

1
2αj

i (cid:107)

xi −

xj
i

1

−

2
2.
(cid:107)

22

1)

)

Based (30), we analyze the probability distribution of the random variable ˆνm, i.e., the m-th
element of ˆν. We denote the m-th row vectors of Γ + and Γ +Γ as γm and ξm, respectively.

Then, ˆνm = ξmν + γm ˜σ. The emptiness of the space grids other than the m-th space grid is
modeled by the vector q−m, where q−m,m(cid:48) = 0 and 1 indicate that the m(cid:48) space grid is empty
and nonempty, respectively, (m(cid:48)

= m). When the m-th space grid is empty (or

[1, M ], m(cid:48)

∈

nonempty), we denote the probability density functions (PDFs) of the real and imaginary parts

of ˆνm, i.e., ˆνR,m and ˆνI,m, by

0
I,i(x) (or
We judge the emptiness of the m-th space grid according to the sum of ˆνR,m and ˆνI,m, i.e.,

1
I,i(x)), respectively.

1
R,i(x) and

0
R,i(x) and

P

P

P

P

µm = ˆνR,m + ˆνI,m. When the m-th space grid is empty, given q−m, the sum of ˆνR,m and ˆνI,m,

i.e., µm, follows a normal distribution, i.e., µm

(0, (cid:15)0

m(q−m)), where

∼ N

(cid:15)0
m(q−m) =

(cid:88)

q−m,m(cid:48)

(cid:15)ref,m(cid:48)

·

ξR,m(cid:48)

(
(cid:107)

·

2 +
(cid:107)

ξI,m(cid:48)
(cid:107)

(cid:107)

2)

m(cid:48)(cid:54)=m,
m(cid:48)∈M

(cid:88)

+

m(cid:48)∈M

γR,m(cid:48)

(cid:15)

(
(cid:107)

·

(cid:107)

2 +

γI,m(cid:48)

(cid:107)

2).
(cid:107)

(31)

Here,

is the set of indexes of M space grids, and subscripts R and I indicate the real and

M

imaginary parts of a vector, respectively. The ﬁrst summation term in (31) corresponds to the

variance due to the reﬂection coefﬁcients at the space grids other than the m-th space grid, and

the second summation term in (31) corresponds to the variance due to the noise at the Rx unit.

(cid:54)
On the other hand, when the q-th space grid is nonempty, the variance due to reﬂection

coefﬁcient of the m-th space grid needs to be added. Denote the variance of the reﬂection

coefﬁcient of the m-th space grid by (cid:15)ref,m, and the variance of µm can be expressed as

23

m(q−m) = (cid:15)0
(cid:15)1

m(q−m) + (cid:15)ref,m

ξR,m

(
(cid:107)

·

(cid:107)

2 +

ξI,m

2).

(cid:107)

(cid:107)

Given the emptiness of the m-th space grid, the PDF of µm can be written as follows

i
m(x) =

P

(cid:88)

q−m∈Q−m

Pm(q−m)

norm(x; 0, (cid:15)i

m(q−m)), i = 0, 1

P

(32)

(33)

where

−m indicates the set of all possible q−m,

norm(x; 0, (cid:15)i

PDF of a normal distribution with zero mean and variance (cid:15)i

P

m(q−m)) (i = 0, 1) denotes the
m(q−m), and Pm(q−m) denotes the

Q

probability for the existence indicated by q−m to be true, i.e.,

Pm(q−m) =

(cid:89)

P rm(cid:48)(q−m,m(cid:48)).

(34)

m(cid:48)(cid:54)=m,m(cid:48)∈M
Here, P rm(cid:48)(x) with x being 0 and 1 indicates the probabilities that the m(cid:48)-th space grid are

empty and nonempty, respectively.

We use the difference between

0
m(q−m) as the judgement variable to deter-
mine whether the m-th space grid is empty or not. To facilitate the analysis, we adopt the log-sum

1
m(q−m) and

P

P

as a substitute for the sum in (33). Therefore, the judgement variable can be calculated as

(cid:88)

τm =

q−m∈Q−m
(cid:88)

−

q−m∈Q−m

ln (cid:0)pm(q−m)
P

norm(x; 0, (cid:15)1

m(q−m))(cid:1)

(35)

ln (cid:0)pm(q−m)

norm(x; 0, (cid:15)0

m(q−m))(cid:1) .

P

It can be observed from (35) that τm increases as

1
m(µm) increases, and that it decreases
0
m(µm) increases. Therefore, we can judge the emptiness of the m-th space grid through the
value of τm. Speciﬁcally, the sensing result of the m-th space grid is determined by comparing the

as

P

P

judging variable τm with the judging threshold, which is denoted by ρm. If τm

≤
result of the m-th space grid is “empty”, which is denoted by the hypothesis

if τm > ρm, the sensing result is “non-empty”, which is denoted by the hypothesis

simplifying (35), the detection criterion for

0 and

τm = µ2
m

(cid:88)

q−m∈Q−m

H
(cid:15)0
m(q−m)
m(q−m)(cid:15)0
m(q−m) −

(cid:15)1
m(q−m)
2(cid:15)1

−

1 can be expressed as
(cid:18) (cid:15)1
m(q−m)
(cid:15)0
m(q−m)

(cid:88)

ln

H
1
2

q−m∈Q−m

(cid:19) H1≷

H0

ρm.

(36)

Since µ2

m > 0, the range of ρm can be expressed [

1
2

−

(cid:80)

q−m∈Q−m

ln( (cid:15)1
m(q−m)
m(q−m)),
(cid:15)0

].

∞

ρm, the sensing

0. Otherwise,

H

1. After

H

2) Upper Bound of Cross Entropy Loss: We analyze the cross-entropy loss incurred by the

detection criterion in (36), which can be considered as a non-trivial upper-bound for the cross-

entropy loss deﬁned in (7). As the sensing result given by (36) is either 0 or 1, if the sensing

24

result is accurate, the incurred cross-entropy loss will be

ln(1) = 0; otherwise, the incurred

cross-entropy loss will be

ln(0)

. In practice, the cross-entropy loss due to an inaccurate

−
sensing result is bounded by a large number CIn0. Given

→ ∞

0 (or

1) being true, the probability

H

H

τm > ρm
for the sensing result to be inaccurate is the probability of τm > ρm, i.e., Pr
{

0
|H

}

(or

τm
Pr
{

ρm

1

). Denote the probability for an object to be at the m-th space grid by ˜pm, and
}

|H

≤

the cross-entropy loss of the m-th space grid can be calculated as

−

Lm = CIn0

(1

·

−

˜pm)

Pr
{

·

τm > ρm

0
|H

}

+ CIn0

˜pm

τm
Pr
{

·

ρm

1
|H

,
}

≤

·

(37)

τm > ρm
where Pr
{

0
|H

}

and Pr
{

τm

≤

ρm

1

|H

}

can be calculated by using Proposition 1.

Proposition 1: The conditional probability for sensing the m-th space grid inaccurately can

be calculated as follows

τm > ρm

Pr
{

0
|H

}

= Pr
{

µ2
m > ˆρm

0
|H

}

= 1

−

(cid:88)

q−m∈Q−m

Pm(q−m)

erf

·

(cid:32)(cid:115)

(cid:33)

ˆρm
m(q−m)

2(cid:15)0

,

(38)

τm

Pr
{

≤

ρm

1
|H

}

= Pr
{

µ2
m ≤

ˆρm

1
|H

}

=

(cid:88)

q−m∈Q−m

Pm(q−m)

erf

·

(cid:32)(cid:115)

(cid:33)

,

ˆρm
m(q−m)

2(cid:15)1

(39)

) denotes the error function [33], and
where erf(
·

(cid:80)

1
2

ln((cid:15)1

ˆρm =

q−m∈Q−m

q−m∈Q−m
(cid:80)

Proof 6: Based on (36), the judging condition τm

m(q−m)/(cid:15)0
m(q−m)−(cid:15)0
(cid:15)1
m(q−m)·(cid:15)0
(cid:15)1
H1≷
H0
µ2
m ≤
m follows a chi-squared distribution with one degree of freedom. Therefore, the cumulative
m is a weighted sum of error functions, and thus the conditional

µ2
m > ˆρm
Pr
{
q−m, µ2
distribution function of µ2

m(q−m)) + ρm
m(q−m)
m(q−m)

ρm is equivalent to µ2
m

τm > ρm
= Pr
{

ˆρm. Therefore,

and Pr
{

. Also, given

= Pr
{

H1≷
H0

0
|H

0
|H

1
|H

(40)

ˆρm

ρm

τm

|H

≤

}

}

}

}

.

1

probabilities can be calculated by using (38) and (39).

Besides, we can observe in (37) that Lm is determined by the judgment threshold ρm. Then,

based on (37) to (40), ∂Lm/∂ρm can be calculated as

∂Lm/∂ρm =

2CIn0
√π ·

∂ ˆρm
∂ρm ·

−

(cid:88)

q−m∈Q−m

Pm(q−m)

·

φm(q−m),

(41)

φm(q−m) =

(1

e−ˆρm/2(cid:15)0
˜pm)
·
−
(cid:112)8(cid:15)0
m(q−m)ˆρm

m(q−m)

−

˜pm
·
(cid:112)8(cid:15)1

e−ˆρm/2(cid:15)1

m(q−m)

m(q−m)ˆρm

25

(42)

.

Then, the optimal ρ∗

corresponding to ρ∗

m can be obtained by solving ∂Lm/∂ρm = 0. Denoting the minimal Lm
m, the upper bound for the cross-entropy loss in (7) can be calculated as

m as L∗

Lub =

(cid:88)

L∗
m.

m∈M

(43)

When the emptiness of the space grids other than the m-th is given, the upper bound of the

cross-entropy loss can be calculated from Proposition 2. Since the sensing accuracy is inversely

proportional to the cross-entropy loss, a lower-bound for the sensing accuracy is derived.

Proposition 2: When the emptiness of the space grids other than the m-th is given, i.e.,

−m =

Q

q−m

{

, the optimal judging threshold for the m-th space grid is
}

ρ∗
m(q−m) =




1

2 ln( (cid:15)0
2 ln( 1−˜pm
˜pm

m(q−m)
m(q−m) ), if ˜pm >
(cid:15)1

m(q−m)

(cid:113) (cid:15)1
m(q−m)+(cid:15)1
(cid:15)0
m(q−m)
m(q−m) ), otherwise.
(cid:15)1

m(q−m) ,

2 ln( (cid:15)0
Proof 7: The sign of ∂Lm/∂pm is determined by φm(q−m). We calculate the ratio between



−

)

1

(44)

the two terms of φm(q−m), which can be expressed as

ιm(q−m) =

(cid:115)

1

˜pm
−
˜pm ·

(cid:15)1
m(q−m)
(cid:15)0
m(q−m) ·

e

−ˆρm·

m(q−m)−(cid:15)0
(cid:15)1
m(q−m)·(cid:15)1
2(cid:15)0

m(q−m)
m(q−m) .

(45)

Since (cid:15)1

m(q−m) > (cid:15)0
respect to ρm and ιm(q−m)

m(q−m) and ˆρm

∝

0. Also, φm(q−m)

0

≥

⇐⇒

≥

ιm(q−m)

1, and thus,

≥

∂Lm/∂pm

0 if and only if ιm(q−m)

1. Therefore, the minimal Hm is obtained when ρm

≥

≥

satisﬁes the condition ιm(q−m) = 1. Then, we can prove Proposition 2 by solving ιm(q−m) = 1

ρm, ιm(q−m) is a monotonic decreasing function with

and considering that ρm

m(q−m)
m(q−m) ).
(cid:15)0
However, since the number of possible q−m can be large, (typically,

≥ −

2 ln( (cid:15)1

1

= 2M −1),

|Q

−m

|

calculating the exact ∂Lm/∂ρm in (41) is time-consuming, which makes it hard to ﬁnd the

exact ρ∗

subset of

m and L∗
sam
−m, which is denoted by

Q

sam
−m ⊂ Q

Q

−m.

m. Therefore, in practice, we approximate Hub by using a random sampled

Moreover, since the sign of ∂Lm/∂ρm is determined by the sum of φm(q−m), and φm(q−m)

has a zero point, which can be calculated by (44). If ρm is less than the zero point of φm(q−m),

φm(q−m)

≥

0; and otherwise φm(q−m) < 0. Therefore, we use the mean of the optimal

26

Fig. 6. Simulation layout.

ρ∗
m(q−m) for each q−m
The estimated ρ∗

∈ Q
m is denoted by ˜ρ∗

m, which can be formulated as follows

−m to estimate ρ∗
sam

m, and approximate the upper-bound accordingly.

˜ρ∗
m =

1
sam
−m|

|Q

(cid:88)

ρ∗
m(q−m),

q−m∈Qsam
−m

(46)

where ρ∗

m(q−m) can be obtained by Proposition 2. When

approximate ρ∗
m.

sam
−m|

|Q

is large enough, ˜ρ∗

m in (46) can

Finally, given the approximated upper bound of the cross-entropy loss as ˜Lub, then it can be

observed from (37) that the upper bound of average probability of sensing error for a space grid
is Perr,ub = ˜Lub/CIn0. Therefore, the lower bound of the average sensing accuracy for a space
grid is Pacc,lb = 1

Perr,ub.

−

VI. SIMULATION AND EVALUATION

In this section, we ﬁrst describe the setting of the simulation scenario and summarize the

simulation parameters. Then, we provide simulation results to verify the effectiveness of the

proposed PRPG algorithm. Finally, using the proposed algorithm, we evaluate the cross-entropy

loss of the metasurface assisted RF sensing scenario with respect to different numbers of sizes

of the metasurface, and numbers of space grids. Besides, we also compare the proposed method

with the benchmark, i.e., the MIMO RF sensing systems.

A. Simulation Settings

The layout of the considered scenario is provided in Fig. 6. The metasurface adopted in this

paper is the same as the one used in [27], and the reﬂection coefﬁcients of the reconﬁgurable

!"#!"!"#$%&’"#$(&")#$"!"&"&’"#*)#+,$%&’"#$%&’"#!"&"&")#!"#-!"#!"#!"#!"#$%("#!%&’()*+,%-(./.()%+/&0%-(27

TABLE I

SIMULATION PARAMETERS.

element in different conﬁgurations are simulated in CST software, Microwave Studio, Transient

Simulation Package [43], by assuming 60◦ incident RF signals with vertical polarization. Besides,

to increase the reﬂected signal power in the simulation, we combine G reconﬁgurable elements

as an independently controllable group. The reconﬁgurable elements of an independently con-

trollable group are in the same conﬁguration, and thus they can be considered as a single

one. Therefore, the proposed algorithm is suitable for this case. The number of independently

controllable group is denoted by NG.

The origin of the coordinate is at the center of the metasurface, and the metasurface is in the

y-z plane. In addition, the z-axis is vertical to the ground and pointing upwards, and the x- and

y-axes are parallel to the ground. The Tx and Rx antennas are located at (0.87,

0.84, 0) m and

−

(0, 0,

0.5) m, respectively. The target space a cuboid region located at 1 m from the metasurface,

and is divided into M space blocks each with size 0.1

0.1

×

×

0.1 m3. The simulation parameters

−

are summarized in Table I.

B. Results

In Fig. 7, we compare the training results for different algorithms. Speciﬁcally, the ﬁrst

algorithm in the legend is the proposed PRPG algorithm where a sensing network (SensNet) and

a policy network (PolicyNet) are adopted. The second algorithm adopts a sensing network but

adopt a random control matrix. The third algorithm adopts both a sensing network and a policy

Parameter Value Tx antenna gain () 15.0 dBi Rx antenna gain () 6.0 dBi Tx power () 100 mW Number of reconfigurable elements per group () 144 Signal frequency () 3.198 GHz Number of available states () 4 Number of frames () 10 Probability of space grid being nonempty () 0.5 Size of reflection vector set () 100 Number of space grids () 18 Size of random sampled subset () 1000 Power of noise () 10-9 dBm Size of space of interest (!!,!",!#	) (0.1, 0.1, 0.1) m Initial learning rate () 0.001 Variance of reflection coefficient () 1  (cid:22)(cid:20)(cid:23)(cid:20)(cid:24)(cid:20)(cid:25)(cid:20)(cid:3)(cid:14)(cid:4)(cid:19)(cid:6)(cid:15)(cid:9)(cid:10)(cid:18)(cid:11)(cid:5)(cid:17)(cid:12)(cid:8)(cid:2)(cid:16)(cid:7)(cid:13)(cid:1)(cid:26)(cid:20)(cid:27)(cid:20)(cid:28)(cid:20)Parameter Parameter Parameter gTgRPGfcNSKpm,1|V|MQsam−mǫα0ǫref28

Fig. 7. Cross-entropy loss versus the number of training epochs for different algorithms.

Fig. 8.

Illustrations of ground-truth and the sensing results of different training epochs for a target object.

network, but the sensing network does not contain a model-aided decoder as in the proposed

algorithm. The fourth algorithm only uses the model-aided decoder to map the received signals

to the sensing results.

It can be observed that the proposed PRPG algorithm converges with high speed and it

results in the lowest cross-entropy loss among all the considered algorithms. In particular, Fig. 8

shows a ground-truth object and the corresponding sensing results versus the number of training

epochs. As the number of training epochs increases, the sensing result approaches the ground

truth and becomes approximately the same after 104 training epochs.

In Fig. 9, it shows the ground-truths and the sensing results for different algorithms and the

target objects with different shapes. Comparing the sensing results with the ground truths, we can

observe that the proposed algorithm outperforms other benchmark algorithms to a large extent.

Besides, by comparing the sensing results of the proposed algorithm in the second column with

!"#$$%&’("#)*+,#$$!"#$!$#$%#"$#$$%$$$&$$$’$$$($$$!$$$$-./0&"+#1+2"34’4’5+6)#78$!"#$#%&’(!)!*(+,&-%.&/(0(!#1234.&/5(,&-%.&/(0("6-’(!,&-%.&/(78#(9#’&1:62’&’(’&3#’&"(0(!#1234.&/(;-14(9#’&1:62’&’(’&3#’&""#$)#"!%#"%$#$!)#"!"!#$%&’&’()*+,-./!""#$%&’&’()*+,-./!"##$%&’&’()*+,-./!"$#$%&’&’()*+,-./!"#$%&’("$)*!"#!"#!"#!"%#$%&’&’()*+,-./!"#######!"#!"#!"#!"#29

Fig. 9.

Illustrations of the ground-truths and the sensing results of objects with different shapes for different algorithms.

Fig. 10. Cross-entropy loss of the mapping of the received signals in high, normal, and low learning rate cases.

the ground truths in the ﬁrst column, we can observe that the proposed algorithm obtains the

accurate sensing results despite the different shapes of the target objects.”

In Fig. 10, the training results versus the number of training epochs for the PRPG algorithm

are given and compared for different learning rates are compared. The initial learning rates in

!"#$#%&’(!)!*+&,%-&./(!#0123-&.+&,%-&./("4,’(2#,."#0(54."16+&,%-&.78#(#$.(01,&4"(’&2#’&"(/(!#0123-&.9,03(#$.(01,&4"(’&2#’&"!"#$%&’("$)*!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#!"#$$(cid:16)(cid:72)’("#)*%+#$$%!"!!""!"#!"#"""$"""%"""&"""!"""",-./0"%#1%2"34’4’5%&)#67$!!"#$"#!!"#$"$!!"#$"%30

Fig. 11. Cross-entropy loss versus the number of training epochs for different sizes of the metasurface.

each case are set to be α0 = 10−1, 10−3, 10−5, which then decrease inversely as the number of

training epochs increases. It can be observed that large values of the learning rates prevent the

algorithm to converge, while low values of the learning rates result in a slow decrease of the

cross entropy loss. The setup α0 = 10−3 outperforms the others, which veriﬁes our learning rate

selection.

In Fig. 11, it can be observed that as the size of the metasurface, i.e., NG, increases, the

result cross-entropy loss after training decreases. This is because the received energy can be

improved with more reconﬁgurable elements to reﬂect transmitted signals, as indicated by (4).

Besides, more reconﬁgurable elements create a larger design freedom and higher controllability

of the beamforming, which makes gains of these reﬂection paths via different space grids more

distinguishable. Therefore, objects at different space grids can be sensed with a higher precision.

However, the cross-entropy cannot be reduced inﬁnitely. When NG is sufﬁciently large, the cross-

entropy will remains stable. As shown in Fig. 11, the cross-entropy loss results for NG = 9 and

NG = 16 are almost the same. Besides, comparing the curves for NG = 9 and NG = 16 within

the ﬁrst 2000 training epochs, we can observe that increasing the number of reconﬁgurable

elements when NG

9 has a negative impact on the training speed and convergence rate. This

≥

is because increasing the number of reconﬁgurable elements leads to a higher complexity of

ﬁnding the optimal policy for the metasurface to determine its control matrix, since the policy

network of the metasurface needs to handle a higher-dimensional state space.

0200040006000800010000Number of Training Epochs024681012Cross-entropy Loss31

Fig. 12. Estimated upper-bound and the results of the proposed algorithm for the cross-entropy loss versus different numbers

of space grids in 2D and 3D scenarios. The drawings at the bottom indicate the arrangement of the space grids

In Fig. 12, we compare the theoretical upper-bound derived in (46) and the proposed PRPG

algorithm for different values of M in 2D and 3D scenarios. It can be observed that, in both

2D and 3D scenarios, the probability of sensing error increases with M . Also, the cross-entropy

loss in 3D scenarios is higher than those for 2D scenarios. This is because the space grids in the

3D scenarios are more closely spaced to each other, which make them hard to be distinguished.

Finally, it can be observed that, as M increases, the cross-entropy loss of the proposed algorithm

increases more quickly in 3D scenarios compared to that in 2D scenarios. This which veriﬁes

that 3D sensing is more difﬁcult than 2D sensing.

In Fig. 13, we show the comparison between the proposed metasurface assisted scenario

and the benchmark, which is the MIMO RF sensing scenarios with no metasurface. Both the

metasurface assisted scenario and the MIMO scenarios adopted a similar layout described in

Section VI-A, and the result cross-entropy loss is obtained by Algorithm 1. Nevertheless, in the

MIMO sensing scenarios, a static reﬂection surface takes the place of the metasurface, which

cannot change the beamformer pattern for the reﬂection signals. When the size of the MIMO

array in Fig. 13 is n

n (n = 1, 2, . . . , 5), it indicates that n Tx antennas and n Rx antennas

are adopted in the scenario. Speciﬁcally, the Tx/Rx antennas are arranged along the y-axis with

a space interval of 0.1 m. The Tx antennas transmit continuous signals with phase interval

×

4916818320.10.20.30.40.50.6!"#$%&’(#)&*(*+"#$%&’(#)&*(*!"#$%&#’()*++’,-./01(2,/+/"’()345/,$#6%!"#$$%&’("#)*+,#$$!"#$$"#%%"#&’($)*$*&%-./0&"+#1+2)34&+5"67$8+!32

Fig. 13. Comparison between the metasurface assisted scenario and the MIMO scenarios with different numbers of Tx/Rx

antennas. The bars illustrate the results for different MIMO scenarios, and the dash lines depict the results of the metasurface

assisted scenario with different numbers of frames.

2π/n, and the n received signals of the Rx antennas with suppressed LoS signals are used as

the measurement vector. Comparing the MIMO benchmarks and the proposed method, we can

observe that the proposed method outperforms the (1

1)

(5

5) MIMO sensing scenarios

×

∼

×

in terms of the resulted cross-entropy loss. This shows the signiﬁcance of using metasurface to

assisted RF sensing.

VII. CONCLUSION

In this paper, we have considered a metasurface assisted RF sensing scenario, where the

existence and locations of objects within a 3D target space can be sensed. To facilitate the

beamformer pattern design, we have proposed a frame-based RF sensing protocol. Based on the

proposed protocol, we have formulated an optimization problem to design of the beamformer

pattern and the mapping of the received signals. To solve the optimization problem, we have for-

mulated an MDP framework and have proposed a deep reinforcement learning algorithm named

PRPG to solve it. Also, we have analyzed the computational complexity and the convergence

of the proposed algorithm and have computed a theoretical upper-bound for the cross-entropy

loss given the beamformer patterns of the metasurface. Simulation results have veriﬁed that

the proposed algorithm outperforms other benchmark algorithms in terms of training speed and

the resulted cross-entropy loss. Besides, they have also illustrated the inﬂuence of the sizes of

Cross-entropy Loss!"#$#!"#$"%"%!"#$"&"&!"#$"’"’!"#$"("(!"#$")")*+,-,./01/23,0the metasurface and the target space on the cross-entropy loss, which provides insights on the

implementation of practical metasurface assisted RF sensing systems.

33

APPENDIX A

PROOF OF THEOREM 1

The complexities of embedding K and N as one-hot vectors is

Based on [44], the complexity of multiplex the K beamformer patterns that are N

dimensional by A

CN NS ×M is

∈

(K

N

NS

·

·

O

·

M ). For a fully connected neural network

with a ﬁxed number of hidden layers and neurons, the computational complexity of the back-

(K) and

(N ), respectively.

O

O

NS-

·

propagation algorithm is proportional to the product of the input size and the output size [45].

Therefore, the computational complexities of the symmetric MLP group and the Q-value MLP are

(K

M ) and

(K

NS +N

NS), respectively. As the connecting operation has complexity

(1)

·

O
and ﬁnding the maximum Q-value is

O

·

·

(KN NSM ) and is therefore dominated by the matrix multiplication.

O

(NS), the total computational complexity has complexity
(cid:4)

O

O

APPENDIX B

PROOF OF LEMMA 1

We consider the worst case scenario for the computation, i.e., the former states in all

the samples are terminal states. In this case, the rewards are calculated from (25). The term

inside the second summation consists of two part, i.e., the cross-entropy calculation which has

computational complexity

(M ), and the calculation of ˆp by using the sensing network. The

computational complexity of calculating CA is

(KN NSM ).

O

Based on [46], calculating the pseudo-inverse matrix Γ +, where Γ is a K

M matrix,

has complexity

(K 2M ). Similar to the analysis of the computational complexity of MLPs

in the proof of Theorem 1, the computational complexity of the MLP is

(M 2). Therefore,

the computational complexity of calculating ˆp is

Lemma 1.

O

(KN NSM + K 2M + M 2), which proves
(cid:4)

×

O

O

O

APPENDIX C

PROOF OF LEMMA 2

For a fully connected neural network with a ﬁxed number of hidden layers and neurons, the

computational complexity of the back-propagation algorithm is proportional to the product of

the input size and the output size [45]. Therefore, the computational complexity of using the

34

back-propagation algorithm for updating the parameter vector of the sensing network is

(M 2).

O

The policy network can be considered as two connected MLPs: the ﬁrst one takes the one-

hot embedding vectors of k and n as the input, and the second one takes the K measuring

vectors with 2M dimensions as the input. Moreover, as a symmetric MLP group is considered,

the actual size of the input vector for the second MLP is 2M instead of 2KM . Therefore,

the computational complexity of training the ﬁrst and second MLP of the policy network are

(NS

(NS

O

O

·

·

(K + N )) and

(NSM ), respectively, and the total computational complexity is thus

(K + N + M )).

O

Furthermore, if a single large MLP with layer sizes (2KM, 64, 32K) is used to substitute

the symmetric MLP group, the computational complexities of training the second MLP is

(KNSM ), and the total computational complexity of training the policy network is

O
N NS). Therefore, Lemma 2 is proved.

O

(KM NS+
(cid:4)

APPENDIX D

PROOF OF THEOREM 2

Based on (26) and (27), the complexity of calculating the loss functions are determined by

the computation of the reward and action probabilities. From Theorem 1 and Lemma 2, it follows

that the complexity of calculating the reward is of higher order than that of calculating the action

probabilities. Therefore, the computational complexity of the training phase is dominated by the

calculation of the Nb rewards, which is

(KN NSM + K 2M + M 2).

O

(cid:4)

REFERENCES

[1] S. Kianoush, S. Savazzi, F. Vicentini, V. Rampa, and M. Giussani, “Device-free RF human body fall detection and

localization in industrial workplaces,” IEEE Internet of Things J., vol. 4, no. 2, pp. 351–362, Apr. 2017.

[2] P. W. Q. Lee, W. K. G. Seah, H. Tan, and Z. Yao, “Wireless sensing without sensors – An experimental approach,” in

Proc. IEEE Int. Symp. Pers. Indoor Mobile Radio Commun., Tokyo, Japan, Sep. 2009.

[3] T. He, S. Krishnamurthy, L. Luo, T. Yan, L. Gu, R. Stoleru, G. Zhou, Q. Cao, P. Vicaire, J. A. Stankovic, T. F. Abdeizaher,

J. Hui, and B. Krogh, “Vigilnet: An integrated sensor network system for energy-efﬁcient surveillance,” ACM Trans. Sensor

Netw., vol. 2, no. 1, pp. 1–38, Feb. 2006.

[4] K. Ota, M. Dong, J. Gui, and A. Liu, “Quoin: Incentive mechanisms for crowd sensing networks,” IEEE Netw., vol. 32,

no. 2, pp. 114–119, Mar. 2018.

[5] D. J. Cook and M. Schmitter-Edgecombe, “Assessing the quality of activities in a smart environment,” Methods Inform.

Medicine, vol. 48, no. 5, pp. 480–485, Oct. 2009.

[6] M. G. Amin, Y. D. Zhang, F. Ahmad, and K. D. Ho, “Radar signal processing for elderly fall detection: The future for

in-home monitoring,” IEEE Signal Process. Mag., vol. 33, no. 2, pp. 71–80, Mar. 2016.

35

[7] D. Zhang, J. Wang, J. Jang, J. Zhang, and S. Kumar, “On the feasibility of Wi-Fi based material sensing,” in Proc.

MobiCom, Los Cabos, Mexico, Oct. 2019.

[8] W. Jiang, C. Miao, F. Ma, S. Yao, Y. Wang, Y. Yuan, H. Xue, C. Song, X. Ma, D. Koutsonikolas, and et al., “Towards

environment independent device free human activity recognition,” in Proc. MobiCom, New Delhi, India, Oct. 2018.

[9] C.-Y. Hsu, R. Hristov, G.-H. Lee, M. Zhao, and D. Katabi, “Enabling identiﬁcation and behavioral sensing in homes using

radio reﬂections,” in Proc. CHI, Glasgow, U.K., May 2019.

[10] F. Adib, C.-Y. Hsu, H. Mao, D. Katabi, and F. Durand, “Capturing the human ﬁgure through a wall,” ACM Trans. Graphics,

vol. 34, no. 6, p. 219, Oct. 2015.

[11] M. Zhao, Y. Tian, H. Zhao, M. A. Alsheikh, T. Li, R. Hristov, Z. Kabelac, D. Katabi, and A. Torralba, “Rf-based 3d

skeletons,” in Proc. SIGCOMM, New York, NY, Aug. 2018.

[12] A. Pedross-Engel, D. Arnitz, J. N. Gollub, O. Yurduseven, K. P. Trofatter, M. F. Imani, T. Sleasman, M. Boyarsky, X. Fu,

D. L. Marks, D. R. Smith, and M. S. Reynolds, “Orthogonal coded active illumination for millimeter wave, massive-MIMO

computational imaging with metasurface antennas,” IEEE Trans. Comput. Imaging, vol. 4, no. 2, pp. 184–193, Jun. 2018.

[13] N. Honma, D. Sasakawa, N. Shiraki, T. Nakayama, and S. Iizuka, “Human monitoring using MIMO radar,” in Proc. iWEM,

Nagoya, Japan, Aug. 2018.

[14] Z. Li, Y. Xie, L. Shangguan, R. I. Zelaya, J. Gummeson, W. Hu, and K. Jamieson, “Programmable radio environments

with large arrays of inexpensive antennas,” GetMobile: Mobile Comput. Commun., vol. 23, no. 3, pp. 23–27, Sep. 2019.

[15] M. Di Renzo, M. Debbah, D.-T. Phan-Huy, A. Zappone, M.-S. Alouini, C. Yuen, V. Sciancalepore, G. C. Alexandropoulos,

J. Hoydis, H. Gacanin, J. D. Rosny, A. Bounceu, G. Lerosey, and M. Fink, “Smart radio environments empowered by ai

reconﬁgurable meta-surfaces: An idea whose time has come,” arXiv:1903.08925.

[16] H. Gacanin and M. D. Renzo, “Wireless 2.0: Towards an intelligent radio environment empowered by reconﬁgurable

meta-surfaces and artiﬁcial intelligence,” arXiv:2002.11040.

[17] E. Basar, M. D. Renzo, J. D. Rosny, M. Debbah, M. Alouini, and R. Zhang, “Wireless communications through

reconﬁgurable intelligent surfaces,” IEEE Access, vol. 7, pp. 116 753–116 773, Aug. 2019.

[18] S. Zhang, H. Zhang, B. Di, Y. Tan, Z. Han, and L. Song, “Reﬂective-transmissive metasurface aided communications for

full-dimensional coverage extension,” IEEE Trans. Veh. Technol., early access.

[19] M. A. ElMossallamy, H. Zhang, L. Song, K. G. Seddik, Z. Han, and G. Y. Li, “Reconﬁgurable intelligent surfaces for

wireless communications: Principles, challenges, and opportunities,” IEEE Trans. Cognitive Commun. Netw., vol. 6, no. 3,

pp. 990–1002, Sep. 2020.

[20] H. Hashida, Y. Kawamoto, and N. Kato, “Intelligent reﬂecting surface placement optimization in air-ground communication

networks toward 6G,” IEEE Wireless Commu., early access.

[21] L. Li, H. Ruan, C. Liu, Y. Li, Y. Shuang, A. Al`u, C.-W. Qiu, and T. J. Cui, “Machine-learning reprogrammable metasurface

imager,” Nature Commun., vol. 10, no. 1082, pp. 1–8, Jun. 2019.

[22] H. Zhang, H. Zhang, B. Di, K. Bian, Z. Han, and L. Song, “Towards ubiquitous positioning by leveraging reconﬁgurable

intelligent surface,” IEEE Commun. Lett., early access.

[23] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press, 2018.

[24] Y. Liu, X. Liu, T. Hou, J. Xu, Z. Qin, M. D. Renzo, and N. Al-Dhahir, “Reconﬁgurable intelligent surfaces: Principles

and opportunities,” arXiv:2007.03435.

[25] L. Dai, B. Wang, M. Wang, X. Yang, J. Tan, S. Bi, S. Xu, F. Yang, Z. Chen, M. D. Renzo, C. B. Chae, and L. Hanzo,

“Reconﬁgurable intelligent surface-based wireless communications: Antenna design, prototyping, and experimental results,”

IEEE Access, vol. 8, pp. 45 913–45 923, Mar. 2020.

[26] B. Di, H. Zhang, L. Li, L. Song, Z. Han, and H. V. Poor, “Hybrid beamforming for reconﬁgurable intelligent surface based

36

multi-user communications: Achievable rates with limited discrete phase shifts,” IEEE J. Sel. Areas Commun., vol. 38,

no. 8, pp. 1809–1822, Aug. 2020.

[27] J. Hu, H. Zhang, B. Di, L. Li, K. Bian, L. Song, Y. Li, Z. Han, and H. V. Poor, “Reconﬁgurable intelligent surface based

RF sensing: Design, optimization, and implementation,” IEEE J. Sel. Areas Commun., early access, arXiv:1912.09198.

[28] A. Goldsmith, Wireless communications. Cambridge University Press, 2005.

[29] W. Tang, M. Z. Chen, X. Chen, J. Y. Dai, Y. Han, M. Di Renzo, Y. Zeng, S. Jin, Q. Cheng, and T. J. Cui,

“Wireless communications with reconﬁgurable intelligent surface: Path loss modeling and experimental measurement,”

arXiv:1911.05326.

[30] H. Zhang, B. Di, L. Song, and Z. Han, “Reconﬁgurable intelligent surfaces assisted communications with limited phase

shifts: How many phase shifts are enough?” IEEE Trans. Veh. Technol., vol. 69, no. 4, pp. 4498–4502, Feb. 2020.

[31]

I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA: MIT press, 2016.

[32] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press: Cambridge, U.K., 2004.

[33] R. N. McDonough and A. D. Whalen, Detection of signals in noise. Academic Press: San Diego, CA, USA, 2004.

[34] J. C. Bezdek and R. J. Hathaway, “Convergence of alternating optimization,” Neural, Parallel & Sci. Comput., vol. 11,

no. 4, pp. 351–368, Dec. 2003.

[35] M. Volodymyr, K. Koray, S. David, A. A. Rusu, V. Joel, M. G. Bellemare, G. Alex, R. Martin, A. K. Fidjeland, and

O. Georg, “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, Feb.

2015.

[36] H. Li, K. Ota, and M. Dong, “Deep reinforcement scheduling for mobile crowdsensing in fog computing,” ACM Trans.

Internet Technol., vol. 19, no. 2, Apr. 2019.

[37] A. Zappone, M. Di Renzo, M. Debbah, T. T. Lam, and X. Qian, “Model-aided wireless artiﬁcial intelligence: Embedding

expert knowledge in deep neural networks for wireless system optimization,” IEEE Veh. Technol. Mag., vol. 14, no. 3, pp.

60–69, Jul. 2019.

[38] A. Zappone, M. Di Renzo, and M. Debbah, “Wireless networks design in the era of deep learning: Model-based, ai-based,

or both?” IEEE Trans. Commun., vol. 67, no. 10, pp. 7331–7376, Jun. 2019.

[39] R. Y. Rubinstein and D. P. Kroese, Simulation and the Monte Carlo Method.

John Wiley & Sons,, 2008.

[40] J. F. Bailyn, “Generalized inversion,” Natural Language & Linguistic Theory, vol. 22, no. 1, pp. 1–50, 2004.

[41] F. J. Pineda, “Generalization of back-propagation to recurrent neural networks,” Physical Rev. Lett., vol. 59, no. 19, p.

2229, Jun. 1987.

[42] Y. Xu and W. Yin, “Block stochastic gradient iteration for convex and nonconvex optimization,” SIAM J. Optimization,

vol. 25, no. 3, pp. 1686–1716, Jan. 2015.

[43] F. Hirtenfelder, “Effective antenna simulations using CST MICROWAVE STUDIO(cid:114),” in Proc. INICA, Munich, Germany,

Mar. 2007.

[44] S. S. Skiena, Sorting and searching. London, U.K.: Springer, 2012.

[45] M. Sipper, “A serial complexity measure of neural networks,” in Proc. IEEE ICNN, San Francisco, CA, Mar. 1993.

[46] M. D. Petkovi´c and P. S. Stanimirovi´c, “Generalized matrix inversion is not harder than matrix multiplication,” J. Comput.

Appl. Math., vol. 230, no. 1, pp. 270 – 282, Aug. 2009.

