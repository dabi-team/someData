Neural Network Training with Asymmetric Crosspoint Elements  
Authors: Murat Onen1,2,*, †, Tayfun Gokmen1,*, †, Teodor K. Todorov1, Tomasz Nowicki1, Jesús 
A. del Alamo2, John Rozen1, Wilfried Haensch1, Seyoung Kim1,*, ‡ 

Affiliations: 
1 IBM Thomas. J. Watson Research Center, Yorktown Heights, NY, 10598, USA. 
2 Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA. 

* Correspondence to: monen@mit.edu, tgokmen@us.ibm.com, kimseyoung@postech.ac.kr  

† Authors contributed equally to this work. 
‡ Current address: Department of Materials Science and Engineering, POSTECH, Pohang, 
Korea. 

Abstract: Analog crossbar arrays comprising programmable nonvolatile resistors are under 
intense investigation for acceleration of deep neural network training. However, the ubiquitous 
asymmetric conductance modulation of practical resistive devices critically degrades the 
classification performance of networks trained with conventional algorithms. Here we first 
describe the fundamental reasons behind this incompatibility. Then, we explain the theoretical 
underpinnings of a novel fully-parallel training algorithm that is compatible with asymmetric 
crosspoint elements. By establishing a powerful analogy with classical mechanics, we explain 
how device asymmetry can be exploited as a useful feature for analog deep learning processors. 
Instead of conventionally tuning weights in the direction of the error function gradient, network 
parameters can be programmed to successfully minimize the total energy (Hamiltonian) of the 
system that incorporates the effects of device asymmetry. Our technique enables immediate 
realization of analog deep learning accelerators based on readily available device technologies. 

Main Text:  

Deep learning has caused a paradigm shift in domains such as object recognition, natural 
language processing, and bioinformatics which benefit from classifying and clustering 
representations of data at multiple levels of abstraction (1). However, the computational 
workloads to train state-of-the-art deep neural networks (DNNs) demand enormous computation 
time and energy costs for data centers (2). Since larger neural networks trained with bigger data 
sets generally provide better performance, this trend is expected to accelerate in the future. As a 
result, the necessity to provide fast and energy-efficient solutions for deep learning has invoked a 
massive collective research effort by industry and academia (3–5).  

Highly optimized digital application-specific integrated circuit (ASIC) implementations have 
attempted to accelerate DNN workloads using reduced-precision arithmetic for the 
computationally intensive matrix operations. Although acceleration of inference tasks was 
achieved by using 2-bit resolution (6), learning tasks were found to require at least hybrid 8-bit 
floating-point formats (7) which still imposes considerable energy consumption and processing 

1 

 
 
 
 
time for large networks. Therefore, beyond-digital approaches that can efficiently handle training 
workloads are actively sought for. 

The concept of in-memory computation with analog resistive crossbar arrays is under intense 
study as a promising alternative. These frameworks were first designed to make use of Ohm’s 
and Kirchhoff’s Laws to perform parallel vector–matrix multiplications (See S2.1 an S2.2 for 
details), which constitute ≈ 2/3 of the overall computational load (8). However, unless the 
remaining ≈ 1/3 of computations during the update cycle is parallelized as well, the acceleration 
factors provided by analog arrays will be a mere 3 × at best with respect to conventional digital 
processors. It was much later discovered that rank-one outer products can also be achieved in 
parallel, using pulse-coincidence and incremental changes in device conductance (9, 10). Using 
this method, an entire crossbar array can be updated in parallel, without explicitly computing the 
outer product1 or having to read the value of any individual crosspoint element (11). As a result, 
all basic primitives for DNN training using the Stochastic Gradient Descent (SGD) algorithm can 
be performed in a fully-parallel fashion using analog crossbar architectures. However, this 
parallel update method imposes stringent device requirements since its performance is critically 
affected by the conductance modulation characteristics of the crosspoint elements. In particular, 
asymmetric conductance modulation characteristics (i.e. having mismatch between positive and 
negative conductance adjustments) are found to deteriorate classification accuracy by causing 
inaccurate gradient accumulation (9, 11–16). Unfortunately, all analog resistive devices to date 
have asymmetric characteristics, which poses a major technical barrier before the realization of 
analog deep learning processors. 

In addition to widespread efforts to engineer ideal resistive devices (17–20), many high-level 
mitigation techniques have been proposed to remedy device asymmetry. Despite numerous 
published simulated and experimental demonstrations, none of these studies so far provides a 
solution for which the analog processor still achieves its original purpose: energy-efficient 
acceleration of deep learning. The critical issue with the existing techniques is the requirement of 
serial accessing to crosspoint elements one-by-one or row-by-row (14–16, 21–26). Methods 
involving serial operations include reading conductance values individually, engineering update 
pulses to artificially force symmetric modulation, and carrying or resetting weights periodically. 
Furthermore, some approaches offload the gradient computation to digital processors, which not 
only requires consequent serial programming of the analog matrix, but also bears the cost of 
outer product calculation (14, 22–26). Updating an 𝑁 × 𝑁 crossbar array with these serial 
routines would require at least 𝑁 or even 𝑁2 operations. For practical array sizes, the update 
cycle would simply take too much computational time and energy. In conclusion, for 
implementations that compromise parallelism, whether or not the asymmetry issue is resolved 
becomes beside the point since computational throughput and energy efficiency benefits over 
conventional digital processors are lost for practical applications. It is therefore urgent to devise 
a method that deals with device asymmetry while employing only fully-parallel operations. 

Recently, our group proposed a novel fully-parallel training method, Tiki-Taka, that can 
successfully train DNNs based on asymmetric resistive devices with asymmetric modulation 

1 The result of the outer product is not returned to the user, but implicitly applied to the network. 

2 

 
 
                                                           
characteristics (27). This algorithm was empirically shown in simulation to deliver ideal-device-
equivalent classification accuracy for a variety of network types and sizes emulated with 
asymmetric device models (27). However, the missing theoretical underpinnings of the proposed 
algorithmic solution as well as the cost of doubling analog hardware previously limited the 
method described in Ref. (27). 

In this paper, we first theoretically explain why device asymmetry has been a fundamental 
problem for SGD-based training. By establishing a powerful analogy with classical mechanics., 
we further establish that the Tiki-Taka algorithm minimizes the total energy (Hamiltonian) of the 
system, incorporating the effects of device asymmetry. The present work formalizes this new 
method as Stochastic Hamiltonian Descent (SHD) and describes how device asymmetry can be 
exploited as a useful feature in a fully-parallel training. The advanced physical intuition allows 
us to enhance the original algorithm and achieve a reduction in hardware cost of 50%, improving 
its practical relevance. Using simulated training results for different device families, we conclude 
that SHD provides better classification accuracy and faster convergence with respect to SGD-
based training in all applicable scenarios. The contents of this paper provide a guideline for the 
next generation of crosspoint elements as well as specialized algorithms for analog computing. 

Theory 

Neural networks can be construed as many layers of matrices (i.e. weights, 𝑊) performing affine 
transformations followed by nonlinear activation functions. Training (i.e. learning) process refers 
to the adjustment of 𝑊 such that the network response to a given input produces the target output 
for a labeled dataset. The discrepancy between the network and target outputs is represented with 
a scalar error function, 𝐸, which the training algorithm seeks to minimize. In the case of the 
conventional SGD algorithm (28), values of 𝑊 are incrementally modified by taking small steps 
(scaled by the learning rate, 𝜂) in the direction of the gradient of the error function sampled for 
each input. Computation of the gradients is performed by the backpropagation algorithm 
consisting of forward pass, backward pass, and update subroutines (29) (Fig. 1A). When the 
discrete nature of DNN training is analyzed in the continuum limit, the time evolution of 𝑊 can 
be written as a Langevin equation: 

𝑊̇ =   −𝜂 [

𝜕𝐸
𝜕𝑊

+ 𝜖(𝑡)] 

(1) 

where 𝜂 is the learning rate and 𝜖(𝑡) is a fluctuating term with zero-mean, accounting for the 
inherent stochasticity of the training procedure (30). As a result of this training process, 𝑊 
converges to the vicinity of an optimum 𝑊0, at which  𝜕𝐸
= 0 but 𝑊̇  is only on average 0 due to 
the presence of 𝜖(𝑡). For visualization, if the training dataset is a cluster of points in space, 𝑊0 is 
the center of that cluster, where each individual point still exerts a force (𝜖(𝑡)) that averages out 
to 0 over the whole dataset.  

𝜕𝑊

In the case of analog crossbar-based architectures, the linear matrix operations are performed on 
arrays of physical devices, whereas all nonlinear computations (e.g. activation and error 

3 

 
 
functions) are handled at peripheral circuitry. The strictly positive nature of device conductance 
requires representation of each weight by means of the differential conductance of a pair of 
crosspoint elements (i.e. 𝑊 ∝ 𝐺𝑚𝑎𝑖𝑛 − 𝐺𝑟𝑒𝑓). Consequently, vector-matrix multiplications for 
the forward and backward passes are computed by using both the main and the reference arrays 
(Fig.1A). On the other hand, the gradient accumulation and updates are only performed on the 
main array using bidirectional conductance changes while the values of the reference array are 
kept constant2. In this section, to illustrate the basic dynamics of DNN training with analog 
architectures, we study a single-parameter optimization problem (linear regression) which can be 
considered as the simplest "neural network". 

The weight updates in analog implementations are carried out through modulation of the 
conductance values of the crosspoint elements, which are often applied by means of pulses. 
These pulses cause incremental changes in device conductance (Δ𝐺+,−). In an ideal device, these 
modulation increments are of equal magnitude in both directions and independent of the device 
conductance, as shown in Fig. 1B. It should be noted that the series of modulations in the 
training process is inherently non-monotonic as different input samples in the training set create 
gradients with different magnitudes and signs in general. Furthermore, as stated above, even 
when an optimum conductance, 𝐺0, is reached (𝑊0 ∝ 𝐺0 − 𝐺𝑟𝑒𝑓), continuing the training 
operation would continue modifying the conductance in the vicinity of 𝐺0, as shown in Fig. 1C. 
Consequently, 𝐺0 can be considered as a dynamic equilibrium point of the device conductance 
from the training algorithm point of view. 

Despite considerable technological efforts in the last decade, analog resistive devices with the 
ideal characteristics illustrated in Fig. 1B have yet to be realized. Instead, practical analog 
resistive devices display asymmetric conductance modulation characteristics such that unitary 
(i.e. single-pulse) modulations in opposite directions do not cancel each other in general, i.e., 
Δ𝐺+(𝐺) ≠ −Δ𝐺−(𝐺). However, with the exception of some device technologies such as Phase 
Change Memory (PCM) which reset abruptly (15, 21, 31), many crosspoint elements can be 
modeled by a smooth, monotonic, nonlinear function that shows saturating behavior at its 
extrema as shown in Fig. 1E (19, 32, 33). For such devices, there exists a unique conductance 
point, 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦, at which the magnitude of an incremental conductance change is equal to that 
of a decremental one. As a result, the time evolution of 𝐺 during training can be rewritten as: 

𝐺̇ = −𝜂 [

𝜕𝐸
𝜕𝐺

+ 𝜖(𝑡)] − 𝜂𝜅 |

𝜕𝐸
𝜕𝐺

+ 𝜖(𝑡)| . 𝑓ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒  

(2) 

where 𝜅 is the asymmetry factor and 𝑓ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 is the functional form of the device asymmetry 
+ 𝜖(𝑡)| signifies that the direction 
(See S1.1 for derivation). In this expression, the term −𝜂 |
of the change related to asymmetric behavior is solely determined by 𝑓ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒, irrespective of 
the direction of the intended modulation. For the exponentially saturating device model shown in 
Fig.1E, 𝑓ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒  = 𝐺 − 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦, which indicates that each and every update event has a 

𝜕𝐺

𝜕𝐸

2 For implementations using devices showing unidirectional conductance modulation characteristics, both the main and the reference 
array are updated. When SGD is used as the training algorithm, values of 𝐺𝑟𝑒𝑓 are not critical as long as they fall in the midrange of 
𝐺𝑚𝑎𝑖𝑛’s conductance span (9). 

4 

 
 
                                                           
component that drifts the device conductance towards its symmetry point. A simple observation 
of this effect is when enough equal number of incremental and decremental changes are applied 
to these devices in a random order, the conductance value converges to the vicinity of 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 
(33). Therefore, this point can be viewed as the physical equilibrium point for the device, as it is 
the only conductance value that is dynamically stable.  

It is essential to realize that there is in general no relation between 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 and 𝐺0, as the 
former is entirely device-dependent while the latter is problem-dependent. As a result, for an 
asymmetric device, two equilibria of hardware and software create a competing system, such that 
the conductance value converges to a particular conductance somewhere between 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 and 
𝐺0, for which the driving forces of the training algorithm and device asymmetry are balanced 
out. (Fig.1F). In examples shown in Fig.1C and 1F, 𝐺0 of the problem is purposefully designed 
to be far away from 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦, so as to depict a case for which the effect of asymmetry is 
pronounced. Indeed, it can be seen that the discrepancy between the final converged value, 
𝐺𝑓𝑖𝑛𝑎𝑙, and 𝐺0 strongly depends on the relative position of 𝐺0 with respect to the 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 
(Fig.1G), unlike that of ideal devices (Fig.1D). Detailed derivation of these dynamics can be 
found in S1.2. 

Fig.1 Effect of asymmetric conductance modulation for SGD-based training. (A) Schematic and 
pseudocode of processes for conventional SGD algorithm (28). Vectors 𝑥, 𝑦, represent the input and 
output vectors in the forward pass whereas 𝛿, 𝑧 contain the backpropagated error information. The analog 
architecture schematic is only shown for a single layer, where all vectors are propagated between upper 
and lower network layers in general. The pseudocode only describes operations computed in the analog 
domain, whereas digital computations such as activation functions are not shown for simplicity. (B) 
Sketch of conductance modulation behavior of a symmetric crosspoint device. (C) Simulated single-

5 

 
 
parameter optimization result for the symmetric device shown in (B). conductance successfully converges 
to the optimal value for the problem at hand, 𝐺0. (D) Simulated residual distance between the final 
converged value, 𝐺𝑓𝑖𝑛𝑎𝑙, and 𝐺0 for training the device with characteristics shown in (B) for datasets with 
different optimal values. (E) Sketch of conductance modulation behavior of an asymmetric crosspoint 
device. The point at which Δ𝐺+ = Δ𝐺− is defined as the symmetry point of the device (𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦) (F) 
Simulated training result for the same single-parameter optimization with the asymmetric device shown in 
(E). Device conductance fails to converge to 𝐺0, but instead settles at a level between 𝐺0 and 𝐺𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦.  
(G) Simulated residual distance (in semilog scale) between the final value, 𝐺𝑓𝑖𝑛𝑎𝑙, and 𝐺0 for training the 
device with characteristics shown in (E) for datasets with different optimal values. All simulation details 
can be found in S2.5. 

In contrast to SGD, our new training algorithm, illustrated in Fig.2A, separates both the forward 
path and error backpropagation from the update function. For this purpose, two array pairs 
(instead of a single pair), namely 𝐴𝑚𝑎𝑖𝑛, 𝐴𝑟𝑒𝑓, 𝐶𝑚𝑎𝑖𝑛, 𝐶𝑟𝑒𝑓 are utilized to represent each layer 
(27). In this representation, 𝐴 = 𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑟𝑒𝑓 stands for the auxiliary array and 𝐶 = 𝐶𝑚𝑎𝑖𝑛 −
𝐶𝑟𝑒𝑓 stands for the core array.  

The new training algorithm operates as follows. At the beginning of the training process, 𝐴𝑟𝑒𝑓 
and 𝐶𝑟𝑒𝑓 are initialized to 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 and 𝐶𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦, respectively (reasons will be 
clarified later, detailed method can be found in S2.3), following the method described in 
Ref.(33). As illustrated in Fig.2A, first, forward and backward pass cycles are performed on the 
array-pair 𝐶 (Steps 𝐼 and 𝐼𝐼), and corresponding updates are performed on 𝐴𝑚𝑎𝑖𝑛 (scaled by the 
learning rate 𝜂𝐴) using the parallel update scheme discussed in Ref. (9) (Step 𝐼𝐼𝐼). In other 
words, the updates that would have been applied to 𝐶 in a conventional SGD scheme are directed 
to 𝐴 instead. 

Then, every 𝜏 cycles, another forward pass is performed on 𝐴, with a vector 𝑢, which produces 
𝑣 = 𝐴𝑢 (Step 𝐼𝑉). In its simplest form, 𝑢 can be a vector of all "0"s but one "1", which then 
makes 𝑣 equal to the row of 𝐴 corresponding to the location of "1" in 𝑢. Finally, the vectors 
𝑢 and 𝑣 are used to update 𝐶𝑚𝑎𝑖𝑛 with the same parallel update scheme (scaled by the learning 
rate 𝜂𝑐) (Step 𝑉). These steps (𝐼𝑉 and 𝑉 shown in Fig 2.A) essentially partially add the 
information stored in 𝐴 to 𝐶𝑚𝑎𝑖𝑛. The complete pseudocode for the algorithm can be found in 
S2.4. 

At the end of the training procedure 𝐶 alone contains the optimized network, to be later used in 
inference operations (hence the name core). Since A receives updates computed over 𝜕𝐸
, which 
𝜕𝐶
have zero-mean once 𝐶 is optimized, its active component, 𝐴𝑚𝑎𝑖𝑛, will be driven towards 
𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦. The choice to initialize the stationary reference array, 𝐴𝑟𝑒𝑓, at 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 
ensures that 𝐴 = 0 at this point (i.e. when 𝐶 is optimized), thus generating no updates to 𝐶 in 
return. 

With the choice of 𝑢 vectors made above, every time steps 𝐼𝑉 and 𝑉 are performed, the location 
of the "1" for the 𝑢 vector would change in a cyclic fashion, whereas in general any set of 
orthogonal 𝑢 vectors can be used for this purpose (27). We emphasize that these steps should not 

6 

 
 
be confused with weight carrying (15, 16), as 𝐶 is updated by only a fractional amount in the 
direction of 𝐴 as 𝜂𝐶<< 1 and at no point information stored in 𝐴 is externally erased (i.e. 𝐴 is 
never reset). Instead, 𝐴 and 𝐶 create a coupled-dynamical-system, as the changes performed on 
both are determined by the values of one another.  

Furthermore, it is critical to realize that the algorithm shown in Fig. 2 consists of only fully-
parallel operations. Similar to steps 𝐼 and 𝐼𝐼 (forward and backward pass on 𝐶), steps 𝐼𝑉 is yet 
another matrix-vector multiplication that is performed by means of Ohm’s and Kirchhoff’s 
Laws. On the other hand, the update steps 𝐼𝐼𝐼 and 𝑉 are performed by the stochastic update 
scheme (9). This update method does not explicitly compute the outer products (𝑥 × 𝛿 and 
𝑢 × 𝑣), but instead uses a statistical method to modify all weights in parallel proportional to 
those outer products. As a result, no serial operations are required at any point throughout the 
training operation, enabling high throughput and energy efficiency benefits in deep learning 
computations.  

Fig.2 DNN training with Stochastic Hamiltonian Descent (SHD) algorithm and dynamics of a 

7 

 
 
dissipative harmonic oscillator. (A) Schematic and pseudocode of training process using the SHD 
algorithm. The pseudocode only describes operations computed in the analog domain, whereas digital 
computations such as nonlinear error functions are not shown for simplicity. (B) Illustration of a damped 
harmonic oscillator system. (C) Differential equations describing the evolution of the parameters with the 
SHD training algorithm in the continuum limit. (D) Equations of motion describing the dynamics of a 
harmonic oscillator. (E) Simulated results for a single-parameter optimization task using the SHD 
algorithm with symmetric devices described in Fig.1B. (F) Simulated results for a single-parameter 
optimization task using the SHD algorithm with asymmetric devices described in Fig.1E. All simulation 
details can be found in S2.5. 

For the same linear regression problem studied above, the discrete-time update rules given in 
Fig.2A can be rewritten as a pair of differential equations in the continuum limit that describe the 
time evolution of subsystems 𝐴 and 𝐶 (Fig.2C) as: 

𝐴̇ = −𝜂𝐴 [

𝜕𝐸
𝜕𝐶

  + 𝜖(𝑡) ] − 𝜂𝐴𝜅𝐴 |

𝜕𝐸
𝜕𝐶

+ 𝜖(𝑡)| (𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑚𝑎𝑖𝑛, 𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦)  

𝐶̇ = 𝜂𝐶𝐴  + 𝜂𝐶𝜅𝐶|𝐴|(𝐶𝑚𝑎𝑖𝑛 − 𝐶𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦)

(3) 

(4)  

2

1

𝑘𝑠𝑝𝑟𝑖𝑛𝑔(𝑥 − 𝑥0)2.  Moreover, for implementations with 

It can be noticed that this description of the coupled system has the same arrangement as the 
equations governing the motion of a damped harmonic oscillator (Fig.2B, D). In this analogy, 
subsystem 𝐴 corresponds to velocity, 𝜈, while subsystem 𝐶 maps to position, 𝑥, allowing the 
scalar error function of the optimization problem3, (𝐶 − 𝐶0)2, to map onto the scalar potential 
energy of the physical framework, 
asymmetric devices, an additional force term, 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒, needs to be included in the differential 
equations to reflect the hardware-induced effects on the conductance modulation. As discussed 
earlier, for the device model shown in Fig.1E this term is proportional to 𝐴𝑚𝑎𝑖𝑛 −
𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦. If we assume 𝐴𝑟𝑒𝑓 = 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 (this assumption will be explained later), 
we can rewrite 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 as a function of 𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑟𝑒𝑓, which then resembles a drag force, 
𝐹𝑑𝑟𝑎𝑔, that is linearly proportional to velocity (𝜈 ∝ 𝐴 = 𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑟𝑒𝑓) with a variable (but 
strictly nonnegative) drag coefficient 𝑘𝑑𝑟𝑎𝑔. In general, the 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 term can have various 
functional forms for devices with different conductance modulation characteristics but is 
completely absent for ideal devices. Note that, only to simplify the physical analogy, we ignore 
the effect of asymmetry in subsystem 𝐶, which yields the equation shown in Fig.2C (instead of 
Eq. 4). This decision will be justified in the Discussions section and is derived in detail in S1.3. 

Analogous to the motion of a lossless harmonic oscillator, the steady-state solution for this 
modified optimization problem with ideal devices (i.e. 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 = 0) has an oscillatory 
behavior (Fig 2E). This result is expected, as in the absence of any dissipation mechanism, the 
total energy of the system cannot be minimized (it is constant) but can only be continuously 
transformed between its potential and kinetic components. On the other hand, for asymmetric 

3 Conventionally error functions are written in terms of the difference between the network response and the target output and 
gradients are computed accordingly. However, in the absence of any stochasticity, 𝜖, it can instead be written in terms of the network 
weights and their optimal values as well for notational purposes. 

8 

 
 
                                                           
devices, the dissipative force term 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 gradually annihilates all energy in the system, 
allowing 𝐴 ∝ 𝜈 to converge to 0 (𝐸𝑘𝑖𝑛𝑒𝑡𝑖𝑐 → 0) while 𝐶 ∝ 𝑥 converges to 𝐶0 ∝ 𝑥0 (𝐸𝑝𝑜𝑡𝑒𝑛𝑡𝑖𝑎𝑙  →
0). Based on these observations, we rename the new training algorithm as Stochastic 
Hamiltonian Descent (SHD) to highlight the evolution of the system parameters in the direction 
of reducing the system’s total energy (Hamiltonian). These dynamics can be visualized by 
plotting the time evolution of 𝐴 versus that of 𝐶, which yields a spiraling path representing 
decaying oscillations for the optimization process with asymmetric devices (Fig 2F), in contrast 
to elliptical trajectories observed for ideal lossless systems (Fig 2E).  

𝜕𝐶

𝜕𝐸

+ 𝜖(𝑡)| (𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦) and the 

Following the establishment of the necessity to have dissipative characteristics, here we analyze 
conditions at which device asymmetry provides this behavior. It is well-understood in mechanics 
that for a force to be considered dissipative, its product with velocity (i.e. power) should be 
negative (otherwise it would imply energy injection into the system). In other words, the 
hardware-induced force term 𝐹ℎ𝑎𝑟𝑑𝑤𝑎𝑟𝑒 = −𝜅𝐴𝜂𝐴 |
velocity, 𝜈 = 𝐴𝑚𝑎𝑖𝑛 − 𝐴𝑟𝑒𝑓, should always have opposite signs. Furthermore, from the steady-
state analysis, for the system to be stationary (𝜈 = 0) at the point with minimum potential energy 
(𝑥 = 𝑥0), there should be no net force (𝐹 = 0). Both of these arguments indicate that, for the 
SHD algorithm to function properly, 𝐴𝑟𝑒𝑓 must be set to 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦. Note that as long as the 
crosspoint elements are realized with asymmetric devices (opposite to SGD requirement) and a 
symmetry point exists for each device, the shape of their modulation characteristics is not critical 
for successful DNN training with the SHD algorithm. Importantly, while a technologically viable 
solution for symmetric devices has not yet been found over decades of investigation, asymmetric 
devices that satisfy the aforementioned properties are abundant. To validate this claim, we 
present an experimental demonstration these dynamics using metal-oxide based electrochemical 
devices (32) in S3.1 (Fig.S3). 

A critical aspect to note is that the SGD and the SHD algorithms are fundamentally disjunct 
methods governed by completely different dynamics. The SGD algorithm attempts to optimize 
the system parameters while disregarding the effect of device asymmetry and thus converges to 
the minimum of a wrong energy function. On the other, the system variables in an SHD-based 
training do not conventionally evolve in directions of the error function gradient, but instead, are 
tuned to minimize the total energy incorporating the hardware-induced terms. The most obvious 
manifestation of these properties can be observed when the training is initialized from the 
optimal point (i.e. the very lucky guess scenario) since any “training” algorithm should at least 
be able to maintain this optimal state. For the conventional SGD, when 𝑊 = 𝑊0, the zero-mean 
updates applied to the network were shown above to drift 𝑊 away from 𝑊0 towards 𝑊𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦. 
On the other hand, for the SHD method, when 𝐴 = 0 and 𝐶 = 𝐶0, the zero-mean updates applied 
on 𝐴 do not have any adverse effect since 𝐴𝑚𝑎𝑖𝑛 is already at 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦 for 𝐴 = 0. 
Consequently, no updates are applied to 𝐶 either as 𝐶̇ = 𝐴 = 0. Therefore, it is clear that SGD is 
fundamentally incompatible with asymmetric devices, even when the solution is guessed 
correctly from the beginning, whereas the SHD does not suffer from this problem (See S1.2 and 
S1.3). Note that the propositions made for SGD can be further generalized to other crossbar-
compatible training methods such as equilibrium propagation (34) and deep Boltzmann machines 

9 

 
 
(35), which can also be adapted to be used with asymmetric devices following the approach 
discussed in this paper. Additional discussions on SHD operation can be found in S3.3 and S3.4.  

Finally, we appreciate that large-scale neural networks are much more complicated systems with 
respect to the problem analyzed here. Similarly, different analog devices show a wide range of 
conductance modulation behaviors, as well as bearing other non-idealities such as analog noise, 
imperfect retention, and limited endurance. However, the theory we provide here finally provides 
an intuitive explanation for: (1) why device asymmetry is fundamentally incompatible with 
SGD-based training and (2) how to ensure accurate optimization while only using fully-parallel 
operations. We conclude that asymmetry-related issues within SGD should be analyzed in the 
context of competing equilibria, where the optimum for the classification problem is not even a 
stable solution at steady-state. In addition to this simple stability analysis, the insight to modify 
the optimization landscape to include nonideal hardware effects allows other fully-parallel 
solutions to be designed in the future using advanced concepts from optimal control theory. As a 
result, these parallel methods enable analog processors to provide high computational throughput 
and energy efficiency benefits over their conventional digital counterparts. 

Discussion 

In this section, we first discuss how to implement the SHD algorithm with 3 arrays (instead of 4) 
using the intuition obtained from the theoretical analysis of the coupled-system. Then we provide 
simulated results for a large-scale neural network for different asymmetry characteristics to 
benchmark our method against SGD-based training. 

Considering a sequence of 𝑚 + 𝑛 incremental and 𝑛 decremental changes at random order, the 
net modulation obtained for a symmetric device is on average 𝑚. On the other hand, we have 
shown above that for asymmetric devices the conductance value eventually converges to the 
symmetry point for increasing 𝑛 (irrespective of 𝑚 or the initial conductance). It can be seen by 
inspection that for increasing statistical variation present in the training data (causing more 
directional changes for updates), the effect of device asymmetry gets further pronounced, leading 
to heavier degradation of classification accuracy for networks trained with conventional SGD 
(See S3.6 and Fig. S1). However, this behavior can alternatively be viewed as nonlinear filtering, 
where only signals with persistent sign information,  𝑚
,  are passed. Indeed, the SHD 
algorithm exploits this property within the auxiliary array, 𝐴, which filters the gradient 
information that is used to train the core array, 𝐶. As a result, 𝐶 is updated with less frequency 
and only in directions with a high confidence level of minimizing the error function of the 
problem at hand. A direct implication of this statement is that the asymmetric modulation 
behavior of 𝐶 is much less critical than that of 𝐴 (See S3.7 and Fig. S2) for successful 
optimization as its update signal contains less amount of statistical variation. Therefore, 
symmetry point information of  𝐶𝑚𝑎𝑖𝑛 is not relevant either. Using these results and intuition, we 
modified the original algorithm by discarding 𝐶𝑟𝑒𝑓 and using 𝐴𝑟𝑒𝑓 (set to 𝐴𝑚𝑎𝑖𝑛,𝑠𝑦𝑚𝑚𝑒𝑡𝑟𝑦) as a 
common reference array for differential readout. This modification reduces the hardware cost of 
SHD implementations by 50% to significantly improve their practicality. 

𝑚+2𝑛

10 

 
 
Our description of asymmetry as the mechanism of dissipation indicates that it is a necessary and 
useful device property for convergence within the SHD framework (Fig.2E). However, this 
argument does not imply that the convergence speed would be determined by the magnitude of 
device asymmetry for practical-sized applications. Unlike the single-parameter regression 
problem considered above, the exploration space for DNN training is immensely large, causing 
optimization to take place over many iterations of the dataset. In return, the level of asymmetry 
required to balance (i.e. damp) the system evolution is very small and can be readily achieved by 
any practical level of asymmetry. 

To prove these assertations, we show simulated results in Fig. 4 for a Long Short-Term Memory 
(LSTM) network, using device models with increasing levels of asymmetry, trained with both 
the SGD and SHD algorithms. The network was trained on Leo Tolstoy’s War and Peace novel, 
to predict the next character for a given text string (37). For reference, training the same network 
with a 32-bit digital floating-point architecture yields a cross-entropy level of 1.33 (complete 
learning curve shown in Fig. S6). We have particularly chosen this network as LSTM’s are 
known for being particularly vulnerable to device asymmetry (12).  

Fig 3. Simulated training results for different resistive device technologies. (A)  Simulated learning 
curves of a Long Short-Term Memory (LSTM) network trained on Leo Tolstoy’s War and Peace novel, 
using different crosspoint device models under the SGD algorithm. Details of the network can be found in 
Ref. (37) (B) Simulated learning curves for the same network using the SHD algorithm. All simulation 
details can be found in S2.7. See Fig. S5 for device-to-device variation included in the simulations and 
Fig. S6 for floating-point baseline comparison. 

The insets in Fig. 4 show the average conductance modulation characteristics representative for 
each asymmetry level. The simulations further included device-to-device variation, cycle-to-

11 

 
 
 
cycle variation, analog read noise, and stochastic updating similar to the work conducted in Ref. 
(9). The learning curves show the evolution of the cross-entropy error, which measures the 
performance of a classification model, with respect to the epochs of training. First, Fig 3A shows 
that even for minimally asymmetric devices (blue trace) trained with SGD, the penalty in 
classification performance is already severe. This result also demonstrates once more the 
difficulty of engineering a device that is symmetric-enough to be trained accurately with SGD. 
On the other hand, for SHD (Fig 3.B), all depicted devices are trained successfully, with the sole 
exception being the perfectly symmetric devices (black trace), as expected (See S.3.2 and Fig. S5 
for devices with abrupt modulation characteristics). Furthermore, Fig 3B demonstrates that SHD 
can even provide training results with higher accuracy and faster convergence than those for 
perfectly symmetric devices trained with SGD. As a result, we conclude that SHD is generically 
superior to SGD for analog deep learning architectures. 

Finally, although we present SHD in the context of analog computing specifically, it can also be 
potentially useful on conventional processors (with simulated asymmetry). The filtering 
dynamics described above allows SHD to guide its core component selectively in directions with 
high statistical persistence. Therefore, at the expense of increasing the overall memory and 
number of operations, SHD might outperform conventional training algorithms by providing 
faster convergence, better classification accuracy, and/or superior generalization performance.  

Conclusion 

In this paper, we described a fully-parallel neural network training algorithm for analog crossbar-
based architectures, Stochastic Hamiltonian Descent (SHD), based on resistive devices with 
asymmetric conductance modulation characteristics, as is the case for all practical technologies. 
In contrast to previous work that resorted to serial operations to mitigate asymmetry, SHD is a 
fully-parallel and scalable method that can enable high throughput and energy-efficiency deep 
learning computations with analog hardware. Our new method uses an auxiliary array to 
successfully tune the system variables in order to minimize the total energy (Hamiltonian) of the 
system that includes the effect of device asymmetry. Standard techniques, such as Stochastic 
Gradient Descent, perform optimization without accounting for the effect of device asymmetry 
and thus converge to the minimum of a wrong energy function. Therefore, our theoretical 
framework describes the inherent fundamental incompatibility of asymmetric devices with 
conventional training algorithms. The SHD framework further enables the exploitation of device 
asymmetry as a useful feature to selectively filter and apply the updates only in directions with 
high confidence. The new insights shown here have allowed a 50% reduction in the hardware 
cost of the algorithm. This method is immediately applicable to a variety of existing device 
technologies, and complex neural network architectures, enabling the realization of analog 
training accelerators to tackle the ever-growing computational demand of deep learning 
applications.  

References 

1.   Y. Lecun, Y. Bengio, G. Hinton, Deep learning. Nature. 521, 436–444 (2015). 

12 

 
 
2.  

3.  

E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations for deep learning 
in NLP. ACL 2019 - 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf., 3645–3650 
(2020). 

S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, ZeRO : Memory Optimizations Toward 
Training Trillion Parameter Models (2020). 

4.   N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, 

N. Boden, A. Borchers, R. Boyle, P. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. 
Dau, J. Dean, B. Gelb, T. Vazir, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, J. Hu, 
R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, H. Khaitan, D. Killebrew, A. Koch, N. 
Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. 
Mackean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, 
K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, A. Salek, E. 
Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. 
Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. 
Wilcox, D. H. Yoon, In - Datacenter Performance Analysis of a Tensor Processing Unit. 
Proc. 44th Annu. Int. Symp. Comput. Archit., 1–12 (2017). 

5.   Y. Chen, S. Member, T. Krishna, J. S. Emer, V. Sze, Eyeriss : An Energy-Efficient 

Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Solid-State 
Circuits. 52, 127–138 (2016). 

6.  

J. Choi, S. Venkataramani, V. Srinivasan, K. Gopalakrishnan, Z. Wang, P. Chuang, 
Accurate and Efficient 2-Bit Quantized Neural Networks. Proc. 2nd SysML Conf. (2019). 

7.   X. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. Srinivasan, X. Cui, W. 

Zhang, K. Gopalakrishnan, Hybrid 8-bit Floating Point ( HFP8 ) Training and Inference 
for Deep Neural Networks. Adv. Neural Inf. Process. Syst. (2019). 

8.   K. Steinbuch, Die Lernmatrix. Kybernetik. 1, 36–45 (1961). 

9.  

T. Gokmen, Y. Vlasov, Acceleration of Deep Neural Network Training with Resistive 
Cross-Point Devices : Design Considerations. Front. Neurosci. 10, 333 (2016). 

10.   G. W. Burr, R. M. Shelby, S. Sidler, C. Di Nolfo, J. Jang, I. Boybat, R. S. Shenoy, P. 
Narayanan, K. Virwani, E. U. Giacometti, B. N. Kurdi, H. Hwang, Experimental 
Demonstration and Tolerancing of a Large-Scale Neural Network (165 000 Synapses) 
Using Phase-Change Memory as the Synaptic Weight Element. IEEE Trans. Electron 
Devices. 62, 3498–3507 (2015). 

11.   T. Gokmen, M. Onen, W. Haensch, Training Deep Convolutional Neural Networks with 

Resistive Cross-Point Devices. Front. Neurosci. 11, 538 (2017). 

12.   T. Gokmen, M. J. Rasch, W. Haensch, Training LSTM Networks With Resistive Cross-

Point Devices. Front. Neurosci. 12, 745 (2018). 

13.   S. Agarwal, S. J. Plimpton, D. R. Hughart, A. H. Hsia, I. Richter, J. A. Cox, C. D. James, 

13 

 
 
M. J. Marinella, Resistive memory device requirements for a neural algorithm accelerator. 
Proc. Int. Jt. Conf. Neural Networks, 929–938 (2016). 

14.   S. Yu, P. Y. Chen, Y. Cao, L. Xia, Y. Wang, H. Wu, Scaling-up resistive synaptic arrays 
for neuro-inspired architecture: Challenges and prospect. Tech. Dig. - Int. Electron 
Devices Meet. IEDM (2015), doi:10.1109/IEDM.2015.7409718. 

15.   S. Ambrogio, P. Narayanan, H. Tsai, R. M. Shelby, I. Boybat, C. Nolfo, S. Sidler, B. 

Killeen, C. Cheng, Y. Jaoudi, M. Giordano, M. Bodini, N. C. P. Farinha, B. Killeen, C. 
Cheng, Y. Jaoudi, G. W. Burr, Equivalent-accuracy accelerated neural-network training 
using analogue memory. Nature. 558, 60–67 (2018). 

16.   S. Agarwal, R. B. J. Gedrim, A. H. Hsia, D. R. Hughart, E. J. Fuller, A. A. Talin, C. D. 

James, S. J. Plimpton, M. J. Marinella, Achieving Ideal Accuracies in Analog 
Neuromorphic Computing Using Periodic Carry. Symp. VLSI Technol., 174–175 (2017). 

17.   E. J. Fuller, S. T. Keene, A. Melianas, Z. Wang, S. Agarwal, Y. Li, Y. Tuchman, C. D. 
James, M. J. Marinella, J. J. Yang, A. Salleo, A. A. Talin, Parallel programming of an 
ionic floating-gate memory array for scalable neuromorphic computing. Science (80-. ). 
364, 570–574 (2019). 

18.  

J. Grollier, D. Querlioz, K. Y. Camsari, K. Everschor-Sitte, S. Fukami, M. D. Stiles, 
Neuromorphic spintronics. Nat. Electron. (2020), doi:10.1038/s41928-019-0360-9. 

19.   X. Yao, K. Klyukin, W. Lu, M. Onen, S. Ryu, D. Kim, N. Emond, I. Waluyo, A. Hunt, J. 

del Alamo, J. Li, B. Yildiz, Nat. Commun., in press. 

20.  

J. Woo, S. Yu, Resistive memory-based analog synapse: The pursuit for linear and 
symmetric weight update. IEEE Nanotechnol. Mag. 12, 36–44 (2018). 

21.   G. W. Burr, R. M. Shelby, A. Sebastian, S. Kim, S. Sidler, K. Virwani, M. Ishii, P. 

Narayanan, A. Fumarola, L. L. Sanches, I. Boybat, M. Le Gallo, K. Moon, J. Woo, H. 
Hwang, Y. Leblebici, G. W. Burr, R. M. Shelby, A. Sebastian, S. Kim, S. Sidler, K. 
Virwani, M. Ishii, P. Narayanan, A. Fumarola, L. L. Sanches, I. Boybat, M. Le Gallo, K. 
Moon, J. Woo, H. Hwang, Y. Leblebici, Neuromorphic computing using non-volatile 
memory. Adv. Phys. X. 2, 89–124 (2017). 

22.   C. Li, M. Hu, Y. Li, H. Jiang, N. Ge, E. Montgomery, J. Zhang, W. Song, N. Dávila, C. E. 

Graves, Z. Li, J. P. Strachan, P. Lin, Z. Wang, M. Barnell, Q. Wu, R. S. Williams, J. J. 
Yang, Q. Xia, Analogue signal and image processing with large memristor crossbars. Nat. 
Electron. 1, 52–59 (2018). 

23.   C. Li, Z. Wang, M. Rao, D. Belkin, W. Song, H. Jiang, P. Yan, Y. Li, P. Lin, M. Hu, N. 

Ge, J. P. Strachan, M. Barnell, Q. Wu, R. S. Williams, J. J. Yang, Q. Xia, Long short-term 
memory networks in memristor crossbar arrays. Nat. Mach. Intell. 1, 49–57 (2019). 

24.   A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, E. Eleftheriou, Memory devices and 

applications for in-memory computing. Nat. Nanotechnol., 246–253 (2020). 

14 

 
 
25.   F. Cai, J. M. Correll, S. H. Lee, Y. Lim, V. Bothra, Z. Zhang, M. P. Flynn, W. D. Lu, A 
fully integrated reprogrammable memristor– CMOS system for efficient multiply–
accumulate operations. Nat. Electron. (2019). 

26.   M. Prezioso, F. Merrikh-Bayat, B. D. Hoskins, G. C. Adam, K. K. Likharev, D. B. 

Strukov, Training and operation of an integrated neuromorphic network based on metal-
oxide memristors. Nature. 521, 61–64 (2015). 

27.   T. Gokmen, W. Haensch, Algorithm for Training Neural Networks on Resistive Device 

Arrays. Front. Neurosci. 14 (2020), doi:10.3389/fnins.2020.00103. 

28.   A. Cauchy, Méthode générale pour la résolution des systemes d’équations simultanées. 

Comp. Rend. Sci. Paris. 25, 536–538 (1847). 

29.   D. E. Rumelhart, G. E. Hinton, R. J. Willams, Learning representations by back-

propagating errors. Nature. 323, 533–536 (1986). 

30.   Y. Feng, Y. Tu, How neural networks find generalizable solutions: Self-tuned annealing in 

deep learning (available at https://arxiv.org/abs/2001.01678). 

31.   A. Sebastian, T. Tuma, N. Papandreou, M. Le Gallo, L. Kull, T. Parnell, E. Eleftheriou, 
Temporal correlation detection using computational phase-change memory. Nat. 
Commun. 8 (2017), doi:10.1038/s41467-017-01481-9. 

32.   S. Kim, T. Todorov, M. Onen, T. Gokmen, D. Bishop, P. Solomon, K. Lee, M. Copel, D. 
B. Farmer, J. A. Ott, T. Ando, H. Miyazoe, V. Narayanan, J. Rozen, Metal-oxide based , 
CMOS-compatible ECRAM for Deep Learning Accelerator. IEEE Int. Electron Devices 
Meet., 847–850 (2019). 

33.   H. Kim, M. Rasch, T. Gokmen, T. Ando, H. Miyazoe, J.-J. Kim, J. Rozen, S. Kim, Zero-

shifting Technique for Deep Neural Network Training on Resistive Cross-point Arrays 
(2020) (available at https://arxiv.org/abs/1907.10228). 

34.   B. Scellier, Y. Bengio, Equilibrium propagation: Bridging the gap between energy-based 

models and backpropagation. Front. Comput. Neurosci. 11, 1–21 (2017). 

35.   R. Salakhutdinov, G. Hinton, Deep Boltzmann machines. J. Mach. Learn. Res. 5, 448–455 

(2009). 

36.   T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. 

Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. 
Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. 
Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, 
D. Amodei, Language Models are Few-Shot Learners (2020) (available at 
http://arxiv.org/abs/2005.14165). 

37.   A. Karpathy, J. Johnson, L. Fei-Fei, Visualizing and Understanding Recurrent Networks, 

1–12 (2015). 

15 

 
 
Acknowledgments: We thank James B. Hannon for careful reading of our manuscript and many 
useful suggestions. 

Funding: This work is funded by IBM Research. 

Author contributions: - 

Competing interests: The authors declare no competing interests. 

Data and materials availability: All data are available in the main text. 

16 

 
 
 
 
  
 
