1
2
0
2

y
a
M
6
1

]

C
D
.
s
c
[

3
v
1
4
6
4
1
.
4
0
1
2
:
v
i
X
r
a

Tuna: A Static Analysis Approach to Optimizing
Deep Neural Networks

Yao Wang
Amazon Web Services
wayao@amazon.com

Rui Li
University of Utah
lirui@cs.utah.edu

Xingyu Zhou
Amazon Web Services
zhoxingy@amazon.com

Yong Wu
Amazon Web Services
yongwu@amazon.com

Yanming Wang
Amazon Web Services
yanmwang@amazon.com

Vin Sharma
Amazon Web Services
vinarm@amazon.com

Abstract—We introduce Tuna, a static analysis approach to
optimizing deep neural network programs. The optimization of
tensor operations such as convolutions and matrix multiplications
is the key to improving the performance of deep neural networks.
Many deep learning model optimization mechanisms today use
dynamic analysis, which relies on experimental execution on a
target device to build a data-driven cost model of the program.
The reliance on dynamic proﬁling not only requires access to
target hardware at compilation time but also incurs signiﬁcant
in machine resources. We introduce an approach that
cost
proﬁles the program by constructing features based on the target
hardware characteristics in order. We use static analysis of the
relative performance of tensor operations to optimize the deep
learning program. Experiments show that our approach can
achieve up to 11× performance compared to dynamic proﬁling
based methods with the same compilation time.

I. INTRODUCTION

Deep neural networks (DNN) are the mainstay of many
applications including image classiﬁcation, natural language
processing, speech recognition, and automated driving. Cloud
service providers are offering machine learning services that
can compile, optimize, and deploy DNN on various target
hardware. Optimizing a large set of DNN models, from
convolutional neural networks to transformer-based networks,
for a wide range of target hardware, from general purpose
processors to AI accelerator ASICs, is constrained by two
primary factors:

Compilation time and expense. A long compilation
time can be a bad user experience. In addition, the longer the
compilation time, the greater the infrastructure costs.

Cross compilation. A compiler cannot assume that
it
has access to the target hardware and it must be able to
cross-compile from a host machine with a different hardware
than the target.

Current machine learning compilers use two common ways
to generate high performance deep learning code for multiple
target hardware. The ﬁrst method is based on auto-tuning,
as implemented in AutoTVM [1], Ansor [2], FlexTensor [3],

It

and Halide auto-scheduler [4]. Systems using this approach
usually conduct a machine-learning driven auto-tuning search
across a large predeﬁned program transformations space.
The system generates code samples and measures their
performance on the target machine.
trains a machine
learning model based on the collected measurement data, and
use this model to guide subsequent code sample selection and
code generation. The second method uses vendor-supplied
kernel libraries (e.g., TensorRT, OneDNN). This is widely
adopted by current machine learning frameworks such as
MXNet, PyTorch, and TensorFlow. Both these methods,
however, have several drawbacks that restrict their application
at scale. Although the auto-tuning approach signiﬁcantly
reduces the engineering effort needed to hand-craft efﬁcient
code for each target platform, training an effective cost model
for that target hardware requires collecting proﬁle data from
the execution of entire DNN on a real device. This breaks
the cross compilation constraints for production compilation
service. Furthermore, auto-tuning based compilation requires
large number of samplings in program transformation space
to converge to local optima, which can be prohibitively long.
For example, tuning TensorFlow SSD MobileNet for Amazon
Graviton2 target with AutoTVM takes 240 hours. On the other
hand, vendor kernel
libraries provide efﬁcient handcrafted
kernel codes which do not require auto-tuning. However,
vendor libraries don’t cover every operation used in DNN
models. Adding more kernels requires heavy engineering
effort. In addition, one vendor library usually only supports a
speciﬁc hardware architecture. We have to integrate various
libraries into production service to support different target
hardware, making it hard to manage service infrastructure
and extend to new target.

In order to resolve the challenge of building a compilation
service with limited compilation time and cross-compilation
mechanism, this paper proposes a system, Tuna, that utilizes
the static analysis and a combination of analytical cost mod-
eling to optimizing DNN. Tuna has the following advantages
comparing to auto-tuning based compilation:

• Optimization is done with static analysis and no real

 
 
 
 
 
 
hardware is required.

• Hardware-based cost model is transferable to different
micro architectures. Only one single cost model is re-
quired for one major architecture, such as CPU and GPU.
• Unlike performance measurement which requires sequen-
tial execution on a target device, static analysis tasks can
be fully paralleled on a multi-core CPU machine, which
largely reduces total analyzing time.

Comparing to vendor kernel libraries, Tuna supports a large
set of DNN kernels. Tuna provides a uniform compilation
stack for various target hardware.

In this paper, we create cost model for both CPU and Nvidia
GPU architectures. We evaluate our approach on cloud servers
and edge devices. Experimental results show our approach
largely reduces total compilation time while achieving bet-
ter average performance comparing to dynamic proﬁled cost
model.

II. OVERVIEW

Tuna is a deep learning kernel optimization framework
fully relying on compile-time static analysis. Figure 1 shows
the overall architecture of Tuna. The system inputs are a
tensor program e together with corresponding loop transfor-
mation candidate space Te . For a transformation t ∈ Te , let
i = g(e, t) be the intermediate representation of transformed
program. Code generator then generates low-level code a
(e.g., assembly, PTX). Hardware-based program feature is
extracted via pf = f (i , a). Program performance score is
calculated through c(pf ), with cost model function c. The
objective of Tuna is formalized as the following:

Fig. 1: Overview of the static analysis approach to optimizing
tensor programs.

[6] as

search algorithm. Evolution
evolution strategies
strategies is a parallel black-box optimization algorithm.
The computation among each iteration of population can
be executed simultaneously. With multi-core CPU machine
or a multi-machine compilation system, the total searching
time can be signiﬁcantly reduced to be ﬁt into the normal
compilation ﬂow.

III. HARDWARE RELATED COST MODEL

This section describes the analytical cost model to predict
tensor program performance. Tuna system can be formulated
as following: we extract a series of hardware related features,
f0 , f1 , ..., fn , which have great impact on deep learning tensor
programs performance. These features can fall
into two
categories:

arg min
t∈Te

c(f (g(e, t), a)).

(1)

Tuna is built upon TVM [5], a deep learning compiler
stack. While we reuse the tensor expression system, IR and
code generator from TVM, the following two key components
are the enhancements:

low-level

Performance related instructions. Tuna counts the number
of
instructions that dominates tensor program
performance, which mainly includes arithmetic and data
movement instructions. We propose a uniﬁed method which
jointly parses high-level program IR and low-level assembly
code to get an accurate estimation of instruction numbers.

Hardware
related cost model. Existing search-based
approaches mainly rely on high-level loop structure features,
loop vectorization and number
such as loop unrolling,
of arithmetic operations. We argue that
these high level
loop features don’t
include hardware speciﬁcations which
have great
impact on kernel performance. Tuna analyzes
both program IR and low-level generated codes to extract
target hardware related features, such as number of SIMD
instructions, CPU cache locality, GPU shared memory
utilization, etc. With these low-level hardware features,
Tuna cost model is accurate enough to predict the relative
performance for a set of transformations of a tensor program,
rather than relying on experimental execution on real hardware
device.

Multi-threaded
algorithm. A multi-threaded
search algorithm which can fully utilize multi-core CPU
resources is the key to accelerate searching. We choose

search

General hardware features. Hardware features such as
cache locality and instruction level parallelism, have great
impact on program performance. Speciﬁc algorithms are
required to extract such features from program IR and
assembly codes.

The program performance
with respect to features:

score

is

computed linearly

score = a0 ∗ f0 + a1 ∗ f1 + ... + an ∗ fn

(2)

The coefﬁcients a0 , a1 , ..., an are generated for each hardware
architecture through hardware instruction latency and empiri-
cal proﬁling data.

Tuna is generic enough to support different hardware archi-
tectures. This paper covers Intel CPU, ARM CPU and Nvidia
GPU. We also investigate the transferability of the Tuna cost
model across different micro architectures. Experiment results
of high-end sever level chips versus resource-limited edge

devices show that if two different micro architectures share the
same set of Single-Instruction-Multiple-Data (SIMD) related
instructions, a single cost model can be applied to both target
platforms without modiﬁcation.

A. CPU cost model

This section describes Tuna CPU cost model
inference

in detail.
To achieve decent
speed for deep learning
program on modern CPU, it is common to do the following
optimizations:
thread-level parallelism to utilize multi-core
CPU resources. Loop tiling to ensure good cache locality.
Efﬁcient utilization of SIMD registers to maximize vector
computation throughput. Similarly, it’s necessary to include
these hardware features as parts of our CPU cost model to
accurately analyze program performance. Tuna CPU cost
model includes the following features:

Number of SIMD instructions. Compute-intensive tensor
program performance is dominated by vector arithmetic and
data movement instructions. For Intel AVX instruction set,
vfmadd and vmov are the most common instructions in
conv2d and dense operators, while for AARCH64 Neon fmla,
ld and st are used. We parse the program IR and assembly
codes to get
the total number of these signiﬁcant SIMD
instructions.

Estimation of L1 cache miss. Cache locality is a key
factor for tensor program on CPU. An analytical method is
proposed in this paper to estimate L1 cache miss for a given
program IR.

Algorithm 1 shows the main procedure of jointly parsing.
Loop blocks are extracted from program abstract syntax tree
with pre-order depth-ﬁrst-search. From assembly control ﬂow
graph, we identify a local basic block as a loop candidate
with the following condition: Traversing from top to bottom
of assembly code, there exists a jump instruction j targeting
a basic block LBBx , and the position of LBBx is above
j . Each pair of loop and basic block are then matched by
checking whether they have the same iteration boundary. For
matched basic blocks, we count the number of selected SIMD
instructions. Finally, the algorithm calculates the total number
of SIMD instructions for every loop block in original program
IR.

LO O P-Map(IR, assembly)

ForLoops = PR E O R D E R-DFS-For-Loop(IR)
LoopLBBs = ID E N T I F Y-Loop-LBB(assembly)
matchedIdx = 0
matchedLBBs = []
for each basicBlock in LoopLBBs do
forLoop = ForLoops[matchedIdx]
if PA T T E R N-Match-Loop(forLoop, basicBlock)
then

matchedLBBs.append(basicBlock)
matchedIdx += 1

end

end
return
CO U N T-Instruction(F orLoops, matchedLBBs)
Algorithm 1: Algorithm for mapping program IR and
assembly code

Instruction level parallelism. Comparing to handcraft
kernel libraries which maximize fma instruction parallelism,
search-based approach involves more complicated instruction
arrangements.
the
generated codes can keep CPU pipeline busy.

to evaluate whether

signiﬁcant

is

It

1) Loop structure mapping: We started from counting
the number of SIMD instructions. Program IR represents
the transformed tensor program in high level pattern, which
preserves the complete loop structures. However, the actual
SIMD instruction arrangement is opaque in high-level program
IR, due to optimizations done in code generation process,
such as register allocation and superword-level parallelism
vectorization. On the other side, low-level assembly codes
provide detailed instruction information in each basic block.
However, it is difﬁcult to restore original loop structure from
assembly control-ﬂow graph. It’s infeasible to extract instruc-
tion information solely from either program IR or assembly
code.

In this paper we propose an algorithm which jointly parse
high-level program IR and low-level assembly code. The key
idea is to extract full loop information from program IR and
low-level instruction quantity from assembly code. A pattern
matching is executed to match loop bodies with assembly basic
blocks and calculate total number of SIMD instructions.

2) Cache locality: In this section, we developed an im-
proved data locality model as an analyisis pass in TVM
IR (TIR). This pass can analyze all kinds of computations
supported by TVM in a short time. Therefore it provides a
fast data locality approximation for the whole DNN network
scheduling, and can be easily and systematically combined
with other important models and passes together to guide the
schedule selection of the whole network. The main idea of
the model is that we approximately estimate the data footprint
and data movement volume required to move into the cache
of the object code. The object code was abstracted as a tree
consists of loop-nodes and access-nodes. All loop-nodes are
non-leaf nodes and carry all information of a loop statement.
All access-nodes are leaf nodes that represent either a tensor
load access or a tensor store access. The data footprint and
data movement are calculated by traversing the bottom to the
top of the tree.

We use Two Matrix Multiply (2MM) as an example to
demonstrate the method for building data movement. Listing
1 shows an example of fused and tiled 2MM code. In this
example the ﬁrst Matmul uses i and j as free index and
perform contraction over k . The second Matmul uses i and
l as free index and perform contraction over j . The tiling loop
of i and j are tiled and fused together, and other loops are

/ / Ni / Nj / Nk
f o r ( i t = 0 ;

a r e p e r f e c t m u l t i p l e s o f T i / T j / Tk
i t < Ni ;

i t += T i )

f o r ( j t = 0 ;

j t < Nj ;

j t += T j )

i 1 < T i ;

f o r ( i 1 = 0 ;

f o r ( j 1 = 0 ;

f o r ( k = 0 ; k < Nk ; k ++)
|
|
|
|
f o r ( l = 0 ;

C[ i + i t ] [ j + j t ]+=
A[ i + i t ] [ k ] *B[ k ] [ j + j t ] ;

j 1 < T j ;

l < Nl ;

l ++)

i 1 ++)

j 1 ++)

f o r ( i 2 = 0 ;

i 2 < T i ;

i 2 ++)

f o r ( j 2 = 0 ;

j 2 < T j ;

j 2 ++)

E [ i + i t ] [ l ]+=
C[ i + i t ] [ j + j t ] *D[ j + j t ] [ l ]

Listing 1: Fused and tiled two matrix multiplication

non-tiled and non-fused. The cache capacity and values of all
tile-size and problem size are known before the analysis starts.
We assume the cache capacity S is enough to store all data
footprints below tiling loops, and is not enough to store data
footprints of any tiling loop. This means Ni > S and Nj > S ,
but S > Ti Tj + Ti Nl + Tj Nl + Tj Nk + Ti Nk .

The data footprint and data movement are computed from
leaf node to the root of the tree. Since all leaf nodes are tensor
accesses, the footprint and data movement for leaf nodes are
both 1. The data footprint of a loop node is the number of
distinct data elements accessed during all iterations of this
loop. The data movement of a loop node is calculated based on
its footprint and cache capacity. If the footprint of a single loop
iteration is smaller than cache capacity, the data movement of
this loop node is equal to its node footprint. Otherwise, the
data movement of this loop node is evaluated as product of
the number of iterations and the data movement of its single
iteration. The data movement of single iteration is correlated
to data movement of sub nodes, which is computed earlier in
the bottom-up procedure.

In the 2MM example, based on our capacity assumption,
the footprint of a single iteration of loop jt is Ti Tj + Ti Nl +
Tj Nl + Tj Nk + Ti Nk which ﬁts in cache. for all sub-loop
nodes of jt, the data movement is equal to data footprint.
For loop jt, since its single iteration footprint ﬁt in cache,
when the control ﬂow goes to the next iteration, tensor A
and E could get reuse because their access functions do not
include index jt. Therefore, the data movement of loop jt is
Ti Nj + Ti Nl + Nj Nl + Nj Nk + Ti Nk , which is still equal
to its footprint. However, for loop it, even a single iteration
footprint does not ﬁt in cache. So the data movement of loop it
will be the product of number of iterations and data movement
of a single iteration, which is (Ti Nj +Ti Nl +Nj Nl +Nj Nk +
Ti Nk ) ∗ Ni /Ti . Figure 2 shows the loop structure and data
movement calculation of the 2MM example.

Algorithm 2 shows the main procedure of visiting each node
in the TIR node tree. When visiting a loop node, the algorithm
recursively visits all its children, and compute the union of
data footprint of a single iteration. The algorithm will detect
whether the union of data footprint can ﬁt in cache, and track
the reuse status of each tensor. If the data footprint of that
iteration ﬁts in cache, the data movement volume is same as
its footprint. Otherwise, the algorithm will use the tracked

reuse status information to calculate the data movement. If
the reuse status of the tensor is true, the movement volume of
that tensor will be equal to footprint, otherwise the movement
volume will be the movement volume of a single iteration
multiply the trip-count. The reuse status will be true at the leaf
node by default. While our analysis move from the bottom to
top, the reuse status will be ﬂipped to false if the following
conditions are true. The ﬁrst case is if tensor footprint exceed
cache. The second case is if there exists a set of continuous
loop nodes that do not access this tensor, such that their
footprints exceed cache. Both implies the reuse distance of
the tensor under discussion exceed the cache capacity. The
whole analysis module is implemented by using Integer Set
Library [7].

VI S I T-Node(node, cache)

if node is AccessNode then

return Dmov=1, Dfp=1, DataSpace

end
else if node is LoopNode then

subDspace, subDfp, subDmov = []
for each childNode do

dspace, dfp, dmov = VI S I T-Node(childNode,
cache)
append dspace, dfp, dmov to subDspace,
subDfp, subDmov

end
DataSpace =
CR E A T E-IntegerSet(subDspace)
if DataSpace.cardinality > cache then

Dmov = 0
for each tensor ts do

get ts.dmov, ts.dspace, ts.expr from
subDmov, subDspace
if ts.reuse is True then

Dmov +=
ES T I M A T E-Dfp(node.iterVar,
ts.dspace)
UP D A T E-Reuse-Status(ts)

end
else

Dmov += ts.dmov * node.tripcount

end

end
return Dmov, Dfp=DataSpace.cardinality,
DataSpace

end
else

return Dmov=Dfp, Dfp=DataSpace.cardinality,
DataSpace

end

end
Algorithm 2: Algorithm for modeling data movement at
each TIR tree node

3) Instruction level parallelization: Modern CPU utilizes
pipelines and out-of-order execution to increase the instruction
level parallelism (ILP) and hide the latency. Therefore, to

is a write-after-read (WAR) dependency or write-after-write
dependency, the latter instruction who writes to the resource
cannot be scheduled before the prior instruction.

B. Nvidia GPU cost model

This section describes our Nvidia GPU cost model
in
detail. To achieve efﬁcient execution, GPU tensor program
needs to exhibit decent
thread-level parallelism to utilize
GPU thread resources. Data locality is also signiﬁcant for
shared memory efﬁciency. Our GPU cost model includes the
following features:

Number of PTX instructions. Similar to CPU assembly
codes, we select fma,
ld and st as the most signiﬁcant
instructions for tensor programs. We parse the Nvidia GPU
PTX code to get the total number of these instructions.

Thread level parallelism. Utilization of GPU threads
largely determines the performance of GPU kernels. We
evaluate several aspects that directly affect GPU thread-
level parallelism: number of operations in a single thread,
Streaming Multiprocessor
(SM) occupancy, warp latency
hiding and shared memory bank conﬂict.

1) Loop structure mapping: The NVCC compilation pro-
cess unrolls small loops with a known trip count by default,
which makes it hard to identify the corresponding loop struc-
ture in high-level program IR from the low-level PTX code.
In this paper we propose an algorithm that parses the PTX
code, identiﬁes loop structures and calculates total number of
instructions. The key idea is to identify the loop structure from
PTX code and loop iterations from registers.

Algorithm 3 shows the key idea of how we get the number
of instructions. We adopt the same idea from Algorithm 1 of
identifying loop structures in assembly code since PTX code
has similar condition and jump structures. After we have the
loop structures, we maintain a register initial value map and
register value update map by parsing the PTX code. Since we
already know the loop structure, once we reach the line with
eligible condition check, we know it is for a certain loop.
Based on the condition check, we get the register used for
comparing and the end condition. From the two maps we
maintain, we can easily calculate the loop iterations by initial
value, update value and end condition. Finally, the algorithm
calculates the total number of PTX instructions for every loop.
2) Thread level parallelization: In this section we discuss
the details about features regarding to thread-level parallelism.

Workload per thread We count
the total number of
PTX instructions for a single thread with Algorithm 3. We
calculate the total number of cycles based on instruction
cycles from [8]:

P T XInstruction
(cid:88)

i=1

Count(i) ∗ Cost(i)

(3)

Fig. 2: Loop structure and data movement of 2MM

achieve high performance, the generated assembly code should
allow the processor issue as much instructions as possible to
keep the pipeline busy. Since the TVM framework includes
more types of operators which will generates more types
of instructions, a speciﬁc model for speciﬁc operator is not
adequate to apply on the TVM-generated code. To solve this
problem, we propose a static analysis model to estimate the
instruction level efﬁciency.

The key idea of the model is to design a simpliﬁed fast
out-of-order instruction scheduler that schedule instructions
in each basic block. The scheduler consists of two major
components, the data dependency builder and the instruction
scheduler. The data dependency builder ﬁrst scan the whole
basic block, and creates two instruction dependency graph
for true dependency and false dependency respectively. Then
the instruction scheduler will schedule the instructions based
on the dependency graph and hardware speciﬁcations such
as instruction latency and number of different processing
unit. During the scheduling, a timestamp will be assigned to
each instruction, indicating the time point of the instruction
starting executed. The ﬁrst ready-to-execute instruction will be
scheduled at cycle zero, and total cycles required for ﬁnalize
all instructions will be used as the ILP cost of this basic block.
We calculate the product of ILP cost and number of executions
for a single basic block, and add up all products of all basic
block. The summation is used as the ILP cost of the whole
program.

During the scheduling, the scheduler will manage two differ-
ent hazard, the structural hazard and data hazard. The structure
hazard is controlled by limiting the maximum number of
instruction issued at each cycle. If the maximum number of
issued instruction reached the number of processing unit, the
next instruction to be issued will be delayed to next cycle. The
data hazard is identiﬁed by analyzing the dependency graph.
If there is a read-after-write (RAW) dependency between two
instructions, the consumer instruction should be scheduled
after the producer instruction ﬁnishing execution. If there

itkljti1j1i2j2ABCDECDM:1FP:1DM:1FP:1DM:1FP:1DM:1FP:1DM:1FP:1DM:1FP:1DM:1+Tj+TjFP:1+Tj+TjDM:1+Tj+TjFP:1+Tj+TjDM:Ti+Tj+TiTjFP:Ti+Tj+TiTjDM:Ti+Tj+TiTjFP:Ti+Tj+TiTjDM:TiNk+TjNk+TiTjFP:TiNk+TjNk+TiTjDM:TiNl+TjNl+TiTjFP:TiNl+TjNl+TiTjDM:TiNl+NjNl+TiNj+TiNk+NjNkFP:TiNl+NjNl+TiNj+TiNk+NjNkDM:(TiNl+NjNl+TiNj+TiNk+NjNk)*Ni/TiFP:NiNl+NjNl+NiNj+NiNk+NjNkSingle iteration footprint: TiNk+TjNk+TiTj+TiNl+TjNlfit in cacheLO O P-Map-PTX(P T X)

LoopBBs = ID E N T I F Y-Loop-BB(P T X)
regInitMap, regUpdateMap =
RE G I S T E R-Match-Loop()
LoopIterations = []
for each basicBlock in LoopBBs do

loopIteration =
GE T-Iterations(regInitM ap, regU pdateM ap)
LoopIterations.append(loopIteration)

end
return
CO U N T-Instruction(LoopBBs, LoopIterations)
Algorithm 3: Algorithm for identifying loop iterations in
PTX code

The workload per thread is represented by the total number
of cycles.

Streaming Multiprocessor (SM) occupancy It is important
to keep all SMs busy for compute-intensive tensor programs.
We check the total number of thread blocks to determine
whether it is greater than the total number of SMs, so that all
SMs have at least one block to execute. A penalty is added
if the total number of thread blocks is too small.

Warp latency hiding Warp scheduling is a vital mechanism
to hide GPU instruction latency. With more warps on each
SM, GPU warp scheduler has a better chance to hide memory
latency, thus achieving better performance. We calculate the
maximum number of thread blocks that can be concurrently
scheduled in one SM by checking the number of registers
and shared memory usage per block. These information can
be extracted with nvcc ptxas-option command.

Shared memory bank conﬂict For modern Nvidia GPUs
(compute capability >= 5.0), shared memory has 32 banks
that are organized such that successive 32-bit words map
to successive banks. Each bank has a bandwidth of 32
bits per clock cycle and any shared memory load or store
of n addresses that spans n distinct memory banks can
be served simultaneously, yielding an effective bandwidth
that is n times as high as the bandwidth of a single bank.
However, if multiple addresses of a memory request map
to the same memory bank, the accesses are serialized. The
hardware splits a memory request
that has bank conﬂicts
into as many separate conﬂict-free requests as necessary,
decreasing the effective bandwidth by a factor equal to the
number of separate memory requests. The one exception
here is when multiple threads in a warp address the same
shared memory location, resulting in a broadcast. In this case,
multiple broadcasts from different banks are coalesced into
a single cast from the requested shared memory locations to
the threads. To incorporate the effect of bank conﬂict, we
ﬁrst numerically evaluate the shared memory access indices
of all threads in the ﬁrst warp from the IR to compute the

actual shared memory throughput. We then use the ratio
between actual shared memory throughput and requested
shared memory throughput to adjust the number of shared
memory operations.

IV. SEARCH ALGORITHM

(a) Top-10 Performance Ratio on Intel Xeon Platinum 8124M CPU

(b) Top-10 Performance Ratio on AWS Graviton2 ARM CPU

(c) Top-10 Performance Ratio on AWS P3 V100 GPU

Fig. 3: Top10 performance ratio for single operators from Tuna
VS AutoTVM.

In this paper, we need to search in a well-deﬁned large
search space to ﬁnd the optimal conﬁguration. We choose
Evolution Strategies (ES) as the search algorithm and treat
the search as an arbitrary black-box optimization problem.

ES works by treating the model of interest as an arbitrary
optimization problem. Given parameters of the model and
the associated performance of that model on the task of
interest, ES is able to optimize and train the model without
any knowledge of the structure or architecture of the model
itself. Speciﬁcally, at each iteration, random gaussian noise is
added to the parameters to generate variations of the current

0.00.20.40.60.81.01.2direct_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVM0.00.20.40.60.81.01.2direct_convwinograd_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVM0.00.20.40.60.81.01.2direct_convwinograd_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVMUnit: ms

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

Framework
AutoTVM Partial
AutoTVM Full
Tuna

22.09
30.41
22.7
23.3

24.29
47.7
30.29
30.16

31.26
11.47
6.37
6.85

253.04
86.39
16.66
15.12

(a) Entire network performance on a system with Intel Xeon Platinum 8124M CPU

Unit: ms

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

Framework
AutoTVM Partial
AutoTVM Full
Tuna

37.15
46.95
29.55
30.24

43.39
75.37
45.56
44.3

307.55
195.38
16.11
17.77

56.7
63.62
15.11
16.13

(b) Entire network performance on a system with AWS Graviton2 ARM CPU

Unit: ms

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM Partial
AutoTVM Full
Tuna

3499.72
618.45
614.43

4487.97
1419.83
1389.35

7111.88
1114.69
1259.88

1058.85
574.37
541.17

(c) Entire network performance on a system with ARM Quad-core Cortex-A53 64-bit CPU(Acer aiSage)

Unit: ms

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

Framework
AutoTVM Partial
AutoTVM Full
Tuna

31.65
44.77
27.43
28.45

33.2
70.97
30.69
34.87

8.65
105.16
2.34
2.65

16.34
13.03
3.24
4.62

(d) Entire network performance on a system with Nvidia V100 GPU

Unit: ms

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

Framework
AutoTVM Partial
AutoTVM Full
Tuna

126.7
202.21
50.78
59.81

124.13
426.44
57.25
80.05

26.04
21.16
15.21
18.82

19.66
51.32
7.64
11.49

(e) Entire network performance on a system with Nvidia Jetson AGX Xavier GPU

TABLE I: Entire network performance of Tuna and the selected baselines.

model. Each variation is then tested for performance on the
task of interest. Finally, a weighted sum is taken, based on
performance, to generate an updated model. This becomes
the current model for the next iteration. In standard ES, we
treat the performance of the model within its environment as a
black-box optimization problem. Now we extend that concept
to treat ES itself as a black-box optimization problem with
respect to the learning rate α and standard deviation of noise
σ.

The main steps about ES are given in Algorithm 4, F

Get learning rate α, noise standard deviation σ, initial
policy parameters θ0
for t = 0, 1, 2, ... do

Sample ε1, ...εn ∼ N (0, I)
for i = 1, ..., n do

Fi = F (θt + σεi)

end
θt+1 = θt + α 1
nσ

end

(cid:80)n

i=1 Fiεi

Algorithm 4: Evolution Strategies

Unit: hour

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

53
0.7

50
1

12
0.13

2.92
0.012

(a) Entire network compilation time for Intel Xeon Platinum 8124M CPU.

Unit: hour

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

240
3

280
5

49
0.45

3.2
0.062

(b) Entire network compilation time for AWS Graviton2 ARM CPU.

Unit: hour

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

299
3

316
5

64
0.45

5.14
0.062

(c) Entire network compilation time for ARM Quad-core Cortex-A53 64-bit CPU(Acer aiSage).

Unit: hour

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

93.5
1.6

167.5
1.04

37
0.13

4.4
0.05

(d) Entire network compilation time for AWS P3 V100 GPU.

Unit: hour

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

202.5
1.7

356.3
1.05

83
0.13

4.4
0.05

(e) Entire network compilation time for Nvidia Jetson AGX Xavier GPU.

TABLE II: Entire network compilation time of Tuna VS AutoTVM.

denotes objective function working with parameters θ.

V. EVALUATION

This section evaluates the performance of Tuna. The follow-
ing hardware platforms are selected to cover the most widely
used cases for deep learning inference: Intel Xeon Platinum
8124M CPU (Amazon EC2 C5.9xlarge), AWS Graviton2
(Amazon EC2 M6G.4xlarge, ARM 64-bit CPU), ARM Quad-
core Cortex-A53 64-bit CPU (Acer aiSage), Nvidia Tesla
V100 GPU (Amazon EC2 P3.2xlarge) and Nvidia Jetson AGX
Xavier GPU (512-core Volta GPU with Tensor Cores).

We evaluate Tuna on two levels: single operator and entire
neural network. We use AutoTVM as search-based tuning
baseline for both levels of evaluation. We also compare Tuna
with state-of-the-art deep learning framework solutions for the
entire neural network evaluation.

We use TVM v0.7.0 release for all target devices. For Intel
CPU targets, we chose TensorFlow 1.15.3 and Pytorch 1.6.0 as
deep learning framework solutions. For AWS Graviton2 which

has AARCH64 CPU, we use ARM Tool Solution docker
container which provides AARCH64 optimized version of
TensorFlow and Pytorch. For AWS P3 instance which has
Nvidia V100 GPU, we use NGC docker container with tag
21.03-tf1-py3 and pytorch:21.03-py3 which optimizes Tensor-
Flow and Pytorch for Nvidia GPU. For Nvidia Jetson AGX
Xavier GPU, we choose TensorFlow 1.15.2 and Pytorch 1.6.0.

A. Entire Network

We ﬁrst report Tuna performance on a set of deep learning
models pre-trained with TensorFlow and PyTorch, two popular
deep learning frameworks: TensorFlow SSD MobileNet v2,
TensorFlow SSD Inception v2, PyTorch ResNet50 v1 and
PyTorch Bert base uncased. Selected models cover the most
popular deep learning applications:
image classiﬁcation,
language processing. Three
object detection and natural
metrics are measures to demonstrate the advantages of Tuna:

Compilation time. Table II records the compilation time

Unit: dollar

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

81.09
2.86

76.5
4.08

18.36
0.53

4.47
0.05

(a) Entire network compilation cost for Amazon EC2 C5.9xlarge(price $1.53 per hour).

Unit: dollar

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

147.84
12.24

172.48
20.4

30.18
1.84

1.97
0.25

(b) Entire network compilation cost for AWS Graviton2 ARM CPU(price $0.616 per hour).

Unit: dollar

TF SSD MobileNet

TF SSD Inception

PT ResNet50

PT Bert

AutoTVM
Tuna

286.32
6.53

512.61
4.24

113.22
0.53

13.46
0.2

(c) Entire network compilation cost for Amazon EC2 P3.2xlarge(price $3.06 per hour).

TABLE III: Entire network compilation cost of Tuna VS AutoTVM.

of both Tuna and AutoTVM to optimize neural networks.
Experiments show that Tuna can speed up compilation
process up to 339× comparing to AutoTVM.

in dollar

Compilation cost. We measure the cost
to
optimize deep learning models on Amazon EC2 instances
to demonstrate the cost reduction provided by Tuna. We
multiply instance on-demand price with compilation time to
get the cost to compile. We benchmark Tuna on Amazon
EC2 C5.24xlarge which takes $4.08 per hour. Table III shows
Tuna reduces compilation cost down to 1.1% of the original
cost comparing to AutoTVM.

Inference latency. Table II shows the model
inference
latency from different methods. The row of AutoTVM Partial
shows the model inference latency compiled with AutoTVM
in the same compilation time of Tuna. Results show that
with the same compilation time, Tuna achieves up to 11×
performance of AutoTVM. We also compare Tuna latency
with AutoTVM full tuning models. On average Tuna achieves
91.5% performance comparing to best possible schedules
generated by AutoTVM. This result shows that Tuna is able
to achieve similar performance of full tuning with signiﬁcant
less time and cost. Finally we compare Tuna performance
with deep learning framework solutions. Tuna achieves up to
17.3× performance. Due to memory resource limitation on
Acer aiSage device, directly running inference through deep
learning framework is infeasible. We only compare Tuna with
AutoTVM on this platform.

B. Single Operator

In this section we report

the performance of a set
of widely used compute-intensive deep learning operators.

We use AutoTVM tuned operator performance as baseline.
We reuse the optimization search space deﬁned in Au-
toTVM to make apple to apple comparison. We bench-
mark conv2d , conv2d winograd , depthwise conv2d and
batch matrix multiplication performance on ARM CPU and
Nvidia GPU devices. We don’t measure conv2d winograd on
Intel CPU device since AutoTVM doesn’t deﬁne optimization
space for this operator.

We deﬁne top − k performance ratio as metric. Tuna gener-
ates top-k best programs and execution latency of all programs
are sum up. Similarly we calculate the latency summation for
top-k best programs generated with AutoTVM. We divide two
AutoTVM latency value with Tuna to evaluate the effective-
ness of Tuna to select decent optimizations from search space.
Higher value approaching 1 indicates Tuna can accurately
predict the real execution performance. Figure 3 and Figure 4
shows the benchmark result on Intel CPU, ARM CPU and
Nvidia GPU. On average We got 0.869 for top 10 and 0.873
for top 50. Experimental data shows Tuna is able to achieve
quite close performance comparing to full tuning method with
AutoTVM.

VI. RELATED WORKS

Performance modeling has been a challenging problem due
to the diversities of hardware architectures. Over the years,
researchers have proposed many analytical approaches. Here
we summarize notable related works.

optimization: Several

CPU performance
state-of-the-
art prior works has focused on analyzing the performance
on CPU and generating efﬁcient code. They includes cache
modeling tools [9] [10], static tensor libraries [11] [12]
[13], model-driven tensor computation optimizers [14] [15]

automated and can only optimize speciﬁc computations such
as tensor contractions and 2D convolutions.

GPU performance modeling: Static
analysis method
has been used for GPU performance modeling. Arafa et
al. [8] developed a GPU performance prediction framework
named PPT-GPU, which utilizes a hybrid approach between
analytical modeling and cycle-accurate modeling. They used
features such as instruction statistics from the PTX code,
instruction latencies, and L2 cache miss rate. Guerreiro et al.
[24] proposed a GPU performance evaluation method based
on recurrent neural networks and features extracted from
PTX code. We need to emphasize the goal of Tuna is to sort
and ﬁnd the best kernels in a deﬁned search space, while
these approaches aim to accurately predict the performance
of different kernels. One signiﬁcant advantage of Tuna is that
it leverages information from both low level assembly code
and high level intermediate representations.

Search-based auto-tuning: Search-based tuning approach
has become an effective method to optimize deep learning
relies on user-speciﬁed search
programs. AutoTVM [1]
space. Ansor
search space
[2] automatically generates
for small sub-graphs. Both AutoTVM and Ansor create
machine-learning based cost model and do training during
exploration. Comparing to AutoTVM and Ansor, Tuna applies
hardware feature related cost model which doesn’t require
relative performance.
any training and accurately predict
Halide auto-scheduler [4] generates schedules for the whole
input program. All these search-based tuning methods require
experimental execution on target device, while Tuna fully
relies on static analysis.

VII. CONCLUSION

In this paper, we proposed Tuna as an analytical approach
to analyze and optimize deep learning programs on modern
CPU and GPU hardware. The experiments show that we
are able to achieve 99.21% average throughput for three
categories of deep learning models on various kinds of hard-
ware platforms, with merely 1.65% average compilation time
comparing to search-based tuning approach. Tuna achieves
1.54x average performance comparing to state-of-the-art deep
learning framework inference solutions. Similar methodology
can be applied to ASIC hardware. Extending Tuna to other
architecture is a future work.

REFERENCES

[1] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau,
Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Learning to
optimize tensor programs. arXiv preprint arXiv:1805.08166, 2018.
[2] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu,
Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen,
et al. Ansor: Generating high-performance tensor programs for deep
learning. In 14th {USENIX} Symposium on Operating Systems Design
and Implementation ({OSDI} 20), pages 863–879, 2020.

(a) Top-50 Performance Ratio on Intel Xeon Platinum 8124M CPU

(b) Top-50 Performance Ratio on AWS Graviton2 ARM CPU

(c) Top-50 Performance Ratio on AWS P3 V100 GPU

Fig. 4: Top50 performance ratio for single operators from Tuna
VS AutoTVM.

[16], polyhedral compilers and polyhedral optimizations [17]
[18] [19] [20] [21] [22], and auto-tuning based compilers
[5] [23] [2]. However, all of the state-of-the-art researches
cannot be the off-the-shelf solution to our system for the
following reasons. Cache miss analyzers are accurate on
analyzing the cache misses but are not able to ﬁnd the
optimal
loop schedule. Polyhedral compilers are usually
separating the tile-size selection and loop transformation,
which makes its performance not competitive to other
frameworks. Auto-tuning based compilers are tuning the loop
schedule comprehensively for the whole network but results in
very long tuning and compiling time. Libraries are achieving
high-performance on some important operators but their static
optimization may not be optimal for all types of problem
sizes, and they could block further graph-level optimizations
such as fusion. Model-driven tensor computation optimizers
are efﬁcient and comprehensive, but they are not yet fully

0.00.20.40.60.81.01.2direct_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVM0.00.20.40.60.81.01.2direct_convwinograd_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVM0.00.20.40.60.81.01.2direct_convwinograd_convdepthwise_convbatch_matmulPerformance RatioTunaAutoTVM[3] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng.
Flextensor: An automatic schedule exploration and optimization frame-
work for tensor computation on heterogeneous system. In Proceedings
of the Twenty-Fifth International Conference on Architectural Support
for Programming Languages and Operating Systems, pages 859–873,
2020.

[4] Ravi Teja Mullapudi, Andrew Adams, Dillon Sharlet, Jonathan Ragan-
Kelley, and Kayvon Fatahalian. Automatically scheduling halide image
processing pipelines. ACM Transactions on Graphics (TOG), 35(4):1–
11, 2016.

[5] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,
Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze,
et al. {TVM}: An automated end-to-end optimizing compiler for deep
learning. In 13th {USENIX} Symposium on Operating Systems Design
and Implementation ({OSDI} 18), pages 578–594, 2018.

[6] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever.
Tensor comprehensions: Framework-agnostic high-performance machine
learning abstractions. arXiv preprint arXiv:1703.03864, 2017.

[7] Sven Verdoolaege. isl: An integer set library for the polyhedral model.
In International Congress on Mathematical Software, pages 299–302.
Springer, 2010.

[8] Yehia Arafa, Abdel-Hameed A Badawy, Gopinath Chennupati, Nan-
dakishore Santhi, and Stephan Eidenbenz.
Ppt-gpu: Scalable gpu
performance modeling. IEEE Computer Architecture Letters, 18(1):55–
58, 2019.

[9] Wenlei Bao, Sriram Krishnamoorthy, Louis-Noel Pouchet, and Pon-
nuswamy Sadayappan. Analytical modeling of cache behavior for
afﬁne programs. Proceedings of the ACM on Programming Languages,
2(POPL):1–26, 2017.

[10] Tobias Gysi, Tobias Grosser, Laurin Brandner, and Torsten Hoeﬂer. A
fast analytical model of fully associative caches. In Proceedings of the
40th ACM SIGPLAN Conference on Programming Language Design and
Implementation, pages 816–829, 2019.

[11] Tze Meng Low, Francisco D Igual, Tyler M Smith, and Enrique S
Quintana-Orti. Analytical modeling is enough for high-performance
blis. ACM Transactions on Mathematical Software (TOMS), 43(2):1–
18, 2016.

[12] Field G Van Zee and Robert A Van De Geijn. Blis: A framework
ACM Transactions on

for rapidly instantiating blas functionality.
Mathematical Software (TOMS), 41(3):1–33, 2015.

[13] Intel.

Intel oneapi deep neural network library (onednn).

https://software.intel.com/content/www/us/en/develop/documentation/
oneapi-programming-guide/top/api-based-programming/
intel-oneapi-deep-neural-network-library-onednn.html, 2020.

[14] Rui Li, Aravind Sukumaran-Rajam, Richard Veras, Tze Meng Low,
Fabrice Rastello, Atanas Rountev, and P Sadayappan. Analytical cache
modeling and tilesize optimization for tensor contractions. In Proceed-

ings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, pages 1–13, 2019.

[15] Rui Li, Yufan Xu, Aravind Sukumaran-Rajam, Atanas Rountev, and
P Sadayappan. Analytical characterization and design space exploration
for optimization of cnns. In Proceedings of the 26th ACM International
Conference on Architectural Support for Programming Languages and
Operating Systems, pages 928–942, 2021.

[16] Rui Li, Yufan Xu, Aravind Sukumaran-Rajam, Atanas Rountev, and
P Sadayappan. Efﬁcient distributed algorithms for convolutional neural-
networks. In Proceedings of the 33nd ACM Symposium on Parallelism
in Algorithms and Architectures, 2021.

[17] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya
Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, An-
drew Adams, and Albert Cohen. Tensor comprehensions: Framework-
agnostic high-performance machine learning abstractions. arXiv preprint
arXiv:1802.04730, 2018.

[18] Uday Bondhugula, Albert Hartono, Jagannathan Ramanujam, and Pon-
nuswamy Sadayappan. A practical automatic polyhedral parallelizer
In Proceedings of the 29th ACM SIGPLAN
and locality optimizer.
Conference on Programming Language Design and Implementation,
pages 101–113, 2008.

[19] Sven Verdoolaege, Juan Carlos Juega, Albert Cohen, Jose Igna-
cio Gomez, Christian Tenllado, and Francky Catthoor.
Polyhedral
parallel code generation for cuda. ACM Transactions on Architecture
and Code Optimization (TACO), 9(4):1–23, 2013.

[20] Tobias Grosser, Hongbin Zheng, Raghesh Aloor, Andreas Simb¨urger,
Armin Gr¨oßlinger, and Louis-No¨el Pouchet. Polly-polyhedral optimiza-
In Proceedings of the First International Workshop on
tion in llvm.
Polyhedral Compilation Techniques (IMPACT), volume 2011, page 1,
2011.

[21] Martin Kong, Richard Veras, Kevin Stock, Franz Franchetti, Louis-
No¨el Pouchet, and Ponnuswamy Sadayappan. When polyhedral trans-
In Proceedings of the 34th
formations meet simd code generation.
ACM SIGPLAN conference on Programming language design and
implementation, pages 127–138, 2013.

[22] C´edric Bastoul. Code generation in the polyhedral model is easier than
you think. In Proceedings. 13th International Conference on Parallel
Architecture and Compilation Techniques, 2004. PACT 2004., pages 7–
16. IEEE, 2004.

[23] Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, and Yida Wang.
In 2019 {USENIX}
Optimizing {CNN} model inference on cpus.
Annual Technical Conference ({USENIX}{ATC} 19), pages 1025–1040,
2019.

[24] Jo˜ao Guerreiro, Aleksandar Ilic, Nuno Roma, and Pedro Tom´as. Gpu
static modeling using ptx and deep structured learning. IEEE Access,
7:159150–159161, 2019.

