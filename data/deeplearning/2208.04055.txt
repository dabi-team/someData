2
2
0
2

g
u
A
8

]

G
L
.
s
c
[

1
v
5
5
0
4
0
.
8
0
2
2
:
v
i
X
r
a

Neural Set Function Extensions:
Learning with Discrete Functions in High Dimensions

Nikolaos Karalias∗
EPFL
nikolaos.karalias@epfl.ch

Joshua Robinson∗
MIT CSAIL
joshrob@mit.edu

Andreas Loukas
Prescient Design, Genentech, Roche
andreas.loukas@roche.com

Stefanie Jegelka
MIT CSAIL
stefje@csail.mit.edu

Abstract

Integrating functions on discrete domains into neural networks is key to develop-
ing their capability to reason about discrete objects. But, discrete domains are
(1) not naturally amenable to gradient-based optimization, and (2) incompatible
with deep learning architectures that rely on representations in high-dimensional
vector spaces. In this work, we address both difﬁculties for set functions, which
capture many important discrete problems. First, we develop a framework for
extending set functions onto low-dimensional continuous domains, where many
extensions are naturally deﬁned. Our framework subsumes many well-known
extensions as special cases. Second, to avoid undesirable low-dimensional neural
network bottlenecks, we convert low-dimensional extensions into representations
in high-dimensional spaces, taking inspiration from the success of semideﬁnite
programs for combinatorial optimization. Empirically, we observe beneﬁts of our
extensions for unsupervised neural combinatorial optimization, in particular with
high-dimensional representations.

1

Introduction

While neural networks are highly effective at solving tasks grounded in basic perception (Chen et al.,
2020; Vaswani et al., 2017), discrete algorithmic and combinatorial tasks such as partitioning graphs,
and ﬁnding optimal routes or shortest paths have proven more challenging. This is, in part, due to the
difﬁculty of integrating discrete operations into neural network architectures (Battaglia et al., 2018;
Bengio et al., 2021; Cappart et al., 2021a). One immediate difﬁculty with functions on discrete spaces
is that they are not amenable to standard gradient-based training. Another is that discrete functions
are typically expressed in terms of scalar (e.g., Boolean) variables for each item (e.g., node, edge to
be selected), in contrast to the high-dimensional and continuous nature of neural networks’ internal
representations. A natural approach to addressing these challenges is to carefully choose a function
on a continuous domain that extends the discrete function, and can be used as a drop-in replacement.

There are several important desiderata that such an extension should satisfy in order to be suited to
neural network training. First, an extension should be valid, i.e., agree with the discrete function
on discrete points. It should also be amenable to gradient-based optimization, and should avoid
introducing spurious minima. Beyond these requirements, there is one additional critical consideration.
In both machine learning and optimization, it has been observed that high-dimensional representations
can make problems “easier”. For instance, neural networks rely on high-dimensional internal

∗Equal contribution.

Preprint. Under review.

 
 
 
 
 
 
representations for representational power and to allow information to ﬂow through gradients, and
performance suffers considerably when undesirable low-dimensional bottlenecks are introduced into
network architectures (Belkin et al., 2019; Veliˇckovi´c & Blundell, 2021). In optimization, lifting
to higher-dimensional spaces can make the problem more well-behaved (Goemans & Williamson,
1995; Shawe-Taylor et al., 2004; Du et al., 2018). Therefore, extending discrete functions to high-
dimensional domains may be critical to the effectiveness of the resulting learning process, yet remains
largely an open problem.

With those considerations in mind, we propose a framework for constructing extensions of discrete
set functions onto high-dimensional continuous spaces. The core idea is to view a continuous point x
in space as an expectation over a distribution (that depends on x) supported on a few carefully chosen
discrete points, to retain tractability. To evaluate the discrete function at x, we compute the expected
value of the set function over this distribution. The method resulting from a principled formalization
of this idea is computationally efﬁcient and addresses the key challenges of building continuous
extensions. Namely, our extensions allow gradient-based optimization and address the dimensionality
concerns, allowing any function on sets to be used as a computation step in a neural network.

First, to enable gradient computations, we present a method based on a linear programming (LP)
relaxation for constructing extensions on continuous domains where exact gradients can be computed
using standard automatic differentiation software (Abadi et al., 2016; Bastien et al., 2012; Paszke
et al., 2019). Our approach allows task-speciﬁc considerations (e.g., a cardinalilty constraint) to
be built into the extension design. While our initial LP formulation handles gradients, and is a
natural formulation for explicitly building extensions, it replaces discrete Booleans with scalars in the
unit interval [0, 1], and hence does not yet address potential dimensionality bottlenecks. Second, to
enable higher-dimensional representations, we take inspiration from classical SDP relaxations, such
as the celebrated Goemans-Williamson maximum cut algorithm (Goemans & Williamson, 1995),
which recast low-dimensional problems in high-dimensions. Speciﬁcally, our key contribution is to
develop an SDP analog of our original LP formulation, and show how to lift LP-based extensions
into a corresponding high-dimensional SDP-based extensions. Our general procedure for lifting
low-dimensional representations into higher dimensions aligns with the neural algorithmic reasoning
blueprint (Veliˇckovi´c & Blundell, 2021), and suggests that classical techniques such as SDPs may be
effective tools for combining deep learning with algorithmic processes more generally.

2 Problem Setup

Consider a ground set [n] = {1, . . . , n} and an arbitrary function f : 2[n] → R ∪ {∞} deﬁned on
subsets of [n]. For instance, f could determine if a set of nodes or edges in a graph has some structural
property, such as being a path, tree, clique, or independent set (Bello et al., 2016; Cappart et al.,
2021a). Our aim is to build neural networks that use such discrete functions f as an intermediate layer
or loss. In order to produce a model that is trainable using standard auto-differentiation software, we
consider a continuous domain X onto which we would like to extend f , with sets embedded into
X via an injective map e : 2[n] → X . For instance, when X = [0, 1]n we may take e(S) = 1S, the
Boolean vector whose ith entry is 1 if i ∈ S, and 0 otherwise. Our approach is to design an extension
F : X → R

of f and consider the neural network NN2 ◦ F ◦ NN1 (if f is used as a loss, NN2 is simply the
identity). To ensure that the extension is valid and amenable to automatic differentiation, we require
that 1) it agrees with f on all discrete points: F(e(S)) = f (S) for all S ⊆ [n] with f (S) < ∞, and
2) F is continuous.

There is a rich existing literature on extensions of functions on discrete domains, particularly in the
context of discrete optimization (Lovász, 1983; Grötschel et al., 1981; Calinescu et al., 2011; Vondrák,
2008; Bach, 2019; Obozinski & Bach, 2012; Tawarmalani & Sahinidis, 2002). These works provide
promising tools to reach our goal of neural network training. Building on these, our method is the
ﬁrst to use semi-deﬁnite programming (SDP) to combine neural networks with set functions. There
are, however, different considerations in the neural network setting as compared to optimization.
The optimization literature often focuses on a class of set functions and aims to build extensions
with desirable optimization properties, particularly convexity. We do not focus on convexity, aiming
instead to develop a formalism that is as ﬂexible as possible. Doing so maximizes the applicability of
our method, and allows extensions adapted to task-speciﬁc desiderata (see Section 3.1).

2

Figure 1: SFEs: Fractional points x are reinterpreted as expectations x = ES∼px[1S] over the
distribution px(S) on sets. A value is assigned at x by exchanging the order of f and the expectation:
F(x)S∼px [f (S)]. Unlike f , the extension F is amenable to gradient-based optimization.

3 Scalar Set Function Extensions

We start by presenting a general framework for extending set functions onto X = [0, 1]n, where a set
S ⊆ [n] is viewed as the Boolean indicator vector e(S) = 1S ∈ {0, 1}n whose ith entry is 1 if i ∈ S
and 0 otherwise. We call extensions onto [0, 1]n scalar since each item i is represented by a single
scalar value—the ith coordinate of x ∈ X . These scalar extensions will become the core building
blocks in developing high-dimensional extensions in Section 4.

A classical approach to extending discrete functions on sets represented as Boolean indicator vectors
1S is by computing the convex-envelope, i.e., the point-wise supremum over linear functions that
lower bound f (Falk & Hoffman, 1976; Bach, 2019). Doing so yields a convex function whose value
at a point x ∈ [0, 1]n is the solution of the following linear program (LP):

(cid:101)F(x) = max

z,b∈Rn×R

{x(cid:62)z + b} subject to 1(cid:62)

S z + b ≤ f (S) for all S ⊆ [n].

(primal LP)

The set Pf of all feasible solutions (z, b) is known as the (canonical) polyhedron of f (Obozinski
& Bach, 2012) and can be seen to be non-empty by taking the coordinates of z to be sufﬁciently
small (possibly negative). Variants of this optimization program are frequently encountered in the
theory of matroids and submodular functions (Edmonds, 2003) where Pf is commonly known as the
submodular polyhedron (see Appendix A for an extended discussion). By strong duality, we may
solve the primal LP by instead solving its dual:

(cid:101)F(x) =

min
{yS ≥0}S⊆[n]

(cid:88)

S⊆[n]

ySf (S) subject to

(cid:88)

S⊆[n]

yS1S = x,

(cid:88)

S⊆[n]

yS = 1,

for all S ⊆ [n],

(dual LP)

whose optimal value is the same as the primal LP. The dual LP is always feasible (see e.g., the Lovász
extension in Section 3.1). However, (cid:101)F does not necessarily agree with f on discrete points in general,
unless the function is convex-extensible (Murota, 1998).

To address this important missing piece, we relax our goal from solving the dual LP to instead seeking
a feasible solution to the dual LP that is an extension of f . Since the dual LP is deﬁned for a ﬁxed
x, a feasible solution must be a function yS = px(S) of x. If px were to be continuous and a.e.
differentiable in x then the value (cid:80)
S px(S)f (S) attained by the dual LP would also be continuous
and a.e. differentiable in x since gradients ﬂow through the coefﬁcients yS = px(S), while f (S) is
treated as a constant in x. This leads us to the following deﬁnition:
Deﬁnition (Scalar SFE). A scalar SFE F of f is deﬁned at a point x ∈ [0, 1]n by coefﬁcients px(S)
such that yS = px(S) is a feasible solution to the dual LP. The extension value is given by
F(x) (cid:44) (cid:88)

px(S)f (S)

S⊆[n]

3

and we require the following properties to hold for all S ⊆ [n]: 1) px(S) is a continuous function of
x and 2) F(1S) = f (S) for all S ⊆ [n].

Efﬁcient evaluation of F requires that px(S) is supported on a small collection of carefully chosen
sets S. This choice is a key inductive bias of the extension, and Section 3.1 gives many examples
with only O(n) non-zero coefﬁcients. Examples include well-known extensions, such as the Lovász
extension, as well as a number of novel extensions, illustrating the versatility of the SFE framework.
Thanks to the constraint (cid:80)
S yS = 1 in the dual LP, scalar SFEs have a natural probabilistic
interpretation. An SFE is deﬁned by a probability distribution px such that fractional points x can
be written as an expectation ES∼px[1S] = x over discrete points using px. The extension itself can
be viewed as arising from exchanging f and the expectation operation: F(x) = ES∼px[f (S)]. This
interpretation is summarized in Figure 1.

Scalar SFEs also enjoy the property of not introducing any spurious minima. That is, the minima of
F coincide with the minima of f up to convex combinations. This property is especially important
when training models of the form f ◦ NN1 (i.e., f is a loss function) since F will guide the network
NN1 towards the same solutions as f .

Proposition 1 (Scalar SFEs have no bad minima). If F is a scalar SFE of f then:

1. minx∈X F(x) = minS⊆[n] f (S)
2. arg minx∈X F(x) ⊆ Hull(cid:0) arg min1S :S⊆[n] f (S)(cid:1)

See Appendix B for proofs.

Obtaining set solutions. Given an architecture F ◦ NN1 and input problem instance G, we often
wish to produce sets as outputs at inference time. To do this, simply compute x = NN1(G), and
select the set S in suppS{px(S)} with the smallest value f (S). This can be done efﬁciently if, as is
typically the case, the cardinality of suppS{px(S)} is small.

3.1 Constructing Scalar Set Function Extensions

A key characteristic of scalar SFEs is that there are many potential extensions of any given f . In
this section, we provide examples of scalar SFEs, illustrating the capacity of the SFE framework for
building knowledge about f into the extension. See Appendix C for all proofs and further discussion.

Lovász extension. Re-indexing the coordinates of x so that x1 ≥ x2 . . . ≥ xn, we deﬁne px to
be supported on the sets S1 ⊆ S2 ⊆ · · · ⊆ Sn with Si = {1, 2, . . . , i} for i = 1, 2, . . . , n. The
coefﬁcient are deﬁned as ySi = px(Si) := xi − xi+1 and px(S) = 0 for all other sets. The resulting
Lovász extension—known as the Choquet integral in decision theory (Choquet, 1954; Marichal,
2000)—is a key tool in combinatorial optimization due to a seminal result: the Lovász extension is
convex if and only if f is submodular (Lovász, 1983), implying that submodular minimization can be
solved in polynomial-time (Grötschel et al., 1981).
Bounded cardinality Lovász extension. A collection {Si}n
i=1 of subsets of [n] can be encoded in
an n × n matrix S ∈ {0, 1}n×n whose ith column is 1Si. In this notation, the dual LP constraint
(cid:80)
S⊆[n] yS1S = x can be written as Sp = x, where the ith coordinate of p deﬁnes px(Si). The
bounded cardinality extension generalizes the Lovász extension to focus only on sets of cardinality at
most k ≤ n. Again, re-index x so that x1 ≥ x2 . . . ≥ xn. Use the ﬁrst k sets S1 ⊆ S2 ⊆ · · · ⊆ Sk,
where Si = {1, 2, . . . , i}, to populate the ﬁrst k columns of matrix S. We add further n − k sets:
Sk+i = {j + i | j ∈ Sk} for i = 1, . . . , n − k, to ﬁll the rest of S. Finally, px(Si) can be analytically
calculated from p = S−1x, where S is invertible since it is a Toeplitz banded upper triangular matrix.

Permutations and involutory extensions. We use the same S, p notation. Let S be an elementary
permutation matrix. Then it is involutory, i.e., SS = I, and we may easily determine p = Sx given
S and x. Note that px(Si) = pi must be non-negative since x and S are non-negative entry-wise.
Finally, restricting x to the n-dimensional Simplex guarantees that (cid:107)p(cid:107)1 ≤ 1, which ensures px is a
probability distribution (any remaining mass is placed on the empty set). The extension property can
be guaranteed on singleton sets as long as the chosen permutation admits a ﬁxed point at the argmax
of x. Any elementary permutation matrix S with such a ﬁxed point yields a valid SFE.

4

Singleton extension. Consider a set function f for which f (S) = ∞ unless S has cardinality one.
To ensure F is ﬁnite valued, px must be supported only on the sets Si = {i}, i = 1, . . . , n. Assuming
x is sorted so that x1 ≥ x2 . . . ≥ xn, deﬁne px(Si) = xi − xi+1. It is shown in Appendix C that this
deﬁnes a scalar SFE, except for the dual LP feasibility. However, when using F as a loss function,
minimization drives x towards the minima minx F(x) which are dual feasible. So dual infeasibility
is benign in this instance and we approach the feasible set from the outside.

Multilinear extension. The multilinear extension, widely used in combinatorial optimization (Cali-
nescu et al., 2011), is supported on all sets with coefﬁcients px(S) = (cid:81)
i /∈S(1 − xi), the
product distribution. In general, evaluating the multilinear extension exactly requires 2n calls to f ,
but for several interesting set functions, e.g., graph cut, set cover, and facility location, it can be
computed efﬁciently in (cid:101)O(n2) time (Iyer et al., 2014).

i∈S xi

(cid:81)

4 Neural Set Function Extensions

This section builds on the scalar SFE framework—where each item i in the ground set [n] is
represented by a single scalar—to develop extensions that use high-dimensional embeddings to avoid
introducing low-dimensional bottlenecks into neural network architectures. The core motivation that
lifting problems into higher dimensions can make them easier is not unique to deep learning. For
instance, it also underlies kernel methods (Shawe-Taylor et al., 2004) and the lift-and-project method
for integer programming (Lovász & Schrijver, 1991).

Our method takes inspiration from prior successes of semi-deﬁnite programming for combinatorial
optimization (Goemans & Williamson, 1995) by extending onto X = Sn
+, the set of n × n positive
semi-deﬁnite (PSD) matrices. With this domain, each item is represented by a vector, not a scalar.

4.1 Lifting Set Function Extensions to Higher Dimensions

We embed sets into Sn
translate the linear programming approach of Section 3 into an analogous SDP formulation:

S . To deﬁne extensions on this matrix domain, we

+ via the map e(S) = 1S1(cid:62)

max
Z(cid:23)0,b∈R

{Tr(X(cid:62)Z) + b} subject to Tr((1S1(cid:62)

T + 1T 1(cid:62)

S )Z) + 2b ≤ 2f (S ∩ T ) for S, T ⊆ [n],

(primal SDP)
where we switch from lower case letters to upper case since we are now using matrices. Next,
we show that this choice of primal SDP is a natural analog of the original LP that provides the
right correspondences between vectors and matrices by proving that primal LP feasible solutions
correspond to primal SDP feasible solutions with the same objective value (see Appendix A for a
discussion on the SDP and its dual). To state the result, note that the embedding e(S) = 1S1(cid:62)
S is a
particular case of the correspondence x ∈ [0, 1]n (cid:55)→
Proposition 2. (Containment of LP in SDP) For any x ∈ [0, 1]n, deﬁne X =
square-root taken entry-wise. Then, for any (z, b) ∈ Rn
where Z = diag(z), is primal SDP feasible and the objective values agree: Tr(X(cid:62)Z) = z(cid:62)x.

x(cid:62) with the
+ × R that is primal LP feasible, the pair (Z, b)

x(cid:62).

√

√

√

√

x

x

Proposition 2 establishes that the primal SDP feasible set is a spectrahedral lift of the positive primal
LP feasible set, i.e., feasible solutions of the primal LP lead to feasible solutions of the primal SDP.
As with scalar SFEs, to deﬁne neural SFEs we consider the dual SDP:

yS,T f (S ∩ T ) subject to X (cid:22)

(cid:88)

yS,T (1S1(cid:62)

T + 1T 1(cid:62)

S ) and

(cid:88)

yS,T = 1

(cid:88)

min
{yS,T }

S,T ⊆[n]

S,T ⊆[n]

S,T ⊆[n]

(dual SDP)
We demonstrate that for suitable X this SDP has feasible solutions via an explicit construction in
Section 4.2. This leads us to deﬁne a neural SFE which, as with scalar SFEs, is given by a feasible
solution to the dual SDP that satisﬁes the extension property whose coefﬁcients are continuous in X:
Deﬁnition (Neural SFE). A neural set function extension of f at a point X ∈ Sn
F(X) (cid:44) (cid:88)

+ is deﬁned as

pX(S, T )f (S ∩ T ),

where yS,T = pX(S, T ) is a feasible solution to the dual SDP and for all S, T ⊆ [n]: 1) pX(S, T ) is
continuous at X and 2) it is valid, i.e., F(1S1(cid:62)

S ) = f (S) for all S ⊆ [n].

S,T ⊆[n]

5

4.2 Constructing Neural Set Function Extensions

We constructed a number of explicit examples of scalar SFEs in Section 3.1. For neural SFEs we
employ a different strategy. Instead of providing individual examples of neural SFEs, we develop a
single recipe for converting any scalar SFE into a corresponding neural SFE. Doing so allows us to
build on the variety of scalar SFEs and provides an additional connection between scalar and neural
SFEs. In Section 5 we show the empirical superiority of neural SFEs over their scalar counterparts.

Our construction is given in the following proposition:
Proposition 3. Let px induce a scalar SFE of f . For X ∈ Sn
eigendecomposition X = (cid:80)n

i and ﬁx

i=1 λixix(cid:62)
n
(cid:88)

pX(S, T ) =

λi pxi(S)pxi (T ) for all S, T ⊆ [n].

+ with distinct eigenvalues, consider the

i=1
Then, pX deﬁnes a neural SFE F at X.

See Appendix D for proof. The continuity of F follows from a variant of the Davis–Kahan theorem
(Yu et al., 2015), which uses the distinct eigenvalue assumption. For efﬁciency, in practice we do not
use all n eigenvectors, and use only the k with largest eigenvalue. This is justiﬁed by Figure 3, which
shows that in practical applications X often has a rapidly decaying spectrum.

Evaluating a neural SFE requires an accessible closed-form expression, the precise form of which de-
pends on the underlying scalar SFE. Further, from the deﬁnition of Neural SFEs we see that if a scalar
SFE is supported on sets with a property that is closed under intersection (e.g., bounded cardinality),
then the supporting sets of the corresponding neural SFE will also inherit that property. This implies
that the neural counterparts of the Lovász, bounded cardinality Lovász, and singleton/permutation
extensions have the same support as their scalar counterparts. An immediate corollary is that we can
easily compute the neural counterpart of the Lovász extension which has a simple closed form:
+ consider the eigendecomposition X = (cid:80)n
Corollary 1. For X ∈ Sn
i . Let pxi be as in the
Lovász extension: pxi (Sij) = xi,j −xi,j+1 with xi sorted so xi,1 ≥ . . . ≥ xi,n and Sij = {1, . . . , j},
with pxi(S) = 0 for all other sets. Then, the neural Lovász extension is:

i=1 λixix(cid:62)

F(X) =

n
(cid:88)

i,j=1

(cid:18)

λipxi(Sij) ·

pxi(Sij) + 2

(cid:19)

pxi(Si(cid:96))

· f (Sij).

(cid:88)

(cid:96):(cid:96)>j

Complexity and obtaining sets as solutions.
In general, the neural SFE relies on all pairwise
intersections S ∩ T of the scalar SFE sets, requiring O(m2) evaluations of f when the scalar SFE
is supported on m sets. However, when the scalar SFE is supported on a family of sets that is
closed under intersection—e.g., the Lovász and singleton extensions—the corresponding neural SFE
requires only O(m) function evaluations. Discrete solutions can be obtained efﬁciently by returning
the best set out of all scalar SFEs pxi.

5 Experiments

We experiment with SFEs as loss functions in neural network pipelines on discrete objectives arising
in combinatorial and vision tasks. For combinatorial optimization, SFEs network training with a
continuous version of the objective without supervision. For supervised image classiﬁcation, they
allow us to directly relax the training error instead of optimizing a proxy like cross entropy.

5.1 Unsupervised Neural Combinatorial Optimization

We begin by evaluating the suitability of neural SFEs for unsupervised learning of neural solvers
for combinatorial optimization problems on graphs. We use the ENZYMES, PROTEINS, IMDB,
MUTAG, and COLLAB datasets from the TUDatasets benchmark (Morris et al., 2020), using a
60/30/10 split for train/test/val. We test on two problems: ﬁnding maximum cliques, and maximum
independent sets. We compare with three neural network based methods. We compare to two common
approaches for backpropogating through discrete functions: the REINFORCE algorithm (Williams,
1992), and the Straight-Through estimator (Bengio et al., 2013). The third is the recently proposed

6

Straight-through (Bengio et al., 2013)
Erd˝os (Karalias & Loukas, 2020)
REINFORCE (Williams, 1992)

Lovász scalar SFE
Lovász neural SFE

Straight-through (Bengio et al., 2013)
Erd˝os (Karalias & Loukas, 2020)
REINFORCE (Williams, 1992)

Lovász scalar SFE
Lovász neural SFE

ENZYMES

PROTEINS

IMDB-Binary

MUTAG

COLLAB

Maximum Clique

0.725±0.268
0.883±0.156
0.751±0.301
0.723±0.272
0.933±0.148

0.722±0.26
0.905±0.133
0.725±0.285
0.778±0.270
0.926±0.165

0.917±0.253
0.936±0.175
0.881±0.240
0.975±0.125
0.961±0.143

0.965±0.162
1.000±0.000
1.000±0.000
0.977±0.125
1.000±0.000

0.856±0.221
0.852±0.212
0.781±0.316
0.855±0.225
0.864±0.205

ENZYMES

PROTEINS

IMDB-Binary

MUTAG

COLLAB

Maximum Independent Set

0.505±0.244
0.821±0.124
0.617±0.214
0.311±0.289
0.775±0.155

0.430±0.252
0.903±0.114
0.579±0.340
0.462±0.260
0.729±0.205

0.701±0.252
0.515±0.310
0.899±0.275
0.716±0.269
0.679±0.287

0.721±0.257
0.939±0.069
0.744±0.121
0.737±0.154
0.854±0.132

0.331±0.260
0.886±0.198
0.053±0.164
0.302±0.238
0.392±0.253

Table 1: Unsupervised neural combinatorial optimization: Approximation ratios for combinato-
rial problems. Values closer to 1 are better (↑). Neural SFEs are competitive with other methods, and
consistently improve over vector SFEs.

Figure 2: k-clique constraint satisfaction: higher F1-score is better. The k-bounded cardinality
Lovasz extension is better aligned with the task and signiﬁcantly improves over the Lovász extension.

probabilistic penalty relaxation (Karalias & Loukas, 2020) for combinatorial optimization objectives.
All methods use the same GNN backbone, comprising a single GAT layer (Veliˇckovi´c et al., 2018)
followed by multiple gated graph convolution layers Li et al. (2015).

In all cases, given an input graph G = (V, E) with |V | = n nodes, a GNN produces an embedding
for each node: X ∈ Rn×d. For scalar SFEs d = 1, while for neural SFEs we consider XX(cid:62) in
order to produce an n × n PSD matrix, which is passed as input to the SFE F. The set function
f used is problem dependent, which we discuss below. Finally, see Appendix F for training and
hyper-parameter optimization details, and Appendix E for details on data, hardware, and software.

Maximum Clique. A set S ⊆ V is a clique of G = (V, E) if (i, j) ∈ E for all i, j ∈ S. The
MaxClique problem is to ﬁnd the largest set S that is a clique: i.e., f (S) = |S| · 1{S a clique}.

Maximum Independent Set (MIS). A set S ⊆ V is an independent set of G = (V, E) if (i, j) /∈ E
for all i, j ∈ S. The goal is to ﬁnd the largest S in the graph that is independent, i.e., f (S) =
|S| · 1{S an ind. set}. MIS differs signiﬁcantly from MaxClique due to its high heterophily.
Results. Table 1 displays the mean and standard deviation of the approximation ratio f (S)/f (S∗) of
the solver solution S and an optimal S∗ on the test set graphs. The neural Lova´sz extension outper-
forms its scalar counterpart in 8 out of 10 cases, often by signiﬁcant margins, for instance improving
a score of 0.778 on PROTEINS MaxClique to 0.926. The neural SFE proved effective at boosting
poor scalar SFE performance, e.g., 0.311 on ENZYMES MIS, to the competitive performance of
0.775. Neural Lova´sz outperformed or equalled REINFORCE and straight-through in 9 out of 10
cases, and the method of Karalias & Loukas (2020) in 6 out of 10.

5.2 Constraint Satisfaction Problems

Constraint satisfaction problems ask if there exists a set satisfying a given set of conditions (Kumar,
1992; Cappart et al., 2021b). In this section, we apply SFEs to the k-clique problem: given a graph,
determine if it contains a clique of size k or more. We test on the ENZYMES and PROTEINS
datasets. Since satisﬁability is a binary classiﬁcation problem we evaluate using F1 score.

7

k−Lov́aszLov́aszREINFORCEPROTEINS (k=3)0.40.60.81.0F1-scorek−Lov́aszLov́aszREINFORCEENZYMES (k=3)0.40.60.81.0F1-scorek−Lov́aszLov́aszREINFORCEPROTEINS (k=4)0.30.40.50.60.7F1-scorek−Lov́aszLov́aszREINFORCEENZYMES (k=4)0.30.40.50.60.7F1-scoreFigure 3: Left: Runtime and performance of neural SFEs on MaxClique using different numbers of
eigenvectors. Right: Histogram of spectrum of matrix X, outputted by a GNN trained on MaxClique.

Figure 5: Top: CIFAR10. Bottom: SVHN. The singleton extension loss (left) is the only loss that
approximates the true non-differentiable training error at the same numerical scale.

Results. Figure 2 shows that by speciﬁcally searching over sets of size k using the cardinality
constrained Lovász extension from Section 3.1, we signiﬁcantly improve performance compared to
the Lovász extension, and REINFORCE. This illustrates the value of SFEs in allowing task-dependent
considerations (in this case a cardinality constraint) to be built into extension design.

5.3 Training Error as a Classiﬁcation Objective

(cid:80)n

During training the performance of a classiﬁer
h is typically assessed using the training error
1
i=1 1{yi (cid:54)= h(xi)}. Since training error itself is
n
non-differentiable, it is standard to train h to optimize
a differentiable surrogate such as the cross-entropy
loss. Here we offer an alternative training method by
continuously extending the non-differentiable map-
ping ˆy (cid:55)→ 1{yi (cid:54)= ˆy}. This map is a set function
deﬁned on single item sets, so we use the singleton
extension (deﬁnition in Section 3.1). Our goal is to
demonstrate that the resulting differentiable loss func-
tion closely tracks the training error, and can be used
to minimize it. We do not focus on test time gener-
alization. Figure 6 shows the results. The singleton
extension loss (left plot) closely tracks the true train-
ing error at the same numerical scale, unlike other common loss functions (see Appendix G for setup
details). While we leave further consideration to future work, training error extensions may be useful
for model calibration (Kennedy & O’Hagan, 2001) and uncertainty estimation (Abdar et al., 2021).

Figure 4: Neural SFEs outperform a naive
alternative high-dimensional extension.

5.4 Ablations

Number of Eigenvectors. Figure 3 compares the runtime and performance of neural SFEs using only
the top-k eigenvectors from the eigendecomposition X = (cid:80)n
i with k ∈ {1, 2, 3, 4, 5, 6}
on the maximum clique problem. For both ENZYMES and PROTEINS, performance increases
with k—easily outperforming scalar SFEs and REINFORCE—until saturation around k = 4, while

i=1 λixix(cid:62)

8

123456Number of Eigenvectors0.60.81.0Approx. RatioENZYMESApprox. RatioNeural LovaszLovaszREINFORCE01020sec / Epochsec / EpochNeural LovaszLovaszREINFORCE10−510−410−310−210−1100Eigenvalue SizeFrequencyENZYMES123456Number of Eigenvectors0.60.81.0Approx. RatioPROTEINSApprox. RatioNeural LovaszLovaszREINFORCE020sec / Epochsec / EpochNeural LovaszLovaszREINFORCE10−510−410−310−210−1100Eigenvalue SizeFrequencyPROTEINS050100Epochs0.00.20.40.60.81.0Singleton Extension050100Epochs0.00.20.40.60.81.0Cross Entropy050100Epochs0.00.20.40.60.81.0Exponential050100Epochs0.00.20.40.60.81.0MSE050100Epochs0.00.20.40.60.81.0Hingetrain losstrain error050100Epochs0.00.20.40.60.81.0Singleton Extension050100Epochs0.00.20.40.60.81.0Cross Entropy050100Epochs0.00.20.40.60.81.0Exponential050100Epochs0.00.20.40.60.81.0MSE050100Epochs0.00.20.40.60.81.0Hingetrain losstrain error0.00.20.40.60.81.0MaxCliqueNeural Lov́aszLov́asz (multiple)Lov́asz0.00.20.40.60.81.00.00.20.40.60.81.0ENZYMESMIS0.00.20.40.60.81.0PROTEINSruntime grows linearly with k. Histograms of the eigenvalues produced by trained networks show a
rapid decay in the spectrum, suggesting that the smaller eigenvalues have little effect on F.

Comparison to Naive High-Dimensional Extension. We compare neural SFEs to a naive high-
dimensional alternative which, given an n×d matrix X simply computes a scalar SFE on each column
independently and sums them up. This naive function design is not an extension, and the dependence
on the d dimensions is linearly separable, in contrast to the complex non-linear interactions between
columns of X in neural SFEs. Figure 4 shows that this naive extension, whilst improving over
one-dimensional extensions, performs considerably worse than neural SFEs.

6 Related Work

Lifting to High-Dimensions. Neural SFEs are heavily inspired by the Goemans-Williamson (Goe-
mans & Williamson, 1995) algorithm and other SDP techniques (Iguchi et al., 2015), which lift
problems onto higher dimensional spaces, solve them, and then project back down. Our approach
to lifting set functions to high dimensions is motivated by the algorithmic alignment principle (Xu
et al., 2019): neural networks whose computations emulate classical algorithms often generalize
better with improved sample complexity (Yan et al., 2020; Li et al., 2020; Xu et al., 2019). Emulating
algorithmic and logical operations is the focus of Neural Algorithmic Reasoning (Veliˇckovi´c et al.,
2019; Dudzik & Veliˇckovi´c, 2022; Deac et al., 2021) and work on knowledge graphs (Hamilton et al.,
2018; Ren et al., 2019; Arakelyan et al., 2020), which also emphasize operating in higher dimensions.

Extensions. Scalar SFEs use an LP formulation of the convex closure (El Halabi, 2018, Def. 20), a
classical approach for deﬁning convex extensions of discrete functions (Murota, 1998, Eq. 3.57). See
Bach (2019) for a study of extensions of submodular functions. The constraints of our dual LP arise
in contexts from global optimization (Tawarmalani & Sahinidis, 2002) to barycentric approximation
and interpolation schemes in computer graphics (Guessab, 2013; Hormann, 2014). Convex extensions
have also been used for combinatorial penalties with structured sparsity (Obozinski & Bach, 2012,
2016), and general minimization algorithms for set functions (El Halabi & Jegelka, 2020).

Stochastic gradient estimation. SFEs produce gradients for f requiring only black-box access.
There is a wide literature on sampling-based approaches to gradient estimation, for instance the
REINFORCE algorithm (Williams, 1992) (i.e., score function estimator). However, sampling
introduces noise which can cause unstable training and convergence issues, prompting signiﬁcant
study of variance reducing control variates (Gu et al., 2017; Liu et al., 2018; Grathwohl et al., 2018;
Wu et al., 2018; Cheng et al., 2020). SFEs avoid sampling (and noise) all-together, instead providing
a deterministic procedure for obtaining gradients. A closely related, yet distinct, task is to produce
gradients through sampling operations, which introduce non-differentiable nodes in neural network
computation graphs. The Straight-Through Estimator (Bengio et al., 2013), arguably the simplest
solution, treats sampling as the identity map in the backward pass, yielding biased gradient estimates.
The Gumbel-Softmax trick (Maddison et al., 2017; Jang et al., 2017), provides an alternative method
for categorical distributions (also beneﬁting from variance reduction (Paulus et al., 2020a)), and has
recently been generalized to more complex distributions (Paulus et al., 2020b). These techniques
relax a discrete distribution into a continuous one, assuming access to a continuous loss function.
SFEs are complementary to this setup, addressing the problem of designing continuous extensions.

Differentiating through Convex Programs. Recent years have seen a surge of interest in neural
networks with differentiable solvers, e.g., LP solvers, as layers (Agrawal et al., 2019; Amos & Kolter,
2017; Paulus et al., 2021; Poganˇci´c et al., 2019; Wang et al., 2019), including via sampling (Niepert
et al., 2021). Whilst sharing the algorithmic alignment motivation of SFEs, the convex programming
connection is mostly cosmetic: these works directly embed solvers into network architectures, while
SFEs use convex programs as an analytical tool, without requiring solver access.

Finally, our experimental setup largely follows recent work on unsupervised neural combinatorial
optimization (Karalias & Loukas, 2020; Schuetz et al., 2022; Xu et al., 2020; Toenshoff et al.,
2021; Amizadeh et al., 2018), where continuous relaxations of discrete objectives are utilized. For
background on neural combinatorial optimization, we refer the reader to the surveys (Bengio et al.,
2021; Cappart et al., 2021a; Mazyavkina et al., 2021).

9

7 Conclusion

We introduced Neural Set Function Extensions, a framework that enables evaluating set functions on
continuous and high dimensional representations. We showed how to construct such extensions and
demonstrated their viability in a range of tasks including combinatorial optimization and image clas-
siﬁcation. Notably, neural extensions deliver good results and improve over their scalar counterparts,
further afﬁrming the beneﬁts of problem-solving in high dimensions.

10

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes] All claims made are backed. up either empirically or
theoretically.

(b) Did you describe the limitations of your work? [Yes] See Appendix I.1.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See

Appendix I.2.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes] We have read the guidelines, and conﬁrmed that our paper conforms.

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] All theoretical

result are stated exactly.

(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix for

proofs.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See anonymized
URL for all code.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See Appendix F and Appendix G

(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] Except in cases where HPO was run. In these cases we
report the test performance of the. model with best validation performance.

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We cite all creators

of. existing assets, either in the main paper or appendix.

(b) Did you mention the license of the assets? [Yes] Yes, see Appendix E.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]

We provide anonymized code. Open source code will be released after review.

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [Yes] See Appendix E.

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [Yes] See Appendix E.
5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A] No crowdsourcing or human subjects used.

(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A] No crowdsourcing or human subjects
used.

(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A] No crowdsourcing or human subjects used.

11

A Optimization programs: extended discussion

In this section, we provide an extended discussion of the key components of our LP and SDP
formulations and the relationships between them. Apart from supplying derivations, another goal of
this section is to illustrate that there is in fact ﬂexibility in the exact choice of formulation for the LP
(and consequently the SDP). We provide details on possible variations as part of this discussion as a
guide to users who may wish to adapt the SFE framework.

A.1 LP formulation: Derivation of the dual.

First, recall that our primal LP is deﬁned as

max
z,b∈Rn×R

{x(cid:62)z + b} subject to 1(cid:62)

S z + b ≤ f (S) for all S ⊆ [n].

The dual is

min
{yS ≥0}S⊆[n]

(cid:88)

S⊆[n]

ySf (S) subject to

(cid:88)

S⊆[n]

yS1S = x,

(cid:88)

S⊆[n]

yS = 1,

for all S ⊆ [n].

In order to standardize the derivation, we ﬁrst convert the primal maximization problem into mini-
mization (this will be undone at the end of the derivation). We have

min
z,b∈Rn×R

{−x(cid:62)z − b} subject to 1(cid:62)

S z + b ≤ f (S) for all S ⊆ [n].

The Lagrangian is

L(z, yS, b)
yS ≥0

= −x(cid:62)z − b −

(cid:88)

S⊆[n]

yS(f (S) − 1(cid:62)

S z − b)

(cid:88)

= −

ySf (S) + (

S⊆[n]

(cid:88)

S⊆[n]

yS1(cid:62)

S − x(cid:62))z + b(

(cid:88)

yS − 1)

S⊆[n]

The optimal solution p∗ to the primal problem is then

p∗ = min
z,b
= max
yS ≥0

max
yS ≥0
min
z,b

L(z, yS, b)

L(z, yS, b)

= d∗,

where d∗ is the optimal solution to the dual. From the Lagrangian,

min
z,b

L(z, yS, b) =

(cid:40)

S⊆[n] ySf (S), if (cid:80)

− (cid:80)
−∞, otherwise.

S⊆[n] yS1S = x and (cid:80)

(strong duality)

S⊆[n] yS = 1,

Thus, we can write the dual problem as

d∗ = max
yS ≥0

−

(cid:88)

S⊆[n]

ySf (S) subject to

(cid:88)

S⊆[n]

yS1S = x and

(cid:88)

S⊆[n]

yS = 1.

Our proposed dual formulation is then obtained by switching from maximization to minimization
and negating the objective. It can also be veriﬁed that by taking the dual of our dual, the primal is
recovered (see El Halabi (2018, Def. 20) for the derivation).

A.2 Connections to submodularity, related linear programs, and possible alternatives.

Our LP formulation depends on a linear program known to correspond to the convex closure (Murota,
1998, Eq. 3.57) (convex envelope) of a discrete function. Some readers may recognize the formal
similarities of this formulation with the one used to deﬁne the Lovász extension (Bilmes, 2022).
Namely, for x ∈ Rn we can deﬁne the Lovász Extension as

F(x) = max
z∈Bf

x(cid:62)z,

12

where the feasible set, known as the base polytope of a submodular function, is deﬁned as Bf = {z ∈
Rn : z(cid:62)1S ≤ f (S) S ⊂ [n], and z(cid:62)1S = f (S) when S = [n]}. Another option is to consider
x ∈ Rn

+, then the Lovász extension is given by

F(x) = max
z∈Pf

x(cid:62)z,

where Pf is the submodular polyhedron as deﬁned in our original primal LP. The subtle differences
between those formulations lead to differences in the respective dual formulations. In principle, those
formulations can be just as easily used to deﬁne set function extensions. Overall, there are three key
properties to consider when deﬁning a suitable LP:

• The constraints of the primal.
• The domain of the primal variables z, b and the cost x.

• The properties of the function being extended.

We provide a few example cases for different choices of the above properties:

• Adding the constraint z(cid:62)1S = f (S) when S = [n] leads to y[n] ∈ Rn for the dual. This
implies that the coefﬁcients cannot be interpreted as probabilities in general which is
what provides the guarantee that the extension will not introduce any spurious minima.
(cid:80)

S⊆[n] yS = 1 is just an afﬁne hull constraint in that case.

• For b = 0, the constraint (cid:80)

S⊆[n] yS = 1 is not imposed in the dual and the probabilistic
interpretation of the extension cannot be guaranteed. Examples that do not rely on this
constraint include the homogeneous convex envelope (El Halabi et al., 2018) and the Lovász
extension as presented above. However, even for b = 0, from the deﬁnition of the Lovász
extension it is easy to see that it retains the probabilistic interpretation when x ∈ [0, 1].

(cid:84) Rn

+ and let x ∈ Rn

• Consider a feasible set deﬁned by Pf

+. If the function f is submodular,
non-decreasing and normalized so that f (∅) = 0 (e.g., the rank function of a matroid), then
the feasible set is called polymatroid and f is a polymatroid function. Again, in that case the
Lovász extension achieves the best objective value (Schrijver et al., 2003, Eq. 44.32). In that
S⊆[n] yS1S = x of the dual is relaxed to (cid:80)
case, the constraint (cid:80)
S⊆[n] yS1S ≥ x. This
feasible set of the dual will allow for more ﬂexible deﬁnitions of an extension but it comes
at the cost of generality. For instance, for a submodular function that is not non-decreasing,
one cannot obtain the Lovász extension as a feasible solution to the primal LP, and the
solutions to this LP will not be the convex envelope in general.

A.3 SDP formulation: Geometric intuition and deriving the dual.

In order to motivate the SDP formulation, ﬁrst we have to identify the essential ingredients of the LP
formulation. First, the constraint (cid:80)
S⊆[n] yS1S = x captures the simple idea that each continuous
point is expressed as a combination of discrete ones, which is at the core of our extensions. Then,
ensuring that the continuous point lies in the convex hull of those discrete points confers additional
beneﬁts w.r.t. optimization and offers a probabilistic perspective.

Consider the following example. The Lovász extension identiﬁes each continuous point in the
hypercube with a simplex. Then the continuous point is viewed as an expectation over a distribution
supported on the simplex corners. The set function at the continuous point is the expected value of
the function over the same distribution on those corners. It is clear that the construction depends on
being able to identify a small convex set of discrete vectors that can express the continuous one.

A similar concept can identiﬁed in higher dimensions, where our goal will be again to identify a small
convex set of matrices that can express a matrix of continuous embeddings. A geometric analogue to
the simplex in the space of matrices is the spectraplex On, a spectrahedron that is the set of PSD
n × n matrices with trace one (Blekherman et al., 2012), i.e.,

On = {X ∈ Sn

+ : Tr(X = 1)}.

Intuitively, the PSD trace one property means that the eigenvalues can be interpreted as probabilities,
and each matrix is a convex combination (through the eigenvector expansion) of rank one matrices

13

vv(cid:62) formed by the corresponding eigenvectors. The goal is then to express each vv(cid:62) as a combina-
tion of matrices that correspond to sets, which is what our dual SDP is achieving through the linear
matrix inequality constraint X (cid:22) (cid:80)

S,T ⊆[n] yS,T (1S1(cid:62)

T + 1T 1(cid:62)

S ).

The above considerations set the stage for a transition from linear programming to semideﬁnite
programming, where the feasible sets are spectrahedra. Our SDP formulation attempts to capture the
intuition described in the previous paragraphs while also maintaining formal connections to the LP
by showing that feasible LP regions correspond to feasible SDP regions by simply projecting the LP
regions on the space of diagonal matrices (see Proposition 2).

Derivation of the dual. Recall that our primal SDP is deﬁned as

max
Z(cid:23)0,b∈R

{Tr(X(cid:62)Z) + b} subject to Tr((1S1(cid:62)

T + 1T 1(cid:62)

S )Z) + 2b ≤ 2f (S ∩ T ) for S, T ⊆ [n].

We will show that the dual is

min
{yS,T ≥0}

(cid:88)

S,⊆[n]

yS,T f (S ∩ T ) subject to X (cid:22)

(cid:88)

S,T ⊆[n]

yS,T (1S1(cid:62)

T + 1T 1(cid:62)

S ) and

(cid:88)

S,T ⊆[n]

yS,T = 1.

As before, we convert the primal to a minimization problem:
{−Tr(X(cid:62)Z) − b} subject to Tr((1S1(cid:62)

T + 1T 1(cid:62)

max
Z(cid:23)0,b∈R

S )Z) + 2b ≤ 2f (S ∩ T ) for S, T ⊆ [n].

First, we will standardize the formulation by converting the inequality constraints into equality
constraints. This can be achieved by adding a positive slack variable dS,T to each constraint such that

Tr((1S1(cid:62)

T + 1T 1(cid:62)

S )Z) + 2b + dS,T = 2f (S ∩ T ).

In matrix notation this is done by introducing the positive diagonal slack matrix D to the decision
variable Z, and extending the symmetric matrices in each constraint
(cid:20)(1S1(cid:62)

T + 1T 1(cid:62)
S )

(cid:21)

(cid:21)

Z(cid:48) =

(cid:20)Z 0
0 D

, X(cid:48) =

(cid:21)
(cid:20)X 0
0
0

, A(cid:48)

S,T =

0
I

.

0

Using this reformulation, we obtain an equivalent SDP in standard form:

max
Z(cid:48)(cid:23)0,b∈R

{−Tr(X(cid:48)(cid:62)Z(cid:48)) − b} subject to Tr(A(cid:48)

S,T Z(cid:48)) + 2b = 2f (S ∩ T ) for S, T ⊆ [n].

Next, we form the Lagrangian which features a decision variable yS,T for each inequality, and a dual
matrix variable Λ. We have
L(Z(cid:48), b, yS,T , Λ) = −Tr(X(cid:48)(cid:62)Z(cid:48)) − b −

2f (S ∩ T ) − Tr(A(cid:48)

− Tr(ΛZ(cid:48))

S,T Z(cid:48)) − 2b

yS,T

(cid:88)

(cid:17)

(cid:16)



= Tr

(

(cid:88)

S,T ⊆[n]

S,T ⊆[n]



yS,T A(cid:48)

S,T ) − X(cid:48) − ΛZ(cid:48)

 + b(

(cid:88)

2yS,T − 1) −

(cid:88)

yS,T 2f (S ∩ T )

S,T ⊆[n]

S,T ⊆[n]

For the solution to the primal p∗, we have
p∗ = min
Z(cid:48),b

L(Z(cid:48), b, yS,T , Λ)

max
Λ,yS,T
min
Z(cid:48),b

≥ max
Λ,yS,T

L(Z(cid:48), b, yS,T , Λ)

(weak duality)

For our Lagrangian we have the dual function

= d∗.

L(Z(cid:48), b, yS,T , Λ) =

min
Z(cid:48),b

(cid:40)

0, if Λ (cid:23) 0,
−∞, otherwise .

14

Thus, the dual function min
Z(cid:48),b

L(Z(cid:48), b, yS,T , Λ) takes non-inﬁnite values under the conditions

(cid:88)

(

S,T ⊆[n]

yS,T A(cid:48)

S,T ) − X(cid:48) − Λ = 0,

Λ (cid:23) 0,

2yS,T − 1 = 0.

(cid:88)

and

S,T ⊆[n]

The ﬁrst two conditions imply the linear matrix inequality (LMI)
S,T − X(cid:48) (cid:23) 0.

yS,T A(cid:48)

(cid:88)

(Λ (cid:23) 0)

From the deﬁnition of A(cid:48)
variables yS,T . Combined with the conditions above, we arrive at the constraints of the dual

S,T we know that its additional diagonal entries will correspond to the

S,T ⊆[n]

yS,T ≥ 0,

(cid:88)

S,T ⊆[n]

yS,T (1S1(cid:62)

T + 1T 1(cid:62)

S ) (cid:23) X,

(cid:88)

S,T ⊆[n]

2yS,T = 1.

This leads us to the dual formulation

max
yS,T ≥0

−

(cid:88)

S,T ⊆[n]

yS,T 2f (S ∩ T ) subject to

(cid:88)

S,T ⊆[n]

yS,T (1S1(cid:62)

T + 1T 1(cid:62)

S ) (cid:23) X and

(cid:88)

S,T ⊆[n]

2yS,T = 1.

Then, we can obtain our original dual by switching to minimization and negating the objective. For
consistency in the presentation we omit the factor of two in the objective as it does not affect the
solutions. Similarly we drop the factor of two in the equality constraint since the sum to unity can be
secured easily through a sufﬁciently large coefﬁcient for the empty set which leaves the objective and
the LMI constraint unaffected.

B Scalar Set Function Extensions Have No Bad Minima

In this section we re-state and prove the results from Section 3. The ﬁrst result concerns the minima
of F, showing that the minimum value is the same as that of f , and no additional minima are added
(besides convex combinations of discrete minimizers). These properties are especially desirable when
using an extension F as a loss function (see Section 5) since it is important that F drive the neural
network NN1 towards producing discrete 1S outputs.
Proposition 4 (Scalar SFEs have no bad minima). If F is a scalar SFE of f then:

1. minx∈X F(x) = minS⊆[n] f (S)
2. arg minx∈X F(x) ⊆ Hull(cid:0) arg min1S :S⊆[n] f (S)(cid:1)

Proof. The inequality minx∈X F(x) ≤ minS⊆[n] f (S) automatically holds since minS⊆[n] f (S) =
min1S :S⊆[n] F(1S), and {1S : S ⊆ [n]} ⊆ X . So it remains to show the reverse. Indeed, letting
x ∈ X be an arbitrary point we have,

F(x) = ES∼px[f (S)]
(cid:88)

=

px(S) · f (S)

S⊆[n]
(cid:88)

≥

S⊆[n]
= min
S⊆[n]

px(S) · min
S⊆[n]

f (S)

f (S)

15

where the last equality simply uses the fact that (cid:80)

S⊆[n] px(S) = 1. This proves the ﬁrst claim.

To prove the second claim, suppose that x minimizes F(x) over x ∈ X . This implies that the
inequality in the above derivation must be tight, which is true if and only if

px(S) · f (S) = px(S) · min
S⊆[n]

f (S)

for all S ⊆ [n].

S⊆[n] px(S) · 1S = (cid:80)

For a given S, this implies that either px(S) = 0 or f (S) = minS⊆[n] f (S). Since x = Epx [1S] =
(cid:80)
S:px(S)>0 px(S) · 1S. This is precisely a convex combination of points 1S
for which f (S) = minS⊆[n] f (S). Since F is a convex combination of exactly this set of points 1S,
we have the second claim.

C Examples of Vector Set Function Extensions

This section re-deﬁnes the vector SFEs given in Section 3.1, and prove that they satisfy the deﬁnition
of an SFEs. One of the conditions we must check is that F is continuous. A sufﬁcient condition for
continuity (and almost everywhere differentiability) that we shall use for a number of constructions
is to show that F is Lipschitz. A very simple computation shows that it sufﬁces to show that
x ∈ X (cid:55)→ px(S) is Lipschitz continuous.
Lemma 1. If the mapping x ∈ [0, 1]n (cid:55)→ px(S) is Lipschitz continuous and f (S) is ﬁnite for all S
in the support of px, then F is also Lipschitz continuous. In particular, F is continuous and almost
everywhere differentiable.

Proof. The Lipschitz continuity of F(x) follows directly from deﬁnition:

(cid:12)F(x) − F(x(cid:48))(cid:12)
(cid:12)

(cid:12) =

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

S⊆[n]

(cid:88)

S⊆[n]

px(S) · f (S) −

px(cid:48)(S) · f (S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

S⊆[n]

(cid:12)
(cid:12)
(cid:0)px(S) − px(cid:48)(S)(cid:1) · f (S)
(cid:12)
(cid:12)

≤

(cid:32)

(cid:33)

2kL max
S⊆[n]

f (S)

· (cid:107)x − x(cid:48)(cid:107),

where L is the maximum Lipschitz constant of x (cid:55)→ px(S) over any S in the support of px, and k is
the maximal cardinality of the support of any px.

In general k can be trivially bounded by 2n, so F is always Lipschitz. However in may cases the
cardinality of the support of any px is much smaller than 2n, leading too a smaller Lipschitz constant.
For instance, k = n in the case of the Lovász extension.

C.1 Lovász extension.

Recall the deﬁnition: x is sorted so that x1 ≥ x2 ≥ . . . ≥ xd. Then the Lovász extension corresponds
to taking Si = {1, . . . , i}, and letting px(Si) = xi − xi+1, the non-negative increments of x (where
recall we take xn+1 = 0). All other sets have zero probability. For convenience, we introduce the
shorthand notation ai = px(Si) = xi − xi+1

Feasibility. Clearly all ai = xi − xi+1 ≥ 0, and (cid:80)n
i=1(xi − xi+1) = x1 ≤ 1. Any
remaining probability mass is assigned to the empty set: px(∅) = 1 − x1, which contributes nothing
to the extension F since f (∅) = 0 by assumption. All that remains is to check that

i=1 ai = (cid:80)n

n
(cid:88)

i=1

px(Si) · 1Si = x.

For a given k ∈ [n], note that the only sets Si with non-zero kth coordinate are S1, . . . , Sk, and in
all cases (1Si )k = 1. So the kth coordinate is precisely (cid:80)k
i=1(xi − xi+1) = xk,
yielding the desired formula.

i=1 px(Si) = (cid:80)k

16

Extension. Consider an arbitrary S ⊆ [n]. Since we assume x = 1S is sorted, it has the form
, 0, 0, . . . 0)(cid:62). Therefore, for each j < k we have aj = xj − xj+1 = 1 − 1 = 0
1S = (1, 1, . . . , 1
(cid:125)
(cid:124)

(cid:123)(cid:122)
k times

and for each j > k we have aj = xj − xj+1 = 0 − 0 = 0. The only non-zero probability is
ak = xk − xk+1 = 1 − 0 = 1. So,

F(1S) =

n
(cid:88)

i=1

aif (Si) =

(cid:88)

i:i(cid:54)=k

aif (Si) + akf (Sk) = 0 + 1 · f (Sk) = f (S)

where the the ﬁnal equality follows since by deﬁnition Sk corresponds exactly to the vector
(1, 1, . . . , 1
(cid:124)
(cid:123)(cid:122)
(cid:125)
k times

, 0, 0, . . . 0)(cid:62) = 1S and so Sk = S.

Continuity. The Lovász is a well-known extension, whose properties have been carefully studied.
In particular it is well known to be a Lipschitz function Bach (2019). However, for completeness we
provide a simple proof here nonetheless.
Lemma 2. Let px be as deﬁned for the Lovász extension. Then x (cid:55)→ px(S) is Lipschitz for all
S ⊆ [n].

Proof. First note that px is piecewise linear, with one piece per possible ordering x1 ≥ x2 ≥ . . . ≥ xn
(so n! pieces in total). Within the interior of each piece px is linear, and therefore Lipschitz. So
in order to prove global Lipschitzness, it sufﬁces to show that px is continuous at the boundaries
between pieces (the Lipschitz constant is then the maximum of the Lipschitz constants for each linear
piece).

Now consider a point x with x1 ≥ . . . ≥ xi = xi+1 ≥ . . . ≥ xn. Consider the perturbed point
xδ = x − δei with δ > 0, and ei denoting the ith standard basis vector. To prove continuity of px it
sufﬁces to show that for any S ∈ Ω we have pxδ (S) → px(S) as δ → 0+.
There are two sets in the support of px whose probabilities are different under pxδ , namely: Si =
{1, . . . , i} and Si+1 = {1, . . . , i, i + 1}. Similarly, there are two sets in the support of pxδ whose
probabilities are different under px, namely: S(cid:48)
i+1 = {1, . . . , i, i+1} =
Si+1. So it sufﬁces to show the convergence pxδ (S) → px(S) for these four S. Consider ﬁrst Si:
(cid:12) = (cid:12)

i = {1, . . . , i−1, i+1} and S(cid:48)

(cid:12)pxδ (Si) − px(Si)(cid:12)
(cid:12)

(cid:12)0 − (xi − xi+1)(cid:12)

(cid:12) = 0

where the ﬁnal equality uses the fact that xi = xi+1. Next consider Si+1 = S(cid:48)
(cid:12)
(cid:12)pxδ (Si+1) − px(Si+1)(cid:12)
Finally, we consider S(cid:48)
i:
(cid:12)
(cid:12)pxδ (S(cid:48)

i+2) − (xi+1 − xi+2)(cid:12)

i+1 − x(cid:48)

(cid:12) = (cid:12)

(cid:12) = (cid:12)

(cid:12)(x(cid:48)

(cid:12)(x(cid:48)

i+1:

(cid:12) = (cid:12)
i)(cid:12)
i) − px(S(cid:48)
= (cid:12)
= (cid:12)
= δ → 0

i − x(cid:48)
(cid:12)(x(cid:48)
(cid:12)(x(cid:48)
i+1 − xi+1) − (x(cid:48)
(cid:12)(xi+1 − δ − xi+1) − (x(cid:48)

i+1) − (xi − xi+1)(cid:12)
(cid:12)
i+1 − xi+1)(cid:12)
(cid:12)
i+1 − xi+1)(cid:12)
(cid:12)

i+1 − xi+1) − (x(cid:48)

i+2 − xi+2)(cid:12)

(cid:12) = 0

completing the proof.

C.2 Bounded cardinality Lova´sz extension.

The bounded cardinality extension coefﬁcients px(S) are the coordinates of the vector y, where
y = S−1x and the entries (i, j) of the inverse are

S−1(i, j) =






1, if (j − i) mod k = 0 and i ≤ j,
−1, if (j − i) mod k = 1 and i ≤ j,
0, otherwise.

17

Equivalence to the Lova´sz extension. We want to show that the bounded cardinality extension is
equivalent to the Lova´sz extension when k = n. Let Ti,k = {j | (j − i) mod k = 0, for i ≤ j ≤
n, j ∈ Z+}, i.e., Ti,k stores the indices where j − i is perfectly divided by k. From the analytic form
of the inverse, observe that the i-th coordinate of y is px(Si) = (cid:80)
(xj − xj+1). For k = n,
we have Ti,n = {j | (j − i) mod n = 0} = {i}, and therefore px(Si) = xi − xi+1, which are the
coefﬁcients of the Lovász extension.

j∈Ti,k

Feasibility. The equation y = S−1x guarantees that the constraint x = (cid:80)n
i=1 ySi1Si is obeyed.
Recall that x is sorted in descending order like in the case of the Lovász extension. Then, it is
easy to see that px(Si) = (cid:80)
(xj − xj+1) ≤ xi, because xi − xi+1 is always contained in
the summation for px(Si). Therefore, by restricting x in the probability simplex it is easy to see
that (cid:80)n
i=1 px(Si) ≤ (cid:80)n
i=1 xi = 1. To secure tight equality, we allocate the rest of the mass to the
empty set, i.e., px(∅) = 1 − (cid:80)n
i=1 px(Si), which does not affect the value of the extension sicne the
corresponding Boolean is the zero vector.

j∈Ti,k

Extension. To prove the extension property we need to show that F(1S) = f (S) for all S with
|S| ≤ k. Consider any such set S and recall that we have sorted 1S with arbitrary tie breaks, such
that xi = 1 for i ≤ |S| and xi = 0 otherwise. Due to the equivalence with the Lova´sz extension, the
extension property is guaranteed when k = n for all possible sets. For k < n, consider the following
three cases for Ti,k.

• When i > |S|, Ti,k = ∅ because for sorted x of cardinality at most k, we know for the

coordinates that xi = xi+1 = 0. For i > k, this implies that px(Si) = 0.

• When i < |S|, (cid:80)

j∈Ti,k

px(Si) = 0.

(xj − xj+1) = 0 because xj = xj+1 = 1 and we have again

• When i = |S|, observe that (cid:80)

j∈Ti,k

in that case.

(xj −xj+1) = xi −xi+1 = xi. Therefore, px(Si) = 1.

Bringing it all together, F(1S) = (cid:80)n
one nonzero term, the one that corresponds to i = |S|.

i=1 pxf (Si) = px(S)f (S) = f (S) since the sum contains only

Continuity. Similar to the Lova´sz extension, px in the bounded cardinality extension is piecewise
linear and therefore a.e. differentiable with respect to x, where each piece corresponds to an ordering
of the coordinates of x. On the other hand, unlike the Lova´sz extension, the mapping x (cid:55)→ px(S) is
not necessarily globally Lipschitz when k < n, because it is not guaranteed to be Lipschitz continuous
at the boundaries.

C.3 Singleton extension.

Feasibility. The singleton extension is not dual LP feasible. However, one of the key reasons why
feasibility is important is that it implies Proposition 1, which show that optimizing F is a reasonable
surrogate to f . In the case of the singleton extension, however, Proposition 1 still holds even without
feasibility for f . This includes the case of the training accuracy loss, which can be viewed as
minimizing the set function f ({ˆy}) = −1{yi = ˆy}.

Here we give an alternative proof of Proposition 1 for the singleton extension. Consider the same
assumptions as Proposition 1 with the additional requirement that minS f (S) < 0 (this merely asserts
hat S = ∅ is not a trivial solution to the minimization problem, and that the minimizer of f is unique.
This is true, for example, for the training accuracy objective we consider in Section 5.

18

Proof of Proposition 1 for singleton extension. For x ∈ X = [0, 1]n,

F(x) =

=

≥

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

px(Si)f (Si)

(xi − xi+1)f (Si)

(xi − xi+1) min
j∈[n]

f (Sj)

≥ (x1 − xn+1) min
j∈[n]

f (Sj)

f (Sj)

≥ x1 · min
j∈[n]
f (Sj)

≥ min
j∈[n]

where the ﬁnal inequality follows since minj∈[n] f (Sj) < 0. Taking x = (1, 0, 0, . . . , 0)(cid:62) shows
that all the inequalities can be made tight, and the ﬁrst statement of Proposition 1 holds. For the
second statement, suppose that x ∈ X = [0, 1]n minimizes F. Then all the inequality in the preceding
argument must be tight. In particular, tightness of the ﬁnal inequality implies that x1 = 1. Meanwhile,
tightness of the ﬁrst inequaliity implies that xi −xi+1 = 0 for all i for which f (Si) (cid:54)= minj∈[n] f (Sj),
and tightness of the second inequality implies that xn+1 = 0. These together imply that x = 1⊕0n−1
where 1 is a 1 × 1 vector with entry equal to one, and 0n−1 is an all zeros vectors of length n − 1,
and ⊕ denotes concatenation. Since f (S1) = minj∈[n] f (Sj) is the unique minimize we have that
x = 1S1 ∈ Hull(cid:0) arg min1Si :i∈[n] f (Si)(cid:1), completing the proof.

Extension. Consider an arbitrary i ∈ [n]. Since we assume x = 1{i} is sorted, we are without loss
of generality considering 1{1} = (1, 0, . . . , 0, 0, . . . 0)(cid:62). Therefore, we have px(S1) = x1 − x2 =
1 − 0 = 1 and for each j > 1 we have px(Sj) = xj − xj+1 = 0 − 0 = 0. The only non-zero
probability is px(S1), and so

F(1{1}) =

n
(cid:88)

j=1

px(Sj)f (Sj) = f (S1) = f ({1}).

Continuity. The proof of continuity of the singleton extension is a simple adaptation of the proof
used for the Lova´sz extension, which we omit.

C.4 Permutations and Involutory Extension.

Feasibility.
such an elementary permutation matrix S, since S(Sx) = Spx = x, the constraint (cid:80)
is satisﬁed. Furthermore, (cid:80)
elements of a vector is invariant to permutations of the entries.

It is known that every elementary permutation matrix is involutory, i.e., SS = I. Given
S⊆[n] yS1S = x
S⊆[n] yS = 1 can be secured if x is in the simplex, since the sum of the

If the permutation has a ﬁxed point at the maximum element of x, i.e., it maps the
Extension.
maximum element to itself, then any elementary permutation matrix with such a ﬁxed point yields an
extension on singleton vectors. Without loss of generality, let x = e1, where e1 is the standard basis
vector in Rn. Then Se1 = e1 and therefore px(e1) = 1. This in turn implies F(e1) = 1 · f (e1).
This argument can be easily applied to all singleton vectors.

Continuity. The permutation matrix S can be chosen in advance for each x in the simplex. Since
px = Sx, the probabilities are piecewise-linear and each piece is determined by the ﬁxed point
induced by the maximum element of x. Consequently, px depends continuously on x.

19

C.5 Multilinear extension.

Recall that the multiliniear extension is deﬁned via px(S) = (cid:81)
all subsets S ⊆ [n] in general.

i∈S xi

(cid:81)

i /∈S(1 − xi) supported on

Feasibility. The deﬁnition of px(S) is equivalent to:

px(S) =

n
(cid:89)

i=1

xyi
i (1 − xi)1−yi

where yi = 1 if i ∈ S and zero otherwise. That is, px(S) is the product of n independent Bernoulli
distributions. So we clearly have px(S) ≥ 0 and (cid:80)
S⊆[n] px(S) = 1. The ﬁnal feasibility condition,
that (cid:80)
S⊆[n] px(S) · 1S = x can be checked by induction on n. For n = 1 there are only two sets:
{1} and the empty set. And clearly px({1}) · 1{1} = x1(1 − x1)0 = x1, so we have the base case.

Extension. For any S ⊆ [n] we have p1S (S) = (cid:81)
1. So F(1S) = ET ∼pxf (T ) = f (S).

i∈S xi

(cid:81)

i /∈S(1 − xi) = (cid:81)

i∈S 1 (cid:81)

i /∈S(1 − 0) =

Continuity. Fix and S ⊆ [n]. Again we check Lipschitzness. We use ∂xk to denote the derivative
operator with respect to xk. If k ∈ S we have

(cid:12)∂xk p1S (S)(cid:12)
(cid:12)

(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂xk

(cid:89)

xi

(cid:89)

i∈S

i /∈S

(cid:12)
(cid:12)
(cid:12)
(1 − xi)
(cid:12)
(cid:12)
(cid:12)

=

(cid:89)

(cid:89)

xi

(1 − xi) ≤ 1.

i∈S\{k}

i /∈S

Similarly, if k /∈ S we have,

(cid:12)∂xk p1S (S)(cid:12)
(cid:12)

(cid:12) =

(cid:12)
(cid:12)
(cid:12)
∂xk
(cid:12)
(cid:12)
(cid:12)

(cid:89)

xi

(cid:89)

i∈S

i /∈S

(1 − xi)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)
(cid:12)
(cid:12)

(cid:89)

xi

(cid:89)

i∈S

i /∈S∪{k}

(cid:12)
(cid:12)
(cid:12)
(1 − xi)
(cid:12)
(cid:12)
(cid:12)

≤ 1.

Hence the spectral norm of the Jacobian Jpx(S) is bounded, and so x (cid:55)→ px(S) is a Lipschitz map.

D Neural Set Function Extensions

This section re-states and proves the results from Section 4. To start, recall the deﬁnition of the primal
LP:

max
z,b

{x(cid:62)z + b}, where (z, b) ∈ Rn × R and 1(cid:62)

S z + b ≤ f (S) for all S ⊆ [n].

and primal SDP:

sup
Z(cid:23)0,b∈R

{Tr(ZX) + b} subject to Tr(1S1(cid:62)

T Z) + b ≤ f (S ∩ T ) for S, T ⊆ [n].

Proposition 5. (Containment of LP in SDP) For any x ∈ [0, 1]n, deﬁne X =
square-root taken entry-wise. Then, for any (z, b) ∈ Rn
where Z = diag(z), is primal SDP feasible and the objective values agree: Tr(X(cid:62)Z) = z(cid:62)x.

x(cid:62) with the
+ × R that is primal LP feasible, the pair (Z, b)

x

√

√

Proof. We start with the feasibility claim. Suppose that (z, b) ∈ Rn
primal LP. We must show that (Z, b) is a feasible solution to the primal SDP with X =
where Z = diag(z).
Recall the general formula for the trace of a matrix product: Tr(AB) = (cid:80)
mind, and noting that the (i, j) entry of 1S1(cid:62)

i,j AijBji. With this in
T is equal to 1 if i, j ∈ S ∩ T , and zero otherwise, we

+ × R is a feasible solution to the
x(cid:62) and

√

√

x

20

have for any S, T ⊆ [n] that

Tr(1S1(cid:62)

T Z) + b =

n
(cid:88)

(1S1(cid:62)

T )ij · diag(z)ij + b

i,j=1
(cid:88)

i,j∈S∩T
(cid:88)

(1S1(cid:62)

T )ij · diag(z)ij + b

diag(z)ij + b

i,j∈S∩T
(cid:88)

zi + b

=

=

=

i∈S∩T
= 1(cid:62)
S∩T z + b
≤ f (S ∩ T )

showing SDP feasibility. That the objective values agree is easily seen since:

Tr(ZX) =

n
(cid:88)

i,j=1

diag(z)ij ·

√

√

xi

xj =

√

√

xi

zi ·

xi = x(cid:62)z.

n
(cid:88)

i=1

i=1 λixix(cid:62)
n
(cid:88)

Recall the statement of the main result on constructing neural SFEs.
Proposition 6. Let px induce a scalar SFE of f . For X ∈ Sn
eigendecomposition X = (cid:80)n

i and ﬁx

+ with distinct eigenvalues, consider the

pX(S, T ) =

λi pxi(S)pxi (T ) for all S, T ⊆ [n].

Then, pX deﬁnes a neural SFE F at X.

i=1

S,T pX(S, T ) = 1. Recall from the eigendecomposition we have X = (cid:80)n

Proof. We begin by showing that the F deﬁned by pX(S, T ) is dual SDP feasible. It is clear that
(cid:80)
i where
each vi ∈ [0, 1]n. Using the scalar SFE px we may write each vi as a convex combination vi =
(cid:80)
S pvi(S)1S. For each i we may use this representation to re-express the outer product of vi with
itself:

i=1 λiviv(cid:62)

viv(cid:62)

i = (cid:0) (cid:88)

pvi (S)1S

(cid:1)(cid:0) (cid:88)

pvi(T )1T

(cid:1)(cid:62)

S
(cid:88)

T

pvi(S)21S1(cid:62)

S +

=

=

S
(cid:88)

S,T ⊆[n]

pvi (S)pvi(T )(1S1(cid:62)

T + 1T 1(cid:62)
S )

(cid:88)

S(cid:54)=T

pvi (S)pvi(T )(1S1(cid:62)

T + 1S1(cid:62)
T )

Summing over all eigenvectors vi yields the relation X = (cid:80)
proving dual SDP feasibility.
Next, consider an input X = 1S1(cid:62)
since X1S = 1S(1(cid:62)
For X = 1S1(cid:62)
1S|S|. So, taking ¯1S = 1S/(cid:112)|S| to be the normalized eigenvector of X, we have X = |S|¯1S¯1(cid:62)

S , 1S is clearly an eigenvector with eigenvalue λ = |S| because X1S = 1S(1(cid:62)

S 1S) = 1S|S|. That is, pX(T (cid:48), T ) = p1S (T (cid:48))p1S (T ).

S . In this case, the only eigenvector is 1S with eigenvalue λ = |S|

S,T ⊆[n] pX(S, T )(1S1(cid:62)

T + 1T 1(cid:62)

S 1S) =
S =

S ),

(cid:18)

|S|

1S√
|S|

(cid:19)(cid:18)

(cid:19)(cid:62)

1S√
|S|

= pX(S, S)1S1(cid:62)

S for pX(S, S) = 1. Therefore, the corresponding neural

SFE is

F(1S1(cid:62)

S ) = pX(S, S)f (S ∩ S) = f (S).

21

All that remains is to show continuity of neural SFEs. Since the scalar SFE px is continuous in
x by assumption, all that remains is to show that the map sending X to its eigenvector with i-th
largest eigenvalue is continuous. We handle sign ﬂip invariance of eignevectors by assuming a
standard choice for eigenvector signs—e.g., by ﬂipping the sign where necessary to ensure that the
ﬁrst non-zero coordinate is greater than zero. The continuity of the mapping X (cid:55)→ vi follows directly
from Theorem 2 from Yu et al. (2015), which is a variant of the Davis–Kahan theorem. The result
shows that the angle between the i-th eigenspaces of two matrices X and X(cid:48) goes to zero in the limit
as X → X(cid:48).

E General Experimental Background Information

E.1 Hardware and Software Setup

All training runs were done on a single GPU at a time. Experiments were either run on 1) a server
with 8 NVIDIA RTX 2080 Ti GPUs, or 2) 4 NVIDIA RTX 2080 Ti GPUs. All experiments are run
using Python, speciﬁcally the PyTorch (Paszke et al., 2019) framework (see licence here). For GNN
speciﬁc functionality, such as graph data batching, use the PyTorch Geometric (PyG) (Fey & Lenssen,
2019) (MIT License).

We shall open source our code with MIT License, and have provided anonymized code as part of the
supplementary material for reviewers.

E.2 Data Details

This paper uses ﬁve graph datasets: ENZYMES, PROTEINS, IMDB-BINART, MUTAG, and COL-
LAB. All data is accessed via the standardized PyG API. In the case of COLLAB, which has 5000
samples available, we subsample the ﬁrst 1000 graphs only for training efﬁciency. All experiments
Use a train/val/test split ratio of 60/30/10, which is done in exactly one consistent way across all
experiments for each dataset.

F Unsupervised Neural Combinatorial Optimization Experiments

All methods use the same GNN backbone: a combination of GAT Veliˇckovi´c et al. (2018) and
Gated Graph Convolution layer (Yujia et al., 2016). We use the Adam optimizer Kingma & Ba
(2014) with initial lr = 10−4 and default PyTorch settings for other parameters Paszke et al. (2019).
We use grid search HPO over batch size {4, 32, 64}, number of GNN layers {6, 10, 16} network
width {64, 128, 256}. All models are trained for 200 epochs. For the model with the best validation
performance, we report the test performance and the standard deviation of performance over test
graphs as a measure of method reliability.

F.1 Discrete Objectives

Maximum Clique. For the maximum clique problem, we could simply take f to compute the
clique size (with the size being zero if S is not a clique). However, we found that this objective led to
poor results and unstable training dynamics. So, instead, we select a discrete objective that yielded
the much more stable results across datasets. It is deﬁned for a graph G = ([n], E) as,

fMaxClique(S; G) = w(S)qc(S),

where w is a measure of size of S and q measures the density of edges within S (i.e., distance from
being a clique). The scalar c is a constant, taken to be c = 2 in all cases except REINFORCE for
which c = 2 proved ineffective, so we use c = 4 instead. Speciﬁcally, w(S) = (cid:80)
i,j∈S 1{(i, j) ∈ E}
simply counts up all the edges between nodes in S, and q(S) = −2w(S)/(|S|2 − |S|) is the ratio
(with a sign ﬂip) between the number of edges in S, and the number of undirected edges (|S|2 −|S|)/2
there would be in a clique of size |S|. If G were directed, simply remove the factor of 2. Note that
this f is minimized when S is a maximum clique.

22

Maximum Independent Set. Similarly for maximum independent set we use the discrete objective,

fMIS(S; G) = w(S)qc(S),

where w is a measure of size of S and q measures the number of edges between nodes in S (the number
should be zero for an independent set), and c = 2 as before. Speciﬁcally, we take w(S) = |S|/n, and
q(s) = 2 (cid:80)

i,j∈S 1{(i, j) ∈ E}/(|S|2 − |S|), as before.

F.2 Neural SFE details.

All Neural SFEs, unless otherwise stated, use the top k = 4 eigenvectors corresponding to the largest
eigenvalues. This is an important efﬁciency saving step, since with k = n, i.e., using all eigenvectors,
the resulting Neural Lova´sz extension requires O(n2) set function evaluations, compared to O(n)
for the scalar Lova´sz extension. By only using the top k we reduce the number of evaluations to
O(kn). Wall clock runtime experiments given in Figure 3 show that the runtime of the Neural Lova´sz
extension is around ×k its scalar counterpart, and that the performance of the neural extension
gradually increases then saturates when k gets large. To minimize compute overheads we pick the
smallest k at which performance saturation approximately occurs.

Instead of calling the pre-implemented PyTorch eigensolver torch.linalg.eigh, which calls
LAPACK routines, we use the power method to approximate the ﬁrst k eignevectors of X. This
is because we found the PyTorch function to be too numerically unstable in our case. In contrast,
we found the power method, which approximates eigenvectors using simple recursively deﬁned
polynomials of X, to be signiﬁcantly more reliable. In all cases we run the power method for 5
iterations, which we found to be sufﬁcient for convergence.

F.3 Baselines.

This section discusses various implementation details of the baseline methods we used. The basic
training pipeline is kept identical to SFEs, unless explicitly said otherwise. Namely, we use nearly
identical model architectures, identical data loading, and identical HPO parameter grids.

REINFORCE. We compared with REINFORCE (Williams (1992)) which enables backpropaga-
tion through (discrete) black-box functions. We opt for a simple instantiation for the score estimator

ˆgREINFORCE = f (S)

∂
∂θ

log p(S|θ),

where p(S|θ) = (cid:81)
j /∈S(1 − pj), i.e., each node is selected independently with probability
pi = gθ(y) for i = 1, 2, . . . [n], where gθ is a neural network and y some input attributes. We
maximize the expected reward, i.e.,

i∈S pi

(cid:81)

LREINFORCE(θ) = ES∼θ[ˆgREINFORCE].

For all experiments with REINFORCE, the expected reward is computed over 250 sampled actions S
which is approximately the number of function evaluations of neural SFEs in most of the datasets.
Here, f is taken to be the corresponding discrete objective of each problem (as described earlier in
section F.1). For maximum clique, we normalize rewards f (S) by removing the mean and dividing by
the standard deviation. For the maximum independent set, the same strategy led to severe instability
during training. To alleviate the issue, we introduced an additional modiﬁcation to the rewards:
among the sampled actions S, only the ones that achieved higher than average reward were retained
and the rewards of the rest were set to 0. This led to more stable results in most datasets, with the
exception of COLLAB were the trick was not sufﬁcient. These issues highlight the instability of
the score function estimator in this kind of setting. Additionally, we experimented by including
simple control variates (baselines). These were: i) a simple greedy baseline obtained by running a
greedy algorithm on each input graph ii) a simple uniform distribution baseline, where actions S
were sampled uniformly at random. Unfortunately, we were not able to obtain any consistent boost in
either performance or stability using those techniques. Finally, to improve stability, the architectures
employed with REINFORCE were slightly modiﬁed according to the problem. Speciﬁcally, for the
independent set we additionally applied a sigmoid to the outputs of the ﬁnal layer.

23

Figure 6: Top: Additional experimental results on the tinyImageNet dataset. Bottom: test accuracies
of different losses. The singleton extension performs broadly comparably to other losses.

Erdos Goes Neural. We compare with recent work on unsupervised combinatorial optimization
(Karalias & Loukas, 2020). We use the probabilistic methodology described in the paper to obtain a
loss function for each problem. For the MaxClique, we use the loss provided in the paper, where for
an input graph G = ([n], E) and learned probabilities p it is calculated by

LClique(p; G) = (β + 1)

(cid:88)

(i,j)∈E

wijpipj +

β
2

(cid:88)

vi(cid:54)=vj

pipj.

We omit additive constants as in practice they not affect the optimization. For the maximum
independent set, we follow the methodology from the paper to derive the following loss:

LIndepSet(p; G) = β

(cid:88)

(i,j)∈E

wijpipj −

(cid:88)

vi∈V

pi.

β was tuned through a simple line search over a few possible values in each case. Following the
implementation of the original paper, we use the same simple decoding algorithm to obtain a discrete
solution from the learned probabilities.

Straight Through Estimator. We also compared with the Straight-Through gradient estimator
(Bengio et al., 2013). This estimator can be used to pass gradients through sampling and thresholding
operations, by assuming in the backward pass that the operation is the identity. In order to obtain a
working baseline with the straight-through estimator, we generate level sets according to the ranking
of elements in the output vector x of the neural network. Speciﬁcally, given x ∈ [0, 1]n outputs from
a neural network, we generate indicator vectors 1Sk , where Sk = {j| xj ≥ xk} for k = 1, 2, . . . , n.
Then our loss function was computed as

LST (x; G) =

1
n

n
(cid:88)

k=1

f (1Sk ),

where f is the corresponding discrete objective from section F.1. At inference, we select the set that
achieves the best value in the objective while complying with the constraints.

Ground truths. We obtain the maximum clique size and the maximum independent set size s
for each graph by expressing it as a mixed integer program and using the Gurobi solver (Gurobi
Optimization, LLC, 2021).

F.4 k-Clique Constraint Satisfaction

Ground truths. As before, we obtain the maximum clique size s for each graph by expressing it
as a mixed integer program and using the Gurobi solver (Gurobi Optimization, LLC, 2021). This is
converted into a binary label 1{s ≥ k} indicating if there is a clique of size k or bigger.

24

0100200Epochs0.00.20.40.60.81.0Singleton Extension0100200Epochs0.00.20.40.60.81.0Cross Entropy0100200Epochs0.00.20.40.60.81.0Exponential0100200Epochs0.00.20.40.60.81.0MSE0100200Epochs0.00.20.40.60.81.0Hingetrain losstrain errorsingleton  (ours)exphingeMSExentSVHN94959697accuracy (%)singleton  (ours)exphingeMSExentCIFAR10919293949596singleton  (ours)exphingeMSExenttinyImageNet0204060Implementation details. The training pipeline, including HPO, is identical to the MaxClique setup.
The only difference comes in the evaluation—at test time the GNN produces an embedding x, and the
largest clique S in the support of px is selected. The model prediction for the constraint satisfaction
problem is then 1{|S| ≥ k}, indicating whether the GNN found a clique of size k or more. Since
this problem is. binary classiﬁcation problem we compute the F1-score on a validation set, and report
as the ﬁnal result the F1-score of that same model on the test set.

G Training error as an objective

Recall that for a K-way classiﬁer h : X → RK with ˆy(x) = arg maxk=1,...,K h(x)k, we consider
the training error 1
i=1 to
n
be a discrete non-differentiable loss. The set function in question is y (cid:55)→ 1{yi (cid:54)= y}, which we relax
using the singleton method described in Section 3.1.

i=1 1{yi (cid:54)= ˆy(xi)} calculated over a labeled training dataset {(xi, yi)}n

(cid:80)n

Training details. For all datasests we use a standard ResNet-18 backbone, with a ﬁnal layer to
output a vector of the correct dimension depending on the number of classes in the dataset. CIFAR10
and tinyImageNet models are trained for 200 epochs, while SVHN uses 100 (which is sufﬁcient for
convergence). We use SGD with momentum mom = 0.9 and weight decay wd = 5 × 10−4 and a
cosine learning rate schedule. We tune the learning rate for each loss via a simple grid search of the
values lr ∈ {0.01, 0.05, 0.1, 0.2}. For each loss we select the learning rate with highest accuracy on
a validation set, then display the training loss and accuracy for this run.

Algorithm 1: Scalar set function extension
def ScalarSFE(setFunction, x):

n x 1 tensor of embeddings, the output of a neural network
number of items in ground set (e.g.

# x:
# n:
setsScalar = getSupportSetsScalar(x) # n x n, i-th column is Si.
coeffsScalar = getCoeffsScalar(x) # 1 x n:
extension = (coeffsScalar*setFunction(setsScalar)).sum()
return extension

coefficients ySi.

number of nodes in graph)

Algorithm 2: Neural set function extension
def NeuralSFE(setFunction, X):

number of items in ground set (e.g.
embedding dimension

# X: n x d tensor of embeddings, the output of a neural network
# n:
# d:
X = normalize(X, dim=1)
Gram = X @ X.T #
n x n
eigenvalues, eigenvectors = powerMethod(Gram)
extension = 0 # initialize variable
for (eigval,eigvec) in zip(eigenvalues,eigenvectors):

number of nodes in graph)

# Compute scalar extension data.
setsScalar = getSupportSetsScalar(eigvec)
coeffsScalar = getCoeffsScalar(eigvec)
# Compute neural extension data from scalar extension data.
setsNeural = getSupportSetsNeural(setsScalar)
coeffsNeural = getCoeffsNeural(coeffsScalar)
extension += eigval*((coeffsNeural*setFunction(setsNeural)).sum())

return extension

H Pseudocode: A forward pass of Scalar and Neural SFEs

To illustrate the main conceptual steps in the implementation of SFEs, we include two torch-like
pseudocode examples for SFEs, one for scalar and one for neural SFEs. The key to the practical
implementation of SFEs within PyTorch is that it is only necessary to deﬁne the forward pass.
Gradients are then handled automatically during the backwards pass.

25

Observe that in both Algorithm, 1 and Algorithm 2, there are two key functions that have to be
implemented: i) getSupportSets, which generates the sets on which the extension is supported. ii)
getCoeffs, which generates the coefﬁcients of each set. Those depend on the choice of the extension
and have to be implemented from scratch whenever a new extension is designed. The sets of the
neural extension and their coefﬁcients can be calculated from the corresponding scalar ones, using
the deﬁnition of the Neural SFE and Proposition 3.

I Further Discussion

I.1 Limitations and Future Directions

Our SFEs have proven useful for learning solvers for a number of combinatorial optimization
problems. However there remain many directions for improvement. One direction of particular
interest is to scale our methods to instances with very large n. This could include simply considering
larger graphs, or problems with larger ground sets—e.g., selecting paths. We believe that a promising
approach to this would be to develop localized extensions that are supported on sets corresponding to
suitably chosen sub-graphs, which would enable us to build in additional task-speciﬁc information
about the problem.

I.2 Broader Impact

Our work focuses on a core machine learning methodological goal of designing neural networks that
are able to learn to simulate algorithmic behavior. This program may lead to a number of promising
improvements in neural networks such as making their generalization properties more reliable (as
with classical algorithms) and more interpretable decision making mechanisms. As well as injecting
algorithmic properties into neural network models, our work studies the use of neural networks
for solving combinatorial problems. Advances in neural network methods may lead to advances in
numerical computing more widely. Numerical computing in general—and combinatorial optimization
in particular—impacts a wide range of human activities, including scientiﬁc discovery and logistics
planning. Because of this, the methodologies developed in this paper and any potential further
developments in this line of work are intrinsically neutral with respect to ethical considerations; the
main responsibility lies in their ethical application in any given scenario.

References

Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.

Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad
Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A
review of uncertainty quantiﬁcation in deep learning: Techniques, applications and challenges.
Information Fusion, 76:243–297, 2021.

Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter.
Differentiable convex optimization layers. Advances in Neural Information Processing Systems,
32:9562–9574, 2019.

Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An
unsupervised differentiable approach. In International Conference on Learning Representations,
2018.

Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.

In International Conference on Machine Learning, pp. 136–145. PMLR, 2017.

Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering
with neural link predictors. In International Conference on Learning Representations, 2020.

Francis Bach. Submodular functions: from discrete to continuous domains. Mathematical Program-

ming, 175(1):419–459, 2019.

26

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features and speed
improvements. arXiv preprint arXiv:1211.5590, 2012.

Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854, 2019.

Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial

optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through

stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization:
a methodological tour d’horizon. European Journal of Operational Research, 290(2):405–421,
2021.

Jeff Bilmes. Submodularity in machine learning and artiﬁcial intelligence.

arXiv preprint

arXiv:2202.00132, 2022.

Grigoriy Blekherman, Pablo A Parrilo, and Rekha R Thomas. Semideﬁnite optimization and convex

algebraic geometry. SIAM, 2012.

G. Calinescu, C. Chekuri, M. Pál, and J. Vondrák. Maximizing a submodular set function subject to a

matroid constraint. SIAM J. Computing, 40(6), 2011.

Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar
Veliˇckovi´c. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint
arXiv:2102.09544, 2021a.

Quentin Cappart, Didier Chételat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar
Veliˇckovi´c. Combinatorial optimization and reasoning with graph neural networks. In Zhi-Hua
Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence,
IJCAI-21, pp. 4348–4355. International Joint Conferences on Artiﬁcial Intelligence Organization,
8 2021b. doi: 10.24963/ijcai.2021/595. URL https://doi.org/10.24963/ijcai.2021/595.
Survey Track.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597–1607. PMLR, 2020.

Ching-An Cheng, Xinyan Yan, and Byron Boots. Trajectory-wise control variates for variance
reduction in policy gradient methods. In Conference on Robot Learning, pp. 1379–1394. PMLR,
2020.

Gustave Choquet. Theory of capacities. In Annales de l’institut Fourier, volume 5, pp. 131–295,

1954.

Andreea-Ioana Deac, Petar Veliˇckovi´c, Ognjen Milinkovic, Pierre-Luc Bacon, Jian Tang,
and Mladen Nikolic. Neural algorithmic reasoners are implicit planners.
In M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad-
vances in Neural Information Processing Systems, volume 34, pp. 15529–15542. Curran
URL https://proceedings.neurips.cc/paper/2021/file/
Associates,
82e9e7a12665240d13d0b928be28f230-Paper.pdf.

Inc., 2021.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.

27

Andrew Dudzik and Petar Veliˇckovi´c. Graph neural networks are dynamic programmers. arXiv

preprint arXiv:2203.15544, 2022.

Jack Edmonds. Submodular functions, matroids, and certain polyhedra. In Combinatorial Optimiza-

tion—Eureka, You Shrink!, pp. 11–26. Springer, 2003.

Marwa El Halabi. Learning with structured sparsity: From discrete to convex and back. Technical

report, EPFL, 2018.

Marwa El Halabi and Stefanie Jegelka. Optimal approximation for unconstrained non-submodular
minimization. In International Conference on Machine Learning, pp. 3961–3972. PMLR, 2020.

Marwa El Halabi, Francis Bach, and Volkan Cevher. Combinatorial penalties: Which structures
are preserved by convex relaxations? In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 1551–1560. PMLR, 2018.

James E Falk and Karla R Hoffman. A successive underestimation method for concave minimization

problems. Mathematics of operations research, 1(3):251–259, 1976.

Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. In

ICLR (Workshop on Representation Learning on Graphs and Manifolds), volume 7, 2019.

Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM), 42(6):
1115–1145, 1995.

Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. In International
Conference on Learning Representations, 2018.

M. Grötschel, L. Lovász, and A. Schrijver. The ellipsoid algorithm and its consequences in combina-

torial optimization. Combinatorica, 1:499–513, 1981.

S Gu, T Lillicrap, Z Ghahramani, RE Turner, and S Levine. Q-prop: Sample-efﬁcient policy gradient
with an off-policy critic. In 5th International Conference on Learning Representations, ICLR
2017-Conference Track Proceedings, 2017.

Allal Guessab. Generalized barycentric coordinates and approximations of convex functions on
arbitrary convex polytopes. Computers & Mathematics with Applications, 66(6):1120–1136, 2013.

Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.

gurobi.com.

Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical
queries on knowledge graphs. Advances in neural information processing systems, 31, 2018.

Kai Hormann. Barycentric interpolation. In Approximation Theory XIV: San Antonio 2013, pp.

197–218. Springer, 2014.

Takayuki Iguchi, Dustin G Mixon, Jesse Peterson, and Soledad Villar. On the tightness of an sdp

relaxation of k-means. arXiv preprint arXiv:1505.04778, 2015.

Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. Monotone closure of relaxed constraints in submod-
ular optimization: Connections between minimization and maximization: Extended version. In
UAI, 2014.

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In Int.

Conf. on Learning Representations (ICLR), 2017.

Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for

combinatorial optimization on graphs. In NeurIPS, 2020.

Marc C Kennedy and Anthony O’Hagan. Bayesian calibration of computer models. Journal of the

Royal Statistical Society: Series B (Statistical Methodology), 63(3):425–464, 2001.

28

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

Vipin Kumar. Algorithms for constraint-satisfaction problems: A survey. AI magazine, 13(1):32–32,

1992.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural

networks. arXiv preprint arXiv:1511.05493, 2015.

Yujia Li, Felix Gimeno, Pushmeet Kohli, and Oriol Vinyals. Strong generalization and efﬁciency in

neural programs. arXiv preprint arXiv:2007.03629, 2020.

Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent
control variates for policy optimization via stein identity. In International Conference on Learning
Representations, 2018.

László Lovász. Submodular functions and convexity. In Mathematical programming the state of the

art, pp. 235–257. Springer, 1983.

László Lovász and Alexander Schrijver. Cones of matrices and set-functions and 0–1 optimization.

SIAM journal on optimization, 1(2):166–190, 1991.

C Maddison, A Mnih, and Y Teh. The concrete distribution: A continuous relaxation of discrete

random variables. In Int. Conf. on Learning Representations (ICLR), 2017.

J-L Marichal. An axiomatic approach of the discrete choquet integral as a tool to aggregate interacting

criteria. IEEE transactions on fuzzy systems, 8(6):800–807, 2000.

Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for
combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021.

Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint
arXiv:2007.08663, 2020.

Kazuo Murota. Discrete convex analysis. Mathematical Programming, 83(1):313–371, 1998.

Mathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit mle: Backpropagating through
discrete exponential family distributions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34,
pp. 14567–14579. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/
paper/2021/file/7a430339c10c642c4b2251756fd1b484-Paper.pdf.

Guillaume Obozinski and Francis Bach. Convex Relaxation for Combinatorial Penalties. PhD thesis,

INRIA, 2012.

Guillaume Obozinski and Francis Bach. A uniﬁed perspective on convex structured sparsity: Hierar-

chical, symmetric, submodular norms and beyond. 2016.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32,
2019.

Anselm Paulus, Michal Rolinek, Vit Musil, Brandon Amos, and Georg Martius. Comboptnet: Fit the
right np-hard problem by learning integer programming constraints. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pp. 8443–8453. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/paulus21a.html.

Max B Paulus, Chris J Maddison, and Andreas Krause. Rao-blackwellizing the straight-through
gumbel-softmax gradient estimator. In International Conference on Learning Representations,
2020a.

29

Max Benedikt Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient

estimation with stochastic softmax tricks. In NeurIPS 2020, 2020b.

Marin Vlastelica Poganˇci´c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differenti-
ation of blackbox combinatorial solvers. In International Conference on Learning Representations,
2019.

Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in
vector space using box embeddings. In International Conference on Learning Representations,
2019.

Alexander Schrijver et al. Combinatorial optimization: polyhedra and efﬁciency, volume 24. Springer,

2003.

Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with

physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367–377, 2022.

John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge

university press, 2004.

Mohit Tawarmalani and Nikolaos V Sahinidis. Convex extensions and envelopes of lower semi-

continuous functions. Mathematical Programming, 93(2):247–263, 2002.

Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum

constraint satisfaction. Frontiers in artiﬁcial intelligence, 3:98, 2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.

Petar Veliˇckovi´c, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution

of graph algorithms. In International Conference on Learning Representations, 2019.

Petar Veliˇckovi´c and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021.

ISSN 2666-3899.

J. Vondrák. Optimal approximation for the submodular welfare problem in the value oracle model.

In Symposium on Theory of Computing (STOC), 2008.

Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical
reasoning using a differentiable satisﬁability solver. In International Conference on Machine
Learning, pp. 6545–6554. PMLR, 2019.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3):229–256, 1992.

Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. In International Conference on Learning Representations, 2018.

Hao Xu, Ka-Hei Hui, Chi-Wing Fu, and Hao Zhang. Tilingnn: learning to tile with self-supervised

graph neural network. ACM Transactions on Graphics (TOG), 39(4):129–1, 2020.

Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
What can neural networks reason about? In International Conference on Learning Representations,
2019.

Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural
execution engines: Learning to execute subroutines. Advances in Neural Information Processing
Systems, 33, 2020.

30

Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis–kahan theorem for

statisticians. Biometrika, 102(2):315–323, 2015.

Li Yujia, Tarlow Daniel, Brockschmidt Marc, Zemel Richard, et al. Gated graph sequence neural

networks. In International Conference on Learning Representations, 2016.

31

