1
2
0
2

v
o
N
1

]
I

A
.
s
c
[

1
v
6
6
4
3
0
.
1
1
1
2
:
v
i
X
r
a

Learning Large Neighborhood Search Policy for
Integer Programming

Yaoxin Wu
SCALE@NTU Corp Lab
Nanyang Technological University, Singapore
wuyaoxin@ntu.edu.sg

Wen Song∗
Shandong University
Qingdao, China
wensong@email.sdu.edu.cn

Zhiguang Cao
Singapore Institute of Manufacturing Technology
A*STAR, Singapore
zhiguangcao@outlook.com

Jie Zhang
Nanyang Technological University
Singapore
zhangj@ntu.edu.sg

Abstract

We propose a deep reinforcement learning (RL) method to learn large neighborhood
search (LNS) policy for integer programming (IP). The RL policy is trained as the
destroy operator to select a subset of variables at each step, which is reoptimized
by an IP solver as the repair operator. However, the combinatorial number of
variable subsets prevents direct application of typical RL algorithms. To tackle
this challenge, we represent all subsets by factorizing them into binary decisions
on each variable. We then design a neural network to learn policies for each
variable in parallel, trained by a customized actor-critic algorithm. We evaluate the
proposed method on four representative IP problems. Results show that it can ﬁnd
better solutions than SCIP in much less time, and signiﬁcantly outperform other
LNS baselines with the same runtime. Moreover, these advantages notably persist
when the policies generalize to larger problems. Further experiments with Gurobi
also reveal that our method can outperform this state-of-the-art commercial solver
within the same time limit.

1

Introduction

Combinatorial optimization problems (COPs) have been widely studied in computer science and
operations research, which cover numerous real-world tasks in many ﬁelds such as communication,
transportation and manufacturing [1]. Most COPs are very difﬁcult to solve efﬁciently due to their
NP-hardness. The performance of classic methods, including exact and heuristic algorithms [2], is
generally limited by hand-crafted policies that are costly to design, since considerable trial-and-error
and domain knowledge are needed. On the other hand, it is common in practice that similar instances
with shared structure are frequently solved, and differ only in data that normally follows a distribution
[3]. This provides a chance for machine learning to automatically generate heuristics or policies. In
doing so, the learned alternatives are expected to save massive manual work in algorithm design, and
raise the performance of the algorithm on a class of problems.

Recently, a number of works apply deep (reinforcement) learning to automatically design heuristic
algorithms, either in constructive or improving fashion. Different from construction heuristics that
sequentially extend partial solutions to complete ones [4, 5, 6, 7, 8, 9, 10, 11], learning improvement
heuristics can often deliver high solution quality by iteratively reoptimizing an initial solution using

∗Wen Song is the corresponding author.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
local operations [12, 13, 14]. In this line, some methods are developed under the Large Neighborhood
Search (LNS) framework [15, 16], which is a powerful improving paradigm to ﬁnd near-optimal
solutions for COPs.

However, the above methods are restricted to speciﬁc problem types, and cannot generalize to those
from different domains. This motivates the studies of learning to directly solve Integer Programs
(IPs), which is very powerful and ﬂexible in modelling a wide range of COPs. The standard approach
to solve IPs is branch-and-bound (B&B) [17], which lies at the core of common solvers such as
SCIP, Gurobi, and CPLEX. Thus, most of existing methods improve the performance of a solver on a
distribution of instances, by training models for critical search decisions in B&B such as variable and
node selection [18, 19, 20]. Nevertheless, these methods are generally limited to small instances and
require sufﬁcient interface access to the internal solving process of the solvers.

This paper mainly tackle the issue that how to improve a solver from externals such that it can
ﬁnd high-quality solutions more quickly? In speciﬁc, we propose a high-level, learning based LNS
method to solve general IP problems. Based on deep reinforcement learning (RL), we train a policy
network as the destroy operator in LNS, which decides a subset of variables in the current solution for
reoptimization. Then we use a solver as the repair operator, which solves sub-IPs to reoptimize the
destroyed variables. Despite being heuristic, our method can effectively handle the large-scale IP by
solving a series of smaller sub-IPs. Moreover, complex interface to the solver’s internal logic is not
required. However, the above RL task is challenging, mainly because the action space, i.e., number
of variable subsets at each LNS step, is exponentially large. To resolve this issue, we represent all the
subsets by factorizing them into binary decisions on each variable, i.e., whether a variable should be
destroyed. In doing so, we make it possible to learn a policy to select any subset from large discrete
action spaces (at least 21000 candidates in our experiments). To this end, we design a Graph Neural
Network (GNN) based policy network that enables learning policies for each variable in parallel, and
train it by a customized actor-critic algorithm.

A recent work [21] also attempts to learn LNS policy to solve IP problems, and we generalize this
framework to enable learning more ﬂexible and powerful LNS algorithms. One limitation of [21] is
that it hypothesizes a constant cardinality of the destroyed variable subset at each LNS step, which is
a predeﬁned hyperparameter. However, the number and choice of optimized variables at each step
should be adaptive according to instance information and solving status, which is achieved in our
method. In doing so, the LNS policies trained by our method signiﬁcantly outperform those trained
by the method in [21].

We evaluate our method on four NP-hard benchmark problems with SCIP as the repair solver.
Extensive results show that our method generally delivers better solutions than SCIP with mere 1/2 or
1/5 of runtime, and signiﬁcantly outperforms LNS baselines with the same runtime. These advantages
notably persist when the trained policies are directly applied to much larger problems. We also apply
our LNS framework to Gurobi, which shows superiority over the solver itself and other baselines.

2 Related work

In this section, we brieﬂy review existing works related to ours. We ﬁrst describe two main streams
of learning based methods to solve COPs, and then introduce the literature that study RL with large
discrete action space, which is also an essential issue we confront in this paper.

Learning to solve speciﬁc COPs. Quite a few works attempt to learn heuristics to solve certain
types of COPs. Compared to construction ones, methods that learn improvement heuristics can often
deliver smaller optimality gap, by training policies to iteratively improve the solution. Chen and
Tian [12] propose to learn how to locally rewrite a solution; Wu et al. [13] train policies to pick
the next solution in local moves; Lu et al. [14] learn to select local operators to reform a solution.
These methods are generally limited by simple local operations. A few methods learn more powerful
operators under the LNS framework. Hottung and Tierney [15] train an attention model to repair the
solution every time it is broken by a predeﬁned destroy operator. Similarly, Gao et al. [16] combine
GNN and Recurrent Neural Network to learn a reinsertion operator to repair sequence-based solution.
However, all the above methods are limited to speciﬁc problem types, e.g., the LNS methods in
[15, 16] are designed only for routing problems. In contrast, this paper aims to solve general IP
problems with a high-level LNS framework and raise its performance by learning better policies.

2

Learning to solve IP problems. Most of learning based methods for IPs aim to improve inner
policies of B&B algorithm. For example, He et al. [18] learn to explore nodes in B&B tree by
imitating an oracle. Gasse et al. [20] train a GNN model to predict the strong branching rule by
imitation learning. Khalil et al. [19] predict the use of primal heuristics by logistic regression. Other
components of B&B are also considered to improve its practical performance, such as learning to
select cutting planes by Tang et al. [22], predicting local branching at root node by Ding et al. [23]
or reﬁning the primal heuristic and branching rule concurrently by Nair et al. [24]. Different from
these works, we employ RL to improve practical performance of IP solvers especially for large-scale
problems, without much engineering effort on interfacing with inner process of solvers.2 This is also
noted in [21], which proposes to combine learning and LNS to solve IPs. However, a major drawback
of this method is that the subsets of destroyed variables are assumed to be ﬁxed-sized, which limits
its performance. In contrast, our LNS framework allows picking variable subsets in a more adaptive
way, and thus empowers learning broader classes of policies.

RL with large action spaces. Learning with large-sized, high-dimensional discrete action spaces is
still intricate in current RL research. In this direction, Pazis and Parr [25] use binary format to encode
all actions, and learn value functions for each bit. Tavakoli et al. [26] design neural networks with
branches to decide on each component of the action. Besides, Dulac-Arnold et al. [27] and Chandak
et al. [28] update the action representation by solving continuous control problems so as to alleviate
learning complexity via generalization. Other similar works can be found in [29, 30, 31]. However,
we note that all these methods are generally designed for tasks with (discretized) continuous action
spaces, which are relatively low-dimensional and small-sized (at most 1025 as in [26]). In contrast,
action spaces in our scenario are much larger (at least 21000). In this paper, we propose to factorize
the action space into binary actions on each dimension, and train the individual policies in parallel
through parameter sharing.

3 Preliminaries

Integer Program (IP) is a typically deﬁned as arg minx{µ(cid:62)x|Ax ≤ b; x ≥ 0; x ∈ Zn}, where x is
a vector of n decision variables; µ ∈ Rn denotes the vector of objective coefﬁcients; the incidence
matrix A ∈ Rm×n and right-hand-side (RHS) vector b ∈ Rm together deﬁne m linear constraints.
With the above formulation, the size of an IP problem is generally reﬂected by the number of variables
(n) and constraints (m).

Large Neighborhood Search (LNS) is a type of improvement heuristics, which iteratively reopti-
mizes a solution by the destroy and repair operator until certain termination condition is reached
[32]. Speciﬁcally, the former one breaks part of the solution xt at step t, then the latter one ﬁxes the
broken solution to derive the next solution xt+1. The destroy and repair operator together deﬁne the
solution neighborhood N (xt), i.e., solution candidates that can be accessed at time step t. Compared
to typical local search heuristics, LNS is more effective in exploring the solution space, since a larger
neighborhood is considered at each step [32]. Most of existing LNS methods rely on problem speciﬁc
operators, e.g., removal and reinsertion for solutions with sequential structure [33, 34]. Instead, we
propose a RL based LNS for general IP problems, with the learned destroy operator to select variables
for reoptimization at each step.

4 Methodology

In this section, we ﬁrst formulate our LNS framework as a Markov Decision Process (MDP). Then,
we present the factorized representation of the large-scale action space, and parametrize the policy
by a specialized GNN. Finally, we introduce a customized actor-critic algorithm to train our policy
network for deciding the variable subset.

4.1 MDP formulation

Most of existing works learn LNS for speciﬁc problems [15, 16], which rely on extra domain
knowledge and hinder their applicability to other COPs. In this paper, we apply LNS to general IP

2Nevertheless, our method can also work with solvers enhanced by the above methods as repair operators.

3

Figure 1: An example of LNS solving an instance with 4 variables and 3 constraints. Given the
current state characterized by static and dynamic features, the RL agent (destroy operator) selects
a subset of variables from the current solution to be reoptimized. Then this action inﬂuences the
environment, through which a sub-IP is formulated and solved by the repair operator (i.e., solver).
As feedbacks, the new solution updates the state and a reward is provided to the agent. The above
process is repeated until reaching the time limit.

problems, in which RL is employed to learn a policy that at each step, selects a variable subset from
the solution to be reoptimized. We formulate this sequential decision problem as a discrete-time MDP.
Speciﬁcally, we regard the destroy operator as the agent and the remainder in LNS as the environment.
In one episode of solving an IP instance, the MDP can be described as follows:

States. A state st ∈ S at each LNS step needs to not only reﬂect the instance information but
also the dynamic solving status. For the former, we represent it by static features of variables and
constraints, along with the incidence matrix in an instance. The latter is represented by dynamic
features, including the current solution xt and dynamic statistics of incumbent solutions up to step t.
Speciﬁcally, both the current and incumbent solution at t = 0 are deﬁned as the initial solution x0.

Actions. At each state st, an action of the agent is to select a variable subset at from all candidate
subsets A for reoptimization.

Transition. The repair operator (i.e., an IP solver) solves the sub-IP, where only the variables in
action at are optimized with the others equaling to their current values in xt. Accordingly, the next
state st+1 is deterministically attained by updating st with the new solution:

xt+1 = arg min

x

{µ(cid:62)x|Ax ≤ b; x ≥ 0; x ∈ Zn; xi = xi

t, ∀xi /∈ at}.

(1)

Rewards. The reward function is deﬁned as rt = r(st, at) = µ(cid:62)(xt − xt+1), which is the change of
the objective value. Given the step limit T of the interaction between the agent and environment, the
return (i.e., cumulative rewards) from step t of the episode is Rt = (cid:80)T
k=t γk−trk, with the discount
factor γ ∈ [0, 1]. The goal of RL is to maximize the expected return E[R1] over all episodes, i.e., the
expected improvement over initial solutions, by learning a policy.

Policy. The stochastic policy π represents a conditional probability distribution over all possible
variable subsets given a state. Starting from s0, it iteratively picks an action at based on the sate st at
each step t, until reaching the step limit T .

An example of the proposed LNS framework solving an IP instance is illustrated in Figure 1, in
which the GNN based policy network has the potential to process instances of any size via sharing
parameters across all variables.

4.2 Action factorization

Given the vector of decision variables x in an IP instance, we gather all its elements in a set
X = {x1, x2, . . . , xn}. Accordingly, we deﬁne the action space A = {a|a ⊆ X}, which contains all
possible variable subsets. Thus our RL task is learning a policy to select at ∈ A for reoptimization at
each step t of LNS, and the cardinality of at (i.e., |at|) reﬂects the destroy degree on the solution.
Apparently, the size of the combinatorial space A is 2n, which grows exponentially with the number

4

Current solution value  Variable to optimize  Reoptimizedsolution value  𝑎11𝑥1+𝑎12𝑥2+𝑎13𝑥3+𝑎14𝑥4≤𝑏1min.𝑎21𝑥1+𝑎22𝑥2+𝑎23𝑥3+𝑎24𝑥4≤𝑏2𝑎31𝑥1+𝑎32𝑥2+𝑎33𝑥3+𝑎34𝑥4≤𝑏3𝜋(𝑎𝑡|𝑠𝑡)4×1SampleGNN based PolicyState 𝑠𝑡Environment Update dynamic featuresConstraintfeatures𝐶Variablefeatures𝑉4×𝑑ℎ3×𝑑𝑐DestroyoperatorIP formulationIP formulationIncidencematrix𝐴3×4Staticfeatures𝑥1𝑥2𝑥3𝑥4𝑥1𝑥2𝑥3𝑥4SOLVER(Repair operator)𝜇1𝑥1+𝜇2𝑥2+𝜇3𝑥3+𝜇4𝑥4of variables n. This prevents the application of general RL algorithms on large-scale problems, since
they require an explicit representation of all actions and exploration over such huge action space.

As a special case under our LNS framework, Song et al. [21] assumes the action space as the subspace
of A that merely contains equal-sized variable subsets, that is, Az = {az|az ∈ A, |az| = z}. In
doing so, they instead learn classifying variables into groups of equal size, to optimize each iteratively.
Despite the good performance on some COPs, such action representation with the ﬁxed destroy
degree makes the LNS search inﬂexible and thus limits the class of policies that can be learned. Also,
the issue of large action space still exists, since the patterns to group variables could be combinatorial.

To involve larger action space and in the meanwhile keep the learning tractable, we factorize the
combinatorial action space A into elementary actions on each dimension (i.e., variable). Speciﬁcally,
we denote ai
t ∈ {xi, ∅} as the elementary action for variable xi at step t. It means that xi is either
selected for reoptimization, or not selected and thus ﬁxed as its value in the current solution. With
such representation, any variable subset can also be expressed as at = (cid:83)n
t. Therefore, our task
can be converted into learning policies for binary decisions on each variable. This enables exploration
of large action space by traversing binary action spaces, the number of which grows linearly with the
number of variables. To this end, we factorize the original policy as below:

i=1 ai

π(at|st) =

(cid:89)n

i=1

πi(ai

t|st),

(2)

which expresses π(at|st), the probability of selecting an action, as the product of probabilities of
selecting its elements. Since (cid:80)
t∈{xi,∅}πi(ai
t|st) = 1, ∀ i ∈ {1, . . . , n}, it is easy to verify that the
ai
sum of probabilities of all actions in A still equals to 1, i.e., (cid:80)
at∈Aπ(at|st) = 1. Based on this
factorization, we can apply RL algorithms to train policies πi for each variable xi. In the following
subsections, we ﬁrst parametrize the policy π by a GNN πθ, and then train it by a customized
actor-critic algorithm.

4.3 Policy parametrization

Policy networks in deep RL algorithms are generally designed to map states to probabilities of
selecting each action. In our case, the complexity of directly training such policy networks could
exponentially increase with the growing number of variables. Based on Equation (2), an ad hoc way
to reduce the complexity is training individual policies for each dimension. However, the IP problems
in our study comprise a high volume of variables, and it is unmanageable to train and store this
many networks. Learning disjoint policies without coordination could also suffer from convergence
problem as shown in [35]. Another possible paradigm could be constructing a semi-shared network, in
which all dimensions share a base network and derive outputs from separate sub-networks. Following
a similar idea, Tavakoli et al. [26] design a network with branches for deep Q-learning algorithms.
However, it is not suitable for IP problems, since the number of sub-networks is only predeﬁned for
single problem size, without the generalization ability to different-sized instances.

To circumvent the above issues, we design a GNN based policy network to share parameters across all
dimensions, and also enable generalization to any problem size [36, 37]. To this end, we ﬁrst describe
the state st by a bipartite graph G = (V, C, A), where V = {v1, · · · , vn} denotes variable nodes
with features V ∈ Rn×dv ; C = {c1, · · · , cm} denotes constraint nodes with features C ∈ Rm×dc;
A ∈ Rm×n denotes the adjacency matrix with aji being the weight (or feature) of the edge between
nodes cj and vi, which is practically the incidence matrix A in an IP instance. This state representation
is similar to the one in [20], which depicts the status in B&B tree search to learn branching policy.
We extend its usage outside the solver to reﬂect the solving process in our LNS framework.

Then, we parametrize the policy as πθ(at|st) by a graph convolutional network (GCN), a GNN
variant broadly used in various tasks [38, 39, 40]. The architecture of our design is illustrated in
Appendix A.1, with the graph convolution layer expressed as below:

C(k+1) = C(k) + σ

V(k+1) = V(k) + σ

(cid:16)

LN
(cid:16)

LN

(cid:16)

(cid:17)(cid:17)

,

AV(k)W (k)
(cid:16)

v

A(cid:62)C(k+1)W (k)

c

(cid:17)(cid:17)

, k = 0, . . . , K

(3)

where W (k)
, W (k)
v
and C(k) = [c(k)

c ∈ Rdh×dh are trainable weight matrices in the k-th layer; V(k) = [v(k)

n ](cid:62)
m ](cid:62) are node embeddings for variables and constraints respectively in k-th

· · · v(k)

· · · c(k)

1

1

5

i }n

layer; LN and σ(·) denote layer normalization and Tanh activation function respectively. In particular,
we linearly project raw features of variables and constraints into the initial node embeddings V(0) and
C(0) with dh dimensions (dh = 128), and keep this dimension through all layers. After K iterations
of convolution (K = 2), the embeddings for the two clusters of heterogeneous nodes are advanced
as {vK
j=1. We ﬁnally process the former by a multi-layer perceptron (MLP) with a
single-value output activated by Sigmoid. In this way, the output value can represent the probability
of a variable being selected, i.e., πi(ai
i ), ∀ i ∈ {1, . . . , n}, such that we
can conduct Bernoulli sampling accordingly on each variable and attain the subset. In this paper, we
structure the MLP by two hidden layers, which have 256 and 128 dimensions respectively and are
activated by Tanh.

t|st) = MLP(vK

t|st) = πθ(ai

i=1 and {cK

j }m

4.4 Training algorithm

Actor-critic is one of the policy gradient methods developed from REINFORCE [41]. It parametrizes
state-value or action-value function as a critic network to estimate the expected return. In this paper,
we adopt Q-actor-critic with Qω(s, a) ≈ Q(s, a) = E[Rt|st = s, at = a], where ω is the parameter
set to be learned. In doing so, Qω (i.e., the critic) can be updated through bootstrapping and leveraged
in the training of the policy network (i.e., the actor). Speciﬁcally, the loss functions for the critic and
the actor are deﬁned as follows:

L(ω) = ED[(γQω(st+1, at+1) + rt − Qω(st, at))2],
L(θ) = ED[Qω(st, at) log πθ(at|st)],

(4)

(5)

where the experience replay buffer D contains transitions (st, at, rt, st+1, at+1), which are collected
during solving a batch of instances. γQω(st+1, at+1) + rt is the one-step temporal-difference (TD)
target to update the critic.

The above Q-actor-critic is not directly applicable to the primitive high-dimensional action spaces in
our IP problems. To learn the factorized policies we designed in Section 4.2, one way is to customize
it by training an actor and critic on each dimension, and updating the parameters via the loss functions:

˜L(ω) = ED[

(cid:88)n

i=1

(γQω(st+1, ai

t+1) + rt − Qω(st, ai

t))2],

˜L(θ) = ED[

(cid:88)n

i=1

Qω(st, ai

t) log(πθ(ai

t|st))],

1
n
1
n

(6)

(7)

where the parameter-sharing Qω is used across all dimensions as the actor πθ, i.e., Qω(st, ai
t) =
Qi(st, ai
t), ∀ i ∈ {1, . . . , n}. However, our experiments show that the critic trained by bootstrapping
on each elementary action delivers inferior performance, which is similar to the ﬁnding in [26].
Intuitively, this may stem from excessive action-value regressions with one single neural network. To
circumvent this issue, we keep the global TD learning in Equation (4) and adjust elementary policies
by the Q value of the state-action pair (st, at), such that:

˜L(θ) = ED[Qω(st, at)

(cid:88)n

i=1

log(πθ(ai

t|st))],

(8)

where the critic Qω is structured in the same manner as πθ, except that: 1) we add a binary value to
the raw features of each variable ai
t to indicate whether it is selected; 2) we use MLP to process the
graph embedding, which aggregates the embeddings of variables by mean-pooling, to output a real
value that represents the Q value.

Clipping & masking. To enhance exploration, we clip the probabilities of being selected for each
variable in a range [(cid:15), 1 − (cid:15)], (cid:15) <0.5. It helps avoid always or never traversing some variables with
extreme probabilities. We also do not consider empty or universal sets of variables, which lead to
unsuitable sub-IPs. Though the chance to select these two sets are low, we mask them by resampling.

Details of the training algorithm are given in Appendix A.2. Besides the policy network we designed
in Section 4.3, we also adopt this algorithm to train a MLP based semi-shared network similar to the
one in [26], which indicates that the fully-shared one (ours) is more suitable for IP problems. More
details are given in Appendix A.3.

6

5 Experimental results

We perform experiments in this section on four NP-hard benchmark problems: Set Covering (SC),
Maximal Independent Set (MIS), Combinatorial Auction (CA) and Maximum Cut (MC), which are
widely used in existing works. Our code is available.3

Instance generation. We generate SC instances with 1000 columns and 5000 rows following the
procedure in [42]. MIS instances are generated following [43], where we use the Erd˝os-Rényi random
graphs with 1500 nodes and set the afﬁnity number to 4. CA instances with 2000 items and 4000
bids are generated according to arbitrary relationships in [44]. MC instances are generated according
to Barabasi-Albert random graph models [45], with average degree 4, and we adopt graphs of 500
nodes. For each problem type, we generate 100, 20, 50 instances for training, validation, and testing.
In addition, we double and quadruple the number of variables and generate 50 larger and even larger
instances respectively for each problem, to verify the generalization performance. We name the
instance groups and display their average sizes in Table 1.

Table 1: Average sizes of problem instances.

Training

Generalization

Num. of

SC MIS

CA

MC

SC2 MIS2

CA2 MC2

SC4 MIS4

CA4

MC4

Variables
Constraints

1000
5000

1500
5939

4000
2674

2975
4950

2000
5000

3000
11932

8000
5357

5975
9950

4000
5000

6000
23917

16000
10699

11975
19950

Features. To represent an instance, we extract static features of variables and constraints, along with
the incidence matrix after presolving by the solver at the root node of B&B. In this way, redundant
information could be removed and the extracted features could be more clear and compact in reﬂecting
the problem structure. For the dynamic features, we record for each variable its value in the current
solution and incumbent, as well as its average value in all incumbents to represent the solving status
of LNS. Specially, at the step t = 0, the average incumbent values are naturally attained from the
initial solution, i.e., the incumbent at the root node. Note that in practice, we concatenate the static
and dynamic features for each variable, and attach them (V) to variable nodes in the bipartite graph.
The features of constraints (C) and the incidence matrix (A) are attached to the constraint nodes and
edges, respectively. Detailed description of the above features are available in Appendix A.4.

Hyperparameters. We use the state-of-the-art open source IP solver SCIP (v6.0.1) [46] as the repair
operator, which also serves as a major baseline. We run all experiments on an Intel(R) Xeon(R)
E5-2698 v4 2.20GHz CPU. For each problem, we train 200 iterations, during each we randomly
draw M =10 instances. We set the training step limit T =50, 50, 70, 100 for SC, MIS, CA and MC
respectively. The time limit for repair at each step is 2 seconds, unless stated otherwise. We use (cid:15)=0.2
for probability clipping. For the Q-actor-critic algorithm, we set the length of the experience replay
T M , the number of updating the network U =4 and the batch size B=T M/U . We set the discount
factor γ=0.99 for all problems, and use Adam optimizer with learning rate 1 × 10−4. To show the
applicability of our method on other solvers, we have also performed experiments where Gurobi [47]
is used as the repair solver, which will be discussed in Section 5.3.

5.1 Comparative analysis

Baselines. We compare our method with four baselines:

• SCIP with default settings.

• U-LNS: a LNS version which uniformly samples a subset size, then ﬁlls it by uniformly sampling
variables. We compare with it to show that our method can learn useful subset selection policies.

• R-LNS: a LNS version with hand-crafted rule proposed in [21], which randomly groups variables

into disjoint equal-sized subsets and reoptimizes them in order.

• FT-LNS: the best-performing LNS version in [21], which applies forward training, an imitation

learning algorithm, to mimic the best demonstrations collected from multiple R-LNS runs.

3https://github.com/WXY1427/Learn-LNS-policy

7

Table 2: Comparison with SCIP and LNS baselines.

SC

MIS

CA

MC

Methods

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

SCIP
SCIP∗
SCIP∗∗
U-LNS
R-LNS
FT-LNS
Ours

567.66 ± 8.76
552.82 ± 8.69
550.68 ± 8.60
568.60 ± 12.17
560.54 ± 8.07
564.00 ± 8.03
551.50 ± 8.59

3.62
0.91
0.53
5.99
2.38
3.02
0.68

-681.02 ± 1.14
-681.76 ± 1.06
-682.46 ± 1.02
-681.38 ± 0.95
-682.20 ± 0.93
-681.82 ± 0.93
-682.52 ± 0.98

0.29
0.18
0.07
0.23
0.11
0.17
0.06

-110181 ± 2.03
-111511 ± 1.85
-112638 ± 1.68
-103717 ± 1.92
-109550 ± 1.62
-107370 ± 2.03
-112666 ± 1.72

2.98
1.85
0.82
8.67
3.44
5.45
0.77

-852.57 ± 1.22
-861.10 ± 1.26
-863.63 ± 1.32
-869.20 ± 1.53
-882.18 ± 1.27
-867.05 ± 1.64
-889.61 ± 1.32

4.37
3.41
3.13
2.50
1.05
2.75
0.27

1 ∗ and ∗∗ mean the method run with 500s and 1000s.

Table 3: Generalization to large instances.

SC2

MIS2

CA2

MC2

Methods

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

SCIP
SCIP∗
SCIP∗∗
U-LNS
R-LNS
FT-LNS
Ours

303.18 ± 8.62
298.12 ± 8.08
295.70 ± 7.89
303.36 ± 8.24
300.84 ± 7.74
303.52 ± 7.86
297.90 ± 8.20

SC4

6.80
5.04
4.21
6.90
6.06
6.98
4.98

-1323.90 ± 0.81
-1357.04 ± 1.34
-1361.98 ± 1.06
-1364.66 ± 0.69
-1339.00 ± 0.81
-1345.58 ± 0.86
-1367.78 ± 0.68

MIS4

3.22
0.80
0.44
0.24
2.12
1.63
0.01

-205542 ± 2.87
-214654 ± 1.44
-217271 ± 1.93
-197453 ± 1.86
-204145 ± 1.57
-212264 ± 1.35
-216006 ± 1.15

CA4

7.14
3.02
1.84
10.79
7.77
4.10
2.40

-1691.48 ± 1.18
-1706.45 ± 1.26
-1714.71 ± 1.02
-1769.00 ± 1.03
-1767.09 ± 1.00
-1700.58 ± 1.64
-1803.71 ± 0.92

MC4

6.22
5.39
4.93
1.92
2.03
5.72
0.00

Methods

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

SCIP
SCIP∗
SCIP∗∗
U-LNS
R-LNS
FT-LNS
Ours

179.88 ± 6.35
177.44 ± 6.64
175.38 ± 6.98
196.60 ± 10.13
188.02 ± 7.13
179.40 ± 8.47
176.84 ± 7.42

6.37
4.91
3.68
16.25
11.24
6.04
4.57

-2652.56 ± 0.58
-2652.56 ± 0.58
-2673.64 ± 1.54
-2653.42 ± 0.63
-2683.30 ± 0.59
-2684.94 ± 0.81
-2735.86 ± 0.50

3.05
3.05
2.28
3.02
1.93
1.87
0.00

-372291 ± 1.22
-372291 ± 1.22
-372291 ± 1.22
-419973 ± 1.11
-427478 ± 0.95
-424052 ± 0.96
-428052 ± 0.12

13.44
13.44
13.44
1.41
0.61
1.41
0.49

-3392.02 ± 0.86
-3392.94 ± 0.82
-3394.42 ± 0.81
-3522.67 ± 0.80
-3521.90 ± 0.82
-3525.28 ± 0.83
-3587.72 ± 0.76

5.45
5.43
5.39
1.81
1.83
1.74
0.00

Following [21], we tune the group number of R-LNS (and FT-LNS since it imitates R-LNS) from
2 to 5, and apply the best one to each problem. To train FT-LNS, we collect 10 demonstrations for
each instance, and tune the step limit to 20 for SC, MIS, CA and 50 for MC, which perform the
best. Same as our method, all LNS baselines also use SCIP as the repair operator with 2s time limit.
To compare solution quality, we use the average objective value and standard deviation over the 50
testing instances as metrics. Also, since all problems are too large to be solved optimally, we measure
the primal gap [19] to reﬂect the difference between the solution ˜x of a method to the best one x∗
found by all methods. We compute |µ(cid:62) ˜x − µ(cid:62)x∗|/max{|µ(cid:62) ˜x|, |µ(cid:62)x∗|} · 100% for each instance,
then average the gaps for all 50 ones. Below we report the results on testing instances of the same
sizes as in training.

In this paper, we aim to improve an IP solver from externals to enable more efﬁcient search of high-
quality solutions in a broad range of COPs. To this end, we compare all methods for time-bounded
optimization with the same 200s time limit, and further allow SCIP to run for longer time, i.e., 500s
and 1000s. The results are gathered in Table 2. As shown, our method signiﬁcantly outperforms
all baselines on all problems with the same 200s time limit. It is notable that FT-LNS is inferior to
R-LNS which yields demos for its imitation learning. The reason might be that FT-LNS only mimics
the random demos of short (training) step limits, and hence lacks the ability of generalizing to longer
steps. This limitation might hinder its application since in practice, IP problems are often solved in an
anytime manner with ﬂexible time/step limits. In contrast, our method avoids this myopic issue by RL
training. In Appendix A.5, we also show that FT-LNS can outperform R-LNS with the same number
of LNS steps as in training. Another key observation from Table 2 is that with longer time limits,
SCIP is able to ﬁnd better solutions than the three LNS baselines on SC, MIS and CA. However, our
method still surpasses SCIP (500s) on all problems and SCIP (1000s) on MIS, CA and MC.

8

Table 4: Generalization to large instances (500s).

SC2

MIS2

CA2

MC2

Methods
SCIP∗∗
U-LNS
R-LNS
FT-LNS
Ours

Methods
SCIP∗∗
U-LNS
R-LNS
FT-LNS
Ours

Obj.±Std.% Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

295.70 ± 7.89
302.94 ± 8.15
298.24 ± 7.43
303.34 ± 7.97
295.36 ± 7.81

4.48
7.04
5.39
7.18
4.36

-1361.98 ± 1.06
-1368.58 ± 0.72
-1362.04 ± 0.71
-1345.60 ± 0.83
-1368.68 ± 0.65

0.56
0.05
0.52
1.76
0.04

-217271 ± 1.93
-200256 ± 2.08
-207937 ± 1.44
-213464 ± 1.21
-218920 ± 2.13

1.59
9.28
5.81
3.30
0.85

-1714.71 ± 1.02
-1777.98 ± 1.01
-1776.44 ± 1.02
-1767.81 ± 1.04
-1813.02 ± 0.91

5.44
1.95
2.04
2.51
0.02

SC4

MIS4

CA4

MC4

Obj.±Std.% Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

175.38 ± 6.99
185.62 ± 8.19
172.96 ± 6.43
175.20 ± 6.59
172.38 ± 7.14

5.21
11.26
3.73
5.11
3.36

-2673.64 ± 1.54
-2737.24 ± 0.54
-2736.60 ± 0.52
-2685.30 ± 0.81
-2738.24 ± 0.50

2.41
0.09
0.11
1.97
0.04

-372291 ± 1.22
-426480 ± 0.93
-431786 ± 1.03
-431234 ± 0.91
-437880 ± 0.72

14.98
2.60
1.39
1.52
0.00

-3394.42 ± 0.81
-3556.69 ± 0.80
-3554.98 ± 0.80
-3526.24 ± 0.79
-3612.52 ± 0.74

6.04
1.55
1.59
6.10
0.00

5.2 Generalization analysis

Training deep models that perform well on larger problems is a desirable property for solving IPs,
since practical problems are often large-scale. Here we evaluate such generalization performance on
instances with the average sizes listed in Table 1. We run our method and baselines on these instances
with the same time limits as those in the experiments for comparative analysis. For our method and
FT-LNS, we directly apply the policies trained in Section 5.1.

All results are displayed in Table 3. It is revealed that with the same 200s time limit, while the LNS
baselines only outperform SCIP on speciﬁc problems, our LNS policies trained on small instances
are consistently superior to all baselines, showing a stronger generalization ability. Also, our method
delivers much smaller gaps, e.g., at least 38.35% smaller than that of SCIP (200s), which are more
prominent than the results in Section 5.1. It indicates that our policies are more efﬁcient in improving
SCIP for larger instances solely by generalization. When SCIP runs with 500s, it surpasses all three
LNS baselines on SC2, CA2 and SC4, while our method can still deliver better results on all problems.
Compared to SCIP with 1000s, our method is inferior on SC2, CA2 and SC4 but apparently better on
the remaining 5 instance groups.

We further test all methods with 500s time limit except SCIP, which is allowed to run with 1000s.
All results are gathered in Table 4. It is revealed that our method still has clear advantages over
others on all problems, and consistently outperforms SCIP with mere 1/2 runtime. We ﬁnd that LNS
baselines can outperform SCIP on some problems, especially the three largest ones, i.e., MIS4, CA4
and MC4. It suggests that as the problem size becomes larger, LNS could be more effective to deliver
high-quality solutions by solving successive sub-IPs which have much lower complexity than the
original problem. On the other hand, our method outperforms all LNS baselines, showing that it is
more efﬁcient in improving the solution. In summary, our LNS policies learned on small instances
generalize well to larger ones, with a persistent advantage over other methods.

5.3 Experiments with Gurobi

Our LNS framework is generally applicable to any IP solver. Here we evaluate its performance by
leveraging Gurobi (v9.0.3) [47] as the repair operator. Gurobi is a commercial solver and offers less
interfaces to the internal solving process. Thus, we condense the static features of variables to mere
objective coefﬁcients, and attain the initial solution by running the solver with 2s time limit. During
training, we set the step limit T =50 with 1s time limit for Gurobi at each step. For FT-LNS, we use
the same 1s time limit, and tune the group number to 2 in all problems for its best performance. The
remaining settings are the same as those with SCIP. During testing, we let all methods run with 100s
time limit, roughly twice as much as that for training. To save space, we only show results of two
instance groups for each problem in Table 5.4 Despite the shorter runtime, we ﬁnd that Gurobi (100s)
generally attains lower objective values than SCIP (200s), showing a better performance to solve IP

4Specially, we observe that MIS is fairly easy for Gurobi (76 out of 100 instances can be solved optimally

with average 40s). Thus, we evaluate this problem with less time limit and show the results in Appendix A.6.

9

Table 5: Results with Gurobi. The left part shows the results of inference on the testing set in SC, CA
and MC; the right part shows the results of generalization to larger instances in SC2, CA2 and MC2.

SC

CA

MC

SC2

CA2

MC2

Methods Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap%

Gurobi 554.94 ± 8.34 1.15 -111668 ± 1.96 1.10 -863.91 ± 3.77 3.31 302.52 ± 7.73 2.43 -214271 ± 1.52 3.63 -1652.83 ± 3.63 5.81
U-LNS 562.08 ± 8.16 2.46 -110402 ± 1.67 2.21 -862.59 ± 1.75 3.45 301.48 ± 7.62 2.07 -218986 ± 1.42 1.51 -1733.57 ± 1.22 1.21
R-LNS 563.98 ± 8.29 2.81 -110230 ± 1.56 2.36 -860.22 ± 1.96 3.72 302.86 ± 7.41 2.56 -219462 ± 1.16 1.29 -1723.75 ± 1.32 1.77
FT-LNS 564.14 ± 8.37 2.84 -110041 ± 1.56 2.53 -866.22 ± 1.65 3.03 348.50 ± 9.05 17.99 -206189 ± 1.39 7.26 -1726.07 ± 1.19 1.64
Ours 551.88 ± 8.31 0.59 -111787 ± 2.60 1.00 -888.97 ± 1.55 0.50 297.70 ± 7.40 0.80 -222346 ± 1.35 0.00 -1752.98 ± 1.21 0.11

problems. Hence, the LNS baselines lose their advantages over the solver on several problems, e.g.,
SC, CA and SC2. In contrast, our method outperforms Gurobi across all problems, showing good
performance on instances of both training and generalization sizes. Moreover, our method can be
well applied to much larger instances and we provide this evaluation in Appendix A.7.

5.4 Testing on MIPLIB

The mixed integer programming library (MIPLIB) [48] contains real-world COPs from various
domains. Since the instances in MIPLIB are severely diverse in problem types, structures and sizes,
it is not a very suitable testing set to directly apply learning based models and thus seldom used in
the related works. We evaluate our method (with Gurobi as the repair solver) on this realistic dataset,
in the style of active search on each instance [5, 6], and compare it to SCIP and Gurobi. Results show
that: 1) with the same 1000s time limit, our method is superior to both solvers on 24/35 instances and
comparable to them on 9/35 instances; 2) our method with 1000s time limit outperforms both solvers
with 3600s time limit on 13/35 instances; 3) for an open instance, we ﬁnd a better solution than the
best known one. More details are provided in Appendix A.8.

6 Conclusions and future work

We propose a deep RL method to learn LNS policy for solving IP problems in bounded time. To
tackle the issue of large action space, we apply action factorization to represent all potential variable
subsets. On top of it, we design a parameter-sharing GNN to learn policies for each variable, and
train it by a customized actor-critic algorithm. Results show that our method outperforms SCIP
with much less time, and signiﬁcantly surpasses LNS baselines with the same time. The learned
policies also generalize well to larger problems. Furthermore, the evaluation of our method with
Gurobi reveals that it can effectively improve this leading commercial solver. For limitations, since
we mainly aim to reﬁne off-the-shelf solvers for general IP problems, it is not sufﬁcient to conclude
that our method can transcend specialized and highly-optimized algorithms in different domains. In a
practical view, our method could be a choice when new IP problems are produced with little expertise,
or extensive dependence on domain knowledge is expected to be avoided. Also, our LNS policies
are more suitable to improve solvers for large-scale problems in bounded time, but cannot provide
optimality guarantee. For future work, we will apply our method to other (mixed) IP problems, and
extend it by combining with other learning techniques for IPs, such as learning to branch.

Acknowledgments

This research was conducted at Singtel Cognitive and Artiﬁcial Intelligence Lab for Enterprises
(SCALE@NTU), which is a collaboration between Singapore Telecommunications Limited (Singtel)
and Nanyang Technological University (NTU) that is supported by A*STAR under its Industry
Alignment Fund (LOA Award number: I1701E0013). Wen Song was supported by the National
Natural Science Foundation of China under Grant 62102228, and the Young Scholar Future Plan of
Shandong University under Grant 62420089964188. Zhiguang Cao was supported by the National
Natural Science Foundation of China under Grant 61803104.

10

References

[1] Vangelis Th Paschos. Applications of combinatorial optimization. John Wiley & Sons, 2013.

[2] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and

complexity. Courier Corporation, 1998.

[3] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial
optimization: a methodological tour d’horizon. European Journal of Operational Research,
290(2):405–421, 2020.

[4] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proceedings of the
29th Conference on Neural Information Processing Systems (NIPS), pages 2692–2700, 2015.

[5] Irwan Bello and Hieu Pham. Neural combinatorial optimization with reinforcement learning.

In the 5th International Conference on Learning Representations (ICLR), 2017.

[6] Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-
timization algorithms over graphs. In Proceedings of the 31st Conference on Neural Information
Processing Systems (NIPS), pages 6348–6358, 2017.

[7] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In

the 7th International Conference on Learning Representations (ICLR), 2019.

[8] Liang Xin, Wen Song, Zhiguang Cao, and Jie Zhang. Multi-decoder attention model with
embedding glimpse for solving vehicle routing problems. In Proceedings of the 35th AAAI
Conference on Artiﬁcial Intelligence, page 12042–12049, 2021.

[9] Jingwen Li, Yining Ma, Ruize Gao, Zhiguang Cao, Andrew Lim, Wen Song, and Jie Zhang.
Deep reinforcement learning for solving the heterogeneous capacitated vehicle routing problem.
IEEE Transactions on Cybernetics, 2021.

[10] Jingwen Li, Liang Xin, Zhiguang Cao, Andrew Lim, Wen Song, and Jie Zhang. Heterogeneous
attentions for solving pickup and delivery problem via deep reinforcement learning. IEEE
Transactions on Intelligent Transportation Systems, 2021.

[11] Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Chi Xu. Learning to
dispatch for job shop scheduling via deep reinforcement learning. In Proceedings of the 34th
Advances in Neural Information Processing Systems (NIPS), pages 1621–1632, 2020.

[12] Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial
optimization. In Proceedings of the 33rd Conference on Neural Information Processing Systems
(NIPS), pages 6278–6289, 2019.

[13] Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. Learning improvement
heuristics for solving routing problems. IEEE Transactions on Neural Networks and Learning
Systems, 2021.

[14] Hao Lu, Xingwen Zhang, and Shuang Yang. A learning-based iterative method for solving
vehicle routing problems. In the 8th International Conference on Learning Representations
(ICLR), 2020.

[15] André Hottung and Kevin Tierney. Neural large neighborhood search for the capacitated vehicle
routing problem. In Proceedings of the 24th European Conference on Artiﬁcial Intelligence
(ECAI), 2020.

[16] Lei Gao, Mingxiang Chen, Qichang Chen, Ganzhong Luo, Nuoyi Zhu, and Zhixin Liu. Learn
to design the heuristics for vehicle routing problem. arXiv preprint arXiv:2002.08539, 2020.

[17] Ailsa H Land and Alison G Doig. An automatic method for solving discrete programming

problems. In 50 Years of Integer Programming 1958-2008, pages 105–132. Springer, 2010.

[18] He He, Hal Daume III, and Jason M Eisner. Learning to search in branch and bound algorithms.
In Proceedings of the 27th Conference on Neural Information Processing Systems (NIPS), pages
3293–3301, 2014.

11

[19] Elias B Khalil, Bistra Dilkina, George L Nemhauser, Shabbir Ahmed, and Yufen Shao. Learning
to run heuristics in tree search. In Proceedings of the 26th International Joint Conference on
Artiﬁcial Intelligence (IJCAI), pages 659–666, 2017.

[20] Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact
combinatorial optimization with graph convolutional neural networks. In Proceedings of the
33rd Conference on Neural Information Processing Systems (NIPS), 2019.

[21] Jialin Song, Ravi Lanka, Yisong Yue, and Bistra Dilkina. A general large neighborhood search
framework for solving integer linear programs. In Proceedings of the 34th Conference on
Neural Information Processing Systems (NIPS), 2020.

[22] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer program-
ming: Learning to cut. In International Conference on Machine Learning (ICML), pages
9367–9376, 2020.

[23] Jian-Ya Ding, Chao Zhang, Lei Shen, Shengyin Li, Bing Wang, Yinghui Xu, and Le Song.
Accelerating primal solution ﬁndings for mixed integer programs based on solution prediction.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 1452–1459,
2020.

[24] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov,
Brendan O’Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al.
Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.

[25] Jason Pazis and Ronald Parr. Generalized value functions for large action sets. In Proceedings
of the 28th International Conference on Machine Learning (ICML), pages 1185–1192, 2011.

[26] Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

reinforcement learning.
volume 32, 2018.

[27] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, and Timothy Lillicrap.
Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679,
2015.

[28] Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning
action representations for reinforcement learning. In Proceedings of the 36th International
Conference on Machine Learning (ICML), pages 941–950, 2019.

[29] Yunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy opti-
mization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages
5981–5988, 2020.

[30] Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon
Schmitt, and David Silver. Learning and planning in complex action spaces. arXiv preprint
arXiv:2104.06303, 2021.

[31] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob Mc-
Grew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3–20,
2020.

[32] David Pisinger and Stefan Ropke. Large neighborhood search. In Handbook of metaheuristics,

pages 399–419. Springer, 2010.

[33] Stefan Ropke and David Pisinger. An adaptive large neighborhood search heuristic for the
pickup and delivery problem with time windows. Transportation science, 40(4):455–472, 2006.

[34] Eric Prescott-Gagnon, Guy Desaulniers, and Louis-Martin Rousseau. A branch-and-price-
based large neighborhood search algorithm for the vehicle routing problem with time windows.
Networks: An International Journal, 54(4):190–204, 2009.

12

[35] Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforce-
ment learners in cooperative markov games: a survey regarding coordination problems. The
Knowledge Engineering Review, 27(1):1–31, 2012.

[36] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018.

[37] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 32(1):4–24, 2012.

[38] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional
networks. In the 5th International Conference on Learning Representations (ICLR), 2017.

[39] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli,
Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for
learning molecular ﬁngerprints. In Proceedings of the 29th Conference on Neural Information
Processing Systems (NIPS), pages 2224–2232, 2015.

[40] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classi-
ﬁcation. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages
7370–7377, 2019.

[41] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning, 8(3-4):229–256, 1992.

[42] Egon Balas and Andrew Ho. Set covering algorithms using cutting planes, heuristics, and
subgradient optimization: a computational study. In Combinatorial Optimization, pages 37–60.
Springer, 1980.

[43] David Bergman, Andre A Cire, Willem-Jan Van Hoeve, and John Hooker. Decision diagrams

for optimization. Springer, 2016.

[44] Kevin Leyton-Brown, Mark Pearson, and Yoav Shoham. Towards a universal test suite for
combinatorial auction algorithms. In Proceedings of the 2nd ACM conference on Electronic
commerce, pages 66–76, 2000.

[45] Réka Albert and Albert-László Barabási. Statistical mechanics of complex networks. Reviews

of modern physics, 74(1):47, 2002.

[46] Tobias Achterberg. Scip: solving constraint integer programs. Mathematical Programming

Computation, 1(1):1–41, 2009.

[47] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2021. URL http://www.

gurobi.com.

[48] Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe,
Timo Berthold, Philipp M. Christophel, Kati Jarck, Thorsten Koch, Jeff Linderoth, Marco
Lübbecke, Hans D. Mittelmann, Derya Ozyurt, Ted K. Ralphs, Domenico Salvagnin, and Yuji
Shinano. MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer Programming
Library. Mathematical Programming Computation, 2021. doi: 10.1007/s12532-020-00194-3.
URL https://doi.org/10.1007/s12532-020-00194-3.

13

Learning Large Neighborhood Search Policy for
Integer Programming (Appendix)

A.1 Architecture of bipartite GCN

In this paper, we propose to factorize the selection of a variable subset into decisions on selection
of each variable, under our LNS framework. To represent such action factorization, we employ
the bipartite GCN as the destroy operator, as shown in Figure A.1. In speciﬁc, the bipartite GCN
comprises two stacks of graph convolution layers to compute the embeddings of variables, and one
MLP module that computes probabilities of selecting each variable in parallel.

Figure A.1: Illustration of our LNS framework with the bipartite GCN based destroy operator.

A.2 Training details

Our RL algorithm for training LNS policies is depicted by the pseudo code in Algorithm 1. Compared
to the standard actor-critic algorithm, we use experience replay to empower the reuse of past samples
(lines 2-8). In addition, we customize the standard Q-actor-critic algorithm for the proposed action
factorization, by specializing the loss functions.

Algorithm 1: Customized Q-actor-critic for LNS

Input: actor πθ with parameters θ; critic Qω with parameters ω; empty reply buffer D; number of

iterations J ; step limit T ; number of updates U ; learning rates αθ, αω; discount factor γ.

1 for j = 1, 2, · · · , J do
2

draw M training instances;
for m = 1, 2, · · · , M do

3

4

5

6

7

8

9

10

11

for t = 1, 2, · · · , T do
t ∼ πθ(ai

t|st), derive the union at ;

sample ai
receive reward rt and next state st+1;
sample ai
t+1|st+1), derive at+1;
store transition (st, at, rt, st+1, at+1) in D;

t+1 ∼ πθ(ai

for u = 1, 2, · · · , U do

randomly sample a batch of transitions B from D;
update the parameters of actor and critic with yt = γQω(st+1, at+1) + rt;
t|st));

i=1 log(πθ(ai

(cid:80)

1
ω ← ω + αω∇ω
|B|
1
θ ← θ + αθ∇θ
|B|

(cid:80)

B(yt − Qω(st, at))2; zt = (cid:80)n
B Qω(st, at)zt;

14

Current solution value  Variable to optimize  Reoptimizedsolution value  𝑘-thgraph convolution layer Variable NodeConstraint Node𝑎11𝑎12𝑎22𝑎23𝑎24𝑎34𝑎11𝑎12𝑎22𝑎23𝑎24𝑎34𝑐2𝑐3𝑐1𝑉(𝑘)𝐶(𝑘+1)𝑉(𝑘+1)𝜇1𝑥1+𝜇2𝑥2+𝜇3𝑥3+𝜇4𝑥4𝑎11𝑥1+𝑎12𝑥2+𝑎13𝑥3+𝑎14𝑥4≤𝑏1min.𝑣1𝑎21𝑥1+𝑎22𝑥2+𝑎23𝑥3+𝑎24𝑥4≤𝑏2𝑎31𝑥1+𝑎32𝑥2+𝑎33𝑥3+𝑎34𝑥4≤𝑏3𝑐2𝑐3𝑐1𝑣2𝑣3𝑣4Variableswith features 𝑉Linear projection to 𝑉(0)Linear projection to 𝐶(0)Constraints with features 𝐶MLP𝝅(𝒂𝒕|𝒔𝒕)4×1283×1284×1284×1SampleBipartite GCN (destroy operator)Environment Update dynamic featuresSOLVER(repair operator)IP formulation𝑥1𝑥2𝑥3𝑥4𝑣1𝑣2𝑣3𝑣4𝑣1𝑣2𝑣3𝑣4IP formulation𝑉(𝐾)4×128𝑥1𝑥2𝑥3𝑥4State 𝑠𝑡(𝑘=0,…,𝐾)A.3 Semi-shared vs. fully-shared network

As stated in the main paper, our method can be employed to train any kind of policy network that is
able to represent the factorized action. We choose the fully-shared neural network since it can be
generalized to different problem sizes, which is critical to solve IP problems. Nevertheless, we also
experiment with a semi-shared neural network to show its performance on problem instances of the
ﬁxed size. The architecture of the neural network is displayed in the upper half of Figure A.2, which
is similar to the one used for DQN based RL algorithms in [26]. Speciﬁcally, given a collection of
features for each variable, we ﬁrst process them by a MLP to obtain variable embeddings. Then,
these embeddings are averaged and advanced by another MLP to obtain the state embedding. Lastly,
we concatenate each variable embedding with the state embedding and pass them through n MLPs
separately to get probabilities of n variables. For the fully-shared counterpart, we only replace the n
MLPs by a parameter-sharing MLP, as shown in the lower half of Figure A.2. All layers in MLPs are
activated by Tanh except the ﬁnal output by Sigmoid.

Following the experimental setting for SCIP in
the main paper, we train both the fully-shared
and semi-shared networks on SC, MIS, CA and
MC, with the customized actor-critic algorithm
we designed. We evaluate the average objective
value over the validation set after each training
iteration. All training curves of initial 50 itera-
tions are displayed in Figure A.3. We ﬁnd that
the fully-shared network is able to learn efﬁ-
ciently on SC, CA and MC, while semi-shared
network performs better on MIS. It indicates that
our training algorithm can be applied to different
kinds of policy networks, and the fully-shared
network is more effective in learning LNS poli-
cies for IP problems. For the semi-shared net-
work, despite the good performance with rela-
tively low-dimensional action spaces in [26], it
needs far more sub-networks in our RL tasks
with thousands of action dimensions, which are
intractable to train together and also prevent the
generalization to different problem sizes.

Figure A.2: Architectures of the semi-shared and
fully-shared policy networks.

(a) SC

(b) MIS

(c) CA

(d) MC

Figure A.3: Training curves of the semi-shared and fully-shared networks.

A.4 State features

In this paper, we represent the state by a bipartite graph G = (V, C, A) attached by the features of
variables, constraints and edges (i.e. V, C and A), which are listed in Table A.1. The logic behind
this design is to reﬂect the instance information and the dynamic solving status, both of which are
critical to learn effective policies in LNS. For the static features, we consider the ones used in [20],
which learns variable selection policies in B&B algorithm. It has been shown that these features
have the potential to predict variables for branching. From the perspective of learning LNS, we only
extract these features at the root node as the instance information. Also, we preprocess these features

15

in two ways: 1) we delete the ones with zero variance from the features of variables, which are the
same constant across all training instances; 2) we only use the right-hand-side (RHS) vector as the
features for constraints. For dynamic features, we consider the efﬁciency of the solving process and
directly record values of the current solution and incumbent at each step of LNS. These dynamic
features are linked with the static features of variables and then attached to variable nodes V.

Table A.1: The list of features for variables, constraints and edges. S. and D. denote the static and
dynamic attribute, respectively.

Feature Types
Variable features (V)

Constraint features (C)
Edge features (A)

Description
Normalized reduced cost.
Normalized objective coefﬁcient.
Normalized LP age.
Equality of solution value and lower bound, 0 or 1.
Equality of solution value and upper bound, 0 or 1.
Fractionality of solution value.
One-hot encoding of simplex basis status (i.e., lower,
basic, upper).
Solution value at root node.
Solution value at the current step.
Value in the incumbent.
Average value in historical incumbents.
Constraint right-hand side.
Coefﬁcient in incidence matrix.

Length
1
1
1
1
1
1

3

1
1
1
1
1
1

S./D.
S.
S.
S.
S.
S.
S.

S.

S.
D.
D.
D.
S.
S.

A.5 Comparison of short-term performance

In the main paper, we found that FT-LNS cannot outperform R-LNS with the long time limit due to
its poor generalization to unseen states. However, according to [21], FT-LNS can outperform R-LNS
with relatively short time limit, i.e. the similar runtime used in training of FT-LNS. To verify this
point, we compare our method and LNS baselines with such setting. Speciﬁcally, we ﬁrst test FT-LNS
with the same number of LNS steps as in its training, and record its runtime. Then we test our method
and other baselines using the runtime of FT-LNS as time limit. All results are summarized in Table
A.2. As shown, FT-LNS can indeed surpass R-LNS on all problems, which indicates its effectiveness
in short-term improvement and is consistent with [21]. Nevertheless, for these experiments with short
time limits, it is clear that our method still consistently outperforms all LNS baselines with smaller
gaps across all problems.

Table A.2: Results with short time limits.

SC

MIS

CA

MC

Methods Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap% Obj.±Std.% Gap%

SCIP
U-LNS
R-LNS
FT-LNS
Ours

586.72 ±9.14
615.78±12.68
588.70±8.54
578.38±9.23
575.80±8.94

3.38
12.44
3.79
1.89
1.46

-659.78±1.14
-660.30±1.13
-667.54±1.14
-670.48±1.13
-673.78±0.11

2.23
2.15
1.08
0.64
0.15

-93715±2.64
-99302±2.75
-98705±1.93
-100323±2.00
-100867±2.11

8.34
2.77
3.37
1.89
1.37

-840.68±1.49
-852.39±1.65
-851.57±1.45
-865.01±1.72
-872.47±1.24

3.93
2.59
2.68
1.15
0.30

A.6 Testing on MIS with Gurobi

On the training set of MIS, Gurobi solves most instances optimally with 40s in average. As stated in
the main paper, our method aims to improve solvers in bounded time and cannot guarantee optimality.
Thus, in contrast to 100s time limit for other problems, we evaluate all methods on MIS with 20s.
The other experimental settings follow those in Section 5.3. The results are displayed in Table A.3.
As shown, our method effectively improves Gurobi to achieve smaller gaps, although it is already
able to deliver high-quality solutions quickly. Also, our method is consistently superior to baselines
on all instances groups, demonstrating good generalization to different-sized problems.

16

Table A.3: Results on MIS with Gurobi.

MIS

MIS2

MIS4

Methods

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Obj.±Std.%

Gap%

Gurobi
U-LNS
R-LNS
FT-LNS
Ours

682.22 ± 1.06
682.02 ± 0.95
681.82 ± 0.98
682.20 ± 0.94
682.24 ± 0.94

0.08
0.11
0.14
0.09
0.08

-1359.54 ± 0.84
-1359.36 ± 0.74
-1365.64 ± 0.67
-1358.86 ± 0.77
-1367.48 ± 0.65

0.59
0.61
0.15
0.64
0.02

-2645.88 ± 1.10
-2723.44 ± 0.52
-2722.60 ± 0.52
-2722.18 ± 0.56
-2724.08 ± 0.52

2.96
0.12
0.15
0.16
0.09

A.7 Generalization with Gurobi

Here we further evaluate the generalization of our LNS framework with Gurobi as the repair operator.
We test all methods with 100s time limit, same as in Section 5.3. For FT-LNS and our method, the
learned policies on small instances are directly used. The results are summarized in Table A.4. We
can observe that while our method is slightly inferior to U-LNS and R-LNS on SC4, it can still
generalize well to much larger instances on the problems CA4 and MC4, and outperform all baselines.
This indicates that our method has a good generalization ability to improve Gurobi, the leading
commercial solver, for solving instances of different scales.

Table A.4: Generalization to large instances with Gurobi.

Methods

Gurobi
U-LNS
R-LNS
FT-LNS
Ours

SC4
Obj.±Std.%

183.60 ± 7.29
176.84 ± 6.89
176.08 ± 6.47
201.14 ± 9.65
177.66 ± 6.65

Gap%

5.30
1.43
1.01
15.35
1.92

CA4
Obj.±Std.%

-377557 ± 0.85
-436224 ± 1.12
-435669 ± 0.67
-395027 ± 3.56
-437735 ± 0.80

Gap%

13.87
0.49
0.62
9.89
0.15

MC4
Obj.±Std.%

-3373.11 ± 1.05
-3388.31 ± 0.73
-3389.47 ± 0.70
-3373.20 ± 2.31
-3390.32 ± 0.73

Gap%

1.46
1.02
0.98
1.48
0.95

A.8 Testing on real-world instances in MIPLIB

In this appendix, we provide details of the experiment on real-world instances in MIPLIB. These
instances are grouped into "easy", "hard" and "open", according to their difﬁculties to solve. Since our
method is more suitable for large-scale IP problems, we ﬁlter out the "easy" instances with relatively
small sizes. We also ﬁlter out those instances that both SCIP and Gurobi cannot ﬁnd any feasible
solution with 3600s time limit, and ﬁnally choose 35 representative "hard" or "open" instances with
only integer variables. Among the chosen instances, the number of variables ranges from 150 to
393800 (the average is 49563), and the number of constraints range from 301 to 850513 (the average
is 96778). Also, these instances cover the typical application of COP from distinct domains, e.g.,
vehicle routing, cryptographic research and wireless network. To cope with such heterogeneous
problems, we employ our method in the active search mode. Speciﬁcally, we apply the customized
Q-actor-critic in Algorithm 1 to each instance, with only two instances solved in each iteration, i.e.,
M = 2. In doing so, we can save computation memory and also raise the frequency of training. We
use Gurobi as the repair operator and set its time limit to 2s in each LNS step. In addition, we set the
step limit T =100, number of updates U =10, and batch size B=32. For the initial solution, we use the
one returned by Gurobi with 100s time limit. We set the whole time limit of active search to 1000s,
and compare the results of SCIP and Gurobi with 1000s and 3600s time limits. The other settings
follow those in Section 5.3.

All results are displayed in Table A.5. As shown, the proposed LNS framework can improve the
solver effectively, and achieve better solutions than SCIP and Gurobi for most instances with the
same or less runtime. Moreover, for the open instance "neos-3682128-sandon", we managed to ﬁnd a
new best solution.

17

Table A.5: Results on MIPLIB. The "BKS" column lists the best know solutions given in MIPLIB.
Bold and * mean our method outperforms the solvers with 1000s and 3600s respectively. "-" means
no feasible solution is found.

Instance

SCIP (1000s)

SCIP (3600s)

Gurobi (1000s)

Gurobi (3600s)

Ours (1000s)

BKS

a2864-99blp
bab3
bley_xs1noM
cdc7-4-3-2
comp12-2idx
ds
ex1010-pi
graph20-80-1rand
graph40-20-1rand
neos-3426085-ticino
neos-3594536-henty
neos-3682128-sandon
ns1828997
nursesched-medium-hint03
opm2-z12-s8
pb-grow22
proteindesign121hz512p9
queens-30
ramos3
rmine13
rmine15
rococoC12-010001
s1234
scpj4scip
scpk4
scpl4
sorrell3
sorrell4
sorrell7
supportcase2
t1717
t1722
tokyometro
v150d30-2hopcds
z26

-71
-
5227928.57
-230
-
509.5625
254
-1
-1
234
410578
40971070.0
43
8074
-36275
0.0
-
-33
242
-611.536750
-759.289522
44206.0
319
141
346
296
-11
-18
-152
-
236546
138927
-
42
-1029

-71
-
5227928.57
-230
676
461.9725
248
-1
-1
232
410578
35804070.0
32
8074
-38015
0.0
-
-39
242
-611.536750
-759.289522
38905.0
319
141
342
296
-15
-18
-152
-
228907
138927
33134.6
41
-1029

-72
-654709.9511
3999391.53
-253
416
309
241
-3
0
226
402572
34674767.94751
145
144
-33269
-31152.0
1499
-39
252
-2854.351313
-192.372262
35463
41
134
331
281
-16
-22
-183
397
201342
123984
8493.3
41
-1005

-72
-655388.1120
3938322.37
-257
380
177
239
-6
-15
226
401948
34666770.0
133
115
-33269
-46217.0
1499
-39
245
-3493.781904
-1979.559046
34673
29
133
330
279
-16
-23
-187
397
201342
117171
8479.5
41
-1083

-72
-654912.9204
3975481.35
-276*
363*
319
238*
-6
-14
226
402426
34666765.12338*
128*
131
-53379*
-46881.0*
1489*
-39
248
-3487.807859
-5001.279118*
35443
41
134
329*
281
-16
-23
-188*
397
195894*
117978
8582.70
41
-1171*

-257
-656214.9542
3874310.51
-289
291
93.52
235
-6
-15
225
401382
34666770
9
115
-58540
-342763.0
1473
-40
192
-3494.715232
-5018.006238
34270
29
128
321
262
-16
-24
-196
109137
158260
109137
8329.4
41
-1187

18

