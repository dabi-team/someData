0
2
0
2

y
a
M
6
1

]
I

A
.
s
c
[

2
v
0
3
0
7
0
.
5
0
9
1
:
v
i
X
r
a

Knowledge-Based Sequential Decision-Making
Under Uncertainty

Daoming Lyu (cid:63)

Auburn University, Auburn, AL 36849, USA
daoming.lyu@auburn.edu

Abstract. Deep reinforcement learning (DRL) algorithms have achieved
great success on sequential decision-making problems, yet is criticized for
the lack of data-eﬃciency and explainability. Especially, explainability of
subtasks is critical in hierarchical decision-making since it enhances the
transparency of black-box-style DRL methods and helps the RL practi-
tioners to understand the high-level behavior of the system better. To
improve the data-eﬃciency and explainability of DRL, declarative knowl-
edge is introduced in this work and novel algorithm is proposed based on
the symbolic representation. An experimental analysis on publicly avail-
able benchmarks validates the explainability of the subtasks and shows
that our method can outperform the state-of-the-art approach in terms
of data-eﬃciency.

1

Introduction

As shown in AlphaGo or autonomous driving, it is critical to making sequential
decisions. Reinforcement Learning is a type of machine learning that is good
at sequential decision-making problems. With the help of deep learning, deep
reinforcement learning (DRL) algorithms have made a lot of achievements on
sequential decision-making problems involving high-dimensional sensory inputs
such as Atari games[7]. However, there are some key issues in DRL approach, al-
though it can learn policies that are able to surpass the overall performance of a
professional human player. The ﬁrst issue is the data ineﬃciency–DRL approach
usually requires several millions of samples but still cannot learn long-horizon
sequential actions for problems with sparse feedback and delayed rewards, such
as Montezuma’s Revenge [7]. The second is about the explainability–the learn-
ing behavior based on the black-box neural network is nontransparent and hard
to explain and understand. In real applications of decision-making, however, it
is crucial to enable the system behavior to be explainable, in order to gain the
conﬁdence from the users and provide insights for their decision-making process
[3] with reasonable less data samples. To address the issues, we introduce the
declarative knowledge into DRL and propose a framework of Symbolic Deep
Reinforcement Learning (SDRL) by utilizing Symbolic Planning (SP) [2]. In ad-
dition, we propose the intrinsic goal, a measurement of plan quality based on an

(cid:63) This report is for Doctoral Consortium session only.

 
 
 
 
 
 
2

D. Lyu

internal utility function, to enable reward-driven planning. Therefore, the data
eﬃciency in the decision-making process is improved from the meaningful explo-
ration guided by symbolic planning [8,5], and the explainability of the agent’s
behavior in task level can be achieved by the white-box algorithm of planning
and reasoning with predeﬁned and human-readable symbolic knowledge.

2 SDRL Framework

Symbolic Deep Reinforcement Learning (SDRL) framework features a planner–
controller–meta-controller architecture, as shown in Fig.1, which takes charge
of subtask scheduling, data-driven subtask learning, and subtask evaluation,
respectively.

Fig. 1: Architecture of SDRL

We ﬁrst assume a symbolic representation is given by human experts, which
describes domain dynamics by causal rules. Note that the pre-deﬁned symbolic
representation is built for general-purpose. Next, we deﬁne the workﬂow as fol-
lows. The symbolic planner will generate high-level plans, i.e., a sequence of
subtasks, to meet its intrinsic goal. An intrinsic goal is a measurement on plan
quality, which approximates how much cumulative reward the plan may achieve.
We assume a pre-trained mapping function can associate each sensory input with
a symbolic state, i.e., performing symbol grounding, so that a set of options on
the problem MDP can be induced based on symbolic states and the mapping
function. We extend the reward structure of core MDP by introducing intrinsic
reward and extrinsic reward to facilitate two levels of learning tasks. The sub-
policies for the action level are learned using DRL algorithms based on intrinsic
reward, with pseudo-rewards to encourage the agent to learn skills to achieve
each subtask. As DRL continues, a metric is used to evaluate the competence of
learned sub-policies, such as the success ratio over a number of episodes, from
which extrinsic rewards is derived. When the sub-policy is learned and reliably
achieves the subtask, the extrinsic reward is equivalent to the environmental re-
ward. Using extrinsic rewards, meta-controller performs R-learning that reﬂects
the long-term average reward and gains the reward of selecting each subtask.

Knowledge-Based Sequential Decision-Making Under Uncertainty

3

The learned values are returned to the symbolic planner and are used to mea-
sure plan quality and propose new intrinsic goals for the planner to improve the
plan, by either exploring new subtasks or by sequencing learned subtasks that
supposedly can achieve higher rewards in the next iteration.

In this process, the components of planner, controllers, and meta-controller
cross-fertilize each other and eventually converge to an optimal symbolic plan
along with the learned subtasks. While our framework is generic enough so that
various planning and DRL techniques can be used, we instantiate our frame-
work using action language BC for planning and R-learning for meta-controller
learning.

3 Experiment

The proposed approach is evaluated on Taxi domain [1] and Montezuma’s Re-
venge [7]. Due to the space limitation, we skip the description of experimental set-
tings and the complete results. Here we only show some results of Montezuma’s
Revenge. The interested readers are referred to [6] for more details.

Fig. 2: Pre-deﬁned Locations or Objects

As shown in Fig.2 and Fig.3, we formulated domain knowledge of Mon-
tezuma’s Revenge in action language BC based on 6 pre-deﬁned locations or
objects: middle platform (mp), right door (rd), left of rotating skull (ls), lower
left ladder (lll), lower right ladder (lrl), and key (key). All 13 subtasks are
pre-deﬁned and shown in Table 1. Acutually, subtasks 1–10 can be successfully
learned during the experiments, but only 7 of them (1–7) were selected in the ﬁ-
nal solution. This can be explained by the extrinsic rewards derived from training
performance. For subtasks 11 – 13, they were shown to be too diﬃcult to learn
in our experiments and discarded by the planner due to poor extrinsic rewards.
In Fig.4 (Learning Curve), our method (SDRL) is compared with state-of-the-
art approach, hierarchical DQN (hDQN) [4]. The learning curve of SDRL shows
that the agent ﬁrst discovered the plan of collecting key after 0.5M samples by
sequencing subtasks 1–3. Intrinsically motivated planning encourages exploring

4

D. Lyu

% object declaration
location(mp;rd;ls;lll;lrl;key).
% dynamic causal law declaration
move(L) causes loc=L if location(L).
move(L) causes cost=L+Z if rho((at(L1)),move(L))=Z,

loc=L1,picked(key)=false.

move(L) causes cost=L+Z if rho((at(L1),picked(key)),

move(L))=Z,loc=L1,picked(key)=true.

inertial loc. inertial quality.
% static causal law declaration
picked(key)=true if loc=key.
nonexecutable move(key) if picked(key).
default rho((at(L1)),move(L))=10.
default rho((at(L1),picked(key)),move(L))=10.

Fig. 3: Montezuma’s Revenge in BC

subtask
MP to LRL, no key
LRL to LLL, no key
LLL to key, no key
key to LLL, with key

No.
1
2
3
4
5 LLL to LRL, with or without key
6 LRL to MP, with or without key
7
8 LRL to LS, with or without key
LS to key, with or without key
9
10
MP to RD, no key
11 LRL to key, with or without key
12
13

key to LRL, with key
LRL to RD, with key

MP to RD, with key

policy learned in optimal plan

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table 1: Subtasks for Montezuma’s Revenge

Fig. 4: Experimental Results on Montezuma’s Revenge

(a) Learning Curve

(b) Final Solution

untried subtasks, and by learning more subtasks to move to other locations, the

Knowledge-Based Sequential Decision-Making Under Uncertainty

5

agent ﬁnally converges to the maximal cumulative external reward of 400 around
1.5M samples by sequencing subtasks 1–7 (Fig.4 (Final Solution)). By compari-
son, hDQN cannot reliably achieve the score of 400 around 2.5M samples. The
shadow of the curves in Fig.4 (Learning Curve) represents the variance among
multiple runs, which shows our SDRL has a small variance and can lead to more
robust and stable learning.

4 Conclusion

This work demonstrates that by integrating symbolic planning with DRL for
decision-making, explicitly represented symbolic knowledge can be used to per-
form high-level symbolic planning based on intrinsic goal which leads to im-
proved task-level interpretability for DRL and data-eﬃciency. This framework
makes the ﬁnal solution converge to an optimal symbolic plan along with the
learned subtasks, bringing together the advantages of long-term planning capa-
bility with symbolic knowledge and end-to-end reinforcement learning. In the
future work, one promising direction is to investigate on subtask discovery and
we are working on this.

References

1. Barto, A., Mahadevan, S.: Recent advances in hierarchical reinforcement learning.

Discrete Event Systems Journal 13, 41–77 (2003)

2. Cimatti, A., Pistore, M., Traverso, P.: Automated planning. In: van Harmelen, F.,
Lifschitz, V., Porter, B. (eds.) Handbook of Knowledge Representation. Elsevier
(2008)

3. Gilpin, L.H., Bau, D., Yuan, B.Z., Bajwa, A., Specter, M., Kagal, L.: Explaining
explanations: An approach to evaluating interpretability of machine learning. arXiv
preprint arXiv:1806.00069 (2018)

4. Kulkarni, T.D., Narasimhan, K., Saeedi, A., Tenenbaum, J.: Hierarchical deep re-
inforcement learning: Integrating temporal abstraction and intrinsic motivation. In:
Advances in Neural Information Processing Systems. pp. 3675–3683 (2016)

5. Lu, K., Zhang, S., Stone, P., Chen, X.: Robot represention and reasoning with
knowledge from reinforcement learning. arXiv preprint arXiv:1809.11074 (2018)
6. Lyu, D., Yang, F., Liu, B., Gustafson, S.: Sdrl: Interpretable and data-eﬃcient deep

reinforcement learning leveraging symbolic planning. In: AAAI (2019)

7. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level con-
trol through deep reinforcement learning. Nature 518(7540), 529–533 (2015)

8. Yang, F., Lyu, D., Liu, B., Gustafson, S.: Peorl: Integrating symbolic planning
and hierarchical reinforcement learning for robust decision-making. In: International
Joint Conference of Artiﬁcial Intelligence (IJCAI) (2018)

