Performance Evaluation of Deep Learning Tools in
Docker Containers

Pengfei Xu
Department of Computer Science
Hong Kong Baptist University
Email: pengfeixu@comp.hkbu.edu.hk

Shaohuai Shi
Department of Computer Science
Hong Kong Baptist University
Email: csshshi@comp.hkbu.edu.hk

Xiaowen Chu
Department of Computer Science
Hong Kong Baptist University
Email: chxw@comp.hkbu.edu.hk

7
1
0
2

v
o
N
9

]

C
D
.
s
c
[

1
v
6
8
3
3
0
.
1
1
7
1
:
v
i
X
r
a

Abstract—With the success of deep learning techniques in
a broad range of application domains, many deep learning
software frameworks have been developed and are being updated
frequently to adapt to new hardware features and software
libraries, which bring a big challenge for end users and system
administrators. To address this problem, container techniques are
widely used to simplify the deployment and management of deep
learning software. However, it remains unknown whether con-
tainer techniques bring any performance penalty to deep learning
applications. The purpose of
this work is to systematically
evaluate the impact of docker container on the performance of
deep learning applications. We ﬁrst benchmark the performance
of system components (IO, CPU and GPU) in a docker container
and the host system and compare the results to see if there’s any
difference. According to our results, we ﬁnd that computational
intensive jobs, either running on CPU or GPU, have small
overhead indicating docker containers can be applied to deep
learning programs. Then we evaluate the performance of some
popular deep learning tools deployed in a docker container and
the host system. It turns out that the docker container will not
cause noticeable drawbacks while running those deep learning
tools. So encapsulating deep learning tool in a container is a
feasible solution.

I. INTRODUCTION
Ever since the great success of deep learning techniques
in many application domains, more and more deep learning
software tools have been developed by different research
institutions and companies for both academic research and
commercial use [1]. Popular tools like Caffe [2], CNTK [3],
MXNet [4], TensorFlow [5], Torch [6], etc. are still being
actively developed and their new versions are being released
frequently, which brings signiﬁcant software management
challenge to system administrators. It is even worse when
different tools or different versions of the same tool need to
be installed in a system that is shared by multiple users. A
practical solution to simplify the management of deep learning
tools is to make use of docker containers so that environmental
setting conﬂicts can be easily resolved by packaging a software
and its all required libraries into a single image [7]. Despite its
popularity in practical usage, there lacks a systematic analysis
on the performance overhead brought by docker containers for
deep learning tools. This paper aims to investigate the impact
of docker containers on the performance of deep learning tools.
A typical deep learning training workﬂow involves data
access from/to disk drives and intensive data processing on
CPUs and/or accelerators such as GPUs [8]. Therefore we

evaluate the performance of CPU, GPU, disk I/O, and the
overall deep learning training with and without dock container,
respectively. For CPU performance, we make use of two
classical and representative benchmarks, HPL and HPCG. For
GPU performance, a set of GPU programs are selected to test
different types of GPU operations. Disk I/O performance can
be another important factor when huge amount of data are
fed to neural networks during training process [9]. We test
I/O performance from several aspects, including I/O access
latency, random access throughput, and sequential access
throughput. At last, we evaluate the training performance of
ﬁve popular deep learning software tools with different neural
network models and datasets. Based on our experimental re-
sults, we ﬁnd that docker containers have negligible overhead
in computing-intensive tasks on both CPU and GPU. The I/O
performance of sequential access under the docker container
is found to be at the same level as the host system. When
it comes to random access, we observe even shorter response
time on docker container than on the host system using one
of the tested disk drives. This is because docker containers
can make better use of the NAND cache on the hard disk to
gain faster random data access. Since each factor mentioned
above has satisfactory results on docker, it is not surprising
to ﬁnd that running deep learning tools in docker containers
has negligible overhead compared to running on host systems
directly.

This paper is organized as follows. Background of deep
learning and docker containers and related work are introduced
in Section II. The design of our experiments is presented in
Section III. We show our experimental results and analysis in
Section IV. We conclude our work in Section V.

II. BACKGROUND

A. Deep Learning

Deep learning is a class of machine learning techniques
which powers great number of facets in our everyday life.
Deep neural networks are built of many processing layers
and are able to learn features from a mass of data with
various stages of abstraction [9]. This technology has many
applications like speech recognition [10] [11] [12] [13], image
recognition [14] [15] [16] [17], natural language processing
[18] [19] [20], and the list
is getting longer and longer.
Comparing with conventional machine leaning techniques,

 
 
 
 
 
 
deep learning has less limitation on the data fed to the
computer to learn [9]. But training a deep neural network for
a certain problem is not an easy task and it requires signiﬁcant
computational power.

To this end, many-core parallel processors like GPUs are
widely used to facilitate deep learning tasks [8]. Some stages
of deep learning process can be eventually mapped to linear al-
gebra operations which can usually be efﬁciently implemented
on parallel processors. As a matter of fact, many popular
deep learning software such as Caffe [2], CNTK [3], MXNet
[4], TensorFlow [5], and Torch [6], have all implemented the
support of GPUs whose performance is signiﬁcantly better
than CPUs [1].

B. Docker Container

Docker is a container virtualization technology which be-
haves similar to a light-weighted virtual machine, and it is
the most popular open source application-oriented approach
[21]. Docker isolates each independent container running on
the same instance of operating system by making use of
Linux kernel features like control groups and namespaces
[21]. A simple illustration of docker can be found in Figure
1. The simple architecture of docker leads to less overhead

Fig. 1: Docker Architecture

as compared to other multi-layered types of virtualization
technology. Each docker container encapsulates an application
and its required dependencies, and can be run on different
machines on top of a docker engine. Docker images can also
be easily shared and distributed once they have been built.

C. Related work

Docker as a light-weighted virtualization solution has drawn
attentions from the research community to explore its potential
in different applications. A few studies have done great jobs
in comparing the performance of docker with other types of
virtualization solution [22] [23] [24].

It has been shown that the docker container outperforms
QEMU [22] when running popular GPU gaming benchmark,
and it runs as good as native OS. The purpose of [22] is

indicates that

to study the possibilities of hosting cloud gaming service
using docker containers. We can only see the overall gaming
performance from it. Another work [25] also tests the per-
formance of GPUs in docker containers and VMs by running
the algorithm SGEMM which performs matrix multiplication
followed by an addition operation: C = αA × B + βC.
invoking GPUs in docker containers will
It
not cause much overhead. Others also put docker into high
performance computing conditions [24] and compare it with
virtual machines. Chung’s work [24] evaluates the perfor-
mance by running HPL benchmark which is compiled with
OpenMPI and OpenBLAS and run with different problem
sizes. According to their experimental results, docker container
manages to have better performance than VMs while using
less RAM. Docker also has better scalability than VMs. As
the number of VM instances and docker containers increases,
docker can keep its performance without dramatically losing
computing ability due to large overhead like VMs do. Different
from the previous work, we aim to quantitatively measure
the performance overhead caused by docker containers when
running deep learning software.

III. EXPERIMENTAL DESIGN

In this section, we will introduce the design of experiments
to compare the performance between using docker container
and without docker container. Four groups of experiments (i.e.,
CPU, GPU, disk I/O, and deep learning tools) are designed to
evaluate the performance differences of the same task running
in docker container environment and in host system directly.
To make a fair comparison, we make sure that the irrelevant
variables like compiler versions and software libraries are the
same in docker container and host system. All experiments are
performed on two hardware platforms: a desktop PC and a rack
server. Detailed information of our hardware conﬁgurations
can be found in Table I and II. All reported results are the
average of 20 runs unless otherwise speciﬁed.

Item

Model

Intel i7-6800K

CPU
Motherboard ASUS X99-A II
RAM
GPU1
GPU2
Hard Drive

Kingston 64G DDR4
Nvidia GTX TITAN X
Nvidia GTX 980
Seagate ST3000DM008 7200RPM HDD

TABLE I: Hardware Conﬁguration of Desktop PC Platform

Item

Model

CPU
RAM
Hard Drive Lenovo System X 10K RPM 600GB (SAS 12Gb)

Intel Xeon E5-2620 v3
Samsung 32G DDR4

TABLE II: Hardware Conﬁguration of Server Platform
(Lenovo x3650 M5)

Since we will perform our experiments with NVIDIA GPUs,
we use NVIDIA docker1 which is a thin wrapper on top
of docker. When we start the NVIDIA docker, it calls the
docker and relies on NVIDIA Docker plugin to load GPU
driver and communicate with GPUs directly. NVIDIA docker
only changes the behavior of docker run and docker create
commands as stated in its ofﬁcial document.

seen in real applications [27]. This benchmark program also
solves a linear system Ax = b, but with a conjugate gradient
method. As mentioned in [28], a system that is designed for
good HPL performance can result in wrong choices for adding
unnecessary components or complexity to the system. We run
HPCG with the problem dimension of 192 and running time
being 1800 seconds to get valid benchmark results.

A. CPU Performance

B. GPU Performance

In order to test the performance of CPU, we run both
HPL and HPCG benchmarks from Intel MKL library in Intel
Parallel Studio 2017 Update 2. Both of them are measured in
GFlops (Giga ﬂoating point operations per second).

For HPL benchmark, the problem sizes are set from 2,000 to
45,000, simulating different levels of computational intensity.
The detailed experimental settings are shown in Table III.
HPL is originated from Linpack benchmark that measures

Problem Size Leading Dimension

2000
5000
10000
15000
18000
20000
22000
25000
26000
27000
30000
35000
40000
45000

2000
5008
10000
15000
18008
20016
22008
25000
26000
27000
30000
35000
40000
45000

TABLE III: HPL Conﬁguration

the ﬂoating-point performance by solving a linear system of
equations of order n (i.e., problem size) [26]:

Ax = b; A ∈ (cid:60)n×n; x, b ∈ (cid:60)n

To solve this linear system, it ﬁrst computes the LU factor-
ization with row partial pivoting of the n-by-n + 1 coefﬁcient
matrix:

[A b] = [[L, U ]y]

Since the lower triangular factor L is applied to b as the
factorization progresses, the solution x is obtained by solving
the upper triangular system U x = y. The lower triangular
matrix L is left unpivoted and the array of pivots is not
returned [26]. HPL serves as system stress test due to its
intense computing property.

HPCG is another popular benchmark designed for HPC
systems to be closer to real application. Basically, high per-
formance conjugate gradient (HPCG) is consisted of compu-
tations and data access patterns which are more commonly

1Details about NVIDIA docker: https://github.com/NVIDIA/nvidia-docker

As for comparing GPU performance, we choose some
representative applications from CUDA 8.0 samples. Each of
them performs different operations on GPU.

First we test the effective data transmission throughput from
CPU to GPU, GPU to CPU, and GPU to GPU, aiming to check
if docker container will affect the speed of data transmission
which is crucial in training neural networks as huge amount
of training data need to be delivered between CPU and GPU.
Then we perform a convolution operation on a 18432 ×
18432 image, which is commonly used in deep learning
to extract features from training data like images [29], and
measure the throughput in mega-pixels per second.

Matrix multiplication is our third GPU application, as it
is widely used in not only fully-connected layers, but also
layers [29]. So the efﬁciency of performing
convolutional
matrix multiplication on GPU is crucial
to deep learning
performance. We generate matrix A of size 23040 × 17280
and matrix B of size 17280 × 11520 and calculate matrix C
by calling CUBLAS function:

A × B = C

where

A ∈ (cid:60)23040×17280, B ∈ (cid:60)17280×11520, C ∈ (cid:60)23040×11520.

Such sizes are selected to ﬁll the 4GB memory of GTX 980
so as to make the workload sufﬁciently large.

Next, we benchmark the performance of self-deﬁned kernels
by calculating 64-bin and 256-bin histogram of 67108864
random numbers ranging from 0 to 255. The pseudo-code of
the algorithm is shown in Algorithm 1. The CUDA kernel
function divides the data into many individual parts. Each
thread processes one part and stores the sub-histogram in its
own storage space. Then it merges all sub-histograms to get
the ﬁnal result [30].

Input: Random number array data
Output: Histogram array result
for counter < BIN_COUNT do

result[counter] = 0;

end
for counter < number of data do

result[data[counter]]++;

end

Algorithm 1: Histogram Calculation

Lastly, we test different versions of matrix transpose which
are memory intensive [31] [32]. We consider 8 different

Fig. 2: A Simple Example of CNN(LeNet) [35]

approaches including simple copy, simple copy with shared
memory, naive transpose, coalesced transpose, shared memory
bank conﬂicts, decomposing transpose, partition camping, and
diagonal block reordering from [33]. We use matrix transpose
to test the efﬁciency of GPU memory access [33]. Matrix
transpose operations are also commonly used in deep learning
tools [34].

C. I/O Performance

In this part we test disk I/O performance in different ways
with dd and ioping tools. dd is used to write multiple ﬁles
with different sizes to hard disk and we can get access speed
from the outputs of dd directly. Below shows an example of
writing out a 1 GB ﬁle:

dd if=/dev/zero of=bigfile bs=64k \

count=16k conv=fdatasync

Basically, this command reads a stream of null characters
in blocks and each block contains 64KB of data. Then it
physically writes to an output ﬁle named bigﬁle to make I/O
operations. There are 16,000 blocks, which mean that in total
16, 000 × 64KB = 1GB of data are written to the hard drive
for measuring the sequential writing throughput. We can use
the number of blocks, i.e., the count ﬂag, to control the ﬁle
size. Speciﬁcally, we set count to 16, 1600, 8k, and 16k to test
with ﬁles of size 1MB, 100MB, 512MB, and 1GB respectively.
Then we use ioping to benchmark different access types
including cache I/O random access test, direct I/O random
access test, as well as I/O latency test. Details are listed in
Table IV.

Purpose

Bash cmd

IO latency
Cached IO random access
Direct IO random access

ioping -c 10 .
ioping -C -R -w 5 .
ioping -D -R -w 5 .

TABLE IV: ioping Tests

D. Performance of Deep Learning Tools

After measuring the performance of individual factors, we
come to evaluate the training performance of three represen-
tative neural networks: Fully Connected Networks (FCNs),

Convolutional Neural Networks (CNNs), and Recurrent Neural
Networks (RNNs) using different deep learning tools, includ-
ing Caffe [2], CNTK [3], MXNet [4], TensorFlow [5], and
Torch [6]. The tested software versions are shown in Table V.
We measure the speed in unit of second per batch.

Software Major Version GitHub Commit ID cuDNN

1.0
Caffe
2.0
CNTK
MXNet
0.9
TensorFlow 1.0
Torch

7

39f28e4
1ae666d
32dc3a2
4ac9c09
748f5e3

v5.1
v5.1
v5.1
v5.1
v5.1

TABLE V: Deep Learning Tools

1) FCN: Fully connected networks are the simplest neural
network model. Each neuron performs a simple forward active
function and sends the result to all the neurons in the next layer
(see Fig. 3).

Fig. 3: Fully Connected Network

We design a FCN with 5 layers (including the input and
output layers) whose conﬁguration is shown in Table VI. We
name it FCN5 and train it with MNIST dataset [36] which
consists of 60000 labeled handwriting images.

Layer

# of nerons Active function

Input
Hidden 1
Hidden 2
Hidden 3
Output

784
2048
4096
1024
10

-
Sigmoid
Sigmoid
Sigmoid
Softmax

TABLE VI: FCN5 Conﬁguration

2) CNN: Convolutional Neural Networks (CNNs) are a set
of models inspired by biology studies to simulate the way
animal brains process images by introducing convolutional
layers in artiﬁcial neural networks. As shown in Figure 2,
in front of fully connected layers several convolutional layers
are used to extract features from input images so that the
classiﬁer can better distinguish them with different labels. In
this part, we select AlexNet [16] and ResNet [14] to train
Cifar10 dataset [37].

3) RNN: Recurrent Neural Networks (RNNs) are widely
used in applications like speech recognition, machine transla-
tion, language modeling, etc. [38]. Long short-term memory
(LSTM) network is one of the most commonly used types in
this category.

IV. EXPERIMENTAL RESULTS AND ANALYSIS

In this section, we will present our detailed experimental
results. We mainly focus on presenting the performance differ-
ence between using docker container and without using docker
container. We introduce the symbol Dif f% to represent the
difference which is deﬁned as follows:

Dif f% =

H − D
H

× 100%

(1)

where H is the result without using docker container and D
is result of using docker container.

A. CPU Performance

1) HPL Benchmark: We run HPL experiments on both Intel
Xeon E5-2620 v3 and Intel i7-6800K platforms with different
workloads. We gradually increase the problem size from 2,000
to 45,000. As illustrated in Fig. 5, HPL tests on CPU performs
nearly the same in docker container and host system in general.
The differences are mostly less than 1%. It is very interesting
to notice that the HPL performance of using docker container
can be even better on our server platform.

Fig. 4: Example of LSTM layer

As Figure 4 illustrates, each LSTM layer packs a few
sub-models and forms very deep neural networks by putting
them together. In our experiments, we build a 2-layer LSTM
network and train with PTB dataset. Each layer has 256 states
and input texts are pre-processed into a character sequence
with the length of 32.

In summary, there are three categories of neural networks
and four models will be included in our experiments. We will
also train each model with different batch sizes to simulate the
real training environment. Our experimental conﬁgurations are
shown in Table VII.

Network Type Model

Dataset Batch Size

FCN

CNN

RNN

FCN5
FCN5
FCN5

Alexnet
Alexnet
Alexnet

MNIST
MNIST
MNIST

Cifar10
Cifar10
Cifar10

Resnet50 Cifar10
Resnet50 Cifar10
Resnet50 Cifar10

LSTM
LSTM
LSTM

PTB
PTB
PTB

512
1024
2048

256
512
1024

16
32
64

128
256
512

TABLE VII: Deep Learning Tools Experiment Design

Fig. 5: HPL Benchmark Results

2) HPCG Benchmark: HPCG benchmark experiments also
show that there is little overhead when using docker container.
the differences between docker
Table VIII illustrates that
container and host system are merely 0.24425% with E5-2620
v3 and 0.55624% with i7-6800k respectively.

Docker Host

Difference Dif f%

E5-2620 v3
i7-6800k

5.525
5.288

5.539
5.317

0.013
0.029

0.244%
0.556%

TABLE VIII: HPCG Benchmark Results

Above all, minor performance differences are found in both
HPL and HPCG benchmark experiments meaning that using
CPU in a docker container won’t introduce much overhead.

B. GPU Performance

We perform our GPU tests on two generations of NVIDIA
GPUs which are built in different architectures: the latest GTX
Titan X (Pascal) and GTX 980 (Maxwell). GPU performance
is a crucial factor for all the deep learning tools shown in
Table V. The experimental results are summarized in Fig. 6.
We can see that the bandwidth tests show tiny performance
difference in all
three data transmission experiments. The
most widely used matrix operation, matrix multiplication,
also shows little performance difference. As for self-deﬁned
kernels like the histogram experiments, it is shown that the
host system does perform better than in the docker con-
tainer, especially when running on GTX980. A set of matrix
transpose operations serves as a comprehensive evaluation of
utilizing GPU for general purpose computation. Again there
is no huge performance difference found in this set of ex-
periments. GPU performance varies because of many physical
environmental factors such as temperature change. Overall, it
is quite promising that the maximum absolute Dif f% value is
only 0.61% (in the experiment of histogram256). These most
frequently used operations in deep learning processes like data
transmission, matrix multiplication, convolution operation, and
matrix transpose all perform very well under docker container.

C. I/O Performance

1) dd Test: We use dd command to sequentially write
several ﬁles of different sizes to hard disks and collect speed
information from outputs.

PC Platform Dif f% Server Platform Dif f%

1MiB ﬁle
100MiB ﬁle
512MiB ﬁle
1GiB ﬁle

-4.263%
-1.408%
0.000%
0.636%

-4.023%
6.893%
5.714%
3.448%

TABLE X: dd Write Test

From Table X we can see that I/O speeds are very close
when comparing docker containers and host systems. Ac-
cessing smaller ﬁles tends to have a higher speed in docker

Fig. 6: GPU Benchmark Results

containers, whilst accessing large ﬁles in the host system are
a little bit better than in docker containers. We also measure
the standard deviation among each set of 20-runs to estimate
the stability as illustrated in Table XI. The values of standard
deviation from docker container and host system are very
close. We can claim that docker containers are as stable as
host system in sequentially writing out large ﬁles.

2) ioping Test: To measure the random access speed, we
make use of ioping command. The results are rather interesting
compared with dd tests. As we can see in Table XII, random
I/O access results are found to be close in our server platform.
However, we observe huge performance differences in our
PC platform. Docker containers are extremely faster than host
system in direct I/O test and I/O latency test. We go deeper into
this phenomenon and ﬁnd that the speed of direct I/O random
access in docker container is almost the same as accessing the
disk cache.

Best Average

STD

Server with docker
Server w/o docker
PC with docker
PC w/o docker

322
309
103
103

306.476
292.333
100.561
102.04

19.6069
21.6272
0.97408
0.64888

TABLE XI: dd Sequential Access Test

Caffe GTX Titan Caffe GTX 980 CNTK GTX Titan CNTK GTX 980 MXNet GTX Titan MXNet GTX 980 TF GTX Titan TF GTX 980 Torch GTX Titan Torch GTX 980

FCN 512
FCN 1024
FCN 2048

Alexnet 256
Alexnet 512
Alexnet 1024

Resnet 16
Resnet 32
Resnet 64

LSTM128
LSTM256
LSTM512

1.084%
-0.313%
1.033%

-0.848%
-0.241%
0.535%

-0.282%
-0.010%
0.394%

-1.901%
-1.084%
0.006%

-3.308%
0.705%
-0.061%

-0.552%
-0.467%
-0.043%

Not Support
Not Support
Not Support

Not Support
Not Support
Not Support

0.077%
-0.923%
-1.993%

-0.357%
-5.474%
-1.222%

-3.189%
0.120%
-0.173%

-3.058%
1.326%
3.486%

4.299%
4.814%
4.540%

1.397%
1.619%
1.996%

-0.704%
0.108%
-0.119%

0.089%
-0.208%
0.310%

3.157%
-0.315%
0.190%

-1.603%
-1.799%
-3.607%

-0.043%
0.291%
0.940%

-1.390%
3.870%
-4.484%

1.047%
0.115%
0.324%

0.193%
-0.397%
-0.714%

3.014%
0.689%
1.174%

-0.399%
0.024%
0.339%

-5.139%
2.966%
-2.013%

-0.473%
0.028%
0.388%

-0.581%
3.108%
2.524%

1.084%
0.708%
0.759%

2.944%
3.134%
4.165%

1.166%
2.336%
1.182%

2.001%
2.143%
0.905%

0.540%
0.429%
0.558%

6.891%
8.641%
9.615%

3.369%
3.858%
3.767%

1.407%
0.308%
0.146%

0.929%
0.998%
-1.237%

4.096%
4.611%
4.863%

2.007%
1.978%
1.771%

-0.300%
0.061%
0.174%

0.080%
0.031%
Not Support

TABLE IX: Deep Learning Tools Benchmark Batch Time Dif f%

The results in Table XII show that docker container reacts to
I/O request almost 100% faster than it is in the host system. We
take a look at the speciﬁcation of our hard drive in GPU server
and notice that it come equipped with Multi-Tier Caching
Technology(MTC). Basically in addition to the actual spinning
disk storing data, there are multiple layers of NAND Flash
installed on the hard drive for quick access of frequently
used data for ongoing processes. Docker containers depend
on docker engine running in the background taking advantage
of MTC for fast small data access and IO requests. Because of
that, ioping doesn’t go into the actual disk of hard drive during
ioping tests and that’s why direct IO random access speed and
IO latency time have such huge performance difference.

Server Dif f% PC Dif f%

difference is usually within 5%. As for smaller networks like
FCN and Alexnet, we also ﬁnd that the performance of each
tool running in docker container and in the host system are
similar in terms of computational time costs. An interesting
phenomenon is found on MXNet, whose training time of the
ﬁrst epoch in docker container is much shorter than the host,
as shown in Table XIII. Notice that starting from the second
epoch, the difference drops back to normal. This is because
the ﬁrst epoch includes the initial I/O time which has different
performance under docker container and host system, while
later on the I/O time is hidden by the computing time. These
deep learning frameworks implement parallel data loading that
data for the next run are pre-loaded during the training process,
so that the I/O time is covered by computing time and we don’t
see much difference after the ﬁrst epoch.

17.447%

23.065%

MXNet GTX980

Epoch 1 Dif f% Epoch 2 Dif f%

Cache IO random
access speed
Direct IO random
access speed
IO latency time

-2.521%

-282.075%

3.509%

98.999%

TABLE XII: ioping Test Result

FCN 512
FCN 1024
FCN 2048

Alexnet 256
Alexnet 512
Alexnet 1024

75.707%
77.169%
74.563%

69.468%
68.451%
68.871%

-1.654%
0.000%
1.720%

-0.798%
0.145%
-0.428%

D. Deep Learning Tools

TABLE XIII: MXNet 1st Epoch vs. 2nd Epoch

After we evaluate individual factors that may affect the
performance of deep learning tools, we then test each deep
learning tool by training different types of neural networks.
Table IX shows the results of all networks trained by all the
tools. Different tools have their own metrics of measuring
performance, so we convert them into the time they take to
train one batch of data. The left most column indicates the
type of neural network and the number of samples we put
into each batch.

Note that positive numbers in Table IX illustrate that the
docker container outperforms our host system because the
host system needs more time to train one batch of data. As
Table IX shows that deep learning tools perform well in docker
container overall. For large networks Resnet and LSTM, we
can see that the performance of deep learning tools in docker
container is as good as in the host system. In those cases
that docker container runs slower than the host system, the

V. CONCLUSION AND FUTURE WORK

In conclusion, even though there are extra layers lying
between applications and hardware resources by using docker
containers, docker engine manages to minimize the overhead
pretty well. We don’t ﬁnd noticeable drawbacks of docker
containers in CPU and GPU tests. Testing programs running
in docker containers perform just as good as in the host
system. So putting deep learning tools into docker containers
is a feasible solution that we can beneﬁt from its ﬂexibility,
lightweight, and resource isolation abilities. Different deep
learning tools or the same tools with different version numbers
can coexist in one system yet maintaining good performance.
System administrators of shared servers and cloud platforms
can install docker engine in the system and let users download
and run their desired images by their own. System administra-
tors can prepare docker images with deep learning tools pre-

installed and properly conﬁgured so that users only need to
focus on their models and algorithms without getting annoyed
by dependencies and environment settings.

In the future, this work can be further extended on multiple
machines that train large-scale neural networks on a cluster
to gain even more acceleration. In this situation, data trans-
mission efﬁciency needs to be tested among docker containers
located in different physical machines. Even within the same
node there can be multiple GPUs installed, efﬁciently making
use of more than one GPU at the same time is also important
in deep learning. On the other hand, more types of hardware
platform can be included. In our work, we mainly focus
on the combination of CPU + NVIDIA GPUs. There are
other accelerators such as AMD GPUs and Intel Xeon Phi
processors. The results we get from our docker container are
based on NVIDIA docker which has ofﬁcial support directly
from the GPU manufacturer. Whether it is efﬁcient or not to
invoke computing devices from other kind needs to be further
studied.

VI. ACKNOWLEDGEMENT

The authors would like to thank all

the reviewers for
their insightful comments and valuable suggestions. This work
is supported by Shenzhen Basic Research Grant SCI-2015-
SZTIC-002.

REFERENCES

[1] S. Shi, Q. Wang, P. Xu, and X. Chu, “Benchmarking state-of-the-art
deep learning software tools,” arXiv preprint arXiv:1608.07249, 2016.
[2] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia. ACM, 2014, pp. 675–678.

[3] D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter,
O. Kuchaiev, Y. Zhang, F. Seide, H. Wang et al., “An introduction
to computational networks and the computational network toolkit,”
Microsoft Technical Report MSR-TR-2014–112, 2014.

[4] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,
C. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine
learning library for heterogeneous distributed systems,” arXiv preprint
arXiv:1512.01274, 2015.

[5] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems,” arXiv preprint
arXiv:1603.04467, 2016.

[6] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like
environment for machine learning,” in BigLearn, NIPS Workshop, no.
EPFL-CONF-192376, 2011.

[7] S. Fu, J. Liu, X. Chu, and Y. Hu, “Toward a standard interface for cloud
providers: the container as the narrow waist,” IEEE Internet Computing,
vol. 20, no. 2, pp. 66–71, 2016.

[8] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cuDNN: Efﬁcient primitives for deep learning,”
arXiv preprint arXiv:1410.0759, 2014.

[9] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural
networks for acoustic modeling in speech recognition: The shared views
of four research groups,” IEEE Signal Processing Magazine, vol. 29,
no. 6, pp. 82–97, 2012.

[11] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-
trained deep neural networks for large-vocabulary speech recognition,”
IEEE Transactions on Audio, Speech, and Language Processing, vol. 20,
no. 1, pp. 30–42, 2012.

[12] L. Deng, G. Hinton, and B. Kingsbury, “New types of deep neural
network learning for speech recognition and related applications: An
overview,” in Acoustics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on.

IEEE, 2013, pp. 8599–8603.

[13] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in Acoustics, speech and signal pro-
cessing (icassp), 2013 ieee international conference on.
IEEE, 2013,
pp. 6645–6649.

[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 770–778.

[15] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are
easily fooled: High conﬁdence predictions for unrecognizable images,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 427–436.

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on.
IEEE,
2009, pp. 248–255.

[18] R. Collobert and J. Weston, “A uniﬁed architecture for natural language
processing: Deep neural networks with multitask learning,” in Proceed-
ings of the 25th international conference on Machine learning. ACM,
2008, pp. 160–167.

[19] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and
language processing

D. McClosky, “The Stanford CoreNLP natural
toolkit,” in ACL (System Demonstrations), 2014, pp. 55–60.

[20] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
P. Kuksa, “Natural language processing (almost) from scratch,” Journal
of Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.
[21] M. Plauth, L. Feinbube, and A. Polze, “A performance evaluation of
lightweight approaches to virtualization,” Cloud Computing 2017, p. 14,
2017.

[22] T. Kämäräinen, Y. Shan, M. Siekkinen, and A. Ylä-Jääski, “Virtual
machines vs. containers in cloud gaming systems,” in Proceedings of
the 2015 International Workshop on Network and Systems Support for
Games.

IEEE Press, 2015, p. 1.

[23] D. Beserra, E. D. Moreno, P. T. Endo, and J. Barreto, “Performance
evaluation of a lightweight virtualization solution for HPC I/O scenar-
ios,” in Systems, Man, and Cybernetics (SMC), 2016 IEEE International
Conference on.
IEEE, 2016, pp. 004 681–004 686.

[24] M. T. Chung, N. Quang-Hung, M.-T. Nguyen, and N. Thoai, “Using
docker in high performance computing applications,” in Communica-
tions and Electronics (ICCE), 2016 IEEE Sixth International Conference
on.

IEEE, 2016, pp. 52–57.

[25] N. Haydel, S. Gesing, I. Taylor, G. Madey, A. Dakkak, S. G. De Gon-
zalo, and W.-M. W. Hwu, “Enhancing the usability and utilization of
accelerated architectures via docker,” in Utility and Cloud Computing
(UCC), 2015 IEEE/ACM 8th International Conference on.
IEEE, 2015,
pp. 361–367.

[26] J. J. Dongarra and P. Luszczek, “Introduction to the HPC challenge

benchmark suite,” DTIC Document, Tech. Rep., 2004.

[27] J. Dongarra and M. A. Heroux, “Toward a new metric for ranking high
performance computing systems,” Sandia Report, SAND2013-4744, vol.
312, 2013.

[28] J. Dongarra, M. A. Heroux, and P. Luszczek, “HPGC benchmark: A new
metric for ranking high performance computing systems,” Knoxville,
Tennessee, 2015.

[29] V. Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, “Efﬁcient process-
ing of deep neural networks: A tutorial and survey,” arXiv preprint
arXiv:1703.09039, 2017.

[30] V. Podlozhnyuk, “Histogram calculation in CUDA,” NVIDIA Corpora-

tion, White Paper, 2007.

[31] X. Mei, K. Zhao, C. Liu, and X. Chu, “Benchmarking the memory
hierarchy of modern GPUs,” in Network and Parallel Computing.
Springer, 2014, pp. 144–156.

[32] X. Mei and X. Chu, “Dissecting GPU memory hierarchy through
microbenchmarking,” IEEE Transactions on Parallel and Distributed
Systems, vol. 28, no. 1, pp. 72–86, 2017.

[33] G. Ruetsch and P. Micikevicius, “Optimizing matrix transpose in

CUDA,” Nvidia CUDA SDK Application Note, vol. 18, 2009.

[34] S. Shi, P. Xu, and X. Chu, “Improving the performance of fully
connected neural networks by out-of-place matrix transpose,” arXiv
preprint arXiv:1702.03192, 2017.

[35] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker,
H. Drucker, I. Guyon, U. Muller, E. Sackinger et al., “Comparison of
learning algorithms for handwritten digit recognition,” in International
conference on artiﬁcial neural networks, vol. 60. Perth, Australia, 1995,
pp. 53–60.

[36] Y. LeCun, C. Cortes, and C. J. Burges, “The MNIST database of

handwritten digits,” 1998.

[37] “Cifar-10 and cifar-100 datasets.” [Online]. Available: https://www.cs.

toronto.edu/~kriz/cifar.html

[38] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network

regularization,” arXiv preprint arXiv:1409.2329, 2014.

