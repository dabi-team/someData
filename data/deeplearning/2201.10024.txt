PROTEOME-SCALE DEPLOYMENT OF PROTEIN STRUCTURE
PREDICTION WORKFLOWS ON THE SUMMIT SUPERCOMPUTER ∗

2
2
0
2

n
a
J

5
2

]

M
Q
.
o
i
b
-
q
[

1
v
4
2
0
0
1
.
1
0
2
2
:
v
i
X
r
a

Mu Gao
Center for the Study of Systems Biology
Georgia Institute of Technology
Atlanta, Georgia, USA
mu.gao@gatech.edu

Mark Coletti
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
colettima@ornl.gov

Russell B. Davidson
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
davidsonrb@ornl.gov

Ryan Prout
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
proutrc@ornl.gov

Subil Abraham
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
abrahams@ornl.gov

Benjamín Hernández
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
hernandezarb@ornl.gov

Ada Sedova
Oak Ridge National Laboratory
Oak Ridge, Tennessee, United States
sedovaaa@ornl.gov

ABSTRACT

Deep learning has contributed to major advances in the prediction of protein structure from sequence,
a fundamental problem in structural bioinformatics. With predictions now approaching the accuracy
of crystallographic resolution in some cases, and with accelerators like GPUs and TPUs making
inference using large models rapid, fast genome-level structure prediction becomes an obvious
aim. Leadership-class computing resources can be used to perform genome-scale protein structure
prediction using state-of-the-art deep learning models, providing a wealth of new data for systems
biology applications. Here we describe our efforts to efﬁciently deploy the AlphaFold2 program,
for full-proteome structure prediction, at scale on the Oak Ridge Leadership Computing Facility’s
resources, including the Summit supercomputer. We performed inference to produce the predicted
structures for 35,634 protein sequences, corresponding to three prokaryotic proteomes and one
plant proteome, using under 4,000 total Summit node hours, equivalent to using the majority of
the supercomputer for one hour. We also designed an optimized structure reﬁnement that reduced
the time for the relaxation stage of the AlphaFold pipeline by over 10X for longer sequences. We
demonstrate the types of analyses that can be performed on proteome-scale collections of sequences,
including a search for novel quaternary structures and implications for functional annotation.

∗Notice: This manuscript has been authored in part by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the
U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication,
acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or
reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department
of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan
(http://energy.gov/downloads/doe-public-access-plan).

 
 
 
 
 
 
Proteome-scale structure prediction

Keywords deep learning · high-performance computing · protein structure prediction · proteomics · workﬂow
management software

1

Introduction

Understanding the function of all of the proteins coded for by an organism’s genome—its proteome—is a grand
challenge for the biological sciences, with implications for synthetic biology, medicine, molecular genetics, and many
other efforts [1]. Knowledge of the atomic structure of a protein can help with multiple aspects of biological research
[2]. For instance, it can inform our understanding of protein function, such as small-molecule binding pockets that
one can only make sense of in three-dimensional space. A structure can enable comparisons across databases [3],
analyses such as ligand-binding [4] or protein-protein interface determination [5], and molecular docking and simulation
[6, 7]. However, obtaining protein structures experimentally is time consuming, costly, and difﬁcult; for instance,
only about 17% of the human proteome has an experimentally determined three-dimensional structure [8]. Therefore,
being able to accurately predict protein structure computationally has been a goal for decades. Over the past several
years, exciting breakthroughs have been made in the use of deep learning together with advanced structural informatics
methods to improve protein structure prediction [9, 10, 11]. Most recently, Google DeepMind’s AlphaFold2 achieved
unprecedented accuracy in the Critical Assessment of protein Structure Prediction experiments (CASP14) [8]. The
accuracy of this method is attributable partially to the size of the neural network as well as to innovations in the network
architecture. The use of accelerators such as tensor processing units (TPUs) or graphics processing units (GPUs) to
both train and perform inference is important for high performance, and essential for genome-scale applications. With
this breakthrough comes the possibility of generating large datasets of high-accuracy predictions of protein structures
containing orders of magnitude more elements than existing crystallographic databases hold. Thus, deployment of
AlphaFold at scale to predict the structure of an organism’s full proteome is a natural desire.

Considerable advances in next-generation sequencing technologies have driven an exponential growth in the number of
sequenced genomes [12, 13]. Computational approaches that can take advantage of this massive amount of data and
provide important information about the proteome will help advance biology into a new digital and big-data age. In
fact, some of the leading names in high-performance computing (HPC) have recognized not only the need for HPC
in biology, but also the potential in this area, calling this next phase of computational biology “the digital biology
revolution." 2 However, fewer computational biology applications have made use of HPC or even code acceleration with
GPUs than in other ﬁelds such as astrophysics. Barriers have included heterogeneous sizes and ﬁle formats of biological
datasets and the multiple steps required for processing and analysis, each of which may use specialized programs
often developed by third-parties. These factors are in part a result of variability in genome structure across taxa, and
the multi-scale and multi-disciplinary complexity of gene expression, regulation and function. The convergence of
HPC and artiﬁcial intelligence (AI), especially deep learning (DL), promises to open the door to the use of HPC
resources for large-scale biological problems. These tools are designed for high-level developers, and can efﬁciently
incorporate highly non-linear and multi-dimensional information to provide accurate predictions; many DL programs
can also automatically make use of both GPUs and parallel processors. As such, use of DL and other AI methods
in the biosciences has recently ﬂourished [14]. The success of DL to provide accurate prediction enabled by the use
of high-performance hardware such as GPUs is evidenced by AlphaFold2. Here we describe the implementation
and deployment of a proteome-scale, high-throughput protein structure prediction workﬂow using AlphaFold2 on
the Summit supercomputer at the Oak Ridge Leadership Computing Facility (OLCF). We detail the performance
optimizations and workﬂow management aspects of this deployment, and demonstrate two useful applications of large
datasets of predicted structures: for informing functional annotation, and for discovering new assemblies of protein
domains.

2 Background and Related Work

To our knowledge, the only previous deployment of AlphaFold2 at proteome-scale was performed by the Google
DeepMind AlphaFold team in collaboration with the European Molecular Biology Laboratory’s (EMBL) European
Bioinformatics Institute (EBI). This dataset has been deposited for public use and contains 360,000 predicted structures
across 21 model-organism proteomes; the goal of the project is to predict the structures of all proteins in the UniRef90
sequence database [15], a database that can be considered a (growing) list of all known non-redundant protein sequences
that is continuously updated with protein representatives from clusters with 90% sequence identity [13]. Currently,
it contains over 100 million sequences, and will require close to 280 times more compute time than was used to
generate the initial 360,000 entries in the AlphaFold Protein Structure Database. It is therefore important for the
scientiﬁc community that additional resources be enlisted to help reduce the time it takes to create this dataset: such a

2https://blogs.nvidia.com/blog/2021/07/07/ceo-unveils-cambridge-1/

2

Proteome-scale structure prediction

compendium of information could lead to fundamental breakthroughs in our understanding of both protein structure
and function [16].

The deployment of high-throughput computational biology workﬂows on HPC systems has become more common.
Even for large sets of mostly parallel calculations, HPC systems have been used [17, 18, 19]. Often providing larger
numbers of compute nodes simultaneously, HPC resources can enable a large high-throughput job to be performed in a
fraction of the total time than is possible in the cloud, helping to address urgent and timely problems [7]. Furthermore,
shared parallel ﬁlesystems allow for rapid, in-place data analysis on large output datasets using parallel tools [19]. The
ability to make use of either the high-speed interconnect for traditional parallelization with MPI [17], or alternately, a
dataﬂow execution model [20, 21], or a combination of the two [18], helps with design of ﬂexible, multi-task workﬂows
that are often found in computational biology.

2.1 Leadership-scale HPC Workﬂows

While numerous workﬂow management tools exist, few have been deployed at scale on the largest supercomputers. The
FireWorks workﬂow management software [22] is frequently used for materials science workﬂows on the National
Energy Research Scientiﬁc Computing Center (NERSC) resources; it was ﬁrst tested at the scale of Summit for
high-throughput molecular docking screens [19, 7]. The Dask library was designed to enable parallel computing
in Python by distributing scientiﬁc calculations such as linear algebra operations and singular-value decomposition
[23, 24]; it can also be used purely as a distributed workﬂow manager via Python subprocess calls. On Summit,
Dask for the former use case was tested on small numbers of nodes [25], and deployed on larger numbers of nodes for
parallel, distributed dataframe and database query processing tasks [19]; it was deployed as a workﬂow manager for
large-scale evolutionary algorithms at the scale of 500 Summit nodes [26, 27].

3 Methodology

Here we describe our optimized workﬂow for running the AlphaFold2 pipeline in a high-throughput HPC setting, and
testing on the OLCF resources. For this pipeline, pre-processing steps and feature generation were performed on the
Andes analysis cluster. The Summit supercomputer with its GPUs was used in large parallel batches to run the DL
inference as well as the geometry optimization calculations on the ﬁnal models. These last two steps utilized Dask as
the workﬂow management software.

The Summit supercomputer is an IBM system containing approximately 4,600 IBM Power System AC922 compute
nodes. Each node contains two IBM POWER9 processors and 6 NVIDIA Tesla V100 accelerators. Each processor is
connected via dual NVLINK connections capable of a 25GB/s transfer rate in each direction3. Andes is a commodity-
type Linux cluster with 704 nodes, each containing two 16-core 3.0 GHz AMD EPYC 7302 processors with AMD’s
Simultaneous Multithreading (SMT) Technology and 256GB of main memory4. Some preliminary testing of the
pipeline was also performed on the Partnership for an Advanced Computing Environment (PACE) resources at Georgia
Institute of Technology. PACE houses the Phoenix cluster, a mixture of CPU and GPU nodes, with approximately 1100
CPU nodes and 100 GPU nodes. GPU nodes consist of dual Intel Xeon Gold 6226 CPUs each with 12 cores, and 4 x
RTX6000 GPUs (24GB memory), totaling 96GB GPU memory and 24 CPU cores per node.

3.1 AlphaFold2 on the Summit Supercomputer using Singularity

The computing power of the Summit supercomputer is an important resource for proteome-scale protein structure
prediction; one of our aims is to supplement the AlphaFold/EMBL-EBI database with organisms of interest to the
Department of Energy (some of which may not yet be found in the UniRef90 database, the target of AlphaFold/EMBL-
EBI efforts), and to produce full-genome structural predictions for organisms central to our research. Challenges to
deploying AlphaFold on Summit include the Power9 CPU architecture that prevents the use of package managers to
download pre-compiled binaries, combined with the use of build systems non-standard in the HPC setting. AlphaFold
provides a Docker5 recipe for building all of its dependencies and packaging, but that recipe is not directly usable on
Summit or on other systems which do not support Docker. Summit does not yet support container builds, but it does
provide a functioning container runtime with Singularity. As described in [1], we used an external POWER9/NVIDIA
V100 GPU system to build a Singularity6 container that could run on Summit using Podman[28] as the container build
mechanism. We altered the AlphaFold Dockerﬁle to build JAX and other components from source, and to remove

3https://docs.olcf.ornl.gov/systems/summit_user_guide.html
4https://docs.olcf.ornl.gov/systems/andes_user_guide.html
5https://www.docker.com/
6https://sylabs.io/singularity/

3

Proteome-scale structure prediction

feature generation steps which did not require GPUs and thus could be pre-computed on Andes. The ﬁnal image was
converted to Singularity’s format for running on Summit with Singularity.

3.2 Optimization of the AlphaFold Method for the High-throughput Setting

The release of the source code and neural network models of AlphaFold (version 2.0.1 and above) by DeepMind
sparked the interests of both the biological and computational research communities [29]. However, the code released
by AlphaFold was designed for small-scale studies, and not optimized for large-scale deployments, especially on an
HPC system designed for many users such as at the OLCF. This is in part due to the use of three task stages each with
very different computing requirements and characteristics described below.

3.2.1 Input feature generation on CPU resources

Given an input protein sequence, AlphaFold ﬁrst runs multiple searches for homologous sequences and generates the
input multiple sequence alignment (MSA), using third-party sequence alignment tools such as those provided in the
HMMER [30] and HH-suite [31] packages. These programs are run against several sequence libraries (UniProt [13],
MGnify [32], BFD [33]), and sequences of experimentally solved structures collected from the Protein Data Bank [34].
The total storage amount of these sequence libraries is about 2.1 TB for the full dataset [8], and 420 GB for a reduced
dataset described below. This ﬁrst computational task, while I/O and memory intensive, uses CPU-based codes that
make extensive use of vectorization as their primary performance strategy; tasks are run in batches and alignment of
a single sequence to tens of thousands of sequences in the databases is ﬁnished in minutes. On the OLCF resources,
the Summit computer is the most expensive and precious resource in terms of cost of maintenance, hardware, and
power, and also in terms of the competitiveness of compute allocations. Therefore, it would be wasteful to use Summit
time to deploy the original AlphaFold pipeline, wherein GPUs would be idle for the ﬁrst part of the calculation. A
recent version of AlphaFold was designed to allow for parallel execution of the CPU portion and the GPU portion
of the pipeline when running in larger batches [35]; however, our tests showed that the compute patterns, resource
requirements, and execution times are too imbalanced to make this the optimal solution when running inference at scale
on Summit. We therefore pre-computed input features on Andes.

For HHSuite, the number of ﬁle reading tasks performed by one alignment to the database can be large, and this
becomes the bottleneck on shared ﬁlesystems due to trafﬁc on the metadata servers and disk accesses7. Unfortunately,
due to the shared-use design of open-science HPC resources such as the OLCF, it is not possible to copy these large
databases into compute node memory or onto NVME burst buffers and leave them there for multiple jobs. Therefore,
the time saved from using this type of memory can be cancelled-out by repeated copying with every job allocation. In
order to reduce I/O overhead, we created 24 identical copies of the reduced sequence libraries on the parallel ﬁlesystem
using mpiFileUtils8, and ran 4 parallel jobs on each copy of the library. After testing with the full 2.1 TB dataset, we
moved to using a reduced version, which is obtained by removing identical and near-identical sequences in the largest
of the sub-datasets, the BFD. According to DeepMind’s benchmark study, it yields virtually identical performance as
the results from the full dataset [36]. The sequence searches yield MSAs between the input target sequence and its
homologous sequences found in the libraries, and also structures of homologs to be used as templates from aligning to
the PDB databases. AlphaFold then builds input features from the MSAs and the structural templates. The structural
features are only used by two of the ﬁve DL models that each output a structure in AlphaFold2. The other three models
take only the sequence features to make predictions. Thus, the most important features are the MSAs, which dictate
the ﬁnal quality of all predicted structures. The input features are then fed into the DL neural network models that
eventually predict the three-dimensional coordinates of the input sequence.

3.2.2 Inference

The model inference procedure is iterated in multiple recycles, feeding back a predicted 3D structural model out of each
inference cycle and regenerating input features as feedback. In practice, multiple iterations are essential and the ofﬁcial
release of AlphaFold uses three recycles. Overall, the calculations in this stage involve many tensor operations, thus a
newer GPU, such as those available on Summit, can dramatically accelerate the calculation speed. The ofﬁcial release
of AlphaFold version 2.0.1 provides two presets, reduced_dbs and casp14, that combine a few key conﬁguration
parameters for application or benchmark purpose. The casp14 preset was adopted during the CASP14 competition;
it employs eight ensembles reulting in approximately eight times the computational cost in comparison to the single
ensemble preset deﬁned in reduced_dbs. Both presets use a ﬁxed number of 3 recycles. The reduced_dbs is the
preset that DeepMind used in their proteome-scale applications of AlphaFold [36]. While this preset is efﬁcient and

7https://github.com/soedinglab/hh-suite/wiki#running-hhblits-efficiently-on-a-computer-cluster
8https://hpc.github.io/mpifileutils/

4

Proteome-scale structure prediction

often sufﬁcient for most protein sequences, it is also known that some challenging protein sequences may not be folded
properly in a short number of recycles [8]. To address these challenging sequences without dramatically increasing
run time, we implemented a method to dynamically control recycle numbers ﬁrst proposed in ColabFold [37]. In
this strategy, the change of the protein residue contact distogram is calculated after each recycle in comparison to
the previous recycle. If the change is less than a threshold value, it is consider converged and the program will stop
recycling. In our implementation, we use two threshold cutoff values of 0.5 and 0.1 for our customized genome and
more stringent super presets, respectively. Additionally, we increased the maximum number of recycle values to 20,
but also reduced it progressively (to a minimum of 6) with increased input sequence length, if the length is longer than
500 amino acid residues (AAs). While our implementation also provides other options that are useful to reduce memory
requirements for very long sequences (e.g. > 2500 AAs), we do not report predicted structures for these large proteins
here as they comprise less than 1% of the sequences. Such long sequences are best processed separately with adjusted
parameters to more appropriately balance accuracy and time to solution.

3.2.3 Geometry optimization

The model inference generates high-quality 3D structural models that are typically sufﬁcient as-is for most research
purposes. However, it is not uncommon that these models still contain structural ﬂaws that would not be found in
experimental crystal structures and may be considered non-physical. These deviations in geometry have been classiﬁed
for the CASP assessments as “clashes" and “bumps," with the following deﬁnitions: a clash is a Cα-Cα pairwise
distance < 1.9 Å, a bump is a Cα-Cα pairwise distance < 3.6 Å. Models are considered to be “clashed" if they contain
more than 4 clashes or 50 bumps [38]. For deposition of model structures into databases, it may be considered good
practice to remove clashed structures for aesthetic purposes. To address this issue, the ﬁnal step of AlphaFold is
to remove these clashes using a standard molecular mechanics optimization procedure called the “relaxation." This
consists of an optimization of the Cartesian coordinate geometry using a minimization with respect to some function,
often the potential energy as deﬁned by a molecular mechanics Hamiltonian and force ﬁeld. The critical task is to
maintain the accurate structure provided by the inferred model while removing the non-physical clashes and bumps;
only small perturbations to the overall structure are desired. AlphaFold employs the OpenMM program [39] for this
procedure, which can run either purely on CPUs or take advantage of GPUs. In the original AlphaFold program, the
CPU version is used. We have found that the GPU version can provide exceptional performance on Summit [20], so we
designed a relaxation protocol that used OpenMM’s GPU platform. We also decoupled this step into its own separate
workﬂow.

For each predicted structure, atoms were assigned force ﬁeld parameters and hydrogen atoms were added. Then,
a single energy-minimization calculation was performed with an unlimited number of optimization steps until the
energy difference between steps reached a convergence criteria (2.39 kcal · mol−1). The force ﬁeld and minimization
protocol mirror those used in AlphaFold2 exactly, including the application of a harmonic restraint to all non-hydrogen
atoms using a force constant of 10 kcal · mol−1 · Å−2. Where our method does differ from AlphaFold2 is in the
number of energy minimization calculations that may be attempted on each structural model. The original AlphaFold2
procedure checks for “violations” (e.g. clashes and bumps); if any violations are found, another iteration of minimization
calculations are performed on the model. However, at this point in the pipeline, the model structure is described with a
molecular mechanics force ﬁeld that strongly destabilizes non-physical interactions between any atoms in the model
(beyond those deﬁned by Cα-Cα distances). More than a single energy minimization calculation is rarely needed, so
we removed the unnecessary violation calculations and the possibility for repeated energy minimization calculations.
We compared both the runtime and performance of the geometry optimization between our protocol and the original
AlphaFold2 version to test ability of our modiﬁed method to remove violations as well as to speedup the time-to-solution.
For gauging the ability of our method to perform adequate relaxation, we used proteins from the CASP14 assessment for
which crystal structures are available for comparison. Relaxation with the original AlphaFold method was performed
on the PACE Phoenix cluster. Our protocol using the CPU version of OpenMM was tested on Andes using a full node
(2 AMD EPYC 7302 Processors with 16 cores each) with the default threading scheme in OpenMM that sets the thread
count based on the number of cores it detects. Our protocol using Summit assigned a single CPU core and a single
GPU for each minimization calculation; this is more than sufﬁcient for small systems of thousands to tens of thousands
of atoms. Therefore, 6 instances of our geometry optimization task can be run on one Summit node for the full-scale
workﬂow.

3.3 Deployment of Inference Workﬂow on Summit with Dask

After pre-calculating the input features, the DL inference is deployed using a workﬂow manager with a dataﬂow
execution model where workers receive tasks as soon as they are free to accept them from a queue. AlphaFold uses ﬁve
different models to perform inference on each target protein sequence, producing ﬁve different predicted structures.
Therefore, computing tasks are composed of pairs of DL models and target sequences. In each task, we run one DL

5

Proteome-scale structure prediction

model given the input features of one target sequence, and it generates one structural model. This task decomposition
strategy helps with load distribution and balance in a large-scale parallel-job run.

Figure 1: Illustration of the Summit AlphaFold workﬂow for asynchronously processing a batch of inferences from
input features. The top oval box represents the Dask scheduler task queue that has three protein inputs waiting for an
available worker. The four bottom GPU icons represent Dask workers with an associated GPU running the inference.
The arrow shows the Dask scheduler assigning the next task in its queue to a Dask worker that recently ﬁnished
processing a prediction. This process repeats until the scheduler queue is empty via a dataﬂow execution model.

Fig. 1 shows the Summit AlphaFold2 workﬂow. Dask [24] is used as the workﬂow management tool, assigning input
features for each of the ﬁve models for each protein target to the AlphaFold inference portion, using the following steps
from a single Summit batch submission script (Summit uses IBM’s LSF batch queueing system9):

1. The Dask scheduler is started.

2. Dask workers, one per GPU for all Summit nodes used, are started; they use a JSON ﬁle written by the Dask

scheduler to register with the scheduler.

3. The main driving Python script is started.

(a) A Dask client connects to the scheduler.
(b) A text ﬁle is read that contains a list of all the protein targets to be processed.
(c) The list of targets are sorted in descending order of sequence length.
(d) All input tasks are added to the scheduler task queue via a single Dask client.map() call.
(e) As each Dask task is completed, statistics about that task, such as the start and end processing times, are

appended to a CSV ﬁle.

A concern with high-throughput biological workﬂows is load balance, due to the variable range in input sizes [40].
We implemented a greedy approach to load balancing by sorting proteins in descending order by sequence length
(3c), allowing for lengthier processing to happen earlier in the run. Smaller tasks ﬁll in gaps later. With a random
task-processing order, some of longer-running tasks could happen at the end and be assigned to a single worker to
run sequentially; in those scenarios, the run would continue until those tasks complete even though the remaining
workers have ﬁnished all the remaining tasks and are idle. Some of the proteins are too large to ﬁt onto the memory of a
standard Summit node, and after a run these proteins will have failed to process. For these we used Summit’s additional
high memory nodes with 2TB of DDR4 memory, 192GB of High Bandwidth Memory (HBM2).10 The LSF script
for processing AlphaFold inference on Summit used three jsrun statements; jsrun is IBM’s version of mpirun for
Summit. The ﬁrst was to run a Dask scheduler using just two cores. The second jsrun allocated a Dask worker per
GPU. The third jsrun dedicated a single core for the controlling Python script that was the Dask client; this was the
script that read the ﬁle of proteins to be processed, assigned tasks via Dask for AlphaFold inference, and created a CSV
ﬁle of the processing times for each task, as listed in 3 above.

9https://en.wikipedia.org/wiki/IBM_Spectrum_LSF
10https://docs.olcf.ornl.gov/systems/summit_user_guide.html

6

Proteome-scale structure prediction

3.4 Deployment of Geometry Optimization Workﬂow on Summit with Dask

The geometry optimization portion of the pipeline was also deployed with a Dask workﬂow, in a nearly identical setup
as described in 3.3. Instead of input features, inputs were the unrelaxed model structures output by the inference
workﬂow. Instead of model inference, the relaxation steps described in 3.2.3 were run on each structure, using one GPU
per task.

4 Results and Discussion

Here we describe the results of the ﬁrst proteome-scale deployments of our AlphaFold structure prediction workﬂow
on the OLCF resources, including statistics on performance, conﬁdence, and quality. The total number of structures
predicted is ﬁve times the total number of input target sequences, due to the generation of a structure for each of 5
different models. The top model is chosen based on the conﬁdence scores predicted by AlphaFold2. We used the output
pTMS value for this choice. We predicted the structures of all proteins with a sequence length of less than 2500 AAs for
the following species: prokaryotes Pseudodesulfovibrio mercurii, Rhodospirillum rubrum, and Desulfovibrio vulgaris
strain Hildenborough, and the plant species Sphagnum divinum as a representative eukaryote. All four species are
important to research efforts within the Department of Energy’s Ofﬁce of Biological and Environmental Research. P.
mercurii is a newly reclassiﬁed species of bacteria found to be extremely efﬁcient at methylating mercury; methylated
mercury is highly toxic and thus this organism is important to environmental science research [41]. R. rubrum is a
photosynthetic bacterium that has been studied for its important metabolic pathways such as light harvesting, elemental
sulfur production [42], and recently, a newly discovered ability to produce ethylene, useful for bioproduction [43]. D.
vulgaris Hildenborough is the ﬁrst sulfate-reducing bacterium with a sequenced genome and has been chosen to be
a model organism by the ENIGMA collaboration; these organisms play a key role in biogeochemical cycles on the
planet, and may be useful for bioremediation, biomedical research, and biofuels [44, 45]. S. divinum is a species of
peat moss; peat is responsible for the sequestration of over half of the earth’s carbon, and under warming temperatures,
may release this carbon, adding to greenhouse gases. Formerly classiﬁed as the S. magellanicum species, S. divinum
has recently been considered divergent enough to be classiﬁed as a new species, and may be able to better withstand
varying climates [46]; thus it is important to investigate this species at genome-scale to aid remediation efforts [47, 48].
The number of ﬁnal (top) predicted protein structures for each species is 3446, 3849, 3205, and 25134 for P. mercurii,
R. rubrum, D. vulgaris Hildenborough, and S. divinum, respectively.

4.1 Feature Generation

Feature generation using the Andes CPU cluster required approximately half the resources to run than inference on
Summit, when measured solely by node-hour. For one of the bacterial proteomes containing 3205 protein sequences
with a mean of 328 AAs, feature generation took about 240 Andes node hours (vs. about 400 Summit node hours for
inference). After testing, we found the reduced sequence dataset was sufﬁcient for accuracy and better for large-scale
applications because it dramatically reduced the storage and copy requirements, and the I/O overhead.

4.2 Model Inference

The performance and the run-time costs of the presets described in 3.2.2 are compared to the ofﬁcial AlphaFold
presets in Table 1 using 559 protein sequences from D. vulgaris Hildenborough as a benchmark. The length of the
benchmark sequences ranges from 29 to 1266 AAs with a mean of 202 AAs. All runs are conducted on 32 Summit
nodes, except for the casp14 run on 91 nodes. Results of the eight longest sequences for the casp14 runs are missing
due to out-of-memory errors caused by high ensemble number. Wall time is total wall-clock time for all 559 input
sequences and includes overhead, which is about 16% of the total time in the super preset run. Means were calculated
on the top-ranked model, ranked by either predicted LDDT (pLDDT) or predicted TM-score (pTMS). Higher pLDDT or
pTMS score is better; their perfect values are 100 and 1.0, respectively. Best performance is indicated by bold font. The
pLDDT metric measures local model quality, and provides a good measure for individual domains, i.e., single-domain
protein folds. However, as most proteins have multiple domains, a better metric to evaluate the global model quality is
the pTMS.

The genome and super presets yield better top-ranked models in comparison to the reduced_dbs or casp14 presets,
thanks to longer recycles. If we use a pLDDT score of 70 as the high-quality cutoff, the genome and super presets
deliver a high-quality top model in 80% of target sequences, whereas the reduced_dbs delivers high-quality models in
77% of cases. If we use pTMS of 0.60 as the threshold for high-quality global model, we ﬁnd that the genome and
super both provide a high-quality model for 62% of targets, in comparison to 59% of reduced_db. Our presets have
higher run time costs in comparison to the reduced_db, but signiﬁcantly better than the casp14 preset. Since the

7

Proteome-scale structure prediction

performance of genome and super presets are very close, and the former costs slightly less, we adopt it as the main
preset for our proteome-scale prediction workﬂows.

Table 1: Benchmark tests of presets on 559 protein sequencesa

Mean pLDDT Mean pTMS Count Walltimea

Preset
reduced_db
genome
super
casp14c

78.4
79.5
80.7
78.6

0.631
0.644
0.650
0.631

559
559
559
551

44
50
58
>150

aMeans across top structure ranked by either pLDDT or pTMS.
bWall time (min) includes overhead.
c8 longest sequences missing due to out-of-memory errors.

Although the mean improvement using either metric for genome or super presets appears small, it is important to note
that the improvement is not uniformly distributed among individual sequences. If that were true, one may argue that the
extra time spent may not be worth it. Rather, the improvement is largely attributed to a few cases where signiﬁcantly
better models emerge from longer recycles. For example, with the super preset run, about 45% of the improvement
measured by total sum of pTMS comes from 28 (5%) targets with a signiﬁcant 0.1 or higher improvement in their
individual model pTMS, and 74% of improvement comes from 68 (12%) targets with an pTMS improvement above
0.05. Virtually all of these models are generated from numerous recycles (close to 20, the upper limit), with a mean of
19. Therefore, this option is useful for challenging targets where a better chance of a good model may be possible with
longer recycles.

4.3

Inference Workﬂow

Fig. 2 shows how the inference processing workload was distributed among Dask workers over an approximately ﬁve
hour run. Each row corresponds to a single Dask worker where a portion of their UUID is given as a “short_id” to
differentiate each worker and otherwise has no bearing on given worker’s capabilities. The blue blocks denote the
time spent processing an individual protein, and the white dividing lines indicate the overhead of switching between
proteins. Note that only 10 randomly selected workers are shown from the full set of 1200 workers for the sake of
brevity, but are representative of the observed behavior for all of them. The ﬁrst set of proteins for each worker took
signiﬁcantly longer to process than those at the end due to task sorting by length as described in 3.3; given that all the
Dask workers ﬁnished all of their respective tasks within minutes of one another indicates that sorting, together with the
Dask dataﬂow execution model, provides effective load balancing and distribution, despite the heterogeneously sized
inputs. Workﬂows using up to 1000 Summit nodes (6000 GPUs/Dask workers) were successfully deployed; to our
knowledge this is one of the largest deployments of Dask on Summit to date.

4.3.1 Proteome-scale structure prediction for S. divinum

Like other eukaryotic genomes, the protein sequences in the proteome of S. divinum are more challenging to model
than those of prokaryotic genomes. This proteome is unique in the sense that the vast majority of its sequences have not
been publicly released on the either the National Center for Biotechnology Information (NCBI) database or Uniprot.
Search in the RCSB PDB experimental structural database returns only 37 sequences with a sequence identity of 90%
or above. Therefore, our models can be considered virtually all new structures; while many of them would most likely
be structurally similar to existing known folds, subtle but potentially functionally important differences have yet to be
explored. In terms of model conﬁdence/quality, using the pLDDT scores and considering the top-ranked models, about
57% of sequences have a mean pLDDT score at greater than 70, which is considered high-quality. The total coverage of
high-conﬁdence pLDDT across all residues of the full proteome is 58%, and about 36% with an ultra-high conﬁdence
pLDDT > 90. Using the global metric pTMS, about 53% of top-ranked models have a pTMS greater than 0.6, a
threshold indicating high quality. The mean number of recycles of these top-ranked models is 12. Total compute costs
are about 2000 Andes node hours for the feature generation step, and 3000 Summit node hours for model inference,
including the overheads.

4.4 Geometry Optimization Method

The capabilities of the performance-optimized relaxation method reported here are quantiﬁed and compared to the
AlphaFold2 results for a subset of CASP14 targets. Speciﬁcally, 19 of the CASP14 target sequences have publicly
available high resolution crystal structures. For each model associated with this subset of CASP14 targets, we have
an unrelaxed model saved as an intermediate output from the AlphaFold2 run, the AlphaFold2 relaxed model, and

8

Proteome-scale structure prediction

Figure 2: Distribution of AlphaFold processing among the Dask workers, where each row denotes the processing load
for each worker, designated by a shortened portion of its UUID. Blue bars indicate time spent running AlphaFold; white
dividing lines the Dask scheduler overhead. The results of 10 workers are shown out of 1200 and are representative of
the whole.

relaxed models produced from the optimized OpenMM method using CPUs and GPUs. The template modeling score
(TM-score) [49] and superposition-based protein embedded Cα-sidechain score (SPECS-score) [50] are used to quantify
the quality of model relative to the experimentally determined protein structure. Fig. 3 shows the TM-score (left) and
SPECS-score (right) of relaxed models versus the respective scores of the AlphaFold unrelaxed models. TM-score
addresses only backbone atoms, while SPECS-score reports on sidechain position also. Based on the strong correlation
between structural metric values of the unrelaxed and relaxed models, we can see that no major structural changes have
been made in the clash-removal procedure, as is required to preserve the inferred structure; importantly, no decreases
in these metrics are seen. However, SPECS-scores do improve slightly after minimization for structural models that
already have high SPECS-scores, indicating that sidechain positions are being further optimized towards their native
crystallographic values by the molecular mechanics force ﬁeld. All three relaxation methods recover equivalent model
quality, which indicates that the additional steps used in the original AlphaFold relaxation method do not ensure higher
quality models and, so, are not necessary.

Beyond the TM-scores and SPECS-scores, we also quantiﬁed the reduction in structural violations such as clashes and
bumps deﬁned in 3.2.3 after a relaxation method is applied. The full set of CASP14 targets (160 models in total) were
analyzed to gather the average reduction in both types of violations. The AlphaFold unrelaxed models had, on average,
0.22 ± 1.09 clashes with a maximum of 8 clashes in a single structure. For all three minimization methods, clash
violations are completely removed; this is unsurprising since Cα-Cα atomic distances less than 1.9 Å are energetically
unfavorable within the molecular mechanics force ﬁeld and are quickly resolved by the energy minimization procedure.
For the smaller bump violations, the unrelaxed models had an average 3.76 ± 12.74 bumps with an observed maximum
of 148 in a single structure. Application of the AlphaFold2, OpenMM GPU, and OpenMM CPU relaxation methods
resulted in a reduction of bumps down to an average of 2.12 ± 3.70 (max of 26), 2.71 ± 5.90 (max of 58), and 2.59 ±
5.34 (max of 8) bumps, showing that relaxation also consistently reduces bumps. It should be noted that the optimization

9

02:00Oct 1, 202103:0004:0005:0006:0007:002086655092b422175968535c3856e031d6ecac0947321e2203de3208b58b0283c37c44bbded013b7short_idProteome-scale structure prediction

Figure 3: Correlation plots of structural metrics for unrelaxed models relaxed models using the three minimization
methods, for (left) TM-score and (right) SPECS-score results.

algorithm that underlies all three methods is not deterministic; variability in violation counts between different runs
with the same method is expected.

4.5 Geometry Optimization Workﬂow

The CASP14 results indicate that our methodology, optimized for the high-throughput HPC setting by removal of
extraneous quantiﬁcation of violations and the potential for multiple, unnecessary iterations of calculations, and use
of GPUs, results in equal success in reduction of crystallographic violations. Here we show the performance gains
realized with this method.

Fig. 4 depicts the time-to-completion results for the CASP14 targets in relation to the systems’ total number of heavy
(non-hydrogen) atoms, which is a better metric to quantify size of a job in a molecular mechanics calculation than
the number of residues. As the number of heavy atoms increases, so does computational cost; however our optimized
OpenMM implementation run on the GPUs was able to achieve marked performance boosts relative to the original
AlphaFold relaxation method. Execution of the minimization calculations using a Dask workﬂow on Summit’s GPU
nodes provides up to a 14-fold speedup. Notably, a single structure (T1080) took close to 4.5 hours using the original
AlphaFold method. This degree of speed up indicates that one of the limiting bottleneck steps from the original
AlphaFold workﬂow has been removed, enabling the application of this protein structure prediction workﬂow to the
proteome-scale. To further demonstrate the performance of the optimized relaxation method, geometry optimization
calculations were performed on the genome-scale set of top structural models (3205) for D. vulgaris Hildenborough.
Relaxation of the 3205 D. vulgaris Hildenborough structures was completed in 22.89 minutes using 8 Summit nodes
with 6 Dask workers per node (48 workers in total).

4.6 Proteome-scale data analysis

Our research on organisms important to DOE research includes a strong focus on the determination of the functions of
proteins with no known function. These “hypothetical" proteins are usually those with a sequence identity match below
20% to annotated proteins in databases [1]. For those with match below 10%, even the state-of-the-art HMM-based
methods fail. The use of a predicted structure to ﬁnd homology to annotated proteins is now becoming a viable option,
as the structures of proteins are often more conserved than their sequences. To demonstrate this type of analysis, we
performed a structural TM-score based alignment against the pdb70 database using the global alignment module of the
APoc program [4] on the set of 559 proteins labeled as “hypothetical" in the March 2020 NCBI GenBank proteome ﬁle
for D. vulgaris Hildenborough. A top alignment TM-score of 0.60 or above was found for 239 predicted structures,
215 of which had a sequence identity match of < 20%, and 112 for match < 10%, with the majority of aligned

10

Proteome-scale structure prediction

Figure 4: Computational efﬁciency for relaxation methods. (A) Time to solution results for relaxation of AlphaFold2
(AF2) predicted CASP14 targets with the AF2 relaxation method (grey) and with our method run on OLCF Andes CPU
nodes (red) and on OLCF Summit GPU nodes (blue). A large outlier in the AF2 data is not included in timing results,
where a relaxation calculation of T1080 took close to 4.5 hours. (B) Computational speedups for these jobs, relative to
the average timing for the AF2 method, across a range of system sizes.

proteins having useful annotations. This suggests that a wealth of potential new information providing clues to the
functions of these unknown proteins could be provided from matching high-conﬁdence predicted structures to existing
crystallographic databases.

An additional important use for this proteome-scale approach is to ﬁnd potential new folds (tertiary structure) or novel
quaternary structures in multi-domain proteins, and possibly, help discover new metabolic pathways. For example, on
the same set of 559 hypothetical proteins, there were several instances of predicted structures with very high model
conﬁdence scores and very poor TM alignment scores to pdb70. One of these, with over 98% of residues having pLDDT
scores of over 90, has a top TM-score of 0.358 against the pdb70. Further analysis of this sequence revealed that it,
and its homologs, had been independently annotated by experimental methods which discovered a novel mechanism
for homocysteine synthesis [51, 52]. Therefore, it is possible that high-conﬁdence predicted structures with no strong
structural matches to any experimental structures may provide leads for the discovery of new enzymatic pathways.

5

Implications and Conclusion

Accurate structure predictions for all proteins in an organism’s proteome, and collection of these proteome-scale models
into large databases across species, has profound implications for biological research, as recognized by the AlphaFold
Database project [15]. Here, we have demonstrated that the resources of the DOE leadership computing facilities can
be harnessed to help in this effort. In fact, with our optimized pipeline for Summit and Andes, we have the ability to
predict the structure of over 25,000 proteins in a plant proteome in a matter of hours, including all three stages of the
pipeline from input generation to the ﬁnal geometry optimization. Our optimizations for high-throughput deployment
of AlphaFold on Summit were also included in AF2Complex [53], which is a generalization of AlphaFold that extends
the model inference to prediction of protein-protein complexes, using the DL models trained for predicting either single
protein sequences or complexes. The prediction of accurate protein complex structures at scale is an exciting new
possibility especially relevant to HPC computing due to a quadratic (or higher) order dependence on the number of
protein sequences.

It should be mentioned that while the CPU-based feature-generation step required fewer total node hours than the model
inference step, the total wall times were higher, due to the fact that Andes, an analysis cluster, does not contain as many
nodes as Summit and that the queue policies for Andes favor small, long jobs rather than large, shorter jobs as is the

11

Proteome-scale structure prediction

case on Summit. Rather than marking the need for larger CPU clusters with different queue policies, however, the
problem begs the attention of the community for considering the use of GPU acceleration for MSA programs. In fact,
GPU implementations of HMMER were ﬁrst reported over a decade ago with one version reported in 2009 achieving
a 38-fold speedup over the native CPU HMMER [54]. Unfortunately, none of these implementations seem to have
been seriously considered for adoption by the developers of the two most commonly used programs for this calculation,
HMMER and HHSuite.

We have also demonstrated the as-yet unexplored possibilities that these large structural datasets can provide for
understanding protein function and structure-function relationships. In conclusion, we anticipate a paradigm shift in
biological research with the recent accuracy of DL structure prediction coupled with HPC workﬂows, beginning a new
era of big data and large-scale computing in biology.

Acknowledgments

This research was sponsored in part by the Ofﬁce of Biological and Environmental Research’s Genomic Science
program within the US Department of Energy Ofﬁce of Science, under award number ERKP917, the Laboratory
Directed Research and Development Program at Oak Ridge National Laboratory (ORNL), and used resources of the
Oak Ridge Leadership Computing Facility, which is a DOE Ofﬁce of Science User Facility supported under Contract
DE-AC05-00OR22725, granted in part by the Advanced Scientiﬁc Computing Research (ASCR) Leadership Computing
Challenge (ALCC) program, resources supported by the Partnership for an Advanced Computing Environment (PACE)
at Georgia Tech. We thank Bryan Piatkowski, Jerry Parks and Justin North for genome information.

References

[1] Mu Gao, Peik Lund-Andersen, Alex Morehead, Sajid Mahmud, Chen Chen, Xiao Chen, Nabin Giri, Raj S. Roy,
Farhan Quadir, T. Chad Efﬂer, Ryan Prout, Subil Abraham, Wael Elwasif, N. Quentin Haas, Jeffrey Skolnick,
Jianlin Cheng, and Ada Sedova. High-performance deep learning toolbox for genome-scale prediction of protein
structure and function. In 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing
Environments (MLHPC), pages 46–57, 2021.

[2] Ian Sillitoe, Tony E. Lewis, Alison Cuff, Sayoni Das, Paul Ashford, Natalie L. Dawson, Nicholas Furnham,
Roman A. Laskowski, David Lee, Jonathan G. Lees, Sonja Lehtinen, Romain A. Studer, Janet Thornton, and
Christine A. Orengo. CATH: comprehensive structural and functional annotations for genome sequences. Nucleic
Acids Research, 43(D1):D376–D381, 10 2014.

[3] L. Holm and C. Sander. Protein structure comparison by alignment of distance matrices. J Mol Biol, 233(1):123–38,

1993.

[4] M. Gao and J. Skolnick. Apoc: large-scale identiﬁcation of similar protein pockets. Bioinformatics, 29(5):597–604,

2013.

[5] Mu Gao and Jeffrey Skolnick.

iAlign: a method for the structural comparison of protein–protein interfaces.

Bioinformatics, 26(18):2259–2265, 07 2010.

[6] Scott LeGrand, Aaron Scheinberg, Andreas F Tillack, Mathialakan Thavappiragasam, Josh V Vermaas, Rupesh
Agarwal, Jeff Larkin, Duncan Poole, Diogo Santos-Martins, Leonardo Solis-Vasquez, Andreas Koch, Stefano
Forli, Oscar Hernandez, Jeremy C. Smith, and Ada Sedova. GPU-accelerated drug discovery with docking on
the summit supercomputer: porting, optimization, and application to covid-19 research. In Proceedings of the
11th ACM international conference on bioinformatics, computational biology and health informatics, pages 1–10,
2020.

[7] Josh Vincent Vermaas, Ada Sedova, Matthew B Baker, Swen Boehm, David M Rogers, Jeff Larkin, Jens Glaser,
Micholas D Smith, Oscar Hernandez, and Jeremy C Smith. Supercomputing pipelines search for therapeutics
against covid-19. Computing in Science & Engineering, 23(1):7–16, 2020.

[8] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, page 1, 2021.

[9] Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin,
Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve
Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved
protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020.

12

Proteome-scale structure prediction

[10] J. Xu. Distance-based protein folding powered by deep learning. Proc Natl Acad Sci U S A, 116(34):16856–16865,

2019.

[11] M. Gao, H. Zhou, and J. Skolnick. Destini: A deep-learning approach to contact-driven protein structure prediction.

Sci Rep, 9(1):3514, 2019.

[12] Tatiana Tatusova, Michael DiCuccio, Azat Badretdin, Vyacheslav Chetvernin, Eric P. Nawrocki, Leonid Zaslavsky,
Alexandre Lomsadze, Kim D. Pruitt, Mark Borodovsky, and James Ostell. NCBI prokaryotic genome annotation
pipeline. Nucleic Acids Research, 44(14):6614–6624, 06 2016.

[13] The UniProt Consortium. UniProt: a worldwide hub of protein knowledge. Nucleic Acids Research, 47(D1):D506–

D515, 11 2018.

[14] Bharath Ramsundar, Peter Eastman, Patrick Walters, and Vijay Pande. Deep learning for the life sciences:
applying deep learning to genomics, microscopy, drug discovery, and more. " O’Reilly Media, Inc.", 2019.

[15] Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova,
David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, Augustin Žídek, Tim Green, Kathryn Tunyasuvunakool,
Stig Petersen, John Jumper, Ellen Clancy, Richard Green, Ankur Vora, Mira Lutﬁ, Michael Figurnov, Andrew
Cowie, Nicole Hobbs, Pushmeet Kohli, Gerard Kleywegt, Ewan Birney, Demis Hassabis, and Sameer Velankar.
AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space
with high-accuracy models. Nucleic Acids Research, 50(D1):D439–D444, 11 2021.

[16] Louise C. Serpell, Sheena E. Radford, and Daniel E. Otzen. Alphafold: A special issue and a special time for
protein science. Journal of Molecular Biology, 433(20):167231, 2021. From Protein Sequence to Structure at
Warp Speed: How Alphafold Impacts Biology.

[17] Wayne Joubert, James Nance, Deborah Weighill, and Daniel Jacobson. Parallel accelerated vector similarity

calculations for genomics applications. Parallel Computing, 75:130–145, 2018.

[18] Atanu Acharya, Rupesh Agarwal, Matthew B Baker, Jerome Baudry, Debsindhu Bhowmik, Swen Boehm,
Kendall G Byler, SY Chen, Leighton Coates, Connor J Cooper, et al. Supercomputer-based ensemble docking
drug discovery pipeline with application to covid-19. Journal of chemical information and modeling, 60(12):5832–
5852, 2020.

[19] Jens Glaser, Josh V Vermaas, David M Rogers, Jeff Larkin, Scott LeGrand, Swen Boehm, Matthew B Baker,
Aaron Scheinberg, Andreas F Tillack, Mathialakan Thavappiragasam, Ada Sedova, and Oscar Hernandez. High-
throughput virtual laboratory for drug discovery using massive datasets. The International Journal of High
Performance Computing Applications, 35(5):452–468, 2021.

[20] John Ossyra, Ada Sedova, Arnold Tharrington, Frank Noé, Cecilia Clementi, and Jeremy C Smith. Porting
adaptive ensemble molecular dynamics workﬂows to the summit supercomputer. In International Conference on
High Performance Computing, pages 397–417. Springer, 2019.

[21] John R Ossyra, Ada Sedova, Matthew B Baker, and Jeremy C Smith. Highly interactive, steered scientiﬁc
workﬂows on hpc systems: Optimizing design solutions. In International Conference on High Performance
Computing, pages 514–527. Springer, 2019.

[22] Anubhav Jain, Shyue Ping Ong, Wei Chen, Bharat Medasani, Xiaohui Qu, Michael Kocher, Miriam Brafman,
Guido Petretto, Gian-Marco Rignanese, Geoffroy Hautier, et al. Fireworks: A dynamic workﬂow system designed
for high-throughput applications. Concurrency and Computation: Practice and Experience, 27(17):5037–5059,
2015.

[23] Matthew Rocklin. Dask: Parallel computation with blocked algorithms and task scheduling. In Proceedings of the

14th python in science conference, volume 130, page 136. Citeseer, 2015.

[24] Dask Development Team. Dask: Library for dynamic task scheduling, 2016.

[25] Benjamín Hernández, Suhas Somnath, Junqi Yin, Hao Lu, Joe Eaton, Peter Entschev, John Kirkham, and Zahra
Ronaghi. Performance evaluation of python based data analytics frameworks in summit: Early experiences.
In Jeffrey Nichols, Becky Verastegui, Arthur ‘Barney’ Maccabe, Oscar Hernandez, Suzanne Parete-Koon, and
Theresa Ahearn, editors, Driving Scientiﬁc and Engineering Discoveries Through the Convergence of HPC, Big
Data and AI, pages 366–380, Cham, 2020. Springer International Publishing.

[26] Mark A. Coletti, Shang Gao, Spencer Paulissen, Nicholas Quentin Haas, and Robert Patton. Diagnosing
autonomous vehicle driving criteria with an adversarial evolutionary algorithm. In Proceedings of the 2021
Genetic and Evolutionary Computation Conference Companion, GECCO ’21, pages 301–302, New York, NY,
USA, July 2021. Association for Computing Machinery.

13

Proteome-scale structure prediction

[27] Mark Coletti, Alex Fafard, and David Page. Troubleshooting deep-learner training data problems using an

evolutionary algorithm on summit. IBM Journal of Research and Development, 64(3/4):1–12, 2019.

[28] Holger Gantikow, Steffen Walter, and Christoph Reich. Rootless containers with podman for hpc. In International

Conference on High Performance Computing, pages 343–354. Springer, 2020.

[29] Jeffrey Skolnick, Mu Gao, Hongyi Zhou, and Suresh Singh. Alphafold 2: Why it works and its implications for
understanding the relationships of protein sequence, structure, and function. Journal of chemical information and
modeling, 61(10):4827–4831, 2021.

[30] Sean R. Eddy. Accelerated proﬁle hmm searches. PLOS Computational Biology, 7(10):e1002195, 2011.
[31] Martin Steinegger, Markus Meier, Milot Mirdita, Harald Vöhringer, Stephan J Haunsberger, and Johannes Söding.
Hh-suite3 for fast remote homology detection and deep protein annotation. BMC bioinformatics, 20(1):1–15,
2019.

[32] Alex L Mitchell, Alexandre Almeida, Martin Beracochea, Miguel Boland, Josephine Burgin, Guy Cochrane,
Michael R Crusoe, Varsha Kale, Simon C Potter, Lorna J Richardson, Ekaterina Sakharova, Maxim Scheremetjew,
Anton Korobeynikov, Alex Shlemov, Olga Kunyavskaya, Alla Lapidus, and Robert D Finn. MGnify: the
microbiome analysis resource in 2020. Nucleic Acids Research, 48(D1):D570–D578, 11 2019.

[33] Martin Steinegger, Milot Mirdita, and Johannes Söding. Protein-level assembly increases protein sequence

recovery from metagenomic samples manyfold. Nature methods, 16(7):603–606, 2019.

[34] Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N. Shindyalov,

and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235–242, 01 2000.

[35] Bozitao Zhong, Xiaoming Su, Minhua Wen, Sicheng Zuo, Liang Hong, and James Lin. Parafold: Paralleling
alphafold for large-scale predictions. In International Conference on High Performance Computing in Asia-Paciﬁc
Region Workshops, HPCAsia 2022 Workshop, page 1–9, New York, NY, USA, 2022. Association for Computing
Machinery.

[36] Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin Žídek, Alex
Bridgland, Andrew Cowie, Clemens Meyer, Agata Laydon, Sameer Velankar, Gerard J. Kleywegt, Alex Bateman,
Richard Evans, Alexander Pritzel, Michael Figurnov, Olaf Ronneberger, Russ Bates, Simon A. A. Kohl, Anna
Potapenko, Andrew J. Ballard, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Ellen Clancy, David
Reiman, Stig Petersen, Andrew W. Senior, Koray Kavukcuoglu, Ewan Birney, Pushmeet Kohli, John Jumper, and
Demis Hassabis. Highly accurate protein structure prediction for the human proteome. Nature, 2021.

[37] Milot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger.

Colabfold-making protein folding accessible to all. bioRxiv, 2021.

[38] Michael Tress, Iakes Ezkurdia, Osvaldo Graña, Gonzalo López, and Alfonso Valencia. Assessment of predictions
submitted for the casp6 comparative modeling category. Proteins: Structure, Function, and Bioinformatics,
61(S7):27–45, 2005.

[39] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping
Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. OpenMM 7: Rapid development of high
performance algorithms for molecular dynamics. PLoS Computational Biology, 13(7):e1005659, 2017.

[40] Mathialakan Thavappiragasam, Vivek Kale, Oscar Hernandez, and Ada Sedova. Addressing load imbalance in
bioinformatics and biomedical applications: Efﬁcient scheduling across multiple gpus. In 2021 IEEE International
Conference on Bioinformatics and Biomedicine (BIBM), pages 1992–1999, 2021.

[41] Cynthia C Gilmour, Ally Bullock Soren, Caitlin M Gionfriddo, Mircea Podar, Judy D Wall, Steven D Brown,
Joshua K Michener, Maria Soledad Goñi Urriza, and Dwayne A Elias. Pseudodesulfovibrio mercurii sp. nov., a
mercury-methylating bacterium isolated from sediment. International Journal of Systematic and Evolutionary
Microbiology, 71(3):004697, 2021.

[42] A Christine Munk, Alex Copeland, Susan Lucas, Alla Lapidus, Tijana Glavina Del Rio, Kerrie Barry, John C
Detter, Nancy Hammon, Sanjay Israni, Sam Pitluck, et al. Complete genome sequence of rhodospirillum rubrum
type strain (s1 t). Standards in genomic sciences, 4(3):293–302, 2011.

[43] Justin A. North, Adrienne B. Narrowe, Weili Xiong, Kathryn M. Byerly, Guanqi Zhao, Sarah J. Young, Srividya
Murali, John A. Wildenthal, William R. Cannon, Kelly C. Wrighton, Robert L. Hettich, and F. Robert Tabita. A
nitrogenase-like enzyme system catalyzes methionine, ethylene, and methane biogenesis. Science, 369(6507):1094–
1098, 2020.

[44] Judy D. Wall, Grant M. Zane, Thomas R. Juba, Jennifer V. Kuehl, Jayashree Ray, Swapnil R. Chhabra, Valentine V.
Trotter, Maxim Shatsky, Kara B. De León, Kimberly L. Keller, Kelly S. Bender, Gareth Butland, Adam P. Arkin,

14

Proteome-scale structure prediction

Adam M. Deutschbauer, and Irene L. G. Newton. Deletion mutants, archived transposon library, and tagged
protein constructs of the model sulfate-reducing bacterium desulfovibrio vulgaris hildenborough. Microbiology
Resource Announcements, 10(11):e00072–21, 2021.

[45] John F Heidelberg, Rekha Seshadri, Shelley A Haveman, Christopher L Hemme, Ian T Paulsen, James F Kolonay,
Jonathan A Eisen, Naomi Ward, Barbara Methe, Lauren M Brinkac, et al. The genome sequence of the anaerobic,
sulfate-reducing bacterium desulfovibrio vulgaris hildenborough. Nature biotechnology, 22(5):554–559, 2004.

[46] Kristian Hassel, Magni O. Kyrkjeeide, Narjes Youseﬁ, Tommy Prestø, Hans K. Stenøien, Jonathan A. Shaw, and
Kjell Ivar Flatberg. Sphagnum divinum (sp. nov.) and s. medium limpr. and their relationship to s. magellanicum
brid. Journal of Bryology, 40(3):197–222, 2018.

[47] Avni Malhotra, Deanne J Brice, Joanne Childs, Jake D Graham, Erik A Hobbie, Holly Vander Stel, Sarah C Feron,
Paul J Hanson, and Colleen M Iversen. Peatland warming strongly increases ﬁne-root growth. Proceedings of the
National Academy of Sciences, 117(30):17627–17634, 2020.

[48] Laurel A Kluber, Eric R Johnston, Samantha A Allen, J Nicholas Hendershot, Paul J Hanson, and Christopher W
Schadt. Constraints on microbial communities, decomposition and methane production in deep peat deposits.
PloS one, 15(2):e0223744, 2020.

[49] Y. Zhang and J. Skolnick. Scoring function for automated assessment of protein structure template quality.

Proteins-Structure Function and Bioinformatics, 57(4):702–710, 2004.

[50] Rahul Alapati, M. Hossain Shuvo, and Debswapna Bhattacharya. SPECS: Integration of side-chain orientation and
global distance-based measures for improved evaluation of protein structural models. PLoS ONE, 15(2):e0228245,
2020.

[51] Kylie D Allen, Danielle V Miller, Benjamin J Rauch, John J Perona, and Robert H White. Homocysteine
is biosynthesized from aspartate semialdehyde and hydrogen sulﬁde in methanogenic archaea. Biochemistry,
54(20):3129–3132, 2015.

[52] Morgan N Price, Grant M Zane, Jennifer V Kuehl, Ryan A Melnyk, Judy D Wall, Adam M Deutschbauer, and
Adam P Arkin. Filling gaps in bacterial amino acid biosynthesis pathways with high-throughput genetics. PLoS
Genetics, 14(1):e1007147, 2018.

[53] Mu Gao, Davi Nakajima An, Jerry M Parks, and Jeffrey Skolnick. Predicting direct physical interactions in

multimeric proteins with deep learning. bioRxiv, 2021.

[54] John Paul Walters, Vidyananth Balu, Suryaprakash Kompalli, and Vipin Chaudhary. Evaluating the use of GPUs
in liver image segmentation and HMMER database searches. In 2009 IEEE International Symposium on Parallel
Distributed Processing, pages 1–12, 2009.

15

