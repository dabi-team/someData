1
2
0
2

n
a
J

8
2

]

R
P
.
h
t
a
m

[

3
v
0
0
3
4
0
.
2
1
8
1
:
v
i
X
r
a

DEEP NEURAL NETWORKS ALGORITHMS FOR
STOCHASTIC CONTROL PROBLEMS ON FINITE HORIZON:
CONVERGENCE ANALYSIS

C ˆOME HUR´E∗, HUYˆEN PHAM†, ACHREF BACHOUCH‡ , AND NICOLAS LANGREN´E§

Abstract. This paper develops algorithms for high-dimensional stochastic control problems
based on deep learning and dynamic programming. Unlike classical approximate dynamic program-
ming approaches, we ﬁrst approximate the optimal policy by means of neural networks in the spirit
of deep reinforcement learning, and then the value function by Monte Carlo regression. This is
achieved in the dynamic programming recursion by performance or hybrid iteration, and regress now
methods from numerical probabilities. We provide a theoretical justiﬁcation of these algorithms.
Consistency and rate of convergence for the control and value function estimates are analyzed and
expressed in terms of the universal approximation error of the neural networks, and of the statistical
error when estimating network function, leaving aside the optimization error. Numerical results on
various applications are presented in a companion paper [3] and illustrate the performance of the
proposed algorithms.

Key words. Deep learning, dynamic programming, performance iteration, regress now, conver-

gence analysis, statistical risk.

AMS subject classiﬁcations. 65C05, 90C39, 93E35

0
∈
\ {
Rd is given by

}

⊂
(1.1)

1. Introduction. A large class of dynamic decision-making problems under un-
certainty can be mathematically modeled as discrete-time stochastic optimal control
problems in ﬁnite horizon. This paper is devoted to the analysis of novel probabilistic
numerical algorithms based on neural networks for solving such problems. Let us
consider the following discrete-time stochastic control problem over a ﬁnite horizon
. The dynamics of the controlled state process X α = (X α
N

N

n )n valued in

X

X α

n+1 = F (X α

n , αn, εn+1), n = 0, . . . , N

1, X α

0 = x0 ∈

−

Rd,

×

×

where (εn)n is a sequence of i.i.d.
(E)), and deﬁned on some probability space (Ω,
(E,
B
F = (
Fn)n generated by the noise (εn)n (
= (αn)n is an F-adapted process valued in A
E into Rd.
from Rd

random variables valued in some Borel space
, P) equipped with the ﬁltration
F0 is the trivial σ-algebra), the control α
Rq, and F is a measurable function

Rq

⊂

F

−

1
N
n=0 f (X α

Given a running cost function f deﬁned on Rd

Rq, a terminal cost function
g deﬁned on Rd, the cost functional associated with a control process α is J(α) =
E
of admissible control is the set of control

n , αn) + g(X α
N )
hP
i
∗LPSM, Universit´e de Paris (Paris Diderot) (hure at lpsm.paris).
†LPSM, Universit´e de Paris (Paris Diderot) and CREST-ENSAE (pham at lpsm.paris). The work
of this author is supported by the ANR project CAESARS (ANR-15-CE05-0024), and also by FiME
and the “Finance and Sustainable Development” EDF - CACIB Chair.

. The set

×

C

‡Department of Mathematics, University of Oslo, Norway. (achrefb at math.uio.no). The author’s
research was carried out with support of the Norwegian Research Council, within the research project
Challenges in Stochastic Control, Information and Applications (STOCONINF), project number
250768/F20.

§CSIRO Data61, Melbourne, Australia (nicolas.langrene at csiro.au).

1

 
 
 
 
 
 
2

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

processes α satisfying some integrability conditions ensuring that the cost functional
J(α) is well-deﬁned and ﬁnite. The control problem, also called Markov decision
process (MDP), is formulated as

(1.2)

J(α),

V0(x0) := inf
∈C
, i.e., attaining the optimal value:
and the goal is to ﬁnd an optimal control α∗
V0(x0) = J(α∗). Notice that problem (1.1)-(1.2) may also be viewed as the time
discretization of a continuous time stochastic control problem, in which case F is
typically the Euler scheme for a controlled diﬀusion process, and V0 is the discrete-
time approximation of a fully nonlinear Hamilton-Jacobi-Bellman equation.

∈ C

α

P a(x, dx′), a

Problem (1.2) is tackled by the dynamic programming approach, and we introduce
the standard notations for MDP: denote by
, the family
of transition probabilities associated with the controlled (homogenous) Markov chain
(1.1), given by P a(x, dx′) := P [F (x, a, ε1)
dx′] and for any measurable function
∈
ϕ(x′)P a(x, dx′) = E
. With these notations,
F (x, a, ε1)
ϕ on
ϕ
we have for any measurable function ϕ on
|Fn] =
, for any α
R
(cid:0)
(cid:1)(cid:3)
X
N. The optimal value V0(x0) is then determined in backward
P αn ϕ(X α
, and by the
induction starting from the terminal condition VN (x) = g(x), x
dynamic programming (DP) formula, for n = N

: P aϕ(x) =

, E[ϕ(X α

1, . . . , 0:

A, x

∈ X }

n+1)

∈ X

∈ C

n ),

X

∈

∈

n

∀

{

(cid:2)

−

(1.3)

Qn(x, a) = f (x, a) + P aVn+1(x), x

(

Vn(x) = inf
A
∈

a

Qn(x, a),

, a

∈

∈ X

A,

The function Qn is called optimal state-action value function, and Vn is the (optimal)
value function. Moreover, when the inﬁmum is attained in the DP formula at any
time n by a∗n(x), we get an optimal control in feedback form given by: α∗ = (a∗n(X ∗n))n
where X ∗ = X α

is the Markov process deﬁned by

∗

X ∗n+1 = F (X ∗n, a∗n(X ∗n), εn+1), n = 0, . . . , N

1, X ∗0 = x0.

−

The implementation of the DP formula requires the knowledge and explicit com-
putation of the transition probabilities P a(x, dx′). In situations when they are un-
known, this leads to the problematic of reinforcement learning for computing the
optimal control and value function by relying on simulations of the environment. The
challenging tasks from a numerical point of view are then twofold:

1. Transition probability operator. Calculations for any x

A, of
P aVn+1(x), for n = 0, . . . , N
1. This is a computational challenge in high dimen-
sion d for the state space with the “curse of dimensionality” due to the explosion
of grid points in deterministic methods.

, and any a

∈ X

−

∈

2. Optimal control. Computation of the inﬁmum in a

A of f (x, a) + P aVn+1(x)
for ﬁxed x and n, and of ˆan(x) attaining the minimum if it exists. This is also a
computational challenge especially in high dimension q for the control space.

∈

The classical probabilistic numerical methods based on DP for solving the MDP
are sometimes called approximate dynamic programming methods, see e.g. [6], [23],
and consist basically of the two following steps:

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

3

(i) Approximate the Qn. This can be performed by regression Monte Carlo (RMC)
techniques or quantization. RMC is typically done by least squares linear regression
on a set of basis function following the popular approach by Longstaﬀ and Schwartz
[21] initiated for the Bermudan option problem, where the suitable choice of basis
functions might be delicate. Conditional expectation can also be approximated
by regression on neural networks as in [17] for the American option problem and
appears as a promising and eﬃcient alternative in high dimension.

(ii) Control search. Once we get an approximation (x, a)

ˆQn(x, a) of the Qn value
A of
function, the optimal control ˆan(x) which achieves the minimum over a
Qn(x, a) can be obtained either by an exhaustive search when A is discrete (with
relatively small cardinality), or by a (deterministic) gradient-based algorithm for
continuous control space (with relatively small dimension).

7→

∈

1

−

−

×

∈

−

1)

Recently, numerical methods by direct approximation, without DP, have been
developed and made implementable thanks to the power of computers: the basic
idea is to focus directly on the control approximation by considering feedback control
(policy) in a parametric form: an(x) = A(x; θn), n = 0, . . . , N
1, for some given
N , and minimize over θ
function A(., θn) with parameters θ = (θ0, . . . , θN
N
the parametric functional ˜J(θ) = E
n=0 f (X A
n )n
denotes the controlled process with feedback control (A(., θn))n. This approach was
ﬁrst adopted in [19], which used the EM algorithm for optimizing over the param-
eter θ, and was further investigated in [11], [8], [13], which considered deep neural
networks (DNN) for the parametric feedback control, and stochastic gradient descent
methods (SGD) for computing the optimal parameter θ. The theoretical foundation
of these DNN algorithms has been recently investigated in [12]. Let us mention that
DNN approximation in stochastic control has already been explored in the context
of reinforcement learning (RL) (see [6] and [24]), and is called deep reinforcement
learning in the artiﬁcial intelligence community [22] (see also [20] for a recent survey)
but usually for inﬁnite horizon (stationary) control problems.

Rq
n , A(x; θn)) + g(X A
N )
i

, where (X A

hP

In this paper, we combine diﬀerent ideas from the mathematics (numerical proba-
bility) and the computer science (reinforcement learning) communities to propose and
compare several algorithms based on dynamic programming (DP), and deep neural
networks (DNN) for the approximation/learning of (i) the optimal policy, and then
of (ii) the value function. Notice that this diﬀers from the classical approach in DP
recalled above, where we ﬁrst approximate the Q-optimal state/control value func-
tion, and then approximate the optimal control. Our learning of the optimal policy is
achieved in the spirit of [11] by DNN, but sequentially in time through DP instead of
a global learning over the whole period 0, . . . , N
1. Once we get an approximation
of the optimal policy, we approximate the value function by Monte Carlo (MC) re-
gression based on simulations of the forward process with the approximated optimal
control. In particular, we avoid the issue of a priori endogenous simulation of the
controlled process in the classical Q-approach. The MC regressions for the approx-
imation of the optimal policy and/or value function, rely on performance iteration
(PI) or hybrid iteration (HI). Numerical results on several applications are devoted
to a companion paper [3]. The theoretical contribution of the current paper is to

−

4

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

provide a detailed convergence analysis of our two proposed algorithms: Theorem 4.6
for the NNContPI algorithm based on control learning by performance iteration with
DNN, and Theorem 4.13 for the Hybrid-Now algorithm based on control learning by
DNN and then value function learning by the regress-now method. We rely mainly
on arguments from statistical learning and non parametric regression as developed
notably in the book [10] to give estimates of approximated control and value func-
tion in terms of the universal approximation error of the neural networks, and of the
statistical error in the estimation of network functions. In particular, the estimation
of the optimal control using the control that minimizes the empirical loss is studied
based on the uniform law of large numbers (see [10]); and the estimation of the value
function in Hybrid-Now relies on Lemma H.1, proved in [17], which bounds the diﬀer-
ence between the conditional expectation and the best approximation of the empirical
conditional expectation by neural networks. In practical implementation, the com-
putation of the approximate optimal policy in the minimization of the empirical loss
function is performed by a stochastic gradient descent algorithm, which leads to the
so-called optimization error, see e.g. [5], [15]. We do not address in the current paper
this type of error in machine learning algorithms, and we refer to the recent paper [4]
for a full error analysis of deep neural networks training.

The paper is organized as follows. We recall in Section 2 some basic results about
deep neural networks (DNN) and stochastic optimization gradient descent methods.
Section 3 is devoted to the description of our two algorithms. We analyze in Section
4 the convergence of the algorithms. Finally the Appendices collect some Lemmas
used in the proof of the convergence results.

2. Preliminaries on DNN and SGD.

2.1. Neural network approximations. Deep Neural networks (DNN) aim to
approximate (complex nonlinear) functions deﬁned on ﬁnite-dimensional space, and
in contrast with the usual additive approximation theory built via basis functions,
like polynomial, they rely on composition of layers of simple functions. The rele-
vance of neural networks comes from the universal approximation theorem and the
Kolmogorov-Arnold representation theorem (see [18], [7] or [14]), and this has been
shown to be successful in numerous practical applications.

We consider here a feedforward artiﬁcial network (also called multilayer percep-
Rq) and the value
Rd. The architecture is
Ro, with o = q or 1 in our context,

tron) for the approximation of the optimal policy (valued in A
function (valued in R), both deﬁned on the state space
represented by functions x
and where θ

Rp are the weights (or parameters) of the neural networks.

∈ X 7−→

Φ(z; θ)

X ⊂

⊂

Θ

∈

∈

⊂

2.2. Stochastic optimization in DNN. Approximation by means of DNN
requires a stochastic optimization with respect to a set of parameters, which can be
written in a generic form as

(2.1)

E

inf
θ

L(Z; θ)

,

where Z is a random variable from which the training samples Z (m), m = 1, . . . , M
Rp, and
are drawn, and L is a loss function involving DNN with parameters θ

(cid:2)

(cid:3)

∈

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

5

typically diﬀerentiable w.r.t. θ with known gradient DθL(Z (m)).

Several algorithms based on stochastic gradient-descent are often used in practice
for the search of inﬁmum in (2.1); see e.g. basic SGD, AdaGrad, RMSProp, or Adam.
In the theoretical study of the algorithms proposed in this paper, we did not
address the convergence of the gradient-descent algorithms, and rather assumed that
the agent successfully solved the minimization problem (2.1), i.e. minimized the
M
m=1 L(Z (m)) using (non stochastic) gradient-descent, a stochastic
empirical loss
gradient-descent method with many epochs or any other methods that allow over-
ﬁtting. One can then rely on statistical learning results as in [10] and [17] that are
speciﬁc to neural networks. Notice that overﬁtting should be avoided in practice, and
so our framework remains theoretical.

P

3. Description of the algorithms. Let us introduce a set

of neural networks

for approximating optimal policies, that is a set of parametric functions x
A(x; β)
∈
approximating value functions, that is a set of parametric functions x

∈ X 7→
of neural networks functions for
Φ(x; θ)

A, with parameters β

Rl, and a set

∈

V

A

∈ X 7→

R, with parameters θ

Rp.

∈

∈

We are also given at each time n a probability measure µn on the state space
,
X
which we refer to as a training distribution. Some comments about the choice of the
training measure are discussed in Section 3.3.

3.1. Control learning by performance iteration. This algorithm, referred

in short as NNcontPI algorithm, is designed as follows:

For n = N

n + 1, . . . , N

−

1, . . . , 0, keep track of the approximated optimal policies ˆak, k =
−
1, and approximate the optimal policy at time n by ˆan = A(.; ˆβn) with

(3.1)

ˆβn ∈

argmin

Rl

β

∈

f (Xn, A(Xn; β)) +

E

"

N

1

−

Xk=n+1

f ( ˆX β

k , ˆak( ˆX β

k )) + g( ˆX β
N )
#

,

µn, ˆX β
1, ˆX β

where Xn ∼
n + 1, . . . , N
−
ˆaM
k of ˆak, k = n + 1, . . . , N
a training sample

P A(Xn;β)(Xn, dx′), and for k =
n+1 = F (Xn, A(Xn; β), εn+1)
k , ˆak( ˆX β
k+1 = F ( ˆX β
k , dx′). Given estimate
1, the approximated policy ˆan is estimated by using
for

, m = 1, . . . , M of

∼
P ˆak( ˆX β

k ), εk+1)

−
n , (ε(m)
k+1)k=N

Xn, (εk+1)k=N
k=n

k )( ˆX β

k=n

∼

−

−

1

1

simulating
variants) as described in Section (2.2).

−

1

(cid:16)

(cid:17)
The estimated value function at time n is then given as

(cid:0)
, and optimizing β of A(.; β)

(cid:17)

, by SGD (or its

(cid:1)

∈ A

X (m)
(cid:16)
Xn, ( ˆX β
k+1)k=N

k=n

(3.2)

ˆV M
n (x) = EM

N

1

−

"

Xk=n

f ( ˆX n,x
k

, ˆaM

k ( ˆX n,x

k

)) + g( ˆX n,x
N )
#

,

N

where EM is the expectation conditioned on the training set used for computing
ˆX n,x
ˆaM
is driven by the estimated optimal controls. The dependence
k
k
of the estimated value function ˆV M
, for m = 1, . . . , M ,
(cid:0)
used at time k = n, . . . , N , is emphasized through the exponent M in the notations.

n upon the training samples X (m)

k; and
(cid:1)

k=n

(cid:16)

(cid:17)

k

6

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

Remark 3.1. The NNcontPI algorithm can be viewed as a combination of the
DNN algorithm designed in [11] and dynamic programming. In the algorithm pre-
sented in [11], which totally ignores the dynamic programming principle, one learns
all the optimal controls A(.; βn), n = 0, . . . , N
1 at the same time, by performing
one unique stochastic gradient descent. This is eﬃcient as all the parameters of all
the NN are getting trained at the same time, using the same mini-batches. However,
when the number of layers of the global neural network gathering all the A(.; βn), for
100, where ℓn is the number of layers in
n = 0, . . . , N
A(., βn)), then one is likely to observe vanishing or exploding gradient problems that
will aﬀect the training of the weights and bias of the ﬁrst layers of the global NN (see
✷
[9] for more details).

n=0 ℓn ≥

1 is large (say

P

−

−

N

−

1

Remark 3.2. The NNcontPI algorithm does not require value function iteration,
but instead is based on performance iteration by keeping track of the estimated opti-
mal policies computed in backward recursion. The value function is then computed
in (3.2) as the gain functional associated with the estimated optimal policies (ˆaM
k )k.
Consequently, it provides usually a low bias estimate but induces possibly high vari-
✷
ance estimate and large complexity, especially when N is large.

3.2. Control learning by hybrid iteration. Instead of keeping track of all the
approximated optimal policies as in the NNcontPI algorithm, we use an approximation
of the value function at time n + 1 in order to compute the optimal policy at time
n. The approximated value function is then updated at time n. This leads to the
following algorithm:

Hybrid-Now algorithm
1. Initialization: ˆVN = g

2. For n = N

1, . . . , 0,

−

(i) Approximate the optimal policy at time n by ˆan = A(.; ˆβn) with

(3.3)

ˆβn ∈

argmin

Rl

β

∈

E

f (Xn, A(Xn; β)) + ˆVn+1(X A(.,β)
h

n+1

)

,

i

where Xn ∼

µn, ˆX A(.,β)

n+1 = F (Xn, A(Xn; β), εn+1)

P A(Xn;β)(Xn, dx′).

∼

(ii) Updating: approximate the value function by NN:

(3.4)

ˆV M
n ∈

argmin
Φ(.;θ)

E

f (Xn, ˆaM

n (Xn)) + ˆV M

n+1(X ˆaM
n+1)

n

Φ(Xn; θ)

2

−

using samples X (m)

∈V

(cid:12)
(cid:12)
n , X ˆaM
n ,(m)

n+1

, m = 1, . . . , M of Xn ∼

(cid:12)
(cid:12)
n ,(m)

µn, and X ˆaM

n+1

of X ˆaM

n
n+1

The approximated policy ˆan is estimated by using a training sample (X (m)

n , ε(m)

n+1),

m = 1, . . . , M of (Xn, εn+1) to simulate
over the parameters β

∈ A
Rl and the expectation in (3.3) by stochastic gradient descent
(cid:17)

, and optimizing A(.; β)

Xn, X A(.;β)
(cid:16)

n+1

∈

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

7

methods as described in Section (2.2). We then get an estimate ˆaM
. The
value function is represented by neural network and its optimal parameter is estimated
by MC-regression in (3.4).

n = A

.; ˆβM
n
(cid:16)

(cid:17)

3.3. Training sets design. We discuss here the choice of the training measure
µn used to generate the training sets on which will be computed the estimates. Two
cases are considered in this section. The ﬁrst one is a knowledge-based selection,
relevant when the controller knows with a certain degree of conﬁdence where the
process has to be driven in order to optimize her cost functional. The second case,
on the other hand, is when the controller has no idea where or how to drive the
process to optimize the cost functional. Separating the construction of the training
set from the training procedure of the optimal control and/or value function simpliﬁes
the theoretical study of our algorithms. It is nevertheless common to merge them in
practice, as discussed in Remark 3.3.

Exploitation only strategy In the knowledge-based setting, there is no need for
exhaustive and expensive (in time mainly) exploration of the state space, and the
controller can directly choose training sets Γn constructed from distributions µn that
assign more points to the parts of the state space where the optimal process is likely
to be driven. In practice, at time n, assuming we know that the optimal process is
likely to stay in the ball centered around the point mn and with radius rn, we choose
n), and build the
a training measure µn centered around mn as, for example
training set as sample of the latter.

(mn, r2

N

−

Explore ﬁrst, exploit later If the agent has no idea of where to drive the process
to receive large rewards, she can always proceed to an exploration step to discover
favorable subsets of the state space. To do so, Γn, the training sets at time n, for
n = 0, . . . , N
1, can be built by forward simulation using optimal strategies esti-
mated using algorithms developed e.g. in [11]. We are then back to the Exploitation
only strategy framework. An alternative suggestion is to use the approximate policy
obtained from a ﬁrst iteration of the hybrid-now algorithm, in order to derive the
associated approximated optimal state process, ˆX (1).
In a second iteration of the
n of ˆX (1)
hybrid-now algorithm, we then use the marginal distribution µ(1)
n . We can
even iterate this procedure in order to improve the choice of the training measure µn,
as proposed in [19].

Remark 3.3. For stationary control problems, which are the common framework
in reinforcement learning, it is usual to combine exploration of the (state and action)
space and learning of the value function by drawing the mini-batches under the law
given by the current estimate of the Q-value plus some noise (commonly called ε-
In practice, this method works well, and oﬀers a ﬂexible trade-oﬀ
greedy policy).
between exploration and exploitation.
It looks challenging, however, to study its
performance theoretically because it mixes the gradient-descent steps for the training
of the value function and the optimal control. Also, note that this method is not
easily adaptable to ﬁnite-horizon control problems.

8

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

4. Convergence analysis. This section is devoted to the convergence of the
n of the value function Vn obtained from a training sample of size M and

estimator ˆV M
using DNN algorithms listed in Section 3.

Training samples rely on a given family of probability distributions µn on

, for
n = 0, . . . , N , referred to as training distribution (see Section 3.3 for a discussion on
the choice of µ). For the sake of simplicity, we consider that µn does not depend on
n, and denote then by µ the training distribution. We shall assume that the family
of controlled transition probabilities has a density w.r.t. µ, i.e.,

X

P a(x, dx′) = r(x, a; x′)µ(dx′).

We shall assume that r is uniformly bounded in (x, x′, a)
Lipschitz w.r.t. (x, a), i.e.,

2

∈ X

×

A, and uniformly

(Hd) There exists some positive constants

r

and [r]L s.t.

|

|

|

r

−

k∞

| ≤

| ≤ k

∈ X
+

r(x, a; x′)

r(x1, a1; x′)

r(x2, a2; x′)

k∞
[r]L (
|

,
∀
x1 −

, a
a1 −

A,
∈
),
a2|

k
x, x′
x2|
Remark 4.1. Assumption (Hd) is usually satisﬁed when the state and control
space are compacts. While the compactness on the control space A is not explicitly
assumed, the compactness condition on the state space
turns out to be more crucial
for deriving estimates on the estimation error (see Lemma 4.10), and will be assumed
can be relaxed
to hold true for simplicity. Actually, this compactness condition on
by truncation and localization arguments (see proposition A.1 in Appendix A) by
considering a training distribution µ such that (Hd) is true and which admits a
✷
.
moment of order 1, i.e.
∞

x1, x2 ∈ X
∀

, a1, a2 ∈

dµ(y) < +

A.

X

X

y

|

|

We shall also assume some boundedness and Lipschitz condition on the reward func-
tions:

R

(HR) There exists some positive constants

f

,

g

, [f ]L, and [g]L s.t.

|

|

f

k

k∞

| ≤ k

f (x, a)

f (x1, a1)

f (x2, a2)

−
g(x1)

k
g(x)
x2|
,
x2|
Under this boundedness condition, it is clear that the value function Vn is also
f

k∞
,
g
k∞
a2|
),
a1 −
x1, x2 ∈ X
∀

n)
k
We shall ﬁnally assume a Lipschitz condition on the dynamics of the MDP.

,
k∞
|
[f ]L (
x1 −
|
x1 −
[g]L |

, a1, a2 ∈

Vnk∞ ≤

| ≤ k
+

0, ..., N

, for n

g(x2)

∈ X

∈ {

| ≤

| ≤

k∞

k∞

, a

A,

A.

(N

−

+

−

∈

x

∀

k

}

k

g

|

|

.

bounded, and we have:

(HF) For any e

A:

X ×

∈

E, there exists C(e) such that for all couples (x, a) and (x′, a′) in

F (x, a, e)

|

−

F (x′, a′, e)

| ≤

C(e) (

x

|

+

x′

|

a

|

a′

) .

|

−

−

For M

N∗, let us deﬁne ρM = E

sup
m
≤
sample of the noise ε. The rate of convergence of ρM toward inﬁnity will play a crucial
role to show the convergence of the algorithms.

, where the (εm)m is an i.i.d.

C(εm)
(cid:21)

∈

M

(cid:20)

≤

1

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

9

Remark 4.2. A typical example when (HF) holds is the case where F is deﬁned
through the time discretization of an Euler scheme, i.e. F (x, a, ε) := b(x, a)+σ(x, a)ε,
(0, Id), where
with b and σ Lipschitz-continuous w.r.t. the couple (x, a), and ε
d. Indeed, in this case, it is straightforward to see
Id is the identity matrix of size d
kd, where [b]L and [σ]L stand for the Lipschitz coeﬃcients
that C(ε) = [b]L + [σ]Lk
kd stands for the Euclidean norm in Rd. Moreover, one can show
of b and σ, and
that:

∼ N

×

k

ε

.

(4.1)

ρM ≤

[b]L + d[σ]L

2 log(2dM ),

which implies in particular that ρM

=
+
→
inequality (4.1). For this, let us ﬁx some integer M ′ > 0 and let Z := sup
≤

. Let us indeed check the
ǫm
1 |
(0, 1). From Jentzen inequality to the r.v. Z and

1 are i.i.d. such that ǫ1

log(M )

(cid:16)p

M ′ |

(cid:17)

M

m

≤

1

p
∞ O

1 ∼ N

exp(tz), where t > 0 will be ﬁxed later, we get

where ǫm
the convex function z

7→

etE[Z]

≤

E

exp

tZ

E

≤

(cid:2)

(cid:0)

(cid:1)(cid:3)

sup
m

≤

M ′

"
1

≤

exp

t

ǫm
1 |
|
(cid:1)

(cid:0)

# ≤

m=1
X

h

ǫm
1 |
|

(cid:0)

(cid:1)i

′

M

E

exp

t

2M ′ exp

≤

t2
2

,

(cid:0)

(cid:1)

where we used the closed-form expression of the moment generating function of the
folded normal distributiona to write the last inequality. Hence, we have for all t > 0:

We get, after taking t =

(cid:2)
2 log(2M ′):

(cid:3)

E

Z

log(2M ′)
t

+

t
2

.

≤

(4.2)

p

Since inequality

x
kd ≤

k

d
k

x

k∞

2 log(2M ′).

E

Z

≤
holds for all x

p

(cid:2)

(cid:3)

Rd, we derive

∈

E

(cid:20)

sup
m

≤

C(εm)
(cid:21)

≤

M

[b]L + d[σ]LE

1

≤

(cid:20)

sup
m

≤

dM

C(ǫm
1 )
(cid:21)

,

1

≤

and apply (4.2) with M ′ = dM , to complete the proof of (4.1).

✷

Remark 4.3. Under (Hd), (HR) and (HF), it is straightforward to see from the
dynamic programming formula (1.3) that Vn is Lipschitz for all n = 0, . . . , N , with a
Lipschitz coeﬃcient [Vn]L that can be bounded by standard arguments as follows:

[VN ]L = [g]L
[Vn]L ≤

min

Vn
k

[f ]L +

[r]L , ρ1



N∗ and x = 0. The Lipschitz continuity
using the usual convention 1

−
1
−
of Vn plays a signiﬁcant role in proving the convergence of the Hybrid algorithms
✷
described and studied in section 4.2.

xp
x = p for p

, for n = 0, . . . , N

[g]L

k∞

∈

1,

−

(cid:19)

(cid:18)

−

−

+ ρN
1

n

1

ρN −n
1
−
ρ1
1

aThe folded normal distribution is the one of |Z| with Z ∼ N1(µ, σ). Its moment generating
,
(cid:1)(cid:3)

σ2t2
2 − µt(cid:17) (cid:2)

σ2t2
2 + µt(cid:17) (cid:2)

+ exp (cid:16)

µ
σ − σt

σ − σt

− µ
(cid:0)

1 − Φ

1 − Φ

(cid:1)(cid:3)

(cid:0)

function is given by t 7→ exp (cid:16)
where Φ is the c.d.f. of N1(0, 1).

10

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

4.1. Control learning by performance iteration (NNcontPI). In this
paragraph, we analyze the convergence of the NN control learning by performance
iteration as described in Section 3.1. We shall consider the following set of neural
networks for approximating the optimal policy:

(4.3)

γ
η
K :=
A

x

n

∈ X 7→

A(x; β) = (A1(x; β), . . . , Aq(x; β))

A,

∈

K

Ai(x; β) = σA

cij (aij.x + bij)+ + c0j

,

i = 1, . . . , q,

(cid:16)

j=1
X

(cid:17)

K

R,

i=0
X

β = (aij , bij, cij)i,j, aij ∈

Rd,

aijk ≤

k

η, bij, cij ∈

cij | ≤

|

γ

,

.

k

k

o
is the Euclidean norm in Rd. Note that the considered neural networks have
where
one hidden layer, K neurons, Relu activation functions, and σA as output layer. γ
and η are often referred to in the literature as respectively total variation and kernel.
The activation function σA is chosen depending on the form of the control space A.
For example, when A = Rq, we simply take σA as the identity function; when A =
q

Rq

+, or more generally in the form A =

[ai,

), one can take the component-wise

i=1
Y
Relu activation function (possibly shifted and scaled); and when A = [0, 1]q, or more

∞

q

i=1
Y

generally A =

[ai, bi], for ai ≤

bi, i = 1, . . . , q, one can take the component-wise

sigmoid activation function (possibly shifted and scaled); when A is a discrete space
corresponding to discrete control values
, we may consider randomized
{
control, hence valued in the simplex of ˆA of [0, 1]L, and use a softmax output layer
activation function for parametrizing the discrete probabilities valued in ˆA (such al-
gorithm, called ClassifPI, has been developed in [3]; see also [9] on softmax output for
classiﬁcation problems, and [1] about softmax policies for action selection and possible
improvements.)

, aL}

a1,

· · ·

Let KM , ηM and γM be sequences of integers such that

(4.4)

KM , γM , ηM −−−−→M
AM := ηM
A

∞

We denote by
with parameters ηM , KM and γM that satisfy the conditions (4.4).

→∞
γM
KM the class of neural network for policy as deﬁned in (4.3)

and ρN

1

M γN

M ηN

−

−

M

1

−

log(M )

2

r

M −−−−→M
→∞

0.

Remark 4.4. In the case where F is deﬁned in dimension d as: F (x, a, ε) =
✷

b(x, a) + σ(x, a)ε, we can use (4.1) to get: ρN
M

log(M )

N

−

−

n

n

.

(cid:16)p
Recall that the approximation of the optimal policy in the NNcontPI algorithm is
1, . . . , 0, generate a training
n , m = 1, . . . , M from the training distribution µ, and samples

computed in backward induction as follows: For n = N
sample for the state X (m)
of the exogenous noise

−

(cid:17)

=
+
→

∞

M

O

Compute the approximated policy at time n
(cid:0)

•

εm
k

M,N
m=1,k=n+1.
(cid:1)

(4.5)

ˆaM
n

∈

1
M

argmin
A
M

∈A

M
m=1

P

n , A(X (m)

n )) + ˆY (m),A

n+1

f (X (m)
h

i

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

11

where

(4.6)

with

ˆY (m),A
n+1 =

N

1

−

f

, ˆaM
k

X (m),A
k
(cid:16)

X (m),A
k
(cid:16)

+ g

X (m),A
N

,

X (m),A
k

Xk=n+1

(cid:17)(cid:17)

(cid:17)
N
k=n+1 deﬁned by induction as follows, for m=1, . . . , M :
(cid:1)
X (m),A
n+1
X (m),A
k

X m
X m
n , A
n
X (m),A
, ˆaM
(cid:0)
k
1
k
−

, εm
n+1
X (m),A
(cid:17)
(cid:1)
1
k
−

= F

= F

, εm
k

(cid:16)

(cid:16)

,

(cid:0)




for k = n + 2, . . . , N.

Compute the estimated value function ˆV M



n as in (3.2).

(cid:16)

(cid:0)

(cid:1)

(cid:17)

•

Remark 4.5. Finding the argmin in (4.5) is highly non-trivial and impossible in

real situation.
It is slowly approximated by (non stochastic) gradient-descent, i.e.
with one large batch of size M , for which some convergence results are known under
convexity or smoothness assumptions of F , f , g; and is approximated in practice by
stochastic gradient-descent with a large enough number of epochs. We refer to [15]
for error analysis of stochastic gradient descent optimization algorithms.
In order
to simplify the theoretical analysis, we assume that the argmin in (4.5) is exactly

reached. Our main focus here is to understand how the ﬁnite size of the training set
✷
aﬀects the convergence of our algorithms.

We now state our main result about the convergence of the NNcontPI algorithm.
Theorem 4.6. Assume that there exists an optimal feedback control (aopt

1
k )N
−
k=n
µ. Then,

for the control problem with value function Vn, n = 0, . . . , N , and let Xn ∼
as M

b
→ ∞

E

ˆV M
n (Xn)

−

Vn(Xn)

=

(4.7)

(cid:2)

(cid:3)

ρN
M

−

1

n

−

n

γN
−
−
M
√M

O 

1

ηN
M

−

2

n

−

+ sup
N
k

n

inf
∈A

M

A

1

E

A(Xk)

|

aopt
k (Xk)
|

−

,
!

≤
where E stands for the expectation over the training set used to evaluate the approx-
imated optimal policies (ˆaM
N controlled by
the latter. Moreover, as M

1, as well as the path (Xn)n

k )n

N

−

≤

≤

≤

−

(cid:2)

(cid:3)

k

k

≤
≤
c
→ ∞

EM

ˆV M
n (Xn)

(4.8)

(cid:2)

Vn(Xn)

=

P
O

−

ρN
M

−

n

−

1

γN
M

−

n

−

1

ηN
M

−

2

n

−

(cid:3)

log(M )
M

r

≤
where EM stands for the expectation conditioned by the training set used to estimate
the optimal policies (ˆaM

−

≤

(cid:2)

(cid:3)

k )n

k

N

≤

≤

−

1.

+ sup
N
k

n

inf
∈A

M

A

1

E

A(Xk)

|

aopt
k (Xk)
|

−

,

!

bThe notation xM = O(yM ) as M → ∞, means that the ratio |xM |/|yM | is bounded as M goes

to inﬁnity.

cThe notation xM = OP(yM ) as M → ∞, means that there exists c > 0 such that P

c|yM |

(cid:1)

→ 0 as M goes to inﬁnity.

|xM | >
(cid:0)

 
12

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

M

Remark 4.7. 1. The term ρN −n−1

should be seen as the estimation
error.
It is due to the approximation of the optimal controls by means of neural
AM using empirical cost functional in (4.5). We show in section B that
networks in
this term disappears in the ideal case where the real cost functional (i.e. not the
empirical one) is minimized.

γN −n−1
M
√M

ηN −n−2
M

2. The rate of convergence depends dramatically on N since it becomes exponen-
tially slower when N goes to inﬁnity. This is a huge drawback for this performance
iteration-based algorithm. We will see in the next section that the rate of convergence
✷
of value iteration-based algorithms does not suﬀer from this problem.
ˆV M
Comment: Since we clearly have Vn ≤
n , estimation (4.7) implies the convergence
in L1 norm of the NNcontPI algorithm, under condition (4.4), and in the case where

sup
k

≤

≤

n

N

inf
∈A

M

A

E

|

(cid:2)

A(Xk)

aopt
k (Xk)
|

−

0 .

−−−−−→M
+
∞
→

(cid:3)

This is actually the case under some regularity assumptions on the optimal controls,
as stated in the following proposition.

Proposition 4.8. The two following assertions hold:

1. Assume that aopt

k ∈

L1(µ) for k = n, ..., N

1. Then

−

(4.9)

sup
N
k

≤

n

≤

A

1

−

inf
∈A

M

2. Assume that the function aopt

k

A(Xk)

aopt
k (Xk)
|

−

E

|

(cid:2)

0.

−−−−−→M
+
∞
→

(cid:3)

is c-Lipschitz for k = n, ..., N

1. Then

−

sup
N
k

≤

n

≤

A

1

−

inf
∈A

M

E

|

(cid:2)

(4.10)

A(Xk)

< c

(cid:16)

−
γM
c

aopt
k (Xk)
|

(cid:3)
2d/(d+1)

−

(cid:17)

log

γM
c

(cid:16)

(cid:17)

+ γM K −
M

(d+3)/(2d)

.

Proof. The ﬁrst statement of Proposition 4.8 relies essentially on the universal ap-
proximation theorem, and the second assertion is stated and proved in [2]. For the
✷
sake of completeness, we provide more details in Appendix E.

Remark 4.9. The consistency of NNContPI is proved in Theorem 4.6, but no
results on its variance are provided. We refer e.g. to Section 3.2 in [3] to observe the
small variance of NNContPI for a linear quadratic problem in dimension up to 100.

The rest of this section is devoted to the proof of Theorem 4.6. Let us introduce
some useful notations. Denote by AX the set of Borelian functions from the state space
1, and given a feedback control (policy)

into the control space A. For n = 0, . . . , N

X
represented by a sequence (Ak)k=n,...,N
the
cost functional associated with the policy (Ak)k. Notice that with this notation, we
have ˆV M
. We deﬁne the estimation error at time n associated with the
NNContPI algorithm by

(Ak)N −1
1, with Ak in AX , we denote by J
k=n
n

n = J

k )N −1

(ˆaM
n

−

k=n

−

εesti
PI,n := sup
∈A

A

M

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1 h
X

EM

J

(cid:2)

−

i

1
M

(cid:12)
(cid:12)
(cid:12)

k )N −1

k=n+1

A,(ˆaM
n

(Xn)

,

(cid:3)(cid:12)
(cid:12)
(cid:12)

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

13

µ: It measures how well the chosen estimator (e.g. mean square estimate)
with Xn ∼
can approximate a certain quantity (e.g. the conditional expectation). Of course we
expect the latter to cancel when the size of the training set used to build the estimator
goes to inﬁnity. Actually, we have

Lemma 4.10. For n = 0, . . . , N

1, the following holds:

E[εesti

PI,n]

≤

√2 + 16

(N

n)

−

+

(cid:0)
16γM
√M (

[f ]L

(cid:0)

(cid:1)
1+ρM

1

−

+

−
g
k

k∞

(cid:1)

f
k∞
k
√M
n

−

ρN
M
1

−

1

−
ρM
(cid:0)

(4.11)

=

O

(cid:18)

ρN
M

−

1

n

−

n

γN
−
−
M
√M

1

ηN
M

−

2

n

−

This implies in particular that

1+ηM γM

N

1

n

−

−

1+ηM γM
(cid:1)

!

(cid:0)

,
(cid:19)

(cid:1)

as M

.

→ ∞

+

1+ηM γM

N

n

−

−

(cid:0)

(cid:1)

1ρN

n
M [g]L

−

)

(4.12)

εesti
PI,n =

P
O

ρN
M

−

n

−

1

γN
M

−

n

−

1

ηN
M

−

2

n

−

log(M )

r

M !

,

as M

,
→ ∞

where we recall that ρM = E

(cid:20)

sup
m

≤

C(εm)
(cid:21)

M

is deﬁned in (HF).

1

≤

Proof. The relation (4.11) states that the estimation error cancels when M

with a rate of convergence of order

ρN −n−1
M

γN −n−1
M
√M

ηN −n−2
M

O

→ ∞
. The proof is in the

spirit of the one that can be found in chapter 9 of [10]. It relies on a technique of
symmetrization by a ghost sample, and a wise introduction of additional randomness
by random signs. The details are postponed to Section C in the Appendix. The proof
✷
of (4.12) follows from (4.11) by a direct application of the Markov inequality.

(cid:16)

(cid:17)

Let us also deﬁne the approximation error at time n associated with the NNCon-

tPI algorithm by

(4.13)

εapprox
PI,n

:= inf
A
∈A

M

EM

J

k )N −1

k=n+1

A,(ˆaM
n

h

EM

J

k )N −1

k=n+1

A,(ˆaM
n

(Xn)
i

−

A

inf
AX
∈

h

,

(Xn)
i

where we recall that EM denotes the expectation conditioned by the training set used
to compute the estimates (ˆaM
εapprox
PI,n measures how well the regression function can be approximated by means of
neural networks functions in
AM (notice that the class of neural networks is not dense
in the set AX of all Borelian functions).
Lemma 4.11. For n = 0, . . . , N

k=n+1 and the one of Xn ∼

1, the following holds as M

k )N

µ.

−

1

,
→ ∞

−

(4.14)

E[εapprox

PI,n ] =

O

(cid:18)

ρN
M

−

1

n

−

n

γN
−
−
M
√M

1

ηN
M

−

2

n

−

This implies in particular

+ sup
N
k

n

≤

≤

E

inf
∈AM

A

1

−

(cid:2)

A(Xk)
|

−

aopt
k (Xk)

.

|
(cid:3)

(cid:19)

εapprox
PI,n =

P
O

ρN
M

−

n

−

1

γN
M

−

n

−

1

ηN
M

−

2

n

−

log(M )
M

r

(4.15)

+ sup
N

n

k

≤

≤

inf
∈A

M

A

1

−

E

|

(cid:2)

A(Xk)

aopt
k (Xk)
|

−

.

(cid:19)

(cid:3)

 
 
 
14

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

Proof. See Appendix D for the proof of (4.14). (4.15) is a direct application of the
✷
Markov inequality.

Proof of Theorem 4.6.
Step 1. Let us denote by ˆJ
:= 1
,
M
the empirical cost function, from time n to N , associated with the sequence of controls
i
k )N
(A, (ˆaM
is deﬁned in (4.6).
We then have

k=n+1) and the training set, where we recall that ˆY (m),A

n , A(X (m)
n )

+ ˆY (m),A
n+1

A,(ˆaM
n,M

X (m)

k )N −1

M
m=1

k=n+1

n+1

P

f

h

(cid:0)

(cid:1)

−

1

EM

ˆV M
n (Xn)

= EM

J

k )N −1

k=n

(ˆaM
n

k )N −1

k=n

(ˆaM
ˆJ
n,M

−

k )N −1

k=n

+ ˆJ

(ˆaM
n,M

(Xn)
i

(4.16)

by deﬁnition of ˆV M

k )N −1

k=n+1

A,(ˆaM
ˆJ
n,M

(4.17)

Recalling that ˆaM

(cid:2)

(cid:3)

h
PI,n + ˆJ
εesti

(ˆaM
n,M

k )N −1

k=n

,

≤
n and εesti
PI,n. Moreover, for any A
k )N −1
A,(ˆaM
n,M

A,(ˆaM
n

k )N −1

EM

k=n+1

k=n+1

J

= ˆJ

−
A,(ˆaM
n

J

h
k )N −1
k=n+1

PI,n + EM
εesti

≤

h
A,(ˆaM
ˆJ
n = argmin
n,M
A

M

∈A

k )N −1

k=n+1

.

(Xn)
i

l.h.s. of (4.17) ﬁrst, and in the r.h.s. second, we then get

∈ AM ,
(Xn)
i

+ EM

k )N −1

k=n+1

A,(ˆaM
n

J

h

(Xn)
i

, and taking the inﬁmum over

AM in the

k )N −1

k=n

(ˆaM
ˆJ
n,M

≤

εesti
PI,n + inf
∈A

A

M

EM

J

k )N −1

k=n+1

A,(ˆaM
n

.

(Xn)
i

Plugging this last inequality into (4.16) yields the following estimate

(4.18)

EM

ˆV M
n (Xn)

inf A

M

∈A

−

EM

J

k )N −1

k=n+1

A,(ˆaM
n

(Xn)
i

2εesti

PI,n.

≤

(cid:2)

(cid:3)

Step 2. By deﬁnition (4.13) of the approximation error, using the law of iterated
conditional expectations for Jn, and the dynamic programming principle for Vn with
the optimal control aopt
n
k )N −1

at time n, we have

EM

A,(ˆaM
n

J

k=n+1

(Xn)

EM [Vn(Xn)]

h

h

inf
∈A

A

M

(cid:2)

−

= εapprox

EM

(cid:3)
PI,n + inf
AX
∈

A
f (Xn, aopt
h

−
PI,n + EM Eaopt
εapprox

EM

n

≤

f (Xn, A(Xn)) + EA
n

J

k )N −1

k=n+1

(ˆaM
n+1

n

(cid:8)

n (Xn)) + Eaopt
(ˆaM
n+1

k )N −1

k=n+1

J

n

n

h

(cid:2)
Vn+1(Xn+1)

(cid:2)
(Xn+1)

(cid:3)i
Vn+1(Xn+1)
i

,

−

(Xn+1)

(cid:3)(cid:9)

where EA
n [.] stands for the expectation conditioned by Xn at time n and the training
set, when strategy A is followed at time n. Under the bounded density assumption
in (Hd), we then get

EM

J

k )N −1

k=n+1

A,(ˆaM
n

(Xn)

EM [Vn(Xn)]

inf
∈A

M

A

(4.19)

(cid:2)
εapprox
PI,n +

εapprox
PI,n +

≤

≤

r

r

k

k

k∞

k∞

Z
(cid:2)
EM

−
(ˆaM
n+1

(cid:3)
J

k )N −1

k=n+1

(x′)

−

Vn+1(x′)

µ(dx′)

ˆV M
n+1(Xn+1)
h

(cid:3)
, with Xn+1 ∼
Vn+1(Xn+1)
i

−

µ.

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

15

Step 3. From (4.18) and (4.19), we have

EM

ˆV M
n (Xn)

Vn(Xn)

−
A,(ˆaM
n

J

EM

= EM

ˆV M
n (Xn)

−

A

inf
∈A

M

k )N −1
(cid:3)
k=n+1

(cid:2)
(Xn)

(cid:3)
EM [Vn(Xn)]

k )N −1

k=n+1

A,(ˆaM
n

EM

J

(cid:2)

(Xn)

(cid:3)i

(cid:2)
+ inf
A
∈A
2εesti

M

≤

(4.20)

(cid:2)
PI,n + εapprox
PI,n +

−
(cid:3)
ˆV M
n+1(Xn+1)

EM

r

k

k∞

h

−

Vn+1(Xn+1)
i

.

By induction, this implies

EM

ˆV M
n (Xn)

Vn(Xn)

−

≤

(cid:2)

(cid:3)

N

1

−

2εesti

PI,k + εapprox

PI,k

Xk=n

(cid:0)

.

(cid:1)

Use the estimations (4.12) for εesti
in Lemma
4.11, and observe that ˆVn(Xn)
Vn(Xn) holds a.s., to complete the proof of (4.8).
Finally, the proof of (4.7) is obtained by taking expectation in (4.20), and using
✷
estimations (4.11) and (4.14).

PI,n in Lemma 4.10, and (4.15) for εapprox

PI,n

≥

4.2. Hybrid-Now algorithm. In this paragraph, we analyze the convergence
of the hybrid-now algorithm. We shall consider the following set of neural networks
for the value function approximation:

γ
K :=

η
V

x

n

∈ X 7→

θ = (ai, bi, ci)i,

K

Φ(x; θ) =

ciσ(ai.x + bi) + c0,

i=1
X
aik ≤

k

η, bi ∈

R,

K

i=0
X

ci| ≤

|

γ

.

o

Let ηM , KM and γM be integers such that:

(4.21)

ηM , γM , KM −−−−→M

→∞ ∞

s.t.

γ4
M KM log(M )
M

,

M ρ2
γ4

M η2
M log(M )
M

0,

−−−−→M
→∞

with ρM deﬁned in (HF).

In what follows we denote by

estimated value functions at time n = 0, . . . , N
γM and KM that satisfy (4.21). We also consider the class
estimated feedback optimal control at time n = 0, . . . , N
4.1, with the same parameters ηM , γM and KM .

γM
KM the space of neural networks for the
1, parametrized by the values ηM ,
AM of neural networks for
1, as described in Section
−

−

VM := ηM
V

Recall that the approximation of the value function and optimal policy in the

hybrid-now algorithm is computed in backward induction as follows:

•

•

Initialize ˆV M

N = g

1, . . . , 0, generate a training sample X (m)

For n = N
training distribution µ, and a training sample for the exogenous noise ε(m)
1, . . . , M .

n , m = 1, . . . , M from the
n+1, m =

−

16

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

(i) compute the approximated policy at time n

ˆaM
n ∈

argmin
A

M

∈A

1
M

M

f (X (m)

n , A(X (m)

n )) + ˆV M

n+1(X (m),A
n+1 )

where X (m),A

n+1 = F (X (m)

n ), ε(m)
n+1)

m=1
X

(cid:2)
n , A(X (m)

P A(X (m)

n )(X (m)

n , dx′).

∼

(cid:3)

(ii) compute the untruncated estimation of the value function at time n

˜V M
n ∈

argmin
Φ

M

∈V

1
M

M

f (X (m)

n , ˆaM

n (X (m)

n )) + ˆV M

n+1(X (m),ˆaM

n+1

n

m=1 h
X

)

−

2

Φ(X (m)
n )
i

and set the truncated estimated value function at time n

(4.22)

ˆV M
n = max

min

˜V M
n ,

(cid:16)

(cid:0)

Vnk∞

k

,

Vnk∞

−k

(cid:1)

.

(cid:17)

Remark 4.12. Notice that we have truncated the estimated value function in
(4.22) by an a priori bound on the true value function. This truncation step is natural
from a practical implementation point of view, and is also used for simplifying the
✷
proof of the convergence of the algorithm.

We now state our main result about the convergence of the Hybrid-Now algorithm.
Theorem 4.13. Assume that there exists an optimal feedback control (aopt

for the control problem with value function Vn, n = 0, . . . , N , and let Xn ∼
as M

+

1
k )N
−
k=n
µ. Then,

→
EM

∞
ˆV M
n (Xn)

|
h

Vn(Xn)
|

−

=

P
O

(4.23)

M η2
ρ2

M log(M )

1
4(N −n)

i
γ4
M

KM log(M )
M

  (cid:18)

1
2(N −n)

+

γ4
M

(cid:18)

(cid:19)
EM

(cid:16)

E

+ sup
n

k

≤

≤

N

inf
∈V

Φ

M

+ sup
n

k

A

N

inf
∈A

M

Φ(Xk)

Vk(Xk)
|

−

2

|
h

A(Xk)

|

aopt
k (Xk)
|

−

M

(cid:19)

1
2(N −n)

i(cid:17)

1
2(N −n)

,

!

h
where EM stands for the expectation conditioned by the training set used to estimate
the optimal policies (ˆaM

i(cid:17)

(cid:16)

≤

≤

k )n

k

N

≤

≤

−

1.

Remark 4.14. The consistency of Hybrid-Now is proved in Theorem 4.13, but no
results on its variance are provided. The stability of Hybrid-Now has nevertheless
been observed numerically in [3], and we refer e.g. to its Section 3.2 to observe the
small variance of Hybrid-Now in a linear-quadratic problem in dimension up to 100.

Comment: Theorem 4.13 states that the estimator for the value function provided by
the hybrid-now algorithm converges in L1(µ) when the size of the training set goes to

stand for the
inﬁnity. Note that
estimation error made by estimating empirically respectively the value functions and

and

M

γ4
M

KM log(M)
M

γ4
M

M η2
ρ2

M log(M)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

1
4(N −n)

1
2(N −n)

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

17

the optimal control by neural networks; also:

and

sup
k

≤

≤

n

inf
∈A

M

A

N

E

|

A(Xk)

aopt
k (Xk)
|

−

i
mating respectively the value function and the optimal control by neural networks.

h

sup
k

n

Φ

N

inf
∈V

M r

|
h

E

Φ(Xk)

Vk(Xk)
|

2

−

≤

i
are the approximation error made by esti-

≤

In order to prove Theorem 4.13, let us ﬁrst introduce the estimation error at time

n associated with the Hybrid-Now algorithm by

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

1
M

εesti
HN,n := sup
∈A

A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m=1 h
X
EA

−

M,n,Xn

i

Xn+1

f (Xn, A(Xn)) + ˆV M
n+1
h

(cid:0)
n , A(X (m)
n ), εm

X (m)

(cid:1)i

n+1

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

. We have

n+1 = ˆV M

, and X (m),A
where ˆY (m),A
the following bound on this estimation error:
(cid:0)

X (m),A
n+1

n+1

(cid:1)

n+1 = F

(cid:16)

Lemma 4.15. For n = 0, ..., N

1, the following holds:

−

√2 + 16

(N

−

f
n)
k
k∞
√M

+

g

k

k∞

E

εesti
HN,n

(cid:3)

(cid:2)

(4.24)

≤ (cid:0)
=
→∞ O
Proof. See Appendix F.

M

(cid:1)(cid:0)

ρM ηM γ2
M

.
√M (cid:19)

(cid:18)

+ 16[f ]L

+ 16

ρM ηM γ2
M
√M

(cid:1)

✷

Remark 4.16. The result stated by lemma 4.15 is sharper than the one stated in
Lemma 4.10 for the performance iteration procedure. The main reason is that we can
make use of the γM ηM -Lipschitz-continuity of the estimate of the value function at
✷
time n + 1.

We next introduce the approximation error at time n associated with the Hybrid

Now algorithm by

εapprox
HN,n := inf
∈A

A

M

EM

where ˆY A
n+1 := ˆV M
approximation error:

+ ˆY A

f

Xn, A(Xn)

inf
AX
∈
n+1 (F (Xn, A(Xn), εn+1)). We have the following bound on this

Xn, A(Xn)

n+1

n+1

−

f

i

h

i

h

(cid:1)

(cid:0)

(cid:1)

(cid:0)

A

,

+ ˆY A

EM

Lemma 4.17. For n = 0, ..., N

1, the following holds:

εapprox
HN,n ≤

([f ]L +

k

(4.25)

+ 2

r

k

Proof. See Appendix G.

k∞

−
Vn+1k∞
EM

[r]L) inf
AX
∈

A

Vn+1(Xn+1)

h(cid:12)
(cid:12)
(cid:12)

EM

A(Xn)

aopt
n (Xn)

−

−

.

(cid:2)(cid:12)
ˆV M
n+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i

(cid:3)

(cid:12)
(cid:12)

✷

Proof of Theorem 4.13
Observe that not only the optimal strategy but also the value function is estimated
1 using neural networks in the hybrid algorithm. It
at each time step n = 0, ..., N

−

18

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

spurs us to introduce the following auxiliary process ( ¯V M
induction as:

n )N

n=0 deﬁned by backward

¯V M
N (x) = g(x),
¯V M
n (x) = f (x, ˆaM






,
for x
∈ X
ˆV M
n (x)) + E
n+1(F (x, ˆaM
h
1, ¯V M

n (x), εn+1))
i

,

for x

,

∈ X

−

n is the quantity estimated by ˆV M
n .
1,

Step 1. We state the following estimates: for n = 0, ..., N
(4.26)

−

and we notice that for n = 0, ..., N

EM

¯V M
n (Xn)

(cid:20)

inf
A
a
∈

−

n

f (Xn, a) + Ea

M,n,Xn

ˆV M
n+1(Xn+1)
h

io(cid:21)

≤

2εesti

HN,n + εapprox
HN,n ,

0

≤

and,

2

(4.27)

EM

¯V M
n (Xn)

"(cid:12)
(cid:12)
(cid:12)
(cid:12)

inf
A
a
∈

−

f (Xn, a) + Ea

M,n,Xn

n

2 ((N

f

n)
k

−

k∞

+

g

k

k∞

)

≤

ˆV M
n+1(Xn+1)
h
2εesti

io(cid:12)
(cid:12)
(cid:12)
HN,n + εapprox
(cid:12)
(cid:17)

HN,n

#

,

(cid:16)

where EM,n,Xn stands for the expectation conditioned by the training set and Xn.

Let us ﬁrst show the estimate (4.26). Note that inequality

−

¯V M
n (Xn)

inf
A
a
∈

f (Xn, a) + Ea

ˆV M
n+1(Xn+1)
h
n cannot do better than the optimal strategy. Take its expectation

M,n,Xn

io

≥

n

0

holds because ˆaM
to get the ﬁrst inequality in (4.26). Moreover, we write

EM

¯V M
n (Xn)

≤

(cid:2)

(cid:3)

≤

A

EM

f

Xn, ˆaM

n (Xn)

+ ˆV M
n+1

X ˆaM

n
n+1

h
inf
∈A

M

(cid:0)
EM

(cid:16)
(cid:1)
f (Xn, A(Xn)) + ˆV M
n+1

(cid:17)i
X A
n+1

+ 2εesti

HN,n,

h

(cid:0)

(cid:1)i

which holds by the same arguments as those used to prove (4.18). We deduce that

≤

A

inf
AX
∈
EM

(cid:2)

(cid:3)

≤

EM

¯V M
n (Xn)

EM

f (Xn, A(Xn)) + ˆV M
n+1

X A

n+1

+ εapprox

HN,n + 2εesti
HN,n

h
f (Xn, a) + Ea
M

n

(cid:0)
ˆV M
n+1 (Xn+1)
h

inf
A
a
∈

(cid:20)

(cid:1)i
Xn

+ εapprox

HN,n + 2εesti

HN,n.

io(cid:21)

(cid:12)
(cid:12)

This completes the proof of the second inequality stated in (4.26).
On the other hand, noting:

¯V M
n (Xn)

inf
A
a
∈

−

f (Xn, a) + Ea
M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
and using (4.26), we obtain the inequality (4.27).

n

(cid:12)
(cid:12)

ˆV M
n+1(Xn+1)
h

Xn

2 ((N

f

n)
k

−

k∞

+

g

k

k∞

) ,

≤

io(cid:12)
(cid:12)
(cid:12)
(cid:12)

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

19

Step 2. We state the following estimation: for all n

0, ..., N

}

∈ {

(4.28)

ˆV M
n (Xn)
(cid:13)
(cid:13)
(cid:13)
=

γ2
M

P
O

M,1

¯V M
n (Xn)
(cid:13)
(cid:13)
(cid:13)

KM

log(M )
M

−

r

+ inf
Φ
∈V

M

Φ(Xn)
k
q

−

V M
n (Xn)

kM,1

+ inf
AX
A

A(Xn)

aopt
n (Xn)

M,1 +

−

Vn+1(Xn+1)

(cid:13)
(cid:13)

1
p

r(cid:13)
(cid:13)
(cid:13)

−

ˆV M
n+1(Xn+1)
(cid:13)
(cid:13)
(cid:13)

,

M,1!

EM [

∈ {

p]
.
|
|
1, 2

.

, i.e.

kM,p stands for the Lp norm conditioned by
. The proof relies on Lemma H.1 and Lemma H.2 (see
(cid:17)
}

k

∈

q(cid:13)
(cid:13)
kM,p =

.

where

k
the training set, for p
(cid:16)
Appendix H).

Let us ﬁrst show the following relation:

(4.29)
EM

ˆV M
n (Xn)

h(cid:12)
(cid:12)

For this, take δM = γ4

¯V M
n (Xn)

2

−

i
(cid:12)
log(M)
(cid:12)
M KM

=

(cid:18)

log(M )
M

γ4
M KM

P
O

+ inf
Φ
∈V
M , let δ > δM , and denote
M

M

Ωg :=

f

(cid:26)

g : f

∈ VM ,

−

Apply Lemma H.2 to obtain:

f (xm)

g(xm)

−

(cid:12)
(cid:12)

m=1
X

(cid:12)
(cid:12)

1
M

√δ

c2δ/γ2
M

Z

log

N2

(cid:18)

u
4γM

(cid:18)
√δ

, Ωg, xM
1

1/2

du

(cid:19)(cid:19)
u
4γM

(cid:18)

,

VM , xM

1

1/2

du

(cid:19)(cid:19)

log

N2

(cid:18)

Φ(Xn)

|

¯V M
n (Xn)
|

2

−

E

h

.
i(cid:19)

2

≤

δ
.
γ2
M (cid:27)

≤

≤

≤

≤

c2δ/γ2
M

Z

√δ

c2δ/γ2
M

Z

√δ

(cid:0)

c2δ/γ2
M

Z
√δ
c5√δ
(cid:0)

(4d + 9)KM + 1

1/2

(cid:0)
(4d + 9)KM + 1

(cid:1)
1/2

(4d + 9)KM + 1

1/2

(cid:1)
log

log

"

48eγ2
M

KM + 1
u

(cid:0)

1/2

du

!#

(cid:1)

1/2

γ4
M
δ

log

48e

(cid:18)
(cid:20)
48eγ4
M M

(cid:0)
KM + 1

KM + 1

du

(cid:19)(cid:21)
(cid:1)
1/2

(4.30)

KM

log(M ),
(cid:1)

(cid:2)

(cid:0)

(cid:0)

(cid:1)(cid:1)(cid:3)

≤
, xM
1 ) stands for the ε-covering number of

where
in section H, and where the last line holds since we assumed MδM

N2(ε,

on xM

p

p

V

V

1 , which is introduced
0. Since δ

γ2
M −−−−→M
0
√Mδ
> δM := γ4
, which implies that
γ2
M
(H.1) holds by (4.30). Therefore, by application of Lemma H.1, the following holds:

M , we then have √δ√KM

log(M )

M KM

log(M)

≤

→

EM

˜V M
n (Xn)

¯V M
n (Xn)

2

=

−

γ4
M KM

P
O

log(M )
M

h(cid:12)
(cid:12)

i
(cid:12)
(cid:12)
It remains to note that EM
always holds, and this completes the proof of (4.29).
i

¯V M
n (Xn)

(cid:18)
ˆV M
n (Xn)

−

2

+ inf
Φ
∈V

M

E

Φ(Xn)

|

¯V M
n (Xn)
|

2

−

EM

≤

˜V M
n (Xn)

−

¯V M
n (Xn)

h

h(cid:12)
(cid:12)

h(cid:12)
(cid:12)

.
i(cid:19)
2

i

(cid:12)
(cid:12)

p

(cid:12)
(cid:12)

 
 
20

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

Next, let us show

inf
∈V

Φ

M

Φ(Xn)

−

¯Vn(Xn)

M,2

=

(cid:13)
(cid:13)
O 

γ2
M

r

(cid:13)
(cid:13)

KM log(M )
M

+ sup
n

k

≤

≤

N

inf
∈V

Φ

M k

Φ(Xn)

Vn(Xn)

kM,2

−

(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:3)
(cid:12)
∈ VM and split
¯V M
n (Xn)

M,2 ≤

(cid:13)
(cid:13)

(4.31)

+ inf
A
∈A

M

EM

A(Xn)

aopt
n (Xn)

−

+

Vn+1(Xn+1)

For this, take some arbitrary Φ

(cid:2)(cid:12)
(cid:12)

−

ˆV M
n+1(Xn+1)
(cid:13)
(cid:13)
(cid:13)

M,2 !

.

(4.32)

(4.33)

inf
∈V

Φ

M

Φ(Xn)

−

(cid:13)
(cid:13)

To bound the last term in the r.h.s. of (4.32), we write

(cid:13)
(cid:13)

Vn(Xn)

−

¯V M
n (Xn)

M,2

Φ(Xn)

inf
∈V

Φ

+

M k
Vn(Xn)

−

Vn(Xn)

−
¯V M
n (Xn)

kM,2
M,2 .
(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

(cid:13)
Vn(Xn)
(cid:13)

f (Xn, a) + Ea
M

inf
A
a
∈

−

n
f (Xn, a) + Ea
M

ˆV M
n+1 (Xn+1)
h
ˆV M
n+1 (Xn+1)
h

Xn

io

(cid:12)
(cid:12)

Xn

M,2

io(cid:13)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
¯V M
n (Xn)
(cid:13)
−
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
+
(cid:13)

inf
A
a
(cid:13)
∈
(cid:13)
(cid:13)
(cid:13)

n

.

M,2

Use the dynamic programming principle, assumption (Hd) and (4.27) to get:

Vn(Xn)

−

¯V M
n (Xn)

M,2 ≤ k

r

k∞

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
2 ((N

Vn+1(Xn+1)

−

ˆV M
n+1 (Xn+1)
(cid:13)
(cid:13)
HN,n + εapprox
2εesti
(cid:13)

HN,n

M,2

+

g

)

f

n)
k

−

k∞

k

k∞

(cid:16)

.

(cid:17)

We then notice that

r

Vn+1(Xn+1)

(cid:12)
(cid:12)
(4.34)
(cid:12)

−

holds a.s., so that

2

ˆV M
n+1(Xn+1)
(cid:12)
(cid:12)
((N
(cid:12)

2

r

k∞

≤

k

f

n)
k

−

k∞

+

g

k

k∞

)

Vn+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)

−

ˆV M
n+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)

Vn(Xn)

−

¯V M
n (Xn)

M,2

(cid:13)
(cid:13)

(cid:13)
r
(cid:13)
k∞

k

≤

2
r

((N

f

n)
k

−

k∞

+

g

k

k∞

+

2 ((N

r

f

n)
k

−

k∞

+

g

k

k∞

)

)

Vn+1(Xn+1)

(cid:13)
(cid:13)
HN,n + εapprox
(cid:13)
2εesti

HN,n

−

ˆV M
n+1 (Xn+1)
(cid:13)
(cid:13)
(cid:13)

,

M,1

(cid:16)

(cid:17)

and use Lemma 4.17 to bound εapprox
HN,n . By plugging into (4.32), and using the estima-
tions in Lemmas 4.15 and 4.17, we obtain the estimate (4.31). Together with (4.29),

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

21

this proves the required estimate(4.28). By induction, we get as M

ˆV M
n (Xn)

Vn(Xn)
|

−

EM

|
h

=

P
O

γ4
M

  (cid:18)

i

KM log(M )
M

1
2(N −n)

+

γ4
M

(cid:18)

,
→ ∞

ρ2
M η2

M log(M )

1
4(N −n)

+ sup
n

k

≤

≤

inf
∈V

M

Φ

N

Φ(Xk)

|

Vk(Xk)
|

−

2

(cid:19)
EM
(cid:16)

h

M

(cid:19)

1
2(N −n)

i(cid:17)

1
2(N −n)

+ sup
n

k

≤

≤

N

inf
∈A

M

A

E

(cid:16)

h

A(Xk)

|

aopt
k (Xk)
|

−

which completes the proof of Theorem 4.13.

i(cid:17)

,
!

✷

5. Conclusion. This paper develops new machine learning algorithms for high-
dimensional Markov decision processes. We propose and compare two algorithms
based on dynamic programming, performance/hybrid iteration and neural networks
for approximating the control and value function. The main theoretical contribution is
to provide a detailed convergence analysis for each of these algorithms: by using least
squares neural network regression, we state error estimates in terms of the universal
approximation error of neural networks, and of the statistical risk when estimating
the network function. Numerical tests on various applications are presented in a
companion paper [3].

Appendix A. Localization.
In this section, we show how to relax the boundedness condition on the state

space by a localization argument.

Let R > 0. Consider the localized state space ¯BXd (0, R) :=

where
by its transition probabilities as

kd is the Euclidean norm of Rd. Let

k

.

¯Xn

(cid:0)

(cid:1)

0

≤

n

≤

,
}
X ∩ {k
N be the Markov chain deﬁned

x
kd ≤

R

O

¯Xn = x, a

=

P

¯Xn+1 ∈
(cid:16)

r(x, a; y)dπR ◦

ZO

µ(y),

for n = 0, . . . , N

1,

−

(cid:17)

(cid:12)
(cid:12)
(cid:12)
for all Borelian O in ¯BXd (0, R), where πR is the Euclidean projection of Rd on
¯BXd (0, R), and πR ◦
µ is the pushforward measure of µ. Notice that the transition
probability of ¯X admits the same density r, for which (Hd) holds, w.r.t. πR ◦
Deﬁne ( ¯V R
problem for

n )n as the value function associated with the following stochastic control

µ.

¯Xn

N
n=0:
(cid:1)

(cid:0)
¯V R
N (x) = g(x),

(A.1) 


¯V R
n (x) = inf
∈C

α

E

"

N

1

−

f

¯Xk, αk

+ g

¯XN

Xk=n

(cid:0)

(cid:1)

(cid:0)

#
(cid:1)

, for n = 0, . . . , N

1,

−


¯BXd (0, R). By the dynamic programming principle, ( ¯V R

for x
following Bellman backward equation:

∈

n )n is solution of the

¯V R
N (x) = g(x)
¯V R
n (x) = inf
A
∈

a






f (x, a) + Ea
n

¯V R
n+1

πR

F (x, a, εn+1)

(cid:26)

h

(cid:0)

(cid:0)

,
(cid:1)(cid:1)i(cid:27)

x

∀

∈

BXd (0, R).

22

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

We shall assume two conditions on the measure µ.
(Hloc) µ is such that:

E

|

πR(X)

X

|

−

0

and

−−−−→R
→∞

P (
|

X

|

> R)

−−−−→R
→∞

0,

where X

µ.

∼

(cid:2)

(cid:3)

Using the dominated convergence theorem, it is straightforward to see that (Hloc)
holds if µ admits a moment of order 1.
Proposition A.1. Let Xn ∼
E

µ. The following holds:

πR(Xn)

Vn

Xn

¯V R
n

h(cid:12)
(cid:12)
(cid:12)

(cid:0)

(cid:1)

−

≤ k

V

(cid:1)(cid:12)
i
(cid:0)
(cid:12)
[r]LE [
(cid:12)
k∞
(cid:16)
+ [g]Lk
k
= sup
0

r

N

−

|
n

Xn|
, and use the convention 1
−
1
N k
−
and p > 1. Consequently, for all n = 0, ..., N , we get under (Hloc):

|
Vkk∞

where we denote

πR(Xn)

k∞

−

] ,

V

∞

k

≤

≤

k

E [

πR(Xn)

Xn|

] + 2P (
|

Xn|

−

> R)

n

1

r

− k
1

k
r
− k

N

−

∞
k∞

(cid:17)

xp
x = p for x = 0

E

¯V R
n

πR(Xn)

Vn

Xn

−

0,

where Xn ∼

µ.

−−−−→R
→∞

(cid:1)

(cid:0)

i

h(cid:12)
(cid:12)
(cid:12)

(cid:1)(cid:12)
(cid:0)
(cid:12)
Comment: Proposition A.1 states that if
is not bounded, the control problem
(cid:12)
(A.1) associated with a bounded controlled process ¯X can be as close as desired, in
L1(µ) sense, to the original control problem by taking R large enough. Moreover, as
stated before, the transition probability of ¯X admits the same density r as X w.r.t.
the pushforward measure πR ◦
Proof of Proposition A.1. Take Xn ∼

µ and write:

µ.

X

1
(cid:3)

Xn

|

|≥

R

.

(cid:3)

(cid:12)
(cid:12)

E

¯V R
n (πR(Xn))

(A.2)

h(cid:12)
(cid:12)

−

Vn(Xn)

i

(cid:12)
(cid:12)

E

≤

Vn(Xn)

¯V R
n (Xn)
−
¯V R
n (πR(Xn))

1

Xn

R

|

|≤
(cid:12)
Vn(Xn)
(cid:12)

+ E
(cid:2)(cid:12)
(cid:12)

−

Let us ﬁrst bound the ﬁrst term in the r.h.s. of (A.2), by showing that, for n =
0, . . . , N

1:

(cid:2)(cid:12)
(cid:12)

−

E

¯V R
n (Xn)

Vn(Xn)

1

−

Xn

|

|≤

R

(cid:2)(cid:12)
(A.3)
(cid:12)

Take x

∈

(cid:3)
+ [r]Lk
¯Bd(0, R) and notice that

(cid:12)
(cid:12)

r

E

¯V R
n+1(πR(Xn+1))

≤ k
k∞
Vn+1k∞

h(cid:12)
E [
(cid:12)
|

πR(Xn+1)

−

−
Xn+1|

Vn+1(Xn+1)

i
] , with Xn+1 ∼

(cid:12)
(cid:12)

µ.

¯V R
n (x)

Vn(x)

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

inf
A
a
∈

≤

(cid:26) ZA
+

(cid:12)
(cid:12)

¯V R
n+1 (πR(y))

−

Vn+1(y)

r (x, a; πR(y)) dµ(y)

r(x, a; πR(y))

(cid:12)
(cid:12)

Vn+1(y)

|

| |

r(x, a; y)
|

−

dµ(y)

(cid:27)

Z
¯V R
r
n+1 (π(Xn+1))
k∞
E [
Vn+1k∞
+ [r]Lk
(cid:2)(cid:12)
(cid:12)
It remains to inject this bound in the expectation to obtain (A.3).

Vn+1 (Xn+1)
Xn+1|

πR(Xn+1)

≤ k

−

−

] ,

E

(cid:12)
(cid:12)

(cid:3)

|

where Xn+1 ∼

µ.

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

23

To bound the second term in the r.h.s. of (A.2), notice that

¯V R
n (πR(Xn))

Vn(Xn)

−

2

Vnk∞

k

≤

(cid:12)
(cid:12)
holds a.s., which implies:

(cid:12)
(cid:12)

(A.4)

E

¯V R
n (πR(Xn))

Vn(Xn)

1

Xn

|

|≥

R

2

Vnk∞

k

P (
|

Xn|

≤

> R) .

−

Plugging (A.3) and (A.4) into (A.2) yields:

(cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:3)

E

¯V R
n (πR(Xn))

h(cid:12)
(cid:12)

−

Vn(Xn)

+ [r]Lk

r

≤ k
i
(cid:12)
Vn+1k∞
(cid:12)

k∞
E [

E

¯V R
n+1(πR(Xn+1))
Xn+1|

h(cid:12)
πR(Xn+1)
(cid:12)

−

|

−
] + 2

Vn+1(Xn+1)

Vnk∞

k

P (
|

i
(cid:12)
Xn|
(cid:12)

> R) ,

with Xn and Xn+1 i.i.d.
then follows by induction.

following the law µ. The result stated in proposition A.1
✷

Appendix B. Forward evaluation of the optimal controls in
We evaluate in this section the real performance of the best controls in
M

n=0 be the sequence of optimal controls in the class of neural networks

AM .

n )N

1

(aA
−
and denote by (J A
n
and characterized as solution of the Bellman equation:

)0

≤

≤

n

M

N the cost functional sequence associated with (aA

AM . Let
AM ,
n )N

1
−
n=0

M

M

J A
N (x) = g(x)
J A
(x) = inf
n
∈A

A

M

M




n

f (x, A(x)) + EA

n,Xn [J A

n+1(Xn+1)]

M

,

o

where EA
n,Xn [
is applied at time n.


] stands for the expectation conditioned by Xn and when the control A
·

In this section, we are interested in comparing J A
n
(x) holds for all x
, since
. We can actually show the following:

≤
AM is included in the set of the Borelian functions

to Vn. Note that Vn(x)

∈ X

M

M

J A
n
of

X

Proposition B.1. Assume that there exists a sequence of optimal feedback con-
1 for the control problem with value function Vn, n = 0, . . . , N .

n

trols (aopt
N
≤
Then the following holds, as M

n )0

≤

−

:
→ ∞

(B.1)

E

M

J A
n

(Xn)

(cid:2)

Vn(Xn)

=

−

(cid:3)

O

n
(cid:18)

≤

sup
N
k

≤

A

1

−

inf
∈A

M

E

|

(cid:2)

A(Xk)

aopt
k (Xk)
|

−

.

(cid:19)

(cid:3)

Remark B.2. Notice that there is no estimation error term in (B.1), since the
AM are deﬁned as those minimizing the real cost functionals in
✷

optimal strategies in
AM , and not the empirical ones.
Proof of Proposition B.1. Let n
and denote J A

∈ {
n (Xn) = f (x, A(x)) + EA

0, ..., N
M
n,Xn [J A

1

, and Xn ∼

}

−

µ. Take A

n+1(Xn+1)]. Clearly, we have J A

∈ AM ,
n =

M

i

i

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

J A
n . Moreover:

24

min
A
∈A

M

J A
n (Xn)

E

f (Xn, A(Xn))

Vn(Xn)
i

≤

|
J A
n+1(F (Xn, A(Xn), εn+1))

h

M

f (Xn, aopt

n (Xn))
|

−

Vn+1(F (Xn, aopt

≤

[f ]LE
h

aopt
n (Xn)

|

A(Xn)
|

Vn+1(F (Xn, A(Xn), εn+1))

i
Vn+1(F (Xn, aopt

E

h

(B.2)

−
+ E
h

+ E
h
+ E
h

|

|

|

−

−

−

−

i

n (Xn), εn+1))
|

i

n (Xn), εn+1))
|
.

i

M

J A
n+1(F (Xn, A(Xn), εn+1))

Vn+1(F (Xn, A(Xn), εn+1))
|

Applying assumption (Hd) to bound the last term in the r.h.s. of (B.2) yields

E

J A
n (Xn)

h

−

Vn(Xn)
i

≤

[f ]L +

(cid:0)
+

r

E

[r]L

Vn+1k∞
k
h
E
n+1(Xn+1)
J A
−

(cid:1)

M

|

k∞

|
h

aopt
n (Xn)

A(Xn)
|

−

Vn+1(Xn+1)
|

,

i

which holds for all A

k
∈ AM , so that:
[f ]L +

E

M

J A
n

(Xn)

h

−

Vn(Xn)
i

≤

[r]L

Vn+1k∞
k
E
J A
n+1(Xn+1)

(cid:1)

A

M

E

inf
∈A

M

h

(cid:0)
+

r

aopt
n (Xn)

|

A(Xn)
|

−

Vn+1(Xn+1)
|

−

.

i

i

✷

|
h
(B.1) then follows directly by induction.

k∞

k

Appendix C. Proof of Lemma 4.10.
The proof is divided into four steps.

Step 1: Symmetrization by a ghost sample. We take ε > 0 and show that for

2

∞+

k
ε2

∞

g

k

k

M

, the following holds:

(cid:1)

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1 h
X

E

−

i

A,(ˆaM
n

J

k )N −1

k=n+1

(Xn)

h

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

f (X

(m)
n , A(X
n

′

n+1 −

′

(m)

))

−

M > 2

(N

n)

f

k

−

(cid:0)

1
M

P

"

(C.1)

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2P

≤

"
A

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where:

′

M

1
M

m=1h
X

> ε

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
ˆY ′
n+1

(m),A

>

ε
,
2 #

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

•

•

k

≤

≤

≤

m

)1

M,n

(m)
k

N is a copy of (X (m)
(X
)1
k
≤
≤
(m)
pendent copy of the exogenous noises (ε′
k
k
≤
m=1, following the same control ˆaM
)M
of initial positions at time n, (X
k= n + 1, . . . , N

(m)
n
1, and control A at time n,

N generated from an inde-
N , and independent copy
k at time

m
≤
)1

≤
M,n

≤
m

M,n

≤

≤

≤

k

′

−

We recall that Y (m),A

n+1

has already been deﬁned in (4.6), and we similarly deﬁne

(m),A

Y ′

n+1

N

1

−

:=

Xk=n+1

f (X ′
k

(m),A

, ˆaM

k (X ′
k

(m),A

)) + g(X ′
N

(m),A

).

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

25

Let A∗

∈ AM be such that:

1
M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M

f (X (m)

n , A∗(X (m)

n )) + ˆY (m),A

n+1

∗

E

∗

A
n

J

,(ˆaM

k )N −1

k=n+1

−

m=1 h
X

i

h
if such a function exists, and an arbitrary function in
exist. Note that 1
M
is a r.v., which implies that A∗ also depends on ω
h
ability conditioned by the training set of exogenous noises (ε(m)
)1
initial positions (X (m)
conditioned by the latter. An application of Chebyshev’s inequality yields

AM if such a function does not
,(ˆaM
(Xn)
Ω. Denote by PM the prob-
i
N and
m
N , and recall that EM stands for the expectation

n )) + ˆY (m),A

n , A∗(X (m)

f (X (m)

M
m=1

M,n

M,n

n+1

P

A
n

)1

−

E

∈

J

m

i

h

≤

≤

≤

≤

≤

≤

≤

≤

k

k

k

k

∗

> ε

(Xn)
(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
k )N −1
k=n+1

∗

PM

" (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤

EM

∗

A
n

J

,(ˆaM

k )N −1

k=n+1

h
VarM

∗

A
n

J

,(ˆaM

k )N −1

k=n+1

h

M (ε/2)2

−

(X ′n)
i
(X ′n)
i

1
M

M

(m)
f (X ′
n

(m)
, A∗(X ′
n

m=1 h
X

∗

(m),A

)) + ˆY ′

n+1

(N

≤ (cid:0)

−

f
n)
k
k∞
M ε2

+

g

k

k∞

2

,

(cid:1)

>

ε
2 #

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

where we have used 0

≤

VarM

∗

A
n

J

,(ˆaM

k )N −1

k=n+1

(X ′n)

(cid:20)

E

≤

∗

,(ˆaM
k )

N −1
k=n+1

A
n

J
"(cid:18)

(N

f

n)
k

−

k∞

+

g

k

k∞

which implies

,(ˆaM

k )N −1

k=n+1

∗

A
n

J

(cid:12)
(cid:12)
(cid:12)
(cid:21)

= VarM

∗

A
n

J

(cid:20)

(X ′n)
≤
(cid:12)
(cid:12)
k )N −1
(cid:12)
k=n+1

,(ˆaM

(N

n)

f
k

−

k∞
2

+

g
k

k∞

(X ′n)

−

2

(X ′n)

−

(N

n)

f
k

−

+

g
k

k∞

k∞
2

(N

n)

f
k

−

k∞
4

# ≤ (cid:0)

(cid:19)

(cid:21)
g
k

k∞

+

2

.

(cid:1)

Thus, for M > 2

(N

n)

f

k

−

∞+

k
ε2

g

k

∞

k

(cid:0)
k )N −1
,(ˆaM

k=n+1

EM

∗

A
n

J

h

(Xn)
i

−

2

, we have

(cid:1)
1
M

M

(m)
f (X ′
n

(m)
, A∗(X ′
n

m=1 h
X

M

≥

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

(m)
f (X ′
n

, A(X ′

m
n ))

−

(m),A

ˆY ′
n+1

∗

(m),A

)) + ˆY ′

n+1

ε
2 #

≤

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
.
(cid:12)

1
2

>

ε
2 #

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(m),A
(cid:12)

∗

ˆY ′
n+1

−

>

ε
2 #

PM

" (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(C.2)

Hence:

1
M

P

"

A

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
M

P

≥

m=1 h
X
M

∗

−

f (X (m)

n , A∗(X (m)

n )) + ˆY (m),A

n+1

(m)
f (X ′
n

(m)
, A∗(X ′
n

))

∗

P

≥

1
M

f (X (m)

m=1 h
X
M

m=1 h
X
M

n , A∗(X (m)

n )) + ˆY (m),A

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Observe that 1
is
M
measurable w.r.t. the σ-algebra generated by the training set, so that conditioning

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
k )N −1
(cid:12)
k=n+1

n )) + ˆY (m),A

n , A∗(X (m)

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
(Xn)

f (X (m)
h

(Xn)
i

(m)
, A∗(X ′
n

(m)
f (X ′
n

ε
.
2 #

)) + ˆY ′

k )N −1

k )N −1

m=1 h
X

M
m=1

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

(Xn)

EM

1
M

> ε,

EM

EM

,(ˆaM

,(ˆaM

,(ˆaM

(m),A

k=n+1

k=n+1

n+1

A
n

A
n

A
n

n+1

n+1

h
∗

−

−

≤

−

J

h

i

J

J

i

i

h

∗

∗

∗

∗

P

26

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

by the training set and injecting (C.2) yields

2P

"

M

1
M

P

≥

m=1 h
X
M

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
M
"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n)
−

m=1 h
X
1
M

= P

(N

"

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

(m)
f (X ′
n

(m)
, A(X ′
n

))

−

(m),A

ˆY ′
n+1

>

f (X (m)

n , A∗(X (m)

n )) + ˆY (m),A

n+1

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1 h
X

f

∞+

g

∞

2

∗

−

i

EM

∗

A
n

J

,(ˆaM

k )N −1

k=n+1

(Xn)

h

E

−

i

A,(ˆaM
n

J

k )N −1

k=n+1

h

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(Xn)

#

> ε

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

ε
2 #

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
> ε

for M > 2
second-to-last to the last line. The proof of (C.1) is then completed.

, where we use the deﬁnition of A∗ to go from the

k
ε2

(cid:0)

(cid:1)

k

k

k

Step 2: We show that

M

m=1 h
X
M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

f (X (m)

n , A(X (m)

i
n )) + ˆY (m),A

n+1 −

k )N −1

k=n+1

A,(ˆaM
n

−

E

J

h

(Xn)
i

(m)
f (X ′
n

(m)
, A(X ′
n

))

−

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆY ′
(cid:12)
n+1

(m),A

E

"

4E
"

≤

sup

A

∈A

sup

A

∈A

(C.3)

1
M

1
M

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m=1 h
X
+

O

(cid:18)

1
√M (cid:19)

.

Indeed, let M ′ = √2 (N

−

n)

∞+

f
k
k
√M

g

k

∞

k

, and notice

E

"

1
M

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
P
(cid:12)
"

∞

0

Z

=

′

M

P

"

=

0

Z

A

+

∞

P

M ′

Z

"

(N

√2

≤

−

m=1 h
X

1
M

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
f
n)
(cid:12)
k∞
k
(cid:12)
√M

1
M

1
M

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

E

A,(ˆaM
n

J

k )N −1

k=n+1

(Xn)

(cid:12)
#
(cid:12)
i
(cid:12)
k )N −1
(cid:12)
(cid:12)
k=n+1

M

f (X (m)

n , A(X (m)

−

h

i
n )) + ˆY (m),A

n+1

m=1 h
X
M

f (X (m)

n , A(X (m)

i
n )) + ˆY (m),A

n+1

m=1 h
X
M

f (X (m)

n , A(X (m)

i
n )) + ˆY (m),A

n+1

m=1 h
X
g
k∞
k

i

+

−

−

E

A,(ˆaM
n

J

−

(Xn)

> ε

h

E

A,(ˆaM
n

J

k )N −1

k=n+1

(Xn)

h

E

A,(ˆaM
k )
n

J

N −1
k=n+1

h

(Xn)

> ε

dε

#

> ε

dε

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

+ 4

∞

P

0

Z

(C.4)

1
M

"

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1 h
X

(m)
f (X ′
n

(m)
, A(X ′
n

))

−

(m),A

ˆY ′
n+1

−

i

dε.

> ε

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The second term in the r.h.s. of (C.4) comes from (C.1). It remains to write the
latter as an expectation to obtain (C.3).

Step 3: Introduction of additional randomness by random signs.

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dε

#

E

"

sup

A

∈A

(C.5)

≤

1
M

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
4E
(cid:12)
"

M

m=1 h
X

sup

1
M

M

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

27

Let (rm)1

≤

m

≤

M be i.i.d. Rademacher r.v.d. We show that:

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

(m)
f (X ′
n

(m)
, A(X ′
n

))

(m),A

ˆY ′
n+1

−

n , A(X (m)

n )) + ˆY (m),A

n+1

.

rm

f (X (m)
h

A

∈A

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(m)
Since for each m = 1, ..., M the set of exogenous noises (ε′
)n
(cid:12)
N
k
are i.i.d., their joint distribution remains the same if one randomly interchanges the
corresponding components. Thus, the following holds for ε

N and (ε(m)

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m=1
X

)n

0:

#

≤

≤

≤

≤

k

k

k

≥

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

f (X ′

m
n , A(X ′

m
n ))

−

(m),A

ˆY ′
n+1

> ε

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

"

= P

P

≤

m=1 h
X
M
1
M

m=1
X
M

m=1
X
1
M

M

1
M

A

"

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
M

1
M

+ P

"

"

A

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (X (m)
h

f (X (m)
h

rm

rm

M

rm

h

m=1
X

M

2P

≤

"

(cid:12)
(cid:12)
i
(cid:12)
It remains to integrate on R+ w.r.t. ε to get (C.5).
(cid:12)
(cid:12)

m=1
X

n , A(X (m)

n )) + ˆY (m),A

n+1

rm

f (X (m)
h

n , A(X (m)

n )) + ˆY (m),A

n+1 −

f (X ′

m
n , A(X ′

m
n ))

−

n , A(X (m)

n )) + ˆY (m),A

n+1

>

(cid:12)
(cid:12)
(cid:12)
(cid:12)
n )) + ˆY (m),A
(cid:12)
n+1

i

f (X (m)

n , A(X (m)

ε
2 #

>

ε
2 #

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)
>

ε
2 #

.

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(m),A
ˆY ′
(cid:12)
n+1

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

> ε

#

Step 4: We show that

E

"

sup

A

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤

1
M

(N

M

rm

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1
h
X
f
n)
k∞
k
−
√M

+

g

k

k∞

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

+

[f ]L + [f ]L

N

1

−

1 + ηM γM

n

k

−

(C.6)

=

O

(cid:18)

Xk=n+1
,

(cid:0)
as M

n

M ηN
γN

−

M

−

n

√M (cid:19)

(cid:1)

.
→ ∞

[F ]k

n

L + ηN

M γN

M [F ]N

−

−

−

L

n

n

n

−

[g]L

!

Adding and removing the cost obtained by control 0 at time n yields:

E

"

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(C.7)

1
M

M

rm

m=1
X

n , A(X (m)

n )) + ˆY (m),A

n+1

f (X (m)
(cid:16)
M

f (X (m)

n , 0) + ˆY (m),0

n+1

E

≤

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
"

+ E

1
M

rm

m=1
X

A

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M

(cid:16)

M

rm

m=1
X

(cid:16)

(cid:17)

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)
#

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
n ))

dThe probability mass function of a Rademacher r.v. is by deﬁnition 1
2 δ

1 + 1

2 δ1.

−

f (X (m)

n , A(X (m)

f (X (m)

n , 0) + ˆY (m),A

n+1 −

−

ˆY (m),0
n+1

.

(cid:17)

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

 
28

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

We now bound the ﬁrst term of the r.h.s. of (C.7). By the Cauchy-Schwarz inequality,
and recalling that (rm)1

M are i.i.d. with zero mean such that r2

m = 1, we get

m

≤

≤

1
M

E

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(C.8)

M

rm

f (X (m)

n , 0) + ˆY (m),0

n+1

m=1
X

(cid:16)

1
M v
u
u
t
1
√M

E

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
(N

M

rm

m=1
X
n)
k

−

f

# ≤
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

f (X (m)

n , 0) + ˆY (m),0

n+1

(cid:16)

+

g

k

k∞

k∞

2

#

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Turn now to the second term of (C.7). By the Lipschitz continuity of f , it stands:

(cid:0)

(cid:1)

E

"

M

sup
∈AM (cid:12)
A
m=1
(cid:12)
X
(cid:12)
(cid:12)
[f ]LE
(cid:12)

≤

rm

f (X (m)

n , A(X (m)

n ))

f (X (m)

n , 0) + ˆY (m),A

n+1 −

−

ˆY (m),0
n+1

(cid:16)

sup
∈AM

A

(cid:20)

M

m=1
(cid:12)
X
(cid:12)
1
N
(cid:12)
−

Xk=n+1
(cid:0)
M γN

−

n

M

1 + ηN

rmA(X (m)
n )

+ E

(cid:21)

(cid:12)
(cid:12)
(cid:12)

1 + ηM γM

k

nE

−

(cid:1)

M

rm

m=1
X
k

(cid:16)

j=n+1
Y

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M

sup
m

≤

≤

"

A

"

1

N

−

n

E

"

(cid:17)

sup
m

≤

1

≤

M

j=n+1
Y

C

εm
j

(cid:0)

(cid:1)

# !

C

εm
j

(cid:0)

E

#

(cid:1)

sup
∈AM

"

A

#

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
ˆY (m),A
(cid:12)
n+1 −

[f ]L + [f ]L

≤  

(C.9)

+ [g]L

(cid:16)

ˆY (m),0
n+1

#

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

rmA(X (m)
n )

M

m=1
X

(cid:12)
(cid:12)
(cid:12)

#

(cid:12)
(cid:12)
(cid:12)

where we condition by the exogenous noise, use assumption (HF-PI) and the ηM γM -
Lipschitz continuity of the estimated optimal controls at time k, for k = n+1, . . . , N
1. Now, notice ﬁrst that

−

(C.10)

E

"

sup
m

≤

1

≤

N

M

Yk=n+1

C (εm
k )

# ≤

N

E

Yk=n+1

(cid:20)

sup
m

≤

C (εm
k )
(cid:21)

M

≤

ρN
n
M ,
−

1

≤

and moreover:

M

E

sup

A

(cid:20)

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)

m=1
X

rmA(X (m)
n )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(C.11)

γM E
(cid:20)

v

|

≤

(cid:21)

≤

γM E
(cid:20)

v

|

M

m=1
X
M

m=1
X

sup
|2≤

sup
|2≤

1/R (cid:12)
(cid:12)
(cid:12)
(cid:12)
1/R (cid:12)
(cid:12)
(cid:12)
(cid:12)

rm(vT X (m)

n )+

rmvT X (m)

n

,

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

where R > 0 is a bound for the state space (see e.g. the discussion on the Frank-Wolfe
step p.10 of [2] for a proof of this inequality), which implies by the Cauchy-Schwarz
inequality:

E

sup

A

(cid:20)

∈A

M

m=1
X

M (cid:12)
(cid:12)
(cid:12)
(cid:12)

rmA(X (m)
n )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:21)

γM
R v
u
u
t

m=1
X

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

M

2

rmX (m)
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

γM √M

≤

(cid:21)

since (rm)m are i.i.d. Rademacher r.v.. Plug (C.10) and (C.11) into (C.9) to obtain

(C.12)

M

rm

m=1
X

f (X (m)
(cid:16)

N

1

−

n , A(X (m)

n ))

[f ]L + [f ]L

1 + ηM γM

E

"

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤  

f (X (m)

n , 0) + ˆY (m),A

n+1 −

ˆY (m),0
n+1

nρk

n

M + [g]L
−

1 + ηN

M γN

−

M

n

−

n

−

k

−

#

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
ρN
(cid:12)
M

−

Xk=n+1

(cid:0)

(cid:1)

(cid:16)

(cid:17)

γM √M .

n

!

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

29

Plug then (C.8) and (C.12) into (C.7) to get (C.6).

Step 5: Conclusion. Plug (C.6) into (C.5) and combine it with (C.3) to obtain the
✷
bound on the estimation error, as stated in (4.11) of Lemma 4.10.

Appendix D. Proof of Lemma 4.11.
Let (ˆaM

1

k )N

k=n+1 be the sequence of estimated controls at time k = n + 1, ..., N

−

1.

−

∈ AM and recall that we denote by J

Take A
with the control A at time n, and ˆaM
characterized as solution of the Bellman equation

k at time k = n + 1, . . . , N

−

the cost functional associated
1. The latter is

A,(ˆa)N −1
n

k=n+1

k=n+1

A,(ˆa)N −1
N
A,(ˆa)N −1
n

k=n+1

J

J




(x) = g(x)
(x) = f (x, A(x)) + EA
n,x

A,(ˆa)N −1
n+1

k=n+1

J

(Xn+1)

,

where EA
control A is followed at time n.

n,x[

·

] stands for the expectation conditioned by

(cid:2)

Xn = x
}

{

(cid:3)
when feedback

Take n

1, ..., N

. The following holds:

}

∈ {
εapprox
PI,n

:= inf
A
∈A

M

= inf
A
∈A
+ E

M

EM

A,(ˆa)N −1
n

k=n+1

J

(Xn)

−

A

EM

(cid:2)
J

A,(ˆa)N −1
n

k=n+1

(Xn)

EM

inf
AX
∈
(cid:2)
Vn(Xn)

E

(cid:3)
−
A,(ˆa)N −1
(cid:2)
k=n+1
n

(cid:3)
J

(cid:3)
(Xn)

(cid:2)
Vn(Xn)

(cid:2)

EM

J

A

inf
−
AX
∈
A,(ˆa)N −1
(cid:3)
n

k=n+1

EM

(cid:2)
(Xn)

−

E

Vn(Xn)

,

(cid:3)

(cid:3)

(cid:2)

(cid:3)

(D.1)

≤

A

inf
∈A

M

(cid:2)

J

A,(ˆa)N −1
n

k=n+1

(Xn)

(cid:3)

where the last inequality stands because the value function is smaller than the cost
functional associated with any other strategy. We then apply the dynamic program-
ming principle to obtain:

min
A
M
∈A

(D.2)

EM

A,(ˆa)N −1
n

k=n+1

J

(Xn)

E

Vn(Xn)

−

(cid:2)

≤

A

M

inf
∈A
E

−

h

(cid:0)

(cid:3)
EM

(cid:2)
(cid:3)
Xn, A(Xn)

f

+ EA
n

(ˆa)N −1
n+1

k=n+1

J

(Xn+1)

h

(cid:0)
Xn, aopt

n (Xn)

f

(cid:1)
+ Eaopt
n

(cid:2)
Vn+1(Xn+1)

(cid:3)i

.
(cid:3)i

To bound the r.h.s. of (D.2), ﬁrst observe that for A

(cid:1)

(cid:2)
∈ AM :

EM

f

Xn, A(Xn)

+ EA
n

(ˆa)N −1
n+1

k=n+1

J

(Xn+1)

h

(cid:0)

(cid:1)

(cid:2)

E

≤

f

|

Xn, A(Xn)

−
f

−

E

f

(cid:3)i
Xn, aopt

n (Xn)

+ Eaopt
n

Vn+1(Xn+1)

h
Xn, aopt
(cid:0)

n (Xn)

(cid:1)

(cid:2)

(cid:3)i

(cid:2)

(cid:0)

(cid:1)

+ EM
(cid:0)

EA

n J

|
(ˆa)N −1
(cid:1)
(cid:3)
k=n+1
n+1

(Xn+1)

aopt
n (Xn)
|
(ˆa)N −1
n+1

k=n+1

−
J

(cid:3)
(Xn+1)

−

Eaopt

n Vn+1(Xn+1)
i

−

Vn+1(Xn+1)
i

,

E

h
A(Xn)

|
(cid:2)
r
k

k∞

EM

h

[f ]L +

Vn+1k∞

k

[r]L

≤

(D.3)

(cid:0)

(cid:1)
+

30

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

where we used twice assumption (Hd) at the second-last line of (D.3). Inject

EM

(ˆa)N −1
n+1

k=n+1

J

into (D.3) to obtain:

h

(Xn+1)
i

≤

A

inf
∈A

M

EM

A,(ˆa)N −1
n+1

k=n+2

J

h

+ 2εesti
n+1

(Xn+1)
i

EM

f
(cid:20)

Xn, A(Xn)

+ EA
n

(ˆa)N −1
n+1

k=n+1

J

(Xn+1)

(cid:0)

(cid:1)

(cid:20)

(cid:21)(cid:21)
Xn, aopt

n (Xn)

E

f

+ Eaopt
n

[Vn+1(Xn+1)]

−

[f ]L +

k

≤

[r]L

E

h
(cid:0)
A(Xn)

|
A,(ˆa)N −1
(cid:1)
(cid:2)
J
n+1

k=n+2

Vn+1k∞
EM
inf
∈A

M

A

aopt
(cid:1)
n (Xn)
|

−

i

(Xn+1)

(cid:3)

Vn+1(Xn+1)
(cid:21)

−

+ 2

r

k

k∞

εesti
n+1.

(D.4)

(cid:0)
+

r

k

k∞

(cid:20)
Plugging (D.4) into (D.2) yields

EM

A,(ˆa)N −1
n

k=n+1

J

inf
∈A

M

A

(cid:20)

r

≤ k

k∞

A

inf
∈A
[f ]L +

M

+

(Xn)
(cid:21)
EM

E [Vn(Xn)]

−

A,(ˆa)N −1
n+1

k=n+2

J

(Xn+1)

(cid:20)
Vn+1k∞

[r]L

A

k

E

inf
∈A

M

|

(cid:2)

which implies by induction, as M

(cid:0)

+

(cid:1)
:
∞

→

E

inf
∈A

M

A

(cid:20)

EM

A,(ˆa)N −1
n

k=n+1

J

(cid:20)

=

O

sup
k

≤

≤

N

1

−

n+1

(cid:18)

(Xn)
(cid:21)
E

−

εesti
k

E [Vn(Xn)]

(cid:21)
+ sup
N
k

n

≤

≤

−

−

Vn+1(Xn+1)
(cid:21)

A(Xn)

aopt
n (Xn)
|

−

+ 2

r

k

k∞

εesti
n+1

,

(cid:3)

inf
∈A

M

A

1

E

|

A(Xn)

aopt
n (Xn)
|

−

.
(cid:19)

1,
✷

−

(cid:2)
(cid:3)
We now use Lemma 4.10 to bound the expectations of the εesti
PI,k for k = n+1, . . . , N
and plug the result into (D.1) to complete the proof of Lemma 4.11.

(cid:2)

(cid:3)

Appendix E. Function approximation by neural networks.
We assume aopt

L2(µ), and show the relation (4.9) in Proposition 4.8.

k ∈

The universal approximator theorem applies for
:=
all ε > 0, for all k, there exists a neural network a∗ in

A∞

∞M=1 AM , and states that for
aopt
<
such that:
k −
S
k
A∞
seen as a compact of

k∞

a∗

X

Vd(
X

) , where

) stands for the volume of compact set

ε
d(
X
V
the Euclidean space Rd. By integrating, we then get: supn
≤
a∗(x)
limM

aopt
k (x)
1 is increasing, which implies that
∞ AM and gives the existence of M > 0 that depends on ε such that a∗

(cid:12)
→
(cid:12)
Therefore, we have shown that for n = 0, ..., N

dµ(x) < ε. Also, notice that

AM
(cid:0)

−
=
A∞
∈ AM .

X
R

M

(cid:12)
(cid:12)

1

(cid:1)

N

≥

≤

−

+

k

1

−

sup
N
k

≤

n

≤

A

1

−

inf
∈A

M

E

|

A(Xk)

aopt
k (Xk)
|

−

(cid:2)

which is the required result stated in (4.9).

0,

with Xk ∼

µ,

−−−−→M
→∞

(cid:3)

✷

We now show (4.10) of proposition 4.8. As stated in Proposition 6 of Section 4.7
in [2]: we can approximate a c-Lipschitz function by a γ1-norm less than γM and

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

31

2d/(d+1)

log γM
uniform error less than c
c , and proposition 1 in [2] shows that a
function with γ1 less than γM may be approximated with KM neurons with uniform
error γM K −
M
Thus, given KM and γM , there exists a neural network a∗ in

(d+3)/(2d)

γM
c

(cid:0)

(cid:1)

−

.

VM such that

(E.1)

a∗

k

−

aopt

k∞ ≤

c

2d/(d+1)

−

log

γM
c

(cid:16)

(cid:17)

γM
c

(cid:16)

(cid:17)

+ γM K −
M

(d+3)/(2d)

.

Appendix F. Proof of Lemma 4.15.
We prove Lemma 4.15 in four steps. Since the proof is very similar to the one of

Lemma 4.10, we only detail the arguments that are modiﬁed.
Step 1: Symmetrization by a ghost sample. We take ε > 0 and show that for M >

(N

n)

f

k

−

∞+

k
ε2

∞

g

k

k

2

, the following holds

M

(cid:1)
f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

2

P

(cid:0)

"

1
M

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

"

A

2P

≤

(F.1)

m=1 h
X
M
1
M

m=1 h
X

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

E

h

−

i

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

(m)
f (X ′
n

(m)
, A(X ′
n

))

−

ˆY ′
n+1

of

where
εm
(cid:0)
n+1

M
(m)
X ′
m=1 is an i.i.d.
n
M
m=1; ˆY (m),A
:= ˆV M
(cid:1)
n+1
(m)
(m)
(cid:1)
X ′
X ′
, A
n
n
Since ˆV M
(cid:16)
(cid:16)

m
n+1

, ε′

n+1

F

ˆV M
(cid:0)
n+1

(cid:16)

(cid:17)(cid:17)

F

copy of
X (m)
(cid:16)
.

n , A
(cid:0)

M
X (m)
m=1;
n
X (m)
, εm
(cid:1)
n

(cid:0)
n+1

m
ε′
n+1

M
m=1 is an i.i.d.
; and ﬁnally ˆY ′
(cid:1)

n+1

(cid:0)

(cid:1)

(cid:17)(cid:17)

n the estimated value function at time n, for n=0, ..., N

−
by construction, (F.1) is proved the same as in step 1 of Lemma 4.10.

(cid:0)

(cid:1)

1, is bounded
✷

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(m),A
(cid:12)

> ε

#

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

>

ε
2 #

,

copy

(m),A

:=

Step 2: The following result holds

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n , A(X (m)

n )) + ˆY (m),A

n+1

E

−

n+1

f (X (m)
i
h
n )) + ˆY (m),A

n+1 −

f (X (m)

n , A(X (m)

(m)
f (X ′
n

(m)
, A(X ′
n

))

−

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(m),A
ˆY ′
(cid:12)
n+1

E

"

A

sup
∈AM (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
"

4E

≤

(F.2)

+

M

1
M

m=1 h
X
1
M

sup
∈AM (cid:12)
A
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
√M

(cid:18)

O

(cid:19)

M

m=1 h
X
,

and it is proved the same way as step 2 in the proof of Lemma 4.10.

Step 3: Introduction of additional randomness by random signs. The following result
holds:

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1 −

(m)
f (X ′
n

(m)
, A(X ′
n

))

(m),A

ˆY ′
n+1

−

E

"

sup

A

∈A

(F.3)

≤

1
M

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
4E
(cid:12)
"

M

m=1 h
X

sup

A

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M

M

rm

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

m=1
X

h

,

#

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

as proved already in step 3 in the proof of Lemma 4.10.

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

✷

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

✷

"(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ E
"

m=1
X

sup

A

∈A

M

(cid:16)
1
M

The ﬁrst term in the r.h.s.
(N
g

∞+

n)

∞

f
k
k
√M

k

k

−
term:

M

E

"

sup

A

∈A

M (cid:12)
m=1
(cid:12)
X
(cid:12)
(cid:12)
[f ]LE
(cid:12)
(cid:20)

(cid:16)

sup
M

∈A

A

32

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

Step 4: We show that

1
M

M

m=1
X

rm

sup

A

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

"

(F.4)

h

≤

=

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

(N

−

+

f
n)
k
k∞
√M
M ηM
,
√M (cid:19)

ρM γ2

O

(cid:18)

i

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g

k

k∞

+ ([f ]L + ρM γM ηM )

γM
√M

as M

+

.
∞

→

Adding and removing the cost obtained by control 0 at time n yields:

(F.5)

E

"

sup

A

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
M

E

≤

M

1
M

m=1
X

M

f (X (m)

n , A(X (m)

n )) + ˆY (m),A

n+1

rm

(cid:16)

rm

f (X (m)

n , 0) + ˆY (m),0

n+1

(cid:17)

(cid:12)
#
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
n ))

rm

f (X (m)

n , A(X (m)

m=1
X

M (cid:12)
(cid:12)
(cid:16)
(cid:12)
(cid:12)
in (F.5) is bounded as in the proof of Lemma 4.10 by
(cid:12)
. We use the Lipschitz-continuity of f as follows, to bound its second

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

f (X (m)

n , 0) + ˆY (m),A

ˆY (m),0
n+1

n+1 −

−

.
#

rm

f (X (m)

n , A(X (m)

n ))

f (X (m)

n , 0) + ˆY (m),A

ˆY (m),0
n+1

n+1 −

−

≤

≤

M

m=1
X

(cid:12)
(cid:12)
(cid:12)

rmA(X (m)
n )
(cid:21)
(cid:12)
(cid:12)
M
(cid:12)

M

m=1
X

,

#

+ E
"

sup

A

∈A

M (cid:12)
(cid:12)
(cid:12)
(cid:12)
rmA(X (m)
(cid:12)
n )
(cid:12)
(cid:12)
(cid:12)

m=1
X

(cid:12)
(cid:12)
(cid:12)

([f ]L + ρM ηM γM ) E
"

sup
M

∈A

A

#

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
ˆY (m),A
ˆY (m),0
(cid:12)
n+1 −
n+1

rm

(cid:16)

,
#

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

where we conditioned by the exogenous noise, used assumption (HF), and the ηM γM -
Lipschitz continuity of the estimated value fonction at time n + 1.

By using the same arguments as those presented to prove Lemma 4.10, we have

E

supA

M

M

m=1 rmA(X (m)
n )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:20)

∈A
Step 5: Conclusion
Combining(F.2),(F.3) and (F.4) results in the bound on the estimation error as stated
✷
in (4.24).

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

γM √M , and then conclude that (F.4) holds.

Appendix G. Proof of Lemma 4.17.
We divide the proof of Lemma 4.17 into two steps. First write

εapprox
HN,n ≤

A

(G.1)

inf
∈A

M

EM

f (Xn, A(Xn)) + ˆV M
n+1
h
+ E [Vn(Xn)]

EM

−

A

inf
AX
∈

X A

n+1

E [Vn(Xn)]

−

(cid:0)

(cid:1)i
f (Xn, A(Xn)) + ˆV M
n+1
h

X A

n+1

.

(cid:0)

(cid:1)i

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

33

Step 1: We show

inf
∈A

M

A

EM

f (Xn, A(Xn)) + ˆV M
n+1

X A

n+1

(G.2)

Take A

h
≤

([f ]L +

(cid:0)
[r]L) inf
AX
∈

A

k

Vn+1k∞
EM
r
+

(cid:2)(cid:12)
Vn+1(Xn+1)
(cid:12)

k∞
∈ AM , and apply the dynamic programming principle to write

h(cid:12)
(cid:12)
(cid:12)

−

k

ˆV M
n+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)

(cid:3)

(cid:12)
(cid:12)

.

i

−
(cid:1)i
EM

E [Vn(Xn)]

A(Xn)

−

aopt
n (Xn)

EM

≤

≤

f (Xn, A(Xn)) + ˆV M
n+1
h
[f ]LEM

A(Xn)

(cid:16)
aopt
n (Xn)

X A

n+1

−
(cid:17)i
+ EM

([f ]L +

(cid:2)(cid:12)
Vn+1
(cid:12)
k

k∞

−
[r]L) EM

E [Vn(Xn)]

ˆV M
n+1(X A

n+1)

EM
h
h
aopt
n (Xn)

+ EM

(cid:3)

(cid:12)
(cid:12)

h(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:3)
A(Xn)
(cid:12)

−

(cid:2)(cid:12)
(cid:12)

EM

Vn+1

−
i
h
ˆV M
n+1(X A
n+1)

(cid:16)
−

X aopt

n
n+1

(cid:17)ii
Vn+1(X A

n+1)

,

(cid:12)
i
(cid:12)
(cid:12)

where we used (Hd) at the second-to-last line. By using one more time assumption
(Hd), we then get:

EM

f (Xn, A(Xn)) + ˆV M
n+1
h

([f ]L +

≤

X A

n+1

E [Vn(Xn)]

−
(cid:1)i
[r]L) EM
(cid:0)
Vn+1k∞
k
ˆV M
EM
(cid:2)(cid:12)
n+1(Xn+1)
(cid:12)
−

+

r

k

k∞

h(cid:12)
(cid:12)
which is the result stated in (G.2).
(cid:12)

A(Xn)

aopt
n (Xn)

−
Vn+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)

,

(cid:12)
(cid:12)

(cid:3)
with Xn+1 ∼

µ,

i

Step 2: We show

E [Vn(Xn)]

EM

−

inf
AX
∈

A

(G.3)

Write

f (Xn, A(Xn)) + ˆV M
n+1
h

Vn+1(Xn+1)

(cid:0)

r

EM

≤ k

k∞

X A

n+1

−

(cid:1)i
ˆV M
n+1(Xn+1)
(cid:12)
(cid:12)
(cid:12)

.

i

h(cid:12)
(cid:12)
(cid:12)

X A

n+1

E [Vn(Xn)]

A

EM

inf
AX
∈

f (Xn, A(Xn)) + ˆV M
n+1
h
f (Xn, A(Xn)) + Vn+1

X A

n+1

(cid:16)

−
EM

h

EM

Vn+1

X A

n+1

ˆV M
n+1

(cid:16)
X A

n+1

(cid:17)i

−

(cid:16)
which completes the proof of (G.3).

(cid:16)

(cid:17)

h

(cid:17)i

≤

A

≤

A

inf
AX
∈
inf
AX
∈

(cid:17)i
EM

EM

A

−

inf
AX
∈
r
≤ k

k∞

f (Xn, A(Xn)) + ˆV M
n+1
h
Vn+1(Xn+1)

ˆV M
n+1(Xn+1)

(cid:16)

X A

n+1

−

h(cid:12)
(cid:12)
(cid:12)

(cid:17)i

i

(cid:12)
(cid:12)
(cid:12)

Step 3 Conclusion:
We complete the proof of Lemma 4.17 by plugging (G.2) and (G.3) into (G.1).

✷

Appendix H. Some useful Lemmas for the proof of Theorem 4.13.
Fix M

Rd, and set xM = (x1, . . . , xM ). Deﬁne the distance

∈
d2(f, g) between f : Rd

N∗, let x1, . . . , xM ∈
→

R and g : Rd

R by

→

d2(f, g) =

1
M

M

|

m=1
X

1/2

f (xm)

2

g(xm)
|

−

!

.

 
34

C. HUR´E, H. PHAM, A. BACHOUCH AND N. LANGREN´E

An ε-cover of

V

is a set of functions f1, . . . , fP : Rd

R such that

→

min
p=1,...,P

d2 (f, fp) < ε,

for f

.

∈ V

N2(ε,

Let
V
and set by convention
ﬁnite size.

N2(ε,

V

, xM ) denote the size of the smallest ε-cover of
, xM ) = +

V
if there does not exist any ε-cover of

w.r.t. the distance d2,
of

N2(ε,

V

, xM ) is called  L2-ε-covering number of

∞

V

on xM .

V

Lemma H.1. Let (X, Y ) be a random variable. Assume

L a.s. and let

m(x) = E[Y
|
c2E
h

max
m=1,...,M

X = x]. Assume Y
m(X))2/c2
e(Y
1

−

−
X

≤
assume that the regression function is bounded by L and that γM −−−−−→M

−

i

+

|

≥

∞

+

. Set

σ2

a.s. for some c, σ > 0. Let γM , L

1 and

→

∞

m(X) is sub-Gaussian in the sense that

Y

|

| ≤

ˆmM := argmin

Φ

M

∈V

1
M

M

m=1
X

Φ(xi)

−

2

Ym

(cid:12)
(cid:12)

(cid:12)
(cid:12)

for some
[
→
which are bounded by L, and denote

VM of functions Φ : Rd

−

γM , γM ] and some random variables Y1, ..., YM

Ωg :=

f

g : f

−

(cid:26)

∈ VM ,

1
M

M

f (xm)

2

g(xm)

−

δ
.
γ2
M (cid:27)

≤

m=1
X
Then there exist constants c1, c2 > 0 which depend only on σ and c such that for any
δM > 0 with

(cid:12)
(cid:12)

(cid:12)
(cid:12)

δM −−−−−→M

+

→

∞

0,

M δM
γM −−−−−→M
∞

→

+

+

,
∞

(H.1)

c1

√M δ
γ2
M ≥

√δ

c2δ/γ2
M

Z

log

2

N

(cid:18)

(cid:18)

u
4γM

, Ωg, xM
1

(cid:19)(cid:19)

1/2

du

for all δ

≥

δM and all g

∈ VM ∪ {

m

}

we have as M

E

ˆmM (X)

m(X)

2

=

−

P
O

(cid:18)

δM + inf
∈V

Φ

M

i
Proof. We refer to [16] (see its Theorem 3) for a proof.

h(cid:12)
(cid:12)

h(cid:12)
(cid:12)

(cid:12)
(cid:12)

+

:
∞

→

E

Φ(X)

Lemma H.2. For any ε > 0, we have

VM , (X (m)
n )1

≤

m

≤

N2

ε,

(cid:16)

where the class of neural networks

M

12eγM

KM + 1
ε

≤  

(cid:0)
(cid:17)
VM is deﬁned in Section 4.2.

(cid:1)

!

Proof. We refer to [17] for a proof.

−

2

m(X)

.

i(cid:19)

(cid:12)
(cid:12)

(4d+9)KM +1

,

NEURAL NETWORKS ALGORITHMS FOR STOCHASTIC CONTROL

35

REFERENCES

[1] K. Asadi and M. Littman, An alternative softmax operator for reinforcement learning, in
Proceedings of the 34th International Conference on Machine Learning, ICML’17, vol. 70,
2017, pp. 243–252.

[2] F. Bach, Breaking the curse of dimensionality with convex neural networks, Journal of Machine

Learning Research, 18 (2017), pp. 1–53.

[3] A. Bachouch, C. Hur´e, N. Langren´e, and H. Pham, Deep neural networks algorithms for
stochastic control problems on ﬁnite horizon: numerical applications, Methodology and
Computing in Applied Probability, (2021). To appear.

[4] C. Beck, A. Jentzen, and B. Kuckuck, Full error analysis for the training of deep neural

networks, arXiv:1910.00121v2, (2020).

[5] B. Bercu and J. Fort, Generic stochastic gradient methods, in Wiley Encyclopedia of Opera-

tions Research and Management Science, 2011, pp. 1–8.

[6] D. P. Bertsekas and J. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientiﬁc, 1996.
[7] G. Cybenko, Approximations by superpositions of sigmoidal functions, Mathematics of Con-

trol, Signals, and Systems, 2 (1989), pp. 303–314.

[8] W. E, J. Han, and A. Jentzen, Deep learning-based numerical methods for high-dimensional
parabolic partial diﬀerential equations and backward stochastic diﬀerential equations, Com-
munications in Mathematics and Statistics 5, 5 (2017), pp. 349–380.

[9] A. G´eron, Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, O’Reilly

Media, 2nd ed., 2019.

[10] L. Gy¨orfi, M. Kohler, A. Krzyzak, and H. Walk, A Distribution-Free Theory of Nonpara-

metric Regression, Springer Series in Statistics, 2002.

[11] J. Han and W. E, Deep learning approximation for stochastic control problems, in NIPS 2016,

Deep Reinforcement Learning Workshop, 2016.

[12] J. Han and J. Long, Convergence of the deep BSDE method for coupled FBSDEs, Probability,

Uncertainty and Quantitative Risk, 5 (2020), pp. 1–33.

[13] P. Henry-Labord`ere, Deep primal-dual algorithm for BSDEs: Applications of machine learn-

ing to CVA and IM, SSRN:3071506, (2017).

[14] K. Hornick, Approximation capabilities of multilayer feedforward networks, Neural Networks,,

4 (1991), pp. 251–257.

[15] A. Jentzen, B. Kuckuck, A. Neufeld, and P. von Wurstemberger, Strong error analysis

for stochastic gradient descent optimization algorithms, arXiv: 1801.09324v1, (2018).

[16] M. Kohler, Nonparametric regression with additional measurement errors in the dependent
variable, Journal of Statistical Planning and Inference, 136 (2006), pp. 3339–3361.
[17] M. Kohler, A. Krzy˙zak, and N. Todorovic, Pricing of high-dimensional American options

by neural networks, Mathematical Finance, 20 (2010), pp. 383–410.

[18] A. N. Kolmogorov, On the representation of continuous functions of several variables by
superpositions of continuous functions of a smaller number of variables, Mathematics and
Its Applications (Soviet Series), 25 (1991).

[19] S. Kou, X. Peng, and X. Xu, EM algorithm and stochastic control

in economics.

SSRN:2865124, 2016.

[20] Y. Li, Deep reinforcement learning: an overview, arXiv 1701.07274v3, (2017).
[21] F. A. Longstaff and E. S. Schwartz, Valuing American options by simulation: A simple
least-squares approach, The Review of Financial Studies, 14 (2001), pp. 113–147.
[22] V. Mnih, K. Kavukcuoglu, D. Silver, and A. A. Rusu, Human-level control through deep

reinforcement learning, Nature, 518 (2015), pp. 529–533.

[23] W. B. Powell, Approximate Dynamic Programming: Solving the Curses of Dimensionality,

Wiley & Sons, 2011.

[24] R. S. Sutton and A. G. Barto, Reinforcement Learning, The MIT Press, 1998.

