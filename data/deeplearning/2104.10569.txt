GraphTheta: A Distributed Graph Neural Network Learning
System With Flexible Training Strategy

Yongchao Liu1,∗, Houyi Li2,†,∗, Guowei Zhang1, Xintan Zeng1, Yongyong Li1, Bin Huang1, Peng
Zhang3,†, Zhao Li4,†, Xiaowei Zhu1, Changhua He1, Wenguang Chen1,5
1Ant Group, 2Fudan University, 3Guangzhou University, 4 Zhejiang University, 5 Tsinghua University
China

2
2
0
2

p
e
S
2

]

G
L
.
s
c
[

2
v
9
6
5
0
1
.
4
0
1
2
:
v
i
X
r
a

ABSTRACT
Graph neural networks (GNNs) have been demonstrated as a pow-
erful tool for analysing non-Euclidean graph data. However, the
lack of efficient distributed graph learning systems severely hinders
applications of GNNs, especially when graphs are big and GNNs are
relatively deep. Herein, we present GraphTheta, a novel distributed
and scalable graph learning system implemented in vertex-centric
graph programming model. GraphTheta is the first graph learning
system built upon distributed graph processing with neural net-
work operators implemented as user-defined functions. This system
supports multiple training strategies, and enables efficient and scal-
able big graph learning on distributed (virtual) machines with low
memory each. To facilitate graph convolution implementations,
GraphTheta puts forward a new graph learning abstraction named
NN-TGAR (Neural Network Transform-Gather-Apply-Reduce) to
bridge the gap between graph processing and graph deep learning.
A distributed graph engine is proposed to conduct the stochas-
tic gradient descent optimization with a hybrid-parallel execution.
Moreover, we add support for a new cluster-batched training strat-
egy besides the conventional global-batched and mini-batched ones.
We evaluate GraphTheta using a number of datasets with network
size ranging from small-, modest- to large-scale. Experimental re-
sults show that GraphTheta can scale well to 1,024 workers for
training an in-house developed GNN on an industry-scale Alipay
dataset of 1.4 billion nodes and 4.1 billion attributed edges, with a
cluster of CPU virtual machines (dockers) of small memory each
(5∼12GB). Moreover, GraphTheta obtains comparable or better
prediction results than the state-of-the-art GNN implementations,
demonstrating its capability of learning GNNs as well as existing
frameworks, and can outperform DistDGL by up to 2.02× with bet-
ter scalability. To the best of our knowledge, this work presents the
largest edge-attributed GNN learning task conducted on a billion-
scale industrial network in the literature.

PVLDB Reference Format:
Yongchao Liu1,∗, Houyi Li2,†,∗, Guowei Zhang1, Xintan Zeng1, Yongyong
Li1, Bin Huang1, Peng Zhang3,†, Zhao Li4,†, Xiaowei Zhu1, Changhua He1,
Wenguang Chen1,5. GraphTheta: A Distributed Graph Neural Network
Learning System With Flexible Training Strategy. PVLDB, 14(1): XXX-XXX,
2020.
doi:XX.XX/XXX.XX

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights

∗ Joint first authors; † Contributed to this work when the authors worked in Ant Group
or Alibaba Group; Correspond to {yongchao.ly, yuanben.cwg}@antgroup.com.

PVLDB Artifact Availability:
The source code, data, and/or other artifacts have been made available at
URL_TO_YOUR_ARTIFACTS.

1 INTRODUCTION
Graph neural networks (GNNs) [15, 29, 42, 59, 72–74, 106] have
been popularly used in analyzing non-Euclidean graph data and
achieved promising results in various applications, such as node
and graph classification [16, 24, 93, 95], link prediction [51, 105],
programme analysis [2, 4, 60], recommendation [19, 54, 98, 100, 113]
and quantum chemistry [25]. Existing GNNs can be categorized
into two groups, i.e., transductive graph learning (GL) and induc-
tive GL. From the perspective of transductive learning [16, 48],
although it is easy to implement by directly learning embeddings
from nodes and edges, the shortcomings of transductive learning
are obvious. First, the number of parameters expands linearly with
graph size, which is hard to train on big graphs. Second, it is unable
to calculate embeddings for new nodes and edges that are unseen
before training. These two limitations hinder the applications of
transductive learning on real-world graphs. In contrast, inductive
learning [9, 19, 31, 38, 100] introduces a trainable generator to com-
pute node and edge embeddings, where the generator is often a
neural network and can be trained to infer embeddings for new
unseen nodes and edges.

From the perspective of inductive learning, global-batch [16, 48]
and mini-batch [31] are two popular training strategies. Global-
batch performs full graph convolutions across an entire graph by
multiplying feature matrices with graph Laplacian. It requires calcu-
lations on the entire graph, regardless of graph density. In contrast,
mini-batch conducts localized convolutions on a batch of subgraphs,
where a subgraph is constructed from a node with its neighbors.
Typically, subgraph construction meets the challenge of size explo-
sion, especially when the node degrees and the neighborhood explo-
ration depth (neighborhood exploration depth is equal to the num-
ber of graph convolution layers) are very large [10, 52, 53, 96, 97].
Therefore, it is difficult to learn on dense graphs (dense graphs refer
to graphs of high density) or sparse ones with highly skewed node
degree distributions and the situation further deteriorates very
much for deep GNNs [10, 52, 53, 96, 97]. To alleviate the challenge
of mini-batch, a number of neighbor sampling methods are pro-
posed to reduce the size of neighbors [9, 38, 40, 101]. However, the

licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

 
 
 
 
 
 
only one type of training strategies. Refer to Section 6 for more
details about existing frameworks and systematic comparisons.

In fact, existing mini-batch based distributed methods use a data-
parallel execution, where each worker takes a batch of subgraphs,
and performs the forward and gradient computation independently.
Unfortunately, these methods severely limit the system scalability
in processing graphs,which are dense or have a highly skewed node
degree distribution (e.g. power-law), as well as deep neighborhood
exploration. On one hand, not even mentioning dense graphs, high-
degree nodes in a highly skewed graph can often cause a worker to
become short of memory space to fully store a subgraph. For exam-
ple, in Alipay dataset (see Table 2), the degree of a node can reach
hundreds of thousands. On the other hand, a deep neighborhood
exploration results in explosion of a subgraph. In Alipay dataset,
a batch of subgraphs obtained from a two-hop neighborhood ex-
ploration with only 0.002% nodes can generate a large size of 4.3%
nodes of the entire graph. If the neighborhood exploration reaches
0.05%, the batch of subgraphs reaches up to 34% of the entire graph.
In this way, one training step of this mini-batch will include 1/3
nodes in the forward and backward computation.

In this paper, we propose a new parallel and distributed GL sys-
tem named GraphTheta (Figure 2 illustrates its workflow) to solve
the observed challenges above. GraphTheta is built from the ground
up to support GNN training and inference and does not depend on
existing DL frameworks like TensorFlow [1] and PyTorch [70]. The
rationale behind such a design is that for deep learning, the training
procedure can be modeled as an auto-differentiable computational
graph of data and operator (or neural network function) nodes.
For each training step, we can build the computational graph by
means of treating vertices/edges as data nodes and then building
data-operator or operator-operator linkages following the GNN
and the input graph topology. In this case, large graphs will make
the computational graph inefficiently processed by the runtime of
existing frameworks. Subject to specific implementations, the size
of the computation graph can have a higher order of magnitude
than the input graph. In the following, we will elaborate the key
challenges that GraphTheta intends to solve.

The first challenge is to support both dense graphs and highly
skewed sparse graphs. To address this challenge, we adopt a dis-
tributed graph training engine with customizable graph partitioning
methods (refer to Sections 4 and 5.4). Along with this distributed en-
gine, we implement a hybrid-parallel execution, in contrast with the
conventional model-parallel and data-parallel training paradigms.
This hybrid-parallel paradigm performs the forward and backward
propagation computation of given graph data by a group of workers
in a distributed fashion, thus enabling our system to scale up to big
dense or sparse graphs (refer to Figure 2).

The second challenge is to allow for conveniently exploring new
training strategies in addition to the existing mini-batch and global-
batch methods. For this challenge, existing tensor-based frameworks
use sparse tensors to represent subgraphs and full graphs, which
poses challenges to support the multiple training strategies de-
scribed in this paper (especially global-batch) for the following
reasons. First, sparse tensors are nodes of the auto-differential com-
putational graph and excessive sparse tensors will make the compu-
tational graph too large to be efficiently processed by the runtime
of these frameworks, as described above. Second, sparse tensors

Figure 1: A typical architecture of distributed frameworks.

Figure 2: The workflow of GraphTheta.

sampling methods can bring unstable results during inference [19]
and result in lower performance [41, 80, 101].

Currently, GNNs are implemented as proof-of-concept built upon
deep learning (DL) frameworks like Tensorflow [1], PyTorch [70]
and MXNet [12]. However, these DL frameworks are not specially
designed for non-Euclidean graph data and thus inefficient on
graphs. Moreover, existing graph processing systems [11, 27, 65,
114] are incapable of training GNNs. Therefore, developing a new
GL system becomes predominant in industry [19, 64, 86, 100, 113].
Motivation and challenges. A number of GL frameworks have
been proposed, among which some are shared-memory [19, 57,
58, 64, 66, 68, 69, 84, 86, 99] and some use distributed comput-
ing [20, 23, 41, 56, 79, 85, 94, 103, 108–110, 113]. The shared-memory
frameworks are restricted to the host memory size and difficult to
handle big graphs. One approach to overcoming this constraint
is to use external storage to store large graphs [69]. Actually, this
method is equivalent to constructing a graph storage server in disk
for use. Most of the distributed frameworks employ graph storage
servers to store graphs, which either requires every server to hold
the entire graph or distributes the whole graph over a set of servers.
In principle, the architecture of these distributed frameworks can
be abstracted as Figure 1. To identify the neighbors of a given node,
classical graph computing and MapReduce [17, 102] methods can
be used. Futhermore, these distributed frameworks usually target

2

Deep Learning FrameworkGraph/KV StoreTensorFlowPyTorchMxNetDeep Learning SDKSamplingMethodsGraph/KV StoreClient APIsCustom DL Operators/ModulesEmbeddingsSub-graphs/features/embeddingsEdge NNVertex NNVertexNN{e12,e13}{v1}v1v2e12e13v3Worker 0Distributed Graph Partition 1Vertex NNEdge NNVertexNN{v2,v3}{e21}v1v2v3Worker 1Distributed Graph Partition 2MasterBuild Computation GraphTraining/Inference ProcedureCommand workers to load graphs;for step in [0, …, max_steps]doCommand workers to perform a step;doneWorker HealthMonitorWorkerCoordinatorvvCheckpointManagementNN-Th12,h13NN-Th1NN-GSumNN-ANN-Th2,h3NN-Th21NN-GSumNN-Ah2,h3h1Mirrornodee21MirrornodeIncludes1node& 2edgesIncludes2nodes& 1edgeAlgorithm 1 MPGNN under different training strategies.

1: Partition graph G into 𝑃 non-intersect subgraphs, denoted as

G = (cid:208){G1, G2, ..., G𝑃 }.

2: G𝑝 ← G𝑝 ∪ 𝑁𝐾 (G𝑝 ) ∀𝑝 ∈ [1, 𝑃]
3: for each 𝑟 ∈ [1, 𝑆𝑡] do
4:

5:

6:

7:

8:

9:

10:

11:

Pick 𝛾 subgraphs randomly, B𝑟 ← (cid:208)𝛾 {G𝑖 }
for each 𝑘 ∈ [1, 𝐾] do
𝑖 ← Proj𝑘 (𝒉𝑘−1
𝒏𝑘
|𝑾𝑘 ) ∀𝑣𝑖 ∈ B𝑟
𝑖
(cid:16)
𝑗 , 𝒆𝑖,𝑗 , 𝒏𝑘
𝒏𝑘
𝒎𝑘
𝑗→𝑖 ← Prop𝑘
𝑖
(cid:9)
, (cid:8)𝒎𝑘
𝒉𝑘
𝒉𝑘−1
𝑖 ← Agg𝑘
𝑖
end for
(cid:12)
ˆ𝒚𝑖 ← Dec(𝒉𝐾
(cid:12)𝝎) ∀𝑣𝑖 ∈ B𝑟
𝑖
𝐿𝑟 ← (cid:205)𝑣𝑖 ∈B𝑟
𝑙 (𝒚𝑖, ˆ𝒚𝑖 )
Update parameters by gradients of 𝐿𝑟

(cid:12)
(cid:12)𝜽𝑘 (cid:1) ∀𝑒𝑖,𝑗 ∈ B𝑟
(cid:12)
(cid:12)
(cid:12)𝝁𝑘
𝑗 ∈𝑁𝑂 (𝑖)

𝑗→𝑖

(cid:17)

(cid:16)

12:
13: end for

∀𝑣𝑖 ∈ B𝑟

are with shared-memory and their sizes must be considerably con-
strained in local machines, thus infeasible for global-batch on large
graphs. To address global-batch, practitioners usually transform its
computation to that of mini-batch. Third, sparse tensors must care-
fully maintain the node/edge correspondence to the original graph
(e.g. by some re-indexing techniques). In contrast, we introduce a
subgraph abstraction to transform these strategies into a unified
computation (refer to Section 4.2).

The third challenge is to enable deep neighborhood exploration
without neighbor sampling. To solve this challenge, with the help of
our subgraph abstraction built in the distributed graph training en-
gine, neighbourhood exploration only introduces little extra storage
overhead. Therefore, sampling is no longer a necessity for the sake
of reducing space complexity. Furthermore, our subgraph abstrac-
tion stores node/edge embedding (or activations) in place along
with the graph topology, and thus naturally leads to distributed
storage of embeddings in a cluster of machines (refer to Section 4).
Overall, our system is designed to solve the systematic and tech-
nical challenges faced by the combinatorial optimizations of diverse
factors including big or highly-skewed graphs, multiple training
strategies, deep GNN layers and not using neighbor sampling, for
both GNN training and inference. In this paper, we have conducted
extensive experiments on various networks to evaluate the per-
formance of our system. Our experimental results revealed that
compared with the state-of-the-art methods, GraphTheta yields
comparable or even better prediction results, indicating that our sys-
tem can learn GNNs as well as existing frameworks. Furthermore,
GraphTheta scales well to 1,024 CPU workers in our production
Kubernetes [50] CPU cluster, and can train an in-house developed
GNN on Alipay dataset of 1.4 billion nodes and 4.1 billion attributed
edges using three training strategies. To our knowledge, this is the
largest GNN learning task that uses edge attributes of a billion-scale
industrial network in the literature 1. Furthermore, GraphTheta
outperforms DistDGL both in terms of training speed (up to 2.02×
faster on the same machines) and scalability. Our technical contri-
butions are summarized as follows.

1PinSage [100] used a network of 3 billion nodes and 18 billion edges but did not apply
edge attributes to GNNs

3

• We present a distributed graph learning system implemented
based on vertex-centric graph processing programming model.
This system proposes a new distributed graph training engine
developed with gradient propagation support. This engine im-
plements a new hybrid-parallel execution and supports GNN
training and inference via a unified implementation. Compared
to existing data-parallel executions, the new engine can scale
up to big dense/sparse graphs and enable deep neighborhood
exploration.

• GraphTheta introduces a new GL abstraction NN-TGAR, which
enables user-friendly programming (supporting distributed train-
ing) and bridges the gap between graph processing paradigms
and deep learning computational patterns. This abstraction cen-
ters around the semantic of nodes and edges, instead of tensors
as in conventional DL frameworks, and is used to compose DNNs
by orchestrating operations on nodes and edges.

• To alleviate the redundant calculation among batches, Graph-

Theta supports a new type of training strategy, i.e. cluster-batch [14],
which performs graph convolution on a cluster of nodes and can
be taken as a generalization of mini-batch or global-batch. In
particular, for each training strategy our system scales well to
1,024 workers on the billion-scale industrial Alipay dataset in a
cluster of CPU virtual machines of small memory each.

2 PRELIMINARY
2.1 Notations
A graph can be defined as G = {V, E}, where V and E are nodes
and edges respectively. For simplicity, we define 𝑁 = |V | and
𝑀 = |E |. Each node 𝑣𝑖 ∈ V is associated with a feature vector 𝒉0
𝑖 ,
and each edge 𝑒 (𝑖, 𝑗) ∈ E has a feature vector 𝒆𝑖,𝑗 , a weight value
𝑎𝑖,𝑗 . To represent the structure of G, we use an adjacency matrix
𝑨 ∈ R𝑁 ×𝑁 , where 𝐴(𝑖, 𝑗) = 𝑎𝑖,𝑗 if there exists an edge between
nodes 𝑖 and 𝑗, otherwise 0.

2.2 Graph Neural Networks
Existing graph learning tasks are often comprised of encoders
and decoders as stated in [32]. Encoders map high-dimensional
graph information into low-dimensional embeddings. Decoders are
application-driven. The essential difference between GNNs relies
on the encoders. Encoder computation can be performed by two
established methods [107]: spectral and propagation. The spectral
method generalizes convolution operations on a two-dimensional
grid data to graph-structured data as defined in [18, 48], which uses
sparse matrix multiplications to perform graph convolutions. The
propagation method describes graph convolutions as a message
propagation operation, which is equivalent to the spectral method
(refers to Section A.1 in supplementary material for the proof).

In this paper, we present an algorithmic framework of Message
Propagation based Graph Neural Network (MPGNN) as a typical
use case to elaborate the compute pattern of NN-TGAR and our new
system GraphTheta. As shown in Algorithm 1, MPGNN can unify
existing GNN algorithms under different training strategies. Recent
work [26, 31, 83] focuses on propagating and aggregating messages
and aims to deliver a general framework for different message
propagation methods. In fact, the core difference between existing

propagation methods lies in the projection (Line 6), the message
propagation (Line 7) and the aggregation (Line 8) functions.

2.3 Cluster-batched Training
We study a new training strategy, namely cluster-batched gradient
descent, to address the challenge of redundant neighbor embedding
computation under the mini-batch strategy. This training strategy
was first used by the Cluster-GCN [14] algorithm and shows supe-
rior performance to mini-batch in some applications. This method
can maximize per-batch neighborhood sharing by taking advantage
of community detection (graph clustering) algorithms.

Cluster-batch first partitions a big graph into a set of smaller
clusters, and then generates a batch of data either based on one
cluster or a combination of multiple clusters. Similar to mini-batch,
cluster-batch also performs localized graph convolutions. However,
cluster-batch restricts the neighbors of a target node into only one
cluster, which is equivalent to conducting a full graph convolution
on a cluster of nodes. Typically, cluster-batch generates clusters by
using a community detection algorithm based on maximizing intra-
community edges and minimizing inter-community connections [5].
Note that community detection can run either beforehand or at
runtime, based on the requirements. Moreover, cluster sizes are
often irregular, resulting in varied batch sizes.

Figures 3(c) and 3(d) illustrate an example of cluster-batched
computation, where the graph is partitioned into two communities
(or clusters). The black nodes in Figure 3(c) are in community A and
the black nodes in Figure 3(d) belong to community B. The green
or blue nodes in both graphs are the 1-hop or 2-hop boundaries
of a community. To train a 2-layered GNN model, we can select
community A as the first batch and community B as the second.
In this case, to achieve more flexibility, our system allows users
to configure whether to get their 1-hop or 2-hop boundary neigh-
bors involved in the embedding computation of the black nodes in
communities A and B. By default, this feature is disabled, i.e. only
nodes in the communities participate in the computation as done in
Cluster-GCN. For readers’ information, we compare the pros and
cons of global-batch, mini-batch and cluster-batch in Table 1. These
comparisons imply the necessity to design a new GNN learning
system that enables the exploration of different training strategies
and a solution to address the limitations of existing architectures.

3 COMPUTE PATTERN
3.1 NN-TGAR
To solve the GNN learning in a hybrid-parallel fashion, we present
a general computation pattern abstraction, namely NN-TGAR, which
can perform the forward and backward computation of GNNs on
a big (sub-)graph distributively. This abstraction decomposes an
encoding layer (explained in the context of encoders and decoders
as described in Section 2.2) into a sequence of independent stages,
i.e., NN-Transform, NN-Gather, Sum, NN-Apply, and Reduce. Our
method allows distributing the computation over a cluster of ma-
chines. The NN-Transform (NN-T) stage is executed on each node to
transform the results and generate messages. The NN-Gather (NN-G)
stage is applied to each edge, where the inputs are edge values,
source node, destination node, and the messages generated in the

previous stage. After each iteration, this stage updates edge val-
ues and sends messages to the destination node (maybe on other
workers). In the Sum stage, each node accumulates the received
messages by means of a non-parameterized method like averag-
ing, concatenation or a parameterized one such as LSTM [62]. The
resulting summation is updated to the node by NN-Apply (NN-A).

Different from the vertex-centric programming abstraction Gather-

Sum-Apply-Scatter (GAS) proposed by PowerGraph [27] for con-
ventional graph processing applications, NN-T, NN-G and NN-A are
implemented as neural network functions. In forwards, the train-
able parameters also join in the calculation of the three stages,
but kept unchanged. In backwards, the gradients of these param-
eters are generated in NN-T, NN-G and NN-A stages, which is used
in the final stage NN-Reduce for parameter updating. NN-TGAR can
be executed either on the entire graph or subgraphs, subject to the
training strategy used.

3.2 Forward
The forward of a GNN model can be described as 𝐾 + 2 passes of
NN-TGA, as each encoding layer can be described as one pass of
NN-TGA. The decoder and loss functions can be separately described
as a single NN-T operation. Recall that the decoder functions are
application-driven. A decoder function can be described by a single
NN-T operation in node classification, and a combination of NN-T
and NN-G in link prediction. Without loss of generality, we use node
classification as the default task in this paper. The forward is com-
prised of 𝐾 passes of NN-TGA from the first to K-th encoding layer,
one NN-T operation for the decoder function, and one for the loss
calculation. In the forward of each encoding layer, the projection
function is executed on NN-T and the propagation function on NN-G.
However, the aggregation function is implemented by a combina-
tion of Sum and NN-A, which corresponds to the accumulate part
and the apply part defined as follows:

𝑘
𝑖 = Acc𝑘 (cid:0)(cid:8)𝒎

𝑴

(cid:9)

𝑘
𝑗→𝑖

𝑗 ∈𝑁 (𝑣𝑖 )

(cid:12)
(cid:12)𝝁

(1)
𝑘

(cid:1),

(1a)

(cid:12)
(cid:12)𝝁

(cid:1),

, 𝝁

(1b)

(1)
𝑘

(2)
𝑘

𝑘
𝑁𝑂 (𝑖)

𝑘
(cid:0)𝒉
𝑖 , 𝑴

𝑘
𝑖 = Apy𝑘
𝒉

is transformed to 𝒏𝑘

where 𝝁𝑘 = (cid:2)𝝁
ized by mean-pooling, 𝝁

(2)
𝑘
(cid:3). If the accumulate part is not parameter-
(1)
(2)
𝑘 = 0 and 𝝁𝑘 = 𝝁
𝑘 .
As shown in Figure 4(a), the embedding of the (𝑘 − 1)-th layer
𝒉𝑘−1
𝑖 using a neural network Proj𝑘 (∗|𝑾𝑘 ) for
𝑖
all the three nodes in NN-T. The NN-G stage collects messages both
from the neighboring nodes {𝑣2, 𝑣3} (and 𝑣1) and from adjacent
edges {𝑒1,2, 𝑒1,3} (and 𝑒2,1) to the centric node 𝑣1 (and 𝑣2) through
the propagation function Prop𝑘 (∗|𝜃𝑘 ). The output of this stage is
automatically accumulated by Acc𝑘 (∗), resulting in the summed
message 𝑴𝑘
𝑁 (1) and 𝑴𝑘
𝑁 (2) . The NN-A stage computes the new
2 and 𝒉𝑘
embeddings of nodes {𝑣1, 𝑣2, 𝑣3}, i.e. 𝒉𝑘
3 , as the output
of this encoding layer.

1 , 𝒉𝑘

3.3 Auto-Differentiation and Backward
Auto-differentiation is a prominent feature in deep learning and
GraphTheta also implements auto-differentiation to simplify GNN
programming by automating the backward gradient propagation
during training. Like existing deep learning training systems, a

4

(a) Nodes A and B form a batch

(b) Nodes C and D form a batch

(c) Community A forms a batch

(d) Community B forms a batch

Figure 3: An example of a two-hop node classification: (a) and (b) are of mini-batch, (c) and (d) are of cluster-batch. The black
solid points are the target nodes to generate mini-batches. The green and blue solid nodes are the one-hop and two-hop neigh-
bors of their corresponding target nodes. The hollow nodes are ignored during the embedding computation. The nodes with
circles are the shared neighbors between mini-batches.

Strategies Advantages

Disadvantages

Table 1: Comparison of three GNN training strategies.

Global-
batch

Mini-
batch

Cluster-
batch

• No redundant calculation;
• Stable training convergence.

• Friendly for parallel computing;
• Easy to implement on modern DL frameworks.

• Advantages of mini-batch but with less redundant calcula-

tion.

• The highest cost in one step.

• Redundant calculation among batches;
• Exponential complexity with depth;
• Power law graph challenge.
• Limited support for graphs without obvious community struc-

tures;

• Instable learning speed and imbalanced batch size.

𝑥 (𝑾 ) and 𝜕𝒚/𝜕𝑾 = 𝑓 ′

primitive operation has two implementations: a forward version
and a backward one. For instance, a data transformation function
can implement its forward computation by a sequence of primitive
operations. In this case, its backward computation can be automati-
cally interpreted as a reverse sequence of the backward versions
of these primitive operations. Assuming that 𝒚 = 𝑓 (𝒙, 𝑾 ) is a user-
defined function composed of a sequence of built-in operations,
GraphTheta can generate the two derivative functions automati-
cally: 𝜕𝒚/𝜕𝒙 = 𝑓 ′
𝑊 (𝒙). NN-TGAR will organize
all these derivative functions to implement the backward progress
of the whole GNN model. Refer to Section A.2 in supplementary
material for a general proof of backward computation with message
propagation as well as Section A.3 for the derivatives of MPGNN.
In the task of node classification, the backward of a GNN model
can be described as 𝐾 + 2 passes of NN-TGAR, but in a reverse order.
First, the differential of a loss function 𝜕𝐿/𝜕 ˆ𝒚𝑖 = 𝑙 ′(𝒚𝑖 ) is executed
on each labeled node by a single NN-T stage. Then, the two stages
NN-T and Reduce are used to the calculate the differential of the
decoder function. In this phase, the gradients of the final embedding
for each node are calculated as 𝜕𝐿/𝜕𝒉𝐾
𝑖 = 𝜕𝐿/𝜕 ˆ𝒚𝑖 · Dec′(𝝎) and
updated to the corresponding node. Meanwhile, the gradients of the
decoder parameters are calculated as 𝜕𝐿/𝜕𝝎 = 𝜕𝐿/𝜕 ˆ𝒚𝑖 · Dec′(𝒉𝐾
𝑖 )

and sent to the optimizer. The 𝐾 passes of NN-TGAR are executed
backwards from the K-th to the first encoding layer.

Differing from forward that runs the apply part at the last step,
backward executes the differential of the apply part on nodes
{𝑣1, 𝑣2, 𝑣3} in the NN-T stage, as shown in Figure 4(b). This stage
calculates 𝜕𝐿/𝜕𝝁𝑘 that will be sent to the optimizer, {𝜕𝐿/𝜕𝒉𝑘−1
·
𝜕Apy𝑘 /𝜕𝒏𝑘
𝑖 |𝑖 = 1, 2, 3} that will be updated to the nodes as values,
and {𝜕𝐿/𝜕𝑴𝑘
𝑖 |𝑖 = 1, 2} that will be consumed by the next stage
NN-G. Besides receiving messages from the previous stage, stage
NN-G takes as input the values of the corresponding adjacent edges
and centric nodes, and sends the result to neighbors and centric
nodes, as well as the optimizer.

𝑖

In NN-G, taking Gather1,3 as an example, the differential of the ac-
cumulate function calculates 𝜕𝐿/𝜕𝒎𝑘
3→1. Subsequently, the differen-
tial of the propagation function computes 𝜕Prop𝑘 /𝜕𝒏𝑘
3 , 𝜕Prop𝑘 /𝜕𝒏𝑘
1 ,
and 𝜕Prop𝑘 /𝜕𝜽𝑘 , all of which are multiplied by 𝜕𝐿/𝜕𝒎𝑘
3→1 and then
sent to the source node 𝑣3, destination node 𝑣1, and the optimizer.
The Sum stage receives gradient vectors computed in NN-G, as well
as new node values computed in NN-T, and then element-wisely
adds the gradients by node values for each of the three nodes, re-
sulting in {𝜕𝐿/𝜕𝒏𝑘
𝑖 |𝑖 = 1, 2, 3}. These three results will be passed to
the next stage. The NN-A computes the gradients of the (𝑘 − 1)-th

5

ABCDCommunityACommunityBCommunityBCommunityA(a) Forward from ℎ𝑘−1
𝑖 : NN-T uses the 𝑘th projection func-
tion, NN-G uses the 𝑘th propagation function and NN-A uses the
𝑘th aggregation function

to ℎ𝑘

𝑖

(b) Backward from ∇ℎ𝑘
: NN-T uses the derivation of 𝐴𝑝𝑦𝑘 ,
NN-G uses the deriv. of 𝐴𝑐𝑐𝑘 &𝑃𝑟𝑜𝑝𝑘 , NN-A uses the deriv. of 𝑃𝑟𝑜 𝑗𝑘 , and
NN-R processes gradients of parameters.

𝑖 to ∇ℎ𝑘−1

𝑖

(c) The example graph used
in (a\b), nodes may on dif-
ferent workers.

Figure 4: The computation pattern of NN-TGAR.

𝑖

· Proj′

layer embeddings 𝜕𝐿/𝜕𝒉𝑘−1
as 𝜕𝐿/𝜕𝒏𝑘
𝑘 (𝑾𝑘 ), where the gra-
𝑖
dients of 𝑾𝑘 are calculated similarly. The optimizer invokes Reduce
to aggregate all the gradients of parameters (i.e., 𝝁𝑘 , 𝜽𝑘 , and 𝑾𝑘 ),
which are generated in stages NN-T, NN-G and NN-A and distributed
over nodes/edges, and updates the parameters with this gradient
estimation.

4 IMPLEMENTATION
Inspired by distributed graph processing systems, we present a new
GNN training system which can simultaneously support all of the
three training strategies. Our new system enables deep GNN explo-
ration without pruning graphs. Moreover, our system can balance
memory footprint, time cost per epoch, and convergence speed.
Figure 5 shows the architecture of our system. It consists of five
components: (𝑖) a graph storage component with distributed par-
titioning and heterogeneous features and attributes management,
(𝑖𝑖) a subgraph generation component with sampling methods, (𝑖𝑖𝑖)
graph operators which manipulate nodes and edges, and (𝑖𝑣) learn-
ing core operations including neural network operators (including
fully-connected layer, attention layer, batch normalization, concat,
mean/attention pooling layers and etc.), typical loss functions (in-
cluding softmax cross-entropy loss and binary cross-entropy loss),
and optimizers (including SGD, Adam [47] and AdamW [63]).

4.1 Distributed Graph Representation
Our graph programming abstraction follows the vertex-program
paradigm and fits the computational pattern of GNNs which centers
around nodes. In our system, the underlying graphs are stored
distributively which require fast graph partitioning algorithms for
efficient processing. A number of graph partitioning approaches
have been proposed, such as vertex-cut [7, 27, 39], edge-cut [46,
76, 81], and hybrid-cut [11] solutions. Typical graph partitioning

6

algorithms are included in our system to support popular graph
processing methods (The effect of different partitioning methods
on performance will be elaborated in Section 5.4).

To efficiently run GNN-oriented vertex-programs, we propose
a new graph partitioning method which evenly distributes nodes
to partitions and cuts off cross-partition edges. Similar to Power-
Graph, we use master and mirror nodes, where a master node is
assigned to one partition and its mirrors are created in other parti-
tions. For each edge, our method assigns it to the partition in which
its source node is a master (target nodes also can be used as the
indicator). This way, any edge contains at least one master node.
The vertex-cut approach used by PowerGraph has the disadvan-
tage of duplicating mirror nodes, resulting in memory overhead
for the multi-layer GNN learning. To address this problem, our
method allows mirror nodes to act as placeholders and only hold
node states instead of the actual values. With this strategy, our
method not only reduces memory overhead but also lowers com-
munication overhead for two reasons. On one hand, by allowing
mirror nodes not to hold actual values, our method can significantly
reduce memory overhead. On the other hand, in the end of each
superstep, PowerGraph will synchronize all master nodes to their
mirrors. Instead, our method removes this global synchronization,
and only synchronizes the masters actually used. This means that
the communication overhead of PowerGraph is an upper bound
of ours. Moreover, our method can reduce the replica factor to 1
from (𝑁𝑚𝑎𝑠𝑡𝑒𝑟 + 𝑁𝑚𝑖𝑟𝑟𝑜𝑟 )/𝑁𝑚𝑎𝑠𝑡𝑒𝑟 , where 𝑁𝑚𝑎𝑠𝑡𝑒𝑟 and 𝑁𝑚𝑖𝑟𝑟𝑜𝑟
are the number of master and mirror nodes. With the partitioning
method, the implementation of each primitive in GAS abstraction is
composed of several phases. Figure 7 illustrates the computation of
the Gather primitive. To traverse graph efficiently, GraphTheta
organizes outgoing edges in Compressed Sparse Row (CSR) and
incoming edges in Compressed Sparse Column (CSC), and stores
node and edge values separately. Our distributed graph traversal

𝑃𝑟𝑜𝑗%(∗|𝑾𝒌)NN-T1NN-T2NN−T	3𝑣-:ℎ-%01𝑣1:𝑛1%𝑒1,5𝑃𝑟𝑜𝑝%(∗|𝜽𝒌)NN-G1,2NN-G2,1NN-G1,3Sum1Sum2𝑣1:𝑀9(1)%,ℎ1%01𝑣5:𝑛5%𝑣-:𝑛-%𝑣-:ℎ-%01𝐴𝑝𝑦%(∗|𝝁𝒌)NN-A1NN-A2NN-A3𝑣5:ℎ5%01𝑣1:ℎ1%01𝑣-:ℎ-%𝑣5:ℎ5%𝑣1:ℎ1%𝑒5,1𝑒1,-𝑣5:𝑀9(5)%,ℎ5%01𝐴𝑐𝑐%(∗)𝝁𝒌,𝐴𝑝𝑦’((∗)NN-T1NN-T2NN−T	3𝑣-:𝛻ℎ-’𝑣1,𝛻𝑀3(1)’𝑒1,5:𝑚5→1’𝑒5,1:𝑚5→1	’𝜽𝒌,𝐴𝑐𝑐’((∗),𝑃𝑟𝑜𝑝’((∗)NN-G1,2NN-G2,1NN-G1,3Sum1Sum2𝑣1:𝛻𝑛1’Sum3𝑣5,𝛻𝑀3(5)’𝑣-𝑒1,-:𝑚-→1’𝑣5:𝛻𝑛5’𝑣-:𝛻𝑛-’𝑾𝒌,𝑃𝑟𝑜𝑗’((∗)NN-A1NN-A2NN-A3∇𝜇’∇𝜃’∇𝑊’𝑣5:𝛻ℎ5’𝑣1:𝛻ℎ1’𝑣-:𝛻ℎ-’E1𝑣5:𝛻ℎ5’E1𝑣1:𝛻ℎ1’E1ComputationStageVertexanditsvaluesEdgeanditsvaluesFunction&ParametersCombine&Reduce𝑣"𝑣#𝑣$𝑒"#𝑒#"𝑒"$the mirror node 1 from node 3. Herein, the mirror node 1 receives
its value sent from worker 1 and will be passively gathered by node
3 in worker 0. Overall, we can see that communication only occurs
between master and mirror nodes.

The abstraction can address the local message bombing problem.
On one hand, for a master-mirror pair, we only need one time of
message propagation of node values and the results, which can
reduces the traffic load from 𝑂 (𝑀) to 𝑂 (𝑁 ). On the other hand,
we only synchronize node values involved in the computation per
neural network layer, during the dataflow execution of a GNN
model. Moreover, we remove the implicit synchronization phase
after the original Apply primitive, instead of introducing implicit
master-mirror synchronization to overlap computation and com-
munication. Additionally, heuristic graph partitioning algorithms,
such as METIS [45] and Louvain [5], are also supported to adapt
cluster-batched training.

4.2 Subgraph Training
To unify the processing of all the three types of training strategies,
our new system uses subgraphs as the abstraction of graph struc-
tures and the GNN operations are applied. Both mini-batch and
cluster-batch train a model on the subgraphs generated from the
initial batches of target nodes, whereas global-batch does on the
entire graph. The size of a subgraph can vary from one node to
the entire graph based on three factors, i.e., number of GNN layers,
graph topology and community detection algorithms. The number
of GNN layers determines the neighbor exploration depth and has
an exponential growth of subgraph sizes. For graph topology, node
degree determines the exponential factor of subgraph growth.

Figure 9 shows a mini-batch training example, where a GNN
model containing two graph convolution layers is interleaved by
two fully-connected layers. The right part shows a subgraph con-
structed from a batch of initial target nodes {1, 2, 3}, which has
one-hop neighbors {4, 5, 6} and two-hop neighbors {7, 8}. The left
part shows the forward and backward computation of this subgraph,
with arrows indicating the propagation direction.

For subgraph computation, a straightforward method is to load
the subgraph structure and the related data into memory and per-
form matrix/tensor operations on the subgraph located at the same
machine. As the memory overhead of a subgraph may exceed the
memory limit, this method has inherent limitation in generaliza-
tion. Instead, GraphTheta completes the training procedure with
distributed computing by only spanning the structure of the dis-
tributed subgraph.

To construct a subgraph, our system introduces a breadth-first-
search traversal operation. For each target node, this operation
initializes a minimal number of layers per node, which are involved
in the computation, in order to reduce unnecessary propagation
of graph computing. Furthermore, to avoid the cost of subgraph
structure construction and preserve graph access efficiency, we
build a vertex-ID mapping between the subgraph and the local
graph within each process/worker to reuse CSR/CSC indexing. In
addition, our system has implemented a few sampling methods,
including random neighbor sampling [31], which can be applied to
subgraph construction.

Figure 5: The system architecture of GraphTheta.

Figure 6: An example of GraphTheta partitioning with
nodes evenly assigned to three partitions. Mirrors are de-
noted by the dotted line.

Figure 7: An example of computation and communication in
the Gather primitive: compute, combine and synchronized.

Figure 8: An example of distributed graph traversal
is completed in two concurrent operations: one traverses nodes
with CSR and the other with CSC. For the operation with CSR,
each master node sends its related values to all the mirrors and
then gathers its outgoing edges with master neighbors, where the
edges with mirror neighbors are directly skipped. For the operation
with CSC, each mirror node gathers incoming edges with master
neighbors. Mirror nodes receiving values from their corresponding
masters will be passively gathered by their neighbors.

Figure 8 illustrates an example of traversing all outgoing edges of
the master node 3 in partition 0 held by worker 0. For the operation
with CSR, worker 0 processes the outgoing edge of node 3 to the
master node 0. As node 1 in worker 0 is a mirror, the edge from
node 3 to node 1 is skipped. Meanwhile, worker 1 sends the value
of its master node 1 to the mirror in worker 0. Concurrently, for
the operation with CSC, worker 0 processes the incoming edge of

7

GAS-PrimitiveDataflowSub-graphGenerationGraphPartitionTensorSched-ulerModelParserNN-FunctionOptimizerAuto-DiffNN-TGAR31212345001423541525143310151503computationcombinedmessagesynchronizedvalue01Partition 0312Partition 1mastermirroredgemessageworker 0worker 1Figure 9: The tensor stacks of a target node and its 2-hop
neighbors in the training process with a 2-hop GNN model.

Figure 10: The parallel batched training paradigm.

4.3 Parallel Execution Model
Training subgraphs sequentially cannot fully unleash the power
of a distributed system. GraphTheta is designed to concurrently
train multiple subgraphs with multi-versioned parameters in a dis-
tributed environment, and support concurrent lookups and updates,
which lead to two distinguished features from the existing GNN
training systems: (𝑖) parallel subgraph tensor storage built upon dis-
tributed graphs, and (𝑖𝑖) GraphView abstraction and multi-versioned
parameter management to enable parallelized batched training.

Parallel tensors storage. For subgraph training, we investigate
two key techniques to enable low-latency access to distributed
subgraphs and lower total memory overhead. The first technique is
reusing the CSR/CSC indexing. It is inefficient in high-concurrency
environment to construct and release the indexing data for each
subgraph on the fly. Instead, the global indexing of the whole graph
is reused, and a private cache-friendly vertex-ID mapping is adopted
to efficiently access a graph topology, as described in section 4.2.

The second technique is task-oriented tensor storage. In GNNs,
the same node can be incorporated in different subgraphs con-
structed from different batches of target nodes, especially for the
nodes of high degrees. To make tasks context-independent and hide
underlying implementation details, the memory layout of nodes is
task-specific (a task can be an individual forward, backward, or ag-
gregation phase) and sliced into frames. A frame of a given node is a
stack of consecutive resident memory, storing raw data and tensors.
To alleviate peak memory pressure, the memory can be allocated
and released dynamically per frame on the fly. More specifically,
in the forward/backward phase, output tensors for each layer is
calculated and released immediately after use. As the allocation
and de-allocation of tensors has a context-aware memory usage
pattern, we design a tensor caching between frames and standard

memory manipulation libraries to avoid frequently trapping into
operating system kernel spaces.

GraphView and multi-versioned parameters. To train multi-
ple subgraphs concurrently, we design the abstraction GraphView,
which maintains all key features of the underlying parallel graph
storage including reused indexing, embedding lookup, and the dis-
tributed graph representation. Implemented as a light-weighted
logic view of the global graph, the GraphView exposes a set of inter-
faces necessary to all training strategies, and allows to conveniently
communicate with storages. Besides the global-, mini- and cluster-
batch, other training strategies can also be implemented based on
GraphView. And training tasks with GraphViews are scheduled in
parallel. That enables to concurrently assign the separated forward,
backward and aggregation phases to a training worker. Due to var-
ied workloads of subgraphs, a work-stealing scheduling strategy is
adopted to improve load balance and efficiency.

Figure 10 depicts the parallel batched training paradigm with
graph view and multi-versioned parameter management. In the
figure, ParameterManager manages multiple versions of trainable
parameters. In a training step, workers can fetch parameters of a
specific version from ParameterManager, and use these parame-
ters within the step. For each worker, it computes on a local slice
of the target subgraph being trained, and uses a task queue (i.e.,
TaskQueue) to manage all the tasks assigned to itself and then ex-
ecute them concurrently. In the end of a training step, parameter
gradients are aggregated, and sent to ParameterManager for ver-
sion update. UpdateParam performs the actual parameter update
operations either in a synchronous or an asynchronous mode [35].
It needs to be stressed that we will only use synchronous training
for each test all throughout this paper.

5 EXPERIMENTS AND RESULTS
5.1 Experiment Setups
Datasets Information. We evaluate the performance of our sys-
tem by training node classification models using 6 datasets. As
shown in Table 2, the network sizes vary from small-, modest-, to
large-scale. The first 5 datasets including Cora, Citeseer, Pubmed,
Reddit and Aamzon are publicly available and have only node
attributes, without edge attributes or types. Cora, Citeseer and
Pubmed [75] are three citation networks with nodes representing
documents and edges indicating citation relationship between docu-
ments. In the three datasets, the attribute of a node is a bag-of-words
vector, which is sparse and high-dimensional, while the label of
the node is the category of the corresponding document. Reddit is
a post-to-post graph in which one post forms one node and two
posts are linked if they are both commented by the same user. In
Reddit, a node label presents the community a post belongs to [31].
Amazon is a co-purchasing graph, where nodes are products and
two nodes are connected if purchased together. In Amazon, node
labels represent the categories of products [36].

The last Alipay dataset is an industrial big graph, which contains
1.4 billion nodes and 4.1 billion attributed edges and is relatively
sparse with the density of about 3. In this dataset, nodes are users,
attributes are user profiles and class labels are the financial risk
levels of users. Edges are built from a series of relations among users
such as chatting, online financial cooperation, payment, and trade.

8

layer 0layer 1 (GCN)layer 2layer 3 (GCN)layer 4input target 1-hop 2-hop 1 hop2 hoptarget 12345678ParameterManagerParameters  Parameters  Parameters  AssignTaskSyncUpdateParamFetchParamTaskQueueDistributed Graph Computing EngineGraph  viewGraph  view…Training TaskTraining Task…TrainingStrategyForwardBackwardAggregateTo the best of our knowledge, Alipay is the largest edge-attributed
industry-scale graph ever used to test GNNs that use edge attributes
in their graph convolutions in the literature. In our experiments,
we equally split the data set into two parts, one for training and
the other for testing.

GNN Training Settings. We employ the node classification task
as the application and use the popular GCN [48] model and our in-
house developed GAT-E model for performance comparison. Herein,
we would like to emphasize that in these tests, our purpose is not to
conclude which DL framework or GNN implementation is superior
to others, but to demonstrate that GraphTheta is capable of learning
GNNs as well as existing frameworks (Section B in supplementary
material also gives another showcase with GAT [83] model). Note
that although there are only node classification tasks in our tests,
our system can actually support other types of tasks with moderate
changes, such as revising the decoders to accommodate specific
task and keeping graph embedding encoder part unchanged.

Different datasets are configured to have varied hidden layer
sizes. Specifically, hidden layer sizes are 16, 128 and 200 for the
three citation networks (i.e., Cora, Citeseer and PubMed), Reddit
and Amazon. Except for Amazon, all the others have their own sub-
sets for validation. Moreover, the latter enables dropout for each
layer, whereas the former does not. In terms of global-batch, we
train the model up to 1,000 epochs and select the trained model with
the highest validation accuracy to test generalization performance
for the latter. With respect to mini-batch, early stop is exerted once
stable convergence has reached for each dataset. In all tests, cross-
entropy loss is used to measure convergence and prediction results
are produced by feeding node embeddings to a Softmax layer.

For global-batch, we set 500 epochs for Reddit at maximum, and
activate early stop as long as validation accuracy reaches stable.
Meanwhile, a maximum number of 750 epochs is used for Amazon.
For mini-batch, each training step randomly chooses 1% labeled
nodes to form the initial batch for Reddit, and 0.1% for Amazon.
This results in a batch size of 1,500 for the former and 1,710 for the
latter. We train Reddit for 600 steps and Amazon for 2,750 steps with
respect to mini-batch. Note that as Reddit (and Amazon) is a dense
co-comment (and co-purchasing) network, the two-hop neighbors
of only 1% (and 0.1%) labeled nodes almost touch 80% (and 65%) of
all nodes. For cluster-batch, each training step randomly chooses
1% clusters to form the initial batch for Reddit and Amazon, where
clusters are created beforehand. In addition, cluster-batch applies
the same early stop strategy as mini-batch.

5.2 Accuracy Assessment
5.2.1 Evaluation on Public Datasets. On the public datasets, we
train a two-layer GCN model [48] using our system and compare
the performance with the state-of-the-art counterparts, including
a TensorFlow-based implementation (TF-GCN) from [48], a DGL-
based one (DGL), Cluster-GCN and two neighbor-sampling-based
ones: FastGCN and VR-GCN.

Comparison with non-sampling-based methods. First, we
compare our system with TF-GCN, DGL and Cluster-GCN to demon-
strate that our system can achieve highly competitive or superior
generalization performance on the same datasets and models, even

9

without using sampling. Table 3 shows the performance compari-
son on the three small-scale citation networks, i.e., Cora, Citeseer
and PubMed [75]. For DGL and Cluster-GCN, we resue the re-
sults presented in [86] and [14], respectively, instead of evaluating
them. This is because both of them did not completely expose
their hyper-parameters and pose challenges for result reproduction.
Both GraphTheta and TF-GCN use the same set of hyper-parameter
values, including learning rate, dropout keep probability, regular-
ization coefficient, and batch size, as proposed in [48], and train
the model 300 steps for mini-batch. From the table, global-batch
yields the best accuracy for each dataset, with an exception that its
performance is neck-by-neck with that of mini-batch on Citeseer.
Mini-batch also outperforms both DGL and TF-GCN for each case.
Due to the small sizes of the three networks, we run one worker
of 1 GB memory, in our Kubernetes CPU cluster, to train each of
them. In terms of the average time per epoch/step, our method
obtains on the Cora, Citeseer and PubMed datasets of 0.18 (0.21)
seconds, 0.85 (0.24) seconds, and 0.3 (0.27) seconds respectively
with respect to the global-batch (mini-batch) training. For Cora, its
mini-batch is slower than global-batch because of two reasons. One
is that Cora is too small and thereby the runtime per epoch/step is
very short, and the other is that in each training step, mini-batch
does some extra preparation work like neighborhood subgraph
identification whose runtime is considerable for small datasets.

Comparison with sampling-based methods. Second, we com-
pare our implementations (without sampling) with FastGCN, VR-
GCN and Cluster-GCN on the two modest-scale datasets: Reddit
and Amazon. FastGCN employs importance neighbor sampling, VR-
GCN adopts variance reduction, whereas Cluster-GCN does not use
sampling same as GraphTheta. Both FastGCN and VR-GCN train
models in a mini-batched manner. Table 4 gives the test accuracy
of each experiment. For FastGCN, VR-GCN and Cluster-GCN, we
directly use the results from their corresponding publications. Note
that the performance of FastGCN on Amazon is unavailable, and
the algorithm will not participate in the performance evaluation
with respect to Amazon.

On both datasets, global-batch yields the best accuracy, while
mini-batch performs the worst with cluster-batch in between. In
terms of Reddit, FastGCN is inferior to VR-GCN, while Cluster-
GCN is superior to VR-GCN. However, with respect to Amazon,
Cluster-GCN becomes slightly worse than VR-GCN. It is worth
mentioning that the relatively lower performance of our cluster-
batch may be caused by multiple factors, e.g. number of clusters,
cluster batching, and the quality of graph partitioning. In sum, based
on the aforementioned observations, it is reasonable to draw the
conclusion that sampling-based training methods are not always
better than non-sampling-based ones.

5.2.2 Evaluation on Alipay dataset. In this test, we use our in-
house developed GAT-E model, a graph attention network that
incorporates edge attributes to attention computation along with
node attributes. GAT-E is a simplified version of the GIPA [111]
algorithm and is trained on Alipay dataset with all of the three
training strategies.

Table 5 shows the accuracy on Alipay dataset. We run 400 epochs
for global-batch, and 3,000 steps for all the others. Cluster-batch
performs the best, while mini-batch outperforms global-batch, in

Table 2: Information of the datasets used.

Name

#Nodes

#Node attr.

#Edges

#Edge attr. Max cluster size Cluster number

Cora
Citeseer
Pubmed
Reddit
Amazon
Alipay

2,708
3,327
19,717
232,965
2,449,029
1.40 Billion

1,433
3,703
500
602
100
575

5,429
4,732
44,338
11,606,919
61,859,140
4.14 Billion

0
0
0
0
0
57

−
−
−
30,269
36,072
24 Million

−
−
−
4,300
8,500
4.9 Million

Table 3: Comparison to counterparts without sampling.

Accuracy in Test Set (%)

Dataset

GCN
w/ GB

GCN
w/ MB

GCN
On DGL

GCN
On TF

Cluster
-GCN

82.5
Cora
−
Citeseer
79.9
Pubmed
GB: global-batch, MB: mini-batch, and CB: cluster-batch

82.40
71.90
79.50

82.70
71.90
80.00

81.50
70.30
79.00

81.31
70.98
79.00

Table 4: Comparison to counterparts with sampling.

Accuracy in Test Set (%)

Dataset

GB

MB

CB

Reddit
Amazon

96.44
89.77

95.84
87.99

95.60
88.34

Fast
GCN

93.70
–

VR-
GCN

96.30
89.03

Cluster
-GCN

96.34
89.00

Table 5: Accuracy comparison on Alipay dataset.

Strategies

Performance in Test Set (%)
F1 Score AUC

Time (h)

Global-batch
Mini-batch
Cluster-batch

12.18
13.33
13.51

87.64
88.12
88.36

30
36
26

terms of both F1 score and accuracy. In terms of convergence speed
in the same distributed environment, we can observe that cluster-
batch converges the fastest. The second fastest method is global-
batch, and the third one is mini-batch. Specifically, the overall
training time of 1,024 workers is 30 hours for global-batch, 36 hours
for mini-batch, and 26 hours for cluster-batch, and the peak memory
footprint per worker is 12 GB, 5 GB and 6 GB respectively.

5.3 Scalability Assessment
5.3.1 Comparison of Different Training Strategies. We evaluate
the capability of strong scaling to big graphs of GraphTheta with
respect to the number of workers using Alipay dataset. Each worker
is equipped with one computing thread and runs in a Linux CPU
Docker. Due to the large memory footprint of Alipay dataset, we
start from 256 workers and use this performance as the baseline.
Figures. 11(a), 11(b) and 11(c) illustrate the speedup results in
the function of the number of workers for global-batch, cluster-
batch and mini-batch respectively. From the figures, we can see
that each of three training strategies can scale to 1,024 workers,
and the speedups for the forward, backward and full training steps
are consistent in terms of the scaling size for each training strategy.
This can be explained by our observation that the neural network

functions are usually compute-intensive and thus lead to better
computation and communication overlap, significantly lowering
the impact of communication. By investigating the speedups of
the three training strategies, it can be observed that global-batch
has the best scalability, followed by cluster-batch and mini-batch
in decreasing order. This is because (𝑖) global-batch has relatively
balanced workloads among workers, since all nodes in the graph
participate in the computation simultaneously, and (𝑖𝑖) cluster-batch
has better data locality among distributed machines, resulting in
less inter-machine communication than mini-batch.

In the following we will analyze the speedups and parallel effi-
ciency gained as the number of workers varies. For global-batch, by
increasing the number of workers from 256 to 512, the forward runs
1.66× faster, the backward 1.75× faster, and the full training step
1.72× faster. Furthermore, when increasing the number of workers
further to 1,024, the speedups become 2.81, 3.21 and 3.09 times
respectively. In terms of parallel efficiency, the forward achieves
83% (and 70%), backward 87% (and 80%), and full training steps 86%
(and 77%) by using 512 workers (and 1,024 workers), respectively.
In terms of cluster-batch, when moving from 256 to 512 (and
1024) workers, the speedup is 1.70 (and 1.75) for the forward, 1.74
(and 2.45) for the backward, and 1.52 (and 1.80) for the full training
step. In this case, the corresponding parallel efficiency becomes 85%
(and 44%) for the forward, 87% (and 61%) for the backward, and 76%
(and 45%) for the full training step.

With respect to mini-batch, compared to the baseline perfor-
mance at 256 workers, the speedup and parallel efficiency at 512
workers is 1.83 and 91% for the forward, 1.77 and 88% for the back-
ward, and 1.71 and 86% for the full training step, respectively. Mean-
while, the corresponding values at 1024 workers are 1.84 and 46%
for the forward, 2.85 and 71% for the backward, and 2.23 and 56%
for the full training step, respectively.

5.3.2 Comparison with DistDGL. In this section, we will compare
with DistDGL [109, 110] in terms of scalability, as well as best
performance with the same computing resources, by training GCN
model on the Reddit dataset. In this test, node sampling in the node
data loader of DistDGL is disabled for the sake of fair comparison.
Both GraphTheta and DistDGL are launched to synchronously train
four GCN models of different layers ranging from 2 to 5, with mini-
batch on a cluster of 8 virtual machines with 64 CPU cores and 300
GB RAM each.

For scalability assessment, we set to use 4 cores for each worker/trainer

and keep the overall batch size of value 24K unchanged (i.e. the
batch size per worker/trainer times the number of workers/trainers
is equal to 24K). Considering that DistDGL runs a distributed graph
storage server within each machine, the computational resources

10

(a) Global-batch

(b) Cluster-batch

(c) Mini-batch

Figure 11: Scalability of GraphTheta on Alipay for the three training strategies.

Figure 12: Scalibility of Graph-
Theta on Reddit.

of each machine has to be split among the servers and all trainers
running in the same machine. Herein, we set the number of threads
per server to be max{16, 64 − 4 × 𝑝}, where 𝑝 representing the
number of trainers per machine.

Figure 12 shows the scalability of GraphTheta, which demon-
strates consistent scalability for all models. Specifically, for each
model it scales to 64 workers with a slight performance degrada-
tion at 128 workers. Section C.1 in supplementary material shows
that DistDGL does not scale at all for each model. We analyze that
this phenomenon is caused by the increasing amount of redun-
dant computation among batches as the number of trainers grows.
Specifically, as more trainers are launched, the batch size per trainer
is reduced proportionally since the overall batch size is invariant
in our synchronous training. In this case, some neighbor nodes
shared by the input targets nodes of a large batch have to be repli-
cated within the set of small batches. These replicated nodes will be
computed multiple times between batches and thus lower training
speed and scalability. However, GraphTheta does not suffer from
this problem, because the subgraph constructed from the target
nodes are independent of the number of workers and its overall
amount of computation does not grow as more workers join. In
addition, our hybrid-parallel execution can further reduce runtime
by using more workers.

Besides scalability, we also compare with the best performance of
DistDGL gained on this cluster of virtual machines. As suggested by
one DistDGL developer, we launch only one trainer in each machine
but tune the number of threads assigned to the trainer and the
distributed server. Refer to Section C.2 in supplementary material
for more details. Figure 13 illustrates the speedup of GraphTheta
over DistDGL for each model. More specifically, the speedup is 1.09,
1.53, 2.02 and 1.81 for 2-layered, 3-layered, 4-layered and 5-layered
model, respectively.

of addressing the highly skewed node degree distribution prob-
lem, while 1D-edge partition leads to better edge locality for the
perspective of source (or destination) nodes.

In GraphTheta, we set 1D-edge partition as the default par-
titioning method, based on the following two considerations. On
one hand, both the use of edge attributes and the computation of
edge attention and embedding has been becoming more and more
popular in GNNs [8, 92, 107]. With vertex-cut, the edges of a
given master node are likely distributed onto multiple workers. In
this case, both the loading of edge attributes and the computation
of edge attention and embedding will incur considerable extra com-
munication. On the contrary, 1D-edge partition always places
a master node and all of its edges together in the same worker.
This way, we can load the edge attributes and compute the edge
attention and embedding for the master node locally, with no need
of extra communication. On the other hand, in practice we observe
that vertex-cut usually has higher peak memory footprint than
1D-edge partition. Taking the benchmark on Amazon as an ex-
ample, the peak memory of vertex-cut is about 20% larger than
that of 1D-edge partition on average. Nevertheless, users can
still specify their preferred partitioning method at the runtime.

Figures 14(a), 14(b) and 14(c) show the normalized average run-
times of the forward, the backward and the full epoch steps, re-
spectively in terms of global-batch, cluster-batch and mini-batch.
The normalization uses the corresponding runtime of 1D-edge
partition as the baseline. From the figures, vertex-cut is su-
perior to 1D-edge partition in terms of global-batch and mini-
batch, and inferior to the latter with respect to cluster-batch. Based
on these observations, we would like to make the following sug-
gestion. If there is sufficient memory in distributed machines, we
recommend users trying vertex-cut first in an optimistic manner.
Otherwise, 1D-edge partition is supposed to be in preference.

5.4 Effect of Graph Partitioning Methods
In this section, we will evaluate the effect of the following two graph
partitioning methods: vertex-cut and 1D-edge partition, on
the execution of different training strategies using Amazon dataset.
vertex-cut is a 2D-grid partitioning method, which targets to
evenly distribute edges to all workers. Given an edge, vertex-cut
determines the worker it is assigned to by computing a hash value
from the source and destination nodes of the edge. 1D-edge partition
aims to evenly distribute source (or destination) nodes to all work-
ers. Given an edge, 1D-edge partition determines its destination
worker by computing a hash value from the source (or destination)
node. In our implementation, we compute the hash value from the
source node, which is also the master node, but allow for users to
configure to use the destination. vertex-cut has the advantage

6 RELATED WORK
Existing GL systems/frameworks for GNNs are either shared-memory
or distributed-memory. They are designed for mini-batch and global-
batch trainings. Specifically, DGL [86] is a shared-memory GL sys-
tem, which implements a graph message passing method [25]. To
overcome the memory constraint when processing big graphs, Dist-
DGL [109, 110] extends DGL by introducing a graph storage server
to hold the graph distributively. However, this method does not
always work well because clients have to pull subgraphs from the
server remotely.

Pixie [19] and PinSage [100] are shared-memory frameworks
based on mini-batched training. Pixie targets real-time training and
inference, while PinSage is designed for offline applications with a
MapReduce-based inference procedure.

11

00.511.522.533.5ForwardBackwardEpochSpeedup256 workers512 workers1024 workers0.00.51.01.52.02.53.0ForwardBackwardEpochSpeedup256 workers512 workers1024 workers0.01.02.03.04.0123Speedup256 workers512 workers1024 workers0204060801001201408163264128Time per Mini-batch (s)Number of workers2-layer3-layer4-layer5-layerFigure 13: Speedups of GraphTheta
over DistDGL on Reddit.

Figure 14: Performance comparison between vertex-cut and 1D-edge partitioning
methods on Amazon for each training strategy.

(a) Global-batch

(b) Cluster-batch

(c) Mini-batch

GPNN [56] is a distributed global-batched training method spe-
cialized for node classification. NeuGraph [64] takes advantage
of multiple GPUs sharing the same host to train GNN in global-
batch. ROC [41] implements global-batched training on distributed
GPU machines, but needs each machine to load the entire graph
into memory at the runtime. DistGNN [66] targets to optimize
global-batched traning of DGL from the perspectives of shared
memory implementation and communication reduction and avoid-
ance. PipeGNN [84] introduces a pipelined feature communication
mechanism to accelerate distributed global-batched training.

AliGraph [113], ByteGNN [108] and AGL [103] are distributed
methods for the mini-batched training with data-parallel execution.
Similar to DGL, both AliGraph and ByteGNN use a graph storage
server (distributed) as the solution to big graphs, while AGL extracts
subgraphs offline beforehand with MapReduce and stores them in
disk for future use. PaGraph [57] introduces graph data loader
caching for neighbor sampling and performs mini-batched training
on multiple GPUs sharing the same host. FlexGraph [85] builds a
hierarchical dependency graph to facilitate mini-batched neighbor
aggregation, which can be cached and shared among layers for
some GNNs, and uses the distributed graph processing GRAPE [21]
to express graph-related operations.

P3 [23] combines model-parallel with data-parallel, but applies
model-parallel only to the first GCN layer and data-parallel to the
rest. However, the memory problem with big graphs and deep
GNNs can only be solved in the first layer, but remains for the
rest. Dorylus [79] adopts serverless computing [22, 44, 49] and
separates graph operations from the execution of neural network
functions. One drawback is that it still needs to load all needed
neighbors of given target nodes into shared memory. In addition,
some works propose to enhance graph learning from the perspec-
tives of FPGA [3, 55], distributed file system [82], hybrid data
dependency [87], approximate PageRank sampling [6], data effi-
ciency [106] and federated learning [30, 34, 43, 67, 71, 89–91, 112].
Our system intrinsically differs from existing methods from the
following aspects. First, our system is built from scratch based on
distributed graph processing engine with neural network operators
as user-defined functions, and introduces an NN-TGAR abstraction
to bridge the gap between conventional graph processing and deep
learning. Second, our system employs a hybrid-parallel execution,
which computes each batch by a group of workers (processes) in a
distributed manner, and flexibly addresses the challenges imposed
by the combinatorial optimizations of several factors in GNN learn-
ing, including large or highly-skewed graphs, multiple training
strategies, deep GNNs and free from neighbor sampling. Finally, it
is worth noting that although general-purpose graph processing

systems [11, 27, 28, 37, 61, 65, 88, 104, 114] are incapable of directly
supporting GNN learning, their graph processing techniques can
be taken as the infrastructure of graph learning systems.

7 CONCLUSION
In this paper, we have presented GraphTheta, a distributed and scal-
able GNN learning system, which is implemented based on graph
processing programming model and introduces a NN-TGAR abstrac-
tion to ease graph convolution implementations. Differing from
conventional data-parallel and model-parallel paradigms, Graph-
Theta adopts a hybrid parallel execution, which distributively com-
putes each batch of graph data by a group of processes/workers.
Moreover, our system supports flexible training strategies and per-
forms inference through a unified implementation with training.
These features distinguish our system from existing frameworks.
Extensive experiments show that GraphTheta can scale to 1,024
workers in distributed CPU virtual machines. Considering the af-
fordable access to CPU virtual machine clusters in public clouds
like Aliyun, Amazon Web Services, Google Cloud and Azure, our
system shows promising potential to achieve low-cost graph learn-
ing on big graphs. Moreover, our system demonstrates comparable
or better generalization than the state-of-the-art on a diverse set
of networks. Compared with DistDGL, GraphTheta yields much
better scalability and runs up to 2.02× faster in terms of best per-
formance on Reddit dataset using the same machines. In particular,
GraphTheta can scale well with good parallel efficiency on the large
edge-attributed Alipay dataset of 1.4 billion nodes and 4.1 billion
edges. Meanwhile, cluster-batch is observed capable of obtaining
the best generalization and the fastest convergence speed on Alipay.
This suggests that it is worthwhile of developing new graph learn-
ing systems to allow for exploring a diversity of training strategies
to meet different requirements of various applications.

It is worth mentioning that the internal code name of GraphTheta
is GeaLearning (Graph Extended and Accelerated Learning) and it
has been deployed in production and used by several businesses in
mobile Alipay App including Zhima Credit and risk management.
Currently, GraphTheta merely runs on CPUs and the performance
of its implementation based on fine-grained vertex programs still
has a large room for improvement. Therefore, the performance
improvement of GraphTheta in favor of GPUs is part of our future
work. Nonetheless, through this system, we have proven the feasi-
bility of developing a distributed graph learning system based on
graph processing. This characteristic enables our system to execute
graph processing and learning procedures in the same program,

12

1.091.532.021.810.000.501.001.502.002.502345Speedup over DistDGLNumber of GCN layers0.000.250.500.751.00ForwardBackwardEpochNormalized Avg Time1D-Edge PartitionVertex-Cut0.000.250.500.751.001.251.50ForwardBackwardEpochNormalized Avg Time1D-Edge PartitionVertex-Cut0.000.250.500.751.00ForwardBackwardEpochNormalized Avg Time1D-Edge PartitionVertex-Cutthereby opening up more opportunities to address graph intelli-
gence problems involving diverse graph processing techniques (e.g.
graph mining [13, 77, 78]).

ACKNOWLEDGMENTS
We thank Kefeng Deng and other members of our GeaLearn team
in Ant Group, including Wei Qin, Zhiqiang Guo, Yice Luo, Peng
Du, Yue Jin and Xiabao Wu, for their contributions to this project.
We would like to thank Yanminng Fang for building the Alipay
dataset. Here is the Data Protection Statement: 1. The data used in
this research does not involve any Personal Identifiable Informa-
tion(PII). 2. The data used in this research were all processed by data
abstraction and data encryption, and the researchers were unable
to restore the original data. 3. Sufficient data protection was carried
out during the process of experiments to prevent the data leakage
and the data was destroyed after the experiments were finished.
4. The data is only used for academic research and sampled from
the original data, therefore it does not represent any real business
situation in Ant Financial Services Group.

REFERENCES

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: A system for large-scale machine learning. In 12th {USENIX}
Symposium on Operating Systems Design and Implementation ({OSDI} 16). 265–
283.

[2] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn-
ing to Represent Programs with Graphs. In International Conference on Learning
Representations.

[3] Adam Auten, Matthew Tomei, and Rakesh Kumar. 2020. Hardware acceleration
of graph neural networks. In 2020 57th ACM/IEEE Design Automation Conference
(DAC). IEEE, 1–6.

[4] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
Daniel Tarlow. 2017. DeepCoder: Learning to Write Programs. In International
Conference on Learning Representations.

[5] Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of Statistical
Mechanics: Theory and Experiment P10008 (Oct 2008), 1–12.

[6] Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin
Blais, Benedek Rózemberczki, Michal Lukasik, and Stephan Günnemann. 2020.
Scaling graph neural networks with approximate pagerank. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 2464–2473.

[7] Ümit V Çatalyürek and Cevdet Aykanat. 1996. Decomposing irregularly sparse
matrices for parallel matrix-vector multiplication. In International Workshop on
Parallel Algorithms for Irregularly Structured Problems. Springer, 75–86.

[8] Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. 2021.
An attentive survey of attention models. ACM Transactions on Intelligent Systems
and Technology (TIST) 12, 5 (2021), 1–32.

[9] Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph con-
volutional networks via importance sampling. arXiv preprint arXiv:1801.10247
(2018).

[10] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. 2020.
Simple and deep graph convolutional networks. In International Conference on
Machine Learning. PMLR, 1725–1735.

[11] Rong Chen, Jiaxin Shi, Yanzhe Chen, Binyu Zang, Haibing Guan, and Haibo
Chen. 2019. Powerlyra: Differentiated graph computation and partitioning on
skewed graphs. ACM Transactions on Parallel Computing (TOPC) 5, 3 (2019),
1–39.

[12] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and
efficient machine learning library for heterogeneous distributed systems. arXiv
preprint arXiv:1512.01274 (2015).

[13] Xuhao Chen, Roshan Dathathri, Gurbinder Gill, and Keshav Pingali. 2020. Pan-
golin: An Efficient and Flexible Graph Mining System on CPU and GPU. Proc.
VLDB Endow. 13, 8 (apr 2020), 1190–1205.

[14] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
2019. Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph
Convolutional Networks. In Proceedings of the 25th ACM SIGKDD International

13

Conference on Knowledge Discovery and Data Mining (Anchorage, AK, USA).
Association for Computing Machinery, New York, NY, USA, 257–266.

[15] Yue Cui, Kai Zheng, Dingshan Cui, Jiandong Xie, Liwei Deng, Feiteng Huang,
and Xiaofang Zhou. 2021. METRO: A Generic Graph Neural Network Frame-
work for Multivariate Time Series Forecasting. Proc. VLDB Endow. 15, 2 (2021),
224–236.

[16] Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative embeddings of latent
variable models for structured data. In International conference on machine
learning. 2702–2711.

[17] Jeffrey Dean and Sanjay Ghemawat. 2010. MapReduce: a flexible data processing

tool. Commun. ACM 53, 1 (2010), 72–77.

[18] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo-
lutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In
Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett (Eds.). Curran Associates, Inc., 3844–
3852.

[19] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma,
Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A system for
recommending 3+ billion items to 200+ million users in real-time. In Proceedings
of the 2018 world wide web conference. 1775–1784.

[20] Wenfei Fan, Tao He, Longbin Lai, Xue Li, Yong Li, Zhao Li, Zhengping Qian,
Chao Tian, Lei Wang, Jingbo Xu, Youyang Yao, Qiang Yin, Wenyuan Yu, Jingren
Zhou, Diwen Zhu, and Rong Zhu. 2021. GraphScope: A Unified Engine for Big
Graph Processing. Proc. VLDB Endow. 14, 12 (jul 2021), 2879–2892.

[21] Wenfei Fan, Jingbo Xu, Yinghui Wu, Wenyuan Yu, and Jiaxin Jiang. 2017. GRAPE:
Parallelizing sequential graph computations. Proceedings of the VLDB Endow-
ment 10, 12 (2017), 1889–1892.

[22] Sadjad Fouladi, Riad S. Wahby, Brennan Shacklett, Karthikeyan Vasuki Bal-
asubramaniam, William Zeng, Rahul Bhalerao, Anirudh Sivaraman, George
Porter, and Keith Winstein. 2017. Encoding, Fast and Slow: Low-Latency Video
Processing Using Thousands of Tiny Threads. In Proceedings of the 14th USENIX
Conference on Networked Systems Design and Implementation. 363–376.
[23] Swapnil Gandhi and Anand Padmanabha Iyer. 2021. P3: Distributed deep graph
learning at scale. In 15th {USENIX} Symposium on Operating Systems Design
and Implementation ({OSDI} 21). 551–568.

[24] Yang Gao, Peng Zhang, Zhao Li, Chuan Zhou, Yongchao Liu, and Yue Hu. 2021.
Heterogeneous Graph Neural Architecture Search. In 2021 IEEE International
Conference on Data Mining (ICDM). 1066–1071.

[25] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70. JMLR. org,
1263–1272.

[26] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In Proceedings
of the 34th International Conference on Machine Learning - Volume 70 (Sydney,
NSW, Australia) (ICML’17). JMLR.org, 1263–1272.

[27] Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, and Carlos
Guestrin. 2012. Powergraph: Distributed graph-parallel computation on natural
graphs. In Presented as part of the 10th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 12). 17–30.

[28] Joseph E Gonzalez, Reynold S Xin, Ankur Dave, Daniel Crankshaw, Michael J
Franklin, and Ion Stoica. 2014. Graphx: Graph processing in a distributed
dataflow framework. In 11th {USENIX} Symposium on Operating Systems Design
and Implementation ({OSDI} 14). 599–613.

[29] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model
for learning in graph domains. In Proceedings. 2005 IEEE International Joint
Conference on Neural Networks, 2005., Vol. 2. IEEE, 729–734.

[30] Zeli Guan, Yawen Li, Zhe Xue, Yuxin Liu, Hongrui Gao, and Yingxia Shao. 2021.
Federated Graph Neural Network for Cross-graph Node Classification. In 2021
IEEE 7th International Conference on Cloud Computing and Intelligent Systems
(CCIS). IEEE, 418–422.

[31] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Advances in neural information processing systems.
1024–1034.

[32] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning
on graphs: Methods and applications. IEEE Data Engineering Bulletin 40, 3 (2017),
52–74.

[33] David K. Hammond, Pierre Vandergheynst, and Rémi Gribonval. 2011. Wavelets
on graphs via spectral graph theory. Applied and Computational Harmonic
Analysis 30, 2 (2011), 129 – 150.

[34] Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie,
Lichao Sun, Lifang He, Liangwei Yang, S Yu Philip, Yu Rong, et al. 2021. Fed-
graphnn: A federated learning benchmark system for graph neural networks.
(2021).

[35] Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B
Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing. 2013. More effective
distributed ml via a stale synchronous parallel parameter server. Advances in
neural information processing systems 26 (2013).

[36] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure
Leskovec. 2021. OGB-LSC: A Large-Scale Challenge for Machine Learning on
Graphs. In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=
qkcLxoC52kL

[37] Chengying Huan, Hang Liu, Mengxing Liu, Yongchao Liu, Changhua He, Kang
Chen, Jinlei Jiang, Yongwei Wu, and Shuaiwen Leon Song. 2022. TeGraph: A
Novel General-Purpose Temporal Graph Computing Engine. In 2022 IEEE 38th
International Conference on Data Engineering (ICDE). IEEE, 578–592.

[38] Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. 2018. Adaptive
sampling towards fast graph representation learning. In Advances in neural
information processing systems. 4558–4567.

[39] Nilesh Jain, Guangdeng Liao, and Theodore L Willke. 2013. Graphbuilder:
scalable graph etl framework. In First international workshop on graph data
management experiences and systems. 1–6.

[40] Yugang Ji, Mingyang Yin, Hongxia Yang, Jingren Zhou, Vincent W Zheng, Chuan
Shi, and Yuan Fang. 2020. Accelerating Large-Scale Heterogeneous Interaction
Graph Embedding Learning via Importance Sampling. ACM Transactions on
Knowledge Discovery from Data (TKDD) 15, 1 (2020), 1–23.

[41] Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex Aiken. 2020. Im-
proving the accuracy, scalability, and performance of graph neural networks
with roc. Proceedings of Machine Learning and Systems 2 (2020), 187–198.
[42] Zhihao Jia, Sina Lin, Rex Ying, Jiaxuan You, Jure Leskovec, and Alex Aiken. 2020.
Redundancy-Free Computation for Graph Neural Networks. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 997–1005.

[43] Meng Jiang, Taeho Jung, Ryan Karl, and Tong Zhao. 2022. Federated Dynamic
Graph Neural Networks with Secure Aggregation for Video-based Distributed
Surveillance. ACM Transactions on Intelligent Systems and Technology (TIST) 13,
4 (2022), 1–23.

[44] Eric Jonas, Qifan Pu, Shivaram Venkataraman, Ion Stoica, and Benjamin Recht.
2017. Occupy the Cloud: Distributed Computing for the 99%. In Proceedings of
the 2017 Symposium on Cloud Computing. 445–451.

[45] George Karypis and Vipin Kumar. 1998. A Fast and High Quality Multilevel
Scheme for Partitioning Irregular Graphs. SIAM J. Scientific Computing 20, 1
(1998), 359–392.

[46] George Karypis and Vipin Kumar. 1999. Parallel multilevel series k-way parti-
tioning scheme for irregular graphs. Siam Review 41, 2 (1999), 278–300.
[47] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-

mization. In International Conference on Learning Representations.

[48] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with

graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).

[49] Ana Klimovic, Yawen Wang, Patrick Stuedi, Animesh Trivedi, Jonas Pfefferle,
and Christos Kozyrakis. 2018. Pocket: Elastic Ephemeral Storage for Serverless
Analytics. In Proceedings of the 13th USENIX Conference on Operating Systems
Design and Implementation. 427–444.
[50] Kubernetes. 2022. https://kubernetes.io.
[51] Kai Lei, Meng Qin, Bo Bai, Gong Zhang, and Min Yang. 2019. GCN-GAN: A
non-linear temporal link prediction model for weighted dynamic networks.
In IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE,
388–396.

[52] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. 2021. Train-
ing graph neural networks with 1000 layers. In International conference on
machine learning. PMLR, 6437–6449.

[53] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. 2019. Deepgcns:
Can gcns go as deep as cnns?. In Proceedings of the IEEE/CVF international
conference on computer vision. 9267–9276.

[54] Houyi Li, Zhihong Chen, Chenliang Li, Rong Xiao, Hongbo Deng, Peng Zhang,
Yongchao Liu, and Haihong Tang. 2021. Path-Based Deep Network for Candidate
Item Matching in Recommenders. In Proceedings of the 44th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Virtual
Event, Canada) (SIGIR ’21). Association for Computing Machinery, 1493–1502.
[55] Shuangchen Li, Dimin Niu, Yuhao Wang, Wei Han, Zhe Zhang, Tianchan Guan,
Yijin Guan, Heng Liu, Linyong Huang, Zhaoyang Du, et al. 2022. Hyperscale
FPGA-as-a-service architecture for large-scale distributed graph neural net-
work. In Proceedings of the 49th Annual International Symposium on Computer
Architecture. 946–961.

[56] Renjie Liao, Marc Brockschmidt, Daniel Tarlow, Alexander L Gaunt, Raquel
Urtasun, and Richard Zemel. 2018. Graph partition neural networks for semi-
supervised classification. In Workshop of International Conference on Learning
Representations.

[57] Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong Xu. 2020. PaGraph:
Scaling GNN Training on Large Graphs via Computation-Aware Caching. In Pro-
ceedings of the 11th ACM Symposium on Cloud Computing (SoCC ’20). 401–415.
[58] Husong Liu, Shengliang Lu, Xinyu Chen, and Bingsheng He. 2020. G3: When
graph Neural Networks Meet Parallel graph Processing Systems on GPUs. Proc.
VLDB Endow. 13, 12 (2020), 2813–2816.

14

[59] Qi Liu, Maximilian Nickel, and Douwe Kiela. 2019. Hyperbolic Graph Neural Net-
works. In Proceedings of the 33rd International Conference on Neural Information
Processing Systems. 8230–8241.

[60] Shangqing Liu. 2020. A Unified Framework to Learn Program Semantics with
Graph Neural Networks. In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering. 1364–1366.

[61] Yongchao Liu, Bertil Schmidt, and Douglas L. Maskell. 2011. Parallelized short
read assembly of large genomes using de Bruijn graphs. BMC Bioinformatics 12,
1 (2011), 354.

[62] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan
Qi. 2019. Geniepath: Graph neural networks with adaptive receptive paths. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4424–4431.
[63] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
In International Conference on Learning Representations. https://openreview.
net/forum?id=Bkg6RiCqY7

[64] Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, and
Yafei Dai. 2019. Neugraph: parallel deep neural network computation on large
graphs. In 2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19).
443–458.

[65] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan
Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system for large-
scale graph processing. In Proceedings of the 2010 ACM SIGMOD International
Conference on Management of data. 135–146.

[66] Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty, Evangelos
Georganas, Alexander Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and
Sasikanth Avancha. 2021. Distgnn: Scalable distributed training for large-scale
graph neural networks. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis. 1–14.

[67] Guangxu Mei, Ziyu Guo, Shijun Liu, and Li Pan. 2019. Sgnn: A graph neural
network based federated learning approach by hiding structure. In 2019 IEEE
International Conference on Big Data (Big Data). IEEE, 2560–2568.

[68] Seung Won Min, Kun Wu, Sitao Huang, Mert Hidayetoğlu, Jinjun Xiong, Eiman
Ebrahimi, Deming Chen, and Wen-mei Hwu. 2021. Large Graph Convolutional
Network Training with GPU-Oriented Data Communication Architecture. Proc.
VLDB Endow. 14, 11 (2021), 2087–2100.

[69] Jason Mohoney, Roger Waleffe, Henry Xu, Theodoros Rekatsinas, and Shivaram
Venkataraman. 2021. Marius: Learning Massive Graph Embeddings on a Sin-
gle Machine. In 15th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 21). 533–549.

[70] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019. PyTorch: An imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems. 8024–8035.

[71] Yang Pei, Renxin Mao, Yang Liu, Chaoran Chen, Shifeng Xu, Feng Qiang, and
Blue Elephant Tech. 2021. Decentralized federated graph neural networks. In
International Workshop on Federated and Transfer Learning for Data Sparsity and
Confidentiality in Conjunction with IJCAI.

[72] Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. 2019. Gated Graph Convo-
lutional Recurrent Neural Networks. In 2019 27th European Signal Processing
Conference (EUSIPCO). 1–5.

[73] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. 2008. The graph neural network model. IEEE Transactions
on Neural Networks 20, 1 (2008), 61–80.

[74] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. 2009. The Graph Neural Network Model. IEEE Transactions
on Neural Networks 20, 1 (2009), 61–80.

[75] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and
Tina Eliassi-Rad. 2008. Collective Classification in Network Data. AI Magazine
29, 3 (2008), 93–106.

[76] Isabelle Stanton and Gabriel Kliot. 2012. Streaming Graph Partitioning for
Large Distributed Graphs. In Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (Beijing, China) (KDD ’12).
Association for Computing Machinery, 1222–1230.

[77] N. Talukder and M. J. Zaki. 2016. A Distributed Approach for Graph Mining in

Massive Networks. Data Min. Knowl. Discov. 30, 5 (2016), 1024–1052.

[78] Carlos H. C. Teixeira, Alexandre J. Fonseca, Marco Serafini, Georgos Siganos,
Mohammed J. Zaki, and Ashraf Aboulnaga. 2015. Arabesque: A System for
Distributed Graph Mining. In Proceedings of the 25th Symposium on Operating
Systems Principles. 425–440.

[79] John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao
Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, et al. 2021. Dorylus:
Affordable, Scalable, and Accurate {GNN} Training with Distributed {CPU}
Servers and Serverless Threads. In 15th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 21). 495–514.

[80] Alok Tripathy, Katherine Yelick, and Aydın Buluç. 2020. Reducing communi-
cation in graph neural network training. In SC20: International Conference for
High Performance Computing, Networking, Storage and Analysis. IEEE, 1–14.

[102] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, Ion
Stoica, et al. 2010. Spark: Cluster computing with working sets. HotCloud 10,
10-10 (2010), 95.

[103] Dalong Zhang, Xin Huang, Ziqi Liu, Zhiyang Hu, Xianzheng Song, Zhibang
Ge, Zhiqiang Zhang, Lin Wang, Jun Zhou, and Yuan Qi. 2020. AGL: a Scalable
System for Industrial-purpose Graph Machine Learning. Proceedings of the
VLDB Endowment 13, 12 (2020).

[104] Heng Zhang, Lingda Li, Hang Liu, Donglin Zhuang, Rui Liu, Chengying Huan,
Shuang Song, Dingwen Tao, Yongchao Liu, Charles He, Yanjun Wu, and Shuai-
wen Leon Song. 2022. Bring Orders into Uncertainty: Enabling Efficient Uncer-
tain Graph Processing via Novel Path Sampling on Multi-Accelerator Systems.
In Proceedings of the 36th ACM International Conference on Supercomputing.
Article 11, 14 pages.

[105] Muhan Zhang and Yixin Chen. 2018. Link Prediction Based on Graph Neu-
ral Networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems. 5171–5181.

[106] Wentao Zhang, Zhi Yang, Yexin Wang, Yu Shen, Yang Li, Liang Wang, and Bin
Cui. 2021. GRAIN: Improving Data Efficiency of Graph Neural Networks via
Diversified Influence Maximization. Proc. VLDB Endow. 14, 11 (2021), 2473–2482.
[107] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep learning on graphs: A

survey. IEEE Transactions on Knowledge and Data Engineering (2020).

[108] Chenguang Zheng, Hongzhi Chen, Yuxuan Cheng, Zhezheng Song, Yifan Wu,
Changji Li, James Cheng, Hao Yang, and Shuai Zhang. 2022. ByteGNN: Efficient
Graph Neural Network Training at Large Scale. Proc. VLDB Endow. 15, 6 (2022),
1228–1242.

[109] Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan
Gan, Zheng Zhang, and George Karypis. 2020. Distdgl: distributed graph neural
network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on
Irregular Applications: Architectures and Algorithms (IA3). IEEE, 36–44.
[110] Da Zheng, Xiang Song, Chengru Yang, Dominique LaSalle, Qidong Su, Minjie
Wang, Chao Ma, and George Karypis. 2021. Distributed Hybrid CPU and GPU
training for Graph Neural Networks on Billion-Scale Graphs. arXiv preprint
arXiv:2112.15345 (2021).

[111] Qinkai Zheng, Houyi Li, Peng Zhang, Zhixiong Yang, Guowei Zhang, Xin-
tan Zeng, and Yongchao Liu. 2021. GIPA: General Information Propagation
Algorithm for Graph Learning. arXiv preprint arXiv:2105.06035 (2021).
[112] Jun Zhou, Chaochao Chen, Longfei Zheng, Huiwen Wu, Jia Wu, Xiaolin Zheng,
Bingzhe Wu, Ziqi Liu, and Li Wang. 2020. Vertically federated graph neural net-
work for privacy-preserving node classification. arXiv preprint arXiv:2005.11903
(2020).

[113] Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong
Li, and Jingren Zhou. 2019. AliGraph: a comprehensive graph neural network
platform. Proceedings of the VLDB Endowment 12, 12 (2019), 2094–2105.
[114] Xiaowei Zhu, Wenguang Chen, Weimin Zheng, and Xiaosong Ma. 2016. Gemini:
A computation-centric distributed graph processing system. In 12th {USENIX}
Symposium on Operating Systems Design and Implementation ({OSDI} 16). 301–
316.

[81] Charalampos Tsourakakis, Christos Gkantsidis, Bozidar Radunovic, and Milan
Vojnovic. 2014. FENNEL: Streaming Graph Partitioning for Massive Scale Graphs.
In Proceedings of the 7th ACM International Conference on Web Search and Data
Mining (New York, New York, USA) (WSDM ’14). Association for Computing
Machinery, 333–342.

[82] Duong Thi Thu Van, Muhammad Numan Khan, Tariq Habib Afridi, Irfan Ullah,
Aftab Alam, and Young-Koo Lee. 2022. GDLL: A Scalable and Share Nothing
Architecture Based Distributed Graph Neural Networks Framework. IEEE Access
10 (2022), 21684–21700.

[83] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2017. Graph Attention Networks. ArXiv abs/1710.10903
(2017).

[84] Cheng Wan, Youjie Li, Cameron R Wolfe, Anastasios Kyrillidis, Nam Sung
Kim, and Yingyan Lin. 2022. PipeGCN: Efficient full-graph training of graph
convolutional networks with pipelined feature communication. arXiv preprint
arXiv:2203.10428 (2022).

[85] Lei Wang, Qiang Yin, Chao Tian, Jianbang Yang, Rong Chen, Wenyuan Yu,
Zihang Yao, and Jingren Zhou. 2021. FlexGraph: a flexible and efficient dis-
tributed framework for GNN training. In Proceedings of the Sixteenth European
Conference on Computer Systems. 67–82.

[86] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li,
Jinjing Zhou, Qi Huang, Chao Ma, et al. 2019. Deep graph library: Towards effi-
cient and scalable deep learning on graphs. In Seventh International Conference
on Learning Representations.

[87] Qiange Wang, Yanfeng Zhang, Hao Wang, Chaoyi Chen, Xiaodong Zhang, and
Ge Yu. 2022. NeutronStar: Distributed GNN Training with Hybrid Dependency
Management. In Proceedings of the 2022 International Conference on Management
of Data. 1301–1315.

[88] Yangzihao Wang, Andrew Davidson, Yuechao Pan, Yuduo Wu, Andy Riffel, and
John D. Owens. 2016. Gunrock: A High-Performance Graph Processing Library
on the GPU. In Proceedings of the 21st ACM SIGPLAN Symposium on Principles
and Practice of Parallel Programming. Article 11, 12 pages.

[89] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022. FederatedScope-GNN: Towards a Unified, Comprehensive
and Efficient Package for Federated Graph Learning. (2022), 4110–4120.
[90] Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. 2021.
Fedgnn: Federated graph neural network for privacy-preserving recommenda-
tion. arXiv preprint arXiv:2102.04925 (2021).

[91] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Tao Qi, Yongfeng Huang, and Xing
Xie. 2022. A federated graph neural network framework for privacy-preserving
personalization. Nature Communications 13, 1 (2022), 1–10.

[92] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4–24.
[93] Yu Xie, Yanfeng Liang, Maoguo Gong, A. K. Qin, Yew-Soon Ong, and Tiantian
He. 2022. Semisupervised Graph Neural Networks for Graph Classification.
IEEE Transactions on Cybernetics (2022), 1–14.

[94] Jingbo Xu, Zhanning Bai, Wenfei Fan, Longbin Lai, Xue Li, Zhao Li, Zhengping
Qian, Lei Wang, Yanyan Wang, Wenyuan Yu, and Jingren Zhou. 2021. Graph-
Scope: A One-Stop Large Graph Processing System. Proc. VLDB Endow. 14, 12
(jul 2021), 2703–2706.

[95] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Pow-
erful are Graph Neural Networks?. In International Conference on Learning
Representations.

[96] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International Conference on Machine
Learning. PMLR, 5453–5462.

[97] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. 2021. Self-
supervised graph-level representation learning with local and global structure.
In International Conference on Machine Learning. PMLR, 11548–11558.

[98] Gang Yang, Xiaofeng Zhang, and Yueping Li. 2020. Session-Based Recommen-
dation with Graph Neural Networks for Repeat Consumption. In Proceedings
of the 2020 9th International Conference on Computing and Pattern Recognition.
519–524.

[99] Jianbang Yang, Dahai Tang, Xiaoniu Song, Lei Wang, Qiang Yin, Rong Chen,
Wenyuan Yu, and Jingren Zhou. 2022. GNNLab: A Factored System for Sample-
Based GNN Training over GPUs. In Proceedings of the Seventeenth European
Conference on Computer Systems. 417–434.

[100] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 974–983.

[101] Minji Yoon, Théophile Gervet, Baoxu Shi, Sufeng Niu, Qi He, and Jaewon Yang.
2021. Performance-Adaptive Sampling Strategy Towards Fast and Accurate
Graph Neural Networks. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 2046–2056.

15

A THEORETICAL ANALYSIS
A.1 Equivalence Relation
Indeed, the propagation on graphs and sparse matrix multiplication
are equivalent in the foward. The general convolutional operation
on graph G can be defined [48][18] as,

𝒙 ∗G K = 𝑼 ((𝑼

𝑇

𝒙) ⊙ (𝑼

𝑇 K)),

(2)

where 𝑈 = [𝑢0, 𝑢1, ..., 𝑢𝑁 −1] ∈ RN×N is the complete set of or-
thonormal eigenvectors of normalized graph Laplacian 𝑳, ⊙ is the
element-wise Hadamard product, 𝒙 ∈ RN×1 is the 1-dimension
signal on graph G and K is the convolutional kernel on the graph.
The graph Laplacian defined as 𝑳 = 𝑰𝑁 − 𝑫 (−1/2) 𝑨𝑫 (−1/2) ), where
𝑫 = 𝑑𝑖𝑎𝑔{𝑑1, 𝑑2, ..., 𝑑𝑁 } and 𝑑𝑖 = (cid:205)𝑁
𝐴(𝑖, 𝑗). Equation (2) can be
𝑗=0
approximated by a truncated expansion with 𝐾-order Chebyshev
polynomials as follows (more details are given in [33][48]).

𝒙 ∗G K ≈

𝐾
∑︁

𝑘=0

𝜃𝑘𝑇𝑘 ( ˆ𝑳)𝒙

=𝜃0𝑇0 ( ˆ𝑳)𝒙 + 𝜃1𝑇1 ( ˆ𝑳)𝒙 + · · · + 𝜃𝐾𝑇𝐾 ( ˆ𝑳)𝒙,

where the Chebyshev polynomials are recursively defined by,

𝑇𝑘 ( ˆ𝑳) = 2 ˆ𝑳𝑇𝑘−1 ( ˆ𝑳) − 𝑇𝑘−2 ( ˆ𝑳)
with 𝑇0 ( ˆ𝑳) = 𝑰𝑁 , 𝑇1 ( ˆ𝑳) = ˆ𝑳,

(3)

(4a)

(4b)

where ˆ𝑳 = 2𝑳/𝜆𝑀 − 𝑰𝑁 , 𝜆𝑀 is the the largest eigenvalue of 𝑳, and
𝜃0, 𝜃1, · · · , 𝜃𝐾 are the Chebyshev coefficients, which parameterize
the convolutional kernel K. Substituting (4a) into (3), and consid-
ering 𝜆𝑀 as a learnable parameters, the truncated expansion can
be rewritten as,

𝐾
∑︁

𝑘=0

𝑘

𝜂𝑘 𝑳

𝒙 = 𝜂0𝑰𝑁 𝒙 + 𝜂1𝑳𝒙 + · · · + 𝜂𝐾 𝑳

𝐾

𝒙,

(5)

where 𝜂0, 𝜂1, · · · , 𝜂𝐾 are learnable parameters. This polynomials
can also be written in a recursive form of

𝐾
∑︁

𝑘

𝜂𝑘 𝑳

𝒙 =

𝐾
∑︁

𝑘𝑇 ′
𝜂 ′

𝑘 (𝒙)

𝑘=0
with 𝑇 ′
and 𝜂 ′

𝑘=0
𝑇 ′
𝑘 (𝒙) = 𝑳𝜂 ′
𝑘−1
𝑘−1
𝑘 = 𝜂𝑘 /𝜂𝑘−1, 𝜂 ′
0 = 𝜂0.

(𝒙), 𝑇 ′

0 (𝑥) = 𝑥

(6a)

(6b)

(6c)

, 𝜂 ′
1

, · · · , 𝜂 ′

and 𝜂 ′
𝐾 are also considered as a series of learnable pa-
0
rameters. We generalize it to high-dimensional signal 𝑿 ∈ RN×dI
as follows: each item in (6a) can be written as 𝑯𝑘 = 𝑇 ′
𝑘 (𝑿 )𝑾𝑘 .
Recalling (6b), the convolutional operation on graph with high-
dimensional signal can be simplified as 𝐾-order polynomials and
defined as,

𝒙 ∗G K ≈

𝐾
∑︁

𝑘=0

𝑯𝑘, with 𝑯𝑘 = 𝑳𝑯𝑘−1𝑾𝑘, 𝑯0 = 𝑿𝑾0,

(7)

16

where 𝑯𝑘 ∈ RN×d, and 𝑾𝑘 ∈ Rd×d, which are also learnable and
parameterize the convolutional kernel. Based on the matrix multi-
plication rule, the 𝑖-th line 𝒉𝑘
𝑁
∑︁

𝑖 in 𝑯𝑘 can be written as,

𝑘
𝑖 =
𝒉

𝑳(𝑖, 𝑗)(𝒉

𝑘−1
𝑗 𝑾𝑘 ).

(8)

𝑗=1

𝑖 = 𝒉𝑘−1

𝑖 , the aggregation function is 𝒉𝑘
𝑗

Thus, we can translate convolutional operation into 𝐾 rounds of
propagation and aggregation on graphs. Specifically, at round 𝑘, the
projection function is 𝒏𝑘
𝑖 𝑾𝑘 , the propagation function is
𝒎𝑘
𝑗→𝑖 = 𝑳(𝑖, 𝑗)𝒏𝑘
𝑗→𝑖 .
In other words, 𝒉𝑘−1
𝑖 ) is the 𝑗-th line (or 𝑖-th line) of 𝑯𝑘 (or
𝑯𝑘−1) and also the output embedding of 𝑣𝑖 (or the input embed-
ding of 𝑣 𝑗 ). Each node propagates its message 𝒏𝑘
𝑖 to its neighbors,
and also aggregates the received messages sent from neighbors by
summing up the values weighted by the corresponding Laplacian
weights.

𝑖 = (cid:205)𝑗 ∈𝑁 (𝑖) 𝒎𝑘

(or 𝒉𝑘

A.2 Backwards of GNN
A general GNN model can be abstracted into 𝐾 combinations of
individual stage and conjunction stage. Each node on the graph can
be treated as a data node and transformed separately in a separated
stage, which can be written as,

𝑘
𝑖 = 𝑓𝑘 (𝒉
𝒏

𝑘−1
𝑖

|𝑾𝑘 ).

(9)

𝑘
, 𝐴(𝑖, 2)𝒏
2

But the conjunction stage is related to the node itself and its neigh-
bors, without loss of generality, it can be written as,
𝑘
𝑘
𝑘
𝑁 |𝝁𝑘 ),
, · · · , 𝐴(𝑖, 𝑁 )𝒏
𝑖 = 𝑔𝑘 (𝐴(𝑖, 1)𝒏
𝒉
1

(10)
where 𝐴(𝑖, 𝑗) is the element of adjcency matrix and is equal to
the weight of 𝑒𝑖,𝑗 (refer to the first paragraph in 2). The forward
formula (10) can be implemented by message passing, as 𝑛𝑘
is
𝑗
propagated from 𝑣 𝑗 along the edges like 𝑒𝑖,𝑗 to its neighbor 𝑣𝑖 . The
final summarized loss is related to the final embedding of all the
nodes, so can be written as,
𝐾
𝐿 = 𝑙 (𝒉
0

𝐾
, 𝒉
1
According to the multi-variable chain rule, the derivative of previ-
ous embeddings of a certain node is,
𝜕𝒉𝑘
𝜕𝒉𝑘
2
1
𝜕𝒏𝑘
𝜕𝒏𝑘
𝑖
𝑖

𝐾
𝑁 ).
, · · · , 𝒉

𝜕𝒉𝑘
𝑁
𝜕𝒏𝑘
𝑖

𝜕𝐿
𝜕𝒏𝑘
𝑖

𝜕𝐿
𝜕𝒉𝑘
𝑁

+ · · · +

𝑎𝑁 ,𝑖

𝑎2,𝑖

𝑎1,𝑖

(11)

(12)

=

+

𝜕𝐿
𝜕𝒉𝑘
1

𝜕𝐿
𝜕𝒉𝑘
2

.

Thus, (12) can be rewritten as,

𝜕𝐿
𝜕𝒏𝑘
𝑖

=

∑︁

𝑎 𝑗,𝑖

𝑗 ∈𝑁𝑖𝑛 (𝑖)

𝜕𝐿
𝜕𝒉𝑘
𝑗

𝜕𝒉𝑘
𝑗
𝜕𝒏𝑘
𝑖

(13)

𝑗 /𝜕𝒏𝑘

So the backwards of a conjunction stage also can be calculated
by a message passing, where each node (i) broadcasts the current
gradient 𝜕𝐿/𝜕𝒉𝑘
𝑗 to its neighbors along edges 𝑒 𝑗,𝑖 ; (ii) calculates
the differential 𝜕𝒉𝑘
𝑖 on each edge 𝑒 𝑗,𝑖 , multiplies the received
gradient and edge weight 𝑎 𝑗,𝑖 , and sends the results (vectors) to the
destination node 𝑣𝑖 ; (iii) sums up the received derivative vectors, and
obtains the gradient 𝜕𝐿/𝜕𝒏𝑘
𝑖 . Meanwhile, the forward and backward
of each stage is similar to the normal neural network. The above
derivation can expand to the edge-attributed graph.

A.3 Derivation in MPGNN
The previous section gives the derivatives for a general GNN model,
and this part describes the derivation for the MPGNN framework.
As in Algorithm 1, a MPGNN framework contains 𝐾 passes’ proce-
dure of "projection-propagation-aggregation", where the projection
function (Line 6) can be considered as the implementation of an
individual stage, while the propagation function (Line 7) and ag-
gregation function for the conjunction stage. Aggregation function
adopts the combination of (1a) and (1b). If the gradients of the 𝑘-th
layer node embeddings 𝜕𝐿/𝜕𝒉𝑘
𝑖 are given, the gradients of (𝑘 − 1)-
th layer node embeddings and the corresponding parameters are
computed as,

𝜕𝐿

𝜕𝝁

(2)
𝑘

=

𝑁
∑︁

𝑖=1

𝜕𝐿
𝜕𝒉𝑘
𝑖

𝜕𝒉𝑘
𝑖
(2)
𝜕𝝁
𝑘

,

𝜕𝐿

𝜕𝝁

(1)
𝑘

=

𝑁
∑︁

𝑖=1

𝜕𝐿
𝜕𝒉𝑘
𝑖

𝜕𝒉𝑘
𝑖
𝜕𝑴𝑘
𝑖

𝜕𝑴𝑘
𝑖
(1)
𝜕𝝁
𝑘

,

𝜕𝐿
𝜕𝒎𝑘

𝑗→𝑖

=

𝜕𝐿
𝜕𝒉𝑘
𝑖

𝜕𝒉𝑘
𝑖
𝜕𝑴𝑘
𝑖

𝜕𝑴𝑘
𝑖
𝜕𝒎𝑘

𝑗→𝑖

,

𝜕𝐿
𝜕𝜽𝑘

𝑁
∑︁

=

∑︁

𝑖=1

𝑗 ∈𝑁𝑂 (𝑖)

𝜕𝐿
𝜕𝒎𝑘

𝑗→𝑖

𝑗→𝑖

𝜕𝒎𝑘
𝜕𝜃𝑘

,

𝜕𝐿
𝜕𝒏𝑘
𝑖

=

∑︁

𝑗 ∈𝑁𝑂 (𝑖)

𝜕𝐿
𝜕𝒎𝑘

𝑗→𝑖

∑︁

𝑗 ∈𝑁𝐼 (𝑖)

𝜕𝐿
𝜕𝒎𝑘

𝑖→𝑗

𝜕𝒎𝑘
𝑗→𝑖
𝜕𝒏𝑘
𝑖
𝜕𝒎𝑘
𝑖→𝑗
𝜕𝒏𝑘
𝑖

+

+

𝜕𝐿
𝜕𝒉𝑘
𝑖

𝜕Apy𝑘
𝜕𝒏𝑘
𝑖

,

𝜕𝐿
𝜕𝑾𝑘

=

𝑁
∑︁

𝑖=1

𝜕𝐿
𝜕𝒏𝑘
𝑖

𝜕𝒏𝑘
𝑖
𝜕𝑾𝑘

,

𝜕𝐿
𝜕𝒉𝑘−1
𝑖

=

𝜕𝐿
𝜕𝒏𝑘
𝑖

𝜕𝒏𝑘
𝑖
𝜕𝒉𝑘−1
𝑖

.

(14)

(15)

(16)

(17)

(18)

(19)

(20)

B ACCURACY COMPARISON WITH GAT

MODEL

In the main text, we have used the popular GCN algorithm for per-
formance comparison between our system and existing DL frame-
works. As stated in the main text, the purpose of these tests is to
show that our system is able to learn GNNs as well as existing
frameworks. Herein, we use the GAT model as another example
algorithm and three publicly available datasets, i.e. Cora. Citeseer
and Pubmed, to compare the performance between our system and
DGL for readers’ information. Table A1 shows the accuracy com-
parison with GAT model, where it can be observed that our system
yields comparable accuracy with DGL.

17

Table A1: Accuracy comparison with GAT model.

Dataset

Accuracy in Test Set (%)
GraphTheta w/GB GraphTheta w/MB DGL

Cora
Citeseer
Pubmed
GB: global-batch, MB: mini-batch.

81.1
71.2
78.7

80.0
70.8
78.6

81.4
72.6
78.0

Table A2: Runtimes (in seconds) of DistDGL in the function
of number of trainers.

#Trainers

8
16
32
64
128

2

22.031
22.793
25.458
30.967
41.301

Number of layers

3

4

5

55.775
60.959
73.233
105.259
Socket Error

88.941
99.537
124.891
Socket Error
Socket Error

123.507
137.468
174.956
Socket Error
Socket Error

C ASSESSMENT OF DISTDGL
C.1 Scalability Assessment
In this section, we show the detailed scalability of DistDGL [109,
110] by means of training four GCN models of different number of
layers on the Reddit dataset. Same with the tests in the main text, the
number of layers ranges from 2 to 5 and node sampling is disabled
for fair comparison. Table A2 shows the runtime per mini-batch
in the function of number of trainers. From the table, the runtime
increases as the number of trainers grows, indicating that DistDGL
does not scale at all with respect to number of trainers. Further,
except for the 2-layered model, all other models encountered socket
errors when the number of trainers is large. Specifically, the 3-
layered model failed in 128 trainers, while both the 4-layered and
5-layered models started to fail from 64 trainers.

We would like to express that we have excluded Dorylus [79] and
ROC [41] from this comparison for the following reasons. Firstly,
Dorylus is based on AWS Lambda and cannot run in our Kubernetes
cluster. Secondly, Dorylus performs graph operations in distributed
graph servers and conducts neural network operations by Lambda
threads. However, it still needs to load all neighbors of given targets
nodes into Lambda threads. This is same as DistDGL. Thirdly, Do-
rylus runs slower than DGL (non-sampling) on Reddit as shown in
its own paper. Finally, ROC requires each graph server to load the
entire graph into memory and thus does not match the objective of
our tests for fully distributed graphs.

C.2 Parameter Tuning
As mentioned in the main text, we launch only one trainer in each
machine and tune the number of threads assigned to the trainer and
the distributed server. In this test, in each machine the number of
threads assigned to the server is calculated as 64−𝑝, where 𝑝 denotes
the number of threads used by the trainer. Figure 15 illustrates the
performance changes along with the tuning of value 𝑝 for each GCN
model. According to our tuning, the best performance of DistDGL
is gained by setting 𝑝 = 44 for the 2-layered model, 𝑝 = 48 for the

3-layered model, 𝑝 = 36 for the 4-layered model and 𝑝 = 58 for the
5-layered model.

(a) 2-layered GCN

(b) 3-layered GCN

(c) 4-layered GCN

(d) 5-layered GCN

Figure 15: Parameter tuning of DistDGL for 4 GCN models
on Reddit.

18

05101520812162024283236404448525660Time per Mini-batch (s)Number of  threads in the trainerDistDGL with 2-layerd GCN010203040506048121620242832364044485256Time per Mini-batch (s)Number of  threads in the trainerDistDGL with 3-layered GCN02040608010048121620242832364044485256Time per Mini-batch (s)Number of  threads in the trainerDistDGL with 4-layered GCN020406080100120140812162024283236404448525660Time per Mini-batch (s)Number of  threads in the trainerDistDGL with 5-layered GCN