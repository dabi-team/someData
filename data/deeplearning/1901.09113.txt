Generative Adversarial Networks for Black-Box
API Attacks with Limited Training Data

Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, and Jason H. Li

Intelligent Automation, Inc.
Rockville, MD 20855, USA
Email:{yshi, ysagduyu, kdavaslioglu, jli}@i-a-i.com

9
1
0
2

n
a
J

5
2

]

G
L
.
s
c
[

1
v
3
1
1
9
0
.
1
0
9
1
:
v
i
X
r
a

Abstract—As online systems based on machine learning
are offered to public or paid subscribers via application
programming interfaces (APIs), they become vulnerable to
frequent exploits and attacks. This paper studies adversar-
ial machine learning in the practical case when there are
rate limitations on API calls. The adversary launches an
exploratory (inference) attack by querying the API of an
online machine learning system (in particular, a classiﬁer)
with input data samples, collecting returned labels to build up
the training data, and training an adversarial classiﬁer that
is functionally equivalent and statistically close to the target
classiﬁer. The exploratory attack with limited training data is
shown to fail to reliably infer the target classiﬁer of a real text
classiﬁer API that is available online to the public. In return, a
generative adversarial network (GAN) based on deep learning
is built to generate synthetic training data from a limited
number of real training data samples, thereby extending the
training data and improving the performance of the inferred
classiﬁer. The exploratory attack provides the basis to launch
the causative attack (that aims to poison the training process)
and evasion attack (that aims to fool the classiﬁer into making
wrong decisions) by selecting training and test data samples,
respectively, based on the conﬁdence scores obtained from the
inferred classiﬁer. These stealth attacks with small footprint
(using a small number of API calls) make adversarial machine
learning practical under the realistic case with limited training
data available to the adversary.

Index Terms—Adversarial machine learning, exploratory
attack, causative attack, evasion attack, deep learning, gen-
erative adversarial network.

I. INTRODUCTION

Artiﬁcial intelligence (AI) ﬁnds diverse and far-reaching
applications ranging from cyber security and intelligence
analysis to Internet of Things (IoT) and autonomous driv-
ing. To support AI, machine learning provides computa-
tional resources with the ability to learn without being ex-
plicitly programmed. Enabled by hardware accelerations for
computing, the next generation machine learning systems
such as those based on deep learning are effectively used
for different classiﬁcation, detection, estimation, tracking,
and prediction tasks.

Machine learning applications often involve sensitive
and proprietary information such as training data, machine
learning algorithm and its hyperparameters, and function-
ality of underlying tasks. As such applications become
more ubiquitous, machine learning itself becomes subject to

various exploits and attacks that expose the underlying tasks
to unprecedented threats. Adversarial machine learning is
an emerging ﬁeld that studies machine learning in the
presence of an adversary [1]. In one prominent example
adversarial images were generated by slightly perturbing
images to fool a state-of-the-art image classiﬁer, e.g., a
perturbed panda image was recognized as a gibbon by the
classiﬁer [2].

Machine learning systems are typically offered to pub-
lic or paid subscribers via an application programming
interface (API), e.g., Google Cloud Vision [3], provide
cloud-based machine learning tools to build and train ma-
chine learning models. This online service paradigm makes
machine learning vulnerable to exploratory (or inference)
attacks [4]–[10] that aim to understand how the underlying
machine learning algorithm works. An exploratory attack
can be launched as a black-box attack [5]–[7] without any
prior knowledge on the machine learning algorithm and the
training data. In this attack, the adversary calls the target
classiﬁer T with a large number of samples, collects the
labels, and then uses this data to train a deep learning
classiﬁer ˆT that is functionally equivalent (i.e., statistically
similar) to the target classiﬁer T . This attack infers the
full functionality of the classiﬁer and implicitly steals its
underlying training data, machine learning algorithm, and
hyperparameter selection.

After an adversary infers ˆT , it can launch further attacks
such as causative attacks [11]–[13] and evasion attacks
[13]–[16]. In a causative attack,
the adversary provides
the target classiﬁer with incorrect information as additional
training data to reduce the reliability of the retrained
classiﬁer. In particular, the adversary runs some samples
through the inferred classiﬁer ˆT . If their deep learning
scores (likelihood of labels) are far away from the decision
boundary, the adversary changes their labels and sends these
mislabeled samples as training data to the target classiﬁer
T . In an evasion attack, the adversary provides the target
classiﬁer with test data that will result in incorrect labels
(e.g., the classiﬁer is fooled into accepting an adversary as
legitimate). In particular, the adversary runs some samples
through the inferred classiﬁer ˆT . If their deep learning
scores are conﬁned within the decision region for label j
and are close to the decision region for the other label i,

 
 
 
 
 
 
real data that are gradually improved with labels queried
from the target classiﬁer. Our setting is different in several
aspects. First, the size of training data used in [6] (6,400
samples) is much larger than ours (100 samples). Second,
we select real samples that
the target will misclassify,
instead of altering real samples. Third, we consider a text
classiﬁer as the target classiﬁer, whereas [6] considered an
image classiﬁer. Fourth, we improve the inferred classiﬁer
(not the adversarial samples) that is used later to provide an
arbitrary number of selected samples to evasion or causative
attacks without further querying the target classiﬁer.

Data augmentation with the GAN was studied for the
low-data regime, where synthetic data samples were gener-
ated for image applications in [19] and wireless communi-
cations in [20] and [21]. In this paper, we used the GAN
to support attacks by augmenting the training data for an
adversary that operates with strict rate limitations on the
API calls made to a target text classiﬁer. A similar setting
was considered in [22]. Instead of adding synthetic data
samples, [22] used active learning to reduce the number of
API calls needed to obtain the labels of real data samples.
The rest of the paper is organized as follows. Section II
studies the exploratory attack on an online classiﬁer with
a large number of API calls. Training data augmentation
with the GAN is presented in Section III to launch the
exploratory attack with a limited number of API calls.
The subsequent causative and evasion attacks are studied
in Section IV. Section V concludes the paper.

II. BLACK-BOX EXPLORATORY ATTACK

The system model is shown in Fig. 1. There are various
online APIs for machine learning services with either free
subscriptions or paid licenses. For example, DatumBox
provides a number of machine learning APIs for text
analysis [17]. A user can write its own code to call
these APIs, specify some input text data, and obtain the
returned labels, e.g., subjective or objective. These services
are often based on sophisticated algorithms (e.g., machine
learning) and tuned by a large amount of training data,
which requires a signiﬁcant effort. The number of calls
made by a user depends on the license and is limited
in general, e.g., 1000 calls per day for free license of
DatumBox. The adversary launches a black-box exploratory
attack. The target classiﬁer T is based on an algorithm
(e.g., Naive Bayes or Support Vector Machine (SVM) or
a more sophisticated neural network) that is unknown to
the adversary. In addition, the adversary does not know the
training data that has been used to train the target classiﬁer
T . The adversary pursues the following steps:

1) The adversary sends a set S of samples to the target
classiﬁer T and receives the label T (s) returned by T
for each sample s ∈ S.

2) The adversary trains a deep neural network using the
training data from Step 1 and builds its own inferred
classiﬁer ˆT .

Fig. 1. Adversarial deep learning with limited training data.

the adversary feeds those samples to the target classiﬁer T
that is more likely to classify their labels to j instead of i.
One major limitation of exploratory attacks that has been
overlooked so far is the extent to which training data can
be collected from the target application. Usually, APIs have
strict rate limitations on how many application samples can
be collected over a time period (ranging from a limited
number samples per second to per day). We consider the
realistic case that the number of calls is limited due to
various reasons, e.g., the target classiﬁer T may limit the
number of calls that a user can make, or identify a large
number of calls as malicious by a defense mechanism.

In this paper, we select a real online text analysis API,
DatumBox [17], as the target classiﬁer T . The underlying
machine learning algorithm is unknown to the adversary
that treats T as a black box. First, we show that as the
API call rate is limited, the adversary cannot reliably infer
T using the exploratory attack. Second, we present an
approach to enhance the training process of the exploratory
attack. This approach is based on the generative adversarial
network (GAN) [18] to augment training data with synthetic
the exploratory attack with the
samples. We show that
GAN is successful even with a small number of training
samples (acquired from the API of the target classiﬁer
T ) and the inferred classiﬁer ˆT based on the deep neural
network is statistically close to T . The GAN consists of
a generator G and a discriminator D playing a minimax
game. G aims to generate realistic data (with labels), while
D aims to distinguish data generated by G as real or
synthetic. By playing a game between G and D, the GAN
generates additional training data without further calls to
T . With this additional training data, ˆT is better trained.
We show that the proposed approach signiﬁcantly improves
the performance of ˆT , which is measured as the statistical
difference of classiﬁcation results, i.e., labels returned, by
T and ˆT .

Adversarial machine learning with limited training data
was studied in [6] from a different perspective, where the
adversary ﬁrst infers the target classiﬁer with limited real
training data and then generates synthetic data samples in
an evasion attack by adding adversarial perturbations to

Machine Learning Classifier API under AttackLabelsBuild Training DatasetTest DataTrain a Deep Learning ClassifierFunctionally Equivalent Classifierrequires signiﬁcantly more data compared to standard ma-
chine learning algorithms (e.g., Naive Bayes and SVM).
To train a deep neural network, the weights and biases
of the multi-layered neural network are selected. In the
meantime, its hyperparameters such as the number of layers
and the number of neurons per layer are tuned. To build a
set of features as inputs for each text sample, the adversary
ﬁrst obtains a list of words, sorted by their frequencies of
occurrence in text such that W = (w1, w2, · · · ), where the
frequency of occurrence for word wi is pi and p1 ≥ p2 ≥
· · · . Then, for each sample of text, the adversary counts
the number of occurrence oi for each word wi and builds
a set of features (o1, o2, · · · ). Overall, 1000 features are
obtained by considering distributions of top 1000 words.
These features are used by the adversary to train the deep
learning classiﬁer ˆT .

The following two differences between ˆT and T are
computed. The difference d1( ˆT , T ) between ˆT and T on
label 1 is the number of samples with ˆT (s) = 2 and
T (s) = 1 divided by the number of samples with label
1 in test data, and the difference d2( ˆT , T ) between ˆT and
T on label 2 is the number of samples with ˆT (s) = 1 and
T (s) = 2 divided by the number of samples with label 2
in test data.

The adversary builds the optimal ˆT by selecting hy-
perparameters (e.g., the number of layers and the num-
ber of neurons per layer) to minimize dmax( ˆT , T ) =
max{d1( ˆT , T ), d2( ˆT , T )} based on the training data,
thereby balancing the false alarm and misdetection errors.
We used the Microsoft Cognitive Toolkit (CNTK) to train
the FNN and developed the Python code for hyperparameter
optimization. The adversary collects 6000 samples over
time and uses half of them as training data and half of
them as test data to train a classiﬁer ˆT . We optimize the
hyperparameters of the deep neural network as follows:

• The number of hidden layers is 2.
• The number of neurons per layer is 30.
• The loss function is cross entropy.
• The activation function in hidden layers is sigmoid.
• The activation function in output layer is softmax.
• All weights and biases are not initially scaled.
• Input values are unit normalized in the ﬁrst training

pass.

• The minibatch size is 20.
• The momentum coefﬁcient to update the gradient is 0.9.
• The number of epochs per time slot is 10.

The difference between labels returned by T and ˆT is found
as d1( ˆT , T ) = 25.66%, d2( ˆT , T ) = 26.04%, and d( ˆT , T ) =
25.80%. The difference can be further reduced with more
data collected over additional days.

In this attack, the adversary needs to collect training data
by calling the target classiﬁer many times over multiple
days (with 1000 samples per day). First, such an attack
incurs signiﬁcant delay. Second, a simple mechanism that
limits the number of allowed calls would force this attack
to take over a long time. Such a long-term attack can be

Fig. 2. One of the neural network layers.

Fig. 3. Word count frequencies of top 20 words in text data.

The adversary applies deep learning (based on a multi-
layered neural network) to infer T . Deep learning refers to
training a deep neural network for computing some func-
tion. Neural network consists of neurons that are joined by
weighted connections (synapses). In a feedforward neural
network (FNN) architecture, neurons are arranged in layers.
A signal xk at the input of synapse k is connected to
neuron j and multiplied by the synaptic weight wjk. An
adder sums these weighted multiplications and inputs the
sum to an activation function denoted by σ(·). The bias
bj of the jth layer increases or decreases the sum that is
input of the activation function. These operations can be
expressed as yj = σ ((cid:80)m
k=1 wjkxk + bj). The operation of
an FNN is illustrated in Fig. 2 for one of the layers with
m neurons. The weights and biases are determined by the
backpropagation algorithm.

We consider an exploratory attack on a real online classi-
ﬁer, namely the text subjectivity analysis API of DatumBox
[17]. The adversary builds training data by calling this
API with different text samples and collecting the returned
labels (subjective or objective). There are two types of
labels. Label 1 includes subjective text and label 2 includes
objective text. A user is allowed to make 1000 calls per day
with a free license, i.e., one can collect 1000 samples with
labels per day. The Twitter API is used to collect random
tweets. These tweets are ﬁrst preprocessed by removing
the stop words, web links (e.g., http and https links),
and punctuations. Then, word tokens are created from the
cleaned tweets. The word count frequencies of top 20 words
that the adversary prepares are presented in Fig. 3. Each
token is represented by a k-dimensional feature vector.

The inferred classier is built with deep learning that

Fig. 5. GAN for training data augmentation.

Fig. 6. Conditional GAN for training data augmentation.

Fig. 4. Adversarial deep learning with GAN-based data augmentation.

detected over time and identiﬁed as malicious behavior, and
the adversary can be blocked. Therefore, it is essential that
the adversary performs the exploratory attack with a limited
number of API calls. In the next section, we will present
a novel approach to enable a successful exploratory attack
with a limited number of training data samples.

III. TRAINING DATA AUGMENTATION WITH THE GAN

Consider the adversarial deep learning system model in
Fig. 4, which extends the exploratory attack in Section II
by utilizing a GAN to generate synthetic data samples
and augment the training data set to train the classiﬁer
better without additional calls. Data augmentation can be
performed by using neural networks, e.g., generating a new
sample by injecting noise to the input of a neural network
and dropout layers without changing the labels. Recently,
the GAN [18] was proposed to generate synthetic data
and was shown to outperform prior methods due to the
better quality of generated synthetic data that cannot be
reliably distinguished from real data [18], [23], [24]. An
example of the GAN is illustrated in Fig. 5. In a GAN,
there are two neural networks, namely the generator and
the discriminator, playing a minimax game:

min
G

max
D

Ex∼pdata [log(D(x))]
−Ez∼pz [log(1 − D(G(z)))],

(1)

where z is a noise input to generator G with a model dis-
tribution of pz and G(z) is the generator output. Input data
x has distribution pdata and discriminator D distinguishes
between the real and generated samples. Both G and D are
trained with backpropagation of error. However, when the
generator is trained with the objective in (1), the gradients
of the generator rapidly vanish that makes the training of
the GAN very difﬁcult. To address the vanishing gradient
problem, the following objective function at the generator
[23] is used:

Fig. 7. Loss function of G and D as a function of the training epochs.

Thus, in the ﬁrst step, we have the discriminator D trained
to distinguish between the real and synthetic data. Then
generator G takes random noise as input and maximizes
the probability of D making a mistake and fools D by
creating samples that resemble the real data in the second
step. The conditional GAN extends the GAN concept such
that the generator can generate synthetic data samples with
labels [24]. The objective function in the conditional GAN
is similar to the one of a regular GAN, but as illustrated
in Fig. 6, the terms D(x) and D(G(z)) are replaced by
D(x, y) and D(G(z, y)), respectively, to accommodate the
labels y as conditions. We assume that the adversary uses
the conditional GAN.

In data augmentation, the objective of the adversary is
to use Nr real training data samples with labels, generate
Ns synthetic training data samples with labels by applying
a conditional GAN, and augment the training data set to
the total of Naug = Nr + Ns samples. The deep neural
networks to build the conditional GAN are developed in
TensorFlow. The generator and the discriminator are made
up of FNNs with the following features.

max
G

Ez∼pz [log(D(G(z)))].

(2)

• The generator G takes in a noise vector z that is ran-
domly distributed with a zero mean and unit variance

Machine Learning Classifier API under AttackLabelsBuild Training DatasetTest DataAdding synthetic training dataTrain a Deep Learning ClassifierConditional Generative Adversarial NetworkFunctionally Equivalent ClassifierDiscriminator DGenerator GReal data: xSynthetic dataG(z)Noise: zDiscriminator DGenerator GReal data: xLabels: ySynthetic dataG(z,y)Noise: zLabels: y0100200300400500Epoch0.60.70.80.91.01.11.21.31.4LossD-lossG-lossTABLE I
PERFORMANCE (DIFFERENCE BETWEEN LABELS RETURNED BY T AND ˆT ) WITH REAL AND SYNTHETIC TRAINING DATA.

Training data size
(total number of samples)
100
150
200
250
300
400

Number of
real samples (Nr)
100
100
100
100
100
100

Number of
synthetic samples (Ns)
0
50
100
150
200
300

d1( ˆT , T )
44.59%
32.79%
27.21%
30.49%
28.52%
31.48%

Performance
d2( ˆT , T )
d( ˆT , T )
45.64% 45.00%
28.72% 31.20%
27.69% 27.40%
26.67% 29.00%
28.21% 28.40%
32.31% 31.80%

of 100 dimensions, i.e., z ∈ R100, and binary labels
y ∈ R2 as input, and generates synthetic data G(z|y)
with the same dimensions of the original data. The ﬁrst
two layers of G have 100 and 500 neurons per layer,
respectively, and they are activated by rectifying linear
unit (ReLU) that performs max(x, 0) operation on
input x. The output layer of G has the same dimensions
as the original data and uses the hyperbolic tangent
(tanh) activation function.

• The discriminator D takes in the original data or the
synthetic data generated by G. The ﬁrst two layers of
D have 500 neurons each, which are ReLU activated.
The last layer of D has one neuron that is sigmoid
activated.

Both G and D are trained with the Adam optimizer [25]
using a learning rate of 10−5. For the exploratory attack,
the adversary starts with Nr = 100 samples in training data
to build the classiﬁer ˆT . 500 samples are used in test data.
The hyperparatemers of the deep neural network used for
inferring the target classiﬁer are optimized as follows:

• The number of hidden layers is 3.
• The number of neurons per layer is 50.
• The loss function is cross entropy.
• The activation function in hidden layers is sigmoid.
• The activation function in output layer is softmax.
• All weights and biases are initially scaled by 3.
• Input values are unit normalized in the ﬁrst training

pass.

• The minibatch size is 25.
• The momentum coefﬁcient to update the gradient is 0.1.
• The number of epochs per time slot is 10.

The difference between the labels returned by T and ˆT is
found as d1( ˆT , T ) = 44.59%, d2( ˆT , T ) = 45.64%, and
d( ˆT , T ) = 45.00%. The adversary applies the GAN to
generate additional synthetic training data samples based on
100 real training data samples. Then the adversary retrains
ˆT with updated training data. The difference between labels
returned by T and ˆT is shown in Table I. The GAN is
trained with 500 epochs with 32 samples in each batch.
In one epoch, two discriminator updates and one generator
update are performed, as suggested in [18]. Fig. 7 shows the
loss function of the generator and the discriminator during
training.

When additional Ns = 50 or 100 training data samples
are generated by the GAN (i.e., when the total training data
size is Naug = 150 or 200 samples), the performance of
ˆT improves signiﬁcantly. However, adding more data (e.g.,
Ns = 150, 200 or 300 synthetic data samples) generated
by the GAN cannot further improve ˆT . While synthetic
data samples generated by the GAN can provide more
information to train a classiﬁer, their labels may also incur
some error (i.e., labels are different from those by T ). Thus,
adding too many additional synthetic data samples from
the GAN hurts the performance of ˆT . Results in Table I
show that training data augmentation with 100 real and 100
synthetic samples reduces d( ˆT , T ) up to 27.40%, which
is only 1.6% different from the case when all 3000 real
samples are used for training in Section II.

IV. CAUSATIVE AND EVASION ATTACKS

After an exploratory attack, causative and evasion attacks

are launched by using the inferred classiﬁer ˆT .

A. Causative Attack

In a causative attack, the adversary provides incorrect
training data such that the classiﬁer is retained with wrong
data and the accuracy of the updated classiﬁer ˜T drops.
In the extreme case, the adversary may change labels of
all data samples. However, to avoid being detected, the
adversary may not prefer switching labels of too many
data samples. Thus, it is important to select the best set of
data samples and switch their labels. This selection requires
the knowledge of T , but the adversary does not know T ’s
algorithm or training data. Therefore, a successful causative
attack can be performed only if the adversary has performed
an exploratory attack and obtained the inferred classiﬁer ˆT
that is similar to T .

The adversary applies deep learning to build ˆT , which
also provides a likelihood score in [0, 1] on the classiﬁcation
of each sample. If this score is less than a threshold, this
sample is identiﬁed as label 1, otherwise it is identiﬁed as
label 2. This score measures the conﬁdence of classiﬁcation.
That
its
label assignment has a high conﬁdence. On the other hand,
if a sample has a score close to the threshold, its label
assignment has a low conﬁdence. With ˆT , an adversary can
perform a causative attack by following three steps:

if a sample has a score close to 0 or 1,

is,

1) The adversary calls ˆT with a number of samples to

and receives their scores and labels.

2) Suppose the adversary can change the labels of for
p% of samples. The adversary selects samples with
2 % scores and samples with bottom p
top p
2 % scores.
3) The adversary switches labels of selected samples and
sends all labels of samples to retrain the classiﬁer.

We measure the impact of a causative attack by comparing
the outputs of the original classiﬁer T and the updated
classiﬁer ˜T on samples with label 1 or label 2 (identiﬁed by
T ) and on all data samples. Suppose there are n1 samples
with label 1 and n2 samples with label 2. Among n1 (or
n2) samples, m1 (or m2) samples are classiﬁed differently
by ˜T . Thus, we have three difference measures to represent
, d2(T, ˜T ) =
the difference of T and ˆT : d1(T, ˜T ) = m1
n1
m2
. We use 1000 samples and
n2
set p = 10. For the classiﬁer ˆT built
in Section III
with 400 training samples, we obtain d1(T, ˜T ) = 49.52%,
d2(T, ˜T ) = 45.41%, and d(T, ˜T ) = 48%. This result shows
that the causative attack reduces the updated classiﬁer ˜T ’s
performance signiﬁcantly.

, and d(T, ˜T ) = m1+m2
n1+n2

B. Evasion Attack

In an evasion attack, the adversary identiﬁes a particular
set of data samples that the classiﬁer T cannot reliably
classify. The adversary uses the classiﬁcation scores by ˆT to
perform the evasion attack. If the objective of the attack is
to maximize the error, the adversary should select samples
with scores close to the threshold (i.e., with low conﬁdence
on classiﬁcation). If the objective is to maximize the error
of misclassifying a sample with label 1 as label 2 (or
vice versa), the adversary should select samples classiﬁed
as label 2 (or 1) and with score close to the threshold.
Although the evasion attack is performed with the inferred
classiﬁer, the evaluation of the attack’s performance requires
the ground truth on labels. Since the adversary does not
have access to the knowledge of ground truth, we cannot
measure the performance of the evasion attack.

V. CONCLUSION
In this paper, we addressed adversarial deep learning
with limitations on the training data. We considered the
exploratory attack to infer an online classiﬁer by training
deep learning with a limited number of calls to a real online
API. However, with limited training data, we observed
that there is a signiﬁcant difference between the labels
returned by the inferred classiﬁer and the target classiﬁer.
We designed an approach using the GAN to reduce this dif-
ference and enhance the exploratory attack. Building upon
the classiﬁer inferred by the exploratory attack, we designed
causative and evasion attacks to select training and test
data samples, respectively, to poison the training process
and fool the target classiﬁer into making wrong decisions
for these samples. Our results show that adversarial deep
learning is feasible as a practical threat to online APIs even
when only limited training data is used by the adversary.

REFERENCES

[1] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman, “Towards
the science of security and privacy in machine learning,” arXiv preprint
arXiv:1611.03814, 2016.

[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[3] Google Cloud Vision API, available at https://cloud.google.com/vision.
[4] M. Barreno, B. Nelson, R. Sears, A. Joseph, and J. Tygar, “Can machine
learning be secure?” ACM Symposium on Information, Computer and
Communications Security, 2006.

[5] F. Tramer, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction APIs,” USENIX Security Sym-
posium, 2016.

[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A. Swami,
“Practical black-box attacks against deep learning systems using adver-
sarial examples,” ACM Conference on Computer and Communications
Security, 2017.

[7] Y. Shi, Y. E. Sagduyu, and A. Grushin, “How to steal a machine learning
classiﬁer with deep learning,” IEEE Symposium on Technologies for
Homeland Security (HST), 2017.

[8] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, “A methodology for
formalizing model-inversion attacks,” Computer Security Foundations,
2016.

[9] Y. Shi, Y. E. Sagduyu, K. Davaslioglu, and R. Levy, “Vulnerability detec-
tion and analysis in adversarial deep learning,” in Guide to Vulnerability
Analysis for Computer Networks and Systems: An Artiﬁcial Intelligence
Approach, Springer, 2018.

[10] Y. Shi, Y. E Sagduyu, T. Erpek, K. Davaslioglu, Z. Lu, and J. Li, “Ad-
versarial deep learning for cognitive radio security: Jamming attack and
defense strategies,” IEEE International Conference on Communications
(ICC) Workshop on Promises and Challenges of Machine Learning in
Communication Networks, 2018.

[11] L. Pi, Z. Lu, Y. Sagduyu, and S. Chen, “Defending active learning
against adversarial inputs in automated document classiﬁcation,” IEEE
Global Conference on Signal and Information Processing (GlobalSIP),
2016.

[12] S. Alfeld, X. Zhu, and P. Barford, “Data poisoning attacks against
autoregressive models,” AAAI Conference on Artiﬁcial Intelligence,
2016.

[13] Y. Shi and Y. E Sagduyu, “Evasion and causative attacks with adversarial
deep learning,” IEEE Military Communications Conference (MILCOM),
2017.

[14] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and A.
Swami, “The limitations of deep learning in adversarial settings,” IEEE
European Symposium on Security and Privacy, 2016. 2016.

[15] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the

physical world,” arXiv preprint arXiv:1607.02533, 2016.

[16] S. M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: A
simple and accurate method to fool deep neural networks,” IEEE
Conference on Computer Vision and Pattern Recognition, 2015.
[17] DatumBox Machine Learning API, available at http://www.datumbox.

com/machine-learning-api/

[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Advances in Neural Information Processing Systems, 2014.

[19] A. Antoniou, A. Storkey, and H. Edwards. “Data augmentation genera-
tive adversarial networks,” arXiv preprint arXiv:1711.04340, 2017.
[20] K. Davaslioglu and Y. E. Sagduyu, “Generative adversarial learning for
spectrum sensing,” IEEE International Conference on Communications
(ICC), 2018.

[21] T. Erpek, Y. E. Sagduyu, and Y. Shi, “Deep learning for launching and
mitigating wireless jamming attacks,” arXiv preprint arXiv:1807.02567,
2018.

[22] Y. Shi, Y. E. Sagduyu, K. Davaslioglu, and J. Li, “Active Deep Learning
Attacks under Strict Rate Limitations for Online API Calls,” IEEE
Symposium on Technologies for Homeland Security, 2018

[23] M. Arjovsky and L. Bottou, “Towards principled methods for training
generative adversarial networks,” International Conference on Machine
Learning (ICLR), 2017.

[24] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”

arXiv preprint arXiv:1411.1784, 2014.

[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

