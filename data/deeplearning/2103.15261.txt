1
2
0
2

r
a

M
9
2

]

G
L
.
s
c
[

1
v
1
6
2
5
1
.
3
0
1
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2021

ONE NETWORK FITS ALL? MODULAR VERSUS
MONOLITHIC TASK FORMULATIONS IN NEURAL
NETWORKS

Atish Agarwala & Abhimanyu Das
Google Research
{thetish,abhidas}@google.com

Brendan Juba
Washington U. St. Louis∗
bjuba@wustl.edu

Rina Panigrahy
Google Research
rinap@google.com

Vatsal Sharan
MIT†
vsharan@mit.edu

Xin Wang & Qiuyi Zhang
Google Research
{wanxin,qiuyiz}@google.com

ABSTRACT

Can deep learning solve multiple tasks simultaneously, even when they are unre-
lated and very different? We investigate how the representations of the underlying
tasks affect the ability of a single neural network to learn them jointly. We present
theoretical and empirical ﬁndings that a single neural network is capable of si-
multaneously learning multiple tasks from a combined data set, for a variety
of methods for representing tasks—for example, when the distinct tasks are en-
coded by well-separated clusters or decision trees over certain task-code attributes.
More concretely, we present a novel analysis that shows that families of simple
programming-like constructs for the codes encoding the tasks are learnable by
two-layer neural networks with standard training. We study more generally how
the complexity of learning such combined tasks grows with the complexity of the
task codes; we ﬁnd that combining many tasks may incur a sample complexity
penalty, even though the individual tasks are easy to learn. We provide empirical
support for the usefulness of the learning bounds by training networks on clusters,
decision trees, and SQL-style aggregation.

1

INTRODUCTION

Standard practice in machine learning has long been to only address carefully circumscribed, often
very related tasks. For example, we might train a single classiﬁer to label an image as containing
objects from a certain predeﬁned set, or to label the words of a sentence with their semantic roles.
Indeed, when working with relatively simple classes of functions like linear classiﬁers, it would be
unreasonable to expect to train a classiﬁer that handles more than such a carefully scoped task (or
related tasks in standard multitask learning). As techniques for learning with relatively rich classes
such as neural networks have been developed, it is natural to ask whether or not such scoping of tasks
is inherently necessary. Indeed, many recent works (see Section 1.2) have proposed eschewing this
careful scoping of tasks, and instead training a single, “monolithic” function spanning many tasks.

Large, deep neural networks can, in principle, represent multiple classiﬁers in such a monolithic
learned function (Hornik, 1991), giving rise to the ﬁeld of multitask learning. This combined function
might be learned by combining all of the training data for all of the tasks into one large batch–see
Section 1.2 for some examples. Taken to an extreme, we could consider seeking to learn a universal
circuit—that is, a circuit that interprets arbitrary programs in a programming language which can
encode various tasks. But, the ability to represent such a monolithic combined function does not
necessarily entail that such a function can be efﬁciently learned by existing methods. Cryptographic
hardness theorems (Kearns & Valiant, 1994) establish that this is not possible in general by any
method, let alone the speciﬁc training methods used in practice. Nevertheless, we still can ask how

∗Work performed in part while visiting Google.
†Work performed in part while afﬁliated with Stanford, and in part while interning at Google.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2021

Figure 1: Our framework shows that it is possible to learn analytic functions such as the gravitational
force law, decision trees with different functions at the leaf nodes, and programming constructs such
as those on the right, all using a non-modular monolithic architecture.

rich a family of tasks can be learned by these standard methods. In this work, we study the extent to
which backpropagation with stochastic gradient descent (SGD) can learn such monolithic functions
on diverse, unrelated tasks. There might still be some inherent beneﬁt to an architecture in which tasks
are partitioned into sub-tasks of such small scope, and the training data is correspondingly partitioned
prior to learning. For example, in the early work on multitask learning, Caruana (1997) observed that
training a network to solve unrelated tasks simultaneously seemed to harm the overall performance.
Similarly, the seminal work of Jacobs et al. (1991) begins by stating that “If backpropagation is used
to train a single, multilayer network to perform different subtasks on different occasions, there will
generally be strong interference effects that lead to slow learning and poor generalization”. We
therefore ask if, for an unfortunate choice of tasks in our model, learning by standard methods might
be fundamentally impaired.

As a point of reference from neuroscience, the classical view is that distinct tasks are handled in the
brain by distinct patches of the cortex. While it is a subject of debate whether modularity exists for
higher level tasks (Samuels, 2006), it is accepted that there are dedicated modules for low-level tasks
such as vision and audio processing. Thus, it seems that the brain produces a modular architecture,
in which different tasks are handled by different regions of the cortex. Conceivably, this division
into task-speciﬁc regions might be driven by fundamental considerations of learnability: A single,
monolithic neural circuit might simply be too difﬁcult to learn because the different tasks might
interfere with one another. Others have taken neural networks trained by backpropagation as a model
of learning in the cortex (Musslick et al., 2017); to the extent that this is reasonable, our work has
some bearing on these questions as well.

1.1 OUR RESULTS

We ﬁnd, perhaps surprisingly, that combining multiple tasks into one cannot fundamentally impair
learning with standard training methods. We demonstrate this for a broad family of methods for
combining individual tasks into a single monolithic task. For example, inputs for each individual
tasks may come from a disjoint region (for example, a disjoint ball) in a common input space, and
each individual task could then involve applying some arbitrary simple function (e.g., a separate
linear classiﬁer for each region). Alternately there may be an explicit “task code” attribute (e.g., a
one-hot code), together with the usual input attributes and output label(s), where examples with the
same task code are examples for the same learning task. Complementing our results that combining
multiple tasks does not impair learning, we also ﬁnd that some task coding schemes do incur a sample
complexity penalty.

A vast variety of task coding schemes may be used. As a concrete example, when the data points
for each task are well-separated into distinct clusters, and the tasks are linear classiﬁcation tasks, we
show that a two-layer architecture trained with SGD successfully learns the combined, monolithic
function; the required amount of data simply scales as the sum of the amount required to learn each

2

Published as a conference paper at ICLR 2021

task individually (Theorem 2). Meanwhile, if the tasks are determined by a balanced decision tree of
height h on d code attributes (as in Fig. 1, left), we ﬁnd that the training time and amount of data
needed scales as ∼ dh—quasipolynomial in the 2h leaves (distinct tasks) when d is of similar size to
h, and thus when the coding is efﬁcient (Theorem 3). We also prove a corresponding lower bound,
which shows that this bound is in fact asymptotically tight (Theorem 3). More generally, for task
codings based on decision trees using linear splits with a margin of at least γ (when the data has unit
(cid:96)2 norm), the training time and required data are asymptotically bounded by ∼ eO(h/γ2), which for
constant γ is polynomial in the 2h functions (Theorem 4).

We generalize from these cluster-based and decision-tree based task codings to more complex codes
that are actually simple programs. For instance, we show that SQL-style aggregation queries over a
ﬁxed database, written as a functions of the parameters of the query, can also be learned this way.
More generally, simple programming constructs (such as in Fig. 1, right), built by operations such
as compositions, aggregation, concatenation, and branching on a small number of such learnable
functions, are also learnable (Theorem 5). In general, we can learn a low-depth formula (circuit with
fan-out 1) in which each gate is not merely a switch (as in a decision tree), but can be any analytic
function on the inputs, including arithmetic operations. Again, our key technical contribution is that
we show that all of these functions are efﬁciently learned by SGD. This is non-trival since, although
universal approximation theorems show that such functions can be expressed by (sufﬁciently wide)
two-layer neural networks, under standard assumptions some expressible functions are not learnable
Klivans & Sherstov (2009). We supplement the theoretical bounds with experiments on clusters,
decision trees, and SQL-style aggregation showing that such functions are indeed learned in practice.

We note that the learning of such combined functions could have been engineered by hand: for
example, there exist efﬁcient algorithms for learning clusterings or such decision trees, and it is easy
to learn the linear classiﬁers given the partitioned data. Likewise, these classes of functions are all
known to be learnable by other methods, given an appropriate transformation of the input features.
The key point is that the two-layer neural network can jointly learn the task coding scheme and the
task-speciﬁc functions without special engineering of the architecture. That is, it is unnecessary to
engineer a way of partitioning of the data into separate tasks prior to learning. Relatedly, the time
and sample requirements of learning multiple tasks on a single network in general is insufﬁcient to
explain the modularity observed in biological neural networks if their learning dynamics are similar
to SGD —i.e., we cannot explain the presence of modularity from such general considerations.

All our theoretical results are based upon a fundamental theorem that shows that analytic functions
can be efﬁciently learnt by wide (but ﬁnite-width) two-layer neural networks with standard activation
functions (such as ReLU), using SGD from a random initialization. Speciﬁcally, we derive novel
generalization bounds for multivariate analytic functions (Theorems 1 and 8) by relating wide
networks to kernel learning with a speciﬁc network-induced kernel (Jacot et al., 2018; Du et al.,
2019; Allen-Zhu et al., 2019; Arora et al., 2019a; Lee et al., 2019), known as the neural tangent
kernel (NTK) (Jacot et al., 2018). We further develop a calculus of bounds showing that the sum,
product, ratio, and composition of analytic functions is also learnable, with bounds constructed
using the familiar product and chain rules of univariate calculus (Corollaries 1, 2). These above
learnability results may be of independent interest; for example, they can be used to show that natural
physical laws like the gravitational force equations (shown in Fig. 1) can be efﬁciently learnt by neural
networks (Section B.1). Furthermore, our bounds imply that the NTK kernel for ReLU activation has
theoretical learning guarantees that are superior to the Gaussian kernel (Section A.2), which we also
demonstrate empirically with experiments on learning the gravitational force law (Section B.2).

1.2 RELATED WORK

Most related to our work are a number of works in application areas that have sought to learn a single
network that can perform many different tasks. In natural language processing, Tsai et al. (2019) show
that a single model can solve machine translation across more than 50 languages. Many other works
in NLP similarly seek to use one model for multiple languages, or even multiple tasks (Johnson et al.,
2017; Aharoni et al., 2019; Bapna et al., 2019; Devlin et al., 2018). Monolithic models have also been
successfully trained for tasks in very different domains, such as speech and language (Kaiser et al.,
2017). Finally, there is also work on training extremely large neural networks which have the capacity
to learn multiple tasks (Shazeer et al., 2017; Raffel et al., 2019). These works provide empirical clues

3

Published as a conference paper at ICLR 2021

that suggest that a single network can successfully be trained to perform a wide variety of tasks. But,
they do not provide a systematic theoretical investigation of the extent of this ability as we do here.

Caruana (1997) proposed multitask learning in which a single network is trained to solve multiple
tasks on the same input simultaneously, as a vector of outputs. He observed that average generalization
error for the multiple tasks may be much better than when the tasks are trained separately, and this
observation initiated an active area of machine learning research (Zhang & Yang, 2017). Multitask
learning is obviously related to our monolithic architectures. The difference is that whereas in
multitask learning all of the tasks are computed simultaneously and output on separate gates, here
all of the tasks share a common set of outputs, and the task code inputs switch between the various
tasks. Furthermore, contrary to the main focus of multitask learning, we are primarily interested in
the extent to which different tasks may interfere, rather than how much similar ones may beneﬁt.

Our work is also related to studies of neural models of multitasking in cognitive science. In particular,
Musslick et al. (2017) consider a similar two-layer architecture in which there is a set of task code
attributes. But, as in multitask learning, they are interested in how many of these tasks can be
performed simultaneously, on distinct outputs. They analyze the tradeoff between improved sample
complexity and interference of the tasks with a handcrafted “gating” scheme, in which the parts of
activity are zeroed out depending on the input (as opposed to the usual nonlinearities); in this model,
they ﬁnd out that the speedup from multitask learning comes at the penalty of limiting the number of
tasks that can be correctly computed as the similarity of inputs varies. Thus, in contrast to our model
where the single model is computing distinct tasks sequentially, they do ﬁnd that the distinct tasks
can interfere with each other when we seek to solve them simultaneously.

2 TECHNICAL OVERVIEW

We now give a more detailed overview of our theoretical techniques and results, with informal
statements of our main theorems. For full formal statements and proofs, please see the Appendix.

2.1 LEARNING ANALYTIC FUNCTIONS

Our technical starting point is to generalize the analysis of Arora et al. (2019b) in order to show that
two-layer neural networks with standard activation, trained by SGD from random initialization, can
learn analytic functions on the unit sphere. We then obtain our results by demonstrating how our
representations of interest can be captured by analytic functions with power series representations of
appropriately bounded norms. Formal statements and proofs for this section appear in Appendix A.2.
Let Sd denote the unit sphere in d dimensions.
Theorem 1. (Informal) Given an analytic function g(y), the function g(β · x), for ﬁxed β ∈ Rd (with
β def= (cid:107)β(cid:107)2) and inputs x ∈ Sd is learnable to error (cid:15) with n = O((β˜g(cid:48)(β) + ˜g(0))2/(cid:15)2) examples
using a single-hidden-layer, ﬁnite width neural network of width poly(n) trained with SGD, with

˜g(y) =

∞
(cid:88)

k=0

|ak|yk

(1)

where the ak are the power series coefﬁcients of g(y).

We will refer to ˜g(cid:48)(1) as the norm of the function g—this captures the Rademacher complexity of
learning g, and hence the required sample complexity. We also show that the ˜g function in fact tightly
captures the Rademacher complexity of learning g, i.e. there is a lower bound on the Rademacher
complexity based on the coefﬁcients of ˜g for certain input distributions (see Corollary 5 in Section C
in the appendix).

We also note that we can prove a much more general version for multivariate analytic functions g(x),
with a modiﬁed norm function ˜g(y) constructed from the multivariate power series representation
of g(x) (Theorem 8 in Appendix A.2). The theorems can also be extended to develop a “calculus
of bounds” which lets us compute new bounds for functions created via combinations of learnable
functions. In particular, we have a product rule and a chain rule:

4

Published as a conference paper at ICLR 2021

Figure 2: Some of the task codings which ﬁt in our framework. On the left, we show a task coding
via clusters. Here, c(i) is the code for the ith cluster. On the right, we show a task coding based on
low-depth decision trees. Here, ci is the ith coordinate of the code c of the input datapoint.

Corollary 1 (Product rule). Let g(x) and h(x) meet the conditions of Theorem 1. Then the product
g(x)h(x) is efﬁciently learnable as well, with O(Mg·h/(cid:15)2) samples where
(cid:112)Mg·h = ˜g(cid:48)(1)˜h(1) + ˜g(1)˜h(cid:48)(1) + ˜g(0)˜h(0).

(2)

Corollary 2 (Chain rule). Let g(y) be an analytic function and h(x) be efﬁciently learnable, with
auxiliary functions ˜g(y) and ˜h(y) respectively. Then the composition g(h(x)) is efﬁciently learnable
as well with O(Mg◦h/(cid:15)2) samples where

(cid:112)Mg◦h = ˜g(cid:48)(˜h(1))˜h(cid:48)(1) + ˜g(˜h(0)),

(3)

provided that ˜h(0) and ˜h(1) are in the radius of convergence of g.

The calculus of bounds enables us to prove learning bounds on increasingly expressive functions, and
we can prove results that may be of independent interest. As an example, we show in Appendix B.1
that forces on k bodies interacting via Newtonian gravitation, as shown in Figure 1, can be learned to
error (cid:15) using only kO(ln(k/(cid:15))) examples (even though the function 1/x has a singularity at 0).

2.2 TASK CODING VIA CLUSTERS

Our analysis of learning analytic functions allows us to prove that a single network with standard
training can learn multiple tasks. We formalize the problem of learning multiple tasks as follows. In
general, these networks take pairs of inputs (c, x) where c is a task code and x is the input (vector)
for the chosen task represented by c. We assume both c and x have ﬁxed dimensionality. These
pairs are then encoded by the concatenation of the two vectors, which we denote by c; x. Given k
tasks, corresponding to evaluation of functions f1, . . . , fk respectively on the input x, the ith task
has a corresponding code c(i). Now, we wish to learn a function g such that g(c(i); x) = fi(x) for
examples of the form (c(i); x, fi(x)). This g is a “monolithic” function combining the k tasks. More
generally, there may be some noise (bounded within a small ball around c(i)) in the task codes which
would require learning the monolithic function g(c, x) = fj(x) where j = argmini(cid:107)c − c(i)(cid:107)2 .
Alternately the task-codes are not given explicitly but are inferred by checking which ball-center c(i)
(unique per task) is closest to the input x (see Fig. 2 (left) for an example). Note that these are all
generalizations of a simple one-hot coding.

We assume throughout that the fi are analytic, with bounded-norm multinomial Taylor series rep-
resentations. Our technical tool is the following Lemma (proved in Appendix A.2) which shows
that the univariate step function 1(x ≥ 0) can be approximated with error (cid:15) and margin γ using a
low-degree polynomial which can be learnt using SGD.

Lemma 1. Given a scalar x, let

Φ(x, γ, (cid:15)) = (1/2)

(cid:16)

1 + erf

(cid:16)

Cx(cid:112)log(1/(cid:15))/γ

(cid:17)(cid:17)

5

Published as a conference paper at ICLR 2021

where erf is the Gauss error function and C is a constant. Let Φ(cid:48)(x, γ, (cid:15)) be the function Φ(x, γ, (cid:15))
with its Taylor series truncated at degree O(log(1/(cid:15))/γ). Then,

Φ(cid:48)(x, γ, (cid:15)) =

(cid:26)O((cid:15))

x ≤ −γ/2,

1 − O((cid:15)) x ≥ γ/2.

Also, Φ(cid:48)(x, γ, (cid:15)) can be learnt using SGD with at most eO((log(1/(cid:15))/γ2)) examples.

Using this lemma, we show that indicator functions for detecting membership in a ball near a
prototype c(i) can also be sufﬁciently well approximated by functions with such a Taylor series
representation. Speciﬁcally, we use the truncated representation of the erf function to indicate that
(cid:107)c − c(i)(cid:107) is small. As long as the centers are sufﬁciently well-separated, we can ﬁnd a low-degree,
low-norm function this way using Lemma 1. For example, to check if c is within distance r of center
c(i) we can use 1((cid:107)c − c(i)(cid:107)2 ≤ r2), which can be approximated using the φ(cid:48) function in Lemma 1.
Then given such approximate representations for the task indicators I1(c), . . . , Ik(c), the function
g(c; x) = I1(c)f1(x) + · · · + Ik(c)fk(x) has norm linear in the complexities of the task functions,
so that they are learnable by Theorem 1 (we scale to inputs to lie within the unit ball as required by
Theorem 1). We state the result below, for the formal statement and proof see Appendix A.3.
Theorem 2. (Informal) Given k analytic functions having Taylor series representations with norm
at most poly(k/(cid:15)) and degree at most O(log(k/(cid:15))), a two-layer neural network trained with SGD
can learn the following functions g(c; x) on the unit sphere to accuracy (cid:15) with sample complexity
poly(k/(cid:15)) times the sum of the sample complexities for learning each of the individual functions:

• for Ω(1)-separated codes c(1), . . . , c(k), if (cid:107)c − c(i)(cid:107)2 ≤ O(1), then g(c; x) = fi(x).

2.3 TASK CODING VIA LOW-DEPTH DECISION TREES

Theorem 2 can be viewed as performing a single k-way branching choice of which task function
to evaluate. Alternatively, we can consider a sequence of such choices, and obtain a decision tree
in which the leaves indicate which task function is to be applied to the input. We ﬁrst consider
the simple case of a decision tree when c is a {±1}-valued vector. We can check that the values
c1, . . . , ch match the ﬁxed assignment c(i)
h that reaches a given leaf of the tree using the
function Ic(i)(c) = (cid:81)h
(or similarly for any subset of up to h of the indices). Then
g(c; x) = Ic(1)(c)f1(x) + · · · + Ic(k) (c)fk(x) represents our decision tree coding of the tasks (see
Fig. 2 (right) for an example). For the theorem, we again scale the inputs to lie within the unit ball:
Theorem 3. (Informal) Two-layer neural networks trained with SGD can learn such a decision tree
with depth h within error (cid:15) with sample complexity O(dh/(cid:15)2) times the sum of the sample complexity
for learning each of the individual functions at the leaves. Furthermore, conditioned on the hardness
of learning parity with noise, dΩ(h) examples are in fact necessary to learn a decision tree of depth h.

1 , . . . , c(i)

cj +c(i)
2

j=1

j

We can generalize the previous decision tree to allow a threshold based decision at every internal
node, instead of just looking at a coordinate. Assume that the input data lies in the unit ball and
that each decision is based on a margin of at least γ. We can then use a product of our truncated erf
polynomials to represent branches of the tree. We thus show:
Theorem 4. (Informal) If we have a decision tree of depth h where each decision is based on a
margin of at least γ, then we can learn such a such a function within error (cid:15) with sample complexity
eO(h log(1/(cid:15))/γ2) times the sample complexity of learning each of the leaf functions.

For the formal statements and proofs, see Appendix A.4. Note that by Theorem 3, the exponential
dependence on the depth in these theorems is necessary.

2.4 SIMPLE PROGRAMMING CONSTRUCTS

So far, we have discussed jointly learning k functions with task codings represented by clusters
and decision trees. We now move to a more general setup, where we allow simple programming
constructs such as compositions, aggregation, concatenation, and branching on different functions. At
this stage, the distinction between “task codes” and “inputs” becomes somewhat arbitrary. Therefore,

6

Published as a conference paper at ICLR 2021

we will generally drop the task codes c from the inputs. The class of programming constructs we can
learn is a generalization of the decision tree and we refer to it as a generalized decision program.
Deﬁnition 1. We deﬁne a generalized decision program to be a circuit with fan-out 1 (i.e., a tree
topology). Each gate in the circuit computes a function of the outputs of its children, and the root
(top) node computes the ﬁnal output. All gates, including the leaf gates, have access to the input x.

We can learn generalized decision programs where each node evaluates one among a large family of
operations, ﬁrst described informally below, and then followed by a formal deﬁnition.

Arithmetic/analytic formulas As discussed in Section 2.1, learnability of analytic functions not
only allows us to learn functions with bounded Taylor series, but also sums, products, and ratios of
such functions. Thus, we can learn constant-depth arithmetic formulas with bounded outputs and
analytic functions (with appropriately bounded Taylor series) applied to such learnable functions.

Aggregation We observe that the sum of k functions with bounded Taylor representations yields a
function of the same degree and norm that is at most k times greater; the average of these k functions,
meanwhile does not increase the magnitude of the norm. Thus, these standard aggregation operations
are represented very efﬁciently. These enable us to learn functions that answer a family of SQL-style
queries against a ﬁxed database as follows: suppose I(x, r) is an indicator function for whether or
not the record r satisﬁes the predicate with parameters x. Then a sum of the m entries of a database
that satisfy the predicate given by x is represented by I(x, r(1))r(1) + · · · + I(x, r(m))r(m). Thus,
as long as the predicate function I and records r(i) have bounded norms, the function mapping the
parameters x to the result of the query is learnable. We remark that max aggregation can also be
represented as a sum of appropriately scaled threshold indicators, provided that there is a sufﬁcient
gap between the maximum value and other values.

Structured data We note that our networks already receive vectors of inputs and may produce
vectors of outputs. Thus, one may trivially structured inputs and outputs such as those in Fig. 1 (right)
using these vectors. We now formalize this by deﬁning the class of functions we allow.
Deﬁnition 2. We support the following operations at any gate in the generalized decision program.
Let every gate have at most k children. Let g be the output of some gate and {f1, . . . , fk} be the
outputs of the children of that gate.

1. Any analytic function of the child gates which can be approximated by a polynomial of degree at

most p, including sum g = (cid:80)k

i=1 fi and product of p terms g = Πp

i=1fi.

2. Margin-based switch (decision) gate with children {f1, f2} and some constant margin γ, i.e.,
g = f1 if (cid:104)β, x(cid:105) − α ≤ −γ/2, and g = f2 if (cid:104)β, x(cid:105) − α ≥ γ/2, for a vector β and constant α.
3. Cluster-based switch gate with k centers {c(1), . . . , c(k)}, with separation r (for some constant
r), i.e. the output is fi if (cid:107)x − c(i)(cid:107) ≤ r/3. A special case of this is a look-up table which returns
value vi if x = c(i), and 0 if x does not match any of the centers.

4. Composition of two functions, g(x) = f1(f2(x)).
5. Create a tuple out of separate ﬁelds by concatenation: given inputs {f1, . . . , fk} g outputs a
tuple [f1, . . . , fk], which creates a single data structure out of the children. Or, extract a ﬁeld out
of a tuple: for a ﬁxed ﬁeld i, given the tuple [f1, . . . , fk], g returns fi.

6. For a ﬁxed table T with k entries {r1, . . . , rk}, a Boolean-valued function b, and an analytic
function f , SQL queries of the form SELECT SUM f(r_i), WHERE b(r_i, x) for the
input x, i.e., g computes (cid:80)
i:b(ri,x)=1 f (ri). (We assume that f takes bounded values and b can
be approximated by an analytic function of degree at most p.) For an example, see the function
avg_income_zip_code() in Fig. 1 (right).

As an example of a simple program we can support, refer to Fig. 1 (right) which involves table
lookups, decision nodes, analytic functions such as Euclidean distance, and SQL queries. Theorem 5
is our learning guarantee for generalized decision programs. See Section A.5 in the Appendix for
proofs, formal statements, and a detailed description of the program in Fig. 1 (right).
Theorem 5. (Informal) Any generalized decision program of constant depth h using the above
operations with p ≤ O(log(k/(cid:15))) can be learnt within error (cid:15) with sample complexity kpoly(log(k/(cid:15))).
For the speciﬁc case of the program in Fig. 1 (right), it can be learnt using (k/(cid:15))O(log(1/(cid:15))) examples,
where k is the number of individuals in the database.

7

Published as a conference paper at ICLR 2021

(a) Random linear classiﬁer for each cluster.

(b) Random teacher network for each cluster.

Figure 3: Binary classiﬁcation on multiple clusters, results are an average over 3 trials. A single neural
network does well even when there are multiple clusters. The error does not increase substantially on
increasing the number of clusters k

3 EXPERIMENTS

We next empirically explore the learnability of multiple functions by a two layer neural network when
the tasks are coded by well-separated clusters or decision trees, and more generally the learnability
of SQL-style aggregation for a ﬁxed database. We ﬁnd good agreement between the empirical
performance and the bounds of Section 2. See Appendix D for more details of the experimental setup.

Learning binary classiﬁcation for well-separated clusters data We demonstrate through ex-
periments on synthetic data that a single neural network can learn multiple tasks if the tasks are
well-separated into clusters, as we discussed in Section 2.2. Here the data is drawn from a mixture
of k well-separated Gaussians in d = 50 dimensions. Within each Gaussian, the data points are
marked with either of two labels. For the label generation, we consider two cases, ﬁrst when the
labels within each cluster are determined by a simple linear classiﬁer, and second when the labels
are given by a random teacher neural network with one hidden layer of 10 hidden units. Fig. 3
shows the performance of a single two-layer neural network with 50k hidden units on this task. The
performance of the neural network changes only slightly on increasing the number of clusters (k),
suggesting that a single neural network can learn across all clusters.

Learning polynomial functions on leaves of a decision tree We consider the problem of learning
polynomial functions selected by a decision tree. The data generation process is as follows. We
ﬁrst ﬁx parameters: tree depth h, decision variable threshold margin γ, number of variables k,
and degree p for leaf functions. Then we specify a full binary decision tree of depth h with a
random polynomial function on each leaf. To do this, we ﬁrst generate thresholds t1, t2, ..., th
from the uniform distribution on [0, 1] and 2h leaf functions which are homogeneous polynomials
of k variables and degree p, with uniformly distributed random coefﬁcients in [0, 1]. A train/test
example (x, y) where x = (x1, ..., xh, xh+1, ..., xh+p) is generated by ﬁrst randomly sampling the
xi’s from the uniform distribution on [0, 1], selecting the corresponding leaf based on x1, ..., xh (that
is, go left at the ﬁrst branch if x1 ≤ t1, otherwise go right, etc), and computing y by evaluating
the leaf function at (xh+1, ..., xh+p). The data is generated with the guarantee that each leaf has
the same number of data points. Fig. 4 shows the performance of a two-layer neural network with
32 × 2h hidden units, measured in the R-squared metric. Here the R-squared metric is deﬁned as
1 − (cid:80)
i(yi − y)2, and is the fraction of the underlying variance explained by the model.
Note that for a model that outputs the mean y for any input, the R-squared metric would be zero. We
observed for a ﬁxed number of training samples, accuracy increases as threshold margin increases,
and the dependence of sample complexity on test error agrees with the bound in Theorem 4.

i(ˆyi − yi)2/ (cid:80)

Learning SQL-style aggregation queries We demonstrate the learnability of SQL-style
the form SELECT SUM/MIN/MAX f(x)
aggregation queries, which are functions of
WHERE p(x) from DATABASE. The train and test datasets are generated from the Penn World

8

101102103Number of examples per cluster5060708090100Test accuracyk = 1k = 50k = 100k = 250k = 500k = 1000102103104Number of examples per cluster455055606570758085Test accuracyk = 1k = 10k = 30k = 50Published as a conference paper at ICLR 2021

(a) Fixed threshold margin γ = 0.1.

(b) Fixed tree depth h = 10.

Figure 4: Learning random homogeneous polynomials of 4 variables and degree 4 on the leaves of a
decision tree, the results are averaged over 7 trials. (a) Sample complexity scales as eO(h log(1/(cid:15))/γ2)
with error (cid:15), where error is measured by (1-Test R-squared). (b) For ﬁxed tree depth, accuracy
increases with increasing margin.

Table dataset (Feenstra et al., 2015), which contains 11830 rows of economic data. The WHERE
clause takes the form of (xi1 ≥ ti1 ) AND . . . AND (xik ≥ tik ), where xi1, . . . , xik are k randomly
selected columns and ti1 , . . . , tik are randomly selected values from the columns. The query target
function is randomly selected from SUM, MAX, and MIN and is over a ﬁxed column (pl_x in the
table, which stands for price level for imports). The R-squared metric for a two-layer neural network
with 40k hidden units is summarized in Table 1. We observe that a neural network learns to do
SQL-style aggregation over dozens of data points, and for a ﬁxed database, the test error only varies
slightly for different numbers of columns in the WHERE clause.

Table 1: R-Squared for SQL-style aggregation. A single network with one hidden layer gets high
R-Squared values, and the error does not increase substantially if the complexity of the aggregation is
increased by increasing the number of columns in the WHERE clause.

# columns in WHERE clause

Median # data points

1

21

2

12

3

9

4

4

5

3

Test R-Squared

(93.31 ± 0.11) % (93.01 ± 2.7)% (91.86 ± 2.59) % (94.84 ± 1.86) % (92.51 ± 2.2) %

4 CONCLUSION AND FUTURE WORK

Our results indicate that even using a single neural network, we can still learn tasks across multiple,
diverse domains. However, modular architectures may still have beneﬁts over monolithic ones: they
might use less energy and computation, as only a portion of the total network needs to evaluate
any data point. They may also be more interpretable, as it is clearer what role each part of the
network is performing. It is an open question if any of these beneﬁts of modularity can be extended
to monolothic networks. For instance, is it necessary for a monolithic network to have modular parts
which perform identiﬁable simple computations? And if so, can we efﬁciently identify these from the
larger network? This could help in interpreting and understanding large neural networks.

Our work also begins to establish how neural networks can learn functions which are represented as
simple programs. This perspective raises the question, how rich can these programs be? Can we learn
programs from a full-featured language? In particular, supposing that they combine simpler programs
using other basic operations such as composition, can such libraries of tasks be learned as well, i.e.,
can these learned programs be reused? We view this as a compelling direction for future work.

9

0.0010.0020.0030.0040.0050.0061 - Test R-squared100200300400500# examples per leafdepth=6depth=7depth=8depth=9depth=100.0250.0500.0750.1000.1250.1500.1750.200Decision variable threshold margin0.99550.99600.99650.99700.99750.9980Test R-squared# examples per leaf=64# examples per leaf=128# examples per leaf=256# examples per leaf=512Published as a conference paper at ICLR 2021

ACKNOWLEDGEMENTS

Brendan Juba was partially supported by NSF Awards CCF-1718380, IIS-1908287, and IIS-1939677,
and was visiting Google during a portion of this work. Vatsal Sharan was supported in part by NSF
award 1704417.

REFERENCES

Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation.

arXiv preprint arXiv:1903.00089, 2019.

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. In Advances in Neural Information Processing
Systems 32, pp. 6155–6166. Curran Associates, Inc., 2019.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis
of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.
In
International Conference on Machine Learning, pp. 322–332, May 2019a.

Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.

Ankur Bapna, Colin Andrew Cherry, Dmitry Dima Lepikhin, George Foster, Maxim Krikun, Melvin
Johnson, Mia Chen, Naveen Ari, Orhan Firat, Wolfgang Macherey, et al. Massively multilingual
neural machine translation in the wild: Findings and challenges. 2019.

Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem, and the

statistical query model. Journal of the ACM (JACM), 50(4):506–519, 2003.

Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient Descent Provably Optimizes
Over-parameterized Neural Networks. In 7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.

Robert C Feenstra, Robert Inklaar, and Marcel P Timmer. The next generation of the penn world

table. American economic review, 105(10):3150–82, 2015.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):

251–257, 1991.

Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of

local experts. Neural computation, 3(1):79–87, 1991.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31, pp.
8571–8580. Curran Associates, Inc., 2018.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. Google’s multilingual neural
machine translation system: Enabling zero-shot translation. Transactions of the Association for
Computational Linguistics, 5:339–351, 2017.

Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and

Jakob Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.

Michael Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM (JACM),

45(6):983–1006, 1998.

10

Published as a conference paper at ICLR 2021

Michael Kearns and Leslie Valiant. Cryptographic limitations on learning boolean formulae and

ﬁnite automata. Journal of the ACM (JACM), 41(1):67–95, 1994.

Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of

halfspaces. Journal of Computer and System Sciences, 75(1):2–12, 2009.

Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems 32, pp. 8570–8581.
Curran Associates, Inc., 2019.

Sebastian Musslick, Andrew Saxe, Kayhan Özcimder, Biswadip Dey, Greg Henselman, and
Jonathan D Cohen. Multitasking capability versus learning efﬁciency in neural network ar-
chitectures. In CogSci, pp. 829–834, 2017.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed
Text-to-Text Transformer. arXiv:1910.10683 [cs, stat], October 2019.

Oded Regev. On lattices, learning with errors, random linear codes, and cryptography. Journal of the

ACM (JACM), 56(6):1–40, 2009.

Richard Samuels. Is the mind massively modular? 2006.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538, 2017.

Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks.

In Advances in neural information processing systems, pp. 5514–5522, 2017.

Michel Talagrand. Sharper bounds for gaussian and empirical processes. The Annals of Probability,

pp. 28–76, 1994.

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small

and practical bert models for sequence labeling. arXiv preprint arXiv:1909.00100, 2019.

Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and

the closest pair problem. Journal of the ACM (JACM), 62(2):1–45, 2015.

Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017.

11

Published as a conference paper at ICLR 2021

A THEORETICAL RESULTS

A.1 KERNEL LEARNING BOUNDS

In this section, we develop the theory of learning analytic functions. For a given function g, we deﬁne
a parameter Mg related to the sample complexity of learning g with small error with respect to a
given loss function:
Deﬁnition 3. Fix a learning algorithm, and a 1-Lipschitz loss function L. For a function g over a
distribution of inputs D, a given error scale (cid:15), and a conﬁdence parameter δ, let the sample complexity
ng,D((cid:15), δ) be the smallest integer such that when the algorithm is given ng,D((cid:15), δ) i.i.d. examples
of g on D, with probability greater than 1 − δ, it produces a trained model ˆg with generalization
error Ex∼D[L(g(x), ˆg(x))] less than (cid:15). Fix a constant C > 0. We say g is efﬁciently learned by
the algorithm (w.r.t. C) if there exists a constant Mg (depending on g) such that for all (cid:15), δ, and
distributions D on the inputs of g, ng,D((cid:15), δ) ≤ C([Mg + log(δ−1)]/(cid:15)2).

For example, it is known (Talagrand (1994)) that there exists a suitable choice of C such that empirical
risk minimization for a class of functions efﬁciently learns those functions with Mg at most the
VC-dimension of that class.

Previous work focused on computing Mg, for functions deﬁned on the unit sphere, for wide neural
networks trained with SGD. We extend the bounds derived in Arora et al. (2019a) to analytic functions,
and show that they apply to kernel learning methods as well as neural networks.

The analysis in Arora et al. (2019a) focused on the case of training the hidden layers of wide networks
with SGD. We ﬁrst show that these bounds are more general and in particular apply to the case where
only the ﬁnal layer weights are trained (corresponding to the NNGP kernel in Lee et al. (2019)), and
therefore our results will apply to general kernel learning as well. The proof strategy consists of
showing that ﬁnite-width networks have a sensible inﬁnite-width limit, and showing that training
causes only a small change in parameters of the network.

Let m be the number of hidden units, and n be the number of data points. Let y be the n × 1
dimensional vector of training outputs. Let h be a n × m random matrix denoting the activations of
the hidden layer (as a function of the weights of the lower layer) for all n data points. We will ﬁrst
show the following:
Theorem 6. For sufﬁciently large m, a function g can be learned efﬁciently in the sense of Deﬁnition
3 by training the ﬁnal layer weights only with SGD, where the constant Mg given by

where we deﬁne H∞ as

Mg ≤ yT(H∞)−1y

H∞ = E[hhT]

which is the NNGP kernel from Lee et al. (2019).

(4)

(5)

We require some technical lemmas in order to prove the theorem. We ﬁrst need to show that H∞ is,
with high probability, invertible. If K(x, x(cid:48)), the kernel function which generates H∞ is given by a
inﬁnite Taylor series in x · x(cid:48) it can be argued that H∞ has full rank for most real world distributions.
For example, the ReLU activation this holds as long as no two data points are co-linear (see Deﬁnition
5.1 in Arora et al. (2019a)). We can prove this more explicitly in the following lemma:
Lemma 2. If all the n data points x are distinct and the Taylor series of K(x, x(cid:48)) in x · x(cid:48) has
positive coefﬁcients everywhere then H∞ is not singular.

Proof. First consider the case where the input x is a scalar. Since the Taylor series corresponding
to K(x, x(cid:48)) consists of monomials of all degrees of xx(cid:48), we can view it as some inner product in
a kernel space induced by the function φ(x) = (1, x, x2, . . .), where the inner product is diagonal
(but with potentially different weights) in this basis. For any distinct set of inputs {x1, .., xn} the
set of vectors φ(xi) are linearly independent. The ﬁrst n columns produce the Vandermonde matrix
obtained by stacking rows 1, x, x, ..., xn−1 for n different values of x, which is well known to be
non-singular (since a zero eigenvector would correspond to a degree n − 1 polynomial with n distinct
roots {x1, .., xn}).

12

Published as a conference paper at ICLR 2021

This extends to the case of multidimensional x if the values, projected along some dimension, are
distinct. In this case, the kernel space corresponds to the direct sum of copies of φ applied elementwise
to each coordinate xi. If all the points are distinct and and far apart from each other, the probability
that a given pair coincides under random projection is negligible. From a union bound, the probability
that a given pair coincide is also bounded – so there must be directions such that projections along
that direction are distinct. Therefore, H∞ can be considered to be invertible in general.

As m → ∞, hhT concentrates to its expected value. More precisely, (hhT)−1 approaches (H∞)−1
for large m if we assume that the smallest eigenvalue λmin(H∞) ≥ λ0, which from the above lemma
we know to be true for ﬁxed n. (For the ReLU NTK the difference becomes negligible with high
probability for m = poly(n/λ0) Arora et al. (2019a).) This allows us to replace hhT with H∞ in
any bounds involving the former.
We can get learning bounds in terms of hhT by studying the upper layer weights w of the network
after training. After training, we have y = w · h. If hhT is invertible (which the above arguments
show is true with high probability for large m), the following lemma holds:
Lemma 3. If we initialize a random lower layer and train the weights of the upper layer, then there
exists a solution w with norm (cid:112)yT(hhT)−1y.

Proof. The minimum norm solution to y = wTh is

w∗ = (hTh)−1hTy.

(6)

The norm squared (w∗)Tw∗ of this solution is given by yTh(hTh)−2hTy.
We claim that h(hTh)−2hT = (hhT)−1. To show this, consider the SVD decomposition h =
USVT. Expanding we have

h(hTh)−2hT = USVT(VS2VT)−2VSUT.

(7)

Evaluating the right hand side gets us US−2UT = (hhT)−1.
Therefore, the norm of the minimum norm solution is yT(hhT)−1y.

We can now complete the proof of Theorem 6.

Proof of Theorem 6. For large m, the squared norm of the weights approaches yT(H∞)−1y. Since
the lower layer is ﬁxed, the optimization problem is linear and therefore convex in the trained weights
w. Therefore SGD with small learning rate will reach this optimal solution. The Rademacher
complexity of this function class is at most (cid:112)yT(H∞)−1y which we at most by (cid:112)Mg where Mg is
an upper bound on yT(H∞)−1y. The optimal solution has 0 train error based on the assumption that
H∞ is full rank and the generalization error will be no more than O(
) which is at most
(cid:15) if we use at least n = Ω(Mg/(cid:15)2) training samples - note that this is identical to the previous results
for training the hidden layer only Arora et al. (2019a); Du et al. (2019).

(cid:113) yT(H∞)−1y
2n

A.2 LEARNING ANALYTIC FUNCTIONS

Now, we derive our generalization bounds for single variate functions. We use Theorem 6 to prove
the following corollary, a more general version of Corollary 6.2 proven in Arora et al. (2019a) for
wide ReLU networks with trainable hidden layer only:
Corollary 3. Consider the function g : Rd → R given by:

g(x) =

(cid:88)

ak(βT

k x)k

(8)

k
Then, if g is restricted to ||x|| = 1, and the NTK or NNGP kernel can be written as H(x, x(cid:48)) =
(cid:80)
k bk(x · x(cid:48))k, the function can be learned efﬁciently with a wide one-hidden-layer network in the
sense of Deﬁnition 3 with

(cid:112)Mg =

(cid:88)

b−1/2
k

|ak|||βk||k
2

(9)

k

13

Published as a conference paper at ICLR 2021

up to g-independent constants of O(1), where βk ≡ ||βk||2. In the particular case of a ReLU network,
the bound is

(cid:112)Mg =

(cid:88)

k|ak|||βk||k
2

(10)

k

The original corollary applied only to networks with trained hidden layer, and the bound on the ReLu
network excluded odd monomials of power greater than 1.

Proof. The extension to NNGP follows from Theorem 6, which allows for the application of the
arguments used to prove Corollary 6.2 from Arora et al. (2019a) (particularly those found in Appendix
E).

The extension of the ReLu bound to odd powers can be acheived with the following modiﬁcation.
consider appending a constant component to the input x so that the new input to the network is
(x/

2). The kernel then becomes:

2, 1/

√

√

K(x, x(cid:48)) =

x · x(cid:48) + 1
4π

(cid:18)

π − arccos

(cid:18) x · x(cid:48) + 1
2

(cid:19)(cid:19)

.

(11)

Re-writing the power series as an expansion around x · x(cid:48) = 0, we have terms of all powers. An
asymptotic analysis of the coefﬁcients using known results shows that coefﬁcients bk are asymp-
totically O(k−3/2) - meaning in Equation 10 applies to these kernels, without restriction to even
k.

Equation 9 suggests that kernels with slowly decaying (but still convergent) bk will give the best
bounds for learning polynomials. Many popular kernels do not meet this criteria. For example, for
inputs on the sphere of radius r, the Gaussian kernel K(x, x(cid:48)) = e−||x−x(cid:48)||2/2 can be written as
√
K(x, x(cid:48)) = e−r2
k!, which increases rapidly with k. This provides
theoretical justiﬁcation for the empirically inferior performance of the Gaussian kernel which we will
present in Section B.2.

. This has b−1/2

= er2/2

ex·x(cid:48)

k

Guided by this theory, we focus on kernels where b−1/2
modiﬁed ReLu meets this criterion, as well as hand-crafted kernels of the form

≤ O(k), for all k (or, bk ≥ O(k−2)). The

k

K(x, x(cid:48)) =

(cid:88)

k−s(x · x(cid:48))k

(12)

k
with s ∈ (1, 2] is a valid slowly decaying kernel on the sphere. We call these slowly decaying kernels.
We note that by Lemma 3, the results of Corollary 3 apply to networks with output layer training
only, as well as kernel learning (which can be implemented by training wide networks).

Using the extension of Corollary 3 to odd powers, we ﬁrst show that analytic functions with appropri-
ately bounded norms can be learnt.
Theorem 7. Let g(y) be a function analytic around 0, with radius of convergence Rg. Deﬁne the
auxiliary function ˜g(y) by the power series

˜g(y) =

∞
(cid:88)

|ak|yk

(13)

k=0
where the ak are the power series coefﬁcients of g(y). Then the function g(β · x), for some ﬁxed
vector β ∈ Rd with ||x|| = 1 is efﬁciently learnable in the sense of Deﬁnition 3 using a model with a
slowly decaying kernel K with

(cid:112)Mg = β˜g(cid:48)(β) + ˜g(0)

(14)

if the norm β ≡ ||β||2 is less than Rg.

Proof. We ﬁrst note that the radius of convergence of the power series of ˜g(y) is also Rg since g(y)
is analytic. Applying Equation 10, pulling out the 0th order term, and factoring out β, we get
∞
(cid:88)

(cid:112)Mg = |a0| + β

k|ak|βk = β˜g(cid:48)(β) + ˜g(0)

(15)

since β < Rg.

k=1

14

Published as a conference paper at ICLR 2021

The tilde function is the notion of complexity which measures how many samples we need to learn
a given function. Informally, the tilde function makes all coefﬁcients in the Taylor series positive.
The sample complexity is given by the value of the function at 1 (in other words, the L1 norm of the
coefﬁcients in the Taylor series). For a multivariate function g(x), we deﬁne its tilde function ˜g(y)
by substituting any inner product term (cid:104)α, x(cid:105) by a univariate y. The above theorem can then also be
generalized to multivariate analytic functions:
Lemma 4. Given a collection of p vectors βi in Rd, the function f (x) = (cid:81)p
learnable with

i=1 βi · x is efﬁciently

(cid:112)Mf = p

(cid:89)

βi

(16)

where βi ≡ ||βi||2.

i

Proof. The proof of Corollary 6.2 in Arora et al. (2019a) relied on the following statement: given
positive semi-deﬁnite matrices A and B, with A (cid:23) B, we have:

PBA−1PB (cid:22) B+

(17)

where + is the Moore-Penrose pseudoinverse, and P is the projection operator.

We can use this result, along with the Taylor expansion of the kernel and a particular decomposition
of a multivariate monomial in the following way. Let the matrix X to be the training data, such that
the αth column xi is a unit vector in Rd. Given K ≡ XTX, the matrix of inner products, the Gram
matrix H∞ of the kernel can be written as

H∞ =

∞
(cid:88)

k=0

bkK◦k

(18)

where ◦ is the Hadamard (elementwise) product. Consider the problem of learning the function
f (x) = (cid:81)p

i=1 βi · x. Note that we can write:

f (X) = (X(cid:12)k)T ⊗k

i=1 βi.

(19)

Here ⊗ is the tensor product, which for vectors takes an n1-dimensional vector and an n2 dimensional
vector as inputs vectors and returns a n1n2 dimensional vector:

w ⊗ v =





















w1v1
w1v2
· · ·
w1vn2
w2v1
· · ·
wn1vn2

.

(20)

The operator (cid:12) is the Khatri-Rao product, which takes an n1 × n3 matrix A = (a1, · · · , an3 ) and a
n2 ⊗ n3 matrix B = (b1, · · · , bn3) and returns the n1n2 × n3 dimensional matrix

For p = 2, this form of f (X) can be proved explicitly:

A (cid:12) B = (a1 ⊗ b1, · · · , an3 ⊗ bn3).

(X(cid:12)2)Tβ1 ⊗ β2 = (x1 ⊗ x1, · · · , xP ⊗ xP )T β1 ⊗ β2.

The αth element of the matrix product is

(xα ⊗ xα) · (β1 ⊗ β2) = (β1 · xα)(β2 · xα)

which is exactly f (xα). The formula can be proved for p > 2 by ﬁnite induction.

(21)

(22)

(23)

With this form of f (X), we can follow the steps of the proof in Appendix E of Arora et al. (2019a),
which was written for the case where the βi were identical:
yT(H∞)−1y = (⊗p

i=1βi)TX(cid:12)p(H∞)−1(X(cid:12)p)T ⊗p

i=1 βi.

(24)

15

Published as a conference paper at ICLR 2021

Using Equation 17, applied to K◦p, we have:

yT(H∞)−1y ≤
p (⊗p
b−1

i=1βi)TX(cid:12)pPK◦p (K◦p)+PK◦p (X(cid:12)p)T ⊗p
Since the X(cid:12)p are eigenvectors of PK◦p with eigenvalue 1, and X(cid:12)p(K◦p)+(X(cid:12)p)T = PX(cid:12)p , we
have:

i=1 βi

.

(25)

yT(H∞)−1y ≤ b−1

p (⊗p

i=1βi)TPX(cid:12)p ⊗p

i=1 βi

yT(H∞)−1y ≤ b−1

p

p
(cid:89)

i=1

βi · βi.

For the slowly decaying kernels, bp ≥ p−2. Therefore, we have (cid:112)yT(H∞)−1y ≤ (cid:112)Mf for
(cid:112)Mf = p

(cid:89)

βi

where βi ≡ ||βi||2, as desired.

i

This leads to the following generalization of Theorem 7:
Theorem 8. Let g(x) be a function with multivariate power series representation:

(cid:88)

(cid:88)

g(x) =

k
(cid:89)

av

(βv,i · x)

(26)

(27)

(28)

(29)

v∈Vk
where the elements of Vk index the kth order terms of the power series. We deﬁne ˜g(y) = (cid:80)
with coefﬁcients

i=1

k

k ˜akyk

˜ak =

(cid:88)

v∈Vk

|av|

k
(cid:89)

i=1

βv,i.

(30)

If the power series of ˜g(y) converges at y = 1 then with high probability g(x) can be learned
efﬁciently in the sense of Deﬁnition 3 with (cid:112)Mg = ˜g(cid:48)(1) + ˜g(0).

Proof. Follow the construction in Theorem 7, using Lemma 4 to get bounds on the individual terms.
Then sum and evaluate the power series of ˜g(cid:48)(1) to arrive at the bound.

Remark 1. Note that the ˜g function deﬁned above for multivariate functions depends on the repre-
sentation, i.e. choice of the vectors β. Therefore to be fully formal ˜g(y) should instead be ˜gβ(y). For
clarity, we drop β from the expression ˜gβ(y) and it is implicit in the ˜g notation.
Remark 2. If g(x) can be approximated by some function gapp such that |g(x) − gapp| ≤ (cid:15)(cid:48) for all x
in the unit ball, then Theorem 8 can be used to learn g(x) within error (cid:15)(cid:48) + (cid:15) with sample complexity
O(Mgapp/(cid:15)2).

To verify Remark 2, note that we are doing regression on the upper layer of the neural network,
where the lower layer is random. So based on gapp there exists a low-norm solution for the regression
coefﬁcients for the upper layer weights which gets error at most (cid:15)(cid:48). If we solve the regression under
the appropriate norm ball, then we get training error at most (cid:15)(cid:48), and the generalization error will be at
most (cid:15) with O(Mgapp /(cid:15)2) samples.
We can also derive the equivalent of the product and chain rule for function composition.

Proof of Corollary 1. Consider the power series of g(x)h(x), which exists and is convergent since
each individual series exists and is convergent. Let the elements of Vj,g and Vk,h index the jth order
terms of g and the kth order terms of h respectively. The individual terms in the series look like:

avbw

j
(cid:89)

j(cid:48)=1

k
(cid:89)

(βv,j(cid:48) · x)

(βw,k(cid:48) · x) for v ∈ Vj,g, w ∈ Vk,h

(31)

k(cid:48)=1

16

Published as a conference paper at ICLR 2021

with bound

(j + k)|av||bw|

j
(cid:89)

βv,j(cid:48)

k
(cid:89)

j(cid:48)=1

k(cid:48)=1

βw,k(cid:48) for v ∈ Vj,g, w ∈ Vk,h

(32)

for all terms with j + k > 0 and ˜g(0)˜h(0) for the term with j = k = 0.

Distribute the j + k product, and ﬁrst focus on the j term only. Summing over all the Vk,h for all k,
we get

(cid:88)

(cid:88)

k

w∈Vk,h

j|av||bw|

j
(cid:89)

βv,j(cid:48)

k
(cid:89)

j(cid:48)=1

k(cid:48)=1

βw,k(cid:48) =

|av|

j
(cid:89)

j(cid:48)=1

βv,j(cid:48) ˜h(1).

(33)

Now summing over the j and Vj,g we get ˜g(cid:48)(1)˜h(1). If we do the same for the k term, after summing
we get ˜g(1)˜h(cid:48)(1). These bounds add and we get the desired formula for (cid:112)Mgh, which, up to the
additional ˜g(0)˜h(0) term looks is the product rule applied to ˜g and ˜h.

One immediate application for this corollary is the product of many univariate analytic functions. If
we deﬁne

(cid:89)

G(x) =

gi(βi · x)

(34)

i
where each of the corresponding ˜gi(y) have the appropriate convergence properties, then G is
efﬁciently learnable with bound MG given by

(cid:112)

MG =

d
dy

(cid:89)

i

(cid:12)
(cid:12)
(cid:12)
˜gi(βiy)
(cid:12)
(cid:12)y=1

(cid:89)

+

˜gi(0).

i

Proof of Corollary 2. Writing out g(h(x)) as a power series in h(x), we have:

g(h(x)) =

∞
(cid:88)

k=0

ak(h(x))k.

(35)

(36)

We can bound each term individually, and use the k-wise product rule to bound each term of (h(x))k.
Doing this, we have:

(cid:112)Mg◦h =

∞
(cid:88)

k=1

k|ak|˜h(cid:48)(1)˜h(1)k−1 +

∞
(cid:88)

k=0

|ak|˜h(0)k.

(37)

Factoring out ˜h(cid:48)(1) from the ﬁrst term and then evaluating each of the series gets us the desired
result.

The following corollary considers the case where the function g(x) is low-degree and directly follows
from Theorem 8.
Fact 1. The following facts about the tilde function will be useful in our analysis—

1. Given a multivariate analytic function g(x) of degree p for x in the d-dimensional unit ball,
there is a function ˜g(y) as deﬁned in Theorem 8 such that g(x) is learnable to error (cid:15) with
O(p˜g(1)/(cid:15)2) samples.

2. The tilde of a sum of two functions is at most the sum of the tilde of each of the functions, i.e.

if f = g + h then ˜f (y) ≤ ˜g(y) + ˜h(y) for y ≥ 0.

3. The tilde of a product of two functions is at most the product of the tilde of each of the

functions, i.e. if f = g · h then ˜f (y) ≤ ˜g(y)˜h(y) for y ≥ 0.

17

Published as a conference paper at ICLR 2021

4. If g(x) = f (αx), then ˜g(y) ≤ ˜f (αy) for y ≥ 0.

5. If g(x) = f (x + c) for some (cid:107)c(cid:107) ≤ 1, then ˜g(y) ≤ ˜f (y + 1) for y ≥ 0. By combining this
with the previous fact, if g(x) = f (α(x − c)) for some (cid:107)c(cid:107) ≤ 1, then ˜g(1) ≤ ˜f (2α).

To verify the last part, note that in the deﬁnition of ˜g we replace (cid:104)β, x(cid:105) with y. Therefore, we will
have an additional (cid:104)β, c(cid:105) term when we compute the tilde function for g(x) = f (x + c). As (cid:107)c(cid:107) ≤ 1,
the additional term is at most 1.

The following lemma shows how we can approximate the indicator 1(x > α) with a low-degree
polynomial if x is at least γ/2 far away from α. We will use this primitive several times to construct
low-degree analytic approximations of indicator functions. The result is based on the following
simple fact.
Fact 2. If the Taylor series of g(x) is exponentially decreasing, then we can truncate it at degree
O(log(1/(cid:15))) to get (cid:15) error. We will use this fact to construct low-degree approximations of functions.
Lemma 5. Given a scalar x, let the function
(cid:16)

(cid:17)(cid:17)

(cid:16)

Φ(x, γ, (cid:15), α) = (1/2)

1 + erf

(x − α)c(cid:112)log(1/(cid:15))/γ

for some constant c. Let Φ(cid:48)(x, γ, (cid:15), α) be the function Φ(x, γ, (cid:15), α) with its Taylor series truncated at
degree O(log(1/(cid:15))/γ). Then for |α| < 1,

Φ(cid:48)(x, γ, (cid:15), α) =

(cid:26)(cid:15)

x ≤ α − γ/2,
1 − (cid:15) x ≥ α + γ/2.

Also, MΦ(cid:48) is at most eO((log(1/(cid:15))/γ2)).

Proof. Note that Φ(x, γ, (cid:15), α) is the cumulative distribution function (cdf) of a normal distribution
with mean α and standard deviation O(γ/(cid:112)log(1/(cid:15))). Note that at most (cid:15)/100 of the probability
mass of a Gaussian distribution lies more than O((cid:112)log(1/(cid:15))) standard deviations away from the
mean. Therefore,

Φ(x, γ, (cid:15), α) =

Note that

(cid:26)(cid:15)/100

x ≤ α − γ/2,
1 − (cid:15)/100 x ≥ α + γ/2.

erf(x) =

=

2
√
π

2
√
π

(cid:90) x

e−t2

dt

0
(cid:32) ∞
(cid:88)

i=0

(−1)ix2i+1
i!(2i + 1)

(cid:33)

.

Therefore, the coefﬁcients in the Taylor series expansion of erf((x − α)c(cid:112)log(1/(cid:15))/γ)) in terms
of (x − α) are smaller than (cid:15) for i > O(log(1/(cid:15))/γ2) and are geometrically decreasing hence-
forth. Therefore, we can truncate the Taylor series at degree O(log(1/(cid:15))/γ2) and still have an O((cid:15))
approximation. Note that for f (x) = erf(x),
(cid:90) y

˜f (y) ≤

2
√
π

0

et2

dt ≤

yey2

≤ eO(y2).

2
√
π

After shifting by α and scaling by O((cid:112)log(1/(cid:15))/γ), we get ˜Φ(cid:48)(y) = eO((y+α)2 log(1/(cid:15))/γ2). For
x = 1, this is at most eO(log(1/(cid:15))/γ2). Hence the result now follows by Fact 1.

A.3 LEARNABILITY OF CLUSTER BASED DECISION NODE

In the informal version of the result for learning cluster based decisions we assumed that the task-codes
c are preﬁxed to the input datapoints, which we refer to as xinp. For the formal version of the theorem,
we use a small variation. The task code and the input c, xinp gets mapped to x = c + xinp · (r/3) for

18

Published as a conference paper at ICLR 2021

some constant r < 1/6. Since xinp resides on the unit sphere, x will be distance at most (r/3) from
the center it gets mapped to. Note that the overall function f can be written as follows,

f (x) =

k
(cid:88)

j=1

1 (cid:0)(cid:107)x − cj(cid:107)2 ≤ (r/2)2(cid:1) fj ((x − cj)/(r/3))

where fj is the function corresponding to the center cj. The main idea will be to show that the
indicator function can be expressed as an analytic function.

Theorem 9. (formal version of Theorem 2) Assume that d ≥ 10 log k (otherwise we can pad by extra
coordinates to increase the dimensionality). Then we can ﬁnd k centers in the unit ball which are at
least r apart, for some constant r. Let

f (x) =

k
(cid:88)

j=1

1 (cid:0)(cid:107)x − cj(cid:107)2 ≤ (r/2)2(cid:1) fj ((x − cj)/(r/3))

where fj is the function corresponding to the center cj. Then if each fj is a degree p polynomial,
Mf of the function f is p · poly(k/(cid:15)) (cid:80) ˜fj(6/r) ≤ p · poly(k/(cid:15))(6/r)p (cid:80) ˜fj(1).

Proof. Let

fapp(x) =

k
(cid:88)

j=1

Φ(cid:48) (cid:0)(cid:107)x − cj(cid:107)2, (r/2)2, (cid:15)/k, (r/4)2(cid:1) fj ((x − cj)/(r/3))

where Φ(cid:48) is deﬁned in Lemma 5. Let

Ij(x) = Φ(cid:48)((cid:107)x − cj(cid:107)2, (r/2)2, (cid:15)/k, (r/4)2).

The indicator Ij(x) checks if (cid:107)x − cj(cid:107) is a constant fraction less than r/2, or a constant fraction more
than r/2. Note that if x is from a different cluster, then (cid:107)x − cj(cid:107) is at least some constant, and hence
Ij(x) is at most (cid:15)/k. The contribution from k such clusters would be at most (cid:15). If (cid:107)x − cj(cid:107) < (cid:15)/k,
then the indicator is at least 1 − O((cid:15)/k). Hence as fapp is an O((cid:15))-approximation to f , by Remark 2
it sufﬁces to show learnability of fapp.

If y = (cid:104)x, cj(cid:105) and assuming x and the centers cj are all on unit sphere,

˜Ij(y) = ˜Φ(cid:48)(2 + 2y, r/3, (cid:15)/k, r/3) ≤ eO(log(k/(cid:15)) = poly(k/(cid:15)).

By Fact 1,

As fj are at most degree p,

˜f (y) ≤ poly(k/(cid:15))

(cid:88)

˜fj(6/r).

j

˜f (y) ≤ poly(k/(cid:15))

(cid:88)

˜fj(6/r) ≤ p · poly(k/(cid:15))(6/r)p (cid:88) ˜fj(1).

j

Corollary 4. The previous theorem implies that we can also learn f where f is a lookup table with
Mf = poly(k/(cid:15)), as long as the keys ci are well separated. Note that as long as the keys ci are
distinct (for example, names) we can hash them to random vectors on a sphere so that they are all
well-separated.

Note that the indicator function for the informal version of Theorem 9 stated in the main body is
the same as that for the lookup table in Corollary 4. Therefore, the informal version of Theorem 9
follows as a Corollary of Theorem 9.

19

Published as a conference paper at ICLR 2021

A.4 LEARNABILITY OF FUNCTIONS DEFINED ON LEAVES OF A DECISION TREE

We consider decision trees on inputs drawn from {−1, 1}d. We show that such a decision tree g can
be learnt with Mg ≤ O(dh). From this section onwards, we view the combined input c, x as x.

The decision tree g can be written as follows,

g(x) =

(cid:88)

j

Ij(x)vj,

where the summation runs over all the leaves, Ij(x) is the indicator function for leaf j, and vj ∈
[−1, 1] is the constant value on the leaf j. We scale the inputs by
d to make them lie on the unit
sphere, and hence each coordinate of x is either ±1/

√

√

d.

Let the total number of leaves in the decision tree be B. The decision tree indicator function of the
j-th leaf can be written as the product over the path of all internal decision nodes. Let jl be variable
at the l-th decision node on the path used by the j-th leaf. We can write,

Ij(x) =

(cid:89)

l

(ajl xjl + bjl ) ,

√

√

√

√

d/2,

d, 1/

d} and ajl ∈ {−

where each xjl ∈ {−1/
d/2} and bjl ∈ {−1/2, 1/2}. Note that
the values of ajl and bj,l are chosen depending on whether the path for the j-th leaf choses the left
child or the right child at the l-th decision variable. For ease of exposition, the following theorem is
stated for the case where the leaf functions are constant functions, and the case where there are some
analytic functions at the leaves also follows in the same way.
Theorem 10. If a function is given by g(x) = (cid:80)B
in the above form, with tree depth h, then Mg is at most O(dh).

j=1 Ij(x)vj, where Ij(x) is a leaf indicator function

Proof. Note that

˜g(y) ≤

≤

(cid:88) ˜Ij(y)|vj|
(cid:16)√
(cid:88) (cid:89)

dy/2 + 1/2

(cid:17)

=⇒ ˜g(1) ≤ 2h(

l
√
d/2 + 1/2)h ≤ dh.

As the degree of g is at most h, therefore Mg ≤ h˜g(1) ≤ hdh.
Remark 3. Note that by Theorem 10 we need O (cid:0)(log k)log k(cid:15)−2(cid:1) samples to learn a lookup table
based on a decision tree. On the other hand, by Corollary 4 we need poly(k/(cid:15)) samples to learn a
lookup table using cluster based decision nodes. This shows that using a hash function to obtain a
random O(log k) bit encoding of the indexes for the k lookups is more efﬁcient than using a ﬁxed
log k length encoding for the k lookups.

We also prove a corresponding lower bound in Theorem 14 which shows that dΩ(h) samples are
necessary to learn decision trees of depth h.

We will now consider decision trees where the branching is based on the inner product of x with
some direction βj,l. Assume that there is a constant gap for each decision split, then the decision tree
indicator function can be written as,

Ij(x) =

(cid:89)

l

1((cid:104)x, βj,l(cid:105) > αj,l).

Theorem 11. (formal version of Theorem 4) A decision tree of depth h where every node partitions
in a certain direction with margin γ can be written as g(x) = (cid:80)B

j=1 Ij(x)fj(x), then the ﬁnal
(cid:88) ˜fj(1),

Mg = eO(h log(1/(cid:15))/γ2)(p + h log 1/(cid:15))

where p is the maximum degree of fj.

20

Published as a conference paper at ICLR 2021

Proof. Deﬁne gapp,

gapp(x) =

B
(cid:88)

j=1

ΠlΦ(cid:48)((cid:104)x, βj,l(cid:105), γ, (cid:15)/h, αj,l)fj(x)

where Φ(cid:48) is as deﬁned in Lemma 5. Note that for all y = 1,

˜Φ(cid:48)(1, γ, (cid:15)/h, αj,l) ≤ eO(log(1/(cid:15))/γ2).

Therefore,

˜gapp(1) ≤

B
(cid:88)

j=1

Πl ˜Φ(cid:48)(1, γ, (cid:15)/h, αj,l) ˜fj(1),

≤ eO(log(1/(cid:15))/γ2) (cid:88) ˜fj(1).

Note that the degree of gapp is at most O(p + h log(1/(cid:15))/γ2). Therefore,

Mgapp ≤ eO(h log(1/(cid:15))/γ2)(p + h log(1/(cid:15))/γ2)

(cid:88) ˜fj(1).

By Remark 2, learnability of g follows from the learnability of its analytic approximation gapp.

A.5 GENERALIZED DECISION PROGRAM

In this section, instead a decision tree, we will consider a circuit with fan-out 1, where each gate
(node) evaluates some function of the values returned by its children and the input x. A decision tree
is a special case of such circuits in which the gates are all switches.

So far, the function outputs were univariate but we will now generalize and allow multivariate (vector)
outputs as well. Hence the functions can now evaluate and return data structures, represented by
vectors. We assume that each output is at most d dimensional and lies in the unit ball.
Deﬁnition 4. For a multivariate output function f , we deﬁne ˜f (y) as the sum of ˜fi(y) for each of
the output coordinates fi.
Remark 4. Theorem 9 , 10 and 11 extend to the multivariate output case. Note that if each of the
individual functions has degree at most p, then the sample complexity for learning the multivariate
output f is at most O(p ˜f (1)/(cid:15)2)) (where the multivariate tilde function is deﬁned in Deﬁnition 4).

We now deﬁne a generalized decision program and the class of functions that we support.
Deﬁnition 5. We deﬁne a generalized decision program to be a circuit with fan-out 1 (i.e., a tree
topology) where each gate evaluates a function of the values returned by its children and the input x,
and the root node evaluates the ﬁnal output. All gates, including those at the leaves, have access to
the input x. We support the following gate operations. Let h be the output of a gate, let each gate
have at most k children, and let {f1, . . . , fk} be the outputs of its children.

1. Any analytic function of the child gates of degree at most p, including sum h = (cid:80)k

i=1 fi

and product of p terms h = Πp

i=1fi.

2. Margin based switch (decision) gate with children {f1, f2}, some constant margin γ, vector

β and constant α,

h =

(cid:26)f1
f2

if (cid:104)β, x(cid:105) − α ≤ −γ/2,
if (cid:104)β, x(cid:105) − α ≥ γ/2.

3. Cluster based switch gate with k centers {c(1), . . . , c(k)}, with separation r for some
constant r, and the output is fi if (cid:107)x − c(i)(cid:107) ≤ r/3. A special case of this is a look-up table
which returns value vi if x = c(i), and 0 if x does not match any of the centers.

4. Create a data structure out of separate ﬁelds by concatenation such as constructing a tuple
[f1, . . . , fk] which creates a single data structure out of its children, or extract a ﬁeld out of
a data structure.

21

Published as a conference paper at ICLR 2021

5. Given a table T with k entries {r1, . . . , rk}, a Boolean-valued function p and an analytic
function f , SQL queries of the form SELECT SUM f(r_i), WHERE p(r_i, x).
Here, we assume that f has bounded value and p can be approximated by an analytic
function of degree at most p.

6. Compositions of functions, h(x) = f (g(x)).

First, we note that all of the above operators can be approximated by low-degree polynomials.
Claim 1. If p ≤ O(log(k/(cid:15))), each of the above operators in the generalized decision program can
be expressed as a polynomial of degree at most O(log(k/(cid:15))), where k is maximum out-degree of any
of the nodes.
Remark 5. Note that for the SQL query, we can also approximate other aggregation operators apart
from SUM, such as MAX or MIN. For example, to approximate MAX of x1, . . . , xk up to (cid:15) where the
input lies between [0, 1] we can ﬁrst write it as

MAX(x1, . . . , xk) = (cid:15)

(cid:32)

(cid:88)

(cid:88)

1

j

i

(cid:33)

(1(xi > (cid:15)j) > 1/2)

,

and then approximate the indicators by analytic functions.

Lemma 6 shows how we can compute the tilde function of the generalized decision program.
Lemma 6. The tilde function for a generalized decision program can be computed recursively with
the following steps:

1. For a sum gate h = f + g, ˜h(y) = ˜f (y) + ˜g(y).

2. For a product gate, h = f.g, ˜h(y) = ˜f (y) · ˜g(y).

3. For a margin based decision gate (switch) with children f and g, h = Ilef tf + (1 − Ilef t)g
and ˜h(y) = ˜Ilef t( ˜f (y) + ˜g(y)) + ˜g(y). Here Ilef t is the indicator for the case where the
left child is chosen.

4. For cluster based decision gate (switch) with children {f1, ..., fk}, ˜h(y) ≤ (cid:80)

Here Ii is the indicator for the cluster corresponding to the i-th child.

˜Ii

˜fi(6y/r).

i

5. For a look-up table with k key-values, ˜h(y) ≤ k ˜I(y) as long as the (cid:96)1 norm of each key-value

is at most 1.

6. Creating a data structure out of separate ﬁelds can be done by concatenation, and ˜h for the
result is at most sum of the original tilde functions. Extracting a ﬁeld out of a data structure
can also be done in the same way.

h

over

7. Given an analytic

function f
table
a

for a SQL
operator
representing
SELECT SUM f(r_i), WHERE p(r_i, x),
in
=
(cid:80)
˜Ip,ri (y), where Ip,ri is the indicator for p(ri, x). For
example, x here can denote some threshold value to be applied to a column of the table, or
selecting some subset of entries (in Fig. 1, x is the zip-code).

and a Boolean function p,
entries
{r1, . . . , rk}
or

i f (ri)p(ri, x), ˜h(y) ≤ (cid:80)

other words h

T with

k

i

8. For h(x) = f (g(x)), ˜h(y) ≤ ˜f (˜g(y)).

All except for the last part of the above Lemma directly follow from the results in the previous
sub-section. Below, we prove the result for the last part regarding function compositions.
Lemma 7. Assume that all functions have input and output dimension at most d. If f and g are
two functions with degree at most p1 and p2, then h(x) = f (g(x)) has degree at most p1p2 and
˜h(y) ≤ ˜f (˜g(y)).

Proof. Note that this follows if f and g are both scalar outputs and inputs. Let g(x) =
(g1(x), ..., gd(x)). Let us begin with the case where f = (cid:104)β, x(cid:105), where (cid:107)β(cid:107) = 1. Then

22

Published as a conference paper at ICLR 2021

i |βi|˜gi(y) ≤ (cid:80)

˜h(y) = (cid:80)
i=1(cid:104)βi, x(cid:105), ˜h(y) ≤ ˜g(y)p1 ≤ ˜f (˜g(y)).
The same argument works when we take a linear combination, and also for a multivariate function f
(as ˜f for a multivariate f is the summation of individual ˜fi, by deﬁnition).

i ˜gi(y) ≤ ˜g(y). When f = Πp1

We now present our result for learning generalized decision programs.
Theorem 12. Let the in-degree of any gate be at most k. The sample complexity for learning the
following classes of generalized decision programs is as follows:

1. If every gate is either a decision node with margin γ, a sum gate, or a lookup of size at most

k, then Mg ≤ eO(h log(1/(cid:15))/γ2)kO(h).

2. For some constant C, if there are at most C product gates with degree at most C, and every
other gate is a decision gate with margin γ or a sum gate with constant functions at the
leaves, then Mg ≤ eO(h log(1/(cid:15))/γ2).

3. Given a function f and a Boolean function p which can be approximated by a polynomial of
degree at most O(log(k/(cid:15))), for a SQL operator g over a table T with k entries {r1, . . . , rk}
representing SELECT SUM f(r_i), WHERE p(r_i, x), Mg ≤ (cid:80)

˜Ip,ri(1).

i

4. Let the function at every gate be an analytic function f of degree at most p and the sum of the
coefﬁcients of f is upper bounded by cp for some constant c. Then note that ˜f (y) ≤ (cy)p
for y ≥ 1. Therefore, the ﬁnal function ˜g(y) ≤ (cky)ph

and hence Mg ≤ (ck)ph

.

Proof. The ﬁrst three claims can be obtained using Lemma 6.

For the ﬁnal claim, consider the ﬁnal polynomial obtained by expanding the function at each gate
in a bottom-up way. We will upper bound ˜g(y) for the overall function g corresponding to the
generalized decision program. ˜g(y) can be upper bounded by starting with ˜f (y) for the leaf nodes f .
For any internal gate i, let gi(x) = fi(fj1(x), . . . , fjp (x)) where fjt are the outputs of the children
of the gate i. We recursively compute ˜gi(y) = ˜fi((cid:80)
˜fjl (y)). Therefore, for a gate with k children
˜gi(y) ≤ (c (cid:80)

l ˜gjl (y))p. Therefore, for the root gate g0, ˜g0(y) ≤ (cky)ph

.

l

Remark 6. Note that the dependence on h is doubly exponential. We show a corresponding lower
bound in Theorem 15 that this is necessary.

Theorem 12 implies that we can learn programs such as the following formal version of Fig. 1
(right)—which involves analytic functions, SQL queries, data structures, and table look-up.
Example 1. Consider the following program:

class Person{

string name;
Address address;
int income;
public string get_zip_code(){

return address.zip_code;

}
init(input_name, input_address, input_income){

name = input_name;
address = input_address;
income = input_income;

}

}
class Address{

int street_number;
string street_name;
string city;
string state;
string zip_code;

23

Published as a conference paper at ICLR 2021

public string get_zip_code(){

return zip_code;

}
init(...){

... # function to create new object with input values

}

}
dictionary name_to_address_table;
dictionary zip_code_to_lat_long; #maps zip_code to tuple of (latitute, longitude)

boolean in_same_zip_code(Person A, Person B){

return A.get_zip_code() == B.get_zip_code();

}

float get_straight_line_distance(Person A, Person B){

lat_longA = zip_code_to_lat_long[A.get_zip_code()];
lat_longB = zip_code_to_lat_long[B.get_zip_code()];
return euclidean_distance(lat_longA, lat_longB);

}

float avg_income_zip_code(string zip_code){

construct SQL table T with income, zip_code from name_to_address_table;
return output of SQL query "SELECT AVG(INCOME) FROM T WHERE ZIP_CODE=zip_code"

}

The following claim follows from Theorem 12.

Claim 2. The above classes and functions can be implemented and learnt using (k/(cid:15))O(log(1/(cid:15)))
samples, where the tables are of size at most k.

Proof. We begin with the in_same_zip_code() function. Note that this is a special case of the
cluster based functions. As in Corollary 4 all attributes such as zip-code are appropriately hashed
such that they are well-separated. We can now test equality by doing an indicator function for a
ball around the zip-code of Person A. The indicator function for a ball can be approximated by a
low-degree polynomial as in the cluster-based branching results in Theorem 9. As the total number
of individuals is at most k, therefore by Theorem 9 the sample complexity is at most poly(k/(cid:15)).

For the avg_income_zip_code() function, we use the SQL query result in Theorem 12. Note
that the indicators are testing equality in the case of our program, and hence as in the previous
case we can use the cluster-based branching result in Theorem 9 to approximate these indicators by
polynomial functions, to obtain a sample complexity of poly(k/(cid:15)).

Finally, we argue that we can learn the get_straight_line_distance() function. Here,
we are composing two functions f and (g1, g2) where f is the distance function and (g1, g2) are
the lookups for the latitude and longitude for Person A and B. By Corollary 4, the lookups have
˜gi(1) ≤ poly(k/(cid:15)). By part 6 of Lemma 6, the tilde for the concatenation is the sum of the tilde for
the individual functions. For computing the Euclidean distance (cid:112)(cid:80)(xi − yi)2, note that the square
root function does not have a Taylor series deﬁned at 0. However, we can use the same analysis as in
the proof for learning the 1/x function in the gravitational law (see Appendix B.1) to get a polynomial
of degree at most O(log(1/(cid:15))), and hence ˜f (y) ≤ (O(y))log(1/(cid:15)). Thus using the composition rule
in Lemma 6, the sample complexity is (k/(cid:15))O(log(1/(cid:15))).

24

Published as a conference paper at ICLR 2021

B LEARNING DYNAMICAL SYSTEMS

B.1 GRAVITATIONAL FORCE LAW

We can use the product and chain rules to show that many functions important in scientiﬁc applications
can be efﬁciently learnable. This is true even when the function has a singularity. As an example
demonstrating both, we prove the following bound on learning Newton’s law of gravitation:
Theorem 13. Consider a system of k bodies with positions xi ∈ R3 and masses mi, interacting via
the force:

Fi =

(cid:88)

j(cid:54)=i

mimj
r3
ij

(xj − xi)

(38)

where rij ≡ ||xi − xj||. We assume that R = rmax/rmin, the ratio between the largest and smallest
pairwise distance between any two bodies, is constant. Suppose the mi have been rescaled to be
between 0 and 1. Then the force law is efﬁciently learnable in the sense of Deﬁnition 3 using the
modiﬁed ReLU kernel to generalization error less than (cid:15) using kO(ln(k/(cid:15))) samples.

Proof. We will prove learning bounds for each component of F separately, showing efﬁcient learning
with probability greater than 1−δ/3k. Then, using the union bound, the probability of simultaneously
learning all the components efﬁciently will be 1 − δ.

There are two levels of approximation: ﬁrst, we will construct a function which is within (cid:15)/2 of the
original force law, but more learnable. Secondly, we will prove bounds on learning that function to
within error (cid:15)/2.

We ﬁrst rescale the vector of collective {xi} so that their collective length is at most 1. In these new
units, this gives us r2

k . The ﬁrst component of the force on x1 can be written as:

max ≤ 2

(F1)1 =

k
(cid:88)

j=2

m1mj
r2
1j

((xj)1 − (x1)1)
r1j

.

(39)

If we ﬁnd a bound (cid:112)Mf for an individual contribution f to the force, we can get a bound on the total
√
MF = (k − 1)(cid:112)Mf . Consider an individual force term in the sum. The force has a singularity at

r1j = 0. In addition, the function r1j itself is non-analytic due to the branch cut at 0.

We instead will approximate the force law with a ﬁnite power series in r2
1j, and get bounds on
learning said power series. The power series representation of (1 − x)−3/2 is (cid:80)∞
(2n)!! xn. If
we approximate the function with d terms, the error can be bounded using Taylor’s theorem. The
Lagrange form of the error gives us the bound
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

πd|x|d+1
(1 − |x|)5/2+d

1
(1 − x)3/2

(2n + 1)!!
(2n)!!

d
(cid:88)

(2n+1)!!

(40)

xn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n=0

√

−

≤

n=0

where we use (2n+1)!!

(2n)!! ≈

√

πn for large n. We can use the above expansion by rewriting

r−3
1j = a−3(1 − (1 − r2

1j/a2))−3/2

(41)

for some shift a. Approximation with fd(r2
gives us the error:

1j), the ﬁrst d terms of the power series in (1 − r2

1j/a2)

√

πd|1 − r2
a3(1 − |1 − r2

1j/a2|d+1
1j/a2|)5/2+d

(42)

|fd(r2

1j) − r−3

1j | ≤

which we want to be small over the range rmin ≤ r1j ≤ rmax.
The bound is optimized when it takes the same value at rmin and rmax, so we set a2 = (r2
r2
max)/2. In the limit that rmax (cid:29) rmin, where learning is most difﬁcult, the bound becomes

min +

|fd(r2

1j) − r−3

1j | ≤

(cid:0)R2/2(cid:1)5/2+d

e−2(d+1)/R2

(43)

√

8πd

r3
max

25

Published as a conference paper at ICLR 2021

where R = rmax/rmin, which is constant by assumption.

In order to estimate an individual contribution to the force force to error (cid:15)/2k (so the total error is
(cid:15)/2), we must have:

m1mjrmax|fd(r1j) − r−3

1j | ≤

(cid:15)
2k

(44)

This allows us to choose the smallest d which gives us this error. Taking the logarithm of both sides,
we have:

1
2
where we use that r2
max ≤ 2/k after rescaling. The choice d ≥ R2 ln(k2/(cid:15)) ensures error less than
(cid:15)/2k per term.

ln(d) − (5/2 + d) ln (cid:0)2/R2(cid:1) − 2(d + 1)/R2 ≤ ln((cid:15)/k2).

(45)

Using this approximation, we can use the product and chain rules to get learning bounds on the force
law. We can write the approximation

F(cid:15)(x) =

(cid:88)

j(cid:54)=1

m1mjfd(hj(x))kj(x)

(46)

where hj(x) = ||x1 − xj|| and kj(x) = (x1)1 − (xj)j The number of samples needed for efﬁcient
learning is bounded by (cid:112)MF(cid:15) =

AF(cid:15) , for

8k

√

max

r3
AF(cid:15) = ˜f (cid:48)

d(˜h(1))˜h(cid:48)(1)˜k(1) + ˜fd(˜h(1))˜k(cid:48)(1)

with

Evaluating, we have

√

˜k(y) =

2y, ˜h(y) = 6y2, ˜fd(y) =

√

πd(1 + y/a2)d.

√

(cid:18)

AF(cid:15) =

2πd

1 +

(cid:19)d

12

√

(cid:18)

+

πd3

1 +

r2
r2
max
max
max ≤ 2/k and d = R2 ln(k2/(cid:15)) gives us the bound

(cid:19)d−1

12

which, after using r2

(cid:112)MF(cid:15) ≤ k−1/2 (cid:0)R2 ln(k2/(cid:15))(cid:1)3/2

(24k)R2 ln(k2/(cid:15)) .

The asymptotic behavior is

since R is bounded.

(cid:112)MF(cid:15) = kO(ln(k/(cid:15)))

(47)

(48)

(49)

(50)

(51)

We can therefore learn an (cid:15)/2-approximation of one component of F1, with probability at least
1 − δ/3k and error (cid:15)/2 with O(4(MF(cid:15) + log(3k/δ))/(cid:15)2) samples. Therefore, we can learn F1 to
error (cid:15) with the same number of samples. Using a union bound, with probability at least 1 − δ we can
simultaneously learn all components of all {Fi} with that number of samples.

We note that since the cutoff of the power series at d((cid:15)) = O(R2 ln(k2/(cid:15))) dominates the bound,
we can easily compute learning bounds for other power-series kernels as well. If the dth power
series coefﬁcient of the kernel is bd, then the bound on (cid:112)MF(cid:15) is increased by (d((cid:15))2bd((cid:15)))−1/2. For
√
example, for the Gaussian kernel, since b−1/2

d!, the bound becomes

=

d

(cid:112)MF(cid:15) = (R2 ln(k2/(cid:15))k)O(ln(k/(cid:15)))
which increases the exponent of k by a factor of ln(R2 ln(k2/(cid:15))).

(52)

B.2 EMPIRICAL CONFIRMATION OF LEARNING BOUNDS

We empirically validated our analytical learning bounds by training models to learn the gravitational
force function for k bodies (with k ranging from 5 to 400) in a 3−dimensional space. We created
synthetic datasets by randomly drawing k points from [0, 1]3 corresponding to the location of k
bodies, and compute the gravitational force (according to Figure 1) on a target body also drawn
randomly from [0, 1]3. To avoid singularities, we ensured a minimum distance of 0.1 between the
target body and the other bodies (corresponding to the choice R = 10). As predicted by the theory,

26

Published as a conference paper at ICLR 2021

none of the models learn well if R is not ﬁxed. We randomly drew the masses corresponding to
the k + 1 bodies from [0, 10]. We generated 5 million such examples - each example with 4(k + 1)
features corresponding to the location and mass of each of the bodies, and a single label corresponding
to the gravitational force F on the target body along the x-axis. We held out 10% of the dataset as
test data to compute the root mean square error (RMSE) in prediction. We trained three different
neural networks on this data, corresponding to various kernels we analyzed in the previous section:

1. A wide one hidden-layer ReLU network (corresponding to the ReLU NTK kernel).
2. A wide one hidden-layer ReLU network with a constant bias feature added to the input

(corresponding to the NTK kernel).

3. A wide one hidden-layer network with exponential activation function, where only the top

layer of the network is trained (corresponding to the Gaussian kernel).

We used a hidden layer of width 1000 for all the networks, as we observed that increasing the network
width further did not improve results signiﬁcantly. All the hidden layer weights were initialized
randomly.

In Figure 5 we show the normalized RMSE (RMSE/[Fmax − Fmin]) for each of the neural networks
for different values of the number of bodies k.

Figure 5: RMSE vs number of bodies k for learning gravitational force law for different kernels.
Normalized by the range Fmax − Fmin of the forces. Gaussian kernels learn worse than ReLU at
large k.

All three networks are able to learn the gravitational force equation with small normalized RMSE
for hundreds of bodies. Both the ReLU network and ReLU with bias outperform the network
corresponding to the Gaussian kernel (in terms of RMSE) as k increases. In particular, the Gaussian
kernel learning seems to quickly degrade at around 400 bodies, with a normalized RMSE exceeding
50%. This is consistent with the learning bounds for these kernels in Section A.2, and suggests that
those bounds may in fact be useful to compare the performances of different networks in practice.

We did not, however, observe much difference in the performance of the ReLU network when adding
a bias to the input, which suggests that the inability to get an analytical bound due to only even powers
in the ReLU NTK kernel might be a shortcoming of the proof technique, rather than a property which
fundamentally limits the model.

C LOWER BOUNDS

First, we show an exponential dependence on the depth h is necessary for learning decision trees.
The result depends on the hardness of solving parity with noise.
Conjecture 1. (hardness of parity with noise) Let a, x ∈ {0, 1}d be d-dimensional Boolean vectors.
In the parity with noise problem, we are given noisy inner products modulo 2 of the unknown vector
x with the examples ai, i.e. bi = (cid:104)ai, x(cid:105) + ηi mod 2 where ηi is a Binomial random variable which
is 1 with probability 0.1. Then any algorithm for ﬁnding x needs at least 2 ˜Ω(d) time or examples

27

050100150200250300350400k (number of bodies)10-310-210-1100Normalized RMSEGeneralization error vs. kReLUReLU with biasGaussian kernelPublished as a conference paper at ICLR 2021

(where ˜Ω hides poly-logarithmic factors in d). Similarly, if x is given to be s-sparse for s (cid:28) d, then
any algorithm for ﬁnding x needs at least dΩ(s) time or examples.

Note that the hardness of learning parity with noise is a standard assumption in computational learning
theory and forms the basis of many cryptographic protocols (Regev, 2009). The best known algorithm
for solving parity needs 2O(d/ log d) time and examples (Blum et al., 2003). Learning parities is
also known to provably require 2Ω(d) samples for the class of algorithm known as statistical query
algorithms—these are algorithms are only allowed to obtain estimates of statistical properties of the
examples but cannot see the examples themselves (Kearns, 1998). Note that the usual stochastic
algorithms for training neural networks such as SGD can be implemented in the statistical query
model (Song et al., 2017). Similar hardness result are conjectured for the problem of learning sparse
parity with noise, and the best known algorithm runs in time dΩ(s) (Valiant, 2015).

Based on the hardness of parity with noise, we show that exponential dependence on the depth for
learning decision trees is necessary.

Theorem 14. Conditioned on the hardness of the sparse parity with noise problem, any algorithm
for learning decision trees of depth h needs at least dΩ(h) time or examples.

Proof. Note that we can represent a parity with noise problem where the answer is h-sparse by a
decision tree of depth h where the leaves represent the solutions to the parity problem. The result
then follows by the hardness of the sparse parity with noise problem.

We also show that the doubly exponential dependence on the depth for learning generalized decision
programs is necessary.

Theorem 15. Learning a generalized decision program which is a binary tree of depth h using
stochastic gradient descent requires at least 22Ω(h)
examples. Conditioned on the hardness of
learning noisy parities, any algorithm for learning a generalized program of depth h needs at least
22

time or examples (where ˜Ω hides poly-logarithmic factors in h).

˜Ω(h)

Proof. Note that a generalized decision program of depth h can encode a parity function over D = 2h
bits. Any statistical query algorithm to learn a parity over D bits needs at least 2Ω(D) samples. As
stochastic gradient descent can be implemented in the statistical query model, hence the bound for
stochastic gradient descent follows.

To prove the general lower bound, note that a generalized decision program of depth h can also
encode a noisy parity function over D = 2h bits. Conditioned on the hardness of parity with noise,
any algorithm for learning noisy parities needs at least 2 ˜Ω(D) samples. Hence the bound for general
algorithms also follows.

In our framework, we assume that all the underlying functions that we learn are analytic, or have
an analytic approximation. It is natural to ask if such an assumption is necessary. Next, we show
that learning even simple compositions of functions such as their sum is not possible without some
assumptions on the individual functions.

Lemma 8. There exists function classes F1 and F2 which can be learnt efﬁciently but for every
f1 ∈ F1 there exists f2 ∈ F2 such that f1 + f2 is hard to learn (conditioned on the hardness of
learning parity with noise)

Proof. Both f1 and f2 are modiﬁcations of the parity with noise problem. The input in both cases is
x ∈ {0, 1}d. Let β be the solution to the noisy parity problem. The output for the function class F1
is [β, y], where y is the value of the noisy parity for the input. The output for the function class F2 is
[−β, y], where y is again the value of the noisy parity for the input. Note that F1 and F2 are trivial to
learn, as the solution β to the noisy parity problem is already a part of the output. For any f1 ∈ F1,
choose f2 ∈ F2 to be the function with the same vector β. Note that conditioned on the hardness of
learning parity with noise, f1 + f2 is hard to learn.

28

Published as a conference paper at ICLR 2021

C.1 LOWER BOUNDS FOR LEARNING ANY ANALYTIC FUNCTION

In this section, we show that there is a lower bound on the Rademacher complexity ¯yT ¯H −1y based
on the coefﬁcients in the polynomial expansion of the ˜g function. Hence the ˜g function characterizes
the complexity of learning g.
For any J = (J1, . . . , Jn) ∈ Nn, write a monomial XJ = xJ1
polynomial p(x) = (cid:80)
shows that monomials form an orthogonal basis over the unit circle in the complex plane.
Fact 3. (cid:104)XJ , XJ (cid:48)(cid:105)Cn = 1 if J = J (cid:48) and 0 otherwise (here, (cid:104)·, ·(cid:105)Cn denotes the inner product over
the unit circle in the complex plane).

k Jk. For a
J aJ xJ , where aJ ∈ C, its degree deg(p) = maxaJ (cid:54)=0 |J|. The following fact

n . Deﬁne |J| = (cid:80)

1 . . . xJn

Note that according to Theorem 7 the sample complexity for learning g(x) depends on ˜g(cid:48)(1) =
(cid:80)
j j|aj|, and hence is the (cid:96)1 norm of the derivative. The following Lemma shows that this is tight in
the sense that Ω((cid:80)
j ) samples or the (cid:96)2 norm of the derivative are necessary for learning g(x).

j ja2

For any variable x let ¯x denote the complex conjugate of x. Let x1, x2, . . . , xn denote the training
i qiti.
examples. Let Q denote the kernel polynomial so that K(xi, xj) = Q( ¯xi
For simplicity, let us look at the case where the power series and the kernel polynomial are univariate
polynomials of a bounded degree deg(q). We will assume that we have enough samples that Fact 3
hold when averaging over all samples. Let qJ be the coefﬁcient of TJ in the polynomial expansion of
Q(t1 + · · · + tn).
Lemma 9. For a univariate polynomial y = p(x) , ¯yT H −1y = (cid:80)
j /qj asymptotically in the
sample size, where aj are the coefﬁcients of the polynomial p. For a multivariate polynomial,
¯yT H −1y = (cid:80)
J /qJ asymptotically in the sample size. Here, H −1 denotes the pseudoinverse of
H.

T xj). Let Q(t) = (cid:80)

j a2

j a2

Proof. We will begin with the univariate case. Let {(x1, y1), (x2, y2, . . . , (xn, yn)} denote the
training examples and their labels. Let y be the vector of all the labels {yi}. Let d =
max{deg(p), deg(q)} (where we assume that deg(q) is bounded for simplicity). Now consider
i . Note that ¯GT trans-
the matrix G with n rows and d columns where the (i, j)-th entry is xj
the expected value of (1/n) ¯GT y
forms y from the standard basis to the monomial basis, i.e.
is (a1, . . . , ad) (by Fact 3). Therefore, (1/n) ¯GT y = (a1, . . . , ad) asymptotically in the sample
size n. We claim that H = GD ¯GT where D is the diagonal matrix where Dk,k = qk. To
verify this, let G(i) denote that i-th row of G and observe that the (i, j)-th entry G(i)D ¯GT
(j) =
(cid:80)
k = qk(xi ¯xj)k = K(xi, xj) = Hi,j. Now given the orthonormality of the monomial
basis, (1/n) ¯GT G = I. Therefore since H = GD ¯GT is the SVD of H, H −1 = (1/n2)GD−1 ¯GT .
T
Hence ¯yT H −1y = ((1/n)GT ¯y)

D−1((1/n) ¯GT y) = (cid:80)

i qk ¯xj

k xk

j(1/qj)a2
j .

For the multivariate case, instead of having d columns for G, we will have one column for every
possible value of J of degree at most d. In the diagonal entry DJ,J we put qJ , where qJ is the
coefﬁcient of TJ in the polynomial expansion of Q(t1 + · · · + tn).
Corollary 5. For the ReLU activation qj = Ω(1/j), and hence ¯yT ¯H −1y ≥ Ω((cid:80)
cally in the sample size.

j ) asymptoti-

j ja2

Note that in Theorem 7, the upper bound for the sample complexity was O((cid:80)
j j|aj|), hence
Theorem 7 is tight up to the distinction between the (cid:96)1 and (cid:96)2 norm (which can differ by at most
(cid:112)deg(p)).

D ADDITIONAL DETAILS FOR EXPERIMENTS

D.1 SETUP DETAILS

All the experiments are done in TensorFlow, trained with a GPU accelerator. We use the default
TensorFlow values for all hyper parameters involved in the training of the neural networks. All the
experiment results averaged over 3 runs. The number of training epochs for each experiment and

29

Published as a conference paper at ICLR 2021

(a) An instance of the problem with multiple
clusters, each cluster is indicated by a red
circle.

(b) Test accuracy vs. number of points per
cluster

Figure 6: Experiment where data is clustered into tasks with a separate linear function for each task.
A single neural network does well even when there are multiple clusters.

average runtime (for one run) are summarized in Table 2. For cluster experiments, number of training
examples per cluster varies 1000 to 100000, average runtime varies from 2 minutes to 100 minutes.
For the decision tree experiments, number of training examples per leaf node varies from 64 to 512,
avarage runtime varies from 14 minutes to 42 minutes. For the SQL-style aggregation experiment, the
train dataset contains 16384 examples, and test dataset contains 4096 examples, average runtime is
50 minutes. The source for the Penn World Table dataset Feenstra et al. (2015) used in the SQL query
experiment is https://www.rug.nl/ggdc/productivity/pwt/ and it is also available
at https://www.kaggle.com/jboysen/penn-world-table.

Table 2: Number of epochs and average runtime

Experiment name

Number of epochs Average runtime

Cluster

Decision Tree

SQL-style aggregation

100

200

6400

2 - 100 minutes

14 - 42 minutes

50 minutes

D.2 ADDITIONAL DETAILS FOR LEARNING CLUSTERS OF LINEAR FUNCTIONS

We provide a more detailed setup of the experiment reported in Fig. 3a where the task codes are
given by clusters, and there is a separate linear function for every cluster. In this experiment, the data
is drawn from k clusters, and from a mixture of two well-separated Gaussians in each cluster. Data
points from the two Gaussians within each cluster are assigned two different labels, for 2k labels
in total. Fig. 6a below shows an instance of this task in two dimensions, the red circles represent
the clusters, and there are two classes drawn from well-separated Gaussians from each cluster. In
high dimensions, the clusters are very well-separated, and doing a k-means clustering to identify
the k cluster centers and then learning a simple linear classiﬁer within each cluster gets near perfect
classiﬁcation accuracy. Fig. 6b shows the performance of a single neural network trained on this
task (same as Fig. 3a in the main body). We can see that a single neural network still gets good
performance with a modest increase in the required number of samples.

30

101102103Number of examples per cluster5060708090100Test accuracyk = 1k = 50k = 100k = 250k = 500k = 1000