Lyapunov-stable neural-network control

Hongkai Dai ∗, Benoit Landry†, Lujie Yang‡, Marco Pavone† and Russ Tedrake∗‡
∗ Toyota Research Institute
†Stanford University
‡Massachusetts Institute of Technology
Email: hongkai.dai@tri.global, blandry@stanford.edu lujie@mit.edu pavone@stanford.edu russ.tedrake@tri.global

1
2
0
2

p
e
S
9
2

]

O
R
.
s
c
[

1
v
2
5
1
4
1
.
9
0
1
2
:
v
i
X
r
a

Abstract—Deep learning has had a far reaching impact in
robotics. Speciﬁcally, deep reinforcement learning algorithms
have been highly effective in synthesizing neural-network con-
trollers for a wide range of tasks. However, despite this empirical
success, these controllers still lack theoretical guarantees on their
performance, such as Lyapunov stability (i.e., all trajectories of
the closed-loop system are guaranteed to converge to a goal state
under the control policy). This is in stark contrast to traditional
model-based controller design, where principled approaches (like
LQR) can synthesize stable controllers with provable guarantees.
To address this gap, we propose a generic method to synthesize
a Lyapunov-stable neural-network controller, together with a
neural-network Lyapunov function to simultaneously certify
its stability. Our approach formulates the Lyapunov condition
veriﬁcation as a mixed-integer linear program (MIP). Our MIP
veriﬁer either certiﬁes the Lyapunov condition, or generates
counter examples that can help improve the candidate controller
and the Lyapunov function. We also present an optimization
program to compute an inner approximation of the region of
attraction for the closed-loop system. We apply our approach
to robots including an inverted pendulum, a 2D and a 3D
quadrotor, and showcase that our neural-network controller
outperforms a baseline LQR controller. The code is open sourced
at https://github.com/StanfordASL/neural-network-lyapunov.

I. INTRODUCTION

The last few years have seen sweeping popularity of apply-
ing neural networks to a wide range of robotics problems [48],
such as perception [30, 40, 19], reasoning [16] and planning
[25]. In particular, researchers have had great success train-
ing control policies with neural networks on different robot
platforms [32, 50, 23, 27]. Typically these control policies
are obtained through reinforcement learning (RL) algorithms
[49, 44, 22]. Although immensely successful, these neural-
network controllers still generally lack theoretical guarantees
on their performance, which could hinder their adoption in
many safety-critical applications.

A crucial guarantee currently missing for neural-network
controllers is the stability of the closed-loop system, especially
Lyapunov stability. A system is regionally stable in the sense
of Lyapunov if starting from any states within a region, the
system eventually converges to an equilibrium. This region is
called the region of attraction (ROA) [46]. Lyapunov stability
provides a strong guarantee on the asymptotic behavior of
the system for any state within the region of attraction. It is
well known that a system is Lyapunov stable if and only if
there exists a Lyapunov function [46] that is strictly positive
deﬁnite and strictly decreasing everywhere except at the goal
equilibrium state. Therefore, our goal is to synthesize a pair:

Fig. 1: (left) Snapshots of stabilizing a 3D quadrotor with
our neural-network controller to the hovering position at the
origin (red snapshot) from different initial states. The green
curves are the paths of the quadrotor center. (right) value of
the neural-network Lyapunov functions along the simulated
trajectories. The Lyapunov function has positive values, and
decreases along the trajectories.

a neural-network controller to stabilize the system, and a
Lyapunov function to certify its stability.

In the absence of neural networks in the loop, a signif-
icant body of work from the control community provides
tools to synthesize Lyapunov-stable controllers [46, 9]. For
example, for a linear dynamical system, one can synthesize
a linear LQR controller to achieve Lyapunov stability (with
the quadratic Lyapunov function solved through the Riccati
equation). For a control-afﬁne system with polynomial dy-
namics, Javis-Wloszek et al. [26] and Majumdar et al. [35]
have demonstrated that a Lyapunov-stable controller together
with a Lyapunov function, both polynomial functions of the
state, can be obtained by solving a sum-of-squares (SOS)
program. Recently, for more complicated systems, researchers
have started to represent Lyapunov functions (but not their
associated controllers) using neural networks. For example,
Chang et al. synthesized linear controllers and neural network
Lyapunov functions for simple nonlinear systems [11]. In a
similar spirit, there is growing interest to approximate the
system dynamics with neural networks, such as for racing cars
[56], actuators with friction/stiction [24], perceptual measure-
ment like keypoints [36], system with contacts [41], and soft
robots [21], where an accurate Lagrangian dynamics model
is hard to obtain, while the neural-network dynamics model
can be extracted from rich measurement data. Hence we are
interested in systems whose dynamics are given as a neural
network.

Unlike previous work which is restricted to linear [9, 11]

024time(s)0.00.20.40.60.8VLyapunovfunctionvaluealongsimulatedtrajectories 
 
 
 
 
 
(b)

(a)

Fig. 2: (Left) The forward system1(which contains a neural
network) is given, and we aim at ﬁnding the controller
and a Lyapunov function to prove Lyapunov stability of the
closed-loop system. Both the controller and Lyapunov function
contain neural networks. (right) Visualization of a Lyapunov
function for a 2-dimensional system. The Lyapunov function is
usually a bowl-shaped function that is strictly positive except
at the goal state.

or polynomial controllers [26, 35], our paper provides a novel
approach to synthesize a stable neural-network controller,
together with a neural-network Lyapunov function, for a given
dynamical system whose forward dynamics is approximated
by another neural network. The overall picture together with
a Lyapunov function is visualized in Fig. 2.

In order to synthesize neural-network controllers and Lya-
punov functions, one has to ﬁrst be able to verify that the
neural-network functions satisfy the Lyapunov condition for
all states within a region. There are several techniques to
verify certain properties of neural network outputs for all
inputs within a range. These techniques can be categorized
by whether the veriﬁcation is exact, e.g., using Satisﬁability
Modulo Theories (SMT) solvers [29, 11, 1] or mixed-integer
programs (MIP) solvers [10, 52, 14, 12], versus inexact veri-
ﬁcation by solving a relaxed convex problem [7, 17, 57, 45].
Another important distinction among these techniques is the
activation functions used in the neural networks. For example,
Abate et al. [1] and Chang et al. [11] learn neural-network Lya-
punov functions with quadratic and tanh activation functions
respectively. On the other hand, the piecewise linear nature of
(leaky) ReLU activation implies that the input and output of a
(leaky) ReLU network satisfy mixed-integer linear constraints,
and hence network properties can be exactly veriﬁed by MIP
solvers [52, 14]. In this work, due to its widespread use,
we choose the (leaky) ReLU unit for all neural networks.
This enables us to perform exact veriﬁcation of the Lyapunov
condition without relaxation for safety-critical robot missions.
The veriﬁers (both SMT and MIP solvers) can either
deﬁnitively certify that a given candidate function satisﬁes
the Lyapunov condition everywhere in the region, or generate
counter examples violating the Lyapunov condition. In this
work, we solve MIPs to ﬁnd the most adversarial counter

1The quadrotor picture is taken from [8].

examples, namely the states with the maximal violation of the
Lyapunov condition. Then, in order to improve the satisfaction
of the Lyapunov conditions, we propose two approaches to
jointly train the controller and the Lyapunov function. The ﬁrst
approach is a standard procedure in counter-example guided
training, where we add the counter examples to the training
set and minimize a surrogate loss function of the Lyapunov
condition violation on this training set [1, 11, 42]. The second
approach is inspired by the bi-level optimization community
[6, 31, 14], where we directly minimize the maximal violation
as a min-max problem through gradient descent.

Our contributions include: 1) we synthesize a Lyapunov-
stable neural-network controller
together with a neural-
network Lyapunov function. To the best of our knowledge,
this is the ﬁrst work capable of doing this. 2) We compute an
inner approximation of the region of attraction for the closed-
loop system. 3) We present two approaches to improve the
networks based on the counter examples found by the MIP
veriﬁer. 4) We demonstrate that our approach can successfully
synthesize Lyapunov-stable neural-network controllers for sys-
tems including inverted pendulums, 2D and 3D quadrotors,
and that they outperform a baseline LQR controller.

II. PROBLEM STATEMENT

We consider a discrete-time system whose forward dynam-

ics is

xt+1 = f (xt, ut) = φdyn(xt, ut)

−

φdyn(x∗, u∗) + x∗
umax
ut
umin

(1a)

(1b)

∈

∈

≤

Rnx, ut

≤
Rnu , umin and umax are the lower/upper
where xt
limits. φdyn is a feedforward fully connected neural
input
network with leaky ReLU activation functions 2 3. x∗/u∗
are the state/control at the goal equilibrium. By deﬁnition
the dynamics equation (1a) guarantees that at the equilibrium
state/control xt = x∗, ut = u∗, the next state xt+1 remains the
equilibrium state xt+1 = x∗. Due to the universal approxima-
tion theorem [33], we can approximate an arbitrary smooth
dynamical system written as (1a) with a neural network. Our
goal is to ﬁnd a control policy ut = π(xt) and a Lyapunov
function V (xt) : Rnx
R, such that the following Lyapunov
→
conditions are satisﬁed:

V (xt+1)

V (xt)

−

V (xt) > 0
(cid:15)2V (xt)

≤ −

xt
xt

∀

∀

∈ S

∈ S

, xt
, xt

= x∗
= x∗
V (x∗) = 0

(2a)

(2b)

(2c)

S

is a compact sub-level set

where
, and
}
(cid:15)2 > 0 is a given positive scalar. The Lyapunov conditions
in (2) guarantee that starting from any state inside
, the
state converges exponentially to the equilibrium state x∗,
is a region of attraction of the closed-loop system.
and

V (xt)

xt

≤

=

S

S

ρ

{

|

S

2Since ReLU can be regarded as a special case of leaky ReLU, we present

our work with leaky ReLU for generality.

3Our approach can also handle other architectures such as convolution. For

simplicity of presentation we don’t discuss them in this paper.

Given modelSynthesize controllersand Lyapunov functionsx12.42.62.83.03.23.43.63.8x21.51.00.50.00.51.01.5V0.51.01.52.02.53.03.54.0(cid:54)
(cid:54)
Fig. 3: A leaky ReLU activation function.

In addition to the control policy and the Lyapunov func-
tion, we will ﬁnd an inner approximation of the region of
attraction. Note that condition (2b) is a constraint on the
Lyapunov function V (
),
·
·
since V (xt+1) = V (f (xt, π(xt))) depends on both the control
policy to compute xt+1 together with the Lyapunov function
V (
·

) as well as the control policy π(

).

III. BACKGROUND ON RELU AND MIP

In this section we give a brief overview of the mixed-integer
linear formulation which encodes the input/output relationship
of a neural network with leaky ReLU activation. This MIP
formulation arises from the network output being a piecewise-
afﬁne function of the input, hence intuitively one can use
linear constraints for each afﬁne piece, and binary variables for
the activated piece. Previously researchers have solved mixed-
integer programs (MIP) to verify certain properties of the
feedforward neural network in machine learning applications
such as verifying image classiﬁers [10, 52].

For a general

fully-connected neural network,

the in-

put/output relationship in each layer is

zi =σ(Wizi−1 + bi), i = 1, . . . , n
zn =Wnzn−1 + bn, z0 = x,

1

−

(3a)

(3b)

where Wi, bi are the weights/biases of the i’th layer. The
activation function σ(
) is the leaky ReLU function shown
·
in Fig.3, as a piecewise linear function σ(y) = max(y, cy)
c < 1 is a given scalar. If we suppose that for one
where 0
R is bounded in the range
leaky ReLU neuron, the input y
ylo
yup (where ylo < 0 and yup > 0), then we can use
the big-M technique to write out the input/output relationship
of a leaky ReLU unit w = σ(y) as the following mixed-integer
linear constraints

≤

≤

≤

∈

y

w

cy

(c

−

−

≤

1)yupβ, w

y

−

≤

w
(c

≥
−

y, w
1)ylo(β
β

∈ {

cy
1)

≥
−
,
0, 1
}

(4a)
(4b)

(4c)

where the binary variable β is active when y
0. Since the
only nonlinearity in the neural network (3) is the leaky ReLU
), by replacing it with constraints (4), the relationship
unit σ(
·
between the network output zn and input x is fully captured
by mixed-integer linear constraints.

≥

We expect bounded input to the neural networks since we
care about states within a neighbourhood of the equilibrium
so as to prove regional Lyapunov stability, and the system
input ut is restricted within the input limits (Eq. (1b)). With a
bounded neural network input, the bound of each ReLU neuron

input can be computed by either Interval Arithmetic [57], by
solving a linear programming (LP) problem [52], or by solving
a mixed-integer linear programming (MILP) problem [13, 18].
After formulating neural network veriﬁcation as a mixed-
integer program (MIP), we can efﬁciently solve MIPs to global
optimality with off-the-shelf solvers, such as Gurobi [39] and
CBC [20] via branch-and-cut method.

IV. APPROACH

In this section we present our approach to ﬁnding a pair
of neural networks as controller and the Lyapunov function.
We will ﬁrst use the technique described in the previous
section III, and demonstrate that one can verify the Lyapunov
condition (2) through solving MIPs. Then we will present two
approaches to reduce the Lyapunov condition violation using
the MIP results. Finally we explain how to compute an inner-
approximation of the region of attraction.

A. Verify Lyapunov condition via solving MIPs

We represent the Lyapunov function with a neural network

φV : Rnx

R as

→

(5)

V (xt) = φV (xt)

φV (x∗) +

R(xt
|

1,

x∗)
|
x∗)

|

|

−

−

−

R(xt

−
R(xt
where R is a matrix with full column rank.
1 is the
|
x∗). Eq. (5) guarantees V (x∗) =
1-norm of the vector R(xt
−
0, hence condition (2c) is trivially satisﬁed. Notice that even
x∗)
without the term
1 in (5), the Lyapunov function
|
would still satisfy V (x∗) = 0, but adding this 1-norm term
) in satisfying the Lyapunov condition V (xt) >
assists V (
·
φV (x∗) is a piecewise-
0. As visualized in Fig 4, φV (xt)
afﬁne function of xt passing through the point (x∗, 0). Most
likely (x∗, 0) is in the interior of one of the linear pieces,
instead of on the boundary of a piece; hence locally around
x∗, the term φV (xt)
φV (x∗) is a linear function of xt, which
will become negative away from x∗, violating the positivity
condition V (xt) > 0 ((2a)). To remedy this, we add the term
1 to the Lyapunov function. Due to R being full-
R(xt
|
|
rank, this 1-norm is strictly positive everywhere except at x∗.
With sufﬁciently large R, we guarantee that at least locally
around x∗ the Lyapunov function is positive. Notice that V (xt)
is a piecewise-afﬁne function of xt.

x∗)

−

−

−

Our approach will entail searching for both the neural
network φV and the full column-rank matrix R in (5). To
guarantee R being full column-rank, we parameterize it as

R = U (cid:0)Σ + diag(r2

1, . . . , r2
nx

)(cid:1) V T ,

(6)

where U, V are given orthonormal matrices, Σ is a given
diagonal matrix with strictly positive diagonal entries, and
scalars r1, . . . , rnx are free variables. The parameterization
(6) guarantees R being full column-rank since the minimal
singular value of R is strictly positive.

We represent the control policy using a neural network φπ :

Rnx

Rnu as

→

ut = π(xt) = clamp (φπ(xt)

−

φπ(x∗) + u∗, umin, umax) ,

(7)

−

x∗)
|

R(xt
|

where the objectives are the violation of condition (9) and
(2b) respectively. If the optimal values of both problems are 0
(attained at xt = x∗), then we certify the Lyapunov condition
(2). The objective in (10a) is a piecewise-afﬁne function of the
variable xt since both V (xt) and
1 are piecewise-
afﬁne. Likewise in optimization problem (10b), since the
control policy (7) is a piecewise-afﬁne functions of xt, and
the forward dynamics (1a) is a piecewise-afﬁne function of
xt and ut, the next state xt+1 = f (xt, π(xt)), its Lyapunov
value V (xt+1) and eventually the objective function in (10b)
are all piecewise-afﬁne functions of xt. It is well known in the
optimization community that one can maximize a piecewise-
afﬁne function within a bounded domain (
in this case)
through solving an MIP [55]. In section III we have shown the
MIP formulation on neural networks with leaky ReLU units;
in Appendix VII-B we present the MIP formulation for the
x∗)
1-norm in
1 and the clamp function in the control
|
policy.

R(xt
|

−

B

By solving the mixed-integer programs in (10), we either
verify that the candidate controller is Lyapunov-stable with the
candidate Lyapunov function V (xt) as a stability certiﬁcate;
or we generate counter examples of xt, where the objective
values are positive, hence falsify the candidates. By maximiz-
ing the Lyapunov condition violation in the MIP (10), we ﬁnd
not only a counter example if one exists, but the worst counter
example with the largest violation. Moreover, since the MIP
solver traverses a binary tree during branch-and-cut, where
each node of the tree might ﬁnd a counter example, the solver
ﬁnds a list of counter examples during the solving process. In
the next subsection, we use both the worst counter example
and the list of all counter examples to reduce the Lyapunov
violation.

B. Trainer

After the MIP veriﬁer generates counter examples violating
Lyapunov conditions, to reduce the violation, we use these
counter examples to improve the candidate control policy and
the candidate Lyapunov function. We present two iterative
approaches. The ﬁrst one minimizes a surrogate function on
a training set, and the counter examples are appended to the
training set in each iteration. This technique is widely used
in the counter-example guided training [11, 1, 12, 28]. The
second approach minimizes the maximal Lyapunov condition
violation directly by solving a min-max problem through
gradient descent. In both approaches, we denote the parameters
we search for as θ, including

• The weights/biases in the controller network φπ;
• The weights/biases in the Lyapunov network φV ;
• r1, . . . , rnx in the full column-rank matrix R (Eq. (6)).
namely we optimize both the control policy and the Lyapunov
function simultaneously, so as to satisfy the Lyapunov condi-
tion on the closed-loop system.

1) Approach 1, growing training set with counter examples:
A necessary condition for satisfying the Lyapunov condition
, is that the Lyapunov condition holds for
for any state in
. Hence we could reduce a
many sampled states within

B

B

−

φV (x∗) is a piecewise-afﬁne
Fig. 4: The term φV (xt)
function that passes through (xt, V (xt)) = (x∗, 0). Most likely
x∗ is within the interior of a linear piece, but not at the
boundary between pieces. This linear piece will go negative in
the neighbourhood of x∗. By adding the 1-norm
x∗)
1
|
(red lines), the Lyapunov function (blue lines) is at least locally
positive around x∗.

R(xt
|

−

where clamp(
) clamps the value φπ(xt)
·
elementwisely within the input limits [umin, umax], namely



−

φπ(x∗) + u∗

clamp(α, lo, up) =

up

.

(8)

up if α > up
α if lo
α
lo if α < lo

≤

≤



The control policy (7) is a piecewise-afﬁne function of the
state xt. Notice that (7) guarantees that at the equilibrium state
xt = x∗, the control action is ut = u∗.

It is worth noting that our approach is only applicable to sys-
tems that can be stabilized by regular (e.g., locally Lipschitz
bounded) controllers. Some dynamical systems, for example
a unicycle, require non-regular controllers for stabilization,
where our approach would fail. The readers can refer to [47]
for more background on regular controllers.

The Lyapunov condition (2), in particular, (2a), is a strict
inequality. To verify this through MIP which only handles non-
strict inequalities constraints
, we change condition
and
(2a) to the following condition with

≥

≤
≥

V (xt)

(cid:15)1

R(xt
|

−

≥

x∗)
|

1

x

∀

,

∈ S

(9)

where 0 < (cid:15)1 < 1 is a given positive scalar. Since R is full
column-rank, the right-hand side is 0 only when xt = x∗.
Hence the non-strict inequality constraint (9) is a sufﬁcient
condition for the strict inequality constraint (2a). In Appendix
VII-C we prove that it is also a necessary condition.

In order to verify the Lyapunov condition (2) for a candidate
Lyapunov function and a controller, we consider verifying
the condition (9) and (2b) for a given bounded polytope
B
around the equilibrium state. The veriﬁer solves the following
optimization problems

R(xt
|

(cid:15)1

max
xt∈B
V (xt+1)

−

max
xt∈B

V (xt)

x∗)

1
|

−

−
V (xt) + (cid:15)2V (xt),

(10a)

(10b)

Algorithm 1 Train controller/Lyapunov function on training
sets constructed from veriﬁer

1: Start with a candidate neural-network controller π, a
2.

candidate Lyapunov function V , and training sets

1,

X

X

2: while not converged do
3:

Solve MIPs (10a) and (10b).
if MIP (10a) or MIP (10b) has maximal objective > 0
then

if MIP (10a) maximal objective > 0 then

Add the counter examples from MIP (10a) to

end if
if MIP (10b) maximal objective > 0 then

Add the counter examples from MIP (10b) to

end if
Perform batched gradient descent on the parameters
θ to reduce the loss function (12) on the training
2) = 0, or
set
reaches a maximal epochs.

2. Stop until either lossθ(

1,

1,

X

X

X

X

1.

X

2.

X

4:

5:
6:
7:

8:
9:
10:
11:

converged = true.

else

12:
13:
14:
15: end while

end if

Fig. 5: Flow chart of Algorithm 1.

surrogate loss function on a training set
containing sampled
grows after each MIP solve by
states. The training set
appending the counter examples generated from the MIP solve.
Since the MIP (10a) and the MIP (10b) generate different
1 and
counter examples, we keep two separate training sets

X

X

X

2 for MIP (10a) and MIP (10b) respectively.
1,
We design a surrogate loss function for

2 to measure
the violation of the Lyapunov condition on the training set.
We denote the violation of condition (9) on a sample state
xi
1), and the violation of condition (2b) on a
1 ∈ X
sample state xi

1 as η1(xi

2 as η2(xi

2), deﬁned as

X

X

X

2 ∈ X

η1(xi
η2(xi

R(xi
1) = max((cid:15)1
|
2) = max(V (f (xi

x∗)
1
1 −
|
2, π(xi
2)))

−

−

V (xi
V (xi

1), 0)
2) + (cid:15)2V (xi

2), 0),

(11b)

1) and η2(
We denote η1(
entry is the violation on the i’th sample η1(xi
respectively, then our surrogate function is deﬁned as

2) as the vectors whose i’th
1) and η2(xi
2)

X

X

(11a)

directly through the following min-max problem



min
θ



max

xt∈B

(cid:124)

(cid:15)1

R(xt
|

x∗)

−
(cid:123)(cid:122)
MIP (10a)

1
|

−

V (xt)

(cid:125)

V (xt) + (cid:15)2V (xt)

−

(cid:123)(cid:122)
MIP (10b)

(13)








,

(cid:125)

lossθ(

1,

X

2) =

X

η1(

X

p +

1)
|

|

|

η2(

X

p,

2)
|

(12)

+ max
xt∈B
(cid:124)

V (xt+1)

−

| · |

∞−
norm (a smooth approximation of the

where
p denotes the p-norm of a vector, such as 1-norm
(mean of the violation),
norm (maximal of the violation)
and 4
norm). The
subscript θ in the loss function (12) emphasizes its dependency
on θ, the parameters in both the controller and the Lyapunov
function. We then minimize the surrogate loss function on the
training set via standard batched gradient descent on θ. The
ﬂow chart of this approach is depicted in Fig. 5. Algorithm 1
presents the pseudo-code.

∞−

the sampled states,

Since the surrogate loss function is the Lyapunov condition
the batched gradient
violation on just
descent will overﬁt to the training set, and potentially cause
large violation away from the sampled states. To avoid this
overﬁtting problem, we consider an alternative approach with-
out constructing the training sets.

2) Approach 2, minimize the violation via min-max pro-
gram: Instead of minimizing a surrogate loss function on a
training set, we can minimize the Lyapunov condition violation

where θ are the parameters in the controller and the Lyapunov
function, introduced at the beginning of this subsection IV-B.
Unlike the traditional optimization problem, where the ob-
jective function is a closed-form expression of the decision
variable θ, in our problem (13) the objective function is the
result of other maximization problems, whose coefﬁcients and
bounds of the constraint/cost matrices depend on θ. In order to
solve this min-max problem, we adopt an iterative procedure.
In each iteration we ﬁrst solve the inner maximization problem
using MIP solvers, and then compute the gradient of the
MIP optimal objective w.r.t the variables θ, ﬁnally we apply
gradient descent along this gradient direction, so as to reduce
the objective in the outer minimization problem.

To compute the gradient of the maximization problem
objective w.r.t θ, after solving the inner MIP to optimality,
we ﬁx all the binary variables to their optimal solutions, and
keep only the active linear constraints. The inner maximization

Trainer reducessurrogate lossfunction ontraining setMIP veriﬁercandidate controllerand Lyapunov functionMIP objectives = 0 ?output stable controllerand Lyapunov functionYesAdd counter examplesto training setNoproblem can then be simpliﬁed to

γ(θ) = max

cT
θ s + dθ

s
s.t Aθs = bθ,

(14a)

(14b)

θ A−1

where the problem coefﬁcients/bounds cθ, dθ, Aθ, bθ are all
explicit functions of θ. s contains all the continuous variables
including xt and other slack variables. The
in the MIP,
optimal cost of (14) can be written in the closed form as
γ(θ) = cT
θ bθ + dθ, and then we can compute the gradient
∂γ(θ)/∂θ by back-propagating this closed-form expression.
Note that this gradient is well deﬁned if a tiny perturbation on
θ changes only the optimal value of the continuous variables
s, but not the set of active constraints or the optimal binary
variable values (changing them would make the gradient
ill-deﬁned). This technique to differentiate the optimization
objective w.r.t neural network parameters is becoming increas-
ingly popular in the deep learning community. The interested
readers can refer to [4, 2] for a more complete treatment on
differentiating an optimization layer.

Algorithm 2 shows pseudo-code for this min-max optimiza-

tion approach.

Algorithm 2 Train controller/Lyapunov function through min-
max optimization

1: Given a candidate control policy π and a candidate Lya-

punov function V .

2: while not converged do
3:
4:

Solve MIP (10a) and (10b).
if Either of MIP (10a) of (10b) has maximal objective
> 0 then

5:

Compute the gradient of the MIP objectives w.r.t θ,
denote this gradient as ∂γ/∂θ.
θ = θ

StepSize

∂γ/∂θ.

6:
7:
8: end while

end if

−

∗

C. Computing region of attraction

B

B

After the training process in section IV-B converges to
satisfy the Lyapunov condition for every state inside the
bounded polytope
, we compute an inner approximation of
the region of attraction for the closed-loop system. (Notice
is not a region of attraction, since
that the veriﬁed region
it’s not an invariant set, while the sub-level sets of V are
guaranteed to be invariant). One valid inner approximation is
xt
the largest sub-level set
contained inside
{
≤
the veriﬁed region
, as illustrated in Fig. 6. Since we already
obtained the Lyapunov function V (xt) in the previous section,
we only need to ﬁnd the largest value of ρ such that
.
S ⊂ B
Equivalently we can ﬁnd ρ through the following optimization
problem

V (xt)
|

ρ
}

=

B

S

ρ = min
xt∈∂B

V (xt),

(15)

where the compact set ∂
region

, and the constraint xt

is the boundary of the polytopic
can be formulated as

B

∂

B

∈

B

Fig. 6: An inner approx-
imation of the region of
is the largest
attraction
S
sub-level set V (xt)
ρ
contained inside the veri-
ﬁed region
, where the
B
Lyapunov function is pos-
itive deﬁnite and strictly
decreasing.

≤

Fig. 7: Heatmap of the Lyapunov
the inverted pen-
function for
dulum. The red contour is the
boundary of the veriﬁed inner
approximation of the region of
attraction, as the largest sub-level
set contained in the veriﬁed box
5.
region 0

2π,

˙θ

θ

5
−

≤

≤

≤

≤

mixed-integer linear constraints (with one binary variable for a
face of the polytope
). As explained previously, the Lyapunov
function V (xt) is a piecewise-afﬁne function of xt, hence the
optimization problem (15) is again an MIP, and can be solved
efﬁciently by MIP solvers.

B

It is worth noting that the size of this inner approximation of
the region of attraction can be small, as we ﬁx the Lyapunov
function and only search for its sub-level set. To verify a larger
inner approximation, one possible future research direction
is to search for the Lyapunov function and the sub-level set
simultaneously, as in [43].

V. RESULTS

We synthesize stable controllers and Lyapunov functions on
pendulum, 2D and 3D quadrotors. We use Gurobi as the MIP
solver. All code runs on an Intel Xeon CPU. The sizes of the
neural networks are shown in Table III in Appendix VII-A.

A. Inverted pendulum

We ﬁrst test our approach on an inverted pendulum. We
approximate the pendulum Lagrangian dynamics using a
neural network, by ﬁrst simulating the system with many
state/action pairs, and then approximating the simulation data
through regression. To stabilize the pendulum at
the top
equilibrium θ = π, ˙θ = 0, we synthesize a neural-network
controller and a Lyapunov function using both Algorithm 1
and 2. We verify the Lyapunov condition in the box region
0
5. The Lyapunov function V is shown
θ
≤
in Fig. 7.

5
−

2π,

≤

≤

≤

˙θ

We simulate the synthesized controller with the original
pendulum Lagrangian dynamics model (not the neural network
dynamics φdyn). The result
is shown in Fig. 8. Although
the neural network dynamics φdyn has approximation error,
the simulation results show that the neural-network controller
swings up and stabilizes the pendulum for not only the ap-
proximated neural network dynamics, but also for the original
Lagrangian dynamics. Moreover, starting from many states

00.5ππ1.5π2πθ(radian)-5-2.502.55˙θ(radian/s)V010203040LQR succeeds
LQR fails

NN succeeds
8078
1918

NN fails
0
4

TABLE I: Number of success/failure for 10, 000 simulations of
2D quadrotor with the neural network (NN) controller and an
LQR controller. The simulation uses the Lagrangian dynamics.

≤

−

−

−

−

−

3]

0.9,

4.5,

0.5π,

[x, z, θ, ˙x, ˙z, ˙θ]

We sample 10000 initial states uniformly in the box
0.9,

[
4.5,
≤
−
[0.9, 0.9, 0.5π, 4.5, 4.5, 3]. For each initial state we simulate
the Lagrangian dynamics with the neural network and an LQR
controller. We summarize the result in table I on whether the
simulation converges to the goal state or not. More states
can be stabilized by the neural-network controller than the
LQR controller. Moreover, the off-diagonal entries in Table I
demonstrates that the set of sampled states that are stabilized
by the neural-network controller is a strict super-set of the set
of states stabilized by the LQR controller. We believe there
are two factors contributing to the advantage of our neural-
network controller against an LQR: 1) the neural-network
controller is piecewise linear while the LQR controller is
linear; the latter can be a special case of the former. 2) the
neural-network controller is aware of the input limits while
the LQR controller is not.

We then focus on certain two dimensional slices of the state
space, and sample many initial states on these slices. For each
sampled initial state we simulate the Lagrangian dynamics
using both the neural-network and the LQR controller. We
visualize the simulation results in Fig. 10. Each dot represents
a sampled initial state, and we color each initial state based
on whether the neural-network (NN)/LQR controllers succeed
in stabilizing that initial state to the goal
• Purple: NN succeeds but LQR fails.
• Green: both NN and LQR succeed
• Red: both NN and LQR fail.

Evidently the large purple region suggests that the region of
attraction with the neural-network controller is a strict super-
set of that with the LQR controller. We observe that for the
initial states where LQR fails, the LQR controller requires
thrusts beyond the input limits. If we increase the input limits
then LQR can stabilize many of these states. Hence by taking
input limits into consideration, the neural-network controller
achieves better performance than the LQR.

Both algorithm 1 and 2 ﬁnd the stabilizing controller. For
0.1,

a small box region [
≤
[x, z, θ, ˙x, ˙z, ˙θ]
[0.1, 0.1, 0.1π, 0.5, 0.5, 0.3], both algorithms
converge in 20 minutes. For the larger box used in Table I,
the algorithms converge in 1 day.

0.1π,

0.5,

0.5,

0.1,

0.3]

−

−

≤

−

−

−

−

C. 3D quadrotor

We apply our approach to a 3D quadrotor model with 12
states [38]. Again, our goal is to steer the quadrotor to hover
at the origin. As visualized in Fig.11, our neural-network
controller can stabilize the system. Training this controller
took 3 days.

Fig. 8: (left) phase plot of simulating the pendulum Lagrangian
dynamics with the neural-network controller. The red contour
is the boundary of the veriﬁed region of attraction, as the
largest level set within the veriﬁed box region
(black dashed
box). All the simulated trajectories (even starting outside of
the dashed box) converge to the goal state. (right) Lyapunov
function value along the simulated trajectories. The Lyapunov
function decreases monotonically along the trajectories.

B

Fig. 9: Snapshots of the 2D quadrotor simulation (with
the original Lagrangian dynamics) using our neural-network
controller from different initial states. The red lines are the
trajectories of the quadrotor body frame origin.

outside the veriﬁed region of attraction, and even outside
our veriﬁed box region, the trajectories still converge to the
equilibrium. This suggests that the controller generalizes well.
The small veriﬁed region of attraction suggests that in the
future we can improve its size by searching over the Lyapunov
function and the sub-level set simultaneously.
θ
We start with a small box region 0.8π

≤
˙θ
1, and then gradually increase the veriﬁed region. We
initialize the controller/Lyapunov network as the solution in
the previous iteration on a smaller box region (at the ﬁrst
iteration, all parameters are initialized arbitrarily). For the
smaller box 0.8π
1, both algorihm
1 and 2 converge within a few minutes. For the larger box
5, both algorithms converge within 3
θ
0
≤
hours.

≤
˙θ

1
−

1.2π,

1.2π,

2π,

−

≤

≤

−

≤

≤

≤

≤

≤

≤

≤

˙θ

5

1

θ

B. 2D quadrotor

We synthesize a stabilizing controller and a Lyapunov
function for the 2D quadrotor model used in [51]. Again
train a neural network φdyn to approximate the
we ﬁrst
Lagrangian dynamics. Our goal
is to steer the quadrotor
to hover at
the origin. In Fig.9 we visualize the snap-
shots of the quadrotor stabilized by our neural-network con-
troller. We veriﬁed the Lyapunov conditions in the region
[
4,
4,
−
[0.75, 0.75, 0.5π, 4, 4, 2.75].

[x, z, θ, ˙x, ˙z, ˙θ]

0.5π,

0.75,

0.75,

2.75]

≤

−

−

−

−

−

≤

00.5ππ1.5π2πθ(radian)-5.5-2.502.55.5˙θ(radian/s)0246time(s)02040VLyapunovfunctionvaluealongsimulatedtrajectories−101x(m)−1.0−0.50.00.51.0z(m)t=0st=0.5s−101x(m)−1.0−0.50.00.51.0z(m)t=0st=0.6s≤

−

−

≤

8]

8,

[ ˙x, ˙z]
0.75, 0.3, 0.3π, 2] (left), and [0.75, 0.5,

Fig. 10: We sample 2500 initial states within the box re-
[8, 8], with [x, z, θ, ˙θ] ﬁxed to
gion [
0.4π, 2] (right).
[
−
We simulate from each initial state with the neural-network
controller (NN) and the LQR controller, and color each initial
state based on whether the simulation converges to the goal.
All red dots (where the NN controller fails) are outside of
the black box region within which we veriﬁed the Lyapunov
conditions.

−

Fig. 11: Snapshots of simulating the quadrotor using our
neural-network controller with the Lagrangian dynamics. The
quadrotor converge to the hovering state at the origin (red).

The quadrotor Lagrangian dynamics is approximated by a
neural network φdyn with comparatively large mean-squared
error (MSE) around 10−4 (reducing MSE would require a
neural network too large for our MIP solver), while other
examples in this paper have MSE in the order of 10−6. Hence
there are noticeable discrepancies between the simulation with
Lagrangian dynamics and with the neural network dynamics
φdyn. In Fig 12 we select some results to highlight the dis-
crepancy, that the Lyapunov function always decreases along
the trajectories simulated with neural-network dynamics, while
it could increase with Lagrangian dynamics. Nevertheless,
the quadrotor eventually always converges to the goal state.
We note that the same phenomenon would also happen if
we took a linear approximation of the quadrotor dynamics
and stabilized the quadrotor with an LQR controller. If we
were to plot the quadratic Lyapunov function (which is valid
for the LQR controller and the linearized dynamics), that
Lyapunov function could also increase along the trajectories
simulated with the nonlinear Lagrangian dynamics (see Fig 14
in the Appendix). Analogous to approximating the nonlinear
dynamics with a linear one and stabilizing it with a linear LQR
controller, our approach can be regarded as approximating the
nonlinear dynamics with a neural network and stabilizing it
with another neural-network controller.

Finally we compare the performance of Algorithm 1 which
appends counter examples to training sets, against Algorithm

Fig. 12: 3D quadrotor Lyapunov function value along the
simulated trajectories with our neural-network controller. The
quadrotor is simulated with Lagrangian dynamics (left) vs the
dynamics approximated by a neural network φdyn (right). In
both left and right sub-plots, the initial states are the same.

Pendulum
2D quadrotor
3D quadrotor

Algorithm 1
8.4s
948.3s
Time out after 5 days

Algorithm 2
224s
1004.7s
65.7hrs

TABLE II: Average computation time of 10 runs for both
algorithms. To speed up the computation, the veriﬁed region

is relatively small.

B

2 with min-max optimization. We take 10 runs of each
algorithm, and report the average computation time for each
algorithm in Table II 4. For the small-sized task (pendulum
with 2 states), Algorithm 1 is orders of magnitude faster
than Algorithm 2, and they take about the same time on
the medium-sized task (2D quadrotor with 6 states). On the
large-sized task (3D quadrotor with 12 states), Algorithm 1
doesn’t converge while Algorithm 2 can ﬁnd the solution. We
speculate this is because Algorithm 1 overﬁts to the training
set. For a small-sized task the overﬁtting is not a severe
problem as a small number of sampled states are sufﬁcient
to represent the state space; while for a large-sized task it
would require a huge number of samples to cover the state
space. With the limited number of counter examples Algorithm
1 overﬁts to these samples while causing large Lyapunov
condition violation elsewhere. This is evident from the loss
curve plot in Fig.13 for a 2D quadrotor task. Although both
algorithms converge, the loss curve decreases steadily with
Algorithm 2, while it ﬂuctuates wildly with Algorithm 1.
We believe that the ﬂuctuation is caused by overﬁtting to
the training set in the previous iteration. Nevertheless, this
comparison is not yet conclusive, and we are working to
improve the performance of Algorithm 1 on the large-size task.

VI. CONCLUSION AND FUTURE WORK

In this paper, we demonstrate a method to synthesize a
neural-network controller to stabilize a robot system, as well
as a neural-network Lyapunov function to prove the resulting
stability. We propose an MIP veriﬁer to either certify Lyapunov
stability, or generate counter examples that can be used to
improve the candidate controller and the Lyapunov function.

4There are 2 failed runs with Algorithm 1 on the 2D quadrotor example,
that they time out after 6 hours, and are not included in Table II. For the 3D
quadrotor we only include 3 runs as they are too time-consuming

7.55.02.50.02.55.07.5x (m/s)864202468z (m/s)7.55.02.50.02.55.07.5x (m/s)864202468z (m/s)0.02.55.07.510.0time (s)0.00.10.20.3VSimulated with Lagrangian dynamics0.02.55.07.510.0time (s)0.00.10.20.3VSimulated with dyn dynamicssolely reﬂects the opinions and conclusions of its authors and
not any funding agencies. We would like to thank Vincent
Tjeng and Shen Shen for the valuable discussion.

REFERENCES

[1] Alessandro Abate, Daniele Ahmed, Mirco Giacobbe, and
Andrea Peruffo. Formal synthesis of lyapunov neural
networks. IEEE Control Systems Letters, 5(3):773–778,
2020.

[2] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen
Boyd, Steven Diamond, and Zico Kolter. Differentiable
In Advances in neural
convex optimization layers.
information processing systems, 2019.

[3] Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and
Paulo Tabuada. Control barrier function based quadratic
programs for safety critical systems. IEEE Transactions
on Automatic Control, 62(8):3861–3876, 2016.

[4] Brandon Amos and J Zico Kolter. Optnet: Differentiable
optimization as a layer in neural networks. In Interna-
tional Conference on Machine Learning, pages 136–145.
PMLR, 2017.

[5] Ross Anderson, Joey Huchette, Will Ma, Christian Tjan-
draatmadja, and Juan Pablo Vielma. Strong mixed-integer
programming formulations for trained neural networks.
Mathematical Programming, pages 1–37, 2020.

[6] Jonathan F Bard. Practical bilevel optimization: algo-
rithms and applications, volume 30. Springer Science &
Business Media, 2013.

[7] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos,
Dimitrios Vytiniotis, Aditya V Nori, and Antonio Crim-
inisi. Measuring neural net robustness with constraints.
In Proceedings of the 30th International Conference on
Neural Information Processing Systems, pages 2621–
2629, 2016.

[8] R Bonna and JF Camino. Trajectory tracking control of a
quadrotor using feedback linearization. In International
Symposium on Dynamic Problems of Mechanics, 2015.
[9] Stephen Boyd, Laurent El Ghaoui, Eric Feron, and
Venkataramanan Balakrishnan. Linear matrix inequal-
ities in system and control theory. SIAM, 1994.

[10] Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet
Kohli, and M Pawan Kumar. A uniﬁed view of piecewise
linear neural network veriﬁcation. In Advances in Neural
Information Processing Systems, 2018.

[11] Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural
lyapunov control. Advances in neural information pro-
cessing systems, 2019.

[12] Shaoru Chen, Mahyar Fazlyab, Manfred Morari,
George J Pappas, and Victor M Preciado. Learning
lyapunov functions for hybrid systems. arXiv preprint
arXiv:2012.12015, 2020.

[13] Chih-Hong Cheng, Georg N¨uhrenberg, and Harald Ruess.
In
Maximum resilience of artiﬁcial neural networks.
International Symposium on Automated Technology for
Veriﬁcation and Analysis, pages 251–268. Springer,
2017.

Fig. 13: Loss curves on the 2D quadrotor task for Algorithm
1 and 2

We present another MIP to compute an inner approximation
of the region of attraction. We demonstrate our approach on
an inverted pendulum, 2D and 3D quadrotors, and showcase
that it can outperform a baseline LQR controller.

Currently, the biggest challenge of our approach is scal-
ability. The speed bottleneck lies in solving MIPs, where
the number of binary variables scales linearly with the total
number of neurons in the networks. In the worst case, the
complexity of solving an MIP scales exponentially with the
number of binary variables, when the solver has to check
every node of a binary tree. However, in practice, the branch-
and-cut process signiﬁcantly reduces the number of nodes to
explore. Recently, with the growing interest from the machine
learning community, many approaches were proposed to speed
up verifying neural networks through MIP by tightening the
formulation [5, 54]. We plan to explore these approaches in
the future.

Our proposed MIP formulation works for discrete-time
dynamical systems. For continuous-time dynamical systems,
neural networks have been previously used either to approxi-
mate the system dynamics [34], or to synthesize optimal con-
trollers [15]. We plan to extend our approach to continuous-
time dynamical systems. Moreover, we can readily apply our
approach to systems whose dynamics are approximated by
piecewise-afﬁne dynamical systems, such as soft robots [53]
and hybrid systems with contact [37], since piecewise-afﬁne
dynamic constraints can easily be encoded into MIP.

Many safety-critical missions also require the robot to avoid
unsafe regions. We can readily extend our framework to
synthesize barrier functions [3] so that the robot certiﬁably
stays within the safe region.

ACKNOWLEDGMENTS

Benoit Landry is sponsored by the NASA University Lead-
ership initiative (grant #80NSSC20M0163), and Lujie Yang is
sponsored by Amazon Research Award #6943503. This article

02505007501000time(s)0.00000.00010.00020.00030.00040.0005MIP(10a)optimalcostLosscurvewithAlgorithm102505007501000time(s)0.0000.0020.0040.0060.008MIP(10b)optimalcostLosscurvewithAlgorithm105001000time(s)0.00000.00010.00020.00030.00040.0005MIP(10a)optimalcostLosscurvewithAlgorithm205001000time(s)0.0000.0020.0040.0060.008MIP(10b)optimalcostLosscurvewithAlgorithm2[14] Hongkai Dai, Benoit Landry, Marco Pavone, and Russ
Tedrake. Counter-example guided synthesis of neural
network lyapunov functions for piecewise linear systems.
In 2020 59th IEEE Conference on Decision and Control
(CDC), pages 1274–1281. IEEE, 2020.

[15] Farbod Farshidian, David Hoeller, and Marco Hutter.
Deep value model predictive control. In 3rd Conference
on Robot Learning (CoRL 2019), 2019.

[16] Nima Fazeli, Miquel Oller, Jiajun Wu, Zheng Wu,
Joshua B Tenenbaum, and Alberto Rodriguez. See, feel,
act: Hierarchical learning for complex manipulation skills
with multisensory fusion. Science Robotics, 4(26), 2019.
[17] Mahyar Fazlyab, Manfred Morari, and George J Pappas.
Safety veriﬁcation and robustness analysis of neural
networks via quadratic constraints and semideﬁnite pro-
IEEE Transactions on Automatic Control,
gramming.
2020.

[18] Matteo Fischetti and Jason Jo. Deep neural networks
and mixed integer linear optimization. Constraints, 23
(3):296–309, 2018.

[19] Peter R Florence, Lucas Manuelli, and Russ Tedrake.
Dense object nets: Learning dense visual object descrip-
tors by and for robotic manipulation. In Conference on
Robot Learning, pages 373–385. PMLR, 2018.

[20] John Forrest and Robin Lougee-Heimer. Cbc user guide.
In Emerging theory, methods, and applications, pages
257–277. INFORMS, 2005.

[21] Morgan T Gillespie, Charles M Best, Eric C Townsend,
David Wingate, and Marc D Killpack. Learning nonlinear
dynamic models of soft robots for model predictive
control with neural networks. In 2018 IEEE International
Conference on Soft Robotics (RoboSoft), pages 39–45.
IEEE, 2018.

[22] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic
actor. In International Conference on Machine Learning,
pages 1861–1870. PMLR, 2018.

[23] Jemin Hwangbo, Inkyu Sa, Roland Siegwart, and Marco
Hutter. Control of a quadrotor with reinforcement learn-
ing. IEEE Robotics and Automation Letters, 2(4):2096–
2103, 2017.

[24] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario
Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco
Hutter. Learning agile and dynamic motor skills for
legged robots. Science Robotics, 4(26), 2019.

[25] Brian Ichter, James Harrison, and Marco Pavone. Learn-
ing sampling distributions for robot motion planning. In
2018 IEEE International Conference on Robotics and
Automation (ICRA), pages 7087–7094. IEEE, 2018.
[26] Zachary Jarvis-Wloszek, Ryan Feeley, Weehong Tan,
Kunpeng Sun, and Andrew Packard.
Some controls
In 42nd
applications of sum of squares programming.
IEEE international conference on decision and control
(IEEE Cat. No. 03CH37475), volume 5, pages 4676–
4681. IEEE, 2003.

[27] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,
et al. Qt-opt: Scalable deep reinforcement
learning
for vision-based robotic manipulation. arXiv preprint
arXiv:1806.10293, 2018.

[28] James Kapinski,

Jyotirmoy V Deshmukh, Sriram
Sankaranarayanan, and Nikos Arechiga.
Simulation-
guided lyapunov analysis for hybrid dynamical systems.
In Proceedings of the 17th international conference on
Hybrid systems: computation and control, pages 133–
142, 2014.

[29] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and
Mykel J Kochenderfer. Reluplex: An efﬁcient smt solver
In International
for verifying deep neural networks.
Conference on Computer Aided Veriﬁcation, pages 97–
117. Springer, 2017.

[30] Wadim Kehl, Fabian Manhardt, Federico Tombari, Slo-
bodan Ilic, and Nassir Navab. Ssd-6d: Making rgb-
based 3d detection and 6d pose estimation great again.
In Proceedings of the IEEE international conference on
computer vision, pages 1521–1529, 2017.

[31] Benoit Landry, Zachary Manchester, and Marco Pavone.
A differentiable augmented lagrangian method for bilevel
In Proceedings of Robotics:
nonlinear optimization.
Science and Systems, 2019.

[32] Joonho Lee,

Jemin Hwangbo, Lorenz Wellhausen,
Vladlen Koltun, and Marco Hutter. Learning quadrupedal
locomotion over challenging terrain. Science robotics, 5
(47), 2020.

[33] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and
Shimon Schocken. Multilayer feedforward networks with
a nonpolynomial activation function can approximate any
function. Neural networks, 6(6):861–867, 1993.

[34] Michael Lutter, Christian Ritter, and Jan Peters. Deep
lagrangian networks: Using physics as model prior for
deep learning. In International Conference on Learning
Representations, 2018.

[35] Anirudha Majumdar, Amir Ali Ahmadi, and Russ
Tedrake. Control design along trajectories with sums
In 2013 IEEE International
of squares programming.
Conference on Robotics and Automation, pages 4054–
4061. IEEE, 2013.

[36] Lucas Manuelli, Yunzhu Li, Pete Florence, and Russ
Tedrake. Keypoints into the future: Self-supervised
correspondence in model-based reinforcement learning.
arXiv preprint arXiv:2009.05085, 2020.

[37] Tobia Marcucci and Russ Tedrake. Mixed-integer formu-
lations for optimal control of piecewise-afﬁne systems. In
Proceedings of the 22nd ACM International Conference
on Hybrid Systems: Computation and Control, pages
230–239, 2019.

[38] Daniel Mellinger and Vijay Kumar. Minimum snap
trajectory generation and control for quadrotors.
In
2011 IEEE international conference on robotics and
automation, pages 2520–2525. IEEE, 2011.

[39] Gurobi Optimization.
manual,” 2015, 2014.

Inc.,“gurobi optimizer reference

[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning
continuous signed distance functions for shape represen-
tation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 165–
174, 2019.

[41] Samuel Pfrommer, Mathew Halm, and Michael Posa.
Contactnets: Learning of discontinuous contact dynamics
arXiv preprint
with smooth, implicit representations.
arXiv:2009.11193, 2020.

[42] Hadi Ravanbakhsh and Sriram Sankaranarayanan. Learn-
ing control
lyapunov functions from counterexamples
and demonstrations. Autonomous Robots, 43(2):275–307,
2019.

[43] Spencer M Richards, Felix Berkenkamp, and Andreas
Krause. The lyapunov neural network: Adaptive stability
certiﬁcation for safe learning of dynamical systems. In
Conference on Robot Learning, pages 466–476. PMLR,
2018.

[44] John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz. Trust region policy optimiza-
In International conference on machine learning,
tion.
pages 1889–1897. PMLR, 2015.

[45] Shen Shen. Convex Optimization and Machine Learn-
ing for Scalable Veriﬁcation and Control. PhD thesis,
Massachusetts Institute of Technology, 2020.
[46] Jean-Jacques E Slotine, Weiping Li, et al.

Applied
nonlinear control, volume 199. Prentice hall Englewood
Cliffs, NJ, 1991.

[47] Eduardo D Sontag. Stability and stabilization: discon-
In Nonlinear
tinuities and the effect of disturbances.
analysis, differential equations and control, pages 551–
598. Springer, 1999.

[48] Niko S¨underhauf, Oliver Brock, Walter Scheirer, Raia
Hadsell, Dieter Fox, J¨urgen Leitner, Ben Upcroft, Pieter
Abbeel, Wolfram Burgard, Michael Milford, et al. The
limits and potentials of deep learning for robotics. The
International Journal of Robotics Research, 37(4-5):405–
420, 2018.

[49] Richard S Sutton and Andrew G Barto. Reinforcement

learning: An introduction. MIT press, 2018.

[50] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen,
Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent
Vanhoucke. Sim-to-real: Learning agile locomotion for
arXiv preprint arXiv:1804.10332,
quadruped robots.
2018.

[51] Russ Tedrake. Underactuated robotics: Learning, plan-
ning, and control for efﬁcient and agile machines. Course
notes for MIT, 6:832, 2009.

[52] Vincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evalu-
ating robustness of neural networks with mixed integer
programming. In International Conference on Learning
Representations, 2018.

[53] Sander Tonkens, Joseph Lorenzetti, and Marco Pavone.

forward dy-
namics net-
work φdyn

Inverted pen-
dulum
2D quadrotor
3D quadrotor

(5, 5)

(7, 7)
(10, 10)

controller
network φπ

Lyapunov
network φV

(2, 2)

(6, 4)
(16, 8)

(8, 4, 4)

(10, 10, 4)
(16, 12, 8)

TABLE III: Size of the neural network hidden layers in each
task.

Soft robot optimal control via reduced order ﬁnite ele-
ment models. arXiv preprint arXiv:2011.02092, 2020.

[54] Calvin Tsay, Jan Kronqvist, Alexander Thebelt, and Ruth
Misener. Partition-based formulations for mixed-integer
arXiv
optimization of trained relu neural networks.
preprint arXiv:2102.04373, 2021.

[55] Juan Pablo Vielma, Shabbir Ahmed, and George
Nemhauser. Mixed-integer models for nonseparable
piecewise-linear optimization: Unifying framework and
extensions. Operations research, 58(2):303–315, 2010.
[56] Grady Williams, Nolan Wagener, Brian Goldfain, Paul
Drews, James M Rehg, Byron Boots, and Evangelos A
Theodorou. Information theoretic mpc for model-based
In 2017 IEEE International
reinforcement
Conference on Robotics and Automation (ICRA), pages
1714–1721. IEEE, 2017.

learning.

[57] Eric Wong and Zico Kolter. Provable defenses against
adversarial examples via the convex outer adversarial
In International Conference on Machine
polytope.
Learning, pages 5286–5295. PMLR, 2018.

VII. APPENDIX

A. Network structures in each example

For each task in the result section, we use three fully
connected feedforward neural networks to represent forward
the control policy and the Lyapunov function
dynamics,
respectively. All
the networks use leaky ReLU activation
function. The size of the network is summarized in Table
III. Each entry represents the size of the hidden layers. For
example (3, 4) represents a neural network with 2 hidden
layers, the ﬁrst hidden layer has 3 neurons, and the second
hidden layer has 4 neurons.

B. MIP formulation for l1 norm and clamp function

For the l1 norm constraint on x

Rn

∈

y =

x
|

|

1

(16)

u, we
where x is bounded elementwisely as l
convert this constraint (16) to the following mixed-integer
linear constraint

≤

≤

x

zi

≤

xi + 2li(αi

−

y = z1 + . . . + zn
xi
zi
xi, zi
≥
xi
1), zi
−
0, 1

2uiαi
α

≥ −

≤

∈ {

}

(17a)

(17b)

(17c)

(17d)

Fig. 14: Value of the Lyapunov function V = xT Sx for an
LQR controller on a 3D quadrotor.

network dynamics, the discrepancy between the approximated
dynamics and the actual dynamics will likewise cause the
Lyapunov function to increase on the actual dynamical system.

E. Termination tolerance

In practice, due to solver’s numerical tolerance, we declare
convergence of Algorithm 1 and 2 when the MIPs (10a) (10b)
has optimal cost in the order of 10−6.

where we assume l < 0, u > 0 (the case when l
is trivial).

≥

0 or u

0

≤

For a clamp function

y =






l, if x
x, if l
u, if x

l
x
u

≤
≤
≥

u

≤

(18)

This clamp function can be rewritten in the following form
using ReLU function

y = u

ReLU (u

(ReLU (x

l) + l))

−

−
As explained in (4) in section III, we can convert ReLU
function to mixed-integer linear constraints, hence we obtain
the MIP formulation of (18).

−

(19)

C. Necessity of condition (9)

Lemma 7.1: There exists a piecewise-afﬁne function V (x) :

Rn

R satisfying

→

V (x∗) = 0, V (x) > 0

= x∗

x

∀

(20)

if and only if for a given positive scalar (cid:15) > 0 and a given full
column-rank matrix R, there exists another piecewise-afﬁne
function ¯V (x) : Rn

R satisfying

→

¯V (x∗) = 0, ¯V (x)

(cid:15)

R(x
|

−

≥

1
|

∀

x∗)

x

= x∗

(21)

∈

−

R(x
|

Proof: The “only if” part is trivial, if such ¯V (x) exists,
then just setting V (x) = ¯V (x) and (20) holds. To prove
the “if” part, assume that V (x) exists, and we will show
that there exists a positive scalar, such that by scaling V (
)
·
we get ¯V (
). Intuitively this scalar could be found as the
·
x∗)
smallest ratio between V (x) and (cid:15)
1. Formally,
|
Rn, we deﬁne a scalar function
given a unit length vector d
R is
) : R
φd(t) = V (x∗ + td), this scalar function φ(
→
·
also piecewise-afﬁne, as it is just the value of V (
) along
·
the direction d. Likewise we deﬁne another scalar function
ψd(t) = (cid:15)
1 which is just the value of the right-hand
side of (21) along the direction d. Since both φd(t), ψd(t)
are piecewise-afﬁne and positive deﬁnite, the minimal ratio
ζ(d) = mint(cid:54)=0 φd(t)/ψd(t) is strictly positive. Moreover,
consider the value κ = mindT d=1 ζ(d), since the domain
dT d = 1
d
is a compact set, and the minimal of a positive
{
}
|
function ζ(d) on a compact set is still positive, hence κ > 0.
As a result, setting ¯V (
)/κ will satisfy (21), because
) = V (
·
·
¯V (x) = ¯V (x∗ + td) = V (x∗ + td)/κ = φd(t)/κ
φd(t)/ζ(d)
≥
where we choose d = (x

R(x
φd(t)/(φd(t)/ψd(t)) = ψd(t) = (cid:15)
|
x∗)/
x
and t =
x
|
|

≥
x∗)
1
|
.

R(td)
|
|

−
D. LQR simulation on 3D quadrotor

−
x∗

x∗

−

−

|

|

We simulate the 3D quadrotor with an LQR controller, and
plot its Lyapunov function xT Sx (where S is the solution
to the Riccati equation) along the simulated trajectories in
then this quadratic
Fig. 14. If the dynamics were linear,
Lyapunov function would always decrease; on the other hand,
with the actual nonlinear dynamics the Lyapunov function
(for the linear dynamical system) can increase. For our neural

012345t (s)020406080100V=xTSxLQR Lyapunov function value along simulated trajectories(cid:54)
(cid:54)
