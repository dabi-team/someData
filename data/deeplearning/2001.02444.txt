Learning to Encode and Classify Test Executions

Foivos Tsimpourlas
University of Edinburgh
F.Tsimpourlas@sms.ed.ac.uk

Ajitha Rajan
University of Edinburgh
arajan@ed.ac.uk

Miltiadis Allamanis
Microsoft Research
miallama@microsoft.com

0
2
0
2

n
a
J

8

]
E
S
.
s
c
[

1
v
4
4
4
2
0
.
1
0
0
2
:
v
i
X
r
a

ABSTRACT
The challenge of automatically determining the correctness of test
executions is referred to as the test oracle problem and is one of the
key remaining issues for automated testing. The goal in this paper
is to solve the test oracle problem in a way that is general, scalable
and accurate.

To achieve this, we use supervised learning over test execution
traces. We label a small fraction of the execution traces with their
verdict of pass or fail. We use the labelled traces to train a neu-
ral network (NN) model to learn to distinguish runtime patterns
for passing versus failing executions for a given program. Our ap-
proach for building this NN model involves the following steps, 1.
Instrument the program to record execution traces as sequences of
method invocations and global state, 2. Label a small fraction of the
execution traces with their verdicts, 3. Designing a NN component
that embeds information in execution traces to fixed length vectors,
4. Design a NN model that uses the trace information for classi-
fication, 5. Evaluate the inferred classification model on unseen
execution traces from the program.

We evaluate our approach using case studies from different appli-
cation domains - 1. Module from Ethereum Blockchain, 2. Module
from PyTorch deep learning framework, 3. Microsoft SEAL en-
cryption library components, 4. Sed stream editor, 5. Value pointer
library and 6. Nine network protocols from Linux packet identifier,
L7-Filter. We found the classification models for all subject pro-
grams resulted in high precision, recall and specificity, over 95%,
while only training with an average 9% of the total traces. Our ex-
periments show that the proposed neural network model is highly
effective as a test oracle and is able to learn runtime patterns to
distinguish passing and failing test executions for systems and tests
from different application domains.

ACM Reference Format:
Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis. 2020. Learning
to Encode and Classify Test Executions. In Proceedings of ACM Conference
(Conference’17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

1 INTRODUCTION
As the scale and complexity of software increases, the number of
tests needed for effective validation becomes extremely large, slow-
ing down development, hindering programmer productivity, and

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ultimately making development costly [11]. The need for large num-
bers of tests is magnified in agile software development practices,
like Continuous Integration (CI) and Test-Driven Development
(TDD), that require extensive testing to be performed [22]. To make
testing faster, cheaper and more reliable, it is desirable to automate
as much of the process as possible.

One of the biggest hurdles in test automation is the test oracle -
“a procedure that distinguishes between the correct and incorrect
behaviours of the System Under Test (SUT)” [13]. Literature refers
to the challenge of automatically determining the correctness of
test executions as the test oracle problem and acknowledges it as
one of the key remaining issues for automated testing [13]. Recent
surveys on the test oracle problem [13, 26, 28] show that existing
techniques based on formal specifications, metamorphic relations
and independent program versions are not widely applicable and
difficult to use in practice.

Key Idea. In this paper, we explore supervised machine learn-
ing to infer a test oracle from labelled execution traces of a given
system. In particular, we use neural networks (NNs), well suited
to learning complex functions, to design the test oracles. We be-
lieve NNs would be a good fit as they can help understand the
runtime patterns of a system that indicate correct versus incorrect
executions. Previous work exploring the use of NNs for test oracles
has been in a restricted context – applied to very small programs
with primitive data types, and only considering their inputs and
outputs [24, 33]. Dynamic execution data and program state has not
been considered by existing NN-based approaches. Recent work
using NNs over programs to predict method or variable names and
detecting name-based bug patterns [9, 31] relies on static program
information, namely, embeddings of the Abstract Syntax Tree (AST)
or source code. Our proposed approach is the first attempt at using
dynamic execution trace information in NN models for classifying
program executions.

Our approach for inferring a test oracle has the following steps,
(1) Instrument a program to gather execution traces as sequences

of method invocations and the final global state.

(2) Label a small fraction of the traces with their classification

decision.

(3) Design a NN component that embeds the execution traces

to fixed length vectors.

(4) Design a NN component that uses the line-by-line trace

information to classify traces as pass or fail.

(5) Train a NN model that combines the above components and
evaluate it on unseen execution traces for that program.

The novel contributions in this paper are in Steps 3, 4 and 5. Ex-
ecution traces from a program vary widely in their length and
information. We propose a technique to encode and summarise the
information in a trace to a fixed length vector that can be handled
by a NN. We then design and train a NN to serve as a test oracle.

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

Labelled execution traces. Effectively learning a NN classi-
fier for a SUT that distinguishes correct from incorrect executions
requires labelled data with both passing and failing examples of
traces. Given a SUT and a test suite, we gather execution traces
corresponding to each of the test inputs in the test suite with our
instrumentation framework. We require a small fraction of the over-
all execution traces to be labelled. We hypothesize that the time
invested in labelling a small proportion of the traces is justified
with respect to the benefit gained in automatically classifying the
remaining majority of traces. In contrast, with no classifier, the de-
veloper would have had to specify expected output for all the tests,
which is clearly more time consuming than the small proportion of
tests we need labelled. It is common for case studies to have many
passing test inputs but only a limited number of failing tests. To
address this imbalance in training, we generate failing traces by
mutating the existing code with common bugs. Test inputs that
were labelled with passing traces through the original code will be
labelled with failing traces through the mutated code if the output
deviates from the expected output.

NN Architecture. An execution trace in our approach com-
prises multiple lines, with each line containing information on a
method invocation or the global state. Our architecture for encod-
ing and classifying an execution trace uses multiple components:
(1) Value encoder for encoding values within the trace line to a
distributed vector representation, (2) Trace encoder encoding trace
lines within a variable-length trace to a single vector, and (3) Trace
Classifier that accepts the trace representation and global variable
values and classifies the trace. The components in our architec-
ture is made up of LSTMs, one-hot encoders, and a multi-layer
perceptron.

Case Studies. We evaluate our approach using 15 subject pro-
grams and tests from different application domains - a single module
from Ethereum project [4], a module from Pytorch [29], two differ-
ent components within Microsoft SEAL encryption library [32], a
smart pointer library [2], a Linux stream editor [1] and 9 network
protocols from L7-filter [3]. We found our approach for designing a
NN classification model was feasible and effective for each of these
programs. We achieved high accuracies in detecting both failing
and passing traces, with an average precision of 97% and recall of
98%. Only a small fraction of the overall traces (average 9%) needed
to be labelled for training the classification models.
In summary, the paper makes the following contributions:

• Given a SUT and its test inputs, we provide a framework
that instruments the SUT and gathers test execution traces
as sequences of method invocations and final global state.
• A NN component for encoding variable-sized execution

traces into fixed length vectors.

• A NN for classifying the execution traces as pass or fail.
• We provide empirical evidence that this approach yields
effective test oracles for programs and tests from different
application domains.

2 BACKGROUND
When a test oracle observes a test execution, it returns a test verdict,
which is either pass or fail, depending on whether the observations
match expected behaviour. A test execution is execution of the SUT
with a test input. The importance of oracles as an integral part of

the testing process has been a key topic of research for over three
decades. We distinguish four different kinds of test oracles, based on
the survey by Barr et al. in 2015 [13]. The most common form of test
oracle is a specified oracle, one that judges behavioural aspects of the
system under test with respect to formal specifications. Although
formal specifications are effective in identifying failures, defining
and maintaining such specifications is expensive and also relatively
rare in practice. Implicit test oracles require no domain knowledge
and are easy to obtain at practically no cost. However, they are
limited in their scope as they are only able to reveal particular
anomalies in program executions like buffer overflows, segmen-
tation faults, deadlocks. Derived test oracles use documentations
or system executions, to judge a system’s behaviour, when spec-
ified test oracles are unavailable. However, derived test oracles,
like metamorphic relations and inferring invariants, is either not
automated or it is inaccurate and irrelevant making it a challenge
for practical use.

For many systems and much of testing as currently practised in
industry, the tester does not have the luxury of formal specifications
or assertions or even automated partial oracles [18, 19]. Statistical
analysis and machine learning techniques provide a useful alter-
native for understanding software behaviour using data gathered
from a large set of test executions.

2.1 Machine Learning for Software Testing
Briand et al. [15], in 2008, presented a comprehensive overview
of existing techniques that apply machine learning for addressing
testing challenges. Among these, the closest related work is that
of Bowring et al. in 2004 [14]. They proposed an active learning
approach to build a classifier of program behaviours using a fre-
quency profile of single events in the execution trace. Evaluation
of their approach was conducted over one small program whose
specific structure was well suited to their technique. Machine learn-
ing techniques have also been used in fault detection. Brun and
Ernst, in 2004 [16], explored the use of support vector machines
and decision trees to rank program properties, provided by the user,
that are likely to indicate errors in the program. Podgurski et al.,
in 2003 [30], use clustering over function call profiles to determine
which failure reports are likely to be manifestations of an underly-
ing error. A training step determines which features are of interest
by evaluating features that enable a model to distinguish failures
from non-failures, The technique does not consider passing runs.
In their experiments, for most clusters, the cluster contains failures
resulting from a single error.

More recently, Almaghairbe et al. [8] proposed an unsupervised
learning technique to classify unlabelled execution traces of simple
programs. They gather two kinds of execution traces, one with
only inputs and outputs, and another that includes the sequence of
method entry and exit points, with only method names. Arguments
and return values are not used. They use agglomerative hierarchical
clustering algorithms to build an automated test oracle, assuming
passing traces are grouped into large, dense clusters and failing
traces into many small clusters. They evaluate their technique on 3
programs from the SIR repository [17]. The proposed approach has
several limitations. They only support programs with strings as
inputs. They do not consider correct classification of passing traces.

Learning to Encode and Classify Test Executions

Conference’17, July 2017, Washington, DC, USA

The accuracy achieved by the technique is not high, classifying
approximately 60% of the failures. Additionally, fraction of outputs
that need to be examined by the developer is around 40% of the total
tests, which is considerably higher than the labelled data used in our
approach. We objectively compared the accuracy achieved by the
hierarchical clustering technique against our approach using 15 case
studies, discussed in Section 5. We found that our approach achieves
significantly higher accuracy in classifying program executions
across all case studies.

Existing work using execution traces for bug detection has pri-
marily been based on clustering techniques. Neural networks, es-
pecially with deep learning, have been very successful for complex
classification problems in other domains like natural language pro-
cessing, speech recognition, computer vision. There is limited work
exploring their benefits for software testing problems.

Neural Networks for Test Oracles. NNs were first used by Vanmali
et al. [33] in 2002 to simulate behaviour of simple programs using
their previous version, and applied this model to regression testing
of unchanged functionalities. Aggarwal et al. [5] and Jin et al. [24]
applied the same approach to test a triangle classification program,
that computes the relationship among three edge inputs to deter-
mine the type of triangle. The few existing approaches using NNs
have been applied to simple programs having small I/O domains.
The following challenges have not been addressed in existing work,
1. Training with test execution data and their vector representation
– Existing work only considers program inputs and outputs that
are of primitive data types (integers, doubles, characters). Test data
for real programs often use complex data structures and data types
defined in libraries. There is a need for techniques that encode such
data. In addition, existing work has not attempted to use program
execution information in NNs to classify tests. Achieving this will
require novel techniques for encoding execution traces and design-
ing a NN that can learn from them.
2. Test oracles for industrial case studies - Realistic programs with
complex behaviours and input data structures has not been previ-
ously explored.
3. Effort for generating labelled training data - Training data in
existing work has been over simple programs, like the triangle clas-
sification program, where labelling the tests was straightforward.
Generating labelled data for failing tests has not been previously
addressed. Additionally, the proportion of labelled test data to total
number of tests needed and its effect on model prediction accuracy
has not been systematically explored.

Deep Learning for Software Testing. The performance of neural
networks as classifiers was boosted with the birth of deep learn-
ing in 2006 [20]. Deep learning methods have not been explored
extensively for software testing, and in particular for the test oracle
problem. Recently, a few techniques have been proposed for auto-
matic pattern-based bug detection. For example, Pradel et al. [31]
proposed a deep learning-based static analysis for automatic name-
based bug detection and Allamanis et al. [6] used graph-based
neural static analysis for detecting variable misuse bugs. In addi-
tion to these techniques, several other deep learning methods for
statically representing code have been developed [7, 10]. We do
not discuss these further since we are interested in execution trace

classification and in NNs that use dynamic trace information rather
than a static view of the code.

Embedding Execution Traces for Neural Networks. One of the
main contributions in this paper is an approach for embedding
information in execution traces as a fixed length vector to be fed
into the neural network. There is limited work in using representa-
tions of execution traces. Wang et al. [34] proposed embeddings of
execution traces in 2017. They use execution traces captured as a
sequence of variable values at different program points. A program
point is when a variable gets updated. Their approach uses recurrent
NNs to summarise the information in the execution trace. Embed-
ding of the traces is applied to an existing program repair tool. The
work presented by Wang et al. has several limitations - 1. Capturing
execution traces as sequences of updates to every variable in the
program has an extremely high overhead and will not scale to large
programs. The paper does not describe how the execution traces are
captured, they simply assume they have them. 2. The approach does
not discuss how variables of complex data types such as structs,
arrays, pointers, objects are encoded. It is not clear if the traces only
capture updates to user-defined variables, or if system variables are
also taken into account. 3. The evaluation uses three simple, small
programs (eg. counting parentheses in a string) from students in an
introductory programming course. The complexity and scale of real
programs is not assessed in their experiments. Their technique for
capturing and directly embedding traces as sequences of updates
to every variable is infeasible in real programs. Our approach cap-
tures and embeds traces as sequences of method invocations and
updates to global variables, which scales better than tracking every
program variable. We have implemented our instrumentation in
the LLVM compiler framework that is language agnostic and scales
to industry-sized programs. We support all types of variables and
objects, including system defined variables.

3 APPROACH
Our approach for building an automated test oracle for classifying
execution traces has the following steps,
Step 1: Instrument the SUT to gather traces when executing the

test inputs.

Step 2: Preprocess the traces to prune unnecessary information.
Step 3: Encode the preprocessed traces into vectors that can be

accepted by the neural network.

Step 4: Design a NN model that takes as input an encoded trace,

and outputs a verdict of pass or fail for that trace.

Figure 1a illustrates the steps in our approach, with the bottom half
of the figure depicting steps 3 and 4 for any given preprocessed
trace from step 2. We discuss each of the steps in the rest of this
Section.

3.1 Instrument and Gather Traces
For every test input executed through the SUT, we aim to collect
an execution trace as a sequence of method invocations, where we
capture the name of the method being called, values and data types
of parameters, return values and their types, and, finally, the name
of the parent method in the call graph. We also capture a snapshot
of the final global state before execution terminates. We find gather-
ing further information, eg. updates to local variables within each

Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

(a) Gathering traces, encoding them, and using NNs to classify them.

(b) Encoder 1 representing a single line within an execution
trace as a vector containing function caller, callee names, ar-
guments and return values.

Figure 1: High-level architecture of our approach and Encoder 1 description.

depth first fashion, until all primitive types are traced. For pointers,
we monitor the values they refer to.

The second part of the trace represents the final global state
before termination and contains the values of all global variables.
Primitive types have a single value, encoded as a number. For com-
plex types, like a class object, the variable is associated with a
sequence of values, for the internal fields.

3.2 Labelled Passing and Failing Traces
We execute the instrumented program with each test input in the
test suite to gather a set of traces. A subset of the traces is la-
belled and used in training the classification model, discussed in
Section 3.4. For the subject programs in our experiment, expected
output is provided with the tests and we use that for labelling the
trace. It is worth noting that in our approach, the developer will
only need to provide expected outputs for a small proportion of tests
rather than the whole test suite. In current practices, the developer
or tester provides expected outputs for the whole test suite which is
considerably more effort than the 5 - 10% we require. In the absence
of expected output in tests, how will tests be labelled is a common
question. Answering this question will depend on what is currently
being done by the developer or organisation for classifying tests as
pass or fail. Our approach will entail applying the same practice to
labelling, albeit to a significantly smaller proportion of tests.

We also find that some programs are only accompanied by pass-
ing tests. To avoid an imbalanced training set with only passing
traces, we generate failing execution traces by running the orig-
inally passing tests through buggy implementations, generated
using code mutations. We ensure that only traces from buggy imple-
mentations whose outputs differ from expected outputs are marked
as failing. This avoids the problem of equivalent mutations. We
apply the following mutations representing some common bug
patterns [23, 31]:

(1) Logical connector replacement applied to {&&, ||, !}.
(2) Relational operator replacement applied to

{<, >, ==, <=, >=, ! =}.

(3) Argument swapping in function calls.
(4) Scalar variable replacement.
(5) Loop boundary value replacement.

Figure 2: Encoder 2 representing a sequence of trace lines
and global state as a single vector.

method, incurs a significant overhead and is difficult to scale to
large programs. To gather this information we use the middleware
of LLVM [27] and instrument the intermediate representation (IR)
of programs. Working at LLVM’s IR allows our implementation to
be language-agnostic. LLVM provides front-end support for multi-
ple programming languages, such as C/C++, CUDA, Haskell, Swift,
Rust among others, along with numerous libraries for optimisation
and code generation.

To perform the instrumentation, we traverse the SUT, visiting
each method. Every time a method invocation is identified, code is
injected to trace the caller-callee pair, the arguments and the return
values. At the end of the program, code is inserted to capture the
final values of all the global variables.

Each trace can be considered to have two parts: 1. a sequence
of method invocations and 2. the final global state. The first part
of the trace comprises multiple lines, each line being a tuple (np ,
nc , r , a ) that represents a single method invocation within the
sequence having:

• The names of the caller (parent) np and called nc functions.
• Return values r of the call, if any.
• Arguments passed a , if any.

The order of method invocations in the sequence is the order in
which they complete and return to the calling point. We support
all variable types including primitive types (such as int, float,
char, bool), composite data types (such as structs, classes, arrays)
defined by a user or library, and pointers for return and argument
values. Structs and classes are associated with a sequence of values
for their internal fields. We instrument these data structures in a

Learning to Encode and Classify Test Executions

Conference’17, July 2017, Washington, DC, USA

To avoid data leakage in our experiment in Section 4, we ensure
that expected output is removed from the traces. We also remove
exceptions, assertions and any other information in the program
or test code that may act as a test oracle. This is further discussed
in Section 4.1.

3.3 Preprocessing
The execution traces gathered with our approach include informa-
tion on methods declared in external libraries, called during the
linking phase. To keep the length of the traces tractable and rele-
vant, we preprocess the traces to only keep trace lines for methods
that are defined, i.e. have a function body, in the module (which may
be included from other files), and remove trace lines for declared
functions that are not defined, but simply linked to later.

For method invocations within loops, a new trace line is created
for each invocation of the same method within the loop. For loops
with large numbers of iterations, this can lead to redundancy when
the method is invoked with similar arguments and return values.
We address this potential redundancy issue by applying average
pooling to trace lines with identical caller-callee methods within
loops. No preprocessing is applied to global variables in the trace.

3.4 Neural Network Model
In this step, we perform the crucial task of designing a neural net-
work that learns to classify the pre-processed traces as passing or
failing. Shape and size of the input traces vary widely, and this
presents a challenge when designing a NN that accepts fixed length
vectors summarizing the traces. To address this, our network com-
prises three components that are trained jointly and end-to-end:
1. a ValEnc that encodes values (such as the values of arguments
and return values) into DV -dimensional distributed vector repre-
sentations, shown within Encoder 1 in Figure 1b, 2. a TrEnc that
encodes variable-sized traces into a single DT -dimensional vector,
shown as Encoder 2 in Figure 1a, and finally, 3. a TraceClassifier
that accepts the trace representation and the global variable values
for state and predicts whether the trace is passing or failing. The
Multi-layer Perceptron in Figure 1a represents the TraceClas-
sifier . We describe each component in detail in the rest of this
section.

Encoding Values Values within the trace provide useful indi-
cations about classifying a trace. However, values — such as ints,
structs, and floats — vary widely in shape and size. We, therefore,
design models that can summarize variable-sized sequences into
fixed-length representations. In the machine learning literature, we
predominantly find three kinds of models that can achieve this: re-
current neural networks (RNNs), 1D convolutional neural networks
(CNN) and transformers. In this work, we employ LSTMs [21] — a
commonly used flavour of RNNs. Testing other models is left as fu-
ture work. At a high-level RNNs are recurrent functions that accept
a vector ht of the current state and an input vector xt and compute
a new state vector ht +1 = RN N (xt , ht ) which “summarizes” the
sequence of inputs up to time t. A special initial state h0 is used at
t = 0.

To encode a value v, we decompose it into a sequence of primi-
tives v = [p0, p1, ...] (integers, floats, characters, etc.). Each primi-
tive pi is then represented as a binary vector bi = e(pi ) containing

its bit representation padded to the largest primitive data type of
the task. For example, if int64 is the largest primitive then all bi s
have dimensionality of 64. This allows us to represent all values
(integers, floats, strings, structs, pointers, etc.) as a unified sequence
of binary vectors. We encode v into a DV -dimensional vector by
computing

ValEnc(v) = LST Mv (e(pL)L, ValEnc([p0, p1, ..., pL−1])),
where LST Mv is the LSTM that sequentially encodes the bi s. Note
that we use the same ValEnc for encoding arguments, return values
and global variables, as seen in Figures 1b and 2. The intuition
behind this approach is that the bits of each primitive can contain
valuable information. For example, the bits corresponding to the
exponent range of a float can provide information about the order
of magnitude of the represented number, which in turn may be able
to discriminate between passing and failing traces.

Representing a Single Trace Line Armed with a neural net-
work component that encodes values, we can now represent a
single line (np , nc , r , a ) of the trace. To do this, we use ValEnc
to encode the arguments a and the return value r . We concatenate
these representations along with one-hot representations of the
caller and callee identities, as shown in Figure 1b. Specifically, the
vector encoding ti of the ith trace line is the concatenation
ti = (cid:2)ValEnc(a), ValEnc(r ), 1Hot(np ), 1Hot(nc )(cid:3) ,
where 1Hot is a function that takes as input the names of the
parent or called methods and returns a one-hot vector that uniquely
encodes that method name. For methods that are rare (appear fewer
than kmin times) in our data, 1Hot collapses them to a single special
UNK name. This is similar to other machine learning and natural
language processing models and reduces sparsity often improving
generalization. The resulting vector ti has size 2DV + 2k where k
is the size of each one-hot vector.

Encoding Traces Now that we have built a neural network
component that encodes single lines within a trace, we design
TrEnc that accepts a sequence of trace line representations t0...tN
and summarizes them into a single DT -dimensional vector. We use
an LSTM with a hidden size DT , and thus

TrEnc(t0...tN ) = LST Mt r (tN, TrEnc(t0...tN −1)) ,
where LST Mt r () is an LSTM network that summarizes the trace
line representations.

Encoding Global State We encode the final values of global
variables in each trace. Assuming global variables д0, ..., дM , we
first encode them using ValEnc and then summarize the global
state into a single vector

rG = Pool(ValEnc(д0), ...ValEnc(дM )),
where Pool is a permutation-invariant pooling function and rG is a
DV -sized vector. In this work, we use max pooling (i.e. element-wise
maximum). Note that the permutation invariance is a necessary
design requirement since the representation of the global state
should be invariant to the ordering of the global variables. Figure 2
shows TrEnc along with global state encoding.

Classifying Traces With the neural network components de-
scribed so far we have managed to encode traces into fixed length
vector representations. The final step is to use those computed rep-
resentations to make a classification decision. We treat failing traces

Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

as the positive class and passing traces as the negative class since
detecting failing runs is of more interest in testing. We compute
the probability that a trace is failing as

P(fail) = TraceClassifier([TrEnc(t0...tN ), rG ]),

where the input of TraceClassifier is the concatenation of the two
vectors. Our implementation of TraceClassifier is a multilayer
perceptron (MLP) with sigmoid non-linearities and a single output,
which can be viewed as the probability that the trace is a failing
trace. It follows that P(pass) = 1 − P(fail).

Training and Implementation Details We train our network
end-to-end in a supervised fashion, minimizing the binary cross
entropy loss. All network parameters (parameters of LST Mv and
LST Mt r and parameters of the MLP) are initialized with random
noise. For all the runs on our network we use DV = 64, DT = 128.
The TraceClassifier is an MLP with 3 hidden layers of size 128,
64 and 32. We use the Adam optimizer [25] with a learning rate of
8E-6.

For our subject programs, we find the aforementioned feature
values to be optimal for performance and training time, after having
experimented with other NN architectures, varying the DV , DT
sizes, and the hidden layers in the MLP. We explored increasing
DV to 128, 256, 512, DT to 256, 512, 1024 and size of hidden layers
to 256, 512 and 1024.

Our implementation of each of the steps in the proposed ap-

proach for building a test oracle is available at
https:// github.com/ anon-0/ ICSE-ClassifyTestExec.

4 EXPERIMENT
In our experiment, we evaluate the feasibility and accuracy of the
NN architecture proposed in Section 3 to classify test execution
traces for 15 subject programs and their associated test suites. We
investigate the following questions regarding feasibility and effec-
tiveness:
Q1. Precision, Recall and Specificity: What is the precision, re-
call and specificity achieved over the subject programs?
To answer this question, we use our tool to instrument
the source code to record execution traces as sequences
of method invocations, along with information on global
state, arguments and return values. A small fraction of the
execution traces are labelled (training set) and fed to our
framework to infer a classification model. We then evalu-
ate precision, recall and specificity achieved by the model
over unseen execution traces (test set) for that program. The
test set includes both passing and failing test executions. In
our experiments, we do not use a validation set to tune the
hyper-parameters in the NN model.

Q2. Size of training set: How does size of the training set affect

precision and recall of the classification model?
For each program, we vary the size of training set from 5%
to 30% of the overall execution traces and observe its effect
on precision and recall achieved.

Q3. Comparison against state of art: How does the precision, re-
call and specificity achieved by our technique compare against
agglomerative hierarchical clustering, proposed by Almaghairbe
et al. [8] in 2017?

We choose to compare against the the hierarchical clustering
work as it is the most relevant and recent in classifying
execution traces. Traces used in their work are sequences
of method invocations, similar to our approach. Other test
oracle work that use NNs is not used in the comparison
as they do not work over execution traces, and are limited
in their applicability to programs with numerical input and
output which is not the case for programs in our experiment.
Q4. Generalisation of classification model: Can a classification
model inferred from a program in a particular application do-
main be used to classify test executions over other programs in
the same domain?
For the network protocol domain, we evaluate the accuracy
of using a classification model inferred using traces from
a single protocol detection finite state machine (FSM) for
classifying test executions from other protocol FSMs.
All experiments are performed on a single machine with 4 Intel

i5-6500 CPU cores, 16GB of memory.

4.1 Subject Programs
The subject programs are from the networking, blockchain, deep
learning, encryption, text editing, and memory management do-
mains. A description of the programs and their tests is as follows.
1. Ethereum [4] is an open-source software platform based on
blockchain technology, which supports smart contracts. Within the
Ethereum project, we evaluate our approach over the Difficulty
module that calculates the mining difficulty of a block, in relation to
different versions (eras) of the cryptocurrency (Byzantium, Home-
stead, Constantinople etc.). The calculation is based on five fields
of an Ethereum block, specified in the test input.

Tests. We use the default test inputs provided by Ethereum’s
master test suite for the Difficulty module. We test this module
for the Byzantium era of the cryptocurrency (version 3.0). The
test suite contains 2254 test inputs. Each test input contains one
hex field representing the input fields of the difficulty formula and
one hex field which was the expected output of the program. All
the test inputs provided with the module are passing tests with
the actual output equal to the expected output. To address the
imbalance in the data set, we produce failing execution traces by
running the test inputs through buggy implementations, generated
by mutating the code, as discussed in Section 3.2. Finally, expected
outputs and assertions are removed from the traces, so that there
is no existing test oracle information. It is worth noting that for
all our subject programs, we systematically remove all forms of
test oracle information (expected output, assertions, exception, etc.)
prior to applying our approach.
2. Pytorch [29] is an optimized tensor library for deep learning
using GPUs and CPUs. Due to its flexibility and efficiency, it is com-
monly used as a research platform for deep learning applications.
In our experiment, we evaluate our model over the intrusive_ptr
class, which implements a pointer type with an embedded reference
count. We chose this class because it had a sizeable number of tests
(other modules had < 20 published tests).

Tests. Implementation of the class is accompanied by 320 tests,
all of which are passing. As with Ethereum, we apply mutations to

Learning to Encode and Classify Test Executions

Conference’17, July 2017, Washington, DC, USA

the intrusive_ptr implementation to generate 320 failing traces
with the test inputs.
3. Microsoft SEAL [32] is an open-source encryption library. In
our experiment, we study two different components within Mi-
crosoft SEAL - Biguint and the Encryptor that are both accompa-
nied by tests. Biguint is a project-specific structure for arbitrarily
large integers. The Encryptor component is responsible for per-
forming data encryption.

that may act directly or indirectly as a test oracle. For example,
Ethereum uses BOOST testing framework to deploy its unit tests.
We remove expected outputs and assertions in the test code that
compare actual with the expected output e.g. BOOST_CHECK_EQUAL
or BOOST_THROW_EXCEPTION). For all the execution traces used in
our evaluation, we ensure that it is not possible to trivially classify
it as pass or fail by simply observing the test output or execution
trace.

Tests. The Biguint component is accompanied by 184 tests. The
Encryptor component is accompanied by 48 tests. As the number
of tests was small, we additionally generated 144 tests, providing
random numbers within the specified range for the encryptor to
encode. For both components, the provided tests were all passing
tests, with matching expected and actual output. As with previous
programs, we generated failing traces using code mutations for a
better balanced data set.
4. Sed [1] is a Linux stream editor that performs text transforma-
tions on an input stream.

Tests. We use the fifth version of Sed available in the SIR reposi-
tory [17]. This version is accompanied by 370 tests, of which 352
are passing and 18 are failing. The failing tests point to real faults
uncovered in this version. We did not generate failing traces using
code mutations for this program.
5. Value Pointer (value_ptr) [2] is an open-source library, imple-
menting a smart pointer with polymorphic value and copy seman-
tics. The functionality of value_ptr is largely similar to standard
smart pointers (e.g. shared_ptr), but with the additional ability to
point and efficiently copy polymorphic objects.

Tests. The library came with 74 test inputs. We additionally gen-
erated 49 test inputs to increase the size of our data set. All the tests
in the library were passing tests, requiring us to generate failing
traces using code mutations.
6. L7-Filter [3] is a packet identifier for Linux. It uses regular
expression matching on the application layer data to determine
what protocols are used. It works with unpredictable, non-standard
and shared ports. We study the following 9 protocols, implemented
as FSMs, separately in our evaluation –

(1) Ares - P2P filesharing
(2) BGP - Border Gateway Protocol
(3) Biff - new mail notification
(4) Finger - User information server
(5) FTP - File Transfer Protocol
(6) Rlogin - remote login
(7) TeamSpeak - VoIP application
(8) Telnet - Insecure remote login
(9) Whois - query/response system (eg. for domain name)

Tests. For each of the network protocol FSMs, we use test suites
generated by Yaneva et al.[35] that provide all-transition pair cover-
age. The test suites for the FSMs, unlike many of the other subject
programs, have both passing and failing tests. As a result, we did
not apply mutation operators to generate failing traces.

Checks to avoid data leakage. We ensure there is no test ora-
cle data that is leaked into the traces. We remove expected out-
puts, assertions, exceptions, test names and any other information

4.2 Performance Measurement
For each subject program, we evaluate performance of the clas-
sification model over unseen execution traces. As mentioned in
Section 3.4, we use positive labels for failing traces and negative
labels for passing. We measure

(1) Precision as the ratio of number of traces correctly classified
as “fail” (TP) to the total number of traces labelled as “fail”
by the model (TP + FP).

(2) Recall as the ratio of failing traces that were correctly identi-

fied (TP/(TP + FN)).

(3) Specificity or true negative rate (TNR) as the ratio of passing
traces that were correctly identified (TN /(TN + FP)).

TP, FP, TN, FN represent true positive, false positive, true nega-
tive and false negative, respectively.

4.3 Hierarchical Clustering
In research question 3 in our experiment, we compare the classifi-
cation accuracy of our approach against agglomerative hierarchical
clustering proposed by Almaghairbe et al. [8]. Their technique also
considers execution traces as sequences of method calls, but only
encoding callee names, while caller names, return values, argu-
ments and global state information are discarded. We attempted to
add the discarded information, but found the technique was unable
to scale to large number of traces due to both memory limitations
3) where n is the number of traces.
and a time complexity of O(n
For setting clustering parameters for each subject program, we
evaluate different types of linkage (single, average, complete)
and a range of different cluster counts (as a percentage of the total
number of tests): 1, 5, 10, 20 and 25%. We use Euclidean distance as
the distance measure for clustering. For each program, we report
the best clustering results achieved over all parameter settings.

5 RESULTS AND ANALYSIS
In this section, we present and discuss our results in the context of
the research questions presented in Section 4.

5.1 Q1. Precision, Recall and Specificity
Table 1 shows the precision, recall and specificity achieved by the
classification models in our approach for the different subject pro-
grams. Results with the hierarchical clustering approach by Al-
maghairbe et al. [8] are also presented in Table 1 for comparison,
but this is discussed in Q3 in Section 5.3. The column showing % of
traces used in training varies across programs, we show the lowest
percentage that is needed to achieve near maximum precision and
recall.

Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

Figure 3: Precision and recall achieved by classification model over each subject program.

Learning to Encode and Classify Test Executions

Conference’17, July 2017, Washington, DC, USA

Subject Program

Ethereum
Pytorch
SEAL Biguint
SEAL Encryptor
Sed
Value pointer
Ares protocol
BGP protocol
Biff protocol
Finger protocol
FTP protocol
Rlogin protocol
Teamspeak protocol
Telnet protocol
Whois protocol

Lines of
Code
55927
21090
18464
25967
4492
15001
1261
1025
627
791
995
955
3284
1019
784

% Traces
for training
8
5
11
10
10
10
3
5
15
10
10
10
10
10
9

Total
# Traces
13524
640
364
384
370
246
16066
16009
1958
2775
9677
4121
1945
319
4412

Our Approach
Recall
0.99
0.97
0.98
0.98
0.94
0.99
0.98
0.99
0.99
0.99
0.99
0.96
0.99
0.96
0.99

Precision
0.99
1.0
0.95
0.95
0.94
0.98
0.97
0.99
0.97
0.99
0.99
0.97
0.95
0.98
0.98

TNR
0.99
1.0
0.96
0.95
0.99
0.99
0.97
0.99
0.99
0.99
0.98
0.99
0.96
0.95
0.99

Hierarchical Clustering [8]
TNR
Precision
0.31
0.56
0.16
0.48
0.82
0.75
0.0
0.5
0.86
0.35
0.94
0.67
0.0
0.94
0.98
0.18
0.72
0.43
0.92
0.53
0.98
0.07
1.0
1.0
1.0
1.0
0.87
0.29
0.98
0.49

Recall
0.87
1.0
0.55
1.0
0.63
0.13
0.24
0.01
0.22
0.13
0.001
0.04
0.11
0.02
0.03

Table 1: Precision, Recall and True Negative rate (TNR) using our approach and hierarchical clustering.

Precision and Recall. The classification models for all the pro-
grams perform exceptionally well, achieving at least 94% precision
and recall (average precision = 97% and average recall = 98%). This
implies that the number of false positives in the classification is very
low and a large majority of the failing traces are correctly identified.
The fraction of traces needed in training to achieve near maximum
performance was 8% to 15% for 12 of the 15 subject programs. The
other three programs, Ares, BGP protocols and Pytorch, needed
smaller fractions (<= 5%) of traces in training. This was because
these 3 programs had a significantly larger number of total traces.
Overall, for all our subject programs, we observe that we only need
a relatively small fraction of the total traces to train classification
models with high precision and recall. Accuracy improves as the
training set gets larger. We discuss this effect in Section 5.2.

Specificity. We report specificity to understand number of traces
correctly identified as passing out of the total passing traces. The
classification models for all subject programs achieve high speci-
ficity (average specificity = 98%). This implies that the NN models
are able to learn runtime patterns that distinguish not only failing
executions, but also passing executions with a high degree of accu-
racy. These results are unprecedented as we are not aware of any
technique in the literature that can classify both passing and failing
executions at this level of accuracy.

Ablation study. To better understand which parts of the traces
contribute most to model performance, we systematically remove
information (one parameter a time) from the traces, also referred
to as ablation, training new classification models with the modified
traces and observing their effect on precision, recall and specificity
(TNR). In our experiments, we remove function call names, argu-
ments, and return values from the original traces. The performance
of different models for 8 of the 15 subject programs is shown in
Table 2. The 7 network protocols missing in Table 2 have results
similar to Finger and Telnet protocols in the table, and were omit-
ted due to space limitations. Nevertheless, the ablation study results
for them can be found in our repository1.

We observe that each ablation affects subject programs differ-
ently. For instance, we find removing function names reduces model
performance for programs like Sed, Ethereum, Value pointer,
Encryptor that have different function call sequences between
passing and failing traces. For FSM protocols, where the sequence
of method call names between passing and failing traces is largely
the same, removing function names has little impact on perfor-
mance. For FSM protocols, Finger and Telnet, return values and
arguments have a dominant effect on model performance. We per-
formed ablation of global state only for the FSM protocols since
the other programs in our experiment do not use global variables.
Removing global state reduces model performance for BGP, Biff
and Telnet protocols. For Pytorch, removing arguments in the
trace has the biggest effect. Overall, we find all parts of the trace
– function names, return values, arguments — is useful to our NN
model to achieve high prediction performance across all our subject
programs.

Bug detection. Failing test executions for Ethereum, Pytorch,
Encryptor, Biguint, and Value pointer were generated us-
ing mutations representing different bug classes (discussed in Sec-
tion 3.2)2. Traces with instances of the bug classes were seen during
training, albeit applied at a different location to a different operator
than in the test set. We find the classification models for all 3 subject
programs were highly accurate, detecting nearly all unseen failing
traces from the different bug classes. Given the promising results
for the different bug classes, we wanted to check if the model was
capable of detecting a previously unseen bug class in the test set. We
used the Encryptor and Ethereum programs for this assessment.
We find the classification model for Encryptor trained using failure
instances from the “Scalar variable replacement” bug class, could
detect failures from the following unseen bug classes with high
accuracy - 1. Bool operator (0.79 precision), 2. Relation operator
(0.96), 3. Argument swapping (0.97), 4. Loop boundary (0.98). On the
other hand, the Ethereum model trained using the “Scalar variable
replacement” bug class has mixed performance over other unseen
bug classes - 1. Bool operator (0.98 precision), 2. Relation operator
(0.31), 3. Argument swapping (0.16), 4. Loop boundary (0.36). Upon

1https://github.com/anon-0/ICSE-ClassifyTestExec/blob/master/fsm_ablation_study.
pdf

2For Sed and the 9 FSM protocols, we used failing tests provided with the implementa-
tion. No mutations were used.

Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

Program

Ethereum

Pytorch

Seal
Biguint

Seal
Encryptor

Sed

Value
pointer

Finger
Protocol

Telnet
Protocol

Omitted Info.
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Function names
Return values
Arguments
Half the #trace lines
Global state
Function names
Return values
Arguments
Half the #trace lines
Global state

P
0.49
0.50
0.52
0.70
0.99
0.99
0.51
0.99
0.95
0.97
0.90
0.81
0.92
0.79
0.70
0.72
0.11
0.20
0.67
1.0
0.62
0.47
0.52
0.47
0.99
0.98
0.52
0.49
0.97
0.93
0.82
0.76
0.75
0.88

R
0.98
0.94
0.91
0.86
1.0
0.99
0.99
0.91
0.58
0.94
0.88
0.90
0.14
0.98
0.98
1.0
1.0
0.24
0.08
0.08
0.86
1.0
0.96
1.0
0.95
0.97
0.19
0.60
0.94
1.0
1.0
1.0
0.93
1.0

TNR
0.02
0.07
0.36
0.63
1.0
0.99
0.04
0.99
0.97
0.97
0.91
0.79
0.98
0.74
0.59
0.58
0.02
0.89
0.99
1.0
0.37
0.00
0.04
0.00
0.99
0.99
0.88
0.59
0.97
0.76
0.25
0.00
0.00
0.49

subject programs. For certain programs, like the Seal Encryptor,
the clustering approach achieves a slightly better recall (1.0 vs
0.98) than our approach but the precision (of 0.5) and TNR (of 0)
achieved with clustering are disappointing, making it unusable
as it incorrectly classifies many failing and passing traces. The
clustering approach for network protocols, BGP, FTP, Whois,
have comparable TNRs to our approach. However, the precision
and recall rates for these protocols are dramatically lower with the
clustering approach (< 0.19) than our approach (> 0.96) implying
failing traces are not correctly identified.

Overall, for all subject programs, we find the clustering approach
is unable to accurately distinguish failing and passing executions.
This is because the hierarchical clustering assumption does not hold
for the programs in our evaluation. According to this assumption,
passing traces tend to be grouped in a few big clusters and failing
traces are grouped as many small clusters. However, we find failing
traces are grouped in large clusters for some programs as they have
similar function call sequences as passing traces. This leads them
to be incorrectly classified as passing in some cases. In addition,
passing traces may also be grouped into many small clusters when
they have significant differences in method invocation sequences,
especially for programs with heavy control flow, causing them to
be incorrectly classified as failing in some instances.

Table 2: Precision (P), Recall (R) and Specificity (TNR) for
each subject program omitting certain trace information.

analysis, we find high precision is achieved when the bug classes
have similar method invocation patterns allowing the classification
learned by the model to be more generally applicable to unseen bug
classes. Poor precision over some unseen bug classes in Ethereum
is because the method invocation patterns are different between
them. For programs like Ethereum, it is important for the training
set to contain representations from a wide set of bug classes.

5.2 Q2. Size of training set
Figure 3 shows precision and recall achieved by our approach with
different training set sizes. Initially, increasing the size of training
set results in better precision and recall. The extent of improvement
depends on the program and execution traces. For instance, when
training set size is increased from 5% to 10% of the overall traces,
precision for Encryptor improves from 67% to 95%, while precision
for Biguint increased steeply from 37% to 95%. Nevertheless, we
find this observation to be true with diminishing results: after a
certain point, increasing the training set size does not result in
a noticeable improvement in precision and recall. We find this
threshold, typically, to be between 10 to 15%, across our subject
programs.

5.3 Q3. Comparison against state of art
Table 1 presents precision, recall, and specificity (TNR) achieved by
the agglomerative hierarchical clustering proposed by Almaghairbe
et al. [8] on each of the subject programs. Comparing the precision,
recall and TNR of our approach versus hierarchical clustering, we
find our approach clearly outperforms the clustering approach on all

5.4 Q4. Generalisation
In this research question, we conduct an initial exploration into the
ambitious possibility of using a model, trained using traces from
one subject program, to classify traces from other programs in the
same application domain. We use FSMs from the network protocol
domain to evaluate this possibility. Figures 4 and 5 represent preci-
sion and recall achieved by models trained using traces from Biff
protocol and Whois protocol, respectively, to classify traces pro-
duced by other FSMs. We find that the model trained using traces
from Biff achieves high accuracy over the Ares protocol with pre-
cision and recall close to 1.0, and reasonable precision (> 0.8) for
BGP, FTP, Rlogin, Teamspeak, Whois protocols. Lowest preci-
sion (0.17) was observed with Telnet. Average precision achieved
in classifying traces from unseen FSMs was 0.79. Recall achieved
by the model is lower than precision indicating that the model
missed identifying failing traces in each of these protocols. Overall,
the model trained with Biff traces was successful in identifying
failing traces in other FSMs that have similar patterns to Biff. Fail-
ing traces with differing patterns were missed. We confirmed this
observation by checking results from the Whois model. Although
precision and recall numbers are different from the Biff model, the
reasoning for classification success was the same - extent of similar-
ity in execution patterns between FSMs. With the current approach,
we find there is scope to generalise a classification model from a
single FSM to multiple FSMs in the networking domain. However,
achieving high accuracies with generalisation is a difficult problem
and we plan to take small, definitive steps towards addressing this
challenge in the future. As a next step, we will explore tuning the
classification model from an individual FSM with sample traces
from other FSMs to improve generalisation performance.

Learning to Encode and Classify Test Executions

Conference’17, July 2017, Washington, DC, USA

as sequences of method invocations and final global state, (2) Gen-
erating failing traces, if necessary, to provide a balanced training
set, (3) Encoding variable length execution traces into a fixed length
vector, (4) Designing a NN model that uses the trace information
to classify the trace as pass or fail.

We evaluated the approach using 15 realistic subject programs
and tests. We found the classification model for each of the subject
programs was highly effective in classifying passing and failing
executions, achieving over 95% precision, recall and specificity
while only training with an average 9% of the total traces. For
programs with a large number of traces, a lower proportion of
traces was adequate for training. We outperform the hierarchical
clustering technique proposed in recent literature by a large margin
of accuracy across all our subject programs.

Practical use: Our approach can be applied out of the box for
classifying tests for any software that can be compiled to LLVM IR.
We require 5 - 10% of the tests to be labelled with their pass or fail
outcomes. The remaining tests will be classified automatically with
high accuracy. Our approach is clearly better than current industry
practices where developers or testers label all the tests, either by
providing expected outputs or through inspection.

Generalisation: In this paper, we focus on designing a classifica-
tion model for each subject program. We did an initial experiment
with generalising a classification model learned over one protocol
FSM to classify executions over other network protocol FSMs. The
results for precision and recall over other unseen FSMs was not
as high as the individual FSM classification models. In the future,
we plan to explore techniques that will improve the generalisation
performance of the NN models.

REFERENCES
[1] Sed, linux stream editor. https://linux.die.net/man/1/sed, 2009.
[2] value_pointer, smart pointer library implementation. https://github.com/Baltoli/

value_ptr, 2009.

[3] L7-filter, application layer packet classifier for linux. http://l7-filter.clearos.com/,

2013.

[4] Ethereum Project (release 3.5), 2019. https://github.com/ethereum/aleth.
[5] Aggarwal et al. A neural net based approach to test oracle. ACM SIGSOFT

Software Engineering Notes, 29(3):1–6, 2004.

[6] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to

represent programs with graphs. In ICLR, 2018.

[7] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A Convolutional Attention

Network for Extreme Summarization of Source Code. In ICML, 2016.

[8] Rafig Almaghairbe and Marc Roper. Separating passing and failing test executions

by clustering anomalies. Software Quality Journal, 25(3):803–840, 2017.

[9] Uri Alon et al. code2vec: Learning distributed representations of code. arXiv

preprint arXiv:1803.09473, 2018.

[10] Uri Alon, Omer Levy, and Eran Yahav. code2seq: Generating sequences from
structured representations of code. arXiv preprint arXiv:1808.01400, 2018.
[11] Paul Ammann and Jeff Offutt. Introduction to software testing. Cambridge Univ.

Press, 2016.

[12] James H Andrews, Lionel C Briand, Yvan Labiche, and Akbar Siami Namin. Using
mutation analysis for assessing and comparing testing coverage criteria. IEEE
Transactions on Software Engineering, 32(8):608–624, 2006.

[13] Earl Barr et al. The oracle problem in software testing: A survey.

IEEE TSE,

41(5):507–525, 2015.

[14] James Bowring et al. Active learning for automatic classification of software
behavior. In ACM SIGSOFT Software Engineering Notes, pages 195–205, 2004.
[15] Lionel C Briand. Novel applications of machine learning in software testing. In

QSIC’08, pages 3–10. IEEE, 2008.

[16] Yuriy Brun and Michael D Ernst. Finding latent code errors via machine learning
over program executions. In Proceedings of the 26th ICSE, pages 480–490, 2004.
[17] Hyunsook Do, Sebastian Elbaum, and Gregg Rothermel. Supporting controlled
experimentation with testing techniques: An infrastructure and its potential
impact. Empirical Software Engineering, 10(4):405–435, 2005.

Figure 4: Biff trained model - Precision and recall for un-
seen fsms.

Figure 5: Whois trained model - Precision and recall for un-
seen fsms.

5.5 Threats to Validity
We see three threats to validity of our experiment based on the
selection of subject programs and associated tests.

First, implementations for 5 out of the 15 subject programs in
our experiment only had passing tests. To avoid an imbalanced
training set, we generated failing execution traces using seeded
faults representing common bug patterns [23, 31]. It is possible
using real faults would lead to different results. However, Andrews
et al. have shown the use of seeded faults leads to conclusions simi-
lar to those obtained using real faults [12]. It is also worth noting
that for the remaining 10 subject programs we did not artificially
seed faults, but instead used the failing tests that came with the
implementation which helps mitigate this threat.

Second, for three programs, Seal Biguint, Encryptor, and
Value Pointer, we augmented existing tests with our own tests to
increase the number of tests available for training and evaluation.
To reduce this threat, we used random test generation to avoid bias
in the inputs generated. We also use 12 subject programs in our
experiment for which we did not generate test inputs but instead
used test inputs that came with their implementation. Our approach
achieved high classification performance over all programs.

Finally, we conducted our study on 15 subject programs from
6 different application domains which is not representative of all
application domains. Given the superior performance using subject
programs in 6 application domains that are sufficiently different,
and the fact that our approach has no domain specific constraints,
we believe our approach will be widely applicable.

6 CONCLUSION
In this paper, we propose a novel approach for designing a test
oracle as a NN model learning from execution traces of a given
program. We have implemented an end to end framework for au-
tomating the steps in the approach, (1) Gathering execution traces

Conference’17, July 2017, Washington, DC, USA

Foivos Tsimpourlas, Ajitha Rajan, and Miltiadis Allamanis

[18] Robert M Hierons. Verdict functions in testing with a fault domain or test

hypotheses. ACM TOSEM, 18(4):14, 2009.

[19] Robert M Hierons. Oracles for distributed testing. IEEE TSE, 38(3):629–641, 2012.
[20] Geoffrey Hinton et al. A fast learning algorithm for deep belief nets. Neural

[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural

computation, 18(7):1527–1554, 2006.

computation, 9(8):1735–1780, 1997.

[22] Jez Humble and David Farley. Continuous Delivery: Reliable Software Releases
through Build, Test, and Deployment Automation. Pearson Education, 2010.
[23] Yue Jia and Mark Harman. An analysis and survey of the development of mutation

testing. IEEE transactions on software engineering, 37(5):649–678, 2011.

[24] Hu Jin et al. Artificial neural network for automatic test oracles generation. In

Proceedings of CSSE, volume 2, pages 727–730. IEEE, 2008.

[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

In 3rd International Conference for Learning Representations, 2015.

[26] William Langdon et al. Inferring automatic test oracles. In Proceedings of the

10th SBST, pages 5–6, 2017.

[27] Chris Lattner. LLVM: An Infrastructure for Multi-Stage Optimization. Master’s
thesis, Computer Science Dept., University of Illinois at Urbana-Champaign,

Urbana, IL, Dec 2002. See http://llvm.cs.uiuc.edu.

[28] Paulo Augusto Nardi and Eduardo Damasceno. A survey on test oracles. Advances

in Theoretical and Applied Informatics, 1(2):50–59, 2015.

[29] Adam Paszke and Soumith Chintala. Pytorch, 2017.
[30] Andy Podgurski et al. Automated support for classifying software failure reports.

In Proceedings of 25th ICSE 2003., pages 465–475. IEEE, 2003.

[31] Michael Pradel and Koushik Sen. Deepbugs: a learning approach to name-based
bug detection. Proceedings of the ACM on Programming Languages, 2(OOP-
SLA):147, 2018.

[32] Microsoft SEAL (release 3.2). https://github.com/Microsoft/SEAL, 2019. Microsoft

Research, Redmond, WA.

[33] Meenakshi Vanmali et al. Using a neural network in the software testing process.

International Journal of Intelligent Systems, 17(1):45–62, 2002.

[34] Ke Wang, Rishabh Singh, and Zhendong Su. Dynamic neural program embedding

for program repair. arXiv preprint arXiv:1711.07163, 2017.

[35] Vanya Yaneva, Arnav Kapoor, Ajitha Rajan, and Christophe Dubach. Accelerated

finite state machine test execution using gpus. In APSEC, 2018.

