Attention-based model for predicting question
relatedness on Stack Overﬂow

Jiayan Pei∗, Yimin Wu∗†, Zishan Qin∗, Yao Cong∗, Jingtao Guan†
∗South China University of Technology, Guangzhou, China
†Research Institute of SCUT in Yangjiang, Yangjiang, China
seasensio@mail.scut.edu.cn, csymwu@scut.edu.cn, csqzs@mail.scut.edu.cn,
congyao95@hotmail.com, jingtao0337@163.com

1
2
0
2

r
p
A
5

]
L
C
.
s
c
[

6
v
3
6
7
0
1
.
3
0
1
2
:
v
i
X
r
a

Abstract—Stack Overﬂow is one of the most popular Program-
ming Community-based Question Answering (PCQA) websites
that has attracted more and more users in recent years. When
users raise or inquire questions in Stack Overﬂow, providing re-
lated questions can help them solve problems. Although there are
many approaches based on deep learning that can automatically
predict the relatedness between questions, those approaches are
limited since interaction information between two questions may
be lost. In this paper, we adopt the deep learning technique, pro-
pose an Attention-based Sentence pair Interaction Model (ASIM)
to predict the relatedness between questions on Stack Overﬂow
automatically. We adopt the attention mechanism to capture the
semantic interaction information between the questions. Besides,
we have pre-trained and released word embeddings speciﬁc to
the software engineering domain for this task, which may also
help other related tasks. The experiment results demonstrate
that ASIM has made signiﬁcant improvement over the baseline
approaches in Precision, Recall, and Micro-F1 evaluation metrics,
achieving state-of-the-art performance in this task. Our model
also performs well in the duplicate question detection task of
AskUbuntu, which is a similar but different task, proving its
generalization and robustness.

Index Terms—Stack Overﬂow, Question Relatedness, Deep

Learning, Attention Mechanism, Word Embeddings

I. INTRODUCTION

The continuous progress of the information industry has
caused more and more people to engage in software devel-
opment. Therefore, Programming Community-based Question
Answering (PCQA) websites have attracted a large number
of users. Stack Overﬂow is one of the most popular PCQA
websites, where users can ask and answer questions related to
program problems and gain knowledge [1]. By October 2020,
Stack Overﬂow had more than 20 million questions, and there
could be semantic relatedness between them. For example, two
or more questions may be duplicated, or information in one
question may help solve other questions. Figure 1 presents
an example of a duplicate question pair. The same solution
can answer these two questions, so the recent question (’Q2’)
was closed and marked as ’[duplicate]’ and linked to the
’Q1’. Duplicate questions are not conducive to the websites’
maintenance. Users who ask duplicate questions will wait
a long time for the question to be answered while ready
answers are already available [2]. Figure 2 shows two related
questions, and the answer to the question above (’Q1’) can
directly solve the question below (’Q2’). Therefore, providing

Fig. 1. An example of a duplicate question pair.

questions related to the question raised or being inquired by
users can effectively help them solve problems. However,
semantic relatedness between questions often requires manual
analysis by users, such as identifying and labeling duplicate
questions by programmers with high reputation or indicating
questions that help answer the current question in the way
of URL sharing. Due to a large number of questions on
the websites, and the same question can be expressed in a
variety of ways, the artiﬁcial recognition method is inefﬁcient
and time-consuming, resulting in a large number of questions
with semantic relatedness are not recognized. Thus,
is
necessary to propose an automated approach that can help
predict semantic relatedness between questions to solve this
problem.

it

Although Stack Overﬂow recommends related questions
when users view a question, its recommendations are based on
word overlap between questions that are not reliable due to the

 
 
 
 
 
 
on the attention mechanism. The model aims to identify the
semantic relatedness between questions on Stack Overﬂow.

This paper conducts research based on the work of Shirani
et al. [12]. Their paper refers to a question and the entire set
of its answers as a knowledge unit and constructs a dataset
of 347,372 pairs of knowledge units (hereafter, Knowledge
Unit dataset). In the Knowledge Unit dataset, the relationships
between knowledge units are divided into four classes based on
the degree of relatedness from high to low: duplicate, direct,
indirect, and isolated. They also construct two models DOT-
BILSTM and SOFTSVM, based on deep learning and SVM
classiﬁer, respectively, among which the DOTBILSTM is the
state-of-the-art model in this dataset. We regard the task as a
multi-class classiﬁcation task and proposes an Attention-based
Sentence pair Interaction Model (ASIM) to solve this problem.
Moreover, the most common pre-trained word embeddings are
trained on a large amount of data unrelated to the software
engineering domain, which may lead to ambiguous results
[18]. Therefore, we construct a corpus of software engineering
based on stack Overﬂow data dump and pre-train the word
embeddings of domain-speciﬁc based on this corpus. Our word
embeddings are openly available, which may also help design
models in other software engineering tasks. In general, we
answer the following ﬁve research questions in this paper:
(1) How much improvement can ASIM achieve over the two
baseline models SOFTSVM and DOTBILSTM, to predict
question relatedness?

(2) Compared with the SOFTSVM and the DOTBILSTM,
how effective is ASIM in predicting knowledge unit pairs
of different relatedness classes?

(3) How much inﬂuence does the attention mechanism con-

tribute to the improvement of ASIM?

(4) What is the impact of domain-speciﬁc word embeddings

on the performance improvement of ASIM?
(5) How is the generalization ability of ASIM?

The rest of the paper is organized as follows. Section 2
brieﬂy describes the related work of our study. Section 3
details our approaches to predict question relatedness. Sec-
tion 4 describes the experimental settings and presents the
experiment results. We present a case study of the attention
mechanism and discuss the threats to our study’s validity in
Section 5, and Section 6 concludes the paper and discusses
future work.

II. RELATED WORK

Semantic relatedness tasks on PCQA websites are focused
areas in software engineering, including predicting semantic
relatedness between questions and detecting duplicate ques-
tions.

A. Predicting semantic relatedness between questions on
PCQA websites

Predicting the semantic relatedness between questions on
PCQA websites is conducive to improving programmers’ efﬁ-
ciency in ﬁnding questions and helping them solve problems
using information from related questions. Xu et al. [10]

Fig. 2. An example of a related question pair.

lexical gaps [10]. Some previous methods [2]–[4] can automat-
ically calculate the semantic relatedness between questions,
but they do not fully use the semantic information in questions
[5]. Moreover, as there are many different expressions for
the same question, and there may be few words common
between them, such methods’ performance is poor. In recent
years, deep learning has been well applied in many software
engineering domains, such as user intention classiﬁcation [6],
software defect prediction [7], code summarization [8], and
type inference [9]. With the strong non-linear ﬁtting ability,
deep learning can also effectively extract semantic information
in the question. Some previous works have adopted the deep
learning technique to predict the linkable of questions [10]–
[12] or detect duplicate questions [5], [13]–[16] in the PCQA
websites. Compared with traditional methods based on features
and heuristics, their methods can achieve better results. How-
ever, most of them only adopt the sentence encoding model
[17] to learn the vector representation of individual questions
separately and only calculates the semantic relatedness be-
tween questions based on vector distance. Such methods can
not sufﬁciently consider the interaction information between
two questions, which is essential for semantic relatedness
prediction. Recently, the attention mechanism has been widely
applied in the NLP ﬁeld. Using the attention mechanism
to predict question relatedness,
the
interaction information between questions more effectively and
achieve better prediction performance through inter-sentence
alignment. Therefore, in this paper, we adopt the deep learning
technique to build a sentence pair interaction model based

the model can extract

regarded a question on Stack Overﬂow and the entire set of
its answers as a knowledge unit. They divide the relationship
between knowledge units into four classes according to the
degree of relatedness: duplicate, direct, indirect, and isolated.
They adopt a deep learning approach, which encodes the vec-
tor representation of two knowledge units respectively through
a convolutional neural network (CNN) with shared parameters.
They then calculate the cosine similarity of the two feature
vectors extracted from the knowledge unit pairs to obtain the
semantic similarity. The results are predicted to one of four
classes mentioned above according to the semantic similarity.
Compared with the SVM-based model, this neural network
model can achieve better results. To the best of our knowledge,
Xu et al. are the ﬁrst to apply the deep learning technique
to predict semantic relatedness between questions on PCQA
websites. Based on the work of Xu et al., Shirani et al. [12]
built a dataset containing more than 300,000 pairs of knowl-
edge units. They constructed two baseline models based on
the deep learning technique and machine learning technique,
respectively. The deep learning model DOTBILSTM encodes
a pair of knowledge units based on bidirectional LSTM,
calculates the inner dot product of three parts (title, body, and
answer) between a pair of knowledge units, maps it to a low-
dimensional vector space. The vector is then inputted into a
fully-connected layer using dense connection and a softmax
output layer for classiﬁcation. SOFTSVM is a Support Vector
Machine (SVM) model based on the soft-cosine similarity fea-
tures of knowledge unit pairs. The experimental results show
that DOTBILSTM can achieve better performance on this
dataset. Liu et al. [11] also studied the semantic relatedness
between questions. They construct the LinkSO dataset based
on the links between the questions from the Stack Overﬂow
data dump. This dataset contains three different programming
language questions and a total of 26,593 linked question pairs.
The manual sampling analysis of the LinkSO dataset shows
that most of the linked questions are relevant, so the dataset
may help with studying the semantic relatedness between
questions on Stack Overﬂow. They compare the performance
of six existing models on this dataset. Three approaches are
non-learning approaches (TF-IDF, BM25, TransLM), and the
other three are learning-based approaches (DSSM, DRMM,
aNMM). Experiment results show that
the learning-based
approaches can achieve better performance, among which the
aNMM is the best-performing approach on this dataset.

In the previous work, most deep learning approaches were
based on the sentence encoding model [17], which learns the
vector representation of a single question and only calculates
the semantic relevance between questions based on the vector
distance in the high layer. These methods can not sufﬁciently
consider the interaction information between two questions,
which is essential
to predicting the semantic relatedness.
Therefore, based on the deep learning method, ASIM adopts
the attention mechanism to learn the semantic interaction
between two questions, hoping to achieve better performance
in predicting question relatedness on PCQA websites.

B. Duplicate question detection in PCQA websites

There are numerous unrecognized duplicate questions in the
PCQA websites, which are not conducive to websites’ main-
tenance. Moreover, users who ask duplicate questions wait a
long time for the question to be answered, while ready answers
are already available [2]. Although some PCQA websites
such as Stack Overﬂow recommend users search for related
questions before posing new ones [3], as the same questions
can be expressed in many different ways, the website will
inevitably face duplicate questions. Taking Stack Overﬂow as
an example, users of high reputation mark duplicate questions
through manual analysis [3], which is ineffective and time-
consuming. Therefore, automated detection approaches are re-
quired. Zhang et al. [2] proposed a tool DUPPREDICTOR for
automatically detecting duplicate questions on Stack Overﬂow.
The tool uses Latent Dirichlet Allocation (LDA) to transform
the natural language in a question into a topic distribution.
Then it calculates the semantic similarity of the question pair
by considering the title, description, topic, and tag similarity.
This tool
is the ﬁrst work that addresses the problem of
duplicate questions on Stack Overﬂow. Ahasanuzzaman et
al. [3] proposed a duplicate question detection model Dupe
for Stack Overﬂow. This model uses logistic regression to
detect the duplicate question based on the ﬁve features of the
question pair, such as cosine similarity value, term overlap,
entity overlap, entity type overlap, and WordNet similarity. In
the prediction stage, the BM25 algorithm and other methods
are applied to ﬁlter irrelevant questions. Although the above
methods can automatically detect duplicate questions, they re-
quire the manual design of features, which is time-consuming,
and do not fully use the semantic information in question [5].
Moreover, these features are usually related to speciﬁc tasks,
so such methods’ generalization performance is lacking.

Wang et al. [5] ﬁrst applied the deep learning technique
to detect duplicate questions on Stack Overﬂow. They ex-
plore three different deep learning approaches (e.g., DQ-CNN,
DQ-RNN, and DQ-LSTM) based on Convolutional Neural
Network (CNN), Recurrent Neural Networks (RNN), and
Long Short-Term Memory (LSTM), respectively. Compared
with the baseline detection approaches and machine learning
approaches, their models can achieve better performance, in-
dicating the superiority of deep learning in this ﬁeld. Besides,
the DQ-LSTM model based on LSTM performs better than
other deep learning models. AskUbuntu is another popular
CQA website focused on software programming. For the
duplicate question detection on the AskUbuntu, Bogdanova et
al. [15] applied the convolution neural network with shared
parameters to encode the distributed vector representation
of individual questions and then calculate the similarity of
vectors through cosine similarity to predict whether a pair of
questions are duplicate. Based on Bogdanova et al.’s work,
Rodrigues et al. [16] released the AskUbuntu dataset’s clean
version. They removed the explicit clues from the question to
avoid a biased result [19]. In their paper, the DCNN model,
which combines the CNN model [15] and the MayoNLP

model [20], can achieve state-of-the-art performance on this
dataset. Compared with the traditional automated detection
methods, the above deep learning approaches do not require
the manual design of features and have better performance
and generalization ability. However, such methods also fail
to consider the interaction information between two questions
well. We also expand our approach to the duplicate question
detection task in software engineering, using the clean version
of the AskUbuntu dataset prepared by Rodrigues et al. [16]
to evaluate ASIM’s applicability and robustness in a similar
task.

III. THE APPROACH

According to Shirani et al. [12], a question on Stack Over-
ﬂow and the entire set of its answers is a knowledge unit (KU).
Based on the degree of relatedness between two knowledge
units from high to low, the relatedness types between them
can be deﬁned as the following four classes:

• duplicate: The questions in the two knowledge units are

duplicate questions.

• direct: Information in one knowledge unit can directly

solve the question in another knowledge unit.

• indirect: The information in one knowledge unit is helpful
to the solution of the question in another knowledge
unit, but the information alone cannot directly solve the
question.

• isolated: There is no semantic relatedness between the

two knowledge units.

We treat the task as a multi-class classiﬁcation problem. The
model’s input is a pair of knowledge units, and the relatedness
is predicted to be one of the four classes mentioned above.

Figure 3 gives an illustration of the ASIM framework,
which is mainly composed of the following seven modules: (1)
Word Embedding Layer, (2) Shortcut Connections, (3) Input
Encoding Layer, (4) Attention Layer, (5) Fusion Layer, (6)
Matching Composition Layer, and (7) Prediction Layer.

A. Input of the model

We concatenate the title and body of the question and the
set of the answer to form the text sentence of a knowledge
is
unit. Suppose the text sequence of a knowledge unit
{w1, w2, ..., wn}, where n is the sequence length. The ASIM’s
inputs are KU x and KU y and the label of relatedness,
among which KU x and KU y are text sequences of a pair
of knowledge units. Since the two text sequences are treated
symmetrically before the Prediction Layer, and all parameters
except
the Prediction Layer are shared between the two
sequences. For brevity, we will only introduce the processing
method of KU x in the model.

B. Word Embedding Layer

In NLP tasks, words are usually transferred into the cor-
responding vector representations by word embeddings [21].
Word embeddings are trained on a large number of unlabeled
texts through unsupervised learning, which can capture the
rich semantic and syntactic information of words. Shirani

Fig. 3. The framework of ASIM.

et al. [12] carried out experiments applying the pre-trained
GloVe word embeddings released by Stanford [22]. These
word embeddings pre-trained in the Common Crawl corpus,
which contains a large amount of data irrelevant to software
engineering, may lead to ambiguous results [18]. Therefore,
we hope that there will be word embeddings speciﬁc to the
software engineering domain for this task, but also the research
of other tasks in software engineering. We build a 15GB
corpus based on Stack Overﬂow data dump, where the data
dump spans a temporal interval from January 2014 to June
2020. Speciﬁcally, we used the posts part to extract the content
in the body tag, and clean the content by removing the HTML
tag, code snippets, URL link, punctuation, stop words, and
changing all words to lowercase and stemming. Finally, the
GloVe method [22] was adopted for pre-training to obtain
word embeddings1 with dimensions of 300.

In Word Embedding Layer, each word in the knowledge
unit, namely wi, is transformed into a vector representing
xi ∈ Rd through our pre-trained word embeddings. Therefore,
knowledge units containing n words can be converted into
corresponding matrix representation:

KU x = x1 ⊕ x2 ⊕ · · · ⊕ xn

(1)

Where ⊕ is the concatenation operator, and the result is the
input of the next layer of the model.

C. Shortcut Connections

Between the Input Encoding Layer and the Attention Layer,
and between the Matching Composition Layer and the Predic-
tion Layer, we adopt the approach similar to residual connec-
tion [23], which can effectively mitigate the gradient vanishing
and gradient exploding and retain the original features to some

1Our pre-trained word embeddings are openly available in https://zenodo.

org/record/4641569

extent. Different from the traditional residual connection, we
refer to the idea of Huang et al. [24] and Kim et al. [25],
change the summation operation to concatenation operation so
that the characteristics of the previous layer cannot be modiﬁed
and better retained:

Ol = Hl(I l),

I l = [O(l−2); O(l−1)]

(2)

Where Ol represents the output of the lth layer Hl, and I l rep-
resents the input of the layer Hl, and [; ] denotes the concate-
nation operation. Unlike the previous layers, for the Matching
Composition Layer’s input, we refer to the idea of Yang et
al. [26], adopt another version of the residual connection.
As shown in Figure 3, between the Matching Composition
Layer and the Fusion Layer, the Attention Layer’s output
is not connected. However, the word vector representation
corresponding to the original knowledge unit is connected to
better retain the original meaning of the word in the high layer:

Ol = Hl(I l),

I l = [O(l−1); KN x]

(3)

D. Input Encoding Layer

In this layer, we adopt BiLSTM to fuse contextual infor-
mation into each word’s original representation. BiLSTM is
composed of a forward and a backward LSTM [27]. Through
its three gate structures, LSTM can solve the long-term
dependence problem very well. With BiLSTM, bi-directional
semantic dependencies within the knowledge unit can be well
captured. Precisely, one LSTM captures information from the
ﬁrst time step to the last time step, and the other LSTM
captures information in reverse. Then we connect the outputs
of the two LSTM to obtain the augmented representation of
the word, which incorporates the contextual information. The
following formula can represent this step:

(i ∈ [1, n], j ∈ [1, m]), that is, the output of two words in
the knowledge unit KU x and KU y from the Input Encoding
Layer, ﬁrst calculate the Inter-Attention matrix E ∈ Rn×m:

Ei,j = X T
i

· Yj

(7)

Where · denotes the inner production operation, and the i − th
row j − th column in the Inter-Attention matrix represents
relevance between the i−th word in KU x and the j −th word
in KU y. Based on the Inter-Attention matrix, we can calculate
the Inter-Attention vector eXi and eYj , which represent the
semantic correlation between a word and another knowledge
unit:

eXi = sof tmax(

eYj = sof tmax(

Ei,:√
k
E:,j√
k

)

)

(8)

(9)

It is worth noting that before softmax, we referred to the idea
k,
of the paper [29] and divided the Inter-Attention matrix by
where k is the dimension of vector Xi and Yj. This method
can make the gradient more stable. Finally, we can get the
Inter-Attention representation ˆXi and ˆYj:

√

ˆXi = Y · eXi
ˆYj = X · eYj

(10)

(11)

Where ˆXi is the vector representation of the i − th word in
KU x that fuses the interaction information with KU y, ˆYi is
the vector representation of the j −th word in KU y that fuses
the interaction information with KU x.

F. Fusion Layer

(cid:126)hi = LST M ((cid:126)hi−1, xi), ∀i ∈ [1, ..., n]

←

←

hi = LST M (

hi−1, xi), ∀i ∈ [1, ..., n]

Xi = [(cid:126)hi;

←

hi]

(4)

(5)

(6)

In the Fusion Layer, the contextual representation and the
Inter-Attention representation of word are integrated to fuse
aligned features. We refer to the fusion method in the paper
[26] and use the following three ways to fuse the features:

Where (cid:126)hi−1 is the hidden layer state of the forward LSTM
in the time step i − 1,
hi−1 is the hidden layer state of the
backward LSTM, xi is the input of LSTM in the time step i,
and Xi is the contextual representation of word wi.

←

E. Attention Layer

The most previous deep learning models used in semantic
relatedness tasks of PCQA websites are sentence encoding
models. These methods do not consider the interaction in-
formation between the two questions. In the ﬁeld of NLP,
attention mechanisms were ﬁrst used in the neural machine
translation model [28]. By applying the attention mecha-
nism to the question relatedness prediction task, the model
can effectively extract
the interaction information between
questions through inter-sentence alignment and achieve better
performance.

˜X 1
i = F1([Xi; ˆXi])
i = F2([Xi; Xi − ˆXi])
˜X 2
i = F3([Xi; Xi ◦ ˆXi])
˜X 3

(12)

(13)

(14)

Where − denotes calculate the difference between vectors, ◦
represents element-wise multiplication, and [; ] refers to the
concatenation operation. The subtraction operator highlights
the difference between two vectors, while the multiplication
highlights the similarity between the vectors [26]. F 1, F 2, F 3
are three single-layer feed-forward networks with independent
parameters. Then we concatenate the results obtained by the
above three fusion methods and input them into another single-
layer feed-forward network F to compute the output of the
Fusion Layer:

˜Xi = F ([ ˜X 1

i ; ˜X 2

i ; ˜X 3
i ])

(15)

In this paper, the attention mechanism is applied to capture
the knowledge units’ semantic interaction. For Xi and Yj

Similarly, for the knowledge unit KU y, the same method is
applied to obtain the Fusion Layer’s output ˜Y .

G. Matching Composition Layer

In the Matching Composition Layer, we use another BiL-
STM to extract key information from the output of the Fusion
Layer to obtain the ﬁnal vectors V X
of the two
knowledge units:

and V Y
j

i

i = BiLST M ( ˜X, i), ∀i ∈ [1, ..., n]
V X
j = BiLST M ( ˜Y , j), ∀j ∈ [1, ..., m]
V Y
Where n and m are the length of knowledge units KU x and
KU y, respectively.

(17)

(16)

H. Prediction Layer

In the Prediction Layer, for the vectors obtained from the
Matching Composition Layer, we use max-pooling to convert
them into ﬁxed-size vectors:

V X
max =

V Y
max =

n
max
i=1
m
max
j=1

vX
i

vY
j

(18)

(19)

Then we use a method similar to the paper [26], [30] to
concatenate the original vector, the difference between the
two vectors, and the result of element-wise multiplication, and
input it into a multi-layer feed-forward network L to get the
feature vector:

V = L([V X

max; V Y

max; V X

max − V Y

max; V X

max ◦ V Y

max])

(20)

Finally, the softmax function is applied to obtain the prob-

ability distribution of each class.

IV. EXPERIMENT

A. Dataset and Setup

We carry out experiments on the Knowledge Unit dataset2
built by Shirani et al. [12]. This dataset is constructed based
on Stack Overﬂow data dump and contains 347,372 pairs of
knowledge units. Moreover, all the knowledge units in this
dataset are Java-related because Java is one of the top-3 most
popular tags in Stack Overﬂow. In this dataset, knowledge
unit pairs have four relatedness classes, and the number of
each relatedness class accounts for 1/4. 60% of the dataset’s
knowledge unit pairs are used as a training set, 10% as a
validation set, and 30% as a test set. We randomly pick
knowledge unit pair samples of each class in the dataset and
list them in Table I.

B. Evaluation Metric

We use the same evaluation metrics as Shirani et al. [12]
to evaluate ASIM’s performance, including Precision, Recall,
and Micro-F1.

Precision represents the proportion of samples predicted to
be positive that are truly positive samples. Precision for all
classes is the mean of the precision for each class.

P recision =

T rueP ositive
T rueP ositive + F alseP ositive

(21)

2https://anonymousaaai2019.github.io

Recall represents the proportion of the positive samples that
are correctly predicted to be positive samples. Recall for all
classes is the mean of the recall for each class.

Recall =

T rueP ositive
T rueP ositive + F alseN egative

(22)

F1-score is the weighted average of Precision and Recall.
This metric takes into account both the Precision and Recall
of the model:

F 1 = 2 ·

precision · recall
precision + recall

(23)

In the multi-class classiﬁcation task, precision and recall are
calculated by considering all classes together and measures
the F1-score of all classes’ aggregated contributions to get the
Micro-F1.

C. Implementation Details

ASIM is implemented based on the PyTorch [31] frame-
work, references the work of paper [26] and experimented on
an Nvidia 1080Ti GPU. The input of ASIM is a knowledge
unit pair and its label: < KU x, KU y, label >. We apply
some data pre-processing steps on the input text, including
normalizing URLs and numbers, removing punctuation marks
and stop-words, splitting camel case words, stemming, and
changing all words to lowercase. The knowledge unit’s maxi-
mum length is set to 250 to keep it consistent with the paper
[12]. We initialize word embeddings with our pre-trained word
vector of size 300 and ﬁxed it during training. Compared to
GloVe word embeddings, the word embeddings speciﬁc to the
software engineering domain achieve better performance in
this task. Adam optimizer [32] with an initial learning rate
of 0.0012 was applied. For the Input Encoding Layer and
Matching Aggregation Layer, we use BiLSTM with 200 units
as the encoder. The dropout strategy [33] is adopted at each
BiLSTM and fully-connected layer, and the dropout rate is
set to 0.2. We use a batch size of 128. The model is trained
for 30 epochs to minimize the cross-entropy loss, and the
validation set of the Knowledge Unit dataset was used for the
model’s hyper-parameters tuning. We release the source code
of our model3 for more details and hope to facilitate future
researches.

D. Research Questions

We are interested in answering the following research

questions:

RQ1: How much improvement can ASIM achieve over
the two baseline models SOFTSVM and DOTBILSTM, to
predict question relatedness?

Motivation. ASIM adopts the deep learning technique and
can better learn the interaction information between knowledge
unit pairs based on the attention mechanism, which is quite
different from the SOFTSVM and the DOTBILSTM. More-
over, the DOTBILSTM is the state-of-the-art model on the
Knowledge Unit dataset. The answer to this research question

3Our model is openly available in https://github.com/Anonymousmsr/

ASIM

TABLE I
FOUR EXAMPLES FROM THE KNOWLEDGE UNIT DATASET.

Lable

Knowledge Unit x

duplicate Question Id: 36734301

Title: How to declare a call a 2d array in java
Body: I am trying to read an image’s pixels and ﬁll them in a 2d
array however I do not know how to declare a global array any
help please
Answers: null

Knowledge Unit y

Question Id: 19894714
Title: How can I create 2D arrays in java
Body: How would I go about designing something like this
using 2D arrays in java Everything works but name i j = 200
when i put this it only prints this and nothing else
Answers: You would replace name with what you would like
to name the array and you would replace x and y with the x
and y (80 more words omitted)

direct

indirect

isolated

Question Id: 8147454
Title: how to instantiate a class while jboss startup
Body: I would like to instantiate my own java class One time only
when the time of startup of JBOSS 5 and i will use that object
until i shut down the jboss How can it be possible to instantiate.
Answers: You can implement your class with the ServletCon-
textListener interface which make your class able to receive
notiﬁcations from the application server (42 more words omitted)

Question Id: 36495078
Title: Java execute method when .war ﬁle is deployed
Body: I want to execute some methods as soon as the .war
ﬁle is deployed by Tomcat or JBoss how can I do it I tried
ServletContextListener but it’s not working Thanks.
Answers: ’Have you tried adding your methods in a Servlet
and then run it on startup In your Web.xml ’, ’OK I resolved
this this works with JBoss And this works with Tomcat ’

Question Id: 4858022
Title: How to conﬁgure a log4j ﬁle appender which rolls the log
ﬁle every 15 minutes
Body: I understand that i can use a DailyRollingFileAppender to
roll the log ﬁle every month day half-day hour or minute But how
can i conﬁgure log4j to roll the log ﬁle every 15 minutes If this is
not possible by conﬁguration please suggest direct me on how to
extend log4j’s ﬁle appender to achieve this Thanks and Regards.
Answers: The Javadoc for DailyRollingFileAppender in Log4J
indicates that the time-based rolling only occurs on unit-based
rollovers day week month (319 more words omitted)

Question Id: 5030536
Title: How to log impressions and data in Java for a javascript
widget
Body: I have a javascript widget the loads JSON data from a
Java webapp I want to record impressions and the ids of the
data I return 5 or 10 longs (97 more words omitted)
Answers: Writing to log and processing off-line should be ok
You can program your logging system to create hourly log
ﬁles then process ﬁles that are not written-to any more (62
more words omitted)

Question Id: 25579459
Title: Got apache tomcat error that access denied on this ﬁle
localhost access log.2014-08-30.txt
Body: I installed Apache tomcate on Windows 7 OS I just
installed apache and make it available in eclipse When i run any
simple application on server it will say 404 page not found and in
console it will print the error message like this I cannot understand
what the things is happen is this Please help me.
Answers: It seems that your current user is not having rights on
the tomcat folder I also faced the same problem and solved it by
giving rights to the logged in user on the tomcat folder (66 more
words omitted)

Question Id: 3690114
Title: Set Depth java.util.TreeMap
Body: How can we set depth of a TreeMap object Suppose we
are trying to build an auto suggest feature on top of underlying
data structure of a TreeMap how would depth of a tree as we
know affect the performance
Answers: Your question is vague but if I understand correctly
you’re misunderstanding concepts TreeMap is an implementa-
tion of the Map interface which uses red-black tree for sorting
its contents into natural ascending order while what you’re
asking is something completely unrelated; (99 more words
omitted)

will shed light on whether and to what extent ASIM can
improve the results on predicting question relatedness on Stack
Overﬂow.

Approach. We compare the performance of ASIM on
the test set with the SOFTSVM and the DOTBILSTM. We
evaluate different models on Precision, Recall, and Micro-F1
metrics. The results of the SOFTSVM and the DOTBISLTM
are from the paper [12], which keeps the results to two decimal
places. To present the results more accurately, we keep the
results of ASIM to four decimal places.

Result. Table II presents the experiment result. We can see
that compared with DOTBILSTM and SOFTSVM, ASIM has
improved by 7% and 23%, respectively, in terms of the Micro-
F1. There is a similar improvement to the Precision and Recall.

RQ1: The experiment result shows that ASIM, based
on attention mechanism, outperforms the baseline ap-

TABLE II
MICRO-F1, PRECISION, AND RECALL OF ASIM AND THE BASELINE
MODELS.

Model/Metrics Micro-F1

Precision

Recall

SOFTSVM
DOTBILSTM

0.59
0.75

0.58
0.75

0.59
0.75

ASIM

0.8228

0.8210

0.8228

proaches SOFTSVM and DOTBILSTM in all the eval-
uation metrics and achieves state-of-the-art performance
in the Knowledge Unit dataset.

RQ2: Compared with the SOFTSVM and the DOTBIL-
STM, how effective is ASIM in predicting knowledge unit
pairs of different relatedness classes?

Motivation. The semantic relatedness between the knowl-

TABLE III
COMPARING THE RESULT (F1-SCORE) OF ASIM AND THE BASELINE
MODELS IN EACH RELATEDNESS CLASS.

TABLE IV
COMPARING THE RESULT (F1-SCORE) OF ASIM AND THE REVISED
MODELS IN EACH RELATEDNESS CLASS.

Model/Classes

duplicate

direct

indirect

isolated Micro-F1

Model/Classes

duplicate

direct

indirect

isolated Micro-F1

SOFTSVM
DOTBILSTM

0.53
0.92

0.57
0.55

0.44
0.67

0.79
0.87

0.59
0.75

ASIM

0.9299

0.6845

0.7284

0.9437

0.8228

ASIM
ASIM ( - FL)
ASIM ( - SC)
ASIM ( - FL - SC)
ASIM ( - Attn - FL
- SC)

0.9299
0.9249
0.9222
0.9177
0.9153

0.6845
0.6552
0.6838
0.6745
0.6251

0.7284
0.7245
0.7175
0.7097
0.6697

0.9437
0.9404
0.9384
0.9384
0.9217

0.8228
0.8137
0.8165
0.8124
0.7846

edge unit pairs of different classes is inconsistent, and the
prediction difﬁculty of the model is also different. We hope
to compare ASIM’s prediction performances with that of two
baseline models on each class of knowledge unit to study the
improvement and advantages of ASIM better.

Approach. We compare ASIM’s prediction results on each
knowledge unit class with the SOFTSVM and DOTBILSTM
on the F1-score metric. The results of the two baseline models
are from the paper [12].

Result. It can be seen from Table III that ASIM outperforms
DOTBiLSTM and SOFTSVM in all four classes. For the
duplicate class, because there is a high degree of semantic
relatedness between the two knowledge units, the DOTBIL-
STM’s performance is well in this class. Hence, ASIM has a
slight improvement (1%) compared with DOTBILSTM. For
the remaining three classes, ASIM’s improvement is more
signiﬁcant (5%-13%) compared with DOTBILSTM. However,
there is a certain degree of semantic relatedness between
knowledge units of direct and indirect classes, but the dis-
tinction between these two classes is no obvious. Therefore,
ASIM performs worse in these two classes than duplicate and
isolated classes, and these two classes are also bottlenecks to
limit the overall performance of ASIM. For the isolated class,
ASIM can predict well and achieve competitive performance
because there is no semantic relatedness between the knowl-
edge units, which can be well recognized.

RQ2: ASIM outperforms the two baseline models on
all four classes. Besides, for direct, indirect, and isolated
classes, ASIM’s improvement is more signiﬁcant.

RQ3: How much inﬂuence does the attention mechanism

contribute to the improvement of ASIM?

Motivation. The attention mechanism is a crucial part of
ASIM, and it helps the model capture the semantic interaction
between knowledge units, which is quite different from the
previous models. The answer to this research question helps
us understand the importance of the attention mechanism to
the ASIM.

Approach. We remove the Attention Layer (Attn) and
compare the revised model’s performance with the original
model on the F1-score. Speciﬁcally, we remove the Attention
Layer and the Fusion Layer simultaneously and remove the
related Shortcut Connections. Although this setup may show
the layer importance, it is unclear whether the Fusion Layer
or the Attention Layer or the Shortcut Connection is the

most important. Thus, we also perform the following ablation
studies, consisting in (1) only removing Fusion Layer (FL)
and replacing by a concatenation operation; (2) Only removing
the Shortcut Connections (SC) related to the Attention Layer
and (3) removing both the Fusion Layer and the Shortcut
Connections. Then, we measure how much these isolated
modiﬁcations impact model’s performance.

Result. The experimental results are shown in Table IV.
After removing the Fusion Layer or Shortcut Connection,
respectively, or removing both of them, the revised models’
performance decreases slightly (0.9%, 0.6%, and 1.0% in
terms of Micro-F1, respectively). However, after removing
the Attention Layer, the model’s performance decreases more
obviously (3.8% in Micro-F1), especially in predicting the
direct and the indirect classes. This experiment proves the
importance of the attention mechanism, which plays an es-
sential role in predicting questions relatedness. The attention
mechanism’s introduction has a more prominent effect on the
direct class and the indirect class. Furthermore, even if the
attention mechanism is removed, our model’s performance still
outperforms the two baseline models.

RQ3: The attention mechanism improves the perfor-
mance of ASIM to some extent. Moreover, the attention
mechanism plays an essential role in the prediction of
direct and indirect classes.

RQ4: What is the impact of domain-speciﬁc word embed-

dings on the performance improvement of ASIM?

Motivation. In this work, we built a 15GB corpus based
on Stack Overﬂow data dump and trained word embeddings
speciﬁc to the domain of software engineering on this corpus.
We want to know whether and to what extent the domain-
speciﬁc word embeddings improve the results compared with
the word embeddings in the general domain. The answer to

TABLE V
PERFORMANCE (F1-SCORE) OF ASIM WITH DIFFERENT WORD
EMBEDDINGS.

Model/Classes

duplicate

direct

indirect

isolated Micro-F1

GloVe word embed-
dings
Doman-speciﬁc word
embeddings

0.9242

0.6667

0.7262

0.9365

0.8154

0.9299

0.6845

0.7284

0.9437

0.8228

TABLE VI
TWO EXAMPLE QUESTION PAIRS AND THEIR LABELS FROM THE ASKUBUNTU DATASET.

Lable

duplicate

non-duplicate

Question 1

Question 2

Title: Where can I ﬁnd the source code of Ubuntu?
Body: I would like to know where to ﬁnd the source code of
Ubuntu 12.04. I’d like to see how far it is ”open source”.

Title: How can I know which is the source of an speciﬁc
standard shared libraries?
Body: How can I get access to the source code of standard
shared libraries?

Title: Graﬁcs on Thinkpad R50e
Body: After installing Ubuntu 12.04 LTS on a Thinkpad R50e,
there is no graphics driver, seems to me. The display works,
but in a magenta-red mode, video is yellowish, YouTube has
no problem. In Details, there is no graphics driver.

Title: How to share ﬁles between Windows7(Guest) and
Ubuntu 12.04(Host)?
Body: I searched on the internet but all issues have Ubuntu as
the Guest. I have VMWare Workstation 8 wherein Windows 7
is installed.

this research question helps us understand the improvement of
using our pre-trained word embeddings for this task.

Approach. We replaced the word embeddings used in the
experiment with the GloVe word embeddings 4 consistent with
Shirani et al. and experimented. The rest of the model remains
unchanged and compared with the original model on the F1-
score metric.

Result. Table V presents the experiment result. After re-
placing the word embeddings with GloVe word embeddings,
the ASIM’s performance drops slightly, which proves the
importance of domain-speciﬁc word embeddings.

RQ4: Compared with GloVe word embeddings, using
the word embeddings speciﬁc to the software engineering
domain can make ASIM achieve better performance. So,
appropriate domain-speciﬁc word embeddings are also
important in some tasks.

RQ5: How is the generalization ability of ASIM?
Motivation. The duplicate question detection in PCQA
websites is also a task to study the semantic relatedness
in software engineering. Compared with predicting question
relatedness, the most signiﬁcant difference is that this task
requires boolean prediction to determine whether a pair of
questions is duplicate. We hope to apply ASIM to a similar
but different software engineering task to explore the general-
ization performance and its robustness.

Approach. For the experiment dataset, we choose the no-
clue version of the AskUbuntu duplicate question detection
dataset (hereafter, AskUbuntu dataset) constructed by Ro-
drigues et al. [16]. In the AskUbuntu dataset, 24K question
pairs are used for training, 6K for testing, and 1K for valida-
tion. The two classes of the AskUbuntu dataset are balanced,
thus with an equal number of duplicate and non-duplicate
question pairs. We randomly select two question pair samples
of each class and list them in Table VI. Since the dataset
only contains questions but not answers, we concatenate the
question’s title and body as the model’s input and modify the
Prediction Layer’s output to 2 classes. The other part of the
model remains unchanged. We use the same set of hyper-

4http://nlp.stanford.edu/data/glove.840B.300d.zip

TABLE VII
PERFORMANCE OF DIFFERENT MODELS IN THE ASKUBUNTU DATASET.

Models

Accuracy

Jcrd
SVM-bas
SVM-adv
CNN
DNN
DCNN
DOTBILSTM 0.87
0.90
SOFTSVM

0.7291
0.7025
0.7587
0.7450
0.7865
0.7900

ASIM

0.9625

parameters tuned in the Knowledge Unit dataset’ validation
set. The ASIM is trained for 30 epochs and compares the
model’s accuracy on the test set with the DOTBILSTM,
SOFTSVM, and the baseline models in the paper [16]. The
results of the DOTBILSTM and SOFTSVM are from the paper
[12], and the results of the other baseline models are from the
paper [16].

Result. Table VII presents the results. ASIM signiﬁcantly
outperforms other models, including rule-based approaches
(Jcrd [34]), classiﬁers (SVM-bas [15], SVM-adv [16], and
SOFTSVM), and neural networks (CNN [15], DNN [20],
DCNN [16], and DOTBILSTM), achieves an accuracy of
96.25%. Moreover, DCNN is the best model in the paper
[16] on the AskUbuntu dataset, which combines the CNN and
DNN, and the SOFTSVM is the state-of-the-art model on this
dataset.

RQ5: ASIM can also achieve good performance in the
duplicate question detection task of AskUbuntu, a dif-
ferent software engineering task. ASIM outperforms the
SOTA model on this dataset and proves its generalization
ability and robustness.

V. DISCUSSION

In this section, we present the visualization of attention
with examples to show the attention mechanism’s capability
in ASIM. Then, we discuss threats to our approach’s validity.

A. Attention Visualization

We present a case study through attention visualization to
investigate what ASIM learns in the Attention Layer. We pick
titles of two duplicate question pairs from the Knowledge Unit
dataset. The question titles in the ﬁrst knowledge unit are
”How to remove HTML tag in Java” and ”Removing html
tags with regex Java”, and the question titles in the second
knowledge unit pair are ”How to send HTTP request in java”
and ’How to read my data in servlet from android’. We only
present the question’s title because the full knowledge unit’s
text sequence is too long. We visualize the word-by-word
similarity based on the Inter-Attention matrix from (7). The
attention results are shown in Figure 4, and a darker blue
indicates a stronger similarity value in the attention matrix.

(a) Attention visualization of the ﬁrst question title pair.

We can see that,

through the attention mechanism,

the
important words are emphasized in both cases. Speciﬁcally,
in case (a), because there are many common words between
the two questions, the model can easily ﬁnd these important
words. Also, the model tends to align longer phrases together
instead of individual words (e.g., the phrase ”remove html
tags”). In case (b), although there are fewer common words
between the two sentences, the model can capture the semantic
relatedness between words through learning and make the
correct alignment. For example, the word ”HTTP” is asso-
ciated mostly with ”servlet”, and the word ”java” is strongly
associated with ”android” in this case. Judging by the aligned
words and phrases, ASIM can correctly classify the two labels
as duplicate.

B. Threats to Validity

(b) Attention visualization of the second question title pair.

In this part, we discuss the threats to construct, internal and

Fig. 4. Visualization of attention results for two examples.

external validity.

Construct validity relates to the correct identiﬁcation of
measures used in the measurement procedure [5]. We use
Precision, Recall, and Micro-F1 as evaluation metrics, which
were also used in previous work to evaluate the model’s
effectiveness in predicting the question relatedness [10], [12],
[35].

Internal validity relates to errors in our code and the exper-
iment bias. To reduce the code errors, we double-checked and
comprehensively tested the code, but there may still be errors
that we overlooked. Moreover, The experimental dataset has
been used in previous work [12]. We also randomly selected
experimental data for inspection to ensure the accuracy of the
labels.

External validity relates to the generalization performance
of our study. To evaluate our approach’s generalization per-
formance, we also applied ASIM on the AskUbuntu duplicate
question detection task, another semantic relatedness task in
software engineering. However,
is unclear whether our
approach can be applied to more related tasks. In future work,
we will apply the model to other semantic relatedness tasks
in software engineering to further evaluate its generalization
performance.

it

VI. CONCLUSION AND FUTURE WORK

In this paper, we propose a deep learning model ASIM
based on the attention mechanism to predict the relatedness
between Stack Overﬂow questions, which can better help pro-
grammers obtain information and solve problems. Besides, we
pre-train word embeddings speciﬁc to the domain of software
engineering, based on the corpus collected from the Stack
Overﬂow data dump. The experiment results show ASIM’s
effectiveness and consistency in predicting question related-
ness, outperforming the two baseline models SOFTSVM and
DOTBILSTM in previous work. Moreover, in the duplicate
question detection task of AskUbuntu, ASIM can also achieve
state-of-the-art performance, which proves its generalization
ability. We will explore the application that predicts the
relatedness between a newly posted question and an already
answered question in future work.

REFERENCES

[1] R. Abdalkareem, E. Shihab, and J. Rilling, “What do developers use the
crowd for? a study using stack overﬂow,” IEEE Software, vol. 34, no. 2,
pp. 53–60, 2017. I

Workshop on Semantic Evaluation (SemEval-2016), 2016, pp. 674–679.
II-B, IV-D

[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composi-
tionality,” in Advances in neural information processing systems, 2013,
pp. 3111–3119. III-B

[22] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors
for word representation,” in Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP), 2014, pp.
1532–1543. III-B, III-B

[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778. III-C

[24] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2017, pp. 4700–4708.
III-C

[25] S. Kim, I. Kang, and N. Kwak, “Semantic sentence matching with
densely-connected recurrent and co-attentive information,” in Proceed-
ings of the AAAI conference on artiﬁcial intelligence, vol. 33, 2019, pp.
6586–6593. III-C

[26] R. Yang, J. Zhang, X. Gao, F. Ji, and H. Chen, “Simple and ef-
fective text matching with richer alignment features,” arXiv preprint
arXiv:1908.00300, 2019. III-C, III-F, III-F, III-H, IV-C

[27] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997. III-D

[28] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473,
2014. III-E

[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in neural information processing systems, 2017, pp. 5998–6008. III-E
[30] L. Mou, R. Men, G. Li, Y. Xu, L. Zhang, R. Yan, and Z. Jin, “Natural
language inference by tree-based convolution and heuristic matching,”
arXiv preprint arXiv:1512.08422, 2015. III-H

[31] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
pytorch,” 2017. IV-C

[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014. IV-C

[33] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from over-
ﬁtting,” The journal of machine learning research, vol. 15, no. 1, pp.
1929–1958, 2014. IV-C

[34] Y. Wu, Q. Zhang, and X.-J. Huang, “Efﬁcient near-duplicate detection
for q&a forum,” in Proceedings of 5th International Joint Conference
on Natural Language Processing, 2011, pp. 1001–1009. IV-D

[35] W. Fu and T. Menzies, “Easy over hard: A case study on deep learning,”
in Proceedings of the 2017 11th joint meeting on foundations of software
engineering, 2017, pp. 49–60. V-B

[2] Y. Zhang, D. Lo, X. Xia, and J.-L. Sun, “Multi-factor duplicate question
detection in stack overﬂow,” Journal of Computer Science and Technol-
ogy, vol. 30, no. 5, pp. 981–997, 2015. I, I, II-B, II-B

[3] M. Ahasanuzzaman, M. Asaduzzaman, C. K. Roy, and K. A. Schneider,
“Mining duplicate questions of stack overﬂow,” in 2016 IEEE/ACM 13th
Working Conference on Mining Software Repositories (MSR).
IEEE,
2016, pp. 402–412. I, II-B, II-B, II-B

[4] W. E. Zhang, Q. Z. Sheng, J. H. Lau, and E. Abebe, “Detecting
duplicate posts in programming qa communities via latent semantics and
association rules,” in Proceedings of the 26th International Conference
on World Wide Web, 2017, pp. 1221–1229. I

[5] L. Wang, L. Zhang, and J. Jiang, “Detecting duplicate questions in
stack overﬂow via deep learning approaches,” in 2019 26th Asia-Paciﬁc
Software Engineering Conference (APSEC).
IEEE, 2019, pp. 506–513.
I, II-B, II-B, V-B

[6] Q. Huang, X. Xia, D. Lo, and G. C. Murphy, “Automating intention

mining,” IEEE Transactions on Software Engineering, 2018. I

[7] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via
convolutional neural network,” in 2017 IEEE International Conference
on Software Quality, Reliability and Security (QRS).
IEEE, 2017, pp.
318–328. I

[8] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention
network for extreme summarization of source code,” in International
conference on machine learning, 2016, pp. 2091–2100. I

[9] V. J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, “Deep learning
type inference,” in Proceedings of the 2018 26th acm joint meeting
on european software engineering conference and symposium on the
foundations of software engineering, 2018, pp. 152–162. I

[10] B. Xu, D. Ye, Z. Xing, X. Xia, G. Chen, and S. Li, “Predicting seman-
tically linkable knowledge in developer online forums via convolutional
neural network,” in 2016 31st IEEE/ACM International Conference on
Automated Software Engineering (ASE).
I,
II-A, V-B

IEEE, 2016, pp. 51–62.

[11] X. Liu, C. Wang, Y. Leng, and C. Zhai, “Linkso: a dataset for learning to
retrieve similar question answer pairs on software development forums,”
in Proceedings of the 4th ACM SIGSOFT International Workshop on
NLP for Software Engineering, 2018, pp. 2–5. I, II-A

[12] S. Amirreza, X. Bowen, L. David, T. Solorio, and A. Alipour, “Question
relatedness on stack overﬂow: the task, dataset, and corpus-inspired
models,” in Proceedings of the AAAI Rea-soning for Complex Question
Answering Workshop, 2019. I, II-A, III, III-B, IV-A, IV-B, IV-C, IV-D,
IV-D, IV-D, V-B, V-B

[13] W. E. Zhang, Q. Z. Sheng, J. H. Lau, E. Abebe, and W. Ruan,
“Duplicate detection in programming question answering communities,”
ACM Transactions on Internet Technology (TOIT), vol. 18, no. 3, pp.
1–21, 2018. I

[14] L. Wang, L. Zhang, and J. Jiang, “Duplicate question detection with
deep learning in stack overﬂow,” IEEE Access, vol. 8, pp. 25 964–25 975,
2020. I

[15] D. Bogdanova, C. dos Santos, L. Barbosa, and B. Zadrozny, “Detecting
semantically equivalent questions in online user forums,” in Proceedings
of
the Nineteenth Conference on Computational Natural Language
Learning, 2015, pp. 123–131. I, II-B, II-B, IV-D, IV-D

[16] J. Rodrigues, C. Saedi, V. Maraev, J. Silva, and A. Branco, “Ways of
asking and replying in duplicate question detection,” in Proceedings of
the 6th Joint Conference on Lexical and Computational Semantics (*
SEM 2017), 2017, pp. 262–270. I, II-B, II-B, IV-D, IV-D, IV-D, IV-D,
IV-D, IV-D

[17] W. Lan and W. Xu, “Neural network models for paraphrase identiﬁca-
tion, semantic textual similarity, natural language inference, and question
answering,” in Proceedings of the 27th International Conference on
Computational Linguistics, 2018, pp. 3890–3902. I, II-A

[18] V. Efstathiou, C. Chatzilenas, and D. Spinellis, “Word embeddings
the 15th
for the software engineering domain,” in Proceedings of
International Conference on Mining Software Repositories, 2018, pp.
38–41. I, III-B

[19] J. Silva, J. Rodrigues, V. Maraev, C. Saedi, and A. Branco, “A 20%
jump in duplicate question detection accuracy? replicating ibm teams
experiment and ﬁnding problems in its data preparation,” META, vol. 20,
no. 4k, p. 1k, 2018. II-B

[20] N. Afzal, Y. Wang, and H. Liu, “Mayonlp at semeval-2016 task 1:
Semantic textual similarity based on lexical semantic net and deep
the 10th International
learning semantic model,” in Proceedings of

