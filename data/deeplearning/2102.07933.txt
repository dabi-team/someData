GraphGallery: A Platform for Fast Benchmarking
and Easy Development of Graph Neural Networks
Based Intelligent Software

Jintang Li, Kun Xu, Liang Chen*, Zibin Zheng
Sun Yat-sen University, China
{lijt55, xukun6}@mail2.sysu.edu.cn
{chenliang6, zhzibin}@mail.sysu.edu.cn

Xiao Liu
Deakin University, Australia
xiao.liu@deakin.edu.au

1
2
0
2

b
e
F
6
1

]
I

A
.
s
c
[

1
v
3
3
9
7
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—Graph Neural Networks

(GNNs) have recently
shown to be powerful tools for representing and analyzing graph
data. So far GNNs is becoming an increasingly critical role in soft-
ware engineering including program analysis, type inference, and
code representation. In this paper, we introduce GraphGallery, a
platform for fast benchmarking and easy development of GNNs
based software. GraphGallery is an easy-to-use platform that
allows developers to automatically deploy GNNs even with less
domain-speciﬁc knowledge. It offers a set of implementations of
common GNN models based on mainstream deep learning frame-
works. In addition, existing GNNs toolboxes such as PyG and
DGL can be easily incorporated into the platform. Experiments
demonstrate the reliability of implementations and superiority in
fast coding. The ofﬁcial source code of GraphGallery is available
at https://github.com/EdisonLeeeee/GraphGallery and a demo
video can be found at https://youtu.be/mv7Zs1YeaYo.

Index Terms—Graph Neural Networks, Benchmarking, Intel-

ligent Software Development, Open-source Platform

I. INTRODUCTION

Graph Neural Networks (GNNs) have received a consider-
able amount of attention from academia and industries mainly
due to the powerful ability in representing and modeling
the relationships between nodes or edges in a graph. GNNs
operate on graphs and manifolds by generalizing traditional
deep learning to irregular domains and thus can deal directly
with more general data forms. So far many novel architectures
have been put forward and resulted in recent breakthroughs in
tasks across various domains [1]–[7]. For example, PinSage
[7], a GNNs based system, has been successfully deployed at
Pinterest1 recommendation system with millions of users and
items.

However, mainstream deep learning frameworks, such as
TensorFlow [8] and PyTorch [9], have not yet integrated func-
tional APIs to implement GNNs conveniently and efﬁciently.
Unlike what has been developed and exploited for convolu-
tional neural networks (CNNs), the implementation of core
components of GNNs remains a challenge for developers and
researchers. To this end, a line of specialized GNNs toolboxes
have been developed for efﬁcient training. For instance, PyG

1https://www.pinterest.com/

[10] and DGL [11] are two popular toolboxes for deep learning
on graph-structured data.

While these toolboxes have eased the development of
complex GNN models, they also come with steep learning
curves for inexperienced developers, since it requires neces-
sary domain-speciﬁc knowledge as well as certain scripting
and coding experience in machine learning. Developers have
sought a platform that can deploy GNN models more con-
veniently while requiring less expert knowledge. Speciﬁcally,
there is room for improvement in the following aspects.

Deployability. It is time-consuming, tedious, and demand-
ing for developers who wish to deploy a GNN model. De-
velopers have to build their own pipeline including training
and inference procedures from scratch. Reproducibility. De-
velopers might not be able to use the existing implementations
from published algorithms directly, since they are possibly
implemented with various deep learning frameworks. Conse-
quently, developers have to adapt the code of one to another
accordingly. Benchmark. More efforts would be made for re-
searchers who want to conduct benchmarking experiments and
performance comparisons. As researchers have to implement
different benchmarking models, and it is necessary but usually
tedious work for proper training and fair parameter tuning.

We tackle the aforementioned problems by our proposed
GraphGallery, an open-source platform built on TensorFlow
and PyTorch. It allows technically inexperienced developers
to automatically deploy GNNs based software and also eases
the development of new models. In particular, GraphGallery
includes the following features:

• Uniﬁed and extensible interface. As a user-friendly
platform, GraphGallery offers uniﬁed interfaces for de-
velopers to deploy GNNs based software even without
expert knowledge.

• Multiple frameworks support. GraphGallery interfaces
with the most popular deep learning frameworks: Tensor-
Flow and PyTorch. Additionally, it integrates seamlessly
with PyG, DGL, and other GNNs toolboxes.

• Fast coding. GraphGallery provides a model gallery with
easy and elegant APIs for developers and researchers, a
GNN model can be deployed in a few lines of code.

 
 
 
 
 
 
GraphGallery is an out-of-the-box platform and ﬂexibly
supports multi-level reuse. Developers with minimum scripting
and coding experience are sufﬁcient to deploy a complete
GNN model. With this platform, developers can be more
focused on designing the architecture of GNNs based software
instead of other tedious works. Moreover, GraphGallery pro-
vides comprehensive tutorials and examples, which can serve
as a good starting point for beginners.

In conclusion, we make the following contributions with our

GraphGallery platform:

• An extensible, user-friendly, and open-source platform for

fast benchmarking and easy development.

• A series of building blocks for creating GNNs based

software quickly.

• A collection of out-of-the-box GNN models for re-

searchers and developers.

II. RELATED WORK

One of the closest analogs of GraphGallery is PyTorch
Geometric (PyG) [10] toolbox, which has been recently re-
leased as a deep learning library on irregularly structured data.
PyG follows a message passing scheme and immutable data
ﬂow paradigm and provides the essential building blocks for
creating GNNs.

Deep Graph Library (DGL) [11] is another deep learning
toolbox developed for graph-structured data. DGL takes the
generalized sparse tensor operations as computational schemes
of GNNs and advocates graph as the central programming
abstraction.

In addition, there are other toolboxes proposed to facilitate
the research and development on GNNs, such as NeuGraph
[12] and Spektral [13]. However, they have been built upon
different deep learning frameworks, and the provided APIs
are more complicated for inexperienced developers and re-
searchers in this ﬁeld.

GraphGallery is quite different from the aforementioned
toolboxes in both design principles and concepts, it considers
more about deploying GNNs based software quickly and easily
for developers. We show the comparison of GraphGallery and
other toolboxes in Table I, here we mainly compare the key
differences with PyG and DGL. Importantly, GraphGallery
is designed initially to lower the bar of entry and accelerate
research and development on GNNs based software. To this
end, GraphGallery offers uniﬁed and easy-to-use APIs for
building GNN models and supports various deep learning
frameworks. In addition, it provides uniﬁed interfaces for more
ﬂexible customizations as well as developing interfaces in
implementing GNNs.

III. GRAPHGALLERY PLATFORM

The overall architecture of GraphGallery is presented in Fig.
1. GraphGallery follows an end-to-end design and provides
a set of uniﬁed interfaces for developers. Such interfaces
enable developers to build a custom GNN model and access
the intermediate results during training and testing. With
these interfaces, developers could be more concerned with

TABLE I
COMPARISON OF GRAPHGALLERY AND OTHER GNNS TOOLBOXES.

PyG DGL GraphGallery

Features
Built-in Dataset
Custom Dataset API
Developing Interface
Model Gallery
PyTorch Support
TensorFlow Support
Training/Inference Pipeline

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:55)

(cid:51)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

Fig. 1. Conceptual architecture of GraphGallery.

the design of the model (“Model Building” in Fig. 1) while
other tedious work such as parameter tuning and break-point
resume is automatically done by this platform. Besides, we
also consider the robustness and stability of GNNs [14],
where several specialized methods have been integrated into
the inference pipeline for enhancing the robustness of GNNs
against adversarial attacks.

GraphGallery has high independility which allows devel-
opers to change any components of the framework without
impacting the other. For instance, developers could add custom
code in the components to redeﬁne the data ﬂow through the
model while keeping the rest of the code virtually unchanged.
Such a design of GraphGallery endows its maintainability,
understandability, and extensibility. Moreover, developers are
encouraged to extend GraphGallery to support other GNNs
toolboxes (see Sec. IV-D).

Similar to the popular DGL toolbox, GraphGallery adopts
a framework-neutral design instead of being framework-
agnostic. That is, a PyTorch model still needs to be made some
adjustments if it is to be run in TensorFlow. Since TensorFlow
and PyTorch are two different deep learning frameworks and
vary in many aspects, being completely framework-agnostic
requires massive effort over all conceivable operators across
the two frameworks, and this makes it hard to maintain and
extend. Instead, we adopt another practical approach to reduce
the dependencies as much as possible and endow the platform
a higher degree of ﬂexibility.

Figure 2 demonstrates the framework-neutral design of

Unified Interfaces for Training/ValidationEarly StoppingModel CheckpointLogger…•Loss•Accuracy•Time•…Output ReportTrained ModelData FlowInput GraphTraining PipelineInference PipelineModel BuildingBuilding Blocks Model Gallery…Built-in orUser-definedTensorFlow/PyTorchIndicatorBackendUnified Interfaces for InferenceGraph PurificationAdversaryDetection…Fig. 2. Framework-neutral design of GraphGallery.

GraphGallery. We adopt Numpy2 and Scipy3 compressed
sparse row (CSR) matrices as pure inputs, which are popular
forms of data in machine learning. This is quite different from
PyG and DGL that have their own data paradigms, and it
beneﬁts the extensibility of GraphGallery. Then, the inputs
could be transformed into various framework-speciﬁc tensors
with uniﬁed interfaces. The indicator (dotted square) can be
switched dynamically to another backend during program
running, developers hardly notice the differences of GNNs
when working with TensorFlow or PyTorch.

IV. FEATURES OF GRAPHGALLERY

GraphGallery is featured in many aspects, including general
and user-deﬁned datasets, comprehensive benchmark models,
multiple frameworks support, and high extensibility.

A. Built-in and User-deﬁned Dataset

GraphGallery has already assembled a number of bench-
mark datasets commonly seen in the literature. The datasets
are saved as a Numpy compressed NpzFile (A zipped archive
of ﬁles) and researchers can directly use them in their works
without taking into consideration extra preprocessing. More-
over, it is much easier to deﬁne and employ custom datasets
with the provided interfaces.

B. Model Gallery

GraphGallery is an easy-to-use platform that provides an
out-of-the-box model gallery. It
is suitable for the three
typical scenarios: (i) A beginner with less domain-speciﬁc
knowledge who wants to understand how GNN works; (ii) A
researcher who wants to conduct benchmarking experiments;
(iii) A developer who wants to deploy GNNs on existing
applications/systems. Speciﬁcally, they do not need to consider
the API details with different frameworks backend. Instead,
they can call a complete GNN implementation from the model
gallery directly by just 3-4 lines of code.

1 # Load data
2 data = load_data()
3 # Load GCN from model gallery
4 from graphgallery.gallery import GCN
5 model = GCN(data)
6 model.process(optional_paras).build(optional_paras)
7 history = model.train(training_set, validation_set)
8 report = model.test(testing_set)

Listing 1. An example to evaluate GCN on a graph using GraphGallery

2https://numpy.org/
3https://www.scipy.org/

We showcase a typical example for GraphGallery in Listing
1, here a simple GCN [1] with default hyperparameters has
been deployed and tested. Developers only need to feed the
dataset to initialize the corresponding model class (line 5),
and optionally specify the hyperparameters into preprocessing
and building methods: process and build (line 6). The
complete training pipeline is applied to optimize the model
and the training details are automatically recorded (line 7).
Finally, GraphGallery will assess the model on the testing set
and generate the corresponding report (line 8).

C. Multiple Frameworks Support

GraphGallery supports multiple frameworks: TensorFlow
and PyTorch. Alternatively, developers can write codes with
TensorFlow or PyTorch backend without considering the dif-
ferences of APIs. In addition, the model gallery introduced
in Sec. IV-B has also multiple versions of implementations.
Developers can dynamically switch it to another backend by
using graphgallery.set_backend(backend), where
backend could be TensorFlow or PyTorch. For example, sup-
pose developers want to deploy a GCN model with PyTorch
implementation, they can directly use the codes in Listing 1
with an extra line of code set_backend("PyTorch")
above, and likewise for deploying model with TensorFlow
implementations. This indeed gives the developers a high
degree of ﬂexibility and requires less effort in programming
with different frameworks.

D. PyG and DGL Integration

PyG and DGL can be integrated seamlessly into Graph-
Gallery. For researchers that develop GNN models using PyG
or DGL, the integration requires a small amount of effort, since
all these works have in common that training and inference can
be formulated as a uniﬁed operation over reusable data ﬂows.
Alternatively, GraphGallery also has a line of implementations
using PyG and DGL, which researchers are free to use by
dynamically switching the backend to another like Sec.IV-C,
e.g., set_backend("PyG").

V. BENCHMARK AND EVALUATION

In this section, we present numerical experiments with
the proposed platform. For our experiment, three mainstream
GNNs, GCN [1], SGC [15] and GAT [16] are selected and
evaluated on three common citation datasets (CiteSeer, Cora
and PubMed [17]) with TensorFlow and PyTorch backend,
respectively.

Setup. In order to evaluate the correctness and reliability of
GraphGallery implementations, we compare all methods with
corresponding implementations from other GNNs toolboxes:
PyG and DGL. We keep model architectures and hyperparam-
eters the same as the original literature or implementations.
Follow this work [1], we use ﬁxed dataset splits and report the
average accuracy and standard deviation for semi-supervised
node classiﬁcation performance over ten runs with different
random seeds. The experiments are running on a single

PyTorchTensorUnified InterfacesScipyPure InputsNumpyTensorFlowTensorTensorFlowOperatorPyTorchOperatorIndicatorTABLE II
BENCHMARK: EVALUATION RESULTS OF SELECTED MODELS IN MODEL GALLERY.

CiteSeer

GCN

SGC

GAT

GCN

Cora

SGC

PubMed

GAT

GCN

SGC

GAT

PyG

DGL

70.76±0.81

71.96±0.05

71.55±1.18

81.22±0.75

80.77±0.58

82.72±1.08

78.43±0.47

78.94±0.22

77.34±0.45

70.76±1.24

71.98±0.07

71.03±0.48

81.39±0.44

80.70±0.54

82.18±0.47

78.94±0.62

78.88±0.07

77.86±0.49

PyTorch

70.93±0.66

71.93±0.06

71.63±0.45

81.25±0.88

80.53±0.48

82.74±0.58

79.06±0.34

78.99±0.21

77.62±0.48

TensorFlow

71.36±1.04

72.80±0.59

72.19±0.39

80.48±0.97

81.33±0.61

81.96±1.02

78.91±0.36

79.24±0.29

77.91±0.45

NVIDIA TITAN RTX GPU, using TensorFlow 2.1.2, PyTorch
1.6, PyG 1.6.1, and DGL 0.5.2.

Reliability of implementation. Table II shows the perfor-
mance of three GNN models with different implementations.
Here we only show partial results of our experiments for the
sake of brevity. As we can see, accuracy values obtained by our
PyTorch implementation are almost the same with PyG and
DGL (here DGL adopts PyTorch backend). The differences
are within 0.5%. The remaining TensorFlow backend has a
relatively larger difference than the rest due to different deep
learning frameworks. However, the maximum difference is
0.8% and the average difference is admissible around 0.6%.
Therefore, the above results show that our implementations
are reliable across different frameworks and toolboxes.

Fast coding. As described in Listing 1, the benchmarking
experiments could be ﬁnished within a few lines of code.
Speciﬁcally, after loading the data, the model can be set up
automatically. Moreover, training and inference pipelines are
built with the provided interfaces. And for developers who
wish to use another framework implementation of the selected
model, what they simply need is one extra line above to call the
set_backend function. In a word, the whole benchmarking
experiment takes much less effort and could be completed
within a few lines of code. In contrast, if developers plan to run
benchmarks on a couple of models without GraphGallery, they
need to implement their own training and inference procedures
while considering lots of tedious works, which involves tens of
code and more time. Above all, GraphGallery is a fast-coding
and user-friendly platform for researchers and developers.

VI. CONCLUSION
We open-source GraphGallery, a platform for fast bench-
marking and easy development of GNNs based software.
GraphGallery supports multiple frameworks, it hides cum-
bersome details from developers and performs training and
optimization automatically. It is easy for absolute beginners
and inexperienced developers to deploy GNNs based software,
and it also beneﬁts researchers to simplify implementing and
working with GNNs. In the future, we hope more researchers
and developers in the deep graph learning community will
use GraphGallery for software development and benchmarking
experiment to accelerate their works.

VII. ACKNOWLEDGEMENT
The research is supported by the Key-Area Research
and Development Program of Guangdong Province (No.

2020B010165003), the National Natural Science Foundation
of China (No. U1711267), the Guangdong Basic and Ap-
plied Basic Research Foundation (No. 2020A1515010831),
and the Program for Guangdong Introducing Innovative and
Entrepreneurial Teams (No. 2017ZT07X355).

REFERENCES

[1] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph

convolutional networks,” in ICLR, 2017.

[2] L. Chen, Y. Liu, X. He, L. Gao, and Z. Zheng, “Matching user with item
set: Collaborative bundle recommendation with deep attention network,”
in IJCAI, 2019, pp. 2095–2101.

[3] L. Chen, Y. Liu, Z. Zheng, and P. Yu, “Heterogeneous neural attentive
factorization machine for rating prediction,” in CIKM. ACM, 2018, pp.
833–842.

[4] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, “A novel
neural source code representation based on abstract syntax tree,” in
ICSE.

IEEE / ACM, 2019, pp. 783–794.

[5] N. D. Q. Bui, Y. Yu, and L. Jiang, “Autofocus: Interpreting attention-
IEEE, 2019, pp.

based neural networks by code perturbation,” in ASE.
38–41.

[6] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in ICPC. ACM, 2020,
pp. 184–195.

[7] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec, “Graph convolutional neural networks for web-scale rec-
ommender systems,” in KDD, 2018, pp. 974–983.

[8] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning,” in OSDI, 2016, pp. 265–283.

[9] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” in NeurIPS,
2019, pp. 8026–8037.

[10] M. Fey and J. E. Lenssen, “Fast graph representation learning with
PyTorch Geometric,” in ICLR Workshop on Representation Learning
on Graphs and Manifolds, 2019.

[11] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma,
L. Yu, Y. Gai, T. Xiao, T. He, G. Karypis, J. Li, and Z. Zhang, “Deep
graph library: A graph-centric, highly-performant package for graph
neural networks,” arXiv preprint arXiv:1909.01315, 2019.

[12] L. Ma, Z. Yang, Y. Miao, J. Xue, M. Wu, L. Zhou, and Y. Dai,
“Neugraph: Parallel deep neural network computation on large graphs,”
in USENIX. USENIX Association, 2019, pp. 443–458.

[13] D. Grattarola and C. Alippi, “Graph neural networks in tensorﬂow and

keras with spektral,” arXiv preprint arXiv:2006.12138, 2020.

[14] L. Chen, J. Li, J. Peng, T. Xie, Z. Cao, K. Xu, X. He, and
Z. Zheng, “A survey of adversarial learning on graph,” arXiv preprint
arXiv:2003.05730, 2020.

[15] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger,
“Simplifying graph convolutional networks,” in ICML, vol. 97. PMLR,
2019, pp. 6861–6871.

[16] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and

Y. Bengio, “Graph Attention Networks,” ICLR, 2018.

[17] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-
Rad, “Collective classiﬁcation in network data,” AI magazine, vol. 29,
no. 3, pp. 93–93, 2008.

