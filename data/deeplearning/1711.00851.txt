Provable Defenses against Adversarial Examples
via the Convex Outer Adversarial Polytope

Eric Wong 1 J. Zico Kolter 2

Abstract

1. Introduction

8
1
0
2

n
u
J

8

]

G
L
.
s
c
[

3
v
1
5
8
0
0
.
1
1
7
1
:
v
i
X
r
a

We propose a method to learn deep ReLU-based
classiﬁers that are provably robust against norm-
bounded adversarial perturbations on the training
data. For previously unseen examples, the ap-
proach is guaranteed to detect all adversarial ex-
amples, though it may ﬂag some non-adversarial
examples as well. The basic idea is to consider
a convex outer approximation of the set of acti-
vations reachable through a norm-bounded per-
turbation, and we develop a robust optimization
procedure that minimizes the worst case loss over
this outer region (via a linear program). Cru-
cially, we show that the dual problem to this lin-
ear program can be represented itself as a deep
network similar to the backpropagation network,
leading to very efﬁcient optimization approaches
that produce guaranteed bounds on the robust
loss. The end result is that by executing a few
more forward and backward passes through a
slightly modiﬁed version of the original network
(though possibly with much larger batch sizes),
we can learn a classiﬁer that is provably robust
to any norm-bounded adversarial attack. We il-
lustrate the approach on a number of tasks to
train classiﬁers with robust adversarial guaran-
tees (e.g. for MNIST, we produce a convolutional
classiﬁer that provably has less than 5.8% test er-
ror for any adversarial attack with bounded (cid:96)∞
norm less than (cid:15) = 0.1), and code for all exper-
iments is available at http://github.com/
locuslab/convex_adversarial.

1Machine Learning Department, Carnegie Mellon Univer-
sity, Pittsburgh PA, 15213, USA 2Computer Science Department,
Carnegie Mellon University, Pittsburgh PA, 15213, USA. Cor-
respondence to: Eric Wong <ericwong@cs.cmu.edu>, J. Zico
Kolter <zkolter@cs.cmu.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Recent work in deep learning has demonstrated the preva-
lence of adversarial examples (Szegedy et al., 2014; Good-
fellow et al., 2015), data points fed to a machine learning
algorithm which are visually indistinguishable from “nor-
mal” examples, but which are speciﬁcally tuned so as to fool
or mislead the machine learning system. Recent history in
adversarial classiﬁcation has followed something of a virtual
“arms race”: practitioners alternatively design new ways of
hardening classiﬁers against existing attacks, and then a new
class of attacks is developed that can penetrate this defense.
Distillation (Papernot et al., 2016) was effective at prevent-
ing adversarial examples until it was not (Carlini & Wagner,
2017b). There was no need to worry about adversarial ex-
amples under “realistic” settings of rotation and scaling (Lu
et al., 2017) until there was (Athalye & Sutskever, 2017).
Nor does the fact that the adversary lacks full knowledge
of the model appear to be a problem: “black-box” attacks
are also extremely effective (Papernot et al., 2017). Even
detecting the presence of adversarial examples is challeng-
ing (Metzen et al., 2017; Carlini & Wagner, 2017a), and
attacks are not limited to synthetic examples, having been
demonstrated repeatedly on real-world objects (Sharif et al.,
2016; Kurakin et al., 2016). Somewhat memorably, many of
the adversarial defense papers at the most recent ICLR con-
ference were broken prior to the review period completing
(Athalye et al., 2018).

Given the potentially high-stakes nature of many machine
learning systems, we feel this situation is untenable: the
“cost” of having a classiﬁer be fooled just once is potentially
extremely high, and so the attackers are the de-facto “win-
ners” of this current game. Rather, one way to truly harden
classiﬁers against adversarial attacks is to design classiﬁers
that are guaranteed to be robust to adversarial perturbations,
even if the attacker is given full knowledge of the classiﬁer.
Any weaker attempt of “security through obscurity” could
ultimately prove unable to provide a robust classiﬁer.

In this paper, we present a method for training provably
robust deep ReLU classiﬁers, classiﬁers that are guaranteed
to be robust against any norm-bounded adversarial pertur-
bations on the training set. The approach also provides a
provable method for detecting any previously unseen adver-
sarial example, with zero false negatives (i.e., the system

 
 
 
 
 
 
Provable Defenses via the Convex Outer Adversarial Polytope

will ﬂag any adversarial example in the test set, though it
may also mistakenly ﬂag some non-adversarial examples).
The crux of our approach is to construct a convex outer
bound on the so-called “adversarial polytope”, the set of all
ﬁnal-layer activations that can be achieved by applying a
norm-bounded perturbation to the input; if we can guaran-
tee that the class prediction of an example does not change
within this outer bound, we have a proof that the example
could not be adversarial (because the nature of an adversar-
ial example is such that a small perturbation changed the
class label). We show how we can efﬁciently compute and
optimize over the “worst case loss” within this convex outer
bound, even in the case of deep networks that include rela-
tively large (for veriﬁed networks) convolutional layers, and
thus learn classiﬁers that are provably robust to such pertur-
bations. From a technical standpoint, the outer bounds we
consider involve a large linear program, but we show how
to bound these optimization problems using a formulation
that computes a feasible dual solution to this linear program
using just a single backward pass through the network (and
avoiding any actual linear programming solvers).

Using this approach we obtain, to the best of our knowledge,
by far the largest veriﬁed networks to date, with provable
guarantees of their performance under adversarial perturba-
tions. We evaluate our approach on classiﬁcation tasks such
as human activity recognition, MNIST digit classiﬁcation,
“Fashion MNIST”, and street view housing numbers. In the
case of MNIST, for example, we produce a convolutional
classiﬁer that provably has less than 5.8% test error for any
adversarial attack with bounded (cid:96)∞ norm less than (cid:15) = 0.1.

2. Background and Related Work

In addition to general work in adversarial attacks and de-
fenses, our work relates most closely to several ongoing
thrusts in adversarial examples. First, there is a great deal of
ongoing work using exact (combinatorial) solvers to verify
properties of neural networks, including robustness to adver-
sarial attacks. These typically employ either Satisﬁability
Modulo Theories (SMT) solvers (Huang et al., 2017; Katz
et al., 2017; Ehlers, 2017; Carlini et al., 2017) or integer pro-
gramming approaches (Lomuscio & Maganti, 2017; Tjeng
& Tedrake, 2017; Cheng et al., 2017). Of particular note is
the PLANET solver (Ehlers, 2017), which also uses linear
ReLU relaxations, though it employs them just as a sub-step
in a larger combinatorial solver. The obvious advantage of
these approaches is that they are able to reason about the
exact adversarial polytope, but because they are fundamen-
tally combinatorial in nature, it seems prohibitively difﬁcult
to scale them even to medium-sized networks such as those
we study here. In addition, unlike in the work we present
here, the veriﬁcation procedures are too computationally
costly to be integrated easily to a robust training procedure.

The next line of related work are methods for computing

tractable bounds on the possible perturbation regions of
deep networks. For example, Parseval networks (Cisse et al.,
2017) attempt to achieve some degree of adversarial robust-
ness by regularizing the (cid:96)2 operator norm of the weight ma-
trices (keeping the network non-expansive in the (cid:96)2 norm);
similarly, the work by Peck et al. (2017) shows how to limit
the possible layerwise norm expansions in a variety of dif-
ferent layer types. In this work, we study similar “layerwise”
bounds, and show that they are typically substantially (by
many orders of magnitude) worse than the outer bounds we
present.

Finally, there is some very recent work that relates sub-
stantially to this paper. Hein & Andriushchenko (2017)
provide provable robustness guarantees for (cid:96)2 perturbations
in two-layer networks, though they train their models using
a surrogate of their robust bound rather than the exact bound.
Sinha et al. (2018) provide a method for achieving certiﬁed
robustness for perturbations deﬁned by a certain distribu-
tional Wasserstein distance. However, it is not clear how to
translate these to traditional norm-bounded adversarial mod-
els (though, on the other hand, their approach also provides
generalization guarantees under proper assumptions, which
is not something we address in this paper).

By far the most similar paper to this work is the concur-
rent work of Raghunathan et al. (2018), who develop a
semideﬁnite programming-based relaxation of the adver-
sarial polytope (also bounded via the dual, which reduces
to an eigenvalue problem), and employ this for training a
robust classiﬁer. However, their approach applies only to
two-layer networks, and only to fully connected networks,
whereas our method applies to deep networks with arbitrary
linear operator layers such as convolution layers. Likely
due to this fact, we are able to signiﬁcantly outperform their
results on medium-sized problems: for example, whereas
they attain a guaranteed robustness bound of 35% error on
MNIST, we achieve a robust bound of 5.8% error. However,
we also note that when we do use the smaller networks they
consider, the bounds are complementary (we achieve lower
robust test error, but higher traditional test error); this sug-
gests that ﬁnding ways to combine the two bounds will be
useful as a future direction.

Our work also fundamentally relates to the ﬁeld of robust
optimization (Ben-Tal et al., 2009), the task of solving an
optimization problem where some of the problem data is
unknown, but belong to a bounded set. Indeed, robust opti-
mization techniques have been used in the context of linear
machine learning models (Xu et al., 2009) to create clas-
siﬁers that are robust to perturbations of the input. This
connection was addressed in the original adversarial exam-
ples paper (Goodfellow et al., 2015), where it was noted that
for linear models, robustness to adversarial examples can
be achieved via an (cid:96)1 norm penalty on the weights within

Provable Defenses via the Convex Outer Adversarial Polytope

Figure 1. Conceptual illustration of the (non-convex) adversarial polytope, and an outer convex bound.

the loss function.1 Madry et al. (2017) revisited this connec-
tion to robust optimization, and noted that simply solving
the (non-convex) min-max formulation of the robust opti-
mization problem works very well in practice to ﬁnd and
then optimize against adversarial examples. Our work can
be seen as taking the next step in this connection between
adversarial examples and robust optimization. Because we
consider a convex relaxation of the adversarial polytope, we
can incorporate the theory from convex robust optimization
and provide provable bounds on the potential adversarial
error and loss of a classiﬁer, using the speciﬁc form of dual
solutions of the optimization problem in question without
relying on any traditional optimization solver.

3. Training Provably Robust Classiﬁers

This section contains the main methodological contribution
of our paper: a method for training deep ReLU networks
that are provably robust to norm-bounded perturbations. Our
derivation roughly follows three steps: ﬁrst, we deﬁne the
adversarial polytope for deep ReLU networks, and present
our convex outer bound; second, we show how we can ef-
ﬁciently optimize over this bound by considering the dual
problem of the associated linear program, and illustrate how
to ﬁnd solutions to this dual problem using a single modi-
ﬁed backward pass in the original network; third, we show
how to incrementally compute the necessary elementwise
upper and lower activation bounds, using this dual approach.
After presenting this algorithm, we then summarize how the
method is applied to train provably robust classiﬁers, and
how it can be used to detect potential adversarial attacks on
previously unseen examples.

3.1. Outer Bounds on the Adversarial Polytope

In this paper we consider a k layer feedforward ReLU-based
neural network, fθ : R|x| → R|y| given by the equations

ˆzi+1 = Wizi + bi,
zi = max{ˆzi, 0},

for i = 1, . . . , k − 1

for i = 2, . . . , k − 1

(1)

with z1 ≡ x and fθ(x) ≡ ˆzk (the logits input to the clas-
siﬁer). We use θ = {Wi, bi}i=1,...,k to denote the set of
all parameters of the network, where Wi represents a linear
operator such as matrix multiply or convolution.

1This fact is well-known in robust optimization, and we merely

mean that the original paper pointed out this connection.

Figure 2. Illustration of the convex ReLU relaxation over the
bounded set [(cid:96), u].

We use the set Z(cid:15)(x) to denote the adversarial polytope, or
the set of all ﬁnal-layer activations attainable by perturbing
x by some ∆ with (cid:96)∞ norm bounded by (cid:15):2

Z(cid:15)(x) = {fθ(x + ∆) : (cid:107)∆(cid:107)∞ ≤ (cid:15)}.

(2)

For multi-layer networks, Z(cid:15)(x) is a non-convex set (it
can be represented exactly via an integer program as in
(Lomuscio & Maganti, 2017) or via SMT constraints (Katz
et al., 2017)), so cannot easily be optimized over.

The foundation of our approach will be to construct a convex
outer bound on this adversarial polytope, as illustrated in
Figure 1. If no point within this outer approximation exists
that will change the class prediction of an example, then we
are also guaranteed that no point within the true adversarial
polytope can change its prediction either, i.e., the point is ro-
bust to adversarial attacks. Our eventual approach will be to
train a network to optimize the worst case loss over this con-
vex outer bound, effectively applying robust optimization
techniques despite non-linearity of the classiﬁer.

The starting point of our convex outer bound is a linear re-
laxation of the ReLU activations. Speciﬁcally, given known
lower and upper bounds (cid:96), u for the pre-ReLU activations,
we can replace the ReLU equalities z = max{0, ˆz} from
(1) with their upper convex envelopes,

z ≥ 0, z ≥ ˆz, −uˆz + (u − (cid:96))z ≤ −u(cid:96).

(3)

The procedure is illustrated in Figure 2, and we note that if
(cid:96) and u are both positive or both negative, the relaxation is
exact. The same relaxation at the activation level was used
in Ehlers (2017), however as a sub-step for exact (combina-
torial) veriﬁcation of networks, and the method for actually
computing the crucial bounds (cid:96) and u is different. We denote
this outer bound on the adversarial polytope from replacing
the ReLU constraints as ˜Z(cid:15)(x).

2For the sake of concreteness, we will focus on the (cid:96)∞ bound
during this exposition, but the method does extend to other norm
balls, which we will highlight shortly.

InputxandallowableperturbationsFinallayer^zkandadversarialpolytopeDeepnetworkConvexouterboundℓuℓuBoundedReLUsetConvexrelaxation^zz^zzProvable Defenses via the Convex Outer Adversarial Polytope

Robustness guarantees via the convex outer adversarial
polytope. We can use this outer bound to provide prov-
able guarantees on the adversarial robustness of a classiﬁer.
Given a sample x with known label y(cid:63), we can ﬁnd the
point in ˜Z(cid:15)(x) that minimizes this class and maximizes
some alternative target ytarg, by solving the optimization
problem

minimize
ˆzk

(ˆzk)y(cid:63) − ( ˆzk)ytarg ≡ cT ˆzk

subject to ˆzk ∈ ˜Z(cid:15)(x)

(4)

where c ≡ ey(cid:63) −eytarg . Importantly, this is a linear program
(LP): the objective is linear in the decision variables, and our
convex outer approximation consists of just linear equalities
and inequalities.3 If we solve this LP for all target classes
ytarg (cid:54)= y(cid:63) and ﬁnd that the objective value in all cases is
positive (i.e., we cannot make the true class activation lower
than the target even in the outer polytope), then we know
that no norm-bounded adversarial perturbation of the input
could misclassify the example.

We can conduct similar analysis on test examples as well.
If the network predicts some class ˆy on an example x, then
we can use the same procedure as above to test whether the
network will output any different class for a norm-bounded
perturbation. If not, then the example cannot be adversarial,
because no input within the norm ball takes on a different
class (although of course, the network could still be predict-
ing the wrong class). Although this procedure may incor-
rectly “ﬂag” some non-adversarial examples, it will have
zero false negatives, e.g., there may be a normal example
that can still be classiﬁed differently due to a norm-bounded
perturbation, but all norm-bounded adversarial examples
will be detected.

Of course, two major issues remain: 1) although the LP
formulation can be solved “efﬁciently”, actually solving an
LP via traditional methods for each example, for each target
class, is not tractable; 2) we need a way of computing the
crucial (cid:96) and u bounds for the linear relaxation. We address
these in the following two sections.

3.2. Efﬁcient Optimization via the Dual Network

Because solving an LP with a number of variables equal to
the number of activations in the deep network via standard
approaches is not practically feasible, the key aspect of our
approach lies in our method for very efﬁciently bounding
these solutions. Speciﬁcally, we consider the dual problem
of the LP above; recall that any feasible dual solution pro-
vides a guaranteed lower bound on the solution of the primal.
Crucially, we show that the feasible set of the dual problem
can itself be expressed as a deep network, and one that is
very similar to the standard backprop network. This means
that providing a provable lower bound on the primal LP (and

hence also a provable bound on the adversarial error), can
be done with only a single backward pass through a slightly
modiﬁed network (assuming for the time being, that we still
have known upper and lower bounds for each activation).
This is expressed in the following theorem
Theorem 1. The dual of (4) is of the form

maximize
α

J(cid:15)(x, gθ(c, α))

subject to αi,j ∈ [0, 1], ∀i, j

(5)

where J(cid:15)(x, ν) is equal to

−

k−1
(cid:88)

i=1

i+1bi − xT ˆν1 − (cid:15)(cid:107)ˆν1(cid:107)1 +
νT

k−1
(cid:88)

(cid:88)

i=2

j∈Ii

(cid:96)i,j[νi,j]+ (6)

and gθ(c, α) is a k layer feedforward neural network given
by the equations

νk = −c
ˆνi = W T
i νi+1,
0
ˆνi,j




νi,j =



ui,j
ui,j −(cid:96)i,j

for i = k − 1, . . . , 1

j ∈ I −
i
j ∈ I +
i
[ˆνi,j]+ − αi,j[ˆνi,j]− j ∈ Ii,

(7)

for i = k − 1, . . . , 2

where ν is shorthand for (νi, ˆνi) for all i (needed because
the objective J depends on all ν terms, not just the ﬁrst),
and where I −
i , and Ii denote the sets of activations in
layer i where the lower and upper bounds are both negative,
both positive, or span zero respectively.

i , I +

The “dual network” from (7) in fact is almost identical to the
backpropagation network, except that for nodes j in Ii there
is the additional free variable αi,j that we can optimize over
to improve the objective. In practice, rather than optimizing
explicitly over α, we choose the ﬁxed, dual feasible solution

αi,j =

ui,j
ui,j − (cid:96)i,j

.

(8)

This makes the entire backward pass a linear function, and
is additionally justiﬁed by considerations regarding the con-
jugate set of the ReLU relaxation (see Appendix A.3 for
discussion). Because any solution α is still dual feasible,
this still provides a lower bound on the primal objective,
and one that is reasonably tight in practice.4 Thus, in the
remainder of this work we simply refer to the dual objective
as J(x, gθ(c)), implicitly using the above-deﬁned α terms.

We also note that norm bounds other than the (cid:96)∞ norm are
also possible in this framework: if the input perturbation
is bounded within some convex (cid:96)p norm, then the only
difference in the dual formulation is that the (cid:96)1 norm on (cid:107)ˆν(cid:107)1
changes to (cid:107)ˆν(cid:107)q where q is the dual norm of p. However,
because we focus solely on experiments with the (cid:96)∞ norm
below, we don’t emphasize this point in the current paper.

3The full explicit form of this LP is given in Appendix A.1.

4The tightness of the bound is examined in Appendix B.

Provable Defenses via the Convex Outer Adversarial Polytope

1 (cid:107)1,:
1 (cid:107)1,:

i=1 , data point x,

1 + bT
1 + bT

1 − (cid:15)(cid:107)W T
1 + (cid:15)(cid:107)W T

Algorithm 1 Computing Activation Bounds
input: Network parameters {Wi, bi}k−1
ball size (cid:15)
// initialization
ˆν1 := W T
1
γ1 := bT
1
(cid:96)2 := xT W T
u2 := xT W T
// (cid:107) · (cid:107)1,: for a matrix here denotes (cid:96)1 norm of all columns
for i = 2, . . . , k − 1 do
form I −
i , I +
// initialize new terms
νi,Ii := (Di)IiW T
i
γi := bT
i
// propagate existing terms
νj,Ij := νj,Ij DiW T
γj := γjDiW T
i , j = 1, . . . , i − 1
ˆν1 := ˆν1DiW T
i
// compute bounds
ψi := xT ˆν1 + (cid:80)i
(cid:96)i+1 := ψi − (cid:15)(cid:107)ˆν1(cid:107)1,: + (cid:80)i
ui+1 := ψi + (cid:15)(cid:107)ˆν1(cid:107)1,: − (cid:80)i

(cid:96)j,i(cid:48)[−νj,i(cid:48)]+
(cid:96)j,i(cid:48)[νj,i(cid:48)]+

i , Ii; form Di as in (10)

i , j = 2, . . . , i − 1

j=1 γj

i(cid:48)∈Ii

(cid:80)

(cid:80)

j=2

j=2

i(cid:48)∈Ii

end for
output: bounds {(cid:96)i, ui}k

i=2

3.3. Computing Activation Bounds

Thus far, we have ignored the (critical) issue of how we actu-
ally obtain the elementwise lower and upper bounds on the
pre-ReLU activations, (cid:96) and u. Intuitively, if these bounds
are too loose, then the adversary has too much “freedom” in
crafting adversarial activations in the later layers that don’t
correspond to any actual input. However, because the dual
function J(cid:15)(x, gθ(c)) provides a bound on any linear func-
tion cT ˆzk of the ﬁnal-layer coefﬁcients, we can compute J
for c = I and c = −I to obtain lower and upper bounds on
these coefﬁcients. For c = I, the backward pass variables
(where ˆνi is now a matrix) are given by

i Di+1W T

i+1 . . . DnW T
n

ˆνi = −W T
νi = Di ˆνi

where Di is a diagonal matrix with entries

(Di)jj =






0
1

ui,j
ui,j −(cid:96)i,j

j ∈ I −
i
j ∈ I +
i
j ∈ Ii

.

(9)

(10)

We can compute (νi, ˆνi) and the corresponding upper bound
J(cid:15)(x, ν) (which is now a vector) in a layer-by-layer fashion,
ﬁrst generating bounds on ˆz2, then using these to generate
bounds on ˆz3, etc.

The resulting algorithm, which uses these backward pass
variables in matrix form to incrementally build the bounds,

is described in Algorithm 1. From here on, the computa-
tion of J will implicitly assume that we also compute the
bounds. Because the full algorithm is somewhat involved,
we highlight that there are two dominating costs to the full
bound computation: 1) computing a forward pass through
the network on an “identity matrix” (i.e., a basis vector ei
for each dimension i of the input); and 2) computing a for-
ward pass starting at an intermediate layer, once for each
activation in the set Ii (i.e., for each activation where the
upper and lower bounds span zero). Direct computation of
the bounds requires computing these forward passes explic-
itly, since they ultimately factor into the nonlinear terms in
the J objective, and this is admittedly the poorest-scaling
aspect of our approach. A number of approaches to scale
this to larger-sized inputs is possible, including bottleneck
layers earlier in the network, e.g. PCA processing of the im-
ages, random projections, or other similar constructs; at the
current point, however, this remains as future work. Even
without improving scalability, the technique already can be
applied to much larger networks than any alternative method
to prove robustness in deep networks that we are aware of.

3.4. Efﬁcient Robust Optimization

Using the lower bounds developed in the previous sec-
tions, we can develop an efﬁcient optimization approach to
training provably robust deep networks. Given a data set
(xi, yi)i=1,...,N , instead of minimizing the loss at these data
points, we minimize (our bound on) the worst location (i.e.
with the highest loss) in an (cid:15) ball around each xi, i.e.,

minimize
θ

N
(cid:88)

i=1

max
(cid:107)∆(cid:107)∞≤(cid:15)

L(fθ(xi + ∆), yi).

(11)

This is a standard robust optimization objective, but prior
to this work it was not known how to train these classiﬁers
when f is a deep nonlinear network.

We also require that a multi-class loss function have the
following property (all of cross-entropy, hinge loss, and
zero-one loss have this property):
Property 1. A multi-class loss function L : R|y|×R|y| → R
is translationally invariant if for all a ∈ R,

L(y, y(cid:63)) = L(y − a1, y(cid:63)).

(12)

Under this assumption, we can upper bound the robust op-
timization problem using our dual problem in Theorem 2,
which we prove in Appendix A.4.
Theorem 2. Let L be a monotonic loss function that satis-
ﬁes Property 1. For any data point (x, y), and (cid:15) > 0, the
worst case adversarial loss from (11) can be upper bounded
by

max
(cid:107)∆(cid:107)∞≤(cid:15)

L(fθ(x + ∆), y) ≤ L(−J(cid:15)(x, gθ(ey1T − I)), y),

(13)

Provable Defenses via the Convex Outer Adversarial Polytope

where J(cid:15) is vector valued and as deﬁned in (6) for a given (cid:15),
and gθ is as deﬁned in (7) for the given model parameters θ.

Corollary 2. For a data point x, model prediction ˆy =
maxy fθ(x)y and (cid:15) > 0, if

We denote the upper bound from Theorem 2 as the robust
loss. Replacing the summand of (11) with the robust loss
results in the following minimization problem

minimize
θ

N
(cid:88)

i=1

L(−J(cid:15)(xi, gθ(eyi1T − I)), yi).

(14)

All the network terms, including the upper and lower bound
computation, are differentiable, so the whole optimization
can be solved with any standard stochastic gradient variant
and autodiff toolkit, and the result is a network that (if we
achieve low loss) is guaranteed to be robust to adversarial
examples.

3.5. Adversarial Guarantees

Although we previously described, informally, the guaran-
tees provided by our bound, we now state them formally.
The bound for the robust optimization procedure gives rise
to several provable metrics measuring robustness and detec-
tion of adversarial attacks, which can be computed for any
ReLU based neural network independently from how the
network was trained; however, not surprisingly, the bounds
are by far the tightest and the most useful in cases where the
network was trained explicitly to minimize a robust loss.

Robust error bounds The upper bound from Theorem 2
functions as a certiﬁcate that guarantees robustness around
an example (if classiﬁed correctly), as described in Corollary
1. The proof is immediate, but included in Appendix A.5.
Corollary 1. For a data point x, label y(cid:63) and (cid:15) > 0, if

J(cid:15)(x, gθ(ey(cid:63) 1T − I)) ≥ 0

(15)

(this quantity is a vector, so the inequality means that all
elements must be greater than zero) then the model is guar-
anteed to be robust around this data point. Speciﬁcally,
there does not exist an adversarial example ˜x such that
(cid:107)˜x − x(cid:107)∞ ≤ (cid:15) and fθ(˜x) (cid:54)= y(cid:63).

We denote the fraction of examples that do not have this
certiﬁcate as the robust error. Since adversaries can only
hope to attack examples without this certiﬁcate, the robust
error is a provable upper bound on the achievable error by
any adversarial attack.

Detecting adversarial examples at test time The certiﬁ-
cate from Theorem 1 can also be modiﬁed trivially to detect
adversarial examples at test time. Speciﬁcally, we replace
the bound based upon the true class y(cid:63) to a bound based
upon just the predicted class ˆy = maxy fθ(x)y. In this case
we have the following simple corollary.

J(cid:15)(x, gθ(eˆy1T − I)) ≥ 0

(16)

then x cannot be an adversarial example. Speciﬁcally, x
cannot be a perturbation of a “true” example x(cid:63) with (cid:107)x −
x(cid:63)(cid:107)∞ ≤ (cid:15), such that the model would correctly classify x(cid:63),
but incorrectly classify x.

This corollary follows immediately from the fact that the
robust bound guarantees no example with (cid:96)∞ norm within
(cid:15) of x is classiﬁed differently from x. This approach may
classify non-adversarial inputs as potentially adversarial,
but it has zero false negatives, in that it will never fail to
ﬂag an adversarial example. Given the challenge in even
deﬁning adversarial examples in general, this seems to be
as strong a guarantee as is currently possible.

(cid:15)-distances to decision boundary Finally, for each exam-
ple x on a ﬁxed network, we can compute the largest value
of (cid:15) for which a certiﬁcate of robustness exists, i.e., such
that the output fθ(x) provably cannot be ﬂipped within the (cid:15)
ball. Such an epsilon gives a lower bound on the (cid:96)∞ distance
from the example to the decision boundary (note that the
classiﬁer may or may not actually be correct). Speciﬁcally,
if we ﬁnd (cid:15) to solve the optimization problem

maximize
(cid:15)

(cid:15)

subject to J(cid:15)(x, gθ(efθ(x)1T − I))y ≥ 0,

(17)

then we know that x must be at least (cid:15) away from the deci-
sion boundary in (cid:96)∞ distance, and that this is the largest (cid:15)
for which we have a certiﬁcate of robustness. The certiﬁ-
cate is monotone in (cid:15), and the problem can be solved using
Newton’s method.

4. Experiments

Here we demonstrate the approach on small and medium-
scale problems. Although the method does not yet scale to
ImageNet-sized classiﬁers, we do demonstrate the approach
on a simple convolutional network applied to several im-
age classiﬁcation problems, illustrating that the method can
apply to approaches beyond very small fully-connected net-
works (which represent the state of the art for most existing
work on neural network veriﬁcation). Scaling challenges
were discussed brieﬂy above, and we highlight them more
below. Code for these experiments is available at http://
github.com/locuslab/convex_adversarial.

A summary of all the experiments is in Table 1. For all exper-
iments, we report the clean test error, the error achieved by
the fast gradient sign method (Goodfellow et al., 2015), the
error achieved by the projected gradient descent approach
(Madry et al., 2017), and the robust error bound. In all cases,

Provable Defenses via the Convex Outer Adversarial Polytope

Figure 3. Illustration of classiﬁcation boundaries resulting from
standard training (left) and robust training (right) with (cid:96)∞ balls of
size (cid:15) = 0.08 (shown in ﬁgure).

the robust error bound for the robust model is signiﬁcantly
lower than the achievable error rates by PGD under standard
training. All experiments were run on a single Titan X GPU.
For more experimental details, see Appendix B.

4.1. 2D Example

We consider training a robust binary classiﬁer on a 2D in-
put space with randomly generated spread out data points.
Speciﬁcally, we use a 2-100-100-100-100-2 fully connected
network. Note that there is no notion of generalization
here; we are just visualizing and evaluating the ability of the
learning approach to ﬁt a classiﬁcation function robustly.

Figure 3 shows the resulting classiﬁers produced by standard
training (left) and robust training via our method (right). As
expected, the standard training approach results in points
that are classiﬁed differently somewhere within their (cid:96)∞ ball
of radius (cid:15) = 0.08 (this is exactly an adversarial example for
the training set). In contrast, the robust training method is
able to attain zero robust error and provides a classiﬁer that
is guaranteed to classify all points within the balls correctly.

4.2. MNIST

We present results on a provably robust classiﬁer on the
MNIST data set. Speciﬁcally, we consider a ConvNet ar-
chitecture that includes two convolutional layers, with 16
and 32 channels (each with a stride of two, to decrease the
resolution by half without requiring max pooling layers),
and two fully connected layers stepping down to 100 and
then 10 (the output dimension) hidden units, with ReLUs
following each layer except the last.

Figure 4 shows the training progress using our procedure
with a robust softmax loss function and (cid:15) = 0.1. As de-
scribed in Section 3.4, any norm-bounded adversarial tech-
nique will be unable to achieve loss or error higher than the
robust bound. The ﬁnal classiﬁer after 100 epochs reaches
a test error of 1.80% with a robust test error of 5.82%. For
a traditionally-trained classiﬁer (with 1.07% test error) the
FGSM approach results in 50.01% error, while PGD results
in 81.68% error. On the classiﬁer trained with our method,
however, FGSM and PGD only achieve errors of 3.93%
and 4.11% respectively (both, naturally, below our bound of

Figure 4. Loss (left) and error rate (right) when training a robust
convolutional network on the MNIST dataset. Similar learning
curves for the other experiments can be found in Appendix B.

Figure 5. Maximum (cid:15) distances to the decision boundary of each
data point in increasing (cid:15) order for standard and robust models
(trained with (cid:15) = 0.1). The color encodes the fraction of points
which were correctly classiﬁed.

5.82%). These results are summarized in Table 1.

Maximum (cid:15)-distances Using Newton’s method with
backtracking line search, for each example, we can compute
in 5-6 Newton steps the maximum (cid:15) that is robust as de-
scribed in (17) for both a standard classiﬁer and the robust
classiﬁer. Figure 5 shows the maximum (cid:15) values calculated
for each testing data point under standard training and robust
training. Under standard training, the correctly classiﬁed
examples have a lower bound of around 0.007 away from
the decision boundary. However, with robust training this
value is pushed to 0.1, which is expected since that is the ro-
bustness level used to train the model. We also observe that
the incorrectly classiﬁed examples all tend to be relatively
closer to the decision boundary.

4.3. Other Experiments

Fashion-MNIST We present the results of our robust clas-
siﬁer on the Fashion-MNIST dataset (Xiao et al., 2017), a
harder dataset with the same size (in dimension and number
of examples) as MNIST (for which input binarization is
a reasonable defense). Using the same architecture as in
MNIST, for (cid:15) = 0.1, we achieve a robust error of 34.53%,
which is fairly close to the PGD error rate of 31.63% (Table
1). Further experimental details are in Appendix B.3.

HAR We present results on a human activity recognition
dataset (Anguita et al., 2013). Speciﬁcally, we consider a
fully connected network with one layer of 500 hidden units

0.00.51.00.00.51.00.00.51.0050100Epoch10−1100Cross entropy lossrobust trainnormal trainrobust testnormal test050100Epoch10−210−1100Error rate0500010000Datapoint # (testing set)10−310−210−1ε (standard model)0500010000Datapoint # (testing set)ε (robust model)0.40.50.60.70.80.91.0Fraction of correctlyclassified pointsProvable Defenses via the Convex Outer Adversarial Polytope

Table 1. Error rates for various problems and attacks, and our robust bound for baseline and robust models.

PROBLEM

MNIST
MNIST

FASHION-MNIST
FASHION-MNIST

HAR
HAR

SVHN
SVHN

ROBUST

×
√

×
√

×
√

×
√

(cid:15)

0.1
0.1

0.1
0.1

0.05
0.05

0.01
0.01

TEST ERROR

FGSM ERROR

PGD ERROR

ROBUST ERROR BOUND

1.07%
1.80%

9.36%
21.73%

4.95%
7.80%

16.01%
20.38%

50.01%
3.93%

77.98%
31.25%

60.57%
21.49%

62.21%
33.28%

81.68%
4.11%

81.85%
31.63%

63.82%
21.52%

83.43%
33.74%

100%
5.82%

100%
34.53%

81.56%
21.90%

100%
40.67%

and (cid:15) = 0.05, achieving 21.90% robust error.

SVHN Finally, we present results on SVHN. The goal
here is not to achieve state of the art performance on SVHN,
but to create a deep convolutional classiﬁer for real world
images with provable guarantees. Using the same archi-
tecture as in MNIST, for (cid:15) = 0.01 we achieve a robust
error bound of 42.09%, with PGD achieving 34.52% error.
Further experimental details are in Appendix B.5.

4.4. Discussion

Although these results are relatively small-scale, the some-
what surprising ability here is that by just considering a few
more forward/backward passes in a modiﬁed network to
compute an alternative loss, we can derive guaranteed error
bounds for any adversarial attack. While this is by no means
state of the art performance on standard benchmarks, this is
by far the largest provably veriﬁed network we are currently
aware of, and 5.8% robust error on MNIST represents rea-
sonable performance given that it is against any adversarial
attack strategy bounded in (cid:96)∞ norm, in comparison to the
only other robust bound of 35% from Raghunathan et al.
(2018).

Scaling to ImageNet-sized classiﬁcation problems remains
a challenging task; the MNIST classiﬁer takes about 5 hours
to train for 100 epochs on a single Titan X GPU, which
is between two and three orders of magnitude more costly
than naive training. But because the approach is not com-
binatorially more expensive in its complexity, we believe
it represents a much more feasible approach than those
based upon integer programming or satisﬁability, which
seem highly unlikely to ever scale to such problems. Thus,
we believe the current performance represents a substantial
step forward in research on adversarial examples.

5. Conclusion

In this paper, we have presented a method based upon linear
programming and duality theory for training classiﬁers that
are provably robust to norm-bounded adversarial attacks.

Crucially, instead of solving anything costly, we design an
objective equivalent to a few passes through the original
network (with larger batch size), that is a guaranteed bound
on the robust error and loss of the classiﬁer.

While we feel this is a substantial step forward in defend-
ing classiﬁers, two main directions for improvement exist,
the ﬁrst of which is scalability. Computing the bounds
requires sending an identity matrix through the network,
which amounts to a sample for every dimension of the input
vector (and more at intermediate layers, for each activation
with bounds that span zero). For domains like ImageNet,
this is completely infeasible, and techniques such as using
bottleneck layers, other dual bounds, and random projec-
tions are likely necessary. However, unlike many past ap-
proaches, this scaling is not fundamentally combinatorial,
so has some chance of success even in large networks.

Second, it will be necessary to characterize attacks beyond
simple norm bounds. While (cid:96)∞ bounded examples offer a
compelling visualization of images that look “identical” to
existing examples, this is by no means the only set of pos-
sible attacks. For example, the work in Sharif et al. (2016)
was able to break face recognition software by using manu-
factured glasses, which is clearly not bounded in (cid:96)∞ norm,
and the work in Engstrom et al. (2017) was able to fool con-
volutional networks with simple rotations and translations.
Thus, a great deal of work remains to understand both the
space of adversarial examples that we want classiﬁers to be
robust to, as well as methods for dealing with these likely
highly non-convex sets in the input space.

Finally, although our focus in this paper was on adversarial
examples and robust classiﬁcation, the general techniques
described here (optimizing over relaxed convex networks,
and using a non-convex network representation of the dual
problem to derive guaranteed bounds), may ﬁnd applica-
bility well beyond adversarial examples in deep learning.
Many problems that invert neural networks or optimize over
latent spaces involve optimization problems that are a func-
tion of the neural network inputs or activations, and similar
techniques may be brought to bear in these domains as well.

Provable Defenses via the Convex Outer Adversarial Polytope

Acknowledgements

This work was supported by a DARPA Young Faculty
Award, under grant number N66001-17-1-4036. We thank
Frank R. Schmidt for providing helpful comments on an
earlier draft of this work.

References

Anguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-
Ortiz, J. L. A public domain dataset for human activity
recognition using smartphones. In ESANN, 2013.

Athalye, A. and Sutskever, I. Synthesizing robust adversarial

examples. arXiv preprint arXiv:1707.07397, 2017.

Athalye, A., Carlini, N., and Wagner, D. Obfuscated
gradients give a false sense of security: Circumvent-
ing defenses to adversarial examples.
2018. URL
https://arxiv.org/abs/1802.00420.

Ben-Tal, A., El Ghaoui, L., and Nemirovski, A. Robust

optimization. Princeton University Press, 2009.

Carlini, N. and Wagner, D. Adversarial examples are not
easily detected: Bypassing ten detection methods.
In
Proceedings of the 10th ACM Workshop on Artiﬁcial
Intelligence and Security, pp. 3–14. ACM, 2017a.

Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In Security and Privacy (SP),
2017 IEEE Symposium on, pp. 39–57. IEEE, 2017b.

Carlini, N., Katz, G., Barrett, C., and Dill, D. L.
arXiv preprint

Ground-truth adversarial examples.
arXiv:1709.10207, 2017.

Cheng, C.-H., N¨uhrenberg, G., and Ruess, H. Maximum
resilience of artiﬁcial neural networks. In International
Symposium on Automated Technology for Veriﬁcation and
Analysis, pp. 251–268. Springer, 2017.

Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and
Usunier, N. Parseval networks: Improving robustness
to adversarial examples. In International Conference on
Machine Learning, pp. 854–863, 2017.

Ehlers, R. Formal veriﬁcation of piece-wise linear feed-
forward neural networks. In International Symposium
on Automated Technology for Veriﬁcation and Analysis,
2017.

Engstrom, L., Tsipras, D., Schmidt, L., and Madry, A. A ro-
tation and a translation sufﬁce: Fooling cnns with simple
transformations. arXiv preprint arXiv:1712.02779, 2017.

Goodfellow, I., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. In International
Conference on Learning Representations, 2015. URL
http://arxiv.org/abs/1412.6572.

Hein, M. and Andriushchenko, M. Formal guarantees on the
robustness of a classiﬁer against adversarial manipulation.
In Advances in Neural Information Processing Systems.
2017.

Huang, X., Kwiatkowska, M., Wang, S., and Wu, M. Safety
veriﬁcation of deep neural networks. In International
Conference on Computer Aided Veriﬁcation, pp. 3–29.
Springer, 2017.

Katz, G., Barrett, C., Dill, D., Julian, K., and Kochenderfer,
M. Reluplex: An efﬁcient smt solver for verifying deep
neural networks. arXiv preprint arXiv:1702.01135, 2017.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. In International Conference on Learning
Representations, 2015.

Kurakin, A., Goodfellow, I., and Bengio, S. Adversar-
arXiv preprint

ial examples in the physical world.
arXiv:1607.02533, 2016.

Lomuscio, A. and Maganti, L. An approach to reachability
analysis for feed-forward relu neural networks. arXiv
preprint arXiv:1706.07351, 2017.

Lu, J., Sibai, H., Fabry, E., and Forsyth, D. No need to
worry about adversarial examples in object detection in
autonomous vehicles. arXiv preprint arXiv:1707.03501,
2017.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.

Metzen, J. H., Genewein, T., Fischer, V., and Bischoff, B.
On detecting adversarial perturbations. In International
Conference on Learning Representations, 2017.

Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
A. Distillation as a defense to adversarial perturbations
against deep neural networks. In Security and Privacy
(SP), 2016 IEEE Symposium on, pp. 582–597. IEEE,
2016.

Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik,
Z. B., and Swami, A. Practical black-box attacks against
deep learning systems using adversarial examples. In Pro-
ceedings of the 2017 ACM Asia Conference on Computer
and Communications Security, 2017.

Peck, J., Roels, J., Goossens, B., and Saeys, Y. Lower
bounds on the robustness to adversarial perturbations. In
Advances in Neural Information Processing Systems, pp.
804–813. 2017.

Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed
defenses against adversarial examples. In International
Conference on Learning Representations, 2018.

Provable Defenses via the Convex Outer Adversarial Polytope

Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K.
Accessorize to a crime: Real and stealthy attacks on state-
of-the-art face recognition. In Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communica-
tions Security, pp. 1528–1540. ACM, 2016.

Sinha, A., Namkoong, H., and Duchi, J. Certiﬁable distribu-
tional robustness with principled adversarial training. In
International Conference on Learning Representations,
2018.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er-
han, D., Goodfellow, I., and Fergus, R.
Intriguing
properties of neural networks. In International Confer-
ence on Learning Representations, 2014. URL http:
//arxiv.org/abs/1312.6199.

Tjeng, V. and Tedrake, R.

Verifying neural net-
CoRR,
works with mixed integer programming.
abs/1711.07356, 2017. URL http://arxiv.org/
abs/1711.07356.

Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a
novel image dataset for benchmarking machine learning
algorithms. arXiv preprint arXiv:1708.07747, 2017.

Xu, H., Caramanis, C., and Mannor, S. Robustness and
regularization of support vector machines. Journal of
Machine Learning Research, 10(Jul):1485–1510, 2009.

Provable Defenses via the Convex Outer Adversarial Polytope

A. Adversarial Polytope

A.1. LP Formulation

Recall (4), which uses a convex outer bound of the adver-
sarial polytope.

where ν is shorthand for (νi, ˆνi) for all i (needed because
the objective J depends on all ν terms, not just the ﬁrst),
and where I −
i , and Ii denote the sets of activations in
layer i where the lower and upper bounds are both negative,
both positive, or span zero respectively.

i , I +

minimize
ˆzk

cT ˆzk, subject to ˆzk ∈ ˜Z(cid:15)(x)

(18)

Proof. In detail, we associate the following dual variables
with each of the constraints

With the convex outer bound on the ReLU constraint and
the adversarial perturbation on the input, this minimization
problem is the following linear program

minimize
ˆzk

cT ˆzk, subject to

ˆzi+1 = Wizi + bi, i = 1, . . . , k − 1
z1 ≤ x + (cid:15)
z1 ≥ x − (cid:15)
zi,j = 0, i = 2, . . . , k − 1, j ∈ I −
i
zi,j = ˆzi,j, i = 2, . . . , k − 1, j ∈ I +
i
zi,j ≥ 0,
zi,j ≥ ˆzi,j,
(cid:18)



(ui,j − (cid:96)i,j)zi,j

(cid:19)

− ui,j ˆzi,j

≤ −ui,j(cid:96)i,j




i = 2, . . . , k − 1, j ∈ Ii

ˆzi+1 = Wizi + bi ⇒ νi+1 ∈ R|ˆzi+1|

z1 ≤ x + (cid:15) ⇒ ξ+ ∈ R|x|
−z1 ≤ −x + (cid:15) ⇒ ξ− ∈ R|x|
−zi,j ≤ 0 ⇒ µi,j ∈ R
ˆzi,j − zi,j ≤ 0 ⇒ τi,j ∈ R
−ui,j ˆzi,j + (ui,j − (cid:96)i,j)zi,j ≤ −ui,j(cid:96)i,j ⇒ λi,j ∈ R

(23)

where we note that can easily eliminate the dual variables
corresponding to the zi,j = 0 and zi,j = ˆzi,j from the opti-
mization problem, so we don’t deﬁne explicit dual variables
for these; we also note that µi,j, τi,j, and λi,j are only de-
ﬁned for i, j such that j ∈ Ii, but we keep the notation
as above for simplicity. With these deﬁnitions, the dual
problem becomes

(19)

(cid:18)

maximize

− (x + (cid:15))T ξ+ + (x − (cid:15))T ξ−

A.2. Proof of Theorem 1

In this section we derive the dual of the LP in (19), in order
to prove Theorem 1, reproduced below:

Theorem. The dual of (4) is of the form

maximize
α

J(cid:15)(x, gθ(c, α))

subject to αi,j ∈ [0, 1], ∀i, j

(20)

(cid:18)

where J(cid:15)(x, ν) =

−

k−1
(cid:88)

i=1

i+1bi − xT ˆν1 − (cid:15)(cid:107)ˆν1(cid:107)1 +
νT

k−1
(cid:88)

(cid:88)

i=2

j∈Ii

(cid:96)i,j[νi,j]+ (21)

and gθ(c, α) is a k layer feedforward neural network given
by the equations

νk = −c
ˆνi = W T
i νi+1,
0
ˆνi,j




νi,j =



ui,j
ui,j −(cid:96)i,j

for i = k − 1, . . . , 1

j ∈ I −
i
j ∈ I +
i
[ˆνi,j]+ − αi,j[ˆνi,j]− j ∈ Ii,

(22)

for i = k − 1, . . . , 2

−

k−1
(cid:88)

i=1

νT
i+1bi +

(cid:19)

λT
i (ui(cid:96)i)

k−1
(cid:88)

i=2

subject to

νk = −c
νi,j = 0, j ∈ I −
i
νi,j = (W T



i νi+1)j, j ∈ I +
i



i νi+1)j

i = 2, . . . , k − 1
j ∈ Ii

(ui,j − (cid:96)i,j)λi,j

(cid:19)

−µi,j − τi,j

= (W T

νi,j = ui,jλi,j − µi
1 ν2 = ξ+ − ξ−

W T
λ, τ, µ, ξ+, ξ− ≥ 0

(24)

The key insight we highlight here is that the dual problem
can also be written in the form of a deep network, which
provides a trivial way to ﬁnd feasible solutions to the dual
problem, which can then be optimized over. Speciﬁcally,
consider the constraints

(ui,j − (cid:96)i,j)λi,j − µi,j − τi,j = (W T

i νi+1)j

νi,j = ui,jλi,j − µi.

(25)

Provable Defenses via the Convex Outer Adversarial Polytope

Note that the dual variable λ corresponds to the upper
bounds in the convex ReLU relaxation, while µ and τ corre-
spond to the lower bounds z ≥ 0 and z ≥ ˆz respectively; by
the complementarity property, we know that at the optimal
solution, these variables will be zero if the ReLU constraint
is non-tight, or non-zero if the ReLU constraint is tight.
Because we cannot have the upper and lower bounds be
simultaneously tight (this would imply that the ReLU input
ˆz would exceed its upper or lower bound otherwise), we
know that either λ or µ + τ must be zero. This means that
at the optimal solution to the dual problem

(ui,j − (cid:96)i,j)λi,j = [(W T
τi,j + µi,j = [(W T

i νi+1)j]+
i νi+1)j]−

(26)

i.e., the dual variables capture the positive and negative
portions of (W T
i νi+1)j respectively. Combining this with
the constraint that

νi,j = ui,jλi,j − µi

(27)

means that

νi,j =

ui,j
ui,j − (cid:96)i,j

[(W T

i νi+1)j]+ − α[(W T

i νi+1)j]− (28)

for j ∈ Ii and for some 0 ≤ α ≤ 1 (this accounts for the
fact that we can either put the “weight” of [(W T
i νi+1)j]−
into µ or τ , which will or will not be passed to the next νi).
This is exactly a type of leaky ReLU operation, with a slope
in the positive portion of ui,j/(ui,j − (cid:96)i,j) (a term between
0 and 1), and a negative slope anywhere between 0 and 1.
Similarly, and more simply, note that ξ+ and ξ− denote the
positive and negative portions of W T
1 ν2, so we can replace
these terms with an absolute value in the objective. Finally,
we note that although it is possible to have µi,j > 0 and
τi,j > 0 simultaneously, this corresponds to an activation
that is identically zero pre-ReLU (both constraints being
tight), and so is expected to be relatively rare. Putting this
all together, and using ˆν to denote “pre-activation” variables
in the dual network, we can write the dual problem in terms
of the network

νk = −c
ˆνi = W T
i νi+1, i = k − 1, . . . , 1
0
ˆνi,j




νi,j =



ui,j
ui,j −(cid:96)i,j

j ∈ I −
i
j ∈ I +
i
[ˆνi,j]+ − αi,j[ˆνi,j]− j ∈ Ii,

(29)

for i = k − 1, . . . , 2

The ﬁnal objective we are seeking to optimize can also be
written

i+1bi − (x + (cid:15))T [ˆν1]+ + (x − (cid:15))T [ˆν1]−
νT

J(cid:15)(x, ν) = −

+

k−1
(cid:88)

i=1

k−1
(cid:88)

(cid:88)

ui,j(cid:96)i,j
ui,j − (cid:96)i,j

[ˆνi,j]+

i=2

j∈Ii

k−1
(cid:88)

i=1

k−1
(cid:88)

= −

+

i+1bi − xT ˆν1 − (cid:15)(cid:107)ˆν1(cid:107)1
νT

(cid:88)

(cid:96)i,j[νi,j]+

i=2

j∈Ii

(30)

A.3. Justiﬁcation for Choice in α

While any choice of α results in a lower bound via the
dual problem, the speciﬁc choice of α = ui,j
is also
motivated by an alternate derivation of the dual problem
from the perspective of general conjugate functions. We can
represent the adversarial problem from (2) in the following,
general formulation

ui,j −(cid:96)i,j

minimize cT ˆzk + f1(z1) +

k−1
(cid:88)

i=2

fi(ˆzi, zi)

(31)

subject to ˆzi+1 = Wizi + bi, i = 1, . . . , k − 1

where f1 represents some input condition and fi represents
some non-linear connection between layers. For example,
we can take fi(ˆzi, zi) = I(max(ˆzi, 0) = zi) to get ReLU
activations, and take f1 to be the indicator function for an
(cid:96)∞ ball with radius (cid:15) to get the adversarial problem in an
(cid:96)∞ ball for a ReLU network.

Forming the Lagrangian, we get

L(z, ν, ξ) = cT ˆzk + νT

k ˆzk + f1(z1) − νT

2 W1z1

+

−

k−1
(cid:88)

i=2

k−1
(cid:88)

i=1

(cid:0)fi(ˆzi, zi) − νT

i+1Wizi + νT

i ˆzi

(cid:1)

(32)

νT
i+1bi

Conjugate functions We can re-express this using conju-
gate functions deﬁned as

which we will abbreviate as ν = gθ(c, α) to emphasize the
fact that −c acts as the “input” to the network and α are per-
layer inputs we can also specify (for only those activations
in Ii), where ν in this case is shorthand for all the νi and ˆνi
activations.

f ∗(y) = max

x

yT x − f (x)

but speciﬁcally used as

−f ∗(y) = min

x

f (x) − yT x

Provable Defenses via the Convex Outer Adversarial Polytope

Plugging this in, we can minimize over each ˆzi, zi pair
independently

f1(z1) − νT

2 W1z1 = −f ∗

1 (W T

1 ν2)

fi(ˆzi, zi) − νT

i+1Wizi + νT

i ˆzi

= −f ∗
cT ˆzk + νT

i (−νi, W T
k ˆzk = I(νk = −c)

i νi+1), i = 2, . . . , k − 1

(33)

min
z1
min
ˆzi,zi

min
ˆzk

Substituting the conjugate functions into the Lagrangian,
and letting ˆνi = W T

i νi+1, we get

maximize
ν

− f ∗

1 (ˆν1) −

k−1
(cid:88)

i=2

f ∗
i (−νi, ˆνi) −

k−1
(cid:88)

i=1

νT
i+1bi

subject to νk = −c
ˆνi = W T

i νi+1, i = 1, . . . , k − 1

(34)
This is almost the form of the dual network. The last step is
to plug in the indicator function for the outer bound of the
ReLU activation (we denote the ReLU polytope) for fi and
derive f ∗
i .

ReLU polytope Suppose we have a ReLU polytope

=

=

=

=

Let S be the set of the third case. Then:
I ∗
S (ˆy, y)
(cid:20)

(cid:21)
(ˆz − (cid:96)) + ˆy · ˆz

max
0<ˆz<u

y ·

u
u − (cid:96)

(cid:20)

max
0<ˆz<u

(cid:18) u

u − (cid:96)

(cid:19)

y + ˆy

ˆz −

+

u(cid:96)
u − (cid:96)

y

(cid:21)

+

(cid:21)

(ˆz − (cid:96)) + ˆy · ˆz = g(ˆy, y)

u
u − (cid:96)
(cid:21)

(cid:20)





y ·

max
0<ˆz<u
(cid:20)

−

u(cid:96)
u − (cid:96)

(cid:20)(cid:18) u

u − (cid:96)

y

+

(cid:19)

y + ˆy

u −

u(cid:96)
u − (cid:96)

y

(cid:21)

+

+
u
u − (cid:96)

u
u − (cid:96)

if

if

y + ˆy ≤ 0

y + ˆy > 0

(36)
Observe that the second case is always larger than ﬁrst, so
we get a tighter upper bound when u
u−(cid:96) y + ˆy ≤ 0. If we
plug in ˆy = −ν and y = ˆν, this condition is equivalent to

u
u − (cid:96)

ˆν ≤ ν

Recall that in the LP form, the forward pass in this case was
deﬁned by

ν =

[ˆν]+ + α[ˆν]−

u
u − (cid:96)

Then, α = u
u−l can be interpreted as the largest choice of
α which does not increase the bound (because if α was any
larger, we would enter the second case and add an additional
(cid:16) u

(cid:17)

u term to the bound).

u−(cid:96) ˆν − ν

Si = {(ˆzi, zi) : ˆzi,j ≥ 0,

zi,j ≥ ˆzi,j,

(35)

We can verify that using α = u
problem by ﬁrst simplifying the above to

u−(cid:96) results in the same dual

−ui,j ˆzi,j + (ui,j − (cid:96)i,j)zi,j ≤ −ui,j(cid:96)i,j}

I ∗
S (ν, ˆν) = −l[ν]+

So IS is the indicator for this set, and I ∗
S is its conjugate.
We will omit subscripts (i, j) for brevity, but we can do this
case by case elementwise.

1. If u ≤ 0 then S ⊂ {(ˆz, z) : z = 0}.

Then, I ∗

S (ˆy, y) ≤ maxˆz ˆy · ˆz = I(ˆy = 0).

2. If (cid:96) ≥ 0 then S ⊂ {(ˆz, z) : ˆz = z}.

Then, I ∗
I(ˆy + y = 0).

S (ˆy, y) ≤ maxz ˆy · z + y · z = (ˆy + y)z =

3. Otherwise S = {(ˆz, z) : ˆz ≥ 0, z ≥ ˆz, −uˆz + (u −
(cid:96))z = −u(cid:96)}. The maximum must occur either on the
line −uˆz + (u − (cid:96))z = −u(cid:96) over the interval [0, u],
or at the point (ˆz, z) = (0, 0) (so the maximum must
have value at least 0). We proceed to examine this last
case.

Combining this with the earlier two cases and plugging into
(34) using f ∗

S results in

i = I ∗

νT
i+1bi

k−1
(cid:88)

i=1


maximize
ν

− xT ˆν1 − f ∗

1 (ˆν1) −

k−1
(cid:88)

+





(cid:88)

i=2

j∈I

subject to

li,j[νi,j]+



i νi+1, i = 1, . . . , k − 1

νk = −c
ˆνi = W T
νi,j = 0, i = 2, . . . , k − 1, j ∈ I −
i
νi,j = ˆνi,j, , i = 2, . . . , k − 1, j ∈ I +
i

νi,j =

ui,j
ui,j − li,j

ˆνi,j, i = 2, . . . , k − 1, j ∈ Ii

(37)
where the dual network here matches the one from (7) ex-
actly when α = ui,j

.

ui,j −li,j

Provable Defenses via the Convex Outer Adversarial Polytope

A.4. Proof of Theorem 2

A.5. Proof of Corollary 1

In this section, we prove Theorem 2, reproduced below:
Theorem. Let L be a monotonic loss function that satisﬁes
Property 1. For any data point (x, y), and (cid:15) > 0, the worst
case adversarial loss from (11) can be upper bounded with

max
(cid:107)∆(cid:107)∞≤(cid:15)

L(fθ(x + ∆), y) ≤ L(−J(cid:15)(x, gθ(ey1T − I)), y)

where J(cid:15) is as deﬁned in (6) for a given x and (cid:15), and gθ is
as deﬁned in (7) for the given model parameters θ.

In this section, we prove Corollary 1, reproduced below:

Theorem. For a data point x and (cid:15) > 0, if

min
y(cid:54)=f (x)

[J(cid:15)(x, gθ(ef (x)1T − I, α))]y ≥ 0

(40)

then the model is guaranteed to be robust around this data
point. Speciﬁcally, there does not exist an adversarial exam-
ple ˜x such that |˜x − x|∞ ≤ (cid:15) and fθ(˜x) (cid:54)= fθ(x).

Proof. First, we rewrite the problem using the adversarial
polytope Z(cid:15)(x).

Proof. Recall that J from (6) is a lower bound on (2). Com-
bining this fact with the certiﬁcate in (40), we get that for
all y (cid:54)= f (x),

max
(cid:107)∆(cid:107)∞≤(cid:15)

L(fθ(x + ∆), y) = max

ˆzk∈Z(cid:15)(x)

L(ˆzk, y)

Since L(x, y) ≤ L(x − a1, y) for all a, we have

max
ˆzk∈Z(cid:15)(x)

L(ˆzk, y) ≤ max

ˆzk∈Z(cid:15)(x)

L(ˆzk − (ˆzk)y1, y)

= max

ˆzk∈Z(cid:15)(x)

= max

ˆzk∈Z(cid:15)(x)

L((I − ey1T )ˆzk, y)

(38)

L(C ˆzk, y)

where C = (I − ey1T ). Since L is a monotone loss func-
tion, we can upper bound this further by using the element-
wise maximum over [C ˆzk]i for i (cid:54)= y, and elementwise-
minimum for i = y (note, however, that for i = y,
[C ˆzk]i = 0). Speciﬁcally, we bound it as

max
ˆzk∈Z(cid:15)(x)

L(C ˆzk, y) ≤ L(h(ˆzk))

where, if Ci is the ith row of C, h(zk) is deﬁned element-
wise as

h(zk)i = max

ˆzk∈Z(cid:15)(x)

Ci ˆzk

This is exactly the adversarial problem from (2) (in its max-
imization form instead of a minimization). Recall that J
from (6) is a lower bound on (2) (using c = −Ci).

J(cid:15)(x, gθ(−Ci)) ≤ min

ˆzk∈Z(cid:15)(x)

−C T

i ˆzk

(39)

Multiplying both sides by −1 gives us the following upper
bound

−J(cid:15)(x, gθ(−Ci)) ≥ max

ˆzk∈Z(cid:15)(x)

C T

i ˆzk

Applying this upper bound to h(zk)i, we conclude

h(zk)i ≤ −J(cid:15)(x, gθ(−Ci))

Applying this to all elements of h gives the ﬁnal upper
bound on the adversarial loss.

max
(cid:107)∆(cid:107)∞≤(cid:15)

L(fθ(x + ∆), y) ≤ L(−J(cid:15)(x, gθ(ey1T − I)), y)

min
ˆzk∈Z(cid:15)(x)

(ˆzk)f (x) − (ˆzk)y ≥ 0

Crucially, this means that for every point in the adversarial
polytope and for any alternative label y, (ˆzk)f (x) ≥ (ˆzk)y,
so the classiﬁer cannot change its output within the adver-
sarial polytope and is robust around x.

B. Experimental Details

B.1. 2D Example

Problem Generation We incrementally randomly sample
12 points within the [0, 1] xy-plane, at each point waiting
until we ﬁnd a sample that is at least 0.16 away from other
points via (cid:96)∞ distance, and assign each point a random label.
We then attempt to learn a robust classiﬁer that will correctly
classify all points with an (cid:96)∞ ball of (cid:15) = 0.08.

Parameters We use the Adam optimizer (Kingma & Ba,
2015) (over the entire batch of samples) with a learning rate
of 0.001.

Visualizations of the Convex Outer Adversarial Poly-
tope We consider some simple cases of visualizing the
outer approximation to the adversarial polytope for random
networks in Figure 6. Because the output space is two-
dimensional we can easily visualize the polytopes in the
output layer, and because the input space is two dimen-
sional, we can easily cover the entire input space densely to
enumerate the true adversarial polytope. In this experiment,
we initialized the weights of the all layers to be normal
N (0, 1/
nin) and biases normal N (0, 1) (due to scaling,
the actual absolute value of weights is not particularly im-
portant except as it relates to (cid:15)). Although obviously not
too much should be read into these experiments with ran-
dom networks, the main takeaways are that 1) for “small”
(cid:15), the outer bound is an extremely good approximation to
the adversarial polytope; 2) as (cid:15) increases, the bound gets
substantially weaker. This is to be expected: for small (cid:15), the
number of elements in I will also be relatively small, and

√

Provable Defenses via the Convex Outer Adversarial Polytope

Figure 6. Illustrations of the true adversarial polytope (gray) and our convex outer approximation (green) for a random 2-100-100-100-
n) weight initialization. Polytopes are shown for (cid:15) = 0.05 (top row), (cid:15) = 0.1 (middle row), and (cid:15) = 0.25
100-2 network with N (0, 1/
(bottom row).

√

thus additional terms that make the bound lose are expected
to be relatively small (in the extreme, when no activation
can change, the bound will be exact, and the adversarial
polytope will be a convex set). However, as (cid:15) gets larger,
more activations enter the set I, and the available freedom
in the convex relaxation of each ReLU increases substan-
tially, making the bound looser. Naturally, the question of
interest is how tight this bound is for networks that are actu-
ally trained to minimize the robust loss, which we will look
at shortly.

Comparison to Naive Layerwise Bounds One addi-
tional point is worth making in regards to the bounds we
propose. It would also be possible to achieve a naive “layer-
wise” bound by iteratively determining absolute allowable
ranges for each activation in a network (via a simple norm
bound), then for future layers, assuming each activation can
vary arbitrarily within this range. This provides a simple iter-
ative formula for computing layer-by-layer absolute bounds
on the coefﬁcients, and similar techniques have been used
e.g. in Parseval Networks (Cisse et al., 2017) to produce

more robust classiﬁers (albeit there considering (cid:96)2 pertur-
bations instead of (cid:96)∞ perturbations, which likely are better
suited for such an approach). Unfortunately, these naive
bounds are extremely loose for multi-layer networks (in the
ﬁrst hidden layer, they naturally match our bounds exactly).
For instance, for the adversarial polytope shown in Figure 6
(top left), the actual adversarial polytope is contained within
the range

ˆzk,1 ∈ [1.81, 1.85], ˆzk,2 ∈ [−1.33, −1.29]

(41)

with the convex outer approximation mirroring it rather
closely.
In contrast, the layerwise bounds produce the
bound:

ˆzk,1 ∈ [−11.68, 13.47], ˆzk,2 ∈ [−16.36, 11.48].

(42)

Such bounds are essentially vacuous in our case, which
makes sense intuitively. The naive bound has no way to
exploit the “tightness” of activations that lie entirely in the
positive space, and effectively replaces the convex ReLU
approximation with a (larger) box covering the entire space.
Thus, such bounds are not of particular use when consider-
ing robust classiﬁcation.

1.8101.8151.8201.8251.8301.8351.8401.8451.850ˆzk,11.3301.3251.3201.3151.3101.3051.3001.2951.290ˆzk,2Outer approximationTrue adversarial polytope0.3450.3500.3550.3600.3650.3700.3750.3800.385ˆzk,10.3350.3400.3450.3500.3550.3600.365ˆzk,20.0940.0960.0980.1000.1020.1040.1060.108ˆzk,10.640.650.660.670.680.690.70ˆzk,20.820.800.780.760.740.720.70ˆzk,10.3300.3250.3200.3150.3100.3050.3000.2950.290ˆzk,20.320.310.300.290.280.270.26ˆzk,10.670.680.690.700.71ˆzk,21.031.041.051.061.071.081.091.101.11ˆzk,11.331.321.311.301.29ˆzk,20.450.500.550.600.650.700.750.800.85ˆzk,10.30.40.50.60.70.80.91.0ˆzk,21.701.751.801.851.901.952.002.05ˆzk,13.23.13.02.92.82.72.6ˆzk,20.050.100.150.200.250.300.350.40ˆzk,11.51.61.71.81.92.02.1ˆzk,2Provable Defenses via the Convex Outer Adversarial Polytope

Figure 7. Illustration of the actual adversarial polytope and the
convex outer approximation for one of the training points after the
robust optimization procedure.

Outer Bound after Training
It is of some interest to see
what the true adversarial polytope for the examples in this
data set looks like versus the convex approximation, eval-
uated at the solution of the robust optimization problem.
Figure 7 shows one of these ﬁgures, highlighting the fact
that for the ﬁnal network weights and choice of epsilon, the
outer bound is empirically quite tight in this case. In Ap-
pendix B.2 we calculate exactly the gap between the primal
problem and the dual bound on the MNIST convolutional
model. In Appendix B.4, we will see that when training on
the HAR dataset, even for larger (cid:15), the bound is empirically
tight.

B.2. MNIST

Parameters We use the Adam optimizer (Kingma & Ba,
2015) with a learning rate of 0.001 (the default option) with
no additional hyperparameter selection. We use minibatches
of size 50 and train for 100 epochs.

(cid:15) scheduling Depending on the random weight initial-
ization of the network, the optimization process for train-
ing a robust MNIST classiﬁer may get stuck and not con-
verge. To improve convergence, it is helpful to start with
a smaller value of (cid:15) and slowly increment it over epochs.
For MNIST, all random seeds that we observed to not con-
verge for (cid:15) = 0.1 were able to converge when started with
(cid:15) = 0.05 and taking uniform steps to (cid:15) = 0.1 in the ﬁrst
half of all epochs (so in this case, 50 epochs).

MNIST convolutional ﬁlters Random ﬁlters from the
two convolutional layers of the MNIST classiﬁer after ro-
bust training are plotted in Figure 9. We see a similar story

Figure 8. Learned convolutional ﬁlters for MNIST of the ﬁrst layer
of a trained robust convolutional network, which are quite sparse
due to the (cid:96)1 term in (6).

in both layers: they are highly sparse, and some ﬁlters have
all zero weights.

2 set than in the I −

Activation index counts We plot histograms to visualize
the distributions of pre-activation bounds over examples in
Figure 10. We see that in the ﬁrst layer, examples have
on average more than half of all their activations in the
I −
1 set, with a relatively small number of activations in the
I1 set. The second layer has signiﬁcantly more values in
the I +
2 set, with a comparably small
number of activations in the I2 set. The third layer has
extremely few activations in the I3 set, with 90% all of
the activations in the I −
3 set. Crucially, we see that in
all three layers, the number of activations in the Ii set is
small, which beneﬁts the method in two ways: a) it makes
the bound tighter (since the bound is tight for activations
through the I +
i sets) and b) it makes the bound more
computationally efﬁcient to compute (since the last term of
(6) is only summed over activations in the Ii set).

i and I −

Tightness of bound We empirically evaluate the tight-
ness of the bound by exactly computing the primal LP and
comparing it to the lower bound computed from the dual
problem via our method. We ﬁnd that the bounds, when
computed on the robustly trained classiﬁer, are extremely
tight, especially when compared to bounds computed for
random networks and networks that have been trained under
standard training, as can be seen in Figure 11.

3456789101112ˆzk,1−ˆzk,20.200.150.100.050.00ˆzk,1+ˆzk,2Outer approximationTrue adversarial polytope0.80.60.40.20.00.20.40.60.8Provable Defenses via the Convex Outer Adversarial Polytope

Figure 9. Learned convolutional ﬁlters for MNIST of the second
layer of a trained robust convolutional network, which are quite
sparse due to the (cid:96)1 term in (6).

Figure 10. Histograms of the portion of each type of index set (as
deﬁned in 10 when passing training examples through the network.

B.3. Fashion-MNIST

Parameters We use exactly the same parameters as for
MNIST: Adam optimizer with the default learning rate
0.001, minibatches of size 50, and trained for 100 epochs.

Learning curves Figure 12 plots the error and loss curves
(and their robust variants) of the model over epochs. We
observe no overﬁtting, and suspect that the performance on
this problem is limited by model capacity.

B.4. HAR

Parameters We use the Adam optimizer with a learning
rate 0.0001, minibatches of size 50, and trained for 100
epochs.

Learning Curves Figure 13 plots the error and loss
curves (and their robust variants) of the model over epochs.
The bottleneck here is likely due to the simplicity of the
problem and the difﬁculty level implied by the value of (cid:15), as
we observed that scaling to more more layers in this setting
did not help.

Table 2. Tightness of the bound on a single layer neural network
with 500 hidden units after training on the HAR dataset with
various values of (cid:15). We observe that regardless of how large (cid:15) is,
after training, the bound matches the error achievable by FGSM,
implying that in this case the robust bound is tight.

(cid:15)

TEST ERROR

FGSM ERROR

ROBUST BOUND

0.05
0.1
0.25
0.5
1

9.20%
15.74%
47.66%
47.08%
81.80%

22.20%
36.62%
64.24%
67.32%
81.80%

22.80%
37.09%
64.47%
67.86%
81.80%

on the HAR dataset with a single hidden layer, the bound
still stays quite tight, as seen in Table 2. As expected, train-
ing a robust model with larger (cid:15) results in a less accurate
model since the adversarial problem is more difﬁcult (and
potentially impossible to solve for some data points), how-
ever the key point is that the robust bounds are extremely
close to the achievable error rate by FGSM, implying that
in this case, the bound is tight.

B.5. SVHN

Tightness of bound with increasing (cid:15) Earlier, we ob-
served that on random networks, the bound gets progres-
sively looser with increasing (cid:15) in Figure 6. In contrast, we
ﬁnd that even if we vary the value of (cid:15), after robust training

Parameters We use the Adam optimizer with the default
learning rate 0.001, minibatches of size 20, and trained for
100 epochs. We used an (cid:15) schedule which took uniform
steps from (cid:15) = 0.001 to (cid:15) = 0.01 over the ﬁrst 50 epochs.

420241200130014000100002000030000400005000060000# of examples in  layer 017001800190005010015011001150120012500100002000030000400005000060000# of examples in  layer 12002503003501002006810# of positive indices0100002000030000400005000060000# of examples in  layer 290919293# of negative indices024# of origin crossing indicesProvable Defenses via the Convex Outer Adversarial Polytope

Figure 11. Plots of the exact solution of the primal linear program and the corresponding lower bound from the dual problem for a (left)
robustly trained model, (middle) randomly intialized model, and (right) model with standard training.

Learning Curves Note that the robust testing curve is the
only curve calculated with (cid:15) = 0.01 throughout all 100
epochs. The robust training curve was computed with the
scheduled value of (cid:15) at each epoch. We see that all metrics
calculated with the scheduled (cid:15) value steadily increase after
the ﬁrst few epochs until the desired (cid:15) is reached. On the
other hand, the robust testing metrics for (cid:15) = 0.01 steadily
decrease until the desired (cid:15) is reached. Since the error
rate here increases with (cid:15), it suggests that for the given
model capacity, the robust training cannot achieve better
performance on SVHN, and a larger model is needed.

Figure 12. Loss (top) and error rate (bottom) when training a robust
convolutional network on the Fashion-MNIST dataset.

010002000Linear program #01020Adversarial loss010002000Linear program #3.53.02.52.0Lower boundPrimal solution010002000Linear program #10000800060004000020406080100Epoch1006×1012×100Cross entropy lossrobust trainnormal trainrobust testnormal test020406080100Epoch2×1013×1014×1016×101Error raterobust trainnormal trainrobust testnormal testProvable Defenses via the Convex Outer Adversarial Polytope

Figure 13. Loss (top) and error rate (bottom) when training a robust
fully connected network on the HAR dataset with one hidden layer
of 500 units.

Figure 14. Loss (top) and error rate (bottom) when training a robust
convolutional network on the SVHN dataset. The robust test curve
is the only curve calculated with (cid:15) = 0.01 throughout; the other
curves are calculated with the scheduled (cid:15) value.

020406080100Epoch101100Cross entropy lossrobust trainnormal trainrobust testnormal test020406080100Epoch102101100Error raterobust trainnormal trainrobust testnormal test020406080100Epoch100101Cross entropy lossrobust trainnormal trainrobust testnormal test020406080100Epoch101100Error raterobust trainnormal trainrobust testnormal test