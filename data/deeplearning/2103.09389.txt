1
2
0
2

g
u
A
2
1

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
9
8
3
9
0
.
3
0
1
2
:
v
i
X
r
a

PythonFOAM: In-situ data analyses with OpenFOAM
and Python

Romit Maulika, Dimitrios K. Fytanidisb,c, Bethany Luscha, Venkatram
Vishwanatha, Saumil Patelb

aArgonne Leadership Computing Facility, Bldg. 240, Argonne National Laboratory,
Lemont, IL-60439
bComputational Science Division, Bldg. 240, Argonne National Laboratory, Lemont,
IL-60439
cDepartment of Civil and Environmental Engineering, Ven Te Chow Hydrosystems
Laboratory, University of Illinois at Urbana-Champaign, IL-61801

Abstract

We outline the development of a general-purpose Python-based data analysis
tool for OpenFOAM. Our implementation relies on the construction of Open-
FOAM applications that have bindings to data analysis libraries in Python.
Double precision data in OpenFOAM is cast to a NumPy array using the
NumPy C-API and Python modules may then be used for arbitrary data
analysis and manipulation on ﬂow-ﬁeld information. We highlight how the
proposed wrapper may be used for an in-situ online singular value decom-
position (SVD) implemented in Python and accessed from the OpenFOAM
solver PimpleFOAM. Here, ‘in-situ’ refers to a programming paradigm that
allows for a concurrent computation of the data analysis on the same compu-
tational resources utilized for the partial diﬀerential equation solver. In ad-
dition, to demonstrate data-parallel analyses, we deploy a distributed SVD,
which collects snapshot data across the ranks of a distributed simulation
to compute the global left singular vectors. Crucially, both OpenFOAM
and Python share the same message passing interface (MPI) communicator
for this deployment which allows Python objects and functions to exchange
NumPy arrays across ranks. Subsequently, we provide scaling assessments of
this distributed SVD on multiple nodes of Intel Broadwell and KNL architec-
tures for canonical test cases such as the large eddy simulations of a backward

Email address: rmaulik@anl.gov (Romit Maulik)

Preprint submitted to Elsevier

August 13, 2021

 
 
 
 
 
 
facing step and a channel ﬂow at friction Reynolds number of 395. Finally,
we demonstrate the deployment of a deep neural network for compressing
the ﬂow-ﬁeld information using an autoencoder to demonstrate an ability to
use state-of-the-art machine learning tools in the Python ecosystem.

Keywords: OpenFOAM, Python, Data analytics, Deep learning

1. Introduction

For most large-scale computational studies, users are frequently required
to make diﬃcult decisions with respect to how often simulation data must
be exported to storage. This mainly pertains to the limitations of input-
output (I/O) bandwidth and the desire to limit the ratio of compute time to
I/O time. In-situ data analyses and machine learning strategies promise an
alternate route to reducing this ratio and consequently reduce the time to
solution for the simulation workﬂow. This allows for the user to export the
post-analysis quantities of interest to storage (as compressed data, visualiza-
tion, or discovered models). This also allows a user to have greater control
over the temporal resolution of the analyses; for instance passing checkpoint
data to a machine learning algorithm would not be limited by a storage
bottleneck. In essence, in-situ algorithms and software provide a promising
avenue to bypass the limitations of the classical pre-processing, simulation,
and post-processing workﬂow, particularly as modern solvers start leveraging
exascale infrastructure [1].

In this article we focus on OpenFOAM [2], a well-established open-source
ﬁnite-volume code for computational ﬂuid dynamics. OpenFOAM has been
used across industry and academia for a diverse range of applications such
as coastal engineering [3], wave-structure interaction [4], multiphase heat
transfer [5], turbulent separated ﬂows [6], cavitation [7], non-Newtonian ﬂows
[8], design optimization [9], rareﬁed gas dynamics [10], large eddy simulations
[11, 12] etc. Therefore, any tool for in-situ data analysis in OpenFOAM
has the potential to be highly impactful across many domains. Currently,
there are no packages that can provide a robust, easy-to-use, and ﬂexible
interface between Python and OpenFOAM. Instead, most studies have relied
on application-speciﬁc combinations of OpenFOAM and data analysis tools
that do not readily generalize to arbitrary problems.

Geneva and Zabaras [13] embed a neural network model into OpenFOAM
4.1 using the PyTorch C backend. However, this procedure requires the in-

2

stallation of additional software packages such as ONNX and Caﬀe2 that
may cause issues with dependencies. In addition, Caﬀe2 is deprecated (to be
subsumed into PyTorch), and future incorporation of PyTorch models into
OpenFOAM through this route is unclear. Another tool under active devel-
opment is the Fortran-Keras Bridge (FKB) [14], which successfully couples
densely connected neural networks to Fortran simulation codes. However,
FKB is yet to support more complicated architectures such as convolutional
neural networks, and development revolves around the programming of neu-
ral network subroutines in Fortran before a Keras model can be imported.
A similar tool, MagmaDNN [15], has been developed in C++, with an em-
phasis on neural architectures. While the neural networks supported in this
package are much more extensive, oﬀ-nominal architectures (for instance gen-
erative models for density estimation) would require specialized development.
Another recent development for in-situ visualization and analysis has been
demonstrated for Nek5000 [16] where the SENSEI framework has been used
to couple simulations with VisIt [17]. While this capability allows for visu-
alization at extreme scales due to the excellent scaling of the higher-order
spectral methods implemented in Nek5000, the primary focus of VisIt is to
visualize data and to perform rudimentary analyses. Another popular soft-
ware for extreme scale in-situ visualizations and data-analyses is provided
by Paraview Catalyst [18]. However, Catalyst is predominantly set up as a
data visualization tool and does not possess advanced state-of-the-art analy-
sis techniques that may require deep learning technologies. We note that the
Python/C coupling proposed in this article has the ability to complement
Catalyst since the latter can be interfaced with NumPy arrays. To build on
the successes of existing technologies, the current authors have previously
demonstrated how the C API of TensorFlow may be utilized from within
OpenFOAM for in-situ surrogate modeling applications [19, 20]. While these
tools have removed the limitations of neural network development in C++ or
Fortran, and also allows for the utilization of a wider choice of data analysis
functions, the user is limited to data-driven analyses in TensorFlow alone.
Another example, PAR-RL, has been developed along the same lines to al-
low for deep reinforcement learning integration with OpenFOAM [21] but
relies on a ﬁlesystem-based information exchange between the reinforcement
learning agent and the numerical simulation being controlled. Therefore, in
this project, we demonstrate the usage of the Python/C++ interoperabil-
ity for tighter integration of data-science and computational ﬂuid dynamics
capabilities.

3

Our goal, through this research, is to address the inﬂexible nature of
data analysis tools for computational ﬂuid dynamics codes by using the
Python/C++ API for integrating data-science capability with to OpenFOAM.
These bindings may then be used for a broad range of data analyses in con-
currence with the simulation. For example, we shall demonstrate how one
may leverage NumPy [22] linear algebra capabilities for rapid deployment of
data analysis routines. This is achieved by enabling a data interface from
OpenFOAM data structures to functions that can handle NumPy arrays.
The end result is a pipeline that can allow for the application of arbitrary
functions on the simulation data, now represented as NumPy arrays. We
demonstrate our wrapper through the implementation of an online singular
value decomposition (SVD) [23] that is used to extract coherent structures
from the ﬂow ﬁeld without any storage I/O. We also demonstrate the cal-
culation of a distributed SVD using the approximate partitioned method of
snapshots [24] where data from multiple ranks is accessed by the MPI4PY
library in Python [25]. We outline results from our deployment on multiple
ranks of Intel Broadwell and Knights-Landing CPUs and assess strong scal-
ing for moderate sized problems. Finally, we also demonstrate how our data
science integration technique enables the use of state-of-the-art tools in ma-
chine learning such as TensorFlow [26] by including an example of nonlinear
compression using an autoencoder (a deep neural network with a bottleneck
architecture). Note that interoperability with other Python libraries such as
scikit-learn and PyTorch are also possible [27]. The experiments presented in
this paper assume a blocking usage of the data analyses routines in Python,
wherein the numerical solver is paused while the same resource is utilized for
the Python function execution. While this may be a limitation in terms of
the optimal usage of computational resources in certain scenarios, and will
be addressed in future augmentations to this work, we address the issue of
minimizing read and write operations to and from the disk.

2. PythonFOAM: Calling Python modules in OpenFOAM

2.1. Python embedding

In this section, we introduce how one can call Python from OpenFOAM.
We utilize the Python/C API 1 which conveniently allows for C++ code to

1https://docs.Python.org/3/c-api/intro.html

4

import Python modules with generic class objects and functions within them.
While the API is more commonly used for extending Python capabilities with
functions written in C or C++, it may also be used for calling Python within
a larger application, which is generally referred to as embedding Python in an
application. We note that there are alternative packages available for estab-
lishing the bridge between Python and C++ such as Pybind and Boost and
the overall coupling strategy we have demonstrated here may be replicated
with them as well.

In the following, we outline how one may embed Python in OpenFOAM.
Speciﬁcally, we highlight how a pre-existing solver (such as pimpleFOAM)
may be readily modiﬁed to exchange information with a Python interpreter.
The ﬁrst step in this process is for OpenFOAM to initialize a Python inter-
preter that must remain live for the entire duration of the simulation. This is
accomplished by using Py Initialize(). Following this, the Python inter-
preter may be interacted with from within OpenFOAM in a manner similar
to the command line using PyRun SimpleString(), where the argument to
this function is the Python code one wishes to execute. For instance, to
ensure that Python is able to discover modules in the current working di-
rectory, it is common to execute lines 3 and 4 in Listing 1. In practice, this
functionality by itself is insuﬃcient for arbitrary interaction of data, visu-
alization, and compute between C++ and Python. Therefore, one needs
to utilize Python modules and their functions to interact with OpenFOAM
data structures directly. Information about modules and function names are
stored in pointers to PyObject. Similarly, data that is sent to (or received
from) Python is stored in these pointers as well.

Lines 13 and 14 in Listing 1, for example, show a pointer pName that
stores the name of the module we wish to import (python module) and a
pointer pModule that stores the imported module respectively. Modules are
imported using PyUnicode DecodeFSDefault() with the name of Python
module as the argument (which should be present in the current working
directory). Here a module from a Python ﬁle, python module.py, present
in the OpenFOAM case directory will be loaded once at the start of the
solver. Similarly, line 17 details how PyObject GetAttrString() may be
used to import a function (in this case python func from python module).
Finally, any arguments that may be needed for calling must be stored in a
tuple as given in Line 20, before they may be passed through the API to
the Python interpreter. Before proceeding, we note that an additional com-
mand, import array1(-1), is utilized to initialize the ability to use NumPy

5

data structures from within OpenFOAM. NumPy allows for ﬂexible and ef-
ﬁcient data analysis within Python and can interface with a vast number of
specialized tools in the Python ecosystem. Py DECREF() is used for memory
deallocation after pName and pModule use is completed.

/* Initialize Python and add current directory to path */
Py_Initialize () ;
PyRun_SimpleString ( " import sys " ) ;
PyRun_SimpleString ( " sys . path . append (\".\") " ) ;

/* Python API datastructures */
PyObject * array_2d , * rank_val ;

/* initialize numpy array library */
import_array1 ( -1) ;

/* Import the " python_module . py " module from the current

directory */

PyObject * pName = P y U n ic o d e _ D e c o d e F S D e f a u l t ( " python_module " ) ;

// Python filename

PyObject * pModule = PyImport_Import ( pName ) ;

/* Import the function " python_func " */
PyObject * python_func = P yOb je ct _G etA tt rS tri ng ( pModule , "

python_func " ) ;

/* The tuple contains the items that need to be passed to the

" python_func " function */

PyObject * python_func_args = PyTuple_New (2) ;

/* Numpy datastructure to obtain return value from python

method */

PyArrayObject * pValue ;

/* Release memory */
Py_DECREF ( pName ) ;
Py_DECREF ( pModule ) ;
Listing 1: Enabling interoperability between C++ and Python. This listing details how
one may load arbitrary Python modules and functions from C++ for downstream use in
computation and is modiﬁed from the Python C/API manual.

Following the initialization of the Python interpreter and the loading
of modules and functions, Listing 2 outlines how one may pass data from
OpenFOAM to Python via the Python/C and NumPy/C APIs. Here we
show how a generic Python function may be called from OpenFOAM while

6

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

passing data from OpenFOAM to it, and how its return value may be stored
in an OpenFOAM compatible data structure. We ﬁrst retrieve data from
OpenFOAM’s volVectorField data structure and store it in an interme-
diate double precision array as shown between lines 1 and 9. Following
this a double precision NumPy array is created using this data using the
PyArray SimpleNewFromData command (lines 12-13). Note how NPY DOUBLE
is speciﬁed as the data type and the array has dimensions {num cells,1}
where the ﬁrst dimension refers to the number of degrees of freedom present
on this particular rank. Lines 15-20 use the PyTuple SetItem function to set
the arguments for calling a function. In this case, the ﬁrst argument is the
NumPy array that was just created and the second dimension is the integer
value of the current rank. The second argument is required for the purpose of
book-keeping from within the Python module. Finally, the function is called
with the speciﬁed tuple of arguments in line 23 using PyObject CallObject.
The return value is cast to a pointer to a PyArrayObject and stored in
pValue. Now we detail the process of receiving data from the Python func-
tions (usually in the form of NumPy arrays) and interfacing it with the Open-
FOAM data structure. This may be beneﬁcial for in-situ computations of
quantities that require Python packages, but are then utilized in the classical
partial diﬀerential equation computation of OpenFOAM. Examples include
the hybridized use of machine learning for bypassing one portion of the entire
numerical solution. Line 25 to 37 detail how one may use PyArray GETPTR2
to move data from the return value (a two-dimensional numpy array) to
OpenFOAM’s native data structure. This allows for preserving connectivity
information when writing the results of this analysis to disk. Thus, classical
visualization tools interfaced with OpenFOAM (such as Paraview) may be
used for visualizing Python results.

/* Extract double precision data from OpenFOAM datastructure

*/

volScalarField ux_ = U . component ( vector :: X ) ;

/* Storing data in double precision array for sending to

Python */

forAll ( ux_ . internalField () , id )
/* Storing velocities */
{
input_vals [ id ][0] = ux_ [ id ];

}

/* The dimensions of the numpy array to be created */

1

2

3

4

5

6

7

8

9

10

7

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

npy_intp dim [] = { num_cells , 1};
/* Pass data to numpy array */
array_2d = P y A r ra y _ S i m p l e N e w F r o m D a t a (2 , dim , NPY_DOUBLE , &

input_vals [0]) ;

/* Pass the array to the python function */
PyTuple_SetItem ( python_func_args , 0 , array_2d ) ;

/* Pass the rank to the function */
rank_val = PyLong_FromLong ( rank ) ;
PyTuple_SetItem ( python_func_args , 1 , rank_val ) ;

/* Call the function - pValue stores the return value of the

function */

pValue = ( PyArrayObject *) PyObject_CallObject ( python_func ,

python_func_args ) ;

/* Return data from Python into OpenFOAM datastructure -

allocate memory */

volScalarField out_ = U . component ( vector :: X ) ;

/* Load return value from Python into OpenFOAM data structure

*/

for ( int mode = 0; mode < truncation ; ++ mode )
{

/* Overwriting data */
forAll ( out_ . internalField () , id )
{

// Here we assume that pValue has a numpy array of

dimension 2 with 1 column only

out_ [ id ] = *(( double *) PyArray_GETPTR2 ( pValue , id , 0) )

;

}

}
Listing 2: Passing OpenFOAM data to NumPy through Python C-API and retrieving
return value. Here we assume the x-component of velocity is sent to Python function
which returns a pointwise computation

2.2. Compiling and linking

We brieﬂy go over how the compiling and linking to Python and NumPy
are performed using wmake, OpenFOAM’s build system. A sample conﬁgu-
ration ﬁle is shown in Listing 3 which shows how one must add the paths
to the various header ﬁles for the Python and Numpy APIs in the EXE INC

8

ﬁeld (lines 10 and 11). Similarly, paths to shared objects and link ﬂags must
also be provided in the EXE LIB ﬁeld (line 13 and 25). It is particularly care-
ful to avoid missing the right linking ﬂags as a new solver may be compiled
successfully but crash at runtime.

EXE_INC = \

- I$ ( LIB_SRC ) / M om e nt um T ra n sp o rt Mo d el s /
m om e nt um T ra n sp or t Mo d el s / lnInclude \
- I$ ( LIB_SRC ) / M om e nt um T ra n sp o rt Mo d el s / incompressible /
lnInclude \
- I$ ( LIB_SRC ) / transportModels / lnInclude \
- I$ ( LIB_SRC ) / finiteVolume / lnInclude \
- I$ ( LIB_SRC ) / sampling / lnInclude \
- I$ ( LIB_SRC ) / dynamicFvMesh / lnInclude \
- I$ ( LIB_SRC ) / dynamicMesh / lnInclude \
- I$ ( LIB_SRC ) / meshTools / lnInclude \
-I / p a t h _ t o _ p y t h o n _ v i r t u a l _ e n v i r o n m e n t / include / python3 .6 m /
\
-I / p a t h _ t o _ p y t h o n _ v i r t u a l _ e n v i r o n m e n t / lib / python3 .6/ site -
packages / numpy / core / include \

EXE_LIBS = \

-L / path_tolibpython / \
- l mo m e n tu m T r an s p o rt M o d el s \
- l i n c o m p r e s s i b l e M o m e n t u m T r a n s p o r t M o d e l s \
- l i n c o m p r e s s i b l e T r a n s p o r t M o d e l s \
- lfiniteVolume \
- lfvOptions \
- lsampling \
- ldynamicFvMesh \
- ltopoChangerFvMesh \
- ldynamicMesh \
- lmeshTools \
- lpython3 .6 m

Listing 3: OpenFOAM solver conﬁguration for linking to Python.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

3. Algorithms

3.1. Online Singular Value Decomposition

For demonstrating our tool, we will ﬁrst use Levy and Lindenbaums
method for performing a streaming SVD [23] in-situ. Our target applica-
tion is to use this SVD for analyzing the presence of coherent structures in
the ﬂow ﬁeld. Usually, this analysis is performed by constructing a data

9

matrix A ∈ RM ×N . N refers to the number of ‘snapshots’ of data collected
for the analysis and M is the number of degrees of freedom in each snapshot.
For the purpose of this analysis, M >> N and the regular SVD gives us

A = U ΣV T

(1)

where U ∈ RM ×N , Σ ∈ RN ×N , V ∈ RN ×N . The classical SVD computation
scales as O(M N 2) and requires O(M N ) memory. This analysis becomes
intractable for computational physics applications such as CFD where the
degrees of freedom may grow very large for simulating high wavenumber con-
tent. In their seminal paper, Levy and Lindenbaum proposed a streaming
variant of the SVD that reduces the computational and memory complexity
signiﬁcantly. It performs this by extracting solely the ﬁrst K left singular vec-
tors, which correspond to the K largest coherent structures. Consequently,
we are able to reduce the cost of the SVD to O(M N K) operations and the
memory footprint also reduces to O(M K). This technique also has a stream-
ing component to it, where the left singular eigenvectors may be updated in
a batch-like manner. We summarize the procedure in Algorithm 2 of the
Appendix. A Python and NumPy implementation for this algorithm and
how it may interface with OpenFOAM is shown in Listing 4. We remark
that the scalar forget factor f f (line 11), set between 0 and 1, controls the
eﬀect of older data batches on the ﬁnal result for Ui. Setting this value to
1.0 implies that the online-SVD convergences to the regular SVD utilizing
all the snapshots in one-shot. Setting values of f f less than one reduces the
impact of the snapshots observed in previous batches of the past. We utilize
an f f = 0.95 for this study. Speciﬁc examples that have been integrated
with a novel OpenFOAM solver are available in our supporting repository
https://github.com/argonne-lcf/PythonFOAM.

import numpy as np

class onli ne_s vd_ca lcul ator ( object ) :

"""
K : Number of modes to truncate
ff : Forget factor
"""
def __init__ ( self , K , ff ) :

super ( online_svd_calculator , self ) . __init__ ()
self . K = K
self . ff = ff

10

1

2

3

4

5

6

7

8

9

10

11

12

def initialize ( self , A ) :

# Algorithm 1 , Step I1
q , r = np . linalg . qr ( A )

# Algorithm 1 , Step I2
ui , self . di , _ = np . linalg . svd ( r )

self . ui = np . matmul (q , ui ) [: ,: self . K ]
self . di = self . di [: self . K ]

def incorporate_data ( self , A ) :

"""
A is the new data matrix
"""
# Algorithm 1 , Step 1
m_ap = self . ff * np . matmul ( self . ui , np . diag ( self . di ) )
m_ap = np . concatenate (( m_ap , A ) , axis = -1)
udashi , ddashi = np . linalg . qr ( m_ap )

# Algorithm 1 , Step 2
utildei , dtildei , vtildeti = np . linalg . svd ( ddashi )

# Algorithm 1 , Step 3
self . di = dtildei [: self . K ]
utildei = utildei [: ,: self . K ]

# Algorithm 1 , Step 4
self . ui = np . matmul ( udashi , utildei )

def snapshot_func ( array , rank ) :

global iter , u_snapshots , v_snapshots , w_snapshots

if iter == 0:

print ( ’ Collecting snapshots iteration : ’ , iter )

u_snapshots = array [: ,0]. reshape ( -1 ,1)
v_snapshots = array [: ,1]. reshape ( -1 ,1)
w_snapshots = array [: ,2]. reshape ( -1 ,1)

iter +=1

else :

print ( ’ Collecting snapshots iteration : ’ , iter )

u_temp = array [: ,0]. reshape ( -1 ,1)

11

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

v_temp = array [: ,1]. reshape ( -1 ,1)
w_temp = array [: ,2]. reshape ( -1 ,1)

# Preallocation is possible to avoid array

concatenation overhead

u_snapshots = np . concatenate (( u_snapshots , u_temp ) ,

axis = -1)

v_snapshots = np . concatenate (( v_snapshots , v_temp ) ,

axis = -1)

w_snapshots = np . concatenate (( w_snapshots , w_temp ) ,

axis = -1)

iter +=1

return 0

def svd_func ( rank ) :

global online_mode
global iter , u_snapshots , v_snapshots , w_snapshots
global init_mode , u_svd_calc , v_svd_calc , w_svd_calc

if init_mode :

print ( ’ Performing online SVD on snapshots rankwise -

initialization ’)

u_svd_calc . initialize ( u_snapshots )
v_svd_calc . initialize ( v_snapshots )
w_svd_calc . initialize ( w_snapshots )

init_mode = False

else :

u_svd_calc . incorporate_data ( u_snapshots )
v_svd_calc . incorporate_data ( v_snapshots )
w_svd_calc . incorporate_data ( w_snapshots )

u_modes = u_svd_calc . ui
v_modes = v_svd_calc . ui
w_modes = w_svd_calc . ui

print ( ’ Modal calculation completed ’)

u_snapshots = None
v_snapshots = None
w_snapshots = None
iter = 0

12

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

89

90

91

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

return_data = np . concatenate (( u_modes , v_modes , w_modes ) ,

axis =0)

return return_data

iter = 0
u_snapshots = None
v_snapshots = None
w_snapshots = None

online_mode = True
init_mode = True
u_svd_calc = onl ine_s vd_c alcul ator (5 ,0.95)
v_svd_calc = onl ine_s vd_c alcul ator (5 ,0.95)
w_svd_calc = onl ine_s vd_c alcul ator (5 ,0.95)
Listing 4: Python module for online SVD update according to Levy and Lindenbaum [23].

3.2. Distributed singular value decomposition

In this section, we will introduce the approximate partitioned method of
snapshots (APMOS) for computing distributed left singular vectors for our
provided test cases. Note that the primary diﬀerence from the Online SVD is
that this algorithm does not provide for a batch-wise update of the singular
vectors. Instead, each batch has its respective basis vector calculation which
is stored in an OpenFOAM compatible data structure to disk. While this
algorithm loses the ability to construct a set of bases for the entire dura-
tion of the simulation, its distributed nature allows for the construction of
a global basis even in the presence of a domain decomposition. This par-
allelized computation of the SVD was introduced in [24] and we recall its
main algorithm below. First, APMOS relies on the local calculation of the
left singular vectors for the data matrix on each rank of the simulation. To
construct this data matrix, snapshots of the local data may be collected over
multiple timesteps. Each row of this matrix corresponds to a particular grid
point and each column corresponds to a snapshot of data at one time instant.
The ﬁrst stage of local operations is thus

Ai = U iΣiV ∗i

(2)

where i refers to the index of the rank ranging from 1 to Nr (the total number
of ranks), U i ∈ RMi×N , Σi ∈ RN ×N , and V i ∈ RN ×N . Here, Mi refers to

13

the number of grid points in rank i of the distributed simulation. Note that
instead of an SVD, one may also perform a method of snapshots approach for
computing V i at each rank provided Mi >> N . A column-truncated subset
of the right singular vectors, ˜V i, and the singular values, ˜Σi, may then be
sent to one rank to perform the exchange of global information for computing
the POD basis vectors. This is obtained by collecting the following matrix
at rank 0 using the MPI gather command

W =

(cid:104) ˜V 1( ˜Σ1)T , ..., ˜V Nr( ˜ΣNr)T (cid:105)

.

(3)

In this study, we utilize a truncation factor r1 =50 columns of Vi and Σi
for broadcasting. Subsequently a singular value decomposition of W is per-
formed to obtain

W = XΛY ∗.

(4)

Given another threshold factor r2 corresponding to the number of columns
retained for X, a reduced matrix ˜X and reduced singular values ˜Λ is broad-
cast to all ranks. The distributed global left singular vectors may then be
assembled at each rank as follows for each basis vector j

˜U i

j =

Ai ˜Xj

1
˜Λj

(5)

where ˜U i
j is the jth singular vector in the ith rank, ˜Λj is the jth singular value
and ˜Xj is the jth column of the reduced matrix ˜X. In this study, we choose
r2 = 5 columns for our threshold factor for this last stage. We note that
the choices for r1 and r2 may be used to balance communication costs and
accuracy for this algorithm. Pseudocode 3, in the appendix, summarizes this
procedure. Listing 5 details the Python implementation for this algorithm
to interface with OpenFOAM.

import numpy as np
import mpi4py
mpi4py . rc . initialize = False
mpi4py . rc . finalize = False
from mpi4py import MPI

iter = 0
u_snapshots = None
v_snapshots = None

14

1

2

3

4

5

6

7

8

9

w_snapshots = None
num_modes = 5 # This should match truncation

def snapshot_func ( array , rank ) :

global iter , u_snapshots , v_snapshots , w_snapshots

if iter == 0:

print ( ’ Collecting snapshots iteration : ’ , iter )

u_snapshots = array [: ,0]. reshape ( -1 ,1)
v_snapshots = array [: ,1]. reshape ( -1 ,1)
w_snapshots = array [: ,2]. reshape ( -1 ,1)

iter +=1

else :

print ( ’ Collecting snapshots iteration : ’ , iter )

u_temp = array [: ,0]. reshape ( -1 ,1)
v_temp = array [: ,1]. reshape ( -1 ,1)
w_temp = array [: ,2]. reshape ( -1 ,1)

u_snapshots = np . concatenate (( u_snapshots , u_temp ) ,

axis = -1)

v_snapshots = np . concatenate (( v_snapshots , v_temp ) ,

axis = -1)

w_snapshots = np . concatenate (( w_snapshots , w_temp ) ,

axis = -1)

iter +=1

return 0

# Method of snapshots to accelerate
def g e n e r a t e _ r i g h t _ v e c t o r s _ m o s ( Y ) :

new_mat = np . matmul ( np . transpose ( Y ) ,Y )
w , v = np . linalg . eig ( new_mat )

svals = np . sqrt ( np . abs ( w [: rval ]) )

# Threshold r1
rval = 50

return v [: ,: rval ]. astype ( ’ double ’) , svals . astype ( ’ double ’

)

15

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

def apmos_func ( placeholder ) :

global iter , u_snapshots , v_snapshots , w_snapshots #

Iteration and local data

comm = MPI . COMM_WORLD
rank = comm . Get_rank ()
nprocs = comm . Get_size ()

snapshots_list = [ u_snapshots , v_snapshots , w_snapshots ]
phi_list = []

for local_data in snapshots_list :

local_data_mean = np . mean ( local_data , axis =1)
local_data = local_data - local_data_mean [: , None ]

# Run a method of snapshots
vlocal , slocal = g e n e r a t e _ r i g h t _ v e c t o r s _ m o s (

local_data )

# Find W
wlocal = np . matmul ( vlocal , np . diag ( slocal ) . T )

# Gather data at rank 0:
wglobal = comm . gather ( wlocal , root =0)

# perform SVD at rank 0:
if rank == 0:

temp = wglobal [0]
for i in range ( nprocs -1) :

temp = np . concatenate (( temp , wglobal [ i +1]) ,

axis = -1)

wglobal = temp

x , s , y = np . linalg . svd ( wglobal )

# Truncation threshold r2
rval = num_modes

x = x [: ,: rval ]
s = s [: rval ]

else :

x = None
s = None

16

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

89

90

91

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

x = comm . bcast (x , root =0)
s = comm . bcast (s , root =0)

# perform singular vector calculation at each local

rank

phi_local = []
for mode in range ( rval ) :

phi_temp = 1.0/ s [ mode ]* np . matmul ( local_data , x [: ,

mode : mode +1])

phi_local . append ( phi_temp )

temp = phi_local [0]
for i in range ( rval -1) :

temp = np . concatenate (( temp , phi_local [ i +1]) , axis

= -1)

phi_list . append ( temp )

# Clean memory
u_snapshots = None
v_snapshots = None
w_snapshots = None

iter = 0
return_data = np . concatenate (( phi_list [0] , phi_list [1] ,

phi_list [2]) , axis =0)
return return_data

Listing 5: Python module for approximate partitioned method of snapshots for left singular
vector computation [24].

3.3. Nonlinear compression using deep autoencoder

Now we demonstrate an application where a deep learning architecture
is utilized to ﬁnd a nonlinear low-dimensional embedding from several snap-
shots of transient data. Autoencoders have been successfully used for var-
ious reduced-order modeling, data-compression, and data exploration prob-
lems [28, 29, 30, 31, 32]. Classical workﬂows for training autoencoders are
similar to performing singular value decompositions, and involve storing sev-
eral snapshots to disk at predeﬁned temporal checkpoints. Subsequently, a
separate computational workﬂow is executed to train the deep learning ar-
chitecture, usually on specialty hardware. In this study, we deploy a deep

17

Figure 1: A representative schematic of the deep neural network autoencoder. A bottle-
neck architecture is used while learning an identity function to obtain a low-dimensional
embedding of the ﬂow-ﬁeld.

neural network autoencoder to use a snapshot from each iteration of the Pim-
pleFOAM solver and obtain a latent space representation. A representative
schematic of the fully-connected autoencoder architecture is shown in Figure
1. We use the Swish activation function at each layer, a batch size of 128
snapshots, the ADAM optimizer with a learning rate of 0.001, and more im-
portantly, a bottleneck width of 4 neurons for this architecture. This means
that the trained encoder of the autoencoder can compress the ﬂow-ﬁeld to a
four-dimensional state and reconstruct from the same. A TensorFlow model
deﬁnition of the autoencoder is also given in Listing 6. We clarify that the
implementation of the deep neural network autoencoder is performed on the
same resource as that used for the numerical solve and at periodic inter-
vals when adequate training data has been collected. Also, while concurrent
training of autoencoder and data collection is possible, we leave that to a
future implementation.

# Encoder
encoder_inputs = Input ( shape =( num_points * num_fields ) , name = ’

Field ’)

x = Dense (50 , activation = my_swish ) ( encoder_inputs )
x = Dense (25 , activation = my_swish ) ( x )

1

2

3

4

5

18

x = Dense (10 , activation = my_swish ) ( x )
encoded = Dense ( self . num_latent ) ( x )
self . encoder = Model ( inputs = encoder_inputs , outputs = encoded )

# Decoder
decoder_inputs = Input ( shape =( self . num_latent ,) , name = ’ decoded

’)

x = Dense (10 , activation = my_swish ) ( decoder_inputs )
x = Dense (25 , activation = my_swish ) ( x )
x = Dense (50 , activation = my_swish ) ( x )
decoded = Dense ( num_points * num_fields ) ( x )

self . decoder = Model ( inputs = decoder_inputs , outputs = decoded )

# Autoencoder
ae_outputs = self . decoder ( self . encoder ( encoder_inputs ) )
self . model = Model ( inputs = encoder_inputs , outputs = ae_outputs ,

name = ’ Autoencoder ’)

weights_filepath = ’ weights . h5 ’
my_adam = optimizers . Adam ( lr =0.001 , beta_1 =0.9 , beta_2 =0.999 ,

epsilon = None , decay =0.0 , amsgrad = False )

checkpoint = ModelCheckpoint ( weights_filepath , monitor = ’

val_loss ’ , verbose =1 , save_best_only = True , mode = ’ min ’ ,
save_weights_only = True )

earlystopping = EarlyStopping ( monitor = ’ val_loss ’ , min_delta
=0 , patience =100 , verbose =0 , mode = ’ auto ’ , baseline = None ,
restore_best_weights = True )

callbacks_list = [ checkpoint , earlystopping ]

# Compile network
self . model . compile ( optimizer = my_adam , loss = ’ mean_squared_error

’)

self . model . summary ()
Listing 6: A deep neural network autoencoder used for compressing ﬂow-ﬁeld information

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

4. Experiments

In this section, we shall outline several experiments that demonstrate the
utility of our Python bindings to OpenFOAM. We shall discuss results of an
in-situ data analysis using the previously introduced online-SVD, distributed
SVD, and the deep learning autoencoder. Scaling analyses on diﬀerent ar-
chitectures will also be provided for the distributed SVD.

19

All our experiments are performed with the PimpleFOAM solver in Open-
FOAM. PimpleFOAM is an unsteady incompresible solver which allows for
turbulence-scale resolving Large Eddy simulations (LES), Reynolds average
Navier Stokes (RANS) simulations or hybrid RANS/LES simulations. The
solver also uses a merged PISO (Pressure Implicit with Splitting of Oper-
ator) and SIMPLE (Semi-Implicit Method for Pressure-Linked Equations)
algorithm (PIMPLE) for the velocity-pressure coupling for which an inner
PISO iteration is performed to get an initial solution which is then corrected
using an outer SIMPLE iteration. The PIMPLE algorithm provides for en-
hanced stability of the solver.
In particular, we are able to utilize larger
time-steps and, hence, larger Courant numbers that are greater than unity.
This is an advantage over the PISO algorithm which is restricted by a sta-
ble Courant number criterion of less then unity [33]. At the end of each
PimpleFOAM solver update, our Python bindings are called to either send
snapshots to a NumPy array for collection or to perform a data analysis
operation (such as an SVD computation or a neural network training). For
timesteps during which the analyses are performed, postprocessed data are
also returned to OpenFOAM which uses its native I/O operations to write
to disk. This enables the use of Paraview for visualization as is usual.We
note that our implementation is independent of the numerical discretization
or solver algorithm. An overall workﬂow for our deployments is as follows:

4.1. LES of backward facing step

Our ﬁrst experiment is that of the large-eddy simulation (LES) of a two-
dimensional backward facing step using the standard Smagorinsky implemen-
tation in OpenFOAM. We note that the two-dimensional assessment does not
truly qualify as a physical case (since LES requires a three-dimensional do-
main). However, for this assessment, we merely wish to perform an in-situ
data analysis using the online-SVD and compare with the instantaneous ﬂow
features obtained from the numerical methods. The purpose of this compu-
tational assessment to conﬁrm that the online-SVD is able to detect coherent
structures in the ﬂow ﬁeld. The left singular vectors of this SVD correspond
to structures in the ﬂow ﬁeld where there are concentrations of kinetic energy.
This can be seen by comparing the instantaneous velocity contours of the x
and y components of the velocity and the structures observed when overlap-
ping the singular vectors on to the computational mesh. The singular vectors
for both u and v show structures emanating downstream of the step where
shedding and recirculation are present. These singular vectors may also be

20

Algorithm 1: A prototypical PythonFOAM coupling workﬂow for
the various experiments in this article.

Result: Postprocessed data (Singular vectors, autoencoder

reconstructions)

Initialize solver, Python/C API coupling;
while While not ﬁnal time do
Perform solver integration;
Cast OpenFOAM data to NumPy array;
if Data analysis checkpoint then

Retrieve data from NumPy array;
Perform data analyses (deep learning/SVD);
Send postprocessed data back to OpenFOAM;
Write data to disk using OpenFOAM I/O;

else

Collect data in NumPy arrays;

end

end

used to represent the ﬂow ﬁeld in a low-dimensional form through forming a
subspace from a limited number of orthonormal basis vectors. However, that
study is not explored in this work. Figure 2 shows instantaneous snapshots
for the x and y components of velocity for the backward facing step exhibit-
ing unsteady separation and shedding behavior downstream of the step. This
behavior is successfully captured in the singular vectors shown in Figures 3
and 4. Coherent structures in the singular vectors of the y component of the
velocity depict the oscillatory nature of the shedding as well.

Figure 5 shows a visualization indicating how the backward facing step
was distributed across 4 ranks and how APMOS is able to approximate the
global left singular vectors eﬀectively. Figure 6 outlines the second singular
vectors obtained using APMOS. In this experiment, snapshots were collected
and utilized for the distributed SVD every 2000 iterations of PimpleFOAM.
One can observe that the singular vectors correspond to the coherent struc-
tures in the ﬂow-ﬁeld in a continuous sense, despite the distributed nature
of the computation. We remark here that diﬀerences may be observed with
the online-SVD results from the previous set of experiments which may be
attributed to factors such as the additional level of truncation employed in

21

Figure 2: Dimensionless instantaneous streamwise and vertical velocities u and v at three
characteristic time instances.

Figure 3: First singular vectors for u and v at three characteristic time instances.

22

instantaneous u instantaneous v t = 0.028 sect = 0.047 sect = 0.047 sect = 0.071 sect = 0.071 sec0.0-2230-1010t = 0.028 sec2040m/su 1st modev 1st modet = 0.028 sect = 0.028 sect = 0.047 sect = 0.047 sect = 0.071 sect = 0.071 sec0.0-2.52.5-1.251.2510-2Figure 4: Second singular vectors for u and v at three characteristic time instances.

the global communication of local right singular vectors (see step I2 and I3
in Algorithm 3). However, the basis vectors obtained through APMOS ad-
equately reveal coherent structures in the ﬂow and are orthonormal, which
allows for their use in downstream tasks such as projection-based modeling
or analyses. An important capability is highlighted here - the utilization of
one MPI communicator across two languages C++ and Python. While the
MPI communication is initiated in OpenFOAM, the Python MPI4PY library
is able to send and receive NumPy array data between Python interpreters
residing at diﬀerent ranks. This enables possibilities for the use of distributed
algorithms such as data-parallel machine learning in future extensions.

Subsequently, we analyse the scaling of our distributed SVD in the fol-
lowing. We evaluate computational eﬃciency for two diﬀerent processing ar-
chitectures which are commonly available in modern Petascale systems. Our
computational set up is given by a well-known OpenFOAM tutorial case:
the large eddy simulation of a turbulent channel ﬂow at friction Reynolds
number 395 using the WALE turbulence model. We perform strong scal-
ing assessments on a grid with 3.84 million degrees of freedom on 1, 4, 8,
16, 32, 64, and 128 compute nodes of Bebop at the Laboratory Comput-
ing Resource Center (LCRC) of Argonne National Laboratory. Cells were

23

u 2nd modev 2nd  modet = 0.028 sect = 0.028 sect = 0.047 sect = 0.047 sect = 0.071 sect = 0.071 sec0.0-2.52.5-1.251.2510-2Figure 5: First singular vectors for u and v at three characteristic time instances using
the APMOS method. Note how information from 4 ranks is used to construct the global
basis vectors.

24

u 1st modev 1st modet = 0.028 sect = 0.028 sect = 0.046 sect = 0.046 sect = 0.070 sect = 0.070 sec0.02.52.51.251.2510-2Figure 6: Second singular vectors for u and v at three characteristic time instances using
the APMOS method. Note how information from 4 ranks is used to construct the global
basis vectors.

split across ranks using the Scotch decomposition technique [34] to ensure
appropriate load-balancing. Our cell-counts for distributed experiments were
approximately 960,000 cells per rank for a 4 rank simulation, 480,000 cells
per rank of a 8 rank simulation, 240,000 cells per rank for a 16 rank simu-
lation, 120,000 cells per rank for a 32 rank simulation, 60,000 cells per rank
for a 64 rank simulation, and 30,000 cells per rank for a 128 rank simulation.
As mentioned previously, these experiments are for two diﬀerent computer
architectures: Intel’s Knights Landing (KNL) and Broadwell (BDW). Each
BDW node (Xeon E5-2695v4) comes with 36 cores, 128 GB of DDR4 memory
and maximum memory bandwidth of 76.8 GB/s [35]. BDW also supports
the Advanced Vector Extensions (AVX2) instruction set which is capable
of 256-bit wide SIMD operations. In contrast, each KNL node (Xeon Phi
7230) has almost twice the core-count with 64 cores and 96 GB of DDR4
memory. The KNL comes with the AVX-512 instruction set which has 512-
bit wide SIMD operations. Each KNL also comes with MCDRAM which
is a high-bandwidth memory that is integrated on-package with a total of
16 GB space; for the STREAM TRIAD benchmark, the bandwidth coming

25

u 2nd modev2ndmodet = 0.028 sect = 0.028 sect = 0.046 sect = 0.046 sect = 0.070 sect = 0.070 sec0.02.52.51.251.2510-2from the MCDRAM has been reported to be over 450 GB/s [36].

The total computational cost at each time step may be decomposed into
several sub-costs as outlined in the following. We have costs associated with
the partial diﬀerential equation update (i.e., the PimpleFOAM solver up-
date), the time taken to record snapshots, the time taken to perform the
distributed SVD, the time taken to copy singular vector data into Open-
FOAM data structures, the I/O time to write out the ﬂow ﬁeld information,
and the time required to cast data from the OpenFOAM data structures
to NumPy arrays. Note that OpenFOAM function objects allow for runtime
and post-hoc data analyses without typecasts but we choose to construct our
wrapper through the pathway of a novel solver construction for the ease of
downstream modiﬁcation for arbitrary tasks. In principle, function objects
that utilize Python/C coupling via NumPy arrays may be constructed and
used in concurrence with the demonstrated procedure. Averaged times for
the aforementioned operations are recorded over the duration of a simulation
across multiple ranks and shown in addition to strong scaling plots. We note
that SVD computation, data copy, and IO computation times are obtained
by averaging across only ﬁve such instances for each simulation. In contrast,
other computations such as the cost for PDE solution update, snapshot col-
lection, and NumPy casting were averaged across all the time steps for each
simulation.

Strong scaling results for the distributed SVD are presented in Figures
7 and 8 for the Broadwell and KNL architectures respectively. The x-axis
in these plots shows the ranks while the y-axis indicates the time required
to perform a given operation (e.g. PimpleFOAM compute time, SVD time,
etc.). The cost breakdowns provided with each plot show how computa-
tion associated with the numerical solver signiﬁcantly dominates other costs
(since this cost is incurred at each time step of the numerical simulation). It
must be noted here that for utilizations of the OpenFOAM-Python coupling
where data must be sent and received at frequent intervals, the data copy
time may prove to be a potential bottleneck that adds costs on the order of
the PDE-compute itself. An example is the use of a machine learning algo-
rithm that predicts a transient ﬂow-ﬁeld quantity. This must be accounted
for prior to designing data-driven solutions to classical problems in scien-
tiﬁc computing such as for deep learning surrogates for turbulence modeling.
We also remark that, for the implementation tested here, performance on
Broadwell nodes is remarkably faster than Knights-Landing. However, we
note that several optimization strategies may be deployed for the latter that

26

(a) Cost per timestep

(b) Cost per 2000 timesteps

(c) Scaling

Figure 7: Scaling analyses of PythonFOAM for an in-situ distributed singular value de-
composition on multiple ranks of the LCRC machine Bebop (with the Intel Broadwell
architecture). Figure (a) shows the breakdown of costs incurred at each time step. Figure
(b) shows the breakdown of costs incurred every 2000 timesteps. Figure (c) shows strong
scaling for the total time to solution.

can improve performance considerably.

4.2. Deep learning autoencoder

Our ﬁnal set of experiments demonstrates the use of a deep neural net-
work autoencoder for compressing ﬂow-ﬁeld information without having to
checkpoint data to disk. For this experiment, we use snapshots of the
x−component of velocity that are collected into sets of 400 numerical itera-
tions and subsequently used to train the deep neural network. The trained
network weights are saved to disk at the end of 400 iterations and the learned
model is subsequently used to obtain low-dimensional representations of the

27

(a) Cost per timestep

(b) Cost per 2000 timesteps

(c) Scaling

Figure 8: Scaling analyses of PythonFOAM for an in-situ distributed singular value decom-
position on multiple ranks of the LCRC machine Bebop (with the Intel Knights-Landing
architecture). Figure (a) shows the breakdown of costs incurred at each time step. Figure
(b) shows the breakdown of costs incurred every 2000 timesteps. Figure (c) shows strong
scaling for the total time to solution.

28

Figure 9: Evolution of backward facing step in encoded coordinates as obtained by training
several deep neural network autoencoders at intervals of 400 iterations (left) and histogram
of reconstruction snapshot mean squared errors (right). Note that a new set of coordinates
are obtained at the end of each 400 iteration period of full-order snapshot collection.

ﬂow-ﬁeld snapshots for the next set of 400 iterations (which are also col-
lected for training the next autoencoder). We remark here, for clarity, that a
unique autoencoder is trained at the end of each collection of 400 snapshots.
In practice, algorithmic augmentations could be added such as carrying over
the previous best model and retraining through transfer learning. The evolu-
tion of our backward facing step (introduced in Section 4.1), in the encoded
coordinates, is shown in Figure 9. We remark here that the discontinuities at
the end of each 400-iteration window correspond to a novel set of coordinates
having been obtained by a new network training. These low-dimensional
representations may then be used to reconstruct, approximately, the high di-
mensional ﬂow ﬁeld using the decoder network of the appropriately trained
autoencoder. Figure 9 also shows the reconstruction root-mean-squared er-
rors for the various test snapshots. While the original magnitude of the
ﬂow-ﬁeld is comparable to the inlet velocity (i.e., 32.2 m/s), these errors are
an order of magnitude lower. Examples of the CFD ﬂow ﬁelds obtained by
the numerical solver for this experiment are shown in Figure 10 and their
corresponding reconstructions from the low-dimensional embedding can be
seen in Figure 11. We note that these reconstructions are approximate - and
retrieved from a nonlinear transformation of the low-dimensional embedding
using the decoder of the autoencoder deep neural network.

29

Figure 10: Streamwise velocity component u, obtained using numerical solver Pimple-
FOAM.

30

Figure 11: Streamwise velocity component u, obtained using deep neural network autoen-
coder reconstructions from low-dimensional embedding in TensorFlow.

31

5. Conclusions and future work

This article outlines results, and provides source codes, for an extension of
OpenFOAM that allows for an arbitrary interface with Python. Our interface
is built on the Python/C API and allows for the utilization of Python mod-
ules, class objects, and functions in addition to the exchange of data between
OpenFOAM and Python via the NumPy/C API. This tool can therefore pre-
cipitate a wide variety of in-situ data analytics, visualization, and machine
learning for computational ﬂuid dynamics applications within a reproducible
open source environment.

Our interface is demonstrated through the deployment of a streaming
singular value decomposition on a transient canonical backward facing step
problem that exhibits shedding and separation downstream of the step. The
singular vectors obtained from the streaming approach can be used to detect
the presence of coherent structures for similar problems. Our example shows
how one may extract these singular vectors by calling a snapshot record
function and a singular value decomposition function during the deployment
of a transient OpenFOAM solver (pimpleFOAM). The singular vectors may
then be stored within OpenFOAM itself to preserve connectivity informa-
tion and to utilize integrated visualization with Paraview. Furthermore, we
also deploy the proposed wrapper on a canonical channel ﬂow problem at
Reτ = 395 to assess the parallel eﬃciency of the OpenFOAM/Python in-
terface. We do this on diﬀerent computing architectures for a distributed
SVD based on the approximate partitioned method of snapshots. Scaling
analyses of these experiments indicate scaling eﬃciency in the presence of
distributed data analyses may be preserved if the computational cost of the
solver dominates that of the data storage and typecast costs at each time
step. This has implications for the use of distributed data science frame-
works in concurrence with PDE solvers at extreme scales. Finally, we also
demonstrate a state-of-the-art use case where a deep neural network with a
bottleneck architecture is used to learn low-dimensional representations of
the ﬂow-ﬁeld. This network, an autoencoder, is used to track the evolution
of dynamics on the embedding coordinates and provide approximate recon-
structions from this space on demand through decoding. This proves that
arbitrary deep learning tasks may be deployed within OpenFOAM using the
Python API of TensorFlow.

Our extensions to this library shall investigate asynchronous couplings
with specialized computational resources to mitigate the aforementioned in-

32

situ data analytics bottleneck at extreme scales. The series of experiments
in this article ﬁrmly fall within the umbrella of tightly coupled analyses,
where PDE computation and analyses are performed on the same computa-
tional resource in a blocking procedure. In other words, until data analysis
is complete, PDE compute is halted and vice versa. Our future goals are
to loosen this coupling on heterogeneous HPC systems, where data anal-
ysis may be performed on devoted accelerators while PDE computation is
performed on simulation-friendly devices. An asynchronous interface (i.e.,
a ‘loose-coupling’) between the two would allow for data analysis and sim-
ulation to be performed concurrently and for improvements in scaling char-
acteristics. Our current and future investigations are structured along the
aforementioned verticals.

6. CRediT author statement

Romit Maulik: Conceptualization, methodology, software, formal analy-
sis, writing - original draft, writing - reviewing & editing. Dimitrios K. Fy-
tanidis: analysis, visualization, reviewing & editing, Bethany Lusch: method-
ology, supervision, analysis, Venkatram Vishwanath: methodology, supervi-
sion, analysis, Saumil Patel: analysis.

Acknowledgements

The authors thank Zhu Wang and Traian Iliescu for helpful discussions re-
lated to the distributed singular value decomposition. This material is based
upon work supported by the U.S. Department of Energy (DOE), Oﬃce of
Science, Oﬃce of Advanced Scientiﬁc Computing Research, under Contract
DE-AC02-06CH11357. This research was funded in part and used resources
of the Argonne Leadership Computing Facility, which is a DOE Oﬃce of
Science User Facility supported under Contract DE-AC02-06CH11357. RM
acknowledges support from the Margaret Butler Fellowship at the Argonne
Leadership Computing Facility. This paper describes objective technical re-
sults and analysis. Any subjective views or opinions that might be expressed
in the paper do not necessarily represent the views of the U.S. DOE or the
United States Government.

The submitted manuscript has been created by UChicago Argonne, LLC,
Operator of Argonne National Laboratory (“Argonne”). Argonne, a

33

is operated
U.S. Department of Energy Oﬃce of Science laboratory,
under Contract No. DE-AC02-06CH11357. The U.S. Government re-
tains for itself, and others acting on its behalf, a paid-up nonexclu-
sive, irrevocable worldwide license in said article to reproduce, prepare
derivative works, distribute copies to the public, and perform publicly
and display publicly, by or on behalf of the Government. The Depart-
ment of Energy will provide public access to these results of federally
sponsored research in accordance with the DOE Public Access Plan
(http://energy.gov/downloads/doe-public-access-plan).

References

[1] S. Klasky, H. Abbasi, J. Logan, M. Parashar, K. Schwan, A. Shoshani,
M. Wolf, S. Ahern, I. Altintas, W. Bethel, et al., In situ data processing
for extreme-scale computing, Scientiﬁc Discovery through Advanced
Computing Program (SciDAC’11) (2011) 1–16.

[2] H. G. Weller, G. Tabor, H. Jasak, C. Fureby, A tensorial approach to
computational continuum mechanics using object-oriented techniques,
Computers in physics 12 (1998) 620–631.

[3] P. Higuera, J. L. Lara, I. J. Losada, Simulating coastal engineering
processes with OpenFOAM®, Coastal Engineering 71 (2013) 119–134.

[4] L. Chen, J. Zang, A. J. Hillis, G. C. Morgan, A. R. Plummer, Numerical
investigation of wave–structure interaction using OpenFOAM, Ocean
Engineering 88 (2014) 91–109.

[5] C. Kunkelmann, P. Stephan, Cfd simulation of boiling ﬂows using the
volume-of-ﬂuid method within OpenFOAM, Numerical Heat Transfer,
Part A: Applications 56 (2009) 631–646.

[6] D. A. Lysenko, I. S. Ertesv˚ag, K. E. Rian, Modeling of turbulent sepa-
rated ﬂows using OpenFOAM, Computers & Fluids 80 (2013) 408–422.

[7] R. E. Bensow, G. Bark, Simulating cavitating ﬂows with LES in Open-
in: V European conference on computational ﬂuid dynamics,

Foam,
2010, pp. 14–17.

34

[8] J. Favero, A. Secchi, N. Cardozo, H. Jasak, Viscoelastic ﬂow analysis
using the software OpenFOAM and diﬀerential constitutive equations,
Journal of non-newtonian ﬂuid mechanics 165 (2010) 1625–1636.

[9] P. He, C. A. Mader, J. R. Martins, K. J. Maki, An aerodynamic design
optimization framework using a discrete adjoint approach with Open-
FOAM, Computers & Fluids 168 (2018) 285–303.

[10] C. White, M. K. Borg, T. J. Scanlon, S. M. Longshaw, B. John, D. R.
Emerson, J. M. Reese, dsmcfoam+: An openfoam based direct simula-
tion monte carlo solver, Computer Physics Communications 224 (2018)
22–43.

[11] G. R. Tabor, M. Baba-Ahmadi, Inlet conditions for large eddy simula-

tion: A review, Computers & Fluids 39 (2010) 553–567.

[12] E. Laurila, J. Roenby, V. Maakala, P. Peltonen, H. Kahila, V. Vuorinen,
Analysis of viscous ﬂuid ﬂow in a pressure-swirl atomizer using large-
eddy simulation, International Journal of Multiphase Flow 113 (2019)
371–388.

[13] N. Geneva, N. Zabaras, Quantifying model

form uncertainty in
Reynolds-averaged turbulence models with Bayesian deep neural net-
works, Journal of Computational Physics 383 (2019) 125–147.

[14] J. Ott, M. Pritchard, N. Best, E. Linstead, M. Curcic, P. Baldi,
A fortran-keras deep learning bridge for scientiﬁc computing, arXiv
preprint arXiv:2004.10652 (2020).

[15] D. Nichols, N.-S. Tomov, F. Betancourt, S. Tomov, K. Wong, J. Don-
garra, Magmadnn: towards high-performance data analytics and ma-
chine learning for data-driven scientiﬁc computing,
in: International
Conference on High Performance Computing, Springer, 2019, pp. 490–
503.

[16] B. Bernardoni, N. Ferrier, J. Insley, M. E. Papka, S. Patel, S. Rizzi,
In situ visualization and analysis to design large scale experiments in
computational ﬂuid dynamics, in: 2018 IEEE 8th Symposium on Large
Data Analysis and Visualization (LDAV), IEEE, 2018, pp. 94–95.

35

[17] H. Childs, VisIt: An end-user tool for visualizing and analyzing very

large data (2012).

[18] U. Ayachit, A. Bauer, B. Geveci, P. O’Leary, K. Moreland, N. Fabian,
J. Mauldin, Paraview catalyst: Enabling in situ data analysis and visual-
ization, in: Proceedings of the First Workshop on In Situ Infrastructures
for Enabling Extreme-Scale Analysis and Visualization, 2015, pp. 25–29.

[19] R. Maulik, H. Sharma, S. Patel, B. Lusch, E. Jennings, A turbulent
eddy-viscosity surrogate modeling framework for Reynolds-Averaged
Navier-Stokes simulations, Computers & Fluids (2020) 104777.

[20] R. Maulik, H. Sharma, S. Patel, B. Lusch, E. Jennings, Deploying deep
learning in OpenFOAM with tensorﬂow, in: AIAA Scitech 2021 Forum,
2021, p. 1485.

[21] S. Pawar, R. Maulik, Distributed deep reinforcement learning for sim-
ulation control, Machine Learning: Science and Technology (in-press)
(2021).

[22] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Vir-
tanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith,
R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Hal-
dane, J. F. del R’ıo, M. Wiebe, P. Peterson, P. G’erard-Marchant,
K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, T. E.
Oliphant, Array programming with NumPy, Nature 585 (2020) 357–362.
URL: https://doi.org/10.1038/s41586-020-2649-2. doi:10.1038/
s41586-020-2649-2.

[23] A. Levy, M. Lindenbaum, Sequential Karhunen-Loeve basis extraction
and its application to images, in: Proceedings 1998 International Con-
ference on Image Processing. ICIP98 (Cat. No. 98CB36269), volume 2,
IEEE, 1998, pp. 456–460.

[24] Z. Wang, B. McBee, T. Iliescu, Approximate partitioned method of
snapshots for POD, Journal of Computational and Applied Mathematics
307 (2016) 374–384.

[25] L. Dalc´ın, R. Paz, M. Storti, MPI for Python, Journal of Parallel and

Distributed Computing 65 (2005) 1108–1115.

36

[26] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,
G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Good-
fellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Mur-
ray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-
war, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, Tensor-
Flow: Large-scale machine learning on heterogeneous systems, 2015.
URL: https://www.tensorflow.org/, software available from tensor-
ﬂow.org.

[27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high-
performance deep learning library,
in: H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, R. Garnett (Eds.), Advances
in Neural Information Processing Systems 32, Curran Associates,
Inc., 2019, pp. 8024–8035. URL: http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.

[28] R. Maulik, B. Lusch, P. Balaprakash, Reduced-order modeling of
advection-dominated systems with recurrent neural networks and con-
volutional autoencoders, Physics of Fluids 33 (2021) 037106.

[29] K. Hasegawa, K. Fukami, T. Murata, K. Fukagata, Cnn-lstm based
reduced order modeling of two-dimensional unsteady ﬂows around a cir-
cular cylinder at diﬀerent reynolds numbers, Fluid Dynamics Research
52 (2020) 065501.

[30] T. Murata, K. Fukami, K. Fukagata, Nonlinear mode decomposition
with convolutional neural networks for ﬂuid dynamics, Journal of Fluid
Mechanics 882 (2020).

[31] T. Nakamura, K. Fukami, K. Hasegawa, Y. Nabae, K. Fukagata, Con-
volutional neural network and long short-term memory based reduced
order surrogate for minimal turbulent channel ﬂow, Physics of Fluids
33 (2021) 025116.

37

[32] F. J. Gonzalez, M. Balajewicz, Deep convolutional recurrent autoen-
coders for learning low-dimensional feature dynamics of ﬂuid systems,
arXiv preprint arXiv:1808.01346 (2018).

[33] T. Holzmann, Mathematics, numerics, derivations and OpenFOAM®,

Loeben, Germany: Holzmann CFD (2016).

[34] F. Pellegrini, J. Roman, Scotch: A software package for static map-
ping by dual recursive bipartitioning of process and architecture graphs,
in: International Conference on High-Performance Computing and Net-
working, Springer, 1996, pp. 493–498.

[35] Intel

xeon

processor

e5-2695

v4,

https://ark.

intel.com/content/www/us/en/ark/products/91316/
intel-xeon-processor-e5-2695-v4-45m-cache-2-10-ghz.html,
2021. Accessed:2021-01-31.

[36] J. Jeﬀers, J. Reinders, A. Sodani, Intel Xeon Phi processor high per-
formance programming: Knights Landing edition, Morgan Kaufmann,
2016.

38

Appendix A

Algorithm 2: Streaming singular value decomposition [23].

Result: The truncated left singular vector matrix Ui after i batch

iterations.

Parameters:
Forget factor f f ;
Initialization:
Initial data matrix A0 ∈ RM ×B where B is the number of snapshots
per batch;
I1. Perform QR-decomposition : A0 = QR ;
I2. Perform SVD of R = U (cid:48)D0V T
while New data Ai available do

0 and obtain U0 = QU (cid:48) ;

1. Compute QR decomposition after column-wise concatenation
of new data: [f f · Ui−1Di−1 | Ai] = U (cid:48)
˜Di−1
i−1 = ˜Ui−1
2. Compute SVD of D(cid:48)
3. Preserve the ﬁrst K columns of ˜Ui−1 and denote ˆUi−1 ;
4. Obtain the updated left singular vectors: Ui = U (cid:48)
5. Truncate to retain K values of ˜Di−1 to obtain Di ;

i−1D(cid:48)
˜V T ;

ˆUi−1 ;

i−1;

i−1

end

Algorithm 3: Distributed singular value decomposition [24]

Result: The truncated left singular vector matrix ˜U i in each rank i

of a distributed computation

Parameters:
Threshold factors r1 and r2;
Algorithm:
Local data matrix Ai at each rank i of distributed simulation;
1. Perform local SVD or method of snapshots calculation of local
right singular vectors : Ai = U iΣiV ∗i. ;
2. Truncate V i, Σi by retaining only r1 columns to obtain ˜V i and ˜Σi
respectively. ;

3. Obtain W =

(cid:104) ˜V 1( ˜Σ1)T , ..., ˜V Nr( ˜ΣNr)T (cid:105)

at rank 0 using MPI

Gather.;
4. Perform SVD, W = XΛY ∗ at rank 0.;
5. Truncate X, Λ by retaining only r2 columns to obtain ˜X, ˜Λ
respectively.;
6. Send ˜X, ˜Λ to each rank using MPI Broadcast.;
7. Obtain local partition of jth global left singular vector
˜U i
j = 1
˜Λj
matrices.

Ai ˜Xj where j corresponds to the column of the respective

39

.

