1
2
0
2

t
c
O
0
3

]

V
C
.
s
c
[

1
v
2
1
3
0
0
.
1
1
1
2
:
v
i
X
r
a

3DP3: 3D Scene Perception via
Probabilistic Programming

Nishad Gothoskar1

Marco Cusumano-Towner1

Ben Zinberg1

Matin Ghavamizadeh1

Falk Pollok2

Austin Garrett1

Joshua B. Tenenbaum1

Dan Gutfreund2

Vikash K. Mansinghka1

1MIT

2MIT-IBM Watson AI Lab

{nishad,marcoct,bzinberg,mghavami,jbt,vkm}@mit.edu

{falk.pollok,austin.garrett}@ibm.com

dgutfre@us.ibm.com

Abstract

We present 3DP3, a framework for inverse graphics that uses inference in a struc-
tured generative model of objects, scenes, and images. 3DP3 uses (i) voxel models
to represent the 3D shape of objects, (ii) hierarchical scene graphs to decompose
scenes into objects and the contacts between them, and (iii) depth image likelihoods
based on real-time graphics. Given an observed RGB-D image, 3DP3’s inference
algorithm infers the underlying latent 3D scene, including the object poses and
a parsimonious joint parametrization of these poses, using fast bottom-up pose
proposals, novel involutive MCMC updates of the scene graph structure, and, op-
tionally, neural object detectors and pose estimators. We show that 3DP3 enables
scene understanding that is aware of 3D shape, occlusion, and contact structure.
Our results demonstrate that 3DP3 is more accurate at 6DoF object pose estimation
from real images than deep learning baselines and shows better generalization to
challenging scenes with novel viewpoints, contact, and partial observability.

1

Introduction

A striking feature of human visual intelligence is our ability to learn representations of novel objects
from a limited amount of data and then robustly percieve 3D scenes containing those objects. We can
immediately generalize across large variations in viewpoint, occlusion, lighting, and clutter. How
might we develop computational vision systems that can do the same?

This paper presents a generative model for 3D scene perception, called 3DP3. Object shapes are
learned via probabilistic inference in a voxel occupancy model that coarsely captures 3D shape
and uncertainty due to self-occlusion (Section 4). Scenes are modeled via hierarchical 3D scene
graphs that can explain planar contacts between objects without forcing scenes to ﬁt rigid structual
assumptions (Section 3). Images are modeled by real-time graphics and robust likelihoods on point
clouds. We cast 3D scene understanding as approximate probabilistic inference in this generative
model. We develop a novel inference algorithm that combines data-driven Metropolis-Hastings
kernels over object poses, involutive MCMC kernels over scene graph structure, pseudo-marginal
integration over uncertain object shape, and existing deep learning object detectors and pose estimators
(Section 5). This architecture leverages inference in the generative model to provide common sense
constraints that ﬁx errors made by bottom-up neural detectors. Our experiments show that 3DP3
is more accurate and robust than deep learning baselines at 6DoF pose estimation for challenging
synthetic and real-world scenes (Section 6). Our model and inference algorithm are implemented in
the Gen [13] probabilistic programming system.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
2 Related Work

Analysis-by-synthesis approaches to computer vision A long line of work has interpreted computer
vision as the inverse problem to computer graphics [25, 45, 30, 26]. This ‘analysis-by-synthesis’
approach has been used for various tasks including character recognition, CAPTCHA-breaking,
lane detection, object pose estimation, and human pose estimation [46, 41, 31, 34, 21, 35]. To our
knowledge, our work is the ﬁrst to use an analysis-by-synthesis approach to infer a hierarchical 3D
object-based representation of real multi-object scenes while exploiting inductive biases about the
contacts between objects.

Hierarchical latent 3D scene representations We use a scene graph representation [47] that is
closely related to hierarchical scene graph representations in computer graphics [11]. Unlike in
graphics, we address the inverse problem of inferring hierarchical scene graphs from observed image
data. Inferring hierarchical 3D scene graphs from RGB or depth images in a probabilistic framework
is relatively unexplored. One concurrent1 and independent work, Generative Scene Graph Networks
(GSGN [15]), proposes a variational autoencoder architecture for decomposing images into objects
and parts using a tree-structured latent scene graph that is similar to our scene graph representation.
However, GSGN learns RGB appearance models of objects and their parts, uses an inference network
instead of a hybrid of data-driven and model-based inference, was not evaluated on real images or
scenes, and uses more restricted scene graphs that cannot represent objects with independent 6DoF
pose. GSGN builds on an earlier deep generative model [19] that generates multi-object scenes but
does not model dependencies between object poses and was not quantitatively evaluated on real
3D scenes. Incorporating a learned inference network for jointly proposing scene graphs into our
framework is an interesting area for future work. The term ‘scene graph’ has also been used in
computer vision to refer to various less related graph representations of scenes [3, 10, 36].

Probabilistic programming for computer vision Prior work has used probabilistic programs to
represent generative models of images and implemented inference in these models using probabilistic
programming systems [31, 28]. Unlike these prior works, which relied on manually speciﬁed and/or
semi-parametric shape models, 3DP3 learns object shapes non-parametrically. 3DP3 also models
occlusion of one 3D object by another; uses a novel hierarchical scene graph prior that allows for
dependencies between object poses in the prior; uses a novel involutive MCMC [12] kernel for
inferring scene graph structure; and uses a novel pseudo-marginal approach for handling uncertainty
about object shape during inference. We also present a proof of concept that our system can infer the
presence and pose of fully occluded objects.

6DoF object pose estimation We use 6DoF estimation of object pose from RGBD images as an
example application. Registration of point clouds [6] can be used to estimate the 6DoF pose of objects
with known 3D geometry from depth images. Many recent 6DoF object pose estimators use deep
learning [43, 40] and many also take depth images [42, 39]. Some pose estimation methods model
scene structure, contact relationships, stability, or other semantic information [24, 10, 27, 18, 5], and
some use probabilistic inference [17, 9, 22, 16]. To our knowledge, we present the ﬁrst 6DoF pose
estimator that uses Bayesian inference about the structure of hierarchical 3D scene graphs.

Learning models of novel 3D objects Classic algorithms for structure-from-motion infer a 3D
model of a scene from multiple images [38, 1]. Our approach for learning the shape of novel 3D
objects produces coarse-grained probabilistic voxel models of objects that can represent uncertainty
about the occupancy of self-occluded volumes. Integrating other representations of object shape and
object appearance [33] with our scene graph representation is a promising area of future work.

3 3DP3 generative modeling framework

The core of 3DP3 is a generative modeling framework that represents a scene in terms of discrete
objects, the 3D shape of each object, and a hierarchical structure called a scene graph that relates the
poses (position and orientation) of the objects. This section describes 3DP3’s object shape and scene
graph latent representations, a family of prior distributions on these latent representations, and an
observation model for image-based observations of scenes. Figure 1 shows the combined generative
model written as a probabilistic program.

1An early version of our work [47] is concurrent with an early version of GSGN [14]

2

(a) Inferring a hierarchical 3D scene graph from an RGB-D image with 3DP3. Our model knows that objects
often lay ﬂat on other objects, which allows for the depth pixels of one object to inform the pose of other objects.
Our algorithm also infers when this knowledge is relevant (e.g. the clamp on the left, represented by the purple
node, is laying ﬂat on the box), and when it is not (e.g. the clamp on the right, represented by the red node, is not
laying ﬂat on any other object).

(b) 3DP3 uses a structured generative model of 3D scenes, represented as a probabilistic program. The model
uses a prior over object shapes that can be learned from data, a prior over scene structure that is a probability
distribution on graphs, a traversal of the scene graph starting at the world node r to compute object poses, and a
robust likelihood model for depth images. In the graph at right, the world node r (not shown) is the parent of the
grey node (box) and the red node (right clamp) because those objects are not layting ﬂat on other objects.

Figure 1: (a) A scene graph inference task and (b) the 3DP3 generative model.

3.1 Objects

The most basic element of our generative modeling framework are rigid objects. The ﬁrst stage in our
generative model prior encodes uncertainty about the 3D shape of M types of rigid objects that may
or may not be encountered in any given scene.

Voxel 3D object shapes We model the coarse 3D shape of rigid objects using a voxel grid with
dimensions h, w, l ∈ N and cells indexed by (i, j, (cid:96)) ∈ [h]×[w]×[l]. Each cell has dimension s×s×s
for resolution s ∈ R+, so that the entire voxel grid represents the cuboid [0, h · s] × [0, w · s] × [0, l · s].
All objects are assumed to ﬁt within the cuboid. An object’s shape is deﬁned by a binary assignment
O ∈ {0, 1}h×w×l of occupancy states to each cell in the voxel grid, where Oij(cid:96) = 1 indicates that
cell (i, j, (cid:96)) is occupied and Oij(cid:96) = 0 indicates it is free. Each object also has a ﬁnite set of contact
planes through which the object may be in ﬂush contact with the contact planes of other objects in
physically stable scenes. For example, in Figure 2, the table has a contact plane for its top surface, the
yellow sugar box has six contact planes, one for each of its six faces, and the bottom contact plane of
the sugar box is in ﬂush contact with the top contact plane of the table. The pose of a contact plane
relative to its object is a function of the object shape O. To simplify notation, we denote the set of
contact planes for any object by F .

Prior distributions on 3D object shape We assume there are M distinct object types, and each
object type m ∈ {1, . . . , M } has an a-priori unknown shape, denoted O(m). Let O(1:M ) :=

3

boxInput RGB ImageInput Depth ImageInferred PosesInferred Scene GraphDepth ReconstructionInferenceNotation
O(m)
M
N
ci
G = (V, E)
r ∈ V
v ∈ V \ {r}
θv
fv , f (cid:48)
v
(av , bv , zv , φv )
xv ∈ SE(3)
∆xv (θv )
˜I
˜Y
Y

Meaning
Object shape
Number of object types
Number of objects
Type of object i
Scene graph structure
World coord. frame
Object coord. frame
Parameters of v
Two contact planes
Planar contact parameters
6DoF pose of v w.r.t. r
6DoF pose of v w.r.t parent
Rendered depth image
Rendered point cloud
Observed point cloud

Section
3.1
3.1
3.2
3.2
3.2
3.2
3.2
3.2
3.2
3.2
3.2
3.2
3.3
3.3
3.3

Figure 2: Our hierarchical scene graphs encode a tree of coordinate frames representing entities in a
scene and their geometric relationships (e.g. ﬂush contact between faces of two objects).

(O(1), . . . , O(M )). The prior distribution on the shape of each object type m is denoted p(O(m)). Al-
though our inference algorithm (Section 5) only requires the ability to sample jointly from p(O(1:M )),
we assume shapes of object types are independent in the prior (p(O(1:M )) = (cid:81)M
m=1 p(O(m))).
Section 4 shows how to learn a speciﬁc shape prior p(O(m)) for an object type from depth images.

3.2 Scenes

Given a collection of M known object types and their shapes, our model generates scenes with N
objects by randomly selecting an object type for each object and then sampling a 6DoF object pose for
each object. Instead of assuming that object poses are independent, our model encodes an inductive
bias about the regularities in real-world scenes: objects are often resting in ﬂush contact with other
objects (e.g. see Figure 2). We jointly sample dependent object poses using a ﬂexible hierarchical
scene graph, while maintaining uncertainty over the structure of the graph.

Hierarchical scene graphs We model the geometric state of a scene as a scene graph G (Figure 2),
which is a tuple G = (G, θ) where G = (V, E) is a directed rooted tree and θ are parameters. The
vertices V := {r, v1, . . . , vN } represent N + 1 3D coordinate frames, with r representing the world
coordinate frame. An edge (u, v) ∈ E indicates that coordinate frame v is parametrized relative to
frame u, with parameters θv. The 6DoF pose of frame v relative to frame u with pose xu ∈ SE(3) is
given by a function ∆xv, where ∆xv(θv) ∈ SE(3) and xv := xu · ∆xv(θv). Here, · is the SE(3)
group operation, and the world coordinate frame is deﬁned as the identity element (xr := I).

Modeling ﬂush contact between rigid objects While the vertices of scene graphs G can represent
arbitrary coordinate frames in a scene (e.g. the coordinate frames of articulated joints, object poses),
in the remainder of this paper we assume that each vertex v ∈ V \ {r} corresponds to the pose
of a rigid object. We index objects by 1, . . . , N , with corresponding vertices v1, . . . , vN . We
assume that each object i has an object type ci ∈ {1, . . . , M }. For vertices v that are children
of the root vertex r, θv ∈ SE(3) deﬁnes the absolute 6DoF pose of the corresponding object
(∆xv(θv) = θv). For vertices v that are children of a non-root vertex u, the parameters take the
form θv = (fv, f (cid:48)
v, av, bv, zv, φv) and represent a contact relationship between the two objects: fv
and f (cid:48)
v indicate which contact planes of the parent and child objects, respectively, are in contact.
(av, bv) ∈ R2 is the in-plane offset of the origin of plane fv of object v from the origin of plane f (cid:48)
v of
object u. zv ∈ R is the perpendicular distance of the origin of plane fv of object v from plane f (cid:48)
v of
object u. φv ∈ S2 × S1 represents the deviation of the normal vectors of the two contact planes from
anti-parallel (in S2) and a relative in-plane rotation of the two contact planes (in S1). The relative
pose ∆xv(θv) of v with respect to u is the composition (in SE(3)) of three relative poses: (i) v with
respect to its plane fv, (ii) v’s plane fv with respect to u’s plane f (cid:48)
v with respect
to u. The 6DoF poses of all objects (xv for v ∈ V \ {r}) are computed by traversing the scene graph
while taking products of relative poses along paths from the root r.

v, and (iii) u’s plane f (cid:48)

Prior distributions on scene graphs We now describe our prior on scene graphs, given object
models O(1:M ). We assume the number of objects N in the scene is known (see the supplement for
a generalization to unknown N ). We ﬁrst sample the types ci ∈ {1, . . . , M } of all objects from an

4

soup can coord. frametable coord. frameworld coord. framesugar box coord. frameexchangeable distribution p(c) where c := (c1, . . . , cN ). This includes as a special case distributions
where all types are represented at most once among the objects ((cid:80)N
i=1 1[ci = c] ≤ 1), which is the
case in our experiments. Next, we sample the scene graph structure G from p(G). We experiment
with two priors p(G): (i) a uniform distribution on the set of (N + 1)N −1 directed trees that are
rooted at a vertex r, and (ii) δG0(G), where G0 is a graph on N + 1 vertices where (r, v) ∈ E for
all v ∈ V \ {r} so that each object vertex has an independent 6DoF pose. For objects whose parent
is r (the world coordinate frame), we sample the pose θv ∼ punif , which samples the translation
component uniformly from a cuboid scene extent, and the orientation uniformly over SO(3). For
objects whose parent is another object u, we sample the choice of contact planes (fv, f (cid:48)
v) ∈ F × F
uniformly, (av, bv) ∼ Uniform([−50cm, 50cm]2), zv ∼ N(0, 1cm), the S2 component of φv from a
von Mises–Fisher (vMF) distribution concentrated (κ = 250) on anti-parallel plane normals, and the
S1 component from Uniform(S1). We denote this distribution pcont(θv). Note that the parameters
of pcont were not tuned or tailored in any detailed way—they were chosen heuristically based on the
rough dimensions of table-top objects. The resulting prior over all of the latent variables is:

p(O(1:M ), c, G, θ) =

(cid:33)

p(O(m))

(cid:32) M
(cid:89)

m=1

1

(N + 1)N −1 p(c)

(cid:89)

v∈V :
(r,v)∈E

punif (θv)

(cid:89)

pcont(θv) (1)

(u,v)∈E:
u(cid:54)=r

3.3

Images

Our generative model uses an observation model that generate synthetic image data given object
shapes O(1:M ) and a scene graph G containing N objects. We now describe the observation model
for depth images that is used in our main experiments (Section 6).

Likelihood model for depth images We ﬁrst convert an observed depth image into a point cloud
Y. To model a point cloud Y ∈ RK×3 with K points denoted yi ∈ R3, we use a likelihood model
based on rendering a synthetic depth image of the scene graph. Speciﬁcally, given the object models
O(m) for each m ∈ {1, . . . , M }, the object types c, the scene graph G, and the camera intrinsic and
extrinsic parameters relative to the world frame, we (i) compute meshes from each O(m), (ii) compute
the 6DoF poses (xv) of objects with respect to the world frame by traversing the scene graph G, and
(iii) render a depth image ˜I of G using an OpenGL depth buffer, and (iv) unproject the rendered depth
image to obtain a point cloud ˜Y with ˜K points ( ˜K is the number of pixels in the depth image). We
then generate an observed point cloud Y ∈ RK×3 by drawing each point from a mixture:

p(Y|O(1:M ), c, G, θ) :=



C ·

K
(cid:89)

i=1

1
B

+

1 − C
˜K

˜K
(cid:88)

j=1

1[||yi − ˜yj||2 ≤ r]
4
3 πr3





(2)

for some 0 < C < 1 and some r > 0. The components of this mixture are uniform distributions over
the balls of radius r centered at each point in ˜Y (with weights (1 − C)/ ˜K) and a uniform distribution
over the scene bounding volume B (weight C).2

4 Learning object shape models

3DP3 does not require hard-coded shape models. Instead, it uses probabilistic inference to learn
non-parametric models of 3D object shape p(O(m)) that account for uncertainty due to self-occlusion.
We focus on the restricted setting of learning from scenes containing a single isolated object (N = 1)
of known type (c1). Our approach works best for views that lead to minimal uncertainty about the
exterior shape of the object; more general, ﬂexible treatments of shape learning and shape uncertainty
are beyond the scope of this paper.

First, we group the depth images by the object type (c1), so that we have M independent learning
problems. Let I1:T := (I1, . . . , IT ) denote the depth observations for one object type, with object
shape denoted O. The learning algorithm uses Bayesian inference in another generative model p(cid:48).
The posterior p(cid:48)(O|I1:T ) produced by this algorithm becomes the prior p(O) used in Section 3.1.

2By using a distribution that is uniform over a small, spherical region rather than a Gaussian distribution, we
avoid (via k-d trees) computing pairwise distances between all points in Y and ˜Y, resulting in ≈ 10x speedup.

5

Figure 3: Learning a voxel-based shape models p(O(m)) for a novel object from a set of 5 depth
images. Our shape priors capture uncertainty about voxel occupancy due to self-occlusion (right).

i=1

j=1

(cid:81)l

(cid:81)w

(cid:96)=1 pOij(cid:96)

We start with a uninformed prior distribution p(cid:48)(O) := (cid:81)h
occ (1 − pocc)(1−Oij(cid:96))
on the 3D shape of an object type, for a per-voxel occupancy probability pocc (in our experiments,
0.5). We learn about the object’s shape by observing a sequence of depth images I1:T that contain
views of the object, which is assumed to be static relative to other contents of the scene, which we
call the ‘map’ M. (In our experiments the map contains the novel object, a ﬂoor, a ceiling, and four
walls of a rectangular room). We posit the following joint distribution over object shape (O) and the
observed depth images, conditioned on the map (M) and the poses of the camera relative to the map
over time (x1, . . . , xT ∈ SE(3)): p(cid:48)(O, I1:T |M, x1:T ) := p(cid:48)(O) (cid:81)T
The likelihood p(cid:48) is a depth image likelihood on a latent 3D voxel occupancy grid (see supplement
for details). For this model, we can compute p(cid:48)(O|M, x1:T , I1:T ) = (cid:81)
ij(cid:96) p(cid:48)(Oij(cid:96)|M, x1:T , I1:T ) ex-
actly using ray marching to decide if a voxel cell is occupied, unoccupied, or unobserved (due to being
occluded by another occupied cell), and the resulting distribution on O can be compactly represented
as an array of probabilities (∈ [0, 1]h×w×l). However, in real-world scenarios the map M and the
camera poses x1:T are not known with certainty. To handle this, our algorithm takes as input uncertain
beliefs about M and x1:T (qSLAM(M, x1:T ) ≈ p(cid:48)(M, x1:T |I1:T )) that are produced by a separate
probabilistic SLAM (simultaneous localization and mapping) module, and take the form of a weighted
1:T ): qSLAM(M, x1:T ) = (cid:80)K
collection of K particles (M(k), x(k)
(x1:T ). Vari-
ous approaches to probabilistic SLAM can be used; we implemented it using sequential Monte Carlo
(SMC) in Gen (more detail in supplement). From the beliefs qSLAM(M, x1:T ) produced by SLAM,
we approximate the object shape posterior via:

k=1 wkδM(k)(M)δx(k)

t=1 p(cid:48)(It|O, M, xt).

1:T

ˆp(cid:48)(O|I1:T ) :=

(cid:90) (cid:90)

p(cid:48)(O|M, x1:T , I1:T )qSLAM(M, x1:T )dMdx1:T =

K
(cid:88)

k=1

wkp(cid:48)(O|M(k), x(k)

1:T , I1:T )

Note that while p(cid:48)(O|M(k), x(k)
1:T , I1:T ) for each k can be compactly represented, the mixture distri-
bution ˆp(cid:48)(O|I1:T ) lacks the conditional independencies that make this possible. To produce a more
compact representation of beliefs about the object’s shape, we ﬁt a variational approximation qϕ(O)
that assumes independence among voxels (qϕ(O) := (cid:81)
·(1−ϕij(cid:96))(1−Oij(cid:96)))
to ˆp(cid:48)(O|I1:T ) using ϕ∗ := arg minϕ KL(ˆp(cid:48)(O|I1:T )||qϕ(O)) (see supplement for details). This
choice of variational family is sufﬁcient for representing uncertainty about the occupancy of voxels in
the interior of an object shape. Note that our shape-learning experiments did not result in signiﬁcant
uncertainty about the exterior shape of objects3, and in the presence of such uncertainty, a less severe
variational approximation may be needed for robust inference of scene graphs from depth images.
Fig. 3 shows input depth images (I1:T ) and resulting shape prior learned from T = 5 observations.
After learning these shape distributions qϕ(O) ≈ ˆp(cid:48)(O|I1:T ) for each distinct object type, we use
them as the shape priors p(Oi) within the generative model of Section 3. The supplement includes
the results of a quantitative evaluation of the accuracy of shape learning.

(cid:96)∈[l] ϕOij(cid:96)

j∈[w]

i∈[h]

(cid:81)

(cid:81)

ij(cid:96)

3The lack of signiﬁcant exterior shape uncertainty in shape-learning experiments allowed us to implement an
optimization: Instead of the relative poses of an object’s contact planes depending on O as described in Section 3,
we assign each object type a set of six contact planes derived from the faces of the smallest axis-aligned bounding
cuboid that completely contains all occupied voxels in one sample O from the learned prior p(O) := qϕ(O).

6

Inferred ShapePosterior(Cross section)CertainUncertainFigure 4: A reversible transition between scene graph structure G and scene graph structure G(cid:48).

5 Building blocks for approximate inference algorithms

This section ﬁrst describes a set of building blocks for approximate inference algorithms that are
based on the generative model of Section 3. We then describe how to combine these components into
a scene graph inference algorithm that we evaluate in Section 6.

It is possible to infer the types of objects in the scene (c) via Bayesian
Trained object detectors
inference in the generative model (see supplement for an example that infers c as well as N in a
scene with a fully occluded object, via Bayesian inference). However, for scenes where objects are
not fully or nearly-fully occluded, and where object types have dissimilar appearance, it is possible to
train fast object detectors that produce an accurate point estimate of c given an RGB image.

Trained pose estimators
In scenes without full or nearly-full occlusion, it is also possible to
employ trained pose estimation methods [42] to give independent estimates of the 6DoF pose of
each object instance in the image. However, inferring pose is more challenging than inferring c,
and occlusion, self-occlusion, and symmetries can introduce signiﬁcant pose uncertainty. Therefore,
we only use trained pose estimators (e.g. [42]) to (optionally) initialize the poses of objects before
Bayesian inference in the generative model, using the building blocks below.

Data-driven Metropolis-Hastings kernels on object pose We employ Metropolis-Hastings (MH)
kernels, parametrized by choice of object i ∈ {1, . . . , N }, that take as input a scene graph G, propose
vi) for the scene graph parameters of object i, construct a new proposed scene graph G(cid:48),
new values (θ(cid:48)
and then accept or reject the move from G to G(cid:48) based on the MH rule. For objects v whose parent is
the world frame ((r, v) ∈ E), we use a data-driven proposal distribution centered on an estimate (ˆxv)
of the 6DoF object pose obtained with ICP (a spherical normal distribution concentrated around the
estimated position, and a vMF distribution concentrated around the estimated orientation). We also
use kernels with random-walk proposals centered on the current pose. For objects whose parent is
another object ((u, v) ∈ E for u (cid:54)= r), we use a random-walk proposal on parameters (avi, bvi, zvi).
Note that when the pose of an object is changed in the proposed graph G(cid:48), the pose of any descendant
objects is also changed.4 Each of these MH kernels is invariant with respect to p(G, θ|c, Y).

Involutive MCMC kernel on scene graph structure To infer the scene graph structure G, we
employ a family of involutive MCMC kernels [12] that propose a new graph structure G(cid:48) while
keeping the poses (xv) of all objects ﬁxed. The kernel takes a graph structure G and proposes a new
graph structure G(cid:48) (Figure 4) by: (i) randomly sampling a node v ∈ V \ {r} to ‘sever’ from the tree,
(ii) randomly choosing a node u ∈ V \ {v} that is not a descendant of the severed node on which to
graft v, (iii) forming a new directed graph G(cid:48) over vertices V by grafting v to u; by Lemma O.7.1
the resulting graph G(cid:48) is also a tree. Note that there is an involution g on the set of all pairs (G, v, u)
satisfying the above constraints. That is, if (G(cid:48), v(cid:48), u(cid:48)) = g(G, v, u) then (G, v, u) = g(G(cid:48), v(cid:48), u(cid:48)).
(This implies, for example, that u(cid:48) is the parent of v in G.) Note that this set of transitions is capable
of changing the parent vertex of an object to a different parent object, changing the parent vertex
of an object from the root (world frame) to any other object, or changing the parent vertex from
another object to the root, depending on the random choice of v and u. We compute new values for
parameters (θv) for the severed node v and possibly other vertices such that the poses of all vertices
are unchanged. See supplement for the full kernel and a proof that it is invariant w.r.t. p(G, θ|c, Y).

4It is possible to construct an involutive MCMC kernel that does not change the poses of descendant objects.

7

Randomly choose:1)  a vertex to sever (    )2)  a graft vertex (    )Randomly choose:1)  a vertex to sever (    )2)  a graft vertex (    )Figure 5: Qualitative comparison between DenseFusion’s pose estimates (top row) and estimates
from 3DP3-based algorithm that is initialized with DenseFusion (bottom row) for YCB-Video frames
where DenseFusion gives incorrect results. 3DP3’s depth-rendering likelihood and scene graph prior
can correct large errors made by DenseFusion.

Approximately Rao–Blackwellizing object shape via pseudo-marginal MCMC The accep-
tance probability expressions for our involutive MCMC and MH kernels targeting p(G, θ|c, Y)
include factors of the form p(Y|c, G, θ), which is an intractable sum over the latent object models:
p(Y|c, G, θ) = (cid:80)
O(1:M ) p(O(1:M ))p(Y|O(1:M ), c, G, θ). To overcome this challenge, we employ
a pseudo-marginal MCMC approach [2] that uses unbiased estimates of p(Y|c, G, θ) obtained via
likelihood weighting (that is, sampling several times from p(O(1:M )) and averaging the resulting
p(Y|O(1:M ), c, G, θ)). The resulting MCMC kernels are invariant with respect to an extended target
distribution of which p(G, θ|c, Y) is a marginal (see supplement for details). We implemented an
optimization where we sampled 5 values for O(1:M ) and used these samples within every estimate of
p(Y|c, G, θ) instead of sampling new values for each estimate. Because our learned shape priors did
not have signiﬁcant exterior shape uncertainty, this optimization did not negatively impact the results.

Scene graph inference and implementation The end-to-end scene graph inference algorithm has
three stages. First, we obtain c from either an object detector or because it is given as part of the task
(this is the case in our experiments; see Section 6 for details). Second, we obtain initial estimates
ˆxv of 6DoF object poses xv for all object vertices v via maximum-a-posteriori (MAP) inference
in a restricted variant of the generative model with graph structure G ﬁxed to G0 (so there are no
edges between object vertices). This MAP inference stage uses the data-driven Metropolis-Hastings
kernels on poses, and (optionally) trained pose estimators (see Section 6 for the details, which differ
between experiments). Third, we use the estimated poses to initialize an MCMC algorithm targeting
p(G, θ|c, Y) with state G ← G0 and θv ← ˆxv for each v ∈ V \ {r}. The Markov chain is a cycle
of the involutive MCMC kernel described above with a mixture of the Metropolis-Hastings kernels
described above, uniformly mixed over objects. We wrote the probabilistic program of Figure 1 in
Gen’s built-in modeling language. We implemented the data-driven and involutive MCMC kernels,
and pseudo-marginal likelihood, and integrated all components together, using Gen’s programmable
inference support. Our code is available at https://github.com/probcomp/ThreeDP3.

6 Experiments

We evaluate our scene graph inference algorithm on the YCB-Video [7] dataset consisting of real RGB-
D images and YCB-Challenging, our own synthetic dataset of scenes containing novel viewpoints,
occlusions, and contact structure. We use the evaluation protocol of the Benchmark for 6DoF Object
Pose Estimation (BOP) Challenge [23], in which an RGB-D image and the number of objects in the
scene and their types are given, and the task is to estimate the 6DoF pose of each object.

6.1 Pose estimation from real RGB-D images

YCB-Video is a standard robotics dataset for training and evaluating 3D perception systems [7].
We ﬁrst learn shape priors (Section 4) from just 5 synthetic images for each object type. We use
DenseFusion [42], a neural 6DoF pose estimator, for pose initialization in the MAP phase of our
inference algorithm. To measure pose estimation accuracy, we use the average closest point distance
(ADD-S [43, 42]) which estimates the average closest point distance between points on the object
model placed at the predicted pose and points on the model placed at the ground-truth pose. Table 1

8

Object Type

# of Scenes

Accuracy
3DP3 3DP3* DF

Accuracy
3DP3 3DP3* DF

Accuracy
3DP3 3DP3* DF

0.5cm Threshold

1.0cm Threshold

2.0cm Threshold

002_master_chef_can
003_cracker_box
004_sugar_box
005_tomato_soup_can
006_mustard_bottle
007_tuna_ﬁsh_can
008_pudding_box
009_gelatin_box
010_potted_meat_can
011_banana
019_pitcher_base
021_bleach_cleanser
024_bowl
025_mug
035_power_drill
036_wood_block
037_scissors
040_large_marker
051_large_clamp
052_extra_large_clamp
061_foam_brick

1006
868
1182
1440
357
1148
214
214
766
379
570
1029
406
636
1057
242
181
648
712
682
288

0.74
0.90
1.00
0.95
0.99
0.81
1.00
1.00
0.80
0.98
1.00
0.94
0.93
0.89
0.98
0.36
0.75
1.00
0.68
0.33
0.26

0.79
0.83
0.99
0.93
0.98
0.80
0.97
1.00
0.78
0.96
0.99
0.88
0.87
0.89
0.96
0.33
0.69
1.00
0.64
0.27
0.24

0.84
0.79
0.98
0.93
0.94
0.91
0.70
1.00
0.79
0.82
0.99
0.80
0.50
0.92
0.88
0.07
0.20
0.99
0.25
0.12
0.01

0.99
0.99
1.00
0.97
0.99
1.00
1.00
1.00
0.89
1.00
1.00
1.00
0.96
0.98
0.99
0.96
0.87
1.00
0.71
0.38
1.00

1.00
0.98
1.00
0.97
0.99
1.00
1.00
1.00
0.88
1.00
1.00
1.00
0.96
0.98
0.99
0.93
0.84
1.00
0.70
0.34
1.00

1.00
0.97
1.00
0.97
0.98
0.99
1.00
1.00
0.87
0.97
1.00
0.99
0.56
0.99
0.98
0.88
0.70
1.00
0.33
0.17
0.99

1.00
0.99
1.00
0.97
1.00
1.00
1.00
1.00
0.93
1.00
1.00
1.00
0.96
1.00
0.99
1.00
0.99
1.00
0.79
0.69
1.00

1.00
0.99
1.00
0.97
1.00
1.00
1.00
1.00
0.93
1.00
1.00
1.00
0.96
1.00
0.99
1.00
0.99
1.00
0.79
0.70
1.00

1.00
0.99
1.00
0.97
1.00
1.00
1.00
1.00
0.92
1.00
1.00
1.00
0.94
1.00
0.99
1.00
0.98
1.00
0.79
0.74
1.00

Table 1: Accuracy results on the real YCB-Video test set, for accuracy thresholds 0.5cm, 1.0cm,
and 2.0cm, and per object type. 3DP3 is our full scene graph inference algorithm and 3DP3* is an
ablation that does not infer contact relationships. ‘# of Scenes’ = The number of test images in which
that object appears, out of the total 2,949 images. ‘DF’ = DenseFusion [42], a deep learning baseline.

shows the quantitative results. For almost all objects, our algorithm (3DP3) is more accurate than an
ablation (3DP3*) that ﬁxes the structure so that there are no contact relationships, and the ablation
is more accurate than DenseFusion. This suggests that both the rendering-based likelihood and
inference of structure contribute to 3DP3’s more accurate 6DoF pose estimation. Figure 5 shows
examples of corrections that 3DP3 makes to DenseFusion’s estimates.

6.2 Generalization to challenging scenes

Next, we evaluated our algorithm’s performance on challenging scenes containing novel viewpoints,
occlusions, and contact structure. Our synthetic YCB-Challenging dataset consists of 2000 RGB-D
images containing objects from the YCB object set [7] in the following 4 categories of challenging
scenes: (i) Single object: Single object in contact with table, (ii) Stacked: Stack of two objects on a
table, (iii) Partial view: Single object not fully in ﬁeld-of-view, (iv) Partially Occluded: One object
partially occluded by another. For this experiment, the MAP stage of our algorithm uses an alternative
initialization (see supplement) that does not use DenseFusion. We evaluate 3DP3 and the 3DP3*
ablation alongside DenseFusion [42] and another state-of-the-art baseline, Robust6D [39]. For most
scenes and objects, our approach signiﬁcantly outperforms the baselines (Table 2). In Table 3, we
assess 3DP3’s robustness by inspecting the error distribution at the 1st, 2nd, and 3rd quartile for each
scene type and object type. At Q3, 3DP3 consistently outperforms the baselines and we ﬁnd that the
drop in performance from Q1 and Q3 is less for 3DP3 than the baselines.

7 Discussion

This paper presented 3DP3, a framework for generative modeling, learning, and inference with
structured scenes and image data; and showed that it improves the accuracy of 6DoF object pose
estimation in cluttered scenes. We used probabilistic programs to conceive of our generative model
and represent it concisely; and we used a probabilistic programming system [13] with programmable
inference [32] to manage the complexity of our inference and learning algorithm implementations.
The current work has several limitations: Our algorithm runs ≈ 20x slower than the DenseFusion
baseline. Our shape-learning algorithm requires that the training scenes contain only the single
novel object, whose identity is known across training frames. Adding the ability to segment and
learn models of novel objects in cluttered scenes and automatically train object detectors and pose
estimators for these objects from short RGB-D video sequences, is an ongoing direction of work. The
model also does not yet incorporate some important prior knowledge about scenes—interpenetration
of objects is permitted, and constraints on physical stability are not incorporated. More experiments
are also needed to understand the implications of a Bayesian treatment of 3D scene perception.

9

Scene Type Object Type

# of Scenes

Accuracy
3DP3 3DP3* DF R6D 3DP3 3DP3* DF R6D 3DP3 3DP3* DF R6D

Accuracy

Accuracy

0.5cm Threshold

1.0cm Threshold

2.0cm Threshold

Single
Object

Stacked

Partial
View

Partially
Occluded

002_master_chef_can
003_cracker_box
004_sugar_box
005_tomato_soup_can
006_mustard_bottle

002_master_chef_can
003_cracker_box
004_sugar_box
005_tomato_soup_can
006_mustard_bottle

002_master_chef_can
003_cracker_box
004_sugar_box
005_tomato_soup_can
006_mustard_bottle

002_master_chef_can
003_cracker_box
004_sugar_box
005_tomato_soup_can
006_mustard_bottle

94
92
109
108
97

190
204
214
193
199

106
99
111
87
97

130
500
117
124
129

0.99
0.55
0.90
0.88
0.86

0.86
0.41
0.63
0.67
0.73

0.81
0.18
0.63
0.34
0.55

0.71
0.37
0.02
0.04
0.70

0.95
0.39
0.87
0.81
0.79

0.79
0.24
0.61
0.52
0.60

0.80
0.16
0.59
0.33
0.62

0.52
0.35
0.01
0.00
0.43

0.45 0.03
0.16 0.00
0.17 0.00
0.18 0.00
0.48 0.01

0.28 0.02
0.16 0.00
0.14 0.01
0.13 0.00
0.44 0.03

0.11 0.00
0.00 0.00
0.00 0.00
0.00 0.00
0.08 0.00

0.04 0.00
0.59 0.00
0.06 0.00
0.01 0.00
0.55 0.03

1.00
0.98
1.00
1.00
1.00

0.94
0.85
0.92
0.89
0.94

1.00
0.60
0.89
0.72
0.87

0.93
1.00
0.30
0.31
0.84

1.00
0.98
1.00
1.00
1.00

0.93
0.81
0.91
0.86
0.90

1.00
0.57
0.89
0.71
0.86

0.90
1.00
0.27
0.23
0.74

0.69 0.46
0.39 0.02
0.72 0.32
0.36 0.07
0.57 0.36

0.56 0.39
0.41 0.04
0.61 0.33
0.28 0.06
0.54 0.30

0.30 0.04
0.01 0.00
0.08 0.04
0.13 0.00
0.23 0.00

0.13 0.02
1.00 0.02
0.40 0.12
0.14 0.06
0.95 0.29

1.00
1.00
1.00
1.00
1.00

0.95
0.97
0.94
0.90
0.94

1.00
0.82
1.00
0.83
0.96

0.99
1.00
0.94
0.81
0.94

1.00
1.00
1.00
1.00
1.00

0.95
0.96
0.94
0.88
0.94

1.00
0.80
1.00
0.82
0.95

0.99
1.00
0.93
0.75
0.90

1.00
0.78
1.00
0.86
0.81

1.00
0.76
0.99
0.75
0.85

0.67
0.14
0.73
0.40
0.37

0.22
1.00
0.84
0.50
1.00

0.98
0.42
1.00
0.74
0.89

0.98
0.40
0.99
0.66
0.88

0.42
0.04
0.68
0.13
0.26

0.12
1.00
0.77
0.49
0.99

Table 2: Accuracy results on our synthetic YCB-Challenging data set. We report the number of
scenes over which this accuracy is computed for each object and scene type. Accuracy is shown for
3DP3 and 3DP3*, which are our full method and an ablation that does not model contact relationships,
respectively, and two deep learning baselines (DenseFusion (DF) [42] and Robust6D (R6D) [39]).

Tomato Soup

Cracker Box

Potted Meat

Sugar Box

Master Chef

Scene Type

Method

ADD-S

ADD-S
Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3

ADD-S

ADD-S

ADD-S

Single object

Stacked

Partial view

0.35 0.39 0.41 0.43 0.49 0.54 0.36 0.40 0.45 0.38 0.43 0.48 0.37 0.43 0.48
3DP3 (ours)
3DP3* (ours) 0.35 0.40 0.43 0.47 0.52 0.62 0.36 0.39 0.44 0.40 0.45 0.49 0.35 0.41 0.49
DenseFusion 0.35 0.55 1.11 0.65 1.35 1.72 0.58 0.88 1.02 0.67 1.25 1.72 0.32 0.61 1.85
0.84 1.05 1.29 1.65 2.22 2.90 0.97 1.09 1.21 1.25 1.61 2.02 0.83 1.48 1.89
Robust6D

0.37 0.42 0.46 0.46 0.52 0.60 0.39 0.45 0.60 0.40 0.46 0.52 0.40 0.45 0.51
3DP3 (ours)
3DP3* (ours) 0.38 0.42 0.48 0.50 0.60 0.79 0.40 0.46 0.64 0.43 0.49 0.61 0.41 0.47 0.56
DenseFusion 0.49 0.87 1.21 0.66 1.33 1.97 0.63 0.92 1.16 0.93 1.42 1.96 0.37 0.66 1.83
0.84 1.15 1.36 1.68 2.21 2.86 0.92 1.11 1.26 1.37 1.72 2.21 0.94 1.38 1.82
Robust6D

0.34 0.40 0.47 0.54 0.76 1.56 0.36 0.45 0.59 0.47 0.55 1.80 0.36 0.47 0.57
3DP3 (ours)
3DP3* (ours) 0.33 0.40 0.47 0.56 0.90 1.54 0.37 0.45 0.63 0.46 0.59 1.81 0.36 0.46 0.58
DenseFusion 0.79 1.52 2.10 2.33 2.93 3.81 1.26 1.65 2.07 1.52 2.14 2.78 1.05 2.22 2.71
1.43 2.25 2.93 3.40 4.03 4.77 1.51 1.83 2.13 2.24 2.99 4.30 1.97 2.50 3.27
Robust6D

Partially Occluded

0.36 0.42 0.52 0.48 0.52 0.55 0.91 1.25 1.57 0.89 1.69 1.97 0.36 0.42 0.55
3DP3 (ours)
3DP3* (ours) 0.39 0.49 0.64 0.48 0.53 0.58 0.97 1.29 1.66 1.03 1.72 1.99 0.43 0.58 1.01
– 0.41 0.48 0.58 0.81 1.11 1.53 1.47 2.01 3.18 0.38 0.48 0.64
DenseFusion
– 1.30 1.48 1.61 1.15 1.46 1.87 1.48 2.02 3.24 0.94 1.10 1.33
Robust6D

–
–

–
–

Table 3: Robustness of inference. We quantify the ADD-S error at 1st, 2nd, and 3rd quartiles for
each scene type and object type in the synthetic dataset of hard scenes. A value of – indicates the
method made no prediction for the object’s pose. 3DP3* denotes an ablated version of our method
without inference of the scene graph structure and thus object-object contact.

8 Acknowledgements

The authors acknowledge Javier Felip Leon (Intel) for helpful discussions and a prototype depth
renderer, and Omesh Tickoo (Intel) for helpful discussions. This work was funded in part by the
DARPA Machine Common Sense program (Award ID: 030523-00001); by the Singapore DSTA /
MIT SCC collaboration; by Intel’s Probabilistic Computing Center; and by philanthropic gifts from
the Aphorism Foundation and the Siegel Family Foundation. We thank Alex Lew, Tan Zhi-Xuan,
Feras Saad, Cameron Freer, McCoy Becker, Sam Witty, and George Matheos for helpful feedback.

10

References

[1] Sameer Agarwal, Noah Snavely, Steven M Seitz, and Richard Szeliski. Bundle Adjustment in
the Large. In ECCV 2010: Proceedings of the European Conference on Computer Vision, pages
29–42. Springer, 2010.

[2] Christophe Andrieu, Gareth O Roberts, et al. The pseudo-marginal approach for efﬁcient Monte

Carlo computations. The Annals of Statistics, 37(2):697–725, 2009.

[3] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and
Silvio Savarese. 3D Scene Graph: A Structure for Uniﬁed Semantics, 3D Space, and Camera.
In ICCV 2019: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 5664–5673, 2019.

[4] Jonathan Baldwin. Hopf Fibration. Available at https://vrs.amsi.org.au/wp-content/
uploads/sites/78/2017/05/baldwin_jonathan_vrs-report.pdf (2021/06/04).
[5] Sid Yingze Bao and Silvio Savarese. Semantic structure from motion. In CVPR 2011: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2025–2032.
IEEE, 2011.

[6] Paul J Besl and Neil D McKay. Method for registration of 3D shapes. In Sensor Fusion IV:
Control Paradigms and Data Structures, volume 1611, pages 586–606. International Society
for Optics and Photonics, 1992.

[7] Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M.
Dollar. Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and
Model Set. IEEE Robotics Automation Magazine, 22(3):36–52, 2015.

[8] Arthur Cayley. A Theorem on Trees, volume 13 of Cambridge Library Collection - Mathematics,

page 26–28. Cambridge University Press, 2009.

[9] Xiaotong Chen, Rui Chen, Zhiqiang Sui, Zhefan Ye, Yanqi Liu, R Iris Bahar, and Odest Chad-
wicke Jenkins. GRIP: Generative Robust Inference and Perception for Semantic Robot Manip-
ulation in Adversarial Environments. In IROS 2019: IEEE/RSJ International Conference on
Intelligent Robots and Systems, pages 3988–3995. IEEE, 2019.

[10] Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, and Song-Chun Zhu. Holistic++
Scene Understanding: Single-view 3D Holistic Scene Parsing and Human Pose Estimation with
Human-Object Interaction and Physical Commonsense. In ICCV 2019: Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 8648–8657, 2019.

[11] James H. Clark. Hierarchical Geometric Models for Visible Surface Algorithms. Commun.

ACM, 19(10):547–554, October 1976.

[12] Marco Cusumano-Towner, Alexander K Lew, and Vikash K Mansinghka. Automating Involutive
MCMC using Probabilistic and Differentiable Programming. arXiv preprint arXiv:2007.09871,
2020.

[13] Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. Gen:
A General-Purpose Probabilistic Programming System with Programmable Inference. In PLDI
2019: Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design
and Implementation, pages 221–236, 2019.

[14] Fei Deng, Zhuo Zhi, and Sungjin Ahn. Generative Hierarchical Models for Parts, Objects, and

Scenes. arXiv preprint arXiv:1910.09119, 2019.

[15] Fei Deng, Zhuo Zhi, Donghun Lee, and Sungjin Ahn. Generative Scene Graph Networks. In

ICLR 2021: International Conference on Learning Representations, 2021.

[16] Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl, and Dieter Fox. PoseRBPF:
A Rao-Blackwellized Particle Filter for 6-D Object Pose Tracking. IEEE Transactions on
Robotics, 2021.

[17] Karthik Desingh, Shiyang Lu, Anthony Opipari, and Odest Chadwicke Jenkins. Efﬁcient
nonparametric belief propagation for pose estimation and manipulation of articulated objects.
Science Robotics, 4(30):eaaw4523, 2019.

[18] Yilun Du, Zhijian Liu, Hector Basevi, Ales Leonardis, Bill Freeman, Josh Tenenbaum, and
Jiajun Wu. Learning to Exploit Stability for 3D Scene Parsing. In S. Bengio, H. Wallach,

11

H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, NIPS 2018: Advances in
Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.

[19] SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E
Hinton, et al. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models. NIPS
2016: Advances in Neural Information Processing Systems, 29:3225–3233, 2016.

[20] Jean Gallier. The Quaternions and the Spaces S3, SU(2), SO(3), and RP3, pages 248–266.

Springer New York, New York, NY, 2001.

[21] Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel Lázaro-Gredilla, Christopher Laan,
Bhaskara Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, Alex Lavin, and D. Scott
Phoenix. A generative vision model that trains with high data efﬁciency and breaks text-based
captchas. Science, 358(6368):eaag2612, 2017.

[22] Jared Glover, Gary Bradski, and Radu Bogdan Rusu. Monte Carlo Pose Estimation with
In RSS 2012: Robotics: Science and

Quaternion Kernels and the Bingham Distribution.
Systems, volume 7, page 97, 2012.

[23] Tomáš Hodaˇn, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft,
Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, Caner Sahin, Fabian Manhardt,
Federico Tombari, Tae-Kyun Kim, Jiˇrí Matas, and Carsten Rother. BOP: Benchmark for 6D
object pose estimation. ECCV 2018: Proceedings of the European Conference on Computer
Vision, 2018.

[24] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic
3D scene parsing and reconstruction from a single RGB image. In ECCV 2018: Proceedings of
the European Conference on Computer Vision, pages 187–203, 2018.

[25] D Knill D Kersten and A Yuille. Introduction: A Bayesian Formulation of Visual Perception.

Perception as Bayesian inference, pages 1–21, 1996.

[26] Daniel Kersten, Pascal Mamassian, and Alan Yuille. Object Perception as Bayesian Inference.

Annu. Rev. Psychol., 55:271–304, 2004.

[27] Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhinav Gupta. 3D-RelNet: Joint object
In Proceedings of the IEEE/CVF International

and relational network for 3D prediction.
Conference on Computer Vision, pages 2212–2221, 2019.

[28] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A
Probabilistic Programming Language for Scene Perception. In CVPR 2015: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 4390–4399, 2015.

[29] J.M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer, 2003.

[30] Tai Sing Lee and David Mumford. Hierarchical Bayesian Inference in the Visual Cortex. JOSA

A, 20(7):1434–1448, 2003.

[31] Vikash K Mansinghka, Tejas D Kulkarni, Yura N Perov, and Josh Tenenbaum. Approxi-
mate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs. NIPS
2013:Advances in Neural Information Processing Systems, 26:1520–1528, 2013.

[32] Vikash K Mansinghka, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and
Martin Rinard. Probabilistic Programming with Programmable Inference. In PLDI 2018:
Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and
Implementation, pages 603–616, 2018.

[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi,
and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In
ECCV 2020: Proceedings of the European Conference on Computer Vision, pages 405–421.
Springer, 2020.

[34] Pol Moreno, Christopher KI Williams, Charlie Nash, and Pushmeet Kohli. Overcoming
Occlusion with Inverse Graphics. In ECCV 2016: Proceedings of the European Conference on
Computer Vision, pages 170–185. Springer, 2016.

[35] Lukasz Romaszko, Christopher K. I. Williams, Pol Moreno, and Pushmeet Kohli. Vision-as-
Inverse-Graphics: Obtaining a Rich 3D Explanation of a Scene from a Single Image. In ICCVW
2017: IEEE International Conference on Computer Vision Workshops, pages 940–948, 2017.

12

[36] Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi, and Luca Carlone. 3D Dynamic
Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans. arXiv preprint
arXiv:2002.06289, 2020.

[37] Ruwen Schnabel, Roland Wahl, and Reinhard Klein. Efﬁcient RANSAC for point-cloud shape
detection. In Computer graphics forum, volume 26, pages 214–226. Wiley Online Library,
2007.

[38] Johannes L. Schönberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In CVPR
2016: IEEE Conference on Computer Vision and Pattern Recognition, pages 4104–4113, 2016.
[39] Meng Tian, Liang Pan, Marcelo H Ang Jr, and Gim Hee Lee. Robust 6D Object Pose Estimation
by Learning RGB-D Features. In ICRA 2020: International Conference on Robotics and
Automation, 2020.

[40] Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan
Birchﬁeld. Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects.
In CORL 2018: Conference on Robot Learning, 2018.

[41] Zhuowen Tu and Song-Chun Zhu. Image segmentation by data-driven Markov chain Monte
Carlo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):657–673, 2002.
[42] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martín-Martín, Cewu Lu, Li Fei-Fei, and Silvio
Savarese. DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion. In CVPR 2019:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
3343–3352, 2019.

[43] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. PoseCNN: A Convolu-

tional Neural Network for 6D Object Pose Estimation in Cluttered Scenes. 2018.

[44] Anna Yershova, Swati Jain, Steven M. LaValle, and Julie C. Mitchell. Generating Uniform
Incremental Grids on SO(3) Using the Hopf Fibration. The International Journal of Robotics
Research, 29(7):801–812, 2010.

[45] Alan Yuille and Daniel Kersten. Vision as Bayesian Inference: Analysis by Synthesis? Trends

in Cognitive Sciences, 10(7):301–308, 2006.

[46] Song Chun Zhu and Alan Yuille. Region competition: Unifying snakes, region growing, and
bayes/mdl for multiband image segmentation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 18(9):884–900, 1996.

[47] Ben Zinberg, Marco Cusumano-Towner, and K Mansinghka Vikash. Structured differentiable
models of 3D scenes via generative scene graphs. In Workshop on Perception as Generative
Reasoning, NeurIPS 2019, Vancouver, Canada, 2019.

13

A Broader Impact

While the goal of robust scene parsing and pose estimation is challenging, and the present work
is an early step with much more work lying ahead, it is important to consider potential societal
impacts of this work, both positive and negative. Robust pose estimation will be instrumental in
improving the reliability of a wide variety of applications—including assistive technologies for
people with limited mobility, improved fault detection in manufacturing plants, and safer autopilot
for autonomous vehicles. On the other hand, these same technologies, if used toward the wrong ends,
could have negative societal impacts as well, such as unjust or inequitable surveillance, or weapon
guidance systems that fall into the wrong hands. Even applications that are largely beneﬁcial must
be implemented thoughtfully to avoid negative side effects. For example, in the present work, the
choice of prior distribution on contact structures implies an inductive bias that, if chosen incorrectly,
could lead to technologies that are less reliable when the scene being parsed contains a person
in a wheelchair. As a scientiﬁc community, it is important that we place continued emphasis on
developing technical safeguards against both overt misuse and unintended consequences like the
above. Furthermore, we must remember that technical safeguards on their own are not sufﬁcient: we
must communicate to broader society not just the beneﬁts, but also the risks of this technology, so
that users can be informed participants and apply this technology towards a better world.

B Pose estimation from synthetic RGB images

In the previous two sections, 3DP3 was used with a depth-rendering-based likelihood on depth images
since an RGB-D image was given as input. In this section, we show that 3DP3 can be used to do
pose estimation without depth data i.e. from just an RGB image. Instead of a depth likelihood, we
substitute an RGB renderer and simple color likelihood. We qualitatively compare with Attend, Infer,
Repeat (AIR) [19], an amortized inference approach based on recurrent neural networks which can
be applied to infer poses of 3D objects. We generated scenes that resemble the tabletop scenes on
which AIR qualitatively assessed pose inference accuracy. Figure 6 shows pairs of input RGB images
and corresponding reconstructions from pose inferences made by 3DP3. Qualitatively, our system
produces pose inferences of better or equal accuracy to AIR. Importantly, our system does not require
training. In contrast, AIR takes approximately 3 days for training to converge. Also at these lower
resolutions, our inference can run in 0.5s per frame.

Figure 6: A variant of our scene graph inference algorithm that uses a RGB-based likelihood applied
to synthetic RGB scenes designed to resemble those used in the evaluation of AIR [19]. Our algorithm
gives accurate reconstructions with 0.5 seconds of inference time on these scenes and no training.

14

RGB Image Input3DP3 ReconstructionC Shape Learning Accuracy Quantitative Evaluation

Object Type

IoU

0.9544
002_master_chef_can
0.9716
003_cracker_box
0.9484
004_sugar_box
0.9433
005_tomato_soup_can
0.9671
006_mustard_bottle
0.9696
007_tuna_ﬁsh_can
0.9617
008_pudding_box
0.9451
009_gelatin_box
0.9654
010_potted_meat_can
0.9599
011_banana
0.9808
019_pitcher_base
0.9582
021_bleach_cleanser
0.9694
024_bowl
0.9621
025_mug
0.966
035_power_drill
0.9679
036_wood_block
0.9505
037_scissors
0.9767
040_large_marker
051_large_clamp
0.9218
052_extra_large_clamp 0.9228
0.9405
061_foam_brick

Table 4: We include a quantitative evaluation comparing the learned shape models to the ground
truth shape models. To get a shape model from the learned shape prior, we take all voxels which the
prior says are more likely to be occupied than unoccupied and compute the IoU between that volume
and the ground truth object volume.

D YCB-Challenging Dataset

YCB-Challenging is a synthetic test dataset of 2000 RGB-D images, 500 in each of the following 4
categories:

Single object: Single object in contact with table

Stacked: Stack of two objects on a table

Partial view: Single object not fully in ﬁeld-of-view

Partially Occluded: One object occluded by another

15

E YCB-Challenging Extended Experimental Results

Figure 7: Accuracy of our method and two deep learning baselines (DenseFusion [42] and Ro-
bust6D [39]) on the task of 6DoF pose estimation in our synthetic ‘YCB-Challenging’ dataset. For
each of the 4 scene types (rows) and 5 object types (columns), we measure accuracy for a range of
ADD-S thresholds. 3DP3* denotes an ablated version of our method without inference of the scene
graph structure and thus object-object contact (i.e. we ﬁx the scene graph to G0)

F YCB-Video Dataset

Figure 8: The YCB-Video [7] test data set consists of 2,949 real RGB-D images featuring the 21
YCB objects. These 2,949 images are collected from videos of 12 different scenes where the camera
pans around the scene to view it from different perspectives. The 12 scenes contain different subsets
of the 21 YCB objects, and some objects appear in multiple scenes (e.g 007_tuna_ﬁsh_can appears in
Scenes 1, 2, and 12).

16

0.00.250.50.751.0Accuracy005_tomato_soup_can003_cracker_box010_potted_meat_can004_sugar_box002_master_chef_can0.00.250.50.751.0Accuracy0.00.250.50.751.0Accuracy0.00.51.01.52.0ADD-S Threshold (cm)0.00.250.50.751.0Accuracy0.00.51.01.52.0ADD-S Threshold (cm)0.00.51.01.52.0ADD-S Threshold (cm)0.00.51.01.52.0ADD-S Threshold (cm)0.00.51.01.52.0ADD-S Threshold (cm)DenseFusionRobust6D3DP3* (ours)3DP3 (ours)Scene TypeSingleObjectStackedPartialViewPartiallyOccludedYCB-Video Test Data Set Scene 2 024_bowl004_sugar_box007_tuna_ﬁsh_can010_potted_meat_can Scene 1 025_mug002_master_chef_can051_large_clamp007_tuna_ﬁsh_can052_extra_large_clamp ... Scene 12 003_cracker_box005_tomato_soup_can007_tuna_ﬁsh_can 010_potted_meat_can 035_power_drill040_large_marker ... ... ... ...2,949 total imagesG Ablation Qualitative Results on YCB-Video

Figure 9: Comparison with ablated model on YCB-V real scenes. For each of the 21 YCB objects,
we show 2 images of scenes containing that object and the poses estimated by our full method (3DP3)
and an ablated version of our method (3DP3*) that does not model contact relationships.

H Qualitative Results on YCB-Video

17

002_master_chef_can003_cracker_box004_sugar_box005_tomato_soup_can006_mustard_bottle007_tuna_ﬁsh_can008_pudding_box009_gelatin_box010_potted_meat_can011_banana019_pitcher_base021_bleach_cleanser024_bowl025_mug035_power_drill036_wood_block037_scissors040_large_marker051_large_clamp052_extra_large_clamp061_foam_brick3DP33DP3* (ablation)Figure 10: YCB frames for each object, overlayed with pose estimates of DenseFusion (cyan) and
3DP3 (green), where there is a large performance difference between the two methods.

18

002_master_chef_can Scene 9 Frame 32003_cracker_box Scene 7 Frame 56004_sugar_box Scene 8 Frame 122005_tomato_soup_can Scene 3 Frame 345006_mustard_bottle Scene 3 Frame 142007_tuna_fish_can Scene 2 Frame 1488008_pudding_box Scene 11 Frame 21009_gelatin_box Scene 11 Frame 867010_potted_meat_can Scene 2 Frame 858011_banana Scene 9 Frame 441019_pitcher_base Scene 11 Frame 1248021_bleach_cleanser Scene 7 Frame 479024_bowl Scene 6 Frame 309025_mug Scene 1 Frame 155035_power_drill Scene 12 Frame 676036_wood_block Scene 8 Frame 1824037_scissors Scene 4 Frame 1474040_large_marker Scene 12 Frame 1822051_large_clamp Scene 1 Frame 1783052_extra_large_clamp Scene 1 Frame 135061_foam_brick Scene 10 Frame 1774Figure 11: YCB frames for each object, overlayed with pose estimates of DenseFusion (cyan) and
3DP3 (green), where there is almost no performance difference between the two methods.

19

002_master_chef_can Scene 1 Frame 1806003_cracker_box Scene 12 Frame 176004_sugar_box Scene 4 Frame 636005_tomato_soup_can Scene 12 Frame 1027006_mustard_bottle Scene 5 Frame 567007_tuna_fish_can Scene 12 Frame 400008_pudding_box Scene 11 Frame 849009_gelatin_box Scene 11 Frame 776010_potted_meat_can Scene 2 Frame 1641011_banana Scene 3 Frame 1656019_pitcher_base Scene 5 Frame 341021_bleach_cleanser Scene 8 Frame 912024_bowl Scene 2 Frame 1067025_mug Scene 8 Frame 1717035_power_drill Scene 9 Frame 62036_wood_block Scene 8 Frame 205037_scissors Scene 4 Frame 1021040_large_marker Scene 10 Frame 486051_large_clamp Scene 7 Frame 1668052_extra_large_clamp Scene 10 Frame 1500061_foam_brick Scene 10 Frame 818I Distilling shape distributions via variational inference

Recall that:

p(cid:48)(O|I1:T ) =

K
(cid:88)

k=1

wkp(cid:48)(O|M(k), x(k)

1:T , I1:T )

Consider the following variational family:

qϕ(O) :=

(cid:89)

(cid:89)

(cid:89)

i∈[h]

j∈[w]

(cid:96)∈[l]

ϕOij(cid:96)
ij(cid:96)

· (1 − ϕij(cid:96))(1−Oij(cid:96))

(3)

(4)

where each 0 ≤ ϕij(cid:96) ≤ 1 can be interpreted as a per-voxel occupancy probability. Then

KL(p(cid:48)(O|I1:T )||qϕ(O)) = EO∼p(cid:48)(·|I1:T )

(cid:20)

log

(cid:21)

p(cid:48)(O|I1:T )
qϕ(O)

Note that minimizing this KL divergence with respect to ϕ is equivalent to maximizing the following
quantity:
EO∼p(cid:48)(·|I1:T ) [log qϕ(O)]

=

=

=

=

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

wkE

O∼p(cid:48)(·|M(k),x(k)

1:T ,I1:T ) [log qϕ(O)]


wkE

O∼p(cid:48)(·|M(k),x(k)

1:T ,I1:T )

wkE

O∼p(cid:48)(·|M(k),x(k)

1:T ,I1:T )

(cid:88)

(cid:88)

(cid:88)



i∈[h]

j∈[w]

(cid:96)∈[l]





(cid:88)

(cid:88)

(cid:88)

i∈[h]

j∈[w]

(cid:96)∈[l]



log qϕ(Oij(cid:96))



Oij(cid:96) log ϕij(cid:96) + (1 − Oij(cid:96)) log(1 − ϕij(cid:96))





(cid:88)

(cid:88)

(cid:88)

K
(cid:88)

i∈[h]

j∈[w]

(cid:96)∈[l]

k=1

wkp(cid:48)(Oij(cid:96) = 1|M(k), x(k)

1:T , I1:T ) log ϕij(cid:96) + p(cid:48)(Oij(cid:96) = 0|M(k), x(k)

1:T , I1:T ) log(1 − ϕij(cid:96))

The optimization decomposes into separate problems for each ϕij(cid:96), with global optimum:

ϕ∗

ij(cid:96) =

K
(cid:88)

k=1

wkp(cid:48)(Oij(cid:96) = 1|M(k), x(k)

1:T , I1:T )

J Probabilistic SLAM using Sequential Monte Carlo

To infer the camera poses x1:T corresponding to the sequence of T depth images, we implemented a
probabilistic SLAM using Sequential Monte Carlo. We assume the depth images contain background,
which can be mapped and used for localization between frames. (In our data, the object is placed in
the center of a rectangular room with a ﬂoor, ceiling, and four walls.) We also assume that the camera
is the same distance away from the object in all T images. Finally, in order to ensure a reference
frame match between the learned object model and ground truth model (such that at test time, object
pose estimates of our system can be compared to the ground truth object poses), we provide the initial
pose of the object in the camera frame.

To perform SLAM, we initialize a set of K particles with the observation, camera pose, and implied
map at t = 1. Then, we enumerate over the position and viewing angle at t = 2, given the map at
t = 1, observation at t = 2, and a prior on the camera pose conditioned on the pose at t = 1, and
compute scores for each pose. We then construct a Gaussian mixture proposal where each component
is centered on a different pose and has weight corresponding to the normalized score computed by the
enumeration. We step the particles forward to t = 2 with this proposal distribution. Then, for each of
the particles, we update the map given the observations and inferred poses. We repeat this for all T
timesteps, at which point we have K particles with inferred camera poses for each of the T timesteps.
The depth image likelihood p(cid:48)(It|O, M, xt) = δd(O,M,xt)(It) where d is a depth rendering function.

20

K Pose initialization for Scene Graph Inference

In the ﬁrst stage of our scene graph inference algorithm, we obtain initial estimates of the 6DoF object
poses via maximum-a-posteriori (MAP) inference in a restricted variant of our model that assumes no
edges between objects in the scene graph. We maintain a set of particles with each particle assigned
to a different object, and we have at least one particle assigned to each object. Then, for each particle
we apply Metropolis-Hastings (MH) kernels to pose of the object that the particle is assigned to.
After applying these MH kernels, we resample the set of particles using their normalized weights.
We repeat this process of applying the MH kernels and resampling for a ﬁxed number of iterations
(proportional to the number of objects). We construct the MH kernels for each object by using spatial
clustering and iterative closest point (ICP) to compute a set of “high-quality” poses for each object
type given the observed scene. We ﬁrst apply DBSCAN to the set of points that are unexplained by
the current hypothesized scene. Then we create a set of initial object pose hypotheses with translation
selected from the C cluster centers output by DBSCAN and orientation selected from the set of 24
nominal orientations, for a total of 24 · C poses. (The 24 orientations are the rotational symmetries of
a cube.). Next, we reﬁne these initial pose estimates using ICP. The ICP does not use the full object
model, but rather renders the object at the hypothesized pose and computes the corresponding point
cloud. We score the resulting pose estimates under the generative model and use the normalized
weights to construct a mixture proposal that serves as the MH kernel.

In addition to the above MH kernel, we also experimented with kernels based on Boltzmann proposals
where the Hamiltonian is determined by performing a 3D convolution of a mask with the observed
point cloud. Such proposals can potentially be used as “compiled detectors" of the object models
O1:M , enabling us to perform online object learning and scene parsing. This class of proposals takes
the following general form:

1. Discretize the observation into a 3D grid Γ.

2. Given the object model O, create k convolutional masks to be convolved with the grid. Each
mask is meant to detect O at a certain orientation. The candidate orientations are obtained
from an appropriately ﬁne geodesic grid on a sphere.

3. Slide each mask over Γ and calculate the convolution of the mask and Γ.

4. Fix β > 0, and propose a pose from a Boltzmann distribution with temperature β, where the

Hamiltonian of each pose is given by the convolution of its associated mask with Γ.

We tried multiple approaches for deriving convolutional masks from objects models. Maximally
informative and maximally correlated masks require us to solve ill-posed optimization problems.
Small windows sampled from the object model are not informative. These masks can give good
proposals when combined with expensive ensembling and outlier detection, but they are unsuitable
for online settings. Our best results come from globally-sparse, locally-dense [37], randomly selected
masks. These masks are computationally efﬁcient to apply and give results that are qualitatively
comparable to the ICP-based kernel, but the sampling distribution of the masks have high-variance.
In future work, we plan to further investigate this class of proposals.

L Parsing scenes with fully occluded objects and number uncertainty

Consider the setting when the number of objects in the scene (N ) is unknown a-priori. Possibilities
for prior probability distributions on N (p(N )) include (i) an a-priori known number of objects
N0 (used in the experiments in Section 6), (ii) Binomial(N0, ppresent), which is induced by a prior
belief that each of N0 objects is present with probability ppresent (used in experiments described in
Supplement L), and (iii) Poisson(λ), which places no a-priori upper bound on the number of objects.

In this section, we apply our framework to do probabilistic inference about the 6DoF pose and
presence or absence of a fully occluded object, and investigate the dynamics of these inferences as
we vary the fraction of the volume in the scene that is occluded.

Suppose a robot is tasked with assembling a piece of furniture, performing maintenance on a vehicle,
or retrieving something from the kitchen.
In each of these cases, the robot has a strong prior
expectation that some object (e.g. a tool, component, or kitchen item) is present in the environment.
However, in complex cluttered real-world environments the target object is likely to be fully occluded

21

from the robot’s view. That target object may even even be absent, especially in human-robot
interactive task situations (e.g. the component or item is missing or misplaced). To perform rationally
in such situations, the robot will need to generate possible poses of the object that are concordant with
its absence from its visual ﬁeld. Also, the robot must consider the possibility that the object is indeed
not present, by weighing the lack of observed presence of the object against the prior expectations.

Prior Consider a scenario where there are N0 = M unique objects that may or may not be present
in a scene (N0 denotes the total number of object instances, and M denote the number of object
types). Suppose that the prior probability that the object of type m is present with probability p(m)
pres
for m ∈ {1, . . . , M }. Then, the prior p(N, c) is:

p(N, c) =

(cid:40)

1
N !
0

(cid:81)M

m=1 p(m)

pres

1[m∈c]

(1 − p(m)
pres)

1[m(cid:54)∈c]

if |c| = N and (cid:80)N
otherwise

i=1 1[ci = m] ≤ 1 ∀m

In the special case when p(m)
can write the marginal distribution p(N ) and conditional distribution p(c|N ) as:

(5)
pres is the same for all m (this is the case in our experiments below), we

N ∼ Binomial(M = N0, ppres)

and

p(c|N ) =

(cid:26) (M −N )!

M !

0

if |c| = N and (cid:80)N
otherwise

i=1 1[ci = m] ≤ 1 ∀m

(6)

(7)

For each possible (N, c), we ﬁx the scene graph G to be the graph G0(N ) on N vertices that has
with no object-object edges:

p(G|N ) =

(cid:26) 1
0

if G = G0(N )
otherwise

(8)

That is, each object v has an independent 6DoF pose xv. The prior distribution on the orientation
component used the uniform distribution on an Euler angle parametrization, and the prior on the
translation component (i.e. the location of the object) the uniform distribution on a cuboid volume
representing the extent of the scene.

Likelihood Instead of the likelihood on point clouds used in Section 3.3, here we use an alternative
likelihood based on (i) rendering a depth image ˜I(O1:M , c, G, θ) and then (ii) adding noise to generate
an observed depth image I. The likelihood is a per-pixel mixture between a uniform distribution on
the range of possible depth values, and a normal distribution with ﬁxed variance σ2:
(cid:18)

(cid:19)

p(I|x) =

0.1 ·

+ 0.9 · N (Ii; ˜Ii, σ))

(cid:89)

i

1
D

where i indexes pixels of the depth image. Pixels whose ray does not intersect an object are assigned
the maximum depth value D. A similar likelihood function on depth images was used in [34].

MCMC inference algorithm We use a Markov chain Monte Carlo (MCMC) inference algorithm
that cycles through each object type m ∈ {1, . . . , M }, and applies several types of MCMC moves
for each type, based on the following proposals: (i) involutive MCMC kernels that switch an object
type from being absent to being present and vice versa (proposing its pose from the prior) , (ii)
Metropolis–Hastings kernels that propose the translational components of the pose xv for each object
from the prior, (iii) Metropolis–Hastings kernels that propose the rotational component of the pose
for each object from the prior, and (iv) coordinate-wise random-walk proposals to each of the 6
dimensions of the pose of each object (the 3 coordinates of its location and its Euler angles). We
initialize the Markov chain with a sample from the prior distribution.

Inferring the 6DoF pose of a fully occluded object We ﬁrst investigated inference about the
6DoF pose of an object (the mug from the YCB object set [7]) that is assumed to be in a volume in
front of the camera, but that is not visible. This scenario arises when searching for a component or
tool that is expected to be in the environment. Narrowing down where the object could be, based on
observing where it is not, is important for efﬁciently planning and acting to obtain more information

22

(a) The scenario (left). A depth camera is viewing a scene that may or may not contain a cracker box (middle)
and a mug (right). We perform Bayesian inference on the existence and contingently, their 6DoF poses within
the scene, of both objects. Only the cracker box is visible in the observed depth images (see below).

(b) Observed depth image and posterior samples where the existence of both the box and mug are assumed.

(c) Observed depth image and posterior samples where the existences of each object have prior probability 0.90.
The posterior probabilities of existence for the mug and box are 0.37 and 1.0, respectively.

(d) Observed depth image and posterior samples where the existences of each object have prior probability 0.90.
Note that the observed image has the box angled so that it occupies less of the ﬁeld of view than in (c). The
posterior probabilities of existence for the mug and box are 0.33 and 1.0, respectively.

(e) Observed depth image and posterior samples where the existences of each object have prior probability 0.90.
Note that the observed image has the box closer to the camera, so that it occupies more of the ﬁeld of view than
in (c). The posterior probabilities of existence for the mug and box are 0.63 and 1.0, respectively.

Figure 12: Inferring the 6DoF pose and existence of multiple objects from depth images using
MCMC in a generative model. Several scenarios are shown, with ﬁve approximate posterior samples
from each. To estimate posterior probabilities of object existence, 20 posterior samples were used.
The lack of percept of the mug in the visual ﬁeld (i) reduces the posterior probability of its presence,
but also (ii) informs the distribution on its 6DoF pose, if it is present. Note that as the fraction of
volume in the scene that is occluded by the box decreases, the posterior probability that mug is
present decreases.

23

and retrieve the object. In order for inference to be coherent, the lack of the object’s visible presence
must be explained away by the occluding presence of another object. Therefore, we also assume
the presence of another object (the cracker box). The results (Figure 12(b)) show that the algorithm
successfully infer a variety of 6DoF poses of the mug in which it lies behind the cracker box.

Jointly inferring the existence and poses of multiple objects
If an object is not visible, we may
conclude that it is not present in the scene. The degree of belief in the presence of an object that is
not visible depends on the degree of prior belief in its presence and the volume of possible states in
which the object is fully occluded. If there are no other objects in the scene, then intuitively, there
is nowhere the object could be hiding in the scene, so it must be absent. If there are other objects
in the scene, then the pose of these other objects interacts with the object’s existence, in our beliefs.
To investigate the interaction between the beliefs about the poses and existence of multiple objects,
we generated synthetic depth data for several scenarios (Figure 12(c-e)). In all scenarios, the prior
probabilities that the box and mug are present are both 0.9. In the ﬁrst scenario (Figure 12(c)) the
box is oriented so a wide face is facing the camera. We correctly infer the existence of the box with
high conﬁdence (1.0 posterior probability) and we assign 0.37 posterior probability to the existence
of the mug. In the second scenario (Figure 12(d)) the box is rotated so that it occupies less of the
camera’s ﬁeld of view. As expected, this causes the posterior probability of the mug’s existence to
decrease to 0.31. In the third scenario (Figure 12(e)), we move the box closer to the camera so that it
occupies more of the ﬁeld of view. The posterior probability of the mug’s presence then increases to
0.63. These experiments illustrate the dependence between one object’s pose and another object’s
existence, which is a consequence of occlusion.

M Experiment Details

The deep learning baseline experiments were run on a 3.70GHz Intel i7 processor with 64GB RAM
and a Nvidia GeForce GTX 1080Ti GPU. All other experiments were run a 2.40GHz Intel i9 processor
with 32GB RAM and a Nvidia GeForce GTX 1650 Mobile GPU. Our model is implemented using
Julia in Gen, a probabilistic programming system [13].

N Pseudomarginal shape inference

We use MH and involutive MCMC kernels that are stationary with respect to a target distribution
on an extended state space that includes auxiliary variables O(i) for i = 1, . . . , R (R copies of all
object shape models) by replacing the likelihood p(Y|N, c, G(cid:48), θ(cid:48)) for each proposed state in the
acceptance probability with the following unbiased estimate obtained by sampling object models
from the prior:

1
R

R
(cid:88)

i=1

p(Y|O(i)

1 , . . . , O(i)

M , N, c, G, θ) where O(i)

c

i.i.d.∼ p(·)

(9)

and replacing the likelihood in the denominator of the acceptance ratios with the unbiased estimate
computed by the last accepted proposal. This is an instance of the pseudomarginal MCMC [2]
framework. For example, for the scene graph involutive MCMC kernel described in Section O, each
factor of the form:

p(Y|N, c, G(cid:48), θ(cid:48))
p(Y|N, c, G, θ)

is replaced with a factor:

1
R
1
R

(cid:48)

(cid:48)

(cid:80)R

i=1 p(Y|O(i)
i=1 p(Y|O(i)

, . . . , O(i)
M
1 , . . . , O(i)
M , N, c, G)

, N, c, G)

1

(cid:80)R

(10)

(11)

are the shape models that were sampled when proposing the last state that was

where the O(i)
j
accepted, and where the O(i)
j
Note that the old sampled shape models O(i)
themselves do not need to be persisted across steps of
j
the Markov chain—only the denominator in the expression above needs to be stored. The resulting

are the shape models that are sampled during the current proposal.

(cid:48)

24

moves can be interpreted as MH (or involutive MCMC) moves on an extended state space that
includes additional auxiliary random variables O(1)
1:M . The moves are stationary with
respect to the following target distribution on the extended state space:

1:M , . . . , O(R)

p(G, θ|N, c, Y)

1
R

R
(cid:88)

i=1

p(O(i)

1:M |N, c, G, θ, Y)

p(O(j)

1:M )

(cid:89)

j(cid:54)=i

(12)

of which the marginal on (G, θ) is the original target distribution p(G, θ|N, c, Y).

O Involutive MCMC kernel on scene graph structure and parameters

This section gives details of the involutive MCMC kernel on scene graphs introduced in Section 5.

O.1 Notation for coordinate projections

In several places below, we deﬁne sets of tuples using set-builder notation such as

X := {(x, y, z) | some condition on x, y, z}.

In such a case, we may also deﬁne coordinate projections that get their names from the formal
variables (“x,” “y,” “z”) used in the set-builder expression. Our convention is to denote these
coordinate projections by the name proj•, where • is either a variable name, e.g.,

or a comma-separated list of variable names, e.g.,

projx(x, y, z) := x

projy,z(x, y, z) := (y, z).

This deﬁnition depends not just on the set X, but on the notation used to deﬁne it; thus, in the
exposition below, we explicitly declare each time we deﬁne a function proj• using the above
convention. Note that the name projx is to be taken as a single unit, i.e., x does not have independent
meaning; in particular, if there is also a variable x in the scope of discourse, the name projx does not
have anything to do with that variable’s value.

O.2 The number of scene graph structures on a ﬁxed set of objects

Proposition O.2.1. For a given set of N objects, the number of possible scene graph structures is
(N + 1)N −1. (Here the root node r is not considered an object.)

Proof. Let V (cid:48) be a set of N objects, and let V := V (cid:48) ∪ {r}. For a given undirected tree (cid:101)G on vertices
V , there is a unique way to assign edge directions to (cid:101)G to turn it into a directed tree rooted at r. This
gives a one-to-one correspondence

directed trees on V rooted at r ←→ undirected trees on V .

By Cayley [8], the number of undirected trees on V is (N + 1)N −1.

O.3 An involutive MCMC kernel on scene graph structure only

For some set V of vertices and a root vertex r ∈ V , let G(V ) denote the set of directed trees over
vertices V rooted at r. For each G = (V, E) ∈ G(V ) and each v ∈ V \ {r}, let S(G, v) ⊂ V denote
the vertices of the subtree rooted at v, i.e., the set containing v and its descendants. Let

T (G) := {(v, u) ⊂ V × V : v (cid:54)= r, u /∈ S(G, v)} .

(13)

That is, T (G) contains a every pair of vertices (v, u) such that v is not the root note, and u is not a
descendant of v. (Intuitively, we can think of T (G) as the set of pairs of vertices (v, u) such that it is
possible to sever the subtree rooted at v and re-attach that subtree as a child of u.) Next, let

U (V ) := {(G, v, u) : G ∈ G(V ), (v, u) ∈ T (G)}

(14)

25

and equip U (V ) with coordinate projections projG, projv, etc. as in Section O.1. (Intuitively, we
can think of the triples (G, v, u) ∈ U (V ) as denoting a graph G, a choice of vertex v at which to
sever, and a choice of vertex u at which to graft.) Finally, deﬁne the function

g : U (V ) → U (V )

by g(G, v, u) = (G(cid:48), v, u(cid:48)), where (i) u(cid:48) is the parent of v in G, and (ii) G(cid:48) is the graph obtained
from G by removing the edge (u(cid:48), v) and adding the edge (u, v). (By Lemma O.7.1, G(cid:48) is a tree, so
(G(cid:48), v, u(cid:48)) ∈ U (V ).)
Proposition O.3.1. The function g : U (V ) → U (V ) is an involution.

Proof. Let (G(cid:48), v(cid:48), u(cid:48)) := g(G, v, u); let (G(cid:48)(cid:48), v(cid:48)(cid:48), u(cid:48)(cid:48)) := g(G(cid:48), v(cid:48), u(cid:48)); and let E, E(cid:48) and E(cid:48)(cid:48) be the
edge sets of G, G(cid:48) and G(cid:48)(cid:48) respectively. Then, by the deﬁnition of g, we have v(cid:48)(cid:48) = v(cid:48) = v. Also,
because G(cid:48) is a tree that contains the edge (u, v), it follows that u is the parent of v in G(cid:48). Thus
u(cid:48)(cid:48) = u, since u(cid:48)(cid:48) is also (by deﬁnition) the parent of v in G(cid:48). Next, by the deﬁnition of g, we have
E(cid:48) = (E \ {(u(cid:48), v)}) ∪ {(u, v)} and

E(cid:48)(cid:48) = (E(cid:48) \ {(u, v)}) ∪ {(u, v)} = (E \ {(u(cid:48), v)}) ∪ {(u(cid:48), v)} = E

(here we are using the fact that (u(cid:48), v) ∈ E, which holds by the deﬁnition of g). Thus G(cid:48)(cid:48) = G, so
g(g(G, v, u)) = (G, v, u).

Proposition O.3.2. Let G, G(cid:48) ∈ G(V ). Then there exists a sequence of triples

(G0, v0, u0), . . . , (Gk, vk, uk)

satisfying all of the following:

(i) (Gi, vi, ui) ∈ U (V ) for all5 i = 0, . . . , k − 1
(ii) G0 = G
(iii) Gk = G(cid:48)
(iv) for each i < k we have Gi+1 = projG(g(Gi, vi, ui)).

Proof. Let G◦ denote the scene structure that has no relations between objects; that is,

G◦ := (V, {(r, v) : v ∈ V \ {r}}).

We ﬁrst prove the result in the case where G = G◦; then we extend to the general case.
For the case where G = G◦, the intuition is to work backwards from G(cid:48) to G◦ by grafting subtrees
onto r until there are no non-singleton subtrees left. Let G(cid:48) = (V, E(cid:48)), and let (cid:101)E := {(u, v) ∈
E : v (cid:54)= r}. We then take6 k := | (cid:101)E| − 1. We deﬁne vi (from the proposition statement) and u(cid:48)
i (a
variable we’re now introducing) by arbitrarily choosing an ordered enumeration of (cid:101)E and deﬁning
the sequence of pairs (u(cid:48)

k, vk) to equal that enumeration. Thus,

0, v0), . . . , (u(cid:48)

0, v0), . . . , (u(cid:48)
Next, we deﬁne Gi and ui simultaneously by backward recursion: we take Gk := G(cid:48) (and choose uk
arbitrarily; the proposition doesn’t actually say anything about uk); and for 0 ≤ i < k, we deﬁne Gi
and ui by the equation

(cid:101)E = {(u(cid:48)

k, vk)}.

g(Gi+1, vi, r) = (Gi, vi, ui).
(15)
(To justify the left-hand side being well-deﬁned, we must show (Gi+1, vi, r) ∈ U (V ) for all i. By
construction, vi (cid:54)= r for all i, so r /∈ S(Gi+1, v); hence (Gi+1, vi, r) ∈ U (V ).) By applying g to
both sides of (15), we get (iv). Finally, note that by induction, Gi has exactly i edges whose source
node is not r. Thus G0 = G◦. This completes the proof of the case where G = G◦.
We now move to the general case, where G ∈ G(V ) is arbitrary. Using the above special case,
let (G0 = G◦, v0, u0), (G1, v1, u1), . . . , (Gk, vk, uk) be a sequence satisfying (i)–(iv) and (15) with
G0 = G◦. Let (G0, v0, u0), (G1, v1, u1), . . . , (Gk, vk, uk) be a sequence satisfying (i)–(iv) and (15)
5This proposition doesn’t say anything about uk and vk; we leave them in just to simplify the exposition and

notation.

6Except in the degenerate case where (cid:101)E is empty. In that case we have G(cid:48) = G◦, so we simply take k := 0,

G0 := G◦, and u0 and v0 to be any vertices.

26

but with G0 = G◦ and Gk = G. Let Gj := Gk−j for 0 ≤ j < k and Gk := G◦. Substituting
j := k − (i + 1) into (15) gives

(cid:16)

g

Gj, vk−j−1, r

(cid:17)

(cid:16)

=

Gj+1, vk−j−1, uk−j−1

(cid:17)

for each j = 0, . . . , k − 1. Thus, letting k := k and vj := vk−j−1 and uj := r, the sequence

(G0, v0, u0), . . . , (Gk, uk, vk)

satisﬁes (i)–(iv) but with G0 = G and Gk = G◦. Then, the sequence
(G0, v0, u0), . . . , (Gk−1, uk−1, vk−1), (Gk = G0 = G◦, v0, u0), (G1, v1, u1), . . . , (Gk, vk, uk)

satisﬁes (i)–(iv): the only piece of this claim that wasn’t already proved above is (iv) at the concatena-
tion boundary where G◦ appears; i.e., it remains only to check that
G◦ = projG(g(Gk−1, vk−1, uk−1)).

Unpacking the deﬁnitions, this condition is

and indeed the condition is satisﬁed, by (15).

G◦ = projG(g(G1, v0, r)),

For some ﬁxed V with r ∈ V and N := |V | − 1, suppose we have in hand (i) a prior probability
distribution p(G) on G ∈ G(V ); (ii) some data D; and (iii) a likelihood function p(D|G). From these,
we can construct an involutive MCMC [12] kernel on the space of graphs (G(V )) by combining
the involution g deﬁned above with a family of auxiliary probability distributions q(v, u; G) on
V × V such that q(v, u; G) > 0 if and only if (v, u) ∈ T (G). The kernel takes as input a graph G,
samples (v, u) ∼ q(·; G), computes (G(cid:48), v(cid:48), u(cid:48)) := g(G, v, u), and then returns the new graph G(cid:48)
with probability

(cid:26)

min

1,

p(G(cid:48))p(D|G(cid:48))q(v(cid:48), u(cid:48); G(cid:48))
p(G)p(D|G)q(v, u; G)

(cid:27)

(16)

and otherwise returns the previous graph G (i.e. rejects). Because this kernel satisﬁes the requirements
of involutive MCMC, it is stationary with respect to the following target distribution on G(V ):

p(G|D) =

p(G)p(D|G)
G(cid:48)∈G(V ) p(G(cid:48))p(D|G(cid:48))

(cid:80)

(17)

Furthermore, if p(G) > 0 and p(D|G) > 0 for all G ∈ G(V ), then the Markov chain generated by
repeated application of this kernel converges to the target distribution as the number of steps goes
to inﬁnity (the chain is irreducible by Prop. O.3.2 and aperiodic since the proposal has a positive
probability of choosing u to be the parent of v, and in that case G(cid:48) = G).

O.4 Transforming between two alternative 6DoF pose parametrizations

Before deﬁning our full involutive MCMC kernel on scene graphs, we deﬁne a transformation
between continuous parameter spaces that will be used as a building block of the full kernel.

Objects v that are children of the root vertex have their 6DoF pose relative to the world coordinate
frame parametrized directly via θv ∈ SE(3). Recall that the pose of an object v that is child of
another object u(cid:48) is parametrized via the relative pose between a face of u(cid:48) and a face of v. We choose
a parametrization for the pose of one face relative to another that makes it natural to express a prior
distribution in which: (i) the two faces are nearly in ﬂush contact with high probability, and (ii) we
know little about the relative in-plane offset of the two faces and their relative in-plane orientation. A
natural parametrization for this prior uses:

• Two dimensions for the in-plane offset (a ∈ R and b ∈ R) with relatively broad priors
• One dimension for the perpendicular offset (z ∈ R) with a concentrated prior
• An (outward) face normal vector (ν ∈ S2) with a prior that concentrates on anti-parallel

face normals

27

• One dimension of in-plane angular rotation (ϕ ∈ S1) with a uniform prior.

We now deﬁne a bijection

ξ : R3 × SO(3) ⊇ R3 × {ω ∈ SO(3) : ω(0, 0, 1)(cid:62) (cid:54)= (0, 0, −1)}

→ R × R × R × (S2 \ {(0, 0, −1)(cid:62)}) × S1 ⊆ R × R × R × S2 × S1.

(18)

Note that ξ is a.e. bijective on the supersets as well, in the sense that in (18), ξ is a bijection between
the subsets, and each of the subsets has a complement of measure zero in its superset. In ξ, the ﬁrst
three coordinates are copied directly and the orientations are transformed as follows.

In the ﬁrst parameter space, SO(3), orientations are represented “directly,” as the linear transformation
that carries the parent coordinate frame to the child coordinate frame (translated to have the same
origin). The base measure on SO(3) is the Haar measure.
In the second parameter space, S2 × S1, orientations are represented in Hopf coordinates [44]:
intuitively, these coordinates characterize a rotation by where it carries the north pole (0, 0, 1)(cid:62) (we
call this η ∈ S2) and how much planar rotation it does after carrying the north pole to η (we call
this ϕ ∈ S1). However, there is no globally consistent (i.e., jointly continuous in η and ϕ) way to
choose where the rotation corresponding to ϕ “starts” (i.e. which orientations have ϕ = 0)—formally,
the ﬁber bundle induced by the Hopf ﬁbration is not a trivial bundle. But, it is possible to make
these choices consistently on an open subset of S2 × S1 whose complement has measure zero, as
we do in Section O.6.2. In particular, provided that η is not the south pole (0, 0, −1), there is a
unique unit quaternion (w, x, y, z) ∈ S3 that satisﬁes spin(w, x, y, z)(0, 0, 1)(cid:62) = η and has minimal
geodesic distance to the identity (where spin : S3 → SO(3) is the usual covering map, described
in Section O.6.1). Then, spin(w, x, y, z) ∈ SO(3) is the rotation we take to correspond to η = η,
ϕ = 0. The resulting map (S2 \ {(0, 0, −1)(cid:62)}) × S1 → SO(3) is smooth; in Section O.6.2 we
compute the mapping explicitly in terms of coordinates.

O.5 Full involutive MCMC kernel on scene graphs

Our full involutive MCMC kernel on scene graphs (including their parameters θ) is based on an
extension of the involution g deﬁned above. We ﬁrst deﬁne the latent space of pairs (G, θ), as:













X :=

(cid:71)

G∈G(V )








 ×

v∈V \{r}
(r,v)∈E

SE(3)





×




 ×

v∈V \{r}
(r,v)(cid:54)∈E

(F × F × R × R × R × S2 × S1)









(19)

((cid:116) denotes disjoint union), and we endow this set with a reference measure µP formed by sums (over
the disjoint union) of product measures that are composed from: the Lebesgue measure on R, the
Haar measure on SO(3), the uniform (spherical) measures on S2 and S1, and the counting measure
on F × F . Then, we deﬁne the space of auxiliary variables as:

Y := {(v, r) : v ∈ V \ {r}} (cid:116) {(v, u, f, f (cid:48)) : v, u ∈ V \ {r} and f, f (cid:48) ∈ F }

(20)

with the reference measure µQ being the counting measure. We deﬁne an auxiliary probability
distribution q(y; G, θ) such that

q(v, r; G, θ)
> 0
q(v, u, f, f (cid:48); G, θ) > 0
= 0
q(y; G, θ)

for all v ∈ V \ {r}
for all (v, u) ∈ T (G) and all f, f (cid:48) ∈ F
otherwise.

The extended state space of the involutive MCMC kernel is then

Z := {(x, y) ∈ X × Y : p(x)q(y; x) > 0}.

(21)

(22)

We construct an involution h on the space Z using the graph involution g deﬁned above as a building
block. In particular, Z consists of tuples of four forms, and we deﬁne the involution h piecewise
depending on which of these four forms the input has: we take

Z = Z1 (cid:116) Z2 (cid:116) Z3 (cid:116) Z4

28

Figure 13: A subset of the possible transition types for our involutive MCMC kernel on scene
graphs. Left: ‘contact to ﬂoating‘ (forwards) and ‘ﬂoating to contact’ (backwards). Right: ‘contact
to contact‘ (forwards) and ‘contact to contact’ (backwards). The vertex v is the chosen ‘sever’
vertex, and its subtree S(G, v) is shaded. Parameters θv ∈ SE(3), which are the independent
6DoF pose of an object relative to the world coordinate frame, are shown in blue; and parameters
θv ∈ F × F × R × R × R × S2 × S1, which parametrize the pose of an object relative to another
object (speciﬁcally the relative pose between two faces of the two objects) are shown in red.

where the deﬁnitions of Z1, Z2, Z3, Z4, and the parametric form that θv takes on each component,
are as follows:

Z1 := {(G, v, r, θ) : (r, v) ∈ E}
Z2 := {(G, v, r, θ) : (r, v) /∈ E}
Z3 := {(G, v, u, θ, f, f (cid:48)) : u (cid:54)= r, (r, v) ∈ E}
Z4 := {(G, v, u, θ, f, f (cid:48)) : u (cid:54)= r, (r, v) /∈ E}

θv ∈ SE(3)
θv ∈ F × F × R × R × R × S2 × S1
θv ∈ SE(3)
θv ∈ F × F × R × R × R × S2 × S1.

We deﬁne coordinate projections proj• on Z1 and Z2 (for the free parameters G, v, θ), and on Z3
and Z4 (for the free parameters G, u, θ, f, f (cid:48)), as in Section O.1. Also, for notational convenience
below, we extend proju to Z1 and Z2 by deﬁning proju(G, v, r, θ) := r; i.e., proju is constant on
Z1 (cid:116) Z2 with value r.

Then, h is deﬁned piecewise as follows:

h(z) =






hf→f (z)
hc→f (z)
hf→c(z)
hc→c(z)

if z ∈ Z1
if z ∈ Z2
if z ∈ Z3
if z ∈ Z4

(ﬂoating to ﬂoating)
(contact to ﬂoating)
(ﬂoating to contact)
(contact to contact).

(23)

We next deﬁne the function h•→• corresponding to each of these four components, and give the
acceptance probability in each case (the acceptance probabilities will be derived later in this section).

Floating to ﬂoating This transition makes no change to the structure G or the parameters θ:

hf→f (G, v, r, θ) := (G, v, r, θ)

(no change)

(24)

Contact to ﬂoating This transition severs the edge from the parent of v in G (another object) and
replaces it with a new edge from the root r to v in G(cid:48). The parameters of all vertices other than v are
unchanged. The parameters θv are set to the absolute pose (relative to r) of v in (G, θ):

hc→f (G, v, r, θ) := (G(cid:48), v, u(cid:48), θ(cid:48), f, f (cid:48)),

where

(G(cid:48), v, u(cid:48)) = g(G, v, r)

(f, f (cid:48)) = projf,f (cid:48)(θv)
θ(cid:48)
w = θw for w (cid:54)= v
θ(cid:48)
v = xv(G, θ)

(pose of v with respect to r in (G, θ))

Floating to contact This transition severs the edge from the parent of v in G (which is r) and
replaces it with a new edge from another (object) vertex u to v in G(cid:48). The parameters of all vertices
other than v are unchanged. The parameters θv are computed by (i) computing the relative pose

29

∆x(u,f (cid:48))→(v,f )(G, θ) ∈ SE(3) between the face f of object v (oriented according to its outward
normal) and face f (cid:48) of object u in (G, θ), and then (ii) transforming this pose into an element of
R × R × R × S2 × S1 via the function ξ deﬁned in Section O.4:

where

hf→c(G, v, u, θ, f, f (cid:48)) := (G(cid:48), v, r, θ(cid:48)),

(G(cid:48), v, r) = g(G, v, u)

θ(cid:48)
w = θw for w (cid:54)= v
v = (f, f (cid:48), a, b, z, η, ϕ)
θ(cid:48)

(a, b, z, η, ϕ) = ξ(∆x(u,f (cid:48))→(v,f )(G, θ))

Contact to contact This transition severs the edge from the parent of v in G (which is some object
u(cid:48)) and replaces it with a new edge from another object u to v in G(cid:48). The parameters of all vertices
other than v are unchanged. The parameters θv are again computed by ﬁrst computing the relative
pose computing the relative pose ∆x(u,f (cid:48))→(v,f )(G, θ) ∈ SE(3), then applying ξ:

hc→c(G, v, u, θ, f, f (cid:48)) := (G(cid:48), v, u(cid:48), θ(cid:48), f2, f (cid:48)
2)

where

(G(cid:48), v, u(cid:48)) = g(G, v, u)

θ(cid:48)
w = θw for w (cid:54)= v
θ(cid:48)
v = (f, f (cid:48), a, b, z, η, ϕ)
(f2, f (cid:48)
2) = projf,f (cid:48)(θv)
(a, b, z, η, ϕ) = ξ(∆x(u,f (cid:48))→(v,f )(G, θ))

Proposition O.5.1. The function h is an involution.

Proof. First note that in each of the four cases (z ∈ Zi for i = 1, 2, 3, 4), we have

Therefore, since g is an involution, we have

projG,v,u(h(z)) = g(projG,v,u(z)).

projG,v,u(h(h(z))) = g(projG,v,u(h(z))) = g(g(projG,v,u(z))) = projG,v,u(z).

Next, note that if z ∈ Z2 (cid:116) Z4, then projf,f (cid:48)(h(h(z))) = projf,f (cid:48)(z) simply by unraveling the
deﬁnitions.

It remains to show that projθ(h(h(z))) = projθ(z). This clearly holds when z ∈ Z1, as hf→f is the
identity.
For the case z ∈ Z2, let z = (G, v, r, θ), and let f, f (cid:48) := projf,f (cid:48)(θv), and let G and u be such that
g(G, v, r) = (G(cid:48), v, u(cid:48)). Then, unraveling the deﬁnitions, we have
(cid:26)w (cid:55)→ θw for w (cid:54)= v

(cid:27)

projθ(h(z)) =

v (cid:55)→ xv(G, θ)

.

Unraveling the deﬁnitions one step further, we have

projθ(h(h(z))) =





w (cid:55)→ θw for w (cid:54)= v

(cid:18)

v (cid:55)→ (f, f (cid:48), ξ

∆x(u(cid:48),f (cid:48))→(v,f )

(cid:18)

G(cid:48),

(cid:26)w (cid:55)→ θw for w (cid:54)= v

(cid:27)(cid:19)(cid:19)

v (cid:55)→ xv(G, θ)

(cid:124)

(cid:123)(cid:122)

:= θ(cid:48)(cid:48)

v := (a(cid:48),b(cid:48),z(cid:48),η(cid:48),ϕ(cid:48))



)


(cid:125)

.

So we need to show θ(cid:48)(cid:48)
v is the contact-parametrized relative pose for v
(face f ) relative to u(cid:48) (the parent of v in G, face f (cid:48)) that, when converted to an absolute (relative to r)
pose in the scene graph (G, θ), gives xv(G, θ). In other words, indeed θ(cid:48)(cid:48)

v = θv. By construction, θ(cid:48)(cid:48)

v = θv.

30

For the case z ∈ Z3, let z = (G, v, u, θ, f, f (cid:48)) and G(cid:48) := projG(g(G, v, u)). Unraveling two layers
of deﬁnitions similarly to above, we have

projθ(h(h(z))) =





G(cid:48),

v (cid:55)→ xv

(cid:124)

w (cid:55)→ θw for w (cid:54)= v
(cid:18)

(cid:26)w (cid:55)→ θw for w (cid:54)= v

v (cid:55)→ ∆x(u,f )→(v,f (cid:48))(G, θ)

(cid:123)(cid:122)
:= θ(cid:48)(cid:48)
v



(cid:27)(cid:19)

(cid:125)

.

v = θv. By construction, θ(cid:48)(cid:48)

It again sufﬁces to show θ(cid:48)(cid:48)
v is the pose in world frame (relative to r) of
object v in G(cid:48); and the contact-parametrized relative pose of v (face f ) relative to u (face f (cid:48)) in G(cid:48)
by construction has the property that, when converted to an absolute pose in (G, θ), the result is θv.
Thus θ(cid:48)(cid:48)
For the case z ∈ Z4, let z = (G, v, u, θ, f, f (cid:48)), and let (f2, f (cid:48)
such that (G(cid:48), v, u(cid:48)) = g(G, v, u). Unraveling two layers of deﬁnitions again, we have

2) := projf,f (cid:48)(θv), and let G(cid:48) and u(cid:48) be

v = θv.

projθ(h(h(z))) =






w (cid:55)→ θw for w (cid:54)= v

(cid:32)

v (cid:55)→ (f, f (cid:48), ξ

∆x(u(cid:48),f (cid:48)

2)→(v,f2)

(cid:32)

G(cid:48),

(cid:124)

(cid:40)w (cid:55)→ θw for w (cid:54)= v

(cid:123)(cid:122)

:= θ(cid:48)(cid:48)

v := (a(cid:48),b(cid:48),z(cid:48),η(cid:48),ϕ(cid:48))

v (cid:55)→ (f, f (cid:48), ξ (cid:0)∆x(u,f (cid:48))→(v,f )(G, θ)(cid:1) , f, f (cid:48))

(cid:41)(cid:33)(cid:33)
)






(cid:125)

.

v = θv. Similarly to the above, θ(cid:48)(cid:48)

It again sufﬁces to show θ(cid:48)(cid:48)
v is a contact-parametrized relative pose
for v (face f ) relative to u(cid:48) (the parent of v in G, face f (cid:48)) deﬁned by the property that it produces the
same absolute pose for v as a second contact-parametrized relative pose. This second relative pose is
for v (face f2) relative to u(cid:48) (face f (cid:48)
2) that by construction produces the same absolute pose as v has
in (G, θ). It again follows that θ(cid:48)(cid:48)

v = θv, and this completes the proof.

The automated involutive MCMC implementation in Gen [13] includes an optional dynamic check
that applies the involution twice to check that it is indeed an involution. We applied this check during
testing of the algorithm to gain conﬁdence in our implementation.

O.6 The Radon–Nikodym derivative

The acceptance ratio for involutive MCMC [12] includes a “generalized Jacobian correction” term,
equal to the Radon–Nikodym derivative of a pushforward measure µ∗ with respect to a base measure
µ deﬁned on the state space Z (µ is constructed the product measure of µP and µQ [12]). Next,
µ∗ := µ ◦ h−1 is the pushforward of µ by the involution h : Z → Z described above. To justify the
validity of this involutive MCMC kernel, we must show that µ∗ is absolutely continuous with respect
to µ, i.e., that the Radon–Nikodym derivative dµ∗
dµ exists. Because all the discrete choices in the model
(graph structure, contact faces, etc.) are assigned positive probability mass in both the model and the
proposal, it sufﬁces to show absolute continuity for the continuous part of the involution: the mapping
(call it (cid:96)◦) that, for given contact faces f, f (cid:48) ∈ F , converts between a 6DoF pose θ1 ∈ SE(3) and a
contact-parameterized relative pose θ2 = (a, b, z, η, ϕ) ∈ R × R × R × S2 × S1 (note that in this
section we use θ2 to denote only the continuous part of the contact-parameterized relative pose).
Note that (cid:96)◦ depends on not just θ1, but also on the scene graph, objects and faces (G, θ, v, u(cid:48), f, f (cid:48)).
Speciﬁcally, the absolute pose θ1 is gotten by pre- and post-composing ∆x(u(cid:48),f (cid:48))→(v,f )(G, θ) with
rigid motions that depend on the absolute poses of face f (cid:48) of u and face f of v in scene graph (G, θ),
but these rigid motions do not depend on θ1 or θ2 themselves. Thus, in the sections below, rather
than (cid:96)◦ itself, we analyze (cid:96), the variant of (cid:96)◦ which operates on 6DoF relative poses ∆x where (cid:96)◦
operates on 6DoF absolute poses x. Because rigid transformations are diffeomorphisms and their
Radon–Nikodym derivatives (Jacobian determinants) are identically 1, the results in the sections
below, which show that (cid:96) has a Radon–Nikodym derivative that is piecewise constant on A (cid:116) B
(deﬁned below), apply equally well to the map (cid:96)◦ which parameterizes θ1 relative to the world
coordinate frame in some particular scene graph.
We can denote a rigid motion by the pair (t, ω), where t ∈ R3 is the translation component and
ω ∈ SO(3) is the rotation component. (In algebraic terms, we are identifying SE3 with the semidirect

31

product R3 (cid:111) SO(3).) Accordingly, deﬁne projection functions projt and projω on SE(3) as in
Section O.1.
Let Z := A (cid:116) B where A := R3 × SO(3) and B := R × R × R × S2 × S1, and let ν denote
the base measure on Z.7 In the sections below, we discuss the pushforward ν∗ := ν ◦ (cid:96)−1 and its
Radon–Nikodym derivative dν∗
dν .

O.6.1 Existence of the Radon–Nikodym derivative

In this section, we prove ν∗ (cid:28) ν. Because ν∗ = ν ◦ (cid:96)−1, the proof proceeds by analyzing the
involution (cid:96). First we show that (cid:96) is deﬁned almost everywhere, so that ν∗ is well-deﬁned. Then we
show that there exists a subset Z (cid:48)(cid:48) ⊆ Z whose complement has measure zero, such that the restriction
of (cid:96) to Z (cid:48)(cid:48) is a diffeomorphism. It follows that ν∗ (cid:28) ν by [29, Prop 6.5], since (cid:96)−1 is a smooth map.
First, to show that ν∗ is well-deﬁned, we show that (cid:96) is deﬁned almost everywhere on Z. Indeed, the
domain of (cid:96) is A(cid:48) (cid:116) B(cid:48), where A(cid:48) := R3 × domain(ξ) and B(cid:48) := R × R × R × domain(ξ−1) (where
ξ is as deﬁned in Section O.4). Now, domain(ξ) = {ω ∈ SO(3) : ω(0, 0, 1)(cid:62) (cid:54)= (0, 0, −1)(cid:62)} has
a complement of measure zero in SO(3), and domain(ξ−1) = (S2 \ {(0, 0, −1)(cid:62)}) × S1 has a
complement of measure zero in S2 × S1, so indeed domain((cid:96)) = A(cid:48) (cid:116) B(cid:48) has a complement of
measure zero in Z.
Next, we show that there exist subsets A(cid:48)(cid:48) ⊆ A(cid:48), B(cid:48)(cid:48) ⊆ B(cid:48), whose complements also have measure
zero, such that (cid:96) ﬁxes A (cid:116) B setwise and the restriction of (cid:96) to A(cid:48)(cid:48) (cid:116) B(cid:48)(cid:48) is a diffeomorphism. First,
note that the coordinates t in A and the coordinates a, b, z in B represent the same translation in a
different coordinate frame. Thus, for any ﬁxed ω ∈ SO(3), the function t (cid:55)→ proja,b,z((cid:96)(t, ω)) is a
rigid motion, hence a diffeomorphism. Furthermore, the rotation component of (cid:96)(θ) (regardless of
whether θ ∈ A or θ ∈ B) depends only on the rotation component of θ, not at all on the translation
component. Thus, (cid:96) is a diffeomorphism from A(cid:48)(cid:48) to B(cid:48)(cid:48) if and only if (cid:96)(cid:63) is a diffeomorphism
from projω(A(cid:48)(cid:48)) to projη,ϕ(B(cid:48)(cid:48)), where (cid:96)(cid:63)(ω) := projη,ϕ((cid:96)(0, ω)). Taking A(cid:48)(cid:48) := R3 × A(cid:63) and
B(cid:48)(cid:48) := R × R × R × B(cid:63), we see that it sufﬁces to ﬁnd subsets A(cid:63) ⊆ SO(3) and B(cid:63) ⊆ S2 × S1
whose complements have measure zero, such that the restriction of (cid:96)(cid:63) to A(cid:63) is a diffeomorphism onto
B(cid:63).
Denote elements of SO(3) as ω = spin(w, x, y, z), where spin : S3 → SO(3) is the 2-to-1 smooth
covering map that carries a unit quaternion w + xi + yj + zk to its corresponding rotation. Then, we
take

A(cid:63) = (cid:8)spin(w, x, y, z) (cid:12)
B(cid:63) = (cid:8)((a, b, c), ϕ) ∈ S2 × S1 (cid:12)

(cid:12) (w, x, y, z) ∈ S3; z (cid:54)= 0(cid:9)
(cid:12) c (cid:54)= ±1(cid:9)

To show that (cid:96) is a diffeomorphism from A(cid:63) to B(cid:63), we give an explicit formula for (cid:96)(cid:63) in terms of
coordinates below (Section O.6.2).

O.6.2 Formula for the mapping in coordinates

In this section we give an explicit formula for the map (cid:96)(cid:63) deﬁned in SectionO.6.1 in terms of
coordinates. For elements ω = spin(w, x, y, z) ∈ A(cid:63), the S2 component of (cid:96)(cid:63)(ω) is the image of
(0, 0, 1)(cid:62) under the rotation, and is given by [20, §8.2]:




η = spin(w, x, y, z)

(cid:33)
(cid:32)0
0
1

=



2(xz + wy)
2(yz − wx)
1 − 2(x2 + y2)

 .

Even though (w, x, y, z) is not uniquely determined by spin(w, x, y, z), the above expression is
well-deﬁned because both possible choices of quaternion—(w, x, y, z) and (−w, −x, −y, −z)—give
the same value for the right-hand side.
To compute the S1 component ϕ, note that the set of all rotations that carry (0, 0, 1)(cid:62) to η is

{spin(w, x, y, z) ◦ R(0,0,1)(−ϕ(cid:48)) : 0 ≤ ϕ(cid:48) < 2π},

7That is, the sum of (i) the product of Lebesgue measure on R3 and Haar measure on SO(3), and (ii) the
product of Lebesgue measure on R × R × R, spherical uniform measure on S2, and uniform spherical measure
on S1.

32

where R(0,0,1)(ϕ(cid:48)) is a rotation about the axis (0, 0, 1)(cid:62) by angle ϕ(cid:48). Because the action of S3 on
itself by quaternion multiplication is a geometric rotation of S3 (in particular, an isometry) [20, §8.3],
minimizing geodesic distance among the above family of rotations is equivalent to minimizing (over
ϕ(cid:48)) geodesic distance from R(0,0,1)(ϕ(cid:48)) to (w, x, y, z). Explicitly, R(0,0,1)(ϕ(cid:48)) corresponds to the
unit quaternions

± (cos(ϕ(cid:48)/2), 0, 0, sin(ϕ(cid:48)/2)) .

Note that minimizing geodesic distance on the sphere S3 is equivalent to minimizing the cosine
between the corresponding vectors in S3 ⊆ R4. By the cosine double angle formula, the cosine
between R(0,0,1)(ϕ(cid:48)) and (w, x, y, z) in this sense is

(cid:16)

2

± (cos(ϕ(cid:48)/2), 0, 0, sin(ϕ(cid:48)/2)) · (w, x, y, z)

(cid:17)2

− 1,

where · denotes the dot product in R4. This quantity is maximized precisely when the doct product is
either maximized or minimized, so we can drop the ±. We can then compute the minima and maxima
of the dot product by setting the derivative equal to zero: we have

(cos(ϕ(cid:48)/2), 0, 0, sin(ϕ(cid:48)/2)) · (w, x, y, z) = w cos(ϕ(cid:48)/2) + z sin(ϕ(cid:48)/2)

and the above expression is minimized or maximized when

ϕ(cid:48)/2 = arctan(z/w) + πn for some n ∈ Z

or equivalently, ϕ(cid:48) = 2 arctan(z/w) + 2πn. Thus, the S1 component of (cid:96)(cid:63)(spin(w, x, y, z)) that we
set out to compute is

ϕ = 2 arctan(z/w),
where the branch cut in arctan is chosen so that the output lies in the interval [0, π), and we allow
z/w to lie on the extended real line, with arctan(±∞) = π/2.
Since z (cid:54)= 0 in A(cid:63), ϕ never lands on the branch cut. Thus, (cid:96)(cid:63)(spin(w, x, y, z)) is a smooth function
of the quaternion (w, x, y, z). Because spin is a smooth covering map [29, ch. 4], it follows8 that
spin is also a smooth function of the element ω = spin(w, x, y, z) ∈ SO(3).

The above deﬁnition expresses in coordinates the geometry of Hopf ﬁbration and choice of branch
cut described in Section O.4. We now need only show that (cid:96)(cid:63) has a smooth inverse. For elements
(η, ϕ) ∈ B(cid:63), we take [4, §7]

((cid:96)(cid:63))−1 ((a, b, c), ϕ) =

(cid:18)

spin

1√

2(1+c)

((1 + c) cos(ϕ), a sin(ϕ) − b cos(ϕ), a cos(ϕ) + b sin(ϕ), (1 + c) sin(ϕ))

.

(cid:19)

This function is clearly smooth on B(cid:63), and direct computation shows that ((cid:96)(cid:63))−1 ◦ (cid:96)(cid:63) is the identity.

O.6.3 Value of the Radon–Nikodym derivative

In the preceding sections we showed that ν∗ := ν ◦ (cid:96)−1 has a density with respect to ν, the Radon–
Nikodym derivative ρ := dν∗
dν . In this section we argue that ρ is (a.e.) constant on each of the
connected components A and B. Because h acts as isometries on the translation components, and
acts on rotation components in a way that doesn’t depend on the translation components, we need
only look at orientation components. That is, the Radon–Nikodym derivative ρ is equal to the
Radon–Nikodym derivative of the pushforward ν(cid:63)

dν∗

dν (t, ω) = dν(cid:63)

dν(cid:63) (ω)

∗

and

∗ of the base measure9 ν(cid:63) on A(cid:63) (cid:116) B(cid:63) by (cid:96)(cid:63):
dν (a, b, z, η, ϕ) = dν(cid:63)

dν(cid:63) (η, ϕ).

dν∗

∗

Note the following chain of equivalences: dν(cid:63)
the Haar measure on A(cid:63) ⇐⇒ ν(cid:63)

∗ is a scalar multiple of
∗ is invariant under the action of SO(3) on itself by multiplication.

dν(cid:63) is a.e. constant on A(cid:63) ⇐⇒ ν(cid:63)

∗

8This can be seen by pre-composing h(cid:48)(cid:48) with a lifting to one of the sheets in an evenly covered neighborhood

of ω.

9In this case, sum of the Haar measure on A(cid:63) ⊆ SO(3) and the product of uniform measures on B(cid:63) ⊆

S2 × S1.

33

We can see that the latter statement holds by noting three things: First, the Haar measure on SO(3)
equals the pushforward by spin of the Haar measure on unit quaternions. Next, the action of S3
on itself by group multiplication is an action by isometries [20, §8.3]. Finally, by [44], the volume
element on S3 equals the product of the volume elements on S2 and S1 (here we are using the fact
that the Haar measure on S3 coincides with the Borel measure when it is viewed as a Riemannian
manifold). Thus the action of SO(3) on itself by multiplication, when pushed through ((cid:96)(cid:63))−1,
becomes an action by local isometries on S2 × S1, and is thus invariant under the base measure ν(cid:63)
(which on B(cid:63) is the product of uniform measures). Therefore indeed dν(cid:63)
dν(cid:63) is a.e. constant on A(cid:63).
Since (cid:96)(cid:63) is an involution and (cid:96)(cid:63)(A(cid:63)) = B(cid:63), it follows that dν(cid:63)
dν(cid:63) is a.e. constant on B(cid:63), and the values
of the Radon–Nikodym derivative on A(cid:63) and B(cid:63) are reciprocals of each other.

∗

∗

O.6.4 Acceptance probability

For our choice of scaling constants, if we assign total measures SO(3) (cid:55)→ π2, S2 (cid:55)→ 4π, and
S1 (cid:55)→ 2π, then the Radon–Nikodym derivative corrections in the involutive MCMC acceptance
probability [12] are: 1 (for ‘ﬂoating to ﬂoating’ and ‘contact to contact’ moves), (4π · 2π)/π2 = 8
(for a ‘ﬂoating to contact’ move), and π2/(4π · 2π) = 1/8 (for a ‘contact to ﬂoating’ move). This
gives the following acceptance probabilities for each of the four possible proposed moves from (G, θ)
to (G(cid:48), θ(cid:48)) that can be proposed within our kernel:

Floating to ﬂoating When u = r and u(cid:48) = r, the state is unchanged, and the move always accepts:

(cid:26)

α = min

1,

p(Y|N, c, G, θ)
p(Y|N, c, G, θ)

(cid:27)

= 1

(25)

Floating to contact When u (cid:54)= r and u(cid:48) = r (we are severing v from the root and grafting v onto
another object), the acceptance probability is:

(cid:26)

α = min

1,

p(Y|N, c, G(cid:48), θ(cid:48))
p(Y|N, c, G, θ)

p(θ(cid:48)|N, c, G(cid:48))
p(θ|N, c, G)

q(v(cid:48), u(cid:48); G(cid:48))

q(v, u, f1, f (cid:48)

1; G)

(cid:27)

· 8

(26)

where (f1, f (cid:48)

1) = projf,f (cid:48)(θ(cid:48)

v).

Contact to ﬂoating When u = r and u(cid:48) (cid:54)= r (we are severing v from an object and grafting v onto
the root), the acceptance probability is:

(cid:26)

α = min

1,

p(Y|N, c, G(cid:48), θ(cid:48))
p(Y|N, c, G, θ)

p(θ(cid:48)|N, c, G(cid:48))
p(θ|N, c, G)

q(v(cid:48), u(cid:48), f2, f (cid:48)
q(v, u; G)

2; G(cid:48))

·

1
8

(cid:27)

(27)

where (f2, f (cid:48)

2) = projf,f (cid:48)(θv).

Contact to contact When u (cid:54)= r and u(cid:48) (cid:54)= r (we are severing v from an object and grafting v onto
another object), the acceptance probability is:

(cid:26)

α = min

1,

p(Y|N, c, G(cid:48), θ(cid:48))
p(Y|N, c, G, θ)

p(θ(cid:48)|N, c, G(cid:48))
p(θ|N, c, G)

q(v(cid:48), u(cid:48), f2, f (cid:48)
q(v, u, f1, f (cid:48)

2; G(cid:48))
1; G)

(cid:27)

(28)

where (f1, f (cid:48)

1) = projf,f (cid:48)(θ(cid:48)

v) and (f2, f (cid:48)

2) = projf,f (cid:48)(θv).

O.7 Lemmas

Lemma O.7.1. Let G = (V, E) be a directed tree rooted at r ∈ V , and suppose u, u(cid:48), v ∈ V are
such that the following conditions hold:

(i) u (cid:54)= v
(ii) u is not a descendant of v
(iii) (u(cid:48), v) ∈ E.

Let G(cid:48) be the directed graph obtained from G by deleting the edge (u(cid:48), v) and adding the edge (u, v),
that is, G(cid:48) = (V, (E \ {(u(cid:48), v)}) ∪ {(u, v)}). Then G(cid:48) is a directed tree rooted at r.

34

Proof. We show that for any vertex w, there is a unique path in G(cid:48) from r to w.
First, suppose w is not a descendant of v in G(cid:48). Then w is not a descendant of v in G, since the set of
descendants of v is the same in G and G(cid:48). Thus no path from r to w in either G or G(cid:48) passes through
v; consequently, no path from r to w in either G or G(cid:48) contains either of the edges (u(cid:48)v) or (u, v).
Thus, a sequence of vertices r = x0, x1, . . . , xn = w is a path in G(cid:48) if and only if it is a path in G.
Since there is a unique path from r to w in G, it follows that there is a unique path from r to w in G(cid:48).
Next, suppose w is a descendant of v in G(cid:48) (and hence also in G). Because u(cid:48) is the only in-neighbor
of v in G, it follows that u is the only in-neighbor of v in G(cid:48). Thus, every path from r to w in G(cid:48) is
the concatenation of a path from r to u with a path from v to w. But since u is not a descendant of v
in G (hence neither in G(cid:48)), there is a unique path from r to u in G(cid:48), by the above paragraph. And of
course, there is a unique path from v to w in G, hence also in G(cid:48) since the subtrees rooted at v are the
same. Therefore there is a unique path from v to w in G(cid:48).

35

