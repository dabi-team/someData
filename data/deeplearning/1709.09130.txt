Output Range Analysis for Deep Neural Networks

Souradeep Dutta ∗, Susmit Jha(cid:63), Sriram Sanakaranarayanan†, Ashish Tiwari‡
∗souradeep.dutta@colorado.edu, (cid:63)susmit.jha@sri.com, † srirams@colorado.edu, ‡ tiwari@csl.sri.com

7
1
0
2

p
e
S
6
2

]

G
L
.
s
c
[

1
v
0
3
1
9
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Deep neural networks (NN) are extensively used for machine
learning tasks such as image classiﬁcation, perception and
control of autonomous systems. Increasingly, these deep NNs
are also been deployed in high-assurance applications. Thus,
there is a pressing need for developing techniques to verify
neural networks to check whether certain user-expected prop-
erties are satisﬁed. In this paper, we study a speciﬁc veriﬁca-
tion problem of computing a guaranteed range for the output
of a deep neural network given a set of inputs represented as
a convex polyhedron. Range estimation is a key primitive for
verifying deep NNs. We present an efﬁcient range estimation
algorithm that uses a combination of local search and lin-
ear programming problems to efﬁciently ﬁnd the maximum
and minimum values taken by the outputs of the NN over the
given input set. In contrast to recently proposed “monolithic”
optimization approaches, we use local gradient descent to re-
peatedly ﬁnd and eliminate local minima of the function. The
ﬁnal global optimum is certiﬁed using a mixed integer pro-
gramming instance. We implement our approach and com-
pare it with Reluplex, a recently proposed solver for deep
neural networks. We demonstrate the effectiveness of the pro-
posed approach for veriﬁcation of NNs used in automated
control as well as those used in classiﬁcation.

Introduction
Deep neural networks have emerged as a versatile and pop-
ular representation model for machine learning due to their
ability to approximate complex functions and the efﬁciency
of methods for learning these from large data sets. The black
box nature of NN models and the absence of effective meth-
ods for their analysis has conﬁned their use in systems with
low integrity requirements but more recently, deep NNs are
also been adopted in high-assurance systems such as auto-
mated control and perception pipeline of autonomous vehi-
cles (Kahn et al. 2016). While traditional system design ap-
proaches include rigorous system veriﬁcation and analysis
techniques to ensure the correctness of systems deployed in
safety-critical applications (Baier and Katoen 2008), the in-
clusion of complex machine learning models in the form of
deep NNs has created a new challenge to verify these mod-
els. In this paper, we focus on the range estimation problem,
wherein, given a neural network N and a polyhedron φ(x)

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

representing a set of inputs to the network, we wish to esti-
mate a range range(li, φ) for each of the network’s output
li that subsumes all possible outputs and is “tight” within a
given tolerance δ. We restrict our attention to feedforward
deep NNs. Furthermore, the NNs are assumed to use only
rectiﬁed linear units (ReLUs) (LeCun, Bengio, and Hinton
2015) as activation functions. Adapting to other activation
functions will be discussed in an extended version.

Despite these restrictions, the range estimation problem
has several applications. Consider a deep NN classiﬁca-
tion model with the last layer neural units, before the soft-
max computation to determine the class label, denoted by
l0, l1, . . . , lk. Given an image i, lj(i) denotes the value of the
j-th last layer neural unit for the input i and poly(i) denote
a compact polyhedron region around i modeling perturba-
tions of the image i. The range of label li for the polyhedron
input poly(i) is denoted by range(li, poly(i)). Label j is
possible for a perturbation of input in poly(i) if and only if
(range(l0) × range(l1) × . . . × range(lk)) ∩ {∀k lk ≤ lj}
is not empty. Hence, we can verify if a label j is possible
for a given set of perturbations of image i by solving the
range estimation problem. This will establish the robustness
of the deep NN classiﬁcation models. Another application of
the range estimation problem is to prove the safety of deep
NN controllers by proving bounds on the outputs generated
by these models. This is important because out of bounds
outputs can drive the physical system into undesirable con-
ﬁgurations such as the locking of robotic arm, or command
a car’s throttle beyond its rated limits. Finding these er-
rors through veriﬁcation will enable design-time detection
of potential failures instead of relying on runtime monitor-
ing which can have signiﬁcant overhead and also may not
allow graceful recovery. Additionally, range analysis can be
useful in proving the safety of the overall system.

Related Work The importance of analytical certiﬁcation
methods for neural networks has been well-recognized in
literature. (Kurd and Kelly 2003) present one of the ﬁrst
categorization of veriﬁcation goals for NNs used in safety-
critical applications. The proposed approach here targets cri-
teria G4 and G5 in (Kurd and Kelly 2003) which aim at en-
suring robustness of NNs to disturbances in inputs, and en-
suring the output of NNs are not hazardous. The veriﬁcation
of neural networks is a hard problem, and even proving sim-

 
 
 
 
 
 
ple properties about them is known to be NP-complete (Katz
et al. 2017). The nonlinearity and non-convexity of deep
NNs make their analysis very difﬁcult.

There has been few recent results reported for verifying
neural networks. A methodology for the analysis of ReLU
feed-forward networks is studied in (Katz et al. 2017). Their
approach relies on Satisﬁability Modulo Theory (SMT)
solving (Barrett et al. 2009), while we only use a combina-
tion of gradient-based local search and MILP solving (Bixby
2012). The linear programming used for comparison in Re-
luplex (Katz et al. 2017) performs signiﬁcantly less efﬁ-
ciently according to the experiments reported in this paper.
Note, however, that the scenarios used by Katz et al. are dif-
ferent from those studied here, and are not publicly avail-
able for comparison. A related goal of ﬁnding adversarial
inputs for deep NNs has received a lot of attention, and can
be viewed as a testing approach to NNs instead of veriﬁca-
tion method discussed in this paper. A linear programming
based approach for ﬁnding adversarial inputs is presented
in (Bastani et al. 2016). A related approach for ﬁnding ad-
versarial inputs using SMT solvers that relies on a layer-by-
layer analysis is presented in (Huang et al. 2016). The use
of SMT solvers for analysis of NNs has also been studied
in (Pulina and Tacchella 2012). In contrast, our goal is to
not just ﬁnd adversarial or failing inputs that violate some
property of the output but instead, we aim at establishing
the guaranteed range of the output for a given polyhedral
region of possible inputs. An abstraction-reﬁnement based
iterative approach for veriﬁcation of NNs was proposed in
(Pulina and Tacchella 2010). It relies on abstracting NNs as a
Boolean combinations of linear arithmetic SMT constraints
that is guaranteed to be conservative. Spurious counterex-
amples arising due to abstraction are used to reﬁne the en-
coding. Note that our approach can ﬁt well inside such an
abstraction-reﬁnement framework.

Contributions We present a novel algorithm for prop-
agating convex polyhedral inputs through a feedforward
deep neural network with ReLU activation units to estab-
lish ranges s for the outputs of the network. We have imple-
mented our approach in a tool called SHERLOCK. We com-
pare SHERLOCK with a recently proposed deep NN veriﬁ-
cation engine - Reluplex (Katz et al. 2017). We demonstrate
the application of SHERLOCK to establish output range of
deep NN controllers as well as to prove the robustness of
deep NN image classiﬁcation models. Our approach seems
to scale consistently to randomly generated sparse networks
with up to 250 neurons and random dense networks with
up to 100 neurons. Furthermore, we demonstrate its applica-
tions on neural networks with as many as 1500 neurons.

Preliminaries
We present the preliminary notions including deep neural
networks, polyhedra, and mixed integer linear programs.

Deep Neural Networks
We will study feed forward neural networks (NN) using so-
called “ReLU” units throughout this paper with n > 0 inputs
and a single output. Let x ∈ Rn denote the inputs and y ∈ R

...

...

x1

xn

...

...

· · ·

· · ·
. . .
· · ·
· · ·
. . .
· · ·
y

Figure 1: Feedforward Neural Network with ReLU Units.

be the output of the network. Structurally, a NN N consists
of k > 0 hidden layers wherein we assume (for simplicity)
that each layer has N > 0 neurons. We use Nij to denote
the jth neuron of the ith layer for j ∈ [1, N ] and i ∈ [1, k].
A k layer neural network with N neurons in each hidden

layer is described by a set of matrices:

(W0, b0), . . . , (Wk−1, bk−1), (Wk, bk) ,
wherein (a) W0, b0 are N × n and N × 1 matrices denoting
the weights connecting the inputs to the ﬁrst hidden layer,
(b) Wi, bi for i ∈ [1, k − 1] connect layer i to layer i + 1
and (c) Wk, bk connect the last layer k to the output.
Deﬁnition 1.1 (ReLU Unit). Each neuron in the network
implements a nonlinear function σ linking its input value to
the output value. In this paper, we consider ReLU units that
implement the function σ(z) : max(z, 0).

We extend the deﬁnition of σ to apply component-wise to




vectors z as σ(z) :




σ(z1)
...
σ(zn)


.

Taking σ to be the ReLU function, we describe the overall

function deﬁned by a given network N .
Deﬁnition 1.2 (Function Computed by NN). Given a neural
network N as described above, the function F : Rn → R
computed by the neural network is given by the composition
F := Fk ◦ · · · ◦ F0 wherein Fi(z) : σ(Wiz + bi) is the func-
tion computed by the ith hidden layer with F0 denoting the
function linking the inputs to the ﬁrst layer and Fk linking
the last layer to the output.

For a ﬁxed input x, we say that a neuron Nij is activated
if the value input to it is nonnegative. This corresponds to
the output of the ReLU unit being the same as the input. It
is easily seen that the function F computed by a NN N is
continuous and piecewise afﬁne, and differentiable almost
everywhere in Rn. If it exists, we denote the gradient of this
function ∇F : (∂x1F, . . . , ∂xn F ). Computing the gradient
can be performed efﬁciently (as described subsequently).

Mixed Integer Linear Programs
Throughout this paper, we will formulate linear optimiza-
tion problems with integer variables. We brieﬂy recall these

optimization problems, their computational complexity and
solution techniques used in practice.
Deﬁnition 1.3. A mixed integer linear program (MILP) in-
volves a set of real-valued variables x and integer valued
variables w of the following form:

u∗

u1

max

aT x + bT w
s.t. Ax + Bw ≤ c

x ∈ Rn
w ∈ Zm

The problem is called a linear program (LP) if there are no
integer variables w. The special case wherein w ∈ {0, 1}m
is called a binary MILP. Finally, the case without an explicit
objective function is called a MILP feasibility problem. It
is well known that MILPs are NP-hard problems: the best
known algorithms, thus far, have exponential time complex-
ity in the worst case. In contrast, LPs can be solved efﬁ-
ciently using interior point methods.

Problem Deﬁnition
Let N be a neural network with n inputs x, a single out-
put y and weights (cid:104)(W0, b0), . . . , (Wk, bk)(cid:105). Let FN be the
function deﬁned by such a network.
Deﬁnition 1.4 (Range Estimation Problem). The range es-
timation problem is deﬁned as follows:

• INPUTS: Neural network N , input constraints P : Ax ≤

b and a tolerance parameter δ > 0.

• OUTPUT: An interval [(cid:96), u] such that (∀ x ∈ P ) FN (x) ∈
[(cid:96), u]. I.e, [(cid:96), u] contains the range of FN over inputs x ∈
P . Furthermore, we require the interval to be tight:

(max
x∈P

FN (x) ≥ u − δ), (min
x∈P

FN (x) ≤ (cid:96) + δ) .

We will assume that the input polyhedron P is compact:

i.e, it is closed and has a bounded volume.

Overall Approach
Without loss of generality, we will focus on estimating the
upper bound u. The case for the lower bound will be entirely
analogous. First, we note that a single MILP can be used to
directly compute u, but can be quite expensive in practice.
Therefore, our approach will combine a series of MILP fea-
sibility problems alternating with local search steps.

Figure 2 shows a pictorial representation of the overall
approach. The approach incrementally ﬁnds a series of ap-
proximations to the upper bound u1 < u2 < · · · < u∗,
culminating in the ﬁnal bound u = u∗.

1. The ﬁrst level u1 is found by choosing a randomly sam-

pled point x0 ∈ P .

2. Next, we perform a series of local iteration steps resulting
in samples x1, . . . , xi that perform gradient ascent until
these steps cannot obtain any further improvement. We
take u1 = FN (xi).

3. A “global search” step is now performed to check if there
is any point x ∈ P such that FN (x) ≥ u1 + δ. This is
obtained by solving a MILP feasibility problem.

L6

L5

G2

L4

L3

u2

L1

L2

G1

x0

x1 x2

x3x4x5

x8

x7

x6

Figure 2: A schematic ﬁgure showing our approach show-
ing alternating series of local search L1, . . . , L6 and “global
search” G1, G2 iterations. The points x2, x5, x8 represent
local minima wherein our approach transitions from local
search iterations to global search iterations.

4. If the global search fails to ﬁnd a solution, then we declare

u∗ = u1 + δ.

5. Otherwise, we obtain a new witness point xi+1 such that

FN (xi+1) ≥ u1 + δ.

6. We now go back to the local search step.

x ← Sample(P )
terminate ← false
while not terminate do

Algorithm 1 Estimate maximum value u for a neural net-
work N over a range x ∈ P with tolerance δ > 0.
1: procedure FINDUPPERBOUND(N , P , δ)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end procedure

(x, u) ← LocalSearch(N , x, P )
u ← u + δ
(x(cid:48), u(cid:48), f easible) ← GlobalSearch(N , u, P )
if feasible then

end if
end while
return (x, u)

(x, u) ← (x(cid:48), u(cid:48))

terminate ← true

else

Algorithm 1 presents the pseudocode for the overall algo-
rithm. The procedure starts by generating a sample from the
interior of P (line 2) using an interior-point LP solver. Next,
we iterate between local search iterations (line 5) and global
search (line 7). Each local search call provides a new point
x ∈ P and u = FN (x) that is deemed a local minimum
of the search procedure. We increment the current estimate
u by δ, the given tolerance parameter (line 6) and ask the
global solver to ﬁnd a new point x(cid:48) such that u(cid:48) = FN (x(cid:48))
is at least as large as the current value of u (line 7). If this
succeeds, then we update the current values of x, u (line 9).
Otherwise, we conclude that the current value of u is a valid
upper bound within the tolerance δ.

We assume that the procedures LocalSearch and Glob-

The computation of the gradient uses the chain rule to ob-

alSearch satisfy the following properties:
• (P1) Given a point x ∈ P , the LocalSearch procedure

returns x(cid:48) ∈ P such that FN (x(cid:48)) ≥ FN (x).

• (P2) Given a neural network N and the current estimate u,
the GlobalSearch procedure either declares feasible
along with x(cid:48) ∈ P such that FN (x(cid:48)) ≥ FN (x), or de-
clares not feasible if no x(cid:48) ∈ P satisﬁes FN (x(cid:48)) ≥
FN (x).
We recall the basic assumptions thus far: (a) P is com-
pact, (b) δ > 0 and (c) properties P1, P2 apply to the Lo-
calSearch and GlobalSearch procedures. Let us denote the
ideal upper bound by u∗ : maxx∈P FN (x).
Theorem 1.1. Algorithm 1 always terminates. Furthermore,
the output u satisﬁes u ≥ u∗ and u ≤ u∗ + δ.

Proof. Since P is compact and FN is a continuous function.
Therefore, the maximum is always attained.

To see why it terminates, we note that the value of u in-
creases by at least δ each time we execute the loop body of
the While loop in line 4. Furthermore, letting u0 be the value
of u attained by the sample obtained in line 2, we can upper
bound the number of steps by

(cid:109)
.

(cid:108) (u∗−u0)
δ

We note that the procedure terminates only if Glob-
alSearch returns infeasible. Therefore, appealing to prop-
erty P2, we note that (∀ x ∈ P ) FN (x) ≤ u. Or in other
words, u∗ ≤ u.

Likewise, consider the value of u returned by the call to
LocalSearch in the last iteration of the loop, denoted as un
along with the point xn. We note that FN (xn) = un. There-
fore, we have un ≤ u∗ ≤ u. However, un = u − δ. This
completes the proof.

Local and Global Search
In this section, we will describe the local and global
search algorithms used in our approach for estimating up-
per bounds. The approach for estimating lower bounds is
identical replacing ascent steps with descent steps.

Local Search Technique
The local search algorithm uses a steepest projected gradient
ascent algorithm, starting from an input sample point x0 ∈
P and u = FN (x0), iterating through a sequence of points
(x0, u0), (x1, u1), . . . , (xn, un), such that xi ∈ P and u0 <
u1 < u2 < · · · < un.

The new iterate xi+1 is obtained from xi as follows:

1. Compute the gradient zi : ∇FN (xi).
2. Compute a “locally active region” L(xi).
3. Solve a linear program (LP) to compute xi+1.

We now describe the calculation of zi, the deﬁnition of a

“locally active region” and the setup of the LP.

Gradient Calculation: Technically,
the gradient of
FN (x) need not exist for each input x. However, this hap-
pens for a set of points of measure 0, and is dealt with in
practice by using a smoothed version of the function σ deﬁn-
ing the ReLU units.

tain the gradient as a product of matrices:

z : J0 × J1 × · · · × Jk ,

wherein Ji represents the Jacobian matrix of partial deriva-
tives of the output of the (i + 1)th layer zi+1 with respect to
those of the ith layer zi. Since zi+1 = σ(Wizi + bi) we can
compute the gradient Ji using the following simple rule:
1. If the jth entry zi+1,j ≥ 0 then copy jth row of Wi to Ji.
2. Otherwise, set the jth row of Ji to be all zeros.
In practice, the gradient calculation can be piggybacked onto
the function evaluation FN (x) so that we obtain both the
output u and the gradient ∇FN (x).

First order optimization approaches present numerous
rules such as the Armijo step sizing rules for calculating the
step size (Luenberger 1969). However, in our approach we
exploit the local linear nature of the function FN around the
current sample input x to setup an LP.
Deﬁnition 1.5 (Locally Active Region). For an input x to
the neural network N , the locally active region L(x) de-
scribes the set of all inputs x(cid:48) such that x(cid:48) activates exactly
the same neurons as x.

Given the deﬁnition of a locally active region, we obtain

the following property.
Lemma 1.1. The region L(x) is described by a polyhedron
with possibly strict inequality constraints. If x(cid:48) ∈ L(x), then
∇FN (x) = ∇FN (x(cid:48)).

Proof. (Sketch). To see why L(x) is a polyhedron, we pro-
ceed by induction on the number of hidden layers at the net-
work. Let HN1 be the subset of neurons in the ﬁrst hidden
layer that are activated by x. We can write linear inequali-
ties that ensure that for any other input x(cid:48), the inputs to the
neurons in HN1 are ≥ 0 and likewise the inputs to the neu-
rons not activated in the ﬁrst layer are < 0. Proceeding layer
by layer, we can write a series of constraints that describe
L(x). Since the gradient is entirely dictated by the set of
active neurons, it is clear that ∇FN (x) = ∇FN (x(cid:48)).

Let L(x) denote the closure of the local active set by con-
verting the strict > constraints to their non-strict ≥ versions.
Therefore, the local maximum is simply obtained by solving
the following LP.

max wT y s.t. y ∈ L(x) ∩ P .

The solution of the LP above yields a step of the local search.

Termination: The local search is terminated when each
step no longer provides a sufﬁcient increase, or alternatively
the length of each step is deemed too small. These are con-
trolled by user speciﬁed thresholds in practice. Another ter-
mination criterion simply stops the local search when a pre-
set maximum number of iterations is exceeded. In practical
implementations, all three criteria are used.

We note that local search described thus far satisﬁes prop-

erty P1 recalled below.
Lemma 1.2. Given a point x ∈ P , the LocalSearch proce-
dure returns a new x(cid:48) ∈ P , such that FN (x(cid:48)) ≥ FN (x).

Global Search
We will now detail the global search procedure. The overall
goal of the global search procedure is to search for a point
x ∈ P such that FN (x) ≥ u for a given estimate u of the
current upper bound. The approach formulates a MILP fea-
sibility problem, whose real-valued variables are:
1. x: the inputs to the network with n variables.
2. z1, . . . , zk−1, the outputs of the hidden layer. Each zi is

a vector of N variables.

3. y: the overall output of the network.

Additionally, we

introduce binary (0/1) variables
t1, . . . , tk−1, wherein each vector ti has the same size as
zi. These variables will be used to model the piecewise be-
havior of the ReLU units.

Now we encode the constraints. The ﬁrst set of constraints
ensure that x ∈ P . Suppose P is deﬁned as Ax ≤ b then
we simply add these as constraints.

We wish to add constraints so that for each hidden layer
i, zi+1 = σ(Wizi + bi) holds. Since σ is not linear, we use
the binary variables ti+1 to encode the same behavior:

zi+1 ≥ Wizi + bi,
zi+1 ≤ Wizi + bi + M ti+1,
zi+1 ≥ 0,
zi+1 ≤ M (1 − ti+1)

Note that for the ﬁrst hidden layer, we simply substitute x
for z0. This trick of using binary variables to encode piece-
wise linear function is standard in optimization (Vanderbei
2001, Ch. 22.4) (Williams 2013, Ch. 9). Here M is taken to
be a very large constant as a placeholder for ∞. Addition-
ally, we can derive fast estimates for M by using the norms
||Wi||∞ and the bounding box of the input polyhedron.

The output y is constrained as: y = Wkzk + bk. Finally,

we require that y ≥ u.

The MILP, obtained by combining these constraints, is a

feasibility problem without any objective.
Lemma 1.3. The MILP encoding is feasible if and only if
there is an input x ∈ P such that y = FN (x) ≥ u.

Experimental Evaluation
We ﬁrst describe our C++ implementation of the ideas de-
scribed thus far, called SHERLOCK. SHERLOCK combines
local search with the commercial MILP solver Gurobi,
freely available for academic use (Gurobi Optimization
2016). The tolerance parameter δ is ﬁxed to 10−3.

For comparison with Reluplex, we used the proof of con-
cept implementation available at (Katz et al. 2017). How-
ever, at its core, Reluplex does not compute intervals. There-
fore, we use a bisection scheme to repeatedly query Reluplex
with ever smaller intervals until, we have narrowed down the
actual range within a tolerance ˆδ = 10−2.

Randomly Generated Networks
First, we generate numerous randomly generated networks
with 1 output by varying the number of inputs (n), the num-
ber of hidden layers (k), the number of neurons per hidden
layer (N ) and the fraction of non-zero edge weights (s).
Columns 1-4 of Table 1 shows the set of values chosen for

(cid:104)n, k, N, s(cid:105). For each chosen set, we generated 100 random
neural networks. The zero weight edges were chosen by ﬂip-
ping a coin with probability 1 − s of choosing a weight to
be zero. The nonzero weights were uniformly chosen in the
range [−1, 1]. Next, we compare our approach against Relu-
plex solver assuming that the inputs are in the range [−1, 1].
Each tool is run with a timeout of 20 minutes. Table 1 shows
the comparison. SHERLOCK outperforms Reluplex in terms
of the number of examples completed within the given time-
out. However, we note that many of the Reluplex instances
failed before the timeout due to an “error” reported by the
solver. From the table we conclude that the total number of
neurons and the sparsity of the network have a large effect
on the overall performance. In particular, no approach is able
to handle dense examples with 250+ neurons.

Microbenchmarks on Known Functions
Next, we consider a set of 6 microbenchmarks that consist of
networks trained on known 2 input, single output functions
shown in Figure 3. These functions provide varying numbers
of local minima. We then sampled inputs/output pairs, and
trained a neural network with 1 hidden layer for the ﬁrst 5
functions and 6 hidden layer for the last. Next, we compared
SHERLOCK against the Reluplex solver in terms of the abil-
ity to predict the output range of the function given the input
range [−1, 1]2 for the two inputs.

Table 2 summarizes the results of our comparison. We
note that in all cases, our approach can solve the resulting
problem within 10 minutes. However, with a timeout of 1
hour, Reluplex is unable to deduce a range for 4 out of 6
instances.

Illustrative Applications
We illustrate the use of SHERLOCK to infer properties of
deep NNs for important applications, starting with a neu-
ral network that controls a nonlinear system. As mentioned
earlier, such networks are increasingly popular using deep
policy learning (Kahn et al. 2016).

We trained a NN to control a nonlinear plant model (Ex-
ample 7 from (Sassi, Bartocci, and Sankaranarayanan 2017))
whose dynamics are describe by the ODE:
˙x = z3 − y,

˙z = u .

˙y = z,

We ﬁrst devise a model predictive control (MPC) scheme
to stabilize this system to the origin, and train the NN by
[−0.5, 0.5]3 and
sampling inputs from the state space X :
using the MPC to provide the corresponding control.

We trained a 3 input, 1 output network, with 2 hidden
layers, having 300 neurons in the ﬁrst layer and 200 neu-
rons in the second layer. For any state (x, y, z) ∈ X, we
seek to know the range of the output of the network to de-
duce the maximum/minimum control input. This is a di-
rect application of the range estimation problem. SHER-
LOCK is able to deduce a tight range for the control input:
[−2.68772, 3.00263].

Handwriting Recognition Networks
We illustrate an application of range estimation to large neu-
ral networks involved in pattern classiﬁcation, wherein, we

Network Params.
s
k N
n

2
3
4
5
5
5
5
5
10
10
10
10
10
10

2
2
5
8
8
8
8
8
2
5
5
5
5
5

10
10
10
10
10
10
10
10
10
10
50
50
50
50

0.5
0.5
0.5
0.05
0.2
0.5
0.8
0.95
0.5
0.5
0.05
0.2
0.4
0.5

SHERLOCK

Reluplex

Nc

100
100
100
100
100
64
44
40
99
91
99
8
8
0

avg
0.47
0.72
4.6
0.01
0.16
40
167
51
1.65
16.5
6.7
354.8
35.4
x

T
min
0.0
0.0
0.01
0
0.01
0.03
0.18
0.2
0.0
0.02
0.08
0.2
1.6
x

max
1.95
4.6
25.2
0.02
1.93
1023.4
1068.8
1201
7.74
86.4
76.5
1429.1
114.5
x

# Iters
avg min max
17
2
3.5
15
2
4.8
19
2
4.8
2
2
2
3
2
2
31
2
4.5
11
2
3
4
2
2.5
104
2
24
407
2
22
12
2
3
3
2
2
3
3
3
x
x
x

Nc

66
71
53
3
7
12 (*8)
0
0
86 (*1)
7
32 (*1)
0
0
0

avg
2.5
4
207.5
1.65
6.1
621.6
x
x
42.1
433.2
70.3
x
x
x

T
min
1.0
0.85
21.8
1.2
1.9
121.7
x
x
1.6
23.2
9
x
x
x

max
2.6
11.0
1036.3
1.89
11
1025.7
x
x
339.7
1111.2
452
x
x
x

Table 1: Performance on randomly generated neural networks. Legend: n: # inputs, k: # hidden layers, N : # neurons/layer, s:
sparsity fraction, Nc: successfully solved (out of 100), T : running time, # Iters: number of iterations. The number (∗m) denotes
that Reluplex solved m instances that were not solved by our approach. All experiments were run on a Linux workstation
running Ubuntu 17.04 with 64 GB RAM and 23 cores.

ID
N0
N1
N2
N3
N4
N5

k × N
1 × 100
1 × 200
1 × 500
1 × 500
1 × 1000
6 × 250

Ts
1.9s
2.4s
17.8s
7.6s
7m 57.8s
9m 48.4s

Trplex
1m 55s
13m 58s
timeout
timeout
timeout
timeout

Table 2: Performance results on networks trained from the 2
input functions shown in Fig. 3. Legend: k number of lay-
ers, N : number of neurons/layer, Ts: Time taken by SHER-
LOCK, Trplex: Time taken by Reluplex solver. All results in
this table were obtained on a Macbook Pro running ubuntu
16.04 with 3 cores and 8 GB RAM.

wish to infer classiﬁcation labels for images. The MNIST
handwriting recognition data set has emerged as a prototype
application (LeCun and Cortes 2010).

Given a neural network classiﬁer N , and a given image
input I, we wish to explore a set of possible perturbations to
the image to ﬁnd a nearby input that can alter the label that N
provides to the perturbed image. Alternatively, given a space
of possible perturbations, we wish to prove that the network
N is robust to these perturbations: i.e, no perturbation can
alter the label that N provides to the perturbed image.

We trained a NN with 28 × 28 inputs, and 10 outputs, and
3 hidden layers with 200, 100 and 50 neurons in the layers
respectively on the MNIST digit recognition data set (LeCun
and Cortes 2010). The network has 10 outputs that are then

fed to a “softmax” output layer to provide the ﬁnal classiﬁ-
cation. For the purposes of our application, we remove this
softmax layer and examine these 10 outputs that represent
numerical scores corresponding to the digits 0 − 9.

We ﬁrst use our approach to generate adversarial pertur-
bations to the images, which can ﬂip the output. To do so,
we deﬁne for each image a set of pixels and a range of pos-
sible perturbation of these pixels. This forms a polyhedron
that we will call the perturbation space. Next, we use our
approach to explore points in this perturbation space, stop-
ping as soon as we obtain a point with a different label. This
is achieved simply using local search iterations. Figure 4,
demonstrates such an effect, we started with images that are
classiﬁed correctly as the digits “0” and “1”. The perturba-
tions are applied to selected pixels resulting in the digit “0”
labeled as “8”, and “1” labeled as “2”.

However, rather ﬁnding a perturbation, we wish to prove
robustness to a class of perturbations. I.e, for all possible
perturbations, the label ascribed by the network to an image
remains the same.

We chose an image labeled as 5 and speciﬁed a set of
perturbations to a block of size 5 × 10 on the top left corner
of the image. One such perturbation is shown in Figure 6. We
used SHERLOCK to compute a range over the output labels
for all such perturbations. Figure 5 shows the output ranges
for each label. It is clear that all perturbations will lead the
softmax layer to choose the label 5 over all other labels for
the image.

Figure 3: Plots of the benchmark functions used to train the neural networks shown in Table 2.

Figure 4: (Left) Perturbing pixels in the top right corner of
a 28 × 28 image changes the network’s output label from 0
to 8. (Right) Perturbation changes label from 1 to 2.

range(l0) ⊆ [0.0, 0.763]
range(l2) ⊆ [0, 0]
range(l4) ⊆ [0, 0]
range(l6) ⊆ [1.24, 2.25]
range(l8) ⊆ [2.31, 3.5]

range(l1) ⊆ [0, 0]
range(l3) ⊆ [3.365, 5.71]
range(l5) ⊆ [9.62, 13.4]
range(l7) ⊆ [0.016, 0.592]
range(l9) ⊆ [3.39, 5.3]

Figure 5: Output range computed by SHERLOCK for all pos-
sible perturbations of 5 × 10 block as shown in Figure 6.

Conclusion

Thus, we have presented a combination of local and global
search for estimating the output ranges of neural networks
given constraints on the input. Our approach has been imple-

Figure 6: The image above shows the effect of perturbation
on an image that the trained NN is robust to. The perturba-
tion were only allowed to effect a block of size 5 × 10 on the
top left of the image

mented inside the tool SHERLOCK and compared the results
obtained with the solver Reluplex. We have also demon-
strated the application of our approach to interesting appli-
cations to control and classiﬁcation problems.

In the future, we wish to improve SHERLOCK in many
directions, including the treatment of recurrent neural net-
works, handling activation functions beyond ReLU units and
providing faster alternatives to the MILP for global search.

[Williams 2013] Williams, H. P. 2013. Model Building in
Mathematical Programming (Fifth Edition). Wiley.

References
[Baier and Katoen 2008] Baier, C., and Katoen, J.-P. 2008.
Principles of Model Checking. MIT Press.
[Barrett et al. 2009] Barrett, C. W.; Sebastiani, R.; Seshia,
S. A.; and Tinelli, C. 2009. Satisﬁability modulo theories.
Handbook of satisﬁability 185:825–885.
[Bastani et al. 2016] Bastani, O.; Ioannou, Y.; Lampropou-
los, L.; Vytiniotis, D.; Nori, A.; and Criminisi, A. 2016.
In Ad-
Measuring neural net robustness with constraints.
vances in Neural Information Processing Systems, 2613–
2621.
[Bixby 2012] Bixby, R. E. 2012. A brief history of linear
and mixed-integer programming computation. Documenta
Mathematica 107–121.
[Gurobi Optimization 2016] Gurobi Optimization, I. 2016.
Gurobi optimizer reference manual.
[Huang et al. 2016] Huang, X.; Kwiatkowska, M.; Wang, S.;
and Wu, M. 2016. Safety veriﬁcation of deep neural net-
works. CoRR abs/1610.06940.
[Kahn et al. 2016] Kahn, G.; Zhang, T.; Levine, S.; and
Abbeel, P. 2016. Plato: Policy learning using adaptive tra-
jectory optimization. arXiv preprint arXiv:1603.00622.
[Katz et al. 2017] Katz, G.; Barrett, C.; Dill, D. L.; Julian, K.;
and Kochenderfer, M. J. 2017. Reluplex: An Efﬁcient SMT
Solver for Verifying Deep Neural Networks. Cham: Springer
International Publishing. 97–117.
[Katz et al. 2017] Katz et al.
Reluplex: CAV
2017 prototype. https://github.com/guykatzz/
ReluplexCav2017.
[Kurd and Kelly 2003] Kurd, Z., and Kelly, T. 2003. Es-
tablishing safety criteria for artiﬁcial neural networks. In In-
ternational Conference on Knowledge-Based and Intelligent
Information and Engineering Systems, 163–169. Springer.
[LeCun and Cortes 2010] LeCun, Y., and Cortes, C. 2010.
MNIST handwritten digit database.
[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;
Nature
and Hinton, G.
521(7553):436–444.
[Luenberger 1969] Luenberger, D. G. 1969. Optimization By
Vector Space Methods. Wiley.
[Pulina and Tacchella 2010] Pulina, L., and Tacchella, A.
2010. An abstraction-reﬁnement approach to veriﬁcation of
artiﬁcial neural networks. In Computer Aided Veriﬁcation,
243–257. Springer.
[Pulina and Tacchella 2012] Pulina, L., and Tacchella, A.
2012. Challenging smt solvers to verify neural networks.
AI Commun. 25(2):117–135.
[Sassi, Bartocci, and Sankaranarayanan 2017] Sassi,
M.
A. B.; Bartocci, E.; and Sankaranarayanan, S. 2017. A
linear programming-based iterative approach to stabilizing
polynomial dynamics. In Proc. IFAC’17. Elsevier.
Linear Pro-
[Vanderbei 2001] Vanderbei, R. J.
gramming: Foundations & Extensions (Second Edition).
Springer. Cf. http://www.princeton.edu/˜rvdb/
LPbook/.

Deep learning.

2001.

2015.

2017.

