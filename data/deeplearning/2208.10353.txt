Neuro-Symbolic Visual Dialog

Adnen Abdessaied∗, Mihai Bâce, Andreas Bulling
Institute for Visualization and Interactive Systems (VIS)
University of Stuttgart, Germany
adnen.abdessaied@vis.uni-stuttgart.de
mihai.bace@vis.uni-stuttgart.de
andreas.bulling@vis.uni-stuttgart.de

2
2
0
2

g
u
A
2
2

]

V
C
.
s
c
[

1
v
3
5
3
0
1
.
8
0
2
2
:
v
i
X
r
a

Abstract

We propose Neuro-Symbolic Visual Dialog
(NSVD)1 —the ﬁrst method to combine
deep learning and symbolic program execu-
tion for multi-round visually-grounded reason-
ing. NSVD signiﬁcantly outperforms existing
purely-connectionist methods on two key chal-
lenges inherent to visual dialog: long-distance
co-reference resolution as well as vanishing
question-answering performance. We demon-
strate the latter by proposing a more realistic
and stricter evaluation scheme in which we use
predicted answers for the full dialog history
when calculating accuracy. We describe two
variants of our model and show that using this
new scheme, our best model achieves an ac-
curacy of 99.72% on CLEVR-Dialog —a rel-
ative improvement of more than 10% over the
state of the art —while only requiring a frac-
tion of training data. Moreover, we demon-
strate that our neuro-symbolic models have a
higher mean ﬁrst failure round, are more ro-
bust against incomplete dialog histories, and
generalise better not only to dialogs that are
up to three times longer than those seen during
training but also to unseen question types and
scenes.

1

Introduction

Modelled after human-human communication, vi-
sual dialog involves reasoning about a visual scene
through multiple question-answering rounds in nat-
ural language (Das et al., 2019). Its multi-round
nature gives rise to one of its unresolved key chal-
lenges: co-reference resolution (Kottur et al., 2018;
Das et al., 2019). That is, as dialogs unfold over
time, questions tend to include more and more
pronouns, such as “it”, “that”, and “those” that
have to be resolved to the appropriate previously-
mentioned entities in the scene. Co-reference reso-
lution is profoundly challenging (Das et al., 2019;

∗*Corresponding Author.
1Project page:

https://perceptualui.org/

publications/abdessaied22_coling/

Hu et al., 2017), even for models speciﬁcally de-
signed for this task (Kottur et al., 2018). Existing
models follow a purely connectionist approach and
suffer from several limitations: ﬁrst, they require
large amounts of training data, which is prohibitive
for most settings. Second, these models are not ex-
plainable, making it difﬁcult to troubleshoot their
logic when co-references are incorrectly resolved.
Finally, current models lack generalisability, in par-
ticular for real-world dialogs that include incom-
plete or inaccurate dialog histories, longer dialogs
than those seen during training, or unseen question
types. While neuro-symbolic hybrid models have
proven effective as a more robust, explainable, and
data-efﬁcient alternative, e.g. for VQA (Yi et al.,
2018), video QA (Yi et al., 2020), or commonsense
reasoning (Arabshahi et al., 2021), they have not
yet been explored for visual dialog.

We ﬁll this gap by proposing Neuro-Symbolic
Visual Dialog (NSVD) —the ﬁrst neuro-symbolic
method geared towards visual dialog. Our method
combines three novel contributions to disentangle
vision and language understanding from reasoning:
First, it introduces two different program genera-
tors: a caption and a question program generator,
the former of which induces a program from the
caption to initialise the knowledge base of the ex-
ecutor at the beginning of each dialog. Second, a
question program generator that predicts a program
in each round using not only the current question
but also the dialog history. We describe two vari-
ants of this generator: one that uses a question
encoder to concatenate the caption and question-
answer pairs of previous rounds to encode the dia-
log history, as well as one that stacks them. Third, a
symbolic executor with a dynamic knowledge base
keeps track of all entities mentioned in the dialog.
NSVD also addresses another limitation of ex-
isting models that was “hidden” by the dominant
evaluation scheme used so far: vanishing question-
answering performance over the course of the dia-

 
 
 
 
 
 
log. Adopted from VQA (Antol et al., 2015), the
dominant scheme assumes that the model has full
access to the dialog history, in particular all ground
truth answers. We argue that this assumption is
overly optimistic and overestimates real-world per-
formance on the visual dialog task. We instead pro-
pose a more realistic and stricter evaluation scheme
in which prediction in the current round is condi-
tioned on previous predicted answers. This scheme
better represents real-world dialogs in which com-
munication partners rarely know whether their pre-
vious answers were correct or not.

Through extensive experiments on CLEVR-
Dialog (Kottur et al., 2019), we show that our
models are signiﬁcantly better at resolving co-
references and at maintaining performance over
many rounds. Using our stricter evaluation scheme,
we still achieve an accuracy of 99.72% while re-
quiring only a fraction of the training data. Our
results further suggest that NSVD has a higher
mean First Failure Round, is more robust to in-
complete dialog histories, and generalises better
to dialogs that are up to three times longer than
those seen during training as well as to unseen
question types and scenes. The contributions of
our work are threefold: (1) We introduce the ﬁrst
neuro-symbolic visual dialog model that is more
robust again incomplete histories, is signiﬁcantly
better at resolving co-references and at maintaining
performance over more rounds on CLEVR-Dialog.
(2) We contribute a new Domain Speciﬁc Language
(DSL) for CLEVR-Dialog that we augment with
ground truth caption and question programs. (3)
We unveil a fundamental limitation of the domi-
nant evaluation scheme for visual dialog models
and propose a more realistic and stricter alternative
that better represents real-world dialogs.

2 Related Work

Neuro-Symbolic Models
and Reasoning.
Many works have used end-to-end connectionist
models on the CLEVR dataset (Johnson et al.,
2017a) with varying degrees of success (Johnson
et al., 2017b; Hu et al., 2017; Perez et al., 2018;
Hudson and Manning, 2018). NS-VQA (Yi et al.,
2018) was one of the ﬁrst neuro-symbolic models
on CLEVR, achieving a near-perfect test accuracy.
Mao et al. proposed NS-CL, a neuro-symbolic
VQA network that,
to NS-VQA,
learned simply by looking at images and reading
In parallel, other works
question-answer pairs.

in contrast

explored neuro-symbolic models for mono-modal
conversational settings (Williams et al., 2017; Suhr
et al., 2018; Arabshahi et al., 2021). Recently,
Andreas et al. introduced a method that represents
dialog states as a dataﬂow graph to better deal with
co-references. Although a number of works have
demonstrated the signiﬁcant potential of neuro-
symbolic methods for mono-modal task-oriented
dialog settings or tasks at the intersection of
computer vision and natural language processing,
we are the ﬁrst to use them for the multi-modal
visual dialog task.

Visual Dialog. Das et al.
introduced early vi-
sual dialog models that used an encoder-decoder
approach to rank a set of possible answers. Oth-
ers explored explicit reasoning based on the dia-
log structure (Zheng et al., 2019; Niu et al., 2019;
Gan et al., 2019). However, these models focused
mainly on the real-world VisDial dataset (Das et al.,
2019). Although popular, this dataset is not well-
suited to study a key challenge of visual dialog, i.e.
co-reference resolution, because it lacks complete
annotation of all images and dialogs. Several works
have focused on co-reference resolution in videos
(Ramanathan et al., 2014; Rohrbach et al., 2017)
and 3D data (Kong et al., 2014). Kottur et al. intro-
duced CLEVR-Dialog – a fully-annotated diagnos-
tic dataset for multi-round visual reasoning with
a grammar grounded in the CLEVR scene graphs.
More recently, Shah et al. introduced models that
build on the Memory, Attention, and Composition
(MAC) network (Hudson and Manning, 2018). Al-
though their models achieved promising results
on CLEVR-Dialog, they are computationally and
memory inefﬁcient (Shah et al., 2020). While the
neuro-symbolic approach has signiﬁcant potential
to address these shortcomings, it has not yet been
explored for visual dialog.

3 Method

Our method consists of four components (see Fig-
ure 1): a scene understanding method, a program
generator with caption and question encoders and
a decoder, and a symbolic program executor with a
dynamic knowledge base.

Scene Understanding. We used a pre-trained
Mask R-CNN (He et al., 2017) to predict segmen-
tation masks and attributes (colour, shape, material,
size) for each entity in the visual scene. We then
learned the 3D coordinates of each segment paired

Figure 1: Overview of our method: ﬁrst, a structured scene representation is created. Then, a caption program is
induced and run by our executor to initialise its knowledge base. At each subsequent round, the question and the
history are used to induce a program that answers the question and updates the dynamic knowledge base.

Figure 2: Top: The concatenative encoder (“concat”) takes the question and the concatenated previous rounds
as input and outputs a latent vector and a question representation to the decoder. Bottom: The stacking encoder
(“stack”) takes the question and the previous rounds as input and attends to each round separately. Then, it outputs
a latent vector and a question representation to the decoder.

with the original image using a ResNet-34 (He
et al., 2016). The Mask R-CNN was pre-trained on
the CLEVR-mini dataset (Yi et al., 2018).

DSL for CLEVR-Dialog. Because CLEVR-
Dialog implements its own grammar and vocabu-
lary, we designed a novel domain-speciﬁc language
(DSL) for it by implementing a collection of de-

terministic functions in Python that our symbolic
executor can run over a CLEVR scene. In previous
works (Johnson et al., 2017b; Yi et al., 2018; Mao
et al., 2019), these functional modules shared the
same input/output interface and were arranged one
after another to predict the answer. Instead, we
followed a stricter approach by executing only one
function that expects a different number of input

Bi-LSTMSAARFCOutputAttended FeaturesCaption EncoderCaptionAnswerCaptionQ1A1Qi-1Ai-1...QiHistoryQuestion EncoderEncoderMask R-CNNCNNID0ShapeCylinderCylinderSizeBigBigMaterialColorXYZSmall12SphereRubberMetalMetalGreenCyanRed-0.21-0.42-0.19-1.24-1.15-1.410.170.290.16Original SceneStackedObjectsScene UnderstandingDecoderDecoderProgram GeneratorCaption ProgramQuestion ProgramProgram ExecutorExecutorDynamic KnowledgeBaseUpdateFetchConcat Question EncoderCaptionQ1A1Qi-1Ai-1...Bi-LSTMSAAROutputAttended FeaturesQiBi-LSTMSAARFCHistory.........Stack Question EncoderQiBi-LSTMBi-LSTMBi-LSTMBi-LSTMSumFCInner ProductCaptionQ1A1Qi-1Ai-1...History+......WeightSharing[C, Q1, A1, .., Qi−1, Ai−1] of previous question-
answer pairs including the caption. We propose
two different encoders based on how the question
interacts with the history.

– Concat Encoder: The concat encoder is similar
in structure to the caption encoder. The caption and
the question-answer pairs of previous rounds are
concatenated to form the history Hi. Then, the to-
kens of the current question Qi and history Hi are
qj }nq
embedded into a 300-dim. space to give {w(i)
j=1
and {w(i)
}nh
j=1, respectively, which are then pro-
hj
cessed by two separate bi-directional LSTMs (Fig-
ure 2). The reduced attended question and history
features ¯zq and ¯zh are obtained in a similar man-
ner to ¯zc. Finally, ¯zq and ¯zh are concatenated and
linearly transformed to produce the question latent
vector zq = Linear([¯zq, ¯zh]). Similarly, zq and
the question LSTM output Qi = [q1, ..., qnq] are
passed to the decoder.

– Stack Encoder: The approach of concatenat-
ing the question-answer pairs to form the history
suffers from two main drawbacks. First, since the
history is processed by an LSTM, its encoding be-
comes inefﬁcient, in particular for later rounds as
the LSTM tends to forget crucial information that
was mentioned in the ﬁrst rounds. Second, this
approach does not scale well for longer dialogs
as it becomes computationally and memory de-
manding especially because we use self-attention
(Vaswani et al., 2017) at a later stage in the con-
cat encoder. To overcome these limitations, we
introduce the stack encoder that separately encodes
each question-answer pair in order to equally pre-
serve the information from all previous rounds. The
question Qi and each previous round Rj<i, includ-
ing the caption, are embedded then processed by
separate bidirectional LSTMs (Figure 2). The last
hidden states are used as feature representations of
the question and previous rounds, i.e.

q = [

where

−−−→
hRj<i,

←−−−
hRj<i],

−−→
hQi,
−→
h(.) and

←−−
hQi] and rj<i = [
←−
h(.) are the bi-directional
LSTM’s last forward and backward hidden states,
respectively. ¯zh is obtained by applying an inner-
product attention between the question and history
features:

a = softmax(qTH),
H = [r0, ..., ri−1].

Finally, q and ¯zh = (cid:80)i−1

j=1 ajrj are concatenated

Figure 3: a: An example of a CLEVR-Dialog instance
with a caption and two rounds. b: Inference sample of
the caption program generation. c: Inference sample of
the question program generation on the ﬁrst round.

arguments to answer a particular question. The full
list of our functions, their arguments, and expected
output can be found in Appendix A.1.

Program Generation. Semantic parsing meth-
ods were shown to be effective for mapping sen-
tences to logical forms via a knowledge base or a
program (Guu et al., 2017; Liang et al., 2013; Suhr
et al., 2018). We adopted this approach and used
a sequence-to-sequence model with an encoder-
decoder structure to generate the programs. Al-
though we used the same decoder, we propose two
types of encoders that differ in the way they encode
the dialog history (Figure 1).

Caption Encoder. The caption encoder ﬁrst em-
beds the caption tokens into a 300-dim. space to
give {wcj}nc
j=1 that are then fed into a bi-directional
LSTM (Hochreiter and Schmidhuber, 1997). The
self-attended (Vaswani et al., 2017) LSTM outputs
C = [c1, ..., cnc] are reduced following:

a = softmax(MLP(C))

¯zc =

nc(cid:88)

i=1

aici.

Finally, the latent vector zc = Linear(¯zc) and the
LSTM output C are passed to the decoder.

Question Encoders. To generate the ques-
tion program at the current round i, we use not
only the question Qi but also the history Hi =

LSTMSoftmax<S>C : There is a small cylinder in the center.Q1: What is its colour? | A1: GreenQ2: If there is an objct to its right, what is its shape? | A2: Sphere....extreme-centerextreme-centerLSTMSoftmaxcylindercylindersmallLSTMSoftmaxsmall<E>LSTMSoftmaxLSTMSoftmax<S>seek-attribute-earlyseek-attribute-earlyLSTMSoftmaxcolourcolourcylinderLSTMSoftmaxcylinder<E>LSTMSoftmaxabcand linearly transformed to produce the latent ques-
tion vector zq = Linear([q, ¯zh]). Similarly, zq
and the question LSTM output Q = [q1, ..., qnq]
are passed to the decoder.

Decoder. We use the same decoder architecture
to generate all the caption as well as the question
programs. First, the ground truth program sequence
Yi of the i-th dialog round is embedded into a 300-
yj }ny
dim. space to give {w(i)
j=1 which are then pro-
cessed by a simple LSTM whose hidden states are
initialised by the encoder latent vector, i.e. zc or
zq. The output P of the LSTM is used with the
encoder output, i.e. C or Q, to generate a context
vector ∆ following:

A = softmax(QTP),
∆ = ATQ.

Finally, the context vector ∆ is concatenated with
the program output and the result is mapped to
the program vocabulary dimension followed by a
softmax function to obtain a distribution for the
current program token yj, i.e.

p(yj|Y[1:j−1]; Qi, Hi, ) ∼ softmax(
Linear(tanh([P, ∆]))),

where Y[1:j−1] is the sequence of previous ground
truth program tokens. For training, we follow
the teacher forcing strategy by Williams and
Zipser. For inference, we ﬁrst start with the
<S> and sequentially generate the next program
token until we reach the end token <E>. Figure 3
illustrates an example of the CLEVR-Dialog
i.e.
dataset alongside the generated programs,
the program extreme-centre(cylinder,
small) was generated from the caption “There
is a small cylinder in the center” to initialize
our executor and its knowledge base and the pro-
gram seek-attribute-early(colour,
cylinder) was generated to answer the ﬁrst
question of the dialog “What is its colour?”. Our
full DSL grammar and further concrete examples
can be found in the Appendices A.1 and A.8,
respectively.

Executor. We add a dynamic knowledge base
to the symbolic executor
to keep track of
the previously-mentioned entities in the dia-
the beginning of
log.
each dialog by executing the caption pro-
gram.
For instance, by executing the cap-
tion program extreme-centre(cylinder,

is initialised at

It

small), the executor searches for the centre
entity satisfying the function’s arguments and
stores it in the knowledge base under the handle
small-cylinder. The executor interacts with
its knowledge base via two main operations:

fetch. The fetch-operation is performed
when executing a function that
requires co-
reference resolution. Given a set of attributes, the
executor fetches the appropriate entity in the knowl-
edge base by searching the stored handles. For
example seek-attribute-early(colour,
cylinder) ﬁrst searches the previously stored
handles and fetches the corresponding entity (in
our example that is the cylinder mentioned in the
caption with the handle small-cylinder) and
then queries its colour to answer the question.

update. The update-operation is performed
after each question function. We differentiate be-
tween four update types:

1. Handle update:
If a fetched entity is refer-
enced by a new attribute, its handle in the knowl-
If
edge base should be updated accordingly.
the colour of the previous cylinder is red, then
its handle changes from small-cylinder to
small-cylinder-red.

2. Conversation subject update:
If the ques-
tion program addresses a new entity, the latter
becomes the new conversation subject.
In our
example, the conversation subject is still the small
red cylinder. However, the question program
exist-obj-exclude-early(colour,
small, cylinder) searches for other po-
tential entities that share the same colour as the
previous small cylinder. If there is one, it becomes
the new conversation subject.

3. Seen entities update: Each time a new entity is
addressed, the executor saves it in its knowledge
base together with the appropriate handle.

4. Groups update: Some questions refer to a group
count-attribute(red)
of entities, e.g.
counts all red entities in the scene. These sets
might be relevant for subsequent questions, e.g.
count-attribute-group(large) counts
how many of the previous red entities are large.

4 Experiments

We modiﬁed the publicly available code for
CLEVR-Dialog (Kottur et al., 2019) to generate
datasets with ground truth caption and question

programs required to train our program generators.
Similar to (Kottur et al., 2019), we used the 70, 000
training and 15, 000 validation CLEVR images as
our visual groundings when generating the dialogs.
We left out the CLEVR test images because they
lack ground truth scene annotations. For each im-
age, we generated ﬁve dialogs each consisting of
L = 10 question-answer rounds as in (Kottur et al.,
2019). We used 1, 000 training images and their
corresponding dialogs to create a validation set and
tested our models and the baselines on the dialogs
generated using the CLEVR validation images.

Performance Evaluation. Alongside the answer
accuracy, the First Failure Round (FFR), i.e. the
number of dialog rounds necessary for a model to
make its ﬁrst mistake, is commonly used to eval-
uate visual dialog models. Although popular, this
metric has one major limitation as it only allows
us to compare the performance of models across
datasets with the same dialog length but not across
datasets with different ones. Thus, we propose the
Normalised First Failure Round (NFFR) ∈ [0, 1]
as an improvement and use it alongside the answer
accuracy to assess the performance of all models.
See Appendix A.2 for more details.

History during Evaluation. One key limitation
in the way that visual dialog models are currently
evaluated is the use of ground truth answers when
calculating the correctness of an answer in any
given round (Kottur et al., 2018; Das et al., 2019;
Shah et al., 2020). The problem of this approach is
that it leads to overly-optimistic performance that
do not reﬂect the true capabilities of the models
in real-world scenarios: in real-world dialogs, full
information on which previous answers were cor-
rect or not is typically not available. We instead
propose to condition the generation of the current
answer on all previous predicted answers. This
new evaluation scheme is geared to the visual di-
alog task, better represents real-world use, and is
stricter. We call these evaluation schemes “Hist. +
GT” and “Hist. + Pred.”, respectively.

Model

MAC-CQ
+ CAA
+ MTM

HCN
NSVD-concat
NSVD-stack

Hist. + GT

Hist. + Pred.

Acc. NFFR ↑
97.34‡
97.87‡
97.58‡
75.88
99.59‡
99.72‡

0.92
0.94
0.92
0.34
0.98
0.99

Acc. NFFR ↑
41.10
89.39‡
70.39‡
74.42‡
99.59‡
99.72‡

0.15
0.75
0.46
0.32
0.98
0.99

Table 1: Performance comparison of our models with
the state of the art on CLEVR-Dialog test. Results are
shown for both “Hist. + GT” and “Hist. + Pred.” Our
proposed models are highlighted in grey; best perfor-
mance is in bold. ‡ represents p < 0.00001 compared
to the second best score in the respective column.

performed the previous state of the art (Kottur et al.,
2018) by 30% in accuracy. Furthermore, we com-
pared our models to the Hybrid Code Networks
(HCN) (Williams et al., 2017) that also operate
on symbolic dialog state representation but follow
a different approach to parse programs than our
generative one. They represent programs as tem-
plates in an action space and select the one with
the highest probability during inference. This ac-
tion space might become intractable if the DSL has
many functions and arguments.

Table 1 shows the performance of our mod-
els and the baselines on the test split using both
evaluation schemes (“Hist. + GT” and “Hist. +
Pred.”). As the table shows, our models achieve
new state-of-the-art performance with NSVD-stack
topping with an overall accuracy of 99.72% and
a NFFR = 0.99. The high NFFR demonstrates
our models’ ability to answer correctly across all
rounds of the dialogs with only few failures in
between. More ﬁne-grained evaluations (e.g. on
individual rounds, question categories and types)
are available in the Appendix A.5. While Table 1
shows results obtained when training on the entire
dataset, our method achieves the same performance
when trained on only 20% of the data, while the
performance of other methods deteriorate signif-
icantly with less data as shown in Appendix A.6.

5 Results

Visual Dialog Performance. After validating
our implemented logic (see Appendix A.4), we
compared the performance of our models with the
visual dialog MAC networks introduced by Shah
et al. (2020). We limited our comparison to their
top three performing models given that these out-

History Length vs Co-reference Distance.
CLEVR-Dialog provides co-reference distances
for each question, i.e. the number of rounds be-
tween the current and earlier mention of an entity
in a question. A co-reference distance of 1 means
that the co-referent was mentioned in the previous
question while a co-reference distance of 10 means
that the question at round 10 refers to an entity

Figure 4: Robustness for different co-reference distance bins and varying number of rounds in the history. All
models were trained with full histories. Independent of the evaluation scheme, the performance of our models is
only slightly affected by incomplete histories across all bins. In contrast, performance of the baselines deteriorates
when the full history is not available, especially for questions with large co-reference distances.

in the caption. “All” and “None” mean that the
question either depends on all previous rounds or is
stand-alone, i.e. it does not depend on the history.

To assess performance with respect to the co-
reference distance, we evaluated accuracy on dif-
ferent co-reference distance bins. In our evalua-
tions, we further limited the histories to the last Nh
question-answer rounds to assess the robustness
of the models to incomplete dialog histories. All
of the models were trained with histories contain-
ing all previous rounds except for HCN (Williams
et al., 2017) that only uses the last round.

Our models consistently achieve a performance
of over 99% across all co-reference distance bins,
independent of the evaluation scheme (Figure 4).
Furthermore, their performance is only slightly af-
fected by incomplete dialog histories. Contrarily,
performance of all connectionist baselines deterio-
rates quickly without access to the complete history.
This deterioration is more conspicuous when the
“Hist. + Pred.” evaluation scheme is used (sec-
ond row of Figure 4). However, their performance
is consistent with the difﬁculty levels of the co-
reference distance bins, i.e. the accuracy decreases
with increasing co-reference distance. In contrast,
this behaviour is not reﬂected by the popular eval-
uation approach “Hist. + GT” (ﬁrst row, middle
three plots of Figure 4). The most likely reason
for this is that, as is currently common practice
when evaluating visual dialog models, the ground
truth answers of all previous rounds are used for
prediction.

Generalisation to Unseen Scenes and At-
tributes.
In previous experiments, our training
and validation sets had similar distributions both
in the number of objects (between three and 10)
as well their sizes, shapes, colours, and materi-
als. To further test generalisability, we created
a new training set consisting of 1500 images in
which we restricted the type of objects to small,
rubber cubes and spheres with the colours grey,
red, or blue. We kept the number of objects in this
dataset between three and 10. For testing, we gener-
ated three datasets consisting of 1000 images each
in which we allowed all CLEVR object classes
(cubes, spheres and cylinders) and materials (rub-
ber, metal) to appear. However, we excluded the
training colours and increased the number of ob-
jects Nobjects in each one to 10, 15, and 20, re-
spectively. Finally, we generated three ﬁne-tuning
datasets containing 1500 images each in a similar
way to the testing ones. Figure 5 illustrates some
examples of our new images. As in (Kottur et al.,
2019), all dialogs had a length of 10 rounds.

We can see that the purely-connectionist models
outperform the neuro-symbolic ones without ﬁne-
tuning in all scene complexities (Table 2). This
outcome is expected since these models rely on
a Mask-RCNN to understand the scenes. By in-
creasing their complexities, i.e. more objects and
attributes, the Mask-RCNN fails to accurately re-
construct these scenes which is reﬂected by the
poor test accuracies. However, after ﬁne-tuning,
neuro-symbolic models are the best performing

NoneAll1-23-45-1030405060708090100Accuracy (Hist. + GT) [%]NoneAll1-23-45-1030405060708090100Accuracy (Hist. + Pred.) [%]NoneAll1-23-45-1030405060708090100Nh= 2 roundsNoneAll1-23-45-1030405060708090100NoneAll1-23-45-1030405060708090100Nh= 3 roundsNoneAll1-23-45-1030405060708090100NoneAll1-23-45-1030405060708090100Nh= 4 roundsNoneAll1-23-45-1030405060708090100NoneAll1-23-45-1030405060708090100MAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stackNoneAll1-23-45-1030405060708090100Nh= 10 roundsNh= 1 roundModel

T
G
+

.
t
s
i
H

MAC-CQ
+ CAA
+ MTM

HCN
NSVD-concat
NSVD-stack

. MAC-CQ
d
+ CAA
e
r
P
+ MTM
+

.
t
s
i
H

HCN
NSVD-concat
NSVD-stack

Nobjects = 10

Nobjects = 15

Nobjects = 20

Without FT

After FT

Without FT

After FT

Without FT

After FT

Acc. NFFR ↑
38.52†
37.72‡
38.59∗
19.59
25.05∗
24.95‡
37.74∗
37.70‡
36.29‡
19.50
25.05∗
24.95‡

0.14
0.14
0.14
0.11
0.12
0.12
0.14
0.13
0.14
0.11
0.12
0.12

Acc. NFFR ↑
53.49∗
53.35†
52.62
73.07‡
99.32‡
99.33∗
52.26†
51.36†
50.58
71.55‡
99.32‡
99.33∗

0.21
0.21
0.21
0.30
0.97
0.98
0.21
0.21
0.20
0.29
0.97
0.98

Acc. NFFR ↑

36.87∗
36.82†
36.22‡
14.42
18.51∗
18.45‡
36.32∗
36.76∗
35.62‡
14.40
18.51∗
18.45‡

0.13
0.13
0.13
0.11
0.11
0.11
0.12
0.13
0.14
0.11
0.11
0.11

Acc. NFFR ↑
48.89∗
48.67†
47.41
56.65‡
70.59‡
70.62∗
47.58∗
47.08‡
45.67
55.55‡
70.59‡
70.62∗

0.19
0.18
0.17
0.22
0.44
0.44
0.18
0.18
0.17
0.21
0.44
0.44

Acc. NFFR ↑

36.12∗
35.52‡
36.01∗
12.33
15.67‡
15.65∗
35.36‡
35.57∗
33.98‡
12.26
15.67‡
15.65∗

0.13
0.13
0.13
0.11
0.11
0.11
0.13
0.13
0.13
0.11
0.11
0.11

Acc.
47.44∗
47.43†
46.54
53.14‡
64.82‡
64.95∗
46.30†
45.56‡
44.02
51.95‡
64.82‡
64.95∗

NFFR ↑
0.18
0.18
0.17
0.19
0.38
0.38
0.18
0.17
0.17
0.19
0.38
0.38

Table 2: Results when training on simple scenes and testing on more complex ones. Best results in bold. ‡, †, and
∗ represent p < 0.00001, p < 0.05 and p ≥ 0.05 compared to the second best score in each column, respectively.

Figure 5: From left to right: Samples of a training image, ﬁne-tuning image with 10 objects, ﬁne-tuning image
with 15 objects, and ﬁne-tuning image with 20 objects.

datasets for this experiment not only contain di-
alogs that are up to three times the length of the
training dialogs but also visual scenarios never seen
during training. Finally, we evaluated the best
ﬁne-tuned models of the previous experiment on
this data without ﬁne-tuning them again on longer
dialogs. Figure 6 shows that our models gener-
alise better across all datasets for both evaluation
schemes. As expected, the performance of all mod-
els decreases with longer dialogs and more com-
plex scenes. However, our models suffer less and
still signiﬁcantly outperform all baselines in all test
scenarios. Our best model NSVD-stack achieves an
overall answer accuracy of 97.02%, 54.25%, and
49.14% on the longest dialogs (L = 30) that are
created from scenes with 10, 15, and 20 objects,
respectively.

Generalisation to Unseen Questions Types.
Similar to prior works (Johnson et al., 2017a; Yi
et al., 2018; Mao et al., 2019), we addressed the
generalisability to new scenes and object combina-
tions in our previous experiments. However, gener-
alisability to unseen questions remains unexplored.
To address this, we created two splits (AA and BB)
on CLEVR-Dialog as follows: we ﬁrst split the
CLEVR validation images into two disjoint halves
A and B. We then split the question types into split
A and split B. We randomly assigned half of the
question types in each category to split A and the

Figure 6: Answer accuracy for different dialog lengths
and scene complexities. Our models generalise better
to longer dialogs without the need for any ﬁne-tuning.

with our best model NSVD-stack scoring 99.33%,
70.62%, and 64.95% on the test datasets with 10,
15, and 20 objects, respectively.

Generalisation to Longer Dialogs. Typically,
evaluation of visual dialog models is limited to
dialogs having the same length as the training data,
i.e. L = 10 (Kottur et al., 2018; Das et al., 2019;
Shah et al., 2020). In order to assess the gener-
alisation capabilities of our models to longer di-
alogs, we used the testing images of the previ-
ous experiment to generate three dialog datasets
with increasing numbers of rounds, i.e. L = 15,
20, 25, and 30, respectively. That is, our testing

10152025305060708090100Accuracy (Hist. + GT.) [%]101520253040455055606570NObjects=15101520253035404550556065NObjects=201015202530Dialog Length5060708090100Accuracy (Hist. + Pred.) [%]1015202530Dialog Length35404550556065701015202530Dialog Length35404550556065MAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stackNObjects=10Model

T
G
+

.
t
s
i

H

MAC-CQ
+ CAA
+ MTM

HCN
NSVD-concat
NSVD-stack

. MAC-CQ
d
+ CAA
e
r
P
+ MTM
+

.
t
s
i

H

HCN
NSVD-concat
NSVD-stack

Without FT

FT on split BB

Acc. NFFR ↑
36.12‡
35.09†
34.73
47.67‡
64.07‡
71.55‡
35.09‡
36.40‡
6.19
46.91‡
64.07‡
71.55‡

0.14
0.14
0.15
0.14
0.24
0.28
0.13
0.14
0.09
0.13
0.24
0.28

Acc. NFFR ↑
40.33†
40.36∗
35.09
70.43‡
99.44‡
99.51†
39.53‡
37.72‡
7.03
68.82‡
99.44‡
99.51†

0.16
0.16
0.13
0.27
0.96
0.97
0.15
0.15
0.09
0.25
0.96
0.97

Table 3: Results when training on split AA and testing
on split BB. Best results in bold. ‡, †, and ∗ represent
p < 0.00001, p < 0.05 and p ≥ 0.05 compared to the
second best score in each column, respectively.

other half to split B to prevent biasing either one
to a particular question category. For each image
in both splits, we generated ﬁve dialogs consisting
of 10 rounds as in (Kottur et al., 2019). Split AA
contains a training and a validation set based on
6, 000 and 1500 images, respectively. Split BB has
a ﬁne-tuning, a validation, and a test set generated
on 2000, 500, and 5000 images, respectively. The
desired behaviour for a model that generalises well
is to perform well on split BB when only trained
on split AA.

NSVD-concat and NSVD-stack achieve an accu-
racy of 64.07% and 71.55%, respectively, when
tested on split BB without ﬁne-tuning, thereby sig-
niﬁcantly outperforming all baselines (Table 3).
However, low NFRR values indicate that ﬁrst fail-
ures occur early on in the dialog. After ﬁne-tuning
all models on a small amount of data from split BB,
our models achieve accuracies and NFRR values
comparable to previous experiments. In stark con-
trast, purely-connectionist baselines’ performance
only improves by a small margin with the high-
est jump of 5.26% being achieved by MAC-CQ-
CAA. This shows an impressive data efﬁciency
of the neuro-symbolic models that, in contrast to
the data-hungry purely-connectionist baselines, are
able to learn and adapt efﬁciently from a very small
amount of data. More details regarding the training
data efﬁciency can be found in the Appendix A.6.

6 Conclusion and Future Work

We proposed Neuro-Symbolic Visual Dialog
(NSVD) —the ﬁrst hybrid method to combine deep
learning and symbolic program execution for multi-
round visual reasoning —and a new, stricter and

more realistic evaluation scheme for visual dialog.
Our method outperforms state-of-the-art purely-
connectionist baselines on CLEVR-Dialog and sets
a new near-perfect test accuracy of 99.72%. Fur-
thermore, NSVD has a higher mean First Failure
Round, is more robust to incomplete dialog histo-
ries, and generalises better to longer dialogs and
to unseen question types and scenes. These perfor-
mance improvements are not to be seen in isolation
of the stricter supervision our models have as they
require a supervised ﬁne-tuning of a Mask-RCNN
in addition to a supervised training of the program
parsers. Finally, additional evaluations show that
our models generalise to other scene domains (Ap-
pendix A.7) and we expect them to even generalise
to naturalistic datasets as well if they provide the
necessary supervision requirements for our models
similar to the VQA scenario (Wang et al., 2017;
Gan et al., 2017). To the best of our knowledge,
such datasets for visual dialog do not yet exist.

Acknowledgments

We would like to thank the anonymous review-
ers for their insightful comments, Jiayuan Mao
for providing us with the Minecraft data, and Do-
minike Thomas for paper editing support. M. Bâce
was funded by a Swiss National Science Founda-
tion (SNSF) Early Postdoc Mobility Fellowship
(No. 199991). A. Bulling was funded by the Eu-
ropean Research Council (ERC; grant agreement
801708).

References

Jacob Andreas, John Bufe, David Burkett, Charles
Chen, Josh Clausman, Jean Crawford, Kate Crim,
Jordan DeLoach, Leah Dorner, Jason Eisner, Hao
Fang, Alan Guo, David Hall, Kristin Hayes, Kellie
Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan
Klein, Jayant Krishnamurthy, Theo Lanman, Percy
Liang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-
Govern, Aleksandr Nisnevich, Adam Pauls, Dmitrij
Petters, Brent Read, Dan Roth, Subhro Roy, Jesse
Rusak, Beth Short, Div Slomin, Ben Snyder,
Stephon Striplin, Yu Su, Zachary Tellman, Sam
Thomson, Andrei Vorobev, Izabela Witoszko, Jason
Wolfe, Abby Wray, Yuchen Zhang, and Alexander
Zotov. 2020. Semantic machines et al. task-oriented
dialogue as dataﬂow synthesis. Transactions of the
Association for Computational Linguistics, 8:556–
571.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-

swering.
ference on Computer Vision, pages 2425–2433.

In Proceedings of the International Con-

Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki,
Kathryn Mazaitis, Amos Azaria, and Tom M.
Conversational neuro-symbolic
Mitchell. 2021.
commonsense reasoning. In Proceedings of the Con-
ference on Artiﬁcial Intelligence.

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, Stefan Lee, Jose M.F. Moura,
Devi Parikh, and Dhruv Batra. 2019. Visual Dialog.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 41(5):1242–1256.

Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and
Boqing Gong. 2017. VQS: Linking Segmentations
to Questions and Answers for Supervised Attention
in VQA and Question-Focused Semantic Segmenta-
tion. In Proceedings of the International Conference
on Computer Vision, pages 1829–1838.

Zhe Gan, Yu Cheng, Ahmed Kholy, Linjie Li, Jingjing
Liu, and Jianfeng Gao. 2019. Multi-step reasoning
via recurrent dual attention for visual dialog. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.

Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy
Liang. 2017. From language to programs: Bridg-
ing reinforcement learning and maximum marginal
In Proceedings of the Annual Meeting
likelihood.
of the Association for Computational Linguistics,
pages 1051–1062.

Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross
Girshick. 2017. Mask r-cnn. In Proceedings of the
International Conference on Computer Vision, pages
2961–2969.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the Conference on Com-
nition.
puter Vision and Pattern Recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Neural Comput.,

Long short-term memory.
9(8):1735–1780.

Ronghang Hu, Jacob Andreas, Marcus Rohrbach,
Trevor Darrell, and Kate Saenko. 2017. Learning
to reason: End-to-end module networks for visual
question answering. In Proceedings of the Interna-
tional Conference on Computer Vision, pages 804–
813.

Drew A Hudson and Christopher D Manning. 2018.
Compositional attention networks for machine rea-
soning. In Proceedings of the International Confer-
ence on Learning Representations.

Justin Johnson, Li Fei-Fei, Bharath Hariharan,
C. Lawrence Zitnick, Laurens Van Der Maaten,
and Ross Girshick. 2017a. CLEVR: A diagnostic
dataset for compositional language and elementary

visual reasoning. In Proceedings of the Conference
on Computer Vision and Pattern Recognition, pages
1988–1997.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zit-
nick, and Ross Girshick. 2017b.
Inferring and ex-
ecuting programs for visual reasoning. In Proceed-
ings of the International Conference on Computer
Vision.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
In Proceedings
about? text-to-image coreference.
of the Conference on Computer Vision and Pattern
Recognition, pages 3558–3565.

Satwik Kottur, José MF Moura, Devi Parikh, Dhruv Ba-
tra, and Marcus Rohrbach. 2018. Visual coreference
resolution in visual dialog using neural module net-
works. In Proceedings of the European Conference
on Computer Vision, pages 153–169.

Satwik Kottur, José M.F. Moura, Devi Parikh, Dhruv
Batra, and Marcus Rohrbach. 2019. Clevr-dialog:
A diagnostic dataset for multi-round reasoning in vi-
sual dialog. In Proceeding of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, volume 1, pages 582–595.

Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2).

Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.
The neuro-
Interpreting scenes,
In
the International Conference on

Tenenbaum, and Jiajun Wu. 2019.
symbolic concept
learner:
words, and sentences from natural supervision.
Proceedings of
Learning Representations, pages 1–28.

Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong
Zhang, Zhiwu Lu, and Ji-Rong Wen. 2019. Recur-
sive visual attention in visual dialog. In Proceedings
of the Conference on Computer Vision and Pattern
Recognition.

Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-
moulin, and Aaron C. Courville. 2018. Film: Visual
reasoning with a general conditioning layer. In Pro-
ceedings of the Conference on Artiﬁcial Intelligence.

Vignesh Ramanathan, Armand Joulin, Percy Liang,
and Li Fei-Fei. 2014. Linking people in videos with
“their” names using coreference resolution. In Pro-
ceedings of the European Conference on Computer
Vision, pages 95–110.

Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong
Joon Oh, and Bernt Schiele. 2017. Generating de-
scriptions with grounded and co-referenced people.
In Proceedings of the Conference on Computer Vi-
sion and Pattern Recognition, pages 4979–4989.

Muhammad Shah, Shikib Mehri, and Tejas Srinivasan.
2020. Reasoning Over History: Context Aware Vi-
In Proceedings of the International
sual Dialog.
Workshop on Natural Language Processing Beyond
Text, pages 75–83. Association for Computational
Linguistics.

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the An-
nual Conference of the North American Chapter of
the Association for Computational Linguistic, pages
2238–2249.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems.

Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and
Anton Van Den Hengel. 2017. Explicit knowledge-
based reasoning for visual question answering.
In
Proceedings of the International Joint Conference
on Artiﬁcial Intelligence, pages 1290–1296.

Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: Practical and efﬁcient
end-to-end dialog control with supervised and rein-
In Proceedings of the Annual
forcement learning.
Meeting of the Association for Computational Lin-
guistics, volume 1, pages 665–677.

Ronald J Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural Comput., 1(2):270–280.

Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli.
2017. Neural Scene De-rendering. In Proceedings
of the Conference on Computer Vision and Pattern
Recognition.

Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli,
Jiajun Wu, Antonio Torralba, and Joshua B. Tenen-
baum. 2020. CLEVRER: collision events for video
representation and reasoning. In Proceedings of the
International Conference on Learning Representa-
tions.

Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba,
Pushmeet Kohli, and Joshua B Tenenbaum. 2018.
Neural-Symbolic VQA: Disentangling Reasoning
In Ad-
from Vision and Language Understanding.
vances in Neural Information Processing Systems.

Zilong Zheng, Wenguan Wang, Siyuan Qi, and Song-
Chun Zhu. 2019. Reasoning visual dialogs with
structural and partial observations. In Proceedings
of the Conference on Computer Vision and Pattern
Recognition.

A Appendix

A.1 The CLEVR-Dialog DSL

Our Domain Speciﬁc Language (DSL) for CLEVR-
Dialog is depicted in Table 4. We present each func-

tion with its expected argument types, output, and
knowledge base operations. The argument types
are further deﬁned in Table 5. We use the vari-
ables attr, attr_obj_1, attr_obj_2 and
attr_i for i = 1,..,4 to denote the set of
possible CLEVR attributes, i.e. colour, material,
shape, and size. Furthermore, the variable pos
denote one possible position, i.e. right, left,
front, or behind. Finally, the variable num
denotes the set of possible numerical values , i.e.
between 0 and N , where N is the maximum num-
ber of objects in the scene. If not explicitly stated
otherwise, we assume N = 10.

A.2 Normalised First Failure Round

The Normalised First Failure Round NFFR is cal-
culated as

NFFR =

1
N

N
(cid:88)

i=1

1
L + 1

L
(cid:88)

j=1

∆(i)
j δ

pred(i)

j ,gt(i)

j

α(i)
j ,

j and gt(i)

where N is the total number of dialogs, L is the
length of each dialog, and pred(i)
j are the
predicted and ground truth answers at round j of
dialog i, respectively. Furthermore, for each round
j of every dialog i, we deﬁne ∆(i)
, and
α(i)
j as:

j ,gt(i)

j , δ

pred(i)

j

∆(i)

j =

(cid:40) j

if
L + 1

j ≤ L
if

j = L ∧ δ

= 1 ,

pred(i)

j ,gt(i)

j

δ
pred(i)

j ,gt(i)

j

(cid:40)

=

pred(i)
1 if
j
0 otherwise

(cid:54)= gt(i)
j

,

α(i)
j =

(cid:40)

0

1

if ∃k < j s.t. δ

pred(i)

k ,gt(i)

k

= 1

.

otherwise

By deﬁnition, we set the NFFR of a model to be
L + 1 if it correctly answers all L dialog rounds.

A.3 Training Details

Our Models.
In order to encode our raw data, we
generated two different vocabularies from the train-
ing data: one that handles the captions, questions,
and answers in form of natural language and an-
other that deals with ground truth caption and ques-
tion programs. For encoding, we pad all captions,
questions, and programs to a maximum length of
nq = 21, nc = 16, and ny = 6, respectively. Then,
the corresponding tokens are transformed into a

e
t
a
d
p
u

s
p
u
o
r
G

.
s
j
b
O

n
e
e
S

.
j
b
u
S

.
v
n
o
C

e
l
d
n
a
H

h
c
t
e
f

.
t
u
O

.
c
n
u
F

.
s
g
r
A

.
c
n
u
F

e
m
a
N

.
c
n
u
F

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:55)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

e
n
o
n

e
n
o
n

e
n
o
n

e
n
o
n

e
n
o
n

e
n
o
n

e
n
o
n

e
n
o
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

m
u
n

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

o
n
/
s
e
y

r
t
t
a

r
t
t
a

r
t
t
a

r
t
t
a

r
t
t
a

r
t
t
a

r
t
t
a

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

2
_
j
b
o
_
r
t
t
a
,
s
o
p
,
1
_
j
b
o
_
r
t
t
a

-

-

-

r
t
t
a

r
t
t
a

s
o
p

s
o
p

r
t
t
a
,
s
o
p

e
p
y
t
_
r
t
t
a

t
h
g
i
r
-
e
m
e
r
t
x
e

t
f
e
l
-
e
m
e
r
t
x
e

d
n
i
h
e
b
-
e
m
e
r
t
x
e

t
n
o
r
f
-
e
m
e
r
t
x
e

e
r
t
n
e
c
-
e
m
e
r
t
x
e

t
t
a
-
t
n
u
o
c

n
o
i
t
a
l
e
r
-
j
b
o

j
b
o
-
e
u
q
i
n
u

r
e
h
t
o
-
t
n
u
o
c

l
l
a
-
t
n
u
o
c

p
u
o
r
g
-
l
l
a
-
t
n
u
o
c

e
t
u
b
i
r
t
t
a
-
t
n
u
o
c

p
u
o
r
g
-
e
t
u
b
i
r
t
t
a
-
t
n
u
o
c

2
-
m
m
i
-
l
e
r
-
j
b
o
-
t
n
u
o
c

y
l
r
a
e
-
l
e
r
-
j
b
o
-
t
n
u
o
c

m
m
i
-
l
e
r
-
j
b
o
-
t
n
u
o
c

m
m
i
-
e
d
u
l
c
x
e
-
j
b
o
-
t
n
u
o
c

r
t
t
a
,
e
p
y
t
_
r
t
t
a

y
l
r
a
e
-
e
d
u
l
c
x
e
-
j
b
o
-
t
n
u
o
c

-

r
t
t
a

r
t
t
a

s
o
p

s
o
p

r
t
t
a
,
s
o
p

e
p
y
t
_
r
t
t
a

p
u
o
r
g
-
e
t
u
b
i
r
t
t
a
-
t
s
i
x
e

2
m
m
i
-
l
e
r
-
j
b
o
-
t
s
i
x
e

m
m
i
-
l
e
r
-
j
b
o
-
t
s
i
x
e

y
l
r
a
e
-
l
e
r
-
j
b
o
-
t
s
i
x
e

m
m
i
-
e
d
u
l
c
x
e
-
j
b
o
-
t
s
i
x
e

e
t
u
b
i
r
t
t
a
-
t
s
i
x
e

r
e
h
t
o
-
t
s
i
x
e

e
p
y
t
_
r
t
t
a

e
p
y
t
_
r
t
t
a

r
t
t
a
,
e
p
y
t
_
r
t
t
a

r
t
t
a
,
e
p
y
t
_
r
t
t
a

e
p
y
t
_
r
t
t
a

r
t
t
a
,
s
o
p
,
e
p
y
t
_
r
t
t
a

y
l
r
a
e
-
m
i
s
-
r
t
t
a
-
k
e
e
s

y
l
r
a
e
-
l
e
r
-
r
t
t
a
-
k
e
e
s

m
m
i
-
l
e
r
-
r
t
t
a
-
k
e
e
s

2
m
m
i
-
r
t
t
a
-
k
e
e
s

m
m
i
-
r
t
t
a
-
k
e
e
s

y
l
r
a
e
-
r
t
t
a
-
k
e
e
s

r
t
t
a
,
e
p
y
t
_
r
t
t
a

y
l
r
a
e
-
e
d
u
l
c
x
e
-
j
b
o
-
t
s
i
x
e

CaptionPrograms

QuestionPrograms

n
e
v
i
G

.
s
n
o
i
t
a
r
e
p
o

e
s
a
b

e
g
d
e
l
w
o
n
k

d
n
a

,
t
u
p
t
u
o

,
s
e
p
y
t

t
n
e
m
u
g
r
a

d
e
t
c
e
p
x
e

s
t
i

h
t
i

w
n
o
i
t
c
n
u
f

h
c
a
e

t
n
e
s
e
r
p

e

W

-

.
g
o
l
a
i
D
R
V
E
L
C

r
o
f

)
L
S
D

(

e
g
a
u
g
n
a
L
c
ﬁ
i
c
e
p
S
n
i
a
m
o
D

r
u
O

:
4

e
l
b
a
T

.

]
3
_
r
t
t
a
,
2
_
r
t
t
a
[
=
]
4
_
r
t
t
a
,
.
.
,
1
_
r
t
t
a
[
(cid:93)

.
g
.
e

,
s
e
n
o

e
l
b
i
s
s
o
p
n
≤
m

f
o

t
e
s
b
u
s

a

s
t
c
e
l
e
s

r
o
t
a
r
e
p
o
(cid:93)
e
h
t

,
s
t
n
e
m
u
g
r
a
n
f
o
t
e
s

a

attr_1 ∈ COLOURS=[blue,brown,cyan,grey,green,purple,red,yellow],
attr_2 ∈ MATERIALS=[rubber,metal],
attr_3 ∈ SHAPES=[cube,cylinder,sphere],
attr_4 ∈ SIZES=[large,small],
attr, attr_obj_1, attr_obj_2 ∈ (cid:83) {COLOURS,MATERIALS,SHAPES,SIZES},
attr_type ∈ [colour,material,shape,size],
pos ∈ [right,left,front,behind],
num ∈ [0,1,2,3,4,5,6,7,8,9,10].

Table 5: Argument types of our DSL.

300-dim. space using either the encoder text em-
bedding or the decoder program embedding. We
trained the caption and question program genera-
tors separately. We used a 2-layered bi-directional
LSTM with a hidden size of 256 in both caption
and question encoders. In the decoder, we used
a 2-layered LSTM with a hidden size of 512. We
ﬁxed the batch size to 64 and used the Adam opti-
miser (Kingma and Ba, 2014) with a learning rate
of 7 × 10−4 to train for one epoch. Every 2000
iteration, we validated the models based on the
program accuracy in order not to select one that
follows a ﬂawed logic, i.e. a wrong program, to
predict an answer.

Baselines. We trained the purely-connectionist
baselines using the ofﬁcial code 2 we obtained from
the authors of Shah et al. (2020). We used the
same hyperparameters and training scheme, i.e. we
trained the models for a maximum of 25 epochs and
used early stopping when the validation accuracy
did not improve for ﬁve consecutive epochs. For
the HCN models, we used a publicly available code-
base3 for training with the same hyper-parameters
as in (Williams et al., 2017).

Runtime Analysis. We conducted all of our ex-
periments on a single NVIDIA Tesla V100 GPU.
The training runtimes for one epoch when using
a batch size of 64 are illustrated in Figure 7. As
can be seen, all the purely-connectionist baselines
need more than seven hours to complete one epoch.
Although NSVD-concat concatenates the previous
dialog rounds to form the history similarly to the
baselines, it is more computationally efﬁcient as it
only needs circa 22 minutes to complete one epoch.
Finally, HCN needs around 19 minutes to complete

2https://github.com/ahmedshah1494/
clevr-dialog-mac-net/tree/dialog-macnet

3https://github.com/jojonki/

Hybrid-Code-Networks

Figure 7: Training runtime comparison of the models.
The training was conducted on a single NVIDIA Tesla
V100 GPU with batch size 64.

Model

Prog. Acc.

Caption Question

Executor
Acc.

Caption-Net
NSVD-concat
NSVD-stack

99.79
-
-

-
99.87
99.99

99.99

Table 6: Quantitative analysis of our models’ logic.
The high program accuracies demonstrate that our mod-
els follow the implemented logic to predict the correct
they do not execute false programs that
answer, i.e.
by chance might lead to a correct prediction.
In ad-
dition, when tested with the ground truth scene anno-
tations and programs, our executor reaches an answer-
accuracy of 99.99% showcasing its ﬂawless logic.

one epoch and NSVD-stack only around 14 which
solidiﬁes our aforementioned efﬁciency claims.

A.4 Quantitative Analysis of (our) Logic.

To quantify the logical capabilities of our models,
we measured the caption and question program
accuracies on the test split. As we can see from Ta-
ble 6, our model achieves 99.79% caption program
accuracy and our best question generator reaches
the 99.99% accuracy mark. That is, they do not fol-
low a ﬂawed logic, i.e. a wrong program, to predict
the correct answer. Furthermore, we evaluated the
logic of our program executor by measuring its an-
swer accuracy when provided with the ground truth
programs and scene annotations of the test split.
The last column of Table 6 shows that it reaches
99.99% answer accuracy indicating that its logic
is close to ﬂawless. The reason why it does not

050100150200250300350400Runtime [mn/epoch]MAC-CQ+CAA+MTMHCNNSVD-concatNSVD-stackExample of a situation where a caption cannot be uniquely interpreted.

The program
Figure 8:
extreme-centre(cylinder, small) induced from the caption “there is a small round thing sitting in
the centre of the view” does not lead to a unique initialisation of our executor’s knowledge base as there are two
small spheres in the centre of the scene. By our logic, we consider it to be the cyan one. Incorrectly initialising the
knowledge base leads to confusion when answering the subsequent questions. The blue and red colours indicate a
match or a mismatch between the predicted answer and the ground truth, respectively.

Figure 9: Detailed performance comparison for the
different question categories (Count, Exist, Seek).
Independent of the evaluation approach, our models
achieve new state-of-art results on all question cate-
gories. These improvements are statistically signiﬁcant
with p < 0.0001 in all categories.

reach the 100% mark is that some scene captions
cannot be uniquely interpreted leading to potential
confusions when answering the subsequent ques-
tions. Figure 8 illustrates a concrete example of
such a case. The caption “there is a small round
thing sitting in the centre of the view” induces
the program extreme-centre(cylinder,
small). However, this can be interpreted in two
different ways since there are two small spheres in
the centre of the scene, i.e. the cyan and the green
ones. Therefore, the performance of our executor at
answering the following dialog questions depends
on which object is considered as the central one.
By our logic, we consider it to be the cyan one.
The subsequent questions, ground truth answers
and predictions are also shown in Figure 8.

A.5 Fine-grained Evaluations

When looking at the performance for different ques-
tion rounds (Figure 10), we can see that while the
performance of the baselines starts to deteriorate

Figure 10: Accuracy for different question rounds. Our
models signiﬁcantly outperform the baselines with p <
0.0001 in all question rounds.

quickly with longer dialogs, our models achieve
a consistently-high performance across all rounds.
The inability to maintain performance becomes
even more apparent (especially for the purely-
connectionist baselines) when using the stricter
“Hist. + Pred.” evaluation scheme. The best base-
line, MAC-CQ-CAA, suffers from a drop of 8.48%
in accuracy and 0.19 in NFFR. The same observa-
tion can be made when analysing performance for
the different CLEVR-Dialog question categories
(Count, Exist, and Seek). Our models outper-
form all baselines for all categories and both evalu-
ation schemes (Figure 9). These improvements are
statistically signiﬁcant with p < 0.00001.

Figure 11 illustrates the performance of our mod-
els on individual question types compared to the
baselines. As can be seen, our models outperform
the baselines on almost all question types with
NSVD-stack topping them all with an accuracy of
over 98% for all types. As seen from previous
experiments, the gap between our models and the
baseline becomes ever more conspicuous when the
“Hist. + Pred.” evaluation scheme is deployed (bot-
tom table of Figure 11).

CountExistSeek0102030405060708090100Accuracy (Hist. + Pred.) [%]CountExistSeek0102030405060708090100Accuracy (Hist. + GT) [%]MAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stackMAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stack12345678910Round #30405060708090100Accuracy (Hist. + Pred.) [%]12345678910Round #65707580859095100Accuracy (Hist. + GT) [%]Figure 11: Performance comparison on the individual question types. Top: Models were evaluated following the
“Hist. + GT” evaluation scheme. Bottom: Models were evaluated following the “Hist. + Pred.” evaluation scheme

A.6 Data Efﬁciency

To study the data efﬁciency of our models fur-
ther, we trained them and the baselines on
20%, 40%, 60%, 80%, and 100% of the available
training data. Not to bias the data towards any spe-
ciﬁc question type, we used the same distribution
of question types to construct the reduced training
sets. After training, we evaluated the models on
the test split using both evaluation schemes. Our
models not only outperform all baselines for the
same reduced dataset but also in the extreme case
of training the baselines with 100% of the data and
our models with only 20% (Figure 13). While the
performance of the neuro-symbolic models is only
slightly affected by the size of the training data, the
connectionist baselines’ performance deteriorates
with less data. This deterioration becomes even
more signiﬁcant when the stricter “Hist. + Pred.”
evaluation scheme is used.

A.7 Generalisation to Other Scene Domains

In this experiment, we show that our method could
be extended to a new reasoning testbed. Contrar-
ily to CLEVR, the new scenes are grounded in
Minecraft and are, as can be seen in Figure 12,
drastically different in terms of context and scene
constellations. Speciﬁcally, they comes with more
entities (12 vs 3 in CLEVR) that have drastically

Model

MAC-CQ
+ CAA
+ MTM

HCN
NSVD-concat
NSVD-stack

Hist. + GT

Hist. + Pred.

Acc. NFFR ↑
64.30∗
64.28‡
61.55‡
47.31
91.57‡
92.46‡

0.27
0.27
0.25
0.14
0.76
0.83

Acc. NFFR ↑
59.96‡
57.69‡
52.04‡
46.50
91.57‡
92.46‡

0.24
0.23
0.20
0.14
0.76
0.83

Table 7: Performance comparison on Minecraft-Dialog
test. Results are shown for both “Hist. + GT” and “Hist.
+ Pred.” Our proposed models are highlighted in grey;
best performance is in bold. ‡ and ∗ represents p <
0.00001 and p ≥ 0.05 compared to the second best
score in the respective column, respectively.

different visual appearances. These entities can be
grouped in a hierarchical manner (e.g. “a cow” and
“a wolf” are both “animals” whereas “an animal”
and “a human” are both “creatures”).

Following (Yi et al., 2018), were rendered
10, 000 using the generation tool provided by Wu
et al. (2017). The scenes consist of three to six
objects in a 2D plane that are sampled from 12
entities with 4 different facing directions. Finally,
we ﬁltered out scenes that contain fully-occluded
objects. We used 5, 273 images for training, 1, 500
for validation, and 1, 000 for testing. Furthermore,
we adapted the dialog generation tool provided by
Kottur et al. (2019) to be able to account for the

seek-attr-rel-immseek-attr-imm2count-obj-rel-earlycount-obj-exclude-earlyexist-obj-exclude-earlyseek-attr-earlyexist-obj-rel-earlyseek-attr-rel-earlyseek-attr-immexist-obj-rel-immexist-obj-rel-imm2count-allexist-attribute-groupcount-otherexist-obj-exclude-immexist-attributeexist-othercount-obj-rel-immcount-attributecount-obj-exclude-immcount-obj-rel-imm2count-attribute-groupMAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stack38.939.619.229.175.040.070.237.540.066.771.213.368.520.180.450.579.017.029.928.922.084.289.693.680.486.393.192.697.291.891.086.288.779.293.947.189.599.879.061.096.275.967.478.677.684.632.371.889.271.873.254.168.268.368.176.593.270.871.898.982.526.791.541.728.789.061.752.061.094.496.996.081.065.868.991.268.998.791.995.277.795.899.866.796.970.438.593.899.699.899.499.699.999.999.999.899.899.80.098.899.499.399.999.799.999.299.899.399.399.399.899.899.499.699.999.999.999.899.899.898.198.899.799.399.999.999.999.299.899.398.799.6MAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stack97.899.195.596.298.697.599.096.798.098.798.198.798.385.396.699.993.394.898.793.795.097.698.599.496.197.099.398.299.397.698.798.598.997.999.084.898.399.994.293.698.995.694.298.197.099.095.896.999.198.699.597.798.697.698.098.298.486.297.299.994.190.298.894.589.895.262.054.364.797.698.998.082.466.469.991.470.098.792.095.377.896.299.866.597.870.442.494.499.699.899.499.699.999.999.999.899.899.80.098.899.499.399.999.799.999.299.899.399.399.399.899.899.499.699.999.999.999.899.899.898.198.899.799.399.999.999.999.299.899.398.799.6Accuracy (Hist. + GT.) [%]Accuracy (Hist. + Pred.) [%]Figure 12: Sample images from the Minecraft dataset.

Figure 13: Accuracy when trained on limited amounts
of data (20%, 40%, 60%, 80%, and 100% of the overall
training data) and evaluated on the test split following
the “Hist. + GT” and “Hist. + Pred.” approaches.

different scene properties.

Similar to previous experiments, we generated
ﬁve dialogs for every image consisting of 10 rounds
each. We call this dataset Minecraft-Dialog. The re-
sults are summarised in Table 7. We compared our
models to the same baselines as in previous experi-
ments in terms of accuracy as well as NFRR. Once
again, our neuro-symbolic models managed to sig-
niﬁcantly outperform all the baselines by achieving
92.64% and 91.57% accuracies for NSVD-stack
and NSVD-concat, respectively, while maintaining
high NFRR values.

Contrarily,

the best connectionist model
achieved test accuracies of 64.30% and 59.96%
using the “Hist. + GT” and “Hist. + Pred.” evalua-
tion schemes, respectively. Finally, HCN achieved
its best performance of 47.31 accuracy and 0.14
NFRR when the “Hist. + GT” evaluation scheme
was used. Compared to CLEVR-Dialog (Table 1),
the performance of our models witnessed a drop
in this experiment which is attributed to the difﬁ-
culty of the Minecraft scenes that come with heavy-
occluded and diverse objects. However, the promis-
ing results of our models showcase that they indeed
can generalise to new scene domains other then
CLEVR.

A.8

Input/Output Samples

1008060402065707580859095100Accuracy (Hist. + GT) [%]10080604020Trainig Data [%]30405060708090100Accuracy (Hist. + Pred.) [%]Trainig Data [%]MAC-CQMAC-CQ-CAAMAC-CQ-CAA-MTMHCNNSVD-concatNSVD-stackn
e
e
w
t
e
b
h
c
t
a
m
a

s
e
t
a
c
i
d
n
i

r
u
o
l
o
c

e
u
l
b
e
h
T

.
s
r
e
w
s
n
a

e
h
t

t
c
i
d
e
r
p
o
t

s

m
a
r
g
o
r
p
t
c
e
r
r
o
c
d
e
t
u
c
e
x
e

s
l
e
d
o
m

r
u
o
f
o
h
t
o
B

.
e
c
n
a
t
s
n
i

t
s
e
t

a
n
o
s
l
e
d
o
m

r
u
o
f
o
e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
4
1
e
r
u
g
i
F

.
h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p
d
e
t
c
i
d
e
r
p
e
h
t

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.

7

d
n
u
o
r

t
a

d
e
l
i
a
f

t
a
c
n
o
c
-
D
V
S
N

,

y
l
t
c
e
r
r
o
c

s
n
o
i
t
s
e
u
q

l
l
a

d
e
r
e
w
s
n
a

k
c
a
t
s
-
D
V
S
N
e
l
i
h
W

.
e
c
n
a
t
s
n
i

t
s
e
t

a

n
o

s
l
e
d
o
m

r
u
o

f
o

e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
5
1

e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,
h
t
u
r
t

d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p
e
h
t
n
e
e
w
t
e
b
h
c
t
a
m
s
i
m
a

r
o
h
c
t
a
m
a

e
t
a
c
i
d
n
i

d
e
t
c
i
d
e
r
p
e
h
t
n
e
e
w
t
e
b
h
c
t
a
m
s
i
m
a

r
o
h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c
d
e
r
d
n
a

e
u
l
b
e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
e
r
o
f
e
b
s
t
c
e
j
b
o
0
1
h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a
n
o
s
l
e
d
o
m

r
u
o
f
o
e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
6
1
e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

n
e
e
w
t
e
b

h
c
t
a
m
s
i
m
a

r
o

h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
r
e
t
f
a

s
t
c
e
j
b
o

0
1

h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a

n
o

s
l
e
d
o
m

r
u
o

f
o

e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
7
1

e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p
e
h
t
n
e
e
w
t
e
b
h
c
t
a
m
s
i
m
a

r
o
h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c
d
e
r
d
n
a

e
u
l
b
e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
e
r
o
f
e
b
s
t
c
e
j
b
o
5
1
h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a
n
o
s
l
e
d
o
m

r
u
o
f
o
e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
8
1
e
r
u
g
i
F

.

n
o
i
t
u
c
e
x
e

g
n
i
r
u
d
n
o
i
t
p
e
c
x
e

n
a

d
e
r
e
t
n
u
o
c
n
e
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

t
a
h
t

s
n
a
e
m
r
o
r
r
E

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

n
e
e
w
t
e
b

h
c
t
a
m
s
i
m
a

r
o

h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
r
e
t
f
a

s
t
c
e
j
b
o

5
1

h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a

n
o

s
l
e
d
o
m

r
u
o

f
o

e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
9
1

e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p
e
h
t
n
e
e
w
t
e
b
h
c
t
a
m
s
i
m
a

r
o
h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c
d
e
r
d
n
a

e
u
l
b
e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
e
r
o
f
e
b
s
t
c
e
j
b
o
0
2
h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a
n
o
s
l
e
d
o
m

r
u
o
f
o
e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
0
2
e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

n
e
e
w
t
e
b

h
c
t
a
m
s
i
m
a

r
o

h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.
g
n
i
n
u
t
-
e
n
ﬁ
r
e
t
f
a

s
t
c
e
j
b
o

0
2

h
t
i

w
e
c
n
a
t
s
n
i

t
s
e
t

a

n
o

s
l
e
d
o
m

r
u
o

f
o

e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
1
2

e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t
d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

4
t
a
d
e
l
i
a
f

t
a
c
n
o
c
-
D
V
S
N

,

n
o
i
t
s
e
u
q
e
n
o
y
l
n
o
g
n
i
r
e
w
s
n
a

t
a
d
e
l
i
a
f

k
c
a
t
s
-
D
V
S
N
e
l
i
h
W

.
g
n
i
n
u
t
-
e
n
ﬁ
e
r
o
f
e
b
B
B

t
i
l
p
s

f
o
e
c
n
a
t
s
n
i

t
s
e
t

a
n
o
s
l
e
d
o
m

r
u
o
f
o
e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
2
2
e
r
u
g
i
F

.

y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t

d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

n
e
e
w
t
e
b

h
c
t
a
m

s
i

m
a

r
o

h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.
s
d
n
u
o
r

-
o
r
p

d
e
t
c
i
d
e
r
p

e
h
t

n
e
e
w
t
e
b

h
c
t
a
m
s
i
m
a

r
o

h
c
t
a
m
a

e
t
a
c
i
d
n
i

s
r
u
o
l
o
c

d
e
r

d
n
a

e
u
l
b

e
h
T

.
g
o
l
a
i
D

-
t
f
a
r
c
e
n
i
M

f
o

e
c
n
a
t
s
n
i

t
s
e
t

a

n
o

s
l
e
d
o
m

r
u
o

f
o

e
l
p
m
a
x
e

e
c
n
e
r
e
f
n
I

:
3
2

e
r
u
g
i
F

.
y
l
e
v
i
t
c
e
p
s
e
r

,

h
t
u
r
t

d
n
u
o
r
g

e
h
t

d
n
a

r
e
w
s
n
a
/
m
a
r
g

