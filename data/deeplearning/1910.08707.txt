Introduction to Coresets:
Accurate Coresets

Ibrahim Jubran, Alaa Maalouf, and Dan Feldman

{ibrahim.jub, Alaamalouf12, dannyf.post}@gmail.com
The Robotics and Big Data Lab
Department of Computer Science
University of Haifa, Israel

Abstract

A coreset (or core-set) of an input set is its small summation, such that solving
a problem on the coreset as its input, provably yields the same result as solving the
same problem on the original (full) set, for a given family of problems (models, classi-
ﬁers, loss functions). Over the past decade, coreset construction algorithms have been
suggested for many fundamental problems in e.g. machine/deep learning, computer
vision, graphics, databases, and theoretical computer science. This introductory paper
was written following requests from (usually non-expert, but also colleagues) regarding
the many inconsistent coreset deﬁnitions, lack of available source code, the required
deep theoretical background from diﬀerent ﬁelds, and the dense papers that make it
hard for beginners to apply coresets and develop new ones.

The paper provides folklore, classic and simple results including step-by-step proofs
and ﬁgures, for the simplest (accurate) coresets of very basic problems, such as: sum
of vectors, minimum enclosing ball, SVD/ PCA and linear regression. Nevertheless,
we did not ﬁnd most of their constructions in the literature. Moreover, we expect
that putting them together in a retrospective context would help the reader to grasp
modern results that usually extend and generalize these fundamental observations.
Experts might appreciate the uniﬁed notation and comparison table that links between
existing results.

Open source code with example scripts are provided for all the presented algorithms,
to demonstrate their practical usage, and to support the readers who are more familiar
with programming than math.

9
1
0
2

t
c
O
9
1

]

G
L
.
s
c
[

1
v
7
0
7
8
0
.
0
1
9
1
:
v
i
X
r
a

1

Introduction

Coreset (or core-set) is a modern data summarization that approximates the original data in
some provable sense with respect to a (usually inﬁnite) set of questions, queries or models and
an objective loss/cost function. The goal is usually to compute the model that minimizes this

1

 
 
 
 
 
 
objective function on the small coreset instead of the original (possibly big) data, without
compromising the accuracy by more than a small multiplicative factor. Moreover, it has
many other applications such as handling constraints, streaming , distributed data, parallel
computation, model compression, parameter tuning, model selection and many more.

The simplest coreset is a (possibly weighted) subset of the input data. The advantages
of such subset coresets are: (i) preserved sparsity of the input, (ii) interpretability, (iii)
coreset may be used (heuristically) for other problems, (iv) less numerical issues that occur
when non-exact linear combination of points are used (Maalouf, Jubran, & Feldman, 2019).
Unfortunately, not all problems admit such a subset coreset, as we show throughout the
paper.

Although coreset constructions are usually practical and not hard to implement, the
theory behind them may be complicated and based on good understanding of linear al-
gebra, statistics, probability, computational geometry and machine learning. Similarly to
approximation algorithms in computer science, there are some generic techniques for coreset
constructions, but many of their constructions are heavily tailored and related to the prob-
lem at hand and its existing solvers. Furthermore, there are many inconsistent deﬁnitions
of coresets in the papers. Nevertheless, it seems that after understanding the intuition and
math behind simple coreset constructions, it is much easier to read modern academic papers
and construct coresets for new problems.

To this end, this paper focuses only on what seems to be the simplest type of core-
sets, namely “accurate coresets”, which do not introduce any approximation error when
compressing the original data, but give accurate solutions.

Most of the coresets in this paper are easy to construct and may be considered as “folk-
lore” results. However, we did not ﬁnd them in the literature, and we realized that many
experts in the ﬁeld are not familiar with them. Furthermore, since most of these results are
easy to construct and explain, we found them to be suitable for tutorials, as the case in this
paper. These results may also be of great interest to people from various ﬁelds of study, who
may not be familiar even with the simple techniques presented in this paper. We assume no
previous knowledge except from basic linear algebra, and therefore we target both experts
and beginners in the ﬁeld, as well as data scientists and analysts. To better understand
the results presented in this paper and to encourage people to use them, we provide full
open-source code for these results (Jubran, Maalouf, & Feldman, 2019).

Another motivation of this introductory survey is to show the many possible diﬀerent
deﬁnitions of coresets and the resulting diﬀerent constructions, as well as summarizing them
in a single place.

Table 1 summarises the diﬀerent accurate coresets that we present in this paper.

2 Preliminalies

In this section, we give basic notations and deﬁnitions that will be used throughout this
paper.

The set of all real numbers is denoted by R. We denote [n] = {1, · · · , n} for every integer
d the (cid:96)2 norm of a point p = (p1, · · · , pd) ∈ Rd, by
i=1 |pi|q the (cid:96)q norm of p for every q > 0, by (cid:107)p(cid:107)∞ = maxi |pi| the (cid:96)∞ norm,

n ≥ 1 by (cid:107)p(cid:107) = (cid:107)p(cid:107)2 = (cid:112)p2
(cid:107)p(cid:107)q = 1/q

1 + . . . + p2

(cid:113)(cid:80)d

2

Coreset C

Coreset Weights

Const.
time

Query
time

Section

O(n)

O(d)

3.1

O(n)

O(1)

3.2

u ≡ 1

u ≡ 1

Name

1-Center

Monotonic
function

Vectors
sum (1)

Vectors
sum (2)

Vectors
sum (3)

1-Mean (1)

1-Mean (2)

Input Weighted Set
(P, w) of size |P | = n

Query Set X

cost function f : P × X

P ⊆ (cid:96) ⊆ Rd
w ≡ 1

P ⊆ R
w ≡ 1

P ⊆ Rd
w : P → R

P ⊆ Rd
w : P → R

P ⊆ Rd
w : P → [0, ∞)

P ⊆ Rd
w : P → R

P ⊆ Rd
w : P → R

X = Rd

f (p, x) = (cid:107)p − x(cid:107)

X = {g | g is monotonic
decreasing/increasing or
increasing and then
decreasing function}

X = Rd

X = Rd

f (p, g) = g(p)

f (p, x) = p − x

f (p, x) = p − x

X = Rd

f (p, x) = p − x

X = Rd

f (p, x) = w(p) (cid:107)p − x(cid:107)2

X = Rd

f (p, x) = w(p) (cid:107)p − x(cid:107)2

1-Mean (3)

P ⊆ Rd
w : P → [0, ∞)

X = Rd

f (p, x) = w(p) (cid:107)p − x(cid:107)2

P = {(ti | pi)}n

i=1 ⊆ Rd+1

w : P → [0, ∞)
P ⊆ Rd
w : P → [0, ∞)

X = (cid:8)g | g : R → Rd(cid:9)

f ((t, p), g) = (cid:107)p − g(t)(cid:107)2

X = Rd

f (p, x) = (pT x)2

P ⊆ Rd
w : P → [0, ∞)

X = Rd

f (p, x) = (pT x)2

loss for
f (p, x)
over p ∈ P

(cid:107)·(cid:107)∞

(cid:107)·(cid:107)∞

Σ

Σ

Σ

(cid:107)·(cid:107)1

(cid:107)·(cid:107)1

(cid:107)·(cid:107)1

(cid:107)·(cid:107)1

(cid:107)·(cid:107)1

(cid:107)·(cid:107)1

C ⊆ P
|C| = 2

C ⊆ P
|C| = 2

C ⊆ Rd
|C| = 1

C ⊆ P
|C| ≤ d + 1

C ⊆ P
|C| ≤ d + 2

C ⊆ Rd × Z × R
|C| = 3
Diﬀerent loss

C ⊆ P
|C| ≤ d + 2

C ⊆ P
|C| ≤ d + 3

C ⊆ Rd+1
|C| = d + 2
C ⊆ Rd
|C| = d

C ⊆ P
|C| ≤ d2 + 1

P = (cid:8)(aT
i

| bi)(cid:9)n
w : P → [0, ∞)

i=1 ⊆ Rd+1

X = Rd

f ((aT | b), x) = (aT x − b)2

(cid:107)·(cid:107)1

C ⊆ P
|C| ≤ (d + 1)2 + 1

1-Segment

Matrix
2-norm (1)

Matrix
2-norm (2)

Least
Mean
Squares

(cid:88)

p∈C

u ≡ (cid:80)

p∈P w(p)

O(n)

O(d)

3.3

u : C → R
(cid:88)
u(p) =

w(p)

O(nd2)

O(d2)

3.3.1



u : C →

0,

u(p) =

(cid:88)

p∈C



w(p)



w(p)

p∈P

(cid:88)

p∈P
(cid:88)

p∈P

O(cid:0) min{n2d2,
nd + d4logn}(cid:1)

O(d2)

3.3.2

unweighted

O(nd)

O(d)

3.4

(cid:88)

p∈C

(cid:88)

p∈C

(cid:88)

u : C → R
(cid:88)
u(p) =

w(p)

p∈C
u(p) (cid:107)p(cid:107)2 =

p∈P
(cid:88)

w(p) (cid:107)p(cid:107)2



u : C →

0,

(cid:88)

u(p) =

p∈P

(cid:88)

p∈P
(cid:88)



w(p)



w(p)

p∈C
u(p) (cid:107)p(cid:107)2 =

p∈P
(cid:88)

w(p) (cid:107)p(cid:107)2

p∈P

u ≡ 1

u ≡ 1


u : C →

0,

(cid:88)

p∈C

u(p) =



u : C →

0,

u(p) =

(cid:88)

p∈C

(cid:88)

p∈P
(cid:88)

p∈P

(cid:88)

p∈P
(cid:88)

p∈P



w(p)



w(p)



w(p)



w(p)

O(nd2)

O(d2)

3.4.1

O(cid:0) min{n2d2,
nd + d4logn}(cid:1)

O(d2)

3.4.2

O(nd2)

O(nd2)

O(d2)

O(d2)

3.5

3.6

O(cid:0) min{n2d4,
nd2 + d8logn}(cid:1) O(d3)

3.6.1

O(cid:0) min{n2d4,
nd2 + d8logn}(cid:1) O(d3)

3.7

Table 1: Coresets that are presented in this paper. The input set, the query set, and the roles
of the functions f and loss are as deﬁned in Deﬁnition 1. The ﬁrst and second arguments of the
function f are elements from the input set and the query set respectively. We assume that the input
set is of size |P | = n, and we wish to compute the loss over the n ﬁtting errors that are deﬁned by
f , each input point and a given query.

(cid:113)(cid:80)m
i=1

(cid:80)n

ij the Frobenius norm of a matrix A ∈ Rm×n, where aij is the
and by (cid:107)A(cid:107)F =
jth entry at the ith row of A. The d-dimensional identity matrix is denoted by Id ∈ Rd×d.
For a function f we denote f 2(·, ·) = (f (·, ·))2. For a set Z of elements, we denote by P(Z)
the power set of Z, i.e., the set of all subsets of Z.

j=1 a2

A weighted set is a pair P (cid:48) = (P, w) where P is a set of items called points, and w : P → R
is a function that maps every p ∈ P to w(p) ∈ R, called the weight of p. A weighted point
is a weighted set of size |P | = 1. A weighted set (P, 1) where 1 is the weight function
w : P → {1} that assigns w(p) = 1 for every p ∈ P may be denoted by P for short.

In order to have a uniﬁed framework, we make the following deﬁnition of a query space,
which will simplify and unify the deﬁnitions of every example coreset presented in this
paper. A query space basically includes all the ingredients needed to deﬁne a coreset for a
new problem that we wish to tackle.

Deﬁnition 1 (query space) Let X be a (possibly inﬁnite) set called query set, P (cid:48) = (P, w)
be a weighted set called the input set, f : P × X → [0, ∞) be called a cost function, and
loss be a function that assigns a non-negative real number for every real vector. The tuple

3

(P, w, X , f, loss) is called a query space. For every weighted set C (cid:48) = (C, u) such that
C = {c1, · · · , cm}, and every x ∈ X we deﬁne the overall ﬁtting error of C (cid:48) to x by

floss(C (cid:48), x) := loss((w(c)f (c, x))c∈C) = loss(w(c1)f (c1, x), · · · , w(cm)f (cm, x)).

An accurate coreset that approximates a set of models (queries) for a speciﬁc problem is

deﬁned as follows:

Deﬁnition 2 (accurate coreset) Let P (cid:48) = (P, w) be a weighted set and (P, w, X , f, loss)
be a query space. The weighted set C (cid:48) is called an accurate coreset for (P, w, X , f, loss) if
for every x ∈ X we have

floss(P (cid:48), x) = floss(C (cid:48), x).

3 Accurate Coresets

In what follows, each subsection presents an accurate coreset construction for a given query
space. Each section is marked with one of the following diﬃculty indicators: (cid:5) (easy), (cid:5)(cid:5)
(intermediate) or (cid:5) (cid:5) (cid:5)(advanced).

3.1 1-Center (cid:5)

Suppose that we want to open a shop on our street that will be close to all the residents in
the street, and suppose that the street is represented by a linear segment, say the x-axis,
while the residents are represented by points on this segment. Since we want to be close to
all the potential n ≥ 1 residents in the street, if we decide to open the shop at some location,
our loss will be measured as the distance to the farthest resident. Suppose that tomorrow we
will be given few locations to choose from to position our store. Can we pre-process the given
positions of the residents so that computing the cost (farthest resident) from the suggested
store location will take only O(1) time for each suggested location? That is, constant time
that is independent of the number of residents n?

Formally, in the query-space notation from Deﬁnition 1, we have that

P = {p1, · · · , pn} ⊆ R, w ≡ 1, X = R, f (p, x) = |p − x|, loss(·) = (cid:107)·(cid:107)∞ .

Our goal is to compute a data structure (accurate coreset) C such that for every number
x ∈ X we can compute

floss((P, 1), x) = (cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)|p1 − x|, · · · , |pn − x|(cid:1)(cid:12)
(cid:12)

(cid:12)
(cid:12)∞ = max
p∈P

|p − x|

in O(1) time using only C. This can be easily done by observing that for every x ∈ X , the
farthest point from x is either the smallest point pmin or the largest point pmax in P ; See
Fig. 1a. That is, simply choosing C = {pmin, pmax} ⊆ P yields

floss((P, 1), x) = max
p∈P

|p − x| = max {|pmin − x|, |pmax − x|} = floss((C, 1), x),

4

i.e., the distance to the farthest input point from x is either the distance to the leftmost or
the rightmost point of P .

Even for the case that X = Rd and where P is contained on a line (cid:96) in Rd, we would still
have floss((P, 1), x) = floss((C, 1), x) where C contains the two edge points on (cid:96); See Fig. 1b.
Indeed, denote by x(cid:48) the projection (closest point) of x onto (cid:96) to obtain by the Pythagorean
Theorem that

f 2
loss((P, 1), x) = max
p∈P

= max
p∈C

(cid:107)p − x(cid:107)2 = max
p∈P
(cid:107)p − x(cid:48)(cid:107)2 + (cid:107)x(cid:48) − x(cid:107)2 = max
p∈C

(cid:107)p − x(cid:48)(cid:107)2 + (cid:107)x(cid:48) − x(cid:107)2

(cid:107)p − x(cid:107)2 = f 2

loss((C, 1), x).

See implementation of function one center in (Jubran et al., 2019).

(a) An input set P ⊆ R (in blue) on the x-axis
and a query point x ∈ R (in green).

(b) A line (cid:96) in Rd, an input set P ⊆ (cid:96) (in blue)
and a query point x ∈ Rd (in green).

Figure 1: An accurate coreset for the 1-center query space (red points). Both in Fig. 1a
and Fig. 1b, the farthest point p ∈ P from the query x is either the ﬁrst point (pmin) or the last
point (pmax) on the line, i.e., floss((P, 1), x) = max {(cid:107)pmin − x(cid:107) , (cid:107)pmax − x(cid:107)}.

The solution in this section does not generalize to an arbitrary set of points in Rd. In fact,
the following set of points on the plane does not have any subset which is an accurate coreset.
Let P ⊆ R2 denote n points on the unit circle in the plane, and let C ⊂ P . For p ∈ P \ C
and x = −p we have that floss((P, 1), x) = floss({p} , x) (cid:54)= floss((C, 1), x). Hence, there is no
subset C which is a coreset for P in this sense. Nevertheless approximated coresets can be
found in (Paul, Feldman, Rus, & Newman, 2014) and references therein.

Furthermore, the above solution does not generalize for the case where the input is
weighted, even for d = 1, i.e., there is a weighted set of points (P, w) where P ⊆ R such that
for every p ∈ P there is x ∈ R that satisﬁes floss((P, w), x) = w(p)|p − x|. In other words,
assume pj ∈ P was not chosen for the coreset (C, u), then there is some query x ∈ R such
that floss((P, w), x) = w(pj)|pj − x| (cid:54)= floss((C, u), x).

We now construct such an example. Let (P, w) be a weighted set of |P | = n points where

and

w(p1) = 2, w(pi) = wpi−1 +

(cid:19)i−1

,

(cid:18) 1
4

pi =

25−i
w(pi)

,

5

Figure 2: No accurate coreset exists for the weighted 1-center problem. A weighted set
(P, w), P ⊆ R, where w(p1) = 2, w(pi) = wpi−1 + (cid:0) 1
w(pi) . For every pi ∈ P , we can
compute a query xi ∈ R where xi = −24+i + 1 such that floss((P, w), xi) = maxp∈P w(p)|p − xi| =
w(pi)|pi − xi|.

(cid:1)i−1, and pi = 25−i

4

as illustrated in Fig. 2. For every pi ∈ P , by deﬁning xi = −24+i + 1, it is easy to verify
that floss((P, w), xi) = maxp∈P w(p)|p − xi| = w(pi)|pi − xi|. Therefore, any coreset for this
problem must include all the input points in (P, w). Thus there is no accurate coreset for
the weighted 1-center problem, even in 1-dimensional space.

3.2 Monotonic functions (cid:5)

What if the function f (p, x) = (cid:107)p − x(cid:107) from Section 3.1 is not an Euclidean distance, but a
function of this distance g((cid:107)p − x(cid:107)), where g(y) = y2 so f (p, x) = (cid:107)p − x(cid:107)2 is the squared
Euclidean distance from x, or g(y) = min {y, 1} so f (p, x) = min (cid:8)1, (cid:107)p − x(cid:107)2(cid:9)? The latter
one is called an M-estimator and is robust to points that are very far from x (outliers). It
turns out that the coreset from the previous section holds for the following cases. Consider
the query space (P, w, Rd, f, loss) where P = {p1, · · · , pn} ⊆ R, w ≡ 1, the query set X is
the union over every function g : R → [0, ∞) that is a non-negative decreasing, increasing,
or decreasing and then increasing monotonic function, f (p, g) = g(p), and loss(·) = (cid:107)·(cid:107)∞.
Note that here every query is actually a function and not a point. Hence,

floss((P, 1), g) = (cid:107)(g(p1), · · · , g(pn))(cid:107)∞ = max
p∈P

g(p).

Again, the main observation is that the maximum value of g(p) over p ∈ P is obtained in
p; See Fig. 3. Therefore, the coreset
one of the points pmax ∈ arg max

p or pmin ∈ arg min

p∈P
C = {pmin, pmax} from Section 3.1 is also valid here.

p∈P

Figure 3: Accurate coreset for monotonic functions. A set P ⊆ R and a “decreasing then
increasing” monotonic function g : R → [0, ∞). The point that maximizes g(p) over every p ∈ P is
either pmin or pmax, i.e., floss((P, 1), g) = max {g(pmin), g(pmax}).

6

3.3 Vectors sum (cid:5)(cid:5)

The accurate coreset for vectors sum example presented in Sections 3.3–3.3.2 is a warm-up
example, that will be used in later sections.

Consider the query space (P, w, Rd, f, loss) where P = {p1, · · · , pn} is a set of n points in
Rd, w : P → R, X = Rd, f (p, x) = p − x and loss maps every tuple (v1, v2, · · · , ) of vectors to
their sum (cid:80)
i vi (which is also a vector). In this section, unlike other sections, the function
f , as well as loss, return a vector and not a positive scalar as in Deﬁnition 1. This query set
deﬁnes the weighted mean or sum of the diﬀerences (p − x) over p ∈ P , i.e.,

floss((P, w), x) =

w(p)(p − x).

(cid:88)

p∈P

It is easy to see that there is a coreset of a single weighted point to this query space.

Indeed, let

c =

(cid:80)
(cid:80)

p∈P w(p)p
p∈P w(p)

,

p∈P w(p). Here we assume that (cid:80)

p∈P w(p) (cid:54)= 0. Otherwise, set

C = {c} and u(c) = (cid:80)
c = (cid:80)

p∈P w(p)p and u(c) = 1.

We now have that the weighted mean of (c − x) over c ∈ C (a single point) is

floss((C, u), x) = u(c)(c − x) =

=

(cid:88)

p∈P

w(p)p − x

(cid:88)

p∈P
(cid:88)

p∈P

w(p)(c − x) =

w(p)p − x ·

(cid:88)

p∈P

w(p)

(cid:88)

p∈P

w(p) =

(cid:88)

p∈P

w(p)(p − x) = floss((P, w), x).

See implementation of function vectors sum 1 in (Jubran et al., 2019).

3.3.1 Subset coreset

The coreset for vectors sum in the previous section was not a subset of the input set P , i.e.,
C = {c} (cid:54)⊆ P . In this section, the goal is to compute a weighted set C (cid:48) = (C, u) where
C ⊆ P is a subset of the input such that floss(C (cid:48), x) = floss(P (cid:48), x). The motivation of a
subset coreset is explained in Section 1. Let ˆp = (pT | 1)T ∈ Rd+1 for every p ∈ P denote
the concatenation of p with 1, and let ˆP = {ˆp | p ∈ P }.

Given a distinct pair of points p, q on a line in Rd, we can describe any third point z
on the line as a linear combination of p and q. More generally, every point in Rd can be
described by d independent vectors (points). Recall that in linear algebra such a set of d
vectors is called a basis of Rd. Speciﬁcally, since (cid:80)
ˆp∈ ˆP w(p)ˆp ∈ Rd+1 is spanned by (i.e., a
linear combination of) the points of ˆP , there exists a set ˆC ⊆ ˆP of | ˆC| = d + 1 points, and
a weight function ˆu : ˆC → R such that

ˆu(ˆp)ˆp =

(cid:88)

ˆp∈ ˆC

(cid:88)

ˆp∈ ˆP

w(p)ˆp =

(cid:32)

(cid:88)

p∈P

w(p)p

(cid:33)

w(p)

.

(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈P

(1)

7

The set ˆC can be computed as explained in Section A. Here we assume |P | > d+1, otherwise
we let (C, u) = (P, w) be our coreset, since P is already small. More generally, it is not hard
to verify that |C| can be small as the rank of the matrix whose rows are the points in ˆP .

(cid:110)

(cid:111)

p ∈ P | ˆp ∈ ˆC

and u : C → R such that u(p) = ˆu(ˆp) for every p ∈ C. It now

Let C =
follows that

u(p)(p | 1) =

(cid:88)

p∈C

ˆu(ˆp)ˆp =

(cid:88)

ˆp∈ ˆC

(cid:32)

(cid:88)

p∈P

w(p)p

(cid:33)

w(p)

,

(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈P

(2)

where the ﬁrst equality is by the deﬁnition of u and C, and the second equality is by (1).
By (2) we obtain that

u(p)p =

(cid:88)

p∈C

(cid:88)

p∈P

w(p)p and

u(p) =

(cid:88)

p∈C

w(p).

(cid:88)

p∈P

(3)

Hence, the weighted mean of (p − x) over p ∈ C is

floss((C, u), x) =

=

(cid:88)

p∈C
(cid:88)

p∈P

u(p)(p − x) =

(cid:88)

p∈C

u(p)p − x

(cid:88)

u(p)

w(p)p − x ·

w(p) =

(cid:88)

p∈P

(cid:88)

p∈P

p∈C
w(p)(p − x) = floss(P (cid:48), x),

where the third equality is by (3).

Observe that by (3) we obtain that the sum of weights of the coreset is the same as
the sum of original weights. We thus have that C (cid:48) = (C, u) is an accurate coreset of size
|C| ≤ d + 1, which is also a subset of the input set P , for the given query space.

3.3.2 Subset Coreset of bounded positive weights

In the coreset of the previous section, the coreset’s weights might be both negative and
unbounded, even if the weights of the input points are bounded. This may cause serious
issues as explained in (Maalouf et al., 2019). In this section we prove that if the input weights
are non-negative and sum to one, i.e., w : P → [0, 1] and (cid:80)
p∈P w(p) = 1, then there is an
accurate coreset C (cid:48) = (C, u), where C ⊆ P consists of |C| = d + 2 points which is larger
by one than the previous coreset, but has a non-negative weight function u : C → [0, 1]
which also sums to one, i.e., (cid:80)
p∈C u(p) = 1, instead of the previous unbounded weight
function. This means that the weights are both non-negative and cannot be arbitrarily
large, which reduces numerical issues. We then show how to naturally extend this result for
the case where the input weights are non-negative but do not necessarily sum to one. In this
generalized case, we compute an accurate coreset (C, u) also of size |C| = d + 1 such that
u : C → [0, (cid:80)

p∈P w(p)] and (cid:80)

p∈P w(p) = (cid:80)

p∈C u(p).

Input weights are non-negative and sum to one. Let ˆp = (p | 1)T ∈ Rd+1 for every
p ∈ P , and let ˆP = {ˆp | p ∈ P } as in the previous example. Observe that if P is a set
of points on a line, its mean z must lie in the interval between the rightmost and leftmost
points p and q, respectively, of P . This implies that the mean is a convex combination of p

8

and q, i.e., z = w1p + w2q for some w1, w2 ≥ 0 such that w1 + w2 = 1 and w1, w2 ≥ 0. For
a set P of points on the plane, the mean of P is inside the convex hull of P , which is the
smallest polygon that contains P , and there must be a triangle whose vertices are in P , that
also contains z; See Fig. 4.

More generally, Caratheodory’s Theorem (Carath´eodory, 1907; Cook & Webster, 1972)
states that if a point z lies inside a convex hull of the set ˆP ⊆ Rd(cid:48), then z also lies inside the
convex hull (i.e., it is a convex combination) of at most d(cid:48) + 1 points in ˆP . Hence, z can be
expressed as a convex combination of at most d(cid:48) + 1 points in ˆP . When the input weights
sum to one, i.e., (cid:80)
ˆp∈ ˆP w(p)ˆp of ˆP lies inside the convex
hull of ˆP . Therefore, since ˆP ⊆ Rd(cid:48) = Rd+1 and each point ˆp is given a weight of w(p) where
(cid:80)
ˆp∈ ˆP w(p) = 1, by Caratheodory’s theorem there is a subset ˆC ⊆ ˆP , | ˆC| = d(cid:48) + 1 = d + 2
and ˆu : ˆC → [0, 1] such that

p∈P w(p) = 1, the weighted mean (cid:80)

ˆu(p)p =

(cid:88)

p∈ ˆC

(cid:88)

ˆp∈ ˆP

w(p)ˆp =

(cid:32)

(cid:88)

p∈P

w(p)p

(cid:33)

w(p)

,

(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈P

(4)

where the second equality is by the deﬁnition of ˆp. ˆC and ˆu can be computed as explained
in Section 4.
Let C =
now follows that

and u : C → [0, 1] such that u(p) = ˆu(ˆp) for every p ∈ C. It

p ∈ P | ˆp ∈ ˆC

(cid:110)

(cid:111)

u(p)(p | 1) =

(cid:88)

p∈C

ˆu(ˆp)ˆp =

(cid:88)

p∈ ˆC

(cid:32)

(cid:88)

p∈P

w(p)p

(cid:33)

w(p)

,

(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈P

(5)

where the ﬁrst equality holds by the deﬁnition of u and C, and the second equality holds
by (4).

We now have that

floss((C, u), x) =

=

(cid:88)

p∈C
(cid:88)

p∈P

u(p)(p − x) =

w(p)p − x

(cid:88)

p∈P

(cid:88)

p∈C

u(p)p − x

u(p)

(cid:88)

p∈C

w(p) = floss((P, w), x),

where the third equality is by (5).

Hence, we obtain that C (cid:48) = (C, u) is an accurate coreset for P (cid:48) = (P, w), where C is of
size |C| = d + 2, and its weight function u is non-negative and sums to one over the points
of C.

Generalized case of non-negative weights. We ﬁrst remind the reader that ˆp = (p | 1)
for every p ∈ P and ˆP = {ˆp | p ∈ P }. Since the Caratheodory Theorem cannot be applied to
the weighted set (P, w) whose weights do not necessarily sum to one, we apply the following
steps: (i) deﬁne a new weighted set ( ˆP , ˆw) where ˆw : ˆP → [0, 1] such that ˆw(ˆp) =

w(p)
q∈P w(q)
for every p ∈ P , (ii) apply the Caratheodory Theorem to ( ˆP , ˆw) to compute a weighted set

(cid:80)

9

( ˆC, ˆu) with the same weighted sum as ( ˆP , ˆw) as explained in Section 4, and (iii) return
the weighted set (C, u) such that C =
p∈P w(p)] where
u(p) = ˆu(ˆp) · (cid:80)
p∈P w(p).

and u : C → [0, (cid:80)

p ∈ P | ˆp ∈ ˆC

Similarly to the proof in the previous case, it is easy to verify that u(p) ∈ [0, (cid:80)

(cid:110)

(cid:111)

p∈P w(p)]

for every p ∈ C, (cid:80)

p∈P w(p) = (cid:80)

p∈C u(p), and for every x ∈ Rd

floss((C, u), x) = floss((P, w), x).

See implementation of function vectors sum 3 in (Jubran et al., 2019).

Figure 4: Caratheodory’s Theorem. A set P ⊆ R2 (blue points), its mean z (red point), and
the convex hull of P (green segments). The mean z is contained inside the convex hull of P . There
exists a set ˆP ⊆ P of | ˆP | = d + 1 = 3 points (bigger blue points) such that z lies inside the convex
hull of ˆP (the black lines).

3.4 1-Mean queries (cid:5)(cid:5)

Trying to minimize the distance from our shop to the farthest client is very sensitive to what
is called outliers in the sense that a location of a single client p (e.g. p approaching inﬁnity)
may signiﬁcantly change our cost function floss((P, w), x). A less sensitive cost function
may wish to select the location that minimizes the average sum of squared (Least Mean
Squared, LMS) distances from the clients to our shop. To this end, let P ⊆ R be a set of
|P | = n numbers, X = R, f (p, x) = (p − x)2, and loss(v) = 1
i=1 vi for every
v = (v1, · · · , vn) ∈ Rn. The name 1-mean queries or coreset was given since the mean of P
is the query x ∈ X that minimizes this cost. We wish to be able to compute the average
sum of squared distances floss(P, x) = 1
p∈P (p − x)2 in O(1) time to a (currently unknown)
n
location x ∈ X which will be given tomorrow, after pre-processing time of O(n) today; See
Fig. 5.

n (cid:107)v(cid:107)1 = 1

(cid:80)n

(cid:80)

n

This can be done by observing that

floss(P, x) =

1
n

(cid:88)

(p − x)2 =

p∈P

1
n

(cid:88)

p∈P

(cid:0)p2 − 2xp + x2(cid:1) =

1
n

(cid:88)

p∈P

p2 + x2 − 2x ·

1
n

(cid:88)

p.

p∈P

10

(a)

(b)

Figure 5: 1-mean queries. A set of resident locations, marked by the same humans in each of the
two images. We are given O(n) time to pre-process the resident locations. Then, given two potential
locations for a shop as in the left or right images, we need to select the location that minimizes the
average sum of squares distances (blue lines / red lines) in O(1) time.

(cid:80)

(cid:110) 1
n

Hence, to evaluate the sum of squared distances from P to x, all we need is to store the
which consists of two numbers. Clearly C can be
(coreset) C =
computed in O(n) time and using C we can compute floss(P, x) exactly for every number
x ∈ X = R by deﬁning a new function h : P(R) × R → R where h({a, b} , x) = a + x2 − 2xb.
Hence, h(C, x) = floss(P, x) for every x ∈ R.

p∈P p2, 1

p∈P p

(cid:80)

A similar solution holds for a set P in Rd and X = Rd for any d ≥ 1 Euclidean space,

(cid:111)

n

and for any w : P → R as

floss((P, w), x) =

(cid:88)

p∈P

w(p) (cid:107)p − x(cid:107)2 =

(cid:88)

w(p) (cid:107)p(cid:107)2 + (cid:107)x(cid:107)2 (cid:88)

w(p) − 2xT (cid:88)

w(p)p,

p∈P

p∈P

p∈P

where we assume that every p ∈ P and x ∈ Rd is a column vector. Here, by letting
p∈P w(p), (cid:80)
p∈P w(p) (cid:107)p(cid:107)2 , (cid:80)
C =
contain two numbers and the weighted
mean vector of P and modifying h as

p∈P w(p)p

(cid:110)(cid:80)

(cid:111)

h({a, b, c} , x) = a + (cid:107)x(cid:107)2 · b − 2xT c

for every a, b ≥ 0 and c ∈ Rd, we obtain floss((P, w), x) = h(C, x) for every x ∈ Rd. This set
C contains the second (variance), ﬁrst (center of mass or mean), and zero moments of P .

Unlike in previous sections, in this example, this coreset C is not a subset of the input

data and we also use a new cost function.

3.4.1 Subset coreset

In this section we wish to construct a coreset for 1-mean queries which uses the same cost
function and is also a subset of the input. For every p ∈ P , let ˆp = (pT | (cid:107)p(cid:107)2 | 1)T be a

11

corresponding vector in Rd+2 and ˆP = {ˆp | p ∈ P } be the union of these vectors. Since the
weighted mean (cid:80)
ˆp∈ ˆP w(p)ˆp is spanned by ˆP (i.e., linear combination of its subset), there is a
subset ˆC ⊆ ˆP of at most | ˆC| = d + 2 points with a corresponding weight function ˆu : ˆC → R
such that

ˆu(ˆp)ˆp =

w(p)ˆp.

(6)

(cid:88)

(cid:88)

ˆp∈ ˆC

ˆp∈ ˆP

The set ˆC can be computed as explained in Section A.
(cid:111)

(cid:110)

p ∈ P | ˆp ∈ ˆC

, and let u : C → R such that u(p) = ˆu(ˆp) for every p ∈ C. We

Let C =
now have that





(cid:80)

(cid:80)
p∈C u(p)p
p∈C u(p) (cid:107)p(cid:107)2
(cid:80)
p∈C u(p)



 =

(cid:88)

ˆp∈ ˆC

ˆu(ˆp)ˆp =

w(p)ˆp =

(cid:88)

ˆp∈ ˆP





(cid:80)

(cid:80)
p∈P w(p)p
p∈P w(p) (cid:107)p(cid:107)2
(cid:80)
p∈P w(p)



 ,

(7)

where the ﬁrst equality is by the deﬁnitions of C and u, the second equality is by (6), and
the last equality is by the deﬁnition of ˆP . Therefore, for every x ∈ Rd, we have that

floss((C, u), x) =

u(p) (cid:107)p − x(cid:107)2 =

(cid:88)

p∈C

=

(cid:88)

p∈C
(cid:88)

p∈P

u(p) (cid:107)p(cid:107)2 + (cid:107)x(cid:107)2 ·

(cid:88)

u(p) − 2xT (cid:88)

u(p)p

w(p) (cid:107)p(cid:107)2 + (cid:107)x(cid:107)2 ·

p∈C
(cid:88)

p∈P

p∈C

w(p) − 2xT (cid:88)

w(p)p

p∈P

= floss((P, w), x),

where the third derivation holds by (7).

Unlike in the previous case, here the coreset is simply a scaled (weighted) subset of P

and the cost function floss is the same as for the input.

A natural question that comes to mind at this point is “can we have a subset which is
not weighted, i.e., w(p) = 1 for every p ∈ P ?” Probably not, since this would imply that
the mean of P is the mean of a (non-weighted) subset of P which cannot hold in general,
even for a set P of 3 points on a line. Nevertheless, we can construct such a coreset that
yields approximated answers to 1-mean queries (Inaba, Katoh, & Imai, 1994). For the case
of exact solution, we can still bound the weights as follows.

3.4.2 Subset coreset of bounded weights
In Section 3.4.1 we used a linear combination of d+2 points from ˆP to represent the weighted
sum (cid:80)
ˆp∈ ˆP w(p)ˆp. Instead, if the input weights are non-negative, i.e., w : P → [0, ∞), we
can apply Caratheodory’s theorem, similarly to Section 3.3.2, to compute a subset ˆC ⊆ ˆP
of size | ˆC| = d + 3 along with a weights function ˆu : ˆC → [0, (cid:80)
p∈P w(p)] that satisﬁes
(cid:80)

p∈ ˆC ˆu(p) = (cid:80)

p∈P w(p) and

ˆu(ˆp)ˆp =

(cid:88)

ˆp∈ ˆC

w(p)ˆp.

(cid:88)

ˆp∈ ˆP

12

Figure 6: A signal P = (t1 | pT
4 ) (red dots), where t1 = 1, t2 = 2.2, t3 = 4, and t4 = 5,
and a 1-segment g : R → R2 (blue line). The cost floss((P, 1), g) is the sum over the squared vertical
distances (segments in greens) (cid:107)pi − g(ti)(cid:107)2 for every i ∈ {1, · · · , 4}.

1 ), · · · , (t4 | pT

Now, by deﬁning C =

(cid:110)

p ∈ P | ˆp ∈ ˆC

(cid:111)

and u(p) = ˆu(ˆp) for every p ∈ C, we obtain for

every x ∈ Rd that

See implementation of function one mean 3 in (Jubran et al., 2019).

floss((C, u), x) = floss((P, w), x).

3.5 Coreset for 1-segment queries (cid:5) (cid:5) (cid:5)

As stated in (Rosman, Volkov, Feldman, Fisher III, & Rus, 2014), there is an increasing
demand for systems that learn long-term, high-dimensional data streams. Examples include
video streams from wearable cameras, mobile sensors, GPS data, ﬁnancial data, audio signals,
and many more. In such data, a time instance is usually represented as a high-dimensional
signal, for example location vectors, stock prices, or image content feature histograms. In
other words, such data is usually represented as a set of linear segments. Fast and real-
time algorithms for summarization and segmentation of such large streams are of great
importance, and can be made possible by compressing the input signals into a compact
meaningful representation, which we call coreset for 1-segment.

Let (P, w) be a weighted set where P = (cid:8)(ti | pT

i=1 ⊆ Rd+1 represents a (discrete)
signal, for every i ∈ [n], ti ∈ R represents the time stamp and pi ∈ Rd is a point, and
w : P → [0, ∞). For simplicity we abuse notation and use w(p) to denote w((t | pT )) for
every (t | pT ) ∈ P . Let X = (cid:8)g | g : R → Rd(cid:9) be the set of all 1-segments, f ∈ X be a
1-segment such that f ((t | pT ), g) = (cid:107)p − g(t)(cid:107)2 for every (t | pT ) ∈ P and g ∈ X , and
loss(·) = (cid:107)·(cid:107)1. Therefore, as shown in Fig. 6,

i )(cid:9)n

floss((P, w)g) =

(cid:88)

(t|pT )∈P

w(p) (cid:107)p − g(t)(cid:107)2 .

The goal is to compute a weighted set (C, u) that represents a weighted (discrete) signal

of size |C| = d + 2, and u : C → [0, ∞) such that for every 1-segment g ∈ X , it holds that

floss((P, w), g) = c · floss((C, u), g).

13

Put g ∈ X . Since g is a linear segment, there exists a, b ∈ Rd such that g(t) = a + b · t

for every t ∈ R.

Let X ∈ Rn×(d+2) be a matrix whose ith row is (cid:112)w(pi) · (1 | ti | pT

i ) for every i ∈ [n]. Let
U ΣV T be the thin SVD of X; see Section A, and u ∈ Rd+2 be the leftmost column of ΣV T .

Let c = (cid:107)u(cid:107)2

d+2 and Z ∈ R(d+2)×(d+2) be an orthogonal matrix such that

√

Zu = (

c, · · · ,

√

c)T ∈ Rd+2,

i.e., Z can be regarded as a rotation matrix that rotates u to the vector (
√
a matrix Z exists since (cid:107)(

c)(cid:107) = (cid:107)u(cid:107).

c, · · · ,

√

(8)

√

c, · · · ,

√

c)T . Such

Let B ∈ R(d+2)×(d+1) be the (d+1) rightmost columns of ZΣV T
c
√

√

√

of u, B, and the fact that Zu = (

c, · · · ,

c)T yields that

. Combining the deﬁnitions

ZΣV T =






√
c
...
√
c



√

cB


 .

(9)

Let C ⊆ Rd+2 be the union of rows of B and u : C → [0, ∞) such that u(p) = c for every

p ∈ C. Then

floss((P, w), g) =

(cid:88)

(t|pT )∈P

w(p) (cid:107)p − g(t)(cid:107)2 =

(cid:88)

(t|pT )∈P

w(p) (cid:107)a + b · t − p(cid:107)2

(10)

(cid:88)

=

(cid:13)
(cid:112)w(p)(a + b · t − p)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

=

(t|pT )∈P

(cid:13)
(cid:13)
(cid:13)
U ΣV T
(cid:13)
(cid:13)
(cid:13)

=





aT
bT
−I





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)
ZΣV T
(cid:13)
(cid:13)
(cid:13)





aT
bT
−I







(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:112)w(p1)(1 | t1 | pT
1 )
...


(cid:112)w(pn)(1 | tn | pT
n )
√


c
...
√
c

cB







√

=





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)










aT
bT
−I





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

aT
bT
−I





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(11)

= c


1
... B


1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)










aT
bT
−I





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

=

(t|pT )∈B

c (cid:107)a + b · t − p(cid:107)2 = floss((C, u), g),

where (t | pT ) ∈ B denotes a row (t | pT ) of B which is the concatenation of a scalar
t ∈ R and pT ∈ Rd, the ﬁrst derivation in (11) is by the deﬁnition of U ΣV T = X, the second
derivation in (11) holds since U and Z are orthogonal matrices and the last derivation in (11)
is by (9). Therefore, the coreset (C, u) is an accurate coreset for the given query space.

See implementation of function one segment in (Jubran et al., 2019).

3.6 Coreset for Matrix 2-norm (cid:5)(cid:5)

A common approach to reduce the dimension of a high-dimensional data set P in Rd is to
project the vectors of P (database records) onto some low k-dimensional aﬃne subspace (k

14

is usually much smaller than d). For example, a subspace that minimizes the sum of squared
distances ((cid:96)2 norm) to these input vectors, maybe under some constraints. Example algo-
rithms include the Principle Component Analysis (PCA), Low-rank approximation (k-rank
SVD) and Latent Dirichlet Analysis (LDA), and non-negative matrix factorization (NNMF).
Learning algorithms such as k-means clustering can then be applied on the low-dimensional
data to obtain faster approximations with provable guarantees (Feldman, Schmidt, & Sohler,
2013). Dimensionality reduction is also used to avoid overﬁtting. Small number of features
usually implies faster running/classiﬁcation times and simpler models. Furthermore, smaller
dimension means faster training of algorithms, less storage, less redundant features, and
many more advantages. However, the dimensionality reduction algorithms might be both
time and space consuming. Therefore, to boost the running time of such algorithms, we can
use accurate coresets as follows

Let (P, w) be a weighted set where P = {p1, · · · , pn} is a set of n points in Rd and
w : P → [0, ∞) is a non-negative weights function, let X = Rd, x ∈ X , f (p, x) = (pT x)2
where pT x = (cid:104)p, x(cid:105) is the inner product of p and x, for every p ∈ Rd and x ∈ X , and
loss(·) = (cid:107)·(cid:107)1. If we deﬁne A as an n × d matrix whose ith row is (cid:112)w(pi)pi, then
1 x)2, · · · , w(pn)(pT

floss((P, w), x) = (cid:13)
(cid:0)w(p1)(pT
(cid:13)

w(p)(pT x)2 = (cid:107)Ax(cid:107)2 .

n x)2(cid:1)(cid:13)

(cid:88)

(cid:13)1 =

p∈P

We aim to compute a matrix C ∈ Rd×d such that (cid:107)Ax(cid:107)2 = (cid:107)Cx(cid:107)2. This can be done by
letting Q ∈ Rn×d be a matrix of orthogonal columns (QT Q = I) that spans the d columns
of A, e.g. via Grahm-Shmidt (also known as the A = QR as shown in Fig. 7), or the Thin
Singular Value Decomposition (A = UrDrV T
r as shown in Fig. 8–9). By letting C = R be a
d × d matrix whose columns correspond to the columns of A under the base Q, we obtain
A = QR = QC. Since Q has orthogonal columns, we have (cid:107)Qx(cid:107) = (cid:107)x(cid:107) for every x ∈ Rd.
Therefore, by deﬁning C ⊆ Rd to contain the rows of C we obtain that

floss((P, w), x) = (cid:107)Ax(cid:107)2 = (cid:107)QCx(cid:107)2 = (cid:107)Cx(cid:107)2 = floss((C, 1), x).

(12)

Note that, without loss of generality, we can assume that (cid:107)x(cid:107) = 1, i.e., that X is a set of
only unit vectors. This is because for every vector y ∈ Rd there is c ≥ 0 and a unit vector
x = y/ (cid:107)y(cid:107) such that

floss((P, w), y) = (cid:107)Ay(cid:107)2 = (cid:107)Acx(cid:107)2 = c2 (cid:107)Ax(cid:107)2 = c2 (cid:107)Cx(cid:107)2 = (cid:107)Ccx(cid:107)2 = (cid:107)Cy(cid:107)2 = floss((C, 1), y).

Geometrically, if x is a unit vector, f (p, x) = (pT x)2 is the squared distance between
a point p in Rd and a hyperplane that intersects the origin and is orthogonal to x. More
generally, the coreset (C, 1) from (12) can be used to compute the weighted sum of squared
distances dist2((P, w), S) over the points of P to a j-dimensional subspace of Rd that is
spanned by the orthonormal columns of a matrix S ∈ Rd×j, i.e., ST S = I. Let S⊥ ∈ Rd×(d−j)
denote the matrix whose columns span the subspace that is orthogonal to S, i.e., [S | S⊥]T [S |
S⊥] = I. Observe that by the deﬁnition of S⊥ we have dist2((P, w), S) = (cid:13)
(cid:13)AS⊥(cid:13)
2
F . Then,
(cid:13)
by the Pythagorean Theorem

dist2((P, w), S) = (cid:13)

(cid:13)AS⊥(cid:13)
2
F =
(cid:13)

d−j
(cid:88)

i=1

(cid:13)
(cid:13)AS⊥

(cid:13)
2
(cid:13)

∗i

=

d−j
(cid:88)

i=1

15

(cid:13)
(cid:13)CS⊥

(cid:13)
2
(cid:13)

∗i

= dist2((C, 1), S),

(13)

where S⊥
property in (12).

∗i denotes the ith column of S⊥, and where the third equality is by the coreset

Figure 7: QR-Decomposition. Left: Given a set of linearly independant vectors {a1, a2} ⊆ R2.
The goal is generate an orthogonal set {e1, e2} ⊆ R2 that spans the same subspace as the given set.
This process is also called the Gram-Schmidt algorithm in Linear Algebra. Right: The result is
a QR-decomposition of the matrix A whose columns are a1, a2 into an orthogonal matrix Q (i.e.,
QT Q = I2) and an upper triangular matrix R.

3.6.1 Subset coreset of bounded weights

Recall that A ∈ Rn×d is a matrix whose rows are the weighted points
. The
set C ⊆ Rd from the previous example contains d points in Rd but are not a subset of the
input set P . This has few disadvantages that were discussed in Section 1. We now show how
to obtain such a weighted set (C, u) where C is a subset of P , where every point p in C is
also assigned to a non-negative weight u(p) ∈ [0, ∞). Observe that for every x ∈ Rd,

i=1

(cid:110)(cid:112)w(pi)pi

(cid:111)n

floss((P, w), x) = (cid:107)Ax(cid:107)2 = xT AT Ax = xT

(cid:33)

w(p)ppT

x.

(cid:32)

(cid:88)

p∈P

its rows. Hence, (cid:80)
p∈P w(p)vec(ppT ) = (cid:80)
of the matrix M ∈ Rd×d. Let ˆP = {ˆp | p ∈ P } and let ˆw(ˆp) =

For every p ∈ P , the d × d matrix ppT corresponds to a vector ˆp ∈ Rd2 by concatenating
p∈P w(p)ˆp, where vec(M ) ∈ Rd2 is a row stacking
q∈P w(q) for every ˆp ∈ ˆP .
lies inside the convex hull of ˆP , by applying

Since ˆP ⊆ Rd2 and the weighted sum of

(cid:16) ˆP , ˆw

w(p)

(cid:17)

(cid:80)

Caratheodory’s Theorem on the weighted set
of size | ˆC| = d2 + 1 and a weights function ˆu : ˆC → [0, 1] such that

(cid:16) ˆP , ˆw

(cid:17)

, we obtain that there is a subset ˆC ⊆ ˆP

1
p∈P w(p)

(cid:80)

(cid:88)

ˆp∈ ˆP

w(p)ˆp =

ˆw(ˆp)ˆp =

(cid:88)

ˆp∈ ˆP

ˆu(ˆp)ˆp.

(cid:88)

ˆp∈ ˆC

Multiplying (14) by (cid:80)

p∈P w(p) we obtain
(cid:88)

w(p)ˆp =

ˆp∈ ˆP

w(p) ·

(cid:88)

p∈P

(cid:88)

ˆp∈ ˆC

ˆu(ˆp)ˆp.

16

(14)

(15)

Figure 8: Singular Value Decomposition. Left: A Singular Value Decomposition U DV T of
a matrix A ∈ Rm×n, where U ∈ Rm×m and V ∈ Rn×n are orthogonal matrices and D ∈ Rm×n is
a diagonal matrix that contains the singular values of A. Right: Visualization of the SVD of a
matrix A ∈ R2×2. The matrix M distorts the unit disc to an ellipse. The SVD is a decomposition
of A into three simple transformations: an initial rotation (possibly with reﬂection) V T , a scaling
D along the coordinate axes, and a ﬁnal rotation (possibly with reﬂection) U . The lengths σ1 and
σ2 of the semi-axes of the ellipse are the singular values of A, namely d1 and d2. Illustrations taken
from (Wikipedia contributors, 2019).

Figure 9: Thin Singular Value Decomposition. A Thin Singular Value Decomposition (thin
SVD) UrDrV T
r Ur = Ir,
r Vr = Ir, and D ∈ Rr×r is a diagonal matrix containing the singular values d1 ≥ · · · ≥ dr > 0.
V T

r of a matrix A ∈ Rm×n of rank r ≤ n, where Ur ∈ Rm×r, Vr ∈ Rn×r, U T

Let C =

(cid:110)

p ∈ P | ˆp ∈ ˆC

(cid:111)

and let u : C → [0, (cid:80)

p∈P w(p)] such that u(p) = ˆu(ˆp) ·

w(p)

(cid:88)

p∈P

for every p ∈ C. Combining the deﬁnitions of C, u and (15) yields

w(p)ppT =

(cid:88)

p∈P

w(p) ·

(cid:88)

p∈P

(cid:88)

p∈C

ˆu(ˆp)ppT =

u(p)ppT .

(cid:88)

p∈C

Hence, by letting Z =

(cid:16)(cid:112)u(p) · p

(cid:17)T

p∈C

∈ R(d2+1)×d be a matrix whose rows (cid:112)u(p) · pT are

17

weighted (scaled) points of C, we obtain that for every x ∈ Rd
(cid:33)

(cid:32)

floss((P, w), x) = (cid:107)Ax(cid:107)2 = xT AT Ax = xT

(cid:88)

w(p)ppT

x

p∈P

(cid:33)

(16)

u(p)ppT

x = xT Z T Zx = (cid:107)Zx(cid:107)2 = floss((C, u), x).

= xT

(cid:32)

(cid:88)

p∈C

Therefore, (C, u) is an accurate coreset for the query space (P, w, Rd, f, (cid:107)·(cid:107)1) where f (p, x) =
(pT x)2.

Similarly to (13) in Section 3.6, for every j-dimensional subspace of Rd that is spanned
by the orthonormal columns of a matrix S ∈ Rd×j, and its orthogonal complement S⊥, we
have that

dist2((P, w), S) = (cid:13)

(cid:13)AS⊥(cid:13)
2
F =
(cid:13)

d−j
(cid:88)

i=1

(cid:13)
(cid:13)AS⊥

∗i

(cid:13)
2
(cid:13)

=

d−j
(cid:88)

i=1

(cid:13)
(cid:13)ZS⊥

(cid:13)
2
(cid:13)

∗i

= dist2((C, u), S),

where the third derivation holds by (16).

See implementation of function matrix norm2 in (Jubran et al., 2019).

3.7 Least-Mean-Squares Solvers (cid:5) (cid:5) (cid:5)

Least-mean-squares solvers are very common optimization methods in machine learning and
statistics. They are typically used for normalization, spectral clustering, feature selection,
prediction, classiﬁcation, and many more. In this section, we deﬁne and derive a coreset for
such problems, based on the coreset presented in Section 3.6.1.

The corresponding query space (P, w, Rd, f, loss) for least mean squares problems is as
n | bn)(cid:9) ⊆ Rd+1, w : P →
follows. Let (P, w) be a weighted set where P = (cid:8)(aT
[0, ∞), and for every i ∈ [n], ai ∈ Rd and bi ∈ R. Let X = Rd, and for every (aT | b)T ∈ Rd+1
where a ∈ Rd and b ∈ R let f ((aT | b), x) = (aT x − b)2, and deﬁne loss = (cid:107)·(cid:107)1. For simplicity
let wi = w((aT
i

| bi)) for every i ∈ [n]. Therefore,

1 | b1), · · · , (aT

floss((P, w), x) =

n
(cid:88)

i=1

wi(aT

i x − bi)2.

(17)

To obtain an accurate coreset for the above query space (P, w, Rd, f, loss), we shall deﬁne
a new and slightly diﬀerent query space, and use the accurate coreset from Section 3.6.1 as
follows.

Let f2(p, x) = (pT x)2 for every p, x ∈ Rd+1. Let (C, u) be an accurate coreset of size
|C| = (d + 1)2 + 1 for the new query space (P, w, Rd+1, f2, (cid:107).(cid:107)1) as explained in Section 3.6.1,
i , ˆbi)) for every
where C =
i ∈ [|C|]. Then for every x(cid:48) ∈ Rd+1,

⊆ P . For simplicity deﬁne ui = u((ˆaT

1 | ˆb1), · · · , (ˆaT

(cid:111)
|C| | ˆb|C|)

(ˆaT

(cid:110)

n
(cid:88)

i=1

wi((aT
i

| bi)x(cid:48))2 =

|C|
(cid:88)

i=1

ui((ˆaT
i

| ˆbi)x(cid:48))2.

18

Since the last equality holds for every x(cid:48) ∈ Rd+1, in particular, for every x(cid:48) = (xT |

−1)T ∈ Rd+1 where x ∈ Rd, we have that :

floss((P, w), x) =

=

n
(cid:88)

i=1

|C|
(cid:88)

i=1

wi(aT

i x − b)2 =

n
(cid:88)

i=1

wi((aT
i

| bi)x(cid:48))2

(18)

ui((ˆaT
i

| ˆbi)x(cid:48))2 =

|C|
(cid:88)

i=1

ui(ˆaT

i x − ˆb)2 = floss((C, u), x).

(19)

Hence, (C, u) is an accurate coreset for the query space (P, w, Rd, f, loss).

Commonly in the ﬁeld of machine learning, least mean squares optimization problems
wnan)T ∈ Rn×d and

are deﬁned using matrix notations as follows. Let A = (
b = (

wnbn)T ∈ Rn. Least-mean-squares solvers typically aim to minimize

w1a1 | · · · |

w1b1, · · · ,

√

√

√

√

(cid:32) n

(cid:88)

g

i=1

(cid:33)

wi(aT

i x − bi)2

+ h(x) = g (cid:0)(cid:107)Ax − b(cid:107)2

2

(cid:1) + h(x)

(20)

over every x ∈ X ⊆ Rd. Here, h : Rd → [0, ∞) is called a regularization function on the
parameters of x, and it is independent of (P, w), and g : R → R is a real function. See
Table 2 for example objective functions.

Solver name

Linear regression (Bjorck, 1967)

Objective function
(cid:107)Ax − b(cid:107)2
2

Ridge regression (Hoerl & Kennard, 1970)

Lasso regression (Tibshirani, 1996)

Elastic-Net regression (Zou & Hastie, 2005)

1
2n

(cid:107)Ax − b(cid:107)2

2 + α (cid:107)x(cid:107)2

2

1
2n

(cid:107)Ax − b(cid:107)2

2 + α (cid:107)x(cid:107)1

(cid:107)Ax − b(cid:107)2

2 + ρα (cid:107)x(cid:107)2

2 +

(1 − ρ)
2

α (cid:107)x(cid:107)1

g(x)

x

x

x
2n

x
2n

h(x)

0

α (cid:107)x(cid:107)2
2

α (cid:107)x(cid:107)1

ρα (cid:107)x(cid:107)2

2 + (1−ρ)

2 α (cid:107)x(cid:107)1

Table 2: Example solvers that aim to minimize objective functions in the form of (20). Each solver
gets a matrix A ∈ Rn×d, a vector b ∈ Rn and aims to compute x ∈ Rd that minimizes the objective
function. Additional given regularization parameters include α > 0 and ρ ∈ [0, 1].

Hence, as we deﬁned the matrix A and the vector b based on the weighed set (P, w),
we deﬁne the matrix Z and the vector v based on the coreset (C, u) of the query space
(P, w, Rd, f, loss), i.e., Z ∈ Rm×d such that the ith row of Z is
i , and v ∈ Rm such that
v = (

uiˆaT

u|C|

ˆb1, · · · ,

ˆb|C|) and m = (d + 1)2 + 1.
Now as desired, the family of functions from (20) satisfy that,
(cid:32) n

u1

√

√

√

(cid:33)

g((cid:107)Ax − b(cid:107)2

2) + h(x) = g

(cid:88)

wi(aT

i x − bi)2

+ h(x)

(cid:33)

wi(ˆaT

i x − ˆbi)2

+ h(x) = g((cid:107)Zx − v(cid:107)2

2) + h(x).

i=1
(cid:32) m
(cid:88)

i=1

= g

See implementation of function LMS solvers in (Jubran et al., 2019).

19

4 Caratheodory’s Theorem

The Caratheodory Theorem (Carath´eodory, 1907; Cook & Webster, 1972) is a fundamental
result in computational geometry that states that if a point x ∈ Rd lies inside the convex
hull of a set P ⊆ Rd, of |P | = n points i.e., x is a convex combination of the points of P ,
then there is a subset of at most d + 1 points from P that contains x in its convex hull, i.e., x
can be represented as a convex combination of at most d + 1 points from P ; see Theorem 3.
The proof of Caratheodory’s theorem is constructive, and the above subset of d+1 points
can be computed in O(n2d2) time; see Algorithm 1 which implements this constructive proof.
An intuition behind the proof of correctness is shown in Fig. 10. The algorithm takes as
input a weighted set (P, w) such that w : P → [0, 1] and (cid:80)
p∈P w(p) = 1, and computes
in O(n2d2) time a new weighted set (S, u) such that S ⊆ P , |S| ≤ d + 1, u : s → [0, 1],
(cid:80)

s∈S u(s) = 1 and (cid:80)

s∈S u(s)s = (cid:80)

p∈P w(p)p.

Theorem 3 If a point x ∈ Rd is in the convex hull of a set P ⊆ Rd, then x is also in the
convex hull of a set of at most d + 1 points from P .

Figure 10: A weighted set (P, w) whose weighted sum is (cid:80)4
i=1 w(pi)pi = 0 corresponds to four points (in blue) whose
weighted sum is the point x (in orange), which we assume is the origin x = (cid:126)0. Algorithm 1 ﬁrst computes a weights vector
v = (v1, · · · , v4)T such that the weighted sum (cid:80)4
i=1 vi = 0. The
weights are scaled by α > 0 until αvi = w(pi) for some i (i = 1 in the ﬁgure). Every point in the resulting set {αvipi}n
i=1
in green is subtracted from its corresponding point in the input set {w(pi)pi}n
i=1 =
{(w(pi) − αvi)pi}n
i=1, where u(p1) = 0 so p1 can be removed. Algorithm 1 then continues iteratively with the remaining points
until (P, u) has |P | = d + 1 weighted points. The ﬁgure is taken from (Nasser et al., 2015).

i=1 vipi (red points) is the origin, and sum of weights is (cid:80)4

i=1 to obtain the output set {u(pi)pi}n

The proof of correctness for Theorem 3. The proof of correctness for Theorem 3
follows from the correctness of the procedure Caratheodory and vice versa; see Algo-
rithm 1. The procedure Caratheodory takes as input a weighted set (P, w) whose points
are denoted by P = {p1, · · · , pn} and where (cid:80)
p∈P w(p) = 1. We assume n > d+1, otherwise
(S, u) = (P, w) is the desired output. Hence, the n − 1 > d points p2 − p1, p3 − p1, . . . , pn − p1
must be linearly dependent. This implies that there are reals v2, · · · , vn, which are not all
zeros, such that

vi(pi − p1) = 0.

(21)

n
(cid:88)

i=2

20

Algorithm 1: Caratheodory(P, w)

Input : A weighted set (P, w) of n points in Rd, where P = {p1, · · · , pn},
p∈P w(p) = 1.
Output: A weighted set (S, u) computed in O(n2d2) time such that S ⊆ P ,

w : P → [0, 1] and (cid:80)

|S| ≤ d + 1, u : S → [0, 1], (cid:80)

s∈S u(s) = 1 and (cid:80)

s∈S u(s)s = (cid:80)

p∈P w(p)p.

1 if n ≤ d + 1 then
return (P, w)
2
3 for every i ∈ {2, · · · , n} do
4
5 A := (a2 | · · · | an) // A ∈ Rd×(n−1)
6 Compute v = (v2, · · · , vn)T (cid:54)= 0 such that Av = 0.

ai := pi − p1 // pi is the ith point of P .

7 v1 := −

8 α := min

n
(cid:88)

i=2

vi
(cid:26) w(pi)
vi

| i ∈ {1, · · · , n} and vi > 0

(cid:27)

9 u(pi) := (w(pi) − αvi) for every i ∈ {1, · · · , n} such that u(pi) > 0.
10 S := {pi | i ∈ {1, · · · , n} and u(pi) > 0}
11 if |S| > d + 1 then
12

(S, u) := Caratheodory(S, u) // Recursive call that reduces S by at

least 1.

13 return (S, u)

These reals are computed in Line 6 by solving a system of linear equations. This step domi-
nates the running time of the algorithm and takes O(nd2) time using e.g. SVD, where the de-
sired vector of coeﬃcients (v2, · · · , vn)T is simply the right singular vector that corresponds to
the smallest singular value in the SVD of the matrix M = (p2 − p1, · · · , pn − p1)T ∈ R(n−1)×d.

The deﬁnition

in Line 7, guarantees that

and that

n
(cid:88)

i=1

v1 = −

n
(cid:88)

i=2

vi

vj < 0 for some j ∈ [n],

vipi = v1p1 +

n
(cid:88)

vipi

(cid:32)

=

−

=

n
(cid:88)

i=2

i=2

(cid:33)

vi

p1 +

n
(cid:88)

i=2

n
(cid:88)

i=2

vipi

vi(pi − p1) = 0,

21

(22)

(23)

(24)

(25)

Figure 11: Overview of the improved Caratheodory algorithm in (Maalouf et al., 2019). Images left to right: Steps (i) and
(ii): A partition of the input weighted set of n = 48 points (in blue) into k = 8 equal clusters (in circles) whose corresponding
means are µ, . . . , µ8 (in red). The mean of P (and these means) is x (in green). Step (iii): Caratheodory (sub)set of d + 1 = 3
points (bold red) with corresponding weights (in green) is computed only for these k = 8 (cid:28) n means. Step (iv): the Caratheodory
set is replaced by its corresponding original points (dark blue). The remaining points in P (bright blue) are deleted. Step (v):
Previous steps are repeated until only d + 1 = 3 points remains. This takes O(log n) iterations for k = Θ(d). This ﬁgure was
taken from (Maalouf et al., 2019).

where (24) is by (22) and the second equality in (25) is by (21). Hence, for every α ∈ R, the
weighted sum of P is

n
(cid:88)

i=1

w(pi)pi =

n
(cid:88)

i=1

w(pi)pi − α

n
(cid:88)

i=1

vipi =

n
(cid:88)

i=1

(w(pi) − αvi) pi,

(26)

where the ﬁrst equality holds since (cid:80)n
i=1 vipi = 0 by (25). The deﬁnition of α in Line 8
guarantees that αvi∗ = w(pi∗) for some i∗ ∈ [n], and that w(pi) − αvi ≥ 0 for every i ∈ [n].
Hence, the set S that is deﬁned in Line 10 contains at most n − 1 points, its weighted sum
is equal to the weighted sum of (P, w), and its set of weights {w(pi) − αvi} is non-negative.
Notice that if α = 0, we have that u(pk) = w(pk) > 0 for some k ∈ [n]. Otherwise,
by (23), there is j ∈ [n] such that u(pj) = w(pj) − αvj > 0. Hence, |S| (cid:54)= ∅. The sum of the
positive weights is thus equal to the sum of input weights,

n
(cid:88)

pi∈S

u(pi) =

n
(cid:88)

i=1

(w(pi) − αvi) =

n
(cid:88)

i=1

w(pi) − α ·

n
(cid:88)

i=1

vi = 1,

where the last equality holds by (22) and since the sum of the input weights is 1, i.e.,
(cid:80)n
i=1 w(pi) = 1. This and (26) proves the desired properties of (S, u), which is of size n − 1.
In Line 12 we repeat this process recursively until there are at most d + 1 points left in S.
For O(n) iterations the overall time is thus O(n2d2).

4.1 Faster Construction

As explained in Section 4, Algorithm 1 takes as input a weighted set of n points, and aims to
compute a weighted subset of at most d + 1 points which have the same weighted sum and
whose weights sum to one. At each iteration, the algorithm sets at least one of the weights
of the remaining points to zero. Hence, O(n) such iterations are required to compute the
required output, for a total running time of O(n2d2).

However, it was suggested in (Nasser et al., 2015) that instead of taking as input the
entire set of n weighted points at once, it would be more eﬃcient to run Algorithm 1 in a
streaming fashion as follows. Start with a set of d + 1 weighted input points. Then, applying

22

the following process O(n) times: Add 1 new weighted input point to the existing set and
apply Caratheodorys theorem to reduce the d + 2 points to d + 1 points in O(d3) time. The
running time of this algorithm is O(nd3) since it executed n times the above procedure on
only d + 2 points.

Inspired by the streaming fashion of the previous algorithm, a more eﬃcient algorithm

was then proposed in (Maalouf et al., 2019). It suggests to do the following.

(i) Partition the input weighted set into k ∈ O(n/d) subsets P1, · · · , Pk, each of size at

most O(d) (speciﬁcally 2d).

(ii) Compute the weighted sum µi of each chunk Pi.
(iii) Apply the Caratheodory theorem only to the set µ = {µ1, · · · , µk} of k weighted

sums to obtain a weighted subset ˆµ of |ˆµ| = d + 1 elements from µ.

(iv) Delete every chunk Pj whose weighted mean µj was not chosen by the Caratheodory

theorem (i.e., µj (cid:54)∈ ˆµ).

(v) Continue recursively until only d+1 input points remain. This algorithm is illustrated

in Fig. 11. The running time of this algorithm is O(nd + d4 log n); see Theorem 4.

The following theorem is a restatement of Theorem 3.1 in (Maalouf et al., 2019) for

k = 2d.

Theorem 4 (Theorem 3.1 in (Maalouf et al., 2019)) Let (P, w) be a weighted set of n
points in Rd such that w : P → [0, 1] and (cid:80)
p∈P w(p) = 1. Then a weighted set (S, u)
s∈S u(s) = 1 and (cid:80)
that satisﬁes that S ⊆ P , |S| ≤ d + 1, u : S → [0, 1], (cid:80)
s∈S u(s)s =
(cid:80)
p∈P w(p)p can be computed in time O (nd + d4 log n).

References

Bjorck, A. (1967). Solving linear least squares problems by gram-schmidt orthogonalization.

BIT Numerical Mathematics, 7 (1), 1–21.

Carath´eodory, C. (1907). ¨Uber den variabilit¨atsbereich der koeﬃzienten von potenzreihen,
die gegebene werte nicht annehmen. Mathematische Annalen, 64 (1), 95–115.

Cook, W., & Webster, R. (1972). Caratheodory’s theorem. Canadian Mathematical Bulletin,

15 (2), 293–293.

Feldman, D., Schmidt, M., & Sohler, C. (2013). Turning big data into tiny data: Constant-
size coresets for k-means, pca and projective clustering. In Proceedings of the twenty-
fourth annual acm-siam symposium on discrete algorithms (pp. 1434–1453).

Golub, G., & Van Loan, C.

(1996). Matrix computations 3rd edition the john hopkins

university press. Baltimore, MD.

Hoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthog-

onal problems. Technometrics, 12 (1), 55–67.

Inaba, M., Katoh, N., & Imai, H.

(1994). Applications of weighted voronoi diagrams
and randomization to variance-based k-clustering. In Proceedings of the tenth annual
symposium on computational geometry (pp. 332–339).

Jubran, I., Maalouf, A., & Feldman, D. (2019). Open source code for all the algorithms

presented in this paper. (Link for open-source code.)

Klema, V., & Laub, A. (1980). The singular value decomposition: Its computation and

some applications. IEEE Transactions on automatic control , 25 (2), 164–176.

23

Maalouf, A., Jubran, I., & Feldman, D. (2019). Fast and accurate least-mean-squares solvers.

arXiv preprint arXiv:1906.04705 .

Nasser, S., Jubran, I., & Feldman, D. (2015). Coresets for kinematic data: From theorems

to real-time systems. arXiv preprint arXiv:1511.09120 .

Paul, R., Feldman, D., Rus, D., & Newman, P.

(2014). Visual precis generation using
coresets. In 2014 ieee international conference on robotics and automation (icra) (pp.
1304–1311).

Rosman, G., Volkov, M., Feldman, D., Fisher III, J. W., & Rus, D. (2014). Coresets for k-
segmentation of streaming data. In Advances in neural information processing systems
(pp. 559–567).

Schmidt, E. (1908). ¨Uber die auﬂ¨osung linearer gleichungen mit unendlich vielen unbekan-
nten. Rendiconti del Circolo Matematico di Palermo (1884-1940), 25 (1), 53–77.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological), 58 (1), 267–288.

Trefethen, L. N., & Bau III, D. (1997). Numerical linear algebra (Vol. 50). Siam.
Wikipedia contributors. (2019). Singular value decomposition. https://en.wikipedia.org/

wiki/Singular value decomposition.

Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal
of the royal statistical society: series B (statistical methodology), 67 (2), 301–320.

A The Basics of Linear Algebra

In this section, we give a brief overview of tools and deﬁnitions from linear algebra which
are used along the paper.

Recall that the d × d identity matrix is denoted by Id. An orthogonal matrix is a square
matrix M ∈ Rd×d whose columns and rows are orthogonal unit vectors, i.e. M T M =
M M T = Id. The rank of a matrix A ∈ Rm×n is the dimension of the its columns space, i.e.,
the vector space that is spanned by its columns, which is identical to the dimension of its
rows space. This is also the maximal number of linearly independent columns of A. The
matrix A is said to have full rank if its rank equals the smaller value between m and n.

The GramSchmidt process or QR-decomposition (Schmidt, 1908; Golub & Van Loan,
1996) is a method for orthonormalising a set of vectors in an inner product space (for example
the Euclidean space Rd equipped with the standard inner product). The GramSchmidt
process takes a ﬁnite, linearly independent set S = {a1, · · · , ak} of vectors for k ≤ d and
generates an orthogonal set S = {u1, · · · , uk} that spans the same k-dimensional subspace of
Rd as S. The application of the GramSchmidt process to the column vectors of a full column
rank matrix A ∈ Rm×n with m ≥ n yields the QR decomposition A = QR (Trefethen &
Bau III, 1997) where Q ∈ Rm×m is an orthogonal matrix and R ∈ Rm×n is a right triangular
matrix; see Fig. 7.

A Singular Value Decomposition (SVD) of a matrix (Klema & Laub, 1980) A ∈ Rm×n is
a factorization A = U DV T such that U ∈ Rm×m and V ∈ Rn×n are orthogonal matrices and
D ∈ Rm×n is a diagonal matrix whose diagonal entries are called singular values of A and
are non-negative and non-increasing; see Fig. 8.

A Thin Singular Value Decomposition (thin SVD) of a matrix A ∈ Rm×n of rank r ≤ n

24

is a factorization A = UrDrV T
r Vr = Ir, and
D ∈ Rr×r is a diagonal matrix containing the singular values d1 ≥ · · · ≥ dr > 0; see Fig. 9.

r such that Ur ∈ Rm×r, Vr ∈ Rn×r, U T

r Ur = Ir, V T

Every matrix A ∈ Rm×n has a QR, SVD, and thin SVD decompositions.
Let L ⊆ Rd be a j-dimensional linear subspace and let S ∈ Rd×j be a matrix whose
columns are mutually orthogonal and span L. Let S⊥ ∈ Rd×(d−j) be a matrix whose columns
are mutually orthogonal and span the orthogonal complement L⊥ of L. By the Pythagorean
theorem, the squared Euclidean distance between a point p ∈ Rd and L can be computed by
2 = (cid:13)
(cid:13)pT S⊥(cid:13)
taking the norm of its projection onto A⊥, namely dist2(p, L) = (cid:107)p(cid:107)2 − (cid:13)
(cid:13)pT S(cid:13)
2.
(cid:13)
(cid:13)
The sum of squared distances from the rows of a matrix A ∈ Rn×d to L is thus (cid:13)
(cid:13)AS⊥(cid:13)
2
F .
(cid:13)

A natural application for SVD is to compute the unit vector x ∈ Rn that minimizes
(cid:107)Ax(cid:107)2
2 (Klema & Laub, 1980) A ∈ Rm×n is a given matrix. The desired vector of coeﬃcients
x is simply the vector in the matrix V that corresponds to the smallest singular value in the
SVD of the matrix A. Minimizing the over determined system (cid:107)Ax − b(cid:107)2
2 given an additional
non-zero vector b ∈ Rm can be done as follows. Let U DV T be the SVD of A. We now have
that Ax = U DV T x = b. Multiplying both sides by U T yields DV T = U T b. Multiplying
again by the pseudo inverse D† of D yields V T x = D†(U T b). Hence, by multiplying both
sides by V we get that x = V (D†(U T b)) is the desired vector of coeﬃcients.

If the matrix A contains at least n independent vectors in its rows, then the rows of A is
a basis for Rn. This implies that every vector v ∈ Rn is a linear combination of only n such
vectors. Section 3.7 suggests such a ”coreset” with only positive coeﬃcients in the linear
combination. Computing these coeﬃcients can be done by solving the linear system Bx = v
where the columns of B are the vectors that span v.

25

