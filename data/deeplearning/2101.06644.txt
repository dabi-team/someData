HySTER: A Hybrid Spatio-Temporal Event Reasoner

Theophile Sautory, Nuri Cingillioglu, Alessandra Russo
Imperial College London, Department of Computing
London SW7 2BU, United Kingdom
{theophile.sautory15, nuric, a.russo}@imperial.ac.uk

1
2
0
2

n
a
J

7
1

]

V
C
.
s
c
[

1
v
4
4
6
6
0
.
1
0
1
2
:
v
i
X
r
a

Abstract

The task of Video Question Answering (VideoQA) consists
in answering natural language questions about a video and
serves as a proxy to evaluate the performance of a model in
scene sequence understanding. Most methods designed for
VideoQA up-to-date are end-to-end deep learning architec-
tures which struggle at complex temporal and causal reason-
ing and provide limited transparency in reasoning steps. We
present the HySTER: a Hybrid Spatio-Temporal Event Rea-
soner to reason over physical events in videos. Our model
leverages the strength of deep learning methods to extract
information from video frames with the reasoning capabil-
ities and explainability of symbolic artiﬁcial intelligence in
an answer set programming framework. We deﬁne a method
based on general temporal, causal and physics rules which
can be transferred across tasks. We apply our model to the
CLEVRER dataset and demonstrate state-of-the-art results in
question answering accuracy. This work sets the foundations
for the incorporation of inductive logic programming in the
ﬁeld of VideoQA.

Introduction
Visual reasoning over videos is a task relevant for au-
tonomous agents performing in real world applications such
as autonomous vehicles or dynamic robot manipulation. The
task of Video Question Answering (VideoQA) consists of
the following: given a video alongside a question in natural
language a model must return an answer in natural language
(Yu et al. 2019). Due to the joint reasoning on vision, lan-
guage, and spatio-temporal properties, VideoQA serves to
evaluate visual reasoning over videos.

In recent years, two main approaches to solve VideoQA
and Visual Question Answering (VQA, the equivalent on
single images) tasks have been explored. (Mun et al. 2016;
Xu et al. 2017) rely on attention mechanisms and fully end-
to-end differentiable systems to extract the relevant infor-
mation from the image frames to answer questions. Their
reasoning steps and failure mechanisms remain opaque to
humans whilst requiring large amounts of data to train.

(Hu et al. 2017; Hudson and Manning 2018; Yi et al.
2018; 2019) employ neuro-symbolic methods which per-

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

form compositional reasoning by breaking down the an-
swering task into applying a series of symbolic ﬁlters to
extract the answer. Neural network ﬁlters (Hu et al. 2017;
Hudson and Manning 2018) allow the interpretation of re-
spective attention feature maps for each reasoning step, but
do not explain the decision making process of the model and
still consume thousands of samples to train. Heuristic ﬁlters
with functional programs have been used (Yi et al. 2018;
2019) and elucidate the decision process of the model. How-
ever, they do not beneﬁt from the full representational power
of symbolic reasoning methods such as answer set program-
ming (ASP) and would have difﬁculties generalising to new
symbolic domains.

Motivated by the potential of neuro-symbolic methods to
simulate human intelligence and the application of Kahne-
man’s human’s thinking systems: “Thinking, Fast and Slow”
(Kahneman 2011) in artiﬁcial intelligence (Booch et al.
2020), we propose a neuro-symbolic model which disentan-
gles the visual reasoning task into neural-based perception
and symbolic reasoning. We build a perception module us-
ing neural systems, similarly to System 1 (fast), which op-
erates quickly and automatically with no effort. We employ
symbolic artiﬁcial intelligence to model System 2 (slow),
which focuses on cognitive abilities based on reasoning and
planning.

We tackle the VideoQA CLEVRER task (Yi et al. 2019)
which focuses on temporal and causal reasoning over phys-
ical events. The contributions of our work stand in our
threefold general neuro-symbolic reasoning framework for
VideoQA tasks: 1) we propose a symbolic scene repre-
sentation method to combine the output of deep learning
object detectors with symbolic representation, 2) we com-
bine domain independent temporal, causal and physics back-
ground knowledge rules for reasoning on physical events
in videos, 3) we present the application of our method on
the CLEVRER dataset (Yi et al. 2019) including engineered
event detection rules, outperforming state-of-the-art models
on question answering accuracy.

Background
Answer Set Programming. ASP is a declarative program-
ming paradigm to solve search problems by computing an-

 
 
 
 
 
 
Table 1: Basic Predicates of the Event Calculus framework

Table 2: Event Calculus Axioms in the ASP framework

Predicates
initiates(E, F )
terminates(E, F )
happens(E, T )
holdsAt(F, T )
initially(F )
clipped(F, T )

Meaning
event E causes ﬂuent F to start holding
event E causes ﬂuent F to cease holding
event E occurs at time T
ﬂuent F holds at time T
ﬂuent F holds from T = 0
ﬂuent F is terminated at time T

swer sets of logic programs (Lifschitz 2008). We limit our-
selves to answer set programs composed of facts and normal
rules of the form:

h ← b1, ..., bn, not c1, ..., not cm

where h, b1, ..., bn, c1, ..., cm are all logical atoms. h is de-
scribed as the head of the rule, whereas the conjunction of
literals (atoms and negated atoms) on the right hand side
form the body of the rule. If all the literals in the body of
a rule hold in the answer set, then the head must hold as
well. A rule without a body is known as a fact. The negation
“not” describes the negation-as-failure, where “not ci” holds
when all attempts to prove ci fail. Predicates will be referred
to with lowercase letters, whilst variables will be in upper-
case. For instance we set the number predicate to hold for
all integers, and use the variable E to deﬁne events.
The Event Calculus. The Event Calculus framework pro-
vides a logic-based formalism which inherently depicts
causal relations from an event with its effects (Shanahan
1999). It has successfully been used for reasoning on and
detecting events (Law, Russo, and Broda 2018; Katzouris,
Artikis, and Paliouras 2016).

The ontology of the Event Calculus consists of: 1) actions,
temporal entities which happen over a time point, 2) ﬂu-
ents, quantities whose value is subject to change over time,
and, 3) time points, numerical time references. Basic predi-
cates of the Event Calculus which allow to reason over time,
presented in Table 1 are: initiates, terminates, happens,
holdsAt, initially, and clipped (Shanahan 1999).

We provide in Table 2 the Event Calculus axioms used in
our framework, which allow to perform temporal reasoning.
Axiom 1 implies that a ﬂuent F is true as time T = 0. Axiom
2 states that a ﬂuent F is clipped at time T when an event
E that terminates it occurs at time T . In conjunction to this
rule, Axiom 3 afﬁrms that a ﬂuent F is true at time T + 1
if an event E happens which causes the ﬂuent F to become
true whilst no event which terminates it happens. Finally,
Axiom 4 represents the law of inertia and the permanence of
ﬂuents: if a ﬂuent F holds at time T and no event E happens
which terminates it, then it still holds at time T + 1.

Related Work
Our work resides at the intersection of neuro-symbolic
VideoQA and logic-based temporal reasoning on events.
Neuro-symbolic VideoQA. The CLEVRER dataset (Yi et
al. 2019) serves as a reference dataset to evaluate neuro-
symbolic models on complex reasoning tasks. The Neuro-
Symbolic Dynamic Reasoner developed alongside the paper
introduces a solution based on four pillars: 1) Video Parser,

holdsAt(F, 0) ← initially(F ).
(1)
clipped(F, T ) ← happens(E, T ), terminates(E, F ),
(2)

time(T ).

holdsAt(F, T + 1) ← happens(E, T ), initiates(E, F ),

holdsAt(F, T + 1) ← holdsAt(F, T ), not clipped(F, T ),

not clipped(F, T ), time(T ).

time(T ).

(3)

(4)

2) Dynamics Predictor, 3) Question Parser, 4) Program Ex-
ecutor. The authors had proposed a similar methodology for
a neuro-symbolic VQA (Yi et al. 2018). This method allows
for interpretable reasoning steps but does not set the ground
for symbolic-rule learning systems which could enable to re-
move the human engineering from their heuristic ﬁlters and
functional programs.
Logic program in VQA. First-order-logic (FOL) and ASP
have been used in VQA (Amizadeh et al. 2020; Basu,
Shakerin, and Gupta 2020; Aditya, Yang, and Baral 2018;
Gokhale et al. 2020). A fully differentiable FOL formalism
is presented in (Amizadeh et al. 2020) where the ﬁlter se-
quences are translated into FOL. Doing so generalizes the
ﬁlters to any domain speciﬁc language representable by FOL
but does not provide insight on the decision steps taken by
the model to answer the questions, which remain under the
form of neural networks. The use of ASP is presented in
(Basu, Shakerin, and Gupta 2020) to solve the VQA task
after having translated the questions into logic queries and
reconstructed the scene as a logic program. This method al-
lows for explainable reasoning and building a proof tree to
justify the answer provided to the logic query. These meth-
ods have not been applied to VideoQA nor temporal and
causal reasoning yet.
Logic-based Event Detection. Recently, hybrid methods
have emerged to perform reasoning on events from videos.
(Prapas et al. 2018) extract features from the video using a
3D CNN for an SVM classiﬁer to detect simple events. For
more complex event they apply the Event Calculus on-top of
simple events detected. By using an SVM classiﬁer to recog-
nise events, the authors base their recognition on features
which are not interpretable. In contrast, (Khan et al. 2019)
employ a functional program to detect simple events on sym-
bols before applying the Event Calculus to detect more com-
plex events. These methods do not build their event detection
directly from the symbolic scene representation and general
background knowledge, which hinders their generalisation
across tasks.

HySTER
In this section, we present our Hybrid Spatio-Temporal
Event Reasoner (HySTER) whose implementation for the
CLEVRER dataset (Yi et al. 2019) is shown in Figure 1.
Our model functions by combining three main components,
1) Video Parser, 2) Question Parser, 3) Symbolic Represen-
tation and Reasoning.

Figure 1: Overview of the HySTER architecture. The video is fed to the video parser through the Mask R-CNN which segments
the objects on the scene and extracts their classes. The frame is then fed alongside one of the detected object’s binary mask into
the ResNet18 to predict the 3D coordinate positions of the object in the scene. The information is then grounded in symbols to
construct the symbolic scene representation. The natural language question is translated into a logic query and then combined
with the background knowledge, the symbolic scene representation and the rules for event deﬁnition into an ASP program for
the clingo ASP solver to reason over, providing an answer to the question. The yellow shaded parts refer to general components
transferable between tasks. The green shaded parts refer to task speciﬁc symbolic components and information.

Video Parser
We deﬁne a video parser as a neural object detector which
classiﬁes objects and detects their positions in the scene, ei-
ther in 2D pixel or 3D space position. The detector must
provide the spatial information for each of the objects in the
scene at each time frame of the video.

Question Parser
We deﬁne the question parser as a component which in-
takes a natural language question and translates it into a
logic query based on the dataset vocabulary. The parser
can be constructed with neural (Amizadeh et al. 2020;
Singh, Aggrawal, and Krishnamurthy 2020) or symbolic
(Basu, Shakerin, and Gupta 2020) natural language process-
ing tools.

Symbolic Representation and Reasoning
We then construct the reasoning module pipeline which ties
the outputs of the video parser and the question parser to
provide an answer to the questions based on spatio-temporal
properties of the video.

Symbolic Scene Reconstruction We extract general
spatio-temporal properties from any video parser through
three domain independent predicates: 1) object as an ob-
ject identiﬁer, 2) position, providing spatial properties of
the object at a given time, and 3) on camera encapsu-
lating the temporal information of an object being in the

scene at a given time frame. We assume the existence of
predicates p1, ..., pn that deﬁne task speciﬁc information ex-
tracted from the video parser such as an object’s class.

Background Knowledge We equip the model with do-
main agnostic background knowledge on physical deﬁni-
tions under the form of logic rules, built on top of the sym-
bolic scene representation and presented in Table 3. The
next time and number predicates hold for successive time
frames and integers respectively. These rules deﬁne general
physical concepts such as the displacement of an object V 1
in the scene (Rule (5)), the euclidean distance between two
objects V 1 and V 2 (Rule (8)), or the linear velocity change
of an object between two time frames (Rule (9)). We can
apply such rules to any task which requires reasoning over
the dynamics of detected objects in a scene in an explainable
manner.

Event Detection and Reasoning We employ the general
physics rules from the background knowledge to construct
task speciﬁc event detection rules. These rules enable the
user to speciﬁcally update each event detection, which pro-
vides a direct framework for improving speciﬁc abilities of
the model. This contrasts with fully retraining end-to-end
neural networks and allows to target the efforts where the
model fails whilst reducing the amount of data used. For in-
stance, by detecting that the model fails in all the questions
involving a given type of event E, we can update the rule
used to deﬁne the event to improve its detection.

Table 3: Physics Background Knowledge Rules

displacement(D, V 1, T 1, T 2) ← position(V 1, X1, Y 1, Z1, T 1), position(V 1, X2, Y 2, Z2, T 2),

D = (X1 − X2)2 + (Y 1 − Y 2)2 + (Z1 − Z2)2, next time(T 1, T 2).

disp greater(D1, V 1, T 1, T 2) ← displacement(D2, V 1, T 1, T 2), D2 > D1, number(D1).
disp smaller(D1, V 1, T 1, T 2) ← displacement(D2, V 1, T 1, T 2), D2 <= D1, number(D1).

euc distance(D, V 1, V 2, T ) ← position(V 1, X1, Y 1, Z1, T ), position(V 2, X2, Y 2, Z2, T ),

velocity change(D, V 1, T 2) ← displacement(D1, V 1, T 1, T 2), displacement(D2, V 1, T 2, T 3),
D >= |D1 − D2|, number(D).

(X1 − X2)2 + (Y 1 − Y 2)2 + (Z1 − Z2)2 <= D, number(D), V 1 ! = V 2.

(5)
(6)
(7)

(8)

(9)

Temporal reasoning results from the Event Calculus
framework. The symbolic representation presents the advan-
tage of dealing with the time component T directly, which
allows reasoning over time with arithmetic inequalities.

We deﬁne general rules for causal reasoning on physical
events over videos in Table 4, where the events of interest
must affect the state - position, velocity, acceleration - of
each object it involves. Rule (10) ensures that an object re-
lated to an event is responsible for that event. Rule (11) en-
sures that an event causes a subsequent event if they are re-
lated. Finally, the recursive Rule (12) ensures that indirect
causes are captured.

Experiments
We present the application of the HySTER framework to
the CLEVRER dataset (Yi et al. 2019). This VideoQA
task explores temporal and causal reasoning on physical
events - collision, entry, exit - taking place in synthetic
videos through four question types: descriptive, explana-
tory, predictive and counterfactual. Motivated by temporal
and causal reasoning rather than predicting the dynamics of
the scene, we focus on the descriptive and explanatory ques-
tions. We then explore how we can apply our framework to
the predictive and counterfactual questions.

Video Parser
We process each image frame in two steps: 1) object de-
tection, 2) 3D position mapping. (Yi et al. 2019) released
the output of their Mask R-CNN (He et al. 2017) using
a ResNet-50 FPN (Lin et al. 2016; He et al. 2015) as the
backbone for object detection and scene de-rendering (Wu,
Tenenbaum, and Kohli 2017), which allows research to fo-
cus on the reasoning task. We use these outputs to obtain the
three attributes of each detected object (color, shape, ma-
terial), its segmentation mask, and a detection conﬁdence
score.

Similarly to (Yi et al. 2018), we train a 3D mapping net-
work on top of the object detector to provide the 3D coor-
dinates of the objects in the scene. We assume that the 3D
coordinates will provide depth information necessary to de-
scribe events in the videos using symbolic rules. We select a
ResNet18 model (He et al. 2015) with a 4 channel input to
the network: 3 RGB for the full image, 1 Binary map for the
segmentation mask. We hypothesize that the full image will

provide the context required to predict the 3D position from
a 2D image. We train the model using 4 random frames of
3000 videos respectively, for 5 epochs, using a learning rate
of 1 × 10−4 and a batch size of 8, on an NVIDIA Tesla K80
GPU. We use the mean squared error loss function applied
on a vector y ∈ R3 to learn each of the Cartesian coordi-
nates.

Question Parser

We translate natural language queries into logic queries to
be answered by the ASP solver applied over the symbolic
reconstruction of the scene, its reasoning and event de-
tection. We build an Extended Backus-Naur form context
free grammar to build sample grammar of questions. The
∼150,000 descriptive questions are mapped into six differ-
ent sentences, whilst the three other question types provide
one sentence grammar each. The deﬁned grammars are then
mapped into logic queries through a functional program. An
example of such a query is shown in Figure 1 which reveals
how the model extracts the answer from the symbolic repre-
sentation through the color variable C. The answer(Q, C)
predicate, where Q represents the question number, holds
if the body of the rule holds, extracting the queried color
variable from the symbolic scene reconstruction. The ques-
tions in the CLEVRER dataset expect one word answers:
yes or no for existence questions, a characteristic attribute
for open-ended questions, an integer for counting questions.
We use the clingo ASP solver (Gebser et al. 2014) and logic
queries to extract the attributes, the yes or no answer, or to
count speciﬁc predicates.

Symbolic Representation and Reasoning

Symbolic Scene Reconstruction Given the detected ob-
jects intrinsic characteristics and position obtained from
the video parser, we build a set of ASP facts describ-
ing the scene. We recover the general predicates object,
on camera, position and additional predicates for the
shape, color and material of each detected object in the
scene. These predicates implicitly provide additional com-
mon sense knowledge to the model, such as the fact that cyan
and red are colors or that rubber and metal are materials and
allow to apply symbolic reasoning over the constituents in
the scene.

Table 4: HySTER Causal Reasoning Rules

cause(object(V 1), event(E1)) ← related(object(V 1), event(E1)).
cause(event(E1), event(E2)) ← happens(event(E1), T 1), happens(event(E2), T 2),

related(event(E1), event(E2)), T 1 < T 2.

cause(V 1, V 2) ← cause(V 1, V 3), cause(V 3, V 2).

(10)

(11)
(12)

Table 5: HySTER Events and their Effects for
CLEVRER dataset (Yi et al. 2019)

the

initiates(collision(V 1, V 2), collided(V 1, V 2))

← object(V 1), object(V 2).

initiates(entry(V ), present(V )) ← object(V ).
initiates(entry(V ), moving(V )) ← object(V ).
initiates(move(V ), moving(V )) ← object(V ).
terminates(stop(V ), moving(V )) ← object(V ).
terminates(exit(V ), moving(V ) ← object(V ).
terminates(exit(V ), present(V )) ← object(V ).

Temporal Reasoning We deﬁne unary predicates for the
move, stop, entry, and exit events, and a binary predi-
cate for the collision event, which involves two objects. We
specify the effects of detected events on the ﬂuents to an-
swer the questions from the CLEVRER task according to
the rules presented in Table 5 and the Event Calculus frame-
work. These rules allow to reason on the dynamics of the
scene and answer questions such as “How many objects are
moving when the video ends?”.

Causal Reasoning The dataset requires to reason on the
responsibilities of objects and events over the exit and
collision events. We employ the rules deﬁned in Table 4
by letting the related predicate refer to an object taking part
in the event. The objects in the CLEVRER dataset are all of
similar dimensions, none of them are rooted to the ground,
and no external forces other than friction apply, hence we
assume that all collisions affect the dynamics of the two ob-
jects involved and respect the conditions for our rules. As
such, Rule (11) ensures that a collision is responsible for
a subsequent collision if one of the objects participated in
both. Similar reasoning can be applied over Rules (10-12)
and responsibilities for the exit event.

Event Detection We describe events occurring in the
video with rules of a minimum number of predicates, which
provide transparent model reasoning and can be changed to
allow for model update. We present them in Table 6.

HySTER-0. We assume an absence of noise in the sym-
bolic scene reconstruction which allows us to deﬁne the
entry and exit events occurrences as Rule (13) and Rule
(14) respectively, describing the appearance and disappear-
ance of objects in the scene.

We build the rules for move and stop from the back-
ground knowledge comparative displacement Rules(6, 7) re-

spectively. We assume that we can provide threshold dis-
placement values indicating if an object is moving or sta-
tionary. Being in movement depends on the context and the
inertia of an object, hence, we propose separate thresholds
for each rules.

We deﬁne collisions in Rule (17) by a threshold distance
in conjunction with a threshold change in velocity for each
involved object. The euclidean distance threshold forces the
objects to be physically close, which is necessary for them
to collide. We then assume a collision will lead to mo-
mentary speed changes in the two objects due to the reac-
tion force imposed by the collision upon contact (Newton’s
third law of motion). In the absence of external forces and
by conservation of momentum, one object would accelerate
whilst the other would decelerate. Collisions between ob-
jects pairs are unique in a video, hence the presence of the
not holdsAt(collided(V 1, V 2), T ) predicate.

HySTER-1. We remove our assumption of a noiseless
perception module and update the HySTER-0 entry detec-
tion Rule(13) to Rule (18), which ensures that an object must
be present in the scene for two consecutive frames to enter
the scene. This removes noisy object detection where an ob-
ject is wrongly classiﬁed over one time-step and relaxes the
importance of a perfect perception module.

HySTER-2. On top of HySTER-1, we update the move
event detection rule from Rule (15) to Rule (19), requiring
an object’s displacement to be above a threshold value D for
two consecutive time frames. This ensures that the object’s
displacement is not simply due to noise in the 3D position
prediction.

HySTER-2 (2D). By deﬁning a position predicate with
X and Y positions only, the model can be applied on 2D
scene reconstructions from the Mask R-CNN output, with-
out additional ground truth data to train our video parser.

We perform hyper-parameter grid searches over 500 train-
ing videos to obtain the threshold values for the rules which
provide the highest question answering accuracy, making
our model extremely data efﬁcient. By updating such spe-
ciﬁc rules, we are able to target our model’s weak compo-
nents and improve its overall performance in the task.

Results
We compare our results with the state-of-the-art NS-DR (Yi
et al. 2019) solution on question answering accuracy. The
NS-DR outperforms other existing baseline methods such
as implementations of MAC (Hudson and Manning 2018)
or Tbd-Net (Mascharka et al. 2018) on the task.

In our HySTER (3D) we use the 3D position informa-
tion during training whereas the authors of the NS-DR do

Table 6: Event Detection Rules for the CLEVRER dataset

First Event Detection Rules

happens(entry(V 1), T 1) ← not on camera(V 1, T 1), on camera(V 1, T 2), next time(T 1, T 2).
happens(exit(V 1), T 1) ← on camera(V 1, T 1), not on camera(V 1, T 2), next time(T 1, T 2).

happens(move(V 1), T 1) ← disp greater(D, V 1, T 1, T 2), next time(T 1, T 2), not holdsAt(moving(V 1), T 1).

happens(stop(V 1), T 1) ← disp smaller(D, V 1, T 1, T 2), next time(T 1, T 2), holdsAt(moving(V 1), T 1).

happens(collision(V 1, V 2), T ) ← euc distance(D1, V 1, V 2, T ), velocity change(D2, V 1, T ),

velocity change(D3, V 2, T ), not holsdsAt(collided(V 1, V 2), T ).

Updated Rules

happens(entry(V 1), T 1) ← not on camera(V 1, T 1), on camera(V 1, T 2), on camera(V 1, T 3),
next time(T 1, T 2), next time(T 2, T 3).

happens(move(V 1), T 1) ← disp greater(D, V 1, T 1, T 2), disp greater(D, V 1, T 2, T 3),

next time(T 1, T 2), next time(T 2, T 3), not holdsAt(moving(V 1), T 1).

(13)
(14)
(15)
(16)

(17)

(18)

(19)

Table 7: Question-answering accuracy on the descriptive and
explanatory questions of the CLEVRER’s test set.

Table 9: Question-answering accuracy on predictive and
counterfactual questions of the CLEVRER’s test set.

Methods

Descriptive

NS-DR
NS-DR (NE)

HySTER-2 (2D)

HySTER-2 (3D)

88.1
85.8

88.3

89.6

Explanatory

per opt.

per ques.

87.6
85.9

90.9

95.9

79.6
74.3

83.0

92.0

Table 8: Improvement pipeline accuracy on descriptive ques-
tions of the CLEVRER’s test set. 0 corresponds to the HyS-
TER with the ﬁrst event detection rules. 1 corresponds to the
updated event detection rule for entry, whilst 2 corresponds
to the further update of the move event detection rule.

Methods - HySTER (3D)

0

1

2

Descriptive

86.2

88.8

89.6

not take advantage of this information. Hence our results are
not directly comparable with theirs in this setting. However,
the results prove the performance of our method, where our
HySTER-2 (3D) outperforms the NS-DR by approximately
1.5, 8, and 12% in accuracy for the descriptive, explana-
tory per option and per question questions respectively. We
present the results in Table 7. Even though the results from
our HySTER-2 (2D) on these questions reveal that a compo-
nent of the advantage comes from the use of the 3D coordi-
nates which help in detecting collisions, these results high-
light the potential of using logic rules in an ASP framework
to model the reasoning task.

The 2D setting allows us to directly compare the perfor-
mance of our HySTER with the one from the NS-DR (Yi
et al. 2019) for descriptive and explanatory questions. Com-
paring the questions accuracy allows us to disentangle the

Methods

Predictive

Counterfactual

per opt.

per ques.

per opt.

per ques.

NS-DR
NS-DR (NE)
HySTER-2 (2D)

82.9
75.4
79.5

68.7
54.1
61.5

74.1
76.1
79.4

42.2
42.0
47.1

comparison of the perception and reasoning modules, as the
perception is identical. We outperform both the NS-DR and
the NS-DR (NE) - which stands for “no-events” when train-
ing their propagation network and detecting collision from
a heuristic rule - on these questions, as shown in Table 7.
These results reveal that our symbolic rule for detecting col-
lision events surpasses both the propagation network col-
lision detection ability whilst being explainable and their
heuristic ﬁlter for collision detection (explanatory questions
heavily rely on collision detection). This underlines the idea
that apparent complex reasoning and event detection tasks
can be efﬁciently modelled by a low number of symbolic
rules and supports the implementation of ASP for the sym-
bolic component of the model.

Table 8 shows that removing noise in the entry and move
event predicates allows to improve the results on descriptive
question accuracy. This highlights the possibility of chang-
ing one component in the model and directly seeing its im-
pact on the overall performance. The compositional nature
of our logic system inherently holds a solution to speciﬁ-
cally notice and update failing modules.

Finally, we compare the implementation of our HySTER
symbolic scene reconstruction and reasoning on top of the
propagation network output from (Yi et al. 2019) for predic-
tive and counterfactual questions with the NS-DR methods.
The fact that we outperform the NS-DR (NE) in all question
types suggests that our symbolic representation models the
scene and events more efﬁciently. The greater performance

Figure 2: Absolute distances in the x and y directions of col-
liding objects as red dots, and non-colliding objects as blue
dots, for training videos 0-200 in the 3D setting. The optimal
threshold on the euclidean distance between two objects in
the velocity change rule is plotted in black.

Figure 3: Velocity change distributions of colliding objects:
2 frames before collision in blue, 1 frame before collision in
orange, at collision in green, 1 frame after collision in red,
for training videos 0-5000 in the 3D setting. The black ver-
tical represents the thresholds from Rule (17) (equal here).

of the NS-DR on predictive questions could be due to col-
lisions occurring in the last frame of the videos, which are
not detectable via our method due to the velocity change
predicate which compares the velocity at two time points.

Analysis

The performance of our model relies on three pillars: 1) the
accuracy of the perception module, 2) the reasoning ability
of the symbolic framework, 3) the event detection capability.
The accuracy of the output of the Mask R-CNN from (Yi
et al. 2019) is crucial for efﬁcient reconstruction of the scene
into symbols. The model shows an F1 score of 0.991 (over
1000 training videos) in object detection and classiﬁcation
which ensures such an ability.

Symbolic AI inherently presents a framework to handle
compositional, temporal and causal reasoning via symbols,
the Event Calculus and causal rules. This empowers our
model with human-like reasoning abilities which allows to
solve some of the complex reasoning tasks from the dataset.
Moreover, the logic component allows to compose with
negation or counting questions, which remain difﬁcult tasks
for fully deep learning algorithms (Gokhale et al. 2020) and
(Zhang, Hare, and Pr¨ugel-Bennett 2018) respectively.

To understand the HySTER event detection ability we plot
the collision detection thresholds over videos 0-200 from the
training dataset. Figure 2 shows the ground-truth distances
in x and y direction between two colliding objects as red
dots, and between two non colliding objects as blue dots.
We plot the euclidean distance threshold from the HySTER-
2 (3D) Rule (17) which is represented as a circular arc: all
points on that arc are at distance D1 form the origin, D1 be-
ing the square root of our computed threshold. The thresh-
old reveals that a large proportion of non-colliding object
pairs are removed, whereas most of the colliding object pairs
are conserved. This prevents from having false collisions de-
tected without hindering the detection of actual collisions.

The velocity components of Rule (17) allows to remove
non-colliding object pairs by looking into their velocity
changes. We plot the distributions of velocity change of col-
liding objects two frames before collision in blue, one frame
before collision in orange, upon collision in green, and one
frame after collision in red, from training videos 0-5000 in
Figure 3. Non-colliding objects have velocity changes cen-
tered around 0, as expected by Newton’s ﬁrst law of motion
because no force is imposed over them. The distributions
of colliding objects - green and orange - are more uniform
and the vertical black line representing the velocity change
thresholding helps in removing object pairs which may be
closer than the euclidean threshold without actually collid-
ing, as the velocities of objects do not change. We note that
the similar distributions for the previous and upon collision
frames reveal that collisions can span over two time frames.

Conclusion & Future Work
We present a model which possesses the advantages of both
deep learning and symbolic artiﬁcial intelligence, yield-
ing state-of-the-art performance on the CLEVRER dataset
VideoQA task. By equipping our model with commonsense
knowledge based on temporal, causal and physics rules we
propose a general framework applicable for reasoning over
video’s physical events. These rules contain enough infor-
mation to model the causal and temporal reasoning of a ref-
erence dataset.

By introducing ASP into VideoQA we expand the breadth
of the symbolic component of neuro-symbolic methods and
pave the way for the incorporation of research ﬁelds such
as Inductive Logic Programming (ILP). Future works will
employ symbolic-rule learning systems to remove the spe-
ciﬁc human engineered rules built to detect the events over
the videos, whilst remaining explainable. This would also
allow for automating the improvement pipeline presented.
Systems such as ILASP (Law, Russo, and Broda 2020) or
FastLAS (Law et al. 2020) provide the ASP framework for

ILP learning tasks and would enable the model to build an
interpretable cognitive model of the events in the scene from
videos, perception, and background knowledge directly.

References

Aditya, S.; Yang, Y.; and Baral, C. 2018. Explicit reason-
ing over end-to-end neural architectures for visual question
answering. CoRR abs/1803.08896.
Amizadeh, S.; Palangi, H.; Polozov, O.; Huang, Y.; and
Koishida, K. 2020. Neuro-symbolic visual reasoning: Dis-
entangling ”visual” from ”reasoning”. In ICML 2020.
Basu, K.; Shakerin, F.; and Gupta, G. 2020. Aqua: Asp-
based visual question answering.
In Komendantskaya, E.,
and Liu, Y. A., eds., Practical Aspects of Declarative Lan-
guages - 22nd International Symposium, PADL 2020, New
Orleans, LA, USA, January 20-21, 2020, Proceedings, vol-
ume 12007 of Lecture Notes in Computer Science, 57–72.
Springer.
Booch, G.; Fabiano, F.; Horesh, L.; Kate, K.; Lenchner, J.;
Linck, N.; Loreggia, A.; Murugesan, K.; Mattei, N.; Rossi,
F.; and Srivastava, B. 2020. Thinking fast and slow in ai.
Gebser, M.; Kaminski, R.; Kaufmann, B.; and Schaub, T.
2014. Clingo = ASP + control: Preliminary report. CoRR
abs/1405.3694.
Gokhale, T.; Banerjee, P.; Baral, C.; and Yang, Y. 2020.
Vqa-lol: Visual question answering under the lens of logic.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep residual
learning for image recognition. CoRR abs/1512.03385.
He, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. B. 2017.
Mask R-CNN. CoRR abs/1703.06870.
Hu, R.; Andreas, J.; Rohrbach, M.; Darrell, T.; and Saenko,
K. 2017. Learning to reason: End-to-end module networks
for visual question answering. CoRR abs/1704.05526.
Hudson, D. A., and Manning, C. D.
2018. Composi-
tional attention networks for machine reasoning. CoRR
abs/1803.03067.
Kahneman, D. 2011. Thinking, fast and slow. New York:
Farrar, Straus and Giroux.
Katzouris, N.; Artikis, A.; and Paliouras, G. 2016. Online
learning of event deﬁnitions. Theory and Practice of Logic
Programming 16(5-6):817–833.
Khan, A.; Bozzato, L.; Seraﬁni, L.; and Lazzerini, B. 2019.
Visual reasoning on complex events in soccer videos using
answer set programming. In Calvanese, D., and Iocchi, L.,
eds., GCAI 2019. Proceedings of the 5th Global Confer-
ence on Artiﬁcial Intelligence, Bozen/Bolzano, Italy, 17-19
September 2019, volume 65 of EPiC Series in Computing,
42–53. EasyChair.
Law, M.; Russo, A.; Bertino, E.; Broda, K.; and Lobo, J.
2020. Fastlas: Scalable inductive logic programming in-
In The
corporating domain-speciﬁc optimisation criteria.
Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applications of
Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth

AAAI Symposium on Educational Advances in Artiﬁcial In-
telligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, 2877–2885. AAAI Press.
Law, M.; Russo, A.; and Broda, K. 2018. Inductive learn-
ing of answer set programs from noisy examples. CoRR
abs/1808.08441.
Law, M.; Russo, A.; and Broda, K. 2020. The ilasp system
for inductive learning of answer set programs.
Lifschitz, V. 2008. What is answer set programming? In
Proceedings of the 23rd National Conference on Artiﬁcial
Intelligence - Volume 3, AAAI’08, 1594–1597. AAAI Press.
Lin, T.; Doll´ar, P.; Girshick, R. B.; He, K.; Hariharan, B.;
and Belongie, S. J. 2016. Feature pyramid networks for
object detection. CoRR abs/1612.03144.
Mascharka, D.; Tran, P.; Soklaski, R.; and Majumdar, A.
2018. Transparency by design: Closing the gap between
performance and interpretability in visual reasoning. CoRR
abs/1803.05268.
Mun, J.; Seo, P. H.; Jung, I.; and Han, B. 2016. Marioqa:
Answering questions by watching gameplay videos. CoRR
abs/1612.01669.
Prapas, I.; Paliouras, G.; Artikis, A.; and Baskiotis, N. 2018.
Towards human activity reasoning with computational logic
and deep learning. In Proceedings of the 10th Hellenic Con-
ference on Artiﬁcial Intelligence, SETN ’18. New York, NY,
USA: Association for Computing Machinery.
Shanahan, M. 1999. The Event Calculus Explained. Berlin,
Heidelberg: Springer-Verlag. 409–430.
Singh, H.; Aggrawal, M.; and Krishnamurthy, B. 2020. Ex-
ploring neural models for parsing natural language into ﬁrst-
order logic.
Wu, J.; Tenenbaum, J. B.; and Kohli, P. 2017. Neural scene
de-rendering. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).
Xu, D.; Zhao, Z.; Xiao, J.; Wu, F.; Zhang, H.; He, X.; and
Zhuang, Y. 2017. Video question answering via gradually
reﬁned attention over appearance and motion. In Proceed-
ings of the 25th ACM International Conference on Multime-
dia, MM ’17, 1645–1653. New York, NY, USA: Association
for Computing Machinery.
Yi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenen-
baum, J. B. 2018. Neural-symbolic vqa: Disentangling rea-
In Pro-
soning from vision and language understanding.
ceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS’18, 1039–1050. Red
Hook, NY, USA: Curran Associates Inc.
Yi, K.; Gan, C.; Li, Y.; Kohli, P.; Wu, J.; Torralba, A.; and
Tenenbaum, J. B. 2019. Clevrer: Collision events for video
representation and reasoning.
Yu, Z.; Xu, D.; Yu, J.; Yu, T.; Zhao, Z.; Zhuang, Y.; and
Tao, D. 2019. Activitynet-qa: A dataset for understand-
ing complex web videos via question answering. CoRR
abs/1906.02467.
Zhang, Y.; Hare, J. S.; and Pr¨ugel-Bennett, A. 2018. Learn-
ing to count objects in natural images for visual question
answering. CoRR abs/1802.05766.

