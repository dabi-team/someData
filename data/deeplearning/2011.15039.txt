Combinatorial Learning of Graph Edit Distance via Dynamic Embedding

Runzhong Wang1,2

Tianqi Zhang1,2

Tianshu Yu3

Junchi Yan1,2*

Xiaokang Yang2

1 Department of Computer Science and Engineering, Shanghai Jiao Tong University

2 MoE Key Lab of Artiﬁcial Intelligence, Shanghai Jiao Tong University

3 Arizona State University

{runzhong.wang,lygztq,yanjunchi,xkyang}@sjtu.edu.cn

tianshuy@asu.edu

0
2
0
2

c
e
D
1

]

G
L
.
s
c
[

2
v
9
3
0
5
1
.
1
1
0
2
:
v
i
X
r
a

Abstract

Graph Edit Distance (GED) is a popular similarity mea-
surement for pairwise graphs and it also refers to the re-
covery of the edit path from the source graph to the target
graph. Traditional A* algorithm suffers scalability issues
due to its exhaustive nature, whose search heuristics heavily
rely on human prior knowledge. This paper presents a hy-
brid approach by combing the interpretability of traditional
search-based techniques for producing the edit path, as well
as the efﬁciency and adaptivity of deep embedding models
to achieve a cost-effective GED solver. Inspired by dynamic
programming, node-level embedding is designated in a dy-
namic reuse fashion and suboptimal branches are encour-
aged to be pruned. To this end, our method can be read-
ily integrated into A* procedure in a dynamic fashion, as
well as signiﬁcantly reduce the computational burden with
a learned heuristic. Experimental results on different graph
datasets show that our approach can remarkably ease the
search process of A* without sacriﬁcing much accuracy. To
our best knowledge, this work is also the ﬁrst deep learning-
based GED method for recovering the edit path.

1. Introduction

Graph edit distance (GED) is a popular similarity mea-
surement between graphs, which lies in the core of many
vision and pattern recognition tasks including image match-
ing [10], signature veriﬁcation [27], scene-graph edition [9],
drug discovery [30], and case-based reasoning [46]. In gen-
eral, GED algorithms aim to ﬁnd an optimal edit path from
source graph to target graph with minimum edit cost, which
is inherently an NP-complete combinatorial problem [2]:

GED(G1, G2) =

min
(e1,...,el)∈γ(G1,G2)

l
(cid:88)

i=1

c(ei)

(1)

where γ(G1, G2) denote the set of all possible “edit paths”
transforming source graph G1 to target graph G2. c(ei) mea-

*Junchi Yan is the corresponding author.

Figure 1. Top: an edit path between two simple graphs G1, G2.
Bottom: an example of querying images via GED, where only ge-
ometric information is involved. The last image shows an “unsim-
ilar” image based on GED measurement.

sures the cost of edit operation ei.

Exact GED solvers [2, 32] guarantee to ﬁnd the optimal
solution under dynamic condition, at the cost of poor scal-
ability on large graphs, and these exact solvers heavily rely
on heuristics to estimate the corresponding graph similarity
based on the current partial solution. Recent efforts in deep
graph similarity learning [3, 4, 26] adopt graph neural net-
works [22, 34] to directly regress graph similarity scores,
without explicitly incorporating the intrinsic combinatorial
nature of GED, hence fail to recover the edit path. However,
the edit path is often of the central interest in many applica-
tions [9, 10] and most GED works [2, 31, 13, 45, 32] still
are more focused on ﬁnding the edit path itself.

As the growth of graph size, it calls for more scalable
GED solvers which are meanwhile expected to recover the
exact edit path. However, these two merits cannot both hold
by existing methods. As discussed above, deep learning-
based solvers have difﬁculty in recovering the edit path
while the learning-free methods suffer scalability issue. In
this paper, we are aimed to design a hybrid solver by com-
bining the best of the two worlds.

Speciﬁcally, we resort to A* algorithm [32] which is a
popular solution among open source GED softwares [8, 20],
and we adopt neural networks to predict similarity scores
which are used to guide A* search, in replacement of man-

1

𝓖𝟐GED(𝓖𝟏,𝓖𝟐)= 3edge deletionedge deletionnode deletion𝒄𝒆𝟏=𝟏𝒄𝒆𝟐=𝟏𝒄𝒆𝟑=𝟏𝓖𝟏Source GraphGED=1.62GED=1.94GED=2.34GED=3.80 
 
 
 
 
 
ually designed heuristics in traditional A*. We want to high-
light our proposed Graph Edit Neural Network (GENN) in
two aspects regarding the dynamic programming concepts:
Firstly, we propose to reuse the previous embedding in-
formation given a graph modiﬁcation (e.g. node deletion)
where among the states of A* search tree the graph nodes
are deleted progressively1; Secondly, we propose to learn
more effective heuristic to avoid unnecessary exploration
over suboptimal branches to achieve signiﬁcant speed-up.

The contributions made in this paper are:
1) We propose the ﬁrst (to our best knowledge) deep net-
work solver for GED, where a search tree state selection
heuristic is learned by dynamic graph embedding. It out-
performs traditional heuristics in efﬁcacy.

2) Speciﬁcally, we devise a speciﬁc graph embedding
method in the spirit of dynamic programming to reuse the
previous computation to the utmost extent. In this sense, our
method can be naturally integrated with the A* procedure
where a dynamical graph similarity prediction is involved
after each graph modiﬁcation, achieving much lower com-
plexity compared to vanilla graph embeddings.

3) Experimental results on real-world graph data show
that our learning-based approach achieves higher accuracy
than state-of-the-art manually designed inexact solvers [13,
31]. It also runs much faster than A* exact GED solvers [6,
32] that perform exhaustive search to ensure the global op-
timum, with comparable accuracy.

2. Related Work

2.1. Traditional GED Solvers

Exact GED solvers. For small-scale problems, an exhaus-
tive search can be used to ﬁnd the global optimum. Exact
methods are mostly based on tree-search algorithms such
as A* algorithm [32], whereby a priority queue is main-
tained for all pending states to search, and the visiting order
is controlled by the cost of the current partial edit path and
a heuristic prediction on the edit distance between the re-
maining subgraphs [31, 45]. Other combinatorial optimiza-
tion techniques, e.g. depth-ﬁrst branch-and-bound [2] and
linear programming lower bound [25] can also be adopted
to prune unnecessary branches in the searching tree. How-
ever, exact GED methods are too time-consuming and they
suffer from poor scalability on large graphs [1].
Inexact GED solvers aim to mitigate the scalability issue
by predicting sub-optimal solutions in (usually) polynomial
time. To our knowledge, bipartite matching based meth-
ods [13, 31, 45] so far show competitive trade-off between
time and accuracy, where edge edition costs are encoded
into node costs and the resulting bipartite matching prob-
lem can be solved in polynomial time by either Hungar-

1To distinguish the “nodes” in graphs and the “nodes” in the search

tree, we name “state” for the ones in the search tree.

ian [23, 31] or Volgenant-Jonker [13, 19] algorithm. Beam
search [20] is the greedy version of the exact A* algorithm.
Another line of works namely approximate graph match-
ing [11, 18, 39, 41, 43, 48] are closely related to inexact
GED, and there are efforts adopting graph matching meth-
ods e.g. IPFP [24] to solve GED problems [7]. Two draw-
backs in inexact solvers are that they rely heavily on human
knowledge and their solution qualities are relatively poor.

2.2. Deep Graph Similarity Learning

Regression-based Similarity Learning. The recent suc-
cess in machine learning on non-euclidean data (i.e. graphs)
via GNNs [14, 22, 34, 49] has encouraged researchers to
design approximators for graph similarity measurements
such as GED. SimGNN [3] ﬁrst formulates graph similar-
ity learning as a regression task, where its GCN [22] and
attention [36] layers are supervised by GED scores solved
by A* [20]. Bai et al. [4] extends their previous work by
processing a multi-scale node-wise similarity map using
CNNs. Li et al. [26] propose a cross-graph module in feed-
forward GNNs which elaborates similarity learning. Such a
scheme is also adopted in information retrieval, where [12]
adopts a convolutional net to predict the edit cost between
texts. However, all these regression models can not predict
an edit path, which is mandatory in the GED problem.
Deep Graph Matching. As another combinatorial prob-
lem closely related to GED, there is increasing attention in
developing deep learning graph matching approaches [16,
17, 37] since the seminal work [44], and many researchers
[33, 37, 38, 42] start to take a combinatorial view of graph
matching learning rather than a regression task. Compared
to graph similarity learning methods, deep graph matching
can predict the edit path, but they are designated to match
similarly structured graphs and lack particular mechanisms
to handle node/edge insertion/deletions. Therefore, modi-
ﬁcation is needed to ﬁt deep graph matching methods into
GED, which is beyond the scope of this paper.

2.3. Dynamic Graph Embedding

The major line of graph embedding methods [14, 22, 34,
49] assumes that graphs are static which limit their appli-
cation on real-world graphs that evolve over time. A line
of works namely dynamic graph embedding [29, 28, 47]
aims to solve such issue, whereby recurrent neural net-
works (RNNs) are typically combined with GNNs to cap-
ture the temporal information in graph evolution. The appli-
cations include graph sequence classiﬁcation [28], dynamic
link prediction [29], and anomaly detection [47]. Dynamic
graph embedding is also encountered in our GED learning
task, however, all these aforementioned works cannot be ap-
plied to our setting where the graph structure evolves at dif-
ferent states of the search tree, instead of time steps.

2

Algorithm 1: A* Algorithm for Exact GED

Input: Graphs G1 = (V1, E1), G2 = (V2, E2), where
V1 = {u1, ..., un1 }, V2 = {v1, ..., vn2 }

1 Initialize OPEN as an empty priority queue;
2 Insert (u1 → w) to OPEN for all w ∈ V2;
3 Insert (u1 → (cid:15)) to OPEN;
4 while no solution is found do
5

Select p with minimum (g(p) + h(p)) in OPEN;
if p is a valid edit path then
return p as the solution;

else

Let p contains {u1, ..., uk} ⊆ V1 and W ⊆ V2;
if k ≤ n1 then

Insert p ∪ (uk+1 → vi) to OPEN for all
vi ∈ V2\W ;
Insert p ∪ (uk+1 → (cid:15)) to OPEN;

else

Insert p ∪ (cid:83)

vi∈V2\W ((cid:15) → vi) to OPEN;

Figure 2. A partial edit path as one state of A* search tree. Given
the partial solution p = (u(cid:7) → v(cid:7), u(cid:78) → v(cid:78)), the edge edition
(u(cid:7)u(cid:78) → v(cid:7)v(cid:78)) can be induced from node editions.

3. Our Approach

In this section, we ﬁrst introduce the A* algorithm for
GED in Sec. 3.1, then we present our efﬁcient dynamic
graph embedding approach GENN for A* in Sec. 3.2.

6

7

8

9

10

11

12

13

14

3.1. Preliminaries on A* Algorithm for GED

Output: An optimal edit path from G1 to G2.

To exactly solve the GED problem, researchers usually
adopt tree-search based algorithms which traverse all possi-
ble combinations of edit operations. Among them, A* algo-
rithm is rather popular [31, 20, 32, 8] and we base our learn-
ing method on it. In this section, we introduce notations for
GED and discuss the key components in A* algorithm.

GED aims to ﬁnd the optimal edit path with minimum
edit cost, to transform the source graph G1 = (V1, E1) to the
target graph G2 = (V2, E2), where |V1| = n1, |V2| = n2.
We denote V1 = {u1, ..., un1}, V2 = {v1, ..., vn2} as the
nodes in the source graph and the target graph, respectively,
and (cid:15) as the “void node”. Possible node edit operations in-
clude node substitution ui → vj, node insertion (cid:15) → vj
and node deletion ui → (cid:15), and the cost of each operation
is deﬁned by the problem. As shown in Fig. 2, the edge
editions can be induced given node editions, therefore only
node editions are explicitly considered in A* algorithm.2

Alg. 1 illustrates a standard A* algorithm in line with
[31, 32]. A priority queue is maintained where each state of
the search tree contains a partial solution to the GED prob-
lem. As shown in Fig. 2, the priority of each state is deﬁned
as the summation of two metrics: g(p) representing the cost
of the current partial solution which can be computed ex-
actly, and h(p) means the heuristic prediction of GED be-
tween the unmatched subgraphs. A* always explores the
state with minimum g(p) + h(p) at each iteration and the
optimality is guaranteed if h(p) ≤ hopt(p) holds for all par-

2Node substitution can be viewed as node-to-node matching between
two graphs, and node insertion/deletion can be viewed as matching nodes
in source/target graph to the void node, respectively. The concepts “match-
ing” and “edition” may interchange with each other through this paper.

tial solutions [31], where hopt(p) means the optimal edit
cost between the unmatched subgraphs.

A proper h(p) is rather important to speed up the al-
gorithm, and we discuss three variants of A* accordingly:
1) If h(p) = hopt(p), one can directly ﬁnd the optimal path
greedily. However, computing hopt(p) requires another
exponential-time solver which is intractable. 2) Heuris-
tics can be utilized to predict h(p) where 0 ≤ h(p) ≤
hopt(p). Hungarian bipartite heuristic [32] is among the
best-performing heuristic where the time complexity is
O((n1 + n2)3). In our experiments, Hungarian-A* [32]
is adopted as the baseline traditional exact solver. 3) Plain-
A* is the simplest, where it always holds h(p) = 0 and
such strategy introduces no overhead when computing h(p).
However, the search tree may become too large without any
“look ahead” on the future cost.

The recent success of graph similarity learning [3, 4, 26]
inspires us to predict high-quality h(p) which is close to
hopt(p) in a cost-efﬁcient manner via learning. In this pa-
per, we propose to mitigate the scalability issue of A* by
predicting h(p) via dynamic graph embedding networks,
where h(p) is efﬁciently learned and predicted and the sub-
optimal branches in A* are pruned. It is worth noting that
we break the optimality condition h(p) ≤ hopt(p), but the
loss of accuracy is acceptable, as shown in experiments.

3.2. Graph Edit Neural Network

An overview of our proposed Graph Edit Neural
Network-based A* (GENN-A*)
learning algorithm is
shown in Fig. 3. Our GENN-A* can be split into node
embedding module (Sec. 3.2.1), dynamic embedding tech-
nique (Sec. 3.2.2), graph similarity prediction module

3

𝒈(𝒑)𝒉(𝒑)Exact edit cost of 𝒑Predicted edit cost for the unmatched subgraphFigure 3. Our proposed GENN-A*. Left: Node embedding. Input graphs are fed into GNN to extract node-level embeddings. These
embeddings are cached to be reused in the following computation. Middle: A* search tree. The state in the search tree is a matching of
nodes between graphs. All matched nodes are masked (light color) and the unmatched subgraphs (dark color) will be involved to predict
h(p). Right: Dynamic graph similarity prediction. Cached embeddings are loaded for nodes in the unmatched subgraphs, and a graph-
level embedding is obtained via attention. Finally the predicted graph similarity s(p) ∈ (0, 1) is obtained from graph-level embeddings by
neural tensor network and transformed to the heuristic score h(p).

(Sec. 3.2.3) and ﬁnally the training procedure (Sec. 3.2.4).

3.2.1 Node Embedding Module

The overall pipeline of our GENN is built in line with
SimGNN [3], and we remove the redundant histogram mod-
ule in SimGNN in consideration of efﬁciency. Given input
graphs, node embeddings are computed via GNNs.

Initialization. Firstly, the node embeddings are initial-
ized as the one-hot encoding of the node degree. For graphs
with node labels (e.g. molecule graphs), we encode the node
labels by one-hot vector and concatenate it to the degree
embedding. The edges can be initialized as weighted or un-
weighted according to different deﬁnitions of graphs.

GNN backbone. Based on different types of graph
data, Graph Convolutional Network (GCN) [22] is utilized
for ordinary graph data (e.g. molecule graphs and program
graphs) and SplineCNN [14] is adopted for graphs built
from 2D images, considering the recent success of adopting
spline kernels to learn geometric features [16, 33]. The node
embeddings obtained by the GNN backbone are cached for
further efﬁcient dynamic graph embedding. We build three
GNN layers for our GENN in line with [3].

3.2.2 Dynamic Embedding with A* Search Tree

A* is inherently a dynamic programming (DP) algorithm
where matched nodes in partial solutions are progressively
masked. When solving GED, each state of A* contains a
partial solution and in our method embedding networks are
adopted to predict the edit distance between two unmatched
subgraphs. At each state, one more node is masked out in
the unmatched subgraph compared to its parent state. Such
a DP setting differs from existing so-called dynamic graph
embedding problems [29, 28, 47] and calls for efﬁcient cues
since the prediction of h(p) is encountered at every state

Figure 4. Comparison of three graph neural network variants for
dynamic graph embedding in A* algorithm. We assume three
graph convolution layers in line with our implementation.
In
vanilla GNN, a complete forward pass is required for all nodes
which contains redundant operations. The exact dynamic GNN
caches all intermediate embeddings and only the 3-hop neighbors
of the masked node are updated. Finally, our proposed GENN re-
quires no convolution operation and is the most efﬁcient.

of the search tree.
In this section, we discuss and com-
pare three possible dynamic embedding approaches, among
which our proposed GENN is built based on DP concepts.

Vanilla GNN. The trivial way of handling the dynamic
condition is that when the graph is modiﬁed, a complete
feed-forward pass is called for all nodes in the new graph.
However, such practice involves redundant computation,
which is discussed as follows. We denote n as the num-
ber of nodes, F as embedding dimensions, and K as the
number of GNN layers. Assuming fully-connected graph
as the worst case, the time complexity of vanilla GNN is
O(n2F K + nF 2K) and no caching is needed.

Exact Dynamic GNN. As shown in the second row of
Fig. 4, when a node is masked, only the embeddings of
neighboring nodes are affected.
If we cache all interme-
diate embeddings of the forward pass, one can compute the

4

……GNN backboneCached Node EmbeddingsA* Search TreeNode EmbeddingDynamic Graph Similarity PredictionEmbedding 1Embedding 2AttentionAttentionNeural Tensor NetworkPredicted Similarity 𝒔𝒑Cached Node Embeddings (Masked)𝒉𝒑=−𝟎.𝟓 (𝐧𝟏+𝐧𝟐) 𝐥𝐨𝐠 𝒔𝒑masked nodemasked edgenode with cached embeddingVanilla GNNExact Dynamic GNNOur GENNmasked nodeupdated embeddingConv1Conv2Conv3Conv1Conv2Conv3QueryMask nodemasked edgecached embeddingcached graphembeddingexact embedding at a minimum computational cost. Based
on the message-passing nature of GNNs, at the k-th convo-
lution layer, only the k-hop neighbors of the masked node
are updated. However, the worst-case time complexity is
still O(n2F K + nF 2K) (for fully-connected graphs), and
it requires O(nF K) memory cache for all convolution lay-
ers. If all possible subgraphs are cached for best time ef-
ﬁciency, the memory cost grows to O(n2nF K) which is
unacceptable. Experiment result shows that the speed-up of
this strategy is negligible with our testbed.

Our GENN. As shown in the last row of Fig. 4, we ﬁrstly
perform a forward convolution pass and cache the embed-
dings of the last convolution layer. During A* algorithm, if
some nodes are masked out, we simply delete their embed-
dings from the last convolution layer and feed the remain-
ing embeddings into the similarity prediction module. Our
GENN involves single forward pass which is negligible, and
the time complexity of loading caches is simply O(1) and
the memory consumption of caching is O(nF ).

Our design of the caching scheme of GENN is mainly in-
spired by DP: given modiﬁcation on the input graph (node
deletion in our A* search case), the DP algorithm reuses
the previous results for further computations in considera-
tion of best efﬁciency. In our GENN, the node embeddings
are cached for similarity computation on its subgraphs. In
addition, DP algorithms tend to minimize the exploration
space for best efﬁciency, and our learned h(p) prunes sub-
optimal branches more aggressively than traditional heuris-
tics which speeds up the A* solver.

3.2.3 Graph Similarity Prediction

After obtaining the embedding vectors from cache, the at-
tention module and neural tensor network are called to pre-
dict the similarity score. For notation simplicity, our discus-
sions here are based on full-sized, original input graphs.

Attention module for graph-level embedding. Given
node-level embeddings, the graph-level embedding is ob-
tained through attention mechanism [36]. We denote X1 ∈
Rn1×F , X2 ∈ Rn2×F as the node embeddings from GNN
backbone. The global keys are obtained by mean aggrega-
tion followed with nonlinear transform:

¯X1 = mean(X1), ¯X2 = mean(X2)

k1 = tanh( ¯X1W1), k2 = tanh( ¯X2W1)

(2)

(3)

where mean(·) is performed on the ﬁrst dimension (node
dimension) and W1 ∈ RF ×F is learnable attention
weights. Aggregation coefﬁcients are computed from
k1, k2 ∈ R1×F and X1, X2:

c1 = δ(X1k(cid:62)

1 · α), c2 = δ(X2k(cid:62)

2 · α)

(4)

5

where α = 10 is the scaling factor and δ(·) means sigmoid.
The graph-level embedding is obtained by weighted sum-
mation of node embeddings based on aggregation coefﬁ-
cients c1 ∈ Rn1×1, c2 ∈ Rn2×1:

g1 = c(cid:62)

1 X1, g2 = c(cid:62)

2 X2

(5)

Neural Tensor Network for similarity prediction.
Neural Tensor Network (NTN) [35] is adopted to measure
the similarity between g1, g2 ∈ R1×F :

s(G1, G2) = f (g1W[1:t]

2 g(cid:62)

2 + W3cat(g1, g2) + b) (6)

where W2 ∈ RF ×F ×t, W3 ∈ Rt×2F , b ∈ Rt are learn-
able, the ﬁrst term means computing g1W2[:, :, i]g(cid:62)
2 for all
i ∈ [1...t] and then stacking them, f : Rt → (0, 1) denotes
a fully-connected layer with sigmoid activation, and cat(·)
means to concat along the last dimension. t controls the
number of channels in NTN and we empirically set t = 16.
In line with [3], the model prediction lies within (0, 1)
which represents a normalized graph similarity score with
the following connection to GED:

s(G1, G2) = exp (−GED(G1, G2) × 2/(n1 + n2))

(7)

For partial edit path encountered in A* algorithm, the pre-
dicted similarity score s(p) can be transformed to h(p) fol-
lowing Eq. 7:

h(p) = −0.5(n(cid:48)

1 + n(cid:48)

2) log s(p)

(8)

1, n(cid:48)

1 + n(cid:48)

2)F 2) and O(n(cid:48)

where n(cid:48)
2 means the number of nodes in the unmatched
subgraph. The time complexities of attention and NTN are
O((n(cid:48)
1n(cid:48)
2F t), respectively. Since the
convolution layers are called only once which is negligible,
and the time complexity of loading cached GENN embed-
ding is O(1), the overall time complexity of each predic-
tion is O((n(cid:48)
1n(cid:48)
2F t). Our time complexity
is comparable to the best-known learning-free prediction of
h(p) [32] which is O((n(cid:48)

2)F 2 + n(cid:48)

1 + n(cid:48)

1 + n(cid:48)

2)3).

3.2.4 Supervised Dynamic Graph Learning

The training of our GENN consists of two steps: Firstly,
GENN weights are initialized with graph similarity score
labels from the training dataset. Secondly, the model is ﬁne-
tuned with the optimal edit path solved by A* algorithm.
The detailed training procedure is listed in Alg. 2.

Following deep graph similarity learning peer meth-
ods [3, 4], our GENN weights are supervised by ground
truth labels provided by the dataset. For datasets with rel-
atively small graphs, optimal GED scores can be solved as
ground truth labels. In cases where optimal GEDs are not
available, we can build the training set based on other mean-
ingful measurements, e.g. adopting semantic node match-
ing ground truth to compute GED labels.

Figure 5. Average search tree size w.r.t. problem size (n1 + n2). The search tree reduces signiﬁcantly when the problem size grows,
especially on more challenging AIDS and Willow-Cars where about ×5 and ×4 reductions of state are achieved respectively via GENN.

multiple hopt(p) labels by solving the GED only once.

Theorem 1. (Optimal Partial Cost) Given an optimal edit
path p∗ and the corresponding GED(p∗), for any partial
edit path p ⊆ p∗, there holds g(p) + hopt(p) = GED(p∗).

Proof. If g(p) + hopt(p) > GED(p∗), then the minimum
edit cost following p is larger than GED(p∗), therefore p
is not a partial optimal edit path, which violates p ⊆ p∗.
If g(p) + hopt(p) < GED(p∗), it means that there exists a
better edit path whose cost is smaller than GED(p∗), which
violates the condition that p∗ is the optimal edit path. Thus,
g(p) + hopt(p) = GED(p∗).

Based on Theorem 1, there holds hopt(p) = GED(p∗)−
g(p) for any partial optimal edit path. Therefore, if we solve
an optimal p∗ with m node editions, (2m−1) optimal partial
edit paths can be used for ﬁnetuning. In experiments, we
randomly select 200 graph pairs for ﬁnetuning since we ﬁnd
it adequate for convergence.

4. Experiment

4.1. Settings and Datasets

We evaluate our learning-based A* method on three
challenging real-world datasets: AIDS, LINUX [40], and
Willow dataset [10].
AIDS dataset contains chemical compounds evaluated for
the evidence of anti-HIV activity3. AIDS dataset is pre-
processed by [3] who remove graphs more than 10 nodes
and the optimal GED between any two graphs is provided.
Following [3], we deﬁne the node edition cost c(ui →
vj) = 1 if ui, vj are different atoms, else c(ui → vj) = 0.
The node insertion and deletion costs are both deﬁned as
1. The edges are regraded as non-attributed, therefore edge
substitution cost = 0 and edge insertion/deletion cost = 1.
LINUX dataset is proposed by [40] which contains Pro-
gram Dependency Graphs (PDG) from the LINUX kernel,

3https://wiki.nci.nih.gov/display/NCIDTPdata/

AIDS+Antiviral+Screen+Data

Figure 6. The visualization of a query on Willow-Cars dataset by
GENN-A*. All of the 4 most similar graphs are close to the source
graph in terms of poses and graph structures, yet the 4 least similar
ones vary greatly in their poses and appearances. Green letters
mean our GENN-A* solves the optimal GED.

Algorithm 2: The Training Procedure of GENN-A*

Input: Training set of graphs pairs {(Gi, Gj)} with
similarity score labels {sgt(Gi, Gj)}.

1 while not converged do # training with GT labels
Randomly sample (Gi, Gj) from training set;
2
Compute s(Gi, Gj) by vanilla GENN;
Update parameters by MSE(s(Gi, Gj), sgt(Gi, Gj));

4

3

5 while not converged do # ﬁnetune with optimal path
Randomly sample (Gi, Gj) from training set;
6
Solve the optimal edit path p∗ and GED(p∗) by A*;
Call GENN on (Gi, Gj) and cache the embeddings;
for partial edit path p ⊆ p∗ do

9

8

7

10

11

12

13

compute g(p) and hopt(p) = GED(p∗) − g(p);
sopt(p) = exp(−2hopt(p)/(n(cid:48)
compute s(p) from cached GENN embeddings;
Update parameters by MSE(s(p), sopt(p));

1 + n(cid:48)

2));

Output: GENN with learned parameters.

We further propose a ﬁnetuning scheme of GENN to bet-
ter suit the A* setting. However, tuning GENN with the
states of the search tree means we require labels of hopt(p),
while solving the hopt(p) for an arbitrary partial edit path
is again NP-complete. Instead of solving as many hopt(p)
as needed, here we propose an efﬁcient way of obtaining

6

Source GraphGENN-A*=1.68Optimal=1.40GENN-A*=1.79Optimal=1.26GENN-A*=1.91Optimal=1.91GENN-A*=2.18Optimal=2.18GENN-A*=2.83Optimal=2.83GENN-A*=2.90Optimal=2.90GENN-A*=3.10Optimal=3.01GENN-A*=3.11Optimal=3.11Most Similar Graphs:Least Similar Graphs:Method

Edit
Path mse (×10−3)

AIDS

ρ

p@10 mse (×10−3)

LINUX
ρ

p@10 mse (×10−3)

Willow-Cars

×
SimGNN [3]
×
GMN [26]
×
GraphSim [4]
×
GENN (ours)
Beam Search [20] (cid:88)
Hungarian [31] (cid:88)
VJ [13] (cid:88)
GENN-A* (ours) (cid:88)

1.189
1.886
0.787
1.618

12.090
25.296
29.157
0.635

0.843
0.751
0.874
0.901

0.609
0.510
0.517
0.959

0.421
0.401
0.534
0.880

0.481
0.360
0.310
0.871

1.509
1.027
0.058
0.438

9.268
29.805
63.863
0.324

0.939
0.933
0.981
0.955

0.827
0.638
0.581
0.991

0.942
0.833
0.992
0.527

0.973
0.913
0.287
0.962

ρ

-
-
-
-

p@10

-
-
-
-

-
-
-
-

1.820
29.936
45.781
0.599

0.815
0.553
0.438
0.928

0.725
0.650
0.512
0.938

Table 1. Evaluation on benchmarks AIDS, LINUX and Willow-Cars. Our method can work either in a way involving explicit edit path
generation as traditional GED solvers [31, 13, 32], or based on direct similarity computing without deriving the edit distance [3, 26, 4].
The evaluation metrics are deﬁned and used by [3, 4]: mse stands for mean square error between predicted similarity score and ground
truth similarity score. ρ means the Spearman’s correlation between prediction and ground truth. p@10 means the precision of ﬁnding the
closest graph among the predicted top 10 most similar ones. Willow-Cars is not compared with deep learning methods because optimal
GED labels are not available for the training set. The AIDS and LINUX peer method results are quoted from [4].

and the authors of [3] also provides a pre-processed ver-
sion where graphs are with maximum 10 nodes and optimal
GED values are provided as ground truth. All nodes and
edges are unattributed therefore the substitution cost is 0,
and the insertion/deletion cost is 1.
Willow dataset is originally proposed by [10] for seman-
tic image keypoint matching problem, and we validate the
performance of our GENN-A* on computer vision prob-
lems with the Willow dataset. All images from the same
category share 10 common semantic keypoints.
“Cars”
dataset is selected in our experiment. With Willow-Cars
dataset, graphs are built with 2D keypoint positions by De-
launay triangulation, and the edge edition cost is deﬁned
as c(Ei → Ej) = |Ei − Ej| where Ei, Ej are the length
of two edges. Edge insertion/deletion costs of Ei are de-
ﬁned as |Ei|. All edge lengths are normalized by 300 for
numerical concerns. The node substitution has 0 cost, and
c(ui → (cid:15)) = c((cid:15) → vj) = ∞ therefore node inser-
tion/deletion are prohibited. We build the training set la-
bels by computing the GED based on semantic keypoint
matching relationship, and it is worth noting such GEDs
are different from the optimal ones. However, experiment
results show that such supervision is adequate to initialize
the model weights of GENN.

Among all three datasets, LINUX has the simplest def-
inition of edit costs.
In comparison, AIDS has attributed
nodes and Willow dataset has attributed edges, making
these two datasets more challenging than LINUX dataset.
In line with [3], we split all datasets by 60% for training,
20% for validation, and 20% for testing.

is

Our GENN-A*

implemented with Pytorch-
Geometric [15] and the A* algorithm is implemented
with Cython [5] in consideration of performance. We
adopt GCN [22] for AIDS and LINUX datasets and
SplineCNN [14] for 2D Euclidean data from Willow-Cars
(#kernels=16). The number of feature channels are deﬁned

Method AIDS

LINUX Willow-Cars

Hungarian-A* [32]
GENN-A* (ours)

29.915
13.323

2.332
2.177

188.234
78.481

Table 2. Averaged time (sec) for solving GED problems.

Vanilla GNN Exact Dynamic GNN GENN (ours) Hungarian [32]

time

2.329

3.145

0.417

0.358

Table 3. Averaged time (msec) of different methods to predict
h(p). Statistics are collected on LINUX dataset.

as 64, 32, 16 for three GNN layers. Adam optimizer [21] is
used with 0.001 learning rate and 5 × 10−5 weight decay.
We set batch size=128 for LINUX and AIDS, and 16 for
Willow. All experiments are run on our workstation with
Intel i7-7820X@3.60GHz and 64GB memory. Paralleliza-
tion techniques e.g. multi-threading and GPU parallelism
are not considered in our experiment.

4.2. Peer Methods

Hungarian-A* [32] is selected as the exact solver base-
line, where Hungarian bipartite matching is used to predict
h(p). We reimplement Hungarian-A* based on our Cython
implementation for fair comparison. We also select Hun-
garian solver [31] as the traditional inexact solver baseline
in our experiments. It is worth noting that Hungarian bipar-
tite matching can be either adopted as heuristic in A* algo-
rithm (Hungarian heuristic for A*), or to provide a fast sub-
optimal solution to GED (Hungarian solver), and readers
should distinguish between these two methods. Other inex-
act solvers are also considered including Beam search [20]
which is the greedy version of A* and VJ [13] which is an
variant from Hungarian solver.

For regression-based deep graph similarity learning
methods, we compare SimGNN [3], GMN [26] and
GraphSim [4]. Our GENN backbone can be viewed as a
simpliﬁed version from these methods, because the time ef-

7

Figure 7. The scatter plots of our proposed GENN-A* (red), inexact Hungarian solver [31] (blue, upper bound), our GENN network (cyan)
and Hungarian heuristic for A* [32] (yellow, lower bound) on AIDS, LINUX and Willow-Cars datasets. The left two columns are GED
solvers and the right two columns are methods used to predict h(p) in A* algorithm. Every dot is plotted with optimal GED value on x-axis
and the solved (or predicted) GED value on y-axis. Optimal black dots are plotted as references. Our GENN-A* (red) achieves tighter
upper bounds than inexact Hungarian solver [31] (blue), where a signiﬁcant amount of problems are solved to optimal. Our regression
model GENN (cyan) also predicts more accurate h(p) than Hungarian heuristic [32] (yellow), resulting in reduced search tree size of
GENN-A* compared to Hungarian-A*.

ﬁciency with dynamic graphs is our main concern.

4.3. Results and Discussions

The evaluation of AIDS, LINUX, and Willow-Cars
dataset in line with [4] is presented in Tab. 1, where the
problem is deﬁned as querying a graph in the test dataset
from all graphs in the training set. The similarity score is
deﬁned as Eq. 7. Our regression model GENN has compa-
rable performance against state-of-the-art with a simpliﬁed
pipeline, and our GENN-A* best performs among all inex-
act GED solvers. We would like to point out that mse may
not be a fair measurement when comparing GED solvers
with regression-based models: Firstly, GED solvers can
predict edit paths while such a feature is not supported
by regression-based models. Secondly, the solutions of
GED solvers are upper bounds of the optimal values, but

regression-based graph similarity models [3, 4, 26] predicts
GED values on both sides of the optimums. Actually, one
can reduce the mse of GED solvers by adding a bias to the
predicted GED values, which is exactly what the regression
models are doing.

The number of states which have been added to OPEN
in Alg. 1 is plotted in Fig. 5, where our GENN-A* signiﬁ-
cantly reduces the search tree size compared to Hungarian-
A*. Such search-tree reduction results in the speed-up of
A* algorithm, as shown in Tab. 2. Both evidences show
that our GENN learns stronger h(p) than Hungarian heuris-
tic [32] whereby redundant explorations on suboptimal so-
lutions are pruned. We further compare the inference time
of three discussed dynamic graph embedding method in
Tab. 3, where our GENN runs comparatively fast against
Hungarian heuristic, despite the overhead of calling Py-

8

Torch functions from Cython. Exact Dynamic GNN is even
slower than the vanilla version, since its frequent caching
and loading operations may consume additional time. It is
worth noting that further speedup can be achieved by im-
plementing all algorithms in C++ and adopting parallelism
techniques, but these may be beyond the scope of this paper.
In Fig. 7 we show the scatter plot of GENN-A* and
inexact Hungarian solver [31] as GED solvers, as well as
GENN and Hungarian heuristic as the prediction methods
on h(p). Our GENN-A* beneﬁts from the more accurate
prediction of h(p) by GENN, solving the majority of prob-
lem instances to optimal. We also visualize a query example
on Willow-Car images in Fig. 6 done by our GENN-A*.

5. Conclusion

This paper has presented a hybrid approach for solv-
ing the classic graph edit distance (GED) problem by inte-
grating a dynamic graph embedding network for similarity
score prediction into the edit path search procedure. Our
approach inherits the good interpretability of classic GED
solvers as it can recover the explicit edit path between two
graphs while it achieves better cost-efﬁciency by replacing
the manual heuristics with the fast embedding module. Our
learning-based A* algorithm can reduce the search tree size
and save running time, at the cost of little accuracy lost.

Acknowledgments

This research was supported by China Major State Re-
search Development Program (2020AAA0107600), NSFC
(61972250, U19B2035).

References

[1] Zeina Abu-Aisheh, Benoit Ga¨uz`ere, S´ebastien Bougleux,
Jean-Yves Ramel, Luc Brun, Romain Raveaux, Pierre
H´eroux, and S´ebastien Adam. Graph edit distance contest:
Results and future challenges. Pattern Recognition Letters,
100:96–103, 2017. 2

[2] Zeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel, and
Patrick Martineau. An exact graph edit distance algorithm
In 4th Interna-
for solving pattern recognition problems.
tional Conference on Pattern Recognition Applications and
Methods, 2015. 1, 2

[3] Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou
Sun, and Wei Wang. Simgnn: A neural network approach
to fast graph similarity computation. In Proceedings of the
Twelfth ACM International Conference on Web Search and
Data Mining, pages 384–392, 2019. 1, 2, 3, 4, 5, 6, 7, 8
[4] Yunsheng Bai, Hao Ding, Ken Gu, Yizhou Sun, and Wei
Wang. Learning-based efﬁcient graph similarity computa-
In AAAI,
tion via multi-scale convolutional set matching.
pages 3219–3226, 2020. 1, 2, 3, 5, 7, 8

[5] S. Behnel, R. Bradshaw, C. Citro, L. Dalcin, D. S. Seljebotn,
and K. Smith. Cython: The best of both worlds. Computing
in Science Engineering, 13(2):31–39, 2011. 7

[6] Ralph Bergmann and Yolanda Gil. Similarity assessment and
efﬁcient retrieval of semantic workﬂows. Information Sys-
tems, 40:115–127, 2014. 2

[7] S´ebastien Bougleux, Benoit Ga¨uzere, and Luc Brun. Graph
In 2016 23rd Inter-
edit distance as a quadratic program.
national Conference on Pattern Recognition (ICPR), pages
1701–1706. IEEE, 2016. 2

[8] Lijun Chang, Xing Feng, Xuemin Lin, Lu Qin, Wenjie
Zhang, and Dian Ouyang. Speeding up ged veriﬁcation for
graph similarity search. In Proc. of ICDE’20, 2020. 1, 3
[9] Lichang Chen, Guosheng Lin, Shijie Wang, and Qingyao
Wu. Graph edit distance reward: Learning to edit scene
graph. European Conference on Computer Vision, 2020. 1

[10] Minsu Cho, Karteek Alahari, and Jean Ponce. Learning

graphs to match. In ICCV, 2013. 1, 6, 7

[11] Minsu Cho, Jungmin Lee, and Kyoung Mu Lee. Reweighted
In Eur. Conf. Comput.

random walks for graph matching.
Vis., 2010. 2

[12] Xinyan Dai, Xiao Yan, Kaiwen Zhou, Yuxuan Wang, Han
Yang, and James Cheng. Edit distance embedding using
In International ACM SI-
convolutional neural networks.
GIR Conference on Research and Development in Informa-
tion Retrieval, 2020. 2

[13] Stefan Fankhauser, Kaspar Riesen, and Horst Bunke. Speed-
ing up graph edit distance computation through fast bi-
In International Workshop on Graph-
partite matching.
Based Representations in Pattern Recognition, pages 102–
111. Springer, 2011. 1, 2, 7

[14] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Hein-
rich M¨uller. Splinecnn: Fast geometric deep learning with
In Proceedings of the IEEE
continuous b-spline kernels.
Conference on Computer Vision and Pattern Recognition,
pages 869–877, 2018. 2, 4, 7

[15] Matthias Fey and Jan E. Lenssen. Fast graph representa-
tion learning with PyTorch Geometric. In ICLR Workshop on
Representation Learning on Graphs and Manifolds, 2019. 7
[16] Matthias Fey, Jan E Lenssen, Christopher Morris, Jonathan
Masci, and Nils M Kriege. Deep graph matching consensus.
In Int. Conf. Learn. Represent., 2020. 2, 4

[17] Bo Jiang, Pengfei Sun, Jin Tang, and Bin Luo. Glm-
net: Graph learning-matching networks for feature match-
ing. arXiv preprint arXiv:1911.07681, 2019. 2

[18] Bo Jiang, Jin Tang, Chris Ding, Yihong Gong, and Bin
Luo. Graph matching via multiplicative update algorithm. In
Advances in Neural Information Processing Systems, pages
3187–3195, 2017. 2

[19] Roy Jonker and Anton Volgenant. A shortest augmenting
path algorithm for dense and sparse linear assignment prob-
lems. Computing, 38(4):325–340, 1987. 2

[20] Riesen Kaspar.

https :
/ / github . com / dzambon / graph - matching -
toolkit, 2017. 1, 2, 3, 7

Graph matching toolkit.

[21] Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. International Conference on Learn-
ing Representations, 12 2014. 7

[22] Thomas N Kipf and Max Welling. Semi-supervised classiﬁ-
cation with graph convolutional networks. Int. Conf. Learn.
Represent., 2017. 1, 2, 4, 7

9

Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017. 2,
5

[37] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learning
combinatorial embedding networks for deep graph matching.
In Int. Conf. Comput. Vis., 2019. 2

[38] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Neural
graph matching network: Learning lawler’s quadratic assign-
ment problem with extension to hypergraph and multiple-
graph matching. arXiv preprint arXiv:1911.11308, 2019. 2
[39] Tao Wang, Haibin Ling, Congyan Lang, and Songhe Feng.
Graph matching with adaptive and branching path following.
IEEE Trans. Pattern Anal. Mach. Intell., 2017. 2

[40] Xiaoli Wang, Xiaofeng Ding, Anthony K. H. Tung, Shan-
shan Ying, and Hai Jin. An efﬁcient graph indexing method.
In Proceedings of the 2012 IEEE 28th International Confer-
ence on Data Engineering, ICDE ’12, page 210–221, USA,
2012. IEEE Computer Society. 6

[41] Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng,
Hongyuan Zha, and Xiaokang Yang. A short survey of recent
advances in graph matching. In ICMR, 2016. 2

[42] Tianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li.
Learning deep graph matching with channel-independent
embedding and hungarian attention. In Int. Conf. Learn. Rep-
resent., 2020. 2

[43] Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, and Baoxin
Li. Generalizing graph matching beyond quadratic assign-
ment model. In Advances in Neural Information Processing
Systems 31, pages 853–863, 2018. 2

[44] Andrei Zanﬁr and Cristian Sminchisescu. Deep learning of
graph matching. In IEEE Conf. Comput. Vis. Pattern Recog.,
2018. 2

[45] Zhiping Zeng, Anthony KH Tung, Jianyong Wang, Jianhua
Feng, and Lizhu Zhou. Comparing stars: On approximating
graph edit distance. Proceedings of the VLDB Endowment,
2(1):25–36, 2009. 1, 2

[46] Christian Zeyen and Ralph Bergmann. A*-based similarity
assessment of semantic graphs. In International Conference
on Case-Based Reasoning, pages 17–32. Springer, 2020. 1

[47] Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao. Add-
graph: Anomaly detection in dynamic graph using attention-
based temporal gcn. In Proceedings of the Twenty-Eighth In-
ternational Joint Conference on Artiﬁcial Intelligence, pages
4419–4425, 2019. 2, 4

[48] Feng Zhou and Fernando De la Torre. Factorized graph
matching. In IEEE Conf. Comput. Vis. Pattern Recog., 2012.
2

[49] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang,
Zhiyuan Liu, and Maosong Sun. Graph neural networks:
A review of methods and applications. arXiv:1812.08434,
2018. 2

[23] Harold W. Kuhn. The hungarian method for the assignment
In Export. Naval Research Logistics Quarterly,

problem.
pages 83–97, 1955. 2

[24] Marius Leordeanu, Martial Hebert, and Rahul Sukthankar.
An integer projected ﬁxed point method for graph matching
In Adv. Neural Inform. Process. Syst.,
and map inference.
2009. 2

[25] Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre
H´eroux, and S´ebastien Adam. New binary linear program-
ming formulation to compute the graph edit distance. Pattern
Recognition, 72:254–265, 2017. 2

[26] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and
Pushmeet Kohli. Graph matching networks for learning the
similarity of graph structured objects. In International Con-
ference on Machine Learning, pages 3835–3845, 2019. 1, 2,
3, 7, 8

[27] Paul Maergner, Vinaychandran Pondenkandath, Michele Al-
berti, Marcus Liwicki, Kaspar Riesen, Rolf Ingold, and An-
dreas Fischer. Combining graph edit distance and triplet net-
works for ofﬂine signature veriﬁcation. Pattern Recognition
Letters, 125:527–533, 2019. 1

[28] Franco Manessi, Alessandro Rozza, and Mario Manzo. Dy-
namic graph convolutional networks. Pattern Recognition,
97:107000, 2020. 2, 4

[29] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma,
Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao B.
Schardl, and Charles E. Leiserson. Evolvegcn: Evolving
graph convolutional networks for dynamic graphs. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
34(4):5363–5370, 2020. 2, 4

[30] Kaspar Riesen and Horst Bunke. Iam graph database repos-
itory for graph based pattern recognition and machine learn-
In Joint IAPR International Workshops on Statisti-
ing.
cal Techniques in Pattern Recognition (SPR) and Structural
and Syntactic Pattern Recognition (SSPR), pages 287–297.
Springer, 2008. 1

[31] Kaspar Riesen and Horst Bunke. Approximate graph edit
distance computation by means of bipartite graph matching.
Image and Vision Computing, 27(7):950–959, 2009. 1, 2, 3,
7, 8, 9

[32] Kaspar Riesen, Stefan Fankhauser, and Horst Bunke. Speed-
ing up graph edit distance computation with a bipartite
heuristic. In Mining and Learning with Graphs, pages 21–
24, 2007. 1, 2, 3, 5, 7, 8

[33] Michal Rol´ınek, Paul Swoboda, Dominik Zietlow, Anselm
Paulus, V´ıt Musil, and Georg Martius. Deep graph matching
via blackbox differentiation of combinatorial solvers. In Eur.
Conf. Comput. Vis., 2020. 2, 4

[34] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-
genbuchner, and Gabriele Monfardini. The graph neural net-
work model. Trans. on Neural Networks, 2009. 1, 2

[35] Richard Socher, Danqi Chen, Christopher D. Manning, and
Andrew Y. Ng. Reasoning with neural tensor networks for
knowledge base completion. In Adv. Neural Inform. Process.
Syst., NIPS’13, page 926–934, Red Hook, NY, USA, 2013.
Curran Associates Inc. 5

[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia

10

