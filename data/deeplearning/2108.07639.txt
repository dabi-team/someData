1
2
0
2

g
u
A
7
1

]
I

A
.
s
c
[

1
v
9
3
6
7
0
.
8
0
1
2
:
v
i
X
r
a

LEARNING C TO X86 TRANSLATION: AN EXPERIMENT IN
NEURAL COMPILATION

PREPRINT

Jordi Armengol-Estap´e & Michael F.P. O’Boyle
School of Informatics
University of Edinburgh
jordi.armengol.estape@ed.ac.uk, mob@inf.ed.ac.uk

August 18, 2021

ABSTRACT

Deep learning has had a signiﬁcant impact on many ﬁelds. Recently, code-to-code neural models
have been used in code translation, code reﬁnement and decompilation. However, the question
of whether these models can automate compilation has yet to be investigated. In this work, we
explore neural compilation, building and evaluating Transformer models that learn how to produce
x86 assembler from C code. Although preliminary results are relatively weak, we make our data,
models and code publicly available to encourage further research in this area.

1 Introduction

Machine learning based compilation has been explored for over a decade [1]. Early work focused on learning prof-
itability heuristics while more recently, deep learning models have been used to build code-to-code models, for trans-
lating or decompiling code. However, to the best of our knowledge, there has been no prior work on using machine
learning to entirely automate compilation i.e given a high level source code program generate the equivalent assembler
code.

In this paper, we investigate whether it is possible to learn an end-to-end machine compiler using neural machine
translation.
In particular, we focus on the translation of small C functions to x86 assembler We use an existing
function-level C corpus, Anghabench [2], to build a parallel C-x86 assembler corpus. Then, we model the compilation
task as a sequence-to-sequence task (akin to machine translation) with the Transformer architecture [3]. We study
the effect of modifying different settings by varying training data size, model size, number of epochs, and other
hyperparameters. While we can successfully generate syntactically correct assembler over 80% of the time and obtain
high BLEU scores c. 90%, generating semantically correct assembler is more challenging
The best model can only compile correctly1 about 33% of the functions in a benchmark built from an existing program
synthesis evaluation set [4]; it specially struggles to compile functions with numerous arguments and arrays. Given
the complexity of the problem, we make all the resources and code generated in this work publicly available.

The article is structured as follows. In Section 2, we brieﬂy summarise related work in NLP and machine learning
for code. In Section 3, we formalize the task of machine compilation and propose how to effectively build neural
compilers and fairly evaluate them. Then, in Section 4 we establish our experimental framework and report results.
Finally, we discuss our approach and conclude, in sections 5 and 6.

2 Related Work

Natural Language Processing and Machine Translation In Natural Language Processing (NLP), the current state-
of-the-art typically involves using some variant the Transformer architecture [3] together with some form of subword

1See Section 3.2 to see how we evaluate correctness.

 
 
 
 
 
 
Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

tokenization [5]. Apart from the task itself, the literature encourages also leveraging unlabelled text to pretrain with-
out supervision, as in BERT [6], a Transformer encoder, and then apply transfer learning. In the case of machine
translation, NLP practitioners use the full Transformer (encoder-decoder), potentially starting from pretrained weights
as well. Similarly, abstractive summarization is another NLP task that is also posed as a sequence-to-sequence task
and can beneﬁt from pretraining [7], with the peculiarity of having source sequence considerably longer than target
sequences.

Deep Learning for Code and Symbolic Data Recent works have proposed to use the encoder-decoder Transformer
architecture out of the box for symbolic mathematics [8, 9], or even for automated symbolic proving with decoder-
only Transformers [10]. The state-of-art NLP systems for unsupervised pretraining have also been with successfully
applied to code, as in CodeBERT [11]. However, other research lines explore the use of alternative modeling strategies
for code instead of ﬂat sequences, such as trees to leverage the grammar [12] or other kinds of graphs for data ﬂow
analysis [13].

Machine Learning for Compilers Many works have proposed the use of machine learning for improving code
optimization [14] and the ﬁeld is gaining momentum with recent works such as the CompilerGym [15], a reinforcement
learning environment for compilers optimization. However, the common approach is to use machine learning for
decision-making, not to directly generate assembler with a machine learning decoder.

Code Translation and Code-to-Code models Code-to-code models have been applied in tasks such as 1. program-
ming language translation [16], even in unsupervised settings [17], 2. code reﬁnement [18], or 3. decompilation [19].
Remarkably, the latter would be roughly the inverse task of the one we are posing, as in Katz et al. [20], where authors
built a system to predict C code from binary programs. Note that machine compilation would also be a code-to-code
task.

To the best of our knowledge, no other previous machine learning work has addressed the task of machine compila-
tion, the one we pose in this work. One speciﬁc challenge we note, apart from the usual ones in sequence modeling
and, especially, source code modeling, is that in our case the target sequences are considerably longer (being assem-
bler) instead of a similar size (as usual in machine translation) or considerably shorter (as usual in summarization or
decompilation).

3 Methods

We pose machine compilation as a sequence-to-sequence task. Akin to machine translation, machine compilation is
the task of translating code into assembler language. More formally, given a dataset D with N pairs (xi, yi), where xi
is an input program and yi is the corresponding assembler code, the system is trained with max likelihood estimation:

ℓ(θ, D) = X

ln p(yi|xi, θ)

(xi,yi∈D)

Posing the task as a sequence-to-sequence task conditions both the data generation and the model building processes.

3.1 Training data

Regarding the granularity, as a ﬁrst approach we decided to consider functions, following [17]. Functions, unlike
statements, are standalone units of meaning that can be translated, but at the same they are shorter and easier to test
(unit tests) than a whole program (integration tests).

Since we investigate a supervised setting, we need pairs (C functions, x86 assembler). However, C functions cannot be
directly compiled; they typically need additional context (inclusion of headers, type deﬁnitions, constant deﬁnitions).
Thus, even if we have pre-exiting C compilers, generating these data pairs is not trivial.

For this work, we base our dataset on Anghabench [2], a benchmark of around 1 million C functions coupled with
the required minimal C code to compile them. Anghabench is built by crawling C code from Github2 repositories.
The authors extracted individual functions, and applied type-inference to reconstruct the missing deﬁnitions required
to compile them (e.g., declarations of auxiliary functions, type deﬁnitions, etc). However, while these reconstructions
makes the functions compilable, they are not executable. Apart from not necessarily having a main function and
input/output calls, the declared auxiliary functions are not deﬁned. This, among other issues, prevents execution.

2https://github.com

2

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

DATASET
Angha-Par500k
Angha-Par250k
Synthesis-Bench

Filter
# ORIG PROGRAMS
1.044M
Max. length
1.044M Max. length + random
Manual (difﬁculty)

112

# FILT. PROGRAMS
500k
250k
64

IO EXAMPLES
✗
✗
✓

Table 1: Used datasets, original number of programs, ﬁlter criteria, number of kept programs after ﬁltering, and
whether they have input/output examples (which only Synthesis-Bench does). The AnghaPar corpus was ﬁltered with
a maximum combined (C + assembler) length of 314 tokens. The 250k subset was further subsampled randomly.
Finally, the Synthesis-Bench was built from a manual selection of 64 functions from the original benchmark, based on
implementation difﬁculty.

SPLIT
Angha-Par500k Train
Angha-Par250k Train
Angha-Par Valid
Angha-Par Test

PROGRAMS
500,439
250,000
1,000
1,000

TOKENS C (AVG)
22,653,480 (45.27)
11,281,616 (45.12)
45,737 (45.74)
44,643 (44.64)

TOKENS ASM (AVG)
65,910,582 (131.71)
32,992,914 (131.97)
132,424 (132.42)
132,446 (132.37)

Table 2: Dataset splits. assembler code has almost 3x tokens than its corresponding C code.

It is not practical to directly use this dataset for neural compilation. The inclusion of headers and type deﬁnitions
while necessary for compilation, adds noise to the translation task Insterad, we follow the best practices from the NLP
literature, and apply the following pre-processing steps.

Our preprocessing pipeline has the following steps:

1. Compilation: We use the GCC compiler to compile the C code into x86 assembler. We do not apply any

optimizations (-O0).

2. Boilerplate removal: We remove the headers and type and constant deﬁnitions. Likewise, we remove the
header and footer of the assembler. In both cases, we believe those inject noise and make sequences longer
than need be.

3. Pre-tokenization: We use the GCC C and x86 assembler (GAS) tokenizers with the Pygments3 library. In C,
new lines are meaningless and just used to make code more human readable, but in GAS end of lines delimits
the end of each instruction. Thus, in the latter we replace end of lines by a special token <newline>.

4. Length ﬁltering: Due to computational restrictions and potentially easing the task, we discard the (C, assem-
bler) pairs such that when summing the length of tokens of the C code and assembler we get more than 314
tokens.

5. Train-valid-test split: We randomly split the pairs into training, validation, and test sets, with 2k programs for

validation and test and the rest for training.

6. Subword tokenization: We use subword encoding to automatically split tokens into further tokens based on
n-gram frequencies in the train set. Speciﬁcally, subword-nmt [5] .4 This has the beneﬁt of decreasing the
vocabulary size while making out-of-vocabulary tokens virtually impossible (since unknown tokens can be
reconstructed from ASCII characters or other subwords present in the vocabulary).

7. Formatting: We write each C and assembler programs in plain text ﬁles, such that we have one program for

each line.

See Appendix A for some data samples (together with model outputs) after the preprocessing. After ﬁltering for
length, we kept as many as 500k programs (Angha-Par500k) and a subset (250k) of those for an ablation study
(Angha-Par250k). Tables 1 and 2 shows the statistics of the cleaning process and train, validation and test splits,
respectively. Table 3 shows the subwords per token of the different trained vocabularies.

3https://pygments.org/
4https://github.com/rsennrich/subword-nmt

3

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

VOCAB
4k
8k
16k

SUBWORDS/TOKEN C (AVG LENGTH)
1.55 (69.85)
1.42 (64.22)
1.33 (59.96)

SUBWORDS/TOKEN ASM (AVG LENGTH) COVERAGE

1.14 (149.85)
1.10 (144.99)
1.08 (143.12)

100%
100%
100%

Table 3: Subwords per token. All vocabularies have a coverage of 100% (i.e., no unknowns) since they include
all ASCII characters. C code length is more sensitive to the vocabulary size, since it has a larger vocabulary (e.g.,
identiﬁers, except procedure names, are translated as memory positions or registers). There is a clear trade-off between
sequence length and vocabulary size.

3.2 Evaluation

Machine translation is usually evaluated with BLEU score [21], based on n-gram overlaps between the generated
sequence and the ground truth one. This metric does not take into account syntactical or semantic correctness. We
use GCC to check if the assembler generated is syntactically correct, by asking it to generate object code from the
assembler.

We evaluate semantic correctness using observational equivalence between the reference GCC assembler and the one
output by the model, following recent works on program translation [17]. That is, we check whether for a given set of
inputs, the assembler predicted by the models have the same output as the reference GCC compilation (in other words,
we evaluated whether the assembler functions generated by the models pass the available unit tests). While this is no
proof that the two programs are formally equivalent, in practice it is a high indicator that it is.

Anghabench programs, however, are not executable. Thus, we cannot use them to test for observational equivalence.
For this work, we base our dataset on a subset5 of 64 functions extracted from the program synthesis benchmark
collated in Collie et al. [4].6 We then add a main function with the required input/output calls to execute them with
randomly generated input/output pairs (referred as IO examples, from now on). In Table 1, we show the size of this
benchmark, referred from now on as Synthesis-Bench, compared with Angha-Par.

We show the syntactic accuracy i.e., how many output assembler programs are syntactically correct and the BLEU
score with respect to the GCC assembler, as a reference. We do so for both Synthesis-Bench and Angha-Par’s test, but
the latter does not have IO annotations and thus cannot be evaluated in terms of observational equivalence.

3.3 Model

Following previous work on machine translation and deep learning for symbolic mathematics and source code model-
ing, we use a Transformer model (encoder-decoder) in different settings.

We implement all models with Fairseq [22], a PyTorch [23] sequence modeling library.

As usual in sequence-to-sequence models, we train with teacher forcing and use a special token to denote the end of
the sequence, which is also predicted by the model.

4 Experiments

In this section, we describe the conducted experiments and their corresponding settings.

4.1 Experimental framework

We experiment with the following models:

• Transformer-Small: The small model follows the transformer iwslt de en conﬁguration in Fairseq, that

is, 6 encoder layers and 6 decoder layers, an embedding size of 512 and 4 attention heads.

• Transformer-Big (base): The big model follows the transformer wmt en de big t2t conﬁguration in

Fairseq, with 6 encoder layers and 6 decoder layers, an embedding size of 1024 and 16 attention heads

– -50% data: Transformer-Big trained with Angha-Par250k instead of Angha-Par500k.
– -1/2x vocab: Transformer-Big trained with a vocabulary of 4k tokens (instead of 8k tokens).

5Arbitrarily selected based on difﬁculty of implementation.
6https://github.com/mob-group/synthesis-eval

4

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

– +1/2x vocab: Transformer-Big trained with a vocabulary of 16k tokens (instead of 8k tokens).
– +1e2x weight-decay: Transformer-Big further regularized (a weight decay of 0.01 instead of 0.0001)
– +1/2 epochs: Transformer-Big trained for a total of 10 epochs (instead of 5).

• Transformer-Med: The medium-size model roughly follows the Transformer-Big conﬁguration, but with 8

attention heads (instead of 16) and a Feed-Forward hidden size of 2048 (instead of 4096).

• Transformer-Big+: This model has the same conﬁguration as Transformer-Big, but with 2 additional layers

for both the encoder and the decoder.

Table 4 shows the total parameter count for each model. We train all models with the same data (except from the
ones that have a different vocabulary, which use a different tokenizer, and the one that uses half of data) and the same
number of epochs, 5 (except for the model additionally trained for 5 more epochs).

Regarding other hyperparameters, all models are trained with the Adam optimizer [24]. We refer to Fairseq and our
source code for additional details. We do not conduct any hyperparameter search, aside from the different conﬁgura-
tions reported in Table 4.

We then evaluate the models in terms of BLEU (against the GCC reference) and syntactic accuracy in the Angha-Par
test, and do the same, plus observational equivalence (accuracy in IO Examples), in Synthesis-Bench. In inference,
we use beam search with k = 5 and select the best hypothesis (among the top 5) in terms of the evaluated metric.

4.2 Results

Table 4 shows the results summary of each model, together with their respective size. The best model, as per the most
relevant metric (IO evaluation, that is, observational equivalence) is the Transformer-Big trained for 10 epochs.

We report the ﬁne-grained IO evaluation for the best model in Table 10, together with other metrics to ease the analysis
of the results. Speciﬁcally, apart from the aforementioned syntactic accuracy and BLEU scores, we also report:

• LOC: Lines of Code, the number of lines of the C implementation.

• Tokens: The number of tokens of the C implementation.

• Cyclo: The cyclomatic complexity:

Cyclomatic complexity = E − N + 2 × P

where E is the number of edges in the ﬂow graph, N is the number of nodes in the ﬂow graph, and P is the
number of nodes that have exit points.

• Params: The number of parameters of the C function.

• Pointers: The number of pointer parameters (typically arrays) of the C function.

Finally, Tables 5, 6, 7, 8 show the correlations between IO errors and other metrics, the mean output length of each
model, the most frequent syntactical errors, and the most frequent IO errors, respectively. We will refer to those in the
error analysis in Section 5.

As supplementary material, we include output samples in Appendix A.

4.3 Code and Data Availability

We plan to release7 the code and data used in this work with an open license.

5 Discussion

Results Transformer-Big+1/2x epochs is the best model in terms of the most relevant metric, IO accuracy (obser-
vational equivalence). It also happens to be the best model in terms of syntactical accuracy in Synthesis-Bench and
BLEU score in the Angha-Par test. The smallest model clearly underﬁts the task of machine compilation, while all
reasonably sized models achieve similar enough results (except the model trained with half of the data, which performs
considerably worse).

7At https://github.com/jordiae/neural-compilers

5

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

MODEL
Transformer-Small
Transformer-Med
Transformer-Big
- 50% data
- 1/2x vocab
+ 1/2x vocab
+ 1e2x weight-decay
+ 1/2x epochs
Transformer-Big+

PARAMS
30.9M
142.7M
193.1M
193.1M
184.7M
209.8M
193.1M
193.1M
251.9M

SYNTHESIS-BENCH
IO EXAMPLES
0/64
18/64
19/64
13/64
19/64
20/64
18/64
21/64
19/64

SYNTAX BLEU
32.68
77.99
78.03
76.81
78.07
79.48
77.73
78.10
78.19

0/64
35/64
37/64
34/64
36/64
36/64
34/64
37/64
34/64

ANGHA-PAR

SYNTAX BLEU
47.53
89.52
89.20
83.80
88.63
89.21
89.55
90.19
89.76

98.50
81.60
82.70
88.16
75.60
79.30
82.00
82.50
82.50

Table 4: Results summary. For each model (namely, the small Transformer variant, the medium-size Transformer,
the bigger Transformer variant, the latter plus varying training data size, vocabulary size, with additional weight
decay regularization, and with additional training iterations, and an even bigger Transformer variant) we show the
total parameter count and report their results in Synthesis-Bench and the Angha-Par test set. Speciﬁcally, we report
the correct IO examples in Synthesis-Bench, and the syntactic accuracy and BLEU score in Synthesis-Bench and the
Angha-Par test. The syntactic accuracy is reported as a fraction for Synthesis-Bench and as a percentage for Angha-
Par (due to having a considerably larger number of instances). In bold, the best results for each metric and dataset, and
the best model (Transformer-Big + 1/2 epochs) as per the most relevant variant, correct IO examples.

METRIC CORRELATION (P-VALUE)
Syntax
BLEU
LOC
Tokens
Cyclo
Params
Pointers

0.597 (1.92E-07)
0.536 (4.96E-06)
0.174 (1.69E-01)
-0.269 (3.13E-02)
-0.106 (4.04E-01)
-0.607 (1.04E-07)
-0.573 (7.56E-07)

Table 5: Pearson correlations between different metrics (syntactical accuracy, BLEU score, lines of code and number
of tokens in the C implementation, cyclomatic complexity of the C implementation, number of parameters in the C
function, and number of pointer parameters in the C function) and IO accuracy. Bold values are statistically signiﬁcant.

On the surprisingly high syntax accuracy of the small model The smallest model variant, Transformer-Small,
while generally performing badly, obtains a surprisingly high syntactical accuracy in Angha-Par, as shown in Table 4.
This result is even more surprising if we take into account that their outputs are abnormally long (see Table 6. After
an inspection of the results, we can afﬁrm that the outputs do not correlate with the inputs (even function names are
different). Instead, the model behaves as a sort of unconditional assembler language model. Furthermore, we observe
repeated outputs with different inputs. This phenomenon is reminiscent of the hallucinations described in the literature
of other Sequence-to-Sequence models [25] and the mode collapse of some generative models [26].

MODEL
Transformer-Small
Transformer-Med
Transformer-Big
- 50% data
- 1/2x vocab
+ 1/2x vocab
+ 1e2x weight-decay
+ 1/2x epochs
Transformer-Big+
Ground truth

AVG OUTPUT LENGTH
162.29
124.94
124.61
125.00
127.22
124.13
124.74
124.59
124.99
132.37

Table 6: Average length of the output of the different models in the Angha-Par test, vs. the ground truth (GCC) one.

6

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

ERROR
open CFI at the end of file; missing .cfi endproc directive
expecting operand after ’,’; got nothing
unbalanced brackets in operand 1.
number of operands mismatch for ‘mov’
number of operands mismatch for ‘add’
unbalanced brackets in operand 2.
bad or irreducible absolute expression
CFI instruction used without previous .cfi startproc
junk at end of line, first unrecognised character is ‘%’
symbol ‘.L3’ is already defined
number of operands mismatch for ‘cmp’
symbol ‘.L5’ is already defined
number of operands mismatch for ‘movq’
number of operands mismatch for ‘lea’
.cfi endproc without corresponding .cfi startproc
symbol ‘.L4’ is already defined
operand type mismatch for ‘sar’
number of operands mismatch for ‘pop’
number of operands mismatch for ‘sal’
number of operands mismatch for ‘pxor’
number of operands mismatch for ‘movslq’
.size expression for sum n does not evaluate to a constant

Table 7: Frequent syntactical errors (sorted by frequency).

ERROR
Syntax error
Compiled but 0 tests passed
Compiled but only 1 test passed
Compiled but more than 1 test passed

27
15
1
0

Table 8: IO error typology for the best model.

MODEL
Transformer-Small
Transformer-Med
Transformer-Big
- 50% data
- 1/2x vocab
+ 1/2x vocab
+ 1e2x weight-decay
+ 1/2x epochs
Transformer-Big+

INTERSECTIONS
0/0
18/19
19/19
13/13
19/19
20/20
18/18
21/21
19/19

Table 9: Intersections between the correct outputs of each model and the correct outputs of the best one (Transformer-
Big + 1/2x epochs).

7

FUNCTION
add
array inc
array prod
array sum
binary digits
binary mul sum
clamp
collatz
count odds
cube in place
digit prod
digits
diveq
diveq sca
dot
elementwise sum of

negated sum and max

eq
fact
fact fact
fib n
fourth in place
int sqrt
last elem
last zero idx
length
max
max elt
min
min elt
min so far subtracted
mirror image
muleq
muleq sca
negate
pluseq
prod elts
prod n squared
prod sq elts
replace first
replace last
reverse
reverse int
search
sort
subeq
subeq sca
subtract of min reverse
sum abs
sum elts
sum n
sum n squared
sum of lists multiplied

after dividing by three

sum of positives
sum of squares
triangle prod
triangle sum
vadd
vcopy
vfill
vmul
vneg
voffset
vscal
vsub

IO SYNTAX
✓
✓
✓
✓
✓
✗
✓
✓
✓
✗
✗
✓
✓
✓
✓
✗

✗
✗
✓
✓
✓
✗
✗
✓
✓
✗
✗
✓
✗
✗
✗
✗

BLEU LOC
6
5
7
7
8
8
7
12
9
5
9
8
5
5
7
15

85.23
87.6
97.96
97.8
97.27
50.59
96.77
98.2
87.58
65.55
59.63
82.32
75.31
82.79
97.5
3.25

TOKENS
39
34
42
42
31
66
45
54
52
47
38
31
41
37
51
122

CYCLO PARAMS
3
2
2
2
1
3
2
1
2
2
1
1
3
3
3
4

2
2
2
2
2
2
3
3
3
2
2
2
2
2
2
4

POINTERS
1
1
1
1
0
2
1
0
1
1
0
0
2
1
2
3

✗
✓
✓
✓
✗
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
✓
✓
✓
✗

✗
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗

✗
✓
✓
✓
✗
✓
✓
✓
✗
✗
✗
✗
✗
✓
✗
✓
✓
✓
✗
✓
✓
✗
✗
✗
✗
✗
✓
✗
✗
✓
✗
✗
✓
✓
✓
✗

✗
✓
✓
✓
✗
✓
✓
✗
✓
✓
✓
✗

81.47
96.94
96.94
97.42
45.37
86.34
97.8
98.04
41.35
79.59
87.36
80.04
88.04
0.0
77.16
73.78
85.0
87.71
76.4
97.96
97.66
85.46
79.77
79.89
55.01
61.04
95.23
33.63
74.26
89.73
46.71
59.81
97.8
96.74
92.65
18.44

44.02
98.27
97.95
97.86
73.97
84.61
96.03
72.34
87.71
85.23
85.0
71.26

9
8
8
10
6
9
7
9
1
11
9
11
9
18
9
5
5
5
5
7
8
8
9
9
7
9
9
9
5
5
8
7
7
8
8
13

10
7
9
9
5
5
5
5
5
5
5
5

57
31
31
46
57
43
42
50
14
63
53
63
53
157
61
41
37
38
41
42
39
49
62
62
62
37
59
84
41
37
82
57
42
30
32
105

91
47
51
51
50
41
37
50
38
37
37
50

3
2
2
2
2
2
2
3
1
3
3
3
3
6
3
2
2
2
2
2
2
2
3
3
2
2
4
4
2
2
3
3
2
2
2
4

4
2
3
3
2
2
2
2
2
2
2
2

3
1
1
1
2
1
2
2
2
2
2
2
2
4
3
3
3
2
3
2
1
2
2
2
2
1
3
2
3
3
4
2
2
1
1
3

4
2
1
1
4
3
3
4
2
3
3
4

2
0
0
0
1
0
1
1
1
1
1
1
1
3
2
2
1
1
2
1
0
1
1
1
1
0
1
1
2
1
3
1
1
0
0
2

3
1
0
0
3
2
1
3
1
1
1
3

Table 10: Fine-grained results of the best model in Synthesis-Bench: IO accuracy, syntactic accuracy and BLEU score
of the model output, and cyclomatic complexity, number of parameters and pointer parameters of the C function.

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

Error analysis Focusing on the outputs of the best model, we observe:

• When the model has one correct IO test in a given function, it is likely that the others will be also correct, as
shown in Table 8. The probability of generating a program that only passes one unit test by chance is, indeed,
very low.

• After manually inspecting the most frequent syntactical errors (Table 7), we ﬁnd that most of these occur
because the output ﬁnishes prematurely. For instance, it is common to ﬁnd outputs with operators with
unbalanced parentheses as the last instruction, not because the model has not learned the syntax, but because
the decoding terminated in the middle of the program. This occurs when outputs are long and the model
predicts the end of the program special token prematurely.

• In our experiments, IO accuracy does not correlate with cyclomatic complexity, a well-known code complex-
ity measure, as shown in Table 5. We see two potential reasons for that, namely, 1. in Synthesis-Bench there
are not enough functions to observe sufﬁcient variability in cyclomatic complexity to observe the expected
correlation, or 2. the sources of the errors are more simple (e.g., the mere presence of an array) than the
complexity captured by cyclomatic complexity.. In fact, the number of function parameters and the number
of points seems to be indeed negatively correlated with the IO accuracy. Thus, we conclude that the more
function parameters and more pointers, the more difﬁcult is for neural models to correctly interpret C and
generate the corresponding assembler. Finally, with no surprise, syntactical accuracy and BLEU score pos-
itively correlate with IO accuracy, since correct solutions are clearly syntactically correct and, with a lesser
degree, lexically similar to the GCC solution. However, the correlation is not strong enough for these metrics
to be used as reliable proxies of the IO accuracy in case unit tests are not available.

• All models fail in the same functions: Table 9 shows that the intersection of IO errors between the different
models is almost full, meaning that errors are related to some intrinsic difﬁculty of these functions (at least to
neural compilers) and not to randomness in the training process.

• Model outputs do appear like GCC outputs, but with some artifacts such as unnecessary nop operations in

some cases (see supplementary material).

• We observe some trivial errors. For instance, true and false (boolean values from stdbool) are confused
with variable names. If they are manually replaced with 1 and 0, the models usually generate a correct output.

Scaling There is no compelling reason to believe that neural networks would not scale with data, model size, and
compute in a similar way to other domains [27]. Indeed, we have found that models generally perform better the more
data, compute, and parameters can one afford, even though the largest model we trained was not the best one. Although
the biggest model is not the best one, we hypothesize that is the case due to not having enough data to train it or a
sub-optimal training procedure (e.g., not enough training updates). However, unlike other domains, code is usually
evaluated in a binary fashion (that is, it is either correct or it is not). This, together with the fact that complexity in a
given benchmark is not linear (e.g., going from a IO accuracy of 70% to a IO accuracy of 71% might be more difﬁcult
than going from 10% to 20%) causes sharp jumps.

Limits Our best model can correctly compile less than half of the examples in the IO evaluation. Thus, it is far from
being usable in practice as a compiler. Moreover, we have no control over the output space, and we operate on small
functions instead of entire programs. In this work, apart from using code tokenizers and IO evaluation, we have not
included any domain knowledge.

6 Conclusions and Future Work

We conclude that our neural compilation approach shows that sequence-to-sequence deep learning models can, indeed,
learn to compile end-to-end. Nevertheless, the performance is far from optimal and the restrictions make it still far from
being usable in practice. The task presents many challenges, such as output length or hard syntactic and correctness
constraints, that were not explicitly tackled in this work.

As future work, we suggest 1. scaling up our approach, in terms of data, compute, and model parameters, 2. investi-
gating how to incorporate domain knowledge in form of inductive biases or alternative data representations and inputs,
and 3. researching unsupervised techniques to leverage unlabelled (i.e., not parallel) code or assembler.

9

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

References

[1] Zheng Wang and Michael O’Boyle. Machine learning in compiler optimization. Proceedings of the IEEE, 106

(11):1879–1901, 2018.

[2] A. F. da Silva, B. C. Kind, J. W. de Souza Magalh˜aes, J. N. Rocha, B. C. Ferreira Guimar˜aes, and F. M. Quin˜ao
In 2021
Pereira. Anghabench: A suite with one million compilable c benchmarks for code-size reduction.
IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pages 378–390, 2021. doi:
10.1109/CGO51591.2021.9370322.

[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
URL

CoRR, abs/1706.03762, 2017.

Attention is all you need.

Kaiser, and Illia Polosukhin.
http://arxiv.org/abs/1706.03762.

[4] Bruce Collie, Jackson Woodruff, and Michael F. P. O’Boyle. Modeling black-box components with probabilistic

synthesis. CoRR, abs/2010.04811, 2020. URL https://arxiv.org/abs/2010.04811.

[5] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword

units. CoRR, abs/1508.07909, 2015. URL http://arxiv.org/abs/1508.07909.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee,

and Kristina Toutanova.

deep bidirectional
http://arxiv.org/abs/1810.04805.

transformers for

language understanding.

CoRR, abs/1810.04805, 2018.

BERT: pre-training of
URL

[7] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for nat-
ural
URL
http://arxiv.org/abs/1910.13461.

translation, and comprehension.

CoRR, abs/1910.13461, 2019.

language generation,

[8] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities

of neural models. CoRR, abs/1904.01557, 2019. URL http://arxiv.org/abs/1904.01557.

[9] Guillaume Lample and Franc¸ois Charton. Deep learning for symbolic mathematics. CoRR, abs/1912.01412,

2019. URL http://arxiv.org/abs/1912.01412.

[10] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. CoRR,

abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.

[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting
Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for programming and natural languages, 2020.
[12] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation. CoRR,

abs/1802.03691, 2018. URL http://arxiv.org/abs/1802.03691.

[13] Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun, Torsten Hoeﬂer, and Hugh Leather. Programl: Graph-based

deep learning for program optimization and analysis, 2020.

[14] Hugh Leather and Chris Cummins. Machine learning in compilers: Past, present and future. In 2020 Forum for

Speciﬁcation and Design Languages (FDL), pages 1–8, 2020. doi: 10.1109/FDL50818.2020.9232934.

[15] Chris Cummins, Hugh Leather, Benoit Steiner, Horace He, and Soumith Chintala. CompilerGym: A reinforce-
ment learning toolkit for compilers. https://github.com/facebookresearch/CompilerGym/, 2020.
[16] Mehdi Drissi, Olivia Watkins, Aditya Khant, Vivaswat Ojha, Pedro Sandoval, Rakia Segev, Eric Weiner, and

Robert Keller. Program language translation using a grammar-driven tree-to-tree model, 2018.

[17] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of

programming languages, 2020.

[18] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshy-
vanyk. An empirical study on learning bug-ﬁxing patches in the wild via neural machine translation. CoRR,
abs/1812.08693, 2018. URL http://arxiv.org/abs/1812.08693.

[19] Omer Katz, Yuval Olshaker, Yoav Goldberg, and Eran Yahav. Towards neural decompilation. CoRR,

abs/1905.08325, 2019. URL http://arxiv.org/abs/1905.08325.

[20] D. S. Katz, J. Ruchti, and E. Schulte. Using recurrent neural networks for decompilation. In 2018 IEEE 25th
International Conference on Software Analysis, Evolution and Reengineering (SANER), pages 346–356, 2018.
doi: 10.1109/SANER.2018.8330222.

[21] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL
’02, page 311–318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135.
URL https://doi.org/10.3115/1073083.1073135.

10

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

[22] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demon-
strations, 2019.

[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala.
In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances
in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

Pytorch: An imperative style, high-performance deep learning library.

[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
[25] Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. The curious case of hallucinations in neural

machine translation, 2021.

[26] Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. On catastrophic forgetting and mode collapse in gen-
erative adversarial networks. CoRR, abs/1807.04015, 2018. URL http://arxiv.org/abs/1807.04015.
[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361,
2020. URL https://arxiv.org/abs/2001.08361.

11

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

A Output samples

We include some model output samples, together with the reference GCC output and the original C, selected as follows:

• All outputs belong to the best model, speciﬁcally they are one of the top 5 hypothesis.

• All outputs belong to Synthesis-Bench.

• They have not been randomly sampled, or cherry-picked to show the best outputs only, but manually curated

to try to provide representative outputs of different cases, which may not appear proportionally.

Listing 1: C input

Listing 2: Model assembler

Listing 3: Ground truth Assembler

i n t

t r i a n g l e s u m ( i n t n ) {

t r i a n g l e s u m :

i n t
f o r

r = 0 ;
( i n t
++ i ) {

i = 1 ;

i < n ;

f o r

( i n t m = 1 ; m < i ;
++m) {

r += m;

}

}
r e t u r n r ;

}

. LFB0 :
. c f i s t a r t p r o c
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movl %e d i , −20(% r b p )
movl $0 , −12(% r b p )
movl $1 , −8(% r b p )
jmp . L2
. L5 :
movl $1 , −4(% r b p )
jmp . L3
. L4 :
movl −4 (% r b p ) , %e a x
a d d l %eax , −12(% r b p )
a d d l $1 , −4(% r b p )
. L3 :
movl −4(% r b p ) , %e a x
cmpl −8(% r b p ) , %e a x
j l
a d d l $1 , −8(% r b p )
. L2 :
movl −8(% r b p ) , %e a x
cmpl −20(% r b p ) , %e a x
j l
movl −12(% r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

. L4

. L5

t r i a n g l e s u m :
. LFB0 :

. c f i s t a r t p r o c
e n d b r 6 4
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movl %e d i , −20(% r b p )
movl $0 , −12(% r b p )
movl $1 , −8(% r b p )
jmp
. L5 :
movl $1 , −4(% r b p )
jmp
. L4 :
movl −4(% r b p ) , %e a x
a d d l %eax , −12(% r b p )
a d d l $1 , −4(% r b p )

. L3

. L2

. L3 :
movl −4(% r b p ) , %e a x
cmpl −8(% r b p ) , %e a x
j l
a d d l $1 , −8(% r b p )

. L4

. L5

. L2 :
movl −8(% r b p ) , %e a x
cmpl −20(% r b p ) , %e a x
j l
movl −12(% r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

Figure 1:
one.
for
https://stackoverflow.com/questions/59896145/why-endbr64-instruction-is-always-executed?noredirect=1&lq=1.

to the GCC
the new instruction endbr64, which is used
See
not

Correct output
The only difference
is

sample
is
the
executed

in which the
inclusion of

system output

a NOPs

identical

security

support

reasons

almost

targets

that

and

do

as

it.

in

is

12

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

Listing 4: C input

Listing 5: Model assembler

Listing 6: Ground truth assembler

i n t

f i b n ( i n t n ) {

f i b n :

i = 1 ;
i n t
i n t
r = 1 ;
w h i l e ( n > 1 ) {
i = r − i ;
r = i + r ;
n = n − 1 ;

}
r e t u r n r ;

}

, −20 ( %r b p )

. LFB0 :
. c f i s t a r t p r o c
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movl %e d i
movl $1 , −8 ( %r b p )
movl $1 , −4 ( %r b p )
jmp . L2
. L3 :
movl −4 ( %r b p ) , %e a x
s u b l −8 ( %r b p ) , %e a x
movl %e a x , −8 ( %r b p )
movl −8 ( %r b p ) , %edx
movl −4 ( %r b p ) , %e a x
a d d l %edx , %e a x
movl %e a x , −4 ( %r b p )
s u b l $1 , −20 ( %r b p )
. L2 :
cmpl $1 , −20 ( %r b p )
j g . L3
movl −4 ( %r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

f i b n :
. LFB0 :

. c f i s t a r t p r o c
e n d b r 6 4
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movl %e d i , −20(% r b p )
movl $1 , −8(% r b p )
movl $1 , −4(% r b p )
jmp
. L3 :
movl −4(% r b p ) , %e a x
s u b l −8(% r b p ) , %e a x
movl %eax , −8(% r b p )
movl −8(% r b p ) , %e a x
a d d l %eax , −4(% r b p )
s u b l $1 , −20(% r b p )

. L2

. L2 :

. L3

cmpl $1 , −20(% r b p )
j g
movl −4(% r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

Figure 2: Correct compilation of an iterative Fibonacci implementation, in which the system output subtly differs from
the GCC one.

13

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

i n t

Listing 7: C input
s e a r c h ( i n t * a r r ,

i n t v ,
i n t n ) {

i = 0 ;

i d x = −1;
( i n t
++ i ) {
( i d x == −1 &&

i < n ;

a r r [ i ] == v ) {

i d x = i ;

i n t
f o r

i f

}

}
r e t u r n i d x ;

}

Listing 8: Model assembler

Listing 9: Ground truth assembler

. L3

s e a r c h :
. LFB0 :
. c f i s t a r t p r o c
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movq %r d i , −24(% r b p )
movl %e s i , −28(% r b p )
movl %edx , −32(% r b p )
movl $ −1 , −8(% r b p )
movl $0 , −4(% r b p )
jmp . L2
. L4 :
cmpl $ −1 , −8(% r b p )
j n e
movl −4(% r b p ) , %e a x
movslq %eax , %r d x
movq −24(% r b p ) , %r a x
addq %rdx , %r a x
movl
cmpl −28(% r b p ) , %e a x
j n e
movl −4(% r b p ) , %e a x
movl %eax , −8(% r b p )
. L3 :
a d d l $1 , −4(% r b p )
. L2 :
movl −4(% r b p ) , %e a x
cmpl −32(% r b p ) , %e a x
j l
movl −8(% r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

(% r a x ) , %e a x

. L4

. L3

s e a r c h :
. LFB0 :

. c f i s t a r t p r o c
e n d b r 6 4
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movq %r d i , −24(% r b p )
movl %e s i , −28(% r b p )
movl %edx , −32(% r b p )
movl $ −1 , −8(% r b p )
movl $0 , −4(% r b p )
jmp
. L4 :

. L2

. L3

cmpl $ −1 , −8(% r b p )
j n e
movl −4(% r b p ) , %e a x
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movq −24(% r b p ) , %r a x
addq %rdx , %r a x
movl
cmpl %eax , −28(% r b p )
j n e
movl −4(% r b p ) , %e a x
movl %eax , −8(% r b p )
. L3 :

(% r a x ) , %e a x

. L3

a d d l $1 , −4(% r b p )

. L4

. L2 :
movl −4(% r b p ) , %e a x
cmpl −32(% r b p ) , %e a x
j l
movl −8(% r b p ) , %e a x
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

Figure 3: Incorrect output sample (top 3 hypothesis of the best model in the search function) that passes only some
(5/9) of the IO examples. This is relatively uncommon, since usually either it passes all tests or none, and when one
hypothesis in the top 5 ones obtains better than half of the examples right, it is likely that some other hypothesis will
pass all tests, as happened in this case (top 1 and 2 hypothesis were correct).

14

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

Listing 10: C input

Listing 11: Model assembler

Listing 12: Ground truth assembler

v o i d m i n s o f a r s u b t r a c t e d
( i n t * a r r ,
i n t * r e s u l t ,

i n t * a r r 2 ,
i n t n ) {

i n t m i n s o f a r a r r a y [ n ] ;
i n t m i n s o f a r = 1 ;

f o r
( i n t
i ++) {

i = 0 ;

i < n ;

m i n s o f a r =
( m i n s o f a r < a r r [ i ] )
? m i n s o f a r

: 1 ;

m i n s o f a r a r r a y [ i ] =

m i n s o f a r ;

}
i n t m a x o f m i n s o f a r

a n d o t h e r a r r a y [ n ] ;
i < n ;
i = 0 ;

f o r
( i n t
i ++) {

i n t m1 = m i n s o f a r

a r r a y [ i ] ;

i n t m2 = a r r 2 [ i ] ;
i n t max = ( m1 > m2

? m1 : m2 ) ;
m a x o f m i n s o f a r

a n d o t h e r a r r a y [ i ]

= max ;

}
f o r

( i n t

i = 0 ;

i < n ;

i ++) {

r e s u l t [ i ] =
m i n s o f a r

a r r a y [ i ] −
m a x o f m i n s o

f a r a n d

o t h e r a r r a y
[ i ] ;

}

}

m i n s o f a r s u b t r a c t e d :

. LFB0 :
. c f i s t a r t p r o c
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movq %r d i , −8(% r b p )
movq %r s i , −16(% r b p )
movq %rdx , −24(% r b p )
movl %ecx , −28(% r b p )
movl %r8d , −32(% r b p )
nop
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

m i n s o f a r s u b t r a c t e d :
. LFB0 :

. c f i s t a r t p r o c
e n d b r 6 4
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
p u s h q %r 1 5
p u s h q %r 1 4
p u s h q %r 1 3
p u s h q %r 1 2
s u b q $112 , %r s p
. c f i o f f s e t 1 5 , −24
. c f i o f f s e t 1 4 , −32
. c f i o f f s e t 1 3 , −40
. c f i o f f s e t 1 2 , −48
movq %r d i , −120(% r b p )
movq %r s i , −128(% r b p )
movq %rdx , −136(% r b p )
movl %ecx , −140(% r b p )
movq %f s : 4 0 , %r a x
movq %r a x , −40(% r b p )
x o r l %eax , %e a x
movq %r s p , %r a x
movq %r a x , %r d i
movl −140(% r b p ) , %e a x
movslq %eax , %r d x
s u b q $1 , %r d x
movq %rdx , −72(% r b p )
movslq %eax , %r d x
movq %rdx , %r 1 4
movl $0 , %r 1 5 d
movslq %eax , %r d x
movq %rdx , %r 8
movl $0 , %r 9 d
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movl $16 , %e a x
s u b q $1 , %r a x
addq %rdx , %r a x
movl $16 , %e s i
movl $0 , %edx
d i v q %r s i
i m u l q
movq %r a x , %r d x
andq $ −4096 , %r d x
movq %r s p , %r s i
s u b q %rdx , %r s i
movq %r s i , %r d x
. L2 :
cmpq %rdx , %r s p
# CONTINUES FOR 168
# INSTRUCTIONS MORE . . .
. c f i e n d p r o c

$16 , %r a x , %r a x

Figure 4: Syntactically correct, yet meaningless output. This case follows a common pattern we have observed:
when the input C is too complex for the model, the model just outputs a short, syntactically correct assembler that
semantically has almost nothing to do with the input C. The model seems to behave like an unconditional assembler
language model.

15

Learning C to x86 Translation: An Experiment in Neural Compilation

PREPRINT

Listing 13: C input
v o i d norm ( f l o a t * vec ,

f l o a t *o ,
i n t n ) {
sum = 0 . 0 f ;
i = 0 ;

( i n t
++ i ) {

f l o a t
f o r

i < n ;

sum += v e c [ i ] ;

}

f o r

i = 0 ;

( i n t
++ i ) {

i < n ;

o [ i ]

/ = sum ;

}

}

Listing 14: Model assembler

Listing 15: Ground truth assembler

s u b e q :

. LFB0 :
. c f i s t a r t p r o c
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movq %r d i , −24(% r b p )
movq %r s i , −32(% r b p )
movl %edx , −36(% r b p )
movl $0 , −4(% r b p )
jmp . L2
. L3 :
movl −4(% r b p ) ,% e a x
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movq −24(% r b p ) , %r a x
addq %rdx , %r a x
movl −4(% r b p ) , %edx
movslq %edx , %r d x
l e a q 0 ( ,% rdx , 4 ) , %r c x
movq −24(% r b p ) , %r d x
addq %r c x , %r d x
movl
movl %edx , %e c x
movl −4(% r b p ) , %edx
movslq %edx , %r d x
l e a q 0 ( , %r d x , 4 ) , %r s i
movq −32 (% r b p ) , %r d x
addq %r s i , %r d x
movl
s u b l %edx , %e c x
movl %ecx , %edx
movl %edx ,
a d d l $1 , −4(% r b p

(% r d x ) , %edx

(% r d x ) , %edx

(% r a x )

norm :
. LFB0 :

%xmm0 , −12(% r b p )

. c f i s t a r t p r o c
e n d b r 6 4
p u s h q %r b p
. c f i d e f c f a o f f s e t 16
. c f i o f f s e t 6 , −16
movq %r s p , %r b p
. c f i d e f c f a r e g i s t e r 6
movq %r d i , −24(% r b p )
movq %r s i , −32(% r b p )
movl %edx , −36(% r b p )
p x o r %xmm0 , %xmm0
movss
movl $0 , −8(% r b p )
jmp
. L3 :
movl −8(% r b p ) , %e a x
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movq −24(% r b p ) , %r a x
addq %rdx , %r a x
movss
movss
a d d s s
movss
a d d l $1 , −8(% r b p )

(% r a x ) , %xmm0
−12(% r b p ) , %xmm1
%xmm1 , %xmm0
%xmm0 , −12(% r b p )

. L2

. L4

. L3

. L2 :
movl −8(% r b p ) , %e a x
cmpl −36(% r b p ) , %e a x
j l
movl $0 , −4(% r b p )
jmp
. L5 :
movl −4(% r b p ) , %e a x
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movq −32(% r b p ) , %r a x
addq %rdx , %r a x
movss
movl −4(% r b p ) , %e a x
c l t q
l e a q 0( ,% r a x , 4 ) , %r d x
movq −32(% r b p ) , %r a x
addq %rdx , %r a x
d i v s s
movss
a d d l $1 , −4(% r b p )

(% r a x ) , %xmm0

−12(% r b p ) , %xmm0
%xmm0 ,

(% r a x )

. L5

. L4 :
movl −4(% r b p ) , %e a x
cmpl −36(% r b p ) , %e a x
j l
nop
nop
popq %r b p
. c f i d e f c f a 7 , 8
r e t
. c f i e n d p r o c

Figure 5: Syntactically incorrect output (unbalanced parentheses in the last addl instruction) that actually is caused
by the hypothesis terminating before it should have, like most detected syntax errors.

16

