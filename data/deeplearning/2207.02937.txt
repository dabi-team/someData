2
2
0
2

l
u
J

6

]

G
L
.
s
c
[

1
v
7
3
9
2
0
.
7
0
2
2
:
v
i
X
r
a

Learning Optimal Solutions via an LSTM-Optimization Framework

Dogacan Yilmaz1 and ˙I. Esra B¨uy¨uktahtakın†2

1Department of Mechanical and Industrial Engineering, New Jersey Institute of Technology

2Department of Industrial and Systems Engineering, Virginia Tech

July 8, 2022

Abstract

In this study, we present a deep learning-optimization framework to tackle dynamic mixed-

integer programs. Speciﬁcally, we develop a bidirectional Long Short Term Memory (LSTM)

framework that can process information forward and backward in time to learn optimal solutions

to sequential decision-making problems. We demonstrate our approach in predicting the optimal

decisions for the single-item capacitated lot-sizing problem (CLSP), where a binary variable

denotes whether to produce in a period or not. Due to the dynamic nature of the problem, the

CLSP can be treated as a sequence labeling task where a recurrent neural network can capture

the problem’s temporal dynamics. Computational results show that our LSTM-Optimization

(LSTM-Opt) framework signiﬁcantly reduces the solution time of benchmark CLSP problems

without much loss in feasibility and optimality. For example, the predictions at the 85% level

reduce the CPLEX solution time by a factor of 9 on average for over 240,000 test instances

with an optimality gap of less than 0.05% and 0.4% infeasibility in the test set. Also, models

trained using shorter planning horizons can successfully predict the optimal solution of the

instances with longer planning horizons. For the hardest data set, the LSTM predictions at

the 25% level reduce the solution time of 70 CPU hours to less than 2 CPU minutes with an

optimality gap of 0.8% and without any infeasibility. The LSTM-Opt framework outperforms

classical ML algorithms, such as the logistic regression and random forest, in terms of the

solution quality, and exact approaches, such as the ((cid:96), S) and dynamic programming-based

†Corresponding author email: esratoy@vt.edu

1

 
 
 
 
 
 
inequalities, with respect to the solution time improvement. Our machine learning approach

could be beneﬁcial in tackling sequential decision-making problems similar to CLSP, which need

to be solved repetitively, frequently, and in a fast manner.

Keywords: Machine learning, recurrent neural networks, bidirectional Long Short Term Memory

(LSTM), mixed integer programming, capacitated lot-sizing, sequential decision-making

1

Introduction

In the recent decade, signiﬁcant progress has been achieved with the use of machine learning

(ML) in various ﬁelds, such as image recognition and natural language processing. A subﬁeld of

ML, deep learning, has inspired much success over the last decade and has led to a growing in-

terest in research and practice. ML and operations research (OR) are historically interconnected

through optimization, but only recently the use of ML for OR has received more attention. In

this study, we will focus on this direction. Speciﬁcally, we will leverage deep learning algorithms

to predict solutions to an OR problem by taking advantage of previously solved problems. In

various applications in operations planning and management, such as energy demand-side man-

agement, airline scheduling, and vehicle routing, problems with the same structures must be

solved repeatedly with diﬀerent parameters within a very short period of time. In such settings,

a reduced solution time obtained by fast algorithms can be highly beneﬁcial for improving the

eﬃciency and performance of businesses.

One complex and recurring problem for industrial companies is to determine the amount

and timing of production over a planning horizon under resource constraints. It is an important

challenge in industry and supply chain management because a production plan directly impacts

companies’ output and their ability to compete in operational costs and customer service levels

(Gicquel et al., 2008). Production planning is also a highly complex task because ﬁrms strive

to optimize multiple conﬂicting objectives, such as minimizing production and inventory costs,

while maximizing customer satisfaction under tight constraints on resources, such as budget,

raw materials, and machine availability.

In this paper, we present a general prediction framework to learn optimal solutions of com-

binatorial optimization problems, while focusing on tackling one core production planning prob-

lem: the single-item Capacitated Lot Sizing Problem (CLSP). The practical importance of the

CLSP is apparent from numerous examples of its application in various production and man-

ufacturing industries, including but not limited to the textile industry, oil and gas companies,

car manufacturers, and pharmaceutical industry (Gicquel et al., 2008; Karimi et al., 2003). The

2

CLSP determines the optimal production and inventory levels that meet periodic demand under

a given production capacity by minimizing the sum of production, setup, and inventory holding

cost over a ﬁnite planning horizon. In the mixed-integer programming (MIP) formulation of the

CLSP, the decision of whether to produce or not is represented by a binary variable. Thus, the

CLSP with time-varying capacity is NP-hard, a very diﬃcult problem to optimize (Bitran and

Yanasse, 1982; Hartman et al., 2010). In this paper, we focus on tackling the computational

diﬃculty of the CLSP and provide an ML-based optimization framework to solve its MIP for-

mulation more eﬃciently. Thus, we study the CLSP at a high formulation level rather than

focusing on a speciﬁc real-life application.

The CLSP is a sequential decision-making problem because, in each time period, the pro-

duction level is determined to meet the periodic demand, and any additional produced items

not used for current demand are placed in inventory to be used for future demand. Therefore,

demand, capacity, cost inputs, and production decisions constitute highly correlated temporal

sequences that the classical supervised classiﬁcation might not capture. Thus, the CLSP can be

treated as a sequence labeling task where a recurrent neural network (RNN) is applicable.

The RNN is a specialized type of neural network that can process sequential data by enabling

information ﬂow through various time steps. The neural network with the same parameters is

applied at each time step of the sequence. Input to layer at each time step consists of data at

that time step and the network activations from the previous step. As a result, RNNs allow

previous inputs to aﬀect the output rather than just the current input. Developed by Hochreiter

and Schmidhuber (1997), Long Short-Term Memory (LSTM) is a specialized RNN that can store

information for long time steps, which can be challenging to handle by a classical RNN (Bengio

et al., 1994). Bidirectional RNN (BRNN) allows using input information of future time steps

rather than processing information in sequential order (Schuster and Paliwal, 1997). The main

idea is to train two separate RNNs in both time directions that connect to the same output

layer. Bidirectional LSTM is an extension of BRNNs by using LSTM architecture (Graves and

Schmidhuber, 2005). The LSTM architecture might be preferable to the classical RNN due to

its ability to capture long-term dependencies that come at a computational cost. We train the

bidirectional LSTM network on datasets with diﬀerent characteristics and evaluate the quality

of resulting predictions in terms of feasibility and optimality. Our computational results show

that a signiﬁcant reduction in solution time can be achieved without much loss in feasibility and

optimality.

Our main contribution is to develop an LSTM-based framework for learning optimal solu-

tions to CLSP quickly. We propose using bidirectional LSTM to predict the binary production

3

decision variable. Bidirectional LSTM can process information in both time directions, which is

critical to predicting solutions of various OR problems with dynamic nature and where data is

available for the planning horizon. Also, instead of using all the predicted variables, we propose

using them partially to reduce the number of infeasible solutions. We present the results on

how the quality of the predictions changes regarding feasibility and optimality with diﬀerent

levels of predicted variables. Additionally, we show the results of generalization on instances

with diﬀerent characteristics and instances with longer time horizons and compare them with

those of traditional dynamic programming and cutting plane algorithms and well-known learning

algorithms: logistic regression and random forest.

2 Literature Review and Contributions

2.1 Literature Review

In recent years, signiﬁcant results were achieved in various ﬁelds by deep learning, which is a

sub-ﬁeld of ML. As a result, there has been a growing literature on the interaction between ML

and OR. In this paper, we focus on the use of ML to improve solving OR problems, particularly

focusing on the CLSP. The origin of the interest in using ML algorithms for OR can be traced

back to the 1980s when Neural Computation was used for solving Combinatorial Optimization

problems (see, e.g., the survey of Smith (1999) on this topic). The approaches in the literature

are structured into two parts: approaches that use ML for predicting the solutions directly

from inputs and approaches that predict valuable pieces of information to utilize in the solution

algorithms.

In one of the studies, which focuses on predicting the optimal solution directly, Larsen et al.

(2021) propose a new methodology to predict solution descriptions of a stochastic load planning

problem using deep learning. According to the authors, a solution can be described at diﬀerent

levels. The most detailed solution describes the values taken by each variable, and the least

detailed solution gives the value of the objective function. Their desired level of description is

somewhere in the middle. At the time of the prediction, using a deterministic optimization model

is not possible because the information available is imperfect, and the computational budget is

limited. They generate training data by solving a large number of deterministic problems oﬄine

and combining solutions to the desired level of description at the prediction time. They train

feedforward neural networks using this generated data and predict the actual problem instances.

With a similar approach, Fischetti and Fraccaro (2019) use various ML techniques, including

neural networks, to estimate the optimal value of the oﬀshore wind farm layout optimization

4

problem. Their goal is to determine the optimal allocation of the wind turbines in a site to

maximize park power production. The authors argue that ML can be used as a fast tool to

estimate the optimal value of the problem for pre-selecting between candidate wind farm sites.

The optimization model can be evaluated at these promising sites instead of all candidates.

Based on their ﬁndings, a fast ML+OR tool can dramatically increase the number of sites and

turbine types investigated.

In a recent study, Bertsimas and Stellato (2019) use neural networks to exploit the repetitive

nature of online optimization, where problems with diﬀerent parameters are solved frequently.

They utilize the structure of mixed-integer quadratic optimization problems using neural net-

works to predict the strategy, which is deﬁned as a tuple of indexes of tight inequality constraints

and values of the integer variables. At the time of prediction, they do not require a solver. They

evaluate a single neural network prediction and a single linear system solution.

Oroojlooyjadid et al. (2019) utilize deep neural networks to determine the optimal order

quantity in the newsvendor problem. They establish an algorithm that integrates demand

forecasting with deciding optimal order quantity rather than doing both separately. The input

data consists of features of demand, and the output is the optimal order quantities. Additionally,

they modify the loss function of the neural network as the newsvendor objective.

In the group of studies where ML is used to generate vital information to use in solution

algorithms, Khalil et al. (2016) propose an ML framework for strong branching decisions, leading

to signiﬁcantly smaller search trees. In Khalil et al. (2017), authors use ML to decide if a primal

heuristic should be run at which nodes during the branch-and-bound tree search so that the

overall performance of the solver is optimized. The reader is referred to the survey of Lodi

and Zarpellon (2017) on learning algorithms to improve branch-and-bound decisions. Xavier

et al. (2019) propose the usage of ML algorithms to improve the computational performance of

MIP solvers by predicting redundant constraints, reasonable initial feasible solutions, and aﬃne

subspaces where the optimal solution is likely to lie. Kruber et al. (2017) address whether or not

a reformulation should be performed and which decomposition method to choose when several

are possible using ML algorithms. Bonami et al. (2018) suggest a methodology that determines

the linearization decision for a mixed-integer quadratic programming problem.

The CLSP has been widely studied in the OR literature by developing exact and heuristic al-

gorithms. Florian et al. (1980) provide a solution methodology based on dynamic programming

(DP) for lot-sizing. An exact solution approach presented by Barany et al. (1984) involves gen-

erating valid ((cid:96),S) inequalities and adding them to the formulation with a separation algorithm.

Eppen and Martin (1987) redeﬁne variables to generate a graph representation of the problem

5

which has a tighter linear relaxation than the original formulation. More recently, DP-based

and partial-objective inequalities have been proposed for the single-item CLSP (Hartman et al.,

2010) and multi-item CLSP (B¨uy¨uktahtakın et al., 2018), respectively. For a detailed discussion

of the exact and heuristic approaches to diﬀerent versions of the lot-sizing problem, we refer the

readers to the excellent review of Pochet and Wolsey (2006).

Readers are referred to Goodfellow et al. (2016) for a detailed discussion on deep learning

algorithms. We refer to Graves (2012) for a detailed discussion on RNN, LSTM, and sequence

labeling. Readers are referred to Karimi et al. (2003) and Pochet and Wolsey (2006) for an

extensive survey on the capacitated lot-sizing problem, their variants, and exact and heuristic

approaches for their solution.

There has been a growing interest in using ML algorithms to help solve OR problems in

recent years. Despite all the advancements in the ML-OR integration, there is still a research

gap in learning optimal solutions to MIP problems, including CLSP from previously-solved

instances and evaluating the eﬀectiveness and generalization of the learning-based optimization

approach.

2.2 Key Contributions of the Paper

To our knowledge, none of the former studies have used a deep learning algorithm, such as LSTM,

to capture the sequential nature of multi-period MIP models and predict their optimal solution.

Decisions are closely linked over multiple periods in a multi-stage or sequential problem. Thus an

ML approach that does not consider patterns across time may not capture the dynamic nature

of the problem. The LSTM, on the other hand, is a recurrent network capable of understanding

long and short-term dependencies and temporal diﬀerences in the data of optimal solutions given

speciﬁc problem characteristics.

In this study, we present a new deep learning LSTM-Optimization (LSTM-Opt) architecture

to learn the optimal solutions for one of the most famous combinatorial optimization problems

and a classical example of a sequential decision-making problem, CLSP. Our goal here is to

reduce the solution time, where numerous similar CLSP need to be solved repetitively and in a

fast manner with a small optimality gap. Our speciﬁc contributions are:

1. To our knowledge, this is the ﬁrst study that utilizes an LSTM approach to make pre-

dictions from the optimal solutions of CLSP instances and use those predictions to solve

similar CLSP with diﬀerent data. Speciﬁcally, we propose an LSTM-Opt framework, which

predicts binary decision variables of the CLSP problem. The bidirectional LSTM learns

6

optimal solutions to sequential decision-making problems where the input data is available

for the planning horizon. We compare the computational performance of our algorithm

with other ML approaches, such as logistic regression and random forest. We show that

the LSTM networks capture the time-wise dependency in sequential decision making and

thus are superior compared to those ML algorithms.

2. We evaluate the eﬀectiveness of predictions in terms of their feasibility and optimality for

the original CLSP by deﬁning optimization-based metrics, such as the optimality gap and

the percent of feasibly-predicted instances in the test set. The use of all predictions could

help reduce the solution time but also may increase the infeasibility in the test set. To im-

prove the feasibility of the solutions, we propose using the predictions partially as an input

into the MIP solver, CPLEX (ILOG, 2016). This approach provides a signiﬁcant reduction

in solution time while improving the optimality gap and the feasibility of solutions. To

remedy the infeasibility problem, additional methods, such as the CPLEX user cuts, are

utilized to solve the problem with a reasonable optimality gap with no infeasibility.

3. We utilize benchmark CLSP instances in the literature to demonstrate the eﬃciency of

our LSTM-Opt approach.

In addition to comparing with direct solutions of CLSP by

CPLEX, we utilize a dynamic programming formulation (Florian et al., 1980), dynamic

programming-based inequalities (Hartman et al., 2010), and ((cid:96),S) inequalities (Barany

et al., 1984) to show that the LSTM-Opt can be beneﬁcial to reduce the solution time

even when compared with these traditional exact OR methodologies proposed for solving

the CLSP more eﬃciently.

4. Our LSTM-Opt framework helps decrease the CPLEX solution time by multiple orders

of magnitude when predicting CLSP instances. Furthermore, this prediction architecture

provides more time-gain beneﬁts as the CLSP instances get harder, i.e., for the most

diﬃcult test problems that are generated with the same distribution as training instances,

the solution time is reduced by a factor of 13 without any infeasibility or an optimality

gap.

5. We investigate if the trained LSTM model can predict instances with diﬀerent underlying

data distributions or instances with a larger planning horizon. The results imply that one

must be careful in picking the prediction level to solve instances with diﬀerent character-

istics. The computational results also show that the trained LSTM model can successfully

predict longer and thus harder instances without extra training. As an example, in those

generalization experiments to predict longer planning horizons, using a prediction level of

25%, we have reduced an average solution time of 70 CPU hours to only 2 CPU minutes

7

with a 0.8% optimality gap, which is a quite signiﬁcant computational achievement.

6. Once an LSTM model is trained from previously solved instances, predictions to new prob-

lems can be generated in milliseconds in an online setting. Thus, our LSTM-Opt approach

could, in particular, be useful for solving practical and recurring sequential decision-making

problems, such as power generation scheduling, energy demand-side management, and pric-

ing optimization, where the same problem formulations are solved repeatedly over time

with updated parameters.

7. Our LSTM-Opt framework is generalizable since it does not assume any speciﬁc informa-

tion about CLSP. Thus it can be applied to other MIPs, such as the Binary Knapsack

problem, one of the most well-known MIP formulations and a relaxation of the CLSP.

The remainder of the paper is as follows. Section 3 presents the MIP formulation of the

CLSP. Section 4 describes the proposed LSTM-Opt framework. Section 5 describes the details of

implementation and experimentation. Section 6 presents the computational results on datasets

with diﬀerent characteristics and a comparison with other ML and exact approaches. Section 7

concludes the paper with future research directions. Appendix A1-A3 provides a discussion on

the LSTM training time and more results with diﬀerent datasets and characteristics, respectively.

3 Capacitated Lot Sizing Problem

CLSP is a fundamental problem in production planning. The CLSP determines the production

and inventory levels in a multi-period planning horizon to fulﬁll the deterministic demand with-

out back-ordering to minimize the sum of production, setup, and inventory holding costs. The

CLSP with time-varying capacity is NP-Hard, and it has numerous variations and applications

in the production and manufacturing industries (Quadt and Kuhn, 2007).

To formulate the CLSP as a mixed-integer program (MIP), the following parameters and

decision variables are deﬁned. Let T be the number of periods considered in the planning

horizon. For each period t ∈ {1, 2, . . . , T } demand dt is known in advance. For each period

t ∈ {1, 2, . . . , T } associated costs are unit production cost pt, setup cost ft, and unit inventory

holding cost ht. Note that setup cost ft is not per unit based. For each period t ∈ {1, 2, . . . , T }

production capacity is denoted by ct. Without loss of generality, all parameters can be assumed

to be non-negative. The number of units produced and ending inventory in period t is repre-

sented by non-negative variables xt and st, respectively. Binary variable yt takes value 1 if there

8

is production in period t, and takes value 0 otherwise. The CLSP can be formulated as:

min

s.t.

T
(cid:88)

(ptxt + ftyt + htst)

t=1
st−1 + xt − dt = st

∀t = 1, 2, . . . , T

xt ≤ ytct

∀t = 1, 2, . . . , T

xt, st ≥ 0

∀t = 1, 2, . . . , T

yt ∈ {0, 1}

∀t = 1, 2, . . . , T.

(1a)

(1b)

(1c)

(1d)

(1e)

The objective function (1a) minimizes the sum of production costs, setup costs, and inventory

holding costs over all periods t ∈ {1, 2, . . . , T }. Constraints (1b) ensure the inventory ﬂow over

multiple periods. Speciﬁcally, the demand in period t must be satisﬁed by inventory at the

end of period t − 1 and units produced in period t. The remaining amount is the inventory

at the end of period t. Constraints (1c) limit the production by capacity and ensure that a

ﬁxed cost of production is incurred in the objective function if there is production in period

t. Constraints (1d) enforce that the amounts of units produced and kept in inventory are

non-negative. Finally, constraints (1e) ensure that yt are binary variables. The parameter s0

represents the initial inventory and is assumed to be zero.

4 LSTM-Optimization Framework

In this section, we present the LSTM-Opt framework that we develop to predict the optimal

solution of the CLSP. Using the LSTM-Opt framework, we only predict the binary decision

variables yt that correspond to a production decision instead of predicting all decision variables.

As depicted in Figure 1, the LSTM-based framework starts with data generation. The datasets

with diﬀerent characteristics are constructed according to the data-generation scheme described

in Section 5.1. The resulting datasets are divided into three categories involving the training,

validation, and test sets, which consist of 64%, 16%, and 20% of the data, respectively. The

LSTM network parameters are optimized using a training set. This is done by minimizing a loss

function that measures the performance of the model’s predictions compared to actual values.

The binary cross-entropy is a common choice as a smooth loss function for binary classiﬁcation

because it leads to faster training with a better generalization performance than the sum of

squares error (Bishop et al., 1995). The objective function of the CLSP given by equation

(1a) is not minimized by the LSTM network.

In the training step, we minimize the binary

cross-entropy loss function given in the following equation (2):

9

Figure 1: LSTM-Opt framework.

(cid:32)L(y∗, ˆy) = −

1
T

×

T
(cid:88)

t=1

(y∗

t × log(ˆyt) + (1 − y∗

t ) × log(1 − ˆyt))

(2)

where y∗ represents the optimal values of the binary decision variables, and ˆy represents pre-

dicted values of the binary decision variables of a CLSP instance. The binary cross-entropy loss

function in equation (2) measures the discrepancy between y∗ and ˆy.

Figure 2: Bidirectional LSTM (Schuster and Paliwal, 1997) adapted to represent the CLSP multi-

period structure.

10

The LSTM model consists of several bidirectional LSTM layers that can process the infor-

mation in both time directions and an output layer with a sigmoid activation function. Figure

2 shows the ﬂow of information in the forward and backward layers in bidirectional LSTM. The

input layer for LSTM consists of available features for that period: unit production cost pt,

setup cost ft, production capacity ct, and demand dt for t ∈ {1, 2, . . . , T }. Note that we omitted

the holding cost ht because it is taken as constant. For period t, information is carried from

period t − 1 in the forward layer and used to generate output in period t. In the backward layer,

information is carried from period t + 1 to period t, and it is used to generate output together

with inputs in period t. The outputs of forward and backward layers are combined to generate

prediction ˆyt. After each hidden layer, a dropout layer is added for regularization.

We compare the models with diﬀerent parameters, using the instances in the validation set,

in a method known as hyperparameter tuning. We then choose the model with the highest

validation accuracy, which is the proportion of the correctly predicted variables. Note that the

validation set is not used to minimize the binary cross-entropy in equation (2); it is only used

to compare LSTM networks with diﬀerent hyperparameters, such as learning rate, number of

layers, hidden nodes, and dropout rate. Then for each instance in the test set, a prediction is

generated using the picked model. The framework described does not provide results on the

feasibility of the resulting prediction and how good it is compared to the objective function

value. The resulting predictions are added to problem (1) as constraints, and then CLSP is re-

solved using CPLEX. The described approach can deliver optimal solutions fast and accurately

without much loss in feasibility and optimality, as demonstrated in the computational results

under Section 6.

CLSP is an MIP because of the binary decision variables. Predicting all binary decision

variables and then ﬁxing the predicted values in the MIP formulation (1) makes the problem a

linear program, which yields a signiﬁcant reduction in the solution time. This approach often

leads to infeasibility due to its strict nature. Instead, predicting some of the binary decision

variables results in more ﬂexibility when resolving the problem instance and reduces the number

of infeasible problems while still improving the solution time.

Additionally, the integral nature of the other two decision variables is preserved by solely

predicting the binary variable because once the binary variables are ﬁxed in the CLSP, it reduces

to a linear program (Pochet and Wolsey, 2006). Also, predicting the binary variable carries an

interpretable meaning of the production decision and its timing. Once the decision of whether

to produce or not is determined and ﬁxed in a period, the MIP solver determines the amount

of production and the inventory levels.

11

Our integrated ML+OR tool can be beneﬁcial for real-time applications where problems

with diﬀerent parameters are solved repeatedly. Lot-sizing and its variants commonly arise in

the energy, pharmaceutical, electronics, food, processing, and consumer goods industries (Copil

et al., 2017). After an ML model is trained, it is not necessary to update the trained model

after each prediction. Therefore, once an ML model is trained, predictions can be achieved in

milliseconds by an LSTM forward pass to solve many CLSP instances in a quite fast manner.

5

Implementation and Experimentation

This section presents the CLSP instance generation scheme and the implementation details of

our LSTM-Opt framework. All the codes are written in C++ and Python to generate CLSP

instances and run the LSTM-Opt framework.

5.1 CLSP Instance Generation

The training, validation, and testing data were generated by the scheme presented in Atamt¨urk

and Mu˜noz (2004). The diﬃculty of problems was determined by two main factors: tightness of

the capacities with respect to demand and the ratio between setup and holding cost. Following

the parameters used in B¨uy¨uktahtakın and Liu (2016), instances are generated from capacity-to-

demand ratios c ∈ {3, 5, 8}, setup-to-holding cost ratios f ∈ {1, 000, 10, 000} and the number of

periods T ∈ {90, 120}. The parameters regarding demand dt, unit production cost pt, production

capacity ct, and setup cost ft are generated from integer uniform distribution with the ranges
dt ∈ [1, 600], pt ∈ [1, 5], ct ∈ (cid:2)0.7c ¯d, 1.1c ¯d(cid:3), ft ∈ (cid:2)0.9f ¯h, 1.1f ¯h(cid:3), where ¯d = 1
¯h = 1
T

, respectively. Unit inventory holding cost ht is set at one at each period.

t=1 ht

t=1 dt

(cid:16)(cid:80)T

(cid:16)(cid:80)T

and

(cid:17)

(cid:17)

T

For each of the 12 combinations of parameters c, f , and T as described above, 100,000

instances (problems) are generated, resulting in a total of 1,200,000 instances. All instances are

solved using CPLEX. Infeasible problems are eliminated and replaced by feasible instances by

regenerating new instances. The training, validation, and test set consists of 64,000, 16,000, and

20,000 CLSP instances, respectively, for each combination of parameters.

5.2 LSTM-Opt Implementation Details

Before the training, the data is standardized by subtracting the feature mean and dividing by

feature standard deviation as a preprocessing step, which is often practically useful for faster

convergence if diﬀerent inputs have typical values that diﬀer signiﬁcantly (LeCun et al., 2012).

In the hyperparameter tuning step, we compared LSTM models with diﬀerent parameters, such

12

as learning rate, number of layers, hidden nodes, and dropout rate using the validation set.

The values considered are [2, 6] for the number of hidden layers, [10, 150] for the number of

units in hidden layers, [0.1, 0.5] for the dropout rate, and [0.1, 0.001] for the learning rate. The

selected LSTM model contains three hidden LSTM layers, each with 40 hidden units in each

time direction. Therefore, bidirectional LSTM for each layer has 80 hidden units. After each

LSTM layer, a dropout layer with a drop rate of 0.3 is added to regularize the network. We

used Adam optimizer with an initial learning rate of 0.01, which is an adaptive learning rate

optimization algorithm that has been shown to work well in practice (Kingma and Ba, 2014).

For each instance in the test set, the values of binary variables are predicted. For each period

t ∈ {1, 2, . . . , T }, the LSTM network generates a prediction in the range of [0, 1] for each yt.

The value of max( ˆyt, 1 − ˆyt) for t ∈ {1, 2, . . . , T }, where ˆyt represents the predicted value of the

binary variable yt, is calculated and ordered in decreasing order. Predicted variables are selected

up to the desired level, and ˆyt is labeled as 0 or 1 using a cut-oﬀ value of 0.5. Those variables

can be interpreted as the ones closest to either zero or one, and thus we are more conﬁdent in

the LSTM model’s prediction. Let D ⊆ T be the set of indices of those binary decision variables

predicted and selected using the max( ˆyt, 1 − ˆyt) function and a pre-set prediction percentage.

Finally, those values are added as a constraint to the original model (1), as shown in the following

modiﬁed problem (3):

min

s.t.

T
(cid:88)

(ptxt + ftyt + htst)

t=1
Constraints

(1b) − (1e)

yt = ˆyt

∀t ∈ D.

(3a)

(3b)

(3c)

Problem (3) is solved again to assess the quality of the LSTM predictions.

6 Computational Results

This section presents results from computational experiments performed using the LSTM-Opt

framework described in Section 4 on randomly generated CLSP instances with various charac-

teristics as deﬁned in Section 5. All experiments are performed on a computer running Windows

10 Intel i7 with 3.6 GHz GPU and 64 GB of memory. The CLSP instances are solved with IBM

ILOG CPLEX 12.7.1. All of the results regarding the test-set solution times are presented in

CPU seconds. The detailed results for training LSTM models are presented in Appendix A1.

We solve problem (1) instances using the default CPLEX as a benchmark to compare the

performance of our LSTM-Opt framework for solving similar and diﬀerent CLSP instances. The

13

test dataset consists of 20,000 CLSP instances for each combination of the parameters f , T , and

c.

As an alternative to solving the problem (3), where we ﬁx the predicted values in the CLSP

formulation (1) as constraints, we utilize two CPLEX solver methods–AddUserCuts and Ad-

dMIPStart methods of CPLEX to eliminate infeasibility as described below:

• 100(UC): AddUserCuts method of CPLEX, which enable CPLEX to add cuts into the

user cut pool and use the cuts as needed.

• 100(MS): AddMIPStart method of CPLEX, which enable CPLEX to provide a starting

solution to the model with a user cut pool.

6.1 Quality of Predictions

Here, we present a number of metrics with their formal deﬁnitions that are used to evaluate

the eﬀectiveness of our LSTM-Opt framework. Speciﬁcally, those metrics assess the proposed

method to solve optimization instances with respect to the improvement in the solution time as

well as the feasibility and optimality of the resulting solutions. The following metrics are used

in Tables 1-6 and A2-A5:

• timeCPX: Mean solution time of a CLSP instance of the problem (1) in CPU seconds

without any predictions using default CPLEX.

• timeML: Mean solution time of the LSTM-Opt framework, including the prediction gen-

eration time by the LSTM model.

• pred(%): Percent of binary variables predicted by the LSTM-Opt framework.

Additionally, we provide the following metrics and their formal deﬁnitions and use a combi-

nation of them to present the results:

Deﬁnition 6.1 The solution time factor improvement factor obtained by ﬁxing the predicted

variables as a constraint (or using AddUserCuts and addMIPStart) is given by:

timeimp =

timeCP X
timeM L

.

(4)

Deﬁnition 6.2 The percent solution time gain obtained by ﬁxing the predicted variables as a

constraint (or using AddUserCuts and addMIPStart) is given by:

timegain(%) =

(timeCP X − timeM L)
timeCP X

× 100.

(5)

14

Deﬁnition 6.3 The percent infeasibility of a test set resulted from using predicted binary vari-

ables is given by:

inf (%) =

ˆm
m

× 100,

(6)

where ˆm represents the number of CLSP instances that become infeasible by adding predictions

as a constraint and m represents the total number of CLSP instances in the test set.

Deﬁnition 6.4 Let (x∗, y∗, s∗) be the optimal solution for the original MIP problem (1) that

is obtained by the CPLEX solver and Z(x∗, y∗, s∗) be the corresponding optimal objective value.

Let ˆy be the partial or full prediction of binary variables, (˜x, ˜y, ˜s) be the optimal solution obtained

by CPLEX using predictions ˆy in problem (3), and Z(˜x, ˜y, ˜s) be the resulting objective function

value. Note ˜y is equivalent to ˆy when a full prediction is made. The optimality gap due to

using solutions in our LSTM-Opt prediction framework is deﬁned over feasibly-solved instances

as follows:

optgap(%) =

(Z(˜x, ˜y, ˜s) − Z(x∗, y∗, s∗))
Z(x∗, y∗, s∗)

× 100.

(7)

In the next section, we present computational results to demonstrate the eﬀectiveness of our

prediction-optimization method, using the training and test instances with the same distribution.

The predictions fed into the CPLEX solver may not be feasible for the CLSP instance. The test

set instances for which the LSTM prediction leads to an infeasible solution are not included in

the calculations of timeML, timeimp, timegain(%), and optgap(%).

6.2 Predicting Instances with Same Distribution

Each row of Tables 1, 3, A2, A3, A4, and A5 presents the averages of 20,000 instances, whereas

Tables 4, 5, and 6 present the mean result for 10 instances due to long solution times for each c

value and each f − T pair. Table 1 presents results for instances with T = 120 with f = 10, 000.

The dataset of c = 3 is harder than the dataset with c = 5 and c = 8. Both 25% and 50%

prediction levels achieve all-feasible predictions that do not increase the objective function value.

With the 50% prediction level, the mean solution time decreases by more than 10-fold. As the

prediction level increases, the time factor improvement increases as well. Predictions at the

75% level reduce the solution time by a factor of 50 with an infeasibility of the test set below

0.3% and without any optimality gap. However, as the prediction levels increases, predictions

lead to more infeasible instances in the test set. At the full prediction level (pred(%)=100),

more than half of the predictions results in infeasible solutions to problem (3); however, the

issue of infeasibility is remedied by the CPLEX’s user cuts approach (AddUserCuts), which

15

eliminates the infeasible solutions in the cut pool. The user cuts (UC) approach provides a

signiﬁcant solution time factor improvement of 68 with an optimality gap of over 1% without

any infeasibility in the test set. The detailed results of experiments with f = 10, 000, T = 90

and f = 1, 000, T = 90, 120 are presented in Tables A2-A4 in Appendix A2.

Table 1: Summary of experiments for f = 10, 000 and T = 120

c
3

5

8

pred(%)
25
50
75
85
90
95
100
100(MS)
100(UC)
25
50
75
85
90
95
100
100(MS)
100(UC)
25
50
75
85
90
95
100
100(MS)
100(UC)

timeCPX timeML timeimp timegain(%)

22.6

3.0

1.4

6.9
1.7
0.4
0.3
0.2
0.2
0.1
0.3
0.3
2.1
1.3
0.5
0.3
0.3
0.2
0.1
2.8
0.3
1.0
0.6
0.4
0.4
0.3
0.3
0.1
1.3
0.3

3
13
50
84
94
104
208
68
68
1
2
6
9
11
12
33
1
9
1
2
3
4
4
5
17
1
4

69.3
92.5
98.0
98.8
98.9
99.0
99.5
98.5
98.5
28.7
56.0
83.5
88.6
90.7
91.6
97.0
8.4
89.2
29.4
56.7
69.9
72.7
77.5
81.3
94.1
10.1
77.6

inf(%)
0.0
0.0
0.3
1.7
4.3
13.5
57.5
0.0
0.0
0.0
0.0
0.0
0.3
0.8
3.0
24.0
0.0
0.0
0.0
0.0
0.0
0.1
0.1
0.4
5.4
0.0
0.0

optgap(%)
0.0
0.0
0.0
0.1
0.3
0.8
2.1
1.3
1.3
0.0
0.0
0.0
0.0
0.1
0.2
1.7
0.0
0.7
0.0
0.0
0.0
0.0
0.0
0.0
1.1
0.0
0.4

Figures 3a-3d summarize the results with the optgap(%), inf(%), and timeimp for changing

c for instances with T = 90, 120 and f = 1, 000, 10, 000. Figures 3a-3d show that as the

level of predicted variables increases, the time improvement also increases at the price of an

increased optimality gap and infeasibility in the test set. The problems are harder when c = 3

(f = 10, 000) compared to c = 8 (f = 1, 000) for the same T . Predicting at lower levels

provides good results for harder problems with signiﬁcant time improvement without causing

much optimality gap and infeasibility, e.g., a time improvement factor of 13 is achieved with

the 50% prediction level without any infeasibility or optimality gap for instances with c = 3,

16

T = 120, and f = 10, 000 (Figure 3d). The time improvement factor is the highest when using

the 100% prediction. For instances with f = 1, 000, the full (100%) prediction results in less

than a 1.5% infeasibility. However, it could provide high levels of infeasibility in the test set

for harder problems with f = 10, 000. On the other hand, the 50% prediction level provides

over a time factor improvement of 3 and reduces the infeasibility to 0.01% and the optimality

gap to zero (Figure 3b). When f = 1, 000, time improvement increases signiﬁcantly with the

level of prediction, but the increase in the optimality gap and infeasibility is much less than the

counterpart instances with f = 10, 000, e.g., an infeasibility of 0.4% and optimality gap of zero

is obtained for instances with c = 5, T = 90, and f = 1, 000 compared to an infeasibility of

23.3% and optimality gap of 1.2% is observed for instances with c = 5, T = 90, and f = 10, 000,

using full predictions.

Figure 3: Summary of Results with optgap(%), inf(%), and timeimp

(a) T = 90, f = 1, 000

(b) T = 90, f = 10, 000

(c) T = 120, f = 1, 000

(d) T = 120, f = 10, 000

17

255075859095100MSUCpredicted(%)0.000.050.10optgap(%)255075859095100MSUCpredicted(%)0.00.51.01.5inf(%)255075859095100MSUCpredicted(%)0123timeimpc=3c=5c=8c=3c=5c=8255075859095100MSUCpredicted(%)0.00.51.01.5optgap(%)255075859095100MSUCpredicted(%)02040inf(%)255075859095100MSUCpredicted(%)01020timeimpc=3c=5c=8c=3c=5c=8255075859095100MSUCpredicted(%)0.000.050.10optgap(%)255075859095100MSUCpredicted(%)0.00.51.0inf(%)255075859095100MSUCpredicted(%)0123timeimpc=3c=5c=8c=3c=5c=8255075859095100MSUCpredicted(%)012optgap(%)255075859095100MSUCpredicted(%)0204060inf(%)255075859095100MSUCpredicted(%)0100200timeimpc=3c=5c=8c=3c=5c=8Figures 4a-4d show averages for changing c, f , and T , and the overall average. As the value

of c increases, the optimality gap, infeasibility, and time improvement generally decrease (Figure

4a). Figure 4b shows that the value of f has a signiﬁcant impact on results. The optimality gap

and infeasibility are signiﬁcantly lower when f = 10, 000, with lower-level predictions. Also, the

time factor improvement is signiﬁcantly greater at all prediction levels when f = 10, 000. Both

the optimality gap and time improvement are slightly higher when T = 120 compared to the

instances with T = 90. Figure 4d shows that using a prediction level of around 85% can balance

all evaluation metrics by providing a solution time factor improvement of 9.

Figure 4: Summary of Results with Diﬀerent Data Generation Parameters

(a) Averages for c

(b) Averages for f

(c) Averages for T

(d) Overall averages

Table 2 shows the averages presented in Tables 1, A2, A3, and A4 for the LSTM-Opt 85%

prediction level, the 100(MS), and the 100(UC). When f = 1, 000, the average infeasibility and

the optimality gap is zero with the 85% prediction level. The UC provides a similar average

18

255075859095100MSUCpredicted(%)0.00.51.0optgap(%)255075859095100MSUCpredicted(%)01020inf(%)255075859095100MSUCpredicted(%)0204060timeimpc=3c=5c=8c=3c=5c=8255075859095100MSUCpredicted(%)0.00.51.0optgap(%)255075859095100MSUCpredicted(%)01020inf(%)255075859095100MSUCpredicted(%)02040timeimpf=1,000f=10,000f=1,000f=10,000255075859095100MSUCpredicted(%)0.000.250.500.75optgap(%)255075859095100MSUCpredicted(%)051015inf(%)255075859095100MSUCpredicted(%)02040timeimpT=90T=120T=90T=120255075859095100MSUCpredicted(%)0.000.250.500.75optgap(%)255075859095100MSUCpredicted(%)051015inf(%)255075859095100MSUCpredicted(%)01020timeimptime improvement without infeasibility. When f = 10, 000, the average time improvement is

around 6 and 27 for the instances with T = 90 and T = 120, respectively, with the prediction

level of 85%. The infeasibility is slightly higher than the instances with f = 1, 000 since the

higher-level predictions increase percent infeasibility for the harder test instances. The UC

remedies the infeasibility problem and improves the solution time with a factor of 6 and 28 and

with an optimality gap of around 0.7% and 0.8% for the instances with T = 90 and T = 120,

respectively. Also, the UC outperforms the MS in time gain for all cases. When looking at

the overall averages in Table 2, the LSTM-Opt predictions at the 85% level reduce the CPLEX

solution time by a factor of 9 on average for over 240,000 test instances with an infeasibility

below 0.4% and an optimality gap of less than 0.05%. The UC provides a similar time gain

without any infeasibility and a slightly higher optimality gap of 0.4% than the 85% level of

prediction.

f
1,000

T
90

10,000

90

1,000

10,000

120

120

Avg.

1.8

0.4

timeCPX timeML timeimp timegain(%)

Table 2: Summary of averages in Tables 1, A2, A3, and A4
infeasible(%)
0.0
0.0
0.0
0.8
0.0
0.0
0.0
0.0
0.0
0.7
0.0
0.0
0.4
0.0
0.0

predicted(%)
85
100(MS)
100(UC)
85
100(MS)
100(UC)
85
100(MS)
100(UC)
85
100(MS)
100(UC)
85
100(MS)
100(UC)

18.3
3.1
25.5
83.3
-12.3
83.2
21.0
1.8
25.4
96.2
83.9
96.4
54.7
19.1
57.6

0.3
0.3
0.3
0.3
2.1
0.3
0.3
0.4
0.3
0.3
1.4
0.3
0.3
1.1
0.3

1
1
1
6
1
6
1
1
1
27
6
28
9
2
9

0.4

2.8

8.8

optgap(%)
0.0
0.0
0.0
0.1
0.0
0.7
0.0
0.0
0.0
0.1
0.4
0.8
0.0
0.1
0.4

In summary, the level of predictions used to get the best results varies notably between

datasets with diﬀerent characteristics. This level should be adjusted carefully considering the

trade-oﬀ between time gain, infeasibility, and optimality gap. Using an appropriate level of

predicted variables leads to major reductions in solution time up to an order of magnitude

without increasing any infeasibility or optimality gap. It is beneﬁcial to use lower prediction

levels for harder instances and higher prediction levels for easier instances, but a prediction

level of around 85% can be a reasonable level for all instances considered in this study. Also,

the UC outperforms the MS in terms of providing lower optimality gaps. The UC can also

be an alternative to the approach that uses predictions as constraints because it achieves zero

infeasibility at the cost of a slightly higher optimality gap. As the c, f , and T increase, i.e., the

instances get harder, and we observe a higher time factor improvement using the LSTM-Opt

19

framework, highlighting the potential of our approach for solving instances with varying sizes

and distributions, as discussed in the next section.

6.3 Results on Generalization

Here, we present the results on the generalization of our approach to instances with a larger

planning horizon T . It is not uncommon to have long production planning periods for industries

where a daily (even hourly) production planning is necessary, such as large-scale semiconduc-

tor manufacturing, and energy production Shrouf and Miragliotta (2015); Uzsoy et al. (1992).

Results on diﬀerent data distributions are presented in Appendix A3. We omit the results with

the MS approach in favor of UC due to its lack of performance. Generalization is a desired

property because it might be beneﬁcial to train the LSTM model in a relatively small horizon

to predict instances with a larger planning horizon, saving from the training time. Speciﬁcally,

the time to train the LSTM model is shorter than the training time for the instance for which

the prediction is made due to the smaller number of model parameters. We also compare our

framework with two other well-known ML algorithms (logistic regression and random forests)

and the state-of-the-art cutting plane algorithms proposed for the CLSP.

6.3.1 Predicting Instances with Longer Horizons

Table 3 presents the results for predicting datasets with longer planning horizons. The predic-

tions for a larger horizon are generated by concatenating the smaller LSTM predictions obtained

by the LSTM model, which has a shorter planning horizon. For example, in the second block

of rows in Table 3, the LSTM model with c = 3, f = 1, 000, and T = 90 is used to generate

predictions for the dataset with the same c and f , and T = 360. Here, four separate prediction

sets, each with 90 periods, are concatenated into a single set of predictions for generating a

prediction for the test set with T = 360.

For those instances, predicting 85% of variables results in a time improvement of 3, with a

0.3% optimality gap and zero infeasibility in the test set of 20,000 instances. The dataset with

c = 5, f = 10, 000, and T = 180 is predicted with the LSTM model trained using instances

with the same c and f but a half-length planning horizon of T = 90, as shown in the third

block of rows in Table 3. For those instances, predicting 50% of variables yields a signiﬁcant

time improvement of 9 and all feasible solutions in the test set at the cost of an optimality

gap, which is below 0.5%. The dataset with c = 8, f = 10, 000, and T = 480 constitutes the

hardest instances presented in Table 3 with the mean solution time over 40 seconds and are

predicted using the LSTM model trained with T = 120. Here, we observe signiﬁcant solution

20

time factor improvements over CPLEX using our LSTM-Opt framework. Predictions at the 75%

level reduce the CPLEX solution time by a factor of 25 with no infeasibility and an optimality

gap of 1%.

Table 3: Summary of generalization experiments to test datasets with longer plan-
ning horizons
LSTM Train

timeCPX timeML timeimp timegain

Test Data

optgap

pred

inf

c
3

f
1,000

T
90

c
3

f
1,000

T
180

3

1,000

90

3

1,000

360

5

10,000

90

5

10,000

180

8

10,000

120

8

10,000

480

0.5

1.2

19.0

42.4

(%)
25
50
75
85
90
95
100
100(UC)
25
50
75
85
90
95
100
100(UC)
25
50
75
85
90
95
100
100(UC)
25
50
75
85
90
95
100
100(UC)

0.5
0.4
0.4
0.4
0.3
0.3
0.1
0.3
1.0
0.7
0.5
0.4
0.4
0.3
0.1
0.4
6.2
2.2
0.7
0.4
0.3
0.2
0.1
0.4
11.6
4.3
1.7
0.8
0.5
0.4
0.1
0.6

1
1
1
1
2
2
6
2
1
2
2
3
3
3
12
3
3
9
28
46
59
78
166
49
4
10
25
55
86
112
364
71

(%)
7.6
20.5
29.1
33.3
38.2
44.2
82.4
36.2
19.7
41.6
59.6
64.2
66.4
71.4
91.9
62.8
67.2
88.4
96.4
97.8
98.3
98.7
99.4
98.0
72.7
89.9
96.1
98.2
98.8
99.1
99.7
98.6

(%)
0.0
0.0
0.0
0.0
0.0
0.1
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.2
1.3
0.0
0.0
0.0
0.1
0.3
0.9
2.8
27.5
0.0
0.0
0.0
0.0
0.0
0.1
0.4
5.2
0.0

(%)
0.1
0.1
0.2
0.2
0.2
0.2
0.4
0.4
0.1
0.2
0.2
0.3
0.3
0.3
0.5
0.5
0.3
0.5
0.5
0.6
0.7
0.9
3.5
1.6
0.6
0.9
0.9
0.9
1.0
1.0
2.8
1.5

∗Experiments include 20,000 test instances.

Table 4 presents the results for predicting datasets with signiﬁcantly longer planning hori-

zons; therefore, the test instances are much harder than the training instances. The predictions

are generated using the model trained with c = 8, f = 10, 000, and T = 120. The test sets

for all three datasets consist of 10 instances, instead of 20,000 as previously presented, due to

computational complexity and long solution times. For the ﬁrst dataset with T = 600, problems

are solved 70 times faster than the default CPLEX using predictions at the 50% level without

any infeasibility and with an optimality gap below 1%. The mean CPLEX solution time for the

next dataset with T = 720 is more than 8 CPU hours. Here, the solution time of 8 hours is

21

reduced to under 1 minute, with the predictions used at the 25% level without any infeasibility

and with an optimality gap below 1%. Predictions used at 75% reduce the solution time by

more than four orders of magnitude from more than 8 CPU hours to 2.5 CPU seconds without

infeasibility and with an optimality gap below 2%. The last test dataset with c = 8, f = 10, 000,

and T = 960 constitutes the hardest instances presented with a mean solution time of over 70

hours using CPLEX at its default settings. For those instances, predictions at the 25% level

reduce the solution time of 70 CPU hours to only 79 CPU seconds with an optimality gap of

0.8% and without any infeasibility. Predictions at the 50% level reduce the solution time by

more than a factor of 16,000, with an optimality gap of 1.2% and zero infeasibility.

Generating 100,000 instances for training data with c = 8, f = 10, 000, and T = 120 takes

140,170 seconds whereas the LSTM training time takes 52,724 seconds. In this speciﬁc example,

it can be concluded that generating training data, training the LSTM model, and resolving with

predictions for a single instance takes 16 hours less than solving the instance with CPLEX. For

such hard problems, our LSTM-Opt framework achieves a signiﬁcant time reduction even in the

case where just a single instance must be solved. The results discussed above highlight that our

approach could be generalizable to predict larger instances with substantial beneﬁts in reducing

the solution time of those hard CLSPs with the cost of a small optimality gap.

6.4 Comparison with other ML and exact algorithms

Table 5 presents the computational comparison of our LSTM-Opt framework with two other

machine learning approaches that perform a binary classiﬁcation task and shows that their

prediction quality is not comparable to LSTM-Opt. Additionally, the comparison with two

other exact approaches is presented in Table 6 to show that the LSTM-Opt framework could

produce good-quality solutions in much less time compared to those exact approaches. The

machine learning and exact approaches used to compare with the LSTM-Opt are deﬁned as

follows.

ML Approaches:

• Logistic Regression (LR): An extension of linear regression, is more interpretable than the

tree-based ensemble methods such as random forest at the cost of accuracy.

• Random Forest (RF): One of the best algorithms for classiﬁcation tasks (Fern´andez-

Delgado et al., 2014) in terms of classiﬁcation accuracy at the cost of reduced interpretabil-

ity.

Exact Approaches:

22

Table 4: Summary of generalization experiments to test datasets with longer plan-
ning horizons contd.∗

LSTM Train

Test Data

pred

timeCPX timeML

timeimp

timegain

inf

optgap

c
8

f
10,000

T
120

c
8

f
10,000

T
600

8

10,000

120

8

10,000

720

8

10,000

120

8

10,000

960

409

30,038

252,186

(%)
25
50
75
85
90
95
100
100(UC)
25
50
75
85
90
95
100
100(UC)
25
50
75
85
90
95
100
100(UC)

31.1
5.8
2.5
1.2
0.9
0.6
0.2
6.0
54.5
7.2
2.5
1.0
0.8
0.6
0.2
9.7
78.6
15.2
3.1
1.3
0.8
0.6
0.2
14.3

13
70
161
343
476
712
1,990
68
552
4,168
12,014
28,888
38,788
51,267
164,035
3,150
3,208
16,543
80,922
199,593
310,612
411,262
1,245,361
17,678

(%)
92.4
98.6
99.4
99.7
99.8
99.9
99.9
98.5
99.8
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

(%)
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
10.0
1.4
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

(%)
0.6
0.9
1.2
1.5
1.7
2.0
3.0
1.6
0.9
1.2
1.7
2.1
2.5
3.0
6.4
2.5
0.8
1.2
1.6
2.0
2.3
2.7
5.6
2.3

∗Experiments only include ten test instances due to long solution times.

• CPLEX (CPX): Direct solution of the CLSP formulation (1a)-(1e) with default CPLEX.

• Dynamic programming-based inequalities (DPineq) of Hartman et al. (2010): We used

the weaklu strategy to create a tighter CLSP polyhedron. The generated inequalities are

added to the formulation (1), and the proposed algorithm is shown to outpace the dynamic

programming algorithm by Hartman et al. (2010) for some cases. In the experiments, we

generate cuts for the ﬁrst 100 periods with c = 3, for the ﬁrst 75 periods with c = 5,

and for the ﬁrst 50 periods with c = 8 for instances with T = 360 to combat the growing

DP-based inequality generation time with increasing c.

• The ((cid:96), S) inequalities (LSineq) of Barany et al. (1984): Implemented with a separation

algorithm since the number of ((cid:96), S) inequalities grows exponentially. The separation algo-

rithm is iterated ﬁve times which is inclined to give the best computational achievements

(B¨uy¨uktahtakın et al., 2018).

• Dynamic programming (DP) solution approach for CLSP (Florian et al., 1980; Hartman

et al., 2010): Results are omitted from the tables due to lack of performance. For example,

while the mean solution times of c = 3, 5, 8 instances with CPLEX were 22.4, 3.3, and 1.3

23

seconds, respectively, the dynamic programming solution times were 610.7, 1054.9, and

1938.8 seconds for the same ﬁrst 20 instances presented in Table 1. Additionally, the

dynamic programming approach has a complexity of O(T D2

t=1 dt. We
do not further include the dynamic programming solution to compare with the LSTM-Opt

T ) where DT = (cid:80)T

framework since CPLEX is superior for the considered instances.

Table 5 presents a set of instances that are tested for the comparison of LSTM-Opt, LR,

and RF. Here, we have utilized a diﬀerent structure to generate our test instances. For the

instances with c = 3, 5, a solution time limit of 86,400 CPU seconds (24 CPU hours) is set for

CPLEX to restrict the solution time. The metrics, including time improvement, time gain, and

optimality gap, are calculated based on the best solution found by CPLEX within the solution

time limit. The IGap = 100 × (objCP X − objLP )/objCP X, where objCP X is the objective

function value of the best feasible solution to the original problem and objLP is the objective

function to its linear programming relaxation, is 7.5%, 16.1%, and 27.7% for the instances with

c = 3, 5, and 8, respectively. CPLEX reports an MIP optimality gap of 0.64%, 0.06%, and 0.00%

on average for test instances with c = 3, 5, and 8, respectively, with a one-day time limit. For

the same instances with c = 3, 5, our preliminary results revealed that the test problems were

still computationally very expensive to solve without a time limit with the 25% prediction level.

Therefore the results with the 25% prediction level are omitted from the results in this section.

In Table 5, for the c = 3 instances, predictions at the 50% level improve the solution time

by more than a factor of 7,500 by reducing the limited average solution time from one day to

only 12 seconds, without any infeasibility and with an optimality gap of 0.8%. Predictions of

more than 50% lead to some infeasibility in the test set, while the predictions at the 100%

level lead to all infeasible predictions, and thus the corresponding results are presented in a

“-” in the ﬁrst-row block of results in Table 5. For the same instances solved at the 50%

prediction level, LR and RF have caused an infeasibility of 70% and 50%, respectively, since

neither considers sequential dependency like the LSTM networks. The optimality gaps of the

feasible instances were signiﬁcantly higher than the 0.8% of the LSTM-Opt framework at 1.9%

and 2.3%, respectively, for both LR and RF. Also, the time improvements are not as big as the

ones of the LSTM-Opt framework. For the instances with c = 5, predictions using LSTM-Opt

at the 50% level decrease the limited solution time from 1-day to 20 CPU seconds and reduce the

solution time by more than a factor of 4,000, including the prediction generation time without

any infeasibility and with an optimality gap of 0.9%. The 85% level with LSTM-Opt reduces

the solution time by ﬁve orders of magnitude without infeasibility and with an optimality gap

of 2.3%. For the same prediction level, both the LR ad RF causes all infeasible predictions.

24

The datasets with c = 8 constitute signiﬁcantly faster to solve compared to instances with

c = 3, 5, and the results resemble the structure with easier instances. The LR and RF cause

high infeasibility in all prediction levels compared to the LSTM-Opt framework. Table 5 shows

that a signiﬁcant solution time reduction of around four to ﬁve orders of magnitude is achieved

by LSTM-Opt without any infeasibility depending on the desired optimality gap. We anticipate

that the solution time gains would have further increased if the original solution time was not

limited to 24 hours. Additionally, both the LR and RF cause higher infeasibility, a higher

optimality gap, and a lower time improvement.

Table 6 presents a comparison of LSTM-Opt at the 50% prediction level with two exact ap-

proaches, namely ((cid:96), S) valid inequalities (LSineq) and DP-based cutting planes (DPineq). The

solution time is limited by 1 hour, including the inequality generation time for both approaches.

Here, the time improvement metric has been calculated with respect to the set solution time of

1-hour. The optimality gap metric has been calculated with respect to the best integer solution

found by the CPLEX within a 1-day solution time limit. For the c = 3 instances, the LSTM-

Opt framework achieves a 1.6% optimality gap. The objective function value is reduced below

the 1-day limited CPLEX solution value with a 1-hour time limit in both formulations with

DP-based and ((cid:96), S) inequalities, resulting in a negative optimality gap of -0.01%. Therefore,

both types of inequalities are eﬀective at reducing the optimality gap, but they can not solve

the CLSP very fast though they speed up the solution for harder instances of c = 3, 5. Even

though the LSTM-Opt framework has an optimality gap of 0.8%, the solution was found 322

times faster than CPLEX, showing that the LSTM-Opt can solve those problems very fast. The

results with c = 5 show a similar pattern, and the LSTM-Opt framework has an optimality gap

of 0.9% on top of the CPLEX gap. The results with c = 8 show that LSTM-Opt framework can

achieve a time improvement of 3 while inequality-based methods increase the solution time for

easier instances.

Figures 5a and 5a present a summary of the results for the instances presented in Tables 5

and 6, for c = 3 and 5, respectively. Both DPineq and LSineq improve over the CPLEX gap in

Figure 5a with c = 3 instances. LSTM achieves a solution with a slightly larger optimality gap

much faster than those exact inequality methods and also is faster than LR and RF with a lower

gap. The latter two algorithms result in high infeasibility of 70% and 50%, respectively, while

LSTM-Opt achieves all-feasible predictions. The results show similarity for the c = 5 instances

in Figure 5b, with the exception that both exact methods do not improve over the CPLEX gap

or solution time. Even though LR and RF have a higher time improvement than LSTM-Opt,

both lead to high and unpractical infeasibility rates of 60% and 40%, respectively.

25

)

%
(
p
a
g
t
p
o

)

%

(
f
n
i

p
m
i
e
m

i
t

e
m

i
t

)

%
(
p
a
g
t
p
o

)

%

(
f
n

i

p
m
i
e
m

i
t

e
m

i
t

)

%
(
p
a
g
t
p
o

)

%

(
f
n

i

p
m
i
e
m

i
t

e
m

i
t

X
P
C
e
m

i
t

d
e
r
p

t
s
e
T

n
i
a
r
T

∗
s

m
h
t
i
r
o
g
l
a

L
M

r
e
h
t
o

h
t
i
w
t
p
O
M
T
S
L

-

g
n
i
r
a
p
m
o
c

r
o
f

s
t
l
u
s
e
r

l
a
n
o
i
t
a
t
u
p
m
o
C

:
5

e
l
b
a
T

3
.
2

-

-

-

-

-

9
.
1

2
.
4
2

-

-

-

-

0
.
1

5
.
2
2

-

-

-

-

0
.
0
5

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
4

0
.
0
9

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
3

0
.
0
9

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

F
R

9
2
1
,
1

6
.
6
7

-

-

-

-

-

-

-

-

-

-

9
.
1

3
.
7
1

-

-

-

-

1
9
4
,
6

5
1
4
,
5
4
1

4
.
3
1

6
.
0

4
.
1

8
.
5
2

-

-

-

-

2

8

-

-

-

-

-

-

-

-

1
.
3

7
.
0

-

-

-

-

-

-

-

-

5
.
0

3
.
4
1

1
.
1
1
1

-

-

-

0
.
0
7

0
.
0
9

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
6

0
.
0
8

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

0
.
0
4

0
.
0
8

0
.
0
9

0
.
0
0
1

0
.
0
0
1

0
.
0
0
1

R
L

4
2
0
,
2

9
5
4
,
6
4
1

7
.
2
4

6
.
0

-

-

-

-

-

-

-

-

2
5
5
,
0
1

3
8
8
,
4
2
1

2
.
8

7
.
0

-

-

-

-

2

8

4
1

-

-

-

-

-

-

-

2
.
3

7
.
0

4
.
0

-

-

-

8
.
0

0
.
2

9
.
2

6
.
3

5
.
4

-

9
.
0

6
.
1

3
.
2

7
.
2

3
.
3

2
.
4
1

6
.
1

9
.
1

2
.
2

4
.
2

8
.
2

8
.
3

-

t
p
O
M
T
S
L

0
.
0

0
.
0
1

0
.
0
2

0
.
0
3

0
.
0
5

5
7
6
,
7

9
8
1
,
3
6

2
4
0
,
9
3
1

7
4
6
,
9
5
1

3
0
4
,
3
9
1

0
.
0
0
1

-

0
.
0

0
.
0

0
.
0

0
.
0

0
.
0
1

0
.
0
6

0
.
0

0
.
0
1

0
.
0
1

0
.
0
2

0
.
0
2

0
.
0
2

3
5
2
,
4

1
7
3
,
4
3

7
2
7
,
2
0
1

9
8
1
,
8
1
1

5
6
8
,
0
9
1

0
8
6
,
6
1
4

3

8

1
1

3
1

6
1

8
3

4
.
1

6
.
0

5
.
0

4
.
0

-

3
.
1
1

5
1
4
,
6
8

4
.
0
2

3
6
7
,
6
8

5
.
2

8
.
0

7
.
0

5
.
0

2
.
0

6
.
2

0
.
1

7
.
0

6
.
0

5
.
0

2
.
0

5
.
7

)

%

(

0
5

5
7

5
8

0
9

5
9

0
0
1

0
5

5
7

5
8

0
9

5
9

0
0
1

0
5

5
7

5
8

0
9

5
9

0
0
1

T

0
6
3

T

0
9

f

0
0
0
,
0
1

c

3

0
6
3

0
9

0
0
0
,
0
1

5

0
6
3

0
9

0
0
0
,
0
1

8

)

%
(
p
a
g
t
p
o

∗
s
d
o
h
t
e
m

t
c
a
x
e

t
n
e
r
e
ﬀ
i
d

h
t
i
w
t
p
O
M
T
S
L

-

g
n
i
r
a
p
m
o
c

r
o
f

s
t
l
u
s
e
r

l
a
n
o
i
t
a
t
u
p
m
o
C

:
6

e
l
b
a
T

p
m
i
e
m

i
t

e
m

i
t

f
n

i

d
e
r
p

t
s
e
T

n
i
a
r
T

-

t
p
O
M
T
S
L

q
e
n
i
S
L

q
e
n
i
p
D

X
P
C

-

t
p
O
M
T
S
L

q
e
n
S
L

i

q
e
n
p
D

i

-

t
p
O
M
T
S
L

q
e
n
S
L

i

q
e
n
p
D

i

8
.
0

9
.
0

6
.
1

1
.
0
-

0
.
0

0
.
0

1
.
0
-

0
.
0

0
.
0

3
0
.
0

5
0
.
0

0
0
.
0

2
2
3

7
7
1

3

1

1

0

1

1

0

2
.
1
1

3
.
0
2

5
.
2

2
0
6
,
3

0
0
6
,
3

6
4
1

9
0
6
,
3

1
2
4
,
3

0
2
7

X
P
C

2
0
6
,
3

2
0
6
,
3

5
.
7

)

%

(

0
.
0

0
.
0

0
.
0

0
5

0
5

0
5

T

0
6
3

0
6
3

0
6
3

T

0
9

0
9

0
9

f

0
0
0
,
0
1

0
0
0
,
0
1

0
0
0
,
0
1

c

3

5

8

.
s
e
m

i
t

n
o
i
t
u
l
o
s

g
n
o
l

o
t

e
u
d

t
i

m

i
l

e
m

i
t

r
u
o
h
-
1

h
t
i
w
d
e
t
i

m

i
l

d
n
a

s
e
c
n
a
t
s
n
i

t
s
e
t

n
e
t

e
d
u
l
c
n
i

y
l
n
o

s
t
n
e
m

i
r
e
p
x
E

∗

.
s
e
m

i
t

n
o
i
t
u
l
o
s

g
n
o
l

o
t

e
u
d

s
e
c
n
a
t
s
n

i

t
s
e
t

n
e
t

e
d
u

l
c
n

i

y
l

n
o

s
t
n
e
m

i
r
e
p
x
E

∗

26

Figure 5: Comparison of exact and ML algorithms

(a) Instances with c = 3, f = 10, 000, and T = 360

(b) Instances with c = 5, f = 10, 000, and T = 360

6.5 Summary of Results

The results presented on generalization experiments show that a network trained on a smaller

planning horizon can be used to successfully predict the optimal solutions of the instances with

larger horizons without any additional training. The solution time can be reduced up to six

orders of magnitude without increasing the optimality gap or infeasibility much, especially in

harder problems. Also, LSTM-Opt can capture sequential dependencies while LR and RF can

not. Classical exact approaches can not produce very fast solutions like the LSTM-Opt.

Figures 6a-6f present the results for datasets for longer planning horizons. The results for

T = 360 in Figure 6a are similar to dataset where LSTM model is trained with c = 3, f = 1, 000,

and T = 90, as shown in Figure 3a. For the dataset with c = 8, f = 10, 000, and T = 720 in

Figure 6c, using predictions at the 50% level reduces the solution time from more than 8 hours

to under 8 seconds without any infeasibility and with an optimality gap of 1.2%. Figure 6d

constitutes the instances with the longest solution times. The instances with c = 8, f = 10, 000,

and T = 960 have a mean solution time over 70 hours. Predictions at the 25% and 50% levels

reduce the solution time of those instances by more than a factor of 3,000 and 16,000 with an

optimality gap of 0.8% and 1.2%, without any infeasibility in the test set, respectively.

Overall averages in Table 4 show that the solution time can be decreased with a factor of

more than 9,000 with an infeasibility in the test set of only 0.5% and an optimality gap of

approximately 2.1%. The UC reduces the solution time by more than a factor of 90,000 on aver-

age without infeasibility in the test set and an average optimality gap of around 3.5%. Overall,

predictions at the levels between 25% and 85% provide signiﬁcant time improvements with less

than a 1% optimality gap and without any infeasibility in the test set. Speciﬁcally, predictions

27

CPXLSDPLSTMLRRFMethodology0100200300timeimpinf: 0%inf: 70%inf: 50%012optgap(%)CPXLSDPLSTMLRRFMethodology0100200300400timeimpinf: 0%inf: 60%inf: 40%0.00.81.6optgap(%)at the 85% level can balance a high solution time factor improvement with infeasibility and

optimality gap at reasonable levels when predicting for longer periods using the LSTM model

trained with instances of shorter and thus easier instances.

7 Conclusions and Future Work

In this study, we present a new LSTM-Opt framework to predict the optimal solution of the

CLSP, a fundamental production planning problem in various industry settings. Our ML ap-

proach could be beneﬁcial in reducing the solution time for many practical problems that are

solved repeatedly with diﬀerent parameters. We utilize bidirectional LSTMs to process infor-

mation in both time directions. The metrics, deﬁned as time factor improvement, infeasibility,

and optimality gap, are presented to assess the quality of the predictions. The results for the

CLSP instances with the same characteristics show that a time factor improvement of more

than an order of magnitude can be achieved without much loss in feasibility or the optimality

gap if the level of predictions used to solve the problem is well adjusted. Also, we tested if the

trained LSTM models could generalize to instances with diﬀerent data distributions or longer

planning horizons. The results show that one should be careful in selecting the prediction level

for predicting instances with diﬀerent data distributions. The LSTM models trained on shorter

planning horizons achieve great success in predicting instances with longer planning horizons

with any prediction level and reduce the solution time up to six orders of magnitude with a small

optimality gap. Speciﬁcally, we observe the highest computational beneﬁt from our LSTM-Opt

approach when predicting the hardest set of instances. Also, LSTM-Opt framework outperforms

classical ML algorithms in terms of the quality of the solution and exact approaches with respect

to the solution time improvement.

Our LSTM-Opt framework can be especially useful for reducing the solution time of dynamic

combinatorial optimization problems that are solved in a repetitive setting. In this paper, we

have used the CLSP as a speciﬁc case to show that deep learning approaches have great potential

for learning optimal solutions to MIP problems. Future research could further investigate the

generalizability of our approach to instances with a larger planning horizon and diﬀerent distri-

butions in more detail. Another possible research direction is to develop methods to eliminate

infeasible predictions. Additionally, the developed LSTM-Opt framework can be extended to

solve more complex versions of the CLSP, such as the multi-item or multi-level CLSP, as well

as other sequential decision-making problems.

28

Figure 6: Summary of Generalization Experiments

(a) LSTM trained with c = 3, f = 1, 000, T = 90
predicts c = 3, f = 1, 000, T = 360

(b) LSTM trained with c = 5, f = 10, 000, T = 90
predicts c = 5, f = 10, 000, T = 180

(c) LSTM trained with c = 8, f = 10, 000, T = 120
predicts c = 8, f = 10, 000, T = 720

(d) LSTM trained with c = 8, f = 10, 000, T = 120
predicts c = 8, f = 10, 000, T = 960

(e) LSTM trained with c = 3, f = 10, 000, T = 90
predicts c = 3, f = 10, 000, T = 360

(f) LSTM trained with c = 5, f = 10, 000, T = 90
predicts c = 5, f = 10, 000, T = 360

29

255075859095100UCpredicted(%)0.20.4optgap(%)255075859095100UCpredicted(%)0.00.51.0inf(%)255075859095100UCpredicted(%)0510timeimp255075859095100UCpredicted(%)123optgap(%)255075859095100UCpredicted(%)01020inf(%)255075859095100UCpredicted(%)050100150timeimp255075859095100UCpredicted(%)246optgap(%)255075859095100UCpredicted(%)0510inf(%)255075859095100UCpredicted(%)050000100000150000timeimp255075859095100UCpredicted(%)24optgap(%)255075859095100UCpredicted(%)0inf(%)255075859095100UCpredicted(%)05000001000000timeimp5075859095UCpredicted(%)24optgap(%)5075859095UCpredicted(%)02040inf(%)5075859095UCpredicted(%)0100000200000timeimp5075859095100UCpredicted(%)510optgap(%)5075859095100UCpredicted(%)0204060inf(%)5075859095100UCpredicted(%)0200000400000timeimpAcknowledgment

We gratefully acknowledge the support of the National Science Foundation CAREER Award

co-funded by the CBET/ENG Environmental Sustainability program and the Division of Math-

ematical Sciences in MPS/NSF under Grant No. CBET-1554018.

References

Atamt¨urk, A. and Mu˜noz, J. C. (2004). A study of the lot-sizing polytope. Mathematical

Programming, 99(3):443–465.

Barany, I., Van Roy, T. J., and Wolsey, L. A. (1984). Strong formulations for multi-item

capacitated lot sizing. Management Science, 30(10):1255–1261.

Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient

descent is diﬃcult. IEEE transactions on neural networks / a publication of the IEEE Neural

Networks Council, 5:157–66.

Bertsimas, D. and Stellato, B. (2019). Online mixed-integer optimization in milliseconds. arXiv

preprint arXiv:1907.02206.

Bishop, C. M. et al. (1995). Neural networks for pattern recognition. Oxford university press.

Bitran, G. R. and Yanasse, H. H. (1982). Computational complexity of the capacitated lot size

problem. Management Science, 28(10):1174–1186.

Bonami, P., Lodi, A., and Zarpellon, G. (2018). Learning a classiﬁcation of mixed-integer

quadratic programming problems.

In van Hoeve, W.-J., editor, Integration of Constraint

Programming, Artiﬁcial Intelligence, and Operations Research, pages 595–604, Cham.

Springer International Publishing.

B¨uy¨uktahtakın, ˙I. E. and Liu, N. (2016). Dynamic programming approximation algorithms for

the capacitated lot-sizing problem. Journal of Global Optimization, 65(2):231–259.

B¨uy¨uktahtakın, ˙I. E., Smith, J. C., and Hartman, J. C. (2018). Partial objective inequalities for

the multi-item capacitated lot-sizing problem. Computers & Operations Research, 91:132–

144.

Copil, K., W¨orbelauer, M., Meyr, H., and Tempelmeier, H. (2017). Simultaneous lot-sizing and

scheduling problems: a classiﬁcation and review of models. OR spectrum, 39(1):1–64.

30

Eppen, G. D. and Martin, R. K. (1987). Solving multi-item capacitated lot-sizing problems

using variable redeﬁnition. Operations Research, 35(6):832–848.

Fern´andez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds

of classiﬁers to solve real world classiﬁcation problems? The journal of machine learning

research, 15(1):3133–3181.

Fischetti, M. and Fraccaro, M. (2019). Machine learning meets mathematical optimization to

predict the optimal production of oﬀshore wind parks. Computers & Operations Research,

106:289–297.

Florian, M., Lenstra, J. K., and Rinnooy Kan, A. (1980). Deterministic production planning:

Algorithms and complexity. Management science, 26(7):669–679.

Gicquel, C., Minoux, M., and Dallery, Y. (2008). Capacitated lot sizing models: a literature

review.

Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. http://www.

deeplearningbook.org.

Graves, A. (2012). Supervised Sequence Labelling with Recurrent Neural Networks. Springer

Berlin Heidelberg.

Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classiﬁcation with bidirectional

lstm networks. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks,

2005., volume 4, pages 2047–2052 vol. 4.

Hartman, J. C., B¨uy¨uktahtakın, ˙I. E., and Smith, J. C. (2010). Dynamic-programming-based

inequalities for the capacitated lot-sizing problem. IIE Transactions, 42(12):915–930.

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation,

9:1735–80.

ILOG, I. (2016). Cplex optimizer 12.7. 0.

Karimi, B., Ghomi, S. F., and Wilson, J. (2003). The capacitated lot sizing problem: a review

of models and algorithms. Omega, 31(5):365–378.

Khalil, E. B., Bodic, P. L., Song, L., Nemhauser, G., and Dilkina, B. (2016). Learning to branch

in mixed integer programming. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial

Intelligence, AAAI’16, page 724–731. AAAI Press.

31

Khalil, E. B., Dilkina, B., Nemhauser, G. L., Ahmed, S., and Shao, Y. (2017). Learning to run

heuristics in tree search. In Proceedings of the Twenty-Sixth International Joint Conference

on Artiﬁcial Intelligence, IJCAI-17, pages 659–666.

Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization.

Kruber, M., L¨ubbecke, M., and Parmentier, A. (2017). Learning when to use a decomposition.

pages 202–210.

Larsen, E., Lachapelle, S., Bengio, Y., Frejinger, E., Lacoste-Julien, S., and Lodi, A. (2021).

Predicting tactical solutions to operational planning problems under imperfect information.

INFORMS Journal on Computing.

LeCun, Y. A., Bottou, L., Orr, G. B., and M¨uller, K.-R. (2012). Eﬃcient backprop. In Neural

networks: Tricks of the trade, pages 9–48. Springer.

Lodi, A. and Zarpellon, G. (2017). On learning and branching: a survey. TOP, 25(2):207–236.

Oroojlooyjadid, A., Snyder, L. V., and Tak´aˇc, M. (2019). Applying deep learning to the newsven-

dor problem. IISE Transactions, 52(4):444–463.

Pochet, Y. and Wolsey, L. A. (2006). Production planning by mixed integer programming.

Springer Science & Business Media.

Quadt, D. and Kuhn, H. (2007). Capacitated lot-sizing with extensions: a review. 4OR, 6(1):61–

83.

Schuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks. Signal Processing,

IEEE Transactions on, 45:2673 – 2681.

Shrouf, F. and Miragliotta, G. (2015). Energy management based on internet of things: practices

and framework for adoption in production management. Journal of Cleaner Production,

100:235–246.

Smith, K. A. (1999). Neural networks for combinatorial optimization: A review of more than a

decade of research. INFORMS Journal on Computing, 11(1):15–34.

Uzsoy, R., Lee, C.-Y., and Martin-Vega, L. A. (1992). A review of production planning and

scheduling models in the semiconductor industry part i: system characteristics, performance

evaluation and production planning. IIE transactions, 24(4):47–60.

32

Xavier, A. S., Qiu, F., and Ahmed, S. (2019). Learning to solve large-scale security-constrained

unit commitment problems.

33

