Hierarchical Program-Triggered Reinforcement
Learning Agents For Automated Driving

Briti Gangopadhyay Harshit Soora Pallab Dasgupta*
Department of Computer Science and Engineering, IIT Kharagpur, India
{briti gangopadhyay,soora}@iitkgp.ac.in, pallab@cse.iitkgp.ac.in

1

1
2
0
2

r
a

M
5
2

]
I

A
.
s
c
[

1
v
1
6
8
3
1
.
3
0
1
2
:
v
i
X
r
a

Abstract—Recent advances in Reinforcement Learning (RL)
combined with Deep Learning (DL) have demonstrated im-
pressive performance in complex tasks, including autonomous
driving [1]. The use of RL agents in autonomous driving leads
to a smooth human-like driving experience, but the limited
interpretability of Deep Reinforcement Learning (DRL) creates
a veriﬁcation and certiﬁcation bottleneck. Instead of relying
on RL agents to learn complex tasks, we propose HPRL -
Hierarchical Program-triggered Reinforcement Learning, which
uses a hierarchy consisting of a structured program along with
multiple RL agents, each trained to perform a relatively simple
task. The focus of veriﬁcation shifts to the master program under
simple guarantees from the RL agents, leading to a signiﬁcantly
more interpretable and veriﬁable implementation as compared
to a complex RL agent. The evaluation of
the framework
is demonstrated on different driving tasks, and NHTSA pre-
crash scenarios using CARLA, an open-source dynamic urban
simulation environment.

I. INTRODUCTION

There has been a steady increase in the development of
self-driving cars as they possess the potential to radically
change the future of mobility. Deriving safe driving policies
remains a key challenge in achieving deployable autonomous
driving systems. The formulation of driving strategies has been
studied by three schools of work, namely rule-based methods,
imitation based learning, and reinforcement learning.

Rule-based methods for behaviour and motion planning
have been studied and developed for decades. They rely on
symbolic computation techniques such as ﬁnite state machines
as used by [2, 3] in the DARPA challenges or classical plan-
ning algorithms [4, 5]. However, rule-based methods require
the driving problem to be modelled in terms of domain-speciﬁc
rules in an underlying logical language. This often is not
feasible for autonomous driving where the environment is
dynamic as well as stochastic and can give rise to an enormous
number of driving scenarios. The manual encoding of logical
rules also has high human involvement.

To minimize the human component in the formulation of
driving policies, imitation based learning techniques have been
explored [6, 7]. The goal of imitation learning is to mimic
driving behaviour from data that has been extracted from
human driving in a supervised fashion. The key idea is to map
sensor information like camera images to some indicators that
can directly be translated into control values such as throttle

*The authors would like to thank DST, Govt of India, and TCS Research

Scholarship for partial ﬁnancial support of this project.

and steering angle by ﬁtting a function on available driving
data [8]. However, imitation learning has a high reliance on
a huge amount of annotated driving data, is challenging to
scale, and the control policies learnt are only as good as the
data available.

Reinforcement learning (RL) eliminates the requirement of
prior knowledge and labelled data as it learns policies by trial
and error while trying to optimize a cumulative future reward
function. These algorithms have displayed superhuman perfor-
mance in games like GO [9]. Classical reinforcement learn-
ing techniques, both model-based and model-free such as Q
learning cannot be adopted directly to solve complex domains
such as autonomous driving as they have large state/action
space. Hence, deep learning techniques have played a vital role
where an approximation of the Q function or the RL policy
is learned implicitly by a neural network-based architecture.
RL techniques leveraging deep learning have been successfully
used to learn stochastic policies in simulation environment like
TORCS [10, 11].

Driving is a sequence of complex manoeuvres which often
leads to the problem of receiving sparse rewards when it
is modelled solely as a RL problem. Deep learning tech-
niques are known to suffer from a cold start and require
extensive amounts of training for converging to reasonable
policy. Also, because of the non-interpretable and opaque
nature of neural networks, functional safety, which is a vital
requirement for certiﬁcation of safety-critical systems, cannot
be guaranteed [12].

Human beings often use a delicate combination of traf-
ﬁc/road rules and experience while driving. The human driving
policies are semantic rather than being geometric and numeri-
cal, for example, “drive straight till the junction and turn right”
rather than “drive 50 meters at the current speed and steer
90o” [13]. These semantic instructions have sub-goals which
can be contoured into hierarchical abstractions. The sub-goals,
in turn, are heavily regulated by symbolic rules that are applied
at various points of time and space (for example, stop at the
red light). Inspired by this, we propose a Hierarchical Program
triggered Reinforcement Learning (HPRL) framework for ex-
ecuting autonomous driving tasks. We asynchronously train
reinforcement learning agents for learning different driving
manoeuvres using manoeuvre speciﬁc actions, observation
space and reward signals, making them more sample efﬁcient
than a single ﬂat DRL policy. The agents are then triggered
in execution by a symbolic rule-based system interpreted in
terms of a structured program to complete a driving task.

 
 
 
 
 
 
2

(a)

(b)

Figure 1: (a) The Hierarchical Program triggered Reinforcement Learning (HPRL) Framework. The right side of the ﬁgure depicts the
asynchronous training of RL agents with individual observation/action space and reward signal. The left part depicts the program-controlled
execution of a driving task where the program checks for functional safety and triggers RL agents as per route plan generated by the
mission planner or as per control strategy recommended by safety module. (b) State machine for right turn manoeuvre at a 4-way trafﬁc
light controlled intersection without any dynamic objects as described in example IV-A

Functional safety requirements are incorporated as embedded
assertions in the program, thereby facilitating a formal proof
of the correctness of the driving strategy. The framework has
been tested on NHTSA pre-crash scenarios in CARLA [14].
In summary, this work makes the following novel contribu-
tions:

• Training a

right turn,

set of generic manoeuvres

containing
the behaviours drive straight,
left turn,
change left lane, change right lane, using model-free
value-based Deep Q Learning (DQN) and policy
optimization-based Deep Deterministic Policy Gradient
(DDPG) networks with manoeuvre speciﬁc constrained
action and state spaces and reward signals.

• Decomposing driving tasks in terms of the generic ma-
noeuvres and controlling them using a structured Program
P to satisfy the overall driving intent while shielding the
RL agents with respect to the safety speciﬁcations ϕ. The
correctness of P with respect to ϕ is veriﬁed using the
python program veriﬁcation tool Nagini [15].

The paper is organized as follows. Section II discusses related
work. Section III presents the preliminaries and the problem
statement, Section IV presents the design methodology, and
Section V presents details on the veriﬁcation methodology and
experimental work. Section VI provides concluding remarks.

II. RELATED WORK
The growth of deep reinforcement learning has given trac-
tion to the development of self-driving policies in the absence
of labelled data. However,
the non-interpretable and non-
explainable nature of deep learning techniques do not make
them suitable for use in safety-critical domains. As a result,
the neuro-symbolic research community has been working
towards merging the classical symbolic computation based
techniques with deep learning, making them more sample
efﬁcient and interpretable. We explore each of these relevant
areas of research in more details.

A. Reinforcement Learning in Autonomous Driving

The competitive and cooperative nature of driving tasks
makes model-free reinforcement learning a suitable choice for
learning driving policies as the model is not ﬁxed apriori.
In [16] it has been shown how a Markov Decision Process
(MDP) formulation can be used for self-driving cars in a
highway with state discretization. Several prior works attempt
to learn a complete policy directly from perception and sensor
data [17, 18, 19, 20]. This approach is both computationally
expensive and time-consuming. Recently there has been a shift
from learning entire policies using vanilla deep reinforcement
learning, to decomposing the policies into sub-policies and
learning them via Hierarchical Deep Reinforcement Learning
[21]. The use of an option graph in a multi-agent setting has
been explored in [12] and tested on a lane negotiation scenario.
In this case, the reinforcement learning agents are only trained
to learn desires such as comfort. In [22, 23], a hierarchical re-
inforcement learning framework has been proposed for multi-
lane autonomous driving, where both behaviour and motion
planners are trained using deep RL, but this does not enable
us to provide any guarantees of functional safety. [24] Use
Hierarchical DRL to learn sub-policies related to lane-change
with temporal and spatial attention to image data, while [25]
uses HRL in trajectory planning. The RL agents in these works
are limited to the manoeuvres, switch lane left, switch lane
right, and keep lane.
DRL, though extensively used to approximate policies for
complex systems like autonomous drive in simulation, are
impeded in practical control applications for well-known draw-
backs like sample complexity, sensitivity to hyper-parameters,
network architecture and reward signal, interpretability and
safety [26]. Our work investigates whether we can retain
equivalent performance, as compared to using a single com-
plex RL policy, by breaking the policy into multiple sub-
policies and training individual RL agents for each of them

facilitating sample efﬁciency. A deterministic supervisory con-
troller designed as a structured program triggers RL agents
during execution. This enables us in establishing safety guar-
antees over the structured program rather than the RL policies.

B. Symbolic computation and Reinforcement Learning

An important direction of modern AI is to ﬁnd synergistic
combinations of classical AI, which uses symbolic reasoning,
and connection-based AI, which uses deep neural networks.
Hierarchical reinforcement learning guided exploration using
AI planning has been studied in [27, 28, 29]. Planning guided
reinforcement
learning drastically reduces the exploration
space, thereby handling the problem of cold start. The planning
framework requires the environment to be predeﬁned in terms
of logical rules. A recent study [30] leverages information
from path planners by integrating distance to closest way-
point as a part of RL state space. Our present contribution
is most closely related to [31], where given a task in a
natural language, ambiguities are resolved by a structured
formal program which triggers a single agent to fulﬁl the
task. On the other hand, we decompose driving tasks into a
set of sub-goals and train different agents for each of these
goals, and therefore our use case and our approach is very
different from that of [31]. Moreover, we demonstrate the
veriﬁability of our designs by translating functional safety
requirements into assertions over the structured program. Our
framework works on a continuous space discrete/continuous
action problem using a driving simulator.

III. PROBLEM STATEMENT

A ﬁnite horizon discounted Markov Decision Process

(MDP) is deﬁned over the tuple (cid:104)S, A,P a

ss(cid:48) , R, γ(cid:105) where:

• S is the set of states,
• A denotes the entire set of low-level actions an agent can

• P a

perform,
ss(cid:48) denotes the probability of transition from state s ∈ S
to state s
∈ S by taking action a ∈ A. This transition
probability is not explicitly deﬁned in model free RL.

(cid:48)

• R denotes the reward signal.
• γ, 0 ≤ γ ≤ 1 is the discount factor.

Given an MDP, the goal of Reinforcement Learning (RL) is to
ﬁnd a policy π : S → A that maximizes the reward signal over
the ﬁnite horizon. In Hierarchical Reinforcement Learning
(HRL), a task is solved by decomposing it into a series of sub-
goals using temporal abstractions. In [32] options with three
components are considered, namely a policy π : S × A →
[0,1], a termination condition β : S → [0,1], and an initiation
set I ⊆ S. An option (I, π, β) is available in state st iff
st ∈ I. In our framework, the initiation set and the terminating
conditions of an option is decided by a program P. P triggers
RL agents trained for different generic driving manoeuvres to
perform the overall task. The lower level RL agents orchestrate
with each other via the meta controller program to satisfy the
global intent of the driving task.

Our aim is to develop a veriﬁable framework where we
decompose a driving task T into a set of sub-tasks t ∈ T .

3

For each sub-task, a set of RL agents X is pre-trained. Each
xt is only exposed to a subset of relevant actions at ∈ A and
trained using reward signal rt, so that the training is sample-
efﬁcient. Program P triggers policy πt of agent xt ∈ X with
initialisation states It and terminating condition βt depending
on the way-points generated by the mission planner. P also
has embedded safety speciﬁcations ϕ for shielding the agents
X from taking unsafe actions. Agents from X are triggered
until task T is completed (Fig. 1a shows the framework).

IV. METHODOLOGY OVERVIEW

We provide the methodology overview using a running
example. Section IV-A outlines a simple driving task as an
example. Section IV-B deﬁnes the state space, action restric-
tions and reward function for behaviour speciﬁc DRL agents.
Section IV-C develops the formal safety speciﬁcation, and
Section IV-D outlines the veriﬁcation methodology.

A. Four-Way Intersection Scenario

We consider a simple scenario of a straight drive and right
turn on a 4-way trafﬁc light controlled intersection. We refer
to the subject vehicle, namely the one controlled by HPRL as
the ego vehicle. The driving task, T , is to drive the ego vehicle
from location A to location B, which involves the following
sub-tasks as deﬁned in natural language:

1) Sub-Task t1 : The ego vehicle should drive straight while
maintaining lane and desired speed till the junction.
2) Sub-Task t2 : The ego vehicle must stop on observing a
red light and remain stationary until the light turns green.
3) Sub-Task t3 : The ego vehicle should take a right turn to

reach the destination (for this manoeuvre).

The desired behaviour can be represented as the state machine
in Fig. 1b. A program P derived according to the state machine
ﬁrst triggers the RL agent x1 that controls the straight driving
of the ego vehicle while tracking speed and maintaining lane
for sub-task t1. The ego vehicle decelerates as it approaches
the intersection. When the ego vehicle stops at the intersection
P checks the trafﬁc light and keeps the vehicle stationary
until the light turns green. This behaviour is according to
the rules of the road and does not require learning. Once
the light turns green P initiates the right turning RL agent
x2 and completes sub-task t3, thereby completing the global
driving task T . An inherent advantage of using a hybrid
hierarchical framework like this is that certain safety properties
can be formally asserted, such as the ego vehicle must remain
stationary at a red light until the light turns green. This kind
of functional safety guarantees cannot be easily obtained in
ﬂat or hierarchical RL policies.

B. Reinforcement Learning Agents

The proposed HPRL platform maintains a library of simple
manoeuvres and corresponding pre-trained RL agents. For
example, let us consider the following set of manoeuvres:

• Agent drive straight. Track speed, keep lane, and main-

tain speciﬁed safe distance from leading vehicle.

• Agent right turn. Negotiate a right turn. More details to

C1 is a constant. β = −1 if vtarget ≥ vego, else β = 1.

4

follow.

• Agent left turn. Similar to Agent right turn.
• Agent change left lane. Responsible for making a safe
transition to the lane on the left of the present lane. More
details to follow.

• Agent

change right lane.

Similar

to

Agent

change left lane.

For each of these manoeuvres, we train a value-based Deep
Q Learning (DQN) agent and policy-based DDPG agent in
the CARLA simulator. These agents can have overlapping
behaviours like lane-keeping, keeping safe distance etc. For
the policy network θµ, which suggests actions that maximize
expected reward, and objective function J(θ),
the policy
gradient update is calculated as:
∇θµ Ji(θ) = (cid:2)E(s,a,r) (∇θµ µ(si|θµ)∇aQ(si, ai|θq))(cid:3)

(1)

The loss for each DQN agent and critic network for DDPG
agent is calculated using the following loss function at each
iteration, i:

Li(θ) =
(cid:34)

E(s,a,r)

(cid:18)

r + γ max
at+1

Qθq−

i (st+1, at+1) − Qθq

i (st, at)

(cid:19)2(cid:35)

(2)
Q-Learning updates are applied on (cid:104)state, action, reward(cid:105)
samples by drawing random samples from the data batch. θq
i
represents the Q/critic-network parameters and θq−
are the
target network parameters at iteration i.

i

We distribute the reward over a continuous display of correct
behaviour and provide a positive reward rsub goal when the
episode terminates correctly by achieving the sub-goal. For
example, for the straight driving agent maintaining lane and
keeping a safe distance is a display of correct behaviour and
will fetch positive reward at each time step and rsubgoal will
be added on reaching the target way-point. The state space,
allowed actions and reward signal for each of the manoeuvres
are as follows:

1) Straight Driving Agent: The objective of this agent
is to track speed, keep lane and maintain a safe distance
from leading vehicles. The observation space Sstraight
is
(δxt,δxc,vego,δxlon) where δxt is the L2 norm distance from
the sub-task target way-point, δxc is the distance from the
centre of the lane, vego is the current speed of the ego
vehicle and δxlon is the distance from the leading vehicle. The
action space Astraight can control acceleration, deceleration
and restricted steering. The reward function rstraight is a
combination of the following rewards:

rstraight = rc + rl + rv + rlon + rt + rsub goal

2) Right / Left Turning Agents: These agents execute the
turning instructions. The observation space for Sright and
Slef t
is (δxc, δxt, δxn, vego, δangle) where δxn denotes
the distance from the nearest object and δangle refers to the
difference between the ego vehicle’s headway angle and target
way-point angle. The action space Aright and Alef t contain
restricted acceleration. Aright has access to only right steering
actions, and Alef t has access to only left steering actions. The
reward function rright and rlef t are deﬁned as follows:







Collision
Diff Center Lane
Diff Nearest Object

rc = −C1

rl = −1 ∗ (δxc)
rn = 1 ∗ (δxn)

ra = −1 ∗ (δangle) Diff Angle
rt = −1 ∗ (δxt)
rlef t/right = rc + rl + rn + ra + rt + rsub goal

Diff Target Location

(4)

3) Change Left / Right Lane Agents: The left/right lane
change agents can be triggered based on the proposed path
by the mission planner. They can also be triggered if the
path of the ego vehicle is blocked beyond a particular time
threshold, t(cid:15), and if the clearance distance, d(cid:15), is available
for lane change. This helps in avoiding static obstacles and
slow leading vehicles. The observation space Slef t change /
Sright change is deﬁned over the tuple (yawego, δyaw, δxt,
vego), where yawego is the motion about the perpendicular
axes, and δyaw is the difference between the yaw of the way-
point and the vehicle. When δyaw = 0, the vehicle is per-
fectly aligned with the road. The action spaces Aright change
and Alef t change contain restricted acceleration and restricted
left and right steering. The reward function, rright change /
rlef t change is as follows:






rc = −C1
rch = −C2
rst = −C3 ∗ (steer)
ryaw = −1 ∗ (δyaw) Diff Yaw

Collision
Incorrect Lane
Incorrect Steer






(5)

C2, C3 are constants. The reward rch applies when the ego
vehicle is not driving in the desired lane. rst effects the reward
when there is an undesired steering. For example if the left
lane change is underway and the ego vehicle has reached the
desired lane but is still steering towards left, then rst adds
to the reward. ryaw accounts for the difference between the
targeted yaw angle and the current yaw angle.

rlef t/rightchange = rc + rch + rst + ryaw + rsub goal

C. Safety Speciﬁcations






rc = −C1
Collision
rl = −1 ∗ (δxc)
Diff Center Lane
rv = β ∗ (vtarget − vego) Diff Target Speed
rlon = 1 ∗ (δxlon)
Diff Lead Vehicle
rt = −1 ∗ (δxt)
Diff Target Location






(3)

The structured program P, besides acting as the sequencer
and trigger for the RL agents, also acts as a safety shield
developed from safety speciﬁcations gleaned from well known
driving rules. These kinds of safety speciﬁcations have been
studied [13, 34, 35]. In symbolic model checking instead
of enumerating reachable states of a state machine with
the state
embedded entry exit conditions one at a time,

5

(b)

(d)

(a)

(c)

Figure 2: (a) Comparison between individual agents learning separate policies, Hierarchical DQN choosing between two policies and ﬂat
DQN learning the overall policy for task IV-A. (b) Average Rewards per step for all manoeuvre speciﬁc DQN models (c) HPRL agent tested
on scenarios inspired by NHTSA pre-crash scenarios in CARLA and route based tasks with random trafﬁc and scenarios. The scenario
descriptions are at Table at II and simulation video links are available [33] (d) Actor Loss, Critic Loss for DDPG straight driving model and
Average Reward for DDPG straight, right turn and left lane change models

machine can be efﬁciently checked by taking a cross product
with automaton constructed from the safety property (B ¨uchi
automaton). B ¨uchi automaton is an automata-theoretic version
of a formula in Linear Temporal Logic [36] which can encode
temporal safety speciﬁcations [37]. Temporal logic extends
classical propositional logic with a set of temporal operators
that navigate between a set of time steps. For brevity we
use LTL for describing the safety speciﬁcations rather than
showing its b¨uchi automata equivalent. The syntax of LTL is
given by the following grammar:

φ := T | ¬ φ | φ1 ∨ φ2| (cid:13) φ | ♦ φ | (cid:3) φ | φ1 U φ2

where T is true, (cid:13) is the next operator
(the property should
hold in the next time step), ♦ is the future operator (the
property should hold eventually sometime in the future), (cid:3)
is the global operator (the property should hold at all time
(φ1Uφ2 means φ1 should
steps) and U is the until operator
hold until the time step where φ2 becomes true). We also use
the release operator R where φ1 R φ2 ≡ ¬(¬φ1 U ¬φ2). The
semantics of LTL can be found in [38].

The Release operator φ1 R φ2 means that φ2 always holds
up to the time when φ1 becomes true. We use a modiﬁed
version of the release operator as proposed in [34], R, which
does not require φ2 to hold at all if φ1 has occurred in the
past. We shall ﬁrst specify the functional safety requirements
in a restricted fragment of LTL, and then demonstrate the

translation of this speciﬁcation to embedded assertions in the
structured program. Essentially, the functional safety module
in P checks and triggers emergency responses overriding the
current RL policies if any of the following properties hold.

1) The ego vehicle shall stop if the trafﬁc light is red, and
remain stationary until the trafﬁc light is not red. Suppose
e denotes the ego vehicle, proposition elat
stop represents no
lateral movement, and elon
stop represents no longitudinal
movement of the ego vehicle. We deﬁne:

estop = elat

stop ∧ elon
stop

The proposition Tred is set to true when the trafﬁc light
is red. We want to encode the property that if the trafﬁc
light is not red at time step t1 and changes to red at time
step t2 then from time step t2 the ego vehicle should
start applying break and come at a stop both laterally
and longitudinally till the stop manoeuvre is released
by the lights changing to not red at some time step tr.
The property should hold at all time steps and hence is
wrapped by global operator. The property can then be
coded in LTL as:

(cid:3)(¬Tred ∧ (cid:13)Tred → (cid:13)(¬Tred R estop))

2) The ego vehicle shall not make any longitudinal move-
ment if its distance with the lead vehicle falls below

a speciﬁed safety threshold. Let the proposition, Dlon
e,v ,
be true when the longitudinal safe distance between
ego vehicle e and leading vehicle v is safe, and false
otherwise. The proposition, Le, becomes true when lane
change is triggered for e. The property can be coded in
LTL as:

(cid:3)(Dlon

e,v ∧ ¬ (cid:13) Dlon

e,v → (cid:13)((Dlon

e,v ∨ Le) R elon

stop))

It be noted that
the restriction is removed when the
leading vehicle has moved forward to a safe distance or
a when lane change is triggered.

3) The ego vehicle shall not make any lateral movement
towards a neighboring vehicle if
the lateral distance
between the two vehicles fall below a speciﬁed safety
threshold. Suppose the proposition, Dlat
e,v, be true when the
lateral distance between the ego vehicle e and the lateral
neighbor v is safe, and false otherwise. The proposition,
Le/v, is true when a lane change is triggered for e, but
not towards the lane occupied by v. The property can be
coded in LTL as:

stop))

(cid:3)(Dlat

e,v ∧ ¬ (cid:13) Dlat

e,v ∨ Le/v) R elat

e,v → (cid:13)((Dlat
4) At a junction, the ego vehicle must remain stationary until
the vehicles of higher priority have cleared the junction.
The priority of a vehicle is higher if the vehicle has
entered the junction earlier. Also, if the routes r1 of ego
vehicle and r2 of vehicle v intersect, then the vehicle
having a smaller distance to the set r1 ∩ r2 has higher
priority. Suppose the proposition, Je,v, is true when the
ego vehicle and another vehicle, v, have both entered the
route to junction. The safety requirement may be speciﬁed
in LTL as:

(cid:3)(Je,v ∧ (Priority(v) > Priority(e)) → ¬Je,v R estop)

5) If the ego vehicle has initiated a lane change manoeuvre,
Le, and the target lane does not have a clearance of at
least C(cid:15) required for the lane change operation then the
ego vehicle remains stationary until the target lane is
clear. We may express this requirement in LTL as:

(cid:3)(Le ∧ ¬C(cid:15) → C(cid:15) R estop)

Position estimation and tracking of vehicles as well as
vulnerable road users such as pedestrian and bicycles can
be done using any standard methods, such as use of object
localization neural networks and sensors such as radar/lidar,
stated in [39]. In our experiments we do position estimation
via sensors such as available in CARLA and occupancy grid
map. While changing lane we take C(cid:15) to be the length of
the car with a 2m space in front and rear. For an opposite
lane manoeuvre C(cid:15) is a function of the time taken for the ego
vehicle to overtake the static/dynamic obstacle and velocity of
the incoming trafﬁc.

D. From Safety Speciﬁcation to Embedded Assertions

Though there exists a rich arsenal of tools for formal
veriﬁcation of software, and an equally rich arsenal of tools
for LTL model checking, there are no mature offerings for

6

checking LTL properties on C, C++, or Python. One direction
pursued by some researchers is to translate the program into
languages accepted by LTL model checking tools, such as
Promela (for using SPIN, LTSmin) [40, 41], or SMV (for using
NuSMV) [42]. The main challenge here is to establish that
the translation is semantically correct and correctly models
the behavior with respect to the truth of the speciﬁed LTL
properties.

An alternative approach, which we choose to follow in this
work, is to translate the LTL speciﬁcation into a form that can
be formally veriﬁed using program veriﬁcation tools. Since our
code is developed in Python, we choose Nagini [15] as our
veriﬁcation platform. Nagini is an automatic veriﬁcation tool,
based on the Viper veriﬁcation infrastructure, for statically
typed Python programs. In this subsection, we outline the
methodology for translating the LTL properties into embedded
assertions in Nagini. This is not an easy task in general, but the
HPRL framework has a structure which makes this possible, as
discussed in this section. Our aim is to formally prove that the
safety shield always guards against any failure of the formal
properties speciﬁed earlier. We illustrate how this is done using
one of the speciﬁed properties.

A high-level view of our structured program is shown in
Algorithm 1. As described before, the role of the structured
program is to take a sequence of sub-routes for a driving task
as input and to invoke suitable (pre-trained) RL agents for
executing each sub-route. The sub-route division from source
to destination is obtained by CARLA’s inbuilt path planner
GlobalRoutePlanner (A class which can be used to dynami-
cally compute trajectories from an origin to target waypoints).
Since the vehicle operates in a dynamical environment, the
structured program must examine the state of the system at
periodic intervals to determine whether the scenario permits
the present RL agent to continue, or whether a new agent
needs to be invoked. For example, it may choose to replace
the straight driving agent by a lane change agent
if the
lead vehicle stops or slows down in the present
lane. In
order to implement this behavior, the structured program uses
the function, neural control(RouteList), to invoke the suitable
RL agent for the current state and current sub-route in the
RouteList. The return value is the actuation recommended by
the RL agent, that is, values of throttle, steering, etc.

In order

Instead of

to ensure safe execution, we build a safety
seeking the
wrapper around the RL agent.
control actuation values directly from the function, neu-
ral control(RouteList), our structured program calls the func-
tion, check functional safety(RouteList), which uses a safety
shield over the RL agents. This function evaluates the state
of the system and determines whether it is safe to allow the
RL agent to recommend the next actuation. If so it calls
neural control(RouteList) and returns the actuation values
computed by it; otherwise, it overrides the RL agent and seeks
safe actuation values from a safe controller via the function,
safe control ϕ( ). The safe controller in our context is the
high level program responsible for providing set points to
an actual underlying controller such as the braking control.
The actuation values are determined based on the safety
speciﬁcation for which safe control ϕ( ) is overriding the

Algorithm 1: Structured Program P
Input: EgoV ehicle, RouteList, Environment
Function Main(RouteList):

while True do

control = check functional safety(RouteList)
EgoV ehicle.Apply(control)
T ime.sleep(δt)

end

Function check functional safety(RouteList):

for each ϕ in specif ications do

if ¬ϕ then

control = saf e control ϕ()
return control

end

end
control = neural control(RouteList)
return control

RL agent currently in execution. For example,
if the ego
vehicle encounters a red light IV-C the safe actuation set by
safe control ϕ( ) is to start applying brake and no steering
and throttle (setting brake = 1.0, steering = 0.0 and throttle =
0.0 in CARLA).

Nagini requires input programs to comply with the static,
nominal type system deﬁned in PEP 484 1 (standard syntax
for function annotations in python). Hence, the modules of
P that we intend to verify are converted to their statically
typed equivalents and annotated with assertions. For stati-
cally typed concurrent python programs, Nagini is capable of
proving memory safety, data race freedom, and user-supplied
assertions [15]. Assertions in Nagini are provided in the
form of Assert(Implies(e1, a2)) which denotes that assertion
a2 holds if Boolean expression e1 is true. We consider the
validation of Speciﬁcation 2 discussed in section IV-C to
elucidate the conversion from LTL speciﬁcation to embedded
assertions in Nagini speciﬁcation language. Speciﬁcation 2, is
expressed as:

ϕ = Dlon

e,v ∧ ¬ (cid:13) Dlon

e,v → (cid:13)((Dlon

e,v ∨ Le) R elon

stop)

The following methods are annotated with assertions:

1) Method

is vehicle hazard(...) invoked from
within the function, check functional safety(), is respon-
sible for checking whether there is a violation of longitu-
dinal safety constraint with all detected leading vehicles.
This method inspects if the L2 distance between the
leading vehicles and the ego vehicle is hazardous, and
if so, sets the variable long hazard detected to True and
returns True.
In
ϕ, we

assertion,
the method
True when
the L2 distance is less than the speciﬁed safe distance,
is, norm distance ≤ proximity threshold. This is
that
formally proven using the following assertion in Nagini.

is vehicle hazard(...)

formally
ﬁrst

that
returns

to
need

prove

prove

order

the

to

1https://www.python.org/dev/peps/pep-0484/

7

is vehicle hazard(vehicle List :
def
List[Vehicle],ego vehicle : Vehicle)→ bool:
... Assert(Implies(norm distance ≤
proximity threshold,Result()==True))
This assertion guarantees that this method sets the vari-
able long hazard detected when the antecedent of ϕ is
true, namely Dlon
e,v ∧ ¬ (cid:13) Dlon
e,v .

e,v → elon
stop.

e,v ∧ ¬ (cid:13) Dlon

2) Inside the check functional safety( ) method, the emer-
gency stop( ) method applies full brake and keeps ap-
plying brake, when long hazard detected is True, until
the leading vehicle is beyond unsafe distance or a lane
change is triggered. This should cause ego vehicle brake
to take a value of 1.0 which implies full brake in CARLA
environment and steering and throttle value to be 0.0
which is the required behaviour elon
stop. The assertion
verifying long hazard detected leads to vehicle.brake ==
1.0 and accounts for Dlon
Also, as part of the implementation the ego vehicle
initiates a lane change by setting lane change = True,
which is equivalent to setting the truth value of Le, if
ego vehicle remains stopped beyond blocking time ≥
t(cid:15) (self. blocking threshold). This behaviour is ensured
by the assertion : if self.time ≥ blocking threshold then
lane change is set to True.
def check functional safety(self) → Control:
long hazard detected
self. is vehicle hazard(vehicle list)
if long hazard detected and not lane change:
self.time count = self.time count + 1
if self.time count > self. blocking threshold:
lane change = True
if long hazard detected and not lane change:
control = self.emergency stop()
Assert(Implies((self.time count >
self. blocking threshold
lane change))
Assert(Implies((long hazard detected
lane change),
self.vehicle.brake == 1.0 and
self.vehicle.steer == 0.0 and
self.vehicle.throttle == 0.0))

lane change),

and

and

not

not

=

3) Finally we want

e,v ∨ Le) R elon

to ensure (Dlon
stop. If
longitudinal hazard is not detected and lane change is not
underway we validate that check functional safety() re-
turns the control suggested by triggering neural networks
via the function execute nn control() which changes the
elon
stop behaviour. If a lane change is underway then
lane change is True and this triggers the function ex-
ecute lane change() which also releases the elon
stop be-
haviour.
while True:
control
world.ego vehicle.apply control(control)
def check functional safety(self) → Control:
...
not(lane change)),
control == self.execute nn control()))

Assert(Implies((not(long hazard detected)

agent.check functional safety()

and

=

8

Architecture θq−, θq

Architecture θµ−

, θµ

(cid:15)-greedy parameters

Straight Driving Agent

DQN : (State Space) X 64 X 32 X (Action Space),
DDPG : Concatenate(Radar Space,State Space) X 256 X 128 X 64 X 32 X (Action Space)

Radar : (Radar Space) X 512 X 256, Odometry: (State Space) X 64 X 32 X 16,
Total Input : Concatenate(Radar,Odometry,272) X 256 X 128 X 32 X (Action Space)

(cid:15)0 = 1, λdecay = 0.995, (cid:15)min = 0.03

Throttlerange = 0.2. . . 1, Steerrange = -0.1. . . 0.1

Right/Left Turning Agents

Throttlerange = 0.3. . . 0.6, Steerright = 0.2. . . 0.5, Steerlef t = -0.2. . . -0.5

Right/Left Lane Change Agents

Throttlerange = 0.4. . . 0.5, Steerright = 0.1. . . 0.3, Steerlef t = -0.1. . . -0.3

Table I: Architecture, Hyper-parameters and action range for the DQN and DDPG agents. The throttle value ranges from 0 to +1 and steering
value ranges from -1 to +1 which have been restricted for different agents. The action space has been discretized for DQN Agents and is
continuous for DDPG Agents.

Assert(Implies lane change,
control == self.execute lane change()))
return control

In general given a safety speciﬁcation in the form of

ϕ = t0p0t1p1 . . . tkpk → tkaktk+1ak . . . trpr

where t0 . . . tr are time steps, p0 . . . pk are the truth values of
a set of propositions at each time step, tr, pr are the release
time and release proposition respectively and ak is an action,
the validation task is to validate the following.

1) For the antecedent: Validate that the proposition set pi
is set to True if the conditions for the propositions are
satisﬁed at a given time step ti. For example, combina-
tions of many evaluation factors can lead the condition
proximity.threshold ≤ norm distance to be True in the
the validation
method
module only needs to validate whether the corresponding
to True if the
predicate long hazard detected is set
condition holds at any time step.

is vehicle hazard(). However,

2) For the consequent: Validate if the antecedent is satisﬁed
time step tk then the recommended action ak is
at
triggered from tk till set of propositions pr are True at
release time tr. For example, we validate the ego-vehicle
remains at a stop till it is released by lane change or safe
longitudinal distance.

V. EMPIRICAL STUDIES

The implementation of our framework consists of two parts,
as shown in Fig. 1a. The ﬁrst part is the implementation of RL
agents using DQN and DDPG networks which use state, action
space and the reward functions introduced in section IV-B.
The architectural details of the networks used are mentioned
in Table I. The simulator CARLA is used for both training and
validation of HPRL agents. To evaluate the sample efﬁciency
of sub-DQN agents, with P as a trigger, we compare them to
a ﬂat DQN agent and a Hierarchical DQN agent where all of
them try to learn the task described in Example IV-A as shown
in Fig 2a. The Hierarchical DQN learns to choose between
the straight DQN and the Right Turn DQN to accomplish the
task. The HPRL framework can directly trigger the straight
driving and right turning agents to complete the task. Both
the Hierarchical DQN and Flat DQN take more training steps

to achieve the task, and neither of them can provide functional
safety.

The average reward per 100 training steps obtained by all
the DQN sub-agents on complete training (learning generic
manoeuvres) is shown in Fig 2b. The trade-off between
exploration and exploitation while training DQN sub-agents
is handled by following an (cid:15)-greedy policy. We use a replay
buffer of length 1000000 and a discount factor of γ = 0.99.
We also train our agents using Policy optimization-based
DDPG networks to achieve action over a continuous action
space. The DDPG network uses an extra input of radar cloud-
points along with the states. The average reward obtained by
the DDPG sub-agents at each training step is illustrated in
Fig 2d. The training and testing of all the experiments are
performed on a machine with 6-core 2.4 GHz Intel Core i7
8th Gen and 4GB NVIDIA GeForce GTX TITAN.
We validate the second part of our implementation P with
Nagini. Nagini takes 50.71s to validate all assertions for LTL
speciﬁcations in P, using Z3 SMT solver in the back-end,
which generates 18,477 clauses.
We test the performance of HPRL framework on 10 NHTSA
inspired pre-crash scenarios modelled in CARLA2. The de-
scription of each scenario, along with the RL agents and
assertions triggered in each case are described Table II, and
pictorial representation is shown is Figure 2c. The implementa-
tion code is open-sourced and available at [43]. The framework
is also tested on longer driving tasks where the ego vehicle
travels on a predeﬁned route. We spawn 120 trafﬁc vehicles
which randomly move throughout the town and trigger random
scenarios on the ego vehicle’s path.

VI. CONCLUSIONS AND FUTURE WORK

In this paper, we present the Hierarchical Program Trig-
gered Reinforcement Learning (HPRL) framework, which uses
deep reinforcement learning agents triggered by a structured
program embedded with rule-based safety speciﬁcations. The
experiments demonstrate that DRL agents trained with ma-
noeuvre speciﬁc actions and state spaces are sample efﬁcient.
We show that the framework is capable of handling chal-
lenging pre-crash scenarios for autonomous driving vehicles.
The framework integrates formal validation with the program
veriﬁcation tool Nagini to ensure functional safety. In future,

2https://carlachallenge.org/challenge/nhtsa/

9

Scenario

Control Loss

Longitudinal control

Obstacle avoidance with/
without prior action

Lane change

Expected Behaviour

Ego vehicle loses control without prior action and must
recover

Leading vehicle decelerates suddenly and ego-vehicle must
perform an emergency brake or an avoidance maneuver.

While performing a maneuver, the ego-vehicle ﬁnds an
obstacle must perform an emergency brake or an avoidance
maneuver.

Ego-vehicle performs a lane changing to evade a leading
vehicle, which is moving too slowly.

DRL Agents

Straight Drive

Assertions

None

Straight Drive, Lane Change

2,5

Straight Drive, Lane Change

2,3,5

Straight Drive, Lane Change

2,3,5

Vehicle Passing with on-
coming trafﬁc

Ego-vehicle must go around a blocking object using the
opposite lane, yielding to oncoming trafﬁc

Straight Drive, Lane Change

2,3,5

Running Red Light at In-
tersection

Ego-vehicle is going straight at an intersection but a
crossing vehicle runs a red light, ego-vehicle must perform
a collision avoidance maneuver

Straight Drive

1,2,4

Unprotected left turn at in-
tersection

Ego-vehicle is performing an unprotected left turn at an
intersection, yielding to oncoming trafﬁc.

Straight Drive, Left Turn

1,3,4

Right turn at an intersec-
tion

Ego-vehicle is performing a right turn at an intersection,
yielding to crossing trafﬁc

Straight Drive, Right Turn

1,3,4

Crossing
signalized intersection

an

at

un-

Ego-vehicle needs to negotiate with other vehicles to cross
an un-signalized intersection with the ﬁrst to enter the
intersection having priority.

Straight Drive

2,4

Table II: Description of the test scenarios along with the triggered agents and assertions. The simulation videos are available at [33]

the authors would like to reduce the DRL agent’s granularity,
for example, to braking and steering, to facilitate continuous
switching between them making the control smoother. The
authors also wish to study how safety speciﬁcations can
directly be embedded as a part of the DRL agents, thereby
eliminating the requirement of an explicit safety shield.

REFERENCES

[1] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion,
Ahmad A. Al Sallab, Senthil Yogamani, and Patrick P´erez. Deep
Reinforcement Learning for Autonomous Driving: A Survey.
Arxiv, pages 1–18, 2020.

[2] Mohammed Q. Wen-Hua C. Lipika D. Christos K. Real-
time motion planning methods for autonomous on-road driving:
State-of-the-art and future research directions. In Transportation
Research Part C: Emerging Technologies, pages 416–442, 2015.
[3] Becker J. Bhat S. Montemerlo M. Junior: The stanford entry
in the urban challenge. Journal ofﬁeld Robotics, page 569–597,
2008.

[4] S. Glaser, B. Vanholme, S. Mammar, D. Gruyer, and L. Nou-
veli`ere. Maneuver-based trajectory planning for highly au-
tonomous vehicles on real road with trafﬁc and driver interac-
tion. IEEE Transactions on Intelligent Transportation Systems,
11(3):589–606, 2010.

[5] Warwick K. Kala R. Motion planning of autonomous vehi-
cles in a non- autonomous vehicle environment without speed
lanes. Engineering Applications of Artiﬁcial Intelligence, page
1588–1601, 2013.

[6] Dean A. Pomerleau. Alvinn: An autonomous land vehicle in a
neural network. In D. S. Touretzky, editor, Advances in Neural
Information Processing Systems 1, pages 305–313. Morgan-
Kaufmann, 1989.

[7] M. Bojarski, D. Testa, Daniel Dworakowski, Bernhard Firner,
Beat Flepp, Prasoon Goyal, L. Jackel, M. Monfort, U. Muller,
Jiakai Zhang, X. Zhang, Jake Zhao, and Karol Zieba. End to
end learning for self-driving cars. ArXiv, abs/1604.07316, 2016.

[8] Yazhou Yao, Jian Zhang, Fumin Shen, Li Liu, Fan Zhu, Dongxi-
ang Zhang, and Heng Tao Shen. Towards automatic construction
of diverse, high-quality image datasets. IEEE Transactions on
Knowledge and Data Engineering, 2019.

[9] A. Huang C. J. Maddison A. Guez L. Sifre G. van den Driessche
J. Schrittwieser I. Antonoglou V. Panneershelvam M. Lanctot
S. Dieleman D. Grewe J. Nham N. Kalchbrenner I. Sutskever
T. P. Lillicrap M. Leach K. Kavukcuoglu T. Graepel D. Silver
and D. Hassabis. Mastering the game of go with deep neural
networks and tree search. In Nature, page 484–489, 2016.
[10] Y. Zhang, P. Sun, Y. Yin, L. Lin, and X. Wang. Human-
like autonomous vehicle speed control by deep reinforcement
In 2018 IEEE Intelligent
learning with double q-learning.
Vehicles Symposium (IV), pages 1251–1256, 2018.

[11] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza,
Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and
Koray Kavukcuoglu. Asynchronous methods for deep rein-
forcement learning. In Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceedings
of Machine Learning Research, pages 1928–1937, New York,
New York, USA, 20–22 Jun 2016. PMLR.

[12] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
Safe, multi-agent, reinforcement learning for autonomous driv-
ing. In Proceedings of NIPS Workshop Learn. Inference Control
Multi-Agent Syst, 10 2016.

[13] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
On a Formal Model of Safe and Scalable Self-driving Cars.
pages 1–37, 2017.

[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
Lopez, and Vladlen Koltun. CARLA: An open urban driving
In Proceedings of the 1st Annual Conference on
simulator.
Robot Learning, pages 1–16, 2017.

[15] Marco Eilers and Peter M¨uller. Nagini: A static veriﬁer for
python. Lecture Notes in Computer Science (including subseries
Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in
Bioinformatics), 10981 LNCS:596–603, 2018.

[16] Yang Guan, Shengbo Eben Li, Jingliang Duan, Wenjun Wang,
and Bo Cheng. Markov probabilistic decision making of self-

driving cars in highway with random trafﬁc ﬂow: a simulation
study. Journal of Intelligent and Connected Vehicles, 1(2):77–
84, 2018.

[17] E. Perot, M. Jaritz, M. Toromanoff, and R. De Charette. End-to-
end driving in a realistic racing game with deep reinforcement
learning. In IEEE Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), pages 474–475, 2017.
[18] Ahmad Sallab, Mohammed Abdou, Etienne Perot, and Senthil
Yogamani. Deep reinforcement learning framework for au-
tonomous driving. Electronic Imaging, 2017:70–76, 01 2017.

[19] Mustafa Mukadam, Akansel Cosgun, Alireza Nakhaei, and
Kikuo Fujimura. Tactical decision making for lane changing
with deep reinforcement learning. 2017.

[20] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J. Allen,
V. Lam, A. Bewley, and A. Shah. Learning to drive in a day.
In 2019 International Conference on Robotics and Automation
(ICRA), pages 8248–8254, 2019.

[21] Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and
Joshua B. Tenenbaum. Hierarchical deep reinforcement learn-
ing: Integrating temporal abstraction and intrinsic motivation.
In Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, page 3682–3690,
Red Hook, NY, USA, 2016. Curran Associates Inc.

[22] M. Nosrati, Elmira A. Abolfathi, M. Elmahgiubi, P. Yadmellat,
Jun Luo, Yunfei Zhang, Hengshuai Yao, Hongbo Zhang, and
Anas K. Jamil. Towards practical hierarchical reinforcement
learning for multi-lane autonomous driving. 2018.

[23] Jingliang Duan, Shengbo Eben Li, Yang Guan, Qi Sun, and
Bo Cheng. Hierarchical reinforcement learning for self-driving
decision-making without reliance on labelled driving data. IET
Intelligent Transport Systems, pages 1–9, 2020.

[24] Y. Chen, C. Dong, P. Palanisamy, P. Mudalige, K. Muelling, and
J. M. Dolan. Attention-based hierarchical deep reinforcement
learning for lane change behaviors in autonomous driving. In
2019 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pages 3697–3703, 2019.

[25] Z. Xu, C. Tang, and M. Tomizuka. Zero-shot deep reinforcement
learning driving policy transfer for autonomous vehicles based
on robust control. In 21st International Conference on Intelli-
gent Transportation Systems (ITSC), pages 2865–2871, 2018.

[26] Dario Amodei, Chris Olah, J. Steinhardt, Paul F. Christiano,
John Schulman, and Dan Man´e. Concrete problems in ai safety.
ArXiv, abs/1606.06565, 2016.

[27] Matteo Leonetti, Luca Iocchi, and Peter Stone. A synthesis
of automated planning and reinforcement learning for efﬁcient,
robust decision-making. Artif. Intell., 241:103–130, 2016.
[28] Fangkai Yang, Daoming Lyu, Bo Liu, and Steven Gustafson.
Peorl: Integrating symbolic planning and hierarchical reinforce-
ment learning for robust decision-making. IJCAI International
Joint Conference on Artiﬁcial Intelligence, 2018-July:4860–
4866, 2018.

[29] Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson.
SDRL: Interpretable and Data-Efﬁcient Deep Reinforcement
Proceedings of
Learning Leveraging Symbolic Planning.
the AAAI Conference on Artiﬁcial Intelligence, 33:2970–2977,
2019.

[30] Ekim Yurtsever, L. Capito, K. Redmill, and ¨U. ¨Ozg¨uner.

In-
tegrating deep reinforcement learning with model-based path
planners for automated driving. ArXiv, abs/2002.00434, 2020.
[31] Shao-Hua Sun, Te-Lin Wu, and Joseph J. Lim. Program guided
In International Conference on Learning Representa-

agent.
tions, 2020.

[32] Andrew Barto and Sridhar Mahadevan. Recent advances in
hierarchical reinforcement learning. Discrete Event Dynamic
Systems: Theory and Applications, 13, 12 2003.

[33] Briti Gangopadhyay.

Hierarchical-program-triggered-rl
(simulation videos) [available online]. https://github.com/britig/
Hierarchical-Program-Triggered-RL/tree/main/Hierarchical%
20Program%20Triggered%20Reinforcement%20Learning%

10

20Agents/Experiment%20Videos/videos.

[34] Mohammad Hekmatnejad, Shakiba Yaghoubi, Adel Dokhanchi,
Heni Ben Amor, Aviral Shrivastava, Lina Karam, and Georgios
Fainekos. Encoding and monitoring responsibility sensitive
safety rules for automated vehicles in signal temporal logic.
MEMOCODE - 17th ACM-IEEE International Conference on
Formal Methods and Models for System Design, 2019.

[35] Abolfazl Karimi. Formalizing trafﬁc rules for uncontrolled
In Proceedings of International Conference on

intersections.
Cyber-Physical Systems (ICCPS), pages 41–50, 2020.

[36] A. Pnueli. The temporal logic of programs.

18th Annual
Symposium on Foundations of Computer Science (sfcs 1977),
pages 46–57, 1977.

[37] Fabio Somenzi and Roderick Bloem. Efﬁcient b¨uchi automata
from ltl formulae. In E. Allen Emerson and Aravinda Prasad
Sistla, editors, Computer Aided Veriﬁcation, pages 248–263,
Berlin, Heidelberg, 2000. Springer Berlin Heidelberg.

[38] C. Baier and J. Katoen. Principles of model checking. The MIT

Press, 2008.

[39] Sarfraz Ahmed, M. Nazmul Huda, Sujan Rajbhandari, Chitta
Saha, Mark Elshaw, and Stratis Kanarachos. Pedestrian and
cyclist detection and intent estimation for autonomous vehicles:
A survey. Applied Sciences, 9(11), 2019.

[40] Gerard J. Holzmann.

Software model checking with spin.
volume 65 of Advances in Computers, pages 77 – 108. 2005.
[41] Gijs Kant, Alfons Laarman, Jeroen Meijer, Jaco van de Pol,
Stefan Blom, and Tom van Dijk. Ltsmin: High-performance
In Christel Baier and
language-independent model checking.
Cesare Tinelli, editors, Tools and Algorithms for the Con-
struction and Analysis of Systems, pages 692–707, Berlin,
Heidelberg, 2015. Springer Berlin Heidelberg.

[42] Alessandro Cimatti, Edmund Clarke, Enrico Giunchiglia, Fausto
Giunchiglia, Marco Pistore, Marco Roveri, Roberto Sebastiani,
and Armando Tacchella. Nusmv 2: An opensource tool for
symbolic model checking. In Ed Brinksma and Kim Guldstrand
Larsen, editors, Computer Aided Veriﬁcation, pages 359–364,
Berlin, Heidelberg, 2002. Springer Berlin Heidelberg.

[43] Briti Gangopadhyay. Hierarchical-program-triggered-rl (code
distribution and videos) [available online]. https://github.com/
britig/Hierarchical-Program-Triggered-RL.

Briti Gangopadhyay (Student Member, IEEE) is a
research scholar at department of computer science
and engineering, IIT Kharagpur, Kharagpur, India.
She is a part of the Formal Methods and Trusted
AI Group, IIT Kharagpur. Her current research areas
include Explainable Artiﬁcial Intelligence, Veriﬁable
Autonomous driving policies, and Neuro-Symbolic
Reasoning. She is also recipient of TCS Research
Fellowship.

Harshit Soora is a ﬁnal year undergraduate stu-
dent
in the department of computer science and
engineering, IIT Kharagpur, Kharagpur, India. He is
currently a part of Trusted AI Group. His current
research areas include reliable autonomous policies
and veriﬁable deep reinforcement learning.

11

Prof. Pallab Dasgupta (Senior Member, IEEE)
received the Ph.D. degree in computer science and
in 1995. He is
engineering from IIT Kharagpur,
currently a Professor in Computer Science and En-
gineering with IIT Kharagpur. He leads the Formal
Methods and Trusted AI Group with collaborations
at Intel, Synopsys, SRC, Texas Instruments, Indian
Railways, and HAL. He has more than 200 research
papers. Dr. Dasgupta is a Fellow of the Indian
National Academy of Engineering and the Indian
Academy of Science.

