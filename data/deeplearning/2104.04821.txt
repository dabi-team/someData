1
2
0
2

r
p
A
0
1

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

1
v
1
2
8
4
0
.
4
0
1
2
:
v
i
X
r
a

This draft was prepared using the LaTeX style ﬁle belonging to the Journal of Fluid Mechanics

1

End-to-end diﬀerentiable learning of
turbulence models from indirect observations

Carlos A. Michel´en Str¨ofer1

and Heng Xiao1

†

‡

1Kevin T. Crofton Department of Aerospace and Ocean Engineering, Virginia Tech,
Blacksburg, VA 24060, USA

(Received xx; revised xx; accepted xx)

The emerging push of the diﬀerentiable programming paradigm in scientiﬁc computing
is conducive to training deep learning turbulence models using indirect observations.
This paper demonstrates the viability of this approach and presents an end-to-end
diﬀerentiable framework for training deep neural networks to learn eddy viscosity models
from indirect observations derived from the velocity and pressure ﬁelds. The framework
consists of a Reynolds-averaged Navier–Stokes (RANS) solver and a neural-network-
represented turbulence model, each accompanied by its derivative computations. For
computing the sensitivities of the indirect observations to the Reynolds stress ﬁeld,
we use the continuous adjoint equations for the RANS equations, while the gradient
of the neural network is obtained via its built-in automatic diﬀerentiation capability.
We demonstrate the ability of this approach to learn the true underlying turbulence
closure when one exists by training models using synthetic velocity data from linear and
nonlinear closures. We also train a linear eddy viscosity model using synthetic velocity
measurements from direct numerical simulations of the Navier–Stokes equations for which
no true underlying linear closure exists. The trained deep-neural-network turbulence
model showed predictive capability on similar ﬂows.

Key words: turbulence modelling, turbulence theory, variational methods

1. Introduction

There still is a practical need for improved closure models for the Reynolds-averaged
Navier–Stokes (RANS) equations. Currently, the most widely used turbulence models are
linear eddy viscosity models (LEVM), which presume the Reynolds stress is proportional
to the mean strain rate. Although widely used, LEVM do not provide accurate predictions
in many ﬂows of practical interest, including the inability to predict secondary ﬂows
in non-circular ducts (Speziale 1982). Alternatively, non-linear eddy viscosity models
(NLEVM) capture nonlinear relations from both the strain and rotation rate tensors.
NLEVM, however, do not result in consistent improvement over LEVM and can suﬀer
from poor convergence (Gatski & Speziale 1993). Data-driven turbulence models are an
emerging alternative to traditional single-point closures. Data-driven NLEVM use the
integrity basis representation (Pope 1975; Ling et al. 2016) to learn the mapping from
the velocity gradient ﬁeld to the normalised Reynolds stress anisotropy ﬁeld, and retain
the transport equations for turbulence quantities from a traditional model.

† Email address for correspondence: cmich@vt.edu
‡ Email address for correspondence: hengxiao@vt.edu

 
 
 
 
 
 
2

C. A. Michel´en Str¨ofer and H. Xiao

It has been natural to train such models using Reynolds stress data (e.g. Ling et al.
2016; Weatheritt & Sandberg 2017). However, Reynolds stress data from high-ﬁdelity
simulations, i.e. from direct numerical simulations (DNS) of the Navier–Stokes equations,
is mostly limited to simple geometries and low Reynolds number. It is therefore desirable
to train with indirect observations, such as quantities based on the velocity or pressure
ﬁelds, for which experimental data is available for a much wider range of complex ﬂows.
Such measurements include full ﬁeld data, e.g. from particle image velocimetry, sparse
point measurements such as from pressure probes, and integral quantities such as lift and
drag. Training with indirect observations has the additional advantage of circumventing
the need to extract turbulence scales that are consistent with the RANS modelled scales
from the high ﬁdelity data (e.g. Weatheritt & Sandberg 2017).

Recently, Zhao et al. (2020) learned a zonal turbulence model for the wake-mixing
regions in turbomachines in symbolic form (e.g., polynomials and logarithms) from
indirect observation data by using genetic algorithms. However, while symbolic models
are easier to interpret, they may have limited expressive power as compared to, for
instance, deep neural networks (Raghu et al. 2017), which are successive composition
of nonlinear functions. Symbolic models may therefore not be generalisable and be
limited to zonal approaches. More importantly, gradient-free optimisation method such
as genetic programming may not be as eﬃcient as gradient-descent methods, and the
latter should be used whenever available (Audet & Kokkolaras 2016). In particular, deep
learning methods (LeCun et al. 2015) have achieved remarkable success in many ﬁelds
and represent a promising approach for data-driven NLEVM (e.g. Ling et al. 2016).

A major obstacle for gradient-based learning from indirect observations is that a
RANS solver must be involved in the training and the RANS sensitivites are required
to learn the model. While, such sensitivites can be obtained by using adjoint equations,
which have long been used in aerodynamic shape optimisation (Jameson 1988), these
are not generally rapidly available or straight forward to implement. The emerging
interest in diﬀerentiable programming is resulting in eﬃcient methods to develop adjoint
accompanying physical models, including modern programming languages that come
with built-in automatic diﬀerentiation (Bezanson et al. 2019), or neural-network-based
solutions of partial diﬀerential equations (Raissi et al. 2019). Recently, Holland et al.
(2019) used the discrete adjoint to learn a corrective scalar multiplicative ﬁeld in the
production term of the Spalart–Allmaras transport model. This is based on an alternative
approach to data-driven turbulence modelling (Parish & Duraisamy 2016) in which
empirical correction terms for the turbulence transport equations are learned while
retaining a traditional linear closure (LEVM).

In this work we demonstrate the viability of training deep neural networks to learn
general eddy viscosity closures (NLEVM) using indirect observations. We embed a
neural-network-represented turbulence model into a RANS solver using the integrity
basis representation, and as a proof of concept we use the continuous adjoint equations
to obtain the required RANS sensitivities. This leads to an end-to-end diﬀerentiable
framework that provides the gradient information needed to learn turbulence models
from indirect observations.

2. Diﬀerentiable framework for learning turbulence models

In this proposed framework a neural network is trained by optimising an objective
function that depends on quantities derived from the network’s outputs rather than on
those outputs directly. The training framework is illustrated schematically in ﬁgure 1
and consists of two components: the turbulence model and the objective function. Each

Diﬀerentiable learning of turbulence models from indirect observations

3

Turbulence Model
w

Obervation Operator

g

τ

u · ∇u − ν∇2u + ∇·τ + ∇p − s = 0
∇ · u = 0

T (k, tτ ) = 0

Neural Network

Backpropagation

τ

∂J
∂τ

J = J(u, p)

PDEs

J
J

Adjoint Model

θ

w

∂J
∂w

Figure 1. Schematic of the end-to-end diﬀerentiable training framework. The framework
consists of two main components, the turbulence model and the observation operator, each of
which has a forward and backwards (adjoint) model. For any value of the trainable parameters
w the gradient of the objective function J can be obtained by solving these four problems. The
turbulence model consists of a deep neural network representing the closure function using the
integrity basis representation θ (cid:55)→ g and transport equations T (k, tτ ) = 0 for the turbulence
quantities. The observation operator consists of solving the RANS equations with the proposed
turbulence model and extracting the quantities of interest that are compared to the observations
in the cost function J.

of these two components has a forward model that propagates inputs to outputs and a
backwards model that provides the derivatives of the outputs with respect to the inputs
or parameters. The gradient of the objective function J with respect to the network’s
trainable parameters w is obtained by combining the derivative information from the
two components through the chain rule as

∂J
∂w

=

∂J
∂τ

∂τ
∂w

,

(2.1)

where τ is the Reynolds stress tensor predicted by the turbulence model.

2.1. Forward model
For given values of the trainable parameters w, the forward model evaluates the cost
function J, which is the discrepancy between predicted and observed quantities. The
forward evaluation consists of two main components: (i) evaluating the neural network
turbulence model and (ii) mapping the network’s outputs to observation space by ﬁrst
solving the incompressible RANS equations.

The turbulence model, shown on the left box in ﬁgure 1, maps the velocity gradient
ﬁeld to the Reynolds stress ﬁeld. The integrity basis representation for a general eddy
viscosity model (Pope 1975) is given as

τ = a +

2k
3

I,

a = 2kb,

b =

10
(cid:88)

i=1

g(i)T (i),

g(i) = g(i)(θ1, . . . , θ5),

(2.2)

where a is the anisotropic (deviatoric) component of the Reynolds stress, b is the
normalised anisotropic Reynolds stress, T and θ are the basis tensor functions and
scalar invariants of the input tensors, g are the scalar coeﬃcient functions to be learned,
and I
is the second rank identity tensor. The input tensors are the symmetric and
∇u + ∇u(cid:62)(cid:17)
antisymmetric components of the normalised velocity gradient: S = 1
, where tτ is the turbulence time-scale and u is the mean

and R = 1

∇u(cid:62)(cid:17)

∇u

2 tτ

(cid:16)

(cid:16)

2 tτ

−

4

C. A. Michel´en Str¨ofer and H. Xiao

velocity. The linear and quadratic terms in the integrity basis representation are given
as

T (1) = S, T (2) = SR

−

RS, T (3) = S2
θ1 = (cid:8)S2(cid:9)
indicate the tensor trace.

−

1
3
θ2 = (cid:8)R2(cid:9) ,

(cid:8)S2(cid:9) I, T (4) = R2

1
3

−

(cid:8)R2(cid:9) I

(2.3)

where curly braces

·
{

}

(cid:55)→

Diﬀerent eddy viscosity models diﬀer in the form of the scalar coeﬃcient functions
θ
g and in the models for the two turbulence scale quantities k and tτ . We represent
the scalar coeﬃcient functions using a deep neural network with 10 hidden layers with 10
neurons each and a rectiﬁed linear unit (ReLU) activation function. The turbulence
scaling parameters are modelled using traditional transport equations T (k, tτ ) = 0
with the TKE production term P modiﬁed to account for the expanded formulation
of Reynolds stress: P = τ : ∇u, where : denotes double contraction of tensors.

The RANS solver along with its post-processing serves as an observation operator that
maps the turbulence model’s outputs (Reynolds stress τ ) to the observation quantities
(e.g., sparse velocity measurement, drag). This is shown in the right box in ﬁgure 1. The
ﬁrst step in this operation is to solve the RANS equations with the given Reynolds
stress closure to obtain mean velocity u and pressure p ﬁelds. This is followed by
post-processing to obtain the observation quantities (e.g., sampling velocities at certain
locations or integrating surface pressure to obtain drag). When solving the RANS
equations, explicit treatment of the divergence of Reynolds stress can make the RANS
equations ill-conditioned (Wu et al. 2019; Brener et al. 2021). We treat part of the linear
term implicitly by use of an eﬀective viscosity νeff which is easily obtained since with the
integrity basis representation the linear term is learned independently. The incompressible
RANS equations are then given as

u · ∇u

−

∇·νeff∇u

∇u·

νeff + ∇·aNL +

p∗ = s,

∇

νeff = ν

g(1)ktτ ,

−

g(i)T (i),

p∗ = p +

2k
3

(2.4)

∇

−
∇ · u = 0,
10
(cid:88)

aNL = 2k

i=2

where the term ∇·νeff∇u is treated implicitly. Here aNL represents the non-linear com-
ponent of the Reynolds stress anisotropy, the isotropic component of Reynolds stress is
incorporated into the pressure term p∗, and s is the momentum source term representing
any external body forces. The coeﬃcients g(i) are outputs of the neural network-based
turbulence model that have the ﬁelds θ as input.

2.2. Adjoint model
For a proposed value of the trainable parameters w the backwards model (represented
by left-pointing arrows in ﬁgure 1) provides the gradient ∂J/∂w of the cost function with
respect to the trainable parameters. This is done by separately obtaining the two terms
on the right hand side of equation (2.1): (i) the gradient of the Reynolds stress ∂τ /∂w
from the neural network turbulence model and (ii) the sensitivity ∂J/∂τ of the cost
function to the Reynolds stress, using their respective adjoint models. Combining the two
adjoint models results in an end-to-end diﬀerentiable framework, whereby the gradient
of observation quantities (e.g. sparse velocity) with respect to the neural network’s
parameters can be obtained.

The gradient of the Reynolds stress with respect to the neural network’s parameters w
is obtained in two parts using the chain rule. The gradient of the neural network’s outputs

Diﬀerentiable learning of turbulence models from indirect observations

5

with respect to its parameters, ∂g/∂w, is eﬃciently obtained by backpropagation, which
is a reverse accumulation automatic diﬀerentiation algorithm for deep neural networks
that applies the chain rule on a per-layer basis. The sensitivities of the Reynolds stress
to the coeﬃcient functions are obtained as ∂τ /∂g(i) = 2kT (i) from diﬀerentiation of
equation 2.2, which is a linear tensor equation.

For the sensitivity of the objective function with respect to the Reynolds stress we
derived the appropriate continuous adjoint equations. Since the Reynolds stress must
satisfy the RANS equations, this is a constrained optimisation problem. The problem
is reformulated as the optimisation of an unconstrained Lagrangian function with the
Lagrange multipliers described by the adjoint equations. The resulting adjoint equations
are

u · ∇ˆu + ∇ˆu · u + ν∇2 ˆu
∂JΩ
∂p

∇ · ˆu =

−

,

− ∇

ˆp =

∂JΩ
∂u

(2.5)

where ˆu and ˆp are the Lagrange multipliers, referred to as adjoint velocity and adjoint
pressure, respectively, and JΩ is the scalar ﬁeld such that the objective function is given
as the integral J = (cid:82) JΩdΩ over the solution domain Ω. When the cost function is
instead given as an integral over the domain boundary (e.g. drag) the cost function
aﬀects the boundary conditions of the adjoint equations instead (Othmer 2008). When
solving the adjoint equation using a segregated approach such as the SIMPLE algorithm
used here, the adjoint transpose convection term ∇ˆu · u is treated explicitly and can
result in instabilities (Oriani & Pierrot 2016). For this reason it is common to dampen
or eliminate this term (Othmer 2014), and here we eliminate it. After solving the adjoint
equations, the sensitivity of the function J to the Reynolds stress is given as the gradient
of the adjoint velocity

−
The details of the derivation and use of the continuous adjoint equations are shown
in (Michel´en Str¨ofer 2021) for interested readers.

(2.6)

∂J
∂τ

=

∇ˆu.

2.3. Gradient descent procedure

The training is done using the Adam algorithm, a gradient descent algorithm with
momentum and adaptive gradients commonly used in training deep neural networks. The
default values for the Adam algorithm are used, including a learning rate of 0.001. The
training requires solving the RANS equations at each training step. In a given training
step the inputs θi are updated based on the previous RANS solution and scaled to the
range 0–1, and the RANS equations are then solved with ﬁxed values for the coeﬃcient
functions g. The inputs θi and their scaling parameters are ﬁxed at a given training step
and converge alongside the main optimisation of the trainable parameters w.

initialisation of the neural network’s parameters requires special consideration. The
usual practice of random initialisation of the weights is not suitable in this case since it
leads to divergence of the RANS solution. We use existing closures (e.g. a laminar model
with g(i) = 0 or a linear model with g(1) =
0.09) to generate data for pre-training the
−
neural network and thus provide a suitable initialisation. This has the additional beneﬁt
of embedding existing insight into the training by choosing an informed initial point in
the parameter space. When pre-training to constant values (e.g. g(1) =
0.09) we add
noise to the pre-training data, since starting from very accurate constant values can make
the network diﬃcult to train.

−

6

(a)

C. A. Michel´en Str¨ofer and H. Xiao

(b)

(c)

Figure 2. Results of learning a LEVM from a single velocity measurement in a turbulent
channel ﬂow: (a) the learned coeﬃcient function, (b) the anisotropic Reynolds stress ﬁeld, and
(c) the velocity. The ﬁnal (trained) results overlap with the truth and the two are visually
indistinguishable.

3. Test cases

The viability of the proposed framework is demonstrated by testing on three test cases.
The ﬁrst two cases use synthetic velocity data obtained from a linear and a non-linear
closure, respectively, to train the neural network. The use of synthetic data allows us to
evaluate the ability of the training framework to learn the true underlying turbulence
closure when one exists. In the ﬁnal test case realistic velocity measurements, obtained
from a DNS solution and for which no known true underlying closure exists, are used to
learn a linear eddy viscosity model. The trained LEVM is then used to predict similar
ﬂows and the predictions are compared to those from a traditional LEVM.

3.1. Learning a synthetic LEVM from channel ﬂow
As a ﬁrst test case we use a synthetic velocity measurement at a single point from the
turbulent channel ﬂow to learn the underlying linear model. The ﬂow has a Reynolds
number of 10, 000 based on bulk velocity ub and half channel height h. The turbulent
equations used are the k–ω model of Wilcox (1998), and the synthetic model corresponds
to a constant g(1) = 0.09. For the channel ﬂow there is only one independent scalar
invariant and T (1) is the only linear tensor function in the basis. We therefore use a
g(1). The network
neural network with one input and one output which maps θ1 (cid:55)→
has 1021 trainable parameters and is pre-trained to the laminar model g(1) = 0. The
sensitivity of the predicted point velocity to the Reynolds stress is obtained by solving
the adjoint equations with JΩ equal to the velocity ﬁeld times a radial basis function.
Figure 2 shows the results of the training. The trained model not only results in the
correct velocity ﬁeld, but the correct underlying model is learned.

3.2. Learning a synthetic NLEVM from ﬂow through a square duct
As a second test case we use a synthetic full ﬁeld velocity measurement from ﬂow
in a square duct to learn the underlying nonlinear model. The ﬂow has a Reynolds
number of 3, 500 based on bulk axial velocity ub and half duct side length h. This ﬂow
contains a secondary in-plane ﬂow that is not captured by LEVM (Speziale 1982). For the
objective function, JΩ is the diﬀerence between the measured and predicted ﬁelds, with
the discrepancy of the in-plane velocity scaled by a factor of 1, 000 as to have a similar

InitialFinalTruthObservation0.050.00Functiong(1)0.000.250.500.751.00Positiony/h0.10.0Reynoldsstressbxy0.000.250.500.751.00Positiony/h01Velocityux/ub0.000.250.500.751.00Positiony/hDiﬀerentiable learning of turbulence models from indirect observations

7

(a)

(b)

Figure 3. Results of learning a NLEVM, the Shih quadratic model, from full ﬁeld velocity
measurements in ﬂow through a square duct. The results shown are the two combinations of
coeﬃcient functions that have an eﬀect on velocity plotted against the scalar invariant θ1 ≈ −θ2.

(a)

ux/ub

uy/ub
×103

bxy
×101

byz
×103

(byy − bzz)
×102

l
a
i
t
i
n
I

l
a
n
i
F

h
t
u
r
T

(b)

y

z

2h

Figure 4. (a): Velocity and anisotropic Reynolds stress results of learning a NLEVM from
full ﬁeld velocity measurements in ﬂow through a square duct. The uz and bxz ﬁelds are the
reﬂection of uy and bxy along the diagonal. (b): Schematic of ﬂow through a square duct showing
the secondary in-plane velocities. The simulation domain (bottom left quadrant) is highlighted.

weight to the axial velocity discrepancy. The NLEVM is the Shih quadratic model (Shih
et al. 1993) which, using the basis in equation 2.3, can be written as

g1(θ1, θ2) =

g3(θ1, θ2) =

2/3

−
1.25 + √2θ1 + 0.9√

1.5
1000 + (√2θ1)3

,

2θ2

−

,

g2(θ1, θ2) =

g4(θ1, θ2) =

7.5
1000 + (√2θ1)3
9.5
1000 + (√2θ1)3

−

,

.

(3.1)

For the ﬂow in a square duct only four combinations of the Reynolds stress components
aﬀect the predicted velocity (Speziale 1982): τxy and τxz in the axial equation and τyz
τyy) in the in-plane equation. In this ﬂow the in-plane velocity gradients are
and (τzz −
orders of magnitude smaller than the gradients of the axial velocity ux. For these reasons
only two combinations of coeﬃcient functions can be learned, g(1) and the combination
0.5g(3) + 0.5g(4), and there is only one independent scalar invariant with θ1 ≈ −
g(2)
θ2.

−

05θ1−0.4−0.3−0.2−0.1g(1)05θ10.0000.0010.0020.003g(2)−0.5g(3)+0.5g(4)TruthInitialFinal01.32-1.3101.82-1.500-7.480-1.5201.518

C. A. Michel´en Str¨ofer and H. Xiao

Figure 5. Velocity results of learning an eddy viscosity model from sparse velocity data of ﬂow
over periodic hills. Six diﬀerent proﬁles of ux velocity are shown. The training case corresponds
to the baseline geometry α = 1.

−

The neural network has two inputs and four outputs and was pre-trained to the LEVM
with g(1) =
0.09. The turbulent equations used are those from the Shih quadratic k–
−
ε model. Figure 3 shows the learned model which shows improved agreement with the
truth. The combination g2
0.5g(3) + 0.5g(4) shows good agreement only for the higher
range of scalar invariant θ1. This is due to the smaller scalar invariants corresponding to
smaller velocity gradients and smaller magnitudes of the tensors T . The velocity ﬁeld is
therefor expected to be less sensitive to the value of the Reynolds stress in these regions.
It was observed that the smaller range of the invariant, where the learned model fails to
capture the truth, occurs mostly in the center channel. Figure 4 shows the ability of the
learned model to capture the correct velocity, including predicting the in-plane velocities,
and the Reynolds stress. The trained model fails to predict the correct τyz in the center
channel, but this does not propagate to the predicted velocities. Additionally, it was
observed that obtaining signiﬁcant improvement in the velocity ﬁeld requires only a few
tens of training steps and only requires the coeﬃcients to have roughly the correct order
of magnitude. On the other hand obtaining better agreement of the scalar coeﬃcients
took 1–2 orders of magnitude more training steps with diminishing returns in velocity
improvement. This shows the importance of using synthetic data to evaluate the ability
of a training framework to learn the true underlying model when one exists rather than
only comparing the quantities of interest.

3.3. Learning a LEVM from realistic data of ﬂow over periodic hills
As a ﬁnal test case, a LEVM is trained using sparse velocity measurements from DNS of
ﬂow over periodic hills. The DNS data comes from Xiao et al. (2020) who performed DNS
of ﬂow over periodic hills of varying slopes. This ﬂow is characterised by a recirculation
region on the leeward side of the hill and scaling the hill width (scale factor α) modiﬁes
the slope and the characteristics of the recirculation region (e.g. from mild separation
for α = 1.5 to massive separation for α = 0.5). For all ﬂows, the Reynolds number
based on hill height h and bulk velocity ub through the vertical proﬁle at the hill top
is Re = 5, 600. The training data consists of four point measurements of both velocity
components in the ﬂow over periodic hills with the baseline α = 1 geometry. The two
components of velocity are scaled equally in the objective function. The training data
and training results are shown in ﬁgure 5. The neural network in this case has one input
and one output and is pre-trained to laminar ﬂow, i.e. g(1) = 0. The trained model is a
spatially varying LEVM g(1) = g(1)(θ1) that closely predicts the true velocity in most of
the ﬂow with the exception of the free shear layer in the leeward side of the hills.

0246810x/h,0.05ux/ub+x/h0123y/hInitial(laminar)FinalTruth(DNS)ObservationsDiﬀerentiable learning of turbulence models from indirect observations

9

(a)

(b)

Figure 6. Comparison of horizontal velocity ux predictions using the trained eddy viscosity
model g(1) = g(1)(θ1) and the k–ω model g(1) = −0.09 on two periodic hills geometries: (a)
α = 0.5 and (b) α = 1.5.

To test the extrapolation performance of the trained LEVM, we use it to predict the
[0.5, 0.8, 1.2, 1.5], and compare them to
ﬂow over the other periodic hill geometries, α
results with the k–ω model g(1) =
0.09. The results for α = 0.5 and α = 1.5 are shown
in ﬁgure 6. For the α > 1.0 cases the trained linear model outperforms the k–ω model
in the entire ﬂow. For the α < 1.0 cases the the trained model results in better velocity
predictions in some regions, particulary the upper channel, while the k–ω model results
in better velocities in the lower channel.

−

∈

4. Conclusions

In this paper we present a framework to train deep learning turbulence models using
quantities derived from velocity and pressure that are readily available for a wide range
of ﬂows. The method was ﬁrst tested using synthetic data obtained from two traditional
closure models: the linear k–ω and the Shih quadratic k–ε models. These two cases
demonstrate the ability to learn the true underlying turbulence closure from measurement
data when one exists. The method was then used to learn a linear eddy viscosity model
from synthetic sparse velocity data derived from DNS simulations of ﬂow over periodic
hills. The trained model was used to predict ﬂow over periodic hills of diﬀerent geometries.
This work demonstrates that deep learning turbulence models can be trained from
indirect observations when the relevant sensitivities of the RANS equations are available.
With the growing interest in diﬀerentiable programming for scientiﬁc simulations, it is
expected that the availability of derivative information will become more commonplace
in scientiﬁc and engineering computations, making it more seamless to couple scientiﬁc
computations with novel deep learning methods.

Acknowledgements

This material is based on research sponsored by the U.S. Air Force under agree-
ment number FA865019-2-2204. The U.S. Government is authorised to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright notation
thereon.

Declaration of interests

The authors report no conﬂict of interest.

k–ωLearnedNLEVMTruth(DNS)02468x/h,0.05ux/ub+x/h0123y/h024681012x/h,0.05ux/ub+x/h0123y/h10

C. A. Michel´en Str¨ofer and H. Xiao

REFERENCES
Audet, C. & Kokkolaras, M. 2016 Blackbox and derivative-free optimization: theory,

algorithms and applications. Optimization and Engineering 17, 1–2.

Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. 2019 Scientiﬁc machine learning:

How Julia employs diﬀerentiable programming to do it best. SIAM News 5.

Brener, B. P., Cruz, M. A., Thompson, R. L. & Anjos, R. P. 2021 Conditioning
and accurate solutions of Reynolds average Navier–Stokes equations with data-driven
turbulence closures. Journal of Fluid Mechanics 915.

Gatski, T. B. & Speziale, C. G. 1993 On explicit algebraic stress models for complex turbulent

ﬂows. Journal of ﬂuid Mechanics 254, 59–78.

Holland, J. R., Baeder, J. D. & Duraisamy, K. 2019 Field inversion and machine learning
with embedded neural networks: Physics-consistent neural network training. In AIAA
Aviation Forum. Paper 2019-3200 .

Jameson, A. 1988 Aerodynamic design via control theory. Journal of Scientiﬁc Computing

3 (3), 233–260.

LeCun, Y., Bengio, Y. & Hinton, G. 2015 Deep learning. Nature 521 (7553), 436–444.
Ling, J., Kurzawski, A. & Templeton, J. 2016 Reynolds averaged turbulence modelling
using deep neural networks with embedded invariance. Journal of Fluid Mechanics 807,
155–166.

Michel´en Str¨ofer, C. A. 2021 Machine learning and ﬁeld inversion approaches to data-driven
turbulence modeling. PhD thesis, Virginia Tech. https://vtechworks.lib.vt.edu.
Oriani, M & Pierrot, G 2016 Alternative solution algorithms for primal and adjoint

incompressible navier-stokes. In ECCOMAS Congress, pp. 3858–3882.

Othmer, C. 2008 A continuous adjoint formulation for the computation of topological and
surface sensitivities of ducted ﬂows. International journal for numerical methods in ﬂuids
58 (8), 861–877.

Othmer, C. 2014 Adjoint methods for car aerodynamics. Journal of Mathematics in Industry

4 (1), 6.

Parish, Eric J & Duraisamy, Karthik 2016 A paradigm for data-driven predictive modeling
using ﬁeld inversion and machine learning. Journal of Computational Physics 305, 758–
774.

Pope, SB 1975 A more general eﬀective-viscosity hypothesis. Journal of Fluid Mechanics 72 (2),

331–340.

Raghu, M., Poole, B., Kleinberg, J., Ganguli, S. & Sohl-Dickstein, J. 2017 On
the expressive power of deep neural networks. In International conference on machine
learning, pp. 2847–2854.

Raissi, Maziar, Perdikaris, Paris & Karniadakis, George E 2019 Physics-informed neural
networks: A deep learning framework for solving forward and inverse problems involving
nonlinear partial diﬀerential equations. Journal of Computational Physics 378, 686–707.
Shih, T.-H., Zhu, J. & Lumley, J. L. 1993 A realizable Reynolds stress algebraic equation

model. Technical Memorandum 105993. NASA.

Speziale, C. G. 1982 On turbulent secondary ﬂows in pipes of noncircular cross-section.

International Journal of Engineering Science 20 (7), 863–872.

Weatheritt, J. & Sandberg, R. D. 2017 The development of algebraic stress models using a
novel evolutionary algorithm. International Journal of Heat and Fluid Flow 68, 298–318.
Wilcox, D. C. 1998 Turbulence modeling for CFD, , vol. 2. DCW industries La Canada, CA.
Wu, J. L., Xiao, H., Sun, R. & Wang, Q. 2019 Reynolds-averaged Navier–Stokes equations
with explicit data-driven Reynolds stress closure can be ill-conditioned. Journal of Fluid
Mechanics 869, 553–586.

Xiao, H., Wu, J. L., Laizet, S. & Duan, L. 2020 Flows over periodic hills of parameterized
geometries: A dataset for data-driven turbulence modeling from direct simulations.
Computers & Fluids 200, 104431.

Zhao, Y., Akolekar, H. D., Weatheritt, J., Michelassi, V. & Sandberg, R. D. 2020
RANS turbulence model development using CFD-driven machine learning. Journal of
Computational Physics p. 109413.

