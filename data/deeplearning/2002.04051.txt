Advances in Deep Space Exploration via Simulators & Deep
Learning

James Bird1,∗, Linda Petzold1, Philip Lubin2, Julia Deacon3

0
2
0
2

n
u
J

6

]

M

I
.
h
p
-
o
r
t
s
a
[

2
v
1
5
0
4
0
.
2
0
0
2
:
v
i
X
r
a

Abstract

The NASA Starlight and Breakthrough Starshot programs conceptualizes fast interstellar
travel via small relativistic spacecraft that are propelled by directed energy. This process
is radically diﬀerent from traditional space travel and trades large and slow spacecraft for
small, fast, inexpensive, and fragile ones. The main goal of these wafer satellites is to gather
useful images during their deep space journey. We introduce and solve some of the main
problems that accompany this concept. First, we need an object detection system that can
detect planets that we have never seen before, some containing features that we may not
even know exist in the universe. Second, once we have images of exoplanets, we need a
way to take these images and rank them by importance. Equipment fails and data rates
are slow, thus we need a method to ensure that the most important images to humankind
are the ones that are prioritized for data transfer. Finally, the energy on board is minimal
and must be conserved and used sparingly. No exoplanet images should be missed, but
using energy erroneously would be detrimental. We introduce simulator-based methods that
leverage artiﬁcial intelligence, mostly in the form of computer vision, in order to solve all
three of these issues. Our results conﬁrm that simulators provide an extremely rich training
environment that surpasses that of real images, and can be used to train models on features
that have yet to be observed by humans. We also show that the immersive and adaptable
environment provided by the simulator, combined with deep learning, lets us navigate and
save energy in an otherwise implausible way.

Keywords:
detection, novelty detection

computer vision, simulator, deep learning, space, universe, exoplanet, object

∗Corresponding author
Email addresses: james_bird@ucsb.edu
(James Bird), petzold@ucsb.edu (Linda Petzold),
lubin@ucsb.edu (Philip Lubin),
jcdeacon@alumni.caltech.edu (Julia Deacon)

1Department of Computer Science, University of
California Santa Barbara, 2104 Harold Frank Hall,
Santa Barbara, CA 93106-9530

2Department of Physics, Broida Hall, University

Preprint submitted to Advances in Space Research

1. Introduction

Space travel, up until recently, was
large

constrained by chemical propulsion,

of California Santa Barbara, Santa Barbara, CA
93106-9530

3Department of Computing and Mathematical
Sciences, California Institute of Technology, 1200 E.
California Blvd., MC 305-16, Pasadena, California
91125-2100

June 9, 2020

 
 
 
 
 
 
spacecraft, and therefore, relatively slow
speeds. Since the main objective has been
exploration of our solar system, these meth-
ods were suﬃcient. In contrast, the recent
Starlight program (Kulkarni et al., 2017)
has introduced methods for deep space
travel that utilize small discs, which travel
at approximately one-fourth of the speed of
light via directed energy.

for space travel

Alongside the prospect of

fast deep
space travel comes many new challenges.
The normal model
in-
cludes spacecraft capable of housing instru-
ments, propulsion and navigational equip-
ment, telescopes, energy banks, and much
more. Since the Starlight program will be
utilizing small wafersats that are approxi-
mately the size of a coﬀee can lid, all of
these features need to be reworked or dis-
carded.

Besides physical constraints, this new
model of space travel introduces feasibility
constraints as well. The star of interest is
beyond four light-years away, meaning that
transmission of data and response command
transmissions are a combined eight years or
more. Thus, the wafersats need to be able
to make decisions without human interven-
tion, and for that, artiﬁcial intelligence (AI)
is paramount.

The major hurdles that we will discuss
are those concerning computer vision via
planetary detection, data and storage block-
ages via novelty detection and ranking, and
energy management via combining simula-
tor features with subtraction-algorithm-fed
computer vision. For all of these issues, tak-
ing advantage of a universe simulator will
introduce solutions that were otherwise in-
eﬀective or impossible to ﬁnd.

2

1.1. Previous Work

The eﬀectiveness of machine learning,
speciﬁcally deep learning via TensorFlow
and cuDNN, has been indisputably demon-
strated in the last decade (Abadi et al.
(2016), Chetlur et al. (2014), Canziani
et al. (2016)). The ﬁght over the best
model and the most accurate results, espe-
cially between the most popular models like
ResNets, DenseNets (Huang et al. (2018)),
Inception(Szegedy et al.
(2015)), Masks
(He et al. (2018)), and models that com-
bine some of these together(Szegedy et al.
(2016)), is one that has produced a plethora
of potent options to choose from. Models
that are more accurate than human beings
at doing extremely diﬃcult tasks are still
being discovered (Rajpurkar et al. (2017)).
The areas of deep learning and astron-
omy have come together in recent years
(Ruﬃo et al. (2018), Morad et al. (2018),
(2018), Pearson et al.
Schaefer et al.
(2017)), mostly in the form of light curves
(Shallue & Vanderburg (2017), Zucker et
al. (2018), Carrasco-Davis et al. (2018)).
The results and general concepts promote
a healthy symbiosis between deep learn-
ing and the problems that arise in astron-
omy. Yet, the processes are carried out from
Earth, not space, and do not address real
images, two big issues that create a gap in
comparability.

Outside of astronomy, simulators have
been used to train data in speciﬁc instances
where the beneﬁts outweigh the drawbacks.
Smyth et al. (2018) outlines some major
drawbacks, namely that the process takes a
lot of time and knowledge, as well as a note
that simulator-based training may not gen-
eralize well to real images. Alongside those
concerns, McDuﬀ et al. (2018) begins with a
common issue in machine learning models,
which is that training data sets are often

biased. This bias arises when there are mi-
norities in the training set, which in turn
produces poor results when the model is
asked to evaluate a similar entity in the pop-
ulation. These issues are handled through-
out this paper and are shown to not be an
issue with the speciﬁc problem at hand.

Simulators also introduce a lot of ben-
eﬁts. One large one, also seen in Con-
nor & Leeuwen (2018), is that ”the small
catalogue of real events is probably not
yet a representative sample of the under-
lying .. population, nor is it big enough to
build a meaningful training set for machine
learning, deep or otherwise.” An important
theme throughout this paper, and an ex-
tremely useful aspect of simulators, is that
they provide an untold amount of training
data, assuming that one can create realistic
simulations.

1.2. Unsupervised Learning for Planetary

Detection

The intuition behind object detection, in
particular planetary detection, might point
toward an unsupervised learning technique.
After all, one might reasonably think that
detecting a nearby planet after months of
traveling through deep space would be easy.
We test this idea using an unsupervised
technique called a Grow When Required
(GWR) Network (Marsland et al., 2002).

1.2.1. GWR Setup

Using the worldwide telescope, we gen-
erated a 9,000 frame series of solar system
images. It begins with Neptune, then it ex-
plores Mercury, the Sun, and ﬁnally Mars.
The majority of images contain only back-
ground stars.

The images were down-scaled to a
320x180 resolution in order to improve com-
putational speed. For learning, they were

3

decomposed into red, green, and blue chan-
nels and vectors were constructed of length
320 × 180 × 3 = 172, 800.

Our challenge is to label each image as
novel or regular. That is, we wish to gener-
ate a classiﬁcation n s.t. for each input x,
n(x) ∈ {0, 1}, where a 0 indicates regular-
ity and a 1 indicates novelty. The algorithm
should hopefully yield a large number of 1’s
when a planet or the Sun is clearly in view
and should yield very few 1’s when the im-
age is mostly distant stars.

1.2.2. GWR Algorithm

Deﬁne A as the set of nodes in our net-
work and C as the set of edges between
these nodes. We denote our inputs as ξ and
the weight vector for any node n as wn.
Each node n has a habituation hn which
represents how familiar that node is to the
system.

1. Initialize two nodes that represent two
random samples from the dataset. We
set their habituations each to 1. The
set of edges between nodes begins
empty.

2. Iterate through the dataset. For each

data sample ξ:
(a) For each node i

in the net-
work, calculate its distance from
ξ, which is ||ξ − wi||.

(b) Find the node s with the smallest
distance and the node t with the
second-smallest distance.

(c) Add an edge between s and t if it

does not already exist.

(d) Calculate

the

activity
a = exp(− (cid:80)
j(ξ[j] − ws[j])2/C),
where C = 29,203,200 was
chosen to prevent an integer
overﬂow. There are 172,800 ﬁelds
in each data vector, and since

the average of
the quantities
in each vector is close to 13,
and 132 = 169, we divide by
172, 800 × 169 = 29, 203, 200.
(e) If a < aT and s’s habituation
hs < hT (where aT is some inser-
tion threshold and hT is some ha-
bituation threshold), then add a
new node r. Set wr = ws+ξ
. Insert
edges between s and r and r and
t and remove the edge between s
and t.

2

(f) Otherwise, update the weight and
habituation of s as follows: δws =
(cid:15)b × (ξ − ws) and δhs = τb ×
1.05 × (1 − hs) − τb, where (cid:15)b and
τb are parameters. Next, update
the weight and habituation of s’s
neighbors i as follows: δwi = (cid:15)n ×
(ξ − wi) and δhi = τn × 1.05 × (1 −
hi) − τn, where (cid:15)n and τn are other
parameters.

(g) Remove any nodes without any

neighbors.

Our chosen values are: aT = 0.7, hT =
0.1, τb = 0.3, τn = 0.1, (cid:15)b = 0.1, and
(cid:15)n = 0.01.

1.2.3. Results

Figure 1 is a scatter plot that was gener-
ated to visualize the novelty detected from
the data. The x-axis is the id of each pic-
ture, and the y-axis is the number of novel
images that were detected in each bin of 100
images. Figure 2 is a continuous represen-
tation of the same concept.

Moving along the Image ID axis, we
see that novelty was detected in clumps
around 0-500, 900, 4000-4500, 6000-6500,
7000-7500, 7700-8000, and 8200-8500. We
observed that novelty was detected ﬁrst on
Neptune, then again on some particularly

Figure 1: A scatter plot of the detected novelty of
the data.

bright stars. No novelty is detected dur-
ing the long period of only stars. Next we
see increased novelty detection when Mer-
cury is plainly in view, and then when the
sun appears, and ﬁnally when we zoom into
Mars.

Figure 2: A binned scatter plot of the novelty of the
data. Image ranges that are salient to the human
eye are labeled on the plot.

We notice that Neptune’s collection of
novelty is roughly one quarter the size of the
other three celestial objects that come into

4

view. We also notice a huge spike around
image 8300. This is very interesting because
there are no large celestial objects in view
at this time.

1.2.4. GWR Discussion

A deep space exploration mission would
come with many challenging objectives. A
small but connected subset of those would
involve detecting objects, deciding whether
they are important, extracting key features
that we would want to study or observe, and
prioritizing their information retrieval.

GWR wouldn’t be able to decide impor-
tance, extract features, or prioritize infor-
mation retrieval, yet if it could detect novel
objects in deep space, this would be useful.
We can see from Figure 1 and Figure 2 that
the detection is inconsistent and unreliable.
Neptune is almost completely missed and
the three smaller peaks at the Sun, as seen
in Figure 2, are larger than Neptune. The
largest peak of all happens while Mars is
minuscule and essentially not in view.

Although GWR had high novelty de-
tection peaks while passing by Mercury
and the Sun, it failed to correctly activate
at Mars or Neptune. These observations,
paired with its inability to do anything fur-
ther with the data, introduces a need for a
more advanced model that can achieve all
of the above objectives.

1.3. Object Detection vs. Novelty Detection

Throughout this paper, our main goals
will constantly be alluding to object detec-
tion and novelty detection.
In a general
computer science setting, object detection
is used to identify something in an image
that has already been trained via some algo-
rithm. For example, we may feed thousands
of images of human beings into a YOLO al-
gorithm, and then once it is trained, we can

5

walk the streets of New York and see if our
In
algorithm can identify human beings.
this setting, identifying a human being is a
success, and not identifying a car or stop
sign as a human being would also be a suc-
cess. Yet, identifying anything non-human
as a human being would be a failure. The
accuracy of a model, which is mathemat-
ically computed per identiﬁcation, can be
used as a measure of how sure the algorithm
is that the object being identiﬁed is the cor-
rect type. In this paper, we will delve into
why this is diﬃcult for our speciﬁc scenario,
and we will test whether this can beneﬁt
severely from the use of simulators.

On the other hand, novelty detection is
used to attempt to identify something that
has never been seen before. One powerful
example is self-driving cars being able to
see traﬃc signs that are unique to a certain
country, and therefore have never been seen
or used during the training process (Kim et
al., 2017). In this example, the self-driving
car algorithm has never seen this speciﬁc
sign before, and so identifying it without
In our
any training data is very diﬃcult.
paper, unseen planetary features are analo-
gous to the unseen traﬃc sign in the exam-
ple, and we delve into methods of solving
this via simulators.

2. The Simulator

Although there are quite a few uni-
verse simulators available today. Here, we
utilized SpaceEngine (SpaceEngine.org) for
its realism, expansive set of options and
customizations, and unique informational
tools.

2.1. Simulator Features

One of the best features of the simula-
tor is its extremely realistic rendering capa-
In combination with a 3840x2160
bilities.

4K monitor and GTX 1080 Ti, the simula-
tor produces extremely detailed and realis-
tic images.

and lack lens ﬂares. Second, a feature called
overbright can drastically adjust how bright
the background stars and nebula appear.
Training on images that embrace the en-
tire spectrum of overbright will allow this
machine to deal with novelty detection in a
very advanced manner. Having a plethora
of options to enable and tweak can intro-
duce a much larger set of training images
and will let the AI absorb more information
before embarking into deep space.

Figure 3: Examples of 3D-rendered randomly gen-
erated exoplanets

The simulator also includes the ability
to edit any planet, so that instantly ren-
dering an exoplanet with a very particular
feature set is simple. Alongside image fea-
tures are astronomical features, which are
tracked and shown for every body in the
universe. Some of these features are type,
class, orbital period and mass, but most im-
portantly, distance.

2.2. Simulator Options

Graphical options in the simulator are
abundant, which allow for complete con-
trol of the simulated universe. In general,
the feature set should be optimally set for
realism while traveling through space, but
the ability to tweak these options speaks
to greater breadth for learning and adapt-
ing to unique situations that may arise in
space. For example, the image of an exo-
planet while traveling at one-fourth of the
speed of light with a nebula in the back-
ground is a completely new concept. Yet,
two features in the simulator may be able
to deal with that combination. First, the
ability to toggle lens ﬂares will provide the
AI with training images that both contain

Figure 4: Eight lens options applied to the same
star

Some other important options, besides
those that deal with graphics and render-
ing, are diﬀraction spike intricacy and size,
lens eﬀect on stars, and planetary shine. Al-
tering all of these settings and training on
the resulting images enables the capture of
more information.

2.3. Overall Simulator Importance

In this paper, We consider three main ar-
eas of deep space travel that can be drasti-
cally improved with the use of a simulator.

6

First, computer vision is an extremely
useful tool for detecting objects and mak-
ing decisions based on what is seen. The
training process consists of tagging images
and providing a label for each tag, feeding
those images and tags into a model, and
having that model learn the associations.
The model can then be given images, and
based on how successfully it was trained, it
may be able to identify parts of the image.
Since we have never photographed exo-
planets in detail, training a model using real
images is not feasible. Therefore, we rely
on training using images of planets that we
have photographed, which would be those
in our own solar system. Yet, detailed pho-
tographs of planets are not very abundant
and would only teach the model to look for
those speciﬁc features. In realizing that this
would not be suﬃcient, we may move to-
ward novelty detection, a branch of com-
puter vision that tries to classify data that
deviates from the data used during training.
Co-domain embedding (Kim et al., 2017)
has proven useful in some situations, such as
those where a template design would resem-
ble a real image almost exactly, but plane-
tary features do not translate well to use in
novelty detection. This is because planetary
features, such as atmospheric patterns, are
extremely unique.

Simulators can provide very detailed and
randomly generated images of planets that
obey universal physical laws. Therefore, we
will be able to generate countless images of
planets that resemble real images of possi-
ble exoplanets. Training on these images
and features, the model will learn an exor-
bitant amount of information. While travel-
ing through space and faced with an image
of a real exoplanet, the model will now have
a much broader knowledge base.

Second, we introduce the notion of nov-

7

elty ranking. A major hurdle in deep space
wafersat travel is data storage and transmis-
sion. On-board memory is limited by phys-
ical constraints of the wafersat and astro-
physical exposure, while transmitting data
from a wafersat to a communications hub
would be slow and dependent on energy re-
serves.

A system that can deal with this issue
is one that prioritizes the most important
on-board data and sends that ﬁrst. This
not only ensures that the critical images are
sent in descending order of importance in
case of some malfunction, but that the most
relevant data is quickly known for the next
wafersats in line.

With the overarching goal being the iden-
tiﬁcation and transmission of the most im-
portant data, novelty ranking will quantify
the on-board images based on importance.
Simulators will provide the breadth of plan-
etary features that are needed to ﬁnd out
what importance means, as perceived by hu-
mans, and then this information can be ap-
plied to software.

Third and last, sending a small disc into
deep space means that on-board energy re-
serves will be very small. Yet, the objective
of detecting and imaging astronomical bod-
ies while traveling must still be met.

3. Simulator for Planetary Detection

Our main objective here is to identify
novel planets while traveling through deep
In order to do so, and for subse-
space.
quent sections, we will require a basic con-
ceptual understanding of object detection
in order to logically progress. We should
point out that the main backbone of object
detection, through a few core processes, is
the same as that used by humans when they
naturally process information and identify

objects. We will discuss these fundamental
core processes.

First, the object that we will try to have
the machine identify should be seen before-
hand in order to train the model. Unsu-
pervised techniques have their uses and do
not require this training process, but we
will only deal with supervised learning mod-
els from here on out. Mainly, this is done
because planetary detection is the simplest
task, so we need a model that can adapt
afterwards in order to successfully identify
planetary features and rank novelty.

Second, the machine will

learn these
trained models better with more images. Of
course there are exceptions here, such as
feeding poor images or images that do not
match the objects category. We will test
this concept thoroughly while we also test
the importance of simulated images.

3.1. Setup

Here we will discuss the details of our
model, our hypothesis, and how we will go
about testing the importance of simulator
images. Our main goals when choosing a
model are ﬁnding one that has high accu-
racy, low to medium computation time, and
has been tested to be a reliable model. Be-
cause of this, no new models that haven’t
had time to be tested thoroughly through-
out the computer vision community will be
used. Also, the ideal model will sacriﬁce
computation time for accuracy, if needed.
Since we will be testing diﬀerent combina-
tions of image sets with the same model, our
model choice will be a control and therefore
we will again stress the importance of relia-
bility and accuracy over computation time
or creation date.

Our hypothesis stems from our sec-
ond core process and states that simula-
tor images will not decrease accuracy for

8

planetary detection and planetary features.
These two processes, the detection of plan-
ets while traveling through space, as well
as the detection and recognition of features
on those planets, are the inspiration for the
two main experiments that are set up. Cur-
rently, our collection of useful astronomi-
cal images is very limited. Therefore, us-
ing only real images of planets would limit
us to those found in our solar system. Also,
planetary features would suﬀer since our so-
lar system contains very few features out of
the set of total planetary feature combina-
tions.

The ﬁrst experiment will test the validity
of simulator images in general. It is set up in
three diﬀerent stages using the same object
detection framework and always testing on
the set of real images of Jupiter. First, we
will train on real images of every planet in
our solar system except for Jupiter. These
images will be collected from NASA image
repositories and will not include composite
images, artist renditions, or any other varia-
tions except for true unaltered images. Sec-
ond, we will train on only simulator images
with the goal of solidifying whether simula-
tor images alone are useful in detecting real
planets. Third, we will combine the ﬁrst
and second training sets, comprised of sim-
ulator images and all real images (exclud-
ing Jupiter),to determine whether simulator
images and real images together provide the
best of both worlds.

The second experiment will

introduce
and test an extremely important feature
of using simulator images - the ability to
detect novel planetary features, i.e. those
which have never been seen in any real im-
ages. Since simulators can be programmed
to emulate real physics, the outcome can
give us an extremely large number of realis-
tic looking planets with features that have

never been observed. In order to proceed,
we need to use a planetary feature that
exists in our solar system so that we can
train using simulator images and test using
real images. Planetary rings have a solid
theoretical foundation and would easily ap-
pear in any physics-based simulator, while
also being present around Saturn. Rings
are also fairly complex, as they contain ex-
tremely unique striation patterns, can look
wildly diﬀerent depending on the viewing
angle, and can even co-exist with other rings
around the same host planet.

3.2. The Model

When using deep learning models for a
speciﬁc purpose,
it is imperative that a
model is chosen that optimizes what it can
while prioritizing what it must. For exam-
ple, an object detection model that may be
implemented on a smart phone for real-time
detection of human faces might prioritize
speed and give up a small amount of ac-
curacy.

For our purposes, accuracy is of the ut-
most importance, while operation count is
also of some importance. In deep space, we
have plenty of time to do calculations, but
we also have very little energy. Therefore,
reaching maximum accuracy with a small
amount of operations is the ideal scenario.
As we can see from Figure 5, origi-
nally presented in Canziani et al. (2016)
whereas the authors compared many models
for practical applications like this one, there
are few models that ﬁt into the optimal
space of accuracy and operations. The main
choice was ResNet architecture vs. Incep-
tion architecture. The accuracy and opera-
tions for both models are almost identical,
yet the residual neural network (ResNet) ar-
chitecture provides a shortcut in case the

Figure 5: Results shown in Canziani et al.(2016)
that compare model accuracy vs. operation count
during the ImageNet challenge.

training phase introduces the vanishing gra-
dient problem (He et al., 2015). Along with
this feature, ResNet is a very established
model in many domains, and for these rea-
sons, will be our model of choice going for-
ward.

3.3. Experiment #1 - Planetary Detection
In this experiment, we tested the theory
that simulator images could be used to train
a model that could then detect real objects,
and in particular, planets. Our hypothesis
is that simulator images are at least as good
as real images in terms of information gain
during training. Although simulator images
can be produced in bulk, the idea was to
test the theory using similar image count in
order to avoid any bias. The table below
shows the number of images used for each
model and for their testing phase.

Real Sim Real+Sim

120

122

242

For each of the three separate runs, the
models reached a minimum of 60,000 itera-

9

tions in order to ensure ample training time
and accuracy. Concerning the testing im-
ages, the images were broken down into 2
sections. The ﬁrst section was comprised of
independent images taken at diﬀering an-
gles. The second section was the exact same
frame of reference, including angle and dis-
tance, but included a time-lapsed series of
images.

The results of the testing phase produced
a model accuracy, which is a mathematical
score given by the model as to how surely
it has identiﬁed an object that it has previ-
ously seen during the training phase. The
table below shows the ﬁnal accuracy results.

Section

Real

Sim Real+Sim

1
2
Total

99.875 99.375
98.889
99.353

100
99.706

99.875
100
99.941

The results show quite a few compelling
results right oﬀ the bat. The most direct
one being that Real+Sim has achieved equal
or better results than Real or Sim alone did
in all categories. Besides Real+Sim, we can
also say something about Real vs. Sim. Al-
though Real achieved slightly better accu-
racy in Section 1, Sim not only achieved
better accuracy in Section 2, the total ac-
curacy of Sim was also higher and Sim con-
tained at least one section that had perfect
accuracy.

Our initial hypothesis was that using
simulator images would be as good as real
images. Diﬀerent models and training im-
age sets will always produce diﬀerent re-
sults, but considering total accuracy with
our two sections, Sim produced equal or
better results when compared to Real.

10

3.4. Experiment #2 - Novel Features

In order to be able to show the impor-
tance of using a simulator for novel plane-
tary features, we use the results from Exper-
iment #1 as proof of concept. Those show
that Sim images do not decrease accuracy
on real testing images, with the added ben-
eﬁt of being able to mass produce them and
customize feature information in each im-
age.

With this in mind, Experiment #2 will
gather simulator images of ringed planets
and train a new model with the same frame-
work as Experiment #1. We will then test
novel feature detection on real images of
Saturn. The machine will have never seen
any real image and will have never been
exposed to prior knowledge of Saturn or
our solar system at all. This experiment
is, in theory, identical to training a model
with simulator images on Earth and send-
ing it out into deep space in order to iden-
tify novel, never-before-seen features found
on real exoplanets and in real images.

One of the main beneﬁts of simulator im-
ages can be observed here. Even a planetary
feature that we can observe will be found
once, or perhaps a few times at best. There-
fore, we have limited variability to work
with in terms of ring structure, width, pat-
tern, count, etc.. Yet, if this experiment
produces promising results, we can simply
build a physics-based simulator that gener-
ates planets, ﬁlter by the presence of rings,
capture an image, and repeat the process
any amount of times. From Experiment #1,
we know that training on these simulator
images will net us generally equivalent in-
formation gain when compared to real im-
ages of ringed planets. Since our simula-
tor is physics-based, it should produce many
features that we have not even seen before,

transforming this problem from novelty de-
tection into object detection.

The experiment was set up in parallel to
what would hypothetically happen during
deep space exploration. The training was
done on a small batch of simulated images
of ringed planets. The idea in the experi-
ment is that, in theory, we have never seen
a ringed planet before. Yet, our physics-
based models of planet formation dictate
that they would naturally occur. So we col-
lect simulated images, train on that, and
then send it deep into space. Upon ﬁnding
a ringed planet for the ﬁrst time, it would
need to recognize those planets. Normally,
we wouldn’t be able to do this since we have
no ringed planets to train on (in this hypo-
thetical experiment), but since we used sim-
ulated images, we now have a model to deal
with this. The experiment goes through
this entire process, and even tests the model
on real images of Saturn. Again, the ma-
chine has only seen a small batch of ran-
domly generated simulated images of hypo-
thetical ringed planets, never a real image
of a planet. The results of the experiment
are extremely positive and can be seen in
the table below.

having to detect unknown features, we can
simply construct planets randomly based on
physical laws and train a model using those
simulator images.

4. Simulator for Novelty Ranking

We have shown previously that simula-
tor images can be used with astounding
accuracy, and with mass production, can
make training via real images unnecessary.
Therefore, we can train using hundreds or
thousands of simulated images and when we
encounter a planet, we can detect it, image
it, and send those images back to Earth.

Say, for instance, that the wafer passes
and images the ﬁve planets in a hypothetical
solar system. Soon after that, it may be on
an inevitable course toward that solar sys-
tem’s star, which will destroy the wafer and
all of the images. One downside to small
wafers is that they are easily destroyed or
corrupted. This makes a priority system vi-
tally important, as it would allow the wafer
to possibly send back one or two images
from the ﬁve that it collected before it is
destroyed. This section is dedicated to ﬁg-
uring out which images should be sent back,
and discussing the approach in doing so.

Type Accuracy Detection Errors

4.1. The Concept

Planet
Rings

99.22
99.11

None
None

As we can see, the model is dependably
accurate based on solely simulated images.
This experiment shows a key point of us-
ing simulators - by combining planet gener-
ation theory and realistic rendering, we have
turned a novelty detection problem into an
object detection problem, which is signiﬁ-
cantly easier to deal with. Now, instead of

11

We will assume that we have a small stor-
age of images that we need to send back to
Earth in an order that is based on impor-
tance. Wafers could be destroyed relatively
easily and data transmission rates in space
are very slow, so sending data based on a
notion of importance is paramount.

Figure 6 helps show us the extremely ab-
stract deﬁnition of importance that we, as
humans, may place on new planets. The
top planet is colorful and full of land and
diﬀerent bodies of liquid, while the bottom

4.2. The Human Experiment

The implausibility of teaching vast con-
ceptual knowledge to a machine in hopes of
it gaining context made us seek out a dif-
ferent approach:

1. Generate simulated images of planets
that range in features. This will re-
move the bias that some people may
have about our own solar system, since
we are not using any real images of our
own planets. It will also allow data to
be gathered about features that do not
currently exist in our solar system, but
based on astrophysical theory, could
exist in the universe.

2. Ask experimental subjects to rate each
planet by Importance. This is posed via
the question: ”On a scale from 1-7, how
important would it be for humankind
to see this image if it were gathered by
a spacecraft during deep space explo-
ration?”

3. Ask experimental subjects to rate each
planetary feature. This will comprise
our total planetary feature set. For
instance: On a scale from 1-7, how
much does this planet exhibit the pres-
ence.. of rings? ..of an atmosphere?
..of moons? ..of a livable environment
for humans?

4. Using the data gathered from human
thought processes and individual anal-
ysis of importance and interestingness
of a planet, train a model to predict
importance given a feature set.

5. Rank all planets in storage based on
importance and send them back to
Earth via this priority system.

This process takes in human deﬁnitions
and thought processes in order to break

12

Figure 6: Hypothetical storage of two images that
need to be ranked based on importance.

planet has a unique double ring, an atmo-
sphere, and a single large ocean, one that
may be assumed to be water by visual in-
spection alone. The main question we want
to ask here is: If you could only send one of
these images back to humanity, which would
you send?

An astrobiologist might choose the top
planet since the presence of land and many
diﬀerently-composed bodies of liquid exist,
giving multiple opportunities for life to pos-
sibly ﬂourish. Yet, someone interested in
another planet that may be able to accom-
modate human existence might choose the
bottom planet since it seems to oﬀer two
important features for us, water and an at-
mosphere. The question of importance to
humans is very subjective, yet we need a
solution that would be able to rank these
two planets, and many more, in order of
importance.

down the concept of what we ﬁnd inter-
esting in planets that we have not even
seen before. Using this method, we can by-
pass a problem of novelty detection, which
is diﬃcult, and machine contextual learn-
ing, which is extremely diﬃcult, and turn it
into a problem of human information gain,
which is easy, and object detection, which
is also easy.

Future work will show experimental re-
sults beyond this proof of concept solution.

5. Simulator for Energy Management

During a deep space voyage, the wafer-
sat will need to be supplied with enough en-
ergy to perform necessary functions, such as
imaging, analyzing the images, and trans-
mitting data. We don’t assume that the
system is perfect, nor do we need certain re-
strictions on the amount of energy available.
We have one goal: minimizing the amount
of energy needed while ensuring planetary
detection. At one end of the spectrum is
full energy conservation, which would mean
that the camera never turns on and there-
fore we never collect any data. On the other
end of the spectrum is full energy use, mean-
ing that the camera never turns oﬀ until the
energy runs out, which would yield us many
images but most likely none with important
ﬁndings. Somewhere in between is optimal,
but how do we ﬁnd it?

5.1. The Two Phases

Simulators open a whole new universe
that can be utilized in order to make a vir-
tual interstellar journey to Alpha Centauri
hundreds of times in the span of a day. By
doing this process, we can train our models
to identify stars, predict distances, swap be-
tween the two possible phases, and in doing
so, save energy while capturing meaningful
images.

5.1.1. Phase One

Phase One is essentially comprised of
time spent in open-space travel. This would
mean that the probe is beyond a ’fair’ dis-
tance away from any nearby star and that
planetary detection would be a fruitless en-
deavor. Yet, during this phase, the main
objectives would be nearby star detection
and star distance predictions.

5.1.2. Phase Two

Phase Two would be a rare occurrence
whereas the probe has traveled within a
’fair’ distance of a star and we no longer
need to deal with nearby stars until we have
left that star’s system. Instead, this phase
would prioritize planetary detection, imag-
ing, and ranking.

5.2. The Process

The trip from Earth to Alpha Centauri
can be done in approximately 20 years. But,
in the simulator, one can travel at any speed
and cut out the majority of the time spent
in an uneventful space. This makes it pos-
sible to simulate a 20 year journey in a few
hours, or many journeys in just a single day.
Once these are done, we can train a ma-
chine learning model using star type, the
section of the image containing the star, and
the distance from the probe to the star (a
simulator feature). Combined with a sub-
traction algorithm, and only using enough
energy to take two images, the machine will
be able to identify stars and predict their
distance from the probe.

Using this information, the probe will
know the approximate distance to the near-
est star in its forward path. A simple cal-
culation can tell it a safe amount of time to
wait until it should take two more images,
conﬁdent that the time it has waited has
been uneventful.

13

Repeating this process is extremely en-
ergy eﬃcient, and should eventually lead to
coming within a reasonably ’fair’ distance
from a star. When this occurs, we would
change into Phase Two.

Phase Two would use the same intuition
except that instead of stars, we substitute
in planets. Once identiﬁed, instead of be-
ing interested in distances, we would prior-
itize imaging. Details on planetary detec-
tion, imaging, feature extraction, and rank-
ing have been detailed in earlier sections.

5.3. An example of Phase One

One extremely diﬃcult concept in this
entire process is making sure that the probe
can successfully understand what is close to
it versus what is very far away. The con-
cept used is straight-forward: bodies that
are closer will tend to shift more while the
probe travels in a straight path. As an ex-
treme example, a body that is 1 AU away
from the probe will shift from center screen
to completely oﬀ screen in approximately
33 seconds. Yet, a very distant star could
go without changing position for months or
years.

In order to deal with this, a subtraction
algorithm is implemented. The probe will
take a photo, wait a certain amount of time,
and take another photo. Then, the ﬁrst will
be subtracted from the second and the re-
sulting image will show any pixels that have
shifted state during the elapsed time.
If
enough of these pixels shift, we will get a
clear image of something that is relatively
close.

The main problem here, again, is that no-
body has any concept of the ”wait a certain
amount of time” part of the process. How
much time is the right amount of time? If
you do not wait long enough, nothing will
move and your subtracted image will be all

black.
If you wait too long, even things
that are very far away will begin to shift
and you will be left with a large amount of
stars, still unsure about which of those are
actually close. This diﬃcult part becomes
approachable with the use of simulators.

The example deals with a simulated star
that exists 0.08 light-years away from the
probe. We travel at 500c and perform a sub-
traction algorithm in 10 second intervals, re-
setting after each one. This equates to trav-
eling at 0.25c and performing a subtraction
algorithm every 20,000 seconds, or approxi-
mately every 5.55 hours. So, the ﬁrst image
is 5.55 hours in real-time, the second im-
age is 11.11 hours in real-time, then 16.66
hours, and so on. The goal is to see if simu-
lators can be useful, and if so, at what point
we would want to optimally take images in
order to ensure we capture bodies that are
nearby while also saving energy.

As we can see in Figure 7, the ﬁrst few
bins produce a hazy image of the target star.
At the ﬁfth image, which would have the
probe waiting approximately 28 hours be-
tween images, we can see a full image of
the star. By the last image, which is rep-
resented by approximately 50 hours of real
time, other nearby stars were showing very
hazy signs of recognition from the subtrac-
tion algorithm.

This proof of concept is extremely vital
to star recognition and energy management.
Depending how far away from a star we
want the probe to be when it is able to rec-
ognize it, this process can be altered and
honed easily.

6. Conclusion

We began with a set of new challenges
that arise from the Starlight program and
the ability to perform fast interstellar travel.

14

not suﬀer, which is an astounding concept
for such an application. Along with this, we
provide results and many reasons why simu-
lators enable us to identify features that we
have yet to observe in actual images. With
the use of simulators, we can run experi-
ments on humans in order to extract the
features of an important planet and use that
knowledge to dictate decisions for planetary
importance rankings.

Lastly, we demonstrate how simulators
can be utilized to save energy while ensuring
that all necessary functions are completed.
There is much planned future work on
this topic. This includes optimizing simula-
tors for this speciﬁc task, choosing the best
hardware given restraints such as size and
the harsh environment of space, and incor-
porating more human knowledge gain into
the AI process.

7. Acknowledgements

PML gratefully acknowledges funding
from NASA NIAC NNX15AL91G and
NASA NIAC NNX16AL32G for the NASA
Starlight program and the NASA California
Space Grant NASA NNX10AT93H, a gen-
erous gift from the Emmett and Gladys W.
Technology Fund, as well as support from
the Breakthrough Foundation for its Break-
through StarShot program. More details on
the NASA Starlight program can be found
at www.deepspace.ucsb.edu/Starlight.

8. Citations

References

Abadi, M., Agarwal, A., Barham, P., et al.
learn-
on heterogeneous distributed systems,

2016, TensorFlow:large-scale machine
ing
arXiv:1603.04467v2 [cs.DC].

15

Figure 7:
20,000 second intervals

Subtraction algorithm performed in

These include identifying stars,
identify-
ing new planets, extracting never-before-
seen features, conceptually ranking these
new planets against each other in terms
of importance, understanding what impor-
tance means in the context of planets, and
conserving energy while performing needed
tasks.

We started oﬀ by showing that a simple
classiﬁcation model would not suﬃce. Not
only does it perform poorly, but it does not
come with the range of tools that are needed
for further processes down the line.

We show that while training on simulated
images, our accuracy on real images does

strong gravitational lens detectors, A&A, 611,
A2 (2018).

Shallue, C.J., & Vanderburg, A. 2017, Identifying
exoplanets with deep learning: a ﬁve planet reso-
nant chain around Kepler-80 and an eight planet
around Kepler-90, AJ, 155, 94.

Smyth, D.L., Glavin, F.G., & Madden, M.G. 2018,
Using a game engine to simulate critical inci-
dents and data collection by autonomous drones,
IEEE Games, Entertainment, Media Conference
(GEM), Galway, 2018, pp. 1-9.

Szegedy, C., Ioﬀe, S., Vanhoucke, V., & Alemi,
A. 2016,
Inception-ResNet and
the impact of residual connections on learning,
arXiv:1602.07261v2 [cs.CV].

Inception-v4,

Szegedy, C., Vanhoucke, V., Ioﬀe, S., et al. 2015,
Rethinking the inception architecture for com-
puter vision, arXiv:1512.00567v3 [cs.CV].

Zucker, S., Giryes, R. 2018, Shallow transits - deep
learning I: feasibility study of deep learning to
detect periodic transits of exoplanets, AJ, 155,
147.

Canziani, A., Paszke, A., & Culurciello, E. 2016, An
analysis of deep neural network models for prac-
tical applications, arXiv:1605.07678v4 [cs.CV].
Carrasco-Davis, R., Cabrera-Vives, G., Frster,
F., et al. 2018, Deep learning for image se-
quence classiﬁcation of astronomical events,
arXiv:1807.03869v3 [astro-ph.IM].

Chetlur, S., Woolley, C., Vandermersch, P., et al.
2014, cuDNN: eﬃcient primitives for deep learn-
ing, arXiv:1410.0759v3 [cs.NE].

Connor, L. & Leeuwen, J. van 2018, Applying deep
learning to fast radio burst classiﬁcation, AJ,
156, 256.

He, K., Gkioxari, G., Dollr,P., Girshick, R. 2018,
Mask R-CNN, arXiv:1703.06870v3 [cs.CV].
He, K., Zhang, X., Ren, S., Sun, J. 2015,
Deep Residual Learning for Image Recognition,
arXiv:1512.03385v1 [cs.CV].

Huang, G., Liu, Z., Maaten, L.van der, Weinberger,
K.Q. 2018, Densely connected convolutional net-
works, arXiv:1608.06993v5 [cs.CV].

Kim, J., Lee, S., Oh, T.-H., & Kweon,

I.S.
2017, Co-domain embedding using deep quadru-
plet networks for unseen traﬃc sign recognition,
arXiv:1712.01907 [cs.CV].

Kulkarni, N., Lubin, P., & Zhang, Q. 2017, Rela-
tivistic spacecraft propelled by directed energy,
arXiv:1710.10732 [astro-ph.IM].

Marsland, S., Shapiro, J. & Nehmzow, U. 2002,
A self-organising network that grows when re-
quired, Neural Networks, 15, Issue 8-9, 1041-
1058.

McDuﬀ, D., Cheng, R., & Kapoor, A. 2018,
simulation,

in AI using

Identifying bias
arXiv:1810.004171 [cs.LG].

Morad, S., Nallapu, R.T., Kalita, H., et al. 2018,
On-Orbit smart camera system to observe illu-
minated and unilluminated space objects, arXiv:
1809.02042 [cs.CV].

Pearson, K.A., Palafox, L., & Griﬃth, C.A. 2017,
Searching for exoplanets using artiﬁcial intelli-
gence, arXiv:1706.04319v2 [astro-ph.IM].

Rajpurkar, R., Irvin, J., Zhu, K., et al. 2017,
radiologist-level pneumonia detec-
CheXNet:
tion on chest x-rays with deep learning,
arXiv:1711.05225v3 [cs.CV].

Ruﬃo, J.-B., Mawet, D., Czekala, I., et al. 2018, A
bayesian framework for exoplanet direct detec-
tion and non-detection, AJ, 156, 196.

Schaefer, C., Geiger, M., Kuntzer, T., & Kneib, J.-
P. 2018, Deep convolutional neural networks as

16

